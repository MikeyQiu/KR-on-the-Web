7
1
0
2
 
r
a

M
 
6
1
 
 
]

G
L
.
s
c
[
 
 
2
v
8
9
2
5
0
.
3
0
7
1
:
v
i
X
r
a

Neural Networks for Beginners
A fast implementation in Matlab, Torch, TensorFlow

F. Giannini1, V. Laveglia1,2, A. Rossi1,3∗, D. Zanca1,2, A. Zugarini1

1DIISM, University of Siena, Siena, Italy
2DINFO, University of Florence, Florence, Italy
3Fondazione Bruno Kessler, Trento, Italy

rossi111@unisi.it
{giannini7, andrea.zugarini}@student.unisi.it
{vincenzo.laveglia, dario.zanca}@unifi.it

March 17, 2017

What is this report about?

This report provides an introduction to some Machine Learning tools within the most common
development environments. It mainly focuses on practical problems, skipping any theoretical intro-
duction. It is oriented to both students trying to approach Machine Learning and experts looking
for new frameworks.

The dissertation is about Artiﬁcial Neural Networks (ANNs [1, 2]), since currently is the most
trend topic, achieving state of the art performance in many Artiﬁcial Intelligence tasks. After a ﬁrst
individual introduction to each framework, the setting up of general practical problems is carried
out simultaneously, in order to make the comparison easier.

Since the treated argument is widely studied and in continuos and fast growing, we pair this
document with an on-line documentation available at the Lab GitHub repository [3] which is more
dynamic and we hope to be kept updated and possibly enlarged.

∗Corresponding Author

1

Contents

1 Matlab: a uniﬁed friendly environment

1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Setting up the XOR experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Stopping criterions and Regularization . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4 Drawing separation surfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Torch and Lua environment

2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Getting started . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.1 Lua . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.2 Torch enviroment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Setting up the XOR experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Stopping criterions and Regularization . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5 Drawing Separation Surfaces

3 TensorFlow

3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Getting started . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.1 Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.2 TensorFlow environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Installation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.3
3.3 Setting up the XOR experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 MNIST Handwritten Characters Recognition

4.1 MNIST on Matlab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 MNIST on Torch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3 MNIST on Tensor Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Convolutional Neural Networks

5.1 Matlab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Torch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3 Tensor Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 A critical comparison

6.1 Matlab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.2 Torch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.3 Tensor Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.4 An overall picture on the comparison . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.5 Computational issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3
3
4
7
7

10
10
10
10
10
12
14
15

18
18
18
18
18
19
19

23
23
26
30

35
35
38
40

44
44
44
44
45
46

2

1 Matlab: a uniﬁed friendly environment

1.1 Introduction

Matlab R(cid:13) [4] is a very powerful instrument allowing an easy and fast handling of almost every kind
of numerical operation, algorithm, programming and testing. The intuitive and friendly interactive
interface makes it easy to manipulate, visualize and analyze data. The software provides a lot
of mathematical built-in functions for every kind of task and an extensive and easily accessible
documentation. It is mainly designed to handle matrices and, hence, almost all the functions and
operations are vectorized, i.e. they can manage scalars, as well as vectors, matrices and (often)
tensors. For these reasons, it is more eﬃcient to avoid loops cycles (when possible) and to set up
operations exploiting matrices multiplication.

TM

and the Neural Network Toolbox

In this document we just show some simple Machine Learning related instruments in order to
start playing with ANNs. We assume a basic-level knowledge and address to oﬃcial documentation
for further informations. For instance, you can ﬁnd informations on how to obtain the software
from the oﬃcial web site1. Indeed, the license is not for free and even if most universities provide a
classroom license for students use, maybe could not be possible to access to all the current packages.
In particular the Statistic and Machine Learning Toolbox
provide a lot of built-in functions and models to implement diﬀerent ANNs architectures suitable
to face every kind of task. The access to both the tools is fundamental in the prosecution, even if
we refer to some simple independent examples. The most easy to-go is the nnstart function, which
activates a simple GUI guiding the user trough the deﬁnition of a simple 2-layer architecture. It
allows either to load available data samples or to work with customize data (i.e. two matrices of input
data and correspondent target), train the network and analyze the results (Error trend, Confusion
Matrix, ROC, etc.). However, more functions are available for speciﬁc tasks. For instance, the
function patternnet is speciﬁcally designed for pattern recognition problems, newfit is suitable for
regression, whereas feedforwardnet is the most ﬂexible one and allows to build very customized and
complicated networks. All the versions are implemented in a similar way and the main options and
methods apply to everyone. In the next section we show how to manage customizable architectures
starting to face very basic problems. Detailed informations can be ﬁnd in a dedicated section of the
oﬃcial site2.

TM

CUDA R(cid:13) computing

TM

GPU computing in Matlabrequires the Parallel Computing Toolbox
and the CUDA R(cid:13) instal-
lation on the machine. Detailed informations on how to use, check and set GPUs devices can be
found in GPU computing oﬃcial web page3, where issues on Distributed Computing CPUs/GPUs
are introduced too. However, basic operations with graphical cards should in general be quite sim-
ple. Data can be moved to the GPU hardware by the function gpuArray, then back to the CPU
by the function gather. When dealing with ANNs, a dedicated function nndata2gpu is provided,
organizing tensors (representing a dataset) in a eﬃcient conﬁguration on the GPU, in order to speed
up the computation. An alternative way is to carry out just the training process in the GPU by
the correspondent option of the function train (which will be describe in details later). This can

1https://ch.mathworks.com/products/matlab.html?s_tid=hp_products_matlab
2http://ch.mathworks.com/help/nnet/getting-started-with-neural-network-toolbox.html
3https://ch.mathworks.com/help/nnet/ug/neural-networks-with-parallel-and-gpu-computing.html

3

be done directly by passing additional arguments, in the Name,Values pair notation, the option
’useGPU’ and the value ’yes’:

1

nn = t r a i n ( nn ,

. . .

,

’ useGPU ’ , ’ y e s ’ )

1.2 Setting up the XOR experiment

The XOR is a well-known classiﬁcation problem, very simple and eﬀective in order to understand
the basic properties of many Machine Learning algorithms. Even if writing down an eﬃcient and
ﬂexible architecture requires some language expertise, a very elementary implementation can be
found in the Matlabsection of the GitHub repository4 of this document. It is not suitable to face
real tasks, since no customizations (except for the number of hidden units) are allowed, but can
be useful just to give some general tips to design a personal module. The code we present is basic
and can be easily improved, but we try to keep it simple just to understand fundamental steps. As
we stressed above, we avoid loops exploiting the Matlabeﬃciency with matrix operations, both in
forward and backward steps. This is a key point and it can substantially aﬀects the running time
for large data.

Initialization

TM

Here below, we will see how to deﬁne and train more eﬃcient architectures exploiting some built-in
functions from the Neural Network Toolbox
. Since we face the XOR classiﬁcation problem, we
sort out our experiments by using the function patternnet. To start, we have to declare an object
of kind network by the selected function, which contains variables and methods to carry out the
optimization process. The function expects two optional arguments, representing the number of
hidden units (and then of the hidden layers) and the back-propagation algorithm to be exploited
during the training phase. The number of hidden units has to be provided as a single integer number,
expressing the size of the hidden layer, or as an integer row vector, whose elements indicate the size
of the correspondent hidden layers. The command:

1

nn = p a t t e r n n e t ( 3 )

creates on object named nn of kind network, representing a 2-layer ANN with 3 units in the
single hidden layer. The object has several options, which can be reached by the dot notation
object.property or explore by clicking on the interactively visualization of the object in the Mat-
labCommand Window, which allows to see all the available options for each property too. The
second optional parameter selects the training algorithm by a string saved in the trainFcn property,
which in the default case takes the value ’trainscg’ (Scaled Conjugate Gradient Descent methods).
The network object is still not fully deﬁned, since some variables will be adapted to ﬁt the data
dimension at the calling of the function train. However, the function configure, taking as input
the object and the data of the problem to be faced, allows to complete the network and set up the
options before the optimization starts.

4Available at https://github.com/AILabUSiena/NeuralNetworksForBeginners/tree/master/matlab/2layer.

4

Dataset

Data for ANNs training (as well as for others available Machine Learning methods) must be provided
in matrix form, storing each sample column-wise. For example data to deﬁne the XOR problem can
be simply deﬁned via an input matrix X and a target matrix Y as:

1

X = [ 0 , 0 , 1 , 1 ; 0 , 1 , 0 , 1 ]
Y = [ 0 , 1 , 1 , 0 ]

Matlabexpects targets to be provided in 0/1 form (other values will be rounded). For 2-class
problem targets can be provided as a row vector of the same length of the number of samples. For
multi-class problem (and as an alternative for 2-class problem too) targets can be provided in the
one-hot encoding form, i.e. as a matrix with as many columns as the number of samples, each one
composed by all 0 with only a 1 in the position indicating the class.

Conﬁguration

nn = c o n f i g u r e ( nn , X,Y)

Once we have deﬁned data, the network can be fully deﬁned and designed by the command:

For each layer, an object of kind nnetLayer is created and stored in a cell array under the ﬁeld
layers of the network object. The number of connections (the weights of the network) for each
units corresponds to the layer input dimension. The options of each layer can be reached by the
dot notation object. layer{numberOf Layer}.property. The ﬁeld initFcn contains the weights
initialization methods. The activation function is stored in the transferFcn property.
In the
hidden layers the default values is the ’tansig’ (Hyperbolic Tangent Sigmoid), whereas the output
layers has the ’logsig’ (Logistic Sigmoid) or the ’sof tmax’ for 1-dimensional and multi-dimensional
target respectively. The ’crossentropy’ penalty function is set by default in the ﬁeld performFcn.
At this point, the global architecture of the network can be visualized by the command:

1

v iew ( nn )

Training

The function train itself makes available many options (as for instance useParallel and useGPU
for heavy computations) directly accessible from its interactive help window. However, it can take
as input just the network object, the input and the target matrices. The optimization starts by
dividing data in Training, Validation and Test sets. The splitting ratio can be changed by the options
divideParam. In the default setting, data are randomly divided, but if you want for example to
decide which data are used for test, you can change the way the data are distributed by the option
divideFcn5. In this case, because of the small size of the dataset, we drop validation and test by
setting:

5Click on divideFcn property from the MatlabCommand Window visualization of your object to see the available

methods.

5

1

nn . d i v i d e F c n = ’ ’

In the following code, we set the training function to the classic gradient descent method ’traingd’,
we deactivate the training interactive GUI by nn.trainParam.showWindow (boolean) and activate
the printing of the training state in the Command Window by nn.trainParam.showCommandLine
(boolean). Also the learning rate is part of the trainParam options under the ﬁelds lr.

1

3

nn . t r a i n F c n = ’ t r a i n g d ’
nn . trainParam . showWindow = 0
nn . trainParam . showCommandLine = 1
nn . trainParam . l r = 0 . 0 1

Training starts by the calling:

[ nn ,

t r ] = t r a i n ( nn , X, Y)

this generates a printing, ending in this case with:

This indicates that the training stops after the max number of epoch is reached (which can be set by
options object. trainParam.epochs). Each column shows the state of one of the stopping criterions
used, which we will analyze in details in the next section. The output variable tr stores the training
options. The ﬁelds perf, vperf and tperf contain the performance of the network evaluated at
each epoch on the Training, Validation and Test sets respectively (the last two are NaN in this case),
which can be used for example to plot performances. If we pass data organized in a single matrix,
the function will exploit the full batch learning method accumulating gradients overall the training
set. To set a mini-batch mode, data have to be manually split in sub-matrix with the same number
of column and organized in a cell array. However, let us consider for a moment a general data set
composed by N samples in the features space RD with a target of dimension C, so that X ∈ RD×N
and Y ∈ RC×N . All the mini-batches have to be of the same size b, so that it is in general convenient
to choose the batch size to be a factor of N . In this case, we can generate data for the training
function organizing the input and target in the correspondent cell-array by:

1 N = s i z e (X, 2 ) ; % number o f

s a m p l e s

n_batch = N/ b a t c h s i z e ; % number o f b a t c h e s

3

i n p u t { n_batch } = [ ] ; % i n p u t c e l l −a r r a y i n i t i a l i z a t i o n
5 t a r g e t { n_batch} = [ ] ; % t a r g e t c e l l −a r r a y i n i t i a l i z a t i o n

7 p = randperm (N) ; % g e n e r a t i n g a random permutated i n d e x f o r data s h u f f l i n g

X = X( : , p ) ; % s a m p l e s p e r m u t a t i o n

9 Y = Y( : , p ) ; % t a r g e t permutaion

6

11 f o r

i =1: n_batch

i n p u t { i } = X ( : , ( 1 : b a t c h s i z e ) +( i −1)∗ b a t c h s i z e ) ;
t a r g e t { i } = Y ( : , ( 1 : b a t c h s i z e ) +( i −1)∗ b a t c h s i z e ) ;

13

end

However, in order to perform a pure Stochastic Gradient Descent optimization, in which the ANNs
parameters are updated for each sample, the training function ’trains’ have to be employed
skipping to split data previously. A remark has to be done since this particular function does not
support the GPU computing.

The network (trained or not) can be easily evaluated on data by passing the input data as
argument to a function named as the network object. Performance of the prediction with respect to
the targets can be evaluated by the function perform according to the correspondent loss function
option object. performFcn:

f = nn (X)
perform ( nn , Y,

f )

2

1.3 Stopping criterions and Regularization

TM

Early stopping is a well known procedure in Machine Learning to avoid overﬁtting and improve
generalization. Routines from Neural Network Toolbox
use diﬀerent kind of stopping criterions
regulated by the network object options in the ﬁeld trainParam. Arbitrarily methods are based on
the number of epochs (epochs) and the training time (time, default Inf ). A criterion based on
the training set check the loss (object. trainParam.goal, default = 0) or the parameters gradients
(object. trainParam.min_grad, default = 10−6) to reach a minimum threshold. A general early
stopping method is implemented by checking the error on the validation set and interrupting training
when validation error does not improve for a number of consecutive epochs given by max_fail
(default = 6).

Further regularization methods can be conﬁgured by the property performParam of the network
object. The ﬁeld regularization contains the weight (real number in [0, 1]) balancing the contri-
bution of a term trying to minimizing the norm of the network weights versus the satisfaction of the
penalty function. However, the network is designed to mainly rely on the validation checks, indeed
regularization applies only to few kind of penalties and the default weight is 0.

1.4 Drawing separation surfaces

When dealing with low dimensional data (as in the XOR case), can be useful to visualize the
prediction of the network directly in the input space. For this kind of task, Matlabmakes available
a lot of built-in functions with many options for interactive data visualization. In this section, we
will show the main functions useful to realize customized separation surfaces learned by an ANN
with respect to some speciﬁc experiments. We brieﬂy add some comments for each instruction,
referring to the suite help for speciﬁc knowledge of each single function. The network predictions
will be evaluated on a grid of the input space, generated by the Matlabfunction meshgrid, since
the main functions used for the plotting (contour or, if you want a color surface pcolor) require

7

as input three matrices of the same dimensions expressing, in each correspondent element, the
coordinates of a 3-D point (which in our case will be ﬁrst input dimension, second input dimension
and prediction). Once we trained the network described until now, the boundary for the 2-classes
separation showed in Figure 1a is generated by the code in Listing 1, whereas in Figure 1b we
report the same evaluation after the training of a 4-layers network using 5, 3 and 2 units in the ﬁrst,
second and third hidden layers respectively, each one using the ReLU as activation (’poslin’ in
Matlab). This new network can be deﬁned by:

n = p a t t e r n n e t ( [ 5 , 3 , 2 ] ) ; % 3+output

l a y e r s network i n i t i a l i z a t i o n

2 nn = c o n f i g u r e ( n , X,Y) ; % network c o n f i g u r a t i o n

nn . t r a i n F c n = ’ t r a i n g d ’ ; % s e t t i n g o p t i m i z a t i o n f u n c t i o n

4 nn . div ideParam . t r a i n R a t i o = 1 ; % s e t t i n g data s p l i t t i n g r a t i o s

( i l l u s t r a t i v e )

nn . div ideParam . v a l R a t i o = 0 ;
6 nn . div ideParam . t e s t R a t i o = 0 ;

nn . trainParam . showCommandLine = 1 ;

8 nn . trainParam . l r = 0 . 0 1 ;

10 nn . l a y e r s { 2 } . t r a n s f e r F c n = ’ p o s l i n ’ ;
nn . l a y e r s { 3 } . t r a n s f e r F c n = ’ p o s l i n ’ ;

nn . l a y e r s { 1 } . t r a n s f e r F c n = ’ p o s l i n ’ ; % s e t t i n g t h e a c t i v a t i o n l a y e r −w i s e

Listing 1: Drawing separation surfaces

Separation Surfaces

Separation Surfaces

Classes Bound
Class 0
Class 1

Classes Bound
Class 0
Class 1

1.5

2

1

0

2

x

0.5

X2

X1

1.5

2

1

0

2

x

0.5

X2

X1

X4

X3

X4

X3

-0.5

-0.5

0

0.5

1

1.5

2

-0.5

-0.5

0

0.5

1

1.5

2

x

1

(a) 2-layers, 3 Hidden Units

x

1

(b) 4-layers, 5, 3 and 2 Hidden Units, ReLU for
all the activations

Figure 1: Separation surfaces on the XOR classiﬁcation task.

5 % network e v a l u a t i o n on t h e g r i d d i n g ( r e s h a p e d t o f i t network i n p u t d i m e n s i o n )

1 % % % % P l o t t i n g S e p a r a t i o n S u r f a c e

3 % g e n e r a t i n g i n p u t s p a c e g r i d

[ xp1 , xp2 ] = meshgrid ( − 0 . 5 : . 0 1 : 2 , − 0 . 5 : . 0 1 : 2 ) ;

f = nn ( [ xp1 ( : ) ’ ; xp2 ( : ) ’ ] ) ;

7 % r e s h a p i n g p r e d i c t i o n i n c o r r e s p o n d e n t matrix form

f = r e s h a p e ( f , s i z e ( xp1 , 1 ) , [ ] ) ;

8

% drawing s e p a r a t i o n s u r f a c e s

11 c o n t o u r ( xp1 , xp2 , f , [ . 5 , . 5 ] , ’ LineWidth ’ , 3 , ’ C o l o r ’ , ’ c ’ ) ;

h o l d on ;

% drawing data p o i n t s

15 s c a t t e r (X( 1 , [ 1 , 4 ] ) ,X( 2 , [ 1 , 4 ] ) , 2 0 0 , ’ o ’ , ’ f i l l e d ’ , ’ MarkerEdgeColor ’ , ’ k ’ , . . .

17 s c a t t e r (X( 1 , [ 2 , 3 ] ) ,X( 2 , [ 2 , 3 ] ) , 2 0 0 , ’ ^ ’ , ’ f i l l e d ’ , ’ MarkerEdgeColor ’ , ’ k ’ , . . .

’ Mark erFaceColor ’ , ’ k ’ , ’ LineWidth ’ , 2 ) ;

’ Mark erFaceColor ’ , ’ k ’ , ’ LineWidth ’ , 2 ) ;

a x i s ( [ − 0 . 5 , 2 , − 0 . 5 , 2 ] ) ; % s e t t i n g a x i s bounds

% l a b e l i n g data p o i n t s

23 c = { ’X^1 ’ , ’X^2 ’ , ’X^3 ’ , ’X^4 ’ } ; % l a b e l s

dx = [ − . 1 5 , −.15 ,

. 1 ,

. 1 ] ; % l a b e l s h o r i z o n t a l

t r a n s l a t i o n wrt p o i n t s

25 dy = [ − . 1 ,

. 1 , −.1 ,
t e x t (X( 1 , : )+dx , X( 2 , : )+dy , c ,

. 1 ] ; % l a b e l s v e r t i c a l

t r a n s l a t i o n wrt p o i n t s
’ F o n t S i z e ’ , 1 4 ) ; % showing l a b e l s a s

t e x t

9

13

19

21

27

% p l o t

l a b e l s

29 x l a b e l ( ’ x_1 ’ , ’ F o n t S i z e ’ , 1 4 )
y l a b e l ( ’ x_2 ’ , ’ F o n t S i z e ’ , 1 4 )

31 t i t l e ( ’ S e p a r a t i o n S u r f a c e s ’ , ’ F o n t S i z e ’ , 1 6 ) ;

33 h = l e g e n d ( { ’ C l a s s e s Bound ’ , ’ C l a s s 0 ’ , ’ C l a s s 1 ’ } , ’ L o c a t i o n ’ , ’ NorthEast ’ ) ;

35 s e t ( h , ’ F o n t S i z e ’ , 1 4 ) ;

9

2 Torch and Lua environment

2.1 Introduction

Torch7 is an easy to use and eﬃcient scientiﬁc computing framework, essentially oriented to Machine
Learning algorithms. The package is written in C which guarantees an high eﬃciency. However, a
completely interaction is possible (and usually convenient) by the LuaJIT interface, which provides
a fast and intuitively scripting language. Moreover, it contains all the libraries necessary for the
integration with the CUDA R(cid:13) environment for GPU computing. At the moment of writing it is
one of the most used tool for prototyping ANNs of any kind of topology. Indeed, there are many
packages, constantly updated and improved by a large community, allowing to develop almost any
kind of architectures in a very simple way.

Informations about the installation can be found at the getting started section of the oﬃcial
site6. The procedure is straightforward for UNIX based operative systems, whereas is not oﬃcially
supported for Windows, even if an alternative way is provided7. If CUDA R(cid:13) is already installed, also
the packages cutorch and cunn will be added automatically, containing all the necessary utilities
to deal with Nvidia GPUs.

2.2 Getting started

2.2.1 Lua

Lua, in torch7, acts as an interface for C/CUDA routines. A programmer, in most of the cases, will
not have to worry about C functions. Therefore, we explain here only how the Lua language works,
because is the only one necessary to deal with Torch. It is a scripting language with a syntax similar
to Python and semantic close to Javascript. A variable is considered as global by default. The local
declaring, which is usually recommended, require the explicit declaration by placing the keyword
local before the name fo the variable. Lua has been chosen over other scripting languages, such
as Python, because is the fastest one, a crucial feature when dealing with large data and complex
programs, as common in Machine Learning.

There are seven native types in lua: nil, boolean, number, string, userdata, function
and table, even if most of the Lua power is related to the last one. A table behaves either as
an hash map (general case) or as an array (which have the 1-based indexing as in Matlaband
Python). The table will be considered as an array when contains only numerical keys, starting from
the value 1. Any other complex structure such as classes, are built from it (formally deﬁned as a
Metatable).

A detailed documentation on Lua can be ﬁnd at the oﬃcial webpage8, however, an essential and

fast introduction can be found at http://tylerneylon.com/a/learn-lua/.

2.2.2 Torch enviroment

Torch extends the capabilities of the Lua table implementing the Tensor class. Many Matlab-like
functions9 are provided in order to initialize and manipulate tensors in a concise fashion. Most

6http://torch.ch/docs/getting-started.html
7https://github.com/torch/torch7/wiki/Windows
8Lua 5.1 reference manual is available here: https://www.lua.org/manual/5.1/
9http://atamahjoubfar.github.io/Torch_for_Matlab_users.pdf

10

commons are reported in Listing 2.

1 l o c a l
l o c a l
3 l o c a l

t 1 = t o r c h . T e n s o r ( ) −− no d i m e n s i o n t e n s o r c o n s t r u c t o r
t 2 = t o r c h . T e n s o r ( 4 , 3 ) −− 4 x3 empty t e n s o r
t 3 = t o r c h . e y e ( 3 , 5 ) −− 3 x5 1− d i a g o n a l matrix

t 2 : f i l l ( 1 ) −− f i l l

t h e matrix with t h e v a l u e 1

5 t 1 = torch.mm ( t2 , t 3 ) −− a s s i g n t o t 1 t h e r e s u l t o f matrix m u l t i p l i c a t i o n between t 2

and t 3

t 1 [ 1 ] [ 2 ] = 5 −− a s s i g n 5 t o t h e e l e m e n t

i n f i r s t

row and s e c o n d column

Listing 2: Example of torch tensor basic usages

All the provided packages are developed following a strong modularization, which is a crucial
feature to keep the code stable and dynamic. Each one provides several already built-in function-
alities, and all of them can be easily imported from Lua code. The main one is, of course, torch,
which is installed at the beginning. Not all the packages are included at ﬁrst installation, but it is
easy to add a new one by the shell command:

luarocks install packagename

where luarocks is the package manager, and packagename is the name of the package you want to
install.

The nn package

All (almost) you need to create (almost) any kind of ANNs is contained in the nn package (which is
usually automatically installed). Every element inside the package inherits from the abstract Lua
class nn.Module. The main state variables are output and gradInput, where the result of forward
and backward steps (in back-propagation) will be stored. forward and backward are methods of
such class (which can be accessed by the object:method() notation). They invoke updateOutput
and updateGradInput respectively, that here are abstract and the deﬁnition must be in the derived
classes.

The main advantage of this package is that all the gradients computations in the back-propagation
step are automatically realized thanks to these built-in functions. The only requirement is to call
the forward step before the backward.

The weights of the network will be updated by the method updateGradParameters, assigning
a new value to each parameter of a module (according to the Gradient Descent rule) exploiting the
learning rate passed an argument of the function.

The bricks you can use to construct a network can be divided as follows:

• Simple layers: the common modules to implement a layer. The main is nn.Linear, com-

puting a basic linear transformation.

• Transfer functions: here you can ﬁnd many activation functions, such as nn.Sigmoid or

nn.Tanh

• Criterions: loss functions for supervised tasks, a for instance is nn.MSECriterion

• Containers: abstract modules that allow us to build multi-layered networks. nn.Sequential
connect several layers in a feed-forward manner. nn.Parallel and nn.Concat are important
to build more complex structure, where the input ﬂows in separated architectures. Layers,
activation functions and even criterions can be added inside those containers.

11

For detailed documentation of the nn package we refer to the oﬃcial webpage10. Another useful
package for whom could be interested on building more complex architectures can be found at the
nngraph repository11.

CUDA R(cid:13) computing

Since C++/Cuda programming and integration are not trivial to develop, it is important to have
an interface as simple as possible linking such tools. Torch provides a clean solution for that with
the two dedicated packages cutorch and cunn (requiring, of course, a capable GPU and CUDA R(cid:13)
installed). All the objects can be transferred into the memory of GPUs by the method :cuda()
and then back to the CPU by :double(). Operations are executed on the hardware of the involved
In Listing 3 we show some
objects and are possible only among variables from the same unit.
examples of correct and wrong statements.

l o c a l cpuTensor1 = t o r c h . T e n s o r ( 3 , 3 ) : f i l l ( 1 )
2 l o c a l cpuTensor2 = t o r c h . T e n s o r ( 3 , 3 ) : f i l l ( 2 )

l o c a l cudaTensor1 = t o r c h . T e n s o r ( 3 , 3 ) : f i l l ( 3 ) : cuda ( )
4 l o c a l cudaTensor2 = t o r c h . T e n s o r ( 3 , 3 ) : f i l l ( 4 ) : cuda ( )

6 cpuTensor1 : cmul ( cpuTensor2 ) −− OK

cpuTensor1 : cmul ( cudaTensor2 ) −− WRONG

8 cudaTensor1 : cmul ( cudaTensor2 ) −− OK

cudaTensor1 : cmul ( cpuTensor2 ) −− WRONG

Listing 3: Samples of CUDA operations.

2.3 Setting up the XOR experiment

In order to give a concrete feeling about the presented tools, we show some examples on the classical
XOR problem as in the previous section. The code showed here below can be found in the Torch
section of the document’s GitHub repository12 and can be useful to play with the parameters and
become more familiar with the environment.

Architecture

1 r e q u i r e

’ nn ’

When writing a a script, the ﬁrst command is usually the import of all the necessary packages by
the keyword require. In this case, only the nn toolbox is required:

We deﬁne a standard ANNs with one hidden layer composed by 2 hidden units, the hyperbolic
tangen (tanh) as transfer function and identity as output function. The structure of the network
will be stored in a container where all the necessary modules will be added. A standard feed-forward
architecture can be deﬁned into a Sequential container, which we named mlp. The network can
be then assembled by adding sequentially all the desired modules by the function add():

10https://github.com/torch/nn
11Detailed documentation at https://github.com/torch/nngraph
12Available at https://github.com/AILabUSiena/NeuralNetworksForBeginners/tree/master/torch/xor.

12

1 mlp = n n . S e q u e n t i a l ( ) −− c o n t a i n e r

i n i t i a l i z a t i o n

3 mlp : add ( n n . L i n e a r ( i n p u t s , HUs) ) −− f i r s t

i n p u t s = 2 ; o u t p u t s = 1 ; HUs = 2 ; −− g e n e r a l o p t i o n s
l a y e r
t h e hidden l a y e r
l a y e r

5 mlp : add ( n n . L i n e a r (HUs , o u t p u t s ) ) −− output

mlp : add ( nn.Tanh ( ) ) −− a c t i v a t i o n f o r

l i n e a r

Listing 4: Code to create a 2-layer ANNs

Dataset

The training set will be composed by a tensor of 4 samples (organized again column-wise) paired
with a tensor of targets. Usually, true and false boolean values are respectively associated to 1 and
0. However, just to propose an equivalent but diﬀerent approach, here we shift both values by −0.5,
so they will be in [−0.5, 0.5] as showed in Listing 5. Both input and target are initialized with false
values (a tensor ﬁlled with 0), and then true values are placed according to the XOR truth table.

1 l o c a l d a t a s e t = t o r c h . T e n s o r ( 4 , 2 ) : f i l l ( 0 )

l o c a l

t a r g e t = t o r c h . T e n s o r ( 4 , 1 ) : f i l l ( 0 )

3 d a t a s e t [ 2 ] [ 1 ] = 1 −− True F a l s e
d a t a s e t [ 3 ] [ 2 ] = 1 −− F a l s e True

5 d a t a s e t [ 4 ] [ 1 ] = 1 ; d a t a s e t [ 4 ] [ 2 ] = 1 −− True True

d a t a s e t = d a t a s e t : add(−0 . 5 ) −− s h i f t

t r u e and f a l s e by −0 . 5

7 t a r g e t [ 2 ] [ 1 ] = 1 ;

t a r g e t [ 3 ] [ 1 ] = 1

t a r g e t = t a r g e t : add(−0 . 5 )

Listing 5: creation of 4 examples and their targets

Training

We set up a full–batch mode learning, i.e. we update the parameters after accumulating the gradients
over the whole dataset. We exploit the following function:

forward(input) returns the output of the multi layer perceptron w.r.t the given input; it updates
the input/output states variables of each modules, preparing the network for the backward
step; its output will be immediately passed to the loss function to compute the error.

zeroGradParameters() resets to null values the state of the gradients of the all the parameters.

backward(gradients) actually computes and accumulates (averaging them on the number of sam-
ples) the gradients with respect to the weights of the network, given the data in input and
the gradient of the loss function.

updateParameters(learningrate) modiﬁes the weights according to the Gradient Descent proce-

dure using the learning rate as input argument.

As loss function we use the Mean Square Error, created by the statement:

c r i t e r i o n = nn.MSECriterion ( )

13

When a criterion is forwarded, it returns the error between the two input arguments. It updates
its own modules state variable and gets ready to compute the gradients tensor of the loss in the
backward step, which will be back-propagated through the multilayer perceptron. As a nn modules,
all the possible criterions used the functions forward() and backward() as the others. The whole
training procedure can be set up by:

1 nepochs = 1 0 0 0 ;

l e a r n i n g _ r a t e = 0 . 0 5 ; −− g e n e r a l o p t i o n s

l o c a l

l o s s = t o r c h . T e n s o r ( nepochs ) : f i l l ( 0 ) ; −− t r a i n i n g l o s s

i n i t i a l i z a t i o n

3 f o r

i = 1 , nepochs do −− i t e r a t i n g o v e r 1000 e p o c h s

5

7

9

i n p u t = d a t a s e t

l o c a l
l o c a l o u t p u t s = mlp : f o r w a r d ( i n p u t ) −− network f o r w a r d s t e p
t a r g e t ) −− e r r o r
l o s s [ i ] = c r i t e r i o n : f o r w a r d ( outputs ,
mlp : zeroGradParame te rs ( ) −− z e r o r e s e t o f g r a d i e n t s
l o c a l g r a d i e n t s = c r i t e r i o n : backward ( mlp.output ,
mlp : backward ( input , g r a d i e n t s ) −− network backward s t e p
mlp : updateParameters ( l e a r n i n g _ r a t e ) −− update p a r a m e t e r s g i v e n t h e l e a r n i n g r a t e

t a r g e t ) −− l o s s g r a d i e n t s

e v a l u a t i o n

11 end

Listing 6: training of the network

2.4 Stopping criterions and Regularization

Since the training procedure is manually deﬁned, particular stopping criterion are completely up to
the user. The simplest one, based on the reaching of a ﬁxed number of epochs explicitly depends of
the upper bound of the for cycle. Since other methods are related to the presence of a validation
set, we will deﬁne an example of early stopping criterion in Listing 14 in Section 4. A simple
criterion based on the vanishing of the gradients can be simply set up by exploiting the function
getParameters deﬁned for the modules of nn, which returns all the weights and the gradients of
the network in two 1-Dimensional vector:

1 param , grad = mlp : g e t P a r a m e t e r s ( )

A simple check on the minimum value of the absolute values of gradients saved in grad can be used
to stop the training procedure.

Another regularization method can be accomplished by implementing the weight decay method
as shown in Listing 7. The presented code is intended to be an introductory example even to
understand the class inheritance mechanisms in Lua and Torch.

1 −− d e f i n i n g c l a s s

i n h e r i t a n c e s

l o c a l WeightDecay , p a r e n t = t o r c h . c l a s s ( ’ nn.WeightDecayWrapper ’ ,

’ n n . S e q u e n t i a l ’ )

i s a keyword r e f e r r i n g t o t h e a b s t r a c t o b j e c t

3

5

7

9

11

13

f u n c t i o n WeightDecay : __init ( ) −− c o n s t r u c t o r

p a r e n t . _ _ i n i t ( s e l f )
s e l f . w e i g h t D e c a y = 0 −− s e l f
s e l f . c u r r e n t O u t p u t = 0

end

f u n c t i o n WeightDecay : getWeightDecay ( a l p h a )

l o c a l a l p h a = a l p h a o r 0
l o c a l
f o r

weightDecay = 0
i =1,# s e l f . m o d u l e s do

14

l o c a l params ,_ = s e l f . m o d u l e s [ i ] : p a r a m e t e r s ( )
i f params then

f o r

j =1,#params do

weightDecay = weightDecay + t o r c h . d o t ( params [ j ] , params [ j ] ) ∗ a l p h a /2

end

end

end
s e l f . w e i g h t D e c a y = weightDecay
r e t u r n s e l f . w e i g h t D e c a y

23 end

15

17

19

21

27

29

31

33

end

end

end

35 end

25 f u n c t i o n WeightDecay : updateParameters ( l e a r n i n g R a t e , a l p h a )

l o c a l a l p h a = a l p h a o r 0
i =1,# s e l f . m o d u l e s do
f o r

l o c a l params , gradParams = s e l f . m o d u l e s [ i ] : p a r a m e t e r s ( )
i f params then

f o r

j =1,#params do

params [ j ] : add(− l e a r n i n g R a t e , gradParams [ j ] + ( a l p h a ∗ params [ j ] ) )

Listing 7: Code to implement the weight decay regularization

To implement the weight decay coherently with the nn package, we need to create a novel class, inher-
iting from nn.Sequential, that overloads the method updateParameters() of the nn.Module. We
ﬁrst have to declare a new class name in Torch, where you can optionally specify the parent class. In
our case the new class has been called nn.WeightDecayWrapper, while the class it inherits from is the
Container nn.Sequential. The constructor is deﬁned within the function WeightDecay:__init().
In the scope of this function the variable self is a table used to refer to all the attributes and
methods of the abstract object. The calling of the function __init() from the parent class au-
tomatically add all the original properties. The functions WeigthDecay:getWeigthDecay() and
WeigthDecay:updateParameters() compute respectively the weight decay and its gradient. Both
methods loop over all the modules of the container (the symbol # returns the number-indexed
element of a table) and, for each one that has parameters, use them in order to compute either
the error or the gradients coming from the weight decay contribution. The argument alpha repre-
sent the regularization parameter of the weight decay and, if not provided, is assumed null. It is
also worth to mention the fact that, WeigthDecay:updateParameters() overloads the method that
implemented in nn.Module, updating the parameters according to the standard Gradient Descent
rule. At this point, an ANN expecting a possible weight decay regularization can be declared by
replacing the nn.Sequential container by the proposed nn.WeightDecayWrapper.

2.5 Drawing Separation Surfaces

In this framework, data visualization is allowed by the package gnuplot, which provides some tools
to plot points, lines, curves and so on. For example, in the training procedure presented in Listing 6,
a vector storing the penalty evaluated at each epoch is produced. To have an idea of the state of
the network during training, we can save an image ﬁle containing the trend of the error by the code
in Listing 8, whom output is shown in Figure 2(a).

15

1

3

5

g n u p l o t . e p s f i g u r e ( ’ XOR loss.eps ’ ) −− c r e a t e an e p s
g n u p l o t . p l o t ( { ’ l o s s
f u n c t i o n ’ ,
g n u p l o t . t i t l e ( ’ Loss ’ )
g n u p l o t . g r i d ( t r u e )
g n u p l o t . p l o t f l u s h ( )
g n u p l o t . f i g u r e ( )

t o r c h . r a n g e ( 1 , nepochs ) , l o s s } ) −− l o s s

t r e n d p l o t

f i l e with t h e image

Listing 8: Error evaluated at each training epoch.

Torch does not have dedicated functions to visualize separation surfaces produced on data and,
hence, we generate a random grid across the input space, plotting only those points predicted close
enough (with respect to a a certain threshold) to the half of possible target (0 in this case). The
correspondent result, showed in Figure 2(b), is generated by the code in Listing 9, exploiting the
fact that Lua support the logical indexing as in Matlab.

Loss

Xor

Separation Surface

 0.3

 0.25

 0.2

 0.15

 0.1

 0.05

 0

 0

 100

 200

 300

 400

 500

 600

 700

 800

 900

 1000

-0.5

-0.4

-0.3

-0.2

-0.1

 0

 0.1

 0.2

 0.3

 0.4

 0.5

(a)

(b)

Figure 2: (a) Trend of loss versus the number of epochs. (b) The estimated separation surface
obtained by a 2-Layers ANN composed by 2 Hidden Units, Hyperbolic Tangent as activation and
linear output.

−− i n p u t s p a c e g r i d by n 2D−p o i n t s u n i f o r m l y d i s t r i b u t e d i n [ −0 . 5 , 0 . 5 ]

l o c a l e p s = 2 e −3; −− s e p a r a t i o n t h r e s h o l d
n = 1000000 −− g r i d d i n g s i z e

2 l o c a l

4 l o c a l x = t o r c h . r a n d ( n , 2 ) : add(−0 . 5 )

l o c a l mlpOutput = mlp : f o r w a r d ( x ) −− e v a l u a t i o n

6 −− compare whether t h e a b s o l u t e v a l u e o f

t h e network output

i s

l e s s

e q u a l than e p s

l o c a l mask = t o r c h . l e ( t o r c h . a b s ( mlpOutput ) , mlpOutput : c l o n e ( ) : f i l l ( e p s ) )

8 i f

t o r c h . s u m ( mask ) > 0 then

g n u p l o t . e p s f i g u r e ( ’ S e p a r a t i o n s u r f a c e . e p s ’ )
l o c a l x1 = x : narrow ( 2 , 1 , 1 ) −− r e s h a p i n g f i r s t
i n p u t d i m e n s i o n f o r x a x i s
l o c a l x2 = x : narrow ( 2 , 2 , 1 ) −− r e s h a p i n g s e c i n d i n p u t d i m e n s i o n f o r y a x i s
−− p l o t t i n g o f
g n u p l o t . p l o t ( x1 [ mask ] , x2 [ mask ] ,
g n u p l o t . t i t l e ( ’ S e p a r a t i o n s u r f a c e ’ )

t h e c o l l e c t i o n o f p o i n t s

t h a t match t h e mask

’+ ’ )

10

12

14

 0.5

 0.4

 0.3

 0.2

 0.1

 0

-0.1

-0.2

-0.3

-0.4

-0.5

16

g n u p l o t . g r i d ( t r u e )
g n u p l o t . p l o t f l u s h ( ) −− s a v e t h e image
g n u p l o t . f i g u r e ( )

16

18 end

Listing 9: Drawing separation surfaces in torch

17

3 TensorFlow

3.1 Introduction

TensorFlow [5] is an open source software library for numerical computation and is the youngest with
respect to the others Machine Learning frameworks. It was originally developed by researchers and
engineers from the Google Brain Team, with the purpose of encourage research on deep architectures.
Nevertheless, the environment provides a large set of tools suitable for several domains of numerical
programming. The computation is conceived under the concept of Data Flow Graphs. Nodes in the
graph represent mathematical operations, while the graph edges represent tensors (multidimensional
data arrays). The core of the package is written in C++, but provides a well documented Python
API. The main characteristic is its symbolic approach, which allows a general deﬁnition of a forward
models, leaving the computation of the correspondent derivatives entirely to the environment itself.

3.2 Getting started

3.2.1 Python

A TensorFlow model can be easily written using Python, a very intuitive object-oriented program-
ming language. Python is distributed with an open-source license for commercial use too. It oﬀers
a nice integration with many other programming languages and provides an extended standard
library which includes numpy (modules designed for matrix operations, very similar to the Matlab
syntax). Python runs on Windows, Linux/Unix, Mac OS X and other operative systems.

3.2.2 TensorFlow environment

Assuming that the reader is familiar with Python, here we present the building blocks of TensorFlow
framework:

The Data Flow Graph To leverage the parallel computational power of multi-core CPU, GPU
and even clusters of GPUs, the dynamic of the numerical computations has been conceived as a
directed graph, where each node represents a mathematical operation and the edges describe the
input/output relation between nodes.

Tensor

It is a typed n-dimensional array that ﬂows through the Data Flow Graph.

Variable Symbolic objects designed to represent parameters. They are exploited to compute the
derivatives at a symbolical level, but in general must be explicitly initialized in a session.

Optimizer
It is the component which provides methods to compute gradients from the loss func-
tion and to apply back-propagation through all the variables. A collection is available in TensorFlow
to implement classic optimization algorithms.

Session A graph must be launched in a Session, which places the graph onto CPU or GPU and
provides methods to run computation.

18

3.2.3 Installation

Information about download and installation of Python and TensorFlow are available in the oﬃcial
webpages13. Notice that a dedicated procedure must be followed for GPU installation. It’s worth
a quick remark on the CUDA R(cid:13) versions. Indeed, versions from 7.0 are oﬃcially supported, but the
installation could be not straightforward in versions preceding the 8.0. Moreover, a registration to
the Accelerate Computing Developer Program 14 is required to install the package cuDNN, which is
mandatory to enable GPU support.

3.3 Setting up the XOR experiment

As in the previous sections of this tutorial, we show how to start managing the TensorFlow frame-
work by facing the simple XOR classiﬁcation problem by a standard ANN.

At the beginning, as for every Python library, we need to import the TensorFlow package by:

Import tensor ﬂow

import

t e n s o r f l o w a s

t f

ciao

Dataset deﬁnition

2

3

Again, data can be deﬁned as two matrices containing the input data and its correspondent target,
called X and Y respectively. Data can be deﬁned as a list or numpy array. After they will be used
to ﬁll the placeholder that actually deﬁne a type and dimensionality.

1 X = [ [ 0 , 0 ] , [ 0 , 1 ] , [ 1 , 0 ] , [ 1 , 1 ] ]

Y = [ [ 0 ] , [ 1 ] , [ 1 ] , [ 0 ] ]

Placeholders

TensorFlow provides Placeholders which are symbolic variables representing data during the com-
putation. A Placeholders object have to be initialized with given type and dimensionality, suitable
to represent the desired element. In this case we deﬁne two object x_ and y_ respectively for input
data and target:

x_ = t f . p l a c e h o l d e r ( t f . f l o a t 3 2 ,
2 y_ = t f . p l a c e h o l d e r ( t f . f l o a t 3 2 ,

shape = [ 4 , 2 ] )
shape = [ 4 , 1 ] )

13Python webpage: https://www.python.org/, TensorFlow webpage: https://www.tensorflow.org/
14https://developer.nvidia.com/accelerated-computing-developer

19

The description of the network depends essentially on its architecture and parameters (weights and
biases). Since the parameters have to be estimated, they are deﬁned as the variabile of the model,
whereas the architecture is determined by the conﬁguration of symbolic operations. For a 2-layers
ANN we can deﬁne:

Model deﬁnition

# Hidden u n i t s

2 HU = 3
# 1 s t

l a y e r

4 W1 = t f . V a r i a b l e ( t f . random_uniform ( [ 2 ,HU] , −1.0 , 1 . 0 ) ) # w e i g h t s matrix

b1 = t f . V a r i a b l e ( t f . z e r o s ( [HU] ) )

# b i a s

6 O = t f . nn . s i g m o i d ( t f . matmul (x_, W1) + b1 ) # non−l i n e a r a c t i v a t i o n output

# 2nd l a y e r

8 W2 = t f . V a r i a b l e ( t f . random_uniform ( [ HU, 1 ] , −1.0 , 1 . 0 ) )

b2 = t f . V a r i a b l e ( t f . z e r o s ( [ 1 ] ) )

10 y = t f . nn . s i g m o i d ( t f . matmul (O, W2) + b2 )

The matmul() function performs tensors multiplication. Variable() is the constructor of the class
variable. It needs an initialization value which must be a tensor. The function random_uniform()
returns a tensor of a speciﬁed shape, ﬁlled with valued picked from a uniform distribution between
two speciﬁed values. The nn module contains the most common activation functions, taking as
input a tensor and evaluating the non-linear transferring component-wise (the Logistic Sigmoid is
chosen in the reported example by tf.nn.sigmoid()).

Loss and optimizer deﬁnition

The cost function and the optimizer are deﬁned by the following two lines

# q u a d r a t i c l o s s

f u n c t i o n

2 c o s t = t f . reduce_sum ( t f . s q u a r e (y_ − y ) ,

r e d u c t i o n _ i n d i c e s = [ 0 ] )

# o p t i m i z i n g t h e f u n c t i o n c o s t by g r a d i e n t d e s c e n t with l e a r n i n g s t e p 0 . 1

4 t r a i n _ s t e p = t f . t r a i n . G r a d i e n t D e s c e n t O p t i m i z e r ( 0 . 1 ) . minimize ( c o s t )

TensorFlow provides functions to perform common operations between tensors. The function
reduce_sum() for example reduces the tensor to one (or more) dimension, by summing up along
the speciﬁed dimension. The train module provide the most common optimizers, which will be
employed during the training process. The previous code chose the Gradient Descent algorithm to
optimize the network parameters, with respect to the penalty function deﬁned in cost by using a
learning rate equal to 0.1.

At this point the variables are still not initialized. The whole graph exist at a symbolic level, but it
is instantiated when creating a session. For example, placeholders are fed with the assigned elements
in this moment.

Start the session

% C r e a t e s e s s i o n

2 s e s s = t f . S e s s i o n ( )

20

% I n i t i a l i z e v a r i a b l e s

4 s e s s . run ( t f . i n i t i a l i z e _ a l l _ v a r i a b l e s ( ) )

More speciﬁcally, initialize_all_variables() creates an operation (a node in the Data Flow
Graph) running variables initializer. The function Session() creates an instance of the class session,
while the correspondent method run() moves for the ﬁrst time the Data Flow Graph on CPU/GPU,
allocates variables and ﬁlls them with the initial values.

Training

The training phase can be deﬁned in a for loop where each iteration represent a single gradient
descend epoch. In the following code, some printing on the training information are added each 100
epochs.

Epochs = 5000 # Number o f

i t e r a t i o n s

f o r

i

i n r a n g e ( Epochs ) :

s e s s . run ( t r a i n _ s t e p ,
i % 100 == 0 :
i f
p r i n t ( ’ Epoch ’ ,
p r i n t ( ’ Cost

’ ,

i )

f e e d _ d i c t ={x_ : X, y_ : Y} ) # o p t i m i z e r

s t e p

s e s s . run ( c o s t ,

f e e d _ d i c t ={x_ : X, y_ : Y} ) )

The sess.run() calling runs the operations previously deﬁned for the ﬁrst argument, which in this
case is an optimizer step (deﬁned by train_step). The second (optional) argument for run is a
dictionary feed_dict, pairing each placeholder with the correspondent input. The function run()
is used also to evaluate the cost each 100 epochs.

Evaluation

The performance of the trained model can be easily evaluated by:

1 c o r r e c t _ p r e d i c t i o n = abs (y_ − y ) < 0 . 5

c a s t = t f . c a s t ( c o r r e c t _ p r e d i c t i o n , " f l o a t " )

3 a c c u r a c y = t f . reduce_mean ( c a s t )

5 yy , aa = s e s s . run ( [ y , a c c u r a c y ] , f e e d _ d i c t ={x_ : X, y_ : Y} )

7 p r i n t " Output : " , yy

p r i n t " Accuracy : " , aa

2

4

6

8

9

Draw separation surfaces

In order to visualize separation surfaces computed by the network, it can be useful to generate a
random sample of points on which test results, as showed in Figure 3.

21

p l t . f i g u r e ( )

2 # P l o t t i n g d a t a s e t

c1 = p l t . s c a t t e r ( [ 1 , 0 ] ,
4 c0 = p l t . s c a t t e r ( [ 1 , 0 ] ,

[ 0 , 1 ] , marker= ’ s ’ , c o l o r= ’ gray ’ ,
[ 1 , 0 ] , marker= ’ ^ ’ , c o l o r= ’ gray ’ ,

s =100)
s =100)

# G e n e r a t i n g p o i n t s

i n [ − 1 , 2 ] x [ − 1 , 2 ]

6 DATA_x = ( np . random . rand ( 1 0 ∗ ∗ 6 , 2 ) ∗3)−1

DATA_y = s e s s . run ( y , f e e d _ d i c t ={x_ : DATA_x} )

8 # S e l e c t i n g b o r d e r l i n e p r e d i c t i o n s

i n d = np . where ( np . l o g i c a l _ a n d ( 0 . 4 9 < DATA_y, DATA_y< 0 . 5 1 ) ) [ 0 ]

10 DATA_ind = DATA_x[ i n d ]

# P l o t t i n g s e p a r a t i o n s u r f a c e s

12 s s = p l t . s c a t t e r (DATA_ind [ : , 0 ] , DATA_ind [ : , 1 ] , marker= ’_ ’ , c o l o r= ’ b l a c k ’ ,

s =5)

14 p l t . l e g e n d ( ( c1 , c0 ,

( ’ C l a s s 1 ’ ,

’ C l a s s 0 ’ ,

’ S e p a r a t i o n s u r f a c e s ’ ) ,

s c a t t e r p o i n t s =1)

# Some f i g u r e ’ s

s e t t i n g s
s s ) ,

p l t . x l a b e l ( ’ Input x1 ’ )
16 p l t . y l a b e l ( ’ Input x2 ’ )
p l t . a x i s ( [ − 1 , 2 , − 1 , 2 ] )

18 p l t . show ( )

Listing 10: Draw separation surfaces

Figure 3: Separation surfaces on the XOR classiﬁcation task obtained by 2-layer ANN with 3 Hidden
Units and the Logistic Sigmoid as activation and output function.

22

4 MNIST Handwritten Characters Recognition

In this Section we show how to set up a 2-Layer ANN in order to face the MNIST [6] classiﬁcation
problem, a well known data set for handwritten characters recognition. It is extensively used to
test and compare general Machine Learning algorithms and Computer Vision methods. Data are
provided as 28×28 pixels (grayscale) images of handwritten digits. The training and test sets contain
respectively 60,000 and 10,000 instances. Files .zip are available at the oﬃcial site15, together with
a list of performance achieved by most common algorithms. We show the setting up of a standard
2-Layer ANN with 300 units in the hidden layer, represented in Figure 4, since it is one of the
architecture reported in the oﬃcial website and the obtained results can be easily compared. The
input will be reshaped so as to feed the network with a 1-Dimensional vector with 28 · 28 = 784
elements. Each image is originally represented by a matrix containing the grayscale value of the
pixels in [0, 255], which will be normalized in [0, 1]. The output will be a 10 elements prediction
vector, since labels for each element will be expressed by the one-hot encoding binary vector of 10
null bits, with only a 1 in the position indicating the class. Activation and penalty functions are
diﬀerent within diﬀerent environments to provide an overview on diﬀerent approaches.

Figure 4: General architecture of a 2-Layer network model proposed to face the MNIST data.

4.1 MNIST on Matlab

Once data have been downloaded from the oﬃcial MNIST site, we can use Matlabfunctions avail-
able at the Stanford University site16 to extract data from ﬁles and organize them in inputSize
–by–numberOfSamples matrix form. The extraction routines reshape (so as that each digit is rep-
resented by a 1-D column vector of size 784) and normalizes data (so as that each feature lies in
the interval [0, 1]) . Once unzipped data and functions in the same folder, it is straightforward to
upload images in the Matlabworkspace by the loadMNISTImages function:

15http://yann.lecun.com/exdb/mnist/.
16http://ufldl.stanford.edu/wiki/index.php/Using_the_MNIST_Dataset.

23

2

1

3

images_Train = loadMNISTImages ( ’ t r a i n −images . idx 3−uby te ’ ) ;
images_Test = loadMNISTImages ( ’ t10k−images . idx 3−uby te ’ ) ;
images = [

images_Train ,

images_Test ] ;

where training and test set have been grouped in the same matrix to evaluate performance on the
provided test set during the training. Correspondent labels can be loaded and grouped in a similar
way by the function loadMNISTLabels:

l a b e l s _ T r a i n = loadMNISTLabels ( ’ t r a i n −l a b e l s . idx 1−uby te ’ ) ;
l a b e l s _ T e s t = loadMNISTLabels ( ’ t10k−l a b e l s . idx 1−uby te ’ ) ;
l a b e l s = [

l a b e l s _ T r a i n ;

l a b e l s _ T e s t ] ;

The original labels are provided as a 1-Dimensional vector containing a number from 0 to 9 according
to the correspondent digit. The one-hot encoding target matrix for the whole dataset can be
generated exploiting the Matlabfunction ind2vec17:

1 l a b e l s = f u l l (

i n d 2 v e c (

l a b e l s ’ + 1 )

) ;

To check the obtained results, we replied one of the 2-layer architectures listed at the oﬃcial website,
which is supposed to reach around 96% of accuracy with 300 hidden units and can be initialized by:

1 nn = p a t t e r n n e t ( 300 ) ;

As already said, this command creates a 2-Layer ANN where the hidden layer has 300 units and the
Hyperbolic Tangent as activation, whereas the output function is computed by the softmax. The
(default) penalty function is the Cross-Entropy Criterion.

In this case we change the data splitting so as that data used for test comes only from the original
test set (which has been concatenated with the training one), prevent to mix samples among Train,
Validation and Test set. This step is completely customizable by the method divideFcn and the
ﬁelds of the options divideParam. The divideind method picks data according to the provided
indexes for the data matrix:

1 nn . d i v i d e F c n = ’ d i v i d e i n d ’ ;

nn . div ideParam . t r a i n I n d = 1 : 4 5 0 0 0 ;
3 nn . div ideParam . v a l I n d = 4 5 0 0 0 : 6 0 0 0 0 ;

nn . div ideParam . t e s t I n d = 6 0 0 0 0 : 7 0 0 0 0 ;

In this case, we arbitrarily decided to use the last 25% of the Training data for Validation, since
the samples are more or less equally distributed by classes.
As already said, network training can be started by:

17The function full prevent for Matlabautomatically convert to sparse matrix, which in our tests may cause some

problems at the calling of the function train.

24

[ nn ,

t r

] = t r a i n ( nn ,

images ,

l a b e l s

) ;

In the reported case, the training stopped after 107 epochs because of an increasing in the validation
error (see Section 1.3). The performance during training are shown in Figure 5(a), which is obtained
by the following code:

1 % % % % P l o t t i n g MNIST t r a i n i n g p e r f o r m a n c e

3 p l o t ( t r . p e r f , ’ LineWidth ’ , 2 ) ; % p l o t

t r a i n i n g e r r o r

h o l d on ;

5 p l o t ( t r . v p e r f , ’ r −. ’ , ’ LineWidth ’ , 2 ) ; % p l o t v a l i d a t i o n e r r o r

p l o t ( t r . t p e r f , ’ g : ’ , ’ LineWidth ’ , 2 ) ; % p l o t
7 s e t ( gca , ’ y s c a l e ’ , ’ l o g ’ ) ; % s e t t i n g l o g s c a l e

t e s t

e r r o r

a x i s ( [ 1 , 1 0 7 , 0 . 0 0 1 , 1 . 8 ] ) ;

9 x l a b e l ( ’ T r a i n i n g Epochs ’ , ’ F o n t S i z e ’ , 1 4 ) ;
y l a b e l ( ’ Cross−Entropy ’ , ’ F o n t S i z e ’ , 1 4 ) ;

11 t i t l e ( ’ Performance Trend on MNIST ’ , ’ F o n t S i z e ’ , 1 6 ) ;

h = l e g e n d ( { ’ T r a i n i n g ’ , ’ V a l i d a t i o n ’ , ’ Test ’ } , ’ L o c a t i o n ’ , ’ North East ’ ) ;

13 s e t ( h , ’ F o n t S i z e ’ , 1 4 ) ;

Performance Trend on MNIST

Training
Validation
Test

5 → 6

8 → 0

2 → 8

10

20

30

40

50

60

70

80

90

100

Training Epochs

(a)

(b)

4 → 6

7 → 3

3 → 7

Figure 5: On the left the performance on the MNIST dataset during the training of a 2-layer ANN
with 300 hidden units. Training is stopped after 107 epochs for validation checking. On the right
we report some misclassiﬁed samples. The network reaches about 96% of classiﬁcation accuracy on
the test set (in accordance with the ones provided at the MNIST web page).

In Figure 5(b) we show some misclassiﬁed digits, indicating the original label and the predicted one.
The visualization is obtained by the Matlab function image (after a reshaping to the original square
dimensions and grayscale, multiplying by 255). In Listing 11 we show how to evaluate classiﬁcation
accuracy and confusion matrix on data, which should give coherent results with respect to which
reported in the oﬃcial site for the same architecture (about 4% error on test set).

25

y
p
o
r
t
n
E
-
s
s
o
r
C

10 0

10 -1

10 -2

10 -3

1 % network e v a l u a t i o n

f = nn (
3 f v = nn (
f t = nn (

images (

: , 1 : 4 5 0 0 0 )

) ; % t r a i n i n g p r e d i c t i o n s

images (
images (

: , 4 5 0 0 1 : 6 0 0 0 0 )
: , 6 0 0 0 1 : end )

) ; % v a l i d a t i o n p r e d i c t i o n s

) ; % t e s t p r e d i c t i o n s

5 % c l a s s i f i c a t i o n a c c u r a c y

A = mean ( v e c 2 i n d ( f ) == v e c 2 i n d (
7 Av = mean ( v e c 2 i n d ( f v ) == v e c 2 i n d (
At = mean ( v e c 2 i n d ( f t ) == v e c 2 i n d (

l a b e l s (

: , 1 : 4 5 0 0 0 )

)

) ;

l a b e l s (
l a b e l s (

: , 4 5 0 0 1 : 6 0 0 0 0 )
)
: , 6 0 0 0 1 : end )

) ;

)
) ;

9 % c o n f u s i o n matrix
C = c o n f u s i o n m a t (

11 Cv = c o n f u s i o n m a t ( v e c 2 i n d ( f v ) , v e c 2 i n d (
Ct = c o n f u s i o n m a t ( v e c 2 i n d ( f t ) , v e c 2 i n d (

l a b e l s (
l a b e l s (

: , 4 5 0 0 1 : 6 0 0 0 0 )
)
: , 6 0 0 0 1 : end )

) ;

)
) ;

v e c 2 i n d ( f ) , v e c 2 i n d (

l a b e l s (

: , 1 : 4 5 0 0 0 )

)

)

;

Listing 11: Performance evaluation of the network trained on the MNIST

4.2 MNIST on Torch

As already said, the Torch environment provides a lot of tools for Machine Learning, included a
lot of routines to download and prepare most common Datasets. A wide overview on most useful
tutorials, demos and introduction to most common methods can be found in a dedicate webpage18,
including a Loading popular datasets section. Here, a link to the MNIST loader page 19 is available,
where data and all the informations for the correspondent mnist package installation are provided.
After the installation, data can be loaded by the code in Listing 12.

2 % l o a d i n g data

images_Train = loadMNISTImages ( ’ t r a i n −images −idx 3−uby te ’ ) ;

4 images_Test = loadMNISTImages ( ’ t10k−images −idx 3−uby te ’ ) ;

images = [

images_Train ,

images_Test ] ;

6

% l o a d i n g t a r g e t s

8 l a b e l s _ T r a i n = loadMNISTLabels ( ’ t r a i n −l a b e l s −idx 1−uby te ’ ) ;
l a b e l s _ T e s t = loadMNISTLabels ( ’ t10k−l a b e l s −idx 1−uby te ’ ) ;

10 l a b e l s = [

l a b e l s = f u l l (

l a b e l s _ T r a i n ;
i n d 2 v e c (

l a b e l s _ T e s t ] ;
l a b e l s ’ + 1 )

) ;

12 %%

% i n i t i a l i z a t i o n

14 nn = p a t t e r n n e t ( 300 ) ;

n n . d i v i d e F c n = ’ d i v i d e i n d ’ ;

16 n n . d i v i d e P a r a m . t r a i n I n d = 1 : 45000 ;

n n . d i v i d e P a r a m . v a l I n d = 45001 : 60000 ;
18 n n . d i v i d e P a r a m . t e s t I n d = 60001 : 70000 ;

Listing 12: Loading MNIST data

Data will be loaded as a table named train, where digits are expressed as a numberOfSamples-by-
28-by-28 tensor of type ByteTensor stored in the ﬁeld data, expressing the value of the gray levels
of each pixel between 0 and 255. Targets will be stored as a 1-D vector, expressing the digits labels,
in the ﬁeld label. We have to convert data to the DoubleTensor format and, again, normalize the
input features to have values in [0, 1] and reshape the original 3-D tensor in a 1-D input vector.

18https://github.com/torch/torch7/wiki/Cheatsheet#machine-learning
19https://github.com/andresy/mnist

26

2

4

6

4

6

8

Labels have to be incremented by 1, since CrossEntropyCriterion accepts target indicating the
class avoiding null values (i.e. 1 means a sample to belong to the ﬁrst class and so on). In the last
row we perform a random shuﬄing of data in order to prepare the train/validation splitting by the
function:

f u n c t i o n r n d S h u f f l e ( d a t a s e t )

−− random data s h u f f l e
l o c a l n = d a t a s e t . d a t a : s i z e ( 1 )
l o c a l perm = t o r c h . L o n g T e n s o r ( ) : randperm ( n )
d a t a s e t . d a t a = d a t a s e t . d a t a : i n d e x ( 1 , perm )
d a t a s e t . l a b e l = d a t a s e t . l a b e l : i n d e x ( 1 , perm )
r e t u r n d a t a s e t

8 end

At this point we can create a validation set from the last quarter of the training data by:

−− s p l i t t i n g f u n c t i o n d e f i n i t i o n

2 f u n c t i o n s p l i t D a t a ( data , p )

−− s p l i t s data depending on t h e r a t e p
l o c a l p = p > 0 and p <= 1 and p o r 0 . 9
l o c a l
r e t u r n data : narrow ( 1 , 1 , t r a i n S i z e ) , data : narrow ( 1 , 1 + t r a i n S i z e , data : s i z e ( 1 ) −

t r a i n S i z e = t o r c h . f l o o r ( p∗ data : s i z e ( 1 ) )

t r a i n S i z e )

end

v a l i d a t i o n = {}
10 t r a i n R a t e = 0 . 7 5

t r a i n . d a t a , v a l i d a t i o n . d a t a = s p l i t D a t a ( t r a i n . d a t a ,
12 t r a i n . l a b e l , v a l i d a t i o n . l a b e l = s p l i t D a t a ( t r a i n . l a b e l ,

t r a i n R a t e )

t r a i n R a t e )

The code to build the proposed 2-layer ANN model is reported in Listing 13, where the network
is assembled in the Sequential container, using this time the ReLU as activation for the hidden
layer, whereas output and penalty functions are the same used in the previous section (softmax and
Cross-Entropy respectively).

−− mlpwork d e f i n i t i o n
’ nn ’

2 r e q u i r e

4 l o c a l mlp = n n . S e q u e n t i a l ( )

mlp : add ( n n . L i n e a r ( 7 8 4 , 3 0 0 ) )

6 mlp : add ( nn.ReLU ( ) )

mlp : add ( n n . L i n e a r ( 3 0 0 , 1 0 ) )

8 mlp : add ( nn.SoftMax ( ) )

10 −− l o s s

f u n c t i o n

l o c a l c = n n . C r o s s E n t r o p y C r i t e r i o n ( )

Listing 13: Network deﬁnition for MNIST data

The network training can be deﬁned in a way similar to the one proposed in Listing 6. Because of
the width of the training data, this time is more convenient to set up a minibatch training as showed
in Listing 14. Moreover, we also deﬁne an early stopping criterion which stops the training when

27

the penalty on the validation set start to increase, preventing overﬁtting problems. The training
function expects as inputs the network (named mlp), the criterion to evaluate the loss (named
criterion), training and validation data (named trainset and validation respectively) organized as a
table with ﬁelds data and label as deﬁned in Listing 12. An optional conﬁguration table options can
be provided, indicating the number of training epochs (nepochs), the learning rate (learning_rate),
the mini-batch size (batchSize) and the number of consecutive increasings in the validation loss
which causes a preventive training stop (maxSteps). It is worth a remark on the function split,
deﬁned for the Tensor class, used to divide data in batches stored in an indexed table. At the end
of the training, a vector containing the loss evaluated at each epoch is returned. The validation loss
is computed with the help of the function evaluate, which splits again the computation in smaller
batches, preventing from too heavy computations when the number of parameters and samples is
very large.

1 f u n c t i o n t r a i n i n g ( mlp ,
−− m i n i b a t c h t r a i n i n g

c r i t e r i o n ,

t r a i n s e t , v a l i d a t i o n , o p t i o n s )

3 a s s e r t ( mlp and t r a i n s e t , "At l e a s t 2 arguments a r e e x p e c t e d " )

l o c a l nepochs = o p t i o n s and o p t i o n s . n e p o c h s o r 1000 −− max number o f epoch

5 l o c a l

l e a r n i n g _ r a t e = o p t i o n s and o p t i o n s . l e a r n i n g _ r a t e o r 0 . 0 1

l o c a l b a t c h S i z e = o p t i o n s and o p t i o n s . b a t c h S i z e o r 32

7 l o c a l maxSteps = o p t i o n s and o p t i o n s . m a x S t e p s o r 10 −− max v a l i d a t i o n f a i l s

l o c a l

input ,

t a r g e t = t r a i n s e t . d a t a ,

t r a i n s e t . l a b e l

9 −− v e c t o r

f o r

s a v i n g l o s s d u r i n g epoch

l o c a l

l o s s = t o r c h . T e n s o r ( nepochs ) : typeAs ( i n p u t ) : f i l l ( 0 )

11 l o c a l v a l L o s s = t o r c h . T e n s o r ( nepochs ) : typeAs ( i n p u t ) : f i l l ( 0 )

−− m i n i b a t c h e s

s p l i t t i n g

13 l o c a l m i n i b a t c h e s , m i n i t a r g e t = i n p u t : s p l i t ( b a t c h S i z e ) ,

t a r g e t : s p l i t ( b a t c h S i z e )

−− l a s t

l o s s and v a l i d a t i o n f a i l s

f o r

e a r l y s t o p p i n g c r i t e r i o n

l o s s [ epoch ] = l o s s [ epoch ] + c r i t e r i o n : f o r w a r d ( mlp : f o r w a r d ( batch ) , m i n i t a r g e t [ b i ] )

15 −− based on v a l i d a t i o n l o s s
l a s t L o s s , s t e p = 0 , −1

l o c a l

17 l o c a l epoch = 1

w h i l e epoch < nepochs and s t e p < maxSteps do

19 f o r bi , batch i n p a i r s ( m i n i b a t c h e s ) do

21 −− r e s e t g r a d i e n t s

t o n u l l v a l u e s

mlp : zeroGradParame ter s ( )
23 −− accumulate g r a d i e n t s
mlp : backward ( batch ,
25 −− update p a r a m e t e r s

c r i t e r i o n : backward ( mlp.output , m i n i t a r g e t [ b i ] ) )

mlp : updateParameters ( l e a r n i n g _ r a t e )

27 i f b i %100 == 0 then c o l l e c t g a r b a g e ( ) end −− c l e a n i n g n i l v a r i a b l e

end

29 −− e v a l u a t i n g v a l i d a t i o n l o s s
c u r r L o s s = e v a l u a t e ( mlp ,

l o c a l

31 v a l L o s s [ epoch ] = c u r r L o s s

i f

c u r r L o s s >= l a s t L o s s ∗ 0 . 9 9 9 9 then

c r i t e r i o n , v a l i d a t i o n )

33 s t e p = s t e p + 1

35 s t e p = 0

e l s e

end

37 l a s t L o s s = c u r r L o s s

39 epoch = epoch + 1

end

41 i f

s t e p == maxStep then

x l u a . p r o g r e s s ( epoch , nepochs ) −− p r i n t i n g e p o c h s p r o g r e s s

28

p r i n t ( ’ T r a i n i n g s t o p p e d a t Epoch :

’ . . e p o c h . .

43 ’ b e c a u s e o f V a l i d a t i o n E r r o r

i n c r e a s i n g i n t h e l a s t

’

. . m a x S t e p . . ’ e p o c h s ’ )

45 end

47 end

r e t u r n l o s s : narrow ( 1 , 1 , epoch −1)/#m i n i b a t c h e s , v a l L o s s : narrow ( 1 , 1 , epoch −1)

49 f u n c t i o n e v a l u a t e ( mlp ,
−− e v a l u a t e t h e l o s s

c r i t e r i o n , d a t a s e t , o p t i o n s )

from c r i t e r i o n between mpl p r e d i c t i o n s

51 −− by a c c u m u l a t i n g w i t h i n m i n i b a t c h e s from d a t a s e t

a s s e r t ( mlp and d a t a s e t , "At l e a s t 2 arguments a r e e x p e c t e d " )

53 l o c a l b a t c h S i z e = o p t i o n s and o p t i o n s . b a t c h S i z e o r 32

t a r g e t = d a t a s e t . d a t a , d a t a s e t . l a b e l

l o c a l
55 l o c a l

input ,
l o s s = 0

l o c a l m i n i b a t c h e s , m i n i t a r g e t = i n p u t : s p l i t ( b a t c h S i z e ) ,

t a r g e t : s p l i t ( b a t c h S i z e )

57 f o r bi , batch i n p a i r s ( m i n i b a t c h e s ) do

l o s s = l o s s + c r i t e r i o n : f o r w a r d ( mlp : f o r w a r d ( batch ) , m i n i t a r g e t [ b i ] )

mlp : zeroGradParame ter s ( )
61 r e t u r n l o s s /#m i n i b a t c h e s

59 end

end

Listing 14: Sample of a mini-batch training function

In Listing 15 we show how to compute the Confusion Matrix and the Classiﬁcation Accuracy on
data by the function confusionMtx, taking in input the network (mlp), data (dataset) and the
expected number of classes (nclasses).

f u n c t i o n c o n f u s i o n M t x ( mlp , d a t a s e t , n c l a s s e s )

2 l o c a l

input ,

t a r g e t = d a t a s e t . d a t a , d a t a s e t . l a b e l

l o c a l confMtx = t o r c h . z e r o s ( n c l a s s e s , n c l a s s e s ) : typeAs ( i n p u t )

4

l o c a l p r e d i c t i o n = mlp : f o r w a r d ( i n p u t )

6 −− g e t

t h e p o s i t i o n o f

t h e u n i t with t h e maximum v a l u e

l o c a l _, pos = t o r c h . m a x ( p r e d i c t i o n , 2 )

8 l o c a l a c c = 0

f o r

i = 1 , n c l a s s e s do

10 l o c a l p r e d i c t e d = t o r c h . e q ( pos , i ) : typeAs ( i n p u t )

f o r
12 l o c a l

j = 1 , n c l a s s e s do

t r u t h = t o r c h . e q ( t a r g e t ,
confMtx [ i ] [ j ] = t o r c h . c m u l ( p r e d i c t e d ,

j ) : typeAs ( i n p u t )

t r u t h ) : sum ( 1 )

14 i f

i == j

then a c c = a c c + confMtx [ i ] [ j ] end

end
16 end

18 end

r e t u r n confMtx , a c c / i n p u t : s i z e ( 1 )

Listing 15: Evaluating Confusion Matrix and Accuracy

At this point we can start a trial by the following code:

o p t i o n s = { nepochs = 2 5 0 , b a t c h S i z e = 6 4 ,

l e a r n i n g _ r a t e = 0 . 05 , maxStep = 50}

2 t r a i n i n g ( mlp , c ,

t r a i n , v a l i d a t i o n , o p t i o n s )

confMtx , a c c = c o n f u s i o n M t x ( mlp ,

t e s t , 1 0 )

29

In this case the training is stopped by the validation criterion after epoch 117, producing a Clas-
siﬁcation Accuracy on test of about 97%. In Figure 6(a) we report the trend of the error during
training. Since in general can be useful to visualize the confusion matrix (which in this case is al-
most diagonal), in Figure 6(b) we show the one obtained by the function imagesc from the package
gnuplot, which just give a color map of the matrix passed as input.

Figure 6: Confusion matrix on the MNIST test set.

4.3 MNIST on Tensor Flow

Even TensorFlow environment makes available many tutorials and preloaded dataset, including the
MNIST. A very fast introductive section for beginners can be found at the oﬃcial web page20.
However, we will show some functions to download and import the dataset in a very simple way.
Indeed, data can be directly loaded by:

1 from t e n s o r f l o w . ex amples . t u t o r i a l s . mnist

import

input_data

mnist = input_data . read_data_sets ( "MNIST_data/ " , one_hot=True )

We ﬁrstly deﬁne two auxiliary functions. The ﬁrst (init_weights) will be used to initialize the
parameters of the model, whereas the second (mlp_output) to compute the predictions of the model.

d e f

i n i t _ w e i g h t s ( shape ) :

2 r e t u r n t f . V a r i a b l e ( t f . random_uniform ( shape , −0.1 , 0 . 1 ) )

4 d e f mlp_output (X, W_h, W_o, b_h , b_o ) :

6 a1 = t f . matmul (X, W_h) + b_h
o1 = t f . nn . r e l u ( a1 ) #output

l a y e r 1

8

20https://www.tensorflow.org/versions/r0.12/tutorials/mnist/beginners/index.html

30

a2 = t f . matmul ( o1 , W_o) + b_o
10 o2 = t f . nn . softmax ( a2 ) #output

l a y e r 2

12 r e t u r n o2

Now, with the help of the proposed function init_weights, we deﬁne the parameters to be learned
during the computation. W 1 and b1 represent respectively the weights and the biases of the hidden
layer. Similarly, W 2 and b2 are respectively the weights and the biases of the output layer.

W1 = i n i t _ w e i g h t s ( [ x_dim , h_layer_dim ] )

2 b1 = i n i t _ w e i g h t s ( [ h_layer_dim ] )

W2 = i n i t _ w e i g h t s ( [ h_layer_dim , y_dim ] )

4 b2 = i n i t _ w e i g h t s ( [ y_dim ] )

Once we deﬁned the weights, we can symbolically compose our model by the calling of our function
mlp_output. As in the XOR case, we have to deﬁne a placeholder storing the input samples.

2 x = t f . p l a c e h o l d e r ( t f . f l o a t 3 2 ,

[ None , x_dim ] )

# Input

# P r e d i c t i o n

4 y = mlp_output ( x , W1, W2, b1 , b2 )

Then we need to deﬁne a cost function and an optimizer. However, this time we add the square of the
Euclidean Norm as regularizer, and the global cost function is composed by the Cross-Entropy plus
the regularization term scaled by a coeﬃcient of 10−4. At the beginning of the session, TensorFlow
moves the Data Flow Graph to the CPUs or GPUs and initializes all variables.

# Model S p e c i f i c a t i o n s

2 LEARNING_RATE = 0 . 5

EPOCHS = 5000

4 MINI_BATCH_SIZE = 50
# Sy mbolic v a r i a b l e

6 y_ = t f . p l a c e h o l d e r ( t f . f l o a t 3 2 ,

[ None , y_dim ] )

# Loss

f u n c t i o n and o p t i m i z e r

f o r

t h e t a r g e t

8 c r o s s _ e n t r o p y = t f . reduce_mean ( t f . nn . s o f t m a x _ c r o s s _ e n t r o p y _ w i t h _ l o g i t s ( y , y_ ) )

r e g u l a r i z a t i o n = ( t f . reduce_sum ( t f . s q u a r e (W1) ,

[ 0 , 1 ] )

10

+ t f . reduce_sum ( t f . s q u a r e (W2) ,

[ 0 , 1 ] )

)

c o s t = c r o s s _ e n t r o p y + 10 ∗ ∗−4 ∗ r e g u l a r i z a t i o n

12 t r a i n _ s t e p = t f . t r a i n . G r a d i e n t D e s c e n t O p t i m i z e r (LEARNING_RATE) . minimize ( c o s t )

# S t a r t

s e s s i o n and i n i t i a l i z a t i o n

14 s e s s = t f . S e s s i o n ( )

s e s s . run ( t f . i n i t i a l i z e _ a l l _ v a r i a b l e s ( ) )

TensorFlow provides the function next_batch for the mnist class to randomly extract batches of
a speciﬁed size. Data are split in shuﬄed partitions of the indicated size and, by means of an
implicit counter, the function slide along batches at each calling allowing a fast implementation
for mini-batch Gradient Descent method. In our case, we used a for loop to scan across batches,
executing a training step on the extracted data at each iteration. Once the whole Training set

31

has been processed, the loss on Training, Validation and Test sets is computed. These operations
are repeated in a while loop, whose steps representing the epochs of training. The loop stops
when the maximum number of epochs is reached or the network start to overﬁt Training data.
The early stopping is implemented by checking the Validation error and training is stopped when
no improvements are obtained for a ﬁxed number of consecutive epochs (val_max_steps). The
maximum number of epochs and learning rate must be set in advance.

1 # Save v a l u e s

t o be p l o t t e d

e r r o r s _ t r a i n = [ ]

3 e r r o r s _ t e s t = [ ]
e r r o r s _ v a l = [ ]

5

# E a r l y s t o p p i n g ( i n i t v a r i a b l e s )

7 p r e c _ e r r = 10 ∗ ∗ 6 # j u s t a v e r y b i g v a l u e

v al_count = 0

9 val_max_steps = 5

11 # T r a i n i n g

BATCH_SIZE = np . shape ( mnist . t r a i n . images ) [ 0 ]

13 MINI_BATCH_SIZE = 50

15 i = 1
w h i l e

i <= e p o c h s and v al_count < val_max_steps :

17

19

21

25

27

29

31

35

37

39

41

43

45

47

# Train o v e r
f o r

j

i n r a n g e (BATCH_SIZE/MINI_BATCH_SIZE ) :

t h e f u l l batch i s p e r f o r m e d with mini−b a t c h e s a l g o r i t h m

batch_xs , batch_ys = mnist . t r a i n . next_batch ( 5 0 )
s e s s . run ( t r a i n _ s t e p ,

f e e d _ d i c t ={x : batch_xs , y_ : batch_ys } )

23 # Compute e r r o r on v a l i d a t i o n s e t and c o n t r o l

f o r e a r l y −s t o p p i n g

f e e d _ d i c t ={x : mnist . v a l i d a t i o n . images , y_ : mnist . v a l i d a t i o n . l a b e l s } )

c u r r _ e r r = s e s s . run ( c r o s s _ e n t r o p y ,

i f

c u r r _ e r r >= p r e c _ e r r ∗ 0 . 9 9 9 9 :

v al_count = v al_count + 1

e l s e :

v al_count = 0

p r e c _ e r r = c u r r _ e r r

33 # Save v a l u e s

f o r p l o t

e r r o r s _ v a l . append ( c u r r _ e r r )
c _ t e s t = s e s s . run ( c r o s s _ e n t r o p y ,

e r r o r s _ t e s t . append ( c _ t e s t )
c _ t r a i n = s e s s . run ( c r o s s _ e n t r o p y ,

e r r o r s _ t r a i n . append ( c _ t r a i n )

f e e d _ d i c t ={x : mnist . t e s t . images , y_ : mnist . t e s t . l a b e l s } )

f e e d _ d i c t ={x : mnist . t r a i n . images , y_ : mnist . t r a i n . l a b e l s } )

i n f o about

t h e c u r r e n t epoch

# P r i n t
p r i n t " \n\nEPOCH: " , i , " / " , e p o c h s
p r i n t " TRAIN ERR: " , c _ t r a i n
p r i n t " VALIDATION ERR: " , c u r r _ e r r
p r i n t " TEST ERR: " , c _ t e s t
p r i n t " \n ( E a r l y s t o p p i n g c r i t e r i o n : " , val_count , " / " , val_max_steps , " ) "

32

49 # I n c r e m e n t epochs−i n d e x

i = i +1

The prediction accuracy of the trained model can be evaluated over the test set in way similar to the
one presented for the XOR problem. This time we need to exploit the argmax function to convert
the one-hot encoding in the correspondent labels.

# Sy mbolic f o r m u l a s

2 c o r r e c t _ p r e d i c t i o n = t f . e q u a l ( t f . argmax ( y , 1 ) ,

t f . argmax (y_ , 1 ) )

a c c u r a c y = t f . reduce_mean ( t f . c a s t ( c o r r e c t _ p r e d i c t i o n ,

t f . f l o a t 3 2 ) )

4 # Compute a c c u r a c y on t h e t e s t

s e t

aa = s e s s . run ( a c c u r a c y ,

f e e d _ d i c t ={x : mnist . t e s t . images , y_ : mnist . t e s t . l a b e l s } )

6 p r i n t " Accuracy : " , aa

The trend of the network performance showed in Figure 7 during training can be obtained by the
following code:

E = r a n g e ( np . shape ( e r r o r s _ t r a i n ) [ 0 ] )

2

l i n e _ t r a i n , = p l t . p l o t (E ,

e r r o r s _ t r a i n )

4 l i n e _ t e s t , = p l t . p l o t (E ,

e r r o r s _ t e s t )

l i n e _ v a l , = p l t . p l o t (E , e r r o r _ v a l )

p l t . y l a b e l ( ’ Cross−Entropy ’ )

8 p l t . x l a b e l ( ’ Epochs ’ )

p l t . show ( )

6 p l t . l e g e n d ( [ l i n e _ t r a i n ,

l i n e _ v a l ,

l i n e _ t e s t ] ,

[ ’ T r a i n i n g ’ ,

’ V a l i d a t i o n ’ ,

’ Test ’ ] )

33

Figure 7: Error trend on the MNIST dataset during the training of a 2-layer ANN with 300 hidden
units.

34

5 Convolutional Neural Networks

Figure 8: General architecture of the CNN model proposed to face the MNIST data.

In this section we introduce the Convolutional Neural Networks (CNNs [7, 6, 8]), an important and
powerful kind of learning architecture widely diﬀused especially for Computer Vision applications.
They currently represent state of the art algorithm for image classiﬁcation tasks and constitute the
main architecture used in Deep Learning. We show how to build and train such a structure within
all the proposed frameworks, exploring the most general functions and setting up few experiments
on MNIST pointing out some important features.

5.1 Matlab

TM

TM

and Statistic and Machine Learning Toolbox

Main function and classes to build and train CNNs with Matlabare contained again in Neural
. Nevertheless, the Parallel
Network Toolbox
becomes necessary too. Moreover, to train the network a CUDA R(cid:13) -enabled
Computing Toolbox
NVIDIA R(cid:13) GPU is required. Again, we do not focus too much on main theoretical properties and
general issues about CNNs but only on main implementation instruments.
The network has to be stored in an Matlabobject of kind Layer, which can be sequentially com-
posed by diﬀerent sub-layers. For each one, we do not list all the available options, which as always
can be explored by the interactive help options from the Command Window21. Most common
convolutional objects can be deﬁned by the functions:

TM

imageInputLayer creates the layer which deals with the original input image, requiring as argument
a vector expressing the size of the input image given by height by width by number of channels;

convolution2dLayer deﬁnes a layer of 2-D convolutional ﬁlters whose size is speciﬁed by the
ﬁrst argument (a real number for a square ﬁlter, a 2-D vector to specify both height and
width), whereas the second argument speciﬁes the total number of ﬁlters; main options are
’Stride’, which indicates the sliding step (default [1, 1] means 1 pixel in both directions),
and ’Padding’ (default [0, 0]), whose have to appear in Name,Value pairs;

reluLayer deﬁnes a layer computing the Rectiﬁer Activation Linear Unit (ReLU) for the ﬁlter

outputs;

21Further documentation is available at the oﬃcial site https://it.mathworks.com/help/nnet/convolutional-neural-networks.html

35

averagePooling2dLayer layer computing a spatial reduction of the input by averaging the values
of the input on each grid of given dimension (default [2, 2], ’Stride’ and ’Padding’ are
options too);

maxPooling2dLayer layer computing a spatial reduction of the input assigning the max value to

each grid of given dimensions (default [2, 2], ’Stride’ and ’Padding’ are options too);

fullyConnectedLayer requires the desired output dimension as argument and instantiates a classic

fully connected linear layer, the number of the connections is adapted to ﬁt the input size;

dropoutLayer executes a dropout units selection with the probability given as argument;

softmaxLayer computes a probability normalization based on the softmax function;

classificationLayer adds the ﬁnal classiﬁcation layer evaluating the Cross-Entropy loss function

between predictions and labels

1 CnnM = [

imageInputLay er ( [ 2 8 28 1 ] ) ,

. . . % i n p u t

l a y e r

. . . % c o n v o l u t i o n a l

l a y e r 12 f i l t e r s o f

s i z e 5 x5 and 2 p i x e l s 0−padding

3 c o n v o l u t i o n 2 d L a y e r ( 5 , 1 2 , ’ Padding ’ , 2 ) ,

. . .

r e l u L a y e r ( ) ,

. . .

5 . . . % max−p o o l i n g l a y e r on 2 x2 g r i d and 2 p i x e l s

s t e p i n both d i r e c t i o n s

maxPooling2dLayer ( 2 , ’ S t r i d e ’ , 2 ) ,

. . .

7 . . . % c o n v o l u t i o n a l

l a y e r with 16 f i l t e r s o f

s i z e 3 x3 and 1 p i x e l s 0−padding

c o n v o l u t i o n 2 d L a y e r ( 3 , 1 6 , ’ Padding ’ , 1 ) ,

. . .

9 r e l u L a y e r ( ) ,

. . .

. . . % max−p o o l i n g l a y e r on 2 x2 g r i d and 2 p i x e l s

s t e p i n both d i r e c t i o n s

11 maxPooling2dLayer ( 2 , ’ S t r i d e ’ , 2 ) ,

. . .

. . . % c l a s s i c

f u l l y c o n n e c t e d l a y e r with 256 output

13 f u l l y C o n n e c t e d L a y e r ( 2 5 6 ) ,

. . .

r e l u L a y e r ( ) ,
15 . . . % c l a s s i c

. . .

f u l l y C o n n e c t e d L a y e r ( 1 0 ) ,

. . .

17 softmax Lay er ( ) ,

. . .

c l a s s i f i c a t i o n L a y e r ( )

] ;

f u l l y c o n n e c t e d l a y e r with 10 output

Listing 16: Deﬁnition of a CNN to face the MNIST within Matlab framework.

In Listing 16 we show how to set up a basic CNN to face the 28 × 28 pixels images from MNIST
showed in Figure 8. The global structure of the network is deﬁned as a vector composed by the
diﬀerent modules. The initialized object Layer, named CnnM, can be visualized from the command
window giving:

36

Data for the MNIST can be loaded by the Stanford routines showed in Section 4.1. This time input
images are required as a 4-D tensor of size numberOfSamples–by–channels–by–height –by–width and,
hence, we have to modify the provided function loadMNISTimages or just to reshape data, as showed
in the ﬁrst line of the following code:

X = r e s h a p e ( images_Train , 2 8 , 2 8 , 1 , [ ] )

2 Y = nominal ( l a b e l s _ T r a i n , { ’ 0 ’ , ’ 1 ’ , ’ 2 ’ , ’ 3 ’ , ’ 4 ’ , ’ 5 ’ , ’ 6 ’ , ’ 7 ’ , ’ 8 ’ , ’ 9 ’ } ) ;

The second command is used to convert targets in a categorical Matlabvariable of kind nominal,
which is required to train the network exploiting the function trainNetwork.
It also require
as input an object specifying the training options which can be instantiated by the function
trainingOptions. The command:

o p t s = t r a i n i n g O p t i o n s ( ’ sgdm ’ ) ;

selects (by the ’sgdm’ string) the Stochastic Gradient Descent algorithm using momentum. Many
optional parameters are available, which can be set by additional parameter in the Name,Value
pairs notation again. The most common are:

Momentum (default 0.9)

InitialLearnRate (default 0.01)

L2Regularization (default 0.0001)

MaxEpochs (default 30)

MiniBatchSize (default 128)

After this conﬁguration, the training can be started by the command:

1 [ t r a i n e d N e t , trainOp ] = t r a i n N e t w o r k (X, Y, CnnM, o p t s )

where trainedNet will contain the trained network and trainOp the training variables. Training starts a
command line printing of some useful variables indicating the training state, which will be similar to:

At the end of the training, we can evaluate the performance on a suitable test set Xtest, together with its
correspondent target Ytest, by the function classify:

1 P r e d i c t i o n s = c l a s s i f y ( t r a i n e d N e t , X t e s t ) ;

Accuracy = mean ( P r e d i c t i o n s==Y t e s t ) ;

3 C = c o n f u s i o n m a t (P , c a t e g o r i c a l (Y) ) ;

37

Assuming Ytest to be a 1-Dimensional vector of class labels, classiﬁcation accuracy can be calculated as
before by the meaning of the boolean comparing with the computed predictions vector Predictions. An
useful built-in function to compute the confusion matrix is provided, requiring the nominal labels to be
converted into categorical as the predictions. In this setting, the ﬁnal classiﬁcation accuracy on the test
set should be close to the 99%.

When dealing with CNNs, an important new type of object introduced are the convolutional ﬁlters. We
do not want to go in deep with theoretical explanations, however sometimes it could be useful to visualize
the composition of the convolutional ﬁlters in order to get an idea of which kind of features each ﬁlter
detects. Filters are represented by the weights of each convolution, stored in each layers in the 4-D weights
tensor of size height –by–width–by–numberOfChannels–by–numberOfFilters.
In our case for example, the
ﬁlters of the ﬁrst convolutional be accessed by the notation CnnM.Layer(2).Weights. In Figure 9, we show
their conﬁguration after the training (again exploiting the function image and the colormap gray and a
normalization in [0, 255] for a suitable visualization).

Figure 9: First Convolutional Layer ﬁlters of dimension 5 × 5 after the training on the MNIST
images with Matlab.

5.2 Torch

Within the Torch environment is straightforward to deﬁne a CNN from the nn package presented in Sec-
tion 2.2.2. Indeed, the network can be stored in a container and composed by speciﬁc modules. Again, we
give a short list description of the most common ones, which can be integrated with the standard transfer
functions or with a standard linear (fully connected) layer introduced before:

SpatialConvolution deﬁnes a convolutional layers, the required arguments are the number of input chan-
nels, the number of output channels, the height and the width of the ﬁlters. The step-size and
zero-padding height and width are optional parameters (with default value 1 and 0 respectively)

SpatialMaxPooling standard max pooling layer, requiring as inputs height and width of the pooling win-

dow, whereas the step-sizes are optional parameters (default the same as the window size)

SpatialAveragePooling standard average pooling layer, same features of the previous one

SpatialDropout set a dropout layer taking as optional argument the deactivating rate (default 0.5)

Reshape is a module which is usually used to unroll the output after a convolutional/pooling process as a
1-D vector to be feed to a linear layer, takes as input the size of the desired output dimensions

The assembly of the network follows from what seen until now. To have a diﬀerent comparison with the
previous experiment, we operate an initial 2 × 2 window max-pooling on the input image, in order to provide

38

the network by images of lower resolution. The general architecture will diﬀer from the one deﬁned in
Section 5.1 only by the ﬁrst layers. The proposed network is generated by the code in Listing 17, whereas
in Figure 10 we show the global architecture.

Figure 10: General architecture of the CNN model proposed to face the MNIST data within the
Torch environment (Section 5.2).

1 r e q u i r e

’ nn ’

−− c o n t a i n e r d e f i n i t i o n
3 c n n e t = n n . S e q u e n t i a l ( )

−− network assembly

c n n e t : add ( nn.ReLU ( ) )

5 c n n e t : add ( n n . S p a t i a l C o n v o l u t i o n ( 1 , 1 2 , 5 , 5 , 1 , 1 , 2 , 2 ) )

7 c n n e t : add ( n n . S p a t i a l M a x P o o l i n g ( 2 , 2 , 2 , 2 ) )

c n n e t : add ( n n . S p a t i a l C o n v o l u t i o n ( 1 2 , 1 6 , 3 , 3 , 1 , 1 , 1 , 1 ) )

9 c n n e t : add ( nn.ReLU ( ) )

c n n e t : add ( nn.R eshape ( 7 ∗ 7 ∗ 1 6 ) )
11 c n n e t : add ( n n . L i n e a r ( 7 ∗ 7 ∗ 1 6 , 2 5 6 ) )

c n n e t : add ( nn.ReLU ( ) )

13 c n n e t : add ( n n . L i n e a r ( 2 5 6 , 1 0 ) )

c n n e t : add ( nn.SoftMax ( ) )

Listing 17: Network deﬁnition for MNIST data reduced to 14×14 pixels images by 2×2 max-pooling

If we use the training function deﬁned in Listing 14, the optimization (starting with the same options) stops
for validation check after 123 epochs, producing a Classiﬁcation Accuracy of about 90%. This just to give an
idea of the diﬀerence in the obtained performances when there is a reduction in the information expressed
by input images. In Figure 11 we show the 12 ﬁlters of size 5 × 5 extracted by the ﬁrst convolutional layer.
The weights can be obtained by the function parameters from the package nn:

myParam = c n n e t : p a r a m e t e r s ( )

which return an indexed table storing the weights of each layer. In this case, the ﬁrst element of the table
contains a tensor of dimension 12 by 25 representing the weights of the ﬁlters. The visualization can be
generated exploiting the function imagesc from the package gnuplot, after reshaping each line in the 5 × 5
format.

39

Figure 11: First Convolutional Layer ﬁlters of dimension 5 × 5 after the training with Torch on the
MNIST images halved by 2 × 2 max pooling.

5.3 Tensor Flow

In this section we will show how to build a CNN to face the MNIST data using TensorFlow. The ﬁrst step
is to import libraries, the Mnist Dataset and to deﬁne main variables. Two additional package are required:
numpy for matrices computations and matplotlib.pyplot for visualization issues.

1 import

t e n s o r f l o w a s

t f

import numpy a s np

3 import m a t p l o t l i b . p y p l o t a s p l t

5 from t e n s o r f l o w . ex amples . t u t o r i a l s . mnist

import

input_data

mnist = input_data . read_data_sets ( ’ MNIST_data ’ , one_hot=True )

x = t f . p l a c e h o l d e r ( t f . f l o a t 3 2 ,
9 y_ = t f . p l a c e h o l d e r ( t f . f l o a t 3 2 ,

shape =[None , 7 8 4 ] )
shape =[None , 1 0 ] )

We deﬁne now some tool functions to specify variable initialization.

1 d e f w e i g h t _ v a r i a b l e ( shape ) :

i n i t i a l = t f . truncated_normal ( shape ,
r e t u r n t f . V a r i a b l e ( i n i t i a l )

s t d d e v =0.1)

5 d e f b i a s _ v a r i a b l e ( shape ) :

i n i t i a l = t f . c o n s t a n t ( 0 . 1 ,
r e t u r n t f . V a r i a b l e ( i n i t i a l )

shape=shape )

The following two function deﬁne convolution and (3-by-3) max pooling. The vector strides speciﬁes how
the ﬁlter or the sliding window move along each dimension. The vector ksize speciﬁes the dimension of the
sliding window. The padding option ’SAME’ automatically adds empty (zero valued) pixels to allow the
convolution to be centered even in the boundary pixels.

1 d e f conv2d ( x , W) :

r e t u r n t f . nn . conv2d ( x , W,

s t r i d e s =[1 , 1 , 1 , 1 ] , padding= ’SAME ’ )

# max p o o l i n g o v e r 3 x3 b l o c k s

40

7

3

7

3

5 d e f max_pool_3x3 ( x ) :

r e t u r n t f . nn . max_pool ( x , k s i z e =[1 , 3 , 3 , 1 ] ,
s t r i d e s =[1 , 3 , 3 , 1 ] , padding= ’SAME ’ )

7

In order to deﬁne the model, we start by reshaping the input (where each sample is provided as 1-D vector)
to its original size, i.e. each sample is represented by a matrix of 28x28 pixels. Then we deﬁne the ﬁrst
convolution layer which computes 12 features by using 5x5 ﬁlters. Finally we perform the ReLU activation
and the ﬁrst max pooling step.

1 # Input

r e s i z e
x_image = t f . r e s h a p e ( x ,

[ − 1 , 2 8 , 2 8 , 1 ] )

# F i r s t

c o n v o l u t i o n l a y e r − 5 x5 f i l t e r s

5 INPUT_C1 = 1 # i n p u t c h a n n e l

OUTPUT_C1 = 12 # output c h a n n e l

( f e a t u r e s )

7 W_conv1 = w e i g h t _ v a r i a b l e ( [ 5 , 5 , INPUT_C1 , OUTPUT_C1 ] )

b_conv1 = b i a s _ v a r i a b l e ( [OUTPUT_C1 ] )

#c o n v o l u t i o n s t e p

11 h_conv1 = t f . nn . r e l u ( conv2d ( x_image , W_conv1) + b_conv1 )

13 #max p o o l i n g s t e p

h_pool1 = max_pool_3x3 ( h_conv1 )

The second convolution layer can be built up in an analogous way.

# Second c o n v o l u t i o n l a y e r

2 INPUT_C2 = OUTPUT_C1

OUTPUT_C2 = 16 # output c h a n n e l

( f e a t u r e s )

4 W_conv2 = w e i g h t _ v a r i a b l e ( [ 5 , 5 , INPUT_C2 , OUTPUT_C2 ] )

b_conv2 = b i a s _ v a r i a b l e ( [OUTPUT_C2 ] )

#c o n v o l u t i o n s t e p

8 h_conv2 = t f . nn . r e l u ( conv2d ( h_pool1 , W_conv2) + b_conv2 )

10 #max p o o l i n g s t e p

h_pool2 = max_pool_3x3 ( h_conv2 )

3

9

6

5

At this point the network returns 16 feature maps 4x4. These will be reshaped to 1–D vectors and given
as input to the last fully connected linear layer. The linear layer is equipped with 1024 hidden units with
ReLU activation functions.

1 # D e f i n i t i o n o f
FS = 4 # f i n a l

s i z e

t h e f u l l y c o n n e c t e d l i n e a r

l a y e r

3 W_fc1 = w e i g h t _ v a r i a b l e ( [ FS ∗ FS ∗ OUTPUT_C2, 1 0 2 4 ] )

b_fc1 = b i a s _ v a r i a b l e ( [ 1 0 2 4 ] )

# Reshape images

7 h _ p o o l 2 _ f l a t = t f . r e s h a p e ( h_pool2 ,

[ −1 , FS ∗ FS ∗ OUTPUT_C2 ] )

41

11

15

5

13

17

25

27

29

31

33

35

9 # hidden l a y e r

h_fc1 = t f . nn . r e l u ( t f . matmul ( h_pool2_flat , W_fc1 ) + b_fc1 )

# Output

l a y e r

13 W_fc2 = w e i g h t _ v a r i a b l e ( [ 1 0 2 4 , 1 0 ] )

b_fc2 = b i a s _ v a r i a b l e ( [ 1 0 ] )

# P r e d i c t i o n

17 y_conv = t f . matmul ( h_fc1 , W_fc2 ) + b_fc2

In the following piece of code we report the optimization process of the deﬁned CNN. As in the standard
ANN case, it is organized in a for loop. This time, we chose the Adam gradient-based optimization by the
function AdamOptimizer. Each epoch performs a training step over mini-batches extracted again by the
dedicated function next_batch(), introduced in Section 4.3. This time the computations are run within
InteractiveSession. The diﬀerence with the regular Session is that an InteractiveSession sets itself as the
default session during building, allowing to run variables without needing to constantly refer to the session
object. As a for instance, the method eval() will implicitly use that session to run operations.

1 c r o s s _ e n t r o p y = t f . reduce_mean ( t f . nn . s o f t m a x _ c r o s s _ e n t r o p y _ w i t h _ l o g i t s ( y_conv , y_) )

t r a i n _ s t e p = t f . t r a i n . AdamOptimizer ( 1 e −4). minimize ( c r o s s _ e n t r o p y )

3 c o r r e c t _ p r e d i c t i o n = t f . e q u a l ( t f . argmax ( y_conv , 1 ) ,

t f . argmax (y_ , 1 ) )

a c c u r a c y = t f . reduce_mean ( t f . c a s t ( c o r r e c t _ p r e d i c t i o n ,

t f . f l o a t 3 2 ) )

s e s s = t f . I n t e r a c t i v e S e s s i o n ( )

7 s e s s . run ( t f . g l o b a l _ v a r i a b l e s _ i n i t i a l i z e r ( ) )

9 # E a r l y s t o p p i n g setup ,

t o check on v a l i d a t i o n s e t

p r e c _ e r r = 10 ∗ ∗ 6 # j u s t a v e r y b i g v a l u e

11 v al_count = 0

val_max_steps = 6

e p o c h s = 100

15 b a t c h _ s i z e = 1000

num_of_batches = 60000/ b a t c h _ s i z e

i =1
19 w h i l e

i <= e p o c h s and v al_count < val_max_steps :

21

p r i n t

’ Epoch : ’ , i , ’ ( E a r l y s t o p p i n g c r i t e r i o n : ’ , val_count , ’ / ’ , val_max_steps , ’ ) ’

23 # t r a i n i n g s t e p
j

f o r

i n r a n g e ( num_of_batches ) :

batch = mnist . t r a i n . next_batch ( b a t c h _ s i z e )
s e s s . run ( t r a i n _ s t e p ,

f e e d _ d i c t ={x : batch [ 0 ] , y_ : batch [ 1 ] } )

# v i s u a l i z e a c c u r a c y each 10 e p o c h s
i %10 == 0 :
i f
t r a i n _ a c c u r a c y = a c c u r a c y . e v a l (

i == 1 o r

f e e d _ d i c t ={x : mnist . t r a i n . images , y_ : mnist . t r a i n . l a b e l s } )

t e s t _ a c c u r a c y = a c c u r a c y . e v a l (

f e e d _ d i c t ={x : mnist . t e s t . images , y_ : mnist . t e s t . l a b e l s } )

p r i n t " \ nAccuracy a t epoch " ,
p r i n t ( " t r a i n a c c u r a c y %g ,

i , " : "

t e s t a c c u r a c y %g \n"%( t r a i n _ a c c u r a c y ,

t e s t _ a c c u r a c y ) )

42

f e e d _ d i c t ={x : mnist . v a l i d a t i o n . images , y_ : mnist . v a l i d a t i o n . l a b e l s } )

37 # v a l i d a t i o n check

c u r r _ e r r = s e s s . run ( c r o s s _ e n t r o p y ,

i f

c u r r _ e r r >= p r e c _ e r r ∗ 0 . 9 9 9 9 :

v al_count = v al_count + 1

39

41

43

45

47

v al_count = 0

p r e c _ e r r = c u r r _ e r r

e l s e :

i+=1

49 p r i n t ( " \n\ n R e s u l t : \ nTest a c c u r a c y %g " % a c c u r a c y . e v a l (

f e e d _ d i c t ={x : mnist . t e s t . images , y_ : mnist . t e s t . l a b e l s } ) )

In this setting, the ﬁnal classiﬁcation accuracy on the test set should be close to the 99%. As in the previous
Sections, in Figure 12 we show the learned ﬁlters of the ﬁrst convolutional layer obtained by:

# G e t t i n g f i l t e r s a s an a r r a y

2 FILTERS = W_conv1 . e v a l ( )

4 f i g = p l t . f i g u r e ( )

f o r

i

i n r a n g e ( np . shape (FILTERS ) [ 3 ] ) :

6

ax = f i g . add_subplot ( 2 , 6 ,
ax . matshow (FILTERS [ : , : , 0 , i ] , cmap= ’ gray ’ )

i +1)

8 p l t . show ( )

In the ﬁrst line eval() computes the current value of W_conv1, saving it in the 4–D numpy array
FILTERS, where the fourth dimension (acceded by the index 3 in np.shape(FILTERS)[3]) corresponds to
the number of ﬁlters.

Figure 12: First Convolutional Layer ﬁlters of dimension 5 × 5 after the training with TensorFlow
on the MNIST images with the described arcitecture.

43

6 A critical comparison

In this Section we would like to outline an overall picture across the presented environments. Even if in
Table 1 we provide a scoring based on some features we thought mainly relevant for Machine Learning
software development, this work would not like to bound this analysis to a poor evaluation. Instead, we
hope to propose an useful guideline to help people trying to approach ANNs and Machine Learning in general,
in order to orientate within the environments depending on personal background and requirements. More
complete and statistically relevant comparisons can be found on the web22, but we try to summarize so as
to help and speed up single and global task developing.

We ﬁrst give general description of each environment, then we try to compare pros and cons on speciﬁc
requirements. At the end, we carry out an indicative numerical analysis on the computational performances
on diﬀerent tasks, which could be also a topic for comparison and discussion.

6.1 Matlab

The programming language is intuitive and the software provides a complete package, allowing the user to
deﬁne and train almost all kind of ANNs architecture without writing a single line of speciﬁc code. The code
parallelization is automatic and the integration with CUDA R(cid:13) is straightforward too. The available built-
in functions are very customizable and optimized, providing fast and extended setting up of experiments
and an easy access to the variable of the network for in-depth analysis. However, enlarging and integrating
Matlabtools require an advanced knowledge of the environment. This could drive the user to start rewriting
its own code from the beginning, leading to a general decay of computational performances. These features
make it perfect as a statistical and analysis toolbox, but maybe a bit slow as developmental environment. The
GUI results sometimes heavy to be handled by the calculator, but, on the other hand, it is very user-friendly
and provides the best graphical data visualization. The documentation is complete and well organized within
the oﬃcial site.

6.2 Torch

The programming language (Lua) can sometimes results a little bit tricky, but it supposed to be the faster
among these languages. It provides all the needed CUDA R(cid:13) integrations and the CPU parallelization au-
tomatic. The module-based structure allows ﬂexibility in the ANNs architecture and it is relatively easy
to extend the provided packages. There are also other powerful packages23, but in general they require to
acquire some expertise to achieve a conscious handling. Torch could be easily used as a prototyping envi-
ronment for speciﬁc and generic algorithms testing. The documentation is spread all over the torch GitHub
repository and sometimes solve speciﬁc issues could not be immediate.

6.3 Tensor Flow

The employment of a programming language as dynamic as Python makes the code scripting light for the
user. The CPU parallelization is automatic, and, exploiting the graph-structure of the computation is easy
to take advantage of GPU computing. It provides a good data visualization and the possibility for beginners
to access to ready to go packages, even if not treated in this document. The power of symbolic computation
involves the user only in the forward step, whereas the backward step is entirely derived by the TensorFlow
environment. This ﬂexibility allows a very fast development for users from any level of expertise.

22Look for example at the webpage http://hammerprinciple.com/therighttool
23We skip the treatment of optim, which provides various Gradient Descent and Back-Propagation procedure

44

6.4 An overall picture on the comparison

As already said, in Table 1 we try to sum up a global comparison trying assigning a score from 1 to 5 on
diﬀerent perspectives. Here below, we explain the main motivation when necessary:

Programming Language All the basic language are quite intuitive.
GPU Integration Matlabis penalized since an extra toolbox is required.

CPU Parallelization All the environments exploit as more core as possible
Function Customizability Matlabscore is lower since integrate well-optimized functions with the pro-

vided ones is diﬃcult

Symbolic Calculus Not expected in Lua

Network Structure Customizability Every kind of network is possible
Data Visualization The interactive Matlabmode outperforms the others
Installation Quite simple for all of them, but the Matlabinteractive GUI is an extra point

OS Compatibility Torch installation is not easy on Windows
Built-In Function Availability Matlabprovided simple-tools with an easy access
Language Performance Matlabinterface can sometimes appear heavy
Development Flexibility Again, Matlabis penalized because it forces medium users to become very
specialized with the language to integrate the provided tools or to write proper code, which in general
can slow down the software development

Programming Language
GPU Integration
CPU Parallelization
Function Customizability
Symbolic Calculus
Network Structure Customizability
Data Visualization
Installation
OS Compatibility
Built-In Function Availability
Language Performance
Development Flexibility
License

Matlab Torch
4
3
5
2
3
5
5
5
5
5
3
2
EULA

3
5
5
4
1
5
2
4
4
4
5
4
BSD (Open source) Apache 2.0 (Open source)

TensorFlow
4
5
5
5
5
5
3
4
5
4
4
5

Table 1: Environments individual scoring

45

6.5 Computational issues

In Table 2 we compare running times for diﬀerent tasks, analyzing the advantages and diﬀerences of
CPU/GPU computing. Results are averaged on 5 trials, carried out in the same machine with an In-
tel(R) Xeon(R) CPU E5-2650 v2 @ 2.60GHz with 32 cores, 66 GB of RAM, and a Geforce GTX 960 with
4GB of memory. The OS is Debian GNU/Linux 8 (jessie). We test a standard Gradient Descent proce-
dure varying the network architecture, the batches size (among Stochastic Gradient Descent (SGD), 1000
samples batch and Full Batch) and the hardware (indicated in HW column). The CNN architecture is the
same of the one proposed in Figure 8. Performances are obtained trying to use optimization procedures as
similar as possible. In practice, it is very diﬃcult to reply the speciﬁc optimization techniques exploited in
Matlabbuilt-in toolboxes. We skip the SGD case for the second architecture (eighth row) in Torch because
of the huge computational time obtained for the ﬁrst architecture. We miss the SGD case for the ANNs
architecture in the Matlabcase since the training function ’trains’ it is not supported for GPU computing
(rows fourth and tenth). As a matter of fact, this could be an uncommon case of study, but we report the
results for best completeness. We skip the CNN Full Batch trials on GPU because of the too large memory
requirement24 .

an

24For
packages)
(including
https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software

comparison
with

computational

on
other

comparison

exhaustive

existent

the

performance

on

several

the

user

can

refer

tasks
to

46

Architecture

Batches size

SGD

1000

Full Batch

Env.

HW

2-Layers ANN

914.56

1000 HUs

4-Layers ANN

893.27

300-300-300 HUs

52.46

3481.00

378.19

911.32

44.66

–

–

–

517.87

1024.29

30.40

46.55

13.82

3.75

1.93

5.40

28.33

52.99

10.95

2.74

1.51

4.73

CNN

7794.33

54.21

647.75

100.06

1850.30

20.22

28.38

24.43

12.00

3.73

1.46

4.92

27.07

20.00

8.83

3.44

1.08

4.27

–

–

–

Matlab

Torch

TF

CPU

GPU

CPU

GPU

GPU

Table 2: Averaged time (in seconds) on 5 running of 10 epochs training with diﬀerent architectures
on the MNIST data within the presented environments. All the architectures are built up using the
ReLU as activation, the softmax as output function and the Cross-Entropy penalty.

47

References

[1] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.

[2] David E Rumelhart, Geoﬀrey E Hinton, and Ronald J Williams. Learning representations by back-

propagating errors. Cognitive modeling, 5(3):1, 1988.

[3] F. Giannini, V. Laveglia, A. Rossi, D. Zanca, and A. Zugarini. NeuralNetworksForBeginners.

https://github.com/AILabUSiena/NeuralNetworksForBeginners, 2016.

[4] The Mathworks, Inc., Natick, Massachusetts. MATLAB version 8.5.0.197613 (R2015a), 2015.

[5] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Cor-
rado, Andy Davis, Jeﬀrey Dean, Matthieu Devin, et al. Tensorﬂow: Large-scale machine learning on
heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.

[6] Y. Lecun, L. Bottou, Y. Bengio, and P. Haﬀner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE, 86(11):2278–2324, Nov 1998.

[7] Kunihiko Fukushima. Neocognitron: A hierarchical neural network capable of visual pattern recognition.

Neural networks, 1(2):119–130, 1988.

[8] Thomas Serre, Lior Wolf, Stanley Bileschi, Maximilian Riesenhuber, and Tomaso Poggio. Robust object
recognition with cortex-like mechanisms. IEEE transactions on pattern analysis and machine intelligence,
29(3), 2007.

48

7
1
0
2
 
r
a

M
 
6
1
 
 
]

G
L
.
s
c
[
 
 
2
v
8
9
2
5
0
.
3
0
7
1
:
v
i
X
r
a

Neural Networks for Beginners
A fast implementation in Matlab, Torch, TensorFlow

F. Giannini1, V. Laveglia1,2, A. Rossi1,3∗, D. Zanca1,2, A. Zugarini1

1DIISM, University of Siena, Siena, Italy
2DINFO, University of Florence, Florence, Italy
3Fondazione Bruno Kessler, Trento, Italy

rossi111@unisi.it
{giannini7, andrea.zugarini}@student.unisi.it
{vincenzo.laveglia, dario.zanca}@unifi.it

March 17, 2017

What is this report about?

This report provides an introduction to some Machine Learning tools within the most common
development environments. It mainly focuses on practical problems, skipping any theoretical intro-
duction. It is oriented to both students trying to approach Machine Learning and experts looking
for new frameworks.

The dissertation is about Artiﬁcial Neural Networks (ANNs [1, 2]), since currently is the most
trend topic, achieving state of the art performance in many Artiﬁcial Intelligence tasks. After a ﬁrst
individual introduction to each framework, the setting up of general practical problems is carried
out simultaneously, in order to make the comparison easier.

Since the treated argument is widely studied and in continuos and fast growing, we pair this
document with an on-line documentation available at the Lab GitHub repository [3] which is more
dynamic and we hope to be kept updated and possibly enlarged.

∗Corresponding Author

1

Contents

1 Matlab: a uniﬁed friendly environment

1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Setting up the XOR experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Stopping criterions and Regularization . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4 Drawing separation surfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Torch and Lua environment

2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Getting started . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.1 Lua . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.2 Torch enviroment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Setting up the XOR experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Stopping criterions and Regularization . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5 Drawing Separation Surfaces

3 TensorFlow

3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Getting started . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.1 Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.2 TensorFlow environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Installation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.3
3.3 Setting up the XOR experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 MNIST Handwritten Characters Recognition

4.1 MNIST on Matlab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 MNIST on Torch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3 MNIST on Tensor Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Convolutional Neural Networks

5.1 Matlab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Torch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3 Tensor Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 A critical comparison

6.1 Matlab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.2 Torch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.3 Tensor Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.4 An overall picture on the comparison . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.5 Computational issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3
3
4
7
7

10
10
10
10
10
12
14
15

18
18
18
18
18
19
19

23
23
26
30

35
35
38
40

44
44
44
44
45
46

2

1 Matlab: a uniﬁed friendly environment

1.1 Introduction

Matlab R(cid:13) [4] is a very powerful instrument allowing an easy and fast handling of almost every kind
of numerical operation, algorithm, programming and testing. The intuitive and friendly interactive
interface makes it easy to manipulate, visualize and analyze data. The software provides a lot
of mathematical built-in functions for every kind of task and an extensive and easily accessible
documentation. It is mainly designed to handle matrices and, hence, almost all the functions and
operations are vectorized, i.e. they can manage scalars, as well as vectors, matrices and (often)
tensors. For these reasons, it is more eﬃcient to avoid loops cycles (when possible) and to set up
operations exploiting matrices multiplication.

TM

and the Neural Network Toolbox

In this document we just show some simple Machine Learning related instruments in order to
start playing with ANNs. We assume a basic-level knowledge and address to oﬃcial documentation
for further informations. For instance, you can ﬁnd informations on how to obtain the software
from the oﬃcial web site1. Indeed, the license is not for free and even if most universities provide a
classroom license for students use, maybe could not be possible to access to all the current packages.
In particular the Statistic and Machine Learning Toolbox
provide a lot of built-in functions and models to implement diﬀerent ANNs architectures suitable
to face every kind of task. The access to both the tools is fundamental in the prosecution, even if
we refer to some simple independent examples. The most easy to-go is the nnstart function, which
activates a simple GUI guiding the user trough the deﬁnition of a simple 2-layer architecture. It
allows either to load available data samples or to work with customize data (i.e. two matrices of input
data and correspondent target), train the network and analyze the results (Error trend, Confusion
Matrix, ROC, etc.). However, more functions are available for speciﬁc tasks. For instance, the
function patternnet is speciﬁcally designed for pattern recognition problems, newfit is suitable for
regression, whereas feedforwardnet is the most ﬂexible one and allows to build very customized and
complicated networks. All the versions are implemented in a similar way and the main options and
methods apply to everyone. In the next section we show how to manage customizable architectures
starting to face very basic problems. Detailed informations can be ﬁnd in a dedicated section of the
oﬃcial site2.

TM

CUDA R(cid:13) computing

TM

GPU computing in Matlabrequires the Parallel Computing Toolbox
and the CUDA R(cid:13) instal-
lation on the machine. Detailed informations on how to use, check and set GPUs devices can be
found in GPU computing oﬃcial web page3, where issues on Distributed Computing CPUs/GPUs
are introduced too. However, basic operations with graphical cards should in general be quite sim-
ple. Data can be moved to the GPU hardware by the function gpuArray, then back to the CPU
by the function gather. When dealing with ANNs, a dedicated function nndata2gpu is provided,
organizing tensors (representing a dataset) in a eﬃcient conﬁguration on the GPU, in order to speed
up the computation. An alternative way is to carry out just the training process in the GPU by
the correspondent option of the function train (which will be describe in details later). This can

1https://ch.mathworks.com/products/matlab.html?s_tid=hp_products_matlab
2http://ch.mathworks.com/help/nnet/getting-started-with-neural-network-toolbox.html
3https://ch.mathworks.com/help/nnet/ug/neural-networks-with-parallel-and-gpu-computing.html

3

be done directly by passing additional arguments, in the Name,Values pair notation, the option
’useGPU’ and the value ’yes’:

1

nn = t r a i n ( nn ,

. . .

,

’ useGPU ’ , ’ y e s ’ )

1.2 Setting up the XOR experiment

The XOR is a well-known classiﬁcation problem, very simple and eﬀective in order to understand
the basic properties of many Machine Learning algorithms. Even if writing down an eﬃcient and
ﬂexible architecture requires some language expertise, a very elementary implementation can be
found in the Matlabsection of the GitHub repository4 of this document. It is not suitable to face
real tasks, since no customizations (except for the number of hidden units) are allowed, but can
be useful just to give some general tips to design a personal module. The code we present is basic
and can be easily improved, but we try to keep it simple just to understand fundamental steps. As
we stressed above, we avoid loops exploiting the Matlabeﬃciency with matrix operations, both in
forward and backward steps. This is a key point and it can substantially aﬀects the running time
for large data.

Initialization

TM

Here below, we will see how to deﬁne and train more eﬃcient architectures exploiting some built-in
functions from the Neural Network Toolbox
. Since we face the XOR classiﬁcation problem, we
sort out our experiments by using the function patternnet. To start, we have to declare an object
of kind network by the selected function, which contains variables and methods to carry out the
optimization process. The function expects two optional arguments, representing the number of
hidden units (and then of the hidden layers) and the back-propagation algorithm to be exploited
during the training phase. The number of hidden units has to be provided as a single integer number,
expressing the size of the hidden layer, or as an integer row vector, whose elements indicate the size
of the correspondent hidden layers. The command:

1

nn = p a t t e r n n e t ( 3 )

creates on object named nn of kind network, representing a 2-layer ANN with 3 units in the
single hidden layer. The object has several options, which can be reached by the dot notation
object.property or explore by clicking on the interactively visualization of the object in the Mat-
labCommand Window, which allows to see all the available options for each property too. The
second optional parameter selects the training algorithm by a string saved in the trainFcn property,
which in the default case takes the value ’trainscg’ (Scaled Conjugate Gradient Descent methods).
The network object is still not fully deﬁned, since some variables will be adapted to ﬁt the data
dimension at the calling of the function train. However, the function configure, taking as input
the object and the data of the problem to be faced, allows to complete the network and set up the
options before the optimization starts.

4Available at https://github.com/AILabUSiena/NeuralNetworksForBeginners/tree/master/matlab/2layer.

4

Dataset

Data for ANNs training (as well as for others available Machine Learning methods) must be provided
in matrix form, storing each sample column-wise. For example data to deﬁne the XOR problem can
be simply deﬁned via an input matrix X and a target matrix Y as:

1

X = [ 0 , 0 , 1 , 1 ; 0 , 1 , 0 , 1 ]
Y = [ 0 , 1 , 1 , 0 ]

Matlabexpects targets to be provided in 0/1 form (other values will be rounded). For 2-class
problem targets can be provided as a row vector of the same length of the number of samples. For
multi-class problem (and as an alternative for 2-class problem too) targets can be provided in the
one-hot encoding form, i.e. as a matrix with as many columns as the number of samples, each one
composed by all 0 with only a 1 in the position indicating the class.

Conﬁguration

nn = c o n f i g u r e ( nn , X,Y)

Once we have deﬁned data, the network can be fully deﬁned and designed by the command:

For each layer, an object of kind nnetLayer is created and stored in a cell array under the ﬁeld
layers of the network object. The number of connections (the weights of the network) for each
units corresponds to the layer input dimension. The options of each layer can be reached by the
dot notation object. layer{numberOf Layer}.property. The ﬁeld initFcn contains the weights
initialization methods. The activation function is stored in the transferFcn property.
In the
hidden layers the default values is the ’tansig’ (Hyperbolic Tangent Sigmoid), whereas the output
layers has the ’logsig’ (Logistic Sigmoid) or the ’sof tmax’ for 1-dimensional and multi-dimensional
target respectively. The ’crossentropy’ penalty function is set by default in the ﬁeld performFcn.
At this point, the global architecture of the network can be visualized by the command:

1

v iew ( nn )

Training

The function train itself makes available many options (as for instance useParallel and useGPU
for heavy computations) directly accessible from its interactive help window. However, it can take
as input just the network object, the input and the target matrices. The optimization starts by
dividing data in Training, Validation and Test sets. The splitting ratio can be changed by the options
divideParam. In the default setting, data are randomly divided, but if you want for example to
decide which data are used for test, you can change the way the data are distributed by the option
divideFcn5. In this case, because of the small size of the dataset, we drop validation and test by
setting:

5Click on divideFcn property from the MatlabCommand Window visualization of your object to see the available

methods.

5

1

nn . d i v i d e F c n = ’ ’

In the following code, we set the training function to the classic gradient descent method ’traingd’,
we deactivate the training interactive GUI by nn.trainParam.showWindow (boolean) and activate
the printing of the training state in the Command Window by nn.trainParam.showCommandLine
(boolean). Also the learning rate is part of the trainParam options under the ﬁelds lr.

1

3

nn . t r a i n F c n = ’ t r a i n g d ’
nn . trainParam . showWindow = 0
nn . trainParam . showCommandLine = 1
nn . trainParam . l r = 0 . 0 1

Training starts by the calling:

[ nn ,

t r ] = t r a i n ( nn , X, Y)

this generates a printing, ending in this case with:

This indicates that the training stops after the max number of epoch is reached (which can be set by
options object. trainParam.epochs). Each column shows the state of one of the stopping criterions
used, which we will analyze in details in the next section. The output variable tr stores the training
options. The ﬁelds perf, vperf and tperf contain the performance of the network evaluated at
each epoch on the Training, Validation and Test sets respectively (the last two are NaN in this case),
which can be used for example to plot performances. If we pass data organized in a single matrix,
the function will exploit the full batch learning method accumulating gradients overall the training
set. To set a mini-batch mode, data have to be manually split in sub-matrix with the same number
of column and organized in a cell array. However, let us consider for a moment a general data set
composed by N samples in the features space RD with a target of dimension C, so that X ∈ RD×N
and Y ∈ RC×N . All the mini-batches have to be of the same size b, so that it is in general convenient
to choose the batch size to be a factor of N . In this case, we can generate data for the training
function organizing the input and target in the correspondent cell-array by:

1 N = s i z e (X, 2 ) ; % number o f

s a m p l e s

n_batch = N/ b a t c h s i z e ; % number o f b a t c h e s

3

i n p u t { n_batch } = [ ] ; % i n p u t c e l l −a r r a y i n i t i a l i z a t i o n
5 t a r g e t { n_batch} = [ ] ; % t a r g e t c e l l −a r r a y i n i t i a l i z a t i o n

7 p = randperm (N) ; % g e n e r a t i n g a random permutated i n d e x f o r data s h u f f l i n g

X = X( : , p ) ; % s a m p l e s p e r m u t a t i o n

9 Y = Y( : , p ) ; % t a r g e t permutaion

6

11 f o r

i =1: n_batch

i n p u t { i } = X ( : , ( 1 : b a t c h s i z e ) +( i −1)∗ b a t c h s i z e ) ;
t a r g e t { i } = Y ( : , ( 1 : b a t c h s i z e ) +( i −1)∗ b a t c h s i z e ) ;

13

end

However, in order to perform a pure Stochastic Gradient Descent optimization, in which the ANNs
parameters are updated for each sample, the training function ’trains’ have to be employed
skipping to split data previously. A remark has to be done since this particular function does not
support the GPU computing.

The network (trained or not) can be easily evaluated on data by passing the input data as
argument to a function named as the network object. Performance of the prediction with respect to
the targets can be evaluated by the function perform according to the correspondent loss function
option object. performFcn:

f = nn (X)
perform ( nn , Y,

f )

2

1.3 Stopping criterions and Regularization

TM

Early stopping is a well known procedure in Machine Learning to avoid overﬁtting and improve
generalization. Routines from Neural Network Toolbox
use diﬀerent kind of stopping criterions
regulated by the network object options in the ﬁeld trainParam. Arbitrarily methods are based on
the number of epochs (epochs) and the training time (time, default Inf ). A criterion based on
the training set check the loss (object. trainParam.goal, default = 0) or the parameters gradients
(object. trainParam.min_grad, default = 10−6) to reach a minimum threshold. A general early
stopping method is implemented by checking the error on the validation set and interrupting training
when validation error does not improve for a number of consecutive epochs given by max_fail
(default = 6).

Further regularization methods can be conﬁgured by the property performParam of the network
object. The ﬁeld regularization contains the weight (real number in [0, 1]) balancing the contri-
bution of a term trying to minimizing the norm of the network weights versus the satisfaction of the
penalty function. However, the network is designed to mainly rely on the validation checks, indeed
regularization applies only to few kind of penalties and the default weight is 0.

1.4 Drawing separation surfaces

When dealing with low dimensional data (as in the XOR case), can be useful to visualize the
prediction of the network directly in the input space. For this kind of task, Matlabmakes available
a lot of built-in functions with many options for interactive data visualization. In this section, we
will show the main functions useful to realize customized separation surfaces learned by an ANN
with respect to some speciﬁc experiments. We brieﬂy add some comments for each instruction,
referring to the suite help for speciﬁc knowledge of each single function. The network predictions
will be evaluated on a grid of the input space, generated by the Matlabfunction meshgrid, since
the main functions used for the plotting (contour or, if you want a color surface pcolor) require

7

as input three matrices of the same dimensions expressing, in each correspondent element, the
coordinates of a 3-D point (which in our case will be ﬁrst input dimension, second input dimension
and prediction). Once we trained the network described until now, the boundary for the 2-classes
separation showed in Figure 1a is generated by the code in Listing 1, whereas in Figure 1b we
report the same evaluation after the training of a 4-layers network using 5, 3 and 2 units in the ﬁrst,
second and third hidden layers respectively, each one using the ReLU as activation (’poslin’ in
Matlab). This new network can be deﬁned by:

n = p a t t e r n n e t ( [ 5 , 3 , 2 ] ) ; % 3+output

l a y e r s network i n i t i a l i z a t i o n

2 nn = c o n f i g u r e ( n , X,Y) ; % network c o n f i g u r a t i o n

nn . t r a i n F c n = ’ t r a i n g d ’ ; % s e t t i n g o p t i m i z a t i o n f u n c t i o n

4 nn . div ideParam . t r a i n R a t i o = 1 ; % s e t t i n g data s p l i t t i n g r a t i o s

( i l l u s t r a t i v e )

nn . div ideParam . v a l R a t i o = 0 ;
6 nn . div ideParam . t e s t R a t i o = 0 ;

nn . trainParam . showCommandLine = 1 ;

8 nn . trainParam . l r = 0 . 0 1 ;

10 nn . l a y e r s { 2 } . t r a n s f e r F c n = ’ p o s l i n ’ ;
nn . l a y e r s { 3 } . t r a n s f e r F c n = ’ p o s l i n ’ ;

nn . l a y e r s { 1 } . t r a n s f e r F c n = ’ p o s l i n ’ ; % s e t t i n g t h e a c t i v a t i o n l a y e r −w i s e

Listing 1: Drawing separation surfaces

Separation Surfaces

Separation Surfaces

Classes Bound
Class 0
Class 1

Classes Bound
Class 0
Class 1

1.5

2

1

0

2

x

0.5

X2

X1

1.5

2

1

0

2

x

0.5

X2

X1

X4

X3

X4

X3

-0.5

-0.5

0

0.5

1

1.5

2

-0.5

-0.5

0

0.5

1

1.5

2

x

1

(a) 2-layers, 3 Hidden Units

x

1

(b) 4-layers, 5, 3 and 2 Hidden Units, ReLU for
all the activations

Figure 1: Separation surfaces on the XOR classiﬁcation task.

5 % network e v a l u a t i o n on t h e g r i d d i n g ( r e s h a p e d t o f i t network i n p u t d i m e n s i o n )

1 % % % % P l o t t i n g S e p a r a t i o n S u r f a c e

3 % g e n e r a t i n g i n p u t s p a c e g r i d

[ xp1 , xp2 ] = meshgrid ( − 0 . 5 : . 0 1 : 2 , − 0 . 5 : . 0 1 : 2 ) ;

f = nn ( [ xp1 ( : ) ’ ; xp2 ( : ) ’ ] ) ;

7 % r e s h a p i n g p r e d i c t i o n i n c o r r e s p o n d e n t matrix form

f = r e s h a p e ( f , s i z e ( xp1 , 1 ) , [ ] ) ;

8

% drawing s e p a r a t i o n s u r f a c e s

11 c o n t o u r ( xp1 , xp2 , f , [ . 5 , . 5 ] , ’ LineWidth ’ , 3 , ’ C o l o r ’ , ’ c ’ ) ;

h o l d on ;

% drawing data p o i n t s

15 s c a t t e r (X( 1 , [ 1 , 4 ] ) ,X( 2 , [ 1 , 4 ] ) , 2 0 0 , ’ o ’ , ’ f i l l e d ’ , ’ MarkerEdgeColor ’ , ’ k ’ , . . .

17 s c a t t e r (X( 1 , [ 2 , 3 ] ) ,X( 2 , [ 2 , 3 ] ) , 2 0 0 , ’ ^ ’ , ’ f i l l e d ’ , ’ MarkerEdgeColor ’ , ’ k ’ , . . .

’ Mark erFaceColor ’ , ’ k ’ , ’ LineWidth ’ , 2 ) ;

’ Mark erFaceColor ’ , ’ k ’ , ’ LineWidth ’ , 2 ) ;

a x i s ( [ − 0 . 5 , 2 , − 0 . 5 , 2 ] ) ; % s e t t i n g a x i s bounds

% l a b e l i n g data p o i n t s

23 c = { ’X^1 ’ , ’X^2 ’ , ’X^3 ’ , ’X^4 ’ } ; % l a b e l s

dx = [ − . 1 5 , −.15 ,

. 1 ,

. 1 ] ; % l a b e l s h o r i z o n t a l

t r a n s l a t i o n wrt p o i n t s

25 dy = [ − . 1 ,

. 1 , −.1 ,
t e x t (X( 1 , : )+dx , X( 2 , : )+dy , c ,

. 1 ] ; % l a b e l s v e r t i c a l

t r a n s l a t i o n wrt p o i n t s
’ F o n t S i z e ’ , 1 4 ) ; % showing l a b e l s a s

t e x t

9

13

19

21

27

% p l o t

l a b e l s

29 x l a b e l ( ’ x_1 ’ , ’ F o n t S i z e ’ , 1 4 )
y l a b e l ( ’ x_2 ’ , ’ F o n t S i z e ’ , 1 4 )

31 t i t l e ( ’ S e p a r a t i o n S u r f a c e s ’ , ’ F o n t S i z e ’ , 1 6 ) ;

33 h = l e g e n d ( { ’ C l a s s e s Bound ’ , ’ C l a s s 0 ’ , ’ C l a s s 1 ’ } , ’ L o c a t i o n ’ , ’ NorthEast ’ ) ;

35 s e t ( h , ’ F o n t S i z e ’ , 1 4 ) ;

9

2 Torch and Lua environment

2.1 Introduction

Torch7 is an easy to use and eﬃcient scientiﬁc computing framework, essentially oriented to Machine
Learning algorithms. The package is written in C which guarantees an high eﬃciency. However, a
completely interaction is possible (and usually convenient) by the LuaJIT interface, which provides
a fast and intuitively scripting language. Moreover, it contains all the libraries necessary for the
integration with the CUDA R(cid:13) environment for GPU computing. At the moment of writing it is
one of the most used tool for prototyping ANNs of any kind of topology. Indeed, there are many
packages, constantly updated and improved by a large community, allowing to develop almost any
kind of architectures in a very simple way.

Informations about the installation can be found at the getting started section of the oﬃcial
site6. The procedure is straightforward for UNIX based operative systems, whereas is not oﬃcially
supported for Windows, even if an alternative way is provided7. If CUDA R(cid:13) is already installed, also
the packages cutorch and cunn will be added automatically, containing all the necessary utilities
to deal with Nvidia GPUs.

2.2 Getting started

2.2.1 Lua

Lua, in torch7, acts as an interface for C/CUDA routines. A programmer, in most of the cases, will
not have to worry about C functions. Therefore, we explain here only how the Lua language works,
because is the only one necessary to deal with Torch. It is a scripting language with a syntax similar
to Python and semantic close to Javascript. A variable is considered as global by default. The local
declaring, which is usually recommended, require the explicit declaration by placing the keyword
local before the name fo the variable. Lua has been chosen over other scripting languages, such
as Python, because is the fastest one, a crucial feature when dealing with large data and complex
programs, as common in Machine Learning.

There are seven native types in lua: nil, boolean, number, string, userdata, function
and table, even if most of the Lua power is related to the last one. A table behaves either as
an hash map (general case) or as an array (which have the 1-based indexing as in Matlaband
Python). The table will be considered as an array when contains only numerical keys, starting from
the value 1. Any other complex structure such as classes, are built from it (formally deﬁned as a
Metatable).

A detailed documentation on Lua can be ﬁnd at the oﬃcial webpage8, however, an essential and

fast introduction can be found at http://tylerneylon.com/a/learn-lua/.

2.2.2 Torch enviroment

Torch extends the capabilities of the Lua table implementing the Tensor class. Many Matlab-like
functions9 are provided in order to initialize and manipulate tensors in a concise fashion. Most

6http://torch.ch/docs/getting-started.html
7https://github.com/torch/torch7/wiki/Windows
8Lua 5.1 reference manual is available here: https://www.lua.org/manual/5.1/
9http://atamahjoubfar.github.io/Torch_for_Matlab_users.pdf

10

commons are reported in Listing 2.

1 l o c a l
l o c a l
3 l o c a l

t 1 = t o r c h . T e n s o r ( ) −− no d i m e n s i o n t e n s o r c o n s t r u c t o r
t 2 = t o r c h . T e n s o r ( 4 , 3 ) −− 4 x3 empty t e n s o r
t 3 = t o r c h . e y e ( 3 , 5 ) −− 3 x5 1− d i a g o n a l matrix

t 2 : f i l l ( 1 ) −− f i l l

t h e matrix with t h e v a l u e 1

5 t 1 = torch.mm ( t2 , t 3 ) −− a s s i g n t o t 1 t h e r e s u l t o f matrix m u l t i p l i c a t i o n between t 2

and t 3

t 1 [ 1 ] [ 2 ] = 5 −− a s s i g n 5 t o t h e e l e m e n t

i n f i r s t

row and s e c o n d column

Listing 2: Example of torch tensor basic usages

All the provided packages are developed following a strong modularization, which is a crucial
feature to keep the code stable and dynamic. Each one provides several already built-in function-
alities, and all of them can be easily imported from Lua code. The main one is, of course, torch,
which is installed at the beginning. Not all the packages are included at ﬁrst installation, but it is
easy to add a new one by the shell command:

luarocks install packagename

where luarocks is the package manager, and packagename is the name of the package you want to
install.

The nn package

All (almost) you need to create (almost) any kind of ANNs is contained in the nn package (which is
usually automatically installed). Every element inside the package inherits from the abstract Lua
class nn.Module. The main state variables are output and gradInput, where the result of forward
and backward steps (in back-propagation) will be stored. forward and backward are methods of
such class (which can be accessed by the object:method() notation). They invoke updateOutput
and updateGradInput respectively, that here are abstract and the deﬁnition must be in the derived
classes.

The main advantage of this package is that all the gradients computations in the back-propagation
step are automatically realized thanks to these built-in functions. The only requirement is to call
the forward step before the backward.

The weights of the network will be updated by the method updateGradParameters, assigning
a new value to each parameter of a module (according to the Gradient Descent rule) exploiting the
learning rate passed an argument of the function.

The bricks you can use to construct a network can be divided as follows:

• Simple layers: the common modules to implement a layer. The main is nn.Linear, com-

puting a basic linear transformation.

• Transfer functions: here you can ﬁnd many activation functions, such as nn.Sigmoid or

nn.Tanh

• Criterions: loss functions for supervised tasks, a for instance is nn.MSECriterion

• Containers: abstract modules that allow us to build multi-layered networks. nn.Sequential
connect several layers in a feed-forward manner. nn.Parallel and nn.Concat are important
to build more complex structure, where the input ﬂows in separated architectures. Layers,
activation functions and even criterions can be added inside those containers.

11

For detailed documentation of the nn package we refer to the oﬃcial webpage10. Another useful
package for whom could be interested on building more complex architectures can be found at the
nngraph repository11.

CUDA R(cid:13) computing

Since C++/Cuda programming and integration are not trivial to develop, it is important to have
an interface as simple as possible linking such tools. Torch provides a clean solution for that with
the two dedicated packages cutorch and cunn (requiring, of course, a capable GPU and CUDA R(cid:13)
installed). All the objects can be transferred into the memory of GPUs by the method :cuda()
and then back to the CPU by :double(). Operations are executed on the hardware of the involved
In Listing 3 we show some
objects and are possible only among variables from the same unit.
examples of correct and wrong statements.

l o c a l cpuTensor1 = t o r c h . T e n s o r ( 3 , 3 ) : f i l l ( 1 )
2 l o c a l cpuTensor2 = t o r c h . T e n s o r ( 3 , 3 ) : f i l l ( 2 )

l o c a l cudaTensor1 = t o r c h . T e n s o r ( 3 , 3 ) : f i l l ( 3 ) : cuda ( )
4 l o c a l cudaTensor2 = t o r c h . T e n s o r ( 3 , 3 ) : f i l l ( 4 ) : cuda ( )

6 cpuTensor1 : cmul ( cpuTensor2 ) −− OK

cpuTensor1 : cmul ( cudaTensor2 ) −− WRONG

8 cudaTensor1 : cmul ( cudaTensor2 ) −− OK

cudaTensor1 : cmul ( cpuTensor2 ) −− WRONG

Listing 3: Samples of CUDA operations.

2.3 Setting up the XOR experiment

In order to give a concrete feeling about the presented tools, we show some examples on the classical
XOR problem as in the previous section. The code showed here below can be found in the Torch
section of the document’s GitHub repository12 and can be useful to play with the parameters and
become more familiar with the environment.

Architecture

1 r e q u i r e

’ nn ’

When writing a a script, the ﬁrst command is usually the import of all the necessary packages by
the keyword require. In this case, only the nn toolbox is required:

We deﬁne a standard ANNs with one hidden layer composed by 2 hidden units, the hyperbolic
tangen (tanh) as transfer function and identity as output function. The structure of the network
will be stored in a container where all the necessary modules will be added. A standard feed-forward
architecture can be deﬁned into a Sequential container, which we named mlp. The network can
be then assembled by adding sequentially all the desired modules by the function add():

10https://github.com/torch/nn
11Detailed documentation at https://github.com/torch/nngraph
12Available at https://github.com/AILabUSiena/NeuralNetworksForBeginners/tree/master/torch/xor.

12

1 mlp = n n . S e q u e n t i a l ( ) −− c o n t a i n e r

i n i t i a l i z a t i o n

3 mlp : add ( n n . L i n e a r ( i n p u t s , HUs) ) −− f i r s t

i n p u t s = 2 ; o u t p u t s = 1 ; HUs = 2 ; −− g e n e r a l o p t i o n s
l a y e r
t h e hidden l a y e r
l a y e r

5 mlp : add ( n n . L i n e a r (HUs , o u t p u t s ) ) −− output

mlp : add ( nn.Tanh ( ) ) −− a c t i v a t i o n f o r

l i n e a r

Listing 4: Code to create a 2-layer ANNs

Dataset

The training set will be composed by a tensor of 4 samples (organized again column-wise) paired
with a tensor of targets. Usually, true and false boolean values are respectively associated to 1 and
0. However, just to propose an equivalent but diﬀerent approach, here we shift both values by −0.5,
so they will be in [−0.5, 0.5] as showed in Listing 5. Both input and target are initialized with false
values (a tensor ﬁlled with 0), and then true values are placed according to the XOR truth table.

1 l o c a l d a t a s e t = t o r c h . T e n s o r ( 4 , 2 ) : f i l l ( 0 )

l o c a l

t a r g e t = t o r c h . T e n s o r ( 4 , 1 ) : f i l l ( 0 )

3 d a t a s e t [ 2 ] [ 1 ] = 1 −− True F a l s e
d a t a s e t [ 3 ] [ 2 ] = 1 −− F a l s e True

5 d a t a s e t [ 4 ] [ 1 ] = 1 ; d a t a s e t [ 4 ] [ 2 ] = 1 −− True True

d a t a s e t = d a t a s e t : add(−0 . 5 ) −− s h i f t

t r u e and f a l s e by −0 . 5

7 t a r g e t [ 2 ] [ 1 ] = 1 ;

t a r g e t [ 3 ] [ 1 ] = 1

t a r g e t = t a r g e t : add(−0 . 5 )

Listing 5: creation of 4 examples and their targets

Training

We set up a full–batch mode learning, i.e. we update the parameters after accumulating the gradients
over the whole dataset. We exploit the following function:

forward(input) returns the output of the multi layer perceptron w.r.t the given input; it updates
the input/output states variables of each modules, preparing the network for the backward
step; its output will be immediately passed to the loss function to compute the error.

zeroGradParameters() resets to null values the state of the gradients of the all the parameters.

backward(gradients) actually computes and accumulates (averaging them on the number of sam-
ples) the gradients with respect to the weights of the network, given the data in input and
the gradient of the loss function.

updateParameters(learningrate) modiﬁes the weights according to the Gradient Descent proce-

dure using the learning rate as input argument.

As loss function we use the Mean Square Error, created by the statement:

c r i t e r i o n = nn.MSECriterion ( )

13

When a criterion is forwarded, it returns the error between the two input arguments. It updates
its own modules state variable and gets ready to compute the gradients tensor of the loss in the
backward step, which will be back-propagated through the multilayer perceptron. As a nn modules,
all the possible criterions used the functions forward() and backward() as the others. The whole
training procedure can be set up by:

1 nepochs = 1 0 0 0 ;

l e a r n i n g _ r a t e = 0 . 0 5 ; −− g e n e r a l o p t i o n s

l o c a l

l o s s = t o r c h . T e n s o r ( nepochs ) : f i l l ( 0 ) ; −− t r a i n i n g l o s s

i n i t i a l i z a t i o n

3 f o r

i = 1 , nepochs do −− i t e r a t i n g o v e r 1000 e p o c h s

5

7

9

i n p u t = d a t a s e t

l o c a l
l o c a l o u t p u t s = mlp : f o r w a r d ( i n p u t ) −− network f o r w a r d s t e p
t a r g e t ) −− e r r o r
l o s s [ i ] = c r i t e r i o n : f o r w a r d ( outputs ,
mlp : zeroGradParame te rs ( ) −− z e r o r e s e t o f g r a d i e n t s
l o c a l g r a d i e n t s = c r i t e r i o n : backward ( mlp.output ,
mlp : backward ( input , g r a d i e n t s ) −− network backward s t e p
mlp : updateParameters ( l e a r n i n g _ r a t e ) −− update p a r a m e t e r s g i v e n t h e l e a r n i n g r a t e

t a r g e t ) −− l o s s g r a d i e n t s

e v a l u a t i o n

11 end

Listing 6: training of the network

2.4 Stopping criterions and Regularization

Since the training procedure is manually deﬁned, particular stopping criterion are completely up to
the user. The simplest one, based on the reaching of a ﬁxed number of epochs explicitly depends of
the upper bound of the for cycle. Since other methods are related to the presence of a validation
set, we will deﬁne an example of early stopping criterion in Listing 14 in Section 4. A simple
criterion based on the vanishing of the gradients can be simply set up by exploiting the function
getParameters deﬁned for the modules of nn, which returns all the weights and the gradients of
the network in two 1-Dimensional vector:

1 param , grad = mlp : g e t P a r a m e t e r s ( )

A simple check on the minimum value of the absolute values of gradients saved in grad can be used
to stop the training procedure.

Another regularization method can be accomplished by implementing the weight decay method
as shown in Listing 7. The presented code is intended to be an introductory example even to
understand the class inheritance mechanisms in Lua and Torch.

1 −− d e f i n i n g c l a s s

i n h e r i t a n c e s

l o c a l WeightDecay , p a r e n t = t o r c h . c l a s s ( ’ nn.WeightDecayWrapper ’ ,

’ n n . S e q u e n t i a l ’ )

i s a keyword r e f e r r i n g t o t h e a b s t r a c t o b j e c t

3

5

7

9

11

13

f u n c t i o n WeightDecay : __init ( ) −− c o n s t r u c t o r

p a r e n t . _ _ i n i t ( s e l f )
s e l f . w e i g h t D e c a y = 0 −− s e l f
s e l f . c u r r e n t O u t p u t = 0

end

f u n c t i o n WeightDecay : getWeightDecay ( a l p h a )

l o c a l a l p h a = a l p h a o r 0
l o c a l
f o r

weightDecay = 0
i =1,# s e l f . m o d u l e s do

14

l o c a l params ,_ = s e l f . m o d u l e s [ i ] : p a r a m e t e r s ( )
i f params then

f o r

j =1,#params do

weightDecay = weightDecay + t o r c h . d o t ( params [ j ] , params [ j ] ) ∗ a l p h a /2

end

end

end
s e l f . w e i g h t D e c a y = weightDecay
r e t u r n s e l f . w e i g h t D e c a y

23 end

15

17

19

21

27

29

31

33

end

end

end

35 end

25 f u n c t i o n WeightDecay : updateParameters ( l e a r n i n g R a t e , a l p h a )

l o c a l a l p h a = a l p h a o r 0
i =1,# s e l f . m o d u l e s do
f o r

l o c a l params , gradParams = s e l f . m o d u l e s [ i ] : p a r a m e t e r s ( )
i f params then

f o r

j =1,#params do

params [ j ] : add(− l e a r n i n g R a t e , gradParams [ j ] + ( a l p h a ∗ params [ j ] ) )

Listing 7: Code to implement the weight decay regularization

To implement the weight decay coherently with the nn package, we need to create a novel class, inher-
iting from nn.Sequential, that overloads the method updateParameters() of the nn.Module. We
ﬁrst have to declare a new class name in Torch, where you can optionally specify the parent class. In
our case the new class has been called nn.WeightDecayWrapper, while the class it inherits from is the
Container nn.Sequential. The constructor is deﬁned within the function WeightDecay:__init().
In the scope of this function the variable self is a table used to refer to all the attributes and
methods of the abstract object. The calling of the function __init() from the parent class au-
tomatically add all the original properties. The functions WeigthDecay:getWeigthDecay() and
WeigthDecay:updateParameters() compute respectively the weight decay and its gradient. Both
methods loop over all the modules of the container (the symbol # returns the number-indexed
element of a table) and, for each one that has parameters, use them in order to compute either
the error or the gradients coming from the weight decay contribution. The argument alpha repre-
sent the regularization parameter of the weight decay and, if not provided, is assumed null. It is
also worth to mention the fact that, WeigthDecay:updateParameters() overloads the method that
implemented in nn.Module, updating the parameters according to the standard Gradient Descent
rule. At this point, an ANN expecting a possible weight decay regularization can be declared by
replacing the nn.Sequential container by the proposed nn.WeightDecayWrapper.

2.5 Drawing Separation Surfaces

In this framework, data visualization is allowed by the package gnuplot, which provides some tools
to plot points, lines, curves and so on. For example, in the training procedure presented in Listing 6,
a vector storing the penalty evaluated at each epoch is produced. To have an idea of the state of
the network during training, we can save an image ﬁle containing the trend of the error by the code
in Listing 8, whom output is shown in Figure 2(a).

15

1

3

5

g n u p l o t . e p s f i g u r e ( ’ XOR loss.eps ’ ) −− c r e a t e an e p s
g n u p l o t . p l o t ( { ’ l o s s
f u n c t i o n ’ ,
g n u p l o t . t i t l e ( ’ Loss ’ )
g n u p l o t . g r i d ( t r u e )
g n u p l o t . p l o t f l u s h ( )
g n u p l o t . f i g u r e ( )

t o r c h . r a n g e ( 1 , nepochs ) , l o s s } ) −− l o s s

t r e n d p l o t

f i l e with t h e image

Listing 8: Error evaluated at each training epoch.

Torch does not have dedicated functions to visualize separation surfaces produced on data and,
hence, we generate a random grid across the input space, plotting only those points predicted close
enough (with respect to a a certain threshold) to the half of possible target (0 in this case). The
correspondent result, showed in Figure 2(b), is generated by the code in Listing 9, exploiting the
fact that Lua support the logical indexing as in Matlab.

Loss

Xor

Separation Surface

 0.3

 0.25

 0.2

 0.15

 0.1

 0.05

 0

 0

 100

 200

 300

 400

 500

 600

 700

 800

 900

 1000

-0.5

-0.4

-0.3

-0.2

-0.1

 0

 0.1

 0.2

 0.3

 0.4

 0.5

(a)

(b)

Figure 2: (a) Trend of loss versus the number of epochs. (b) The estimated separation surface
obtained by a 2-Layers ANN composed by 2 Hidden Units, Hyperbolic Tangent as activation and
linear output.

−− i n p u t s p a c e g r i d by n 2D−p o i n t s u n i f o r m l y d i s t r i b u t e d i n [ −0 . 5 , 0 . 5 ]

l o c a l e p s = 2 e −3; −− s e p a r a t i o n t h r e s h o l d
n = 1000000 −− g r i d d i n g s i z e

2 l o c a l

4 l o c a l x = t o r c h . r a n d ( n , 2 ) : add(−0 . 5 )

l o c a l mlpOutput = mlp : f o r w a r d ( x ) −− e v a l u a t i o n

6 −− compare whether t h e a b s o l u t e v a l u e o f

t h e network output

i s

l e s s

e q u a l than e p s

l o c a l mask = t o r c h . l e ( t o r c h . a b s ( mlpOutput ) , mlpOutput : c l o n e ( ) : f i l l ( e p s ) )

8 i f

t o r c h . s u m ( mask ) > 0 then

g n u p l o t . e p s f i g u r e ( ’ S e p a r a t i o n s u r f a c e . e p s ’ )
l o c a l x1 = x : narrow ( 2 , 1 , 1 ) −− r e s h a p i n g f i r s t
i n p u t d i m e n s i o n f o r x a x i s
l o c a l x2 = x : narrow ( 2 , 2 , 1 ) −− r e s h a p i n g s e c i n d i n p u t d i m e n s i o n f o r y a x i s
−− p l o t t i n g o f
g n u p l o t . p l o t ( x1 [ mask ] , x2 [ mask ] ,
g n u p l o t . t i t l e ( ’ S e p a r a t i o n s u r f a c e ’ )

t h e c o l l e c t i o n o f p o i n t s

t h a t match t h e mask

’+ ’ )

10

12

14

 0.5

 0.4

 0.3

 0.2

 0.1

 0

-0.1

-0.2

-0.3

-0.4

-0.5

16

g n u p l o t . g r i d ( t r u e )
g n u p l o t . p l o t f l u s h ( ) −− s a v e t h e image
g n u p l o t . f i g u r e ( )

16

18 end

Listing 9: Drawing separation surfaces in torch

17

3 TensorFlow

3.1 Introduction

TensorFlow [5] is an open source software library for numerical computation and is the youngest with
respect to the others Machine Learning frameworks. It was originally developed by researchers and
engineers from the Google Brain Team, with the purpose of encourage research on deep architectures.
Nevertheless, the environment provides a large set of tools suitable for several domains of numerical
programming. The computation is conceived under the concept of Data Flow Graphs. Nodes in the
graph represent mathematical operations, while the graph edges represent tensors (multidimensional
data arrays). The core of the package is written in C++, but provides a well documented Python
API. The main characteristic is its symbolic approach, which allows a general deﬁnition of a forward
models, leaving the computation of the correspondent derivatives entirely to the environment itself.

3.2 Getting started

3.2.1 Python

A TensorFlow model can be easily written using Python, a very intuitive object-oriented program-
ming language. Python is distributed with an open-source license for commercial use too. It oﬀers
a nice integration with many other programming languages and provides an extended standard
library which includes numpy (modules designed for matrix operations, very similar to the Matlab
syntax). Python runs on Windows, Linux/Unix, Mac OS X and other operative systems.

3.2.2 TensorFlow environment

Assuming that the reader is familiar with Python, here we present the building blocks of TensorFlow
framework:

The Data Flow Graph To leverage the parallel computational power of multi-core CPU, GPU
and even clusters of GPUs, the dynamic of the numerical computations has been conceived as a
directed graph, where each node represents a mathematical operation and the edges describe the
input/output relation between nodes.

Tensor

It is a typed n-dimensional array that ﬂows through the Data Flow Graph.

Variable Symbolic objects designed to represent parameters. They are exploited to compute the
derivatives at a symbolical level, but in general must be explicitly initialized in a session.

Optimizer
It is the component which provides methods to compute gradients from the loss func-
tion and to apply back-propagation through all the variables. A collection is available in TensorFlow
to implement classic optimization algorithms.

Session A graph must be launched in a Session, which places the graph onto CPU or GPU and
provides methods to run computation.

18

3.2.3 Installation

Information about download and installation of Python and TensorFlow are available in the oﬃcial
webpages13. Notice that a dedicated procedure must be followed for GPU installation. It’s worth
a quick remark on the CUDA R(cid:13) versions. Indeed, versions from 7.0 are oﬃcially supported, but the
installation could be not straightforward in versions preceding the 8.0. Moreover, a registration to
the Accelerate Computing Developer Program 14 is required to install the package cuDNN, which is
mandatory to enable GPU support.

3.3 Setting up the XOR experiment

As in the previous sections of this tutorial, we show how to start managing the TensorFlow frame-
work by facing the simple XOR classiﬁcation problem by a standard ANN.

At the beginning, as for every Python library, we need to import the TensorFlow package by:

Import tensor ﬂow

import

t e n s o r f l o w a s

t f

ciao

Dataset deﬁnition

2

3

Again, data can be deﬁned as two matrices containing the input data and its correspondent target,
called X and Y respectively. Data can be deﬁned as a list or numpy array. After they will be used
to ﬁll the placeholder that actually deﬁne a type and dimensionality.

1 X = [ [ 0 , 0 ] , [ 0 , 1 ] , [ 1 , 0 ] , [ 1 , 1 ] ]

Y = [ [ 0 ] , [ 1 ] , [ 1 ] , [ 0 ] ]

Placeholders

TensorFlow provides Placeholders which are symbolic variables representing data during the com-
putation. A Placeholders object have to be initialized with given type and dimensionality, suitable
to represent the desired element. In this case we deﬁne two object x_ and y_ respectively for input
data and target:

x_ = t f . p l a c e h o l d e r ( t f . f l o a t 3 2 ,
2 y_ = t f . p l a c e h o l d e r ( t f . f l o a t 3 2 ,

shape = [ 4 , 2 ] )
shape = [ 4 , 1 ] )

13Python webpage: https://www.python.org/, TensorFlow webpage: https://www.tensorflow.org/
14https://developer.nvidia.com/accelerated-computing-developer

19

The description of the network depends essentially on its architecture and parameters (weights and
biases). Since the parameters have to be estimated, they are deﬁned as the variabile of the model,
whereas the architecture is determined by the conﬁguration of symbolic operations. For a 2-layers
ANN we can deﬁne:

Model deﬁnition

# Hidden u n i t s

2 HU = 3
# 1 s t

l a y e r

4 W1 = t f . V a r i a b l e ( t f . random_uniform ( [ 2 ,HU] , −1.0 , 1 . 0 ) ) # w e i g h t s matrix

b1 = t f . V a r i a b l e ( t f . z e r o s ( [HU] ) )

# b i a s

6 O = t f . nn . s i g m o i d ( t f . matmul (x_, W1) + b1 ) # non−l i n e a r a c t i v a t i o n output

# 2nd l a y e r

8 W2 = t f . V a r i a b l e ( t f . random_uniform ( [ HU, 1 ] , −1.0 , 1 . 0 ) )

b2 = t f . V a r i a b l e ( t f . z e r o s ( [ 1 ] ) )

10 y = t f . nn . s i g m o i d ( t f . matmul (O, W2) + b2 )

The matmul() function performs tensors multiplication. Variable() is the constructor of the class
variable. It needs an initialization value which must be a tensor. The function random_uniform()
returns a tensor of a speciﬁed shape, ﬁlled with valued picked from a uniform distribution between
two speciﬁed values. The nn module contains the most common activation functions, taking as
input a tensor and evaluating the non-linear transferring component-wise (the Logistic Sigmoid is
chosen in the reported example by tf.nn.sigmoid()).

Loss and optimizer deﬁnition

The cost function and the optimizer are deﬁned by the following two lines

# q u a d r a t i c l o s s

f u n c t i o n

2 c o s t = t f . reduce_sum ( t f . s q u a r e (y_ − y ) ,

r e d u c t i o n _ i n d i c e s = [ 0 ] )

# o p t i m i z i n g t h e f u n c t i o n c o s t by g r a d i e n t d e s c e n t with l e a r n i n g s t e p 0 . 1

4 t r a i n _ s t e p = t f . t r a i n . G r a d i e n t D e s c e n t O p t i m i z e r ( 0 . 1 ) . minimize ( c o s t )

TensorFlow provides functions to perform common operations between tensors. The function
reduce_sum() for example reduces the tensor to one (or more) dimension, by summing up along
the speciﬁed dimension. The train module provide the most common optimizers, which will be
employed during the training process. The previous code chose the Gradient Descent algorithm to
optimize the network parameters, with respect to the penalty function deﬁned in cost by using a
learning rate equal to 0.1.

At this point the variables are still not initialized. The whole graph exist at a symbolic level, but it
is instantiated when creating a session. For example, placeholders are fed with the assigned elements
in this moment.

Start the session

% C r e a t e s e s s i o n

2 s e s s = t f . S e s s i o n ( )

20

% I n i t i a l i z e v a r i a b l e s

4 s e s s . run ( t f . i n i t i a l i z e _ a l l _ v a r i a b l e s ( ) )

More speciﬁcally, initialize_all_variables() creates an operation (a node in the Data Flow
Graph) running variables initializer. The function Session() creates an instance of the class session,
while the correspondent method run() moves for the ﬁrst time the Data Flow Graph on CPU/GPU,
allocates variables and ﬁlls them with the initial values.

Training

The training phase can be deﬁned in a for loop where each iteration represent a single gradient
descend epoch. In the following code, some printing on the training information are added each 100
epochs.

Epochs = 5000 # Number o f

i t e r a t i o n s

f o r

i

i n r a n g e ( Epochs ) :

s e s s . run ( t r a i n _ s t e p ,
i % 100 == 0 :
i f
p r i n t ( ’ Epoch ’ ,
p r i n t ( ’ Cost

’ ,

i )

f e e d _ d i c t ={x_ : X, y_ : Y} ) # o p t i m i z e r

s t e p

s e s s . run ( c o s t ,

f e e d _ d i c t ={x_ : X, y_ : Y} ) )

The sess.run() calling runs the operations previously deﬁned for the ﬁrst argument, which in this
case is an optimizer step (deﬁned by train_step). The second (optional) argument for run is a
dictionary feed_dict, pairing each placeholder with the correspondent input. The function run()
is used also to evaluate the cost each 100 epochs.

Evaluation

The performance of the trained model can be easily evaluated by:

1 c o r r e c t _ p r e d i c t i o n = abs (y_ − y ) < 0 . 5

c a s t = t f . c a s t ( c o r r e c t _ p r e d i c t i o n , " f l o a t " )

3 a c c u r a c y = t f . reduce_mean ( c a s t )

5 yy , aa = s e s s . run ( [ y , a c c u r a c y ] , f e e d _ d i c t ={x_ : X, y_ : Y} )

7 p r i n t " Output : " , yy

p r i n t " Accuracy : " , aa

2

4

6

8

9

Draw separation surfaces

In order to visualize separation surfaces computed by the network, it can be useful to generate a
random sample of points on which test results, as showed in Figure 3.

21

p l t . f i g u r e ( )

2 # P l o t t i n g d a t a s e t

c1 = p l t . s c a t t e r ( [ 1 , 0 ] ,
4 c0 = p l t . s c a t t e r ( [ 1 , 0 ] ,

[ 0 , 1 ] , marker= ’ s ’ , c o l o r= ’ gray ’ ,
[ 1 , 0 ] , marker= ’ ^ ’ , c o l o r= ’ gray ’ ,

s =100)
s =100)

# G e n e r a t i n g p o i n t s

i n [ − 1 , 2 ] x [ − 1 , 2 ]

6 DATA_x = ( np . random . rand ( 1 0 ∗ ∗ 6 , 2 ) ∗3)−1

DATA_y = s e s s . run ( y , f e e d _ d i c t ={x_ : DATA_x} )

8 # S e l e c t i n g b o r d e r l i n e p r e d i c t i o n s

i n d = np . where ( np . l o g i c a l _ a n d ( 0 . 4 9 < DATA_y, DATA_y< 0 . 5 1 ) ) [ 0 ]

10 DATA_ind = DATA_x[ i n d ]

# P l o t t i n g s e p a r a t i o n s u r f a c e s

12 s s = p l t . s c a t t e r (DATA_ind [ : , 0 ] , DATA_ind [ : , 1 ] , marker= ’_ ’ , c o l o r= ’ b l a c k ’ ,

s =5)

14 p l t . l e g e n d ( ( c1 , c0 ,

( ’ C l a s s 1 ’ ,

’ C l a s s 0 ’ ,

’ S e p a r a t i o n s u r f a c e s ’ ) ,

s c a t t e r p o i n t s =1)

# Some f i g u r e ’ s

s e t t i n g s
s s ) ,

p l t . x l a b e l ( ’ Input x1 ’ )
16 p l t . y l a b e l ( ’ Input x2 ’ )
p l t . a x i s ( [ − 1 , 2 , − 1 , 2 ] )

18 p l t . show ( )

Listing 10: Draw separation surfaces

Figure 3: Separation surfaces on the XOR classiﬁcation task obtained by 2-layer ANN with 3 Hidden
Units and the Logistic Sigmoid as activation and output function.

22

4 MNIST Handwritten Characters Recognition

In this Section we show how to set up a 2-Layer ANN in order to face the MNIST [6] classiﬁcation
problem, a well known data set for handwritten characters recognition. It is extensively used to
test and compare general Machine Learning algorithms and Computer Vision methods. Data are
provided as 28×28 pixels (grayscale) images of handwritten digits. The training and test sets contain
respectively 60,000 and 10,000 instances. Files .zip are available at the oﬃcial site15, together with
a list of performance achieved by most common algorithms. We show the setting up of a standard
2-Layer ANN with 300 units in the hidden layer, represented in Figure 4, since it is one of the
architecture reported in the oﬃcial website and the obtained results can be easily compared. The
input will be reshaped so as to feed the network with a 1-Dimensional vector with 28 · 28 = 784
elements. Each image is originally represented by a matrix containing the grayscale value of the
pixels in [0, 255], which will be normalized in [0, 1]. The output will be a 10 elements prediction
vector, since labels for each element will be expressed by the one-hot encoding binary vector of 10
null bits, with only a 1 in the position indicating the class. Activation and penalty functions are
diﬀerent within diﬀerent environments to provide an overview on diﬀerent approaches.

Figure 4: General architecture of a 2-Layer network model proposed to face the MNIST data.

4.1 MNIST on Matlab

Once data have been downloaded from the oﬃcial MNIST site, we can use Matlabfunctions avail-
able at the Stanford University site16 to extract data from ﬁles and organize them in inputSize
–by–numberOfSamples matrix form. The extraction routines reshape (so as that each digit is rep-
resented by a 1-D column vector of size 784) and normalizes data (so as that each feature lies in
the interval [0, 1]) . Once unzipped data and functions in the same folder, it is straightforward to
upload images in the Matlabworkspace by the loadMNISTImages function:

15http://yann.lecun.com/exdb/mnist/.
16http://ufldl.stanford.edu/wiki/index.php/Using_the_MNIST_Dataset.

23

2

1

3

images_Train = loadMNISTImages ( ’ t r a i n −images . idx 3−uby te ’ ) ;
images_Test = loadMNISTImages ( ’ t10k−images . idx 3−uby te ’ ) ;
images = [

images_Train ,

images_Test ] ;

where training and test set have been grouped in the same matrix to evaluate performance on the
provided test set during the training. Correspondent labels can be loaded and grouped in a similar
way by the function loadMNISTLabels:

l a b e l s _ T r a i n = loadMNISTLabels ( ’ t r a i n −l a b e l s . idx 1−uby te ’ ) ;
l a b e l s _ T e s t = loadMNISTLabels ( ’ t10k−l a b e l s . idx 1−uby te ’ ) ;
l a b e l s = [

l a b e l s _ T r a i n ;

l a b e l s _ T e s t ] ;

The original labels are provided as a 1-Dimensional vector containing a number from 0 to 9 according
to the correspondent digit. The one-hot encoding target matrix for the whole dataset can be
generated exploiting the Matlabfunction ind2vec17:

1 l a b e l s = f u l l (

i n d 2 v e c (

l a b e l s ’ + 1 )

) ;

To check the obtained results, we replied one of the 2-layer architectures listed at the oﬃcial website,
which is supposed to reach around 96% of accuracy with 300 hidden units and can be initialized by:

1 nn = p a t t e r n n e t ( 300 ) ;

As already said, this command creates a 2-Layer ANN where the hidden layer has 300 units and the
Hyperbolic Tangent as activation, whereas the output function is computed by the softmax. The
(default) penalty function is the Cross-Entropy Criterion.

In this case we change the data splitting so as that data used for test comes only from the original
test set (which has been concatenated with the training one), prevent to mix samples among Train,
Validation and Test set. This step is completely customizable by the method divideFcn and the
ﬁelds of the options divideParam. The divideind method picks data according to the provided
indexes for the data matrix:

1 nn . d i v i d e F c n = ’ d i v i d e i n d ’ ;

nn . div ideParam . t r a i n I n d = 1 : 4 5 0 0 0 ;
3 nn . div ideParam . v a l I n d = 4 5 0 0 0 : 6 0 0 0 0 ;

nn . div ideParam . t e s t I n d = 6 0 0 0 0 : 7 0 0 0 0 ;

In this case, we arbitrarily decided to use the last 25% of the Training data for Validation, since
the samples are more or less equally distributed by classes.
As already said, network training can be started by:

17The function full prevent for Matlabautomatically convert to sparse matrix, which in our tests may cause some

problems at the calling of the function train.

24

[ nn ,

t r

] = t r a i n ( nn ,

images ,

l a b e l s

) ;

In the reported case, the training stopped after 107 epochs because of an increasing in the validation
error (see Section 1.3). The performance during training are shown in Figure 5(a), which is obtained
by the following code:

1 % % % % P l o t t i n g MNIST t r a i n i n g p e r f o r m a n c e

3 p l o t ( t r . p e r f , ’ LineWidth ’ , 2 ) ; % p l o t

t r a i n i n g e r r o r

h o l d on ;

5 p l o t ( t r . v p e r f , ’ r −. ’ , ’ LineWidth ’ , 2 ) ; % p l o t v a l i d a t i o n e r r o r

p l o t ( t r . t p e r f , ’ g : ’ , ’ LineWidth ’ , 2 ) ; % p l o t
7 s e t ( gca , ’ y s c a l e ’ , ’ l o g ’ ) ; % s e t t i n g l o g s c a l e

t e s t

e r r o r

a x i s ( [ 1 , 1 0 7 , 0 . 0 0 1 , 1 . 8 ] ) ;

9 x l a b e l ( ’ T r a i n i n g Epochs ’ , ’ F o n t S i z e ’ , 1 4 ) ;
y l a b e l ( ’ Cross−Entropy ’ , ’ F o n t S i z e ’ , 1 4 ) ;

11 t i t l e ( ’ Performance Trend on MNIST ’ , ’ F o n t S i z e ’ , 1 6 ) ;

h = l e g e n d ( { ’ T r a i n i n g ’ , ’ V a l i d a t i o n ’ , ’ Test ’ } , ’ L o c a t i o n ’ , ’ North East ’ ) ;

13 s e t ( h , ’ F o n t S i z e ’ , 1 4 ) ;

Performance Trend on MNIST

Training
Validation
Test

5 → 6

8 → 0

2 → 8

10

20

30

40

50

60

70

80

90

100

Training Epochs

(a)

(b)

4 → 6

7 → 3

3 → 7

Figure 5: On the left the performance on the MNIST dataset during the training of a 2-layer ANN
with 300 hidden units. Training is stopped after 107 epochs for validation checking. On the right
we report some misclassiﬁed samples. The network reaches about 96% of classiﬁcation accuracy on
the test set (in accordance with the ones provided at the MNIST web page).

In Figure 5(b) we show some misclassiﬁed digits, indicating the original label and the predicted one.
The visualization is obtained by the Matlab function image (after a reshaping to the original square
dimensions and grayscale, multiplying by 255). In Listing 11 we show how to evaluate classiﬁcation
accuracy and confusion matrix on data, which should give coherent results with respect to which
reported in the oﬃcial site for the same architecture (about 4% error on test set).

25

y
p
o
r
t
n
E
-
s
s
o
r
C

10 0

10 -1

10 -2

10 -3

1 % network e v a l u a t i o n

f = nn (
3 f v = nn (
f t = nn (

images (

: , 1 : 4 5 0 0 0 )

) ; % t r a i n i n g p r e d i c t i o n s

images (
images (

: , 4 5 0 0 1 : 6 0 0 0 0 )
: , 6 0 0 0 1 : end )

) ; % v a l i d a t i o n p r e d i c t i o n s

) ; % t e s t p r e d i c t i o n s

5 % c l a s s i f i c a t i o n a c c u r a c y

A = mean ( v e c 2 i n d ( f ) == v e c 2 i n d (
7 Av = mean ( v e c 2 i n d ( f v ) == v e c 2 i n d (
At = mean ( v e c 2 i n d ( f t ) == v e c 2 i n d (

l a b e l s (

: , 1 : 4 5 0 0 0 )

)

) ;

l a b e l s (
l a b e l s (

: , 4 5 0 0 1 : 6 0 0 0 0 )
)
: , 6 0 0 0 1 : end )

) ;

)
) ;

9 % c o n f u s i o n matrix
C = c o n f u s i o n m a t (

11 Cv = c o n f u s i o n m a t ( v e c 2 i n d ( f v ) , v e c 2 i n d (
Ct = c o n f u s i o n m a t ( v e c 2 i n d ( f t ) , v e c 2 i n d (

l a b e l s (
l a b e l s (

: , 4 5 0 0 1 : 6 0 0 0 0 )
)
: , 6 0 0 0 1 : end )

) ;

)
) ;

v e c 2 i n d ( f ) , v e c 2 i n d (

l a b e l s (

: , 1 : 4 5 0 0 0 )

)

)

;

Listing 11: Performance evaluation of the network trained on the MNIST

4.2 MNIST on Torch

As already said, the Torch environment provides a lot of tools for Machine Learning, included a
lot of routines to download and prepare most common Datasets. A wide overview on most useful
tutorials, demos and introduction to most common methods can be found in a dedicate webpage18,
including a Loading popular datasets section. Here, a link to the MNIST loader page 19 is available,
where data and all the informations for the correspondent mnist package installation are provided.
After the installation, data can be loaded by the code in Listing 12.

2 % l o a d i n g data

images_Train = loadMNISTImages ( ’ t r a i n −images −idx 3−uby te ’ ) ;

4 images_Test = loadMNISTImages ( ’ t10k−images −idx 3−uby te ’ ) ;

images = [

images_Train ,

images_Test ] ;

6

% l o a d i n g t a r g e t s

8 l a b e l s _ T r a i n = loadMNISTLabels ( ’ t r a i n −l a b e l s −idx 1−uby te ’ ) ;
l a b e l s _ T e s t = loadMNISTLabels ( ’ t10k−l a b e l s −idx 1−uby te ’ ) ;

10 l a b e l s = [

l a b e l s = f u l l (

l a b e l s _ T r a i n ;
i n d 2 v e c (

l a b e l s _ T e s t ] ;
l a b e l s ’ + 1 )

) ;

12 %%

% i n i t i a l i z a t i o n

14 nn = p a t t e r n n e t ( 300 ) ;

n n . d i v i d e F c n = ’ d i v i d e i n d ’ ;

16 n n . d i v i d e P a r a m . t r a i n I n d = 1 : 45000 ;

n n . d i v i d e P a r a m . v a l I n d = 45001 : 60000 ;
18 n n . d i v i d e P a r a m . t e s t I n d = 60001 : 70000 ;

Listing 12: Loading MNIST data

Data will be loaded as a table named train, where digits are expressed as a numberOfSamples-by-
28-by-28 tensor of type ByteTensor stored in the ﬁeld data, expressing the value of the gray levels
of each pixel between 0 and 255. Targets will be stored as a 1-D vector, expressing the digits labels,
in the ﬁeld label. We have to convert data to the DoubleTensor format and, again, normalize the
input features to have values in [0, 1] and reshape the original 3-D tensor in a 1-D input vector.

18https://github.com/torch/torch7/wiki/Cheatsheet#machine-learning
19https://github.com/andresy/mnist

26

2

4

6

4

6

8

Labels have to be incremented by 1, since CrossEntropyCriterion accepts target indicating the
class avoiding null values (i.e. 1 means a sample to belong to the ﬁrst class and so on). In the last
row we perform a random shuﬄing of data in order to prepare the train/validation splitting by the
function:

f u n c t i o n r n d S h u f f l e ( d a t a s e t )

−− random data s h u f f l e
l o c a l n = d a t a s e t . d a t a : s i z e ( 1 )
l o c a l perm = t o r c h . L o n g T e n s o r ( ) : randperm ( n )
d a t a s e t . d a t a = d a t a s e t . d a t a : i n d e x ( 1 , perm )
d a t a s e t . l a b e l = d a t a s e t . l a b e l : i n d e x ( 1 , perm )
r e t u r n d a t a s e t

8 end

At this point we can create a validation set from the last quarter of the training data by:

−− s p l i t t i n g f u n c t i o n d e f i n i t i o n

2 f u n c t i o n s p l i t D a t a ( data , p )

−− s p l i t s data depending on t h e r a t e p
l o c a l p = p > 0 and p <= 1 and p o r 0 . 9
l o c a l
r e t u r n data : narrow ( 1 , 1 , t r a i n S i z e ) , data : narrow ( 1 , 1 + t r a i n S i z e , data : s i z e ( 1 ) −

t r a i n S i z e = t o r c h . f l o o r ( p∗ data : s i z e ( 1 ) )

t r a i n S i z e )

end

v a l i d a t i o n = {}
10 t r a i n R a t e = 0 . 7 5

t r a i n . d a t a , v a l i d a t i o n . d a t a = s p l i t D a t a ( t r a i n . d a t a ,
12 t r a i n . l a b e l , v a l i d a t i o n . l a b e l = s p l i t D a t a ( t r a i n . l a b e l ,

t r a i n R a t e )

t r a i n R a t e )

The code to build the proposed 2-layer ANN model is reported in Listing 13, where the network
is assembled in the Sequential container, using this time the ReLU as activation for the hidden
layer, whereas output and penalty functions are the same used in the previous section (softmax and
Cross-Entropy respectively).

−− mlpwork d e f i n i t i o n
’ nn ’

2 r e q u i r e

4 l o c a l mlp = n n . S e q u e n t i a l ( )

mlp : add ( n n . L i n e a r ( 7 8 4 , 3 0 0 ) )

6 mlp : add ( nn.ReLU ( ) )

mlp : add ( n n . L i n e a r ( 3 0 0 , 1 0 ) )

8 mlp : add ( nn.SoftMax ( ) )

10 −− l o s s

f u n c t i o n

l o c a l c = n n . C r o s s E n t r o p y C r i t e r i o n ( )

Listing 13: Network deﬁnition for MNIST data

The network training can be deﬁned in a way similar to the one proposed in Listing 6. Because of
the width of the training data, this time is more convenient to set up a minibatch training as showed
in Listing 14. Moreover, we also deﬁne an early stopping criterion which stops the training when

27

the penalty on the validation set start to increase, preventing overﬁtting problems. The training
function expects as inputs the network (named mlp), the criterion to evaluate the loss (named
criterion), training and validation data (named trainset and validation respectively) organized as a
table with ﬁelds data and label as deﬁned in Listing 12. An optional conﬁguration table options can
be provided, indicating the number of training epochs (nepochs), the learning rate (learning_rate),
the mini-batch size (batchSize) and the number of consecutive increasings in the validation loss
which causes a preventive training stop (maxSteps). It is worth a remark on the function split,
deﬁned for the Tensor class, used to divide data in batches stored in an indexed table. At the end
of the training, a vector containing the loss evaluated at each epoch is returned. The validation loss
is computed with the help of the function evaluate, which splits again the computation in smaller
batches, preventing from too heavy computations when the number of parameters and samples is
very large.

1 f u n c t i o n t r a i n i n g ( mlp ,
−− m i n i b a t c h t r a i n i n g

c r i t e r i o n ,

t r a i n s e t , v a l i d a t i o n , o p t i o n s )

3 a s s e r t ( mlp and t r a i n s e t , "At l e a s t 2 arguments a r e e x p e c t e d " )

l o c a l nepochs = o p t i o n s and o p t i o n s . n e p o c h s o r 1000 −− max number o f epoch

5 l o c a l

l e a r n i n g _ r a t e = o p t i o n s and o p t i o n s . l e a r n i n g _ r a t e o r 0 . 0 1

l o c a l b a t c h S i z e = o p t i o n s and o p t i o n s . b a t c h S i z e o r 32

7 l o c a l maxSteps = o p t i o n s and o p t i o n s . m a x S t e p s o r 10 −− max v a l i d a t i o n f a i l s

l o c a l

input ,

t a r g e t = t r a i n s e t . d a t a ,

t r a i n s e t . l a b e l

9 −− v e c t o r

f o r

s a v i n g l o s s d u r i n g epoch

l o c a l

l o s s = t o r c h . T e n s o r ( nepochs ) : typeAs ( i n p u t ) : f i l l ( 0 )

11 l o c a l v a l L o s s = t o r c h . T e n s o r ( nepochs ) : typeAs ( i n p u t ) : f i l l ( 0 )

−− m i n i b a t c h e s

s p l i t t i n g

13 l o c a l m i n i b a t c h e s , m i n i t a r g e t = i n p u t : s p l i t ( b a t c h S i z e ) ,

t a r g e t : s p l i t ( b a t c h S i z e )

−− l a s t

l o s s and v a l i d a t i o n f a i l s

f o r

e a r l y s t o p p i n g c r i t e r i o n

l o s s [ epoch ] = l o s s [ epoch ] + c r i t e r i o n : f o r w a r d ( mlp : f o r w a r d ( batch ) , m i n i t a r g e t [ b i ] )

15 −− based on v a l i d a t i o n l o s s
l a s t L o s s , s t e p = 0 , −1

l o c a l

17 l o c a l epoch = 1

w h i l e epoch < nepochs and s t e p < maxSteps do

19 f o r bi , batch i n p a i r s ( m i n i b a t c h e s ) do

21 −− r e s e t g r a d i e n t s

t o n u l l v a l u e s

mlp : zeroGradParame ter s ( )
23 −− accumulate g r a d i e n t s
mlp : backward ( batch ,
25 −− update p a r a m e t e r s

c r i t e r i o n : backward ( mlp.output , m i n i t a r g e t [ b i ] ) )

mlp : updateParameters ( l e a r n i n g _ r a t e )

27 i f b i %100 == 0 then c o l l e c t g a r b a g e ( ) end −− c l e a n i n g n i l v a r i a b l e

end

29 −− e v a l u a t i n g v a l i d a t i o n l o s s
c u r r L o s s = e v a l u a t e ( mlp ,

l o c a l

31 v a l L o s s [ epoch ] = c u r r L o s s

i f

c u r r L o s s >= l a s t L o s s ∗ 0 . 9 9 9 9 then

c r i t e r i o n , v a l i d a t i o n )

33 s t e p = s t e p + 1

35 s t e p = 0

e l s e

end

37 l a s t L o s s = c u r r L o s s

39 epoch = epoch + 1

end

41 i f

s t e p == maxStep then

x l u a . p r o g r e s s ( epoch , nepochs ) −− p r i n t i n g e p o c h s p r o g r e s s

28

p r i n t ( ’ T r a i n i n g s t o p p e d a t Epoch :

’ . . e p o c h . .

43 ’ b e c a u s e o f V a l i d a t i o n E r r o r

i n c r e a s i n g i n t h e l a s t

’

. . m a x S t e p . . ’ e p o c h s ’ )

45 end

47 end

r e t u r n l o s s : narrow ( 1 , 1 , epoch −1)/#m i n i b a t c h e s , v a l L o s s : narrow ( 1 , 1 , epoch −1)

49 f u n c t i o n e v a l u a t e ( mlp ,
−− e v a l u a t e t h e l o s s

c r i t e r i o n , d a t a s e t , o p t i o n s )

from c r i t e r i o n between mpl p r e d i c t i o n s

51 −− by a c c u m u l a t i n g w i t h i n m i n i b a t c h e s from d a t a s e t

a s s e r t ( mlp and d a t a s e t , "At l e a s t 2 arguments a r e e x p e c t e d " )

53 l o c a l b a t c h S i z e = o p t i o n s and o p t i o n s . b a t c h S i z e o r 32

t a r g e t = d a t a s e t . d a t a , d a t a s e t . l a b e l

l o c a l
55 l o c a l

input ,
l o s s = 0

l o c a l m i n i b a t c h e s , m i n i t a r g e t = i n p u t : s p l i t ( b a t c h S i z e ) ,

t a r g e t : s p l i t ( b a t c h S i z e )

57 f o r bi , batch i n p a i r s ( m i n i b a t c h e s ) do

l o s s = l o s s + c r i t e r i o n : f o r w a r d ( mlp : f o r w a r d ( batch ) , m i n i t a r g e t [ b i ] )

mlp : zeroGradParame ter s ( )
61 r e t u r n l o s s /#m i n i b a t c h e s

59 end

end

Listing 14: Sample of a mini-batch training function

In Listing 15 we show how to compute the Confusion Matrix and the Classiﬁcation Accuracy on
data by the function confusionMtx, taking in input the network (mlp), data (dataset) and the
expected number of classes (nclasses).

f u n c t i o n c o n f u s i o n M t x ( mlp , d a t a s e t , n c l a s s e s )

2 l o c a l

input ,

t a r g e t = d a t a s e t . d a t a , d a t a s e t . l a b e l

l o c a l confMtx = t o r c h . z e r o s ( n c l a s s e s , n c l a s s e s ) : typeAs ( i n p u t )

4

l o c a l p r e d i c t i o n = mlp : f o r w a r d ( i n p u t )

6 −− g e t

t h e p o s i t i o n o f

t h e u n i t with t h e maximum v a l u e

l o c a l _, pos = t o r c h . m a x ( p r e d i c t i o n , 2 )

8 l o c a l a c c = 0

f o r

i = 1 , n c l a s s e s do

10 l o c a l p r e d i c t e d = t o r c h . e q ( pos , i ) : typeAs ( i n p u t )

f o r
12 l o c a l

j = 1 , n c l a s s e s do

t r u t h = t o r c h . e q ( t a r g e t ,
confMtx [ i ] [ j ] = t o r c h . c m u l ( p r e d i c t e d ,

j ) : typeAs ( i n p u t )

t r u t h ) : sum ( 1 )

14 i f

i == j

then a c c = a c c + confMtx [ i ] [ j ] end

end
16 end

18 end

r e t u r n confMtx , a c c / i n p u t : s i z e ( 1 )

Listing 15: Evaluating Confusion Matrix and Accuracy

At this point we can start a trial by the following code:

o p t i o n s = { nepochs = 2 5 0 , b a t c h S i z e = 6 4 ,

l e a r n i n g _ r a t e = 0 . 05 , maxStep = 50}

2 t r a i n i n g ( mlp , c ,

t r a i n , v a l i d a t i o n , o p t i o n s )

confMtx , a c c = c o n f u s i o n M t x ( mlp ,

t e s t , 1 0 )

29

In this case the training is stopped by the validation criterion after epoch 117, producing a Clas-
siﬁcation Accuracy on test of about 97%. In Figure 6(a) we report the trend of the error during
training. Since in general can be useful to visualize the confusion matrix (which in this case is al-
most diagonal), in Figure 6(b) we show the one obtained by the function imagesc from the package
gnuplot, which just give a color map of the matrix passed as input.

Figure 6: Confusion matrix on the MNIST test set.

4.3 MNIST on Tensor Flow

Even TensorFlow environment makes available many tutorials and preloaded dataset, including the
MNIST. A very fast introductive section for beginners can be found at the oﬃcial web page20.
However, we will show some functions to download and import the dataset in a very simple way.
Indeed, data can be directly loaded by:

1 from t e n s o r f l o w . ex amples . t u t o r i a l s . mnist

import

input_data

mnist = input_data . read_data_sets ( "MNIST_data/ " , one_hot=True )

We ﬁrstly deﬁne two auxiliary functions. The ﬁrst (init_weights) will be used to initialize the
parameters of the model, whereas the second (mlp_output) to compute the predictions of the model.

d e f

i n i t _ w e i g h t s ( shape ) :

2 r e t u r n t f . V a r i a b l e ( t f . random_uniform ( shape , −0.1 , 0 . 1 ) )

4 d e f mlp_output (X, W_h, W_o, b_h , b_o ) :

6 a1 = t f . matmul (X, W_h) + b_h
o1 = t f . nn . r e l u ( a1 ) #output

l a y e r 1

8

20https://www.tensorflow.org/versions/r0.12/tutorials/mnist/beginners/index.html

30

a2 = t f . matmul ( o1 , W_o) + b_o
10 o2 = t f . nn . softmax ( a2 ) #output

l a y e r 2

12 r e t u r n o2

Now, with the help of the proposed function init_weights, we deﬁne the parameters to be learned
during the computation. W 1 and b1 represent respectively the weights and the biases of the hidden
layer. Similarly, W 2 and b2 are respectively the weights and the biases of the output layer.

W1 = i n i t _ w e i g h t s ( [ x_dim , h_layer_dim ] )

2 b1 = i n i t _ w e i g h t s ( [ h_layer_dim ] )

W2 = i n i t _ w e i g h t s ( [ h_layer_dim , y_dim ] )

4 b2 = i n i t _ w e i g h t s ( [ y_dim ] )

Once we deﬁned the weights, we can symbolically compose our model by the calling of our function
mlp_output. As in the XOR case, we have to deﬁne a placeholder storing the input samples.

2 x = t f . p l a c e h o l d e r ( t f . f l o a t 3 2 ,

[ None , x_dim ] )

# Input

# P r e d i c t i o n

4 y = mlp_output ( x , W1, W2, b1 , b2 )

Then we need to deﬁne a cost function and an optimizer. However, this time we add the square of the
Euclidean Norm as regularizer, and the global cost function is composed by the Cross-Entropy plus
the regularization term scaled by a coeﬃcient of 10−4. At the beginning of the session, TensorFlow
moves the Data Flow Graph to the CPUs or GPUs and initializes all variables.

# Model S p e c i f i c a t i o n s

2 LEARNING_RATE = 0 . 5

EPOCHS = 5000

4 MINI_BATCH_SIZE = 50
# Sy mbolic v a r i a b l e

6 y_ = t f . p l a c e h o l d e r ( t f . f l o a t 3 2 ,

[ None , y_dim ] )

# Loss

f u n c t i o n and o p t i m i z e r

f o r

t h e t a r g e t

8 c r o s s _ e n t r o p y = t f . reduce_mean ( t f . nn . s o f t m a x _ c r o s s _ e n t r o p y _ w i t h _ l o g i t s ( y , y_ ) )

r e g u l a r i z a t i o n = ( t f . reduce_sum ( t f . s q u a r e (W1) ,

[ 0 , 1 ] )

10

+ t f . reduce_sum ( t f . s q u a r e (W2) ,

[ 0 , 1 ] )

)

c o s t = c r o s s _ e n t r o p y + 10 ∗ ∗−4 ∗ r e g u l a r i z a t i o n

12 t r a i n _ s t e p = t f . t r a i n . G r a d i e n t D e s c e n t O p t i m i z e r (LEARNING_RATE) . minimize ( c o s t )

# S t a r t

s e s s i o n and i n i t i a l i z a t i o n

14 s e s s = t f . S e s s i o n ( )

s e s s . run ( t f . i n i t i a l i z e _ a l l _ v a r i a b l e s ( ) )

TensorFlow provides the function next_batch for the mnist class to randomly extract batches of
a speciﬁed size. Data are split in shuﬄed partitions of the indicated size and, by means of an
implicit counter, the function slide along batches at each calling allowing a fast implementation
for mini-batch Gradient Descent method. In our case, we used a for loop to scan across batches,
executing a training step on the extracted data at each iteration. Once the whole Training set

31

has been processed, the loss on Training, Validation and Test sets is computed. These operations
are repeated in a while loop, whose steps representing the epochs of training. The loop stops
when the maximum number of epochs is reached or the network start to overﬁt Training data.
The early stopping is implemented by checking the Validation error and training is stopped when
no improvements are obtained for a ﬁxed number of consecutive epochs (val_max_steps). The
maximum number of epochs and learning rate must be set in advance.

1 # Save v a l u e s

t o be p l o t t e d

e r r o r s _ t r a i n = [ ]

3 e r r o r s _ t e s t = [ ]
e r r o r s _ v a l = [ ]

5

# E a r l y s t o p p i n g ( i n i t v a r i a b l e s )

7 p r e c _ e r r = 10 ∗ ∗ 6 # j u s t a v e r y b i g v a l u e

v al_count = 0

9 val_max_steps = 5

11 # T r a i n i n g

BATCH_SIZE = np . shape ( mnist . t r a i n . images ) [ 0 ]

13 MINI_BATCH_SIZE = 50

15 i = 1
w h i l e

i <= e p o c h s and v al_count < val_max_steps :

17

19

21

25

27

29

31

35

37

39

41

43

45

47

# Train o v e r
f o r

j

i n r a n g e (BATCH_SIZE/MINI_BATCH_SIZE ) :

t h e f u l l batch i s p e r f o r m e d with mini−b a t c h e s a l g o r i t h m

batch_xs , batch_ys = mnist . t r a i n . next_batch ( 5 0 )
s e s s . run ( t r a i n _ s t e p ,

f e e d _ d i c t ={x : batch_xs , y_ : batch_ys } )

23 # Compute e r r o r on v a l i d a t i o n s e t and c o n t r o l

f o r e a r l y −s t o p p i n g

f e e d _ d i c t ={x : mnist . v a l i d a t i o n . images , y_ : mnist . v a l i d a t i o n . l a b e l s } )

c u r r _ e r r = s e s s . run ( c r o s s _ e n t r o p y ,

i f

c u r r _ e r r >= p r e c _ e r r ∗ 0 . 9 9 9 9 :

v al_count = v al_count + 1

e l s e :

v al_count = 0

p r e c _ e r r = c u r r _ e r r

33 # Save v a l u e s

f o r p l o t

e r r o r s _ v a l . append ( c u r r _ e r r )
c _ t e s t = s e s s . run ( c r o s s _ e n t r o p y ,

e r r o r s _ t e s t . append ( c _ t e s t )
c _ t r a i n = s e s s . run ( c r o s s _ e n t r o p y ,

e r r o r s _ t r a i n . append ( c _ t r a i n )

f e e d _ d i c t ={x : mnist . t e s t . images , y_ : mnist . t e s t . l a b e l s } )

f e e d _ d i c t ={x : mnist . t r a i n . images , y_ : mnist . t r a i n . l a b e l s } )

i n f o about

t h e c u r r e n t epoch

# P r i n t
p r i n t " \n\nEPOCH: " , i , " / " , e p o c h s
p r i n t " TRAIN ERR: " , c _ t r a i n
p r i n t " VALIDATION ERR: " , c u r r _ e r r
p r i n t " TEST ERR: " , c _ t e s t
p r i n t " \n ( E a r l y s t o p p i n g c r i t e r i o n : " , val_count , " / " , val_max_steps , " ) "

32

49 # I n c r e m e n t epochs−i n d e x

i = i +1

The prediction accuracy of the trained model can be evaluated over the test set in way similar to the
one presented for the XOR problem. This time we need to exploit the argmax function to convert
the one-hot encoding in the correspondent labels.

# Sy mbolic f o r m u l a s

2 c o r r e c t _ p r e d i c t i o n = t f . e q u a l ( t f . argmax ( y , 1 ) ,

t f . argmax (y_ , 1 ) )

a c c u r a c y = t f . reduce_mean ( t f . c a s t ( c o r r e c t _ p r e d i c t i o n ,

t f . f l o a t 3 2 ) )

4 # Compute a c c u r a c y on t h e t e s t

s e t

aa = s e s s . run ( a c c u r a c y ,

f e e d _ d i c t ={x : mnist . t e s t . images , y_ : mnist . t e s t . l a b e l s } )

6 p r i n t " Accuracy : " , aa

The trend of the network performance showed in Figure 7 during training can be obtained by the
following code:

E = r a n g e ( np . shape ( e r r o r s _ t r a i n ) [ 0 ] )

2

l i n e _ t r a i n , = p l t . p l o t (E ,

e r r o r s _ t r a i n )

4 l i n e _ t e s t , = p l t . p l o t (E ,

e r r o r s _ t e s t )

l i n e _ v a l , = p l t . p l o t (E , e r r o r _ v a l )

p l t . y l a b e l ( ’ Cross−Entropy ’ )

8 p l t . x l a b e l ( ’ Epochs ’ )

p l t . show ( )

6 p l t . l e g e n d ( [ l i n e _ t r a i n ,

l i n e _ v a l ,

l i n e _ t e s t ] ,

[ ’ T r a i n i n g ’ ,

’ V a l i d a t i o n ’ ,

’ Test ’ ] )

33

Figure 7: Error trend on the MNIST dataset during the training of a 2-layer ANN with 300 hidden
units.

34

5 Convolutional Neural Networks

Figure 8: General architecture of the CNN model proposed to face the MNIST data.

In this section we introduce the Convolutional Neural Networks (CNNs [7, 6, 8]), an important and
powerful kind of learning architecture widely diﬀused especially for Computer Vision applications.
They currently represent state of the art algorithm for image classiﬁcation tasks and constitute the
main architecture used in Deep Learning. We show how to build and train such a structure within
all the proposed frameworks, exploring the most general functions and setting up few experiments
on MNIST pointing out some important features.

5.1 Matlab

TM

TM

and Statistic and Machine Learning Toolbox

Main function and classes to build and train CNNs with Matlabare contained again in Neural
. Nevertheless, the Parallel
Network Toolbox
becomes necessary too. Moreover, to train the network a CUDA R(cid:13) -enabled
Computing Toolbox
NVIDIA R(cid:13) GPU is required. Again, we do not focus too much on main theoretical properties and
general issues about CNNs but only on main implementation instruments.
The network has to be stored in an Matlabobject of kind Layer, which can be sequentially com-
posed by diﬀerent sub-layers. For each one, we do not list all the available options, which as always
can be explored by the interactive help options from the Command Window21. Most common
convolutional objects can be deﬁned by the functions:

TM

imageInputLayer creates the layer which deals with the original input image, requiring as argument
a vector expressing the size of the input image given by height by width by number of channels;

convolution2dLayer deﬁnes a layer of 2-D convolutional ﬁlters whose size is speciﬁed by the
ﬁrst argument (a real number for a square ﬁlter, a 2-D vector to specify both height and
width), whereas the second argument speciﬁes the total number of ﬁlters; main options are
’Stride’, which indicates the sliding step (default [1, 1] means 1 pixel in both directions),
and ’Padding’ (default [0, 0]), whose have to appear in Name,Value pairs;

reluLayer deﬁnes a layer computing the Rectiﬁer Activation Linear Unit (ReLU) for the ﬁlter

outputs;

21Further documentation is available at the oﬃcial site https://it.mathworks.com/help/nnet/convolutional-neural-networks.html

35

averagePooling2dLayer layer computing a spatial reduction of the input by averaging the values
of the input on each grid of given dimension (default [2, 2], ’Stride’ and ’Padding’ are
options too);

maxPooling2dLayer layer computing a spatial reduction of the input assigning the max value to

each grid of given dimensions (default [2, 2], ’Stride’ and ’Padding’ are options too);

fullyConnectedLayer requires the desired output dimension as argument and instantiates a classic

fully connected linear layer, the number of the connections is adapted to ﬁt the input size;

dropoutLayer executes a dropout units selection with the probability given as argument;

softmaxLayer computes a probability normalization based on the softmax function;

classificationLayer adds the ﬁnal classiﬁcation layer evaluating the Cross-Entropy loss function

between predictions and labels

1 CnnM = [

imageInputLay er ( [ 2 8 28 1 ] ) ,

. . . % i n p u t

l a y e r

. . . % c o n v o l u t i o n a l

l a y e r 12 f i l t e r s o f

s i z e 5 x5 and 2 p i x e l s 0−padding

3 c o n v o l u t i o n 2 d L a y e r ( 5 , 1 2 , ’ Padding ’ , 2 ) ,

. . .

r e l u L a y e r ( ) ,

. . .

5 . . . % max−p o o l i n g l a y e r on 2 x2 g r i d and 2 p i x e l s

s t e p i n both d i r e c t i o n s

maxPooling2dLayer ( 2 , ’ S t r i d e ’ , 2 ) ,

. . .

7 . . . % c o n v o l u t i o n a l

l a y e r with 16 f i l t e r s o f

s i z e 3 x3 and 1 p i x e l s 0−padding

c o n v o l u t i o n 2 d L a y e r ( 3 , 1 6 , ’ Padding ’ , 1 ) ,

. . .

9 r e l u L a y e r ( ) ,

. . .

. . . % max−p o o l i n g l a y e r on 2 x2 g r i d and 2 p i x e l s

s t e p i n both d i r e c t i o n s

11 maxPooling2dLayer ( 2 , ’ S t r i d e ’ , 2 ) ,

. . .

. . . % c l a s s i c

f u l l y c o n n e c t e d l a y e r with 256 output

13 f u l l y C o n n e c t e d L a y e r ( 2 5 6 ) ,

. . .

r e l u L a y e r ( ) ,
15 . . . % c l a s s i c

. . .

f u l l y C o n n e c t e d L a y e r ( 1 0 ) ,

. . .

17 softmax Lay er ( ) ,

. . .

c l a s s i f i c a t i o n L a y e r ( )

] ;

f u l l y c o n n e c t e d l a y e r with 10 output

Listing 16: Deﬁnition of a CNN to face the MNIST within Matlab framework.

In Listing 16 we show how to set up a basic CNN to face the 28 × 28 pixels images from MNIST
showed in Figure 8. The global structure of the network is deﬁned as a vector composed by the
diﬀerent modules. The initialized object Layer, named CnnM, can be visualized from the command
window giving:

36

Data for the MNIST can be loaded by the Stanford routines showed in Section 4.1. This time input
images are required as a 4-D tensor of size numberOfSamples–by–channels–by–height –by–width and,
hence, we have to modify the provided function loadMNISTimages or just to reshape data, as showed
in the ﬁrst line of the following code:

X = r e s h a p e ( images_Train , 2 8 , 2 8 , 1 , [ ] )

2 Y = nominal ( l a b e l s _ T r a i n , { ’ 0 ’ , ’ 1 ’ , ’ 2 ’ , ’ 3 ’ , ’ 4 ’ , ’ 5 ’ , ’ 6 ’ , ’ 7 ’ , ’ 8 ’ , ’ 9 ’ } ) ;

The second command is used to convert targets in a categorical Matlabvariable of kind nominal,
which is required to train the network exploiting the function trainNetwork.
It also require
as input an object specifying the training options which can be instantiated by the function
trainingOptions. The command:

o p t s = t r a i n i n g O p t i o n s ( ’ sgdm ’ ) ;

selects (by the ’sgdm’ string) the Stochastic Gradient Descent algorithm using momentum. Many
optional parameters are available, which can be set by additional parameter in the Name,Value
pairs notation again. The most common are:

Momentum (default 0.9)

InitialLearnRate (default 0.01)

L2Regularization (default 0.0001)

MaxEpochs (default 30)

MiniBatchSize (default 128)

After this conﬁguration, the training can be started by the command:

1 [ t r a i n e d N e t , trainOp ] = t r a i n N e t w o r k (X, Y, CnnM, o p t s )

where trainedNet will contain the trained network and trainOp the training variables. Training starts a
command line printing of some useful variables indicating the training state, which will be similar to:

At the end of the training, we can evaluate the performance on a suitable test set Xtest, together with its
correspondent target Ytest, by the function classify:

1 P r e d i c t i o n s = c l a s s i f y ( t r a i n e d N e t , X t e s t ) ;

Accuracy = mean ( P r e d i c t i o n s==Y t e s t ) ;

3 C = c o n f u s i o n m a t (P , c a t e g o r i c a l (Y) ) ;

37

Assuming Ytest to be a 1-Dimensional vector of class labels, classiﬁcation accuracy can be calculated as
before by the meaning of the boolean comparing with the computed predictions vector Predictions. An
useful built-in function to compute the confusion matrix is provided, requiring the nominal labels to be
converted into categorical as the predictions. In this setting, the ﬁnal classiﬁcation accuracy on the test
set should be close to the 99%.

When dealing with CNNs, an important new type of object introduced are the convolutional ﬁlters. We
do not want to go in deep with theoretical explanations, however sometimes it could be useful to visualize
the composition of the convolutional ﬁlters in order to get an idea of which kind of features each ﬁlter
detects. Filters are represented by the weights of each convolution, stored in each layers in the 4-D weights
tensor of size height –by–width–by–numberOfChannels–by–numberOfFilters.
In our case for example, the
ﬁlters of the ﬁrst convolutional be accessed by the notation CnnM.Layer(2).Weights. In Figure 9, we show
their conﬁguration after the training (again exploiting the function image and the colormap gray and a
normalization in [0, 255] for a suitable visualization).

Figure 9: First Convolutional Layer ﬁlters of dimension 5 × 5 after the training on the MNIST
images with Matlab.

5.2 Torch

Within the Torch environment is straightforward to deﬁne a CNN from the nn package presented in Sec-
tion 2.2.2. Indeed, the network can be stored in a container and composed by speciﬁc modules. Again, we
give a short list description of the most common ones, which can be integrated with the standard transfer
functions or with a standard linear (fully connected) layer introduced before:

SpatialConvolution deﬁnes a convolutional layers, the required arguments are the number of input chan-
nels, the number of output channels, the height and the width of the ﬁlters. The step-size and
zero-padding height and width are optional parameters (with default value 1 and 0 respectively)

SpatialMaxPooling standard max pooling layer, requiring as inputs height and width of the pooling win-

dow, whereas the step-sizes are optional parameters (default the same as the window size)

SpatialAveragePooling standard average pooling layer, same features of the previous one

SpatialDropout set a dropout layer taking as optional argument the deactivating rate (default 0.5)

Reshape is a module which is usually used to unroll the output after a convolutional/pooling process as a
1-D vector to be feed to a linear layer, takes as input the size of the desired output dimensions

The assembly of the network follows from what seen until now. To have a diﬀerent comparison with the
previous experiment, we operate an initial 2 × 2 window max-pooling on the input image, in order to provide

38

the network by images of lower resolution. The general architecture will diﬀer from the one deﬁned in
Section 5.1 only by the ﬁrst layers. The proposed network is generated by the code in Listing 17, whereas
in Figure 10 we show the global architecture.

Figure 10: General architecture of the CNN model proposed to face the MNIST data within the
Torch environment (Section 5.2).

1 r e q u i r e

’ nn ’

−− c o n t a i n e r d e f i n i t i o n
3 c n n e t = n n . S e q u e n t i a l ( )

−− network assembly

c n n e t : add ( nn.ReLU ( ) )

5 c n n e t : add ( n n . S p a t i a l C o n v o l u t i o n ( 1 , 1 2 , 5 , 5 , 1 , 1 , 2 , 2 ) )

7 c n n e t : add ( n n . S p a t i a l M a x P o o l i n g ( 2 , 2 , 2 , 2 ) )

c n n e t : add ( n n . S p a t i a l C o n v o l u t i o n ( 1 2 , 1 6 , 3 , 3 , 1 , 1 , 1 , 1 ) )

9 c n n e t : add ( nn.ReLU ( ) )

c n n e t : add ( nn.R eshape ( 7 ∗ 7 ∗ 1 6 ) )
11 c n n e t : add ( n n . L i n e a r ( 7 ∗ 7 ∗ 1 6 , 2 5 6 ) )

c n n e t : add ( nn.ReLU ( ) )

13 c n n e t : add ( n n . L i n e a r ( 2 5 6 , 1 0 ) )

c n n e t : add ( nn.SoftMax ( ) )

Listing 17: Network deﬁnition for MNIST data reduced to 14×14 pixels images by 2×2 max-pooling

If we use the training function deﬁned in Listing 14, the optimization (starting with the same options) stops
for validation check after 123 epochs, producing a Classiﬁcation Accuracy of about 90%. This just to give an
idea of the diﬀerence in the obtained performances when there is a reduction in the information expressed
by input images. In Figure 11 we show the 12 ﬁlters of size 5 × 5 extracted by the ﬁrst convolutional layer.
The weights can be obtained by the function parameters from the package nn:

myParam = c n n e t : p a r a m e t e r s ( )

which return an indexed table storing the weights of each layer. In this case, the ﬁrst element of the table
contains a tensor of dimension 12 by 25 representing the weights of the ﬁlters. The visualization can be
generated exploiting the function imagesc from the package gnuplot, after reshaping each line in the 5 × 5
format.

39

Figure 11: First Convolutional Layer ﬁlters of dimension 5 × 5 after the training with Torch on the
MNIST images halved by 2 × 2 max pooling.

5.3 Tensor Flow

In this section we will show how to build a CNN to face the MNIST data using TensorFlow. The ﬁrst step
is to import libraries, the Mnist Dataset and to deﬁne main variables. Two additional package are required:
numpy for matrices computations and matplotlib.pyplot for visualization issues.

1 import

t e n s o r f l o w a s

t f

import numpy a s np

3 import m a t p l o t l i b . p y p l o t a s p l t

5 from t e n s o r f l o w . ex amples . t u t o r i a l s . mnist

import

input_data

mnist = input_data . read_data_sets ( ’ MNIST_data ’ , one_hot=True )

x = t f . p l a c e h o l d e r ( t f . f l o a t 3 2 ,
9 y_ = t f . p l a c e h o l d e r ( t f . f l o a t 3 2 ,

shape =[None , 7 8 4 ] )
shape =[None , 1 0 ] )

We deﬁne now some tool functions to specify variable initialization.

1 d e f w e i g h t _ v a r i a b l e ( shape ) :

i n i t i a l = t f . truncated_normal ( shape ,
r e t u r n t f . V a r i a b l e ( i n i t i a l )

s t d d e v =0.1)

5 d e f b i a s _ v a r i a b l e ( shape ) :

i n i t i a l = t f . c o n s t a n t ( 0 . 1 ,
r e t u r n t f . V a r i a b l e ( i n i t i a l )

shape=shape )

The following two function deﬁne convolution and (3-by-3) max pooling. The vector strides speciﬁes how
the ﬁlter or the sliding window move along each dimension. The vector ksize speciﬁes the dimension of the
sliding window. The padding option ’SAME’ automatically adds empty (zero valued) pixels to allow the
convolution to be centered even in the boundary pixels.

1 d e f conv2d ( x , W) :

r e t u r n t f . nn . conv2d ( x , W,

s t r i d e s =[1 , 1 , 1 , 1 ] , padding= ’SAME ’ )

# max p o o l i n g o v e r 3 x3 b l o c k s

40

7

3

7

3

5 d e f max_pool_3x3 ( x ) :

r e t u r n t f . nn . max_pool ( x , k s i z e =[1 , 3 , 3 , 1 ] ,
s t r i d e s =[1 , 3 , 3 , 1 ] , padding= ’SAME ’ )

7

In order to deﬁne the model, we start by reshaping the input (where each sample is provided as 1-D vector)
to its original size, i.e. each sample is represented by a matrix of 28x28 pixels. Then we deﬁne the ﬁrst
convolution layer which computes 12 features by using 5x5 ﬁlters. Finally we perform the ReLU activation
and the ﬁrst max pooling step.

1 # Input

r e s i z e
x_image = t f . r e s h a p e ( x ,

[ − 1 , 2 8 , 2 8 , 1 ] )

# F i r s t

c o n v o l u t i o n l a y e r − 5 x5 f i l t e r s

5 INPUT_C1 = 1 # i n p u t c h a n n e l

OUTPUT_C1 = 12 # output c h a n n e l

( f e a t u r e s )

7 W_conv1 = w e i g h t _ v a r i a b l e ( [ 5 , 5 , INPUT_C1 , OUTPUT_C1 ] )

b_conv1 = b i a s _ v a r i a b l e ( [OUTPUT_C1 ] )

#c o n v o l u t i o n s t e p

11 h_conv1 = t f . nn . r e l u ( conv2d ( x_image , W_conv1) + b_conv1 )

13 #max p o o l i n g s t e p

h_pool1 = max_pool_3x3 ( h_conv1 )

The second convolution layer can be built up in an analogous way.

# Second c o n v o l u t i o n l a y e r

2 INPUT_C2 = OUTPUT_C1

OUTPUT_C2 = 16 # output c h a n n e l

( f e a t u r e s )

4 W_conv2 = w e i g h t _ v a r i a b l e ( [ 5 , 5 , INPUT_C2 , OUTPUT_C2 ] )

b_conv2 = b i a s _ v a r i a b l e ( [OUTPUT_C2 ] )

#c o n v o l u t i o n s t e p

8 h_conv2 = t f . nn . r e l u ( conv2d ( h_pool1 , W_conv2) + b_conv2 )

10 #max p o o l i n g s t e p

h_pool2 = max_pool_3x3 ( h_conv2 )

3

9

6

5

At this point the network returns 16 feature maps 4x4. These will be reshaped to 1–D vectors and given
as input to the last fully connected linear layer. The linear layer is equipped with 1024 hidden units with
ReLU activation functions.

1 # D e f i n i t i o n o f
FS = 4 # f i n a l

s i z e

t h e f u l l y c o n n e c t e d l i n e a r

l a y e r

3 W_fc1 = w e i g h t _ v a r i a b l e ( [ FS ∗ FS ∗ OUTPUT_C2, 1 0 2 4 ] )

b_fc1 = b i a s _ v a r i a b l e ( [ 1 0 2 4 ] )

# Reshape images

7 h _ p o o l 2 _ f l a t = t f . r e s h a p e ( h_pool2 ,

[ −1 , FS ∗ FS ∗ OUTPUT_C2 ] )

41

11

15

5

13

17

25

27

29

31

33

35

9 # hidden l a y e r

h_fc1 = t f . nn . r e l u ( t f . matmul ( h_pool2_flat , W_fc1 ) + b_fc1 )

# Output

l a y e r

13 W_fc2 = w e i g h t _ v a r i a b l e ( [ 1 0 2 4 , 1 0 ] )

b_fc2 = b i a s _ v a r i a b l e ( [ 1 0 ] )

# P r e d i c t i o n

17 y_conv = t f . matmul ( h_fc1 , W_fc2 ) + b_fc2

In the following piece of code we report the optimization process of the deﬁned CNN. As in the standard
ANN case, it is organized in a for loop. This time, we chose the Adam gradient-based optimization by the
function AdamOptimizer. Each epoch performs a training step over mini-batches extracted again by the
dedicated function next_batch(), introduced in Section 4.3. This time the computations are run within
InteractiveSession. The diﬀerence with the regular Session is that an InteractiveSession sets itself as the
default session during building, allowing to run variables without needing to constantly refer to the session
object. As a for instance, the method eval() will implicitly use that session to run operations.

1 c r o s s _ e n t r o p y = t f . reduce_mean ( t f . nn . s o f t m a x _ c r o s s _ e n t r o p y _ w i t h _ l o g i t s ( y_conv , y_) )

t r a i n _ s t e p = t f . t r a i n . AdamOptimizer ( 1 e −4). minimize ( c r o s s _ e n t r o p y )

3 c o r r e c t _ p r e d i c t i o n = t f . e q u a l ( t f . argmax ( y_conv , 1 ) ,

t f . argmax (y_ , 1 ) )

a c c u r a c y = t f . reduce_mean ( t f . c a s t ( c o r r e c t _ p r e d i c t i o n ,

t f . f l o a t 3 2 ) )

s e s s = t f . I n t e r a c t i v e S e s s i o n ( )

7 s e s s . run ( t f . g l o b a l _ v a r i a b l e s _ i n i t i a l i z e r ( ) )

9 # E a r l y s t o p p i n g setup ,

t o check on v a l i d a t i o n s e t

p r e c _ e r r = 10 ∗ ∗ 6 # j u s t a v e r y b i g v a l u e

11 v al_count = 0

val_max_steps = 6

e p o c h s = 100

15 b a t c h _ s i z e = 1000

num_of_batches = 60000/ b a t c h _ s i z e

i =1
19 w h i l e

i <= e p o c h s and v al_count < val_max_steps :

21

p r i n t

’ Epoch : ’ , i , ’ ( E a r l y s t o p p i n g c r i t e r i o n : ’ , val_count , ’ / ’ , val_max_steps , ’ ) ’

23 # t r a i n i n g s t e p
j

f o r

i n r a n g e ( num_of_batches ) :

batch = mnist . t r a i n . next_batch ( b a t c h _ s i z e )
s e s s . run ( t r a i n _ s t e p ,

f e e d _ d i c t ={x : batch [ 0 ] , y_ : batch [ 1 ] } )

# v i s u a l i z e a c c u r a c y each 10 e p o c h s
i %10 == 0 :
i f
t r a i n _ a c c u r a c y = a c c u r a c y . e v a l (

i == 1 o r

f e e d _ d i c t ={x : mnist . t r a i n . images , y_ : mnist . t r a i n . l a b e l s } )

t e s t _ a c c u r a c y = a c c u r a c y . e v a l (

f e e d _ d i c t ={x : mnist . t e s t . images , y_ : mnist . t e s t . l a b e l s } )

p r i n t " \ nAccuracy a t epoch " ,
p r i n t ( " t r a i n a c c u r a c y %g ,

i , " : "

t e s t a c c u r a c y %g \n"%( t r a i n _ a c c u r a c y ,

t e s t _ a c c u r a c y ) )

42

f e e d _ d i c t ={x : mnist . v a l i d a t i o n . images , y_ : mnist . v a l i d a t i o n . l a b e l s } )

37 # v a l i d a t i o n check

c u r r _ e r r = s e s s . run ( c r o s s _ e n t r o p y ,

i f

c u r r _ e r r >= p r e c _ e r r ∗ 0 . 9 9 9 9 :

v al_count = v al_count + 1

39

41

43

45

47

v al_count = 0

p r e c _ e r r = c u r r _ e r r

e l s e :

i+=1

49 p r i n t ( " \n\ n R e s u l t : \ nTest a c c u r a c y %g " % a c c u r a c y . e v a l (

f e e d _ d i c t ={x : mnist . t e s t . images , y_ : mnist . t e s t . l a b e l s } ) )

In this setting, the ﬁnal classiﬁcation accuracy on the test set should be close to the 99%. As in the previous
Sections, in Figure 12 we show the learned ﬁlters of the ﬁrst convolutional layer obtained by:

# G e t t i n g f i l t e r s a s an a r r a y

2 FILTERS = W_conv1 . e v a l ( )

4 f i g = p l t . f i g u r e ( )

f o r

i

i n r a n g e ( np . shape (FILTERS ) [ 3 ] ) :

6

ax = f i g . add_subplot ( 2 , 6 ,
ax . matshow (FILTERS [ : , : , 0 , i ] , cmap= ’ gray ’ )

i +1)

8 p l t . show ( )

In the ﬁrst line eval() computes the current value of W_conv1, saving it in the 4–D numpy array
FILTERS, where the fourth dimension (acceded by the index 3 in np.shape(FILTERS)[3]) corresponds to
the number of ﬁlters.

Figure 12: First Convolutional Layer ﬁlters of dimension 5 × 5 after the training with TensorFlow
on the MNIST images with the described arcitecture.

43

6 A critical comparison

In this Section we would like to outline an overall picture across the presented environments. Even if in
Table 1 we provide a scoring based on some features we thought mainly relevant for Machine Learning
software development, this work would not like to bound this analysis to a poor evaluation. Instead, we
hope to propose an useful guideline to help people trying to approach ANNs and Machine Learning in general,
in order to orientate within the environments depending on personal background and requirements. More
complete and statistically relevant comparisons can be found on the web22, but we try to summarize so as
to help and speed up single and global task developing.

We ﬁrst give general description of each environment, then we try to compare pros and cons on speciﬁc
requirements. At the end, we carry out an indicative numerical analysis on the computational performances
on diﬀerent tasks, which could be also a topic for comparison and discussion.

6.1 Matlab

The programming language is intuitive and the software provides a complete package, allowing the user to
deﬁne and train almost all kind of ANNs architecture without writing a single line of speciﬁc code. The code
parallelization is automatic and the integration with CUDA R(cid:13) is straightforward too. The available built-
in functions are very customizable and optimized, providing fast and extended setting up of experiments
and an easy access to the variable of the network for in-depth analysis. However, enlarging and integrating
Matlabtools require an advanced knowledge of the environment. This could drive the user to start rewriting
its own code from the beginning, leading to a general decay of computational performances. These features
make it perfect as a statistical and analysis toolbox, but maybe a bit slow as developmental environment. The
GUI results sometimes heavy to be handled by the calculator, but, on the other hand, it is very user-friendly
and provides the best graphical data visualization. The documentation is complete and well organized within
the oﬃcial site.

6.2 Torch

The programming language (Lua) can sometimes results a little bit tricky, but it supposed to be the faster
among these languages. It provides all the needed CUDA R(cid:13) integrations and the CPU parallelization au-
tomatic. The module-based structure allows ﬂexibility in the ANNs architecture and it is relatively easy
to extend the provided packages. There are also other powerful packages23, but in general they require to
acquire some expertise to achieve a conscious handling. Torch could be easily used as a prototyping envi-
ronment for speciﬁc and generic algorithms testing. The documentation is spread all over the torch GitHub
repository and sometimes solve speciﬁc issues could not be immediate.

6.3 Tensor Flow

The employment of a programming language as dynamic as Python makes the code scripting light for the
user. The CPU parallelization is automatic, and, exploiting the graph-structure of the computation is easy
to take advantage of GPU computing. It provides a good data visualization and the possibility for beginners
to access to ready to go packages, even if not treated in this document. The power of symbolic computation
involves the user only in the forward step, whereas the backward step is entirely derived by the TensorFlow
environment. This ﬂexibility allows a very fast development for users from any level of expertise.

22Look for example at the webpage http://hammerprinciple.com/therighttool
23We skip the treatment of optim, which provides various Gradient Descent and Back-Propagation procedure

44

6.4 An overall picture on the comparison

As already said, in Table 1 we try to sum up a global comparison trying assigning a score from 1 to 5 on
diﬀerent perspectives. Here below, we explain the main motivation when necessary:

Programming Language All the basic language are quite intuitive.
GPU Integration Matlabis penalized since an extra toolbox is required.

CPU Parallelization All the environments exploit as more core as possible
Function Customizability Matlabscore is lower since integrate well-optimized functions with the pro-

vided ones is diﬃcult

Symbolic Calculus Not expected in Lua

Network Structure Customizability Every kind of network is possible
Data Visualization The interactive Matlabmode outperforms the others
Installation Quite simple for all of them, but the Matlabinteractive GUI is an extra point

OS Compatibility Torch installation is not easy on Windows
Built-In Function Availability Matlabprovided simple-tools with an easy access
Language Performance Matlabinterface can sometimes appear heavy
Development Flexibility Again, Matlabis penalized because it forces medium users to become very
specialized with the language to integrate the provided tools or to write proper code, which in general
can slow down the software development

Programming Language
GPU Integration
CPU Parallelization
Function Customizability
Symbolic Calculus
Network Structure Customizability
Data Visualization
Installation
OS Compatibility
Built-In Function Availability
Language Performance
Development Flexibility
License

Matlab Torch
4
3
5
2
3
5
5
5
5
5
3
2
EULA

3
5
5
4
1
5
2
4
4
4
5
4
BSD (Open source) Apache 2.0 (Open source)

TensorFlow
4
5
5
5
5
5
3
4
5
4
4
5

Table 1: Environments individual scoring

45

6.5 Computational issues

In Table 2 we compare running times for diﬀerent tasks, analyzing the advantages and diﬀerences of
CPU/GPU computing. Results are averaged on 5 trials, carried out in the same machine with an In-
tel(R) Xeon(R) CPU E5-2650 v2 @ 2.60GHz with 32 cores, 66 GB of RAM, and a Geforce GTX 960 with
4GB of memory. The OS is Debian GNU/Linux 8 (jessie). We test a standard Gradient Descent proce-
dure varying the network architecture, the batches size (among Stochastic Gradient Descent (SGD), 1000
samples batch and Full Batch) and the hardware (indicated in HW column). The CNN architecture is the
same of the one proposed in Figure 8. Performances are obtained trying to use optimization procedures as
similar as possible. In practice, it is very diﬃcult to reply the speciﬁc optimization techniques exploited in
Matlabbuilt-in toolboxes. We skip the SGD case for the second architecture (eighth row) in Torch because
of the huge computational time obtained for the ﬁrst architecture. We miss the SGD case for the ANNs
architecture in the Matlabcase since the training function ’trains’ it is not supported for GPU computing
(rows fourth and tenth). As a matter of fact, this could be an uncommon case of study, but we report the
results for best completeness. We skip the CNN Full Batch trials on GPU because of the too large memory
requirement24 .

an

24For
packages)
(including
https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software

comparison
with

computational

on
other

comparison

exhaustive

existent

the

performance

on

several

the

user

can

refer

tasks
to

46

Architecture

Batches size

SGD

1000

Full Batch

Env.

HW

2-Layers ANN

914.56

1000 HUs

4-Layers ANN

893.27

300-300-300 HUs

52.46

3481.00

378.19

911.32

44.66

–

–

–

517.87

1024.29

30.40

46.55

13.82

3.75

1.93

5.40

28.33

52.99

10.95

2.74

1.51

4.73

CNN

7794.33

54.21

647.75

100.06

1850.30

20.22

28.38

24.43

12.00

3.73

1.46

4.92

27.07

20.00

8.83

3.44

1.08

4.27

–

–

–

Matlab

Torch

TF

CPU

GPU

CPU

GPU

GPU

Table 2: Averaged time (in seconds) on 5 running of 10 epochs training with diﬀerent architectures
on the MNIST data within the presented environments. All the architectures are built up using the
ReLU as activation, the softmax as output function and the Cross-Entropy penalty.

47

References

[1] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.

[2] David E Rumelhart, Geoﬀrey E Hinton, and Ronald J Williams. Learning representations by back-

propagating errors. Cognitive modeling, 5(3):1, 1988.

[3] F. Giannini, V. Laveglia, A. Rossi, D. Zanca, and A. Zugarini. NeuralNetworksForBeginners.

https://github.com/AILabUSiena/NeuralNetworksForBeginners, 2016.

[4] The Mathworks, Inc., Natick, Massachusetts. MATLAB version 8.5.0.197613 (R2015a), 2015.

[5] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Cor-
rado, Andy Davis, Jeﬀrey Dean, Matthieu Devin, et al. Tensorﬂow: Large-scale machine learning on
heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.

[6] Y. Lecun, L. Bottou, Y. Bengio, and P. Haﬀner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE, 86(11):2278–2324, Nov 1998.

[7] Kunihiko Fukushima. Neocognitron: A hierarchical neural network capable of visual pattern recognition.

Neural networks, 1(2):119–130, 1988.

[8] Thomas Serre, Lior Wolf, Stanley Bileschi, Maximilian Riesenhuber, and Tomaso Poggio. Robust object
recognition with cortex-like mechanisms. IEEE transactions on pattern analysis and machine intelligence,
29(3), 2007.

48

7
1
0
2
 
r
a

M
 
6
1
 
 
]

G
L
.
s
c
[
 
 
2
v
8
9
2
5
0
.
3
0
7
1
:
v
i
X
r
a

Neural Networks for Beginners
A fast implementation in Matlab, Torch, TensorFlow

F. Giannini1, V. Laveglia1,2, A. Rossi1,3∗, D. Zanca1,2, A. Zugarini1

1DIISM, University of Siena, Siena, Italy
2DINFO, University of Florence, Florence, Italy
3Fondazione Bruno Kessler, Trento, Italy

rossi111@unisi.it
{giannini7, andrea.zugarini}@student.unisi.it
{vincenzo.laveglia, dario.zanca}@unifi.it

March 17, 2017

What is this report about?

This report provides an introduction to some Machine Learning tools within the most common
development environments. It mainly focuses on practical problems, skipping any theoretical intro-
duction. It is oriented to both students trying to approach Machine Learning and experts looking
for new frameworks.

The dissertation is about Artiﬁcial Neural Networks (ANNs [1, 2]), since currently is the most
trend topic, achieving state of the art performance in many Artiﬁcial Intelligence tasks. After a ﬁrst
individual introduction to each framework, the setting up of general practical problems is carried
out simultaneously, in order to make the comparison easier.

Since the treated argument is widely studied and in continuos and fast growing, we pair this
document with an on-line documentation available at the Lab GitHub repository [3] which is more
dynamic and we hope to be kept updated and possibly enlarged.

∗Corresponding Author

1

Contents

1 Matlab: a uniﬁed friendly environment

1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Setting up the XOR experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Stopping criterions and Regularization . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4 Drawing separation surfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Torch and Lua environment

2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Getting started . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.1 Lua . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.2 Torch enviroment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Setting up the XOR experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Stopping criterions and Regularization . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5 Drawing Separation Surfaces

3 TensorFlow

3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Getting started . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.1 Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.2 TensorFlow environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Installation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.3
3.3 Setting up the XOR experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 MNIST Handwritten Characters Recognition

4.1 MNIST on Matlab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 MNIST on Torch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3 MNIST on Tensor Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Convolutional Neural Networks

5.1 Matlab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Torch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3 Tensor Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 A critical comparison

6.1 Matlab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.2 Torch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.3 Tensor Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.4 An overall picture on the comparison . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.5 Computational issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3
3
4
7
7

10
10
10
10
10
12
14
15

18
18
18
18
18
19
19

23
23
26
30

35
35
38
40

44
44
44
44
45
46

2

1 Matlab: a uniﬁed friendly environment

1.1 Introduction

Matlab R(cid:13) [4] is a very powerful instrument allowing an easy and fast handling of almost every kind
of numerical operation, algorithm, programming and testing. The intuitive and friendly interactive
interface makes it easy to manipulate, visualize and analyze data. The software provides a lot
of mathematical built-in functions for every kind of task and an extensive and easily accessible
documentation. It is mainly designed to handle matrices and, hence, almost all the functions and
operations are vectorized, i.e. they can manage scalars, as well as vectors, matrices and (often)
tensors. For these reasons, it is more eﬃcient to avoid loops cycles (when possible) and to set up
operations exploiting matrices multiplication.

TM

and the Neural Network Toolbox

In this document we just show some simple Machine Learning related instruments in order to
start playing with ANNs. We assume a basic-level knowledge and address to oﬃcial documentation
for further informations. For instance, you can ﬁnd informations on how to obtain the software
from the oﬃcial web site1. Indeed, the license is not for free and even if most universities provide a
classroom license for students use, maybe could not be possible to access to all the current packages.
In particular the Statistic and Machine Learning Toolbox
provide a lot of built-in functions and models to implement diﬀerent ANNs architectures suitable
to face every kind of task. The access to both the tools is fundamental in the prosecution, even if
we refer to some simple independent examples. The most easy to-go is the nnstart function, which
activates a simple GUI guiding the user trough the deﬁnition of a simple 2-layer architecture. It
allows either to load available data samples or to work with customize data (i.e. two matrices of input
data and correspondent target), train the network and analyze the results (Error trend, Confusion
Matrix, ROC, etc.). However, more functions are available for speciﬁc tasks. For instance, the
function patternnet is speciﬁcally designed for pattern recognition problems, newfit is suitable for
regression, whereas feedforwardnet is the most ﬂexible one and allows to build very customized and
complicated networks. All the versions are implemented in a similar way and the main options and
methods apply to everyone. In the next section we show how to manage customizable architectures
starting to face very basic problems. Detailed informations can be ﬁnd in a dedicated section of the
oﬃcial site2.

TM

CUDA R(cid:13) computing

TM

GPU computing in Matlabrequires the Parallel Computing Toolbox
and the CUDA R(cid:13) instal-
lation on the machine. Detailed informations on how to use, check and set GPUs devices can be
found in GPU computing oﬃcial web page3, where issues on Distributed Computing CPUs/GPUs
are introduced too. However, basic operations with graphical cards should in general be quite sim-
ple. Data can be moved to the GPU hardware by the function gpuArray, then back to the CPU
by the function gather. When dealing with ANNs, a dedicated function nndata2gpu is provided,
organizing tensors (representing a dataset) in a eﬃcient conﬁguration on the GPU, in order to speed
up the computation. An alternative way is to carry out just the training process in the GPU by
the correspondent option of the function train (which will be describe in details later). This can

1https://ch.mathworks.com/products/matlab.html?s_tid=hp_products_matlab
2http://ch.mathworks.com/help/nnet/getting-started-with-neural-network-toolbox.html
3https://ch.mathworks.com/help/nnet/ug/neural-networks-with-parallel-and-gpu-computing.html

3

be done directly by passing additional arguments, in the Name,Values pair notation, the option
’useGPU’ and the value ’yes’:

1

nn = t r a i n ( nn ,

. . .

,

’ useGPU ’ , ’ y e s ’ )

1.2 Setting up the XOR experiment

The XOR is a well-known classiﬁcation problem, very simple and eﬀective in order to understand
the basic properties of many Machine Learning algorithms. Even if writing down an eﬃcient and
ﬂexible architecture requires some language expertise, a very elementary implementation can be
found in the Matlabsection of the GitHub repository4 of this document. It is not suitable to face
real tasks, since no customizations (except for the number of hidden units) are allowed, but can
be useful just to give some general tips to design a personal module. The code we present is basic
and can be easily improved, but we try to keep it simple just to understand fundamental steps. As
we stressed above, we avoid loops exploiting the Matlabeﬃciency with matrix operations, both in
forward and backward steps. This is a key point and it can substantially aﬀects the running time
for large data.

Initialization

TM

Here below, we will see how to deﬁne and train more eﬃcient architectures exploiting some built-in
functions from the Neural Network Toolbox
. Since we face the XOR classiﬁcation problem, we
sort out our experiments by using the function patternnet. To start, we have to declare an object
of kind network by the selected function, which contains variables and methods to carry out the
optimization process. The function expects two optional arguments, representing the number of
hidden units (and then of the hidden layers) and the back-propagation algorithm to be exploited
during the training phase. The number of hidden units has to be provided as a single integer number,
expressing the size of the hidden layer, or as an integer row vector, whose elements indicate the size
of the correspondent hidden layers. The command:

1

nn = p a t t e r n n e t ( 3 )

creates on object named nn of kind network, representing a 2-layer ANN with 3 units in the
single hidden layer. The object has several options, which can be reached by the dot notation
object.property or explore by clicking on the interactively visualization of the object in the Mat-
labCommand Window, which allows to see all the available options for each property too. The
second optional parameter selects the training algorithm by a string saved in the trainFcn property,
which in the default case takes the value ’trainscg’ (Scaled Conjugate Gradient Descent methods).
The network object is still not fully deﬁned, since some variables will be adapted to ﬁt the data
dimension at the calling of the function train. However, the function configure, taking as input
the object and the data of the problem to be faced, allows to complete the network and set up the
options before the optimization starts.

4Available at https://github.com/AILabUSiena/NeuralNetworksForBeginners/tree/master/matlab/2layer.

4

Dataset

Data for ANNs training (as well as for others available Machine Learning methods) must be provided
in matrix form, storing each sample column-wise. For example data to deﬁne the XOR problem can
be simply deﬁned via an input matrix X and a target matrix Y as:

1

X = [ 0 , 0 , 1 , 1 ; 0 , 1 , 0 , 1 ]
Y = [ 0 , 1 , 1 , 0 ]

Matlabexpects targets to be provided in 0/1 form (other values will be rounded). For 2-class
problem targets can be provided as a row vector of the same length of the number of samples. For
multi-class problem (and as an alternative for 2-class problem too) targets can be provided in the
one-hot encoding form, i.e. as a matrix with as many columns as the number of samples, each one
composed by all 0 with only a 1 in the position indicating the class.

Conﬁguration

nn = c o n f i g u r e ( nn , X,Y)

Once we have deﬁned data, the network can be fully deﬁned and designed by the command:

For each layer, an object of kind nnetLayer is created and stored in a cell array under the ﬁeld
layers of the network object. The number of connections (the weights of the network) for each
units corresponds to the layer input dimension. The options of each layer can be reached by the
dot notation object. layer{numberOf Layer}.property. The ﬁeld initFcn contains the weights
initialization methods. The activation function is stored in the transferFcn property.
In the
hidden layers the default values is the ’tansig’ (Hyperbolic Tangent Sigmoid), whereas the output
layers has the ’logsig’ (Logistic Sigmoid) or the ’sof tmax’ for 1-dimensional and multi-dimensional
target respectively. The ’crossentropy’ penalty function is set by default in the ﬁeld performFcn.
At this point, the global architecture of the network can be visualized by the command:

1

v iew ( nn )

Training

The function train itself makes available many options (as for instance useParallel and useGPU
for heavy computations) directly accessible from its interactive help window. However, it can take
as input just the network object, the input and the target matrices. The optimization starts by
dividing data in Training, Validation and Test sets. The splitting ratio can be changed by the options
divideParam. In the default setting, data are randomly divided, but if you want for example to
decide which data are used for test, you can change the way the data are distributed by the option
divideFcn5. In this case, because of the small size of the dataset, we drop validation and test by
setting:

5Click on divideFcn property from the MatlabCommand Window visualization of your object to see the available

methods.

5

1

nn . d i v i d e F c n = ’ ’

In the following code, we set the training function to the classic gradient descent method ’traingd’,
we deactivate the training interactive GUI by nn.trainParam.showWindow (boolean) and activate
the printing of the training state in the Command Window by nn.trainParam.showCommandLine
(boolean). Also the learning rate is part of the trainParam options under the ﬁelds lr.

1

3

nn . t r a i n F c n = ’ t r a i n g d ’
nn . trainParam . showWindow = 0
nn . trainParam . showCommandLine = 1
nn . trainParam . l r = 0 . 0 1

Training starts by the calling:

[ nn ,

t r ] = t r a i n ( nn , X, Y)

this generates a printing, ending in this case with:

This indicates that the training stops after the max number of epoch is reached (which can be set by
options object. trainParam.epochs). Each column shows the state of one of the stopping criterions
used, which we will analyze in details in the next section. The output variable tr stores the training
options. The ﬁelds perf, vperf and tperf contain the performance of the network evaluated at
each epoch on the Training, Validation and Test sets respectively (the last two are NaN in this case),
which can be used for example to plot performances. If we pass data organized in a single matrix,
the function will exploit the full batch learning method accumulating gradients overall the training
set. To set a mini-batch mode, data have to be manually split in sub-matrix with the same number
of column and organized in a cell array. However, let us consider for a moment a general data set
composed by N samples in the features space RD with a target of dimension C, so that X ∈ RD×N
and Y ∈ RC×N . All the mini-batches have to be of the same size b, so that it is in general convenient
to choose the batch size to be a factor of N . In this case, we can generate data for the training
function organizing the input and target in the correspondent cell-array by:

1 N = s i z e (X, 2 ) ; % number o f

s a m p l e s

n_batch = N/ b a t c h s i z e ; % number o f b a t c h e s

3

i n p u t { n_batch } = [ ] ; % i n p u t c e l l −a r r a y i n i t i a l i z a t i o n
5 t a r g e t { n_batch} = [ ] ; % t a r g e t c e l l −a r r a y i n i t i a l i z a t i o n

7 p = randperm (N) ; % g e n e r a t i n g a random permutated i n d e x f o r data s h u f f l i n g

X = X( : , p ) ; % s a m p l e s p e r m u t a t i o n

9 Y = Y( : , p ) ; % t a r g e t permutaion

6

11 f o r

i =1: n_batch

i n p u t { i } = X ( : , ( 1 : b a t c h s i z e ) +( i −1)∗ b a t c h s i z e ) ;
t a r g e t { i } = Y ( : , ( 1 : b a t c h s i z e ) +( i −1)∗ b a t c h s i z e ) ;

13

end

However, in order to perform a pure Stochastic Gradient Descent optimization, in which the ANNs
parameters are updated for each sample, the training function ’trains’ have to be employed
skipping to split data previously. A remark has to be done since this particular function does not
support the GPU computing.

The network (trained or not) can be easily evaluated on data by passing the input data as
argument to a function named as the network object. Performance of the prediction with respect to
the targets can be evaluated by the function perform according to the correspondent loss function
option object. performFcn:

f = nn (X)
perform ( nn , Y,

f )

2

1.3 Stopping criterions and Regularization

TM

Early stopping is a well known procedure in Machine Learning to avoid overﬁtting and improve
generalization. Routines from Neural Network Toolbox
use diﬀerent kind of stopping criterions
regulated by the network object options in the ﬁeld trainParam. Arbitrarily methods are based on
the number of epochs (epochs) and the training time (time, default Inf ). A criterion based on
the training set check the loss (object. trainParam.goal, default = 0) or the parameters gradients
(object. trainParam.min_grad, default = 10−6) to reach a minimum threshold. A general early
stopping method is implemented by checking the error on the validation set and interrupting training
when validation error does not improve for a number of consecutive epochs given by max_fail
(default = 6).

Further regularization methods can be conﬁgured by the property performParam of the network
object. The ﬁeld regularization contains the weight (real number in [0, 1]) balancing the contri-
bution of a term trying to minimizing the norm of the network weights versus the satisfaction of the
penalty function. However, the network is designed to mainly rely on the validation checks, indeed
regularization applies only to few kind of penalties and the default weight is 0.

1.4 Drawing separation surfaces

When dealing with low dimensional data (as in the XOR case), can be useful to visualize the
prediction of the network directly in the input space. For this kind of task, Matlabmakes available
a lot of built-in functions with many options for interactive data visualization. In this section, we
will show the main functions useful to realize customized separation surfaces learned by an ANN
with respect to some speciﬁc experiments. We brieﬂy add some comments for each instruction,
referring to the suite help for speciﬁc knowledge of each single function. The network predictions
will be evaluated on a grid of the input space, generated by the Matlabfunction meshgrid, since
the main functions used for the plotting (contour or, if you want a color surface pcolor) require

7

as input three matrices of the same dimensions expressing, in each correspondent element, the
coordinates of a 3-D point (which in our case will be ﬁrst input dimension, second input dimension
and prediction). Once we trained the network described until now, the boundary for the 2-classes
separation showed in Figure 1a is generated by the code in Listing 1, whereas in Figure 1b we
report the same evaluation after the training of a 4-layers network using 5, 3 and 2 units in the ﬁrst,
second and third hidden layers respectively, each one using the ReLU as activation (’poslin’ in
Matlab). This new network can be deﬁned by:

n = p a t t e r n n e t ( [ 5 , 3 , 2 ] ) ; % 3+output

l a y e r s network i n i t i a l i z a t i o n

2 nn = c o n f i g u r e ( n , X,Y) ; % network c o n f i g u r a t i o n

nn . t r a i n F c n = ’ t r a i n g d ’ ; % s e t t i n g o p t i m i z a t i o n f u n c t i o n

4 nn . div ideParam . t r a i n R a t i o = 1 ; % s e t t i n g data s p l i t t i n g r a t i o s

( i l l u s t r a t i v e )

nn . div ideParam . v a l R a t i o = 0 ;
6 nn . div ideParam . t e s t R a t i o = 0 ;

nn . trainParam . showCommandLine = 1 ;

8 nn . trainParam . l r = 0 . 0 1 ;

10 nn . l a y e r s { 2 } . t r a n s f e r F c n = ’ p o s l i n ’ ;
nn . l a y e r s { 3 } . t r a n s f e r F c n = ’ p o s l i n ’ ;

nn . l a y e r s { 1 } . t r a n s f e r F c n = ’ p o s l i n ’ ; % s e t t i n g t h e a c t i v a t i o n l a y e r −w i s e

Listing 1: Drawing separation surfaces

Separation Surfaces

Separation Surfaces

Classes Bound
Class 0
Class 1

Classes Bound
Class 0
Class 1

1.5

2

1

0

2

x

0.5

X2

X1

1.5

2

1

0

2

x

0.5

X2

X1

X4

X3

X4

X3

-0.5

-0.5

0

0.5

1

1.5

2

-0.5

-0.5

0

0.5

1

1.5

2

x

1

(a) 2-layers, 3 Hidden Units

x

1

(b) 4-layers, 5, 3 and 2 Hidden Units, ReLU for
all the activations

Figure 1: Separation surfaces on the XOR classiﬁcation task.

5 % network e v a l u a t i o n on t h e g r i d d i n g ( r e s h a p e d t o f i t network i n p u t d i m e n s i o n )

1 % % % % P l o t t i n g S e p a r a t i o n S u r f a c e

3 % g e n e r a t i n g i n p u t s p a c e g r i d

[ xp1 , xp2 ] = meshgrid ( − 0 . 5 : . 0 1 : 2 , − 0 . 5 : . 0 1 : 2 ) ;

f = nn ( [ xp1 ( : ) ’ ; xp2 ( : ) ’ ] ) ;

7 % r e s h a p i n g p r e d i c t i o n i n c o r r e s p o n d e n t matrix form

f = r e s h a p e ( f , s i z e ( xp1 , 1 ) , [ ] ) ;

8

% drawing s e p a r a t i o n s u r f a c e s

11 c o n t o u r ( xp1 , xp2 , f , [ . 5 , . 5 ] , ’ LineWidth ’ , 3 , ’ C o l o r ’ , ’ c ’ ) ;

h o l d on ;

% drawing data p o i n t s

15 s c a t t e r (X( 1 , [ 1 , 4 ] ) ,X( 2 , [ 1 , 4 ] ) , 2 0 0 , ’ o ’ , ’ f i l l e d ’ , ’ MarkerEdgeColor ’ , ’ k ’ , . . .

17 s c a t t e r (X( 1 , [ 2 , 3 ] ) ,X( 2 , [ 2 , 3 ] ) , 2 0 0 , ’ ^ ’ , ’ f i l l e d ’ , ’ MarkerEdgeColor ’ , ’ k ’ , . . .

’ Mark erFaceColor ’ , ’ k ’ , ’ LineWidth ’ , 2 ) ;

’ Mark erFaceColor ’ , ’ k ’ , ’ LineWidth ’ , 2 ) ;

a x i s ( [ − 0 . 5 , 2 , − 0 . 5 , 2 ] ) ; % s e t t i n g a x i s bounds

% l a b e l i n g data p o i n t s

23 c = { ’X^1 ’ , ’X^2 ’ , ’X^3 ’ , ’X^4 ’ } ; % l a b e l s

dx = [ − . 1 5 , −.15 ,

. 1 ,

. 1 ] ; % l a b e l s h o r i z o n t a l

t r a n s l a t i o n wrt p o i n t s

25 dy = [ − . 1 ,

. 1 , −.1 ,
t e x t (X( 1 , : )+dx , X( 2 , : )+dy , c ,

. 1 ] ; % l a b e l s v e r t i c a l

t r a n s l a t i o n wrt p o i n t s
’ F o n t S i z e ’ , 1 4 ) ; % showing l a b e l s a s

t e x t

9

13

19

21

27

% p l o t

l a b e l s

29 x l a b e l ( ’ x_1 ’ , ’ F o n t S i z e ’ , 1 4 )
y l a b e l ( ’ x_2 ’ , ’ F o n t S i z e ’ , 1 4 )

31 t i t l e ( ’ S e p a r a t i o n S u r f a c e s ’ , ’ F o n t S i z e ’ , 1 6 ) ;

33 h = l e g e n d ( { ’ C l a s s e s Bound ’ , ’ C l a s s 0 ’ , ’ C l a s s 1 ’ } , ’ L o c a t i o n ’ , ’ NorthEast ’ ) ;

35 s e t ( h , ’ F o n t S i z e ’ , 1 4 ) ;

9

2 Torch and Lua environment

2.1 Introduction

Torch7 is an easy to use and eﬃcient scientiﬁc computing framework, essentially oriented to Machine
Learning algorithms. The package is written in C which guarantees an high eﬃciency. However, a
completely interaction is possible (and usually convenient) by the LuaJIT interface, which provides
a fast and intuitively scripting language. Moreover, it contains all the libraries necessary for the
integration with the CUDA R(cid:13) environment for GPU computing. At the moment of writing it is
one of the most used tool for prototyping ANNs of any kind of topology. Indeed, there are many
packages, constantly updated and improved by a large community, allowing to develop almost any
kind of architectures in a very simple way.

Informations about the installation can be found at the getting started section of the oﬃcial
site6. The procedure is straightforward for UNIX based operative systems, whereas is not oﬃcially
supported for Windows, even if an alternative way is provided7. If CUDA R(cid:13) is already installed, also
the packages cutorch and cunn will be added automatically, containing all the necessary utilities
to deal with Nvidia GPUs.

2.2 Getting started

2.2.1 Lua

Lua, in torch7, acts as an interface for C/CUDA routines. A programmer, in most of the cases, will
not have to worry about C functions. Therefore, we explain here only how the Lua language works,
because is the only one necessary to deal with Torch. It is a scripting language with a syntax similar
to Python and semantic close to Javascript. A variable is considered as global by default. The local
declaring, which is usually recommended, require the explicit declaration by placing the keyword
local before the name fo the variable. Lua has been chosen over other scripting languages, such
as Python, because is the fastest one, a crucial feature when dealing with large data and complex
programs, as common in Machine Learning.

There are seven native types in lua: nil, boolean, number, string, userdata, function
and table, even if most of the Lua power is related to the last one. A table behaves either as
an hash map (general case) or as an array (which have the 1-based indexing as in Matlaband
Python). The table will be considered as an array when contains only numerical keys, starting from
the value 1. Any other complex structure such as classes, are built from it (formally deﬁned as a
Metatable).

A detailed documentation on Lua can be ﬁnd at the oﬃcial webpage8, however, an essential and

fast introduction can be found at http://tylerneylon.com/a/learn-lua/.

2.2.2 Torch enviroment

Torch extends the capabilities of the Lua table implementing the Tensor class. Many Matlab-like
functions9 are provided in order to initialize and manipulate tensors in a concise fashion. Most

6http://torch.ch/docs/getting-started.html
7https://github.com/torch/torch7/wiki/Windows
8Lua 5.1 reference manual is available here: https://www.lua.org/manual/5.1/
9http://atamahjoubfar.github.io/Torch_for_Matlab_users.pdf

10

commons are reported in Listing 2.

1 l o c a l
l o c a l
3 l o c a l

t 1 = t o r c h . T e n s o r ( ) −− no d i m e n s i o n t e n s o r c o n s t r u c t o r
t 2 = t o r c h . T e n s o r ( 4 , 3 ) −− 4 x3 empty t e n s o r
t 3 = t o r c h . e y e ( 3 , 5 ) −− 3 x5 1− d i a g o n a l matrix

t 2 : f i l l ( 1 ) −− f i l l

t h e matrix with t h e v a l u e 1

5 t 1 = torch.mm ( t2 , t 3 ) −− a s s i g n t o t 1 t h e r e s u l t o f matrix m u l t i p l i c a t i o n between t 2

and t 3

t 1 [ 1 ] [ 2 ] = 5 −− a s s i g n 5 t o t h e e l e m e n t

i n f i r s t

row and s e c o n d column

Listing 2: Example of torch tensor basic usages

All the provided packages are developed following a strong modularization, which is a crucial
feature to keep the code stable and dynamic. Each one provides several already built-in function-
alities, and all of them can be easily imported from Lua code. The main one is, of course, torch,
which is installed at the beginning. Not all the packages are included at ﬁrst installation, but it is
easy to add a new one by the shell command:

luarocks install packagename

where luarocks is the package manager, and packagename is the name of the package you want to
install.

The nn package

All (almost) you need to create (almost) any kind of ANNs is contained in the nn package (which is
usually automatically installed). Every element inside the package inherits from the abstract Lua
class nn.Module. The main state variables are output and gradInput, where the result of forward
and backward steps (in back-propagation) will be stored. forward and backward are methods of
such class (which can be accessed by the object:method() notation). They invoke updateOutput
and updateGradInput respectively, that here are abstract and the deﬁnition must be in the derived
classes.

The main advantage of this package is that all the gradients computations in the back-propagation
step are automatically realized thanks to these built-in functions. The only requirement is to call
the forward step before the backward.

The weights of the network will be updated by the method updateGradParameters, assigning
a new value to each parameter of a module (according to the Gradient Descent rule) exploiting the
learning rate passed an argument of the function.

The bricks you can use to construct a network can be divided as follows:

• Simple layers: the common modules to implement a layer. The main is nn.Linear, com-

puting a basic linear transformation.

• Transfer functions: here you can ﬁnd many activation functions, such as nn.Sigmoid or

nn.Tanh

• Criterions: loss functions for supervised tasks, a for instance is nn.MSECriterion

• Containers: abstract modules that allow us to build multi-layered networks. nn.Sequential
connect several layers in a feed-forward manner. nn.Parallel and nn.Concat are important
to build more complex structure, where the input ﬂows in separated architectures. Layers,
activation functions and even criterions can be added inside those containers.

11

For detailed documentation of the nn package we refer to the oﬃcial webpage10. Another useful
package for whom could be interested on building more complex architectures can be found at the
nngraph repository11.

CUDA R(cid:13) computing

Since C++/Cuda programming and integration are not trivial to develop, it is important to have
an interface as simple as possible linking such tools. Torch provides a clean solution for that with
the two dedicated packages cutorch and cunn (requiring, of course, a capable GPU and CUDA R(cid:13)
installed). All the objects can be transferred into the memory of GPUs by the method :cuda()
and then back to the CPU by :double(). Operations are executed on the hardware of the involved
In Listing 3 we show some
objects and are possible only among variables from the same unit.
examples of correct and wrong statements.

l o c a l cpuTensor1 = t o r c h . T e n s o r ( 3 , 3 ) : f i l l ( 1 )
2 l o c a l cpuTensor2 = t o r c h . T e n s o r ( 3 , 3 ) : f i l l ( 2 )

l o c a l cudaTensor1 = t o r c h . T e n s o r ( 3 , 3 ) : f i l l ( 3 ) : cuda ( )
4 l o c a l cudaTensor2 = t o r c h . T e n s o r ( 3 , 3 ) : f i l l ( 4 ) : cuda ( )

6 cpuTensor1 : cmul ( cpuTensor2 ) −− OK

cpuTensor1 : cmul ( cudaTensor2 ) −− WRONG

8 cudaTensor1 : cmul ( cudaTensor2 ) −− OK

cudaTensor1 : cmul ( cpuTensor2 ) −− WRONG

Listing 3: Samples of CUDA operations.

2.3 Setting up the XOR experiment

In order to give a concrete feeling about the presented tools, we show some examples on the classical
XOR problem as in the previous section. The code showed here below can be found in the Torch
section of the document’s GitHub repository12 and can be useful to play with the parameters and
become more familiar with the environment.

Architecture

1 r e q u i r e

’ nn ’

When writing a a script, the ﬁrst command is usually the import of all the necessary packages by
the keyword require. In this case, only the nn toolbox is required:

We deﬁne a standard ANNs with one hidden layer composed by 2 hidden units, the hyperbolic
tangen (tanh) as transfer function and identity as output function. The structure of the network
will be stored in a container where all the necessary modules will be added. A standard feed-forward
architecture can be deﬁned into a Sequential container, which we named mlp. The network can
be then assembled by adding sequentially all the desired modules by the function add():

10https://github.com/torch/nn
11Detailed documentation at https://github.com/torch/nngraph
12Available at https://github.com/AILabUSiena/NeuralNetworksForBeginners/tree/master/torch/xor.

12

1 mlp = n n . S e q u e n t i a l ( ) −− c o n t a i n e r

i n i t i a l i z a t i o n

3 mlp : add ( n n . L i n e a r ( i n p u t s , HUs) ) −− f i r s t

i n p u t s = 2 ; o u t p u t s = 1 ; HUs = 2 ; −− g e n e r a l o p t i o n s
l a y e r
t h e hidden l a y e r
l a y e r

5 mlp : add ( n n . L i n e a r (HUs , o u t p u t s ) ) −− output

mlp : add ( nn.Tanh ( ) ) −− a c t i v a t i o n f o r

l i n e a r

Listing 4: Code to create a 2-layer ANNs

Dataset

The training set will be composed by a tensor of 4 samples (organized again column-wise) paired
with a tensor of targets. Usually, true and false boolean values are respectively associated to 1 and
0. However, just to propose an equivalent but diﬀerent approach, here we shift both values by −0.5,
so they will be in [−0.5, 0.5] as showed in Listing 5. Both input and target are initialized with false
values (a tensor ﬁlled with 0), and then true values are placed according to the XOR truth table.

1 l o c a l d a t a s e t = t o r c h . T e n s o r ( 4 , 2 ) : f i l l ( 0 )

l o c a l

t a r g e t = t o r c h . T e n s o r ( 4 , 1 ) : f i l l ( 0 )

3 d a t a s e t [ 2 ] [ 1 ] = 1 −− True F a l s e
d a t a s e t [ 3 ] [ 2 ] = 1 −− F a l s e True

5 d a t a s e t [ 4 ] [ 1 ] = 1 ; d a t a s e t [ 4 ] [ 2 ] = 1 −− True True

d a t a s e t = d a t a s e t : add(−0 . 5 ) −− s h i f t

t r u e and f a l s e by −0 . 5

7 t a r g e t [ 2 ] [ 1 ] = 1 ;

t a r g e t [ 3 ] [ 1 ] = 1

t a r g e t = t a r g e t : add(−0 . 5 )

Listing 5: creation of 4 examples and their targets

Training

We set up a full–batch mode learning, i.e. we update the parameters after accumulating the gradients
over the whole dataset. We exploit the following function:

forward(input) returns the output of the multi layer perceptron w.r.t the given input; it updates
the input/output states variables of each modules, preparing the network for the backward
step; its output will be immediately passed to the loss function to compute the error.

zeroGradParameters() resets to null values the state of the gradients of the all the parameters.

backward(gradients) actually computes and accumulates (averaging them on the number of sam-
ples) the gradients with respect to the weights of the network, given the data in input and
the gradient of the loss function.

updateParameters(learningrate) modiﬁes the weights according to the Gradient Descent proce-

dure using the learning rate as input argument.

As loss function we use the Mean Square Error, created by the statement:

c r i t e r i o n = nn.MSECriterion ( )

13

When a criterion is forwarded, it returns the error between the two input arguments. It updates
its own modules state variable and gets ready to compute the gradients tensor of the loss in the
backward step, which will be back-propagated through the multilayer perceptron. As a nn modules,
all the possible criterions used the functions forward() and backward() as the others. The whole
training procedure can be set up by:

1 nepochs = 1 0 0 0 ;

l e a r n i n g _ r a t e = 0 . 0 5 ; −− g e n e r a l o p t i o n s

l o c a l

l o s s = t o r c h . T e n s o r ( nepochs ) : f i l l ( 0 ) ; −− t r a i n i n g l o s s

i n i t i a l i z a t i o n

3 f o r

i = 1 , nepochs do −− i t e r a t i n g o v e r 1000 e p o c h s

5

7

9

i n p u t = d a t a s e t

l o c a l
l o c a l o u t p u t s = mlp : f o r w a r d ( i n p u t ) −− network f o r w a r d s t e p
t a r g e t ) −− e r r o r
l o s s [ i ] = c r i t e r i o n : f o r w a r d ( outputs ,
mlp : zeroGradParame te rs ( ) −− z e r o r e s e t o f g r a d i e n t s
l o c a l g r a d i e n t s = c r i t e r i o n : backward ( mlp.output ,
mlp : backward ( input , g r a d i e n t s ) −− network backward s t e p
mlp : updateParameters ( l e a r n i n g _ r a t e ) −− update p a r a m e t e r s g i v e n t h e l e a r n i n g r a t e

t a r g e t ) −− l o s s g r a d i e n t s

e v a l u a t i o n

11 end

Listing 6: training of the network

2.4 Stopping criterions and Regularization

Since the training procedure is manually deﬁned, particular stopping criterion are completely up to
the user. The simplest one, based on the reaching of a ﬁxed number of epochs explicitly depends of
the upper bound of the for cycle. Since other methods are related to the presence of a validation
set, we will deﬁne an example of early stopping criterion in Listing 14 in Section 4. A simple
criterion based on the vanishing of the gradients can be simply set up by exploiting the function
getParameters deﬁned for the modules of nn, which returns all the weights and the gradients of
the network in two 1-Dimensional vector:

1 param , grad = mlp : g e t P a r a m e t e r s ( )

A simple check on the minimum value of the absolute values of gradients saved in grad can be used
to stop the training procedure.

Another regularization method can be accomplished by implementing the weight decay method
as shown in Listing 7. The presented code is intended to be an introductory example even to
understand the class inheritance mechanisms in Lua and Torch.

1 −− d e f i n i n g c l a s s

i n h e r i t a n c e s

l o c a l WeightDecay , p a r e n t = t o r c h . c l a s s ( ’ nn.WeightDecayWrapper ’ ,

’ n n . S e q u e n t i a l ’ )

i s a keyword r e f e r r i n g t o t h e a b s t r a c t o b j e c t

3

5

7

9

11

13

f u n c t i o n WeightDecay : __init ( ) −− c o n s t r u c t o r

p a r e n t . _ _ i n i t ( s e l f )
s e l f . w e i g h t D e c a y = 0 −− s e l f
s e l f . c u r r e n t O u t p u t = 0

end

f u n c t i o n WeightDecay : getWeightDecay ( a l p h a )

l o c a l a l p h a = a l p h a o r 0
l o c a l
f o r

weightDecay = 0
i =1,# s e l f . m o d u l e s do

14

l o c a l params ,_ = s e l f . m o d u l e s [ i ] : p a r a m e t e r s ( )
i f params then

f o r

j =1,#params do

weightDecay = weightDecay + t o r c h . d o t ( params [ j ] , params [ j ] ) ∗ a l p h a /2

end

end

end
s e l f . w e i g h t D e c a y = weightDecay
r e t u r n s e l f . w e i g h t D e c a y

23 end

15

17

19

21

27

29

31

33

end

end

end

35 end

25 f u n c t i o n WeightDecay : updateParameters ( l e a r n i n g R a t e , a l p h a )

l o c a l a l p h a = a l p h a o r 0
i =1,# s e l f . m o d u l e s do
f o r

l o c a l params , gradParams = s e l f . m o d u l e s [ i ] : p a r a m e t e r s ( )
i f params then

f o r

j =1,#params do

params [ j ] : add(− l e a r n i n g R a t e , gradParams [ j ] + ( a l p h a ∗ params [ j ] ) )

Listing 7: Code to implement the weight decay regularization

To implement the weight decay coherently with the nn package, we need to create a novel class, inher-
iting from nn.Sequential, that overloads the method updateParameters() of the nn.Module. We
ﬁrst have to declare a new class name in Torch, where you can optionally specify the parent class. In
our case the new class has been called nn.WeightDecayWrapper, while the class it inherits from is the
Container nn.Sequential. The constructor is deﬁned within the function WeightDecay:__init().
In the scope of this function the variable self is a table used to refer to all the attributes and
methods of the abstract object. The calling of the function __init() from the parent class au-
tomatically add all the original properties. The functions WeigthDecay:getWeigthDecay() and
WeigthDecay:updateParameters() compute respectively the weight decay and its gradient. Both
methods loop over all the modules of the container (the symbol # returns the number-indexed
element of a table) and, for each one that has parameters, use them in order to compute either
the error or the gradients coming from the weight decay contribution. The argument alpha repre-
sent the regularization parameter of the weight decay and, if not provided, is assumed null. It is
also worth to mention the fact that, WeigthDecay:updateParameters() overloads the method that
implemented in nn.Module, updating the parameters according to the standard Gradient Descent
rule. At this point, an ANN expecting a possible weight decay regularization can be declared by
replacing the nn.Sequential container by the proposed nn.WeightDecayWrapper.

2.5 Drawing Separation Surfaces

In this framework, data visualization is allowed by the package gnuplot, which provides some tools
to plot points, lines, curves and so on. For example, in the training procedure presented in Listing 6,
a vector storing the penalty evaluated at each epoch is produced. To have an idea of the state of
the network during training, we can save an image ﬁle containing the trend of the error by the code
in Listing 8, whom output is shown in Figure 2(a).

15

1

3

5

g n u p l o t . e p s f i g u r e ( ’ XOR loss.eps ’ ) −− c r e a t e an e p s
g n u p l o t . p l o t ( { ’ l o s s
f u n c t i o n ’ ,
g n u p l o t . t i t l e ( ’ Loss ’ )
g n u p l o t . g r i d ( t r u e )
g n u p l o t . p l o t f l u s h ( )
g n u p l o t . f i g u r e ( )

t o r c h . r a n g e ( 1 , nepochs ) , l o s s } ) −− l o s s

t r e n d p l o t

f i l e with t h e image

Listing 8: Error evaluated at each training epoch.

Torch does not have dedicated functions to visualize separation surfaces produced on data and,
hence, we generate a random grid across the input space, plotting only those points predicted close
enough (with respect to a a certain threshold) to the half of possible target (0 in this case). The
correspondent result, showed in Figure 2(b), is generated by the code in Listing 9, exploiting the
fact that Lua support the logical indexing as in Matlab.

Loss

Xor

Separation Surface

 0.3

 0.25

 0.2

 0.15

 0.1

 0.05

 0

 0

 100

 200

 300

 400

 500

 600

 700

 800

 900

 1000

-0.5

-0.4

-0.3

-0.2

-0.1

 0

 0.1

 0.2

 0.3

 0.4

 0.5

(a)

(b)

Figure 2: (a) Trend of loss versus the number of epochs. (b) The estimated separation surface
obtained by a 2-Layers ANN composed by 2 Hidden Units, Hyperbolic Tangent as activation and
linear output.

−− i n p u t s p a c e g r i d by n 2D−p o i n t s u n i f o r m l y d i s t r i b u t e d i n [ −0 . 5 , 0 . 5 ]

l o c a l e p s = 2 e −3; −− s e p a r a t i o n t h r e s h o l d
n = 1000000 −− g r i d d i n g s i z e

2 l o c a l

4 l o c a l x = t o r c h . r a n d ( n , 2 ) : add(−0 . 5 )

l o c a l mlpOutput = mlp : f o r w a r d ( x ) −− e v a l u a t i o n

6 −− compare whether t h e a b s o l u t e v a l u e o f

t h e network output

i s

l e s s

e q u a l than e p s

l o c a l mask = t o r c h . l e ( t o r c h . a b s ( mlpOutput ) , mlpOutput : c l o n e ( ) : f i l l ( e p s ) )

8 i f

t o r c h . s u m ( mask ) > 0 then

g n u p l o t . e p s f i g u r e ( ’ S e p a r a t i o n s u r f a c e . e p s ’ )
l o c a l x1 = x : narrow ( 2 , 1 , 1 ) −− r e s h a p i n g f i r s t
i n p u t d i m e n s i o n f o r x a x i s
l o c a l x2 = x : narrow ( 2 , 2 , 1 ) −− r e s h a p i n g s e c i n d i n p u t d i m e n s i o n f o r y a x i s
−− p l o t t i n g o f
g n u p l o t . p l o t ( x1 [ mask ] , x2 [ mask ] ,
g n u p l o t . t i t l e ( ’ S e p a r a t i o n s u r f a c e ’ )

t h e c o l l e c t i o n o f p o i n t s

t h a t match t h e mask

’+ ’ )

10

12

14

 0.5

 0.4

 0.3

 0.2

 0.1

 0

-0.1

-0.2

-0.3

-0.4

-0.5

16

g n u p l o t . g r i d ( t r u e )
g n u p l o t . p l o t f l u s h ( ) −− s a v e t h e image
g n u p l o t . f i g u r e ( )

16

18 end

Listing 9: Drawing separation surfaces in torch

17

3 TensorFlow

3.1 Introduction

TensorFlow [5] is an open source software library for numerical computation and is the youngest with
respect to the others Machine Learning frameworks. It was originally developed by researchers and
engineers from the Google Brain Team, with the purpose of encourage research on deep architectures.
Nevertheless, the environment provides a large set of tools suitable for several domains of numerical
programming. The computation is conceived under the concept of Data Flow Graphs. Nodes in the
graph represent mathematical operations, while the graph edges represent tensors (multidimensional
data arrays). The core of the package is written in C++, but provides a well documented Python
API. The main characteristic is its symbolic approach, which allows a general deﬁnition of a forward
models, leaving the computation of the correspondent derivatives entirely to the environment itself.

3.2 Getting started

3.2.1 Python

A TensorFlow model can be easily written using Python, a very intuitive object-oriented program-
ming language. Python is distributed with an open-source license for commercial use too. It oﬀers
a nice integration with many other programming languages and provides an extended standard
library which includes numpy (modules designed for matrix operations, very similar to the Matlab
syntax). Python runs on Windows, Linux/Unix, Mac OS X and other operative systems.

3.2.2 TensorFlow environment

Assuming that the reader is familiar with Python, here we present the building blocks of TensorFlow
framework:

The Data Flow Graph To leverage the parallel computational power of multi-core CPU, GPU
and even clusters of GPUs, the dynamic of the numerical computations has been conceived as a
directed graph, where each node represents a mathematical operation and the edges describe the
input/output relation between nodes.

Tensor

It is a typed n-dimensional array that ﬂows through the Data Flow Graph.

Variable Symbolic objects designed to represent parameters. They are exploited to compute the
derivatives at a symbolical level, but in general must be explicitly initialized in a session.

Optimizer
It is the component which provides methods to compute gradients from the loss func-
tion and to apply back-propagation through all the variables. A collection is available in TensorFlow
to implement classic optimization algorithms.

Session A graph must be launched in a Session, which places the graph onto CPU or GPU and
provides methods to run computation.

18

3.2.3 Installation

Information about download and installation of Python and TensorFlow are available in the oﬃcial
webpages13. Notice that a dedicated procedure must be followed for GPU installation. It’s worth
a quick remark on the CUDA R(cid:13) versions. Indeed, versions from 7.0 are oﬃcially supported, but the
installation could be not straightforward in versions preceding the 8.0. Moreover, a registration to
the Accelerate Computing Developer Program 14 is required to install the package cuDNN, which is
mandatory to enable GPU support.

3.3 Setting up the XOR experiment

As in the previous sections of this tutorial, we show how to start managing the TensorFlow frame-
work by facing the simple XOR classiﬁcation problem by a standard ANN.

At the beginning, as for every Python library, we need to import the TensorFlow package by:

Import tensor ﬂow

import

t e n s o r f l o w a s

t f

ciao

Dataset deﬁnition

2

3

Again, data can be deﬁned as two matrices containing the input data and its correspondent target,
called X and Y respectively. Data can be deﬁned as a list or numpy array. After they will be used
to ﬁll the placeholder that actually deﬁne a type and dimensionality.

1 X = [ [ 0 , 0 ] , [ 0 , 1 ] , [ 1 , 0 ] , [ 1 , 1 ] ]

Y = [ [ 0 ] , [ 1 ] , [ 1 ] , [ 0 ] ]

Placeholders

TensorFlow provides Placeholders which are symbolic variables representing data during the com-
putation. A Placeholders object have to be initialized with given type and dimensionality, suitable
to represent the desired element. In this case we deﬁne two object x_ and y_ respectively for input
data and target:

x_ = t f . p l a c e h o l d e r ( t f . f l o a t 3 2 ,
2 y_ = t f . p l a c e h o l d e r ( t f . f l o a t 3 2 ,

shape = [ 4 , 2 ] )
shape = [ 4 , 1 ] )

13Python webpage: https://www.python.org/, TensorFlow webpage: https://www.tensorflow.org/
14https://developer.nvidia.com/accelerated-computing-developer

19

The description of the network depends essentially on its architecture and parameters (weights and
biases). Since the parameters have to be estimated, they are deﬁned as the variabile of the model,
whereas the architecture is determined by the conﬁguration of symbolic operations. For a 2-layers
ANN we can deﬁne:

Model deﬁnition

# Hidden u n i t s

2 HU = 3
# 1 s t

l a y e r

4 W1 = t f . V a r i a b l e ( t f . random_uniform ( [ 2 ,HU] , −1.0 , 1 . 0 ) ) # w e i g h t s matrix

b1 = t f . V a r i a b l e ( t f . z e r o s ( [HU] ) )

# b i a s

6 O = t f . nn . s i g m o i d ( t f . matmul (x_, W1) + b1 ) # non−l i n e a r a c t i v a t i o n output

# 2nd l a y e r

8 W2 = t f . V a r i a b l e ( t f . random_uniform ( [ HU, 1 ] , −1.0 , 1 . 0 ) )

b2 = t f . V a r i a b l e ( t f . z e r o s ( [ 1 ] ) )

10 y = t f . nn . s i g m o i d ( t f . matmul (O, W2) + b2 )

The matmul() function performs tensors multiplication. Variable() is the constructor of the class
variable. It needs an initialization value which must be a tensor. The function random_uniform()
returns a tensor of a speciﬁed shape, ﬁlled with valued picked from a uniform distribution between
two speciﬁed values. The nn module contains the most common activation functions, taking as
input a tensor and evaluating the non-linear transferring component-wise (the Logistic Sigmoid is
chosen in the reported example by tf.nn.sigmoid()).

Loss and optimizer deﬁnition

The cost function and the optimizer are deﬁned by the following two lines

# q u a d r a t i c l o s s

f u n c t i o n

2 c o s t = t f . reduce_sum ( t f . s q u a r e (y_ − y ) ,

r e d u c t i o n _ i n d i c e s = [ 0 ] )

# o p t i m i z i n g t h e f u n c t i o n c o s t by g r a d i e n t d e s c e n t with l e a r n i n g s t e p 0 . 1

4 t r a i n _ s t e p = t f . t r a i n . G r a d i e n t D e s c e n t O p t i m i z e r ( 0 . 1 ) . minimize ( c o s t )

TensorFlow provides functions to perform common operations between tensors. The function
reduce_sum() for example reduces the tensor to one (or more) dimension, by summing up along
the speciﬁed dimension. The train module provide the most common optimizers, which will be
employed during the training process. The previous code chose the Gradient Descent algorithm to
optimize the network parameters, with respect to the penalty function deﬁned in cost by using a
learning rate equal to 0.1.

At this point the variables are still not initialized. The whole graph exist at a symbolic level, but it
is instantiated when creating a session. For example, placeholders are fed with the assigned elements
in this moment.

Start the session

% C r e a t e s e s s i o n

2 s e s s = t f . S e s s i o n ( )

20

% I n i t i a l i z e v a r i a b l e s

4 s e s s . run ( t f . i n i t i a l i z e _ a l l _ v a r i a b l e s ( ) )

More speciﬁcally, initialize_all_variables() creates an operation (a node in the Data Flow
Graph) running variables initializer. The function Session() creates an instance of the class session,
while the correspondent method run() moves for the ﬁrst time the Data Flow Graph on CPU/GPU,
allocates variables and ﬁlls them with the initial values.

Training

The training phase can be deﬁned in a for loop where each iteration represent a single gradient
descend epoch. In the following code, some printing on the training information are added each 100
epochs.

Epochs = 5000 # Number o f

i t e r a t i o n s

f o r

i

i n r a n g e ( Epochs ) :

s e s s . run ( t r a i n _ s t e p ,
i % 100 == 0 :
i f
p r i n t ( ’ Epoch ’ ,
p r i n t ( ’ Cost

’ ,

i )

f e e d _ d i c t ={x_ : X, y_ : Y} ) # o p t i m i z e r

s t e p

s e s s . run ( c o s t ,

f e e d _ d i c t ={x_ : X, y_ : Y} ) )

The sess.run() calling runs the operations previously deﬁned for the ﬁrst argument, which in this
case is an optimizer step (deﬁned by train_step). The second (optional) argument for run is a
dictionary feed_dict, pairing each placeholder with the correspondent input. The function run()
is used also to evaluate the cost each 100 epochs.

Evaluation

The performance of the trained model can be easily evaluated by:

1 c o r r e c t _ p r e d i c t i o n = abs (y_ − y ) < 0 . 5

c a s t = t f . c a s t ( c o r r e c t _ p r e d i c t i o n , " f l o a t " )

3 a c c u r a c y = t f . reduce_mean ( c a s t )

5 yy , aa = s e s s . run ( [ y , a c c u r a c y ] , f e e d _ d i c t ={x_ : X, y_ : Y} )

7 p r i n t " Output : " , yy

p r i n t " Accuracy : " , aa

2

4

6

8

9

Draw separation surfaces

In order to visualize separation surfaces computed by the network, it can be useful to generate a
random sample of points on which test results, as showed in Figure 3.

21

p l t . f i g u r e ( )

2 # P l o t t i n g d a t a s e t

c1 = p l t . s c a t t e r ( [ 1 , 0 ] ,
4 c0 = p l t . s c a t t e r ( [ 1 , 0 ] ,

[ 0 , 1 ] , marker= ’ s ’ , c o l o r= ’ gray ’ ,
[ 1 , 0 ] , marker= ’ ^ ’ , c o l o r= ’ gray ’ ,

s =100)
s =100)

# G e n e r a t i n g p o i n t s

i n [ − 1 , 2 ] x [ − 1 , 2 ]

6 DATA_x = ( np . random . rand ( 1 0 ∗ ∗ 6 , 2 ) ∗3)−1

DATA_y = s e s s . run ( y , f e e d _ d i c t ={x_ : DATA_x} )

8 # S e l e c t i n g b o r d e r l i n e p r e d i c t i o n s

i n d = np . where ( np . l o g i c a l _ a n d ( 0 . 4 9 < DATA_y, DATA_y< 0 . 5 1 ) ) [ 0 ]

10 DATA_ind = DATA_x[ i n d ]

# P l o t t i n g s e p a r a t i o n s u r f a c e s

12 s s = p l t . s c a t t e r (DATA_ind [ : , 0 ] , DATA_ind [ : , 1 ] , marker= ’_ ’ , c o l o r= ’ b l a c k ’ ,

s =5)

14 p l t . l e g e n d ( ( c1 , c0 ,

( ’ C l a s s 1 ’ ,

’ C l a s s 0 ’ ,

’ S e p a r a t i o n s u r f a c e s ’ ) ,

s c a t t e r p o i n t s =1)

# Some f i g u r e ’ s

s e t t i n g s
s s ) ,

p l t . x l a b e l ( ’ Input x1 ’ )
16 p l t . y l a b e l ( ’ Input x2 ’ )
p l t . a x i s ( [ − 1 , 2 , − 1 , 2 ] )

18 p l t . show ( )

Listing 10: Draw separation surfaces

Figure 3: Separation surfaces on the XOR classiﬁcation task obtained by 2-layer ANN with 3 Hidden
Units and the Logistic Sigmoid as activation and output function.

22

4 MNIST Handwritten Characters Recognition

In this Section we show how to set up a 2-Layer ANN in order to face the MNIST [6] classiﬁcation
problem, a well known data set for handwritten characters recognition. It is extensively used to
test and compare general Machine Learning algorithms and Computer Vision methods. Data are
provided as 28×28 pixels (grayscale) images of handwritten digits. The training and test sets contain
respectively 60,000 and 10,000 instances. Files .zip are available at the oﬃcial site15, together with
a list of performance achieved by most common algorithms. We show the setting up of a standard
2-Layer ANN with 300 units in the hidden layer, represented in Figure 4, since it is one of the
architecture reported in the oﬃcial website and the obtained results can be easily compared. The
input will be reshaped so as to feed the network with a 1-Dimensional vector with 28 · 28 = 784
elements. Each image is originally represented by a matrix containing the grayscale value of the
pixels in [0, 255], which will be normalized in [0, 1]. The output will be a 10 elements prediction
vector, since labels for each element will be expressed by the one-hot encoding binary vector of 10
null bits, with only a 1 in the position indicating the class. Activation and penalty functions are
diﬀerent within diﬀerent environments to provide an overview on diﬀerent approaches.

Figure 4: General architecture of a 2-Layer network model proposed to face the MNIST data.

4.1 MNIST on Matlab

Once data have been downloaded from the oﬃcial MNIST site, we can use Matlabfunctions avail-
able at the Stanford University site16 to extract data from ﬁles and organize them in inputSize
–by–numberOfSamples matrix form. The extraction routines reshape (so as that each digit is rep-
resented by a 1-D column vector of size 784) and normalizes data (so as that each feature lies in
the interval [0, 1]) . Once unzipped data and functions in the same folder, it is straightforward to
upload images in the Matlabworkspace by the loadMNISTImages function:

15http://yann.lecun.com/exdb/mnist/.
16http://ufldl.stanford.edu/wiki/index.php/Using_the_MNIST_Dataset.

23

2

1

3

images_Train = loadMNISTImages ( ’ t r a i n −images . idx 3−uby te ’ ) ;
images_Test = loadMNISTImages ( ’ t10k−images . idx 3−uby te ’ ) ;
images = [

images_Train ,

images_Test ] ;

where training and test set have been grouped in the same matrix to evaluate performance on the
provided test set during the training. Correspondent labels can be loaded and grouped in a similar
way by the function loadMNISTLabels:

l a b e l s _ T r a i n = loadMNISTLabels ( ’ t r a i n −l a b e l s . idx 1−uby te ’ ) ;
l a b e l s _ T e s t = loadMNISTLabels ( ’ t10k−l a b e l s . idx 1−uby te ’ ) ;
l a b e l s = [

l a b e l s _ T r a i n ;

l a b e l s _ T e s t ] ;

The original labels are provided as a 1-Dimensional vector containing a number from 0 to 9 according
to the correspondent digit. The one-hot encoding target matrix for the whole dataset can be
generated exploiting the Matlabfunction ind2vec17:

1 l a b e l s = f u l l (

i n d 2 v e c (

l a b e l s ’ + 1 )

) ;

To check the obtained results, we replied one of the 2-layer architectures listed at the oﬃcial website,
which is supposed to reach around 96% of accuracy with 300 hidden units and can be initialized by:

1 nn = p a t t e r n n e t ( 300 ) ;

As already said, this command creates a 2-Layer ANN where the hidden layer has 300 units and the
Hyperbolic Tangent as activation, whereas the output function is computed by the softmax. The
(default) penalty function is the Cross-Entropy Criterion.

In this case we change the data splitting so as that data used for test comes only from the original
test set (which has been concatenated with the training one), prevent to mix samples among Train,
Validation and Test set. This step is completely customizable by the method divideFcn and the
ﬁelds of the options divideParam. The divideind method picks data according to the provided
indexes for the data matrix:

1 nn . d i v i d e F c n = ’ d i v i d e i n d ’ ;

nn . div ideParam . t r a i n I n d = 1 : 4 5 0 0 0 ;
3 nn . div ideParam . v a l I n d = 4 5 0 0 0 : 6 0 0 0 0 ;

nn . div ideParam . t e s t I n d = 6 0 0 0 0 : 7 0 0 0 0 ;

In this case, we arbitrarily decided to use the last 25% of the Training data for Validation, since
the samples are more or less equally distributed by classes.
As already said, network training can be started by:

17The function full prevent for Matlabautomatically convert to sparse matrix, which in our tests may cause some

problems at the calling of the function train.

24

[ nn ,

t r

] = t r a i n ( nn ,

images ,

l a b e l s

) ;

In the reported case, the training stopped after 107 epochs because of an increasing in the validation
error (see Section 1.3). The performance during training are shown in Figure 5(a), which is obtained
by the following code:

1 % % % % P l o t t i n g MNIST t r a i n i n g p e r f o r m a n c e

3 p l o t ( t r . p e r f , ’ LineWidth ’ , 2 ) ; % p l o t

t r a i n i n g e r r o r

h o l d on ;

5 p l o t ( t r . v p e r f , ’ r −. ’ , ’ LineWidth ’ , 2 ) ; % p l o t v a l i d a t i o n e r r o r

p l o t ( t r . t p e r f , ’ g : ’ , ’ LineWidth ’ , 2 ) ; % p l o t
7 s e t ( gca , ’ y s c a l e ’ , ’ l o g ’ ) ; % s e t t i n g l o g s c a l e

t e s t

e r r o r

a x i s ( [ 1 , 1 0 7 , 0 . 0 0 1 , 1 . 8 ] ) ;

9 x l a b e l ( ’ T r a i n i n g Epochs ’ , ’ F o n t S i z e ’ , 1 4 ) ;
y l a b e l ( ’ Cross−Entropy ’ , ’ F o n t S i z e ’ , 1 4 ) ;

11 t i t l e ( ’ Performance Trend on MNIST ’ , ’ F o n t S i z e ’ , 1 6 ) ;

h = l e g e n d ( { ’ T r a i n i n g ’ , ’ V a l i d a t i o n ’ , ’ Test ’ } , ’ L o c a t i o n ’ , ’ North East ’ ) ;

13 s e t ( h , ’ F o n t S i z e ’ , 1 4 ) ;

Performance Trend on MNIST

Training
Validation
Test

5 → 6

8 → 0

2 → 8

10

20

30

40

50

60

70

80

90

100

Training Epochs

(a)

(b)

4 → 6

7 → 3

3 → 7

Figure 5: On the left the performance on the MNIST dataset during the training of a 2-layer ANN
with 300 hidden units. Training is stopped after 107 epochs for validation checking. On the right
we report some misclassiﬁed samples. The network reaches about 96% of classiﬁcation accuracy on
the test set (in accordance with the ones provided at the MNIST web page).

In Figure 5(b) we show some misclassiﬁed digits, indicating the original label and the predicted one.
The visualization is obtained by the Matlab function image (after a reshaping to the original square
dimensions and grayscale, multiplying by 255). In Listing 11 we show how to evaluate classiﬁcation
accuracy and confusion matrix on data, which should give coherent results with respect to which
reported in the oﬃcial site for the same architecture (about 4% error on test set).

25

y
p
o
r
t
n
E
-
s
s
o
r
C

10 0

10 -1

10 -2

10 -3

1 % network e v a l u a t i o n

f = nn (
3 f v = nn (
f t = nn (

images (

: , 1 : 4 5 0 0 0 )

) ; % t r a i n i n g p r e d i c t i o n s

images (
images (

: , 4 5 0 0 1 : 6 0 0 0 0 )
: , 6 0 0 0 1 : end )

) ; % v a l i d a t i o n p r e d i c t i o n s

) ; % t e s t p r e d i c t i o n s

5 % c l a s s i f i c a t i o n a c c u r a c y

A = mean ( v e c 2 i n d ( f ) == v e c 2 i n d (
7 Av = mean ( v e c 2 i n d ( f v ) == v e c 2 i n d (
At = mean ( v e c 2 i n d ( f t ) == v e c 2 i n d (

l a b e l s (

: , 1 : 4 5 0 0 0 )

)

) ;

l a b e l s (
l a b e l s (

: , 4 5 0 0 1 : 6 0 0 0 0 )
)
: , 6 0 0 0 1 : end )

) ;

)
) ;

9 % c o n f u s i o n matrix
C = c o n f u s i o n m a t (

11 Cv = c o n f u s i o n m a t ( v e c 2 i n d ( f v ) , v e c 2 i n d (
Ct = c o n f u s i o n m a t ( v e c 2 i n d ( f t ) , v e c 2 i n d (

l a b e l s (
l a b e l s (

: , 4 5 0 0 1 : 6 0 0 0 0 )
)
: , 6 0 0 0 1 : end )

) ;

)
) ;

v e c 2 i n d ( f ) , v e c 2 i n d (

l a b e l s (

: , 1 : 4 5 0 0 0 )

)

)

;

Listing 11: Performance evaluation of the network trained on the MNIST

4.2 MNIST on Torch

As already said, the Torch environment provides a lot of tools for Machine Learning, included a
lot of routines to download and prepare most common Datasets. A wide overview on most useful
tutorials, demos and introduction to most common methods can be found in a dedicate webpage18,
including a Loading popular datasets section. Here, a link to the MNIST loader page 19 is available,
where data and all the informations for the correspondent mnist package installation are provided.
After the installation, data can be loaded by the code in Listing 12.

2 % l o a d i n g data

images_Train = loadMNISTImages ( ’ t r a i n −images −idx 3−uby te ’ ) ;

4 images_Test = loadMNISTImages ( ’ t10k−images −idx 3−uby te ’ ) ;

images = [

images_Train ,

images_Test ] ;

6

% l o a d i n g t a r g e t s

8 l a b e l s _ T r a i n = loadMNISTLabels ( ’ t r a i n −l a b e l s −idx 1−uby te ’ ) ;
l a b e l s _ T e s t = loadMNISTLabels ( ’ t10k−l a b e l s −idx 1−uby te ’ ) ;

10 l a b e l s = [

l a b e l s = f u l l (

l a b e l s _ T r a i n ;
i n d 2 v e c (

l a b e l s _ T e s t ] ;
l a b e l s ’ + 1 )

) ;

12 %%

% i n i t i a l i z a t i o n

14 nn = p a t t e r n n e t ( 300 ) ;

n n . d i v i d e F c n = ’ d i v i d e i n d ’ ;

16 n n . d i v i d e P a r a m . t r a i n I n d = 1 : 45000 ;

n n . d i v i d e P a r a m . v a l I n d = 45001 : 60000 ;
18 n n . d i v i d e P a r a m . t e s t I n d = 60001 : 70000 ;

Listing 12: Loading MNIST data

Data will be loaded as a table named train, where digits are expressed as a numberOfSamples-by-
28-by-28 tensor of type ByteTensor stored in the ﬁeld data, expressing the value of the gray levels
of each pixel between 0 and 255. Targets will be stored as a 1-D vector, expressing the digits labels,
in the ﬁeld label. We have to convert data to the DoubleTensor format and, again, normalize the
input features to have values in [0, 1] and reshape the original 3-D tensor in a 1-D input vector.

18https://github.com/torch/torch7/wiki/Cheatsheet#machine-learning
19https://github.com/andresy/mnist

26

2

4

6

4

6

8

Labels have to be incremented by 1, since CrossEntropyCriterion accepts target indicating the
class avoiding null values (i.e. 1 means a sample to belong to the ﬁrst class and so on). In the last
row we perform a random shuﬄing of data in order to prepare the train/validation splitting by the
function:

f u n c t i o n r n d S h u f f l e ( d a t a s e t )

−− random data s h u f f l e
l o c a l n = d a t a s e t . d a t a : s i z e ( 1 )
l o c a l perm = t o r c h . L o n g T e n s o r ( ) : randperm ( n )
d a t a s e t . d a t a = d a t a s e t . d a t a : i n d e x ( 1 , perm )
d a t a s e t . l a b e l = d a t a s e t . l a b e l : i n d e x ( 1 , perm )
r e t u r n d a t a s e t

8 end

At this point we can create a validation set from the last quarter of the training data by:

−− s p l i t t i n g f u n c t i o n d e f i n i t i o n

2 f u n c t i o n s p l i t D a t a ( data , p )

−− s p l i t s data depending on t h e r a t e p
l o c a l p = p > 0 and p <= 1 and p o r 0 . 9
l o c a l
r e t u r n data : narrow ( 1 , 1 , t r a i n S i z e ) , data : narrow ( 1 , 1 + t r a i n S i z e , data : s i z e ( 1 ) −

t r a i n S i z e = t o r c h . f l o o r ( p∗ data : s i z e ( 1 ) )

t r a i n S i z e )

end

v a l i d a t i o n = {}
10 t r a i n R a t e = 0 . 7 5

t r a i n . d a t a , v a l i d a t i o n . d a t a = s p l i t D a t a ( t r a i n . d a t a ,
12 t r a i n . l a b e l , v a l i d a t i o n . l a b e l = s p l i t D a t a ( t r a i n . l a b e l ,

t r a i n R a t e )

t r a i n R a t e )

The code to build the proposed 2-layer ANN model is reported in Listing 13, where the network
is assembled in the Sequential container, using this time the ReLU as activation for the hidden
layer, whereas output and penalty functions are the same used in the previous section (softmax and
Cross-Entropy respectively).

−− mlpwork d e f i n i t i o n
’ nn ’

2 r e q u i r e

4 l o c a l mlp = n n . S e q u e n t i a l ( )

mlp : add ( n n . L i n e a r ( 7 8 4 , 3 0 0 ) )

6 mlp : add ( nn.ReLU ( ) )

mlp : add ( n n . L i n e a r ( 3 0 0 , 1 0 ) )

8 mlp : add ( nn.SoftMax ( ) )

10 −− l o s s

f u n c t i o n

l o c a l c = n n . C r o s s E n t r o p y C r i t e r i o n ( )

Listing 13: Network deﬁnition for MNIST data

The network training can be deﬁned in a way similar to the one proposed in Listing 6. Because of
the width of the training data, this time is more convenient to set up a minibatch training as showed
in Listing 14. Moreover, we also deﬁne an early stopping criterion which stops the training when

27

the penalty on the validation set start to increase, preventing overﬁtting problems. The training
function expects as inputs the network (named mlp), the criterion to evaluate the loss (named
criterion), training and validation data (named trainset and validation respectively) organized as a
table with ﬁelds data and label as deﬁned in Listing 12. An optional conﬁguration table options can
be provided, indicating the number of training epochs (nepochs), the learning rate (learning_rate),
the mini-batch size (batchSize) and the number of consecutive increasings in the validation loss
which causes a preventive training stop (maxSteps). It is worth a remark on the function split,
deﬁned for the Tensor class, used to divide data in batches stored in an indexed table. At the end
of the training, a vector containing the loss evaluated at each epoch is returned. The validation loss
is computed with the help of the function evaluate, which splits again the computation in smaller
batches, preventing from too heavy computations when the number of parameters and samples is
very large.

1 f u n c t i o n t r a i n i n g ( mlp ,
−− m i n i b a t c h t r a i n i n g

c r i t e r i o n ,

t r a i n s e t , v a l i d a t i o n , o p t i o n s )

3 a s s e r t ( mlp and t r a i n s e t , "At l e a s t 2 arguments a r e e x p e c t e d " )

l o c a l nepochs = o p t i o n s and o p t i o n s . n e p o c h s o r 1000 −− max number o f epoch

5 l o c a l

l e a r n i n g _ r a t e = o p t i o n s and o p t i o n s . l e a r n i n g _ r a t e o r 0 . 0 1

l o c a l b a t c h S i z e = o p t i o n s and o p t i o n s . b a t c h S i z e o r 32

7 l o c a l maxSteps = o p t i o n s and o p t i o n s . m a x S t e p s o r 10 −− max v a l i d a t i o n f a i l s

l o c a l

input ,

t a r g e t = t r a i n s e t . d a t a ,

t r a i n s e t . l a b e l

9 −− v e c t o r

f o r

s a v i n g l o s s d u r i n g epoch

l o c a l

l o s s = t o r c h . T e n s o r ( nepochs ) : typeAs ( i n p u t ) : f i l l ( 0 )

11 l o c a l v a l L o s s = t o r c h . T e n s o r ( nepochs ) : typeAs ( i n p u t ) : f i l l ( 0 )

−− m i n i b a t c h e s

s p l i t t i n g

13 l o c a l m i n i b a t c h e s , m i n i t a r g e t = i n p u t : s p l i t ( b a t c h S i z e ) ,

t a r g e t : s p l i t ( b a t c h S i z e )

−− l a s t

l o s s and v a l i d a t i o n f a i l s

f o r

e a r l y s t o p p i n g c r i t e r i o n

l o s s [ epoch ] = l o s s [ epoch ] + c r i t e r i o n : f o r w a r d ( mlp : f o r w a r d ( batch ) , m i n i t a r g e t [ b i ] )

15 −− based on v a l i d a t i o n l o s s
l a s t L o s s , s t e p = 0 , −1

l o c a l

17 l o c a l epoch = 1

w h i l e epoch < nepochs and s t e p < maxSteps do

19 f o r bi , batch i n p a i r s ( m i n i b a t c h e s ) do

21 −− r e s e t g r a d i e n t s

t o n u l l v a l u e s

mlp : zeroGradParame ter s ( )
23 −− accumulate g r a d i e n t s
mlp : backward ( batch ,
25 −− update p a r a m e t e r s

c r i t e r i o n : backward ( mlp.output , m i n i t a r g e t [ b i ] ) )

mlp : updateParameters ( l e a r n i n g _ r a t e )

27 i f b i %100 == 0 then c o l l e c t g a r b a g e ( ) end −− c l e a n i n g n i l v a r i a b l e

end

29 −− e v a l u a t i n g v a l i d a t i o n l o s s
c u r r L o s s = e v a l u a t e ( mlp ,

l o c a l

31 v a l L o s s [ epoch ] = c u r r L o s s

i f

c u r r L o s s >= l a s t L o s s ∗ 0 . 9 9 9 9 then

c r i t e r i o n , v a l i d a t i o n )

33 s t e p = s t e p + 1

35 s t e p = 0

e l s e

end

37 l a s t L o s s = c u r r L o s s

39 epoch = epoch + 1

end

41 i f

s t e p == maxStep then

x l u a . p r o g r e s s ( epoch , nepochs ) −− p r i n t i n g e p o c h s p r o g r e s s

28

p r i n t ( ’ T r a i n i n g s t o p p e d a t Epoch :

’ . . e p o c h . .

43 ’ b e c a u s e o f V a l i d a t i o n E r r o r

i n c r e a s i n g i n t h e l a s t

’

. . m a x S t e p . . ’ e p o c h s ’ )

45 end

47 end

r e t u r n l o s s : narrow ( 1 , 1 , epoch −1)/#m i n i b a t c h e s , v a l L o s s : narrow ( 1 , 1 , epoch −1)

49 f u n c t i o n e v a l u a t e ( mlp ,
−− e v a l u a t e t h e l o s s

c r i t e r i o n , d a t a s e t , o p t i o n s )

from c r i t e r i o n between mpl p r e d i c t i o n s

51 −− by a c c u m u l a t i n g w i t h i n m i n i b a t c h e s from d a t a s e t

a s s e r t ( mlp and d a t a s e t , "At l e a s t 2 arguments a r e e x p e c t e d " )

53 l o c a l b a t c h S i z e = o p t i o n s and o p t i o n s . b a t c h S i z e o r 32

t a r g e t = d a t a s e t . d a t a , d a t a s e t . l a b e l

l o c a l
55 l o c a l

input ,
l o s s = 0

l o c a l m i n i b a t c h e s , m i n i t a r g e t = i n p u t : s p l i t ( b a t c h S i z e ) ,

t a r g e t : s p l i t ( b a t c h S i z e )

57 f o r bi , batch i n p a i r s ( m i n i b a t c h e s ) do

l o s s = l o s s + c r i t e r i o n : f o r w a r d ( mlp : f o r w a r d ( batch ) , m i n i t a r g e t [ b i ] )

mlp : zeroGradParame ter s ( )
61 r e t u r n l o s s /#m i n i b a t c h e s

59 end

end

Listing 14: Sample of a mini-batch training function

In Listing 15 we show how to compute the Confusion Matrix and the Classiﬁcation Accuracy on
data by the function confusionMtx, taking in input the network (mlp), data (dataset) and the
expected number of classes (nclasses).

f u n c t i o n c o n f u s i o n M t x ( mlp , d a t a s e t , n c l a s s e s )

2 l o c a l

input ,

t a r g e t = d a t a s e t . d a t a , d a t a s e t . l a b e l

l o c a l confMtx = t o r c h . z e r o s ( n c l a s s e s , n c l a s s e s ) : typeAs ( i n p u t )

4

l o c a l p r e d i c t i o n = mlp : f o r w a r d ( i n p u t )

6 −− g e t

t h e p o s i t i o n o f

t h e u n i t with t h e maximum v a l u e

l o c a l _, pos = t o r c h . m a x ( p r e d i c t i o n , 2 )

8 l o c a l a c c = 0

f o r

i = 1 , n c l a s s e s do

10 l o c a l p r e d i c t e d = t o r c h . e q ( pos , i ) : typeAs ( i n p u t )

f o r
12 l o c a l

j = 1 , n c l a s s e s do

t r u t h = t o r c h . e q ( t a r g e t ,
confMtx [ i ] [ j ] = t o r c h . c m u l ( p r e d i c t e d ,

j ) : typeAs ( i n p u t )

t r u t h ) : sum ( 1 )

14 i f

i == j

then a c c = a c c + confMtx [ i ] [ j ] end

end
16 end

18 end

r e t u r n confMtx , a c c / i n p u t : s i z e ( 1 )

Listing 15: Evaluating Confusion Matrix and Accuracy

At this point we can start a trial by the following code:

o p t i o n s = { nepochs = 2 5 0 , b a t c h S i z e = 6 4 ,

l e a r n i n g _ r a t e = 0 . 05 , maxStep = 50}

2 t r a i n i n g ( mlp , c ,

t r a i n , v a l i d a t i o n , o p t i o n s )

confMtx , a c c = c o n f u s i o n M t x ( mlp ,

t e s t , 1 0 )

29

In this case the training is stopped by the validation criterion after epoch 117, producing a Clas-
siﬁcation Accuracy on test of about 97%. In Figure 6(a) we report the trend of the error during
training. Since in general can be useful to visualize the confusion matrix (which in this case is al-
most diagonal), in Figure 6(b) we show the one obtained by the function imagesc from the package
gnuplot, which just give a color map of the matrix passed as input.

Figure 6: Confusion matrix on the MNIST test set.

4.3 MNIST on Tensor Flow

Even TensorFlow environment makes available many tutorials and preloaded dataset, including the
MNIST. A very fast introductive section for beginners can be found at the oﬃcial web page20.
However, we will show some functions to download and import the dataset in a very simple way.
Indeed, data can be directly loaded by:

1 from t e n s o r f l o w . ex amples . t u t o r i a l s . mnist

import

input_data

mnist = input_data . read_data_sets ( "MNIST_data/ " , one_hot=True )

We ﬁrstly deﬁne two auxiliary functions. The ﬁrst (init_weights) will be used to initialize the
parameters of the model, whereas the second (mlp_output) to compute the predictions of the model.

d e f

i n i t _ w e i g h t s ( shape ) :

2 r e t u r n t f . V a r i a b l e ( t f . random_uniform ( shape , −0.1 , 0 . 1 ) )

4 d e f mlp_output (X, W_h, W_o, b_h , b_o ) :

6 a1 = t f . matmul (X, W_h) + b_h
o1 = t f . nn . r e l u ( a1 ) #output

l a y e r 1

8

20https://www.tensorflow.org/versions/r0.12/tutorials/mnist/beginners/index.html

30

a2 = t f . matmul ( o1 , W_o) + b_o
10 o2 = t f . nn . softmax ( a2 ) #output

l a y e r 2

12 r e t u r n o2

Now, with the help of the proposed function init_weights, we deﬁne the parameters to be learned
during the computation. W 1 and b1 represent respectively the weights and the biases of the hidden
layer. Similarly, W 2 and b2 are respectively the weights and the biases of the output layer.

W1 = i n i t _ w e i g h t s ( [ x_dim , h_layer_dim ] )

2 b1 = i n i t _ w e i g h t s ( [ h_layer_dim ] )

W2 = i n i t _ w e i g h t s ( [ h_layer_dim , y_dim ] )

4 b2 = i n i t _ w e i g h t s ( [ y_dim ] )

Once we deﬁned the weights, we can symbolically compose our model by the calling of our function
mlp_output. As in the XOR case, we have to deﬁne a placeholder storing the input samples.

2 x = t f . p l a c e h o l d e r ( t f . f l o a t 3 2 ,

[ None , x_dim ] )

# Input

# P r e d i c t i o n

4 y = mlp_output ( x , W1, W2, b1 , b2 )

Then we need to deﬁne a cost function and an optimizer. However, this time we add the square of the
Euclidean Norm as regularizer, and the global cost function is composed by the Cross-Entropy plus
the regularization term scaled by a coeﬃcient of 10−4. At the beginning of the session, TensorFlow
moves the Data Flow Graph to the CPUs or GPUs and initializes all variables.

# Model S p e c i f i c a t i o n s

2 LEARNING_RATE = 0 . 5

EPOCHS = 5000

4 MINI_BATCH_SIZE = 50
# Sy mbolic v a r i a b l e

6 y_ = t f . p l a c e h o l d e r ( t f . f l o a t 3 2 ,

[ None , y_dim ] )

# Loss

f u n c t i o n and o p t i m i z e r

f o r

t h e t a r g e t

8 c r o s s _ e n t r o p y = t f . reduce_mean ( t f . nn . s o f t m a x _ c r o s s _ e n t r o p y _ w i t h _ l o g i t s ( y , y_ ) )

r e g u l a r i z a t i o n = ( t f . reduce_sum ( t f . s q u a r e (W1) ,

[ 0 , 1 ] )

10

+ t f . reduce_sum ( t f . s q u a r e (W2) ,

[ 0 , 1 ] )

)

c o s t = c r o s s _ e n t r o p y + 10 ∗ ∗−4 ∗ r e g u l a r i z a t i o n

12 t r a i n _ s t e p = t f . t r a i n . G r a d i e n t D e s c e n t O p t i m i z e r (LEARNING_RATE) . minimize ( c o s t )

# S t a r t

s e s s i o n and i n i t i a l i z a t i o n

14 s e s s = t f . S e s s i o n ( )

s e s s . run ( t f . i n i t i a l i z e _ a l l _ v a r i a b l e s ( ) )

TensorFlow provides the function next_batch for the mnist class to randomly extract batches of
a speciﬁed size. Data are split in shuﬄed partitions of the indicated size and, by means of an
implicit counter, the function slide along batches at each calling allowing a fast implementation
for mini-batch Gradient Descent method. In our case, we used a for loop to scan across batches,
executing a training step on the extracted data at each iteration. Once the whole Training set

31

has been processed, the loss on Training, Validation and Test sets is computed. These operations
are repeated in a while loop, whose steps representing the epochs of training. The loop stops
when the maximum number of epochs is reached or the network start to overﬁt Training data.
The early stopping is implemented by checking the Validation error and training is stopped when
no improvements are obtained for a ﬁxed number of consecutive epochs (val_max_steps). The
maximum number of epochs and learning rate must be set in advance.

1 # Save v a l u e s

t o be p l o t t e d

e r r o r s _ t r a i n = [ ]

3 e r r o r s _ t e s t = [ ]
e r r o r s _ v a l = [ ]

5

# E a r l y s t o p p i n g ( i n i t v a r i a b l e s )

7 p r e c _ e r r = 10 ∗ ∗ 6 # j u s t a v e r y b i g v a l u e

v al_count = 0

9 val_max_steps = 5

11 # T r a i n i n g

BATCH_SIZE = np . shape ( mnist . t r a i n . images ) [ 0 ]

13 MINI_BATCH_SIZE = 50

15 i = 1
w h i l e

i <= e p o c h s and v al_count < val_max_steps :

17

19

21

25

27

29

31

35

37

39

41

43

45

47

# Train o v e r
f o r

j

i n r a n g e (BATCH_SIZE/MINI_BATCH_SIZE ) :

t h e f u l l batch i s p e r f o r m e d with mini−b a t c h e s a l g o r i t h m

batch_xs , batch_ys = mnist . t r a i n . next_batch ( 5 0 )
s e s s . run ( t r a i n _ s t e p ,

f e e d _ d i c t ={x : batch_xs , y_ : batch_ys } )

23 # Compute e r r o r on v a l i d a t i o n s e t and c o n t r o l

f o r e a r l y −s t o p p i n g

f e e d _ d i c t ={x : mnist . v a l i d a t i o n . images , y_ : mnist . v a l i d a t i o n . l a b e l s } )

c u r r _ e r r = s e s s . run ( c r o s s _ e n t r o p y ,

i f

c u r r _ e r r >= p r e c _ e r r ∗ 0 . 9 9 9 9 :

v al_count = v al_count + 1

e l s e :

v al_count = 0

p r e c _ e r r = c u r r _ e r r

33 # Save v a l u e s

f o r p l o t

e r r o r s _ v a l . append ( c u r r _ e r r )
c _ t e s t = s e s s . run ( c r o s s _ e n t r o p y ,

e r r o r s _ t e s t . append ( c _ t e s t )
c _ t r a i n = s e s s . run ( c r o s s _ e n t r o p y ,

e r r o r s _ t r a i n . append ( c _ t r a i n )

f e e d _ d i c t ={x : mnist . t e s t . images , y_ : mnist . t e s t . l a b e l s } )

f e e d _ d i c t ={x : mnist . t r a i n . images , y_ : mnist . t r a i n . l a b e l s } )

i n f o about

t h e c u r r e n t epoch

# P r i n t
p r i n t " \n\nEPOCH: " , i , " / " , e p o c h s
p r i n t " TRAIN ERR: " , c _ t r a i n
p r i n t " VALIDATION ERR: " , c u r r _ e r r
p r i n t " TEST ERR: " , c _ t e s t
p r i n t " \n ( E a r l y s t o p p i n g c r i t e r i o n : " , val_count , " / " , val_max_steps , " ) "

32

49 # I n c r e m e n t epochs−i n d e x

i = i +1

The prediction accuracy of the trained model can be evaluated over the test set in way similar to the
one presented for the XOR problem. This time we need to exploit the argmax function to convert
the one-hot encoding in the correspondent labels.

# Sy mbolic f o r m u l a s

2 c o r r e c t _ p r e d i c t i o n = t f . e q u a l ( t f . argmax ( y , 1 ) ,

t f . argmax (y_ , 1 ) )

a c c u r a c y = t f . reduce_mean ( t f . c a s t ( c o r r e c t _ p r e d i c t i o n ,

t f . f l o a t 3 2 ) )

4 # Compute a c c u r a c y on t h e t e s t

s e t

aa = s e s s . run ( a c c u r a c y ,

f e e d _ d i c t ={x : mnist . t e s t . images , y_ : mnist . t e s t . l a b e l s } )

6 p r i n t " Accuracy : " , aa

The trend of the network performance showed in Figure 7 during training can be obtained by the
following code:

E = r a n g e ( np . shape ( e r r o r s _ t r a i n ) [ 0 ] )

2

l i n e _ t r a i n , = p l t . p l o t (E ,

e r r o r s _ t r a i n )

4 l i n e _ t e s t , = p l t . p l o t (E ,

e r r o r s _ t e s t )

l i n e _ v a l , = p l t . p l o t (E , e r r o r _ v a l )

p l t . y l a b e l ( ’ Cross−Entropy ’ )

8 p l t . x l a b e l ( ’ Epochs ’ )

p l t . show ( )

6 p l t . l e g e n d ( [ l i n e _ t r a i n ,

l i n e _ v a l ,

l i n e _ t e s t ] ,

[ ’ T r a i n i n g ’ ,

’ V a l i d a t i o n ’ ,

’ Test ’ ] )

33

Figure 7: Error trend on the MNIST dataset during the training of a 2-layer ANN with 300 hidden
units.

34

5 Convolutional Neural Networks

Figure 8: General architecture of the CNN model proposed to face the MNIST data.

In this section we introduce the Convolutional Neural Networks (CNNs [7, 6, 8]), an important and
powerful kind of learning architecture widely diﬀused especially for Computer Vision applications.
They currently represent state of the art algorithm for image classiﬁcation tasks and constitute the
main architecture used in Deep Learning. We show how to build and train such a structure within
all the proposed frameworks, exploring the most general functions and setting up few experiments
on MNIST pointing out some important features.

5.1 Matlab

TM

TM

and Statistic and Machine Learning Toolbox

Main function and classes to build and train CNNs with Matlabare contained again in Neural
. Nevertheless, the Parallel
Network Toolbox
becomes necessary too. Moreover, to train the network a CUDA R(cid:13) -enabled
Computing Toolbox
NVIDIA R(cid:13) GPU is required. Again, we do not focus too much on main theoretical properties and
general issues about CNNs but only on main implementation instruments.
The network has to be stored in an Matlabobject of kind Layer, which can be sequentially com-
posed by diﬀerent sub-layers. For each one, we do not list all the available options, which as always
can be explored by the interactive help options from the Command Window21. Most common
convolutional objects can be deﬁned by the functions:

TM

imageInputLayer creates the layer which deals with the original input image, requiring as argument
a vector expressing the size of the input image given by height by width by number of channels;

convolution2dLayer deﬁnes a layer of 2-D convolutional ﬁlters whose size is speciﬁed by the
ﬁrst argument (a real number for a square ﬁlter, a 2-D vector to specify both height and
width), whereas the second argument speciﬁes the total number of ﬁlters; main options are
’Stride’, which indicates the sliding step (default [1, 1] means 1 pixel in both directions),
and ’Padding’ (default [0, 0]), whose have to appear in Name,Value pairs;

reluLayer deﬁnes a layer computing the Rectiﬁer Activation Linear Unit (ReLU) for the ﬁlter

outputs;

21Further documentation is available at the oﬃcial site https://it.mathworks.com/help/nnet/convolutional-neural-networks.html

35

averagePooling2dLayer layer computing a spatial reduction of the input by averaging the values
of the input on each grid of given dimension (default [2, 2], ’Stride’ and ’Padding’ are
options too);

maxPooling2dLayer layer computing a spatial reduction of the input assigning the max value to

each grid of given dimensions (default [2, 2], ’Stride’ and ’Padding’ are options too);

fullyConnectedLayer requires the desired output dimension as argument and instantiates a classic

fully connected linear layer, the number of the connections is adapted to ﬁt the input size;

dropoutLayer executes a dropout units selection with the probability given as argument;

softmaxLayer computes a probability normalization based on the softmax function;

classificationLayer adds the ﬁnal classiﬁcation layer evaluating the Cross-Entropy loss function

between predictions and labels

1 CnnM = [

imageInputLay er ( [ 2 8 28 1 ] ) ,

. . . % i n p u t

l a y e r

. . . % c o n v o l u t i o n a l

l a y e r 12 f i l t e r s o f

s i z e 5 x5 and 2 p i x e l s 0−padding

3 c o n v o l u t i o n 2 d L a y e r ( 5 , 1 2 , ’ Padding ’ , 2 ) ,

. . .

r e l u L a y e r ( ) ,

. . .

5 . . . % max−p o o l i n g l a y e r on 2 x2 g r i d and 2 p i x e l s

s t e p i n both d i r e c t i o n s

maxPooling2dLayer ( 2 , ’ S t r i d e ’ , 2 ) ,

. . .

7 . . . % c o n v o l u t i o n a l

l a y e r with 16 f i l t e r s o f

s i z e 3 x3 and 1 p i x e l s 0−padding

c o n v o l u t i o n 2 d L a y e r ( 3 , 1 6 , ’ Padding ’ , 1 ) ,

. . .

9 r e l u L a y e r ( ) ,

. . .

. . . % max−p o o l i n g l a y e r on 2 x2 g r i d and 2 p i x e l s

s t e p i n both d i r e c t i o n s

11 maxPooling2dLayer ( 2 , ’ S t r i d e ’ , 2 ) ,

. . .

. . . % c l a s s i c

f u l l y c o n n e c t e d l a y e r with 256 output

13 f u l l y C o n n e c t e d L a y e r ( 2 5 6 ) ,

. . .

r e l u L a y e r ( ) ,
15 . . . % c l a s s i c

. . .

f u l l y C o n n e c t e d L a y e r ( 1 0 ) ,

. . .

17 softmax Lay er ( ) ,

. . .

c l a s s i f i c a t i o n L a y e r ( )

] ;

f u l l y c o n n e c t e d l a y e r with 10 output

Listing 16: Deﬁnition of a CNN to face the MNIST within Matlab framework.

In Listing 16 we show how to set up a basic CNN to face the 28 × 28 pixels images from MNIST
showed in Figure 8. The global structure of the network is deﬁned as a vector composed by the
diﬀerent modules. The initialized object Layer, named CnnM, can be visualized from the command
window giving:

36

Data for the MNIST can be loaded by the Stanford routines showed in Section 4.1. This time input
images are required as a 4-D tensor of size numberOfSamples–by–channels–by–height –by–width and,
hence, we have to modify the provided function loadMNISTimages or just to reshape data, as showed
in the ﬁrst line of the following code:

X = r e s h a p e ( images_Train , 2 8 , 2 8 , 1 , [ ] )

2 Y = nominal ( l a b e l s _ T r a i n , { ’ 0 ’ , ’ 1 ’ , ’ 2 ’ , ’ 3 ’ , ’ 4 ’ , ’ 5 ’ , ’ 6 ’ , ’ 7 ’ , ’ 8 ’ , ’ 9 ’ } ) ;

The second command is used to convert targets in a categorical Matlabvariable of kind nominal,
which is required to train the network exploiting the function trainNetwork.
It also require
as input an object specifying the training options which can be instantiated by the function
trainingOptions. The command:

o p t s = t r a i n i n g O p t i o n s ( ’ sgdm ’ ) ;

selects (by the ’sgdm’ string) the Stochastic Gradient Descent algorithm using momentum. Many
optional parameters are available, which can be set by additional parameter in the Name,Value
pairs notation again. The most common are:

Momentum (default 0.9)

InitialLearnRate (default 0.01)

L2Regularization (default 0.0001)

MaxEpochs (default 30)

MiniBatchSize (default 128)

After this conﬁguration, the training can be started by the command:

1 [ t r a i n e d N e t , trainOp ] = t r a i n N e t w o r k (X, Y, CnnM, o p t s )

where trainedNet will contain the trained network and trainOp the training variables. Training starts a
command line printing of some useful variables indicating the training state, which will be similar to:

At the end of the training, we can evaluate the performance on a suitable test set Xtest, together with its
correspondent target Ytest, by the function classify:

1 P r e d i c t i o n s = c l a s s i f y ( t r a i n e d N e t , X t e s t ) ;

Accuracy = mean ( P r e d i c t i o n s==Y t e s t ) ;

3 C = c o n f u s i o n m a t (P , c a t e g o r i c a l (Y) ) ;

37

Assuming Ytest to be a 1-Dimensional vector of class labels, classiﬁcation accuracy can be calculated as
before by the meaning of the boolean comparing with the computed predictions vector Predictions. An
useful built-in function to compute the confusion matrix is provided, requiring the nominal labels to be
converted into categorical as the predictions. In this setting, the ﬁnal classiﬁcation accuracy on the test
set should be close to the 99%.

When dealing with CNNs, an important new type of object introduced are the convolutional ﬁlters. We
do not want to go in deep with theoretical explanations, however sometimes it could be useful to visualize
the composition of the convolutional ﬁlters in order to get an idea of which kind of features each ﬁlter
detects. Filters are represented by the weights of each convolution, stored in each layers in the 4-D weights
tensor of size height –by–width–by–numberOfChannels–by–numberOfFilters.
In our case for example, the
ﬁlters of the ﬁrst convolutional be accessed by the notation CnnM.Layer(2).Weights. In Figure 9, we show
their conﬁguration after the training (again exploiting the function image and the colormap gray and a
normalization in [0, 255] for a suitable visualization).

Figure 9: First Convolutional Layer ﬁlters of dimension 5 × 5 after the training on the MNIST
images with Matlab.

5.2 Torch

Within the Torch environment is straightforward to deﬁne a CNN from the nn package presented in Sec-
tion 2.2.2. Indeed, the network can be stored in a container and composed by speciﬁc modules. Again, we
give a short list description of the most common ones, which can be integrated with the standard transfer
functions or with a standard linear (fully connected) layer introduced before:

SpatialConvolution deﬁnes a convolutional layers, the required arguments are the number of input chan-
nels, the number of output channels, the height and the width of the ﬁlters. The step-size and
zero-padding height and width are optional parameters (with default value 1 and 0 respectively)

SpatialMaxPooling standard max pooling layer, requiring as inputs height and width of the pooling win-

dow, whereas the step-sizes are optional parameters (default the same as the window size)

SpatialAveragePooling standard average pooling layer, same features of the previous one

SpatialDropout set a dropout layer taking as optional argument the deactivating rate (default 0.5)

Reshape is a module which is usually used to unroll the output after a convolutional/pooling process as a
1-D vector to be feed to a linear layer, takes as input the size of the desired output dimensions

The assembly of the network follows from what seen until now. To have a diﬀerent comparison with the
previous experiment, we operate an initial 2 × 2 window max-pooling on the input image, in order to provide

38

the network by images of lower resolution. The general architecture will diﬀer from the one deﬁned in
Section 5.1 only by the ﬁrst layers. The proposed network is generated by the code in Listing 17, whereas
in Figure 10 we show the global architecture.

Figure 10: General architecture of the CNN model proposed to face the MNIST data within the
Torch environment (Section 5.2).

1 r e q u i r e

’ nn ’

−− c o n t a i n e r d e f i n i t i o n
3 c n n e t = n n . S e q u e n t i a l ( )

−− network assembly

c n n e t : add ( nn.ReLU ( ) )

5 c n n e t : add ( n n . S p a t i a l C o n v o l u t i o n ( 1 , 1 2 , 5 , 5 , 1 , 1 , 2 , 2 ) )

7 c n n e t : add ( n n . S p a t i a l M a x P o o l i n g ( 2 , 2 , 2 , 2 ) )

c n n e t : add ( n n . S p a t i a l C o n v o l u t i o n ( 1 2 , 1 6 , 3 , 3 , 1 , 1 , 1 , 1 ) )

9 c n n e t : add ( nn.ReLU ( ) )

c n n e t : add ( nn.R eshape ( 7 ∗ 7 ∗ 1 6 ) )
11 c n n e t : add ( n n . L i n e a r ( 7 ∗ 7 ∗ 1 6 , 2 5 6 ) )

c n n e t : add ( nn.ReLU ( ) )

13 c n n e t : add ( n n . L i n e a r ( 2 5 6 , 1 0 ) )

c n n e t : add ( nn.SoftMax ( ) )

Listing 17: Network deﬁnition for MNIST data reduced to 14×14 pixels images by 2×2 max-pooling

If we use the training function deﬁned in Listing 14, the optimization (starting with the same options) stops
for validation check after 123 epochs, producing a Classiﬁcation Accuracy of about 90%. This just to give an
idea of the diﬀerence in the obtained performances when there is a reduction in the information expressed
by input images. In Figure 11 we show the 12 ﬁlters of size 5 × 5 extracted by the ﬁrst convolutional layer.
The weights can be obtained by the function parameters from the package nn:

myParam = c n n e t : p a r a m e t e r s ( )

which return an indexed table storing the weights of each layer. In this case, the ﬁrst element of the table
contains a tensor of dimension 12 by 25 representing the weights of the ﬁlters. The visualization can be
generated exploiting the function imagesc from the package gnuplot, after reshaping each line in the 5 × 5
format.

39

Figure 11: First Convolutional Layer ﬁlters of dimension 5 × 5 after the training with Torch on the
MNIST images halved by 2 × 2 max pooling.

5.3 Tensor Flow

In this section we will show how to build a CNN to face the MNIST data using TensorFlow. The ﬁrst step
is to import libraries, the Mnist Dataset and to deﬁne main variables. Two additional package are required:
numpy for matrices computations and matplotlib.pyplot for visualization issues.

1 import

t e n s o r f l o w a s

t f

import numpy a s np

3 import m a t p l o t l i b . p y p l o t a s p l t

5 from t e n s o r f l o w . ex amples . t u t o r i a l s . mnist

import

input_data

mnist = input_data . read_data_sets ( ’ MNIST_data ’ , one_hot=True )

x = t f . p l a c e h o l d e r ( t f . f l o a t 3 2 ,
9 y_ = t f . p l a c e h o l d e r ( t f . f l o a t 3 2 ,

shape =[None , 7 8 4 ] )
shape =[None , 1 0 ] )

We deﬁne now some tool functions to specify variable initialization.

1 d e f w e i g h t _ v a r i a b l e ( shape ) :

i n i t i a l = t f . truncated_normal ( shape ,
r e t u r n t f . V a r i a b l e ( i n i t i a l )

s t d d e v =0.1)

5 d e f b i a s _ v a r i a b l e ( shape ) :

i n i t i a l = t f . c o n s t a n t ( 0 . 1 ,
r e t u r n t f . V a r i a b l e ( i n i t i a l )

shape=shape )

The following two function deﬁne convolution and (3-by-3) max pooling. The vector strides speciﬁes how
the ﬁlter or the sliding window move along each dimension. The vector ksize speciﬁes the dimension of the
sliding window. The padding option ’SAME’ automatically adds empty (zero valued) pixels to allow the
convolution to be centered even in the boundary pixels.

1 d e f conv2d ( x , W) :

r e t u r n t f . nn . conv2d ( x , W,

s t r i d e s =[1 , 1 , 1 , 1 ] , padding= ’SAME ’ )

# max p o o l i n g o v e r 3 x3 b l o c k s

40

7

3

7

3

5 d e f max_pool_3x3 ( x ) :

r e t u r n t f . nn . max_pool ( x , k s i z e =[1 , 3 , 3 , 1 ] ,
s t r i d e s =[1 , 3 , 3 , 1 ] , padding= ’SAME ’ )

7

In order to deﬁne the model, we start by reshaping the input (where each sample is provided as 1-D vector)
to its original size, i.e. each sample is represented by a matrix of 28x28 pixels. Then we deﬁne the ﬁrst
convolution layer which computes 12 features by using 5x5 ﬁlters. Finally we perform the ReLU activation
and the ﬁrst max pooling step.

1 # Input

r e s i z e
x_image = t f . r e s h a p e ( x ,

[ − 1 , 2 8 , 2 8 , 1 ] )

# F i r s t

c o n v o l u t i o n l a y e r − 5 x5 f i l t e r s

5 INPUT_C1 = 1 # i n p u t c h a n n e l

OUTPUT_C1 = 12 # output c h a n n e l

( f e a t u r e s )

7 W_conv1 = w e i g h t _ v a r i a b l e ( [ 5 , 5 , INPUT_C1 , OUTPUT_C1 ] )

b_conv1 = b i a s _ v a r i a b l e ( [OUTPUT_C1 ] )

#c o n v o l u t i o n s t e p

11 h_conv1 = t f . nn . r e l u ( conv2d ( x_image , W_conv1) + b_conv1 )

13 #max p o o l i n g s t e p

h_pool1 = max_pool_3x3 ( h_conv1 )

The second convolution layer can be built up in an analogous way.

# Second c o n v o l u t i o n l a y e r

2 INPUT_C2 = OUTPUT_C1

OUTPUT_C2 = 16 # output c h a n n e l

( f e a t u r e s )

4 W_conv2 = w e i g h t _ v a r i a b l e ( [ 5 , 5 , INPUT_C2 , OUTPUT_C2 ] )

b_conv2 = b i a s _ v a r i a b l e ( [OUTPUT_C2 ] )

#c o n v o l u t i o n s t e p

8 h_conv2 = t f . nn . r e l u ( conv2d ( h_pool1 , W_conv2) + b_conv2 )

10 #max p o o l i n g s t e p

h_pool2 = max_pool_3x3 ( h_conv2 )

3

9

6

5

At this point the network returns 16 feature maps 4x4. These will be reshaped to 1–D vectors and given
as input to the last fully connected linear layer. The linear layer is equipped with 1024 hidden units with
ReLU activation functions.

1 # D e f i n i t i o n o f
FS = 4 # f i n a l

s i z e

t h e f u l l y c o n n e c t e d l i n e a r

l a y e r

3 W_fc1 = w e i g h t _ v a r i a b l e ( [ FS ∗ FS ∗ OUTPUT_C2, 1 0 2 4 ] )

b_fc1 = b i a s _ v a r i a b l e ( [ 1 0 2 4 ] )

# Reshape images

7 h _ p o o l 2 _ f l a t = t f . r e s h a p e ( h_pool2 ,

[ −1 , FS ∗ FS ∗ OUTPUT_C2 ] )

41

11

15

5

13

17

25

27

29

31

33

35

9 # hidden l a y e r

h_fc1 = t f . nn . r e l u ( t f . matmul ( h_pool2_flat , W_fc1 ) + b_fc1 )

# Output

l a y e r

13 W_fc2 = w e i g h t _ v a r i a b l e ( [ 1 0 2 4 , 1 0 ] )

b_fc2 = b i a s _ v a r i a b l e ( [ 1 0 ] )

# P r e d i c t i o n

17 y_conv = t f . matmul ( h_fc1 , W_fc2 ) + b_fc2

In the following piece of code we report the optimization process of the deﬁned CNN. As in the standard
ANN case, it is organized in a for loop. This time, we chose the Adam gradient-based optimization by the
function AdamOptimizer. Each epoch performs a training step over mini-batches extracted again by the
dedicated function next_batch(), introduced in Section 4.3. This time the computations are run within
InteractiveSession. The diﬀerence with the regular Session is that an InteractiveSession sets itself as the
default session during building, allowing to run variables without needing to constantly refer to the session
object. As a for instance, the method eval() will implicitly use that session to run operations.

1 c r o s s _ e n t r o p y = t f . reduce_mean ( t f . nn . s o f t m a x _ c r o s s _ e n t r o p y _ w i t h _ l o g i t s ( y_conv , y_) )

t r a i n _ s t e p = t f . t r a i n . AdamOptimizer ( 1 e −4). minimize ( c r o s s _ e n t r o p y )

3 c o r r e c t _ p r e d i c t i o n = t f . e q u a l ( t f . argmax ( y_conv , 1 ) ,

t f . argmax (y_ , 1 ) )

a c c u r a c y = t f . reduce_mean ( t f . c a s t ( c o r r e c t _ p r e d i c t i o n ,

t f . f l o a t 3 2 ) )

s e s s = t f . I n t e r a c t i v e S e s s i o n ( )

7 s e s s . run ( t f . g l o b a l _ v a r i a b l e s _ i n i t i a l i z e r ( ) )

9 # E a r l y s t o p p i n g setup ,

t o check on v a l i d a t i o n s e t

p r e c _ e r r = 10 ∗ ∗ 6 # j u s t a v e r y b i g v a l u e

11 v al_count = 0

val_max_steps = 6

e p o c h s = 100

15 b a t c h _ s i z e = 1000

num_of_batches = 60000/ b a t c h _ s i z e

i =1
19 w h i l e

i <= e p o c h s and v al_count < val_max_steps :

21

p r i n t

’ Epoch : ’ , i , ’ ( E a r l y s t o p p i n g c r i t e r i o n : ’ , val_count , ’ / ’ , val_max_steps , ’ ) ’

23 # t r a i n i n g s t e p
j

f o r

i n r a n g e ( num_of_batches ) :

batch = mnist . t r a i n . next_batch ( b a t c h _ s i z e )
s e s s . run ( t r a i n _ s t e p ,

f e e d _ d i c t ={x : batch [ 0 ] , y_ : batch [ 1 ] } )

# v i s u a l i z e a c c u r a c y each 10 e p o c h s
i %10 == 0 :
i f
t r a i n _ a c c u r a c y = a c c u r a c y . e v a l (

i == 1 o r

f e e d _ d i c t ={x : mnist . t r a i n . images , y_ : mnist . t r a i n . l a b e l s } )

t e s t _ a c c u r a c y = a c c u r a c y . e v a l (

f e e d _ d i c t ={x : mnist . t e s t . images , y_ : mnist . t e s t . l a b e l s } )

p r i n t " \ nAccuracy a t epoch " ,
p r i n t ( " t r a i n a c c u r a c y %g ,

i , " : "

t e s t a c c u r a c y %g \n"%( t r a i n _ a c c u r a c y ,

t e s t _ a c c u r a c y ) )

42

f e e d _ d i c t ={x : mnist . v a l i d a t i o n . images , y_ : mnist . v a l i d a t i o n . l a b e l s } )

37 # v a l i d a t i o n check

c u r r _ e r r = s e s s . run ( c r o s s _ e n t r o p y ,

i f

c u r r _ e r r >= p r e c _ e r r ∗ 0 . 9 9 9 9 :

v al_count = v al_count + 1

39

41

43

45

47

v al_count = 0

p r e c _ e r r = c u r r _ e r r

e l s e :

i+=1

49 p r i n t ( " \n\ n R e s u l t : \ nTest a c c u r a c y %g " % a c c u r a c y . e v a l (

f e e d _ d i c t ={x : mnist . t e s t . images , y_ : mnist . t e s t . l a b e l s } ) )

In this setting, the ﬁnal classiﬁcation accuracy on the test set should be close to the 99%. As in the previous
Sections, in Figure 12 we show the learned ﬁlters of the ﬁrst convolutional layer obtained by:

# G e t t i n g f i l t e r s a s an a r r a y

2 FILTERS = W_conv1 . e v a l ( )

4 f i g = p l t . f i g u r e ( )

f o r

i

i n r a n g e ( np . shape (FILTERS ) [ 3 ] ) :

6

ax = f i g . add_subplot ( 2 , 6 ,
ax . matshow (FILTERS [ : , : , 0 , i ] , cmap= ’ gray ’ )

i +1)

8 p l t . show ( )

In the ﬁrst line eval() computes the current value of W_conv1, saving it in the 4–D numpy array
FILTERS, where the fourth dimension (acceded by the index 3 in np.shape(FILTERS)[3]) corresponds to
the number of ﬁlters.

Figure 12: First Convolutional Layer ﬁlters of dimension 5 × 5 after the training with TensorFlow
on the MNIST images with the described arcitecture.

43

6 A critical comparison

In this Section we would like to outline an overall picture across the presented environments. Even if in
Table 1 we provide a scoring based on some features we thought mainly relevant for Machine Learning
software development, this work would not like to bound this analysis to a poor evaluation. Instead, we
hope to propose an useful guideline to help people trying to approach ANNs and Machine Learning in general,
in order to orientate within the environments depending on personal background and requirements. More
complete and statistically relevant comparisons can be found on the web22, but we try to summarize so as
to help and speed up single and global task developing.

We ﬁrst give general description of each environment, then we try to compare pros and cons on speciﬁc
requirements. At the end, we carry out an indicative numerical analysis on the computational performances
on diﬀerent tasks, which could be also a topic for comparison and discussion.

6.1 Matlab

The programming language is intuitive and the software provides a complete package, allowing the user to
deﬁne and train almost all kind of ANNs architecture without writing a single line of speciﬁc code. The code
parallelization is automatic and the integration with CUDA R(cid:13) is straightforward too. The available built-
in functions are very customizable and optimized, providing fast and extended setting up of experiments
and an easy access to the variable of the network for in-depth analysis. However, enlarging and integrating
Matlabtools require an advanced knowledge of the environment. This could drive the user to start rewriting
its own code from the beginning, leading to a general decay of computational performances. These features
make it perfect as a statistical and analysis toolbox, but maybe a bit slow as developmental environment. The
GUI results sometimes heavy to be handled by the calculator, but, on the other hand, it is very user-friendly
and provides the best graphical data visualization. The documentation is complete and well organized within
the oﬃcial site.

6.2 Torch

The programming language (Lua) can sometimes results a little bit tricky, but it supposed to be the faster
among these languages. It provides all the needed CUDA R(cid:13) integrations and the CPU parallelization au-
tomatic. The module-based structure allows ﬂexibility in the ANNs architecture and it is relatively easy
to extend the provided packages. There are also other powerful packages23, but in general they require to
acquire some expertise to achieve a conscious handling. Torch could be easily used as a prototyping envi-
ronment for speciﬁc and generic algorithms testing. The documentation is spread all over the torch GitHub
repository and sometimes solve speciﬁc issues could not be immediate.

6.3 Tensor Flow

The employment of a programming language as dynamic as Python makes the code scripting light for the
user. The CPU parallelization is automatic, and, exploiting the graph-structure of the computation is easy
to take advantage of GPU computing. It provides a good data visualization and the possibility for beginners
to access to ready to go packages, even if not treated in this document. The power of symbolic computation
involves the user only in the forward step, whereas the backward step is entirely derived by the TensorFlow
environment. This ﬂexibility allows a very fast development for users from any level of expertise.

22Look for example at the webpage http://hammerprinciple.com/therighttool
23We skip the treatment of optim, which provides various Gradient Descent and Back-Propagation procedure

44

6.4 An overall picture on the comparison

As already said, in Table 1 we try to sum up a global comparison trying assigning a score from 1 to 5 on
diﬀerent perspectives. Here below, we explain the main motivation when necessary:

Programming Language All the basic language are quite intuitive.
GPU Integration Matlabis penalized since an extra toolbox is required.

CPU Parallelization All the environments exploit as more core as possible
Function Customizability Matlabscore is lower since integrate well-optimized functions with the pro-

vided ones is diﬃcult

Symbolic Calculus Not expected in Lua

Network Structure Customizability Every kind of network is possible
Data Visualization The interactive Matlabmode outperforms the others
Installation Quite simple for all of them, but the Matlabinteractive GUI is an extra point

OS Compatibility Torch installation is not easy on Windows
Built-In Function Availability Matlabprovided simple-tools with an easy access
Language Performance Matlabinterface can sometimes appear heavy
Development Flexibility Again, Matlabis penalized because it forces medium users to become very
specialized with the language to integrate the provided tools or to write proper code, which in general
can slow down the software development

Programming Language
GPU Integration
CPU Parallelization
Function Customizability
Symbolic Calculus
Network Structure Customizability
Data Visualization
Installation
OS Compatibility
Built-In Function Availability
Language Performance
Development Flexibility
License

Matlab Torch
4
3
5
2
3
5
5
5
5
5
3
2
EULA

3
5
5
4
1
5
2
4
4
4
5
4
BSD (Open source) Apache 2.0 (Open source)

TensorFlow
4
5
5
5
5
5
3
4
5
4
4
5

Table 1: Environments individual scoring

45

6.5 Computational issues

In Table 2 we compare running times for diﬀerent tasks, analyzing the advantages and diﬀerences of
CPU/GPU computing. Results are averaged on 5 trials, carried out in the same machine with an In-
tel(R) Xeon(R) CPU E5-2650 v2 @ 2.60GHz with 32 cores, 66 GB of RAM, and a Geforce GTX 960 with
4GB of memory. The OS is Debian GNU/Linux 8 (jessie). We test a standard Gradient Descent proce-
dure varying the network architecture, the batches size (among Stochastic Gradient Descent (SGD), 1000
samples batch and Full Batch) and the hardware (indicated in HW column). The CNN architecture is the
same of the one proposed in Figure 8. Performances are obtained trying to use optimization procedures as
similar as possible. In practice, it is very diﬃcult to reply the speciﬁc optimization techniques exploited in
Matlabbuilt-in toolboxes. We skip the SGD case for the second architecture (eighth row) in Torch because
of the huge computational time obtained for the ﬁrst architecture. We miss the SGD case for the ANNs
architecture in the Matlabcase since the training function ’trains’ it is not supported for GPU computing
(rows fourth and tenth). As a matter of fact, this could be an uncommon case of study, but we report the
results for best completeness. We skip the CNN Full Batch trials on GPU because of the too large memory
requirement24 .

an

24For
packages)
(including
https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software

comparison
with

computational

on
other

comparison

exhaustive

existent

the

performance

on

several

the

user

can

refer

tasks
to

46

Architecture

Batches size

SGD

1000

Full Batch

Env.

HW

2-Layers ANN

914.56

1000 HUs

4-Layers ANN

893.27

300-300-300 HUs

52.46

3481.00

378.19

911.32

44.66

–

–

–

517.87

1024.29

30.40

46.55

13.82

3.75

1.93

5.40

28.33

52.99

10.95

2.74

1.51

4.73

CNN

7794.33

54.21

647.75

100.06

1850.30

20.22

28.38

24.43

12.00

3.73

1.46

4.92

27.07

20.00

8.83

3.44

1.08

4.27

–

–

–

Matlab

Torch

TF

CPU

GPU

CPU

GPU

GPU

Table 2: Averaged time (in seconds) on 5 running of 10 epochs training with diﬀerent architectures
on the MNIST data within the presented environments. All the architectures are built up using the
ReLU as activation, the softmax as output function and the Cross-Entropy penalty.

47

References

[1] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.

[2] David E Rumelhart, Geoﬀrey E Hinton, and Ronald J Williams. Learning representations by back-

propagating errors. Cognitive modeling, 5(3):1, 1988.

[3] F. Giannini, V. Laveglia, A. Rossi, D. Zanca, and A. Zugarini. NeuralNetworksForBeginners.

https://github.com/AILabUSiena/NeuralNetworksForBeginners, 2016.

[4] The Mathworks, Inc., Natick, Massachusetts. MATLAB version 8.5.0.197613 (R2015a), 2015.

[5] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Cor-
rado, Andy Davis, Jeﬀrey Dean, Matthieu Devin, et al. Tensorﬂow: Large-scale machine learning on
heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.

[6] Y. Lecun, L. Bottou, Y. Bengio, and P. Haﬀner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE, 86(11):2278–2324, Nov 1998.

[7] Kunihiko Fukushima. Neocognitron: A hierarchical neural network capable of visual pattern recognition.

Neural networks, 1(2):119–130, 1988.

[8] Thomas Serre, Lior Wolf, Stanley Bileschi, Maximilian Riesenhuber, and Tomaso Poggio. Robust object
recognition with cortex-like mechanisms. IEEE transactions on pattern analysis and machine intelligence,
29(3), 2007.

48

