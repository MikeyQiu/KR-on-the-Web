A Reductions Approach to Fair Classiﬁcation

8
1
0
2
 
l
u
J
 
6
1
 
 
]

G
L
.
s
c
[
 
 
3
v
3
5
4
2
0
.
3
0
8
1
:
v
i
X
r
a

Alekh Agarwal 1 Alina Beygelzimer 2 Miroslav Dud´ık 1 John Langford 1 Hanna Wallach 1

Abstract

We present a systematic approach for achieving
fairness in a binary classiﬁcation setting. While
we focus on two well-known quantitative deﬁni-
tions of fairness, our approach encompasses many
other previously studied deﬁnitions as special
cases. The key idea is to reduce fair classiﬁcation
to a sequence of cost-sensitive classiﬁcation
problems, whose solutions yield a randomized
classiﬁer with the lowest (empirical) error subject
to the desired constraints. We introduce two
reductions that work for any representation of the
cost-sensitive classiﬁer and compare favorably
to prior baselines on a variety of data sets, while
overcoming several of their disadvantages.

1. Introduction

Over the past few years, the media have paid considerable
attention to machine learning systems and their ability to
inadvertently discriminate against minorities, historically
disadvantaged populations, and other protected groups when
allocating resources (e.g., loans) or opportunities (e.g., jobs).
In response to this scrutiny—and driven by ongoing debates
and collaborations with lawyers, policy-makers, social sci-
entists, and others (e.g., Barocas & Selbst, 2016)—machine
learning researchers have begun to turn their attention to the
topic of “fairness in machine learning,” and, in particular, to
the design of fair classiﬁcation and regression algorithms.

In this paper we study the task of binary classiﬁcation sub-
ject to fairness constraints with respect to a pre-deﬁned pro-
tected attribute, such as race or sex. Previous work in this
area can be divided into two broad groups of approaches.

The ﬁrst group of approaches incorporate speciﬁc quanti-
tative deﬁnitions of fairness into existing machine learning

1Microsoft Research, New York 2Yahoo! Research, New York.
Correspondence to: A. Agarwal <alekha@microsoft.com>,
Dud´ık
A.
<mdudik@microsoft.com>, J. Langford <jcl@microsoft.com>,
H. Wallach <wallach@microsoft.com>.

Beygelzimer <beygel@gmail.com>,

M.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

methods, often by relaxing the desired deﬁnitions of fair-
ness, and only enforcing weaker constraints, such as lack of
correlation (e.g., Woodworth et al., 2017; Zafar et al., 2017;
Johnson et al., 2016; Kamishima et al., 2011; Donini et al.,
2018). The resulting fairness guarantees typically only hold
under strong distributional assumptions, and the approaches
are tied to speciﬁc families of classiﬁers, such as SVMs.

The second group of approaches eliminate the restriction
to speciﬁc classiﬁer families and treat the underlying clas-
siﬁcation method as a “black box,” while implementing
a wrapper that either works by pre-processing the data or
post-processing the classiﬁer’s predictions (e.g., Kamiran
& Calders, 2012; Feldman et al., 2015; Hardt et al., 2016;
Calmon et al., 2017). Existing pre-processing approaches
are speciﬁc to particular deﬁnitions of fairness and typically
seek to come up with a single transformed data set that will
work across all learning algorithms, which, in practice, leads
to classiﬁers that still exhibit substantial unfairness (see our
evaluation in Section 4). In contrast, post-processing allows
a wider range of fairness deﬁnitions and results in provable
fairness guarantees. However, it is not guaranteed to ﬁnd the
most accurate fair classiﬁer, and requires test-time access to
the protected attribute, which might not be available.

We present a general-purpose approach that has the key
advantage of this second group of approaches—i.e., the
underlying classiﬁcation method is treated as a black
box—but without the noted disadvantages. Our approach
encompasses a wide range of fairness deﬁnitions,
is
guaranteed to yield the most accurate fair classiﬁer, and
does not require test-time access to the protected attribute.
Speciﬁcally, our approach allows any deﬁnition of fairness
that can be formalized via linear inequalities on conditional
such as demographic parity or equalized
moments,
odds (see Section 2.1). We show how binary classiﬁcation
subject to these constraints can be reduced to a sequence
of cost-sensitive classiﬁcation problems. We require only
black-box access to a cost-sensitive classiﬁcation algorithm,
which does not need to have any knowledge of the desired
deﬁnition of fairness or protected attribute. We show that
the solutions to our sequence of cost-sensitive classiﬁcation
problems yield a randomized classiﬁer with the lowest
(empirical) error subject to the desired fairness constraints.

Corbett-Davies et al. (2017) and Menon & Williamson

A Reductions Approach to Fair Classiﬁcation

(2018) begin with a similar goal to ours, but they analyze
the Bayes optimal classiﬁer under fairness constraints in the
limit of inﬁnite data. In contrast, our focus is algorithmic,
our approach applies to any classiﬁer family, and we obtain
ﬁnite-sample guarantees. Dwork et al. (2018) also begin
with a similar goal to ours. Their approach partitions the
training examples into subsets according to protected at-
tribute values and then leverages transfer learning to jointly
learn from these separate data sets. Our approach avoids par-
titioning the data and assumes access only to a classiﬁcation
algorithm rather than a transfer learning algorithm.

A preliminary version of this paper appeared at the FAT/ML
workshop (Agarwal et al., 2017), and led to extensions with
more general optimization objectives (Alabi et al., 2018)
and combinatorial protected attributes (Kearns et al., 2018).

In the next section, we formalize our problem. While we
focus on two well-known quantitative deﬁnitions of fairness,
our approach also encompasses many other previously stud-
ied deﬁnitions of fairness as special cases. In Section 3, we
describe our reductions approach to fair classiﬁcation and
its guarantees in detail. The experimental study in Section 4
shows that our reductions compare favorably to three base-
lines, while overcoming some of their disadvantages and
also offering the ﬂexibility of picking a suitable accuracy–
fairness tradeoff. Our results demonstrate the utility of
having a general-purpose approach for combining machine
learning methods and quantitative fairness deﬁnitions.

2. Problem Formulation

We consider a binary classiﬁcation setting where the training
examples consist of triples (X, A, Y ), where X ∈ X is a fea-
ture vector, A ∈ A is a protected attribute, and Y ∈ {0, 1}
is a label. The feature vector X can either contain the pro-
tected attribute A as one of the features or contain other fea-
tures that are arbitrarily indicative of A. For example, if the
classiﬁcation task is to predict whether or not someone will
default on a loan, each training example might correspond
to a person, where X represents their demographics, income
level, past payment history, and loan amount; A represents
their race; and Y represents whether or not they defaulted on
that loan. Note that X might contain their race as one of the
features or, for example, contain their zipcode—a feature
that is often correlated with race. Our goal is to learn an ac-
curate classiﬁer h : X → {0, 1} from some set (i.e., family)
of classiﬁers H, such as linear threshold rules, decision trees,
or neural nets, while satisfying some deﬁnition of fairness.
Note that the classiﬁers in H do not explicitly depend on A.

2.1. Fairness Deﬁnitions

We focus on two well-known quantitative deﬁnitions of
fairness that have been considered in previous work on

fair classiﬁcation; however, our approach also encompasses
many other previously studied deﬁnitions of fairness as
special cases, as we explain at the end of this section.

The ﬁrst deﬁnition—demographic (or statistical) parity—
can be thought of as a stronger version of the US Equal
Employment Opportunity Commission’s “four-ﬁfths rule,”
which requires that the “selection rate for any race, sex, or
ethnic group [must be at least] four-ﬁfths (4/5) (or eighty
percent) of the rate for the group with the highest rate.”1
Deﬁnition 1 (Demographic parity—DP). A classiﬁer h
satisﬁes demographic parity under a distribution over
(X, A, Y ) if its prediction h(X) is statistically indepen-
dent of the protected attribute A—that is, if P[h(X) = ˆy |
A = a] = P[h(X) = ˆy] for all a, ˆy. Because ˆy ∈ {0, 1},
this is equivalent to E[h(X) | A = a] = E[h(X)] for all a.

The second deﬁnition—equalized odds—was recently pro-
posed by Hardt et al. (2016) to remedy two previously noted
ﬂaws with demographic parity (Dwork et al., 2012). First,
demographic parity permits a classiﬁer which accurately
classiﬁes data points with one value A = a, such as the
value a with the most data, but makes random predictions
for data points with A (cid:54)= a as long as the probabilities of
h(X) = 1 match. Second, demographic parity rules out
perfect classiﬁers whenever Y is correlated with A.
In
contrast, equalized odds suffers from neither of these ﬂaws.

Deﬁnition 2 (Equalized odds—EO). A classiﬁer h satis-
ﬁes equalized odds under a distribution over (X, A, Y )
if its prediction h(X) is conditionally independent of
the protected attribute A given the label Y —that is, if
P[h(X) = ˆy | A = a, Y = y] = P[h(X) = ˆy | Y = y] for
all a, y, and ˆy. Because ˆy ∈ {0, 1}, this is equivalent to
E[h(X) | A = a, Y = y] = E[h(X) | Y = y] for all a, y.

We now show how each deﬁnition can be viewed as a special
case of a general set of linear constraints of the form

Mµ(h) ≤ c,

(1)

where matrix M ∈ R|K|×|J| and vector c ∈ R|K| describe
the linear constraints, each indexed by k ∈ K, and µ(h) ∈
R|J| is a vector of conditional moments of the form

µj(h) = E(cid:2) gj(X, A, Y, h(X)) (cid:12)

(cid:12) Ej

(cid:3)

for j ∈ J,

where gj : X × A × {0, 1} × {0, 1} → [0, 1] and Ej is
an event deﬁned with respect to (X, A, Y ). Crucially, gj
depends on h, while Ej cannot depend on h in any way.
Example 1 (DP). In a binary classiﬁcation setting, demo-
graphic parity can be expressed as a set of |A| equality
constraints, each of the form E[h(X) | A = a] = E[h(X)].
Letting J = A ∪ {(cid:63)}, gj(X, A, Y, h(X)) = h(X) for all j,

1See the Uniform Guidelines on Employment Selection Proce-

dures, 29 C.F.R. §1607.4(D) (2015).

A Reductions Approach to Fair Classiﬁcation

Ea = {A = a}, and E(cid:63) = {True}, where {True} refers to
the event encompassing all points in the sample space, each
equality constraint can be expressed as µa(h) = µ(cid:63)(h).2
Finally, because each such constraint can be equivalently
expressed as a pair of inequality constraints of the form

µa(h) − µ(cid:63)(h) ≤ 0
−µa(h) + µ(cid:63)(h) ≤ 0,

demographic parity can be expressed as equation (1), where
K = A×{+, −}, M(a,+),a(cid:48) = 1{a(cid:48) = a}, M(a,+),(cid:63) = −1,
M(a,−),a(cid:48) = −1{a(cid:48) = a}, M(a,−),(cid:63) = 1, and c = 0.
Expressing each equality constraint as a pair of inequality
constraints allows us to control the extent to which each
constraint is enforced by positing ck > 0 for some (or all) k.
In a binary classiﬁcation set-
Example 2 (EO).
equalized odds can be expressed as a set
ting,
of 2 |A|
form
constraints,
E[h(X) | A = a, Y = y] = E[h(X) | Y = y]. Letting
J = (A ∪ {(cid:63)}) × {0, 1}, gj(X, A, Y, h(X)) = h(X) for
all j, E(a,y) = {A = a, Y = y}, and E((cid:63),y) = {Y = y},
each equality constraint can be equivalently expressed as

each of

equality

the

µ(a,y)(h) − µ((cid:63),y)(h) ≤ 0
−µ(a,y)(h) + µ((cid:63),y)(h) ≤ 0.

can be

equalized odds

As a result,
expressed
as equation (1), where K = A × Y × {+, −},
M(a,y,+),(a(cid:48),y(cid:48)) = 1{a(cid:48)= a, y(cid:48)= y}, M(a,y,+),((cid:63),y(cid:48)) = −1,
M(a,y,−),(a(cid:48),y(cid:48)) = −1{a(cid:48)= a, y(cid:48)= y}, M(a,y,−),((cid:63),y(cid:48)) = 1,
and c = 0. Again, we can posit ck > 0 for some (or all) k
to allow small violations of some (or all) of the constraints.

Although we omit the details, we note that many other pre-
viously studied deﬁnitions of fairness can also be expressed
as equation (1). For example, equality of opportunity (Hardt
et al., 2016) (also known as balance for the positive class;
Kleinberg et al., 2017), balance for the negative class (Klein-
berg et al., 2017), error-rate balance (Chouldechova,
2017), overall accuracy equality (Berk et al., 2017), and
treatment equality (Berk et al., 2017) can all be expressed
as equation (1); in contrast, calibration (Kleinberg et al.,
2017) and predictive parity (Chouldechova, 2017) cannot
because to do so would require the event Ej to depend on
h. We note that our approach can also be used to satisfy
multiple deﬁnitions of fairness, though if these deﬁnitions
are mutually contradictory, e.g., as described by Kleinberg
et al. (2017), then our guarantees become vacuous.

2.2. Fair Classiﬁcation

In a standard (binary) classiﬁcation setting, the goal is to
learn the classiﬁer h ∈ H with the minimum classiﬁcation

2Note that µ(cid:63)(h) = E[h(X) | True] = E[h(X)].

error: err(h) := P[h(X) (cid:54)= Y ]. However, because our
goal is to learn the most accurate classiﬁer while satisfying
fairness constraints, as formalized above, we instead seek to
ﬁnd the solution to the constrained optimization problem3

min
h∈H

err(h)

subject to Mµ(h) ≤ c.

(2)

Furthermore, rather than just considering classiﬁers in the
set H, we can enlarge the space of possible classiﬁers by
considering randomized classiﬁers that can be obtained via
a distribution over H. By considering randomized classi-
ﬁers, we can achieve better accuracy–fairness tradeoffs than
would otherwise be possible. A randomized classiﬁer Q
makes a prediction by ﬁrst sampling a classiﬁer h ∈ H
from Q and then using h to make the prediction. The result-
ing classiﬁcation error is err(Q) = (cid:80)
h∈H Q(h) err(h) and
the conditional moments are µ(Q) = (cid:80)
h∈H Q(h)µ(h)
(see Appendix A for the derivation). Thus we seek to solve

min
Q∈∆

err(Q)

subject to Mµ(Q) ≤ c,

(3)

where ∆ is the set of all distributions over H.

In practice, we do not know the true distribution over
(X, A, Y ) and only have access to a data set of training ex-
amples {(Xi, Ai, Yi)}n
i=1. We therefore replace err(Q) and
µ(Q) in equation (3) with their empirical versions (cid:99)err(Q)
and (cid:98)µ(Q). Because of the sampling error in (cid:98)µ(Q), we
also allow errors in satisfying the constraints by setting
(cid:98)ck = ck + εk for all k, where εk ≥ 0. After these modiﬁca-
tions, we need to solve the empirical version of equation (3):

Q∈∆ (cid:99)err(Q)
min

subject to M(cid:98)µ(Q) ≤ (cid:98)c.

(4)

3. Reductions Approach

We now show how the problem (4) can be reduced to a se-
quence of cost-sensitive classiﬁcation problems. We further
show that the solutions to our sequence of cost-sensitive clas-
siﬁcation problems yield a randomized classiﬁer with the
lowest (empirical) error subject to the desired constraints.

3.1. Cost-sensitive Classiﬁcation

We assume access to a cost-sensitive classiﬁcation algorithm
for the set H. The input to such an algorithm is a data set
of training examples {(Xi, C 0
i and C 1
i
denote the losses—costs in this setting—for predicting the
labels 0 or 1, respectively, for Xi. The algorithm outputs

i=1, where C 0

i , C 1

i )}n

arg min
h∈H

n
(cid:88)

i=1

h(Xi) C 1

i + (1 − h(Xi)) C 0
i .

(5)

3We consider misclassiﬁcation error for concreteness, but all
the results in this paper apply to any error of the form err(h) =
E[gerr(X, A, Y, h(X))], where gerr(·, ·, ·, ·) ∈ [0, 1].

A Reductions Approach to Fair Classiﬁcation

min
Q∈∆

max
λ∈R|K|

+

L(Q, λ).

(6)

L( (cid:98)Q, (cid:98)λ) ≤ L(Q, (cid:98)λ) + ν

This abstraction allows us to specify different costs for dif-
ferent training examples, which is essential for incorporat-
ing fairness constraints. Moreover, efﬁcient cost-sensitive
classiﬁcation algorithms are readily available for several
common classiﬁer representations (e.g., Beygelzimer et al.,
2005; Langford & Beygelzimer, 2005; Fan et al., 1999). In
particular, equation (5) is equivalent to a weighted classi-
ﬁcation problem, where the input consists of labeled ex-
amples {(Xi, Yi, Wi)}n
i=1 with Yi ∈ {0, 1} and Wi ≥ 0,
and the goal is to minimize the weighted classiﬁcation er-
ror (cid:80)n
i=1 Wi 1{h(Xi) (cid:54)= Yi}. This is equivalent to equa-
tion (5) if we set Wi = |C 0
i }.

i | and Yi = 1{C 0

i ≥ C 1

i − C 1

3.2. Reduction

To derive our fair classiﬁcation algorithm, we rewrite equa-
tion (4) as a saddle point problem. We begin by introducing
a Lagrange multiplier λk ≥ 0 for each of the |K| constraints,
summarized as λ ∈ R|K|

+ , and form the Lagrangian
L(Q, λ) = (cid:99)err(Q) + λ(cid:62)(cid:0)M(cid:98)µ(Q) − (cid:98)c(cid:1).

Thus, equation (4) is equivalent to

For computational and statistical reasons, we impose an
additional constraint on the (cid:96)1 norm of λ and seek to simul-
taneously ﬁnd the solution to the constrained version of (6)
as well as its dual, obtained by switching min and max:

min
Q∈∆

max

L(Q, λ),

λ∈R|K|

+ , (cid:107)λ(cid:107)1≤B

max
+ , (cid:107)λ(cid:107)1≤B

λ∈R|K|

min
Q∈∆

L(Q, λ).

(P)

(D)

Because L is linear in Q and λ and the domains of Q and
λ are convex and compact, both problems have solutions
(which we denote by Q† and λ†) and the minimum value of
(P) and the maximum value of (D) are equal and coincide
with L(Q†, λ†). Thus, (Q†, λ†) is the saddle point of L
(Corollary 37.6.2 and Lemma 36.2 of Rockafellar, 1970).

We ﬁnd the saddle point by using the standard scheme of
Freund & Schapire (1996), developed for the equivalent
problem of solving for an equilibrium in a zero-sum game.
From game-theoretic perspective, the saddle point can be
viewed as an equilibrium of a game between two players:
the Q-player choosing Q and the λ-player choosing λ. The
Lagrangian L(Q, λ) speciﬁes how much the Q-player has to
pay to the λ-player after they make their choices. At the sad-
dle point, neither player wants to deviate from their choice.

Our algorithm ﬁnds an approximate equilibrium in which
neither player can gain more than ν by changing their choice

Algorithm 1 Exp. gradient reduction for fair classiﬁcation

Input: training examples {(Xi, Yi, Ai)}n

i=1

fairness constraints speciﬁed by gj, Ej, M, (cid:98)c
bound B, accuracy ν, learning rate η

Set θ1 = 0 ∈ R|K|
for t = 1, 2, . . . do
Set λt,k = B
1+(cid:80)
ht ← BESTh(λt)
(cid:98)Qt ← 1
t
(cid:98)λt ← 1
t

(cid:80)t

(cid:80)t

νt ← max
if νt ≤ ν then

Return ( (cid:98)Qt, (cid:98)λt)

exp{θk}

k(cid:48) ∈K exp{θk(cid:48) } for all k ∈ K
(cid:16)

(cid:17)

t(cid:48)=1 ht(cid:48), L ← L

t(cid:48)=1 λt(cid:48), L ← L
(cid:110)

(cid:16)

(cid:98)Qt, BESTλ( (cid:98)Qt)
(cid:17)

BESTh((cid:98)λt), (cid:98)λt
(cid:111)

L( (cid:98)Qt, (cid:98)λt) − L, L − L( (cid:98)Qt, (cid:98)λt)

end if
Set θt+1 = θt + η (M(cid:98)µ(ht) − (cid:98)c)

end for

(where ν > 0 is an input to the algorithm). Such an approx-
imate equilibrium corresponds to a ν-approximate saddle
point of the Lagrangian, which is a pair ( (cid:98)Q, (cid:98)λ), where

for all Q ∈ ∆,
for all λ ∈ R|K|

L( (cid:98)Q, (cid:98)λ) ≥ L( (cid:98)Q, λ) − ν

+ , (cid:107)λ(cid:107)1 ≤ B.

We proceed iteratively by running a no-regret algorithm for
the λ-player, while executing the best response of the Q-
player. Following Freund & Schapire (1996), the average
play of both players converges to the saddle point. We run
the exponentiated gradient algorithm (Kivinen & Warmuth,
1997) for the λ-player and terminate as soon as the subop-
timality of the average play falls below the pre-speciﬁed
accuracy ν. The best response of the Q-player can always
be chosen to put all of the mass on one of the candidate
classiﬁers h ∈ H, and can be implemented by a single call
to a cost-sensitive classiﬁcation algorithm for the set H.

Algorithm 1 fully implements this scheme, except for the
functions BESTλ and BESTh, which correspond to the best-
response algorithms of the two players. (We need the best
response of the λ-player to evaluate whether the subopti-
mality of the current average play has fallen below ν.) The
two best response functions can be calculated as follows.

BESTλ(Q): the best response of the λ-player. The best
response of the λ-player for a given Q is any maximizer of
L(Q, λ) over all valid λs. In our setting, it can always be
chosen to be either 0 or put all of the mass on the most vio-
lated constraint. Letting (cid:98)γ(Q) := M(cid:98)µ(Q) and letting ek de-
note the kth vector of the standard basis, BESTλ(Q) returns
(cid:40)

0
Bek∗

if (cid:98)γ(Q) ≤ (cid:98)c,
otherwise, where k∗ = arg maxk[(cid:98)γk(Q) − (cid:98)ck].

A Reductions Approach to Fair Classiﬁcation

BESTh(λ): the best response of the Q-player. Here, the
best response minimizes L(Q, λ) over all Qs in the simplex.
Because L is linear in Q, the minimizer can always be cho-
sen to put all of the mass on a single classiﬁer h. We show
how to obtain the classiﬁer constituting the best response
via a reduction to cost-sensitive classiﬁcation. Letting pj :=
(cid:98)P[Ej] be the empirical event probabilities, the Lagrangian
for Q which puts all of the mass on a single h is then
L(h, λ) = (cid:99)err(h) + λ(cid:62)(cid:0)M(cid:98)µ(h) − (cid:98)c(cid:1)
(cid:88)
= (cid:98)E(cid:2)1{h(X) (cid:54)= Y }(cid:3) − λ(cid:62)

Mk,jλk (cid:98)µj(h)

(cid:98)c +

k,j

= −λ(cid:62)

(cid:88)

+

k,j

(cid:98)c + (cid:98)E(cid:2)1{h(X) (cid:54)= Y }(cid:3)
(cid:104)
Mk,jλk
(cid:98)E
pj

gj

(cid:0)X,A,Y,h(X)(cid:1) 1{(X,A,Y ) ∈ Ej}

(cid:105)
.

Assuming a data set of training examples {(Xi, Ai, Yi)}n
i=1,
the minimization of L(h, λ) over h then corresponds to cost-
sensitive classiﬁcation on {(Xi, C 0

i=1 with costs4

i , C 1

i )}n

Mk,jλk
pj

C 0

i = 1{Yi (cid:54)= 0}
(cid:88)

+

C 1

k,j
i = 1{Yi (cid:54)= 1}
(cid:88)

Mk,jλk
pj

+

k,j

gj(Xi,Ai,Yi, 0) 1{(Xi,Ai,Yi) ∈ Ej}

gj(Xi,Ai,Yi, 1) 1{(Xi,Ai,Yi) ∈ Ej}.

needing more iterations to reach any given suboptimality. In
particular, as we derive in the theorem, achieving subopti-
mality ν may need up to 4ρ2B2 log(|K| + 1) / ν2 iterations.
Example 3 (DP). Using the matrix M for demographic
parity as described in Section 2, the cost-sensitive reduction
for a vector of Lagrange multipliers λ uses costs

C 0

i = 1{Yi (cid:54)= 0}, C 1

i = 1{Yi (cid:54)= 1} +

λAi
pAi

(cid:88)

−

λa,

a∈A

where pa := (cid:98)P[A = a] and λa := λ(a,+) − λ(a,−), effec-
tively replacing two non-negative Lagrange multipliers by a
single multiplier, which can be either positive or negative.
Because ck = 0 for all k, (cid:98)ck = εk. Furthermore, because
all empirical moments are bounded in [0, 1], we can assume
εk ≤ 1, which yields the bound ρ ≤ 2. Thus, Algorithm 1
terminates in at most 16B2 log(2 |A| + 1) / ν2 iterations.
Example 4 (EO). For equalized odds, the cost-sensitive
reduction for a vector of Lagrange multipliers λ uses costs

C 0

i = 1{Yi (cid:54)= 0},

C 1

i = 1{Yi (cid:54)= 1} +

λ(Ai,Yi)
p(Ai,Yi)

(cid:88)

−

a∈A

λ(a,Yi)
p((cid:63),Yi)

,

where p(a,y) := (cid:98)P[A = a, Y = y], p((cid:63),y) := (cid:98)P[Y = y], and
λ(a,y) := λ(a,y,+) − λ(a,y,−). If we again assume εk ≤ 1,
then we obtain the bound ρ ≤ 2. Thus, Algorithm 1 termi-
nates in at most 16B2 log(4 |A| + 1) / ν2 iterations.

Theorem 1. Letting ρ := maxh(cid:107)M(cid:98)µ(h) − (cid:98)c(cid:107)∞, Algo-
rithm 1 satisﬁes the inequality

3.3. Error Analysis

νt ≤

B log(|K| + 1)
ηt

+ ηρ2B.

Thus, for η = ν
saddle point of L in at most 4ρ2B2 log(|K|+1)

2ρ2B , Algorithm 1 will return a ν-approximate

iterations.

ν2

√

This theorem, proved in Appendix B, bounds the subopti-
mality νt of the average play ( (cid:98)Qt, (cid:98)λt), which is equal to its
suboptimality as a saddle point. The right-hand side of the
bound is optimized by η = (cid:112)log(|K| + 1) / (ρ
t), lead-
ing to the bound νt ≤ 2ρB(cid:112)log(|K| + 1) / t. This bound
decreases with the number of iterations t and grows very
slowly with the number of constraints |K|. The quantity ρ
is a problem-speciﬁc constant that bounds how much any
single classiﬁer h ∈ H can violate the desired set of fair-
ness constraints. Finally, B is the bound on the (cid:96)1-norm of
λ, which we introduced to enable this speciﬁc algorithmic
scheme. In general, larger values of B will bring the prob-
lem (P) closer to (6), and thus also to (4), but at the cost of

4For general error, err(h) = E[gerr(X, A, Y, h(X))], the costs
i contain, respectively, the terms gerr(Xi, Ai, Yi, 0) and

C 0
gerr(Xi, Ai, Yi, 1) instead of 1{Yi (cid:54)= 0} and 1{Yi (cid:54)= 1}.

i and C 1

Our ultimate goal, as formalized in equation (3), is to
minimize the classiﬁcation error while satisfying fairness
constraints under a true but unknown distribution over
(X, A, Y ). In the process of deriving Algorithm 1, we in-
troduced three different sources of error. First, we replaced
the true classiﬁcation error and true moments with their
empirical versions. Second, we introduced a bound B on
the magnitude of λ. Finally, we only run the optimization
algorithm for a ﬁxed number of iterations, until it reaches
suboptimality level ν. The ﬁrst source of error, due to the
use of empirical rather than true quantities, is unavoidable
and constitutes the underlying statistical error. The other two
sources of error, the bound B and the suboptimality level ν,
stem from the optimization algorithm and can be driven
arbitrarily small at the cost of additional iterations. In this
section, we show how the statistical error and the optimiza-
tion error affect the true accuracy and the fairness of the ran-
domized classiﬁer returned by Algorithm 1—in other words,
how well Algorithm 1 solves our original problem (3).

To bound the statistical error, we use the Rademacher
complexity of the classiﬁer family H, which we denote
by Rn(H), where n is the number of training examples.
We assume that Rn(H) ≤ Cn−α for some C ≥ 0 and

A Reductions Approach to Fair Classiﬁcation

α ≤ 1/2. We note that α = 1/2 in the vast majority
including norm-bounded linear
of classiﬁer families,
functions (see Theorem 1 of Kakade et al., 2009), neural
networks (see Theorem 18 of Bartlett & Mendelson, 2002),
and classiﬁer families with bounded VC dimension (see
Lemma 4 and Theorem 6 of Bartlett & Mendelson, 2002).

Recall that in our empirical optimization problem we as-
sume that (cid:98)ck = ck + εk, where εk ≥ 0 are error bounds that
account for the discrepancy between µ(Q) and (cid:98)µ(Q). In
our analysis, we assume that these error bounds have been
set in accordance with the Rademacher complexity of H.
Assumption 1. There exists C, C (cid:48) ≥ 0 and α ≤ 1/2
such that Rn(H) ≤ Cn−α and εk = C (cid:48) (cid:80)
j∈J|Mk,j|n−α
,
where nj is the number of data points that fall in Ej,

j

nj := (cid:12)
(cid:12)

(cid:8)i : (Xi, Ai, Yi) ∈ Ej

(cid:9)(cid:12)
(cid:12).

The optimization error can be bounded via a careful analy-
sis of the Lagrangian and the optimality conditions of (P)
and (D). Combining the three different sources of error
yields the following bound, which we prove in Appendix C.
Theorem 2. Let Assumption 1 hold for C (cid:48) ≥ 2C +
2 + (cid:112)ln(4/δ) / 2, where δ > 0. Let ( (cid:98)Q, (cid:98)λ) be any ν-
approximate saddle point of L, let Q(cid:63) minimize err(Q) sub-
j = P[Ej]. Then, with proba-
ject to Mµ(Q) ≤ c, and let p(cid:63)
bility at least 1 − (|J| + 1)δ, the distribution (cid:98)Q satisﬁes

err( (cid:98)Q) ≤ err(Q(cid:63)) + 2ν + (cid:101)O(n−α),
1+2ν
B

γk( (cid:98)Q) ≤ ck +

(cid:88)

+

j∈J

|Mk,j| (cid:101)O(n−α
j )

for all k,

where (cid:101)O(·) suppresses polynomial dependence on ln(1/δ).
If np(cid:63)

j ≥ 8 log(2/δ) for all j, then, for all k,

γk( (cid:98)Q) ≤ ck +

1+2ν
B

(cid:88)

+

j∈J

|Mk,j| (cid:101)O

(cid:16)

(np(cid:63)

j )−α(cid:17)

.

the solution returned by Algorithm 1
In other words,
achieves the lowest feasible classiﬁcation error on the true
distribution up to the optimization error, which grows lin-
early with ν, and the statistical error, which grows as n−α.
Therefore, if we want to guarantee that the optimization er-
ror does not dominate the statistical error, we should set ν ∝
n−α. The fairness constraints on the true distribution are
satisﬁed up to the optimization error (1 + 2ν) /B and up to
the statistical error. Because the statistical error depends on
the moments, and the error in estimating the moments grows
as n−α
j ≥ n−α, we can set B ∝ nα to guarantee that the op-
timization error does not dominate the statistical error. Com-
bining this reasoning with the learning rate setting of Theo-
rem 1 yields the following theorem (proved in Appendix C).
Theorem 3. Let ρ := maxh(cid:107)M(cid:98)µ(h) − (cid:98)c(cid:107)∞. Let Assump-
tion 1 hold for C (cid:48) ≥ 2C + 2 + (cid:112)ln(4/δ) / 2, where δ > 0.

Let Q(cid:63) minimize err(Q) subject to Mµ(Q) ≤ c. Then
Algorithm 1 with ν ∝ n−α, B ∝ nα and η ∝ ρ−2n−2α ter-
minates in O(ρ2n4α ln |K|) iterations and returns (cid:98)Q, which
with probability at least 1 − (|J| + 1)δ satisﬁes

err( (cid:98)Q) ≤ err(Q(cid:63)) + (cid:101)O(n−α),
γk( (cid:98)Q) ≤ ck +

|Mk,j| (cid:101)O(n−α

(cid:88)

j

)

j∈J

for all k.

Example 5 (DP). If na denotes the number of training ex-
amples with Ai = a, then Assumption 1 states that we
should set ε(a,+) = ε(a,−) = C (cid:48)(n−α
a + n−α) and Theo-
rem 3 then shows that for a suitable setting of C (cid:48), ν, B,
and η, Algorithm 1 will return a randomized classiﬁer (cid:98)Q
with the lowest feasible classiﬁcation error up to (cid:101)O(n−α)
while also approximately satisfying the fairness constraints

(cid:12)
(cid:12)
(cid:12)

(cid:12)
E[h(X) | A = a] − E[h(X)]
(cid:12) ≤ (cid:101)O(n−α
(cid:12)
a )

for all a,

where E is with respect to (X, A, Y ) as well as h ∼ (cid:98)Q.
Example 6 (EO). Similarly, if n(a,y) denotes the number
of examples with Ai = a and Yi = y and n((cid:63),y) denotes the
number of examples with Yi = y, then Assumption 1 states
that we should set ε(a,y,+) = ε(a,y,−) = C (cid:48)(n−α
((cid:63),y))
and Theorem 3 then shows that for a suitable setting of C (cid:48), ν,
B, and η, Algorithm 1 will return a randomized classiﬁer (cid:98)Q
with the lowest feasible classiﬁcation error up to (cid:101)O(n−α)
while also approximately satisfying the fairness constraints

(a,y) + n−α

(cid:12)
(cid:12)
(cid:12)

(cid:12)
E[h(X) | A = a, Y = y] − E[h(X) | Y = y]
(cid:12) ≤ (cid:101)O(n−α
(cid:12)

(a,y))

for all a, y. Again, E includes randomness under the true
distribution over (X, A, Y ) as well as h ∼ (cid:98)Q.

3.4. Grid Search

In some situations, it is preferable to select a deterministic
classiﬁer, even if that means a lower accuracy or a modest vi-
olation of the fairness constraints. A set of candidate classi-
ﬁers can be obtained from the saddle point (Q†, λ†). Specif-
ically, because Q† is a minimizer of L(Q, λ†) and L is
linear in Q, the distribution Q† puts non-zero mass only on
classiﬁers that are the Q-player’s best responses to λ†. If we
knew λ†, we could retrieve one such best response via the re-
duction to cost-sensitive learning introduced in Section 3.2.
We can compute λ† using Algorithm 1, but when the number
of constraints is very small, as is the case for demographic
parity or equalized odds with a binary protected attribute,
it is also reasonable to consider a grid of values λ, calculate
the best response for each value, and then select the value
with the desired tradeoff between accuracy and fairness.
Example 7 (DP). When the protected attribute is binary,
e.g., A ∈ {a, a(cid:48)}, then the grid search can in fact be con-
ducted in a single dimension. The reduction formally takes

A Reductions Approach to Fair Classiﬁcation

two real-valued arguments λa and λa(cid:48), and then adjusts the
costs for predicting h(Xi) = 1 by the amounts

δa =

− λa − λa(cid:48)

and δa(cid:48) =

− λa − λa(cid:48),

λa(cid:48)
pa(cid:48)

λa
pa

respectively, on the training examples with Ai = a and
Ai = a(cid:48). These adjustments satisfy paδa + pa(cid:48)δa(cid:48) = 0,
so instead of searching over λa and λa(cid:48), we can carry out
the grid search over δa alone and apply the adjustment
δa(cid:48) = −paδa/pa(cid:48) to the protected attribute value a(cid:48).
With three attribute values, e.g., A ∈ {a, a(cid:48), a(cid:48)(cid:48)}, we sim-
ilarly have paδa + pa(cid:48)δa(cid:48) + pa(cid:48)(cid:48)δa(cid:48)(cid:48) = 0, so it sufﬁces to
conduct grid search in two dimensions rather than three.
Example 8 (EO). If A ∈ {a, a(cid:48)}, we obtain the adjustment

δ(a,y) =

λ(a,y)
p(a,y)

−

λ(a,y) + λ(a(cid:48),y)
p((cid:63),y)

for an example with protected attribute value a and label y,
and similarly for protected attribute value a(cid:48). In this case,
separately for each y, the adjustments satisfy

p(a,y)δ(a,y) + p(a(cid:48),y)δ(a(cid:48),y) = 0,

so it sufﬁces to do the grid search over δ(a,0) and δ(a,1) and
set the parameters for a(cid:48) to δ(a(cid:48),y) = −p(a,y)δ(a,y)/p(a(cid:48),y).

4. Experimental Results

We now examine how our exponentiated-gradient reduc-
tion5 performs at the task of binary classiﬁcation subject to
either demographic parity or equalized odds. We provide an
evaluation of our grid-search reduction in Appendix D.

We compared our reduction with the score-based post-
processing algorithm of Hardt et al. (2016), which takes as
its input any classiﬁer, (i.e., a standard classiﬁer without any
fairness constraints) and derives a monotone transformation
of the classiﬁer’s output to remove any disparity with respect
to the training examples. This post-processing algorithm
works with both demographic parity and equalized odds, as
well as with binary and non-binary protected attributes.

For demographic parity, we also compared our reduction
with the reweighting and relabeling approaches of Kamiran
& Calders (2012). Reweighting can be applied to both
binary and non-binary protected attributes and operates by
changing importance weights on each example with the
goal of removing any statistical dependence between the
protected attribute and label.6 Relabeling was developed for

5https://github.com/Microsoft/fairlearn
6Although reweighting was developed for demographic parity,
the weights that it induces are achievable by our grid search, albeit
the grid search for equalized odds rather than demographic parity.

binary protected attributes. First, a classiﬁer is trained on
the original data (without considering fairness). The training
examples close to the decision boundary are then relabeled
to remove all disparity while minimally affecting accuracy.
The ﬁnal classiﬁer is then trained on the relabeled data.

As the base classiﬁers for our reductions, we used the
weighted classiﬁcation implementations of logistic regres-
sion and gradient-boosted decision trees in scikit-learn (Pe-
In addition to the three baselines
dregosa et al., 2011).
described above, we also compared our reductions to the
“unconstrained” classiﬁers trained to optimize accuracy only.

We used four data sets, randomly splitting each one into
training examples (75%) and test examples (25%):

• The adult income data set (Lichman, 2013) (48,842
examples). Here the task is to predict whether some-
one makes more than $50k per year, with gender as the
protected attribute. To examine the performance for
non-binary protected attributes, we also conducted an-
other experiment with the same data, using both gender
and race (binarized into white and non-white) as the
protected attribute. Relabeling, which requires binary
protected attributes, was therefore not applicable here.
• ProPublica’s COMPAS recidivism data (7,918 exam-
ples). The task is to predict recidivism from someone’s
criminal history, jail and prison time, demographics,
and COMPAS risk scores, with race as the protected
attribute (restricted to white and black defendants).
• Law School Admissions Council’s National Longitu-
dinal Bar Passage Study (Wightman, 1998) (20,649
examples). Here the task is to predict someone’s even-
tual passage of the bar exam, with race (restricted to
white and black only) as the protected attribute.

• The Dutch census data set (Dutch Central Bureau for
Statistics, 2001) (60,420 examples). Here the task is
to predict whether or not someone has a prestigious
occupation, with gender as the protected attribute.

While all the evaluated algorithms require access to the pro-
tected attribute A at training time, only the post-processing
algorithm requires access to A at test time. For a fair com-
parison, we included A in the feature vector X, so all algo-
rithms had access to it at both the training time and test time.

We used the test examples to measure the classiﬁcation error
for each approach, as well as the violation of the desired fair-
(cid:12)
(cid:12)E[h(X) | A = a] − E[h(X)](cid:12)
ness constraints, i.e., maxa
(cid:12)
(cid:12)
(cid:12)E[h(X) | A = a, Y = y] − E[h(X) | Y = y](cid:12)
and maxa,y
(cid:12)
for demographic parity and equalized odds, respectively.

We ran our reduction across a wide range of tradeoffs be-
tween the classiﬁcation error and fairness constraints. We
considered ε ∈ {0.001, . . . , 0.1} and for each value ran
Algorithm 1 with (cid:98)ck = ε across all k. As expected, the
returned randomized classiﬁers tracked the training Pareto

A Reductions Approach to Fair Classiﬁcation

Figure 1. Test classiﬁcation error versus constraint violation with respect to DP (top two rows) and EO (bottom two rows). All data sets
have binary protected attributes except for adult4, which has four protected attribute values, so relabeling is not applicable there. For our
reduction approach we plot the convex envelope of the classiﬁers obtained on training data at various accuracy–fairness tradeoffs. We
show 95% conﬁdence bands for the classiﬁcation error of our reduction approach and 95% conﬁdence intervals for the constraint violation
of post-processing. Our reduction approach dominates or matches the performance of the other approaches up to statistical uncertainty.

frontier (see Figure 2 in Appendix D). In Figure 1, we evalu-
ate these classiﬁers alongside the baselines on the test data.

For all the data sets, the range of classiﬁcation errors
is much smaller than the range of constraint violations.
Almost all the approaches were able to substantially reduce
or remove disparity without much impact on classiﬁer accu-
racy. One exception was the Dutch census data set, where
the classiﬁcation error increased the most in relative terms.

Our reduction generally dominated or matched the baselines.
The relabeling approach frequently yielded solutions that
were not Pareto optimal. Reweighting yielded solutions
on the Pareto frontier, but often with substantial disparity.
As expected, post-processing yielded disparities that were
statistically indistinguishable from zero, but the resulting
classiﬁcation error was sometimes higher than achieved by
our reduction under a statistically indistinguishable dispar-
ity. In addition, and unlike the post-processing algorithm,
our reduction can achieve any desired accuracy–fairness
tradeoff, allows a wider range of fairness deﬁnitions, and
does not require access to the protected attribute at test time.

Our grid-search reduction, evaluated in Appendix D,
sometimes failed to achieve the lowest disparities on

the training data, but its performance on the test data
very closely matched that of our exponentiated-gradient
reduction. However, if the protected attribute is non-binary,
then grid search is not feasible. For instance, for the version
of the adult income data set where the protected attribute
takes on four values, the grid search would need to span
three dimensions for demographic parity and six dimensions
for equalized odds, both of which are prohibitively costly.

5. Conclusion

We presented two reductions for achieving fairness in a
binary classiﬁcation setting. Our reductions work for any
classiﬁer representation, encompass many deﬁnitions of fair-
ness, satisfy provable guarantees, and work well in practice.

Our reductions optimize the tradeoff between accuracy and
any (single) deﬁnition of fairness given training-time access
to protected attributes. Achieving fairness when training-
time access to protected attributes is unavailable remains an
open problem for future research, as does the navigation of
tradeoffs between accuracy and multiple fairness deﬁnitions.

A Reductions Approach to Fair Classiﬁcation

Acknowledgements

We would like to thank Aaron Roth, Sam Corbett-Davies,
and Emma Pierson for helpful discussions.

References

Agarwal, A., Beygelzimer, A., Dud´ık, M., and Langford, J.
A reductions approach to fair classication. In Fairness,
Accountability, and Transparency in Machine Learning
(FATML), 2017.

Alabi, D., Immorlica, N., and Kalai, A. T. Unleashing linear
optimizers for group-fair learning and optimization. In
Proceedings of the 31st Annual Conference on Learning
Theory (COLT), 2018.

Barocas, S. and Selbst, A. D. Big data’s disparate impact.

California Law Review, 104:671–732, 2016.

Bartlett, P. L. and Mendelson, S. Rademacher and gaussian
complexities: Risk bounds and structural results. Journal
of Machine Learning Research, 3:463–482, 2002.

Berk, R., Heidari, H., Jabbari, S., Kearns, M., and Roth, A.
Fairness in criminal justice risk assessments: The state of
the art. arXiv:1703.09207, 2017.

Beygelzimer, A., Dani, V., Hayes, T. P., Langford, J., and
Zadrozny, B. Error limiting reductions between clas-
siﬁcation tasks. In Proceedings of the Twenty-Second
International Conference on Machine Learning (ICML),
pp. 49–56, 2005.

Boucheron, S., Bousquet, O., and Lugosi, G. Theory of
classiﬁcation: a survey of some recent advances. ESAIM:
Probability and Statistics, 9:323–375, 2005.

Calmon, F., Wei, D., Vinzamuri, B., Ramamurthy, K. N.,
and Varshney, K. R. Optimized pre-processing for dis-
crimination prevention. In Advances in Neural Informa-
tion Processing Systems 30, 2017.

Chouldechova, A. Fair prediction with disparate impact: A
study of bias in recidivism prediction instruments. Big
Data, Special Issue on Social and Technical Trade-Offs,
2017.

Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., and Huq,
A. Algorithmic decision making and the cost of fairness.
In Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining,
pp. 797–806, 2017.

Donini, M., Oneto, L., Ben-David, S., Shawe-Taylor, J., and
Pontil, M. Empirical risk minimization under fairness
constraints. 2018. arXiv:1802.08626.

Dwork, C., Hardt, M., Pitassi, T., Reingold, O., and Zemel,
R. Fairness through awareness. In Proceedings of the 3rd
Innovations in Theoretical Computer Science Conference,
pp. 214–226, 2012.

Dwork, C., Immorlica, N., Kalai, A. T., and Leiserson, M.
Decoupled classiﬁers for group-fair and efﬁcient machine
learning. In Conference on Fairness, Accountability and
Transparency (FAT (cid:63)), pp. 119–133, 2018.

Fan, W., Stolfo, S. J., Zhang, J., and Chan, P. K. Adacost:
Misclassiﬁcation cost-sensitive boosting. In Proceedings
of the Sixteenth International Conference on Machine
Learning (ICML), pp. 97–105, 1999.

Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C.,
and Venkatasubramanian, S. Certifying and removing dis-
parate impact. In Proceedings of the 21st ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining, 2015.

Freund, Y. and Schapire, R. E. Game theory, on-line pre-
diction and boosting. In Proceedings of the Ninth Annual
Conference on Computational Learning Theory (COLT),
pp. 325–332, 1996.

Freund, Y. and Schapire, R. E. A decision-theoretic general-
ization of on-line learning and an application to boosting.
Journal of Computer and System Sciences, 55(1):119–
139, 1997.

Hardt, M., Price, E., and Srebro, N. Equality of opportunity
in supervised learning. In Neural Information Processing
Systems (NIPS), 2016.

Johnson, K. D., Foster, D. P., and Stine, R. A. Impartial pre-
dictive modeling: Ensuring fairness in arbitrary models.
arXiv:1608.00528, 2016.

Kakade, S. M., Sridharan, K., and Tewari, A. On the com-
plexity of linear prediction: Risk bounds, margin bounds,
and regularization. In Advances in neural information
processing systems, pp. 793–800, 2009.

Kamiran, F. and Calders, T. Data preprocessing techniques
for classiﬁcation without discrimination. Knowledge and
Information Systems, 33(1):1–33, 2012.

Kamishima, T., Akaho, S., and Sakuma, J. Fairness-aware
learning through regularization approach. In 2011 IEEE
11th International Conference on Data Mining Work-
shops, pp. 643–650, 2011.

Kearns, M., Neel, S., Roth, A., and Wu, Z. S. Preventing
fairness gerrymandering: Auditing and learning for sub-
group fairness. In Proceedings of the 35th International
Conference on Machine Learning (ICML), 2018.

A Reductions Approach to Fair Classiﬁcation

Kivinen, J. and Warmuth, M. K. Exponentiated gradient
versus gradient descent for linear predictors. Information
and Computation, 132(1):1–63, 1997.

Kleinberg, J., Mullainathan, S., and Raghavan, M. Inherent
trade-offs in the fair determination of risk scores. In Pro-
ceedings of the 8th Innovations in Theoretical Computer
Science Conference, 2017.

Langford, J. and Beygelzimer, A. Sensitive error correct-
In Proceedings of the 18th Annual
ing output codes.
Conference on Learning Theory (COLT), pp. 158–172,
2005.

Ledoux, M. and Talagrand, M. Probability in Banach
Spaces: Isoperimetry and Processes. Springer, 1991.

Lichman, M. UCI machine learning repository, 2013. URL

http://archive.ics.uci.edu/ml.

Menon, A. K. and Williamson, R. C. The cost of fairness in
binary classiﬁcation. In Proceedings of the Conference
on Fiarness, Accountability, and Transparency, 2018.

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-
napeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
Scikit-learn: Machine learning in Python. Journal of
Machine Learning Research, 12:2825–2830, 2011.

Rockafellar, R. T. Convex analysis. Princeton University

Press, 1970.

Shalev-Shwartz, S. Online learning and online convex opti-
mization. Foundations and Trends in Machine Learning,
4(2):107–194, 2012.

Wightman, L. LSAC National Longitudinal Bar Passage

Study, 1998.

Woodworth, B. E., Gunasekar, S., Ohannessian, M. I., and
Srebro, N. Learning non-discriminatory predictors. In
Proceedings of the 30th Conference on Learning Theory
(COLT), pp. 1920–1953, 2017.

Zafar, M. B., Valera, I., Rodriguez, M. G., and Gummadi,
K. P. Fairness constraints: Mechanisms for fair classiﬁca-
tion. In Proceedings of the 20th International Conference
on Artiﬁcial Intelligence and Statistics (AISTATS), pp.
962–970, 2017.

A. Error and Fairness for Randomized Classiﬁers

A Reductions Approach to Fair Classiﬁcation

Let D denote the distribution over triples (X, A, Y ). The accuracy of a classiﬁer h ∈ H is measured by 0-1 error,
err(h) := PD[h(X) (cid:54)= Y ], which for a randomized classiﬁer Q becomes

err(Q) :=

P
(X,A,Y )∼D, h∼Q

[h(X) (cid:54)= Y ] =

Q(h) err(h) .

(cid:88)

h∈H

The fairness constraints on a classiﬁer h are Mµ(h) ≤ c. Recall that µj(h) := ED[gj(X, A, Y, h(X)) | Ej]. For a
randomized classiﬁer Q we deﬁne its moment µj as

µj(Q) :=

E
(X,A,Y )∼D, h∼Q

(cid:104)
gj(X, A, Y, h(X))

(cid:105)

Ej

(cid:12)
(cid:12)
(cid:12)

=

(cid:88)

h∈H

Q(h)µj(h) ,

where the last equality follows because Ej is independent of the choice of h.

B. Proof of Theorem 1

The proof follows immediately from the analysis of Freund & Schapire (1996) applied to the Exponentiated Gradient (EG)
algorithm (Kivinen & Warmuth, 1997), which in our speciﬁc case is also equivalent to Hedge (Freund & Schapire, 1997).
Let Λ := {λ ∈ R|K|
that is equal to λ on coordinates 1 through |K| and puts the remaining mass on the coordinate λ(cid:48)

: (cid:107)λ(cid:48)(cid:107)1 = B}. We associate any λ ∈ Λ with the λ(cid:48) ∈ Λ(cid:48)

+ : (cid:107)λ(cid:48)(cid:107)1 ≤ B} and Λ(cid:48) := {λ(cid:48) ∈ R|K|+1

+

|K|+1.

Consider a run of Algorithm 1. For each λt, let λ(cid:48)
t ∈ R|K|+1 be equal to rt on coordinates 1 through |K| and put zero on the coordinate r(cid:48)
r(cid:48)
associated λ(cid:48), we have, for all t,

t ∈ Λ(cid:48) be the associated element of Λ(cid:48). Let rt := M(cid:98)µ(ht) − (cid:98)c and let
t,|K|+1. Thus, for any λ and the

and, in particular,

λ(cid:62)rt = (λ(cid:48))(cid:62)r(cid:48)

t ,

λ(cid:62)
t

(cid:0)M(cid:98)µ(ht) − (cid:98)c(cid:1) = λ(cid:62)

t rt = (λ(cid:48)

t)(cid:62)r(cid:48)

t .

t as the reward vector for the λ-player. The choices of λ(cid:48)

We interpret r(cid:48)
the learning rate η. By the assumption of the theorem we have (cid:107)r(cid:48)
Corollary 2.14 of Shalev-Shwartz (2012), then states that for any λ(cid:48) ∈ Λ(cid:48),

t then correspond to those of the EG algorithm with
t(cid:107)∞ = (cid:107)rt(cid:107)∞ ≤ ρ. The regret bound for EG, speciﬁcally,

Therefore, by equations (7) and (8), we also have for any λ ∈ Λ,

T
(cid:88)

t=1

(λ(cid:48))(cid:62)r(cid:48)

t ≤

(λ(cid:48)

t)(cid:62)r(cid:48)

t +

B log(|K| + 1)
η

(cid:124)

(cid:123)(cid:122)
=:ζT

+ ηρ2BT

.

(cid:125)

T
(cid:88)

t=1

T
(cid:88)

t=1

λ(cid:62)rt ≤

λ(cid:62)

t rt + ζT .

T
(cid:88)

t=1

(7)

(8)

(9)

This regret bound can be used to bound the suboptimality of L( (cid:98)QT , (cid:98)λT ) in (cid:98)λT as follows:

A Reductions Approach to Fair Classiﬁcation

L( (cid:98)QT , λ) =

T
(cid:88)

(cid:16)

(cid:99)err(ht) + λ(cid:62)(cid:0)M(cid:98)µ(ht) − (cid:98)c(cid:1)(cid:17)

Equation (10) follows from the regret bound (9). Equation (11) follows because L(ht, λt) ≤ L(Q, λt) for all Q by the
choice of ht as the best response of the Q-player. Finally, equation (12) follows by linearity of L(Q, λ) in λ. Thus, we have
for all λ ∈ Λ,

Also, for any Q,

1
T

1
T

1
T

1
T

1
T

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

=

≤

=

≤

(cid:16)
(cid:99)err(ht) + λ(cid:62)rt

(cid:17)

(cid:16)
(cid:99)err(ht) + λ(cid:62)

t rt

(cid:17)

+

ζT
T

L(ht, λt) +

L( (cid:98)QT , λt) +

ζT
T

ζT
T

(cid:16)

= L

(cid:98)QT ,

1
T

T
(cid:88)

t=1

(cid:17)

λt

+

ζT
T

= L( (cid:98)QT , (cid:98)λT ) +

ζT
T

.

L( (cid:98)QT , (cid:98)λT ) ≥ L( (cid:98)QT , λ) −

ζT
T

.

L(Q, (cid:98)λT ) =

L(Q, λt)

1
T

1
T

1
T

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

≥

≥

L(ht, λt)

L(ht, (cid:98)λT ) −

ζT
T

= L( (cid:98)QT , (cid:98)λT ) −

ζT
T

,

L( (cid:98)QT , (cid:98)λT ) ≤ L(Q, (cid:98)λT ) +

ζT
T

.

νT ≤

=

ζT
T

B log(|K| + 1)
ηT

+ ηρ2B ,

(10)

(11)

(12)

(13)

(14)

(15)

(16)

(17)

(18)

where equation (14) follows by linearity of L(Q, λ) in λ, equation (15) follows by the optimality of ht with respect to (cid:98)λt,
equation (16) from the regret bound (9), and equation (17) by linearity of L(Q, λ) in Q. Thus, for all Q,

Equations (13) and (18) immediately imply that for any T ≥ 1,

proving the ﬁrst part of the theorem.

The second part of the theorem follows by plugging in η = ν

2ρ2B and verifying that if T ≥ 4ρ2B2 log(|K|+1)

ν2

then

νT ≤

B log(|K| + 1)
2ρ2B · 4ρ2B2 log(|K|+1)

ν2

ν

+

ν
2ρ2B

· ρ2B =

+

.

ν
2

ν
2

A Reductions Approach to Fair Classiﬁcation

C. Proofs of Theorems 2 and 3

The bulk of this appendix proves the following theorem, which will immediately imply Theorems 2 and 3.

Theorem 4. Let ( (cid:98)Q, (cid:98)λ) be any ν-approximate saddle point of L with

(cid:98)ck = ck + εk

and εk ≥

|Mk,j|

2Rnj (H) +

(cid:32)

(cid:88)

j∈J

(cid:115)

2
√
nj

+

ln(2/δ)
2nj

(cid:33)

.

Let Q(cid:63) minimize err(Q) subject to Mµ(Q) ≤ c. Then with probability at least 1 − (|J| + 1)δ, the distribution (cid:98)Q satisﬁes

err( (cid:98)Q) ≤ err(Q(cid:63)) + 2ν + 4Rn(H) +

(cid:114)

4
√
n

+

2 ln(2/δ)
n

,

and for all k, γk( (cid:98)Q) ≤ ck +

+ 2εk .

1 + 2ν
B

Let Λ := {λ ∈ R|K|
pair ( (cid:98)Q, (cid:98)λ) which is a ν-approximate saddle point of L, i.e.,

+ : (cid:107)λ(cid:48)(cid:107)1 ≤ B} denote the domain of λ. In the remainder of the section, we assume that we are given a

L( (cid:98)Q, (cid:98)λ) ≤ L(Q, (cid:98)λ) + ν

for all Q ∈ ∆,

and L( (cid:98)Q, (cid:98)λ) ≥ L( (cid:98)Q, λ) − ν

for all λ ∈ Λ.

We ﬁrst establish that the pair ( (cid:98)Q, (cid:98)λ) satisﬁes an approximate version of complementary slackness. For the statement and
proof of the following lemma, recall that (cid:98)γ(Q) = M(cid:98)µ(Q), so the empirical fairness constraints can be written as (cid:98)γ(Q) ≤ (cid:98)c
and the Lagrangian L can be written as

(19)

(20)

L(Q, λ) = (cid:99)err(Q) +

λk((cid:98)γk(Q) − (cid:98)ck) .

(cid:88)

k∈K

Lemma 1 (Approximate complementary slackness). The pair ( (cid:98)Q, (cid:98)λ) satisﬁes

(cid:88)

k∈K

(cid:0)
(cid:98)λk((cid:98)γk( (cid:98)Q) − (cid:98)ck) ≥ B max
k∈K

(cid:98)γk( (cid:98)Q) − (cid:98)ck

+ − ν ,

(cid:1)

where we abbreviate x+ = max{x, 0} for any real number x.

Proof. We show that the lemma follows from the optimality conditions (19). We consider a dual variable λ deﬁned as

λ =

(cid:40)

0
Bek(cid:63)

if (cid:98)γ( (cid:98)Q) ≤ (cid:98)c,
otherwise, where k(cid:63) = arg maxk[(cid:98)γk( (cid:98)Q) − (cid:98)ck],

where ek denotes the kth vector of the standard basis. Then we have by equations (19) and (20) that

(cid:99)err( (cid:98)Q) +

(cid:98)λk((cid:98)γk( (cid:98)Q) − (cid:98)ck) = L( (cid:98)Q, (cid:98)λ)

(cid:88)

k∈K

≥ L( (cid:98)Q, λ) − ν = (cid:99)err( (cid:98)Q) +

λk((cid:98)γk( (cid:98)Q) − (cid:98)ck) − ν ,

(cid:88)

k∈K

and the lemma follows by our choice of λ.

Next two lemmas bound the empirical error of (cid:98)Q and also bound the amount by which (cid:98)Q violates the empirical fairness
constraints.
Lemma 2 (Empirical error bound). The distribution (cid:98)Q satisﬁes (cid:99)err( (cid:98)Q) ≤ (cid:99)err(Q) + 2ν for any Q satisfying the empirical
fairness constraints, i.e., any Q such that (cid:98)γ(Q) ≤ (cid:98)c.

A Reductions Approach to Fair Classiﬁcation

Proof. Assume that Q satisﬁes (cid:98)γ(Q) ≤ (cid:98)c. Since (cid:98)λ ≥ 0, we have

L(Q, (cid:98)λ) = (cid:99)err(Q) + (cid:98)λ

(cid:62)(cid:0)

(cid:98)γ(Q) − (cid:98)c(cid:1) ≤ (cid:99)err(Q) .

The optimality conditions (19) imply that

Putting these together, we obtain

L( (cid:98)Q, (cid:98)λ) ≤ L(Q, (cid:98)λ) + ν .

L( (cid:98)Q, (cid:98)λ) ≤ (cid:99)err(Q) + ν .

We next invoke Lemma 1 to lower bound L( (cid:98)Q, (cid:98)λ) as

L( (cid:98)Q, (cid:98)λ) = (cid:99)err( (cid:98)Q) +

(cid:98)λk((cid:98)γk( (cid:98)Q) − (cid:98)ck) ≥ (cid:99)err( (cid:98)Q) + B max
k∈K

(cid:98)γk( (cid:98)Q) − (cid:98)ck

+ − ν

(cid:0)

(cid:1)

(cid:88)

k∈K

≥ (cid:99)err( (cid:98)Q) − ν .

Combining the upper and lower bounds on L( (cid:98)Q, (cid:98)λ) completes the proof.

Lemma 3 (Empirical fairness violation). Assume that the empirical fairness constraints (cid:98)γ(Q) ≤ (cid:98)c are feasible. Then the
distribution (cid:98)Q approximately satisﬁes all empirical fairness constraints:

(cid:16)

max
k∈K

(cid:98)γk( (cid:98)Q) − (cid:98)ck

(cid:17)

≤

1 + 2ν
B

.

Proof. Let Q satisfy (cid:98)γ(Q) ≤ (cid:98)c. Applying the same upper and lower bound on L( (cid:98)Q, (cid:98)λ) as in the proof of Lemma 2, we
obtain

(cid:99)err( (cid:98)Q) + B max
k∈K

(cid:0)

(cid:98)γk( (cid:98)Q) − (cid:98)ck

+ − ν ≤ L( (cid:98)Q, (cid:98)λ) ≤ (cid:99)err(Q) + ν .

(cid:1)

We can further upper bound (cid:99)err(Q) − (cid:99)err( (cid:98)Q) by 1 and use x ≤ x+ for any real number x to complete the proof.

It remains to lift the bounds on empirical classiﬁcation error and constraint violation into the corresponding bounds on true
classiﬁcation error and the violation of true constraints. We will use the standard machinery of uniform convergence bounds
via the (worst-case) Rademacher complexity.
Let F be a class of functions f : Z → [0, 1] over some space Z. Then the (worst-case) Rademacher complexity of F is
deﬁned as

Rn(F) := sup

(cid:34)

E

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

sup
f ∈F

n
(cid:88)

σif (zi)

,

(cid:12)
(cid:35)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

z1,...,zn∈Z
where the expectation is over the i.i.d. random variables σ1, . . . , σn with P[σi = 1] = P[σi = −1] = 1/2.
We ﬁrst prove concentration of generic moments derived from classiﬁers h ∈ H and then move to bounding the deviations
from true classiﬁcation error and true fairness constraints.
Lemma 4 (Concentration of moments). Let g : X × A × {0, 1} × {0, 1} → [0, 1] be any function and let D be a distribution
over (X, A, Y ). Then with probability at least 1 − δ, for all h ∈ H,

i=1

(cid:12)
(cid:12)

(cid:12)(cid:98)E(cid:2)g(X, A, Y, h(X))(cid:3) − E(cid:2)g(X, A, Y, h(X))(cid:3)(cid:12)

(cid:12) ≤ 2Rn(H) +
(cid:12)

(cid:114)

2
√
n

+

ln(2/δ)
2n

,

where the expectation is with respect to D and the empirical expectation is based on n i.i.d. draws from D.

Proof. Let F := {fh}h∈H be the class of functions fh : (x, y, a) (cid:55)→ g(cid:0)x, y, a, h(x)(cid:1). By Theorem 3.2 of Boucheron et al.
(2005), we then have with probability at least 1 − δ, for all h,

(cid:12)
(cid:12)

(cid:12)(cid:98)E(cid:2)g(X, A, Y, h(X))(cid:3) − E(cid:2)g(X, A, Y, h(X))(cid:3)(cid:12)
(cid:12)
(cid:12) =

(cid:12)
(cid:12)
(cid:12) ≤ 2Rn(F) +
(cid:12)(cid:98)E[fh] − E[fh]
(cid:12)
(cid:12)

(cid:114)

ln(2/δ)
2n

.

(21)

We will next bound Rn(F) in terms of Rn(H). Since h(x) ∈ {0, 1}, we can write

A Reductions Approach to Fair Classiﬁcation

fh(x, y, a) = h(x)g(x, a, y, 1) +

1 − h(x)

g(x, a, y, 0) = g(x, a, y, 0) + h(x)

g(x, a, y, 1) − g(x, a, y, 0)

.

(cid:16)

(cid:17)

(cid:16)

(cid:17)

(cid:12)g(x, a, y, 0)(cid:12)

Since (cid:12)
(cid:12) ≤ 1, we can invoke Theorem 12(5) of Bartlett & Mendelson
(2002) for bounding function classes shifted by an offset, in our case g(x, a, y, 0), and Theorem 4.4 of Ledoux & Talagrand
(1991) for bounding function classes under contraction, in our case g(x, a, y, 1) − g(x, a, y, 0), yielding

(cid:12)g(x, a, y, 1) − g(x, a, y, 0)(cid:12)

(cid:12) ≤ 1 and (cid:12)

Rn(F) ≤

+ Rn(H) .

1
√
n

Together with the bound (21), this proves the lemma.

Lemma 5 (Concentration of loss). With probability at least 1 − δ, for all Q ∈ ∆,

|(cid:99)err(Q) − err(Q)| ≤ 2Rn(H) +

(cid:114)

2
√
n

+

ln(2/δ)
2n

.

Proof. We ﬁrst use Lemma 4 with g : (x, a, y, ˆy) (cid:55)→ 1{y (cid:54)= ˆy} to obtain, with probability 1 − δ, for all h,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) =
(cid:12)(cid:99)err(h) − err(h)

(cid:12)
(cid:12)(cid:98)E[fh] − E[fh]
(cid:12)

(cid:12)
(cid:12) ≤ 2Rn(H) +
(cid:12)

(cid:114)

2
√
n

+

ln(2/δ)
2n

.

The lemma now follows for any Q by taking a convex combination of the corresponding bounds on h ∈ H.7

Finally, we show a result for the concentration of the empirical constraint violations to their population counterparts. We
will actually show the concentration of the individual moments (cid:98)µj(Q) to µj(Q) uniformly for all Q ∈ ∆. Since M is
a ﬁxed matrix not dependent on the data, this also directly implies concentration of the constraints (cid:98)γ(Q) = M(cid:98)µ(Q) to
γ(Q) = Mµ(Q). For this result, recall that nj = |{i ∈ [n] : (Xi,Ai,Yi) ∈ Ej}| and p(cid:63)
Lemma 6 (Concentration of conditional moments). For any j ∈ J, with probability at least 1 − δ, for all Q,

j = P[Ej].

(cid:12)(cid:98)µj(Q) − µj(Q)(cid:12)
(cid:12)

(cid:12) ≤ 2Rnj (H) +

(cid:115)

2
√
nj

+

ln(2/δ)
2nj

.

If np(cid:63)

j ≥ 8 log(2/δ), then with probability at least 1 − δ, for all Q,

(cid:12)(cid:98)µj(Q) − µj(Q)(cid:12)
(cid:12)

(cid:12) ≤ 2Rnp(cid:63)

j /2(H) + 2

(cid:115)

(cid:115)

2
np(cid:63)
j

+

ln(4/δ)
np(cid:63)
j

.

Proof. Our proof largely follows the proof of Lemma 2 of Woodworth et al. (2017), with appropriate modiﬁcations for our
more general constraint deﬁnition. Let Sj := {i ∈ [n] : (Xi,Ai,Yi) ∈ Ej} be the set of indices such that the corresponding
examples fall in the event Ej. Note that we have deﬁned nj = |Sj|. Let D(·) denote the joint distribution of (X, A, Y ).
Then, conditioned on i ∈ Sj, the random variables gj(Xi,Ai,Yi,h(Xi)) are i.i.d. draws from the distribution D(· | Ej), with
mean µj(h). Applying Lemma 4 with gj and the distribution D(· | Ej) therefore yields, with probability 1 − δ, for all h,

(cid:12)(cid:98)µj(h) − µj(h)(cid:12)
(cid:12)

(cid:12) ≤ 2Rnj (H) +

(cid:115)

2
√
nj

+

ln(2/δ)
2nj

,

The lemma now follows by taking a convex combination over h.

7The same reasoning applies for general error, err(h) = E[gerr(X,A,Y,h(X))], by using g = gerr in Lemma 4.

A Reductions Approach to Fair Classiﬁcation

Proof of Theorem 4. We now use the lemmas derives so far to prove Theorem 4. We ﬁrst use Lemma 6 to bound the gap
between the empirical and population fairness constraints. The lemma implies that with probability at least 1 − |J|δ, for all
k ∈ K and all Q ∈ ∆,

(cid:12)(cid:98)γk(Q) − γk(Q)(cid:12)
(cid:12)

(cid:12) =

(cid:16)
(cid:98)µ(Q) − µ(Q)

(cid:17)(cid:12)
(cid:12)
(cid:12)

≤

(cid:12)
(cid:12)
(cid:12)(cid:98)µj(Q) − µj(Q)
|Mk,j|

(cid:12)
(cid:12)
(cid:12)

(cid:32)

≤

|Mk,j|

2Rnj (H) +

(cid:115)

(cid:33)

2
√
nj

+

ln(2/δ)
2nj

(cid:12)
(cid:12)
(cid:12)Mk
(cid:88)

j∈J

(cid:88)

j∈J
≤ εk .

Note that our choice of (cid:98)c along with equation (22) ensure that (cid:98)γk(Q(cid:63)) ≤ (cid:98)ck for all k ∈ K. Using Lemma 2 allows us to
conclude that

(cid:99)err( (cid:98)Q) ≤ (cid:99)err(Q(cid:63)) + 2ν .

We now invoke Lemma 5 twice, once for (cid:99)err( (cid:98)Q) and once for (cid:99)err(Q(cid:63)), proving the ﬁrst statement of the theorem.
The above shows that Q(cid:63) satisﬁes the empirical fairness constraints, so we can use Lemma 3, which together with
equation (22) yields

γk( (cid:98)Q) ≤ (cid:98)γk( (cid:98)Q) + εk ≤ (cid:98)ck +

+ εk = ck +

+ 2εk ,

1 + 2ν
B

1 + 2ν
B

(22)

proving the second statement of the theorem.

We are now ready to prove Theorems 2 and 3

Proof of Theorem 2. The ﬁrst part of the theorem follows immediately from Assumption 1 and Theorem 4 (with δ/2 instead
of δ). The statement in fact holds with probability at least 1 − (|J| + 1)δ/2. For the second part, we use the multiplicative
Chernoff bound for binomial random variables. Note that E[nj] = np(cid:63)
j , and we assume that np(cid:63)
j ≥ 8 ln(2/δ), so the
multiplicative Chernoff bound implies that nj ≤ np(cid:63)
j /2 with probability at most δ/2. Taking the union bound across all j
and combining with the ﬁrst part of the theorem then proves the second part.

Proof of Theorem 3. This follows immediately from Theorem 1 and the ﬁrst part of Theorem 2.

D. Additional Experimental Results

In this appendix we present more complete experimental results. We present experimental results for both the training and
test data. We evaluate the exponentiated-gradient as well as the grid-search variants of our reductions. And, ﬁnally, we
consider extensions of reweighting and relabeling beyond the speciﬁc tradeoffs proposed by Kamiran & Calders (2012).
Speciﬁcally, we introduce a scaling parameter that interpolates between the prescribed tradeoff (speciﬁc importance weights
or the number of examples to relabel) and the unconstrained classiﬁer (uniform weights or zero examples to relabel). The
training data results are shown in Figure 2. The test set results are shown in Figure 3.

A Reductions Approach to Fair Classiﬁcation

Figure 2. Training classiﬁcation error versus constraint violation, with respect to DP (top two rows) and EO (bottom two rows). Markers
correspond to the baselines. For our two reductions and the interpolants between reweighting (or relabeling) and the unconstrained
classiﬁer, we varied their tradeoff parameters and plot the Pareto frontiers of the sets of classiﬁers obtained for each method. Because the
curves of the different methods often overlap, we use vertical dashed lines to indicate the lowest constraint violations. All data sets have
binary protected attributes except for adult4, which has four protected attribute values, so relabeling is not applicable and grid search is
not feasible for this data set. The exponentiated-gradient reduction dominates or matches other approaches as expected since it solves
exactly for the points on the Pareto frontier of the set of all classiﬁers in each considered class.

A Reductions Approach to Fair Classiﬁcation

Figure 3. Test classiﬁcation error versus constraint violation, with respect to DP (top two rows) and EO (bottom two rows). Markers
correspond to the baselines. For our two reductions and the interpolants between reweighting (or relabeling) and the unconstrained
classiﬁer, we show convex envelopes of the classiﬁers taken from the training Pareto frontier of each method (i.e., the same classiﬁers as
shown in Figure 2). Because the curves of the different methods often overlap, we use vertical dashed lines to indicate the lowest constraint
violations. All data sets have binary protected attributes except for adult4, which has four protected attribute values, so relabeling
is not applicable and grid search is not feasible for this data set. We show 95% conﬁdence bands for the classiﬁcation error of the
exponentiated-gradient reduction and 95% conﬁdence intervals for the constraint violation of post-processing. The exponentiated-gradient
reduction dominates or matches performance of all other methods up to statistical uncertainty.

A Reductions Approach to Fair Classiﬁcation

8
1
0
2
 
l
u
J
 
6
1
 
 
]

G
L
.
s
c
[
 
 
3
v
3
5
4
2
0
.
3
0
8
1
:
v
i
X
r
a

Alekh Agarwal 1 Alina Beygelzimer 2 Miroslav Dud´ık 1 John Langford 1 Hanna Wallach 1

Abstract

We present a systematic approach for achieving
fairness in a binary classiﬁcation setting. While
we focus on two well-known quantitative deﬁni-
tions of fairness, our approach encompasses many
other previously studied deﬁnitions as special
cases. The key idea is to reduce fair classiﬁcation
to a sequence of cost-sensitive classiﬁcation
problems, whose solutions yield a randomized
classiﬁer with the lowest (empirical) error subject
to the desired constraints. We introduce two
reductions that work for any representation of the
cost-sensitive classiﬁer and compare favorably
to prior baselines on a variety of data sets, while
overcoming several of their disadvantages.

1. Introduction

Over the past few years, the media have paid considerable
attention to machine learning systems and their ability to
inadvertently discriminate against minorities, historically
disadvantaged populations, and other protected groups when
allocating resources (e.g., loans) or opportunities (e.g., jobs).
In response to this scrutiny—and driven by ongoing debates
and collaborations with lawyers, policy-makers, social sci-
entists, and others (e.g., Barocas & Selbst, 2016)—machine
learning researchers have begun to turn their attention to the
topic of “fairness in machine learning,” and, in particular, to
the design of fair classiﬁcation and regression algorithms.

In this paper we study the task of binary classiﬁcation sub-
ject to fairness constraints with respect to a pre-deﬁned pro-
tected attribute, such as race or sex. Previous work in this
area can be divided into two broad groups of approaches.

The ﬁrst group of approaches incorporate speciﬁc quanti-
tative deﬁnitions of fairness into existing machine learning

1Microsoft Research, New York 2Yahoo! Research, New York.
Correspondence to: A. Agarwal <alekha@microsoft.com>,
Dud´ık
A.
<mdudik@microsoft.com>, J. Langford <jcl@microsoft.com>,
H. Wallach <wallach@microsoft.com>.

Beygelzimer <beygel@gmail.com>,

M.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

methods, often by relaxing the desired deﬁnitions of fair-
ness, and only enforcing weaker constraints, such as lack of
correlation (e.g., Woodworth et al., 2017; Zafar et al., 2017;
Johnson et al., 2016; Kamishima et al., 2011; Donini et al.,
2018). The resulting fairness guarantees typically only hold
under strong distributional assumptions, and the approaches
are tied to speciﬁc families of classiﬁers, such as SVMs.

The second group of approaches eliminate the restriction
to speciﬁc classiﬁer families and treat the underlying clas-
siﬁcation method as a “black box,” while implementing
a wrapper that either works by pre-processing the data or
post-processing the classiﬁer’s predictions (e.g., Kamiran
& Calders, 2012; Feldman et al., 2015; Hardt et al., 2016;
Calmon et al., 2017). Existing pre-processing approaches
are speciﬁc to particular deﬁnitions of fairness and typically
seek to come up with a single transformed data set that will
work across all learning algorithms, which, in practice, leads
to classiﬁers that still exhibit substantial unfairness (see our
evaluation in Section 4). In contrast, post-processing allows
a wider range of fairness deﬁnitions and results in provable
fairness guarantees. However, it is not guaranteed to ﬁnd the
most accurate fair classiﬁer, and requires test-time access to
the protected attribute, which might not be available.

We present a general-purpose approach that has the key
advantage of this second group of approaches—i.e., the
underlying classiﬁcation method is treated as a black
box—but without the noted disadvantages. Our approach
encompasses a wide range of fairness deﬁnitions,
is
guaranteed to yield the most accurate fair classiﬁer, and
does not require test-time access to the protected attribute.
Speciﬁcally, our approach allows any deﬁnition of fairness
that can be formalized via linear inequalities on conditional
such as demographic parity or equalized
moments,
odds (see Section 2.1). We show how binary classiﬁcation
subject to these constraints can be reduced to a sequence
of cost-sensitive classiﬁcation problems. We require only
black-box access to a cost-sensitive classiﬁcation algorithm,
which does not need to have any knowledge of the desired
deﬁnition of fairness or protected attribute. We show that
the solutions to our sequence of cost-sensitive classiﬁcation
problems yield a randomized classiﬁer with the lowest
(empirical) error subject to the desired fairness constraints.

Corbett-Davies et al. (2017) and Menon & Williamson

A Reductions Approach to Fair Classiﬁcation

(2018) begin with a similar goal to ours, but they analyze
the Bayes optimal classiﬁer under fairness constraints in the
limit of inﬁnite data. In contrast, our focus is algorithmic,
our approach applies to any classiﬁer family, and we obtain
ﬁnite-sample guarantees. Dwork et al. (2018) also begin
with a similar goal to ours. Their approach partitions the
training examples into subsets according to protected at-
tribute values and then leverages transfer learning to jointly
learn from these separate data sets. Our approach avoids par-
titioning the data and assumes access only to a classiﬁcation
algorithm rather than a transfer learning algorithm.

A preliminary version of this paper appeared at the FAT/ML
workshop (Agarwal et al., 2017), and led to extensions with
more general optimization objectives (Alabi et al., 2018)
and combinatorial protected attributes (Kearns et al., 2018).

In the next section, we formalize our problem. While we
focus on two well-known quantitative deﬁnitions of fairness,
our approach also encompasses many other previously stud-
ied deﬁnitions of fairness as special cases. In Section 3, we
describe our reductions approach to fair classiﬁcation and
its guarantees in detail. The experimental study in Section 4
shows that our reductions compare favorably to three base-
lines, while overcoming some of their disadvantages and
also offering the ﬂexibility of picking a suitable accuracy–
fairness tradeoff. Our results demonstrate the utility of
having a general-purpose approach for combining machine
learning methods and quantitative fairness deﬁnitions.

2. Problem Formulation

We consider a binary classiﬁcation setting where the training
examples consist of triples (X, A, Y ), where X ∈ X is a fea-
ture vector, A ∈ A is a protected attribute, and Y ∈ {0, 1}
is a label. The feature vector X can either contain the pro-
tected attribute A as one of the features or contain other fea-
tures that are arbitrarily indicative of A. For example, if the
classiﬁcation task is to predict whether or not someone will
default on a loan, each training example might correspond
to a person, where X represents their demographics, income
level, past payment history, and loan amount; A represents
their race; and Y represents whether or not they defaulted on
that loan. Note that X might contain their race as one of the
features or, for example, contain their zipcode—a feature
that is often correlated with race. Our goal is to learn an ac-
curate classiﬁer h : X → {0, 1} from some set (i.e., family)
of classiﬁers H, such as linear threshold rules, decision trees,
or neural nets, while satisfying some deﬁnition of fairness.
Note that the classiﬁers in H do not explicitly depend on A.

2.1. Fairness Deﬁnitions

We focus on two well-known quantitative deﬁnitions of
fairness that have been considered in previous work on

fair classiﬁcation; however, our approach also encompasses
many other previously studied deﬁnitions of fairness as
special cases, as we explain at the end of this section.

The ﬁrst deﬁnition—demographic (or statistical) parity—
can be thought of as a stronger version of the US Equal
Employment Opportunity Commission’s “four-ﬁfths rule,”
which requires that the “selection rate for any race, sex, or
ethnic group [must be at least] four-ﬁfths (4/5) (or eighty
percent) of the rate for the group with the highest rate.”1
Deﬁnition 1 (Demographic parity—DP). A classiﬁer h
satisﬁes demographic parity under a distribution over
(X, A, Y ) if its prediction h(X) is statistically indepen-
dent of the protected attribute A—that is, if P[h(X) = ˆy |
A = a] = P[h(X) = ˆy] for all a, ˆy. Because ˆy ∈ {0, 1},
this is equivalent to E[h(X) | A = a] = E[h(X)] for all a.

The second deﬁnition—equalized odds—was recently pro-
posed by Hardt et al. (2016) to remedy two previously noted
ﬂaws with demographic parity (Dwork et al., 2012). First,
demographic parity permits a classiﬁer which accurately
classiﬁes data points with one value A = a, such as the
value a with the most data, but makes random predictions
for data points with A (cid:54)= a as long as the probabilities of
h(X) = 1 match. Second, demographic parity rules out
perfect classiﬁers whenever Y is correlated with A.
In
contrast, equalized odds suffers from neither of these ﬂaws.

Deﬁnition 2 (Equalized odds—EO). A classiﬁer h satis-
ﬁes equalized odds under a distribution over (X, A, Y )
if its prediction h(X) is conditionally independent of
the protected attribute A given the label Y —that is, if
P[h(X) = ˆy | A = a, Y = y] = P[h(X) = ˆy | Y = y] for
all a, y, and ˆy. Because ˆy ∈ {0, 1}, this is equivalent to
E[h(X) | A = a, Y = y] = E[h(X) | Y = y] for all a, y.

We now show how each deﬁnition can be viewed as a special
case of a general set of linear constraints of the form

Mµ(h) ≤ c,

(1)

where matrix M ∈ R|K|×|J| and vector c ∈ R|K| describe
the linear constraints, each indexed by k ∈ K, and µ(h) ∈
R|J| is a vector of conditional moments of the form

µj(h) = E(cid:2) gj(X, A, Y, h(X)) (cid:12)

(cid:12) Ej

(cid:3)

for j ∈ J,

where gj : X × A × {0, 1} × {0, 1} → [0, 1] and Ej is
an event deﬁned with respect to (X, A, Y ). Crucially, gj
depends on h, while Ej cannot depend on h in any way.
Example 1 (DP). In a binary classiﬁcation setting, demo-
graphic parity can be expressed as a set of |A| equality
constraints, each of the form E[h(X) | A = a] = E[h(X)].
Letting J = A ∪ {(cid:63)}, gj(X, A, Y, h(X)) = h(X) for all j,

1See the Uniform Guidelines on Employment Selection Proce-

dures, 29 C.F.R. §1607.4(D) (2015).

A Reductions Approach to Fair Classiﬁcation

Ea = {A = a}, and E(cid:63) = {True}, where {True} refers to
the event encompassing all points in the sample space, each
equality constraint can be expressed as µa(h) = µ(cid:63)(h).2
Finally, because each such constraint can be equivalently
expressed as a pair of inequality constraints of the form

µa(h) − µ(cid:63)(h) ≤ 0
−µa(h) + µ(cid:63)(h) ≤ 0,

demographic parity can be expressed as equation (1), where
K = A×{+, −}, M(a,+),a(cid:48) = 1{a(cid:48) = a}, M(a,+),(cid:63) = −1,
M(a,−),a(cid:48) = −1{a(cid:48) = a}, M(a,−),(cid:63) = 1, and c = 0.
Expressing each equality constraint as a pair of inequality
constraints allows us to control the extent to which each
constraint is enforced by positing ck > 0 for some (or all) k.
In a binary classiﬁcation set-
Example 2 (EO).
equalized odds can be expressed as a set
ting,
of 2 |A|
form
constraints,
E[h(X) | A = a, Y = y] = E[h(X) | Y = y]. Letting
J = (A ∪ {(cid:63)}) × {0, 1}, gj(X, A, Y, h(X)) = h(X) for
all j, E(a,y) = {A = a, Y = y}, and E((cid:63),y) = {Y = y},
each equality constraint can be equivalently expressed as

each of

equality

the

µ(a,y)(h) − µ((cid:63),y)(h) ≤ 0
−µ(a,y)(h) + µ((cid:63),y)(h) ≤ 0.

can be

equalized odds

As a result,
expressed
as equation (1), where K = A × Y × {+, −},
M(a,y,+),(a(cid:48),y(cid:48)) = 1{a(cid:48)= a, y(cid:48)= y}, M(a,y,+),((cid:63),y(cid:48)) = −1,
M(a,y,−),(a(cid:48),y(cid:48)) = −1{a(cid:48)= a, y(cid:48)= y}, M(a,y,−),((cid:63),y(cid:48)) = 1,
and c = 0. Again, we can posit ck > 0 for some (or all) k
to allow small violations of some (or all) of the constraints.

Although we omit the details, we note that many other pre-
viously studied deﬁnitions of fairness can also be expressed
as equation (1). For example, equality of opportunity (Hardt
et al., 2016) (also known as balance for the positive class;
Kleinberg et al., 2017), balance for the negative class (Klein-
berg et al., 2017), error-rate balance (Chouldechova,
2017), overall accuracy equality (Berk et al., 2017), and
treatment equality (Berk et al., 2017) can all be expressed
as equation (1); in contrast, calibration (Kleinberg et al.,
2017) and predictive parity (Chouldechova, 2017) cannot
because to do so would require the event Ej to depend on
h. We note that our approach can also be used to satisfy
multiple deﬁnitions of fairness, though if these deﬁnitions
are mutually contradictory, e.g., as described by Kleinberg
et al. (2017), then our guarantees become vacuous.

2.2. Fair Classiﬁcation

In a standard (binary) classiﬁcation setting, the goal is to
learn the classiﬁer h ∈ H with the minimum classiﬁcation

2Note that µ(cid:63)(h) = E[h(X) | True] = E[h(X)].

error: err(h) := P[h(X) (cid:54)= Y ]. However, because our
goal is to learn the most accurate classiﬁer while satisfying
fairness constraints, as formalized above, we instead seek to
ﬁnd the solution to the constrained optimization problem3

min
h∈H

err(h)

subject to Mµ(h) ≤ c.

(2)

Furthermore, rather than just considering classiﬁers in the
set H, we can enlarge the space of possible classiﬁers by
considering randomized classiﬁers that can be obtained via
a distribution over H. By considering randomized classi-
ﬁers, we can achieve better accuracy–fairness tradeoffs than
would otherwise be possible. A randomized classiﬁer Q
makes a prediction by ﬁrst sampling a classiﬁer h ∈ H
from Q and then using h to make the prediction. The result-
ing classiﬁcation error is err(Q) = (cid:80)
h∈H Q(h) err(h) and
the conditional moments are µ(Q) = (cid:80)
h∈H Q(h)µ(h)
(see Appendix A for the derivation). Thus we seek to solve

min
Q∈∆

err(Q)

subject to Mµ(Q) ≤ c,

(3)

where ∆ is the set of all distributions over H.

In practice, we do not know the true distribution over
(X, A, Y ) and only have access to a data set of training ex-
amples {(Xi, Ai, Yi)}n
i=1. We therefore replace err(Q) and
µ(Q) in equation (3) with their empirical versions (cid:99)err(Q)
and (cid:98)µ(Q). Because of the sampling error in (cid:98)µ(Q), we
also allow errors in satisfying the constraints by setting
(cid:98)ck = ck + εk for all k, where εk ≥ 0. After these modiﬁca-
tions, we need to solve the empirical version of equation (3):

Q∈∆ (cid:99)err(Q)
min

subject to M(cid:98)µ(Q) ≤ (cid:98)c.

(4)

3. Reductions Approach

We now show how the problem (4) can be reduced to a se-
quence of cost-sensitive classiﬁcation problems. We further
show that the solutions to our sequence of cost-sensitive clas-
siﬁcation problems yield a randomized classiﬁer with the
lowest (empirical) error subject to the desired constraints.

3.1. Cost-sensitive Classiﬁcation

We assume access to a cost-sensitive classiﬁcation algorithm
for the set H. The input to such an algorithm is a data set
of training examples {(Xi, C 0
i and C 1
i
denote the losses—costs in this setting—for predicting the
labels 0 or 1, respectively, for Xi. The algorithm outputs

i=1, where C 0

i , C 1

i )}n

arg min
h∈H

n
(cid:88)

i=1

h(Xi) C 1

i + (1 − h(Xi)) C 0
i .

(5)

3We consider misclassiﬁcation error for concreteness, but all
the results in this paper apply to any error of the form err(h) =
E[gerr(X, A, Y, h(X))], where gerr(·, ·, ·, ·) ∈ [0, 1].

A Reductions Approach to Fair Classiﬁcation

min
Q∈∆

max
λ∈R|K|

+

L(Q, λ).

(6)

L( (cid:98)Q, (cid:98)λ) ≤ L(Q, (cid:98)λ) + ν

This abstraction allows us to specify different costs for dif-
ferent training examples, which is essential for incorporat-
ing fairness constraints. Moreover, efﬁcient cost-sensitive
classiﬁcation algorithms are readily available for several
common classiﬁer representations (e.g., Beygelzimer et al.,
2005; Langford & Beygelzimer, 2005; Fan et al., 1999). In
particular, equation (5) is equivalent to a weighted classi-
ﬁcation problem, where the input consists of labeled ex-
amples {(Xi, Yi, Wi)}n
i=1 with Yi ∈ {0, 1} and Wi ≥ 0,
and the goal is to minimize the weighted classiﬁcation er-
ror (cid:80)n
i=1 Wi 1{h(Xi) (cid:54)= Yi}. This is equivalent to equa-
tion (5) if we set Wi = |C 0
i }.

i | and Yi = 1{C 0

i ≥ C 1

i − C 1

3.2. Reduction

To derive our fair classiﬁcation algorithm, we rewrite equa-
tion (4) as a saddle point problem. We begin by introducing
a Lagrange multiplier λk ≥ 0 for each of the |K| constraints,
summarized as λ ∈ R|K|

+ , and form the Lagrangian
L(Q, λ) = (cid:99)err(Q) + λ(cid:62)(cid:0)M(cid:98)µ(Q) − (cid:98)c(cid:1).

Thus, equation (4) is equivalent to

For computational and statistical reasons, we impose an
additional constraint on the (cid:96)1 norm of λ and seek to simul-
taneously ﬁnd the solution to the constrained version of (6)
as well as its dual, obtained by switching min and max:

min
Q∈∆

max

L(Q, λ),

λ∈R|K|

+ , (cid:107)λ(cid:107)1≤B

max
+ , (cid:107)λ(cid:107)1≤B

λ∈R|K|

min
Q∈∆

L(Q, λ).

(P)

(D)

Because L is linear in Q and λ and the domains of Q and
λ are convex and compact, both problems have solutions
(which we denote by Q† and λ†) and the minimum value of
(P) and the maximum value of (D) are equal and coincide
with L(Q†, λ†). Thus, (Q†, λ†) is the saddle point of L
(Corollary 37.6.2 and Lemma 36.2 of Rockafellar, 1970).

We ﬁnd the saddle point by using the standard scheme of
Freund & Schapire (1996), developed for the equivalent
problem of solving for an equilibrium in a zero-sum game.
From game-theoretic perspective, the saddle point can be
viewed as an equilibrium of a game between two players:
the Q-player choosing Q and the λ-player choosing λ. The
Lagrangian L(Q, λ) speciﬁes how much the Q-player has to
pay to the λ-player after they make their choices. At the sad-
dle point, neither player wants to deviate from their choice.

Our algorithm ﬁnds an approximate equilibrium in which
neither player can gain more than ν by changing their choice

Algorithm 1 Exp. gradient reduction for fair classiﬁcation

Input: training examples {(Xi, Yi, Ai)}n

i=1

fairness constraints speciﬁed by gj, Ej, M, (cid:98)c
bound B, accuracy ν, learning rate η

Set θ1 = 0 ∈ R|K|
for t = 1, 2, . . . do
Set λt,k = B
1+(cid:80)
ht ← BESTh(λt)
(cid:98)Qt ← 1
t
(cid:98)λt ← 1
t

(cid:80)t

(cid:80)t

νt ← max
if νt ≤ ν then

Return ( (cid:98)Qt, (cid:98)λt)

exp{θk}

k(cid:48) ∈K exp{θk(cid:48) } for all k ∈ K
(cid:16)

(cid:17)

t(cid:48)=1 ht(cid:48), L ← L

t(cid:48)=1 λt(cid:48), L ← L
(cid:110)

(cid:16)

(cid:98)Qt, BESTλ( (cid:98)Qt)
(cid:17)

BESTh((cid:98)λt), (cid:98)λt
(cid:111)

L( (cid:98)Qt, (cid:98)λt) − L, L − L( (cid:98)Qt, (cid:98)λt)

end if
Set θt+1 = θt + η (M(cid:98)µ(ht) − (cid:98)c)

end for

(where ν > 0 is an input to the algorithm). Such an approx-
imate equilibrium corresponds to a ν-approximate saddle
point of the Lagrangian, which is a pair ( (cid:98)Q, (cid:98)λ), where

for all Q ∈ ∆,
for all λ ∈ R|K|

L( (cid:98)Q, (cid:98)λ) ≥ L( (cid:98)Q, λ) − ν

+ , (cid:107)λ(cid:107)1 ≤ B.

We proceed iteratively by running a no-regret algorithm for
the λ-player, while executing the best response of the Q-
player. Following Freund & Schapire (1996), the average
play of both players converges to the saddle point. We run
the exponentiated gradient algorithm (Kivinen & Warmuth,
1997) for the λ-player and terminate as soon as the subop-
timality of the average play falls below the pre-speciﬁed
accuracy ν. The best response of the Q-player can always
be chosen to put all of the mass on one of the candidate
classiﬁers h ∈ H, and can be implemented by a single call
to a cost-sensitive classiﬁcation algorithm for the set H.

Algorithm 1 fully implements this scheme, except for the
functions BESTλ and BESTh, which correspond to the best-
response algorithms of the two players. (We need the best
response of the λ-player to evaluate whether the subopti-
mality of the current average play has fallen below ν.) The
two best response functions can be calculated as follows.

BESTλ(Q): the best response of the λ-player. The best
response of the λ-player for a given Q is any maximizer of
L(Q, λ) over all valid λs. In our setting, it can always be
chosen to be either 0 or put all of the mass on the most vio-
lated constraint. Letting (cid:98)γ(Q) := M(cid:98)µ(Q) and letting ek de-
note the kth vector of the standard basis, BESTλ(Q) returns
(cid:40)

0
Bek∗

if (cid:98)γ(Q) ≤ (cid:98)c,
otherwise, where k∗ = arg maxk[(cid:98)γk(Q) − (cid:98)ck].

A Reductions Approach to Fair Classiﬁcation

BESTh(λ): the best response of the Q-player. Here, the
best response minimizes L(Q, λ) over all Qs in the simplex.
Because L is linear in Q, the minimizer can always be cho-
sen to put all of the mass on a single classiﬁer h. We show
how to obtain the classiﬁer constituting the best response
via a reduction to cost-sensitive classiﬁcation. Letting pj :=
(cid:98)P[Ej] be the empirical event probabilities, the Lagrangian
for Q which puts all of the mass on a single h is then
L(h, λ) = (cid:99)err(h) + λ(cid:62)(cid:0)M(cid:98)µ(h) − (cid:98)c(cid:1)
(cid:88)
= (cid:98)E(cid:2)1{h(X) (cid:54)= Y }(cid:3) − λ(cid:62)

Mk,jλk (cid:98)µj(h)

(cid:98)c +

k,j

= −λ(cid:62)

(cid:88)

+

k,j

(cid:98)c + (cid:98)E(cid:2)1{h(X) (cid:54)= Y }(cid:3)
(cid:104)
Mk,jλk
(cid:98)E
pj

gj

(cid:0)X,A,Y,h(X)(cid:1) 1{(X,A,Y ) ∈ Ej}

(cid:105)
.

Assuming a data set of training examples {(Xi, Ai, Yi)}n
i=1,
the minimization of L(h, λ) over h then corresponds to cost-
sensitive classiﬁcation on {(Xi, C 0

i=1 with costs4

i , C 1

i )}n

Mk,jλk
pj

C 0

i = 1{Yi (cid:54)= 0}
(cid:88)

+

C 1

k,j
i = 1{Yi (cid:54)= 1}
(cid:88)

Mk,jλk
pj

+

k,j

gj(Xi,Ai,Yi, 0) 1{(Xi,Ai,Yi) ∈ Ej}

gj(Xi,Ai,Yi, 1) 1{(Xi,Ai,Yi) ∈ Ej}.

needing more iterations to reach any given suboptimality. In
particular, as we derive in the theorem, achieving subopti-
mality ν may need up to 4ρ2B2 log(|K| + 1) / ν2 iterations.
Example 3 (DP). Using the matrix M for demographic
parity as described in Section 2, the cost-sensitive reduction
for a vector of Lagrange multipliers λ uses costs

C 0

i = 1{Yi (cid:54)= 0}, C 1

i = 1{Yi (cid:54)= 1} +

λAi
pAi

(cid:88)

−

λa,

a∈A

where pa := (cid:98)P[A = a] and λa := λ(a,+) − λ(a,−), effec-
tively replacing two non-negative Lagrange multipliers by a
single multiplier, which can be either positive or negative.
Because ck = 0 for all k, (cid:98)ck = εk. Furthermore, because
all empirical moments are bounded in [0, 1], we can assume
εk ≤ 1, which yields the bound ρ ≤ 2. Thus, Algorithm 1
terminates in at most 16B2 log(2 |A| + 1) / ν2 iterations.
Example 4 (EO). For equalized odds, the cost-sensitive
reduction for a vector of Lagrange multipliers λ uses costs

C 0

i = 1{Yi (cid:54)= 0},

C 1

i = 1{Yi (cid:54)= 1} +

λ(Ai,Yi)
p(Ai,Yi)

(cid:88)

−

a∈A

λ(a,Yi)
p((cid:63),Yi)

,

where p(a,y) := (cid:98)P[A = a, Y = y], p((cid:63),y) := (cid:98)P[Y = y], and
λ(a,y) := λ(a,y,+) − λ(a,y,−). If we again assume εk ≤ 1,
then we obtain the bound ρ ≤ 2. Thus, Algorithm 1 termi-
nates in at most 16B2 log(4 |A| + 1) / ν2 iterations.

Theorem 1. Letting ρ := maxh(cid:107)M(cid:98)µ(h) − (cid:98)c(cid:107)∞, Algo-
rithm 1 satisﬁes the inequality

3.3. Error Analysis

νt ≤

B log(|K| + 1)
ηt

+ ηρ2B.

Thus, for η = ν
saddle point of L in at most 4ρ2B2 log(|K|+1)

2ρ2B , Algorithm 1 will return a ν-approximate

iterations.

ν2

√

This theorem, proved in Appendix B, bounds the subopti-
mality νt of the average play ( (cid:98)Qt, (cid:98)λt), which is equal to its
suboptimality as a saddle point. The right-hand side of the
bound is optimized by η = (cid:112)log(|K| + 1) / (ρ
t), lead-
ing to the bound νt ≤ 2ρB(cid:112)log(|K| + 1) / t. This bound
decreases with the number of iterations t and grows very
slowly with the number of constraints |K|. The quantity ρ
is a problem-speciﬁc constant that bounds how much any
single classiﬁer h ∈ H can violate the desired set of fair-
ness constraints. Finally, B is the bound on the (cid:96)1-norm of
λ, which we introduced to enable this speciﬁc algorithmic
scheme. In general, larger values of B will bring the prob-
lem (P) closer to (6), and thus also to (4), but at the cost of

4For general error, err(h) = E[gerr(X, A, Y, h(X))], the costs
i contain, respectively, the terms gerr(Xi, Ai, Yi, 0) and

C 0
gerr(Xi, Ai, Yi, 1) instead of 1{Yi (cid:54)= 0} and 1{Yi (cid:54)= 1}.

i and C 1

Our ultimate goal, as formalized in equation (3), is to
minimize the classiﬁcation error while satisfying fairness
constraints under a true but unknown distribution over
(X, A, Y ). In the process of deriving Algorithm 1, we in-
troduced three different sources of error. First, we replaced
the true classiﬁcation error and true moments with their
empirical versions. Second, we introduced a bound B on
the magnitude of λ. Finally, we only run the optimization
algorithm for a ﬁxed number of iterations, until it reaches
suboptimality level ν. The ﬁrst source of error, due to the
use of empirical rather than true quantities, is unavoidable
and constitutes the underlying statistical error. The other two
sources of error, the bound B and the suboptimality level ν,
stem from the optimization algorithm and can be driven
arbitrarily small at the cost of additional iterations. In this
section, we show how the statistical error and the optimiza-
tion error affect the true accuracy and the fairness of the ran-
domized classiﬁer returned by Algorithm 1—in other words,
how well Algorithm 1 solves our original problem (3).

To bound the statistical error, we use the Rademacher
complexity of the classiﬁer family H, which we denote
by Rn(H), where n is the number of training examples.
We assume that Rn(H) ≤ Cn−α for some C ≥ 0 and

A Reductions Approach to Fair Classiﬁcation

α ≤ 1/2. We note that α = 1/2 in the vast majority
including norm-bounded linear
of classiﬁer families,
functions (see Theorem 1 of Kakade et al., 2009), neural
networks (see Theorem 18 of Bartlett & Mendelson, 2002),
and classiﬁer families with bounded VC dimension (see
Lemma 4 and Theorem 6 of Bartlett & Mendelson, 2002).

Recall that in our empirical optimization problem we as-
sume that (cid:98)ck = ck + εk, where εk ≥ 0 are error bounds that
account for the discrepancy between µ(Q) and (cid:98)µ(Q). In
our analysis, we assume that these error bounds have been
set in accordance with the Rademacher complexity of H.
Assumption 1. There exists C, C (cid:48) ≥ 0 and α ≤ 1/2
such that Rn(H) ≤ Cn−α and εk = C (cid:48) (cid:80)
j∈J|Mk,j|n−α
,
where nj is the number of data points that fall in Ej,

j

nj := (cid:12)
(cid:12)

(cid:8)i : (Xi, Ai, Yi) ∈ Ej

(cid:9)(cid:12)
(cid:12).

The optimization error can be bounded via a careful analy-
sis of the Lagrangian and the optimality conditions of (P)
and (D). Combining the three different sources of error
yields the following bound, which we prove in Appendix C.
Theorem 2. Let Assumption 1 hold for C (cid:48) ≥ 2C +
2 + (cid:112)ln(4/δ) / 2, where δ > 0. Let ( (cid:98)Q, (cid:98)λ) be any ν-
approximate saddle point of L, let Q(cid:63) minimize err(Q) sub-
j = P[Ej]. Then, with proba-
ject to Mµ(Q) ≤ c, and let p(cid:63)
bility at least 1 − (|J| + 1)δ, the distribution (cid:98)Q satisﬁes

err( (cid:98)Q) ≤ err(Q(cid:63)) + 2ν + (cid:101)O(n−α),
1+2ν
B

γk( (cid:98)Q) ≤ ck +

(cid:88)

+

j∈J

|Mk,j| (cid:101)O(n−α
j )

for all k,

where (cid:101)O(·) suppresses polynomial dependence on ln(1/δ).
If np(cid:63)

j ≥ 8 log(2/δ) for all j, then, for all k,

γk( (cid:98)Q) ≤ ck +

1+2ν
B

(cid:88)

+

j∈J

|Mk,j| (cid:101)O

(cid:16)

(np(cid:63)

j )−α(cid:17)

.

the solution returned by Algorithm 1
In other words,
achieves the lowest feasible classiﬁcation error on the true
distribution up to the optimization error, which grows lin-
early with ν, and the statistical error, which grows as n−α.
Therefore, if we want to guarantee that the optimization er-
ror does not dominate the statistical error, we should set ν ∝
n−α. The fairness constraints on the true distribution are
satisﬁed up to the optimization error (1 + 2ν) /B and up to
the statistical error. Because the statistical error depends on
the moments, and the error in estimating the moments grows
as n−α
j ≥ n−α, we can set B ∝ nα to guarantee that the op-
timization error does not dominate the statistical error. Com-
bining this reasoning with the learning rate setting of Theo-
rem 1 yields the following theorem (proved in Appendix C).
Theorem 3. Let ρ := maxh(cid:107)M(cid:98)µ(h) − (cid:98)c(cid:107)∞. Let Assump-
tion 1 hold for C (cid:48) ≥ 2C + 2 + (cid:112)ln(4/δ) / 2, where δ > 0.

Let Q(cid:63) minimize err(Q) subject to Mµ(Q) ≤ c. Then
Algorithm 1 with ν ∝ n−α, B ∝ nα and η ∝ ρ−2n−2α ter-
minates in O(ρ2n4α ln |K|) iterations and returns (cid:98)Q, which
with probability at least 1 − (|J| + 1)δ satisﬁes

err( (cid:98)Q) ≤ err(Q(cid:63)) + (cid:101)O(n−α),
γk( (cid:98)Q) ≤ ck +

|Mk,j| (cid:101)O(n−α

(cid:88)

j

)

j∈J

for all k.

Example 5 (DP). If na denotes the number of training ex-
amples with Ai = a, then Assumption 1 states that we
should set ε(a,+) = ε(a,−) = C (cid:48)(n−α
a + n−α) and Theo-
rem 3 then shows that for a suitable setting of C (cid:48), ν, B,
and η, Algorithm 1 will return a randomized classiﬁer (cid:98)Q
with the lowest feasible classiﬁcation error up to (cid:101)O(n−α)
while also approximately satisfying the fairness constraints

(cid:12)
(cid:12)
(cid:12)

(cid:12)
E[h(X) | A = a] − E[h(X)]
(cid:12) ≤ (cid:101)O(n−α
(cid:12)
a )

for all a,

where E is with respect to (X, A, Y ) as well as h ∼ (cid:98)Q.
Example 6 (EO). Similarly, if n(a,y) denotes the number
of examples with Ai = a and Yi = y and n((cid:63),y) denotes the
number of examples with Yi = y, then Assumption 1 states
that we should set ε(a,y,+) = ε(a,y,−) = C (cid:48)(n−α
((cid:63),y))
and Theorem 3 then shows that for a suitable setting of C (cid:48), ν,
B, and η, Algorithm 1 will return a randomized classiﬁer (cid:98)Q
with the lowest feasible classiﬁcation error up to (cid:101)O(n−α)
while also approximately satisfying the fairness constraints

(a,y) + n−α

(cid:12)
(cid:12)
(cid:12)

(cid:12)
E[h(X) | A = a, Y = y] − E[h(X) | Y = y]
(cid:12) ≤ (cid:101)O(n−α
(cid:12)

(a,y))

for all a, y. Again, E includes randomness under the true
distribution over (X, A, Y ) as well as h ∼ (cid:98)Q.

3.4. Grid Search

In some situations, it is preferable to select a deterministic
classiﬁer, even if that means a lower accuracy or a modest vi-
olation of the fairness constraints. A set of candidate classi-
ﬁers can be obtained from the saddle point (Q†, λ†). Specif-
ically, because Q† is a minimizer of L(Q, λ†) and L is
linear in Q, the distribution Q† puts non-zero mass only on
classiﬁers that are the Q-player’s best responses to λ†. If we
knew λ†, we could retrieve one such best response via the re-
duction to cost-sensitive learning introduced in Section 3.2.
We can compute λ† using Algorithm 1, but when the number
of constraints is very small, as is the case for demographic
parity or equalized odds with a binary protected attribute,
it is also reasonable to consider a grid of values λ, calculate
the best response for each value, and then select the value
with the desired tradeoff between accuracy and fairness.
Example 7 (DP). When the protected attribute is binary,
e.g., A ∈ {a, a(cid:48)}, then the grid search can in fact be con-
ducted in a single dimension. The reduction formally takes

A Reductions Approach to Fair Classiﬁcation

two real-valued arguments λa and λa(cid:48), and then adjusts the
costs for predicting h(Xi) = 1 by the amounts

δa =

− λa − λa(cid:48)

and δa(cid:48) =

− λa − λa(cid:48),

λa(cid:48)
pa(cid:48)

λa
pa

respectively, on the training examples with Ai = a and
Ai = a(cid:48). These adjustments satisfy paδa + pa(cid:48)δa(cid:48) = 0,
so instead of searching over λa and λa(cid:48), we can carry out
the grid search over δa alone and apply the adjustment
δa(cid:48) = −paδa/pa(cid:48) to the protected attribute value a(cid:48).
With three attribute values, e.g., A ∈ {a, a(cid:48), a(cid:48)(cid:48)}, we sim-
ilarly have paδa + pa(cid:48)δa(cid:48) + pa(cid:48)(cid:48)δa(cid:48)(cid:48) = 0, so it sufﬁces to
conduct grid search in two dimensions rather than three.
Example 8 (EO). If A ∈ {a, a(cid:48)}, we obtain the adjustment

δ(a,y) =

λ(a,y)
p(a,y)

−

λ(a,y) + λ(a(cid:48),y)
p((cid:63),y)

for an example with protected attribute value a and label y,
and similarly for protected attribute value a(cid:48). In this case,
separately for each y, the adjustments satisfy

p(a,y)δ(a,y) + p(a(cid:48),y)δ(a(cid:48),y) = 0,

so it sufﬁces to do the grid search over δ(a,0) and δ(a,1) and
set the parameters for a(cid:48) to δ(a(cid:48),y) = −p(a,y)δ(a,y)/p(a(cid:48),y).

4. Experimental Results

We now examine how our exponentiated-gradient reduc-
tion5 performs at the task of binary classiﬁcation subject to
either demographic parity or equalized odds. We provide an
evaluation of our grid-search reduction in Appendix D.

We compared our reduction with the score-based post-
processing algorithm of Hardt et al. (2016), which takes as
its input any classiﬁer, (i.e., a standard classiﬁer without any
fairness constraints) and derives a monotone transformation
of the classiﬁer’s output to remove any disparity with respect
to the training examples. This post-processing algorithm
works with both demographic parity and equalized odds, as
well as with binary and non-binary protected attributes.

For demographic parity, we also compared our reduction
with the reweighting and relabeling approaches of Kamiran
& Calders (2012). Reweighting can be applied to both
binary and non-binary protected attributes and operates by
changing importance weights on each example with the
goal of removing any statistical dependence between the
protected attribute and label.6 Relabeling was developed for

5https://github.com/Microsoft/fairlearn
6Although reweighting was developed for demographic parity,
the weights that it induces are achievable by our grid search, albeit
the grid search for equalized odds rather than demographic parity.

binary protected attributes. First, a classiﬁer is trained on
the original data (without considering fairness). The training
examples close to the decision boundary are then relabeled
to remove all disparity while minimally affecting accuracy.
The ﬁnal classiﬁer is then trained on the relabeled data.

As the base classiﬁers for our reductions, we used the
weighted classiﬁcation implementations of logistic regres-
sion and gradient-boosted decision trees in scikit-learn (Pe-
In addition to the three baselines
dregosa et al., 2011).
described above, we also compared our reductions to the
“unconstrained” classiﬁers trained to optimize accuracy only.

We used four data sets, randomly splitting each one into
training examples (75%) and test examples (25%):

• The adult income data set (Lichman, 2013) (48,842
examples). Here the task is to predict whether some-
one makes more than $50k per year, with gender as the
protected attribute. To examine the performance for
non-binary protected attributes, we also conducted an-
other experiment with the same data, using both gender
and race (binarized into white and non-white) as the
protected attribute. Relabeling, which requires binary
protected attributes, was therefore not applicable here.
• ProPublica’s COMPAS recidivism data (7,918 exam-
ples). The task is to predict recidivism from someone’s
criminal history, jail and prison time, demographics,
and COMPAS risk scores, with race as the protected
attribute (restricted to white and black defendants).
• Law School Admissions Council’s National Longitu-
dinal Bar Passage Study (Wightman, 1998) (20,649
examples). Here the task is to predict someone’s even-
tual passage of the bar exam, with race (restricted to
white and black only) as the protected attribute.

• The Dutch census data set (Dutch Central Bureau for
Statistics, 2001) (60,420 examples). Here the task is
to predict whether or not someone has a prestigious
occupation, with gender as the protected attribute.

While all the evaluated algorithms require access to the pro-
tected attribute A at training time, only the post-processing
algorithm requires access to A at test time. For a fair com-
parison, we included A in the feature vector X, so all algo-
rithms had access to it at both the training time and test time.

We used the test examples to measure the classiﬁcation error
for each approach, as well as the violation of the desired fair-
(cid:12)
(cid:12)E[h(X) | A = a] − E[h(X)](cid:12)
ness constraints, i.e., maxa
(cid:12)
(cid:12)
(cid:12)E[h(X) | A = a, Y = y] − E[h(X) | Y = y](cid:12)
and maxa,y
(cid:12)
for demographic parity and equalized odds, respectively.

We ran our reduction across a wide range of tradeoffs be-
tween the classiﬁcation error and fairness constraints. We
considered ε ∈ {0.001, . . . , 0.1} and for each value ran
Algorithm 1 with (cid:98)ck = ε across all k. As expected, the
returned randomized classiﬁers tracked the training Pareto

A Reductions Approach to Fair Classiﬁcation

Figure 1. Test classiﬁcation error versus constraint violation with respect to DP (top two rows) and EO (bottom two rows). All data sets
have binary protected attributes except for adult4, which has four protected attribute values, so relabeling is not applicable there. For our
reduction approach we plot the convex envelope of the classiﬁers obtained on training data at various accuracy–fairness tradeoffs. We
show 95% conﬁdence bands for the classiﬁcation error of our reduction approach and 95% conﬁdence intervals for the constraint violation
of post-processing. Our reduction approach dominates or matches the performance of the other approaches up to statistical uncertainty.

frontier (see Figure 2 in Appendix D). In Figure 1, we evalu-
ate these classiﬁers alongside the baselines on the test data.

For all the data sets, the range of classiﬁcation errors
is much smaller than the range of constraint violations.
Almost all the approaches were able to substantially reduce
or remove disparity without much impact on classiﬁer accu-
racy. One exception was the Dutch census data set, where
the classiﬁcation error increased the most in relative terms.

Our reduction generally dominated or matched the baselines.
The relabeling approach frequently yielded solutions that
were not Pareto optimal. Reweighting yielded solutions
on the Pareto frontier, but often with substantial disparity.
As expected, post-processing yielded disparities that were
statistically indistinguishable from zero, but the resulting
classiﬁcation error was sometimes higher than achieved by
our reduction under a statistically indistinguishable dispar-
ity. In addition, and unlike the post-processing algorithm,
our reduction can achieve any desired accuracy–fairness
tradeoff, allows a wider range of fairness deﬁnitions, and
does not require access to the protected attribute at test time.

Our grid-search reduction, evaluated in Appendix D,
sometimes failed to achieve the lowest disparities on

the training data, but its performance on the test data
very closely matched that of our exponentiated-gradient
reduction. However, if the protected attribute is non-binary,
then grid search is not feasible. For instance, for the version
of the adult income data set where the protected attribute
takes on four values, the grid search would need to span
three dimensions for demographic parity and six dimensions
for equalized odds, both of which are prohibitively costly.

5. Conclusion

We presented two reductions for achieving fairness in a
binary classiﬁcation setting. Our reductions work for any
classiﬁer representation, encompass many deﬁnitions of fair-
ness, satisfy provable guarantees, and work well in practice.

Our reductions optimize the tradeoff between accuracy and
any (single) deﬁnition of fairness given training-time access
to protected attributes. Achieving fairness when training-
time access to protected attributes is unavailable remains an
open problem for future research, as does the navigation of
tradeoffs between accuracy and multiple fairness deﬁnitions.

A Reductions Approach to Fair Classiﬁcation

Acknowledgements

We would like to thank Aaron Roth, Sam Corbett-Davies,
and Emma Pierson for helpful discussions.

References

Agarwal, A., Beygelzimer, A., Dud´ık, M., and Langford, J.
A reductions approach to fair classication. In Fairness,
Accountability, and Transparency in Machine Learning
(FATML), 2017.

Alabi, D., Immorlica, N., and Kalai, A. T. Unleashing linear
optimizers for group-fair learning and optimization. In
Proceedings of the 31st Annual Conference on Learning
Theory (COLT), 2018.

Barocas, S. and Selbst, A. D. Big data’s disparate impact.

California Law Review, 104:671–732, 2016.

Bartlett, P. L. and Mendelson, S. Rademacher and gaussian
complexities: Risk bounds and structural results. Journal
of Machine Learning Research, 3:463–482, 2002.

Berk, R., Heidari, H., Jabbari, S., Kearns, M., and Roth, A.
Fairness in criminal justice risk assessments: The state of
the art. arXiv:1703.09207, 2017.

Beygelzimer, A., Dani, V., Hayes, T. P., Langford, J., and
Zadrozny, B. Error limiting reductions between clas-
siﬁcation tasks. In Proceedings of the Twenty-Second
International Conference on Machine Learning (ICML),
pp. 49–56, 2005.

Boucheron, S., Bousquet, O., and Lugosi, G. Theory of
classiﬁcation: a survey of some recent advances. ESAIM:
Probability and Statistics, 9:323–375, 2005.

Calmon, F., Wei, D., Vinzamuri, B., Ramamurthy, K. N.,
and Varshney, K. R. Optimized pre-processing for dis-
crimination prevention. In Advances in Neural Informa-
tion Processing Systems 30, 2017.

Chouldechova, A. Fair prediction with disparate impact: A
study of bias in recidivism prediction instruments. Big
Data, Special Issue on Social and Technical Trade-Offs,
2017.

Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., and Huq,
A. Algorithmic decision making and the cost of fairness.
In Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining,
pp. 797–806, 2017.

Donini, M., Oneto, L., Ben-David, S., Shawe-Taylor, J., and
Pontil, M. Empirical risk minimization under fairness
constraints. 2018. arXiv:1802.08626.

Dwork, C., Hardt, M., Pitassi, T., Reingold, O., and Zemel,
R. Fairness through awareness. In Proceedings of the 3rd
Innovations in Theoretical Computer Science Conference,
pp. 214–226, 2012.

Dwork, C., Immorlica, N., Kalai, A. T., and Leiserson, M.
Decoupled classiﬁers for group-fair and efﬁcient machine
learning. In Conference on Fairness, Accountability and
Transparency (FAT (cid:63)), pp. 119–133, 2018.

Fan, W., Stolfo, S. J., Zhang, J., and Chan, P. K. Adacost:
Misclassiﬁcation cost-sensitive boosting. In Proceedings
of the Sixteenth International Conference on Machine
Learning (ICML), pp. 97–105, 1999.

Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C.,
and Venkatasubramanian, S. Certifying and removing dis-
parate impact. In Proceedings of the 21st ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining, 2015.

Freund, Y. and Schapire, R. E. Game theory, on-line pre-
diction and boosting. In Proceedings of the Ninth Annual
Conference on Computational Learning Theory (COLT),
pp. 325–332, 1996.

Freund, Y. and Schapire, R. E. A decision-theoretic general-
ization of on-line learning and an application to boosting.
Journal of Computer and System Sciences, 55(1):119–
139, 1997.

Hardt, M., Price, E., and Srebro, N. Equality of opportunity
in supervised learning. In Neural Information Processing
Systems (NIPS), 2016.

Johnson, K. D., Foster, D. P., and Stine, R. A. Impartial pre-
dictive modeling: Ensuring fairness in arbitrary models.
arXiv:1608.00528, 2016.

Kakade, S. M., Sridharan, K., and Tewari, A. On the com-
plexity of linear prediction: Risk bounds, margin bounds,
and regularization. In Advances in neural information
processing systems, pp. 793–800, 2009.

Kamiran, F. and Calders, T. Data preprocessing techniques
for classiﬁcation without discrimination. Knowledge and
Information Systems, 33(1):1–33, 2012.

Kamishima, T., Akaho, S., and Sakuma, J. Fairness-aware
learning through regularization approach. In 2011 IEEE
11th International Conference on Data Mining Work-
shops, pp. 643–650, 2011.

Kearns, M., Neel, S., Roth, A., and Wu, Z. S. Preventing
fairness gerrymandering: Auditing and learning for sub-
group fairness. In Proceedings of the 35th International
Conference on Machine Learning (ICML), 2018.

A Reductions Approach to Fair Classiﬁcation

Kivinen, J. and Warmuth, M. K. Exponentiated gradient
versus gradient descent for linear predictors. Information
and Computation, 132(1):1–63, 1997.

Kleinberg, J., Mullainathan, S., and Raghavan, M. Inherent
trade-offs in the fair determination of risk scores. In Pro-
ceedings of the 8th Innovations in Theoretical Computer
Science Conference, 2017.

Langford, J. and Beygelzimer, A. Sensitive error correct-
In Proceedings of the 18th Annual
ing output codes.
Conference on Learning Theory (COLT), pp. 158–172,
2005.

Ledoux, M. and Talagrand, M. Probability in Banach
Spaces: Isoperimetry and Processes. Springer, 1991.

Lichman, M. UCI machine learning repository, 2013. URL

http://archive.ics.uci.edu/ml.

Menon, A. K. and Williamson, R. C. The cost of fairness in
binary classiﬁcation. In Proceedings of the Conference
on Fiarness, Accountability, and Transparency, 2018.

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-
napeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
Scikit-learn: Machine learning in Python. Journal of
Machine Learning Research, 12:2825–2830, 2011.

Rockafellar, R. T. Convex analysis. Princeton University

Press, 1970.

Shalev-Shwartz, S. Online learning and online convex opti-
mization. Foundations and Trends in Machine Learning,
4(2):107–194, 2012.

Wightman, L. LSAC National Longitudinal Bar Passage

Study, 1998.

Woodworth, B. E., Gunasekar, S., Ohannessian, M. I., and
Srebro, N. Learning non-discriminatory predictors. In
Proceedings of the 30th Conference on Learning Theory
(COLT), pp. 1920–1953, 2017.

Zafar, M. B., Valera, I., Rodriguez, M. G., and Gummadi,
K. P. Fairness constraints: Mechanisms for fair classiﬁca-
tion. In Proceedings of the 20th International Conference
on Artiﬁcial Intelligence and Statistics (AISTATS), pp.
962–970, 2017.

A. Error and Fairness for Randomized Classiﬁers

A Reductions Approach to Fair Classiﬁcation

Let D denote the distribution over triples (X, A, Y ). The accuracy of a classiﬁer h ∈ H is measured by 0-1 error,
err(h) := PD[h(X) (cid:54)= Y ], which for a randomized classiﬁer Q becomes

err(Q) :=

P
(X,A,Y )∼D, h∼Q

[h(X) (cid:54)= Y ] =

Q(h) err(h) .

(cid:88)

h∈H

The fairness constraints on a classiﬁer h are Mµ(h) ≤ c. Recall that µj(h) := ED[gj(X, A, Y, h(X)) | Ej]. For a
randomized classiﬁer Q we deﬁne its moment µj as

µj(Q) :=

E
(X,A,Y )∼D, h∼Q

(cid:104)
gj(X, A, Y, h(X))

(cid:105)

Ej

(cid:12)
(cid:12)
(cid:12)

=

(cid:88)

h∈H

Q(h)µj(h) ,

where the last equality follows because Ej is independent of the choice of h.

B. Proof of Theorem 1

The proof follows immediately from the analysis of Freund & Schapire (1996) applied to the Exponentiated Gradient (EG)
algorithm (Kivinen & Warmuth, 1997), which in our speciﬁc case is also equivalent to Hedge (Freund & Schapire, 1997).
Let Λ := {λ ∈ R|K|
that is equal to λ on coordinates 1 through |K| and puts the remaining mass on the coordinate λ(cid:48)

: (cid:107)λ(cid:48)(cid:107)1 = B}. We associate any λ ∈ Λ with the λ(cid:48) ∈ Λ(cid:48)

+ : (cid:107)λ(cid:48)(cid:107)1 ≤ B} and Λ(cid:48) := {λ(cid:48) ∈ R|K|+1

+

|K|+1.

Consider a run of Algorithm 1. For each λt, let λ(cid:48)
t ∈ R|K|+1 be equal to rt on coordinates 1 through |K| and put zero on the coordinate r(cid:48)
r(cid:48)
associated λ(cid:48), we have, for all t,

t ∈ Λ(cid:48) be the associated element of Λ(cid:48). Let rt := M(cid:98)µ(ht) − (cid:98)c and let
t,|K|+1. Thus, for any λ and the

and, in particular,

λ(cid:62)rt = (λ(cid:48))(cid:62)r(cid:48)

t ,

λ(cid:62)
t

(cid:0)M(cid:98)µ(ht) − (cid:98)c(cid:1) = λ(cid:62)

t rt = (λ(cid:48)

t)(cid:62)r(cid:48)

t .

t as the reward vector for the λ-player. The choices of λ(cid:48)

We interpret r(cid:48)
the learning rate η. By the assumption of the theorem we have (cid:107)r(cid:48)
Corollary 2.14 of Shalev-Shwartz (2012), then states that for any λ(cid:48) ∈ Λ(cid:48),

t then correspond to those of the EG algorithm with
t(cid:107)∞ = (cid:107)rt(cid:107)∞ ≤ ρ. The regret bound for EG, speciﬁcally,

Therefore, by equations (7) and (8), we also have for any λ ∈ Λ,

T
(cid:88)

t=1

(λ(cid:48))(cid:62)r(cid:48)

t ≤

(λ(cid:48)

t)(cid:62)r(cid:48)

t +

B log(|K| + 1)
η

(cid:124)

(cid:123)(cid:122)
=:ζT

+ ηρ2BT

.

(cid:125)

T
(cid:88)

t=1

T
(cid:88)

t=1

λ(cid:62)rt ≤

λ(cid:62)

t rt + ζT .

T
(cid:88)

t=1

(7)

(8)

(9)

This regret bound can be used to bound the suboptimality of L( (cid:98)QT , (cid:98)λT ) in (cid:98)λT as follows:

A Reductions Approach to Fair Classiﬁcation

L( (cid:98)QT , λ) =

T
(cid:88)

(cid:16)

(cid:99)err(ht) + λ(cid:62)(cid:0)M(cid:98)µ(ht) − (cid:98)c(cid:1)(cid:17)

Equation (10) follows from the regret bound (9). Equation (11) follows because L(ht, λt) ≤ L(Q, λt) for all Q by the
choice of ht as the best response of the Q-player. Finally, equation (12) follows by linearity of L(Q, λ) in λ. Thus, we have
for all λ ∈ Λ,

Also, for any Q,

1
T

1
T

1
T

1
T

1
T

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

=

≤

=

≤

(cid:16)
(cid:99)err(ht) + λ(cid:62)rt

(cid:17)

(cid:16)
(cid:99)err(ht) + λ(cid:62)

t rt

(cid:17)

+

ζT
T

L(ht, λt) +

L( (cid:98)QT , λt) +

ζT
T

ζT
T

(cid:16)

= L

(cid:98)QT ,

1
T

T
(cid:88)

t=1

(cid:17)

λt

+

ζT
T

= L( (cid:98)QT , (cid:98)λT ) +

ζT
T

.

L( (cid:98)QT , (cid:98)λT ) ≥ L( (cid:98)QT , λ) −

ζT
T

.

L(Q, (cid:98)λT ) =

L(Q, λt)

1
T

1
T

1
T

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

≥

≥

L(ht, λt)

L(ht, (cid:98)λT ) −

ζT
T

= L( (cid:98)QT , (cid:98)λT ) −

ζT
T

,

L( (cid:98)QT , (cid:98)λT ) ≤ L(Q, (cid:98)λT ) +

ζT
T

.

νT ≤

=

ζT
T

B log(|K| + 1)
ηT

+ ηρ2B ,

(10)

(11)

(12)

(13)

(14)

(15)

(16)

(17)

(18)

where equation (14) follows by linearity of L(Q, λ) in λ, equation (15) follows by the optimality of ht with respect to (cid:98)λt,
equation (16) from the regret bound (9), and equation (17) by linearity of L(Q, λ) in Q. Thus, for all Q,

Equations (13) and (18) immediately imply that for any T ≥ 1,

proving the ﬁrst part of the theorem.

The second part of the theorem follows by plugging in η = ν

2ρ2B and verifying that if T ≥ 4ρ2B2 log(|K|+1)

ν2

then

νT ≤

B log(|K| + 1)
2ρ2B · 4ρ2B2 log(|K|+1)

ν2

ν

+

ν
2ρ2B

· ρ2B =

+

.

ν
2

ν
2

A Reductions Approach to Fair Classiﬁcation

C. Proofs of Theorems 2 and 3

The bulk of this appendix proves the following theorem, which will immediately imply Theorems 2 and 3.

Theorem 4. Let ( (cid:98)Q, (cid:98)λ) be any ν-approximate saddle point of L with

(cid:98)ck = ck + εk

and εk ≥

|Mk,j|

2Rnj (H) +

(cid:32)

(cid:88)

j∈J

(cid:115)

2
√
nj

+

ln(2/δ)
2nj

(cid:33)

.

Let Q(cid:63) minimize err(Q) subject to Mµ(Q) ≤ c. Then with probability at least 1 − (|J| + 1)δ, the distribution (cid:98)Q satisﬁes

err( (cid:98)Q) ≤ err(Q(cid:63)) + 2ν + 4Rn(H) +

(cid:114)

4
√
n

+

2 ln(2/δ)
n

,

and for all k, γk( (cid:98)Q) ≤ ck +

+ 2εk .

1 + 2ν
B

Let Λ := {λ ∈ R|K|
pair ( (cid:98)Q, (cid:98)λ) which is a ν-approximate saddle point of L, i.e.,

+ : (cid:107)λ(cid:48)(cid:107)1 ≤ B} denote the domain of λ. In the remainder of the section, we assume that we are given a

L( (cid:98)Q, (cid:98)λ) ≤ L(Q, (cid:98)λ) + ν

for all Q ∈ ∆,

and L( (cid:98)Q, (cid:98)λ) ≥ L( (cid:98)Q, λ) − ν

for all λ ∈ Λ.

We ﬁrst establish that the pair ( (cid:98)Q, (cid:98)λ) satisﬁes an approximate version of complementary slackness. For the statement and
proof of the following lemma, recall that (cid:98)γ(Q) = M(cid:98)µ(Q), so the empirical fairness constraints can be written as (cid:98)γ(Q) ≤ (cid:98)c
and the Lagrangian L can be written as

(19)

(20)

L(Q, λ) = (cid:99)err(Q) +

λk((cid:98)γk(Q) − (cid:98)ck) .

(cid:88)

k∈K

Lemma 1 (Approximate complementary slackness). The pair ( (cid:98)Q, (cid:98)λ) satisﬁes

(cid:88)

k∈K

(cid:0)
(cid:98)λk((cid:98)γk( (cid:98)Q) − (cid:98)ck) ≥ B max
k∈K

(cid:98)γk( (cid:98)Q) − (cid:98)ck

+ − ν ,

(cid:1)

where we abbreviate x+ = max{x, 0} for any real number x.

Proof. We show that the lemma follows from the optimality conditions (19). We consider a dual variable λ deﬁned as

λ =

(cid:40)

0
Bek(cid:63)

if (cid:98)γ( (cid:98)Q) ≤ (cid:98)c,
otherwise, where k(cid:63) = arg maxk[(cid:98)γk( (cid:98)Q) − (cid:98)ck],

where ek denotes the kth vector of the standard basis. Then we have by equations (19) and (20) that

(cid:99)err( (cid:98)Q) +

(cid:98)λk((cid:98)γk( (cid:98)Q) − (cid:98)ck) = L( (cid:98)Q, (cid:98)λ)

(cid:88)

k∈K

≥ L( (cid:98)Q, λ) − ν = (cid:99)err( (cid:98)Q) +

λk((cid:98)γk( (cid:98)Q) − (cid:98)ck) − ν ,

(cid:88)

k∈K

and the lemma follows by our choice of λ.

Next two lemmas bound the empirical error of (cid:98)Q and also bound the amount by which (cid:98)Q violates the empirical fairness
constraints.
Lemma 2 (Empirical error bound). The distribution (cid:98)Q satisﬁes (cid:99)err( (cid:98)Q) ≤ (cid:99)err(Q) + 2ν for any Q satisfying the empirical
fairness constraints, i.e., any Q such that (cid:98)γ(Q) ≤ (cid:98)c.

A Reductions Approach to Fair Classiﬁcation

Proof. Assume that Q satisﬁes (cid:98)γ(Q) ≤ (cid:98)c. Since (cid:98)λ ≥ 0, we have

L(Q, (cid:98)λ) = (cid:99)err(Q) + (cid:98)λ

(cid:62)(cid:0)

(cid:98)γ(Q) − (cid:98)c(cid:1) ≤ (cid:99)err(Q) .

The optimality conditions (19) imply that

Putting these together, we obtain

L( (cid:98)Q, (cid:98)λ) ≤ L(Q, (cid:98)λ) + ν .

L( (cid:98)Q, (cid:98)λ) ≤ (cid:99)err(Q) + ν .

We next invoke Lemma 1 to lower bound L( (cid:98)Q, (cid:98)λ) as

L( (cid:98)Q, (cid:98)λ) = (cid:99)err( (cid:98)Q) +

(cid:98)λk((cid:98)γk( (cid:98)Q) − (cid:98)ck) ≥ (cid:99)err( (cid:98)Q) + B max
k∈K

(cid:98)γk( (cid:98)Q) − (cid:98)ck

+ − ν

(cid:0)

(cid:1)

(cid:88)

k∈K

≥ (cid:99)err( (cid:98)Q) − ν .

Combining the upper and lower bounds on L( (cid:98)Q, (cid:98)λ) completes the proof.

Lemma 3 (Empirical fairness violation). Assume that the empirical fairness constraints (cid:98)γ(Q) ≤ (cid:98)c are feasible. Then the
distribution (cid:98)Q approximately satisﬁes all empirical fairness constraints:

(cid:16)

max
k∈K

(cid:98)γk( (cid:98)Q) − (cid:98)ck

(cid:17)

≤

1 + 2ν
B

.

Proof. Let Q satisfy (cid:98)γ(Q) ≤ (cid:98)c. Applying the same upper and lower bound on L( (cid:98)Q, (cid:98)λ) as in the proof of Lemma 2, we
obtain

(cid:99)err( (cid:98)Q) + B max
k∈K

(cid:0)

(cid:98)γk( (cid:98)Q) − (cid:98)ck

+ − ν ≤ L( (cid:98)Q, (cid:98)λ) ≤ (cid:99)err(Q) + ν .

(cid:1)

We can further upper bound (cid:99)err(Q) − (cid:99)err( (cid:98)Q) by 1 and use x ≤ x+ for any real number x to complete the proof.

It remains to lift the bounds on empirical classiﬁcation error and constraint violation into the corresponding bounds on true
classiﬁcation error and the violation of true constraints. We will use the standard machinery of uniform convergence bounds
via the (worst-case) Rademacher complexity.
Let F be a class of functions f : Z → [0, 1] over some space Z. Then the (worst-case) Rademacher complexity of F is
deﬁned as

Rn(F) := sup

(cid:34)

E

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

sup
f ∈F

n
(cid:88)

σif (zi)

,

(cid:12)
(cid:35)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

z1,...,zn∈Z
where the expectation is over the i.i.d. random variables σ1, . . . , σn with P[σi = 1] = P[σi = −1] = 1/2.
We ﬁrst prove concentration of generic moments derived from classiﬁers h ∈ H and then move to bounding the deviations
from true classiﬁcation error and true fairness constraints.
Lemma 4 (Concentration of moments). Let g : X × A × {0, 1} × {0, 1} → [0, 1] be any function and let D be a distribution
over (X, A, Y ). Then with probability at least 1 − δ, for all h ∈ H,

i=1

(cid:12)
(cid:12)

(cid:12)(cid:98)E(cid:2)g(X, A, Y, h(X))(cid:3) − E(cid:2)g(X, A, Y, h(X))(cid:3)(cid:12)

(cid:12) ≤ 2Rn(H) +
(cid:12)

(cid:114)

2
√
n

+

ln(2/δ)
2n

,

where the expectation is with respect to D and the empirical expectation is based on n i.i.d. draws from D.

Proof. Let F := {fh}h∈H be the class of functions fh : (x, y, a) (cid:55)→ g(cid:0)x, y, a, h(x)(cid:1). By Theorem 3.2 of Boucheron et al.
(2005), we then have with probability at least 1 − δ, for all h,

(cid:12)
(cid:12)

(cid:12)(cid:98)E(cid:2)g(X, A, Y, h(X))(cid:3) − E(cid:2)g(X, A, Y, h(X))(cid:3)(cid:12)
(cid:12)
(cid:12) =

(cid:12)
(cid:12)
(cid:12) ≤ 2Rn(F) +
(cid:12)(cid:98)E[fh] − E[fh]
(cid:12)
(cid:12)

(cid:114)

ln(2/δ)
2n

.

(21)

We will next bound Rn(F) in terms of Rn(H). Since h(x) ∈ {0, 1}, we can write

A Reductions Approach to Fair Classiﬁcation

fh(x, y, a) = h(x)g(x, a, y, 1) +

1 − h(x)

g(x, a, y, 0) = g(x, a, y, 0) + h(x)

g(x, a, y, 1) − g(x, a, y, 0)

.

(cid:16)

(cid:17)

(cid:16)

(cid:17)

(cid:12)g(x, a, y, 0)(cid:12)

Since (cid:12)
(cid:12) ≤ 1, we can invoke Theorem 12(5) of Bartlett & Mendelson
(2002) for bounding function classes shifted by an offset, in our case g(x, a, y, 0), and Theorem 4.4 of Ledoux & Talagrand
(1991) for bounding function classes under contraction, in our case g(x, a, y, 1) − g(x, a, y, 0), yielding

(cid:12)g(x, a, y, 1) − g(x, a, y, 0)(cid:12)

(cid:12) ≤ 1 and (cid:12)

Rn(F) ≤

+ Rn(H) .

1
√
n

Together with the bound (21), this proves the lemma.

Lemma 5 (Concentration of loss). With probability at least 1 − δ, for all Q ∈ ∆,

|(cid:99)err(Q) − err(Q)| ≤ 2Rn(H) +

(cid:114)

2
√
n

+

ln(2/δ)
2n

.

Proof. We ﬁrst use Lemma 4 with g : (x, a, y, ˆy) (cid:55)→ 1{y (cid:54)= ˆy} to obtain, with probability 1 − δ, for all h,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) =
(cid:12)(cid:99)err(h) − err(h)

(cid:12)
(cid:12)(cid:98)E[fh] − E[fh]
(cid:12)

(cid:12)
(cid:12) ≤ 2Rn(H) +
(cid:12)

(cid:114)

2
√
n

+

ln(2/δ)
2n

.

The lemma now follows for any Q by taking a convex combination of the corresponding bounds on h ∈ H.7

Finally, we show a result for the concentration of the empirical constraint violations to their population counterparts. We
will actually show the concentration of the individual moments (cid:98)µj(Q) to µj(Q) uniformly for all Q ∈ ∆. Since M is
a ﬁxed matrix not dependent on the data, this also directly implies concentration of the constraints (cid:98)γ(Q) = M(cid:98)µ(Q) to
γ(Q) = Mµ(Q). For this result, recall that nj = |{i ∈ [n] : (Xi,Ai,Yi) ∈ Ej}| and p(cid:63)
Lemma 6 (Concentration of conditional moments). For any j ∈ J, with probability at least 1 − δ, for all Q,

j = P[Ej].

(cid:12)(cid:98)µj(Q) − µj(Q)(cid:12)
(cid:12)

(cid:12) ≤ 2Rnj (H) +

(cid:115)

2
√
nj

+

ln(2/δ)
2nj

.

If np(cid:63)

j ≥ 8 log(2/δ), then with probability at least 1 − δ, for all Q,

(cid:12)(cid:98)µj(Q) − µj(Q)(cid:12)
(cid:12)

(cid:12) ≤ 2Rnp(cid:63)

j /2(H) + 2

(cid:115)

(cid:115)

2
np(cid:63)
j

+

ln(4/δ)
np(cid:63)
j

.

Proof. Our proof largely follows the proof of Lemma 2 of Woodworth et al. (2017), with appropriate modiﬁcations for our
more general constraint deﬁnition. Let Sj := {i ∈ [n] : (Xi,Ai,Yi) ∈ Ej} be the set of indices such that the corresponding
examples fall in the event Ej. Note that we have deﬁned nj = |Sj|. Let D(·) denote the joint distribution of (X, A, Y ).
Then, conditioned on i ∈ Sj, the random variables gj(Xi,Ai,Yi,h(Xi)) are i.i.d. draws from the distribution D(· | Ej), with
mean µj(h). Applying Lemma 4 with gj and the distribution D(· | Ej) therefore yields, with probability 1 − δ, for all h,

(cid:12)(cid:98)µj(h) − µj(h)(cid:12)
(cid:12)

(cid:12) ≤ 2Rnj (H) +

(cid:115)

2
√
nj

+

ln(2/δ)
2nj

,

The lemma now follows by taking a convex combination over h.

7The same reasoning applies for general error, err(h) = E[gerr(X,A,Y,h(X))], by using g = gerr in Lemma 4.

A Reductions Approach to Fair Classiﬁcation

Proof of Theorem 4. We now use the lemmas derives so far to prove Theorem 4. We ﬁrst use Lemma 6 to bound the gap
between the empirical and population fairness constraints. The lemma implies that with probability at least 1 − |J|δ, for all
k ∈ K and all Q ∈ ∆,

(cid:12)(cid:98)γk(Q) − γk(Q)(cid:12)
(cid:12)

(cid:12) =

(cid:16)
(cid:98)µ(Q) − µ(Q)

(cid:17)(cid:12)
(cid:12)
(cid:12)

≤

(cid:12)
(cid:12)
(cid:12)(cid:98)µj(Q) − µj(Q)
|Mk,j|

(cid:12)
(cid:12)
(cid:12)

(cid:32)

≤

|Mk,j|

2Rnj (H) +

(cid:115)

(cid:33)

2
√
nj

+

ln(2/δ)
2nj

(cid:12)
(cid:12)
(cid:12)Mk
(cid:88)

j∈J

(cid:88)

j∈J
≤ εk .

Note that our choice of (cid:98)c along with equation (22) ensure that (cid:98)γk(Q(cid:63)) ≤ (cid:98)ck for all k ∈ K. Using Lemma 2 allows us to
conclude that

(cid:99)err( (cid:98)Q) ≤ (cid:99)err(Q(cid:63)) + 2ν .

We now invoke Lemma 5 twice, once for (cid:99)err( (cid:98)Q) and once for (cid:99)err(Q(cid:63)), proving the ﬁrst statement of the theorem.
The above shows that Q(cid:63) satisﬁes the empirical fairness constraints, so we can use Lemma 3, which together with
equation (22) yields

γk( (cid:98)Q) ≤ (cid:98)γk( (cid:98)Q) + εk ≤ (cid:98)ck +

+ εk = ck +

+ 2εk ,

1 + 2ν
B

1 + 2ν
B

(22)

proving the second statement of the theorem.

We are now ready to prove Theorems 2 and 3

Proof of Theorem 2. The ﬁrst part of the theorem follows immediately from Assumption 1 and Theorem 4 (with δ/2 instead
of δ). The statement in fact holds with probability at least 1 − (|J| + 1)δ/2. For the second part, we use the multiplicative
Chernoff bound for binomial random variables. Note that E[nj] = np(cid:63)
j , and we assume that np(cid:63)
j ≥ 8 ln(2/δ), so the
multiplicative Chernoff bound implies that nj ≤ np(cid:63)
j /2 with probability at most δ/2. Taking the union bound across all j
and combining with the ﬁrst part of the theorem then proves the second part.

Proof of Theorem 3. This follows immediately from Theorem 1 and the ﬁrst part of Theorem 2.

D. Additional Experimental Results

In this appendix we present more complete experimental results. We present experimental results for both the training and
test data. We evaluate the exponentiated-gradient as well as the grid-search variants of our reductions. And, ﬁnally, we
consider extensions of reweighting and relabeling beyond the speciﬁc tradeoffs proposed by Kamiran & Calders (2012).
Speciﬁcally, we introduce a scaling parameter that interpolates between the prescribed tradeoff (speciﬁc importance weights
or the number of examples to relabel) and the unconstrained classiﬁer (uniform weights or zero examples to relabel). The
training data results are shown in Figure 2. The test set results are shown in Figure 3.

A Reductions Approach to Fair Classiﬁcation

Figure 2. Training classiﬁcation error versus constraint violation, with respect to DP (top two rows) and EO (bottom two rows). Markers
correspond to the baselines. For our two reductions and the interpolants between reweighting (or relabeling) and the unconstrained
classiﬁer, we varied their tradeoff parameters and plot the Pareto frontiers of the sets of classiﬁers obtained for each method. Because the
curves of the different methods often overlap, we use vertical dashed lines to indicate the lowest constraint violations. All data sets have
binary protected attributes except for adult4, which has four protected attribute values, so relabeling is not applicable and grid search is
not feasible for this data set. The exponentiated-gradient reduction dominates or matches other approaches as expected since it solves
exactly for the points on the Pareto frontier of the set of all classiﬁers in each considered class.

A Reductions Approach to Fair Classiﬁcation

Figure 3. Test classiﬁcation error versus constraint violation, with respect to DP (top two rows) and EO (bottom two rows). Markers
correspond to the baselines. For our two reductions and the interpolants between reweighting (or relabeling) and the unconstrained
classiﬁer, we show convex envelopes of the classiﬁers taken from the training Pareto frontier of each method (i.e., the same classiﬁers as
shown in Figure 2). Because the curves of the different methods often overlap, we use vertical dashed lines to indicate the lowest constraint
violations. All data sets have binary protected attributes except for adult4, which has four protected attribute values, so relabeling
is not applicable and grid search is not feasible for this data set. We show 95% conﬁdence bands for the classiﬁcation error of the
exponentiated-gradient reduction and 95% conﬁdence intervals for the constraint violation of post-processing. The exponentiated-gradient
reduction dominates or matches performance of all other methods up to statistical uncertainty.

A Reductions Approach to Fair Classiﬁcation

8
1
0
2
 
l
u
J
 
6
1
 
 
]

G
L
.
s
c
[
 
 
3
v
3
5
4
2
0
.
3
0
8
1
:
v
i
X
r
a

Alekh Agarwal 1 Alina Beygelzimer 2 Miroslav Dud´ık 1 John Langford 1 Hanna Wallach 1

Abstract

We present a systematic approach for achieving
fairness in a binary classiﬁcation setting. While
we focus on two well-known quantitative deﬁni-
tions of fairness, our approach encompasses many
other previously studied deﬁnitions as special
cases. The key idea is to reduce fair classiﬁcation
to a sequence of cost-sensitive classiﬁcation
problems, whose solutions yield a randomized
classiﬁer with the lowest (empirical) error subject
to the desired constraints. We introduce two
reductions that work for any representation of the
cost-sensitive classiﬁer and compare favorably
to prior baselines on a variety of data sets, while
overcoming several of their disadvantages.

1. Introduction

Over the past few years, the media have paid considerable
attention to machine learning systems and their ability to
inadvertently discriminate against minorities, historically
disadvantaged populations, and other protected groups when
allocating resources (e.g., loans) or opportunities (e.g., jobs).
In response to this scrutiny—and driven by ongoing debates
and collaborations with lawyers, policy-makers, social sci-
entists, and others (e.g., Barocas & Selbst, 2016)—machine
learning researchers have begun to turn their attention to the
topic of “fairness in machine learning,” and, in particular, to
the design of fair classiﬁcation and regression algorithms.

In this paper we study the task of binary classiﬁcation sub-
ject to fairness constraints with respect to a pre-deﬁned pro-
tected attribute, such as race or sex. Previous work in this
area can be divided into two broad groups of approaches.

The ﬁrst group of approaches incorporate speciﬁc quanti-
tative deﬁnitions of fairness into existing machine learning

1Microsoft Research, New York 2Yahoo! Research, New York.
Correspondence to: A. Agarwal <alekha@microsoft.com>,
Dud´ık
A.
<mdudik@microsoft.com>, J. Langford <jcl@microsoft.com>,
H. Wallach <wallach@microsoft.com>.

Beygelzimer <beygel@gmail.com>,

M.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

methods, often by relaxing the desired deﬁnitions of fair-
ness, and only enforcing weaker constraints, such as lack of
correlation (e.g., Woodworth et al., 2017; Zafar et al., 2017;
Johnson et al., 2016; Kamishima et al., 2011; Donini et al.,
2018). The resulting fairness guarantees typically only hold
under strong distributional assumptions, and the approaches
are tied to speciﬁc families of classiﬁers, such as SVMs.

The second group of approaches eliminate the restriction
to speciﬁc classiﬁer families and treat the underlying clas-
siﬁcation method as a “black box,” while implementing
a wrapper that either works by pre-processing the data or
post-processing the classiﬁer’s predictions (e.g., Kamiran
& Calders, 2012; Feldman et al., 2015; Hardt et al., 2016;
Calmon et al., 2017). Existing pre-processing approaches
are speciﬁc to particular deﬁnitions of fairness and typically
seek to come up with a single transformed data set that will
work across all learning algorithms, which, in practice, leads
to classiﬁers that still exhibit substantial unfairness (see our
evaluation in Section 4). In contrast, post-processing allows
a wider range of fairness deﬁnitions and results in provable
fairness guarantees. However, it is not guaranteed to ﬁnd the
most accurate fair classiﬁer, and requires test-time access to
the protected attribute, which might not be available.

We present a general-purpose approach that has the key
advantage of this second group of approaches—i.e., the
underlying classiﬁcation method is treated as a black
box—but without the noted disadvantages. Our approach
encompasses a wide range of fairness deﬁnitions,
is
guaranteed to yield the most accurate fair classiﬁer, and
does not require test-time access to the protected attribute.
Speciﬁcally, our approach allows any deﬁnition of fairness
that can be formalized via linear inequalities on conditional
such as demographic parity or equalized
moments,
odds (see Section 2.1). We show how binary classiﬁcation
subject to these constraints can be reduced to a sequence
of cost-sensitive classiﬁcation problems. We require only
black-box access to a cost-sensitive classiﬁcation algorithm,
which does not need to have any knowledge of the desired
deﬁnition of fairness or protected attribute. We show that
the solutions to our sequence of cost-sensitive classiﬁcation
problems yield a randomized classiﬁer with the lowest
(empirical) error subject to the desired fairness constraints.

Corbett-Davies et al. (2017) and Menon & Williamson

A Reductions Approach to Fair Classiﬁcation

(2018) begin with a similar goal to ours, but they analyze
the Bayes optimal classiﬁer under fairness constraints in the
limit of inﬁnite data. In contrast, our focus is algorithmic,
our approach applies to any classiﬁer family, and we obtain
ﬁnite-sample guarantees. Dwork et al. (2018) also begin
with a similar goal to ours. Their approach partitions the
training examples into subsets according to protected at-
tribute values and then leverages transfer learning to jointly
learn from these separate data sets. Our approach avoids par-
titioning the data and assumes access only to a classiﬁcation
algorithm rather than a transfer learning algorithm.

A preliminary version of this paper appeared at the FAT/ML
workshop (Agarwal et al., 2017), and led to extensions with
more general optimization objectives (Alabi et al., 2018)
and combinatorial protected attributes (Kearns et al., 2018).

In the next section, we formalize our problem. While we
focus on two well-known quantitative deﬁnitions of fairness,
our approach also encompasses many other previously stud-
ied deﬁnitions of fairness as special cases. In Section 3, we
describe our reductions approach to fair classiﬁcation and
its guarantees in detail. The experimental study in Section 4
shows that our reductions compare favorably to three base-
lines, while overcoming some of their disadvantages and
also offering the ﬂexibility of picking a suitable accuracy–
fairness tradeoff. Our results demonstrate the utility of
having a general-purpose approach for combining machine
learning methods and quantitative fairness deﬁnitions.

2. Problem Formulation

We consider a binary classiﬁcation setting where the training
examples consist of triples (X, A, Y ), where X ∈ X is a fea-
ture vector, A ∈ A is a protected attribute, and Y ∈ {0, 1}
is a label. The feature vector X can either contain the pro-
tected attribute A as one of the features or contain other fea-
tures that are arbitrarily indicative of A. For example, if the
classiﬁcation task is to predict whether or not someone will
default on a loan, each training example might correspond
to a person, where X represents their demographics, income
level, past payment history, and loan amount; A represents
their race; and Y represents whether or not they defaulted on
that loan. Note that X might contain their race as one of the
features or, for example, contain their zipcode—a feature
that is often correlated with race. Our goal is to learn an ac-
curate classiﬁer h : X → {0, 1} from some set (i.e., family)
of classiﬁers H, such as linear threshold rules, decision trees,
or neural nets, while satisfying some deﬁnition of fairness.
Note that the classiﬁers in H do not explicitly depend on A.

2.1. Fairness Deﬁnitions

We focus on two well-known quantitative deﬁnitions of
fairness that have been considered in previous work on

fair classiﬁcation; however, our approach also encompasses
many other previously studied deﬁnitions of fairness as
special cases, as we explain at the end of this section.

The ﬁrst deﬁnition—demographic (or statistical) parity—
can be thought of as a stronger version of the US Equal
Employment Opportunity Commission’s “four-ﬁfths rule,”
which requires that the “selection rate for any race, sex, or
ethnic group [must be at least] four-ﬁfths (4/5) (or eighty
percent) of the rate for the group with the highest rate.”1
Deﬁnition 1 (Demographic parity—DP). A classiﬁer h
satisﬁes demographic parity under a distribution over
(X, A, Y ) if its prediction h(X) is statistically indepen-
dent of the protected attribute A—that is, if P[h(X) = ˆy |
A = a] = P[h(X) = ˆy] for all a, ˆy. Because ˆy ∈ {0, 1},
this is equivalent to E[h(X) | A = a] = E[h(X)] for all a.

The second deﬁnition—equalized odds—was recently pro-
posed by Hardt et al. (2016) to remedy two previously noted
ﬂaws with demographic parity (Dwork et al., 2012). First,
demographic parity permits a classiﬁer which accurately
classiﬁes data points with one value A = a, such as the
value a with the most data, but makes random predictions
for data points with A (cid:54)= a as long as the probabilities of
h(X) = 1 match. Second, demographic parity rules out
perfect classiﬁers whenever Y is correlated with A.
In
contrast, equalized odds suffers from neither of these ﬂaws.

Deﬁnition 2 (Equalized odds—EO). A classiﬁer h satis-
ﬁes equalized odds under a distribution over (X, A, Y )
if its prediction h(X) is conditionally independent of
the protected attribute A given the label Y —that is, if
P[h(X) = ˆy | A = a, Y = y] = P[h(X) = ˆy | Y = y] for
all a, y, and ˆy. Because ˆy ∈ {0, 1}, this is equivalent to
E[h(X) | A = a, Y = y] = E[h(X) | Y = y] for all a, y.

We now show how each deﬁnition can be viewed as a special
case of a general set of linear constraints of the form

Mµ(h) ≤ c,

(1)

where matrix M ∈ R|K|×|J| and vector c ∈ R|K| describe
the linear constraints, each indexed by k ∈ K, and µ(h) ∈
R|J| is a vector of conditional moments of the form

µj(h) = E(cid:2) gj(X, A, Y, h(X)) (cid:12)

(cid:12) Ej

(cid:3)

for j ∈ J,

where gj : X × A × {0, 1} × {0, 1} → [0, 1] and Ej is
an event deﬁned with respect to (X, A, Y ). Crucially, gj
depends on h, while Ej cannot depend on h in any way.
Example 1 (DP). In a binary classiﬁcation setting, demo-
graphic parity can be expressed as a set of |A| equality
constraints, each of the form E[h(X) | A = a] = E[h(X)].
Letting J = A ∪ {(cid:63)}, gj(X, A, Y, h(X)) = h(X) for all j,

1See the Uniform Guidelines on Employment Selection Proce-

dures, 29 C.F.R. §1607.4(D) (2015).

A Reductions Approach to Fair Classiﬁcation

Ea = {A = a}, and E(cid:63) = {True}, where {True} refers to
the event encompassing all points in the sample space, each
equality constraint can be expressed as µa(h) = µ(cid:63)(h).2
Finally, because each such constraint can be equivalently
expressed as a pair of inequality constraints of the form

µa(h) − µ(cid:63)(h) ≤ 0
−µa(h) + µ(cid:63)(h) ≤ 0,

demographic parity can be expressed as equation (1), where
K = A×{+, −}, M(a,+),a(cid:48) = 1{a(cid:48) = a}, M(a,+),(cid:63) = −1,
M(a,−),a(cid:48) = −1{a(cid:48) = a}, M(a,−),(cid:63) = 1, and c = 0.
Expressing each equality constraint as a pair of inequality
constraints allows us to control the extent to which each
constraint is enforced by positing ck > 0 for some (or all) k.
In a binary classiﬁcation set-
Example 2 (EO).
equalized odds can be expressed as a set
ting,
of 2 |A|
form
constraints,
E[h(X) | A = a, Y = y] = E[h(X) | Y = y]. Letting
J = (A ∪ {(cid:63)}) × {0, 1}, gj(X, A, Y, h(X)) = h(X) for
all j, E(a,y) = {A = a, Y = y}, and E((cid:63),y) = {Y = y},
each equality constraint can be equivalently expressed as

each of

equality

the

µ(a,y)(h) − µ((cid:63),y)(h) ≤ 0
−µ(a,y)(h) + µ((cid:63),y)(h) ≤ 0.

can be

equalized odds

As a result,
expressed
as equation (1), where K = A × Y × {+, −},
M(a,y,+),(a(cid:48),y(cid:48)) = 1{a(cid:48)= a, y(cid:48)= y}, M(a,y,+),((cid:63),y(cid:48)) = −1,
M(a,y,−),(a(cid:48),y(cid:48)) = −1{a(cid:48)= a, y(cid:48)= y}, M(a,y,−),((cid:63),y(cid:48)) = 1,
and c = 0. Again, we can posit ck > 0 for some (or all) k
to allow small violations of some (or all) of the constraints.

Although we omit the details, we note that many other pre-
viously studied deﬁnitions of fairness can also be expressed
as equation (1). For example, equality of opportunity (Hardt
et al., 2016) (also known as balance for the positive class;
Kleinberg et al., 2017), balance for the negative class (Klein-
berg et al., 2017), error-rate balance (Chouldechova,
2017), overall accuracy equality (Berk et al., 2017), and
treatment equality (Berk et al., 2017) can all be expressed
as equation (1); in contrast, calibration (Kleinberg et al.,
2017) and predictive parity (Chouldechova, 2017) cannot
because to do so would require the event Ej to depend on
h. We note that our approach can also be used to satisfy
multiple deﬁnitions of fairness, though if these deﬁnitions
are mutually contradictory, e.g., as described by Kleinberg
et al. (2017), then our guarantees become vacuous.

2.2. Fair Classiﬁcation

In a standard (binary) classiﬁcation setting, the goal is to
learn the classiﬁer h ∈ H with the minimum classiﬁcation

2Note that µ(cid:63)(h) = E[h(X) | True] = E[h(X)].

error: err(h) := P[h(X) (cid:54)= Y ]. However, because our
goal is to learn the most accurate classiﬁer while satisfying
fairness constraints, as formalized above, we instead seek to
ﬁnd the solution to the constrained optimization problem3

min
h∈H

err(h)

subject to Mµ(h) ≤ c.

(2)

Furthermore, rather than just considering classiﬁers in the
set H, we can enlarge the space of possible classiﬁers by
considering randomized classiﬁers that can be obtained via
a distribution over H. By considering randomized classi-
ﬁers, we can achieve better accuracy–fairness tradeoffs than
would otherwise be possible. A randomized classiﬁer Q
makes a prediction by ﬁrst sampling a classiﬁer h ∈ H
from Q and then using h to make the prediction. The result-
ing classiﬁcation error is err(Q) = (cid:80)
h∈H Q(h) err(h) and
the conditional moments are µ(Q) = (cid:80)
h∈H Q(h)µ(h)
(see Appendix A for the derivation). Thus we seek to solve

min
Q∈∆

err(Q)

subject to Mµ(Q) ≤ c,

(3)

where ∆ is the set of all distributions over H.

In practice, we do not know the true distribution over
(X, A, Y ) and only have access to a data set of training ex-
amples {(Xi, Ai, Yi)}n
i=1. We therefore replace err(Q) and
µ(Q) in equation (3) with their empirical versions (cid:99)err(Q)
and (cid:98)µ(Q). Because of the sampling error in (cid:98)µ(Q), we
also allow errors in satisfying the constraints by setting
(cid:98)ck = ck + εk for all k, where εk ≥ 0. After these modiﬁca-
tions, we need to solve the empirical version of equation (3):

Q∈∆ (cid:99)err(Q)
min

subject to M(cid:98)µ(Q) ≤ (cid:98)c.

(4)

3. Reductions Approach

We now show how the problem (4) can be reduced to a se-
quence of cost-sensitive classiﬁcation problems. We further
show that the solutions to our sequence of cost-sensitive clas-
siﬁcation problems yield a randomized classiﬁer with the
lowest (empirical) error subject to the desired constraints.

3.1. Cost-sensitive Classiﬁcation

We assume access to a cost-sensitive classiﬁcation algorithm
for the set H. The input to such an algorithm is a data set
of training examples {(Xi, C 0
i and C 1
i
denote the losses—costs in this setting—for predicting the
labels 0 or 1, respectively, for Xi. The algorithm outputs

i=1, where C 0

i , C 1

i )}n

arg min
h∈H

n
(cid:88)

i=1

h(Xi) C 1

i + (1 − h(Xi)) C 0
i .

(5)

3We consider misclassiﬁcation error for concreteness, but all
the results in this paper apply to any error of the form err(h) =
E[gerr(X, A, Y, h(X))], where gerr(·, ·, ·, ·) ∈ [0, 1].

A Reductions Approach to Fair Classiﬁcation

min
Q∈∆

max
λ∈R|K|

+

L(Q, λ).

(6)

L( (cid:98)Q, (cid:98)λ) ≤ L(Q, (cid:98)λ) + ν

This abstraction allows us to specify different costs for dif-
ferent training examples, which is essential for incorporat-
ing fairness constraints. Moreover, efﬁcient cost-sensitive
classiﬁcation algorithms are readily available for several
common classiﬁer representations (e.g., Beygelzimer et al.,
2005; Langford & Beygelzimer, 2005; Fan et al., 1999). In
particular, equation (5) is equivalent to a weighted classi-
ﬁcation problem, where the input consists of labeled ex-
amples {(Xi, Yi, Wi)}n
i=1 with Yi ∈ {0, 1} and Wi ≥ 0,
and the goal is to minimize the weighted classiﬁcation er-
ror (cid:80)n
i=1 Wi 1{h(Xi) (cid:54)= Yi}. This is equivalent to equa-
tion (5) if we set Wi = |C 0
i }.

i | and Yi = 1{C 0

i ≥ C 1

i − C 1

3.2. Reduction

To derive our fair classiﬁcation algorithm, we rewrite equa-
tion (4) as a saddle point problem. We begin by introducing
a Lagrange multiplier λk ≥ 0 for each of the |K| constraints,
summarized as λ ∈ R|K|

+ , and form the Lagrangian
L(Q, λ) = (cid:99)err(Q) + λ(cid:62)(cid:0)M(cid:98)µ(Q) − (cid:98)c(cid:1).

Thus, equation (4) is equivalent to

For computational and statistical reasons, we impose an
additional constraint on the (cid:96)1 norm of λ and seek to simul-
taneously ﬁnd the solution to the constrained version of (6)
as well as its dual, obtained by switching min and max:

min
Q∈∆

max

L(Q, λ),

λ∈R|K|

+ , (cid:107)λ(cid:107)1≤B

max
+ , (cid:107)λ(cid:107)1≤B

λ∈R|K|

min
Q∈∆

L(Q, λ).

(P)

(D)

Because L is linear in Q and λ and the domains of Q and
λ are convex and compact, both problems have solutions
(which we denote by Q† and λ†) and the minimum value of
(P) and the maximum value of (D) are equal and coincide
with L(Q†, λ†). Thus, (Q†, λ†) is the saddle point of L
(Corollary 37.6.2 and Lemma 36.2 of Rockafellar, 1970).

We ﬁnd the saddle point by using the standard scheme of
Freund & Schapire (1996), developed for the equivalent
problem of solving for an equilibrium in a zero-sum game.
From game-theoretic perspective, the saddle point can be
viewed as an equilibrium of a game between two players:
the Q-player choosing Q and the λ-player choosing λ. The
Lagrangian L(Q, λ) speciﬁes how much the Q-player has to
pay to the λ-player after they make their choices. At the sad-
dle point, neither player wants to deviate from their choice.

Our algorithm ﬁnds an approximate equilibrium in which
neither player can gain more than ν by changing their choice

Algorithm 1 Exp. gradient reduction for fair classiﬁcation

Input: training examples {(Xi, Yi, Ai)}n

i=1

fairness constraints speciﬁed by gj, Ej, M, (cid:98)c
bound B, accuracy ν, learning rate η

Set θ1 = 0 ∈ R|K|
for t = 1, 2, . . . do
Set λt,k = B
1+(cid:80)
ht ← BESTh(λt)
(cid:98)Qt ← 1
t
(cid:98)λt ← 1
t

(cid:80)t

(cid:80)t

νt ← max
if νt ≤ ν then

Return ( (cid:98)Qt, (cid:98)λt)

exp{θk}

k(cid:48) ∈K exp{θk(cid:48) } for all k ∈ K
(cid:16)

(cid:17)

t(cid:48)=1 ht(cid:48), L ← L

t(cid:48)=1 λt(cid:48), L ← L
(cid:110)

(cid:16)

(cid:98)Qt, BESTλ( (cid:98)Qt)
(cid:17)

BESTh((cid:98)λt), (cid:98)λt
(cid:111)

L( (cid:98)Qt, (cid:98)λt) − L, L − L( (cid:98)Qt, (cid:98)λt)

end if
Set θt+1 = θt + η (M(cid:98)µ(ht) − (cid:98)c)

end for

(where ν > 0 is an input to the algorithm). Such an approx-
imate equilibrium corresponds to a ν-approximate saddle
point of the Lagrangian, which is a pair ( (cid:98)Q, (cid:98)λ), where

for all Q ∈ ∆,
for all λ ∈ R|K|

L( (cid:98)Q, (cid:98)λ) ≥ L( (cid:98)Q, λ) − ν

+ , (cid:107)λ(cid:107)1 ≤ B.

We proceed iteratively by running a no-regret algorithm for
the λ-player, while executing the best response of the Q-
player. Following Freund & Schapire (1996), the average
play of both players converges to the saddle point. We run
the exponentiated gradient algorithm (Kivinen & Warmuth,
1997) for the λ-player and terminate as soon as the subop-
timality of the average play falls below the pre-speciﬁed
accuracy ν. The best response of the Q-player can always
be chosen to put all of the mass on one of the candidate
classiﬁers h ∈ H, and can be implemented by a single call
to a cost-sensitive classiﬁcation algorithm for the set H.

Algorithm 1 fully implements this scheme, except for the
functions BESTλ and BESTh, which correspond to the best-
response algorithms of the two players. (We need the best
response of the λ-player to evaluate whether the subopti-
mality of the current average play has fallen below ν.) The
two best response functions can be calculated as follows.

BESTλ(Q): the best response of the λ-player. The best
response of the λ-player for a given Q is any maximizer of
L(Q, λ) over all valid λs. In our setting, it can always be
chosen to be either 0 or put all of the mass on the most vio-
lated constraint. Letting (cid:98)γ(Q) := M(cid:98)µ(Q) and letting ek de-
note the kth vector of the standard basis, BESTλ(Q) returns
(cid:40)

0
Bek∗

if (cid:98)γ(Q) ≤ (cid:98)c,
otherwise, where k∗ = arg maxk[(cid:98)γk(Q) − (cid:98)ck].

A Reductions Approach to Fair Classiﬁcation

BESTh(λ): the best response of the Q-player. Here, the
best response minimizes L(Q, λ) over all Qs in the simplex.
Because L is linear in Q, the minimizer can always be cho-
sen to put all of the mass on a single classiﬁer h. We show
how to obtain the classiﬁer constituting the best response
via a reduction to cost-sensitive classiﬁcation. Letting pj :=
(cid:98)P[Ej] be the empirical event probabilities, the Lagrangian
for Q which puts all of the mass on a single h is then
L(h, λ) = (cid:99)err(h) + λ(cid:62)(cid:0)M(cid:98)µ(h) − (cid:98)c(cid:1)
(cid:88)
= (cid:98)E(cid:2)1{h(X) (cid:54)= Y }(cid:3) − λ(cid:62)

Mk,jλk (cid:98)µj(h)

(cid:98)c +

k,j

= −λ(cid:62)

(cid:88)

+

k,j

(cid:98)c + (cid:98)E(cid:2)1{h(X) (cid:54)= Y }(cid:3)
(cid:104)
Mk,jλk
(cid:98)E
pj

gj

(cid:0)X,A,Y,h(X)(cid:1) 1{(X,A,Y ) ∈ Ej}

(cid:105)
.

Assuming a data set of training examples {(Xi, Ai, Yi)}n
i=1,
the minimization of L(h, λ) over h then corresponds to cost-
sensitive classiﬁcation on {(Xi, C 0

i=1 with costs4

i , C 1

i )}n

Mk,jλk
pj

C 0

i = 1{Yi (cid:54)= 0}
(cid:88)

+

C 1

k,j
i = 1{Yi (cid:54)= 1}
(cid:88)

Mk,jλk
pj

+

k,j

gj(Xi,Ai,Yi, 0) 1{(Xi,Ai,Yi) ∈ Ej}

gj(Xi,Ai,Yi, 1) 1{(Xi,Ai,Yi) ∈ Ej}.

needing more iterations to reach any given suboptimality. In
particular, as we derive in the theorem, achieving subopti-
mality ν may need up to 4ρ2B2 log(|K| + 1) / ν2 iterations.
Example 3 (DP). Using the matrix M for demographic
parity as described in Section 2, the cost-sensitive reduction
for a vector of Lagrange multipliers λ uses costs

C 0

i = 1{Yi (cid:54)= 0}, C 1

i = 1{Yi (cid:54)= 1} +

λAi
pAi

(cid:88)

−

λa,

a∈A

where pa := (cid:98)P[A = a] and λa := λ(a,+) − λ(a,−), effec-
tively replacing two non-negative Lagrange multipliers by a
single multiplier, which can be either positive or negative.
Because ck = 0 for all k, (cid:98)ck = εk. Furthermore, because
all empirical moments are bounded in [0, 1], we can assume
εk ≤ 1, which yields the bound ρ ≤ 2. Thus, Algorithm 1
terminates in at most 16B2 log(2 |A| + 1) / ν2 iterations.
Example 4 (EO). For equalized odds, the cost-sensitive
reduction for a vector of Lagrange multipliers λ uses costs

C 0

i = 1{Yi (cid:54)= 0},

C 1

i = 1{Yi (cid:54)= 1} +

λ(Ai,Yi)
p(Ai,Yi)

(cid:88)

−

a∈A

λ(a,Yi)
p((cid:63),Yi)

,

where p(a,y) := (cid:98)P[A = a, Y = y], p((cid:63),y) := (cid:98)P[Y = y], and
λ(a,y) := λ(a,y,+) − λ(a,y,−). If we again assume εk ≤ 1,
then we obtain the bound ρ ≤ 2. Thus, Algorithm 1 termi-
nates in at most 16B2 log(4 |A| + 1) / ν2 iterations.

Theorem 1. Letting ρ := maxh(cid:107)M(cid:98)µ(h) − (cid:98)c(cid:107)∞, Algo-
rithm 1 satisﬁes the inequality

3.3. Error Analysis

νt ≤

B log(|K| + 1)
ηt

+ ηρ2B.

Thus, for η = ν
saddle point of L in at most 4ρ2B2 log(|K|+1)

2ρ2B , Algorithm 1 will return a ν-approximate

iterations.

ν2

√

This theorem, proved in Appendix B, bounds the subopti-
mality νt of the average play ( (cid:98)Qt, (cid:98)λt), which is equal to its
suboptimality as a saddle point. The right-hand side of the
bound is optimized by η = (cid:112)log(|K| + 1) / (ρ
t), lead-
ing to the bound νt ≤ 2ρB(cid:112)log(|K| + 1) / t. This bound
decreases with the number of iterations t and grows very
slowly with the number of constraints |K|. The quantity ρ
is a problem-speciﬁc constant that bounds how much any
single classiﬁer h ∈ H can violate the desired set of fair-
ness constraints. Finally, B is the bound on the (cid:96)1-norm of
λ, which we introduced to enable this speciﬁc algorithmic
scheme. In general, larger values of B will bring the prob-
lem (P) closer to (6), and thus also to (4), but at the cost of

4For general error, err(h) = E[gerr(X, A, Y, h(X))], the costs
i contain, respectively, the terms gerr(Xi, Ai, Yi, 0) and

C 0
gerr(Xi, Ai, Yi, 1) instead of 1{Yi (cid:54)= 0} and 1{Yi (cid:54)= 1}.

i and C 1

Our ultimate goal, as formalized in equation (3), is to
minimize the classiﬁcation error while satisfying fairness
constraints under a true but unknown distribution over
(X, A, Y ). In the process of deriving Algorithm 1, we in-
troduced three different sources of error. First, we replaced
the true classiﬁcation error and true moments with their
empirical versions. Second, we introduced a bound B on
the magnitude of λ. Finally, we only run the optimization
algorithm for a ﬁxed number of iterations, until it reaches
suboptimality level ν. The ﬁrst source of error, due to the
use of empirical rather than true quantities, is unavoidable
and constitutes the underlying statistical error. The other two
sources of error, the bound B and the suboptimality level ν,
stem from the optimization algorithm and can be driven
arbitrarily small at the cost of additional iterations. In this
section, we show how the statistical error and the optimiza-
tion error affect the true accuracy and the fairness of the ran-
domized classiﬁer returned by Algorithm 1—in other words,
how well Algorithm 1 solves our original problem (3).

To bound the statistical error, we use the Rademacher
complexity of the classiﬁer family H, which we denote
by Rn(H), where n is the number of training examples.
We assume that Rn(H) ≤ Cn−α for some C ≥ 0 and

A Reductions Approach to Fair Classiﬁcation

α ≤ 1/2. We note that α = 1/2 in the vast majority
including norm-bounded linear
of classiﬁer families,
functions (see Theorem 1 of Kakade et al., 2009), neural
networks (see Theorem 18 of Bartlett & Mendelson, 2002),
and classiﬁer families with bounded VC dimension (see
Lemma 4 and Theorem 6 of Bartlett & Mendelson, 2002).

Recall that in our empirical optimization problem we as-
sume that (cid:98)ck = ck + εk, where εk ≥ 0 are error bounds that
account for the discrepancy between µ(Q) and (cid:98)µ(Q). In
our analysis, we assume that these error bounds have been
set in accordance with the Rademacher complexity of H.
Assumption 1. There exists C, C (cid:48) ≥ 0 and α ≤ 1/2
such that Rn(H) ≤ Cn−α and εk = C (cid:48) (cid:80)
j∈J|Mk,j|n−α
,
where nj is the number of data points that fall in Ej,

j

nj := (cid:12)
(cid:12)

(cid:8)i : (Xi, Ai, Yi) ∈ Ej

(cid:9)(cid:12)
(cid:12).

The optimization error can be bounded via a careful analy-
sis of the Lagrangian and the optimality conditions of (P)
and (D). Combining the three different sources of error
yields the following bound, which we prove in Appendix C.
Theorem 2. Let Assumption 1 hold for C (cid:48) ≥ 2C +
2 + (cid:112)ln(4/δ) / 2, where δ > 0. Let ( (cid:98)Q, (cid:98)λ) be any ν-
approximate saddle point of L, let Q(cid:63) minimize err(Q) sub-
j = P[Ej]. Then, with proba-
ject to Mµ(Q) ≤ c, and let p(cid:63)
bility at least 1 − (|J| + 1)δ, the distribution (cid:98)Q satisﬁes

err( (cid:98)Q) ≤ err(Q(cid:63)) + 2ν + (cid:101)O(n−α),
1+2ν
B

γk( (cid:98)Q) ≤ ck +

(cid:88)

+

j∈J

|Mk,j| (cid:101)O(n−α
j )

for all k,

where (cid:101)O(·) suppresses polynomial dependence on ln(1/δ).
If np(cid:63)

j ≥ 8 log(2/δ) for all j, then, for all k,

γk( (cid:98)Q) ≤ ck +

1+2ν
B

(cid:88)

+

j∈J

|Mk,j| (cid:101)O

(cid:16)

(np(cid:63)

j )−α(cid:17)

.

the solution returned by Algorithm 1
In other words,
achieves the lowest feasible classiﬁcation error on the true
distribution up to the optimization error, which grows lin-
early with ν, and the statistical error, which grows as n−α.
Therefore, if we want to guarantee that the optimization er-
ror does not dominate the statistical error, we should set ν ∝
n−α. The fairness constraints on the true distribution are
satisﬁed up to the optimization error (1 + 2ν) /B and up to
the statistical error. Because the statistical error depends on
the moments, and the error in estimating the moments grows
as n−α
j ≥ n−α, we can set B ∝ nα to guarantee that the op-
timization error does not dominate the statistical error. Com-
bining this reasoning with the learning rate setting of Theo-
rem 1 yields the following theorem (proved in Appendix C).
Theorem 3. Let ρ := maxh(cid:107)M(cid:98)µ(h) − (cid:98)c(cid:107)∞. Let Assump-
tion 1 hold for C (cid:48) ≥ 2C + 2 + (cid:112)ln(4/δ) / 2, where δ > 0.

Let Q(cid:63) minimize err(Q) subject to Mµ(Q) ≤ c. Then
Algorithm 1 with ν ∝ n−α, B ∝ nα and η ∝ ρ−2n−2α ter-
minates in O(ρ2n4α ln |K|) iterations and returns (cid:98)Q, which
with probability at least 1 − (|J| + 1)δ satisﬁes

err( (cid:98)Q) ≤ err(Q(cid:63)) + (cid:101)O(n−α),
γk( (cid:98)Q) ≤ ck +

|Mk,j| (cid:101)O(n−α

(cid:88)

j

)

j∈J

for all k.

Example 5 (DP). If na denotes the number of training ex-
amples with Ai = a, then Assumption 1 states that we
should set ε(a,+) = ε(a,−) = C (cid:48)(n−α
a + n−α) and Theo-
rem 3 then shows that for a suitable setting of C (cid:48), ν, B,
and η, Algorithm 1 will return a randomized classiﬁer (cid:98)Q
with the lowest feasible classiﬁcation error up to (cid:101)O(n−α)
while also approximately satisfying the fairness constraints

(cid:12)
(cid:12)
(cid:12)

(cid:12)
E[h(X) | A = a] − E[h(X)]
(cid:12) ≤ (cid:101)O(n−α
(cid:12)
a )

for all a,

where E is with respect to (X, A, Y ) as well as h ∼ (cid:98)Q.
Example 6 (EO). Similarly, if n(a,y) denotes the number
of examples with Ai = a and Yi = y and n((cid:63),y) denotes the
number of examples with Yi = y, then Assumption 1 states
that we should set ε(a,y,+) = ε(a,y,−) = C (cid:48)(n−α
((cid:63),y))
and Theorem 3 then shows that for a suitable setting of C (cid:48), ν,
B, and η, Algorithm 1 will return a randomized classiﬁer (cid:98)Q
with the lowest feasible classiﬁcation error up to (cid:101)O(n−α)
while also approximately satisfying the fairness constraints

(a,y) + n−α

(cid:12)
(cid:12)
(cid:12)

(cid:12)
E[h(X) | A = a, Y = y] − E[h(X) | Y = y]
(cid:12) ≤ (cid:101)O(n−α
(cid:12)

(a,y))

for all a, y. Again, E includes randomness under the true
distribution over (X, A, Y ) as well as h ∼ (cid:98)Q.

3.4. Grid Search

In some situations, it is preferable to select a deterministic
classiﬁer, even if that means a lower accuracy or a modest vi-
olation of the fairness constraints. A set of candidate classi-
ﬁers can be obtained from the saddle point (Q†, λ†). Specif-
ically, because Q† is a minimizer of L(Q, λ†) and L is
linear in Q, the distribution Q† puts non-zero mass only on
classiﬁers that are the Q-player’s best responses to λ†. If we
knew λ†, we could retrieve one such best response via the re-
duction to cost-sensitive learning introduced in Section 3.2.
We can compute λ† using Algorithm 1, but when the number
of constraints is very small, as is the case for demographic
parity or equalized odds with a binary protected attribute,
it is also reasonable to consider a grid of values λ, calculate
the best response for each value, and then select the value
with the desired tradeoff between accuracy and fairness.
Example 7 (DP). When the protected attribute is binary,
e.g., A ∈ {a, a(cid:48)}, then the grid search can in fact be con-
ducted in a single dimension. The reduction formally takes

A Reductions Approach to Fair Classiﬁcation

two real-valued arguments λa and λa(cid:48), and then adjusts the
costs for predicting h(Xi) = 1 by the amounts

δa =

− λa − λa(cid:48)

and δa(cid:48) =

− λa − λa(cid:48),

λa(cid:48)
pa(cid:48)

λa
pa

respectively, on the training examples with Ai = a and
Ai = a(cid:48). These adjustments satisfy paδa + pa(cid:48)δa(cid:48) = 0,
so instead of searching over λa and λa(cid:48), we can carry out
the grid search over δa alone and apply the adjustment
δa(cid:48) = −paδa/pa(cid:48) to the protected attribute value a(cid:48).
With three attribute values, e.g., A ∈ {a, a(cid:48), a(cid:48)(cid:48)}, we sim-
ilarly have paδa + pa(cid:48)δa(cid:48) + pa(cid:48)(cid:48)δa(cid:48)(cid:48) = 0, so it sufﬁces to
conduct grid search in two dimensions rather than three.
Example 8 (EO). If A ∈ {a, a(cid:48)}, we obtain the adjustment

δ(a,y) =

λ(a,y)
p(a,y)

−

λ(a,y) + λ(a(cid:48),y)
p((cid:63),y)

for an example with protected attribute value a and label y,
and similarly for protected attribute value a(cid:48). In this case,
separately for each y, the adjustments satisfy

p(a,y)δ(a,y) + p(a(cid:48),y)δ(a(cid:48),y) = 0,

so it sufﬁces to do the grid search over δ(a,0) and δ(a,1) and
set the parameters for a(cid:48) to δ(a(cid:48),y) = −p(a,y)δ(a,y)/p(a(cid:48),y).

4. Experimental Results

We now examine how our exponentiated-gradient reduc-
tion5 performs at the task of binary classiﬁcation subject to
either demographic parity or equalized odds. We provide an
evaluation of our grid-search reduction in Appendix D.

We compared our reduction with the score-based post-
processing algorithm of Hardt et al. (2016), which takes as
its input any classiﬁer, (i.e., a standard classiﬁer without any
fairness constraints) and derives a monotone transformation
of the classiﬁer’s output to remove any disparity with respect
to the training examples. This post-processing algorithm
works with both demographic parity and equalized odds, as
well as with binary and non-binary protected attributes.

For demographic parity, we also compared our reduction
with the reweighting and relabeling approaches of Kamiran
& Calders (2012). Reweighting can be applied to both
binary and non-binary protected attributes and operates by
changing importance weights on each example with the
goal of removing any statistical dependence between the
protected attribute and label.6 Relabeling was developed for

5https://github.com/Microsoft/fairlearn
6Although reweighting was developed for demographic parity,
the weights that it induces are achievable by our grid search, albeit
the grid search for equalized odds rather than demographic parity.

binary protected attributes. First, a classiﬁer is trained on
the original data (without considering fairness). The training
examples close to the decision boundary are then relabeled
to remove all disparity while minimally affecting accuracy.
The ﬁnal classiﬁer is then trained on the relabeled data.

As the base classiﬁers for our reductions, we used the
weighted classiﬁcation implementations of logistic regres-
sion and gradient-boosted decision trees in scikit-learn (Pe-
In addition to the three baselines
dregosa et al., 2011).
described above, we also compared our reductions to the
“unconstrained” classiﬁers trained to optimize accuracy only.

We used four data sets, randomly splitting each one into
training examples (75%) and test examples (25%):

• The adult income data set (Lichman, 2013) (48,842
examples). Here the task is to predict whether some-
one makes more than $50k per year, with gender as the
protected attribute. To examine the performance for
non-binary protected attributes, we also conducted an-
other experiment with the same data, using both gender
and race (binarized into white and non-white) as the
protected attribute. Relabeling, which requires binary
protected attributes, was therefore not applicable here.
• ProPublica’s COMPAS recidivism data (7,918 exam-
ples). The task is to predict recidivism from someone’s
criminal history, jail and prison time, demographics,
and COMPAS risk scores, with race as the protected
attribute (restricted to white and black defendants).
• Law School Admissions Council’s National Longitu-
dinal Bar Passage Study (Wightman, 1998) (20,649
examples). Here the task is to predict someone’s even-
tual passage of the bar exam, with race (restricted to
white and black only) as the protected attribute.

• The Dutch census data set (Dutch Central Bureau for
Statistics, 2001) (60,420 examples). Here the task is
to predict whether or not someone has a prestigious
occupation, with gender as the protected attribute.

While all the evaluated algorithms require access to the pro-
tected attribute A at training time, only the post-processing
algorithm requires access to A at test time. For a fair com-
parison, we included A in the feature vector X, so all algo-
rithms had access to it at both the training time and test time.

We used the test examples to measure the classiﬁcation error
for each approach, as well as the violation of the desired fair-
(cid:12)
(cid:12)E[h(X) | A = a] − E[h(X)](cid:12)
ness constraints, i.e., maxa
(cid:12)
(cid:12)
(cid:12)E[h(X) | A = a, Y = y] − E[h(X) | Y = y](cid:12)
and maxa,y
(cid:12)
for demographic parity and equalized odds, respectively.

We ran our reduction across a wide range of tradeoffs be-
tween the classiﬁcation error and fairness constraints. We
considered ε ∈ {0.001, . . . , 0.1} and for each value ran
Algorithm 1 with (cid:98)ck = ε across all k. As expected, the
returned randomized classiﬁers tracked the training Pareto

A Reductions Approach to Fair Classiﬁcation

Figure 1. Test classiﬁcation error versus constraint violation with respect to DP (top two rows) and EO (bottom two rows). All data sets
have binary protected attributes except for adult4, which has four protected attribute values, so relabeling is not applicable there. For our
reduction approach we plot the convex envelope of the classiﬁers obtained on training data at various accuracy–fairness tradeoffs. We
show 95% conﬁdence bands for the classiﬁcation error of our reduction approach and 95% conﬁdence intervals for the constraint violation
of post-processing. Our reduction approach dominates or matches the performance of the other approaches up to statistical uncertainty.

frontier (see Figure 2 in Appendix D). In Figure 1, we evalu-
ate these classiﬁers alongside the baselines on the test data.

For all the data sets, the range of classiﬁcation errors
is much smaller than the range of constraint violations.
Almost all the approaches were able to substantially reduce
or remove disparity without much impact on classiﬁer accu-
racy. One exception was the Dutch census data set, where
the classiﬁcation error increased the most in relative terms.

Our reduction generally dominated or matched the baselines.
The relabeling approach frequently yielded solutions that
were not Pareto optimal. Reweighting yielded solutions
on the Pareto frontier, but often with substantial disparity.
As expected, post-processing yielded disparities that were
statistically indistinguishable from zero, but the resulting
classiﬁcation error was sometimes higher than achieved by
our reduction under a statistically indistinguishable dispar-
ity. In addition, and unlike the post-processing algorithm,
our reduction can achieve any desired accuracy–fairness
tradeoff, allows a wider range of fairness deﬁnitions, and
does not require access to the protected attribute at test time.

Our grid-search reduction, evaluated in Appendix D,
sometimes failed to achieve the lowest disparities on

the training data, but its performance on the test data
very closely matched that of our exponentiated-gradient
reduction. However, if the protected attribute is non-binary,
then grid search is not feasible. For instance, for the version
of the adult income data set where the protected attribute
takes on four values, the grid search would need to span
three dimensions for demographic parity and six dimensions
for equalized odds, both of which are prohibitively costly.

5. Conclusion

We presented two reductions for achieving fairness in a
binary classiﬁcation setting. Our reductions work for any
classiﬁer representation, encompass many deﬁnitions of fair-
ness, satisfy provable guarantees, and work well in practice.

Our reductions optimize the tradeoff between accuracy and
any (single) deﬁnition of fairness given training-time access
to protected attributes. Achieving fairness when training-
time access to protected attributes is unavailable remains an
open problem for future research, as does the navigation of
tradeoffs between accuracy and multiple fairness deﬁnitions.

A Reductions Approach to Fair Classiﬁcation

Acknowledgements

We would like to thank Aaron Roth, Sam Corbett-Davies,
and Emma Pierson for helpful discussions.

References

Agarwal, A., Beygelzimer, A., Dud´ık, M., and Langford, J.
A reductions approach to fair classication. In Fairness,
Accountability, and Transparency in Machine Learning
(FATML), 2017.

Alabi, D., Immorlica, N., and Kalai, A. T. Unleashing linear
optimizers for group-fair learning and optimization. In
Proceedings of the 31st Annual Conference on Learning
Theory (COLT), 2018.

Barocas, S. and Selbst, A. D. Big data’s disparate impact.

California Law Review, 104:671–732, 2016.

Bartlett, P. L. and Mendelson, S. Rademacher and gaussian
complexities: Risk bounds and structural results. Journal
of Machine Learning Research, 3:463–482, 2002.

Berk, R., Heidari, H., Jabbari, S., Kearns, M., and Roth, A.
Fairness in criminal justice risk assessments: The state of
the art. arXiv:1703.09207, 2017.

Beygelzimer, A., Dani, V., Hayes, T. P., Langford, J., and
Zadrozny, B. Error limiting reductions between clas-
siﬁcation tasks. In Proceedings of the Twenty-Second
International Conference on Machine Learning (ICML),
pp. 49–56, 2005.

Boucheron, S., Bousquet, O., and Lugosi, G. Theory of
classiﬁcation: a survey of some recent advances. ESAIM:
Probability and Statistics, 9:323–375, 2005.

Calmon, F., Wei, D., Vinzamuri, B., Ramamurthy, K. N.,
and Varshney, K. R. Optimized pre-processing for dis-
crimination prevention. In Advances in Neural Informa-
tion Processing Systems 30, 2017.

Chouldechova, A. Fair prediction with disparate impact: A
study of bias in recidivism prediction instruments. Big
Data, Special Issue on Social and Technical Trade-Offs,
2017.

Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., and Huq,
A. Algorithmic decision making and the cost of fairness.
In Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining,
pp. 797–806, 2017.

Donini, M., Oneto, L., Ben-David, S., Shawe-Taylor, J., and
Pontil, M. Empirical risk minimization under fairness
constraints. 2018. arXiv:1802.08626.

Dwork, C., Hardt, M., Pitassi, T., Reingold, O., and Zemel,
R. Fairness through awareness. In Proceedings of the 3rd
Innovations in Theoretical Computer Science Conference,
pp. 214–226, 2012.

Dwork, C., Immorlica, N., Kalai, A. T., and Leiserson, M.
Decoupled classiﬁers for group-fair and efﬁcient machine
learning. In Conference on Fairness, Accountability and
Transparency (FAT (cid:63)), pp. 119–133, 2018.

Fan, W., Stolfo, S. J., Zhang, J., and Chan, P. K. Adacost:
Misclassiﬁcation cost-sensitive boosting. In Proceedings
of the Sixteenth International Conference on Machine
Learning (ICML), pp. 97–105, 1999.

Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C.,
and Venkatasubramanian, S. Certifying and removing dis-
parate impact. In Proceedings of the 21st ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining, 2015.

Freund, Y. and Schapire, R. E. Game theory, on-line pre-
diction and boosting. In Proceedings of the Ninth Annual
Conference on Computational Learning Theory (COLT),
pp. 325–332, 1996.

Freund, Y. and Schapire, R. E. A decision-theoretic general-
ization of on-line learning and an application to boosting.
Journal of Computer and System Sciences, 55(1):119–
139, 1997.

Hardt, M., Price, E., and Srebro, N. Equality of opportunity
in supervised learning. In Neural Information Processing
Systems (NIPS), 2016.

Johnson, K. D., Foster, D. P., and Stine, R. A. Impartial pre-
dictive modeling: Ensuring fairness in arbitrary models.
arXiv:1608.00528, 2016.

Kakade, S. M., Sridharan, K., and Tewari, A. On the com-
plexity of linear prediction: Risk bounds, margin bounds,
and regularization. In Advances in neural information
processing systems, pp. 793–800, 2009.

Kamiran, F. and Calders, T. Data preprocessing techniques
for classiﬁcation without discrimination. Knowledge and
Information Systems, 33(1):1–33, 2012.

Kamishima, T., Akaho, S., and Sakuma, J. Fairness-aware
learning through regularization approach. In 2011 IEEE
11th International Conference on Data Mining Work-
shops, pp. 643–650, 2011.

Kearns, M., Neel, S., Roth, A., and Wu, Z. S. Preventing
fairness gerrymandering: Auditing and learning for sub-
group fairness. In Proceedings of the 35th International
Conference on Machine Learning (ICML), 2018.

A Reductions Approach to Fair Classiﬁcation

Kivinen, J. and Warmuth, M. K. Exponentiated gradient
versus gradient descent for linear predictors. Information
and Computation, 132(1):1–63, 1997.

Kleinberg, J., Mullainathan, S., and Raghavan, M. Inherent
trade-offs in the fair determination of risk scores. In Pro-
ceedings of the 8th Innovations in Theoretical Computer
Science Conference, 2017.

Langford, J. and Beygelzimer, A. Sensitive error correct-
In Proceedings of the 18th Annual
ing output codes.
Conference on Learning Theory (COLT), pp. 158–172,
2005.

Ledoux, M. and Talagrand, M. Probability in Banach
Spaces: Isoperimetry and Processes. Springer, 1991.

Lichman, M. UCI machine learning repository, 2013. URL

http://archive.ics.uci.edu/ml.

Menon, A. K. and Williamson, R. C. The cost of fairness in
binary classiﬁcation. In Proceedings of the Conference
on Fiarness, Accountability, and Transparency, 2018.

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-
napeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
Scikit-learn: Machine learning in Python. Journal of
Machine Learning Research, 12:2825–2830, 2011.

Rockafellar, R. T. Convex analysis. Princeton University

Press, 1970.

Shalev-Shwartz, S. Online learning and online convex opti-
mization. Foundations and Trends in Machine Learning,
4(2):107–194, 2012.

Wightman, L. LSAC National Longitudinal Bar Passage

Study, 1998.

Woodworth, B. E., Gunasekar, S., Ohannessian, M. I., and
Srebro, N. Learning non-discriminatory predictors. In
Proceedings of the 30th Conference on Learning Theory
(COLT), pp. 1920–1953, 2017.

Zafar, M. B., Valera, I., Rodriguez, M. G., and Gummadi,
K. P. Fairness constraints: Mechanisms for fair classiﬁca-
tion. In Proceedings of the 20th International Conference
on Artiﬁcial Intelligence and Statistics (AISTATS), pp.
962–970, 2017.

A. Error and Fairness for Randomized Classiﬁers

A Reductions Approach to Fair Classiﬁcation

Let D denote the distribution over triples (X, A, Y ). The accuracy of a classiﬁer h ∈ H is measured by 0-1 error,
err(h) := PD[h(X) (cid:54)= Y ], which for a randomized classiﬁer Q becomes

err(Q) :=

P
(X,A,Y )∼D, h∼Q

[h(X) (cid:54)= Y ] =

Q(h) err(h) .

(cid:88)

h∈H

The fairness constraints on a classiﬁer h are Mµ(h) ≤ c. Recall that µj(h) := ED[gj(X, A, Y, h(X)) | Ej]. For a
randomized classiﬁer Q we deﬁne its moment µj as

µj(Q) :=

E
(X,A,Y )∼D, h∼Q

(cid:104)
gj(X, A, Y, h(X))

(cid:105)

Ej

(cid:12)
(cid:12)
(cid:12)

=

(cid:88)

h∈H

Q(h)µj(h) ,

where the last equality follows because Ej is independent of the choice of h.

B. Proof of Theorem 1

The proof follows immediately from the analysis of Freund & Schapire (1996) applied to the Exponentiated Gradient (EG)
algorithm (Kivinen & Warmuth, 1997), which in our speciﬁc case is also equivalent to Hedge (Freund & Schapire, 1997).
Let Λ := {λ ∈ R|K|
that is equal to λ on coordinates 1 through |K| and puts the remaining mass on the coordinate λ(cid:48)

: (cid:107)λ(cid:48)(cid:107)1 = B}. We associate any λ ∈ Λ with the λ(cid:48) ∈ Λ(cid:48)

+ : (cid:107)λ(cid:48)(cid:107)1 ≤ B} and Λ(cid:48) := {λ(cid:48) ∈ R|K|+1

+

|K|+1.

Consider a run of Algorithm 1. For each λt, let λ(cid:48)
t ∈ R|K|+1 be equal to rt on coordinates 1 through |K| and put zero on the coordinate r(cid:48)
r(cid:48)
associated λ(cid:48), we have, for all t,

t ∈ Λ(cid:48) be the associated element of Λ(cid:48). Let rt := M(cid:98)µ(ht) − (cid:98)c and let
t,|K|+1. Thus, for any λ and the

and, in particular,

λ(cid:62)rt = (λ(cid:48))(cid:62)r(cid:48)

t ,

λ(cid:62)
t

(cid:0)M(cid:98)µ(ht) − (cid:98)c(cid:1) = λ(cid:62)

t rt = (λ(cid:48)

t)(cid:62)r(cid:48)

t .

t as the reward vector for the λ-player. The choices of λ(cid:48)

We interpret r(cid:48)
the learning rate η. By the assumption of the theorem we have (cid:107)r(cid:48)
Corollary 2.14 of Shalev-Shwartz (2012), then states that for any λ(cid:48) ∈ Λ(cid:48),

t then correspond to those of the EG algorithm with
t(cid:107)∞ = (cid:107)rt(cid:107)∞ ≤ ρ. The regret bound for EG, speciﬁcally,

Therefore, by equations (7) and (8), we also have for any λ ∈ Λ,

T
(cid:88)

t=1

(λ(cid:48))(cid:62)r(cid:48)

t ≤

(λ(cid:48)

t)(cid:62)r(cid:48)

t +

B log(|K| + 1)
η

(cid:124)

(cid:123)(cid:122)
=:ζT

+ ηρ2BT

.

(cid:125)

T
(cid:88)

t=1

T
(cid:88)

t=1

λ(cid:62)rt ≤

λ(cid:62)

t rt + ζT .

T
(cid:88)

t=1

(7)

(8)

(9)

This regret bound can be used to bound the suboptimality of L( (cid:98)QT , (cid:98)λT ) in (cid:98)λT as follows:

A Reductions Approach to Fair Classiﬁcation

L( (cid:98)QT , λ) =

T
(cid:88)

(cid:16)

(cid:99)err(ht) + λ(cid:62)(cid:0)M(cid:98)µ(ht) − (cid:98)c(cid:1)(cid:17)

Equation (10) follows from the regret bound (9). Equation (11) follows because L(ht, λt) ≤ L(Q, λt) for all Q by the
choice of ht as the best response of the Q-player. Finally, equation (12) follows by linearity of L(Q, λ) in λ. Thus, we have
for all λ ∈ Λ,

Also, for any Q,

1
T

1
T

1
T

1
T

1
T

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

=

≤

=

≤

(cid:16)
(cid:99)err(ht) + λ(cid:62)rt

(cid:17)

(cid:16)
(cid:99)err(ht) + λ(cid:62)

t rt

(cid:17)

+

ζT
T

L(ht, λt) +

L( (cid:98)QT , λt) +

ζT
T

ζT
T

(cid:16)

= L

(cid:98)QT ,

1
T

T
(cid:88)

t=1

(cid:17)

λt

+

ζT
T

= L( (cid:98)QT , (cid:98)λT ) +

ζT
T

.

L( (cid:98)QT , (cid:98)λT ) ≥ L( (cid:98)QT , λ) −

ζT
T

.

L(Q, (cid:98)λT ) =

L(Q, λt)

1
T

1
T

1
T

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

≥

≥

L(ht, λt)

L(ht, (cid:98)λT ) −

ζT
T

= L( (cid:98)QT , (cid:98)λT ) −

ζT
T

,

L( (cid:98)QT , (cid:98)λT ) ≤ L(Q, (cid:98)λT ) +

ζT
T

.

νT ≤

=

ζT
T

B log(|K| + 1)
ηT

+ ηρ2B ,

(10)

(11)

(12)

(13)

(14)

(15)

(16)

(17)

(18)

where equation (14) follows by linearity of L(Q, λ) in λ, equation (15) follows by the optimality of ht with respect to (cid:98)λt,
equation (16) from the regret bound (9), and equation (17) by linearity of L(Q, λ) in Q. Thus, for all Q,

Equations (13) and (18) immediately imply that for any T ≥ 1,

proving the ﬁrst part of the theorem.

The second part of the theorem follows by plugging in η = ν

2ρ2B and verifying that if T ≥ 4ρ2B2 log(|K|+1)

ν2

then

νT ≤

B log(|K| + 1)
2ρ2B · 4ρ2B2 log(|K|+1)

ν2

ν

+

ν
2ρ2B

· ρ2B =

+

.

ν
2

ν
2

A Reductions Approach to Fair Classiﬁcation

C. Proofs of Theorems 2 and 3

The bulk of this appendix proves the following theorem, which will immediately imply Theorems 2 and 3.

Theorem 4. Let ( (cid:98)Q, (cid:98)λ) be any ν-approximate saddle point of L with

(cid:98)ck = ck + εk

and εk ≥

|Mk,j|

2Rnj (H) +

(cid:32)

(cid:88)

j∈J

(cid:115)

2
√
nj

+

ln(2/δ)
2nj

(cid:33)

.

Let Q(cid:63) minimize err(Q) subject to Mµ(Q) ≤ c. Then with probability at least 1 − (|J| + 1)δ, the distribution (cid:98)Q satisﬁes

err( (cid:98)Q) ≤ err(Q(cid:63)) + 2ν + 4Rn(H) +

(cid:114)

4
√
n

+

2 ln(2/δ)
n

,

and for all k, γk( (cid:98)Q) ≤ ck +

+ 2εk .

1 + 2ν
B

Let Λ := {λ ∈ R|K|
pair ( (cid:98)Q, (cid:98)λ) which is a ν-approximate saddle point of L, i.e.,

+ : (cid:107)λ(cid:48)(cid:107)1 ≤ B} denote the domain of λ. In the remainder of the section, we assume that we are given a

L( (cid:98)Q, (cid:98)λ) ≤ L(Q, (cid:98)λ) + ν

for all Q ∈ ∆,

and L( (cid:98)Q, (cid:98)λ) ≥ L( (cid:98)Q, λ) − ν

for all λ ∈ Λ.

We ﬁrst establish that the pair ( (cid:98)Q, (cid:98)λ) satisﬁes an approximate version of complementary slackness. For the statement and
proof of the following lemma, recall that (cid:98)γ(Q) = M(cid:98)µ(Q), so the empirical fairness constraints can be written as (cid:98)γ(Q) ≤ (cid:98)c
and the Lagrangian L can be written as

(19)

(20)

L(Q, λ) = (cid:99)err(Q) +

λk((cid:98)γk(Q) − (cid:98)ck) .

(cid:88)

k∈K

Lemma 1 (Approximate complementary slackness). The pair ( (cid:98)Q, (cid:98)λ) satisﬁes

(cid:88)

k∈K

(cid:0)
(cid:98)λk((cid:98)γk( (cid:98)Q) − (cid:98)ck) ≥ B max
k∈K

(cid:98)γk( (cid:98)Q) − (cid:98)ck

+ − ν ,

(cid:1)

where we abbreviate x+ = max{x, 0} for any real number x.

Proof. We show that the lemma follows from the optimality conditions (19). We consider a dual variable λ deﬁned as

λ =

(cid:40)

0
Bek(cid:63)

if (cid:98)γ( (cid:98)Q) ≤ (cid:98)c,
otherwise, where k(cid:63) = arg maxk[(cid:98)γk( (cid:98)Q) − (cid:98)ck],

where ek denotes the kth vector of the standard basis. Then we have by equations (19) and (20) that

(cid:99)err( (cid:98)Q) +

(cid:98)λk((cid:98)γk( (cid:98)Q) − (cid:98)ck) = L( (cid:98)Q, (cid:98)λ)

(cid:88)

k∈K

≥ L( (cid:98)Q, λ) − ν = (cid:99)err( (cid:98)Q) +

λk((cid:98)γk( (cid:98)Q) − (cid:98)ck) − ν ,

(cid:88)

k∈K

and the lemma follows by our choice of λ.

Next two lemmas bound the empirical error of (cid:98)Q and also bound the amount by which (cid:98)Q violates the empirical fairness
constraints.
Lemma 2 (Empirical error bound). The distribution (cid:98)Q satisﬁes (cid:99)err( (cid:98)Q) ≤ (cid:99)err(Q) + 2ν for any Q satisfying the empirical
fairness constraints, i.e., any Q such that (cid:98)γ(Q) ≤ (cid:98)c.

A Reductions Approach to Fair Classiﬁcation

Proof. Assume that Q satisﬁes (cid:98)γ(Q) ≤ (cid:98)c. Since (cid:98)λ ≥ 0, we have

L(Q, (cid:98)λ) = (cid:99)err(Q) + (cid:98)λ

(cid:62)(cid:0)

(cid:98)γ(Q) − (cid:98)c(cid:1) ≤ (cid:99)err(Q) .

The optimality conditions (19) imply that

Putting these together, we obtain

L( (cid:98)Q, (cid:98)λ) ≤ L(Q, (cid:98)λ) + ν .

L( (cid:98)Q, (cid:98)λ) ≤ (cid:99)err(Q) + ν .

We next invoke Lemma 1 to lower bound L( (cid:98)Q, (cid:98)λ) as

L( (cid:98)Q, (cid:98)λ) = (cid:99)err( (cid:98)Q) +

(cid:98)λk((cid:98)γk( (cid:98)Q) − (cid:98)ck) ≥ (cid:99)err( (cid:98)Q) + B max
k∈K

(cid:98)γk( (cid:98)Q) − (cid:98)ck

+ − ν

(cid:0)

(cid:1)

(cid:88)

k∈K

≥ (cid:99)err( (cid:98)Q) − ν .

Combining the upper and lower bounds on L( (cid:98)Q, (cid:98)λ) completes the proof.

Lemma 3 (Empirical fairness violation). Assume that the empirical fairness constraints (cid:98)γ(Q) ≤ (cid:98)c are feasible. Then the
distribution (cid:98)Q approximately satisﬁes all empirical fairness constraints:

(cid:16)

max
k∈K

(cid:98)γk( (cid:98)Q) − (cid:98)ck

(cid:17)

≤

1 + 2ν
B

.

Proof. Let Q satisfy (cid:98)γ(Q) ≤ (cid:98)c. Applying the same upper and lower bound on L( (cid:98)Q, (cid:98)λ) as in the proof of Lemma 2, we
obtain

(cid:99)err( (cid:98)Q) + B max
k∈K

(cid:0)

(cid:98)γk( (cid:98)Q) − (cid:98)ck

+ − ν ≤ L( (cid:98)Q, (cid:98)λ) ≤ (cid:99)err(Q) + ν .

(cid:1)

We can further upper bound (cid:99)err(Q) − (cid:99)err( (cid:98)Q) by 1 and use x ≤ x+ for any real number x to complete the proof.

It remains to lift the bounds on empirical classiﬁcation error and constraint violation into the corresponding bounds on true
classiﬁcation error and the violation of true constraints. We will use the standard machinery of uniform convergence bounds
via the (worst-case) Rademacher complexity.
Let F be a class of functions f : Z → [0, 1] over some space Z. Then the (worst-case) Rademacher complexity of F is
deﬁned as

Rn(F) := sup

(cid:34)

E

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

sup
f ∈F

n
(cid:88)

σif (zi)

,

(cid:12)
(cid:35)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

z1,...,zn∈Z
where the expectation is over the i.i.d. random variables σ1, . . . , σn with P[σi = 1] = P[σi = −1] = 1/2.
We ﬁrst prove concentration of generic moments derived from classiﬁers h ∈ H and then move to bounding the deviations
from true classiﬁcation error and true fairness constraints.
Lemma 4 (Concentration of moments). Let g : X × A × {0, 1} × {0, 1} → [0, 1] be any function and let D be a distribution
over (X, A, Y ). Then with probability at least 1 − δ, for all h ∈ H,

i=1

(cid:12)
(cid:12)

(cid:12)(cid:98)E(cid:2)g(X, A, Y, h(X))(cid:3) − E(cid:2)g(X, A, Y, h(X))(cid:3)(cid:12)

(cid:12) ≤ 2Rn(H) +
(cid:12)

(cid:114)

2
√
n

+

ln(2/δ)
2n

,

where the expectation is with respect to D and the empirical expectation is based on n i.i.d. draws from D.

Proof. Let F := {fh}h∈H be the class of functions fh : (x, y, a) (cid:55)→ g(cid:0)x, y, a, h(x)(cid:1). By Theorem 3.2 of Boucheron et al.
(2005), we then have with probability at least 1 − δ, for all h,

(cid:12)
(cid:12)

(cid:12)(cid:98)E(cid:2)g(X, A, Y, h(X))(cid:3) − E(cid:2)g(X, A, Y, h(X))(cid:3)(cid:12)
(cid:12)
(cid:12) =

(cid:12)
(cid:12)
(cid:12) ≤ 2Rn(F) +
(cid:12)(cid:98)E[fh] − E[fh]
(cid:12)
(cid:12)

(cid:114)

ln(2/δ)
2n

.

(21)

We will next bound Rn(F) in terms of Rn(H). Since h(x) ∈ {0, 1}, we can write

A Reductions Approach to Fair Classiﬁcation

fh(x, y, a) = h(x)g(x, a, y, 1) +

1 − h(x)

g(x, a, y, 0) = g(x, a, y, 0) + h(x)

g(x, a, y, 1) − g(x, a, y, 0)

.

(cid:16)

(cid:17)

(cid:16)

(cid:17)

(cid:12)g(x, a, y, 0)(cid:12)

Since (cid:12)
(cid:12) ≤ 1, we can invoke Theorem 12(5) of Bartlett & Mendelson
(2002) for bounding function classes shifted by an offset, in our case g(x, a, y, 0), and Theorem 4.4 of Ledoux & Talagrand
(1991) for bounding function classes under contraction, in our case g(x, a, y, 1) − g(x, a, y, 0), yielding

(cid:12)g(x, a, y, 1) − g(x, a, y, 0)(cid:12)

(cid:12) ≤ 1 and (cid:12)

Rn(F) ≤

+ Rn(H) .

1
√
n

Together with the bound (21), this proves the lemma.

Lemma 5 (Concentration of loss). With probability at least 1 − δ, for all Q ∈ ∆,

|(cid:99)err(Q) − err(Q)| ≤ 2Rn(H) +

(cid:114)

2
√
n

+

ln(2/δ)
2n

.

Proof. We ﬁrst use Lemma 4 with g : (x, a, y, ˆy) (cid:55)→ 1{y (cid:54)= ˆy} to obtain, with probability 1 − δ, for all h,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) =
(cid:12)(cid:99)err(h) − err(h)

(cid:12)
(cid:12)(cid:98)E[fh] − E[fh]
(cid:12)

(cid:12)
(cid:12) ≤ 2Rn(H) +
(cid:12)

(cid:114)

2
√
n

+

ln(2/δ)
2n

.

The lemma now follows for any Q by taking a convex combination of the corresponding bounds on h ∈ H.7

Finally, we show a result for the concentration of the empirical constraint violations to their population counterparts. We
will actually show the concentration of the individual moments (cid:98)µj(Q) to µj(Q) uniformly for all Q ∈ ∆. Since M is
a ﬁxed matrix not dependent on the data, this also directly implies concentration of the constraints (cid:98)γ(Q) = M(cid:98)µ(Q) to
γ(Q) = Mµ(Q). For this result, recall that nj = |{i ∈ [n] : (Xi,Ai,Yi) ∈ Ej}| and p(cid:63)
Lemma 6 (Concentration of conditional moments). For any j ∈ J, with probability at least 1 − δ, for all Q,

j = P[Ej].

(cid:12)(cid:98)µj(Q) − µj(Q)(cid:12)
(cid:12)

(cid:12) ≤ 2Rnj (H) +

(cid:115)

2
√
nj

+

ln(2/δ)
2nj

.

If np(cid:63)

j ≥ 8 log(2/δ), then with probability at least 1 − δ, for all Q,

(cid:12)(cid:98)µj(Q) − µj(Q)(cid:12)
(cid:12)

(cid:12) ≤ 2Rnp(cid:63)

j /2(H) + 2

(cid:115)

(cid:115)

2
np(cid:63)
j

+

ln(4/δ)
np(cid:63)
j

.

Proof. Our proof largely follows the proof of Lemma 2 of Woodworth et al. (2017), with appropriate modiﬁcations for our
more general constraint deﬁnition. Let Sj := {i ∈ [n] : (Xi,Ai,Yi) ∈ Ej} be the set of indices such that the corresponding
examples fall in the event Ej. Note that we have deﬁned nj = |Sj|. Let D(·) denote the joint distribution of (X, A, Y ).
Then, conditioned on i ∈ Sj, the random variables gj(Xi,Ai,Yi,h(Xi)) are i.i.d. draws from the distribution D(· | Ej), with
mean µj(h). Applying Lemma 4 with gj and the distribution D(· | Ej) therefore yields, with probability 1 − δ, for all h,

(cid:12)(cid:98)µj(h) − µj(h)(cid:12)
(cid:12)

(cid:12) ≤ 2Rnj (H) +

(cid:115)

2
√
nj

+

ln(2/δ)
2nj

,

The lemma now follows by taking a convex combination over h.

7The same reasoning applies for general error, err(h) = E[gerr(X,A,Y,h(X))], by using g = gerr in Lemma 4.

A Reductions Approach to Fair Classiﬁcation

Proof of Theorem 4. We now use the lemmas derives so far to prove Theorem 4. We ﬁrst use Lemma 6 to bound the gap
between the empirical and population fairness constraints. The lemma implies that with probability at least 1 − |J|δ, for all
k ∈ K and all Q ∈ ∆,

(cid:12)(cid:98)γk(Q) − γk(Q)(cid:12)
(cid:12)

(cid:12) =

(cid:16)
(cid:98)µ(Q) − µ(Q)

(cid:17)(cid:12)
(cid:12)
(cid:12)

≤

(cid:12)
(cid:12)
(cid:12)(cid:98)µj(Q) − µj(Q)
|Mk,j|

(cid:12)
(cid:12)
(cid:12)

(cid:32)

≤

|Mk,j|

2Rnj (H) +

(cid:115)

(cid:33)

2
√
nj

+

ln(2/δ)
2nj

(cid:12)
(cid:12)
(cid:12)Mk
(cid:88)

j∈J

(cid:88)

j∈J
≤ εk .

Note that our choice of (cid:98)c along with equation (22) ensure that (cid:98)γk(Q(cid:63)) ≤ (cid:98)ck for all k ∈ K. Using Lemma 2 allows us to
conclude that

(cid:99)err( (cid:98)Q) ≤ (cid:99)err(Q(cid:63)) + 2ν .

We now invoke Lemma 5 twice, once for (cid:99)err( (cid:98)Q) and once for (cid:99)err(Q(cid:63)), proving the ﬁrst statement of the theorem.
The above shows that Q(cid:63) satisﬁes the empirical fairness constraints, so we can use Lemma 3, which together with
equation (22) yields

γk( (cid:98)Q) ≤ (cid:98)γk( (cid:98)Q) + εk ≤ (cid:98)ck +

+ εk = ck +

+ 2εk ,

1 + 2ν
B

1 + 2ν
B

(22)

proving the second statement of the theorem.

We are now ready to prove Theorems 2 and 3

Proof of Theorem 2. The ﬁrst part of the theorem follows immediately from Assumption 1 and Theorem 4 (with δ/2 instead
of δ). The statement in fact holds with probability at least 1 − (|J| + 1)δ/2. For the second part, we use the multiplicative
Chernoff bound for binomial random variables. Note that E[nj] = np(cid:63)
j , and we assume that np(cid:63)
j ≥ 8 ln(2/δ), so the
multiplicative Chernoff bound implies that nj ≤ np(cid:63)
j /2 with probability at most δ/2. Taking the union bound across all j
and combining with the ﬁrst part of the theorem then proves the second part.

Proof of Theorem 3. This follows immediately from Theorem 1 and the ﬁrst part of Theorem 2.

D. Additional Experimental Results

In this appendix we present more complete experimental results. We present experimental results for both the training and
test data. We evaluate the exponentiated-gradient as well as the grid-search variants of our reductions. And, ﬁnally, we
consider extensions of reweighting and relabeling beyond the speciﬁc tradeoffs proposed by Kamiran & Calders (2012).
Speciﬁcally, we introduce a scaling parameter that interpolates between the prescribed tradeoff (speciﬁc importance weights
or the number of examples to relabel) and the unconstrained classiﬁer (uniform weights or zero examples to relabel). The
training data results are shown in Figure 2. The test set results are shown in Figure 3.

A Reductions Approach to Fair Classiﬁcation

Figure 2. Training classiﬁcation error versus constraint violation, with respect to DP (top two rows) and EO (bottom two rows). Markers
correspond to the baselines. For our two reductions and the interpolants between reweighting (or relabeling) and the unconstrained
classiﬁer, we varied their tradeoff parameters and plot the Pareto frontiers of the sets of classiﬁers obtained for each method. Because the
curves of the different methods often overlap, we use vertical dashed lines to indicate the lowest constraint violations. All data sets have
binary protected attributes except for adult4, which has four protected attribute values, so relabeling is not applicable and grid search is
not feasible for this data set. The exponentiated-gradient reduction dominates or matches other approaches as expected since it solves
exactly for the points on the Pareto frontier of the set of all classiﬁers in each considered class.

A Reductions Approach to Fair Classiﬁcation

Figure 3. Test classiﬁcation error versus constraint violation, with respect to DP (top two rows) and EO (bottom two rows). Markers
correspond to the baselines. For our two reductions and the interpolants between reweighting (or relabeling) and the unconstrained
classiﬁer, we show convex envelopes of the classiﬁers taken from the training Pareto frontier of each method (i.e., the same classiﬁers as
shown in Figure 2). Because the curves of the different methods often overlap, we use vertical dashed lines to indicate the lowest constraint
violations. All data sets have binary protected attributes except for adult4, which has four protected attribute values, so relabeling
is not applicable and grid search is not feasible for this data set. We show 95% conﬁdence bands for the classiﬁcation error of the
exponentiated-gradient reduction and 95% conﬁdence intervals for the constraint violation of post-processing. The exponentiated-gradient
reduction dominates or matches performance of all other methods up to statistical uncertainty.

A Reductions Approach to Fair Classiﬁcation

8
1
0
2
 
l
u
J
 
6
1
 
 
]

G
L
.
s
c
[
 
 
3
v
3
5
4
2
0
.
3
0
8
1
:
v
i
X
r
a

Alekh Agarwal 1 Alina Beygelzimer 2 Miroslav Dud´ık 1 John Langford 1 Hanna Wallach 1

Abstract

We present a systematic approach for achieving
fairness in a binary classiﬁcation setting. While
we focus on two well-known quantitative deﬁni-
tions of fairness, our approach encompasses many
other previously studied deﬁnitions as special
cases. The key idea is to reduce fair classiﬁcation
to a sequence of cost-sensitive classiﬁcation
problems, whose solutions yield a randomized
classiﬁer with the lowest (empirical) error subject
to the desired constraints. We introduce two
reductions that work for any representation of the
cost-sensitive classiﬁer and compare favorably
to prior baselines on a variety of data sets, while
overcoming several of their disadvantages.

1. Introduction

Over the past few years, the media have paid considerable
attention to machine learning systems and their ability to
inadvertently discriminate against minorities, historically
disadvantaged populations, and other protected groups when
allocating resources (e.g., loans) or opportunities (e.g., jobs).
In response to this scrutiny—and driven by ongoing debates
and collaborations with lawyers, policy-makers, social sci-
entists, and others (e.g., Barocas & Selbst, 2016)—machine
learning researchers have begun to turn their attention to the
topic of “fairness in machine learning,” and, in particular, to
the design of fair classiﬁcation and regression algorithms.

In this paper we study the task of binary classiﬁcation sub-
ject to fairness constraints with respect to a pre-deﬁned pro-
tected attribute, such as race or sex. Previous work in this
area can be divided into two broad groups of approaches.

The ﬁrst group of approaches incorporate speciﬁc quanti-
tative deﬁnitions of fairness into existing machine learning

1Microsoft Research, New York 2Yahoo! Research, New York.
Correspondence to: A. Agarwal <alekha@microsoft.com>,
Dud´ık
A.
<mdudik@microsoft.com>, J. Langford <jcl@microsoft.com>,
H. Wallach <wallach@microsoft.com>.

Beygelzimer <beygel@gmail.com>,

M.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

methods, often by relaxing the desired deﬁnitions of fair-
ness, and only enforcing weaker constraints, such as lack of
correlation (e.g., Woodworth et al., 2017; Zafar et al., 2017;
Johnson et al., 2016; Kamishima et al., 2011; Donini et al.,
2018). The resulting fairness guarantees typically only hold
under strong distributional assumptions, and the approaches
are tied to speciﬁc families of classiﬁers, such as SVMs.

The second group of approaches eliminate the restriction
to speciﬁc classiﬁer families and treat the underlying clas-
siﬁcation method as a “black box,” while implementing
a wrapper that either works by pre-processing the data or
post-processing the classiﬁer’s predictions (e.g., Kamiran
& Calders, 2012; Feldman et al., 2015; Hardt et al., 2016;
Calmon et al., 2017). Existing pre-processing approaches
are speciﬁc to particular deﬁnitions of fairness and typically
seek to come up with a single transformed data set that will
work across all learning algorithms, which, in practice, leads
to classiﬁers that still exhibit substantial unfairness (see our
evaluation in Section 4). In contrast, post-processing allows
a wider range of fairness deﬁnitions and results in provable
fairness guarantees. However, it is not guaranteed to ﬁnd the
most accurate fair classiﬁer, and requires test-time access to
the protected attribute, which might not be available.

We present a general-purpose approach that has the key
advantage of this second group of approaches—i.e., the
underlying classiﬁcation method is treated as a black
box—but without the noted disadvantages. Our approach
encompasses a wide range of fairness deﬁnitions,
is
guaranteed to yield the most accurate fair classiﬁer, and
does not require test-time access to the protected attribute.
Speciﬁcally, our approach allows any deﬁnition of fairness
that can be formalized via linear inequalities on conditional
such as demographic parity or equalized
moments,
odds (see Section 2.1). We show how binary classiﬁcation
subject to these constraints can be reduced to a sequence
of cost-sensitive classiﬁcation problems. We require only
black-box access to a cost-sensitive classiﬁcation algorithm,
which does not need to have any knowledge of the desired
deﬁnition of fairness or protected attribute. We show that
the solutions to our sequence of cost-sensitive classiﬁcation
problems yield a randomized classiﬁer with the lowest
(empirical) error subject to the desired fairness constraints.

Corbett-Davies et al. (2017) and Menon & Williamson

A Reductions Approach to Fair Classiﬁcation

(2018) begin with a similar goal to ours, but they analyze
the Bayes optimal classiﬁer under fairness constraints in the
limit of inﬁnite data. In contrast, our focus is algorithmic,
our approach applies to any classiﬁer family, and we obtain
ﬁnite-sample guarantees. Dwork et al. (2018) also begin
with a similar goal to ours. Their approach partitions the
training examples into subsets according to protected at-
tribute values and then leverages transfer learning to jointly
learn from these separate data sets. Our approach avoids par-
titioning the data and assumes access only to a classiﬁcation
algorithm rather than a transfer learning algorithm.

A preliminary version of this paper appeared at the FAT/ML
workshop (Agarwal et al., 2017), and led to extensions with
more general optimization objectives (Alabi et al., 2018)
and combinatorial protected attributes (Kearns et al., 2018).

In the next section, we formalize our problem. While we
focus on two well-known quantitative deﬁnitions of fairness,
our approach also encompasses many other previously stud-
ied deﬁnitions of fairness as special cases. In Section 3, we
describe our reductions approach to fair classiﬁcation and
its guarantees in detail. The experimental study in Section 4
shows that our reductions compare favorably to three base-
lines, while overcoming some of their disadvantages and
also offering the ﬂexibility of picking a suitable accuracy–
fairness tradeoff. Our results demonstrate the utility of
having a general-purpose approach for combining machine
learning methods and quantitative fairness deﬁnitions.

2. Problem Formulation

We consider a binary classiﬁcation setting where the training
examples consist of triples (X, A, Y ), where X ∈ X is a fea-
ture vector, A ∈ A is a protected attribute, and Y ∈ {0, 1}
is a label. The feature vector X can either contain the pro-
tected attribute A as one of the features or contain other fea-
tures that are arbitrarily indicative of A. For example, if the
classiﬁcation task is to predict whether or not someone will
default on a loan, each training example might correspond
to a person, where X represents their demographics, income
level, past payment history, and loan amount; A represents
their race; and Y represents whether or not they defaulted on
that loan. Note that X might contain their race as one of the
features or, for example, contain their zipcode—a feature
that is often correlated with race. Our goal is to learn an ac-
curate classiﬁer h : X → {0, 1} from some set (i.e., family)
of classiﬁers H, such as linear threshold rules, decision trees,
or neural nets, while satisfying some deﬁnition of fairness.
Note that the classiﬁers in H do not explicitly depend on A.

2.1. Fairness Deﬁnitions

We focus on two well-known quantitative deﬁnitions of
fairness that have been considered in previous work on

fair classiﬁcation; however, our approach also encompasses
many other previously studied deﬁnitions of fairness as
special cases, as we explain at the end of this section.

The ﬁrst deﬁnition—demographic (or statistical) parity—
can be thought of as a stronger version of the US Equal
Employment Opportunity Commission’s “four-ﬁfths rule,”
which requires that the “selection rate for any race, sex, or
ethnic group [must be at least] four-ﬁfths (4/5) (or eighty
percent) of the rate for the group with the highest rate.”1
Deﬁnition 1 (Demographic parity—DP). A classiﬁer h
satisﬁes demographic parity under a distribution over
(X, A, Y ) if its prediction h(X) is statistically indepen-
dent of the protected attribute A—that is, if P[h(X) = ˆy |
A = a] = P[h(X) = ˆy] for all a, ˆy. Because ˆy ∈ {0, 1},
this is equivalent to E[h(X) | A = a] = E[h(X)] for all a.

The second deﬁnition—equalized odds—was recently pro-
posed by Hardt et al. (2016) to remedy two previously noted
ﬂaws with demographic parity (Dwork et al., 2012). First,
demographic parity permits a classiﬁer which accurately
classiﬁes data points with one value A = a, such as the
value a with the most data, but makes random predictions
for data points with A (cid:54)= a as long as the probabilities of
h(X) = 1 match. Second, demographic parity rules out
perfect classiﬁers whenever Y is correlated with A.
In
contrast, equalized odds suffers from neither of these ﬂaws.

Deﬁnition 2 (Equalized odds—EO). A classiﬁer h satis-
ﬁes equalized odds under a distribution over (X, A, Y )
if its prediction h(X) is conditionally independent of
the protected attribute A given the label Y —that is, if
P[h(X) = ˆy | A = a, Y = y] = P[h(X) = ˆy | Y = y] for
all a, y, and ˆy. Because ˆy ∈ {0, 1}, this is equivalent to
E[h(X) | A = a, Y = y] = E[h(X) | Y = y] for all a, y.

We now show how each deﬁnition can be viewed as a special
case of a general set of linear constraints of the form

Mµ(h) ≤ c,

(1)

where matrix M ∈ R|K|×|J| and vector c ∈ R|K| describe
the linear constraints, each indexed by k ∈ K, and µ(h) ∈
R|J| is a vector of conditional moments of the form

µj(h) = E(cid:2) gj(X, A, Y, h(X)) (cid:12)

(cid:12) Ej

(cid:3)

for j ∈ J,

where gj : X × A × {0, 1} × {0, 1} → [0, 1] and Ej is
an event deﬁned with respect to (X, A, Y ). Crucially, gj
depends on h, while Ej cannot depend on h in any way.
Example 1 (DP). In a binary classiﬁcation setting, demo-
graphic parity can be expressed as a set of |A| equality
constraints, each of the form E[h(X) | A = a] = E[h(X)].
Letting J = A ∪ {(cid:63)}, gj(X, A, Y, h(X)) = h(X) for all j,

1See the Uniform Guidelines on Employment Selection Proce-

dures, 29 C.F.R. §1607.4(D) (2015).

A Reductions Approach to Fair Classiﬁcation

Ea = {A = a}, and E(cid:63) = {True}, where {True} refers to
the event encompassing all points in the sample space, each
equality constraint can be expressed as µa(h) = µ(cid:63)(h).2
Finally, because each such constraint can be equivalently
expressed as a pair of inequality constraints of the form

µa(h) − µ(cid:63)(h) ≤ 0
−µa(h) + µ(cid:63)(h) ≤ 0,

demographic parity can be expressed as equation (1), where
K = A×{+, −}, M(a,+),a(cid:48) = 1{a(cid:48) = a}, M(a,+),(cid:63) = −1,
M(a,−),a(cid:48) = −1{a(cid:48) = a}, M(a,−),(cid:63) = 1, and c = 0.
Expressing each equality constraint as a pair of inequality
constraints allows us to control the extent to which each
constraint is enforced by positing ck > 0 for some (or all) k.
In a binary classiﬁcation set-
Example 2 (EO).
equalized odds can be expressed as a set
ting,
of 2 |A|
form
constraints,
E[h(X) | A = a, Y = y] = E[h(X) | Y = y]. Letting
J = (A ∪ {(cid:63)}) × {0, 1}, gj(X, A, Y, h(X)) = h(X) for
all j, E(a,y) = {A = a, Y = y}, and E((cid:63),y) = {Y = y},
each equality constraint can be equivalently expressed as

each of

equality

the

µ(a,y)(h) − µ((cid:63),y)(h) ≤ 0
−µ(a,y)(h) + µ((cid:63),y)(h) ≤ 0.

can be

equalized odds

As a result,
expressed
as equation (1), where K = A × Y × {+, −},
M(a,y,+),(a(cid:48),y(cid:48)) = 1{a(cid:48)= a, y(cid:48)= y}, M(a,y,+),((cid:63),y(cid:48)) = −1,
M(a,y,−),(a(cid:48),y(cid:48)) = −1{a(cid:48)= a, y(cid:48)= y}, M(a,y,−),((cid:63),y(cid:48)) = 1,
and c = 0. Again, we can posit ck > 0 for some (or all) k
to allow small violations of some (or all) of the constraints.

Although we omit the details, we note that many other pre-
viously studied deﬁnitions of fairness can also be expressed
as equation (1). For example, equality of opportunity (Hardt
et al., 2016) (also known as balance for the positive class;
Kleinberg et al., 2017), balance for the negative class (Klein-
berg et al., 2017), error-rate balance (Chouldechova,
2017), overall accuracy equality (Berk et al., 2017), and
treatment equality (Berk et al., 2017) can all be expressed
as equation (1); in contrast, calibration (Kleinberg et al.,
2017) and predictive parity (Chouldechova, 2017) cannot
because to do so would require the event Ej to depend on
h. We note that our approach can also be used to satisfy
multiple deﬁnitions of fairness, though if these deﬁnitions
are mutually contradictory, e.g., as described by Kleinberg
et al. (2017), then our guarantees become vacuous.

2.2. Fair Classiﬁcation

In a standard (binary) classiﬁcation setting, the goal is to
learn the classiﬁer h ∈ H with the minimum classiﬁcation

2Note that µ(cid:63)(h) = E[h(X) | True] = E[h(X)].

error: err(h) := P[h(X) (cid:54)= Y ]. However, because our
goal is to learn the most accurate classiﬁer while satisfying
fairness constraints, as formalized above, we instead seek to
ﬁnd the solution to the constrained optimization problem3

min
h∈H

err(h)

subject to Mµ(h) ≤ c.

(2)

Furthermore, rather than just considering classiﬁers in the
set H, we can enlarge the space of possible classiﬁers by
considering randomized classiﬁers that can be obtained via
a distribution over H. By considering randomized classi-
ﬁers, we can achieve better accuracy–fairness tradeoffs than
would otherwise be possible. A randomized classiﬁer Q
makes a prediction by ﬁrst sampling a classiﬁer h ∈ H
from Q and then using h to make the prediction. The result-
ing classiﬁcation error is err(Q) = (cid:80)
h∈H Q(h) err(h) and
the conditional moments are µ(Q) = (cid:80)
h∈H Q(h)µ(h)
(see Appendix A for the derivation). Thus we seek to solve

min
Q∈∆

err(Q)

subject to Mµ(Q) ≤ c,

(3)

where ∆ is the set of all distributions over H.

In practice, we do not know the true distribution over
(X, A, Y ) and only have access to a data set of training ex-
amples {(Xi, Ai, Yi)}n
i=1. We therefore replace err(Q) and
µ(Q) in equation (3) with their empirical versions (cid:99)err(Q)
and (cid:98)µ(Q). Because of the sampling error in (cid:98)µ(Q), we
also allow errors in satisfying the constraints by setting
(cid:98)ck = ck + εk for all k, where εk ≥ 0. After these modiﬁca-
tions, we need to solve the empirical version of equation (3):

Q∈∆ (cid:99)err(Q)
min

subject to M(cid:98)µ(Q) ≤ (cid:98)c.

(4)

3. Reductions Approach

We now show how the problem (4) can be reduced to a se-
quence of cost-sensitive classiﬁcation problems. We further
show that the solutions to our sequence of cost-sensitive clas-
siﬁcation problems yield a randomized classiﬁer with the
lowest (empirical) error subject to the desired constraints.

3.1. Cost-sensitive Classiﬁcation

We assume access to a cost-sensitive classiﬁcation algorithm
for the set H. The input to such an algorithm is a data set
of training examples {(Xi, C 0
i and C 1
i
denote the losses—costs in this setting—for predicting the
labels 0 or 1, respectively, for Xi. The algorithm outputs

i=1, where C 0

i , C 1

i )}n

arg min
h∈H

n
(cid:88)

i=1

h(Xi) C 1

i + (1 − h(Xi)) C 0
i .

(5)

3We consider misclassiﬁcation error for concreteness, but all
the results in this paper apply to any error of the form err(h) =
E[gerr(X, A, Y, h(X))], where gerr(·, ·, ·, ·) ∈ [0, 1].

A Reductions Approach to Fair Classiﬁcation

min
Q∈∆

max
λ∈R|K|

+

L(Q, λ).

(6)

L( (cid:98)Q, (cid:98)λ) ≤ L(Q, (cid:98)λ) + ν

This abstraction allows us to specify different costs for dif-
ferent training examples, which is essential for incorporat-
ing fairness constraints. Moreover, efﬁcient cost-sensitive
classiﬁcation algorithms are readily available for several
common classiﬁer representations (e.g., Beygelzimer et al.,
2005; Langford & Beygelzimer, 2005; Fan et al., 1999). In
particular, equation (5) is equivalent to a weighted classi-
ﬁcation problem, where the input consists of labeled ex-
amples {(Xi, Yi, Wi)}n
i=1 with Yi ∈ {0, 1} and Wi ≥ 0,
and the goal is to minimize the weighted classiﬁcation er-
ror (cid:80)n
i=1 Wi 1{h(Xi) (cid:54)= Yi}. This is equivalent to equa-
tion (5) if we set Wi = |C 0
i }.

i | and Yi = 1{C 0

i ≥ C 1

i − C 1

3.2. Reduction

To derive our fair classiﬁcation algorithm, we rewrite equa-
tion (4) as a saddle point problem. We begin by introducing
a Lagrange multiplier λk ≥ 0 for each of the |K| constraints,
summarized as λ ∈ R|K|

+ , and form the Lagrangian
L(Q, λ) = (cid:99)err(Q) + λ(cid:62)(cid:0)M(cid:98)µ(Q) − (cid:98)c(cid:1).

Thus, equation (4) is equivalent to

For computational and statistical reasons, we impose an
additional constraint on the (cid:96)1 norm of λ and seek to simul-
taneously ﬁnd the solution to the constrained version of (6)
as well as its dual, obtained by switching min and max:

min
Q∈∆

max

L(Q, λ),

λ∈R|K|

+ , (cid:107)λ(cid:107)1≤B

max
+ , (cid:107)λ(cid:107)1≤B

λ∈R|K|

min
Q∈∆

L(Q, λ).

(P)

(D)

Because L is linear in Q and λ and the domains of Q and
λ are convex and compact, both problems have solutions
(which we denote by Q† and λ†) and the minimum value of
(P) and the maximum value of (D) are equal and coincide
with L(Q†, λ†). Thus, (Q†, λ†) is the saddle point of L
(Corollary 37.6.2 and Lemma 36.2 of Rockafellar, 1970).

We ﬁnd the saddle point by using the standard scheme of
Freund & Schapire (1996), developed for the equivalent
problem of solving for an equilibrium in a zero-sum game.
From game-theoretic perspective, the saddle point can be
viewed as an equilibrium of a game between two players:
the Q-player choosing Q and the λ-player choosing λ. The
Lagrangian L(Q, λ) speciﬁes how much the Q-player has to
pay to the λ-player after they make their choices. At the sad-
dle point, neither player wants to deviate from their choice.

Our algorithm ﬁnds an approximate equilibrium in which
neither player can gain more than ν by changing their choice

Algorithm 1 Exp. gradient reduction for fair classiﬁcation

Input: training examples {(Xi, Yi, Ai)}n

i=1

fairness constraints speciﬁed by gj, Ej, M, (cid:98)c
bound B, accuracy ν, learning rate η

Set θ1 = 0 ∈ R|K|
for t = 1, 2, . . . do
Set λt,k = B
1+(cid:80)
ht ← BESTh(λt)
(cid:98)Qt ← 1
t
(cid:98)λt ← 1
t

(cid:80)t

(cid:80)t

νt ← max
if νt ≤ ν then

Return ( (cid:98)Qt, (cid:98)λt)

exp{θk}

k(cid:48) ∈K exp{θk(cid:48) } for all k ∈ K
(cid:16)

(cid:17)

t(cid:48)=1 ht(cid:48), L ← L

t(cid:48)=1 λt(cid:48), L ← L
(cid:110)

(cid:16)

(cid:98)Qt, BESTλ( (cid:98)Qt)
(cid:17)

BESTh((cid:98)λt), (cid:98)λt
(cid:111)

L( (cid:98)Qt, (cid:98)λt) − L, L − L( (cid:98)Qt, (cid:98)λt)

end if
Set θt+1 = θt + η (M(cid:98)µ(ht) − (cid:98)c)

end for

(where ν > 0 is an input to the algorithm). Such an approx-
imate equilibrium corresponds to a ν-approximate saddle
point of the Lagrangian, which is a pair ( (cid:98)Q, (cid:98)λ), where

for all Q ∈ ∆,
for all λ ∈ R|K|

L( (cid:98)Q, (cid:98)λ) ≥ L( (cid:98)Q, λ) − ν

+ , (cid:107)λ(cid:107)1 ≤ B.

We proceed iteratively by running a no-regret algorithm for
the λ-player, while executing the best response of the Q-
player. Following Freund & Schapire (1996), the average
play of both players converges to the saddle point. We run
the exponentiated gradient algorithm (Kivinen & Warmuth,
1997) for the λ-player and terminate as soon as the subop-
timality of the average play falls below the pre-speciﬁed
accuracy ν. The best response of the Q-player can always
be chosen to put all of the mass on one of the candidate
classiﬁers h ∈ H, and can be implemented by a single call
to a cost-sensitive classiﬁcation algorithm for the set H.

Algorithm 1 fully implements this scheme, except for the
functions BESTλ and BESTh, which correspond to the best-
response algorithms of the two players. (We need the best
response of the λ-player to evaluate whether the subopti-
mality of the current average play has fallen below ν.) The
two best response functions can be calculated as follows.

BESTλ(Q): the best response of the λ-player. The best
response of the λ-player for a given Q is any maximizer of
L(Q, λ) over all valid λs. In our setting, it can always be
chosen to be either 0 or put all of the mass on the most vio-
lated constraint. Letting (cid:98)γ(Q) := M(cid:98)µ(Q) and letting ek de-
note the kth vector of the standard basis, BESTλ(Q) returns
(cid:40)

0
Bek∗

if (cid:98)γ(Q) ≤ (cid:98)c,
otherwise, where k∗ = arg maxk[(cid:98)γk(Q) − (cid:98)ck].

A Reductions Approach to Fair Classiﬁcation

BESTh(λ): the best response of the Q-player. Here, the
best response minimizes L(Q, λ) over all Qs in the simplex.
Because L is linear in Q, the minimizer can always be cho-
sen to put all of the mass on a single classiﬁer h. We show
how to obtain the classiﬁer constituting the best response
via a reduction to cost-sensitive classiﬁcation. Letting pj :=
(cid:98)P[Ej] be the empirical event probabilities, the Lagrangian
for Q which puts all of the mass on a single h is then
L(h, λ) = (cid:99)err(h) + λ(cid:62)(cid:0)M(cid:98)µ(h) − (cid:98)c(cid:1)
(cid:88)
= (cid:98)E(cid:2)1{h(X) (cid:54)= Y }(cid:3) − λ(cid:62)

Mk,jλk (cid:98)µj(h)

(cid:98)c +

k,j

= −λ(cid:62)

(cid:88)

+

k,j

(cid:98)c + (cid:98)E(cid:2)1{h(X) (cid:54)= Y }(cid:3)
(cid:104)
Mk,jλk
(cid:98)E
pj

gj

(cid:0)X,A,Y,h(X)(cid:1) 1{(X,A,Y ) ∈ Ej}

(cid:105)
.

Assuming a data set of training examples {(Xi, Ai, Yi)}n
i=1,
the minimization of L(h, λ) over h then corresponds to cost-
sensitive classiﬁcation on {(Xi, C 0

i=1 with costs4

i , C 1

i )}n

Mk,jλk
pj

C 0

i = 1{Yi (cid:54)= 0}
(cid:88)

+

C 1

k,j
i = 1{Yi (cid:54)= 1}
(cid:88)

Mk,jλk
pj

+

k,j

gj(Xi,Ai,Yi, 0) 1{(Xi,Ai,Yi) ∈ Ej}

gj(Xi,Ai,Yi, 1) 1{(Xi,Ai,Yi) ∈ Ej}.

needing more iterations to reach any given suboptimality. In
particular, as we derive in the theorem, achieving subopti-
mality ν may need up to 4ρ2B2 log(|K| + 1) / ν2 iterations.
Example 3 (DP). Using the matrix M for demographic
parity as described in Section 2, the cost-sensitive reduction
for a vector of Lagrange multipliers λ uses costs

C 0

i = 1{Yi (cid:54)= 0}, C 1

i = 1{Yi (cid:54)= 1} +

λAi
pAi

(cid:88)

−

λa,

a∈A

where pa := (cid:98)P[A = a] and λa := λ(a,+) − λ(a,−), effec-
tively replacing two non-negative Lagrange multipliers by a
single multiplier, which can be either positive or negative.
Because ck = 0 for all k, (cid:98)ck = εk. Furthermore, because
all empirical moments are bounded in [0, 1], we can assume
εk ≤ 1, which yields the bound ρ ≤ 2. Thus, Algorithm 1
terminates in at most 16B2 log(2 |A| + 1) / ν2 iterations.
Example 4 (EO). For equalized odds, the cost-sensitive
reduction for a vector of Lagrange multipliers λ uses costs

C 0

i = 1{Yi (cid:54)= 0},

C 1

i = 1{Yi (cid:54)= 1} +

λ(Ai,Yi)
p(Ai,Yi)

(cid:88)

−

a∈A

λ(a,Yi)
p((cid:63),Yi)

,

where p(a,y) := (cid:98)P[A = a, Y = y], p((cid:63),y) := (cid:98)P[Y = y], and
λ(a,y) := λ(a,y,+) − λ(a,y,−). If we again assume εk ≤ 1,
then we obtain the bound ρ ≤ 2. Thus, Algorithm 1 termi-
nates in at most 16B2 log(4 |A| + 1) / ν2 iterations.

Theorem 1. Letting ρ := maxh(cid:107)M(cid:98)µ(h) − (cid:98)c(cid:107)∞, Algo-
rithm 1 satisﬁes the inequality

3.3. Error Analysis

νt ≤

B log(|K| + 1)
ηt

+ ηρ2B.

Thus, for η = ν
saddle point of L in at most 4ρ2B2 log(|K|+1)

2ρ2B , Algorithm 1 will return a ν-approximate

iterations.

ν2

√

This theorem, proved in Appendix B, bounds the subopti-
mality νt of the average play ( (cid:98)Qt, (cid:98)λt), which is equal to its
suboptimality as a saddle point. The right-hand side of the
bound is optimized by η = (cid:112)log(|K| + 1) / (ρ
t), lead-
ing to the bound νt ≤ 2ρB(cid:112)log(|K| + 1) / t. This bound
decreases with the number of iterations t and grows very
slowly with the number of constraints |K|. The quantity ρ
is a problem-speciﬁc constant that bounds how much any
single classiﬁer h ∈ H can violate the desired set of fair-
ness constraints. Finally, B is the bound on the (cid:96)1-norm of
λ, which we introduced to enable this speciﬁc algorithmic
scheme. In general, larger values of B will bring the prob-
lem (P) closer to (6), and thus also to (4), but at the cost of

4For general error, err(h) = E[gerr(X, A, Y, h(X))], the costs
i contain, respectively, the terms gerr(Xi, Ai, Yi, 0) and

C 0
gerr(Xi, Ai, Yi, 1) instead of 1{Yi (cid:54)= 0} and 1{Yi (cid:54)= 1}.

i and C 1

Our ultimate goal, as formalized in equation (3), is to
minimize the classiﬁcation error while satisfying fairness
constraints under a true but unknown distribution over
(X, A, Y ). In the process of deriving Algorithm 1, we in-
troduced three different sources of error. First, we replaced
the true classiﬁcation error and true moments with their
empirical versions. Second, we introduced a bound B on
the magnitude of λ. Finally, we only run the optimization
algorithm for a ﬁxed number of iterations, until it reaches
suboptimality level ν. The ﬁrst source of error, due to the
use of empirical rather than true quantities, is unavoidable
and constitutes the underlying statistical error. The other two
sources of error, the bound B and the suboptimality level ν,
stem from the optimization algorithm and can be driven
arbitrarily small at the cost of additional iterations. In this
section, we show how the statistical error and the optimiza-
tion error affect the true accuracy and the fairness of the ran-
domized classiﬁer returned by Algorithm 1—in other words,
how well Algorithm 1 solves our original problem (3).

To bound the statistical error, we use the Rademacher
complexity of the classiﬁer family H, which we denote
by Rn(H), where n is the number of training examples.
We assume that Rn(H) ≤ Cn−α for some C ≥ 0 and

A Reductions Approach to Fair Classiﬁcation

α ≤ 1/2. We note that α = 1/2 in the vast majority
including norm-bounded linear
of classiﬁer families,
functions (see Theorem 1 of Kakade et al., 2009), neural
networks (see Theorem 18 of Bartlett & Mendelson, 2002),
and classiﬁer families with bounded VC dimension (see
Lemma 4 and Theorem 6 of Bartlett & Mendelson, 2002).

Recall that in our empirical optimization problem we as-
sume that (cid:98)ck = ck + εk, where εk ≥ 0 are error bounds that
account for the discrepancy between µ(Q) and (cid:98)µ(Q). In
our analysis, we assume that these error bounds have been
set in accordance with the Rademacher complexity of H.
Assumption 1. There exists C, C (cid:48) ≥ 0 and α ≤ 1/2
such that Rn(H) ≤ Cn−α and εk = C (cid:48) (cid:80)
j∈J|Mk,j|n−α
,
where nj is the number of data points that fall in Ej,

j

nj := (cid:12)
(cid:12)

(cid:8)i : (Xi, Ai, Yi) ∈ Ej

(cid:9)(cid:12)
(cid:12).

The optimization error can be bounded via a careful analy-
sis of the Lagrangian and the optimality conditions of (P)
and (D). Combining the three different sources of error
yields the following bound, which we prove in Appendix C.
Theorem 2. Let Assumption 1 hold for C (cid:48) ≥ 2C +
2 + (cid:112)ln(4/δ) / 2, where δ > 0. Let ( (cid:98)Q, (cid:98)λ) be any ν-
approximate saddle point of L, let Q(cid:63) minimize err(Q) sub-
j = P[Ej]. Then, with proba-
ject to Mµ(Q) ≤ c, and let p(cid:63)
bility at least 1 − (|J| + 1)δ, the distribution (cid:98)Q satisﬁes

err( (cid:98)Q) ≤ err(Q(cid:63)) + 2ν + (cid:101)O(n−α),
1+2ν
B

γk( (cid:98)Q) ≤ ck +

(cid:88)

+

j∈J

|Mk,j| (cid:101)O(n−α
j )

for all k,

where (cid:101)O(·) suppresses polynomial dependence on ln(1/δ).
If np(cid:63)

j ≥ 8 log(2/δ) for all j, then, for all k,

γk( (cid:98)Q) ≤ ck +

1+2ν
B

(cid:88)

+

j∈J

|Mk,j| (cid:101)O

(cid:16)

(np(cid:63)

j )−α(cid:17)

.

the solution returned by Algorithm 1
In other words,
achieves the lowest feasible classiﬁcation error on the true
distribution up to the optimization error, which grows lin-
early with ν, and the statistical error, which grows as n−α.
Therefore, if we want to guarantee that the optimization er-
ror does not dominate the statistical error, we should set ν ∝
n−α. The fairness constraints on the true distribution are
satisﬁed up to the optimization error (1 + 2ν) /B and up to
the statistical error. Because the statistical error depends on
the moments, and the error in estimating the moments grows
as n−α
j ≥ n−α, we can set B ∝ nα to guarantee that the op-
timization error does not dominate the statistical error. Com-
bining this reasoning with the learning rate setting of Theo-
rem 1 yields the following theorem (proved in Appendix C).
Theorem 3. Let ρ := maxh(cid:107)M(cid:98)µ(h) − (cid:98)c(cid:107)∞. Let Assump-
tion 1 hold for C (cid:48) ≥ 2C + 2 + (cid:112)ln(4/δ) / 2, where δ > 0.

Let Q(cid:63) minimize err(Q) subject to Mµ(Q) ≤ c. Then
Algorithm 1 with ν ∝ n−α, B ∝ nα and η ∝ ρ−2n−2α ter-
minates in O(ρ2n4α ln |K|) iterations and returns (cid:98)Q, which
with probability at least 1 − (|J| + 1)δ satisﬁes

err( (cid:98)Q) ≤ err(Q(cid:63)) + (cid:101)O(n−α),
γk( (cid:98)Q) ≤ ck +

|Mk,j| (cid:101)O(n−α

(cid:88)

j

)

j∈J

for all k.

Example 5 (DP). If na denotes the number of training ex-
amples with Ai = a, then Assumption 1 states that we
should set ε(a,+) = ε(a,−) = C (cid:48)(n−α
a + n−α) and Theo-
rem 3 then shows that for a suitable setting of C (cid:48), ν, B,
and η, Algorithm 1 will return a randomized classiﬁer (cid:98)Q
with the lowest feasible classiﬁcation error up to (cid:101)O(n−α)
while also approximately satisfying the fairness constraints

(cid:12)
(cid:12)
(cid:12)

(cid:12)
E[h(X) | A = a] − E[h(X)]
(cid:12) ≤ (cid:101)O(n−α
(cid:12)
a )

for all a,

where E is with respect to (X, A, Y ) as well as h ∼ (cid:98)Q.
Example 6 (EO). Similarly, if n(a,y) denotes the number
of examples with Ai = a and Yi = y and n((cid:63),y) denotes the
number of examples with Yi = y, then Assumption 1 states
that we should set ε(a,y,+) = ε(a,y,−) = C (cid:48)(n−α
((cid:63),y))
and Theorem 3 then shows that for a suitable setting of C (cid:48), ν,
B, and η, Algorithm 1 will return a randomized classiﬁer (cid:98)Q
with the lowest feasible classiﬁcation error up to (cid:101)O(n−α)
while also approximately satisfying the fairness constraints

(a,y) + n−α

(cid:12)
(cid:12)
(cid:12)

(cid:12)
E[h(X) | A = a, Y = y] − E[h(X) | Y = y]
(cid:12) ≤ (cid:101)O(n−α
(cid:12)

(a,y))

for all a, y. Again, E includes randomness under the true
distribution over (X, A, Y ) as well as h ∼ (cid:98)Q.

3.4. Grid Search

In some situations, it is preferable to select a deterministic
classiﬁer, even if that means a lower accuracy or a modest vi-
olation of the fairness constraints. A set of candidate classi-
ﬁers can be obtained from the saddle point (Q†, λ†). Specif-
ically, because Q† is a minimizer of L(Q, λ†) and L is
linear in Q, the distribution Q† puts non-zero mass only on
classiﬁers that are the Q-player’s best responses to λ†. If we
knew λ†, we could retrieve one such best response via the re-
duction to cost-sensitive learning introduced in Section 3.2.
We can compute λ† using Algorithm 1, but when the number
of constraints is very small, as is the case for demographic
parity or equalized odds with a binary protected attribute,
it is also reasonable to consider a grid of values λ, calculate
the best response for each value, and then select the value
with the desired tradeoff between accuracy and fairness.
Example 7 (DP). When the protected attribute is binary,
e.g., A ∈ {a, a(cid:48)}, then the grid search can in fact be con-
ducted in a single dimension. The reduction formally takes

A Reductions Approach to Fair Classiﬁcation

two real-valued arguments λa and λa(cid:48), and then adjusts the
costs for predicting h(Xi) = 1 by the amounts

δa =

− λa − λa(cid:48)

and δa(cid:48) =

− λa − λa(cid:48),

λa(cid:48)
pa(cid:48)

λa
pa

respectively, on the training examples with Ai = a and
Ai = a(cid:48). These adjustments satisfy paδa + pa(cid:48)δa(cid:48) = 0,
so instead of searching over λa and λa(cid:48), we can carry out
the grid search over δa alone and apply the adjustment
δa(cid:48) = −paδa/pa(cid:48) to the protected attribute value a(cid:48).
With three attribute values, e.g., A ∈ {a, a(cid:48), a(cid:48)(cid:48)}, we sim-
ilarly have paδa + pa(cid:48)δa(cid:48) + pa(cid:48)(cid:48)δa(cid:48)(cid:48) = 0, so it sufﬁces to
conduct grid search in two dimensions rather than three.
Example 8 (EO). If A ∈ {a, a(cid:48)}, we obtain the adjustment

δ(a,y) =

λ(a,y)
p(a,y)

−

λ(a,y) + λ(a(cid:48),y)
p((cid:63),y)

for an example with protected attribute value a and label y,
and similarly for protected attribute value a(cid:48). In this case,
separately for each y, the adjustments satisfy

p(a,y)δ(a,y) + p(a(cid:48),y)δ(a(cid:48),y) = 0,

so it sufﬁces to do the grid search over δ(a,0) and δ(a,1) and
set the parameters for a(cid:48) to δ(a(cid:48),y) = −p(a,y)δ(a,y)/p(a(cid:48),y).

4. Experimental Results

We now examine how our exponentiated-gradient reduc-
tion5 performs at the task of binary classiﬁcation subject to
either demographic parity or equalized odds. We provide an
evaluation of our grid-search reduction in Appendix D.

We compared our reduction with the score-based post-
processing algorithm of Hardt et al. (2016), which takes as
its input any classiﬁer, (i.e., a standard classiﬁer without any
fairness constraints) and derives a monotone transformation
of the classiﬁer’s output to remove any disparity with respect
to the training examples. This post-processing algorithm
works with both demographic parity and equalized odds, as
well as with binary and non-binary protected attributes.

For demographic parity, we also compared our reduction
with the reweighting and relabeling approaches of Kamiran
& Calders (2012). Reweighting can be applied to both
binary and non-binary protected attributes and operates by
changing importance weights on each example with the
goal of removing any statistical dependence between the
protected attribute and label.6 Relabeling was developed for

5https://github.com/Microsoft/fairlearn
6Although reweighting was developed for demographic parity,
the weights that it induces are achievable by our grid search, albeit
the grid search for equalized odds rather than demographic parity.

binary protected attributes. First, a classiﬁer is trained on
the original data (without considering fairness). The training
examples close to the decision boundary are then relabeled
to remove all disparity while minimally affecting accuracy.
The ﬁnal classiﬁer is then trained on the relabeled data.

As the base classiﬁers for our reductions, we used the
weighted classiﬁcation implementations of logistic regres-
sion and gradient-boosted decision trees in scikit-learn (Pe-
In addition to the three baselines
dregosa et al., 2011).
described above, we also compared our reductions to the
“unconstrained” classiﬁers trained to optimize accuracy only.

We used four data sets, randomly splitting each one into
training examples (75%) and test examples (25%):

• The adult income data set (Lichman, 2013) (48,842
examples). Here the task is to predict whether some-
one makes more than $50k per year, with gender as the
protected attribute. To examine the performance for
non-binary protected attributes, we also conducted an-
other experiment with the same data, using both gender
and race (binarized into white and non-white) as the
protected attribute. Relabeling, which requires binary
protected attributes, was therefore not applicable here.
• ProPublica’s COMPAS recidivism data (7,918 exam-
ples). The task is to predict recidivism from someone’s
criminal history, jail and prison time, demographics,
and COMPAS risk scores, with race as the protected
attribute (restricted to white and black defendants).
• Law School Admissions Council’s National Longitu-
dinal Bar Passage Study (Wightman, 1998) (20,649
examples). Here the task is to predict someone’s even-
tual passage of the bar exam, with race (restricted to
white and black only) as the protected attribute.

• The Dutch census data set (Dutch Central Bureau for
Statistics, 2001) (60,420 examples). Here the task is
to predict whether or not someone has a prestigious
occupation, with gender as the protected attribute.

While all the evaluated algorithms require access to the pro-
tected attribute A at training time, only the post-processing
algorithm requires access to A at test time. For a fair com-
parison, we included A in the feature vector X, so all algo-
rithms had access to it at both the training time and test time.

We used the test examples to measure the classiﬁcation error
for each approach, as well as the violation of the desired fair-
(cid:12)
(cid:12)E[h(X) | A = a] − E[h(X)](cid:12)
ness constraints, i.e., maxa
(cid:12)
(cid:12)
(cid:12)E[h(X) | A = a, Y = y] − E[h(X) | Y = y](cid:12)
and maxa,y
(cid:12)
for demographic parity and equalized odds, respectively.

We ran our reduction across a wide range of tradeoffs be-
tween the classiﬁcation error and fairness constraints. We
considered ε ∈ {0.001, . . . , 0.1} and for each value ran
Algorithm 1 with (cid:98)ck = ε across all k. As expected, the
returned randomized classiﬁers tracked the training Pareto

A Reductions Approach to Fair Classiﬁcation

Figure 1. Test classiﬁcation error versus constraint violation with respect to DP (top two rows) and EO (bottom two rows). All data sets
have binary protected attributes except for adult4, which has four protected attribute values, so relabeling is not applicable there. For our
reduction approach we plot the convex envelope of the classiﬁers obtained on training data at various accuracy–fairness tradeoffs. We
show 95% conﬁdence bands for the classiﬁcation error of our reduction approach and 95% conﬁdence intervals for the constraint violation
of post-processing. Our reduction approach dominates or matches the performance of the other approaches up to statistical uncertainty.

frontier (see Figure 2 in Appendix D). In Figure 1, we evalu-
ate these classiﬁers alongside the baselines on the test data.

For all the data sets, the range of classiﬁcation errors
is much smaller than the range of constraint violations.
Almost all the approaches were able to substantially reduce
or remove disparity without much impact on classiﬁer accu-
racy. One exception was the Dutch census data set, where
the classiﬁcation error increased the most in relative terms.

Our reduction generally dominated or matched the baselines.
The relabeling approach frequently yielded solutions that
were not Pareto optimal. Reweighting yielded solutions
on the Pareto frontier, but often with substantial disparity.
As expected, post-processing yielded disparities that were
statistically indistinguishable from zero, but the resulting
classiﬁcation error was sometimes higher than achieved by
our reduction under a statistically indistinguishable dispar-
ity. In addition, and unlike the post-processing algorithm,
our reduction can achieve any desired accuracy–fairness
tradeoff, allows a wider range of fairness deﬁnitions, and
does not require access to the protected attribute at test time.

Our grid-search reduction, evaluated in Appendix D,
sometimes failed to achieve the lowest disparities on

the training data, but its performance on the test data
very closely matched that of our exponentiated-gradient
reduction. However, if the protected attribute is non-binary,
then grid search is not feasible. For instance, for the version
of the adult income data set where the protected attribute
takes on four values, the grid search would need to span
three dimensions for demographic parity and six dimensions
for equalized odds, both of which are prohibitively costly.

5. Conclusion

We presented two reductions for achieving fairness in a
binary classiﬁcation setting. Our reductions work for any
classiﬁer representation, encompass many deﬁnitions of fair-
ness, satisfy provable guarantees, and work well in practice.

Our reductions optimize the tradeoff between accuracy and
any (single) deﬁnition of fairness given training-time access
to protected attributes. Achieving fairness when training-
time access to protected attributes is unavailable remains an
open problem for future research, as does the navigation of
tradeoffs between accuracy and multiple fairness deﬁnitions.

A Reductions Approach to Fair Classiﬁcation

Acknowledgements

We would like to thank Aaron Roth, Sam Corbett-Davies,
and Emma Pierson for helpful discussions.

References

Agarwal, A., Beygelzimer, A., Dud´ık, M., and Langford, J.
A reductions approach to fair classication. In Fairness,
Accountability, and Transparency in Machine Learning
(FATML), 2017.

Alabi, D., Immorlica, N., and Kalai, A. T. Unleashing linear
optimizers for group-fair learning and optimization. In
Proceedings of the 31st Annual Conference on Learning
Theory (COLT), 2018.

Barocas, S. and Selbst, A. D. Big data’s disparate impact.

California Law Review, 104:671–732, 2016.

Bartlett, P. L. and Mendelson, S. Rademacher and gaussian
complexities: Risk bounds and structural results. Journal
of Machine Learning Research, 3:463–482, 2002.

Berk, R., Heidari, H., Jabbari, S., Kearns, M., and Roth, A.
Fairness in criminal justice risk assessments: The state of
the art. arXiv:1703.09207, 2017.

Beygelzimer, A., Dani, V., Hayes, T. P., Langford, J., and
Zadrozny, B. Error limiting reductions between clas-
siﬁcation tasks. In Proceedings of the Twenty-Second
International Conference on Machine Learning (ICML),
pp. 49–56, 2005.

Boucheron, S., Bousquet, O., and Lugosi, G. Theory of
classiﬁcation: a survey of some recent advances. ESAIM:
Probability and Statistics, 9:323–375, 2005.

Calmon, F., Wei, D., Vinzamuri, B., Ramamurthy, K. N.,
and Varshney, K. R. Optimized pre-processing for dis-
crimination prevention. In Advances in Neural Informa-
tion Processing Systems 30, 2017.

Chouldechova, A. Fair prediction with disparate impact: A
study of bias in recidivism prediction instruments. Big
Data, Special Issue on Social and Technical Trade-Offs,
2017.

Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., and Huq,
A. Algorithmic decision making and the cost of fairness.
In Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining,
pp. 797–806, 2017.

Donini, M., Oneto, L., Ben-David, S., Shawe-Taylor, J., and
Pontil, M. Empirical risk minimization under fairness
constraints. 2018. arXiv:1802.08626.

Dwork, C., Hardt, M., Pitassi, T., Reingold, O., and Zemel,
R. Fairness through awareness. In Proceedings of the 3rd
Innovations in Theoretical Computer Science Conference,
pp. 214–226, 2012.

Dwork, C., Immorlica, N., Kalai, A. T., and Leiserson, M.
Decoupled classiﬁers for group-fair and efﬁcient machine
learning. In Conference on Fairness, Accountability and
Transparency (FAT (cid:63)), pp. 119–133, 2018.

Fan, W., Stolfo, S. J., Zhang, J., and Chan, P. K. Adacost:
Misclassiﬁcation cost-sensitive boosting. In Proceedings
of the Sixteenth International Conference on Machine
Learning (ICML), pp. 97–105, 1999.

Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C.,
and Venkatasubramanian, S. Certifying and removing dis-
parate impact. In Proceedings of the 21st ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining, 2015.

Freund, Y. and Schapire, R. E. Game theory, on-line pre-
diction and boosting. In Proceedings of the Ninth Annual
Conference on Computational Learning Theory (COLT),
pp. 325–332, 1996.

Freund, Y. and Schapire, R. E. A decision-theoretic general-
ization of on-line learning and an application to boosting.
Journal of Computer and System Sciences, 55(1):119–
139, 1997.

Hardt, M., Price, E., and Srebro, N. Equality of opportunity
in supervised learning. In Neural Information Processing
Systems (NIPS), 2016.

Johnson, K. D., Foster, D. P., and Stine, R. A. Impartial pre-
dictive modeling: Ensuring fairness in arbitrary models.
arXiv:1608.00528, 2016.

Kakade, S. M., Sridharan, K., and Tewari, A. On the com-
plexity of linear prediction: Risk bounds, margin bounds,
and regularization. In Advances in neural information
processing systems, pp. 793–800, 2009.

Kamiran, F. and Calders, T. Data preprocessing techniques
for classiﬁcation without discrimination. Knowledge and
Information Systems, 33(1):1–33, 2012.

Kamishima, T., Akaho, S., and Sakuma, J. Fairness-aware
learning through regularization approach. In 2011 IEEE
11th International Conference on Data Mining Work-
shops, pp. 643–650, 2011.

Kearns, M., Neel, S., Roth, A., and Wu, Z. S. Preventing
fairness gerrymandering: Auditing and learning for sub-
group fairness. In Proceedings of the 35th International
Conference on Machine Learning (ICML), 2018.

A Reductions Approach to Fair Classiﬁcation

Kivinen, J. and Warmuth, M. K. Exponentiated gradient
versus gradient descent for linear predictors. Information
and Computation, 132(1):1–63, 1997.

Kleinberg, J., Mullainathan, S., and Raghavan, M. Inherent
trade-offs in the fair determination of risk scores. In Pro-
ceedings of the 8th Innovations in Theoretical Computer
Science Conference, 2017.

Langford, J. and Beygelzimer, A. Sensitive error correct-
In Proceedings of the 18th Annual
ing output codes.
Conference on Learning Theory (COLT), pp. 158–172,
2005.

Ledoux, M. and Talagrand, M. Probability in Banach
Spaces: Isoperimetry and Processes. Springer, 1991.

Lichman, M. UCI machine learning repository, 2013. URL

http://archive.ics.uci.edu/ml.

Menon, A. K. and Williamson, R. C. The cost of fairness in
binary classiﬁcation. In Proceedings of the Conference
on Fiarness, Accountability, and Transparency, 2018.

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-
napeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
Scikit-learn: Machine learning in Python. Journal of
Machine Learning Research, 12:2825–2830, 2011.

Rockafellar, R. T. Convex analysis. Princeton University

Press, 1970.

Shalev-Shwartz, S. Online learning and online convex opti-
mization. Foundations and Trends in Machine Learning,
4(2):107–194, 2012.

Wightman, L. LSAC National Longitudinal Bar Passage

Study, 1998.

Woodworth, B. E., Gunasekar, S., Ohannessian, M. I., and
Srebro, N. Learning non-discriminatory predictors. In
Proceedings of the 30th Conference on Learning Theory
(COLT), pp. 1920–1953, 2017.

Zafar, M. B., Valera, I., Rodriguez, M. G., and Gummadi,
K. P. Fairness constraints: Mechanisms for fair classiﬁca-
tion. In Proceedings of the 20th International Conference
on Artiﬁcial Intelligence and Statistics (AISTATS), pp.
962–970, 2017.

A. Error and Fairness for Randomized Classiﬁers

A Reductions Approach to Fair Classiﬁcation

Let D denote the distribution over triples (X, A, Y ). The accuracy of a classiﬁer h ∈ H is measured by 0-1 error,
err(h) := PD[h(X) (cid:54)= Y ], which for a randomized classiﬁer Q becomes

err(Q) :=

P
(X,A,Y )∼D, h∼Q

[h(X) (cid:54)= Y ] =

Q(h) err(h) .

(cid:88)

h∈H

The fairness constraints on a classiﬁer h are Mµ(h) ≤ c. Recall that µj(h) := ED[gj(X, A, Y, h(X)) | Ej]. For a
randomized classiﬁer Q we deﬁne its moment µj as

µj(Q) :=

E
(X,A,Y )∼D, h∼Q

(cid:104)
gj(X, A, Y, h(X))

(cid:105)

Ej

(cid:12)
(cid:12)
(cid:12)

=

(cid:88)

h∈H

Q(h)µj(h) ,

where the last equality follows because Ej is independent of the choice of h.

B. Proof of Theorem 1

The proof follows immediately from the analysis of Freund & Schapire (1996) applied to the Exponentiated Gradient (EG)
algorithm (Kivinen & Warmuth, 1997), which in our speciﬁc case is also equivalent to Hedge (Freund & Schapire, 1997).
Let Λ := {λ ∈ R|K|
that is equal to λ on coordinates 1 through |K| and puts the remaining mass on the coordinate λ(cid:48)

: (cid:107)λ(cid:48)(cid:107)1 = B}. We associate any λ ∈ Λ with the λ(cid:48) ∈ Λ(cid:48)

+ : (cid:107)λ(cid:48)(cid:107)1 ≤ B} and Λ(cid:48) := {λ(cid:48) ∈ R|K|+1

+

|K|+1.

Consider a run of Algorithm 1. For each λt, let λ(cid:48)
t ∈ R|K|+1 be equal to rt on coordinates 1 through |K| and put zero on the coordinate r(cid:48)
r(cid:48)
associated λ(cid:48), we have, for all t,

t ∈ Λ(cid:48) be the associated element of Λ(cid:48). Let rt := M(cid:98)µ(ht) − (cid:98)c and let
t,|K|+1. Thus, for any λ and the

and, in particular,

λ(cid:62)rt = (λ(cid:48))(cid:62)r(cid:48)

t ,

λ(cid:62)
t

(cid:0)M(cid:98)µ(ht) − (cid:98)c(cid:1) = λ(cid:62)

t rt = (λ(cid:48)

t)(cid:62)r(cid:48)

t .

t as the reward vector for the λ-player. The choices of λ(cid:48)

We interpret r(cid:48)
the learning rate η. By the assumption of the theorem we have (cid:107)r(cid:48)
Corollary 2.14 of Shalev-Shwartz (2012), then states that for any λ(cid:48) ∈ Λ(cid:48),

t then correspond to those of the EG algorithm with
t(cid:107)∞ = (cid:107)rt(cid:107)∞ ≤ ρ. The regret bound for EG, speciﬁcally,

Therefore, by equations (7) and (8), we also have for any λ ∈ Λ,

T
(cid:88)

t=1

(λ(cid:48))(cid:62)r(cid:48)

t ≤

(λ(cid:48)

t)(cid:62)r(cid:48)

t +

B log(|K| + 1)
η

(cid:124)

(cid:123)(cid:122)
=:ζT

+ ηρ2BT

.

(cid:125)

T
(cid:88)

t=1

T
(cid:88)

t=1

λ(cid:62)rt ≤

λ(cid:62)

t rt + ζT .

T
(cid:88)

t=1

(7)

(8)

(9)

This regret bound can be used to bound the suboptimality of L( (cid:98)QT , (cid:98)λT ) in (cid:98)λT as follows:

A Reductions Approach to Fair Classiﬁcation

L( (cid:98)QT , λ) =

T
(cid:88)

(cid:16)

(cid:99)err(ht) + λ(cid:62)(cid:0)M(cid:98)µ(ht) − (cid:98)c(cid:1)(cid:17)

Equation (10) follows from the regret bound (9). Equation (11) follows because L(ht, λt) ≤ L(Q, λt) for all Q by the
choice of ht as the best response of the Q-player. Finally, equation (12) follows by linearity of L(Q, λ) in λ. Thus, we have
for all λ ∈ Λ,

Also, for any Q,

1
T

1
T

1
T

1
T

1
T

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

=

≤

=

≤

(cid:16)
(cid:99)err(ht) + λ(cid:62)rt

(cid:17)

(cid:16)
(cid:99)err(ht) + λ(cid:62)

t rt

(cid:17)

+

ζT
T

L(ht, λt) +

L( (cid:98)QT , λt) +

ζT
T

ζT
T

(cid:16)

= L

(cid:98)QT ,

1
T

T
(cid:88)

t=1

(cid:17)

λt

+

ζT
T

= L( (cid:98)QT , (cid:98)λT ) +

ζT
T

.

L( (cid:98)QT , (cid:98)λT ) ≥ L( (cid:98)QT , λ) −

ζT
T

.

L(Q, (cid:98)λT ) =

L(Q, λt)

1
T

1
T

1
T

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

≥

≥

L(ht, λt)

L(ht, (cid:98)λT ) −

ζT
T

= L( (cid:98)QT , (cid:98)λT ) −

ζT
T

,

L( (cid:98)QT , (cid:98)λT ) ≤ L(Q, (cid:98)λT ) +

ζT
T

.

νT ≤

=

ζT
T

B log(|K| + 1)
ηT

+ ηρ2B ,

(10)

(11)

(12)

(13)

(14)

(15)

(16)

(17)

(18)

where equation (14) follows by linearity of L(Q, λ) in λ, equation (15) follows by the optimality of ht with respect to (cid:98)λt,
equation (16) from the regret bound (9), and equation (17) by linearity of L(Q, λ) in Q. Thus, for all Q,

Equations (13) and (18) immediately imply that for any T ≥ 1,

proving the ﬁrst part of the theorem.

The second part of the theorem follows by plugging in η = ν

2ρ2B and verifying that if T ≥ 4ρ2B2 log(|K|+1)

ν2

then

νT ≤

B log(|K| + 1)
2ρ2B · 4ρ2B2 log(|K|+1)

ν2

ν

+

ν
2ρ2B

· ρ2B =

+

.

ν
2

ν
2

A Reductions Approach to Fair Classiﬁcation

C. Proofs of Theorems 2 and 3

The bulk of this appendix proves the following theorem, which will immediately imply Theorems 2 and 3.

Theorem 4. Let ( (cid:98)Q, (cid:98)λ) be any ν-approximate saddle point of L with

(cid:98)ck = ck + εk

and εk ≥

|Mk,j|

2Rnj (H) +

(cid:32)

(cid:88)

j∈J

(cid:115)

2
√
nj

+

ln(2/δ)
2nj

(cid:33)

.

Let Q(cid:63) minimize err(Q) subject to Mµ(Q) ≤ c. Then with probability at least 1 − (|J| + 1)δ, the distribution (cid:98)Q satisﬁes

err( (cid:98)Q) ≤ err(Q(cid:63)) + 2ν + 4Rn(H) +

(cid:114)

4
√
n

+

2 ln(2/δ)
n

,

and for all k, γk( (cid:98)Q) ≤ ck +

+ 2εk .

1 + 2ν
B

Let Λ := {λ ∈ R|K|
pair ( (cid:98)Q, (cid:98)λ) which is a ν-approximate saddle point of L, i.e.,

+ : (cid:107)λ(cid:48)(cid:107)1 ≤ B} denote the domain of λ. In the remainder of the section, we assume that we are given a

L( (cid:98)Q, (cid:98)λ) ≤ L(Q, (cid:98)λ) + ν

for all Q ∈ ∆,

and L( (cid:98)Q, (cid:98)λ) ≥ L( (cid:98)Q, λ) − ν

for all λ ∈ Λ.

We ﬁrst establish that the pair ( (cid:98)Q, (cid:98)λ) satisﬁes an approximate version of complementary slackness. For the statement and
proof of the following lemma, recall that (cid:98)γ(Q) = M(cid:98)µ(Q), so the empirical fairness constraints can be written as (cid:98)γ(Q) ≤ (cid:98)c
and the Lagrangian L can be written as

(19)

(20)

L(Q, λ) = (cid:99)err(Q) +

λk((cid:98)γk(Q) − (cid:98)ck) .

(cid:88)

k∈K

Lemma 1 (Approximate complementary slackness). The pair ( (cid:98)Q, (cid:98)λ) satisﬁes

(cid:88)

k∈K

(cid:0)
(cid:98)λk((cid:98)γk( (cid:98)Q) − (cid:98)ck) ≥ B max
k∈K

(cid:98)γk( (cid:98)Q) − (cid:98)ck

+ − ν ,

(cid:1)

where we abbreviate x+ = max{x, 0} for any real number x.

Proof. We show that the lemma follows from the optimality conditions (19). We consider a dual variable λ deﬁned as

λ =

(cid:40)

0
Bek(cid:63)

if (cid:98)γ( (cid:98)Q) ≤ (cid:98)c,
otherwise, where k(cid:63) = arg maxk[(cid:98)γk( (cid:98)Q) − (cid:98)ck],

where ek denotes the kth vector of the standard basis. Then we have by equations (19) and (20) that

(cid:99)err( (cid:98)Q) +

(cid:98)λk((cid:98)γk( (cid:98)Q) − (cid:98)ck) = L( (cid:98)Q, (cid:98)λ)

(cid:88)

k∈K

≥ L( (cid:98)Q, λ) − ν = (cid:99)err( (cid:98)Q) +

λk((cid:98)γk( (cid:98)Q) − (cid:98)ck) − ν ,

(cid:88)

k∈K

and the lemma follows by our choice of λ.

Next two lemmas bound the empirical error of (cid:98)Q and also bound the amount by which (cid:98)Q violates the empirical fairness
constraints.
Lemma 2 (Empirical error bound). The distribution (cid:98)Q satisﬁes (cid:99)err( (cid:98)Q) ≤ (cid:99)err(Q) + 2ν for any Q satisfying the empirical
fairness constraints, i.e., any Q such that (cid:98)γ(Q) ≤ (cid:98)c.

A Reductions Approach to Fair Classiﬁcation

Proof. Assume that Q satisﬁes (cid:98)γ(Q) ≤ (cid:98)c. Since (cid:98)λ ≥ 0, we have

L(Q, (cid:98)λ) = (cid:99)err(Q) + (cid:98)λ

(cid:62)(cid:0)

(cid:98)γ(Q) − (cid:98)c(cid:1) ≤ (cid:99)err(Q) .

The optimality conditions (19) imply that

Putting these together, we obtain

L( (cid:98)Q, (cid:98)λ) ≤ L(Q, (cid:98)λ) + ν .

L( (cid:98)Q, (cid:98)λ) ≤ (cid:99)err(Q) + ν .

We next invoke Lemma 1 to lower bound L( (cid:98)Q, (cid:98)λ) as

L( (cid:98)Q, (cid:98)λ) = (cid:99)err( (cid:98)Q) +

(cid:98)λk((cid:98)γk( (cid:98)Q) − (cid:98)ck) ≥ (cid:99)err( (cid:98)Q) + B max
k∈K

(cid:98)γk( (cid:98)Q) − (cid:98)ck

+ − ν

(cid:0)

(cid:1)

(cid:88)

k∈K

≥ (cid:99)err( (cid:98)Q) − ν .

Combining the upper and lower bounds on L( (cid:98)Q, (cid:98)λ) completes the proof.

Lemma 3 (Empirical fairness violation). Assume that the empirical fairness constraints (cid:98)γ(Q) ≤ (cid:98)c are feasible. Then the
distribution (cid:98)Q approximately satisﬁes all empirical fairness constraints:

(cid:16)

max
k∈K

(cid:98)γk( (cid:98)Q) − (cid:98)ck

(cid:17)

≤

1 + 2ν
B

.

Proof. Let Q satisfy (cid:98)γ(Q) ≤ (cid:98)c. Applying the same upper and lower bound on L( (cid:98)Q, (cid:98)λ) as in the proof of Lemma 2, we
obtain

(cid:99)err( (cid:98)Q) + B max
k∈K

(cid:0)

(cid:98)γk( (cid:98)Q) − (cid:98)ck

+ − ν ≤ L( (cid:98)Q, (cid:98)λ) ≤ (cid:99)err(Q) + ν .

(cid:1)

We can further upper bound (cid:99)err(Q) − (cid:99)err( (cid:98)Q) by 1 and use x ≤ x+ for any real number x to complete the proof.

It remains to lift the bounds on empirical classiﬁcation error and constraint violation into the corresponding bounds on true
classiﬁcation error and the violation of true constraints. We will use the standard machinery of uniform convergence bounds
via the (worst-case) Rademacher complexity.
Let F be a class of functions f : Z → [0, 1] over some space Z. Then the (worst-case) Rademacher complexity of F is
deﬁned as

Rn(F) := sup

(cid:34)

E

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

sup
f ∈F

n
(cid:88)

σif (zi)

,

(cid:12)
(cid:35)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

z1,...,zn∈Z
where the expectation is over the i.i.d. random variables σ1, . . . , σn with P[σi = 1] = P[σi = −1] = 1/2.
We ﬁrst prove concentration of generic moments derived from classiﬁers h ∈ H and then move to bounding the deviations
from true classiﬁcation error and true fairness constraints.
Lemma 4 (Concentration of moments). Let g : X × A × {0, 1} × {0, 1} → [0, 1] be any function and let D be a distribution
over (X, A, Y ). Then with probability at least 1 − δ, for all h ∈ H,

i=1

(cid:12)
(cid:12)

(cid:12)(cid:98)E(cid:2)g(X, A, Y, h(X))(cid:3) − E(cid:2)g(X, A, Y, h(X))(cid:3)(cid:12)

(cid:12) ≤ 2Rn(H) +
(cid:12)

(cid:114)

2
√
n

+

ln(2/δ)
2n

,

where the expectation is with respect to D and the empirical expectation is based on n i.i.d. draws from D.

Proof. Let F := {fh}h∈H be the class of functions fh : (x, y, a) (cid:55)→ g(cid:0)x, y, a, h(x)(cid:1). By Theorem 3.2 of Boucheron et al.
(2005), we then have with probability at least 1 − δ, for all h,

(cid:12)
(cid:12)

(cid:12)(cid:98)E(cid:2)g(X, A, Y, h(X))(cid:3) − E(cid:2)g(X, A, Y, h(X))(cid:3)(cid:12)
(cid:12)
(cid:12) =

(cid:12)
(cid:12)
(cid:12) ≤ 2Rn(F) +
(cid:12)(cid:98)E[fh] − E[fh]
(cid:12)
(cid:12)

(cid:114)

ln(2/δ)
2n

.

(21)

We will next bound Rn(F) in terms of Rn(H). Since h(x) ∈ {0, 1}, we can write

A Reductions Approach to Fair Classiﬁcation

fh(x, y, a) = h(x)g(x, a, y, 1) +

1 − h(x)

g(x, a, y, 0) = g(x, a, y, 0) + h(x)

g(x, a, y, 1) − g(x, a, y, 0)

.

(cid:16)

(cid:17)

(cid:16)

(cid:17)

(cid:12)g(x, a, y, 0)(cid:12)

Since (cid:12)
(cid:12) ≤ 1, we can invoke Theorem 12(5) of Bartlett & Mendelson
(2002) for bounding function classes shifted by an offset, in our case g(x, a, y, 0), and Theorem 4.4 of Ledoux & Talagrand
(1991) for bounding function classes under contraction, in our case g(x, a, y, 1) − g(x, a, y, 0), yielding

(cid:12)g(x, a, y, 1) − g(x, a, y, 0)(cid:12)

(cid:12) ≤ 1 and (cid:12)

Rn(F) ≤

+ Rn(H) .

1
√
n

Together with the bound (21), this proves the lemma.

Lemma 5 (Concentration of loss). With probability at least 1 − δ, for all Q ∈ ∆,

|(cid:99)err(Q) − err(Q)| ≤ 2Rn(H) +

(cid:114)

2
√
n

+

ln(2/δ)
2n

.

Proof. We ﬁrst use Lemma 4 with g : (x, a, y, ˆy) (cid:55)→ 1{y (cid:54)= ˆy} to obtain, with probability 1 − δ, for all h,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) =
(cid:12)(cid:99)err(h) − err(h)

(cid:12)
(cid:12)(cid:98)E[fh] − E[fh]
(cid:12)

(cid:12)
(cid:12) ≤ 2Rn(H) +
(cid:12)

(cid:114)

2
√
n

+

ln(2/δ)
2n

.

The lemma now follows for any Q by taking a convex combination of the corresponding bounds on h ∈ H.7

Finally, we show a result for the concentration of the empirical constraint violations to their population counterparts. We
will actually show the concentration of the individual moments (cid:98)µj(Q) to µj(Q) uniformly for all Q ∈ ∆. Since M is
a ﬁxed matrix not dependent on the data, this also directly implies concentration of the constraints (cid:98)γ(Q) = M(cid:98)µ(Q) to
γ(Q) = Mµ(Q). For this result, recall that nj = |{i ∈ [n] : (Xi,Ai,Yi) ∈ Ej}| and p(cid:63)
Lemma 6 (Concentration of conditional moments). For any j ∈ J, with probability at least 1 − δ, for all Q,

j = P[Ej].

(cid:12)(cid:98)µj(Q) − µj(Q)(cid:12)
(cid:12)

(cid:12) ≤ 2Rnj (H) +

(cid:115)

2
√
nj

+

ln(2/δ)
2nj

.

If np(cid:63)

j ≥ 8 log(2/δ), then with probability at least 1 − δ, for all Q,

(cid:12)(cid:98)µj(Q) − µj(Q)(cid:12)
(cid:12)

(cid:12) ≤ 2Rnp(cid:63)

j /2(H) + 2

(cid:115)

(cid:115)

2
np(cid:63)
j

+

ln(4/δ)
np(cid:63)
j

.

Proof. Our proof largely follows the proof of Lemma 2 of Woodworth et al. (2017), with appropriate modiﬁcations for our
more general constraint deﬁnition. Let Sj := {i ∈ [n] : (Xi,Ai,Yi) ∈ Ej} be the set of indices such that the corresponding
examples fall in the event Ej. Note that we have deﬁned nj = |Sj|. Let D(·) denote the joint distribution of (X, A, Y ).
Then, conditioned on i ∈ Sj, the random variables gj(Xi,Ai,Yi,h(Xi)) are i.i.d. draws from the distribution D(· | Ej), with
mean µj(h). Applying Lemma 4 with gj and the distribution D(· | Ej) therefore yields, with probability 1 − δ, for all h,

(cid:12)(cid:98)µj(h) − µj(h)(cid:12)
(cid:12)

(cid:12) ≤ 2Rnj (H) +

(cid:115)

2
√
nj

+

ln(2/δ)
2nj

,

The lemma now follows by taking a convex combination over h.

7The same reasoning applies for general error, err(h) = E[gerr(X,A,Y,h(X))], by using g = gerr in Lemma 4.

A Reductions Approach to Fair Classiﬁcation

Proof of Theorem 4. We now use the lemmas derives so far to prove Theorem 4. We ﬁrst use Lemma 6 to bound the gap
between the empirical and population fairness constraints. The lemma implies that with probability at least 1 − |J|δ, for all
k ∈ K and all Q ∈ ∆,

(cid:12)(cid:98)γk(Q) − γk(Q)(cid:12)
(cid:12)

(cid:12) =

(cid:16)
(cid:98)µ(Q) − µ(Q)

(cid:17)(cid:12)
(cid:12)
(cid:12)

≤

(cid:12)
(cid:12)
(cid:12)(cid:98)µj(Q) − µj(Q)
|Mk,j|

(cid:12)
(cid:12)
(cid:12)

(cid:32)

≤

|Mk,j|

2Rnj (H) +

(cid:115)

(cid:33)

2
√
nj

+

ln(2/δ)
2nj

(cid:12)
(cid:12)
(cid:12)Mk
(cid:88)

j∈J

(cid:88)

j∈J
≤ εk .

Note that our choice of (cid:98)c along with equation (22) ensure that (cid:98)γk(Q(cid:63)) ≤ (cid:98)ck for all k ∈ K. Using Lemma 2 allows us to
conclude that

(cid:99)err( (cid:98)Q) ≤ (cid:99)err(Q(cid:63)) + 2ν .

We now invoke Lemma 5 twice, once for (cid:99)err( (cid:98)Q) and once for (cid:99)err(Q(cid:63)), proving the ﬁrst statement of the theorem.
The above shows that Q(cid:63) satisﬁes the empirical fairness constraints, so we can use Lemma 3, which together with
equation (22) yields

γk( (cid:98)Q) ≤ (cid:98)γk( (cid:98)Q) + εk ≤ (cid:98)ck +

+ εk = ck +

+ 2εk ,

1 + 2ν
B

1 + 2ν
B

(22)

proving the second statement of the theorem.

We are now ready to prove Theorems 2 and 3

Proof of Theorem 2. The ﬁrst part of the theorem follows immediately from Assumption 1 and Theorem 4 (with δ/2 instead
of δ). The statement in fact holds with probability at least 1 − (|J| + 1)δ/2. For the second part, we use the multiplicative
Chernoff bound for binomial random variables. Note that E[nj] = np(cid:63)
j , and we assume that np(cid:63)
j ≥ 8 ln(2/δ), so the
multiplicative Chernoff bound implies that nj ≤ np(cid:63)
j /2 with probability at most δ/2. Taking the union bound across all j
and combining with the ﬁrst part of the theorem then proves the second part.

Proof of Theorem 3. This follows immediately from Theorem 1 and the ﬁrst part of Theorem 2.

D. Additional Experimental Results

In this appendix we present more complete experimental results. We present experimental results for both the training and
test data. We evaluate the exponentiated-gradient as well as the grid-search variants of our reductions. And, ﬁnally, we
consider extensions of reweighting and relabeling beyond the speciﬁc tradeoffs proposed by Kamiran & Calders (2012).
Speciﬁcally, we introduce a scaling parameter that interpolates between the prescribed tradeoff (speciﬁc importance weights
or the number of examples to relabel) and the unconstrained classiﬁer (uniform weights or zero examples to relabel). The
training data results are shown in Figure 2. The test set results are shown in Figure 3.

A Reductions Approach to Fair Classiﬁcation

Figure 2. Training classiﬁcation error versus constraint violation, with respect to DP (top two rows) and EO (bottom two rows). Markers
correspond to the baselines. For our two reductions and the interpolants between reweighting (or relabeling) and the unconstrained
classiﬁer, we varied their tradeoff parameters and plot the Pareto frontiers of the sets of classiﬁers obtained for each method. Because the
curves of the different methods often overlap, we use vertical dashed lines to indicate the lowest constraint violations. All data sets have
binary protected attributes except for adult4, which has four protected attribute values, so relabeling is not applicable and grid search is
not feasible for this data set. The exponentiated-gradient reduction dominates or matches other approaches as expected since it solves
exactly for the points on the Pareto frontier of the set of all classiﬁers in each considered class.

A Reductions Approach to Fair Classiﬁcation

Figure 3. Test classiﬁcation error versus constraint violation, with respect to DP (top two rows) and EO (bottom two rows). Markers
correspond to the baselines. For our two reductions and the interpolants between reweighting (or relabeling) and the unconstrained
classiﬁer, we show convex envelopes of the classiﬁers taken from the training Pareto frontier of each method (i.e., the same classiﬁers as
shown in Figure 2). Because the curves of the different methods often overlap, we use vertical dashed lines to indicate the lowest constraint
violations. All data sets have binary protected attributes except for adult4, which has four protected attribute values, so relabeling
is not applicable and grid search is not feasible for this data set. We show 95% conﬁdence bands for the classiﬁcation error of the
exponentiated-gradient reduction and 95% conﬁdence intervals for the constraint violation of post-processing. The exponentiated-gradient
reduction dominates or matches performance of all other methods up to statistical uncertainty.

