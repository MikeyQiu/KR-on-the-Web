Few-Shot Charge Prediction with Discriminative Legal Attributes

Zikun Hu∗ Xiang Li∗ Cunchao Tu Zhiyuan Liu† Maosong Sun
Department of Computer Science and Technology, Tsinghua University
State Key Lab on Intelligent Technology and Systems, Tsinghua University
Beijing National Research Center for Information Science and Technology
{hzk14, x-l15}@mails.tsinghua.edu.cn
tucunchao@gmail.com, {liuzy, sms}@tsinghua.edu.cn

Abstract

Automatic charge prediction aims to predict the ﬁnal charges according to the fact descriptions
in criminal cases and plays a crucial role in legal assistant systems. Existing works on charge
prediction perform adequately on those high-frequency charges but are not yet capable of pre-
dicting few-shot charges with limited cases. Moreover, these exist many confusing charge pairs,
whose fact descriptions are fairly similar to each other. To address these issues, we introduce
several discriminative attributes of charges as the internal mapping between fact descriptions and
charges. These attributes provide additional information for few-shot charges, as well as effective
signals for distinguishing confusing charges. More speciﬁcally, we propose an attribute-attentive
charge prediction model to infer the attributes and charges simultaneously. Experimental results
on real-work datasets demonstrate that our proposed model achieves signiﬁcant and consistent
improvements than other state-of-the-art baselines. Speciﬁcally, our model outperforms other
baselines by more than 50% in the few-shot scenario. Our codes and datasets can be obtained
from https://github.com/thunlp/attribute_charge.

1

Introduction

The task of automatic charge prediction aims to train a machine judge to determine the ﬁnal charges (e.g.,
theft, robbery or trafﬁc offence.) of the defendants in criminal cases. As a representative subtask of legal
judgment prediction, charge prediction plays an important role in legal assistant systems and can beneﬁt
many real-world applications. For example, it can provide a handy reference for legal experts (e.g.,
lawyers and judges) and improve their working efﬁciency. Meanwhile, it can supply ordinary people
who are unfamiliar with legal terminology and complex procedures with legal consulting.

As a typical task in legal intelligence, automatic charge prediction has been studied for decades and
most existing works formalize this task under the text classiﬁcation framework. At the early stage,
researchers pay great efforts to extract efﬁcient features from text or case proﬁles. For example, some
works (Liu et al., 2004; Liu and Hsieh, 2006) utilize shallow textual features, including characters,
words, and phrases, to predict charges. Katz et al. (2017) predict the US Supreme Court’s decisions
with efﬁcient features extracted from case proﬁles (e.g., dates, locations, terms, and types). All these
approaches require numerous human effort to design features and annotate training instances. Besides,
Inspired by the successful usage of deep neural
these methods are hard to scale to other scenarios.
networks on natural language processing tasks (Kim, 2014; Baharudin et al., 2010; Tang et al., 2015),
researchers propose to employ deep neural networks to model legal documents. For example, Luo et al.
(2017) propose an attention-based neural network for charge prediction by incorporating the relevant law
articles.

However, charge prediction is still confronted with two major challenges which make it non-trivial:

∗ Indicates equal contribution.
† Corresponding Author.

This work is licensed under a Creative Commons Attribution 4.0 International License.
creativecommons.org/licenses/by/4.0/.

License details: http://

Figure 1: An illustration of the attribute-based charge prediction.

Few-Shot Charges. In practice, the case numbers of various charges are highly imbalanced. Accord-
ing to our statistics on a real-world dataset, the most frequent 10 charges (e.g., theft, intentional injury,
and trafﬁc offence.) cover 78.1% cases. On the contrary, the most low-frequency 50 (e.g., scalping relics,
disrupting the order of the court, and tax-escaping.) charges only cover less than 0.5% cases and most
of these charges own only around ten cases correspondingly. Previous works usually focus on these
common charges and ignore the few-shot ones. Though deep neural models advance feature-engineering
based charge prediction methods, they are unable to handle few-shot charges well due to the requirement
of sufﬁcient training data. Therefore, how to deal with these charges with limited cases is critical to a
robust and effective charge prediction system.

Confusing Charges. Besides few-shot charges, there also exist many confusing charge pairs, such as
(theft, robbery) and (misappropriation of funds, embezzlement). For each confusing pair, the deﬁnitions
of two charges only differ in the veriﬁcation of a speciﬁc act and the circumstances in corresponding
cases are usually similar to each other. As illustrated in Fig. 1, many robbery case also contain the act of
theft, and the existence of violence is the only key factor to distinguish these two charges. Thus, how to
capture the crucial factors for distinguishing confusing charges is another challenge of charge prediction.
To address these issues, we propose to introduce discriminative legal attributes of charges into consid-
eration and take these attributes as the internal mapping between facts and charge. More speciﬁcally, we
select 10 representative attributes of charges, including violence, proﬁt purpose, buying and selling and
so on. Afterwards, we conduct a low-cost category-level annotation, i.e., for each charge, we annotate
the value (including yes, no, or not available) of each attribute. This annotation indicates if an attribute
is the essential condition of a charge.

With the attribute annotation of charges, we propose a novel multi-task learning framework to predict
In this model, we employ attribute attention
the attributes and charges of each case simultaneously.
mechanism to capture the critical factual information relevant to a speciﬁc attribute. After that, we
combine these attribute-aware representations with an attribute-free fact representation to predict the
ﬁnal charges. There are two reasons for introducing legal attributes into our charge prediction model. On
one hand, these attributes can provide explicit knowledge about how to distinguish confusing charges.
On the other hand, these attributes are shared by all charges, and the knowledge can transfer from high-
frequency charges to low-frequency ones. Even for the few-shot charges, we can learn an efﬁcient
attribute-aware representation for prediction.

To investigate the advantage of our model on handling few-shot and confusing charges, we conduct
experiments on three real-world datasets of Chinese criminal cases. Experimental results demonstrate
that our model signiﬁcantly and consistently outperforms other state-of-the-art models on all datasets
and evaluation metrics. It is worth noting that, our model outperforms other baselines by more than 50%
for the few-shot charges.

To summarize, we make three main contributions as follows:
(1) We are the ﬁrst to focus on the few-shot and confusing problems in charge prediction. To address

these issues, we introduce legal attributes of charges into charge prediction task for the ﬁrst time.

(2) We propose a novel multi-task learning framework to infer the attributes and charges of a case
jointly. To achieve it, we employ attribute attention mechanism to learn attribute-aware fact representa-

tions.

(3) We conduct efﬁcient experiments on several real-world datasets, and our model signiﬁcantly out-

performs other baselines and achieves more than 50% improvements for few-shot charges.

2 Related Work

2.1 Zero-Shot Classiﬁcation

Our work is relevant to zero-shot classiﬁcation in computer vision. Many attribute-based models have
been proposed under this task since attributes are shared among different classes and can offer an in-
termediate representation. Lampert et al. (2014) introduces direct attribute prediction(DAP) and indirect
attribute prediction(IAP), and proposes attribute classiﬁers which can be pre-trained and dont need re-
training when ﬁnding new suitable object class. Akata et al. (2013) proposes to transform the task of
Jayaraman and Grauman (2014) introduces
attribute-based classiﬁcation to the label-embedding task.
a random forest method stressing the unreliability of attribute prediction for unseen classes. They also
extend it to the few-shot scenario.

Other than attributes, other external information can also be introduced to promote zero-shot classi-
ﬁcation. Elhoseiny et al. (2014) makes use of text description of the class label to transfer knowledge
between text features and visual features. Zero-shot learning has also been used in applications besides
object recognition, such as activity recognition (Zellers and Choi, 2017) and event recognition (Wu et
al., 2014).

2.2 Charge Prediction

Researchers in the legal area have been working on automatically making the legal judgment for a long
time. Kort (1957) applies quantitative methods to predict judgment by calculation numerical values for
factual elements. Nagel (1963) makes use of correlation analysis to make predictions for reapportioning
cases. Keown (1980) introduced mathematical models used for legal prediction such as linear models
and the scheme of nearest neighbors. These methods are usually mathematical or quantitative, and they
are restricted to a small dataset with few labels.

Since machine learning has been proven successful in many areas, researchers begin to formalize
charge prediction as a text classiﬁcation problem and make use of machine learning methods. Such
work usually focuses on feature extraction from the case fact. Lin et al. (2012) fetches 21 legal factor
labels for case classiﬁcation. Mackaay and Robillard (1974) extracts N-grams and topics created by
clustering semantically similar N-grams as features. Sulea et al. (2017) proposes a system based on
SVM ensembles using the case description, ruling and time span of a case as input. However, these
methods only extract shallow text features or manual labels which are hard to gather on a larger dataset.
Whats more, the conventional models could not catch the subtle difference between similar crimes, thus
they wouldnt perform well when the number of classes increases and more similar crimes appear.

With the successful usage of neural network methods on speech (Mikolov et al., 2011; Hinton et al.,
2012; Dahl et al., 2012; Sainath et al., 2013), computer vision (CV) (Krizhevsky et al., 2012; Farabet et
al., 2013; Tompson et al., 2014; Szegedy et al., 2015) and natural language processing (NLP) (Collobert
et al., 2011; Kim, 2014; Bordes et al., 2014; Sutskever et al., 2014; Jean et al., 2015; Yang et al., 2016),
researchers propose to employ neural models for legal tasks. Luo et al. (2017) proposes a hierarchical
attentional network to predict charges and extract relevant articles jointly. However, this work only
focuses on high-frequency charges, without paying attention to few-shot and confusing ones. To address
these issues, we propose an attention-based neural model by incorporating several discriminative legal
attributes.

3 Method

In this section, we propose a few-shot neural model which jointly models charge prediction task and legal
attribute prediction task in a uniﬁed framework. In the following parts, we ﬁrst introduce the discrimina-
tive charge attributes. Afterward, we give deﬁnitions of charge prediction and attribute prediction. Then

we describe the neural encoder of fact description and the attention-based attribute predictor. At last, we
show the output layer and the loss function of our model.

3.1 Discriminative Charge Attributes

To distinguish confusing charges and provide additional knowledge for few-shot charges, we introduce
10 discriminative attributes for all the charges in Chinese criminal law. The detailed descriptions of these
attributes are shown in Table 1. For each (charge, attribute) pair, it can be labeled as Yes, No or NA. For
example, the charge of manslaughter should be labeled as No on Intentional Crime, Yes on Death, NA
on State Organ. Note that, the fact-ﬁndings of a speciﬁc case can only be labeled as Yes or No. When
convicting someone of a certain crime, the facts should conform to the description of the certain charge.
Thus for a certain attribute, the label of a speciﬁc case and the label of the corresponding charge should
be the same or not in conﬂict. In other words, for a certain attribute, the label of a case and the charge
can only be (Yes, Yes), (No, No), (Yes, NA), or (No, NA). In practice, we conduct a low-cost annotation
and annotate the attributes of 149 distinct charges manually. Then, we assign each case with the same
attributes of its corresponding charge.

Attributes

Description

Proﬁt Purpose

Whether the criminal commits a crime on the purpose of getting proﬁt.

Buying and Selling Whether the criminal has buying or selling behavior during the commission of the

crime.

Whether death is caused by the criminal.

Whether the criminal has the act of violence.

Death

Violence

State Organ

Public Place

Whether the case or the charge involves State organ or any functionary of a State organ.

Whether the criminal commits a crime in a public place.

Illegal Possession

Whether the criminal commits a crime for the purpose of illegal possession.

Physical Injury

Whether a physical injury is caused by the criminal.

Intentional Crime Whether the criminal commits an intentional crime.

Production

Whether the criminal commits a crime during the production.

Table 1: The descriptions of selected attributes.

3.2 Formalizations

3.2.1 Charge Prediction
The fact description of a case can be seen as a word sequence x = {x1, . . . , xn}, where n represents the
sequence length, xi ∈ T , and T is a ﬁxed vocabulary. Given the fact description x, the charge prediction
task aims to predict a charge y ∈ Y from a charge set Y .

3.2.2 Attributes Prediction
The attributes prediction task can be regarded as a binary classiﬁcation task. It takes the same input
sequence x as in the charge prediction task, and aims to predict the fact-ﬁndings of attributes p =
{p1, . . . , pk} according to the fact. Here, k is the number of selected attributes, and pi ∈ {0, 1} is the
label for a certain attribute.

3.3 Fact Encoder

As illustrated in Fig. 2, fact encoder encodes the discrete input sequence into continuous hidden states.
Here, we employ conventional Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997)
as fact encoder due to its ability to extract semantic meanings. LSTM is a variation of RNN and is
capable of capturing long-term dependencies.

First, LSTM encoder converts each word xi ∈ x into its word embedding xi ∈ Rd, where d is
the dimension of word embeddings. Then, we get the corresponding word embedding sequence as

Figure 2: An illustration of the attribute-based charge prediction.

ˆx = {x1, . . . , xn}.

hidden state ht as follows:

At each time step t ∈ [1, n], the LSTM cell intakes xt, recalculates memory cell ct, and outputs new

ft = σ (Wf xt + Uf ht−1 + bf ) ,
it = σ (Wixt + Uiht−1 + bi) ,
ot = σ (Woxt + Uoht−1 + bo) ,
ˆct = tanh (Wcxt + Ucht−1 + bc) ,
ct = ft (cid:12) ct−1 + it (cid:12) ˆct,
ht = ot (cid:12) tanh (ct) .

Here, ft, it and ot represent forget gate, input gate, and output gate respectively. (cid:12) means element-wise
multiplication and σ is the sigmoid activation function. W , U , and b are weight matrices and bias vectors.
After processing all time steps, we get a hidden state sequence h = {h1, . . . , hn}. At last, we feed it
into a max-pooling layer to get the attribute-free representation e = [e1, . . . , es] as

ei = max(h1,i, · · · , hn,i), ∀i ∈ [1, s].

Here, s is the dimension of hidden states.

3.4 Attentive Attribute Predictor

Given the fact description x, the attribute predictor aims to predict the label of every attribute. Inspired
by (Yang et al., 2016), we employ an attention mechanism to select relevant information from facts and
generate attribute-aware fact representations.

As shown in Fig. 2, attribute predictor takes the hidden state sequence h = {h1, . . . , hn} as input. Our
attentive attribute predictor then calculates attention weights a = {a1, . . . , ak} for all attribute, where
ai = [ai,1, . . . , ai,n]. ∀i ∈ [1, k] and j ∈ [1, n], ai,j is calculated by:

ai,j =

exp(tanh(Wahj)T ui)
t exp(tanh(Waht)T ui)

.

(cid:80)

Here, ui is the context vector of the i-th attribute to calculate how informative an element is to the
attribute i, and Wa is a weight matrix that all attributes share. Afterwards, we get attribute-aware
representations of fact g = {g1, . . . , gk}, where gi = (cid:80)
t ai,tht. At last, with representations g, we
project it into the label space and use softmax function to get the ﬁnal prediction results p = [p1, . . . , pk],
where pi is the prediction result of attribute i and is calculated by:
i gi + bp
i ),

zi = softmax(Wp
pi = arg max(zi).

(1)

(2)

(3)

(4)

Here, zi is the prediction probability distribution on Yes and No. Wp
vector of attribute i.

i and bi are weight matrix and bias

3.5 Output Layer

To integrate the fact descriptions and fact-ﬁndings of all attributes, we use both attribute-free and
attribute-aware representations to predict the ﬁnal charge of a case in the output layer. The predicted
distribution y over all charges is calculated as follows:

r =

(cid:80)

i gi
k

,

v = e ⊕ r,
y = softmax(Wyv + by).

(5)

(6)

(7)

(8)

Here, r is the average of attribute-aware representations. r and e are concatenated into the ﬁnal fact
representation v. Wy and by are weight matrix and bias vector in the output layer.

3.6 Optimization

The training objective of our proposed model consists of two parts. The ﬁrst one is to minimize the
cross-entropy between predicted charge distribution y and the ground-truth distribution ˆy. The other one
is to minimize the cross-entropy between predicted distribution and the ground-truth fact-ﬁnding of each
attribute.

The charge prediction loss can be formalized as:

where yi is the ground-truth label, ˆyi is prediction probability, and C is the number of charges.

As each attribute is equally important in the model, we can easily calculate the attribution loss by sum
up the cross-entropy of all attributes. However, when the attribute of a speciﬁc charge is NA, the label of
the corresponding cases can be Yes or No. Therefore, we only add up the cross-entropy to the attribute
loss when this attribute of the charge belongs to Yes or No. At last, we formulate the attribute loss as:

Lcharge = −

yi · log(ˆyi),

C
(cid:88)

i=1

Lattr = −

zij · log(ˆzij),

k
(cid:88)

2
(cid:88)

Ii

i=1

j=1

where Ii is an indicator function. Ii = 1 if the i-th attribute of current charge is labeled as Yes or No, and
Ii = 0 otherwise. Obviously, zi is the ground-truth label, and ˆzi is predicted probabilities distribution on
Yes and No.

Considering the two objectives, our ﬁnal loss function L is obtained by adding Lcharge and Lattr as

where α is a hyper-parameter to balance the weight of the two parts in the loss function.

L = Lcharge + α · Lattr,

follows:

4 Experiments

In order to investigate the effectiveness of our model on criminal charges prediction, we conduct experi-
ments on several real-world datasets and compare our model with several state-of-the-art baselines.

4.1 Dataset Construction

Since there are no publicly available datasets in previous works for charges prediction, we collect crim-
inal cases published by the Chinese government from China Judgments Online1. As each case is well-
structured and divided into several parts such as fact, court view, and penalty result, we select the fact

1http://wenshu.court.gov.cn.

part of each case as our input. Besides, we can easily extract the charges from the penalty result by
regular expression. We have manually checked the extracted charges and there are few mistakes.

Although there are some cases that contain multiple defendants and multiple charges in real-world,
considering the task would be too complex to solve if these cases contained, we removed the cases
which have more than one charges in a verdict. Besides, in order to examine the performance of our
method on few-shot charges, we keep 149 distinct charges (near 3 times as compared with (Luo et al.,
2017)) with at least 10 cases.

After preprocessing, we randomly select about 400, 000 cases and construct three datasets with dif-
ferent scales, denoted as Criminal-S(small), Criminal-M(medium) and Criminal-L(large). The three
different datasets contain the same number of charges but the different number of cases. The detailed
statistics are shown in Table 2.

Datasets Criminal-S Criminal-M Criminal-L

train
test
valid

61, 589
7, 702
7, 755

153, 521
19, 189
19, 250

306, 900
38, 368
38, 429

Table 2: The statistics of different datasets.

4.2 Attribute Selection and Annotation

As mentioned in previous part, we propose to introduce discriminative attributes to enhance charge pre-
diction. To select these attributes, we ﬁrst train a LSTM based charge prediction model and obtain the
confusion matrix of predicted charges on validation set. Then, we ﬁlter out the confusing charge pairs
and provide them to three master students majoring in criminal. According to these confusing charge
pairs, they deﬁne 10 representative attributes to distinguish these confusing pairs.

With the selected 10 attributes, we conduct a low-cost annotation over all charges. Here, the low-cost
annotation means we only need to annotate 10 attributes for 149 charges manually, rather than all cases.
As the selected attributes are discriminative and unambiguous, we asked these annotators to reach an
agreement for each annotation. Totally, we spent less than 10 hours for annotation.

4.3 Baselines

We employ several typical text classiﬁcation models and one charge predicting model as baselines:

TFIDF+SVM: We implement term-frequency inverse document frequency (TFIDF) (Salton and
Buckley, 1988) to extract features of inputs, and employ SVM (Suykens and Vandewalle, 1999) as the
classiﬁer.

CNN: We implement the CNN with multiple ﬁlter widths (Kim, 2014) as text classiﬁer.
LSTM: We implement a two-layer LSTM (Hochreiter and Schmidhuber, 1997) with a max-pooling

layer as the fact encoder.

Fact-Law Attention Model: Luo et al. (2017) propose an attention-based neural charge prediction

model by incorporating relevant law articles.

4.4 Experiment Settings and Evaluation Metrics

As all the case documents are written in Chinese without word cutting, we employ THULAC (Sun et
al., 2016) for word segmentation and set the maximum document length to 500. For the TFIDF+SVM
model, we set the feature size to 2, 000. For other neural models, we employ Skip-Gram model (Mikolov
et al., 2013) to pre-train word embeddings with the embedding size of 100. We set the hidden state size
of LSTM to 100.For the CNN based models, we set the ﬁlter widths to (2, 3, 4, 5) with each ﬁlter size to
25 for consistency. The weight α of the attribute loss is set to 1.

Note that, the representation size of our model turns into 200 after concatenation. For a fair com-
parison, we add a 100 × 200 fully connected layer between after the pooling layer in CNN and LSTM,

denoted as CNN-200 and LSTM-200.

We use Adam (Kingma and Ba, 2015) as the optimizer, and set the learning rate to 0.001, the dropout
rate (Srivastava et al., 2014) to 0.5 and the batch size to 64. We employ accuracy (Acc.), macro-precision
(MP), macro-recall (MR) and macro-F1 as our evaluation metrics.

4.5 Results and Analysis

Datasets

Criminal-S

Criminal-M

Criminal-L

Metrics

Acc. MP MR

F1

Acc. MP MR

F1

Acc. MP MR

F1

TFIDF+SVM 85.8
91.9
92.6
93.5
92.7
92.8

CNN
CNN-200
LSTM
LSTM-200
Fact-Law Att.

Our Model

93.4

49.7
50.5
51.1
59.4
60.0
57.0

66.7

41.9
44.9
46.3
58.6
58.4
53.9

69.2

43.5
46.1
47.3
57.3
57.0
53.4

64.9

89.6
93.5
92.8
94.7
94.4
94.7

94.4

58.8
57.6
56.2
65.8
66.5
66.7

68.3

5 0.1
48.1
50.0
63.0
62.4
60.4

69.2

52.1
50.5
50.8
62.6
62.7
61.8

67.1

91.8
93.9
94.1
95.5
95.1
95.7

95.8

67.5
66.0
61.9
69.8
72.8
73.3

75.8

54.1
50.3
50.0
67.0
66.7
67.1

73.7

57.5
54.7
53.1
66.8
67.9
68.6

73.1

Table 3: Charge prediction results of three datasets.

As shown in Table 3, we can observe that our model signiﬁcantly and consistently outperforms all
the baselines. Almost all existing methods perform poorly under the macro-F1 metric, which indicates
their shortage of predicting few-shot charges. Conversely, our model achieves promising improvements
(7.9%, 4.4%, and 5.2% absolutely on three datasets respectively), which demonstrates the robustness
and effectiveness of our model.

To further verify the advance of our model on dealing with few-shot charges, we show the performance
on charges with different frequencies. As shown in Table 4, we divide the charges into three parts
according to their frequencies. Here, the charges with ≤ 10 cases are low-frequency, and the charges
with > 100 cases are high-frequency. From this table, we ﬁnd that our model achieves more than 50%
improvements than baseline method for the low-frequency (i.e., few-shot) charges, which veriﬁes the
effectiveness of our model on handling few-shot issues.

Charge Type

Low frequency Medium frequency High frequency

Charge Number

49

51

49

LSTM-200
Our Model

32.6
49.7 (↑ 17.1%)

55.0
60.0 (↑ 5.0%)

83.3
85.2 (↑ 1.9%)

Table 4: Macro-F1 values of various charges on Criminal-S.

Datasets

Metrics

Our model

Criminal-S

Criminal-M

Criminal-L

Acc. MP MR

F1

Acc. MP MR

F1

Acc. MP MR

F1

w/o attention
w/o concatenation

93.4

93.5
93.5

66.7

63.4
59.3

69.2

60.1
59.0

64.9

60.0
57.2

94.4

94.7
95.0

68.3

68.8
64.6

69.2

58.2
62.4

67.1

60.9
62.5

95.8

94.9
95.7

75.8

70.9
69.4

73.7

54.4
64.5

73.1

58.6
65.4

Table 5: Experimental results of ablation test.

4.6 Ablation Test

Our method is characterized by the incorporation of attention mechanism and attribute-aware representa-
tions. Thus, we design ablation test respectively to investigate the effectiveness of these modules. When
taken off the attention mechanism, for each attribute we replace attention mechanism with a fully con-
nected layer. When taken off the attribute-aware representations (i.e., without concatenating the averaged

Task

Charge Prediction Attribute Prediction on physical injury

Ground Truth

Intentional Injury

Our model

Intentional Injury

LSTM-200

Affray

Yes

Yes

N/A

Table 6: Charge and attribute prediction result of the selected case.

attribute-aware representation), our method degrades into a typical multi-task learning based on LSTM
for both charge and attribute prediction.

As shown in Table 5, we can observe that the performance drops obviously after removing the attention
layer or the concatenation. The macro-F1 decreases at least 4%. Therefore, it can be seen that both
attention mechanism and attribute-aware fact representation play irreplaceable roles in our model.

4.7 Case Study

In this part, we select a representative case to give an intuitive illustration of how the predicted attributes
help to promote the performance of charge prediction. In this case, the defendant is convicted of inten-
tional injury. It is often hard to decide whether to judge a case as affray or intentional injury since they
are both related to violence. One important difference between them is that intentional injury has the
feature of physical injury, while affray does not.

So we believe the attribute physical injury is essential in the charge prediction of this case. As shown
in Table 6, our model correctly predicts the label of physical injury as Yes, and consequently predicts
the charge as intentional injury. In contrary, the model LSTM-200 predicts it as affray incorrectly. In
addition, we visual the heat map of this case when predicting the attribute intentionalinjury. Words
with deeper background color have higher attention weights. From this ﬁgure, we observe that the
attention mechanism can capture key patterns and semantics relevant to current attribute.

Figure 3: Visualization of attention mechanism.

.

5 Conclusion

In this work, we focus on the task of charge prediction according to the fact descriptions of criminal cases.
To address the problem of prediction few-shot and confusing charges, we introduce discriminative legal

attributes into consideration and propose a novel attribute-based multi-task learning model for charge
prediction. Speciﬁcally, our model learns attribute-free and attribute-aware fact representation jointly by
utilizing attribute-based attention mechanism.

In future, we will explore the following directions:
(1) There are more complicated criminal cases, such as multiple defendants and charges. Thus, it is

challenging to handle this general form of charge prediction.

(2) In this work, we only utilize several simple attributes of charges, while there exist more complex
essential conditions of charges. How to take full usage of essential conditions of charges is expected to
improve the interpretability of charge prediction models.

Acknowledgements

We thank all the anonymous reviewers for their insightful comments. This work is supported by the
National Natural Science Foundation of China (NSFC No. 61661146007, 61572273) and Tsinghua
University Initiative Scientiﬁc Research Program (20151080406). This research is part of the NExT++
project, supported by the National Research Foundation, Prime Ministers Ofﬁce, Singapore under its
IRC@Singapore Funding Initiative.

References

Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and Cordelia Schmid. 2013. Label-embedding for attribute-

based classiﬁcation. In Proceedings of CVPR, pages 819–826.

Baharum Baharudin, Lam Hong Lee, and Khairullah Khan. 2010. A review of machine learning algorithms for

text-documents classiﬁcation. JAIT, 1(1):4–20.

Antoine Bordes, Sumit Chopra, and Jason Weston. 2014. Question answering with subgraph embeddings. In

Proceedings of EMNLP, pages 615–620.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.

Natural language processing (almost) from scratch. JMLR, 12:2493–2537.

George E Dahl, Dong Yu, Li Deng, and Alex Acero. 2012. Context-dependent pre-trained deep neural networks

for large-vocabulary speech recognition. IEEE TASLP, 20(1):30–42.

Mohamed Elhoseiny, Babak Saleh, and Ahmed Elgammal. 2014. Write a classiﬁer: Zero-shot learning using

purely textual descriptions. In Proceedings of ICCV, pages 2584–2591.

Clement Farabet, Camille Couprie, Laurent Najman, and Yann LeCun. 2013. Learning hierarchical features for

scene labeling. IEEE TPAMI, 35(8):1915–1929.

Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior,
Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. 2012. Deep neural networks for acoustic modeling
in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82–
97.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.

Dinesh Jayaraman and Kristen Grauman. 2014. Zero-shot recognition with unreliable attributes. In Proceedings

of NIPS, pages 3464–3472.

S´ebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. 2015. On using very large target

vocabulary for neural machine translation. In Proceedings of ACL, volume 1, pages 1–10.

Daniel Martin Katz, Michael J Bommarito II, and Josh Blackman. 2017. A general approach for predicting the

behavior of the supreme court of the united states. PloS one, 12(4):e0174698.

R Keown. 1980. Mathematical models for legal prediction. Computer/Law Journal, 2:829.

Yoon Kim. 2014. Convolutional neural networks for sentence classiﬁcation. In Proceedings of EMNLP, pages

1746–1751.

Diederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of ICLR.

Fred Kort. 1957. Predicting supreme court decisions mathematically: A quantitative analysis of the ”right to

counsel” cases. American Political Science Review, 51(1):1–12.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classiﬁcation with deep convolutional

neural networks. In Proceedings of NIPS, pages 1097–1105.

Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. 2014. Attribute-based classiﬁcation for zero-shot

visual object categorization. IEEE TPAMI, 36(3):453–465.

Wan-Chen Lin, Tsung-Ting Kuo, Tung-Jia Chang, Chueh-An Yen, Chao-Ju Chen, and Shou-de Lin. 2012. Ex-
ploiting machine learning models for chinese legal documents labeling, case classiﬁcation, and sentencing pre-
diction. In Processdings of ROCLING, pages 140–141.

Chao-Lin Liu and Chwen-Dar Hsieh. 2006. Exploring phrase-based classiﬁcation of judicial documents for

criminal charges in chinese. In Proceedings of ISMIS, pages 681–690.

Chao-Lin Liu, Cheng-Tsung Chang, and Jim-How Ho. 2004. Case instance generation and reﬁnement for case-

based criminal summary judgments in chinese. JISE, pages 783–800.

Bingfeng Luo, Yansong Feng, Jianbo Xu, Xiang Zhang, and Dongyan Zhao. 2017. Learning to predict charges

for criminal cases with legal basis. In Proceedings of EMNLP, pages 2727–2736.

Ejan Mackaay and Pierre Robillard. 1974. Predicting judicial decisions: The nearest neighbour rule and visual

representation of case patterns. Datenverarbeitung im Recht, 3(3/4):302–331.

Tom´aˇs Mikolov, Anoop Deoras, Daniel Povey, Luk´aˇs Burget, and Jan ˇCernock`y. 2011. Strategies for training

large scale neural network language models. In Proceedings of ASRU workshop, pages 196–201.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of

words and phrases and their compositionality. In Proceedings of NIPS, pages 3111–3119.

Stuart S Nagel. 1963. Applying correlation analysis to case prediction. Texas Law Review, 42:1006.

Tara N Sainath, Abdel-rahman Mohamed, Brian Kingsbury, and Bhuvana Ramabhadran. 2013. Deep convolu-

tional neural networks for lvcsr. In Proceedings of ICASSP, pages 8614–8618.

Gerard Salton and Christopher Buckley. 1988. Term-weighting approaches in automatic text retrieval. Information

processing & management, 24(5):513–523.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout:

a simple way to prevent neural networks from overﬁtting. JMLR, 15(1):1929–1958.

Octavia Maria Sulea, Marcos Zampieri, Mihaela Vela, and Josef Van Genabith. 2017. Exploring the use of text

classi cation in the legal domain. In Proceedings of ASAIL workshop.

Maosong Sun, Xinxiong Chen, Kaixu Zhang, Zhipeng Guo, and Zhiyuan Liu. 2016. THULAC: An efﬁcient

lexical analyzer for chinese.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In

Proceedings of NIPS, pages 3104–3112.

Johan AK Suykens and Joos Vandewalle. 1999. Least squares support vector machine classiﬁers. Neural process-

ing letters, 9(3):293–300.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,
Vincent Vanhoucke, Andrew Rabinovich, Jen-Hao Rick Chang, et al. 2015. Going deeper with convolutions.
In Proceedings of CVPR, pages 1–9.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Document modeling with gated recurrent neural network for sentiment

classiﬁcation. In Proceedings of EMNLP, pages 1422–1432.

Jonathan J Tompson, Arjun Jain, Yann LeCun, and Christoph Bregler. 2014. Joint training of a convolutional

network and a graphical model for human pose estimation. In Proceedings of NIPS, pages 1799–1807.

Shuang Wu, Sravanthi Bondugula, Florian Luisier, Xiaodan Zhuang, and Pradeep Natarajan. 2014. Zero-shot
event detection using multi-modal fusion of weakly supervised concepts. In Proceedings of CVPR, pages 2665–
2672.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J Smola, and Eduard H Hovy. 2016. Hierarchical

attention networks for document classiﬁcation. In Proceedings of NAACL, pages 1480–1489.

Rowan Zellers and Yejin Choi. 2017. Zero-shot activity recognition with verb attribute induction. In Proceedings

of EMNLP, pages 946–958.

Few-Shot Charge Prediction with Discriminative Legal Attributes

Zikun Hu∗ Xiang Li∗ Cunchao Tu Zhiyuan Liu† Maosong Sun
Department of Computer Science and Technology, Tsinghua University
State Key Lab on Intelligent Technology and Systems, Tsinghua University
Beijing National Research Center for Information Science and Technology
{hzk14, x-l15}@mails.tsinghua.edu.cn
tucunchao@gmail.com, {liuzy, sms}@tsinghua.edu.cn

Abstract

Automatic charge prediction aims to predict the ﬁnal charges according to the fact descriptions
in criminal cases and plays a crucial role in legal assistant systems. Existing works on charge
prediction perform adequately on those high-frequency charges but are not yet capable of pre-
dicting few-shot charges with limited cases. Moreover, these exist many confusing charge pairs,
whose fact descriptions are fairly similar to each other. To address these issues, we introduce
several discriminative attributes of charges as the internal mapping between fact descriptions and
charges. These attributes provide additional information for few-shot charges, as well as effective
signals for distinguishing confusing charges. More speciﬁcally, we propose an attribute-attentive
charge prediction model to infer the attributes and charges simultaneously. Experimental results
on real-work datasets demonstrate that our proposed model achieves signiﬁcant and consistent
improvements than other state-of-the-art baselines. Speciﬁcally, our model outperforms other
baselines by more than 50% in the few-shot scenario. Our codes and datasets can be obtained
from https://github.com/thunlp/attribute_charge.

1

Introduction

The task of automatic charge prediction aims to train a machine judge to determine the ﬁnal charges (e.g.,
theft, robbery or trafﬁc offence.) of the defendants in criminal cases. As a representative subtask of legal
judgment prediction, charge prediction plays an important role in legal assistant systems and can beneﬁt
many real-world applications. For example, it can provide a handy reference for legal experts (e.g.,
lawyers and judges) and improve their working efﬁciency. Meanwhile, it can supply ordinary people
who are unfamiliar with legal terminology and complex procedures with legal consulting.

As a typical task in legal intelligence, automatic charge prediction has been studied for decades and
most existing works formalize this task under the text classiﬁcation framework. At the early stage,
researchers pay great efforts to extract efﬁcient features from text or case proﬁles. For example, some
works (Liu et al., 2004; Liu and Hsieh, 2006) utilize shallow textual features, including characters,
words, and phrases, to predict charges. Katz et al. (2017) predict the US Supreme Court’s decisions
with efﬁcient features extracted from case proﬁles (e.g., dates, locations, terms, and types). All these
approaches require numerous human effort to design features and annotate training instances. Besides,
Inspired by the successful usage of deep neural
these methods are hard to scale to other scenarios.
networks on natural language processing tasks (Kim, 2014; Baharudin et al., 2010; Tang et al., 2015),
researchers propose to employ deep neural networks to model legal documents. For example, Luo et al.
(2017) propose an attention-based neural network for charge prediction by incorporating the relevant law
articles.

However, charge prediction is still confronted with two major challenges which make it non-trivial:

∗ Indicates equal contribution.
† Corresponding Author.

This work is licensed under a Creative Commons Attribution 4.0 International License.
creativecommons.org/licenses/by/4.0/.

License details: http://

Figure 1: An illustration of the attribute-based charge prediction.

Few-Shot Charges. In practice, the case numbers of various charges are highly imbalanced. Accord-
ing to our statistics on a real-world dataset, the most frequent 10 charges (e.g., theft, intentional injury,
and trafﬁc offence.) cover 78.1% cases. On the contrary, the most low-frequency 50 (e.g., scalping relics,
disrupting the order of the court, and tax-escaping.) charges only cover less than 0.5% cases and most
of these charges own only around ten cases correspondingly. Previous works usually focus on these
common charges and ignore the few-shot ones. Though deep neural models advance feature-engineering
based charge prediction methods, they are unable to handle few-shot charges well due to the requirement
of sufﬁcient training data. Therefore, how to deal with these charges with limited cases is critical to a
robust and effective charge prediction system.

Confusing Charges. Besides few-shot charges, there also exist many confusing charge pairs, such as
(theft, robbery) and (misappropriation of funds, embezzlement). For each confusing pair, the deﬁnitions
of two charges only differ in the veriﬁcation of a speciﬁc act and the circumstances in corresponding
cases are usually similar to each other. As illustrated in Fig. 1, many robbery case also contain the act of
theft, and the existence of violence is the only key factor to distinguish these two charges. Thus, how to
capture the crucial factors for distinguishing confusing charges is another challenge of charge prediction.
To address these issues, we propose to introduce discriminative legal attributes of charges into consid-
eration and take these attributes as the internal mapping between facts and charge. More speciﬁcally, we
select 10 representative attributes of charges, including violence, proﬁt purpose, buying and selling and
so on. Afterwards, we conduct a low-cost category-level annotation, i.e., for each charge, we annotate
the value (including yes, no, or not available) of each attribute. This annotation indicates if an attribute
is the essential condition of a charge.

With the attribute annotation of charges, we propose a novel multi-task learning framework to predict
In this model, we employ attribute attention
the attributes and charges of each case simultaneously.
mechanism to capture the critical factual information relevant to a speciﬁc attribute. After that, we
combine these attribute-aware representations with an attribute-free fact representation to predict the
ﬁnal charges. There are two reasons for introducing legal attributes into our charge prediction model. On
one hand, these attributes can provide explicit knowledge about how to distinguish confusing charges.
On the other hand, these attributes are shared by all charges, and the knowledge can transfer from high-
frequency charges to low-frequency ones. Even for the few-shot charges, we can learn an efﬁcient
attribute-aware representation for prediction.

To investigate the advantage of our model on handling few-shot and confusing charges, we conduct
experiments on three real-world datasets of Chinese criminal cases. Experimental results demonstrate
that our model signiﬁcantly and consistently outperforms other state-of-the-art models on all datasets
and evaluation metrics. It is worth noting that, our model outperforms other baselines by more than 50%
for the few-shot charges.

To summarize, we make three main contributions as follows:
(1) We are the ﬁrst to focus on the few-shot and confusing problems in charge prediction. To address

these issues, we introduce legal attributes of charges into charge prediction task for the ﬁrst time.

(2) We propose a novel multi-task learning framework to infer the attributes and charges of a case
jointly. To achieve it, we employ attribute attention mechanism to learn attribute-aware fact representa-

tions.

(3) We conduct efﬁcient experiments on several real-world datasets, and our model signiﬁcantly out-

performs other baselines and achieves more than 50% improvements for few-shot charges.

2 Related Work

2.1 Zero-Shot Classiﬁcation

Our work is relevant to zero-shot classiﬁcation in computer vision. Many attribute-based models have
been proposed under this task since attributes are shared among different classes and can offer an in-
termediate representation. Lampert et al. (2014) introduces direct attribute prediction(DAP) and indirect
attribute prediction(IAP), and proposes attribute classiﬁers which can be pre-trained and dont need re-
training when ﬁnding new suitable object class. Akata et al. (2013) proposes to transform the task of
Jayaraman and Grauman (2014) introduces
attribute-based classiﬁcation to the label-embedding task.
a random forest method stressing the unreliability of attribute prediction for unseen classes. They also
extend it to the few-shot scenario.

Other than attributes, other external information can also be introduced to promote zero-shot classi-
ﬁcation. Elhoseiny et al. (2014) makes use of text description of the class label to transfer knowledge
between text features and visual features. Zero-shot learning has also been used in applications besides
object recognition, such as activity recognition (Zellers and Choi, 2017) and event recognition (Wu et
al., 2014).

2.2 Charge Prediction

Researchers in the legal area have been working on automatically making the legal judgment for a long
time. Kort (1957) applies quantitative methods to predict judgment by calculation numerical values for
factual elements. Nagel (1963) makes use of correlation analysis to make predictions for reapportioning
cases. Keown (1980) introduced mathematical models used for legal prediction such as linear models
and the scheme of nearest neighbors. These methods are usually mathematical or quantitative, and they
are restricted to a small dataset with few labels.

Since machine learning has been proven successful in many areas, researchers begin to formalize
charge prediction as a text classiﬁcation problem and make use of machine learning methods. Such
work usually focuses on feature extraction from the case fact. Lin et al. (2012) fetches 21 legal factor
labels for case classiﬁcation. Mackaay and Robillard (1974) extracts N-grams and topics created by
clustering semantically similar N-grams as features. Sulea et al. (2017) proposes a system based on
SVM ensembles using the case description, ruling and time span of a case as input. However, these
methods only extract shallow text features or manual labels which are hard to gather on a larger dataset.
Whats more, the conventional models could not catch the subtle difference between similar crimes, thus
they wouldnt perform well when the number of classes increases and more similar crimes appear.

With the successful usage of neural network methods on speech (Mikolov et al., 2011; Hinton et al.,
2012; Dahl et al., 2012; Sainath et al., 2013), computer vision (CV) (Krizhevsky et al., 2012; Farabet et
al., 2013; Tompson et al., 2014; Szegedy et al., 2015) and natural language processing (NLP) (Collobert
et al., 2011; Kim, 2014; Bordes et al., 2014; Sutskever et al., 2014; Jean et al., 2015; Yang et al., 2016),
researchers propose to employ neural models for legal tasks. Luo et al. (2017) proposes a hierarchical
attentional network to predict charges and extract relevant articles jointly. However, this work only
focuses on high-frequency charges, without paying attention to few-shot and confusing ones. To address
these issues, we propose an attention-based neural model by incorporating several discriminative legal
attributes.

3 Method

In this section, we propose a few-shot neural model which jointly models charge prediction task and legal
attribute prediction task in a uniﬁed framework. In the following parts, we ﬁrst introduce the discrimina-
tive charge attributes. Afterward, we give deﬁnitions of charge prediction and attribute prediction. Then

we describe the neural encoder of fact description and the attention-based attribute predictor. At last, we
show the output layer and the loss function of our model.

3.1 Discriminative Charge Attributes

To distinguish confusing charges and provide additional knowledge for few-shot charges, we introduce
10 discriminative attributes for all the charges in Chinese criminal law. The detailed descriptions of these
attributes are shown in Table 1. For each (charge, attribute) pair, it can be labeled as Yes, No or NA. For
example, the charge of manslaughter should be labeled as No on Intentional Crime, Yes on Death, NA
on State Organ. Note that, the fact-ﬁndings of a speciﬁc case can only be labeled as Yes or No. When
convicting someone of a certain crime, the facts should conform to the description of the certain charge.
Thus for a certain attribute, the label of a speciﬁc case and the label of the corresponding charge should
be the same or not in conﬂict. In other words, for a certain attribute, the label of a case and the charge
can only be (Yes, Yes), (No, No), (Yes, NA), or (No, NA). In practice, we conduct a low-cost annotation
and annotate the attributes of 149 distinct charges manually. Then, we assign each case with the same
attributes of its corresponding charge.

Attributes

Description

Proﬁt Purpose

Whether the criminal commits a crime on the purpose of getting proﬁt.

Buying and Selling Whether the criminal has buying or selling behavior during the commission of the

crime.

Whether death is caused by the criminal.

Whether the criminal has the act of violence.

Death

Violence

State Organ

Public Place

Whether the case or the charge involves State organ or any functionary of a State organ.

Whether the criminal commits a crime in a public place.

Illegal Possession

Whether the criminal commits a crime for the purpose of illegal possession.

Physical Injury

Whether a physical injury is caused by the criminal.

Intentional Crime Whether the criminal commits an intentional crime.

Production

Whether the criminal commits a crime during the production.

Table 1: The descriptions of selected attributes.

3.2 Formalizations

3.2.1 Charge Prediction
The fact description of a case can be seen as a word sequence x = {x1, . . . , xn}, where n represents the
sequence length, xi ∈ T , and T is a ﬁxed vocabulary. Given the fact description x, the charge prediction
task aims to predict a charge y ∈ Y from a charge set Y .

3.2.2 Attributes Prediction
The attributes prediction task can be regarded as a binary classiﬁcation task. It takes the same input
sequence x as in the charge prediction task, and aims to predict the fact-ﬁndings of attributes p =
{p1, . . . , pk} according to the fact. Here, k is the number of selected attributes, and pi ∈ {0, 1} is the
label for a certain attribute.

3.3 Fact Encoder

As illustrated in Fig. 2, fact encoder encodes the discrete input sequence into continuous hidden states.
Here, we employ conventional Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997)
as fact encoder due to its ability to extract semantic meanings. LSTM is a variation of RNN and is
capable of capturing long-term dependencies.

First, LSTM encoder converts each word xi ∈ x into its word embedding xi ∈ Rd, where d is
the dimension of word embeddings. Then, we get the corresponding word embedding sequence as

Figure 2: An illustration of the attribute-based charge prediction.

ˆx = {x1, . . . , xn}.

hidden state ht as follows:

At each time step t ∈ [1, n], the LSTM cell intakes xt, recalculates memory cell ct, and outputs new

ft = σ (Wf xt + Uf ht−1 + bf ) ,
it = σ (Wixt + Uiht−1 + bi) ,
ot = σ (Woxt + Uoht−1 + bo) ,
ˆct = tanh (Wcxt + Ucht−1 + bc) ,
ct = ft (cid:12) ct−1 + it (cid:12) ˆct,
ht = ot (cid:12) tanh (ct) .

Here, ft, it and ot represent forget gate, input gate, and output gate respectively. (cid:12) means element-wise
multiplication and σ is the sigmoid activation function. W , U , and b are weight matrices and bias vectors.
After processing all time steps, we get a hidden state sequence h = {h1, . . . , hn}. At last, we feed it
into a max-pooling layer to get the attribute-free representation e = [e1, . . . , es] as

ei = max(h1,i, · · · , hn,i), ∀i ∈ [1, s].

Here, s is the dimension of hidden states.

3.4 Attentive Attribute Predictor

Given the fact description x, the attribute predictor aims to predict the label of every attribute. Inspired
by (Yang et al., 2016), we employ an attention mechanism to select relevant information from facts and
generate attribute-aware fact representations.

As shown in Fig. 2, attribute predictor takes the hidden state sequence h = {h1, . . . , hn} as input. Our
attentive attribute predictor then calculates attention weights a = {a1, . . . , ak} for all attribute, where
ai = [ai,1, . . . , ai,n]. ∀i ∈ [1, k] and j ∈ [1, n], ai,j is calculated by:

ai,j =

exp(tanh(Wahj)T ui)
t exp(tanh(Waht)T ui)

.

(cid:80)

Here, ui is the context vector of the i-th attribute to calculate how informative an element is to the
attribute i, and Wa is a weight matrix that all attributes share. Afterwards, we get attribute-aware
representations of fact g = {g1, . . . , gk}, where gi = (cid:80)
t ai,tht. At last, with representations g, we
project it into the label space and use softmax function to get the ﬁnal prediction results p = [p1, . . . , pk],
where pi is the prediction result of attribute i and is calculated by:
i gi + bp
i ),

zi = softmax(Wp
pi = arg max(zi).

(1)

(2)

(3)

(4)

Here, zi is the prediction probability distribution on Yes and No. Wp
vector of attribute i.

i and bi are weight matrix and bias

3.5 Output Layer

To integrate the fact descriptions and fact-ﬁndings of all attributes, we use both attribute-free and
attribute-aware representations to predict the ﬁnal charge of a case in the output layer. The predicted
distribution y over all charges is calculated as follows:

r =

(cid:80)

i gi
k

,

v = e ⊕ r,
y = softmax(Wyv + by).

(5)

(6)

(7)

(8)

Here, r is the average of attribute-aware representations. r and e are concatenated into the ﬁnal fact
representation v. Wy and by are weight matrix and bias vector in the output layer.

3.6 Optimization

The training objective of our proposed model consists of two parts. The ﬁrst one is to minimize the
cross-entropy between predicted charge distribution y and the ground-truth distribution ˆy. The other one
is to minimize the cross-entropy between predicted distribution and the ground-truth fact-ﬁnding of each
attribute.

The charge prediction loss can be formalized as:

where yi is the ground-truth label, ˆyi is prediction probability, and C is the number of charges.

As each attribute is equally important in the model, we can easily calculate the attribution loss by sum
up the cross-entropy of all attributes. However, when the attribute of a speciﬁc charge is NA, the label of
the corresponding cases can be Yes or No. Therefore, we only add up the cross-entropy to the attribute
loss when this attribute of the charge belongs to Yes or No. At last, we formulate the attribute loss as:

Lcharge = −

yi · log(ˆyi),

C
(cid:88)

i=1

Lattr = −

zij · log(ˆzij),

k
(cid:88)

2
(cid:88)

Ii

i=1

j=1

where Ii is an indicator function. Ii = 1 if the i-th attribute of current charge is labeled as Yes or No, and
Ii = 0 otherwise. Obviously, zi is the ground-truth label, and ˆzi is predicted probabilities distribution on
Yes and No.

Considering the two objectives, our ﬁnal loss function L is obtained by adding Lcharge and Lattr as

where α is a hyper-parameter to balance the weight of the two parts in the loss function.

L = Lcharge + α · Lattr,

follows:

4 Experiments

In order to investigate the effectiveness of our model on criminal charges prediction, we conduct experi-
ments on several real-world datasets and compare our model with several state-of-the-art baselines.

4.1 Dataset Construction

Since there are no publicly available datasets in previous works for charges prediction, we collect crim-
inal cases published by the Chinese government from China Judgments Online1. As each case is well-
structured and divided into several parts such as fact, court view, and penalty result, we select the fact

1http://wenshu.court.gov.cn.

part of each case as our input. Besides, we can easily extract the charges from the penalty result by
regular expression. We have manually checked the extracted charges and there are few mistakes.

Although there are some cases that contain multiple defendants and multiple charges in real-world,
considering the task would be too complex to solve if these cases contained, we removed the cases
which have more than one charges in a verdict. Besides, in order to examine the performance of our
method on few-shot charges, we keep 149 distinct charges (near 3 times as compared with (Luo et al.,
2017)) with at least 10 cases.

After preprocessing, we randomly select about 400, 000 cases and construct three datasets with dif-
ferent scales, denoted as Criminal-S(small), Criminal-M(medium) and Criminal-L(large). The three
different datasets contain the same number of charges but the different number of cases. The detailed
statistics are shown in Table 2.

Datasets Criminal-S Criminal-M Criminal-L

train
test
valid

61, 589
7, 702
7, 755

153, 521
19, 189
19, 250

306, 900
38, 368
38, 429

Table 2: The statistics of different datasets.

4.2 Attribute Selection and Annotation

As mentioned in previous part, we propose to introduce discriminative attributes to enhance charge pre-
diction. To select these attributes, we ﬁrst train a LSTM based charge prediction model and obtain the
confusion matrix of predicted charges on validation set. Then, we ﬁlter out the confusing charge pairs
and provide them to three master students majoring in criminal. According to these confusing charge
pairs, they deﬁne 10 representative attributes to distinguish these confusing pairs.

With the selected 10 attributes, we conduct a low-cost annotation over all charges. Here, the low-cost
annotation means we only need to annotate 10 attributes for 149 charges manually, rather than all cases.
As the selected attributes are discriminative and unambiguous, we asked these annotators to reach an
agreement for each annotation. Totally, we spent less than 10 hours for annotation.

4.3 Baselines

We employ several typical text classiﬁcation models and one charge predicting model as baselines:

TFIDF+SVM: We implement term-frequency inverse document frequency (TFIDF) (Salton and
Buckley, 1988) to extract features of inputs, and employ SVM (Suykens and Vandewalle, 1999) as the
classiﬁer.

CNN: We implement the CNN with multiple ﬁlter widths (Kim, 2014) as text classiﬁer.
LSTM: We implement a two-layer LSTM (Hochreiter and Schmidhuber, 1997) with a max-pooling

layer as the fact encoder.

Fact-Law Attention Model: Luo et al. (2017) propose an attention-based neural charge prediction

model by incorporating relevant law articles.

4.4 Experiment Settings and Evaluation Metrics

As all the case documents are written in Chinese without word cutting, we employ THULAC (Sun et
al., 2016) for word segmentation and set the maximum document length to 500. For the TFIDF+SVM
model, we set the feature size to 2, 000. For other neural models, we employ Skip-Gram model (Mikolov
et al., 2013) to pre-train word embeddings with the embedding size of 100. We set the hidden state size
of LSTM to 100.For the CNN based models, we set the ﬁlter widths to (2, 3, 4, 5) with each ﬁlter size to
25 for consistency. The weight α of the attribute loss is set to 1.

Note that, the representation size of our model turns into 200 after concatenation. For a fair com-
parison, we add a 100 × 200 fully connected layer between after the pooling layer in CNN and LSTM,

denoted as CNN-200 and LSTM-200.

We use Adam (Kingma and Ba, 2015) as the optimizer, and set the learning rate to 0.001, the dropout
rate (Srivastava et al., 2014) to 0.5 and the batch size to 64. We employ accuracy (Acc.), macro-precision
(MP), macro-recall (MR) and macro-F1 as our evaluation metrics.

4.5 Results and Analysis

Datasets

Criminal-S

Criminal-M

Criminal-L

Metrics

Acc. MP MR

F1

Acc. MP MR

F1

Acc. MP MR

F1

TFIDF+SVM 85.8
91.9
92.6
93.5
92.7
92.8

CNN
CNN-200
LSTM
LSTM-200
Fact-Law Att.

Our Model

93.4

49.7
50.5
51.1
59.4
60.0
57.0

66.7

41.9
44.9
46.3
58.6
58.4
53.9

69.2

43.5
46.1
47.3
57.3
57.0
53.4

64.9

89.6
93.5
92.8
94.7
94.4
94.7

94.4

58.8
57.6
56.2
65.8
66.5
66.7

68.3

5 0.1
48.1
50.0
63.0
62.4
60.4

69.2

52.1
50.5
50.8
62.6
62.7
61.8

67.1

91.8
93.9
94.1
95.5
95.1
95.7

95.8

67.5
66.0
61.9
69.8
72.8
73.3

75.8

54.1
50.3
50.0
67.0
66.7
67.1

73.7

57.5
54.7
53.1
66.8
67.9
68.6

73.1

Table 3: Charge prediction results of three datasets.

As shown in Table 3, we can observe that our model signiﬁcantly and consistently outperforms all
the baselines. Almost all existing methods perform poorly under the macro-F1 metric, which indicates
their shortage of predicting few-shot charges. Conversely, our model achieves promising improvements
(7.9%, 4.4%, and 5.2% absolutely on three datasets respectively), which demonstrates the robustness
and effectiveness of our model.

To further verify the advance of our model on dealing with few-shot charges, we show the performance
on charges with different frequencies. As shown in Table 4, we divide the charges into three parts
according to their frequencies. Here, the charges with ≤ 10 cases are low-frequency, and the charges
with > 100 cases are high-frequency. From this table, we ﬁnd that our model achieves more than 50%
improvements than baseline method for the low-frequency (i.e., few-shot) charges, which veriﬁes the
effectiveness of our model on handling few-shot issues.

Charge Type

Low frequency Medium frequency High frequency

Charge Number

49

51

49

LSTM-200
Our Model

32.6
49.7 (↑ 17.1%)

55.0
60.0 (↑ 5.0%)

83.3
85.2 (↑ 1.9%)

Table 4: Macro-F1 values of various charges on Criminal-S.

Datasets

Metrics

Our model

Criminal-S

Criminal-M

Criminal-L

Acc. MP MR

F1

Acc. MP MR

F1

Acc. MP MR

F1

w/o attention
w/o concatenation

93.4

93.5
93.5

66.7

63.4
59.3

69.2

60.1
59.0

64.9

60.0
57.2

94.4

94.7
95.0

68.3

68.8
64.6

69.2

58.2
62.4

67.1

60.9
62.5

95.8

94.9
95.7

75.8

70.9
69.4

73.7

54.4
64.5

73.1

58.6
65.4

Table 5: Experimental results of ablation test.

4.6 Ablation Test

Our method is characterized by the incorporation of attention mechanism and attribute-aware representa-
tions. Thus, we design ablation test respectively to investigate the effectiveness of these modules. When
taken off the attention mechanism, for each attribute we replace attention mechanism with a fully con-
nected layer. When taken off the attribute-aware representations (i.e., without concatenating the averaged

Task

Charge Prediction Attribute Prediction on physical injury

Ground Truth

Intentional Injury

Our model

Intentional Injury

LSTM-200

Affray

Yes

Yes

N/A

Table 6: Charge and attribute prediction result of the selected case.

attribute-aware representation), our method degrades into a typical multi-task learning based on LSTM
for both charge and attribute prediction.

As shown in Table 5, we can observe that the performance drops obviously after removing the attention
layer or the concatenation. The macro-F1 decreases at least 4%. Therefore, it can be seen that both
attention mechanism and attribute-aware fact representation play irreplaceable roles in our model.

4.7 Case Study

In this part, we select a representative case to give an intuitive illustration of how the predicted attributes
help to promote the performance of charge prediction. In this case, the defendant is convicted of inten-
tional injury. It is often hard to decide whether to judge a case as affray or intentional injury since they
are both related to violence. One important difference between them is that intentional injury has the
feature of physical injury, while affray does not.

So we believe the attribute physical injury is essential in the charge prediction of this case. As shown
in Table 6, our model correctly predicts the label of physical injury as Yes, and consequently predicts
the charge as intentional injury. In contrary, the model LSTM-200 predicts it as affray incorrectly. In
addition, we visual the heat map of this case when predicting the attribute intentionalinjury. Words
with deeper background color have higher attention weights. From this ﬁgure, we observe that the
attention mechanism can capture key patterns and semantics relevant to current attribute.

Figure 3: Visualization of attention mechanism.

.

5 Conclusion

In this work, we focus on the task of charge prediction according to the fact descriptions of criminal cases.
To address the problem of prediction few-shot and confusing charges, we introduce discriminative legal

attributes into consideration and propose a novel attribute-based multi-task learning model for charge
prediction. Speciﬁcally, our model learns attribute-free and attribute-aware fact representation jointly by
utilizing attribute-based attention mechanism.

In future, we will explore the following directions:
(1) There are more complicated criminal cases, such as multiple defendants and charges. Thus, it is

challenging to handle this general form of charge prediction.

(2) In this work, we only utilize several simple attributes of charges, while there exist more complex
essential conditions of charges. How to take full usage of essential conditions of charges is expected to
improve the interpretability of charge prediction models.

Acknowledgements

We thank all the anonymous reviewers for their insightful comments. This work is supported by the
National Natural Science Foundation of China (NSFC No. 61661146007, 61572273) and Tsinghua
University Initiative Scientiﬁc Research Program (20151080406). This research is part of the NExT++
project, supported by the National Research Foundation, Prime Ministers Ofﬁce, Singapore under its
IRC@Singapore Funding Initiative.

References

Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and Cordelia Schmid. 2013. Label-embedding for attribute-

based classiﬁcation. In Proceedings of CVPR, pages 819–826.

Baharum Baharudin, Lam Hong Lee, and Khairullah Khan. 2010. A review of machine learning algorithms for

text-documents classiﬁcation. JAIT, 1(1):4–20.

Antoine Bordes, Sumit Chopra, and Jason Weston. 2014. Question answering with subgraph embeddings. In

Proceedings of EMNLP, pages 615–620.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.

Natural language processing (almost) from scratch. JMLR, 12:2493–2537.

George E Dahl, Dong Yu, Li Deng, and Alex Acero. 2012. Context-dependent pre-trained deep neural networks

for large-vocabulary speech recognition. IEEE TASLP, 20(1):30–42.

Mohamed Elhoseiny, Babak Saleh, and Ahmed Elgammal. 2014. Write a classiﬁer: Zero-shot learning using

purely textual descriptions. In Proceedings of ICCV, pages 2584–2591.

Clement Farabet, Camille Couprie, Laurent Najman, and Yann LeCun. 2013. Learning hierarchical features for

scene labeling. IEEE TPAMI, 35(8):1915–1929.

Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior,
Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. 2012. Deep neural networks for acoustic modeling
in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82–
97.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.

Dinesh Jayaraman and Kristen Grauman. 2014. Zero-shot recognition with unreliable attributes. In Proceedings

of NIPS, pages 3464–3472.

S´ebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. 2015. On using very large target

vocabulary for neural machine translation. In Proceedings of ACL, volume 1, pages 1–10.

Daniel Martin Katz, Michael J Bommarito II, and Josh Blackman. 2017. A general approach for predicting the

behavior of the supreme court of the united states. PloS one, 12(4):e0174698.

R Keown. 1980. Mathematical models for legal prediction. Computer/Law Journal, 2:829.

Yoon Kim. 2014. Convolutional neural networks for sentence classiﬁcation. In Proceedings of EMNLP, pages

1746–1751.

Diederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of ICLR.

Fred Kort. 1957. Predicting supreme court decisions mathematically: A quantitative analysis of the ”right to

counsel” cases. American Political Science Review, 51(1):1–12.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classiﬁcation with deep convolutional

neural networks. In Proceedings of NIPS, pages 1097–1105.

Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. 2014. Attribute-based classiﬁcation for zero-shot

visual object categorization. IEEE TPAMI, 36(3):453–465.

Wan-Chen Lin, Tsung-Ting Kuo, Tung-Jia Chang, Chueh-An Yen, Chao-Ju Chen, and Shou-de Lin. 2012. Ex-
ploiting machine learning models for chinese legal documents labeling, case classiﬁcation, and sentencing pre-
diction. In Processdings of ROCLING, pages 140–141.

Chao-Lin Liu and Chwen-Dar Hsieh. 2006. Exploring phrase-based classiﬁcation of judicial documents for

criminal charges in chinese. In Proceedings of ISMIS, pages 681–690.

Chao-Lin Liu, Cheng-Tsung Chang, and Jim-How Ho. 2004. Case instance generation and reﬁnement for case-

based criminal summary judgments in chinese. JISE, pages 783–800.

Bingfeng Luo, Yansong Feng, Jianbo Xu, Xiang Zhang, and Dongyan Zhao. 2017. Learning to predict charges

for criminal cases with legal basis. In Proceedings of EMNLP, pages 2727–2736.

Ejan Mackaay and Pierre Robillard. 1974. Predicting judicial decisions: The nearest neighbour rule and visual

representation of case patterns. Datenverarbeitung im Recht, 3(3/4):302–331.

Tom´aˇs Mikolov, Anoop Deoras, Daniel Povey, Luk´aˇs Burget, and Jan ˇCernock`y. 2011. Strategies for training

large scale neural network language models. In Proceedings of ASRU workshop, pages 196–201.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of

words and phrases and their compositionality. In Proceedings of NIPS, pages 3111–3119.

Stuart S Nagel. 1963. Applying correlation analysis to case prediction. Texas Law Review, 42:1006.

Tara N Sainath, Abdel-rahman Mohamed, Brian Kingsbury, and Bhuvana Ramabhadran. 2013. Deep convolu-

tional neural networks for lvcsr. In Proceedings of ICASSP, pages 8614–8618.

Gerard Salton and Christopher Buckley. 1988. Term-weighting approaches in automatic text retrieval. Information

processing & management, 24(5):513–523.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout:

a simple way to prevent neural networks from overﬁtting. JMLR, 15(1):1929–1958.

Octavia Maria Sulea, Marcos Zampieri, Mihaela Vela, and Josef Van Genabith. 2017. Exploring the use of text

classi cation in the legal domain. In Proceedings of ASAIL workshop.

Maosong Sun, Xinxiong Chen, Kaixu Zhang, Zhipeng Guo, and Zhiyuan Liu. 2016. THULAC: An efﬁcient

lexical analyzer for chinese.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In

Proceedings of NIPS, pages 3104–3112.

Johan AK Suykens and Joos Vandewalle. 1999. Least squares support vector machine classiﬁers. Neural process-

ing letters, 9(3):293–300.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,
Vincent Vanhoucke, Andrew Rabinovich, Jen-Hao Rick Chang, et al. 2015. Going deeper with convolutions.
In Proceedings of CVPR, pages 1–9.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Document modeling with gated recurrent neural network for sentiment

classiﬁcation. In Proceedings of EMNLP, pages 1422–1432.

Jonathan J Tompson, Arjun Jain, Yann LeCun, and Christoph Bregler. 2014. Joint training of a convolutional

network and a graphical model for human pose estimation. In Proceedings of NIPS, pages 1799–1807.

Shuang Wu, Sravanthi Bondugula, Florian Luisier, Xiaodan Zhuang, and Pradeep Natarajan. 2014. Zero-shot
event detection using multi-modal fusion of weakly supervised concepts. In Proceedings of CVPR, pages 2665–
2672.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J Smola, and Eduard H Hovy. 2016. Hierarchical

attention networks for document classiﬁcation. In Proceedings of NAACL, pages 1480–1489.

Rowan Zellers and Yejin Choi. 2017. Zero-shot activity recognition with verb attribute induction. In Proceedings

of EMNLP, pages 946–958.

Few-Shot Charge Prediction with Discriminative Legal Attributes

Zikun Hu∗ Xiang Li∗ Cunchao Tu Zhiyuan Liu† Maosong Sun
Department of Computer Science and Technology, Tsinghua University
State Key Lab on Intelligent Technology and Systems, Tsinghua University
Beijing National Research Center for Information Science and Technology
{hzk14, x-l15}@mails.tsinghua.edu.cn
tucunchao@gmail.com, {liuzy, sms}@tsinghua.edu.cn

Abstract

Automatic charge prediction aims to predict the ﬁnal charges according to the fact descriptions
in criminal cases and plays a crucial role in legal assistant systems. Existing works on charge
prediction perform adequately on those high-frequency charges but are not yet capable of pre-
dicting few-shot charges with limited cases. Moreover, these exist many confusing charge pairs,
whose fact descriptions are fairly similar to each other. To address these issues, we introduce
several discriminative attributes of charges as the internal mapping between fact descriptions and
charges. These attributes provide additional information for few-shot charges, as well as effective
signals for distinguishing confusing charges. More speciﬁcally, we propose an attribute-attentive
charge prediction model to infer the attributes and charges simultaneously. Experimental results
on real-work datasets demonstrate that our proposed model achieves signiﬁcant and consistent
improvements than other state-of-the-art baselines. Speciﬁcally, our model outperforms other
baselines by more than 50% in the few-shot scenario. Our codes and datasets can be obtained
from https://github.com/thunlp/attribute_charge.

1

Introduction

The task of automatic charge prediction aims to train a machine judge to determine the ﬁnal charges (e.g.,
theft, robbery or trafﬁc offence.) of the defendants in criminal cases. As a representative subtask of legal
judgment prediction, charge prediction plays an important role in legal assistant systems and can beneﬁt
many real-world applications. For example, it can provide a handy reference for legal experts (e.g.,
lawyers and judges) and improve their working efﬁciency. Meanwhile, it can supply ordinary people
who are unfamiliar with legal terminology and complex procedures with legal consulting.

As a typical task in legal intelligence, automatic charge prediction has been studied for decades and
most existing works formalize this task under the text classiﬁcation framework. At the early stage,
researchers pay great efforts to extract efﬁcient features from text or case proﬁles. For example, some
works (Liu et al., 2004; Liu and Hsieh, 2006) utilize shallow textual features, including characters,
words, and phrases, to predict charges. Katz et al. (2017) predict the US Supreme Court’s decisions
with efﬁcient features extracted from case proﬁles (e.g., dates, locations, terms, and types). All these
approaches require numerous human effort to design features and annotate training instances. Besides,
Inspired by the successful usage of deep neural
these methods are hard to scale to other scenarios.
networks on natural language processing tasks (Kim, 2014; Baharudin et al., 2010; Tang et al., 2015),
researchers propose to employ deep neural networks to model legal documents. For example, Luo et al.
(2017) propose an attention-based neural network for charge prediction by incorporating the relevant law
articles.

However, charge prediction is still confronted with two major challenges which make it non-trivial:

∗ Indicates equal contribution.
† Corresponding Author.

This work is licensed under a Creative Commons Attribution 4.0 International License.
creativecommons.org/licenses/by/4.0/.

License details: http://

Figure 1: An illustration of the attribute-based charge prediction.

Few-Shot Charges. In practice, the case numbers of various charges are highly imbalanced. Accord-
ing to our statistics on a real-world dataset, the most frequent 10 charges (e.g., theft, intentional injury,
and trafﬁc offence.) cover 78.1% cases. On the contrary, the most low-frequency 50 (e.g., scalping relics,
disrupting the order of the court, and tax-escaping.) charges only cover less than 0.5% cases and most
of these charges own only around ten cases correspondingly. Previous works usually focus on these
common charges and ignore the few-shot ones. Though deep neural models advance feature-engineering
based charge prediction methods, they are unable to handle few-shot charges well due to the requirement
of sufﬁcient training data. Therefore, how to deal with these charges with limited cases is critical to a
robust and effective charge prediction system.

Confusing Charges. Besides few-shot charges, there also exist many confusing charge pairs, such as
(theft, robbery) and (misappropriation of funds, embezzlement). For each confusing pair, the deﬁnitions
of two charges only differ in the veriﬁcation of a speciﬁc act and the circumstances in corresponding
cases are usually similar to each other. As illustrated in Fig. 1, many robbery case also contain the act of
theft, and the existence of violence is the only key factor to distinguish these two charges. Thus, how to
capture the crucial factors for distinguishing confusing charges is another challenge of charge prediction.
To address these issues, we propose to introduce discriminative legal attributes of charges into consid-
eration and take these attributes as the internal mapping between facts and charge. More speciﬁcally, we
select 10 representative attributes of charges, including violence, proﬁt purpose, buying and selling and
so on. Afterwards, we conduct a low-cost category-level annotation, i.e., for each charge, we annotate
the value (including yes, no, or not available) of each attribute. This annotation indicates if an attribute
is the essential condition of a charge.

With the attribute annotation of charges, we propose a novel multi-task learning framework to predict
In this model, we employ attribute attention
the attributes and charges of each case simultaneously.
mechanism to capture the critical factual information relevant to a speciﬁc attribute. After that, we
combine these attribute-aware representations with an attribute-free fact representation to predict the
ﬁnal charges. There are two reasons for introducing legal attributes into our charge prediction model. On
one hand, these attributes can provide explicit knowledge about how to distinguish confusing charges.
On the other hand, these attributes are shared by all charges, and the knowledge can transfer from high-
frequency charges to low-frequency ones. Even for the few-shot charges, we can learn an efﬁcient
attribute-aware representation for prediction.

To investigate the advantage of our model on handling few-shot and confusing charges, we conduct
experiments on three real-world datasets of Chinese criminal cases. Experimental results demonstrate
that our model signiﬁcantly and consistently outperforms other state-of-the-art models on all datasets
and evaluation metrics. It is worth noting that, our model outperforms other baselines by more than 50%
for the few-shot charges.

To summarize, we make three main contributions as follows:
(1) We are the ﬁrst to focus on the few-shot and confusing problems in charge prediction. To address

these issues, we introduce legal attributes of charges into charge prediction task for the ﬁrst time.

(2) We propose a novel multi-task learning framework to infer the attributes and charges of a case
jointly. To achieve it, we employ attribute attention mechanism to learn attribute-aware fact representa-

tions.

(3) We conduct efﬁcient experiments on several real-world datasets, and our model signiﬁcantly out-

performs other baselines and achieves more than 50% improvements for few-shot charges.

2 Related Work

2.1 Zero-Shot Classiﬁcation

Our work is relevant to zero-shot classiﬁcation in computer vision. Many attribute-based models have
been proposed under this task since attributes are shared among different classes and can offer an in-
termediate representation. Lampert et al. (2014) introduces direct attribute prediction(DAP) and indirect
attribute prediction(IAP), and proposes attribute classiﬁers which can be pre-trained and dont need re-
training when ﬁnding new suitable object class. Akata et al. (2013) proposes to transform the task of
Jayaraman and Grauman (2014) introduces
attribute-based classiﬁcation to the label-embedding task.
a random forest method stressing the unreliability of attribute prediction for unseen classes. They also
extend it to the few-shot scenario.

Other than attributes, other external information can also be introduced to promote zero-shot classi-
ﬁcation. Elhoseiny et al. (2014) makes use of text description of the class label to transfer knowledge
between text features and visual features. Zero-shot learning has also been used in applications besides
object recognition, such as activity recognition (Zellers and Choi, 2017) and event recognition (Wu et
al., 2014).

2.2 Charge Prediction

Researchers in the legal area have been working on automatically making the legal judgment for a long
time. Kort (1957) applies quantitative methods to predict judgment by calculation numerical values for
factual elements. Nagel (1963) makes use of correlation analysis to make predictions for reapportioning
cases. Keown (1980) introduced mathematical models used for legal prediction such as linear models
and the scheme of nearest neighbors. These methods are usually mathematical or quantitative, and they
are restricted to a small dataset with few labels.

Since machine learning has been proven successful in many areas, researchers begin to formalize
charge prediction as a text classiﬁcation problem and make use of machine learning methods. Such
work usually focuses on feature extraction from the case fact. Lin et al. (2012) fetches 21 legal factor
labels for case classiﬁcation. Mackaay and Robillard (1974) extracts N-grams and topics created by
clustering semantically similar N-grams as features. Sulea et al. (2017) proposes a system based on
SVM ensembles using the case description, ruling and time span of a case as input. However, these
methods only extract shallow text features or manual labels which are hard to gather on a larger dataset.
Whats more, the conventional models could not catch the subtle difference between similar crimes, thus
they wouldnt perform well when the number of classes increases and more similar crimes appear.

With the successful usage of neural network methods on speech (Mikolov et al., 2011; Hinton et al.,
2012; Dahl et al., 2012; Sainath et al., 2013), computer vision (CV) (Krizhevsky et al., 2012; Farabet et
al., 2013; Tompson et al., 2014; Szegedy et al., 2015) and natural language processing (NLP) (Collobert
et al., 2011; Kim, 2014; Bordes et al., 2014; Sutskever et al., 2014; Jean et al., 2015; Yang et al., 2016),
researchers propose to employ neural models for legal tasks. Luo et al. (2017) proposes a hierarchical
attentional network to predict charges and extract relevant articles jointly. However, this work only
focuses on high-frequency charges, without paying attention to few-shot and confusing ones. To address
these issues, we propose an attention-based neural model by incorporating several discriminative legal
attributes.

3 Method

In this section, we propose a few-shot neural model which jointly models charge prediction task and legal
attribute prediction task in a uniﬁed framework. In the following parts, we ﬁrst introduce the discrimina-
tive charge attributes. Afterward, we give deﬁnitions of charge prediction and attribute prediction. Then

we describe the neural encoder of fact description and the attention-based attribute predictor. At last, we
show the output layer and the loss function of our model.

3.1 Discriminative Charge Attributes

To distinguish confusing charges and provide additional knowledge for few-shot charges, we introduce
10 discriminative attributes for all the charges in Chinese criminal law. The detailed descriptions of these
attributes are shown in Table 1. For each (charge, attribute) pair, it can be labeled as Yes, No or NA. For
example, the charge of manslaughter should be labeled as No on Intentional Crime, Yes on Death, NA
on State Organ. Note that, the fact-ﬁndings of a speciﬁc case can only be labeled as Yes or No. When
convicting someone of a certain crime, the facts should conform to the description of the certain charge.
Thus for a certain attribute, the label of a speciﬁc case and the label of the corresponding charge should
be the same or not in conﬂict. In other words, for a certain attribute, the label of a case and the charge
can only be (Yes, Yes), (No, No), (Yes, NA), or (No, NA). In practice, we conduct a low-cost annotation
and annotate the attributes of 149 distinct charges manually. Then, we assign each case with the same
attributes of its corresponding charge.

Attributes

Description

Proﬁt Purpose

Whether the criminal commits a crime on the purpose of getting proﬁt.

Buying and Selling Whether the criminal has buying or selling behavior during the commission of the

crime.

Whether death is caused by the criminal.

Whether the criminal has the act of violence.

Death

Violence

State Organ

Public Place

Whether the case or the charge involves State organ or any functionary of a State organ.

Whether the criminal commits a crime in a public place.

Illegal Possession

Whether the criminal commits a crime for the purpose of illegal possession.

Physical Injury

Whether a physical injury is caused by the criminal.

Intentional Crime Whether the criminal commits an intentional crime.

Production

Whether the criminal commits a crime during the production.

Table 1: The descriptions of selected attributes.

3.2 Formalizations

3.2.1 Charge Prediction
The fact description of a case can be seen as a word sequence x = {x1, . . . , xn}, where n represents the
sequence length, xi ∈ T , and T is a ﬁxed vocabulary. Given the fact description x, the charge prediction
task aims to predict a charge y ∈ Y from a charge set Y .

3.2.2 Attributes Prediction
The attributes prediction task can be regarded as a binary classiﬁcation task. It takes the same input
sequence x as in the charge prediction task, and aims to predict the fact-ﬁndings of attributes p =
{p1, . . . , pk} according to the fact. Here, k is the number of selected attributes, and pi ∈ {0, 1} is the
label for a certain attribute.

3.3 Fact Encoder

As illustrated in Fig. 2, fact encoder encodes the discrete input sequence into continuous hidden states.
Here, we employ conventional Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997)
as fact encoder due to its ability to extract semantic meanings. LSTM is a variation of RNN and is
capable of capturing long-term dependencies.

First, LSTM encoder converts each word xi ∈ x into its word embedding xi ∈ Rd, where d is
the dimension of word embeddings. Then, we get the corresponding word embedding sequence as

Figure 2: An illustration of the attribute-based charge prediction.

ˆx = {x1, . . . , xn}.

hidden state ht as follows:

At each time step t ∈ [1, n], the LSTM cell intakes xt, recalculates memory cell ct, and outputs new

ft = σ (Wf xt + Uf ht−1 + bf ) ,
it = σ (Wixt + Uiht−1 + bi) ,
ot = σ (Woxt + Uoht−1 + bo) ,
ˆct = tanh (Wcxt + Ucht−1 + bc) ,
ct = ft (cid:12) ct−1 + it (cid:12) ˆct,
ht = ot (cid:12) tanh (ct) .

Here, ft, it and ot represent forget gate, input gate, and output gate respectively. (cid:12) means element-wise
multiplication and σ is the sigmoid activation function. W , U , and b are weight matrices and bias vectors.
After processing all time steps, we get a hidden state sequence h = {h1, . . . , hn}. At last, we feed it
into a max-pooling layer to get the attribute-free representation e = [e1, . . . , es] as

ei = max(h1,i, · · · , hn,i), ∀i ∈ [1, s].

Here, s is the dimension of hidden states.

3.4 Attentive Attribute Predictor

Given the fact description x, the attribute predictor aims to predict the label of every attribute. Inspired
by (Yang et al., 2016), we employ an attention mechanism to select relevant information from facts and
generate attribute-aware fact representations.

As shown in Fig. 2, attribute predictor takes the hidden state sequence h = {h1, . . . , hn} as input. Our
attentive attribute predictor then calculates attention weights a = {a1, . . . , ak} for all attribute, where
ai = [ai,1, . . . , ai,n]. ∀i ∈ [1, k] and j ∈ [1, n], ai,j is calculated by:

ai,j =

exp(tanh(Wahj)T ui)
t exp(tanh(Waht)T ui)

.

(cid:80)

Here, ui is the context vector of the i-th attribute to calculate how informative an element is to the
attribute i, and Wa is a weight matrix that all attributes share. Afterwards, we get attribute-aware
representations of fact g = {g1, . . . , gk}, where gi = (cid:80)
t ai,tht. At last, with representations g, we
project it into the label space and use softmax function to get the ﬁnal prediction results p = [p1, . . . , pk],
where pi is the prediction result of attribute i and is calculated by:
i gi + bp
i ),

zi = softmax(Wp
pi = arg max(zi).

(1)

(2)

(3)

(4)

Here, zi is the prediction probability distribution on Yes and No. Wp
vector of attribute i.

i and bi are weight matrix and bias

3.5 Output Layer

To integrate the fact descriptions and fact-ﬁndings of all attributes, we use both attribute-free and
attribute-aware representations to predict the ﬁnal charge of a case in the output layer. The predicted
distribution y over all charges is calculated as follows:

r =

(cid:80)

i gi
k

,

v = e ⊕ r,
y = softmax(Wyv + by).

(5)

(6)

(7)

(8)

Here, r is the average of attribute-aware representations. r and e are concatenated into the ﬁnal fact
representation v. Wy and by are weight matrix and bias vector in the output layer.

3.6 Optimization

The training objective of our proposed model consists of two parts. The ﬁrst one is to minimize the
cross-entropy between predicted charge distribution y and the ground-truth distribution ˆy. The other one
is to minimize the cross-entropy between predicted distribution and the ground-truth fact-ﬁnding of each
attribute.

The charge prediction loss can be formalized as:

where yi is the ground-truth label, ˆyi is prediction probability, and C is the number of charges.

As each attribute is equally important in the model, we can easily calculate the attribution loss by sum
up the cross-entropy of all attributes. However, when the attribute of a speciﬁc charge is NA, the label of
the corresponding cases can be Yes or No. Therefore, we only add up the cross-entropy to the attribute
loss when this attribute of the charge belongs to Yes or No. At last, we formulate the attribute loss as:

Lcharge = −

yi · log(ˆyi),

C
(cid:88)

i=1

Lattr = −

zij · log(ˆzij),

k
(cid:88)

2
(cid:88)

Ii

i=1

j=1

where Ii is an indicator function. Ii = 1 if the i-th attribute of current charge is labeled as Yes or No, and
Ii = 0 otherwise. Obviously, zi is the ground-truth label, and ˆzi is predicted probabilities distribution on
Yes and No.

Considering the two objectives, our ﬁnal loss function L is obtained by adding Lcharge and Lattr as

where α is a hyper-parameter to balance the weight of the two parts in the loss function.

L = Lcharge + α · Lattr,

follows:

4 Experiments

In order to investigate the effectiveness of our model on criminal charges prediction, we conduct experi-
ments on several real-world datasets and compare our model with several state-of-the-art baselines.

4.1 Dataset Construction

Since there are no publicly available datasets in previous works for charges prediction, we collect crim-
inal cases published by the Chinese government from China Judgments Online1. As each case is well-
structured and divided into several parts such as fact, court view, and penalty result, we select the fact

1http://wenshu.court.gov.cn.

part of each case as our input. Besides, we can easily extract the charges from the penalty result by
regular expression. We have manually checked the extracted charges and there are few mistakes.

Although there are some cases that contain multiple defendants and multiple charges in real-world,
considering the task would be too complex to solve if these cases contained, we removed the cases
which have more than one charges in a verdict. Besides, in order to examine the performance of our
method on few-shot charges, we keep 149 distinct charges (near 3 times as compared with (Luo et al.,
2017)) with at least 10 cases.

After preprocessing, we randomly select about 400, 000 cases and construct three datasets with dif-
ferent scales, denoted as Criminal-S(small), Criminal-M(medium) and Criminal-L(large). The three
different datasets contain the same number of charges but the different number of cases. The detailed
statistics are shown in Table 2.

Datasets Criminal-S Criminal-M Criminal-L

train
test
valid

61, 589
7, 702
7, 755

153, 521
19, 189
19, 250

306, 900
38, 368
38, 429

Table 2: The statistics of different datasets.

4.2 Attribute Selection and Annotation

As mentioned in previous part, we propose to introduce discriminative attributes to enhance charge pre-
diction. To select these attributes, we ﬁrst train a LSTM based charge prediction model and obtain the
confusion matrix of predicted charges on validation set. Then, we ﬁlter out the confusing charge pairs
and provide them to three master students majoring in criminal. According to these confusing charge
pairs, they deﬁne 10 representative attributes to distinguish these confusing pairs.

With the selected 10 attributes, we conduct a low-cost annotation over all charges. Here, the low-cost
annotation means we only need to annotate 10 attributes for 149 charges manually, rather than all cases.
As the selected attributes are discriminative and unambiguous, we asked these annotators to reach an
agreement for each annotation. Totally, we spent less than 10 hours for annotation.

4.3 Baselines

We employ several typical text classiﬁcation models and one charge predicting model as baselines:

TFIDF+SVM: We implement term-frequency inverse document frequency (TFIDF) (Salton and
Buckley, 1988) to extract features of inputs, and employ SVM (Suykens and Vandewalle, 1999) as the
classiﬁer.

CNN: We implement the CNN with multiple ﬁlter widths (Kim, 2014) as text classiﬁer.
LSTM: We implement a two-layer LSTM (Hochreiter and Schmidhuber, 1997) with a max-pooling

layer as the fact encoder.

Fact-Law Attention Model: Luo et al. (2017) propose an attention-based neural charge prediction

model by incorporating relevant law articles.

4.4 Experiment Settings and Evaluation Metrics

As all the case documents are written in Chinese without word cutting, we employ THULAC (Sun et
al., 2016) for word segmentation and set the maximum document length to 500. For the TFIDF+SVM
model, we set the feature size to 2, 000. For other neural models, we employ Skip-Gram model (Mikolov
et al., 2013) to pre-train word embeddings with the embedding size of 100. We set the hidden state size
of LSTM to 100.For the CNN based models, we set the ﬁlter widths to (2, 3, 4, 5) with each ﬁlter size to
25 for consistency. The weight α of the attribute loss is set to 1.

Note that, the representation size of our model turns into 200 after concatenation. For a fair com-
parison, we add a 100 × 200 fully connected layer between after the pooling layer in CNN and LSTM,

denoted as CNN-200 and LSTM-200.

We use Adam (Kingma and Ba, 2015) as the optimizer, and set the learning rate to 0.001, the dropout
rate (Srivastava et al., 2014) to 0.5 and the batch size to 64. We employ accuracy (Acc.), macro-precision
(MP), macro-recall (MR) and macro-F1 as our evaluation metrics.

4.5 Results and Analysis

Datasets

Criminal-S

Criminal-M

Criminal-L

Metrics

Acc. MP MR

F1

Acc. MP MR

F1

Acc. MP MR

F1

TFIDF+SVM 85.8
91.9
92.6
93.5
92.7
92.8

CNN
CNN-200
LSTM
LSTM-200
Fact-Law Att.

Our Model

93.4

49.7
50.5
51.1
59.4
60.0
57.0

66.7

41.9
44.9
46.3
58.6
58.4
53.9

69.2

43.5
46.1
47.3
57.3
57.0
53.4

64.9

89.6
93.5
92.8
94.7
94.4
94.7

94.4

58.8
57.6
56.2
65.8
66.5
66.7

68.3

5 0.1
48.1
50.0
63.0
62.4
60.4

69.2

52.1
50.5
50.8
62.6
62.7
61.8

67.1

91.8
93.9
94.1
95.5
95.1
95.7

95.8

67.5
66.0
61.9
69.8
72.8
73.3

75.8

54.1
50.3
50.0
67.0
66.7
67.1

73.7

57.5
54.7
53.1
66.8
67.9
68.6

73.1

Table 3: Charge prediction results of three datasets.

As shown in Table 3, we can observe that our model signiﬁcantly and consistently outperforms all
the baselines. Almost all existing methods perform poorly under the macro-F1 metric, which indicates
their shortage of predicting few-shot charges. Conversely, our model achieves promising improvements
(7.9%, 4.4%, and 5.2% absolutely on three datasets respectively), which demonstrates the robustness
and effectiveness of our model.

To further verify the advance of our model on dealing with few-shot charges, we show the performance
on charges with different frequencies. As shown in Table 4, we divide the charges into three parts
according to their frequencies. Here, the charges with ≤ 10 cases are low-frequency, and the charges
with > 100 cases are high-frequency. From this table, we ﬁnd that our model achieves more than 50%
improvements than baseline method for the low-frequency (i.e., few-shot) charges, which veriﬁes the
effectiveness of our model on handling few-shot issues.

Charge Type

Low frequency Medium frequency High frequency

Charge Number

49

51

49

LSTM-200
Our Model

32.6
49.7 (↑ 17.1%)

55.0
60.0 (↑ 5.0%)

83.3
85.2 (↑ 1.9%)

Table 4: Macro-F1 values of various charges on Criminal-S.

Datasets

Metrics

Our model

Criminal-S

Criminal-M

Criminal-L

Acc. MP MR

F1

Acc. MP MR

F1

Acc. MP MR

F1

w/o attention
w/o concatenation

93.4

93.5
93.5

66.7

63.4
59.3

69.2

60.1
59.0

64.9

60.0
57.2

94.4

94.7
95.0

68.3

68.8
64.6

69.2

58.2
62.4

67.1

60.9
62.5

95.8

94.9
95.7

75.8

70.9
69.4

73.7

54.4
64.5

73.1

58.6
65.4

Table 5: Experimental results of ablation test.

4.6 Ablation Test

Our method is characterized by the incorporation of attention mechanism and attribute-aware representa-
tions. Thus, we design ablation test respectively to investigate the effectiveness of these modules. When
taken off the attention mechanism, for each attribute we replace attention mechanism with a fully con-
nected layer. When taken off the attribute-aware representations (i.e., without concatenating the averaged

Task

Charge Prediction Attribute Prediction on physical injury

Ground Truth

Intentional Injury

Our model

Intentional Injury

LSTM-200

Affray

Yes

Yes

N/A

Table 6: Charge and attribute prediction result of the selected case.

attribute-aware representation), our method degrades into a typical multi-task learning based on LSTM
for both charge and attribute prediction.

As shown in Table 5, we can observe that the performance drops obviously after removing the attention
layer or the concatenation. The macro-F1 decreases at least 4%. Therefore, it can be seen that both
attention mechanism and attribute-aware fact representation play irreplaceable roles in our model.

4.7 Case Study

In this part, we select a representative case to give an intuitive illustration of how the predicted attributes
help to promote the performance of charge prediction. In this case, the defendant is convicted of inten-
tional injury. It is often hard to decide whether to judge a case as affray or intentional injury since they
are both related to violence. One important difference between them is that intentional injury has the
feature of physical injury, while affray does not.

So we believe the attribute physical injury is essential in the charge prediction of this case. As shown
in Table 6, our model correctly predicts the label of physical injury as Yes, and consequently predicts
the charge as intentional injury. In contrary, the model LSTM-200 predicts it as affray incorrectly. In
addition, we visual the heat map of this case when predicting the attribute intentionalinjury. Words
with deeper background color have higher attention weights. From this ﬁgure, we observe that the
attention mechanism can capture key patterns and semantics relevant to current attribute.

Figure 3: Visualization of attention mechanism.

.

5 Conclusion

In this work, we focus on the task of charge prediction according to the fact descriptions of criminal cases.
To address the problem of prediction few-shot and confusing charges, we introduce discriminative legal

attributes into consideration and propose a novel attribute-based multi-task learning model for charge
prediction. Speciﬁcally, our model learns attribute-free and attribute-aware fact representation jointly by
utilizing attribute-based attention mechanism.

In future, we will explore the following directions:
(1) There are more complicated criminal cases, such as multiple defendants and charges. Thus, it is

challenging to handle this general form of charge prediction.

(2) In this work, we only utilize several simple attributes of charges, while there exist more complex
essential conditions of charges. How to take full usage of essential conditions of charges is expected to
improve the interpretability of charge prediction models.

Acknowledgements

We thank all the anonymous reviewers for their insightful comments. This work is supported by the
National Natural Science Foundation of China (NSFC No. 61661146007, 61572273) and Tsinghua
University Initiative Scientiﬁc Research Program (20151080406). This research is part of the NExT++
project, supported by the National Research Foundation, Prime Ministers Ofﬁce, Singapore under its
IRC@Singapore Funding Initiative.

References

Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and Cordelia Schmid. 2013. Label-embedding for attribute-

based classiﬁcation. In Proceedings of CVPR, pages 819–826.

Baharum Baharudin, Lam Hong Lee, and Khairullah Khan. 2010. A review of machine learning algorithms for

text-documents classiﬁcation. JAIT, 1(1):4–20.

Antoine Bordes, Sumit Chopra, and Jason Weston. 2014. Question answering with subgraph embeddings. In

Proceedings of EMNLP, pages 615–620.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.

Natural language processing (almost) from scratch. JMLR, 12:2493–2537.

George E Dahl, Dong Yu, Li Deng, and Alex Acero. 2012. Context-dependent pre-trained deep neural networks

for large-vocabulary speech recognition. IEEE TASLP, 20(1):30–42.

Mohamed Elhoseiny, Babak Saleh, and Ahmed Elgammal. 2014. Write a classiﬁer: Zero-shot learning using

purely textual descriptions. In Proceedings of ICCV, pages 2584–2591.

Clement Farabet, Camille Couprie, Laurent Najman, and Yann LeCun. 2013. Learning hierarchical features for

scene labeling. IEEE TPAMI, 35(8):1915–1929.

Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior,
Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. 2012. Deep neural networks for acoustic modeling
in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82–
97.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.

Dinesh Jayaraman and Kristen Grauman. 2014. Zero-shot recognition with unreliable attributes. In Proceedings

of NIPS, pages 3464–3472.

S´ebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. 2015. On using very large target

vocabulary for neural machine translation. In Proceedings of ACL, volume 1, pages 1–10.

Daniel Martin Katz, Michael J Bommarito II, and Josh Blackman. 2017. A general approach for predicting the

behavior of the supreme court of the united states. PloS one, 12(4):e0174698.

R Keown. 1980. Mathematical models for legal prediction. Computer/Law Journal, 2:829.

Yoon Kim. 2014. Convolutional neural networks for sentence classiﬁcation. In Proceedings of EMNLP, pages

1746–1751.

Diederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of ICLR.

Fred Kort. 1957. Predicting supreme court decisions mathematically: A quantitative analysis of the ”right to

counsel” cases. American Political Science Review, 51(1):1–12.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classiﬁcation with deep convolutional

neural networks. In Proceedings of NIPS, pages 1097–1105.

Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. 2014. Attribute-based classiﬁcation for zero-shot

visual object categorization. IEEE TPAMI, 36(3):453–465.

Wan-Chen Lin, Tsung-Ting Kuo, Tung-Jia Chang, Chueh-An Yen, Chao-Ju Chen, and Shou-de Lin. 2012. Ex-
ploiting machine learning models for chinese legal documents labeling, case classiﬁcation, and sentencing pre-
diction. In Processdings of ROCLING, pages 140–141.

Chao-Lin Liu and Chwen-Dar Hsieh. 2006. Exploring phrase-based classiﬁcation of judicial documents for

criminal charges in chinese. In Proceedings of ISMIS, pages 681–690.

Chao-Lin Liu, Cheng-Tsung Chang, and Jim-How Ho. 2004. Case instance generation and reﬁnement for case-

based criminal summary judgments in chinese. JISE, pages 783–800.

Bingfeng Luo, Yansong Feng, Jianbo Xu, Xiang Zhang, and Dongyan Zhao. 2017. Learning to predict charges

for criminal cases with legal basis. In Proceedings of EMNLP, pages 2727–2736.

Ejan Mackaay and Pierre Robillard. 1974. Predicting judicial decisions: The nearest neighbour rule and visual

representation of case patterns. Datenverarbeitung im Recht, 3(3/4):302–331.

Tom´aˇs Mikolov, Anoop Deoras, Daniel Povey, Luk´aˇs Burget, and Jan ˇCernock`y. 2011. Strategies for training

large scale neural network language models. In Proceedings of ASRU workshop, pages 196–201.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of

words and phrases and their compositionality. In Proceedings of NIPS, pages 3111–3119.

Stuart S Nagel. 1963. Applying correlation analysis to case prediction. Texas Law Review, 42:1006.

Tara N Sainath, Abdel-rahman Mohamed, Brian Kingsbury, and Bhuvana Ramabhadran. 2013. Deep convolu-

tional neural networks for lvcsr. In Proceedings of ICASSP, pages 8614–8618.

Gerard Salton and Christopher Buckley. 1988. Term-weighting approaches in automatic text retrieval. Information

processing & management, 24(5):513–523.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout:

a simple way to prevent neural networks from overﬁtting. JMLR, 15(1):1929–1958.

Octavia Maria Sulea, Marcos Zampieri, Mihaela Vela, and Josef Van Genabith. 2017. Exploring the use of text

classi cation in the legal domain. In Proceedings of ASAIL workshop.

Maosong Sun, Xinxiong Chen, Kaixu Zhang, Zhipeng Guo, and Zhiyuan Liu. 2016. THULAC: An efﬁcient

lexical analyzer for chinese.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In

Proceedings of NIPS, pages 3104–3112.

Johan AK Suykens and Joos Vandewalle. 1999. Least squares support vector machine classiﬁers. Neural process-

ing letters, 9(3):293–300.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,
Vincent Vanhoucke, Andrew Rabinovich, Jen-Hao Rick Chang, et al. 2015. Going deeper with convolutions.
In Proceedings of CVPR, pages 1–9.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Document modeling with gated recurrent neural network for sentiment

classiﬁcation. In Proceedings of EMNLP, pages 1422–1432.

Jonathan J Tompson, Arjun Jain, Yann LeCun, and Christoph Bregler. 2014. Joint training of a convolutional

network and a graphical model for human pose estimation. In Proceedings of NIPS, pages 1799–1807.

Shuang Wu, Sravanthi Bondugula, Florian Luisier, Xiaodan Zhuang, and Pradeep Natarajan. 2014. Zero-shot
event detection using multi-modal fusion of weakly supervised concepts. In Proceedings of CVPR, pages 2665–
2672.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J Smola, and Eduard H Hovy. 2016. Hierarchical

attention networks for document classiﬁcation. In Proceedings of NAACL, pages 1480–1489.

Rowan Zellers and Yejin Choi. 2017. Zero-shot activity recognition with verb attribute induction. In Proceedings

of EMNLP, pages 946–958.

Few-Shot Charge Prediction with Discriminative Legal Attributes

Zikun Hu∗ Xiang Li∗ Cunchao Tu Zhiyuan Liu† Maosong Sun
Department of Computer Science and Technology, Tsinghua University
State Key Lab on Intelligent Technology and Systems, Tsinghua University
Beijing National Research Center for Information Science and Technology
{hzk14, x-l15}@mails.tsinghua.edu.cn
tucunchao@gmail.com, {liuzy, sms}@tsinghua.edu.cn

Abstract

Automatic charge prediction aims to predict the ﬁnal charges according to the fact descriptions
in criminal cases and plays a crucial role in legal assistant systems. Existing works on charge
prediction perform adequately on those high-frequency charges but are not yet capable of pre-
dicting few-shot charges with limited cases. Moreover, these exist many confusing charge pairs,
whose fact descriptions are fairly similar to each other. To address these issues, we introduce
several discriminative attributes of charges as the internal mapping between fact descriptions and
charges. These attributes provide additional information for few-shot charges, as well as effective
signals for distinguishing confusing charges. More speciﬁcally, we propose an attribute-attentive
charge prediction model to infer the attributes and charges simultaneously. Experimental results
on real-work datasets demonstrate that our proposed model achieves signiﬁcant and consistent
improvements than other state-of-the-art baselines. Speciﬁcally, our model outperforms other
baselines by more than 50% in the few-shot scenario. Our codes and datasets can be obtained
from https://github.com/thunlp/attribute_charge.

1

Introduction

The task of automatic charge prediction aims to train a machine judge to determine the ﬁnal charges (e.g.,
theft, robbery or trafﬁc offence.) of the defendants in criminal cases. As a representative subtask of legal
judgment prediction, charge prediction plays an important role in legal assistant systems and can beneﬁt
many real-world applications. For example, it can provide a handy reference for legal experts (e.g.,
lawyers and judges) and improve their working efﬁciency. Meanwhile, it can supply ordinary people
who are unfamiliar with legal terminology and complex procedures with legal consulting.

As a typical task in legal intelligence, automatic charge prediction has been studied for decades and
most existing works formalize this task under the text classiﬁcation framework. At the early stage,
researchers pay great efforts to extract efﬁcient features from text or case proﬁles. For example, some
works (Liu et al., 2004; Liu and Hsieh, 2006) utilize shallow textual features, including characters,
words, and phrases, to predict charges. Katz et al. (2017) predict the US Supreme Court’s decisions
with efﬁcient features extracted from case proﬁles (e.g., dates, locations, terms, and types). All these
approaches require numerous human effort to design features and annotate training instances. Besides,
Inspired by the successful usage of deep neural
these methods are hard to scale to other scenarios.
networks on natural language processing tasks (Kim, 2014; Baharudin et al., 2010; Tang et al., 2015),
researchers propose to employ deep neural networks to model legal documents. For example, Luo et al.
(2017) propose an attention-based neural network for charge prediction by incorporating the relevant law
articles.

However, charge prediction is still confronted with two major challenges which make it non-trivial:

∗ Indicates equal contribution.
† Corresponding Author.

This work is licensed under a Creative Commons Attribution 4.0 International License.
creativecommons.org/licenses/by/4.0/.

License details: http://

Figure 1: An illustration of the attribute-based charge prediction.

Few-Shot Charges. In practice, the case numbers of various charges are highly imbalanced. Accord-
ing to our statistics on a real-world dataset, the most frequent 10 charges (e.g., theft, intentional injury,
and trafﬁc offence.) cover 78.1% cases. On the contrary, the most low-frequency 50 (e.g., scalping relics,
disrupting the order of the court, and tax-escaping.) charges only cover less than 0.5% cases and most
of these charges own only around ten cases correspondingly. Previous works usually focus on these
common charges and ignore the few-shot ones. Though deep neural models advance feature-engineering
based charge prediction methods, they are unable to handle few-shot charges well due to the requirement
of sufﬁcient training data. Therefore, how to deal with these charges with limited cases is critical to a
robust and effective charge prediction system.

Confusing Charges. Besides few-shot charges, there also exist many confusing charge pairs, such as
(theft, robbery) and (misappropriation of funds, embezzlement). For each confusing pair, the deﬁnitions
of two charges only differ in the veriﬁcation of a speciﬁc act and the circumstances in corresponding
cases are usually similar to each other. As illustrated in Fig. 1, many robbery case also contain the act of
theft, and the existence of violence is the only key factor to distinguish these two charges. Thus, how to
capture the crucial factors for distinguishing confusing charges is another challenge of charge prediction.
To address these issues, we propose to introduce discriminative legal attributes of charges into consid-
eration and take these attributes as the internal mapping between facts and charge. More speciﬁcally, we
select 10 representative attributes of charges, including violence, proﬁt purpose, buying and selling and
so on. Afterwards, we conduct a low-cost category-level annotation, i.e., for each charge, we annotate
the value (including yes, no, or not available) of each attribute. This annotation indicates if an attribute
is the essential condition of a charge.

With the attribute annotation of charges, we propose a novel multi-task learning framework to predict
In this model, we employ attribute attention
the attributes and charges of each case simultaneously.
mechanism to capture the critical factual information relevant to a speciﬁc attribute. After that, we
combine these attribute-aware representations with an attribute-free fact representation to predict the
ﬁnal charges. There are two reasons for introducing legal attributes into our charge prediction model. On
one hand, these attributes can provide explicit knowledge about how to distinguish confusing charges.
On the other hand, these attributes are shared by all charges, and the knowledge can transfer from high-
frequency charges to low-frequency ones. Even for the few-shot charges, we can learn an efﬁcient
attribute-aware representation for prediction.

To investigate the advantage of our model on handling few-shot and confusing charges, we conduct
experiments on three real-world datasets of Chinese criminal cases. Experimental results demonstrate
that our model signiﬁcantly and consistently outperforms other state-of-the-art models on all datasets
and evaluation metrics. It is worth noting that, our model outperforms other baselines by more than 50%
for the few-shot charges.

To summarize, we make three main contributions as follows:
(1) We are the ﬁrst to focus on the few-shot and confusing problems in charge prediction. To address

these issues, we introduce legal attributes of charges into charge prediction task for the ﬁrst time.

(2) We propose a novel multi-task learning framework to infer the attributes and charges of a case
jointly. To achieve it, we employ attribute attention mechanism to learn attribute-aware fact representa-

tions.

(3) We conduct efﬁcient experiments on several real-world datasets, and our model signiﬁcantly out-

performs other baselines and achieves more than 50% improvements for few-shot charges.

2 Related Work

2.1 Zero-Shot Classiﬁcation

Our work is relevant to zero-shot classiﬁcation in computer vision. Many attribute-based models have
been proposed under this task since attributes are shared among different classes and can offer an in-
termediate representation. Lampert et al. (2014) introduces direct attribute prediction(DAP) and indirect
attribute prediction(IAP), and proposes attribute classiﬁers which can be pre-trained and dont need re-
training when ﬁnding new suitable object class. Akata et al. (2013) proposes to transform the task of
Jayaraman and Grauman (2014) introduces
attribute-based classiﬁcation to the label-embedding task.
a random forest method stressing the unreliability of attribute prediction for unseen classes. They also
extend it to the few-shot scenario.

Other than attributes, other external information can also be introduced to promote zero-shot classi-
ﬁcation. Elhoseiny et al. (2014) makes use of text description of the class label to transfer knowledge
between text features and visual features. Zero-shot learning has also been used in applications besides
object recognition, such as activity recognition (Zellers and Choi, 2017) and event recognition (Wu et
al., 2014).

2.2 Charge Prediction

Researchers in the legal area have been working on automatically making the legal judgment for a long
time. Kort (1957) applies quantitative methods to predict judgment by calculation numerical values for
factual elements. Nagel (1963) makes use of correlation analysis to make predictions for reapportioning
cases. Keown (1980) introduced mathematical models used for legal prediction such as linear models
and the scheme of nearest neighbors. These methods are usually mathematical or quantitative, and they
are restricted to a small dataset with few labels.

Since machine learning has been proven successful in many areas, researchers begin to formalize
charge prediction as a text classiﬁcation problem and make use of machine learning methods. Such
work usually focuses on feature extraction from the case fact. Lin et al. (2012) fetches 21 legal factor
labels for case classiﬁcation. Mackaay and Robillard (1974) extracts N-grams and topics created by
clustering semantically similar N-grams as features. Sulea et al. (2017) proposes a system based on
SVM ensembles using the case description, ruling and time span of a case as input. However, these
methods only extract shallow text features or manual labels which are hard to gather on a larger dataset.
Whats more, the conventional models could not catch the subtle difference between similar crimes, thus
they wouldnt perform well when the number of classes increases and more similar crimes appear.

With the successful usage of neural network methods on speech (Mikolov et al., 2011; Hinton et al.,
2012; Dahl et al., 2012; Sainath et al., 2013), computer vision (CV) (Krizhevsky et al., 2012; Farabet et
al., 2013; Tompson et al., 2014; Szegedy et al., 2015) and natural language processing (NLP) (Collobert
et al., 2011; Kim, 2014; Bordes et al., 2014; Sutskever et al., 2014; Jean et al., 2015; Yang et al., 2016),
researchers propose to employ neural models for legal tasks. Luo et al. (2017) proposes a hierarchical
attentional network to predict charges and extract relevant articles jointly. However, this work only
focuses on high-frequency charges, without paying attention to few-shot and confusing ones. To address
these issues, we propose an attention-based neural model by incorporating several discriminative legal
attributes.

3 Method

In this section, we propose a few-shot neural model which jointly models charge prediction task and legal
attribute prediction task in a uniﬁed framework. In the following parts, we ﬁrst introduce the discrimina-
tive charge attributes. Afterward, we give deﬁnitions of charge prediction and attribute prediction. Then

we describe the neural encoder of fact description and the attention-based attribute predictor. At last, we
show the output layer and the loss function of our model.

3.1 Discriminative Charge Attributes

To distinguish confusing charges and provide additional knowledge for few-shot charges, we introduce
10 discriminative attributes for all the charges in Chinese criminal law. The detailed descriptions of these
attributes are shown in Table 1. For each (charge, attribute) pair, it can be labeled as Yes, No or NA. For
example, the charge of manslaughter should be labeled as No on Intentional Crime, Yes on Death, NA
on State Organ. Note that, the fact-ﬁndings of a speciﬁc case can only be labeled as Yes or No. When
convicting someone of a certain crime, the facts should conform to the description of the certain charge.
Thus for a certain attribute, the label of a speciﬁc case and the label of the corresponding charge should
be the same or not in conﬂict. In other words, for a certain attribute, the label of a case and the charge
can only be (Yes, Yes), (No, No), (Yes, NA), or (No, NA). In practice, we conduct a low-cost annotation
and annotate the attributes of 149 distinct charges manually. Then, we assign each case with the same
attributes of its corresponding charge.

Attributes

Description

Proﬁt Purpose

Whether the criminal commits a crime on the purpose of getting proﬁt.

Buying and Selling Whether the criminal has buying or selling behavior during the commission of the

crime.

Whether death is caused by the criminal.

Whether the criminal has the act of violence.

Death

Violence

State Organ

Public Place

Whether the case or the charge involves State organ or any functionary of a State organ.

Whether the criminal commits a crime in a public place.

Illegal Possession

Whether the criminal commits a crime for the purpose of illegal possession.

Physical Injury

Whether a physical injury is caused by the criminal.

Intentional Crime Whether the criminal commits an intentional crime.

Production

Whether the criminal commits a crime during the production.

Table 1: The descriptions of selected attributes.

3.2 Formalizations

3.2.1 Charge Prediction
The fact description of a case can be seen as a word sequence x = {x1, . . . , xn}, where n represents the
sequence length, xi ∈ T , and T is a ﬁxed vocabulary. Given the fact description x, the charge prediction
task aims to predict a charge y ∈ Y from a charge set Y .

3.2.2 Attributes Prediction
The attributes prediction task can be regarded as a binary classiﬁcation task. It takes the same input
sequence x as in the charge prediction task, and aims to predict the fact-ﬁndings of attributes p =
{p1, . . . , pk} according to the fact. Here, k is the number of selected attributes, and pi ∈ {0, 1} is the
label for a certain attribute.

3.3 Fact Encoder

As illustrated in Fig. 2, fact encoder encodes the discrete input sequence into continuous hidden states.
Here, we employ conventional Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997)
as fact encoder due to its ability to extract semantic meanings. LSTM is a variation of RNN and is
capable of capturing long-term dependencies.

First, LSTM encoder converts each word xi ∈ x into its word embedding xi ∈ Rd, where d is
the dimension of word embeddings. Then, we get the corresponding word embedding sequence as

Figure 2: An illustration of the attribute-based charge prediction.

ˆx = {x1, . . . , xn}.

hidden state ht as follows:

At each time step t ∈ [1, n], the LSTM cell intakes xt, recalculates memory cell ct, and outputs new

ft = σ (Wf xt + Uf ht−1 + bf ) ,
it = σ (Wixt + Uiht−1 + bi) ,
ot = σ (Woxt + Uoht−1 + bo) ,
ˆct = tanh (Wcxt + Ucht−1 + bc) ,
ct = ft (cid:12) ct−1 + it (cid:12) ˆct,
ht = ot (cid:12) tanh (ct) .

Here, ft, it and ot represent forget gate, input gate, and output gate respectively. (cid:12) means element-wise
multiplication and σ is the sigmoid activation function. W , U , and b are weight matrices and bias vectors.
After processing all time steps, we get a hidden state sequence h = {h1, . . . , hn}. At last, we feed it
into a max-pooling layer to get the attribute-free representation e = [e1, . . . , es] as

ei = max(h1,i, · · · , hn,i), ∀i ∈ [1, s].

Here, s is the dimension of hidden states.

3.4 Attentive Attribute Predictor

Given the fact description x, the attribute predictor aims to predict the label of every attribute. Inspired
by (Yang et al., 2016), we employ an attention mechanism to select relevant information from facts and
generate attribute-aware fact representations.

As shown in Fig. 2, attribute predictor takes the hidden state sequence h = {h1, . . . , hn} as input. Our
attentive attribute predictor then calculates attention weights a = {a1, . . . , ak} for all attribute, where
ai = [ai,1, . . . , ai,n]. ∀i ∈ [1, k] and j ∈ [1, n], ai,j is calculated by:

ai,j =

exp(tanh(Wahj)T ui)
t exp(tanh(Waht)T ui)

.

(cid:80)

Here, ui is the context vector of the i-th attribute to calculate how informative an element is to the
attribute i, and Wa is a weight matrix that all attributes share. Afterwards, we get attribute-aware
representations of fact g = {g1, . . . , gk}, where gi = (cid:80)
t ai,tht. At last, with representations g, we
project it into the label space and use softmax function to get the ﬁnal prediction results p = [p1, . . . , pk],
where pi is the prediction result of attribute i and is calculated by:
i gi + bp
i ),

zi = softmax(Wp
pi = arg max(zi).

(1)

(2)

(3)

(4)

Here, zi is the prediction probability distribution on Yes and No. Wp
vector of attribute i.

i and bi are weight matrix and bias

3.5 Output Layer

To integrate the fact descriptions and fact-ﬁndings of all attributes, we use both attribute-free and
attribute-aware representations to predict the ﬁnal charge of a case in the output layer. The predicted
distribution y over all charges is calculated as follows:

r =

(cid:80)

i gi
k

,

v = e ⊕ r,
y = softmax(Wyv + by).

(5)

(6)

(7)

(8)

Here, r is the average of attribute-aware representations. r and e are concatenated into the ﬁnal fact
representation v. Wy and by are weight matrix and bias vector in the output layer.

3.6 Optimization

The training objective of our proposed model consists of two parts. The ﬁrst one is to minimize the
cross-entropy between predicted charge distribution y and the ground-truth distribution ˆy. The other one
is to minimize the cross-entropy between predicted distribution and the ground-truth fact-ﬁnding of each
attribute.

The charge prediction loss can be formalized as:

where yi is the ground-truth label, ˆyi is prediction probability, and C is the number of charges.

As each attribute is equally important in the model, we can easily calculate the attribution loss by sum
up the cross-entropy of all attributes. However, when the attribute of a speciﬁc charge is NA, the label of
the corresponding cases can be Yes or No. Therefore, we only add up the cross-entropy to the attribute
loss when this attribute of the charge belongs to Yes or No. At last, we formulate the attribute loss as:

Lcharge = −

yi · log(ˆyi),

C
(cid:88)

i=1

Lattr = −

zij · log(ˆzij),

k
(cid:88)

2
(cid:88)

Ii

i=1

j=1

where Ii is an indicator function. Ii = 1 if the i-th attribute of current charge is labeled as Yes or No, and
Ii = 0 otherwise. Obviously, zi is the ground-truth label, and ˆzi is predicted probabilities distribution on
Yes and No.

Considering the two objectives, our ﬁnal loss function L is obtained by adding Lcharge and Lattr as

where α is a hyper-parameter to balance the weight of the two parts in the loss function.

L = Lcharge + α · Lattr,

follows:

4 Experiments

In order to investigate the effectiveness of our model on criminal charges prediction, we conduct experi-
ments on several real-world datasets and compare our model with several state-of-the-art baselines.

4.1 Dataset Construction

Since there are no publicly available datasets in previous works for charges prediction, we collect crim-
inal cases published by the Chinese government from China Judgments Online1. As each case is well-
structured and divided into several parts such as fact, court view, and penalty result, we select the fact

1http://wenshu.court.gov.cn.

part of each case as our input. Besides, we can easily extract the charges from the penalty result by
regular expression. We have manually checked the extracted charges and there are few mistakes.

Although there are some cases that contain multiple defendants and multiple charges in real-world,
considering the task would be too complex to solve if these cases contained, we removed the cases
which have more than one charges in a verdict. Besides, in order to examine the performance of our
method on few-shot charges, we keep 149 distinct charges (near 3 times as compared with (Luo et al.,
2017)) with at least 10 cases.

After preprocessing, we randomly select about 400, 000 cases and construct three datasets with dif-
ferent scales, denoted as Criminal-S(small), Criminal-M(medium) and Criminal-L(large). The three
different datasets contain the same number of charges but the different number of cases. The detailed
statistics are shown in Table 2.

Datasets Criminal-S Criminal-M Criminal-L

train
test
valid

61, 589
7, 702
7, 755

153, 521
19, 189
19, 250

306, 900
38, 368
38, 429

Table 2: The statistics of different datasets.

4.2 Attribute Selection and Annotation

As mentioned in previous part, we propose to introduce discriminative attributes to enhance charge pre-
diction. To select these attributes, we ﬁrst train a LSTM based charge prediction model and obtain the
confusion matrix of predicted charges on validation set. Then, we ﬁlter out the confusing charge pairs
and provide them to three master students majoring in criminal. According to these confusing charge
pairs, they deﬁne 10 representative attributes to distinguish these confusing pairs.

With the selected 10 attributes, we conduct a low-cost annotation over all charges. Here, the low-cost
annotation means we only need to annotate 10 attributes for 149 charges manually, rather than all cases.
As the selected attributes are discriminative and unambiguous, we asked these annotators to reach an
agreement for each annotation. Totally, we spent less than 10 hours for annotation.

4.3 Baselines

We employ several typical text classiﬁcation models and one charge predicting model as baselines:

TFIDF+SVM: We implement term-frequency inverse document frequency (TFIDF) (Salton and
Buckley, 1988) to extract features of inputs, and employ SVM (Suykens and Vandewalle, 1999) as the
classiﬁer.

CNN: We implement the CNN with multiple ﬁlter widths (Kim, 2014) as text classiﬁer.
LSTM: We implement a two-layer LSTM (Hochreiter and Schmidhuber, 1997) with a max-pooling

layer as the fact encoder.

Fact-Law Attention Model: Luo et al. (2017) propose an attention-based neural charge prediction

model by incorporating relevant law articles.

4.4 Experiment Settings and Evaluation Metrics

As all the case documents are written in Chinese without word cutting, we employ THULAC (Sun et
al., 2016) for word segmentation and set the maximum document length to 500. For the TFIDF+SVM
model, we set the feature size to 2, 000. For other neural models, we employ Skip-Gram model (Mikolov
et al., 2013) to pre-train word embeddings with the embedding size of 100. We set the hidden state size
of LSTM to 100.For the CNN based models, we set the ﬁlter widths to (2, 3, 4, 5) with each ﬁlter size to
25 for consistency. The weight α of the attribute loss is set to 1.

Note that, the representation size of our model turns into 200 after concatenation. For a fair com-
parison, we add a 100 × 200 fully connected layer between after the pooling layer in CNN and LSTM,

denoted as CNN-200 and LSTM-200.

We use Adam (Kingma and Ba, 2015) as the optimizer, and set the learning rate to 0.001, the dropout
rate (Srivastava et al., 2014) to 0.5 and the batch size to 64. We employ accuracy (Acc.), macro-precision
(MP), macro-recall (MR) and macro-F1 as our evaluation metrics.

4.5 Results and Analysis

Datasets

Criminal-S

Criminal-M

Criminal-L

Metrics

Acc. MP MR

F1

Acc. MP MR

F1

Acc. MP MR

F1

TFIDF+SVM 85.8
91.9
92.6
93.5
92.7
92.8

CNN
CNN-200
LSTM
LSTM-200
Fact-Law Att.

Our Model

93.4

49.7
50.5
51.1
59.4
60.0
57.0

66.7

41.9
44.9
46.3
58.6
58.4
53.9

69.2

43.5
46.1
47.3
57.3
57.0
53.4

64.9

89.6
93.5
92.8
94.7
94.4
94.7

94.4

58.8
57.6
56.2
65.8
66.5
66.7

68.3

5 0.1
48.1
50.0
63.0
62.4
60.4

69.2

52.1
50.5
50.8
62.6
62.7
61.8

67.1

91.8
93.9
94.1
95.5
95.1
95.7

95.8

67.5
66.0
61.9
69.8
72.8
73.3

75.8

54.1
50.3
50.0
67.0
66.7
67.1

73.7

57.5
54.7
53.1
66.8
67.9
68.6

73.1

Table 3: Charge prediction results of three datasets.

As shown in Table 3, we can observe that our model signiﬁcantly and consistently outperforms all
the baselines. Almost all existing methods perform poorly under the macro-F1 metric, which indicates
their shortage of predicting few-shot charges. Conversely, our model achieves promising improvements
(7.9%, 4.4%, and 5.2% absolutely on three datasets respectively), which demonstrates the robustness
and effectiveness of our model.

To further verify the advance of our model on dealing with few-shot charges, we show the performance
on charges with different frequencies. As shown in Table 4, we divide the charges into three parts
according to their frequencies. Here, the charges with ≤ 10 cases are low-frequency, and the charges
with > 100 cases are high-frequency. From this table, we ﬁnd that our model achieves more than 50%
improvements than baseline method for the low-frequency (i.e., few-shot) charges, which veriﬁes the
effectiveness of our model on handling few-shot issues.

Charge Type

Low frequency Medium frequency High frequency

Charge Number

49

51

49

LSTM-200
Our Model

32.6
49.7 (↑ 17.1%)

55.0
60.0 (↑ 5.0%)

83.3
85.2 (↑ 1.9%)

Table 4: Macro-F1 values of various charges on Criminal-S.

Datasets

Metrics

Our model

Criminal-S

Criminal-M

Criminal-L

Acc. MP MR

F1

Acc. MP MR

F1

Acc. MP MR

F1

w/o attention
w/o concatenation

93.4

93.5
93.5

66.7

63.4
59.3

69.2

60.1
59.0

64.9

60.0
57.2

94.4

94.7
95.0

68.3

68.8
64.6

69.2

58.2
62.4

67.1

60.9
62.5

95.8

94.9
95.7

75.8

70.9
69.4

73.7

54.4
64.5

73.1

58.6
65.4

Table 5: Experimental results of ablation test.

4.6 Ablation Test

Our method is characterized by the incorporation of attention mechanism and attribute-aware representa-
tions. Thus, we design ablation test respectively to investigate the effectiveness of these modules. When
taken off the attention mechanism, for each attribute we replace attention mechanism with a fully con-
nected layer. When taken off the attribute-aware representations (i.e., without concatenating the averaged

Task

Charge Prediction Attribute Prediction on physical injury

Ground Truth

Intentional Injury

Our model

Intentional Injury

LSTM-200

Affray

Yes

Yes

N/A

Table 6: Charge and attribute prediction result of the selected case.

attribute-aware representation), our method degrades into a typical multi-task learning based on LSTM
for both charge and attribute prediction.

As shown in Table 5, we can observe that the performance drops obviously after removing the attention
layer or the concatenation. The macro-F1 decreases at least 4%. Therefore, it can be seen that both
attention mechanism and attribute-aware fact representation play irreplaceable roles in our model.

4.7 Case Study

In this part, we select a representative case to give an intuitive illustration of how the predicted attributes
help to promote the performance of charge prediction. In this case, the defendant is convicted of inten-
tional injury. It is often hard to decide whether to judge a case as affray or intentional injury since they
are both related to violence. One important difference between them is that intentional injury has the
feature of physical injury, while affray does not.

So we believe the attribute physical injury is essential in the charge prediction of this case. As shown
in Table 6, our model correctly predicts the label of physical injury as Yes, and consequently predicts
the charge as intentional injury. In contrary, the model LSTM-200 predicts it as affray incorrectly. In
addition, we visual the heat map of this case when predicting the attribute intentionalinjury. Words
with deeper background color have higher attention weights. From this ﬁgure, we observe that the
attention mechanism can capture key patterns and semantics relevant to current attribute.

Figure 3: Visualization of attention mechanism.

.

5 Conclusion

In this work, we focus on the task of charge prediction according to the fact descriptions of criminal cases.
To address the problem of prediction few-shot and confusing charges, we introduce discriminative legal

attributes into consideration and propose a novel attribute-based multi-task learning model for charge
prediction. Speciﬁcally, our model learns attribute-free and attribute-aware fact representation jointly by
utilizing attribute-based attention mechanism.

In future, we will explore the following directions:
(1) There are more complicated criminal cases, such as multiple defendants and charges. Thus, it is

challenging to handle this general form of charge prediction.

(2) In this work, we only utilize several simple attributes of charges, while there exist more complex
essential conditions of charges. How to take full usage of essential conditions of charges is expected to
improve the interpretability of charge prediction models.

Acknowledgements

We thank all the anonymous reviewers for their insightful comments. This work is supported by the
National Natural Science Foundation of China (NSFC No. 61661146007, 61572273) and Tsinghua
University Initiative Scientiﬁc Research Program (20151080406). This research is part of the NExT++
project, supported by the National Research Foundation, Prime Ministers Ofﬁce, Singapore under its
IRC@Singapore Funding Initiative.

References

Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and Cordelia Schmid. 2013. Label-embedding for attribute-

based classiﬁcation. In Proceedings of CVPR, pages 819–826.

Baharum Baharudin, Lam Hong Lee, and Khairullah Khan. 2010. A review of machine learning algorithms for

text-documents classiﬁcation. JAIT, 1(1):4–20.

Antoine Bordes, Sumit Chopra, and Jason Weston. 2014. Question answering with subgraph embeddings. In

Proceedings of EMNLP, pages 615–620.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.

Natural language processing (almost) from scratch. JMLR, 12:2493–2537.

George E Dahl, Dong Yu, Li Deng, and Alex Acero. 2012. Context-dependent pre-trained deep neural networks

for large-vocabulary speech recognition. IEEE TASLP, 20(1):30–42.

Mohamed Elhoseiny, Babak Saleh, and Ahmed Elgammal. 2014. Write a classiﬁer: Zero-shot learning using

purely textual descriptions. In Proceedings of ICCV, pages 2584–2591.

Clement Farabet, Camille Couprie, Laurent Najman, and Yann LeCun. 2013. Learning hierarchical features for

scene labeling. IEEE TPAMI, 35(8):1915–1929.

Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior,
Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. 2012. Deep neural networks for acoustic modeling
in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82–
97.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.

Dinesh Jayaraman and Kristen Grauman. 2014. Zero-shot recognition with unreliable attributes. In Proceedings

of NIPS, pages 3464–3472.

S´ebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. 2015. On using very large target

vocabulary for neural machine translation. In Proceedings of ACL, volume 1, pages 1–10.

Daniel Martin Katz, Michael J Bommarito II, and Josh Blackman. 2017. A general approach for predicting the

behavior of the supreme court of the united states. PloS one, 12(4):e0174698.

R Keown. 1980. Mathematical models for legal prediction. Computer/Law Journal, 2:829.

Yoon Kim. 2014. Convolutional neural networks for sentence classiﬁcation. In Proceedings of EMNLP, pages

1746–1751.

Diederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of ICLR.

Fred Kort. 1957. Predicting supreme court decisions mathematically: A quantitative analysis of the ”right to

counsel” cases. American Political Science Review, 51(1):1–12.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classiﬁcation with deep convolutional

neural networks. In Proceedings of NIPS, pages 1097–1105.

Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. 2014. Attribute-based classiﬁcation for zero-shot

visual object categorization. IEEE TPAMI, 36(3):453–465.

Wan-Chen Lin, Tsung-Ting Kuo, Tung-Jia Chang, Chueh-An Yen, Chao-Ju Chen, and Shou-de Lin. 2012. Ex-
ploiting machine learning models for chinese legal documents labeling, case classiﬁcation, and sentencing pre-
diction. In Processdings of ROCLING, pages 140–141.

Chao-Lin Liu and Chwen-Dar Hsieh. 2006. Exploring phrase-based classiﬁcation of judicial documents for

criminal charges in chinese. In Proceedings of ISMIS, pages 681–690.

Chao-Lin Liu, Cheng-Tsung Chang, and Jim-How Ho. 2004. Case instance generation and reﬁnement for case-

based criminal summary judgments in chinese. JISE, pages 783–800.

Bingfeng Luo, Yansong Feng, Jianbo Xu, Xiang Zhang, and Dongyan Zhao. 2017. Learning to predict charges

for criminal cases with legal basis. In Proceedings of EMNLP, pages 2727–2736.

Ejan Mackaay and Pierre Robillard. 1974. Predicting judicial decisions: The nearest neighbour rule and visual

representation of case patterns. Datenverarbeitung im Recht, 3(3/4):302–331.

Tom´aˇs Mikolov, Anoop Deoras, Daniel Povey, Luk´aˇs Burget, and Jan ˇCernock`y. 2011. Strategies for training

large scale neural network language models. In Proceedings of ASRU workshop, pages 196–201.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of

words and phrases and their compositionality. In Proceedings of NIPS, pages 3111–3119.

Stuart S Nagel. 1963. Applying correlation analysis to case prediction. Texas Law Review, 42:1006.

Tara N Sainath, Abdel-rahman Mohamed, Brian Kingsbury, and Bhuvana Ramabhadran. 2013. Deep convolu-

tional neural networks for lvcsr. In Proceedings of ICASSP, pages 8614–8618.

Gerard Salton and Christopher Buckley. 1988. Term-weighting approaches in automatic text retrieval. Information

processing & management, 24(5):513–523.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout:

a simple way to prevent neural networks from overﬁtting. JMLR, 15(1):1929–1958.

Octavia Maria Sulea, Marcos Zampieri, Mihaela Vela, and Josef Van Genabith. 2017. Exploring the use of text

classi cation in the legal domain. In Proceedings of ASAIL workshop.

Maosong Sun, Xinxiong Chen, Kaixu Zhang, Zhipeng Guo, and Zhiyuan Liu. 2016. THULAC: An efﬁcient

lexical analyzer for chinese.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In

Proceedings of NIPS, pages 3104–3112.

Johan AK Suykens and Joos Vandewalle. 1999. Least squares support vector machine classiﬁers. Neural process-

ing letters, 9(3):293–300.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,
Vincent Vanhoucke, Andrew Rabinovich, Jen-Hao Rick Chang, et al. 2015. Going deeper with convolutions.
In Proceedings of CVPR, pages 1–9.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Document modeling with gated recurrent neural network for sentiment

classiﬁcation. In Proceedings of EMNLP, pages 1422–1432.

Jonathan J Tompson, Arjun Jain, Yann LeCun, and Christoph Bregler. 2014. Joint training of a convolutional

network and a graphical model for human pose estimation. In Proceedings of NIPS, pages 1799–1807.

Shuang Wu, Sravanthi Bondugula, Florian Luisier, Xiaodan Zhuang, and Pradeep Natarajan. 2014. Zero-shot
event detection using multi-modal fusion of weakly supervised concepts. In Proceedings of CVPR, pages 2665–
2672.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J Smola, and Eduard H Hovy. 2016. Hierarchical

attention networks for document classiﬁcation. In Proceedings of NAACL, pages 1480–1489.

Rowan Zellers and Yejin Choi. 2017. Zero-shot activity recognition with verb attribute induction. In Proceedings

of EMNLP, pages 946–958.

