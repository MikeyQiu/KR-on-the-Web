Question Condensing Networks for Answer Selection in Community
Question Answering

Wei Wu1, Xu Sun1, Houfeng Wang1,2

1MOE Key Lab of Computational Linguistics, Peking University, Beijing, 100871, China
2Collaborative Innovation Center for Language Ability, Xuzhou, Jiangsu, 221009, China
{wu.wei, xusun, wanghf}@pku.edu.cn

Abstract

Answer selection is an important subtask
of community question answering (CQA).
In a real-world CQA forum, a question
is often represented as two parts: a sub-
ject that summarizes the main points of
the question, and a body that elaborates on
the subject in detail. Previous researches
on answer selection usually ignored the
difference between these two parts and
concatenated them as the question repre-
sentation.
In this paper, we propose the
Question Condensing Networks (QCN) to
make use of the subject-body relationship
In this model,
of community questions.
the question subject is the primary part of
the question representation, and the ques-
tion body information is aggregated based
on similarity and disparity with the ques-
tion subject. Experimental results show
that QCN outperforms all existing models
on two CQA datasets.

1

Introduction

Community question answering (CQA) has seen a
spectacular increase in popularity in recent years.
With the advent of sites like Stack Overﬂow1 and
Quora2, more and more people can freely ask any
question and expect a variety of answers. With
the inﬂux of new questions and the varied qual-
ity of provided answers, it is very time-consuming
for a user to inspect them all. Therefore, develop-
ing automated tools to identify good answers for a
question is of practical importance.

A typical example for CQA is shown in Table 1.
In this example, Answer 1 is a good answer, be-
cause it provides helpful information, e.g., “check

1https://stackoverflow.com/
2https://www.quora.com/

it to the trafﬁc dept”. Although Answer 2 is rele-
vant to the question, it does not contain any useful
information so that it should be regarded as a bad
answer.

From this example, we can observe two charac-
teristics of CQA that ordinary QA does not pos-
sess. First, a question includes both a subject that
gives a brief summary of the question and a body
that describes the question in detail. The question-
ers usually convey their main concern and key in-
formation in the question subject. Then, they pro-
vide more extensive details about the subject, seek
help, or express gratitude in the question body.
Second, the problem of redundancy and noise is
prevalent in CQA (Zhang et al., 2017). Both ques-
tions and answers contain auxiliary sentences that
do not provide meaningful information.

Previous researches (Tran et al., 2015; Joty
et al., 2016) usually treat each word equally in
the question and answer representation. How-
ever, due to the redundancy and noise problem,
only part of text from questions and answers is
useful to determine the answer quality. To make
things worse, they ignored the difference between
question subject and body, and simply concate-
nated them as the question representation. Due
to the subject-body relationship described above,
this simple concatenation can aggravate the re-
dundancy problem in the question. In this paper,
we propose the Question Condensing Networks
(QCN) to address these problems.

In order to utilize the subject-body relationship
in community questions, we propose to treat the
question subject as the primary part of the ques-
tion, and aggregate the question body information
based on similarity and disparity with the ques-
tion subject. The similarity part corresponds to
the information that exists in both question sub-
ject and body, and the disparity part corresponds
to the additional information provided by the ques-

Question Subject Checking the history of the car.

Question body

How can one check the history of the car like maintenance, accident or service
history. In every advertisement of the car, people used to write “Accident Free", but
in most cases, car have at least one or two accident, which is not easily detectable
through Car Inspection Company. Share your opinion in this regard.
Depends on the owner of the car.. if she/he reported the accident/s i believe u can
check it to the trafﬁc dept.. but some owners are not doing that especially if its only
a small accident.. try ur luck and go to the trafﬁc dept..
How about those who claim a low mileage by tampering with the car fuse box? In
my sense if you’re not able to detect traces of an accident then it is probably not
worth mentioning... For best results buy a new car :)

Answer1

Answer2

Table 1: An example question and its related answers in CQA. The text is shown in its original form,
which may contain errors in typing.

tion body. Both information can be important for
question representation.
In our model, they are
processed separately and the results are combined
to form the ﬁnal question representation.

In order to reduce the impact of redundancy
and noise in both questions and answers, we pro-
pose to align the question-answer pairs using the
multi-dimensional attention mechanism. Differ-
ent from previous attention mechanisms that com-
pute a scalar score for each token pair, multi-
dimensional attention, ﬁrst proposed in Shen et al.
(2018), computes one attention score for each di-
mension of the token embedding. Therefore, it can
select the features that can best describe the word’s
speciﬁc meaning in the given context. Therefore,
we can learn the interaction between questions and
answers more accurately.

The main contributions of our work can be sum-

marized as follows:

• We propose to treat the question subject and
the question body separately in community
question answering. We treat the question
subject as the primary part of the question,
and aggregate the question body information
based on similarity and disparity with the
question subject.

• We introduce a new method that uses the
multi-dimensional attention mechanism to
align question-answer pair. With this at-
tention mechanism, the interaction between
questions and answers can be learned more
accurately.

performance on two SemEval CQA datasets,
outperforming all exisiting SOTA models by
a large margin, which demonstrates the effec-
tiveness of our model.3

2 Task Description

A community question answering consists of four
parts, which can be formally deﬁned as a tuple of
four elements (S, B, C, y). S = [s1, s2, ..., sl] de-
notes the subject of a question whose length is l,
where each si is a one-hot vector whose dimen-
sion equals the size of the vocabulary. Similarly,
B = [b1, b2, ..., bm] denotes the body of a ques-
tion whose length is m. C = [c1, c2, ..., cn] de-
notes an answer corresponding to that question
whose length is n. y ∈ Y is the label represent-
ing the degree to which it can answer that ques-
tion. Y = {Good, PotentiallyUseful, Bad} where
Good indicates the answer can answer that ques-
tion well, PotentiallyUseful indicates the answer
is potentially useful to the user, and Bad indi-
cates the answer is just bad or useless. Given
{S, B, C}, the task of CQA is to assign a label to
each answer based on the conditional probability
P r(y|S, B, C).

3 Proposed Model

In this paper, we propose Question Condensing
Networks (QCN) which is composed of the fol-
lowing modules. The overall architecture of our
model is illustrated in Figure 1.

• Our proposed Question Condensing Net-
works (QCN) achieves the state-of-the-art

3An implementation of our model is available at https:

//github.com/pku-wuwei/QCN.

Figure 1: Architecture for Question Condensing Network (QCN). Each block represents a vector.

3.1 Word-Level Embedding

Word-level embeddings are composed of two
components: GloVe (Pennington et al., 2014)
word vectors trained on the domain-speciﬁc unan-
notated corpus provided by the task 4, and con-
volutional neural network-based character embed-
dings which are similar to (Kim et al., 2016). Web
text in CQA forums differs largely from normal-
ized text in terms of spelling and grammar, so
speciﬁcally trained GloVe vectors can model word
interactions more precisely. Character embedding
has proven to be very useful for out-of-vocabulary
(OOV) words, so it is especially suitable for noisy
web text in CQA.

We concatenate these two embedding vectors
for every word to generate word-level embeddings
Semb ∈ Rd×l, Bemb ∈ Rd×m, Cemb ∈ Rd×n,
where d is the word-level embedding size.

3.2 Question Condensing

In this section, we condense the question repre-
sentation using subject-body relationship. In most
cases, the question subject can be seen as a sum-
mary containing key points of the question, the
question body is relatively lengthy in that it needs
to explain the key points and add more details
about the posted question. We propose to cheat the
question subject as the primary part of the question
representation, and aggregate question body infor-
mation from two perspectives: similarity and dis-
parity with the question subject. To achieve this
goal, we use an orthogonal decomposition strat-
egy, which is ﬁrst proposed by Wang et al. (2016),
to decompose each question body embedding into
a parallel component and an orthogonal compo-

4http://alt.qcri.org/semeval2015/

task3/index.php?id=data-and-tools

nent based on every question subject embedding:
bj
emb · si
si
emb · si
orth = bj
bi,j
emb − bi,j

bi,j
para =

si
emb

para

emb

emb

(2)

(1)

All vectors in the above equations are of length d.
Next we describe the process of aggregating the
question body information based on the parallel
component in detail. The same process can be ap-
plied to the orthogonal component, so at the end
of the fusion gate we can obtain Sorth and Sorth
respectively.

The decomposed components

are passed
through a fully connected layer to compute the
multi-dimensional attention weights. Here we use
the scaled tanh activation, which is similar to Shen
et al. (2018), to prevent large difference among
scores while it still has a range large enough for
output:
para = c · tanh (cid:0)(cid:2)Wp1bi,j
ai,j

para + bp1

(cid:3) /c(cid:1)

(3)

where Wp1 ∈ Rd×d and bp1 ∈ Rd are parame-
ters to be learned, and c is a hyper-parameter to be
tuned.

The obtained word-level alignment

tensor
Apara ∈ Rd×l×m is then normalized along the
third dimension to produce the attention weights
over the question body for each word in the ques-
tion subject. The output of this attention mecha-
nism is a weighted sum of the question body em-
beddings for each word in the question subject:

wi,j

para =

(cid:16)

exp

(cid:17)

ai,j
para
(cid:16)

(cid:17)

ai,j
para

(cid:80)m

j=1 exp

si
ap =

wi,j

para (cid:12) bj

emb

m
(cid:88)

j=1

(4)

(5)

where (cid:12) means point-wise product. This multi-
dimensional attention mechanism has the advan-
tage of selecting features of a word that can best
describe the word’s speciﬁc meaning in the given
context. In order to determine the importance be-
tween the original word in the question subject and
the aggregated information from the question body
with respect to this word, a fusion gate is utilized
to combine these two representations:

Fpara = σ (Wp2Semb + Wp3Sap + bp2)
(6)
Spara = Fpara (cid:12) Semb + (1 − Fpara) (cid:12) Sap (7)

where Wp2, Wp3 ∈ Rd×d, and bp2 ∈ Rd
are learnable parameters of the fusion gate, and
Fpara, Semb, Sap, Spara ∈ Rd×l. The ﬁnal ques-
tion representation Srep ∈ R2d×l is obtained by
concatenating Spara and Sorth along the ﬁrst di-
mension.

3.3 Answer Preprocessing

This module has two purposes. First, we try to
map each answer word from embedding space
Cemb ∈ Rd×n to the same interaction space
Crep ∈ R2d×n as the question. Second, similar to
Wang and Jiang (2017), a gate is utilized to con-
trol the importance of different answer words in
determining the question-answer relation:

and the answer representation :

(9)

(10)

˜ai,j
align = Wa1si
ai,j
align = c · tanh

(cid:16)

rep + Wa2cj
rep + ba
(cid:17)
˜ai,j
align/c
(cid:16)
ai,j
align
(cid:16)
ai,j
align
(cid:17)

j=1 exp
(cid:16)

exp

(cid:17)

(cid:80)n

exp

ai,j
align
(cid:16)
ai,j
align

(cid:80)l

i=1 exp

si
ai =

cj
ai =

n
(cid:88)

j=1

l
(cid:88)

i=1

(cid:17) (cid:12) cj

rep

(11)

(cid:17) (cid:12) si

rep

(12)

where Wa1, Wa2 ∈ R2d×2d and ba ∈ R2d are
parameters to be learned. To attenuate the effect
of incorrect attendance, input and output of this
attention mechanism are concatenated and fed to
the subsequent layer. Finally, we obtain the ques-
tion and answer representation Satt ∈ R4d×l =
[Srep; Sai], Catt ∈ R4d×n = [Crep; Cai].

3.5

Interaction Summarization

In this layer, the multi-dimensional self-attention
mechanism is employed to summarize two se-
quences of vectors (Satt and Catt) into two ﬁxed-
length vectors ssum ∈ R4d and csum ∈ R4d.

As = Ws2tanh (Ws1Satt + bs1) + bs2

(13)

ssum =

n
(cid:88)

i=1

(cid:1)

exp (cid:0)ai
s
i=1 exp (ai
s)

(cid:80)n

(cid:12) si

att

(14)

where Ws1, Ws2 ∈ R4d×4d and bs1, bs2 ∈ R4d are
parameters to be learned. The same process can
be applied to Catt and obtain csum.

In this component, ssum and csum are concate-
nated and fed into a two-layer feed-forward neural
network. At the end of the last layer, the sof tmax
function is applied to obtain the conditional prob-
ability distribution P r(y|S, B, C).

4 Experimental Setup

4.1 Datasets

Crep =σ (Wc1Cemb + bc1) (cid:12)
tanh (Wc2Cemb + bc2)

3.6 Prediction

(8)

where Wc1, Wc2 ∈ Rd×2d and bc1, bc2 ∈ R2d are
parameters to be learned.

3.4 Question Answer Alignment

We apply the multi-dimensional attention mech-
anism to the question and answer representa-
tion Srep and Crep to obtain word-level align-
ment tensor Aalign ∈ R2d×l×n. Similar to the
multi-dimensional attention mechanism described
above, we can compute attention weights and
weighted sum for both the question representation

We use two community question answering
datasets from SemEval (Nakov et al., 2015, 2017)
to evaluate our model. The statistics of these
datasets are listed in Table 2. The corpora contain
data from the QatarLiving forum 5, and are pub-
licly available on the task website. Each dataset

5http://www.qatarliving.com/forum

Statistics

Number of questions
Number of answers
Average length of subject
Average length of body
Average length of answer

SemEval 2015
Dev
266
1447
6.08
39.47
33.90

Train
2376
15013
6.36
39.26
35.82

Test
300
1793
6.24
39.53
37.33

SemEval 2017
Dev
327
3270
6.16
47.98
37.30

Train
5124
38638
6.38
43.01
37.67

Test
293
2930
5.76
54.06
39.50

Table 2: Statistics of two CQA datasets. We can see from the statistics that the question body is much
lengthier than the question subject. Thus, it is necessary to condense the question representation.

consists of questions and a list of answers for each
question, and each question consists of a short ti-
tle and a more detailed description. There are also
some metadata associated with them, e.g., user ID,
date of posting, and the question category. We do
not use the metadata because they failed to boost
performance in our model. Since the SemEval
2017 dataset is an updated version of SemEval
2016 6, and shares the same evaluation metrics
with SemEval 2016, we choose to use the SemEval
2017 dataset for evaluation.

4.2 Evaluation Metrics

In order to facilitate comparison, we adopt the
evaluation metrics used in the ofﬁcial task or prior
work. For the SemEval 2015 dataset, the ofﬁ-
cial scores are macro-averaged F1 and accuracy
over three categories. However, many recent re-
searches (Barrón-Cedeño et al., 2015; Joty et al.,
2015, 2016) switched to a binary classiﬁcation set-
ting, i.e., identifying Good vs. Bad answers. Be-
cause binary classiﬁcation is much closer to a real-
world CQA application. Besides, the Potential-
lyUseful class is both the smallest and the noisiest
class, making it the hardest to predict. To make
it worse, its impact is magniﬁed by the macro-
averaged F1. Therefore, we adopt the F1 score
and accuracy on two categories for evaluation.

SemEval 2017 regards answer selection as a
ranking task, which is closer to the application sce-
nario. As a result, mean average precision (MAP)
is used as an evaluation measure. For a perfect
ranking, a system has to place all Good answers
above the PotentiallyUseful and Bad answers. The
latter two are not actually distinguished and are
considered Bad in terms of evaluation. Addition-

6The SemEval 2017 dataset provides all the data from
2016 for training , and fresh data for testing, but it does not
include a development set. Following previous work (Filice
et al., 2017), we use the 2016 ofﬁcial test set as the develop-
ment set.

ally, standard classiﬁcation measures like accuracy
and F1 score are also reported.

4.3

Implementation Details

We use the tokenizer from NLTK (Bird, 2006)
to preprocess each sentence. All word embed-
dings in the sentence encoder layer are initial-
ized with the 300-dimensional GloVe (Pennington
et al., 2014) word vectors trained on the domain-
speciﬁc unannotated corpus, and embeddings for
out-of-vocabulary words are set to zero. We use
the Adam Optimizer (Kingma and Ba, 2014) for
optimization with a ﬁrst momentum coefﬁcient of
0.9 and a second momentum coefﬁcient of 0.999.
We perform a small grid search over combina-
tions of initial learning rate [1 × 10−6, 3 × 10−6,
1 × 10−5], L2 regularization parameter [1 × 10−7,
3 × 10−7, 1 × 10−6], and batch size [8, 16, 32].
We take the best conﬁguration based on perfor-
mance on the development set, and only evalu-
ate that conﬁguration on the test set. In order to
mitigate the class imbalance problem, median fre-
quency balancing Eigen and Fergus (2015) is used
to reweight each class in the cross-entropy loss.
Therefore, the rarer a class is in the training set, the
larger weight it will get in the cross entropy loss.
Early stopping is applied to mitigate the problem
of overﬁtting. For the SemEval 2017 dataset, the
conditional probability over the Good class is used
to rank all the candidate answers.

5 Experimental Results

In this section, we evaluate our QCN model on two
community question answering datasets from Se-
mEval shared tasks.

5.1 SemEval 2015 Results

Table 3 compares our model with the following
baselines:

F1
Methods
78.96
(1) JAIST
76.52
(2) HITSZ-ICRC
80.55
(3) Graph-cut
81.50
(4) FCCRF
(5) BGMN
77.23
(6) CNN-LSTM-CRF 82.22
83.91
(7) QCN

Acc
79.10
76.11
79.80
80.50
78.40
82.24
85.65

Table 3:
dataset.

Comparisons on the SemEval 2015

• JAIST (Tran et al., 2015): It used an SVM
classiﬁer to incorporate various kinds of fea-
tures , including topic model based features
and word vector representations.

• HITSZ-ICRC (Hou et al., 2015):

It pro-
posed ensemble learning and hierarchical
classiﬁcation method to classify answers.

• Graph-cut (Joty et al., 2015): It modeled the
relationship between pairs of answers at any
distance in the same question thread, based
on the idea that similar answers should have
similar labels.

• FCCRF (Joty et al., 2016): It used locally
learned classiﬁers to predict the label for each
individual node, and applied fully connected
CRF to make global inference.

• CNN-LSTM-CRF (Xiang et al., 2016): The
question and its answers are linearly con-
nected in a sequence and encoded by CNN.
An attention-based LSTM with a CRF layer
is then applied on the encoded sequence.

• BGMN (Wu et al., 2017b): It used the mem-
ory mechanism to iteratively aggregate more
relevant information which is useful to iden-
tify the relationship between questions and
answers.

Baselines include top systems from SemEval
2015 (1, 2), systems relying on thread level infor-
mation to make global inference (3, 4), and neu-
ral network based systems (5, 6). We observe that
our proposed QCN can achieve the state-of-the-art
performance on this dataset, outperforming previ-
ous best model (6) by 1.7% in terms of F1 and
3.4% in terms of accuracy.

Methods
(1) KeLP
(2) Beihang-MSRA
(3) ECNU
(4) LSTM
(5) LSTM-subject-body
(6) QCN

MAP F1
88.43
88.24
86.72
86.32
87.11
88.51

69.87
68.40
77.67
74.41
74.50
78.11

Acc
73.89
51.98
78.43
75.69
77.28
80.71

Table 4:
dataset.

Comparisons on the SemEval 2017

Notably, Systems (1, 2, 3, 4) have heavy feature
engineering, while QCN only uses automatically-
learned feature vectors, demonstrating that our
QCN model is concise as well as effective. Fur-
thermore, our model can outperform systems rely-
ing on thread level information to make global in-
ference (3, 4), showing that modeling interaction
between the question-answer pair is useful enough
for answer selection task. Finally, neural network
based systems (5, 6) used attention mechanism in
sentence representation but ignored the subject-
body relationship in community questions. QCN
can outperform them by a large margin, showing
that condensing question representation helps in
the answer selection task.

5.2 SemEval 2017 Results

Table 4 compares our model with the following
baselines:

• KeLP (Filice et al., 2017):

It used syn-
tactic tree kernels with relational links be-
tween questions and answers, together with
some standard text similarity measures lin-
early combined with the tree kernel.

• Beihang-MSRA (Feng et al., 2017): It used
gradient boosted regression trees to combine
traditional NLP features and neural network-
based matching features.

• ECNU (Wu et al., 2017a): It combined a su-
pervised model using traditional features and
a convolutional neural network to represent
the question-answer pair.

• LSTM: It is a simple neural network based
baseline that we implemented. In this model,
the question subject and the question body
are concatenated, and an LSTM is used to ob-
tain the question and answer representation.

• LSTM-subject-body:

It is another neural
network based baseline that we implemented.
LSTM is applied on the question subject and
body respectively, and the results are concate-
nated to form question representation.

Baselines include top systems from the Se-
mEval 2017 CQA task (1, 2, 3) and two neural net-
work based baselines (4, 5) that we implemented.
(5) can outperform (4), showing that treating ques-
tion subject and body differently can indeed boot
model performance. Comparing (6) with (5), we
can draw the conclusion that orthogonal decom-
position is more effective than simple concatena-
tion, because it can ﬂexibly aggregate related in-
formation from the question body with respect to
the main subject. In the example listed in Table 1,
attention heatmap of Aorth indicates that QCN can
effectively ﬁnd additional information like “main-
tenance, accident or service history”, while (5)
fails to do so.

QCN has a great advantage in terms of accu-
racy. We hypothesize that QCN focuses on mod-
eling interaction between questions and answers,
i.e., whether an answer can match the correspond-
ing question. Many pieces of previous work fo-
cus on modeling relationship between answers in
a question thread, i.e., which answer is more suit-
able in consideration of all other answers. As a
consequence, their models have a greater advan-
tage in ranking while QCN has a greater advan-
tage in classiﬁcation. Despite all this, QCN can
still obtain better ranking performance.

5.3 Ablation Study

For thorough comparison, besides the preceding
models, we implement nine extra baselines on the
SemEval 2017 dataset to analyze the improve-
ments contributed by each part of our QCN model:

Model
(1) w/o task-speciﬁc word embeddings
(2) w/o character embeddings
(3) subject-body alignment
(4) subject-body concatenation
(5) w/o multi-dimensional attention
(6) subject only
(7) body only
(8) similarity only
(9) disparity only
(10) QCN

Acc
78.81
78.05
77.38
76.06
78.33
74.02
75.57
79.11
78.24
80.71

Table 5: Ablation studies on the SemEval 2017
dataset.

the question body for each question subject
word, and then the result is concatenated with
Semb to obtain question representation Srep.

• subject-body concatenation where we con-
catenate question subject and body text, and
use the preprocessing step described in sec-
tion 3.3 to obtain Srep.

• w/o multi-dimensional attention where the
multi-dimensional attention mechanism is re-
placed by vanilla attention in all modules,
i.e., attention score for each token pair is a
scalar instead of a vector.

• subject only where only question subject is

used as question representation.

• body only where only question body is used

as question representation.

• similarity only where the parallel component
alone is used in subject-body interaction.

• disparity only where the orthogonal compo-
nent alone is used in subject-body interaction.

• w/o task-speciﬁc word embeddings where
word embeddings are initialized with the
300-dimensional GloVe word vectors trained
on Wikipedia 2014 and Gigaword 5.

• w/o character embeddings where word-
level embeddings are only composed of 600-
dimensional GloVe word vectors trained on
the domain-speciﬁc unannotated corpus.

• subject-body alignment where we use the
same attention mechanism as Question An-
swer Alignment to obtain weighted sum of

The results are listed in Table 5. We can see that
using task-speciﬁc embeddings and character em-
beddings both contribute to model performance.
This is because CQA text is non-standard. There
are quantities of informal language usage, such
as abbreviations, typos, emoticons, and grammati-
cal mistakes. Using task-speciﬁc embeddings and
character embeddings can help to attenuate the
OOV problem.

Using orthogonal decomposition (10) instead of
subject-body alignment (3) can bring about signif-
icant performance gain. This is because not only

(a) Apara

(b) Aorth

(c) Aalign

Figure 2: Attention probabilities in Apara, Aorth and Aalign. In order to visualize the multi-dimensional
attention vector, we use the L2 norm of the attenion vector for representation.

the similar part of the question body to the ques-
tion subject is useful for the question representa-
tion, the disparity part can also provide additional
information. In the example listed in Table 1, ad-
ditional information like “maintenance, accident
or service history” is also important to determine
answer quality.

QCN outperforms (4) by a great margin,
demonstrating that subject-body relationship in
community questions helps to condense question
representation. Therefore, QCN can identify the
meaningful part of the question representation that
helps to determine answer quality.

Using the multi-dimensional attention can fur-
ther boost model performance, showing that the
multi-dimensional attention can model the inter-
action between questions and answers more pre-
cisely.

Comparing QCN with (6) and (7), we can con-
clude that both the subject and the body are indis-
pensable for question representation. (8) outper-
forms (9), demonstrating the parallel component
is more useful in subject-body interaction.

6 Qualitative Study

To gain a closer view of what dependencies are
captured in the subject-body pair and the question-
answer pair, we visualize the attention probabili-
ties Apara, Aorth and Aalign by heatmap. A train-
ing example from SemEval 2015 is selected for
illustration.

In Figure 2, we can draw the following con-
clusions. First, orthogonal decomposition helps
to divide the labor of identifying similar parts in
the parallel component and collecting related in-
formation in the question body in the orthogonal
component. For instance, for the word “Kuala” in

the question subject, its parallel alignment score
focuses more on “Doha” and “Travel”, while its
orthogonal alignment score focuses on “arrange”
and “package”, which is the purpose of the travel
and therefore is also indispensable for sentence
representation. Second, semantically important
words such as “airline” and “fares” dominate the
attention weights, showing that our QCN model
can effectively select words that are most repre-
sentative for the meaning of the whole sentence.
Lastly, words that are useful to determine answer
quality stand out in the question-answer interac-
tion matrix, demonstrating that question-answer
relationship can be well modeled. For example,
“best” and “low” are the words that are more im-
portant in the question-answer relationship, they
are emphasized in the question-answer alignment
matrix.

7 Related Work

One main task in community question answering
is answer selection, i.e., to rate the answers ac-
cording to their quality. The SemEval CQA tasks
(Nakov et al., 2015, 2016, 2017) provide univer-
sal benchmark datasets for evaluating researches
on this problem.

Earlier work of answer selection in CQA relied
heavily on feature engineering, linguistic tools,
and external resource. Nakov et al. (2016) investi-
gated a wide range of feature types including sim-
ilarity features, content features, thread level/meta
features, and automatically generated features for
SemEval CQA models. Tran et al. (2015) studied
the use of topic model based features and word
vector representation based features in the answer
re-ranking task. Filice et al. (2016) designed var-
ious heuristic features and thread-based features

that can signal a good answer. Although achiev-
ing good performance, these methods rely heav-
ily on feature engineering, which requires a large
amount of manual work and domain expertise.

Since answer selection is inherently a ranking
task, a few recent researches proposed to use local
features to make global ranking decision. Barrón-
Cedeño et al. (2015) was the ﬁrst work that ap-
plies structured prediction model on CQA answer
selection task. Joty et al. (2016) approached the
task with a global inference process to exploit the
information of all answers in the question-thread
in the form of a fully connected graph.

To avoid feature engineering, many deep learn-
ing models have been proposed for answer selec-
tion. Among them, Zhang et al. (2017) proposed
a novel interactive attention mechanism to address
the problem of noise and redundancy prevalent in
CQA. Tay et al. (2017) introduced temporal gates
for sequence pairs so that questions and answers
are aware of what each other is remembering or
forgetting. Simple as their model are, they did not
consider the relationship between question subject
and body, which is useful for question condensing.

8 Conclusion and Future Work

We propose Question Condensing Networks
(QCN), an attention-based model that can utilize
the subject-body relationship in community ques-
tions to condense question representation. By or-
thogonal decomposition, the labor of identifying
similar parts and collecting related information in
the question body can be well divided in two dif-
ferent alignment matrices. To better capture the
interaction between the subject-body pair and the
question-answer pair, the multi-dimensional atten-
tion mechanism is adopted. Empirical results on
two community question answering datasets in Se-
mEval demonstrate the effectiveness of our model.
In future work, we will try to incorporate more
hand-crafted features in our model. Furthermore,
since thread-level features have been explored in
previous work (Barrón-Cedeño et al., 2015; Joty
et al., 2015, 2016), we will verify their effective-
ness in our architecture.

9 Acknowledgments

We would like to thank anonymous reviewers for
their insightful comments. Our work is supported
by National Natural Science Foundation of China
under Grant No.61433015 and the National Key

Research and Development Program of China un-
der Grant No.2017YFB1002101. The correspond-
ing author of this paper is Houfeng Wang.

References

Alberto Barrón-Cedeño, Simone Filice, Giovanni
Da San Martino, Shaﬁq R. Joty, Lluís Màrquez,
Preslav Nakov, and Alessandro Moschitti. 2015.
Thread-level information for comment classiﬁcation
In Proceed-
in community question answering.
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Language
Processing, ACL 2015, July 26-31, 2015, Beijing,
China, Volume 2: Short Papers. pages 687–693.

Steven Bird. 2006. NLTK: the natural language toolkit.
In ACL 2006, 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, Proceedings of the Conference, Sydney, Aus-
tralia, 17-21 July 2006.

David Eigen and Rob Fergus. 2015. Predicting depth,
surface normals and semantic labels with a com-
mon multi-scale convolutional architecture. In 2015
IEEE International Conference on Computer Vision,
ICCV 2015, Santiago, Chile, December 7-13, 2015.
pages 2650–2658.

Wenzheng Feng, Yu Wu, Wei Wu, Zhoujun Li, and
Ming Zhou. 2017. Beihang-msra at semeval-2017
task 3: A ranking system with neural matching fea-
In Pro-
tures for community question answering.
ceedings of the 11th International Workshop on Se-
mantic Evaluation, SemEval@ACL 2017, Vancou-
ver, Canada, August 3-4, 2017. pages 280–286.

Simone Filice, Danilo Croce, Alessandro Moschitti,
and Roberto Basili. 2016. Kelp at semeval-2016
task 3: Learning semantic relations between ques-
In Proceedings of the 10th
tions and answers.
International Workshop on Semantic Evaluation,
SemEval@NAACL-HLT 2016, San Diego, CA, USA,
June 16-17, 2016. pages 1116–1123.

Simone Filice, Giovanni Da San Martino, and Alessan-
dro Moschitti. 2017. Kelp at semeval-2017 task 3:
Learning pairwise patterns in community question
answering. In Proceedings of the 11th International
Workshop on Semantic Evaluation, SemEval@ACL
2017, Vancouver, Canada, August 3-4, 2017. pages
326–333.

Yongshuai Hou, Cong Tan, Xiaolong Wang, Yaoyun
Zhang, Jun Xu, and Qingcai Chen. 2015. HITSZ-
ICRC: exploiting classiﬁcation approach for answer
selection in community question answering. In Pro-
ceedings of the 9th International Workshop on Se-
mantic Evaluation, SemEval@NAACL-HLT 2015,
Denver, Colorado, USA, June 4-5, 2015. pages 196–
202.

rectional self-attention network for rnn/cnn-free lan-
guage understanding. In AAAI Conference on Artiﬁ-
cial Intelligence.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2017.
Cross temporal recurrent networks for ranking ques-
tion answer pairs. CoRR abs/1711.07656.

Quan Hung Tran, Vu Tran, Tu Vu, Minh Nguyen,
and Son Bao Pham. 2015.
JAIST: combining
multiple features for answer selection in commu-
In Proceedings of the
nity question answering.
9th International Workshop on Semantic Evalua-
tion, SemEval@NAACL-HLT 2015, Denver, Col-
orado, USA, June 4-5, 2015. pages 215–219.

Shuohang Wang and Jing Jiang. 2017. A compare-
aggregate model for matching text sequences.
In
Proceedings of
the International Conference on
Learning Representations (ICLR).

Zhiguo Wang, Haitao Mi, and Abraham Ittycheriah.
2016. Sentence similarity learning by lexical de-
In Proceedings of
composition and composition.
COLING 2016, the 26th International Conference
on Computational Linguistics: Technical Papers.

GuoShun Wu, Yixuan Sheng, Man Lan, and Yuanbin
Wu. 2017a. ECNU at semeval-2017 task 3: Us-
ing traditional and deep learning methods to ad-
dress community question answering task. In Pro-
ceedings of the 11th International Workshop on Se-
mantic Evaluation, SemEval@ACL 2017, Vancou-
ver, Canada, August 3-4, 2017. pages 365–369.

Wei Wu, Houfeng Wang, and Sujian Li. 2017b. Bi-
directional gated memory networks for answer se-
lection. In Chinese Computational Linguistics and
Natural Language Processing Based on Naturally
Annotated Big Data. LNAI 10565, Springer. pages
251–262.

Yang Xiang, Xiaoqiang Zhou, Qingcai Chen, Zhihui
Zheng, Buzhou Tang, Xiaolong Wang, and Yang
Qin. 2016. Incorporating label dependency for an-
swer quality tagging in community question an-
In COLING 2016,
swering via CNN-LSTM-CRF.
26th International Conference on Computational
Linguistics, Proceedings of the Conference: Tech-
nical Papers, December 11-16, 2016, Osaka, Japan.
pages 1231–1241.

Xiaodong Zhang, Sujian Li, Lei Sha, and Houfeng
Wang. 2017. Attentive interactive neural networks
for answer selection in community question answer-
ing. In Proceedings of the Thirty-First AAAI Confer-
ence on Artiﬁcial Intelligence, February 4-9, 2017,
San Francisco, California, USA.. pages 3525–3531.

Shaﬁq R. Joty, Alberto Barrón-Cedeño, Giovanni
Da San Martino, Simone Filice, Lluís Màrquez,
Alessandro Moschitti, and Preslav Nakov. 2015.
Global thread-level inference for comment classiﬁ-
In Pro-
cation in community question answering.
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2015,
Lisbon, Portugal, September 17-21, 2015. pages
573–578.

Shaﬁq R. Joty, Lluís Màrquez, and Preslav Nakov.
2016. Joint learning with global inference for com-
ment classiﬁcation in community question answer-
ing. In NAACL HLT 2016, The 2016 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, San Diego California, USA, June 12-17,
2016. pages 703–713.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M. Rush. 2016. Character-aware neural lan-
guage models. In Proceedings of the Thirtieth AAAI
Conference on Artiﬁcial Intelligence, February 12-
17, 2016, Phoenix, Arizona, USA.. pages 2741–
2749.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
CoRR

A method for stochastic optimization.
abs/1412.6980.

Preslav Nakov, Doris Hoogeveen, Lluís Màrquez,
Alessandro Moschitti, Hamdy Mubarak, Timothy
Baldwin, and Karin Verspoor. 2017. Semeval-2017
In Pro-
task 3: Community question answering.
ceedings of the 11th International Workshop on Se-
mantic Evaluation, SemEval@ACL 2017, Vancou-
ver, Canada, August 3-4, 2017. pages 27–48.

Preslav Nakov, Lluís Màrquez, Walid Magdy, Alessan-
dro Moschitti, Jim Glass, and Bilal Randeree. 2015.
Semeval-2015 task 3: Answer selection in com-
In Proceedings of the
munity question answering.
9th International Workshop on Semantic Evalua-
tion, SemEval@NAACL-HLT 2015, Denver, Col-
orado, USA, June 4-5, 2015. pages 269–281.

Preslav Nakov, Lluís Màrquez, Alessandro Moschitti,
Walid Magdy, Hamdy Mubarak, Abed Alhakim
Freihat, Jim Glass, and Bilal Randeree. 2016.
Semeval-2016 task 3: Community question answer-
ing. In Proceedings of the 10th International Work-
shop on Semantic Evaluation, SemEval@NAACL-
HLT 2016, San Diego, CA, USA, June 16-17, 2016.
pages 525–545.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
In Proceedings of the 2014
word representation.
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL. pages 1532–1543.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2018. Disan: Di-

Question Condensing Networks for Answer Selection in Community
Question Answering

Wei Wu1, Xu Sun1, Houfeng Wang1,2

1MOE Key Lab of Computational Linguistics, Peking University, Beijing, 100871, China
2Collaborative Innovation Center for Language Ability, Xuzhou, Jiangsu, 221009, China
{wu.wei, xusun, wanghf}@pku.edu.cn

Abstract

Answer selection is an important subtask
of community question answering (CQA).
In a real-world CQA forum, a question
is often represented as two parts: a sub-
ject that summarizes the main points of
the question, and a body that elaborates on
the subject in detail. Previous researches
on answer selection usually ignored the
difference between these two parts and
concatenated them as the question repre-
sentation.
In this paper, we propose the
Question Condensing Networks (QCN) to
make use of the subject-body relationship
In this model,
of community questions.
the question subject is the primary part of
the question representation, and the ques-
tion body information is aggregated based
on similarity and disparity with the ques-
tion subject. Experimental results show
that QCN outperforms all existing models
on two CQA datasets.

1

Introduction

Community question answering (CQA) has seen a
spectacular increase in popularity in recent years.
With the advent of sites like Stack Overﬂow1 and
Quora2, more and more people can freely ask any
question and expect a variety of answers. With
the inﬂux of new questions and the varied qual-
ity of provided answers, it is very time-consuming
for a user to inspect them all. Therefore, develop-
ing automated tools to identify good answers for a
question is of practical importance.

A typical example for CQA is shown in Table 1.
In this example, Answer 1 is a good answer, be-
cause it provides helpful information, e.g., “check

1https://stackoverflow.com/
2https://www.quora.com/

it to the trafﬁc dept”. Although Answer 2 is rele-
vant to the question, it does not contain any useful
information so that it should be regarded as a bad
answer.

From this example, we can observe two charac-
teristics of CQA that ordinary QA does not pos-
sess. First, a question includes both a subject that
gives a brief summary of the question and a body
that describes the question in detail. The question-
ers usually convey their main concern and key in-
formation in the question subject. Then, they pro-
vide more extensive details about the subject, seek
help, or express gratitude in the question body.
Second, the problem of redundancy and noise is
prevalent in CQA (Zhang et al., 2017). Both ques-
tions and answers contain auxiliary sentences that
do not provide meaningful information.

Previous researches (Tran et al., 2015; Joty
et al., 2016) usually treat each word equally in
the question and answer representation. How-
ever, due to the redundancy and noise problem,
only part of text from questions and answers is
useful to determine the answer quality. To make
things worse, they ignored the difference between
question subject and body, and simply concate-
nated them as the question representation. Due
to the subject-body relationship described above,
this simple concatenation can aggravate the re-
dundancy problem in the question. In this paper,
we propose the Question Condensing Networks
(QCN) to address these problems.

In order to utilize the subject-body relationship
in community questions, we propose to treat the
question subject as the primary part of the ques-
tion, and aggregate the question body information
based on similarity and disparity with the ques-
tion subject. The similarity part corresponds to
the information that exists in both question sub-
ject and body, and the disparity part corresponds
to the additional information provided by the ques-

Question Subject Checking the history of the car.

Question body

How can one check the history of the car like maintenance, accident or service
history. In every advertisement of the car, people used to write “Accident Free", but
in most cases, car have at least one or two accident, which is not easily detectable
through Car Inspection Company. Share your opinion in this regard.
Depends on the owner of the car.. if she/he reported the accident/s i believe u can
check it to the trafﬁc dept.. but some owners are not doing that especially if its only
a small accident.. try ur luck and go to the trafﬁc dept..
How about those who claim a low mileage by tampering with the car fuse box? In
my sense if you’re not able to detect traces of an accident then it is probably not
worth mentioning... For best results buy a new car :)

Answer1

Answer2

Table 1: An example question and its related answers in CQA. The text is shown in its original form,
which may contain errors in typing.

tion body. Both information can be important for
question representation.
In our model, they are
processed separately and the results are combined
to form the ﬁnal question representation.

In order to reduce the impact of redundancy
and noise in both questions and answers, we pro-
pose to align the question-answer pairs using the
multi-dimensional attention mechanism. Differ-
ent from previous attention mechanisms that com-
pute a scalar score for each token pair, multi-
dimensional attention, ﬁrst proposed in Shen et al.
(2018), computes one attention score for each di-
mension of the token embedding. Therefore, it can
select the features that can best describe the word’s
speciﬁc meaning in the given context. Therefore,
we can learn the interaction between questions and
answers more accurately.

The main contributions of our work can be sum-

marized as follows:

• We propose to treat the question subject and
the question body separately in community
question answering. We treat the question
subject as the primary part of the question,
and aggregate the question body information
based on similarity and disparity with the
question subject.

• We introduce a new method that uses the
multi-dimensional attention mechanism to
align question-answer pair. With this at-
tention mechanism, the interaction between
questions and answers can be learned more
accurately.

performance on two SemEval CQA datasets,
outperforming all exisiting SOTA models by
a large margin, which demonstrates the effec-
tiveness of our model.3

2 Task Description

A community question answering consists of four
parts, which can be formally deﬁned as a tuple of
four elements (S, B, C, y). S = [s1, s2, ..., sl] de-
notes the subject of a question whose length is l,
where each si is a one-hot vector whose dimen-
sion equals the size of the vocabulary. Similarly,
B = [b1, b2, ..., bm] denotes the body of a ques-
tion whose length is m. C = [c1, c2, ..., cn] de-
notes an answer corresponding to that question
whose length is n. y ∈ Y is the label represent-
ing the degree to which it can answer that ques-
tion. Y = {Good, PotentiallyUseful, Bad} where
Good indicates the answer can answer that ques-
tion well, PotentiallyUseful indicates the answer
is potentially useful to the user, and Bad indi-
cates the answer is just bad or useless. Given
{S, B, C}, the task of CQA is to assign a label to
each answer based on the conditional probability
P r(y|S, B, C).

3 Proposed Model

In this paper, we propose Question Condensing
Networks (QCN) which is composed of the fol-
lowing modules. The overall architecture of our
model is illustrated in Figure 1.

• Our proposed Question Condensing Net-
works (QCN) achieves the state-of-the-art

3An implementation of our model is available at https:

//github.com/pku-wuwei/QCN.

Figure 1: Architecture for Question Condensing Network (QCN). Each block represents a vector.

3.1 Word-Level Embedding

Word-level embeddings are composed of two
components: GloVe (Pennington et al., 2014)
word vectors trained on the domain-speciﬁc unan-
notated corpus provided by the task 4, and con-
volutional neural network-based character embed-
dings which are similar to (Kim et al., 2016). Web
text in CQA forums differs largely from normal-
ized text in terms of spelling and grammar, so
speciﬁcally trained GloVe vectors can model word
interactions more precisely. Character embedding
has proven to be very useful for out-of-vocabulary
(OOV) words, so it is especially suitable for noisy
web text in CQA.

We concatenate these two embedding vectors
for every word to generate word-level embeddings
Semb ∈ Rd×l, Bemb ∈ Rd×m, Cemb ∈ Rd×n,
where d is the word-level embedding size.

3.2 Question Condensing

In this section, we condense the question repre-
sentation using subject-body relationship. In most
cases, the question subject can be seen as a sum-
mary containing key points of the question, the
question body is relatively lengthy in that it needs
to explain the key points and add more details
about the posted question. We propose to cheat the
question subject as the primary part of the question
representation, and aggregate question body infor-
mation from two perspectives: similarity and dis-
parity with the question subject. To achieve this
goal, we use an orthogonal decomposition strat-
egy, which is ﬁrst proposed by Wang et al. (2016),
to decompose each question body embedding into
a parallel component and an orthogonal compo-

4http://alt.qcri.org/semeval2015/

task3/index.php?id=data-and-tools

nent based on every question subject embedding:
bj
emb · si
si
emb · si
orth = bj
bi,j
emb − bi,j

bi,j
para =

si
emb

para

emb

emb

(1)

(2)

All vectors in the above equations are of length d.
Next we describe the process of aggregating the
question body information based on the parallel
component in detail. The same process can be ap-
plied to the orthogonal component, so at the end
of the fusion gate we can obtain Sorth and Sorth
respectively.

The decomposed components

are passed
through a fully connected layer to compute the
multi-dimensional attention weights. Here we use
the scaled tanh activation, which is similar to Shen
et al. (2018), to prevent large difference among
scores while it still has a range large enough for
output:
para = c · tanh (cid:0)(cid:2)Wp1bi,j
ai,j

para + bp1

(cid:3) /c(cid:1)

(3)

where Wp1 ∈ Rd×d and bp1 ∈ Rd are parame-
ters to be learned, and c is a hyper-parameter to be
tuned.

The obtained word-level alignment

tensor
Apara ∈ Rd×l×m is then normalized along the
third dimension to produce the attention weights
over the question body for each word in the ques-
tion subject. The output of this attention mecha-
nism is a weighted sum of the question body em-
beddings for each word in the question subject:

wi,j

para =

(cid:16)

exp

(cid:17)

ai,j
para
(cid:16)

(cid:17)

ai,j
para

(cid:80)m

j=1 exp

si
ap =

wi,j

para (cid:12) bj

emb

m
(cid:88)

j=1

(4)

(5)

where (cid:12) means point-wise product. This multi-
dimensional attention mechanism has the advan-
tage of selecting features of a word that can best
describe the word’s speciﬁc meaning in the given
context. In order to determine the importance be-
tween the original word in the question subject and
the aggregated information from the question body
with respect to this word, a fusion gate is utilized
to combine these two representations:

Fpara = σ (Wp2Semb + Wp3Sap + bp2)
(6)
Spara = Fpara (cid:12) Semb + (1 − Fpara) (cid:12) Sap (7)

where Wp2, Wp3 ∈ Rd×d, and bp2 ∈ Rd
are learnable parameters of the fusion gate, and
Fpara, Semb, Sap, Spara ∈ Rd×l. The ﬁnal ques-
tion representation Srep ∈ R2d×l is obtained by
concatenating Spara and Sorth along the ﬁrst di-
mension.

3.3 Answer Preprocessing

This module has two purposes. First, we try to
map each answer word from embedding space
Cemb ∈ Rd×n to the same interaction space
Crep ∈ R2d×n as the question. Second, similar to
Wang and Jiang (2017), a gate is utilized to con-
trol the importance of different answer words in
determining the question-answer relation:

and the answer representation :

(9)

(10)

˜ai,j
align = Wa1si
ai,j
align = c · tanh

(cid:16)

rep + Wa2cj
rep + ba
(cid:17)
˜ai,j
align/c
(cid:16)
ai,j
align
(cid:16)
ai,j
align
(cid:17)

j=1 exp
(cid:16)

exp

(cid:17)

(cid:80)n

exp

ai,j
align
(cid:16)
ai,j
align

(cid:80)l

i=1 exp

si
ai =

cj
ai =

n
(cid:88)

j=1

l
(cid:88)

i=1

(cid:17) (cid:12) cj

rep

(11)

(cid:17) (cid:12) si

rep

(12)

where Wa1, Wa2 ∈ R2d×2d and ba ∈ R2d are
parameters to be learned. To attenuate the effect
of incorrect attendance, input and output of this
attention mechanism are concatenated and fed to
the subsequent layer. Finally, we obtain the ques-
tion and answer representation Satt ∈ R4d×l =
[Srep; Sai], Catt ∈ R4d×n = [Crep; Cai].

3.5

Interaction Summarization

In this layer, the multi-dimensional self-attention
mechanism is employed to summarize two se-
quences of vectors (Satt and Catt) into two ﬁxed-
length vectors ssum ∈ R4d and csum ∈ R4d.

As = Ws2tanh (Ws1Satt + bs1) + bs2

(13)

ssum =

n
(cid:88)

i=1

(cid:1)

exp (cid:0)ai
s
i=1 exp (ai
s)

(cid:80)n

(cid:12) si

att

(14)

where Ws1, Ws2 ∈ R4d×4d and bs1, bs2 ∈ R4d are
parameters to be learned. The same process can
be applied to Catt and obtain csum.

In this component, ssum and csum are concate-
nated and fed into a two-layer feed-forward neural
network. At the end of the last layer, the sof tmax
function is applied to obtain the conditional prob-
ability distribution P r(y|S, B, C).

4 Experimental Setup

4.1 Datasets

Crep =σ (Wc1Cemb + bc1) (cid:12)
tanh (Wc2Cemb + bc2)

3.6 Prediction

(8)

where Wc1, Wc2 ∈ Rd×2d and bc1, bc2 ∈ R2d are
parameters to be learned.

3.4 Question Answer Alignment

We apply the multi-dimensional attention mech-
anism to the question and answer representa-
tion Srep and Crep to obtain word-level align-
ment tensor Aalign ∈ R2d×l×n. Similar to the
multi-dimensional attention mechanism described
above, we can compute attention weights and
weighted sum for both the question representation

We use two community question answering
datasets from SemEval (Nakov et al., 2015, 2017)
to evaluate our model. The statistics of these
datasets are listed in Table 2. The corpora contain
data from the QatarLiving forum 5, and are pub-
licly available on the task website. Each dataset

5http://www.qatarliving.com/forum

Statistics

Number of questions
Number of answers
Average length of subject
Average length of body
Average length of answer

SemEval 2015
Dev
266
1447
6.08
39.47
33.90

Train
2376
15013
6.36
39.26
35.82

Test
300
1793
6.24
39.53
37.33

SemEval 2017
Dev
327
3270
6.16
47.98
37.30

Train
5124
38638
6.38
43.01
37.67

Test
293
2930
5.76
54.06
39.50

Table 2: Statistics of two CQA datasets. We can see from the statistics that the question body is much
lengthier than the question subject. Thus, it is necessary to condense the question representation.

consists of questions and a list of answers for each
question, and each question consists of a short ti-
tle and a more detailed description. There are also
some metadata associated with them, e.g., user ID,
date of posting, and the question category. We do
not use the metadata because they failed to boost
performance in our model. Since the SemEval
2017 dataset is an updated version of SemEval
2016 6, and shares the same evaluation metrics
with SemEval 2016, we choose to use the SemEval
2017 dataset for evaluation.

4.2 Evaluation Metrics

In order to facilitate comparison, we adopt the
evaluation metrics used in the ofﬁcial task or prior
work. For the SemEval 2015 dataset, the ofﬁ-
cial scores are macro-averaged F1 and accuracy
over three categories. However, many recent re-
searches (Barrón-Cedeño et al., 2015; Joty et al.,
2015, 2016) switched to a binary classiﬁcation set-
ting, i.e., identifying Good vs. Bad answers. Be-
cause binary classiﬁcation is much closer to a real-
world CQA application. Besides, the Potential-
lyUseful class is both the smallest and the noisiest
class, making it the hardest to predict. To make
it worse, its impact is magniﬁed by the macro-
averaged F1. Therefore, we adopt the F1 score
and accuracy on two categories for evaluation.

SemEval 2017 regards answer selection as a
ranking task, which is closer to the application sce-
nario. As a result, mean average precision (MAP)
is used as an evaluation measure. For a perfect
ranking, a system has to place all Good answers
above the PotentiallyUseful and Bad answers. The
latter two are not actually distinguished and are
considered Bad in terms of evaluation. Addition-

6The SemEval 2017 dataset provides all the data from
2016 for training , and fresh data for testing, but it does not
include a development set. Following previous work (Filice
et al., 2017), we use the 2016 ofﬁcial test set as the develop-
ment set.

ally, standard classiﬁcation measures like accuracy
and F1 score are also reported.

4.3

Implementation Details

We use the tokenizer from NLTK (Bird, 2006)
to preprocess each sentence. All word embed-
dings in the sentence encoder layer are initial-
ized with the 300-dimensional GloVe (Pennington
et al., 2014) word vectors trained on the domain-
speciﬁc unannotated corpus, and embeddings for
out-of-vocabulary words are set to zero. We use
the Adam Optimizer (Kingma and Ba, 2014) for
optimization with a ﬁrst momentum coefﬁcient of
0.9 and a second momentum coefﬁcient of 0.999.
We perform a small grid search over combina-
tions of initial learning rate [1 × 10−6, 3 × 10−6,
1 × 10−5], L2 regularization parameter [1 × 10−7,
3 × 10−7, 1 × 10−6], and batch size [8, 16, 32].
We take the best conﬁguration based on perfor-
mance on the development set, and only evalu-
ate that conﬁguration on the test set. In order to
mitigate the class imbalance problem, median fre-
quency balancing Eigen and Fergus (2015) is used
to reweight each class in the cross-entropy loss.
Therefore, the rarer a class is in the training set, the
larger weight it will get in the cross entropy loss.
Early stopping is applied to mitigate the problem
of overﬁtting. For the SemEval 2017 dataset, the
conditional probability over the Good class is used
to rank all the candidate answers.

5 Experimental Results

In this section, we evaluate our QCN model on two
community question answering datasets from Se-
mEval shared tasks.

5.1 SemEval 2015 Results

Table 3 compares our model with the following
baselines:

F1
Methods
78.96
(1) JAIST
76.52
(2) HITSZ-ICRC
80.55
(3) Graph-cut
81.50
(4) FCCRF
(5) BGMN
77.23
(6) CNN-LSTM-CRF 82.22
83.91
(7) QCN

Acc
79.10
76.11
79.80
80.50
78.40
82.24
85.65

Table 3:
dataset.

Comparisons on the SemEval 2015

• JAIST (Tran et al., 2015): It used an SVM
classiﬁer to incorporate various kinds of fea-
tures , including topic model based features
and word vector representations.

• HITSZ-ICRC (Hou et al., 2015):

It pro-
posed ensemble learning and hierarchical
classiﬁcation method to classify answers.

• Graph-cut (Joty et al., 2015): It modeled the
relationship between pairs of answers at any
distance in the same question thread, based
on the idea that similar answers should have
similar labels.

• FCCRF (Joty et al., 2016): It used locally
learned classiﬁers to predict the label for each
individual node, and applied fully connected
CRF to make global inference.

• CNN-LSTM-CRF (Xiang et al., 2016): The
question and its answers are linearly con-
nected in a sequence and encoded by CNN.
An attention-based LSTM with a CRF layer
is then applied on the encoded sequence.

• BGMN (Wu et al., 2017b): It used the mem-
ory mechanism to iteratively aggregate more
relevant information which is useful to iden-
tify the relationship between questions and
answers.

Baselines include top systems from SemEval
2015 (1, 2), systems relying on thread level infor-
mation to make global inference (3, 4), and neu-
ral network based systems (5, 6). We observe that
our proposed QCN can achieve the state-of-the-art
performance on this dataset, outperforming previ-
ous best model (6) by 1.7% in terms of F1 and
3.4% in terms of accuracy.

Methods
(1) KeLP
(2) Beihang-MSRA
(3) ECNU
(4) LSTM
(5) LSTM-subject-body
(6) QCN

MAP F1
88.43
88.24
86.72
86.32
87.11
88.51

69.87
68.40
77.67
74.41
74.50
78.11

Acc
73.89
51.98
78.43
75.69
77.28
80.71

Table 4:
dataset.

Comparisons on the SemEval 2017

Notably, Systems (1, 2, 3, 4) have heavy feature
engineering, while QCN only uses automatically-
learned feature vectors, demonstrating that our
QCN model is concise as well as effective. Fur-
thermore, our model can outperform systems rely-
ing on thread level information to make global in-
ference (3, 4), showing that modeling interaction
between the question-answer pair is useful enough
for answer selection task. Finally, neural network
based systems (5, 6) used attention mechanism in
sentence representation but ignored the subject-
body relationship in community questions. QCN
can outperform them by a large margin, showing
that condensing question representation helps in
the answer selection task.

5.2 SemEval 2017 Results

Table 4 compares our model with the following
baselines:

• KeLP (Filice et al., 2017):

It used syn-
tactic tree kernels with relational links be-
tween questions and answers, together with
some standard text similarity measures lin-
early combined with the tree kernel.

• Beihang-MSRA (Feng et al., 2017): It used
gradient boosted regression trees to combine
traditional NLP features and neural network-
based matching features.

• ECNU (Wu et al., 2017a): It combined a su-
pervised model using traditional features and
a convolutional neural network to represent
the question-answer pair.

• LSTM: It is a simple neural network based
baseline that we implemented. In this model,
the question subject and the question body
are concatenated, and an LSTM is used to ob-
tain the question and answer representation.

• LSTM-subject-body:

It is another neural
network based baseline that we implemented.
LSTM is applied on the question subject and
body respectively, and the results are concate-
nated to form question representation.

Baselines include top systems from the Se-
mEval 2017 CQA task (1, 2, 3) and two neural net-
work based baselines (4, 5) that we implemented.
(5) can outperform (4), showing that treating ques-
tion subject and body differently can indeed boot
model performance. Comparing (6) with (5), we
can draw the conclusion that orthogonal decom-
position is more effective than simple concatena-
tion, because it can ﬂexibly aggregate related in-
formation from the question body with respect to
the main subject. In the example listed in Table 1,
attention heatmap of Aorth indicates that QCN can
effectively ﬁnd additional information like “main-
tenance, accident or service history”, while (5)
fails to do so.

QCN has a great advantage in terms of accu-
racy. We hypothesize that QCN focuses on mod-
eling interaction between questions and answers,
i.e., whether an answer can match the correspond-
ing question. Many pieces of previous work fo-
cus on modeling relationship between answers in
a question thread, i.e., which answer is more suit-
able in consideration of all other answers. As a
consequence, their models have a greater advan-
tage in ranking while QCN has a greater advan-
tage in classiﬁcation. Despite all this, QCN can
still obtain better ranking performance.

5.3 Ablation Study

For thorough comparison, besides the preceding
models, we implement nine extra baselines on the
SemEval 2017 dataset to analyze the improve-
ments contributed by each part of our QCN model:

Model
(1) w/o task-speciﬁc word embeddings
(2) w/o character embeddings
(3) subject-body alignment
(4) subject-body concatenation
(5) w/o multi-dimensional attention
(6) subject only
(7) body only
(8) similarity only
(9) disparity only
(10) QCN

Acc
78.81
78.05
77.38
76.06
78.33
74.02
75.57
79.11
78.24
80.71

Table 5: Ablation studies on the SemEval 2017
dataset.

the question body for each question subject
word, and then the result is concatenated with
Semb to obtain question representation Srep.

• subject-body concatenation where we con-
catenate question subject and body text, and
use the preprocessing step described in sec-
tion 3.3 to obtain Srep.

• w/o multi-dimensional attention where the
multi-dimensional attention mechanism is re-
placed by vanilla attention in all modules,
i.e., attention score for each token pair is a
scalar instead of a vector.

• subject only where only question subject is

used as question representation.

• body only where only question body is used

as question representation.

• similarity only where the parallel component
alone is used in subject-body interaction.

• disparity only where the orthogonal compo-
nent alone is used in subject-body interaction.

• w/o task-speciﬁc word embeddings where
word embeddings are initialized with the
300-dimensional GloVe word vectors trained
on Wikipedia 2014 and Gigaword 5.

• w/o character embeddings where word-
level embeddings are only composed of 600-
dimensional GloVe word vectors trained on
the domain-speciﬁc unannotated corpus.

• subject-body alignment where we use the
same attention mechanism as Question An-
swer Alignment to obtain weighted sum of

The results are listed in Table 5. We can see that
using task-speciﬁc embeddings and character em-
beddings both contribute to model performance.
This is because CQA text is non-standard. There
are quantities of informal language usage, such
as abbreviations, typos, emoticons, and grammati-
cal mistakes. Using task-speciﬁc embeddings and
character embeddings can help to attenuate the
OOV problem.

Using orthogonal decomposition (10) instead of
subject-body alignment (3) can bring about signif-
icant performance gain. This is because not only

(a) Apara

(b) Aorth

(c) Aalign

Figure 2: Attention probabilities in Apara, Aorth and Aalign. In order to visualize the multi-dimensional
attention vector, we use the L2 norm of the attenion vector for representation.

the similar part of the question body to the ques-
tion subject is useful for the question representa-
tion, the disparity part can also provide additional
information. In the example listed in Table 1, ad-
ditional information like “maintenance, accident
or service history” is also important to determine
answer quality.

QCN outperforms (4) by a great margin,
demonstrating that subject-body relationship in
community questions helps to condense question
representation. Therefore, QCN can identify the
meaningful part of the question representation that
helps to determine answer quality.

Using the multi-dimensional attention can fur-
ther boost model performance, showing that the
multi-dimensional attention can model the inter-
action between questions and answers more pre-
cisely.

Comparing QCN with (6) and (7), we can con-
clude that both the subject and the body are indis-
pensable for question representation. (8) outper-
forms (9), demonstrating the parallel component
is more useful in subject-body interaction.

6 Qualitative Study

To gain a closer view of what dependencies are
captured in the subject-body pair and the question-
answer pair, we visualize the attention probabili-
ties Apara, Aorth and Aalign by heatmap. A train-
ing example from SemEval 2015 is selected for
illustration.

In Figure 2, we can draw the following con-
clusions. First, orthogonal decomposition helps
to divide the labor of identifying similar parts in
the parallel component and collecting related in-
formation in the question body in the orthogonal
component. For instance, for the word “Kuala” in

the question subject, its parallel alignment score
focuses more on “Doha” and “Travel”, while its
orthogonal alignment score focuses on “arrange”
and “package”, which is the purpose of the travel
and therefore is also indispensable for sentence
representation. Second, semantically important
words such as “airline” and “fares” dominate the
attention weights, showing that our QCN model
can effectively select words that are most repre-
sentative for the meaning of the whole sentence.
Lastly, words that are useful to determine answer
quality stand out in the question-answer interac-
tion matrix, demonstrating that question-answer
relationship can be well modeled. For example,
“best” and “low” are the words that are more im-
portant in the question-answer relationship, they
are emphasized in the question-answer alignment
matrix.

7 Related Work

One main task in community question answering
is answer selection, i.e., to rate the answers ac-
cording to their quality. The SemEval CQA tasks
(Nakov et al., 2015, 2016, 2017) provide univer-
sal benchmark datasets for evaluating researches
on this problem.

Earlier work of answer selection in CQA relied
heavily on feature engineering, linguistic tools,
and external resource. Nakov et al. (2016) investi-
gated a wide range of feature types including sim-
ilarity features, content features, thread level/meta
features, and automatically generated features for
SemEval CQA models. Tran et al. (2015) studied
the use of topic model based features and word
vector representation based features in the answer
re-ranking task. Filice et al. (2016) designed var-
ious heuristic features and thread-based features

that can signal a good answer. Although achiev-
ing good performance, these methods rely heav-
ily on feature engineering, which requires a large
amount of manual work and domain expertise.

Since answer selection is inherently a ranking
task, a few recent researches proposed to use local
features to make global ranking decision. Barrón-
Cedeño et al. (2015) was the ﬁrst work that ap-
plies structured prediction model on CQA answer
selection task. Joty et al. (2016) approached the
task with a global inference process to exploit the
information of all answers in the question-thread
in the form of a fully connected graph.

To avoid feature engineering, many deep learn-
ing models have been proposed for answer selec-
tion. Among them, Zhang et al. (2017) proposed
a novel interactive attention mechanism to address
the problem of noise and redundancy prevalent in
CQA. Tay et al. (2017) introduced temporal gates
for sequence pairs so that questions and answers
are aware of what each other is remembering or
forgetting. Simple as their model are, they did not
consider the relationship between question subject
and body, which is useful for question condensing.

8 Conclusion and Future Work

We propose Question Condensing Networks
(QCN), an attention-based model that can utilize
the subject-body relationship in community ques-
tions to condense question representation. By or-
thogonal decomposition, the labor of identifying
similar parts and collecting related information in
the question body can be well divided in two dif-
ferent alignment matrices. To better capture the
interaction between the subject-body pair and the
question-answer pair, the multi-dimensional atten-
tion mechanism is adopted. Empirical results on
two community question answering datasets in Se-
mEval demonstrate the effectiveness of our model.
In future work, we will try to incorporate more
hand-crafted features in our model. Furthermore,
since thread-level features have been explored in
previous work (Barrón-Cedeño et al., 2015; Joty
et al., 2015, 2016), we will verify their effective-
ness in our architecture.

9 Acknowledgments

We would like to thank anonymous reviewers for
their insightful comments. Our work is supported
by National Natural Science Foundation of China
under Grant No.61433015 and the National Key

Research and Development Program of China un-
der Grant No.2017YFB1002101. The correspond-
ing author of this paper is Houfeng Wang.

References

Alberto Barrón-Cedeño, Simone Filice, Giovanni
Da San Martino, Shaﬁq R. Joty, Lluís Màrquez,
Preslav Nakov, and Alessandro Moschitti. 2015.
Thread-level information for comment classiﬁcation
In Proceed-
in community question answering.
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Language
Processing, ACL 2015, July 26-31, 2015, Beijing,
China, Volume 2: Short Papers. pages 687–693.

Steven Bird. 2006. NLTK: the natural language toolkit.
In ACL 2006, 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, Proceedings of the Conference, Sydney, Aus-
tralia, 17-21 July 2006.

David Eigen and Rob Fergus. 2015. Predicting depth,
surface normals and semantic labels with a com-
mon multi-scale convolutional architecture. In 2015
IEEE International Conference on Computer Vision,
ICCV 2015, Santiago, Chile, December 7-13, 2015.
pages 2650–2658.

Wenzheng Feng, Yu Wu, Wei Wu, Zhoujun Li, and
Ming Zhou. 2017. Beihang-msra at semeval-2017
task 3: A ranking system with neural matching fea-
In Pro-
tures for community question answering.
ceedings of the 11th International Workshop on Se-
mantic Evaluation, SemEval@ACL 2017, Vancou-
ver, Canada, August 3-4, 2017. pages 280–286.

Simone Filice, Danilo Croce, Alessandro Moschitti,
and Roberto Basili. 2016. Kelp at semeval-2016
task 3: Learning semantic relations between ques-
In Proceedings of the 10th
tions and answers.
International Workshop on Semantic Evaluation,
SemEval@NAACL-HLT 2016, San Diego, CA, USA,
June 16-17, 2016. pages 1116–1123.

Simone Filice, Giovanni Da San Martino, and Alessan-
dro Moschitti. 2017. Kelp at semeval-2017 task 3:
Learning pairwise patterns in community question
answering. In Proceedings of the 11th International
Workshop on Semantic Evaluation, SemEval@ACL
2017, Vancouver, Canada, August 3-4, 2017. pages
326–333.

Yongshuai Hou, Cong Tan, Xiaolong Wang, Yaoyun
Zhang, Jun Xu, and Qingcai Chen. 2015. HITSZ-
ICRC: exploiting classiﬁcation approach for answer
selection in community question answering. In Pro-
ceedings of the 9th International Workshop on Se-
mantic Evaluation, SemEval@NAACL-HLT 2015,
Denver, Colorado, USA, June 4-5, 2015. pages 196–
202.

rectional self-attention network for rnn/cnn-free lan-
guage understanding. In AAAI Conference on Artiﬁ-
cial Intelligence.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2017.
Cross temporal recurrent networks for ranking ques-
tion answer pairs. CoRR abs/1711.07656.

Quan Hung Tran, Vu Tran, Tu Vu, Minh Nguyen,
and Son Bao Pham. 2015.
JAIST: combining
multiple features for answer selection in commu-
In Proceedings of the
nity question answering.
9th International Workshop on Semantic Evalua-
tion, SemEval@NAACL-HLT 2015, Denver, Col-
orado, USA, June 4-5, 2015. pages 215–219.

Shuohang Wang and Jing Jiang. 2017. A compare-
aggregate model for matching text sequences.
In
Proceedings of
the International Conference on
Learning Representations (ICLR).

Zhiguo Wang, Haitao Mi, and Abraham Ittycheriah.
2016. Sentence similarity learning by lexical de-
In Proceedings of
composition and composition.
COLING 2016, the 26th International Conference
on Computational Linguistics: Technical Papers.

GuoShun Wu, Yixuan Sheng, Man Lan, and Yuanbin
Wu. 2017a. ECNU at semeval-2017 task 3: Us-
ing traditional and deep learning methods to ad-
dress community question answering task. In Pro-
ceedings of the 11th International Workshop on Se-
mantic Evaluation, SemEval@ACL 2017, Vancou-
ver, Canada, August 3-4, 2017. pages 365–369.

Wei Wu, Houfeng Wang, and Sujian Li. 2017b. Bi-
directional gated memory networks for answer se-
lection. In Chinese Computational Linguistics and
Natural Language Processing Based on Naturally
Annotated Big Data. LNAI 10565, Springer. pages
251–262.

Yang Xiang, Xiaoqiang Zhou, Qingcai Chen, Zhihui
Zheng, Buzhou Tang, Xiaolong Wang, and Yang
Qin. 2016. Incorporating label dependency for an-
swer quality tagging in community question an-
In COLING 2016,
swering via CNN-LSTM-CRF.
26th International Conference on Computational
Linguistics, Proceedings of the Conference: Tech-
nical Papers, December 11-16, 2016, Osaka, Japan.
pages 1231–1241.

Xiaodong Zhang, Sujian Li, Lei Sha, and Houfeng
Wang. 2017. Attentive interactive neural networks
for answer selection in community question answer-
ing. In Proceedings of the Thirty-First AAAI Confer-
ence on Artiﬁcial Intelligence, February 4-9, 2017,
San Francisco, California, USA.. pages 3525–3531.

Shaﬁq R. Joty, Alberto Barrón-Cedeño, Giovanni
Da San Martino, Simone Filice, Lluís Màrquez,
Alessandro Moschitti, and Preslav Nakov. 2015.
Global thread-level inference for comment classiﬁ-
In Pro-
cation in community question answering.
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2015,
Lisbon, Portugal, September 17-21, 2015. pages
573–578.

Shaﬁq R. Joty, Lluís Màrquez, and Preslav Nakov.
2016. Joint learning with global inference for com-
ment classiﬁcation in community question answer-
ing. In NAACL HLT 2016, The 2016 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, San Diego California, USA, June 12-17,
2016. pages 703–713.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M. Rush. 2016. Character-aware neural lan-
guage models. In Proceedings of the Thirtieth AAAI
Conference on Artiﬁcial Intelligence, February 12-
17, 2016, Phoenix, Arizona, USA.. pages 2741–
2749.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
CoRR

A method for stochastic optimization.
abs/1412.6980.

Preslav Nakov, Doris Hoogeveen, Lluís Màrquez,
Alessandro Moschitti, Hamdy Mubarak, Timothy
Baldwin, and Karin Verspoor. 2017. Semeval-2017
In Pro-
task 3: Community question answering.
ceedings of the 11th International Workshop on Se-
mantic Evaluation, SemEval@ACL 2017, Vancou-
ver, Canada, August 3-4, 2017. pages 27–48.

Preslav Nakov, Lluís Màrquez, Walid Magdy, Alessan-
dro Moschitti, Jim Glass, and Bilal Randeree. 2015.
Semeval-2015 task 3: Answer selection in com-
In Proceedings of the
munity question answering.
9th International Workshop on Semantic Evalua-
tion, SemEval@NAACL-HLT 2015, Denver, Col-
orado, USA, June 4-5, 2015. pages 269–281.

Preslav Nakov, Lluís Màrquez, Alessandro Moschitti,
Walid Magdy, Hamdy Mubarak, Abed Alhakim
Freihat, Jim Glass, and Bilal Randeree. 2016.
Semeval-2016 task 3: Community question answer-
ing. In Proceedings of the 10th International Work-
shop on Semantic Evaluation, SemEval@NAACL-
HLT 2016, San Diego, CA, USA, June 16-17, 2016.
pages 525–545.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
In Proceedings of the 2014
word representation.
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL. pages 1532–1543.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2018. Disan: Di-

Question Condensing Networks for Answer Selection in Community
Question Answering

Wei Wu1, Xu Sun1, Houfeng Wang1,2

1MOE Key Lab of Computational Linguistics, Peking University, Beijing, 100871, China
2Collaborative Innovation Center for Language Ability, Xuzhou, Jiangsu, 221009, China
{wu.wei, xusun, wanghf}@pku.edu.cn

Abstract

Answer selection is an important subtask
of community question answering (CQA).
In a real-world CQA forum, a question
is often represented as two parts: a sub-
ject that summarizes the main points of
the question, and a body that elaborates on
the subject in detail. Previous researches
on answer selection usually ignored the
difference between these two parts and
concatenated them as the question repre-
sentation.
In this paper, we propose the
Question Condensing Networks (QCN) to
make use of the subject-body relationship
In this model,
of community questions.
the question subject is the primary part of
the question representation, and the ques-
tion body information is aggregated based
on similarity and disparity with the ques-
tion subject. Experimental results show
that QCN outperforms all existing models
on two CQA datasets.

1

Introduction

Community question answering (CQA) has seen a
spectacular increase in popularity in recent years.
With the advent of sites like Stack Overﬂow1 and
Quora2, more and more people can freely ask any
question and expect a variety of answers. With
the inﬂux of new questions and the varied qual-
ity of provided answers, it is very time-consuming
for a user to inspect them all. Therefore, develop-
ing automated tools to identify good answers for a
question is of practical importance.

A typical example for CQA is shown in Table 1.
In this example, Answer 1 is a good answer, be-
cause it provides helpful information, e.g., “check

1https://stackoverflow.com/
2https://www.quora.com/

it to the trafﬁc dept”. Although Answer 2 is rele-
vant to the question, it does not contain any useful
information so that it should be regarded as a bad
answer.

From this example, we can observe two charac-
teristics of CQA that ordinary QA does not pos-
sess. First, a question includes both a subject that
gives a brief summary of the question and a body
that describes the question in detail. The question-
ers usually convey their main concern and key in-
formation in the question subject. Then, they pro-
vide more extensive details about the subject, seek
help, or express gratitude in the question body.
Second, the problem of redundancy and noise is
prevalent in CQA (Zhang et al., 2017). Both ques-
tions and answers contain auxiliary sentences that
do not provide meaningful information.

Previous researches (Tran et al., 2015; Joty
et al., 2016) usually treat each word equally in
the question and answer representation. How-
ever, due to the redundancy and noise problem,
only part of text from questions and answers is
useful to determine the answer quality. To make
things worse, they ignored the difference between
question subject and body, and simply concate-
nated them as the question representation. Due
to the subject-body relationship described above,
this simple concatenation can aggravate the re-
dundancy problem in the question. In this paper,
we propose the Question Condensing Networks
(QCN) to address these problems.

In order to utilize the subject-body relationship
in community questions, we propose to treat the
question subject as the primary part of the ques-
tion, and aggregate the question body information
based on similarity and disparity with the ques-
tion subject. The similarity part corresponds to
the information that exists in both question sub-
ject and body, and the disparity part corresponds
to the additional information provided by the ques-

Question Subject Checking the history of the car.

Question body

How can one check the history of the car like maintenance, accident or service
history. In every advertisement of the car, people used to write “Accident Free", but
in most cases, car have at least one or two accident, which is not easily detectable
through Car Inspection Company. Share your opinion in this regard.
Depends on the owner of the car.. if she/he reported the accident/s i believe u can
check it to the trafﬁc dept.. but some owners are not doing that especially if its only
a small accident.. try ur luck and go to the trafﬁc dept..
How about those who claim a low mileage by tampering with the car fuse box? In
my sense if you’re not able to detect traces of an accident then it is probably not
worth mentioning... For best results buy a new car :)

Answer1

Answer2

Table 1: An example question and its related answers in CQA. The text is shown in its original form,
which may contain errors in typing.

tion body. Both information can be important for
question representation.
In our model, they are
processed separately and the results are combined
to form the ﬁnal question representation.

In order to reduce the impact of redundancy
and noise in both questions and answers, we pro-
pose to align the question-answer pairs using the
multi-dimensional attention mechanism. Differ-
ent from previous attention mechanisms that com-
pute a scalar score for each token pair, multi-
dimensional attention, ﬁrst proposed in Shen et al.
(2018), computes one attention score for each di-
mension of the token embedding. Therefore, it can
select the features that can best describe the word’s
speciﬁc meaning in the given context. Therefore,
we can learn the interaction between questions and
answers more accurately.

The main contributions of our work can be sum-

marized as follows:

• We propose to treat the question subject and
the question body separately in community
question answering. We treat the question
subject as the primary part of the question,
and aggregate the question body information
based on similarity and disparity with the
question subject.

• We introduce a new method that uses the
multi-dimensional attention mechanism to
align question-answer pair. With this at-
tention mechanism, the interaction between
questions and answers can be learned more
accurately.

performance on two SemEval CQA datasets,
outperforming all exisiting SOTA models by
a large margin, which demonstrates the effec-
tiveness of our model.3

2 Task Description

A community question answering consists of four
parts, which can be formally deﬁned as a tuple of
four elements (S, B, C, y). S = [s1, s2, ..., sl] de-
notes the subject of a question whose length is l,
where each si is a one-hot vector whose dimen-
sion equals the size of the vocabulary. Similarly,
B = [b1, b2, ..., bm] denotes the body of a ques-
tion whose length is m. C = [c1, c2, ..., cn] de-
notes an answer corresponding to that question
whose length is n. y ∈ Y is the label represent-
ing the degree to which it can answer that ques-
tion. Y = {Good, PotentiallyUseful, Bad} where
Good indicates the answer can answer that ques-
tion well, PotentiallyUseful indicates the answer
is potentially useful to the user, and Bad indi-
cates the answer is just bad or useless. Given
{S, B, C}, the task of CQA is to assign a label to
each answer based on the conditional probability
P r(y|S, B, C).

3 Proposed Model

In this paper, we propose Question Condensing
Networks (QCN) which is composed of the fol-
lowing modules. The overall architecture of our
model is illustrated in Figure 1.

• Our proposed Question Condensing Net-
works (QCN) achieves the state-of-the-art

3An implementation of our model is available at https:

//github.com/pku-wuwei/QCN.

Figure 1: Architecture for Question Condensing Network (QCN). Each block represents a vector.

3.1 Word-Level Embedding

Word-level embeddings are composed of two
components: GloVe (Pennington et al., 2014)
word vectors trained on the domain-speciﬁc unan-
notated corpus provided by the task 4, and con-
volutional neural network-based character embed-
dings which are similar to (Kim et al., 2016). Web
text in CQA forums differs largely from normal-
ized text in terms of spelling and grammar, so
speciﬁcally trained GloVe vectors can model word
interactions more precisely. Character embedding
has proven to be very useful for out-of-vocabulary
(OOV) words, so it is especially suitable for noisy
web text in CQA.

We concatenate these two embedding vectors
for every word to generate word-level embeddings
Semb ∈ Rd×l, Bemb ∈ Rd×m, Cemb ∈ Rd×n,
where d is the word-level embedding size.

3.2 Question Condensing

In this section, we condense the question repre-
sentation using subject-body relationship. In most
cases, the question subject can be seen as a sum-
mary containing key points of the question, the
question body is relatively lengthy in that it needs
to explain the key points and add more details
about the posted question. We propose to cheat the
question subject as the primary part of the question
representation, and aggregate question body infor-
mation from two perspectives: similarity and dis-
parity with the question subject. To achieve this
goal, we use an orthogonal decomposition strat-
egy, which is ﬁrst proposed by Wang et al. (2016),
to decompose each question body embedding into
a parallel component and an orthogonal compo-

4http://alt.qcri.org/semeval2015/

task3/index.php?id=data-and-tools

nent based on every question subject embedding:
bj
emb · si
si
emb · si
orth = bj
bi,j
emb − bi,j

bi,j
para =

si
emb

para

emb

emb

(1)

(2)

All vectors in the above equations are of length d.
Next we describe the process of aggregating the
question body information based on the parallel
component in detail. The same process can be ap-
plied to the orthogonal component, so at the end
of the fusion gate we can obtain Sorth and Sorth
respectively.

The decomposed components

are passed
through a fully connected layer to compute the
multi-dimensional attention weights. Here we use
the scaled tanh activation, which is similar to Shen
et al. (2018), to prevent large difference among
scores while it still has a range large enough for
output:
para = c · tanh (cid:0)(cid:2)Wp1bi,j
ai,j

para + bp1

(cid:3) /c(cid:1)

(3)

where Wp1 ∈ Rd×d and bp1 ∈ Rd are parame-
ters to be learned, and c is a hyper-parameter to be
tuned.

The obtained word-level alignment

tensor
Apara ∈ Rd×l×m is then normalized along the
third dimension to produce the attention weights
over the question body for each word in the ques-
tion subject. The output of this attention mecha-
nism is a weighted sum of the question body em-
beddings for each word in the question subject:

wi,j

para =

(cid:16)

exp

(cid:17)

ai,j
para
(cid:16)

(cid:17)

ai,j
para

(cid:80)m

j=1 exp

si
ap =

wi,j

para (cid:12) bj

emb

m
(cid:88)

j=1

(4)

(5)

where (cid:12) means point-wise product. This multi-
dimensional attention mechanism has the advan-
tage of selecting features of a word that can best
describe the word’s speciﬁc meaning in the given
context. In order to determine the importance be-
tween the original word in the question subject and
the aggregated information from the question body
with respect to this word, a fusion gate is utilized
to combine these two representations:

Fpara = σ (Wp2Semb + Wp3Sap + bp2)
(6)
Spara = Fpara (cid:12) Semb + (1 − Fpara) (cid:12) Sap (7)

where Wp2, Wp3 ∈ Rd×d, and bp2 ∈ Rd
are learnable parameters of the fusion gate, and
Fpara, Semb, Sap, Spara ∈ Rd×l. The ﬁnal ques-
tion representation Srep ∈ R2d×l is obtained by
concatenating Spara and Sorth along the ﬁrst di-
mension.

3.3 Answer Preprocessing

This module has two purposes. First, we try to
map each answer word from embedding space
Cemb ∈ Rd×n to the same interaction space
Crep ∈ R2d×n as the question. Second, similar to
Wang and Jiang (2017), a gate is utilized to con-
trol the importance of different answer words in
determining the question-answer relation:

and the answer representation :

(9)

(10)

˜ai,j
align = Wa1si
ai,j
align = c · tanh

(cid:16)

rep + Wa2cj
rep + ba
(cid:17)
˜ai,j
align/c
(cid:16)
ai,j
align
(cid:16)
ai,j
align
(cid:17)

j=1 exp
(cid:16)

exp

(cid:17)

(cid:80)n

exp

ai,j
align
(cid:16)
ai,j
align

(cid:80)l

i=1 exp

si
ai =

cj
ai =

n
(cid:88)

j=1

l
(cid:88)

i=1

(cid:17) (cid:12) cj

rep

(11)

(cid:17) (cid:12) si

rep

(12)

where Wa1, Wa2 ∈ R2d×2d and ba ∈ R2d are
parameters to be learned. To attenuate the effect
of incorrect attendance, input and output of this
attention mechanism are concatenated and fed to
the subsequent layer. Finally, we obtain the ques-
tion and answer representation Satt ∈ R4d×l =
[Srep; Sai], Catt ∈ R4d×n = [Crep; Cai].

3.5

Interaction Summarization

In this layer, the multi-dimensional self-attention
mechanism is employed to summarize two se-
quences of vectors (Satt and Catt) into two ﬁxed-
length vectors ssum ∈ R4d and csum ∈ R4d.

As = Ws2tanh (Ws1Satt + bs1) + bs2

(13)

ssum =

n
(cid:88)

i=1

(cid:1)

exp (cid:0)ai
s
i=1 exp (ai
s)

(cid:80)n

(cid:12) si

att

(14)

where Ws1, Ws2 ∈ R4d×4d and bs1, bs2 ∈ R4d are
parameters to be learned. The same process can
be applied to Catt and obtain csum.

In this component, ssum and csum are concate-
nated and fed into a two-layer feed-forward neural
network. At the end of the last layer, the sof tmax
function is applied to obtain the conditional prob-
ability distribution P r(y|S, B, C).

4 Experimental Setup

4.1 Datasets

Crep =σ (Wc1Cemb + bc1) (cid:12)
tanh (Wc2Cemb + bc2)

3.6 Prediction

(8)

where Wc1, Wc2 ∈ Rd×2d and bc1, bc2 ∈ R2d are
parameters to be learned.

3.4 Question Answer Alignment

We apply the multi-dimensional attention mech-
anism to the question and answer representa-
tion Srep and Crep to obtain word-level align-
ment tensor Aalign ∈ R2d×l×n. Similar to the
multi-dimensional attention mechanism described
above, we can compute attention weights and
weighted sum for both the question representation

We use two community question answering
datasets from SemEval (Nakov et al., 2015, 2017)
to evaluate our model. The statistics of these
datasets are listed in Table 2. The corpora contain
data from the QatarLiving forum 5, and are pub-
licly available on the task website. Each dataset

5http://www.qatarliving.com/forum

Statistics

Number of questions
Number of answers
Average length of subject
Average length of body
Average length of answer

SemEval 2015
Dev
266
1447
6.08
39.47
33.90

Train
2376
15013
6.36
39.26
35.82

Test
300
1793
6.24
39.53
37.33

SemEval 2017
Dev
327
3270
6.16
47.98
37.30

Train
5124
38638
6.38
43.01
37.67

Test
293
2930
5.76
54.06
39.50

Table 2: Statistics of two CQA datasets. We can see from the statistics that the question body is much
lengthier than the question subject. Thus, it is necessary to condense the question representation.

consists of questions and a list of answers for each
question, and each question consists of a short ti-
tle and a more detailed description. There are also
some metadata associated with them, e.g., user ID,
date of posting, and the question category. We do
not use the metadata because they failed to boost
performance in our model. Since the SemEval
2017 dataset is an updated version of SemEval
2016 6, and shares the same evaluation metrics
with SemEval 2016, we choose to use the SemEval
2017 dataset for evaluation.

4.2 Evaluation Metrics

In order to facilitate comparison, we adopt the
evaluation metrics used in the ofﬁcial task or prior
work. For the SemEval 2015 dataset, the ofﬁ-
cial scores are macro-averaged F1 and accuracy
over three categories. However, many recent re-
searches (Barrón-Cedeño et al., 2015; Joty et al.,
2015, 2016) switched to a binary classiﬁcation set-
ting, i.e., identifying Good vs. Bad answers. Be-
cause binary classiﬁcation is much closer to a real-
world CQA application. Besides, the Potential-
lyUseful class is both the smallest and the noisiest
class, making it the hardest to predict. To make
it worse, its impact is magniﬁed by the macro-
averaged F1. Therefore, we adopt the F1 score
and accuracy on two categories for evaluation.

SemEval 2017 regards answer selection as a
ranking task, which is closer to the application sce-
nario. As a result, mean average precision (MAP)
is used as an evaluation measure. For a perfect
ranking, a system has to place all Good answers
above the PotentiallyUseful and Bad answers. The
latter two are not actually distinguished and are
considered Bad in terms of evaluation. Addition-

6The SemEval 2017 dataset provides all the data from
2016 for training , and fresh data for testing, but it does not
include a development set. Following previous work (Filice
et al., 2017), we use the 2016 ofﬁcial test set as the develop-
ment set.

ally, standard classiﬁcation measures like accuracy
and F1 score are also reported.

4.3

Implementation Details

We use the tokenizer from NLTK (Bird, 2006)
to preprocess each sentence. All word embed-
dings in the sentence encoder layer are initial-
ized with the 300-dimensional GloVe (Pennington
et al., 2014) word vectors trained on the domain-
speciﬁc unannotated corpus, and embeddings for
out-of-vocabulary words are set to zero. We use
the Adam Optimizer (Kingma and Ba, 2014) for
optimization with a ﬁrst momentum coefﬁcient of
0.9 and a second momentum coefﬁcient of 0.999.
We perform a small grid search over combina-
tions of initial learning rate [1 × 10−6, 3 × 10−6,
1 × 10−5], L2 regularization parameter [1 × 10−7,
3 × 10−7, 1 × 10−6], and batch size [8, 16, 32].
We take the best conﬁguration based on perfor-
mance on the development set, and only evalu-
ate that conﬁguration on the test set. In order to
mitigate the class imbalance problem, median fre-
quency balancing Eigen and Fergus (2015) is used
to reweight each class in the cross-entropy loss.
Therefore, the rarer a class is in the training set, the
larger weight it will get in the cross entropy loss.
Early stopping is applied to mitigate the problem
of overﬁtting. For the SemEval 2017 dataset, the
conditional probability over the Good class is used
to rank all the candidate answers.

5 Experimental Results

In this section, we evaluate our QCN model on two
community question answering datasets from Se-
mEval shared tasks.

5.1 SemEval 2015 Results

Table 3 compares our model with the following
baselines:

F1
Methods
78.96
(1) JAIST
76.52
(2) HITSZ-ICRC
80.55
(3) Graph-cut
81.50
(4) FCCRF
(5) BGMN
77.23
(6) CNN-LSTM-CRF 82.22
83.91
(7) QCN

Acc
79.10
76.11
79.80
80.50
78.40
82.24
85.65

Table 3:
dataset.

Comparisons on the SemEval 2015

• JAIST (Tran et al., 2015): It used an SVM
classiﬁer to incorporate various kinds of fea-
tures , including topic model based features
and word vector representations.

• HITSZ-ICRC (Hou et al., 2015):

It pro-
posed ensemble learning and hierarchical
classiﬁcation method to classify answers.

• Graph-cut (Joty et al., 2015): It modeled the
relationship between pairs of answers at any
distance in the same question thread, based
on the idea that similar answers should have
similar labels.

• FCCRF (Joty et al., 2016): It used locally
learned classiﬁers to predict the label for each
individual node, and applied fully connected
CRF to make global inference.

• CNN-LSTM-CRF (Xiang et al., 2016): The
question and its answers are linearly con-
nected in a sequence and encoded by CNN.
An attention-based LSTM with a CRF layer
is then applied on the encoded sequence.

• BGMN (Wu et al., 2017b): It used the mem-
ory mechanism to iteratively aggregate more
relevant information which is useful to iden-
tify the relationship between questions and
answers.

Baselines include top systems from SemEval
2015 (1, 2), systems relying on thread level infor-
mation to make global inference (3, 4), and neu-
ral network based systems (5, 6). We observe that
our proposed QCN can achieve the state-of-the-art
performance on this dataset, outperforming previ-
ous best model (6) by 1.7% in terms of F1 and
3.4% in terms of accuracy.

Methods
(1) KeLP
(2) Beihang-MSRA
(3) ECNU
(4) LSTM
(5) LSTM-subject-body
(6) QCN

MAP F1
88.43
88.24
86.72
86.32
87.11
88.51

69.87
68.40
77.67
74.41
74.50
78.11

Acc
73.89
51.98
78.43
75.69
77.28
80.71

Table 4:
dataset.

Comparisons on the SemEval 2017

Notably, Systems (1, 2, 3, 4) have heavy feature
engineering, while QCN only uses automatically-
learned feature vectors, demonstrating that our
QCN model is concise as well as effective. Fur-
thermore, our model can outperform systems rely-
ing on thread level information to make global in-
ference (3, 4), showing that modeling interaction
between the question-answer pair is useful enough
for answer selection task. Finally, neural network
based systems (5, 6) used attention mechanism in
sentence representation but ignored the subject-
body relationship in community questions. QCN
can outperform them by a large margin, showing
that condensing question representation helps in
the answer selection task.

5.2 SemEval 2017 Results

Table 4 compares our model with the following
baselines:

• KeLP (Filice et al., 2017):

It used syn-
tactic tree kernels with relational links be-
tween questions and answers, together with
some standard text similarity measures lin-
early combined with the tree kernel.

• Beihang-MSRA (Feng et al., 2017): It used
gradient boosted regression trees to combine
traditional NLP features and neural network-
based matching features.

• ECNU (Wu et al., 2017a): It combined a su-
pervised model using traditional features and
a convolutional neural network to represent
the question-answer pair.

• LSTM: It is a simple neural network based
baseline that we implemented. In this model,
the question subject and the question body
are concatenated, and an LSTM is used to ob-
tain the question and answer representation.

• LSTM-subject-body:

It is another neural
network based baseline that we implemented.
LSTM is applied on the question subject and
body respectively, and the results are concate-
nated to form question representation.

Baselines include top systems from the Se-
mEval 2017 CQA task (1, 2, 3) and two neural net-
work based baselines (4, 5) that we implemented.
(5) can outperform (4), showing that treating ques-
tion subject and body differently can indeed boot
model performance. Comparing (6) with (5), we
can draw the conclusion that orthogonal decom-
position is more effective than simple concatena-
tion, because it can ﬂexibly aggregate related in-
formation from the question body with respect to
the main subject. In the example listed in Table 1,
attention heatmap of Aorth indicates that QCN can
effectively ﬁnd additional information like “main-
tenance, accident or service history”, while (5)
fails to do so.

QCN has a great advantage in terms of accu-
racy. We hypothesize that QCN focuses on mod-
eling interaction between questions and answers,
i.e., whether an answer can match the correspond-
ing question. Many pieces of previous work fo-
cus on modeling relationship between answers in
a question thread, i.e., which answer is more suit-
able in consideration of all other answers. As a
consequence, their models have a greater advan-
tage in ranking while QCN has a greater advan-
tage in classiﬁcation. Despite all this, QCN can
still obtain better ranking performance.

5.3 Ablation Study

For thorough comparison, besides the preceding
models, we implement nine extra baselines on the
SemEval 2017 dataset to analyze the improve-
ments contributed by each part of our QCN model:

Model
(1) w/o task-speciﬁc word embeddings
(2) w/o character embeddings
(3) subject-body alignment
(4) subject-body concatenation
(5) w/o multi-dimensional attention
(6) subject only
(7) body only
(8) similarity only
(9) disparity only
(10) QCN

Acc
78.81
78.05
77.38
76.06
78.33
74.02
75.57
79.11
78.24
80.71

Table 5: Ablation studies on the SemEval 2017
dataset.

the question body for each question subject
word, and then the result is concatenated with
Semb to obtain question representation Srep.

• subject-body concatenation where we con-
catenate question subject and body text, and
use the preprocessing step described in sec-
tion 3.3 to obtain Srep.

• w/o multi-dimensional attention where the
multi-dimensional attention mechanism is re-
placed by vanilla attention in all modules,
i.e., attention score for each token pair is a
scalar instead of a vector.

• subject only where only question subject is

used as question representation.

• body only where only question body is used

as question representation.

• similarity only where the parallel component
alone is used in subject-body interaction.

• disparity only where the orthogonal compo-
nent alone is used in subject-body interaction.

• w/o task-speciﬁc word embeddings where
word embeddings are initialized with the
300-dimensional GloVe word vectors trained
on Wikipedia 2014 and Gigaword 5.

• w/o character embeddings where word-
level embeddings are only composed of 600-
dimensional GloVe word vectors trained on
the domain-speciﬁc unannotated corpus.

• subject-body alignment where we use the
same attention mechanism as Question An-
swer Alignment to obtain weighted sum of

The results are listed in Table 5. We can see that
using task-speciﬁc embeddings and character em-
beddings both contribute to model performance.
This is because CQA text is non-standard. There
are quantities of informal language usage, such
as abbreviations, typos, emoticons, and grammati-
cal mistakes. Using task-speciﬁc embeddings and
character embeddings can help to attenuate the
OOV problem.

Using orthogonal decomposition (10) instead of
subject-body alignment (3) can bring about signif-
icant performance gain. This is because not only

(a) Apara

(b) Aorth

(c) Aalign

Figure 2: Attention probabilities in Apara, Aorth and Aalign. In order to visualize the multi-dimensional
attention vector, we use the L2 norm of the attenion vector for representation.

the similar part of the question body to the ques-
tion subject is useful for the question representa-
tion, the disparity part can also provide additional
information. In the example listed in Table 1, ad-
ditional information like “maintenance, accident
or service history” is also important to determine
answer quality.

QCN outperforms (4) by a great margin,
demonstrating that subject-body relationship in
community questions helps to condense question
representation. Therefore, QCN can identify the
meaningful part of the question representation that
helps to determine answer quality.

Using the multi-dimensional attention can fur-
ther boost model performance, showing that the
multi-dimensional attention can model the inter-
action between questions and answers more pre-
cisely.

Comparing QCN with (6) and (7), we can con-
clude that both the subject and the body are indis-
pensable for question representation. (8) outper-
forms (9), demonstrating the parallel component
is more useful in subject-body interaction.

6 Qualitative Study

To gain a closer view of what dependencies are
captured in the subject-body pair and the question-
answer pair, we visualize the attention probabili-
ties Apara, Aorth and Aalign by heatmap. A train-
ing example from SemEval 2015 is selected for
illustration.

In Figure 2, we can draw the following con-
clusions. First, orthogonal decomposition helps
to divide the labor of identifying similar parts in
the parallel component and collecting related in-
formation in the question body in the orthogonal
component. For instance, for the word “Kuala” in

the question subject, its parallel alignment score
focuses more on “Doha” and “Travel”, while its
orthogonal alignment score focuses on “arrange”
and “package”, which is the purpose of the travel
and therefore is also indispensable for sentence
representation. Second, semantically important
words such as “airline” and “fares” dominate the
attention weights, showing that our QCN model
can effectively select words that are most repre-
sentative for the meaning of the whole sentence.
Lastly, words that are useful to determine answer
quality stand out in the question-answer interac-
tion matrix, demonstrating that question-answer
relationship can be well modeled. For example,
“best” and “low” are the words that are more im-
portant in the question-answer relationship, they
are emphasized in the question-answer alignment
matrix.

7 Related Work

One main task in community question answering
is answer selection, i.e., to rate the answers ac-
cording to their quality. The SemEval CQA tasks
(Nakov et al., 2015, 2016, 2017) provide univer-
sal benchmark datasets for evaluating researches
on this problem.

Earlier work of answer selection in CQA relied
heavily on feature engineering, linguistic tools,
and external resource. Nakov et al. (2016) investi-
gated a wide range of feature types including sim-
ilarity features, content features, thread level/meta
features, and automatically generated features for
SemEval CQA models. Tran et al. (2015) studied
the use of topic model based features and word
vector representation based features in the answer
re-ranking task. Filice et al. (2016) designed var-
ious heuristic features and thread-based features

that can signal a good answer. Although achiev-
ing good performance, these methods rely heav-
ily on feature engineering, which requires a large
amount of manual work and domain expertise.

Since answer selection is inherently a ranking
task, a few recent researches proposed to use local
features to make global ranking decision. Barrón-
Cedeño et al. (2015) was the ﬁrst work that ap-
plies structured prediction model on CQA answer
selection task. Joty et al. (2016) approached the
task with a global inference process to exploit the
information of all answers in the question-thread
in the form of a fully connected graph.

To avoid feature engineering, many deep learn-
ing models have been proposed for answer selec-
tion. Among them, Zhang et al. (2017) proposed
a novel interactive attention mechanism to address
the problem of noise and redundancy prevalent in
CQA. Tay et al. (2017) introduced temporal gates
for sequence pairs so that questions and answers
are aware of what each other is remembering or
forgetting. Simple as their model are, they did not
consider the relationship between question subject
and body, which is useful for question condensing.

8 Conclusion and Future Work

We propose Question Condensing Networks
(QCN), an attention-based model that can utilize
the subject-body relationship in community ques-
tions to condense question representation. By or-
thogonal decomposition, the labor of identifying
similar parts and collecting related information in
the question body can be well divided in two dif-
ferent alignment matrices. To better capture the
interaction between the subject-body pair and the
question-answer pair, the multi-dimensional atten-
tion mechanism is adopted. Empirical results on
two community question answering datasets in Se-
mEval demonstrate the effectiveness of our model.
In future work, we will try to incorporate more
hand-crafted features in our model. Furthermore,
since thread-level features have been explored in
previous work (Barrón-Cedeño et al., 2015; Joty
et al., 2015, 2016), we will verify their effective-
ness in our architecture.

9 Acknowledgments

We would like to thank anonymous reviewers for
their insightful comments. Our work is supported
by National Natural Science Foundation of China
under Grant No.61433015 and the National Key

Research and Development Program of China un-
der Grant No.2017YFB1002101. The correspond-
ing author of this paper is Houfeng Wang.

References

Alberto Barrón-Cedeño, Simone Filice, Giovanni
Da San Martino, Shaﬁq R. Joty, Lluís Màrquez,
Preslav Nakov, and Alessandro Moschitti. 2015.
Thread-level information for comment classiﬁcation
In Proceed-
in community question answering.
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Language
Processing, ACL 2015, July 26-31, 2015, Beijing,
China, Volume 2: Short Papers. pages 687–693.

Steven Bird. 2006. NLTK: the natural language toolkit.
In ACL 2006, 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, Proceedings of the Conference, Sydney, Aus-
tralia, 17-21 July 2006.

David Eigen and Rob Fergus. 2015. Predicting depth,
surface normals and semantic labels with a com-
mon multi-scale convolutional architecture. In 2015
IEEE International Conference on Computer Vision,
ICCV 2015, Santiago, Chile, December 7-13, 2015.
pages 2650–2658.

Wenzheng Feng, Yu Wu, Wei Wu, Zhoujun Li, and
Ming Zhou. 2017. Beihang-msra at semeval-2017
task 3: A ranking system with neural matching fea-
In Pro-
tures for community question answering.
ceedings of the 11th International Workshop on Se-
mantic Evaluation, SemEval@ACL 2017, Vancou-
ver, Canada, August 3-4, 2017. pages 280–286.

Simone Filice, Danilo Croce, Alessandro Moschitti,
and Roberto Basili. 2016. Kelp at semeval-2016
task 3: Learning semantic relations between ques-
In Proceedings of the 10th
tions and answers.
International Workshop on Semantic Evaluation,
SemEval@NAACL-HLT 2016, San Diego, CA, USA,
June 16-17, 2016. pages 1116–1123.

Simone Filice, Giovanni Da San Martino, and Alessan-
dro Moschitti. 2017. Kelp at semeval-2017 task 3:
Learning pairwise patterns in community question
answering. In Proceedings of the 11th International
Workshop on Semantic Evaluation, SemEval@ACL
2017, Vancouver, Canada, August 3-4, 2017. pages
326–333.

Yongshuai Hou, Cong Tan, Xiaolong Wang, Yaoyun
Zhang, Jun Xu, and Qingcai Chen. 2015. HITSZ-
ICRC: exploiting classiﬁcation approach for answer
selection in community question answering. In Pro-
ceedings of the 9th International Workshop on Se-
mantic Evaluation, SemEval@NAACL-HLT 2015,
Denver, Colorado, USA, June 4-5, 2015. pages 196–
202.

rectional self-attention network for rnn/cnn-free lan-
guage understanding. In AAAI Conference on Artiﬁ-
cial Intelligence.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2017.
Cross temporal recurrent networks for ranking ques-
tion answer pairs. CoRR abs/1711.07656.

Quan Hung Tran, Vu Tran, Tu Vu, Minh Nguyen,
and Son Bao Pham. 2015.
JAIST: combining
multiple features for answer selection in commu-
In Proceedings of the
nity question answering.
9th International Workshop on Semantic Evalua-
tion, SemEval@NAACL-HLT 2015, Denver, Col-
orado, USA, June 4-5, 2015. pages 215–219.

Shuohang Wang and Jing Jiang. 2017. A compare-
aggregate model for matching text sequences.
In
Proceedings of
the International Conference on
Learning Representations (ICLR).

Zhiguo Wang, Haitao Mi, and Abraham Ittycheriah.
2016. Sentence similarity learning by lexical de-
In Proceedings of
composition and composition.
COLING 2016, the 26th International Conference
on Computational Linguistics: Technical Papers.

GuoShun Wu, Yixuan Sheng, Man Lan, and Yuanbin
Wu. 2017a. ECNU at semeval-2017 task 3: Us-
ing traditional and deep learning methods to ad-
dress community question answering task. In Pro-
ceedings of the 11th International Workshop on Se-
mantic Evaluation, SemEval@ACL 2017, Vancou-
ver, Canada, August 3-4, 2017. pages 365–369.

Wei Wu, Houfeng Wang, and Sujian Li. 2017b. Bi-
directional gated memory networks for answer se-
lection. In Chinese Computational Linguistics and
Natural Language Processing Based on Naturally
Annotated Big Data. LNAI 10565, Springer. pages
251–262.

Yang Xiang, Xiaoqiang Zhou, Qingcai Chen, Zhihui
Zheng, Buzhou Tang, Xiaolong Wang, and Yang
Qin. 2016. Incorporating label dependency for an-
swer quality tagging in community question an-
In COLING 2016,
swering via CNN-LSTM-CRF.
26th International Conference on Computational
Linguistics, Proceedings of the Conference: Tech-
nical Papers, December 11-16, 2016, Osaka, Japan.
pages 1231–1241.

Xiaodong Zhang, Sujian Li, Lei Sha, and Houfeng
Wang. 2017. Attentive interactive neural networks
for answer selection in community question answer-
ing. In Proceedings of the Thirty-First AAAI Confer-
ence on Artiﬁcial Intelligence, February 4-9, 2017,
San Francisco, California, USA.. pages 3525–3531.

Shaﬁq R. Joty, Alberto Barrón-Cedeño, Giovanni
Da San Martino, Simone Filice, Lluís Màrquez,
Alessandro Moschitti, and Preslav Nakov. 2015.
Global thread-level inference for comment classiﬁ-
In Pro-
cation in community question answering.
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2015,
Lisbon, Portugal, September 17-21, 2015. pages
573–578.

Shaﬁq R. Joty, Lluís Màrquez, and Preslav Nakov.
2016. Joint learning with global inference for com-
ment classiﬁcation in community question answer-
ing. In NAACL HLT 2016, The 2016 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, San Diego California, USA, June 12-17,
2016. pages 703–713.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M. Rush. 2016. Character-aware neural lan-
guage models. In Proceedings of the Thirtieth AAAI
Conference on Artiﬁcial Intelligence, February 12-
17, 2016, Phoenix, Arizona, USA.. pages 2741–
2749.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
CoRR

A method for stochastic optimization.
abs/1412.6980.

Preslav Nakov, Doris Hoogeveen, Lluís Màrquez,
Alessandro Moschitti, Hamdy Mubarak, Timothy
Baldwin, and Karin Verspoor. 2017. Semeval-2017
In Pro-
task 3: Community question answering.
ceedings of the 11th International Workshop on Se-
mantic Evaluation, SemEval@ACL 2017, Vancou-
ver, Canada, August 3-4, 2017. pages 27–48.

Preslav Nakov, Lluís Màrquez, Walid Magdy, Alessan-
dro Moschitti, Jim Glass, and Bilal Randeree. 2015.
Semeval-2015 task 3: Answer selection in com-
In Proceedings of the
munity question answering.
9th International Workshop on Semantic Evalua-
tion, SemEval@NAACL-HLT 2015, Denver, Col-
orado, USA, June 4-5, 2015. pages 269–281.

Preslav Nakov, Lluís Màrquez, Alessandro Moschitti,
Walid Magdy, Hamdy Mubarak, Abed Alhakim
Freihat, Jim Glass, and Bilal Randeree. 2016.
Semeval-2016 task 3: Community question answer-
ing. In Proceedings of the 10th International Work-
shop on Semantic Evaluation, SemEval@NAACL-
HLT 2016, San Diego, CA, USA, June 16-17, 2016.
pages 525–545.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
In Proceedings of the 2014
word representation.
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL. pages 1532–1543.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2018. Disan: Di-

