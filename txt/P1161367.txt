MSCap: Multi-Style Image Captioning with Unpaired Stylized Text

Longteng Guo

1,4

Jing Liu∗ 1

Peng Yao

Jiangwei Li

Hanqing Lu

3

1

2

1National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences
2University of Science and Technology Beijing 3Multimedia Department, Huawei Devices
4University of Chinese Academy of Sciences
{longteng.guo,jliu,luhq}@nlpr.ia.ac.cn,S20180598@xs.ustb.edu.cn,lijiangwei1@huawei.com

Abstract

In this paper, we propose an adversarial learning net-
work for the task of multi-style image captioning (MSCap)
with a standard factual image caption dataset and a multi-
stylized language corpus without paired images. How to
learn a single model for multi-stylized image captioning
with unpaired data is a challenging and necessary task,
whereas rarely studied in previous works. The proposed
framework mainly includes four contributive modules fol-
lowing a typical image encoder. First, a style dependent
caption generator to output a sentence conditioned on an
encoded image and a speciﬁed style. Second, a caption dis-
criminator is presented to distinguish the input sentence to
be real or not. The discriminator and the generator are
trained in an adversarial manner to enable more natural
and human-like captions. Third, a style classiﬁer is em-
ployed to discriminate the speciﬁc style of the input sen-
tence. Besides, a back-translation module is designed to
enforce the generated stylized captions are visually ground-
ed, with the intuition of the cycle consistency for factual
caption and stylized caption. We enable an end-to-end opti-
mization of the whole model with differentiable softmax ap-
proximation.At last, we conduct comprehensive experiments
using a combined dataset containing four caption styles to
demonstrate the outstanding performance of our proposed
method.

1. Introduction

Automatically generating human-like captions for im-
ages, namely image captioning, has emerged as a promi-
nent
the intersec-
tion of computer vision and natural language processing
[36, 33, 40]. It has many important industrial applications,

interdisciplinary research problem at

∗Corresponding Author

Figure 1. Example results of our multi-style image-captioning
model. Given an image, our model learns to generate attractive
image captions with various styles, which could be controlled by
assigning style labels. Sentences under each colored words, i.e.
the style name, are the generated caption corresponding to that
style.

such as visual intelligence in chatting robots, photo sharing
on social media, and assistive facilities for visually impaired
people. To generate true human-like image captions, an im-
age captioning system is required to understand the visual
content and write captions with proper linguistic properties.
However, most existing image captioning systems focus on
the vision side that describes the visual content in an objec-
tive, neutral manner (factual captions), while the language
side, e.g. linguistic style, is often neglected.

In fact, linguistic style [4] is an essential factor in human
language that reﬂects personality, emotion, and sentimen-
t. Style typically refers to linguistic aspects other than the

4204

message content. Figure 1 show the captions of distinctive
styles for a given image, including factual, humorous, ro-
mantic, positive, and negative.
Incorporating appropriate
styles into image captions will greatly enrich their clarity
and attractiveness, and thus foster user engagement and so-
cial interactions. Some efforts have been made on stylized
image captioning, including explicitly modeling sentiment
words [25], transforming word embeddings matrices [10],
and factoring the problem into two separate subprocesses
[24] et al. However, all these models are built to trans-
late images into captions of a single caption style. So far
there has not been an efﬁcient way to simultaneously han-
dle multiple styles. Their inefﬁciency results from the fact
that in order to learn mappings between images and k cap-
tion styles, k distinctive models have to be trained. Mean-
while, the model can only learn from a speciﬁc style out of k
and cannot fully utilize the entire training data, even though
there exists common knowledge that could be learned from
the whole k-style data, e.g. correspondence between words
and image content.

To address this problem, a single-model solution for
multi-style image captioning (MSCap) is desired to gener-
ate visually grounded and any desired stylized captions for a
given image, while multi-style captioning resources includ-
ing images and multi-style captions are explored jointly for
the single-model training. Typically, training such a mod-
el requires fully annotated collections of aligned image-
stylized-caption pairs (paired data) for each style. Howev-
er, it is quite expensive to collect such paired multi-style
captioning collections, especially when the numbers of im-
ages and styles increase. Compared to annotating stylized
captions for each image, it is much easier and cheaper to
collect a corpus of stylized sentences without aligned im-
ages. Therefore, it is challenging but valuable to design
a multi-style captioning model by exploring such unpaired
multi-stylized data in addition to handily available factu-
al image-caption paired data (e.g. MS COCO [22] dataset),
which motivates our work.

In this paper, we propose an adversarial learning network
to handle the problem of multi-style image caption genera-
tion simultaneously with factual image-caption pairs, and
unpaired stylized captions. Given an image and its desired
captioning style as input, the proposed model generates its
corresponding stylized caption. Speciﬁcally, the proposed
adversarial learning framework consists of ﬁve modules in
which the ﬁrst module is a typical image encoder, and the
following four modules are the main focuses of this paper.
First, we design a style dependent caption generator to out-
put a sentence conditioned on an encoded image and a spec-
iﬁed style. Second, a caption discriminator is presented to
distinguish the input sentence is real or not. The discrimi-
nator is performed in an adversarial manner with the gener-
ator during training, and thus guides the generator towards

generating more natural and human-like caption. Third, a
style classiﬁer is introduced to discriminate what the spe-
ciﬁc style of the input sentence is. We further introduce a
back-translation module to ensure that the generated styl-
ized captions are visually grounded. The basic intuition is
that there exists content consistency between a stylized cap-
tion and a factual caption describing the same image. Given
a pair of image and factual caption, if we generate, e.g.,
a humorous caption from the image, and then translate it
into a factual caption, we should arrive at the real factual
caption. We name this process back-translation and imple-
ment it via a multilingual neural machine translation (NMT)
[14] model in which the multi-stylized captions are regard-
ed as source languages, and the factual caption as the tar-
get language. Overview of the framework is illustrated in
Figure 2. We enable an end-to-end optimization of the w-
hole model with differentiable softmax approximation [13]
which anneals smoothly to discrete case. At last, we con-
duct comprehensive experiments using a combined dataset
containing ﬁve caption styles: humorous, romantic, posi-
tive, negative and factual styles. As far as our knowledge
goes, our work is the ﬁrst to successfully perform multi-
style image captioning with unpaired stylized data. In sum-
mary, the main contributions of this paper are:

• We propose MSCap, a uniﬁed multi-style image cap-
tioning model that learns to map images into attrac-
tive captions of multiple styles. The model is end-to-
end trainable without using supervised style-speciﬁc
image-caption paired data.

• We design a novel style-dependent caption generator
that which enables leveraging unpaired stylized cap-
tions for model pre-training. And we introduce a back-
translation module to assure the generated captions to
be consistent with the image content.

• We provide both qualitative and quantitative results on
the multi-style and single-style image captioning tasks,
showing the superiority of our proposed model.

2. Related Work

2.1. Image Captioning

Recent advances in deep learning and release of large
scale datasets, e.g. MS COCO [22] and Flickr30k [27],
have led to end-to-end trainable image captioning models.
Most modern image captioning systems adopts the encoder-
decoder framework [36, 40, 38, 41], where a convolutional
neural network (CNN) encodes images into visual features,
and a RNN takes the image features as inputs to decode
them into sentences, typically trained end-to-end by maxi-
mum likelihood estimation. It has been shown that attention
mechanisms [40, 23, 1] and high-level attributes/concepts

4205

[42, 48] can help image captioning. Recently, reinforce-
ment learning is introduced into image captioning models
to directly optimize task-speciﬁc metrics [28, 46]. Some
works adopts GANs to generate human-like [29] or diverse
captions [21].

3. MSCap for Multi-Style Image Captioning

We ﬁrst present the overview of our MSCap framework
(Sec. 3.1), then describe each module of it and introduce
the objectives and strategy for training.

2.2. Stylized Image Captioning

Stylized image captioning aims at generating captions
that are successfully stylized and describe the image con-
tent accurately. Some works have been proposed to tackle
this task, which could be divided into two categories: mod-
els using parallel stylized image-caption data (supervised
mode) [25, 7, 31, 43] and models using non-parallel styl-
ized corpus (semi-supervised mode) [10, 24]. SentiCap [25]
handles the positive/negative styles and proposes to model
word changes with two parallel Long Short Term Memo-
ry networks (LSTM) and word-level supervisions. StyleNet
[10] handles the humorous/romantic styles by factoring the
input weight matrices to contain a style speciﬁc factor ma-
trix. SF-LSTM [7] experiments on the above four caption
styles and propose to learn two groups of matrices to cap-
ture the factual and stylized knowledge, respectively.

However, all these works are built to translate images in-
to captions of a single caption style, while our model can
simultaneously handle multiple styles. More similar to our
work, You et al. [43] propose two simple methods to inject
sentiments into image captions and can control the senti-
ment by providing different sentiment labels. However, this
model is trained in supervised mode, while our model work-
s in a harder semi-supervised mode with no requirement on
parallel stylized data.

2.3. Generative Adversarial Networks

The Generative Adversarial Networks (GANs) [11]
framework learn generative models without explicitly deﬁn-
ing a loss function for a target distribution. GANs has
shown promising results in ﬁelds of computer vision, in-
cluding image super-resolution [20], photo editing [6, 30],
domain adaptation [35, 5], image-to-image translation [26,
15, 9] and text-to-image translation [45]. Though GAN-
s have achieved great successes on computer vision appli-
cations, there are only little progress on applying it to se-
quence generation tasks because the non-differentiability of
discrete word tokens makes generator optimization difﬁcult.
Recently, some techniques have been proposed to addresses
the none-differentiable challenge [19, 44, 13]. In our work,
we employ the method proposed in [13] which uses continu-
ous relaxation to approximate the discrete sampling process
so that the training procedure can be effectively optimized
through back-propagation.

3.1. Framework Overview

The overall framework of the proposed MSCap is illus-
trated in Figure 2. It is comprised of ﬁve basic subnetworks,
i.e., an image encoder E, a caption generator G, a caption
discriminator D, a style classiﬁer C, and a back-translation
network T . We are given a factual dataset P = {(x, ˆyf )},
with paired image x along with its corresponding factual
caption ˆyf , and a collection of unpaired stylized sentences
P u = {(ˆys, s)}, s ∈ {s1...sk} containing captions of k dis-
tinctive styles, where ˆys denote a stylized caption with style
s. We regard the factual captions ˆyf as having the “factu-
al” style, denoted as s0, which would help model training
since the large dataset of factual captions can be included in
the training data. We denote the extended stylized corpus
dataset as P ′ = {(ˆys, s)}, s ∈ {s0, .., sk}. Given an im-
age x and a style label s, we aim at generating a sentence y
such that: 1) y is a natural sentence, 2) y is of style s, and
3) (x, y) forms a relevant pair.

The caption generator G conditions on the encoded im-
age features E(x) and a target style label s to generate a
sentence y, i.e. y = G(E(x), s). This sentence is fed
into D, C, and T for enforcing it to satisfy the three re-
quirements, respectively. Speciﬁcally, the discriminator D
classiﬁes whether a caption is a natural, human-like cap-
tion by distinguishing the fake generated caption y from re-
al human-written captions (ˆys, s ∈ {s0, .., sk}). The style
classiﬁer C produces probability distributions of y belong-
ing to each of the k + 1 style categories, and a style classi-
ﬁcation loss is thus calculated for enforcing y to be in the
given style s. The back-translation module T ensures y is
visually grounded on x. That is achieved by “translating”
y back into ˆyf (i.e. T (y, s) → ˆyf ) in the sense of cycle-
consistency [49]. The whole system is end-to-end trained
by using differentiable softmax approximation in the cap-
tion generator.

3.2. Image Encoder

Given an image x, we ﬁrst encode it to obtain image
features using a deep CNN. The image features could be
a static, global pooled representation of the image [37], or
the spatial visual features [40]. Based on the features, a
visual context vector is obtained for each time step, by di-
rectly using the static feature or calculating adaptively with
the soft-attention mechanism [40] from the visual features.
In this paper, we use the static feature to be consistent with
precious works, thus the context vector cv is cv = E(x).

4206

Figure 2. Overall framework of our MSCap. The multi-style caption generator takes in the encoded image feature and a style indicator
as input to generate a caption with target style. The adversarial loss, style classiﬁcation loss, and back-translation loss are then calculated
based on the discriminator, classiﬁer, and back-translation network, respectively. The red arrows denote gradient propagation enabled by
the differentiable approximation.

3.3. Caption Generator

We design a style-dependent caption generator G that
fully capture the language properties of each style by en-
abling directly training G with unpaired stylized captions.

Condition G on style labels. To effectively inject style
conditions into G, we use an (k + 1)-dimensional one-hot
vector to represent the k + 1 different styles, with each el-
ement represents a corresponding style. we ﬁrst feed s into
a style embedding layer and then concatenate the resulting
style embedding vector with the input word embedding vec-
tor as the input vector (wt) to the LSTM at each step.

Enable training on unpaired corpus. For unpaired styl-
ized corpus, its syntax and grammar rules are signiﬁcantly
different from that of the paired factual captions. Therefore,
it’s beneﬁcial to explicitly model the language properties
of the unpaired corpus. However, current models usually
adopt the “injecting” mode [34], which deeply couples the
visual and linguistic information inside the recurrent loop of
the RNN/LSTM, as is shown in Figure 3 (a). Such a mode
fails to capture the language properties of unpaired corpus
because the model cannot be trained without the presence
of images.

To address this problem, we base G on the “merging”
mode [34] and a style gate (as is shown in Figure 3 (b) ). We
ﬁrst move the visual context out of the LSTM, leaving the
LSTM modeling the linguistic information only. We then
introduce an additional multimodal fusion module to merge
the visual context cv and linguistic context cl
t for predicting
words. The style gate provides the word predictor a fallback
option to rely only on cl
t when the image is unavailable. In-
spired by [23], we design the style gate to adaptively assign

(2)

(1)

t; ht] + bg)),

gt = σ(wT
ct = gtcl

t is calculated by lt = σ(Wl[wt; cv; ht]+bl), cl

different weights to cv and cl
t:
g tanh(Wg[cl
t + (1 − gt)cv,
where [; ] indicates concatenation, ct is the mixed context
vector, ht is hidden state of LSTM, and σ is the sigmoid ac-
tivation. cl
t =
lt ⊙ tanh(mt), where lt is a gate vector, mt is the memo-
ry cell state of the LSTM, wt is the input vector, σ is the
sigmoid activation and ⊙ represents element-wise produc-
t. A higher gt means more focus on the linguistic context.
Finally, the mixed context vector ct is concatenated with
the hidden state ht and is then fed into the word classiﬁer
to produce the probability over the vocabulary of possible
words:

pt = softmax

Wo[ct; ht]
τ

,

(cid:19)

(cid:18)

(3)

where τ ∈ (0, 1) a temperature parameter. When train-
ing with the unpaired stylized corpus, it is natural to turn
i.e.
the style gate only to the linguistic context vector,
gt = 1, ct = cl
t. In this case, the model relies totally on
the linguistic context for word prediction, and becomes a
pure language model.

We pre-train the caption generator with both paired fac-
tual data P and unpaired stylized corpus P u by maximizing
the log-likelihood of the ground-truth captions:

θ∗ = arg max

E

(x,ˆyf )∈P log p(ˆyf |x, s0; θ)+

θ

(4)

E

(ˆys,s)∈P u log p(ˆys|s; θ),

where θ is the parameters of G and s0 denotes the factual
style.

3.4. Adversarial Loss

To make the generated captions indistinguishable from
real captions, we adopt adversarial training with a discrim-

4207

画的，加大高度

Visual
Context

LSTM

𝒈𝒈𝒕𝒕

Visual
Context

Style & Word
Embeddings

Multimodal 
Fusion

LSTM

Style & Word
Embeddings

(a) The injecting mode

(b) The merging mode

Figure 3. Comparison between the injecting and merging modes.
Our generator cooperates the merging mode with the style gate (gt
in (b)), which enables training directly with unpaired corpus.

inator D, where G generates a fake caption G(x, s) and D
tries to distinguish it from real captions. The adversarial
loss [11] is calculated by:

Ladv = Eˆy[log D(ˆy)] + Ex,s[1 − log D(G(x, s))],

(5)

where ˆy is a real caption from P ′, x is an image from P,
and s is a style label randomly sampled from {s0, .., sk}. G
tries to minimize this objective, while D tries to maximize
it.

3.5. Style Classiﬁcation Loss

Given an image x and a target style label s, it is required
that the generated caption should correctly own the target
style. To satisfy this condition, we employ an style classiﬁer
C to constrain the generated caption y to own the desired
style, i.e. C(G(x, s)) → s. The style classiﬁcation loss for
C and G is formulated as follows:

Lcls = Eˆy[− log C(s0|ˆy)] + Ex,s[− log C(s|G(x, s))].

(6)

3.6. Back Translation Loss

By minimizing the adversarial and classiﬁcation losses
(Eqn. 5 and 6) , G is trained to generate captions that are
human-like and classiﬁed to its correct target style. Howev-
er, minimizing the two losses along does not guarantee that
generated captions accurately describe the content of its in-
put images, i.e. visually grounded. To alleviate this prob-
lem, we introduce the back-translation module T to impose
a condition on the relation among y, ˆyf , and x.

We begin from the observation that the factual image-
caption pair (x, ˆyf ) shares the same content information.
From this point, the relevancy between the generated cap-
tion y and the image x can be approximated by the rele-
vancy between y and the “ground-truth” factual caption ˆyf .
Thus, we constrain y to be consistent with ˆyf in the sense
of sentence content. This is achieved by using the back-
translation module T that “translates” y back into yf , i.e.

T (y, s) → ˆyf . T is implemented as a multilingual neu-
ral machine translation (NMT) network in which the multi-
stylized captions are regarded as source languages, and the
factual caption as the target language. Concretely, T in-
cludes a text encoder that takes y and the target style labels
s as inputs, and a followed text decoder that takes the out-
puts of the text encoder as input to generate a sentence. We
then formulate the back-translation loss as minimizing the
negative log-likelihood of the factual caption:

Ltrans = E

(x,ˆyf ),s[− log p(ˆyf |G(x, s), s; T )].

(7)

Another possible method to enforce cycle-consistency is
directly translating y back into the image x (or image fea-
tures E(x)) [45, 47]. However, the text-to-image synthe-
sis itself is a tough task and so far the performance is far
from satisfaction. While translation between two sentences
is much more mature and practical.

3.7. Full Objectives

written, respectively, as

Finally, the objective functions for G, D, C, and T are

LG = −λadvLadv + λclsLcls + λtransLtrans,
LD = Ladv, LC = Lcls, LT = Ltrans,

(8)

where λadv, λcls and λtrans are hyper-parameters for bal-
ancing the losses.

3.8. Training Strategy

Adversarial training over the discrete samples generated
by G hinders gradients propagation. Although sampling-
based gradient estimator such as REINFORCE [39, 44] can
by adopted, we found that training with these methods can
be unstable due to the high variance of the gradient and also
inefﬁcient since Monte Carlo roll-out if often required. In-
stead, we employ the continuous approximation technique
proposed by Hu et al. [13] to enable end-to-end optimiza-
tion of the whole model.

Speciﬁcally, instead of sampling a single hard word
(one-hot vector) from pt (Eqn. 3), we consider the peaked
distribution vector pt itself as a soft word, which is the out-
put of G at the t-th step and servers as an input in the t+1-th
step. At the (t + 1)-th step, we compute the word embed-
ding vector with et+1 = Wept, where pt ∈ RN , et+1 ∈ Rd,
and We ∈ Rd×N is the word embedding matrix. et+1 is
then fed into the LSTM. The temperature τ gradually an-
neals to 0 (the discrete case) as training proceeds. We em-
pirically ﬁnd that this simple yet effective approach enjoys
low variance and fast convergence. In practice, we employ
the Wasserstein GAN [2] for optimizing the adversarial loss
Ladv.

4208

4. Experimental Setup

4.1. Dataset

We conduct experiments on two publicly available styl-
ized image caption datasets, FlickrStyle10K [10] and Sen-
tiCap [25], and a large factual image-caption dataset, MS
COCO [22]. COCO is a large image captioning dataset,
containing 82783, 40504 and 40775 images for training,
validation and test, respectively, with 5 factual captions for
each image. FlickrStyle10K contains 10K Flickr images
with stylized captions. However, only the 7K training set
are public, in which each image is labeled with 5, 1, and 1
captions for factual, humorous, and romantic styles, respec-
tively. Following [7], we randomly select 6,000 and 1,000
of them as the training and test sets, respectively. SentiCap
is an image sentiment captioning dataset based on COCO
images, which contains images that are labeled by 3 pos-
itive and 3 negative sentiment captions. The positive and
negative subsets contain 998/673 and 997/503 images for
training/testing, respectively. We randomly sample 100 im-
ages from each of the training splits for evaluation. For con-
venience, we denote the humorous, romantic, positive, neg-
ative styles, and factual as Humor, Roman, Pos, Neg, and
Fact, respectively. For stylized data, during training, on-
ly the captions from the training split are used, while when
testing, both the images and captions from the test split are
used for benchmarking the models. The training set of CO-
CO is used as the paired factual dataset P while the captions
from all the ﬁve styles are used as the unpaired stylized cor-
pus P ′.

4.2. Compared Approaches

There are only few works that address the stylized image
captioning problem with unpaired data (semi-supervised
learning) as ours do. Thus, we also compare our mod-
el with models using paired training data, i.e. learning in
fully-supervised mode. We compare our approach with the
following methods:

• NIC [36]:

the standard encoder-decoder model. We
train it with factual image-caption pairs from COCO
and treat it as the factual baseline.

• NIC-FT: We ﬁnetune the trained NIC model with
paired stylized data on each of the four styles sepa-
rately.

• SF-LSTM [7]: the current state-of-the-art supervised

model for single-style image captioning.

• StyleNet [10]: the single-style semi-supervised mod-
el that factors the input weight matrices to contain a
style speciﬁc factor matrix. We implement this mod-
el to ﬁrst pre-train it with paired factual data and then
separately train four models for each style.

4.3. Implementation Details

We extract the 2048-dimensional image features from
the last pooling layer of ResNet-101 [12]. The dimension-
s of the caption generator’s LSTM hidden states and word
embeddings are ﬁxed to 512 for all of the models discussed
herein. The dimensions of the style embeddings are set to
20. The discriminator D and classiﬁer C are implemented
as CNNs [16] with highway connections [17]. The back-
translation network T is built on two Gated Recurrent Unit
(GRU) [8] networks, which are used as the text encoder and
decoder, respectively. The global attention mechanism [3]
is adopted in the decoder to decide which part of the source
sentence to pay attention to. All the sub-networks share
the same word embedding and style embedding. We ﬁrst
pre-train the generator using both the paired factual image-
caption data and unpaired stylized corpus (Eqn. 4), with an
initial learning rate of 5 × 10−4.

After that, we train the whole network,

including
G, D, C, and T , all together according to Eqn. 8. We use
ADAM [18] optimizer for all the sub-networks, and use
ﬁxed learning rates of 5 × 10−5 for G, D, C and 5 × 10−4
for T . We train D for 5 times more than G. We use a mini-
batch size of 80. Beam search with a beam size of 3 is used
when testing. We use a ﬁxed temperature τ of 0.1. We set
λadv, λcls, and λtrans to 0.2, 1, and 5, respectively.

5. Experimental Results

5.1. Quality of Generated Captions

We evaluate the quality of generated captions in terms of
relevance with input images, ﬂuency, and accuracy of style.

Relevancy For each of the ﬁve styles, the image-stylized
caption pairs in the testing split could be used for bench-
marking the models [25, 7]. We report the widely used
automatic evaluation metrics, BLEU-1, BLEU-3, METE-
OR, and CIDEr [22]. These metrics are mostly based on n-
gram overlap, which are not perfect metrics for evaluating
stylied captions because stylized image captioning allows
more ﬂexibility for choosing words and phrases used to de-
scribe an image. Table 1 and 2 summarize the results on the
Pos/Neg and Roman/Humor styles, respectively. Compared
with the semi-supervised model, i.e. StyleNet, our multi-
style model achieves the best performance on all styles,
including Pos, Neg, Roman, and Humor. Compared with
fully-supervised models, our model is close to these models
on the Pos/Neg styles. While on the harder Roman/Humor
styles, the scores are lower because the humor/roman cap-
tions are typically much longer and more ﬂexible. Speciﬁ-
cally, our model gets comparable scores on BLEU-1, while
its BLEU-3 score is lower. That is corresponding to our in-
tuition: because BLEU-n measures the precision and recall

4209

Table 1. Performance comparisons on the test splits of Pos and Neg styles. unpaired means the model uses unpaired stylized text for
training, i.e. semi-supervised learning. B@n, M, C, ppl., cls. are short for BLEU-n, METEOR, CIDEr, perplexity, style classiﬁcation
accuracy (%), respectively. For ppl. smaller is better, for the others larger is better.

Un-
paired

Multi-
style

Model

NIC
NIC-FT
SF-LSTM
StyleNet
MSCap

Model

NIC
NIC-FT
SF-LSTM
StyleNet
MSCap

no
no
no
yes
yes

no
no
no
yes
yes

no
no
no
no
yes

no
no
no
no
yes

Un-
paired

Multi-
style

Positive

Negative

B@1 B@3 M

C

cls.

B@1 B@3 M

C

47.6
48.2
50.5
45.3
46.9

16.3
17.3
19.1
12.1
16.2

14.9
16.6
16.6
12.1
16.8

55.1
54.3
60.0
36.3
55.3

22.4
91.3
–
45.2
92.5

46.9
47.3
50.3
43.7
45.5

16.1
17.8
20.1
10.6
15.4

14.8
16.1
16.2
10.9
16.2

54.0
55.4
59.7
36.6
51.6

Table 2. Performance comparisons on the test splits of Roman and Humor styles.

Romantic

Humorous

B@1 B@3 M

C

cls.

B@1 B@3 M

C

25.1
26.9
27.8
13.3
17.0

7.0
7.5
8.2
1.5
2.0

10.6
11.0
11.2
4.5
5.4

33.0
35.4
37.5
7.2
10.1

24.3
82.6
–
37.8
88.7

25.5
26.3
27.4
13.4
16.3

7.2
7.4
8.5
0.9
1.9

9.7
10.2
11.0
4.3
5.3

33.5
35.1
39.5
11.3
15.2

ppl.

25.6
20.4
–
24.8
19.6

ppl.

61.6
27.7
–
52.9
20.4

ppl.

25.4
21.5
–
25.0
19.2

ppl.

57.1
31.8
–
48.1
22.7

cls.

23.2
89.5
–
56.6
93.4

cls.

25.5
80.1
–
41.9
91.3

of n-grams, it is too hard for a semi-supervised model to
achieve the exact matching of long phrases, e.g. 3-grams.

Table 3. Ablation study results. The scores of each metrics are its
average scores on four styles (e.g. Pos, Neg, Humor, Roman).

Cider

Perplexity ↓

Style acc.%

Fluency We evaluate the ﬂuency of the generated caption-
s in terms of the target style. We use a language modeling
toolkit, SRILM [32], to test the ﬂuency of generated sen-
tences. SRILM calculates the perplexity of the generated
sentences using the trigram language model trained on the
respective corpus. We train such language models on each
of the stylized corpus and compute the perplexity scores
(denoted as ppl.) for the generated captions of each style
and each model. Lower perplexity score of a caption indi-
cates it is more ﬂuent and appropriately stylized. The re-
sults are shown in Table 1 and 2 (see ppl. columns). As we
can see, our approach maintains the lowest perplexity scores
across all styles, including the supervised models. Particu-
larly, our model maintains signiﬁcantly better ﬂuency than
StyleNet.

Style accuracy We measure how often a generated cap-
tion has the correct target style according to a pre-trained
style classiﬁer. For this purpose, we use the TextCNN [16]
as a style judger. It’s trained on the P ′ dataset and achieves
nearly perfect accuracy of 97.8%. The results of the style
classiﬁcation accuracy (denoted as cls.) are shown in Table
1 and 2 (see cls. columns). As can be seen, across all styles,
our model achieves the highest style classiﬁcation accuracy
among all methods, including the oracle method, NIC-FT.

Model

NIC
NIC-FT
StyleNet

MSCap
MSCap w/o adv.
MSCap w/o cls.
MSCap w/o trans.
MSCap w/o XE.

43.9
45.1
22.9

33.1
14.6
20.5
7.72
30.3

42.4
25.4
37.7

20.5
57.1
49.6
13.6
22.2

23.9
85.9
45.4

91.5
66.7
30.0
96.0
88.7

Human evaluations Automatic evaluation metrics can-
not perfectly reﬂect the stylized captions’ quality in the
users’ minds. Therefore, we perform human evaluation
on the generated captions in terms of ﬂuency, relevancy
and style appropriateness. We randomly selected 50 im-
ages from the testing set and generate stylized captions for
each image, resulting totally 50 × 4 image-caption pairs to
be evaluated. We asked 10 volunteers to rate the captions.
The volunteers were asked to rank the generated captions
in terms of their ﬂuency, relevancy, and style appropriate-
ness. Fluency was rated from 0 (unreadable) to 3 (perfect).
Relevancy was rated from 0 (unrelated) to 3 (very related).
Style appropriateness means whether a caption appropriate-
ly owns the desired styles, rated from 0 (bad) to 3 (perfect).
The scores on each style and their average are shown in Ta-
ble 4. As we can see, our MSCap rates between 1.92 ∼ 2.62

4210

Factual

a man smiles near people as he
skateboards indoors.

two giraffes are standing outdoors
near a building.

a man riding skis down a snow
covered slope.

an elephant
grass and some trees.

is in some brown

Roman

a man jumping a skateboard in the
room, proud of his accomplishment.

two giraffes are walking through
the filed, exploring the woods.

a man in a black jacket is jumping
over a snow covered mountain to
experience the thrill of life.

a baby elephant is running through
the grass to meet his lover.

Humor

a man
tricks
skateboard to show off.

does

on

his

two hungry giraffes standing in the
filed looking for things to eat.

a lonely skier goes down an snowy
hill thinking of cute lady skiers.

a elephant is balancing on a grass
covered field.

Pos

Neg

a great image of a young people do
tricks on his skateboards.

two giraffes in a pleasant park are
against beautiful trees.

a amazing people stand on his skis
on a snowy hill.

an elephant enjoying the nice day
while standing on the grass.

a poor boy skateboards in the
crowded room.

two giraffes are against a broken
tree and dead grass.

a man stands on his broken skis in
the dirty snow.

a poor elephant is approaching a
dead filed.

Figure 4. Examples of the stylized captions generated by MSCap. Each column shows an image and its corresponding captions, while
captions at each row correspond to one of the caption styles: factual, romantic, humorous, positive, and negative.

on all the items among all styles, which could be considered
satisfactory since the highest score is 3.

5.3. Example Results

Table 4. Human evaluations results of the generated captions in
terms of ﬂuency, relevancy, and style appropriateness.

Style

Pos Neg Roman Humor Avg.

Fluency
Relevancy
Style

2.62
2.46
2.33

2.43
2.37
2.28

2.12
2.02
2.12

2.04
1.92
2.06

2.30
2.19
2.20

5.2. Ablation Study

We conduct ablation study to show how much each com-
ponent of MSCap contributes to the caption quality. Specif-
ically, we remove the adversarial loss (Ladv), the classiﬁca-
tion loss (Lcls) and the back-translation loss (Ltrans) from
the generator’s objective function (LG in Eqn. 8), denot-
ed as w/o adv., w/o. cls., and w/o.
trans., respectively.
To show the effect of our designed generator that enables
training directly with unpaired text (Eqn. 4), we train an-
other MSCap model that only use paired data during the
XE training, denoted as w/o XE.. The results are summa-
rized in Table 3. As we can see, without Ladv, the model
performs very poorly in almost all metrics. We found that
its output sentences are mostly non-ﬂuent, which contains
many repetitive words, such as “nice nice day”, “a a boy”.
Without Lcls, the model scores very low on the style classi-
ﬁcation accuracy, indicating that it fails to generate stylized
captions of desired style. Without Ltrans, though the mod-
el scores the lowest perplexity and highest style accuracy
scores, however, the CIDEr score decreases markedly. This
is because although the individual sentences are ﬂuent and

stylized, however, the captions are not event related to the
images. Also, we found a large number of captions are i-
dentical. The results validate the signiﬁcance and effective-
ness of the back-translation module to enforce relevancy be-
tween generated captions and images. Without pre-training
on unpaired stylized text (w/o XE.), the performance of the
model drops on all metrics. We infer that pre-training on
unpaired stylized text helps the generator to better capture
the language properties of the stylized data.

In Figure 4, we show four example captions generated by
our MSCap. We can see that the captions are ﬂuent, rele-
vant to the image, and also correctly stylized with the target
style. For example, the captions of the ﬁrst image contain
words (“proud”, “trick”, “great”, and “poor” ) that match
well with the desired styles (factual, romantic, humorous,
positive, and negative styles, respectively).

6. Conclusion

We have proposed the MSCap, a multi-style image cap-
tioning model trained using unpaired stylized corpus. M-
SCap can generate human-like, appropriately stylized, visu-
ally grounded, and style-controllable captions. In addition,
MSCap is a single uniﬁed model that can be easily scaled to
more caption styles. Extensive experiments demonstrated
the efﬁcacy of MSCap.

Acknowledgment

This work was supported by National Natural Science
Foundation of China (61872366 and 61472422) and Beijing
Natural Science Foundation (4192059).

4211

References

[1] Peter Anderson, Xiaodong He, Chris Buehler, Damien
Teney, Mark Johnson, Stephen Gould, and Lei Zhang.
Bottom-up and top-down attention for image captioning and
vqa. arXiv preprint arXiv:1707.07998, 2017.

[2] Martin Arjovsky, Soumith Chintala, and L´eon Bottou.
Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
Neural machine translation by jointly learning to align and
translate. arXiv preprint arXiv:1409.0473, 2014.

[4] Allan Bell. Language style as audience design. Language in

society, 13(2):145–204, 1984.

[5] Konstantinos Bousmalis, Nathan Silberman, David Dohan,
Dumitru Erhan, and Dilip Krishnan. Unsupervised pixel-
level domain adaptation with generative adversarial network-
s. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), volume 1, page 7, 2017.

[6] Andrew Brock, Theodore Lim, James M Ritchie, and Nick
Weston. Neural photo editing with introspective adversarial
networks. arXiv preprint arXiv:1609.07093, 2016.

[7] Tianlang Chen, Zhongping Zhang, Quanzeng You, Chen
Fang, Zhaowen Wang, Hailin Jin, and Jiebo Luo. ” factu-
al” or” emotional”: Stylized image captioning with adaptive
learning and attention. arXiv preprint arXiv:1807.03871,
2018.

[8] Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. Learning phrase representations using rn-
n encoder-decoder for statistical machine translation. arXiv
preprint arXiv:1406.1078, 2014.

[9] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha,
Sunghun Kim, and Jaegul Choo. Stargan: Uniﬁed genera-
tive adversarial networks for multi-domain image-to-image
translation. arXiv preprint, 1711, 2017.

[10] Chuang Gan, Zhe Gan, Xiaodong He, Jianfeng Gao, and Li
Deng. Stylenet: Generating attractive visual captions with
styles. In Proc IEEE Conf on Computer Vision and Pattern
Recognition, pages 3137–3146, 2017.

[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing X-
u, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
In Advances
Yoshua Bengio. Generative adversarial nets.
in neural information processing systems, pages 2672–2680,
2014.

[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. computer vi-
sion and pattern recognition, pages 770–778, 2016.

[13] Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhut-
dinov, and Eric P Xing. Toward controlled generation of text.
arXiv preprint arXiv:1703.00955, 2017.

[14] Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun,
Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi´egas,
Martin Wattenberg, Greg Corrado, et al. Google’s multilin-
gual neural machine translation system: enabling zero-shot
translation. arXiv preprint arXiv:1611.04558, 2016.

[15] Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee,
and Jiwon Kim. Learning to discover cross-domain relations

with generative adversarial networks. arXiv preprint arX-
iv:1703.05192, 2017.

[16] Yoon Kim. Convolutional neural networks for sentence clas-

siﬁcation. arXiv preprint arXiv:1408.5882, 2014.

[17] Yoon Kim, Yacine Jernite, David Sontag, and Alexander M
In AAAI,

Rush. Character-aware neural language models.
pages 2741–2749, 2016.

[18] Diederik Kingma and Jimmy Ba. Adam: A method for
arXiv preprint arXiv:1412.6980,

stochastic optimization.
2014.

[19] Matt J Kusner, Brooks Paige, and Jos´e Miguel Hern´andez-
Lobato. Grammar variational autoencoder. arXiv preprint
arXiv:1703.01925, 2017.

[20] Christian Ledig, Lucas Theis, Ferenc Husz´ar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew P Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-
realistic single image super-resolution using a generative ad-
versarial network. In CVPR, volume 2, page 4, 2017.
[21] Dianqi Li, Xiaodong He, Qiuyuan Huang, Ming-Ting Sun,
and Lei Zhang. Generating diverse and accurate visual cap-
tions by comparative adversarial learning. arXiv preprint
arXiv:1804.00861, 2018.

[22] Tsungyi Lin, Michael Maire, Serge J Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. euro-
pean conference on computer vision, pages 740–755, 2014.
[23] Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher.
Knowing when to look: Adaptive attention via a visual sen-
In Proceedings of the IEEE
tinel for image captioning.
Conference on Computer Vision and Pattern Recognition
(CVPR), volume 6, 2017.

[24] Alexander Mathews, Lexing Xie, and Xuming He. Semstyle:
Learning to generate stylised image captions using unaligned
text. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 8591–8600, 2018.
[25] Alexander Patrick Mathews, Lexing Xie, and Xuming He.
Senticap: Generating image descriptions with sentiments. In
AAAI, pages 3574–3580, 2016.

[26] Augustus Odena, Christopher Olah, and Jonathon Shlens.
Conditional image synthesis with auxiliary classiﬁer gans.
arXiv preprint arXiv:1610.09585, 2016.

[27] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C
Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flick-
r30k entities: Collecting region-to-phrase correspondences
for richer image-to-sentence models. In Proceedings of the
IEEE international conference on computer vision, pages
2641–2649, 2015.

[28] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jarret
Ross, and Vaibhava Goel. Self-critical sequence training for
image captioning. computer vision and pattern recognition,
2017.

[29] Rakshith Shetty, Marcus Rohrbach, Lisa Anne Hendricks,
Mario Fritz, and Bernt Schiele. Speaking the same language:
Matching machine to human captions by adversarial train-
In Proceedings of the IEEE International Conference
ing.
on Computer Vision (ICCV), 2017.

4212

[30] Zhixin Shu, Ersin Yumer, Sunil Hadap, Kalyan Sunkaval-
li, Eli Shechtman, and Dimitris Samaras. Neural face edit-
ing with intrinsic image disentangling. In Computer Vision
and Pattern Recognition (CVPR), 2017 IEEE Conference on,
pages 5444–5453. IEEE, 2017.

[31] Kurt Shuster, Samuel Humeau, Hexiang Hu, Antoine Bor-
des, and Jason Weston. Engaging image captioning via per-
sonality. arXiv preprint arXiv:1810.10665, 2018.

[32] Andreas Stolcke. Srilm-an extensible language modeling
toolkit. In Seventh international conference on spoken lan-
guage processing, 2002.

[33] Jinhui Tang, Xiangbo Shu, Zechao Li, Guo-Jun Qi, and Jing-
dong Wang. Generalized deep transfer networks for knowl-
edge propagation in heterogeneous domains. ACM Transac-
tions on Multimedia Computing, Communications, and Ap-
plications (TOMM), 12(4s):68, 2016.

[34] Marc Tanti, Albert Gatt, and Kenneth P Camilleri. Where to
put the image in an image caption generator. Natural Lan-
guage Engineering, 24(3):467–489, 2018.

[35] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell.
Adversarial discriminative domain adaptation. In Computer
Vision and Pattern Recognition (CVPR), volume 1, page 4,
2017.

[36] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Du-
mitru Erhan. Show and tell: A neural image caption gen-
erator. In Computer Vision and Pattern Recognition (CVPR),
2015 IEEE Conference on, pages 3156–3164. IEEE, 2015.

[37] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Du-
mitru Erhan. Show and tell: Lessons learned from the 2015
mscoco image captioning challenge. IEEE transactions on
pattern analysis and machine intelligence, 39(4):652–663,
2017.

[38] Jing Wang, Jianlong Fu, Jinhui Tang, Zechao Li, and Tao
Mei. Show, reward and tell: Automatic generation of nar-
rative paragraph from photo stream by adversarial training.
In Thirty-Second AAAI Conference on Artiﬁcial Intelligence,
2018.

[39] Ronald J. Williams. Simple statistical gradient-following al-
gorithms for connectionist reinforcement learning. Machine
Learning, 8(3-4):229–256, 1992.

[40] Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho,
Aaron C Courville, Ruslan Salakhudinov, Rich Zemel, and
Yoshua Bengio. Show, attend and tell: Neural image caption
generation with visual attention. international conference on
machine learning, pages 2048–2057, 2015.

[41] Zhilin Yang, Ye Yuan, Yuexin Wu, Ruslan Salakhutdinov,
and William W Cohen. Review networks for caption gener-
ation. 2016.

[42] Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, and Tao
Mei. Boosting image captioning with attributes. 2016.

[43] Quanzeng You, Hailin Jin, and Jiebo Luo.

Image cap-
tioning at will: A versatile scheme for effectively inject-
ing sentiments into image descriptions. arXiv preprint arX-
iv:1801.10121, 2018.

[44] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan:
Sequence generative adversarial nets with policy gradient. In
AAAI, pages 2852–2858, 2017.

[45] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaolei
Huang, Xiaogang Wang, and Dimitris Metaxas. Stackgan:
Text to photo-realistic image synthesis with stacked genera-
tive adversarial networks. arXiv preprint, 2017.

[46] Li Zhang, Flood Sung, Feng Liu, Tao Xiang, Shaogang
Gong, Yongxin Yang, and Timothy M Hospedales. Actor-
critic sequence training for image captioning. arXiv preprint
arXiv:1706.09601, 2017.

[47] Wei Zhao, Wei Xu, Min Yang, Jianbo Ye, Zhou Zhao, Yabing
Feng, and Yu Qiao. Dual learning for cross-domain image
captioning. In Proceedings of the 2017 ACM on Conference
on Information and Knowledge Management, pages 29–38.
ACM, 2017.

[48] Luowei Zhou, Chenliang Xu, Parker Koch, and Jason J Cor-
so. Image caption generation with text-conditional semantic
attention. arXiv preprint arXiv:1606.04621, 2016.

[49] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. arXiv preprint, 2017.

4213

