X-LineNet: Detecting Aircraft in Remote Sensing
Images by a pair of Intersecting Line Segments

Haoran Wei, Yue Zhang*, Bing Wang, Yang Yang, Hao Li and Hongqi Wang

1

9
1
0
2
 
c
e
D
 
4
2
 
 
]

V
C
.
s
c
[
 
 
3
v
4
7
4
2
1
.
7
0
9
1
:
v
i
X
r
a

Abstract—Motivated by the development of deep convolution
neural networks (DCNNs), tremendous progress has been gained
in the ﬁeld of aircraft detection. These DCNNs based detectors
mainly belong to top-down approaches, which ﬁrst enumerate
massive potential locations of objects with the form of rectangular
regions, and then identify whether they are objects or not.
Compared with these top-down approaches, this paper shows
that aircraft detection via bottom-up approach still performs
competitively in the era of deep learning. We present a novel
one-stage and anchor-free aircraft detection model in a bottom-
two
up manner, which formulates the task as detection of
intersecting line segments inside each target and grouping of
them without any rectangular region classiﬁcation. This model
is named as X-LineNet. With simple post-processing, X-LineNet
can simultaneously provide multiple representation forms of
the detection result: the horizontal bounding box, the rotating
bounding box, and the pentagonal mask. The pentagonal mask is
a more accurate representation form which has less redundancy
and can better represent aircraft than that of rectangular box.
Experiments show that X-LineNet outperforms state-of-the-art
one-stage object detectors and is competitive compared with
advanced two-stage detectors on both UCAS-AOD and NWPU
VHR-10 open dataset in the ﬁeld of aircraft detection.

Index Terms—Aircraft detection, Bottom-up methods, Deep

convolution neural network, Remote sensing image.

I. INTRODUCTION

D RIVEN by the rapid development of remote sensing

technologies, remote sensing images with ﬁner resolution
and clearer texture can be easily accessed by the modern
airborne and space borne sensors. In response to the demand
for automatic analysis of remote sensing data, object detection
has been widely researched, among which aircraft detection
occupies an important part owning to the successful applica-
tion in the ﬁeld of military and air transportation. However,
aircraft detection remains a great challenge on account of the
need for accurate and robust detection. Moreover, the complex
background and noise, as well as the variation of spatial
resolution of remote sensing images, increase the difﬁculty
of this task.

Haoran Wei and Bing Wang are with the Aerospace Information Research
Institute, Chinese Academy of Sciences, Beijing 100190, China, the School of
Electronic, Electrical and Communication Engineering, University of Chinese
Academy of Sciences, Beijing 100190, China and the Key Laboratory of
Network Information System Technology (NIST), Aerospace Information
Research Institute, Chinese Academy of Sciences, Beijing 100190, China e-
mail: weihaoran18@mails.ucas.ac.cn, wangbing181@mails.ucas.ac.cn.

Yue Zhang, Hao Li and Hongqi Wang are with the Aerospace Information
Research Institute, Chinese Academy of Sciences, Beijing 100190, China
and the Key Laboratory of Network Information System Technology (NIST),
Aerospace Information Research Institute, Chinese Academy of Sciences,
Beijing 100190, China e-mail: zhangyue@aircas.ac.cn.

Yang Yang is with the Unit 31008 of the PLA.

Conventional methods in aircraft detection mainly depend
on manually engineered features. Cheng et al.[1] develop a
discriminatively trained model for extracting Histogram of
Oriented Gradient (HOG )[2] feature pyramids from multiple
scale. Then threshold operation is performed on the response
of the model to judge the presence of aircraft. Bai et al.[3]
select structural feature description as input of ranking Support
Vector Machine(SVM )[4] to identify the existence of object.
Besides, Rastegar et al.[5] build an aircraft detection system
in combine Wavelet features with SVM classiﬁers. Though
these conventional methods are effective in speciﬁc scenes, the
generalization capability of them needs to be further improved
in practice.

Inspired from the success of deep convolution neural
networks (DCNNs)[6][7], various methods based on DC-
NNs have been proposed in the research ﬁeld of aircraft
detection[8][9][10]. Most of
these algorithms follow the
principle of top-down detectors[11][12][13]. Because they
can automatically extract
features through the backbone
networks[14][15], the accuracy as well as robustness is greatly
improved compared with manually engineered features. Cao et
al.[10] use selective search methods to generate a large set of
region proposals, and then send them into classiﬁers as well
as modify the ﬁnal bounding boxes in a regression manner.
Similarly, Ding et al.[8] adopt a region proposal network[11]
module to generate the proposals from a large set of anchor
boxes[11], which are candidate boxes with ﬁxed size densely
distributed over the feature map.

Although satisfactory results have been achieved by these
top-down approaches, there are still some limits of them. First,
this type of method converts aircraft detection into task of
classifying rectangular regions, but the rectangular region is
not an accurate representation for aircraft: rectangular box
not only includes many redundant pixels which belong to
background actually, but also will ignore some details, such
as direction contexts of aircraft. Besides, top-down aircraft
detectors rely on anchor mechanism heavily. In order to locate
the targets with no omission, a large set of anchor boxes with
different size and ratio are densely placed over the feature
maps. While only tiny set of boxes will have an overlap with
ground truth, leading to the imbalance in positive and negative
samples, which hinders the convergence of the network during
training. In addition, the targets of regression in anchor-based
detectors are the candidate boxes with the overlap with ground
truth above 0.7 typically, which doesn’t truly understand the
concrete aircraft’s visual grammar information[16][17][18].
Consequently, numerous redundant bounding boxes will be
generated, which require a series of post-processing such as

2

Fig. 1: We predict a pair of line segments to obtain the aircraft’s bounding boxes. The left upper ﬁgure is the line segments
inside the aircraft predicted by our model , the right upper ﬁgure is the horizontal bounding boxes, the left bottom one is the
rotating bounding boxes, and the last one is the pentagonal mask.

Non-Maximum Suppression(NMS) to ﬁlter the ﬁnal bounding
boxes. It is complex and wasteful. Moreover, many hyper
parameters are introduced related to anchors, such as sizes
and aspect ratios of anchor boxes. All of these require a lot
of prior knowledge to deploy.

Considering the limits mentioned above in top-down anchor-
based approaches, we promote a bottom-up anchor-free model
named X-LineNet to detect aircraft in this paper. X-LineNet is
inspired by CornerNet[19] which detects objects via predicting
and grouping paired corner points. However, CornerNet is not
suitable for the task of aircraft detection in remote sensing
images: a single optical image may contain a large amount
of aircraft, and this situation will frequently lead to errors
of grouping points detected in CornerNet. Unlike CornerNet,
our X-LineNet formulates this task as the detection of two line
segments inside each aircraft and clustering of linear segments.
These two sets of target segments are deﬁned as a pair of
linear area from the head of the aircraft to the tail part as well
as area between left and right wings. X-LineNet ﬁrst predicts
two heatmaps corresponding to two set of line segments
respectively, and then groups these line segments according
to their geometric relationships. As the result of dimension
ascending from point to line, the difﬁculty of points grouping
in CornerNet is solved. In addition, X-LineNet abandons the
anchor mechanism in top-down detectors[11][12][20][21] and
successfully solves the problems related to anchors aforemen-
tioned. Except for the commonly used horizontal bounding
boxes and rotating bounding boxes, X-LineNet can also pro-
vide a new output form of the detection result as pentagonal
mask, which has richer semantic description of each aircraft

target, such as the reﬁned position and direction information.
Fig. 1 illustrates these results.

Our contributions and innovations are as follows:
(1).We promote a novel aircraft detection model named X-
LineNet, which goes in a bottom-up manner. Different from the
dominant top-down methods in aircraft detection, this paper
shows that bottom-up approach can still perform competitively
in this ﬁled during the era of deep learning.

(2).X-LineNet transforms the task of aircraft detection in
remote sensing images to predicting and grouping of paired
intersecting line segments, which enables the network to learn
richer and speciﬁc visual grammar information.

(3).Besides horizontal and rotating rectangular bounding
boxes of detection result, one more accurate form as the
pentagonal mask can be generated within a single network
simultaneously. It contains less redundant context, and can
provide richer semantic description of an aircraft, such as the
direction and more accurate localization.

The rest of the paper is organized as follows: In Section II,
we introduce the related works done by researchers before and
basic principle in our method. The details of our network and
algorithms are shown in Section III. We place our experiments
results and analysis in Section IV. At
last, our work is
summarized and concluded in Section V.

II. RELATED WORKS

A. Top-down Aircarft Detectors with Anchors

Top-down frameworks based on anchor mechanism have
dominated the ﬁeld of aircraft detection[8][9][10] recently.

3

(a)

(b)

(c)

Fig. 2: The ﬁgure (a) shows the CornerNet’s effect, ﬁgure (b) for ExtremeNet and ﬁgure (c) for X-LineNet.

Generally,
detectors and one-stage detectors.

these methods can be classiﬁed into two-stage

1) Two-stage Detectors: Two-stage detectors decompose
the task of detection into subtasks of generating region pro-
posals and classifying of these candidate boxes. In the early R-
CNN[22], region proposals are produced under the algorithm
of selective search, which can be quite inefﬁcient as the
generation of many redundant proposals. In order to lift the
efﬁciency, SPP[23] and Fast R-CNN[24] design a special
pooling layer to improve it, but both of them are not end-
to-end trainable. Later, a region proposal network(RPN)[11]
is proposed in Faster R-CNN[11]. Mechanism of RPN can be
illustrated in detail as below.

After the feature maps extracted by the backbone[14] from
the original image, a large set of anchor boxes can be produced
by densely placing rectangular boxes with different size and
ratio. On accout of ﬁltering these numerous anchors, NMS and
scoring mechanism within RPN[11] are performed to generate
the ﬁnal regions of candidate. Final detection results can be
accessed after the classiﬁcation and modiﬁcation of these
proposals in parallel. Beneﬁted from the adoption of RPN[11],
the network can be trained end-to-end and the accuracy of
detection results greatly increased.

2) One-stage Detectors:

In pursuit of computation efﬁ-
ciency and inference speed, one-stage detectors get on detec-
tion within a single network away with region proposal mod-
ule. Initially, YOLOv1[25] without anchor mechanism can’t
provide accuracy comparable to that of two-stage detectors.
Later, anchor methods are also extensively utilized in one-
stage detectors[20][12][13] to improve the accuracy, massive
anchors with different scales are densely placed within certain
layers of the network.

Besides the gain in accuracy,

there are some problems
accompanied by the bringing in anchor mechanism in one-
stage detectors. First, a large set of anchors from multi scale

can be produced to supply sufﬁcient location information,
while only a tiny set of them are positive samples with the
overlap with ground truth above 0.7 or 0.5 typically. This
phenomenon leads to the imbalance in positive and negative
samples, which may prolong the process of training. Besides,
many hyper parameters are introduced, for example, concrete
settings for size and radio of anchor boxes in certain layers,
which bring in much inconvenience to adjust them carefully.
On account of drawbacks of anchor mechanism mentioned
above, there has been many works on modifying it. In Reti-
naNet[13], Focal Loss[13] is promoted to avoid the issue of
one-stage detector’s overwhelming by negative samples: In the
process of generating anchors, weights are adjusted according
to the distributions of samples. Furthermore, MetaAnchor[26]
is proposed to decrease the reliance of anchors’ settings on
prior knowledge: Beneﬁted from the sub network of anchor
function generator, hyper parameters of anchor boxes can be
learned dynamically and generalization of the model is greatly
enhanced.

Our approach falls into category of one-stage detectors as
the process of region proposal is discarded and accomplishes
the task of detection away with anchor mechanism. It is worth
noting that the target space of searching is reduced from the
O(w2h2) to O(wh) by abandoning anchors.

B. Bottom-up Detectors Based on DCNNs

In recent times, the research of detection in an Anchor-Free
and bottom-up manner becomes popular. CornerNet[19] and
ExtremeNet[16] are two representative detectors of this new
approach. Their idea is to transform the object detection task
into the processes of predicting and grouping keypoints, which
can be seen as detecting objects in a bottom-up way, and state-
of-the-art results has been attained by them in MS COCO[27]
benchmark. While it is not suitable to apply them into aircraft
detection in remote sensing images for the reasons followed:

4

Fig. 3: Illustration of our framework. The ”Conv” is convolution layer, ”BN” is Batch Normalization layer and ”ReLU” is
ReLU layer.

CornerNet’s[19] clustering algorithm draws on the method
of Newell et al.[28]. More speciﬁcally, they group each corner
points detected by embedding a vector on them. The basic
assumption is that the distance (Euclidean Distance) between
two corner points of the same object is small, and large be-
tween different objects. However, this process is not applicable
to an intensive occasion: with the large number of the same
class objects densely distributed in a single image, such as
remote sensing aircraft image, this algorithm has difﬁculty in
converging and results in embedding errors frequently.

The grouping method of ExtremeNet[16] is to match 4
extreme points with the center point of objects, and basic rules
are based on the assumption that if the geometric center point
of 4 extreme points match the predicted center point then the
same object they should belong to. However, this method is
still unable to be applied to detect aircraft due to the fact that
in many cases airplanes may be spatially symmetric in remote
sensing images.

Our model converts detection of points to line segments,
which effectively solves the clustering problems that present
Anchor-Free and bottom-up detectors fall
into in the task
of aircraft detection. Fig. 2 shows representative contrast
results between X-LineNet and other two bottom-up detectors
aforementioned.

A. Overview

III. X-LINENET

X-LineNet detects each aircraft as a pair of intersecting line
segments. One segment is deﬁned as the line from the head of
the aircraft to the tail. The other one is the line between left
and right wings. Fig. 3 shows an overview of our framework.
104-Hourglass[29] is selected as the backbone network of

our model for its powerful capability of feature fusion. Two
prediction modules in parallel after the backbone network are
in charge of predicting two line segments respectively with the
output form of heatmaps. When two aircraft are parked close in
an image, their corresponding line segments may be connected
together to form a single line segment wrongly, which we
call line segment adhesion problem. In order to solve this
problem, a third prediction module in parallel is added for the
prediction of four endpoints of two target line segments, which
acts as a basis for inspecting the phenomenon of adhesion and
locating the position of adhesion line segment. Furthermore,
in order to provide context of the direction of aircraft, a fourth
prediction module in parallel is designed to output the heatmap
of the head keypoint of each aircraft. With these four heatmaps
provided, simple post-processing algorithms are applied and
ﬁnally three forms of bounding boxes can be generated by
X-LineNet simultaneously. In this paper, we name these four
heatmaps as heatmap A, B, C and D in order.

B. Detection of Line Segments and Keypoints

Based on the feature map extracted by the backbone net-
work, four sets of heatmaps will be generated in parallel after
a series of layers in prediction modules. Supposing the shape
of the original input image is H ×W , the size of each heatmap
is H
D where D corresponds to the output stride. Each pixel
value y ∈ [0, 1] H
D × W
D in heatmaps represents the conﬁdence
of being judged as positive sample.

D × W

As shown in Fig. 4, let (xh, yh) be the head keypoint of the
aircraft and (xt, yt) be the keypoint of tail. And let (xl, yl) and
(xr, yr) be the keypoints of two wings of aircraft respectively.
For heatmap A and B, We deﬁne the segment from the head
of each aircraft to the tail part as L1[(xh, yh), (xt, yt)] , and

5

Fig. 4: Keypoints and line segments. The four points in the
ﬁgure correspond to (xh, yh), (xl, yl), (xt, yt), (xr, yr) respec-
tively. The two segments correspond to L1 and L2 respectively.

L2[(xl, yl), (xr, yr)] as the segment between left wings and
right wings. All pixels value within L1 and L2 set are ﬁxed
to the value of 1 as positive samples, and other pixels are
regarded as the background with their values setting to 0. In
the process of training, objective function is set to the pixel-
level logistic regression with a modiﬁed Focal Loss[13] named
Lls which means the loss of line segments:

Lls = −

(cid:40)

1
N

(cid:88)

xy

(1 − ´Vxy)γ log ´Vxy,
log(1 − ´Vxy),
´Vxy

γ

if Vxy = 1
if Vxy = 0

(1)

where N is the number of aircraft, ´Vxy represents the pixel
value at the coordinate (x, y) in heatmap A and B, and Vxy
corresponds to the ground truth. γ is a hyper-parameter in
original Focal Loss, here we set it to 2 in our model.

The adding of heatmap C is to deal with the line segment
adhesion problem (Section III-A). When one aircraft is parked
close to another in an image, this problem may occur in any
endpoints of two target line. In order to locate the position of
adhesion line segment, we let heatmap C output all the four
endpoints of each aircraft, so the ground truth of heatmap C is
the four endpoints (xh, yh), (xl, yl), (xt, yt), (xr, yr) of each
aircraft with values setting to 1.

We design the fourth prediction module to get the direction
information of aircraft. One head keypoint of each aircraft can
provide the direction information enough, so the ground truth
of heatmap D is the keypoint of the head (xh, yh) of each
aircraft with the value also setting to 1.

The endpoints loss and head keypoints loss corresponding
to heatmap C and heatmap D are named as Lep and Lhp
respectively. They follow the same form of Lls. The total loss
of our model can be expressed as:

Loss = Lls + αLep + βLhp

(2)

Fig. 5: Figure (a) shows the line segments predicted by our
model in heatmap is a line area actually. We ﬁnd the ﬁtting
ellipse of each line area and regard the long axis as the line
segment we need as shown in Figure (b).

Fig. 6: Illustration of the grouping of intersecting line seg-
ments.

C. From Heatmaps to Line Segments

The heatmaps output by our model are the same as con-
ﬁdence maps actually with values Y ∈ [0, 1]H×W . Per pixel
near the ground truth in them will be close to 1 in varying
degrees according to the distance, which represents the score
of this point considered to be ground truth. Therefore, as
shown in Fig. 5, that each line segment we need maps a line
area in the corresponding heatmap. In the test stage, in order
to extract target line segments, a method based on ellipse-
ﬁtting algorithm is applied. Concretely, we ﬁrst use a threshold
to transform heatmaps into binary images to ﬁlter out the
low score pixels. Then we ﬁnd line areas via the method of
extracting connected domains. Last the ﬁtting ellipse can be
obtained by ﬁtting each connected domain via ellipse-ﬁtting
algorithm. We deﬁne the long axis of each ellipse as the
desired line segment.

D. The Grouping of Line Segments

After the line segments extraction, the next step is grouping
them which belong to the same aircraft in pairs. We propose
two simple clustering rules according to the characteristics of
intersecting line segments. Supposing the L1 and L2 represent
the line segments extracted in heatmap A and B respectively
as shown in Fig. 6. The rules are as follows:

where α and β are the corresponding weights of losses.

(a). L2 is split equally by L1.

(b). The angle θ between L1 and L2 ranges in certain ﬁelds:

θ ∈ [60◦, 120◦].

The time complexity of our grouping algorithm is O(n2),
where n is the number of extracted line segments. It is the
same as NMS which used in anchor-based detectors.

Fig. 7: When two aircraft are parked close, the shape and size
of corresponding point region of endpoints where the adhesion
problem occur are different from other ones in heatmap C.

Because the existence of output stride in our model, if two
aircraft are close to each other, they may be stick together due
to the down sampling. It may lead the adhesion of our target
line segments, which we call line segment adhesion problem.
We design an algorithm named Cutting-line Algorithm to
address this problem.

The aim of Cutting-line Algorithm is to inspect close aircraft
and locates the position of adhesion, then separates the target
adhesion line segments. In order to achieve this aim, we
introduce heatmap C which outputs four endpoints of each
aircraft. The values of pixels in heatmap C will be in the range
of 0 to 1 and each endpoint is represented by a small point
region actually as mentioned in (Section III-C). As shown in
Fig. 7, the shape and size of point region in the position
of adhesion are different from other ones, which is the key
of our Cutting-line Algorithm to distinguish adhesion from
non adhesion. We extract each point region via the method of
ﬁnding connected domain aforementioned, and then introduce
the minimum enclosing rectangle of per connected domain,
and the shape as well as the size of point region can be
represented by the relationship among weight and height of
corresponding minimum enclosing rectangle.

Supposing that ˆH and ˆW correspond to the height and
width of the minimum enclosing rectangle, we introduce two
relaxation conditions for the situation of non adhesion:

6

ˆH × ˆW < α
max( ˆH, ˆW ) ÷ min( ˆH, ˆW ) < β

α is threshold 1(3)
β is threshold 2(4)

Then, we can ﬁnd out the minimum enclosing rectangle of
connected domain that do not meet the relaxation conditions
above, and there is a great probability the adhesion problem
occur. With the adhesion connected domains be screened, we
can separate the adhesion line segments in heatmap A and B
according to the positions of these adhesions in heatmap C.
Algorithm 1 shows this speciﬁc process, and Fig. 8 shows the
effect of Cutting-line Algorithm.

Algorithm 1 Cutting-line Algorithm.

1: let D be a collection of 4-connected domains in heatmap

2: for each d ∈ D do
3:
4:
5:

obtain the minimum external rectangle R of d;
let ˆH and ˆW be the length and width of R;
let pc be the center point of R, and P is an empty

let α and β be two constant thresholds.
if ˆH × ˆW < α or max( ˆH, ˆW ) ÷ min( ˆH, ˆW ) < β

C

array;

then

6:
7:

8:

Continue

else

end if

P ← pc

9:
10:
11:
12: end for
13: for each p ∈ P in heatmap A and B do
14:

15:
16: end for

Fig. 8: Illustration of the effect of Cutting-line Algorithm.

F. The Generation of Pentagonal Mask

X-LineNet can generate three forms of bounding boxes as
shown in Fig. 9. For the purpose of generating the pentagonal
mask with extra direction context of aircraft, we need to
identify the speciﬁc meaning of these four endpoints of two

E. Cutting-line Algorithm for Line Segments Adhesion Prob-
lem

per pixel of La[(px − k/2, py), (px + k/2, py)] = 0
per pixel of Lb[(px, py − k/2), (px, py + k/2)] = 0

7

Fig. 9: The process of generating bounding boxes.

target line segments after the clustering in Section III-D. The
output of heatmap D is to achieve this aim. Heatmap D
provides the position of head point of each aircraft, and we
can match it via Euclidean distance to two endpoints of L1
which deﬁned as the line segment between the head of aircraft
to the tail. The endpoint of L1 with the largest matching
value is regarded as the head point and the other one is the
tail. After the head and tail point are found by us, other two
keyponits of L2 that between two wings can be gained easily.
It is noteworthy that the bottom of the pentagonal mask is a
line, so we deﬁne the line as the direction parallel to L2 and
the length of it are ﬁxed as 1
5 length of L2. With the above
operation, the pentagonal mask can be obtained.

IV. EXPERIMENTS

A. Datatsets

There are two datasets in our experiments. The ﬁrst one
is UCAS-AOD[35], which has high quality remote sensing
images and sufﬁcient rotating bounding box annotations of
aircraft. The format of annotations in UCAS-AOD is shown in
Fig. 10. Considering there are no direct line segment anno-
tations of aircraft in current open dataset, the line segments
that our X-LineNet need have to be transformed from the
rotating bounding box annotations. We take the median line
of each rotating rectangle box as the two line segments that
need to be predicted by our model. However, we ﬁnd the
directly converted annotation is not accurate enough, such
as the line segments between left and right wings is not
always located at the middle line of the rotating bounding
box, which may be bad for our model. In order to explore
the full capability of X-LineNet, we release a new aircraft
keypoint dataset named Aircraft-KP. Images of training sets

Fig. 10: The label formats of UCAS-AOD and Aircraft-KP
dataset.

and testing sets in Aircraft-KP are the same as UCAS-AOD and
we just provide a whole new set of labels. In Aircraft-KP, each
aircraft is labeled as ﬁve keypoints along the anti-clockwise
direction by us for better versatility. Concretely, these ﬁve
endpoints are deﬁned as following order: head of aircraft, left
wing, left tail, right tail, right wing as shown in Fig. 10. Note
that we deﬁne the midpoint of both sides of the tail part as

8

TABLE I: State-of-the-art comparisons. The input size is the resolution fed into the model during training. S means single
scale and M means multi-scale. The RRPN is the Rotation Region Proposal Networks. The R2CNN indicates Rotational Region
CNN. The R-DFPN means the Rotation Dense Feature Pyramid Networks.

Models

Backbone

Input size

AP 1

AP 2

AP m

AP 1

AP 2

AP m

Horizontal Bounding Box

Training on Aircraft-KP

Training on UCAS-AOD

Two-stage detectors
Faster R-CNN[11]
Faster R-CNN[15]
Faster R-CNN[15]
Faster R-CNN+FPN[30]
Cascade R-CNN[31]

One-stage detectors
YOLO9000[20]
YOLOv3[21]
SSD[12]
RetinaNet[13]
ConerNet(S)[19]
ConerNet(M)[19]
ExtremeNet[16]
Our

vgg16
ResNet-50
ResNet-101
ResNet-101
ResNet-101

1200×600
1200×600
1200×600
1000×600
1024×512

85.9% 85.6% 85.7% 86.4% 85.2% 85.8%
88.7% 88.3% 88.5% 88.4% 88.2% 88.3%
90.1% 90.5% 90.3% 89.8% 90.4% 90.1%
91.3% 91.2% 91.2% 92.4% 91.5% 91.9%
92.8% 92.1% 92.4% 93.1% 91.8% 92.4%

DarkNet-19
DarkNet-53
ResNet-101
ResNet-101
104-Hourglass
104-Hourglass
104-Hourglass
104-Hourglass

544×544
608×608
513×513
512×512
511×511
511×511
511×511
511×511

82.2% 81.9% 82.1% 82.4% 81.4% 81.9%
86.4% 86.2% 86.3% 85.4% 85.5% 85.4%
89.6% 89.4% 89.5% 88.7% 89.4% 89.0%
90.1% 90.3% 90.2% 89.8% 90.9% 90.3%
76.5% 75.4% 75.9% 76.2% 77.4% 76.8%
76.7% 77.3% 77.0% 76.8% 77.4% 77.1%
78.3% 77.6% 77.9% 78.3% 77.2% 77.7%
92.5% 92.1% 92.3% 91.9% 91.2% 91.5%

Rotating Bounding Box

Training on Aircraft-KP

Training on UCAS-AOD

RRPN[32]
R2CNN[33]
R-DFPN[34]
Our

ResNet-50
ResNet-101
ResNet-101
104-Hourglass

1000×600
1000×600
1000×600
511×511

90.1%
90.7%
91.0%
92.8%

-
-
-
-

-
-
-
-

90.4%
90.9%
90.6%
91.3%

-
-
-
-

-
-
-
-

the tail point in our experiments. Aircraft-KP is available at
https://github.com/Ucas-HaoranWei.

B. Training and Testing Details

There are 7482 targets aircraft in 1000 images totally in
UCAS-AOD and Aircraft-KP. We randomly select 800 images
leaving other 200 for testing. In order
used for training,
to test the generalization performance of X-LineNet, NWPU
VHR-10[36] dataset
is merged to our test experiment. It
contains about 700 aircraft in 80 images. Our experiments
are performed on one RTX-2080Ti GPU, I9-9900K CPU with
PyTorch 1.0[37].

We train X-LineNet on the new datasets(Aircraft-KP) and the
original datasets(UCAS-AOD) respectively. During the training
phase, we set the input resolution to 511 × 511 and the output
resolution to 128 × 128 following settings in CornerNet[19].
In order to get rid of the risk of overﬁtting, some data argu-
mentation methods are applied, including random horizontal
ﬂipping, vertical ﬂipping and color dithering. Adam[38] is
selected as the optimizer for X-LineNet with learning rate of
0.0025. We train our network from scratch for 40000 iterations
with the batch size setting to 4. The total loss(Section III-B)
of X-LineNet is as follows :

Loss = Lls + αLep + βLkp

(5)

where both α and β are set to 0.5 in training process.

During the stage of test, We keep the original resolution
of image instead of resizing it to a ﬁxed size and all of
our test results are obtained from a single scale. In our
experiments, we test the output form of horizontal and rotating
bounding box respectively. We use the average precision(AP)
as the evaluation metrics which are deﬁned in PASCAL VOC
Challenge[39]. We deﬁne AP 1 as the test result on UCAS-
AOD and Aircraft-KP, AP 2 as the result on NWPU VHR-10
and their average as AP m. We choose the default parameters
in PASCAL VOC with IoU(Intersection over Union) which is
0.5 during testing. Note that NWPU VHR-10 dataset is only
added to the horizontal bounding box test process because
it lacks the annotations of the rotating bounding box. The
thresholds are used to transform heatmaps to binary images
are all setting to 0.3. In Cutting-line Algorithm (Section III-E),
the α and β of relaxation conditions are setting to 100 and
1.5 respectively. And number of cutting pixel k is setting to
10 in our experiments.

C. Comparisons with State-of-the-art Frameworks

In this part, we ﬁrst prove the advancement of X-LineNet
on the new dataset Aircraft-KP. For the sake of testifying the
satisfactory performance of our model does not depend on the
introduction of more precise annotations in Aircraft-KP, we
then carry on the experiment on original UCAS-AOD.

1) Experiment on Aircraft-KP: We compare the perfor-
mance of X-LineNet with other state-of-the-art detectors in

9

Fig. 11: The aircraft can still be detected by our model in an occasion that it is severely shielded. Figure (a) is the effect of
X-LineNet. Figure (b) and (c) are effects of Faster R-CNN+FPN and Cascade R-CNN respectively.

aircraft detection. Annotations of training sets used by other
detectors are converted from Aircraft-KP. Table I shows the
results. Our model achieves an AP m of 92.3%, outperforming
all reported one-stage object detectors and going far beyond
other bottom-up models. X-LineNet is even competitive to the
most advanced two-stage detectors.

For the output form of rotating bounding box, we select
several state-of-the-art models used for detecting oriented
objects in remote sensing images for comparison. As shown
in Table I, our X-LineNet improves about 2% AP 1 compared
with other top-down detectors which use a design of rotated
anchors.

It is noteworthy that the aircraft can still be detected by
our model in an occasion that it is severely shielded or not
fully displayed at the edge of the image due to the fact that
our network enhances the learning of target’s visual grammar
information. The effects are shown in Fig. 11.

2) Experiment on UCAS-AOD: In order to prove the excel-
lent performance of X-LineNet is not relied on the additional
annotations in Aircraft-KP, we conduct another experiment
on the original UCAS-AOD. As known in Section IV-A, X-
LineNet can also be trained with the data converted from
the orginal annotations of UCAS-AOD directly instead of
Aircraft-KP labeled by us. We deﬁne the two median lines

10

Fig. 12: Qualitative results output by X-LineNet.

of the original labeled rotating bounding box as the segments
that need to be detected as shown in Fig. 10. It is worth
mentioning that our model can still achieve good accuracy with
the converted training sets without additional manual tagging.
State-of-the-art detectors used for comparison are the same as

Section IV-C1. It’s just that they are trained directly through
the original annotations of UCAS-AOD. Table I also shows
the contrast results. Aircraft-KP gives an AP m improvement
of 0.8% than the training data converted from UCAS-AOD
directly. Because the endpoints of segments in Aircraft-KP

are explicit and clearer which help the Cutting-line Algorithm
(Section III-E) be implemented better. The validation experi-
ment we conducted is shown in Section IV-D3.

D. Ablation Studies

1) 104-Hourglass with Anchors: Unlike X-LineNet, current
top-down and anchor-based object detectors do not use 104-
Hourglass as a backbone. In order to show the excellent perfor-
mance of X-LineNet is not only dependent on the contribution
of 104-Hourglass, we replace the backbone of an anchor-based
and one-stage detector RetinaNet[13] with 104-Hourglass. The
setting of anchors is in multi-scale in unsampling stage of
104-Hourglass. The evaluation metrics are AP of horizontal
bounding boxes. Table II shows that with 104-Hourglass as
the backbone network, our X-LineNet improves AP m by 9.9%
than the anchor-based model. In conclusion, compared with
anchor-based methods, the performance improvement of X-
LineNet is not due to the different backbone networks.

TABLE II: Comparisons of X-LineNet and 104-Hourglass
with anchors.

Methods

X-LineNet
104-Hourglass+anchors

AP 1

92.5%
82.2%

AP 2

92.1%
82.6%

AP m

92.3%
82.4%

2) Different Backbone Networks: To verify the necessity of
104-Hourglass network for our model, we replace the 104-
Hourglass with ResNet-101+FPN[30] which is more com-
monly used in popular detectors at present. We only use the
ﬁnal output of FPN for predictions. The model is trained
following the same training procedure in Section IV-B. The
detection metrics are also AP of horizontal bounding boxes.
Table III shows the contrast results. The experimental results
show that X-LineNet with 104-Hourglass outperforms the X-
LineNet with ResNet-101+FPN by 9.4% AP m. Therefore, the
matching of our model with 104-Hourglass network is very
necessary.

TABLE III: Comparisons of Different Backbone Networks.

Different Backbones

104-Hourglass
ResNet-101+FPN

AP 1

92.5%
83.2%

AP 2

92.1%
82.7%

AP m

92.3%
82.9%

3) Effects of Cutting-line Algorithm:

In order to verify
the signiﬁcance of Cutting-line Algorithm (Section III-E), we
compare the performance of our method with Cutting-line
Algorithm and without it respectively, on both UCAS-AOD
and Aircraft-KP datasets. Evaluation metrics are AP of hori-
zontal bounding boxes. The experimental results on Aircraft-
KP and UCAS-AOD show that Cutting-line Algorithm gives
decent AP m improvements of 1.4% and 0.8% respectively. It
effectively solves the issue of adjacent segments which may
occur in an occasian that aircraft are densely placed in a
single image. As a result, the accuracy is effectively improved
without introducing much cost of computation. Table IV shows
the results.

11

TABLE IV: Effects of Cutting-line Algorithm.

With or without Cutting-line Algorithm

Training on Aircraft-KP

AP 1

AP 2

Training on UCAS-AOD

without cutting
with cutting

without cutting
with cutting

91.2%
92.5%

AP 1

91.0%
91.9%

90.7%
92.1%

AP 2

90.5%
91.2%

AP m

90.9%
92.3%

AP m

90.7%
91.5%

It is notable that Aircraft-KP with Cutting-line Algorithm
gives an AP m improvement of 0.8% than the training data
converted from UCAS-AOD directly because the features of
endpoints of the segments in Aircraft-KP are more dominant
as shown in Fig. 10, so that the location of endpoints can be
detected more easily by X-LineNet (Section III-E).

V. CONCLUSION

In conclusion, we present a novel one-stage and anchor-
free model named X-LineNet, which detects aircraft in remote
sensing images based on a bottom-up method. We switch the
task of aircraft detection into detecting and grouping a pair
of intersecting line segments, which enables our X-LineNet
to learn richer and speciﬁc visual grammar information of
aircraft. As a result, our model outperforms state-of-the-art
one-stage detectors and is still competitive compared with
advanced two-stage detectors in the ﬁeld of aircraft detection.

REFERENCES

[1] G. Cheng, J. Han, L. Guo, X. Qian, P. Zhou, X. Yao, and X. Hu, “Object
detection in remote sensing imagery using a discriminatively trained
mixture model,” ISPRS Journal of Photogrammetry and Remote Sensing,
vol. 85, pp. 32–43, 2013.

[2] N. Dalal and B. Triggs, “Histograms of oriented gradients for human
detection,” in international Conference on computer vision & Pattern
Recognition (CVPR’05), vol. 1.
IEEE Computer Society, 2005, pp.
886–893.

[3] X. Bai, H. Zhang, and J. Zhou, “Vhr object detection based on
structural feature extraction and query expansion,” IEEE Transactions
on Geoscience and Remote Sensing, vol. 52, no. 10, pp. 6508–6520,
2014.

[4] C. Cortes and V. Vapnik, “Support-vector networks,” Machine learning,

vol. 20, no. 3, pp. 273–297, 1995.

[5] S. Rastegar, A. Babaeian, M. Bandarabadi, and Y. Toopchi, “Airplane
detection and tracking using wavelet features and svm classiﬁer,” in
2009 41st Southeastern Symposium on System Theory.
IEEE, 2009,
pp. 64–67.

[6] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner et al., “Gradient-based
learning applied to document recognition,” Proceedings of the IEEE,
vol. 86, no. 11, pp. 2278–2324, 1998.

[7] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in neural infor-
mation processing systems, 2012, pp. 1097–1105.

[8] P. Ding, Y. Zhang, W.-J. Deng, P. Jia, and A. Kuijper, “A light and faster
regional convolutional neural network for object detection in optical
remote sensing images,” ISPRS journal of photogrammetry and remote
sensing, vol. 141, pp. 208–218, 2018.

[9] F. Zhang, B. Du, L. Zhang, and M. Xu, “Weakly supervised learning
based on coupled convolutional neural networks for aircraft detection,”
IEEE Transactions on Geoscience and Remote Sensing, vol. 54, no. 9,
pp. 5553–5563, 2016.

12

[34] X. Yang, H. Sun, K. Fu, J. Yang, X. Sun, M. Yan, and Z. Guo,
“Automatic ship detection in remote sensing images from google earth
of complex scenes based on multiscale rotation dense feature pyramid
networks,” Remote Sensing, vol. 10, no. 1, p. 132, 2018.

[35] H. Zhu, X. Chen, W. Dai, K. Fu, Q. Ye, and J. Jiao, “Orientation
robust object detection in aerial images using deep convolutional neural
network,” in 2015 IEEE International Conference on Image Processing
(ICIP).

IEEE, 2015, pp. 3735–3739.

[36] G. Cheng, P. Zhou, and J. Han, “Learning rotation-invariant convolu-
tional neural networks for object detection in vhr optical remote sensing
images,” IEEE Transactions on Geoscience and Remote Sensing, vol. 54,
no. 12, pp. 7405–7415, 2016.

[37] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,
A. Desmaison, L. Antiga, and A. Lerer, “Automatic differentiation in
pytorch,” 2017.

[38] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2014.

[39] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisser-
man, “The pascal visual object classes (voc) challenge,” International
journal of computer vision, vol. 88, no. 2, pp. 303–338, 2010.

[10] Y. Cao, X. Niu, and Y. Dou, “Region-based convolutional neural
networks for object detection in very high resolution remote sensing
images,” in 2016 12th International Conference on Natural Computa-
tion, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD).
IEEE,
2016, pp. 548–554.

[11] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
object detection with region proposal networks,” in Advances in neural
information processing systems, 2015, pp. 91–99.

[12] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.
Berg, “Ssd: Single shot multibox detector,” in European conference on
computer vision. Springer, 2016, pp. 21–37.

[13] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll´ar, “Focal loss
for dense object detection,” in Proceedings of the IEEE international
conference on computer vision, 2017, pp. 2980–2988.

[14] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.
[15] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.

[16] X. Zhou, J. Zhuo, and P. Krahenbuhl, “Bottom-up object detection by

grouping extreme and center points,” pp. 850–859, 2019.

[17] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan,
“Object detection with discriminatively trained part-based models,”
IEEE transactions on pattern analysis and machine intelligence, vol. 32,
no. 9, pp. 1627–1645, 2009.

[18] R. B. Girshick, P. F. Felzenszwalb, and D. A. Mcallester, “Object

detection with grammar models,” pp. 442–450, 2011.

[19] H. Law and J. Deng, “Cornernet: Detecting objects as paired key-
points,” in Proceedings of the European Conference on Computer Vision
(ECCV), 2018, pp. 734–750.

[20] J. Redmon and A. Farhadi, “Yolo9000: better, faster, stronger,” in
Proceedings of the IEEE conference on computer vision and pattern
recognition, 2017, pp. 7263–7271.

[21] ——,

“Yolov3: An

incremental

improvement,”

arXiv

preprint

arXiv:1804.02767, 2018.

[22] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature
hierarchies for accurate object detection and semantic segmentation,”
in Proceedings of the IEEE conference on computer vision and pattern
recognition, 2014, pp. 580–587.

[23] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep
convolutional networks for visual recognition,” IEEE transactions on
pattern analysis and machine intelligence, vol. 37, no. 9, pp. 1904–
1916, 2015.

[24] R. Girshick, “Fast r-cnn,” in Proceedings of the IEEE international

conference on computer vision, 2015, pp. 1440–1448.

[25] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look
once: Uniﬁed, real-time object detection,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2016, pp. 779–
788.

[26] T. Yang, X. Zhang, Z. Li, W. Zhang, and J. Sun, “Metaanchor: Learning
to detect objects with customized anchors,” in Advances in Neural
Information Processing Systems, 2018, pp. 320–330.

[27] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in European conference on computer vision. Springer, 2014,
pp. 740–755.

[28] A. Newell, Z. Huang, and J. Deng, “Associative embedding: End-to-
end learning for joint detection and grouping,” in Advances in Neural
Information Processing Systems, 2017, pp. 2277–2287.

[29] A. Newell, K. Yang, and J. Deng, “Stacked hourglass networks for
human pose estimation,” in European Conference on Computer Vision.
Springer, 2016, pp. 483–499.

[30] T.-Y. Lin, P. Doll´ar, R. Girshick, K. He, B. Hariharan, and S. Belongie,
“Feature pyramid networks for object detection,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2017,
pp. 2117–2125.

[31] Z. Cai and N. Vasconcelos, “Cascade r-cnn: Delving into high quality
object detection,” in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2018, pp. 6154–6162.

[32] J. Ma, W. Shao, H. Ye, L. Wang, H. Wang, Y. Zheng, and X. Xue,
“Arbitrary-oriented scene text detection via rotation proposals,” IEEE
Transactions on Multimedia, vol. 20, no. 11, pp. 3111–3122, 2018.
[33] Y. Jiang, X. Zhu, X. Wang, S. Yang, W. Li, H. Wang, P. Fu, and
Z. Luo, “R2cnn: Rotational region cnn for orientation robust scene text
detection,” arXiv preprint arXiv:1706.09579, 2017.

