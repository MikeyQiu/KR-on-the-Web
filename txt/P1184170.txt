8
1
0
2
 
n
a
J
 
4
 
 
]
S
D
.
h
t
a
m

[
 
 
1
v
6
3
2
1
0
.
1
0
8
1
:
v
i
X
r
a

Multistep Neural Networks for Data-driven Discovery of
Nonlinear Dynamical Systems

Maziar Raissi1, Paris Perdikaris2, and George Em Karniadakis1

1Division of Applied Mathematics, Brown University,
Providence, RI, 02912, USA
2Department of Mechanical Engineering and Applied Mechanics,
University of Pennsylvania,
Philadelphia, PA, 19104, USA

Abstract

The process of transforming observed data into predictive mathematical
models of the physical world has always been paramount in science and en-
gineering. Although data is currently being collected at an ever-increasing
pace, devising meaningful models out of such observations in an automated
fashion still remains an open problem.
In this work, we put forth a ma-
chine learning approach for identifying nonlinear dynamical systems from
data. Speciﬁcally, we blend classical tools from numerical analysis, namely
the multi-step time-stepping schemes, with powerful nonlinear function ap-
proximators, namely deep neural networks, to distill the mechanisms that
govern the evolution of a given data-set. We test the eﬀectiveness of our
approach for several benchmark problems involving the identiﬁcation of com-
plex, nonlinear and chaotic dynamics, and we demonstrate how this allows us
to accurately learn the dynamics, forecast future states, and identify basins
of attraction. In particular, we study the Lorenz system, the ﬂuid ﬂow be-
hind a cylinder, the Hopf bifurcation, and the Glycoltic oscillator model as
an example of complicated nonlinear dynamics typical of biological systems.

Keywords:
Machine learning, Systems identiﬁcation, Reduced order modeling,
Data-driven forecasting, Lorenz system, Navier-Stokes

Preprint submitted to Journal Name

January 8, 2018

1. Introduction

Dynamical systems play a key role in shaping our understanding of the
physical world and in deﬁning our ability to predict the evolution of a given
process. From the simple swinging motion of a clock pendulum to the com-
plex ﬂow around an airplane wing, the mathematical modeling of dynamical
systems can yield a set of tools with which we can analyze the way the current
state of the system depends on the past, and predict the possible states we
may encounter in the future. Often such tools are precisely known, usually
coming in the form of diﬀerential equations that are derived from ﬁrst phys-
ical principles, such as the conservation of energy, mass, and momentum [1].
However, in many cases, the sheer complexity of a system can prohibit our
complete understanding and render a ﬁrst principles approach infeasible. In
this setting, one may be only able to postulate crude and potentially overly
simpliﬁed models based on a given a set of empirical observations (see e.g.,
models for tumor growth [2], social dynamics [3], and the stock market [4]).
In the present era of abundant data and advanced machine learning capa-
bilities, a natural question arises: can we automatically discover suﬃciently
sophisticated and accurate mathematical models of complex dynamical sys-
tems directly from data?

The answer to this question pertains to the well-established ﬁeld of sys-
tems identiﬁcation [5]. Discriminating between white-, gray-, and black-box
approaches, depending on whether a ﬁrst principles modeling approach is
fully, partially, or not admissible, systems identiﬁcation aims to devise math-
ematical models for predicting a future state of a system, given the evolution
of a set of previously observed or latent states. Speciﬁcally, in the con-
text of identifying nonlinear dynamics, there exist several deterministic and
probabilistic tools including radial basis functions [6], neural networks [7],
Gaussian processes [8, 9, 10, 11, 12], and nonlinear auto-regressive models
such as NARMAX [13] and recurrent neural networks [14]. A common theme
among all such methods is the pursuit of learning a nonlinear and potentially
multi-variate mapping f that predicts the future system states given a set
of data describing the present and past states. More recently, approaches
based on symbolic regression [15], sparse regression, and compressive sensing
[16, 17] were able to go beyond estimating a black-box approximation of the
dynamics given by f , and return more interpretable models that can uncover
the full parametric form of an underlying governing equation. However, in

2

order to obtain sparse representation of the dynamics, the aforementioned
approaches have to rely upon the nontrivial task of choosing “appropriate”
sets of basis functions. Consequently, investigating ways of incorporating
broader function search spaces is an important area of current and future
research.

In this work, we introduce a novel approach to nonlinear systems identiﬁ-
cation that combines the classical multistep family of time-stepping schemes
from numerical analysis [18] with deep neural networks.
Inspired by re-
cent developments in physics-informed deep learning [19, 20], we construct
structured nonlinear regression models that can discover the dynamic depen-
dencies in a given set of temporal data-snapshots, and return a closed form
model that can be subsequently used to forecast future states or identify
basins of attraction. In contrast to recent approaches to systems identiﬁca-
tion [16, 17], here we do not have to have direct access or approximations to
temporal gradients because the time derivatives are discretized using classi-
cal time-stepping rules. Moreover, we are using a richer family of function
approximators and consequently we do not have to commit to a particular
class of basis functions such as polynomials or sines and cosines. This comes
at the cost of losing interpretability of the learned dynamics. However, there
is nothing hindering the use of a particular class of basis functions and obtain
more interpretable equations.

This paper is structured as follows. In section 2 we provide a detailed
In section 3.1, we investigate the
overview of the proposed methodology.
performance of the proposed framework by applying our algorithm to the
two-dimensional damped harmonic oscillator. We then explore the identiﬁ-
cation of chaotic dynamics of the Lorenz system in section 3.2. As an example
of a high dimensional dynamical systems, in section 3.3, we study the Navier-
Stokes equations describing the ﬂuid ﬂow behind a cylinder. To illustrate the
ability of our method to identify parameterized dynamics, we consider the
Hopf normal form in section 3.4. As an example of complicated nonlinear
dynamics typical of biological systems, we explore the glycolytic oscillator
model in section 3.5. It should be highlighted that all of the examples con-
sidered in this work are inspired by the pioneering work of Brunton et. al.
[16]. Moreover, all data and codes used in this manuscript are publicly avail-
able on GitHub at https://github.com/maziarraissi/MultistepNNs.

3

2. Problem setup and solution methodology

Let us consider nonlinear dynamical systems of the form1

d
dt

x(t) = f (x(t)) ,

(1)

where the vector x(t) ∈ RD denotes the state of the system at time t and the
function f describes the evolution of the system. Given noisy measurements
of the state x(t) of the system at several time instances t1, t2, . . . , tN , our
goal is to determine the function f and consequently discover the underlying
dynamical system (1) from data. We proceed by applying the general form
of a linear multistep method with M steps to equation (1) and obtain

M
(cid:88)

m=0

[αmxn

m + ∆tβmf (xn
−

m)] = 0, n = M, . . . , N.
−

(2)

m) at time tn
m denotes the state of the system x(tn
−
−

Here, xn
m. Diﬀerent
−
choices for the parameters αm and βm result in speciﬁc schemes. For instance,
the trapezoidal rule

xn = xn
−

1 +

1)) , n = 1, . . . , N,
∆t (f (xn) + f (xn
−

(3)

1
2

corresponds to the case where M = 1, α0 = −1, α1 = 1, and β0 = β1 =
0.5. We proceed by placing a neural network prior on the function f . The
parameters of this neural network can be learned by minimizing the mean
squared error loss function

M SE :=

1
N − M + 1

N
(cid:88)

n=M

|yn|2,

(4)

1It is straightforward to generalize the dynamics to include parameterization, time
dependence, and forcing. In particular, parameterization, time dependence, and external
forcing or feedback control u(t) may be added to the vector ﬁeld according to

˙x = f (x, u, t; λ) ,

˙t = 1,

˙λ = 0.

4

yn :=

[αmxn
−

m + ∆tβmf (xn
−

m)] , n = M, . . . , N,

(5)

where

M
(cid:88)

m=0

is obtained from the multistep scheme (2).

3. Results

3.1. Two-dimensional damped oscillator

As a ﬁrst illustrative example, let us consider the two-dimensional damped

harmonic oscillator with cubic dynamics; i.e.,

˙x = −0.1 x3 + 2.0 y3,
˙y = −2.0 x3 − 0.1 y3.

(6)

We use [x0 y0]T = [2 0]T as initial condition and collect data from t = 0 to
t = 25 with a time-step size of ∆t = 0.01. The data are plotted in ﬁgure
1. We employ a neural network with one hidden layer and 256 neurons to
represent the nonlinear dynamics. As for the multistep scheme (2) we use
Adams-Moulton with M = 1 steps (i.e., the trapezoidal rule). Upon train-
ing the neural network, we solve the identiﬁed system using the same initial
condition as the one above. Figure 1 provides a qualitative assessment of
the accuracy in identifying the correct nonlinear dynamics. Speciﬁcally, by
comparing the exact and predicted trajectories of the system, as well as the
resulting phase portraits, we observe that the algorithm can correctly cap-
ture the dynamic evolution of the system.

To investigate the performance of the proposed work-ﬂow with respect
to diﬀerent linear multi-step methods, we have considered the three families
that are most commonly used in practice: Adams-Bashforth (AB) methods,
Adams-Moulton (AM) methods, and the backward diﬀerentiation formulas
(BDFs).
In tables 1 and 2, we report the relative L2 error between tra-
jectories of the exact and the identiﬁed systems for diﬀerent members of
the class of linear multi-step methods.
Interestingly, the Adams-Moulton
scheme seems to consistently return more accurate results compared to the
Adams-Bashforth and BDF approaches. One intuitive explanation for this

5

Figure 1: Harmonic Oscillator: Trajectories of the two-dimensional damped harmonic
oscillator with cubic dynamics are depicted in the left panel while the corresponding phase
portrait is plotted in the right panel. Solid colored lines represent the exact dynamics while
the dashed black lines demonstrate the learned dynamics. The identiﬁed system correctly
captures the form of the dynamics and accurately reproduces the phase portrait.

behavior stems from a closer inspection of equation 2. Speciﬁcally, the ar-
rangement of the resulting terms for the Adams-Moulton schemes leads to a
higher throughput of training data ﬂowing through the neural network during
model training as compared to the Adams-Bashforth and BDF cases. This
helps regularize the neural network and eventually achieve a better calibra-
tion during training. Also, out of the Adams-Moulton family, the trapezoidal
rule seems to work the best in practice perhaps due to its superior stability
properties [18]. These performance characteristics should be interpreted as
product of empirical evidence, and not as concrete theoretical properties of
the method. Identiﬁcation of the latter requires more extensive systematic
studies that go beyond the scope of this paper.

In tables 3 and 4, we study the robustness of our results with respect
to the gap ∆t between pairs of data and with respect to noise in the ob-
servations of the system. These results fail to reveal a consistent pattern as
larger time-step sizes ∆t and larger noise corruption levels sometimes lead to
superior accuracy and other times to inferior. In the latter cases, the reasons

6

M

Scheme
Adams-Bashforth
Adams-Moulton
BDF

1

2

3

4

5

1.5e+00
8.8e-03
1.3e+00

3.1e-02
1.2e-02
8.8e-03

1.2e-01
1.6e-02
1.3e-02

4.3e-02
6.3e-03
1.4e-02

1.2e-02
1.1e-02
1.7e-02

Table 1: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the ﬁrst dynamic component x(t) integrated up to time t = 25, for diﬀerent
member families of the class of multistep methods, and diﬀerent number of steps M . Here,
the training data is assumed to be noise free, and the neural network architecture is kept
ﬁxed to have one hidden layer and 256 neurons.

M

Scheme
Adams-Bashforth
Adams-Moulton
BDF

1

2

3

4

5

1.5e+00
8.8e-03
1.3e+00

3.0e-02
1.0e-02
8.6e-03

9.7e-02
1.6e-02
9.9e-03

3.5e-02
5.8e-03
1.4e-02

1.2e-02
1.1e-02
1.5e-02

Table 2: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the second dynamic component y(t) integrated up to time t = 25, for
diﬀerent member families of the class of multistep methods, and diﬀerent number of steps
M . Here, the training data is assumed to be noise free, and the neural network architecture
is kept ﬁxed to have one hidden layer and 256 neurons.

are obvious, namely that a larger gap ∆t makes the approximation in time
less accurate, while too much noise is devastating because it would be harder
to distinguish between noise and the true dynamics. On the other hand, we
do observe some cases in which larger ∆t and noise levels may actually help.
In these cases, we believe that input noise can act as a regularization mech-
anism that increases the robustness of the model training procedure, similar
to how it has been previously proposed in the neural network literature (see
for e.g., denoising autoencoders [21]). Along the same lines, a bigger tem-
poral gap ∆t helps because it makes two consecutive time snapshots carry
more information simply because they are more dissimilar to one another.
On the contrary, if ∆t is too small, the importance of the neural network
becomes less and less pronounced as seen in equation (2), hence model train-
ing becomes infeasible. These empirical results indicate that there exists a
problem-dependent sweet spot for the admissible values of the time-step and
noise levels that can lead to the best predictive accuracy. Although one may

7

Table 3: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the ﬁrst dynamic component x(t) integrated up to time t = 25, for diﬀerent
noise magnitudes, and diﬀerent gap ∆t between pairs of snapshots. Here, we are employing
the trapezoidal time-stepping scheme and the neural network architecture is kept ﬁx to
have one hidden layer and 256 neurons.

noise

0.00% 0.01% 0.02%

5.7e-03
1.8e-02
3.8e-02
5.4e-02
8.3e-02

2.4e-02
1.1e-01
9.2e-02
4.0e-02
2.9e-01

2.2e-01
1.3e-01
7.8e-01
8.7e-01
9.2e-02

noise

0.00% 0.01% 0.02%

5.9e-03
1.7e-02
3.3e-02
5.2e-02
7.2e-02

2.1e-02
1.0e-01
9.0e-02
3.8e-02
2.6e-01

2.2e-01
1.1e-01
7.9e-01
8.7e-01
8.2e-02

∆t
0.01
0.02
0.03
0.04
0.05

∆t
0.01
0.02
0.03
0.04
0.05

Table 4: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the ﬁrst dynamic component y(t) integrated up to time t = 25, for diﬀerent
noise magnitudes, and diﬀerent gap ∆t between pairs of snapshots. Here, we are employing
the trapezoidal time-stepping scheme and the neural network architecture is kept ﬁx to
have one hidden layer and 256 neurons.

have no control over the noise corrupting the data, the temporal gap ∆t
could be treated as another hyper-parameter like the number of neurons and
hidden layers when setting up the neural network.

Finally, tables 5 and 6 study the robustness of our results with respect to
the neural network structure. For this case, more accurate results seem to
be obtained with increasing network depth, although increasing the network
width seems to have a negative aﬀect for more than 128 neurons per layer.
To fully quantify sensitivity with respect to network architecture a more
systematic study involving multiple data-sets is needed.

8

Table 5: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the ﬁrst dynamic component x(t) integrated up to time t = 25, for diﬀerent
neural network architectures. Here, the training data is assumed to be noise free, the time
step size is kept ﬁxed at ∆t = 0.01, and the number of Adams-Moulton steps is ﬁxed at
M = 1.

Layers
1
2
3

Layers
1
2
3

Neurons

64

128

256

9.8e-03
3.6e-03
3.4e-03

6.1e-03
1.2e-02
1.6e-02

3.7e-02
2.4e-02
4.2e-02

Neurons

64

128

256

7.2e-03
3.3e-03
3.0e-03

5.4e-03
9.1e-03
1.4e-02

3.5e-02
2.0e-02
3.7e-02

Table 6: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the second dynamic component y(t) integrated up to time t = 25, for
diﬀerent neural network architectures. Here, the training data is assumed to be noise free,
the time step size is kept ﬁxed at ∆t = 0.01, and the number of Adams-Moulton steps is
ﬁxed at M = 1.

3.2. Lorenz system

To explore the identiﬁcation of chaotic dynamics evolving on a ﬁnite

dimensional attractor, we consider the nonlinear Lorenz system [22]

˙x = 10(y − x),
˙y = x(28 − z) − y,
˙z = xy − (8/3)z.

(7)

We use [x0 y0 z0]T = [−8 7 27]T as initial condition and collect data from
t = 0 to t = 25 with a time-step size of ∆t = 0.01. The data are plotted in
ﬁgures 2 and 3. We employ a neural network with one hidden layer and 256
neurons to represent the nonlinear dynamics. As for the multistep scheme
(2) we use Adams-Moulton with M = 1 steps (i.e., the trapezoidal rule).
Upon training the neural network, we solve the identiﬁed system using the
same initial condition as the one above. As depicted in ﬁgure 2, the learned
system correctly captures the form of the attractor.

9

Figure 2: Lorenz System: The exact phase portrait of the Lorenz system (left panel) is
compared to the corresponding phase portrait of the learned dynamics (right panel).

The Lorenz system has a positive Lyapunov exponent, and small diﬀer-
ences between the exact and learned models grow exponentially, even though
the attractor remains intact. This behavior is evident in ﬁgure 3, as we com-
pare the exact versus the predicted trajectories. Small discrepancies due to
ﬁnite accuracy in the predicted dynamics lead to large errors in the fore-
casted time-series after t > 4, despite the fact that the bi-stable structure of
the attractor is well captured (see ﬁgure 2).

3.3. Fluid ﬂow behind a cylinder

In this example we collect data for the ﬂuid ﬂow past a cylinder (see ﬁg-
ure 4) at Reynolds number 100 using direct numerical simulations of the two
dimensional Navier-Stokes equations.
In particular, following the problem
setup presented in [23] and [24], we simulate the Navier-Stokes equations de-
scribing the two-dimensional ﬂuid ﬂow past a circular cylinder at Reynolds
number 100 using the Immersed Boundary Projection Method [25, 26]. This
approach utilizes a multi-domain scheme with four nested domains, each suc-
cessive grid being twice as large as the previous one. Length and time are
non-dimensionalized so that the cylinder has unit diameter and the ﬂow has
unit velocity. Data is collected on the ﬁnest domain with dimensions 9 × 4 at
a grid resolution of 449 × 199. The ﬂow solver uses a 3rd-order Runge Kutta

10

Figure 3: Lorenz System: The exact trajectories of the Lorenz systems is compared to the
corresponding trajectories of the learned dynamics. Solid blue lines represent the exact
dynamics while the dashed black lines demonstrate the learned dynamics.

integration scheme with a time step of t = 0.02, which has been veriﬁed to
yield well-resolved and converged ﬂow ﬁelds. After simulations converge to
steady periodic vortex shedding, ﬂow snapshots are saved every ∆t = 0.02.
We then reduce the dimension of the system by proper orthogonal decompo-
sition (POD) [27, 16]. The POD results in a hierarchy of orthonormal modes
that, when truncated, capture most of the energy of the original system for
the given rank truncation. The ﬁrst two most energetic POD modes capture
a signiﬁcant portion of the energy; the steady-state vortex shedding is a limit

11

Figure 4: Flow past a cylinder: A snapshot of the vorticity ﬁeld of a solution to the
Navier-Stokes equations for the ﬂuid ﬂow past a cylinder.

cycle in these coordinates [16]. An additional mode, called the shift mode, is
included to capture the transient dynamics connecting the unstable steady
state with the mean of the limit cycle [28]. The resulting POD coeﬃcients
are depicted in ﬁgure 5.

We employ a neural network with one hidden layer and 256 neurons to rep-
resent the nonlinear dynamics shown in ﬁgure 5. As for the linear multistep
scheme (2) we use Adams-Moulton with M = 1 steps (i.e., the trapezoidal
rule). Upon training the neural network, we solve the identiﬁed system. As
depicted in ﬁgure 5, the learned system correctly captures the form of the
dynamics and accurately reproduces the phase portrait, including both the
transient regime as well as the limit cycle attained once the ﬂow dynamics
converge to the well known K´arman vortex street.

3.4. Hopf bifurcation

Many real-world systems depend on parameters and, when the parameters
are varied, they may go through bifurcations. To illustrate the ability of our
method to identify parameterized dynamics, let us consider the Hopf normal
form

˙x = µx + y − x(x2 + y2),
˙y = −x + µy − y(x2 + y2).

(8)

12

Figure 5: Flow past a cylinder: The exact phase portrait of the cylinder wake trajectory
in reduced coordinates (left panel) is compared to the corresponding phase portrait of the
learned dynamics (right panel).

Our algorithm can be readily extended to encompass parameterized systems.
In particular, the system (8) can be equivalently written as

˙µ = 0,
˙x = µx + y − x(x2 + y2),
˙y = −x + µy − y(x2 + y2).

(9)

We collect data from the Hopf system (9) for various initial conditions corre-
sponding to diﬀerent parameter values for µ. The data is depicted in ﬁgure 6.
The identiﬁed parameterized dynamics is shown in ﬁgure 6 for a set of param-
eter values diﬀerent from the ones used during model training. The learned
system correctly captures the transition from the ﬁxed point for µ < 0 to the
limit cycle for µ > 0.

3.5. Glycolytic oscillator

As an example of complicated nonlinear dynamics typical of biological
systems, we simulate the glycolytic oscillator model presented in [29] and [16].
The model consists of ordinary diﬀerential equations for the concentrations

13

Figure 6: Hopf bifurcation: Training data from the Hopf system for various initial con-
ditions corresponding to diﬀerent parameter values for µ (left panel) is compared to the
corresponding phase portrait of the learned dynamics (right panel). It is worth highlight-
ing that the algorithm is tested on initial conditions diﬀerent from the ones used during
training.

of 7 biochemical species; i.e.,

dS1
dt
dS2
dt
dS3
dt
dS4
dt
dS5
dt
dS6
dt
dS7
dt

= J0 −

,

k1S1S6
1 + (S6/K1)q
k1S1S6
1 + (S6/K1)q

= 2

− k2S2(N − S5) − k6S2S5,

= k2S2(N − S5) − k3S3(N − S6),

= k3S3(A − S6) − k4S4S5 − κ(S4 − S7),

(10)

= k2S2(N − S5) − k4S4S5 − k6S2S5,

= −2

k1S1S6
1 + (S6/K1)q

= ψκ(S4 − S7) − kS7.

+ 2k3S3(A − S6) − k5S6,

14

The parameters of the model are chosen according to table 1 of [29]. As shown
in ﬁgure 7, data from a simulation of equation (10) are collected from t = 0
to t = 10 with a time-step size of ∆t = 0.01. We employ a neural network
with one hidden layer and 256 neurons to represent the nonlinear dynamics.
As for the multi-step scheme (2) we use Adams-Moulton with M = 1 steps
(i.e., the trapezoidal rule). Upon training the neural network, we solve the
identiﬁed system using the same initial condition as the ones used for the
exact system. As depicted in ﬁgure 7, the learned system correctly captures
the form of the dynamics.

4. Summary and Discussion

We have presented a machine learning approach for extracting nonlinear
dynamical systems from time-series data. The proposed algorithm lever-
ages the structure of well studied multi-step time-stepping schemes such as
Adams-Bashforth, Adams Moulton, and BDF families, to construct eﬃcient
algorithms for learning dynamical systems using deep neural networks. A
key property of the proposed approach is the use of multiple steps which
enables us to incorporate memory eﬀects in learning the temporal dynamics
and tackle problems with a nonlinear and non-Markovian dynamical struc-
ture. Speciﬁcally, the use of M steps allows us to decouple the regression
complexity due to several temporal lags, ultimately leading to a simpler
D-dimensional regression problem, as opposed to an (M × D)-dimensional
problem in the case of a brute force NARMAX or recurrent neural network
approaches. Although state-of-the-art results are presented for a diverse col-
lection of benchmark problems, there exist a series of open questions mandat-
ing further investigation. How could one handle a variable temporal gap ∆t,
i.e., irregularly sampled data in time? How would common techniques such as
batch normalization, drop out, and L1/L2 regularization enhance the robust-
ness of the proposed algorithm and mitigate the eﬀects of over-ﬁtting? How
could one incorporate partial knowledge of the dynamical system in cases
where certain interaction terms are already known? In terms of future work,
interesting directions include the application of convolutional architectures
[14] for mitigating the complexity associated with very high-dimensional in-
puts, as well as studying possible connections with recent studies linking deep
neural networks with numerical methods and dynamical systems [30, 31].

15

Figure 7: Glycolytic oscillator: Exact versus learned dynamics for random initial condi-
tions chosen from the ranges provided in table 2 of [29].

Acknowledgements

This work received support by the DARPA EQUiPS grant N66001-15-2-
4055 and the AFOSR grant FA9550-17-1-0013. All data and codes used in
this manuscript are publicly available on GitHub at https://github.com/
maziarraissi/MultistepNNs.

16

References

[1] R. Courant, D. Hilbert, Methods of Mathematical Physics, Volume 2:

Diﬀerential Equations, John Wiley & Sons, 2008.

[2] F. Michor, Y. Iwasa, M. A. Nowak, Dynamics of cancer progression,

Nature reviews cancer 4 (2004) 197–205.

[3] C. Castellano, S. Fortunato, V. Loreto, Statistical physics of social

dynamics, Reviews of modern physics 81 (2009) 591.

[4] M. Gavin, The stock market and exchange rate dynamics, Journal of

international money and ﬁnance 8 (1989) 181–200.

[5] L. Ljung, System identiﬁcation,
Springer, 1998, pp. 163–173.

in: Signal analysis and prediction,

[6] S. Chen, S. Billings, C. Cowan, P. Grant, Non-linear systems identi-
ﬁcation using radial basis functions, International Journal of Systems
Science 21 (1990) 2513–2539.

[7] A. Cochocki, R. Unbehauen, Neural networks for optimization and signal

processing, John Wiley & Sons, Inc., 1993.

[8] J. Kocijan, A. Girard, B. Banko, R. Murray-Smith, Dynamic systems
identiﬁcation with gaussian processes, Mathematical and Computer
Modelling of Dynamical Systems 11 (2005) 411–424.

[9] M. Raissi, P. Perdikaris, G. E. Karniadakis, Machine learning of linear
diﬀerential equations using Gaussian processes, Journal of Computa-
tional Physics 348 (2017) 683 – 693.

[10] M. Raissi, G. E. Karniadakis, Hidden physics models: Machine
arXiv preprint

learning of nonlinear partial diﬀerential equations,
arXiv:1708.00588 (2017).

[11] M. Raissi, P. Perdikaris, G. E. Karniadakis, Inferring solutions of dif-
ferential equations using noisy multi-ﬁdelity data, Journal of Computa-
tional Physics 335 (2017) 736–746.

17

[12] M. Raissi, P. Perdikaris, G. E. Karniadakis, Numerical Gaussian pro-
cesses for time-dependent and non-linear partial diﬀerential equations,
arXiv preprint arXiv:1703.10230 (2017).

[13] S. A. Billings, Nonlinear system identiﬁcation: NARMAX methods in
the time, frequency, and spatio-temporal domains, John Wiley & Sons,
2013.

[14] I. Goodfellow, Y. Bengio, A. Courville, Deep learning, MIT press, 2016.

[15] M. Schmidt, H. Lipson, Distilling free-form natural laws from experi-

mental data, science 324 (2009) 81–85.

[16] S. L. Brunton, J. L. Proctor, J. N. Kutz, Discovering governing equa-
tions from data by sparse identiﬁcation of nonlinear dynamical systems,
Proceedings of the National Academy of Sciences 113 (2016) 3932–3937.

[17] S. H. Rudy, S. L. Brunton, J. L. Proctor, J. N. Kutz, Data-driven
discovery of partial diﬀerential equations, Science Advances 3 (2017)
e1602614.

[18] A. Iserles, A ﬁrst course in the numerical analysis of diﬀerential equa-

tions, 44, Cambridge University Press, 2009.

[19] M. Raissi, P. Perdikaris, G. E. Karniadakis, Physics informed deep
learning (part ii): Data-driven discovery of nonlinear partial diﬀerential
equations, arXiv preprint arXiv:1711.10566 (2017).

[20] M. Raissi, P. Perdikaris, G. E. Karniadakis, Physics informed deep
learning (part i): Data-driven solutions of nonlinear partial diﬀerential
equations, arXiv preprint arXiv:1711.10561 (2017).

[21] P. Vincent, H. Larochelle, Y. Bengio, P.-A. Manzagol, Extracting and
composing robust features with denoising autoencoders, in: Proceedings
of the 25th international conference on Machine learning, ACM, pp.
1096–1103.

[22] E. N. Lorenz, Deterministic nonperiodic ﬂow, Journal of the atmospheric

sciences 20 (1963) 130–141.

18

[23] J. N. Kutz, S. L. Brunton, B. W. Brunton, J. L. Proctor, Dynamic
Mode Decomposition: Data-Driven Modeling of Complex Systems, vol-
ume 149, SIAM, 2016.

[24] S. H. Rudy, S. L. Brunton, J. L. Proctor, J. N. Kutz, Data-driven

discovery of partial diﬀerential equations, Science Advances 3 (2017).

[25] K. Taira, T. Colonius, The immersed boundary method: a projection
approach, Journal of Computational Physics 225 (2007) 2118–2137.

[26] T. Colonius, K. Taira, A fast immersed boundary method using a
nullspace approach and multi-domain far-ﬁeld boundary conditions,
Computer Methods in Applied Mechanics and Engineering 197 (2008)
2131–2146.

[27] G. Berkooz, P. Holmes, J. L. Lumley, The proper orthogonal decomposi-
tion in the analysis of turbulent ﬂows, Annual review of ﬂuid mechanics
25 (1993) 539–575.

[28] B. R. Noack, K. Afanasiev, M. MORZY ´NSKI, G. Tadmor, F. Thiele, A
hierarchy of low-dimensional models for the transient and post-transient
cylinder wake, Journal of Fluid Mechanics 497 (2003) 335–363.

[29] B. C. Daniels, I. Nemenman, Eﬃcient inference of parsimonious phe-
nomenological models of cellular dynamics using s-systems and alternat-
ing regression, PloS one 10 (2015) e0119821.

[30] B. Chang, L. Meng, E. Haber, F. Tung, D. Begert, Multi-level residual
networks from dynamical systems view, arXiv preprint arXiv:1710.10348
(2017).

[31] Y. Lu, A. Zhong, Q. Li, B. Dong, Beyond ﬁnite layer neural networks:
Bridging deep architectures and numerical diﬀerential equations, arXiv
preprint arXiv:1710.10121 (2017).

19

8
1
0
2
 
n
a
J
 
4
 
 
]
S
D
.
h
t
a
m

[
 
 
1
v
6
3
2
1
0
.
1
0
8
1
:
v
i
X
r
a

Multistep Neural Networks for Data-driven Discovery of
Nonlinear Dynamical Systems

Maziar Raissi1, Paris Perdikaris2, and George Em Karniadakis1

1Division of Applied Mathematics, Brown University,
Providence, RI, 02912, USA
2Department of Mechanical Engineering and Applied Mechanics,
University of Pennsylvania,
Philadelphia, PA, 19104, USA

Abstract

The process of transforming observed data into predictive mathematical
models of the physical world has always been paramount in science and en-
gineering. Although data is currently being collected at an ever-increasing
pace, devising meaningful models out of such observations in an automated
fashion still remains an open problem.
In this work, we put forth a ma-
chine learning approach for identifying nonlinear dynamical systems from
data. Speciﬁcally, we blend classical tools from numerical analysis, namely
the multi-step time-stepping schemes, with powerful nonlinear function ap-
proximators, namely deep neural networks, to distill the mechanisms that
govern the evolution of a given data-set. We test the eﬀectiveness of our
approach for several benchmark problems involving the identiﬁcation of com-
plex, nonlinear and chaotic dynamics, and we demonstrate how this allows us
to accurately learn the dynamics, forecast future states, and identify basins
of attraction. In particular, we study the Lorenz system, the ﬂuid ﬂow be-
hind a cylinder, the Hopf bifurcation, and the Glycoltic oscillator model as
an example of complicated nonlinear dynamics typical of biological systems.

Keywords:
Machine learning, Systems identiﬁcation, Reduced order modeling,
Data-driven forecasting, Lorenz system, Navier-Stokes

Preprint submitted to Journal Name

January 8, 2018

1. Introduction

Dynamical systems play a key role in shaping our understanding of the
physical world and in deﬁning our ability to predict the evolution of a given
process. From the simple swinging motion of a clock pendulum to the com-
plex ﬂow around an airplane wing, the mathematical modeling of dynamical
systems can yield a set of tools with which we can analyze the way the current
state of the system depends on the past, and predict the possible states we
may encounter in the future. Often such tools are precisely known, usually
coming in the form of diﬀerential equations that are derived from ﬁrst phys-
ical principles, such as the conservation of energy, mass, and momentum [1].
However, in many cases, the sheer complexity of a system can prohibit our
complete understanding and render a ﬁrst principles approach infeasible. In
this setting, one may be only able to postulate crude and potentially overly
simpliﬁed models based on a given a set of empirical observations (see e.g.,
models for tumor growth [2], social dynamics [3], and the stock market [4]).
In the present era of abundant data and advanced machine learning capa-
bilities, a natural question arises: can we automatically discover suﬃciently
sophisticated and accurate mathematical models of complex dynamical sys-
tems directly from data?

The answer to this question pertains to the well-established ﬁeld of sys-
tems identiﬁcation [5]. Discriminating between white-, gray-, and black-box
approaches, depending on whether a ﬁrst principles modeling approach is
fully, partially, or not admissible, systems identiﬁcation aims to devise math-
ematical models for predicting a future state of a system, given the evolution
of a set of previously observed or latent states. Speciﬁcally, in the con-
text of identifying nonlinear dynamics, there exist several deterministic and
probabilistic tools including radial basis functions [6], neural networks [7],
Gaussian processes [8, 9, 10, 11, 12], and nonlinear auto-regressive models
such as NARMAX [13] and recurrent neural networks [14]. A common theme
among all such methods is the pursuit of learning a nonlinear and potentially
multi-variate mapping f that predicts the future system states given a set
of data describing the present and past states. More recently, approaches
based on symbolic regression [15], sparse regression, and compressive sensing
[16, 17] were able to go beyond estimating a black-box approximation of the
dynamics given by f , and return more interpretable models that can uncover
the full parametric form of an underlying governing equation. However, in

2

order to obtain sparse representation of the dynamics, the aforementioned
approaches have to rely upon the nontrivial task of choosing “appropriate”
sets of basis functions. Consequently, investigating ways of incorporating
broader function search spaces is an important area of current and future
research.

In this work, we introduce a novel approach to nonlinear systems identiﬁ-
cation that combines the classical multistep family of time-stepping schemes
from numerical analysis [18] with deep neural networks.
Inspired by re-
cent developments in physics-informed deep learning [19, 20], we construct
structured nonlinear regression models that can discover the dynamic depen-
dencies in a given set of temporal data-snapshots, and return a closed form
model that can be subsequently used to forecast future states or identify
basins of attraction. In contrast to recent approaches to systems identiﬁca-
tion [16, 17], here we do not have to have direct access or approximations to
temporal gradients because the time derivatives are discretized using classi-
cal time-stepping rules. Moreover, we are using a richer family of function
approximators and consequently we do not have to commit to a particular
class of basis functions such as polynomials or sines and cosines. This comes
at the cost of losing interpretability of the learned dynamics. However, there
is nothing hindering the use of a particular class of basis functions and obtain
more interpretable equations.

This paper is structured as follows. In section 2 we provide a detailed
In section 3.1, we investigate the
overview of the proposed methodology.
performance of the proposed framework by applying our algorithm to the
two-dimensional damped harmonic oscillator. We then explore the identiﬁ-
cation of chaotic dynamics of the Lorenz system in section 3.2. As an example
of a high dimensional dynamical systems, in section 3.3, we study the Navier-
Stokes equations describing the ﬂuid ﬂow behind a cylinder. To illustrate the
ability of our method to identify parameterized dynamics, we consider the
Hopf normal form in section 3.4. As an example of complicated nonlinear
dynamics typical of biological systems, we explore the glycolytic oscillator
model in section 3.5. It should be highlighted that all of the examples con-
sidered in this work are inspired by the pioneering work of Brunton et. al.
[16]. Moreover, all data and codes used in this manuscript are publicly avail-
able on GitHub at https://github.com/maziarraissi/MultistepNNs.

3

2. Problem setup and solution methodology

Let us consider nonlinear dynamical systems of the form1

d
dt

x(t) = f (x(t)) ,

(1)

where the vector x(t) ∈ RD denotes the state of the system at time t and the
function f describes the evolution of the system. Given noisy measurements
of the state x(t) of the system at several time instances t1, t2, . . . , tN , our
goal is to determine the function f and consequently discover the underlying
dynamical system (1) from data. We proceed by applying the general form
of a linear multistep method with M steps to equation (1) and obtain

M
(cid:88)

m=0

[αmxn

m + ∆tβmf (xn
−

m)] = 0, n = M, . . . , N.
−

(2)

m) at time tn
m denotes the state of the system x(tn
−
−

Here, xn
m. Diﬀerent
−
choices for the parameters αm and βm result in speciﬁc schemes. For instance,
the trapezoidal rule

xn = xn
−

1 +

1)) , n = 1, . . . , N,
∆t (f (xn) + f (xn
−

(3)

1
2

corresponds to the case where M = 1, α0 = −1, α1 = 1, and β0 = β1 =
0.5. We proceed by placing a neural network prior on the function f . The
parameters of this neural network can be learned by minimizing the mean
squared error loss function

M SE :=

1
N − M + 1

N
(cid:88)

n=M

|yn|2,

(4)

1It is straightforward to generalize the dynamics to include parameterization, time
dependence, and forcing. In particular, parameterization, time dependence, and external
forcing or feedback control u(t) may be added to the vector ﬁeld according to

˙x = f (x, u, t; λ) ,

˙t = 1,

˙λ = 0.

4

yn :=

[αmxn
−

m + ∆tβmf (xn
−

m)] , n = M, . . . , N,

(5)

where

M
(cid:88)

m=0

is obtained from the multistep scheme (2).

3. Results

3.1. Two-dimensional damped oscillator

As a ﬁrst illustrative example, let us consider the two-dimensional damped

harmonic oscillator with cubic dynamics; i.e.,

˙x = −0.1 x3 + 2.0 y3,
˙y = −2.0 x3 − 0.1 y3.

(6)

We use [x0 y0]T = [2 0]T as initial condition and collect data from t = 0 to
t = 25 with a time-step size of ∆t = 0.01. The data are plotted in ﬁgure
1. We employ a neural network with one hidden layer and 256 neurons to
represent the nonlinear dynamics. As for the multistep scheme (2) we use
Adams-Moulton with M = 1 steps (i.e., the trapezoidal rule). Upon train-
ing the neural network, we solve the identiﬁed system using the same initial
condition as the one above. Figure 1 provides a qualitative assessment of
the accuracy in identifying the correct nonlinear dynamics. Speciﬁcally, by
comparing the exact and predicted trajectories of the system, as well as the
resulting phase portraits, we observe that the algorithm can correctly cap-
ture the dynamic evolution of the system.

To investigate the performance of the proposed work-ﬂow with respect
to diﬀerent linear multi-step methods, we have considered the three families
that are most commonly used in practice: Adams-Bashforth (AB) methods,
Adams-Moulton (AM) methods, and the backward diﬀerentiation formulas
(BDFs).
In tables 1 and 2, we report the relative L2 error between tra-
jectories of the exact and the identiﬁed systems for diﬀerent members of
the class of linear multi-step methods.
Interestingly, the Adams-Moulton
scheme seems to consistently return more accurate results compared to the
Adams-Bashforth and BDF approaches. One intuitive explanation for this

5

Figure 1: Harmonic Oscillator: Trajectories of the two-dimensional damped harmonic
oscillator with cubic dynamics are depicted in the left panel while the corresponding phase
portrait is plotted in the right panel. Solid colored lines represent the exact dynamics while
the dashed black lines demonstrate the learned dynamics. The identiﬁed system correctly
captures the form of the dynamics and accurately reproduces the phase portrait.

behavior stems from a closer inspection of equation 2. Speciﬁcally, the ar-
rangement of the resulting terms for the Adams-Moulton schemes leads to a
higher throughput of training data ﬂowing through the neural network during
model training as compared to the Adams-Bashforth and BDF cases. This
helps regularize the neural network and eventually achieve a better calibra-
tion during training. Also, out of the Adams-Moulton family, the trapezoidal
rule seems to work the best in practice perhaps due to its superior stability
properties [18]. These performance characteristics should be interpreted as
product of empirical evidence, and not as concrete theoretical properties of
the method. Identiﬁcation of the latter requires more extensive systematic
studies that go beyond the scope of this paper.

In tables 3 and 4, we study the robustness of our results with respect
to the gap ∆t between pairs of data and with respect to noise in the ob-
servations of the system. These results fail to reveal a consistent pattern as
larger time-step sizes ∆t and larger noise corruption levels sometimes lead to
superior accuracy and other times to inferior. In the latter cases, the reasons

6

M

Scheme
Adams-Bashforth
Adams-Moulton
BDF

1

2

3

4

5

1.5e+00
8.8e-03
1.3e+00

3.1e-02
1.2e-02
8.8e-03

1.2e-01
1.6e-02
1.3e-02

4.3e-02
6.3e-03
1.4e-02

1.2e-02
1.1e-02
1.7e-02

Table 1: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the ﬁrst dynamic component x(t) integrated up to time t = 25, for diﬀerent
member families of the class of multistep methods, and diﬀerent number of steps M . Here,
the training data is assumed to be noise free, and the neural network architecture is kept
ﬁxed to have one hidden layer and 256 neurons.

M

Scheme
Adams-Bashforth
Adams-Moulton
BDF

1

2

3

4

5

1.5e+00
8.8e-03
1.3e+00

3.0e-02
1.0e-02
8.6e-03

9.7e-02
1.6e-02
9.9e-03

3.5e-02
5.8e-03
1.4e-02

1.2e-02
1.1e-02
1.5e-02

Table 2: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the second dynamic component y(t) integrated up to time t = 25, for
diﬀerent member families of the class of multistep methods, and diﬀerent number of steps
M . Here, the training data is assumed to be noise free, and the neural network architecture
is kept ﬁxed to have one hidden layer and 256 neurons.

are obvious, namely that a larger gap ∆t makes the approximation in time
less accurate, while too much noise is devastating because it would be harder
to distinguish between noise and the true dynamics. On the other hand, we
do observe some cases in which larger ∆t and noise levels may actually help.
In these cases, we believe that input noise can act as a regularization mech-
anism that increases the robustness of the model training procedure, similar
to how it has been previously proposed in the neural network literature (see
for e.g., denoising autoencoders [21]). Along the same lines, a bigger tem-
poral gap ∆t helps because it makes two consecutive time snapshots carry
more information simply because they are more dissimilar to one another.
On the contrary, if ∆t is too small, the importance of the neural network
becomes less and less pronounced as seen in equation (2), hence model train-
ing becomes infeasible. These empirical results indicate that there exists a
problem-dependent sweet spot for the admissible values of the time-step and
noise levels that can lead to the best predictive accuracy. Although one may

7

Table 3: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the ﬁrst dynamic component x(t) integrated up to time t = 25, for diﬀerent
noise magnitudes, and diﬀerent gap ∆t between pairs of snapshots. Here, we are employing
the trapezoidal time-stepping scheme and the neural network architecture is kept ﬁx to
have one hidden layer and 256 neurons.

noise

0.00% 0.01% 0.02%

5.7e-03
1.8e-02
3.8e-02
5.4e-02
8.3e-02

2.4e-02
1.1e-01
9.2e-02
4.0e-02
2.9e-01

2.2e-01
1.3e-01
7.8e-01
8.7e-01
9.2e-02

noise

0.00% 0.01% 0.02%

5.9e-03
1.7e-02
3.3e-02
5.2e-02
7.2e-02

2.1e-02
1.0e-01
9.0e-02
3.8e-02
2.6e-01

2.2e-01
1.1e-01
7.9e-01
8.7e-01
8.2e-02

∆t
0.01
0.02
0.03
0.04
0.05

∆t
0.01
0.02
0.03
0.04
0.05

Table 4: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the ﬁrst dynamic component y(t) integrated up to time t = 25, for diﬀerent
noise magnitudes, and diﬀerent gap ∆t between pairs of snapshots. Here, we are employing
the trapezoidal time-stepping scheme and the neural network architecture is kept ﬁx to
have one hidden layer and 256 neurons.

have no control over the noise corrupting the data, the temporal gap ∆t
could be treated as another hyper-parameter like the number of neurons and
hidden layers when setting up the neural network.

Finally, tables 5 and 6 study the robustness of our results with respect to
the neural network structure. For this case, more accurate results seem to
be obtained with increasing network depth, although increasing the network
width seems to have a negative aﬀect for more than 128 neurons per layer.
To fully quantify sensitivity with respect to network architecture a more
systematic study involving multiple data-sets is needed.

8

Table 5: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the ﬁrst dynamic component x(t) integrated up to time t = 25, for diﬀerent
neural network architectures. Here, the training data is assumed to be noise free, the time
step size is kept ﬁxed at ∆t = 0.01, and the number of Adams-Moulton steps is ﬁxed at
M = 1.

Layers
1
2
3

Layers
1
2
3

Neurons

64

128

256

9.8e-03
3.6e-03
3.4e-03

6.1e-03
1.2e-02
1.6e-02

3.7e-02
2.4e-02
4.2e-02

Neurons

64

128

256

7.2e-03
3.3e-03
3.0e-03

5.4e-03
9.1e-03
1.4e-02

3.5e-02
2.0e-02
3.7e-02

Table 6: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the second dynamic component y(t) integrated up to time t = 25, for
diﬀerent neural network architectures. Here, the training data is assumed to be noise free,
the time step size is kept ﬁxed at ∆t = 0.01, and the number of Adams-Moulton steps is
ﬁxed at M = 1.

3.2. Lorenz system

To explore the identiﬁcation of chaotic dynamics evolving on a ﬁnite

dimensional attractor, we consider the nonlinear Lorenz system [22]

˙x = 10(y − x),
˙y = x(28 − z) − y,
˙z = xy − (8/3)z.

(7)

We use [x0 y0 z0]T = [−8 7 27]T as initial condition and collect data from
t = 0 to t = 25 with a time-step size of ∆t = 0.01. The data are plotted in
ﬁgures 2 and 3. We employ a neural network with one hidden layer and 256
neurons to represent the nonlinear dynamics. As for the multistep scheme
(2) we use Adams-Moulton with M = 1 steps (i.e., the trapezoidal rule).
Upon training the neural network, we solve the identiﬁed system using the
same initial condition as the one above. As depicted in ﬁgure 2, the learned
system correctly captures the form of the attractor.

9

Figure 2: Lorenz System: The exact phase portrait of the Lorenz system (left panel) is
compared to the corresponding phase portrait of the learned dynamics (right panel).

The Lorenz system has a positive Lyapunov exponent, and small diﬀer-
ences between the exact and learned models grow exponentially, even though
the attractor remains intact. This behavior is evident in ﬁgure 3, as we com-
pare the exact versus the predicted trajectories. Small discrepancies due to
ﬁnite accuracy in the predicted dynamics lead to large errors in the fore-
casted time-series after t > 4, despite the fact that the bi-stable structure of
the attractor is well captured (see ﬁgure 2).

3.3. Fluid ﬂow behind a cylinder

In this example we collect data for the ﬂuid ﬂow past a cylinder (see ﬁg-
ure 4) at Reynolds number 100 using direct numerical simulations of the two
dimensional Navier-Stokes equations.
In particular, following the problem
setup presented in [23] and [24], we simulate the Navier-Stokes equations de-
scribing the two-dimensional ﬂuid ﬂow past a circular cylinder at Reynolds
number 100 using the Immersed Boundary Projection Method [25, 26]. This
approach utilizes a multi-domain scheme with four nested domains, each suc-
cessive grid being twice as large as the previous one. Length and time are
non-dimensionalized so that the cylinder has unit diameter and the ﬂow has
unit velocity. Data is collected on the ﬁnest domain with dimensions 9 × 4 at
a grid resolution of 449 × 199. The ﬂow solver uses a 3rd-order Runge Kutta

10

Figure 3: Lorenz System: The exact trajectories of the Lorenz systems is compared to the
corresponding trajectories of the learned dynamics. Solid blue lines represent the exact
dynamics while the dashed black lines demonstrate the learned dynamics.

integration scheme with a time step of t = 0.02, which has been veriﬁed to
yield well-resolved and converged ﬂow ﬁelds. After simulations converge to
steady periodic vortex shedding, ﬂow snapshots are saved every ∆t = 0.02.
We then reduce the dimension of the system by proper orthogonal decompo-
sition (POD) [27, 16]. The POD results in a hierarchy of orthonormal modes
that, when truncated, capture most of the energy of the original system for
the given rank truncation. The ﬁrst two most energetic POD modes capture
a signiﬁcant portion of the energy; the steady-state vortex shedding is a limit

11

Figure 4: Flow past a cylinder: A snapshot of the vorticity ﬁeld of a solution to the
Navier-Stokes equations for the ﬂuid ﬂow past a cylinder.

cycle in these coordinates [16]. An additional mode, called the shift mode, is
included to capture the transient dynamics connecting the unstable steady
state with the mean of the limit cycle [28]. The resulting POD coeﬃcients
are depicted in ﬁgure 5.

We employ a neural network with one hidden layer and 256 neurons to rep-
resent the nonlinear dynamics shown in ﬁgure 5. As for the linear multistep
scheme (2) we use Adams-Moulton with M = 1 steps (i.e., the trapezoidal
rule). Upon training the neural network, we solve the identiﬁed system. As
depicted in ﬁgure 5, the learned system correctly captures the form of the
dynamics and accurately reproduces the phase portrait, including both the
transient regime as well as the limit cycle attained once the ﬂow dynamics
converge to the well known K´arman vortex street.

3.4. Hopf bifurcation

Many real-world systems depend on parameters and, when the parameters
are varied, they may go through bifurcations. To illustrate the ability of our
method to identify parameterized dynamics, let us consider the Hopf normal
form

˙x = µx + y − x(x2 + y2),
˙y = −x + µy − y(x2 + y2).

(8)

12

Figure 5: Flow past a cylinder: The exact phase portrait of the cylinder wake trajectory
in reduced coordinates (left panel) is compared to the corresponding phase portrait of the
learned dynamics (right panel).

Our algorithm can be readily extended to encompass parameterized systems.
In particular, the system (8) can be equivalently written as

˙µ = 0,
˙x = µx + y − x(x2 + y2),
˙y = −x + µy − y(x2 + y2).

(9)

We collect data from the Hopf system (9) for various initial conditions corre-
sponding to diﬀerent parameter values for µ. The data is depicted in ﬁgure 6.
The identiﬁed parameterized dynamics is shown in ﬁgure 6 for a set of param-
eter values diﬀerent from the ones used during model training. The learned
system correctly captures the transition from the ﬁxed point for µ < 0 to the
limit cycle for µ > 0.

3.5. Glycolytic oscillator

As an example of complicated nonlinear dynamics typical of biological
systems, we simulate the glycolytic oscillator model presented in [29] and [16].
The model consists of ordinary diﬀerential equations for the concentrations

13

Figure 6: Hopf bifurcation: Training data from the Hopf system for various initial con-
ditions corresponding to diﬀerent parameter values for µ (left panel) is compared to the
corresponding phase portrait of the learned dynamics (right panel). It is worth highlight-
ing that the algorithm is tested on initial conditions diﬀerent from the ones used during
training.

of 7 biochemical species; i.e.,

dS1
dt
dS2
dt
dS3
dt
dS4
dt
dS5
dt
dS6
dt
dS7
dt

= J0 −

,

k1S1S6
1 + (S6/K1)q
k1S1S6
1 + (S6/K1)q

= 2

− k2S2(N − S5) − k6S2S5,

= k2S2(N − S5) − k3S3(N − S6),

= k3S3(A − S6) − k4S4S5 − κ(S4 − S7),

(10)

= k2S2(N − S5) − k4S4S5 − k6S2S5,

= −2

k1S1S6
1 + (S6/K1)q

= ψκ(S4 − S7) − kS7.

+ 2k3S3(A − S6) − k5S6,

14

The parameters of the model are chosen according to table 1 of [29]. As shown
in ﬁgure 7, data from a simulation of equation (10) are collected from t = 0
to t = 10 with a time-step size of ∆t = 0.01. We employ a neural network
with one hidden layer and 256 neurons to represent the nonlinear dynamics.
As for the multi-step scheme (2) we use Adams-Moulton with M = 1 steps
(i.e., the trapezoidal rule). Upon training the neural network, we solve the
identiﬁed system using the same initial condition as the ones used for the
exact system. As depicted in ﬁgure 7, the learned system correctly captures
the form of the dynamics.

4. Summary and Discussion

We have presented a machine learning approach for extracting nonlinear
dynamical systems from time-series data. The proposed algorithm lever-
ages the structure of well studied multi-step time-stepping schemes such as
Adams-Bashforth, Adams Moulton, and BDF families, to construct eﬃcient
algorithms for learning dynamical systems using deep neural networks. A
key property of the proposed approach is the use of multiple steps which
enables us to incorporate memory eﬀects in learning the temporal dynamics
and tackle problems with a nonlinear and non-Markovian dynamical struc-
ture. Speciﬁcally, the use of M steps allows us to decouple the regression
complexity due to several temporal lags, ultimately leading to a simpler
D-dimensional regression problem, as opposed to an (M × D)-dimensional
problem in the case of a brute force NARMAX or recurrent neural network
approaches. Although state-of-the-art results are presented for a diverse col-
lection of benchmark problems, there exist a series of open questions mandat-
ing further investigation. How could one handle a variable temporal gap ∆t,
i.e., irregularly sampled data in time? How would common techniques such as
batch normalization, drop out, and L1/L2 regularization enhance the robust-
ness of the proposed algorithm and mitigate the eﬀects of over-ﬁtting? How
could one incorporate partial knowledge of the dynamical system in cases
where certain interaction terms are already known? In terms of future work,
interesting directions include the application of convolutional architectures
[14] for mitigating the complexity associated with very high-dimensional in-
puts, as well as studying possible connections with recent studies linking deep
neural networks with numerical methods and dynamical systems [30, 31].

15

Figure 7: Glycolytic oscillator: Exact versus learned dynamics for random initial condi-
tions chosen from the ranges provided in table 2 of [29].

Acknowledgements

This work received support by the DARPA EQUiPS grant N66001-15-2-
4055 and the AFOSR grant FA9550-17-1-0013. All data and codes used in
this manuscript are publicly available on GitHub at https://github.com/
maziarraissi/MultistepNNs.

16

References

[1] R. Courant, D. Hilbert, Methods of Mathematical Physics, Volume 2:

Diﬀerential Equations, John Wiley & Sons, 2008.

[2] F. Michor, Y. Iwasa, M. A. Nowak, Dynamics of cancer progression,

Nature reviews cancer 4 (2004) 197–205.

[3] C. Castellano, S. Fortunato, V. Loreto, Statistical physics of social

dynamics, Reviews of modern physics 81 (2009) 591.

[4] M. Gavin, The stock market and exchange rate dynamics, Journal of

international money and ﬁnance 8 (1989) 181–200.

[5] L. Ljung, System identiﬁcation,
Springer, 1998, pp. 163–173.

in: Signal analysis and prediction,

[6] S. Chen, S. Billings, C. Cowan, P. Grant, Non-linear systems identi-
ﬁcation using radial basis functions, International Journal of Systems
Science 21 (1990) 2513–2539.

[7] A. Cochocki, R. Unbehauen, Neural networks for optimization and signal

processing, John Wiley & Sons, Inc., 1993.

[8] J. Kocijan, A. Girard, B. Banko, R. Murray-Smith, Dynamic systems
identiﬁcation with gaussian processes, Mathematical and Computer
Modelling of Dynamical Systems 11 (2005) 411–424.

[9] M. Raissi, P. Perdikaris, G. E. Karniadakis, Machine learning of linear
diﬀerential equations using Gaussian processes, Journal of Computa-
tional Physics 348 (2017) 683 – 693.

[10] M. Raissi, G. E. Karniadakis, Hidden physics models: Machine
arXiv preprint

learning of nonlinear partial diﬀerential equations,
arXiv:1708.00588 (2017).

[11] M. Raissi, P. Perdikaris, G. E. Karniadakis, Inferring solutions of dif-
ferential equations using noisy multi-ﬁdelity data, Journal of Computa-
tional Physics 335 (2017) 736–746.

17

[12] M. Raissi, P. Perdikaris, G. E. Karniadakis, Numerical Gaussian pro-
cesses for time-dependent and non-linear partial diﬀerential equations,
arXiv preprint arXiv:1703.10230 (2017).

[13] S. A. Billings, Nonlinear system identiﬁcation: NARMAX methods in
the time, frequency, and spatio-temporal domains, John Wiley & Sons,
2013.

[14] I. Goodfellow, Y. Bengio, A. Courville, Deep learning, MIT press, 2016.

[15] M. Schmidt, H. Lipson, Distilling free-form natural laws from experi-

mental data, science 324 (2009) 81–85.

[16] S. L. Brunton, J. L. Proctor, J. N. Kutz, Discovering governing equa-
tions from data by sparse identiﬁcation of nonlinear dynamical systems,
Proceedings of the National Academy of Sciences 113 (2016) 3932–3937.

[17] S. H. Rudy, S. L. Brunton, J. L. Proctor, J. N. Kutz, Data-driven
discovery of partial diﬀerential equations, Science Advances 3 (2017)
e1602614.

[18] A. Iserles, A ﬁrst course in the numerical analysis of diﬀerential equa-

tions, 44, Cambridge University Press, 2009.

[19] M. Raissi, P. Perdikaris, G. E. Karniadakis, Physics informed deep
learning (part ii): Data-driven discovery of nonlinear partial diﬀerential
equations, arXiv preprint arXiv:1711.10566 (2017).

[20] M. Raissi, P. Perdikaris, G. E. Karniadakis, Physics informed deep
learning (part i): Data-driven solutions of nonlinear partial diﬀerential
equations, arXiv preprint arXiv:1711.10561 (2017).

[21] P. Vincent, H. Larochelle, Y. Bengio, P.-A. Manzagol, Extracting and
composing robust features with denoising autoencoders, in: Proceedings
of the 25th international conference on Machine learning, ACM, pp.
1096–1103.

[22] E. N. Lorenz, Deterministic nonperiodic ﬂow, Journal of the atmospheric

sciences 20 (1963) 130–141.

18

[23] J. N. Kutz, S. L. Brunton, B. W. Brunton, J. L. Proctor, Dynamic
Mode Decomposition: Data-Driven Modeling of Complex Systems, vol-
ume 149, SIAM, 2016.

[24] S. H. Rudy, S. L. Brunton, J. L. Proctor, J. N. Kutz, Data-driven

discovery of partial diﬀerential equations, Science Advances 3 (2017).

[25] K. Taira, T. Colonius, The immersed boundary method: a projection
approach, Journal of Computational Physics 225 (2007) 2118–2137.

[26] T. Colonius, K. Taira, A fast immersed boundary method using a
nullspace approach and multi-domain far-ﬁeld boundary conditions,
Computer Methods in Applied Mechanics and Engineering 197 (2008)
2131–2146.

[27] G. Berkooz, P. Holmes, J. L. Lumley, The proper orthogonal decomposi-
tion in the analysis of turbulent ﬂows, Annual review of ﬂuid mechanics
25 (1993) 539–575.

[28] B. R. Noack, K. Afanasiev, M. MORZY ´NSKI, G. Tadmor, F. Thiele, A
hierarchy of low-dimensional models for the transient and post-transient
cylinder wake, Journal of Fluid Mechanics 497 (2003) 335–363.

[29] B. C. Daniels, I. Nemenman, Eﬃcient inference of parsimonious phe-
nomenological models of cellular dynamics using s-systems and alternat-
ing regression, PloS one 10 (2015) e0119821.

[30] B. Chang, L. Meng, E. Haber, F. Tung, D. Begert, Multi-level residual
networks from dynamical systems view, arXiv preprint arXiv:1710.10348
(2017).

[31] Y. Lu, A. Zhong, Q. Li, B. Dong, Beyond ﬁnite layer neural networks:
Bridging deep architectures and numerical diﬀerential equations, arXiv
preprint arXiv:1710.10121 (2017).

19

8
1
0
2
 
n
a
J
 
4
 
 
]
S
D
.
h
t
a
m

[
 
 
1
v
6
3
2
1
0
.
1
0
8
1
:
v
i
X
r
a

Multistep Neural Networks for Data-driven Discovery of
Nonlinear Dynamical Systems

Maziar Raissi1, Paris Perdikaris2, and George Em Karniadakis1

1Division of Applied Mathematics, Brown University,
Providence, RI, 02912, USA
2Department of Mechanical Engineering and Applied Mechanics,
University of Pennsylvania,
Philadelphia, PA, 19104, USA

Abstract

The process of transforming observed data into predictive mathematical
models of the physical world has always been paramount in science and en-
gineering. Although data is currently being collected at an ever-increasing
pace, devising meaningful models out of such observations in an automated
fashion still remains an open problem.
In this work, we put forth a ma-
chine learning approach for identifying nonlinear dynamical systems from
data. Speciﬁcally, we blend classical tools from numerical analysis, namely
the multi-step time-stepping schemes, with powerful nonlinear function ap-
proximators, namely deep neural networks, to distill the mechanisms that
govern the evolution of a given data-set. We test the eﬀectiveness of our
approach for several benchmark problems involving the identiﬁcation of com-
plex, nonlinear and chaotic dynamics, and we demonstrate how this allows us
to accurately learn the dynamics, forecast future states, and identify basins
of attraction. In particular, we study the Lorenz system, the ﬂuid ﬂow be-
hind a cylinder, the Hopf bifurcation, and the Glycoltic oscillator model as
an example of complicated nonlinear dynamics typical of biological systems.

Keywords:
Machine learning, Systems identiﬁcation, Reduced order modeling,
Data-driven forecasting, Lorenz system, Navier-Stokes

Preprint submitted to Journal Name

January 8, 2018

1. Introduction

Dynamical systems play a key role in shaping our understanding of the
physical world and in deﬁning our ability to predict the evolution of a given
process. From the simple swinging motion of a clock pendulum to the com-
plex ﬂow around an airplane wing, the mathematical modeling of dynamical
systems can yield a set of tools with which we can analyze the way the current
state of the system depends on the past, and predict the possible states we
may encounter in the future. Often such tools are precisely known, usually
coming in the form of diﬀerential equations that are derived from ﬁrst phys-
ical principles, such as the conservation of energy, mass, and momentum [1].
However, in many cases, the sheer complexity of a system can prohibit our
complete understanding and render a ﬁrst principles approach infeasible. In
this setting, one may be only able to postulate crude and potentially overly
simpliﬁed models based on a given a set of empirical observations (see e.g.,
models for tumor growth [2], social dynamics [3], and the stock market [4]).
In the present era of abundant data and advanced machine learning capa-
bilities, a natural question arises: can we automatically discover suﬃciently
sophisticated and accurate mathematical models of complex dynamical sys-
tems directly from data?

The answer to this question pertains to the well-established ﬁeld of sys-
tems identiﬁcation [5]. Discriminating between white-, gray-, and black-box
approaches, depending on whether a ﬁrst principles modeling approach is
fully, partially, or not admissible, systems identiﬁcation aims to devise math-
ematical models for predicting a future state of a system, given the evolution
of a set of previously observed or latent states. Speciﬁcally, in the con-
text of identifying nonlinear dynamics, there exist several deterministic and
probabilistic tools including radial basis functions [6], neural networks [7],
Gaussian processes [8, 9, 10, 11, 12], and nonlinear auto-regressive models
such as NARMAX [13] and recurrent neural networks [14]. A common theme
among all such methods is the pursuit of learning a nonlinear and potentially
multi-variate mapping f that predicts the future system states given a set
of data describing the present and past states. More recently, approaches
based on symbolic regression [15], sparse regression, and compressive sensing
[16, 17] were able to go beyond estimating a black-box approximation of the
dynamics given by f , and return more interpretable models that can uncover
the full parametric form of an underlying governing equation. However, in

2

order to obtain sparse representation of the dynamics, the aforementioned
approaches have to rely upon the nontrivial task of choosing “appropriate”
sets of basis functions. Consequently, investigating ways of incorporating
broader function search spaces is an important area of current and future
research.

In this work, we introduce a novel approach to nonlinear systems identiﬁ-
cation that combines the classical multistep family of time-stepping schemes
from numerical analysis [18] with deep neural networks.
Inspired by re-
cent developments in physics-informed deep learning [19, 20], we construct
structured nonlinear regression models that can discover the dynamic depen-
dencies in a given set of temporal data-snapshots, and return a closed form
model that can be subsequently used to forecast future states or identify
basins of attraction. In contrast to recent approaches to systems identiﬁca-
tion [16, 17], here we do not have to have direct access or approximations to
temporal gradients because the time derivatives are discretized using classi-
cal time-stepping rules. Moreover, we are using a richer family of function
approximators and consequently we do not have to commit to a particular
class of basis functions such as polynomials or sines and cosines. This comes
at the cost of losing interpretability of the learned dynamics. However, there
is nothing hindering the use of a particular class of basis functions and obtain
more interpretable equations.

This paper is structured as follows. In section 2 we provide a detailed
In section 3.1, we investigate the
overview of the proposed methodology.
performance of the proposed framework by applying our algorithm to the
two-dimensional damped harmonic oscillator. We then explore the identiﬁ-
cation of chaotic dynamics of the Lorenz system in section 3.2. As an example
of a high dimensional dynamical systems, in section 3.3, we study the Navier-
Stokes equations describing the ﬂuid ﬂow behind a cylinder. To illustrate the
ability of our method to identify parameterized dynamics, we consider the
Hopf normal form in section 3.4. As an example of complicated nonlinear
dynamics typical of biological systems, we explore the glycolytic oscillator
model in section 3.5. It should be highlighted that all of the examples con-
sidered in this work are inspired by the pioneering work of Brunton et. al.
[16]. Moreover, all data and codes used in this manuscript are publicly avail-
able on GitHub at https://github.com/maziarraissi/MultistepNNs.

3

2. Problem setup and solution methodology

Let us consider nonlinear dynamical systems of the form1

d
dt

x(t) = f (x(t)) ,

(1)

where the vector x(t) ∈ RD denotes the state of the system at time t and the
function f describes the evolution of the system. Given noisy measurements
of the state x(t) of the system at several time instances t1, t2, . . . , tN , our
goal is to determine the function f and consequently discover the underlying
dynamical system (1) from data. We proceed by applying the general form
of a linear multistep method with M steps to equation (1) and obtain

M
(cid:88)

m=0

[αmxn

m + ∆tβmf (xn
−

m)] = 0, n = M, . . . , N.
−

(2)

m) at time tn
m denotes the state of the system x(tn
−
−

Here, xn
m. Diﬀerent
−
choices for the parameters αm and βm result in speciﬁc schemes. For instance,
the trapezoidal rule

xn = xn
−

1 +

1)) , n = 1, . . . , N,
∆t (f (xn) + f (xn
−

(3)

1
2

corresponds to the case where M = 1, α0 = −1, α1 = 1, and β0 = β1 =
0.5. We proceed by placing a neural network prior on the function f . The
parameters of this neural network can be learned by minimizing the mean
squared error loss function

M SE :=

1
N − M + 1

N
(cid:88)

n=M

|yn|2,

(4)

1It is straightforward to generalize the dynamics to include parameterization, time
dependence, and forcing. In particular, parameterization, time dependence, and external
forcing or feedback control u(t) may be added to the vector ﬁeld according to

˙x = f (x, u, t; λ) ,

˙t = 1,

˙λ = 0.

4

yn :=

[αmxn
−

m + ∆tβmf (xn
−

m)] , n = M, . . . , N,

(5)

where

M
(cid:88)

m=0

is obtained from the multistep scheme (2).

3. Results

3.1. Two-dimensional damped oscillator

As a ﬁrst illustrative example, let us consider the two-dimensional damped

harmonic oscillator with cubic dynamics; i.e.,

˙x = −0.1 x3 + 2.0 y3,
˙y = −2.0 x3 − 0.1 y3.

(6)

We use [x0 y0]T = [2 0]T as initial condition and collect data from t = 0 to
t = 25 with a time-step size of ∆t = 0.01. The data are plotted in ﬁgure
1. We employ a neural network with one hidden layer and 256 neurons to
represent the nonlinear dynamics. As for the multistep scheme (2) we use
Adams-Moulton with M = 1 steps (i.e., the trapezoidal rule). Upon train-
ing the neural network, we solve the identiﬁed system using the same initial
condition as the one above. Figure 1 provides a qualitative assessment of
the accuracy in identifying the correct nonlinear dynamics. Speciﬁcally, by
comparing the exact and predicted trajectories of the system, as well as the
resulting phase portraits, we observe that the algorithm can correctly cap-
ture the dynamic evolution of the system.

To investigate the performance of the proposed work-ﬂow with respect
to diﬀerent linear multi-step methods, we have considered the three families
that are most commonly used in practice: Adams-Bashforth (AB) methods,
Adams-Moulton (AM) methods, and the backward diﬀerentiation formulas
(BDFs).
In tables 1 and 2, we report the relative L2 error between tra-
jectories of the exact and the identiﬁed systems for diﬀerent members of
the class of linear multi-step methods.
Interestingly, the Adams-Moulton
scheme seems to consistently return more accurate results compared to the
Adams-Bashforth and BDF approaches. One intuitive explanation for this

5

Figure 1: Harmonic Oscillator: Trajectories of the two-dimensional damped harmonic
oscillator with cubic dynamics are depicted in the left panel while the corresponding phase
portrait is plotted in the right panel. Solid colored lines represent the exact dynamics while
the dashed black lines demonstrate the learned dynamics. The identiﬁed system correctly
captures the form of the dynamics and accurately reproduces the phase portrait.

behavior stems from a closer inspection of equation 2. Speciﬁcally, the ar-
rangement of the resulting terms for the Adams-Moulton schemes leads to a
higher throughput of training data ﬂowing through the neural network during
model training as compared to the Adams-Bashforth and BDF cases. This
helps regularize the neural network and eventually achieve a better calibra-
tion during training. Also, out of the Adams-Moulton family, the trapezoidal
rule seems to work the best in practice perhaps due to its superior stability
properties [18]. These performance characteristics should be interpreted as
product of empirical evidence, and not as concrete theoretical properties of
the method. Identiﬁcation of the latter requires more extensive systematic
studies that go beyond the scope of this paper.

In tables 3 and 4, we study the robustness of our results with respect
to the gap ∆t between pairs of data and with respect to noise in the ob-
servations of the system. These results fail to reveal a consistent pattern as
larger time-step sizes ∆t and larger noise corruption levels sometimes lead to
superior accuracy and other times to inferior. In the latter cases, the reasons

6

M

Scheme
Adams-Bashforth
Adams-Moulton
BDF

1

2

3

4

5

1.5e+00
8.8e-03
1.3e+00

3.1e-02
1.2e-02
8.8e-03

1.2e-01
1.6e-02
1.3e-02

4.3e-02
6.3e-03
1.4e-02

1.2e-02
1.1e-02
1.7e-02

Table 1: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the ﬁrst dynamic component x(t) integrated up to time t = 25, for diﬀerent
member families of the class of multistep methods, and diﬀerent number of steps M . Here,
the training data is assumed to be noise free, and the neural network architecture is kept
ﬁxed to have one hidden layer and 256 neurons.

M

Scheme
Adams-Bashforth
Adams-Moulton
BDF

1

2

3

4

5

1.5e+00
8.8e-03
1.3e+00

3.0e-02
1.0e-02
8.6e-03

9.7e-02
1.6e-02
9.9e-03

3.5e-02
5.8e-03
1.4e-02

1.2e-02
1.1e-02
1.5e-02

Table 2: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the second dynamic component y(t) integrated up to time t = 25, for
diﬀerent member families of the class of multistep methods, and diﬀerent number of steps
M . Here, the training data is assumed to be noise free, and the neural network architecture
is kept ﬁxed to have one hidden layer and 256 neurons.

are obvious, namely that a larger gap ∆t makes the approximation in time
less accurate, while too much noise is devastating because it would be harder
to distinguish between noise and the true dynamics. On the other hand, we
do observe some cases in which larger ∆t and noise levels may actually help.
In these cases, we believe that input noise can act as a regularization mech-
anism that increases the robustness of the model training procedure, similar
to how it has been previously proposed in the neural network literature (see
for e.g., denoising autoencoders [21]). Along the same lines, a bigger tem-
poral gap ∆t helps because it makes two consecutive time snapshots carry
more information simply because they are more dissimilar to one another.
On the contrary, if ∆t is too small, the importance of the neural network
becomes less and less pronounced as seen in equation (2), hence model train-
ing becomes infeasible. These empirical results indicate that there exists a
problem-dependent sweet spot for the admissible values of the time-step and
noise levels that can lead to the best predictive accuracy. Although one may

7

Table 3: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the ﬁrst dynamic component x(t) integrated up to time t = 25, for diﬀerent
noise magnitudes, and diﬀerent gap ∆t between pairs of snapshots. Here, we are employing
the trapezoidal time-stepping scheme and the neural network architecture is kept ﬁx to
have one hidden layer and 256 neurons.

noise

0.00% 0.01% 0.02%

5.7e-03
1.8e-02
3.8e-02
5.4e-02
8.3e-02

2.4e-02
1.1e-01
9.2e-02
4.0e-02
2.9e-01

2.2e-01
1.3e-01
7.8e-01
8.7e-01
9.2e-02

noise

0.00% 0.01% 0.02%

5.9e-03
1.7e-02
3.3e-02
5.2e-02
7.2e-02

2.1e-02
1.0e-01
9.0e-02
3.8e-02
2.6e-01

2.2e-01
1.1e-01
7.9e-01
8.7e-01
8.2e-02

∆t
0.01
0.02
0.03
0.04
0.05

∆t
0.01
0.02
0.03
0.04
0.05

Table 4: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the ﬁrst dynamic component y(t) integrated up to time t = 25, for diﬀerent
noise magnitudes, and diﬀerent gap ∆t between pairs of snapshots. Here, we are employing
the trapezoidal time-stepping scheme and the neural network architecture is kept ﬁx to
have one hidden layer and 256 neurons.

have no control over the noise corrupting the data, the temporal gap ∆t
could be treated as another hyper-parameter like the number of neurons and
hidden layers when setting up the neural network.

Finally, tables 5 and 6 study the robustness of our results with respect to
the neural network structure. For this case, more accurate results seem to
be obtained with increasing network depth, although increasing the network
width seems to have a negative aﬀect for more than 128 neurons per layer.
To fully quantify sensitivity with respect to network architecture a more
systematic study involving multiple data-sets is needed.

8

Table 5: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the ﬁrst dynamic component x(t) integrated up to time t = 25, for diﬀerent
neural network architectures. Here, the training data is assumed to be noise free, the time
step size is kept ﬁxed at ∆t = 0.01, and the number of Adams-Moulton steps is ﬁxed at
M = 1.

Layers
1
2
3

Layers
1
2
3

Neurons

64

128

256

9.8e-03
3.6e-03
3.4e-03

6.1e-03
1.2e-02
1.6e-02

3.7e-02
2.4e-02
4.2e-02

Neurons

64

128

256

7.2e-03
3.3e-03
3.0e-03

5.4e-03
9.1e-03
1.4e-02

3.5e-02
2.0e-02
3.7e-02

Table 6: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the second dynamic component y(t) integrated up to time t = 25, for
diﬀerent neural network architectures. Here, the training data is assumed to be noise free,
the time step size is kept ﬁxed at ∆t = 0.01, and the number of Adams-Moulton steps is
ﬁxed at M = 1.

3.2. Lorenz system

To explore the identiﬁcation of chaotic dynamics evolving on a ﬁnite

dimensional attractor, we consider the nonlinear Lorenz system [22]

˙x = 10(y − x),
˙y = x(28 − z) − y,
˙z = xy − (8/3)z.

(7)

We use [x0 y0 z0]T = [−8 7 27]T as initial condition and collect data from
t = 0 to t = 25 with a time-step size of ∆t = 0.01. The data are plotted in
ﬁgures 2 and 3. We employ a neural network with one hidden layer and 256
neurons to represent the nonlinear dynamics. As for the multistep scheme
(2) we use Adams-Moulton with M = 1 steps (i.e., the trapezoidal rule).
Upon training the neural network, we solve the identiﬁed system using the
same initial condition as the one above. As depicted in ﬁgure 2, the learned
system correctly captures the form of the attractor.

9

Figure 2: Lorenz System: The exact phase portrait of the Lorenz system (left panel) is
compared to the corresponding phase portrait of the learned dynamics (right panel).

The Lorenz system has a positive Lyapunov exponent, and small diﬀer-
ences between the exact and learned models grow exponentially, even though
the attractor remains intact. This behavior is evident in ﬁgure 3, as we com-
pare the exact versus the predicted trajectories. Small discrepancies due to
ﬁnite accuracy in the predicted dynamics lead to large errors in the fore-
casted time-series after t > 4, despite the fact that the bi-stable structure of
the attractor is well captured (see ﬁgure 2).

3.3. Fluid ﬂow behind a cylinder

In this example we collect data for the ﬂuid ﬂow past a cylinder (see ﬁg-
ure 4) at Reynolds number 100 using direct numerical simulations of the two
dimensional Navier-Stokes equations.
In particular, following the problem
setup presented in [23] and [24], we simulate the Navier-Stokes equations de-
scribing the two-dimensional ﬂuid ﬂow past a circular cylinder at Reynolds
number 100 using the Immersed Boundary Projection Method [25, 26]. This
approach utilizes a multi-domain scheme with four nested domains, each suc-
cessive grid being twice as large as the previous one. Length and time are
non-dimensionalized so that the cylinder has unit diameter and the ﬂow has
unit velocity. Data is collected on the ﬁnest domain with dimensions 9 × 4 at
a grid resolution of 449 × 199. The ﬂow solver uses a 3rd-order Runge Kutta

10

Figure 3: Lorenz System: The exact trajectories of the Lorenz systems is compared to the
corresponding trajectories of the learned dynamics. Solid blue lines represent the exact
dynamics while the dashed black lines demonstrate the learned dynamics.

integration scheme with a time step of t = 0.02, which has been veriﬁed to
yield well-resolved and converged ﬂow ﬁelds. After simulations converge to
steady periodic vortex shedding, ﬂow snapshots are saved every ∆t = 0.02.
We then reduce the dimension of the system by proper orthogonal decompo-
sition (POD) [27, 16]. The POD results in a hierarchy of orthonormal modes
that, when truncated, capture most of the energy of the original system for
the given rank truncation. The ﬁrst two most energetic POD modes capture
a signiﬁcant portion of the energy; the steady-state vortex shedding is a limit

11

Figure 4: Flow past a cylinder: A snapshot of the vorticity ﬁeld of a solution to the
Navier-Stokes equations for the ﬂuid ﬂow past a cylinder.

cycle in these coordinates [16]. An additional mode, called the shift mode, is
included to capture the transient dynamics connecting the unstable steady
state with the mean of the limit cycle [28]. The resulting POD coeﬃcients
are depicted in ﬁgure 5.

We employ a neural network with one hidden layer and 256 neurons to rep-
resent the nonlinear dynamics shown in ﬁgure 5. As for the linear multistep
scheme (2) we use Adams-Moulton with M = 1 steps (i.e., the trapezoidal
rule). Upon training the neural network, we solve the identiﬁed system. As
depicted in ﬁgure 5, the learned system correctly captures the form of the
dynamics and accurately reproduces the phase portrait, including both the
transient regime as well as the limit cycle attained once the ﬂow dynamics
converge to the well known K´arman vortex street.

3.4. Hopf bifurcation

Many real-world systems depend on parameters and, when the parameters
are varied, they may go through bifurcations. To illustrate the ability of our
method to identify parameterized dynamics, let us consider the Hopf normal
form

˙x = µx + y − x(x2 + y2),
˙y = −x + µy − y(x2 + y2).

(8)

12

Figure 5: Flow past a cylinder: The exact phase portrait of the cylinder wake trajectory
in reduced coordinates (left panel) is compared to the corresponding phase portrait of the
learned dynamics (right panel).

Our algorithm can be readily extended to encompass parameterized systems.
In particular, the system (8) can be equivalently written as

˙µ = 0,
˙x = µx + y − x(x2 + y2),
˙y = −x + µy − y(x2 + y2).

(9)

We collect data from the Hopf system (9) for various initial conditions corre-
sponding to diﬀerent parameter values for µ. The data is depicted in ﬁgure 6.
The identiﬁed parameterized dynamics is shown in ﬁgure 6 for a set of param-
eter values diﬀerent from the ones used during model training. The learned
system correctly captures the transition from the ﬁxed point for µ < 0 to the
limit cycle for µ > 0.

3.5. Glycolytic oscillator

As an example of complicated nonlinear dynamics typical of biological
systems, we simulate the glycolytic oscillator model presented in [29] and [16].
The model consists of ordinary diﬀerential equations for the concentrations

13

Figure 6: Hopf bifurcation: Training data from the Hopf system for various initial con-
ditions corresponding to diﬀerent parameter values for µ (left panel) is compared to the
corresponding phase portrait of the learned dynamics (right panel). It is worth highlight-
ing that the algorithm is tested on initial conditions diﬀerent from the ones used during
training.

of 7 biochemical species; i.e.,

dS1
dt
dS2
dt
dS3
dt
dS4
dt
dS5
dt
dS6
dt
dS7
dt

= J0 −

,

k1S1S6
1 + (S6/K1)q
k1S1S6
1 + (S6/K1)q

= 2

− k2S2(N − S5) − k6S2S5,

= k2S2(N − S5) − k3S3(N − S6),

= k3S3(A − S6) − k4S4S5 − κ(S4 − S7),

(10)

= k2S2(N − S5) − k4S4S5 − k6S2S5,

= −2

k1S1S6
1 + (S6/K1)q

= ψκ(S4 − S7) − kS7.

+ 2k3S3(A − S6) − k5S6,

14

The parameters of the model are chosen according to table 1 of [29]. As shown
in ﬁgure 7, data from a simulation of equation (10) are collected from t = 0
to t = 10 with a time-step size of ∆t = 0.01. We employ a neural network
with one hidden layer and 256 neurons to represent the nonlinear dynamics.
As for the multi-step scheme (2) we use Adams-Moulton with M = 1 steps
(i.e., the trapezoidal rule). Upon training the neural network, we solve the
identiﬁed system using the same initial condition as the ones used for the
exact system. As depicted in ﬁgure 7, the learned system correctly captures
the form of the dynamics.

4. Summary and Discussion

We have presented a machine learning approach for extracting nonlinear
dynamical systems from time-series data. The proposed algorithm lever-
ages the structure of well studied multi-step time-stepping schemes such as
Adams-Bashforth, Adams Moulton, and BDF families, to construct eﬃcient
algorithms for learning dynamical systems using deep neural networks. A
key property of the proposed approach is the use of multiple steps which
enables us to incorporate memory eﬀects in learning the temporal dynamics
and tackle problems with a nonlinear and non-Markovian dynamical struc-
ture. Speciﬁcally, the use of M steps allows us to decouple the regression
complexity due to several temporal lags, ultimately leading to a simpler
D-dimensional regression problem, as opposed to an (M × D)-dimensional
problem in the case of a brute force NARMAX or recurrent neural network
approaches. Although state-of-the-art results are presented for a diverse col-
lection of benchmark problems, there exist a series of open questions mandat-
ing further investigation. How could one handle a variable temporal gap ∆t,
i.e., irregularly sampled data in time? How would common techniques such as
batch normalization, drop out, and L1/L2 regularization enhance the robust-
ness of the proposed algorithm and mitigate the eﬀects of over-ﬁtting? How
could one incorporate partial knowledge of the dynamical system in cases
where certain interaction terms are already known? In terms of future work,
interesting directions include the application of convolutional architectures
[14] for mitigating the complexity associated with very high-dimensional in-
puts, as well as studying possible connections with recent studies linking deep
neural networks with numerical methods and dynamical systems [30, 31].

15

Figure 7: Glycolytic oscillator: Exact versus learned dynamics for random initial condi-
tions chosen from the ranges provided in table 2 of [29].

Acknowledgements

This work received support by the DARPA EQUiPS grant N66001-15-2-
4055 and the AFOSR grant FA9550-17-1-0013. All data and codes used in
this manuscript are publicly available on GitHub at https://github.com/
maziarraissi/MultistepNNs.

16

References

[1] R. Courant, D. Hilbert, Methods of Mathematical Physics, Volume 2:

Diﬀerential Equations, John Wiley & Sons, 2008.

[2] F. Michor, Y. Iwasa, M. A. Nowak, Dynamics of cancer progression,

Nature reviews cancer 4 (2004) 197–205.

[3] C. Castellano, S. Fortunato, V. Loreto, Statistical physics of social

dynamics, Reviews of modern physics 81 (2009) 591.

[4] M. Gavin, The stock market and exchange rate dynamics, Journal of

international money and ﬁnance 8 (1989) 181–200.

[5] L. Ljung, System identiﬁcation,
Springer, 1998, pp. 163–173.

in: Signal analysis and prediction,

[6] S. Chen, S. Billings, C. Cowan, P. Grant, Non-linear systems identi-
ﬁcation using radial basis functions, International Journal of Systems
Science 21 (1990) 2513–2539.

[7] A. Cochocki, R. Unbehauen, Neural networks for optimization and signal

processing, John Wiley & Sons, Inc., 1993.

[8] J. Kocijan, A. Girard, B. Banko, R. Murray-Smith, Dynamic systems
identiﬁcation with gaussian processes, Mathematical and Computer
Modelling of Dynamical Systems 11 (2005) 411–424.

[9] M. Raissi, P. Perdikaris, G. E. Karniadakis, Machine learning of linear
diﬀerential equations using Gaussian processes, Journal of Computa-
tional Physics 348 (2017) 683 – 693.

[10] M. Raissi, G. E. Karniadakis, Hidden physics models: Machine
arXiv preprint

learning of nonlinear partial diﬀerential equations,
arXiv:1708.00588 (2017).

[11] M. Raissi, P. Perdikaris, G. E. Karniadakis, Inferring solutions of dif-
ferential equations using noisy multi-ﬁdelity data, Journal of Computa-
tional Physics 335 (2017) 736–746.

17

[12] M. Raissi, P. Perdikaris, G. E. Karniadakis, Numerical Gaussian pro-
cesses for time-dependent and non-linear partial diﬀerential equations,
arXiv preprint arXiv:1703.10230 (2017).

[13] S. A. Billings, Nonlinear system identiﬁcation: NARMAX methods in
the time, frequency, and spatio-temporal domains, John Wiley & Sons,
2013.

[14] I. Goodfellow, Y. Bengio, A. Courville, Deep learning, MIT press, 2016.

[15] M. Schmidt, H. Lipson, Distilling free-form natural laws from experi-

mental data, science 324 (2009) 81–85.

[16] S. L. Brunton, J. L. Proctor, J. N. Kutz, Discovering governing equa-
tions from data by sparse identiﬁcation of nonlinear dynamical systems,
Proceedings of the National Academy of Sciences 113 (2016) 3932–3937.

[17] S. H. Rudy, S. L. Brunton, J. L. Proctor, J. N. Kutz, Data-driven
discovery of partial diﬀerential equations, Science Advances 3 (2017)
e1602614.

[18] A. Iserles, A ﬁrst course in the numerical analysis of diﬀerential equa-

tions, 44, Cambridge University Press, 2009.

[19] M. Raissi, P. Perdikaris, G. E. Karniadakis, Physics informed deep
learning (part ii): Data-driven discovery of nonlinear partial diﬀerential
equations, arXiv preprint arXiv:1711.10566 (2017).

[20] M. Raissi, P. Perdikaris, G. E. Karniadakis, Physics informed deep
learning (part i): Data-driven solutions of nonlinear partial diﬀerential
equations, arXiv preprint arXiv:1711.10561 (2017).

[21] P. Vincent, H. Larochelle, Y. Bengio, P.-A. Manzagol, Extracting and
composing robust features with denoising autoencoders, in: Proceedings
of the 25th international conference on Machine learning, ACM, pp.
1096–1103.

[22] E. N. Lorenz, Deterministic nonperiodic ﬂow, Journal of the atmospheric

sciences 20 (1963) 130–141.

18

[23] J. N. Kutz, S. L. Brunton, B. W. Brunton, J. L. Proctor, Dynamic
Mode Decomposition: Data-Driven Modeling of Complex Systems, vol-
ume 149, SIAM, 2016.

[24] S. H. Rudy, S. L. Brunton, J. L. Proctor, J. N. Kutz, Data-driven

discovery of partial diﬀerential equations, Science Advances 3 (2017).

[25] K. Taira, T. Colonius, The immersed boundary method: a projection
approach, Journal of Computational Physics 225 (2007) 2118–2137.

[26] T. Colonius, K. Taira, A fast immersed boundary method using a
nullspace approach and multi-domain far-ﬁeld boundary conditions,
Computer Methods in Applied Mechanics and Engineering 197 (2008)
2131–2146.

[27] G. Berkooz, P. Holmes, J. L. Lumley, The proper orthogonal decomposi-
tion in the analysis of turbulent ﬂows, Annual review of ﬂuid mechanics
25 (1993) 539–575.

[28] B. R. Noack, K. Afanasiev, M. MORZY ´NSKI, G. Tadmor, F. Thiele, A
hierarchy of low-dimensional models for the transient and post-transient
cylinder wake, Journal of Fluid Mechanics 497 (2003) 335–363.

[29] B. C. Daniels, I. Nemenman, Eﬃcient inference of parsimonious phe-
nomenological models of cellular dynamics using s-systems and alternat-
ing regression, PloS one 10 (2015) e0119821.

[30] B. Chang, L. Meng, E. Haber, F. Tung, D. Begert, Multi-level residual
networks from dynamical systems view, arXiv preprint arXiv:1710.10348
(2017).

[31] Y. Lu, A. Zhong, Q. Li, B. Dong, Beyond ﬁnite layer neural networks:
Bridging deep architectures and numerical diﬀerential equations, arXiv
preprint arXiv:1710.10121 (2017).

19

8
1
0
2
 
n
a
J
 
4
 
 
]
S
D
.
h
t
a
m

[
 
 
1
v
6
3
2
1
0
.
1
0
8
1
:
v
i
X
r
a

Multistep Neural Networks for Data-driven Discovery of
Nonlinear Dynamical Systems

Maziar Raissi1, Paris Perdikaris2, and George Em Karniadakis1

1Division of Applied Mathematics, Brown University,
Providence, RI, 02912, USA
2Department of Mechanical Engineering and Applied Mechanics,
University of Pennsylvania,
Philadelphia, PA, 19104, USA

Abstract

The process of transforming observed data into predictive mathematical
models of the physical world has always been paramount in science and en-
gineering. Although data is currently being collected at an ever-increasing
pace, devising meaningful models out of such observations in an automated
fashion still remains an open problem.
In this work, we put forth a ma-
chine learning approach for identifying nonlinear dynamical systems from
data. Speciﬁcally, we blend classical tools from numerical analysis, namely
the multi-step time-stepping schemes, with powerful nonlinear function ap-
proximators, namely deep neural networks, to distill the mechanisms that
govern the evolution of a given data-set. We test the eﬀectiveness of our
approach for several benchmark problems involving the identiﬁcation of com-
plex, nonlinear and chaotic dynamics, and we demonstrate how this allows us
to accurately learn the dynamics, forecast future states, and identify basins
of attraction. In particular, we study the Lorenz system, the ﬂuid ﬂow be-
hind a cylinder, the Hopf bifurcation, and the Glycoltic oscillator model as
an example of complicated nonlinear dynamics typical of biological systems.

Keywords:
Machine learning, Systems identiﬁcation, Reduced order modeling,
Data-driven forecasting, Lorenz system, Navier-Stokes

Preprint submitted to Journal Name

January 8, 2018

1. Introduction

Dynamical systems play a key role in shaping our understanding of the
physical world and in deﬁning our ability to predict the evolution of a given
process. From the simple swinging motion of a clock pendulum to the com-
plex ﬂow around an airplane wing, the mathematical modeling of dynamical
systems can yield a set of tools with which we can analyze the way the current
state of the system depends on the past, and predict the possible states we
may encounter in the future. Often such tools are precisely known, usually
coming in the form of diﬀerential equations that are derived from ﬁrst phys-
ical principles, such as the conservation of energy, mass, and momentum [1].
However, in many cases, the sheer complexity of a system can prohibit our
complete understanding and render a ﬁrst principles approach infeasible. In
this setting, one may be only able to postulate crude and potentially overly
simpliﬁed models based on a given a set of empirical observations (see e.g.,
models for tumor growth [2], social dynamics [3], and the stock market [4]).
In the present era of abundant data and advanced machine learning capa-
bilities, a natural question arises: can we automatically discover suﬃciently
sophisticated and accurate mathematical models of complex dynamical sys-
tems directly from data?

The answer to this question pertains to the well-established ﬁeld of sys-
tems identiﬁcation [5]. Discriminating between white-, gray-, and black-box
approaches, depending on whether a ﬁrst principles modeling approach is
fully, partially, or not admissible, systems identiﬁcation aims to devise math-
ematical models for predicting a future state of a system, given the evolution
of a set of previously observed or latent states. Speciﬁcally, in the con-
text of identifying nonlinear dynamics, there exist several deterministic and
probabilistic tools including radial basis functions [6], neural networks [7],
Gaussian processes [8, 9, 10, 11, 12], and nonlinear auto-regressive models
such as NARMAX [13] and recurrent neural networks [14]. A common theme
among all such methods is the pursuit of learning a nonlinear and potentially
multi-variate mapping f that predicts the future system states given a set
of data describing the present and past states. More recently, approaches
based on symbolic regression [15], sparse regression, and compressive sensing
[16, 17] were able to go beyond estimating a black-box approximation of the
dynamics given by f , and return more interpretable models that can uncover
the full parametric form of an underlying governing equation. However, in

2

order to obtain sparse representation of the dynamics, the aforementioned
approaches have to rely upon the nontrivial task of choosing “appropriate”
sets of basis functions. Consequently, investigating ways of incorporating
broader function search spaces is an important area of current and future
research.

In this work, we introduce a novel approach to nonlinear systems identiﬁ-
cation that combines the classical multistep family of time-stepping schemes
from numerical analysis [18] with deep neural networks.
Inspired by re-
cent developments in physics-informed deep learning [19, 20], we construct
structured nonlinear regression models that can discover the dynamic depen-
dencies in a given set of temporal data-snapshots, and return a closed form
model that can be subsequently used to forecast future states or identify
basins of attraction. In contrast to recent approaches to systems identiﬁca-
tion [16, 17], here we do not have to have direct access or approximations to
temporal gradients because the time derivatives are discretized using classi-
cal time-stepping rules. Moreover, we are using a richer family of function
approximators and consequently we do not have to commit to a particular
class of basis functions such as polynomials or sines and cosines. This comes
at the cost of losing interpretability of the learned dynamics. However, there
is nothing hindering the use of a particular class of basis functions and obtain
more interpretable equations.

This paper is structured as follows. In section 2 we provide a detailed
In section 3.1, we investigate the
overview of the proposed methodology.
performance of the proposed framework by applying our algorithm to the
two-dimensional damped harmonic oscillator. We then explore the identiﬁ-
cation of chaotic dynamics of the Lorenz system in section 3.2. As an example
of a high dimensional dynamical systems, in section 3.3, we study the Navier-
Stokes equations describing the ﬂuid ﬂow behind a cylinder. To illustrate the
ability of our method to identify parameterized dynamics, we consider the
Hopf normal form in section 3.4. As an example of complicated nonlinear
dynamics typical of biological systems, we explore the glycolytic oscillator
model in section 3.5. It should be highlighted that all of the examples con-
sidered in this work are inspired by the pioneering work of Brunton et. al.
[16]. Moreover, all data and codes used in this manuscript are publicly avail-
able on GitHub at https://github.com/maziarraissi/MultistepNNs.

3

2. Problem setup and solution methodology

Let us consider nonlinear dynamical systems of the form1

d
dt

x(t) = f (x(t)) ,

(1)

where the vector x(t) ∈ RD denotes the state of the system at time t and the
function f describes the evolution of the system. Given noisy measurements
of the state x(t) of the system at several time instances t1, t2, . . . , tN , our
goal is to determine the function f and consequently discover the underlying
dynamical system (1) from data. We proceed by applying the general form
of a linear multistep method with M steps to equation (1) and obtain

M
(cid:88)

m=0

[αmxn

m + ∆tβmf (xn
−

m)] = 0, n = M, . . . , N.
−

(2)

m) at time tn
m denotes the state of the system x(tn
−
−

Here, xn
m. Diﬀerent
−
choices for the parameters αm and βm result in speciﬁc schemes. For instance,
the trapezoidal rule

xn = xn
−

1 +

1)) , n = 1, . . . , N,
∆t (f (xn) + f (xn
−

(3)

1
2

corresponds to the case where M = 1, α0 = −1, α1 = 1, and β0 = β1 =
0.5. We proceed by placing a neural network prior on the function f . The
parameters of this neural network can be learned by minimizing the mean
squared error loss function

M SE :=

1
N − M + 1

N
(cid:88)

n=M

|yn|2,

(4)

1It is straightforward to generalize the dynamics to include parameterization, time
dependence, and forcing. In particular, parameterization, time dependence, and external
forcing or feedback control u(t) may be added to the vector ﬁeld according to

˙x = f (x, u, t; λ) ,

˙t = 1,

˙λ = 0.

4

yn :=

[αmxn
−

m + ∆tβmf (xn
−

m)] , n = M, . . . , N,

(5)

where

M
(cid:88)

m=0

is obtained from the multistep scheme (2).

3. Results

3.1. Two-dimensional damped oscillator

As a ﬁrst illustrative example, let us consider the two-dimensional damped

harmonic oscillator with cubic dynamics; i.e.,

˙x = −0.1 x3 + 2.0 y3,
˙y = −2.0 x3 − 0.1 y3.

(6)

We use [x0 y0]T = [2 0]T as initial condition and collect data from t = 0 to
t = 25 with a time-step size of ∆t = 0.01. The data are plotted in ﬁgure
1. We employ a neural network with one hidden layer and 256 neurons to
represent the nonlinear dynamics. As for the multistep scheme (2) we use
Adams-Moulton with M = 1 steps (i.e., the trapezoidal rule). Upon train-
ing the neural network, we solve the identiﬁed system using the same initial
condition as the one above. Figure 1 provides a qualitative assessment of
the accuracy in identifying the correct nonlinear dynamics. Speciﬁcally, by
comparing the exact and predicted trajectories of the system, as well as the
resulting phase portraits, we observe that the algorithm can correctly cap-
ture the dynamic evolution of the system.

To investigate the performance of the proposed work-ﬂow with respect
to diﬀerent linear multi-step methods, we have considered the three families
that are most commonly used in practice: Adams-Bashforth (AB) methods,
Adams-Moulton (AM) methods, and the backward diﬀerentiation formulas
(BDFs).
In tables 1 and 2, we report the relative L2 error between tra-
jectories of the exact and the identiﬁed systems for diﬀerent members of
the class of linear multi-step methods.
Interestingly, the Adams-Moulton
scheme seems to consistently return more accurate results compared to the
Adams-Bashforth and BDF approaches. One intuitive explanation for this

5

Figure 1: Harmonic Oscillator: Trajectories of the two-dimensional damped harmonic
oscillator with cubic dynamics are depicted in the left panel while the corresponding phase
portrait is plotted in the right panel. Solid colored lines represent the exact dynamics while
the dashed black lines demonstrate the learned dynamics. The identiﬁed system correctly
captures the form of the dynamics and accurately reproduces the phase portrait.

behavior stems from a closer inspection of equation 2. Speciﬁcally, the ar-
rangement of the resulting terms for the Adams-Moulton schemes leads to a
higher throughput of training data ﬂowing through the neural network during
model training as compared to the Adams-Bashforth and BDF cases. This
helps regularize the neural network and eventually achieve a better calibra-
tion during training. Also, out of the Adams-Moulton family, the trapezoidal
rule seems to work the best in practice perhaps due to its superior stability
properties [18]. These performance characteristics should be interpreted as
product of empirical evidence, and not as concrete theoretical properties of
the method. Identiﬁcation of the latter requires more extensive systematic
studies that go beyond the scope of this paper.

In tables 3 and 4, we study the robustness of our results with respect
to the gap ∆t between pairs of data and with respect to noise in the ob-
servations of the system. These results fail to reveal a consistent pattern as
larger time-step sizes ∆t and larger noise corruption levels sometimes lead to
superior accuracy and other times to inferior. In the latter cases, the reasons

6

M

Scheme
Adams-Bashforth
Adams-Moulton
BDF

1

2

3

4

5

1.5e+00
8.8e-03
1.3e+00

3.1e-02
1.2e-02
8.8e-03

1.2e-01
1.6e-02
1.3e-02

4.3e-02
6.3e-03
1.4e-02

1.2e-02
1.1e-02
1.7e-02

Table 1: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the ﬁrst dynamic component x(t) integrated up to time t = 25, for diﬀerent
member families of the class of multistep methods, and diﬀerent number of steps M . Here,
the training data is assumed to be noise free, and the neural network architecture is kept
ﬁxed to have one hidden layer and 256 neurons.

M

Scheme
Adams-Bashforth
Adams-Moulton
BDF

1

2

3

4

5

1.5e+00
8.8e-03
1.3e+00

3.0e-02
1.0e-02
8.6e-03

9.7e-02
1.6e-02
9.9e-03

3.5e-02
5.8e-03
1.4e-02

1.2e-02
1.1e-02
1.5e-02

Table 2: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the second dynamic component y(t) integrated up to time t = 25, for
diﬀerent member families of the class of multistep methods, and diﬀerent number of steps
M . Here, the training data is assumed to be noise free, and the neural network architecture
is kept ﬁxed to have one hidden layer and 256 neurons.

are obvious, namely that a larger gap ∆t makes the approximation in time
less accurate, while too much noise is devastating because it would be harder
to distinguish between noise and the true dynamics. On the other hand, we
do observe some cases in which larger ∆t and noise levels may actually help.
In these cases, we believe that input noise can act as a regularization mech-
anism that increases the robustness of the model training procedure, similar
to how it has been previously proposed in the neural network literature (see
for e.g., denoising autoencoders [21]). Along the same lines, a bigger tem-
poral gap ∆t helps because it makes two consecutive time snapshots carry
more information simply because they are more dissimilar to one another.
On the contrary, if ∆t is too small, the importance of the neural network
becomes less and less pronounced as seen in equation (2), hence model train-
ing becomes infeasible. These empirical results indicate that there exists a
problem-dependent sweet spot for the admissible values of the time-step and
noise levels that can lead to the best predictive accuracy. Although one may

7

Table 3: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the ﬁrst dynamic component x(t) integrated up to time t = 25, for diﬀerent
noise magnitudes, and diﬀerent gap ∆t between pairs of snapshots. Here, we are employing
the trapezoidal time-stepping scheme and the neural network architecture is kept ﬁx to
have one hidden layer and 256 neurons.

noise

0.00% 0.01% 0.02%

5.7e-03
1.8e-02
3.8e-02
5.4e-02
8.3e-02

2.4e-02
1.1e-01
9.2e-02
4.0e-02
2.9e-01

2.2e-01
1.3e-01
7.8e-01
8.7e-01
9.2e-02

noise

0.00% 0.01% 0.02%

5.9e-03
1.7e-02
3.3e-02
5.2e-02
7.2e-02

2.1e-02
1.0e-01
9.0e-02
3.8e-02
2.6e-01

2.2e-01
1.1e-01
7.9e-01
8.7e-01
8.2e-02

∆t
0.01
0.02
0.03
0.04
0.05

∆t
0.01
0.02
0.03
0.04
0.05

Table 4: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the ﬁrst dynamic component y(t) integrated up to time t = 25, for diﬀerent
noise magnitudes, and diﬀerent gap ∆t between pairs of snapshots. Here, we are employing
the trapezoidal time-stepping scheme and the neural network architecture is kept ﬁx to
have one hidden layer and 256 neurons.

have no control over the noise corrupting the data, the temporal gap ∆t
could be treated as another hyper-parameter like the number of neurons and
hidden layers when setting up the neural network.

Finally, tables 5 and 6 study the robustness of our results with respect to
the neural network structure. For this case, more accurate results seem to
be obtained with increasing network depth, although increasing the network
width seems to have a negative aﬀect for more than 128 neurons per layer.
To fully quantify sensitivity with respect to network architecture a more
systematic study involving multiple data-sets is needed.

8

Table 5: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the ﬁrst dynamic component x(t) integrated up to time t = 25, for diﬀerent
neural network architectures. Here, the training data is assumed to be noise free, the time
step size is kept ﬁxed at ∆t = 0.01, and the number of Adams-Moulton steps is ﬁxed at
M = 1.

Layers
1
2
3

Layers
1
2
3

Neurons

64

128

256

9.8e-03
3.6e-03
3.4e-03

6.1e-03
1.2e-02
1.6e-02

3.7e-02
2.4e-02
4.2e-02

Neurons

64

128

256

7.2e-03
3.3e-03
3.0e-03

5.4e-03
9.1e-03
1.4e-02

3.5e-02
2.0e-02
3.7e-02

Table 6: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the second dynamic component y(t) integrated up to time t = 25, for
diﬀerent neural network architectures. Here, the training data is assumed to be noise free,
the time step size is kept ﬁxed at ∆t = 0.01, and the number of Adams-Moulton steps is
ﬁxed at M = 1.

3.2. Lorenz system

To explore the identiﬁcation of chaotic dynamics evolving on a ﬁnite

dimensional attractor, we consider the nonlinear Lorenz system [22]

˙x = 10(y − x),
˙y = x(28 − z) − y,
˙z = xy − (8/3)z.

(7)

We use [x0 y0 z0]T = [−8 7 27]T as initial condition and collect data from
t = 0 to t = 25 with a time-step size of ∆t = 0.01. The data are plotted in
ﬁgures 2 and 3. We employ a neural network with one hidden layer and 256
neurons to represent the nonlinear dynamics. As for the multistep scheme
(2) we use Adams-Moulton with M = 1 steps (i.e., the trapezoidal rule).
Upon training the neural network, we solve the identiﬁed system using the
same initial condition as the one above. As depicted in ﬁgure 2, the learned
system correctly captures the form of the attractor.

9

Figure 2: Lorenz System: The exact phase portrait of the Lorenz system (left panel) is
compared to the corresponding phase portrait of the learned dynamics (right panel).

The Lorenz system has a positive Lyapunov exponent, and small diﬀer-
ences between the exact and learned models grow exponentially, even though
the attractor remains intact. This behavior is evident in ﬁgure 3, as we com-
pare the exact versus the predicted trajectories. Small discrepancies due to
ﬁnite accuracy in the predicted dynamics lead to large errors in the fore-
casted time-series after t > 4, despite the fact that the bi-stable structure of
the attractor is well captured (see ﬁgure 2).

3.3. Fluid ﬂow behind a cylinder

In this example we collect data for the ﬂuid ﬂow past a cylinder (see ﬁg-
ure 4) at Reynolds number 100 using direct numerical simulations of the two
dimensional Navier-Stokes equations.
In particular, following the problem
setup presented in [23] and [24], we simulate the Navier-Stokes equations de-
scribing the two-dimensional ﬂuid ﬂow past a circular cylinder at Reynolds
number 100 using the Immersed Boundary Projection Method [25, 26]. This
approach utilizes a multi-domain scheme with four nested domains, each suc-
cessive grid being twice as large as the previous one. Length and time are
non-dimensionalized so that the cylinder has unit diameter and the ﬂow has
unit velocity. Data is collected on the ﬁnest domain with dimensions 9 × 4 at
a grid resolution of 449 × 199. The ﬂow solver uses a 3rd-order Runge Kutta

10

Figure 3: Lorenz System: The exact trajectories of the Lorenz systems is compared to the
corresponding trajectories of the learned dynamics. Solid blue lines represent the exact
dynamics while the dashed black lines demonstrate the learned dynamics.

integration scheme with a time step of t = 0.02, which has been veriﬁed to
yield well-resolved and converged ﬂow ﬁelds. After simulations converge to
steady periodic vortex shedding, ﬂow snapshots are saved every ∆t = 0.02.
We then reduce the dimension of the system by proper orthogonal decompo-
sition (POD) [27, 16]. The POD results in a hierarchy of orthonormal modes
that, when truncated, capture most of the energy of the original system for
the given rank truncation. The ﬁrst two most energetic POD modes capture
a signiﬁcant portion of the energy; the steady-state vortex shedding is a limit

11

Figure 4: Flow past a cylinder: A snapshot of the vorticity ﬁeld of a solution to the
Navier-Stokes equations for the ﬂuid ﬂow past a cylinder.

cycle in these coordinates [16]. An additional mode, called the shift mode, is
included to capture the transient dynamics connecting the unstable steady
state with the mean of the limit cycle [28]. The resulting POD coeﬃcients
are depicted in ﬁgure 5.

We employ a neural network with one hidden layer and 256 neurons to rep-
resent the nonlinear dynamics shown in ﬁgure 5. As for the linear multistep
scheme (2) we use Adams-Moulton with M = 1 steps (i.e., the trapezoidal
rule). Upon training the neural network, we solve the identiﬁed system. As
depicted in ﬁgure 5, the learned system correctly captures the form of the
dynamics and accurately reproduces the phase portrait, including both the
transient regime as well as the limit cycle attained once the ﬂow dynamics
converge to the well known K´arman vortex street.

3.4. Hopf bifurcation

Many real-world systems depend on parameters and, when the parameters
are varied, they may go through bifurcations. To illustrate the ability of our
method to identify parameterized dynamics, let us consider the Hopf normal
form

˙x = µx + y − x(x2 + y2),
˙y = −x + µy − y(x2 + y2).

(8)

12

Figure 5: Flow past a cylinder: The exact phase portrait of the cylinder wake trajectory
in reduced coordinates (left panel) is compared to the corresponding phase portrait of the
learned dynamics (right panel).

Our algorithm can be readily extended to encompass parameterized systems.
In particular, the system (8) can be equivalently written as

˙µ = 0,
˙x = µx + y − x(x2 + y2),
˙y = −x + µy − y(x2 + y2).

(9)

We collect data from the Hopf system (9) for various initial conditions corre-
sponding to diﬀerent parameter values for µ. The data is depicted in ﬁgure 6.
The identiﬁed parameterized dynamics is shown in ﬁgure 6 for a set of param-
eter values diﬀerent from the ones used during model training. The learned
system correctly captures the transition from the ﬁxed point for µ < 0 to the
limit cycle for µ > 0.

3.5. Glycolytic oscillator

As an example of complicated nonlinear dynamics typical of biological
systems, we simulate the glycolytic oscillator model presented in [29] and [16].
The model consists of ordinary diﬀerential equations for the concentrations

13

Figure 6: Hopf bifurcation: Training data from the Hopf system for various initial con-
ditions corresponding to diﬀerent parameter values for µ (left panel) is compared to the
corresponding phase portrait of the learned dynamics (right panel). It is worth highlight-
ing that the algorithm is tested on initial conditions diﬀerent from the ones used during
training.

of 7 biochemical species; i.e.,

dS1
dt
dS2
dt
dS3
dt
dS4
dt
dS5
dt
dS6
dt
dS7
dt

= J0 −

,

k1S1S6
1 + (S6/K1)q
k1S1S6
1 + (S6/K1)q

= 2

− k2S2(N − S5) − k6S2S5,

= k2S2(N − S5) − k3S3(N − S6),

= k3S3(A − S6) − k4S4S5 − κ(S4 − S7),

(10)

= k2S2(N − S5) − k4S4S5 − k6S2S5,

= −2

k1S1S6
1 + (S6/K1)q

= ψκ(S4 − S7) − kS7.

+ 2k3S3(A − S6) − k5S6,

14

The parameters of the model are chosen according to table 1 of [29]. As shown
in ﬁgure 7, data from a simulation of equation (10) are collected from t = 0
to t = 10 with a time-step size of ∆t = 0.01. We employ a neural network
with one hidden layer and 256 neurons to represent the nonlinear dynamics.
As for the multi-step scheme (2) we use Adams-Moulton with M = 1 steps
(i.e., the trapezoidal rule). Upon training the neural network, we solve the
identiﬁed system using the same initial condition as the ones used for the
exact system. As depicted in ﬁgure 7, the learned system correctly captures
the form of the dynamics.

4. Summary and Discussion

We have presented a machine learning approach for extracting nonlinear
dynamical systems from time-series data. The proposed algorithm lever-
ages the structure of well studied multi-step time-stepping schemes such as
Adams-Bashforth, Adams Moulton, and BDF families, to construct eﬃcient
algorithms for learning dynamical systems using deep neural networks. A
key property of the proposed approach is the use of multiple steps which
enables us to incorporate memory eﬀects in learning the temporal dynamics
and tackle problems with a nonlinear and non-Markovian dynamical struc-
ture. Speciﬁcally, the use of M steps allows us to decouple the regression
complexity due to several temporal lags, ultimately leading to a simpler
D-dimensional regression problem, as opposed to an (M × D)-dimensional
problem in the case of a brute force NARMAX or recurrent neural network
approaches. Although state-of-the-art results are presented for a diverse col-
lection of benchmark problems, there exist a series of open questions mandat-
ing further investigation. How could one handle a variable temporal gap ∆t,
i.e., irregularly sampled data in time? How would common techniques such as
batch normalization, drop out, and L1/L2 regularization enhance the robust-
ness of the proposed algorithm and mitigate the eﬀects of over-ﬁtting? How
could one incorporate partial knowledge of the dynamical system in cases
where certain interaction terms are already known? In terms of future work,
interesting directions include the application of convolutional architectures
[14] for mitigating the complexity associated with very high-dimensional in-
puts, as well as studying possible connections with recent studies linking deep
neural networks with numerical methods and dynamical systems [30, 31].

15

Figure 7: Glycolytic oscillator: Exact versus learned dynamics for random initial condi-
tions chosen from the ranges provided in table 2 of [29].

Acknowledgements

This work received support by the DARPA EQUiPS grant N66001-15-2-
4055 and the AFOSR grant FA9550-17-1-0013. All data and codes used in
this manuscript are publicly available on GitHub at https://github.com/
maziarraissi/MultistepNNs.

16

References

[1] R. Courant, D. Hilbert, Methods of Mathematical Physics, Volume 2:

Diﬀerential Equations, John Wiley & Sons, 2008.

[2] F. Michor, Y. Iwasa, M. A. Nowak, Dynamics of cancer progression,

Nature reviews cancer 4 (2004) 197–205.

[3] C. Castellano, S. Fortunato, V. Loreto, Statistical physics of social

dynamics, Reviews of modern physics 81 (2009) 591.

[4] M. Gavin, The stock market and exchange rate dynamics, Journal of

international money and ﬁnance 8 (1989) 181–200.

[5] L. Ljung, System identiﬁcation,
Springer, 1998, pp. 163–173.

in: Signal analysis and prediction,

[6] S. Chen, S. Billings, C. Cowan, P. Grant, Non-linear systems identi-
ﬁcation using radial basis functions, International Journal of Systems
Science 21 (1990) 2513–2539.

[7] A. Cochocki, R. Unbehauen, Neural networks for optimization and signal

processing, John Wiley & Sons, Inc., 1993.

[8] J. Kocijan, A. Girard, B. Banko, R. Murray-Smith, Dynamic systems
identiﬁcation with gaussian processes, Mathematical and Computer
Modelling of Dynamical Systems 11 (2005) 411–424.

[9] M. Raissi, P. Perdikaris, G. E. Karniadakis, Machine learning of linear
diﬀerential equations using Gaussian processes, Journal of Computa-
tional Physics 348 (2017) 683 – 693.

[10] M. Raissi, G. E. Karniadakis, Hidden physics models: Machine
arXiv preprint

learning of nonlinear partial diﬀerential equations,
arXiv:1708.00588 (2017).

[11] M. Raissi, P. Perdikaris, G. E. Karniadakis, Inferring solutions of dif-
ferential equations using noisy multi-ﬁdelity data, Journal of Computa-
tional Physics 335 (2017) 736–746.

17

[12] M. Raissi, P. Perdikaris, G. E. Karniadakis, Numerical Gaussian pro-
cesses for time-dependent and non-linear partial diﬀerential equations,
arXiv preprint arXiv:1703.10230 (2017).

[13] S. A. Billings, Nonlinear system identiﬁcation: NARMAX methods in
the time, frequency, and spatio-temporal domains, John Wiley & Sons,
2013.

[14] I. Goodfellow, Y. Bengio, A. Courville, Deep learning, MIT press, 2016.

[15] M. Schmidt, H. Lipson, Distilling free-form natural laws from experi-

mental data, science 324 (2009) 81–85.

[16] S. L. Brunton, J. L. Proctor, J. N. Kutz, Discovering governing equa-
tions from data by sparse identiﬁcation of nonlinear dynamical systems,
Proceedings of the National Academy of Sciences 113 (2016) 3932–3937.

[17] S. H. Rudy, S. L. Brunton, J. L. Proctor, J. N. Kutz, Data-driven
discovery of partial diﬀerential equations, Science Advances 3 (2017)
e1602614.

[18] A. Iserles, A ﬁrst course in the numerical analysis of diﬀerential equa-

tions, 44, Cambridge University Press, 2009.

[19] M. Raissi, P. Perdikaris, G. E. Karniadakis, Physics informed deep
learning (part ii): Data-driven discovery of nonlinear partial diﬀerential
equations, arXiv preprint arXiv:1711.10566 (2017).

[20] M. Raissi, P. Perdikaris, G. E. Karniadakis, Physics informed deep
learning (part i): Data-driven solutions of nonlinear partial diﬀerential
equations, arXiv preprint arXiv:1711.10561 (2017).

[21] P. Vincent, H. Larochelle, Y. Bengio, P.-A. Manzagol, Extracting and
composing robust features with denoising autoencoders, in: Proceedings
of the 25th international conference on Machine learning, ACM, pp.
1096–1103.

[22] E. N. Lorenz, Deterministic nonperiodic ﬂow, Journal of the atmospheric

sciences 20 (1963) 130–141.

18

[23] J. N. Kutz, S. L. Brunton, B. W. Brunton, J. L. Proctor, Dynamic
Mode Decomposition: Data-Driven Modeling of Complex Systems, vol-
ume 149, SIAM, 2016.

[24] S. H. Rudy, S. L. Brunton, J. L. Proctor, J. N. Kutz, Data-driven

discovery of partial diﬀerential equations, Science Advances 3 (2017).

[25] K. Taira, T. Colonius, The immersed boundary method: a projection
approach, Journal of Computational Physics 225 (2007) 2118–2137.

[26] T. Colonius, K. Taira, A fast immersed boundary method using a
nullspace approach and multi-domain far-ﬁeld boundary conditions,
Computer Methods in Applied Mechanics and Engineering 197 (2008)
2131–2146.

[27] G. Berkooz, P. Holmes, J. L. Lumley, The proper orthogonal decomposi-
tion in the analysis of turbulent ﬂows, Annual review of ﬂuid mechanics
25 (1993) 539–575.

[28] B. R. Noack, K. Afanasiev, M. MORZY ´NSKI, G. Tadmor, F. Thiele, A
hierarchy of low-dimensional models for the transient and post-transient
cylinder wake, Journal of Fluid Mechanics 497 (2003) 335–363.

[29] B. C. Daniels, I. Nemenman, Eﬃcient inference of parsimonious phe-
nomenological models of cellular dynamics using s-systems and alternat-
ing regression, PloS one 10 (2015) e0119821.

[30] B. Chang, L. Meng, E. Haber, F. Tung, D. Begert, Multi-level residual
networks from dynamical systems view, arXiv preprint arXiv:1710.10348
(2017).

[31] Y. Lu, A. Zhong, Q. Li, B. Dong, Beyond ﬁnite layer neural networks:
Bridging deep architectures and numerical diﬀerential equations, arXiv
preprint arXiv:1710.10121 (2017).

19

8
1
0
2
 
n
a
J
 
4
 
 
]
S
D
.
h
t
a
m

[
 
 
1
v
6
3
2
1
0
.
1
0
8
1
:
v
i
X
r
a

Multistep Neural Networks for Data-driven Discovery of
Nonlinear Dynamical Systems

Maziar Raissi1, Paris Perdikaris2, and George Em Karniadakis1

1Division of Applied Mathematics, Brown University,
Providence, RI, 02912, USA
2Department of Mechanical Engineering and Applied Mechanics,
University of Pennsylvania,
Philadelphia, PA, 19104, USA

Abstract

The process of transforming observed data into predictive mathematical
models of the physical world has always been paramount in science and en-
gineering. Although data is currently being collected at an ever-increasing
pace, devising meaningful models out of such observations in an automated
fashion still remains an open problem.
In this work, we put forth a ma-
chine learning approach for identifying nonlinear dynamical systems from
data. Speciﬁcally, we blend classical tools from numerical analysis, namely
the multi-step time-stepping schemes, with powerful nonlinear function ap-
proximators, namely deep neural networks, to distill the mechanisms that
govern the evolution of a given data-set. We test the eﬀectiveness of our
approach for several benchmark problems involving the identiﬁcation of com-
plex, nonlinear and chaotic dynamics, and we demonstrate how this allows us
to accurately learn the dynamics, forecast future states, and identify basins
of attraction. In particular, we study the Lorenz system, the ﬂuid ﬂow be-
hind a cylinder, the Hopf bifurcation, and the Glycoltic oscillator model as
an example of complicated nonlinear dynamics typical of biological systems.

Keywords:
Machine learning, Systems identiﬁcation, Reduced order modeling,
Data-driven forecasting, Lorenz system, Navier-Stokes

Preprint submitted to Journal Name

January 8, 2018

1. Introduction

Dynamical systems play a key role in shaping our understanding of the
physical world and in deﬁning our ability to predict the evolution of a given
process. From the simple swinging motion of a clock pendulum to the com-
plex ﬂow around an airplane wing, the mathematical modeling of dynamical
systems can yield a set of tools with which we can analyze the way the current
state of the system depends on the past, and predict the possible states we
may encounter in the future. Often such tools are precisely known, usually
coming in the form of diﬀerential equations that are derived from ﬁrst phys-
ical principles, such as the conservation of energy, mass, and momentum [1].
However, in many cases, the sheer complexity of a system can prohibit our
complete understanding and render a ﬁrst principles approach infeasible. In
this setting, one may be only able to postulate crude and potentially overly
simpliﬁed models based on a given a set of empirical observations (see e.g.,
models for tumor growth [2], social dynamics [3], and the stock market [4]).
In the present era of abundant data and advanced machine learning capa-
bilities, a natural question arises: can we automatically discover suﬃciently
sophisticated and accurate mathematical models of complex dynamical sys-
tems directly from data?

The answer to this question pertains to the well-established ﬁeld of sys-
tems identiﬁcation [5]. Discriminating between white-, gray-, and black-box
approaches, depending on whether a ﬁrst principles modeling approach is
fully, partially, or not admissible, systems identiﬁcation aims to devise math-
ematical models for predicting a future state of a system, given the evolution
of a set of previously observed or latent states. Speciﬁcally, in the con-
text of identifying nonlinear dynamics, there exist several deterministic and
probabilistic tools including radial basis functions [6], neural networks [7],
Gaussian processes [8, 9, 10, 11, 12], and nonlinear auto-regressive models
such as NARMAX [13] and recurrent neural networks [14]. A common theme
among all such methods is the pursuit of learning a nonlinear and potentially
multi-variate mapping f that predicts the future system states given a set
of data describing the present and past states. More recently, approaches
based on symbolic regression [15], sparse regression, and compressive sensing
[16, 17] were able to go beyond estimating a black-box approximation of the
dynamics given by f , and return more interpretable models that can uncover
the full parametric form of an underlying governing equation. However, in

2

order to obtain sparse representation of the dynamics, the aforementioned
approaches have to rely upon the nontrivial task of choosing “appropriate”
sets of basis functions. Consequently, investigating ways of incorporating
broader function search spaces is an important area of current and future
research.

In this work, we introduce a novel approach to nonlinear systems identiﬁ-
cation that combines the classical multistep family of time-stepping schemes
from numerical analysis [18] with deep neural networks.
Inspired by re-
cent developments in physics-informed deep learning [19, 20], we construct
structured nonlinear regression models that can discover the dynamic depen-
dencies in a given set of temporal data-snapshots, and return a closed form
model that can be subsequently used to forecast future states or identify
basins of attraction. In contrast to recent approaches to systems identiﬁca-
tion [16, 17], here we do not have to have direct access or approximations to
temporal gradients because the time derivatives are discretized using classi-
cal time-stepping rules. Moreover, we are using a richer family of function
approximators and consequently we do not have to commit to a particular
class of basis functions such as polynomials or sines and cosines. This comes
at the cost of losing interpretability of the learned dynamics. However, there
is nothing hindering the use of a particular class of basis functions and obtain
more interpretable equations.

This paper is structured as follows. In section 2 we provide a detailed
In section 3.1, we investigate the
overview of the proposed methodology.
performance of the proposed framework by applying our algorithm to the
two-dimensional damped harmonic oscillator. We then explore the identiﬁ-
cation of chaotic dynamics of the Lorenz system in section 3.2. As an example
of a high dimensional dynamical systems, in section 3.3, we study the Navier-
Stokes equations describing the ﬂuid ﬂow behind a cylinder. To illustrate the
ability of our method to identify parameterized dynamics, we consider the
Hopf normal form in section 3.4. As an example of complicated nonlinear
dynamics typical of biological systems, we explore the glycolytic oscillator
model in section 3.5. It should be highlighted that all of the examples con-
sidered in this work are inspired by the pioneering work of Brunton et. al.
[16]. Moreover, all data and codes used in this manuscript are publicly avail-
able on GitHub at https://github.com/maziarraissi/MultistepNNs.

3

2. Problem setup and solution methodology

Let us consider nonlinear dynamical systems of the form1

d
dt

x(t) = f (x(t)) ,

(1)

where the vector x(t) ∈ RD denotes the state of the system at time t and the
function f describes the evolution of the system. Given noisy measurements
of the state x(t) of the system at several time instances t1, t2, . . . , tN , our
goal is to determine the function f and consequently discover the underlying
dynamical system (1) from data. We proceed by applying the general form
of a linear multistep method with M steps to equation (1) and obtain

M
(cid:88)

m=0

[αmxn

m + ∆tβmf (xn
−

m)] = 0, n = M, . . . , N.
−

(2)

m) at time tn
m denotes the state of the system x(tn
−
−

Here, xn
m. Diﬀerent
−
choices for the parameters αm and βm result in speciﬁc schemes. For instance,
the trapezoidal rule

xn = xn
−

1 +

1)) , n = 1, . . . , N,
∆t (f (xn) + f (xn
−

(3)

1
2

corresponds to the case where M = 1, α0 = −1, α1 = 1, and β0 = β1 =
0.5. We proceed by placing a neural network prior on the function f . The
parameters of this neural network can be learned by minimizing the mean
squared error loss function

M SE :=

1
N − M + 1

N
(cid:88)

n=M

|yn|2,

(4)

1It is straightforward to generalize the dynamics to include parameterization, time
dependence, and forcing. In particular, parameterization, time dependence, and external
forcing or feedback control u(t) may be added to the vector ﬁeld according to

˙x = f (x, u, t; λ) ,

˙t = 1,

˙λ = 0.

4

yn :=

[αmxn
−

m + ∆tβmf (xn
−

m)] , n = M, . . . , N,

(5)

where

M
(cid:88)

m=0

is obtained from the multistep scheme (2).

3. Results

3.1. Two-dimensional damped oscillator

As a ﬁrst illustrative example, let us consider the two-dimensional damped

harmonic oscillator with cubic dynamics; i.e.,

˙x = −0.1 x3 + 2.0 y3,
˙y = −2.0 x3 − 0.1 y3.

(6)

We use [x0 y0]T = [2 0]T as initial condition and collect data from t = 0 to
t = 25 with a time-step size of ∆t = 0.01. The data are plotted in ﬁgure
1. We employ a neural network with one hidden layer and 256 neurons to
represent the nonlinear dynamics. As for the multistep scheme (2) we use
Adams-Moulton with M = 1 steps (i.e., the trapezoidal rule). Upon train-
ing the neural network, we solve the identiﬁed system using the same initial
condition as the one above. Figure 1 provides a qualitative assessment of
the accuracy in identifying the correct nonlinear dynamics. Speciﬁcally, by
comparing the exact and predicted trajectories of the system, as well as the
resulting phase portraits, we observe that the algorithm can correctly cap-
ture the dynamic evolution of the system.

To investigate the performance of the proposed work-ﬂow with respect
to diﬀerent linear multi-step methods, we have considered the three families
that are most commonly used in practice: Adams-Bashforth (AB) methods,
Adams-Moulton (AM) methods, and the backward diﬀerentiation formulas
(BDFs).
In tables 1 and 2, we report the relative L2 error between tra-
jectories of the exact and the identiﬁed systems for diﬀerent members of
the class of linear multi-step methods.
Interestingly, the Adams-Moulton
scheme seems to consistently return more accurate results compared to the
Adams-Bashforth and BDF approaches. One intuitive explanation for this

5

Figure 1: Harmonic Oscillator: Trajectories of the two-dimensional damped harmonic
oscillator with cubic dynamics are depicted in the left panel while the corresponding phase
portrait is plotted in the right panel. Solid colored lines represent the exact dynamics while
the dashed black lines demonstrate the learned dynamics. The identiﬁed system correctly
captures the form of the dynamics and accurately reproduces the phase portrait.

behavior stems from a closer inspection of equation 2. Speciﬁcally, the ar-
rangement of the resulting terms for the Adams-Moulton schemes leads to a
higher throughput of training data ﬂowing through the neural network during
model training as compared to the Adams-Bashforth and BDF cases. This
helps regularize the neural network and eventually achieve a better calibra-
tion during training. Also, out of the Adams-Moulton family, the trapezoidal
rule seems to work the best in practice perhaps due to its superior stability
properties [18]. These performance characteristics should be interpreted as
product of empirical evidence, and not as concrete theoretical properties of
the method. Identiﬁcation of the latter requires more extensive systematic
studies that go beyond the scope of this paper.

In tables 3 and 4, we study the robustness of our results with respect
to the gap ∆t between pairs of data and with respect to noise in the ob-
servations of the system. These results fail to reveal a consistent pattern as
larger time-step sizes ∆t and larger noise corruption levels sometimes lead to
superior accuracy and other times to inferior. In the latter cases, the reasons

6

M

Scheme
Adams-Bashforth
Adams-Moulton
BDF

1

2

3

4

5

1.5e+00
8.8e-03
1.3e+00

3.1e-02
1.2e-02
8.8e-03

1.2e-01
1.6e-02
1.3e-02

4.3e-02
6.3e-03
1.4e-02

1.2e-02
1.1e-02
1.7e-02

Table 1: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the ﬁrst dynamic component x(t) integrated up to time t = 25, for diﬀerent
member families of the class of multistep methods, and diﬀerent number of steps M . Here,
the training data is assumed to be noise free, and the neural network architecture is kept
ﬁxed to have one hidden layer and 256 neurons.

M

Scheme
Adams-Bashforth
Adams-Moulton
BDF

1

2

3

4

5

1.5e+00
8.8e-03
1.3e+00

3.0e-02
1.0e-02
8.6e-03

9.7e-02
1.6e-02
9.9e-03

3.5e-02
5.8e-03
1.4e-02

1.2e-02
1.1e-02
1.5e-02

Table 2: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the second dynamic component y(t) integrated up to time t = 25, for
diﬀerent member families of the class of multistep methods, and diﬀerent number of steps
M . Here, the training data is assumed to be noise free, and the neural network architecture
is kept ﬁxed to have one hidden layer and 256 neurons.

are obvious, namely that a larger gap ∆t makes the approximation in time
less accurate, while too much noise is devastating because it would be harder
to distinguish between noise and the true dynamics. On the other hand, we
do observe some cases in which larger ∆t and noise levels may actually help.
In these cases, we believe that input noise can act as a regularization mech-
anism that increases the robustness of the model training procedure, similar
to how it has been previously proposed in the neural network literature (see
for e.g., denoising autoencoders [21]). Along the same lines, a bigger tem-
poral gap ∆t helps because it makes two consecutive time snapshots carry
more information simply because they are more dissimilar to one another.
On the contrary, if ∆t is too small, the importance of the neural network
becomes less and less pronounced as seen in equation (2), hence model train-
ing becomes infeasible. These empirical results indicate that there exists a
problem-dependent sweet spot for the admissible values of the time-step and
noise levels that can lead to the best predictive accuracy. Although one may

7

Table 3: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the ﬁrst dynamic component x(t) integrated up to time t = 25, for diﬀerent
noise magnitudes, and diﬀerent gap ∆t between pairs of snapshots. Here, we are employing
the trapezoidal time-stepping scheme and the neural network architecture is kept ﬁx to
have one hidden layer and 256 neurons.

noise

0.00% 0.01% 0.02%

5.7e-03
1.8e-02
3.8e-02
5.4e-02
8.3e-02

2.4e-02
1.1e-01
9.2e-02
4.0e-02
2.9e-01

2.2e-01
1.3e-01
7.8e-01
8.7e-01
9.2e-02

noise

0.00% 0.01% 0.02%

5.9e-03
1.7e-02
3.3e-02
5.2e-02
7.2e-02

2.1e-02
1.0e-01
9.0e-02
3.8e-02
2.6e-01

2.2e-01
1.1e-01
7.9e-01
8.7e-01
8.2e-02

∆t
0.01
0.02
0.03
0.04
0.05

∆t
0.01
0.02
0.03
0.04
0.05

Table 4: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the ﬁrst dynamic component y(t) integrated up to time t = 25, for diﬀerent
noise magnitudes, and diﬀerent gap ∆t between pairs of snapshots. Here, we are employing
the trapezoidal time-stepping scheme and the neural network architecture is kept ﬁx to
have one hidden layer and 256 neurons.

have no control over the noise corrupting the data, the temporal gap ∆t
could be treated as another hyper-parameter like the number of neurons and
hidden layers when setting up the neural network.

Finally, tables 5 and 6 study the robustness of our results with respect to
the neural network structure. For this case, more accurate results seem to
be obtained with increasing network depth, although increasing the network
width seems to have a negative aﬀect for more than 128 neurons per layer.
To fully quantify sensitivity with respect to network architecture a more
systematic study involving multiple data-sets is needed.

8

Table 5: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the ﬁrst dynamic component x(t) integrated up to time t = 25, for diﬀerent
neural network architectures. Here, the training data is assumed to be noise free, the time
step size is kept ﬁxed at ∆t = 0.01, and the number of Adams-Moulton steps is ﬁxed at
M = 1.

Layers
1
2
3

Layers
1
2
3

Neurons

64

128

256

9.8e-03
3.6e-03
3.4e-03

6.1e-03
1.2e-02
1.6e-02

3.7e-02
2.4e-02
4.2e-02

Neurons

64

128

256

7.2e-03
3.3e-03
3.0e-03

5.4e-03
9.1e-03
1.4e-02

3.5e-02
2.0e-02
3.7e-02

Table 6: Harmonic Oscillator: Relative L2 error between the predicted and the exact
trajectory for the second dynamic component y(t) integrated up to time t = 25, for
diﬀerent neural network architectures. Here, the training data is assumed to be noise free,
the time step size is kept ﬁxed at ∆t = 0.01, and the number of Adams-Moulton steps is
ﬁxed at M = 1.

3.2. Lorenz system

To explore the identiﬁcation of chaotic dynamics evolving on a ﬁnite

dimensional attractor, we consider the nonlinear Lorenz system [22]

˙x = 10(y − x),
˙y = x(28 − z) − y,
˙z = xy − (8/3)z.

(7)

We use [x0 y0 z0]T = [−8 7 27]T as initial condition and collect data from
t = 0 to t = 25 with a time-step size of ∆t = 0.01. The data are plotted in
ﬁgures 2 and 3. We employ a neural network with one hidden layer and 256
neurons to represent the nonlinear dynamics. As for the multistep scheme
(2) we use Adams-Moulton with M = 1 steps (i.e., the trapezoidal rule).
Upon training the neural network, we solve the identiﬁed system using the
same initial condition as the one above. As depicted in ﬁgure 2, the learned
system correctly captures the form of the attractor.

9

Figure 2: Lorenz System: The exact phase portrait of the Lorenz system (left panel) is
compared to the corresponding phase portrait of the learned dynamics (right panel).

The Lorenz system has a positive Lyapunov exponent, and small diﬀer-
ences between the exact and learned models grow exponentially, even though
the attractor remains intact. This behavior is evident in ﬁgure 3, as we com-
pare the exact versus the predicted trajectories. Small discrepancies due to
ﬁnite accuracy in the predicted dynamics lead to large errors in the fore-
casted time-series after t > 4, despite the fact that the bi-stable structure of
the attractor is well captured (see ﬁgure 2).

3.3. Fluid ﬂow behind a cylinder

In this example we collect data for the ﬂuid ﬂow past a cylinder (see ﬁg-
ure 4) at Reynolds number 100 using direct numerical simulations of the two
dimensional Navier-Stokes equations.
In particular, following the problem
setup presented in [23] and [24], we simulate the Navier-Stokes equations de-
scribing the two-dimensional ﬂuid ﬂow past a circular cylinder at Reynolds
number 100 using the Immersed Boundary Projection Method [25, 26]. This
approach utilizes a multi-domain scheme with four nested domains, each suc-
cessive grid being twice as large as the previous one. Length and time are
non-dimensionalized so that the cylinder has unit diameter and the ﬂow has
unit velocity. Data is collected on the ﬁnest domain with dimensions 9 × 4 at
a grid resolution of 449 × 199. The ﬂow solver uses a 3rd-order Runge Kutta

10

Figure 3: Lorenz System: The exact trajectories of the Lorenz systems is compared to the
corresponding trajectories of the learned dynamics. Solid blue lines represent the exact
dynamics while the dashed black lines demonstrate the learned dynamics.

integration scheme with a time step of t = 0.02, which has been veriﬁed to
yield well-resolved and converged ﬂow ﬁelds. After simulations converge to
steady periodic vortex shedding, ﬂow snapshots are saved every ∆t = 0.02.
We then reduce the dimension of the system by proper orthogonal decompo-
sition (POD) [27, 16]. The POD results in a hierarchy of orthonormal modes
that, when truncated, capture most of the energy of the original system for
the given rank truncation. The ﬁrst two most energetic POD modes capture
a signiﬁcant portion of the energy; the steady-state vortex shedding is a limit

11

Figure 4: Flow past a cylinder: A snapshot of the vorticity ﬁeld of a solution to the
Navier-Stokes equations for the ﬂuid ﬂow past a cylinder.

cycle in these coordinates [16]. An additional mode, called the shift mode, is
included to capture the transient dynamics connecting the unstable steady
state with the mean of the limit cycle [28]. The resulting POD coeﬃcients
are depicted in ﬁgure 5.

We employ a neural network with one hidden layer and 256 neurons to rep-
resent the nonlinear dynamics shown in ﬁgure 5. As for the linear multistep
scheme (2) we use Adams-Moulton with M = 1 steps (i.e., the trapezoidal
rule). Upon training the neural network, we solve the identiﬁed system. As
depicted in ﬁgure 5, the learned system correctly captures the form of the
dynamics and accurately reproduces the phase portrait, including both the
transient regime as well as the limit cycle attained once the ﬂow dynamics
converge to the well known K´arman vortex street.

3.4. Hopf bifurcation

Many real-world systems depend on parameters and, when the parameters
are varied, they may go through bifurcations. To illustrate the ability of our
method to identify parameterized dynamics, let us consider the Hopf normal
form

˙x = µx + y − x(x2 + y2),
˙y = −x + µy − y(x2 + y2).

(8)

12

Figure 5: Flow past a cylinder: The exact phase portrait of the cylinder wake trajectory
in reduced coordinates (left panel) is compared to the corresponding phase portrait of the
learned dynamics (right panel).

Our algorithm can be readily extended to encompass parameterized systems.
In particular, the system (8) can be equivalently written as

˙µ = 0,
˙x = µx + y − x(x2 + y2),
˙y = −x + µy − y(x2 + y2).

(9)

We collect data from the Hopf system (9) for various initial conditions corre-
sponding to diﬀerent parameter values for µ. The data is depicted in ﬁgure 6.
The identiﬁed parameterized dynamics is shown in ﬁgure 6 for a set of param-
eter values diﬀerent from the ones used during model training. The learned
system correctly captures the transition from the ﬁxed point for µ < 0 to the
limit cycle for µ > 0.

3.5. Glycolytic oscillator

As an example of complicated nonlinear dynamics typical of biological
systems, we simulate the glycolytic oscillator model presented in [29] and [16].
The model consists of ordinary diﬀerential equations for the concentrations

13

Figure 6: Hopf bifurcation: Training data from the Hopf system for various initial con-
ditions corresponding to diﬀerent parameter values for µ (left panel) is compared to the
corresponding phase portrait of the learned dynamics (right panel). It is worth highlight-
ing that the algorithm is tested on initial conditions diﬀerent from the ones used during
training.

of 7 biochemical species; i.e.,

dS1
dt
dS2
dt
dS3
dt
dS4
dt
dS5
dt
dS6
dt
dS7
dt

= J0 −

,

k1S1S6
1 + (S6/K1)q
k1S1S6
1 + (S6/K1)q

= 2

− k2S2(N − S5) − k6S2S5,

= k2S2(N − S5) − k3S3(N − S6),

= k3S3(A − S6) − k4S4S5 − κ(S4 − S7),

(10)

= k2S2(N − S5) − k4S4S5 − k6S2S5,

= −2

k1S1S6
1 + (S6/K1)q

= ψκ(S4 − S7) − kS7.

+ 2k3S3(A − S6) − k5S6,

14

The parameters of the model are chosen according to table 1 of [29]. As shown
in ﬁgure 7, data from a simulation of equation (10) are collected from t = 0
to t = 10 with a time-step size of ∆t = 0.01. We employ a neural network
with one hidden layer and 256 neurons to represent the nonlinear dynamics.
As for the multi-step scheme (2) we use Adams-Moulton with M = 1 steps
(i.e., the trapezoidal rule). Upon training the neural network, we solve the
identiﬁed system using the same initial condition as the ones used for the
exact system. As depicted in ﬁgure 7, the learned system correctly captures
the form of the dynamics.

4. Summary and Discussion

We have presented a machine learning approach for extracting nonlinear
dynamical systems from time-series data. The proposed algorithm lever-
ages the structure of well studied multi-step time-stepping schemes such as
Adams-Bashforth, Adams Moulton, and BDF families, to construct eﬃcient
algorithms for learning dynamical systems using deep neural networks. A
key property of the proposed approach is the use of multiple steps which
enables us to incorporate memory eﬀects in learning the temporal dynamics
and tackle problems with a nonlinear and non-Markovian dynamical struc-
ture. Speciﬁcally, the use of M steps allows us to decouple the regression
complexity due to several temporal lags, ultimately leading to a simpler
D-dimensional regression problem, as opposed to an (M × D)-dimensional
problem in the case of a brute force NARMAX or recurrent neural network
approaches. Although state-of-the-art results are presented for a diverse col-
lection of benchmark problems, there exist a series of open questions mandat-
ing further investigation. How could one handle a variable temporal gap ∆t,
i.e., irregularly sampled data in time? How would common techniques such as
batch normalization, drop out, and L1/L2 regularization enhance the robust-
ness of the proposed algorithm and mitigate the eﬀects of over-ﬁtting? How
could one incorporate partial knowledge of the dynamical system in cases
where certain interaction terms are already known? In terms of future work,
interesting directions include the application of convolutional architectures
[14] for mitigating the complexity associated with very high-dimensional in-
puts, as well as studying possible connections with recent studies linking deep
neural networks with numerical methods and dynamical systems [30, 31].

15

Figure 7: Glycolytic oscillator: Exact versus learned dynamics for random initial condi-
tions chosen from the ranges provided in table 2 of [29].

Acknowledgements

This work received support by the DARPA EQUiPS grant N66001-15-2-
4055 and the AFOSR grant FA9550-17-1-0013. All data and codes used in
this manuscript are publicly available on GitHub at https://github.com/
maziarraissi/MultistepNNs.

16

References

[1] R. Courant, D. Hilbert, Methods of Mathematical Physics, Volume 2:

Diﬀerential Equations, John Wiley & Sons, 2008.

[2] F. Michor, Y. Iwasa, M. A. Nowak, Dynamics of cancer progression,

Nature reviews cancer 4 (2004) 197–205.

[3] C. Castellano, S. Fortunato, V. Loreto, Statistical physics of social

dynamics, Reviews of modern physics 81 (2009) 591.

[4] M. Gavin, The stock market and exchange rate dynamics, Journal of

international money and ﬁnance 8 (1989) 181–200.

[5] L. Ljung, System identiﬁcation,
Springer, 1998, pp. 163–173.

in: Signal analysis and prediction,

[6] S. Chen, S. Billings, C. Cowan, P. Grant, Non-linear systems identi-
ﬁcation using radial basis functions, International Journal of Systems
Science 21 (1990) 2513–2539.

[7] A. Cochocki, R. Unbehauen, Neural networks for optimization and signal

processing, John Wiley & Sons, Inc., 1993.

[8] J. Kocijan, A. Girard, B. Banko, R. Murray-Smith, Dynamic systems
identiﬁcation with gaussian processes, Mathematical and Computer
Modelling of Dynamical Systems 11 (2005) 411–424.

[9] M. Raissi, P. Perdikaris, G. E. Karniadakis, Machine learning of linear
diﬀerential equations using Gaussian processes, Journal of Computa-
tional Physics 348 (2017) 683 – 693.

[10] M. Raissi, G. E. Karniadakis, Hidden physics models: Machine
arXiv preprint

learning of nonlinear partial diﬀerential equations,
arXiv:1708.00588 (2017).

[11] M. Raissi, P. Perdikaris, G. E. Karniadakis, Inferring solutions of dif-
ferential equations using noisy multi-ﬁdelity data, Journal of Computa-
tional Physics 335 (2017) 736–746.

17

[12] M. Raissi, P. Perdikaris, G. E. Karniadakis, Numerical Gaussian pro-
cesses for time-dependent and non-linear partial diﬀerential equations,
arXiv preprint arXiv:1703.10230 (2017).

[13] S. A. Billings, Nonlinear system identiﬁcation: NARMAX methods in
the time, frequency, and spatio-temporal domains, John Wiley & Sons,
2013.

[14] I. Goodfellow, Y. Bengio, A. Courville, Deep learning, MIT press, 2016.

[15] M. Schmidt, H. Lipson, Distilling free-form natural laws from experi-

mental data, science 324 (2009) 81–85.

[16] S. L. Brunton, J. L. Proctor, J. N. Kutz, Discovering governing equa-
tions from data by sparse identiﬁcation of nonlinear dynamical systems,
Proceedings of the National Academy of Sciences 113 (2016) 3932–3937.

[17] S. H. Rudy, S. L. Brunton, J. L. Proctor, J. N. Kutz, Data-driven
discovery of partial diﬀerential equations, Science Advances 3 (2017)
e1602614.

[18] A. Iserles, A ﬁrst course in the numerical analysis of diﬀerential equa-

tions, 44, Cambridge University Press, 2009.

[19] M. Raissi, P. Perdikaris, G. E. Karniadakis, Physics informed deep
learning (part ii): Data-driven discovery of nonlinear partial diﬀerential
equations, arXiv preprint arXiv:1711.10566 (2017).

[20] M. Raissi, P. Perdikaris, G. E. Karniadakis, Physics informed deep
learning (part i): Data-driven solutions of nonlinear partial diﬀerential
equations, arXiv preprint arXiv:1711.10561 (2017).

[21] P. Vincent, H. Larochelle, Y. Bengio, P.-A. Manzagol, Extracting and
composing robust features with denoising autoencoders, in: Proceedings
of the 25th international conference on Machine learning, ACM, pp.
1096–1103.

[22] E. N. Lorenz, Deterministic nonperiodic ﬂow, Journal of the atmospheric

sciences 20 (1963) 130–141.

18

[23] J. N. Kutz, S. L. Brunton, B. W. Brunton, J. L. Proctor, Dynamic
Mode Decomposition: Data-Driven Modeling of Complex Systems, vol-
ume 149, SIAM, 2016.

[24] S. H. Rudy, S. L. Brunton, J. L. Proctor, J. N. Kutz, Data-driven

discovery of partial diﬀerential equations, Science Advances 3 (2017).

[25] K. Taira, T. Colonius, The immersed boundary method: a projection
approach, Journal of Computational Physics 225 (2007) 2118–2137.

[26] T. Colonius, K. Taira, A fast immersed boundary method using a
nullspace approach and multi-domain far-ﬁeld boundary conditions,
Computer Methods in Applied Mechanics and Engineering 197 (2008)
2131–2146.

[27] G. Berkooz, P. Holmes, J. L. Lumley, The proper orthogonal decomposi-
tion in the analysis of turbulent ﬂows, Annual review of ﬂuid mechanics
25 (1993) 539–575.

[28] B. R. Noack, K. Afanasiev, M. MORZY ´NSKI, G. Tadmor, F. Thiele, A
hierarchy of low-dimensional models for the transient and post-transient
cylinder wake, Journal of Fluid Mechanics 497 (2003) 335–363.

[29] B. C. Daniels, I. Nemenman, Eﬃcient inference of parsimonious phe-
nomenological models of cellular dynamics using s-systems and alternat-
ing regression, PloS one 10 (2015) e0119821.

[30] B. Chang, L. Meng, E. Haber, F. Tung, D. Begert, Multi-level residual
networks from dynamical systems view, arXiv preprint arXiv:1710.10348
(2017).

[31] Y. Lu, A. Zhong, Q. Li, B. Dong, Beyond ﬁnite layer neural networks:
Bridging deep architectures and numerical diﬀerential equations, arXiv
preprint arXiv:1710.10121 (2017).

19

