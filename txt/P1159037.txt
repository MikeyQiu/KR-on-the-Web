9
1
0
2
 
r
p
A
 
3
2
 
 
]
L
C
.
s
c
[
 
 
2
v
0
5
2
5
0
.
1
1
8
1
:
v
i
X
r
a

MODALITY ATTENTION FOR END-TO-END AUDIO-VISUAL SPEECH RECOGNITION

Pan Zhou1, Wenwen Yang2, Wei Chen2, Yanfeng Wang2, Jia Jia1

1Department of Computer Science and Technology, Tsinghua University, Beijing, P.R.China
2Voice Interaction Technology Center, Sogou Inc., Beijing, P.R.China
{zh-pan,jjia}@mail.tsinghua.edu.cn, {yangwenwen,chenweibj8871,wangyanfeng}@sogou-inc.com

ABSTRACT

Audio-visual speech recognition (AVSR) system is thought to be one
of the most promising solutions for robust speech recognition, es-
pecially in noisy environment.
In this paper, we propose a novel
multimodal attention based method for audio-visual speech recogni-
tion which could automatically learn the fused representation from
both modalities based on their importance. Our method is realized
using state-of-the-art sequence-to-sequence (Seq2seq) architectures.
Experimental results show that relative improvements from 2% up
to 36% over the auditory modality alone are obtained depending
on the different signal-to-noise-ratio (SNR). Compared to the tra-
ditional feature concatenation methods, our proposed approach can
achieve better recognition performance under both clean and noisy
conditions. We believe modality attention based end-to-end method
can be easily generalized to other multimodal tasks with correlated
information.

Index Terms— multimodal attention, audio-visual speech

recognition, lipreading, sequence-to-sequence

1. INTRODUCTION

Artiﬁcial intelligence attracts more and more attention in recent
years. Man-machine interaction interface for smart devices is nec-
essary. Speech is the most natural and convenient way of communi-
cation between people. Consequently, automatic speech recognition
(ASR) is considered to be the ﬁrst choice for effective man-machine
interaction. In tasks of some quiet environment, recent state-of-the-
art ASR systems [1] are capable of reaching a recognition accuracy
above 95%.
It is recognized that humans understand speech not
only by listening but also by considering visual cues of lips and face
[2]. So speech interaction is multimodal in nature although we don’t
rely much on visual information. This is probably because auditory
modality contains most of the useful information that is enough for
our understanding of other people. The visual modality becomes
important when the acoustic signal is distorted by noise. AVSR aims
to use visual information derived from lip motions to complement
corrupted audio speech signal.

Traditional audio based ASR system is made up of acoustic
model (AM), language model (LM), pronunciation model and de-
coder. These ASR systems become more and more accurate after
hidden Markov model (HMM) had been used for speech recognition.
This progress can be attribute to not only the appropriate acoustic
feature, such as Mel-frequency cepstral coefﬁcients (MFCC) and ﬁl-
ter banks, but also the more expressive model like Gaussian mixture
model (GMM) and deep neural networks (DNN). Recurrent neural
network (RNN), such as long short term memory (LSTM) handles
temporal dependency by different gates, which makes it more ap-
propriate for acoustic modeling. Although DNN and LSTM boost

speech recognition accuracy a lot, ASR is still a complex system
and need to tune each component separately. There is a favor in
end-to-end ASR approach recent years which tries to combine all
components into a single neural network and optimize them jointly.
Connectionist temporal classiﬁcation (CTC) [3, 4], RNN transducer
(RNN-T) [5, 6, 7] and attention based encoder-decoder [8, 9, 10] are
the three main approaches. Such end-to-end systems alleviate effect
of HMM and output meaningful units like phones and characters di-
rectly. The work of [1] shows that attention based listen, attend and
spell (LAS) model can exceed traditional NN-HMM hybrid systems
in speech tasks like dictation and voice search.

Understanding speech from visual signal alone, i.e.

lipread-
ing, has been of interest for decades. Many researchers dedicated
to extract powerful visual features for precise lipreading models.
These feature extraction approaches include top-down approach and
bottom-up approach. The former one uses priori lip shape repre-
sentation within a model, such as active shape models (ASMs) [11]
and active appearance models (AAMs) [12], and extracts model-
based features. The latter one estimates visual features from images
directly by discrete cosine transform (DCT), principal component
analysis (PCA) and discrete wavelet transform (DWT) etc [13, 14].
Thanks to the development in neural networks recent years, learn-
ing visual feature representation automatically by supervised train-
ing of convolutional neural networks (CNN) or LSTM has improved
lipreading signiﬁcantly [15, 16, 17, 18]. After designing or learning
visual features, GMM-HMM based model or softmax classiﬁcation
model is trained to predict isolated words or phonemes been spo-
ken. Just like the development of ASR, LipNet [19] utilizes CNN
and LSTM with CTC loss to train a neural net entirely end-to-end
and operates sentence level lipreading. Another representative work
is [20] which proposed watch, attend and spell (WAS) to transcribe
large vocabulary visual speech at the character level.

The work of [21] proposed to use lipreading to enhance ASR. In-
tegrating both auditory and visual information simultaneously into a
single model for AVSR has been a challenge. One reason is lipread-
ing is indeed a more difﬁcult problem because human lip movements
carry less information than speech. Another reason is the demand for
an effective fusion strategy of two correlated but inherently different
data streams. Some of the earlier audio-visual fusion methods are
reviewed in [22, 23]. Noda [24] makes use of CNN to extract visual
feature and combines it with audio feature by a multi-stream HMM.
Feature fusion and then using a one stream model is the dominant ap-
proach [25, 20, 26] in AVSR. [26] correlates every frame of acoustic
feature with visual context feature acquired by cross modality atten-
tion. The correlated feature is then decoded by an attention based
decoder. The watch, listen, attend and spell (WLAS) model [20]
concatenates two context vectors obtained by attending over indi-
vidual modality to predict the output units. However, as pointed out
by [23] concatenation of features does not explicitly make model to

learn stream reliabilities. Furthermore, WLAS is observed to be-
come over-reliant on the audio modality and is hard to train well.
Consequently, a regularization technique where one of the streams
is randomly dropped during training is applied.

In this work, we propose a novel multi-modality attention
method to integrate information from audio and video for audio-
visual speech recognition. The key contribution of our approach
is that we use an additional attention mechanism that enables the
model to automatically adjust its modality attention to a more re-
liable input modality. Furthermore, by using an LSTM to model
temporal variability for each modality, the attention of different
modality can change over time. We evaluate the use of modality
attention mechanism in WLAS modal architecture. Experimental
results demonstrate that our modality attention equipped end-to-end
system can outperform the audio-only system up to 36% relative im-
provement in 0dB SNR, showing the effectiveness of our proposal.
The remainder of the paper is structured as follows. We present
the multimodal attention approach in details in Section 2. Experi-
mental results are given in Section 3 and the paper is concluded with
our ﬁndings and future work in Section 4.

2. END-TO-END MULTIMODAL ATTENTION AVSR

In this section, a brief review of attention based Seq2seq model is
ﬁrst given. We propose modality attention mechanism in general
form in subsection 2.2 followed by integrating modality attention in
Seq2Seq model in subsection 2.3. Related works are discussed in
subsection 2.4.

2.1. End-to-end Model

The attention based Seq2seq architecture [8] consists of a sequence
encoder, a sequence decoder and an attender. The encoder is usually
based on an RNN which takes input as a sequence of feature vectors
x = (x1, x2, . . . , xT ), generating higher level representations h =
(h1, h2, . . . , hU ) and a ﬁnal state which represents the summary of
input sequence. The hidden representation of encoder is also known
as memory. The ﬁnal state is used to initialize the RNN decoder for
predicting the output units. Because the encoding process tends to be
lossy for long input sequences, an attention mechanism is introduced
to automatically select the most relevant information from encoder
memory to help the decoder to predict accurate unit at each decoding
step.

h = Encoder(x)
si = DecoderRN N (si−1, yi−1, ci−1)
ei,u = Energy(si, hu) = V T tanh(Whhu + Wssi + b)

αi,u =

exp(ei,u)
Pu′ exp(ei,u′)
αi,uhu

ci = X
u
P (yi|x, y<i) = DecoderOut(si, ci)

Speciﬁcally, at decoding step i, the attender typically ﬁnishes in
generating a weighted sum of the memory h, which is the so called
context vector ci. The weights of different encoder memory are com-
puted by their correlation score with decoder hidden state si. In the
end, ci and si is concatenated to form a context-aware state vector
for the output layer to generate an output. The whole process can
be summarized into Eq. (1)-(6), where Wh, Ws, V and b are train-
able parameters. Eq. (3)-(5) tells how the attender works which is
usually called content based attention. All the parameters of this

Seq2seq model are learned by minimizing the cross entropy (CE)
loss between the predict distribution and the ground truth distribu-
tion.

According to the modality of input sequence, LAS and WAS is
used for auditory and visual sequence respectively. WAS usually uti-
lizes multiple CNN layers at the bottom of encoder to extract visual
features from raw pixels. In order to combine information from both
audio and video, WLAS [20] uses two separate attenders to attend
over the outputs of listener and watcher respectively. The acquired
auditory context vector and visual context vector are then concate-
nated with the decoder state to give an output probability distribu-
tion.

2.2. Modality attention mechanism

The proposed modality attention mechanism fuses input from mul-
tiple modality into a single representation by weighted summing the
information from individual modalities.

Considering that there is a multimodal setup with m = 1, . . . , M
input modalities. We suppose the input feature from each modality
has the same length t = 1, . . . , T and is a D-dimensional feature
t ∈ RD. The fusion process can be summarized as follows:
vector f m

t = Z(f m
zm

1...t)

αm

t =

exp(zm
t )
j=1 exp(zj
M
t )

P

f M
t =

t f m
αm
t

M

X
m=1

(7)

(8)

(9)

(1)

(2)

(3)

(4)

(5)

(6)

m=1 αm

Eq. (7) represents the scoring function Z which generates scores
t ∈ R1 based on the features of modality m for time step t. Then
zm
a softmax operation is performed over the scores z1
t , . . . , zM
as in
t
t ∈ R1. Obviously
(8) to get modality attention weights αm
Eq.
M
t = 1. Finally in Eq. (9), a weighted sum is calculated on
to obtain the fused feature representation

P
the individual feature f m
t
f M
for time step t.
t
Comparing Eq. (7)-(9) with content based attention in Eq. (3)-
(5), we could found they are similar in attention weights calculation
and feature fusion method. At each step, the scoring function out-
puts a score for each modality m while the energy function in Eq.
(3) generates a score for each time step u. If we view features from
different modalities as features from different time steps, modality
attention is the similar as content based attention. The only differ-
ence maybe modality feature varies at different attention step while
hidden feature h from encoder keeps constant.

The scoring function Z can be any form and here we select neu-
ral networks to model it. In our experiments, we utilize a network
consists of a layer of LSTM and an output layer with one node to
calculate zm
t :

Z(f m

1...t) = σ(W · LST M (f m

1...t) + b)

(10)

where W , b is the parameters and σ is the sigmoid function. Select-
ing LSTM as the scoring function can make it to learn the temporal
variability of each modality thus make the modality attender con-
sider past history to decide which modality is focused on more heav-
ily at current fusion step. This is more reliable than feed forward
network that only consider the current feature.

2.3. Modality attention for AVSR

As mentioned in Section 1, AVSR is a two-modal task. Auditory and
visual information is commonly concatenated for the ASR system.
In AVSR, the general form of modality attention mentioned above
can be used to generate a merged representation which can then be
used to train a neural network. However, the frame rates of audio and
video are usually different. Up sampling of video or down sampling
of audio needs to perform to make sure the feature lengths are iden-
tical in order to use modality attention. We propose to use modality
attention in a WLAS end-to-end system for the sake of eliminating
the same feature length constraint.

In the original WLAS architecture, after audio context vector ca
and video context vector cv is computed at each decoding step, they
are concatenated to the output of decoder RNN to generate an output
probability distribution. We replace the context vector concatenation
process by the modality attention to calculate the merged context
vector. Concretely, at decoding step i, ca and cv is calculated use
decoder state si with ha and hv which is then feed into the modality
attender to obtain the modality attention weights αa
i . In the
end cav
, the weighted sum of context vector ca and cv is generated
i
to replace the concatenated context vector for computing the output
probability distribution.

i and αv

By combining an additional modality attention mechanism after
the attention process of WLAS framework, our system has several
beneﬁcial properties. First, it is more explicit than perform modal-
ity fusion at raw input feature level or encoder output level, since
the context vectors contain information more relevant to the current
output unit after being attended by the current decoder states. Sec-
ond, the attention weights αa
i and αv
i at decoding step i indicate
the contribution of individual modality to the merged context vector,
which show model preference over modalities. Third, the attention
weights are computed at every decoding step, their values can dy-
namically adjust along with the temporal changes in modality qual-
ity. This is important for the sudden or unstable noise occurs in one
of the modality. By automatically adjusting attention weight to pre-
fer the more reliable modality, modality attention in WLAS alleviate
the noise effect and make AVSR more robust. Fourth, it is a fully
differentiable soft attention mechanism and suitable for end-to-end
optimization, which makes attention weights to be learned automat-
ically. Finally, as the decode steps are much shorter than frames
of raw feature or encoder output feature, it is more computationally
efﬁcient to use individual context vector to get fused feature.

2.4. Related works

In this subsection, we compare our AVSR model to related works.
As mentioned above, the main difference between our method and
WLAS in [20] is we adopt an additional attention over modality con-
text vector to select more reliable modal information while WLAS
treats the two modalities equally by context vector concatenation.
The AV Align model [26] uses audio features to attend over visual
features to get enhanced representation to be used for attention based
decoder. The differences to their work are as follows. First, its addi-
tional attention supplies the lip features associated with the current
audio feature and occurs in the encoder side while our modality at-
tention is integrated in decoder side which gives out the preference
of modalities by attention weights. Second, ours also have the align-
like process between two modal and it is performed by individual
attention which generates individual context vectors associate to the
current output units. Finally, denoting Ta, Tv and Td as length of
audio feature, visual feature and decoding steps respectively, it per-
forms attention for every frame of audio representation whose com-

plexity is O(TaTv) while ours operates on context vector for every
decoding step whose complexity is O(2Td) which is much more efﬁ-
cient. Our modality attention mechanism is most similar to the works
in [27, 28] that use attention to fuse features from multi-channel au-
dio signal. The difference lies in that we aims to combine inherently
different features use attention, e.g. audio and video, while what
they combine are all speech features from different microphones.
While their feature length from different channel are identical, ours
has different feature length which facilitates us to operate modality
attention over attended context vector along with the decoding step
of an attention based decoder.

3. EXPERIMENTS

3.1. Data

Our audio-visual data is obtained from broadcast television news
videos.
It consists about 100 speakers and 104,881 examples of
speech, which is about 150 hours of data. The utterance is about
5 seconds long and contains 22 Chinese characters on average. We
use another 33,026 utterances as our test set which is about 42 hours.
The audio samples are extracted from the videos with fmpeg.
We manually add white Gaussian noise to the original speech at three
different SNRs, e.g. 10dB, 5dB and 0dB. Our acoustic feature is 71
dimension ﬁlter banks extracted every 10 mini-seconds within 25
mini-second window using the conventional ASR front end. First
and second derivatives are not used.

The video samples have a frame speed of 25 and are prepro-
cessed as follows. Each frame of the videos is processed with the
DLib [29] face detector and the DLib face shape predictor generates
68 landmarks. Using these landmarks, we extract mouth crop from
the origin images. With the mouth center calculated by landmarks of
id from 49 to 68, the distance along x-axis between 12th landmark
and 1st landmark is used as the mouth width W and 0.8 ⋆ W is the
mouth height H.

3.2. Baselines

As our AVSR baseline model, 4 different models are trained, namely
LAS, WAS, WLAS, AV Align. They are all attention based end-
to-end models. In order to construct truly end-to-end models, we
choose Chinese character as modeling unit. The total number of
modeling unit is 6812, including 26 English characters, 6784 Chi-
nese characters, start of sequence (SOS), end of sequence (EOS) and
unknown character (UNK). All models are trained by optimizing the
cross entropy loss between ground-truth character transcription and
predicted character sequence via adam optimizer. Curriculum learn-
ing (CL), schedule sampling (SS) and label smoothing (LS) [1] are
all adopted during training to improve performance. The total train-
ing epoch is set to 15. We use teacher force at the ﬁrst 4 epochs and
use output of last step to feed into network with schedule sampling
rate that gradually increases to 0.4 from epoch 5 to epoch 8. From
epoch 8 we ﬁx schedule sampling rate to 0.4. We use an initial learn-
ing rate of 0.0002 and halve it from epoch 11. Beam search decoding
is used without an external language model to evaluate our model on
test sets and the beam width is set to 5. Temperature is also used in
decoding to let the output distribution more smooth for better beam
search results.

The encoder of LAS, is a four layer BLSTM with 256 hidden
units on each LSTM. The third and fourth layers take every two con-
secutive frames of its input feature as input. As a result the ﬁnal
output representation of encoder is 4x subsampled, e.g. 25 frames

per seconds. It is essential to perform down sample in encoder as
our modeling units is Chinese character which is much longer than
phones. The decoder is a one layer LSTM with 512 hidden nodes
followed by a densely connected feed forward layer and a output
softmax layer. The attention is the content based attention as in Sub-
section 2.1.

The lip regions are 3-channel RGB images resized to 64x80 pix-
els. 512 dimension spacial features are extracted from images by 10
layers of CNN which are then feed into a 2 layer BLSTM to model
temporal variability. Max-pooling is performed only along the width
and length dimension of images. We do not perform time pooling or
concatenate features in BLSTMs to reduce the time resolution. The
details of watcher architecture are presented in Table 1. The decoder
and attender are the same as those of LAS.

Table 1. Watcher conﬁgurations. All convolution kernels are
1X3X3 and all maxpooling stride is 1X2X2. Selu is a nonlinear
activation in [30], BN indicate batch normalization and MP repre-
sents maxpooling.
CNN layer
0
1-2
3-4
5-6
7-8
9-10
11-12

output size
Tx3x64x80
Conv-Selu-Conv-Selu-MP-BN Tx32x32x40
Conv-Selu-Conv-Selu-MP-BN Tx48x16x20
Tx72x8x10
Conv-Selu-Conv-Selu-MP-BN
Tx108x4x5
Conv-Selu-Conv-Selu-MP-BN
Tx128x2x2
Conv-Selu-Conv-Selu-MP-BN
Tx512
BLSTM-BLSTM

operation
Resize

For all our AVSR systems, we keep video signal unchanged and
vary the speech signal in different SNR to explore the complemen-
tary effect of video modality for speech recognition, especially in
noisy condition. WLAS model has the same encoder and decoder
architecture as LAS and WAS. The only difference is that it use two
attender to compute context vectors from Listener and Watcher re-
spectively. As mentioned in [20], WLAS is hard to train since audio
signal tends to dominant in the training process. So we train Watcher
and Speller ﬁrst and then we ﬁx Watcher and train Listener and
Speller, in the end we train the three components jointly. AV Align
model also use the same encoder as WLAS before the cross modality
attention steps.

We train the base models at different SNR except WAS and sum-

marize test set CER in Table 2.

in the scoring function in Eq. (10) for each of the modality. Four
MD ATTs are trained with different acoustic speech and test with
matched condition. We also add noise to original wav ﬁles with one
of the 4 SNR randomly. Thus with this multi-condition data, we train
a MD ATT model, denoted as MD ATT MC, and test it on different
SNR test set. The two results are listed in the last two rows of Table
2.

As can be seen from Table 2, although WAS performs much
worse than LAS, it is helpful to integrate video information in
WLAS. The contribution of video depends on the SNR of audio
signal, for example, WLAS improves LAS from 18.65% CER to
12.34% CER in 0dB condition which is a relative 33.8% perfor-
mance gain. AV align model degrades in all conditions comparing
to single modality LAS system. This phenomenon also happens in
task LRS2 of [26]. It happens because speech can not align precisely
to video when it is corrupted by noise. Our MD ATT model per-
forms best among the three AVSR systems for all noisy conditions.
When increasing level of noise, it shows an increased advantage
and achieves a relative performance improvement up to 36% com-
pared to LAS system, showing the effectiveness of our multimodal
attention method for audio-visual speech recognition. Trained by
multi-condition speech data and video data, MD ATT MC models
continue to reduce CER in all SNR except 0dB.

N

n=1 αm

We continue to report in Table 3 the average attention weights
for each modality of MD ATT MC. The attention weights are com-
puted by averaging over all decoding steps of test set, e.g. αm =
1
n . Since CER of WAS is much higher than that of LAS
N P
at all SNRs, we could expect our model distributes its attention more
on audio than on video. Results in Table 3 shows that attention
weight for audio is above 0.6 which justiﬁes our guess. As we grad-
ually increase noise level from clean to 0dB, MD ATT MC lowers
its attention to audio from 0.641 to 0.607 and focuses more and more
on video, showing an adaptation ability to speech quality.

Table 3. The attention weights of MD ATT MC for audio and video
modal at different test SNR averaged over all sentences in testset.
test SNR attention weights
αa
0.641
0.633
0.624
0.607

αv
0.359
0.367
0.376
0.393

clean
10dB
5dB
0dB

Table 2. Recognition performance in CER [%] of various models at
different SNR.

4. FINAL REMARKS

LAS
WAS
WLAS
AV align
MD ATT
MD ATT MC

clean
7.08

7.00
7.6
6.95
6.85

44.62

10dB
10.33

9.07
10.89
8.54
8.12

5dB
12.93

10.23
13.69
9.87
9.74

0dB
18.65

12.34
19.21
11.93
13.65

In this paper, we have proposed a multimodal attention method to
fuse information from multimodal input. Modality attention mech-
anism is integrated in an end-to-end attention based AVSR system.
Experimental results show that our proposed method obtains a 36%
relative improvement comparing to LAS in 0dB SNR and outper-
forms the other feature concatenation based AVSR systems. Further-
more, attention weights can automatically adjust to a more reliable
modality according to the quality of individual signal. We will eval-
uate it with noises collected in the real ﬁeld on larger AVSR dataset
and other modalities is also our next direction.

3.3. Modality attention results

We denote MD ATT as our proposed modality attention based AVSR
system. The encoder for audio and video signal, decoder and at-
tender of MD ATT are the same as WLAS. we use 50 LSTM cells

5. REFERENCES

[1] Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Rohit Prab-
havalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J
Weiss, Kanishka Rao, Katya Gonina, et al., “State-of-the-art
speech recognition with sequence-to-sequence models,” arXiv
preprint arXiv:1712.01769, 2017.

[2] Harry McGurk and John MacDonald, “Hearing lips and seeing

voices,” Nature, vol. 264, no. 5588, pp. 746, 1976.

[3] Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen
Schmidhuber,
la-
belling unsegmented sequence data with recurrent neural net-
works,” in Proceedings of the 23rd International Conference
on Machine Learning. ACM, 2006, pp. 369–376.

“Connectionist temporal classiﬁcation:

[4] Alex Graves and Navdeep Jaitly, “Towards end-to-end speech
recognition with recurrent neural networks,” in International
Conference on Machine Learning, 2014, pp. 1764–1772.

[5] Alex Graves, “Sequence transduction with recurrent neural

networks,” arXiv preprint arXiv:1211.3711, 2012.

[6] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hin-
ton, “Speech recognition with deep recurrent neural networks,”
in Acoustics, Speech and Signal Processing (ICASSP), 2013
IEEE International Conference on. IEEE, 2013, pp. 6645–
6649.

[7] Kanishka Rao, Has¸im Sak, and Rohit Prabhavalkar, “Exploring
architectures, data and units for streaming end-to-end speech
recognition with rnn-transducer,” in Automatic Speech Recog-
nition and Understanding Workshop (ASRU). IEEE, 2017, pp.
193–199.

[8] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio,
“Neural machine translation by jointly learning to align and
translate,” arXiv preprint arXiv:1409.0473, 2014.

[9] Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and
Yoshua Bengio, “End-to-end continuous speech recognition
using attention-based recurrent nn: ﬁrst results,” arXiv preprint
arXiv:1412.1602, 2014.

[10] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals,
“Listen, attend and spell: A neural network for large vocabu-
lary conversational speech recognition,” in Acoustics, Speech
and Signal Processing (ICASSP), 2016 IEEE International
Conference on. IEEE, 2016, pp. 4960–4964.

[11] Juergen Luettin, Neil A Thacker, and Steve W Beet,

“Vi-
sual speech recognition using active shape models and hidden
markov models,” in ICASSP. IEEE, 1996, pp. 817–820.

[12] Timothy F Cootes, Gareth J Edwards, and Christopher J Taylor,
“Active appearance models,” IEEE Transactions on Pattern
Analysis & Machine Intelligence, , no. 6, pp. 681–685, 2001.

[13] Iain Matthews, Gerasimos Potamianos, Chalapathy Neti, and
Juergen Luettin, “A comparison of model and transform-based
visual features for audio-visual lvcsr,” in IEEE International
Conference on Multimedia and Expo. IEEE, 2001.

[14] Petar S Aleksic and Aggelos K Katsaggelos, “Comparison of
low-and high-level visual features for audio-visual continuous
automatic speech recognition,” in Acoustics, Speech, and Sig-
nal Processing (ICASSP). IEEE International Conference on.
IEEE, 2004, vol. 5, pp. V–917.

[15] Kuniaki Noda, Yuki Yamaguchi, Kazuhiro Nakadai, Hiroshi G
Okuno, and Tetsuya Ogata, “Lipreading using convolutional
neural network,” in Fifteenth Annual Conference of the Inter-
national Speech Communication Association, 2014.

[16] Eric Tatulli and Thomas Hueber,

“Feature extraction using
multimodal convolutional neural networks for visual speech
in Acoustics, Speech and Signal Processing
recognition,”
(ICASSP), 2017 IEEE International Conference on. IEEE,
2017, pp. 2971–2975.

[17] Joon Son Chung and Andrew Zisserman, “Lip reading in the
in Asian Conference on Computer Vision. Springer,

wild,”
2016, pp. 87–103.

[18] Michael Wand,

Jan Koutn´ık, and J¨urgen Schmidhuber,
in Acoustics,
“Lipreading with long short-term memory,”
Speech and Signal Processing (ICASSP), 2016 IEEE Interna-
tional Conference on. IEEE, 2016, pp. 6115–6119.

[19] Yannis M Assael, Brendan Shillingford, Shimon Whiteson,
and Nando de Freitas, “Lipnet: Sentence-level lipreading,”
arXiv preprint, 2016.

[20] Joon Son Chung, Andrew W Senior, Oriol Vinyals, and An-
in

“Lip reading sentences in the wild.,”

drew Zisserman,
CVPR, 2017, pp. 3444–3453.

[21] Eric David Petajan, “Automatic lipreading to enhance speech

recognition (speech reading),” 1984.

[22] Aggelos K Katsaggelos, Sara Bahaadini, and Rafael Molina,
“Audiovisual fusion: Challenges and new approaches,” Pro-
ceedings of the IEEE, vol. 103, no. 9, pp. 1635–1653, 2015.

[23] Gerasimos Potamianos, Chalapathy Neti, Guillaume Gravier,
Ashutosh Garg, and Andrew W Senior, “Recent advances in
the automatic recognition of audiovisual speech,” Proceedings
of the IEEE, vol. 91, no. 9, pp. 1306–1326, 2003.

[24] Kuniaki Noda, Yuki Yamaguchi, Kazuhiro Nakadai, Hiroshi G
Okuno, and Tetsuya Ogata, “Audio-visual speech recognition
using deep learning,” Applied Intelligence, vol. 42, no. 4, pp.
722–737, 2015.

[25] Stavros Petridis, Themos Stafylakis, Pingchuan Ma, Feipeng
“End-
arXiv preprint

Cai, Georgios Tzimiropoulos, and Maja Pantic,
to-end audiovisual speech recognition,”
arXiv:1802.06424, 2018.

[26] George Sterpu, Christian Saam, and Naomi Harte, “Attention-
based audio-visual fusion for robust automatic speech recogni-
tion,” in Proceedings of the 2018 on International Conference
on Multimodal Interaction. ACM, 2018, pp. 111–115.

[27] Stefan Braun, Daniel Neil, Jithendar Anumula, Enea Ceolini,
and Shih-Chii Liu, “Multi-channel attention for end-to-end
speech recognition,” Proc. Interspeech 2018, pp. 17–21, 2018.

[28] Suyoun Kim, Ian Lane, S Kim, and I Lane, “End-to-end speech
recognition with auditory attention for multi-microphone dis-
tance speech recognition,” Proc. Interspeech 2017, pp. 3867–
3871, 2017.

[29] Davis E King, “Dlib-ml: A machine learning toolkit,” Jour-
nal of Machine Learning Research, vol. 10, no. Jul, pp. 1755–
1758, 2009.

[30] G¨unter Klambauer, Thomas Unterthiner, Andreas Mayr, and
Sepp Hochreiter, “Self-normalizing neural networks,” in Ad-
vances in Neural Information Processing Systems, 2017, pp.
971–980.

