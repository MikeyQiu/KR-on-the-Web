ExploreKit: Automatic Feature Generation and
Selection

Gilad Katz
University of California, Berkeley
giladk@berkeley.edu

Eui Chul Richard Shin
University of California, Berkeley
ricshin@berkeley.edu

Dawn Song
University of California, Berkeley
dawnsong@cs.berkeley.edu

Abstract—Feature generation is one of the challenging aspects
of machine learning. We present ExploreKit, a framework for
automated feature generation. ExploreKit generates a large set
of candidate features by combining information in the original
features, with the aim of maximizing predictive performance
according to user-selected criteria. To overcome the exponential
growth of
the feature space, ExploreKit uses a novel ma-
chine learning-based feature selection approach to predict the
usefulness of new candidate features. This approach enables
efﬁcient identiﬁcation of the new features and produces superior
results compared to existing feature selection solutions. We
demonstrate the effectiveness and robustness of our approach
by conducting an extensive evaluation on 25 datasets and 3
different classiﬁcation algorithms. We show that ExploreKit can
achieve classiﬁcation-error reduction of 20% overall. Our code
is available at https://github.com/giladkatz/ExploreKit.

I. INTRODUCTION

Effective feature engineering serves as a prerequisite to
many machine learning tasks. Success with learning algo-
rithms requires the creation of features that provide useful
insights into different aspects of the data while taking the
idiosyncracies and limitations of the algorithms into account.
We present ExploreKit, a framework designed to alleviate
difﬁculties involved in feature engineering through automation.
Based on the intuition that highly informative features often
result from manipulations of elementary ones, we identiﬁed
common operators to transform each feature individually
or combine several of them together. ExploreKit uses these
operators to generate many candidate features, and chooses the
subset to add based on the empirical performance of models
trained with candidate features added.

Since evaluating all candidate features isn’t feasible, we pro-
pose a novel machine learning approach to predict candidates’
usefulness. Our approach models the interactions between the
candidate features and the dataset as well as various aspects
of the dataset itself. To the best of our knowledge this is the
ﬁrst attempt to address this problem using machine learning.

Our contributions are as following:
• We develop a modular framework for generating new
candidate features through structured operations and eval-
uating them automatically.

• We present novel methods for efﬁciently evaluating the
large set of generated candidate features, based on a
machine learning approach that predicts the utility of any
given feature.

Fig. 1: ExploreKit system architecture

• We empirically demonstrate the merits of our approach
on a large group of datasets and multiple classiﬁers,
achieving approximately 20% error reduction.

II. RELATED WORK

Feature Generation Using Deep Learning. In recent years,
deep learning has outperformed other forms of machine learn-
ing in a variety of challenging problems such as image classiﬁ-
cation [8] and speech recognition [6]. Its success originates in
its ability to operate directly on the raw data and learn a higher-
level representation automatically. However, the construction
of these representations by deep learning models is notoriously
opaque to human analysts. In contrast, our approach creates a
small set of features using human-understandable operations.

Automatic Feature Generation and Selection. To the best
of our knowledge only one previous work outside the ﬁeld of
deep learning has attempted to automatically generate features
to improve the performance of machine learning algorithms.
The Data Science Machine [7] has the similar goal of generat-
ing new features to improve classiﬁcation performance. Unlike
this work, ours works on “ﬂat” data without having to deﬁne
entity relations. In addition, we use machine learning to predict
the performance of candidate features, leveraging experience
from previous datasets. Finally, our approach ﬁnds a small set
of new features to reach the desired performance, while this
work requires the generation of thousands.

← GenerateCandidateFeatures(Fi); RankedF cand

F cand
i
bestF eatureSoF ar ← ∅; bestImprovementSoF ar ← −∞; Fi+1 ← ∅
for each (candidateFeature, featureScore) in SortDescending(RankedF cand

i

) do

i

← RankCandidateFeatures(F cand

)

i

improvement ← EvaluateWithClassiﬁer(Fi) − EvaluateWithClassiﬁer(Fi ∪ {candidateF eature})
if improvement ≥ (cid:15)w then

bestImprovementSoF ar ← improvement; bestF eatureSoF ar ← candidateF eature

if Fi+1 = ∅ and bestImprovementSoF ar ≥ thresholdw then

Algorithm 1 The ExploreKit Algorithm

break

if f eatureScore < thresholdf then

F0 ← dataset.GetFeaturesSet()
for i = 0 to maxIterations − 1 do

1: procedure GENERATEFEATURES(dataset, maxIterations)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

Fi+1 ← Fi ∪ {bestF eatureSoF ar}

return FmaxIterations

return Fi

else

Fi+1 ← Fi ∪ {candidateF eature}; break
if improvement > bestImprovementSoF ar then

III. PROBLEM DEFINITION

For our task, we assume a dataset of

input-output pairs,
where each instance comes with a set of original features. Our
goal is to generate new features based on the original features,
so that the learning procedure is able to ﬁnd a function within
its restricted class of functions that better approximates the
true underlying input-output function.

More formally, we have:
• original features Finit = {f1, · · · , fn}. For notational
to be one of the

uniformity, we consider the output
features.

• Example instances Iall which we can split into Itrain,

Iholdout, and Itest.

• Dataset D := Iall × Finit; for each instance, we have

values for all of the original features.

• A class of functions CF ⊂ {I × F → Y } considered by
the learning procedure, where F is a set of features.
• A learning procedure L, which takes I × F and produces

a member of CF : L(I × F ) ∈ CF .

• An evaluation procedure E(CF , I × F ), which computes

the error of a learned function CF on I × F .

• Potential new features f cand

1

, f cand
2

, · · · ∈ Fcand.

Then we want to obtain

arg min
Fselect⊂Fcand

E(L(Itrain × Finit ∪ Fselect), Iholdout × Finit ∪ Fselect)

(1)
where Fselect is the set of features selected from all potential
features.

IV. THE PROPOSED METHOD
Overview. Our feature generation process is presented in
Figure 1 and Algorithm 1. It is an iterative process where
each iteration comprises of three phases: candidate features
generation, candidate feature ranking, and candidate features
evaluation & selection.

In the candidate feature generation phase we apply multiple
operators on the current set of features Fi to generate a large
set of candidate features F cand
. In the candidate features
ranking phase we assign a score to each candidate feature
f cand
based on its estimated contribution and
i,j
produce an ordered list of features RankedF cand

∈ F cand
i

.

i

i

Fig. 2: An illustrations of the operators used by ExploreKit
in the generation of a feature in the Pima dataset

As the number of candidate features is very large, we
must use an efﬁcient ranking function. Existing methods
which do not require the training of a classiﬁcation model,
such as Information Gain, proved inadequate for the large
feature space. Moreover, we hypothesize that effective ranking
requires taking into account the size and feature composition
of the analyzed dataset. We therefore propose a novel ML-
based approach for candidate feature ranking. We deﬁne meta-
features to represent both the dataset and the candidate feature
and train a feature ranking classiﬁer. This is, to the best of our
knowledge, the ﬁrst such attempt.

i,j

∈ RankedF cand

In the candidate features evaluation & selection phase we
use greedy search to evaluate the ranked candidate features.
We evaluate the performance of the joint set Fi ∪ {f cand
} for
each f cand
and compute the reduction in
classiﬁcation error compared with Fi. When the performance
improvement exceeds a predeﬁned threshold (cid:15)w, the evaluation
process terminates and we select the current candidate feature,
denoted as f select
} as
the current feature set of the following iteration Fi+1. Next
we describe the phases of this process in detail.

. We deﬁne the joint set Fi ∪ {f select

i,j

i

i

i

A. Generation of Candidate Features

The goal of this phase is to generate a large set of candidate
features F cand
using the current features set Fi. We ﬁrst
present the operators used in the candidate feature generation
and then describe our proposed process.

i

1) Operator Types: We apply three types of operators to
for iteration i: unary,

generate the candidate features set F cand
binary and higher-order.
Unary operators: applied on a single feature. Each operator
in this group belongs to one of two sub-groups:

i

• Discretizers: used to convert continuous and datetime
features into discrete (i.e. categorical) ones. Discretization
[4] is necessary in many popular classiﬁcation algorithms
(e.g., Decision Trees and Naive Bayes) and has also been
shown to contribute to performance [1]. For our evalua-
tion, we have implemented the EqualRange discretization
for numeric features (partition the range of values of
the feature into X equal segments) and the DayOfWeek,
MonthOfYear and IsWeekend for date-time features.
Another important beneﬁt of discretization is that
it
provides us with transformations of continuous features
that can be utilized by the higher–order operators.

• Normalizers: used to ﬁt the scale of continuous (i.e.
numeric) features to speciﬁc distributions. Normalization
has also been shown critical to the performance of multi-
ple machine learning algorithms [3]. For our experiments
we have implemented normalization to the range [0,1].

Binary operators: applied on a pair of features. This group
currently consists of the four basic arithmetic operations:
+, −, ×, ÷.

Higher-order operators: use multiple (two or more) fea-
tures for
the generation of a new one. We have im-
plemented ﬁve operators in this group: GroupByThenMax,
GroupByThenMin, GroupByThenAvg, GroupByThenStdev and
GroupByThenCount. These operators implement
the SQL-
based operations with the same name.

It is important to point out that in our experiments we only
use a small set of operators. Additional operators can be easily
created and added to our framework, including many domain-
speciﬁc operators for ﬁelds such as biology or time-series
analysis.

2) Generating the Candidate Features Set: We generate
the candidate features by applying the operators in the follow-
ing order:

1. We apply the unary operators on all possible features
in the features set. We use this step to create Fu,i, the
normalized and discretized versions of all non-discrete
features in Fi.

2. W apply the binary and higher-order operators on the
uniﬁed set Fi ∪ Fu,i. All possible valid feature combina-
tions are generated. We denote the features generated in
this step as Fo,i.

3. For every applicable (i.e. non-discrete) feature in Fo,i, we
once again apply all the unary operators. We denote the
features set generated by this step as Fou,i.

i

of all generated feature sets: F cand
In order to limit the size of F cand

4. The ﬁnal candidate features set for iteration i is the union
= Fu,i ∪ Fo,i ∪ Fou,i.
, the features are only
combined once: none of the generated candidate features is
re-used to generate additional candidate features.
Example: applying multiple operators. Consider the follow-
ing example from the Pima dataset.1 The goal is to identify
women with risk of diabetes. Two features from this dataset

i

1http://www.openml.org/d/37

are BMI (body mass index) and the number of pregnancies.
One possible way to generate additional features is as follows:
1. Discretize (partition) the BMI attributes into segments:

[20, 25), [25, 30) and so forth.

2. Group all instances based on their BMI assignment.
3. For the instances of each group, ﬁnd the minimal number

of pregnancies of any member of the group.

This feature may enable a classiﬁer to infer a woman’s
relative number of pregnancies while using a smaller, more
relevant reference group. This feature is created by using an
unary discretization operator, followed by a GroupByThenMin
operator. This process is illustrated in Figure 2.

B. Ranking Candidate Features

i

The goal of this phase is to use computationally-efﬁcient
(“lightweight”) methods to rank the large number of candidate
features F cand
. We develop a novel machine learning based
approach that generates a set of meta-features F meta
for each
candidate feature f cand
. Once the meta-features are generated,
we use the ranking classiﬁer C to assign a score to each
f cand
. We train C on previously analyzed datasets (the training
i,j
process is described later in this section).

i,j

i,j

We generate two types of meta-features: dataset-based and

candidate features-based.
Dataset-based meta-features. Every dataset has multiple
characteristics that may affect the likelihood of f cand
being
effective. We generate four types of meta-features:

i,j

1. General information: general statistics on the analyzed
dataset: number of instances and classes, statistics on the
size and other statistics on Fi.

2. Initial evaluation: statistics on the current performance
of the classiﬁer when applied on Fi. The generated meta-
features include AUC, log loss and precision/recall values
at various thresholds.

3. Entropy-based measures: we partition Fi

into sub-
groups based on their type (discrete, numeric, date-time)
and calculate statistics on the Information Gain (IG) of
the features in each.

4. Feature diversity: we partition Fi into type-based groups
and use the chi-squared and paired-t test to calculate the
similarity of each pair in a group. We generate meta-
features using the tests’ statistic values.

Candidate feature-based meta-features. These meta-features
represent the interactions of each f cand
i,j with regard to Fi. Be-
cause our candidate features are generated, our meta-features
also take into account the operator(s) and features (“parent
features”) used to generate f cand
. The meta-features can be
partitioned into three groups:

i,j

1. Entropy and statistical tests for the candidate feature:
we partition Fi into subgroups based on their type and
use the chi-sqaured and paired t-tests to derive statistics
on f cand
’s correlation to each groups. In addition, we
i,j
derive entropy-based measures for f cand

.

i,j

2. General information on parent features and opera-
tors: We generate statistics representing various charac-

TABLE I: The characteristics of the datasets used in the
experiments

Num of
Features

Name

Heart
Horse
Cancer
Indian liver
Credit
Diabetes
German credit
Diabetic ret.
Contraceptive
Cardiography
Seismic bumps
Space
Wind
Puma 8
Puma 32
CPU act
CPU small
Delta elevators
Mammography
Ailerons
Web Data
Bank marketing
Vehicle IT
Vehicle Norm
Poker

Num
of Data
Points
270
368
569
585
690
768
1,000
1,155
1,473
2,126
2,584
3,107
6,574
8,192
8,192
8,192
8,192
9,517
11,183
13,750
36,974
45211
98,528
98,528
1,025,010

%
of
Minority
Class
44.4%
36.9%
37.2%
28.6%
44.4%
34.8%
30%
46.9%
22.6%
22.1%
6.5%
49.6%
46.7%
49.7%
49.6%
30.2%
30.2%
49.7%
2.3%
42.3%
24%
11.6%
50%
50%
49.8%

13
22
30
10
15
8
20
19
9
22
18
6
14
8
32
21
12
6
6
40
123
16
100
100
10

%
of
Numeric
Features
46%
31.8%
100%
90%
40%
100%
35%
100%
66.6%
100%
77%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
43.75%
100%
100%
100%

teristics of the parent features and the operator(s) that
participated in the creation of f cand
. This sub-group
includes the type of each parent feature, the range of
possible values and the number and type of operators
used.

i,j

3. Statistical tests on parent features: we derive statistics
on the inter-correlation of the parent features of f cand
,
as well as their correlation with the remaining features
of Fi. In addition, we generate information on the used
operator(s).

i,j

i,j

∈ F cand
i

We use the ranking classiﬁer C to estimate the likelihood
of each f cand
to reduce the error, based on its
meta-features. We sort the candidates by this likelihood and
create a ranked list RankedF cand
. Candidate features whose
i
probability is below a predeﬁned parameter thresholdf are
ﬁltered.

Training the ranking classiﬁer. To train a ranking classiﬁer
C capable of utilizing the meta-features, we produce a large
labeled training set from a repository of datasets. The entire
process described in this section is done “off-line”.

The training data generation process is presented in Algo-
rithm 2. The process consists of three stages: ﬁrst, for each
training dataset DT we generate the set of candidate features
f cand,1
, using the same process described
T
in Section IV-A for one iteration. By doing this we create a
large set of candidate features that can be analyzed.

... ∈ F cand

, f cand,2
T

T

In the second stage of the process we generate the meta-
features presented above for each of the candidate features.
We conduct this analysis separately for each DT . We then use
the generated meta-features to create the training set for the
ranking classiﬁer.

TABLE II: The percentage of error reduction (relative
to the original set of features) obtained by each of the
evaluated approaches using the decision tree algorithm.
IG Fast

ML Full ML Fast

Dataset Name

IG Full

initial
AUC
0.76
0.84
0.92
0.61
0.82
0.73
0.65
0.66
0.68
0.89
0.57
0.8
0.85
0.87
0.84
0.92
0.91
0.89
0.82
0.87
0.73
0.72
0.8
0.8
0.82

Heart
Horse
Cancer
Indian liver
Credit
Diabetes
German credit
Diabetic ret.
Contraceptive
Cardiography
Seismic bumps
Space
Wind
Puma 8
Puma 32
CPU act
CPU small
Delta elevators
Mammography
Ailerons
Web Data
Bank marketing
Vehicle IT
Vehicle Norm
Poker

Average
(Median)

40.6%
39.5%
63.61%
31.26%
31.8%
23.9%
11.3%
44.6%
10.6%
47.5%
11.7%
48.4%
19.9%
23.9%
31.8%
23.2%
26%
24.1%
47.51%
30.5%
1.7%
29.5%
1.6%
11.4%
57.5%
29.3%
(29.4%)

16.4%
1.6%
3.2%
25.3%
0%
0%
2.9%
8.2%
0%
8.7%
0%
2.4%
0%
0%
0%
0%
4.5%
0%
0%
0.6%
0%
0%
0%
2.3%
5.8%
3.4%
(0.3%)

20.1%
0%
28.8%
11.8%
4.6%
4.9%
6.8%
-9.4%
-2.2%
-35.3%
1%
13.7%
1.1%
18.7%
-16.2%
-20.5%
0.35%
9%
27.8%
5.1%
0.8%
7.5%
-1.6%
6.1%
57.5%
5.6%
(4.8%)

2.7%
0%
-5.9%
14.6%
0%
20.1%
12.3%
0%
0%
0%
0%
-3.3%
0%
4.7%
0%
0%
-22%
0%
0%
0%
0%
3.1%
0%
0.8%
4.4%
1.6%
(0)%

The third stage is assigning labels (“good” or “bad”) to
each candidate feature generated for DT . We use the more
computationally-expensive (wrapper) feature evaluation meth-
to determine the error reduction obtained by applying
ods
the classiﬁer on the joint set FT ∪ {f cand,i
} where FT is the
original feature set of DT . If the error after adding f cand,i
declines by more than (cid:15), we label f cand,i
as “good” and “bad”
otherwise. We assign these labels to the meta-features and use
the labeled set to train C.

T

T

T

By performing this process over a diverse group of datasets,
we created a ranking classiﬁer capable of effectively analyzing
previously unseen datasets. In our experiments, presented in
Section V, we demonstrate that our approach is effective for
datasets with large varieties in size and feature compositions.

C. Candidate Features Evaluation & Selection

The goal of this phase is to conduct a more computationally-
intensive (wrapper) evaluation on a small set of candidate
∈ F cand
features. We created this features set, RankedF cand
,
i
during the candidate feature ranking phase.

i

For each candidate feature f cand

, we
evaluate the performance of the classiﬁer on the joint feature
set Fi ∪ {f cand
}. The evaluation is conducted using k-fold
cross validation. The evaluation continues until either:

∈ RankedF cand

i,j

i,j

i

• The features set Fi ∪ {f cand

} satisﬁes the condition:
}) + (cid:15)w ≤ E(Fi), where E measures the

i,j

E(Fi ∪ {f cand
error and (cid:15)w is a predeﬁned parameter.

i,j

• The number of evaluated candidate features exceeded
the highest
is chosen, given

a predeﬁned parameter Rw. In this case,
performing member of RankedF cand

i

TABLE III: The percentage of error reduction (relative
to the original set of features) obtained by each of the
evaluated approaches using the SVM algorithm.
IG Full

ML Full ML Fast

Dataset Name

IG Fast

TABLE IV: The percentage of error reduction (relative
to the original set of features) obtained by each of the
evaluated approaches using the Random Forest algorithm.
IG Fast

ML Full ML Fast

Dataset Name

IG Full

initial
AUC
0.91
0.83
0.96
0.5
0.85
0.72
0.68
0.69
0.5
0.82
0.5
0.82
0.86
0.82
0.64
0.90
0.86
0.89
0.53
0.87
0.74
0.67
0.85
0.85
0.51

Heart
Horse
Cancer
Indian liver
Credit
Diabetes
German credit
Diabetic ret.
Contraceptive
Cardiography
Seismic bumps
Space
Wind
Puma 8
Puma 32
CPU act
CPU small
Delta elevators
Mammography
Ailerons
Web Data
Bank marketing
Vehicle IT
Vehicle Norm
Poker

Average
(Median)

-14.3%
18.3%
46.3%
9.6%
19%
9.4%
14.7%
36.5%
25%
41.6%
1.5%
27.1%
12.1%
4.2%
23.1%
23.6%
25.1%
0.8%
45.7%
2.4%
10.3%
10.3%
10.6%
10.9%
8.34%
17.4%
(14.7%)

0%
0%
0%
7.1%
0%
0%
2.2%
0%
0%
0%
0%
0%
0%
-1.4%
0.1%
0%
0%
0%
1.9%
0%
0%
2.6%
1.1%
1.3%
0%
0.6%
(0%)

-9.4%
-25.2%
3.5%
-4%
0%
0%
0%
9.6%
0%
-1.3%
0%
7.5%
-2.2%
3.1%
0%
1.2%
0%
14.6%
0%
9.4%
-3.3%
0%
0%
13.9%
0%
7.8%
-0.7%
-1.6%
18.4%
-4.3%
16.7%% 9.6%
14.7%
-3.4%
32.2%
1.1%
-1.3%
4.8%
-1.3%
0.4%
-3.5%
4.6%
(1.2%)

0%
0.9%
-18.5%
0%
0%
0%
-0.5%
0%
-1.1%
-1%
(0%)

initial
AUC
0.94
0.87
0.99
0.78
0.91
0.82
0.78
0.77
0.72
0.95
0.59
0.89
0.94
0.9
0.94
0.98
0.97
0.94
0.96
0.94
0.81
0.83
0.91
0.91
0.93

Heart
Horse
Cancer
Indian liver
Credit
Diabetes
German credit
Diabetic ret.
Contraceptive
Cardiography
Seismic bumps
Space
Wind
Puma 8
Puma 32
CPU act
CPU small
Delta elevators
Mammography
Ailerons
Web Data
Bank marketing
Vehicle IT
Vehicle Norm
Poker

Average
(Median)

15.3%
26%
52.4%
9.3%
24.8%
20.8%
16.2%
0%
12%
48.1%
7.1%
46.6%
8.6%
8%
67.7%
13.9%
13.6%
7.6%
40.6%
49.6%
0.27%
31.4%
1.3%
9.4%
13.2%
23.5%
(16.2%)

0%
0%
4.9%
3.1%
0%
0%
0%
1.6%
0%
3.9%
7.4%
1.9%
0%
0%
0.5%
10.2%
1.8%
0%
0%
0%
0%
4.1%
0%
0%
8.8%
1.9%
(0%)

22.2%
2.6%
-3.3%
-12.7%
-2.86%
-1.85%
1%
-0.1%
-9.3%
-18%
5.3%
24.4%
2.2%
5%
49.31
10%%
1.35%
6.5%
15%
12.2%
-2.3%
3.4%
-0.6%
12.2%
57%
6.87%
(2.2%)

-14.3%
-3.9%
0%
0%
2.6%
0%
3.7%
0%
0%
0%
0%
-3.4%
-3%
0%
0%
0.5%
-0.6%
-0.5%
0%
0%
0.4%
0.7%
0%
0.3%
5.1%
-0.5%
(0%)

that it exceeds the minimal threshold threasholdw. If
no candidate feature meets this criterion, no candidate
feature is selected and the search terminates.

Once a candidate feature that satisﬁes the criteria is found,
it is denoted as f select
. We then deﬁne Fi+1 ← Fi ∪ {f select
}
as the current features set for the next iteration and repeat the
process described in this section

i

i

A. Experimental Setup

V. EVALUATION

We evaluated ExploreKit on 25 supervised classiﬁcation
datasets with large variety in size, number of attributes,
feature type composition and class imbalance. All datasets are
available on the OpenML repository2 and their properties are
presented in Table I. All datasets were randomly partitioned
into training and test sets, with 66% of the data assigned to
the former. Original class ratios were maintained.

We use three types of algorithms in the evaluation: the
C4.5 decision tree algorithm, SVM and random forest. For all
algorithms we used the implementation included in version
3.7 of the Weka ML platform [5] with the default settings.

• For all purposes of feature generation ranking and evalu-
ation, the training set was partitioned into three folds of
equal size and class ratios. The same dataset partitions
were used in all datasets.

• We used random forest to train a ranking model on the
meta-features. This was done for all experiments (see
Section IV-B).

• We used the following parameters: thresholdf = 0.001,
thresholdw ≥ 0, (cid:15)w = 0.01 and Rw = 15, 000 (see
Section IV for details).

• For each dataset the evaluation consisted of 15 search
iterations, with one feature added during each iteration.
The allowed running time for each dataset was three days
– if the search exceeded that time, the last completed
iteration was considered as the ﬁnal one.

• We used a leave-one-out (LOO) approach for the training
of the ranking classiﬁcation model: for each evaluated
dataset di, we trained the ranking classiﬁer using meta-
features from dj ∈ D where i (cid:54)= j.

B. The Evaluated Algorithms

We evaluate two versions of our algorithm and compare

We used the following settings throughout the evaluation:
• We conducted our experiments on a 20-core machine with

them to two baselines:

64GB of RAM.

• We used the error reduction as the evaluation metric.
We calculate it using the formula (1−AU Ci)−(1−AU Cf )
,
where AU Ci and AU Cf are the AUC values obtained
for Fi and Fi ∪ {f cand

}, respectively.

(1−AU Ci)

i,j

2http://www.openml.org/home

• ML Full – the full algorithm presented in Section IV.
• ML Fast – we use the candidate ranking method as in
ML Full, but evaluate only the top ranking candidate (ties
are broken randomly). In this experiment the parameter
(cid:15)w is set to 0 (i.e. the set Fi ∪ {f cand
} needs only not
to have reduced performance compared to Fi). If this
condition is met, the candidate feature is added to Fi and

i,j

Algorithm 2 Generating metafeatures from background datasets

metaFeaturesSet ← ∅
originalP erf ormance ← wrapperEvaluator.Rank((dataset.GetF eatures())
candidatePool ← GenerateCandidateFeatures(dataset.GetFeatures())
for each (candidateFeature) in candidatePool do

1: procedure METAFEATURES(dataset, operators, ﬁlterEvaluator, wrapperEvaluator, improvementThreshold)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:

candidateFeatureMetaSet ← GenerateMetaFeatures(candidateF eature, dataset)
candidateF eatureP erf ormance ← wrapperEvaluator.Rank(Dataset, candidateF eature)
if candidateFeaturePerformance ≥ originalPerformance + improvementThreshold then

candidateF eatureM etaSet.SetLabel(True)

else

candidateF eatureM etaSet.SetLabel(False)
metaFeaturesSet ← candidateF eatureM etaSet

12:

return metaFeaturesSet

a new search iteration begins. If not, we terminate the
search and return Fi as the ﬁnal features set.

• IG Full – this baseline method is identical to ExploreKit
but utilizes Information Gain (IG) for the ranking of
candidate features instead of our ML-based approach.
• IG Fast – This baseline is identical to ML Fast but

utilizes Information Gain as the ranking function.

C. Evaluation Results

Tables II–IV show the results of the evaluation. For each
classiﬁer we compare the results of the evaluated algorithms
to those obtained on the original datasets. ML Full signiﬁ-
cantly outperforms all other methods, achieving an average
error reduction of 17.4%–29.3% over the different classiﬁers,
compared to 1.2%–5.6% for IG Full (the closest competitor).
The results also illustrate the merits of our ML-based
approach, which outperforms the IG-based methods both in
ranking-only and ranking-and-evaluation settings. Another im-
portant advantage is robustness: throughout the evaluation,
both ML-based variations had a negative impact on only a
single dataset compared with 22 and 17 for IG Full and
IG Fast respectively.

VI. DISCUSSION

We analyzed the meta-features used by our approach iden-
tify those that had the nost infulence on the classiﬁcation
model. We identiﬁed the following three groups:

• Intial Evaluation Measures, dataset-based meta-
features – indicative meta-features of this group include
statistics on the IG scores of the dataset’s features and
recall/precision values.

• Feature Diversity Measures, dataset-based meta-
features – the statistical tests we used to calculate the
correlation among the dataset’s features (particularly the
non-discrete) were given a large weight.

• Entropy & Statistical Tests,

features – speciﬁcally, the correlation of f cand
calculated using the paired T and chi-squared tests.

feature-based meta-
i,j with Fi,

These ﬁndings conﬁrm our intuition that analyzing the
classiﬁer’s performance on the initial dataset as well as ana-
lyzing its feature composition are needed for effective feature
selection. To the best of our knowledge, we are the ﬁrst to use
this information in the context of feature selection.

VII. CONCLUSIONS AND FUTURE WORK

In this study we have presented ExploreKit, a novel ap-
proach for the automatic exploration of data. By both develop-
ing a method for the generation of a large number of candidate
features as well as a new type of feature selection approach,
we have been able to signiﬁcantly improve the performance
of multiple classiﬁers on a large variety of datasets.

For future work, we plan to add additional operators of var-
ious types to our framework while also leveraging knowledge
from the previously analyzed datasets for selecting subsets
of operators for different datastes. In addition, we intend
to extend our framework to include classiﬁer and parameter
selection as well as the automatic selection and application
of methods such as PCA [2]. Finally, we are interested in
combining our approach with deep learning algorithms.

VIII. ACKNOWLEDGMENT

This work was supported in part by DARPA under the award

DARPA FA8750-15-2-0104.

REFERENCES

[1] J. Dougherty, R. Kohavi, M. Sahami, et al. Supervised and unsupervised
discretization of continuous features. In Machine learning: proceedings
of the twelfth international conference, volume 12, 1995.

[2] G. H. Dunteman. Principal components analysis, volume 69. Sage, 1989.
[3] P. A. Est´evez, M. Tesmer, C. A. Perez, and J. M. Zurada. Normalized mu-
tual information feature selection. Neural Networks, IEEE Transactions
on, 20(2), 2009.

[4] S. Garcia, J. Luengo, J. A. S´aez, V. L´opez, and F. Herrera. A survey of
discretization techniques: Taxonomy and empirical analysis in supervised
learning. Knowledge and Data Engineering, IEEE Transactions on, 25(4),
2013.

[5] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H.
Witten. The weka data mining software: an update. ACM SIGKDD
explorations newsletter, 11(1), 2009.

[6] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,
A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, et al. Deep neural
networks for acoustic modeling in speech recognition: The shared views
of four research groups. IEEE Signal Processing Magazine, 29(6):82–97,
2012.

[7] J. M. Kanter and K. Veeramachaneni. Deep feature synthesis: Towards
In Data Science and Advanced
automating data science endeavors.
Analytics (DSAA), 2015. 36678 2015. IEEE International Conference on.
IEEE, 2015.

[8] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei.
International
ImageNet Large Scale Visual Recognition Challenge.
Journal of Computer Vision (IJCV), 115(3), 2015.

ExploreKit: Automatic Feature Generation and
Selection

Gilad Katz
University of California, Berkeley
giladk@berkeley.edu

Eui Chul Richard Shin
University of California, Berkeley
ricshin@berkeley.edu

Dawn Song
University of California, Berkeley
dawnsong@cs.berkeley.edu

Abstract—Feature generation is one of the challenging aspects
of machine learning. We present ExploreKit, a framework for
automated feature generation. ExploreKit generates a large set
of candidate features by combining information in the original
features, with the aim of maximizing predictive performance
according to user-selected criteria. To overcome the exponential
growth of
the feature space, ExploreKit uses a novel ma-
chine learning-based feature selection approach to predict the
usefulness of new candidate features. This approach enables
efﬁcient identiﬁcation of the new features and produces superior
results compared to existing feature selection solutions. We
demonstrate the effectiveness and robustness of our approach
by conducting an extensive evaluation on 25 datasets and 3
different classiﬁcation algorithms. We show that ExploreKit can
achieve classiﬁcation-error reduction of 20% overall. Our code
is available at https://github.com/giladkatz/ExploreKit.

I. INTRODUCTION

Effective feature engineering serves as a prerequisite to
many machine learning tasks. Success with learning algo-
rithms requires the creation of features that provide useful
insights into different aspects of the data while taking the
idiosyncracies and limitations of the algorithms into account.
We present ExploreKit, a framework designed to alleviate
difﬁculties involved in feature engineering through automation.
Based on the intuition that highly informative features often
result from manipulations of elementary ones, we identiﬁed
common operators to transform each feature individually
or combine several of them together. ExploreKit uses these
operators to generate many candidate features, and chooses the
subset to add based on the empirical performance of models
trained with candidate features added.

Since evaluating all candidate features isn’t feasible, we pro-
pose a novel machine learning approach to predict candidates’
usefulness. Our approach models the interactions between the
candidate features and the dataset as well as various aspects
of the dataset itself. To the best of our knowledge this is the
ﬁrst attempt to address this problem using machine learning.

Our contributions are as following:
• We develop a modular framework for generating new
candidate features through structured operations and eval-
uating them automatically.

• We present novel methods for efﬁciently evaluating the
large set of generated candidate features, based on a
machine learning approach that predicts the utility of any
given feature.

Fig. 1: ExploreKit system architecture

• We empirically demonstrate the merits of our approach
on a large group of datasets and multiple classiﬁers,
achieving approximately 20% error reduction.

II. RELATED WORK

Feature Generation Using Deep Learning. In recent years,
deep learning has outperformed other forms of machine learn-
ing in a variety of challenging problems such as image classiﬁ-
cation [8] and speech recognition [6]. Its success originates in
its ability to operate directly on the raw data and learn a higher-
level representation automatically. However, the construction
of these representations by deep learning models is notoriously
opaque to human analysts. In contrast, our approach creates a
small set of features using human-understandable operations.

Automatic Feature Generation and Selection. To the best
of our knowledge only one previous work outside the ﬁeld of
deep learning has attempted to automatically generate features
to improve the performance of machine learning algorithms.
The Data Science Machine [7] has the similar goal of generat-
ing new features to improve classiﬁcation performance. Unlike
this work, ours works on “ﬂat” data without having to deﬁne
entity relations. In addition, we use machine learning to predict
the performance of candidate features, leveraging experience
from previous datasets. Finally, our approach ﬁnds a small set
of new features to reach the desired performance, while this
work requires the generation of thousands.

← GenerateCandidateFeatures(Fi); RankedF cand

F cand
i
bestF eatureSoF ar ← ∅; bestImprovementSoF ar ← −∞; Fi+1 ← ∅
for each (candidateFeature, featureScore) in SortDescending(RankedF cand

i

) do

i

← RankCandidateFeatures(F cand

)

i

improvement ← EvaluateWithClassiﬁer(Fi) − EvaluateWithClassiﬁer(Fi ∪ {candidateF eature})
if improvement ≥ (cid:15)w then

bestImprovementSoF ar ← improvement; bestF eatureSoF ar ← candidateF eature

if Fi+1 = ∅ and bestImprovementSoF ar ≥ thresholdw then

Algorithm 1 The ExploreKit Algorithm

break

if f eatureScore < thresholdf then

F0 ← dataset.GetFeaturesSet()
for i = 0 to maxIterations − 1 do

1: procedure GENERATEFEATURES(dataset, maxIterations)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

Fi+1 ← Fi ∪ {bestF eatureSoF ar}

return FmaxIterations

return Fi

else

Fi+1 ← Fi ∪ {candidateF eature}; break
if improvement > bestImprovementSoF ar then

III. PROBLEM DEFINITION

For our task, we assume a dataset of

input-output pairs,
where each instance comes with a set of original features. Our
goal is to generate new features based on the original features,
so that the learning procedure is able to ﬁnd a function within
its restricted class of functions that better approximates the
true underlying input-output function.

More formally, we have:
• original features Finit = {f1, · · · , fn}. For notational
to be one of the

uniformity, we consider the output
features.

• Example instances Iall which we can split into Itrain,

Iholdout, and Itest.

• Dataset D := Iall × Finit; for each instance, we have

values for all of the original features.

• A class of functions CF ⊂ {I × F → Y } considered by
the learning procedure, where F is a set of features.
• A learning procedure L, which takes I × F and produces

a member of CF : L(I × F ) ∈ CF .

• An evaluation procedure E(CF , I × F ), which computes

the error of a learned function CF on I × F .

• Potential new features f cand

1

, f cand
2

, · · · ∈ Fcand.

Then we want to obtain

arg min
Fselect⊂Fcand

E(L(Itrain × Finit ∪ Fselect), Iholdout × Finit ∪ Fselect)

(1)
where Fselect is the set of features selected from all potential
features.

IV. THE PROPOSED METHOD
Overview. Our feature generation process is presented in
Figure 1 and Algorithm 1. It is an iterative process where
each iteration comprises of three phases: candidate features
generation, candidate feature ranking, and candidate features
evaluation & selection.

In the candidate feature generation phase we apply multiple
operators on the current set of features Fi to generate a large
set of candidate features F cand
. In the candidate features
ranking phase we assign a score to each candidate feature
f cand
based on its estimated contribution and
i,j
produce an ordered list of features RankedF cand

∈ F cand
i

.

i

i

Fig. 2: An illustrations of the operators used by ExploreKit
in the generation of a feature in the Pima dataset

As the number of candidate features is very large, we
must use an efﬁcient ranking function. Existing methods
which do not require the training of a classiﬁcation model,
such as Information Gain, proved inadequate for the large
feature space. Moreover, we hypothesize that effective ranking
requires taking into account the size and feature composition
of the analyzed dataset. We therefore propose a novel ML-
based approach for candidate feature ranking. We deﬁne meta-
features to represent both the dataset and the candidate feature
and train a feature ranking classiﬁer. This is, to the best of our
knowledge, the ﬁrst such attempt.

i,j

∈ RankedF cand

In the candidate features evaluation & selection phase we
use greedy search to evaluate the ranked candidate features.
We evaluate the performance of the joint set Fi ∪ {f cand
} for
each f cand
and compute the reduction in
classiﬁcation error compared with Fi. When the performance
improvement exceeds a predeﬁned threshold (cid:15)w, the evaluation
process terminates and we select the current candidate feature,
denoted as f select
} as
the current feature set of the following iteration Fi+1. Next
we describe the phases of this process in detail.

. We deﬁne the joint set Fi ∪ {f select

i,j

i

i

i

A. Generation of Candidate Features

The goal of this phase is to generate a large set of candidate
features F cand
using the current features set Fi. We ﬁrst
present the operators used in the candidate feature generation
and then describe our proposed process.

i

1) Operator Types: We apply three types of operators to
for iteration i: unary,

generate the candidate features set F cand
binary and higher-order.
Unary operators: applied on a single feature. Each operator
in this group belongs to one of two sub-groups:

i

• Discretizers: used to convert continuous and datetime
features into discrete (i.e. categorical) ones. Discretization
[4] is necessary in many popular classiﬁcation algorithms
(e.g., Decision Trees and Naive Bayes) and has also been
shown to contribute to performance [1]. For our evalua-
tion, we have implemented the EqualRange discretization
for numeric features (partition the range of values of
the feature into X equal segments) and the DayOfWeek,
MonthOfYear and IsWeekend for date-time features.
Another important beneﬁt of discretization is that
it
provides us with transformations of continuous features
that can be utilized by the higher–order operators.

• Normalizers: used to ﬁt the scale of continuous (i.e.
numeric) features to speciﬁc distributions. Normalization
has also been shown critical to the performance of multi-
ple machine learning algorithms [3]. For our experiments
we have implemented normalization to the range [0,1].

Binary operators: applied on a pair of features. This group
currently consists of the four basic arithmetic operations:
+, −, ×, ÷.

Higher-order operators: use multiple (two or more) fea-
tures for
the generation of a new one. We have im-
plemented ﬁve operators in this group: GroupByThenMax,
GroupByThenMin, GroupByThenAvg, GroupByThenStdev and
GroupByThenCount. These operators implement
the SQL-
based operations with the same name.

It is important to point out that in our experiments we only
use a small set of operators. Additional operators can be easily
created and added to our framework, including many domain-
speciﬁc operators for ﬁelds such as biology or time-series
analysis.

2) Generating the Candidate Features Set: We generate
the candidate features by applying the operators in the follow-
ing order:

1. We apply the unary operators on all possible features
in the features set. We use this step to create Fu,i, the
normalized and discretized versions of all non-discrete
features in Fi.

2. W apply the binary and higher-order operators on the
uniﬁed set Fi ∪ Fu,i. All possible valid feature combina-
tions are generated. We denote the features generated in
this step as Fo,i.

3. For every applicable (i.e. non-discrete) feature in Fo,i, we
once again apply all the unary operators. We denote the
features set generated by this step as Fou,i.

i

of all generated feature sets: F cand
In order to limit the size of F cand

4. The ﬁnal candidate features set for iteration i is the union
= Fu,i ∪ Fo,i ∪ Fou,i.
, the features are only
combined once: none of the generated candidate features is
re-used to generate additional candidate features.
Example: applying multiple operators. Consider the follow-
ing example from the Pima dataset.1 The goal is to identify
women with risk of diabetes. Two features from this dataset

i

1http://www.openml.org/d/37

are BMI (body mass index) and the number of pregnancies.
One possible way to generate additional features is as follows:
1. Discretize (partition) the BMI attributes into segments:

[20, 25), [25, 30) and so forth.

2. Group all instances based on their BMI assignment.
3. For the instances of each group, ﬁnd the minimal number

of pregnancies of any member of the group.

This feature may enable a classiﬁer to infer a woman’s
relative number of pregnancies while using a smaller, more
relevant reference group. This feature is created by using an
unary discretization operator, followed by a GroupByThenMin
operator. This process is illustrated in Figure 2.

B. Ranking Candidate Features

i

The goal of this phase is to use computationally-efﬁcient
(“lightweight”) methods to rank the large number of candidate
features F cand
. We develop a novel machine learning based
approach that generates a set of meta-features F meta
for each
candidate feature f cand
. Once the meta-features are generated,
we use the ranking classiﬁer C to assign a score to each
f cand
. We train C on previously analyzed datasets (the training
i,j
process is described later in this section).

i,j

i,j

We generate two types of meta-features: dataset-based and

candidate features-based.
Dataset-based meta-features. Every dataset has multiple
characteristics that may affect the likelihood of f cand
being
effective. We generate four types of meta-features:

i,j

1. General information: general statistics on the analyzed
dataset: number of instances and classes, statistics on the
size and other statistics on Fi.

2. Initial evaluation: statistics on the current performance
of the classiﬁer when applied on Fi. The generated meta-
features include AUC, log loss and precision/recall values
at various thresholds.

3. Entropy-based measures: we partition Fi

into sub-
groups based on their type (discrete, numeric, date-time)
and calculate statistics on the Information Gain (IG) of
the features in each.

4. Feature diversity: we partition Fi into type-based groups
and use the chi-squared and paired-t test to calculate the
similarity of each pair in a group. We generate meta-
features using the tests’ statistic values.

Candidate feature-based meta-features. These meta-features
represent the interactions of each f cand
i,j with regard to Fi. Be-
cause our candidate features are generated, our meta-features
also take into account the operator(s) and features (“parent
features”) used to generate f cand
. The meta-features can be
partitioned into three groups:

i,j

1. Entropy and statistical tests for the candidate feature:
we partition Fi into subgroups based on their type and
use the chi-sqaured and paired t-tests to derive statistics
on f cand
’s correlation to each groups. In addition, we
i,j
derive entropy-based measures for f cand

.

i,j

2. General information on parent features and opera-
tors: We generate statistics representing various charac-

TABLE I: The characteristics of the datasets used in the
experiments

Num of
Features

Name

Heart
Horse
Cancer
Indian liver
Credit
Diabetes
German credit
Diabetic ret.
Contraceptive
Cardiography
Seismic bumps
Space
Wind
Puma 8
Puma 32
CPU act
CPU small
Delta elevators
Mammography
Ailerons
Web Data
Bank marketing
Vehicle IT
Vehicle Norm
Poker

Num
of Data
Points
270
368
569
585
690
768
1,000
1,155
1,473
2,126
2,584
3,107
6,574
8,192
8,192
8,192
8,192
9,517
11,183
13,750
36,974
45211
98,528
98,528
1,025,010

%
of
Minority
Class
44.4%
36.9%
37.2%
28.6%
44.4%
34.8%
30%
46.9%
22.6%
22.1%
6.5%
49.6%
46.7%
49.7%
49.6%
30.2%
30.2%
49.7%
2.3%
42.3%
24%
11.6%
50%
50%
49.8%

13
22
30
10
15
8
20
19
9
22
18
6
14
8
32
21
12
6
6
40
123
16
100
100
10

%
of
Numeric
Features
46%
31.8%
100%
90%
40%
100%
35%
100%
66.6%
100%
77%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
43.75%
100%
100%
100%

teristics of the parent features and the operator(s) that
participated in the creation of f cand
. This sub-group
includes the type of each parent feature, the range of
possible values and the number and type of operators
used.

i,j

3. Statistical tests on parent features: we derive statistics
on the inter-correlation of the parent features of f cand
,
as well as their correlation with the remaining features
of Fi. In addition, we generate information on the used
operator(s).

i,j

i,j

∈ F cand
i

We use the ranking classiﬁer C to estimate the likelihood
of each f cand
to reduce the error, based on its
meta-features. We sort the candidates by this likelihood and
create a ranked list RankedF cand
. Candidate features whose
i
probability is below a predeﬁned parameter thresholdf are
ﬁltered.

Training the ranking classiﬁer. To train a ranking classiﬁer
C capable of utilizing the meta-features, we produce a large
labeled training set from a repository of datasets. The entire
process described in this section is done “off-line”.

The training data generation process is presented in Algo-
rithm 2. The process consists of three stages: ﬁrst, for each
training dataset DT we generate the set of candidate features
f cand,1
, using the same process described
T
in Section IV-A for one iteration. By doing this we create a
large set of candidate features that can be analyzed.

... ∈ F cand

, f cand,2
T

T

In the second stage of the process we generate the meta-
features presented above for each of the candidate features.
We conduct this analysis separately for each DT . We then use
the generated meta-features to create the training set for the
ranking classiﬁer.

TABLE II: The percentage of error reduction (relative
to the original set of features) obtained by each of the
evaluated approaches using the decision tree algorithm.
IG Fast

ML Full ML Fast

Dataset Name

IG Full

initial
AUC
0.76
0.84
0.92
0.61
0.82
0.73
0.65
0.66
0.68
0.89
0.57
0.8
0.85
0.87
0.84
0.92
0.91
0.89
0.82
0.87
0.73
0.72
0.8
0.8
0.82

Heart
Horse
Cancer
Indian liver
Credit
Diabetes
German credit
Diabetic ret.
Contraceptive
Cardiography
Seismic bumps
Space
Wind
Puma 8
Puma 32
CPU act
CPU small
Delta elevators
Mammography
Ailerons
Web Data
Bank marketing
Vehicle IT
Vehicle Norm
Poker

Average
(Median)

40.6%
39.5%
63.61%
31.26%
31.8%
23.9%
11.3%
44.6%
10.6%
47.5%
11.7%
48.4%
19.9%
23.9%
31.8%
23.2%
26%
24.1%
47.51%
30.5%
1.7%
29.5%
1.6%
11.4%
57.5%
29.3%
(29.4%)

16.4%
1.6%
3.2%
25.3%
0%
0%
2.9%
8.2%
0%
8.7%
0%
2.4%
0%
0%
0%
0%
4.5%
0%
0%
0.6%
0%
0%
0%
2.3%
5.8%
3.4%
(0.3%)

20.1%
0%
28.8%
11.8%
4.6%
4.9%
6.8%
-9.4%
-2.2%
-35.3%
1%
13.7%
1.1%
18.7%
-16.2%
-20.5%
0.35%
9%
27.8%
5.1%
0.8%
7.5%
-1.6%
6.1%
57.5%
5.6%
(4.8%)

2.7%
0%
-5.9%
14.6%
0%
20.1%
12.3%
0%
0%
0%
0%
-3.3%
0%
4.7%
0%
0%
-22%
0%
0%
0%
0%
3.1%
0%
0.8%
4.4%
1.6%
(0)%

The third stage is assigning labels (“good” or “bad”) to
each candidate feature generated for DT . We use the more
computationally-expensive (wrapper) feature evaluation meth-
to determine the error reduction obtained by applying
ods
the classiﬁer on the joint set FT ∪ {f cand,i
} where FT is the
original feature set of DT . If the error after adding f cand,i
declines by more than (cid:15), we label f cand,i
as “good” and “bad”
otherwise. We assign these labels to the meta-features and use
the labeled set to train C.

T

T

T

By performing this process over a diverse group of datasets,
we created a ranking classiﬁer capable of effectively analyzing
previously unseen datasets. In our experiments, presented in
Section V, we demonstrate that our approach is effective for
datasets with large varieties in size and feature compositions.

C. Candidate Features Evaluation & Selection

The goal of this phase is to conduct a more computationally-
intensive (wrapper) evaluation on a small set of candidate
∈ F cand
features. We created this features set, RankedF cand
,
i
during the candidate feature ranking phase.

i

For each candidate feature f cand

, we
evaluate the performance of the classiﬁer on the joint feature
set Fi ∪ {f cand
}. The evaluation is conducted using k-fold
cross validation. The evaluation continues until either:

∈ RankedF cand

i,j

i,j

i

• The features set Fi ∪ {f cand

} satisﬁes the condition:
}) + (cid:15)w ≤ E(Fi), where E measures the

i,j

E(Fi ∪ {f cand
error and (cid:15)w is a predeﬁned parameter.

i,j

• The number of evaluated candidate features exceeded
the highest
is chosen, given

a predeﬁned parameter Rw. In this case,
performing member of RankedF cand

i

TABLE III: The percentage of error reduction (relative
to the original set of features) obtained by each of the
evaluated approaches using the SVM algorithm.
IG Full

ML Full ML Fast

Dataset Name

IG Fast

TABLE IV: The percentage of error reduction (relative
to the original set of features) obtained by each of the
evaluated approaches using the Random Forest algorithm.
IG Fast

ML Full ML Fast

Dataset Name

IG Full

initial
AUC
0.91
0.83
0.96
0.5
0.85
0.72
0.68
0.69
0.5
0.82
0.5
0.82
0.86
0.82
0.64
0.90
0.86
0.89
0.53
0.87
0.74
0.67
0.85
0.85
0.51

Heart
Horse
Cancer
Indian liver
Credit
Diabetes
German credit
Diabetic ret.
Contraceptive
Cardiography
Seismic bumps
Space
Wind
Puma 8
Puma 32
CPU act
CPU small
Delta elevators
Mammography
Ailerons
Web Data
Bank marketing
Vehicle IT
Vehicle Norm
Poker

Average
(Median)

-14.3%
18.3%
46.3%
9.6%
19%
9.4%
14.7%
36.5%
25%
41.6%
1.5%
27.1%
12.1%
4.2%
23.1%
23.6%
25.1%
0.8%
45.7%
2.4%
10.3%
10.3%
10.6%
10.9%
8.34%
17.4%
(14.7%)

0%
0%
0%
7.1%
0%
0%
2.2%
0%
0%
0%
0%
0%
0%
-1.4%
0.1%
0%
0%
0%
1.9%
0%
0%
2.6%
1.1%
1.3%
0%
0.6%
(0%)

-9.4%
-25.2%
3.5%
-4%
0%
0%
0%
9.6%
0%
-1.3%
0%
7.5%
-2.2%
3.1%
0%
1.2%
0%
14.6%
0%
9.4%
-3.3%
0%
0%
13.9%
0%
7.8%
-0.7%
-1.6%
18.4%
-4.3%
16.7%% 9.6%
14.7%
-3.4%
32.2%
1.1%
-1.3%
4.8%
-1.3%
0.4%
-3.5%
4.6%
(1.2%)

0%
0.9%
-18.5%
0%
0%
0%
-0.5%
0%
-1.1%
-1%
(0%)

initial
AUC
0.94
0.87
0.99
0.78
0.91
0.82
0.78
0.77
0.72
0.95
0.59
0.89
0.94
0.9
0.94
0.98
0.97
0.94
0.96
0.94
0.81
0.83
0.91
0.91
0.93

Heart
Horse
Cancer
Indian liver
Credit
Diabetes
German credit
Diabetic ret.
Contraceptive
Cardiography
Seismic bumps
Space
Wind
Puma 8
Puma 32
CPU act
CPU small
Delta elevators
Mammography
Ailerons
Web Data
Bank marketing
Vehicle IT
Vehicle Norm
Poker

Average
(Median)

15.3%
26%
52.4%
9.3%
24.8%
20.8%
16.2%
0%
12%
48.1%
7.1%
46.6%
8.6%
8%
67.7%
13.9%
13.6%
7.6%
40.6%
49.6%
0.27%
31.4%
1.3%
9.4%
13.2%
23.5%
(16.2%)

0%
0%
4.9%
3.1%
0%
0%
0%
1.6%
0%
3.9%
7.4%
1.9%
0%
0%
0.5%
10.2%
1.8%
0%
0%
0%
0%
4.1%
0%
0%
8.8%
1.9%
(0%)

22.2%
2.6%
-3.3%
-12.7%
-2.86%
-1.85%
1%
-0.1%
-9.3%
-18%
5.3%
24.4%
2.2%
5%
49.31
10%%
1.35%
6.5%
15%
12.2%
-2.3%
3.4%
-0.6%
12.2%
57%
6.87%
(2.2%)

-14.3%
-3.9%
0%
0%
2.6%
0%
3.7%
0%
0%
0%
0%
-3.4%
-3%
0%
0%
0.5%
-0.6%
-0.5%
0%
0%
0.4%
0.7%
0%
0.3%
5.1%
-0.5%
(0%)

that it exceeds the minimal threshold threasholdw. If
no candidate feature meets this criterion, no candidate
feature is selected and the search terminates.

Once a candidate feature that satisﬁes the criteria is found,
it is denoted as f select
. We then deﬁne Fi+1 ← Fi ∪ {f select
}
as the current features set for the next iteration and repeat the
process described in this section

i

i

A. Experimental Setup

V. EVALUATION

We evaluated ExploreKit on 25 supervised classiﬁcation
datasets with large variety in size, number of attributes,
feature type composition and class imbalance. All datasets are
available on the OpenML repository2 and their properties are
presented in Table I. All datasets were randomly partitioned
into training and test sets, with 66% of the data assigned to
the former. Original class ratios were maintained.

We use three types of algorithms in the evaluation: the
C4.5 decision tree algorithm, SVM and random forest. For all
algorithms we used the implementation included in version
3.7 of the Weka ML platform [5] with the default settings.

• For all purposes of feature generation ranking and evalu-
ation, the training set was partitioned into three folds of
equal size and class ratios. The same dataset partitions
were used in all datasets.

• We used random forest to train a ranking model on the
meta-features. This was done for all experiments (see
Section IV-B).

• We used the following parameters: thresholdf = 0.001,
thresholdw ≥ 0, (cid:15)w = 0.01 and Rw = 15, 000 (see
Section IV for details).

• For each dataset the evaluation consisted of 15 search
iterations, with one feature added during each iteration.
The allowed running time for each dataset was three days
– if the search exceeded that time, the last completed
iteration was considered as the ﬁnal one.

• We used a leave-one-out (LOO) approach for the training
of the ranking classiﬁcation model: for each evaluated
dataset di, we trained the ranking classiﬁer using meta-
features from dj ∈ D where i (cid:54)= j.

B. The Evaluated Algorithms

We evaluate two versions of our algorithm and compare

We used the following settings throughout the evaluation:
• We conducted our experiments on a 20-core machine with

them to two baselines:

64GB of RAM.

• We used the error reduction as the evaluation metric.
We calculate it using the formula (1−AU Ci)−(1−AU Cf )
,
where AU Ci and AU Cf are the AUC values obtained
for Fi and Fi ∪ {f cand

}, respectively.

(1−AU Ci)

i,j

2http://www.openml.org/home

• ML Full – the full algorithm presented in Section IV.
• ML Fast – we use the candidate ranking method as in
ML Full, but evaluate only the top ranking candidate (ties
are broken randomly). In this experiment the parameter
(cid:15)w is set to 0 (i.e. the set Fi ∪ {f cand
} needs only not
to have reduced performance compared to Fi). If this
condition is met, the candidate feature is added to Fi and

i,j

Algorithm 2 Generating metafeatures from background datasets

metaFeaturesSet ← ∅
originalP erf ormance ← wrapperEvaluator.Rank((dataset.GetF eatures())
candidatePool ← GenerateCandidateFeatures(dataset.GetFeatures())
for each (candidateFeature) in candidatePool do

1: procedure METAFEATURES(dataset, operators, ﬁlterEvaluator, wrapperEvaluator, improvementThreshold)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:

candidateFeatureMetaSet ← GenerateMetaFeatures(candidateF eature, dataset)
candidateF eatureP erf ormance ← wrapperEvaluator.Rank(Dataset, candidateF eature)
if candidateFeaturePerformance ≥ originalPerformance + improvementThreshold then

candidateF eatureM etaSet.SetLabel(True)

else

candidateF eatureM etaSet.SetLabel(False)
metaFeaturesSet ← candidateF eatureM etaSet

12:

return metaFeaturesSet

a new search iteration begins. If not, we terminate the
search and return Fi as the ﬁnal features set.

• IG Full – this baseline method is identical to ExploreKit
but utilizes Information Gain (IG) for the ranking of
candidate features instead of our ML-based approach.
• IG Fast – This baseline is identical to ML Fast but

utilizes Information Gain as the ranking function.

C. Evaluation Results

Tables II–IV show the results of the evaluation. For each
classiﬁer we compare the results of the evaluated algorithms
to those obtained on the original datasets. ML Full signiﬁ-
cantly outperforms all other methods, achieving an average
error reduction of 17.4%–29.3% over the different classiﬁers,
compared to 1.2%–5.6% for IG Full (the closest competitor).
The results also illustrate the merits of our ML-based
approach, which outperforms the IG-based methods both in
ranking-only and ranking-and-evaluation settings. Another im-
portant advantage is robustness: throughout the evaluation,
both ML-based variations had a negative impact on only a
single dataset compared with 22 and 17 for IG Full and
IG Fast respectively.

VI. DISCUSSION

We analyzed the meta-features used by our approach iden-
tify those that had the nost infulence on the classiﬁcation
model. We identiﬁed the following three groups:

• Intial Evaluation Measures, dataset-based meta-
features – indicative meta-features of this group include
statistics on the IG scores of the dataset’s features and
recall/precision values.

• Feature Diversity Measures, dataset-based meta-
features – the statistical tests we used to calculate the
correlation among the dataset’s features (particularly the
non-discrete) were given a large weight.

• Entropy & Statistical Tests,

features – speciﬁcally, the correlation of f cand
calculated using the paired T and chi-squared tests.

feature-based meta-
i,j with Fi,

These ﬁndings conﬁrm our intuition that analyzing the
classiﬁer’s performance on the initial dataset as well as ana-
lyzing its feature composition are needed for effective feature
selection. To the best of our knowledge, we are the ﬁrst to use
this information in the context of feature selection.

VII. CONCLUSIONS AND FUTURE WORK

In this study we have presented ExploreKit, a novel ap-
proach for the automatic exploration of data. By both develop-
ing a method for the generation of a large number of candidate
features as well as a new type of feature selection approach,
we have been able to signiﬁcantly improve the performance
of multiple classiﬁers on a large variety of datasets.

For future work, we plan to add additional operators of var-
ious types to our framework while also leveraging knowledge
from the previously analyzed datasets for selecting subsets
of operators for different datastes. In addition, we intend
to extend our framework to include classiﬁer and parameter
selection as well as the automatic selection and application
of methods such as PCA [2]. Finally, we are interested in
combining our approach with deep learning algorithms.

VIII. ACKNOWLEDGMENT

This work was supported in part by DARPA under the award

DARPA FA8750-15-2-0104.

REFERENCES

[1] J. Dougherty, R. Kohavi, M. Sahami, et al. Supervised and unsupervised
discretization of continuous features. In Machine learning: proceedings
of the twelfth international conference, volume 12, 1995.

[2] G. H. Dunteman. Principal components analysis, volume 69. Sage, 1989.
[3] P. A. Est´evez, M. Tesmer, C. A. Perez, and J. M. Zurada. Normalized mu-
tual information feature selection. Neural Networks, IEEE Transactions
on, 20(2), 2009.

[4] S. Garcia, J. Luengo, J. A. S´aez, V. L´opez, and F. Herrera. A survey of
discretization techniques: Taxonomy and empirical analysis in supervised
learning. Knowledge and Data Engineering, IEEE Transactions on, 25(4),
2013.

[5] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H.
Witten. The weka data mining software: an update. ACM SIGKDD
explorations newsletter, 11(1), 2009.

[6] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,
A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, et al. Deep neural
networks for acoustic modeling in speech recognition: The shared views
of four research groups. IEEE Signal Processing Magazine, 29(6):82–97,
2012.

[7] J. M. Kanter and K. Veeramachaneni. Deep feature synthesis: Towards
In Data Science and Advanced
automating data science endeavors.
Analytics (DSAA), 2015. 36678 2015. IEEE International Conference on.
IEEE, 2015.

[8] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei.
International
ImageNet Large Scale Visual Recognition Challenge.
Journal of Computer Vision (IJCV), 115(3), 2015.

