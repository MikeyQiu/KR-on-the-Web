CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor
Decompositions

Kevin Tian * 1 Teng Zhang * 2 James Zou 3

8
1
0
2
 
l
u
J
 
7
 
 
]
L
C
.
s
c
[
 
 
2
v
9
3
8
7
0
.
2
0
8
1
:
v
i
X
r
a

Abstract
Word embedding is a useful approach to cap-
ture co-occurrence structures in large text corpora.
However, in addition to the text data itself, we of-
ten have additional covariates associated with indi-
vidual corpus documents—e.g. the demographic
of the author, time and venue of publication—and
we would like the embedding to naturally capture
this information. We propose CoVeR, a new ten-
sor decomposition model for vector embeddings
with covariates. CoVeR jointly learns a base em-
bedding for all the words as well as a weighted
diagonal matrix to model how each covariate af-
fects the base embedding. To obtain author or
venue-speciﬁc embedding, for example, we can
then simply multiply the base embedding by the
associated transformation matrix. The main ad-
vantages of our approach are data efﬁciency and
interpretability of the covariate transformation.
Our experiments demonstrate that our joint model
learns substantially better covariate-speciﬁc em-
beddings compared to the standard approach of
learning a separate embedding for each covari-
ate using only the relevant subset of data, as well
as other related methods. Furthermore, CoVeR
encourages the embeddings to be “topic-aligned”
in that the dimensions have speciﬁc independent
meanings. This allows our covariate-speciﬁc em-
beddings to be compared by topic, enabling down-
stream differential analysis. We empirically eval-
uate the beneﬁts of our algorithm on datasets, and
demonstrate how it can be used to address many
natural questions about covariate effects.

Accompanying code to this paper can be found at
http://github.com/kjtian/CoVeR.

*Equal contribution 1Department of Computer Science, Stan-
ford University 2Department of Management Science and Engi-
neering, Stanford University 3Department of Biomedical Data
Science Stanford University. Correspondence to: Kevin Tian <kj-
tian@stanford.edu>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

1. Introduction

The use of factorizations of co-occurrence statistics in learn-
ing low-dimensional representations of words is an area that
has received a large amount of attention in recent years, per-
haps best represented by how widespread algorithms such
as GloVe (Pennington et al., 2014) and Word2Vec (Mikolov
et al., 2013) are in downstream applications. In particular,
suppose we have a set of words i ∈ [n], where n is the size
of the vocabulary. The aim is to, for a ﬁxed dimensionality
d, assign a vector vi ∈ Rd to each word in the vocabulary
in a way that preserves semantic structure.

In many settings, we have a corpus with additional covari-
ates on individual documents. For example, we might have
news articles from both conservative and liberal-leaning
publications, and using the same word embedding for all
the text can lose interesting information. Furthermore, we
suggest that there are meaningful semantic relationships
that can be captured by exploiting the differences in these
conditional statistics. To this end, we propose the following
two key questions that capture the problems that our work
addresses, and for each, we give a concrete motivating ex-
ample of a problem in the semantic inference literature that
it encompasses.

Question 1: How can we leverage conditional co-
occurrence statistics to capture the effect of a covariate
on word usage?

A simple example of a covariate one may wish to condi-
tion on is the document the writing came from (i.e. for a
group of books, learn a set of word embeddings for each
book). There are many natural ways to capture the effect of
this conditioning; we choose to do so by representing the
covariate as a vector. It is interesting to see if the resulting
vectors cluster by author, for example. An example appli-
cation is addressing the question: did William Shakespeare
truly write all the works credited to him, or have there been
other “ghostwriters” who have contributed to the Shake-
speare canon? This is the famous Shakespeare authorship
question, for which historians have proposed various candi-
dates as the true authors of particular plays or poems (Hope,
1994). If the latter scenario is the case, what in particular
distinguishes the writing style of one candidate from an-

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

other, and how can we infer who the most likely author of a
work is from a set of candidates? We show that it is possible
to simultaneously address the authorship question, and also
learn covariate vectors which are interpretable in terms of
their effects on word usage (measured by reweighting the
importance of topics). This motivates our next question.

Question 2: Traditional factorization-based embedding
methods are rotationally invariant, so that individual di-
mensions do not have semantic meaning. How can we break
this invariance to yield a model which aligns topics with
interpretable dimensions?

There has been much interest in the differences in language
and rhetoric that appeal to different demographics. For
example, studies have been done regarding “ideological sig-
natures” speciﬁc to voters by partisan alignment (Robinson
et al.) in which linguistic differences were proposed along
focal axes, such as the “mind versus the body” in texts with
more liberal or conservative ideologies. How can we sys-
tematically infer topical differences such as these between
different communities?

Questions such as these, or more broadly covariate-speciﬁc
trends in word usage, motivated this study. Concretely, our
goal is to provide a general framework through which em-
beddings of sets of objects with co-occurrence structure,
as well as the effects of conditioning on particular covari-
ates, can be learned jointly. As a byproduct, our model
also gives natural meaning to the different dimensions of
the embeddings, by breaking the rotational symmetry of
previous embedding-learning algorithms, such that the re-
sulting vector representations of words and covariates are
“topic-aligned”.

Our Contributions Our main contributions are CoVeR,
a decomposition algorithm that addresses the goals and
issues discussed in the introduction, and the methods for
systematic analysis we propose. Namely, we propose a
method which pools information between conditional co-
occurrence statistics to learn covariate-speciﬁc embeddings
in a data-efﬁcient way, as well as an interpretable embed-
ding of covariate vectors. We evaluate our method against
conditional GloVe as well as the related method of (Rudolph
et al., 2017).

Paper Organization We discuss related works in section
2 and in section 3, we provide our embedding algorithm, as
well as mathematical justiﬁcation for its design. In section
4, we describe our dataset. In section 5, we validate our
algorithm with respect to intrinsic properties and standard
metrics. In section 6, we propose several experiments for
systematic downstream analysis.

2. Background and related works

When designing an algorithm for learning covariate-speciﬁc
embeddings, we identiﬁed three main goals that the algo-
rithm should satisfy. The algorithm should be 1) covariate-
speciﬁc, 2) data-efﬁcient, and 3) interpretable. Goals 1 and
3 go without saying; goal 2 arises in our setting because of-
tentimes, the conditional co-occurrence counts can be quite
small, especially when there are many covariates. We deﬁne
all three in very general terms, and use them to evaluate
existing methods which can be used to handle our condi-
tional embedding learning task. In this section, we describe
several existing approaches and how well they address these
goals, which will serve as the basis for our evaluation later
in the paper.

Conditional GloVe As a brief introduction to embedding
methods, all such algorithms generally rely on the intuition
that some function of the co-occurrence statistics is low
rank. Studies such as GloVe and Word2Vec proposed based
on minimizing low-rank approximation-error of nonlinear
transforms of the co-occurrence statistics. let A be the n × n
matrix with Aij the co-occurrence between words i and j,
where co-occurrence is deﬁned as the (possibly weighted)
number of times the words occur together in a window of
ﬁxed length. For example, GloVe aimed to ﬁnd vectors
vi ∈ Rd and biases bi ∈ R such that the loss

J(v, b) =

f (Aij)(vT

i vj + bi + bj − log Aij)2

(1)

n
(cid:88)

i,j=1

was minimized, where f was some ﬁxed increasing weight
function. Word2Vec aimed to learn vector representations
via minimizing a neural network-based loss function (it can
be shown that Word2Vec and GloVe are essentially per-
forming the same factorization with respect to a different
loss function). A related embedding approach is to directly
perform principal component analysis on the PMI (point-
wise mutual information) matrix of the words (Bullinaria &
Levy). PMI-factorization based methods aim to ﬁnd vectors
P(i,j)
{vi} such that vT
i vj ≈ P M I(A)ij = log
P(i)P(j) , where
the probabilities are taken over the co-occurrence matrix.
This is essentially the same as ﬁnding a low-rank matrix V
such that V T V ≈ P M I, and empirical results show that
the resulting embedding captures useful semantic structure.

A simple baseline for learning conditional embeddings thus
is simply performing GloVe (or another non-conditional
embedding learning method) on each conditional co-
occurrence matrix, and using the resulting vectors as our
conditional embedding. This certainly satisﬁes goal 1 that
we deﬁned earlier, but neither goal 2 nor goal 3 is addressed.
Regarding goal 2, this naive method does not pool infor-
mation between the co-occurrence matrices, and thus is
inefﬁcient in its learning of semantic meaning, which is

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

very relevant when the conditional counts are not large (for
example, if the covariate is time intervals, the conditional
counts can be very small), which we show empirically. Re-
garding goal 3, because of the rotational invariance of the
method, there is no straightforward way to relate the condi-
tional embeddings to one another. There has been previous
work which learns a different set of embeddings on each
conditional slice and then tries to postprocess to align these
embeddings, such as (Kim et al., 2014). However, these
works still run into the data efﬁciency issues we describe,
and it is difﬁcult to analyze the quality of the alignment (for
example, the embedding algorithm on different slices of the
tensor may converge to minima which are not related by a
rotation, therefore an alignment may not exist).

Tensor-based Conditional Embedding Methods An in-
teresting recent work related to our method is the conditional
embedding method S-EFE (Rudolph et al., 2017). Their
work also aims to learn a different set of word embeddings
for each covariate (the same problem we consider), and does
so via a neural-net based objective function. Their work
is analogous to our work up to the parameterization of the
covariates (our work parameterizes covariates via a diagonal
scaling matrix, and theirs by a small neural network). This
is similar in some ways to how Word2Vec and GloVe are
analogous, but very different in others: the primary differ-
ence between Word2Vec and GloVe is in the choice of loss
between distributions in the objective, and our work differs
from S-EFE more fundamentally in the parameterization.
Furthermore, this work does a sub-optimal job of addressing
goals 2 (due to the large number of parameters necessary
in a neural-net based objective) and 3 (interpreting parame-
ters in a nonlinear transform is difﬁcult), which we show in
evaluations.

There are a few other related works which also try to learn
grouped embeddings via tensor decomposition (Cotterell
et al., 2017) (Liu et al., 2015). However, these works con-
sider conditioning in a restricted sense (typically with re-
spect to conditioning on different semantic meanings of a
word), do not aim to interpret the context vectors, and do not
try to preserve rotational alignment with interpretable dimen-
sions. In particular, because these models do not consider
the direct representation of the covariate in the objective
function, the representations are missing key structural prop-
erties (for example topic alignment), which doesn’t allow
for meaningful downstream analysis. In this sense, our work
aims to solve a more general problem, and provides further
justiﬁcation for tensor-based embedding methods.

Multi-sense Embeddings There has been work which
aims to address the multiple-meaning problem which arises
in learning word embeddings, via multi-sense embeddings
(Reisinger & Mooney, 2010) (Neelakantan et al., 2015).

Typically, these algorithms work by allocating multiple vec-
tors for each word (either a ﬁxed count or a varying number),
and optimizing an objective function which picks out one
of the vectors for each word. For example, if words i and
j occur in the same context, the corresponding term in the
objective function might use the embeddings (or senses) of
i and j which are closest to each other. In some ways, this
is similar to our idea that the same word can have different
meanings in different contexts. However, it is addressing a
fundamentally different problem. The “senses” which are
learned correspond to differing meanings of a word, whereas
our work relies on the intuition that across contexts, a word
will have roughly similar meanings, but usage may differ
in some ways, perhaps with respect to certain topics. Fur-
thermore, while multi-sense embeddings pose an interesting
preliminary way to address our problem, they are certainly
neither covariate-speciﬁc (goal 1), nor interpretable (goal 3,
in that the different learned sense vectors do not need to be
related to each other for the same word).

3. CoVeR: Motivation and Embedding

Algorithm

Notation Throughout this section, we will assume a vo-
cabulary of size n and a discrete covariate to condition on,
where the covariate can take on m values (for example, if
the covariate is the community that the corpus comes from,
i.e. liberal or conservative discussion forums, m is simply
the number of communities). It is easy to see how our al-
gorithm generalizes to higher-order tensor decompositions
when there are multiple dimensions covariates to condition
on (for example, slicing along community and slicing along
timeframe simultaneously). Words will be denoted with
indices i, j ∈ [n] and covariates with index k ∈ [m]. All
embedding vectors will be in Rd, and dimensions in our
embedding are referred to by index t ∈ [d].t
We will denote the co-occurrence tensor as A ∈ Rn×n×m,
where Aijk denotes how many times words i and j occurred
together within a window of some ﬁxed length, in the corpus
coming from covariate k. The result of our algorithm will
be two sets of vectors, {vi ∈ Rd} and {ck ∈ Rd}, as well
as bias terms that also ﬁt into the objective. Finally, let (cid:12)
denote the element-wise product between two vectors.

Objective Function and Discussion Here, we give the
objective function our method minimizes, and provide some
explanation for how one should imagine the effect of the
covariate weights. The objective function we minimize is
the following partial non-negative tensor factorization ob-
jective function for jointly training word vectors and weight
vectors representing the effect of covariates, adapted from
the original GloVe objective (note that ck (cid:12)vi = diag(ck)vi,
where diag(ck) is the diagonal matrix weighting of covari-

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

ate k), which we call J(v, c, b):

n
(cid:88)

m
(cid:88)

i,j=1

k=1

f (Aijk)((ck(cid:12)vi)T (ck(cid:12)vj)+bik+bjk−log Aijk)2

(2)
which is to be optimized over {vi ∈ Rd}, {ck ∈ Rd}, and
{bik ∈ R}. To gain a little more intuition for why this is a
reasonable objective function, note that the resulting objec-
tive for a single “covariate slice”, Jk(v, c, b), is essentially

n
(cid:88)

i,j=1

f (Aijk)((ck (cid:12) vi)T (ck (cid:12) vj) + bik + bjk − log Aijk)2

(3)
which ﬁts the vectors ck (cid:12) vi to the data, thus approximat-
ing the statistic log Aijk with (cid:80)d
kt. Note that in
the case m = 1, the model we use is identical to the stan-
dard GloVe model since the ck can be absorbed into the vi.
We used f (x) = ( min(100,x)
)0.75, to parallel the original
100
objective function in (Pennington et al., 2014).

t=1 vitvjtc2

One can think of the dimensions our model learns as inde-
pendent topics, and the effects of the covariate weights ck
as up- or down-weighting the importance of these topics in
contributing to the conditional co-occurrence statistics.

A Geometric View of Embeddings and Tensor Decom-
position We provide a geometric perspective on our
model. Throughout this section, note at a high level, the aim
of our method is to learn sets {vi ∈ Rd}, {ck ∈ Rd}, and
{bik ∈ R} for 1 ≤ i ≤ n, 1 ≤ k ≤ m, such that for a ﬁxed
k, the vectors {ck (cid:12) vi} and the biases {bik} approximate
the vectors and biases learned from running GloVe on only
the kth slice of the co-occurrence tensor.

We now provide some rationale for why this is a reasonable
objective. The main motivation for our algorithm is that
under standard distributional assumptions, the uniform dis-
tribution that word vectors are sampled from can be seen as
a sphere (Arora et al., 2016), which has been experimentally
veriﬁed. A natural way to model the effect of condition-
ing on a covariate is to replace this spherical distribution
with an ellipse, which is the equivalent of reweighting the
dimensions of the word vectors with respect to some basis.
We model this effect as multiplying the embedding vectors
themselves by a symmetric PSD matrix, speciﬁc to the co-
variate which is being conditioned on. In this framework,
assign each covariate k its own symmetric PSD matrix, Bk.
It is well-known that any symmetric PSD matrix can be
factorized as Bk = RT
k DkRk for some orthonormal basis
Rk and some (nonnegative) diagonal Dk; it thus sufﬁces to
consider the effect of a covariate on some ground truth “base
embedding” M as applying the linear operator Bk to each
embedding vector, resulting in the new embedding BkM .

This model is quite expressive in its own right, but we con-

Figure 1. The effects of conditioning on covariates (covariates are
discussion forums, described in Section 4). Left: baseline em-
bedding with some possible word embedding positionings. Mid-
dle, right: embedding under effect of covariates. For example,
“hillary” and “crooked” are pushed closer together under effects
of The Donald, and “healthcare” pushed closer to “universal” and
“free” under effects of SandersForPresident. “Gun” is moved closer
to “rights” and further from “control” under The Donald, and vice
versa in SandersForPresident.

sider a natural restriction where there exists one basis R
under which the resulting embeddings are affected by co-
variates via multiplication by a diagonal matrix, instead of a
PSD matrix (Fig.1). In particular, we note that

k D2

M T BT

k DkRkRT

k BkM = M T RT

k DkRkM = M (cid:48)T

kM (cid:48)
k
(4)
where M (cid:48)
k = RkM is a rotated version of M . Now, in the
restricted model where all the Rk are equal, we can write
all the M (cid:48)
k as RM , so it sufﬁces to just consider the rota-
tion of the basis that the original embedding was trained in
where R is just the identity (since matrix-factorization based
word embedding models are rotation invariant). Under this
model, the co-occurrence statistics under some transforma-
tion should be equivalent to M T D2
kM . Now, clearly this
is the same as ﬁnding a scaling vector ck (such that Dk is
diag(ci)) and using the vectors vi · ck in the factorization
objective function, exactly which is implied by equation 4.

Note that this is essentially saying that in this distributed
word representation model, there exists some rotation of the
embedding space under which the effect of the covariate
separates along dimensions. The implication is that there
are some set of independent “topics” that each covariate will

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

upweight or downweight in importance (or possibly ignore
altogether with a weight of 0), characterizing the effect of
the conditioning directly in terms of the effect on topics.

Algorithm Details Our model learns the resulting param-
eters {vi ∈ Rd}, {ck ∈ Rd}, and {bik}, by using the Adam
(Kingma & Ba, 2014) algorithm, which was empirically
shown to yield good convergence results in the original
GloVe setting. The speciﬁc hyperparameters used for each
dataset will be described in the next section. All vectors
were initialized as random unit vectors 1.

4. Dataset

We evaluated CoVeR in two primary datasets.
Co-
occurrence statistics were formed by considering size 8 win-
dows and using an inverse-distance weighting (e.g. words
3 apart had 1
3 added), which was suggested by some imple-
mentations of (Pennington et al., 2014).

The ﬁrst dataset, referred to as the “book dataset”, con-
sists of the full text from 29 books written by 4 different
authors. The books we used were J.K. Rowling’s “Harry
Potter” series (7 books), “Cormoran Strike” series (3 books),
and “The Casual Vacancy”; C. S. Lewis’s “The Chronicles
of Narnia” series (7 books), and “The Screwtape Letters”;
George R. R. Martin’s “A Song of Ice and Fire” series (5
books); and Stephenie Meyer’s “Twilight” series (4 books),
and “The Host”. These books are ﬁction works in similar
genres, with highly overlapping vocabularies and common
themes. A trivial way of learning series-speciﬁc tenden-
cies in word usage would be to cluster according to unique
vocabularies (for example, only the “Harry Potter” series
would have words such as “Harry” and “Ron” frequently),
so the co-occurrence tensor was formed by looking at all
words that occurred in all of the series with multiple books,
which eliminated all series-speciﬁc words. Furthermore,
series by the same author had very different themes, so there
is no reason intrinsic to the vocabulary to believe the weight
vectors would cluster by author. The vocabulary size was
5,020, and after tuning our algorithm to embed this dataset,
we used 100 dimensions and a learning rate of 10−5.

The second dataset, the “politics dataset”, was a collection
of comments made in 2016 in 6 different subreddits on the
popular discussion forum reddit, and was selected to address
both Questions 1 and 2. The covariate was the discussion
forum, and the subreddits we used were AskReddit, news,
politics, SandersForPresident, The Donald, and WorldNews.

1We also experimented with initializing covariate weight vec-
tors as random vectors centered around the all 1 vector. This
initialization also yielded the sparsity patterns discussed in the
next section, but converged at a slower rate, and performed simi-
larly on downstream metrics as initializing near all 0, so we kept
this initialization.

AskReddit was a baseline discussion forum with a very gen-
eral vocabulary usage, and the discussion forums for the
Sanders and Trump support bases were also selected, as
well as three politically-relevant but more neutral commu-
nities (it should be noted that the politics discussion forum
tends to be very left-leaning). We considered a vocabulary
of size 15,000, after removing the 28 most common words
(suggested by personal communication amongst the word
embedding community) and entries of the cooccurrence ten-
sor with less than 10 occurrences (for the sake of training
efﬁciency). The embedding used 200 dimensions and a
learning rate of 10−5.

5. Experimental Validation

5.1. Clustering by Weights

We performed CoVeR on the book dataset, and considered
how well the weight vectors of the covariate clustered by se-
ries and also by author. We also considered clustering of the
covariate parameter vectors learned from the S-EFE method.
As a baseline, we considered the topic breakdown on the
same co-occurrence statistics predicted by Latent Dirichlet
Allocation (Blei et al., 2003) with various dimensionalities
(20, 50, 100). For a visualization, see Fig. 2.

For every book in every series, the closest weight vectors
(via the tensor decomposition algorithm) by series were
all the books in the same series. Furthermore, for every
book written by every author, the closest weight vectors
by author were all the books by the same author. This
clear clustering behavior even when only conditioning on
co-occurrence statistics for words that appear in all series
(throwing out series-speciﬁc terms) implies the ability of
the weight vectors to cluster according to higher-order in-
formation, perhaps such as writing style.

In contrast, S-EFE is does not learn interpretable features for
each covariate; simply plotting the S-EFE parameter vector
does not cluster the books by the same author. We recognize
that the comparison of the PCA-projected S-EFE parameter
vectors should not be expected to do well (we tried nonlin-
ear dimension reduction techniques such as T-SNE under
various parameter settings as well, and in no setting did the
projections cluster meaningfully); nonetheless, using the
parameter vectors seemed to be the most natural way of
measuring interpretability with respect to the covariates. All
parameter settings of LDA failed to produce any meaningful
clusters.

5.2. Speciﬁcity of Conditional Embeddings

When measuring comparability across embeddings, one
desirable feature of an embedding algorithm is that it re-
distances words with respect to how much the meaning
changes across covariates. For example, a common word

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

(a) 2D PCA of book dataset weight vec-
tors (CoVeR, 100 dimensions)

Figure 3. Histogram of average pairwise cosine similarity for
context-speciﬁc embeddings of a given word.

(b) 2D PCA of neural net parameter vec-
tors (S-EFE, 100 dimensions).

(c) 2D PCA of book dataset topic vectors,
as predicted by LDA (100 topics)

Figure 2. Visualization of topic vectors, algorithm comparison

such as “the” or “a” has essentially the same meaning in
almost every situation, so the resulting embedding should
not change very much. However, for a word that may have
very different meanings in different contexts (such as “immi-
gration”, which might have different connotations in liberal
vs. conservative texts), we expect the covariate-speciﬁc
embeddings to be more spread out.

A simple way to quantitatively evaluate this notion of “vari-
ance in meaning = spread in embeddings” is to, for each
word, consider the average pairwise cosine distance between
covariate-speciﬁc embeddings. For example, for the word
“dark”, using the book dataset, there will be 29 different
embeddings corresponding to each different set of covari-
ate cooccurrences; over all pairs of books, take the average
cosine distance between the embeddings of the given word.
We plotted the distribution of this average across all words.

The results are quite striking: neural embedding method
such as S-EFE is unable to distinguish between the speci-

ﬁcities of words. This could be a result of the neural net
parameterization learning a different subspace for each co-
variate, which makes comparing embeddings across groups
infeasible. However, CoVeR is able to relate the covariate-
speciﬁc embeddings to each other via a linear re-weighting,
giving meaning to comparisons between the embeddings.
As a baseline, the mean distance for common prepositions
(which we take to represent “stable” words which should
not drift too much under conditioning) is plotted; they were
much more stable in our embedding, as expected.

5.3. Data Efﬁciency and Validation

Consider the problem of learning an embedding for the text
of a particular book (or series). This is fundamentally a
covariate-speciﬁc embedding problem, so we test the data
efﬁciency of our algorithm with respect to two alternative
methods of learning conditional embeddings. The main
advantage given by using CoVeR over applying GloVe to
the individual slices is one of data efﬁciency: by pooling the
co-occurrence statistics of words across other books, we are
able to give less noisy estimates of the vectors, especially
for rare or nonexistent words. We also show that our method
compares very favorably with respect to S-EFE. Individual
books contained between 26747 and 355814 words.

To this end, we performed the following experiment. For
some book k, consider three embeddings: 1) the result of
performing GloVe on the co-occurrence statistics of just
the book, 2) the context-speciﬁc embedding produced by
S-EFE with dimension 100 (different dimensionalities com-
pared similarly) 3) the (weighted) context-speciﬁc embed-
ding resulting from CoVeR. Then, we tested these resulting
embeddings using a standard suite of evaluation metrics
(Jastrzebski et al., 2017), including cluster purity and corre-
lation similarities. Our method outperformed method 1 on
all tasks and method 2 on all but one, often signiﬁcantly.

5.4. Sparsity of Weight Vectors

Because of experimentally veriﬁed isotropic distributional
properties of word embedding vectors (Arora et al., 2016), it
is unreasonable to ask for sparsity in the word embeddings

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

Table 1. Average performance on Harry Potter books, cluster purity.
Larger scores indicate better performance.

Cluster purity
BLESS

AP

CoVeR 0.1602
S-EFE
0.1289
0.1297
GloVe

0.2185
0.1942
0.2042

Battig

0.0877
0.0745
0.0770

Table 2. Average performance on Harry Potter books, correlation
similarities. Larger scores indicate better performance.

Correlation similarities

MEN MTurk

RG65

RW

CoVeR 0.1370
0.0184
S-EFE
0.0593
GloVe

0.0719
0.0986
0.0341

0.1272
-0.0098
0.0133

0.0902
-0.0753
0.0588

themselves. However, our topic-speciﬁc weighting scheme
for the covariate weight vectors implies that sparsity is de-
sirable in the weights. The weight sparsity resulting from
our algorithm was experimentally veriﬁed through many
runs of our embedding method, as well as across different
optimization methods, which to us was a surprising and
noteworthy artifact of our algorithm.

ing with regularizing the word vectors forced the model
into a smaller subspace (i.e. sparse dimensions existed but
were shared across all the words), which is not useful and
provides further evidence for a natural isotropic distribution.

Table 3. Sparse coordinate count, covariate weight vectors, 5 runs.
world
23.2
3.1

politics
25.6
2.2

Sanders Donald

news
16.8
2.4

Ask
18.2
1.4

mean
std

36.8
3.6

27.2
2.6

To conﬁrm that the sparsity was a result of separation of
covariate effects rather than an artifact of our algorithm,
we ran our decomposition on a co-occurrence tensor which
was the result of taking the same slice (subreddit) and sub-
sampling its entries 3 times, creating 3 slices of essentially
similar co-occurrence statistics. We applied our algorithm
with different learning parameters, and the resulting weight
vectors after 90 iterations (when the outputs vectors con-
verged) were extremely non-sparse, with between 0 and 2
sparse coordinates per weight vector. The dimensions that
are speciﬁcally 0 for a covariate corresponds to topics that
are relatively less relevant for that covariate. In the next
section, we develop methods to systematically interpret the
covariate weights in terms of topics.

6. Interpretation

6.1. Inference of Topic Meaning

One interesting problem to the word embedding community
is that of topic inference and modeling, that is discover-
ing underlying topics via analyzing co-occurrence statistics
over corpora (Arora et al., 2013). A simple test of infer-
ring topic meaning (i.e. topics coordinates are associated
with) is to consider the set of words which are large in the
given coordinate. Concretely the task is, given some index
t ∈ [d], output the words whose (normalized) vectors have
largest absolute value in dimension t. We show the results
of this experiment for several of the sparse coordinates in
the AskReddit weight vector:

Figure 4. Histogram, sizes of weights in covariate vectors for pol-
itics dataset. Smallest bucket is zero (< 10−10). Sparsity of
covariates in book dataset deferred to appendix.

Note that in the objective function (3), sparse coordinates
will become “stuck” at 0, because the gradient update ∂J
∂ckt
of ckt is proportional to ckt:

Table 4. Representative sample of top words for selected dimen-
sions: topic inference task. Each word was in top 20 normalized
coordinate values for the dimension t.

n
(cid:88)

i,j=1

4f (Aijk)((ck(cid:12)vi)T (ck(cid:12)vj)+bik+bjk−log Aijk)ckt

(5)
The dimensions that the covariates were sparse in did not
overlap by much: on average, weight vectors had 20.7 sparse
coordinates, and the average number of coordinates that was
sparse in both vectors of a random pair was 5.2. This sug-
gests that the set of “topics” each conditional slice of the
tensor does not care about is fairly speciﬁc. Experiment-

t
99
120
183
194

Highest weighted words, dimension t Meaning
horses, cat, teenager, grandma
tables, driveway, customer, stations
gate, territory, directions, phillipines
sweat, disciplined, beliefs, marines

people/animals
domestic
foreign
military

There are several conclusions to be drawn from this ex-
periment. Firstly, while there is some clear noise, speciﬁc
topic meanings do seem to appear in certain coordinates
(which we infer in the table above). It is reasonable that

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

meaning would appear out of coordinates which are sparsely
weighted in some covariate, because this means that it is
a topic that is relevant in other discussion forums but pur-
posely ignored in some speciﬁc forum, so it is likely to have
a consistent theme. When we performed this task for co-
ordinates which had low variance in their weights between
covariates, the resulting words were much less consistent.

It is also interesting to see how covariates weight a topic
with an identiﬁed meaning. For example, for coordinate 194
(military themes), AskReddit placed negligible weight, news
and worldnews placed weight 2.06 and 2.04, SandersFor-
President and The Donald placed weight 0.41 and 0.38,
and politics placed weight 0.05. This process can also
be reversed - for example, consider coordinates small in
worldnews and large in news. One example was coordi-
nate 188; upon checking the words large in this coordinate
({taser, troops, supremacists, underpaid, rioters, amendment,
racially}) it clearly had themes of rights and protests, which
makes sense as a domestic issue, not a global one.

6.2. Topical Word Drift

We performed the following experiment: which pairs of
words start off close in the baseline embedding, yet under
some covariate weights move far apart (or vice versa)? Con-
cretely, the task is, for a ﬁxed word i and a ﬁxed covariate k,
to identify words j such that ||ck(cid:12)vi−ck(cid:12)vj|| (cid:29) ||vi−vj||
or ||ck (cid:12)vi −ck (cid:12)vj|| (cid:28) ||vi −vj||, where the magnitude of
drift is quantiﬁed by the ratio of distances in the normalized
embedding. The motivation is to ﬁnd words whose general
usage is similar, but have very different meanings in speciﬁc
communities. We present a few representative examples of
this experiment, for k = The Donald.

Table 5. Representative examples of word drift for The Donald.

Word i
Drift dir. Words with strongest drift
Closer
hillary
Further
hillary
Closer
gun
gun
Further
immigrant Closer
Further
immigrant

crooked, lying, shillary, killary
electable, compromise, favored
merchandise, milo, ﬂair, fakenews
assault, child, fanatics, problem, police
unauthorized, uninsured, parasite
child, abused, future, attorneys, protect

Combining the previous two sections allows us to do an end-
to-end case study on words that drift under a covariate, so
we can explain speciﬁcally which topics (under reweighting)
caused this shift. For example, the words “immigrant” and
“parasite” were signiﬁcantly closer under the weights placed
by The Donald, so we considered dimensions that were si-
multaneously large in the vector vimmigrant − vparasite and
sparse in the weight vector cT he Donald. The dimensions
89 and 139 were sparse and also the 2nd and 3rd largest
coordinates in the difference vector, so they had a large

contribution to the subspace which was zeroed out under
the reweighting. Words that were large in these dimensions
(and thus representative of the zeroed out subspace meaning)
included {misunderstanding, populace, scapegoat, rebuild-
ing} for 89, and {answers, barriers, applicants, backstory,
selﬂess, indigenous} for 139. This suggests two indepen-
dent reasons for the drift: dimensions corresponding to
emotional appeal and legal immigration being zeroed out.

6.3. Covariate-Speciﬁc Analogies

One of the most famous downstream applications of recent
embedding methods such as (Pennington et al., 2014) and
(Mikolov et al., 2013) is representing analogies. This is
formulated as a is to b as c is to d ↔ va − vb ≈ vc − vd, for
example vwoman − vqueen ≈ vman − vking. We considered
how well our method captured covariate-speciﬁc analogies,
which appear in a covariate-speciﬁc embedding but not
most others. Our embedding was able to capture differential
meaning in the form of these speciﬁc analogies: we describe
the results more formally in the appendix. An example is
that “hillary is to liberal as trump is to (white / racist)” are
both much stronger analogies in liberal discussion forums,
and a corresponding strong analogy in conservative forums
is “... as trump is to disenfranchised”. Also, across the
board, “hillary is to politician as trump is to businessman”,
but replacing “businessman” with “conﬁdence” is an en-
riched analogy in conservative forums, and replacing with
“irrationality” is enriched in liberal ones.

Discussion We have presented a joint tensor model that
learns an embedding for each word and for each covariate.
This makes it very simple to compute the covariate speciﬁc
embedding: we just take the element-wise vector product.
It also enables us to systematically interpret the covariate
vector by looking at dimensions along which weight is large
or 0. Our experiments show that these dimensions can be
interpreted as coherent topics. While we focus on word
embeddings, our tensor covariate embedding model can be
naturally applied in other settings. For example, there is
a large amount of interest in learning embeddings of indi-
vidual genes to capture biological interactions. The natural
covariates here are the cell types and our method would
be able to model cell-type speciﬁc gene interactions. An-
other interesting setting with conditional covariates would
be time-series speciﬁc embeddings, where data efﬁciency
becomes more of an issue. We hope our framework is gen-
eral enough that it will be of use to practitioners in these
settings and others.

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

Acknowledgements

References

This research was supported by NSF Graduate Fellowship
DGE-1656518. The authors thank Tatsunori Hashimoto for
helpful comments and Lilly Shen for helping with illustra-
tions.

Arora, Sanjeev, Ge, Rong, Halpern, Yoni, Mimno, David,
Moitra, Ankur, Sontag, David, Wu, Yichen, and Zhu,
Michael. A practical algorithm for topic modeling with
provable guarantees. Proceedings of the International
Conference on Machine Learning, 2013.

Arora, Sanjeev, Li, Yuanzhi, Liang, Yingyu, Ma, Tengyu,
and Risteski, Andrej. Rand-walk: A latent variable model
approach to word embeddings. Transactions of the Associ-
ation for Computational Linguistics (TACL), 4:385–399,
2016.

Blei, David M., Ng, Andrew Y., and Jordan, Michael I.
Latent dirichlet allocation. Journal of Machine Learning
Research, 3:993–1022, 2003.

Bullinaria, John A. and Levy, Joseph P. Extracting semantic
representations from work co-occurrence statistics: A
computational study. Behavior Research Methods, 39.

Cotterell, Ryan, Poliak, Adam, Van Durme, Benjamin, and
Eisner, Jason. Explaining and generalizing skip-gram
through exponential family principal component analysis.
In Proceedings of the 15th Conference of the European
Chapter of the Association for Computational Linguistics:
Volume 2, Short Papers, 2017.

Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive
subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12.

Hope, Jonathan. The Authorship of Shakespeare’s Plays:
A Socio-Linguistic Study. Cambridge University Press,
1994.

Jastrzebski, Stanislaw, Lesniak, Damian, and Czarnecki,
Wojciech Marian. How to evaluate word embeddings?
on importance of data efﬁciency and simple supervised
tasks. ArXiv preprint, 2017.

Kim, Yoon, Chiu, Yi-I, Hanaki, Kentaro, Hegde, Darshan,
and Petrov, Slav. Temporal analysis of language through
neural language models. Proceedings of the ACL 2014
Workshop on Language Technologies and Computational
Social Science, 2014.

Kingma, Diederik and Ba, Jimmy. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.

Liu, Pengfei, Qiu, Xipeng, and Huang, Xuanjing. Learning
context-sensitive word embeddings with neural tensor
skip-gram model. In Proceedings of the Twenty-Fourth
International Joint Conference on Artiﬁcial Intelligence,
2015.

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado,
Greg S, and Dean, Jeff. Distributed representations of
words and phrases and their compositionality. pp. 3111–
3119, 2013.

Neelakantan, Arvind, Shankar, Jeevan, Passos, Alexandre,
and McCallum, Andrew. Efﬁcient non-parametric esti-
mation of multiple embeddings per word in vector space.
Conference on Empirical Methods in Natural Language
Processing, 2014, 2015.

Pennington, Jeffrey, Socher, Richard, and Manning, Christo-
pher D. Glove: Global vectors for word representation.
14:1532–1543, 2014.

Reisinger, Joseph and Mooney, Raymond J. Multi-prototype
vector-space models of word meaning. Human Language
Technologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computational
Linguistics, pp. 109–117, 2010.

Robinson, Michael D., Boyd, Ryan L., Fetterman, Adam K.,
and Persich, Michelle R. Journal of Language and Social
Psychology, 36:438–461.

Rudolph, Maja, Ruiz, Francisco, Athey, Susan, and Blei,
David. Structured embedding models for grouped data.
In Advances of Neural Information Processing Systems,
2017.

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

A. Covariate-speciﬁc analogies

We wish to learn covariate-speciﬁc analogies of the form a is to b as c is to d. To this end, we considered experiments of the
form: for ﬁxed words a, b, c, determine words d such that for some covariate k, the quantity

(ck (cid:12) va − ck (cid:12) vb) · (ck (cid:12) vc − ck (cid:12) vd)
||ck (cid:12) va − ck (cid:12) vb||||ck (cid:12) vc − ck (cid:12) vd||

(6)

is small, yet for other k, the quantity is large. The intuition is that under the covariate transform, vc − vd points roughly in
the same direction as va − vb, and d is close to c in semantic meaning.

In particular, we set a = “hillary”, c = “trump”, and found words b for which there existed a d consistently at the top across
subreddits (implying existence of strong analogies). For example, when b = “woman”, d = “man” was the best analogy for
every weighting. Then, for these b, we considered words d whose relative rankings in the subreddits had high variance. The
differential analogies captured were quite striking: the experiment is able to reveal words whose relative meaning in relation
to anchor words such as “hillary” and “trump” drifts signiﬁcantly.

Table 6. Analogies task. Each best analogy d was one of the top-ranked words in every embedding. We present words whose “relative
analogy” rank was enriched in some embedding. Subreddits are color-coded: green for news-related WN and N (worldnews, news), blue
for left-leaning P and S (politics, SandersForPresident), red for right-leaning D (The Donald), black for A (AskReddit).

Word b

Best analogy d Word

woman

man

democrat

republican

liberal

conservative

politician

businessman

abysmal
amateur
zionist
politician
president
nationalists
south
bigot
christian
white
racist
sociopathic
disenfranchised
conﬁdence
questionable
irrational

High rank
1351 (S), 2218 (P)
1543 (P), 3966 (S)
1968 (P), 2327 (S)
2796 (WN), 3155 (D)
2452 (D), 3564 (WN)
208 (S), 606 (D)
3511 (P)
400 (S), 530 (A)
33 (P)
619 (P)
252 (P)
1756 (D)
3693 (D)
2768 (D)
598 (WN)
2153 (N), 3430 (P)

Low rank
14329 (base), 14077 (D)
13840 (base), 13734 (D)
14173 (base), 14248 (A)
11959 (base), 10386 (S)
12257 (base)
8916 (base), 7526 (A)
11091 (base)
12888 (D), 12994 (WN)
12756 (D), 12722 (WN)
12824 (D), 13273 (base)
12930 (S), 12779 (D)
13744 (P), 13389 (A)
11267 (base)
10528 (base)
13002 (base)
13305 (base)

B. Sparsity results: book dataset

Number of sparse coordinates (out of 100) were as follows, by series and then book order: Harry (0, 5, 1, 3, 7, 4, 0),
Chronicles (0, 0, 0, 1, 0, 0, 0), Song (8, 8, 9, 4, 11), Twilight (6, 6, 8, 7), Screwtape (5), Strike (3, 2, 2), Host (6), Vacancy
(4). While not as dramatic as in the politics dataset, the presence of zero (rather than small) coordinates across multiple runs
shows that there still is speciﬁcity of topics being learned. We plot the histogram of coordinate sizes in the following ﬁgure.

C. Algorithm setting notes

We also experimented with using (Duchi et al.) as the optimization method, but the resulting weight vectors in the politics
dataset had highly-overlapping sparse dimensions. This implies that the optimization method tried to ﬁt the model to a
smaller-dimensional subspace, which is not a desirable source of sparsity.

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

Figure 5. Histogram, sizes of weights in covariate vectors of book data.

(a) Non-zero dimensions in weight vectors by epoch and opti-
mization method. Upper: Adam; lower: Adagrad.

(b) Non-zero dimensions in weight vectors by epoch. Upper:
initialization centered around all-1 vector; lower: centered around
all-0 vector.

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor
Decompositions

Kevin Tian * 1 Teng Zhang * 2 James Zou 3

8
1
0
2
 
l
u
J
 
7
 
 
]
L
C
.
s
c
[
 
 
2
v
9
3
8
7
0
.
2
0
8
1
:
v
i
X
r
a

Abstract
Word embedding is a useful approach to cap-
ture co-occurrence structures in large text corpora.
However, in addition to the text data itself, we of-
ten have additional covariates associated with indi-
vidual corpus documents—e.g. the demographic
of the author, time and venue of publication—and
we would like the embedding to naturally capture
this information. We propose CoVeR, a new ten-
sor decomposition model for vector embeddings
with covariates. CoVeR jointly learns a base em-
bedding for all the words as well as a weighted
diagonal matrix to model how each covariate af-
fects the base embedding. To obtain author or
venue-speciﬁc embedding, for example, we can
then simply multiply the base embedding by the
associated transformation matrix. The main ad-
vantages of our approach are data efﬁciency and
interpretability of the covariate transformation.
Our experiments demonstrate that our joint model
learns substantially better covariate-speciﬁc em-
beddings compared to the standard approach of
learning a separate embedding for each covari-
ate using only the relevant subset of data, as well
as other related methods. Furthermore, CoVeR
encourages the embeddings to be “topic-aligned”
in that the dimensions have speciﬁc independent
meanings. This allows our covariate-speciﬁc em-
beddings to be compared by topic, enabling down-
stream differential analysis. We empirically eval-
uate the beneﬁts of our algorithm on datasets, and
demonstrate how it can be used to address many
natural questions about covariate effects.

Accompanying code to this paper can be found at
http://github.com/kjtian/CoVeR.

*Equal contribution 1Department of Computer Science, Stan-
ford University 2Department of Management Science and Engi-
neering, Stanford University 3Department of Biomedical Data
Science Stanford University. Correspondence to: Kevin Tian <kj-
tian@stanford.edu>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

1. Introduction

The use of factorizations of co-occurrence statistics in learn-
ing low-dimensional representations of words is an area that
has received a large amount of attention in recent years, per-
haps best represented by how widespread algorithms such
as GloVe (Pennington et al., 2014) and Word2Vec (Mikolov
et al., 2013) are in downstream applications. In particular,
suppose we have a set of words i ∈ [n], where n is the size
of the vocabulary. The aim is to, for a ﬁxed dimensionality
d, assign a vector vi ∈ Rd to each word in the vocabulary
in a way that preserves semantic structure.

In many settings, we have a corpus with additional covari-
ates on individual documents. For example, we might have
news articles from both conservative and liberal-leaning
publications, and using the same word embedding for all
the text can lose interesting information. Furthermore, we
suggest that there are meaningful semantic relationships
that can be captured by exploiting the differences in these
conditional statistics. To this end, we propose the following
two key questions that capture the problems that our work
addresses, and for each, we give a concrete motivating ex-
ample of a problem in the semantic inference literature that
it encompasses.

Question 1: How can we leverage conditional co-
occurrence statistics to capture the effect of a covariate
on word usage?

A simple example of a covariate one may wish to condi-
tion on is the document the writing came from (i.e. for a
group of books, learn a set of word embeddings for each
book). There are many natural ways to capture the effect of
this conditioning; we choose to do so by representing the
covariate as a vector. It is interesting to see if the resulting
vectors cluster by author, for example. An example appli-
cation is addressing the question: did William Shakespeare
truly write all the works credited to him, or have there been
other “ghostwriters” who have contributed to the Shake-
speare canon? This is the famous Shakespeare authorship
question, for which historians have proposed various candi-
dates as the true authors of particular plays or poems (Hope,
1994). If the latter scenario is the case, what in particular
distinguishes the writing style of one candidate from an-

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

other, and how can we infer who the most likely author of a
work is from a set of candidates? We show that it is possible
to simultaneously address the authorship question, and also
learn covariate vectors which are interpretable in terms of
their effects on word usage (measured by reweighting the
importance of topics). This motivates our next question.

Question 2: Traditional factorization-based embedding
methods are rotationally invariant, so that individual di-
mensions do not have semantic meaning. How can we break
this invariance to yield a model which aligns topics with
interpretable dimensions?

There has been much interest in the differences in language
and rhetoric that appeal to different demographics. For
example, studies have been done regarding “ideological sig-
natures” speciﬁc to voters by partisan alignment (Robinson
et al.) in which linguistic differences were proposed along
focal axes, such as the “mind versus the body” in texts with
more liberal or conservative ideologies. How can we sys-
tematically infer topical differences such as these between
different communities?

Questions such as these, or more broadly covariate-speciﬁc
trends in word usage, motivated this study. Concretely, our
goal is to provide a general framework through which em-
beddings of sets of objects with co-occurrence structure,
as well as the effects of conditioning on particular covari-
ates, can be learned jointly. As a byproduct, our model
also gives natural meaning to the different dimensions of
the embeddings, by breaking the rotational symmetry of
previous embedding-learning algorithms, such that the re-
sulting vector representations of words and covariates are
“topic-aligned”.

Our Contributions Our main contributions are CoVeR,
a decomposition algorithm that addresses the goals and
issues discussed in the introduction, and the methods for
systematic analysis we propose. Namely, we propose a
method which pools information between conditional co-
occurrence statistics to learn covariate-speciﬁc embeddings
in a data-efﬁcient way, as well as an interpretable embed-
ding of covariate vectors. We evaluate our method against
conditional GloVe as well as the related method of (Rudolph
et al., 2017).

Paper Organization We discuss related works in section
2 and in section 3, we provide our embedding algorithm, as
well as mathematical justiﬁcation for its design. In section
4, we describe our dataset. In section 5, we validate our
algorithm with respect to intrinsic properties and standard
metrics. In section 6, we propose several experiments for
systematic downstream analysis.

2. Background and related works

When designing an algorithm for learning covariate-speciﬁc
embeddings, we identiﬁed three main goals that the algo-
rithm should satisfy. The algorithm should be 1) covariate-
speciﬁc, 2) data-efﬁcient, and 3) interpretable. Goals 1 and
3 go without saying; goal 2 arises in our setting because of-
tentimes, the conditional co-occurrence counts can be quite
small, especially when there are many covariates. We deﬁne
all three in very general terms, and use them to evaluate
existing methods which can be used to handle our condi-
tional embedding learning task. In this section, we describe
several existing approaches and how well they address these
goals, which will serve as the basis for our evaluation later
in the paper.

Conditional GloVe As a brief introduction to embedding
methods, all such algorithms generally rely on the intuition
that some function of the co-occurrence statistics is low
rank. Studies such as GloVe and Word2Vec proposed based
on minimizing low-rank approximation-error of nonlinear
transforms of the co-occurrence statistics. let A be the n × n
matrix with Aij the co-occurrence between words i and j,
where co-occurrence is deﬁned as the (possibly weighted)
number of times the words occur together in a window of
ﬁxed length. For example, GloVe aimed to ﬁnd vectors
vi ∈ Rd and biases bi ∈ R such that the loss

J(v, b) =

f (Aij)(vT

i vj + bi + bj − log Aij)2

(1)

n
(cid:88)

i,j=1

was minimized, where f was some ﬁxed increasing weight
function. Word2Vec aimed to learn vector representations
via minimizing a neural network-based loss function (it can
be shown that Word2Vec and GloVe are essentially per-
forming the same factorization with respect to a different
loss function). A related embedding approach is to directly
perform principal component analysis on the PMI (point-
wise mutual information) matrix of the words (Bullinaria &
Levy). PMI-factorization based methods aim to ﬁnd vectors
P(i,j)
{vi} such that vT
i vj ≈ P M I(A)ij = log
P(i)P(j) , where
the probabilities are taken over the co-occurrence matrix.
This is essentially the same as ﬁnding a low-rank matrix V
such that V T V ≈ P M I, and empirical results show that
the resulting embedding captures useful semantic structure.

A simple baseline for learning conditional embeddings thus
is simply performing GloVe (or another non-conditional
embedding learning method) on each conditional co-
occurrence matrix, and using the resulting vectors as our
conditional embedding. This certainly satisﬁes goal 1 that
we deﬁned earlier, but neither goal 2 nor goal 3 is addressed.
Regarding goal 2, this naive method does not pool infor-
mation between the co-occurrence matrices, and thus is
inefﬁcient in its learning of semantic meaning, which is

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

very relevant when the conditional counts are not large (for
example, if the covariate is time intervals, the conditional
counts can be very small), which we show empirically. Re-
garding goal 3, because of the rotational invariance of the
method, there is no straightforward way to relate the condi-
tional embeddings to one another. There has been previous
work which learns a different set of embeddings on each
conditional slice and then tries to postprocess to align these
embeddings, such as (Kim et al., 2014). However, these
works still run into the data efﬁciency issues we describe,
and it is difﬁcult to analyze the quality of the alignment (for
example, the embedding algorithm on different slices of the
tensor may converge to minima which are not related by a
rotation, therefore an alignment may not exist).

Tensor-based Conditional Embedding Methods An in-
teresting recent work related to our method is the conditional
embedding method S-EFE (Rudolph et al., 2017). Their
work also aims to learn a different set of word embeddings
for each covariate (the same problem we consider), and does
so via a neural-net based objective function. Their work
is analogous to our work up to the parameterization of the
covariates (our work parameterizes covariates via a diagonal
scaling matrix, and theirs by a small neural network). This
is similar in some ways to how Word2Vec and GloVe are
analogous, but very different in others: the primary differ-
ence between Word2Vec and GloVe is in the choice of loss
between distributions in the objective, and our work differs
from S-EFE more fundamentally in the parameterization.
Furthermore, this work does a sub-optimal job of addressing
goals 2 (due to the large number of parameters necessary
in a neural-net based objective) and 3 (interpreting parame-
ters in a nonlinear transform is difﬁcult), which we show in
evaluations.

There are a few other related works which also try to learn
grouped embeddings via tensor decomposition (Cotterell
et al., 2017) (Liu et al., 2015). However, these works con-
sider conditioning in a restricted sense (typically with re-
spect to conditioning on different semantic meanings of a
word), do not aim to interpret the context vectors, and do not
try to preserve rotational alignment with interpretable dimen-
sions. In particular, because these models do not consider
the direct representation of the covariate in the objective
function, the representations are missing key structural prop-
erties (for example topic alignment), which doesn’t allow
for meaningful downstream analysis. In this sense, our work
aims to solve a more general problem, and provides further
justiﬁcation for tensor-based embedding methods.

Multi-sense Embeddings There has been work which
aims to address the multiple-meaning problem which arises
in learning word embeddings, via multi-sense embeddings
(Reisinger & Mooney, 2010) (Neelakantan et al., 2015).

Typically, these algorithms work by allocating multiple vec-
tors for each word (either a ﬁxed count or a varying number),
and optimizing an objective function which picks out one
of the vectors for each word. For example, if words i and
j occur in the same context, the corresponding term in the
objective function might use the embeddings (or senses) of
i and j which are closest to each other. In some ways, this
is similar to our idea that the same word can have different
meanings in different contexts. However, it is addressing a
fundamentally different problem. The “senses” which are
learned correspond to differing meanings of a word, whereas
our work relies on the intuition that across contexts, a word
will have roughly similar meanings, but usage may differ
in some ways, perhaps with respect to certain topics. Fur-
thermore, while multi-sense embeddings pose an interesting
preliminary way to address our problem, they are certainly
neither covariate-speciﬁc (goal 1), nor interpretable (goal 3,
in that the different learned sense vectors do not need to be
related to each other for the same word).

3. CoVeR: Motivation and Embedding

Algorithm

Notation Throughout this section, we will assume a vo-
cabulary of size n and a discrete covariate to condition on,
where the covariate can take on m values (for example, if
the covariate is the community that the corpus comes from,
i.e. liberal or conservative discussion forums, m is simply
the number of communities). It is easy to see how our al-
gorithm generalizes to higher-order tensor decompositions
when there are multiple dimensions covariates to condition
on (for example, slicing along community and slicing along
timeframe simultaneously). Words will be denoted with
indices i, j ∈ [n] and covariates with index k ∈ [m]. All
embedding vectors will be in Rd, and dimensions in our
embedding are referred to by index t ∈ [d].t
We will denote the co-occurrence tensor as A ∈ Rn×n×m,
where Aijk denotes how many times words i and j occurred
together within a window of some ﬁxed length, in the corpus
coming from covariate k. The result of our algorithm will
be two sets of vectors, {vi ∈ Rd} and {ck ∈ Rd}, as well
as bias terms that also ﬁt into the objective. Finally, let (cid:12)
denote the element-wise product between two vectors.

Objective Function and Discussion Here, we give the
objective function our method minimizes, and provide some
explanation for how one should imagine the effect of the
covariate weights. The objective function we minimize is
the following partial non-negative tensor factorization ob-
jective function for jointly training word vectors and weight
vectors representing the effect of covariates, adapted from
the original GloVe objective (note that ck (cid:12)vi = diag(ck)vi,
where diag(ck) is the diagonal matrix weighting of covari-

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

ate k), which we call J(v, c, b):

n
(cid:88)

m
(cid:88)

i,j=1

k=1

f (Aijk)((ck(cid:12)vi)T (ck(cid:12)vj)+bik+bjk−log Aijk)2

(2)
which is to be optimized over {vi ∈ Rd}, {ck ∈ Rd}, and
{bik ∈ R}. To gain a little more intuition for why this is a
reasonable objective function, note that the resulting objec-
tive for a single “covariate slice”, Jk(v, c, b), is essentially

n
(cid:88)

i,j=1

f (Aijk)((ck (cid:12) vi)T (ck (cid:12) vj) + bik + bjk − log Aijk)2

(3)
which ﬁts the vectors ck (cid:12) vi to the data, thus approximat-
ing the statistic log Aijk with (cid:80)d
kt. Note that in
the case m = 1, the model we use is identical to the stan-
dard GloVe model since the ck can be absorbed into the vi.
We used f (x) = ( min(100,x)
)0.75, to parallel the original
100
objective function in (Pennington et al., 2014).

t=1 vitvjtc2

One can think of the dimensions our model learns as inde-
pendent topics, and the effects of the covariate weights ck
as up- or down-weighting the importance of these topics in
contributing to the conditional co-occurrence statistics.

A Geometric View of Embeddings and Tensor Decom-
position We provide a geometric perspective on our
model. Throughout this section, note at a high level, the aim
of our method is to learn sets {vi ∈ Rd}, {ck ∈ Rd}, and
{bik ∈ R} for 1 ≤ i ≤ n, 1 ≤ k ≤ m, such that for a ﬁxed
k, the vectors {ck (cid:12) vi} and the biases {bik} approximate
the vectors and biases learned from running GloVe on only
the kth slice of the co-occurrence tensor.

We now provide some rationale for why this is a reasonable
objective. The main motivation for our algorithm is that
under standard distributional assumptions, the uniform dis-
tribution that word vectors are sampled from can be seen as
a sphere (Arora et al., 2016), which has been experimentally
veriﬁed. A natural way to model the effect of condition-
ing on a covariate is to replace this spherical distribution
with an ellipse, which is the equivalent of reweighting the
dimensions of the word vectors with respect to some basis.
We model this effect as multiplying the embedding vectors
themselves by a symmetric PSD matrix, speciﬁc to the co-
variate which is being conditioned on. In this framework,
assign each covariate k its own symmetric PSD matrix, Bk.
It is well-known that any symmetric PSD matrix can be
factorized as Bk = RT
k DkRk for some orthonormal basis
Rk and some (nonnegative) diagonal Dk; it thus sufﬁces to
consider the effect of a covariate on some ground truth “base
embedding” M as applying the linear operator Bk to each
embedding vector, resulting in the new embedding BkM .

This model is quite expressive in its own right, but we con-

Figure 1. The effects of conditioning on covariates (covariates are
discussion forums, described in Section 4). Left: baseline em-
bedding with some possible word embedding positionings. Mid-
dle, right: embedding under effect of covariates. For example,
“hillary” and “crooked” are pushed closer together under effects
of The Donald, and “healthcare” pushed closer to “universal” and
“free” under effects of SandersForPresident. “Gun” is moved closer
to “rights” and further from “control” under The Donald, and vice
versa in SandersForPresident.

sider a natural restriction where there exists one basis R
under which the resulting embeddings are affected by co-
variates via multiplication by a diagonal matrix, instead of a
PSD matrix (Fig.1). In particular, we note that

k D2

M T BT

k DkRkRT

k BkM = M T RT

k DkRkM = M (cid:48)T

kM (cid:48)
k
(4)
where M (cid:48)
k = RkM is a rotated version of M . Now, in the
restricted model where all the Rk are equal, we can write
all the M (cid:48)
k as RM , so it sufﬁces to just consider the rota-
tion of the basis that the original embedding was trained in
where R is just the identity (since matrix-factorization based
word embedding models are rotation invariant). Under this
model, the co-occurrence statistics under some transforma-
tion should be equivalent to M T D2
kM . Now, clearly this
is the same as ﬁnding a scaling vector ck (such that Dk is
diag(ci)) and using the vectors vi · ck in the factorization
objective function, exactly which is implied by equation 4.

Note that this is essentially saying that in this distributed
word representation model, there exists some rotation of the
embedding space under which the effect of the covariate
separates along dimensions. The implication is that there
are some set of independent “topics” that each covariate will

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

upweight or downweight in importance (or possibly ignore
altogether with a weight of 0), characterizing the effect of
the conditioning directly in terms of the effect on topics.

Algorithm Details Our model learns the resulting param-
eters {vi ∈ Rd}, {ck ∈ Rd}, and {bik}, by using the Adam
(Kingma & Ba, 2014) algorithm, which was empirically
shown to yield good convergence results in the original
GloVe setting. The speciﬁc hyperparameters used for each
dataset will be described in the next section. All vectors
were initialized as random unit vectors 1.

4. Dataset

We evaluated CoVeR in two primary datasets.
Co-
occurrence statistics were formed by considering size 8 win-
dows and using an inverse-distance weighting (e.g. words
3 apart had 1
3 added), which was suggested by some imple-
mentations of (Pennington et al., 2014).

The ﬁrst dataset, referred to as the “book dataset”, con-
sists of the full text from 29 books written by 4 different
authors. The books we used were J.K. Rowling’s “Harry
Potter” series (7 books), “Cormoran Strike” series (3 books),
and “The Casual Vacancy”; C. S. Lewis’s “The Chronicles
of Narnia” series (7 books), and “The Screwtape Letters”;
George R. R. Martin’s “A Song of Ice and Fire” series (5
books); and Stephenie Meyer’s “Twilight” series (4 books),
and “The Host”. These books are ﬁction works in similar
genres, with highly overlapping vocabularies and common
themes. A trivial way of learning series-speciﬁc tenden-
cies in word usage would be to cluster according to unique
vocabularies (for example, only the “Harry Potter” series
would have words such as “Harry” and “Ron” frequently),
so the co-occurrence tensor was formed by looking at all
words that occurred in all of the series with multiple books,
which eliminated all series-speciﬁc words. Furthermore,
series by the same author had very different themes, so there
is no reason intrinsic to the vocabulary to believe the weight
vectors would cluster by author. The vocabulary size was
5,020, and after tuning our algorithm to embed this dataset,
we used 100 dimensions and a learning rate of 10−5.

The second dataset, the “politics dataset”, was a collection
of comments made in 2016 in 6 different subreddits on the
popular discussion forum reddit, and was selected to address
both Questions 1 and 2. The covariate was the discussion
forum, and the subreddits we used were AskReddit, news,
politics, SandersForPresident, The Donald, and WorldNews.

1We also experimented with initializing covariate weight vec-
tors as random vectors centered around the all 1 vector. This
initialization also yielded the sparsity patterns discussed in the
next section, but converged at a slower rate, and performed simi-
larly on downstream metrics as initializing near all 0, so we kept
this initialization.

AskReddit was a baseline discussion forum with a very gen-
eral vocabulary usage, and the discussion forums for the
Sanders and Trump support bases were also selected, as
well as three politically-relevant but more neutral commu-
nities (it should be noted that the politics discussion forum
tends to be very left-leaning). We considered a vocabulary
of size 15,000, after removing the 28 most common words
(suggested by personal communication amongst the word
embedding community) and entries of the cooccurrence ten-
sor with less than 10 occurrences (for the sake of training
efﬁciency). The embedding used 200 dimensions and a
learning rate of 10−5.

5. Experimental Validation

5.1. Clustering by Weights

We performed CoVeR on the book dataset, and considered
how well the weight vectors of the covariate clustered by se-
ries and also by author. We also considered clustering of the
covariate parameter vectors learned from the S-EFE method.
As a baseline, we considered the topic breakdown on the
same co-occurrence statistics predicted by Latent Dirichlet
Allocation (Blei et al., 2003) with various dimensionalities
(20, 50, 100). For a visualization, see Fig. 2.

For every book in every series, the closest weight vectors
(via the tensor decomposition algorithm) by series were
all the books in the same series. Furthermore, for every
book written by every author, the closest weight vectors
by author were all the books by the same author. This
clear clustering behavior even when only conditioning on
co-occurrence statistics for words that appear in all series
(throwing out series-speciﬁc terms) implies the ability of
the weight vectors to cluster according to higher-order in-
formation, perhaps such as writing style.

In contrast, S-EFE is does not learn interpretable features for
each covariate; simply plotting the S-EFE parameter vector
does not cluster the books by the same author. We recognize
that the comparison of the PCA-projected S-EFE parameter
vectors should not be expected to do well (we tried nonlin-
ear dimension reduction techniques such as T-SNE under
various parameter settings as well, and in no setting did the
projections cluster meaningfully); nonetheless, using the
parameter vectors seemed to be the most natural way of
measuring interpretability with respect to the covariates. All
parameter settings of LDA failed to produce any meaningful
clusters.

5.2. Speciﬁcity of Conditional Embeddings

When measuring comparability across embeddings, one
desirable feature of an embedding algorithm is that it re-
distances words with respect to how much the meaning
changes across covariates. For example, a common word

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

(a) 2D PCA of book dataset weight vec-
tors (CoVeR, 100 dimensions)

Figure 3. Histogram of average pairwise cosine similarity for
context-speciﬁc embeddings of a given word.

(b) 2D PCA of neural net parameter vec-
tors (S-EFE, 100 dimensions).

(c) 2D PCA of book dataset topic vectors,
as predicted by LDA (100 topics)

Figure 2. Visualization of topic vectors, algorithm comparison

such as “the” or “a” has essentially the same meaning in
almost every situation, so the resulting embedding should
not change very much. However, for a word that may have
very different meanings in different contexts (such as “immi-
gration”, which might have different connotations in liberal
vs. conservative texts), we expect the covariate-speciﬁc
embeddings to be more spread out.

A simple way to quantitatively evaluate this notion of “vari-
ance in meaning = spread in embeddings” is to, for each
word, consider the average pairwise cosine distance between
covariate-speciﬁc embeddings. For example, for the word
“dark”, using the book dataset, there will be 29 different
embeddings corresponding to each different set of covari-
ate cooccurrences; over all pairs of books, take the average
cosine distance between the embeddings of the given word.
We plotted the distribution of this average across all words.

The results are quite striking: neural embedding method
such as S-EFE is unable to distinguish between the speci-

ﬁcities of words. This could be a result of the neural net
parameterization learning a different subspace for each co-
variate, which makes comparing embeddings across groups
infeasible. However, CoVeR is able to relate the covariate-
speciﬁc embeddings to each other via a linear re-weighting,
giving meaning to comparisons between the embeddings.
As a baseline, the mean distance for common prepositions
(which we take to represent “stable” words which should
not drift too much under conditioning) is plotted; they were
much more stable in our embedding, as expected.

5.3. Data Efﬁciency and Validation

Consider the problem of learning an embedding for the text
of a particular book (or series). This is fundamentally a
covariate-speciﬁc embedding problem, so we test the data
efﬁciency of our algorithm with respect to two alternative
methods of learning conditional embeddings. The main
advantage given by using CoVeR over applying GloVe to
the individual slices is one of data efﬁciency: by pooling the
co-occurrence statistics of words across other books, we are
able to give less noisy estimates of the vectors, especially
for rare or nonexistent words. We also show that our method
compares very favorably with respect to S-EFE. Individual
books contained between 26747 and 355814 words.

To this end, we performed the following experiment. For
some book k, consider three embeddings: 1) the result of
performing GloVe on the co-occurrence statistics of just
the book, 2) the context-speciﬁc embedding produced by
S-EFE with dimension 100 (different dimensionalities com-
pared similarly) 3) the (weighted) context-speciﬁc embed-
ding resulting from CoVeR. Then, we tested these resulting
embeddings using a standard suite of evaluation metrics
(Jastrzebski et al., 2017), including cluster purity and corre-
lation similarities. Our method outperformed method 1 on
all tasks and method 2 on all but one, often signiﬁcantly.

5.4. Sparsity of Weight Vectors

Because of experimentally veriﬁed isotropic distributional
properties of word embedding vectors (Arora et al., 2016), it
is unreasonable to ask for sparsity in the word embeddings

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

Table 1. Average performance on Harry Potter books, cluster purity.
Larger scores indicate better performance.

Cluster purity
BLESS

AP

CoVeR 0.1602
S-EFE
0.1289
0.1297
GloVe

0.2185
0.1942
0.2042

Battig

0.0877
0.0745
0.0770

Table 2. Average performance on Harry Potter books, correlation
similarities. Larger scores indicate better performance.

Correlation similarities

MEN MTurk

RG65

RW

CoVeR 0.1370
0.0184
S-EFE
0.0593
GloVe

0.0719
0.0986
0.0341

0.1272
-0.0098
0.0133

0.0902
-0.0753
0.0588

themselves. However, our topic-speciﬁc weighting scheme
for the covariate weight vectors implies that sparsity is de-
sirable in the weights. The weight sparsity resulting from
our algorithm was experimentally veriﬁed through many
runs of our embedding method, as well as across different
optimization methods, which to us was a surprising and
noteworthy artifact of our algorithm.

ing with regularizing the word vectors forced the model
into a smaller subspace (i.e. sparse dimensions existed but
were shared across all the words), which is not useful and
provides further evidence for a natural isotropic distribution.

Table 3. Sparse coordinate count, covariate weight vectors, 5 runs.
world
23.2
3.1

politics
25.6
2.2

Sanders Donald

news
16.8
2.4

Ask
18.2
1.4

mean
std

27.2
2.6

36.8
3.6

To conﬁrm that the sparsity was a result of separation of
covariate effects rather than an artifact of our algorithm,
we ran our decomposition on a co-occurrence tensor which
was the result of taking the same slice (subreddit) and sub-
sampling its entries 3 times, creating 3 slices of essentially
similar co-occurrence statistics. We applied our algorithm
with different learning parameters, and the resulting weight
vectors after 90 iterations (when the outputs vectors con-
verged) were extremely non-sparse, with between 0 and 2
sparse coordinates per weight vector. The dimensions that
are speciﬁcally 0 for a covariate corresponds to topics that
are relatively less relevant for that covariate. In the next
section, we develop methods to systematically interpret the
covariate weights in terms of topics.

6. Interpretation

6.1. Inference of Topic Meaning

One interesting problem to the word embedding community
is that of topic inference and modeling, that is discover-
ing underlying topics via analyzing co-occurrence statistics
over corpora (Arora et al., 2013). A simple test of infer-
ring topic meaning (i.e. topics coordinates are associated
with) is to consider the set of words which are large in the
given coordinate. Concretely the task is, given some index
t ∈ [d], output the words whose (normalized) vectors have
largest absolute value in dimension t. We show the results
of this experiment for several of the sparse coordinates in
the AskReddit weight vector:

Figure 4. Histogram, sizes of weights in covariate vectors for pol-
itics dataset. Smallest bucket is zero (< 10−10). Sparsity of
covariates in book dataset deferred to appendix.

Note that in the objective function (3), sparse coordinates
will become “stuck” at 0, because the gradient update ∂J
∂ckt
of ckt is proportional to ckt:

Table 4. Representative sample of top words for selected dimen-
sions: topic inference task. Each word was in top 20 normalized
coordinate values for the dimension t.

n
(cid:88)

i,j=1

4f (Aijk)((ck(cid:12)vi)T (ck(cid:12)vj)+bik+bjk−log Aijk)ckt

(5)
The dimensions that the covariates were sparse in did not
overlap by much: on average, weight vectors had 20.7 sparse
coordinates, and the average number of coordinates that was
sparse in both vectors of a random pair was 5.2. This sug-
gests that the set of “topics” each conditional slice of the
tensor does not care about is fairly speciﬁc. Experiment-

t
99
120
183
194

Highest weighted words, dimension t Meaning
horses, cat, teenager, grandma
tables, driveway, customer, stations
gate, territory, directions, phillipines
sweat, disciplined, beliefs, marines

people/animals
domestic
foreign
military

There are several conclusions to be drawn from this ex-
periment. Firstly, while there is some clear noise, speciﬁc
topic meanings do seem to appear in certain coordinates
(which we infer in the table above). It is reasonable that

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

meaning would appear out of coordinates which are sparsely
weighted in some covariate, because this means that it is
a topic that is relevant in other discussion forums but pur-
posely ignored in some speciﬁc forum, so it is likely to have
a consistent theme. When we performed this task for co-
ordinates which had low variance in their weights between
covariates, the resulting words were much less consistent.

It is also interesting to see how covariates weight a topic
with an identiﬁed meaning. For example, for coordinate 194
(military themes), AskReddit placed negligible weight, news
and worldnews placed weight 2.06 and 2.04, SandersFor-
President and The Donald placed weight 0.41 and 0.38,
and politics placed weight 0.05. This process can also
be reversed - for example, consider coordinates small in
worldnews and large in news. One example was coordi-
nate 188; upon checking the words large in this coordinate
({taser, troops, supremacists, underpaid, rioters, amendment,
racially}) it clearly had themes of rights and protests, which
makes sense as a domestic issue, not a global one.

6.2. Topical Word Drift

We performed the following experiment: which pairs of
words start off close in the baseline embedding, yet under
some covariate weights move far apart (or vice versa)? Con-
cretely, the task is, for a ﬁxed word i and a ﬁxed covariate k,
to identify words j such that ||ck(cid:12)vi−ck(cid:12)vj|| (cid:29) ||vi−vj||
or ||ck (cid:12)vi −ck (cid:12)vj|| (cid:28) ||vi −vj||, where the magnitude of
drift is quantiﬁed by the ratio of distances in the normalized
embedding. The motivation is to ﬁnd words whose general
usage is similar, but have very different meanings in speciﬁc
communities. We present a few representative examples of
this experiment, for k = The Donald.

Table 5. Representative examples of word drift for The Donald.

Word i
Drift dir. Words with strongest drift
Closer
hillary
Further
hillary
Closer
gun
gun
Further
immigrant Closer
Further
immigrant

crooked, lying, shillary, killary
electable, compromise, favored
merchandise, milo, ﬂair, fakenews
assault, child, fanatics, problem, police
unauthorized, uninsured, parasite
child, abused, future, attorneys, protect

Combining the previous two sections allows us to do an end-
to-end case study on words that drift under a covariate, so
we can explain speciﬁcally which topics (under reweighting)
caused this shift. For example, the words “immigrant” and
“parasite” were signiﬁcantly closer under the weights placed
by The Donald, so we considered dimensions that were si-
multaneously large in the vector vimmigrant − vparasite and
sparse in the weight vector cT he Donald. The dimensions
89 and 139 were sparse and also the 2nd and 3rd largest
coordinates in the difference vector, so they had a large

contribution to the subspace which was zeroed out under
the reweighting. Words that were large in these dimensions
(and thus representative of the zeroed out subspace meaning)
included {misunderstanding, populace, scapegoat, rebuild-
ing} for 89, and {answers, barriers, applicants, backstory,
selﬂess, indigenous} for 139. This suggests two indepen-
dent reasons for the drift: dimensions corresponding to
emotional appeal and legal immigration being zeroed out.

6.3. Covariate-Speciﬁc Analogies

One of the most famous downstream applications of recent
embedding methods such as (Pennington et al., 2014) and
(Mikolov et al., 2013) is representing analogies. This is
formulated as a is to b as c is to d ↔ va − vb ≈ vc − vd, for
example vwoman − vqueen ≈ vman − vking. We considered
how well our method captured covariate-speciﬁc analogies,
which appear in a covariate-speciﬁc embedding but not
most others. Our embedding was able to capture differential
meaning in the form of these speciﬁc analogies: we describe
the results more formally in the appendix. An example is
that “hillary is to liberal as trump is to (white / racist)” are
both much stronger analogies in liberal discussion forums,
and a corresponding strong analogy in conservative forums
is “... as trump is to disenfranchised”. Also, across the
board, “hillary is to politician as trump is to businessman”,
but replacing “businessman” with “conﬁdence” is an en-
riched analogy in conservative forums, and replacing with
“irrationality” is enriched in liberal ones.

Discussion We have presented a joint tensor model that
learns an embedding for each word and for each covariate.
This makes it very simple to compute the covariate speciﬁc
embedding: we just take the element-wise vector product.
It also enables us to systematically interpret the covariate
vector by looking at dimensions along which weight is large
or 0. Our experiments show that these dimensions can be
interpreted as coherent topics. While we focus on word
embeddings, our tensor covariate embedding model can be
naturally applied in other settings. For example, there is
a large amount of interest in learning embeddings of indi-
vidual genes to capture biological interactions. The natural
covariates here are the cell types and our method would
be able to model cell-type speciﬁc gene interactions. An-
other interesting setting with conditional covariates would
be time-series speciﬁc embeddings, where data efﬁciency
becomes more of an issue. We hope our framework is gen-
eral enough that it will be of use to practitioners in these
settings and others.

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

Acknowledgements

References

This research was supported by NSF Graduate Fellowship
DGE-1656518. The authors thank Tatsunori Hashimoto for
helpful comments and Lilly Shen for helping with illustra-
tions.

Arora, Sanjeev, Ge, Rong, Halpern, Yoni, Mimno, David,
Moitra, Ankur, Sontag, David, Wu, Yichen, and Zhu,
Michael. A practical algorithm for topic modeling with
provable guarantees. Proceedings of the International
Conference on Machine Learning, 2013.

Arora, Sanjeev, Li, Yuanzhi, Liang, Yingyu, Ma, Tengyu,
and Risteski, Andrej. Rand-walk: A latent variable model
approach to word embeddings. Transactions of the Associ-
ation for Computational Linguistics (TACL), 4:385–399,
2016.

Blei, David M., Ng, Andrew Y., and Jordan, Michael I.
Latent dirichlet allocation. Journal of Machine Learning
Research, 3:993–1022, 2003.

Bullinaria, John A. and Levy, Joseph P. Extracting semantic
representations from work co-occurrence statistics: A
computational study. Behavior Research Methods, 39.

Cotterell, Ryan, Poliak, Adam, Van Durme, Benjamin, and
Eisner, Jason. Explaining and generalizing skip-gram
through exponential family principal component analysis.
In Proceedings of the 15th Conference of the European
Chapter of the Association for Computational Linguistics:
Volume 2, Short Papers, 2017.

Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive
subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12.

Hope, Jonathan. The Authorship of Shakespeare’s Plays:
A Socio-Linguistic Study. Cambridge University Press,
1994.

Jastrzebski, Stanislaw, Lesniak, Damian, and Czarnecki,
Wojciech Marian. How to evaluate word embeddings?
on importance of data efﬁciency and simple supervised
tasks. ArXiv preprint, 2017.

Kim, Yoon, Chiu, Yi-I, Hanaki, Kentaro, Hegde, Darshan,
and Petrov, Slav. Temporal analysis of language through
neural language models. Proceedings of the ACL 2014
Workshop on Language Technologies and Computational
Social Science, 2014.

Kingma, Diederik and Ba, Jimmy. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.

Liu, Pengfei, Qiu, Xipeng, and Huang, Xuanjing. Learning
context-sensitive word embeddings with neural tensor
skip-gram model. In Proceedings of the Twenty-Fourth
International Joint Conference on Artiﬁcial Intelligence,
2015.

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado,
Greg S, and Dean, Jeff. Distributed representations of
words and phrases and their compositionality. pp. 3111–
3119, 2013.

Neelakantan, Arvind, Shankar, Jeevan, Passos, Alexandre,
and McCallum, Andrew. Efﬁcient non-parametric esti-
mation of multiple embeddings per word in vector space.
Conference on Empirical Methods in Natural Language
Processing, 2014, 2015.

Pennington, Jeffrey, Socher, Richard, and Manning, Christo-
pher D. Glove: Global vectors for word representation.
14:1532–1543, 2014.

Reisinger, Joseph and Mooney, Raymond J. Multi-prototype
vector-space models of word meaning. Human Language
Technologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computational
Linguistics, pp. 109–117, 2010.

Robinson, Michael D., Boyd, Ryan L., Fetterman, Adam K.,
and Persich, Michelle R. Journal of Language and Social
Psychology, 36:438–461.

Rudolph, Maja, Ruiz, Francisco, Athey, Susan, and Blei,
David. Structured embedding models for grouped data.
In Advances of Neural Information Processing Systems,
2017.

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

A. Covariate-speciﬁc analogies

We wish to learn covariate-speciﬁc analogies of the form a is to b as c is to d. To this end, we considered experiments of the
form: for ﬁxed words a, b, c, determine words d such that for some covariate k, the quantity

(ck (cid:12) va − ck (cid:12) vb) · (ck (cid:12) vc − ck (cid:12) vd)
||ck (cid:12) va − ck (cid:12) vb||||ck (cid:12) vc − ck (cid:12) vd||

(6)

is small, yet for other k, the quantity is large. The intuition is that under the covariate transform, vc − vd points roughly in
the same direction as va − vb, and d is close to c in semantic meaning.

In particular, we set a = “hillary”, c = “trump”, and found words b for which there existed a d consistently at the top across
subreddits (implying existence of strong analogies). For example, when b = “woman”, d = “man” was the best analogy for
every weighting. Then, for these b, we considered words d whose relative rankings in the subreddits had high variance. The
differential analogies captured were quite striking: the experiment is able to reveal words whose relative meaning in relation
to anchor words such as “hillary” and “trump” drifts signiﬁcantly.

Table 6. Analogies task. Each best analogy d was one of the top-ranked words in every embedding. We present words whose “relative
analogy” rank was enriched in some embedding. Subreddits are color-coded: green for news-related WN and N (worldnews, news), blue
for left-leaning P and S (politics, SandersForPresident), red for right-leaning D (The Donald), black for A (AskReddit).

Word b

Best analogy d Word

woman

man

democrat

republican

liberal

conservative

politician

businessman

abysmal
amateur
zionist
politician
president
nationalists
south
bigot
christian
white
racist
sociopathic
disenfranchised
conﬁdence
questionable
irrational

High rank
1351 (S), 2218 (P)
1543 (P), 3966 (S)
1968 (P), 2327 (S)
2796 (WN), 3155 (D)
2452 (D), 3564 (WN)
208 (S), 606 (D)
3511 (P)
400 (S), 530 (A)
33 (P)
619 (P)
252 (P)
1756 (D)
3693 (D)
2768 (D)
598 (WN)
2153 (N), 3430 (P)

Low rank
14329 (base), 14077 (D)
13840 (base), 13734 (D)
14173 (base), 14248 (A)
11959 (base), 10386 (S)
12257 (base)
8916 (base), 7526 (A)
11091 (base)
12888 (D), 12994 (WN)
12756 (D), 12722 (WN)
12824 (D), 13273 (base)
12930 (S), 12779 (D)
13744 (P), 13389 (A)
11267 (base)
10528 (base)
13002 (base)
13305 (base)

B. Sparsity results: book dataset

Number of sparse coordinates (out of 100) were as follows, by series and then book order: Harry (0, 5, 1, 3, 7, 4, 0),
Chronicles (0, 0, 0, 1, 0, 0, 0), Song (8, 8, 9, 4, 11), Twilight (6, 6, 8, 7), Screwtape (5), Strike (3, 2, 2), Host (6), Vacancy
(4). While not as dramatic as in the politics dataset, the presence of zero (rather than small) coordinates across multiple runs
shows that there still is speciﬁcity of topics being learned. We plot the histogram of coordinate sizes in the following ﬁgure.

C. Algorithm setting notes

We also experimented with using (Duchi et al.) as the optimization method, but the resulting weight vectors in the politics
dataset had highly-overlapping sparse dimensions. This implies that the optimization method tried to ﬁt the model to a
smaller-dimensional subspace, which is not a desirable source of sparsity.

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

Figure 5. Histogram, sizes of weights in covariate vectors of book data.

(a) Non-zero dimensions in weight vectors by epoch and opti-
mization method. Upper: Adam; lower: Adagrad.

(b) Non-zero dimensions in weight vectors by epoch. Upper:
initialization centered around all-1 vector; lower: centered around
all-0 vector.

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor
Decompositions

Kevin Tian * 1 Teng Zhang * 2 James Zou 3

8
1
0
2
 
l
u
J
 
7
 
 
]
L
C
.
s
c
[
 
 
2
v
9
3
8
7
0
.
2
0
8
1
:
v
i
X
r
a

Abstract
Word embedding is a useful approach to cap-
ture co-occurrence structures in large text corpora.
However, in addition to the text data itself, we of-
ten have additional covariates associated with indi-
vidual corpus documents—e.g. the demographic
of the author, time and venue of publication—and
we would like the embedding to naturally capture
this information. We propose CoVeR, a new ten-
sor decomposition model for vector embeddings
with covariates. CoVeR jointly learns a base em-
bedding for all the words as well as a weighted
diagonal matrix to model how each covariate af-
fects the base embedding. To obtain author or
venue-speciﬁc embedding, for example, we can
then simply multiply the base embedding by the
associated transformation matrix. The main ad-
vantages of our approach are data efﬁciency and
interpretability of the covariate transformation.
Our experiments demonstrate that our joint model
learns substantially better covariate-speciﬁc em-
beddings compared to the standard approach of
learning a separate embedding for each covari-
ate using only the relevant subset of data, as well
as other related methods. Furthermore, CoVeR
encourages the embeddings to be “topic-aligned”
in that the dimensions have speciﬁc independent
meanings. This allows our covariate-speciﬁc em-
beddings to be compared by topic, enabling down-
stream differential analysis. We empirically eval-
uate the beneﬁts of our algorithm on datasets, and
demonstrate how it can be used to address many
natural questions about covariate effects.

Accompanying code to this paper can be found at
http://github.com/kjtian/CoVeR.

*Equal contribution 1Department of Computer Science, Stan-
ford University 2Department of Management Science and Engi-
neering, Stanford University 3Department of Biomedical Data
Science Stanford University. Correspondence to: Kevin Tian <kj-
tian@stanford.edu>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

1. Introduction

The use of factorizations of co-occurrence statistics in learn-
ing low-dimensional representations of words is an area that
has received a large amount of attention in recent years, per-
haps best represented by how widespread algorithms such
as GloVe (Pennington et al., 2014) and Word2Vec (Mikolov
et al., 2013) are in downstream applications. In particular,
suppose we have a set of words i ∈ [n], where n is the size
of the vocabulary. The aim is to, for a ﬁxed dimensionality
d, assign a vector vi ∈ Rd to each word in the vocabulary
in a way that preserves semantic structure.

In many settings, we have a corpus with additional covari-
ates on individual documents. For example, we might have
news articles from both conservative and liberal-leaning
publications, and using the same word embedding for all
the text can lose interesting information. Furthermore, we
suggest that there are meaningful semantic relationships
that can be captured by exploiting the differences in these
conditional statistics. To this end, we propose the following
two key questions that capture the problems that our work
addresses, and for each, we give a concrete motivating ex-
ample of a problem in the semantic inference literature that
it encompasses.

Question 1: How can we leverage conditional co-
occurrence statistics to capture the effect of a covariate
on word usage?

A simple example of a covariate one may wish to condi-
tion on is the document the writing came from (i.e. for a
group of books, learn a set of word embeddings for each
book). There are many natural ways to capture the effect of
this conditioning; we choose to do so by representing the
covariate as a vector. It is interesting to see if the resulting
vectors cluster by author, for example. An example appli-
cation is addressing the question: did William Shakespeare
truly write all the works credited to him, or have there been
other “ghostwriters” who have contributed to the Shake-
speare canon? This is the famous Shakespeare authorship
question, for which historians have proposed various candi-
dates as the true authors of particular plays or poems (Hope,
1994). If the latter scenario is the case, what in particular
distinguishes the writing style of one candidate from an-

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

other, and how can we infer who the most likely author of a
work is from a set of candidates? We show that it is possible
to simultaneously address the authorship question, and also
learn covariate vectors which are interpretable in terms of
their effects on word usage (measured by reweighting the
importance of topics). This motivates our next question.

Question 2: Traditional factorization-based embedding
methods are rotationally invariant, so that individual di-
mensions do not have semantic meaning. How can we break
this invariance to yield a model which aligns topics with
interpretable dimensions?

There has been much interest in the differences in language
and rhetoric that appeal to different demographics. For
example, studies have been done regarding “ideological sig-
natures” speciﬁc to voters by partisan alignment (Robinson
et al.) in which linguistic differences were proposed along
focal axes, such as the “mind versus the body” in texts with
more liberal or conservative ideologies. How can we sys-
tematically infer topical differences such as these between
different communities?

Questions such as these, or more broadly covariate-speciﬁc
trends in word usage, motivated this study. Concretely, our
goal is to provide a general framework through which em-
beddings of sets of objects with co-occurrence structure,
as well as the effects of conditioning on particular covari-
ates, can be learned jointly. As a byproduct, our model
also gives natural meaning to the different dimensions of
the embeddings, by breaking the rotational symmetry of
previous embedding-learning algorithms, such that the re-
sulting vector representations of words and covariates are
“topic-aligned”.

Our Contributions Our main contributions are CoVeR,
a decomposition algorithm that addresses the goals and
issues discussed in the introduction, and the methods for
systematic analysis we propose. Namely, we propose a
method which pools information between conditional co-
occurrence statistics to learn covariate-speciﬁc embeddings
in a data-efﬁcient way, as well as an interpretable embed-
ding of covariate vectors. We evaluate our method against
conditional GloVe as well as the related method of (Rudolph
et al., 2017).

Paper Organization We discuss related works in section
2 and in section 3, we provide our embedding algorithm, as
well as mathematical justiﬁcation for its design. In section
4, we describe our dataset. In section 5, we validate our
algorithm with respect to intrinsic properties and standard
metrics. In section 6, we propose several experiments for
systematic downstream analysis.

2. Background and related works

When designing an algorithm for learning covariate-speciﬁc
embeddings, we identiﬁed three main goals that the algo-
rithm should satisfy. The algorithm should be 1) covariate-
speciﬁc, 2) data-efﬁcient, and 3) interpretable. Goals 1 and
3 go without saying; goal 2 arises in our setting because of-
tentimes, the conditional co-occurrence counts can be quite
small, especially when there are many covariates. We deﬁne
all three in very general terms, and use them to evaluate
existing methods which can be used to handle our condi-
tional embedding learning task. In this section, we describe
several existing approaches and how well they address these
goals, which will serve as the basis for our evaluation later
in the paper.

Conditional GloVe As a brief introduction to embedding
methods, all such algorithms generally rely on the intuition
that some function of the co-occurrence statistics is low
rank. Studies such as GloVe and Word2Vec proposed based
on minimizing low-rank approximation-error of nonlinear
transforms of the co-occurrence statistics. let A be the n × n
matrix with Aij the co-occurrence between words i and j,
where co-occurrence is deﬁned as the (possibly weighted)
number of times the words occur together in a window of
ﬁxed length. For example, GloVe aimed to ﬁnd vectors
vi ∈ Rd and biases bi ∈ R such that the loss

J(v, b) =

f (Aij)(vT

i vj + bi + bj − log Aij)2

(1)

n
(cid:88)

i,j=1

was minimized, where f was some ﬁxed increasing weight
function. Word2Vec aimed to learn vector representations
via minimizing a neural network-based loss function (it can
be shown that Word2Vec and GloVe are essentially per-
forming the same factorization with respect to a different
loss function). A related embedding approach is to directly
perform principal component analysis on the PMI (point-
wise mutual information) matrix of the words (Bullinaria &
Levy). PMI-factorization based methods aim to ﬁnd vectors
P(i,j)
{vi} such that vT
i vj ≈ P M I(A)ij = log
P(i)P(j) , where
the probabilities are taken over the co-occurrence matrix.
This is essentially the same as ﬁnding a low-rank matrix V
such that V T V ≈ P M I, and empirical results show that
the resulting embedding captures useful semantic structure.

A simple baseline for learning conditional embeddings thus
is simply performing GloVe (or another non-conditional
embedding learning method) on each conditional co-
occurrence matrix, and using the resulting vectors as our
conditional embedding. This certainly satisﬁes goal 1 that
we deﬁned earlier, but neither goal 2 nor goal 3 is addressed.
Regarding goal 2, this naive method does not pool infor-
mation between the co-occurrence matrices, and thus is
inefﬁcient in its learning of semantic meaning, which is

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

very relevant when the conditional counts are not large (for
example, if the covariate is time intervals, the conditional
counts can be very small), which we show empirically. Re-
garding goal 3, because of the rotational invariance of the
method, there is no straightforward way to relate the condi-
tional embeddings to one another. There has been previous
work which learns a different set of embeddings on each
conditional slice and then tries to postprocess to align these
embeddings, such as (Kim et al., 2014). However, these
works still run into the data efﬁciency issues we describe,
and it is difﬁcult to analyze the quality of the alignment (for
example, the embedding algorithm on different slices of the
tensor may converge to minima which are not related by a
rotation, therefore an alignment may not exist).

Tensor-based Conditional Embedding Methods An in-
teresting recent work related to our method is the conditional
embedding method S-EFE (Rudolph et al., 2017). Their
work also aims to learn a different set of word embeddings
for each covariate (the same problem we consider), and does
so via a neural-net based objective function. Their work
is analogous to our work up to the parameterization of the
covariates (our work parameterizes covariates via a diagonal
scaling matrix, and theirs by a small neural network). This
is similar in some ways to how Word2Vec and GloVe are
analogous, but very different in others: the primary differ-
ence between Word2Vec and GloVe is in the choice of loss
between distributions in the objective, and our work differs
from S-EFE more fundamentally in the parameterization.
Furthermore, this work does a sub-optimal job of addressing
goals 2 (due to the large number of parameters necessary
in a neural-net based objective) and 3 (interpreting parame-
ters in a nonlinear transform is difﬁcult), which we show in
evaluations.

There are a few other related works which also try to learn
grouped embeddings via tensor decomposition (Cotterell
et al., 2017) (Liu et al., 2015). However, these works con-
sider conditioning in a restricted sense (typically with re-
spect to conditioning on different semantic meanings of a
word), do not aim to interpret the context vectors, and do not
try to preserve rotational alignment with interpretable dimen-
sions. In particular, because these models do not consider
the direct representation of the covariate in the objective
function, the representations are missing key structural prop-
erties (for example topic alignment), which doesn’t allow
for meaningful downstream analysis. In this sense, our work
aims to solve a more general problem, and provides further
justiﬁcation for tensor-based embedding methods.

Multi-sense Embeddings There has been work which
aims to address the multiple-meaning problem which arises
in learning word embeddings, via multi-sense embeddings
(Reisinger & Mooney, 2010) (Neelakantan et al., 2015).

Typically, these algorithms work by allocating multiple vec-
tors for each word (either a ﬁxed count or a varying number),
and optimizing an objective function which picks out one
of the vectors for each word. For example, if words i and
j occur in the same context, the corresponding term in the
objective function might use the embeddings (or senses) of
i and j which are closest to each other. In some ways, this
is similar to our idea that the same word can have different
meanings in different contexts. However, it is addressing a
fundamentally different problem. The “senses” which are
learned correspond to differing meanings of a word, whereas
our work relies on the intuition that across contexts, a word
will have roughly similar meanings, but usage may differ
in some ways, perhaps with respect to certain topics. Fur-
thermore, while multi-sense embeddings pose an interesting
preliminary way to address our problem, they are certainly
neither covariate-speciﬁc (goal 1), nor interpretable (goal 3,
in that the different learned sense vectors do not need to be
related to each other for the same word).

3. CoVeR: Motivation and Embedding

Algorithm

Notation Throughout this section, we will assume a vo-
cabulary of size n and a discrete covariate to condition on,
where the covariate can take on m values (for example, if
the covariate is the community that the corpus comes from,
i.e. liberal or conservative discussion forums, m is simply
the number of communities). It is easy to see how our al-
gorithm generalizes to higher-order tensor decompositions
when there are multiple dimensions covariates to condition
on (for example, slicing along community and slicing along
timeframe simultaneously). Words will be denoted with
indices i, j ∈ [n] and covariates with index k ∈ [m]. All
embedding vectors will be in Rd, and dimensions in our
embedding are referred to by index t ∈ [d].t
We will denote the co-occurrence tensor as A ∈ Rn×n×m,
where Aijk denotes how many times words i and j occurred
together within a window of some ﬁxed length, in the corpus
coming from covariate k. The result of our algorithm will
be two sets of vectors, {vi ∈ Rd} and {ck ∈ Rd}, as well
as bias terms that also ﬁt into the objective. Finally, let (cid:12)
denote the element-wise product between two vectors.

Objective Function and Discussion Here, we give the
objective function our method minimizes, and provide some
explanation for how one should imagine the effect of the
covariate weights. The objective function we minimize is
the following partial non-negative tensor factorization ob-
jective function for jointly training word vectors and weight
vectors representing the effect of covariates, adapted from
the original GloVe objective (note that ck (cid:12)vi = diag(ck)vi,
where diag(ck) is the diagonal matrix weighting of covari-

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

ate k), which we call J(v, c, b):

n
(cid:88)

m
(cid:88)

i,j=1

k=1

f (Aijk)((ck(cid:12)vi)T (ck(cid:12)vj)+bik+bjk−log Aijk)2

(2)
which is to be optimized over {vi ∈ Rd}, {ck ∈ Rd}, and
{bik ∈ R}. To gain a little more intuition for why this is a
reasonable objective function, note that the resulting objec-
tive for a single “covariate slice”, Jk(v, c, b), is essentially

n
(cid:88)

i,j=1

f (Aijk)((ck (cid:12) vi)T (ck (cid:12) vj) + bik + bjk − log Aijk)2

(3)
which ﬁts the vectors ck (cid:12) vi to the data, thus approximat-
ing the statistic log Aijk with (cid:80)d
kt. Note that in
the case m = 1, the model we use is identical to the stan-
dard GloVe model since the ck can be absorbed into the vi.
We used f (x) = ( min(100,x)
)0.75, to parallel the original
100
objective function in (Pennington et al., 2014).

t=1 vitvjtc2

One can think of the dimensions our model learns as inde-
pendent topics, and the effects of the covariate weights ck
as up- or down-weighting the importance of these topics in
contributing to the conditional co-occurrence statistics.

A Geometric View of Embeddings and Tensor Decom-
position We provide a geometric perspective on our
model. Throughout this section, note at a high level, the aim
of our method is to learn sets {vi ∈ Rd}, {ck ∈ Rd}, and
{bik ∈ R} for 1 ≤ i ≤ n, 1 ≤ k ≤ m, such that for a ﬁxed
k, the vectors {ck (cid:12) vi} and the biases {bik} approximate
the vectors and biases learned from running GloVe on only
the kth slice of the co-occurrence tensor.

We now provide some rationale for why this is a reasonable
objective. The main motivation for our algorithm is that
under standard distributional assumptions, the uniform dis-
tribution that word vectors are sampled from can be seen as
a sphere (Arora et al., 2016), which has been experimentally
veriﬁed. A natural way to model the effect of condition-
ing on a covariate is to replace this spherical distribution
with an ellipse, which is the equivalent of reweighting the
dimensions of the word vectors with respect to some basis.
We model this effect as multiplying the embedding vectors
themselves by a symmetric PSD matrix, speciﬁc to the co-
variate which is being conditioned on. In this framework,
assign each covariate k its own symmetric PSD matrix, Bk.
It is well-known that any symmetric PSD matrix can be
factorized as Bk = RT
k DkRk for some orthonormal basis
Rk and some (nonnegative) diagonal Dk; it thus sufﬁces to
consider the effect of a covariate on some ground truth “base
embedding” M as applying the linear operator Bk to each
embedding vector, resulting in the new embedding BkM .

This model is quite expressive in its own right, but we con-

Figure 1. The effects of conditioning on covariates (covariates are
discussion forums, described in Section 4). Left: baseline em-
bedding with some possible word embedding positionings. Mid-
dle, right: embedding under effect of covariates. For example,
“hillary” and “crooked” are pushed closer together under effects
of The Donald, and “healthcare” pushed closer to “universal” and
“free” under effects of SandersForPresident. “Gun” is moved closer
to “rights” and further from “control” under The Donald, and vice
versa in SandersForPresident.

sider a natural restriction where there exists one basis R
under which the resulting embeddings are affected by co-
variates via multiplication by a diagonal matrix, instead of a
PSD matrix (Fig.1). In particular, we note that

k D2

M T BT

k DkRkRT

k BkM = M T RT

k DkRkM = M (cid:48)T

kM (cid:48)
k
(4)
where M (cid:48)
k = RkM is a rotated version of M . Now, in the
restricted model where all the Rk are equal, we can write
all the M (cid:48)
k as RM , so it sufﬁces to just consider the rota-
tion of the basis that the original embedding was trained in
where R is just the identity (since matrix-factorization based
word embedding models are rotation invariant). Under this
model, the co-occurrence statistics under some transforma-
tion should be equivalent to M T D2
kM . Now, clearly this
is the same as ﬁnding a scaling vector ck (such that Dk is
diag(ci)) and using the vectors vi · ck in the factorization
objective function, exactly which is implied by equation 4.

Note that this is essentially saying that in this distributed
word representation model, there exists some rotation of the
embedding space under which the effect of the covariate
separates along dimensions. The implication is that there
are some set of independent “topics” that each covariate will

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

upweight or downweight in importance (or possibly ignore
altogether with a weight of 0), characterizing the effect of
the conditioning directly in terms of the effect on topics.

Algorithm Details Our model learns the resulting param-
eters {vi ∈ Rd}, {ck ∈ Rd}, and {bik}, by using the Adam
(Kingma & Ba, 2014) algorithm, which was empirically
shown to yield good convergence results in the original
GloVe setting. The speciﬁc hyperparameters used for each
dataset will be described in the next section. All vectors
were initialized as random unit vectors 1.

4. Dataset

We evaluated CoVeR in two primary datasets.
Co-
occurrence statistics were formed by considering size 8 win-
dows and using an inverse-distance weighting (e.g. words
3 apart had 1
3 added), which was suggested by some imple-
mentations of (Pennington et al., 2014).

The ﬁrst dataset, referred to as the “book dataset”, con-
sists of the full text from 29 books written by 4 different
authors. The books we used were J.K. Rowling’s “Harry
Potter” series (7 books), “Cormoran Strike” series (3 books),
and “The Casual Vacancy”; C. S. Lewis’s “The Chronicles
of Narnia” series (7 books), and “The Screwtape Letters”;
George R. R. Martin’s “A Song of Ice and Fire” series (5
books); and Stephenie Meyer’s “Twilight” series (4 books),
and “The Host”. These books are ﬁction works in similar
genres, with highly overlapping vocabularies and common
themes. A trivial way of learning series-speciﬁc tenden-
cies in word usage would be to cluster according to unique
vocabularies (for example, only the “Harry Potter” series
would have words such as “Harry” and “Ron” frequently),
so the co-occurrence tensor was formed by looking at all
words that occurred in all of the series with multiple books,
which eliminated all series-speciﬁc words. Furthermore,
series by the same author had very different themes, so there
is no reason intrinsic to the vocabulary to believe the weight
vectors would cluster by author. The vocabulary size was
5,020, and after tuning our algorithm to embed this dataset,
we used 100 dimensions and a learning rate of 10−5.

The second dataset, the “politics dataset”, was a collection
of comments made in 2016 in 6 different subreddits on the
popular discussion forum reddit, and was selected to address
both Questions 1 and 2. The covariate was the discussion
forum, and the subreddits we used were AskReddit, news,
politics, SandersForPresident, The Donald, and WorldNews.

1We also experimented with initializing covariate weight vec-
tors as random vectors centered around the all 1 vector. This
initialization also yielded the sparsity patterns discussed in the
next section, but converged at a slower rate, and performed simi-
larly on downstream metrics as initializing near all 0, so we kept
this initialization.

AskReddit was a baseline discussion forum with a very gen-
eral vocabulary usage, and the discussion forums for the
Sanders and Trump support bases were also selected, as
well as three politically-relevant but more neutral commu-
nities (it should be noted that the politics discussion forum
tends to be very left-leaning). We considered a vocabulary
of size 15,000, after removing the 28 most common words
(suggested by personal communication amongst the word
embedding community) and entries of the cooccurrence ten-
sor with less than 10 occurrences (for the sake of training
efﬁciency). The embedding used 200 dimensions and a
learning rate of 10−5.

5. Experimental Validation

5.1. Clustering by Weights

We performed CoVeR on the book dataset, and considered
how well the weight vectors of the covariate clustered by se-
ries and also by author. We also considered clustering of the
covariate parameter vectors learned from the S-EFE method.
As a baseline, we considered the topic breakdown on the
same co-occurrence statistics predicted by Latent Dirichlet
Allocation (Blei et al., 2003) with various dimensionalities
(20, 50, 100). For a visualization, see Fig. 2.

For every book in every series, the closest weight vectors
(via the tensor decomposition algorithm) by series were
all the books in the same series. Furthermore, for every
book written by every author, the closest weight vectors
by author were all the books by the same author. This
clear clustering behavior even when only conditioning on
co-occurrence statistics for words that appear in all series
(throwing out series-speciﬁc terms) implies the ability of
the weight vectors to cluster according to higher-order in-
formation, perhaps such as writing style.

In contrast, S-EFE is does not learn interpretable features for
each covariate; simply plotting the S-EFE parameter vector
does not cluster the books by the same author. We recognize
that the comparison of the PCA-projected S-EFE parameter
vectors should not be expected to do well (we tried nonlin-
ear dimension reduction techniques such as T-SNE under
various parameter settings as well, and in no setting did the
projections cluster meaningfully); nonetheless, using the
parameter vectors seemed to be the most natural way of
measuring interpretability with respect to the covariates. All
parameter settings of LDA failed to produce any meaningful
clusters.

5.2. Speciﬁcity of Conditional Embeddings

When measuring comparability across embeddings, one
desirable feature of an embedding algorithm is that it re-
distances words with respect to how much the meaning
changes across covariates. For example, a common word

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

(a) 2D PCA of book dataset weight vec-
tors (CoVeR, 100 dimensions)

Figure 3. Histogram of average pairwise cosine similarity for
context-speciﬁc embeddings of a given word.

(b) 2D PCA of neural net parameter vec-
tors (S-EFE, 100 dimensions).

(c) 2D PCA of book dataset topic vectors,
as predicted by LDA (100 topics)

Figure 2. Visualization of topic vectors, algorithm comparison

such as “the” or “a” has essentially the same meaning in
almost every situation, so the resulting embedding should
not change very much. However, for a word that may have
very different meanings in different contexts (such as “immi-
gration”, which might have different connotations in liberal
vs. conservative texts), we expect the covariate-speciﬁc
embeddings to be more spread out.

A simple way to quantitatively evaluate this notion of “vari-
ance in meaning = spread in embeddings” is to, for each
word, consider the average pairwise cosine distance between
covariate-speciﬁc embeddings. For example, for the word
“dark”, using the book dataset, there will be 29 different
embeddings corresponding to each different set of covari-
ate cooccurrences; over all pairs of books, take the average
cosine distance between the embeddings of the given word.
We plotted the distribution of this average across all words.

The results are quite striking: neural embedding method
such as S-EFE is unable to distinguish between the speci-

ﬁcities of words. This could be a result of the neural net
parameterization learning a different subspace for each co-
variate, which makes comparing embeddings across groups
infeasible. However, CoVeR is able to relate the covariate-
speciﬁc embeddings to each other via a linear re-weighting,
giving meaning to comparisons between the embeddings.
As a baseline, the mean distance for common prepositions
(which we take to represent “stable” words which should
not drift too much under conditioning) is plotted; they were
much more stable in our embedding, as expected.

5.3. Data Efﬁciency and Validation

Consider the problem of learning an embedding for the text
of a particular book (or series). This is fundamentally a
covariate-speciﬁc embedding problem, so we test the data
efﬁciency of our algorithm with respect to two alternative
methods of learning conditional embeddings. The main
advantage given by using CoVeR over applying GloVe to
the individual slices is one of data efﬁciency: by pooling the
co-occurrence statistics of words across other books, we are
able to give less noisy estimates of the vectors, especially
for rare or nonexistent words. We also show that our method
compares very favorably with respect to S-EFE. Individual
books contained between 26747 and 355814 words.

To this end, we performed the following experiment. For
some book k, consider three embeddings: 1) the result of
performing GloVe on the co-occurrence statistics of just
the book, 2) the context-speciﬁc embedding produced by
S-EFE with dimension 100 (different dimensionalities com-
pared similarly) 3) the (weighted) context-speciﬁc embed-
ding resulting from CoVeR. Then, we tested these resulting
embeddings using a standard suite of evaluation metrics
(Jastrzebski et al., 2017), including cluster purity and corre-
lation similarities. Our method outperformed method 1 on
all tasks and method 2 on all but one, often signiﬁcantly.

5.4. Sparsity of Weight Vectors

Because of experimentally veriﬁed isotropic distributional
properties of word embedding vectors (Arora et al., 2016), it
is unreasonable to ask for sparsity in the word embeddings

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

Table 1. Average performance on Harry Potter books, cluster purity.
Larger scores indicate better performance.

Cluster purity
BLESS

AP

CoVeR 0.1602
S-EFE
0.1289
0.1297
GloVe

0.2185
0.1942
0.2042

Battig

0.0877
0.0745
0.0770

Table 2. Average performance on Harry Potter books, correlation
similarities. Larger scores indicate better performance.

Correlation similarities

MEN MTurk

RG65

RW

CoVeR 0.1370
0.0184
S-EFE
0.0593
GloVe

0.0719
0.0986
0.0341

0.1272
-0.0098
0.0133

0.0902
-0.0753
0.0588

themselves. However, our topic-speciﬁc weighting scheme
for the covariate weight vectors implies that sparsity is de-
sirable in the weights. The weight sparsity resulting from
our algorithm was experimentally veriﬁed through many
runs of our embedding method, as well as across different
optimization methods, which to us was a surprising and
noteworthy artifact of our algorithm.

ing with regularizing the word vectors forced the model
into a smaller subspace (i.e. sparse dimensions existed but
were shared across all the words), which is not useful and
provides further evidence for a natural isotropic distribution.

Table 3. Sparse coordinate count, covariate weight vectors, 5 runs.
world
23.2
3.1

politics
25.6
2.2

Sanders Donald

news
16.8
2.4

Ask
18.2
1.4

mean
std

27.2
2.6

36.8
3.6

To conﬁrm that the sparsity was a result of separation of
covariate effects rather than an artifact of our algorithm,
we ran our decomposition on a co-occurrence tensor which
was the result of taking the same slice (subreddit) and sub-
sampling its entries 3 times, creating 3 slices of essentially
similar co-occurrence statistics. We applied our algorithm
with different learning parameters, and the resulting weight
vectors after 90 iterations (when the outputs vectors con-
verged) were extremely non-sparse, with between 0 and 2
sparse coordinates per weight vector. The dimensions that
are speciﬁcally 0 for a covariate corresponds to topics that
are relatively less relevant for that covariate. In the next
section, we develop methods to systematically interpret the
covariate weights in terms of topics.

6. Interpretation

6.1. Inference of Topic Meaning

One interesting problem to the word embedding community
is that of topic inference and modeling, that is discover-
ing underlying topics via analyzing co-occurrence statistics
over corpora (Arora et al., 2013). A simple test of infer-
ring topic meaning (i.e. topics coordinates are associated
with) is to consider the set of words which are large in the
given coordinate. Concretely the task is, given some index
t ∈ [d], output the words whose (normalized) vectors have
largest absolute value in dimension t. We show the results
of this experiment for several of the sparse coordinates in
the AskReddit weight vector:

Figure 4. Histogram, sizes of weights in covariate vectors for pol-
itics dataset. Smallest bucket is zero (< 10−10). Sparsity of
covariates in book dataset deferred to appendix.

Note that in the objective function (3), sparse coordinates
will become “stuck” at 0, because the gradient update ∂J
∂ckt
of ckt is proportional to ckt:

Table 4. Representative sample of top words for selected dimen-
sions: topic inference task. Each word was in top 20 normalized
coordinate values for the dimension t.

n
(cid:88)

i,j=1

4f (Aijk)((ck(cid:12)vi)T (ck(cid:12)vj)+bik+bjk−log Aijk)ckt

(5)
The dimensions that the covariates were sparse in did not
overlap by much: on average, weight vectors had 20.7 sparse
coordinates, and the average number of coordinates that was
sparse in both vectors of a random pair was 5.2. This sug-
gests that the set of “topics” each conditional slice of the
tensor does not care about is fairly speciﬁc. Experiment-

t
99
120
183
194

Highest weighted words, dimension t Meaning
horses, cat, teenager, grandma
tables, driveway, customer, stations
gate, territory, directions, phillipines
sweat, disciplined, beliefs, marines

people/animals
domestic
foreign
military

There are several conclusions to be drawn from this ex-
periment. Firstly, while there is some clear noise, speciﬁc
topic meanings do seem to appear in certain coordinates
(which we infer in the table above). It is reasonable that

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

meaning would appear out of coordinates which are sparsely
weighted in some covariate, because this means that it is
a topic that is relevant in other discussion forums but pur-
posely ignored in some speciﬁc forum, so it is likely to have
a consistent theme. When we performed this task for co-
ordinates which had low variance in their weights between
covariates, the resulting words were much less consistent.

It is also interesting to see how covariates weight a topic
with an identiﬁed meaning. For example, for coordinate 194
(military themes), AskReddit placed negligible weight, news
and worldnews placed weight 2.06 and 2.04, SandersFor-
President and The Donald placed weight 0.41 and 0.38,
and politics placed weight 0.05. This process can also
be reversed - for example, consider coordinates small in
worldnews and large in news. One example was coordi-
nate 188; upon checking the words large in this coordinate
({taser, troops, supremacists, underpaid, rioters, amendment,
racially}) it clearly had themes of rights and protests, which
makes sense as a domestic issue, not a global one.

6.2. Topical Word Drift

We performed the following experiment: which pairs of
words start off close in the baseline embedding, yet under
some covariate weights move far apart (or vice versa)? Con-
cretely, the task is, for a ﬁxed word i and a ﬁxed covariate k,
to identify words j such that ||ck(cid:12)vi−ck(cid:12)vj|| (cid:29) ||vi−vj||
or ||ck (cid:12)vi −ck (cid:12)vj|| (cid:28) ||vi −vj||, where the magnitude of
drift is quantiﬁed by the ratio of distances in the normalized
embedding. The motivation is to ﬁnd words whose general
usage is similar, but have very different meanings in speciﬁc
communities. We present a few representative examples of
this experiment, for k = The Donald.

Table 5. Representative examples of word drift for The Donald.

Word i
Drift dir. Words with strongest drift
Closer
hillary
Further
hillary
Closer
gun
gun
Further
immigrant Closer
Further
immigrant

crooked, lying, shillary, killary
electable, compromise, favored
merchandise, milo, ﬂair, fakenews
assault, child, fanatics, problem, police
unauthorized, uninsured, parasite
child, abused, future, attorneys, protect

Combining the previous two sections allows us to do an end-
to-end case study on words that drift under a covariate, so
we can explain speciﬁcally which topics (under reweighting)
caused this shift. For example, the words “immigrant” and
“parasite” were signiﬁcantly closer under the weights placed
by The Donald, so we considered dimensions that were si-
multaneously large in the vector vimmigrant − vparasite and
sparse in the weight vector cT he Donald. The dimensions
89 and 139 were sparse and also the 2nd and 3rd largest
coordinates in the difference vector, so they had a large

contribution to the subspace which was zeroed out under
the reweighting. Words that were large in these dimensions
(and thus representative of the zeroed out subspace meaning)
included {misunderstanding, populace, scapegoat, rebuild-
ing} for 89, and {answers, barriers, applicants, backstory,
selﬂess, indigenous} for 139. This suggests two indepen-
dent reasons for the drift: dimensions corresponding to
emotional appeal and legal immigration being zeroed out.

6.3. Covariate-Speciﬁc Analogies

One of the most famous downstream applications of recent
embedding methods such as (Pennington et al., 2014) and
(Mikolov et al., 2013) is representing analogies. This is
formulated as a is to b as c is to d ↔ va − vb ≈ vc − vd, for
example vwoman − vqueen ≈ vman − vking. We considered
how well our method captured covariate-speciﬁc analogies,
which appear in a covariate-speciﬁc embedding but not
most others. Our embedding was able to capture differential
meaning in the form of these speciﬁc analogies: we describe
the results more formally in the appendix. An example is
that “hillary is to liberal as trump is to (white / racist)” are
both much stronger analogies in liberal discussion forums,
and a corresponding strong analogy in conservative forums
is “... as trump is to disenfranchised”. Also, across the
board, “hillary is to politician as trump is to businessman”,
but replacing “businessman” with “conﬁdence” is an en-
riched analogy in conservative forums, and replacing with
“irrationality” is enriched in liberal ones.

Discussion We have presented a joint tensor model that
learns an embedding for each word and for each covariate.
This makes it very simple to compute the covariate speciﬁc
embedding: we just take the element-wise vector product.
It also enables us to systematically interpret the covariate
vector by looking at dimensions along which weight is large
or 0. Our experiments show that these dimensions can be
interpreted as coherent topics. While we focus on word
embeddings, our tensor covariate embedding model can be
naturally applied in other settings. For example, there is
a large amount of interest in learning embeddings of indi-
vidual genes to capture biological interactions. The natural
covariates here are the cell types and our method would
be able to model cell-type speciﬁc gene interactions. An-
other interesting setting with conditional covariates would
be time-series speciﬁc embeddings, where data efﬁciency
becomes more of an issue. We hope our framework is gen-
eral enough that it will be of use to practitioners in these
settings and others.

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

Acknowledgements

References

This research was supported by NSF Graduate Fellowship
DGE-1656518. The authors thank Tatsunori Hashimoto for
helpful comments and Lilly Shen for helping with illustra-
tions.

Arora, Sanjeev, Ge, Rong, Halpern, Yoni, Mimno, David,
Moitra, Ankur, Sontag, David, Wu, Yichen, and Zhu,
Michael. A practical algorithm for topic modeling with
provable guarantees. Proceedings of the International
Conference on Machine Learning, 2013.

Arora, Sanjeev, Li, Yuanzhi, Liang, Yingyu, Ma, Tengyu,
and Risteski, Andrej. Rand-walk: A latent variable model
approach to word embeddings. Transactions of the Associ-
ation for Computational Linguistics (TACL), 4:385–399,
2016.

Blei, David M., Ng, Andrew Y., and Jordan, Michael I.
Latent dirichlet allocation. Journal of Machine Learning
Research, 3:993–1022, 2003.

Bullinaria, John A. and Levy, Joseph P. Extracting semantic
representations from work co-occurrence statistics: A
computational study. Behavior Research Methods, 39.

Cotterell, Ryan, Poliak, Adam, Van Durme, Benjamin, and
Eisner, Jason. Explaining and generalizing skip-gram
through exponential family principal component analysis.
In Proceedings of the 15th Conference of the European
Chapter of the Association for Computational Linguistics:
Volume 2, Short Papers, 2017.

Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive
subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12.

Hope, Jonathan. The Authorship of Shakespeare’s Plays:
A Socio-Linguistic Study. Cambridge University Press,
1994.

Jastrzebski, Stanislaw, Lesniak, Damian, and Czarnecki,
Wojciech Marian. How to evaluate word embeddings?
on importance of data efﬁciency and simple supervised
tasks. ArXiv preprint, 2017.

Kim, Yoon, Chiu, Yi-I, Hanaki, Kentaro, Hegde, Darshan,
and Petrov, Slav. Temporal analysis of language through
neural language models. Proceedings of the ACL 2014
Workshop on Language Technologies and Computational
Social Science, 2014.

Kingma, Diederik and Ba, Jimmy. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.

Liu, Pengfei, Qiu, Xipeng, and Huang, Xuanjing. Learning
context-sensitive word embeddings with neural tensor
skip-gram model. In Proceedings of the Twenty-Fourth
International Joint Conference on Artiﬁcial Intelligence,
2015.

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado,
Greg S, and Dean, Jeff. Distributed representations of
words and phrases and their compositionality. pp. 3111–
3119, 2013.

Neelakantan, Arvind, Shankar, Jeevan, Passos, Alexandre,
and McCallum, Andrew. Efﬁcient non-parametric esti-
mation of multiple embeddings per word in vector space.
Conference on Empirical Methods in Natural Language
Processing, 2014, 2015.

Pennington, Jeffrey, Socher, Richard, and Manning, Christo-
pher D. Glove: Global vectors for word representation.
14:1532–1543, 2014.

Reisinger, Joseph and Mooney, Raymond J. Multi-prototype
vector-space models of word meaning. Human Language
Technologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computational
Linguistics, pp. 109–117, 2010.

Robinson, Michael D., Boyd, Ryan L., Fetterman, Adam K.,
and Persich, Michelle R. Journal of Language and Social
Psychology, 36:438–461.

Rudolph, Maja, Ruiz, Francisco, Athey, Susan, and Blei,
David. Structured embedding models for grouped data.
In Advances of Neural Information Processing Systems,
2017.

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

A. Covariate-speciﬁc analogies

We wish to learn covariate-speciﬁc analogies of the form a is to b as c is to d. To this end, we considered experiments of the
form: for ﬁxed words a, b, c, determine words d such that for some covariate k, the quantity

(ck (cid:12) va − ck (cid:12) vb) · (ck (cid:12) vc − ck (cid:12) vd)
||ck (cid:12) va − ck (cid:12) vb||||ck (cid:12) vc − ck (cid:12) vd||

(6)

is small, yet for other k, the quantity is large. The intuition is that under the covariate transform, vc − vd points roughly in
the same direction as va − vb, and d is close to c in semantic meaning.

In particular, we set a = “hillary”, c = “trump”, and found words b for which there existed a d consistently at the top across
subreddits (implying existence of strong analogies). For example, when b = “woman”, d = “man” was the best analogy for
every weighting. Then, for these b, we considered words d whose relative rankings in the subreddits had high variance. The
differential analogies captured were quite striking: the experiment is able to reveal words whose relative meaning in relation
to anchor words such as “hillary” and “trump” drifts signiﬁcantly.

Table 6. Analogies task. Each best analogy d was one of the top-ranked words in every embedding. We present words whose “relative
analogy” rank was enriched in some embedding. Subreddits are color-coded: green for news-related WN and N (worldnews, news), blue
for left-leaning P and S (politics, SandersForPresident), red for right-leaning D (The Donald), black for A (AskReddit).

Word b

Best analogy d Word

woman

man

democrat

republican

liberal

conservative

politician

businessman

abysmal
amateur
zionist
politician
president
nationalists
south
bigot
christian
white
racist
sociopathic
disenfranchised
conﬁdence
questionable
irrational

High rank
1351 (S), 2218 (P)
1543 (P), 3966 (S)
1968 (P), 2327 (S)
2796 (WN), 3155 (D)
2452 (D), 3564 (WN)
208 (S), 606 (D)
3511 (P)
400 (S), 530 (A)
33 (P)
619 (P)
252 (P)
1756 (D)
3693 (D)
2768 (D)
598 (WN)
2153 (N), 3430 (P)

Low rank
14329 (base), 14077 (D)
13840 (base), 13734 (D)
14173 (base), 14248 (A)
11959 (base), 10386 (S)
12257 (base)
8916 (base), 7526 (A)
11091 (base)
12888 (D), 12994 (WN)
12756 (D), 12722 (WN)
12824 (D), 13273 (base)
12930 (S), 12779 (D)
13744 (P), 13389 (A)
11267 (base)
10528 (base)
13002 (base)
13305 (base)

B. Sparsity results: book dataset

Number of sparse coordinates (out of 100) were as follows, by series and then book order: Harry (0, 5, 1, 3, 7, 4, 0),
Chronicles (0, 0, 0, 1, 0, 0, 0), Song (8, 8, 9, 4, 11), Twilight (6, 6, 8, 7), Screwtape (5), Strike (3, 2, 2), Host (6), Vacancy
(4). While not as dramatic as in the politics dataset, the presence of zero (rather than small) coordinates across multiple runs
shows that there still is speciﬁcity of topics being learned. We plot the histogram of coordinate sizes in the following ﬁgure.

C. Algorithm setting notes

We also experimented with using (Duchi et al.) as the optimization method, but the resulting weight vectors in the politics
dataset had highly-overlapping sparse dimensions. This implies that the optimization method tried to ﬁt the model to a
smaller-dimensional subspace, which is not a desirable source of sparsity.

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

Figure 5. Histogram, sizes of weights in covariate vectors of book data.

(a) Non-zero dimensions in weight vectors by epoch and opti-
mization method. Upper: Adam; lower: Adagrad.

(b) Non-zero dimensions in weight vectors by epoch. Upper:
initialization centered around all-1 vector; lower: centered around
all-0 vector.

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor
Decompositions

Kevin Tian * 1 Teng Zhang * 2 James Zou 3

8
1
0
2
 
l
u
J
 
7
 
 
]
L
C
.
s
c
[
 
 
2
v
9
3
8
7
0
.
2
0
8
1
:
v
i
X
r
a

Abstract
Word embedding is a useful approach to cap-
ture co-occurrence structures in large text corpora.
However, in addition to the text data itself, we of-
ten have additional covariates associated with indi-
vidual corpus documents—e.g. the demographic
of the author, time and venue of publication—and
we would like the embedding to naturally capture
this information. We propose CoVeR, a new ten-
sor decomposition model for vector embeddings
with covariates. CoVeR jointly learns a base em-
bedding for all the words as well as a weighted
diagonal matrix to model how each covariate af-
fects the base embedding. To obtain author or
venue-speciﬁc embedding, for example, we can
then simply multiply the base embedding by the
associated transformation matrix. The main ad-
vantages of our approach are data efﬁciency and
interpretability of the covariate transformation.
Our experiments demonstrate that our joint model
learns substantially better covariate-speciﬁc em-
beddings compared to the standard approach of
learning a separate embedding for each covari-
ate using only the relevant subset of data, as well
as other related methods. Furthermore, CoVeR
encourages the embeddings to be “topic-aligned”
in that the dimensions have speciﬁc independent
meanings. This allows our covariate-speciﬁc em-
beddings to be compared by topic, enabling down-
stream differential analysis. We empirically eval-
uate the beneﬁts of our algorithm on datasets, and
demonstrate how it can be used to address many
natural questions about covariate effects.

Accompanying code to this paper can be found at
http://github.com/kjtian/CoVeR.

*Equal contribution 1Department of Computer Science, Stan-
ford University 2Department of Management Science and Engi-
neering, Stanford University 3Department of Biomedical Data
Science Stanford University. Correspondence to: Kevin Tian <kj-
tian@stanford.edu>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

1. Introduction

The use of factorizations of co-occurrence statistics in learn-
ing low-dimensional representations of words is an area that
has received a large amount of attention in recent years, per-
haps best represented by how widespread algorithms such
as GloVe (Pennington et al., 2014) and Word2Vec (Mikolov
et al., 2013) are in downstream applications. In particular,
suppose we have a set of words i ∈ [n], where n is the size
of the vocabulary. The aim is to, for a ﬁxed dimensionality
d, assign a vector vi ∈ Rd to each word in the vocabulary
in a way that preserves semantic structure.

In many settings, we have a corpus with additional covari-
ates on individual documents. For example, we might have
news articles from both conservative and liberal-leaning
publications, and using the same word embedding for all
the text can lose interesting information. Furthermore, we
suggest that there are meaningful semantic relationships
that can be captured by exploiting the differences in these
conditional statistics. To this end, we propose the following
two key questions that capture the problems that our work
addresses, and for each, we give a concrete motivating ex-
ample of a problem in the semantic inference literature that
it encompasses.

Question 1: How can we leverage conditional co-
occurrence statistics to capture the effect of a covariate
on word usage?

A simple example of a covariate one may wish to condi-
tion on is the document the writing came from (i.e. for a
group of books, learn a set of word embeddings for each
book). There are many natural ways to capture the effect of
this conditioning; we choose to do so by representing the
covariate as a vector. It is interesting to see if the resulting
vectors cluster by author, for example. An example appli-
cation is addressing the question: did William Shakespeare
truly write all the works credited to him, or have there been
other “ghostwriters” who have contributed to the Shake-
speare canon? This is the famous Shakespeare authorship
question, for which historians have proposed various candi-
dates as the true authors of particular plays or poems (Hope,
1994). If the latter scenario is the case, what in particular
distinguishes the writing style of one candidate from an-

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

other, and how can we infer who the most likely author of a
work is from a set of candidates? We show that it is possible
to simultaneously address the authorship question, and also
learn covariate vectors which are interpretable in terms of
their effects on word usage (measured by reweighting the
importance of topics). This motivates our next question.

Question 2: Traditional factorization-based embedding
methods are rotationally invariant, so that individual di-
mensions do not have semantic meaning. How can we break
this invariance to yield a model which aligns topics with
interpretable dimensions?

There has been much interest in the differences in language
and rhetoric that appeal to different demographics. For
example, studies have been done regarding “ideological sig-
natures” speciﬁc to voters by partisan alignment (Robinson
et al.) in which linguistic differences were proposed along
focal axes, such as the “mind versus the body” in texts with
more liberal or conservative ideologies. How can we sys-
tematically infer topical differences such as these between
different communities?

Questions such as these, or more broadly covariate-speciﬁc
trends in word usage, motivated this study. Concretely, our
goal is to provide a general framework through which em-
beddings of sets of objects with co-occurrence structure,
as well as the effects of conditioning on particular covari-
ates, can be learned jointly. As a byproduct, our model
also gives natural meaning to the different dimensions of
the embeddings, by breaking the rotational symmetry of
previous embedding-learning algorithms, such that the re-
sulting vector representations of words and covariates are
“topic-aligned”.

Our Contributions Our main contributions are CoVeR,
a decomposition algorithm that addresses the goals and
issues discussed in the introduction, and the methods for
systematic analysis we propose. Namely, we propose a
method which pools information between conditional co-
occurrence statistics to learn covariate-speciﬁc embeddings
in a data-efﬁcient way, as well as an interpretable embed-
ding of covariate vectors. We evaluate our method against
conditional GloVe as well as the related method of (Rudolph
et al., 2017).

Paper Organization We discuss related works in section
2 and in section 3, we provide our embedding algorithm, as
well as mathematical justiﬁcation for its design. In section
4, we describe our dataset. In section 5, we validate our
algorithm with respect to intrinsic properties and standard
metrics. In section 6, we propose several experiments for
systematic downstream analysis.

2. Background and related works

When designing an algorithm for learning covariate-speciﬁc
embeddings, we identiﬁed three main goals that the algo-
rithm should satisfy. The algorithm should be 1) covariate-
speciﬁc, 2) data-efﬁcient, and 3) interpretable. Goals 1 and
3 go without saying; goal 2 arises in our setting because of-
tentimes, the conditional co-occurrence counts can be quite
small, especially when there are many covariates. We deﬁne
all three in very general terms, and use them to evaluate
existing methods which can be used to handle our condi-
tional embedding learning task. In this section, we describe
several existing approaches and how well they address these
goals, which will serve as the basis for our evaluation later
in the paper.

Conditional GloVe As a brief introduction to embedding
methods, all such algorithms generally rely on the intuition
that some function of the co-occurrence statistics is low
rank. Studies such as GloVe and Word2Vec proposed based
on minimizing low-rank approximation-error of nonlinear
transforms of the co-occurrence statistics. let A be the n × n
matrix with Aij the co-occurrence between words i and j,
where co-occurrence is deﬁned as the (possibly weighted)
number of times the words occur together in a window of
ﬁxed length. For example, GloVe aimed to ﬁnd vectors
vi ∈ Rd and biases bi ∈ R such that the loss

J(v, b) =

f (Aij)(vT

i vj + bi + bj − log Aij)2

(1)

n
(cid:88)

i,j=1

was minimized, where f was some ﬁxed increasing weight
function. Word2Vec aimed to learn vector representations
via minimizing a neural network-based loss function (it can
be shown that Word2Vec and GloVe are essentially per-
forming the same factorization with respect to a different
loss function). A related embedding approach is to directly
perform principal component analysis on the PMI (point-
wise mutual information) matrix of the words (Bullinaria &
Levy). PMI-factorization based methods aim to ﬁnd vectors
P(i,j)
{vi} such that vT
i vj ≈ P M I(A)ij = log
P(i)P(j) , where
the probabilities are taken over the co-occurrence matrix.
This is essentially the same as ﬁnding a low-rank matrix V
such that V T V ≈ P M I, and empirical results show that
the resulting embedding captures useful semantic structure.

A simple baseline for learning conditional embeddings thus
is simply performing GloVe (or another non-conditional
embedding learning method) on each conditional co-
occurrence matrix, and using the resulting vectors as our
conditional embedding. This certainly satisﬁes goal 1 that
we deﬁned earlier, but neither goal 2 nor goal 3 is addressed.
Regarding goal 2, this naive method does not pool infor-
mation between the co-occurrence matrices, and thus is
inefﬁcient in its learning of semantic meaning, which is

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

very relevant when the conditional counts are not large (for
example, if the covariate is time intervals, the conditional
counts can be very small), which we show empirically. Re-
garding goal 3, because of the rotational invariance of the
method, there is no straightforward way to relate the condi-
tional embeddings to one another. There has been previous
work which learns a different set of embeddings on each
conditional slice and then tries to postprocess to align these
embeddings, such as (Kim et al., 2014). However, these
works still run into the data efﬁciency issues we describe,
and it is difﬁcult to analyze the quality of the alignment (for
example, the embedding algorithm on different slices of the
tensor may converge to minima which are not related by a
rotation, therefore an alignment may not exist).

Tensor-based Conditional Embedding Methods An in-
teresting recent work related to our method is the conditional
embedding method S-EFE (Rudolph et al., 2017). Their
work also aims to learn a different set of word embeddings
for each covariate (the same problem we consider), and does
so via a neural-net based objective function. Their work
is analogous to our work up to the parameterization of the
covariates (our work parameterizes covariates via a diagonal
scaling matrix, and theirs by a small neural network). This
is similar in some ways to how Word2Vec and GloVe are
analogous, but very different in others: the primary differ-
ence between Word2Vec and GloVe is in the choice of loss
between distributions in the objective, and our work differs
from S-EFE more fundamentally in the parameterization.
Furthermore, this work does a sub-optimal job of addressing
goals 2 (due to the large number of parameters necessary
in a neural-net based objective) and 3 (interpreting parame-
ters in a nonlinear transform is difﬁcult), which we show in
evaluations.

There are a few other related works which also try to learn
grouped embeddings via tensor decomposition (Cotterell
et al., 2017) (Liu et al., 2015). However, these works con-
sider conditioning in a restricted sense (typically with re-
spect to conditioning on different semantic meanings of a
word), do not aim to interpret the context vectors, and do not
try to preserve rotational alignment with interpretable dimen-
sions. In particular, because these models do not consider
the direct representation of the covariate in the objective
function, the representations are missing key structural prop-
erties (for example topic alignment), which doesn’t allow
for meaningful downstream analysis. In this sense, our work
aims to solve a more general problem, and provides further
justiﬁcation for tensor-based embedding methods.

Multi-sense Embeddings There has been work which
aims to address the multiple-meaning problem which arises
in learning word embeddings, via multi-sense embeddings
(Reisinger & Mooney, 2010) (Neelakantan et al., 2015).

Typically, these algorithms work by allocating multiple vec-
tors for each word (either a ﬁxed count or a varying number),
and optimizing an objective function which picks out one
of the vectors for each word. For example, if words i and
j occur in the same context, the corresponding term in the
objective function might use the embeddings (or senses) of
i and j which are closest to each other. In some ways, this
is similar to our idea that the same word can have different
meanings in different contexts. However, it is addressing a
fundamentally different problem. The “senses” which are
learned correspond to differing meanings of a word, whereas
our work relies on the intuition that across contexts, a word
will have roughly similar meanings, but usage may differ
in some ways, perhaps with respect to certain topics. Fur-
thermore, while multi-sense embeddings pose an interesting
preliminary way to address our problem, they are certainly
neither covariate-speciﬁc (goal 1), nor interpretable (goal 3,
in that the different learned sense vectors do not need to be
related to each other for the same word).

3. CoVeR: Motivation and Embedding

Algorithm

Notation Throughout this section, we will assume a vo-
cabulary of size n and a discrete covariate to condition on,
where the covariate can take on m values (for example, if
the covariate is the community that the corpus comes from,
i.e. liberal or conservative discussion forums, m is simply
the number of communities). It is easy to see how our al-
gorithm generalizes to higher-order tensor decompositions
when there are multiple dimensions covariates to condition
on (for example, slicing along community and slicing along
timeframe simultaneously). Words will be denoted with
indices i, j ∈ [n] and covariates with index k ∈ [m]. All
embedding vectors will be in Rd, and dimensions in our
embedding are referred to by index t ∈ [d].t
We will denote the co-occurrence tensor as A ∈ Rn×n×m,
where Aijk denotes how many times words i and j occurred
together within a window of some ﬁxed length, in the corpus
coming from covariate k. The result of our algorithm will
be two sets of vectors, {vi ∈ Rd} and {ck ∈ Rd}, as well
as bias terms that also ﬁt into the objective. Finally, let (cid:12)
denote the element-wise product between two vectors.

Objective Function and Discussion Here, we give the
objective function our method minimizes, and provide some
explanation for how one should imagine the effect of the
covariate weights. The objective function we minimize is
the following partial non-negative tensor factorization ob-
jective function for jointly training word vectors and weight
vectors representing the effect of covariates, adapted from
the original GloVe objective (note that ck (cid:12)vi = diag(ck)vi,
where diag(ck) is the diagonal matrix weighting of covari-

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

ate k), which we call J(v, c, b):

n
(cid:88)

m
(cid:88)

i,j=1

k=1

f (Aijk)((ck(cid:12)vi)T (ck(cid:12)vj)+bik+bjk−log Aijk)2

(2)
which is to be optimized over {vi ∈ Rd}, {ck ∈ Rd}, and
{bik ∈ R}. To gain a little more intuition for why this is a
reasonable objective function, note that the resulting objec-
tive for a single “covariate slice”, Jk(v, c, b), is essentially

n
(cid:88)

i,j=1

f (Aijk)((ck (cid:12) vi)T (ck (cid:12) vj) + bik + bjk − log Aijk)2

(3)
which ﬁts the vectors ck (cid:12) vi to the data, thus approximat-
ing the statistic log Aijk with (cid:80)d
kt. Note that in
the case m = 1, the model we use is identical to the stan-
dard GloVe model since the ck can be absorbed into the vi.
We used f (x) = ( min(100,x)
)0.75, to parallel the original
100
objective function in (Pennington et al., 2014).

t=1 vitvjtc2

One can think of the dimensions our model learns as inde-
pendent topics, and the effects of the covariate weights ck
as up- or down-weighting the importance of these topics in
contributing to the conditional co-occurrence statistics.

A Geometric View of Embeddings and Tensor Decom-
position We provide a geometric perspective on our
model. Throughout this section, note at a high level, the aim
of our method is to learn sets {vi ∈ Rd}, {ck ∈ Rd}, and
{bik ∈ R} for 1 ≤ i ≤ n, 1 ≤ k ≤ m, such that for a ﬁxed
k, the vectors {ck (cid:12) vi} and the biases {bik} approximate
the vectors and biases learned from running GloVe on only
the kth slice of the co-occurrence tensor.

We now provide some rationale for why this is a reasonable
objective. The main motivation for our algorithm is that
under standard distributional assumptions, the uniform dis-
tribution that word vectors are sampled from can be seen as
a sphere (Arora et al., 2016), which has been experimentally
veriﬁed. A natural way to model the effect of condition-
ing on a covariate is to replace this spherical distribution
with an ellipse, which is the equivalent of reweighting the
dimensions of the word vectors with respect to some basis.
We model this effect as multiplying the embedding vectors
themselves by a symmetric PSD matrix, speciﬁc to the co-
variate which is being conditioned on. In this framework,
assign each covariate k its own symmetric PSD matrix, Bk.
It is well-known that any symmetric PSD matrix can be
factorized as Bk = RT
k DkRk for some orthonormal basis
Rk and some (nonnegative) diagonal Dk; it thus sufﬁces to
consider the effect of a covariate on some ground truth “base
embedding” M as applying the linear operator Bk to each
embedding vector, resulting in the new embedding BkM .

This model is quite expressive in its own right, but we con-

Figure 1. The effects of conditioning on covariates (covariates are
discussion forums, described in Section 4). Left: baseline em-
bedding with some possible word embedding positionings. Mid-
dle, right: embedding under effect of covariates. For example,
“hillary” and “crooked” are pushed closer together under effects
of The Donald, and “healthcare” pushed closer to “universal” and
“free” under effects of SandersForPresident. “Gun” is moved closer
to “rights” and further from “control” under The Donald, and vice
versa in SandersForPresident.

sider a natural restriction where there exists one basis R
under which the resulting embeddings are affected by co-
variates via multiplication by a diagonal matrix, instead of a
PSD matrix (Fig.1). In particular, we note that

k D2

M T BT

k DkRkRT

k BkM = M T RT

k DkRkM = M (cid:48)T

kM (cid:48)
k
(4)
where M (cid:48)
k = RkM is a rotated version of M . Now, in the
restricted model where all the Rk are equal, we can write
all the M (cid:48)
k as RM , so it sufﬁces to just consider the rota-
tion of the basis that the original embedding was trained in
where R is just the identity (since matrix-factorization based
word embedding models are rotation invariant). Under this
model, the co-occurrence statistics under some transforma-
tion should be equivalent to M T D2
kM . Now, clearly this
is the same as ﬁnding a scaling vector ck (such that Dk is
diag(ci)) and using the vectors vi · ck in the factorization
objective function, exactly which is implied by equation 4.

Note that this is essentially saying that in this distributed
word representation model, there exists some rotation of the
embedding space under which the effect of the covariate
separates along dimensions. The implication is that there
are some set of independent “topics” that each covariate will

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

upweight or downweight in importance (or possibly ignore
altogether with a weight of 0), characterizing the effect of
the conditioning directly in terms of the effect on topics.

Algorithm Details Our model learns the resulting param-
eters {vi ∈ Rd}, {ck ∈ Rd}, and {bik}, by using the Adam
(Kingma & Ba, 2014) algorithm, which was empirically
shown to yield good convergence results in the original
GloVe setting. The speciﬁc hyperparameters used for each
dataset will be described in the next section. All vectors
were initialized as random unit vectors 1.

4. Dataset

We evaluated CoVeR in two primary datasets.
Co-
occurrence statistics were formed by considering size 8 win-
dows and using an inverse-distance weighting (e.g. words
3 apart had 1
3 added), which was suggested by some imple-
mentations of (Pennington et al., 2014).

The ﬁrst dataset, referred to as the “book dataset”, con-
sists of the full text from 29 books written by 4 different
authors. The books we used were J.K. Rowling’s “Harry
Potter” series (7 books), “Cormoran Strike” series (3 books),
and “The Casual Vacancy”; C. S. Lewis’s “The Chronicles
of Narnia” series (7 books), and “The Screwtape Letters”;
George R. R. Martin’s “A Song of Ice and Fire” series (5
books); and Stephenie Meyer’s “Twilight” series (4 books),
and “The Host”. These books are ﬁction works in similar
genres, with highly overlapping vocabularies and common
themes. A trivial way of learning series-speciﬁc tenden-
cies in word usage would be to cluster according to unique
vocabularies (for example, only the “Harry Potter” series
would have words such as “Harry” and “Ron” frequently),
so the co-occurrence tensor was formed by looking at all
words that occurred in all of the series with multiple books,
which eliminated all series-speciﬁc words. Furthermore,
series by the same author had very different themes, so there
is no reason intrinsic to the vocabulary to believe the weight
vectors would cluster by author. The vocabulary size was
5,020, and after tuning our algorithm to embed this dataset,
we used 100 dimensions and a learning rate of 10−5.

The second dataset, the “politics dataset”, was a collection
of comments made in 2016 in 6 different subreddits on the
popular discussion forum reddit, and was selected to address
both Questions 1 and 2. The covariate was the discussion
forum, and the subreddits we used were AskReddit, news,
politics, SandersForPresident, The Donald, and WorldNews.

1We also experimented with initializing covariate weight vec-
tors as random vectors centered around the all 1 vector. This
initialization also yielded the sparsity patterns discussed in the
next section, but converged at a slower rate, and performed simi-
larly on downstream metrics as initializing near all 0, so we kept
this initialization.

AskReddit was a baseline discussion forum with a very gen-
eral vocabulary usage, and the discussion forums for the
Sanders and Trump support bases were also selected, as
well as three politically-relevant but more neutral commu-
nities (it should be noted that the politics discussion forum
tends to be very left-leaning). We considered a vocabulary
of size 15,000, after removing the 28 most common words
(suggested by personal communication amongst the word
embedding community) and entries of the cooccurrence ten-
sor with less than 10 occurrences (for the sake of training
efﬁciency). The embedding used 200 dimensions and a
learning rate of 10−5.

5. Experimental Validation

5.1. Clustering by Weights

We performed CoVeR on the book dataset, and considered
how well the weight vectors of the covariate clustered by se-
ries and also by author. We also considered clustering of the
covariate parameter vectors learned from the S-EFE method.
As a baseline, we considered the topic breakdown on the
same co-occurrence statistics predicted by Latent Dirichlet
Allocation (Blei et al., 2003) with various dimensionalities
(20, 50, 100). For a visualization, see Fig. 2.

For every book in every series, the closest weight vectors
(via the tensor decomposition algorithm) by series were
all the books in the same series. Furthermore, for every
book written by every author, the closest weight vectors
by author were all the books by the same author. This
clear clustering behavior even when only conditioning on
co-occurrence statistics for words that appear in all series
(throwing out series-speciﬁc terms) implies the ability of
the weight vectors to cluster according to higher-order in-
formation, perhaps such as writing style.

In contrast, S-EFE is does not learn interpretable features for
each covariate; simply plotting the S-EFE parameter vector
does not cluster the books by the same author. We recognize
that the comparison of the PCA-projected S-EFE parameter
vectors should not be expected to do well (we tried nonlin-
ear dimension reduction techniques such as T-SNE under
various parameter settings as well, and in no setting did the
projections cluster meaningfully); nonetheless, using the
parameter vectors seemed to be the most natural way of
measuring interpretability with respect to the covariates. All
parameter settings of LDA failed to produce any meaningful
clusters.

5.2. Speciﬁcity of Conditional Embeddings

When measuring comparability across embeddings, one
desirable feature of an embedding algorithm is that it re-
distances words with respect to how much the meaning
changes across covariates. For example, a common word

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

(a) 2D PCA of book dataset weight vec-
tors (CoVeR, 100 dimensions)

Figure 3. Histogram of average pairwise cosine similarity for
context-speciﬁc embeddings of a given word.

(b) 2D PCA of neural net parameter vec-
tors (S-EFE, 100 dimensions).

(c) 2D PCA of book dataset topic vectors,
as predicted by LDA (100 topics)

Figure 2. Visualization of topic vectors, algorithm comparison

such as “the” or “a” has essentially the same meaning in
almost every situation, so the resulting embedding should
not change very much. However, for a word that may have
very different meanings in different contexts (such as “immi-
gration”, which might have different connotations in liberal
vs. conservative texts), we expect the covariate-speciﬁc
embeddings to be more spread out.

A simple way to quantitatively evaluate this notion of “vari-
ance in meaning = spread in embeddings” is to, for each
word, consider the average pairwise cosine distance between
covariate-speciﬁc embeddings. For example, for the word
“dark”, using the book dataset, there will be 29 different
embeddings corresponding to each different set of covari-
ate cooccurrences; over all pairs of books, take the average
cosine distance between the embeddings of the given word.
We plotted the distribution of this average across all words.

The results are quite striking: neural embedding method
such as S-EFE is unable to distinguish between the speci-

ﬁcities of words. This could be a result of the neural net
parameterization learning a different subspace for each co-
variate, which makes comparing embeddings across groups
infeasible. However, CoVeR is able to relate the covariate-
speciﬁc embeddings to each other via a linear re-weighting,
giving meaning to comparisons between the embeddings.
As a baseline, the mean distance for common prepositions
(which we take to represent “stable” words which should
not drift too much under conditioning) is plotted; they were
much more stable in our embedding, as expected.

5.3. Data Efﬁciency and Validation

Consider the problem of learning an embedding for the text
of a particular book (or series). This is fundamentally a
covariate-speciﬁc embedding problem, so we test the data
efﬁciency of our algorithm with respect to two alternative
methods of learning conditional embeddings. The main
advantage given by using CoVeR over applying GloVe to
the individual slices is one of data efﬁciency: by pooling the
co-occurrence statistics of words across other books, we are
able to give less noisy estimates of the vectors, especially
for rare or nonexistent words. We also show that our method
compares very favorably with respect to S-EFE. Individual
books contained between 26747 and 355814 words.

To this end, we performed the following experiment. For
some book k, consider three embeddings: 1) the result of
performing GloVe on the co-occurrence statistics of just
the book, 2) the context-speciﬁc embedding produced by
S-EFE with dimension 100 (different dimensionalities com-
pared similarly) 3) the (weighted) context-speciﬁc embed-
ding resulting from CoVeR. Then, we tested these resulting
embeddings using a standard suite of evaluation metrics
(Jastrzebski et al., 2017), including cluster purity and corre-
lation similarities. Our method outperformed method 1 on
all tasks and method 2 on all but one, often signiﬁcantly.

5.4. Sparsity of Weight Vectors

Because of experimentally veriﬁed isotropic distributional
properties of word embedding vectors (Arora et al., 2016), it
is unreasonable to ask for sparsity in the word embeddings

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

Table 1. Average performance on Harry Potter books, cluster purity.
Larger scores indicate better performance.

Cluster purity
BLESS

AP

CoVeR 0.1602
S-EFE
0.1289
0.1297
GloVe

0.2185
0.1942
0.2042

Battig

0.0877
0.0745
0.0770

Table 2. Average performance on Harry Potter books, correlation
similarities. Larger scores indicate better performance.

Correlation similarities

MEN MTurk

RG65

RW

CoVeR 0.1370
0.0184
S-EFE
0.0593
GloVe

0.0719
0.0986
0.0341

0.1272
-0.0098
0.0133

0.0902
-0.0753
0.0588

themselves. However, our topic-speciﬁc weighting scheme
for the covariate weight vectors implies that sparsity is de-
sirable in the weights. The weight sparsity resulting from
our algorithm was experimentally veriﬁed through many
runs of our embedding method, as well as across different
optimization methods, which to us was a surprising and
noteworthy artifact of our algorithm.

ing with regularizing the word vectors forced the model
into a smaller subspace (i.e. sparse dimensions existed but
were shared across all the words), which is not useful and
provides further evidence for a natural isotropic distribution.

Table 3. Sparse coordinate count, covariate weight vectors, 5 runs.
world
23.2
3.1

politics
25.6
2.2

Sanders Donald

news
16.8
2.4

Ask
18.2
1.4

mean
std

27.2
2.6

36.8
3.6

To conﬁrm that the sparsity was a result of separation of
covariate effects rather than an artifact of our algorithm,
we ran our decomposition on a co-occurrence tensor which
was the result of taking the same slice (subreddit) and sub-
sampling its entries 3 times, creating 3 slices of essentially
similar co-occurrence statistics. We applied our algorithm
with different learning parameters, and the resulting weight
vectors after 90 iterations (when the outputs vectors con-
verged) were extremely non-sparse, with between 0 and 2
sparse coordinates per weight vector. The dimensions that
are speciﬁcally 0 for a covariate corresponds to topics that
are relatively less relevant for that covariate. In the next
section, we develop methods to systematically interpret the
covariate weights in terms of topics.

6. Interpretation

6.1. Inference of Topic Meaning

One interesting problem to the word embedding community
is that of topic inference and modeling, that is discover-
ing underlying topics via analyzing co-occurrence statistics
over corpora (Arora et al., 2013). A simple test of infer-
ring topic meaning (i.e. topics coordinates are associated
with) is to consider the set of words which are large in the
given coordinate. Concretely the task is, given some index
t ∈ [d], output the words whose (normalized) vectors have
largest absolute value in dimension t. We show the results
of this experiment for several of the sparse coordinates in
the AskReddit weight vector:

Figure 4. Histogram, sizes of weights in covariate vectors for pol-
itics dataset. Smallest bucket is zero (< 10−10). Sparsity of
covariates in book dataset deferred to appendix.

Note that in the objective function (3), sparse coordinates
will become “stuck” at 0, because the gradient update ∂J
∂ckt
of ckt is proportional to ckt:

Table 4. Representative sample of top words for selected dimen-
sions: topic inference task. Each word was in top 20 normalized
coordinate values for the dimension t.

n
(cid:88)

i,j=1

4f (Aijk)((ck(cid:12)vi)T (ck(cid:12)vj)+bik+bjk−log Aijk)ckt

(5)
The dimensions that the covariates were sparse in did not
overlap by much: on average, weight vectors had 20.7 sparse
coordinates, and the average number of coordinates that was
sparse in both vectors of a random pair was 5.2. This sug-
gests that the set of “topics” each conditional slice of the
tensor does not care about is fairly speciﬁc. Experiment-

t
99
120
183
194

Highest weighted words, dimension t Meaning
horses, cat, teenager, grandma
tables, driveway, customer, stations
gate, territory, directions, phillipines
sweat, disciplined, beliefs, marines

people/animals
domestic
foreign
military

There are several conclusions to be drawn from this ex-
periment. Firstly, while there is some clear noise, speciﬁc
topic meanings do seem to appear in certain coordinates
(which we infer in the table above). It is reasonable that

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

meaning would appear out of coordinates which are sparsely
weighted in some covariate, because this means that it is
a topic that is relevant in other discussion forums but pur-
posely ignored in some speciﬁc forum, so it is likely to have
a consistent theme. When we performed this task for co-
ordinates which had low variance in their weights between
covariates, the resulting words were much less consistent.

It is also interesting to see how covariates weight a topic
with an identiﬁed meaning. For example, for coordinate 194
(military themes), AskReddit placed negligible weight, news
and worldnews placed weight 2.06 and 2.04, SandersFor-
President and The Donald placed weight 0.41 and 0.38,
and politics placed weight 0.05. This process can also
be reversed - for example, consider coordinates small in
worldnews and large in news. One example was coordi-
nate 188; upon checking the words large in this coordinate
({taser, troops, supremacists, underpaid, rioters, amendment,
racially}) it clearly had themes of rights and protests, which
makes sense as a domestic issue, not a global one.

6.2. Topical Word Drift

We performed the following experiment: which pairs of
words start off close in the baseline embedding, yet under
some covariate weights move far apart (or vice versa)? Con-
cretely, the task is, for a ﬁxed word i and a ﬁxed covariate k,
to identify words j such that ||ck(cid:12)vi−ck(cid:12)vj|| (cid:29) ||vi−vj||
or ||ck (cid:12)vi −ck (cid:12)vj|| (cid:28) ||vi −vj||, where the magnitude of
drift is quantiﬁed by the ratio of distances in the normalized
embedding. The motivation is to ﬁnd words whose general
usage is similar, but have very different meanings in speciﬁc
communities. We present a few representative examples of
this experiment, for k = The Donald.

Table 5. Representative examples of word drift for The Donald.

Word i
Drift dir. Words with strongest drift
Closer
hillary
Further
hillary
Closer
gun
gun
Further
immigrant Closer
Further
immigrant

crooked, lying, shillary, killary
electable, compromise, favored
merchandise, milo, ﬂair, fakenews
assault, child, fanatics, problem, police
unauthorized, uninsured, parasite
child, abused, future, attorneys, protect

Combining the previous two sections allows us to do an end-
to-end case study on words that drift under a covariate, so
we can explain speciﬁcally which topics (under reweighting)
caused this shift. For example, the words “immigrant” and
“parasite” were signiﬁcantly closer under the weights placed
by The Donald, so we considered dimensions that were si-
multaneously large in the vector vimmigrant − vparasite and
sparse in the weight vector cT he Donald. The dimensions
89 and 139 were sparse and also the 2nd and 3rd largest
coordinates in the difference vector, so they had a large

contribution to the subspace which was zeroed out under
the reweighting. Words that were large in these dimensions
(and thus representative of the zeroed out subspace meaning)
included {misunderstanding, populace, scapegoat, rebuild-
ing} for 89, and {answers, barriers, applicants, backstory,
selﬂess, indigenous} for 139. This suggests two indepen-
dent reasons for the drift: dimensions corresponding to
emotional appeal and legal immigration being zeroed out.

6.3. Covariate-Speciﬁc Analogies

One of the most famous downstream applications of recent
embedding methods such as (Pennington et al., 2014) and
(Mikolov et al., 2013) is representing analogies. This is
formulated as a is to b as c is to d ↔ va − vb ≈ vc − vd, for
example vwoman − vqueen ≈ vman − vking. We considered
how well our method captured covariate-speciﬁc analogies,
which appear in a covariate-speciﬁc embedding but not
most others. Our embedding was able to capture differential
meaning in the form of these speciﬁc analogies: we describe
the results more formally in the appendix. An example is
that “hillary is to liberal as trump is to (white / racist)” are
both much stronger analogies in liberal discussion forums,
and a corresponding strong analogy in conservative forums
is “... as trump is to disenfranchised”. Also, across the
board, “hillary is to politician as trump is to businessman”,
but replacing “businessman” with “conﬁdence” is an en-
riched analogy in conservative forums, and replacing with
“irrationality” is enriched in liberal ones.

Discussion We have presented a joint tensor model that
learns an embedding for each word and for each covariate.
This makes it very simple to compute the covariate speciﬁc
embedding: we just take the element-wise vector product.
It also enables us to systematically interpret the covariate
vector by looking at dimensions along which weight is large
or 0. Our experiments show that these dimensions can be
interpreted as coherent topics. While we focus on word
embeddings, our tensor covariate embedding model can be
naturally applied in other settings. For example, there is
a large amount of interest in learning embeddings of indi-
vidual genes to capture biological interactions. The natural
covariates here are the cell types and our method would
be able to model cell-type speciﬁc gene interactions. An-
other interesting setting with conditional covariates would
be time-series speciﬁc embeddings, where data efﬁciency
becomes more of an issue. We hope our framework is gen-
eral enough that it will be of use to practitioners in these
settings and others.

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

Acknowledgements

References

This research was supported by NSF Graduate Fellowship
DGE-1656518. The authors thank Tatsunori Hashimoto for
helpful comments and Lilly Shen for helping with illustra-
tions.

Arora, Sanjeev, Ge, Rong, Halpern, Yoni, Mimno, David,
Moitra, Ankur, Sontag, David, Wu, Yichen, and Zhu,
Michael. A practical algorithm for topic modeling with
provable guarantees. Proceedings of the International
Conference on Machine Learning, 2013.

Arora, Sanjeev, Li, Yuanzhi, Liang, Yingyu, Ma, Tengyu,
and Risteski, Andrej. Rand-walk: A latent variable model
approach to word embeddings. Transactions of the Associ-
ation for Computational Linguistics (TACL), 4:385–399,
2016.

Blei, David M., Ng, Andrew Y., and Jordan, Michael I.
Latent dirichlet allocation. Journal of Machine Learning
Research, 3:993–1022, 2003.

Bullinaria, John A. and Levy, Joseph P. Extracting semantic
representations from work co-occurrence statistics: A
computational study. Behavior Research Methods, 39.

Cotterell, Ryan, Poliak, Adam, Van Durme, Benjamin, and
Eisner, Jason. Explaining and generalizing skip-gram
through exponential family principal component analysis.
In Proceedings of the 15th Conference of the European
Chapter of the Association for Computational Linguistics:
Volume 2, Short Papers, 2017.

Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive
subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12.

Hope, Jonathan. The Authorship of Shakespeare’s Plays:
A Socio-Linguistic Study. Cambridge University Press,
1994.

Jastrzebski, Stanislaw, Lesniak, Damian, and Czarnecki,
Wojciech Marian. How to evaluate word embeddings?
on importance of data efﬁciency and simple supervised
tasks. ArXiv preprint, 2017.

Kim, Yoon, Chiu, Yi-I, Hanaki, Kentaro, Hegde, Darshan,
and Petrov, Slav. Temporal analysis of language through
neural language models. Proceedings of the ACL 2014
Workshop on Language Technologies and Computational
Social Science, 2014.

Kingma, Diederik and Ba, Jimmy. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.

Liu, Pengfei, Qiu, Xipeng, and Huang, Xuanjing. Learning
context-sensitive word embeddings with neural tensor
skip-gram model. In Proceedings of the Twenty-Fourth
International Joint Conference on Artiﬁcial Intelligence,
2015.

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado,
Greg S, and Dean, Jeff. Distributed representations of
words and phrases and their compositionality. pp. 3111–
3119, 2013.

Neelakantan, Arvind, Shankar, Jeevan, Passos, Alexandre,
and McCallum, Andrew. Efﬁcient non-parametric esti-
mation of multiple embeddings per word in vector space.
Conference on Empirical Methods in Natural Language
Processing, 2014, 2015.

Pennington, Jeffrey, Socher, Richard, and Manning, Christo-
pher D. Glove: Global vectors for word representation.
14:1532–1543, 2014.

Reisinger, Joseph and Mooney, Raymond J. Multi-prototype
vector-space models of word meaning. Human Language
Technologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computational
Linguistics, pp. 109–117, 2010.

Robinson, Michael D., Boyd, Ryan L., Fetterman, Adam K.,
and Persich, Michelle R. Journal of Language and Social
Psychology, 36:438–461.

Rudolph, Maja, Ruiz, Francisco, Athey, Susan, and Blei,
David. Structured embedding models for grouped data.
In Advances of Neural Information Processing Systems,
2017.

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

A. Covariate-speciﬁc analogies

We wish to learn covariate-speciﬁc analogies of the form a is to b as c is to d. To this end, we considered experiments of the
form: for ﬁxed words a, b, c, determine words d such that for some covariate k, the quantity

(ck (cid:12) va − ck (cid:12) vb) · (ck (cid:12) vc − ck (cid:12) vd)
||ck (cid:12) va − ck (cid:12) vb||||ck (cid:12) vc − ck (cid:12) vd||

(6)

is small, yet for other k, the quantity is large. The intuition is that under the covariate transform, vc − vd points roughly in
the same direction as va − vb, and d is close to c in semantic meaning.

In particular, we set a = “hillary”, c = “trump”, and found words b for which there existed a d consistently at the top across
subreddits (implying existence of strong analogies). For example, when b = “woman”, d = “man” was the best analogy for
every weighting. Then, for these b, we considered words d whose relative rankings in the subreddits had high variance. The
differential analogies captured were quite striking: the experiment is able to reveal words whose relative meaning in relation
to anchor words such as “hillary” and “trump” drifts signiﬁcantly.

Table 6. Analogies task. Each best analogy d was one of the top-ranked words in every embedding. We present words whose “relative
analogy” rank was enriched in some embedding. Subreddits are color-coded: green for news-related WN and N (worldnews, news), blue
for left-leaning P and S (politics, SandersForPresident), red for right-leaning D (The Donald), black for A (AskReddit).

Word b

Best analogy d Word

woman

man

democrat

republican

liberal

conservative

politician

businessman

abysmal
amateur
zionist
politician
president
nationalists
south
bigot
christian
white
racist
sociopathic
disenfranchised
conﬁdence
questionable
irrational

High rank
1351 (S), 2218 (P)
1543 (P), 3966 (S)
1968 (P), 2327 (S)
2796 (WN), 3155 (D)
2452 (D), 3564 (WN)
208 (S), 606 (D)
3511 (P)
400 (S), 530 (A)
33 (P)
619 (P)
252 (P)
1756 (D)
3693 (D)
2768 (D)
598 (WN)
2153 (N), 3430 (P)

Low rank
14329 (base), 14077 (D)
13840 (base), 13734 (D)
14173 (base), 14248 (A)
11959 (base), 10386 (S)
12257 (base)
8916 (base), 7526 (A)
11091 (base)
12888 (D), 12994 (WN)
12756 (D), 12722 (WN)
12824 (D), 13273 (base)
12930 (S), 12779 (D)
13744 (P), 13389 (A)
11267 (base)
10528 (base)
13002 (base)
13305 (base)

B. Sparsity results: book dataset

Number of sparse coordinates (out of 100) were as follows, by series and then book order: Harry (0, 5, 1, 3, 7, 4, 0),
Chronicles (0, 0, 0, 1, 0, 0, 0), Song (8, 8, 9, 4, 11), Twilight (6, 6, 8, 7), Screwtape (5), Strike (3, 2, 2), Host (6), Vacancy
(4). While not as dramatic as in the politics dataset, the presence of zero (rather than small) coordinates across multiple runs
shows that there still is speciﬁcity of topics being learned. We plot the histogram of coordinate sizes in the following ﬁgure.

C. Algorithm setting notes

We also experimented with using (Duchi et al.) as the optimization method, but the resulting weight vectors in the politics
dataset had highly-overlapping sparse dimensions. This implies that the optimization method tried to ﬁt the model to a
smaller-dimensional subspace, which is not a desirable source of sparsity.

CoVeR: Learning Covariate-Speciﬁc Vector Representations with Tensor Decompositions

Figure 5. Histogram, sizes of weights in covariate vectors of book data.

(a) Non-zero dimensions in weight vectors by epoch and opti-
mization method. Upper: Adam; lower: Adagrad.

(b) Non-zero dimensions in weight vectors by epoch. Upper:
initialization centered around all-1 vector; lower: centered around
all-0 vector.

