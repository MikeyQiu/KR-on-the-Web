Collective Event Detection via a Hierarchical and Bias Tagging Networks
with Gated Multi-level Attention Mechanisms

Yubo Chen1, Hang Yang1, Kang Liu1,2, Jun Zhao1,2 and Yantao Jia3
1 National Laboratory of Pattern Recognition, Institute of Automation,
Chinese Academy of Sciences, Beijing, 100190, China
2 University of Chinese Academy of Sciences, Beijing, 100049, China
3 Huawei Technologies Co., Ltd, Beijing, 100085, China

@nlpr.ia.ac.cn, jamaths.h@163.com
yubo.chen, hang.yang, kliu, jzhao
}

{

Abstract

Traditional approaches to the task of ACE
event detection primarily regard multiple
events in one sentence as independent ones
and recognize them separately by using
sentence-level information. However, events
in one sentence are usually interdependent
and sentence-level information is often insuf-
ﬁcient to resolve ambiguities for some types
of events.
This paper proposes a novel
framework dubbed as Hierarchical and Bias
Tagging Networks with Gated Multi-level
Attention Mechanisms (HBTNGMA) to solve
the two problems simultaneously. Firstly, we
propose a hierarchical and bias tagging net-
works to detect multiple events in one sen-
tence collectively. Then, we devise a gated
multi-level attention to automatically extract
and dynamically fuse the sentence-level and
document-level information. The experimen-
tal results on the widely used ACE 2005
dataset show that our approach signiﬁcantly
outperforms other state-of-the-art methods.

1 Introduction

Event detection (ED) is a crucial subtask of event
extraction, which aims to identify event triggers
and classify them into speciﬁc types from texts.
According to the task deﬁned in Automatic Con-
text Extraction1 (ACE), given the following sen-
tence S1, a robust ED system should be able to rec-
ognize two events: a Die event triggered by died
and an Attack event triggered by ﬁred.

S1: In Baghdad, a cameraman died when an

American tank ﬁred on the Palestine Hotel.

To this end, most methods (Ahn, 2006; Hong
et al., 2011; Chen et al., 2015; Nguyen and Grish-
man, 2016; Liu et al., 2017) model ED as a multi-
classiﬁcation task and predict every word in the

1http://projects.ldc.upenn.edu/ace/

Figure 1: Top 5 event types that co-occur with Attack
event in the same sentence in ACE 2005.

sentence separately to determine whether it trig-
gers a speciﬁc type of event by using sentence-
level information. However, they face two prob-
lems:
(1) Neglecting event interdependency by
separately predicting each event; (2) Sentence-
level information is usually insufﬁcient to resolve
ambiguities for some types of events. In the fol-
lowing, we will use examples to illustrate these
two problems speciﬁcally.
S2: The project

leader was ﬁred for the

bankruptcy of the subsidiary company.

Event interdependency: In S1, ﬁred triggers
an Attack event, while it triggers an End-Position
event in S2. Because of the ambiguity, a tradi-
tional approach may mislabel ﬁred in S1 as a trig-
ger of End-Position event. However, if we know
died triggers a Die event in S1, which is easier to
disambiguate, we tend to predict that ﬁred triggers
an Attack event. The reason is that the events men-
tioned in the same sentence tend to be semanti-
cally coherent and a Die event usually co-occurs
with an Attack event. The similar phenomenon
can be found in S2. We conduct a statistical anal-
ysis on ACE 2005 dataset, and ﬁnd that nearly
30% sentences contain multiple events which is
a proportion we can not ignore. To give an in-
tuitive illustration, the top 5 event types that co-
occur with Attack event in the same sentence are
shown in Figure 1. We call such clues as event

interdependency. Some works (Li et al., 2013;
Yang and Mitchell, 2016; Liu et al., 2016b) rely
on a set of elaborately designed features and com-
plicated natural language processing (NLP) tools
to capture event interdependency. However, these
methods lack generalization, take a large amount
of human effort and are prone to error propaga-
tion problem. Though Nguyen et al. (2016) use a
Recurrent Neural Networks (RNN) based classiﬁ-
cation model to capture the event interdependency
between current event candidate and the former
(left) predicted events, they miss the event interde-
pendency between current event candidate and the
later (right) predicted events, and the later events
can not change the type of current event. The rea-
son is that they classify the words of the sentence
from left to right one by one and only use the for-
mer events to predict the later event types. We
claim that both of the former and later predicted
events are important to predict the event type of
current trigger candidate. For example in S1, the
former predicted Die event can help us to predict
that ﬁred triggers an Attack event, while in S2 the
later predicted bankruptcy event can help us to
predict that ﬁred triggers an End-Position event.
Thus, how to use a neural-based model to capture
all event interdependencies (the interdependencies
between the current event candidate and its for-
mer/later predicted events) in the whole sentence
is a challenging problem.

Sentence-level and document-level informa-
tion: Besides event interdependency, knowing that
American tank is a weapon can also give us addi-
tional evidence to predict that ﬁred triggers an At-
tack event in S1. Similarly in S2, knowing that
project leader is a job title can also help us to
predict that ﬁred triggers an End-Position event.
We call such clues as sentence-level information.
However, sometimes it is difﬁcult even for peo-
ple to classify event types from an isolated sen-
tence. We must resort to document-level informa-
tion. For example, considering the following sen-
tence with an ambiguous word left:

S3: He left the company.
It is hard to tell left triggers a Transport event
which means that he left the place, or an End-
Position event which means that he resigned from
the company. However, if we read the whole doc-
ument, a clue like “He planned to go shopping
before he went home, because he got off work
early today.” would give us more conﬁdence to

believe that left triggers a Transport event, while
a clue like “They held a party for his retire-
ment.” would indicate the aforementioned event
is an End-Position event. We call such clues as
document-level information. Moreover, the conﬁ-
dence of sentence-level and document-level infor-
mation should be taken into consideration when
using them together to construct a broader range
of contextual information. For example in S3,
document-level information will give us more ev-
idence, while in S1 sentence-level information
is enough to disambiguate the types of events.
There have been some feature-based studies (Ji
and Grishman, 2008; Liao and Grishman, 2010;
Huang and Riloff, 2012) that construct rules to
capture document-level information for improv-
ing sentence-level ED. However, they suffer from
two problems:
(1) The features they used of-
ten need to be manually designed and may in-
volve error propagation from existing NLP tools;
(2) Sentence-level and document-level informa-
tion are integrated by a large number of ﬁxed rules,
which is complicated to construct and it will be far
from complete. Thus, how to use a neural-based
model to automatically extract sentence-level and
document-level information and dynamically inte-
grate them is another challenging problem.

In this paper, we propose a Hierarchical and
Bias Tagging Networks with Gated Multi-level
Attention Mechanisms (HBTNGMA) to address
the two problems stated above simultaneously. To
capture event interdependency and collectively de-
tect multiple events in one sentence, we propose a
hierarchical and bias tagging networks for event
In which, we exploit a hierarchical
detection.
RNN-based tagging layer to capture all event in-
terdependencies in the whole sentence and de-
vise a bias objective function to reinforce the in-
ﬂuence of trigger tags on the model2. To use
a broader range of contextual information of the
event candidate, we propose a gated multi-level at-
tention, which can automatically extract sentence-
level and document-level information and inte-
grate them dynamically. In summary, the contri-
butions of this paper are as follows:

•

We propose a novel framework for event
detection, which can automatically extract

2Compared with the task like Named Entities Recogni-
tion, the number of “O” tags is much more than the number
of trigger tags in ED task, i.e. if we use the non-bias objec-
tive function and tag all words in one sentence as “O”, we will
gain a low loss. Thus we devise a bias objective function.

•

and dynamically integrate sentence-level and
document-level information and collectively
detect multiple events in one sentence.

To capture event interdependency, we ex-
ploit a hierarchical and bias tagging net-
works to detect multiple events in one sen-
tence collectively. To automatically extract
and dynamically integrate contextual infor-
mation, we devise a gated multi-level atten-
tion Mechanisms. To our knowledge, this
is the ﬁrst work to jointly use event inter-
dependency, sentence-level information and
document-level information via a neural tag-
ging schema for event detection task.

•

We conduct extensive experiments on a
widely used ACE 2005 dataset, and the ex-
perimental results show that our approach
signiﬁcantly outperforms other state-of-the-
art methods 3.

2 Task Description

Event detection (ED) is a crucial subtask of event
extraction (EE). In this paper, we focus on ED
task deﬁned in ACE evaluation, where an event
is deﬁned as a speciﬁc occurrence involving one
or more participants. Firstly, we introduce some
ACE terminology to facilitate the understanding of
this task: Event trigger: the main word or phrase
that most clearly expresses the occurrence of an
event. Event arguments: the mentions that are in-
volved in an event (viz., participants). Event men-
tion: a phrase or sentence within which an event
is described, including a trigger and arguments.

Given an English text document, an ED system
should identify event triggers and categorize their
event types for each sentence. For instance, in the
sentence “He died in the hospital”, an ED system
is expected to detect a Die event along with the
trigger word “died”. The ACE 2005 evaluation
deﬁnes 8 event types and 33 subtypes, such as At-
tack or Die. Following previous works (Li et al.,
2013; Chen et al., 2015; Liu et al., 2017; Nguyen
and Grishman, 2016), we categorize triggers into
these 33 subtypes.

3 Methodology

In this paper, we formulate event detection as a
sequence labelling task. As shown in Figure 2,

3Our source code, including all hyper-parameter settings
is openly available at

and pre-trained word embeddings,
https://github.com/yubochen/NBTNGMA4ED

we label all words in one sentence collectively via
a Hierarchical and Bias Tagging Networks with
Gated Multi-level Attention Mechanisms (HBT-
NGMA). We assign a tag for each word to indi-
cate whether it triggers a speciﬁc type of event.
We adopt the “BIO” tags schema, where tag “O”
represent the “other” tag which means that the cor-
responding word does not trigger any event, tags
“B-EventType” and “I-EventType” represent the
“Begin-EventType” and “Inside-EventType” tag
respectively. “EventType” means that the word
triggers a speciﬁc type of event. “B” and “I” rep-
resent the position of the word in a trigger to solve
the problem that a trigger word contains multi-
ple words such as “take over”, “go off” and so
on. Thus, the total number of tags is Nt = 2
NeventT ype|
|
the predeﬁned event types and
in this paper as stated above.

NeventT ype|
|

⇤
is the size of
= 33

NeventT ype|

+ 1, where

|

Figure 2 describes the architecture of HBT-
NGMA, which primarily involves the following
four components:
(i) embedding layer, which
transforms each word into a continuous vector; (ii)
BiLSTM layer, which uses a Bidirectional Long
Short Term Memory (BiLSTM) to encode the se-
mantics of each word considering the forward and
backward information; (iii) gated multi-level at-
tention, in which we propose a sentence-level and
document-level attention to automatically extract
sentence-level and document-level information re-
spectively, and we devise a fusion gate to dy-
namically integrate them as context information;
and (iv) Hierarchical tagging layer, in which we
propose two Tagging LSTM (TLSTM1 and TL-
STM2) and a tagging attention to automatically
capture the event interdependency and tag all the
words of the sequence collectively.

3.1 Embedding Layer

This paper uses the learned word embeddings as
the source of basic features. Speciﬁcally, we use
the Skip-gram model (Mikolov et al., 2013) to
learn word embeddings on the NYT corpus.

Given a document d =

s1, s2, ..., si, ..., sNs}
,
{
where Ns is the number of sentences in the doc-
ument. The i-th sentence si can be represented
as token sequence si =
w1, w2, ..., wt, ..., wNw }
,
{
where Nw is length of the sentence and wt is the
t-th token of the sentence. Assume that the word
embedding for token wt is et and we use it as the
input of the following layer.

Figure 2: The architecture of our proposed hierarchical and bias tagging networks with gated multi-level attention.

3.2 BiLSTM Layer

semantic information sht is calculated as follows:

In sequence labelling problems, the BiLSTM has
been proven effective to capture the semantic in-
formation of each word (Lample et al., 2016). In
this paper, we use the LSTM unit as described in
(Zaremba and Sutskever, 2014). For each word
wt, the forward LSTM encodes wt by consider-
ing the contextual information from word w1 to
wt, which is marked as  !h t. Similarly, the back-
ward LSTM encodes wt based on the contextual
information from wNw to wt, which is marked as
  h t. Finally, we concatenate  !h t and   h t to rep-
resent the information of the word wt, denoted as
ht = [ !h t,   h t], and we concatenate  !h Nw and   h 1
to represent the encoding information of the whole
sentence si, denoted as hsi = [ !h Nw ,   h 1].

3.3 Gated Multi-level Attention

Gated multi-level attention primarily involves the
following three components: (i) sentence-level at-
tention layer, which automatically captures im-
portant sentence-level information by consider-
ing the current word; (ii) document-level atten-
tion layer, which automatically captures impor-
tant document-level information by considering
the current sentence; and (iii) fusion gate layer,
which use a fusion gate to dynamically integrate
sentence-level and document-level information.

Sentence-level Attention Layer

Sentence-level attention layer aims to capture the
important clues in sentence level. For each can-
didate word wt in the sentence, its sentence-level

sht =

Nw

k=1

↵k

s hk

where ↵k
tion hk. In this paper, we deﬁne ↵k

s is the weight of each word representa-
s as following:

X

↵k

s =

exp(zk
s )
Nw
j=1 exp(zj
s)

P

where zk
s is the relatedness between the t-th word
representation ht and the k-th word representation
hk, modeled by bilinear attention as:

s = tanh(htWsahT
zk

k + bsa)

(3)

where Wsa is the weight matrix and bsa is the
bias term. Following above sentence-level atten-
tion mechanism, we can get the sentence-level in-
formation for each word wt by considering the se-
mantic information of the word wt.

Document-level Attention Layer

Similar to sentence-level attention, document-
level attention captures the vital clues in the doc-
ument level. The document-level semantic infor-
mation dhi for i-th sentence calculated as follows:

(1)

(2)

(4)

dhi =

X

↵k

d =

Ns

↵k

dhsk

k=1
exp(zk
d )
Ns
j=1 exp(zj
d)

d = tanh(hsi WdahT
zk
P

sk + bda)

where ↵k
sentation hsk , zk

d is the weight of each sentence repre-
d is the relatedness between i-th

sentence representation hsi and the k-th sentence
representation hsk , Wda is the weight matrix and
bda is the bias term. Compared with sentence-level
information, all words in the i-th sentence have the
same document-level information dhi.

Fusion Gate Layer

We devise a fusion gate to dynamically integrate
information sht and document-
sentence-level
level information dhi for the t-th word wt in the
i-th sentence si, and calculate the contextual in-
formation representation crt as follows:

crt = (Gt

sht) + ((1

Gt)

dhi)

(5)

 

 

 

where Gt is a fusion gate aims to model the con-
ﬁdence of clues provided by sentence-level infor-
mation sht and document-level information dhi,
which is calculated as follows:

G =  (Wg[sht, dhi] + bg)

(6)

where Wg is the weight matrix and bg is the bias
term,   is a sigmoid function and
denotes
element-wise multiplication. Finally, the contex-
tual information crt of word wt and its word em-
bedding et are concatenated into a single vector
xrt = [et, crt] as the feature representation of wt.

 

3.4 Hierarchical Tagging Layer

In hierarchical tagging layer, we propose two Tag-
ging LSTMs (TLSTM1 and TLSTM2) and a tag-
ging attention to automatically capture the event
interdependency and tag the sequence collectively.

The First Tagging Layer: TLSTM1
When detecting the tag of word wt in TLSTM1,
the inputs are: the feature representation xrt ob-
tained from embedding layer and gated multi-level
attention layer, former predicted tag vector T 1
1,
t
 
and former hidden vector h1
1 in TLSTM1. The
t
 
detail operations are deﬁned as follows:

t =  (W 1
i1
t =  (W 1
f 1
o1
t =  (W 1
t = '(W 1
u1

t

t

t

 

ix xrt + W 1
fx xrt + W 1
ox xrt + W 1
ux xrt + W 1

ih h1
fh h1
oh h1
uh h1
t
 
t = i1
c1
t + f 1
u1
t  
t  
'(c1
h1
t = o1
t )
t  
t + b1
T h1
t = W 1
T 1
T

 

 

t

t

iT T 1
fT T 1
oT T 1
uT T 1

1 + W 1
 
1 + W 1
1 + W 1
 
1 + W 1
c1
t
 

1

t

t

 

 

1)

1)

1)

1)

(7)

Where it is an input gate, ut is an input modulation
gate, ft is a forget gate, ot is an output gate, ct is a
memory cell and T 1
t is a predicted tagging vector.

The Second Tagging Layer: TLSTM2

Though the TLSTM1 can capture the interdepen-
dency between current event candidate and the for-
mer predicted event tags, it can not capture the
interdependency between the current event candi-
date and the later predicted event tags. Thus, we
devise a second tagging layer (TLSTM2) upon the
LSTM1 to capture the interdependency between
the current event candidate and both of former and
later predicted event tags from TLSTM1. When
detecting the tag of word wt in TLSTM2, the in-
the feature representation xrt, former
puts are:
predicted tag vector T 2
1 in TLSTM2, the pre-
t
 
liminary predicted information T a
t calculated from
TLSTM1, and former hidden vector h2
1 in TL-
t
 
STM2. The detail operations are deﬁned as fol-
lows:

i2
t =  (W 2
t =  (W 2
f 2
o2
t =  (W 2
t = '(W 2
u2

t

t

t

t

t

 

 

iT T 2
1 + W 2
fT T 2
1 + W 2
 
oT T 2
1 + W 2
1 + W 2
uT T 2
c2
t
 

ix xrt + W 2
ih h2
fx xrt + W 2
fh h2
ox xrt + W 2
oh h2
ux xrt + W 2
uh h2
t
 
t = i2
c2
t + f 2
u2
t  
t  
'(c2
h2
t = o2
t )
t  
t + b2
T h2
t = W 2
T 2
T

 

1

t

t

 

 

1 + W 2
1 + W 2
 
1 + W 2
1 + W 2

ia T a
t )
fa T a
t )
oa T a
t )
ua T a
t )

The unit structure of TLSTM2 is similar to the
unit of TLSTM1. The parts need to pay attention
are follows: (1) the initial hidden input h2
0 of the
TLSTM2 is the last hidden vector h1
Nw of the TL-
STM1. (2) the preliminary predicted information
T a
t is calculated from TLSTM1 by using a tagging
attention as follows.

Tagging Attention

Tagging attention aims to automatically encode
the preliminary predicted information T a
for the
t
word wt and the details are as follows:

(8)

(9)

T a
t =

↵k

T =

Nw

T T 1
↵k
k

k=1
X
exp(zk
T )
Nw
j=1 exp(zj
T )
t Wta(T 1

T = tanh(T 1
zk
P

k )T + bta)

where ↵k
dicted tag T 1
k , zk
preliminary predicted tag T 1
inary predicted tag T 1
and bta is the bias term.

T is the weight of each preliminary pre-
T is the relatedness between t-th
t and the k-th prelim-
k , Wta is the weight matrix

The ﬁnal normalized tag probability for word
t from

wt is based on the predicted tag vector T 2

TLSTM2 and computed as follows:

Hyper-parameter Setting

Ot = WyT 2

t + by

p(Oi
t|

sj,✓ ) =

exp(Oi
t)
Nt
k=1 exp(Ok
t )

(10)

P

where p(Oi
sj,✓ ) is the probability that assigning
t|
the i-th tag to word wt in sentence sj when param-
eters is ✓, and Nt is the total number of tags.

3.5 Training with Bias Objective Function

In one sentence, the number of “O” tags is much
more than the number of trigger tags. Thus, we
devise a bias objective function J(✓) to reinforce
the inﬂuence of trigger tags on the model, which
is deﬁned as follows:

J(✓) = max

Nts

j=1
+ ↵ log p(Oyt
t |

X

X
sj,✓ )

Nw

t=1

(log p(Oyt
t |
I(O)))

(1

·

 

sj,✓ )

I(O)

·

(11)

where Nts is the number of training sentences, Nw
is the length of sentence sj, p(Oyt
sj,✓ ) is the nor-
t |
malized probabilities of tags deﬁned in Formula
10 and yt is the golden tag of word wt in sentence
sj, ↵ is the bias weight and the larger ↵ will bring
the greater inﬂuence of trigger tags on the model.
Besides, I(O) is a switching function to distin-
guish the loss of tag “O” and trigger tags, which
is deﬁned as follows:

I(O) =

⇢

1,
0,

if
if

tag = “O”
= “O”
tag

(12)

To compute the network parameter ✓, we maxi-
mize the log likelihood J (✓) through stochastic
gradient descent over shufﬂed mini-batches with
the Adadelta (Zeiler, 2012) update rule.

4 Experiments

4.1 Experimental Setting

Dataset and Evaluation Metrics

We conduct experiments on the widely used ACE
2005 dataset. For comparison, as the same as pre-
vious works (Liao and Grishman, 2010; Li et al.,
2013; Chen et al., 2015; Nguyen et al., 2016; Liu
et al., 2017), we used the same test set with 40
documents and the same development set with 30
documents and the rest 529 documents are used
for training. Finally, we use Precision (P ), Recall
(R) and F measure (F1) as the evaluation metrics
as the same as previous work.

Hyper-parameters are tuned on the development
dataset by grid search. We train the word embed-
ding using Skip-gram algorithm 4 on the NYT cor-
pus 5. We set the dimension of word embeddings
as 100, the dimension of tag vector as 20, all the
size of LSTM in BiLSTM layer, TLSTM1 and TL-
STM2 layer as 100, the bias parameter ↵ in For-
mula 11 as 5, the batch size as 20, the learning rate
as 0.001, the dropout rate as 0.5.

4.2 Our Method vs. State-of-the-art Methods

We select the following state-of-the-art methods
for comparison, which can be classiﬁed as two
types: separate and collective methods:

1) Li’s MaxEnt:

Separate methods:

the
method that detects events in one sentence sepa-
rately by using human-designed features (Li et al.,
2013). 2) Liao’s CrossEvent : the method that
uses cross event information (Liao and Grish-
man, 2010). 3) Hong’s CrossEntity: the method
that uses cross entity information (Hong et al.,
2011). 4) Chen’s DMCNN: the dynamic multi-
pooling convolutional neural networks method
(Chen et al., 2015).
5) Chen’s DMCNN+:
the DMCNN method argumented with automati-
cally labeled data (Chen et al., 2017). 6) Liu’s
FrameNet : the method that leverages FrameNet
as extended training data to improve ED (Liu et al.,
2016a). 7) Liu’s ANN-Aug: the method that use
the annotated argument information via a super-
vised attention to improve ED (Liu et al., 2017).
Collective methods: 1) Li’s Structure:
the
method that collectively detects events by us-
ing human-designed features (Li et al., 2013).
2) Yang’s JointEE:
the method that detects
events and entities in one sentence jointly based
on human-designed features (Yang and Mitchell,
2016). 3) Nguyen’s JRNN: the method that ex-
ploits a RNN model to collectively detects events
by only using sentence-level information (Nguyen
et al., 2016). 4) Liu’s PSL : the method that uses
a probabilistic soft logic to detect events by using
human-designed features (Liu et al., 2016b).

Experimental results are shown in Table 1.
From the table, we have the following observa-
(1) Among all the methods, our HBT-
tions:
NGMA achieves the best performance.
It can
improve the best collective method’s F1 by

4https://code.google.com/p/word2vec/
5https://catalog.ldc.upenn.edu/LDC2008T19

Methods
Li’s MaxEnt (2013)
Liao’s CrossEvent (2010)
Hong’s CrossEntity (2011)
Chen’s DMCNN (2015)

Chen’s DMCNN+
Liu’s FrameNet
†
Liu’s ANN-Aug
†

(2017)
†
(2016a)
(2017)

Li’s Structure (2013)
Yang’s JointEE (2016)
Nguyen’s JRNN (2016)
Liu’s PSL (2016b)
Ours HBTNGMA

P
74.5
68.7
72.9
75.6
75.7
77.6
76.8
73.7
75.1
66.0
75.3
77.9

R
59.1
68.9
64.3
63.6
66.0
65.2
67.5
62.3
63.3
73.0
64.4
69.1

F1
65.9
68.8
68.3
69.1
70.5
70.7
71.9
67.5
68.7
69.3
69.4
73.3

Table 1: Overall performance on blind test data. The
upper table illustrates the performance of separate ED
systems and the lower illustrates collective ED systems.

means the method that uses external resources.

†

3.9% (Liu’s PSL) and improve the best sepa-
rate method’s F1 by 1.4% (Liu’s ANN-Aug) al-
though Liu’s ANN-Aug uses FrameNet as exter-
nal resources. We also perform a t-test (p 6
0.05), which indicates that our method signiﬁ-
cantly outperforms all of the compared methods.
(2) Comparing our HBTNGMA to separate meth-
ods, it achieves a better performance.
It proves
that collectively predicting multiple events in one
sentence is effective.
(3) Our HBTNGMA out-
performs feature-based collective methods (Li’s
Structure, Yang’s JointEE and Liu’s PSL),
it
proves that our automatically learned features
can efﬁciently capture semantic information from
plain texts. (4) Compared with Nguyen’s JRNN,
HBTNGMA gains a 4.0% improvement on F1
value. The reason is that Nguyen’s JRNN only
uses sentence-level information while our model
exploits multi-level information, and our model
can capture the interdependencies between the
current event candidate and its former/later pre-
dicted events simultaneously.

4.3 Effect of The Hierarchical and Bias

Tagging Networks

In this subsection, we prove the effectiveness of
hierarchical and bias tagging networks for collec-
tive ED. We select following methods as base-
lines: 1) LSTM+Softmax: a simpliﬁed version
of our HBTNGMA, which directly use a soft-
max layer to separately detect events after we
get the feature representation xrt of each word
wt. 2) LSTM+CRF: the method is similar to
our HBTNGMA, which uses a CRF layer to tag
words instead of our Hierarchical TLSTM (HTL-
STM) tagging layer.
3) LSTM+TLSTM: the
method is similar to our HBTNGMA, which only

use a TLSTM1 and takes all tags have same in-
ﬂuence in training loss (i.e. ↵ in is set as 1)
. 4) LSTM+HTLSTM: the method is similar
to our HBTNGMA, which use a HTLSTM (TL-
STM1+TLSTM2) and do not use bias objective
function. And LSTM+HTLSTM+Bias is our pro-
posed HBTNGMA. Moreover, we divide the test-
ing data into two parts according the event num-
ber in a sentence (single event and multiple events)
and perform evaluations separately.

Method
LSTM+Softmax
LSTM+CRF
LSTM+TLSTM
LSTM+HTLSTM
LSTM+HTLSTM+Bias

1/1
74.7
75.1
76.8
77.9
78.4

1/N
44.6
49.5
51.2
57.3
59.5

all
66.8
68.5
70.2
72.4
73.3

Table 2: Performance of different ED systems. 1/1
means one sentence that only has one event and 1/N
means that one sentence has multiple events.

Surprisingly,

following observations:

Table 2 shows the results. And we have
the
1) Compared
with LSTM+Softmax, LSTM-based collective
(LSTM+CRF, LSTM+TLSTM,
ED methods
LSTM+HTLSTM, LSTM+HTLSTM+Bias) achie-
ves a better performance.
the
LSTM+HTLSTM+Bias yields a 14.9% improve-
ment on the sentence contains multiple events
over the LSTM+Softmax. It proves neural tagging
schema is effective for ED task especially for
the sentences contain multiple events.
2) The
LSTM+TLSTM achieve better performances than
LSTM+CRF. And the LSTM+HTLSTM achieve
better performances than LSTM+TLSTM. The
results prove the effectiveness of the TLSTM
3) Compared with
layer and HTLSTM layer.
LSTM+HTLSTM+Bias
LSTM+HTLSTM,
the
gains a 0.9% improvement on all sentence.
It
demonstrates the effectiveness of our proposed
bias objective function.

4.4 Effect of The Gated Multi-level Attention

This subsection studies the effectiveness of our
gated multi-level attention. We adopt same archi-
tecture of our HBTNGMA as shown in Figure 2
with different level clues as baselines: 1) Word
Only is the method only uses word embedding et
to identify events. 2) Word+SA uses sentence-
level attention to capture important sentence-level
information as additional clues. 3) Word+DA
uses document-level attention to capture important
document-level information as additional clues. 4)
Word+Average MA uses both of sentence-level

and document-level attention to capture multi-
level information and integrate them with a aver-
age gate (all the dimension of the fusion gate are
set as 0.5 ), which is a special case of our pro-
posed HBTNGMA. And Word+Gated MA is our
proposed HBTNGMA model.

Method
Word Only
Word+SA
Word+DA

P
70.1
75.6
73.1
Word+Average MA 76.5
Word+Gated MA
77.9

R
63.4
68.2
65.8
68.7
69.1

F1
66.6
71.7
69.3
72.4
73.3

Table 3: Performance of gated multi-level attention.

Results are shown in Table 3. From the re-
sults, we have the following observations: 1)
Compared with word only, Word+SA achieves a
better performance. We can make the same ob-
servation when comparing Word+DA with word
only.
It proves that both sentence-level and
document-level information are helpful for ED
2) Compared with Word+DA, Word+SA
task.
achieves a better performance.
It proves that in
most of cases sentence-level information provides
more clues than document-level information. 3)
Word+Gated MA gains a 0.9% improvement than
Word+Average MA. It demonstrates that the effec-
tiveness of our fusion gate to dynamically inte-
grate clues from multiple levels.

4.5 Case Study

Interesting Cases: Our neural tagging schema not
only can model the interdependency between mul-
tiple events in one sentence as proved in Subsec-
tion 4.3, but also the “BIO” tagging schema can
solve the multiple words trigger inherently. We
conduct a statistical analysis on the experimental
results, and ﬁnd that nearly 50% cases with multi-
ple word trigger was solved by our model. Exam-
ple is shown in Figure 3.

Figure 3: The example of case solved by our model.

Attention Visualization: As limited of space,
we take one sentence with high sentence-level
gated weight (example 1) and one sentence with
high document-level gated weight (example 2) as
examples for attention visualization. As shown in
Figure 4, in example 1, sentence-level informa-
tion plays more important role in disambiguating
ﬁred, and the words (tank, died and Baghdad) give

Figure 4: Attention visualization. The heat map indi-
cate the contextual attention. Blue for sentence-level
and orange for document-level. The pie chart indicate
the fusion gate weight.

us ample evidence to predict that ﬁred triggers an
Attack event. While document-level information
plays a more important role in example 2. The
tiresome.” gives
surrounding sentence “this is ...
us more conﬁdence to predict that leave triggers
an End-Position event.

5 Related Works

Event detection is an increasingly hot and chal-
lenging research topic in NLP. Generally, exist-
ing approaches could roughly be divided into two
groups: separate and collective methods.

Separate methods: These methods regard mul-
tiple events in one sentence as independent ones
These meth-
and recognize them separately.
ods include feature-based methods which exploit
a diverse set of strategies to convert classiﬁca-
tion clues into feature vectors (Ahn, 2006; Ji
and Grishman, 2008; Liao and Grishman, 2010;
Hong et al., 2011; Huang and Riloff, 2012), and
neural-based methods which use neural networks
to automatically capture clues from plain texts
(Chen et al., 2015; Nguyen and Grishman, 2015;
Feng et al., 2016; Nguyen and Grishman, 2016;
Chen et al., 2017; Duan et al., 2017; Liu et al.,
2017). Though effective these methods, they ne-
glect event interdependency by separately predict-
ing each event.

Collective methods: These methods try to
model the event interdependency and detect mul-
tiple events in one sentence collectively. How-
ever, nearly all of these methods are feature-based
methods (McClosky et al., 2011; Li et al., 2013;
Yang and Mitchell, 2016; Liu et al., 2016b), which
rely on elaborately designed features and suffer er-
ror propagation from existing NLP tools. Nguyen
et al. (2016) exploits a neural-based method to de-
tect multiple events collectively. However, they
only use the sentence-level information and ne-

glect document-level clues, and can only capture
the interdependencies between the current event
candidate and its former predicted events. More-
over, there method can not handle the multiple
words trigger problem.

Xiaocheng Feng, Lifu Huang, Duyu Tang, Heng Ji,
A language-
Bing Qin, and Ting Liu. 2016.
independent neural network for event detection. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), vol-
ume 2, pages 66–71.

6 Conclusion

This paper proposes a novel framework for event
detection, which can automatically extract and dy-
namically integrate sentence-level and document-
level information and collectively detect multi-
ple events in one sentence. A hierarchical and
bias tagging networks is proposed to detect mul-
tiple events in one sentence collectively. A gated
multi-level attention is devised to automatically
extract and dynamically integrate contextual infor-
mation. The experimental results on the widely
used dataset prove the effectiveness of the pro-
posed method.

Acknowledgments

The research work is supported by the Natural
Science Foundation of China (No.61533018 and
No.61702512), and the independent
research
project of National Laboratory of Pattern Recog-
nition. This work is also supported in part by
Huawei Technologies Co., Ltd.

References

David Ahn. 2006. The stages of event extraction. In
Proceedings of 44th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pages 1–
8.

Yubo Chen, Shulin Liu, Xiang Zhang, Kang Liu, and
Jun Zhao. 2017. Automatically labeled data genera-
tion for large scale event extraction. In Proceedings
of the 55th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 409–419,
Vancouver, Canada. Association for Computational
Linguistics.

Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng,
and Jun Zhao. 2015. Event extraction via dynamic
multi-pooling convolutional neural networks.
In
Proceedings of the 53rd Annual Meeting of the As-
sociation for Computational Linguistic (ACL), pages
167–176.

Shaoyang Duan, Ruifang He, and Wenli Zhao. 2017.
Exploiting document level information to improve
event detection via recurrent neural networks.
In
Proceedings of IJNLP, volume 1, pages 352–361.

Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao,
Guodong Zhou, and Qiaoming Zhu. 2011. Using
cross-entity inference to improve event extraction.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 1127–1136.

Ruihong Huang and Ellen Riloff. 2012. Modeling tex-
tual cohesion for event extraction. In Proceedings of
AAAI.

Heng Ji and Ralph Grishman. 2008. Reﬁning event ex-
traction through cross-document inference. In Pro-
ceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
254–262.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of NAACL, pages 260–270.

Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
In Proceedings of the 51st Annual Meet-
tures.
ing of the Association for Computational Linguistics
(ACL), pages 73–82.

Shasha Liao and Ralph Grishman. 2010. Using doc-
ument level cross-event inference to improve event
extraction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 789–797.

Shulin Liu, Yubo Chen, Shizhu He, Kang Liu, and Jun
Zhao. 2016a. Leveraging framenet to improve auto-
matic event detection. In Proceedings of ACL, pages
2134–2143. Association for Computational Linguis-
tics.

Shulin Liu, Yubo Chen, Kang Liu, and Jun Zhao. 2017.
Exploiting argument information to improve event
detection via supervised attention mechanisms.
In
Proceedings of the 55th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
1789–1798.

Shulin Liu, Kang Liu, Shizhu He, and Jun Zhao. 2016b.
A probabilistic soft logic based approach to exploit-
ing latent and global information in event classiﬁca-
tion. In Proceedings of AAAI, pages 2993–2999.

David McClosky, Mihai Surdeanu, and Christopher
Manning. 2011. Event extraction as dependency
In Proceedings of the 49th Annual Meet-
parsing.
ing of the Association for Computational Linguistics
(ACL), pages 1626–1635.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
Efﬁcient estimation of word
arXiv preprint

frey Dean. 2013.
representations in vector space.
arXiv:1301.3781.

Huu Thien Nguyen and Ralph Grishman. 2015. Event
detection and domain adaptation with convolutional
neural networks. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistic (ACL), pages 365–371.

Thien Huu Nguyen, Kyunghyun Cho, and Ralph Gr-
ishman. 2016. Joint event extraction via recurrent
neural networks. In Proceedings of the 2016 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL), pages
300–309.

Thien Huu Nguyen and Ralph Grishman. 2016. Mod-
eling skip-grams for event detection with convolu-
tional neural networks. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 886–891.

Bishan Yang and Tom M. Mitchell. 2016. Joint extrac-
tion of events and entities within a document con-
text. In Proceedings of NAACL, pages 289–299, San
Diego, California. Association for Computational
Linguistics.

Wojciech Zaremba and Ilya Sutskever. 2014. Learning

to execute. arXiv preprint arXiv:1410.4615.

Matthew D Zeiler. 2012. Adadelta: An adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701.

Collective Event Detection via a Hierarchical and Bias Tagging Networks
with Gated Multi-level Attention Mechanisms

Yubo Chen1, Hang Yang1, Kang Liu1,2, Jun Zhao1,2 and Yantao Jia3
1 National Laboratory of Pattern Recognition, Institute of Automation,
Chinese Academy of Sciences, Beijing, 100190, China
2 University of Chinese Academy of Sciences, Beijing, 100049, China
3 Huawei Technologies Co., Ltd, Beijing, 100085, China

@nlpr.ia.ac.cn, jamaths.h@163.com
yubo.chen, hang.yang, kliu, jzhao
}

{

Abstract

Traditional approaches to the task of ACE
event detection primarily regard multiple
events in one sentence as independent ones
and recognize them separately by using
sentence-level information. However, events
in one sentence are usually interdependent
and sentence-level information is often insuf-
ﬁcient to resolve ambiguities for some types
of events.
This paper proposes a novel
framework dubbed as Hierarchical and Bias
Tagging Networks with Gated Multi-level
Attention Mechanisms (HBTNGMA) to solve
the two problems simultaneously. Firstly, we
propose a hierarchical and bias tagging net-
works to detect multiple events in one sen-
tence collectively. Then, we devise a gated
multi-level attention to automatically extract
and dynamically fuse the sentence-level and
document-level information. The experimen-
tal results on the widely used ACE 2005
dataset show that our approach signiﬁcantly
outperforms other state-of-the-art methods.

1 Introduction

Event detection (ED) is a crucial subtask of event
extraction, which aims to identify event triggers
and classify them into speciﬁc types from texts.
According to the task deﬁned in Automatic Con-
text Extraction1 (ACE), given the following sen-
tence S1, a robust ED system should be able to rec-
ognize two events: a Die event triggered by died
and an Attack event triggered by ﬁred.

S1: In Baghdad, a cameraman died when an

American tank ﬁred on the Palestine Hotel.

To this end, most methods (Ahn, 2006; Hong
et al., 2011; Chen et al., 2015; Nguyen and Grish-
man, 2016; Liu et al., 2017) model ED as a multi-
classiﬁcation task and predict every word in the

1http://projects.ldc.upenn.edu/ace/

Figure 1: Top 5 event types that co-occur with Attack
event in the same sentence in ACE 2005.

sentence separately to determine whether it trig-
gers a speciﬁc type of event by using sentence-
level information. However, they face two prob-
lems:
(1) Neglecting event interdependency by
separately predicting each event; (2) Sentence-
level information is usually insufﬁcient to resolve
ambiguities for some types of events. In the fol-
lowing, we will use examples to illustrate these
two problems speciﬁcally.
S2: The project

leader was ﬁred for the

bankruptcy of the subsidiary company.

Event interdependency: In S1, ﬁred triggers
an Attack event, while it triggers an End-Position
event in S2. Because of the ambiguity, a tradi-
tional approach may mislabel ﬁred in S1 as a trig-
ger of End-Position event. However, if we know
died triggers a Die event in S1, which is easier to
disambiguate, we tend to predict that ﬁred triggers
an Attack event. The reason is that the events men-
tioned in the same sentence tend to be semanti-
cally coherent and a Die event usually co-occurs
with an Attack event. The similar phenomenon
can be found in S2. We conduct a statistical anal-
ysis on ACE 2005 dataset, and ﬁnd that nearly
30% sentences contain multiple events which is
a proportion we can not ignore. To give an in-
tuitive illustration, the top 5 event types that co-
occur with Attack event in the same sentence are
shown in Figure 1. We call such clues as event

interdependency. Some works (Li et al., 2013;
Yang and Mitchell, 2016; Liu et al., 2016b) rely
on a set of elaborately designed features and com-
plicated natural language processing (NLP) tools
to capture event interdependency. However, these
methods lack generalization, take a large amount
of human effort and are prone to error propaga-
tion problem. Though Nguyen et al. (2016) use a
Recurrent Neural Networks (RNN) based classiﬁ-
cation model to capture the event interdependency
between current event candidate and the former
(left) predicted events, they miss the event interde-
pendency between current event candidate and the
later (right) predicted events, and the later events
can not change the type of current event. The rea-
son is that they classify the words of the sentence
from left to right one by one and only use the for-
mer events to predict the later event types. We
claim that both of the former and later predicted
events are important to predict the event type of
current trigger candidate. For example in S1, the
former predicted Die event can help us to predict
that ﬁred triggers an Attack event, while in S2 the
later predicted bankruptcy event can help us to
predict that ﬁred triggers an End-Position event.
Thus, how to use a neural-based model to capture
all event interdependencies (the interdependencies
between the current event candidate and its for-
mer/later predicted events) in the whole sentence
is a challenging problem.

Sentence-level and document-level informa-
tion: Besides event interdependency, knowing that
American tank is a weapon can also give us addi-
tional evidence to predict that ﬁred triggers an At-
tack event in S1. Similarly in S2, knowing that
project leader is a job title can also help us to
predict that ﬁred triggers an End-Position event.
We call such clues as sentence-level information.
However, sometimes it is difﬁcult even for peo-
ple to classify event types from an isolated sen-
tence. We must resort to document-level informa-
tion. For example, considering the following sen-
tence with an ambiguous word left:

S3: He left the company.
It is hard to tell left triggers a Transport event
which means that he left the place, or an End-
Position event which means that he resigned from
the company. However, if we read the whole doc-
ument, a clue like “He planned to go shopping
before he went home, because he got off work
early today.” would give us more conﬁdence to

believe that left triggers a Transport event, while
a clue like “They held a party for his retire-
ment.” would indicate the aforementioned event
is an End-Position event. We call such clues as
document-level information. Moreover, the conﬁ-
dence of sentence-level and document-level infor-
mation should be taken into consideration when
using them together to construct a broader range
of contextual information. For example in S3,
document-level information will give us more ev-
idence, while in S1 sentence-level information
is enough to disambiguate the types of events.
There have been some feature-based studies (Ji
and Grishman, 2008; Liao and Grishman, 2010;
Huang and Riloff, 2012) that construct rules to
capture document-level information for improv-
ing sentence-level ED. However, they suffer from
two problems:
(1) The features they used of-
ten need to be manually designed and may in-
volve error propagation from existing NLP tools;
(2) Sentence-level and document-level informa-
tion are integrated by a large number of ﬁxed rules,
which is complicated to construct and it will be far
from complete. Thus, how to use a neural-based
model to automatically extract sentence-level and
document-level information and dynamically inte-
grate them is another challenging problem.

In this paper, we propose a Hierarchical and
Bias Tagging Networks with Gated Multi-level
Attention Mechanisms (HBTNGMA) to address
the two problems stated above simultaneously. To
capture event interdependency and collectively de-
tect multiple events in one sentence, we propose a
hierarchical and bias tagging networks for event
In which, we exploit a hierarchical
detection.
RNN-based tagging layer to capture all event in-
terdependencies in the whole sentence and de-
vise a bias objective function to reinforce the in-
ﬂuence of trigger tags on the model2. To use
a broader range of contextual information of the
event candidate, we propose a gated multi-level at-
tention, which can automatically extract sentence-
level and document-level information and inte-
grate them dynamically. In summary, the contri-
butions of this paper are as follows:

•

We propose a novel framework for event
detection, which can automatically extract

2Compared with the task like Named Entities Recogni-
tion, the number of “O” tags is much more than the number
of trigger tags in ED task, i.e. if we use the non-bias objec-
tive function and tag all words in one sentence as “O”, we will
gain a low loss. Thus we devise a bias objective function.

•

and dynamically integrate sentence-level and
document-level information and collectively
detect multiple events in one sentence.

To capture event interdependency, we ex-
ploit a hierarchical and bias tagging net-
works to detect multiple events in one sen-
tence collectively. To automatically extract
and dynamically integrate contextual infor-
mation, we devise a gated multi-level atten-
tion Mechanisms. To our knowledge, this
is the ﬁrst work to jointly use event inter-
dependency, sentence-level information and
document-level information via a neural tag-
ging schema for event detection task.

•

We conduct extensive experiments on a
widely used ACE 2005 dataset, and the ex-
perimental results show that our approach
signiﬁcantly outperforms other state-of-the-
art methods 3.

2 Task Description

Event detection (ED) is a crucial subtask of event
extraction (EE). In this paper, we focus on ED
task deﬁned in ACE evaluation, where an event
is deﬁned as a speciﬁc occurrence involving one
or more participants. Firstly, we introduce some
ACE terminology to facilitate the understanding of
this task: Event trigger: the main word or phrase
that most clearly expresses the occurrence of an
event. Event arguments: the mentions that are in-
volved in an event (viz., participants). Event men-
tion: a phrase or sentence within which an event
is described, including a trigger and arguments.

Given an English text document, an ED system
should identify event triggers and categorize their
event types for each sentence. For instance, in the
sentence “He died in the hospital”, an ED system
is expected to detect a Die event along with the
trigger word “died”. The ACE 2005 evaluation
deﬁnes 8 event types and 33 subtypes, such as At-
tack or Die. Following previous works (Li et al.,
2013; Chen et al., 2015; Liu et al., 2017; Nguyen
and Grishman, 2016), we categorize triggers into
these 33 subtypes.

3 Methodology

In this paper, we formulate event detection as a
sequence labelling task. As shown in Figure 2,

3Our source code, including all hyper-parameter settings
is openly available at

and pre-trained word embeddings,
https://github.com/yubochen/NBTNGMA4ED

we label all words in one sentence collectively via
a Hierarchical and Bias Tagging Networks with
Gated Multi-level Attention Mechanisms (HBT-
NGMA). We assign a tag for each word to indi-
cate whether it triggers a speciﬁc type of event.
We adopt the “BIO” tags schema, where tag “O”
represent the “other” tag which means that the cor-
responding word does not trigger any event, tags
“B-EventType” and “I-EventType” represent the
“Begin-EventType” and “Inside-EventType” tag
respectively. “EventType” means that the word
triggers a speciﬁc type of event. “B” and “I” rep-
resent the position of the word in a trigger to solve
the problem that a trigger word contains multi-
ple words such as “take over”, “go off” and so
on. Thus, the total number of tags is Nt = 2
NeventT ype|
|
the predeﬁned event types and
in this paper as stated above.

NeventT ype|
|

⇤
is the size of
= 33

NeventT ype|

+ 1, where

|

Figure 2 describes the architecture of HBT-
NGMA, which primarily involves the following
four components:
(i) embedding layer, which
transforms each word into a continuous vector; (ii)
BiLSTM layer, which uses a Bidirectional Long
Short Term Memory (BiLSTM) to encode the se-
mantics of each word considering the forward and
backward information; (iii) gated multi-level at-
tention, in which we propose a sentence-level and
document-level attention to automatically extract
sentence-level and document-level information re-
spectively, and we devise a fusion gate to dy-
namically integrate them as context information;
and (iv) Hierarchical tagging layer, in which we
propose two Tagging LSTM (TLSTM1 and TL-
STM2) and a tagging attention to automatically
capture the event interdependency and tag all the
words of the sequence collectively.

3.1 Embedding Layer

This paper uses the learned word embeddings as
the source of basic features. Speciﬁcally, we use
the Skip-gram model (Mikolov et al., 2013) to
learn word embeddings on the NYT corpus.

Given a document d =

s1, s2, ..., si, ..., sNs}
,
{
where Ns is the number of sentences in the doc-
ument. The i-th sentence si can be represented
as token sequence si =
w1, w2, ..., wt, ..., wNw }
,
{
where Nw is length of the sentence and wt is the
t-th token of the sentence. Assume that the word
embedding for token wt is et and we use it as the
input of the following layer.

Figure 2: The architecture of our proposed hierarchical and bias tagging networks with gated multi-level attention.

3.2 BiLSTM Layer

semantic information sht is calculated as follows:

In sequence labelling problems, the BiLSTM has
been proven effective to capture the semantic in-
formation of each word (Lample et al., 2016). In
this paper, we use the LSTM unit as described in
(Zaremba and Sutskever, 2014). For each word
wt, the forward LSTM encodes wt by consider-
ing the contextual information from word w1 to
wt, which is marked as  !h t. Similarly, the back-
ward LSTM encodes wt based on the contextual
information from wNw to wt, which is marked as
  h t. Finally, we concatenate  !h t and   h t to rep-
resent the information of the word wt, denoted as
ht = [ !h t,   h t], and we concatenate  !h Nw and   h 1
to represent the encoding information of the whole
sentence si, denoted as hsi = [ !h Nw ,   h 1].

3.3 Gated Multi-level Attention

Gated multi-level attention primarily involves the
following three components: (i) sentence-level at-
tention layer, which automatically captures im-
portant sentence-level information by consider-
ing the current word; (ii) document-level atten-
tion layer, which automatically captures impor-
tant document-level information by considering
the current sentence; and (iii) fusion gate layer,
which use a fusion gate to dynamically integrate
sentence-level and document-level information.

Sentence-level Attention Layer

Sentence-level attention layer aims to capture the
important clues in sentence level. For each can-
didate word wt in the sentence, its sentence-level

sht =

Nw

k=1

↵k

s hk

where ↵k
tion hk. In this paper, we deﬁne ↵k

s is the weight of each word representa-
s as following:

X

↵k

s =

exp(zk
s )
Nw
j=1 exp(zj
s)

P

where zk
s is the relatedness between the t-th word
representation ht and the k-th word representation
hk, modeled by bilinear attention as:

s = tanh(htWsahT
zk

k + bsa)

(3)

where Wsa is the weight matrix and bsa is the
bias term. Following above sentence-level atten-
tion mechanism, we can get the sentence-level in-
formation for each word wt by considering the se-
mantic information of the word wt.

Document-level Attention Layer

Similar to sentence-level attention, document-
level attention captures the vital clues in the doc-
ument level. The document-level semantic infor-
mation dhi for i-th sentence calculated as follows:

(1)

(2)

(4)

dhi =

X

↵k

d =

Ns

↵k

dhsk

k=1
exp(zk
d )
Ns
j=1 exp(zj
d)

d = tanh(hsi WdahT
zk
P

sk + bda)

where ↵k
sentation hsk , zk

d is the weight of each sentence repre-
d is the relatedness between i-th

sentence representation hsi and the k-th sentence
representation hsk , Wda is the weight matrix and
bda is the bias term. Compared with sentence-level
information, all words in the i-th sentence have the
same document-level information dhi.

Fusion Gate Layer

We devise a fusion gate to dynamically integrate
information sht and document-
sentence-level
level information dhi for the t-th word wt in the
i-th sentence si, and calculate the contextual in-
formation representation crt as follows:

crt = (Gt

sht) + ((1

Gt)

dhi)

(5)

 

 

 

where Gt is a fusion gate aims to model the con-
ﬁdence of clues provided by sentence-level infor-
mation sht and document-level information dhi,
which is calculated as follows:

G =  (Wg[sht, dhi] + bg)

(6)

where Wg is the weight matrix and bg is the bias
term,   is a sigmoid function and
denotes
element-wise multiplication. Finally, the contex-
tual information crt of word wt and its word em-
bedding et are concatenated into a single vector
xrt = [et, crt] as the feature representation of wt.

 

3.4 Hierarchical Tagging Layer

In hierarchical tagging layer, we propose two Tag-
ging LSTMs (TLSTM1 and TLSTM2) and a tag-
ging attention to automatically capture the event
interdependency and tag the sequence collectively.

The First Tagging Layer: TLSTM1
When detecting the tag of word wt in TLSTM1,
the inputs are: the feature representation xrt ob-
tained from embedding layer and gated multi-level
attention layer, former predicted tag vector T 1
1,
t
 
and former hidden vector h1
1 in TLSTM1. The
t
 
detail operations are deﬁned as follows:

t =  (W 1
i1
t =  (W 1
f 1
o1
t =  (W 1
t = '(W 1
u1

t

t

t

 

ix xrt + W 1
fx xrt + W 1
ox xrt + W 1
ux xrt + W 1

ih h1
fh h1
oh h1
uh h1
t
 
t = i1
c1
t + f 1
u1
t  
t  
'(c1
h1
t = o1
t )
t  
t + b1
T h1
t = W 1
T 1
T

 

 

t

t

iT T 1
fT T 1
oT T 1
uT T 1

1 + W 1
 
1 + W 1
1 + W 1
 
1 + W 1
c1
t
 

1

t

t

 

 

1)

1)

1)

1)

(7)

Where it is an input gate, ut is an input modulation
gate, ft is a forget gate, ot is an output gate, ct is a
memory cell and T 1
t is a predicted tagging vector.

The Second Tagging Layer: TLSTM2

Though the TLSTM1 can capture the interdepen-
dency between current event candidate and the for-
mer predicted event tags, it can not capture the
interdependency between the current event candi-
date and the later predicted event tags. Thus, we
devise a second tagging layer (TLSTM2) upon the
LSTM1 to capture the interdependency between
the current event candidate and both of former and
later predicted event tags from TLSTM1. When
detecting the tag of word wt in TLSTM2, the in-
the feature representation xrt, former
puts are:
predicted tag vector T 2
1 in TLSTM2, the pre-
t
 
liminary predicted information T a
t calculated from
TLSTM1, and former hidden vector h2
1 in TL-
t
 
STM2. The detail operations are deﬁned as fol-
lows:

i2
t =  (W 2
t =  (W 2
f 2
o2
t =  (W 2
t = '(W 2
u2

t

t

t

t

t

 

 

iT T 2
1 + W 2
fT T 2
1 + W 2
 
oT T 2
1 + W 2
1 + W 2
uT T 2
c2
t
 

ix xrt + W 2
ih h2
fx xrt + W 2
fh h2
ox xrt + W 2
oh h2
ux xrt + W 2
uh h2
t
 
t = i2
c2
t + f 2
u2
t  
t  
'(c2
h2
t = o2
t )
t  
t + b2
T h2
t = W 2
T 2
T

 

1

t

t

 

 

1 + W 2
1 + W 2
 
1 + W 2
1 + W 2

ia T a
t )
fa T a
t )
oa T a
t )
ua T a
t )

The unit structure of TLSTM2 is similar to the
unit of TLSTM1. The parts need to pay attention
are follows: (1) the initial hidden input h2
0 of the
TLSTM2 is the last hidden vector h1
Nw of the TL-
STM1. (2) the preliminary predicted information
T a
t is calculated from TLSTM1 by using a tagging
attention as follows.

Tagging Attention

Tagging attention aims to automatically encode
the preliminary predicted information T a
for the
t
word wt and the details are as follows:

(8)

(9)

T a
t =

↵k

T =

Nw

T T 1
↵k
k

k=1
X
exp(zk
T )
Nw
j=1 exp(zj
T )
t Wta(T 1

T = tanh(T 1
zk
P

k )T + bta)

where ↵k
dicted tag T 1
k , zk
preliminary predicted tag T 1
inary predicted tag T 1
and bta is the bias term.

T is the weight of each preliminary pre-
T is the relatedness between t-th
t and the k-th prelim-
k , Wta is the weight matrix

The ﬁnal normalized tag probability for word
t from

wt is based on the predicted tag vector T 2

TLSTM2 and computed as follows:

Hyper-parameter Setting

Ot = WyT 2

t + by

p(Oi
t|

sj,✓ ) =

exp(Oi
t)
Nt
k=1 exp(Ok
t )

(10)

P

where p(Oi
sj,✓ ) is the probability that assigning
t|
the i-th tag to word wt in sentence sj when param-
eters is ✓, and Nt is the total number of tags.

3.5 Training with Bias Objective Function

In one sentence, the number of “O” tags is much
more than the number of trigger tags. Thus, we
devise a bias objective function J(✓) to reinforce
the inﬂuence of trigger tags on the model, which
is deﬁned as follows:

J(✓) = max

Nts

j=1
+ ↵ log p(Oyt
t |

X

X
sj,✓ )

Nw

t=1

(log p(Oyt
t |
I(O)))

(1

·

 

sj,✓ )

I(O)

·

(11)

where Nts is the number of training sentences, Nw
is the length of sentence sj, p(Oyt
sj,✓ ) is the nor-
t |
malized probabilities of tags deﬁned in Formula
10 and yt is the golden tag of word wt in sentence
sj, ↵ is the bias weight and the larger ↵ will bring
the greater inﬂuence of trigger tags on the model.
Besides, I(O) is a switching function to distin-
guish the loss of tag “O” and trigger tags, which
is deﬁned as follows:

I(O) =

⇢

1,
0,

if
if

tag = “O”
= “O”
tag

(12)

To compute the network parameter ✓, we maxi-
mize the log likelihood J (✓) through stochastic
gradient descent over shufﬂed mini-batches with
the Adadelta (Zeiler, 2012) update rule.

4 Experiments

4.1 Experimental Setting

Dataset and Evaluation Metrics

We conduct experiments on the widely used ACE
2005 dataset. For comparison, as the same as pre-
vious works (Liao and Grishman, 2010; Li et al.,
2013; Chen et al., 2015; Nguyen et al., 2016; Liu
et al., 2017), we used the same test set with 40
documents and the same development set with 30
documents and the rest 529 documents are used
for training. Finally, we use Precision (P ), Recall
(R) and F measure (F1) as the evaluation metrics
as the same as previous work.

Hyper-parameters are tuned on the development
dataset by grid search. We train the word embed-
ding using Skip-gram algorithm 4 on the NYT cor-
pus 5. We set the dimension of word embeddings
as 100, the dimension of tag vector as 20, all the
size of LSTM in BiLSTM layer, TLSTM1 and TL-
STM2 layer as 100, the bias parameter ↵ in For-
mula 11 as 5, the batch size as 20, the learning rate
as 0.001, the dropout rate as 0.5.

4.2 Our Method vs. State-of-the-art Methods

We select the following state-of-the-art methods
for comparison, which can be classiﬁed as two
types: separate and collective methods:

1) Li’s MaxEnt:

Separate methods:

the
method that detects events in one sentence sepa-
rately by using human-designed features (Li et al.,
2013). 2) Liao’s CrossEvent : the method that
uses cross event information (Liao and Grish-
man, 2010). 3) Hong’s CrossEntity: the method
that uses cross entity information (Hong et al.,
2011). 4) Chen’s DMCNN: the dynamic multi-
pooling convolutional neural networks method
(Chen et al., 2015).
5) Chen’s DMCNN+:
the DMCNN method argumented with automati-
cally labeled data (Chen et al., 2017). 6) Liu’s
FrameNet : the method that leverages FrameNet
as extended training data to improve ED (Liu et al.,
2016a). 7) Liu’s ANN-Aug: the method that use
the annotated argument information via a super-
vised attention to improve ED (Liu et al., 2017).
Collective methods: 1) Li’s Structure:
the
method that collectively detects events by us-
ing human-designed features (Li et al., 2013).
2) Yang’s JointEE:
the method that detects
events and entities in one sentence jointly based
on human-designed features (Yang and Mitchell,
2016). 3) Nguyen’s JRNN: the method that ex-
ploits a RNN model to collectively detects events
by only using sentence-level information (Nguyen
et al., 2016). 4) Liu’s PSL : the method that uses
a probabilistic soft logic to detect events by using
human-designed features (Liu et al., 2016b).

Experimental results are shown in Table 1.
From the table, we have the following observa-
(1) Among all the methods, our HBT-
tions:
NGMA achieves the best performance.
It can
improve the best collective method’s F1 by

4https://code.google.com/p/word2vec/
5https://catalog.ldc.upenn.edu/LDC2008T19

Methods
Li’s MaxEnt (2013)
Liao’s CrossEvent (2010)
Hong’s CrossEntity (2011)
Chen’s DMCNN (2015)

Chen’s DMCNN+
Liu’s FrameNet
†
Liu’s ANN-Aug
†

(2017)
†
(2016a)
(2017)

Li’s Structure (2013)
Yang’s JointEE (2016)
Nguyen’s JRNN (2016)
Liu’s PSL (2016b)
Ours HBTNGMA

P
74.5
68.7
72.9
75.6
75.7
77.6
76.8
73.7
75.1
66.0
75.3
77.9

R
59.1
68.9
64.3
63.6
66.0
65.2
67.5
62.3
63.3
73.0
64.4
69.1

F1
65.9
68.8
68.3
69.1
70.5
70.7
71.9
67.5
68.7
69.3
69.4
73.3

Table 1: Overall performance on blind test data. The
upper table illustrates the performance of separate ED
systems and the lower illustrates collective ED systems.

means the method that uses external resources.

†

3.9% (Liu’s PSL) and improve the best sepa-
rate method’s F1 by 1.4% (Liu’s ANN-Aug) al-
though Liu’s ANN-Aug uses FrameNet as exter-
nal resources. We also perform a t-test (p 6
0.05), which indicates that our method signiﬁ-
cantly outperforms all of the compared methods.
(2) Comparing our HBTNGMA to separate meth-
ods, it achieves a better performance.
It proves
that collectively predicting multiple events in one
sentence is effective.
(3) Our HBTNGMA out-
performs feature-based collective methods (Li’s
Structure, Yang’s JointEE and Liu’s PSL),
it
proves that our automatically learned features
can efﬁciently capture semantic information from
plain texts. (4) Compared with Nguyen’s JRNN,
HBTNGMA gains a 4.0% improvement on F1
value. The reason is that Nguyen’s JRNN only
uses sentence-level information while our model
exploits multi-level information, and our model
can capture the interdependencies between the
current event candidate and its former/later pre-
dicted events simultaneously.

4.3 Effect of The Hierarchical and Bias

Tagging Networks

In this subsection, we prove the effectiveness of
hierarchical and bias tagging networks for collec-
tive ED. We select following methods as base-
lines: 1) LSTM+Softmax: a simpliﬁed version
of our HBTNGMA, which directly use a soft-
max layer to separately detect events after we
get the feature representation xrt of each word
wt. 2) LSTM+CRF: the method is similar to
our HBTNGMA, which uses a CRF layer to tag
words instead of our Hierarchical TLSTM (HTL-
STM) tagging layer.
3) LSTM+TLSTM: the
method is similar to our HBTNGMA, which only

use a TLSTM1 and takes all tags have same in-
ﬂuence in training loss (i.e. ↵ in is set as 1)
. 4) LSTM+HTLSTM: the method is similar
to our HBTNGMA, which use a HTLSTM (TL-
STM1+TLSTM2) and do not use bias objective
function. And LSTM+HTLSTM+Bias is our pro-
posed HBTNGMA. Moreover, we divide the test-
ing data into two parts according the event num-
ber in a sentence (single event and multiple events)
and perform evaluations separately.

Method
LSTM+Softmax
LSTM+CRF
LSTM+TLSTM
LSTM+HTLSTM
LSTM+HTLSTM+Bias

1/1
74.7
75.1
76.8
77.9
78.4

1/N
44.6
49.5
51.2
57.3
59.5

all
66.8
68.5
70.2
72.4
73.3

Table 2: Performance of different ED systems. 1/1
means one sentence that only has one event and 1/N
means that one sentence has multiple events.

Surprisingly,

following observations:

Table 2 shows the results. And we have
the
1) Compared
with LSTM+Softmax, LSTM-based collective
(LSTM+CRF, LSTM+TLSTM,
ED methods
LSTM+HTLSTM, LSTM+HTLSTM+Bias) achie-
ves a better performance.
the
LSTM+HTLSTM+Bias yields a 14.9% improve-
ment on the sentence contains multiple events
over the LSTM+Softmax. It proves neural tagging
schema is effective for ED task especially for
the sentences contain multiple events.
2) The
LSTM+TLSTM achieve better performances than
LSTM+CRF. And the LSTM+HTLSTM achieve
better performances than LSTM+TLSTM. The
results prove the effectiveness of the TLSTM
3) Compared with
layer and HTLSTM layer.
LSTM+HTLSTM+Bias
LSTM+HTLSTM,
the
gains a 0.9% improvement on all sentence.
It
demonstrates the effectiveness of our proposed
bias objective function.

4.4 Effect of The Gated Multi-level Attention

This subsection studies the effectiveness of our
gated multi-level attention. We adopt same archi-
tecture of our HBTNGMA as shown in Figure 2
with different level clues as baselines: 1) Word
Only is the method only uses word embedding et
to identify events. 2) Word+SA uses sentence-
level attention to capture important sentence-level
information as additional clues. 3) Word+DA
uses document-level attention to capture important
document-level information as additional clues. 4)
Word+Average MA uses both of sentence-level

and document-level attention to capture multi-
level information and integrate them with a aver-
age gate (all the dimension of the fusion gate are
set as 0.5 ), which is a special case of our pro-
posed HBTNGMA. And Word+Gated MA is our
proposed HBTNGMA model.

Method
Word Only
Word+SA
Word+DA

P
70.1
75.6
73.1
Word+Average MA 76.5
Word+Gated MA
77.9

R
63.4
68.2
65.8
68.7
69.1

F1
66.6
71.7
69.3
72.4
73.3

Table 3: Performance of gated multi-level attention.

Results are shown in Table 3. From the re-
sults, we have the following observations: 1)
Compared with word only, Word+SA achieves a
better performance. We can make the same ob-
servation when comparing Word+DA with word
only.
It proves that both sentence-level and
document-level information are helpful for ED
2) Compared with Word+DA, Word+SA
task.
achieves a better performance.
It proves that in
most of cases sentence-level information provides
more clues than document-level information. 3)
Word+Gated MA gains a 0.9% improvement than
Word+Average MA. It demonstrates that the effec-
tiveness of our fusion gate to dynamically inte-
grate clues from multiple levels.

4.5 Case Study

Interesting Cases: Our neural tagging schema not
only can model the interdependency between mul-
tiple events in one sentence as proved in Subsec-
tion 4.3, but also the “BIO” tagging schema can
solve the multiple words trigger inherently. We
conduct a statistical analysis on the experimental
results, and ﬁnd that nearly 50% cases with multi-
ple word trigger was solved by our model. Exam-
ple is shown in Figure 3.

Figure 3: The example of case solved by our model.

Attention Visualization: As limited of space,
we take one sentence with high sentence-level
gated weight (example 1) and one sentence with
high document-level gated weight (example 2) as
examples for attention visualization. As shown in
Figure 4, in example 1, sentence-level informa-
tion plays more important role in disambiguating
ﬁred, and the words (tank, died and Baghdad) give

Figure 4: Attention visualization. The heat map indi-
cate the contextual attention. Blue for sentence-level
and orange for document-level. The pie chart indicate
the fusion gate weight.

us ample evidence to predict that ﬁred triggers an
Attack event. While document-level information
plays a more important role in example 2. The
tiresome.” gives
surrounding sentence “this is ...
us more conﬁdence to predict that leave triggers
an End-Position event.

5 Related Works

Event detection is an increasingly hot and chal-
lenging research topic in NLP. Generally, exist-
ing approaches could roughly be divided into two
groups: separate and collective methods.

Separate methods: These methods regard mul-
tiple events in one sentence as independent ones
These meth-
and recognize them separately.
ods include feature-based methods which exploit
a diverse set of strategies to convert classiﬁca-
tion clues into feature vectors (Ahn, 2006; Ji
and Grishman, 2008; Liao and Grishman, 2010;
Hong et al., 2011; Huang and Riloff, 2012), and
neural-based methods which use neural networks
to automatically capture clues from plain texts
(Chen et al., 2015; Nguyen and Grishman, 2015;
Feng et al., 2016; Nguyen and Grishman, 2016;
Chen et al., 2017; Duan et al., 2017; Liu et al.,
2017). Though effective these methods, they ne-
glect event interdependency by separately predict-
ing each event.

Collective methods: These methods try to
model the event interdependency and detect mul-
tiple events in one sentence collectively. How-
ever, nearly all of these methods are feature-based
methods (McClosky et al., 2011; Li et al., 2013;
Yang and Mitchell, 2016; Liu et al., 2016b), which
rely on elaborately designed features and suffer er-
ror propagation from existing NLP tools. Nguyen
et al. (2016) exploits a neural-based method to de-
tect multiple events collectively. However, they
only use the sentence-level information and ne-

glect document-level clues, and can only capture
the interdependencies between the current event
candidate and its former predicted events. More-
over, there method can not handle the multiple
words trigger problem.

Xiaocheng Feng, Lifu Huang, Duyu Tang, Heng Ji,
A language-
Bing Qin, and Ting Liu. 2016.
independent neural network for event detection. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), vol-
ume 2, pages 66–71.

6 Conclusion

This paper proposes a novel framework for event
detection, which can automatically extract and dy-
namically integrate sentence-level and document-
level information and collectively detect multi-
ple events in one sentence. A hierarchical and
bias tagging networks is proposed to detect mul-
tiple events in one sentence collectively. A gated
multi-level attention is devised to automatically
extract and dynamically integrate contextual infor-
mation. The experimental results on the widely
used dataset prove the effectiveness of the pro-
posed method.

Acknowledgments

The research work is supported by the Natural
Science Foundation of China (No.61533018 and
No.61702512), and the independent
research
project of National Laboratory of Pattern Recog-
nition. This work is also supported in part by
Huawei Technologies Co., Ltd.

References

David Ahn. 2006. The stages of event extraction. In
Proceedings of 44th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pages 1–
8.

Yubo Chen, Shulin Liu, Xiang Zhang, Kang Liu, and
Jun Zhao. 2017. Automatically labeled data genera-
tion for large scale event extraction. In Proceedings
of the 55th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 409–419,
Vancouver, Canada. Association for Computational
Linguistics.

Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng,
and Jun Zhao. 2015. Event extraction via dynamic
multi-pooling convolutional neural networks.
In
Proceedings of the 53rd Annual Meeting of the As-
sociation for Computational Linguistic (ACL), pages
167–176.

Shaoyang Duan, Ruifang He, and Wenli Zhao. 2017.
Exploiting document level information to improve
event detection via recurrent neural networks.
In
Proceedings of IJNLP, volume 1, pages 352–361.

Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao,
Guodong Zhou, and Qiaoming Zhu. 2011. Using
cross-entity inference to improve event extraction.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 1127–1136.

Ruihong Huang and Ellen Riloff. 2012. Modeling tex-
tual cohesion for event extraction. In Proceedings of
AAAI.

Heng Ji and Ralph Grishman. 2008. Reﬁning event ex-
traction through cross-document inference. In Pro-
ceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
254–262.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of NAACL, pages 260–270.

Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
In Proceedings of the 51st Annual Meet-
tures.
ing of the Association for Computational Linguistics
(ACL), pages 73–82.

Shasha Liao and Ralph Grishman. 2010. Using doc-
ument level cross-event inference to improve event
extraction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 789–797.

Shulin Liu, Yubo Chen, Shizhu He, Kang Liu, and Jun
Zhao. 2016a. Leveraging framenet to improve auto-
matic event detection. In Proceedings of ACL, pages
2134–2143. Association for Computational Linguis-
tics.

Shulin Liu, Yubo Chen, Kang Liu, and Jun Zhao. 2017.
Exploiting argument information to improve event
detection via supervised attention mechanisms.
In
Proceedings of the 55th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
1789–1798.

Shulin Liu, Kang Liu, Shizhu He, and Jun Zhao. 2016b.
A probabilistic soft logic based approach to exploit-
ing latent and global information in event classiﬁca-
tion. In Proceedings of AAAI, pages 2993–2999.

David McClosky, Mihai Surdeanu, and Christopher
Manning. 2011. Event extraction as dependency
In Proceedings of the 49th Annual Meet-
parsing.
ing of the Association for Computational Linguistics
(ACL), pages 1626–1635.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
Efﬁcient estimation of word
arXiv preprint

frey Dean. 2013.
representations in vector space.
arXiv:1301.3781.

Huu Thien Nguyen and Ralph Grishman. 2015. Event
detection and domain adaptation with convolutional
neural networks. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistic (ACL), pages 365–371.

Thien Huu Nguyen, Kyunghyun Cho, and Ralph Gr-
ishman. 2016. Joint event extraction via recurrent
neural networks. In Proceedings of the 2016 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL), pages
300–309.

Thien Huu Nguyen and Ralph Grishman. 2016. Mod-
eling skip-grams for event detection with convolu-
tional neural networks. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 886–891.

Bishan Yang and Tom M. Mitchell. 2016. Joint extrac-
tion of events and entities within a document con-
text. In Proceedings of NAACL, pages 289–299, San
Diego, California. Association for Computational
Linguistics.

Wojciech Zaremba and Ilya Sutskever. 2014. Learning

to execute. arXiv preprint arXiv:1410.4615.

Matthew D Zeiler. 2012. Adadelta: An adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701.

