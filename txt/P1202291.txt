8
1
0
2
 
y
a
M
 
6
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
7
4
2
8
0
.
1
1
7
1
:
v
i
X
r
a

Decomposition Strategies for Constructive Preference Elicitation
(and Supplementary Material)

Paolo Dragone∗
University of Trento, Italy
TIM-SKIL, Trento, Italy
paolo.dragone@unitn.it

Stefano Teso
KU Leuven, Belgium
stefano.teso@cs.kuleuven.be

Mohit Kumar †
KU Leuven, Belgium
mohit.kumar@cs.kuleuven.be

Andrea Passerini
University of Trento, Italy
andrea.passerini@unitn.it

Abstract

We tackle the problem of constructive preference elicitation,
that is the problem of learning user preferences over very
large decision problems, involving a combinatorial space of
possible outcomes. In this setting, the suggested conﬁgura-
tion is synthesized on-the-ﬂy by solving a constrained op-
timization problem, while the preferences are learned itera-
tively by interacting with the user. Previous work has shown
that Coactive Learning is a suitable method for learning user
preferences in constructive scenarios. In Coactive Learning
the user provides feedback to the algorithm in the form of an
improvement to a suggested conﬁguration. When the problem
involves many decision variables and constraints, this type of
interaction poses a signiﬁcant cognitive burden on the user.
We propose a decomposition technique for large preference-
based decision problems relying exclusively on inference and
feedback over partial conﬁgurations. This has the clear ad-
vantage of drastically reducing the user cognitive load. Addi-
tionally, part-wise inference can be (up to exponentially) less
computationally demanding than inference over full conﬁg-
urations. We discuss the theoretical implications of working
with parts and present promising empirical results on one syn-
thetic and two realistic constructive problems.

Introduction
In constructive preference elicitation (CPE) the recom-
mender aims at suggesting a custom or novel product to a
customer (Teso, Passerini, and Viappiani 2016). The prod-
uct is assembled on-the-ﬂy from components or synthe-
sized anew by solving a combinatorial optimization prob-
lem. The suggested products should of course satisfy the
customer’s preferences, which however are unobserved and
must be learned interactively (Pigozzi, Tsouki`as, and Vi-
appiani 2016). Learning proceeds iteratively: the learner
presents one or more candidate recommendations to the cus-
tomer, and employs the obtained feedback to estimate the

∗PD is a fellow of TIM-SKIL Trento and is supported by a
TIM scholarship. This work has received funding from the Euro-
pean Research Council (ERC) under the European Unions Horizon
2020 research and innovation programme (grant agreement No.
[694980] SYNTH: Synthesising Inductive Data Models).
Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

customer’s preferences. Applications include recommend-
ing custom PCs or cars, suggesting touristic travel plans,
designing room and building layouts, and producing recipe
modiﬁcations, among others.

A major weakness of existing CPE methods (Teso,
Passerini, and Viappiani 2016; Teso, Dragone, and Passerini
2017) is that they require the user to provide feedback on
complete conﬁgurations. In real-world constructive prob-
lems such as trip planning and layout design, conﬁgura-
tions can be large and complex. When asked to evaluate or
manipulate a complex product, the user may become over-
whelmed and confused, compromising the reliability of the
obtained feedback (Mayer and Moreno 2003). Human deci-
sion makers can revert to a potentially uninformative prior
when problem solving exceeds their available resources.
This effect was observed in users tasked with solving sim-
ple SAT instances (three variables and eight clauses) (Or-
tega and Stocker 2016). In comparison, even simple con-
structive problems can involve tens of categorical variables
and features, in addition to hard feasibility constraints. On
the computational side, working with complete conﬁgura-
tions poses scalability problems as well. The reason is that,
in order to select recommendations and queries, constructive
recommenders employ constraint optimization techniques.
Clearly, optimization of complete conﬁgurations in large
constructive problems can become computationally imprac-
tical as the problem size increases.

Here we propose to exploit factorized utility func-
tions (Braziunas and Boutilier 2009), which occur very natu-
rally in constructive problems, to work with partial conﬁgu-
rations. In particular, we show how to generalize Coactive
Learning (CL) (Shivaswamy and Joachims 2015) to part-
wise inference and learning. CL is a simple, theoretically
grounded algorithm for online learning and preference elic-
itation. It employs a very natural interaction protocol: at
each iteration the user is presented with a single, appropri-
ately chosen candidate conﬁguration and asked to improve
it (even slightly). In (Teso, Dragone, and Passerini 2017), it
was shown that CL can be lifted to constructive problems
by combining it with a constraint optimization solver to ef-
ﬁciently select the candidate recommendation. Notably, the
theoretical guarantees of CL remain intact in the construc-

tive case.

Our part-wise generalization of CL, dubbed PCL, solves
the two aforementioned problems in one go: (i) by present-
ing the user with partial conﬁgurations, it is possible to (sub-
stantially) lessen her cognitive load, improving the reliabil-
ity of the feedback and enabling learning in larger construc-
tive tasks; (ii) in combinatorial constructive problems, per-
forming inference on partial conﬁgurations can be exponen-
tially faster than on complete ones. Further, despite being
limited to working with partial conﬁgurations, PCL can be
shown to still provide local optimality guarantees in theory,
and to perform well in practice.

This paper is structured as follows. In the next section
we overview the relevant literature. We present PCL in the
Method section, followed by a theoretical analysis. The per-
formance of PCL are then illustrated empirically on one syn-
thetic and two realistic constructive problems. We close the
paper with some concluding remarks.

Related Work
Generalized additive independent (GAI) utilities have been
thoroughly explored in the decision making literature (Fish-
burn 1967). They deﬁne a clear factorization mechanism,
and offer a good trade off between expressiveness and ease
of elicitation (Chajewska, Koller, and Parr 2000; Gonzales
and Perny 2004; Braziunas and Boutilier 2009). Most of the
early work on GAI utility elicitation is based on graphical
models, e.g. UCP and GAI networks (Gonzales and Perny
2004; Boutilier, Bacchus, and Brafman 2001). These ap-
proaches aim at eliciting the full utility function and rely
on the comparison of full outcomes. Both of these are infea-
sible when the utility involves many attributes and features,
as in realistic constructive problems.

Like our method, more recent alternatives (Braziunas and
Boutilier 2005; 2007) handle both partial elicitation, i.e. the
ability of providing recommendations without full utility in-
formation, and local queries, i.e. elicitation of preference
information by comparing only (mostly) partial outcomes.
There exist both Bayesian (Braziunas and Boutilier 2005)
and regret-based (Braziunas and Boutilier 2007; Boutilier
et al. 2006) approaches, which have different shortcomings.
Bayesian methods do not scale to even small size construc-
tive problems (Teso, Passerini, and Viappiani 2016), such as
those occurring when reasoning over individual parts in con-
structive settings. On the other hand, regret-based methods
require the user feedback to be strictly self-consistent, an
unrealistic assumption when interacting with non-experts.
Our approach instead is speciﬁcally designed to scale to
larger constructive problems and, being derived from Coac-
tive Learning, natively handles inconsistent feedback. Cru-
cially, unlike PCL, these local elicitation methods also re-
quire to perform a number of queries over complete con-
ﬁgurations to calibrate the learned utility function. In larger
constructive domains this is both impractical (on the user
side) and computationally infeasible (on the learner side).

Our work is based on Coactive Learning (CL) (Shiv-
aswamy and Joachims 2015), a framework for learning util-
ity functions over structured domains, which has been suc-
cessfully applied to CPE (Teso, Dragone, and Passerini

2017; Dragone et al. 2016). When applied to constructive
problems, a crucial limitation of CL is that the learner and
the user interact by exchanging complete conﬁgurations.
Alas, inferring a full conﬁguration in a constructive prob-
lem can be computationally demanding, thus preventing the
elicitation procedure from being real-time. This can be par-
tially addressed by performing approximate inference, as
in (Raman, Shivaswamy, and Joachims 2012), at the cost of
weaker learning guarantees. A different approach has been
taken in (Goetschalckx, Fern, and Tadepalli 2014), where
the exchanged (complete) conﬁgurations are only required
to be locally optimal, for improved efﬁciency. Like PCL, this
method guarantees the local optimality of the recommended
conﬁguration. All of the previous approaches, however, re-
quire the user to improve a potentially large complete con-
ﬁguration. This is a cognitively demanding task which can
become prohibitive in large constructive problems, even for
domain experts, thus hindering feedback quality and effec-
tive elicitation. By dealing with parts only, PCL avoids this
issue entirely.

Method
Notation. We use rather standard notation: scalars x are
written in italics and column vectors x in bold. The inner
product of two vectors is indicated as (cid:104)a, b(cid:105) = (cid:80)
i aibi, the
Euclidean norm as (cid:107)a(cid:107) = (cid:112)(cid:104)a, a(cid:105) and the max-norm as
(cid:107)a(cid:107)∞ = maxi ai. We abbreviate {1, . . . , n} to [n], and in-
dicate the complement of I ⊆ [n] as −I := [n] \ I.

Setting. Let X be the set of candidate structured products.
Contrarily to what happens in standard preference elicita-
tion, in the constructive case X is deﬁned by a set of hard
constraints rather than explicitly enumerated1. Products are
represented by a function φ(·) that maps them to an m-
dimensional feature space. While the feature map can be
arbitrary, in practice we will stick to features that can be en-
coded as constraints in a mixed-integer linear programming
problem, for efﬁciency; see the Empirical Analysis section
for details. We only assume that the features are bounded,
i.e. ∀x ∈ X it holds that (cid:107)φ(x)(cid:107)∞ ≤ D for some ﬁxed D.

As is common in multi-attribute decision theory (Keeney
and Raiffa 1976), we assume the desirability of a product
x to be given by a utility function u∗ : X → R that is
linear in the features, i.e., u∗(x) := (cid:104)w∗, φ(x)(cid:105). Here the
weights w∗ ∈ Rm encode the true user preferences, and
may be positive, negative, or zero (which means that the cor-
responding feature is irrelevant for the user). Utilities of this
kind can naturally express rich constructive problems (Teso,
Passerini, and Viappiani 2016; Teso, Dragone, and Passerini
2017).

Parts. Here we formalize what parts and partial conﬁgu-
rations are, and how they can be manipulated. We assume to

1In this paper, “hard” constraints refer to the constraints de-
limiting the space of feasible conﬁgurations, as opposed to “soft”
constraints, which determine preferences over feasible conﬁgura-
tions (Meseguer, Rossi, and Schiex 2006).

be given a set of n basic parts p ∈ P. A part is any subset
P ⊆ P of the set of basic parts. Given a part P and an object
x, xP ∈ XP indicates the partial conﬁguration correspond-
ing to P . We require that the union of the basic parts recon-
structs the whole object, i.e. xP = x for all x ∈ X . The
proper semantics of the decomposition into basic parts is
task-speciﬁc. For instance, in a scheduling problem a month
may be decomposed into days, while in interior design a
house may be decomposed into rooms. Analogously, the
non-basic parts could then be weeks or ﬂoors, respectively.
In general, any combination of basic parts is allowed. We
capture the notion of combination of partial conﬁgurations
with the part combination operator ◦ : XP × XQ → XP ∪Q,
so that xP ◦ xQ = xP ∪Q. We denote the complement of part
P as P = P \ P , which satisﬁes x = xP ◦ xP for all x ∈ X .
Each basic part p ∈ P is associated to a feature subset
Ip ⊆ [m], which contains all those features that depend on
p (and only those). In general, the sets Ip may overlap, but
we do require each basic part p to be associated to some
features that do not depend on any other basic part q, i.e. that
(cid:54)= ∅ for all p ∈ P. The features associated
Ip \
to a part P ⊆ P are deﬁned as (cid:83)
p∈P Ip. Since the union of
the basic parts makes up the full object, we also have that
(cid:83)

q(cid:54)=p Iq

(cid:16)(cid:83)

(cid:17)

p∈P Ip = [m].

GAI utility decomposition.
In the previous section we in-
troduced a decomposition of conﬁgurations into parts. In or-
der to elicit the user preferences via part-wise interaction,
which is our ultimate goal, we need to decompose the utility
function as well. Given a part P and its feature subset IP ,
let its partial utility be:

u[IP ](x) := (cid:104)wIP , φIP (x)(cid:105) = (cid:80)

i∈IP

wiφi(x)

(1)

If the basic parts have no shared features, the utility function
is additive: it is easy to verify that u(x) = (cid:80)
p∈P u[Ip](x).
In this case, each part can be managed independently of the
others, and the overall conﬁguration maximizing the utility
can be obtained by separately maximizing each partial utility
and combining the resulting part-wise conﬁgurations.

However, in many applications of interest the feature sub-
sets do overlap. In a travel plan, for instance, one can be
interested in alternating cultural and leisure activities in con-
secutive days, in order to make the experience more diverse
and enjoyable. In this case, the above decomposition does
not apply anymore as the basic parts may depend on each
other through the shared features. Nonetheless, it can be
shown that our utility function is generalized additive inde-
pendent (GAI) over the feature subsets Ip of the basic parts.
Formally, a utility u(x) is GAI if and only if, given n fea-
ture subsets I1, . . . , In ⊆ [m], it can be decomposed into n
independent subutilities (Braziunas and Boutilier 2005):

u(x) = uI1(x) + · · · + uIn (x)

(2)

where each subutility uIk can only depend on the features
in Ik (but does not need to depend on all of them). This de-
composition enables applying ideas from the GAI literature

Algorithm 1 An example of ordering selection procedure
using a GAI network (Gonzales and Perny 2004).
1: procedure SELECTORDERING(P)
2:
3:
4:
5:

Build a GAI network G from Ip, p ∈ P
Produce sequence p1, . . . , pn by sorting the nodes
in G in ascending order of node degree
return p1, . . . , pn

to produce a well-deﬁned part-wise elicitation protocol. In-
tuitively, we will assign features to subutilities so that when-
ever a feature is shared by multiple parts, only the subutility
corresponding to one of them will depend on that feature.

We will now construct a suitable decomposition of u(x)
into n independent subutilities. Fix some order of the basic
parts p1, . . . , pn, and let:

Jk := Ik \ ((cid:83)n

j=k+1 Ij)

(3)

for all k ∈ [n]. We deﬁne the subutilities as uIk (x) :=
u[Jk](x) for all k ∈ [n]. By summing up the subutilities for
all parts, we obtain a utility where each feature is computed
exactly once, thus recovering the full utility u(x):
k=1 uIk (x) = (cid:80)n
wiφi(x)

u(x) = (cid:80)n
= (cid:80)

k=1 u[Ik \ (cid:83)n

j=k+1 Ij](x)

i∈IP

The GAI decomposition allows to elicit each subutility
uIk separately. By doing so, however, we end up ignoring
some of the dependencies between parts, namely the fea-
tures in Ik \ Jk. This is the price to pay in order to achieve
decomposition and partwise elicitation, and it may lead to
suboptimal solutions if too many dependencies are ignored.
It is therefore important to minimize the broken dependen-
cies by an appropriate ordering of the parts. Going back to
the travel planning with diversifying features example, con-
sider a multi-day trip. Here the parts may refer to individual
days, and Ik includes all features of day k, including the
features relating it to the other days, e.g. the alternation of
cultural and leisure activities. Note that the Ik’s overlap. On
the other hand, the Jk’s are subset of features chosen so that
every feature only appears once. A diversifying feature re-
lating days 3 and 4 of the trip is either assigned to J3 or J4,
but not both.

One way to control the ignored dependencies is by lever-
aging GAI networks (Gonzales and Perny 2004). A GAI net-
work is a graph whose nodes represent the subsets Ik and
whose edges connect nodes sharing at least one feature. Al-
gorithm 1 presents a simple and effective solution to provide
an ordering. It builds a GAI network from P and sorts the
basic parts in ascending order of node degree (number of
incoming and outgoing edges). By ordering last the subsets
having intersections with many other parts, this ordering at-
tempts to minimize the lost dependencies in the above de-
composition (Eq. 3). This is one possible way to order the
parts, which we use as an example; more informed or task-
speciﬁc approaches could be devised.

The PCL algorithm. The pseudocode of our algorithm,
PCL, is listed in Algorithm 2. PCL starts off by sorting the

Algorithm 2 The PCL algorithm.
1: procedure PCL(P, T )
2:
3:
4:
5:
6:

p1, . . . , pn ← SELECTORDERING(P)
w1 ← 0, x1 ← initial conﬁguration
for t = 1, . . . , T do

pt ← SELECTPART(P)
xt
pt ← argmaxxpt ∈Xpt ut[J t](xpt ◦ xt
pt ← xt−1
xt
pt
User provides improvement ˆxt
pt ◦ xt

pt) − ut[I t](xt

pt of xt

pt ◦ xt

(cid:16)

7:

8:

9:

pt)

if
wt+1
wt+1

ut[I t](ˆxt
−Qt ← wt
Qt ← wt

10:

11:

−Qt
Qt + φQt(ˆxt

pt ◦ xt

pt) − φQt(xt

pt ◦ xt

pt))

pt, keeping xt
(cid:17)
pt) ≤ 0

pt ﬁxed
then Qt ← I t

else Qt ← J t ;

basic parts, producing an ordering p1, . . . , pn. Algorithm 1
could be employed or any other (e.g. task-speciﬁc) sorting
solution. Then it loops for T iterations, maintaining an esti-
mate wt of the user weights as well as a complete conﬁgu-
ration xt. The starting conﬁguration x1 should be a reason-
able initial guess, depending on the task. At each iteration
t ∈ [T ], the algorithm selects a part pt using the procedure
SELECTPART (see below). Then it updates the object xt by
inferring a new partial conﬁguration xt
pt while keeping the
rest of xt ﬁxed, that is xt
. The inferred partial con-
ﬁguration xt
pt is optimal with respect to the local subutility
I t(·) given xt−1
ut
. Note that inference is over the partial con-
pt
ﬁguration xt
pt only, and therefore can be exponentially faster
than inference over full conﬁgurations.

pt = xt−1
pt

Next, the algorithm presents the inferred partial conﬁgu-
ration xt
pt as well as some contextual information (see be-
low). The user is asked to produce an improved partial con-
ﬁguration ˆxt
pt according to the her own preferences, while
the rest of the object xt
pt is kept ﬁxed. We assume that a
user is satisﬁed with a partial conﬁguration xt
pt if she can-
not improve it further, or equivalently when the object xt is
conditionally optimal with respect to part pt given the rest of
the object xt
pt (the formal deﬁnition of conditional optimal-
ity is given in the Analysis section). When a user is satisﬁed
with a partial conﬁguration, she returns ˆxt
pt, thereby
implying no change in the weights wt+1.

pt = xt

pt ◦ xt

pt ◦ xt

After receiving an improvement, if the user is not satis-
ﬁed, the weights are updated through a perceptron step. The
subset Qt of weights that are actually updated depends on
pt ) − ut[I t](xt
whether ut[I t](ˆxt
pt) is negative or
(strictly) positive. Since we perform inference on ut[J t](·),
we have that ut[J t](xt
pt ) ≤ 0. The
user improvement can, however, potentially change all the
features in I t. Intuitively, the weights associated to a sub-
set of features should change only if the utility computed on
this subset ranks ˆxt
pt. The algorithm there-
fore checks whether ut[I t](xt
pt) ≤ ut[I t](xt
pt),
in which case the weights associated to the whole subset I t
should be updated. If this condition is not met, instead, the

pt lower than xt
pt ◦ xt

pt) − ut[J t](xt

pt ◦ xt

pt ◦ xt

pt ◦ xt

algorithm can only safely update the weights associated to
J t, which, as said, meet this condition by construction.

As for the SELECTPART procedure, we experimented
with several alternative implementations, including priori-
tizing parts with a large feature overlap (|Ik\Jk|) and bandit-
based strategies aimed at predicting a surrogate of the utility
gain (namely, a variant of the UCB1 algorithm (Auer, Cesa-
Bianchi, and Fischer 2002)). Preliminary experiments have
shown that informed strategies do not yield a signiﬁcant per-
formance improvement over the random selection stategy;
hence we stick with the latter in all our experiments.

The algorithm stops either when the maximum number
of iterations T is met or when a “local optimum” has been
found. For ease of exposition we left out the latter case from
Algorithm 2, but we explain what a local optimum is in the
following Analysis section; the stopping criterion will fol-
low directly from Proposition 1.

Interacting through parts.
In order for the user to judge
the quality of a suggested partial conﬁguration xp, some
contextual information may have to be provided. The rea-
son is that, if p depends on other parts via shared features,
these have to be somehow communicated to the user, other-
wise his/her improvement will not be sufﬁciently informed.
We distinguish two cases, depending on whether the fea-
tures of p are local or global. Local features only depend on
small, localized portions of x. This is for instance the case
for features that measure the diversity of consecutive activ-
ities in a touristic trip plan, which depend on consecutive
time slots or days only. Here the context amounts to those
other portions of x that share local features with p. For in-
stance, the user may interact over individual days only. If
the features are local, the context is simply the time slots be-
fore and after the selected day. The user is free to modify the
activities scheduled that day based on the context, which is
kept ﬁxed.

On the other hand, global features depend on all of x (or
large chunks of it). For instance, in house furnishing one
may have features that measure whether the total cost of the
furniture is within a given budget, or how much the cost sur-
passes the budget. A naive solution would be that of show-
ing the user the whole furniture arrangement x, which can

be troublesome when x is large. A better alternative is to
present the user a summary of the global features, in this case
the percentage of the used budget. Such a summary would
be sufﬁcient for producing an informed improvement, inde-
pendently from the actual size of x.

Of course, the best choice of context format is application
speciﬁc. We only note that, while crucial, the context only
provides auxiliary information to the user, and does not af-
fect the learning algorithm directly.

Analysis
In preference elicitation, it is common to measure the quality
of a recommended (full) conﬁguration in terms of the regret:

REG(x) := maxˆx∈X u∗(ˆx) − u∗(x) = (cid:104)w∗, φ(x∗) − φ(x)(cid:105)
where u∗(·) is the true, unobserved user utility and x∗ =
argmaxx∈X u∗(x) is a truly optimal conﬁguration. In PCL,
interaction with the user occurs via partial conﬁgurations,
namely xt
pt. Since the regret is deﬁned in terms of
complete conﬁgurations, it is difﬁcult to analyze it directly
based on information about the partial conﬁgurations alone,
making it hard to prove convergence to globally optimal rec-
ommendations.

pt and ˆxt

The aim of this analysis is, however, to show that our al-
gorithm converges to a locally optimal conﬁguration, which
is in line with guarantees offered by other Coactive Learn-
ing variants (Goetschalckx, Fern, and Tadepalli 2014); the
latter, however still rely on interaction via complete conﬁg-
urations. Here a conﬁguration x is a local optimum for u∗(·)
if no part-wise modiﬁcation can improve x with respect to
u∗(·). Formally, x is a local optimum for u∗(·) if and only
if:

∀ p ∈ P (cid:64) x(cid:48)

p ∈ Xp u∗(x(cid:48)

p ◦ xp) > u∗(xp ◦ xp)

(4)

To measure local quality of a conﬁguration x with respect to
a part p, we introduce the concept of conditional regret of
the partial conﬁguration xp given the rest of the object xp:

CREGp (x) := u∗(x∗

p ◦ xp) − u∗(xp ◦ xp)
p = argmaxˆxp∈Xp u∗(ˆxp ◦ xp). Notice that:

where x∗

CREGp (x) = u∗[Ip](x∗

p ◦ xp) − u∗[Ip](xp ◦ xp)

since u∗[−Ip](x∗

p ◦ xp) − u∗[−Ip](xp ◦ xp) = 0.

We say that a partial conﬁguration x is conditionally op-
timal with respect to part p if CREGp (x) = 0. The follow-
ing lemma gives sufﬁcient and necessary conditions for local
optimality of a conﬁguration x.
Lemma 1. A conﬁguration x is locally optimal with respect
to u∗(·) if and only if x is conditionally optimal for u∗(·)
with respect to all basic parts p ∈ P.

Proof. By contradiction. (i) Assume that x is locally optimal
but not conditionally optimal with respect to p ∈ P. Then
CREGp (x) > 0, and thus there exists a partial conﬁguration
x(cid:48)
p such that u∗(x(cid:48)
p ◦ x ¯p) > u∗(xp ◦ xx ¯p ). This violates
the local optimality of x (Eq. 4). (ii) Assume that all partial
conﬁgurations xp ∀p ∈ P are conditionally optimal but x

is not locally optimal. Then there exists a part q ∈ P and a
partial conﬁguration x(cid:48)
q ◦x¯q) > u∗(xq ◦x¯q).
q such that u∗(x(cid:48)
This in turn means that CREGq (x) > 0. This violates the
conditional optimality of x with respect to q.

The above lemma gives us a part-wise measurable crite-
rion to determine if a conﬁguration x is a local optimum
through the conditional regret of x for all the provided parts.
The rest of the analysis is devoted to derive an upper
bound on the conditional regret incurred by the algorithm
and to prove that PCL eventually reaches a local optimum.

In order to derive the bound, we rely on the concept of
α-informativeness from (Shivaswamy and Joachims 2015),
adapting it to part-wise interaction2. A user is conditionally
α-informative if, when presented with a partial conﬁgura-
pt, he/she provides a partial conﬁguration ˆxt
tion xt
pt that is
at least some fraction α ∈ (0, 1] better than xt
pt in terms of
conditional regret, or more formally:
pt ◦ xt
pt) − u∗[I t](xt

pt) − u∗[I t](xt
pt ◦ xt

pt ◦ xt
α (cid:0)u∗[I t](x∗

u∗[I t](ˆxt

pt ◦ xt

pt) ≥

pt)(cid:1)

(5)

In the rest of the paper we will use the notation u[I t](ˆxt)
pt ◦ xt
meaning u[I t](ˆxt
pt), i.e. drop the complement, when
no ambiguity can arise.

At all iterations t, the algorithm updates the weights spec-
iﬁed by Qt, producing a new estimate wt+1 of w∗. The ac-
tual indices Qt depend on the condition at line 9 of Algo-
rithm 2: at some iterations Qt includes all of I t, while at
others Qt is restricted to J t. We distinguish between these
cases by:

I = {t ∈ [T ] : ut[I t](ˆxt) − ut[I t](xt) ≤ 0}
J = {t ∈ [T ] : ut[I t](ˆxt) − ut[I t](xt) > 0}

so that if t ∈ I then Qt = I t, and Qt = J t if t ∈ J . For all
t ∈ [T ], the quality of wt+1 is:

(cid:104)w∗, wt+1(cid:105) = (cid:104)w∗
= (cid:104)w∗

Qt, wt+1
Qt (cid:105)
Qt, wt
Qt(cid:105)

−Qt(cid:105) + (cid:104)w∗
−Qt(cid:105) + (cid:104)w∗

−Qt, wt+1
−Qt, wt
+ (cid:104)w∗
= (cid:104)w∗, wt(cid:105) + (cid:104)w∗
= (cid:104)w∗, wt(cid:105) + u∗[Qt](ˆxt) − u∗[Qt](xt)

Qt, φQt(ˆxt) − φQt(xt)(cid:105)

Qt, φQt(ˆxt) − φQt(xt)(cid:105)

Therefore if the second summand in the last equation, the
utility gain u∗[Qt](ˆxt) − u∗[Qt](xt), is positive, the update
produces a better weight estimate wt+1.

Since the user is conditionally α-informative, the im-
provement ˆxt always satisﬁes u∗[I t](ˆxt) − u∗[I t](xt) ≥
0. When t ∈ I, we have Qt = I t, and thus the util-
ity gain is guaranteed to be positive. On the other hand,
when t ∈ J we have Qt = J t and the utility gain re-
duces to u∗[J t](ˆxt) − u∗[J t](xt). In this case the update

2Here we adopted the deﬁnition of strict α-informativeness
for simplicity. Our results can be directly extended to the more
general notions of informativeness described in (Shivaswamy and
Joachims 2015).

ignores the weights in I t \ J t, “missing out” a factor of
u∗[I t \ J t](ˆxt) − u∗[I t \ J t](xt).

We compactly quantify the missing utility gain as:

(cid:26)0

ζ t =

u∗[I t \ J t](ˆxt) − u∗[I t \ J t](xt)

if t ∈ I
if t ∈ J

Note that ζ t can be positive, null or negative for t ∈ J .
When ζ t is negative, making the update on J t only actually
avoids a loss in utility gain.

(cid:80)T

We now prove that PCL minimizes the average conditional
t=1 CREGpt (xt) as T → ∞ for

regret CREGT := 1
T
conditionally α-informative users.
Theorem 1. For a conditionally α-informative user, the av-
erage conditional regret of PCL after T iterations is upper
bounded by:

1
T

T
(cid:88)

t=1

CREGpt

(cid:0)xt(cid:1) ≤

2DS(cid:107)w∗(cid:107)
√

+

α

T

1
αT

T
(cid:88)

t=1

ζ t

Proof. The following is a sketch3. We start by splitting the
iterations into the I and J sets deﬁned above, and bound
the norm (cid:107)wT +1(cid:107)2. In both cases we ﬁnd that (cid:107)wT +1(cid:107)2 ≤
4D2S2T . We then expand the term (cid:104)w∗, wT +1(cid:105) for itera-
tions in both I and J , obtaining:

(cid:104)w∗, wT +1(cid:105) =

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105)

(cid:88)

t∈I

+

(cid:88)

t∈J

(cid:104)w∗

J t, φJ t(ˆxt) − φJ t (xt)(cid:105)

With few algebraic manipulations we obtain:

(cid:104)w∗, wT +1(cid:105)

=

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105) −

Which we then bound using the Cauchy-Schwarz inequality:

√

(cid:107)w∗(cid:107)

4D2S2T

≥ (cid:107)w∗(cid:107)(cid:107)wT +1(cid:107)
≥ (cid:104)w∗, wT +1(cid:105)

=

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105) −

T
(cid:88)

t=1

ζ t

T
(cid:88)

t=1

ζ t

T
(cid:88)

t=1

T
(cid:88)

t=1

Applying the conditional α-informative feedback (Eq. 5)
and rearranging proves the claim.

Theorem 1 ensures that the average conditional regret suf-
fered by our algorithm decreases as O(1/
T ). This alone,
however, does not prove that the algorithm will eventually

√

3The complete proof can be found in the Supplementary Mate-

rial.

t=τ0

arrive at a local optimum, even if (cid:80)T
CREGpt (xt) = 0,
for some τ0 ∈ [T ]. This is due to the fact that partial in-
ference is performed keeping the rest of the object xt
pt ﬁxed.
Between iterations an inferred part may change as a result of
a change of the other parts in previous iterations. The object
could, in principle, keep changing at every iterations, even
if CREGpt (xt) is always equal to 0. The next proposition,
however, shows that this is not the case thanks to the utility
decomposition we employ.
Proposition 1. Let τ1 < . . . < τn < ˆτ1 < . . . < ˆτn ≤ T such
that pτk = pˆτk = pk and CREGpt (xt) = 0 for all t ≥ τ1.
The conﬁguration xT is a local optimum.

= xτ1

p1 for all t > τ1, as ut
I1

Proof. Sketch. The proof procedes by strong induction. We
ﬁrst show that xt
(·)
p1
only depends on the features in J1 and by assumption
CREGpt (xt) = 0 for all t ≥ τ1. By strong induction, as-
suming that xt
pj for all j = 1, . . . , k − 1 and all
pj
= xτk
t > τk−1, we can easily show that xt
pk
pk for all t > τn, therefore xˆτk = xτn for
(cid:0)xˆτk (cid:1) = 0 for all k ∈ [n] by
all k ∈ [n]. Since CREGpk
assumption, then xT = xˆτn = xτn is a local optimum and
will not change for all t > τn.

pk as well.

Now, xt
pk

= xτj

= xτk

The algorithm actually reaches a local optimum at t =
τn, but it needs to double check all the parts in order to be
sure that the conﬁguration is actually a local optimum. This
justiﬁes a termination criterion that we use in practice: if
the algorithm completes two full runs over all the parts, and
the user can never improve any of the recommended partial
conﬁgurations, then the full conﬁguration is guaranteed to be
a local optimum, and the algorithm can stop. As mentioned,
we employ this criterion in our implementation but we left it
out from Algorithm 2 for simplicity.

To recap, Theorem 1 guarantees that CREGT → 0 as t →
∞, therefore CREGpt (xt) approaches 0 as well. Combining
this fact with Proposition 1, we proved that our algorithm
approaches a local optimum for T → ∞.

Empirical Analysis
We ran PCL on three constructive preference elicitation tasks
of increasing complexity, comparing different degrees of
user informativeness. According to our experiments, infor-
mativeness is the most critical factor. The three problems in-
volve rather large conﬁgurations, which can not be handled
by coactive interaction via complete conﬁgurations. For in-
stance, in (Ortega and Stocker 2016) the user is tasked to
solve relatively simple SAT instances over three variables
and (at most) eight clauses; in some cases users were ob-
served to show signs of cognitive overload. In comparison,
our simplest realistic problem involve 35 categorical vari-
ables (with 8 possible values) and 74 features, plus addi-
tional hard constraints. As a consequence, Coactive Learn-
ing can not be applied as-is, and part-wise interaction is nec-
essary.

In all of these settings, part-wise inference is cast
as a mixed integer linear problem (MILP), and solved

with Gecode4. Despite being NP-hard in general, MILP
solvers can be very efﬁcient on practical instances. Ef-
improved by inferring only partial
ﬁciency is further
conﬁgurations. Our experimental setup is available at
https://github.com/unitn-sml/pcl.

We employed a user simulation protocol similar to that
of (Teso, Dragone, and Passerini 2017). First, for each prob-
lem, we sampled 20 vectors w∗ at random from a standard
normal distribution. Then, upon receiving a recommenda-
tion xt
pt is generated by solving the
following problem:

pt, an improvement ˆxt

argmin
xpt ∈Xpt

u∗[I t](xpt ◦ xt

pt)

s.t. u∗[I t](xpt ◦ xt

pt) − u∗[I t](xt
pt ◦ xt

pt ◦ xt
pt) − u∗[I t](xt

pt)

≥ α(u∗[I t](x∗

pt ◦ xt

pt))

This
formulation clearly satisﬁes
informativeness assumption (Eq. 5).

the conditional α-

Synthetic setting. We designed a simple synthetic prob-
lem inspired by spin glass models, see Figure 1 for a depic-
tion. In this setting, a conﬁguration x consists of a 4 × 4
grid. Each node in the grid is a binary 0-1 variable. Adjacent
nodes are connected by an edge, and each edge is associ-
ated to an indicator feature that evaluates to 1 if the inci-
dent nodes have different values (green in the ﬁgure), and to
−1 otherwise (red in the ﬁgure). The utility of a conﬁgura-
tion is simply the weighted sum of the values of all features
(edges). The basic parts p consist of all the non-overlapping
2 × 2 sub-grids of x, for a total of 4 basic parts (indicated by
dotted lines in the ﬁgure).

Figure 1: Example grid conﬁguration.

Since the problem is small enough for inference of
complete conﬁgurations to be practical, we compared PCL
to standard Coactive Learning, using the implementation
of (Teso, Dragone, and Passerini 2017). In order to keep the
comparison as fair as possible, the improvements fed to CL
were chosen to match the utility gain obtained by PCL. We
further report the performance of three alternative part se-
lection strategies: random, smallest (most independent) part
ﬁrst, and UCB1.

4http://www.gecode.org/

The results can be found in the ﬁrst column of Figure 2.
We report both the regret (over complete conﬁgurations) and
the cumulative runtime of all algorithms, averaged over all
users, as well as their standard deviation. The regret plot
shows that, despite being restricted to work with 2 × 2 con-
ﬁgurations, PCL does recommend complete conﬁgurations
of quality comparable to CL after enough queries are made.
Out of the three part selection strategies, random performs
best, with the other two more informed alternatives (espe-
cially smallest ﬁrst) quite close. The runtime gap between
full and part-wise inference is already clear in this small
synthetic problem; complete inference quickly becomes im-
practical as the problem size increases.

Training planning. Generating personalized training
plans based on performance and health monitoring has re-
ceived a lot of attention recently in sport analytics (see
e.g. (Fister et al. 2015)). Here we consider the problem of
synthesizing a week-long training plan x from information
about the target athlete. Each day includes 5 time slots (two
for the morning, two for the afternoon, one for the evening),
for 35 slots total. We assume to be given a ﬁxed number of
training activities (7 in our experiments: walking, running,
swimming, weight lifting, pushups, squats, abs), as well as
knowledge of the slots in which the athlete is available. The
training plan x associates an activity to each slot where the
athlete is available. Our formulation tracks the amount of
improvement (e.g. power increase) and fatigue over ﬁve dif-
ferent body parts (arms, torso, back, legs, and heart) induced
by performing an activity for one time slot. Each day deﬁnes
a basic part.

The mapping between training activity and improve-
ment/fatigue over each body part is assumed to be provided
externally. It can be provided by the athlete or medical per-
sonnel monitoring his/her status. The features of x include,
for each body part, the total performance gain and fatigue,
computed over the recommended training plan according to
the aforementioned mapping. We further include inter-part
features to capture activity diversity in consecutive days. The
fatigue accumulated in 3 consecutive time slots in any body
parts does not exceed a given threshold, to prevent injuries.
In this setting, CL is impractical from both the cognitive
and computational points of view. We ran PCL and evaluated
the impact of user informativeness by progressively increas-
ing α from 0.1, to 0.3, to 0.5. The results can be seen in Fig-
ure 2. The plots show clearly that, despite the complexity of
the conﬁguration and constraints, PCL can still produce very
low-regret conﬁgurations after about 50 iterations or less.

Understandably, the degree of improvement α plays an
important role in the performance of PCL and, consequently,
in its runtime (users at convergence do not contribute to the
runtime), at least up to α = 0.5. Recall, however, that the
improvements are part-wise, and hence α quantiﬁes the de-
gree of local improvement: part improvements may be very
informative on their own, but only give a modest amount
of information about the full conﬁguration. However, it is
not unreasonable to expect that users to be very informative
when presented with reasonably sized (and simple) parts.

Figure 2: Regret over complete conﬁgurations (top) and cumulative runtime (bottom) of PCL and CL on our three constructive
problems: synthetic (left), training planning (middle), and hotel planning (right). The x-axis is the number of iterations, while
the shaded areas represent the standard deviation. Best viewed in color.

Crucially PCL allows the system designer to deﬁne the parts
appropriately depending on the application.

Hotel planning. Finally, we considered a complex furni-
ture allocation problem: furnishing an entire hotel. The prob-
lem is encoded as follows. The hotel is represented by a
graph: nodes are rooms and edges indicate which rooms are
adjacent. Rooms can be of three types: normal rooms, suites,
and dorms. Each room can hold a maximum number of fur-
niture pieces, each associated to a cost. Additional, ﬁxed
nodes represent bathrooms and bars. The type of a room is
decided dynamically based on its position and furniture. For
instance, a normal room must contain at most three single or
double beds, no bunk beds, and a table, and must be close
to a bathroom. A suite must contain one bed, a table and a
sofa, and must be close to a bathroom and a bar. Each room
is a basic part, and there are 15 rooms to be allocated.

The feature vector contains 20 global features plus 8 lo-
cal features per room. The global features include different
functions of the number of different types of rooms, the total
cost of the furniture and the total number of guests. The local
features include, instead, characteristics of the current room,
such as its type or the amount of furniture, and other fea-
tures shared by adjacent rooms, e.g. whether two rooms have
the same type. These can encode preferences like “suites
and dorms should not be too close”, or “the hotel should
maintain high quality standards while still being proﬁtable”.
Given the graph structure, room capacities, and total bud-
get, the goal is to furnish all rooms according to the user’s
preferences.

This problem is hard to solve to optimality with current

solvers; part-based inference alleviates this issue by focus-
ing on individual rooms. There are 15 rooms in the hotel,
so that at each iteration only 1/15 of the conﬁguration is af-
fected. Furthermore, the presence of the global features im-
plies dependences between all rooms. Nonetheless, the al-
gorithm manages to reduce the regret by an order of magni-
tude in around a 100 iterations, starting from a completely
uninformed prior. Note also that as for the training planning
scenario, an alpha of 0.3 achieves basically the same results
as those for alpha equal to 0.5.

Conclusion

In this work we presented an approach to constructive pref-
erence elicitation able to tackle large constructive domains,
beyond the reach of previous approaches. It is based on
Coactive Learning (Shivaswamy and Joachims 2015), but
only requires inference of partial conﬁgurations and partial
improvement feedback, thereby signiﬁcantly reducing the
cognitive load of the user. We presented an extensive the-
oretical analysis demonstrating that, despite working only
with partial conﬁgurations, the algorithm converges to a lo-
cally optimal solution. The algorithm has been evaluated
empirically on three constructive scenarios of increasing
complexity, and shown to perform well in practice.

Possible future work includes improving part-based in-
teraction by exchanging additional contextual information
(e.g. features (Teso, Dragone, and Passerini 2017) or expla-
nations) with the user, and applying PCL to large layout syn-
thesis problems (Dragone et al. 2016).

ings of the 18th ACM SIGKDD international conference on
Knowledge discovery and data mining, 705–713. ACM.
Shivaswamy, P., and Joachims, T. 2015. Coactive learning.
JAIR 53:1–40.
Teso, S.; Dragone, P.; and Passerini, A. 2017. Coactive
critiquing: Elicitation of preferences and features. In AAAI.
Teso, S.; Passerini, A.; and Viappiani, P. 2016. Constructive
preference elicitation by setwise max-margin learning.
In
Proceedings of the Twenty-Fifth International Joint Confer-
ence on Artiﬁcial Intelligence, 2067–2073. AAAI Press.

References
Auer, P.; Cesa-Bianchi, N.; and Fischer, P. 2002. Finite-
time analysis of the multiarmed bandit problem. Machine
learning 47(2-3):235–256.
Boutilier, C.; Bacchus, F.; and Brafman, R. I. 2001. Ucp-
networks: A directed graphical representation of conditional
In Proceedings of the Seventeenth conference on
utilities.
Uncertainty in artiﬁcial intelligence, 56–64. Morgan Kauf-
mann Publishers Inc.
Boutilier, C.; Patrascu, R.; Poupart, P.; and Schuurmans, D.
2006. Constraint-based optimization and utility elicitation
using the minimax decision criterion. Artiﬁcial Intelligence
170(8-9):686–713.
Braziunas, D., and Boutilier, C. 2005. Local utility elici-
In Proceedings of the Twenty-First
tation in GAI models.
Conference on Uncertainty in Artiﬁcial Intelligence, 42–49.
AUAI Press.
Braziunas, D., and Boutilier, C. 2007. Minimax regret based
elicitation of generalized additive utilities. In UAI, 25–32.
Braziunas, D., and Boutilier, C. 2009. Elicitation of factored
utilities. AI Magazine 29(4):79.
Chajewska, U.; Koller, D.; and Parr, R. 2000. Making ratio-
nal decisions using adaptive utility elicitation. In AAAI/IAAI,
363–369.
Dragone, P.; Erculiani, L.; Chietera, M. T.; Teso, S.; and
Passerini, A. 2016. Constructive layout synthesis via coac-
tive learning. In Constructive Machine Learning workshop,
NIPS.
Fishburn, P. C. 1967.
Interdependence and additivity in
multivariate, unidimensional expected utility theory. Inter-
national Economic Review 8(3):335–342.
Fister, I.; Rauter, S.; Yang, X.-S.; and Ljubiˇc, K. 2015. Plan-
ning the sports training sessions with the bat algorithm. Neu-
rocomputing 149:993–1002.
Goetschalckx, R.; Fern, A.; and Tadepalli, P. 2014. Coactive
learning for locally optimal problem solving. In Proceedings
of AAAI.
Gonzales, C., and Perny, P. 2004. GAI networks for utility
elicitation. KR 4:224–234.
Keeney, R. L., and Raiffa, H. 1976. Decisions with Multiple
Objectives: Preferences and Value Tradeoffs.
Mayer, R. E., and Moreno, R. 2003. Nine ways to reduce
cognitive load in multimedia learning. Educational psychol-
ogist 38(1):43–52.
Meseguer, P.; Rossi, F.; and Schiex, T. 2006. Soft con-
straints. Foundations of Artiﬁcial Intelligence 2:281–328.
Ortega, P. A., and Stocker, A. A. 2016. Human decision-
making under limited time. In Advances in Neural Informa-
tion Processing Systems, 100–108.
Pigozzi, G.; Tsouki`as, A.; and Viappiani, P. 2016. Prefer-
ences in artiﬁcial intelligence. Ann. Math. Artif. Intell. 77(3-
4):361–401.
Raman, K.; Shivaswamy, P.; and Joachims, T. 2012. Online
In Proceed-
learning to diversify from implicit feedback.

Supplementary Material

We add and subtract the term:

Proof of Theorem 2
We begin by expanding (cid:107)wT +1(cid:107)2. If T ∈ I:

(cid:107)wT +1(cid:107)2

= (cid:107)wT +1
= (cid:107)wT
= (cid:107)wT

−I T (cid:107)2 + (cid:107)wT +1
−I T (cid:107)2 + (cid:107)wT
−I T (cid:107)2 + (cid:107)wT
+ 2(cid:104)wT

I T (cid:107)2
I T + φI T (ˆxt) − φI T (xt)(cid:107)2
I T (cid:107)2 + (cid:107)φI T (ˆxt) − φI T (xt)(cid:107)2

I T , φI T (ˆxt) − φI T (xt)(cid:105)

Since T ∈ I, uT [I T ](ˆxT ) − uT [I T ](xT ) ≤ 0, thus:

I T (cid:107)2 + (cid:107)φI T (ˆxt) − φI T (xt)(cid:107)2

∞|I T |2

(cid:107)wT +1(cid:107)2

−I T (cid:107)2 + (cid:107)wT
≤ (cid:107)wT
≤ (cid:107)wT (cid:107)2 + 4D2S2

If instead T ∈ J :

(cid:107)wT +1(cid:107)2

= (cid:107)wT +1
= (cid:107)wT
= (cid:107)wT
= (cid:107)wT

−J T (cid:107)2 + (cid:107)wT +1
−J T (cid:107)2 + (cid:107)wT +1
−J T (cid:107)2 + (cid:107)wT
−J T (cid:107)2 + (cid:107)wT
+ 2(cid:104)wT

J T (cid:107)2
J T (cid:107)2
J T + φJ T (ˆxt) − φJ T (xt)(cid:107)2
J T (cid:107)2 + (cid:107)φJ T (ˆxt) − φJ T (xt)(cid:107)2

J T , φJ T (ˆxt) − φJ T (xt)(cid:105)

≤ (cid:107)wT (cid:107)2 + (cid:107)φJ T (ˆxt) − φJ T (xt)(cid:107)2
≤ (cid:107)wT (cid:107)2 + 4D2|J T |2
≤ (cid:107)wT (cid:107)2 + 4D2S2

∞|J T |2

We can therefore expand the term:

(cid:107)wT +1(cid:107)2 ≤ 4D2S2T

Applying Cauchy-Schwarz inequality:

(cid:104)w∗, wT +1(cid:105) ≤ (cid:107)w∗(cid:107)(cid:107)wT +1(cid:107)

≤ 2DS(cid:107)w∗(cid:107)

T

√

The LHS of the above inequality expands to:

(cid:104)w∗, wT +1(cid:105) = (cid:104)w∗

−I T (cid:105) + (cid:104)w∗

I T , wT

I T (cid:105)

−I T , wT
+ (cid:104)w∗

I T , φI T (ˆxT ) − φI T (xT )(cid:105)

[ if T ∈ I ]
(cid:104)w∗, wT +1(cid:105) = (cid:104)w∗

−J T , wT
+ (cid:104)w∗

−J T (cid:105) + (cid:104)w∗

J T , wT

J T (cid:105)

J T , φJ T (ˆxT ) − φJ T (xT )(cid:105)

[ if T ∈ J ]

And thus:

(cid:104)w∗, wT +1(cid:105) =

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105)

(cid:88)

t∈I

+

(cid:88)

t∈J

(cid:104)w∗

J t , φJ t(ˆxt) − φJ t(xt)(cid:105)

(cid:104)w∗

I t\J t, φI t\J t(ˆxt) − φI t\J t(xt)(cid:105)

(cid:88)

t∈J

We obtain:

(cid:104)w∗, wT +1(cid:105)

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

=

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105)

−

(cid:88)

t∈J

(cid:104)w∗

I t\J t, φI t\J t(ˆxt) − φJ t(xt)(cid:105)

=

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105) −

≥ α

(cid:104)w∗

I t, φI t(x∗

pt ◦ xt

¯pt) − φI t(xt)(cid:105) −

T
(cid:88)

t=1

ζ t

T
(cid:88)

t=1

ζ t

And thus:

T
(cid:88)

t=1

(cid:104)w∗

I t, φI t(x∗) − φI t(xt)(cid:105) ≤

√

2DS(cid:107)w∗(cid:107)

T

α

+

1
α

T
(cid:88)

t=1

ζ t

Dividing both sides by T proves the claim.

Proof of Proposition 3
We ﬁrst show that xt
pk
iteration t the algorithm produces xt
local subutility:

= xτk

pk for t > τk. Recall that at
pk by maximizing the

ut
Ik

(x) = ut[Jk](x) = ut[Ik \ ((cid:83)n

j=k+1 Ij)](x)

j=k+1 Ij. Each subutility
(·) is only affected by the changes in xp1 , . . . , xpk−1, as

i.e. ignoring all the features in (cid:83)n
ut
Ik
their feature subsets may intersect with Ik.

= wτ1

The subutility ut
I1

(·) only depends on the features that
are exclusively included in I1, so it is not affected by the
changes in all the other parts. At iteration τ1, the condi-
tional regret CREGp1 (xτ1 ) = 0, and thus ˆxτ1
p1 and
p1
wτ1+1
(·)
I t
will not have changed as it only depends on features exclu-
sively included in I1 and thus xt
= xτ1
p1 . Since, by assump-
p1
tion, CREGpt (xt) = 0 for all t ≥ τ1, uτ1
(·) will not change
I1
anymore and so xt
p1

= xτ1
I t . If p1 gets selected again at t > τ1, ut
I1

= xτ1
p1.
By strong induction, suppose that all parts xt
pj

pj for
all j = 1, . . . k − 1 and for all t > τk−1. At iteration τk,
CREGpk (xτk ) = 0, thus ˆxτk
pk and so the weights
pk
wτk+1
will not change. For the following t > τk iterations,
Ik
even if pk gets selected, since the subutility ut
(·) is only
pk
affected by the changes in xp1 , . . . , xpk−1 and those parts do
not change (by inductive assumption), we can conclude that
ut
(·) = uτk
(·) and that xt
pk
pk
pk
This proves that, for any k ∈ [n] and any t > τk, xt
=
pk
xτk
pk . This also means that none of the partial conﬁgurations

= xτk
pk .

= xτj

= xτk

xt
pt will change for t > τn. Since none of the partial conﬁg-
(cid:0)xˆτ1 (cid:1) = 0 by
uration changed, xˆτ1 = xτn . Since CREGp1
assumption, then xˆτ1 is conditionally optimal with respect
(cid:0)xˆτk (cid:1) = 0 for all
to p1. Likewise, xˆτk = xτn and CREGpk
k ∈ [n], and thus xT = xˆτn = xτn is a local optimum.

8
1
0
2
 
y
a
M
 
6
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
7
4
2
8
0
.
1
1
7
1
:
v
i
X
r
a

Decomposition Strategies for Constructive Preference Elicitation
(and Supplementary Material)

Paolo Dragone∗
University of Trento, Italy
TIM-SKIL, Trento, Italy
paolo.dragone@unitn.it

Stefano Teso
KU Leuven, Belgium
stefano.teso@cs.kuleuven.be

Mohit Kumar †
KU Leuven, Belgium
mohit.kumar@cs.kuleuven.be

Andrea Passerini
University of Trento, Italy
andrea.passerini@unitn.it

Abstract

We tackle the problem of constructive preference elicitation,
that is the problem of learning user preferences over very
large decision problems, involving a combinatorial space of
possible outcomes. In this setting, the suggested conﬁgura-
tion is synthesized on-the-ﬂy by solving a constrained op-
timization problem, while the preferences are learned itera-
tively by interacting with the user. Previous work has shown
that Coactive Learning is a suitable method for learning user
preferences in constructive scenarios. In Coactive Learning
the user provides feedback to the algorithm in the form of an
improvement to a suggested conﬁguration. When the problem
involves many decision variables and constraints, this type of
interaction poses a signiﬁcant cognitive burden on the user.
We propose a decomposition technique for large preference-
based decision problems relying exclusively on inference and
feedback over partial conﬁgurations. This has the clear ad-
vantage of drastically reducing the user cognitive load. Addi-
tionally, part-wise inference can be (up to exponentially) less
computationally demanding than inference over full conﬁg-
urations. We discuss the theoretical implications of working
with parts and present promising empirical results on one syn-
thetic and two realistic constructive problems.

Introduction
In constructive preference elicitation (CPE) the recom-
mender aims at suggesting a custom or novel product to a
customer (Teso, Passerini, and Viappiani 2016). The prod-
uct is assembled on-the-ﬂy from components or synthe-
sized anew by solving a combinatorial optimization prob-
lem. The suggested products should of course satisfy the
customer’s preferences, which however are unobserved and
must be learned interactively (Pigozzi, Tsouki`as, and Vi-
appiani 2016). Learning proceeds iteratively: the learner
presents one or more candidate recommendations to the cus-
tomer, and employs the obtained feedback to estimate the

∗PD is a fellow of TIM-SKIL Trento and is supported by a
TIM scholarship. This work has received funding from the Euro-
pean Research Council (ERC) under the European Unions Horizon
2020 research and innovation programme (grant agreement No.
[694980] SYNTH: Synthesising Inductive Data Models).
Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

customer’s preferences. Applications include recommend-
ing custom PCs or cars, suggesting touristic travel plans,
designing room and building layouts, and producing recipe
modiﬁcations, among others.

A major weakness of existing CPE methods (Teso,
Passerini, and Viappiani 2016; Teso, Dragone, and Passerini
2017) is that they require the user to provide feedback on
complete conﬁgurations. In real-world constructive prob-
lems such as trip planning and layout design, conﬁgura-
tions can be large and complex. When asked to evaluate or
manipulate a complex product, the user may become over-
whelmed and confused, compromising the reliability of the
obtained feedback (Mayer and Moreno 2003). Human deci-
sion makers can revert to a potentially uninformative prior
when problem solving exceeds their available resources.
This effect was observed in users tasked with solving sim-
ple SAT instances (three variables and eight clauses) (Or-
tega and Stocker 2016). In comparison, even simple con-
structive problems can involve tens of categorical variables
and features, in addition to hard feasibility constraints. On
the computational side, working with complete conﬁgura-
tions poses scalability problems as well. The reason is that,
in order to select recommendations and queries, constructive
recommenders employ constraint optimization techniques.
Clearly, optimization of complete conﬁgurations in large
constructive problems can become computationally imprac-
tical as the problem size increases.

Here we propose to exploit factorized utility func-
tions (Braziunas and Boutilier 2009), which occur very natu-
rally in constructive problems, to work with partial conﬁgu-
rations. In particular, we show how to generalize Coactive
Learning (CL) (Shivaswamy and Joachims 2015) to part-
wise inference and learning. CL is a simple, theoretically
grounded algorithm for online learning and preference elic-
itation. It employs a very natural interaction protocol: at
each iteration the user is presented with a single, appropri-
ately chosen candidate conﬁguration and asked to improve
it (even slightly). In (Teso, Dragone, and Passerini 2017), it
was shown that CL can be lifted to constructive problems
by combining it with a constraint optimization solver to ef-
ﬁciently select the candidate recommendation. Notably, the
theoretical guarantees of CL remain intact in the construc-

tive case.

Our part-wise generalization of CL, dubbed PCL, solves
the two aforementioned problems in one go: (i) by present-
ing the user with partial conﬁgurations, it is possible to (sub-
stantially) lessen her cognitive load, improving the reliabil-
ity of the feedback and enabling learning in larger construc-
tive tasks; (ii) in combinatorial constructive problems, per-
forming inference on partial conﬁgurations can be exponen-
tially faster than on complete ones. Further, despite being
limited to working with partial conﬁgurations, PCL can be
shown to still provide local optimality guarantees in theory,
and to perform well in practice.

This paper is structured as follows. In the next section
we overview the relevant literature. We present PCL in the
Method section, followed by a theoretical analysis. The per-
formance of PCL are then illustrated empirically on one syn-
thetic and two realistic constructive problems. We close the
paper with some concluding remarks.

Related Work
Generalized additive independent (GAI) utilities have been
thoroughly explored in the decision making literature (Fish-
burn 1967). They deﬁne a clear factorization mechanism,
and offer a good trade off between expressiveness and ease
of elicitation (Chajewska, Koller, and Parr 2000; Gonzales
and Perny 2004; Braziunas and Boutilier 2009). Most of the
early work on GAI utility elicitation is based on graphical
models, e.g. UCP and GAI networks (Gonzales and Perny
2004; Boutilier, Bacchus, and Brafman 2001). These ap-
proaches aim at eliciting the full utility function and rely
on the comparison of full outcomes. Both of these are infea-
sible when the utility involves many attributes and features,
as in realistic constructive problems.

Like our method, more recent alternatives (Braziunas and
Boutilier 2005; 2007) handle both partial elicitation, i.e. the
ability of providing recommendations without full utility in-
formation, and local queries, i.e. elicitation of preference
information by comparing only (mostly) partial outcomes.
There exist both Bayesian (Braziunas and Boutilier 2005)
and regret-based (Braziunas and Boutilier 2007; Boutilier
et al. 2006) approaches, which have different shortcomings.
Bayesian methods do not scale to even small size construc-
tive problems (Teso, Passerini, and Viappiani 2016), such as
those occurring when reasoning over individual parts in con-
structive settings. On the other hand, regret-based methods
require the user feedback to be strictly self-consistent, an
unrealistic assumption when interacting with non-experts.
Our approach instead is speciﬁcally designed to scale to
larger constructive problems and, being derived from Coac-
tive Learning, natively handles inconsistent feedback. Cru-
cially, unlike PCL, these local elicitation methods also re-
quire to perform a number of queries over complete con-
ﬁgurations to calibrate the learned utility function. In larger
constructive domains this is both impractical (on the user
side) and computationally infeasible (on the learner side).

Our work is based on Coactive Learning (CL) (Shiv-
aswamy and Joachims 2015), a framework for learning util-
ity functions over structured domains, which has been suc-
cessfully applied to CPE (Teso, Dragone, and Passerini

2017; Dragone et al. 2016). When applied to constructive
problems, a crucial limitation of CL is that the learner and
the user interact by exchanging complete conﬁgurations.
Alas, inferring a full conﬁguration in a constructive prob-
lem can be computationally demanding, thus preventing the
elicitation procedure from being real-time. This can be par-
tially addressed by performing approximate inference, as
in (Raman, Shivaswamy, and Joachims 2012), at the cost of
weaker learning guarantees. A different approach has been
taken in (Goetschalckx, Fern, and Tadepalli 2014), where
the exchanged (complete) conﬁgurations are only required
to be locally optimal, for improved efﬁciency. Like PCL, this
method guarantees the local optimality of the recommended
conﬁguration. All of the previous approaches, however, re-
quire the user to improve a potentially large complete con-
ﬁguration. This is a cognitively demanding task which can
become prohibitive in large constructive problems, even for
domain experts, thus hindering feedback quality and effec-
tive elicitation. By dealing with parts only, PCL avoids this
issue entirely.

Method
Notation. We use rather standard notation: scalars x are
written in italics and column vectors x in bold. The inner
product of two vectors is indicated as (cid:104)a, b(cid:105) = (cid:80)
i aibi, the
Euclidean norm as (cid:107)a(cid:107) = (cid:112)(cid:104)a, a(cid:105) and the max-norm as
(cid:107)a(cid:107)∞ = maxi ai. We abbreviate {1, . . . , n} to [n], and in-
dicate the complement of I ⊆ [n] as −I := [n] \ I.

Setting. Let X be the set of candidate structured products.
Contrarily to what happens in standard preference elicita-
tion, in the constructive case X is deﬁned by a set of hard
constraints rather than explicitly enumerated1. Products are
represented by a function φ(·) that maps them to an m-
dimensional feature space. While the feature map can be
arbitrary, in practice we will stick to features that can be en-
coded as constraints in a mixed-integer linear programming
problem, for efﬁciency; see the Empirical Analysis section
for details. We only assume that the features are bounded,
i.e. ∀x ∈ X it holds that (cid:107)φ(x)(cid:107)∞ ≤ D for some ﬁxed D.

As is common in multi-attribute decision theory (Keeney
and Raiffa 1976), we assume the desirability of a product
x to be given by a utility function u∗ : X → R that is
linear in the features, i.e., u∗(x) := (cid:104)w∗, φ(x)(cid:105). Here the
weights w∗ ∈ Rm encode the true user preferences, and
may be positive, negative, or zero (which means that the cor-
responding feature is irrelevant for the user). Utilities of this
kind can naturally express rich constructive problems (Teso,
Passerini, and Viappiani 2016; Teso, Dragone, and Passerini
2017).

Parts. Here we formalize what parts and partial conﬁgu-
rations are, and how they can be manipulated. We assume to

1In this paper, “hard” constraints refer to the constraints de-
limiting the space of feasible conﬁgurations, as opposed to “soft”
constraints, which determine preferences over feasible conﬁgura-
tions (Meseguer, Rossi, and Schiex 2006).

be given a set of n basic parts p ∈ P. A part is any subset
P ⊆ P of the set of basic parts. Given a part P and an object
x, xP ∈ XP indicates the partial conﬁguration correspond-
ing to P . We require that the union of the basic parts recon-
structs the whole object, i.e. xP = x for all x ∈ X . The
proper semantics of the decomposition into basic parts is
task-speciﬁc. For instance, in a scheduling problem a month
may be decomposed into days, while in interior design a
house may be decomposed into rooms. Analogously, the
non-basic parts could then be weeks or ﬂoors, respectively.
In general, any combination of basic parts is allowed. We
capture the notion of combination of partial conﬁgurations
with the part combination operator ◦ : XP × XQ → XP ∪Q,
so that xP ◦ xQ = xP ∪Q. We denote the complement of part
P as P = P \ P , which satisﬁes x = xP ◦ xP for all x ∈ X .
Each basic part p ∈ P is associated to a feature subset
Ip ⊆ [m], which contains all those features that depend on
p (and only those). In general, the sets Ip may overlap, but
we do require each basic part p to be associated to some
features that do not depend on any other basic part q, i.e. that
(cid:54)= ∅ for all p ∈ P. The features associated
Ip \
to a part P ⊆ P are deﬁned as (cid:83)
p∈P Ip. Since the union of
the basic parts makes up the full object, we also have that
(cid:83)

q(cid:54)=p Iq

(cid:16)(cid:83)

(cid:17)

p∈P Ip = [m].

GAI utility decomposition.
In the previous section we in-
troduced a decomposition of conﬁgurations into parts. In or-
der to elicit the user preferences via part-wise interaction,
which is our ultimate goal, we need to decompose the utility
function as well. Given a part P and its feature subset IP ,
let its partial utility be:

u[IP ](x) := (cid:104)wIP , φIP (x)(cid:105) = (cid:80)

i∈IP

wiφi(x)

(1)

If the basic parts have no shared features, the utility function
is additive: it is easy to verify that u(x) = (cid:80)
p∈P u[Ip](x).
In this case, each part can be managed independently of the
others, and the overall conﬁguration maximizing the utility
can be obtained by separately maximizing each partial utility
and combining the resulting part-wise conﬁgurations.

However, in many applications of interest the feature sub-
sets do overlap. In a travel plan, for instance, one can be
interested in alternating cultural and leisure activities in con-
secutive days, in order to make the experience more diverse
and enjoyable. In this case, the above decomposition does
not apply anymore as the basic parts may depend on each
other through the shared features. Nonetheless, it can be
shown that our utility function is generalized additive inde-
pendent (GAI) over the feature subsets Ip of the basic parts.
Formally, a utility u(x) is GAI if and only if, given n fea-
ture subsets I1, . . . , In ⊆ [m], it can be decomposed into n
independent subutilities (Braziunas and Boutilier 2005):

u(x) = uI1(x) + · · · + uIn (x)

(2)

where each subutility uIk can only depend on the features
in Ik (but does not need to depend on all of them). This de-
composition enables applying ideas from the GAI literature

Algorithm 1 An example of ordering selection procedure
using a GAI network (Gonzales and Perny 2004).
1: procedure SELECTORDERING(P)
2:
3:
4:
5:

Build a GAI network G from Ip, p ∈ P
Produce sequence p1, . . . , pn by sorting the nodes
in G in ascending order of node degree
return p1, . . . , pn

to produce a well-deﬁned part-wise elicitation protocol. In-
tuitively, we will assign features to subutilities so that when-
ever a feature is shared by multiple parts, only the subutility
corresponding to one of them will depend on that feature.

We will now construct a suitable decomposition of u(x)
into n independent subutilities. Fix some order of the basic
parts p1, . . . , pn, and let:

Jk := Ik \ ((cid:83)n

j=k+1 Ij)

(3)

for all k ∈ [n]. We deﬁne the subutilities as uIk (x) :=
u[Jk](x) for all k ∈ [n]. By summing up the subutilities for
all parts, we obtain a utility where each feature is computed
exactly once, thus recovering the full utility u(x):
k=1 uIk (x) = (cid:80)n
wiφi(x)

u(x) = (cid:80)n
= (cid:80)

k=1 u[Ik \ (cid:83)n

j=k+1 Ij](x)

i∈IP

The GAI decomposition allows to elicit each subutility
uIk separately. By doing so, however, we end up ignoring
some of the dependencies between parts, namely the fea-
tures in Ik \ Jk. This is the price to pay in order to achieve
decomposition and partwise elicitation, and it may lead to
suboptimal solutions if too many dependencies are ignored.
It is therefore important to minimize the broken dependen-
cies by an appropriate ordering of the parts. Going back to
the travel planning with diversifying features example, con-
sider a multi-day trip. Here the parts may refer to individual
days, and Ik includes all features of day k, including the
features relating it to the other days, e.g. the alternation of
cultural and leisure activities. Note that the Ik’s overlap. On
the other hand, the Jk’s are subset of features chosen so that
every feature only appears once. A diversifying feature re-
lating days 3 and 4 of the trip is either assigned to J3 or J4,
but not both.

One way to control the ignored dependencies is by lever-
aging GAI networks (Gonzales and Perny 2004). A GAI net-
work is a graph whose nodes represent the subsets Ik and
whose edges connect nodes sharing at least one feature. Al-
gorithm 1 presents a simple and effective solution to provide
an ordering. It builds a GAI network from P and sorts the
basic parts in ascending order of node degree (number of
incoming and outgoing edges). By ordering last the subsets
having intersections with many other parts, this ordering at-
tempts to minimize the lost dependencies in the above de-
composition (Eq. 3). This is one possible way to order the
parts, which we use as an example; more informed or task-
speciﬁc approaches could be devised.

The PCL algorithm. The pseudocode of our algorithm,
PCL, is listed in Algorithm 2. PCL starts off by sorting the

Algorithm 2 The PCL algorithm.
1: procedure PCL(P, T )
2:
3:
4:
5:
6:

p1, . . . , pn ← SELECTORDERING(P)
w1 ← 0, x1 ← initial conﬁguration
for t = 1, . . . , T do

pt ← SELECTPART(P)
xt
pt ← argmaxxpt ∈Xpt ut[J t](xpt ◦ xt
pt ← xt−1
xt
pt
User provides improvement ˆxt
pt ◦ xt

pt) − ut[I t](xt

pt of xt

pt ◦ xt

(cid:16)

7:

8:

9:

pt)

if
wt+1
wt+1

ut[I t](ˆxt
−Qt ← wt
Qt ← wt

10:

11:

−Qt
Qt + φQt(ˆxt

pt ◦ xt

pt) − φQt(xt

pt ◦ xt

pt))

pt, keeping xt
(cid:17)
pt) ≤ 0

pt ﬁxed
then Qt ← I t

else Qt ← J t ;

basic parts, producing an ordering p1, . . . , pn. Algorithm 1
could be employed or any other (e.g. task-speciﬁc) sorting
solution. Then it loops for T iterations, maintaining an esti-
mate wt of the user weights as well as a complete conﬁgu-
ration xt. The starting conﬁguration x1 should be a reason-
able initial guess, depending on the task. At each iteration
t ∈ [T ], the algorithm selects a part pt using the procedure
SELECTPART (see below). Then it updates the object xt by
inferring a new partial conﬁguration xt
pt while keeping the
rest of xt ﬁxed, that is xt
. The inferred partial con-
ﬁguration xt
pt is optimal with respect to the local subutility
I t(·) given xt−1
ut
. Note that inference is over the partial con-
pt
ﬁguration xt
pt only, and therefore can be exponentially faster
than inference over full conﬁgurations.

pt = xt−1
pt

Next, the algorithm presents the inferred partial conﬁgu-
ration xt
pt as well as some contextual information (see be-
low). The user is asked to produce an improved partial con-
ﬁguration ˆxt
pt according to the her own preferences, while
the rest of the object xt
pt is kept ﬁxed. We assume that a
user is satisﬁed with a partial conﬁguration xt
pt if she can-
not improve it further, or equivalently when the object xt is
conditionally optimal with respect to part pt given the rest of
the object xt
pt (the formal deﬁnition of conditional optimal-
ity is given in the Analysis section). When a user is satisﬁed
with a partial conﬁguration, she returns ˆxt
pt, thereby
implying no change in the weights wt+1.

pt = xt

pt ◦ xt

pt ◦ xt

After receiving an improvement, if the user is not satis-
ﬁed, the weights are updated through a perceptron step. The
subset Qt of weights that are actually updated depends on
pt ) − ut[I t](xt
whether ut[I t](ˆxt
pt) is negative or
(strictly) positive. Since we perform inference on ut[J t](·),
we have that ut[J t](xt
pt ) ≤ 0. The
user improvement can, however, potentially change all the
features in I t. Intuitively, the weights associated to a sub-
set of features should change only if the utility computed on
this subset ranks ˆxt
pt. The algorithm there-
fore checks whether ut[I t](xt
pt) ≤ ut[I t](xt
pt),
in which case the weights associated to the whole subset I t
should be updated. If this condition is not met, instead, the

pt lower than xt
pt ◦ xt

pt) − ut[J t](xt

pt ◦ xt

pt ◦ xt

pt ◦ xt

algorithm can only safely update the weights associated to
J t, which, as said, meet this condition by construction.

As for the SELECTPART procedure, we experimented
with several alternative implementations, including priori-
tizing parts with a large feature overlap (|Ik\Jk|) and bandit-
based strategies aimed at predicting a surrogate of the utility
gain (namely, a variant of the UCB1 algorithm (Auer, Cesa-
Bianchi, and Fischer 2002)). Preliminary experiments have
shown that informed strategies do not yield a signiﬁcant per-
formance improvement over the random selection stategy;
hence we stick with the latter in all our experiments.

The algorithm stops either when the maximum number
of iterations T is met or when a “local optimum” has been
found. For ease of exposition we left out the latter case from
Algorithm 2, but we explain what a local optimum is in the
following Analysis section; the stopping criterion will fol-
low directly from Proposition 1.

Interacting through parts.
In order for the user to judge
the quality of a suggested partial conﬁguration xp, some
contextual information may have to be provided. The rea-
son is that, if p depends on other parts via shared features,
these have to be somehow communicated to the user, other-
wise his/her improvement will not be sufﬁciently informed.
We distinguish two cases, depending on whether the fea-
tures of p are local or global. Local features only depend on
small, localized portions of x. This is for instance the case
for features that measure the diversity of consecutive activ-
ities in a touristic trip plan, which depend on consecutive
time slots or days only. Here the context amounts to those
other portions of x that share local features with p. For in-
stance, the user may interact over individual days only. If
the features are local, the context is simply the time slots be-
fore and after the selected day. The user is free to modify the
activities scheduled that day based on the context, which is
kept ﬁxed.

On the other hand, global features depend on all of x (or
large chunks of it). For instance, in house furnishing one
may have features that measure whether the total cost of the
furniture is within a given budget, or how much the cost sur-
passes the budget. A naive solution would be that of show-
ing the user the whole furniture arrangement x, which can

be troublesome when x is large. A better alternative is to
present the user a summary of the global features, in this case
the percentage of the used budget. Such a summary would
be sufﬁcient for producing an informed improvement, inde-
pendently from the actual size of x.

Of course, the best choice of context format is application
speciﬁc. We only note that, while crucial, the context only
provides auxiliary information to the user, and does not af-
fect the learning algorithm directly.

Analysis
In preference elicitation, it is common to measure the quality
of a recommended (full) conﬁguration in terms of the regret:

REG(x) := maxˆx∈X u∗(ˆx) − u∗(x) = (cid:104)w∗, φ(x∗) − φ(x)(cid:105)
where u∗(·) is the true, unobserved user utility and x∗ =
argmaxx∈X u∗(x) is a truly optimal conﬁguration. In PCL,
interaction with the user occurs via partial conﬁgurations,
namely xt
pt. Since the regret is deﬁned in terms of
complete conﬁgurations, it is difﬁcult to analyze it directly
based on information about the partial conﬁgurations alone,
making it hard to prove convergence to globally optimal rec-
ommendations.

pt and ˆxt

The aim of this analysis is, however, to show that our al-
gorithm converges to a locally optimal conﬁguration, which
is in line with guarantees offered by other Coactive Learn-
ing variants (Goetschalckx, Fern, and Tadepalli 2014); the
latter, however still rely on interaction via complete conﬁg-
urations. Here a conﬁguration x is a local optimum for u∗(·)
if no part-wise modiﬁcation can improve x with respect to
u∗(·). Formally, x is a local optimum for u∗(·) if and only
if:

∀ p ∈ P (cid:64) x(cid:48)

p ∈ Xp u∗(x(cid:48)

p ◦ xp) > u∗(xp ◦ xp)

(4)

To measure local quality of a conﬁguration x with respect to
a part p, we introduce the concept of conditional regret of
the partial conﬁguration xp given the rest of the object xp:

CREGp (x) := u∗(x∗

p ◦ xp) − u∗(xp ◦ xp)
p = argmaxˆxp∈Xp u∗(ˆxp ◦ xp). Notice that:

where x∗

CREGp (x) = u∗[Ip](x∗

p ◦ xp) − u∗[Ip](xp ◦ xp)

since u∗[−Ip](x∗

p ◦ xp) − u∗[−Ip](xp ◦ xp) = 0.

We say that a partial conﬁguration x is conditionally op-
timal with respect to part p if CREGp (x) = 0. The follow-
ing lemma gives sufﬁcient and necessary conditions for local
optimality of a conﬁguration x.
Lemma 1. A conﬁguration x is locally optimal with respect
to u∗(·) if and only if x is conditionally optimal for u∗(·)
with respect to all basic parts p ∈ P.

Proof. By contradiction. (i) Assume that x is locally optimal
but not conditionally optimal with respect to p ∈ P. Then
CREGp (x) > 0, and thus there exists a partial conﬁguration
x(cid:48)
p such that u∗(x(cid:48)
p ◦ x ¯p) > u∗(xp ◦ xx ¯p ). This violates
the local optimality of x (Eq. 4). (ii) Assume that all partial
conﬁgurations xp ∀p ∈ P are conditionally optimal but x

is not locally optimal. Then there exists a part q ∈ P and a
partial conﬁguration x(cid:48)
q ◦x¯q) > u∗(xq ◦x¯q).
q such that u∗(x(cid:48)
This in turn means that CREGq (x) > 0. This violates the
conditional optimality of x with respect to q.

The above lemma gives us a part-wise measurable crite-
rion to determine if a conﬁguration x is a local optimum
through the conditional regret of x for all the provided parts.
The rest of the analysis is devoted to derive an upper
bound on the conditional regret incurred by the algorithm
and to prove that PCL eventually reaches a local optimum.

In order to derive the bound, we rely on the concept of
α-informativeness from (Shivaswamy and Joachims 2015),
adapting it to part-wise interaction2. A user is conditionally
α-informative if, when presented with a partial conﬁgura-
pt, he/she provides a partial conﬁguration ˆxt
tion xt
pt that is
at least some fraction α ∈ (0, 1] better than xt
pt in terms of
conditional regret, or more formally:
pt ◦ xt
pt) − u∗[I t](xt

pt) − u∗[I t](xt
pt ◦ xt

pt ◦ xt
α (cid:0)u∗[I t](x∗

u∗[I t](ˆxt

pt ◦ xt

pt) ≥

pt)(cid:1)

(5)

In the rest of the paper we will use the notation u[I t](ˆxt)
pt ◦ xt
meaning u[I t](ˆxt
pt), i.e. drop the complement, when
no ambiguity can arise.

At all iterations t, the algorithm updates the weights spec-
iﬁed by Qt, producing a new estimate wt+1 of w∗. The ac-
tual indices Qt depend on the condition at line 9 of Algo-
rithm 2: at some iterations Qt includes all of I t, while at
others Qt is restricted to J t. We distinguish between these
cases by:

I = {t ∈ [T ] : ut[I t](ˆxt) − ut[I t](xt) ≤ 0}
J = {t ∈ [T ] : ut[I t](ˆxt) − ut[I t](xt) > 0}

so that if t ∈ I then Qt = I t, and Qt = J t if t ∈ J . For all
t ∈ [T ], the quality of wt+1 is:

(cid:104)w∗, wt+1(cid:105) = (cid:104)w∗
= (cid:104)w∗

Qt, wt+1
Qt (cid:105)
Qt, wt
Qt(cid:105)

−Qt(cid:105) + (cid:104)w∗
−Qt(cid:105) + (cid:104)w∗

−Qt, wt+1
−Qt, wt
+ (cid:104)w∗
= (cid:104)w∗, wt(cid:105) + (cid:104)w∗
= (cid:104)w∗, wt(cid:105) + u∗[Qt](ˆxt) − u∗[Qt](xt)

Qt, φQt(ˆxt) − φQt(xt)(cid:105)

Qt, φQt(ˆxt) − φQt(xt)(cid:105)

Therefore if the second summand in the last equation, the
utility gain u∗[Qt](ˆxt) − u∗[Qt](xt), is positive, the update
produces a better weight estimate wt+1.

Since the user is conditionally α-informative, the im-
provement ˆxt always satisﬁes u∗[I t](ˆxt) − u∗[I t](xt) ≥
0. When t ∈ I, we have Qt = I t, and thus the util-
ity gain is guaranteed to be positive. On the other hand,
when t ∈ J we have Qt = J t and the utility gain re-
duces to u∗[J t](ˆxt) − u∗[J t](xt). In this case the update

2Here we adopted the deﬁnition of strict α-informativeness
for simplicity. Our results can be directly extended to the more
general notions of informativeness described in (Shivaswamy and
Joachims 2015).

ignores the weights in I t \ J t, “missing out” a factor of
u∗[I t \ J t](ˆxt) − u∗[I t \ J t](xt).

We compactly quantify the missing utility gain as:

(cid:26)0

ζ t =

u∗[I t \ J t](ˆxt) − u∗[I t \ J t](xt)

if t ∈ I
if t ∈ J

Note that ζ t can be positive, null or negative for t ∈ J .
When ζ t is negative, making the update on J t only actually
avoids a loss in utility gain.

(cid:80)T

We now prove that PCL minimizes the average conditional
t=1 CREGpt (xt) as T → ∞ for

regret CREGT := 1
T
conditionally α-informative users.
Theorem 1. For a conditionally α-informative user, the av-
erage conditional regret of PCL after T iterations is upper
bounded by:

1
T

T
(cid:88)

t=1

CREGpt

(cid:0)xt(cid:1) ≤

2DS(cid:107)w∗(cid:107)
√

+

α

T

1
αT

T
(cid:88)

t=1

ζ t

Proof. The following is a sketch3. We start by splitting the
iterations into the I and J sets deﬁned above, and bound
the norm (cid:107)wT +1(cid:107)2. In both cases we ﬁnd that (cid:107)wT +1(cid:107)2 ≤
4D2S2T . We then expand the term (cid:104)w∗, wT +1(cid:105) for itera-
tions in both I and J , obtaining:

(cid:104)w∗, wT +1(cid:105) =

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105)

(cid:88)

t∈I

+

(cid:88)

t∈J

(cid:104)w∗

J t, φJ t(ˆxt) − φJ t (xt)(cid:105)

With few algebraic manipulations we obtain:

(cid:104)w∗, wT +1(cid:105)

=

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105) −

Which we then bound using the Cauchy-Schwarz inequality:

√

(cid:107)w∗(cid:107)

4D2S2T

≥ (cid:107)w∗(cid:107)(cid:107)wT +1(cid:107)
≥ (cid:104)w∗, wT +1(cid:105)

=

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105) −

T
(cid:88)

t=1

ζ t

T
(cid:88)

t=1

ζ t

T
(cid:88)

t=1

T
(cid:88)

t=1

Applying the conditional α-informative feedback (Eq. 5)
and rearranging proves the claim.

Theorem 1 ensures that the average conditional regret suf-
fered by our algorithm decreases as O(1/
T ). This alone,
however, does not prove that the algorithm will eventually

√

3The complete proof can be found in the Supplementary Mate-

rial.

t=τ0

arrive at a local optimum, even if (cid:80)T
CREGpt (xt) = 0,
for some τ0 ∈ [T ]. This is due to the fact that partial in-
ference is performed keeping the rest of the object xt
pt ﬁxed.
Between iterations an inferred part may change as a result of
a change of the other parts in previous iterations. The object
could, in principle, keep changing at every iterations, even
if CREGpt (xt) is always equal to 0. The next proposition,
however, shows that this is not the case thanks to the utility
decomposition we employ.
Proposition 1. Let τ1 < . . . < τn < ˆτ1 < . . . < ˆτn ≤ T such
that pτk = pˆτk = pk and CREGpt (xt) = 0 for all t ≥ τ1.
The conﬁguration xT is a local optimum.

= xτ1

p1 for all t > τ1, as ut
I1

Proof. Sketch. The proof procedes by strong induction. We
ﬁrst show that xt
(·)
p1
only depends on the features in J1 and by assumption
CREGpt (xt) = 0 for all t ≥ τ1. By strong induction, as-
suming that xt
pj for all j = 1, . . . , k − 1 and all
pj
= xτk
t > τk−1, we can easily show that xt
pk
pk for all t > τn, therefore xˆτk = xτn for
(cid:0)xˆτk (cid:1) = 0 for all k ∈ [n] by
all k ∈ [n]. Since CREGpk
assumption, then xT = xˆτn = xτn is a local optimum and
will not change for all t > τn.

pk as well.

Now, xt
pk

= xτj

= xτk

The algorithm actually reaches a local optimum at t =
τn, but it needs to double check all the parts in order to be
sure that the conﬁguration is actually a local optimum. This
justiﬁes a termination criterion that we use in practice: if
the algorithm completes two full runs over all the parts, and
the user can never improve any of the recommended partial
conﬁgurations, then the full conﬁguration is guaranteed to be
a local optimum, and the algorithm can stop. As mentioned,
we employ this criterion in our implementation but we left it
out from Algorithm 2 for simplicity.

To recap, Theorem 1 guarantees that CREGT → 0 as t →
∞, therefore CREGpt (xt) approaches 0 as well. Combining
this fact with Proposition 1, we proved that our algorithm
approaches a local optimum for T → ∞.

Empirical Analysis
We ran PCL on three constructive preference elicitation tasks
of increasing complexity, comparing different degrees of
user informativeness. According to our experiments, infor-
mativeness is the most critical factor. The three problems in-
volve rather large conﬁgurations, which can not be handled
by coactive interaction via complete conﬁgurations. For in-
stance, in (Ortega and Stocker 2016) the user is tasked to
solve relatively simple SAT instances over three variables
and (at most) eight clauses; in some cases users were ob-
served to show signs of cognitive overload. In comparison,
our simplest realistic problem involve 35 categorical vari-
ables (with 8 possible values) and 74 features, plus addi-
tional hard constraints. As a consequence, Coactive Learn-
ing can not be applied as-is, and part-wise interaction is nec-
essary.

In all of these settings, part-wise inference is cast
as a mixed integer linear problem (MILP), and solved

with Gecode4. Despite being NP-hard in general, MILP
solvers can be very efﬁcient on practical instances. Ef-
improved by inferring only partial
ﬁciency is further
conﬁgurations. Our experimental setup is available at
https://github.com/unitn-sml/pcl.

We employed a user simulation protocol similar to that
of (Teso, Dragone, and Passerini 2017). First, for each prob-
lem, we sampled 20 vectors w∗ at random from a standard
normal distribution. Then, upon receiving a recommenda-
tion xt
pt is generated by solving the
following problem:

pt, an improvement ˆxt

argmin
xpt ∈Xpt

u∗[I t](xpt ◦ xt

pt)

s.t. u∗[I t](xpt ◦ xt

pt) − u∗[I t](xt
pt ◦ xt

pt ◦ xt
pt) − u∗[I t](xt

pt)

≥ α(u∗[I t](x∗

pt ◦ xt

pt))

This
formulation clearly satisﬁes
informativeness assumption (Eq. 5).

the conditional α-

Synthetic setting. We designed a simple synthetic prob-
lem inspired by spin glass models, see Figure 1 for a depic-
tion. In this setting, a conﬁguration x consists of a 4 × 4
grid. Each node in the grid is a binary 0-1 variable. Adjacent
nodes are connected by an edge, and each edge is associ-
ated to an indicator feature that evaluates to 1 if the inci-
dent nodes have different values (green in the ﬁgure), and to
−1 otherwise (red in the ﬁgure). The utility of a conﬁgura-
tion is simply the weighted sum of the values of all features
(edges). The basic parts p consist of all the non-overlapping
2 × 2 sub-grids of x, for a total of 4 basic parts (indicated by
dotted lines in the ﬁgure).

Figure 1: Example grid conﬁguration.

Since the problem is small enough for inference of
complete conﬁgurations to be practical, we compared PCL
to standard Coactive Learning, using the implementation
of (Teso, Dragone, and Passerini 2017). In order to keep the
comparison as fair as possible, the improvements fed to CL
were chosen to match the utility gain obtained by PCL. We
further report the performance of three alternative part se-
lection strategies: random, smallest (most independent) part
ﬁrst, and UCB1.

4http://www.gecode.org/

The results can be found in the ﬁrst column of Figure 2.
We report both the regret (over complete conﬁgurations) and
the cumulative runtime of all algorithms, averaged over all
users, as well as their standard deviation. The regret plot
shows that, despite being restricted to work with 2 × 2 con-
ﬁgurations, PCL does recommend complete conﬁgurations
of quality comparable to CL after enough queries are made.
Out of the three part selection strategies, random performs
best, with the other two more informed alternatives (espe-
cially smallest ﬁrst) quite close. The runtime gap between
full and part-wise inference is already clear in this small
synthetic problem; complete inference quickly becomes im-
practical as the problem size increases.

Training planning. Generating personalized training
plans based on performance and health monitoring has re-
ceived a lot of attention recently in sport analytics (see
e.g. (Fister et al. 2015)). Here we consider the problem of
synthesizing a week-long training plan x from information
about the target athlete. Each day includes 5 time slots (two
for the morning, two for the afternoon, one for the evening),
for 35 slots total. We assume to be given a ﬁxed number of
training activities (7 in our experiments: walking, running,
swimming, weight lifting, pushups, squats, abs), as well as
knowledge of the slots in which the athlete is available. The
training plan x associates an activity to each slot where the
athlete is available. Our formulation tracks the amount of
improvement (e.g. power increase) and fatigue over ﬁve dif-
ferent body parts (arms, torso, back, legs, and heart) induced
by performing an activity for one time slot. Each day deﬁnes
a basic part.

The mapping between training activity and improve-
ment/fatigue over each body part is assumed to be provided
externally. It can be provided by the athlete or medical per-
sonnel monitoring his/her status. The features of x include,
for each body part, the total performance gain and fatigue,
computed over the recommended training plan according to
the aforementioned mapping. We further include inter-part
features to capture activity diversity in consecutive days. The
fatigue accumulated in 3 consecutive time slots in any body
parts does not exceed a given threshold, to prevent injuries.
In this setting, CL is impractical from both the cognitive
and computational points of view. We ran PCL and evaluated
the impact of user informativeness by progressively increas-
ing α from 0.1, to 0.3, to 0.5. The results can be seen in Fig-
ure 2. The plots show clearly that, despite the complexity of
the conﬁguration and constraints, PCL can still produce very
low-regret conﬁgurations after about 50 iterations or less.

Understandably, the degree of improvement α plays an
important role in the performance of PCL and, consequently,
in its runtime (users at convergence do not contribute to the
runtime), at least up to α = 0.5. Recall, however, that the
improvements are part-wise, and hence α quantiﬁes the de-
gree of local improvement: part improvements may be very
informative on their own, but only give a modest amount
of information about the full conﬁguration. However, it is
not unreasonable to expect that users to be very informative
when presented with reasonably sized (and simple) parts.

Figure 2: Regret over complete conﬁgurations (top) and cumulative runtime (bottom) of PCL and CL on our three constructive
problems: synthetic (left), training planning (middle), and hotel planning (right). The x-axis is the number of iterations, while
the shaded areas represent the standard deviation. Best viewed in color.

Crucially PCL allows the system designer to deﬁne the parts
appropriately depending on the application.

Hotel planning. Finally, we considered a complex furni-
ture allocation problem: furnishing an entire hotel. The prob-
lem is encoded as follows. The hotel is represented by a
graph: nodes are rooms and edges indicate which rooms are
adjacent. Rooms can be of three types: normal rooms, suites,
and dorms. Each room can hold a maximum number of fur-
niture pieces, each associated to a cost. Additional, ﬁxed
nodes represent bathrooms and bars. The type of a room is
decided dynamically based on its position and furniture. For
instance, a normal room must contain at most three single or
double beds, no bunk beds, and a table, and must be close
to a bathroom. A suite must contain one bed, a table and a
sofa, and must be close to a bathroom and a bar. Each room
is a basic part, and there are 15 rooms to be allocated.

The feature vector contains 20 global features plus 8 lo-
cal features per room. The global features include different
functions of the number of different types of rooms, the total
cost of the furniture and the total number of guests. The local
features include, instead, characteristics of the current room,
such as its type or the amount of furniture, and other fea-
tures shared by adjacent rooms, e.g. whether two rooms have
the same type. These can encode preferences like “suites
and dorms should not be too close”, or “the hotel should
maintain high quality standards while still being proﬁtable”.
Given the graph structure, room capacities, and total bud-
get, the goal is to furnish all rooms according to the user’s
preferences.

This problem is hard to solve to optimality with current

solvers; part-based inference alleviates this issue by focus-
ing on individual rooms. There are 15 rooms in the hotel,
so that at each iteration only 1/15 of the conﬁguration is af-
fected. Furthermore, the presence of the global features im-
plies dependences between all rooms. Nonetheless, the al-
gorithm manages to reduce the regret by an order of magni-
tude in around a 100 iterations, starting from a completely
uninformed prior. Note also that as for the training planning
scenario, an alpha of 0.3 achieves basically the same results
as those for alpha equal to 0.5.

Conclusion

In this work we presented an approach to constructive pref-
erence elicitation able to tackle large constructive domains,
beyond the reach of previous approaches. It is based on
Coactive Learning (Shivaswamy and Joachims 2015), but
only requires inference of partial conﬁgurations and partial
improvement feedback, thereby signiﬁcantly reducing the
cognitive load of the user. We presented an extensive the-
oretical analysis demonstrating that, despite working only
with partial conﬁgurations, the algorithm converges to a lo-
cally optimal solution. The algorithm has been evaluated
empirically on three constructive scenarios of increasing
complexity, and shown to perform well in practice.

Possible future work includes improving part-based in-
teraction by exchanging additional contextual information
(e.g. features (Teso, Dragone, and Passerini 2017) or expla-
nations) with the user, and applying PCL to large layout syn-
thesis problems (Dragone et al. 2016).

ings of the 18th ACM SIGKDD international conference on
Knowledge discovery and data mining, 705–713. ACM.
Shivaswamy, P., and Joachims, T. 2015. Coactive learning.
JAIR 53:1–40.
Teso, S.; Dragone, P.; and Passerini, A. 2017. Coactive
critiquing: Elicitation of preferences and features. In AAAI.
Teso, S.; Passerini, A.; and Viappiani, P. 2016. Constructive
preference elicitation by setwise max-margin learning.
In
Proceedings of the Twenty-Fifth International Joint Confer-
ence on Artiﬁcial Intelligence, 2067–2073. AAAI Press.

References
Auer, P.; Cesa-Bianchi, N.; and Fischer, P. 2002. Finite-
time analysis of the multiarmed bandit problem. Machine
learning 47(2-3):235–256.
Boutilier, C.; Bacchus, F.; and Brafman, R. I. 2001. Ucp-
networks: A directed graphical representation of conditional
In Proceedings of the Seventeenth conference on
utilities.
Uncertainty in artiﬁcial intelligence, 56–64. Morgan Kauf-
mann Publishers Inc.
Boutilier, C.; Patrascu, R.; Poupart, P.; and Schuurmans, D.
2006. Constraint-based optimization and utility elicitation
using the minimax decision criterion. Artiﬁcial Intelligence
170(8-9):686–713.
Braziunas, D., and Boutilier, C. 2005. Local utility elici-
In Proceedings of the Twenty-First
tation in GAI models.
Conference on Uncertainty in Artiﬁcial Intelligence, 42–49.
AUAI Press.
Braziunas, D., and Boutilier, C. 2007. Minimax regret based
elicitation of generalized additive utilities. In UAI, 25–32.
Braziunas, D., and Boutilier, C. 2009. Elicitation of factored
utilities. AI Magazine 29(4):79.
Chajewska, U.; Koller, D.; and Parr, R. 2000. Making ratio-
nal decisions using adaptive utility elicitation. In AAAI/IAAI,
363–369.
Dragone, P.; Erculiani, L.; Chietera, M. T.; Teso, S.; and
Passerini, A. 2016. Constructive layout synthesis via coac-
tive learning. In Constructive Machine Learning workshop,
NIPS.
Fishburn, P. C. 1967.
Interdependence and additivity in
multivariate, unidimensional expected utility theory. Inter-
national Economic Review 8(3):335–342.
Fister, I.; Rauter, S.; Yang, X.-S.; and Ljubiˇc, K. 2015. Plan-
ning the sports training sessions with the bat algorithm. Neu-
rocomputing 149:993–1002.
Goetschalckx, R.; Fern, A.; and Tadepalli, P. 2014. Coactive
learning for locally optimal problem solving. In Proceedings
of AAAI.
Gonzales, C., and Perny, P. 2004. GAI networks for utility
elicitation. KR 4:224–234.
Keeney, R. L., and Raiffa, H. 1976. Decisions with Multiple
Objectives: Preferences and Value Tradeoffs.
Mayer, R. E., and Moreno, R. 2003. Nine ways to reduce
cognitive load in multimedia learning. Educational psychol-
ogist 38(1):43–52.
Meseguer, P.; Rossi, F.; and Schiex, T. 2006. Soft con-
straints. Foundations of Artiﬁcial Intelligence 2:281–328.
Ortega, P. A., and Stocker, A. A. 2016. Human decision-
making under limited time. In Advances in Neural Informa-
tion Processing Systems, 100–108.
Pigozzi, G.; Tsouki`as, A.; and Viappiani, P. 2016. Prefer-
ences in artiﬁcial intelligence. Ann. Math. Artif. Intell. 77(3-
4):361–401.
Raman, K.; Shivaswamy, P.; and Joachims, T. 2012. Online
In Proceed-
learning to diversify from implicit feedback.

Supplementary Material

We add and subtract the term:

Proof of Theorem 2
We begin by expanding (cid:107)wT +1(cid:107)2. If T ∈ I:

(cid:107)wT +1(cid:107)2

= (cid:107)wT +1
= (cid:107)wT
= (cid:107)wT

−I T (cid:107)2 + (cid:107)wT +1
−I T (cid:107)2 + (cid:107)wT
−I T (cid:107)2 + (cid:107)wT
+ 2(cid:104)wT

I T (cid:107)2
I T + φI T (ˆxt) − φI T (xt)(cid:107)2
I T (cid:107)2 + (cid:107)φI T (ˆxt) − φI T (xt)(cid:107)2

I T , φI T (ˆxt) − φI T (xt)(cid:105)

Since T ∈ I, uT [I T ](ˆxT ) − uT [I T ](xT ) ≤ 0, thus:

I T (cid:107)2 + (cid:107)φI T (ˆxt) − φI T (xt)(cid:107)2

∞|I T |2

(cid:107)wT +1(cid:107)2

−I T (cid:107)2 + (cid:107)wT
≤ (cid:107)wT
≤ (cid:107)wT (cid:107)2 + 4D2S2

If instead T ∈ J :

(cid:107)wT +1(cid:107)2

= (cid:107)wT +1
= (cid:107)wT
= (cid:107)wT
= (cid:107)wT

−J T (cid:107)2 + (cid:107)wT +1
−J T (cid:107)2 + (cid:107)wT +1
−J T (cid:107)2 + (cid:107)wT
−J T (cid:107)2 + (cid:107)wT
+ 2(cid:104)wT

J T (cid:107)2
J T (cid:107)2
J T + φJ T (ˆxt) − φJ T (xt)(cid:107)2
J T (cid:107)2 + (cid:107)φJ T (ˆxt) − φJ T (xt)(cid:107)2

J T , φJ T (ˆxt) − φJ T (xt)(cid:105)

≤ (cid:107)wT (cid:107)2 + (cid:107)φJ T (ˆxt) − φJ T (xt)(cid:107)2
≤ (cid:107)wT (cid:107)2 + 4D2|J T |2
≤ (cid:107)wT (cid:107)2 + 4D2S2

∞|J T |2

We can therefore expand the term:

(cid:107)wT +1(cid:107)2 ≤ 4D2S2T

Applying Cauchy-Schwarz inequality:

(cid:104)w∗, wT +1(cid:105) ≤ (cid:107)w∗(cid:107)(cid:107)wT +1(cid:107)

≤ 2DS(cid:107)w∗(cid:107)

T

√

The LHS of the above inequality expands to:

(cid:104)w∗, wT +1(cid:105) = (cid:104)w∗

−I T (cid:105) + (cid:104)w∗

I T , wT

I T (cid:105)

−I T , wT
+ (cid:104)w∗

I T , φI T (ˆxT ) − φI T (xT )(cid:105)

[ if T ∈ I ]
(cid:104)w∗, wT +1(cid:105) = (cid:104)w∗

−J T , wT
+ (cid:104)w∗

−J T (cid:105) + (cid:104)w∗

J T , wT

J T (cid:105)

J T , φJ T (ˆxT ) − φJ T (xT )(cid:105)

[ if T ∈ J ]

And thus:

(cid:104)w∗, wT +1(cid:105) =

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105)

(cid:88)

t∈I

+

(cid:88)

t∈J

(cid:104)w∗

J t , φJ t(ˆxt) − φJ t(xt)(cid:105)

(cid:104)w∗

I t\J t, φI t\J t(ˆxt) − φI t\J t(xt)(cid:105)

(cid:88)

t∈J

We obtain:

(cid:104)w∗, wT +1(cid:105)

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

=

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105)

−

(cid:88)

t∈J

(cid:104)w∗

I t\J t, φI t\J t(ˆxt) − φJ t(xt)(cid:105)

=

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105) −

≥ α

(cid:104)w∗

I t, φI t(x∗

pt ◦ xt

¯pt) − φI t(xt)(cid:105) −

T
(cid:88)

t=1

ζ t

T
(cid:88)

t=1

ζ t

And thus:

T
(cid:88)

t=1

(cid:104)w∗

I t, φI t(x∗) − φI t(xt)(cid:105) ≤

√

2DS(cid:107)w∗(cid:107)

T

α

+

1
α

T
(cid:88)

t=1

ζ t

Dividing both sides by T proves the claim.

Proof of Proposition 3
We ﬁrst show that xt
pk
iteration t the algorithm produces xt
local subutility:

= xτk

pk for t > τk. Recall that at
pk by maximizing the

ut
Ik

(x) = ut[Jk](x) = ut[Ik \ ((cid:83)n

j=k+1 Ij)](x)

j=k+1 Ij. Each subutility
(·) is only affected by the changes in xp1 , . . . , xpk−1, as

i.e. ignoring all the features in (cid:83)n
ut
Ik
their feature subsets may intersect with Ik.

= wτ1

The subutility ut
I1

(·) only depends on the features that
are exclusively included in I1, so it is not affected by the
changes in all the other parts. At iteration τ1, the condi-
tional regret CREGp1 (xτ1 ) = 0, and thus ˆxτ1
p1 and
p1
wτ1+1
(·)
I t
will not have changed as it only depends on features exclu-
sively included in I1 and thus xt
= xτ1
p1 . Since, by assump-
p1
tion, CREGpt (xt) = 0 for all t ≥ τ1, uτ1
(·) will not change
I1
anymore and so xt
p1

= xτ1
I t . If p1 gets selected again at t > τ1, ut
I1

= xτ1
p1.
By strong induction, suppose that all parts xt
pj

pj for
all j = 1, . . . k − 1 and for all t > τk−1. At iteration τk,
CREGpk (xτk ) = 0, thus ˆxτk
pk and so the weights
pk
wτk+1
will not change. For the following t > τk iterations,
Ik
even if pk gets selected, since the subutility ut
(·) is only
pk
affected by the changes in xp1 , . . . , xpk−1 and those parts do
not change (by inductive assumption), we can conclude that
ut
(·) = uτk
(·) and that xt
pk
pk
pk
This proves that, for any k ∈ [n] and any t > τk, xt
=
pk
xτk
pk . This also means that none of the partial conﬁgurations

= xτk
pk .

= xτj

= xτk

xt
pt will change for t > τn. Since none of the partial conﬁg-
(cid:0)xˆτ1 (cid:1) = 0 by
uration changed, xˆτ1 = xτn . Since CREGp1
assumption, then xˆτ1 is conditionally optimal with respect
(cid:0)xˆτk (cid:1) = 0 for all
to p1. Likewise, xˆτk = xτn and CREGpk
k ∈ [n], and thus xT = xˆτn = xτn is a local optimum.

8
1
0
2
 
y
a
M
 
6
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
7
4
2
8
0
.
1
1
7
1
:
v
i
X
r
a

Decomposition Strategies for Constructive Preference Elicitation
(and Supplementary Material)

Paolo Dragone∗
University of Trento, Italy
TIM-SKIL, Trento, Italy
paolo.dragone@unitn.it

Stefano Teso
KU Leuven, Belgium
stefano.teso@cs.kuleuven.be

Mohit Kumar †
KU Leuven, Belgium
mohit.kumar@cs.kuleuven.be

Andrea Passerini
University of Trento, Italy
andrea.passerini@unitn.it

Abstract

We tackle the problem of constructive preference elicitation,
that is the problem of learning user preferences over very
large decision problems, involving a combinatorial space of
possible outcomes. In this setting, the suggested conﬁgura-
tion is synthesized on-the-ﬂy by solving a constrained op-
timization problem, while the preferences are learned itera-
tively by interacting with the user. Previous work has shown
that Coactive Learning is a suitable method for learning user
preferences in constructive scenarios. In Coactive Learning
the user provides feedback to the algorithm in the form of an
improvement to a suggested conﬁguration. When the problem
involves many decision variables and constraints, this type of
interaction poses a signiﬁcant cognitive burden on the user.
We propose a decomposition technique for large preference-
based decision problems relying exclusively on inference and
feedback over partial conﬁgurations. This has the clear ad-
vantage of drastically reducing the user cognitive load. Addi-
tionally, part-wise inference can be (up to exponentially) less
computationally demanding than inference over full conﬁg-
urations. We discuss the theoretical implications of working
with parts and present promising empirical results on one syn-
thetic and two realistic constructive problems.

Introduction
In constructive preference elicitation (CPE) the recom-
mender aims at suggesting a custom or novel product to a
customer (Teso, Passerini, and Viappiani 2016). The prod-
uct is assembled on-the-ﬂy from components or synthe-
sized anew by solving a combinatorial optimization prob-
lem. The suggested products should of course satisfy the
customer’s preferences, which however are unobserved and
must be learned interactively (Pigozzi, Tsouki`as, and Vi-
appiani 2016). Learning proceeds iteratively: the learner
presents one or more candidate recommendations to the cus-
tomer, and employs the obtained feedback to estimate the

∗PD is a fellow of TIM-SKIL Trento and is supported by a
TIM scholarship. This work has received funding from the Euro-
pean Research Council (ERC) under the European Unions Horizon
2020 research and innovation programme (grant agreement No.
[694980] SYNTH: Synthesising Inductive Data Models).
Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

customer’s preferences. Applications include recommend-
ing custom PCs or cars, suggesting touristic travel plans,
designing room and building layouts, and producing recipe
modiﬁcations, among others.

A major weakness of existing CPE methods (Teso,
Passerini, and Viappiani 2016; Teso, Dragone, and Passerini
2017) is that they require the user to provide feedback on
complete conﬁgurations. In real-world constructive prob-
lems such as trip planning and layout design, conﬁgura-
tions can be large and complex. When asked to evaluate or
manipulate a complex product, the user may become over-
whelmed and confused, compromising the reliability of the
obtained feedback (Mayer and Moreno 2003). Human deci-
sion makers can revert to a potentially uninformative prior
when problem solving exceeds their available resources.
This effect was observed in users tasked with solving sim-
ple SAT instances (three variables and eight clauses) (Or-
tega and Stocker 2016). In comparison, even simple con-
structive problems can involve tens of categorical variables
and features, in addition to hard feasibility constraints. On
the computational side, working with complete conﬁgura-
tions poses scalability problems as well. The reason is that,
in order to select recommendations and queries, constructive
recommenders employ constraint optimization techniques.
Clearly, optimization of complete conﬁgurations in large
constructive problems can become computationally imprac-
tical as the problem size increases.

Here we propose to exploit factorized utility func-
tions (Braziunas and Boutilier 2009), which occur very natu-
rally in constructive problems, to work with partial conﬁgu-
rations. In particular, we show how to generalize Coactive
Learning (CL) (Shivaswamy and Joachims 2015) to part-
wise inference and learning. CL is a simple, theoretically
grounded algorithm for online learning and preference elic-
itation. It employs a very natural interaction protocol: at
each iteration the user is presented with a single, appropri-
ately chosen candidate conﬁguration and asked to improve
it (even slightly). In (Teso, Dragone, and Passerini 2017), it
was shown that CL can be lifted to constructive problems
by combining it with a constraint optimization solver to ef-
ﬁciently select the candidate recommendation. Notably, the
theoretical guarantees of CL remain intact in the construc-

tive case.

Our part-wise generalization of CL, dubbed PCL, solves
the two aforementioned problems in one go: (i) by present-
ing the user with partial conﬁgurations, it is possible to (sub-
stantially) lessen her cognitive load, improving the reliabil-
ity of the feedback and enabling learning in larger construc-
tive tasks; (ii) in combinatorial constructive problems, per-
forming inference on partial conﬁgurations can be exponen-
tially faster than on complete ones. Further, despite being
limited to working with partial conﬁgurations, PCL can be
shown to still provide local optimality guarantees in theory,
and to perform well in practice.

This paper is structured as follows. In the next section
we overview the relevant literature. We present PCL in the
Method section, followed by a theoretical analysis. The per-
formance of PCL are then illustrated empirically on one syn-
thetic and two realistic constructive problems. We close the
paper with some concluding remarks.

Related Work
Generalized additive independent (GAI) utilities have been
thoroughly explored in the decision making literature (Fish-
burn 1967). They deﬁne a clear factorization mechanism,
and offer a good trade off between expressiveness and ease
of elicitation (Chajewska, Koller, and Parr 2000; Gonzales
and Perny 2004; Braziunas and Boutilier 2009). Most of the
early work on GAI utility elicitation is based on graphical
models, e.g. UCP and GAI networks (Gonzales and Perny
2004; Boutilier, Bacchus, and Brafman 2001). These ap-
proaches aim at eliciting the full utility function and rely
on the comparison of full outcomes. Both of these are infea-
sible when the utility involves many attributes and features,
as in realistic constructive problems.

Like our method, more recent alternatives (Braziunas and
Boutilier 2005; 2007) handle both partial elicitation, i.e. the
ability of providing recommendations without full utility in-
formation, and local queries, i.e. elicitation of preference
information by comparing only (mostly) partial outcomes.
There exist both Bayesian (Braziunas and Boutilier 2005)
and regret-based (Braziunas and Boutilier 2007; Boutilier
et al. 2006) approaches, which have different shortcomings.
Bayesian methods do not scale to even small size construc-
tive problems (Teso, Passerini, and Viappiani 2016), such as
those occurring when reasoning over individual parts in con-
structive settings. On the other hand, regret-based methods
require the user feedback to be strictly self-consistent, an
unrealistic assumption when interacting with non-experts.
Our approach instead is speciﬁcally designed to scale to
larger constructive problems and, being derived from Coac-
tive Learning, natively handles inconsistent feedback. Cru-
cially, unlike PCL, these local elicitation methods also re-
quire to perform a number of queries over complete con-
ﬁgurations to calibrate the learned utility function. In larger
constructive domains this is both impractical (on the user
side) and computationally infeasible (on the learner side).

Our work is based on Coactive Learning (CL) (Shiv-
aswamy and Joachims 2015), a framework for learning util-
ity functions over structured domains, which has been suc-
cessfully applied to CPE (Teso, Dragone, and Passerini

2017; Dragone et al. 2016). When applied to constructive
problems, a crucial limitation of CL is that the learner and
the user interact by exchanging complete conﬁgurations.
Alas, inferring a full conﬁguration in a constructive prob-
lem can be computationally demanding, thus preventing the
elicitation procedure from being real-time. This can be par-
tially addressed by performing approximate inference, as
in (Raman, Shivaswamy, and Joachims 2012), at the cost of
weaker learning guarantees. A different approach has been
taken in (Goetschalckx, Fern, and Tadepalli 2014), where
the exchanged (complete) conﬁgurations are only required
to be locally optimal, for improved efﬁciency. Like PCL, this
method guarantees the local optimality of the recommended
conﬁguration. All of the previous approaches, however, re-
quire the user to improve a potentially large complete con-
ﬁguration. This is a cognitively demanding task which can
become prohibitive in large constructive problems, even for
domain experts, thus hindering feedback quality and effec-
tive elicitation. By dealing with parts only, PCL avoids this
issue entirely.

Method
Notation. We use rather standard notation: scalars x are
written in italics and column vectors x in bold. The inner
product of two vectors is indicated as (cid:104)a, b(cid:105) = (cid:80)
i aibi, the
Euclidean norm as (cid:107)a(cid:107) = (cid:112)(cid:104)a, a(cid:105) and the max-norm as
(cid:107)a(cid:107)∞ = maxi ai. We abbreviate {1, . . . , n} to [n], and in-
dicate the complement of I ⊆ [n] as −I := [n] \ I.

Setting. Let X be the set of candidate structured products.
Contrarily to what happens in standard preference elicita-
tion, in the constructive case X is deﬁned by a set of hard
constraints rather than explicitly enumerated1. Products are
represented by a function φ(·) that maps them to an m-
dimensional feature space. While the feature map can be
arbitrary, in practice we will stick to features that can be en-
coded as constraints in a mixed-integer linear programming
problem, for efﬁciency; see the Empirical Analysis section
for details. We only assume that the features are bounded,
i.e. ∀x ∈ X it holds that (cid:107)φ(x)(cid:107)∞ ≤ D for some ﬁxed D.

As is common in multi-attribute decision theory (Keeney
and Raiffa 1976), we assume the desirability of a product
x to be given by a utility function u∗ : X → R that is
linear in the features, i.e., u∗(x) := (cid:104)w∗, φ(x)(cid:105). Here the
weights w∗ ∈ Rm encode the true user preferences, and
may be positive, negative, or zero (which means that the cor-
responding feature is irrelevant for the user). Utilities of this
kind can naturally express rich constructive problems (Teso,
Passerini, and Viappiani 2016; Teso, Dragone, and Passerini
2017).

Parts. Here we formalize what parts and partial conﬁgu-
rations are, and how they can be manipulated. We assume to

1In this paper, “hard” constraints refer to the constraints de-
limiting the space of feasible conﬁgurations, as opposed to “soft”
constraints, which determine preferences over feasible conﬁgura-
tions (Meseguer, Rossi, and Schiex 2006).

be given a set of n basic parts p ∈ P. A part is any subset
P ⊆ P of the set of basic parts. Given a part P and an object
x, xP ∈ XP indicates the partial conﬁguration correspond-
ing to P . We require that the union of the basic parts recon-
structs the whole object, i.e. xP = x for all x ∈ X . The
proper semantics of the decomposition into basic parts is
task-speciﬁc. For instance, in a scheduling problem a month
may be decomposed into days, while in interior design a
house may be decomposed into rooms. Analogously, the
non-basic parts could then be weeks or ﬂoors, respectively.
In general, any combination of basic parts is allowed. We
capture the notion of combination of partial conﬁgurations
with the part combination operator ◦ : XP × XQ → XP ∪Q,
so that xP ◦ xQ = xP ∪Q. We denote the complement of part
P as P = P \ P , which satisﬁes x = xP ◦ xP for all x ∈ X .
Each basic part p ∈ P is associated to a feature subset
Ip ⊆ [m], which contains all those features that depend on
p (and only those). In general, the sets Ip may overlap, but
we do require each basic part p to be associated to some
features that do not depend on any other basic part q, i.e. that
(cid:54)= ∅ for all p ∈ P. The features associated
Ip \
to a part P ⊆ P are deﬁned as (cid:83)
p∈P Ip. Since the union of
the basic parts makes up the full object, we also have that
(cid:83)

q(cid:54)=p Iq

(cid:16)(cid:83)

(cid:17)

p∈P Ip = [m].

GAI utility decomposition.
In the previous section we in-
troduced a decomposition of conﬁgurations into parts. In or-
der to elicit the user preferences via part-wise interaction,
which is our ultimate goal, we need to decompose the utility
function as well. Given a part P and its feature subset IP ,
let its partial utility be:

u[IP ](x) := (cid:104)wIP , φIP (x)(cid:105) = (cid:80)

i∈IP

wiφi(x)

(1)

If the basic parts have no shared features, the utility function
is additive: it is easy to verify that u(x) = (cid:80)
p∈P u[Ip](x).
In this case, each part can be managed independently of the
others, and the overall conﬁguration maximizing the utility
can be obtained by separately maximizing each partial utility
and combining the resulting part-wise conﬁgurations.

However, in many applications of interest the feature sub-
sets do overlap. In a travel plan, for instance, one can be
interested in alternating cultural and leisure activities in con-
secutive days, in order to make the experience more diverse
and enjoyable. In this case, the above decomposition does
not apply anymore as the basic parts may depend on each
other through the shared features. Nonetheless, it can be
shown that our utility function is generalized additive inde-
pendent (GAI) over the feature subsets Ip of the basic parts.
Formally, a utility u(x) is GAI if and only if, given n fea-
ture subsets I1, . . . , In ⊆ [m], it can be decomposed into n
independent subutilities (Braziunas and Boutilier 2005):

u(x) = uI1(x) + · · · + uIn (x)

(2)

where each subutility uIk can only depend on the features
in Ik (but does not need to depend on all of them). This de-
composition enables applying ideas from the GAI literature

Algorithm 1 An example of ordering selection procedure
using a GAI network (Gonzales and Perny 2004).
1: procedure SELECTORDERING(P)
2:
3:
4:
5:

Build a GAI network G from Ip, p ∈ P
Produce sequence p1, . . . , pn by sorting the nodes
in G in ascending order of node degree
return p1, . . . , pn

to produce a well-deﬁned part-wise elicitation protocol. In-
tuitively, we will assign features to subutilities so that when-
ever a feature is shared by multiple parts, only the subutility
corresponding to one of them will depend on that feature.

We will now construct a suitable decomposition of u(x)
into n independent subutilities. Fix some order of the basic
parts p1, . . . , pn, and let:

Jk := Ik \ ((cid:83)n

j=k+1 Ij)

(3)

for all k ∈ [n]. We deﬁne the subutilities as uIk (x) :=
u[Jk](x) for all k ∈ [n]. By summing up the subutilities for
all parts, we obtain a utility where each feature is computed
exactly once, thus recovering the full utility u(x):
k=1 uIk (x) = (cid:80)n
wiφi(x)

u(x) = (cid:80)n
= (cid:80)

k=1 u[Ik \ (cid:83)n

j=k+1 Ij](x)

i∈IP

The GAI decomposition allows to elicit each subutility
uIk separately. By doing so, however, we end up ignoring
some of the dependencies between parts, namely the fea-
tures in Ik \ Jk. This is the price to pay in order to achieve
decomposition and partwise elicitation, and it may lead to
suboptimal solutions if too many dependencies are ignored.
It is therefore important to minimize the broken dependen-
cies by an appropriate ordering of the parts. Going back to
the travel planning with diversifying features example, con-
sider a multi-day trip. Here the parts may refer to individual
days, and Ik includes all features of day k, including the
features relating it to the other days, e.g. the alternation of
cultural and leisure activities. Note that the Ik’s overlap. On
the other hand, the Jk’s are subset of features chosen so that
every feature only appears once. A diversifying feature re-
lating days 3 and 4 of the trip is either assigned to J3 or J4,
but not both.

One way to control the ignored dependencies is by lever-
aging GAI networks (Gonzales and Perny 2004). A GAI net-
work is a graph whose nodes represent the subsets Ik and
whose edges connect nodes sharing at least one feature. Al-
gorithm 1 presents a simple and effective solution to provide
an ordering. It builds a GAI network from P and sorts the
basic parts in ascending order of node degree (number of
incoming and outgoing edges). By ordering last the subsets
having intersections with many other parts, this ordering at-
tempts to minimize the lost dependencies in the above de-
composition (Eq. 3). This is one possible way to order the
parts, which we use as an example; more informed or task-
speciﬁc approaches could be devised.

The PCL algorithm. The pseudocode of our algorithm,
PCL, is listed in Algorithm 2. PCL starts off by sorting the

Algorithm 2 The PCL algorithm.
1: procedure PCL(P, T )
2:
3:
4:
5:
6:

p1, . . . , pn ← SELECTORDERING(P)
w1 ← 0, x1 ← initial conﬁguration
for t = 1, . . . , T do

pt ← SELECTPART(P)
xt
pt ← argmaxxpt ∈Xpt ut[J t](xpt ◦ xt
pt ← xt−1
xt
pt
User provides improvement ˆxt
pt ◦ xt

pt) − ut[I t](xt

pt of xt

pt ◦ xt

(cid:16)

7:

8:

9:

pt)

if
wt+1
wt+1

ut[I t](ˆxt
−Qt ← wt
Qt ← wt

10:

11:

−Qt
Qt + φQt(ˆxt

pt ◦ xt

pt) − φQt(xt

pt ◦ xt

pt))

pt, keeping xt
(cid:17)
pt) ≤ 0

pt ﬁxed
then Qt ← I t

else Qt ← J t ;

basic parts, producing an ordering p1, . . . , pn. Algorithm 1
could be employed or any other (e.g. task-speciﬁc) sorting
solution. Then it loops for T iterations, maintaining an esti-
mate wt of the user weights as well as a complete conﬁgu-
ration xt. The starting conﬁguration x1 should be a reason-
able initial guess, depending on the task. At each iteration
t ∈ [T ], the algorithm selects a part pt using the procedure
SELECTPART (see below). Then it updates the object xt by
inferring a new partial conﬁguration xt
pt while keeping the
rest of xt ﬁxed, that is xt
. The inferred partial con-
ﬁguration xt
pt is optimal with respect to the local subutility
I t(·) given xt−1
ut
. Note that inference is over the partial con-
pt
ﬁguration xt
pt only, and therefore can be exponentially faster
than inference over full conﬁgurations.

pt = xt−1
pt

Next, the algorithm presents the inferred partial conﬁgu-
ration xt
pt as well as some contextual information (see be-
low). The user is asked to produce an improved partial con-
ﬁguration ˆxt
pt according to the her own preferences, while
the rest of the object xt
pt is kept ﬁxed. We assume that a
user is satisﬁed with a partial conﬁguration xt
pt if she can-
not improve it further, or equivalently when the object xt is
conditionally optimal with respect to part pt given the rest of
the object xt
pt (the formal deﬁnition of conditional optimal-
ity is given in the Analysis section). When a user is satisﬁed
with a partial conﬁguration, she returns ˆxt
pt, thereby
implying no change in the weights wt+1.

pt = xt

pt ◦ xt

pt ◦ xt

After receiving an improvement, if the user is not satis-
ﬁed, the weights are updated through a perceptron step. The
subset Qt of weights that are actually updated depends on
pt ) − ut[I t](xt
whether ut[I t](ˆxt
pt) is negative or
(strictly) positive. Since we perform inference on ut[J t](·),
we have that ut[J t](xt
pt ) ≤ 0. The
user improvement can, however, potentially change all the
features in I t. Intuitively, the weights associated to a sub-
set of features should change only if the utility computed on
this subset ranks ˆxt
pt. The algorithm there-
fore checks whether ut[I t](xt
pt) ≤ ut[I t](xt
pt),
in which case the weights associated to the whole subset I t
should be updated. If this condition is not met, instead, the

pt lower than xt
pt ◦ xt

pt) − ut[J t](xt

pt ◦ xt

pt ◦ xt

pt ◦ xt

algorithm can only safely update the weights associated to
J t, which, as said, meet this condition by construction.

As for the SELECTPART procedure, we experimented
with several alternative implementations, including priori-
tizing parts with a large feature overlap (|Ik\Jk|) and bandit-
based strategies aimed at predicting a surrogate of the utility
gain (namely, a variant of the UCB1 algorithm (Auer, Cesa-
Bianchi, and Fischer 2002)). Preliminary experiments have
shown that informed strategies do not yield a signiﬁcant per-
formance improvement over the random selection stategy;
hence we stick with the latter in all our experiments.

The algorithm stops either when the maximum number
of iterations T is met or when a “local optimum” has been
found. For ease of exposition we left out the latter case from
Algorithm 2, but we explain what a local optimum is in the
following Analysis section; the stopping criterion will fol-
low directly from Proposition 1.

Interacting through parts.
In order for the user to judge
the quality of a suggested partial conﬁguration xp, some
contextual information may have to be provided. The rea-
son is that, if p depends on other parts via shared features,
these have to be somehow communicated to the user, other-
wise his/her improvement will not be sufﬁciently informed.
We distinguish two cases, depending on whether the fea-
tures of p are local or global. Local features only depend on
small, localized portions of x. This is for instance the case
for features that measure the diversity of consecutive activ-
ities in a touristic trip plan, which depend on consecutive
time slots or days only. Here the context amounts to those
other portions of x that share local features with p. For in-
stance, the user may interact over individual days only. If
the features are local, the context is simply the time slots be-
fore and after the selected day. The user is free to modify the
activities scheduled that day based on the context, which is
kept ﬁxed.

On the other hand, global features depend on all of x (or
large chunks of it). For instance, in house furnishing one
may have features that measure whether the total cost of the
furniture is within a given budget, or how much the cost sur-
passes the budget. A naive solution would be that of show-
ing the user the whole furniture arrangement x, which can

be troublesome when x is large. A better alternative is to
present the user a summary of the global features, in this case
the percentage of the used budget. Such a summary would
be sufﬁcient for producing an informed improvement, inde-
pendently from the actual size of x.

Of course, the best choice of context format is application
speciﬁc. We only note that, while crucial, the context only
provides auxiliary information to the user, and does not af-
fect the learning algorithm directly.

Analysis
In preference elicitation, it is common to measure the quality
of a recommended (full) conﬁguration in terms of the regret:

REG(x) := maxˆx∈X u∗(ˆx) − u∗(x) = (cid:104)w∗, φ(x∗) − φ(x)(cid:105)
where u∗(·) is the true, unobserved user utility and x∗ =
argmaxx∈X u∗(x) is a truly optimal conﬁguration. In PCL,
interaction with the user occurs via partial conﬁgurations,
namely xt
pt. Since the regret is deﬁned in terms of
complete conﬁgurations, it is difﬁcult to analyze it directly
based on information about the partial conﬁgurations alone,
making it hard to prove convergence to globally optimal rec-
ommendations.

pt and ˆxt

The aim of this analysis is, however, to show that our al-
gorithm converges to a locally optimal conﬁguration, which
is in line with guarantees offered by other Coactive Learn-
ing variants (Goetschalckx, Fern, and Tadepalli 2014); the
latter, however still rely on interaction via complete conﬁg-
urations. Here a conﬁguration x is a local optimum for u∗(·)
if no part-wise modiﬁcation can improve x with respect to
u∗(·). Formally, x is a local optimum for u∗(·) if and only
if:

∀ p ∈ P (cid:64) x(cid:48)

p ∈ Xp u∗(x(cid:48)

p ◦ xp) > u∗(xp ◦ xp)

(4)

To measure local quality of a conﬁguration x with respect to
a part p, we introduce the concept of conditional regret of
the partial conﬁguration xp given the rest of the object xp:

CREGp (x) := u∗(x∗

p ◦ xp) − u∗(xp ◦ xp)
p = argmaxˆxp∈Xp u∗(ˆxp ◦ xp). Notice that:

where x∗

CREGp (x) = u∗[Ip](x∗

p ◦ xp) − u∗[Ip](xp ◦ xp)

since u∗[−Ip](x∗

p ◦ xp) − u∗[−Ip](xp ◦ xp) = 0.

We say that a partial conﬁguration x is conditionally op-
timal with respect to part p if CREGp (x) = 0. The follow-
ing lemma gives sufﬁcient and necessary conditions for local
optimality of a conﬁguration x.
Lemma 1. A conﬁguration x is locally optimal with respect
to u∗(·) if and only if x is conditionally optimal for u∗(·)
with respect to all basic parts p ∈ P.

Proof. By contradiction. (i) Assume that x is locally optimal
but not conditionally optimal with respect to p ∈ P. Then
CREGp (x) > 0, and thus there exists a partial conﬁguration
x(cid:48)
p such that u∗(x(cid:48)
p ◦ x ¯p) > u∗(xp ◦ xx ¯p ). This violates
the local optimality of x (Eq. 4). (ii) Assume that all partial
conﬁgurations xp ∀p ∈ P are conditionally optimal but x

is not locally optimal. Then there exists a part q ∈ P and a
partial conﬁguration x(cid:48)
q ◦x¯q) > u∗(xq ◦x¯q).
q such that u∗(x(cid:48)
This in turn means that CREGq (x) > 0. This violates the
conditional optimality of x with respect to q.

The above lemma gives us a part-wise measurable crite-
rion to determine if a conﬁguration x is a local optimum
through the conditional regret of x for all the provided parts.
The rest of the analysis is devoted to derive an upper
bound on the conditional regret incurred by the algorithm
and to prove that PCL eventually reaches a local optimum.

In order to derive the bound, we rely on the concept of
α-informativeness from (Shivaswamy and Joachims 2015),
adapting it to part-wise interaction2. A user is conditionally
α-informative if, when presented with a partial conﬁgura-
pt, he/she provides a partial conﬁguration ˆxt
tion xt
pt that is
at least some fraction α ∈ (0, 1] better than xt
pt in terms of
conditional regret, or more formally:
pt ◦ xt
pt) − u∗[I t](xt

pt) − u∗[I t](xt
pt ◦ xt

pt ◦ xt
α (cid:0)u∗[I t](x∗

u∗[I t](ˆxt

pt ◦ xt

pt) ≥

pt)(cid:1)

(5)

In the rest of the paper we will use the notation u[I t](ˆxt)
pt ◦ xt
meaning u[I t](ˆxt
pt), i.e. drop the complement, when
no ambiguity can arise.

At all iterations t, the algorithm updates the weights spec-
iﬁed by Qt, producing a new estimate wt+1 of w∗. The ac-
tual indices Qt depend on the condition at line 9 of Algo-
rithm 2: at some iterations Qt includes all of I t, while at
others Qt is restricted to J t. We distinguish between these
cases by:

I = {t ∈ [T ] : ut[I t](ˆxt) − ut[I t](xt) ≤ 0}
J = {t ∈ [T ] : ut[I t](ˆxt) − ut[I t](xt) > 0}

so that if t ∈ I then Qt = I t, and Qt = J t if t ∈ J . For all
t ∈ [T ], the quality of wt+1 is:

(cid:104)w∗, wt+1(cid:105) = (cid:104)w∗
= (cid:104)w∗

Qt, wt+1
Qt (cid:105)
Qt, wt
Qt(cid:105)

−Qt(cid:105) + (cid:104)w∗
−Qt(cid:105) + (cid:104)w∗

−Qt, wt+1
−Qt, wt
+ (cid:104)w∗
= (cid:104)w∗, wt(cid:105) + (cid:104)w∗
= (cid:104)w∗, wt(cid:105) + u∗[Qt](ˆxt) − u∗[Qt](xt)

Qt, φQt(ˆxt) − φQt(xt)(cid:105)

Qt, φQt(ˆxt) − φQt(xt)(cid:105)

Therefore if the second summand in the last equation, the
utility gain u∗[Qt](ˆxt) − u∗[Qt](xt), is positive, the update
produces a better weight estimate wt+1.

Since the user is conditionally α-informative, the im-
provement ˆxt always satisﬁes u∗[I t](ˆxt) − u∗[I t](xt) ≥
0. When t ∈ I, we have Qt = I t, and thus the util-
ity gain is guaranteed to be positive. On the other hand,
when t ∈ J we have Qt = J t and the utility gain re-
duces to u∗[J t](ˆxt) − u∗[J t](xt). In this case the update

2Here we adopted the deﬁnition of strict α-informativeness
for simplicity. Our results can be directly extended to the more
general notions of informativeness described in (Shivaswamy and
Joachims 2015).

ignores the weights in I t \ J t, “missing out” a factor of
u∗[I t \ J t](ˆxt) − u∗[I t \ J t](xt).

We compactly quantify the missing utility gain as:

(cid:26)0

ζ t =

u∗[I t \ J t](ˆxt) − u∗[I t \ J t](xt)

if t ∈ I
if t ∈ J

Note that ζ t can be positive, null or negative for t ∈ J .
When ζ t is negative, making the update on J t only actually
avoids a loss in utility gain.

(cid:80)T

We now prove that PCL minimizes the average conditional
t=1 CREGpt (xt) as T → ∞ for

regret CREGT := 1
T
conditionally α-informative users.
Theorem 1. For a conditionally α-informative user, the av-
erage conditional regret of PCL after T iterations is upper
bounded by:

1
T

T
(cid:88)

t=1

CREGpt

(cid:0)xt(cid:1) ≤

2DS(cid:107)w∗(cid:107)
√

+

α

T

1
αT

T
(cid:88)

t=1

ζ t

Proof. The following is a sketch3. We start by splitting the
iterations into the I and J sets deﬁned above, and bound
the norm (cid:107)wT +1(cid:107)2. In both cases we ﬁnd that (cid:107)wT +1(cid:107)2 ≤
4D2S2T . We then expand the term (cid:104)w∗, wT +1(cid:105) for itera-
tions in both I and J , obtaining:

(cid:104)w∗, wT +1(cid:105) =

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105)

(cid:88)

t∈I

+

(cid:88)

t∈J

(cid:104)w∗

J t, φJ t(ˆxt) − φJ t (xt)(cid:105)

With few algebraic manipulations we obtain:

(cid:104)w∗, wT +1(cid:105)

=

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105) −

Which we then bound using the Cauchy-Schwarz inequality:

√

(cid:107)w∗(cid:107)

4D2S2T

≥ (cid:107)w∗(cid:107)(cid:107)wT +1(cid:107)
≥ (cid:104)w∗, wT +1(cid:105)

=

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105) −

T
(cid:88)

t=1

ζ t

T
(cid:88)

t=1

ζ t

T
(cid:88)

t=1

T
(cid:88)

t=1

Applying the conditional α-informative feedback (Eq. 5)
and rearranging proves the claim.

Theorem 1 ensures that the average conditional regret suf-
fered by our algorithm decreases as O(1/
T ). This alone,
however, does not prove that the algorithm will eventually

√

3The complete proof can be found in the Supplementary Mate-

rial.

t=τ0

arrive at a local optimum, even if (cid:80)T
CREGpt (xt) = 0,
for some τ0 ∈ [T ]. This is due to the fact that partial in-
ference is performed keeping the rest of the object xt
pt ﬁxed.
Between iterations an inferred part may change as a result of
a change of the other parts in previous iterations. The object
could, in principle, keep changing at every iterations, even
if CREGpt (xt) is always equal to 0. The next proposition,
however, shows that this is not the case thanks to the utility
decomposition we employ.
Proposition 1. Let τ1 < . . . < τn < ˆτ1 < . . . < ˆτn ≤ T such
that pτk = pˆτk = pk and CREGpt (xt) = 0 for all t ≥ τ1.
The conﬁguration xT is a local optimum.

= xτ1

p1 for all t > τ1, as ut
I1

Proof. Sketch. The proof procedes by strong induction. We
ﬁrst show that xt
(·)
p1
only depends on the features in J1 and by assumption
CREGpt (xt) = 0 for all t ≥ τ1. By strong induction, as-
suming that xt
pj for all j = 1, . . . , k − 1 and all
pj
= xτk
t > τk−1, we can easily show that xt
pk
pk for all t > τn, therefore xˆτk = xτn for
(cid:0)xˆτk (cid:1) = 0 for all k ∈ [n] by
all k ∈ [n]. Since CREGpk
assumption, then xT = xˆτn = xτn is a local optimum and
will not change for all t > τn.

pk as well.

Now, xt
pk

= xτj

= xτk

The algorithm actually reaches a local optimum at t =
τn, but it needs to double check all the parts in order to be
sure that the conﬁguration is actually a local optimum. This
justiﬁes a termination criterion that we use in practice: if
the algorithm completes two full runs over all the parts, and
the user can never improve any of the recommended partial
conﬁgurations, then the full conﬁguration is guaranteed to be
a local optimum, and the algorithm can stop. As mentioned,
we employ this criterion in our implementation but we left it
out from Algorithm 2 for simplicity.

To recap, Theorem 1 guarantees that CREGT → 0 as t →
∞, therefore CREGpt (xt) approaches 0 as well. Combining
this fact with Proposition 1, we proved that our algorithm
approaches a local optimum for T → ∞.

Empirical Analysis
We ran PCL on three constructive preference elicitation tasks
of increasing complexity, comparing different degrees of
user informativeness. According to our experiments, infor-
mativeness is the most critical factor. The three problems in-
volve rather large conﬁgurations, which can not be handled
by coactive interaction via complete conﬁgurations. For in-
stance, in (Ortega and Stocker 2016) the user is tasked to
solve relatively simple SAT instances over three variables
and (at most) eight clauses; in some cases users were ob-
served to show signs of cognitive overload. In comparison,
our simplest realistic problem involve 35 categorical vari-
ables (with 8 possible values) and 74 features, plus addi-
tional hard constraints. As a consequence, Coactive Learn-
ing can not be applied as-is, and part-wise interaction is nec-
essary.

In all of these settings, part-wise inference is cast
as a mixed integer linear problem (MILP), and solved

with Gecode4. Despite being NP-hard in general, MILP
solvers can be very efﬁcient on practical instances. Ef-
improved by inferring only partial
ﬁciency is further
conﬁgurations. Our experimental setup is available at
https://github.com/unitn-sml/pcl.

We employed a user simulation protocol similar to that
of (Teso, Dragone, and Passerini 2017). First, for each prob-
lem, we sampled 20 vectors w∗ at random from a standard
normal distribution. Then, upon receiving a recommenda-
tion xt
pt is generated by solving the
following problem:

pt, an improvement ˆxt

argmin
xpt ∈Xpt

u∗[I t](xpt ◦ xt

pt)

s.t. u∗[I t](xpt ◦ xt

pt) − u∗[I t](xt
pt ◦ xt

pt ◦ xt
pt) − u∗[I t](xt

pt)

≥ α(u∗[I t](x∗

pt ◦ xt

pt))

This
formulation clearly satisﬁes
informativeness assumption (Eq. 5).

the conditional α-

Synthetic setting. We designed a simple synthetic prob-
lem inspired by spin glass models, see Figure 1 for a depic-
tion. In this setting, a conﬁguration x consists of a 4 × 4
grid. Each node in the grid is a binary 0-1 variable. Adjacent
nodes are connected by an edge, and each edge is associ-
ated to an indicator feature that evaluates to 1 if the inci-
dent nodes have different values (green in the ﬁgure), and to
−1 otherwise (red in the ﬁgure). The utility of a conﬁgura-
tion is simply the weighted sum of the values of all features
(edges). The basic parts p consist of all the non-overlapping
2 × 2 sub-grids of x, for a total of 4 basic parts (indicated by
dotted lines in the ﬁgure).

Figure 1: Example grid conﬁguration.

Since the problem is small enough for inference of
complete conﬁgurations to be practical, we compared PCL
to standard Coactive Learning, using the implementation
of (Teso, Dragone, and Passerini 2017). In order to keep the
comparison as fair as possible, the improvements fed to CL
were chosen to match the utility gain obtained by PCL. We
further report the performance of three alternative part se-
lection strategies: random, smallest (most independent) part
ﬁrst, and UCB1.

4http://www.gecode.org/

The results can be found in the ﬁrst column of Figure 2.
We report both the regret (over complete conﬁgurations) and
the cumulative runtime of all algorithms, averaged over all
users, as well as their standard deviation. The regret plot
shows that, despite being restricted to work with 2 × 2 con-
ﬁgurations, PCL does recommend complete conﬁgurations
of quality comparable to CL after enough queries are made.
Out of the three part selection strategies, random performs
best, with the other two more informed alternatives (espe-
cially smallest ﬁrst) quite close. The runtime gap between
full and part-wise inference is already clear in this small
synthetic problem; complete inference quickly becomes im-
practical as the problem size increases.

Training planning. Generating personalized training
plans based on performance and health monitoring has re-
ceived a lot of attention recently in sport analytics (see
e.g. (Fister et al. 2015)). Here we consider the problem of
synthesizing a week-long training plan x from information
about the target athlete. Each day includes 5 time slots (two
for the morning, two for the afternoon, one for the evening),
for 35 slots total. We assume to be given a ﬁxed number of
training activities (7 in our experiments: walking, running,
swimming, weight lifting, pushups, squats, abs), as well as
knowledge of the slots in which the athlete is available. The
training plan x associates an activity to each slot where the
athlete is available. Our formulation tracks the amount of
improvement (e.g. power increase) and fatigue over ﬁve dif-
ferent body parts (arms, torso, back, legs, and heart) induced
by performing an activity for one time slot. Each day deﬁnes
a basic part.

The mapping between training activity and improve-
ment/fatigue over each body part is assumed to be provided
externally. It can be provided by the athlete or medical per-
sonnel monitoring his/her status. The features of x include,
for each body part, the total performance gain and fatigue,
computed over the recommended training plan according to
the aforementioned mapping. We further include inter-part
features to capture activity diversity in consecutive days. The
fatigue accumulated in 3 consecutive time slots in any body
parts does not exceed a given threshold, to prevent injuries.
In this setting, CL is impractical from both the cognitive
and computational points of view. We ran PCL and evaluated
the impact of user informativeness by progressively increas-
ing α from 0.1, to 0.3, to 0.5. The results can be seen in Fig-
ure 2. The plots show clearly that, despite the complexity of
the conﬁguration and constraints, PCL can still produce very
low-regret conﬁgurations after about 50 iterations or less.

Understandably, the degree of improvement α plays an
important role in the performance of PCL and, consequently,
in its runtime (users at convergence do not contribute to the
runtime), at least up to α = 0.5. Recall, however, that the
improvements are part-wise, and hence α quantiﬁes the de-
gree of local improvement: part improvements may be very
informative on their own, but only give a modest amount
of information about the full conﬁguration. However, it is
not unreasonable to expect that users to be very informative
when presented with reasonably sized (and simple) parts.

Figure 2: Regret over complete conﬁgurations (top) and cumulative runtime (bottom) of PCL and CL on our three constructive
problems: synthetic (left), training planning (middle), and hotel planning (right). The x-axis is the number of iterations, while
the shaded areas represent the standard deviation. Best viewed in color.

Crucially PCL allows the system designer to deﬁne the parts
appropriately depending on the application.

Hotel planning. Finally, we considered a complex furni-
ture allocation problem: furnishing an entire hotel. The prob-
lem is encoded as follows. The hotel is represented by a
graph: nodes are rooms and edges indicate which rooms are
adjacent. Rooms can be of three types: normal rooms, suites,
and dorms. Each room can hold a maximum number of fur-
niture pieces, each associated to a cost. Additional, ﬁxed
nodes represent bathrooms and bars. The type of a room is
decided dynamically based on its position and furniture. For
instance, a normal room must contain at most three single or
double beds, no bunk beds, and a table, and must be close
to a bathroom. A suite must contain one bed, a table and a
sofa, and must be close to a bathroom and a bar. Each room
is a basic part, and there are 15 rooms to be allocated.

The feature vector contains 20 global features plus 8 lo-
cal features per room. The global features include different
functions of the number of different types of rooms, the total
cost of the furniture and the total number of guests. The local
features include, instead, characteristics of the current room,
such as its type or the amount of furniture, and other fea-
tures shared by adjacent rooms, e.g. whether two rooms have
the same type. These can encode preferences like “suites
and dorms should not be too close”, or “the hotel should
maintain high quality standards while still being proﬁtable”.
Given the graph structure, room capacities, and total bud-
get, the goal is to furnish all rooms according to the user’s
preferences.

This problem is hard to solve to optimality with current

solvers; part-based inference alleviates this issue by focus-
ing on individual rooms. There are 15 rooms in the hotel,
so that at each iteration only 1/15 of the conﬁguration is af-
fected. Furthermore, the presence of the global features im-
plies dependences between all rooms. Nonetheless, the al-
gorithm manages to reduce the regret by an order of magni-
tude in around a 100 iterations, starting from a completely
uninformed prior. Note also that as for the training planning
scenario, an alpha of 0.3 achieves basically the same results
as those for alpha equal to 0.5.

Conclusion

In this work we presented an approach to constructive pref-
erence elicitation able to tackle large constructive domains,
beyond the reach of previous approaches. It is based on
Coactive Learning (Shivaswamy and Joachims 2015), but
only requires inference of partial conﬁgurations and partial
improvement feedback, thereby signiﬁcantly reducing the
cognitive load of the user. We presented an extensive the-
oretical analysis demonstrating that, despite working only
with partial conﬁgurations, the algorithm converges to a lo-
cally optimal solution. The algorithm has been evaluated
empirically on three constructive scenarios of increasing
complexity, and shown to perform well in practice.

Possible future work includes improving part-based in-
teraction by exchanging additional contextual information
(e.g. features (Teso, Dragone, and Passerini 2017) or expla-
nations) with the user, and applying PCL to large layout syn-
thesis problems (Dragone et al. 2016).

ings of the 18th ACM SIGKDD international conference on
Knowledge discovery and data mining, 705–713. ACM.
Shivaswamy, P., and Joachims, T. 2015. Coactive learning.
JAIR 53:1–40.
Teso, S.; Dragone, P.; and Passerini, A. 2017. Coactive
critiquing: Elicitation of preferences and features. In AAAI.
Teso, S.; Passerini, A.; and Viappiani, P. 2016. Constructive
preference elicitation by setwise max-margin learning.
In
Proceedings of the Twenty-Fifth International Joint Confer-
ence on Artiﬁcial Intelligence, 2067–2073. AAAI Press.

References
Auer, P.; Cesa-Bianchi, N.; and Fischer, P. 2002. Finite-
time analysis of the multiarmed bandit problem. Machine
learning 47(2-3):235–256.
Boutilier, C.; Bacchus, F.; and Brafman, R. I. 2001. Ucp-
networks: A directed graphical representation of conditional
In Proceedings of the Seventeenth conference on
utilities.
Uncertainty in artiﬁcial intelligence, 56–64. Morgan Kauf-
mann Publishers Inc.
Boutilier, C.; Patrascu, R.; Poupart, P.; and Schuurmans, D.
2006. Constraint-based optimization and utility elicitation
using the minimax decision criterion. Artiﬁcial Intelligence
170(8-9):686–713.
Braziunas, D., and Boutilier, C. 2005. Local utility elici-
In Proceedings of the Twenty-First
tation in GAI models.
Conference on Uncertainty in Artiﬁcial Intelligence, 42–49.
AUAI Press.
Braziunas, D., and Boutilier, C. 2007. Minimax regret based
elicitation of generalized additive utilities. In UAI, 25–32.
Braziunas, D., and Boutilier, C. 2009. Elicitation of factored
utilities. AI Magazine 29(4):79.
Chajewska, U.; Koller, D.; and Parr, R. 2000. Making ratio-
nal decisions using adaptive utility elicitation. In AAAI/IAAI,
363–369.
Dragone, P.; Erculiani, L.; Chietera, M. T.; Teso, S.; and
Passerini, A. 2016. Constructive layout synthesis via coac-
tive learning. In Constructive Machine Learning workshop,
NIPS.
Fishburn, P. C. 1967.
Interdependence and additivity in
multivariate, unidimensional expected utility theory. Inter-
national Economic Review 8(3):335–342.
Fister, I.; Rauter, S.; Yang, X.-S.; and Ljubiˇc, K. 2015. Plan-
ning the sports training sessions with the bat algorithm. Neu-
rocomputing 149:993–1002.
Goetschalckx, R.; Fern, A.; and Tadepalli, P. 2014. Coactive
learning for locally optimal problem solving. In Proceedings
of AAAI.
Gonzales, C., and Perny, P. 2004. GAI networks for utility
elicitation. KR 4:224–234.
Keeney, R. L., and Raiffa, H. 1976. Decisions with Multiple
Objectives: Preferences and Value Tradeoffs.
Mayer, R. E., and Moreno, R. 2003. Nine ways to reduce
cognitive load in multimedia learning. Educational psychol-
ogist 38(1):43–52.
Meseguer, P.; Rossi, F.; and Schiex, T. 2006. Soft con-
straints. Foundations of Artiﬁcial Intelligence 2:281–328.
Ortega, P. A., and Stocker, A. A. 2016. Human decision-
making under limited time. In Advances in Neural Informa-
tion Processing Systems, 100–108.
Pigozzi, G.; Tsouki`as, A.; and Viappiani, P. 2016. Prefer-
ences in artiﬁcial intelligence. Ann. Math. Artif. Intell. 77(3-
4):361–401.
Raman, K.; Shivaswamy, P.; and Joachims, T. 2012. Online
In Proceed-
learning to diversify from implicit feedback.

Supplementary Material

We add and subtract the term:

Proof of Theorem 2
We begin by expanding (cid:107)wT +1(cid:107)2. If T ∈ I:

(cid:107)wT +1(cid:107)2

= (cid:107)wT +1
= (cid:107)wT
= (cid:107)wT

−I T (cid:107)2 + (cid:107)wT +1
−I T (cid:107)2 + (cid:107)wT
−I T (cid:107)2 + (cid:107)wT
+ 2(cid:104)wT

I T (cid:107)2
I T + φI T (ˆxt) − φI T (xt)(cid:107)2
I T (cid:107)2 + (cid:107)φI T (ˆxt) − φI T (xt)(cid:107)2

I T , φI T (ˆxt) − φI T (xt)(cid:105)

Since T ∈ I, uT [I T ](ˆxT ) − uT [I T ](xT ) ≤ 0, thus:

I T (cid:107)2 + (cid:107)φI T (ˆxt) − φI T (xt)(cid:107)2

∞|I T |2

(cid:107)wT +1(cid:107)2

−I T (cid:107)2 + (cid:107)wT
≤ (cid:107)wT
≤ (cid:107)wT (cid:107)2 + 4D2S2

If instead T ∈ J :

(cid:107)wT +1(cid:107)2

= (cid:107)wT +1
= (cid:107)wT
= (cid:107)wT
= (cid:107)wT

−J T (cid:107)2 + (cid:107)wT +1
−J T (cid:107)2 + (cid:107)wT +1
−J T (cid:107)2 + (cid:107)wT
−J T (cid:107)2 + (cid:107)wT
+ 2(cid:104)wT

J T (cid:107)2
J T (cid:107)2
J T + φJ T (ˆxt) − φJ T (xt)(cid:107)2
J T (cid:107)2 + (cid:107)φJ T (ˆxt) − φJ T (xt)(cid:107)2

J T , φJ T (ˆxt) − φJ T (xt)(cid:105)

≤ (cid:107)wT (cid:107)2 + (cid:107)φJ T (ˆxt) − φJ T (xt)(cid:107)2
≤ (cid:107)wT (cid:107)2 + 4D2|J T |2
≤ (cid:107)wT (cid:107)2 + 4D2S2

∞|J T |2

We can therefore expand the term:

(cid:107)wT +1(cid:107)2 ≤ 4D2S2T

Applying Cauchy-Schwarz inequality:

(cid:104)w∗, wT +1(cid:105) ≤ (cid:107)w∗(cid:107)(cid:107)wT +1(cid:107)

≤ 2DS(cid:107)w∗(cid:107)

T

√

The LHS of the above inequality expands to:

(cid:104)w∗, wT +1(cid:105) = (cid:104)w∗

−I T (cid:105) + (cid:104)w∗

I T , wT

I T (cid:105)

−I T , wT
+ (cid:104)w∗

I T , φI T (ˆxT ) − φI T (xT )(cid:105)

[ if T ∈ I ]
(cid:104)w∗, wT +1(cid:105) = (cid:104)w∗

−J T , wT
+ (cid:104)w∗

−J T (cid:105) + (cid:104)w∗

J T , wT

J T (cid:105)

J T , φJ T (ˆxT ) − φJ T (xT )(cid:105)

[ if T ∈ J ]

And thus:

(cid:104)w∗, wT +1(cid:105) =

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105)

(cid:88)

t∈I

+

(cid:88)

t∈J

(cid:104)w∗

J t , φJ t(ˆxt) − φJ t(xt)(cid:105)

(cid:104)w∗

I t\J t, φI t\J t(ˆxt) − φI t\J t(xt)(cid:105)

(cid:88)

t∈J

We obtain:

(cid:104)w∗, wT +1(cid:105)

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

=

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105)

−

(cid:88)

t∈J

(cid:104)w∗

I t\J t, φI t\J t(ˆxt) − φJ t(xt)(cid:105)

=

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105) −

≥ α

(cid:104)w∗

I t, φI t(x∗

pt ◦ xt

¯pt) − φI t(xt)(cid:105) −

T
(cid:88)

t=1

ζ t

T
(cid:88)

t=1

ζ t

And thus:

T
(cid:88)

t=1

(cid:104)w∗

I t, φI t(x∗) − φI t(xt)(cid:105) ≤

√

2DS(cid:107)w∗(cid:107)

T

α

+

1
α

T
(cid:88)

t=1

ζ t

Dividing both sides by T proves the claim.

Proof of Proposition 3
We ﬁrst show that xt
pk
iteration t the algorithm produces xt
local subutility:

= xτk

pk for t > τk. Recall that at
pk by maximizing the

ut
Ik

(x) = ut[Jk](x) = ut[Ik \ ((cid:83)n

j=k+1 Ij)](x)

j=k+1 Ij. Each subutility
(·) is only affected by the changes in xp1 , . . . , xpk−1, as

i.e. ignoring all the features in (cid:83)n
ut
Ik
their feature subsets may intersect with Ik.

= wτ1

The subutility ut
I1

(·) only depends on the features that
are exclusively included in I1, so it is not affected by the
changes in all the other parts. At iteration τ1, the condi-
tional regret CREGp1 (xτ1 ) = 0, and thus ˆxτ1
p1 and
p1
wτ1+1
(·)
I t
will not have changed as it only depends on features exclu-
sively included in I1 and thus xt
= xτ1
p1 . Since, by assump-
p1
tion, CREGpt (xt) = 0 for all t ≥ τ1, uτ1
(·) will not change
I1
anymore and so xt
p1

= xτ1
I t . If p1 gets selected again at t > τ1, ut
I1

= xτ1
p1.
By strong induction, suppose that all parts xt
pj

pj for
all j = 1, . . . k − 1 and for all t > τk−1. At iteration τk,
CREGpk (xτk ) = 0, thus ˆxτk
pk and so the weights
pk
wτk+1
will not change. For the following t > τk iterations,
Ik
even if pk gets selected, since the subutility ut
(·) is only
pk
affected by the changes in xp1 , . . . , xpk−1 and those parts do
not change (by inductive assumption), we can conclude that
ut
(·) = uτk
(·) and that xt
pk
pk
pk
This proves that, for any k ∈ [n] and any t > τk, xt
=
pk
xτk
pk . This also means that none of the partial conﬁgurations

= xτk
pk .

= xτj

= xτk

xt
pt will change for t > τn. Since none of the partial conﬁg-
(cid:0)xˆτ1 (cid:1) = 0 by
uration changed, xˆτ1 = xτn . Since CREGp1
assumption, then xˆτ1 is conditionally optimal with respect
(cid:0)xˆτk (cid:1) = 0 for all
to p1. Likewise, xˆτk = xτn and CREGpk
k ∈ [n], and thus xT = xˆτn = xτn is a local optimum.

8
1
0
2
 
y
a
M
 
6
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
7
4
2
8
0
.
1
1
7
1
:
v
i
X
r
a

Decomposition Strategies for Constructive Preference Elicitation
(and Supplementary Material)

Paolo Dragone∗
University of Trento, Italy
TIM-SKIL, Trento, Italy
paolo.dragone@unitn.it

Stefano Teso
KU Leuven, Belgium
stefano.teso@cs.kuleuven.be

Mohit Kumar †
KU Leuven, Belgium
mohit.kumar@cs.kuleuven.be

Andrea Passerini
University of Trento, Italy
andrea.passerini@unitn.it

Abstract

We tackle the problem of constructive preference elicitation,
that is the problem of learning user preferences over very
large decision problems, involving a combinatorial space of
possible outcomes. In this setting, the suggested conﬁgura-
tion is synthesized on-the-ﬂy by solving a constrained op-
timization problem, while the preferences are learned itera-
tively by interacting with the user. Previous work has shown
that Coactive Learning is a suitable method for learning user
preferences in constructive scenarios. In Coactive Learning
the user provides feedback to the algorithm in the form of an
improvement to a suggested conﬁguration. When the problem
involves many decision variables and constraints, this type of
interaction poses a signiﬁcant cognitive burden on the user.
We propose a decomposition technique for large preference-
based decision problems relying exclusively on inference and
feedback over partial conﬁgurations. This has the clear ad-
vantage of drastically reducing the user cognitive load. Addi-
tionally, part-wise inference can be (up to exponentially) less
computationally demanding than inference over full conﬁg-
urations. We discuss the theoretical implications of working
with parts and present promising empirical results on one syn-
thetic and two realistic constructive problems.

Introduction
In constructive preference elicitation (CPE) the recom-
mender aims at suggesting a custom or novel product to a
customer (Teso, Passerini, and Viappiani 2016). The prod-
uct is assembled on-the-ﬂy from components or synthe-
sized anew by solving a combinatorial optimization prob-
lem. The suggested products should of course satisfy the
customer’s preferences, which however are unobserved and
must be learned interactively (Pigozzi, Tsouki`as, and Vi-
appiani 2016). Learning proceeds iteratively: the learner
presents one or more candidate recommendations to the cus-
tomer, and employs the obtained feedback to estimate the

∗PD is a fellow of TIM-SKIL Trento and is supported by a
TIM scholarship. This work has received funding from the Euro-
pean Research Council (ERC) under the European Unions Horizon
2020 research and innovation programme (grant agreement No.
[694980] SYNTH: Synthesising Inductive Data Models).
Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

customer’s preferences. Applications include recommend-
ing custom PCs or cars, suggesting touristic travel plans,
designing room and building layouts, and producing recipe
modiﬁcations, among others.

A major weakness of existing CPE methods (Teso,
Passerini, and Viappiani 2016; Teso, Dragone, and Passerini
2017) is that they require the user to provide feedback on
complete conﬁgurations. In real-world constructive prob-
lems such as trip planning and layout design, conﬁgura-
tions can be large and complex. When asked to evaluate or
manipulate a complex product, the user may become over-
whelmed and confused, compromising the reliability of the
obtained feedback (Mayer and Moreno 2003). Human deci-
sion makers can revert to a potentially uninformative prior
when problem solving exceeds their available resources.
This effect was observed in users tasked with solving sim-
ple SAT instances (three variables and eight clauses) (Or-
tega and Stocker 2016). In comparison, even simple con-
structive problems can involve tens of categorical variables
and features, in addition to hard feasibility constraints. On
the computational side, working with complete conﬁgura-
tions poses scalability problems as well. The reason is that,
in order to select recommendations and queries, constructive
recommenders employ constraint optimization techniques.
Clearly, optimization of complete conﬁgurations in large
constructive problems can become computationally imprac-
tical as the problem size increases.

Here we propose to exploit factorized utility func-
tions (Braziunas and Boutilier 2009), which occur very natu-
rally in constructive problems, to work with partial conﬁgu-
rations. In particular, we show how to generalize Coactive
Learning (CL) (Shivaswamy and Joachims 2015) to part-
wise inference and learning. CL is a simple, theoretically
grounded algorithm for online learning and preference elic-
itation. It employs a very natural interaction protocol: at
each iteration the user is presented with a single, appropri-
ately chosen candidate conﬁguration and asked to improve
it (even slightly). In (Teso, Dragone, and Passerini 2017), it
was shown that CL can be lifted to constructive problems
by combining it with a constraint optimization solver to ef-
ﬁciently select the candidate recommendation. Notably, the
theoretical guarantees of CL remain intact in the construc-

tive case.

Our part-wise generalization of CL, dubbed PCL, solves
the two aforementioned problems in one go: (i) by present-
ing the user with partial conﬁgurations, it is possible to (sub-
stantially) lessen her cognitive load, improving the reliabil-
ity of the feedback and enabling learning in larger construc-
tive tasks; (ii) in combinatorial constructive problems, per-
forming inference on partial conﬁgurations can be exponen-
tially faster than on complete ones. Further, despite being
limited to working with partial conﬁgurations, PCL can be
shown to still provide local optimality guarantees in theory,
and to perform well in practice.

This paper is structured as follows. In the next section
we overview the relevant literature. We present PCL in the
Method section, followed by a theoretical analysis. The per-
formance of PCL are then illustrated empirically on one syn-
thetic and two realistic constructive problems. We close the
paper with some concluding remarks.

Related Work
Generalized additive independent (GAI) utilities have been
thoroughly explored in the decision making literature (Fish-
burn 1967). They deﬁne a clear factorization mechanism,
and offer a good trade off between expressiveness and ease
of elicitation (Chajewska, Koller, and Parr 2000; Gonzales
and Perny 2004; Braziunas and Boutilier 2009). Most of the
early work on GAI utility elicitation is based on graphical
models, e.g. UCP and GAI networks (Gonzales and Perny
2004; Boutilier, Bacchus, and Brafman 2001). These ap-
proaches aim at eliciting the full utility function and rely
on the comparison of full outcomes. Both of these are infea-
sible when the utility involves many attributes and features,
as in realistic constructive problems.

Like our method, more recent alternatives (Braziunas and
Boutilier 2005; 2007) handle both partial elicitation, i.e. the
ability of providing recommendations without full utility in-
formation, and local queries, i.e. elicitation of preference
information by comparing only (mostly) partial outcomes.
There exist both Bayesian (Braziunas and Boutilier 2005)
and regret-based (Braziunas and Boutilier 2007; Boutilier
et al. 2006) approaches, which have different shortcomings.
Bayesian methods do not scale to even small size construc-
tive problems (Teso, Passerini, and Viappiani 2016), such as
those occurring when reasoning over individual parts in con-
structive settings. On the other hand, regret-based methods
require the user feedback to be strictly self-consistent, an
unrealistic assumption when interacting with non-experts.
Our approach instead is speciﬁcally designed to scale to
larger constructive problems and, being derived from Coac-
tive Learning, natively handles inconsistent feedback. Cru-
cially, unlike PCL, these local elicitation methods also re-
quire to perform a number of queries over complete con-
ﬁgurations to calibrate the learned utility function. In larger
constructive domains this is both impractical (on the user
side) and computationally infeasible (on the learner side).

Our work is based on Coactive Learning (CL) (Shiv-
aswamy and Joachims 2015), a framework for learning util-
ity functions over structured domains, which has been suc-
cessfully applied to CPE (Teso, Dragone, and Passerini

2017; Dragone et al. 2016). When applied to constructive
problems, a crucial limitation of CL is that the learner and
the user interact by exchanging complete conﬁgurations.
Alas, inferring a full conﬁguration in a constructive prob-
lem can be computationally demanding, thus preventing the
elicitation procedure from being real-time. This can be par-
tially addressed by performing approximate inference, as
in (Raman, Shivaswamy, and Joachims 2012), at the cost of
weaker learning guarantees. A different approach has been
taken in (Goetschalckx, Fern, and Tadepalli 2014), where
the exchanged (complete) conﬁgurations are only required
to be locally optimal, for improved efﬁciency. Like PCL, this
method guarantees the local optimality of the recommended
conﬁguration. All of the previous approaches, however, re-
quire the user to improve a potentially large complete con-
ﬁguration. This is a cognitively demanding task which can
become prohibitive in large constructive problems, even for
domain experts, thus hindering feedback quality and effec-
tive elicitation. By dealing with parts only, PCL avoids this
issue entirely.

Method
Notation. We use rather standard notation: scalars x are
written in italics and column vectors x in bold. The inner
product of two vectors is indicated as (cid:104)a, b(cid:105) = (cid:80)
i aibi, the
Euclidean norm as (cid:107)a(cid:107) = (cid:112)(cid:104)a, a(cid:105) and the max-norm as
(cid:107)a(cid:107)∞ = maxi ai. We abbreviate {1, . . . , n} to [n], and in-
dicate the complement of I ⊆ [n] as −I := [n] \ I.

Setting. Let X be the set of candidate structured products.
Contrarily to what happens in standard preference elicita-
tion, in the constructive case X is deﬁned by a set of hard
constraints rather than explicitly enumerated1. Products are
represented by a function φ(·) that maps them to an m-
dimensional feature space. While the feature map can be
arbitrary, in practice we will stick to features that can be en-
coded as constraints in a mixed-integer linear programming
problem, for efﬁciency; see the Empirical Analysis section
for details. We only assume that the features are bounded,
i.e. ∀x ∈ X it holds that (cid:107)φ(x)(cid:107)∞ ≤ D for some ﬁxed D.

As is common in multi-attribute decision theory (Keeney
and Raiffa 1976), we assume the desirability of a product
x to be given by a utility function u∗ : X → R that is
linear in the features, i.e., u∗(x) := (cid:104)w∗, φ(x)(cid:105). Here the
weights w∗ ∈ Rm encode the true user preferences, and
may be positive, negative, or zero (which means that the cor-
responding feature is irrelevant for the user). Utilities of this
kind can naturally express rich constructive problems (Teso,
Passerini, and Viappiani 2016; Teso, Dragone, and Passerini
2017).

Parts. Here we formalize what parts and partial conﬁgu-
rations are, and how they can be manipulated. We assume to

1In this paper, “hard” constraints refer to the constraints de-
limiting the space of feasible conﬁgurations, as opposed to “soft”
constraints, which determine preferences over feasible conﬁgura-
tions (Meseguer, Rossi, and Schiex 2006).

be given a set of n basic parts p ∈ P. A part is any subset
P ⊆ P of the set of basic parts. Given a part P and an object
x, xP ∈ XP indicates the partial conﬁguration correspond-
ing to P . We require that the union of the basic parts recon-
structs the whole object, i.e. xP = x for all x ∈ X . The
proper semantics of the decomposition into basic parts is
task-speciﬁc. For instance, in a scheduling problem a month
may be decomposed into days, while in interior design a
house may be decomposed into rooms. Analogously, the
non-basic parts could then be weeks or ﬂoors, respectively.
In general, any combination of basic parts is allowed. We
capture the notion of combination of partial conﬁgurations
with the part combination operator ◦ : XP × XQ → XP ∪Q,
so that xP ◦ xQ = xP ∪Q. We denote the complement of part
P as P = P \ P , which satisﬁes x = xP ◦ xP for all x ∈ X .
Each basic part p ∈ P is associated to a feature subset
Ip ⊆ [m], which contains all those features that depend on
p (and only those). In general, the sets Ip may overlap, but
we do require each basic part p to be associated to some
features that do not depend on any other basic part q, i.e. that
(cid:54)= ∅ for all p ∈ P. The features associated
Ip \
to a part P ⊆ P are deﬁned as (cid:83)
p∈P Ip. Since the union of
the basic parts makes up the full object, we also have that
(cid:83)

q(cid:54)=p Iq

(cid:16)(cid:83)

(cid:17)

p∈P Ip = [m].

GAI utility decomposition.
In the previous section we in-
troduced a decomposition of conﬁgurations into parts. In or-
der to elicit the user preferences via part-wise interaction,
which is our ultimate goal, we need to decompose the utility
function as well. Given a part P and its feature subset IP ,
let its partial utility be:

u[IP ](x) := (cid:104)wIP , φIP (x)(cid:105) = (cid:80)

i∈IP

wiφi(x)

(1)

If the basic parts have no shared features, the utility function
is additive: it is easy to verify that u(x) = (cid:80)
p∈P u[Ip](x).
In this case, each part can be managed independently of the
others, and the overall conﬁguration maximizing the utility
can be obtained by separately maximizing each partial utility
and combining the resulting part-wise conﬁgurations.

However, in many applications of interest the feature sub-
sets do overlap. In a travel plan, for instance, one can be
interested in alternating cultural and leisure activities in con-
secutive days, in order to make the experience more diverse
and enjoyable. In this case, the above decomposition does
not apply anymore as the basic parts may depend on each
other through the shared features. Nonetheless, it can be
shown that our utility function is generalized additive inde-
pendent (GAI) over the feature subsets Ip of the basic parts.
Formally, a utility u(x) is GAI if and only if, given n fea-
ture subsets I1, . . . , In ⊆ [m], it can be decomposed into n
independent subutilities (Braziunas and Boutilier 2005):

u(x) = uI1(x) + · · · + uIn (x)

(2)

where each subutility uIk can only depend on the features
in Ik (but does not need to depend on all of them). This de-
composition enables applying ideas from the GAI literature

Algorithm 1 An example of ordering selection procedure
using a GAI network (Gonzales and Perny 2004).
1: procedure SELECTORDERING(P)
2:
3:
4:
5:

Build a GAI network G from Ip, p ∈ P
Produce sequence p1, . . . , pn by sorting the nodes
in G in ascending order of node degree
return p1, . . . , pn

to produce a well-deﬁned part-wise elicitation protocol. In-
tuitively, we will assign features to subutilities so that when-
ever a feature is shared by multiple parts, only the subutility
corresponding to one of them will depend on that feature.

We will now construct a suitable decomposition of u(x)
into n independent subutilities. Fix some order of the basic
parts p1, . . . , pn, and let:

Jk := Ik \ ((cid:83)n

j=k+1 Ij)

(3)

for all k ∈ [n]. We deﬁne the subutilities as uIk (x) :=
u[Jk](x) for all k ∈ [n]. By summing up the subutilities for
all parts, we obtain a utility where each feature is computed
exactly once, thus recovering the full utility u(x):
k=1 uIk (x) = (cid:80)n
wiφi(x)

u(x) = (cid:80)n
= (cid:80)

k=1 u[Ik \ (cid:83)n

j=k+1 Ij](x)

i∈IP

The GAI decomposition allows to elicit each subutility
uIk separately. By doing so, however, we end up ignoring
some of the dependencies between parts, namely the fea-
tures in Ik \ Jk. This is the price to pay in order to achieve
decomposition and partwise elicitation, and it may lead to
suboptimal solutions if too many dependencies are ignored.
It is therefore important to minimize the broken dependen-
cies by an appropriate ordering of the parts. Going back to
the travel planning with diversifying features example, con-
sider a multi-day trip. Here the parts may refer to individual
days, and Ik includes all features of day k, including the
features relating it to the other days, e.g. the alternation of
cultural and leisure activities. Note that the Ik’s overlap. On
the other hand, the Jk’s are subset of features chosen so that
every feature only appears once. A diversifying feature re-
lating days 3 and 4 of the trip is either assigned to J3 or J4,
but not both.

One way to control the ignored dependencies is by lever-
aging GAI networks (Gonzales and Perny 2004). A GAI net-
work is a graph whose nodes represent the subsets Ik and
whose edges connect nodes sharing at least one feature. Al-
gorithm 1 presents a simple and effective solution to provide
an ordering. It builds a GAI network from P and sorts the
basic parts in ascending order of node degree (number of
incoming and outgoing edges). By ordering last the subsets
having intersections with many other parts, this ordering at-
tempts to minimize the lost dependencies in the above de-
composition (Eq. 3). This is one possible way to order the
parts, which we use as an example; more informed or task-
speciﬁc approaches could be devised.

The PCL algorithm. The pseudocode of our algorithm,
PCL, is listed in Algorithm 2. PCL starts off by sorting the

Algorithm 2 The PCL algorithm.
1: procedure PCL(P, T )
2:
3:
4:
5:
6:

p1, . . . , pn ← SELECTORDERING(P)
w1 ← 0, x1 ← initial conﬁguration
for t = 1, . . . , T do

pt ← SELECTPART(P)
xt
pt ← argmaxxpt ∈Xpt ut[J t](xpt ◦ xt
pt ← xt−1
xt
pt
User provides improvement ˆxt
pt ◦ xt

pt) − ut[I t](xt

pt of xt

pt ◦ xt

(cid:16)

7:

8:

9:

pt)

if
wt+1
wt+1

ut[I t](ˆxt
−Qt ← wt
Qt ← wt

10:

11:

−Qt
Qt + φQt(ˆxt

pt ◦ xt

pt) − φQt(xt

pt ◦ xt

pt))

pt, keeping xt
(cid:17)
pt) ≤ 0

pt ﬁxed
then Qt ← I t

else Qt ← J t ;

basic parts, producing an ordering p1, . . . , pn. Algorithm 1
could be employed or any other (e.g. task-speciﬁc) sorting
solution. Then it loops for T iterations, maintaining an esti-
mate wt of the user weights as well as a complete conﬁgu-
ration xt. The starting conﬁguration x1 should be a reason-
able initial guess, depending on the task. At each iteration
t ∈ [T ], the algorithm selects a part pt using the procedure
SELECTPART (see below). Then it updates the object xt by
inferring a new partial conﬁguration xt
pt while keeping the
rest of xt ﬁxed, that is xt
. The inferred partial con-
ﬁguration xt
pt is optimal with respect to the local subutility
I t(·) given xt−1
ut
. Note that inference is over the partial con-
pt
ﬁguration xt
pt only, and therefore can be exponentially faster
than inference over full conﬁgurations.

pt = xt−1
pt

Next, the algorithm presents the inferred partial conﬁgu-
ration xt
pt as well as some contextual information (see be-
low). The user is asked to produce an improved partial con-
ﬁguration ˆxt
pt according to the her own preferences, while
the rest of the object xt
pt is kept ﬁxed. We assume that a
user is satisﬁed with a partial conﬁguration xt
pt if she can-
not improve it further, or equivalently when the object xt is
conditionally optimal with respect to part pt given the rest of
the object xt
pt (the formal deﬁnition of conditional optimal-
ity is given in the Analysis section). When a user is satisﬁed
with a partial conﬁguration, she returns ˆxt
pt, thereby
implying no change in the weights wt+1.

pt = xt

pt ◦ xt

pt ◦ xt

After receiving an improvement, if the user is not satis-
ﬁed, the weights are updated through a perceptron step. The
subset Qt of weights that are actually updated depends on
pt ) − ut[I t](xt
whether ut[I t](ˆxt
pt) is negative or
(strictly) positive. Since we perform inference on ut[J t](·),
we have that ut[J t](xt
pt ) ≤ 0. The
user improvement can, however, potentially change all the
features in I t. Intuitively, the weights associated to a sub-
set of features should change only if the utility computed on
this subset ranks ˆxt
pt. The algorithm there-
fore checks whether ut[I t](xt
pt) ≤ ut[I t](xt
pt),
in which case the weights associated to the whole subset I t
should be updated. If this condition is not met, instead, the

pt lower than xt
pt ◦ xt

pt) − ut[J t](xt

pt ◦ xt

pt ◦ xt

pt ◦ xt

algorithm can only safely update the weights associated to
J t, which, as said, meet this condition by construction.

As for the SELECTPART procedure, we experimented
with several alternative implementations, including priori-
tizing parts with a large feature overlap (|Ik\Jk|) and bandit-
based strategies aimed at predicting a surrogate of the utility
gain (namely, a variant of the UCB1 algorithm (Auer, Cesa-
Bianchi, and Fischer 2002)). Preliminary experiments have
shown that informed strategies do not yield a signiﬁcant per-
formance improvement over the random selection stategy;
hence we stick with the latter in all our experiments.

The algorithm stops either when the maximum number
of iterations T is met or when a “local optimum” has been
found. For ease of exposition we left out the latter case from
Algorithm 2, but we explain what a local optimum is in the
following Analysis section; the stopping criterion will fol-
low directly from Proposition 1.

Interacting through parts.
In order for the user to judge
the quality of a suggested partial conﬁguration xp, some
contextual information may have to be provided. The rea-
son is that, if p depends on other parts via shared features,
these have to be somehow communicated to the user, other-
wise his/her improvement will not be sufﬁciently informed.
We distinguish two cases, depending on whether the fea-
tures of p are local or global. Local features only depend on
small, localized portions of x. This is for instance the case
for features that measure the diversity of consecutive activ-
ities in a touristic trip plan, which depend on consecutive
time slots or days only. Here the context amounts to those
other portions of x that share local features with p. For in-
stance, the user may interact over individual days only. If
the features are local, the context is simply the time slots be-
fore and after the selected day. The user is free to modify the
activities scheduled that day based on the context, which is
kept ﬁxed.

On the other hand, global features depend on all of x (or
large chunks of it). For instance, in house furnishing one
may have features that measure whether the total cost of the
furniture is within a given budget, or how much the cost sur-
passes the budget. A naive solution would be that of show-
ing the user the whole furniture arrangement x, which can

be troublesome when x is large. A better alternative is to
present the user a summary of the global features, in this case
the percentage of the used budget. Such a summary would
be sufﬁcient for producing an informed improvement, inde-
pendently from the actual size of x.

Of course, the best choice of context format is application
speciﬁc. We only note that, while crucial, the context only
provides auxiliary information to the user, and does not af-
fect the learning algorithm directly.

Analysis
In preference elicitation, it is common to measure the quality
of a recommended (full) conﬁguration in terms of the regret:

REG(x) := maxˆx∈X u∗(ˆx) − u∗(x) = (cid:104)w∗, φ(x∗) − φ(x)(cid:105)
where u∗(·) is the true, unobserved user utility and x∗ =
argmaxx∈X u∗(x) is a truly optimal conﬁguration. In PCL,
interaction with the user occurs via partial conﬁgurations,
namely xt
pt. Since the regret is deﬁned in terms of
complete conﬁgurations, it is difﬁcult to analyze it directly
based on information about the partial conﬁgurations alone,
making it hard to prove convergence to globally optimal rec-
ommendations.

pt and ˆxt

The aim of this analysis is, however, to show that our al-
gorithm converges to a locally optimal conﬁguration, which
is in line with guarantees offered by other Coactive Learn-
ing variants (Goetschalckx, Fern, and Tadepalli 2014); the
latter, however still rely on interaction via complete conﬁg-
urations. Here a conﬁguration x is a local optimum for u∗(·)
if no part-wise modiﬁcation can improve x with respect to
u∗(·). Formally, x is a local optimum for u∗(·) if and only
if:

∀ p ∈ P (cid:64) x(cid:48)

p ∈ Xp u∗(x(cid:48)

p ◦ xp) > u∗(xp ◦ xp)

(4)

To measure local quality of a conﬁguration x with respect to
a part p, we introduce the concept of conditional regret of
the partial conﬁguration xp given the rest of the object xp:

CREGp (x) := u∗(x∗

p ◦ xp) − u∗(xp ◦ xp)
p = argmaxˆxp∈Xp u∗(ˆxp ◦ xp). Notice that:

where x∗

CREGp (x) = u∗[Ip](x∗

p ◦ xp) − u∗[Ip](xp ◦ xp)

since u∗[−Ip](x∗

p ◦ xp) − u∗[−Ip](xp ◦ xp) = 0.

We say that a partial conﬁguration x is conditionally op-
timal with respect to part p if CREGp (x) = 0. The follow-
ing lemma gives sufﬁcient and necessary conditions for local
optimality of a conﬁguration x.
Lemma 1. A conﬁguration x is locally optimal with respect
to u∗(·) if and only if x is conditionally optimal for u∗(·)
with respect to all basic parts p ∈ P.

Proof. By contradiction. (i) Assume that x is locally optimal
but not conditionally optimal with respect to p ∈ P. Then
CREGp (x) > 0, and thus there exists a partial conﬁguration
x(cid:48)
p such that u∗(x(cid:48)
p ◦ x ¯p) > u∗(xp ◦ xx ¯p ). This violates
the local optimality of x (Eq. 4). (ii) Assume that all partial
conﬁgurations xp ∀p ∈ P are conditionally optimal but x

is not locally optimal. Then there exists a part q ∈ P and a
partial conﬁguration x(cid:48)
q ◦x¯q) > u∗(xq ◦x¯q).
q such that u∗(x(cid:48)
This in turn means that CREGq (x) > 0. This violates the
conditional optimality of x with respect to q.

The above lemma gives us a part-wise measurable crite-
rion to determine if a conﬁguration x is a local optimum
through the conditional regret of x for all the provided parts.
The rest of the analysis is devoted to derive an upper
bound on the conditional regret incurred by the algorithm
and to prove that PCL eventually reaches a local optimum.

In order to derive the bound, we rely on the concept of
α-informativeness from (Shivaswamy and Joachims 2015),
adapting it to part-wise interaction2. A user is conditionally
α-informative if, when presented with a partial conﬁgura-
pt, he/she provides a partial conﬁguration ˆxt
tion xt
pt that is
at least some fraction α ∈ (0, 1] better than xt
pt in terms of
conditional regret, or more formally:
pt ◦ xt
pt) − u∗[I t](xt

pt) − u∗[I t](xt
pt ◦ xt

pt ◦ xt
α (cid:0)u∗[I t](x∗

u∗[I t](ˆxt

pt ◦ xt

pt) ≥

pt)(cid:1)

(5)

In the rest of the paper we will use the notation u[I t](ˆxt)
pt ◦ xt
meaning u[I t](ˆxt
pt), i.e. drop the complement, when
no ambiguity can arise.

At all iterations t, the algorithm updates the weights spec-
iﬁed by Qt, producing a new estimate wt+1 of w∗. The ac-
tual indices Qt depend on the condition at line 9 of Algo-
rithm 2: at some iterations Qt includes all of I t, while at
others Qt is restricted to J t. We distinguish between these
cases by:

I = {t ∈ [T ] : ut[I t](ˆxt) − ut[I t](xt) ≤ 0}
J = {t ∈ [T ] : ut[I t](ˆxt) − ut[I t](xt) > 0}

so that if t ∈ I then Qt = I t, and Qt = J t if t ∈ J . For all
t ∈ [T ], the quality of wt+1 is:

(cid:104)w∗, wt+1(cid:105) = (cid:104)w∗
= (cid:104)w∗

Qt, wt+1
Qt (cid:105)
Qt, wt
Qt(cid:105)

−Qt(cid:105) + (cid:104)w∗
−Qt(cid:105) + (cid:104)w∗

−Qt, wt+1
−Qt, wt
+ (cid:104)w∗
= (cid:104)w∗, wt(cid:105) + (cid:104)w∗
= (cid:104)w∗, wt(cid:105) + u∗[Qt](ˆxt) − u∗[Qt](xt)

Qt, φQt(ˆxt) − φQt(xt)(cid:105)

Qt, φQt(ˆxt) − φQt(xt)(cid:105)

Therefore if the second summand in the last equation, the
utility gain u∗[Qt](ˆxt) − u∗[Qt](xt), is positive, the update
produces a better weight estimate wt+1.

Since the user is conditionally α-informative, the im-
provement ˆxt always satisﬁes u∗[I t](ˆxt) − u∗[I t](xt) ≥
0. When t ∈ I, we have Qt = I t, and thus the util-
ity gain is guaranteed to be positive. On the other hand,
when t ∈ J we have Qt = J t and the utility gain re-
duces to u∗[J t](ˆxt) − u∗[J t](xt). In this case the update

2Here we adopted the deﬁnition of strict α-informativeness
for simplicity. Our results can be directly extended to the more
general notions of informativeness described in (Shivaswamy and
Joachims 2015).

ignores the weights in I t \ J t, “missing out” a factor of
u∗[I t \ J t](ˆxt) − u∗[I t \ J t](xt).

We compactly quantify the missing utility gain as:

(cid:26)0

ζ t =

u∗[I t \ J t](ˆxt) − u∗[I t \ J t](xt)

if t ∈ I
if t ∈ J

Note that ζ t can be positive, null or negative for t ∈ J .
When ζ t is negative, making the update on J t only actually
avoids a loss in utility gain.

(cid:80)T

We now prove that PCL minimizes the average conditional
t=1 CREGpt (xt) as T → ∞ for

regret CREGT := 1
T
conditionally α-informative users.
Theorem 1. For a conditionally α-informative user, the av-
erage conditional regret of PCL after T iterations is upper
bounded by:

1
T

T
(cid:88)

t=1

CREGpt

(cid:0)xt(cid:1) ≤

2DS(cid:107)w∗(cid:107)
√

+

α

T

1
αT

T
(cid:88)

t=1

ζ t

Proof. The following is a sketch3. We start by splitting the
iterations into the I and J sets deﬁned above, and bound
the norm (cid:107)wT +1(cid:107)2. In both cases we ﬁnd that (cid:107)wT +1(cid:107)2 ≤
4D2S2T . We then expand the term (cid:104)w∗, wT +1(cid:105) for itera-
tions in both I and J , obtaining:

(cid:104)w∗, wT +1(cid:105) =

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105)

(cid:88)

t∈I

+

(cid:88)

t∈J

(cid:104)w∗

J t, φJ t(ˆxt) − φJ t (xt)(cid:105)

With few algebraic manipulations we obtain:

(cid:104)w∗, wT +1(cid:105)

=

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105) −

Which we then bound using the Cauchy-Schwarz inequality:

√

(cid:107)w∗(cid:107)

4D2S2T

≥ (cid:107)w∗(cid:107)(cid:107)wT +1(cid:107)
≥ (cid:104)w∗, wT +1(cid:105)

=

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105) −

T
(cid:88)

t=1

ζ t

T
(cid:88)

t=1

ζ t

T
(cid:88)

t=1

T
(cid:88)

t=1

Applying the conditional α-informative feedback (Eq. 5)
and rearranging proves the claim.

Theorem 1 ensures that the average conditional regret suf-
fered by our algorithm decreases as O(1/
T ). This alone,
however, does not prove that the algorithm will eventually

√

3The complete proof can be found in the Supplementary Mate-

rial.

t=τ0

arrive at a local optimum, even if (cid:80)T
CREGpt (xt) = 0,
for some τ0 ∈ [T ]. This is due to the fact that partial in-
ference is performed keeping the rest of the object xt
pt ﬁxed.
Between iterations an inferred part may change as a result of
a change of the other parts in previous iterations. The object
could, in principle, keep changing at every iterations, even
if CREGpt (xt) is always equal to 0. The next proposition,
however, shows that this is not the case thanks to the utility
decomposition we employ.
Proposition 1. Let τ1 < . . . < τn < ˆτ1 < . . . < ˆτn ≤ T such
that pτk = pˆτk = pk and CREGpt (xt) = 0 for all t ≥ τ1.
The conﬁguration xT is a local optimum.

= xτ1

p1 for all t > τ1, as ut
I1

Proof. Sketch. The proof procedes by strong induction. We
ﬁrst show that xt
(·)
p1
only depends on the features in J1 and by assumption
CREGpt (xt) = 0 for all t ≥ τ1. By strong induction, as-
suming that xt
pj for all j = 1, . . . , k − 1 and all
pj
= xτk
t > τk−1, we can easily show that xt
pk
pk for all t > τn, therefore xˆτk = xτn for
(cid:0)xˆτk (cid:1) = 0 for all k ∈ [n] by
all k ∈ [n]. Since CREGpk
assumption, then xT = xˆτn = xτn is a local optimum and
will not change for all t > τn.

pk as well.

Now, xt
pk

= xτj

= xτk

The algorithm actually reaches a local optimum at t =
τn, but it needs to double check all the parts in order to be
sure that the conﬁguration is actually a local optimum. This
justiﬁes a termination criterion that we use in practice: if
the algorithm completes two full runs over all the parts, and
the user can never improve any of the recommended partial
conﬁgurations, then the full conﬁguration is guaranteed to be
a local optimum, and the algorithm can stop. As mentioned,
we employ this criterion in our implementation but we left it
out from Algorithm 2 for simplicity.

To recap, Theorem 1 guarantees that CREGT → 0 as t →
∞, therefore CREGpt (xt) approaches 0 as well. Combining
this fact with Proposition 1, we proved that our algorithm
approaches a local optimum for T → ∞.

Empirical Analysis
We ran PCL on three constructive preference elicitation tasks
of increasing complexity, comparing different degrees of
user informativeness. According to our experiments, infor-
mativeness is the most critical factor. The three problems in-
volve rather large conﬁgurations, which can not be handled
by coactive interaction via complete conﬁgurations. For in-
stance, in (Ortega and Stocker 2016) the user is tasked to
solve relatively simple SAT instances over three variables
and (at most) eight clauses; in some cases users were ob-
served to show signs of cognitive overload. In comparison,
our simplest realistic problem involve 35 categorical vari-
ables (with 8 possible values) and 74 features, plus addi-
tional hard constraints. As a consequence, Coactive Learn-
ing can not be applied as-is, and part-wise interaction is nec-
essary.

In all of these settings, part-wise inference is cast
as a mixed integer linear problem (MILP), and solved

with Gecode4. Despite being NP-hard in general, MILP
solvers can be very efﬁcient on practical instances. Ef-
improved by inferring only partial
ﬁciency is further
conﬁgurations. Our experimental setup is available at
https://github.com/unitn-sml/pcl.

We employed a user simulation protocol similar to that
of (Teso, Dragone, and Passerini 2017). First, for each prob-
lem, we sampled 20 vectors w∗ at random from a standard
normal distribution. Then, upon receiving a recommenda-
tion xt
pt is generated by solving the
following problem:

pt, an improvement ˆxt

argmin
xpt ∈Xpt

u∗[I t](xpt ◦ xt

pt)

s.t. u∗[I t](xpt ◦ xt

pt) − u∗[I t](xt
pt ◦ xt

pt ◦ xt
pt) − u∗[I t](xt

pt)

≥ α(u∗[I t](x∗

pt ◦ xt

pt))

This
formulation clearly satisﬁes
informativeness assumption (Eq. 5).

the conditional α-

Synthetic setting. We designed a simple synthetic prob-
lem inspired by spin glass models, see Figure 1 for a depic-
tion. In this setting, a conﬁguration x consists of a 4 × 4
grid. Each node in the grid is a binary 0-1 variable. Adjacent
nodes are connected by an edge, and each edge is associ-
ated to an indicator feature that evaluates to 1 if the inci-
dent nodes have different values (green in the ﬁgure), and to
−1 otherwise (red in the ﬁgure). The utility of a conﬁgura-
tion is simply the weighted sum of the values of all features
(edges). The basic parts p consist of all the non-overlapping
2 × 2 sub-grids of x, for a total of 4 basic parts (indicated by
dotted lines in the ﬁgure).

Figure 1: Example grid conﬁguration.

Since the problem is small enough for inference of
complete conﬁgurations to be practical, we compared PCL
to standard Coactive Learning, using the implementation
of (Teso, Dragone, and Passerini 2017). In order to keep the
comparison as fair as possible, the improvements fed to CL
were chosen to match the utility gain obtained by PCL. We
further report the performance of three alternative part se-
lection strategies: random, smallest (most independent) part
ﬁrst, and UCB1.

4http://www.gecode.org/

The results can be found in the ﬁrst column of Figure 2.
We report both the regret (over complete conﬁgurations) and
the cumulative runtime of all algorithms, averaged over all
users, as well as their standard deviation. The regret plot
shows that, despite being restricted to work with 2 × 2 con-
ﬁgurations, PCL does recommend complete conﬁgurations
of quality comparable to CL after enough queries are made.
Out of the three part selection strategies, random performs
best, with the other two more informed alternatives (espe-
cially smallest ﬁrst) quite close. The runtime gap between
full and part-wise inference is already clear in this small
synthetic problem; complete inference quickly becomes im-
practical as the problem size increases.

Training planning. Generating personalized training
plans based on performance and health monitoring has re-
ceived a lot of attention recently in sport analytics (see
e.g. (Fister et al. 2015)). Here we consider the problem of
synthesizing a week-long training plan x from information
about the target athlete. Each day includes 5 time slots (two
for the morning, two for the afternoon, one for the evening),
for 35 slots total. We assume to be given a ﬁxed number of
training activities (7 in our experiments: walking, running,
swimming, weight lifting, pushups, squats, abs), as well as
knowledge of the slots in which the athlete is available. The
training plan x associates an activity to each slot where the
athlete is available. Our formulation tracks the amount of
improvement (e.g. power increase) and fatigue over ﬁve dif-
ferent body parts (arms, torso, back, legs, and heart) induced
by performing an activity for one time slot. Each day deﬁnes
a basic part.

The mapping between training activity and improve-
ment/fatigue over each body part is assumed to be provided
externally. It can be provided by the athlete or medical per-
sonnel monitoring his/her status. The features of x include,
for each body part, the total performance gain and fatigue,
computed over the recommended training plan according to
the aforementioned mapping. We further include inter-part
features to capture activity diversity in consecutive days. The
fatigue accumulated in 3 consecutive time slots in any body
parts does not exceed a given threshold, to prevent injuries.
In this setting, CL is impractical from both the cognitive
and computational points of view. We ran PCL and evaluated
the impact of user informativeness by progressively increas-
ing α from 0.1, to 0.3, to 0.5. The results can be seen in Fig-
ure 2. The plots show clearly that, despite the complexity of
the conﬁguration and constraints, PCL can still produce very
low-regret conﬁgurations after about 50 iterations or less.

Understandably, the degree of improvement α plays an
important role in the performance of PCL and, consequently,
in its runtime (users at convergence do not contribute to the
runtime), at least up to α = 0.5. Recall, however, that the
improvements are part-wise, and hence α quantiﬁes the de-
gree of local improvement: part improvements may be very
informative on their own, but only give a modest amount
of information about the full conﬁguration. However, it is
not unreasonable to expect that users to be very informative
when presented with reasonably sized (and simple) parts.

Figure 2: Regret over complete conﬁgurations (top) and cumulative runtime (bottom) of PCL and CL on our three constructive
problems: synthetic (left), training planning (middle), and hotel planning (right). The x-axis is the number of iterations, while
the shaded areas represent the standard deviation. Best viewed in color.

Crucially PCL allows the system designer to deﬁne the parts
appropriately depending on the application.

Hotel planning. Finally, we considered a complex furni-
ture allocation problem: furnishing an entire hotel. The prob-
lem is encoded as follows. The hotel is represented by a
graph: nodes are rooms and edges indicate which rooms are
adjacent. Rooms can be of three types: normal rooms, suites,
and dorms. Each room can hold a maximum number of fur-
niture pieces, each associated to a cost. Additional, ﬁxed
nodes represent bathrooms and bars. The type of a room is
decided dynamically based on its position and furniture. For
instance, a normal room must contain at most three single or
double beds, no bunk beds, and a table, and must be close
to a bathroom. A suite must contain one bed, a table and a
sofa, and must be close to a bathroom and a bar. Each room
is a basic part, and there are 15 rooms to be allocated.

The feature vector contains 20 global features plus 8 lo-
cal features per room. The global features include different
functions of the number of different types of rooms, the total
cost of the furniture and the total number of guests. The local
features include, instead, characteristics of the current room,
such as its type or the amount of furniture, and other fea-
tures shared by adjacent rooms, e.g. whether two rooms have
the same type. These can encode preferences like “suites
and dorms should not be too close”, or “the hotel should
maintain high quality standards while still being proﬁtable”.
Given the graph structure, room capacities, and total bud-
get, the goal is to furnish all rooms according to the user’s
preferences.

This problem is hard to solve to optimality with current

solvers; part-based inference alleviates this issue by focus-
ing on individual rooms. There are 15 rooms in the hotel,
so that at each iteration only 1/15 of the conﬁguration is af-
fected. Furthermore, the presence of the global features im-
plies dependences between all rooms. Nonetheless, the al-
gorithm manages to reduce the regret by an order of magni-
tude in around a 100 iterations, starting from a completely
uninformed prior. Note also that as for the training planning
scenario, an alpha of 0.3 achieves basically the same results
as those for alpha equal to 0.5.

Conclusion

In this work we presented an approach to constructive pref-
erence elicitation able to tackle large constructive domains,
beyond the reach of previous approaches. It is based on
Coactive Learning (Shivaswamy and Joachims 2015), but
only requires inference of partial conﬁgurations and partial
improvement feedback, thereby signiﬁcantly reducing the
cognitive load of the user. We presented an extensive the-
oretical analysis demonstrating that, despite working only
with partial conﬁgurations, the algorithm converges to a lo-
cally optimal solution. The algorithm has been evaluated
empirically on three constructive scenarios of increasing
complexity, and shown to perform well in practice.

Possible future work includes improving part-based in-
teraction by exchanging additional contextual information
(e.g. features (Teso, Dragone, and Passerini 2017) or expla-
nations) with the user, and applying PCL to large layout syn-
thesis problems (Dragone et al. 2016).

ings of the 18th ACM SIGKDD international conference on
Knowledge discovery and data mining, 705–713. ACM.
Shivaswamy, P., and Joachims, T. 2015. Coactive learning.
JAIR 53:1–40.
Teso, S.; Dragone, P.; and Passerini, A. 2017. Coactive
critiquing: Elicitation of preferences and features. In AAAI.
Teso, S.; Passerini, A.; and Viappiani, P. 2016. Constructive
preference elicitation by setwise max-margin learning.
In
Proceedings of the Twenty-Fifth International Joint Confer-
ence on Artiﬁcial Intelligence, 2067–2073. AAAI Press.

References
Auer, P.; Cesa-Bianchi, N.; and Fischer, P. 2002. Finite-
time analysis of the multiarmed bandit problem. Machine
learning 47(2-3):235–256.
Boutilier, C.; Bacchus, F.; and Brafman, R. I. 2001. Ucp-
networks: A directed graphical representation of conditional
In Proceedings of the Seventeenth conference on
utilities.
Uncertainty in artiﬁcial intelligence, 56–64. Morgan Kauf-
mann Publishers Inc.
Boutilier, C.; Patrascu, R.; Poupart, P.; and Schuurmans, D.
2006. Constraint-based optimization and utility elicitation
using the minimax decision criterion. Artiﬁcial Intelligence
170(8-9):686–713.
Braziunas, D., and Boutilier, C. 2005. Local utility elici-
In Proceedings of the Twenty-First
tation in GAI models.
Conference on Uncertainty in Artiﬁcial Intelligence, 42–49.
AUAI Press.
Braziunas, D., and Boutilier, C. 2007. Minimax regret based
elicitation of generalized additive utilities. In UAI, 25–32.
Braziunas, D., and Boutilier, C. 2009. Elicitation of factored
utilities. AI Magazine 29(4):79.
Chajewska, U.; Koller, D.; and Parr, R. 2000. Making ratio-
nal decisions using adaptive utility elicitation. In AAAI/IAAI,
363–369.
Dragone, P.; Erculiani, L.; Chietera, M. T.; Teso, S.; and
Passerini, A. 2016. Constructive layout synthesis via coac-
tive learning. In Constructive Machine Learning workshop,
NIPS.
Fishburn, P. C. 1967.
Interdependence and additivity in
multivariate, unidimensional expected utility theory. Inter-
national Economic Review 8(3):335–342.
Fister, I.; Rauter, S.; Yang, X.-S.; and Ljubiˇc, K. 2015. Plan-
ning the sports training sessions with the bat algorithm. Neu-
rocomputing 149:993–1002.
Goetschalckx, R.; Fern, A.; and Tadepalli, P. 2014. Coactive
learning for locally optimal problem solving. In Proceedings
of AAAI.
Gonzales, C., and Perny, P. 2004. GAI networks for utility
elicitation. KR 4:224–234.
Keeney, R. L., and Raiffa, H. 1976. Decisions with Multiple
Objectives: Preferences and Value Tradeoffs.
Mayer, R. E., and Moreno, R. 2003. Nine ways to reduce
cognitive load in multimedia learning. Educational psychol-
ogist 38(1):43–52.
Meseguer, P.; Rossi, F.; and Schiex, T. 2006. Soft con-
straints. Foundations of Artiﬁcial Intelligence 2:281–328.
Ortega, P. A., and Stocker, A. A. 2016. Human decision-
making under limited time. In Advances in Neural Informa-
tion Processing Systems, 100–108.
Pigozzi, G.; Tsouki`as, A.; and Viappiani, P. 2016. Prefer-
ences in artiﬁcial intelligence. Ann. Math. Artif. Intell. 77(3-
4):361–401.
Raman, K.; Shivaswamy, P.; and Joachims, T. 2012. Online
In Proceed-
learning to diversify from implicit feedback.

Supplementary Material

We add and subtract the term:

Proof of Theorem 2
We begin by expanding (cid:107)wT +1(cid:107)2. If T ∈ I:

(cid:107)wT +1(cid:107)2

= (cid:107)wT +1
= (cid:107)wT
= (cid:107)wT

−I T (cid:107)2 + (cid:107)wT +1
−I T (cid:107)2 + (cid:107)wT
−I T (cid:107)2 + (cid:107)wT
+ 2(cid:104)wT

I T (cid:107)2
I T + φI T (ˆxt) − φI T (xt)(cid:107)2
I T (cid:107)2 + (cid:107)φI T (ˆxt) − φI T (xt)(cid:107)2

I T , φI T (ˆxt) − φI T (xt)(cid:105)

Since T ∈ I, uT [I T ](ˆxT ) − uT [I T ](xT ) ≤ 0, thus:

I T (cid:107)2 + (cid:107)φI T (ˆxt) − φI T (xt)(cid:107)2

∞|I T |2

(cid:107)wT +1(cid:107)2

−I T (cid:107)2 + (cid:107)wT
≤ (cid:107)wT
≤ (cid:107)wT (cid:107)2 + 4D2S2

If instead T ∈ J :

(cid:107)wT +1(cid:107)2

= (cid:107)wT +1
= (cid:107)wT
= (cid:107)wT
= (cid:107)wT

−J T (cid:107)2 + (cid:107)wT +1
−J T (cid:107)2 + (cid:107)wT +1
−J T (cid:107)2 + (cid:107)wT
−J T (cid:107)2 + (cid:107)wT
+ 2(cid:104)wT

J T (cid:107)2
J T (cid:107)2
J T + φJ T (ˆxt) − φJ T (xt)(cid:107)2
J T (cid:107)2 + (cid:107)φJ T (ˆxt) − φJ T (xt)(cid:107)2

J T , φJ T (ˆxt) − φJ T (xt)(cid:105)

≤ (cid:107)wT (cid:107)2 + (cid:107)φJ T (ˆxt) − φJ T (xt)(cid:107)2
≤ (cid:107)wT (cid:107)2 + 4D2|J T |2
≤ (cid:107)wT (cid:107)2 + 4D2S2

∞|J T |2

We can therefore expand the term:

(cid:107)wT +1(cid:107)2 ≤ 4D2S2T

Applying Cauchy-Schwarz inequality:

(cid:104)w∗, wT +1(cid:105) ≤ (cid:107)w∗(cid:107)(cid:107)wT +1(cid:107)

≤ 2DS(cid:107)w∗(cid:107)

T

√

The LHS of the above inequality expands to:

(cid:104)w∗, wT +1(cid:105) = (cid:104)w∗

−I T (cid:105) + (cid:104)w∗

I T , wT

I T (cid:105)

−I T , wT
+ (cid:104)w∗

I T , φI T (ˆxT ) − φI T (xT )(cid:105)

[ if T ∈ I ]
(cid:104)w∗, wT +1(cid:105) = (cid:104)w∗

−J T , wT
+ (cid:104)w∗

−J T (cid:105) + (cid:104)w∗

J T , wT

J T (cid:105)

J T , φJ T (ˆxT ) − φJ T (xT )(cid:105)

[ if T ∈ J ]

And thus:

(cid:104)w∗, wT +1(cid:105) =

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105)

(cid:88)

t∈I

+

(cid:88)

t∈J

(cid:104)w∗

J t , φJ t(ˆxt) − φJ t(xt)(cid:105)

(cid:104)w∗

I t\J t, φI t\J t(ˆxt) − φI t\J t(xt)(cid:105)

(cid:88)

t∈J

We obtain:

(cid:104)w∗, wT +1(cid:105)

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

=

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105)

−

(cid:88)

t∈J

(cid:104)w∗

I t\J t, φI t\J t(ˆxt) − φJ t(xt)(cid:105)

=

(cid:104)w∗

I t, φI t(ˆxt) − φI t(xt)(cid:105) −

≥ α

(cid:104)w∗

I t, φI t(x∗

pt ◦ xt

¯pt) − φI t(xt)(cid:105) −

T
(cid:88)

t=1

ζ t

T
(cid:88)

t=1

ζ t

And thus:

T
(cid:88)

t=1

(cid:104)w∗

I t, φI t(x∗) − φI t(xt)(cid:105) ≤

√

2DS(cid:107)w∗(cid:107)

T

α

+

1
α

T
(cid:88)

t=1

ζ t

Dividing both sides by T proves the claim.

Proof of Proposition 3
We ﬁrst show that xt
pk
iteration t the algorithm produces xt
local subutility:

= xτk

pk for t > τk. Recall that at
pk by maximizing the

ut
Ik

(x) = ut[Jk](x) = ut[Ik \ ((cid:83)n

j=k+1 Ij)](x)

j=k+1 Ij. Each subutility
(·) is only affected by the changes in xp1 , . . . , xpk−1, as

i.e. ignoring all the features in (cid:83)n
ut
Ik
their feature subsets may intersect with Ik.

= wτ1

The subutility ut
I1

(·) only depends on the features that
are exclusively included in I1, so it is not affected by the
changes in all the other parts. At iteration τ1, the condi-
tional regret CREGp1 (xτ1 ) = 0, and thus ˆxτ1
p1 and
p1
wτ1+1
(·)
I t
will not have changed as it only depends on features exclu-
sively included in I1 and thus xt
= xτ1
p1 . Since, by assump-
p1
tion, CREGpt (xt) = 0 for all t ≥ τ1, uτ1
(·) will not change
I1
anymore and so xt
p1

= xτ1
I t . If p1 gets selected again at t > τ1, ut
I1

= xτ1
p1.
By strong induction, suppose that all parts xt
pj

pj for
all j = 1, . . . k − 1 and for all t > τk−1. At iteration τk,
CREGpk (xτk ) = 0, thus ˆxτk
pk and so the weights
pk
wτk+1
will not change. For the following t > τk iterations,
Ik
even if pk gets selected, since the subutility ut
(·) is only
pk
affected by the changes in xp1 , . . . , xpk−1 and those parts do
not change (by inductive assumption), we can conclude that
ut
(·) = uτk
(·) and that xt
pk
pk
pk
This proves that, for any k ∈ [n] and any t > τk, xt
=
pk
xτk
pk . This also means that none of the partial conﬁgurations

= xτk
pk .

= xτj

= xτk

xt
pt will change for t > τn. Since none of the partial conﬁg-
(cid:0)xˆτ1 (cid:1) = 0 by
uration changed, xˆτ1 = xτn . Since CREGp1
assumption, then xˆτ1 is conditionally optimal with respect
(cid:0)xˆτk (cid:1) = 0 for all
to p1. Likewise, xˆτk = xτn and CREGpk
k ∈ [n], and thus xT = xˆτn = xτn is a local optimum.

