5
1
0
2
 
t
c
O
 
8
1
 
 
]

G
L
.
s
c
[
 
 
1
v
4
1
2
5
0
.
0
1
5
1
:
v
i
X
r
a

Clustering Noisy Signals with Structured Sparsity Using

Time-Frequency Representation

Tom Hope

Avishai Wagner

Or Zuk

{tom.hope/avishai.wagner/or.zuk}@mail.huji.ac.il

Dept. of Statistics, the Hebrew University of Jerusalem
Mt. Scopus, Jerusalem, 91905, Israel

Abstract

We propose a simple and efﬁcient time-series clustering framework particularly suited for low Signal-
to-Noise Ratio (SNR), by simultaneous smoothing and dimensionality reduction aimed at preserving clus-
tering information. We extend the sparse K-means algorithm by incorporating structured sparsity, and use
it to exploit the multi-scale property of wavelets and group structure in multivariate signals. Finally, we
extract features invariant to translation and scaling with the scattering transform, which corresponds to a
convolutional network with ﬁlters given by a wavelet operator, and use the network’s structure in sparse
clustering. By promoting sparsity, this transform can yield a low-dimensional representation of signals that
gives improved clustering results on several real datasets.

1 Introduction

Clustering of high-dimensional signals, sequences or functional data is a common task that arises in many
domains [18, 19]. Such data come up in diverse ﬁelds, as in speech analysis, genomics, mass spectrometry,
MRI or EEG measurements, and many more.

Clustering seeks to partition data into groups with high overall similarity between members (instances) of
the same group and dissimilarity to members of other groups. For time-series signals, this means partitioning
the instances into groups of similarly behaving functions over time, where the measure of similarity is crucial
and often application-speciﬁc.

In many real-world scenarios, signals are high-dimensional (such as in genomics), noisy (as in low-quality
speech recordings), and exhibit non-stationary behavior: for example peaks and other non-smooth local pat-
terns, or changes in frequency over time. In addition, signals are often subject to translations, dilations and
deformations. These properties generally make clustering difﬁcult.

Clustering is often based on the pairwise distances between signals - but in the low Signal-to-Noise
Ratio (SNR) scenario these distances may be unreliable. An apparent solution is to ﬁrst smooth each signal,
independently of the others - but this may be sub-optimal as potentially important clustering information
could be lost when noise is too high (see Figure 1). In addition, typical feature representations and distance
measures are not invariant to the kind of transformations that occur to real-world signals, such as translations.

1

Our contributions. We introduce SPARCWave – Sparse Clustering with Wavelets – a framework of
methods for sparse clustering of noisy signals, which (i) uses “global” cross-signal information for smooth-
ing and dimensionality reduction simultaneously, by sparse clustering using time-frequency representation,
(ii) is geared towards preserving clustering information (rather than individual signals – see Figure 1), (iii)
exploits structure in the signal representation, such as multi-scale properties in univariate signals and inter-
dependencies in multivariate signals, and (iv) uses the scattering transform [1, 6, 7], that generates features
invariant to translations and dilations and robust to small deformations in both time and frequency. We use
the natural structure in the scattering coefﬁcients in our sparse clustering method.

Our methods achieve higher clustering accuracy, compared to methods available in the literature, in sim-
ulations on both univariate and multivariate signals, and on real-world datasets from different domains. We
implemented our algorithms and simulations in a python software package “SPARCWave” available at github
https://github.com/avishaiwa/SPARCWave.

Figure 1: Simulated cluster data and SPARCWave’s cross-signal smoothing and dimensionality reduction. Clustering results for six clusters (I
- V I): Flat curve, Heavisine, Blocks, Bumps, Doppler, Piecewise Polynomial. We generated each individual signal by applying additive Gaussian noise

with σ = 2.5. There are 30 signals for each cluster. We ran SPARCWave as described in the text. (a) True cluster centers. (b) One individual curve from

each cluster. SNR is too low to allow individual curve smoothing to reliably estimate cluster centers. (c) Cluster centers returned by SPARCWave resemble

the original clusters (d) Wavelet smoothing of returned cluster centers improves estimation of original cluster centers. (e) Wavelet coefﬁcients for each true

cluster center from II. − V I. (except for the trivial ﬂat curve) are ordered from the ﬁnest (left) to the coarsest level (right), shown in green. The wi

coefﬁcients ﬁtted in SPARCWave are shown in red. Although a few wavelet coefﬁcients at ﬁne resolutions are large for some of the curves, the informative

coefﬁcients for clustering are all at the coarsest levels.

2 Related Work

Much work has been done both recently and in the past on using time-frequency representation of signals to
cluster them, primarily with wavelets. Giacofci et al. [14] ﬁrst perform wavelets-based denoising for each
signal, and then use model-based clustering on the union set of non-zero coefﬁcients after shrinkage. How-
ever, informative features can be thresholded by single-signal denoising due to the lack of global information
highlighting their importance (See Figure 1).

Antoniadis et al. [2] use the fact that the overall energy of a signal x ∈ RT can be decomposed into a
multi-scale view of the energy, and propose extracting features that involve averaging wavelet coefﬁcients in
each scale and measuring each scale’s relative importance. The averaging of all wavelets at the same scale
in [2] increases SNR and makes the method robust to translations. However, this averaging loses the time

2

information: two different signals with the same power spectrum will have very similar v(j) coefﬁcients,
which may lead to clustering errors.

Non-rigid transformation are typically present for real world signals, making the computed pairwise dis-
tances between signals unreliable if these are not taken into account. A common approach to dealing with
deformations is the Dynamic Time Warping approach (DTW) [5], which uses dynamic programming to align
pairs of signals. However, usage of DTW for our problem is limited by the fact that SNR is typically too low,
such that two individual signals do not contain enough information in order to reliably align them. In our
simulations and real-data applications, we did not obtain satisfactory results with DTW.

2.1 Sparse Clustering

Several methods for sparse clustering have recently been proposed. For example, in [3] the authors consider
a two-component Gaussian Mixture Model (GMM), and provide an efﬁcient sparse estimation method with
theoretical guarantees. While our approach could make use of any sparse clustering method, we chose sparse
K-means [26] for its simplicity and ease of implementation. This simplicity allowed us to more readily incor-
porate our extensions to structured sparsity for both univariate and multivariate signals. We use rich feature
representations, and hope for linear cluster separability in this feature space. By using these representations,
we are able to achieve good results with the simpler (sparse) K-means, which corresponds to assuming a
spherical GMM.

3 Methods

We ﬁrst provide a brief description of wavelets. Next, we introduce our SPARCWave methods - using sparse
K-means in the wavelet domain, incorporating group structure for univariate and multivariate signals, and
ﬁnally applying the scattering transform to obtain transformation-invariant features.

3.1 Wavelets Background

Wavelets are smooth and quickly vanishing oscillating functions, often used to represent data such as curves
or signals in time. More formally, a wavelet family Ψj,k is a set of functions generated by dilations and
translations of a unique mother wavelet Ψ:

Ψj,k(t) = a

j

2 Ψ(ajt − k),

where j, k ∈ Z and a > 0. The constant a represents resolution in frequency, j represents scale and k
translations. We similarly deﬁne a family of functions φ0,k derived from a scaling function φ(t) by using the
dilation and translation formulation given in eq. (1).

Any function f ∈ L2(R) can then be decomposed in terms of these functions:

f (t) =

ckφ0,k(t) +

vj,kΨj,k(t),

(cid:88)

k∈Z

(cid:88)

j,k∈Z

where ck = (cid:104)φ0,k(t), f (t)(cid:105) are the scaling coefﬁcients, and vj,k = (cid:104)Ψj,k(t), f (t)(cid:105) are called the wavelet

coefﬁcients of f (t).

This decomposition has a discrete version known as the Discrete Wavelet Transform (DWT). Given
x1, . . . , xT forming a signal x ∈ RT sampled at times t = 1, .., T , where T = 2J , and taking a = 2,

(1)

(2)

3

the DWT of x is given by taking c0 and vj,k for j = 0, .., J − 1 and k = 0, .., 2j in eq. (2), where φ0,0, Ψj,k
are evaluated at t = 1, .., T to compute the (discrete) inner products (cid:104)φ0,0, x(cid:105) and (cid:104)Ψj,k, x(cid:105). The DWT can
be written conveniently in a matrix form v = Wx where W is an orthogonal matrix deﬁned by the chosen
Ψ, φ, and v is the vector of coefﬁcients c0, Ψj,k properly ordered.

Many real-world signals are approximately sparse in the wavelet domain. This property is typically used

for signal denoising using a three-step procedure [10]:

ˆx ≡ W−1ηλ(Wx)

where ηλ is a nonlinear shrinkage (smoothing) operator, for example entry-wise soft-threshold operator

with parameter λ: ηλ(v) = sgn(v)(|v| − λ)+.

3.2 Sparse Clustering with Wavelets – Univariate Signals

Consider n instances (signal vectors) x(i) ∈ RT , and let v(i) be their DWT transforms. Let di1,i2,j be the
squared (Euclidean) distance between v(i1) and v(i2) over coordinate j, di1,i2,j ≡ (v(i1)
)2. Let Ck
be the set of indices corresponding to cluster k with |Ck| = nk. In standard K-means clustering, the goal is
to maximize the Between Cluster Sum of Squares (BCSS), which can be represented as a sum 1T D where D
is a vector representing the contribution of the wavelet coefﬁcient j to the BCSS, with

j − v(i2)

j

n
(cid:88)

Dj ≡

1
n

di1,i2,j −

K
(cid:88)

1
nk

i1,i2=1

k=1

i1,i2∈Ck

(cid:88)

di1,i2,j.

Let w ∈ RT be a vector of weights, and s a tuning parameter bounding (cid:107)w(cid:107)1. In sparse K-means we
maximize a weighted sum of the contribution from each coordinate, wT D [26]. We get the following sparse
K-means [26] constrained optimization problem, where sparsity is promoted in the wavelet domain:

(3)

(4)

(5)

wT D

argmax
C1,C2,...Ck,w
s.t. w ∈ B2(0, 1) ∩ B1(0, s) ∩ RT
+

where Bp(0, r) ∈ RT is the Lp ball centered at zero with radius r, and RT

+ is the positive orthant,

RT

+ ≡ {w ∈ RT , wj ≥ 0, ∀j = 1, .., T }.

For signals having good sparse approximation in the wavelet domain, we expect that clustering infor-
mation will also be localized to a few wavelet coefﬁcients, where high wj values correspond to informative
coefﬁcients (See Figure 1). In [26] the authors propose an iterative algorithm for solving Problem (5), where
each iteration consists of updating the clusters Cj by running K-means, and updating w with a closed-form
expression. The tuning parameter s is set by the permutation-based Gap Statistic method [24].

3.3 Group Sparse Clustering

In many cases, the features upon which we cluster could have a natural structure. A common example that has
seen much interest is group structure, where features come in groups (blocks of pixels in images, neighboring
genes, etc.).

Exploiting structured sparsity has been shown to be beneﬁcial in the context of supervised learning (re-
gression, classiﬁcation), compressed sensing and signal reconstruction [4, 17]. Here, we extend the idea of

4

using group sparsity to clustering, and propose the following Group-Sparse K-means.

Let G be a partition of {1, .., T } and denote by v(g) the elements of w corresponding to group g ∈ G. Let

|g| the number of elements in g. Deﬁne the group-norm with respect to the partition G [4],

||v||G ≡

(cid:112)|g|||v(g)||2

(cid:88)

g∈G

where we multiply the vector norms ||v(g)||2 by (cid:112)|g| as in [8] and [11], to give greater penalties to larger
groups (other coefﬁcients may also be used).

Using the group norm, we deﬁne the group-sparse K-means problem, where the tuning-parameter s con-
trols group sparsity, with B(G)(0, s) the ball of radius s centered at zero with respect to the group norm
|| · ||G.

wT D

argmax
C1,C2,...CK ,w
s.t. w ∈ B2(0, 1) ∩ B(G)(0, s) ∩ RT
+

Problem (7) generalizes the sparse K-means problem in eq. (5), which is obtained here by setting all

groups to have size 1.

Problem (7) can be solved iteratively as shown in Algorithm 1. (
√

· and (cid:12) in step (a.) represent element-
wise vector operations: [
wj and [w (cid:12) v]j = wjvj). Optimization w.r.t w can be done with
standard Second Order Cone Programming (SOCP) solvers by introducing auxiliary variables. We used the
convex optimization toolbox CVX [15].

w]j =

√

√

(6)

(7)

Algorithm 1 Group-Sparse K-means
Input: v(1), v(2), ..., v(n), K, iters, G, s
Output: Clusters C1, C2, ..., CK

1. Initialize: Set w1 = w2 = ... = wT = 1√
T

2. for i=1 : iters

(a) Apply K-means on

w (cid:12) v(1), ...,

w (cid:12) v(n)

√

√

(b) Hold C1, ..., CK ﬁxed, solve (7) w.r.t w, with tuning parameter s and partition G

3. Return C1, ..., CK

We next show two applications of group-sparse clustering in the wavelets domain: (i.) using group
sparsity to exploit structure of the wavelet coefﬁcients, and (ii.) using group sparsity to exploit correlations
between different curves in multivariate signal clustering.

3.3.1 Exploiting Structure of Wavelet Coefﬁcients using Group Sparsity

In our time-series clustering context, each scale g of the wavelets coefﬁcients can be thought of as a group,
together forming a multi-scale view of the signal. Since the elements of each group represent “similar”
information about individual signals, it is natural to expect that they also express similar information with
respect to the clustering task. Therefore, we deﬁne the partitions G = (cid:8)gj = {w2j , ..., w2j+1−1}, j =
0, .. log2(T )(cid:9) with |gj| = 2j, and solve Problem (7) with the group norm deﬁned by G.

5

Figure 2: Multivariate cluster data. Each column (color) is a multivariate cluster center with G variables (here G = 3). Each instance is X(i) ∈
RG×T , with Gaussian additive noise added to each cluster center.

Our choice of G in this work is not the only one possible exploiting the structure of the wavelets coefﬁ-
cients tree - for example, one may also choose groups corresponding to connected sub-trees (particular nodes
and all of their descendent in the tree). Finally, by allowing partitions G with group overlaps, it is possible to
get a richer structure for wavelet coefﬁcients, for example by using tree-structured sparsity as in [8].

3.3.2 Clustering Multivariate Signals with Group Sparsity

In contrast to the univariate signals discussed so far, in many cases signals are multivariate, and are com-
posed of groups of univariate signals. A simple adaptation of group sparsity enables us to make use of this
multivariate structure too.

Let each instance X(i) be a multivariate signal, represented as a matrix X(i) ∈ RG×T where G is the
number of variables and T as above (See Figure 2). We transform each row of X(i) with DWT to get V(i).
We then concatenate the rows of each V(i) into one vector of length G × T and solve Problem (7) with the
group norm G deﬁned by G = (cid:8)gt = {t, t + T , . . . , t + GT }, t = 1, .., T (cid:9). Each group corresponds to a
“vertical time-frequency slice” across the G univariate signals comprising X(i).

Intuitively, this structure reﬂects the prior assumption that if at some point t on the time-frequency axis we
have an “active clustering feature”, then we expect it to be active across all G univariate signals comprising
each X(i), but also enables ﬂexibility by allowing {wt, wt+T , . . . , wt+GT } to be different.

3.4 The Scattering Transform

The wavelet transform is not invariant to translations and scaling. In addition, while the coarse-grain coefﬁ-
cients are stable under small deformations, the ﬁne resolution coefﬁcients are unstable [20].

To overcome the effects of the above transformations we use the scattering transform [20], a recent exten-
sion of wavelets which is stable to such deformations. The scattering representation of a signal is built with
a non-linear, unitary transform computed in a manner resembling a deep convolutional network where ﬁlter
coefﬁcients are given by a wavelet operator [1].

More formally, a cascade of three operators - wavelet decomposition, complex modulus, and local aver-

aging - is used as follows. For a function f (t) the 0-th scattering layer is obtained by a convolution,

S(0) ≡ f (cid:63) φ

6

(8)

where f (cid:63) g(t) ≡ (cid:82) ∞

s=−∞ f (s)g(t − s)ds and φ is an averaging ﬁlter. Applying the ﬁlter φ provides some
local translation-invariance but loses high frequency information - this information is kept by an additional
convolution with a wavelet transform described next.

We start by constructing the ﬁlters we use in layer 1. Let {Ψ(1)
j1

dilations of a complex mother wavelet function Ψ for a set of indices J1, with Ψ(1)
j1

}j1∈J1 be a ﬁlter-bank containing different
(t) = aj1

1 Ψ(aj1

1 t).

To regain translation invariance lost by applying Ψ(1)
j1

and obtain the scattering functions of the ﬁrst layer:

, we perform an additional convolution step with φ

S(1)
j1

≡ |f (cid:63) Ψ(1)
j1

| (cid:63) φ,

∀j1 ∈ J1

(9)

where the absolute value | · | is a contraction operator - this operator reduces pairwise distances between

signals, and prevents explosion of the energy propagated to deeper scattering layers [20].

To recover the high frequencies lost by averaging, we again take convolutions of |x (cid:63) Ψ(1)
j1

|, with Ψ(2)
j2
for some j2 ∈ J2. More generally, deﬁne Ψ(l)
l t) where al determines the dilation frequency
jl
resolution at layer l. For a scattering layer l, let (j1, .., jl) deﬁne a scale path along the scattering network.
This process above continues in the same way, with the scattering functions at the l-th layer given by

(t) = ajl

l Ψ(ajl

S(l)
j1,..,jl

≡ |..||f (cid:63) Ψ(1)
j1

| (cid:63) Ψ(2)
j2

| (cid:63) Ψ(3)
j3

|(cid:63), ..., (cid:63)Ψ(l)
jl

| (cid:63) φ, ∀(j1, .., jl) ∈ J1 × J2 × .. × Jl.

(10)

The corresponding scattering coefﬁcients v(l)
j1,..,jl
In practice, we let a resolution parameter ∆t determine the points at which we evaluate S(l)

(t) are obtained by evaluating S(l)

j1,..,jl

at time t.

j1,..,jl

, where for

each S(L)
the ﬁnal scattering layer L, obtaining scattering coefﬁcients in layers l = 0, 1, .., L.

we only sample at points t = k∆t

2 with k = 0, . . . , (cid:100) 2T
∆t

j1,..,jl

(cid:101). We stop the process when reaching

We empirically found that in many cases the scattering transform can give a sparse representation of

signals, and that this sparsity helps in the clustering task (see Section 5 for real-data results).

3.4.1 Exploiting Scattering Group Structure

The scattering coefﬁcients have a natural group structure which we exploit in our clustering. For each function
S(l)
, we group all coefﬁcients resulting from the evaluation of this function. We thus solve Problem (7)
j1,..,jl
with the following deﬁnition of groups:

G = (cid:8)gj1,...,jl , ∀l = 0, .., L, ∀(j1, ..., jl) ∈ J1 × .. × Jl

(cid:9)

where

gj1,j2,...,jl =

(cid:110)

v(l)
j1,...,jl

(

k∆t
2

) : k = 0, . . . , (cid:100)

(cid:111)

(cid:101)

2T
∆t

(11)

(12)

4 Simulation Results

For univariate signals, we take curves of dimension 256 from [10] (using the waveband R package) and pad
them with 128 zeros on both ends resulting in K = 6 cluster centers with T = 512 (see Figure 1). Clusters are
uniformly sized. We apply independent additive Gaussian noise N (0, σ2) to generate individual signals. We
clustered signals using K-means (picking the best of 100 random starts), sparse K-means on the raw data as
in [26] (with the sparcl R package, best of 100 random starts, selecting s using the gap statistic method), and
the methods of [14] (using the curvclust R package with burn-in period of 100) and [2] (using code provided

7

by the authors). For our group-sparse K-means we used CVXPY [9], with tuning parameter s selected with
the gap statistic method. In all relevant cases, we used the Symmlet-8 wavelet. We used the Adjusted Rand
Index (ARI) as a measure of clustering accuracy [25]. Results are shown in Figure 3.

Group sparsity improved accuracy substantially when the number of curves n is particularly low (and
thus so is SNR). When n grows, the more ﬂexible method that does not impose group constraints picks up
and the two methods reach about the same accuracy.

Figure 3: Univariate simulation results. Average ARI as a function of number of signals over 1000 simulations, for the simulation described in text
with σ = 2.75

For multivariate signals, we select curves as shown in Figure 2. Each univariate signal is of dimension 128
padded with 64 zeros on each end, resulting in K = 5 cluster centers with G = 3, T = 256. We compared
our Group-Sparse K-means method to (i) K-means, (ii) sparse K-means applied to a concatenation of all
signals (termed SPARCWave-Concat) (iii) a Hidden Markov Model approach in [13] (HMM) (code provided
by the authors), and (iv) a multivariate PCA-similarity measure [27] to construct a pairwise distance matrix,
which we use in spectral clustering [21] (PCA) (self-implementation). Group sparsity improved clustering
accuracy substantially, especially for low n.

8

Figure 4: Multivariate simulation results. The average ARI is shown as a function of number of signals over 1000 simulations, for the simulation
described in text with σ = 1.75.

5 Real-Data Applications

We tested the SPARCWave framework on 3 real-world datasets. In all cases, using the scattering transform
improved clustering accuracy. We used the implementation in the scatnet MATLAB software (available at
http://www.di.ens.fr/data/software/). We used complex Morlet wavelets for all scattering
layers and a moving average for φ. See the Appendix for speciﬁc parameters used in our implementations.
Results for the different methods on all datasets are shown in Table 1.

Figure 5: Clustering results for Berkeley Growth dataset. (a) Fanplots showing the distribution of ﬁrst-order differences of the curves in each
cluster, representing the growth rate for boys and girls as function of age. Intra-class variation increases at ages around 12 − 16 (indicated by larger faded

gray area), motivating the use of the scattering transform which is robust to deformations. (b) In the ﬁrst two heat maps, scattering coefﬁcients are shown

for the growth rate curves in each cluster, with colors representing coefﬁcient magnitude. Each row represents a point t at which we evaluate the coefﬁcient

(see Section 3.4). The third heat map represents the SPARCWave weights w corresponding to each coefﬁcient.

9

5.1 Berkeley Growth Study

The Berkeley growth dataset [22] consists of height measurements for 54 girls and 38 boys between the ages
of 1 and 18 years giving T = 30 data points for each signal. Of interest to researchers is the “velocity” of
the growth curve, i.e. the rate of change. We thus use ﬁrst-order differences of the curves. However, as a
result of time shifts among individuals, simply looking at the cross-sectional mean fails to capture important
common growth [23], which is also not fully captured by the standard wavelet transform. We applied the
scattering transform together with sparse clustering, which lead to substantial improvements in accuracy.
Applying sparse K-means on the scattering features led to the about the same results as ordinary K-means in
this feature space, with ARI = 0.87).

The weights wj selected in the optimization problems (5) or (7) may be sub-optimal for clustering. The
features with high-magnitude coefﬁcients can, however, be used in standard K-means, viewing SPARCWave
as a feature-selection step [26]. We can select all non-zero features wj (or above some threshold) and apply
K-means using only these features. This technique improved accuracy from 0.87 to 0.91.

Group-sparse clustering with the scattering transform lead to similar results, possibly due to the very short

signal length T (see Table 1 and Figure 5).

5.2 Phoneme log-periodograms

The Phoneme dataset contains 4507 noisy log-periodograms corresponding to recordings of speakers of 32
ms duration with T = 256. There are K = 5 phonemes: ”sh” (800 instances), ”dcl” (757), ”iy” (1163), ”aa”
(695), ”ao” (1022) (See [16] and http://statweb.stanford.edu/˜tibs/ElemStatLearn/datasets/
phoneme.info.txt). Each instance is a noisy estimate of a spectral-density function. De-noising, as
suggested for example in [12], is thus attractive here. By applying the scattering transform on data in the
log-frequency domain, we obtain invariance to translation in log-frequency and consequently to scaling in
frequency. This is similar to the approach taken in [1], where the scattering transform is applied to log-
frequencies. Here too group-sparse clustering did not change accuracy, possibly due to large sample size (see
Table 1 and Figure 7 in the Appendix).

5.3 Wheat Spectra

The Wheat dataset consists of near-infrared reﬂectance spectra of 100 wheat samples with T = 701, measured
in 2nm intervals from 1100 to 2500nm, and an associated response variable, the samples’ moisture content.
The response variable is clearly divided into two groups: 41 instances with moisture content lower than 15,
and 59 with moisture content above 15. We use this grouping to create 2 classes. Applying the scattering
transform in combination with sparse clustering improved accuracy signiﬁcantly. However, using group
sparsity reduced accuracy, perhaps because out of 924 features only 2 are found to have non-zero weights
(see Table 1 and Figure 8 in the Appendix).

6 Conclusion and Further Work

We proposed a method for time-series clustering that uses a “built-in” shrinkage of wavelet coefﬁcients based
on their contribution to the clustering information. We extended the sparse K-means framework by incor-
porating group structure and used it to exploit wavelet multi-resolution properties in univariate signals, and
multivariate structure. An interesting future direction is to adapt this approach to other sparse clustering

10

Table 1: Clustering accuracy (ARI) for three real datasets. The SPARCScatter method shows superior
clustering accuracy over all datasets.

Method
SPARCScatter
SPARCScatter Group
Scattering + K-means
SPARCWave
Antoniadis [2]
Giacofci [14]
K-means

Growth Phoneme Wheat
0.73
0.73
0.66
0.30
0.34
0.69
0.68

0.46
0.3
0.30
0.31
0.35
0.30
0.31

0.91
0.91
0.87
0.62
0.62
0.58
0.58

methods. Another direction is to incorporate richer structures, such as tree-sparsity in the wavelet and scat-
tering coefﬁcients, long-range dependencies and interdependencies in multivariate signals. In this work we
applied sparse clustering to one-dimensional signals. The wavelets transform is widely used to represent two-
dimensional signals such as images. In addition, a two-dimensional scattering transform achieved excellent
results in supervised image classiﬁcation tasks due to its invariance to translation, dilation and deformation
[6, 7]. It is thus natural to apply our approach to sparse clustering of images and other multi-dimensional
datasets. Finally, there are few known theoretical guarantees for sparse-clustering methods, and it would be
interesting to develop such guarantees to the framework in [26] and it’s group extension we have proposed.

Acknowledgements

We thank Joakim And´en for suggesting the scattering transform and for many useful comments and discus-
sions.

11

References

62(16):4114–4128, 2014.

[1] J. And´en and S. Mallat. Deep scattering spectrum.

IEEE Transactions on Signal Processing,

[2] A. Antoniadis, X. Brossat, J. Cugliari, and J.-M. Poggi. Clustering functional data using wavelets.
International Journal of Wavelets, Multiresolution and Information Processing, 11(01):1350003, 2013.

[3] M. Azizyan, A. Singh, and L. Wasserman. Efﬁcient sparse clustering of high-dimensional non-spherical
gaussian mixtures. In Proceedings of the 18th International Conference on Artiﬁcial Intelligence and
Statistics, 2015.

[4] F. Bach, R. Jenatton, J. Mairal, G. Obozinski, et al. Convex optimization with sparsity-inducing norms.

Optimization for Machine Learning, pages 19–53, 2011.

[5] D. J. Berndt and J. Clifford. Using dynamic time warping to ﬁnd patterns in time series.

In KDD

workshop, volume 10, pages 359–370. Seattle, WA, 1994.

[6] J. Bruna and S. Mallat. Classiﬁcation with scattering operators. arXiv preprint arXiv:1011.3023, 2010.

[7] J. Bruna and S. Mallat.

Invariant scattering convolution networks. Pattern Analysis and Machine

Intelligence, IEEE Transactions on, 35(8):1872–1886, 2013.

[8] C. Chen and J. Huang. Compressive sensing MRI with wavelet tree sparsity. In Advances in neural

information processing systems, pages 1124–1132, 2012.

[9] S. Diamond, E. Chu, and S. Boyd. CVXPY: A Python-embedded modeling language for convex opti-

mization, version 0.2. http://cvxpy.org/, May 2014.

[10] D. L. Donoho. De-noising by soft-thresholding. IEEE Transactions on Information Theory, 41(3):613–

[11] J. Friedman, T. Hastie, and R. Tibshirani. A note on the group lasso and a sparse group lasso. arXiv

preprint arXiv:1001.0736, 2010.

[12] H.-Y. Gao. Choice of thresholds for wavelet estimation of the log spectrum. Stanford Technical Report,

627, 1995.

1993.

[13] S. Ghassempour, F. Girosi, and A. Maeder. Clustering multivariate time series using hidden markov
models. International journal of environmental research and public health, 11(3):2741–2763, 2014.

[14] M. Giacofci, S. Lambert-Lacroix, G. Marot, and F. Picard. Wavelet-based clustering for mixed-effects

functional models in high dimension. Biometrics, 69(1):31–40, 2013.

[15] M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming, version 2.1. http:

//cvxr.com/cvx, Mar. 2014.

[16] T. Hastie, R. Tibshirani, and J. Friedman. The elements of statistical learning: Data mining, inference

and prediction, 2nd ed. Springer, New York, 2009.

[17] J. Huang, T. Zhang, and D. Metaxas. Learning with structured sparsity. The Journal of Machine

Learning Research, 12:3371–3412, 2011.

[18] J. Jacques and C. Preda. Functional data clustering: a survey. Advances in Data Analysis and Classiﬁ-

cation, pages 1–25, 2014.

[19] W. T. Liao. Clustering of time series data: a survey. Pattern recognition, 38(11):1857–1874, 2005.

[20] S. Mallat. Group invariant scattering. Communications on Pure and Applied Mathematics, 65(10):1331–

1398, 2012.

12

[21] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm. Advances in

neural information processing systems, 2:849–856, 2002.

[22] J. O. Ramsay and B. W. Silverman. Functional Data Analysis, 2nd ed. Springer, New York, 2006.

[23] R. Tang and H.-G. M¨uller. Pairwise curve synchronization for functional data. Biometrika, 95(4):875–

889, 2008.

[24] R. Tibshirani, G. Walther, and T. Hastie. Estimating the number of clusters in a data set via the gap
statistic. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2):411–423,
2001.

[25] S. Wagner and D. Wagner. Comparing clusterings: an overview. Universit¨at Karlsruhe, Fakult¨at f¨ur

Informatik Karlsruhe, 2007.

[26] D. M. Witten and R. Tibshirani. A framework for feature selection in clustering. Journal of the American

Statistical Association, 105(490):713–726, 2010.

[27] K. Yang and C. Shahabi. A PCA-based similarity measure for multivariate time series. In Proceedings

of the 2nd ACM international workshop on Multimedia databases, pages 65–74. ACM, 2004.

13

Appendix

6.1 Evaluating Performance

To evaluate the clustering performance we used the Adjusted Rand Index (ARI) as a measure of clustering
accuracy [25], deﬁned as:

ARI ≡

(cid:80)

(cid:0)nij
2

i<j

1

2 [(cid:80)

i

(cid:0)ai
2

(cid:1) + (cid:80)

(cid:1) − 1
[(cid:80)
i
(n
2)
(cid:1)] − 1
(cid:0)bj
(n
2)
2

j

(cid:0)ai
2
[(cid:80)
i

(cid:1)][(cid:80)
j
(cid:0)ai
2

(cid:0)bj
2

(cid:1)]
(cid:1)][(cid:80)

j

(cid:0)bj
2

(cid:1)]

(13)

where ai denotes the number of signals whose true cluster is i, , bj the number of signals assigned to
cluster j and nij the number of signals whose true cluster is i and are assigned to cluster j, with n =
(cid:80)

i ai = (cid:80)

j bj = (cid:80)

i,j nij.

6.2 Additional Figures for Simulation

The coefﬁcients of the clustering in simulations are shown in Figure 6.

Figure 6: Simulated cluster data coefﬁcients. Wavelet coefﬁcients for six clusters from the simulation described in main text (see Figure 1): I. Flat
curve, II. Heavisine, III. Blocks, IV . Bumps, V . Doppler, V I. Piecewise polynomial. (a) Coefﬁcients for true cluster centers.(b) Coefﬁcients for

one individual curve from each cluster. SNR is too low to allow individual curve smoothing to reliably estimate cluster centers. (c) Coefﬁcients of cluster

centers returned by SPARCWave resemble the original clusters coefﬁcients. (d) Coefﬁcients after Wavelets smoothing of returned cluster centers improves

estimation of original cluster centers for cluster center from II. − V I. (except for the trivial ﬂat curve), shown in blue. The wi coefﬁcients ﬁtted in

SPARCWave, shown in red. Although a few wavelet coefﬁcients at ﬁne resolutions are large for some of the cluster centers, the informative coefﬁcients for

clustering are all at the coarsest levels.

14

6.3 Additional Details and Figures for Real Data

Figure 7: Clustering results for the Phoneme dataset. (a) Fanplots as in Figure 5. (b) Heat maps for different clusters and for weighs as in Figure 5.
Most of the clustering information is in the 0-th layer.

Figure 8: Clustering results for the Wheat dataset. (a) Fanplots as in Figure 5. (b) Heat maps for different clusters and for weighs as in Figure 5.
Most of the clustering information is in the 0-th layer.

15

Table 2: Parameters for running Scattering transform for each of the three real datasets. We used L = 2 in all
datasets. The different sizes T of the original vectors lead to different sizes for the scattering feature vectors
v. In the scatnet software implementation, a few of the scattering coefﬁcients v(l)
(k∆t) are ﬁltered. The
choice of the frequency resolutions a1 and a2 were different for different datasets - for audio datasets (such
as Phenome), it is known that higher frequency is required to represent the information in a signal [1], and
we used a1 = 21/8. The SPARCScatter method selects only a few of the scattering coefﬁcients at the top
two layers.

j1,..,jl

Parameter Growth

Phoneme Wheat

n
T
K
L
|v|
||w||0
∆t
(a1, a2)

92
30
2
2
42
10
32
(21/2, 2)

4507
256
5
2
400
16
32
(21/8, 2)

100
701
2
2
924
2
32
(21/8, 2)

16

