5
1
0
2
 
r
a

M
 
2
2
 
 
]
L
C
.
s
c
[
 
 
4
v
6
6
1
4
.
1
1
4
1
:
v
i
X
r
a

Retroﬁtting Word Vectors to Semantic Lexicons

Manaal Faruqui

Jesse Dodge
Chris Dyer Eduard Hovy Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{mfaruqui,jessed,sjauhar,cdyer,ehovy,nasmith}@cs.cmu.edu

Sujay K. Jauhar

Abstract

Vector space word representations are learned
from distributional information of words in
large corpora. Although such statistics are
semantically informative, they disregard the
valuable information that is contained in se-
mantic lexicons such as WordNet, FrameNet,
and the Paraphrase Database. This paper
proposes a method for reﬁning vector space
representations using relational information
from semantic lexicons by encouraging linked
words to have similar vector representations,
and it makes no assumptions about how the in-
put vectors were constructed. Evaluated on a
battery of standard lexical semantic evaluation
tasks in several languages, we obtain substan-
tial improvements starting with a variety of
word vector models. Our reﬁnement method
outperforms prior techniques for incorporat-
ing semantic lexicons into word vector train-
ing algorithms.

1

Introduction

Data-driven learning of word vectors that capture
lexico-semantic information is a technique of cen-
tral importance in NLP. These word vectors can
in turn be used for identifying semantically related
word pairs (Turney, 2006; Agirre et al., 2009) or
as features in downstream text processing applica-
tions (Turian et al., 2010; Guo et al., 2014). A vari-
ety of approaches for constructing vector space em-
beddings of vocabularies are in use, notably includ-
ing taking low rank approximations of cooccurrence
statistics (Deerwester et al., 1990) and using internal
representations from neural network models of word
sequences (Collobert and Weston, 2008).

Because of their value as lexical semantic repre-
sentations, there has been much research on improv-

ing the quality of vectors. Semantic lexicons, which
provide type-level information about the semantics
of words, typically by identifying synonymy, hyper-
nymy, hyponymy, and paraphrase relations should
be a valuable resource for improving the quality of
word vectors that are trained solely on unlabeled
corpora. Examples of such resources include Word-
Net (Miller, 1995), FrameNet (Baker et al., 1998)
and the Paraphrase Database (Ganitkevitch et al.,
2013).

Recent work has shown that by either changing
the objective of the word vector training algorithm
in neural language models (Yu and Dredze, 2014;
Xu et al., 2014; Bian et al., 2014; Fried and Duh,
2014) or by relation-speciﬁc augmentation of the
cooccurence matrix in spectral word vector models
to incorporate semantic knowledge (Yih et al., 2012;
Chang et al., 2013), the quality of word vectors can
be improved. However, these methods are limited to
particular methods for constructing vectors.

The contribution of this paper is a graph-based
learning technique for using lexical relational re-
sources to obtain higher quality semantic vectors,
which we call “retroﬁtting.” In contrast to previ-
ous work, retroﬁtting is applied as a post-processing
step by running belief propagation on a graph con-
structed from lexicon-derived relational information
to update word vectors (§2). This allows retroﬁtting
to be used on pre-trained word vectors obtained
using any vector training model.
Intuitively, our
method encourages the new vectors to be (i) simi-
lar to the vectors of related word types and (ii) simi-
lar to their purely distributional representations. The
retroﬁtting process is fast, taking about 5 seconds for
a graph of 100,000 words and vector length 300, and
its runtime is independent of the original word vec-
tor training model.

tors to be retroﬁtted (and correspond to VΩ); shaded
nodes are labeled with the corresponding vectors in
ˆQ, which are observed. The graph can be interpreted
as a Markov random ﬁeld (Kindermann and Snell,
1980).

The distance between a pair of vectors is deﬁned
to be the Euclidean distance. Since we want the
inferred word vector to be close to the observed
value ˆqi and close to its neighbors qj, ∀j such that
(i, j) ∈ E, the objective to be minimized becomes:

Figure 1: Word graph with edges between related words
showing the observed (grey) and the inferred (white)
word vector representations.

Ψ(Q) =


αi(cid:107)qi − ˆqi(cid:107)2 +

n
(cid:88)

i=1

(cid:88)

(i,j)∈E

βij(cid:107)qi − qj(cid:107)2





Experimentally, we show that our method works
well with different state-of-the-art word vector mod-
els, using different kinds of semantic lexicons and
gives substantial
improvements on a variety of
benchmarks, while beating the current state-of-the-
art approaches for incorporating semantic informa-
tion in vector training and trivially extends to mul-
tiple languages. We show that retroﬁtting gives
consistent improvement in performance on evalua-
tion benchmarks with different word vector lengths
and show a qualitative visualization of the effect of
retroﬁtting on word vector quality. The retroﬁtting
is available at: https://github.com/
tool
mfaruqui/retrofitting.

2 Retroﬁtting with Semantic Lexicons

Let V = {w1, . . . , wn} be a vocabulary, i.e, the set
of word types, and Ω be an ontology that encodes se-
mantic relations between words in V . We represent
Ω as an undirected graph (V, E) with one vertex for
each word type and edges (wi, wj) ∈ E ⊆ V × V
indicating a semantic relationship of interest. These
relations differ for different semantic lexicons and
are described later (§4).

The matrix ˆQ will be the collection of vector rep-
resentations ˆqi ∈ Rd, for each wi ∈ V , learned
using a standard data-driven technique, where d is
the length of the word vectors. Our objective is
to learn the matrix Q = (q1, . . . , qn) such that the
columns are both close (under a distance metric) to
their counterparts in ˆQ and to adjacent vertices in Ω.
Figure 1 shows a small word graph with such edge
connections; white nodes are labeled with the Q vec-

where α and β values control the relative strengths
of associations (more details in §6.1).

In this case, we ﬁrst train the word vectors inde-
pendent of the information in the semantic lexicons
and then retroﬁt them. Ψ is convex in Q and its so-
lution can be found by solving a system of linear
equations. To do so, we use an efﬁcient iterative
updating method (Bengio et al., 2006; Subramanya
et al., 2010; Das and Petrov, 2011; Das and Smith,
2011). The vectors in Q are initialized to be equal
to the vectors in ˆQ. We take the ﬁrst derivative of Ψ
with respect to one qi vector, and by equating it to
zero arrive at the following online update:

qi =

(cid:80)

j:(i,j)∈E βijqj + αi ˆqi
(cid:80)
j:(i,j)∈E βij + αi

(1)

In practice, running this procedure for 10 iterations
converges to changes in Euclidean distance of ad-
jacent vertices of less than 10−2. The retroﬁtting
approach described above is modular; it can be ap-
plied to word vector representations obtained from
any model as the updates in Eq. 1 are agnostic to the
original vector training model objective.

Semantic Lexicons during Learning. Our pro-
posed approach is reminiscent of recent work on
improving word vectors using lexical resources (Yu
and Dredze, 2014; Bian et al., 2014; Xu et al., 2014)
which alters the learning objective of the original
vector training model with a prior (or a regularizer)
that encourages semantically related vectors (in Ω)
to be close together, except that our technique is ap-
plied as a second stage of learning. We describe the

prior approach here since it will serve as a baseline.
Here semantic lexicons play the role of a prior on Q
which we deﬁne as follows:

p(Q) ∝ exp

−γ



n
(cid:88)

(cid:88)

i=1

j:(i,j)∈E

βij(cid:107)qi − qj(cid:107)2





(2)
Here, γ is a hyperparameter that controls the
strength of the prior. As in the retroﬁtting objec-
tive, this prior on the word vector parameters forces
words connected in the lexicon to have close vec-
tor representations as did Ψ(Q) (with the role of ˆQ
being played by cross entropy of the empirical dis-
tribution).

This prior can be incorporated during learn-
ing through maximum a posteriori (MAP) estima-
tion. Since there is no closed form solution of
the estimate, we consider two iterative procedures.
In the ﬁrst, we use the sum of gradients of the
log-likelihood (given by the extant vector learning
model) and the log-prior (from Eq. 2), with respect
to Q for learning. Since computing the gradient of
Eq. 2 has linear runtime in the vocabulary size n, we
use lazy updates (Carpenter, 2008) for every k words
during training. We call this the lazy method of
MAP. The second technique applies stochastic gra-
dient ascent to the log-likelihood, and after every k
words applies the update in Eq. 1. We call this the
periodic method. We later experimentally compare
these methods against retroﬁtting (§6.2).

3 Word Vector Representations

We now describe the various publicly available pre-
trained English word vectors on which we will test
the applicability of the retroﬁtting model. These
vectors have been chosen to have a balanced mix
between large and small amounts of unlabeled text
as well as between neural and spectral methods of
training word vectors.

Glove Vectors. Global vectors for word represen-
tations (Pennington et al., 2014) are trained on ag-
gregated global word-word co-occurrence statistics
from a corpus, and the resulting representations
show interesting linear substructures of the word
vector space. These vectors were trained on 6 bil-
lion words from Wikipedia and English Gigaword

Lexicon
PPDB
WordNetsyn
WordNetall
FrameNet

Words
102,902
148,730
148,730
10,822

Edges
374,555
304,856
934,705
417,456

Table 1: Approximate size of the graphs obtained from
different lexicons.

and are of length 300.1

(SG). The word2vec
Skip-Gram Vectors
tool (Mikolov et al., 2013a) is fast and currently in
wide use. In this model, each word’s Huffman code
is used as an input to a log-linear classiﬁer with
a continuous projection layer and words within a
given context window are predicted. The available
vectors are trained on 100 billion words of Google
news dataset and are of length 300.2

Global Context Vectors (GC). These vectors are
learned using a recursive neural network that incor-
porates both local and global (document-level) con-
text features (Huang et al., 2012). These vectors
were trained on the ﬁrst 1 billion words of English
Wikipedia and are of length 50.3

Multilingual Vectors (Multi). Faruqui and Dyer
(2014) learned vectors by ﬁrst performing SVD on
text in different languages, then applying canonical
correlation analysis (CCA) on pairs of vectors for
words that align in parallel corpora. The monolin-
gual vectors were trained on WMT-2011 news cor-
pus for English, French, German and Spanish. We
use the Enligsh word vectors projected in the com-
mon English–German space. The monolingual En-
glish WMT corpus had 360 million words and the
trained vectors are of length 512.4

4 Semantic Lexicons

We use three different semantic lexicons to evaluate
their utility in improving the word vectors. We in-
clude both manually and automatically created lexi-
cons. Table 1 shows the size of the graphs obtained

1http://www-nlp.stanford.edu/projects/

glove/

2https://code.google.com/p/word2vec
3http://nlp.stanford.edu/˜socherr/

ACL2012_wordVectorsTextFile.zip

4http://cs.cmu.edu/˜mfaruqui/soft.html

from these lexicons.

PPDB. The paraphrase database (Ganitkevitch et
al., 2013) is a semantic lexicon containing more than
220 million paraphrase pairs of English.5 Of these, 8
million are lexical (single word to single word) para-
phrases. The key intuition behind the acquisition of
its lexical paraphrases is that two words in one lan-
guage that align, in parallel text, to the same word in
a different language, should be synonymous. For ex-
ample, if the words jailed and imprisoned are trans-
lated as the same word in another language, it may
be reasonable to assume they have the same mean-
ing. In our experiments, we instantiate an edge in
E for each lexical paraphrase in PPDB. The lexical
paraphrase dataset comes in different sizes ranging
from S to XXXL, in decreasing order of paraphras-
ing conﬁdence and increasing order of size. We
chose XL for our experiments. We want to give
higher edge weights (αi) connecting the retroﬁtted
word vectors (q) to the purely distributional word
vectors (ˆq) than to edges connecting the retroﬁtted
vectors to each other (βij), so all αi are set to 1 and
βij to be degree(i)−1 (with i being the node the up-
date is being applied to).6

(Miller, 1995)

WordNet. WordNet
is a large
human-constructed semantic lexicon of English
words.
It groups English words into sets of syn-
onyms called synsets, provides short, general deﬁni-
tions, and records the various semantic relations be-
tween synsets. This database is structured in a graph
particularly suitable for our task because it explicitly
relates concepts with semantically aligned relations
such as hypernyms and hyponyms. For example, the
word dog is a synonym of canine, a hypernym of
puppy and a hyponym of animal. We perform two
different experiments with WordNet: (1) connecting
a word only to synonyms, and (2) connecting a word
to synonyms, hypernyms and hyponyms. We refer
to these two graphs as WNsyn and WNall , respec-
tively. In both settings, all αi are set to 1 and βij to
be degree(i)−1.

5http://www.cis.upenn.edu/˜ccb/ppdb
6In principle, these hyperparameters can be tuned to opti-
mize performance on a particular task, which we leave for fu-
ture work.

FrameNet. FrameNet (Baker et al., 1998; Fill-
more et al., 2003) is a rich linguistic resource
containing information about lexical and predicate-
argument semantics in English. Frames can be re-
alized on the surface by many different word types,
which suggests that the word types evoking the same
frame should be semantically related. For exam-
ple, the frame Cause change of position on a scale
is associated with push, raise, and growth (among
many others). In our use of FrameNet, two words
that group together with any frame are given an edge
in E. We refer to this graph as FN. All αi are set to
1 and βij to be degree(i)−1.

5 Evaluation Benchmarks

We evaluate the quality of our word vector represen-
tations on tasks that test how well they capture both
semantic and syntactic aspects of the representations
along with an extrinsic sentiment analysis task.

Word Similarity. We evaluate our word represen-
tations on a variety of different benchmarks that
have been widely used to measure word similarity.
The ﬁrst one is the WS-353 dataset (Finkelstein et
al., 2001) containing 353 pairs of English words that
have been assigned similarity ratings by humans.
The second benchmark is the RG-65 (Rubenstein
and Goodenough, 1965) dataset that contain 65 pairs
of nouns. Since the commonly used word similar-
ity datasets contain a small number of word pairs
we also use the MEN dataset (Bruni et al., 2012) of
3,000 word pairs sampled from words that occur at
least 700 times in a large web corpus. We calculate
cosine similarity between the vectors of two words
forming a test item, and report Spearman’s rank cor-
relation coefﬁcient (Myers and Well, 1995) between
the rankings produced by our model against the hu-
man rankings.

Syntactic Relations (SYN-REL). Mikolov et al.
(2013b) present a syntactic relation dataset com-
It contains pairs
posed of analogous word pairs.
of tuples of word relations that follow a common
syntactic relation. For example, given walking and
walked, the words are differently inﬂected forms of
the same verb. There are nine different kinds of rela-
tions and overall there are 10,675 syntactic pairs of
word tuples. The task is to ﬁnd a word d that best

ﬁts the following relationship: “a is to b as c is to d,”
given a, b, and c. We use the vector offset method
(Mikolov et al., 2013a; Levy and Goldberg, 2014),
computing q = qa − qb + qc and returning the vector
from Q which has the highest cosine similarity to q.

Synonym Selection (TOEFL). The TOEFL syn-
onym selection task is to select the semantically
closest word to a target from a list of four candi-
dates (Landauer and Dumais, 1997). The dataset
contains 80 such questions. An example is “rug →
{sofa, ottoman, carpet, hallway}”, with carpet be-
ing the most synonym-like candidate to the target.

Sentiment Analysis (SA). Socher et al. (2013)
created a treebank containing sentences annotated
with ﬁne-grained sentiment labels on phrases and
sentences from movie review excerpts. The coarse-
grained treebank of positive and negative classes
has been split into training, development, and test
datasets containing 6,920, 872, and 1,821 sentences,
respectively. We train an (cid:96)2-regularized logistic re-
gression classiﬁer on the average of the word vectors
of a given sentence to predict the coarse-grained sen-
timent tag at the sentence level, and report the test-
set accuracy of the classiﬁer.

6 Experiments

We ﬁrst show experiments measuring improvements
from the retroﬁtting method (§6.1), followed by
comparisons to using lexicons during MAP learn-
ing (§6.2) and other published methods (§6.3). We
then test how well retroﬁtting generalizes to other
languages (§6.4).

6.1 Retroﬁtting

We use Eq. 1 to retroﬁt word vectors (§3) using
graphs derived from semantic lexicons (§4).

Results. Table 2 shows the absolute changes in
performance on different tasks (as columns) with
different semantic lexicons (as rows). All of the lexi-
cons offer high improvements on the word similarity
tasks (the ﬁrst three columns). On the TOEFL task,
we observe large improvements of the order of 10
absolute points in accuracy for all lexicons except
for FrameNet. FrameNet’s performance is weaker,
in some cases leading to worse performance (e.g.,

with Glove and SG vectors). For the extrinsic senti-
ment analysis task, we observe improvements using
all the lexicons and gain 1.4% (absolute) in accuracy
for the Multi vectors over the baseline. This increase
is statistically signiﬁcant (p < 0.01, McNemar).

We observe improvements over Glove and SG
vectors, which were trained on billions of tokens on
all tasks except for SYN-REL. For stronger base-
lines (Glove and Multi) we observe smaller im-
provements as compared to lower baseline scores
(SG and GC). We believe that FrameNet does not
perform as well as the other lexicons because its
frames group words based on very abstract concepts;
often words with seemingly distantly related mean-
ings (e.g., push and growth) can evoke the same
Interestingly, we almost never improve on
frame.
the SYN-REL task, especially with higher baselines,
this can be attributed to the fact that SYN-REL is in-
herently a syntactic task and during retroﬁtting we
are incorporating additional semantic information in
the vectors. In summary, we ﬁnd that PPDB gives
the best improvement maximum number of times
aggreagted over different vetor types, closely fol-
lowed by WNall , and retroﬁtting gives gains across
tasks and vectors. An ensemble lexicon, in which
the graph is the union of the WNall and PPDB
lexicons, on average performed slightly worse than
PPDB; we omit those results here for brevity.

6.2 Semantic Lexicons during Learning

To incorporate lexicon information during training,
and compare its performance against retroﬁtting,
we train log-bilinear (LBL) vectors (Mnih and Teh,
2012). These vectors are trained to optimize the
log-likelihood of a language model which predicts
a word token w’s vector given the set of words in its
context (h), also represented as vectors:

p(w | h; Q) ∝ exp

q(cid:62)
i qj + bj

(3)

(cid:33)

(cid:32)

(cid:88)

i∈h

We optimize the above likelihood combined with the
prior deﬁned in Eq. 2 using the lazy and periodic
techniques described in §2. Since it is costly to com-
pute the partition function over the whole vocab-
ulary, we use noise constrastive estimation (NCE)
to estimate the parameters of the model (Mnih and
Teh, 2012) using AdaGrad (Duchi et al., 2010) with
a learning rate of 0.05.

Lexicon
Glove
+PPDB
+WNsyn
+WNall
+FN
SG
+PPDB
+WNsyn
+WNall
+FN
GC
+PPDB
+WNsyn
+WNall
+FN
Multi
+PPDB
+WNsyn
+WNall
+FN

MEN-3k RG-65 WS-353
60.5
–1.2
0.5
0.7
–5.3
65.6
4.4
0.0
1.9
–4.9
62.3
2.0
0.6
2.3
0.0
68.1
6.0
2.2
4.3
0.0

73.7
1.4
0.0
2.2
–3.6
67.8
5.4
0.7
2.5
–3.2
31.3
7.0
3.6
6.7
1.8
75.8
3.8
1.2
2.9
1.8

76.7
2.9
2.7
7.5
–1.0
72.8
3.5
3.9
5.0
2.6
62.8
6.1
6.4
10.2
4.0
75.5
4.0
0.2
8.5
4.0

TOEFL SYN-REL
67.0
–0.4
–12.4
–8.4
–7.0
73.9
–2.3
–13.6
–10.7
–7.3
10.9
5.3
–1.7
–0.6
–0.6
45.5
4.3
–12.3
–10.6
–0.6

89.7
5.1
5.1
2.6
2.6
85.3
10.7
9.3
9.3
1.3
60.8
13.1
7.3
4.4
4.4
84.0
12.0
6.6
6.6
4.4

SA
79.6
1.6
0.7
0.5
0.0
81.2
0.9
0.7
–0.3
0.5
67.8
1.1
0.0
0.2
0.2
81.0
0.6
1.4
1.4
0.2

Table 2: Absolute performance changes with retroﬁtting. Spearman’s correlation (3 left columns) and accuracy (3
right columns) on different tasks. Higher scores are always better. Bold indicates greatest improvement for a vector
type.

Method
LBL (Baseline)

LBL + Lazy

LBL + Periodic

LBL + Retroﬁtting

k = ∞, γ = 0
γ = 1
γ = 0.1
γ = 0.01
k = 100M
k = 50M
k = 25M
–

k, γ MEN-3k RG-65 WS-353
53.6
0.6
0.4
1.7
3.6
4.4
2.7
5.5

58.0
–0.4
0.7
0.7
3.8
3.4
0.5
5.7

42.7
4.2
8.1
9.5
18.4
19.5
18.1
15.6

TOEFL SYN-REL
31.5
0.6
0.7
1.9
4.8
0.6
–3.7
14.7

66.7
–0.1
–1.4
2.6
12.0
18.6
21.3
18.6

SA
72.5
1.2
0.8
0.4
1.3
1.9
0.8
0.9

Table 3: Absolute performance changes for including PPDB information while training LBL vectors. Spearman’s
correlation (3 left columns) and accuracy (3 right columns) on different tasks. Bold indicates greatest improvement.

We train vectors of length 100 on the WMT-2011
news corpus, which contains 360 million words,
and use PPDB as the semantic lexicon as it per-
formed reasonably well in the retroﬁtting experi-
ments (§6.1). For the lazy method we update with
respect to the prior every k = 100,000 words7
and test for different values of prior strength γ ∈
{1, 0.1, 0.01}. For the periodic method, we up-
date the word vectors using Eq. 1 every k ∈
{25, 50, 100} million words.

Results. See Table 3. For lazy, γ = 0.01 performs
best, but the method is in most cases not highly sen-
sitive to γ’s value. For periodic, which overall leads
to greater improvements over the baseline than lazy,
k = 50M performs best, although all other values
of k also outperform the the baseline. Retroﬁtting,
which can be applied to any word vectors, regardless
of how they are trained, is competitive and some-
times better.

7k = 10,000 or 50,000 yielded similar results.

Corpus

WMT-11

Wikipedia

Vector Training
CBOW
Yu and Dredze (2014)
CBOW + Retroﬁtting
SG
Xu et al. (2014)
SG + Retroﬁtting

MEN-3k RG-65 WS-353
54.7
53.7
58.4
68.6
68.3
67.5

55.2
50.1
60.5
76.1
–
65.7

44.8
47.1
57.7
66.7
–
73.9

TOEFL SYN-REL
40.8
29.9
52.5
40.3
44.4
49.9

73.3
61.3
81.3
72.0
–
86.0

SA
74.1
71.5
75.7
73.1
–
74.6

Table 4: Comparison of retroﬁtting for semantic enrichment against Yu and Dredze (2014), Xu et al. (2014). Spear-
man’s correlation (3 left columns) and accuracy (3 right columns) on different tasks.

6.3 Comparisons to Prior Work

inferior score on the WS-353 task.

Two previous models (Yu and Dredze, 2014; Xu
et al., 2014) have shown that the quality of word
vectors obtained using word2vec tool can be im-
proved by using semantic knowledge from lexicons.
Both these models use constraints among words as
a regularization term on the training objective dur-
ing training, and their methods can only be applied
for improving the quality of SG and CBOW vectors
produced by the word2vec tool. We compared the
quality of our vectors against each of these.

Yu and Dredze (2014). We train word vectors us-
ing their joint model training code8 while using ex-
actly the same training settings as speciﬁed in their
best model: CBOW, vector length 100 and PPDB for
enrichment. The results are shown in the top half of
Table 4 where our model consistently outperforms
the baseline and their model.

Xu et al. (2014). This model extracts categori-
cal and relational knowledge among words from
Freebase9 and uses it as a constraint while train-
ing. Unfortunately, neither their word embeddings
nor model training code is publicly available, so
we train the SG model by using exactly the same
settings as described in their system (vector length
300) and on the same corpus: monolingual English
Wikipedia text.10 We compare the performance of
our retroﬁtting vectors on the SYN-REL and WS-
353 task against the best model11 reported in their
paper. As shown in the lower half of Table 4, our
model outperforms their model by an absolute 5.5
points absolute on the SYN-REL task, but a slightly

8https://github.com/Gorov/JointRCM
9https://www.freebase.com
10http://mattmahoney.net/dc/enwik9.zip
11Their best model is named “RC-NET” in their paper.

uwn

6.4 Multilingual Evaluation

We tested our method on three additional languages:
German, French, and Spanish. We used the Univer-
sal WordNet (de Melo and Weikum, 2009), an au-
tomatically constructed multilingual lexical knowl-
edge base based on WordNet.12 It contains words
connected via different lexical relations to other
words both within and across languages. We con-
struct separate graphs for different languages (i.e.,
only linking words to other words in the same lan-
guage) and apply retroﬁtting to each. Since not
many word similarity evaluation benchmarks are
available for languages other than English, we tested
our baseline and improved vectors on one bench-
mark per language.

We used RG-65 (Gurevych, 2005), RG-65
(Joubarne and Inkpen, 2011) and MC-30 (Hassan
and Mihalcea, 2009) for German, French and Span-
ish, respectively.13 We trained SG vectors for each
language of length 300 on a corpus of 1 billion to-
kens, each extracted from Wikipedia, and evaluate
them on word similarity on the benchmarks before
and after retroﬁtting. Table 5 shows that we obtain
high improvements which strongly indicates that our
method generalizes across these languages.

7 Further Analysis

Retroﬁtting vs. vector length. With more di-
mensions, word vectors might be able to cap-
ture higher orders of semantic information and
retroﬁtting might be less helpful. We train SG vec-

12http://www.mpi-inf.mpg.de/yago-naga/

13These benchmarks were created by translating the corre-

sponding English benchmarks word by word manually.

Figure 3: Two-dimensional PCA projections of 100-dimensional SG vector pairs holding the “adjective to adverb”
relation, before (left) and after (right) retroﬁtting.

Language Task
German
French
Spanish

RG-65
RG-65
MC-30

SG Retroﬁtted SG
60.3
53.4
60.6
46.7
59.1
54.0

Table 5: Spearman’s correlation for word similarity eval-
uation using the using original and retroﬁtted SG vectors.

SG word vectors and plot them in R2. In Figure 3 we
plot these projections before (left) and after (right)
retroﬁtting. It can be seen that in the ﬁrst case the
direction of the analogy vectors is not consistent, but
after retroﬁtting all the analogy vectors are aligned
in the same direction.

8 Related Work

The use of lexical semantic information in training
word vectors has been limited. Recently, word sim-
ilarity knowledge (Yu and Dredze, 2014; Fried and
Duh, 2014) and word relational knowledge (Xu et
al., 2014; Bian et al., 2014) have been used to im-
prove the word2vec embeddings in a joint train-
ing model similar to our regularization approach.
In latent semantic analysis, the word cooccurrence
matrix can be constructed to incorporate relational
information like antonym speciﬁc polarity induc-
tion (Yih et al., 2012) and multi-relational latent se-
mantic analysis (Chang et al., 2013).

The approach we propose is conceptually similar
to previous work that uses graph structures to prop-
agate information among semantic concepts (Zhu,
2005; Culp and Michailidis, 2008). Graph-based
belief propagation has also been used to induce
POS tags (Subramanya et al., 2010; Das and Petrov,
2011) and semantic frame associations (Das and
Smith, 2011). In those efforts, labels for unknown
words were inferred using a method similar to
ours. Broadly, graph-based semi-supervised learn-
ing (Zhu, 2005; Talukdar and Pereira, 2010) has
been applied to machine translation (Alexandrescu

Figure 2: Spearman’s correlation on the MEN word sim-
ilarity task, before and after retroﬁtting.

tors on 1 billion English tokens for vector lengths
ranging from 50 to 1,000 and evaluate on the MEN
word similarity task. We retroﬁt these vectors to
PPDB (§4) and evaluate those on the same task. Fig-
ure 2 shows consistent improvement in vector qual-
ity across different vector lengths.

Visualization. We randomly select eight word
pairs that have the “adjective to adverb” relation
from the SYN-REL task (§5). We then take a two-
dimensional PCA projection of the 100-dimensional

and Kirchhoff, 2009), unsupervised semantic role
induction (Lang and Lapata, 2011), semantic docu-
ment modeling (Schuhmacher and Ponzetto, 2014),
language generation (Krahmer et al., 2003) and sen-
timent analysis (Goldberg and Zhu, 2006).

9 Conclusion

We have proposed a simple and effective method
named retroﬁtting to improve word vectors using
word relation knowledge found in semantic lex-
icons. Retroﬁtting is used as a post-processing
step to improve vector quality and is more modu-
lar than other approaches that use semantic informa-
tion while training. It can be applied to vectors ob-
tained from any word vector training method. Our
experiments explored the method’s performance
across tasks, semantic lexicons, and languages and
showed that
it outperforms existing alternatives.
The retroﬁtting tool is available at: https://
github.com/mfaruqui/retrofitting.

Acknowledgements

This research was supported in part by the National
Science Foundation under grants IIS-1143703, IIS-
1147810, and IIS-1251131; by IARPA via De-
Interior National Business Center
partment of
(DoI/NBC) contract number D12PC00337; and by
DARPA under grant FA87501220342. Part of the
computational work was carried out on resources
provided by the Pittsburgh Supercomputing Center.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
Disclaimer:
the views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the ofﬁcial
policies or endorsements, either expressed or im-
plied, of IARPA, DoI/NBC, DARPA, or the U.S.
Government.

References

Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
2009.
Kravalova, Marius Pas¸ca, and Aitor Soroa.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceedings
of NAACL.

Andrei Alexandrescu and Katrin Kirchhoff.

2009.
Graph-based learning for statistical machine transla-
tion. NAACL ’09.

Collin F. Baker, Charles J. Fillmore, and John B. Lowe.

1998. The berkeley framenet project. ACL ’98.

Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux.
2006. Label propagation and quadratic criterion.
In
Semi-Supervised Learning.

Jiang Bian, Bin Gao, and Tie-Yan Liu.

2014.
Knowledge-powered deep learning for word embed-
ding. In Machine Learning and Knowledge Discovery
in Databases.

Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of ACL.

Bob Carpenter. 2008. Lazy sparse stochastic gradient de-
scent for regularized multinomial logistic regression.
Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.
2013. Multi-relational latent semantic analysis.
In
Proceedings of EMNLP.

Ronan Collobert and Jason Weston. 2008. A uniﬁed ar-
chitecture for natural language processing: deep neu-
ral networks with multitask learning. In Proceedings
of ICML.

Mark Culp and George Michailidis. 2008. Graph-based
semisupervised learning. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence.

Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proc. of ACL.

Dipanjan Das and Noah A. Smith.

Semi-
supervised frame-semantic parsing for unknown pred-
icates. In Proc. of ACL.

2011.

Gerard de Melo and Gerhard Weikum. 2009. Towards
a universal wordnet by learning from combined evi-
dence. In Proceedings of CIKM.

S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society for
Information Science.

John Duchi, Elad Hazan, and Yoram Singer.

2010.
Adaptive subgradient methods for online learning
Technical Report
and stochastic optimization.
UCB/EECS-2010-24, Mar.

Manaal Faruqui and Chris Dyer. 2014. Improving vector
space word representations using multilingual correla-
tion. In Proceedings of EACL.

Charles Fillmore, Christopher Johnson, and Miriam
International Journal of Lexicogra-

Petruck. 2003.
phy.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: the concept
revisited. In WWW, New York, NY, USA.

Jerome L. Myers and Arnold D. Well. 1995. Research

Design & Statistical Analysis. Routledge.

Jeffrey Pennington, Richard Socher, and Christopher D.
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Proceedings of EMNLP.

Herbert Rubenstein and John B. Goodenough.

1965.
Contextual correlates of synonymy. Commun. ACM,
8(10):627–633, October.

Michael Schuhmacher and Simone Paolo Ponzetto.
2014. Knowledge-based graph document modeling.
In Proceedings of WSDM.

Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
Christopher D. Manning, Andrew Y. Ng, and Christo-
pher Potts. 2013. Recursive deep models for semantic
In Pro-
compositionality over a sentiment treebank.
ceedings of EMNLP.

Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efﬁcient graph-based semi-supervised
learning of structured tagging models. In Proceedings
of EMNLP.

Partha Pratim Talukdar and Fernando Pereira.

2010.
Experiments in graph-based semi-supervised learning
methods for class-instance acquisition. In Proceedings
of ACL.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proc. of ACL.

Peter D. Turney. 2006. Similarity of semantic relations.

Comput. Linguist., 32(3):379–416, September.

Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang Wang,
Xiaoguang Liu, and Tie-Yan Liu. 2014. Rc-net: A
general framework for incorporating knowledge into
word representations. In Proceedings of CIKM.

Wen-tau Yih, Geoffrey Zweig, and John C. Platt. 2012.
In Pro-

Polarity inducing latent semantic analysis.
ceedings of EMNLP.

Mo Yu and Mark Dredze. 2014. Improving lexical em-

beddings with semantic knowledge. In ACL.

Xiaojin Zhu.

with Graphs.
AAI3179046.

Semi-supervised Learning
2005.
Ph.D. thesis, Pittsburgh, PA, USA.

Daniel Fried and Kevin Duh. 2014. Incorporating both
distributional and relational semantics in word repre-
sentations. arXiv preprint arXiv:1412.4369.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
PPDB: The paraphrase

Callison-Burch.
database. In Proceedings of NAACL.
Andrew B. Goldberg and Xiaojin Zhu.

2013.

2006. See-
ing stars when there aren’t many stars: Graph-based
semi-supervised learning for sentiment categorization.
TextGraphs-1.

Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu.
2014. Revisiting embedding features for simple semi-
supervised learning. In Proceedings of EMNLP.

Iryna Gurevych. 2005. Using the structure of a concep-
tual network in computing semantic relatedness.
In
Proceedings of IJCNLP.

Samer Hassan and Rada Mihalcea. 2009. Cross-lingual
semantic relatedness using encyclopedic knowledge.
In Proc. of EMNLP.

Eric H Huang, Richard Socher, Christopher D Manning,
and Andrew Y Ng. 2012. Improving word representa-
tions via global context and multiple word prototypes.
In Proceedings of ACL.

Colette Joubarne and Diana Inkpen. 2011. Compari-
son of semantic similarity for different languages us-
ing the google n-gram corpus and second- order co-
occurrence measures. In Proceedings of CAAI.

Ross Kindermann and J. L. Snell. 1980. Markov Random

Fields and Their Applications. AMS.

Emiel Krahmer, Sebastian van Erk, and Andr´e Verleg.
2003. Graph-based generation of referring expres-
sions. Comput. Linguist.

Thomas K Landauer and Susan T. Dumais. 1997. A so-
lution to plato’s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological review.

Joel Lang and Mirella Lapata. 2011. Unsupervised se-
mantic role induction with graph partitioning. In Pro-
ceedings of EMNLP.

Omer Levy and Yoav Goldberg. 2014. Linguistic regu-
larities in sparse and explicit word representations. In
Proceedings of CoNLL.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efﬁcient estimation of word representa-
tions in vector space. arXiv preprint arXiv:1301.3781.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL.
George A Miller. 1995. Wordnet: a lexical database for

english. Communications of the ACM.

Andriy Mnih and Yee Whye Teh. 2012. A fast and sim-
ple algorithm for training neural probabilistic language
models. In Proceedings of ICML.

5
1
0
2
 
r
a

M
 
2
2
 
 
]
L
C
.
s
c
[
 
 
4
v
6
6
1
4
.
1
1
4
1
:
v
i
X
r
a

Retroﬁtting Word Vectors to Semantic Lexicons

Manaal Faruqui

Jesse Dodge
Chris Dyer Eduard Hovy Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{mfaruqui,jessed,sjauhar,cdyer,ehovy,nasmith}@cs.cmu.edu

Sujay K. Jauhar

Abstract

Vector space word representations are learned
from distributional information of words in
large corpora. Although such statistics are
semantically informative, they disregard the
valuable information that is contained in se-
mantic lexicons such as WordNet, FrameNet,
and the Paraphrase Database. This paper
proposes a method for reﬁning vector space
representations using relational information
from semantic lexicons by encouraging linked
words to have similar vector representations,
and it makes no assumptions about how the in-
put vectors were constructed. Evaluated on a
battery of standard lexical semantic evaluation
tasks in several languages, we obtain substan-
tial improvements starting with a variety of
word vector models. Our reﬁnement method
outperforms prior techniques for incorporat-
ing semantic lexicons into word vector train-
ing algorithms.

1

Introduction

Data-driven learning of word vectors that capture
lexico-semantic information is a technique of cen-
tral importance in NLP. These word vectors can
in turn be used for identifying semantically related
word pairs (Turney, 2006; Agirre et al., 2009) or
as features in downstream text processing applica-
tions (Turian et al., 2010; Guo et al., 2014). A vari-
ety of approaches for constructing vector space em-
beddings of vocabularies are in use, notably includ-
ing taking low rank approximations of cooccurrence
statistics (Deerwester et al., 1990) and using internal
representations from neural network models of word
sequences (Collobert and Weston, 2008).

Because of their value as lexical semantic repre-
sentations, there has been much research on improv-

ing the quality of vectors. Semantic lexicons, which
provide type-level information about the semantics
of words, typically by identifying synonymy, hyper-
nymy, hyponymy, and paraphrase relations should
be a valuable resource for improving the quality of
word vectors that are trained solely on unlabeled
corpora. Examples of such resources include Word-
Net (Miller, 1995), FrameNet (Baker et al., 1998)
and the Paraphrase Database (Ganitkevitch et al.,
2013).

Recent work has shown that by either changing
the objective of the word vector training algorithm
in neural language models (Yu and Dredze, 2014;
Xu et al., 2014; Bian et al., 2014; Fried and Duh,
2014) or by relation-speciﬁc augmentation of the
cooccurence matrix in spectral word vector models
to incorporate semantic knowledge (Yih et al., 2012;
Chang et al., 2013), the quality of word vectors can
be improved. However, these methods are limited to
particular methods for constructing vectors.

The contribution of this paper is a graph-based
learning technique for using lexical relational re-
sources to obtain higher quality semantic vectors,
which we call “retroﬁtting.” In contrast to previ-
ous work, retroﬁtting is applied as a post-processing
step by running belief propagation on a graph con-
structed from lexicon-derived relational information
to update word vectors (§2). This allows retroﬁtting
to be used on pre-trained word vectors obtained
using any vector training model.
Intuitively, our
method encourages the new vectors to be (i) simi-
lar to the vectors of related word types and (ii) simi-
lar to their purely distributional representations. The
retroﬁtting process is fast, taking about 5 seconds for
a graph of 100,000 words and vector length 300, and
its runtime is independent of the original word vec-
tor training model.

tors to be retroﬁtted (and correspond to VΩ); shaded
nodes are labeled with the corresponding vectors in
ˆQ, which are observed. The graph can be interpreted
as a Markov random ﬁeld (Kindermann and Snell,
1980).

The distance between a pair of vectors is deﬁned
to be the Euclidean distance. Since we want the
inferred word vector to be close to the observed
value ˆqi and close to its neighbors qj, ∀j such that
(i, j) ∈ E, the objective to be minimized becomes:

Figure 1: Word graph with edges between related words
showing the observed (grey) and the inferred (white)
word vector representations.

Ψ(Q) =


αi(cid:107)qi − ˆqi(cid:107)2 +

n
(cid:88)

i=1

(cid:88)

(i,j)∈E

βij(cid:107)qi − qj(cid:107)2





Experimentally, we show that our method works
well with different state-of-the-art word vector mod-
els, using different kinds of semantic lexicons and
gives substantial
improvements on a variety of
benchmarks, while beating the current state-of-the-
art approaches for incorporating semantic informa-
tion in vector training and trivially extends to mul-
tiple languages. We show that retroﬁtting gives
consistent improvement in performance on evalua-
tion benchmarks with different word vector lengths
and show a qualitative visualization of the effect of
retroﬁtting on word vector quality. The retroﬁtting
is available at: https://github.com/
tool
mfaruqui/retrofitting.

2 Retroﬁtting with Semantic Lexicons

Let V = {w1, . . . , wn} be a vocabulary, i.e, the set
of word types, and Ω be an ontology that encodes se-
mantic relations between words in V . We represent
Ω as an undirected graph (V, E) with one vertex for
each word type and edges (wi, wj) ∈ E ⊆ V × V
indicating a semantic relationship of interest. These
relations differ for different semantic lexicons and
are described later (§4).

The matrix ˆQ will be the collection of vector rep-
resentations ˆqi ∈ Rd, for each wi ∈ V , learned
using a standard data-driven technique, where d is
the length of the word vectors. Our objective is
to learn the matrix Q = (q1, . . . , qn) such that the
columns are both close (under a distance metric) to
their counterparts in ˆQ and to adjacent vertices in Ω.
Figure 1 shows a small word graph with such edge
connections; white nodes are labeled with the Q vec-

where α and β values control the relative strengths
of associations (more details in §6.1).

In this case, we ﬁrst train the word vectors inde-
pendent of the information in the semantic lexicons
and then retroﬁt them. Ψ is convex in Q and its so-
lution can be found by solving a system of linear
equations. To do so, we use an efﬁcient iterative
updating method (Bengio et al., 2006; Subramanya
et al., 2010; Das and Petrov, 2011; Das and Smith,
2011). The vectors in Q are initialized to be equal
to the vectors in ˆQ. We take the ﬁrst derivative of Ψ
with respect to one qi vector, and by equating it to
zero arrive at the following online update:

qi =

(cid:80)

j:(i,j)∈E βijqj + αi ˆqi
(cid:80)
j:(i,j)∈E βij + αi

(1)

In practice, running this procedure for 10 iterations
converges to changes in Euclidean distance of ad-
jacent vertices of less than 10−2. The retroﬁtting
approach described above is modular; it can be ap-
plied to word vector representations obtained from
any model as the updates in Eq. 1 are agnostic to the
original vector training model objective.

Semantic Lexicons during Learning. Our pro-
posed approach is reminiscent of recent work on
improving word vectors using lexical resources (Yu
and Dredze, 2014; Bian et al., 2014; Xu et al., 2014)
which alters the learning objective of the original
vector training model with a prior (or a regularizer)
that encourages semantically related vectors (in Ω)
to be close together, except that our technique is ap-
plied as a second stage of learning. We describe the

prior approach here since it will serve as a baseline.
Here semantic lexicons play the role of a prior on Q
which we deﬁne as follows:

p(Q) ∝ exp

−γ



n
(cid:88)

(cid:88)

i=1

j:(i,j)∈E

βij(cid:107)qi − qj(cid:107)2





(2)
Here, γ is a hyperparameter that controls the
strength of the prior. As in the retroﬁtting objec-
tive, this prior on the word vector parameters forces
words connected in the lexicon to have close vec-
tor representations as did Ψ(Q) (with the role of ˆQ
being played by cross entropy of the empirical dis-
tribution).

This prior can be incorporated during learn-
ing through maximum a posteriori (MAP) estima-
tion. Since there is no closed form solution of
the estimate, we consider two iterative procedures.
In the ﬁrst, we use the sum of gradients of the
log-likelihood (given by the extant vector learning
model) and the log-prior (from Eq. 2), with respect
to Q for learning. Since computing the gradient of
Eq. 2 has linear runtime in the vocabulary size n, we
use lazy updates (Carpenter, 2008) for every k words
during training. We call this the lazy method of
MAP. The second technique applies stochastic gra-
dient ascent to the log-likelihood, and after every k
words applies the update in Eq. 1. We call this the
periodic method. We later experimentally compare
these methods against retroﬁtting (§6.2).

3 Word Vector Representations

We now describe the various publicly available pre-
trained English word vectors on which we will test
the applicability of the retroﬁtting model. These
vectors have been chosen to have a balanced mix
between large and small amounts of unlabeled text
as well as between neural and spectral methods of
training word vectors.

Glove Vectors. Global vectors for word represen-
tations (Pennington et al., 2014) are trained on ag-
gregated global word-word co-occurrence statistics
from a corpus, and the resulting representations
show interesting linear substructures of the word
vector space. These vectors were trained on 6 bil-
lion words from Wikipedia and English Gigaword

Lexicon
PPDB
WordNetsyn
WordNetall
FrameNet

Words
102,902
148,730
148,730
10,822

Edges
374,555
304,856
934,705
417,456

Table 1: Approximate size of the graphs obtained from
different lexicons.

and are of length 300.1

(SG). The word2vec
Skip-Gram Vectors
tool (Mikolov et al., 2013a) is fast and currently in
wide use. In this model, each word’s Huffman code
is used as an input to a log-linear classiﬁer with
a continuous projection layer and words within a
given context window are predicted. The available
vectors are trained on 100 billion words of Google
news dataset and are of length 300.2

Global Context Vectors (GC). These vectors are
learned using a recursive neural network that incor-
porates both local and global (document-level) con-
text features (Huang et al., 2012). These vectors
were trained on the ﬁrst 1 billion words of English
Wikipedia and are of length 50.3

Multilingual Vectors (Multi). Faruqui and Dyer
(2014) learned vectors by ﬁrst performing SVD on
text in different languages, then applying canonical
correlation analysis (CCA) on pairs of vectors for
words that align in parallel corpora. The monolin-
gual vectors were trained on WMT-2011 news cor-
pus for English, French, German and Spanish. We
use the Enligsh word vectors projected in the com-
mon English–German space. The monolingual En-
glish WMT corpus had 360 million words and the
trained vectors are of length 512.4

4 Semantic Lexicons

We use three different semantic lexicons to evaluate
their utility in improving the word vectors. We in-
clude both manually and automatically created lexi-
cons. Table 1 shows the size of the graphs obtained

1http://www-nlp.stanford.edu/projects/

glove/

2https://code.google.com/p/word2vec
3http://nlp.stanford.edu/˜socherr/

ACL2012_wordVectorsTextFile.zip

4http://cs.cmu.edu/˜mfaruqui/soft.html

from these lexicons.

PPDB. The paraphrase database (Ganitkevitch et
al., 2013) is a semantic lexicon containing more than
220 million paraphrase pairs of English.5 Of these, 8
million are lexical (single word to single word) para-
phrases. The key intuition behind the acquisition of
its lexical paraphrases is that two words in one lan-
guage that align, in parallel text, to the same word in
a different language, should be synonymous. For ex-
ample, if the words jailed and imprisoned are trans-
lated as the same word in another language, it may
be reasonable to assume they have the same mean-
ing. In our experiments, we instantiate an edge in
E for each lexical paraphrase in PPDB. The lexical
paraphrase dataset comes in different sizes ranging
from S to XXXL, in decreasing order of paraphras-
ing conﬁdence and increasing order of size. We
chose XL for our experiments. We want to give
higher edge weights (αi) connecting the retroﬁtted
word vectors (q) to the purely distributional word
vectors (ˆq) than to edges connecting the retroﬁtted
vectors to each other (βij), so all αi are set to 1 and
βij to be degree(i)−1 (with i being the node the up-
date is being applied to).6

(Miller, 1995)

WordNet. WordNet
is a large
human-constructed semantic lexicon of English
words.
It groups English words into sets of syn-
onyms called synsets, provides short, general deﬁni-
tions, and records the various semantic relations be-
tween synsets. This database is structured in a graph
particularly suitable for our task because it explicitly
relates concepts with semantically aligned relations
such as hypernyms and hyponyms. For example, the
word dog is a synonym of canine, a hypernym of
puppy and a hyponym of animal. We perform two
different experiments with WordNet: (1) connecting
a word only to synonyms, and (2) connecting a word
to synonyms, hypernyms and hyponyms. We refer
to these two graphs as WNsyn and WNall , respec-
tively. In both settings, all αi are set to 1 and βij to
be degree(i)−1.

5http://www.cis.upenn.edu/˜ccb/ppdb
6In principle, these hyperparameters can be tuned to opti-
mize performance on a particular task, which we leave for fu-
ture work.

FrameNet. FrameNet (Baker et al., 1998; Fill-
more et al., 2003) is a rich linguistic resource
containing information about lexical and predicate-
argument semantics in English. Frames can be re-
alized on the surface by many different word types,
which suggests that the word types evoking the same
frame should be semantically related. For exam-
ple, the frame Cause change of position on a scale
is associated with push, raise, and growth (among
many others). In our use of FrameNet, two words
that group together with any frame are given an edge
in E. We refer to this graph as FN. All αi are set to
1 and βij to be degree(i)−1.

5 Evaluation Benchmarks

We evaluate the quality of our word vector represen-
tations on tasks that test how well they capture both
semantic and syntactic aspects of the representations
along with an extrinsic sentiment analysis task.

Word Similarity. We evaluate our word represen-
tations on a variety of different benchmarks that
have been widely used to measure word similarity.
The ﬁrst one is the WS-353 dataset (Finkelstein et
al., 2001) containing 353 pairs of English words that
have been assigned similarity ratings by humans.
The second benchmark is the RG-65 (Rubenstein
and Goodenough, 1965) dataset that contain 65 pairs
of nouns. Since the commonly used word similar-
ity datasets contain a small number of word pairs
we also use the MEN dataset (Bruni et al., 2012) of
3,000 word pairs sampled from words that occur at
least 700 times in a large web corpus. We calculate
cosine similarity between the vectors of two words
forming a test item, and report Spearman’s rank cor-
relation coefﬁcient (Myers and Well, 1995) between
the rankings produced by our model against the hu-
man rankings.

Syntactic Relations (SYN-REL). Mikolov et al.
(2013b) present a syntactic relation dataset com-
It contains pairs
posed of analogous word pairs.
of tuples of word relations that follow a common
syntactic relation. For example, given walking and
walked, the words are differently inﬂected forms of
the same verb. There are nine different kinds of rela-
tions and overall there are 10,675 syntactic pairs of
word tuples. The task is to ﬁnd a word d that best

ﬁts the following relationship: “a is to b as c is to d,”
given a, b, and c. We use the vector offset method
(Mikolov et al., 2013a; Levy and Goldberg, 2014),
computing q = qa − qb + qc and returning the vector
from Q which has the highest cosine similarity to q.

Synonym Selection (TOEFL). The TOEFL syn-
onym selection task is to select the semantically
closest word to a target from a list of four candi-
dates (Landauer and Dumais, 1997). The dataset
contains 80 such questions. An example is “rug →
{sofa, ottoman, carpet, hallway}”, with carpet be-
ing the most synonym-like candidate to the target.

Sentiment Analysis (SA). Socher et al. (2013)
created a treebank containing sentences annotated
with ﬁne-grained sentiment labels on phrases and
sentences from movie review excerpts. The coarse-
grained treebank of positive and negative classes
has been split into training, development, and test
datasets containing 6,920, 872, and 1,821 sentences,
respectively. We train an (cid:96)2-regularized logistic re-
gression classiﬁer on the average of the word vectors
of a given sentence to predict the coarse-grained sen-
timent tag at the sentence level, and report the test-
set accuracy of the classiﬁer.

6 Experiments

We ﬁrst show experiments measuring improvements
from the retroﬁtting method (§6.1), followed by
comparisons to using lexicons during MAP learn-
ing (§6.2) and other published methods (§6.3). We
then test how well retroﬁtting generalizes to other
languages (§6.4).

6.1 Retroﬁtting

We use Eq. 1 to retroﬁt word vectors (§3) using
graphs derived from semantic lexicons (§4).

Results. Table 2 shows the absolute changes in
performance on different tasks (as columns) with
different semantic lexicons (as rows). All of the lexi-
cons offer high improvements on the word similarity
tasks (the ﬁrst three columns). On the TOEFL task,
we observe large improvements of the order of 10
absolute points in accuracy for all lexicons except
for FrameNet. FrameNet’s performance is weaker,
in some cases leading to worse performance (e.g.,

with Glove and SG vectors). For the extrinsic senti-
ment analysis task, we observe improvements using
all the lexicons and gain 1.4% (absolute) in accuracy
for the Multi vectors over the baseline. This increase
is statistically signiﬁcant (p < 0.01, McNemar).

We observe improvements over Glove and SG
vectors, which were trained on billions of tokens on
all tasks except for SYN-REL. For stronger base-
lines (Glove and Multi) we observe smaller im-
provements as compared to lower baseline scores
(SG and GC). We believe that FrameNet does not
perform as well as the other lexicons because its
frames group words based on very abstract concepts;
often words with seemingly distantly related mean-
ings (e.g., push and growth) can evoke the same
Interestingly, we almost never improve on
frame.
the SYN-REL task, especially with higher baselines,
this can be attributed to the fact that SYN-REL is in-
herently a syntactic task and during retroﬁtting we
are incorporating additional semantic information in
the vectors. In summary, we ﬁnd that PPDB gives
the best improvement maximum number of times
aggreagted over different vetor types, closely fol-
lowed by WNall , and retroﬁtting gives gains across
tasks and vectors. An ensemble lexicon, in which
the graph is the union of the WNall and PPDB
lexicons, on average performed slightly worse than
PPDB; we omit those results here for brevity.

6.2 Semantic Lexicons during Learning

To incorporate lexicon information during training,
and compare its performance against retroﬁtting,
we train log-bilinear (LBL) vectors (Mnih and Teh,
2012). These vectors are trained to optimize the
log-likelihood of a language model which predicts
a word token w’s vector given the set of words in its
context (h), also represented as vectors:

p(w | h; Q) ∝ exp

q(cid:62)
i qj + bj

(3)

(cid:33)

(cid:32)

(cid:88)

i∈h

We optimize the above likelihood combined with the
prior deﬁned in Eq. 2 using the lazy and periodic
techniques described in §2. Since it is costly to com-
pute the partition function over the whole vocab-
ulary, we use noise constrastive estimation (NCE)
to estimate the parameters of the model (Mnih and
Teh, 2012) using AdaGrad (Duchi et al., 2010) with
a learning rate of 0.05.

Lexicon
Glove
+PPDB
+WNsyn
+WNall
+FN
SG
+PPDB
+WNsyn
+WNall
+FN
GC
+PPDB
+WNsyn
+WNall
+FN
Multi
+PPDB
+WNsyn
+WNall
+FN

MEN-3k RG-65 WS-353
60.5
–1.2
0.5
0.7
–5.3
65.6
4.4
0.0
1.9
–4.9
62.3
2.0
0.6
2.3
0.0
68.1
6.0
2.2
4.3
0.0

73.7
1.4
0.0
2.2
–3.6
67.8
5.4
0.7
2.5
–3.2
31.3
7.0
3.6
6.7
1.8
75.8
3.8
1.2
2.9
1.8

76.7
2.9
2.7
7.5
–1.0
72.8
3.5
3.9
5.0
2.6
62.8
6.1
6.4
10.2
4.0
75.5
4.0
0.2
8.5
4.0

TOEFL SYN-REL
67.0
–0.4
–12.4
–8.4
–7.0
73.9
–2.3
–13.6
–10.7
–7.3
10.9
5.3
–1.7
–0.6
–0.6
45.5
4.3
–12.3
–10.6
–0.6

89.7
5.1
5.1
2.6
2.6
85.3
10.7
9.3
9.3
1.3
60.8
13.1
7.3
4.4
4.4
84.0
12.0
6.6
6.6
4.4

SA
79.6
1.6
0.7
0.5
0.0
81.2
0.9
0.7
–0.3
0.5
67.8
1.1
0.0
0.2
0.2
81.0
0.6
1.4
1.4
0.2

Table 2: Absolute performance changes with retroﬁtting. Spearman’s correlation (3 left columns) and accuracy (3
right columns) on different tasks. Higher scores are always better. Bold indicates greatest improvement for a vector
type.

Method
LBL (Baseline)

LBL + Lazy

LBL + Periodic

LBL + Retroﬁtting

k = ∞, γ = 0
γ = 1
γ = 0.1
γ = 0.01
k = 100M
k = 50M
k = 25M
–

k, γ MEN-3k RG-65 WS-353
53.6
0.6
0.4
1.7
3.6
4.4
2.7
5.5

58.0
–0.4
0.7
0.7
3.8
3.4
0.5
5.7

42.7
4.2
8.1
9.5
18.4
19.5
18.1
15.6

TOEFL SYN-REL
31.5
0.6
0.7
1.9
4.8
0.6
–3.7
14.7

66.7
–0.1
–1.4
2.6
12.0
18.6
21.3
18.6

SA
72.5
1.2
0.8
0.4
1.3
1.9
0.8
0.9

Table 3: Absolute performance changes for including PPDB information while training LBL vectors. Spearman’s
correlation (3 left columns) and accuracy (3 right columns) on different tasks. Bold indicates greatest improvement.

We train vectors of length 100 on the WMT-2011
news corpus, which contains 360 million words,
and use PPDB as the semantic lexicon as it per-
formed reasonably well in the retroﬁtting experi-
ments (§6.1). For the lazy method we update with
respect to the prior every k = 100,000 words7
and test for different values of prior strength γ ∈
{1, 0.1, 0.01}. For the periodic method, we up-
date the word vectors using Eq. 1 every k ∈
{25, 50, 100} million words.

Results. See Table 3. For lazy, γ = 0.01 performs
best, but the method is in most cases not highly sen-
sitive to γ’s value. For periodic, which overall leads
to greater improvements over the baseline than lazy,
k = 50M performs best, although all other values
of k also outperform the the baseline. Retroﬁtting,
which can be applied to any word vectors, regardless
of how they are trained, is competitive and some-
times better.

7k = 10,000 or 50,000 yielded similar results.

Corpus

WMT-11

Wikipedia

Vector Training
CBOW
Yu and Dredze (2014)
CBOW + Retroﬁtting
SG
Xu et al. (2014)
SG + Retroﬁtting

MEN-3k RG-65 WS-353
54.7
53.7
58.4
68.6
68.3
67.5

55.2
50.1
60.5
76.1
–
65.7

44.8
47.1
57.7
66.7
–
73.9

TOEFL SYN-REL
40.8
29.9
52.5
40.3
44.4
49.9

73.3
61.3
81.3
72.0
–
86.0

SA
74.1
71.5
75.7
73.1
–
74.6

Table 4: Comparison of retroﬁtting for semantic enrichment against Yu and Dredze (2014), Xu et al. (2014). Spear-
man’s correlation (3 left columns) and accuracy (3 right columns) on different tasks.

6.3 Comparisons to Prior Work

inferior score on the WS-353 task.

Two previous models (Yu and Dredze, 2014; Xu
et al., 2014) have shown that the quality of word
vectors obtained using word2vec tool can be im-
proved by using semantic knowledge from lexicons.
Both these models use constraints among words as
a regularization term on the training objective dur-
ing training, and their methods can only be applied
for improving the quality of SG and CBOW vectors
produced by the word2vec tool. We compared the
quality of our vectors against each of these.

Yu and Dredze (2014). We train word vectors us-
ing their joint model training code8 while using ex-
actly the same training settings as speciﬁed in their
best model: CBOW, vector length 100 and PPDB for
enrichment. The results are shown in the top half of
Table 4 where our model consistently outperforms
the baseline and their model.

Xu et al. (2014). This model extracts categori-
cal and relational knowledge among words from
Freebase9 and uses it as a constraint while train-
ing. Unfortunately, neither their word embeddings
nor model training code is publicly available, so
we train the SG model by using exactly the same
settings as described in their system (vector length
300) and on the same corpus: monolingual English
Wikipedia text.10 We compare the performance of
our retroﬁtting vectors on the SYN-REL and WS-
353 task against the best model11 reported in their
paper. As shown in the lower half of Table 4, our
model outperforms their model by an absolute 5.5
points absolute on the SYN-REL task, but a slightly

8https://github.com/Gorov/JointRCM
9https://www.freebase.com
10http://mattmahoney.net/dc/enwik9.zip
11Their best model is named “RC-NET” in their paper.

uwn

6.4 Multilingual Evaluation

We tested our method on three additional languages:
German, French, and Spanish. We used the Univer-
sal WordNet (de Melo and Weikum, 2009), an au-
tomatically constructed multilingual lexical knowl-
edge base based on WordNet.12 It contains words
connected via different lexical relations to other
words both within and across languages. We con-
struct separate graphs for different languages (i.e.,
only linking words to other words in the same lan-
guage) and apply retroﬁtting to each. Since not
many word similarity evaluation benchmarks are
available for languages other than English, we tested
our baseline and improved vectors on one bench-
mark per language.

We used RG-65 (Gurevych, 2005), RG-65
(Joubarne and Inkpen, 2011) and MC-30 (Hassan
and Mihalcea, 2009) for German, French and Span-
ish, respectively.13 We trained SG vectors for each
language of length 300 on a corpus of 1 billion to-
kens, each extracted from Wikipedia, and evaluate
them on word similarity on the benchmarks before
and after retroﬁtting. Table 5 shows that we obtain
high improvements which strongly indicates that our
method generalizes across these languages.

7 Further Analysis

Retroﬁtting vs. vector length. With more di-
mensions, word vectors might be able to cap-
ture higher orders of semantic information and
retroﬁtting might be less helpful. We train SG vec-

12http://www.mpi-inf.mpg.de/yago-naga/

13These benchmarks were created by translating the corre-

sponding English benchmarks word by word manually.

Figure 3: Two-dimensional PCA projections of 100-dimensional SG vector pairs holding the “adjective to adverb”
relation, before (left) and after (right) retroﬁtting.

Language Task
German
French
Spanish

RG-65
RG-65
MC-30

SG Retroﬁtted SG
60.3
53.4
60.6
46.7
59.1
54.0

Table 5: Spearman’s correlation for word similarity eval-
uation using the using original and retroﬁtted SG vectors.

SG word vectors and plot them in R2. In Figure 3 we
plot these projections before (left) and after (right)
retroﬁtting. It can be seen that in the ﬁrst case the
direction of the analogy vectors is not consistent, but
after retroﬁtting all the analogy vectors are aligned
in the same direction.

8 Related Work

The use of lexical semantic information in training
word vectors has been limited. Recently, word sim-
ilarity knowledge (Yu and Dredze, 2014; Fried and
Duh, 2014) and word relational knowledge (Xu et
al., 2014; Bian et al., 2014) have been used to im-
prove the word2vec embeddings in a joint train-
ing model similar to our regularization approach.
In latent semantic analysis, the word cooccurrence
matrix can be constructed to incorporate relational
information like antonym speciﬁc polarity induc-
tion (Yih et al., 2012) and multi-relational latent se-
mantic analysis (Chang et al., 2013).

The approach we propose is conceptually similar
to previous work that uses graph structures to prop-
agate information among semantic concepts (Zhu,
2005; Culp and Michailidis, 2008). Graph-based
belief propagation has also been used to induce
POS tags (Subramanya et al., 2010; Das and Petrov,
2011) and semantic frame associations (Das and
Smith, 2011). In those efforts, labels for unknown
words were inferred using a method similar to
ours. Broadly, graph-based semi-supervised learn-
ing (Zhu, 2005; Talukdar and Pereira, 2010) has
been applied to machine translation (Alexandrescu

Figure 2: Spearman’s correlation on the MEN word sim-
ilarity task, before and after retroﬁtting.

tors on 1 billion English tokens for vector lengths
ranging from 50 to 1,000 and evaluate on the MEN
word similarity task. We retroﬁt these vectors to
PPDB (§4) and evaluate those on the same task. Fig-
ure 2 shows consistent improvement in vector qual-
ity across different vector lengths.

Visualization. We randomly select eight word
pairs that have the “adjective to adverb” relation
from the SYN-REL task (§5). We then take a two-
dimensional PCA projection of the 100-dimensional

and Kirchhoff, 2009), unsupervised semantic role
induction (Lang and Lapata, 2011), semantic docu-
ment modeling (Schuhmacher and Ponzetto, 2014),
language generation (Krahmer et al., 2003) and sen-
timent analysis (Goldberg and Zhu, 2006).

9 Conclusion

We have proposed a simple and effective method
named retroﬁtting to improve word vectors using
word relation knowledge found in semantic lex-
icons. Retroﬁtting is used as a post-processing
step to improve vector quality and is more modu-
lar than other approaches that use semantic informa-
tion while training. It can be applied to vectors ob-
tained from any word vector training method. Our
experiments explored the method’s performance
across tasks, semantic lexicons, and languages and
showed that
it outperforms existing alternatives.
The retroﬁtting tool is available at: https://
github.com/mfaruqui/retrofitting.

Acknowledgements

This research was supported in part by the National
Science Foundation under grants IIS-1143703, IIS-
1147810, and IIS-1251131; by IARPA via De-
Interior National Business Center
partment of
(DoI/NBC) contract number D12PC00337; and by
DARPA under grant FA87501220342. Part of the
computational work was carried out on resources
provided by the Pittsburgh Supercomputing Center.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
Disclaimer:
the views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the ofﬁcial
policies or endorsements, either expressed or im-
plied, of IARPA, DoI/NBC, DARPA, or the U.S.
Government.

References

Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
2009.
Kravalova, Marius Pas¸ca, and Aitor Soroa.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceedings
of NAACL.

Andrei Alexandrescu and Katrin Kirchhoff.

2009.
Graph-based learning for statistical machine transla-
tion. NAACL ’09.

Collin F. Baker, Charles J. Fillmore, and John B. Lowe.

1998. The berkeley framenet project. ACL ’98.

Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux.
2006. Label propagation and quadratic criterion.
In
Semi-Supervised Learning.

Jiang Bian, Bin Gao, and Tie-Yan Liu.

2014.
Knowledge-powered deep learning for word embed-
ding. In Machine Learning and Knowledge Discovery
in Databases.

Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of ACL.

Bob Carpenter. 2008. Lazy sparse stochastic gradient de-
scent for regularized multinomial logistic regression.
Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.
2013. Multi-relational latent semantic analysis.
In
Proceedings of EMNLP.

Ronan Collobert and Jason Weston. 2008. A uniﬁed ar-
chitecture for natural language processing: deep neu-
ral networks with multitask learning. In Proceedings
of ICML.

Mark Culp and George Michailidis. 2008. Graph-based
semisupervised learning. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence.

Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proc. of ACL.

Dipanjan Das and Noah A. Smith.

Semi-
supervised frame-semantic parsing for unknown pred-
icates. In Proc. of ACL.

2011.

Gerard de Melo and Gerhard Weikum. 2009. Towards
a universal wordnet by learning from combined evi-
dence. In Proceedings of CIKM.

S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society for
Information Science.

John Duchi, Elad Hazan, and Yoram Singer.

2010.
Adaptive subgradient methods for online learning
Technical Report
and stochastic optimization.
UCB/EECS-2010-24, Mar.

Manaal Faruqui and Chris Dyer. 2014. Improving vector
space word representations using multilingual correla-
tion. In Proceedings of EACL.

Charles Fillmore, Christopher Johnson, and Miriam
International Journal of Lexicogra-

Petruck. 2003.
phy.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: the concept
revisited. In WWW, New York, NY, USA.

Jerome L. Myers and Arnold D. Well. 1995. Research

Design & Statistical Analysis. Routledge.

Jeffrey Pennington, Richard Socher, and Christopher D.
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Proceedings of EMNLP.

Herbert Rubenstein and John B. Goodenough.

1965.
Contextual correlates of synonymy. Commun. ACM,
8(10):627–633, October.

Michael Schuhmacher and Simone Paolo Ponzetto.
2014. Knowledge-based graph document modeling.
In Proceedings of WSDM.

Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
Christopher D. Manning, Andrew Y. Ng, and Christo-
pher Potts. 2013. Recursive deep models for semantic
In Pro-
compositionality over a sentiment treebank.
ceedings of EMNLP.

Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efﬁcient graph-based semi-supervised
learning of structured tagging models. In Proceedings
of EMNLP.

Partha Pratim Talukdar and Fernando Pereira.

2010.
Experiments in graph-based semi-supervised learning
methods for class-instance acquisition. In Proceedings
of ACL.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proc. of ACL.

Peter D. Turney. 2006. Similarity of semantic relations.

Comput. Linguist., 32(3):379–416, September.

Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang Wang,
Xiaoguang Liu, and Tie-Yan Liu. 2014. Rc-net: A
general framework for incorporating knowledge into
word representations. In Proceedings of CIKM.

Wen-tau Yih, Geoffrey Zweig, and John C. Platt. 2012.
In Pro-

Polarity inducing latent semantic analysis.
ceedings of EMNLP.

Mo Yu and Mark Dredze. 2014. Improving lexical em-

beddings with semantic knowledge. In ACL.

Xiaojin Zhu.

with Graphs.
AAI3179046.

Semi-supervised Learning
2005.
Ph.D. thesis, Pittsburgh, PA, USA.

Daniel Fried and Kevin Duh. 2014. Incorporating both
distributional and relational semantics in word repre-
sentations. arXiv preprint arXiv:1412.4369.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
PPDB: The paraphrase

Callison-Burch.
database. In Proceedings of NAACL.
Andrew B. Goldberg and Xiaojin Zhu.

2013.

2006. See-
ing stars when there aren’t many stars: Graph-based
semi-supervised learning for sentiment categorization.
TextGraphs-1.

Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu.
2014. Revisiting embedding features for simple semi-
supervised learning. In Proceedings of EMNLP.

Iryna Gurevych. 2005. Using the structure of a concep-
tual network in computing semantic relatedness.
In
Proceedings of IJCNLP.

Samer Hassan and Rada Mihalcea. 2009. Cross-lingual
semantic relatedness using encyclopedic knowledge.
In Proc. of EMNLP.

Eric H Huang, Richard Socher, Christopher D Manning,
and Andrew Y Ng. 2012. Improving word representa-
tions via global context and multiple word prototypes.
In Proceedings of ACL.

Colette Joubarne and Diana Inkpen. 2011. Compari-
son of semantic similarity for different languages us-
ing the google n-gram corpus and second- order co-
occurrence measures. In Proceedings of CAAI.

Ross Kindermann and J. L. Snell. 1980. Markov Random

Fields and Their Applications. AMS.

Emiel Krahmer, Sebastian van Erk, and Andr´e Verleg.
2003. Graph-based generation of referring expres-
sions. Comput. Linguist.

Thomas K Landauer and Susan T. Dumais. 1997. A so-
lution to plato’s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological review.

Joel Lang and Mirella Lapata. 2011. Unsupervised se-
mantic role induction with graph partitioning. In Pro-
ceedings of EMNLP.

Omer Levy and Yoav Goldberg. 2014. Linguistic regu-
larities in sparse and explicit word representations. In
Proceedings of CoNLL.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efﬁcient estimation of word representa-
tions in vector space. arXiv preprint arXiv:1301.3781.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL.
George A Miller. 1995. Wordnet: a lexical database for

english. Communications of the ACM.

Andriy Mnih and Yee Whye Teh. 2012. A fast and sim-
ple algorithm for training neural probabilistic language
models. In Proceedings of ICML.

