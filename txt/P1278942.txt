Challenges of Using Text Classiﬁers for Causal Inference

Zach Wood-Doughty∗, Ilya Shpitser†, Mark Dredze∗†
Department of Computer Science
∗Center for Language and Speech Processing
†Malone Center for Engineering in Healthcare
Johns Hopkins University, Baltimore, MD 21218
{zach,ilyas,mdredze}@cs.jhu.edu

8
1
0
2
 
t
c
O
 
1
 
 
]
L
C
.
s
c
[
 
 
1
v
6
5
9
0
0
.
0
1
8
1
:
v
i
X
r
a

Abstract

causal

Causal understanding is essential for many
kinds of decision-making, but causal inference
from observational data has typically only
been applied to structured, low-dimensional
datasets. While text classiﬁers produce
low-dimensional outputs, their use in causal
inference has not previously been studied.
analyses based on
To facilitate
language data, we consider the role that text
classiﬁers can play in causal inference through
established modeling mechanisms from the
causality literature on missing data and
measurement error. We demonstrate how to
conduct causal analyses using text classiﬁers
on simulated and Yelp data, and discuss the
opportunities and challenges of future work
that uses text data in causal inference.

1 Introduction

scientiﬁc

analyses,

in domains

Most
from
economics to medicine, focus on low-dimensional
structured data.
Many such domains also
have unstructured text data; advances in natural
language processing (NLP) have led to an
increased interest in incorporating language data
into scientiﬁc analyses. While language is
inherently unstructured and high dimensional,
NLP systems can be used to process raw text
to produce structured variables.
For example,
work on identifying undiagnosed side effects
from electronic health records (EHR) uses text
classiﬁers to produce clinical variables from the
raw text (Hazlehurst et al., 2009).
NLP tools may also beneﬁt

the study of
causal inference, which seeks to identify causal
from observational data.
relations
Causal
analyses
low-dimensional
traditionally
structured variables, such as clinical markers and
binary health outcomes. Such analyses require
the data-generating process,
assumptions about

use

counterfactual

which are often simpler with low-dimensional
data. Unlike prediction tasks which are validated
by held-out test sets, causal inference involves
modeling
random variables
represent
(Neyman, 1923; Rubin, 1976)
the outcome of some hypothetical intervention.
To rigorously reason about hypotheticals, we
use causal models to link our counterfactuals to
observed data (Pearl, 2009).

that

NLP provides a natural way to incorporate text
data into causal inference models. We can produce
for example,
low-dimensional variables using,
text classiﬁers, and then run our causal analysis.
However, this straightforward integration belies
several potential issues. Text classiﬁcation is not
perfect, and errors in a NLP algorithm may bias
subsequent analyses. Causal inference requires
understanding how variables inﬂuence one another
and how correlations are confounded by common
causes. Classic methods such as stratiﬁcation
provide a means for handling confounding of
categorical or continuous variables, but it is not
immediately obvious how such work can be
extended to high-dimensional data.

But

2016).

confounders

Recent work has approached high-dimensional
domains via random forests (Wager and Athey,
2017) and other machine learning methods
(Chernozhukov et al.,
even
compared to an analysis that requires hundreds
(Belloni et al., 2014), NLP
of
models with millions of variables are very
physiological
high-dimensional.
symptoms reﬂect complex biological
realities,
many symptoms such as blood pressure are
one-dimensional variables. While doctors can
easily quantify the effect of high blood pressure
on some outcome, can we use the “positivity” of
a restaurant review to estimate a causal effect?
More broadly,
is it possible to employ text
classiﬁcation methods in a causal analysis?

While

We explore methods for the integration of
text classiﬁers into causal inference analyses that
consider confounds introduced by imperfect NLP.
We show what assumptions are necessary for
causal analyses using text, and discuss when
those assumptions may or may not be reasonable.
We draw on the causal
inference literature to
consider two modeling aspects: missing data
In the missing data
and measurement error.
formulation, a variable of interest is sometimes
unobserved, and text data gives us a means
to model
In the
the missingness process.
measurement error formulation, we use a text
classiﬁer
the
underlying variable.

to generate a noisy proxy of

We highlight practical considerations of a
causal analysis with text data by conducting
analyses with simulated and Yelp data. We
examine the results of both formulations and show
how a causal analysis which properly accounts for
possible sources of bias produces better estimates
than na¨ıve methods which make unjustiﬁed
assumptions. We conclude by examining how our
approach may enable new research avenues for
inferring causality with text data.

2 Causal Inference, Brieﬂy

While randomized control trials (RCT) are the
gold standard of determining causal effects of
treatments on outcomes, they can be expensive or
impossible in many settings. In contrast, the world
is ﬁlled with observational data collected without
randomization. While most studies simply report
correlations from observational data, the ﬁeld of
causal inference examines what assumptions and
analyses make it possible to identify causal effects.
We formalize a causal statement like “smoking
causes cancer” as “if we were to conduct a RCT
and assign smoking as a treatment, we would
see a higher incidence of cancer among those
assigned smoking than among the control group.”
In the framework of Pearl (1995), we consider
a counterfactual variable of interest: what would
have been the cancer incidence among smokers
if smoking had been randomized? Speciﬁcally,
we consider a causal effect as the counterfactual
outcome of a hypothetical intervention on some
treatment variable.
If we denote smoking as
treatment variable A and cancer as our
our
outcome variable Y , then we are interested in the
counterfactual distribution, denoted p(Y (a)) or

p(Y | do(a)). We interpret this as “the distribution
over Y had A been set, possibly contrary to fact,
to value a.” For a binary treatment A, the causal
effect of A on Y is denoted τ = E[Y (1)] −
E[Y (0)]; the average difference between if you
had received the treatment and if you had not.
Throughout, we use causal directed acyclic graphs
(DAG), which assumes that an intervention on
A is well-deﬁned and results in a counterfactual
variable Y (a) (Pearl, 1995; Dawid, 2010).

| A),

Figure 1a shows an example of

simple
This is the simplest DAG in
confounding.
which counterfactual distribution p(Y (a)) is not
as C inﬂuences both
simply p(Y
the treatment A and the outcome Y .
To
recover the counterfactual distribution p(Y (a))
that would follow an intervention upon A, we must
“adjust” for C, applying the so-called “back-door
criterion” (Pearl, 1995). We can then derive the
counterfactual distribution p(Y (a)) and desired
causal effect, τS as a function of the observed
data, (Fig. 4 Eq. 1.) This derivation is shown
in Appendix A.

it

in fact observed,

Note that p(Y (a)) and τS require data on
C, and if C is not
is
impossible to recover the causal effect. Formally,
identiﬁed in the
we say that p(Y (a)) is not
model, meaning there is no function f such that
p(Y (a))=f (p(A, Y )). Identiﬁability is a primary
concern of causal inference (Shpitser and Pearl,
2008).

Throughout, we assume for simplicity that A,
C, and Y are binary variables. While text
classiﬁers can convert high-dimensional data into
binary variables for such analyses, we need to
make further assumptions about how classiﬁcation
errors affect causal inferences. We cannot assume
that the output of a text classiﬁer can be treated as
if it were ground truth. To conceptualize the ways
in which a text classiﬁer may be biased, we will
consider them as a way to recover from missing
data or measurement error.

3 Causal Models

Real-world observational data is messy and often
imperfectly collected. Work in causal inference
has studied how analyses can be made robust
to missing data or data recorded with systematic
measurement errors.

C

A

RA

C

Y

A

A(1)

Y

C

A

A∗

Y

(a) Simple Confounding

(b) Missing Data

(c) Measurement Error

Figure 1: DAGs for causal inference without text data. Red variables are unobserved.
A is a treatment, Y is an outcome, and C is a confounder.

A C Y
0
1
1
1
1
0
1
0
0
1
0
1

RA A C Y
0
1
1
0
1
1
1
0

1
1
0
0

1
?
0
?

A∗ C Y
0
1
0
1
1
0
1
0
0
1
0
1

A∗ A
1
1
1
0
0
0
1
1

(a) Simple Confounding

(b) Missing Data

(c) Measurement Error

(d) Mismeasurement

Figure 2: Example data rows for causal inference without text data.

3.1 Missing Data

3.2 Measurement Error

Our dataset has “missing data” if it contains
individuals (instances) for which some variables
are unobserved, even though these variables are
typically available.
This may occur if some
survey respondents choose not to answer certain
questions, or if certain variables are difﬁcult to
collect and thus infrequently recorded. Missing
data is closely related to causal inference – both
are interested in hypothetical distributions that
we cannot directly observe (Robins et al., 2000;
Shpitser et al., 2015).

Consider a causal model where A is sometimes
missing (Figure 1b). The variable RA is a binary
indicator for whether A is observed (RA = 1)
or missing. The variable A(RA = 1), written
as A(1), represents the counterfactual value of A
were it never missing. Finally, A is the observed
proxy for A(1): it has the same value as A(1) if
RA = 1, and the special value “?” if RA = 0.

Solving missingness can seen as intervening to
set RA to 1. Given p(A, RA, C, Y ), we want to
recover p(A(1), C, Y ). We may need to make a
“Missing at Random” (MAR) assumption, which
says that the missingness process is independent of
the true missing values, conditional on observed
values. Figure 1b reﬂects the MAR assumption;
RA is independent of A(1) given fully-observed
C and Y .
If an edge existed from A(1) to RA,
we have “Missing Not at Random” (MNAR) and
would not be identiﬁed except in special cases
(Shpitser et al., 2015).

Sometimes a necessary variable is never observed,
but is instead proxied by a variable which differs
Consider the
from the truth by some error.
example of body mass index (BMI) as a proxy for
obesity in a clinical study. Obesity is a known
risk factor for many health outcomes, but has a
complex clinical deﬁnition and is nontrivial to
measure. BMI is a simple deterministic function
of height and weight. To conduct a causal analysis
of obesity on cancer when only BMI and cancer
are measured, we can proceed as if we had
measured obesity and then correct our analysis
for the known error that comes from using BMI
as a proxy for obesity (Hern´an and Cole, 2009;
Michels et al., 1998).

To generalize this concept, we can replace
obesity with our ground truth variable A
and replace BMI with a noisy proxy A∗.
Figure 1c gives
this model.
the DAG for
there is no
Unlike missing data problems,
hypothetical
the
true data distribution p(A, C, Y ).
Instead, we
manipulate the observed distribution p(A∗, C, Y )
with the known relationship p(A∗, A) to recover
the desired p(A, C, Y ).

intervention which recovers

Unlike missing data, measurement

error
conceptualization can be used even when we
never observe A (e.g.
the table in Figure 2c)
as long as we have knowledge about the error
mechanism p(A∗, A). Using this knowledge,
the error using ‘matrix
we can correct

for

C

A

Ti

Y

V

RA

C

A

A(1)

Ti

V

Y

C

A

A∗

Y

Ti

V

(a) Simple Confounding with Text

(b) Missing Data with Text

(c) Measurement Error with Text

Figure 3: DAGs for causal inference with text data. In the Yelp experiments we discuss, Ti inﬂuences Y
and not the other way around.

τS =

(p(Y = 1 | A = 1, C) − p(Y = 1 | A = 0, C)) p(C)

τMD =

(cid:18)

XC

′

p(A = 1 | T, C, Y = 1, RA = 1)
y p(A = 1 | T, C, y′, RA = 1)p(Y = y′ | C)
p(A = 0 | T, C, Y = 1, RA = 1)

y p(A = 0 | T, C, Y = y′, RA = 1)p(Y = y′ | C) (cid:19)

p(Y = 1, C)

XC

P
−

′

P

(1)

(2)

τME =

XC







y′

P

−δc,y=1qc,y=1(0) + (1 − δc,y=1)qc,y=1(1)
(1 − ǫc,y=1 − δc,y=1)

(1 − ǫc,y=1)qc,y=1(0) − ǫc,y=1qc,y=1(1)
(1 − ǫc,y=1 − δc,y=1)

−δc,y′qc,y′ (0) + (1 − δc,y′)qc,y′ (1)
(1 − ǫc,y′ − δc,y′ )

(1 − ǫc,y′ )qc,y′ (0) − ǫc,y′ qc,y′ (1)
(1 − ǫc,y′ − δc,y′)

−

y′

P

p(C)

(3)







Deﬁne ǫc,y = p(A = 0 | A∗ = 1, C = c, Y = y), δc, y = p(A = 1 | A∗ = 0, C = c, Y = y),
qc,y(0) = p(C = c, Y = y, A∗ = 0), and qc,y(1) = p(C = c, Y = y, A∗ = 1).

Figure 4: Functionals for the Causal Effects for Simple Confounding (τSC), Missing Data (τMD) and
Measurement Error (τME). Derivations are in Appendices A, B, and C.

adjustment’ (Pearl, 2010).
In practice we might
learn p(A∗, A) from data such as that found in
Figure 2d. Recent work has also considered
how multiple independent proxies of A could
allow identiﬁcation without any data on p(A∗, A)
(Kuroki and Pearl, 2014).

4 Causal Models for Text Data

We can use conceptualizations for missing data
and measurement error to support causal analyses
with text data. The choice of model depends on the
assumptions we make about the data-generation
process.

to represent

text, which produces

We add new variables to our models (Figure
1a)
the
data-generating distribution shown in Figure 3a.
This model assumes that the underlying A, C, and
Y variables are generated before the text variables;
we use text
the true relationship
between A and Y .
We represent

text as an arbitrary set of
V variables, which are independent of one

to recover

another given the non-text variables.
In our
implemented analyses we will represent text as
a bag-of-words, wherein each Ti is simply the
binary indicator of the presence of the i-th word
in our vocabulary of V words, and T = ∪iTi.
The restriction to simple text models allows
us to explore connections to causal
inference
applications,
though future work could relax
assumptions of the text models to be inclusive
of more sophisticated text models (e.g. neural
sequence models (Lai et al., 2015; Zhang et al.,
2015)), or consider causal relationships between
two text variables.

To motivate our explanations, consider the task
of predicting an individuals’ smoking status from
free-text hospital discharge notes (Uzuner et al.,
2008; Wicentowski and Sydes, 2008).
Some
hospitals do not explicitly record patient smoking
status as structured data, making it difﬁcult to use
such data in a study on the outcomes of smoking.
We will suppose that we are given a dataset with
patient data on lung cancer outcome (Y ) and age

(C), that our data on smoking status (A) is affected
by either missing data or measurement error, but
that we have text data (T) from discharge records
that will allow us to infer smoking status with
reasonable accuracy.

4.1 Missing Data

To show how we might use text data to recover
from missing data, we introduce missingness for
A from Figure 3a to get the model in Figure 3b.
The missing arrow from A(1) to RA encodes the
MAR assumption, which is sufﬁcient to make it
possible to identify the full data distribution from
the observed data.

Suppose our motivation is to estimate the causal
effect of smoking status (A) on lung cancer (Y )
adjusting for age (C).
Imagine that missing
data arises because hospitals sometimes – but not
always – delete explicit data on smoking status
from patient records. If we have access to patients’
discharge notes (T) and know whether a given
patient had smoking status recorded (RA), then the
DAG in Figure 3b may be a reasonable model for
our setting. Note that we must again assume that
A does not directly affect RA.

The causal effect of A on Y in Figure 3b is
identiﬁed as τM D, given in Eq. 2 in Figure 4. The
derivation is given in Appendix B.

4.2 Measurement Error

We model text data with measurement error by
introducing a proxy A∗ to the model in Figure
3c. We assume that the proxied value of A∗ can
depend upon all other variables, and that we will
be able to estimate p(A∗, A) given an external
dataset, e.g.
text classiﬁer accuracy on held-out
data.

Suppose we again want to estimate the causal
effect from §4.1, but this time none of our hospital
records contain explicit data on smoking status.
However, imagine that we have a separate training
dataset of medical discharge records annotated
by expert pulmonologists for patients’ smoking
status. We could then train a classiﬁer to predict
smoking status using discharge record text1.

Working from the derivation for matrix
in binary models given by Pearl
adjustment
(2010), we identify the causal effect of A on
Y (Figure 3c) as τME (Eq 3 in Figure 4.) The
derivation is in Appendix C.

1This is the precise setting of Uzuner et al. (2008).

5 Experiments

We now empirically evaluate the effectiveness
of our two conceptualizations (missing data and
measurement error) for including text data in
causal analyses. We induce missingness or
mismeasurement of the treatment variable and use
text data to recover the true causal relationship
treatment on the outcome. We begin
of that
with a simulation study with synthetic text data,
and then conduct an analysis using reviews from
yelp.com.

5.1 Synthetic Data

A graphical model

We select synthetic data so that we can control
the entire data-generation process.
For each
data row, we ﬁrst sample data on three binary
variables (A, C, Y ) and then sample V different
binary variables Ti representing a V -vocabulary
bag-of-words.
this
distribution appears in Figure 3a. We augment
this distribution to introduce either missing
data (Figure 3b) or measurement error (Figure
For measurement error, we sample two
3c.)
datasets. A small training set which gives data on
p(A, C, Y, T) and a large test set which gives data
on p(C, Y, T).

for

The full data generating process appears in
Appendix D, and the implementation (along with
all our code) is provided online2.

5.2 Yelp Data
We utilize the 2015 Yelp Dataset Challenge3
which provides 4.7M reviews of local businesses.
Each review contains a one- to ﬁve-star rating, up
to 5,000 characters of text. Yelp users can ﬂag
reviews as “Useful” as a mark of quality.

We extract treatment, outcome, and confounder
variables from the structured data. The treatment
is a binarized user rating that takes value 1 if
the review has four or ﬁve stars and value 0
if the review has one or two stars. Three-star
reviews are discarded from our analysis. The
outcome is whether the review received at least
one “Useful” ﬂag. The confounder is whether
two
the review’s author has received at
“Useful” ﬂags across all reviews, according to
their user object.
In our data, 74.2% of reviews
were positive, 42.6% of reviews were ﬂagged as
“Useful,” and 56.7% users had received at least

least

2github.com/zachwooddoughty/emnlp2018-causal
3yelp.com/dataset/challenge

two such ﬂags. We preprocess the text of each
review by lowercasing, stemming, and removing
stopwords, before converting to a bag-of-words
representation with the 4,334 word vocabulary of
all words which appeared at least 1000 times in a
sample of 1M reviews.

Based on this p(A, C, Y, T) distribution,
we assume the data-generating process
that
matches Figure 3a and introduce missingness
and mismeasurement
giving us
data-generating processes matching Figures 3b
and 3c.

as before,

Our intention is not

to argue about a true
real-world causal effect of Yelp reviews on peer
behavior: we do not believe that our confounder
is the only common cause of the author’s rating
and the platform’s response. We leave for future
work a case study that jointly addresses questions
of identiﬁability and estimation of a real-world
In this work, our experiments
causal effect.
focus on a simpler task: can a correctly-speciﬁed
model that uses text data effectively estimate a
causal effect in the presence of missing data or
measurement error.

5.3 Models

We now introduce several baseline methods
which, unlike our correctly speciﬁed models τM D
and τM E, are not consistent estimators of our
desired causal effect. We would expect that the
theoretical bias in these estimators would result in
poor performance in our experiments.

5.3.1 Baseline: Na¨ıve Model

In both the missing data and measurement error
settings, our models use some rows that are full
observed. In missing data, these are rows where
RA = 1; in measurement error, the training set is
sampled from the true distribution. The simplest
approach to handling imperfect data is to throw
away all rows without full data, and calculate Eq
1 from that data. In Figure 5, these are labeled as
*.naive.

5.3.2 Baseline: Textless Model

In Figure 3b, if we do not condition on Ti to
d-separate A(1) from its missingness indicator,
that inﬂuence may bias our estimate. While we
know that ignoring text may introduce asymptotic
bias into our estimates of the causal effect, we
empirically evaluate how much bias is produced
by this “Textless” model compared to a correct

model. This is labeled as *.no text in Figure
5 (a).

In principle, we could conduct a measurement
error analysis using a model that does not include
text. In practice, we found we could not impute A∗
from C and Y alone. The non-textual classiﬁer
had such high error that the adjustment matrix
was singular and we could not compute the
effect. Thus, we have no such baseline in our
measurement error results.

5.3.3 Baseline: no y and unadjusted

Models

In Figure 3b, we must also condition on C and Y
to d-separate A(1) from its missingness indicator.
In our misspeciﬁed model for missing data, we do
not condition on Y , leaving open a path for A(1)
to inﬂuence its missingness. In Figure 5 (a), this
model is labeled as *.no y.

When correcting for measurement error, a
crucial piece of the estimation is the matrix
adjustment using the known error between
the proxy and the truth.
A straightforward
misspeciﬁed model for measurement error is to
impute a proxy for each row in our dataset and
then calculate the causal effect assuming no error
between the proxy and truth. This approach,
while simplistic, can be thought of as using a text
classiﬁer as a proxy without regard for the text
classiﬁer’s biases. In Figure 5 (b), this approach
is labeled as *.unadjusted.

5.3.4 Correct Models

Finally, we consider the estimation approaches
presented in §4.1 and §4.2. For the missing data
causal effect (τMD from Eq 2) we use a multiple
imputation estimator which calculates the average
effect across 20 samples from p(A|T, C, Y ) for
each row where RA = 0. For the measurement
error causal effect (τME from Eq 3), we use the
training set of p(A, C, Y, T) data to estimate ǫc,y
and δc, y and the larger set of p(C, Y, T) data to
estimate qc,y and p(C).

These models are displayed in Figure 5 (a) as

*.full and in Figure 5 (b) *.adjusted.

5.4 Evaluation

takes

Each model
in a data sample with
missingness or mismeasurement, and outputs an
estimate of the causal effect of A on Y in the
underlying data. Rather than comparing models’
estimates against a population-level estimate,

we compare against an estimate of the effect
computed on the same data sample, but without
any missing data or measurement error. This
‘perfect data estimator’ may still make errors
given the ﬁnite data sample. We compare against
this estimator to avoid a small-sample case where
an estimator gets lucky. In Figure 5, we plot data
sample size against the squared distance of each
model’s estimate from a perfect data estimator’s
estimate, averaged over ten runs. Figure 6 in
Appendix E contains a second set of experiments
using a larger vocabulary.

6 Results

Given that our correctly-speciﬁed models are
proven to be asymptotically consistent, we would
expect them to outperform misspeciﬁed models.
However,
asymptotic
for any given dataset,
consistency provides no guarantees.

6.1 Missing Data

The missing data (MD) experiments suggest that
the correct full model does perform best. The
no y model performs approximately as well as the
correct model on the synthetic data, but not on the
Yelp data. The difference between the no y and
full missing data models is simply a function
of the effect of Y on RA. We could tweak our
synthetic data distribution to increase the inﬂuence
of Y to make the no y model perform worse.
initially

other
data-generating distributions for missing data,
we found that when we reduced the inﬂuence
of the text variables on RA, the no text and
naive models approached the performance of
the correctly-speciﬁed model. While intuitive,
this reinforces that
the underlying distribution
matters a great deal in how modeling choices may
introduce biases if incorrectly speciﬁed.

considered

When

we

6.2 Measurement error

results

The measurement error
tell a more
interesting story. We see enormous ﬂuctuations
of the adjusted model, and in the synthetic
data, the unadjusted model appears to be quite
superior.

In the synthetic dataset, this is likely because
our text classiﬁer had near-perfect accuracy, and
so simple approach of assuming its predictions
were ground-truth introduced less bias. A broader
issue with the adjusted model
the

is that

matrix adjustment approach requires dividing
this
by (potentially very small) probabilities,
sometimes resulted in huge over-corrections.
In
addition, since those probabilities are estimated
training dataset, small
from a relatively small
changes to the error-estimate can propagate to
huge changes in the ﬁnal casual estimate.

This

instability of

the matrix adjustment
approach may be a bigger problem for text and
other high-dimensional data: unlike in our earlier
example of BMI and obesity,
there are likely
no simple relationships between text and clinical
variables. However,
instead of using matrix
adjustment as a way to recover the true effect, we
may instead use it to bound the error our proxy
may introduce. As mentioned by Pearl (2010),
when p(A | A∗) is not known exactly, we can
use a Bayesian analysis to bound estimates of a
causal effect. In a downstream task, this would let
us explore the stability of our adjusted results.

7 Related Work

(2017)

explored

considered the
A few recent papers have
combining text data with
for
possibilities
from the
approaches
inference
causal
literature.
Landeiro and Culotta (2016) and
Landeiro and Culotta
text
classiﬁcation when the relationship between text
data and class labels are confounded. Other work
has used propensity scores as a way to extract
features from text data (Paul, 2017) or to match
social media users based on what words they
write (De Choudhury et al., 2016). The only work
we know of which seeks to estimate causal effects
using text data focuses on effects of text or effects
on text (Egami et al., 2018; Roberts et al., 2018).
In our work, our causal effects do not include
text variables: we use text variables to recover an
underlying distribution and then estimate a causal
effect within that distribution.

There is a conceptually related line of work
in the NLP community on inferring causal
relationships expressed in text
(Girju, 2003;
Kaplan and Berry-Rogghe, 1991). However, our
work is fundamentally different. Rather than
identify casual relations expressed via language,
we are using text data in a causal model to identify
the strength of an underlying causal effect.

(a) Missing Data

(b) Measurement Error

100

10−1

10−2

10−3

10−4

10−5

10−6

10−7

10−8

10−9

Yelp.naive
Yelp.no text
Yelp.no y
Yelp.full
Synthetic.naive
Synthetic.no text
Synthetic.no y
Synthetic.full

Yelp.naive
Yelp.unadjusted
Yelp.adjusted
Synthetic.naive
Synthetic.unadjusted
Synthetic.adjusted

102

103

104

105

106

107

102

103

104

105

106

107

Dataset Size

Dataset Size

Figure 5: Experimental results. Squared distance (y-axis, lower is better) of the estimated causal effect
from τSC calculated from the full data with no missing data or measurement error. Error bars (negligible
for larger datasets) are 1.96 times standard error across 10 experiments. Additional experiments with a
larger vocabulary are shown in Appendix E.

8 Future Directions

issues
While this paper addresses some initial
arising from using text classiﬁers in causal
analyses, many challenges remain. We highlight
some of these issues as directions for future
research.

We provided several proof-of-concept models
for estimating effects, but our approach is
ﬂexible to more sophisticated models.
For
a semi-parametric estimator would
example,
make no assumptions
text data
about
distribution by wrapping the text classiﬁer into
an inﬁnite-dimensional nuisance model (Tsiatis,
2007). This would enable estimators robust to
partial model misspeciﬁcation (Bang and Robins,
2005).

the

Choices in the design of statistical models of
text consider issues like accuracy and tractability.
Yet if these models are to be used in a causal
framework, we need to understand how modeling
assumptions introduce biases and other issues that
can interfere with a downstream causal analysis.
To take an example from the medical domain, we

know that doctors write clinical notes throughout
the healthcare process, but it is not obvious how
to model this data-generating process. We could
assume that the doctor’s notes passively record
a patient’s progression, but in reality it may be
that the content of the notes themselves actively
change the patient’s care; causality could work in
either direction.

New lines of work in causality may be
especially helpful for NLP. In this work, we used
simple logistic regression on a bag-of-words
text;
representation of
using state-of-the-art
text models will
likely require more causal
assumptions. Nabi and Shpitser (2017) develops
causality-preserving
reduction,
which could help develop text representations that
preserve causality.

dimensionality

Finally, we are interested in case studies on
incorporating text classiﬁers into real-world causal
analyses. Many health studies have used text
classiﬁers to extract clinical variables from EHR
data (Meystre et al., 2008). These works could
be extended to study causal effects involving

those extracted variables, but such extensions
would require an understanding of the underlying
assumptions.
In any given study, the necessity
and appropriateness of assumptions will hinge
The conceptualizations
on domain expertise.
outlined in this paper, while far from solving all
issues of causality and text, will help those using
text classiﬁers to more easily consider research
questions of cause and effect.

Acknowledgments

This work was in part supported by the National
Institute of General Medical Sciences under grant
number 5R01GM114771 and by the National
Institute of Allergy and Infectious Diseases under
grant number R01 AI127271-01A1. We thank the
anonymous reviewers for their helpful comments.

References

Heejung Bang and James M Robins. 2005. Doubly
robust estimation in missing data and causal
inference models. Biometrics, 61(4):962–973.

Alexandre Belloni, Victor Chernozhukov,

and
Christian Hansen. 2014.
Inference on treatment
effects after selection among high-dimensional
The Review of Economic Studies,
controls.
81(2):608–650.

Victor Chernozhukov, Denis Chetverikov, Mert
Demirer, Esther Duﬂo, Christian Hansen, and
Double machine
Whitney K Newey. 2016.
learning for
treatment and causal parameters.
Technical report, cemmap working paper, Centre
for Microdata Methods and Practice.

A Philip Dawid. 2010. Beware of the dag!

In
Causality: Objectives and Assessment, pages 59–86.

Munmun De Choudhury, Emre Kiciman, Mark Dredze,
and Mrinal Kumar. 2016.
Glen Coppersmith,
Discovering shifts to suicidal ideation from mental
In Proceedings
health content in social media.
of the 2016 CHI conference on human factors in
computing systems, pages 2098–2110. ACM.

Naoki Egami, Christian J Fong, Justin Grimmer,
Margaret E Roberts, and Brandon M Stewart. 2018.
How to make causal inferences using texts. arXiv
preprint arXiv:1802.02163.

Roxana Girju. 2003. Automatic detection of causal
In Proceedings
relations for question answering.
the ACL 2003 workshop on Multilingual
of
summarization and question answering-Volume
12, pages 76–83. Association for Computational
Linguistics.

Brian Hazlehurst, Allison Naleway, and John Mullooly.
2009. Detecting possible vaccine adverse events
in clinical notes of the electronic medical record.
Vaccine, 27(14):2077–2083.

Miguel A Hern´an and Stephen R Cole. 2009. Invited
commentary: causal diagrams and measurement
epidemiology,
bias.
170(8):959–962.

American journal of

Randy M Kaplan and Genevieve Berry-Rogghe. 1991.
Knowledge-based acquisition of causal relationships
in text. Knowledge Acquisition, 3(3):317–337.

Manabu Kuroki and Judea Pearl. 2014. Measurement
inference.

bias and effect restoration in causal
Biometrika, 101(2):423–437.

Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao.
2015. Recurrent convolutional neural networks for
In AAAI, volume 333, pages
text classiﬁcation.
2267–2273.

Virgile Landeiro and Aron Culotta. 2016. Robust text
classiﬁcation in the presence of confounding bias. In
Thirtieth AAAI Conference on Artiﬁcial Intelligence.

Virgile Landeiro and Aron Culotta. 2017. Controlling
classiﬁcation
confounds
arXiv preprint

for
using correlational constraints.
arXiv:1703.01671.

unobserved

in

and

St´ephane M Meystre, Guergana K Savova, Karin C
Kipper-Schuler,
2008.
Extracting information from textual documents
in the electronic health record:
a review of
recent research. Yearbook of medical informatics,
17(01):128–144.

John F Hurdle.

Karin B Michels, Sander Greenland, and Bernard A
Rosner. 1998. Does body mass index adequately
capture the relation of body composition and body
American Journal of
size to health outcomes?
Epidemiology, 147(2):167–172.

Razieh Nabi and Ilya Shpitser. 2017. Semi-parametric
reduction
of
arXiv preprint

treatments.

dimension

causal
sufﬁcient
high dimensional
arXiv:1710.06727.

Jerzy Neyman. 1923. Sur les applications de la thar
des probabilities aux experiences agaricales: Essay
des principle. excerpts reprinted (1990) in English.
Statistical Science, 5:463–472.

Michael J Paul. 2017. Feature selection as causal
inference: Experiments with text classiﬁcation.
the 21st Conference on
In Proceedings of
Learning
Computational Natural
(CoNLL 2017), pages 163–172.

Language

Judea Pearl. 1995. Causal diagrams for empirical

research. Biometrika, 82(4):669–688.

Judea Pearl. 2009. Causality. Cambridge university

press.

Judea Pearl. 2010. On measurement bias in causal
the Twenty-Sixth
inference.
Conference on Uncertainty in Artiﬁcial Intelligence,
pages 425–432. AUAI Press.

In Proceedings of

Margaret E Roberts, Brandon M Stewart,

and
Adjusting for

Richard A Nielsen. 2018.
confounding with text matching.

James M Robins, Andrea Rotnitzky, and Daniel O
Scharfstein. 2000. Sensitivity analysis for selection
bias and unmeasured confounding in missing data
and causal inference models. In Statistical models
in epidemiology, the environment, and clinical trials,
pages 1–94. Springer.

D. B. Rubin. 1976. Causal inference and missing data

(with discussion). Biometrika, 63:581–592.

Ilya Shpitser, Karthika Mohan, and Judea Pearl.
2015. Missing data as a causal and probabilistic
the Thirty-First
problem.
Conference on Uncertainty in Artiﬁcial Intelligence,
pages 802–811. AUAI Press.

In Proceedings of

Ilya Shpitser and Judea Pearl. 2008.

Complete
identiﬁcation methods for the causal hierarchy.
Journal
Research,
9(Sep):1941–1979.

of Machine

Learning

Anastasios Tsiatis. 2007. Semiparametric theory and
missing data. Springer Science & Business Media.

¨Ozlem Uzuner, Ira Goldstein, Yuan Luo, and Isaac
Identifying patient smoking status
Journal of
Informatics Association,

Kohane. 2008.
from medical discharge records.
the American Medical
15(1):14–24.

Stefan Wager and Susan Athey. 2017. Estimation and
inference of heterogeneous treatment effects using
random forests. Journal of the American Statistical
Association.

Richard Wicentowski and Matthew R Sydes. 2008.
Using implicit
information to identify smoking
status in smoke-blind medical discharge summaries.
Journal of
Informatics
the American Medical
Association, 15(1):29–31.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
text
In Advances in neural information

Character-level convolutional networks for
classiﬁcation.
processing systems, pages 649–657.

A Simple Confounding

p(Y (a)) =

p(Y (a) | C)p(C)

(4)

=

=

C
X

C
X

C
X

p(Y (a) | A, C)p(C)

(5)

p(Y | A, C)p(C)

(6)

Eq 5 holds because Y (a) ⊥ A | C, as seen in
Figure 1a. Plugging this distribution into τS =
E[Y (1)] − E[Y (0)] gives us the causal effect
presented in Figure 4, Eq 1.

This assumes that an intervention on A is
if we did conduct a randomized
well-deﬁned;
control trial, we could assign A = a and break
A’s dependence on C.

In general, this step requires that we condition
on all “back-door” paths between the treatment
and the outcome. In Figure 1(a), if we did not have
data on C, we could not block the back-door path
between A and Y .

Eq 6 holds due to consistency. We assume that,
given we intervened to set A = a, if that individual
would have been assigned A = a in nature, then
the distribution over Y is the same.

B Missing Data

Denote p(Y (A(1) = a)) = p(Y (a)).

First, we identify the causal effect in terms of

the true A(1).

p(Y (a))

=

=

=

C
X

C
X

C
X

p(Y (a) | A(1), C)p(C)

p(Y | A(1), C)p(C)

(7)

(8)

(9)

Where 7 holds by chain rule, 8 holds by A(1) ⊥

Y (a) | C, and 9 by consistency.

Now, we identify A(1) in terms of observed

data.

p(A(1), C, Y )

= p(A(1) | C, Y )p(C, Y )
= p(A(1) | C, Y, RA = 1)p(C, Y )
= p(A | C, Y, RA = 1)p(C, Y )

(10)

(11)

(12)

Where 10 holds by chain rule, 11 by A(1) ⊥

RA | C, Y , and 12 by consistency.

Now, use Eq 12 to identify p(Y | A(1), C) from

Eq 9 in terms of observed data.

p(Y | A(1), C)

=

=

=

P

(14)

(13)

p(Y, A(1), C)
p(A(1), C)
p(Y, A(1), C)
Y p(Y, A(1), C)
p(A | C, Y, RA = 1)p(C, Y )
Y p(A | C, Y, RA = 1)p(C, Y )
p(A | C, Y, RA = 1)p(Y | C)
Y p(A | C, Y, RA = 1)p(Y | C)
Where 13 holds by deﬁnition, 14 holds by
marginalization, 15 holds by an application of 12
twice, and 16 holds by canceling out p(C).

(15)

(16)

P

P

=

If we include text in this derivation, we simply
replace p(A | C, Y, RA = 1) with p(A |
T, C, Y, RA = 1), where T is all our text
variables.

Finally, combine Eq 9 and Eq 16 to get:

p(Y (A(1) = a))

=

XC

P

p(A | C, Y, RA = 1)p(Y | C)
Y p(A | C, Y, RA = 1)p(Y | C)

p(C)

(17)

Plugging this distribution into τMD

=
E[Y (1)] − E[Y (0)] gives us the causal effect
presented in Figure 4, Eq 2.

C Measurement Error

ǫc,y = p(A = 0 | A
δc, y = p(A = 1 | A

= 1, C = c, Y = y)
= 0, C = c, Y = y)

∗

∗

qc,y(0) = p(C = c, Y = y, A
qc,y(1) = p(C = c, Y = y, A

= 0)

= 1)

∗

∗

(18)

(19)

(20)

(21)

Eq (5) and (7) from Pearl 2010 gives us:

p(A = 1, C = c, Y = y)

=

−δc,yqc,y(0) + (1 − δc,y)qc,y(1)
(1 − ǫc,y − δc,y)

p(A = 0, C = c, Y = y)

=

(1 − ǫc,y)qc,y(0) − ǫc,yqc,y(1)
(1 − ǫc,y − δc,y)

(22)

(23)

p(Y (a) | C)p(C)

Deﬁne the following terms for convenience:

Now,

D Synthetic Data Distribution

p(Y | A = 1, C)

p(Y, A = 1, C)
p(A = 1, C)
p(Y, A = 1, C)
Y p(Y, A = 1, C)
−δc,yqc,y(0) + (1 − δc,y)qc,y(1)
(1 − ǫc,y − δc,y)

P

=

=

=

−δc,y′qc,y′(0) + (1 − δc,y′)qc,y′(1)
(1 − ǫc,y′ − δc,y′)

y′

P

and then,

p(Y | A = 0, C)

p(Y, A = 0, C)
p(A = 0, C)
p(Y, A = 0, C)
Y p(Y, A = 0, C)
(1 − ǫc,y)qc,y(0) − ǫc,yqc,y(1)
(1 − ǫc,y − δc,y)

P

=

=

=

(1 − ǫc,y′)qc,y′(0) − ǫc,y′qc,y′(1)
(1 − ǫc,y′ − δc,y′)

y′

P

Plugging this distribution

=
E[Y (1)] − E[Y (0)] gives us the causal effect
presented in Figure 4, Eq 3.

into τME

(24)

(25)

(26)

(27)

(28)

In the distributions below, Ber(p) is used as
the abbreviation a Bernoulli distribution with
probability p.

Below, si, ui and vi are the effect of C, A,
and Y on the probability of word Ti; each is
drawn from N (0, ζ), a parameter which controls
how correlated words are with the underlying
variables. When ζ is close to 0,
the words
are essentially random. When ζ is large,
the
words are essentially deterministic functions of
the underlying variables. Similarly wi is the effect
of word Ti on RA, and is drawn from N (0, η).

For both settings, we set vocabulary size to
4,334 (to match Yelp experiments) and ζ = 0.5.
For the missing data setting, we set η = 0.1. We
picked these constants by empirically ﬁnding a
reasonable middle ground between the text data
providing only noise and being a deterministic
function of their parents. We picked all other
constants such that the na¨ıve correlation p(Y | A)
was a poor estimate of the counterfactual p(Y (a))
in the full-data setting.

D.1 Missing data data-generation

C ∼ Ber(0.4)

(29)

A(1) ∼ Ber(−0.3C + 0.4)

Y ∼ Ber(0.2C + 0.1A + 0.5)
Ti ∼ Ber(0.5 + uiA + viC)

RA ∼ Ber

0.7 + 0.2C − 0.4Y +

wiTi

 

i
X

!

D.2 Measurement error data-generation

C ∼ Ber(0.4)
A ∼ Ber(−0.3C + 0.4)
Y ∼ Ber(0.2C + 0.1A + 0.5)
Ti ∼ Ber(0.5 + siC + uiA + viY )

(a) Missing Data

(b) Measurement Error

Yelp.naive
Yelp.no text
Yelp.no y
Yelp.full
Synthetic.naive
Synthetic.no text
Synthetic.no y
Synthetic.full

Yelp.naive
Yelp.unadjusted
Yelp.adjusted
Synthetic.naive
Synthetic.unadjusted
Synthetic.adjusted

101

102

103

104

105

106

107

101

102

103

104

105

106

107

Dataset Size

Dataset Size

Figure 6: Experimental results with a vocabulary of size 53,197. Squared distance (y-axis, lower is better)
of the estimated causal effect from τSC calculated from the full data with no missing data or measurement
error. Error bars (negligible for larger datasets) are 1.96 times standard error across 10 experiments.

100

10−1

10−2

10−3

10−4

10−5

10−6

10−7

10−8

10−9

E Additional Experiments

Figure 6 shows the results of a second set
to those
of experiments, which are identical
described in §5 except the vocabulary size is now
53,197 instead of 4,334. For the Yelp data, the
larger vocabulary consists of all words which
ten times in a sample of 1M
appear at
reviews. As the larger vocabulary introduced
greater memory requirements, we did not run
these experiments with as large of datasets.

least

The results of these experiments show roughly
the same patterns as those seen in Figure 5.
The adjusted measurement error models again
appear erratic, generally performing worse than
the unadjusted models though better than the
naive models.

The full missing data model appeared to
slightly outperform the no y model on Yelp
data but only perform as well on the synthetic
data. Both these models appeared better than the
naive and no text models on both datasets.

Challenges of Using Text Classiﬁers for Causal Inference

Zach Wood-Doughty∗, Ilya Shpitser†, Mark Dredze∗†
Department of Computer Science
∗Center for Language and Speech Processing
†Malone Center for Engineering in Healthcare
Johns Hopkins University, Baltimore, MD 21218
{zach,ilyas,mdredze}@cs.jhu.edu

8
1
0
2
 
t
c
O
 
1
 
 
]
L
C
.
s
c
[
 
 
1
v
6
5
9
0
0
.
0
1
8
1
:
v
i
X
r
a

Abstract

causal

Causal understanding is essential for many
kinds of decision-making, but causal inference
from observational data has typically only
been applied to structured, low-dimensional
datasets. While text classiﬁers produce
low-dimensional outputs, their use in causal
inference has not previously been studied.
analyses based on
To facilitate
language data, we consider the role that text
classiﬁers can play in causal inference through
established modeling mechanisms from the
causality literature on missing data and
measurement error. We demonstrate how to
conduct causal analyses using text classiﬁers
on simulated and Yelp data, and discuss the
opportunities and challenges of future work
that uses text data in causal inference.

1 Introduction

scientiﬁc

analyses,

in domains

Most
from
economics to medicine, focus on low-dimensional
structured data.
Many such domains also
have unstructured text data; advances in natural
language processing (NLP) have led to an
increased interest in incorporating language data
into scientiﬁc analyses. While language is
inherently unstructured and high dimensional,
NLP systems can be used to process raw text
to produce structured variables.
For example,
work on identifying undiagnosed side effects
from electronic health records (EHR) uses text
classiﬁers to produce clinical variables from the
raw text (Hazlehurst et al., 2009).
NLP tools may also beneﬁt

the study of
causal inference, which seeks to identify causal
from observational data.
relations
Causal
analyses
low-dimensional
traditionally
structured variables, such as clinical markers and
binary health outcomes. Such analyses require
the data-generating process,
assumptions about

use

counterfactual

which are often simpler with low-dimensional
data. Unlike prediction tasks which are validated
by held-out test sets, causal inference involves
modeling
random variables
represent
(Neyman, 1923; Rubin, 1976)
the outcome of some hypothetical intervention.
To rigorously reason about hypotheticals, we
use causal models to link our counterfactuals to
observed data (Pearl, 2009).

that

NLP provides a natural way to incorporate text
data into causal inference models. We can produce
for example,
low-dimensional variables using,
text classiﬁers, and then run our causal analysis.
However, this straightforward integration belies
several potential issues. Text classiﬁcation is not
perfect, and errors in a NLP algorithm may bias
subsequent analyses. Causal inference requires
understanding how variables inﬂuence one another
and how correlations are confounded by common
causes. Classic methods such as stratiﬁcation
provide a means for handling confounding of
categorical or continuous variables, but it is not
immediately obvious how such work can be
extended to high-dimensional data.

But

2016).

confounders

Recent work has approached high-dimensional
domains via random forests (Wager and Athey,
2017) and other machine learning methods
(Chernozhukov et al.,
even
compared to an analysis that requires hundreds
(Belloni et al., 2014), NLP
of
models with millions of variables are very
physiological
high-dimensional.
symptoms reﬂect complex biological
realities,
many symptoms such as blood pressure are
one-dimensional variables. While doctors can
easily quantify the effect of high blood pressure
on some outcome, can we use the “positivity” of
a restaurant review to estimate a causal effect?
More broadly,
is it possible to employ text
classiﬁcation methods in a causal analysis?

While

We explore methods for the integration of
text classiﬁers into causal inference analyses that
consider confounds introduced by imperfect NLP.
We show what assumptions are necessary for
causal analyses using text, and discuss when
those assumptions may or may not be reasonable.
We draw on the causal
inference literature to
consider two modeling aspects: missing data
In the missing data
and measurement error.
formulation, a variable of interest is sometimes
unobserved, and text data gives us a means
to model
In the
the missingness process.
measurement error formulation, we use a text
classiﬁer
the
underlying variable.

to generate a noisy proxy of

We highlight practical considerations of a
causal analysis with text data by conducting
analyses with simulated and Yelp data. We
examine the results of both formulations and show
how a causal analysis which properly accounts for
possible sources of bias produces better estimates
than na¨ıve methods which make unjustiﬁed
assumptions. We conclude by examining how our
approach may enable new research avenues for
inferring causality with text data.

2 Causal Inference, Brieﬂy

While randomized control trials (RCT) are the
gold standard of determining causal effects of
treatments on outcomes, they can be expensive or
impossible in many settings. In contrast, the world
is ﬁlled with observational data collected without
randomization. While most studies simply report
correlations from observational data, the ﬁeld of
causal inference examines what assumptions and
analyses make it possible to identify causal effects.
We formalize a causal statement like “smoking
causes cancer” as “if we were to conduct a RCT
and assign smoking as a treatment, we would
see a higher incidence of cancer among those
assigned smoking than among the control group.”
In the framework of Pearl (1995), we consider
a counterfactual variable of interest: what would
have been the cancer incidence among smokers
if smoking had been randomized? Speciﬁcally,
we consider a causal effect as the counterfactual
outcome of a hypothetical intervention on some
treatment variable.
If we denote smoking as
treatment variable A and cancer as our
our
outcome variable Y , then we are interested in the
counterfactual distribution, denoted p(Y (a)) or

p(Y | do(a)). We interpret this as “the distribution
over Y had A been set, possibly contrary to fact,
to value a.” For a binary treatment A, the causal
effect of A on Y is denoted τ = E[Y (1)] −
E[Y (0)]; the average difference between if you
had received the treatment and if you had not.
Throughout, we use causal directed acyclic graphs
(DAG), which assumes that an intervention on
A is well-deﬁned and results in a counterfactual
variable Y (a) (Pearl, 1995; Dawid, 2010).

| A),

Figure 1a shows an example of

simple
This is the simplest DAG in
confounding.
which counterfactual distribution p(Y (a)) is not
as C inﬂuences both
simply p(Y
the treatment A and the outcome Y .
To
recover the counterfactual distribution p(Y (a))
that would follow an intervention upon A, we must
“adjust” for C, applying the so-called “back-door
criterion” (Pearl, 1995). We can then derive the
counterfactual distribution p(Y (a)) and desired
causal effect, τS as a function of the observed
data, (Fig. 4 Eq. 1.) This derivation is shown
in Appendix A.

it

in fact observed,

Note that p(Y (a)) and τS require data on
C, and if C is not
is
impossible to recover the causal effect. Formally,
identiﬁed in the
we say that p(Y (a)) is not
model, meaning there is no function f such that
p(Y (a))=f (p(A, Y )). Identiﬁability is a primary
concern of causal inference (Shpitser and Pearl,
2008).

Throughout, we assume for simplicity that A,
C, and Y are binary variables. While text
classiﬁers can convert high-dimensional data into
binary variables for such analyses, we need to
make further assumptions about how classiﬁcation
errors affect causal inferences. We cannot assume
that the output of a text classiﬁer can be treated as
if it were ground truth. To conceptualize the ways
in which a text classiﬁer may be biased, we will
consider them as a way to recover from missing
data or measurement error.

3 Causal Models

Real-world observational data is messy and often
imperfectly collected. Work in causal inference
has studied how analyses can be made robust
to missing data or data recorded with systematic
measurement errors.

C

A

RA

C

Y

A

A(1)

Y

C

A

A∗

Y

(a) Simple Confounding

(b) Missing Data

(c) Measurement Error

Figure 1: DAGs for causal inference without text data. Red variables are unobserved.
A is a treatment, Y is an outcome, and C is a confounder.

A C Y
0
1
1
1
1
0
1
0
0
1
0
1

RA A C Y
0
1
1
0
1
1
1
0

1
1
0
0

1
?
0
?

A∗ C Y
0
1
0
1
1
0
1
0
0
1
0
1

A∗ A
1
1
1
0
0
0
1
1

(a) Simple Confounding

(b) Missing Data

(c) Measurement Error

(d) Mismeasurement

Figure 2: Example data rows for causal inference without text data.

3.1 Missing Data

3.2 Measurement Error

Our dataset has “missing data” if it contains
individuals (instances) for which some variables
are unobserved, even though these variables are
typically available.
This may occur if some
survey respondents choose not to answer certain
questions, or if certain variables are difﬁcult to
collect and thus infrequently recorded. Missing
data is closely related to causal inference – both
are interested in hypothetical distributions that
we cannot directly observe (Robins et al., 2000;
Shpitser et al., 2015).

Consider a causal model where A is sometimes
missing (Figure 1b). The variable RA is a binary
indicator for whether A is observed (RA = 1)
or missing. The variable A(RA = 1), written
as A(1), represents the counterfactual value of A
were it never missing. Finally, A is the observed
proxy for A(1): it has the same value as A(1) if
RA = 1, and the special value “?” if RA = 0.

Solving missingness can seen as intervening to
set RA to 1. Given p(A, RA, C, Y ), we want to
recover p(A(1), C, Y ). We may need to make a
“Missing at Random” (MAR) assumption, which
says that the missingness process is independent of
the true missing values, conditional on observed
values. Figure 1b reﬂects the MAR assumption;
RA is independent of A(1) given fully-observed
C and Y .
If an edge existed from A(1) to RA,
we have “Missing Not at Random” (MNAR) and
would not be identiﬁed except in special cases
(Shpitser et al., 2015).

Sometimes a necessary variable is never observed,
but is instead proxied by a variable which differs
Consider the
from the truth by some error.
example of body mass index (BMI) as a proxy for
obesity in a clinical study. Obesity is a known
risk factor for many health outcomes, but has a
complex clinical deﬁnition and is nontrivial to
measure. BMI is a simple deterministic function
of height and weight. To conduct a causal analysis
of obesity on cancer when only BMI and cancer
are measured, we can proceed as if we had
measured obesity and then correct our analysis
for the known error that comes from using BMI
as a proxy for obesity (Hern´an and Cole, 2009;
Michels et al., 1998).

To generalize this concept, we can replace
obesity with our ground truth variable A
and replace BMI with a noisy proxy A∗.
Figure 1c gives
this model.
the DAG for
there is no
Unlike missing data problems,
hypothetical
the
true data distribution p(A, C, Y ).
Instead, we
manipulate the observed distribution p(A∗, C, Y )
with the known relationship p(A∗, A) to recover
the desired p(A, C, Y ).

intervention which recovers

Unlike missing data, measurement

error
conceptualization can be used even when we
never observe A (e.g.
the table in Figure 2c)
as long as we have knowledge about the error
mechanism p(A∗, A). Using this knowledge,
the error using ‘matrix
we can correct

for

C

A

Ti

Y

V

RA

C

A

A(1)

Ti

V

Y

C

A

A∗

Y

Ti

V

(a) Simple Confounding with Text

(b) Missing Data with Text

(c) Measurement Error with Text

Figure 3: DAGs for causal inference with text data. In the Yelp experiments we discuss, Ti inﬂuences Y
and not the other way around.

τS =

(p(Y = 1 | A = 1, C) − p(Y = 1 | A = 0, C)) p(C)

τMD =

(cid:18)

XC

′

p(A = 1 | T, C, Y = 1, RA = 1)
y p(A = 1 | T, C, y′, RA = 1)p(Y = y′ | C)
p(A = 0 | T, C, Y = 1, RA = 1)

y p(A = 0 | T, C, Y = y′, RA = 1)p(Y = y′ | C) (cid:19)

p(Y = 1, C)

XC

P
−

′

P

(1)

(2)

τME =

XC







y′

P

−δc,y=1qc,y=1(0) + (1 − δc,y=1)qc,y=1(1)
(1 − ǫc,y=1 − δc,y=1)

(1 − ǫc,y=1)qc,y=1(0) − ǫc,y=1qc,y=1(1)
(1 − ǫc,y=1 − δc,y=1)

−δc,y′qc,y′ (0) + (1 − δc,y′)qc,y′ (1)
(1 − ǫc,y′ − δc,y′ )

(1 − ǫc,y′ )qc,y′ (0) − ǫc,y′ qc,y′ (1)
(1 − ǫc,y′ − δc,y′)

−

y′

P

p(C)

(3)







Deﬁne ǫc,y = p(A = 0 | A∗ = 1, C = c, Y = y), δc, y = p(A = 1 | A∗ = 0, C = c, Y = y),
qc,y(0) = p(C = c, Y = y, A∗ = 0), and qc,y(1) = p(C = c, Y = y, A∗ = 1).

Figure 4: Functionals for the Causal Effects for Simple Confounding (τSC), Missing Data (τMD) and
Measurement Error (τME). Derivations are in Appendices A, B, and C.

adjustment’ (Pearl, 2010).
In practice we might
learn p(A∗, A) from data such as that found in
Figure 2d. Recent work has also considered
how multiple independent proxies of A could
allow identiﬁcation without any data on p(A∗, A)
(Kuroki and Pearl, 2014).

4 Causal Models for Text Data

We can use conceptualizations for missing data
and measurement error to support causal analyses
with text data. The choice of model depends on the
assumptions we make about the data-generation
process.

to represent

text, which produces

We add new variables to our models (Figure
1a)
the
data-generating distribution shown in Figure 3a.
This model assumes that the underlying A, C, and
Y variables are generated before the text variables;
we use text
the true relationship
between A and Y .
We represent

text as an arbitrary set of
V variables, which are independent of one

to recover

another given the non-text variables.
In our
implemented analyses we will represent text as
a bag-of-words, wherein each Ti is simply the
binary indicator of the presence of the i-th word
in our vocabulary of V words, and T = ∪iTi.
The restriction to simple text models allows
us to explore connections to causal
inference
applications,
though future work could relax
assumptions of the text models to be inclusive
of more sophisticated text models (e.g. neural
sequence models (Lai et al., 2015; Zhang et al.,
2015)), or consider causal relationships between
two text variables.

To motivate our explanations, consider the task
of predicting an individuals’ smoking status from
free-text hospital discharge notes (Uzuner et al.,
2008; Wicentowski and Sydes, 2008).
Some
hospitals do not explicitly record patient smoking
status as structured data, making it difﬁcult to use
such data in a study on the outcomes of smoking.
We will suppose that we are given a dataset with
patient data on lung cancer outcome (Y ) and age

(C), that our data on smoking status (A) is affected
by either missing data or measurement error, but
that we have text data (T) from discharge records
that will allow us to infer smoking status with
reasonable accuracy.

4.1 Missing Data

To show how we might use text data to recover
from missing data, we introduce missingness for
A from Figure 3a to get the model in Figure 3b.
The missing arrow from A(1) to RA encodes the
MAR assumption, which is sufﬁcient to make it
possible to identify the full data distribution from
the observed data.

Suppose our motivation is to estimate the causal
effect of smoking status (A) on lung cancer (Y )
adjusting for age (C).
Imagine that missing
data arises because hospitals sometimes – but not
always – delete explicit data on smoking status
from patient records. If we have access to patients’
discharge notes (T) and know whether a given
patient had smoking status recorded (RA), then the
DAG in Figure 3b may be a reasonable model for
our setting. Note that we must again assume that
A does not directly affect RA.

The causal effect of A on Y in Figure 3b is
identiﬁed as τM D, given in Eq. 2 in Figure 4. The
derivation is given in Appendix B.

4.2 Measurement Error

We model text data with measurement error by
introducing a proxy A∗ to the model in Figure
3c. We assume that the proxied value of A∗ can
depend upon all other variables, and that we will
be able to estimate p(A∗, A) given an external
dataset, e.g.
text classiﬁer accuracy on held-out
data.

Suppose we again want to estimate the causal
effect from §4.1, but this time none of our hospital
records contain explicit data on smoking status.
However, imagine that we have a separate training
dataset of medical discharge records annotated
by expert pulmonologists for patients’ smoking
status. We could then train a classiﬁer to predict
smoking status using discharge record text1.

Working from the derivation for matrix
in binary models given by Pearl
adjustment
(2010), we identify the causal effect of A on
Y (Figure 3c) as τME (Eq 3 in Figure 4.) The
derivation is in Appendix C.

1This is the precise setting of Uzuner et al. (2008).

5 Experiments

We now empirically evaluate the effectiveness
of our two conceptualizations (missing data and
measurement error) for including text data in
causal analyses. We induce missingness or
mismeasurement of the treatment variable and use
text data to recover the true causal relationship
treatment on the outcome. We begin
of that
with a simulation study with synthetic text data,
and then conduct an analysis using reviews from
yelp.com.

5.1 Synthetic Data

A graphical model

We select synthetic data so that we can control
the entire data-generation process.
For each
data row, we ﬁrst sample data on three binary
variables (A, C, Y ) and then sample V different
binary variables Ti representing a V -vocabulary
bag-of-words.
this
distribution appears in Figure 3a. We augment
this distribution to introduce either missing
data (Figure 3b) or measurement error (Figure
For measurement error, we sample two
3c.)
datasets. A small training set which gives data on
p(A, C, Y, T) and a large test set which gives data
on p(C, Y, T).

for

The full data generating process appears in
Appendix D, and the implementation (along with
all our code) is provided online2.

5.2 Yelp Data
We utilize the 2015 Yelp Dataset Challenge3
which provides 4.7M reviews of local businesses.
Each review contains a one- to ﬁve-star rating, up
to 5,000 characters of text. Yelp users can ﬂag
reviews as “Useful” as a mark of quality.

We extract treatment, outcome, and confounder
variables from the structured data. The treatment
is a binarized user rating that takes value 1 if
the review has four or ﬁve stars and value 0
if the review has one or two stars. Three-star
reviews are discarded from our analysis. The
outcome is whether the review received at least
one “Useful” ﬂag. The confounder is whether
two
the review’s author has received at
“Useful” ﬂags across all reviews, according to
their user object.
In our data, 74.2% of reviews
were positive, 42.6% of reviews were ﬂagged as
“Useful,” and 56.7% users had received at least

least

2github.com/zachwooddoughty/emnlp2018-causal
3yelp.com/dataset/challenge

two such ﬂags. We preprocess the text of each
review by lowercasing, stemming, and removing
stopwords, before converting to a bag-of-words
representation with the 4,334 word vocabulary of
all words which appeared at least 1000 times in a
sample of 1M reviews.

Based on this p(A, C, Y, T) distribution,
we assume the data-generating process
that
matches Figure 3a and introduce missingness
and mismeasurement
giving us
data-generating processes matching Figures 3b
and 3c.

as before,

Our intention is not

to argue about a true
real-world causal effect of Yelp reviews on peer
behavior: we do not believe that our confounder
is the only common cause of the author’s rating
and the platform’s response. We leave for future
work a case study that jointly addresses questions
of identiﬁability and estimation of a real-world
In this work, our experiments
causal effect.
focus on a simpler task: can a correctly-speciﬁed
model that uses text data effectively estimate a
causal effect in the presence of missing data or
measurement error.

5.3 Models

We now introduce several baseline methods
which, unlike our correctly speciﬁed models τM D
and τM E, are not consistent estimators of our
desired causal effect. We would expect that the
theoretical bias in these estimators would result in
poor performance in our experiments.

5.3.1 Baseline: Na¨ıve Model

In both the missing data and measurement error
settings, our models use some rows that are full
observed. In missing data, these are rows where
RA = 1; in measurement error, the training set is
sampled from the true distribution. The simplest
approach to handling imperfect data is to throw
away all rows without full data, and calculate Eq
1 from that data. In Figure 5, these are labeled as
*.naive.

5.3.2 Baseline: Textless Model

In Figure 3b, if we do not condition on Ti to
d-separate A(1) from its missingness indicator,
that inﬂuence may bias our estimate. While we
know that ignoring text may introduce asymptotic
bias into our estimates of the causal effect, we
empirically evaluate how much bias is produced
by this “Textless” model compared to a correct

model. This is labeled as *.no text in Figure
5 (a).

In principle, we could conduct a measurement
error analysis using a model that does not include
text. In practice, we found we could not impute A∗
from C and Y alone. The non-textual classiﬁer
had such high error that the adjustment matrix
was singular and we could not compute the
effect. Thus, we have no such baseline in our
measurement error results.

5.3.3 Baseline: no y and unadjusted

Models

In Figure 3b, we must also condition on C and Y
to d-separate A(1) from its missingness indicator.
In our misspeciﬁed model for missing data, we do
not condition on Y , leaving open a path for A(1)
to inﬂuence its missingness. In Figure 5 (a), this
model is labeled as *.no y.

When correcting for measurement error, a
crucial piece of the estimation is the matrix
adjustment using the known error between
the proxy and the truth.
A straightforward
misspeciﬁed model for measurement error is to
impute a proxy for each row in our dataset and
then calculate the causal effect assuming no error
between the proxy and truth. This approach,
while simplistic, can be thought of as using a text
classiﬁer as a proxy without regard for the text
classiﬁer’s biases. In Figure 5 (b), this approach
is labeled as *.unadjusted.

5.3.4 Correct Models

Finally, we consider the estimation approaches
presented in §4.1 and §4.2. For the missing data
causal effect (τMD from Eq 2) we use a multiple
imputation estimator which calculates the average
effect across 20 samples from p(A|T, C, Y ) for
each row where RA = 0. For the measurement
error causal effect (τME from Eq 3), we use the
training set of p(A, C, Y, T) data to estimate ǫc,y
and δc, y and the larger set of p(C, Y, T) data to
estimate qc,y and p(C).

These models are displayed in Figure 5 (a) as

*.full and in Figure 5 (b) *.adjusted.

5.4 Evaluation

takes

Each model
in a data sample with
missingness or mismeasurement, and outputs an
estimate of the causal effect of A on Y in the
underlying data. Rather than comparing models’
estimates against a population-level estimate,

we compare against an estimate of the effect
computed on the same data sample, but without
any missing data or measurement error. This
‘perfect data estimator’ may still make errors
given the ﬁnite data sample. We compare against
this estimator to avoid a small-sample case where
an estimator gets lucky. In Figure 5, we plot data
sample size against the squared distance of each
model’s estimate from a perfect data estimator’s
estimate, averaged over ten runs. Figure 6 in
Appendix E contains a second set of experiments
using a larger vocabulary.

6 Results

Given that our correctly-speciﬁed models are
proven to be asymptotically consistent, we would
expect them to outperform misspeciﬁed models.
However,
asymptotic
for any given dataset,
consistency provides no guarantees.

6.1 Missing Data

The missing data (MD) experiments suggest that
the correct full model does perform best. The
no y model performs approximately as well as the
correct model on the synthetic data, but not on the
Yelp data. The difference between the no y and
full missing data models is simply a function
of the effect of Y on RA. We could tweak our
synthetic data distribution to increase the inﬂuence
of Y to make the no y model perform worse.
initially

other
data-generating distributions for missing data,
we found that when we reduced the inﬂuence
of the text variables on RA, the no text and
naive models approached the performance of
the correctly-speciﬁed model. While intuitive,
this reinforces that
the underlying distribution
matters a great deal in how modeling choices may
introduce biases if incorrectly speciﬁed.

considered

When

we

6.2 Measurement error

results

The measurement error
tell a more
interesting story. We see enormous ﬂuctuations
of the adjusted model, and in the synthetic
data, the unadjusted model appears to be quite
superior.

In the synthetic dataset, this is likely because
our text classiﬁer had near-perfect accuracy, and
so simple approach of assuming its predictions
were ground-truth introduced less bias. A broader
issue with the adjusted model
the

is that

matrix adjustment approach requires dividing
this
by (potentially very small) probabilities,
sometimes resulted in huge over-corrections.
In
addition, since those probabilities are estimated
training dataset, small
from a relatively small
changes to the error-estimate can propagate to
huge changes in the ﬁnal casual estimate.

This

instability of

the matrix adjustment
approach may be a bigger problem for text and
other high-dimensional data: unlike in our earlier
example of BMI and obesity,
there are likely
no simple relationships between text and clinical
variables. However,
instead of using matrix
adjustment as a way to recover the true effect, we
may instead use it to bound the error our proxy
may introduce. As mentioned by Pearl (2010),
when p(A | A∗) is not known exactly, we can
use a Bayesian analysis to bound estimates of a
causal effect. In a downstream task, this would let
us explore the stability of our adjusted results.

7 Related Work

(2017)

explored

considered the
A few recent papers have
combining text data with
for
possibilities
from the
approaches
inference
causal
literature.
Landeiro and Culotta (2016) and
Landeiro and Culotta
text
classiﬁcation when the relationship between text
data and class labels are confounded. Other work
has used propensity scores as a way to extract
features from text data (Paul, 2017) or to match
social media users based on what words they
write (De Choudhury et al., 2016). The only work
we know of which seeks to estimate causal effects
using text data focuses on effects of text or effects
on text (Egami et al., 2018; Roberts et al., 2018).
In our work, our causal effects do not include
text variables: we use text variables to recover an
underlying distribution and then estimate a causal
effect within that distribution.

There is a conceptually related line of work
in the NLP community on inferring causal
relationships expressed in text
(Girju, 2003;
Kaplan and Berry-Rogghe, 1991). However, our
work is fundamentally different. Rather than
identify casual relations expressed via language,
we are using text data in a causal model to identify
the strength of an underlying causal effect.

(a) Missing Data

(b) Measurement Error

100

10−1

10−2

10−3

10−4

10−5

10−6

10−7

10−8

10−9

Yelp.naive
Yelp.no text
Yelp.no y
Yelp.full
Synthetic.naive
Synthetic.no text
Synthetic.no y
Synthetic.full

Yelp.naive
Yelp.unadjusted
Yelp.adjusted
Synthetic.naive
Synthetic.unadjusted
Synthetic.adjusted

102

103

104

105

106

107

102

103

104

105

106

107

Dataset Size

Dataset Size

Figure 5: Experimental results. Squared distance (y-axis, lower is better) of the estimated causal effect
from τSC calculated from the full data with no missing data or measurement error. Error bars (negligible
for larger datasets) are 1.96 times standard error across 10 experiments. Additional experiments with a
larger vocabulary are shown in Appendix E.

8 Future Directions

issues
While this paper addresses some initial
arising from using text classiﬁers in causal
analyses, many challenges remain. We highlight
some of these issues as directions for future
research.

We provided several proof-of-concept models
for estimating effects, but our approach is
ﬂexible to more sophisticated models.
For
a semi-parametric estimator would
example,
make no assumptions
text data
about
distribution by wrapping the text classiﬁer into
an inﬁnite-dimensional nuisance model (Tsiatis,
2007). This would enable estimators robust to
partial model misspeciﬁcation (Bang and Robins,
2005).

the

Choices in the design of statistical models of
text consider issues like accuracy and tractability.
Yet if these models are to be used in a causal
framework, we need to understand how modeling
assumptions introduce biases and other issues that
can interfere with a downstream causal analysis.
To take an example from the medical domain, we

know that doctors write clinical notes throughout
the healthcare process, but it is not obvious how
to model this data-generating process. We could
assume that the doctor’s notes passively record
a patient’s progression, but in reality it may be
that the content of the notes themselves actively
change the patient’s care; causality could work in
either direction.

New lines of work in causality may be
especially helpful for NLP. In this work, we used
simple logistic regression on a bag-of-words
text;
representation of
using state-of-the-art
text models will
likely require more causal
assumptions. Nabi and Shpitser (2017) develops
causality-preserving
reduction,
which could help develop text representations that
preserve causality.

dimensionality

Finally, we are interested in case studies on
incorporating text classiﬁers into real-world causal
analyses. Many health studies have used text
classiﬁers to extract clinical variables from EHR
data (Meystre et al., 2008). These works could
be extended to study causal effects involving

those extracted variables, but such extensions
would require an understanding of the underlying
assumptions.
In any given study, the necessity
and appropriateness of assumptions will hinge
The conceptualizations
on domain expertise.
outlined in this paper, while far from solving all
issues of causality and text, will help those using
text classiﬁers to more easily consider research
questions of cause and effect.

Acknowledgments

This work was in part supported by the National
Institute of General Medical Sciences under grant
number 5R01GM114771 and by the National
Institute of Allergy and Infectious Diseases under
grant number R01 AI127271-01A1. We thank the
anonymous reviewers for their helpful comments.

References

Heejung Bang and James M Robins. 2005. Doubly
robust estimation in missing data and causal
inference models. Biometrics, 61(4):962–973.

Alexandre Belloni, Victor Chernozhukov,

and
Christian Hansen. 2014.
Inference on treatment
effects after selection among high-dimensional
The Review of Economic Studies,
controls.
81(2):608–650.

Victor Chernozhukov, Denis Chetverikov, Mert
Demirer, Esther Duﬂo, Christian Hansen, and
Double machine
Whitney K Newey. 2016.
learning for
treatment and causal parameters.
Technical report, cemmap working paper, Centre
for Microdata Methods and Practice.

A Philip Dawid. 2010. Beware of the dag!

In
Causality: Objectives and Assessment, pages 59–86.

Munmun De Choudhury, Emre Kiciman, Mark Dredze,
and Mrinal Kumar. 2016.
Glen Coppersmith,
Discovering shifts to suicidal ideation from mental
In Proceedings
health content in social media.
of the 2016 CHI conference on human factors in
computing systems, pages 2098–2110. ACM.

Naoki Egami, Christian J Fong, Justin Grimmer,
Margaret E Roberts, and Brandon M Stewart. 2018.
How to make causal inferences using texts. arXiv
preprint arXiv:1802.02163.

Roxana Girju. 2003. Automatic detection of causal
In Proceedings
relations for question answering.
the ACL 2003 workshop on Multilingual
of
summarization and question answering-Volume
12, pages 76–83. Association for Computational
Linguistics.

Brian Hazlehurst, Allison Naleway, and John Mullooly.
2009. Detecting possible vaccine adverse events
in clinical notes of the electronic medical record.
Vaccine, 27(14):2077–2083.

Miguel A Hern´an and Stephen R Cole. 2009. Invited
commentary: causal diagrams and measurement
epidemiology,
bias.
170(8):959–962.

American journal of

Randy M Kaplan and Genevieve Berry-Rogghe. 1991.
Knowledge-based acquisition of causal relationships
in text. Knowledge Acquisition, 3(3):317–337.

Manabu Kuroki and Judea Pearl. 2014. Measurement
inference.

bias and effect restoration in causal
Biometrika, 101(2):423–437.

Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao.
2015. Recurrent convolutional neural networks for
In AAAI, volume 333, pages
text classiﬁcation.
2267–2273.

Virgile Landeiro and Aron Culotta. 2016. Robust text
classiﬁcation in the presence of confounding bias. In
Thirtieth AAAI Conference on Artiﬁcial Intelligence.

Virgile Landeiro and Aron Culotta. 2017. Controlling
classiﬁcation
confounds
arXiv preprint

for
using correlational constraints.
arXiv:1703.01671.

unobserved

in

and

St´ephane M Meystre, Guergana K Savova, Karin C
Kipper-Schuler,
2008.
Extracting information from textual documents
in the electronic health record:
a review of
recent research. Yearbook of medical informatics,
17(01):128–144.

John F Hurdle.

Karin B Michels, Sander Greenland, and Bernard A
Rosner. 1998. Does body mass index adequately
capture the relation of body composition and body
American Journal of
size to health outcomes?
Epidemiology, 147(2):167–172.

Razieh Nabi and Ilya Shpitser. 2017. Semi-parametric
reduction
of
arXiv preprint

treatments.

dimension

causal
sufﬁcient
high dimensional
arXiv:1710.06727.

Jerzy Neyman. 1923. Sur les applications de la thar
des probabilities aux experiences agaricales: Essay
des principle. excerpts reprinted (1990) in English.
Statistical Science, 5:463–472.

Michael J Paul. 2017. Feature selection as causal
inference: Experiments with text classiﬁcation.
the 21st Conference on
In Proceedings of
Learning
Computational Natural
(CoNLL 2017), pages 163–172.

Language

Judea Pearl. 1995. Causal diagrams for empirical

research. Biometrika, 82(4):669–688.

Judea Pearl. 2009. Causality. Cambridge university

press.

Judea Pearl. 2010. On measurement bias in causal
the Twenty-Sixth
inference.
Conference on Uncertainty in Artiﬁcial Intelligence,
pages 425–432. AUAI Press.

In Proceedings of

Margaret E Roberts, Brandon M Stewart,

and
Adjusting for

Richard A Nielsen. 2018.
confounding with text matching.

James M Robins, Andrea Rotnitzky, and Daniel O
Scharfstein. 2000. Sensitivity analysis for selection
bias and unmeasured confounding in missing data
and causal inference models. In Statistical models
in epidemiology, the environment, and clinical trials,
pages 1–94. Springer.

D. B. Rubin. 1976. Causal inference and missing data

(with discussion). Biometrika, 63:581–592.

Ilya Shpitser, Karthika Mohan, and Judea Pearl.
2015. Missing data as a causal and probabilistic
the Thirty-First
problem.
Conference on Uncertainty in Artiﬁcial Intelligence,
pages 802–811. AUAI Press.

In Proceedings of

Ilya Shpitser and Judea Pearl. 2008.

Complete
identiﬁcation methods for the causal hierarchy.
Journal
Research,
9(Sep):1941–1979.

of Machine

Learning

Anastasios Tsiatis. 2007. Semiparametric theory and
missing data. Springer Science & Business Media.

¨Ozlem Uzuner, Ira Goldstein, Yuan Luo, and Isaac
Identifying patient smoking status
Journal of
Informatics Association,

Kohane. 2008.
from medical discharge records.
the American Medical
15(1):14–24.

Stefan Wager and Susan Athey. 2017. Estimation and
inference of heterogeneous treatment effects using
random forests. Journal of the American Statistical
Association.

Richard Wicentowski and Matthew R Sydes. 2008.
Using implicit
information to identify smoking
status in smoke-blind medical discharge summaries.
Journal of
Informatics
the American Medical
Association, 15(1):29–31.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
text
In Advances in neural information

Character-level convolutional networks for
classiﬁcation.
processing systems, pages 649–657.

A Simple Confounding

p(Y (a)) =

p(Y (a) | C)p(C)

(4)

=

=

C
X

C
X

C
X

p(Y (a) | A, C)p(C)

(5)

p(Y | A, C)p(C)

(6)

Eq 5 holds because Y (a) ⊥ A | C, as seen in
Figure 1a. Plugging this distribution into τS =
E[Y (1)] − E[Y (0)] gives us the causal effect
presented in Figure 4, Eq 1.

This assumes that an intervention on A is
if we did conduct a randomized
well-deﬁned;
control trial, we could assign A = a and break
A’s dependence on C.

In general, this step requires that we condition
on all “back-door” paths between the treatment
and the outcome. In Figure 1(a), if we did not have
data on C, we could not block the back-door path
between A and Y .

Eq 6 holds due to consistency. We assume that,
given we intervened to set A = a, if that individual
would have been assigned A = a in nature, then
the distribution over Y is the same.

B Missing Data

Denote p(Y (A(1) = a)) = p(Y (a)).

First, we identify the causal effect in terms of

the true A(1).

p(Y (a))

=

=

=

C
X

C
X

C
X

p(Y (a) | A(1), C)p(C)

p(Y | A(1), C)p(C)

(7)

(8)

(9)

Where 7 holds by chain rule, 8 holds by A(1) ⊥

Y (a) | C, and 9 by consistency.

Now, we identify A(1) in terms of observed

data.

p(A(1), C, Y )

= p(A(1) | C, Y )p(C, Y )
= p(A(1) | C, Y, RA = 1)p(C, Y )
= p(A | C, Y, RA = 1)p(C, Y )

(10)

(11)

(12)

Where 10 holds by chain rule, 11 by A(1) ⊥

RA | C, Y , and 12 by consistency.

Now, use Eq 12 to identify p(Y | A(1), C) from

Eq 9 in terms of observed data.

p(Y | A(1), C)

=

=

=

P

(14)

(13)

p(Y, A(1), C)
p(A(1), C)
p(Y, A(1), C)
Y p(Y, A(1), C)
p(A | C, Y, RA = 1)p(C, Y )
Y p(A | C, Y, RA = 1)p(C, Y )
p(A | C, Y, RA = 1)p(Y | C)
Y p(A | C, Y, RA = 1)p(Y | C)
Where 13 holds by deﬁnition, 14 holds by
marginalization, 15 holds by an application of 12
twice, and 16 holds by canceling out p(C).

(15)

(16)

P

P

=

If we include text in this derivation, we simply
replace p(A | C, Y, RA = 1) with p(A |
T, C, Y, RA = 1), where T is all our text
variables.

Finally, combine Eq 9 and Eq 16 to get:

p(Y (A(1) = a))

=

XC

P

p(A | C, Y, RA = 1)p(Y | C)
Y p(A | C, Y, RA = 1)p(Y | C)

p(C)

(17)

Plugging this distribution into τMD

=
E[Y (1)] − E[Y (0)] gives us the causal effect
presented in Figure 4, Eq 2.

C Measurement Error

ǫc,y = p(A = 0 | A
δc, y = p(A = 1 | A

= 1, C = c, Y = y)
= 0, C = c, Y = y)

∗

∗

qc,y(0) = p(C = c, Y = y, A
qc,y(1) = p(C = c, Y = y, A

= 0)

= 1)

∗

∗

(18)

(19)

(20)

(21)

Eq (5) and (7) from Pearl 2010 gives us:

p(A = 1, C = c, Y = y)

=

−δc,yqc,y(0) + (1 − δc,y)qc,y(1)
(1 − ǫc,y − δc,y)

p(A = 0, C = c, Y = y)

=

(1 − ǫc,y)qc,y(0) − ǫc,yqc,y(1)
(1 − ǫc,y − δc,y)

(22)

(23)

p(Y (a) | C)p(C)

Deﬁne the following terms for convenience:

Now,

D Synthetic Data Distribution

p(Y | A = 1, C)

p(Y, A = 1, C)
p(A = 1, C)
p(Y, A = 1, C)
Y p(Y, A = 1, C)
−δc,yqc,y(0) + (1 − δc,y)qc,y(1)
(1 − ǫc,y − δc,y)

P

=

=

=

−δc,y′qc,y′(0) + (1 − δc,y′)qc,y′(1)
(1 − ǫc,y′ − δc,y′)

y′

P

and then,

p(Y | A = 0, C)

p(Y, A = 0, C)
p(A = 0, C)
p(Y, A = 0, C)
Y p(Y, A = 0, C)
(1 − ǫc,y)qc,y(0) − ǫc,yqc,y(1)
(1 − ǫc,y − δc,y)

P

=

=

=

(1 − ǫc,y′)qc,y′(0) − ǫc,y′qc,y′(1)
(1 − ǫc,y′ − δc,y′)

y′

P

Plugging this distribution

=
E[Y (1)] − E[Y (0)] gives us the causal effect
presented in Figure 4, Eq 3.

into τME

(24)

(25)

(26)

(27)

(28)

In the distributions below, Ber(p) is used as
the abbreviation a Bernoulli distribution with
probability p.

Below, si, ui and vi are the effect of C, A,
and Y on the probability of word Ti; each is
drawn from N (0, ζ), a parameter which controls
how correlated words are with the underlying
variables. When ζ is close to 0,
the words
are essentially random. When ζ is large,
the
words are essentially deterministic functions of
the underlying variables. Similarly wi is the effect
of word Ti on RA, and is drawn from N (0, η).

For both settings, we set vocabulary size to
4,334 (to match Yelp experiments) and ζ = 0.5.
For the missing data setting, we set η = 0.1. We
picked these constants by empirically ﬁnding a
reasonable middle ground between the text data
providing only noise and being a deterministic
function of their parents. We picked all other
constants such that the na¨ıve correlation p(Y | A)
was a poor estimate of the counterfactual p(Y (a))
in the full-data setting.

D.1 Missing data data-generation

C ∼ Ber(0.4)

(29)

A(1) ∼ Ber(−0.3C + 0.4)

Y ∼ Ber(0.2C + 0.1A + 0.5)
Ti ∼ Ber(0.5 + uiA + viC)

RA ∼ Ber

0.7 + 0.2C − 0.4Y +

wiTi

 

i
X

!

D.2 Measurement error data-generation

C ∼ Ber(0.4)
A ∼ Ber(−0.3C + 0.4)
Y ∼ Ber(0.2C + 0.1A + 0.5)
Ti ∼ Ber(0.5 + siC + uiA + viY )

(a) Missing Data

(b) Measurement Error

Yelp.naive
Yelp.no text
Yelp.no y
Yelp.full
Synthetic.naive
Synthetic.no text
Synthetic.no y
Synthetic.full

Yelp.naive
Yelp.unadjusted
Yelp.adjusted
Synthetic.naive
Synthetic.unadjusted
Synthetic.adjusted

101

102

103

104

105

106

107

101

102

103

104

105

106

107

Dataset Size

Dataset Size

Figure 6: Experimental results with a vocabulary of size 53,197. Squared distance (y-axis, lower is better)
of the estimated causal effect from τSC calculated from the full data with no missing data or measurement
error. Error bars (negligible for larger datasets) are 1.96 times standard error across 10 experiments.

100

10−1

10−2

10−3

10−4

10−5

10−6

10−7

10−8

10−9

E Additional Experiments

Figure 6 shows the results of a second set
to those
of experiments, which are identical
described in §5 except the vocabulary size is now
53,197 instead of 4,334. For the Yelp data, the
larger vocabulary consists of all words which
ten times in a sample of 1M
appear at
reviews. As the larger vocabulary introduced
greater memory requirements, we did not run
these experiments with as large of datasets.

least

The results of these experiments show roughly
the same patterns as those seen in Figure 5.
The adjusted measurement error models again
appear erratic, generally performing worse than
the unadjusted models though better than the
naive models.

The full missing data model appeared to
slightly outperform the no y model on Yelp
data but only perform as well on the synthetic
data. Both these models appeared better than the
naive and no text models on both datasets.

Challenges of Using Text Classiﬁers for Causal Inference

Zach Wood-Doughty∗, Ilya Shpitser†, Mark Dredze∗†
Department of Computer Science
∗Center for Language and Speech Processing
†Malone Center for Engineering in Healthcare
Johns Hopkins University, Baltimore, MD 21218
{zach,ilyas,mdredze}@cs.jhu.edu

8
1
0
2
 
t
c
O
 
1
 
 
]
L
C
.
s
c
[
 
 
1
v
6
5
9
0
0
.
0
1
8
1
:
v
i
X
r
a

Abstract

causal

Causal understanding is essential for many
kinds of decision-making, but causal inference
from observational data has typically only
been applied to structured, low-dimensional
datasets. While text classiﬁers produce
low-dimensional outputs, their use in causal
inference has not previously been studied.
analyses based on
To facilitate
language data, we consider the role that text
classiﬁers can play in causal inference through
established modeling mechanisms from the
causality literature on missing data and
measurement error. We demonstrate how to
conduct causal analyses using text classiﬁers
on simulated and Yelp data, and discuss the
opportunities and challenges of future work
that uses text data in causal inference.

1 Introduction

scientiﬁc

analyses,

in domains

Most
from
economics to medicine, focus on low-dimensional
structured data.
Many such domains also
have unstructured text data; advances in natural
language processing (NLP) have led to an
increased interest in incorporating language data
into scientiﬁc analyses. While language is
inherently unstructured and high dimensional,
NLP systems can be used to process raw text
to produce structured variables.
For example,
work on identifying undiagnosed side effects
from electronic health records (EHR) uses text
classiﬁers to produce clinical variables from the
raw text (Hazlehurst et al., 2009).
NLP tools may also beneﬁt

the study of
causal inference, which seeks to identify causal
from observational data.
relations
Causal
analyses
low-dimensional
traditionally
structured variables, such as clinical markers and
binary health outcomes. Such analyses require
the data-generating process,
assumptions about

use

counterfactual

which are often simpler with low-dimensional
data. Unlike prediction tasks which are validated
by held-out test sets, causal inference involves
modeling
random variables
represent
(Neyman, 1923; Rubin, 1976)
the outcome of some hypothetical intervention.
To rigorously reason about hypotheticals, we
use causal models to link our counterfactuals to
observed data (Pearl, 2009).

that

NLP provides a natural way to incorporate text
data into causal inference models. We can produce
for example,
low-dimensional variables using,
text classiﬁers, and then run our causal analysis.
However, this straightforward integration belies
several potential issues. Text classiﬁcation is not
perfect, and errors in a NLP algorithm may bias
subsequent analyses. Causal inference requires
understanding how variables inﬂuence one another
and how correlations are confounded by common
causes. Classic methods such as stratiﬁcation
provide a means for handling confounding of
categorical or continuous variables, but it is not
immediately obvious how such work can be
extended to high-dimensional data.

But

2016).

confounders

Recent work has approached high-dimensional
domains via random forests (Wager and Athey,
2017) and other machine learning methods
(Chernozhukov et al.,
even
compared to an analysis that requires hundreds
(Belloni et al., 2014), NLP
of
models with millions of variables are very
physiological
high-dimensional.
symptoms reﬂect complex biological
realities,
many symptoms such as blood pressure are
one-dimensional variables. While doctors can
easily quantify the effect of high blood pressure
on some outcome, can we use the “positivity” of
a restaurant review to estimate a causal effect?
More broadly,
is it possible to employ text
classiﬁcation methods in a causal analysis?

While

We explore methods for the integration of
text classiﬁers into causal inference analyses that
consider confounds introduced by imperfect NLP.
We show what assumptions are necessary for
causal analyses using text, and discuss when
those assumptions may or may not be reasonable.
We draw on the causal
inference literature to
consider two modeling aspects: missing data
In the missing data
and measurement error.
formulation, a variable of interest is sometimes
unobserved, and text data gives us a means
to model
In the
the missingness process.
measurement error formulation, we use a text
classiﬁer
the
underlying variable.

to generate a noisy proxy of

We highlight practical considerations of a
causal analysis with text data by conducting
analyses with simulated and Yelp data. We
examine the results of both formulations and show
how a causal analysis which properly accounts for
possible sources of bias produces better estimates
than na¨ıve methods which make unjustiﬁed
assumptions. We conclude by examining how our
approach may enable new research avenues for
inferring causality with text data.

2 Causal Inference, Brieﬂy

While randomized control trials (RCT) are the
gold standard of determining causal effects of
treatments on outcomes, they can be expensive or
impossible in many settings. In contrast, the world
is ﬁlled with observational data collected without
randomization. While most studies simply report
correlations from observational data, the ﬁeld of
causal inference examines what assumptions and
analyses make it possible to identify causal effects.
We formalize a causal statement like “smoking
causes cancer” as “if we were to conduct a RCT
and assign smoking as a treatment, we would
see a higher incidence of cancer among those
assigned smoking than among the control group.”
In the framework of Pearl (1995), we consider
a counterfactual variable of interest: what would
have been the cancer incidence among smokers
if smoking had been randomized? Speciﬁcally,
we consider a causal effect as the counterfactual
outcome of a hypothetical intervention on some
treatment variable.
If we denote smoking as
treatment variable A and cancer as our
our
outcome variable Y , then we are interested in the
counterfactual distribution, denoted p(Y (a)) or

p(Y | do(a)). We interpret this as “the distribution
over Y had A been set, possibly contrary to fact,
to value a.” For a binary treatment A, the causal
effect of A on Y is denoted τ = E[Y (1)] −
E[Y (0)]; the average difference between if you
had received the treatment and if you had not.
Throughout, we use causal directed acyclic graphs
(DAG), which assumes that an intervention on
A is well-deﬁned and results in a counterfactual
variable Y (a) (Pearl, 1995; Dawid, 2010).

| A),

Figure 1a shows an example of

simple
This is the simplest DAG in
confounding.
which counterfactual distribution p(Y (a)) is not
as C inﬂuences both
simply p(Y
the treatment A and the outcome Y .
To
recover the counterfactual distribution p(Y (a))
that would follow an intervention upon A, we must
“adjust” for C, applying the so-called “back-door
criterion” (Pearl, 1995). We can then derive the
counterfactual distribution p(Y (a)) and desired
causal effect, τS as a function of the observed
data, (Fig. 4 Eq. 1.) This derivation is shown
in Appendix A.

it

in fact observed,

Note that p(Y (a)) and τS require data on
C, and if C is not
is
impossible to recover the causal effect. Formally,
identiﬁed in the
we say that p(Y (a)) is not
model, meaning there is no function f such that
p(Y (a))=f (p(A, Y )). Identiﬁability is a primary
concern of causal inference (Shpitser and Pearl,
2008).

Throughout, we assume for simplicity that A,
C, and Y are binary variables. While text
classiﬁers can convert high-dimensional data into
binary variables for such analyses, we need to
make further assumptions about how classiﬁcation
errors affect causal inferences. We cannot assume
that the output of a text classiﬁer can be treated as
if it were ground truth. To conceptualize the ways
in which a text classiﬁer may be biased, we will
consider them as a way to recover from missing
data or measurement error.

3 Causal Models

Real-world observational data is messy and often
imperfectly collected. Work in causal inference
has studied how analyses can be made robust
to missing data or data recorded with systematic
measurement errors.

C

A

RA

C

Y

A

A(1)

Y

C

A

A∗

Y

(a) Simple Confounding

(b) Missing Data

(c) Measurement Error

Figure 1: DAGs for causal inference without text data. Red variables are unobserved.
A is a treatment, Y is an outcome, and C is a confounder.

A C Y
0
1
1
1
1
0
1
0
0
1
0
1

RA A C Y
0
1
1
0
1
1
1
0

1
1
0
0

1
?
0
?

A∗ C Y
0
1
0
1
1
0
1
0
0
1
0
1

A∗ A
1
1
1
0
0
0
1
1

(a) Simple Confounding

(b) Missing Data

(c) Measurement Error

(d) Mismeasurement

Figure 2: Example data rows for causal inference without text data.

3.1 Missing Data

3.2 Measurement Error

Our dataset has “missing data” if it contains
individuals (instances) for which some variables
are unobserved, even though these variables are
typically available.
This may occur if some
survey respondents choose not to answer certain
questions, or if certain variables are difﬁcult to
collect and thus infrequently recorded. Missing
data is closely related to causal inference – both
are interested in hypothetical distributions that
we cannot directly observe (Robins et al., 2000;
Shpitser et al., 2015).

Consider a causal model where A is sometimes
missing (Figure 1b). The variable RA is a binary
indicator for whether A is observed (RA = 1)
or missing. The variable A(RA = 1), written
as A(1), represents the counterfactual value of A
were it never missing. Finally, A is the observed
proxy for A(1): it has the same value as A(1) if
RA = 1, and the special value “?” if RA = 0.

Solving missingness can seen as intervening to
set RA to 1. Given p(A, RA, C, Y ), we want to
recover p(A(1), C, Y ). We may need to make a
“Missing at Random” (MAR) assumption, which
says that the missingness process is independent of
the true missing values, conditional on observed
values. Figure 1b reﬂects the MAR assumption;
RA is independent of A(1) given fully-observed
C and Y .
If an edge existed from A(1) to RA,
we have “Missing Not at Random” (MNAR) and
would not be identiﬁed except in special cases
(Shpitser et al., 2015).

Sometimes a necessary variable is never observed,
but is instead proxied by a variable which differs
Consider the
from the truth by some error.
example of body mass index (BMI) as a proxy for
obesity in a clinical study. Obesity is a known
risk factor for many health outcomes, but has a
complex clinical deﬁnition and is nontrivial to
measure. BMI is a simple deterministic function
of height and weight. To conduct a causal analysis
of obesity on cancer when only BMI and cancer
are measured, we can proceed as if we had
measured obesity and then correct our analysis
for the known error that comes from using BMI
as a proxy for obesity (Hern´an and Cole, 2009;
Michels et al., 1998).

To generalize this concept, we can replace
obesity with our ground truth variable A
and replace BMI with a noisy proxy A∗.
Figure 1c gives
this model.
the DAG for
there is no
Unlike missing data problems,
hypothetical
the
true data distribution p(A, C, Y ).
Instead, we
manipulate the observed distribution p(A∗, C, Y )
with the known relationship p(A∗, A) to recover
the desired p(A, C, Y ).

intervention which recovers

Unlike missing data, measurement

error
conceptualization can be used even when we
never observe A (e.g.
the table in Figure 2c)
as long as we have knowledge about the error
mechanism p(A∗, A). Using this knowledge,
the error using ‘matrix
we can correct

for

C

A

Ti

Y

V

RA

C

A

A(1)

Ti

V

Y

C

A

A∗

Y

Ti

V

(a) Simple Confounding with Text

(b) Missing Data with Text

(c) Measurement Error with Text

Figure 3: DAGs for causal inference with text data. In the Yelp experiments we discuss, Ti inﬂuences Y
and not the other way around.

τS =

(p(Y = 1 | A = 1, C) − p(Y = 1 | A = 0, C)) p(C)

τMD =

(cid:18)

XC

′

p(A = 1 | T, C, Y = 1, RA = 1)
y p(A = 1 | T, C, y′, RA = 1)p(Y = y′ | C)
p(A = 0 | T, C, Y = 1, RA = 1)

y p(A = 0 | T, C, Y = y′, RA = 1)p(Y = y′ | C) (cid:19)

p(Y = 1, C)

XC

P
−

′

P

(1)

(2)

τME =

XC







y′

P

−δc,y=1qc,y=1(0) + (1 − δc,y=1)qc,y=1(1)
(1 − ǫc,y=1 − δc,y=1)

(1 − ǫc,y=1)qc,y=1(0) − ǫc,y=1qc,y=1(1)
(1 − ǫc,y=1 − δc,y=1)

−δc,y′qc,y′ (0) + (1 − δc,y′)qc,y′ (1)
(1 − ǫc,y′ − δc,y′ )

(1 − ǫc,y′ )qc,y′ (0) − ǫc,y′ qc,y′ (1)
(1 − ǫc,y′ − δc,y′)

−

y′

P

p(C)

(3)







Deﬁne ǫc,y = p(A = 0 | A∗ = 1, C = c, Y = y), δc, y = p(A = 1 | A∗ = 0, C = c, Y = y),
qc,y(0) = p(C = c, Y = y, A∗ = 0), and qc,y(1) = p(C = c, Y = y, A∗ = 1).

Figure 4: Functionals for the Causal Effects for Simple Confounding (τSC), Missing Data (τMD) and
Measurement Error (τME). Derivations are in Appendices A, B, and C.

adjustment’ (Pearl, 2010).
In practice we might
learn p(A∗, A) from data such as that found in
Figure 2d. Recent work has also considered
how multiple independent proxies of A could
allow identiﬁcation without any data on p(A∗, A)
(Kuroki and Pearl, 2014).

4 Causal Models for Text Data

We can use conceptualizations for missing data
and measurement error to support causal analyses
with text data. The choice of model depends on the
assumptions we make about the data-generation
process.

to represent

text, which produces

We add new variables to our models (Figure
1a)
the
data-generating distribution shown in Figure 3a.
This model assumes that the underlying A, C, and
Y variables are generated before the text variables;
we use text
the true relationship
between A and Y .
We represent

text as an arbitrary set of
V variables, which are independent of one

to recover

another given the non-text variables.
In our
implemented analyses we will represent text as
a bag-of-words, wherein each Ti is simply the
binary indicator of the presence of the i-th word
in our vocabulary of V words, and T = ∪iTi.
The restriction to simple text models allows
us to explore connections to causal
inference
applications,
though future work could relax
assumptions of the text models to be inclusive
of more sophisticated text models (e.g. neural
sequence models (Lai et al., 2015; Zhang et al.,
2015)), or consider causal relationships between
two text variables.

To motivate our explanations, consider the task
of predicting an individuals’ smoking status from
free-text hospital discharge notes (Uzuner et al.,
2008; Wicentowski and Sydes, 2008).
Some
hospitals do not explicitly record patient smoking
status as structured data, making it difﬁcult to use
such data in a study on the outcomes of smoking.
We will suppose that we are given a dataset with
patient data on lung cancer outcome (Y ) and age

(C), that our data on smoking status (A) is affected
by either missing data or measurement error, but
that we have text data (T) from discharge records
that will allow us to infer smoking status with
reasonable accuracy.

4.1 Missing Data

To show how we might use text data to recover
from missing data, we introduce missingness for
A from Figure 3a to get the model in Figure 3b.
The missing arrow from A(1) to RA encodes the
MAR assumption, which is sufﬁcient to make it
possible to identify the full data distribution from
the observed data.

Suppose our motivation is to estimate the causal
effect of smoking status (A) on lung cancer (Y )
adjusting for age (C).
Imagine that missing
data arises because hospitals sometimes – but not
always – delete explicit data on smoking status
from patient records. If we have access to patients’
discharge notes (T) and know whether a given
patient had smoking status recorded (RA), then the
DAG in Figure 3b may be a reasonable model for
our setting. Note that we must again assume that
A does not directly affect RA.

The causal effect of A on Y in Figure 3b is
identiﬁed as τM D, given in Eq. 2 in Figure 4. The
derivation is given in Appendix B.

4.2 Measurement Error

We model text data with measurement error by
introducing a proxy A∗ to the model in Figure
3c. We assume that the proxied value of A∗ can
depend upon all other variables, and that we will
be able to estimate p(A∗, A) given an external
dataset, e.g.
text classiﬁer accuracy on held-out
data.

Suppose we again want to estimate the causal
effect from §4.1, but this time none of our hospital
records contain explicit data on smoking status.
However, imagine that we have a separate training
dataset of medical discharge records annotated
by expert pulmonologists for patients’ smoking
status. We could then train a classiﬁer to predict
smoking status using discharge record text1.

Working from the derivation for matrix
in binary models given by Pearl
adjustment
(2010), we identify the causal effect of A on
Y (Figure 3c) as τME (Eq 3 in Figure 4.) The
derivation is in Appendix C.

1This is the precise setting of Uzuner et al. (2008).

5 Experiments

We now empirically evaluate the effectiveness
of our two conceptualizations (missing data and
measurement error) for including text data in
causal analyses. We induce missingness or
mismeasurement of the treatment variable and use
text data to recover the true causal relationship
treatment on the outcome. We begin
of that
with a simulation study with synthetic text data,
and then conduct an analysis using reviews from
yelp.com.

5.1 Synthetic Data

A graphical model

We select synthetic data so that we can control
the entire data-generation process.
For each
data row, we ﬁrst sample data on three binary
variables (A, C, Y ) and then sample V different
binary variables Ti representing a V -vocabulary
bag-of-words.
this
distribution appears in Figure 3a. We augment
this distribution to introduce either missing
data (Figure 3b) or measurement error (Figure
For measurement error, we sample two
3c.)
datasets. A small training set which gives data on
p(A, C, Y, T) and a large test set which gives data
on p(C, Y, T).

for

The full data generating process appears in
Appendix D, and the implementation (along with
all our code) is provided online2.

5.2 Yelp Data
We utilize the 2015 Yelp Dataset Challenge3
which provides 4.7M reviews of local businesses.
Each review contains a one- to ﬁve-star rating, up
to 5,000 characters of text. Yelp users can ﬂag
reviews as “Useful” as a mark of quality.

We extract treatment, outcome, and confounder
variables from the structured data. The treatment
is a binarized user rating that takes value 1 if
the review has four or ﬁve stars and value 0
if the review has one or two stars. Three-star
reviews are discarded from our analysis. The
outcome is whether the review received at least
one “Useful” ﬂag. The confounder is whether
two
the review’s author has received at
“Useful” ﬂags across all reviews, according to
their user object.
In our data, 74.2% of reviews
were positive, 42.6% of reviews were ﬂagged as
“Useful,” and 56.7% users had received at least

least

2github.com/zachwooddoughty/emnlp2018-causal
3yelp.com/dataset/challenge

two such ﬂags. We preprocess the text of each
review by lowercasing, stemming, and removing
stopwords, before converting to a bag-of-words
representation with the 4,334 word vocabulary of
all words which appeared at least 1000 times in a
sample of 1M reviews.

Based on this p(A, C, Y, T) distribution,
we assume the data-generating process
that
matches Figure 3a and introduce missingness
and mismeasurement
giving us
data-generating processes matching Figures 3b
and 3c.

as before,

Our intention is not

to argue about a true
real-world causal effect of Yelp reviews on peer
behavior: we do not believe that our confounder
is the only common cause of the author’s rating
and the platform’s response. We leave for future
work a case study that jointly addresses questions
of identiﬁability and estimation of a real-world
In this work, our experiments
causal effect.
focus on a simpler task: can a correctly-speciﬁed
model that uses text data effectively estimate a
causal effect in the presence of missing data or
measurement error.

5.3 Models

We now introduce several baseline methods
which, unlike our correctly speciﬁed models τM D
and τM E, are not consistent estimators of our
desired causal effect. We would expect that the
theoretical bias in these estimators would result in
poor performance in our experiments.

5.3.1 Baseline: Na¨ıve Model

In both the missing data and measurement error
settings, our models use some rows that are full
observed. In missing data, these are rows where
RA = 1; in measurement error, the training set is
sampled from the true distribution. The simplest
approach to handling imperfect data is to throw
away all rows without full data, and calculate Eq
1 from that data. In Figure 5, these are labeled as
*.naive.

5.3.2 Baseline: Textless Model

In Figure 3b, if we do not condition on Ti to
d-separate A(1) from its missingness indicator,
that inﬂuence may bias our estimate. While we
know that ignoring text may introduce asymptotic
bias into our estimates of the causal effect, we
empirically evaluate how much bias is produced
by this “Textless” model compared to a correct

model. This is labeled as *.no text in Figure
5 (a).

In principle, we could conduct a measurement
error analysis using a model that does not include
text. In practice, we found we could not impute A∗
from C and Y alone. The non-textual classiﬁer
had such high error that the adjustment matrix
was singular and we could not compute the
effect. Thus, we have no such baseline in our
measurement error results.

5.3.3 Baseline: no y and unadjusted

Models

In Figure 3b, we must also condition on C and Y
to d-separate A(1) from its missingness indicator.
In our misspeciﬁed model for missing data, we do
not condition on Y , leaving open a path for A(1)
to inﬂuence its missingness. In Figure 5 (a), this
model is labeled as *.no y.

When correcting for measurement error, a
crucial piece of the estimation is the matrix
adjustment using the known error between
the proxy and the truth.
A straightforward
misspeciﬁed model for measurement error is to
impute a proxy for each row in our dataset and
then calculate the causal effect assuming no error
between the proxy and truth. This approach,
while simplistic, can be thought of as using a text
classiﬁer as a proxy without regard for the text
classiﬁer’s biases. In Figure 5 (b), this approach
is labeled as *.unadjusted.

5.3.4 Correct Models

Finally, we consider the estimation approaches
presented in §4.1 and §4.2. For the missing data
causal effect (τMD from Eq 2) we use a multiple
imputation estimator which calculates the average
effect across 20 samples from p(A|T, C, Y ) for
each row where RA = 0. For the measurement
error causal effect (τME from Eq 3), we use the
training set of p(A, C, Y, T) data to estimate ǫc,y
and δc, y and the larger set of p(C, Y, T) data to
estimate qc,y and p(C).

These models are displayed in Figure 5 (a) as

*.full and in Figure 5 (b) *.adjusted.

5.4 Evaluation

takes

Each model
in a data sample with
missingness or mismeasurement, and outputs an
estimate of the causal effect of A on Y in the
underlying data. Rather than comparing models’
estimates against a population-level estimate,

we compare against an estimate of the effect
computed on the same data sample, but without
any missing data or measurement error. This
‘perfect data estimator’ may still make errors
given the ﬁnite data sample. We compare against
this estimator to avoid a small-sample case where
an estimator gets lucky. In Figure 5, we plot data
sample size against the squared distance of each
model’s estimate from a perfect data estimator’s
estimate, averaged over ten runs. Figure 6 in
Appendix E contains a second set of experiments
using a larger vocabulary.

6 Results

Given that our correctly-speciﬁed models are
proven to be asymptotically consistent, we would
expect them to outperform misspeciﬁed models.
However,
asymptotic
for any given dataset,
consistency provides no guarantees.

6.1 Missing Data

The missing data (MD) experiments suggest that
the correct full model does perform best. The
no y model performs approximately as well as the
correct model on the synthetic data, but not on the
Yelp data. The difference between the no y and
full missing data models is simply a function
of the effect of Y on RA. We could tweak our
synthetic data distribution to increase the inﬂuence
of Y to make the no y model perform worse.
initially

other
data-generating distributions for missing data,
we found that when we reduced the inﬂuence
of the text variables on RA, the no text and
naive models approached the performance of
the correctly-speciﬁed model. While intuitive,
this reinforces that
the underlying distribution
matters a great deal in how modeling choices may
introduce biases if incorrectly speciﬁed.

considered

When

we

6.2 Measurement error

results

The measurement error
tell a more
interesting story. We see enormous ﬂuctuations
of the adjusted model, and in the synthetic
data, the unadjusted model appears to be quite
superior.

In the synthetic dataset, this is likely because
our text classiﬁer had near-perfect accuracy, and
so simple approach of assuming its predictions
were ground-truth introduced less bias. A broader
issue with the adjusted model
the

is that

matrix adjustment approach requires dividing
this
by (potentially very small) probabilities,
sometimes resulted in huge over-corrections.
In
addition, since those probabilities are estimated
training dataset, small
from a relatively small
changes to the error-estimate can propagate to
huge changes in the ﬁnal casual estimate.

This

instability of

the matrix adjustment
approach may be a bigger problem for text and
other high-dimensional data: unlike in our earlier
example of BMI and obesity,
there are likely
no simple relationships between text and clinical
variables. However,
instead of using matrix
adjustment as a way to recover the true effect, we
may instead use it to bound the error our proxy
may introduce. As mentioned by Pearl (2010),
when p(A | A∗) is not known exactly, we can
use a Bayesian analysis to bound estimates of a
causal effect. In a downstream task, this would let
us explore the stability of our adjusted results.

7 Related Work

(2017)

explored

considered the
A few recent papers have
combining text data with
for
possibilities
from the
approaches
inference
causal
literature.
Landeiro and Culotta (2016) and
Landeiro and Culotta
text
classiﬁcation when the relationship between text
data and class labels are confounded. Other work
has used propensity scores as a way to extract
features from text data (Paul, 2017) or to match
social media users based on what words they
write (De Choudhury et al., 2016). The only work
we know of which seeks to estimate causal effects
using text data focuses on effects of text or effects
on text (Egami et al., 2018; Roberts et al., 2018).
In our work, our causal effects do not include
text variables: we use text variables to recover an
underlying distribution and then estimate a causal
effect within that distribution.

There is a conceptually related line of work
in the NLP community on inferring causal
relationships expressed in text
(Girju, 2003;
Kaplan and Berry-Rogghe, 1991). However, our
work is fundamentally different. Rather than
identify casual relations expressed via language,
we are using text data in a causal model to identify
the strength of an underlying causal effect.

(a) Missing Data

(b) Measurement Error

100

10−1

10−2

10−3

10−4

10−5

10−6

10−7

10−8

10−9

Yelp.naive
Yelp.no text
Yelp.no y
Yelp.full
Synthetic.naive
Synthetic.no text
Synthetic.no y
Synthetic.full

Yelp.naive
Yelp.unadjusted
Yelp.adjusted
Synthetic.naive
Synthetic.unadjusted
Synthetic.adjusted

102

103

104

105

106

107

102

103

104

105

106

107

Dataset Size

Dataset Size

Figure 5: Experimental results. Squared distance (y-axis, lower is better) of the estimated causal effect
from τSC calculated from the full data with no missing data or measurement error. Error bars (negligible
for larger datasets) are 1.96 times standard error across 10 experiments. Additional experiments with a
larger vocabulary are shown in Appendix E.

8 Future Directions

issues
While this paper addresses some initial
arising from using text classiﬁers in causal
analyses, many challenges remain. We highlight
some of these issues as directions for future
research.

We provided several proof-of-concept models
for estimating effects, but our approach is
ﬂexible to more sophisticated models.
For
a semi-parametric estimator would
example,
make no assumptions
text data
about
distribution by wrapping the text classiﬁer into
an inﬁnite-dimensional nuisance model (Tsiatis,
2007). This would enable estimators robust to
partial model misspeciﬁcation (Bang and Robins,
2005).

the

Choices in the design of statistical models of
text consider issues like accuracy and tractability.
Yet if these models are to be used in a causal
framework, we need to understand how modeling
assumptions introduce biases and other issues that
can interfere with a downstream causal analysis.
To take an example from the medical domain, we

know that doctors write clinical notes throughout
the healthcare process, but it is not obvious how
to model this data-generating process. We could
assume that the doctor’s notes passively record
a patient’s progression, but in reality it may be
that the content of the notes themselves actively
change the patient’s care; causality could work in
either direction.

New lines of work in causality may be
especially helpful for NLP. In this work, we used
simple logistic regression on a bag-of-words
text;
representation of
using state-of-the-art
text models will
likely require more causal
assumptions. Nabi and Shpitser (2017) develops
causality-preserving
reduction,
which could help develop text representations that
preserve causality.

dimensionality

Finally, we are interested in case studies on
incorporating text classiﬁers into real-world causal
analyses. Many health studies have used text
classiﬁers to extract clinical variables from EHR
data (Meystre et al., 2008). These works could
be extended to study causal effects involving

those extracted variables, but such extensions
would require an understanding of the underlying
assumptions.
In any given study, the necessity
and appropriateness of assumptions will hinge
The conceptualizations
on domain expertise.
outlined in this paper, while far from solving all
issues of causality and text, will help those using
text classiﬁers to more easily consider research
questions of cause and effect.

Acknowledgments

This work was in part supported by the National
Institute of General Medical Sciences under grant
number 5R01GM114771 and by the National
Institute of Allergy and Infectious Diseases under
grant number R01 AI127271-01A1. We thank the
anonymous reviewers for their helpful comments.

References

Heejung Bang and James M Robins. 2005. Doubly
robust estimation in missing data and causal
inference models. Biometrics, 61(4):962–973.

Alexandre Belloni, Victor Chernozhukov,

and
Christian Hansen. 2014.
Inference on treatment
effects after selection among high-dimensional
The Review of Economic Studies,
controls.
81(2):608–650.

Victor Chernozhukov, Denis Chetverikov, Mert
Demirer, Esther Duﬂo, Christian Hansen, and
Double machine
Whitney K Newey. 2016.
learning for
treatment and causal parameters.
Technical report, cemmap working paper, Centre
for Microdata Methods and Practice.

A Philip Dawid. 2010. Beware of the dag!

In
Causality: Objectives and Assessment, pages 59–86.

Munmun De Choudhury, Emre Kiciman, Mark Dredze,
and Mrinal Kumar. 2016.
Glen Coppersmith,
Discovering shifts to suicidal ideation from mental
In Proceedings
health content in social media.
of the 2016 CHI conference on human factors in
computing systems, pages 2098–2110. ACM.

Naoki Egami, Christian J Fong, Justin Grimmer,
Margaret E Roberts, and Brandon M Stewart. 2018.
How to make causal inferences using texts. arXiv
preprint arXiv:1802.02163.

Roxana Girju. 2003. Automatic detection of causal
In Proceedings
relations for question answering.
the ACL 2003 workshop on Multilingual
of
summarization and question answering-Volume
12, pages 76–83. Association for Computational
Linguistics.

Brian Hazlehurst, Allison Naleway, and John Mullooly.
2009. Detecting possible vaccine adverse events
in clinical notes of the electronic medical record.
Vaccine, 27(14):2077–2083.

Miguel A Hern´an and Stephen R Cole. 2009. Invited
commentary: causal diagrams and measurement
epidemiology,
bias.
170(8):959–962.

American journal of

Randy M Kaplan and Genevieve Berry-Rogghe. 1991.
Knowledge-based acquisition of causal relationships
in text. Knowledge Acquisition, 3(3):317–337.

Manabu Kuroki and Judea Pearl. 2014. Measurement
inference.

bias and effect restoration in causal
Biometrika, 101(2):423–437.

Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao.
2015. Recurrent convolutional neural networks for
In AAAI, volume 333, pages
text classiﬁcation.
2267–2273.

Virgile Landeiro and Aron Culotta. 2016. Robust text
classiﬁcation in the presence of confounding bias. In
Thirtieth AAAI Conference on Artiﬁcial Intelligence.

Virgile Landeiro and Aron Culotta. 2017. Controlling
classiﬁcation
confounds
arXiv preprint

for
using correlational constraints.
arXiv:1703.01671.

unobserved

in

and

St´ephane M Meystre, Guergana K Savova, Karin C
Kipper-Schuler,
2008.
Extracting information from textual documents
in the electronic health record:
a review of
recent research. Yearbook of medical informatics,
17(01):128–144.

John F Hurdle.

Karin B Michels, Sander Greenland, and Bernard A
Rosner. 1998. Does body mass index adequately
capture the relation of body composition and body
American Journal of
size to health outcomes?
Epidemiology, 147(2):167–172.

Razieh Nabi and Ilya Shpitser. 2017. Semi-parametric
reduction
of
arXiv preprint

treatments.

dimension

causal
sufﬁcient
high dimensional
arXiv:1710.06727.

Jerzy Neyman. 1923. Sur les applications de la thar
des probabilities aux experiences agaricales: Essay
des principle. excerpts reprinted (1990) in English.
Statistical Science, 5:463–472.

Michael J Paul. 2017. Feature selection as causal
inference: Experiments with text classiﬁcation.
the 21st Conference on
In Proceedings of
Learning
Computational Natural
(CoNLL 2017), pages 163–172.

Language

Judea Pearl. 1995. Causal diagrams for empirical

research. Biometrika, 82(4):669–688.

Judea Pearl. 2009. Causality. Cambridge university

press.

Judea Pearl. 2010. On measurement bias in causal
the Twenty-Sixth
inference.
Conference on Uncertainty in Artiﬁcial Intelligence,
pages 425–432. AUAI Press.

In Proceedings of

Margaret E Roberts, Brandon M Stewart,

and
Adjusting for

Richard A Nielsen. 2018.
confounding with text matching.

James M Robins, Andrea Rotnitzky, and Daniel O
Scharfstein. 2000. Sensitivity analysis for selection
bias and unmeasured confounding in missing data
and causal inference models. In Statistical models
in epidemiology, the environment, and clinical trials,
pages 1–94. Springer.

D. B. Rubin. 1976. Causal inference and missing data

(with discussion). Biometrika, 63:581–592.

Ilya Shpitser, Karthika Mohan, and Judea Pearl.
2015. Missing data as a causal and probabilistic
the Thirty-First
problem.
Conference on Uncertainty in Artiﬁcial Intelligence,
pages 802–811. AUAI Press.

In Proceedings of

Ilya Shpitser and Judea Pearl. 2008.

Complete
identiﬁcation methods for the causal hierarchy.
Journal
Research,
9(Sep):1941–1979.

of Machine

Learning

Anastasios Tsiatis. 2007. Semiparametric theory and
missing data. Springer Science & Business Media.

¨Ozlem Uzuner, Ira Goldstein, Yuan Luo, and Isaac
Identifying patient smoking status
Journal of
Informatics Association,

Kohane. 2008.
from medical discharge records.
the American Medical
15(1):14–24.

Stefan Wager and Susan Athey. 2017. Estimation and
inference of heterogeneous treatment effects using
random forests. Journal of the American Statistical
Association.

Richard Wicentowski and Matthew R Sydes. 2008.
Using implicit
information to identify smoking
status in smoke-blind medical discharge summaries.
Journal of
Informatics
the American Medical
Association, 15(1):29–31.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
text
In Advances in neural information

Character-level convolutional networks for
classiﬁcation.
processing systems, pages 649–657.

A Simple Confounding

p(Y (a)) =

p(Y (a) | C)p(C)

(4)

=

=

C
X

C
X

C
X

p(Y (a) | A, C)p(C)

(5)

p(Y | A, C)p(C)

(6)

Eq 5 holds because Y (a) ⊥ A | C, as seen in
Figure 1a. Plugging this distribution into τS =
E[Y (1)] − E[Y (0)] gives us the causal effect
presented in Figure 4, Eq 1.

This assumes that an intervention on A is
if we did conduct a randomized
well-deﬁned;
control trial, we could assign A = a and break
A’s dependence on C.

In general, this step requires that we condition
on all “back-door” paths between the treatment
and the outcome. In Figure 1(a), if we did not have
data on C, we could not block the back-door path
between A and Y .

Eq 6 holds due to consistency. We assume that,
given we intervened to set A = a, if that individual
would have been assigned A = a in nature, then
the distribution over Y is the same.

B Missing Data

Denote p(Y (A(1) = a)) = p(Y (a)).

First, we identify the causal effect in terms of

the true A(1).

p(Y (a))

=

=

=

C
X

C
X

C
X

p(Y (a) | A(1), C)p(C)

p(Y | A(1), C)p(C)

(7)

(8)

(9)

Where 7 holds by chain rule, 8 holds by A(1) ⊥

Y (a) | C, and 9 by consistency.

Now, we identify A(1) in terms of observed

data.

p(A(1), C, Y )

= p(A(1) | C, Y )p(C, Y )
= p(A(1) | C, Y, RA = 1)p(C, Y )
= p(A | C, Y, RA = 1)p(C, Y )

(10)

(11)

(12)

Where 10 holds by chain rule, 11 by A(1) ⊥

RA | C, Y , and 12 by consistency.

Now, use Eq 12 to identify p(Y | A(1), C) from

Eq 9 in terms of observed data.

p(Y | A(1), C)

=

=

=

P

(14)

(13)

p(Y, A(1), C)
p(A(1), C)
p(Y, A(1), C)
Y p(Y, A(1), C)
p(A | C, Y, RA = 1)p(C, Y )
Y p(A | C, Y, RA = 1)p(C, Y )
p(A | C, Y, RA = 1)p(Y | C)
Y p(A | C, Y, RA = 1)p(Y | C)
Where 13 holds by deﬁnition, 14 holds by
marginalization, 15 holds by an application of 12
twice, and 16 holds by canceling out p(C).

(15)

(16)

P

P

=

If we include text in this derivation, we simply
replace p(A | C, Y, RA = 1) with p(A |
T, C, Y, RA = 1), where T is all our text
variables.

Finally, combine Eq 9 and Eq 16 to get:

p(Y (A(1) = a))

=

XC

P

p(A | C, Y, RA = 1)p(Y | C)
Y p(A | C, Y, RA = 1)p(Y | C)

p(C)

(17)

Plugging this distribution into τMD

=
E[Y (1)] − E[Y (0)] gives us the causal effect
presented in Figure 4, Eq 2.

C Measurement Error

ǫc,y = p(A = 0 | A
δc, y = p(A = 1 | A

= 1, C = c, Y = y)
= 0, C = c, Y = y)

∗

∗

qc,y(0) = p(C = c, Y = y, A
qc,y(1) = p(C = c, Y = y, A

= 0)

= 1)

∗

∗

(18)

(19)

(20)

(21)

Eq (5) and (7) from Pearl 2010 gives us:

p(A = 1, C = c, Y = y)

=

−δc,yqc,y(0) + (1 − δc,y)qc,y(1)
(1 − ǫc,y − δc,y)

p(A = 0, C = c, Y = y)

=

(1 − ǫc,y)qc,y(0) − ǫc,yqc,y(1)
(1 − ǫc,y − δc,y)

(22)

(23)

p(Y (a) | C)p(C)

Deﬁne the following terms for convenience:

Now,

D Synthetic Data Distribution

p(Y | A = 1, C)

p(Y, A = 1, C)
p(A = 1, C)
p(Y, A = 1, C)
Y p(Y, A = 1, C)
−δc,yqc,y(0) + (1 − δc,y)qc,y(1)
(1 − ǫc,y − δc,y)

P

=

=

=

−δc,y′qc,y′(0) + (1 − δc,y′)qc,y′(1)
(1 − ǫc,y′ − δc,y′)

y′

P

and then,

p(Y | A = 0, C)

p(Y, A = 0, C)
p(A = 0, C)
p(Y, A = 0, C)
Y p(Y, A = 0, C)
(1 − ǫc,y)qc,y(0) − ǫc,yqc,y(1)
(1 − ǫc,y − δc,y)

P

=

=

=

(1 − ǫc,y′)qc,y′(0) − ǫc,y′qc,y′(1)
(1 − ǫc,y′ − δc,y′)

y′

P

Plugging this distribution

=
E[Y (1)] − E[Y (0)] gives us the causal effect
presented in Figure 4, Eq 3.

into τME

(24)

(25)

(26)

(27)

(28)

In the distributions below, Ber(p) is used as
the abbreviation a Bernoulli distribution with
probability p.

Below, si, ui and vi are the effect of C, A,
and Y on the probability of word Ti; each is
drawn from N (0, ζ), a parameter which controls
how correlated words are with the underlying
variables. When ζ is close to 0,
the words
are essentially random. When ζ is large,
the
words are essentially deterministic functions of
the underlying variables. Similarly wi is the effect
of word Ti on RA, and is drawn from N (0, η).

For both settings, we set vocabulary size to
4,334 (to match Yelp experiments) and ζ = 0.5.
For the missing data setting, we set η = 0.1. We
picked these constants by empirically ﬁnding a
reasonable middle ground between the text data
providing only noise and being a deterministic
function of their parents. We picked all other
constants such that the na¨ıve correlation p(Y | A)
was a poor estimate of the counterfactual p(Y (a))
in the full-data setting.

D.1 Missing data data-generation

C ∼ Ber(0.4)

(29)

A(1) ∼ Ber(−0.3C + 0.4)

Y ∼ Ber(0.2C + 0.1A + 0.5)
Ti ∼ Ber(0.5 + uiA + viC)

RA ∼ Ber

0.7 + 0.2C − 0.4Y +

wiTi

 

i
X

!

D.2 Measurement error data-generation

C ∼ Ber(0.4)
A ∼ Ber(−0.3C + 0.4)
Y ∼ Ber(0.2C + 0.1A + 0.5)
Ti ∼ Ber(0.5 + siC + uiA + viY )

(a) Missing Data

(b) Measurement Error

Yelp.naive
Yelp.no text
Yelp.no y
Yelp.full
Synthetic.naive
Synthetic.no text
Synthetic.no y
Synthetic.full

Yelp.naive
Yelp.unadjusted
Yelp.adjusted
Synthetic.naive
Synthetic.unadjusted
Synthetic.adjusted

101

102

103

104

105

106

107

101

102

103

104

105

106

107

Dataset Size

Dataset Size

Figure 6: Experimental results with a vocabulary of size 53,197. Squared distance (y-axis, lower is better)
of the estimated causal effect from τSC calculated from the full data with no missing data or measurement
error. Error bars (negligible for larger datasets) are 1.96 times standard error across 10 experiments.

100

10−1

10−2

10−3

10−4

10−5

10−6

10−7

10−8

10−9

E Additional Experiments

Figure 6 shows the results of a second set
to those
of experiments, which are identical
described in §5 except the vocabulary size is now
53,197 instead of 4,334. For the Yelp data, the
larger vocabulary consists of all words which
ten times in a sample of 1M
appear at
reviews. As the larger vocabulary introduced
greater memory requirements, we did not run
these experiments with as large of datasets.

least

The results of these experiments show roughly
the same patterns as those seen in Figure 5.
The adjusted measurement error models again
appear erratic, generally performing worse than
the unadjusted models though better than the
naive models.

The full missing data model appeared to
slightly outperform the no y model on Yelp
data but only perform as well on the synthetic
data. Both these models appeared better than the
naive and no text models on both datasets.

Challenges of Using Text Classiﬁers for Causal Inference

Zach Wood-Doughty∗, Ilya Shpitser†, Mark Dredze∗†
Department of Computer Science
∗Center for Language and Speech Processing
†Malone Center for Engineering in Healthcare
Johns Hopkins University, Baltimore, MD 21218
{zach,ilyas,mdredze}@cs.jhu.edu

8
1
0
2
 
t
c
O
 
1
 
 
]
L
C
.
s
c
[
 
 
1
v
6
5
9
0
0
.
0
1
8
1
:
v
i
X
r
a

Abstract

causal

Causal understanding is essential for many
kinds of decision-making, but causal inference
from observational data has typically only
been applied to structured, low-dimensional
datasets. While text classiﬁers produce
low-dimensional outputs, their use in causal
inference has not previously been studied.
analyses based on
To facilitate
language data, we consider the role that text
classiﬁers can play in causal inference through
established modeling mechanisms from the
causality literature on missing data and
measurement error. We demonstrate how to
conduct causal analyses using text classiﬁers
on simulated and Yelp data, and discuss the
opportunities and challenges of future work
that uses text data in causal inference.

1 Introduction

scientiﬁc

analyses,

in domains

Most
from
economics to medicine, focus on low-dimensional
structured data.
Many such domains also
have unstructured text data; advances in natural
language processing (NLP) have led to an
increased interest in incorporating language data
into scientiﬁc analyses. While language is
inherently unstructured and high dimensional,
NLP systems can be used to process raw text
to produce structured variables.
For example,
work on identifying undiagnosed side effects
from electronic health records (EHR) uses text
classiﬁers to produce clinical variables from the
raw text (Hazlehurst et al., 2009).
NLP tools may also beneﬁt

the study of
causal inference, which seeks to identify causal
from observational data.
relations
Causal
analyses
low-dimensional
traditionally
structured variables, such as clinical markers and
binary health outcomes. Such analyses require
the data-generating process,
assumptions about

use

counterfactual

which are often simpler with low-dimensional
data. Unlike prediction tasks which are validated
by held-out test sets, causal inference involves
modeling
random variables
represent
(Neyman, 1923; Rubin, 1976)
the outcome of some hypothetical intervention.
To rigorously reason about hypotheticals, we
use causal models to link our counterfactuals to
observed data (Pearl, 2009).

that

NLP provides a natural way to incorporate text
data into causal inference models. We can produce
for example,
low-dimensional variables using,
text classiﬁers, and then run our causal analysis.
However, this straightforward integration belies
several potential issues. Text classiﬁcation is not
perfect, and errors in a NLP algorithm may bias
subsequent analyses. Causal inference requires
understanding how variables inﬂuence one another
and how correlations are confounded by common
causes. Classic methods such as stratiﬁcation
provide a means for handling confounding of
categorical or continuous variables, but it is not
immediately obvious how such work can be
extended to high-dimensional data.

But

2016).

confounders

Recent work has approached high-dimensional
domains via random forests (Wager and Athey,
2017) and other machine learning methods
(Chernozhukov et al.,
even
compared to an analysis that requires hundreds
(Belloni et al., 2014), NLP
of
models with millions of variables are very
physiological
high-dimensional.
symptoms reﬂect complex biological
realities,
many symptoms such as blood pressure are
one-dimensional variables. While doctors can
easily quantify the effect of high blood pressure
on some outcome, can we use the “positivity” of
a restaurant review to estimate a causal effect?
More broadly,
is it possible to employ text
classiﬁcation methods in a causal analysis?

While

We explore methods for the integration of
text classiﬁers into causal inference analyses that
consider confounds introduced by imperfect NLP.
We show what assumptions are necessary for
causal analyses using text, and discuss when
those assumptions may or may not be reasonable.
We draw on the causal
inference literature to
consider two modeling aspects: missing data
In the missing data
and measurement error.
formulation, a variable of interest is sometimes
unobserved, and text data gives us a means
to model
In the
the missingness process.
measurement error formulation, we use a text
classiﬁer
the
underlying variable.

to generate a noisy proxy of

We highlight practical considerations of a
causal analysis with text data by conducting
analyses with simulated and Yelp data. We
examine the results of both formulations and show
how a causal analysis which properly accounts for
possible sources of bias produces better estimates
than na¨ıve methods which make unjustiﬁed
assumptions. We conclude by examining how our
approach may enable new research avenues for
inferring causality with text data.

2 Causal Inference, Brieﬂy

While randomized control trials (RCT) are the
gold standard of determining causal effects of
treatments on outcomes, they can be expensive or
impossible in many settings. In contrast, the world
is ﬁlled with observational data collected without
randomization. While most studies simply report
correlations from observational data, the ﬁeld of
causal inference examines what assumptions and
analyses make it possible to identify causal effects.
We formalize a causal statement like “smoking
causes cancer” as “if we were to conduct a RCT
and assign smoking as a treatment, we would
see a higher incidence of cancer among those
assigned smoking than among the control group.”
In the framework of Pearl (1995), we consider
a counterfactual variable of interest: what would
have been the cancer incidence among smokers
if smoking had been randomized? Speciﬁcally,
we consider a causal effect as the counterfactual
outcome of a hypothetical intervention on some
treatment variable.
If we denote smoking as
treatment variable A and cancer as our
our
outcome variable Y , then we are interested in the
counterfactual distribution, denoted p(Y (a)) or

p(Y | do(a)). We interpret this as “the distribution
over Y had A been set, possibly contrary to fact,
to value a.” For a binary treatment A, the causal
effect of A on Y is denoted τ = E[Y (1)] −
E[Y (0)]; the average difference between if you
had received the treatment and if you had not.
Throughout, we use causal directed acyclic graphs
(DAG), which assumes that an intervention on
A is well-deﬁned and results in a counterfactual
variable Y (a) (Pearl, 1995; Dawid, 2010).

| A),

Figure 1a shows an example of

simple
This is the simplest DAG in
confounding.
which counterfactual distribution p(Y (a)) is not
as C inﬂuences both
simply p(Y
the treatment A and the outcome Y .
To
recover the counterfactual distribution p(Y (a))
that would follow an intervention upon A, we must
“adjust” for C, applying the so-called “back-door
criterion” (Pearl, 1995). We can then derive the
counterfactual distribution p(Y (a)) and desired
causal effect, τS as a function of the observed
data, (Fig. 4 Eq. 1.) This derivation is shown
in Appendix A.

it

in fact observed,

Note that p(Y (a)) and τS require data on
C, and if C is not
is
impossible to recover the causal effect. Formally,
identiﬁed in the
we say that p(Y (a)) is not
model, meaning there is no function f such that
p(Y (a))=f (p(A, Y )). Identiﬁability is a primary
concern of causal inference (Shpitser and Pearl,
2008).

Throughout, we assume for simplicity that A,
C, and Y are binary variables. While text
classiﬁers can convert high-dimensional data into
binary variables for such analyses, we need to
make further assumptions about how classiﬁcation
errors affect causal inferences. We cannot assume
that the output of a text classiﬁer can be treated as
if it were ground truth. To conceptualize the ways
in which a text classiﬁer may be biased, we will
consider them as a way to recover from missing
data or measurement error.

3 Causal Models

Real-world observational data is messy and often
imperfectly collected. Work in causal inference
has studied how analyses can be made robust
to missing data or data recorded with systematic
measurement errors.

C

A

RA

C

Y

A

A(1)

Y

C

A

A∗

Y

(a) Simple Confounding

(b) Missing Data

(c) Measurement Error

Figure 1: DAGs for causal inference without text data. Red variables are unobserved.
A is a treatment, Y is an outcome, and C is a confounder.

A C Y
0
1
1
1
1
0
1
0
0
1
0
1

RA A C Y
0
1
1
0
1
1
1
0

1
1
0
0

1
?
0
?

A∗ C Y
0
1
0
1
1
0
1
0
0
1
0
1

A∗ A
1
1
1
0
0
0
1
1

(a) Simple Confounding

(b) Missing Data

(c) Measurement Error

(d) Mismeasurement

Figure 2: Example data rows for causal inference without text data.

3.1 Missing Data

3.2 Measurement Error

Our dataset has “missing data” if it contains
individuals (instances) for which some variables
are unobserved, even though these variables are
typically available.
This may occur if some
survey respondents choose not to answer certain
questions, or if certain variables are difﬁcult to
collect and thus infrequently recorded. Missing
data is closely related to causal inference – both
are interested in hypothetical distributions that
we cannot directly observe (Robins et al., 2000;
Shpitser et al., 2015).

Consider a causal model where A is sometimes
missing (Figure 1b). The variable RA is a binary
indicator for whether A is observed (RA = 1)
or missing. The variable A(RA = 1), written
as A(1), represents the counterfactual value of A
were it never missing. Finally, A is the observed
proxy for A(1): it has the same value as A(1) if
RA = 1, and the special value “?” if RA = 0.

Solving missingness can seen as intervening to
set RA to 1. Given p(A, RA, C, Y ), we want to
recover p(A(1), C, Y ). We may need to make a
“Missing at Random” (MAR) assumption, which
says that the missingness process is independent of
the true missing values, conditional on observed
values. Figure 1b reﬂects the MAR assumption;
RA is independent of A(1) given fully-observed
C and Y .
If an edge existed from A(1) to RA,
we have “Missing Not at Random” (MNAR) and
would not be identiﬁed except in special cases
(Shpitser et al., 2015).

Sometimes a necessary variable is never observed,
but is instead proxied by a variable which differs
Consider the
from the truth by some error.
example of body mass index (BMI) as a proxy for
obesity in a clinical study. Obesity is a known
risk factor for many health outcomes, but has a
complex clinical deﬁnition and is nontrivial to
measure. BMI is a simple deterministic function
of height and weight. To conduct a causal analysis
of obesity on cancer when only BMI and cancer
are measured, we can proceed as if we had
measured obesity and then correct our analysis
for the known error that comes from using BMI
as a proxy for obesity (Hern´an and Cole, 2009;
Michels et al., 1998).

To generalize this concept, we can replace
obesity with our ground truth variable A
and replace BMI with a noisy proxy A∗.
Figure 1c gives
this model.
the DAG for
there is no
Unlike missing data problems,
hypothetical
the
true data distribution p(A, C, Y ).
Instead, we
manipulate the observed distribution p(A∗, C, Y )
with the known relationship p(A∗, A) to recover
the desired p(A, C, Y ).

intervention which recovers

Unlike missing data, measurement

error
conceptualization can be used even when we
never observe A (e.g.
the table in Figure 2c)
as long as we have knowledge about the error
mechanism p(A∗, A). Using this knowledge,
the error using ‘matrix
we can correct

for

C

A

Ti

Y

V

RA

C

A

A(1)

Ti

V

Y

C

A

A∗

Y

Ti

V

(a) Simple Confounding with Text

(b) Missing Data with Text

(c) Measurement Error with Text

Figure 3: DAGs for causal inference with text data. In the Yelp experiments we discuss, Ti inﬂuences Y
and not the other way around.

τS =

(p(Y = 1 | A = 1, C) − p(Y = 1 | A = 0, C)) p(C)

τMD =

(cid:18)

XC

′

p(A = 1 | T, C, Y = 1, RA = 1)
y p(A = 1 | T, C, y′, RA = 1)p(Y = y′ | C)
p(A = 0 | T, C, Y = 1, RA = 1)

y p(A = 0 | T, C, Y = y′, RA = 1)p(Y = y′ | C) (cid:19)

p(Y = 1, C)

XC

P
−

′

P

(1)

(2)

τME =

XC







y′

P

−δc,y=1qc,y=1(0) + (1 − δc,y=1)qc,y=1(1)
(1 − ǫc,y=1 − δc,y=1)

(1 − ǫc,y=1)qc,y=1(0) − ǫc,y=1qc,y=1(1)
(1 − ǫc,y=1 − δc,y=1)

−δc,y′qc,y′ (0) + (1 − δc,y′)qc,y′ (1)
(1 − ǫc,y′ − δc,y′ )

(1 − ǫc,y′ )qc,y′ (0) − ǫc,y′ qc,y′ (1)
(1 − ǫc,y′ − δc,y′)

−

y′

P

p(C)

(3)







Deﬁne ǫc,y = p(A = 0 | A∗ = 1, C = c, Y = y), δc, y = p(A = 1 | A∗ = 0, C = c, Y = y),
qc,y(0) = p(C = c, Y = y, A∗ = 0), and qc,y(1) = p(C = c, Y = y, A∗ = 1).

Figure 4: Functionals for the Causal Effects for Simple Confounding (τSC), Missing Data (τMD) and
Measurement Error (τME). Derivations are in Appendices A, B, and C.

adjustment’ (Pearl, 2010).
In practice we might
learn p(A∗, A) from data such as that found in
Figure 2d. Recent work has also considered
how multiple independent proxies of A could
allow identiﬁcation without any data on p(A∗, A)
(Kuroki and Pearl, 2014).

4 Causal Models for Text Data

We can use conceptualizations for missing data
and measurement error to support causal analyses
with text data. The choice of model depends on the
assumptions we make about the data-generation
process.

to represent

text, which produces

We add new variables to our models (Figure
1a)
the
data-generating distribution shown in Figure 3a.
This model assumes that the underlying A, C, and
Y variables are generated before the text variables;
we use text
the true relationship
between A and Y .
We represent

text as an arbitrary set of
V variables, which are independent of one

to recover

another given the non-text variables.
In our
implemented analyses we will represent text as
a bag-of-words, wherein each Ti is simply the
binary indicator of the presence of the i-th word
in our vocabulary of V words, and T = ∪iTi.
The restriction to simple text models allows
us to explore connections to causal
inference
applications,
though future work could relax
assumptions of the text models to be inclusive
of more sophisticated text models (e.g. neural
sequence models (Lai et al., 2015; Zhang et al.,
2015)), or consider causal relationships between
two text variables.

To motivate our explanations, consider the task
of predicting an individuals’ smoking status from
free-text hospital discharge notes (Uzuner et al.,
2008; Wicentowski and Sydes, 2008).
Some
hospitals do not explicitly record patient smoking
status as structured data, making it difﬁcult to use
such data in a study on the outcomes of smoking.
We will suppose that we are given a dataset with
patient data on lung cancer outcome (Y ) and age

(C), that our data on smoking status (A) is affected
by either missing data or measurement error, but
that we have text data (T) from discharge records
that will allow us to infer smoking status with
reasonable accuracy.

4.1 Missing Data

To show how we might use text data to recover
from missing data, we introduce missingness for
A from Figure 3a to get the model in Figure 3b.
The missing arrow from A(1) to RA encodes the
MAR assumption, which is sufﬁcient to make it
possible to identify the full data distribution from
the observed data.

Suppose our motivation is to estimate the causal
effect of smoking status (A) on lung cancer (Y )
adjusting for age (C).
Imagine that missing
data arises because hospitals sometimes – but not
always – delete explicit data on smoking status
from patient records. If we have access to patients’
discharge notes (T) and know whether a given
patient had smoking status recorded (RA), then the
DAG in Figure 3b may be a reasonable model for
our setting. Note that we must again assume that
A does not directly affect RA.

The causal effect of A on Y in Figure 3b is
identiﬁed as τM D, given in Eq. 2 in Figure 4. The
derivation is given in Appendix B.

4.2 Measurement Error

We model text data with measurement error by
introducing a proxy A∗ to the model in Figure
3c. We assume that the proxied value of A∗ can
depend upon all other variables, and that we will
be able to estimate p(A∗, A) given an external
dataset, e.g.
text classiﬁer accuracy on held-out
data.

Suppose we again want to estimate the causal
effect from §4.1, but this time none of our hospital
records contain explicit data on smoking status.
However, imagine that we have a separate training
dataset of medical discharge records annotated
by expert pulmonologists for patients’ smoking
status. We could then train a classiﬁer to predict
smoking status using discharge record text1.

Working from the derivation for matrix
in binary models given by Pearl
adjustment
(2010), we identify the causal effect of A on
Y (Figure 3c) as τME (Eq 3 in Figure 4.) The
derivation is in Appendix C.

1This is the precise setting of Uzuner et al. (2008).

5 Experiments

We now empirically evaluate the effectiveness
of our two conceptualizations (missing data and
measurement error) for including text data in
causal analyses. We induce missingness or
mismeasurement of the treatment variable and use
text data to recover the true causal relationship
treatment on the outcome. We begin
of that
with a simulation study with synthetic text data,
and then conduct an analysis using reviews from
yelp.com.

5.1 Synthetic Data

A graphical model

We select synthetic data so that we can control
the entire data-generation process.
For each
data row, we ﬁrst sample data on three binary
variables (A, C, Y ) and then sample V different
binary variables Ti representing a V -vocabulary
bag-of-words.
this
distribution appears in Figure 3a. We augment
this distribution to introduce either missing
data (Figure 3b) or measurement error (Figure
For measurement error, we sample two
3c.)
datasets. A small training set which gives data on
p(A, C, Y, T) and a large test set which gives data
on p(C, Y, T).

for

The full data generating process appears in
Appendix D, and the implementation (along with
all our code) is provided online2.

5.2 Yelp Data
We utilize the 2015 Yelp Dataset Challenge3
which provides 4.7M reviews of local businesses.
Each review contains a one- to ﬁve-star rating, up
to 5,000 characters of text. Yelp users can ﬂag
reviews as “Useful” as a mark of quality.

We extract treatment, outcome, and confounder
variables from the structured data. The treatment
is a binarized user rating that takes value 1 if
the review has four or ﬁve stars and value 0
if the review has one or two stars. Three-star
reviews are discarded from our analysis. The
outcome is whether the review received at least
one “Useful” ﬂag. The confounder is whether
two
the review’s author has received at
“Useful” ﬂags across all reviews, according to
their user object.
In our data, 74.2% of reviews
were positive, 42.6% of reviews were ﬂagged as
“Useful,” and 56.7% users had received at least

least

2github.com/zachwooddoughty/emnlp2018-causal
3yelp.com/dataset/challenge

two such ﬂags. We preprocess the text of each
review by lowercasing, stemming, and removing
stopwords, before converting to a bag-of-words
representation with the 4,334 word vocabulary of
all words which appeared at least 1000 times in a
sample of 1M reviews.

Based on this p(A, C, Y, T) distribution,
we assume the data-generating process
that
matches Figure 3a and introduce missingness
and mismeasurement
giving us
data-generating processes matching Figures 3b
and 3c.

as before,

Our intention is not

to argue about a true
real-world causal effect of Yelp reviews on peer
behavior: we do not believe that our confounder
is the only common cause of the author’s rating
and the platform’s response. We leave for future
work a case study that jointly addresses questions
of identiﬁability and estimation of a real-world
In this work, our experiments
causal effect.
focus on a simpler task: can a correctly-speciﬁed
model that uses text data effectively estimate a
causal effect in the presence of missing data or
measurement error.

5.3 Models

We now introduce several baseline methods
which, unlike our correctly speciﬁed models τM D
and τM E, are not consistent estimators of our
desired causal effect. We would expect that the
theoretical bias in these estimators would result in
poor performance in our experiments.

5.3.1 Baseline: Na¨ıve Model

In both the missing data and measurement error
settings, our models use some rows that are full
observed. In missing data, these are rows where
RA = 1; in measurement error, the training set is
sampled from the true distribution. The simplest
approach to handling imperfect data is to throw
away all rows without full data, and calculate Eq
1 from that data. In Figure 5, these are labeled as
*.naive.

5.3.2 Baseline: Textless Model

In Figure 3b, if we do not condition on Ti to
d-separate A(1) from its missingness indicator,
that inﬂuence may bias our estimate. While we
know that ignoring text may introduce asymptotic
bias into our estimates of the causal effect, we
empirically evaluate how much bias is produced
by this “Textless” model compared to a correct

model. This is labeled as *.no text in Figure
5 (a).

In principle, we could conduct a measurement
error analysis using a model that does not include
text. In practice, we found we could not impute A∗
from C and Y alone. The non-textual classiﬁer
had such high error that the adjustment matrix
was singular and we could not compute the
effect. Thus, we have no such baseline in our
measurement error results.

5.3.3 Baseline: no y and unadjusted

Models

In Figure 3b, we must also condition on C and Y
to d-separate A(1) from its missingness indicator.
In our misspeciﬁed model for missing data, we do
not condition on Y , leaving open a path for A(1)
to inﬂuence its missingness. In Figure 5 (a), this
model is labeled as *.no y.

When correcting for measurement error, a
crucial piece of the estimation is the matrix
adjustment using the known error between
the proxy and the truth.
A straightforward
misspeciﬁed model for measurement error is to
impute a proxy for each row in our dataset and
then calculate the causal effect assuming no error
between the proxy and truth. This approach,
while simplistic, can be thought of as using a text
classiﬁer as a proxy without regard for the text
classiﬁer’s biases. In Figure 5 (b), this approach
is labeled as *.unadjusted.

5.3.4 Correct Models

Finally, we consider the estimation approaches
presented in §4.1 and §4.2. For the missing data
causal effect (τMD from Eq 2) we use a multiple
imputation estimator which calculates the average
effect across 20 samples from p(A|T, C, Y ) for
each row where RA = 0. For the measurement
error causal effect (τME from Eq 3), we use the
training set of p(A, C, Y, T) data to estimate ǫc,y
and δc, y and the larger set of p(C, Y, T) data to
estimate qc,y and p(C).

These models are displayed in Figure 5 (a) as

*.full and in Figure 5 (b) *.adjusted.

5.4 Evaluation

takes

Each model
in a data sample with
missingness or mismeasurement, and outputs an
estimate of the causal effect of A on Y in the
underlying data. Rather than comparing models’
estimates against a population-level estimate,

we compare against an estimate of the effect
computed on the same data sample, but without
any missing data or measurement error. This
‘perfect data estimator’ may still make errors
given the ﬁnite data sample. We compare against
this estimator to avoid a small-sample case where
an estimator gets lucky. In Figure 5, we plot data
sample size against the squared distance of each
model’s estimate from a perfect data estimator’s
estimate, averaged over ten runs. Figure 6 in
Appendix E contains a second set of experiments
using a larger vocabulary.

6 Results

Given that our correctly-speciﬁed models are
proven to be asymptotically consistent, we would
expect them to outperform misspeciﬁed models.
However,
asymptotic
for any given dataset,
consistency provides no guarantees.

6.1 Missing Data

The missing data (MD) experiments suggest that
the correct full model does perform best. The
no y model performs approximately as well as the
correct model on the synthetic data, but not on the
Yelp data. The difference between the no y and
full missing data models is simply a function
of the effect of Y on RA. We could tweak our
synthetic data distribution to increase the inﬂuence
of Y to make the no y model perform worse.
initially

other
data-generating distributions for missing data,
we found that when we reduced the inﬂuence
of the text variables on RA, the no text and
naive models approached the performance of
the correctly-speciﬁed model. While intuitive,
this reinforces that
the underlying distribution
matters a great deal in how modeling choices may
introduce biases if incorrectly speciﬁed.

considered

When

we

6.2 Measurement error

results

The measurement error
tell a more
interesting story. We see enormous ﬂuctuations
of the adjusted model, and in the synthetic
data, the unadjusted model appears to be quite
superior.

In the synthetic dataset, this is likely because
our text classiﬁer had near-perfect accuracy, and
so simple approach of assuming its predictions
were ground-truth introduced less bias. A broader
issue with the adjusted model
the

is that

matrix adjustment approach requires dividing
this
by (potentially very small) probabilities,
sometimes resulted in huge over-corrections.
In
addition, since those probabilities are estimated
training dataset, small
from a relatively small
changes to the error-estimate can propagate to
huge changes in the ﬁnal casual estimate.

This

instability of

the matrix adjustment
approach may be a bigger problem for text and
other high-dimensional data: unlike in our earlier
example of BMI and obesity,
there are likely
no simple relationships between text and clinical
variables. However,
instead of using matrix
adjustment as a way to recover the true effect, we
may instead use it to bound the error our proxy
may introduce. As mentioned by Pearl (2010),
when p(A | A∗) is not known exactly, we can
use a Bayesian analysis to bound estimates of a
causal effect. In a downstream task, this would let
us explore the stability of our adjusted results.

7 Related Work

(2017)

explored

considered the
A few recent papers have
combining text data with
for
possibilities
from the
approaches
inference
causal
literature.
Landeiro and Culotta (2016) and
Landeiro and Culotta
text
classiﬁcation when the relationship between text
data and class labels are confounded. Other work
has used propensity scores as a way to extract
features from text data (Paul, 2017) or to match
social media users based on what words they
write (De Choudhury et al., 2016). The only work
we know of which seeks to estimate causal effects
using text data focuses on effects of text or effects
on text (Egami et al., 2018; Roberts et al., 2018).
In our work, our causal effects do not include
text variables: we use text variables to recover an
underlying distribution and then estimate a causal
effect within that distribution.

There is a conceptually related line of work
in the NLP community on inferring causal
relationships expressed in text
(Girju, 2003;
Kaplan and Berry-Rogghe, 1991). However, our
work is fundamentally different. Rather than
identify casual relations expressed via language,
we are using text data in a causal model to identify
the strength of an underlying causal effect.

(a) Missing Data

(b) Measurement Error

100

10−1

10−2

10−3

10−4

10−5

10−6

10−7

10−8

10−9

Yelp.naive
Yelp.no text
Yelp.no y
Yelp.full
Synthetic.naive
Synthetic.no text
Synthetic.no y
Synthetic.full

Yelp.naive
Yelp.unadjusted
Yelp.adjusted
Synthetic.naive
Synthetic.unadjusted
Synthetic.adjusted

102

103

104

105

106

107

102

103

104

105

106

107

Dataset Size

Dataset Size

Figure 5: Experimental results. Squared distance (y-axis, lower is better) of the estimated causal effect
from τSC calculated from the full data with no missing data or measurement error. Error bars (negligible
for larger datasets) are 1.96 times standard error across 10 experiments. Additional experiments with a
larger vocabulary are shown in Appendix E.

8 Future Directions

issues
While this paper addresses some initial
arising from using text classiﬁers in causal
analyses, many challenges remain. We highlight
some of these issues as directions for future
research.

We provided several proof-of-concept models
for estimating effects, but our approach is
ﬂexible to more sophisticated models.
For
a semi-parametric estimator would
example,
make no assumptions
text data
about
distribution by wrapping the text classiﬁer into
an inﬁnite-dimensional nuisance model (Tsiatis,
2007). This would enable estimators robust to
partial model misspeciﬁcation (Bang and Robins,
2005).

the

Choices in the design of statistical models of
text consider issues like accuracy and tractability.
Yet if these models are to be used in a causal
framework, we need to understand how modeling
assumptions introduce biases and other issues that
can interfere with a downstream causal analysis.
To take an example from the medical domain, we

know that doctors write clinical notes throughout
the healthcare process, but it is not obvious how
to model this data-generating process. We could
assume that the doctor’s notes passively record
a patient’s progression, but in reality it may be
that the content of the notes themselves actively
change the patient’s care; causality could work in
either direction.

New lines of work in causality may be
especially helpful for NLP. In this work, we used
simple logistic regression on a bag-of-words
text;
representation of
using state-of-the-art
text models will
likely require more causal
assumptions. Nabi and Shpitser (2017) develops
causality-preserving
reduction,
which could help develop text representations that
preserve causality.

dimensionality

Finally, we are interested in case studies on
incorporating text classiﬁers into real-world causal
analyses. Many health studies have used text
classiﬁers to extract clinical variables from EHR
data (Meystre et al., 2008). These works could
be extended to study causal effects involving

those extracted variables, but such extensions
would require an understanding of the underlying
assumptions.
In any given study, the necessity
and appropriateness of assumptions will hinge
The conceptualizations
on domain expertise.
outlined in this paper, while far from solving all
issues of causality and text, will help those using
text classiﬁers to more easily consider research
questions of cause and effect.

Acknowledgments

This work was in part supported by the National
Institute of General Medical Sciences under grant
number 5R01GM114771 and by the National
Institute of Allergy and Infectious Diseases under
grant number R01 AI127271-01A1. We thank the
anonymous reviewers for their helpful comments.

References

Heejung Bang and James M Robins. 2005. Doubly
robust estimation in missing data and causal
inference models. Biometrics, 61(4):962–973.

Alexandre Belloni, Victor Chernozhukov,

and
Christian Hansen. 2014.
Inference on treatment
effects after selection among high-dimensional
The Review of Economic Studies,
controls.
81(2):608–650.

Victor Chernozhukov, Denis Chetverikov, Mert
Demirer, Esther Duﬂo, Christian Hansen, and
Double machine
Whitney K Newey. 2016.
learning for
treatment and causal parameters.
Technical report, cemmap working paper, Centre
for Microdata Methods and Practice.

A Philip Dawid. 2010. Beware of the dag!

In
Causality: Objectives and Assessment, pages 59–86.

Munmun De Choudhury, Emre Kiciman, Mark Dredze,
and Mrinal Kumar. 2016.
Glen Coppersmith,
Discovering shifts to suicidal ideation from mental
In Proceedings
health content in social media.
of the 2016 CHI conference on human factors in
computing systems, pages 2098–2110. ACM.

Naoki Egami, Christian J Fong, Justin Grimmer,
Margaret E Roberts, and Brandon M Stewart. 2018.
How to make causal inferences using texts. arXiv
preprint arXiv:1802.02163.

Roxana Girju. 2003. Automatic detection of causal
In Proceedings
relations for question answering.
the ACL 2003 workshop on Multilingual
of
summarization and question answering-Volume
12, pages 76–83. Association for Computational
Linguistics.

Brian Hazlehurst, Allison Naleway, and John Mullooly.
2009. Detecting possible vaccine adverse events
in clinical notes of the electronic medical record.
Vaccine, 27(14):2077–2083.

Miguel A Hern´an and Stephen R Cole. 2009. Invited
commentary: causal diagrams and measurement
epidemiology,
bias.
170(8):959–962.

American journal of

Randy M Kaplan and Genevieve Berry-Rogghe. 1991.
Knowledge-based acquisition of causal relationships
in text. Knowledge Acquisition, 3(3):317–337.

Manabu Kuroki and Judea Pearl. 2014. Measurement
inference.

bias and effect restoration in causal
Biometrika, 101(2):423–437.

Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao.
2015. Recurrent convolutional neural networks for
In AAAI, volume 333, pages
text classiﬁcation.
2267–2273.

Virgile Landeiro and Aron Culotta. 2016. Robust text
classiﬁcation in the presence of confounding bias. In
Thirtieth AAAI Conference on Artiﬁcial Intelligence.

Virgile Landeiro and Aron Culotta. 2017. Controlling
classiﬁcation
confounds
arXiv preprint

for
using correlational constraints.
arXiv:1703.01671.

unobserved

in

and

St´ephane M Meystre, Guergana K Savova, Karin C
Kipper-Schuler,
2008.
Extracting information from textual documents
in the electronic health record:
a review of
recent research. Yearbook of medical informatics,
17(01):128–144.

John F Hurdle.

Karin B Michels, Sander Greenland, and Bernard A
Rosner. 1998. Does body mass index adequately
capture the relation of body composition and body
American Journal of
size to health outcomes?
Epidemiology, 147(2):167–172.

Razieh Nabi and Ilya Shpitser. 2017. Semi-parametric
reduction
of
arXiv preprint

treatments.

dimension

causal
sufﬁcient
high dimensional
arXiv:1710.06727.

Jerzy Neyman. 1923. Sur les applications de la thar
des probabilities aux experiences agaricales: Essay
des principle. excerpts reprinted (1990) in English.
Statistical Science, 5:463–472.

Michael J Paul. 2017. Feature selection as causal
inference: Experiments with text classiﬁcation.
the 21st Conference on
In Proceedings of
Learning
Computational Natural
(CoNLL 2017), pages 163–172.

Language

Judea Pearl. 1995. Causal diagrams for empirical

research. Biometrika, 82(4):669–688.

Judea Pearl. 2009. Causality. Cambridge university

press.

Judea Pearl. 2010. On measurement bias in causal
the Twenty-Sixth
inference.
Conference on Uncertainty in Artiﬁcial Intelligence,
pages 425–432. AUAI Press.

In Proceedings of

Margaret E Roberts, Brandon M Stewart,

and
Adjusting for

Richard A Nielsen. 2018.
confounding with text matching.

James M Robins, Andrea Rotnitzky, and Daniel O
Scharfstein. 2000. Sensitivity analysis for selection
bias and unmeasured confounding in missing data
and causal inference models. In Statistical models
in epidemiology, the environment, and clinical trials,
pages 1–94. Springer.

D. B. Rubin. 1976. Causal inference and missing data

(with discussion). Biometrika, 63:581–592.

Ilya Shpitser, Karthika Mohan, and Judea Pearl.
2015. Missing data as a causal and probabilistic
the Thirty-First
problem.
Conference on Uncertainty in Artiﬁcial Intelligence,
pages 802–811. AUAI Press.

In Proceedings of

Ilya Shpitser and Judea Pearl. 2008.

Complete
identiﬁcation methods for the causal hierarchy.
Journal
Research,
9(Sep):1941–1979.

of Machine

Learning

Anastasios Tsiatis. 2007. Semiparametric theory and
missing data. Springer Science & Business Media.

¨Ozlem Uzuner, Ira Goldstein, Yuan Luo, and Isaac
Identifying patient smoking status
Journal of
Informatics Association,

Kohane. 2008.
from medical discharge records.
the American Medical
15(1):14–24.

Stefan Wager and Susan Athey. 2017. Estimation and
inference of heterogeneous treatment effects using
random forests. Journal of the American Statistical
Association.

Richard Wicentowski and Matthew R Sydes. 2008.
Using implicit
information to identify smoking
status in smoke-blind medical discharge summaries.
Journal of
Informatics
the American Medical
Association, 15(1):29–31.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
text
In Advances in neural information

Character-level convolutional networks for
classiﬁcation.
processing systems, pages 649–657.

A Simple Confounding

p(Y (a)) =

p(Y (a) | C)p(C)

(4)

=

=

C
X

C
X

C
X

p(Y (a) | A, C)p(C)

(5)

p(Y | A, C)p(C)

(6)

Eq 5 holds because Y (a) ⊥ A | C, as seen in
Figure 1a. Plugging this distribution into τS =
E[Y (1)] − E[Y (0)] gives us the causal effect
presented in Figure 4, Eq 1.

This assumes that an intervention on A is
if we did conduct a randomized
well-deﬁned;
control trial, we could assign A = a and break
A’s dependence on C.

In general, this step requires that we condition
on all “back-door” paths between the treatment
and the outcome. In Figure 1(a), if we did not have
data on C, we could not block the back-door path
between A and Y .

Eq 6 holds due to consistency. We assume that,
given we intervened to set A = a, if that individual
would have been assigned A = a in nature, then
the distribution over Y is the same.

B Missing Data

Denote p(Y (A(1) = a)) = p(Y (a)).

First, we identify the causal effect in terms of

the true A(1).

p(Y (a))

=

=

=

C
X

C
X

C
X

p(Y (a) | A(1), C)p(C)

p(Y | A(1), C)p(C)

(7)

(8)

(9)

Where 7 holds by chain rule, 8 holds by A(1) ⊥

Y (a) | C, and 9 by consistency.

Now, we identify A(1) in terms of observed

data.

p(A(1), C, Y )

= p(A(1) | C, Y )p(C, Y )
= p(A(1) | C, Y, RA = 1)p(C, Y )
= p(A | C, Y, RA = 1)p(C, Y )

(10)

(11)

(12)

Where 10 holds by chain rule, 11 by A(1) ⊥

RA | C, Y , and 12 by consistency.

Now, use Eq 12 to identify p(Y | A(1), C) from

Eq 9 in terms of observed data.

p(Y | A(1), C)

=

=

=

P

(14)

(13)

p(Y, A(1), C)
p(A(1), C)
p(Y, A(1), C)
Y p(Y, A(1), C)
p(A | C, Y, RA = 1)p(C, Y )
Y p(A | C, Y, RA = 1)p(C, Y )
p(A | C, Y, RA = 1)p(Y | C)
Y p(A | C, Y, RA = 1)p(Y | C)
Where 13 holds by deﬁnition, 14 holds by
marginalization, 15 holds by an application of 12
twice, and 16 holds by canceling out p(C).

(15)

(16)

P

P

=

If we include text in this derivation, we simply
replace p(A | C, Y, RA = 1) with p(A |
T, C, Y, RA = 1), where T is all our text
variables.

Finally, combine Eq 9 and Eq 16 to get:

p(Y (A(1) = a))

=

XC

P

p(A | C, Y, RA = 1)p(Y | C)
Y p(A | C, Y, RA = 1)p(Y | C)

p(C)

(17)

Plugging this distribution into τMD

=
E[Y (1)] − E[Y (0)] gives us the causal effect
presented in Figure 4, Eq 2.

C Measurement Error

ǫc,y = p(A = 0 | A
δc, y = p(A = 1 | A

= 1, C = c, Y = y)
= 0, C = c, Y = y)

∗

∗

qc,y(0) = p(C = c, Y = y, A
qc,y(1) = p(C = c, Y = y, A

= 0)

= 1)

∗

∗

(18)

(19)

(20)

(21)

Eq (5) and (7) from Pearl 2010 gives us:

p(A = 1, C = c, Y = y)

=

−δc,yqc,y(0) + (1 − δc,y)qc,y(1)
(1 − ǫc,y − δc,y)

p(A = 0, C = c, Y = y)

=

(1 − ǫc,y)qc,y(0) − ǫc,yqc,y(1)
(1 − ǫc,y − δc,y)

(22)

(23)

p(Y (a) | C)p(C)

Deﬁne the following terms for convenience:

Now,

D Synthetic Data Distribution

p(Y | A = 1, C)

p(Y, A = 1, C)
p(A = 1, C)
p(Y, A = 1, C)
Y p(Y, A = 1, C)
−δc,yqc,y(0) + (1 − δc,y)qc,y(1)
(1 − ǫc,y − δc,y)

P

=

=

=

−δc,y′qc,y′(0) + (1 − δc,y′)qc,y′(1)
(1 − ǫc,y′ − δc,y′)

y′

P

and then,

p(Y | A = 0, C)

p(Y, A = 0, C)
p(A = 0, C)
p(Y, A = 0, C)
Y p(Y, A = 0, C)
(1 − ǫc,y)qc,y(0) − ǫc,yqc,y(1)
(1 − ǫc,y − δc,y)

P

=

=

=

(1 − ǫc,y′)qc,y′(0) − ǫc,y′qc,y′(1)
(1 − ǫc,y′ − δc,y′)

y′

P

Plugging this distribution

=
E[Y (1)] − E[Y (0)] gives us the causal effect
presented in Figure 4, Eq 3.

into τME

(24)

(25)

(26)

(27)

(28)

In the distributions below, Ber(p) is used as
the abbreviation a Bernoulli distribution with
probability p.

Below, si, ui and vi are the effect of C, A,
and Y on the probability of word Ti; each is
drawn from N (0, ζ), a parameter which controls
how correlated words are with the underlying
variables. When ζ is close to 0,
the words
are essentially random. When ζ is large,
the
words are essentially deterministic functions of
the underlying variables. Similarly wi is the effect
of word Ti on RA, and is drawn from N (0, η).

For both settings, we set vocabulary size to
4,334 (to match Yelp experiments) and ζ = 0.5.
For the missing data setting, we set η = 0.1. We
picked these constants by empirically ﬁnding a
reasonable middle ground between the text data
providing only noise and being a deterministic
function of their parents. We picked all other
constants such that the na¨ıve correlation p(Y | A)
was a poor estimate of the counterfactual p(Y (a))
in the full-data setting.

D.1 Missing data data-generation

C ∼ Ber(0.4)

(29)

A(1) ∼ Ber(−0.3C + 0.4)

Y ∼ Ber(0.2C + 0.1A + 0.5)
Ti ∼ Ber(0.5 + uiA + viC)

RA ∼ Ber

0.7 + 0.2C − 0.4Y +

wiTi

 

i
X

!

D.2 Measurement error data-generation

C ∼ Ber(0.4)
A ∼ Ber(−0.3C + 0.4)
Y ∼ Ber(0.2C + 0.1A + 0.5)
Ti ∼ Ber(0.5 + siC + uiA + viY )

(a) Missing Data

(b) Measurement Error

Yelp.naive
Yelp.no text
Yelp.no y
Yelp.full
Synthetic.naive
Synthetic.no text
Synthetic.no y
Synthetic.full

Yelp.naive
Yelp.unadjusted
Yelp.adjusted
Synthetic.naive
Synthetic.unadjusted
Synthetic.adjusted

101

102

103

104

105

106

107

101

102

103

104

105

106

107

Dataset Size

Dataset Size

Figure 6: Experimental results with a vocabulary of size 53,197. Squared distance (y-axis, lower is better)
of the estimated causal effect from τSC calculated from the full data with no missing data or measurement
error. Error bars (negligible for larger datasets) are 1.96 times standard error across 10 experiments.

100

10−1

10−2

10−3

10−4

10−5

10−6

10−7

10−8

10−9

E Additional Experiments

Figure 6 shows the results of a second set
to those
of experiments, which are identical
described in §5 except the vocabulary size is now
53,197 instead of 4,334. For the Yelp data, the
larger vocabulary consists of all words which
ten times in a sample of 1M
appear at
reviews. As the larger vocabulary introduced
greater memory requirements, we did not run
these experiments with as large of datasets.

least

The results of these experiments show roughly
the same patterns as those seen in Figure 5.
The adjusted measurement error models again
appear erratic, generally performing worse than
the unadjusted models though better than the
naive models.

The full missing data model appeared to
slightly outperform the no y model on Yelp
data but only perform as well on the synthetic
data. Both these models appeared better than the
naive and no text models on both datasets.

