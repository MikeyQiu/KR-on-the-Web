Fast Matrix Factorization for Online Recommendation
with Implicit Feedback∗

Xiangnan He

Hanwang Zhang

Min-Yen Kan

Tat-Seng Chua

School of Computing, National University of Singapore
{xiangnan, hanwang, kanmy, chuats}@comp.nus.edu.sg

7
1
0
2
 
g
u
A
 
6
1
 
 
]

R

I
.
s
c
[
 
 
1
v
4
2
0
5
0
.
8
0
7
1
:
v
i
X
r
a

ABSTRACT
This paper contributes improvements on both the eﬀective-
ness and eﬃciency of Matrix Factorization (MF) methods
for implicit feedback. We highlight two critical issues of ex-
isting works. First, due to the large space of unobserved
feedback, most existing works resort to assign a uniform
weight to the missing data to reduce computational com-
plexity. However, such a uniform assumption is invalid in
real-world settings. Second, most methods are also designed
in an oﬄine setting and fail to keep up with the dynamic
nature of online data.

We address the above two issues in learning MF models
from implicit feedback. We ﬁrst propose to weight the miss-
ing data based on item popularity, which is more eﬀective
and ﬂexible than the uniform-weight assumption. However,
such a non-uniform weighting poses eﬃciency challenge in
learning the model. To address this, we speciﬁcally de-
sign a new learning algorithm based on the element-wise
Alternating Least Squares (eALS) technique, for eﬃciently
optimizing a MF model with variably-weighted missing data.
We exploit this eﬃciency to then seamlessly devise an incre-
mental update strategy that instantly refreshes a MF model
given new feedback. Through comprehensive experiments
on two public datasets in both oﬄine and online protocols,
we show that our eALS method consistently outperforms
state-of-the-art implicit MF methods. Our implementation
is available at https://github.com/hexiangnan/sigir16-eals.

Keywords
Matrix Factorization, Implicit Feedback, Item Recommen-
dation, Online Learning, ALS, Coordinate Descent

1.

INTRODUCTION

User personalization has become prevalent in modern rec-
ommender system. It helps to capture users’ individualized

∗NExT research is supported by the National Research
Foundation, Prime Minister’s Oﬃce, Singapore under its
IRC@SG Funding Initiative.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’16, July 17-21, 2016, Pisa, Italy
c(cid:13) 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2911489

preferences and has been shown to increase both satisfac-
tion for users and revenue for content providers. Among
its various methods, matrix factorization (MF) is the most
popular and eﬀective technique that characterizes users and
items by vectors of latent factors [15, 32]. Early work on
MF algorithms for recommendation [14, 27] have largely fo-
cused on explicit feedback, where users’ ratings that directly
reﬂect their preference on items are provided. These works
formulated recommendation as a rating prediction problem
for which the large volume of unobserved ratings (i.e., miss-
ing data) are assumed to be extraneous for modeling user
preference [12]. This greatly reduces the modeling workload,
and many sophisticated methods have been devised, such as
SVD++ [15] and timeSVD [14].

However, explicit ratings are not always available in many
applications; more often, users interact with items through
implicit feedback, e.g., users’ video viewing and prod-
uct purchase history. Compared to explicit ratings, implicit
feedback is easier to collect for content providers, but more
challenging to utilize due to the natural scarcity of negative
feedback.
It has been shown that modeling only the ob-
served, positive feedback results in biased representations in
user proﬁles [4, 12]; e.g., Marlin et al. [18] ﬁnds that users
listen to music they expect to like and avoid the genres they
dislike, leading to a severe bias in the observed data.

To solve the problem of lacking negative feedback (also
known as the one-class problem [21]), a popular solution
is to model all the missing data as negative feedback [12].
However, this adversely degrades the learning eﬃciency due
to the full consideration of both observed and missing data.
More importantly, the low eﬃciency makes it even more dif-
ﬁcult to deploy implicit MF method online [29]. In practical
recommender systems where new users, items and interac-
tions are continuously streaming in, it is crucial to refresh
the underlying model in real-time to best serve users. In this
work, we concern the above two challenging problems of the
MF method — implicit feedback and online learning. We
note that we are not the ﬁrst to consider both aspects for
MF, as a recent work by Devooght et al. [4] has proposed an
eﬃcient implicit MF method for learning with dynamic data.
However, we argue that Devooght’s method [4] models miss-
ing data in an unrealistic, suboptimal way. Speciﬁcally, it
assigns a uniform weight to the missing data, assuming that
the missing entries are equally likely to be negative feedback.
However, such an assumption limits model’s ﬁdelity and ﬂex-
ibility for real applications. For example, content providers
usually know which items have been frequently featured to
users but seldom clicked; such items are more likely to be

true negative assessments and should be weighted higher
than others. In addition, Devooght’s method learns param-
eters through gradient descent, requiring an expensive line
search to determine the best learning rate at each step.

We propose a new MF method aimed at learning from im-
plicit feedback eﬀectively while satisfying the requirement of
online learning. We develop a new learning algorithm that
eﬃciently optimizes the implicit MF model without impos-
ing a uniform-weight restriction on missing data.
In par-
ticular, we assign the weight of missing data based on the
popularity of items, which is arguably more eﬀective than
the previous methods [4, 12, 23, 30, 31] that are limited by
the uniformity assumption. Our eALS algorithm is fast in
accounting for missing data — analytically K times faster
than ALS [12] where K denotes number of latent factors
— the same time complexity with the recent dynamic MF
solution [4]. This level of eﬃciency makes eALS suitable
for learning online, for which we develop an incremental
update strategy that instantly refreshes model parameters
given new incoming data. Another key advantage of eALS
is that it works without learning rate, bypassing the well-
known diﬃculty for tuning gradient descent methods such
as [4] and Stochastic Gradient Descent (SGD) [25].
We summarize our key contributions as follows.
1. We propose an item popularity-aware weighting scheme
on the full missing data that eﬀectively tailors the MF
model for learning from implicit feedback.

2. We develop a new algorithm for learning model pa-
rameters eﬃciently and devise an incremental update
strategy to support real-time online learning.

3. We conduct extensive experiments with both oﬄine
and online protocols on two real-world datasets, show-
ing that our method consistently outperforms state-of-
the-art implicit MF methods.

2. RELATED WORK

Handling missing data is obligatory for learning from im-
plicit data due to the lack of negative feedback. To this end,
two strategies have been proposed — sample based learn-
ing [21, 25] that samples negative instances from missing
data, or whole-data based learning [12, 30] that treats
all missing data as negative. Both methods have pros and
cons: sample-based methods are more eﬃcient by reduc-
ing negative examples in training, but risk decreasing the
model’s predictiveness; whole-based methods model the full
data with a potentially higher coverage, but ineﬃciency can
be an issue. To retain model’s ﬁdelity, we persist in the
whole-data based learning, developing a fast ALS-based al-
gorithm to resolve the ineﬃciency issue.

For existing whole-data based methods [4, 12, 23, 30, 31],
one major limitation is in the uniform weighting on missing
entries, which favors algorithm’s eﬃciency but limits model’s
ﬂexibility and extensibility. The only works that have con-
sidered non-uniform weighting are from Pan et al. [20, 21];
however their cubic time complexity w.r.t. K makes it un-
suitable to run on large-scale data [23], where a large number
of factors needs to be considered to gain improved perfor-
mance [31].

To optimize MF, various learners have been investigated,
including SGD [14, 25], Coordinate Descent (CD) [4, 32],
and Markov Chain Monto Carlo (MCMC) [26]. SGD is the
most popular one owing to the ease of derivation, however,

it is unsuitable for whole-data based MF [12] due to the
large amount of training instances (the full user–item inter-
action matrix is considered). ALS can be seen as an in-
stantiation of CD and has been widely used to solve the
whole-based MF [12, 20, 21, 30]; however, its ineﬃciency is
the main obstacle for practical use [23, 31]. To resolve this,
[23] describes an approximate solution to ALS. Recently, [4]
employs the Randomized block Coordinate Descent (RCD)
learner [28], reducing the complexity and applying it to a dy-
namic scenario. Similarly, [31] enriches the implicit feedback
matrix with neighbor-based similarly, followed by applying
unweighted SVD. Distinct from previous works, we propose
an eﬃcient element-wise ALS solution for the whole-data
based MF with non-uniform missing data, which has never
been studied before.

Another important aspect for practical recommender sys-
tem lies in handling the dynamic nature of incoming data,
for which timeliness is a key consideration. As it is pro-
hibitive to retrain the full model online, various works have
developed incremental learning strategies for neighbor-based
[13], graph-based [9], probabilistic [3] and MF [4, 5, 17, 27]
methods. For MF, diﬀerent learners have been studied for
online updating, including SGD [5, 27], RCD [4] and dual-
averaging [17]. To our knowledge, this work is the ﬁrst at-
tempt to exploit the ALS technique for online learning.

3. PRELIMINARIES

We ﬁrst introduce the whole-data based MF method for
learning from implicit data, highlighting the ineﬃciency is-
sue of the conventional ALS solution [12, 21]. Then we
describe eALS, an element-wise ALS learner [26] that can
reduce the time complexity to linearity w.r.t. number of
factors. Although the learner is generic in optimizing MF
with all kinds of weighting strategies, the form introduced
is costly in accounting for all missing data and is thus unre-
alistic for practical use. This defect motivates us to further
develop eALS to make it suitable for learning from implicit
feedback (details in Section 4.2).

3.1 MF Method for Implicit Feedback

We start by introducing some basic notation. For a user–
item interaction matrix R ∈ RM ×N , M and N denote the
number of users and items, respectively; R denotes the set
of user–item pairs whose values are non-zero. We reserve the
index u to denote a user and i to denote an item. Vector pu
denotes the latent feature vector for u, and set Ru denotes
the set of items that are interacted by u; similar notations
for qi and Ri. Matrices P ∈ RM ×K and Q ∈ RN ×K denote
the latent factor matrix for users and items.

Matrix factorization maps both users and items into a
joint latent feature space of K dimension such that interac-
tions are modeled as inner products in that space. Mathe-
matically, each entry rui of R is estimated as:

ˆrui =< pu, qi >= pT

u qi.

(1)

The item recommendation problem is formulated as estimat-
ing the scoring function ˆrui, which is used to rank items.
Note that this basic model subsumes the biased MF [14],
commonly used in modeling explicit ratings:

ˆrui = bu + bi+ < pB

u , qB

i >,

where bu (bi) captures the bias of user u (item i) in giving
(receiving) ratings. To recover it, set pu ← [pB
u , bu, 1] and

qi ← [qB
i , 1, bi]. As such, we adopt the basic MF model
to make notations simple and also to enable a fair compari-
son with baselines [4, 12] that also complied with the basic
model.

To learn model parameters, Hu et al. [12] introduced a
weighted regression function, which associates a conﬁdence
to each prediction in the implicit feedback matrix R:

J =

M
(cid:88)

N
(cid:88)

u=1

i=1

wui(rui − ˆrui)2 + λ(

||pu||2 +

||qi||2), (2)

M
(cid:88)

u=1

N
(cid:88)

i=1

where wui denotes the weight of entry rui and we use W =
[wui]M ×N to represent the weight matrix. λ controls the
strength of regularization, which is usually an L2 norm to
prevent overﬁtting. Note that in implicit feedback learning,
missing entries are usually assigned to a zero rui value but
non-zero wui weight, both crucial to performance.

3.2 Optimization by ALS

Alternating Least Square (ALS) is a popular approach
to optimize regression models such as MF and graph reg-
ularization [10]. It works by iteratively optimizing one pa-
rameter, while leaving the others ﬁxed. The prerequisite of
ALS is that the optimization sub-problem can be analyti-
cally solved. Here, we describe how Hu’s work [12] solves
this problem.

First, minimizing J with respect to user latent vector pu

is equivalent to minimizing:

Ju = ||Wu(ru − Qpu)||2 + λ||pu||2,
where Wu is a N × N diagonal matrix with W u
minimum is where the ﬁrst-order derivative is 0:

ii = wui. The

∂Ju
∂pu

= 2QT WuQpu − 2QT Wuru + 2λpu = 0

(3)

⇒ pu = (QT WuQ + λI)−1QT Wuru,

where I denotes the identity matrix. This analytical solution
is also known as the ridge regression [23]. Following the same
process, we can get the solution for qi.

3.2.1 Efﬁciency Issue with ALS

As we can see, in order to update a latent vector, in-
verting a K × K matrix is inevitable. Matrix inversion
is an expensive operation, usually assumed O(K 3) in time
complexity [12]. As such, updating one user latent vector
takes time O(K 3 + N K 2). Thus, the overall time complex-
ity of one iteration that updates all model parameters once
is O((M + N )K 3 + M N K 2). Clearly, this high complexity
makes the algorithm impractical to run on large-scale data,
where there can be millions of users and items and billions
of interactions.

Speed-up with Uniform Weighting. To reduce the
high time complexity, Hu et al. [12] applied a uniform weight
to missing entries; i.e., assuming that all zero entries in R
have a same weight w0. Through this simpliﬁcation, they
can speed up the computation with memoization:

QT WuQ = w0QT Q + QT (Wu − W0)Q,

(4)

where W0 is a diagonal matrix that each diagonal element is
w0. As QT Q is independent of u, it can be pre-computed for
updating all user latent vectors. Considering the fact that
Wu − W0 only has |Ru| non-zero entries, we can compute

Eq. (4) in O(|Ru|K 2) time. Thus, the time complexity of
ALS is reduced to O((M + N )K 3 + |R|K 2).

Even so, we argue that the O((M + N )K 3) term can be
a major cost when (M + N )K ≥ |R|.
In addition, the
O(|R|K 2) part is still much higher than in SGD [25], which
only requires O(|R|K) time. As a result, even with the ac-
celeration, ALS is still prohibitive for running on large data,
where large K is crucial as it can lead to better generaliz-
ability and thus better prediction performance. Moreover,
the uniform weighting assumption is usually invalid in real
applications and adversely degrades model’s predictiveness.
This thus motivates us to design an eﬃcient implicit MF
method not subject to uniform-weights.

3.3 Generic Element-wise ALS Learner

The bottleneck of the previous ALS solution lies in the ma-
trix inversion operation, which is due to the design that up-
dates the latent vector for a user (item) as a whole. As such,
it is natural to optimize parameters at the element level —
optimizing each coordinate of the latent vector, while leav-
ing the others ﬁxed [26]. To achieve this, we ﬁrst get the
derivative of objective function Eq. (2) with respect to puf :

∂J
∂puf

N
(cid:88)

i=1

= −2

(rui − ˆrf

ui)wuiqif + 2puf

wuiq2

if + 2λpuf ,

N
(cid:88)

i=1

where ˆrf
ui = ˆrui − puf qif , i.e., the prediction without the
component of latent factor f . By setting this derivative to
0, we obtain the solution of puf :

Similarly, we can get the solver for an item latent factor:

puf =

(cid:80)N

i=1(rui − ˆrf
(cid:80)N
i=1 wuiq2

ui)wuiqif
if + λ

.

qif =

(cid:80)M

u=1(rui − ˆrf
(cid:80)M
i=1 wuip2

ui)wuipuf
uf + λ

.

(5)

(6)

Given the closed-form solution that optimizes one param-
eter with other ﬁxed, the algorithm iteratively executes it
for all model parameters until a joint optimum is reached.
Due to the non-convexity of the objective function, critical
points where gradients vanish can be local minima.

Time Complexity. As can be seen, by performing opti-
mization at the element level, the expensive matrix inversion
can be avoided. A raw implementation takes O(M N K 2)
time for one iteration, directly speeding up ALS by eliminat-
ing the O(K 3) term. Moreover, by pre-computing ˆrui [26],
we can calculate ˆrf
ui in O(1) time rather than O(K). As
such, the complexity can be further reduced to O(M N K),
which is the same magnitude with evaluating all the user–
item predictions.

4. OUR IMPLICIT MF METHOD

We ﬁrst propose an item-oriented weighting scheme on
the missing data, and follow with a popularity-aware weight-
ing strategy, which is arguably more eﬀective than the uni-
form weighting for the recommendation task. Then, we de-
velop a fast eALS algorithm to optimize the objective func-
tion that signiﬁcantly reduces learning complexity compar-
ing with the conventional ALS [12] and generic element-wise
ALS learner [26]. Lastly, we discuss how to adjust the learn-
ing algorithm for real-time online learning.

(7)

the observed data part:

4.1

Item-Oriented Weighting on Missing Data
Due to the large space of items, the missing entries for a
user are a mixture of negative and unknown feedback. In
specifying the weight wui of missing entries, it is desired to
assign a higher weight to the negative feedback. However, it
is a well-known diﬃculty to diﬀerentiate the two cases. In
addition, as the interaction matrix R is usually large and
sparse, it will be too consuming to store each zero entry an
individualized weight. To this end, existing works [4, 12,
23, 30, 31] have applied a simple uniform weight on missing
entries, which are, however, suboptimal and non-extendable
for real applications.

Considering the ease of content providers in accessing neg-
ative information of the item side (e.g., which items have
been promoted to users but receive little interaction), we
believe it is more realistic to weight missing data based on
some item property. To capture this, we devise a more ﬁne-
grained objective function as follows:

(cid:88)

L =

wui(rui − ˆrui)2 +

(u,i)∈R

M
(cid:88)

u=1

+ λ(

||pu||2 +

||qi||2),

N
(cid:88)

i=1

M
(cid:88)

(cid:88)

u=1

i /∈Ru

ci ˆr2
ui

where ci denotes the conﬁdence that item i missed by users
is a true negative assessment, which can serve as a means to
encode domain knowledge from practitioners. It is clear that
the ﬁrst term denotes the prediction error of the observed
entries, which has been widely adopted in modeling explicit
ratings [15, 27]. The second term accounts for the miss-
ing data, which acts as the role of negative instances and is
crucial for recommendation from implicit feedback [12, 25].
Next, we present a domain-independent strategy to deter-
mine ci by leveraging a ubiquitous feature of modern Web
2.0 systems.

4.1.1 Popularity-aware Weighting Strategy

Existing visual interfaces of many Web 2.0 systems show-
case popular items in their recommendations. All other fac-
tors being equal, popular items are more likely to be known
by users in general [10], and thus it is reasonable to think
that a miss on a popular item is more probable to be truly
irrelevant (as opposed to unknown) to the user. To account
for this eﬀect, we parametrize ci based on item’s popularity:

ci = c0

f α
i
j=1 f α
j

,

(cid:80)N

(8)

where fi denotes the popularity of item i, given by its fre-
quency in the implicit feedback data: |Ri|/ (cid:80)N
j=1 |Rj|, and
c0 determines the overall weight of missing data. Exponent
α controls the signiﬁcance level of popular items over un-
popular ones — when α > 1 the weights of popular items
are promoted to strengthen the diﬀerence against unpop-
ular ones; while setting α within the lower range of (0, 1)
suppresses the weight of popular items and has a smoothing
eﬀect. We empirically ﬁnd α = 0.5 usually leads to good
results. Note that the uniform weighting is a special case by
setting α to 0 with w0 = c0/N .

Relationship to Negative Sampling. Our proposed
popularity-aware weighting strategy has the same intuition
with Rendle’s popularity-based oversampling [24] for learn-
ing BPR, which basically samples popular items as nega-

tive feedback with a higher probability. However, [24] em-
pirically shows the oversampling method underperforms the
basic uniform sampler. We suspect the reason comes from
the SGD learner, which will result in more gradient steps
on popular items, due to oversampling. As a result, popular
items may be over-trained locally at the expense of less pop-
ular items which would then be under-trained. To resolve
this, tricks like subsampling frequent items [19] and adaptive
learning rates like Adagrad [6] have been adopted in other
domains. As the focus of this paper is on whole-data based
implicit MF, we do not further explore the details of SGD. It
is worth pointing out that our proposed eALS learner avoids
these learning issues by an exact optimization on each model
parameter.

4.2 Fast eALS Learning Algorithm

We can speed up learning by avoiding the massive re-
peated computations introduced by the weighted missing
data. We detail the derivation process for puf ; where the
counterpart for qif is achieved likewise.

First, we rewrite the puf update rule Eq. (5) by separating

puf =

(cid:80)

i∈Ru
(cid:80)

(rui − ˆrf

ui)wuiqif − (cid:80)
if + (cid:80)

i /∈Ru

ˆrf
uiciqif
if + λ

i /∈Ru
ciq2

wuiq2

.

i∈Ru

Clearly, the computational bottleneck lies in the summa-
tion over missing data portion, which requires a traversal of
the whole negative space. We ﬁrst focus on the numerator:

(cid:88)

i /∈Ru

ˆrf
uiciqif =

ciqif

pukqik −

ˆrf
uiciqif

N
(cid:88)

i=1

(cid:88)

k(cid:54)=f

(cid:88)

=

puk

N
(cid:88)

k(cid:54)=f

i=1

(cid:88)

i∈Ru

(cid:88)

i∈Ru

ciqif qik −

ˆrf
uiciqif .

(9)

By this reformulation, we can see that the major computa-
tion — the (cid:80)N
i=1 ciqif qik term that iterates over all items
— is independent of u. However, a na¨ıve implementation
repeatedly computes it unnecessarily, when updating the la-
tent factors for diﬀerent users. Clearly, we can achieve a
signiﬁcant speed-up by memoizing it.

We deﬁne the Sq cache as Sq = (cid:80)N

i , which can
be pre-computed and used in updating the latent factors for
all users. Then, Eq. (9) can be evaluated as:

i=1 ciqiqT

(cid:88)

i /∈Ru

ˆrf
uiciqif =

puksq

f k −

ˆrf
uiciqif ,

(10)

(cid:88)

k(cid:54)=f

(cid:88)

i∈Ru

which can be done in O(K + |Ru|) time.

Similarly, we can apply the cache to speed up the calcu-

lation of denominator:

(cid:88)

i /∈Ru

N
(cid:88)

i=1

(cid:88)

i∈Ru

ciq2

if =

ciq2

if −

ciq2

if = sq

f f −

ciq2

if . (11)

(cid:88)

i∈Ru

To summarize the above memoization strategy, we give

the update rule for puf with the use of Sq cache:

puf =

(cid:80)

i∈Ru

[wuirui − (wui − ci)ˆrf
(wui − ci)q2

(cid:80)

ui]qif − (cid:80)
if + sq
f f + λ

i∈Ru

k(cid:54)=f puksq

kf

.

(12)

Algorithm 1: Fast eALS Learning algorithm.

Input: R, K, λ, W and item conﬁdence vector c;
Output: Latent feature matrix P and Q;

1 Randomly initialize P and Q ;
2 for (u, i)∈ R do ˆrui ← Eq. (1) ;
3 while Stopping criteria is not met do

(cid:46) O(|R|K)

// Update user factors
Sq = (cid:80)N
i=1 ciqiqT
;
for u ← 1 to M do

i

(cid:46) O(M K 2)
(cid:46) O(M K 2 + |R|K)

for f ← 1 to K do
for i ∈ Ru do ˆrf
puf ← Eq. (12) ;
for i ∈ Ru do ˆrui ← ˆrf

ui ← ˆrui − puf qif ;

(cid:46) O(K + |Ru|)

ui + puf qif ;

end

end
// Update item factors
Sp ← PT P ;
for i ← 1 to N do

(cid:46) O(N K 2)
(cid:46) O(N K 2 + |R|K)

for f ← 1 to K do
for u ∈ Ri do ˆrf
qif ← Eq. (13) ;
for u ∈ Ri do ˆrui ← ˆrf

ui ← ˆrui − puf qif ;

(cid:46) O(K + |Ri|)

ui + puf qif ;

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

end

end

19
20 end
21 return P and Q

Table 1: Time complexity of implicit MF methods.

Time Complexity
Method
O((M + N )K 3 + |R|K 2)
ALS (Hu et al.[12])
O(|R|K)
BPR (Rendle et al.[25])
O(K 3 + (M + N )K 2 + |R|K)
IALS1 (Pil´aszy et al.[23])
ii-SVD (Volkovs et al.[31]) O((M + N )K 2 + M N log K)
RCD (Devooght et al.[4])
eALS (Algorithm 1)

O((M + N )K 2 + |R|K)
O((M + N )K 2 + |R|K)

|R| denotes the number of non-zeros in user–item matrix R.

Similarly, we can derive the update rule for qif :

qif =

(cid:80)

u∈Ri

[wuirui − (wui − ci)ˆrf
(wui − ci)p2

(cid:80)

(cid:80)

ui]puf − ci
uf + cisp

f f + λ

u∈Ri

k(cid:54)=f qiksp

kf

,

(13)
kf denotes the (k, f )th element of the Sp cache, de-

where sp
ﬁned as Sp = PT P.

Algorithm 1 summarizes the accelerated algorithm for our
element-wise ALS learner, or eALS. For convergence, one
can either monitor the value of objective function on train-
ing set or check the prediction performance on a hold-out
validation data.

4.2.1 Discussion

Time Complexity.

In Algorithm 1, updating a user
latent factor takes O(K + |Ru|) time. Thus, one eALS iter-
ation takes O((M +N )K 2 +|R|K) time. Table 1 summarizes
the time complexity (of one iteration or epoch) of other MF
algorithms that are designed for implicit feedback.

Comparing with the vector-wise ALS [12, 21], our element-
wise ALS learner is K times faster. In addition, our pro-
posed eALS has the same time complexity with RCD [4],

being faster than ii-SVD [31], another recent solution. RCD
is a state-of-the-art learner for whole-data based MF, which
performs a gradient descent step on a randomly chosen la-
tent vector. Since it requires a good learning rate, the work
[4] adaptively determines it by a line search in each gradient
step, which essentially chooses the learning rate that leads to
the steepest descent among pre-deﬁned candidates. A major
advantage of eALS has over RCD is that it avoids the need
for a learning rate by an exact optimization in each param-
eter update, arguably more eﬀective and easier to use than
RCD. The most eﬃcient algorithm is BPR, which applies
the SGD learner on sampled, partial missing data only.

Computing the Objective Function. Evaluating the
objective function is important to check the convergence of
iterations and also to verify the correctness of implementa-
tion. A direct calculation takes O(M N K) time, requiring a
full estimation on the R matrix. Fortunately, with the item-
oriented weighting, we can similarly exploit the sparseness
of R for acceleration. To achieve this, we reformulate the
loss of the missing data part that causes the major cost:

M
(cid:88)

(cid:88)

u=1

i /∈Ru

ci ˆr2

ui =

u Sqpu −
pT

M
(cid:88)

u=1

(cid:88)

ci ˆr2

ui.

(u,i)∈R

(14)

By reusing Sq and the prediction cache ˆrui, we can calculate
the objective function in O(|R| + M K 2) time, much faster
than with direct calculation.

Parallel Learning. The iterations of eALS can be easily
parallelized. First, computing the S caches (line 4 and 12)
is the standard matrix multiplication operation, for which
modern matrix toolkits provide very eﬃcient and parallelized
implementation. Second, in updating the latent vectors for
diﬀerent users (line 5-11), the shared parameters are either
independent with each other (i.e., ˆrui) or remaining un-
changed (i.e., Sq). This nice property means that an exact
parallel solution can be obtained by separating the updates
by users; that is, letting diﬀerent workers update the model
parameters for disjoint sets of users. The same parallelism
can also be achieved in updating item latent vectors.

This is an advantage over the commonly-used SGD learner,
which is a stochastic method that updates model parame-
ters given a training instance.
In SGD, diﬀerent gradient
steps can inﬂuence with each other and there is no exact
way to separate the updates for workers. Thus, sophisti-
cated strategies are required to control the possible losses
introduced by parallelization [7]. Our proposed eALS solu-
tion optimizes by coordinate descent where in each step a
dedicated parameter is updated, making the algorithm em-
barrassingly parallel without any approximate loss.

4.3 Online Update

In practice, after a recommender model is trained oﬄine
on historical data, it will be used online and will need to
adapt to best serve users. Here, we consider the online
learning scenario that refreshes model parameters given a
new user–item interaction.

Incremental Updating. Let ˆP and ˆQ denote the model
parameters learnt from oﬄine training, and (u, i) denotes
the new interaction streamed in. To approximate the model
parameters in accounting for the new interaction, we per-
form optimization steps for pu and qi only. The underlying
assumption is that the new interaction should not change ˆP
and ˆQ too much from a global perspective, while it should

Algorithm 2: Online Incremental Updates for eALS.
Input: ˆP, ˆQ, new interaction (u, i) and its weight wnew
Output: Refreshed parameters P and Q;

1 P ← ˆP; Q ← ˆQ; ;
2 If u is a new user do Randomly initialize pu ;
3 If i is a new item do Randomly initialize qi ;
4 rui ← 1;
5 while Stopping criteria is not met do

ˆrui ← Eq. (1); wui ← wnew;

6

7

8

// Line 6-10 of Algorithm 1
update user(u); ;
update cache(u, Sp); ;
// Line 14-18 of Algorithm 1
update item(i); ;
update cache(i, Sq); ;

(cid:46) O(K 2 + |Ru|K)
(cid:46) O(K 2)

(cid:46) O(K 2 + |Ri|K)
(cid:46) O(K 2)

9
10 end
11 return P and Q

change the local features for u and i signiﬁcantly. Particu-
larly, when u is a new user, executing the local updates will
force pu close to qi, which meets the expectation of latent
factor model. The new item case is similar.

Algorithm 2 summarizes the incremental learning strat-
egy for eALS. For the stopping criteria, our empirical study
shows that one iteration is usually suﬃcient to get good re-
sults. Moreover, it is important to note that after updating
a latent vector, we need to update the S cache accordingly.
Weight of New Interactions. In an online system, new
interactions are more reﬂective of a user’s short-term inter-
est. Comparing to the historical interactions used in oﬄine
training, fresh data should be assigned a higher weight for
predicting user’s future action. We assign a weight wnew to
each new interaction (line 4 of Algorithm 2) as a tunable pa-
rameter. Later in Section 5.3, we investigate how the setting
of this parameter impacts online learning performance.

Time Complexity. The incremental update for a new
interaction (u, i) can be done in O(K 2+(|Ru|+|Ri|)K) time.
It is worth noting that the cost depends on the number of
observed interactions for u and i, while being independent
with number of total interactions, users and items. This lo-
calized complexity make the online learning algorithm suit-
able to deployment in industrial use, as the complex software
stack that deals with data dependencies [29] can be avoided.

5. EXPERIMENTS

We begin by introducing the experimental settings. Then
we perform an empirical study with the traditional oﬄine
protocol, followed by a more realistic online protocol.

5.1 Experimental Settings

Datasets. We evaluate on two publicly accessible datasets:
Yelp1 and Amazon Movies2. We transform the review dataset
into implicit data, where each entry is marked as 0/1 indi-
cating whether the user reviewed the item. Since the high
sparsity of the original datasets makes it diﬃcult to evaluate
recommendation algorithms (e.g., over half users have only
one review), we follow the common practice [25] to ﬁlter out

1We used the Yelp Challenge dataset downloaded on Octo-
ber 2015 that contained 1.6 million reviews: http://www.yelp.
com/dataset challenge
2

http://snap.stanford.edu/data/web-Amazon-links.html

Table 2: Statistics of the evaluation datasets.
Dataset Review# Item# User# Sparsity
Yelp
Amazon

731,671
5,020,705

99.89%
99.94 %

25,677
117,176

25,815
75,389

users and items with less than 10 interactions. Table 2 sum-
marizes the statistics of the ﬁltered datasets.

Methodology. We evaluate using two protocols:
- Oﬄine Protocol. We adopt the leave-one-out evalu-
ation, where the latest interaction of each user is held out
for prediction and the models are trained on the remain-
ing data. Although it is a widely used evaluation protocol
in the literature [9, 25], we point out that it is an artiﬁ-
cial split that does not correspond to the real recommenda-
tion scenario. In addition, the new users problem is averted
in this evaluation, as each test user has a training history.
Thus this protocol only evaluates an algorithm’s capability
in providing one-shot recommendation for existing users by
leveraging the static history data.

- Online Protocol. To create a more realistic recom-
mendation scenario, we simulate the dynamic data stream.
We ﬁrst sort all interactions in chronological order, training
models on the ﬁrst 90% of the interactions and holding out
the last 10% for testing. In the testing phase, given a test in-
teraction (i.e., a user–item pair) from the hold-out data, the
model ﬁrst recommends a ranked list of items to the user;
the performance is judged based on the ranked list. Then
the test interaction is fed into the model for an incremental
update. Note that with the global split by time, 14% and
57% test interactions are from new users for the Yelp and
Amazon dataset, respectively. Overall this protocol evalu-
ates an online learning algorithm’s eﬀectiveness in digesting
the dynamic new data.

To assess the ranked list with the ground-truth (GT) item
that user actually consumed, we adopt Hit Ratio (HR) and
Normalized Discounted Cumulative Gain (NDCG). We trun-
cate the ranked list at 100 for both metrics. HR measures
whether the ground truth item is present on the ranked list,
while NDCG accounts for the position of hit [9]. We report
the score averaged by all test interactions.

Baselines. We compare with the following methods:
- ALS [12]. This is the conventional ALS method that
optimizes the whole-data based MF. Due to the high time
complexity, this method is infeasible in a real-time dynamic
updating scenario, so we only evaluate it with the oﬄine
protocol.

- RCD [4]. This is the state-of-the-art implicit MF method
that has the same time complexity with eALS and is suit-
able for online learning. For the line search parameters, we
use the suggested values in the authors’ implementation3.

- BPR [25]. This is a sample-based method that opti-
mizes the pair-wise ranking between the positive and nega-
tive samples. It learns by SGD, which can be adjusted to
online incremental learning by [27]. We use a ﬁxed learning
rate, varying it and reporting the best performance.

Parameter Settings. For the weight of observed in-
teractions, we set it uniformly as 1, a default setting by
previous works [4, 25]. For regularization, we set λ as 0.01
for all methods for a fair comparison. All methods are im-
plemented in Java and running on the same machine (Intel

3

https://github.com/rdevooght/MF-with-prior-and-updates

(a) eALS vs. c0 (α = 0)

(b) eALS vs. α (c0 = 512)

(c) eALS vs. c0 (α = 0)

(d) eALS vs. α (c0 = 64)

Figure 1: Impact of weighting parameters c0 and α on eALS’s performance evaluated by oﬄine protocol.

(a) Iterations vs. HR

(b) Iterations vs. NDCG

(c) Iterations vs. HR

(d) Iterations vs. NDCG

Figure 2: Prediction accuracy of three whole-data based MF methods in each iteration (K = 128).

Xeon 2.67GHz CPU and 24GB RAM) in a single-thread for
a fair comparison on eﬃciency. As the ﬁndings are consis-
tent across the number of factors K, without any particular
outlier, we only show the results of K = 128, a relatively
large number that maintains good accuracy.

5.2 Ofﬂine Protocol

We ﬁrst study how does the weighting scheme on missing
data impact eALS’s performance. Then we compare with
the whole-data based implicit MF methods ALS and RCD,
as well as the sample-based ranking method BPR.

5.2.1 Weight of Missing Data

In Section 4.1, we propose an item popularity-aware weight-
ing strategy, which has two parameters: c0 determines the
overall weight of missing data and α controls the weight dis-
tribution. First, we set a uniform weight distribution (i.e.,
α = 0), varying c0 to study how does the weight of miss-
ing data impact the performance. For Yelp (Figure 1a),
the peak performance is achieved when c0 is around 512,
corresponding to that the weight of each zero entry is 0.02
(w0 = c0/N ); similarly for Amazon (Figure 1c), the optimal
c0 is around 64, corresponding to w0 = 0.0001. When c0
becomes smaller (where w0 is close to 0), the performance
degrades signiﬁcantly. This highlights the necessity of ac-
counting for the missing data when modeling implicit feed-
back for item recommendation. Moreover, when c0 is set
too large, the performance also suﬀers. Based on this ob-
servation, we believe the traditional SVD technique [2] that
treats all entries equally weighted will be suboptimal here.
Then, we set c0 to the best value (in the case of α =
0), varying α to check the performance change. As can be
seen from Figure 1b and 1d, the performance of eALS is
gradually improved with the increase of α, and the best
result is reached around 0.4. We further conducted the one-
sample paired t-test, verifying that the improvements are
statistically signiﬁcant (p-value < 0.01) for both metrics on
the two datasets. This indicates the eﬀectiveness of our
popularity-biased weighting strategy. Moreover, when α is

Figure 3: NDCG of whole-data based MF across K.

larger than 0.5, the performance starts to drop signiﬁcantly.
This reveals the drawback of over-weighting popular items
as negative instances, thus the importance of accounting for
less popular items with a proper weight.

In the following experiments, we ﬁx c0 and α according to
the best performance evaluated by HR, i.e., c0 = 512, α =
0.4 for Yelp and c0 = 64, α = 0.5 for Amazon.

5.2.2 Compare Whole-data based MF Methods

We performed the same grid search of w0 for RCD and

ALS and reported the best performance.

Convergence. Figure 2 shows the prediction accuracy
with respect to number of iterations. First, we see that
eALS achieves the best performance after converge. All
improvements are statistically signiﬁcant evidenced by the
one-sample paired t-test (p < 0.01). We believe the beneﬁts
mainly come from the popularity-aware objective function,
as both ALS and RCD apply a uniform weighting on the un-
knowns. Second, eALS and ALS converge faster than RCD.
We think the reason is that (e)ALS updates a parameter to
minimize the objective function of the current status, while
RCD updates towards the direction of the negative gradient,
which can be suboptimal. On Amazon, RCD shows high but
turbulent NDCG in early iterations, while the low hit ratio
and later iterations indicate the high NDCG is unstable.
Finally, we point out that in optimizing the same objective
function, ALS outperforms RCD in most cases, demonstrat-
ing the advantage of ALS over the gradient descent learner.

(a) Iterations vs. HR

(b) Iterations vs. NDCG

(c) Iterations vs. HR

(d) Iterations vs. NDCG

Figure 4: Accuracy and convergence comparison of eALS with BPR of diﬀerent learning rate.

Accuracy vs. Number of Factors. Figure 3 shows
the prediction accuracy with varying number of factors K.
We only show the evaluation by NDCG as HR admits the
same trend. First, eALS consistently outperforms ALS and
RCD across K, demonstrating the eﬀectiveness of our eALS
method (n.b. although the three methods seem to perform
on par for Amazon at K = 128, their diﬀerence can be
clearly seen in Figure 2d). Second, all methods can be im-
proved signiﬁcantly with a larger K. Although a large K
might have the risk of overﬁtting, it can increase model’s
representation ability thus better prediction. Especially for
large datasets that can have millions of users and billions
of interactions, a large K is particularly important for the
accuracy of MF methods.

Eﬃciency. Analytically, ALS’s time complexity is O((M +
N )K 3 + |R|K 2), while eALS and RCD are K times faster.
To compare their eﬃciency empirically, we show the actual
training time per iteration in Table 3.

Table 3: Training time per iteration of diﬀerent
whole-based MF methods with varying K.

Yelp
eALS RCD ALS
10s
1s
46s
3s
221s
10s
23m
0.9m
2.5h
2m
25.4h
11m

K
32
64
128
256
512
1024
s, m, and h denote seconds, minutes and hours, respectively.

Amazon
eALS RCD ALS
74s
10s
4.8m
17s
21m
42s
2h
2.8m
11.6h
9m
74h
48m

9s
23s
72s
4m
12m
54m

1s
4s
13s
1m
2m
7m

As can be seen, with the increase of K, ALS takes much
longer time than eALS and RCD. Speciﬁcally, when K is
512, ALS requires 11.6 hours for one iteration on Amazon,
while eALS only takes 12 minutes. Although eALS does
not empirically show K times faster than ALS due to the
more eﬃcient matrix inversion implementation (we used the
fastest known algorithm [1] with time complexity around
O(K 2.376)), the speed-up is already very signiﬁcant. More-
over, as RCD and eALS have the same analytical time com-
plexity, their actual running time are in the same magnitude;
the minor diﬀerence can be caused by some implementation
details, such as the data structures and caches used.

5.2.3

eALS vs. BPR (sample-based)

Figure 4 plots the performance of BPR with diﬀerent
learning rates4 in each iteration. Note that we only run
eALS for 100 iterations, which are enough for eALS to con-
verge. First, it is clear that BPR’s performance is subjected
4We have also tried other intermediate values of learning
rates, and the ﬁndings are consistent. Thus, to make the
ﬁgure more clear, we only show three selected values.

to the choice of learning rate — Figure 4a and 4b show that
a higher learning rate leads to a faster convergence, while the
ﬁnal accuracy may be suﬀered. Second, we see that eALS
signiﬁcantly outperforms BPR on the Yelp dataset evaluated
by both measures (p < 0.001). For Amazon, eALS obtains
a much higher hit ratio but a lower NDCG score, indicat-
ing that most hits occur at a relatively low ranks for eALS.
Comparing with the performance of other whole-based MF
methods ALS and RCD (Figure 2), we draw the conclusion
that BPR is a weak performer in terms of the prediction
recall, while being a strong performer in terms of the preci-
sion at top ranks. We think BPR’s strength in ranking top
items is due to its optimization objective, which is a pair-
wise ranking function tailored for ranking correct item high.
In contrast, the regression-based objective is not directly op-
timized for ranking; instead, by account for all missing data
in regression, it better predicts user’s preference on uncon-
sumed items, leading to a better recall. This is consistent
with [2]’s ﬁnding in evaluating top-K recommendation.

We notice that BPR shows unusual NDCG spike in early
iterations on the Amazon dataset, however the performance
is unstable and goes down with more iterations. The same
phenomenon was also observed for another gradient descent
method RCD on the same dataset (see Figure 2d). We hy-
pothesize that it might be caused by some regularities in
the data. For example, we ﬁnd some Amazon users review
on a movie multiple times5. In early iterations, BPR ranks
these repeated items high, leading to a high but unstable
NDCG score. There might be other reasons responsible for
this, and we do not further explore here.

5.3 Online Protocol

In the evaluation of online protocol, we hold out the latest
10% interactions as the test set, training all methods on the
remaining 90% data with the best parameter settings evi-
denced by the oﬄine evaluation. We ﬁrst study the number
of online iterations required for eALS to converge. Then we
show how does the weight of new interactions impact the
performance. Lastly, we compare with dynamic MF meth-
ods RCD and BPR in the online learning scenario.

5.3.1 Number of Online Iterations

Figure 6 shows how does eALS’s accuracy change with
number of online iterations. Results at the 0-th iteration
benchmark the performance of the oﬄine trained model, as
no incremental update is performed. First, we can see that
the oﬄine trained model performs very poorly, highlight-

5Due to user’s repeat consumption behaviours, we do not
exclude training items when generating recommend list.

(a) Test # vs. HR

(b) Test # vs. NDCG

(c) Test # vs. HR

(d) Test # vs. NDCG

Figure 5: Performance evolution of eALS and other dynamic MF methods in online learning.

ing the importance of refreshing recommender model for an
online system with dynamic data. Second, we ﬁnd most
performance gain comes from the ﬁrst iteration, and more
iterations do not further improve. This is due to the fact
that only the local features regarding to the new interaction
are updated, and one eALS step on a latent factor can ﬁnd
the optimal solution with others ﬁxed. Thus, one iteration
is enough for eALS to learn from a new interaction incre-
mentally, making eALS very eﬃcient for learning online.

Figure 6: Impact of online iterations on eALS.

We have also investigated number of online iterations re-
quired for baselines RCD and BPR. RCD shows the same
trend that good prediction is obtained in the ﬁrst iteration.
While BPR requires more iterations, usually 5-10 iterations
to get a peak performance and more iterations will adversely
hurt the performance due to the local over-training.

5.3.2 Weight of New Interactions

To evaluate how does the weight of new interactions ef-
fect the online learning algorithms, we also apply the same
weight wnew on RCD. Note that the original RCD paper [4]
does not consider the weight of interactions; we encode wnew
the same way with eALS, revising the RCD learner to opti-
mize the weighted regression function.

Figure 7: Impact of wnew on eALS and RCD in online
learning evaluated by NDCG.

Figure 7 shows the performance evaluated by NDCG (re-
sults of HR show the same trend thus omitted for space).
Setting wnew to 1 signiﬁes that new interaction is assigned a
same weight with the old training interaction. As expected,
with a modest increasing on wnew, the prediction of both

models is gradually improved, demonstrating the usefulness
of strengthening user’s short-term interest. The peak per-
formance is obtained around 4, where eALS shows better
prediction than RCD. Overly increasing wnew will adversely
hurt the performance, admitting the utility of user’s histor-
ical data used in oﬄine training. Overall, this experiment
indicates the importance of balancing user’s short-term and
long-term interest for quality recommendation.

5.3.3 Performance Comparison

With the simulated data stream, we show the performance
evolution with respect to number of test instances in Figure
5. First, eALS consistently outperforms RCD and BPR evi-
denced by both measures, and one-sample paired t-test ver-
iﬁes that all improvements are statistically signiﬁcant with
p < 0.001. BPR betters RCD for Yelp, while underper-
forms for Amazon. Second, we observe the trend that the
performance of dynamic learning ﬁrst decreases, and then
increases before becoming stable. This is caused by the new
users problem — when there are few feedback for a user, the
model can not personalize the user’s preference eﬀectively;
with more feedbacks streaming in, the model can adapt itself
to improve the preference modeling accordingly. To show
this, we further breakdown the results of eALS by number
of past interactions of test user in Figure 8.

Figure 8: Results breakdown of eALS by # of past
interactions of test user. Note: Interaction # > 0
denotes the performance for non-cold-start users.

It is clear that when there are no historical feedback for
a test user (i.e., user cold-start cases), the performance is
very poor — no better than random. After the ﬁrst interac-
tion streams in, the prediction is signiﬁcantly improved; and
with more interactions, the performance is further improved.
This highlights the importance of incorporating instanta-
neous user feedback into the model, especially for cold-start
or sparse users that have few history in training.

6. CONCLUSION AND FUTURE WORK

We study the problem of learning MF models from im-
plicit feedback. In contrast to previous work that applied

a uniform weight on missing data, we propose to weight
missing data based on the popularity of items. To address
the key eﬃciency challenge in optimization, we develop a
new learning algorithm — eALS — which eﬀectively learns
parameters by performing coordinate descent with memo-
ization. For online learning, we devise an incremental up-
date strategy for eALS to adapt dynamic data in real-time.
Experiments with both oﬄine and online protocols demon-
strate promising results. Importantly, our work makes MF
more practical to use for modeling implicit data, along two
dimensions. First, we investigate a new paradigm to deal
with missing data which can easily incorporate prior do-
main knowledge. Second, eALS is embarrassingly parallel,
making it attractive for large-scale industrial deployment.

We plan to study the optimal weighting strategy for online
data as a way to explore user’s short-term interest. Along
the technical line, we explored the element-wise ALS learner
in its basic MF form and solved the eﬃciency challenge in
handling missing data. To make our method more applica-
ble to real-world settings, we plan to encode side information
such as user social contexts [8] and reviews [9] by extending
eALS to more generic models, such as collective factoriza-
tion [11] and Factorization machines [26]. In addition, we
will study binary coding for MF on implicit data, since a
recent advance [32] has shown that discrete latent factors
are beneﬁcial to collaborative ﬁltering for explicit ratings.

The strength of eALS can be applied to other domains,
owing to the universality of factorizing sparse data matrices.
For example, recent advances in natural language process-
ing [16] have shown the connection between neural word em-
beddings and MF on the word–context matrix. This bridge
nicely motivates several proposals to use MF to learn word
embeddings; however, when it comes to handling missing
data, they have either ignored [22] or equally weighted the
missing entries, similar to traditional SVD [16]. It will be in-
teresting to see whether eALS will also improve these tasks.
Acknowledgement
The authors would like to thank the additional discussion
and help from Steﬀen Rendle, Bhargav Kanagal, Immanuel
Bayer, Tao Chen, Ming Gao and Jovian Lin.

7. REFERENCES
[1] D. Coppersmith and S. Winograd. Matrix multiplication

via arithmetic progressions. J. Symb. Comput.,
9(3):251–280, 1990.

[2] P. Cremonesi, Y. Koren, and R. Turrin. Performance of

recommender algorithms on top-n recommendation tasks.
In RecSys 2010, pages 39–46.

[3] A. S. Das, M. Datar, A. Garg, and S. Rajaram. Google

news personalization: Scalable online collaborative ﬁltering.
In WWW 2007, pages 271–280.

[4] R. Devooght, N. Kourtellis, and A. Mantrach. Dynamic
matrix factorization with priors on unknown values. In
KDD 2015, pages 189–198.

[5] E. Diaz-Aviles, L. Drumond, L. Schmidt-Thieme, and

W. Nejdl. Real-time top-n recommendation in social
streams. In RecSys 2012, pages 59–66.

[6] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient

methods for online learning and stochastic optimization. J.
Mach. Learn. Res., 12:2121–2159, 2011.

[7] R. Gemulla, E. Nijkamp, P. J. Haas, and Y. Sismanis.

Large-scale matrix factorization with distributed stochastic
gradient descent. In KDD 2011, pages 69–77.

[8] X. Geng, H. Zhang, J. Bian, and T.-S. Chua. Learning

image and user features for recommendation in social
networks. In ICCV 2015, pages 4274–4282.

[9] X. He, T. Chen, M.-Y. Kan, and X. Chen. Trirank:

Review-aware explainable recommendation by modeling
aspects. In CIKM 2015, pages 1661–1670.

[10] X. He, M. Gao, M.-Y. Kan, Y. Liu, and K. Sugiyama.

Predicting the popularity of web 2.0 items based on user
comments. In SIGIR 2014, pages 233–242.

[11] X. He, M.-Y. Kan, P. Xie, and X. Chen. Comment-based
multi-view clustering of web 2.0 items. In Proc. of WWW
’14, pages 771–782, 2014.

[12] Y. Hu, Y. Koren, and C. Volinsky. Collaborative ﬁltering
for implicit feedback datasets. In ICDM 2008, pages
263–272.

[13] Y. Huang, B. Cui, W. Zhang, J. Jiang, and Y. Xu.

Tencentrec: Real-time stream recommendation in practice.
In SIGMOD 2015, pages 227–238.

[14] Y. Koren. Collaborative ﬁltering with temporal dynamics.

In KDD 2009, pages 447–456.

[15] Y. Koren and R. Bell. Advances in collaborative ﬁltering.
In Recommender systems handbook, pages 145–186.
Springer, 2011.

[16] O. Levy and Y. Goldberg. Neural word embedding as
implicit matrix factorization. In NIPS 2014, pages
2177–2185.

[17] G. Ling, H. Yang, I. King, and M. R. Lyu. Online learning
for collaborative ﬁltering. In IJCNN 2012, pages 1–8.
[18] B. M. Marlin, R. S. Zemel, S. Roweis, and M. Slaney.
Collaborative ﬁltering and the missing at random
assumption. In UAI 2007, pages 267–276.

[19] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and

J. Dean. Distributed representations of words and phrases
and their compositionality. In NIPS 2013, pages 3111–3119.

[20] R. Pan and M. Scholz. Mind the gaps: Weighting the

unknown in large-scale one-class collaborative ﬁltering. In
KDD 2009, pages 667–676.

[21] R. Pan, Y. Zhou, B. Cao, N. Liu, R. Lukose, M. Scholz,
and Q. Yang. One-class collaborative ﬁltering. In ICDM
2008, pages 502–511.

[22] J. Pennington, R. Socher, and C. D. Manning. Glove:

Global vectors for word representation. In EMNLP 2014,
pages 1532–1543.

[23] I. Pil´aszy, D. Zibriczky, and D. Tikk. Fast als-based matrix
factorization for explicit and implicit feedback datasets. In
RecSys 2010, pages 71–78.

[24] S. Rendle and C. Freudenthaler. Improving pairwise

learning for item recommendation from implicit feedback.
In WSDM 2014, pages 273–282.

[25] S. Rendle, C. Freudenthaler, Z. Gantner, and

L. Schmidt-Thieme. Bpr: Bayesian personalized ranking
from implicit feedback. In UAI 2009, pages 452–461.

[26] S. Rendle, Z. Gantner, C. Freudenthaler, and

L. Schmidt-Thieme. Fast context-aware recommendations
with factorization machines. In SIGIR 2011, pages 635–644.

[27] S. Rendle and L. Schmidt-Thieme. Online-updating

regularized kernel matrix factorization models for large
scale recommender systems. In RecSys 2008, pages 251–258.

[28] P. Richt´arik and M. Tak´aˇc. Iteration complexity of

randomized block-coordinate descent methods for
minimizing a composite function. Math. Prog., 2014.
[29] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips,

D. Ebner, V. Chaudhary, and M. Young. Machine learning:
The high interest credit card of technical debt. In SE4ML
(NIPS 2014 Workshop), 2014.

[30] H. Steck. Training and testing of recommender systems on
data missing not at random. In KDD 2010, pages 713–722.

[31] M. Volkovs and G. W. Yu. Eﬀective latent models for

binary feedback in recommender systems. In SIGIR 2015,
pages 313–322.

[32] H. Zhang, F. Shen, W. Liu, X. He, H. Luan, and T.-S.
Chua. Discrete collaborative ﬁltering. In SIGIR 2016.

Fast Matrix Factorization for Online Recommendation
with Implicit Feedback∗

Xiangnan He

Hanwang Zhang

Min-Yen Kan

Tat-Seng Chua

School of Computing, National University of Singapore
{xiangnan, hanwang, kanmy, chuats}@comp.nus.edu.sg

7
1
0
2
 
g
u
A
 
6
1
 
 
]

R

I
.
s
c
[
 
 
1
v
4
2
0
5
0
.
8
0
7
1
:
v
i
X
r
a

ABSTRACT
This paper contributes improvements on both the eﬀective-
ness and eﬃciency of Matrix Factorization (MF) methods
for implicit feedback. We highlight two critical issues of ex-
isting works. First, due to the large space of unobserved
feedback, most existing works resort to assign a uniform
weight to the missing data to reduce computational com-
plexity. However, such a uniform assumption is invalid in
real-world settings. Second, most methods are also designed
in an oﬄine setting and fail to keep up with the dynamic
nature of online data.

We address the above two issues in learning MF models
from implicit feedback. We ﬁrst propose to weight the miss-
ing data based on item popularity, which is more eﬀective
and ﬂexible than the uniform-weight assumption. However,
such a non-uniform weighting poses eﬃciency challenge in
learning the model. To address this, we speciﬁcally de-
sign a new learning algorithm based on the element-wise
Alternating Least Squares (eALS) technique, for eﬃciently
optimizing a MF model with variably-weighted missing data.
We exploit this eﬃciency to then seamlessly devise an incre-
mental update strategy that instantly refreshes a MF model
given new feedback. Through comprehensive experiments
on two public datasets in both oﬄine and online protocols,
we show that our eALS method consistently outperforms
state-of-the-art implicit MF methods. Our implementation
is available at https://github.com/hexiangnan/sigir16-eals.

Keywords
Matrix Factorization, Implicit Feedback, Item Recommen-
dation, Online Learning, ALS, Coordinate Descent

1.

INTRODUCTION

User personalization has become prevalent in modern rec-
ommender system. It helps to capture users’ individualized

∗NExT research is supported by the National Research
Foundation, Prime Minister’s Oﬃce, Singapore under its
IRC@SG Funding Initiative.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’16, July 17-21, 2016, Pisa, Italy
c(cid:13) 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2911489

preferences and has been shown to increase both satisfac-
tion for users and revenue for content providers. Among
its various methods, matrix factorization (MF) is the most
popular and eﬀective technique that characterizes users and
items by vectors of latent factors [15, 32]. Early work on
MF algorithms for recommendation [14, 27] have largely fo-
cused on explicit feedback, where users’ ratings that directly
reﬂect their preference on items are provided. These works
formulated recommendation as a rating prediction problem
for which the large volume of unobserved ratings (i.e., miss-
ing data) are assumed to be extraneous for modeling user
preference [12]. This greatly reduces the modeling workload,
and many sophisticated methods have been devised, such as
SVD++ [15] and timeSVD [14].

However, explicit ratings are not always available in many
applications; more often, users interact with items through
implicit feedback, e.g., users’ video viewing and prod-
uct purchase history. Compared to explicit ratings, implicit
feedback is easier to collect for content providers, but more
challenging to utilize due to the natural scarcity of negative
feedback.
It has been shown that modeling only the ob-
served, positive feedback results in biased representations in
user proﬁles [4, 12]; e.g., Marlin et al. [18] ﬁnds that users
listen to music they expect to like and avoid the genres they
dislike, leading to a severe bias in the observed data.

To solve the problem of lacking negative feedback (also
known as the one-class problem [21]), a popular solution
is to model all the missing data as negative feedback [12].
However, this adversely degrades the learning eﬃciency due
to the full consideration of both observed and missing data.
More importantly, the low eﬃciency makes it even more dif-
ﬁcult to deploy implicit MF method online [29]. In practical
recommender systems where new users, items and interac-
tions are continuously streaming in, it is crucial to refresh
the underlying model in real-time to best serve users. In this
work, we concern the above two challenging problems of the
MF method — implicit feedback and online learning. We
note that we are not the ﬁrst to consider both aspects for
MF, as a recent work by Devooght et al. [4] has proposed an
eﬃcient implicit MF method for learning with dynamic data.
However, we argue that Devooght’s method [4] models miss-
ing data in an unrealistic, suboptimal way. Speciﬁcally, it
assigns a uniform weight to the missing data, assuming that
the missing entries are equally likely to be negative feedback.
However, such an assumption limits model’s ﬁdelity and ﬂex-
ibility for real applications. For example, content providers
usually know which items have been frequently featured to
users but seldom clicked; such items are more likely to be

true negative assessments and should be weighted higher
than others. In addition, Devooght’s method learns param-
eters through gradient descent, requiring an expensive line
search to determine the best learning rate at each step.

We propose a new MF method aimed at learning from im-
plicit feedback eﬀectively while satisfying the requirement of
online learning. We develop a new learning algorithm that
eﬃciently optimizes the implicit MF model without impos-
ing a uniform-weight restriction on missing data.
In par-
ticular, we assign the weight of missing data based on the
popularity of items, which is arguably more eﬀective than
the previous methods [4, 12, 23, 30, 31] that are limited by
the uniformity assumption. Our eALS algorithm is fast in
accounting for missing data — analytically K times faster
than ALS [12] where K denotes number of latent factors
— the same time complexity with the recent dynamic MF
solution [4]. This level of eﬃciency makes eALS suitable
for learning online, for which we develop an incremental
update strategy that instantly refreshes model parameters
given new incoming data. Another key advantage of eALS
is that it works without learning rate, bypassing the well-
known diﬃculty for tuning gradient descent methods such
as [4] and Stochastic Gradient Descent (SGD) [25].
We summarize our key contributions as follows.
1. We propose an item popularity-aware weighting scheme
on the full missing data that eﬀectively tailors the MF
model for learning from implicit feedback.

2. We develop a new algorithm for learning model pa-
rameters eﬃciently and devise an incremental update
strategy to support real-time online learning.

3. We conduct extensive experiments with both oﬄine
and online protocols on two real-world datasets, show-
ing that our method consistently outperforms state-of-
the-art implicit MF methods.

2. RELATED WORK

Handling missing data is obligatory for learning from im-
plicit data due to the lack of negative feedback. To this end,
two strategies have been proposed — sample based learn-
ing [21, 25] that samples negative instances from missing
data, or whole-data based learning [12, 30] that treats
all missing data as negative. Both methods have pros and
cons: sample-based methods are more eﬃcient by reduc-
ing negative examples in training, but risk decreasing the
model’s predictiveness; whole-based methods model the full
data with a potentially higher coverage, but ineﬃciency can
be an issue. To retain model’s ﬁdelity, we persist in the
whole-data based learning, developing a fast ALS-based al-
gorithm to resolve the ineﬃciency issue.

For existing whole-data based methods [4, 12, 23, 30, 31],
one major limitation is in the uniform weighting on missing
entries, which favors algorithm’s eﬃciency but limits model’s
ﬂexibility and extensibility. The only works that have con-
sidered non-uniform weighting are from Pan et al. [20, 21];
however their cubic time complexity w.r.t. K makes it un-
suitable to run on large-scale data [23], where a large number
of factors needs to be considered to gain improved perfor-
mance [31].

To optimize MF, various learners have been investigated,
including SGD [14, 25], Coordinate Descent (CD) [4, 32],
and Markov Chain Monto Carlo (MCMC) [26]. SGD is the
most popular one owing to the ease of derivation, however,

it is unsuitable for whole-data based MF [12] due to the
large amount of training instances (the full user–item inter-
action matrix is considered). ALS can be seen as an in-
stantiation of CD and has been widely used to solve the
whole-based MF [12, 20, 21, 30]; however, its ineﬃciency is
the main obstacle for practical use [23, 31]. To resolve this,
[23] describes an approximate solution to ALS. Recently, [4]
employs the Randomized block Coordinate Descent (RCD)
learner [28], reducing the complexity and applying it to a dy-
namic scenario. Similarly, [31] enriches the implicit feedback
matrix with neighbor-based similarly, followed by applying
unweighted SVD. Distinct from previous works, we propose
an eﬃcient element-wise ALS solution for the whole-data
based MF with non-uniform missing data, which has never
been studied before.

Another important aspect for practical recommender sys-
tem lies in handling the dynamic nature of incoming data,
for which timeliness is a key consideration. As it is pro-
hibitive to retrain the full model online, various works have
developed incremental learning strategies for neighbor-based
[13], graph-based [9], probabilistic [3] and MF [4, 5, 17, 27]
methods. For MF, diﬀerent learners have been studied for
online updating, including SGD [5, 27], RCD [4] and dual-
averaging [17]. To our knowledge, this work is the ﬁrst at-
tempt to exploit the ALS technique for online learning.

3. PRELIMINARIES

We ﬁrst introduce the whole-data based MF method for
learning from implicit data, highlighting the ineﬃciency is-
sue of the conventional ALS solution [12, 21]. Then we
describe eALS, an element-wise ALS learner [26] that can
reduce the time complexity to linearity w.r.t. number of
factors. Although the learner is generic in optimizing MF
with all kinds of weighting strategies, the form introduced
is costly in accounting for all missing data and is thus unre-
alistic for practical use. This defect motivates us to further
develop eALS to make it suitable for learning from implicit
feedback (details in Section 4.2).

3.1 MF Method for Implicit Feedback

We start by introducing some basic notation. For a user–
item interaction matrix R ∈ RM ×N , M and N denote the
number of users and items, respectively; R denotes the set
of user–item pairs whose values are non-zero. We reserve the
index u to denote a user and i to denote an item. Vector pu
denotes the latent feature vector for u, and set Ru denotes
the set of items that are interacted by u; similar notations
for qi and Ri. Matrices P ∈ RM ×K and Q ∈ RN ×K denote
the latent factor matrix for users and items.

Matrix factorization maps both users and items into a
joint latent feature space of K dimension such that interac-
tions are modeled as inner products in that space. Mathe-
matically, each entry rui of R is estimated as:

ˆrui =< pu, qi >= pT

u qi.

(1)

The item recommendation problem is formulated as estimat-
ing the scoring function ˆrui, which is used to rank items.
Note that this basic model subsumes the biased MF [14],
commonly used in modeling explicit ratings:

ˆrui = bu + bi+ < pB

u , qB

i >,

where bu (bi) captures the bias of user u (item i) in giving
(receiving) ratings. To recover it, set pu ← [pB
u , bu, 1] and

qi ← [qB
i , 1, bi]. As such, we adopt the basic MF model
to make notations simple and also to enable a fair compari-
son with baselines [4, 12] that also complied with the basic
model.

To learn model parameters, Hu et al. [12] introduced a
weighted regression function, which associates a conﬁdence
to each prediction in the implicit feedback matrix R:

J =

M
(cid:88)

N
(cid:88)

u=1

i=1

wui(rui − ˆrui)2 + λ(

||pu||2 +

||qi||2), (2)

M
(cid:88)

u=1

N
(cid:88)

i=1

where wui denotes the weight of entry rui and we use W =
[wui]M ×N to represent the weight matrix. λ controls the
strength of regularization, which is usually an L2 norm to
prevent overﬁtting. Note that in implicit feedback learning,
missing entries are usually assigned to a zero rui value but
non-zero wui weight, both crucial to performance.

3.2 Optimization by ALS

Alternating Least Square (ALS) is a popular approach
to optimize regression models such as MF and graph reg-
ularization [10]. It works by iteratively optimizing one pa-
rameter, while leaving the others ﬁxed. The prerequisite of
ALS is that the optimization sub-problem can be analyti-
cally solved. Here, we describe how Hu’s work [12] solves
this problem.

First, minimizing J with respect to user latent vector pu

is equivalent to minimizing:

Ju = ||Wu(ru − Qpu)||2 + λ||pu||2,
where Wu is a N × N diagonal matrix with W u
minimum is where the ﬁrst-order derivative is 0:

ii = wui. The

∂Ju
∂pu

= 2QT WuQpu − 2QT Wuru + 2λpu = 0

(3)

⇒ pu = (QT WuQ + λI)−1QT Wuru,

where I denotes the identity matrix. This analytical solution
is also known as the ridge regression [23]. Following the same
process, we can get the solution for qi.

3.2.1 Efﬁciency Issue with ALS

As we can see, in order to update a latent vector, in-
verting a K × K matrix is inevitable. Matrix inversion
is an expensive operation, usually assumed O(K 3) in time
complexity [12]. As such, updating one user latent vector
takes time O(K 3 + N K 2). Thus, the overall time complex-
ity of one iteration that updates all model parameters once
is O((M + N )K 3 + M N K 2). Clearly, this high complexity
makes the algorithm impractical to run on large-scale data,
where there can be millions of users and items and billions
of interactions.

Speed-up with Uniform Weighting. To reduce the
high time complexity, Hu et al. [12] applied a uniform weight
to missing entries; i.e., assuming that all zero entries in R
have a same weight w0. Through this simpliﬁcation, they
can speed up the computation with memoization:

QT WuQ = w0QT Q + QT (Wu − W0)Q,

(4)

where W0 is a diagonal matrix that each diagonal element is
w0. As QT Q is independent of u, it can be pre-computed for
updating all user latent vectors. Considering the fact that
Wu − W0 only has |Ru| non-zero entries, we can compute

Eq. (4) in O(|Ru|K 2) time. Thus, the time complexity of
ALS is reduced to O((M + N )K 3 + |R|K 2).

Even so, we argue that the O((M + N )K 3) term can be
a major cost when (M + N )K ≥ |R|.
In addition, the
O(|R|K 2) part is still much higher than in SGD [25], which
only requires O(|R|K) time. As a result, even with the ac-
celeration, ALS is still prohibitive for running on large data,
where large K is crucial as it can lead to better generaliz-
ability and thus better prediction performance. Moreover,
the uniform weighting assumption is usually invalid in real
applications and adversely degrades model’s predictiveness.
This thus motivates us to design an eﬃcient implicit MF
method not subject to uniform-weights.

3.3 Generic Element-wise ALS Learner

The bottleneck of the previous ALS solution lies in the ma-
trix inversion operation, which is due to the design that up-
dates the latent vector for a user (item) as a whole. As such,
it is natural to optimize parameters at the element level —
optimizing each coordinate of the latent vector, while leav-
ing the others ﬁxed [26]. To achieve this, we ﬁrst get the
derivative of objective function Eq. (2) with respect to puf :

∂J
∂puf

N
(cid:88)

i=1

= −2

(rui − ˆrf

ui)wuiqif + 2puf

wuiq2

if + 2λpuf ,

N
(cid:88)

i=1

where ˆrf
ui = ˆrui − puf qif , i.e., the prediction without the
component of latent factor f . By setting this derivative to
0, we obtain the solution of puf :

Similarly, we can get the solver for an item latent factor:

puf =

(cid:80)N

i=1(rui − ˆrf
(cid:80)N
i=1 wuiq2

ui)wuiqif
if + λ

.

qif =

(cid:80)M

u=1(rui − ˆrf
(cid:80)M
i=1 wuip2

ui)wuipuf
uf + λ

.

(5)

(6)

Given the closed-form solution that optimizes one param-
eter with other ﬁxed, the algorithm iteratively executes it
for all model parameters until a joint optimum is reached.
Due to the non-convexity of the objective function, critical
points where gradients vanish can be local minima.

Time Complexity. As can be seen, by performing opti-
mization at the element level, the expensive matrix inversion
can be avoided. A raw implementation takes O(M N K 2)
time for one iteration, directly speeding up ALS by eliminat-
ing the O(K 3) term. Moreover, by pre-computing ˆrui [26],
we can calculate ˆrf
ui in O(1) time rather than O(K). As
such, the complexity can be further reduced to O(M N K),
which is the same magnitude with evaluating all the user–
item predictions.

4. OUR IMPLICIT MF METHOD

We ﬁrst propose an item-oriented weighting scheme on
the missing data, and follow with a popularity-aware weight-
ing strategy, which is arguably more eﬀective than the uni-
form weighting for the recommendation task. Then, we de-
velop a fast eALS algorithm to optimize the objective func-
tion that signiﬁcantly reduces learning complexity compar-
ing with the conventional ALS [12] and generic element-wise
ALS learner [26]. Lastly, we discuss how to adjust the learn-
ing algorithm for real-time online learning.

(7)

the observed data part:

4.1

Item-Oriented Weighting on Missing Data
Due to the large space of items, the missing entries for a
user are a mixture of negative and unknown feedback. In
specifying the weight wui of missing entries, it is desired to
assign a higher weight to the negative feedback. However, it
is a well-known diﬃculty to diﬀerentiate the two cases. In
addition, as the interaction matrix R is usually large and
sparse, it will be too consuming to store each zero entry an
individualized weight. To this end, existing works [4, 12,
23, 30, 31] have applied a simple uniform weight on missing
entries, which are, however, suboptimal and non-extendable
for real applications.

Considering the ease of content providers in accessing neg-
ative information of the item side (e.g., which items have
been promoted to users but receive little interaction), we
believe it is more realistic to weight missing data based on
some item property. To capture this, we devise a more ﬁne-
grained objective function as follows:

(cid:88)

L =

wui(rui − ˆrui)2 +

(u,i)∈R

M
(cid:88)

u=1

+ λ(

||pu||2 +

||qi||2),

N
(cid:88)

i=1

M
(cid:88)

(cid:88)

u=1

i /∈Ru

ci ˆr2
ui

where ci denotes the conﬁdence that item i missed by users
is a true negative assessment, which can serve as a means to
encode domain knowledge from practitioners. It is clear that
the ﬁrst term denotes the prediction error of the observed
entries, which has been widely adopted in modeling explicit
ratings [15, 27]. The second term accounts for the miss-
ing data, which acts as the role of negative instances and is
crucial for recommendation from implicit feedback [12, 25].
Next, we present a domain-independent strategy to deter-
mine ci by leveraging a ubiquitous feature of modern Web
2.0 systems.

4.1.1 Popularity-aware Weighting Strategy

Existing visual interfaces of many Web 2.0 systems show-
case popular items in their recommendations. All other fac-
tors being equal, popular items are more likely to be known
by users in general [10], and thus it is reasonable to think
that a miss on a popular item is more probable to be truly
irrelevant (as opposed to unknown) to the user. To account
for this eﬀect, we parametrize ci based on item’s popularity:

ci = c0

f α
i
j=1 f α
j

,

(cid:80)N

(8)

where fi denotes the popularity of item i, given by its fre-
quency in the implicit feedback data: |Ri|/ (cid:80)N
j=1 |Rj|, and
c0 determines the overall weight of missing data. Exponent
α controls the signiﬁcance level of popular items over un-
popular ones — when α > 1 the weights of popular items
are promoted to strengthen the diﬀerence against unpop-
ular ones; while setting α within the lower range of (0, 1)
suppresses the weight of popular items and has a smoothing
eﬀect. We empirically ﬁnd α = 0.5 usually leads to good
results. Note that the uniform weighting is a special case by
setting α to 0 with w0 = c0/N .

Relationship to Negative Sampling. Our proposed
popularity-aware weighting strategy has the same intuition
with Rendle’s popularity-based oversampling [24] for learn-
ing BPR, which basically samples popular items as nega-

tive feedback with a higher probability. However, [24] em-
pirically shows the oversampling method underperforms the
basic uniform sampler. We suspect the reason comes from
the SGD learner, which will result in more gradient steps
on popular items, due to oversampling. As a result, popular
items may be over-trained locally at the expense of less pop-
ular items which would then be under-trained. To resolve
this, tricks like subsampling frequent items [19] and adaptive
learning rates like Adagrad [6] have been adopted in other
domains. As the focus of this paper is on whole-data based
implicit MF, we do not further explore the details of SGD. It
is worth pointing out that our proposed eALS learner avoids
these learning issues by an exact optimization on each model
parameter.

4.2 Fast eALS Learning Algorithm

We can speed up learning by avoiding the massive re-
peated computations introduced by the weighted missing
data. We detail the derivation process for puf ; where the
counterpart for qif is achieved likewise.

First, we rewrite the puf update rule Eq. (5) by separating

puf =

(cid:80)

i∈Ru
(cid:80)

(rui − ˆrf

ui)wuiqif − (cid:80)
if + (cid:80)

i /∈Ru

ˆrf
uiciqif
if + λ

i /∈Ru
ciq2

wuiq2

.

i∈Ru

Clearly, the computational bottleneck lies in the summa-
tion over missing data portion, which requires a traversal of
the whole negative space. We ﬁrst focus on the numerator:

(cid:88)

i /∈Ru

ˆrf
uiciqif =

ciqif

pukqik −

ˆrf
uiciqif

N
(cid:88)

i=1

(cid:88)

k(cid:54)=f

(cid:88)

=

puk

N
(cid:88)

k(cid:54)=f

i=1

(cid:88)

i∈Ru

(cid:88)

i∈Ru

ciqif qik −

ˆrf
uiciqif .

(9)

By this reformulation, we can see that the major computa-
tion — the (cid:80)N
i=1 ciqif qik term that iterates over all items
— is independent of u. However, a na¨ıve implementation
repeatedly computes it unnecessarily, when updating the la-
tent factors for diﬀerent users. Clearly, we can achieve a
signiﬁcant speed-up by memoizing it.

We deﬁne the Sq cache as Sq = (cid:80)N

i , which can
be pre-computed and used in updating the latent factors for
all users. Then, Eq. (9) can be evaluated as:

i=1 ciqiqT

(cid:88)

i /∈Ru

ˆrf
uiciqif =

puksq

f k −

ˆrf
uiciqif ,

(10)

(cid:88)

k(cid:54)=f

(cid:88)

i∈Ru

which can be done in O(K + |Ru|) time.

Similarly, we can apply the cache to speed up the calcu-

lation of denominator:

(cid:88)

i /∈Ru

N
(cid:88)

i=1

(cid:88)

i∈Ru

ciq2

if =

ciq2

if −

ciq2

if = sq

f f −

ciq2

if . (11)

(cid:88)

i∈Ru

To summarize the above memoization strategy, we give

the update rule for puf with the use of Sq cache:

puf =

(cid:80)

i∈Ru

[wuirui − (wui − ci)ˆrf
(wui − ci)q2

(cid:80)

ui]qif − (cid:80)
if + sq
f f + λ

i∈Ru

k(cid:54)=f puksq

kf

.

(12)

Algorithm 1: Fast eALS Learning algorithm.

Input: R, K, λ, W and item conﬁdence vector c;
Output: Latent feature matrix P and Q;

1 Randomly initialize P and Q ;
2 for (u, i)∈ R do ˆrui ← Eq. (1) ;
3 while Stopping criteria is not met do

(cid:46) O(|R|K)

// Update user factors
Sq = (cid:80)N
i=1 ciqiqT
;
for u ← 1 to M do

i

(cid:46) O(M K 2)
(cid:46) O(M K 2 + |R|K)

for f ← 1 to K do
for i ∈ Ru do ˆrf
puf ← Eq. (12) ;
for i ∈ Ru do ˆrui ← ˆrf

ui ← ˆrui − puf qif ;

(cid:46) O(K + |Ru|)

ui + puf qif ;

end

end
// Update item factors
Sp ← PT P ;
for i ← 1 to N do

(cid:46) O(N K 2)
(cid:46) O(N K 2 + |R|K)

for f ← 1 to K do
for u ∈ Ri do ˆrf
qif ← Eq. (13) ;
for u ∈ Ri do ˆrui ← ˆrf

ui ← ˆrui − puf qif ;

(cid:46) O(K + |Ri|)

ui + puf qif ;

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

end

end

19
20 end
21 return P and Q

Table 1: Time complexity of implicit MF methods.

Time Complexity
Method
O((M + N )K 3 + |R|K 2)
ALS (Hu et al.[12])
O(|R|K)
BPR (Rendle et al.[25])
O(K 3 + (M + N )K 2 + |R|K)
IALS1 (Pil´aszy et al.[23])
ii-SVD (Volkovs et al.[31]) O((M + N )K 2 + M N log K)
RCD (Devooght et al.[4])
eALS (Algorithm 1)

O((M + N )K 2 + |R|K)
O((M + N )K 2 + |R|K)

|R| denotes the number of non-zeros in user–item matrix R.

Similarly, we can derive the update rule for qif :

qif =

(cid:80)

u∈Ri

[wuirui − (wui − ci)ˆrf
(wui − ci)p2

(cid:80)

(cid:80)

ui]puf − ci
uf + cisp

f f + λ

u∈Ri

k(cid:54)=f qiksp

kf

,

(13)
kf denotes the (k, f )th element of the Sp cache, de-

where sp
ﬁned as Sp = PT P.

Algorithm 1 summarizes the accelerated algorithm for our
element-wise ALS learner, or eALS. For convergence, one
can either monitor the value of objective function on train-
ing set or check the prediction performance on a hold-out
validation data.

4.2.1 Discussion

Time Complexity.

In Algorithm 1, updating a user
latent factor takes O(K + |Ru|) time. Thus, one eALS iter-
ation takes O((M +N )K 2 +|R|K) time. Table 1 summarizes
the time complexity (of one iteration or epoch) of other MF
algorithms that are designed for implicit feedback.

Comparing with the vector-wise ALS [12, 21], our element-
wise ALS learner is K times faster. In addition, our pro-
posed eALS has the same time complexity with RCD [4],

being faster than ii-SVD [31], another recent solution. RCD
is a state-of-the-art learner for whole-data based MF, which
performs a gradient descent step on a randomly chosen la-
tent vector. Since it requires a good learning rate, the work
[4] adaptively determines it by a line search in each gradient
step, which essentially chooses the learning rate that leads to
the steepest descent among pre-deﬁned candidates. A major
advantage of eALS has over RCD is that it avoids the need
for a learning rate by an exact optimization in each param-
eter update, arguably more eﬀective and easier to use than
RCD. The most eﬃcient algorithm is BPR, which applies
the SGD learner on sampled, partial missing data only.

Computing the Objective Function. Evaluating the
objective function is important to check the convergence of
iterations and also to verify the correctness of implementa-
tion. A direct calculation takes O(M N K) time, requiring a
full estimation on the R matrix. Fortunately, with the item-
oriented weighting, we can similarly exploit the sparseness
of R for acceleration. To achieve this, we reformulate the
loss of the missing data part that causes the major cost:

M
(cid:88)

(cid:88)

u=1

i /∈Ru

ci ˆr2

ui =

u Sqpu −
pT

M
(cid:88)

u=1

(cid:88)

ci ˆr2

ui.

(u,i)∈R

(14)

By reusing Sq and the prediction cache ˆrui, we can calculate
the objective function in O(|R| + M K 2) time, much faster
than with direct calculation.

Parallel Learning. The iterations of eALS can be easily
parallelized. First, computing the S caches (line 4 and 12)
is the standard matrix multiplication operation, for which
modern matrix toolkits provide very eﬃcient and parallelized
implementation. Second, in updating the latent vectors for
diﬀerent users (line 5-11), the shared parameters are either
independent with each other (i.e., ˆrui) or remaining un-
changed (i.e., Sq). This nice property means that an exact
parallel solution can be obtained by separating the updates
by users; that is, letting diﬀerent workers update the model
parameters for disjoint sets of users. The same parallelism
can also be achieved in updating item latent vectors.

This is an advantage over the commonly-used SGD learner,
which is a stochastic method that updates model parame-
ters given a training instance.
In SGD, diﬀerent gradient
steps can inﬂuence with each other and there is no exact
way to separate the updates for workers. Thus, sophisti-
cated strategies are required to control the possible losses
introduced by parallelization [7]. Our proposed eALS solu-
tion optimizes by coordinate descent where in each step a
dedicated parameter is updated, making the algorithm em-
barrassingly parallel without any approximate loss.

4.3 Online Update

In practice, after a recommender model is trained oﬄine
on historical data, it will be used online and will need to
adapt to best serve users. Here, we consider the online
learning scenario that refreshes model parameters given a
new user–item interaction.

Incremental Updating. Let ˆP and ˆQ denote the model
parameters learnt from oﬄine training, and (u, i) denotes
the new interaction streamed in. To approximate the model
parameters in accounting for the new interaction, we per-
form optimization steps for pu and qi only. The underlying
assumption is that the new interaction should not change ˆP
and ˆQ too much from a global perspective, while it should

Algorithm 2: Online Incremental Updates for eALS.
Input: ˆP, ˆQ, new interaction (u, i) and its weight wnew
Output: Refreshed parameters P and Q;

1 P ← ˆP; Q ← ˆQ; ;
2 If u is a new user do Randomly initialize pu ;
3 If i is a new item do Randomly initialize qi ;
4 rui ← 1;
5 while Stopping criteria is not met do

ˆrui ← Eq. (1); wui ← wnew;

6

7

8

// Line 6-10 of Algorithm 1
update user(u); ;
update cache(u, Sp); ;
// Line 14-18 of Algorithm 1
update item(i); ;
update cache(i, Sq); ;

(cid:46) O(K 2 + |Ru|K)
(cid:46) O(K 2)

(cid:46) O(K 2 + |Ri|K)
(cid:46) O(K 2)

9
10 end
11 return P and Q

change the local features for u and i signiﬁcantly. Particu-
larly, when u is a new user, executing the local updates will
force pu close to qi, which meets the expectation of latent
factor model. The new item case is similar.

Algorithm 2 summarizes the incremental learning strat-
egy for eALS. For the stopping criteria, our empirical study
shows that one iteration is usually suﬃcient to get good re-
sults. Moreover, it is important to note that after updating
a latent vector, we need to update the S cache accordingly.
Weight of New Interactions. In an online system, new
interactions are more reﬂective of a user’s short-term inter-
est. Comparing to the historical interactions used in oﬄine
training, fresh data should be assigned a higher weight for
predicting user’s future action. We assign a weight wnew to
each new interaction (line 4 of Algorithm 2) as a tunable pa-
rameter. Later in Section 5.3, we investigate how the setting
of this parameter impacts online learning performance.

Time Complexity. The incremental update for a new
interaction (u, i) can be done in O(K 2+(|Ru|+|Ri|)K) time.
It is worth noting that the cost depends on the number of
observed interactions for u and i, while being independent
with number of total interactions, users and items. This lo-
calized complexity make the online learning algorithm suit-
able to deployment in industrial use, as the complex software
stack that deals with data dependencies [29] can be avoided.

5. EXPERIMENTS

We begin by introducing the experimental settings. Then
we perform an empirical study with the traditional oﬄine
protocol, followed by a more realistic online protocol.

5.1 Experimental Settings

Datasets. We evaluate on two publicly accessible datasets:
Yelp1 and Amazon Movies2. We transform the review dataset
into implicit data, where each entry is marked as 0/1 indi-
cating whether the user reviewed the item. Since the high
sparsity of the original datasets makes it diﬃcult to evaluate
recommendation algorithms (e.g., over half users have only
one review), we follow the common practice [25] to ﬁlter out

1We used the Yelp Challenge dataset downloaded on Octo-
ber 2015 that contained 1.6 million reviews: http://www.yelp.
com/dataset challenge
2

http://snap.stanford.edu/data/web-Amazon-links.html

Table 2: Statistics of the evaluation datasets.
Dataset Review# Item# User# Sparsity
Yelp
Amazon

731,671
5,020,705

99.89%
99.94 %

25,677
117,176

25,815
75,389

users and items with less than 10 interactions. Table 2 sum-
marizes the statistics of the ﬁltered datasets.

Methodology. We evaluate using two protocols:
- Oﬄine Protocol. We adopt the leave-one-out evalu-
ation, where the latest interaction of each user is held out
for prediction and the models are trained on the remain-
ing data. Although it is a widely used evaluation protocol
in the literature [9, 25], we point out that it is an artiﬁ-
cial split that does not correspond to the real recommenda-
tion scenario. In addition, the new users problem is averted
in this evaluation, as each test user has a training history.
Thus this protocol only evaluates an algorithm’s capability
in providing one-shot recommendation for existing users by
leveraging the static history data.

- Online Protocol. To create a more realistic recom-
mendation scenario, we simulate the dynamic data stream.
We ﬁrst sort all interactions in chronological order, training
models on the ﬁrst 90% of the interactions and holding out
the last 10% for testing. In the testing phase, given a test in-
teraction (i.e., a user–item pair) from the hold-out data, the
model ﬁrst recommends a ranked list of items to the user;
the performance is judged based on the ranked list. Then
the test interaction is fed into the model for an incremental
update. Note that with the global split by time, 14% and
57% test interactions are from new users for the Yelp and
Amazon dataset, respectively. Overall this protocol evalu-
ates an online learning algorithm’s eﬀectiveness in digesting
the dynamic new data.

To assess the ranked list with the ground-truth (GT) item
that user actually consumed, we adopt Hit Ratio (HR) and
Normalized Discounted Cumulative Gain (NDCG). We trun-
cate the ranked list at 100 for both metrics. HR measures
whether the ground truth item is present on the ranked list,
while NDCG accounts for the position of hit [9]. We report
the score averaged by all test interactions.

Baselines. We compare with the following methods:
- ALS [12]. This is the conventional ALS method that
optimizes the whole-data based MF. Due to the high time
complexity, this method is infeasible in a real-time dynamic
updating scenario, so we only evaluate it with the oﬄine
protocol.

- RCD [4]. This is the state-of-the-art implicit MF method
that has the same time complexity with eALS and is suit-
able for online learning. For the line search parameters, we
use the suggested values in the authors’ implementation3.

- BPR [25]. This is a sample-based method that opti-
mizes the pair-wise ranking between the positive and nega-
tive samples. It learns by SGD, which can be adjusted to
online incremental learning by [27]. We use a ﬁxed learning
rate, varying it and reporting the best performance.

Parameter Settings. For the weight of observed in-
teractions, we set it uniformly as 1, a default setting by
previous works [4, 25]. For regularization, we set λ as 0.01
for all methods for a fair comparison. All methods are im-
plemented in Java and running on the same machine (Intel

3

https://github.com/rdevooght/MF-with-prior-and-updates

(a) eALS vs. c0 (α = 0)

(b) eALS vs. α (c0 = 512)

(c) eALS vs. c0 (α = 0)

(d) eALS vs. α (c0 = 64)

Figure 1: Impact of weighting parameters c0 and α on eALS’s performance evaluated by oﬄine protocol.

(a) Iterations vs. HR

(b) Iterations vs. NDCG

(c) Iterations vs. HR

(d) Iterations vs. NDCG

Figure 2: Prediction accuracy of three whole-data based MF methods in each iteration (K = 128).

Xeon 2.67GHz CPU and 24GB RAM) in a single-thread for
a fair comparison on eﬃciency. As the ﬁndings are consis-
tent across the number of factors K, without any particular
outlier, we only show the results of K = 128, a relatively
large number that maintains good accuracy.

5.2 Ofﬂine Protocol

We ﬁrst study how does the weighting scheme on missing
data impact eALS’s performance. Then we compare with
the whole-data based implicit MF methods ALS and RCD,
as well as the sample-based ranking method BPR.

5.2.1 Weight of Missing Data

In Section 4.1, we propose an item popularity-aware weight-
ing strategy, which has two parameters: c0 determines the
overall weight of missing data and α controls the weight dis-
tribution. First, we set a uniform weight distribution (i.e.,
α = 0), varying c0 to study how does the weight of miss-
ing data impact the performance. For Yelp (Figure 1a),
the peak performance is achieved when c0 is around 512,
corresponding to that the weight of each zero entry is 0.02
(w0 = c0/N ); similarly for Amazon (Figure 1c), the optimal
c0 is around 64, corresponding to w0 = 0.0001. When c0
becomes smaller (where w0 is close to 0), the performance
degrades signiﬁcantly. This highlights the necessity of ac-
counting for the missing data when modeling implicit feed-
back for item recommendation. Moreover, when c0 is set
too large, the performance also suﬀers. Based on this ob-
servation, we believe the traditional SVD technique [2] that
treats all entries equally weighted will be suboptimal here.
Then, we set c0 to the best value (in the case of α =
0), varying α to check the performance change. As can be
seen from Figure 1b and 1d, the performance of eALS is
gradually improved with the increase of α, and the best
result is reached around 0.4. We further conducted the one-
sample paired t-test, verifying that the improvements are
statistically signiﬁcant (p-value < 0.01) for both metrics on
the two datasets. This indicates the eﬀectiveness of our
popularity-biased weighting strategy. Moreover, when α is

Figure 3: NDCG of whole-data based MF across K.

larger than 0.5, the performance starts to drop signiﬁcantly.
This reveals the drawback of over-weighting popular items
as negative instances, thus the importance of accounting for
less popular items with a proper weight.

In the following experiments, we ﬁx c0 and α according to
the best performance evaluated by HR, i.e., c0 = 512, α =
0.4 for Yelp and c0 = 64, α = 0.5 for Amazon.

5.2.2 Compare Whole-data based MF Methods

We performed the same grid search of w0 for RCD and

ALS and reported the best performance.

Convergence. Figure 2 shows the prediction accuracy
with respect to number of iterations. First, we see that
eALS achieves the best performance after converge. All
improvements are statistically signiﬁcant evidenced by the
one-sample paired t-test (p < 0.01). We believe the beneﬁts
mainly come from the popularity-aware objective function,
as both ALS and RCD apply a uniform weighting on the un-
knowns. Second, eALS and ALS converge faster than RCD.
We think the reason is that (e)ALS updates a parameter to
minimize the objective function of the current status, while
RCD updates towards the direction of the negative gradient,
which can be suboptimal. On Amazon, RCD shows high but
turbulent NDCG in early iterations, while the low hit ratio
and later iterations indicate the high NDCG is unstable.
Finally, we point out that in optimizing the same objective
function, ALS outperforms RCD in most cases, demonstrat-
ing the advantage of ALS over the gradient descent learner.

(a) Iterations vs. HR

(b) Iterations vs. NDCG

(c) Iterations vs. HR

(d) Iterations vs. NDCG

Figure 4: Accuracy and convergence comparison of eALS with BPR of diﬀerent learning rate.

Accuracy vs. Number of Factors. Figure 3 shows
the prediction accuracy with varying number of factors K.
We only show the evaluation by NDCG as HR admits the
same trend. First, eALS consistently outperforms ALS and
RCD across K, demonstrating the eﬀectiveness of our eALS
method (n.b. although the three methods seem to perform
on par for Amazon at K = 128, their diﬀerence can be
clearly seen in Figure 2d). Second, all methods can be im-
proved signiﬁcantly with a larger K. Although a large K
might have the risk of overﬁtting, it can increase model’s
representation ability thus better prediction. Especially for
large datasets that can have millions of users and billions
of interactions, a large K is particularly important for the
accuracy of MF methods.

Eﬃciency. Analytically, ALS’s time complexity is O((M +
N )K 3 + |R|K 2), while eALS and RCD are K times faster.
To compare their eﬃciency empirically, we show the actual
training time per iteration in Table 3.

Table 3: Training time per iteration of diﬀerent
whole-based MF methods with varying K.

Yelp
eALS RCD ALS
10s
1s
46s
3s
221s
10s
23m
0.9m
2.5h
2m
25.4h
11m

K
32
64
128
256
512
1024
s, m, and h denote seconds, minutes and hours, respectively.

Amazon
eALS RCD ALS
74s
10s
4.8m
17s
21m
42s
2h
2.8m
11.6h
9m
74h
48m

9s
23s
72s
4m
12m
54m

1s
4s
13s
1m
2m
7m

As can be seen, with the increase of K, ALS takes much
longer time than eALS and RCD. Speciﬁcally, when K is
512, ALS requires 11.6 hours for one iteration on Amazon,
while eALS only takes 12 minutes. Although eALS does
not empirically show K times faster than ALS due to the
more eﬃcient matrix inversion implementation (we used the
fastest known algorithm [1] with time complexity around
O(K 2.376)), the speed-up is already very signiﬁcant. More-
over, as RCD and eALS have the same analytical time com-
plexity, their actual running time are in the same magnitude;
the minor diﬀerence can be caused by some implementation
details, such as the data structures and caches used.

5.2.3

eALS vs. BPR (sample-based)

Figure 4 plots the performance of BPR with diﬀerent
learning rates4 in each iteration. Note that we only run
eALS for 100 iterations, which are enough for eALS to con-
verge. First, it is clear that BPR’s performance is subjected
4We have also tried other intermediate values of learning
rates, and the ﬁndings are consistent. Thus, to make the
ﬁgure more clear, we only show three selected values.

to the choice of learning rate — Figure 4a and 4b show that
a higher learning rate leads to a faster convergence, while the
ﬁnal accuracy may be suﬀered. Second, we see that eALS
signiﬁcantly outperforms BPR on the Yelp dataset evaluated
by both measures (p < 0.001). For Amazon, eALS obtains
a much higher hit ratio but a lower NDCG score, indicat-
ing that most hits occur at a relatively low ranks for eALS.
Comparing with the performance of other whole-based MF
methods ALS and RCD (Figure 2), we draw the conclusion
that BPR is a weak performer in terms of the prediction
recall, while being a strong performer in terms of the preci-
sion at top ranks. We think BPR’s strength in ranking top
items is due to its optimization objective, which is a pair-
wise ranking function tailored for ranking correct item high.
In contrast, the regression-based objective is not directly op-
timized for ranking; instead, by account for all missing data
in regression, it better predicts user’s preference on uncon-
sumed items, leading to a better recall. This is consistent
with [2]’s ﬁnding in evaluating top-K recommendation.

We notice that BPR shows unusual NDCG spike in early
iterations on the Amazon dataset, however the performance
is unstable and goes down with more iterations. The same
phenomenon was also observed for another gradient descent
method RCD on the same dataset (see Figure 2d). We hy-
pothesize that it might be caused by some regularities in
the data. For example, we ﬁnd some Amazon users review
on a movie multiple times5. In early iterations, BPR ranks
these repeated items high, leading to a high but unstable
NDCG score. There might be other reasons responsible for
this, and we do not further explore here.

5.3 Online Protocol

In the evaluation of online protocol, we hold out the latest
10% interactions as the test set, training all methods on the
remaining 90% data with the best parameter settings evi-
denced by the oﬄine evaluation. We ﬁrst study the number
of online iterations required for eALS to converge. Then we
show how does the weight of new interactions impact the
performance. Lastly, we compare with dynamic MF meth-
ods RCD and BPR in the online learning scenario.

5.3.1 Number of Online Iterations

Figure 6 shows how does eALS’s accuracy change with
number of online iterations. Results at the 0-th iteration
benchmark the performance of the oﬄine trained model, as
no incremental update is performed. First, we can see that
the oﬄine trained model performs very poorly, highlight-

5Due to user’s repeat consumption behaviours, we do not
exclude training items when generating recommend list.

(a) Test # vs. HR

(b) Test # vs. NDCG

(c) Test # vs. HR

(d) Test # vs. NDCG

Figure 5: Performance evolution of eALS and other dynamic MF methods in online learning.

ing the importance of refreshing recommender model for an
online system with dynamic data. Second, we ﬁnd most
performance gain comes from the ﬁrst iteration, and more
iterations do not further improve. This is due to the fact
that only the local features regarding to the new interaction
are updated, and one eALS step on a latent factor can ﬁnd
the optimal solution with others ﬁxed. Thus, one iteration
is enough for eALS to learn from a new interaction incre-
mentally, making eALS very eﬃcient for learning online.

Figure 6: Impact of online iterations on eALS.

We have also investigated number of online iterations re-
quired for baselines RCD and BPR. RCD shows the same
trend that good prediction is obtained in the ﬁrst iteration.
While BPR requires more iterations, usually 5-10 iterations
to get a peak performance and more iterations will adversely
hurt the performance due to the local over-training.

5.3.2 Weight of New Interactions

To evaluate how does the weight of new interactions ef-
fect the online learning algorithms, we also apply the same
weight wnew on RCD. Note that the original RCD paper [4]
does not consider the weight of interactions; we encode wnew
the same way with eALS, revising the RCD learner to opti-
mize the weighted regression function.

Figure 7: Impact of wnew on eALS and RCD in online
learning evaluated by NDCG.

Figure 7 shows the performance evaluated by NDCG (re-
sults of HR show the same trend thus omitted for space).
Setting wnew to 1 signiﬁes that new interaction is assigned a
same weight with the old training interaction. As expected,
with a modest increasing on wnew, the prediction of both

models is gradually improved, demonstrating the usefulness
of strengthening user’s short-term interest. The peak per-
formance is obtained around 4, where eALS shows better
prediction than RCD. Overly increasing wnew will adversely
hurt the performance, admitting the utility of user’s histor-
ical data used in oﬄine training. Overall, this experiment
indicates the importance of balancing user’s short-term and
long-term interest for quality recommendation.

5.3.3 Performance Comparison

With the simulated data stream, we show the performance
evolution with respect to number of test instances in Figure
5. First, eALS consistently outperforms RCD and BPR evi-
denced by both measures, and one-sample paired t-test ver-
iﬁes that all improvements are statistically signiﬁcant with
p < 0.001. BPR betters RCD for Yelp, while underper-
forms for Amazon. Second, we observe the trend that the
performance of dynamic learning ﬁrst decreases, and then
increases before becoming stable. This is caused by the new
users problem — when there are few feedback for a user, the
model can not personalize the user’s preference eﬀectively;
with more feedbacks streaming in, the model can adapt itself
to improve the preference modeling accordingly. To show
this, we further breakdown the results of eALS by number
of past interactions of test user in Figure 8.

Figure 8: Results breakdown of eALS by # of past
interactions of test user. Note: Interaction # > 0
denotes the performance for non-cold-start users.

It is clear that when there are no historical feedback for
a test user (i.e., user cold-start cases), the performance is
very poor — no better than random. After the ﬁrst interac-
tion streams in, the prediction is signiﬁcantly improved; and
with more interactions, the performance is further improved.
This highlights the importance of incorporating instanta-
neous user feedback into the model, especially for cold-start
or sparse users that have few history in training.

6. CONCLUSION AND FUTURE WORK

We study the problem of learning MF models from im-
plicit feedback. In contrast to previous work that applied

a uniform weight on missing data, we propose to weight
missing data based on the popularity of items. To address
the key eﬃciency challenge in optimization, we develop a
new learning algorithm — eALS — which eﬀectively learns
parameters by performing coordinate descent with memo-
ization. For online learning, we devise an incremental up-
date strategy for eALS to adapt dynamic data in real-time.
Experiments with both oﬄine and online protocols demon-
strate promising results. Importantly, our work makes MF
more practical to use for modeling implicit data, along two
dimensions. First, we investigate a new paradigm to deal
with missing data which can easily incorporate prior do-
main knowledge. Second, eALS is embarrassingly parallel,
making it attractive for large-scale industrial deployment.

We plan to study the optimal weighting strategy for online
data as a way to explore user’s short-term interest. Along
the technical line, we explored the element-wise ALS learner
in its basic MF form and solved the eﬃciency challenge in
handling missing data. To make our method more applica-
ble to real-world settings, we plan to encode side information
such as user social contexts [8] and reviews [9] by extending
eALS to more generic models, such as collective factoriza-
tion [11] and Factorization machines [26]. In addition, we
will study binary coding for MF on implicit data, since a
recent advance [32] has shown that discrete latent factors
are beneﬁcial to collaborative ﬁltering for explicit ratings.

The strength of eALS can be applied to other domains,
owing to the universality of factorizing sparse data matrices.
For example, recent advances in natural language process-
ing [16] have shown the connection between neural word em-
beddings and MF on the word–context matrix. This bridge
nicely motivates several proposals to use MF to learn word
embeddings; however, when it comes to handling missing
data, they have either ignored [22] or equally weighted the
missing entries, similar to traditional SVD [16]. It will be in-
teresting to see whether eALS will also improve these tasks.
Acknowledgement
The authors would like to thank the additional discussion
and help from Steﬀen Rendle, Bhargav Kanagal, Immanuel
Bayer, Tao Chen, Ming Gao and Jovian Lin.

7. REFERENCES
[1] D. Coppersmith and S. Winograd. Matrix multiplication

via arithmetic progressions. J. Symb. Comput.,
9(3):251–280, 1990.

[2] P. Cremonesi, Y. Koren, and R. Turrin. Performance of

recommender algorithms on top-n recommendation tasks.
In RecSys 2010, pages 39–46.

[3] A. S. Das, M. Datar, A. Garg, and S. Rajaram. Google

news personalization: Scalable online collaborative ﬁltering.
In WWW 2007, pages 271–280.

[4] R. Devooght, N. Kourtellis, and A. Mantrach. Dynamic
matrix factorization with priors on unknown values. In
KDD 2015, pages 189–198.

[5] E. Diaz-Aviles, L. Drumond, L. Schmidt-Thieme, and

W. Nejdl. Real-time top-n recommendation in social
streams. In RecSys 2012, pages 59–66.

[6] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient

methods for online learning and stochastic optimization. J.
Mach. Learn. Res., 12:2121–2159, 2011.

[7] R. Gemulla, E. Nijkamp, P. J. Haas, and Y. Sismanis.

Large-scale matrix factorization with distributed stochastic
gradient descent. In KDD 2011, pages 69–77.

[8] X. Geng, H. Zhang, J. Bian, and T.-S. Chua. Learning

image and user features for recommendation in social
networks. In ICCV 2015, pages 4274–4282.

[9] X. He, T. Chen, M.-Y. Kan, and X. Chen. Trirank:

Review-aware explainable recommendation by modeling
aspects. In CIKM 2015, pages 1661–1670.

[10] X. He, M. Gao, M.-Y. Kan, Y. Liu, and K. Sugiyama.

Predicting the popularity of web 2.0 items based on user
comments. In SIGIR 2014, pages 233–242.

[11] X. He, M.-Y. Kan, P. Xie, and X. Chen. Comment-based
multi-view clustering of web 2.0 items. In Proc. of WWW
’14, pages 771–782, 2014.

[12] Y. Hu, Y. Koren, and C. Volinsky. Collaborative ﬁltering
for implicit feedback datasets. In ICDM 2008, pages
263–272.

[13] Y. Huang, B. Cui, W. Zhang, J. Jiang, and Y. Xu.

Tencentrec: Real-time stream recommendation in practice.
In SIGMOD 2015, pages 227–238.

[14] Y. Koren. Collaborative ﬁltering with temporal dynamics.

In KDD 2009, pages 447–456.

[15] Y. Koren and R. Bell. Advances in collaborative ﬁltering.
In Recommender systems handbook, pages 145–186.
Springer, 2011.

[16] O. Levy and Y. Goldberg. Neural word embedding as
implicit matrix factorization. In NIPS 2014, pages
2177–2185.

[17] G. Ling, H. Yang, I. King, and M. R. Lyu. Online learning
for collaborative ﬁltering. In IJCNN 2012, pages 1–8.
[18] B. M. Marlin, R. S. Zemel, S. Roweis, and M. Slaney.
Collaborative ﬁltering and the missing at random
assumption. In UAI 2007, pages 267–276.

[19] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and

J. Dean. Distributed representations of words and phrases
and their compositionality. In NIPS 2013, pages 3111–3119.

[20] R. Pan and M. Scholz. Mind the gaps: Weighting the

unknown in large-scale one-class collaborative ﬁltering. In
KDD 2009, pages 667–676.

[21] R. Pan, Y. Zhou, B. Cao, N. Liu, R. Lukose, M. Scholz,
and Q. Yang. One-class collaborative ﬁltering. In ICDM
2008, pages 502–511.

[22] J. Pennington, R. Socher, and C. D. Manning. Glove:

Global vectors for word representation. In EMNLP 2014,
pages 1532–1543.

[23] I. Pil´aszy, D. Zibriczky, and D. Tikk. Fast als-based matrix
factorization for explicit and implicit feedback datasets. In
RecSys 2010, pages 71–78.

[24] S. Rendle and C. Freudenthaler. Improving pairwise

learning for item recommendation from implicit feedback.
In WSDM 2014, pages 273–282.

[25] S. Rendle, C. Freudenthaler, Z. Gantner, and

L. Schmidt-Thieme. Bpr: Bayesian personalized ranking
from implicit feedback. In UAI 2009, pages 452–461.

[26] S. Rendle, Z. Gantner, C. Freudenthaler, and

L. Schmidt-Thieme. Fast context-aware recommendations
with factorization machines. In SIGIR 2011, pages 635–644.

[27] S. Rendle and L. Schmidt-Thieme. Online-updating

regularized kernel matrix factorization models for large
scale recommender systems. In RecSys 2008, pages 251–258.

[28] P. Richt´arik and M. Tak´aˇc. Iteration complexity of

randomized block-coordinate descent methods for
minimizing a composite function. Math. Prog., 2014.
[29] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips,

D. Ebner, V. Chaudhary, and M. Young. Machine learning:
The high interest credit card of technical debt. In SE4ML
(NIPS 2014 Workshop), 2014.

[30] H. Steck. Training and testing of recommender systems on
data missing not at random. In KDD 2010, pages 713–722.

[31] M. Volkovs and G. W. Yu. Eﬀective latent models for

binary feedback in recommender systems. In SIGIR 2015,
pages 313–322.

[32] H. Zhang, F. Shen, W. Liu, X. He, H. Luan, and T.-S.
Chua. Discrete collaborative ﬁltering. In SIGIR 2016.

Fast Matrix Factorization for Online Recommendation
with Implicit Feedback∗

Xiangnan He

Hanwang Zhang

Min-Yen Kan

Tat-Seng Chua

School of Computing, National University of Singapore
{xiangnan, hanwang, kanmy, chuats}@comp.nus.edu.sg

7
1
0
2
 
g
u
A
 
6
1
 
 
]

R

I
.
s
c
[
 
 
1
v
4
2
0
5
0
.
8
0
7
1
:
v
i
X
r
a

ABSTRACT
This paper contributes improvements on both the eﬀective-
ness and eﬃciency of Matrix Factorization (MF) methods
for implicit feedback. We highlight two critical issues of ex-
isting works. First, due to the large space of unobserved
feedback, most existing works resort to assign a uniform
weight to the missing data to reduce computational com-
plexity. However, such a uniform assumption is invalid in
real-world settings. Second, most methods are also designed
in an oﬄine setting and fail to keep up with the dynamic
nature of online data.

We address the above two issues in learning MF models
from implicit feedback. We ﬁrst propose to weight the miss-
ing data based on item popularity, which is more eﬀective
and ﬂexible than the uniform-weight assumption. However,
such a non-uniform weighting poses eﬃciency challenge in
learning the model. To address this, we speciﬁcally de-
sign a new learning algorithm based on the element-wise
Alternating Least Squares (eALS) technique, for eﬃciently
optimizing a MF model with variably-weighted missing data.
We exploit this eﬃciency to then seamlessly devise an incre-
mental update strategy that instantly refreshes a MF model
given new feedback. Through comprehensive experiments
on two public datasets in both oﬄine and online protocols,
we show that our eALS method consistently outperforms
state-of-the-art implicit MF methods. Our implementation
is available at https://github.com/hexiangnan/sigir16-eals.

Keywords
Matrix Factorization, Implicit Feedback, Item Recommen-
dation, Online Learning, ALS, Coordinate Descent

1.

INTRODUCTION

User personalization has become prevalent in modern rec-
ommender system. It helps to capture users’ individualized

∗NExT research is supported by the National Research
Foundation, Prime Minister’s Oﬃce, Singapore under its
IRC@SG Funding Initiative.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’16, July 17-21, 2016, Pisa, Italy
c(cid:13) 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2911489

preferences and has been shown to increase both satisfac-
tion for users and revenue for content providers. Among
its various methods, matrix factorization (MF) is the most
popular and eﬀective technique that characterizes users and
items by vectors of latent factors [15, 32]. Early work on
MF algorithms for recommendation [14, 27] have largely fo-
cused on explicit feedback, where users’ ratings that directly
reﬂect their preference on items are provided. These works
formulated recommendation as a rating prediction problem
for which the large volume of unobserved ratings (i.e., miss-
ing data) are assumed to be extraneous for modeling user
preference [12]. This greatly reduces the modeling workload,
and many sophisticated methods have been devised, such as
SVD++ [15] and timeSVD [14].

However, explicit ratings are not always available in many
applications; more often, users interact with items through
implicit feedback, e.g., users’ video viewing and prod-
uct purchase history. Compared to explicit ratings, implicit
feedback is easier to collect for content providers, but more
challenging to utilize due to the natural scarcity of negative
feedback.
It has been shown that modeling only the ob-
served, positive feedback results in biased representations in
user proﬁles [4, 12]; e.g., Marlin et al. [18] ﬁnds that users
listen to music they expect to like and avoid the genres they
dislike, leading to a severe bias in the observed data.

To solve the problem of lacking negative feedback (also
known as the one-class problem [21]), a popular solution
is to model all the missing data as negative feedback [12].
However, this adversely degrades the learning eﬃciency due
to the full consideration of both observed and missing data.
More importantly, the low eﬃciency makes it even more dif-
ﬁcult to deploy implicit MF method online [29]. In practical
recommender systems where new users, items and interac-
tions are continuously streaming in, it is crucial to refresh
the underlying model in real-time to best serve users. In this
work, we concern the above two challenging problems of the
MF method — implicit feedback and online learning. We
note that we are not the ﬁrst to consider both aspects for
MF, as a recent work by Devooght et al. [4] has proposed an
eﬃcient implicit MF method for learning with dynamic data.
However, we argue that Devooght’s method [4] models miss-
ing data in an unrealistic, suboptimal way. Speciﬁcally, it
assigns a uniform weight to the missing data, assuming that
the missing entries are equally likely to be negative feedback.
However, such an assumption limits model’s ﬁdelity and ﬂex-
ibility for real applications. For example, content providers
usually know which items have been frequently featured to
users but seldom clicked; such items are more likely to be

true negative assessments and should be weighted higher
than others. In addition, Devooght’s method learns param-
eters through gradient descent, requiring an expensive line
search to determine the best learning rate at each step.

We propose a new MF method aimed at learning from im-
plicit feedback eﬀectively while satisfying the requirement of
online learning. We develop a new learning algorithm that
eﬃciently optimizes the implicit MF model without impos-
ing a uniform-weight restriction on missing data.
In par-
ticular, we assign the weight of missing data based on the
popularity of items, which is arguably more eﬀective than
the previous methods [4, 12, 23, 30, 31] that are limited by
the uniformity assumption. Our eALS algorithm is fast in
accounting for missing data — analytically K times faster
than ALS [12] where K denotes number of latent factors
— the same time complexity with the recent dynamic MF
solution [4]. This level of eﬃciency makes eALS suitable
for learning online, for which we develop an incremental
update strategy that instantly refreshes model parameters
given new incoming data. Another key advantage of eALS
is that it works without learning rate, bypassing the well-
known diﬃculty for tuning gradient descent methods such
as [4] and Stochastic Gradient Descent (SGD) [25].
We summarize our key contributions as follows.
1. We propose an item popularity-aware weighting scheme
on the full missing data that eﬀectively tailors the MF
model for learning from implicit feedback.

2. We develop a new algorithm for learning model pa-
rameters eﬃciently and devise an incremental update
strategy to support real-time online learning.

3. We conduct extensive experiments with both oﬄine
and online protocols on two real-world datasets, show-
ing that our method consistently outperforms state-of-
the-art implicit MF methods.

2. RELATED WORK

Handling missing data is obligatory for learning from im-
plicit data due to the lack of negative feedback. To this end,
two strategies have been proposed — sample based learn-
ing [21, 25] that samples negative instances from missing
data, or whole-data based learning [12, 30] that treats
all missing data as negative. Both methods have pros and
cons: sample-based methods are more eﬃcient by reduc-
ing negative examples in training, but risk decreasing the
model’s predictiveness; whole-based methods model the full
data with a potentially higher coverage, but ineﬃciency can
be an issue. To retain model’s ﬁdelity, we persist in the
whole-data based learning, developing a fast ALS-based al-
gorithm to resolve the ineﬃciency issue.

For existing whole-data based methods [4, 12, 23, 30, 31],
one major limitation is in the uniform weighting on missing
entries, which favors algorithm’s eﬃciency but limits model’s
ﬂexibility and extensibility. The only works that have con-
sidered non-uniform weighting are from Pan et al. [20, 21];
however their cubic time complexity w.r.t. K makes it un-
suitable to run on large-scale data [23], where a large number
of factors needs to be considered to gain improved perfor-
mance [31].

To optimize MF, various learners have been investigated,
including SGD [14, 25], Coordinate Descent (CD) [4, 32],
and Markov Chain Monto Carlo (MCMC) [26]. SGD is the
most popular one owing to the ease of derivation, however,

it is unsuitable for whole-data based MF [12] due to the
large amount of training instances (the full user–item inter-
action matrix is considered). ALS can be seen as an in-
stantiation of CD and has been widely used to solve the
whole-based MF [12, 20, 21, 30]; however, its ineﬃciency is
the main obstacle for practical use [23, 31]. To resolve this,
[23] describes an approximate solution to ALS. Recently, [4]
employs the Randomized block Coordinate Descent (RCD)
learner [28], reducing the complexity and applying it to a dy-
namic scenario. Similarly, [31] enriches the implicit feedback
matrix with neighbor-based similarly, followed by applying
unweighted SVD. Distinct from previous works, we propose
an eﬃcient element-wise ALS solution for the whole-data
based MF with non-uniform missing data, which has never
been studied before.

Another important aspect for practical recommender sys-
tem lies in handling the dynamic nature of incoming data,
for which timeliness is a key consideration. As it is pro-
hibitive to retrain the full model online, various works have
developed incremental learning strategies for neighbor-based
[13], graph-based [9], probabilistic [3] and MF [4, 5, 17, 27]
methods. For MF, diﬀerent learners have been studied for
online updating, including SGD [5, 27], RCD [4] and dual-
averaging [17]. To our knowledge, this work is the ﬁrst at-
tempt to exploit the ALS technique for online learning.

3. PRELIMINARIES

We ﬁrst introduce the whole-data based MF method for
learning from implicit data, highlighting the ineﬃciency is-
sue of the conventional ALS solution [12, 21]. Then we
describe eALS, an element-wise ALS learner [26] that can
reduce the time complexity to linearity w.r.t. number of
factors. Although the learner is generic in optimizing MF
with all kinds of weighting strategies, the form introduced
is costly in accounting for all missing data and is thus unre-
alistic for practical use. This defect motivates us to further
develop eALS to make it suitable for learning from implicit
feedback (details in Section 4.2).

3.1 MF Method for Implicit Feedback

We start by introducing some basic notation. For a user–
item interaction matrix R ∈ RM ×N , M and N denote the
number of users and items, respectively; R denotes the set
of user–item pairs whose values are non-zero. We reserve the
index u to denote a user and i to denote an item. Vector pu
denotes the latent feature vector for u, and set Ru denotes
the set of items that are interacted by u; similar notations
for qi and Ri. Matrices P ∈ RM ×K and Q ∈ RN ×K denote
the latent factor matrix for users and items.

Matrix factorization maps both users and items into a
joint latent feature space of K dimension such that interac-
tions are modeled as inner products in that space. Mathe-
matically, each entry rui of R is estimated as:

ˆrui =< pu, qi >= pT

u qi.

(1)

The item recommendation problem is formulated as estimat-
ing the scoring function ˆrui, which is used to rank items.
Note that this basic model subsumes the biased MF [14],
commonly used in modeling explicit ratings:

ˆrui = bu + bi+ < pB

u , qB

i >,

where bu (bi) captures the bias of user u (item i) in giving
(receiving) ratings. To recover it, set pu ← [pB
u , bu, 1] and

qi ← [qB
i , 1, bi]. As such, we adopt the basic MF model
to make notations simple and also to enable a fair compari-
son with baselines [4, 12] that also complied with the basic
model.

To learn model parameters, Hu et al. [12] introduced a
weighted regression function, which associates a conﬁdence
to each prediction in the implicit feedback matrix R:

J =

M
(cid:88)

N
(cid:88)

u=1

i=1

wui(rui − ˆrui)2 + λ(

||pu||2 +

||qi||2), (2)

M
(cid:88)

u=1

N
(cid:88)

i=1

where wui denotes the weight of entry rui and we use W =
[wui]M ×N to represent the weight matrix. λ controls the
strength of regularization, which is usually an L2 norm to
prevent overﬁtting. Note that in implicit feedback learning,
missing entries are usually assigned to a zero rui value but
non-zero wui weight, both crucial to performance.

3.2 Optimization by ALS

Alternating Least Square (ALS) is a popular approach
to optimize regression models such as MF and graph reg-
ularization [10]. It works by iteratively optimizing one pa-
rameter, while leaving the others ﬁxed. The prerequisite of
ALS is that the optimization sub-problem can be analyti-
cally solved. Here, we describe how Hu’s work [12] solves
this problem.

First, minimizing J with respect to user latent vector pu

is equivalent to minimizing:

Ju = ||Wu(ru − Qpu)||2 + λ||pu||2,
where Wu is a N × N diagonal matrix with W u
minimum is where the ﬁrst-order derivative is 0:

ii = wui. The

∂Ju
∂pu

= 2QT WuQpu − 2QT Wuru + 2λpu = 0

(3)

⇒ pu = (QT WuQ + λI)−1QT Wuru,

where I denotes the identity matrix. This analytical solution
is also known as the ridge regression [23]. Following the same
process, we can get the solution for qi.

3.2.1 Efﬁciency Issue with ALS

As we can see, in order to update a latent vector, in-
verting a K × K matrix is inevitable. Matrix inversion
is an expensive operation, usually assumed O(K 3) in time
complexity [12]. As such, updating one user latent vector
takes time O(K 3 + N K 2). Thus, the overall time complex-
ity of one iteration that updates all model parameters once
is O((M + N )K 3 + M N K 2). Clearly, this high complexity
makes the algorithm impractical to run on large-scale data,
where there can be millions of users and items and billions
of interactions.

Speed-up with Uniform Weighting. To reduce the
high time complexity, Hu et al. [12] applied a uniform weight
to missing entries; i.e., assuming that all zero entries in R
have a same weight w0. Through this simpliﬁcation, they
can speed up the computation with memoization:

QT WuQ = w0QT Q + QT (Wu − W0)Q,

(4)

where W0 is a diagonal matrix that each diagonal element is
w0. As QT Q is independent of u, it can be pre-computed for
updating all user latent vectors. Considering the fact that
Wu − W0 only has |Ru| non-zero entries, we can compute

Eq. (4) in O(|Ru|K 2) time. Thus, the time complexity of
ALS is reduced to O((M + N )K 3 + |R|K 2).

Even so, we argue that the O((M + N )K 3) term can be
a major cost when (M + N )K ≥ |R|.
In addition, the
O(|R|K 2) part is still much higher than in SGD [25], which
only requires O(|R|K) time. As a result, even with the ac-
celeration, ALS is still prohibitive for running on large data,
where large K is crucial as it can lead to better generaliz-
ability and thus better prediction performance. Moreover,
the uniform weighting assumption is usually invalid in real
applications and adversely degrades model’s predictiveness.
This thus motivates us to design an eﬃcient implicit MF
method not subject to uniform-weights.

3.3 Generic Element-wise ALS Learner

The bottleneck of the previous ALS solution lies in the ma-
trix inversion operation, which is due to the design that up-
dates the latent vector for a user (item) as a whole. As such,
it is natural to optimize parameters at the element level —
optimizing each coordinate of the latent vector, while leav-
ing the others ﬁxed [26]. To achieve this, we ﬁrst get the
derivative of objective function Eq. (2) with respect to puf :

∂J
∂puf

N
(cid:88)

i=1

= −2

(rui − ˆrf

ui)wuiqif + 2puf

wuiq2

if + 2λpuf ,

N
(cid:88)

i=1

where ˆrf
ui = ˆrui − puf qif , i.e., the prediction without the
component of latent factor f . By setting this derivative to
0, we obtain the solution of puf :

Similarly, we can get the solver for an item latent factor:

puf =

(cid:80)N

i=1(rui − ˆrf
(cid:80)N
i=1 wuiq2

ui)wuiqif
if + λ

.

qif =

(cid:80)M

u=1(rui − ˆrf
(cid:80)M
i=1 wuip2

ui)wuipuf
uf + λ

.

(5)

(6)

Given the closed-form solution that optimizes one param-
eter with other ﬁxed, the algorithm iteratively executes it
for all model parameters until a joint optimum is reached.
Due to the non-convexity of the objective function, critical
points where gradients vanish can be local minima.

Time Complexity. As can be seen, by performing opti-
mization at the element level, the expensive matrix inversion
can be avoided. A raw implementation takes O(M N K 2)
time for one iteration, directly speeding up ALS by eliminat-
ing the O(K 3) term. Moreover, by pre-computing ˆrui [26],
we can calculate ˆrf
ui in O(1) time rather than O(K). As
such, the complexity can be further reduced to O(M N K),
which is the same magnitude with evaluating all the user–
item predictions.

4. OUR IMPLICIT MF METHOD

We ﬁrst propose an item-oriented weighting scheme on
the missing data, and follow with a popularity-aware weight-
ing strategy, which is arguably more eﬀective than the uni-
form weighting for the recommendation task. Then, we de-
velop a fast eALS algorithm to optimize the objective func-
tion that signiﬁcantly reduces learning complexity compar-
ing with the conventional ALS [12] and generic element-wise
ALS learner [26]. Lastly, we discuss how to adjust the learn-
ing algorithm for real-time online learning.

(7)

the observed data part:

4.1

Item-Oriented Weighting on Missing Data
Due to the large space of items, the missing entries for a
user are a mixture of negative and unknown feedback. In
specifying the weight wui of missing entries, it is desired to
assign a higher weight to the negative feedback. However, it
is a well-known diﬃculty to diﬀerentiate the two cases. In
addition, as the interaction matrix R is usually large and
sparse, it will be too consuming to store each zero entry an
individualized weight. To this end, existing works [4, 12,
23, 30, 31] have applied a simple uniform weight on missing
entries, which are, however, suboptimal and non-extendable
for real applications.

Considering the ease of content providers in accessing neg-
ative information of the item side (e.g., which items have
been promoted to users but receive little interaction), we
believe it is more realistic to weight missing data based on
some item property. To capture this, we devise a more ﬁne-
grained objective function as follows:

(cid:88)

L =

wui(rui − ˆrui)2 +

(u,i)∈R

M
(cid:88)

u=1

+ λ(

||pu||2 +

||qi||2),

N
(cid:88)

i=1

M
(cid:88)

(cid:88)

u=1

i /∈Ru

ci ˆr2
ui

where ci denotes the conﬁdence that item i missed by users
is a true negative assessment, which can serve as a means to
encode domain knowledge from practitioners. It is clear that
the ﬁrst term denotes the prediction error of the observed
entries, which has been widely adopted in modeling explicit
ratings [15, 27]. The second term accounts for the miss-
ing data, which acts as the role of negative instances and is
crucial for recommendation from implicit feedback [12, 25].
Next, we present a domain-independent strategy to deter-
mine ci by leveraging a ubiquitous feature of modern Web
2.0 systems.

4.1.1 Popularity-aware Weighting Strategy

Existing visual interfaces of many Web 2.0 systems show-
case popular items in their recommendations. All other fac-
tors being equal, popular items are more likely to be known
by users in general [10], and thus it is reasonable to think
that a miss on a popular item is more probable to be truly
irrelevant (as opposed to unknown) to the user. To account
for this eﬀect, we parametrize ci based on item’s popularity:

ci = c0

f α
i
j=1 f α
j

,

(cid:80)N

(8)

where fi denotes the popularity of item i, given by its fre-
quency in the implicit feedback data: |Ri|/ (cid:80)N
j=1 |Rj|, and
c0 determines the overall weight of missing data. Exponent
α controls the signiﬁcance level of popular items over un-
popular ones — when α > 1 the weights of popular items
are promoted to strengthen the diﬀerence against unpop-
ular ones; while setting α within the lower range of (0, 1)
suppresses the weight of popular items and has a smoothing
eﬀect. We empirically ﬁnd α = 0.5 usually leads to good
results. Note that the uniform weighting is a special case by
setting α to 0 with w0 = c0/N .

Relationship to Negative Sampling. Our proposed
popularity-aware weighting strategy has the same intuition
with Rendle’s popularity-based oversampling [24] for learn-
ing BPR, which basically samples popular items as nega-

tive feedback with a higher probability. However, [24] em-
pirically shows the oversampling method underperforms the
basic uniform sampler. We suspect the reason comes from
the SGD learner, which will result in more gradient steps
on popular items, due to oversampling. As a result, popular
items may be over-trained locally at the expense of less pop-
ular items which would then be under-trained. To resolve
this, tricks like subsampling frequent items [19] and adaptive
learning rates like Adagrad [6] have been adopted in other
domains. As the focus of this paper is on whole-data based
implicit MF, we do not further explore the details of SGD. It
is worth pointing out that our proposed eALS learner avoids
these learning issues by an exact optimization on each model
parameter.

4.2 Fast eALS Learning Algorithm

We can speed up learning by avoiding the massive re-
peated computations introduced by the weighted missing
data. We detail the derivation process for puf ; where the
counterpart for qif is achieved likewise.

First, we rewrite the puf update rule Eq. (5) by separating

puf =

(cid:80)

i∈Ru
(cid:80)

(rui − ˆrf

ui)wuiqif − (cid:80)
if + (cid:80)

i /∈Ru

ˆrf
uiciqif
if + λ

i /∈Ru
ciq2

wuiq2

.

i∈Ru

Clearly, the computational bottleneck lies in the summa-
tion over missing data portion, which requires a traversal of
the whole negative space. We ﬁrst focus on the numerator:

(cid:88)

i /∈Ru

ˆrf
uiciqif =

ciqif

pukqik −

ˆrf
uiciqif

N
(cid:88)

i=1

(cid:88)

k(cid:54)=f

(cid:88)

=

puk

N
(cid:88)

k(cid:54)=f

i=1

(cid:88)

i∈Ru

(cid:88)

i∈Ru

ciqif qik −

ˆrf
uiciqif .

(9)

By this reformulation, we can see that the major computa-
tion — the (cid:80)N
i=1 ciqif qik term that iterates over all items
— is independent of u. However, a na¨ıve implementation
repeatedly computes it unnecessarily, when updating the la-
tent factors for diﬀerent users. Clearly, we can achieve a
signiﬁcant speed-up by memoizing it.

We deﬁne the Sq cache as Sq = (cid:80)N

i , which can
be pre-computed and used in updating the latent factors for
all users. Then, Eq. (9) can be evaluated as:

i=1 ciqiqT

(cid:88)

i /∈Ru

ˆrf
uiciqif =

puksq

f k −

ˆrf
uiciqif ,

(10)

(cid:88)

k(cid:54)=f

(cid:88)

i∈Ru

which can be done in O(K + |Ru|) time.

Similarly, we can apply the cache to speed up the calcu-

lation of denominator:

(cid:88)

i /∈Ru

N
(cid:88)

i=1

(cid:88)

i∈Ru

ciq2

if =

ciq2

if −

ciq2

if = sq

f f −

ciq2

if . (11)

(cid:88)

i∈Ru

To summarize the above memoization strategy, we give

the update rule for puf with the use of Sq cache:

puf =

(cid:80)

i∈Ru

[wuirui − (wui − ci)ˆrf
(wui − ci)q2

(cid:80)

ui]qif − (cid:80)
if + sq
f f + λ

i∈Ru

k(cid:54)=f puksq

kf

.

(12)

Algorithm 1: Fast eALS Learning algorithm.

Input: R, K, λ, W and item conﬁdence vector c;
Output: Latent feature matrix P and Q;

1 Randomly initialize P and Q ;
2 for (u, i)∈ R do ˆrui ← Eq. (1) ;
3 while Stopping criteria is not met do

(cid:46) O(|R|K)

// Update user factors
Sq = (cid:80)N
i=1 ciqiqT
;
for u ← 1 to M do

i

(cid:46) O(M K 2)
(cid:46) O(M K 2 + |R|K)

for f ← 1 to K do
for i ∈ Ru do ˆrf
puf ← Eq. (12) ;
for i ∈ Ru do ˆrui ← ˆrf

ui ← ˆrui − puf qif ;

(cid:46) O(K + |Ru|)

ui + puf qif ;

end

end
// Update item factors
Sp ← PT P ;
for i ← 1 to N do

(cid:46) O(N K 2)
(cid:46) O(N K 2 + |R|K)

for f ← 1 to K do
for u ∈ Ri do ˆrf
qif ← Eq. (13) ;
for u ∈ Ri do ˆrui ← ˆrf

ui ← ˆrui − puf qif ;

(cid:46) O(K + |Ri|)

ui + puf qif ;

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

end

end

19
20 end
21 return P and Q

Table 1: Time complexity of implicit MF methods.

Time Complexity
Method
O((M + N )K 3 + |R|K 2)
ALS (Hu et al.[12])
O(|R|K)
BPR (Rendle et al.[25])
O(K 3 + (M + N )K 2 + |R|K)
IALS1 (Pil´aszy et al.[23])
ii-SVD (Volkovs et al.[31]) O((M + N )K 2 + M N log K)
RCD (Devooght et al.[4])
eALS (Algorithm 1)

O((M + N )K 2 + |R|K)
O((M + N )K 2 + |R|K)

|R| denotes the number of non-zeros in user–item matrix R.

Similarly, we can derive the update rule for qif :

qif =

(cid:80)

u∈Ri

[wuirui − (wui − ci)ˆrf
(wui − ci)p2

(cid:80)

(cid:80)

ui]puf − ci
uf + cisp

f f + λ

u∈Ri

k(cid:54)=f qiksp

kf

,

(13)
kf denotes the (k, f )th element of the Sp cache, de-

where sp
ﬁned as Sp = PT P.

Algorithm 1 summarizes the accelerated algorithm for our
element-wise ALS learner, or eALS. For convergence, one
can either monitor the value of objective function on train-
ing set or check the prediction performance on a hold-out
validation data.

4.2.1 Discussion

Time Complexity.

In Algorithm 1, updating a user
latent factor takes O(K + |Ru|) time. Thus, one eALS iter-
ation takes O((M +N )K 2 +|R|K) time. Table 1 summarizes
the time complexity (of one iteration or epoch) of other MF
algorithms that are designed for implicit feedback.

Comparing with the vector-wise ALS [12, 21], our element-
wise ALS learner is K times faster. In addition, our pro-
posed eALS has the same time complexity with RCD [4],

being faster than ii-SVD [31], another recent solution. RCD
is a state-of-the-art learner for whole-data based MF, which
performs a gradient descent step on a randomly chosen la-
tent vector. Since it requires a good learning rate, the work
[4] adaptively determines it by a line search in each gradient
step, which essentially chooses the learning rate that leads to
the steepest descent among pre-deﬁned candidates. A major
advantage of eALS has over RCD is that it avoids the need
for a learning rate by an exact optimization in each param-
eter update, arguably more eﬀective and easier to use than
RCD. The most eﬃcient algorithm is BPR, which applies
the SGD learner on sampled, partial missing data only.

Computing the Objective Function. Evaluating the
objective function is important to check the convergence of
iterations and also to verify the correctness of implementa-
tion. A direct calculation takes O(M N K) time, requiring a
full estimation on the R matrix. Fortunately, with the item-
oriented weighting, we can similarly exploit the sparseness
of R for acceleration. To achieve this, we reformulate the
loss of the missing data part that causes the major cost:

M
(cid:88)

(cid:88)

u=1

i /∈Ru

ci ˆr2

ui =

u Sqpu −
pT

M
(cid:88)

u=1

(cid:88)

ci ˆr2

ui.

(u,i)∈R

(14)

By reusing Sq and the prediction cache ˆrui, we can calculate
the objective function in O(|R| + M K 2) time, much faster
than with direct calculation.

Parallel Learning. The iterations of eALS can be easily
parallelized. First, computing the S caches (line 4 and 12)
is the standard matrix multiplication operation, for which
modern matrix toolkits provide very eﬃcient and parallelized
implementation. Second, in updating the latent vectors for
diﬀerent users (line 5-11), the shared parameters are either
independent with each other (i.e., ˆrui) or remaining un-
changed (i.e., Sq). This nice property means that an exact
parallel solution can be obtained by separating the updates
by users; that is, letting diﬀerent workers update the model
parameters for disjoint sets of users. The same parallelism
can also be achieved in updating item latent vectors.

This is an advantage over the commonly-used SGD learner,
which is a stochastic method that updates model parame-
ters given a training instance.
In SGD, diﬀerent gradient
steps can inﬂuence with each other and there is no exact
way to separate the updates for workers. Thus, sophisti-
cated strategies are required to control the possible losses
introduced by parallelization [7]. Our proposed eALS solu-
tion optimizes by coordinate descent where in each step a
dedicated parameter is updated, making the algorithm em-
barrassingly parallel without any approximate loss.

4.3 Online Update

In practice, after a recommender model is trained oﬄine
on historical data, it will be used online and will need to
adapt to best serve users. Here, we consider the online
learning scenario that refreshes model parameters given a
new user–item interaction.

Incremental Updating. Let ˆP and ˆQ denote the model
parameters learnt from oﬄine training, and (u, i) denotes
the new interaction streamed in. To approximate the model
parameters in accounting for the new interaction, we per-
form optimization steps for pu and qi only. The underlying
assumption is that the new interaction should not change ˆP
and ˆQ too much from a global perspective, while it should

Algorithm 2: Online Incremental Updates for eALS.
Input: ˆP, ˆQ, new interaction (u, i) and its weight wnew
Output: Refreshed parameters P and Q;

1 P ← ˆP; Q ← ˆQ; ;
2 If u is a new user do Randomly initialize pu ;
3 If i is a new item do Randomly initialize qi ;
4 rui ← 1;
5 while Stopping criteria is not met do

ˆrui ← Eq. (1); wui ← wnew;

6

7

8

// Line 6-10 of Algorithm 1
update user(u); ;
update cache(u, Sp); ;
// Line 14-18 of Algorithm 1
update item(i); ;
update cache(i, Sq); ;

(cid:46) O(K 2 + |Ru|K)
(cid:46) O(K 2)

(cid:46) O(K 2 + |Ri|K)
(cid:46) O(K 2)

9
10 end
11 return P and Q

change the local features for u and i signiﬁcantly. Particu-
larly, when u is a new user, executing the local updates will
force pu close to qi, which meets the expectation of latent
factor model. The new item case is similar.

Algorithm 2 summarizes the incremental learning strat-
egy for eALS. For the stopping criteria, our empirical study
shows that one iteration is usually suﬃcient to get good re-
sults. Moreover, it is important to note that after updating
a latent vector, we need to update the S cache accordingly.
Weight of New Interactions. In an online system, new
interactions are more reﬂective of a user’s short-term inter-
est. Comparing to the historical interactions used in oﬄine
training, fresh data should be assigned a higher weight for
predicting user’s future action. We assign a weight wnew to
each new interaction (line 4 of Algorithm 2) as a tunable pa-
rameter. Later in Section 5.3, we investigate how the setting
of this parameter impacts online learning performance.

Time Complexity. The incremental update for a new
interaction (u, i) can be done in O(K 2+(|Ru|+|Ri|)K) time.
It is worth noting that the cost depends on the number of
observed interactions for u and i, while being independent
with number of total interactions, users and items. This lo-
calized complexity make the online learning algorithm suit-
able to deployment in industrial use, as the complex software
stack that deals with data dependencies [29] can be avoided.

5. EXPERIMENTS

We begin by introducing the experimental settings. Then
we perform an empirical study with the traditional oﬄine
protocol, followed by a more realistic online protocol.

5.1 Experimental Settings

Datasets. We evaluate on two publicly accessible datasets:
Yelp1 and Amazon Movies2. We transform the review dataset
into implicit data, where each entry is marked as 0/1 indi-
cating whether the user reviewed the item. Since the high
sparsity of the original datasets makes it diﬃcult to evaluate
recommendation algorithms (e.g., over half users have only
one review), we follow the common practice [25] to ﬁlter out

1We used the Yelp Challenge dataset downloaded on Octo-
ber 2015 that contained 1.6 million reviews: http://www.yelp.
com/dataset challenge
2

http://snap.stanford.edu/data/web-Amazon-links.html

Table 2: Statistics of the evaluation datasets.
Dataset Review# Item# User# Sparsity
Yelp
Amazon

731,671
5,020,705

99.89%
99.94 %

25,677
117,176

25,815
75,389

users and items with less than 10 interactions. Table 2 sum-
marizes the statistics of the ﬁltered datasets.

Methodology. We evaluate using two protocols:
- Oﬄine Protocol. We adopt the leave-one-out evalu-
ation, where the latest interaction of each user is held out
for prediction and the models are trained on the remain-
ing data. Although it is a widely used evaluation protocol
in the literature [9, 25], we point out that it is an artiﬁ-
cial split that does not correspond to the real recommenda-
tion scenario. In addition, the new users problem is averted
in this evaluation, as each test user has a training history.
Thus this protocol only evaluates an algorithm’s capability
in providing one-shot recommendation for existing users by
leveraging the static history data.

- Online Protocol. To create a more realistic recom-
mendation scenario, we simulate the dynamic data stream.
We ﬁrst sort all interactions in chronological order, training
models on the ﬁrst 90% of the interactions and holding out
the last 10% for testing. In the testing phase, given a test in-
teraction (i.e., a user–item pair) from the hold-out data, the
model ﬁrst recommends a ranked list of items to the user;
the performance is judged based on the ranked list. Then
the test interaction is fed into the model for an incremental
update. Note that with the global split by time, 14% and
57% test interactions are from new users for the Yelp and
Amazon dataset, respectively. Overall this protocol evalu-
ates an online learning algorithm’s eﬀectiveness in digesting
the dynamic new data.

To assess the ranked list with the ground-truth (GT) item
that user actually consumed, we adopt Hit Ratio (HR) and
Normalized Discounted Cumulative Gain (NDCG). We trun-
cate the ranked list at 100 for both metrics. HR measures
whether the ground truth item is present on the ranked list,
while NDCG accounts for the position of hit [9]. We report
the score averaged by all test interactions.

Baselines. We compare with the following methods:
- ALS [12]. This is the conventional ALS method that
optimizes the whole-data based MF. Due to the high time
complexity, this method is infeasible in a real-time dynamic
updating scenario, so we only evaluate it with the oﬄine
protocol.

- RCD [4]. This is the state-of-the-art implicit MF method
that has the same time complexity with eALS and is suit-
able for online learning. For the line search parameters, we
use the suggested values in the authors’ implementation3.

- BPR [25]. This is a sample-based method that opti-
mizes the pair-wise ranking between the positive and nega-
tive samples. It learns by SGD, which can be adjusted to
online incremental learning by [27]. We use a ﬁxed learning
rate, varying it and reporting the best performance.

Parameter Settings. For the weight of observed in-
teractions, we set it uniformly as 1, a default setting by
previous works [4, 25]. For regularization, we set λ as 0.01
for all methods for a fair comparison. All methods are im-
plemented in Java and running on the same machine (Intel

3

https://github.com/rdevooght/MF-with-prior-and-updates

(a) eALS vs. c0 (α = 0)

(b) eALS vs. α (c0 = 512)

(c) eALS vs. c0 (α = 0)

(d) eALS vs. α (c0 = 64)

Figure 1: Impact of weighting parameters c0 and α on eALS’s performance evaluated by oﬄine protocol.

(a) Iterations vs. HR

(b) Iterations vs. NDCG

(c) Iterations vs. HR

(d) Iterations vs. NDCG

Figure 2: Prediction accuracy of three whole-data based MF methods in each iteration (K = 128).

Xeon 2.67GHz CPU and 24GB RAM) in a single-thread for
a fair comparison on eﬃciency. As the ﬁndings are consis-
tent across the number of factors K, without any particular
outlier, we only show the results of K = 128, a relatively
large number that maintains good accuracy.

5.2 Ofﬂine Protocol

We ﬁrst study how does the weighting scheme on missing
data impact eALS’s performance. Then we compare with
the whole-data based implicit MF methods ALS and RCD,
as well as the sample-based ranking method BPR.

5.2.1 Weight of Missing Data

In Section 4.1, we propose an item popularity-aware weight-
ing strategy, which has two parameters: c0 determines the
overall weight of missing data and α controls the weight dis-
tribution. First, we set a uniform weight distribution (i.e.,
α = 0), varying c0 to study how does the weight of miss-
ing data impact the performance. For Yelp (Figure 1a),
the peak performance is achieved when c0 is around 512,
corresponding to that the weight of each zero entry is 0.02
(w0 = c0/N ); similarly for Amazon (Figure 1c), the optimal
c0 is around 64, corresponding to w0 = 0.0001. When c0
becomes smaller (where w0 is close to 0), the performance
degrades signiﬁcantly. This highlights the necessity of ac-
counting for the missing data when modeling implicit feed-
back for item recommendation. Moreover, when c0 is set
too large, the performance also suﬀers. Based on this ob-
servation, we believe the traditional SVD technique [2] that
treats all entries equally weighted will be suboptimal here.
Then, we set c0 to the best value (in the case of α =
0), varying α to check the performance change. As can be
seen from Figure 1b and 1d, the performance of eALS is
gradually improved with the increase of α, and the best
result is reached around 0.4. We further conducted the one-
sample paired t-test, verifying that the improvements are
statistically signiﬁcant (p-value < 0.01) for both metrics on
the two datasets. This indicates the eﬀectiveness of our
popularity-biased weighting strategy. Moreover, when α is

Figure 3: NDCG of whole-data based MF across K.

larger than 0.5, the performance starts to drop signiﬁcantly.
This reveals the drawback of over-weighting popular items
as negative instances, thus the importance of accounting for
less popular items with a proper weight.

In the following experiments, we ﬁx c0 and α according to
the best performance evaluated by HR, i.e., c0 = 512, α =
0.4 for Yelp and c0 = 64, α = 0.5 for Amazon.

5.2.2 Compare Whole-data based MF Methods

We performed the same grid search of w0 for RCD and

ALS and reported the best performance.

Convergence. Figure 2 shows the prediction accuracy
with respect to number of iterations. First, we see that
eALS achieves the best performance after converge. All
improvements are statistically signiﬁcant evidenced by the
one-sample paired t-test (p < 0.01). We believe the beneﬁts
mainly come from the popularity-aware objective function,
as both ALS and RCD apply a uniform weighting on the un-
knowns. Second, eALS and ALS converge faster than RCD.
We think the reason is that (e)ALS updates a parameter to
minimize the objective function of the current status, while
RCD updates towards the direction of the negative gradient,
which can be suboptimal. On Amazon, RCD shows high but
turbulent NDCG in early iterations, while the low hit ratio
and later iterations indicate the high NDCG is unstable.
Finally, we point out that in optimizing the same objective
function, ALS outperforms RCD in most cases, demonstrat-
ing the advantage of ALS over the gradient descent learner.

(a) Iterations vs. HR

(b) Iterations vs. NDCG

(c) Iterations vs. HR

(d) Iterations vs. NDCG

Figure 4: Accuracy and convergence comparison of eALS with BPR of diﬀerent learning rate.

Accuracy vs. Number of Factors. Figure 3 shows
the prediction accuracy with varying number of factors K.
We only show the evaluation by NDCG as HR admits the
same trend. First, eALS consistently outperforms ALS and
RCD across K, demonstrating the eﬀectiveness of our eALS
method (n.b. although the three methods seem to perform
on par for Amazon at K = 128, their diﬀerence can be
clearly seen in Figure 2d). Second, all methods can be im-
proved signiﬁcantly with a larger K. Although a large K
might have the risk of overﬁtting, it can increase model’s
representation ability thus better prediction. Especially for
large datasets that can have millions of users and billions
of interactions, a large K is particularly important for the
accuracy of MF methods.

Eﬃciency. Analytically, ALS’s time complexity is O((M +
N )K 3 + |R|K 2), while eALS and RCD are K times faster.
To compare their eﬃciency empirically, we show the actual
training time per iteration in Table 3.

Table 3: Training time per iteration of diﬀerent
whole-based MF methods with varying K.

Yelp
eALS RCD ALS
10s
1s
46s
3s
221s
10s
23m
0.9m
2.5h
2m
25.4h
11m

K
32
64
128
256
512
1024
s, m, and h denote seconds, minutes and hours, respectively.

Amazon
eALS RCD ALS
74s
10s
4.8m
17s
21m
42s
2h
2.8m
11.6h
9m
74h
48m

9s
23s
72s
4m
12m
54m

1s
4s
13s
1m
2m
7m

As can be seen, with the increase of K, ALS takes much
longer time than eALS and RCD. Speciﬁcally, when K is
512, ALS requires 11.6 hours for one iteration on Amazon,
while eALS only takes 12 minutes. Although eALS does
not empirically show K times faster than ALS due to the
more eﬃcient matrix inversion implementation (we used the
fastest known algorithm [1] with time complexity around
O(K 2.376)), the speed-up is already very signiﬁcant. More-
over, as RCD and eALS have the same analytical time com-
plexity, their actual running time are in the same magnitude;
the minor diﬀerence can be caused by some implementation
details, such as the data structures and caches used.

5.2.3

eALS vs. BPR (sample-based)

Figure 4 plots the performance of BPR with diﬀerent
learning rates4 in each iteration. Note that we only run
eALS for 100 iterations, which are enough for eALS to con-
verge. First, it is clear that BPR’s performance is subjected
4We have also tried other intermediate values of learning
rates, and the ﬁndings are consistent. Thus, to make the
ﬁgure more clear, we only show three selected values.

to the choice of learning rate — Figure 4a and 4b show that
a higher learning rate leads to a faster convergence, while the
ﬁnal accuracy may be suﬀered. Second, we see that eALS
signiﬁcantly outperforms BPR on the Yelp dataset evaluated
by both measures (p < 0.001). For Amazon, eALS obtains
a much higher hit ratio but a lower NDCG score, indicat-
ing that most hits occur at a relatively low ranks for eALS.
Comparing with the performance of other whole-based MF
methods ALS and RCD (Figure 2), we draw the conclusion
that BPR is a weak performer in terms of the prediction
recall, while being a strong performer in terms of the preci-
sion at top ranks. We think BPR’s strength in ranking top
items is due to its optimization objective, which is a pair-
wise ranking function tailored for ranking correct item high.
In contrast, the regression-based objective is not directly op-
timized for ranking; instead, by account for all missing data
in regression, it better predicts user’s preference on uncon-
sumed items, leading to a better recall. This is consistent
with [2]’s ﬁnding in evaluating top-K recommendation.

We notice that BPR shows unusual NDCG spike in early
iterations on the Amazon dataset, however the performance
is unstable and goes down with more iterations. The same
phenomenon was also observed for another gradient descent
method RCD on the same dataset (see Figure 2d). We hy-
pothesize that it might be caused by some regularities in
the data. For example, we ﬁnd some Amazon users review
on a movie multiple times5. In early iterations, BPR ranks
these repeated items high, leading to a high but unstable
NDCG score. There might be other reasons responsible for
this, and we do not further explore here.

5.3 Online Protocol

In the evaluation of online protocol, we hold out the latest
10% interactions as the test set, training all methods on the
remaining 90% data with the best parameter settings evi-
denced by the oﬄine evaluation. We ﬁrst study the number
of online iterations required for eALS to converge. Then we
show how does the weight of new interactions impact the
performance. Lastly, we compare with dynamic MF meth-
ods RCD and BPR in the online learning scenario.

5.3.1 Number of Online Iterations

Figure 6 shows how does eALS’s accuracy change with
number of online iterations. Results at the 0-th iteration
benchmark the performance of the oﬄine trained model, as
no incremental update is performed. First, we can see that
the oﬄine trained model performs very poorly, highlight-

5Due to user’s repeat consumption behaviours, we do not
exclude training items when generating recommend list.

(a) Test # vs. HR

(b) Test # vs. NDCG

(c) Test # vs. HR

(d) Test # vs. NDCG

Figure 5: Performance evolution of eALS and other dynamic MF methods in online learning.

ing the importance of refreshing recommender model for an
online system with dynamic data. Second, we ﬁnd most
performance gain comes from the ﬁrst iteration, and more
iterations do not further improve. This is due to the fact
that only the local features regarding to the new interaction
are updated, and one eALS step on a latent factor can ﬁnd
the optimal solution with others ﬁxed. Thus, one iteration
is enough for eALS to learn from a new interaction incre-
mentally, making eALS very eﬃcient for learning online.

Figure 6: Impact of online iterations on eALS.

We have also investigated number of online iterations re-
quired for baselines RCD and BPR. RCD shows the same
trend that good prediction is obtained in the ﬁrst iteration.
While BPR requires more iterations, usually 5-10 iterations
to get a peak performance and more iterations will adversely
hurt the performance due to the local over-training.

5.3.2 Weight of New Interactions

To evaluate how does the weight of new interactions ef-
fect the online learning algorithms, we also apply the same
weight wnew on RCD. Note that the original RCD paper [4]
does not consider the weight of interactions; we encode wnew
the same way with eALS, revising the RCD learner to opti-
mize the weighted regression function.

Figure 7: Impact of wnew on eALS and RCD in online
learning evaluated by NDCG.

Figure 7 shows the performance evaluated by NDCG (re-
sults of HR show the same trend thus omitted for space).
Setting wnew to 1 signiﬁes that new interaction is assigned a
same weight with the old training interaction. As expected,
with a modest increasing on wnew, the prediction of both

models is gradually improved, demonstrating the usefulness
of strengthening user’s short-term interest. The peak per-
formance is obtained around 4, where eALS shows better
prediction than RCD. Overly increasing wnew will adversely
hurt the performance, admitting the utility of user’s histor-
ical data used in oﬄine training. Overall, this experiment
indicates the importance of balancing user’s short-term and
long-term interest for quality recommendation.

5.3.3 Performance Comparison

With the simulated data stream, we show the performance
evolution with respect to number of test instances in Figure
5. First, eALS consistently outperforms RCD and BPR evi-
denced by both measures, and one-sample paired t-test ver-
iﬁes that all improvements are statistically signiﬁcant with
p < 0.001. BPR betters RCD for Yelp, while underper-
forms for Amazon. Second, we observe the trend that the
performance of dynamic learning ﬁrst decreases, and then
increases before becoming stable. This is caused by the new
users problem — when there are few feedback for a user, the
model can not personalize the user’s preference eﬀectively;
with more feedbacks streaming in, the model can adapt itself
to improve the preference modeling accordingly. To show
this, we further breakdown the results of eALS by number
of past interactions of test user in Figure 8.

Figure 8: Results breakdown of eALS by # of past
interactions of test user. Note: Interaction # > 0
denotes the performance for non-cold-start users.

It is clear that when there are no historical feedback for
a test user (i.e., user cold-start cases), the performance is
very poor — no better than random. After the ﬁrst interac-
tion streams in, the prediction is signiﬁcantly improved; and
with more interactions, the performance is further improved.
This highlights the importance of incorporating instanta-
neous user feedback into the model, especially for cold-start
or sparse users that have few history in training.

6. CONCLUSION AND FUTURE WORK

We study the problem of learning MF models from im-
plicit feedback. In contrast to previous work that applied

a uniform weight on missing data, we propose to weight
missing data based on the popularity of items. To address
the key eﬃciency challenge in optimization, we develop a
new learning algorithm — eALS — which eﬀectively learns
parameters by performing coordinate descent with memo-
ization. For online learning, we devise an incremental up-
date strategy for eALS to adapt dynamic data in real-time.
Experiments with both oﬄine and online protocols demon-
strate promising results. Importantly, our work makes MF
more practical to use for modeling implicit data, along two
dimensions. First, we investigate a new paradigm to deal
with missing data which can easily incorporate prior do-
main knowledge. Second, eALS is embarrassingly parallel,
making it attractive for large-scale industrial deployment.

We plan to study the optimal weighting strategy for online
data as a way to explore user’s short-term interest. Along
the technical line, we explored the element-wise ALS learner
in its basic MF form and solved the eﬃciency challenge in
handling missing data. To make our method more applica-
ble to real-world settings, we plan to encode side information
such as user social contexts [8] and reviews [9] by extending
eALS to more generic models, such as collective factoriza-
tion [11] and Factorization machines [26]. In addition, we
will study binary coding for MF on implicit data, since a
recent advance [32] has shown that discrete latent factors
are beneﬁcial to collaborative ﬁltering for explicit ratings.

The strength of eALS can be applied to other domains,
owing to the universality of factorizing sparse data matrices.
For example, recent advances in natural language process-
ing [16] have shown the connection between neural word em-
beddings and MF on the word–context matrix. This bridge
nicely motivates several proposals to use MF to learn word
embeddings; however, when it comes to handling missing
data, they have either ignored [22] or equally weighted the
missing entries, similar to traditional SVD [16]. It will be in-
teresting to see whether eALS will also improve these tasks.
Acknowledgement
The authors would like to thank the additional discussion
and help from Steﬀen Rendle, Bhargav Kanagal, Immanuel
Bayer, Tao Chen, Ming Gao and Jovian Lin.

7. REFERENCES
[1] D. Coppersmith and S. Winograd. Matrix multiplication

via arithmetic progressions. J. Symb. Comput.,
9(3):251–280, 1990.

[2] P. Cremonesi, Y. Koren, and R. Turrin. Performance of

recommender algorithms on top-n recommendation tasks.
In RecSys 2010, pages 39–46.

[3] A. S. Das, M. Datar, A. Garg, and S. Rajaram. Google

news personalization: Scalable online collaborative ﬁltering.
In WWW 2007, pages 271–280.

[4] R. Devooght, N. Kourtellis, and A. Mantrach. Dynamic
matrix factorization with priors on unknown values. In
KDD 2015, pages 189–198.

[5] E. Diaz-Aviles, L. Drumond, L. Schmidt-Thieme, and

W. Nejdl. Real-time top-n recommendation in social
streams. In RecSys 2012, pages 59–66.

[6] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient

methods for online learning and stochastic optimization. J.
Mach. Learn. Res., 12:2121–2159, 2011.

[7] R. Gemulla, E. Nijkamp, P. J. Haas, and Y. Sismanis.

Large-scale matrix factorization with distributed stochastic
gradient descent. In KDD 2011, pages 69–77.

[8] X. Geng, H. Zhang, J. Bian, and T.-S. Chua. Learning

image and user features for recommendation in social
networks. In ICCV 2015, pages 4274–4282.

[9] X. He, T. Chen, M.-Y. Kan, and X. Chen. Trirank:

Review-aware explainable recommendation by modeling
aspects. In CIKM 2015, pages 1661–1670.

[10] X. He, M. Gao, M.-Y. Kan, Y. Liu, and K. Sugiyama.

Predicting the popularity of web 2.0 items based on user
comments. In SIGIR 2014, pages 233–242.

[11] X. He, M.-Y. Kan, P. Xie, and X. Chen. Comment-based
multi-view clustering of web 2.0 items. In Proc. of WWW
’14, pages 771–782, 2014.

[12] Y. Hu, Y. Koren, and C. Volinsky. Collaborative ﬁltering
for implicit feedback datasets. In ICDM 2008, pages
263–272.

[13] Y. Huang, B. Cui, W. Zhang, J. Jiang, and Y. Xu.

Tencentrec: Real-time stream recommendation in practice.
In SIGMOD 2015, pages 227–238.

[14] Y. Koren. Collaborative ﬁltering with temporal dynamics.

In KDD 2009, pages 447–456.

[15] Y. Koren and R. Bell. Advances in collaborative ﬁltering.
In Recommender systems handbook, pages 145–186.
Springer, 2011.

[16] O. Levy and Y. Goldberg. Neural word embedding as
implicit matrix factorization. In NIPS 2014, pages
2177–2185.

[17] G. Ling, H. Yang, I. King, and M. R. Lyu. Online learning
for collaborative ﬁltering. In IJCNN 2012, pages 1–8.
[18] B. M. Marlin, R. S. Zemel, S. Roweis, and M. Slaney.
Collaborative ﬁltering and the missing at random
assumption. In UAI 2007, pages 267–276.

[19] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and

J. Dean. Distributed representations of words and phrases
and their compositionality. In NIPS 2013, pages 3111–3119.

[20] R. Pan and M. Scholz. Mind the gaps: Weighting the

unknown in large-scale one-class collaborative ﬁltering. In
KDD 2009, pages 667–676.

[21] R. Pan, Y. Zhou, B. Cao, N. Liu, R. Lukose, M. Scholz,
and Q. Yang. One-class collaborative ﬁltering. In ICDM
2008, pages 502–511.

[22] J. Pennington, R. Socher, and C. D. Manning. Glove:

Global vectors for word representation. In EMNLP 2014,
pages 1532–1543.

[23] I. Pil´aszy, D. Zibriczky, and D. Tikk. Fast als-based matrix
factorization for explicit and implicit feedback datasets. In
RecSys 2010, pages 71–78.

[24] S. Rendle and C. Freudenthaler. Improving pairwise

learning for item recommendation from implicit feedback.
In WSDM 2014, pages 273–282.

[25] S. Rendle, C. Freudenthaler, Z. Gantner, and

L. Schmidt-Thieme. Bpr: Bayesian personalized ranking
from implicit feedback. In UAI 2009, pages 452–461.

[26] S. Rendle, Z. Gantner, C. Freudenthaler, and

L. Schmidt-Thieme. Fast context-aware recommendations
with factorization machines. In SIGIR 2011, pages 635–644.

[27] S. Rendle and L. Schmidt-Thieme. Online-updating

regularized kernel matrix factorization models for large
scale recommender systems. In RecSys 2008, pages 251–258.

[28] P. Richt´arik and M. Tak´aˇc. Iteration complexity of

randomized block-coordinate descent methods for
minimizing a composite function. Math. Prog., 2014.
[29] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips,

D. Ebner, V. Chaudhary, and M. Young. Machine learning:
The high interest credit card of technical debt. In SE4ML
(NIPS 2014 Workshop), 2014.

[30] H. Steck. Training and testing of recommender systems on
data missing not at random. In KDD 2010, pages 713–722.

[31] M. Volkovs and G. W. Yu. Eﬀective latent models for

binary feedback in recommender systems. In SIGIR 2015,
pages 313–322.

[32] H. Zhang, F. Shen, W. Liu, X. He, H. Luan, and T.-S.
Chua. Discrete collaborative ﬁltering. In SIGIR 2016.

Fast Matrix Factorization for Online Recommendation
with Implicit Feedback∗

Xiangnan He

Hanwang Zhang

Min-Yen Kan

Tat-Seng Chua

School of Computing, National University of Singapore
{xiangnan, hanwang, kanmy, chuats}@comp.nus.edu.sg

7
1
0
2
 
g
u
A
 
6
1
 
 
]

R

I
.
s
c
[
 
 
1
v
4
2
0
5
0
.
8
0
7
1
:
v
i
X
r
a

ABSTRACT
This paper contributes improvements on both the eﬀective-
ness and eﬃciency of Matrix Factorization (MF) methods
for implicit feedback. We highlight two critical issues of ex-
isting works. First, due to the large space of unobserved
feedback, most existing works resort to assign a uniform
weight to the missing data to reduce computational com-
plexity. However, such a uniform assumption is invalid in
real-world settings. Second, most methods are also designed
in an oﬄine setting and fail to keep up with the dynamic
nature of online data.

We address the above two issues in learning MF models
from implicit feedback. We ﬁrst propose to weight the miss-
ing data based on item popularity, which is more eﬀective
and ﬂexible than the uniform-weight assumption. However,
such a non-uniform weighting poses eﬃciency challenge in
learning the model. To address this, we speciﬁcally de-
sign a new learning algorithm based on the element-wise
Alternating Least Squares (eALS) technique, for eﬃciently
optimizing a MF model with variably-weighted missing data.
We exploit this eﬃciency to then seamlessly devise an incre-
mental update strategy that instantly refreshes a MF model
given new feedback. Through comprehensive experiments
on two public datasets in both oﬄine and online protocols,
we show that our eALS method consistently outperforms
state-of-the-art implicit MF methods. Our implementation
is available at https://github.com/hexiangnan/sigir16-eals.

Keywords
Matrix Factorization, Implicit Feedback, Item Recommen-
dation, Online Learning, ALS, Coordinate Descent

1.

INTRODUCTION

User personalization has become prevalent in modern rec-
ommender system. It helps to capture users’ individualized

∗NExT research is supported by the National Research
Foundation, Prime Minister’s Oﬃce, Singapore under its
IRC@SG Funding Initiative.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’16, July 17-21, 2016, Pisa, Italy
c(cid:13) 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2911489

preferences and has been shown to increase both satisfac-
tion for users and revenue for content providers. Among
its various methods, matrix factorization (MF) is the most
popular and eﬀective technique that characterizes users and
items by vectors of latent factors [15, 32]. Early work on
MF algorithms for recommendation [14, 27] have largely fo-
cused on explicit feedback, where users’ ratings that directly
reﬂect their preference on items are provided. These works
formulated recommendation as a rating prediction problem
for which the large volume of unobserved ratings (i.e., miss-
ing data) are assumed to be extraneous for modeling user
preference [12]. This greatly reduces the modeling workload,
and many sophisticated methods have been devised, such as
SVD++ [15] and timeSVD [14].

However, explicit ratings are not always available in many
applications; more often, users interact with items through
implicit feedback, e.g., users’ video viewing and prod-
uct purchase history. Compared to explicit ratings, implicit
feedback is easier to collect for content providers, but more
challenging to utilize due to the natural scarcity of negative
feedback.
It has been shown that modeling only the ob-
served, positive feedback results in biased representations in
user proﬁles [4, 12]; e.g., Marlin et al. [18] ﬁnds that users
listen to music they expect to like and avoid the genres they
dislike, leading to a severe bias in the observed data.

To solve the problem of lacking negative feedback (also
known as the one-class problem [21]), a popular solution
is to model all the missing data as negative feedback [12].
However, this adversely degrades the learning eﬃciency due
to the full consideration of both observed and missing data.
More importantly, the low eﬃciency makes it even more dif-
ﬁcult to deploy implicit MF method online [29]. In practical
recommender systems where new users, items and interac-
tions are continuously streaming in, it is crucial to refresh
the underlying model in real-time to best serve users. In this
work, we concern the above two challenging problems of the
MF method — implicit feedback and online learning. We
note that we are not the ﬁrst to consider both aspects for
MF, as a recent work by Devooght et al. [4] has proposed an
eﬃcient implicit MF method for learning with dynamic data.
However, we argue that Devooght’s method [4] models miss-
ing data in an unrealistic, suboptimal way. Speciﬁcally, it
assigns a uniform weight to the missing data, assuming that
the missing entries are equally likely to be negative feedback.
However, such an assumption limits model’s ﬁdelity and ﬂex-
ibility for real applications. For example, content providers
usually know which items have been frequently featured to
users but seldom clicked; such items are more likely to be

true negative assessments and should be weighted higher
than others. In addition, Devooght’s method learns param-
eters through gradient descent, requiring an expensive line
search to determine the best learning rate at each step.

We propose a new MF method aimed at learning from im-
plicit feedback eﬀectively while satisfying the requirement of
online learning. We develop a new learning algorithm that
eﬃciently optimizes the implicit MF model without impos-
ing a uniform-weight restriction on missing data.
In par-
ticular, we assign the weight of missing data based on the
popularity of items, which is arguably more eﬀective than
the previous methods [4, 12, 23, 30, 31] that are limited by
the uniformity assumption. Our eALS algorithm is fast in
accounting for missing data — analytically K times faster
than ALS [12] where K denotes number of latent factors
— the same time complexity with the recent dynamic MF
solution [4]. This level of eﬃciency makes eALS suitable
for learning online, for which we develop an incremental
update strategy that instantly refreshes model parameters
given new incoming data. Another key advantage of eALS
is that it works without learning rate, bypassing the well-
known diﬃculty for tuning gradient descent methods such
as [4] and Stochastic Gradient Descent (SGD) [25].
We summarize our key contributions as follows.
1. We propose an item popularity-aware weighting scheme
on the full missing data that eﬀectively tailors the MF
model for learning from implicit feedback.

2. We develop a new algorithm for learning model pa-
rameters eﬃciently and devise an incremental update
strategy to support real-time online learning.

3. We conduct extensive experiments with both oﬄine
and online protocols on two real-world datasets, show-
ing that our method consistently outperforms state-of-
the-art implicit MF methods.

2. RELATED WORK

Handling missing data is obligatory for learning from im-
plicit data due to the lack of negative feedback. To this end,
two strategies have been proposed — sample based learn-
ing [21, 25] that samples negative instances from missing
data, or whole-data based learning [12, 30] that treats
all missing data as negative. Both methods have pros and
cons: sample-based methods are more eﬃcient by reduc-
ing negative examples in training, but risk decreasing the
model’s predictiveness; whole-based methods model the full
data with a potentially higher coverage, but ineﬃciency can
be an issue. To retain model’s ﬁdelity, we persist in the
whole-data based learning, developing a fast ALS-based al-
gorithm to resolve the ineﬃciency issue.

For existing whole-data based methods [4, 12, 23, 30, 31],
one major limitation is in the uniform weighting on missing
entries, which favors algorithm’s eﬃciency but limits model’s
ﬂexibility and extensibility. The only works that have con-
sidered non-uniform weighting are from Pan et al. [20, 21];
however their cubic time complexity w.r.t. K makes it un-
suitable to run on large-scale data [23], where a large number
of factors needs to be considered to gain improved perfor-
mance [31].

To optimize MF, various learners have been investigated,
including SGD [14, 25], Coordinate Descent (CD) [4, 32],
and Markov Chain Monto Carlo (MCMC) [26]. SGD is the
most popular one owing to the ease of derivation, however,

it is unsuitable for whole-data based MF [12] due to the
large amount of training instances (the full user–item inter-
action matrix is considered). ALS can be seen as an in-
stantiation of CD and has been widely used to solve the
whole-based MF [12, 20, 21, 30]; however, its ineﬃciency is
the main obstacle for practical use [23, 31]. To resolve this,
[23] describes an approximate solution to ALS. Recently, [4]
employs the Randomized block Coordinate Descent (RCD)
learner [28], reducing the complexity and applying it to a dy-
namic scenario. Similarly, [31] enriches the implicit feedback
matrix with neighbor-based similarly, followed by applying
unweighted SVD. Distinct from previous works, we propose
an eﬃcient element-wise ALS solution for the whole-data
based MF with non-uniform missing data, which has never
been studied before.

Another important aspect for practical recommender sys-
tem lies in handling the dynamic nature of incoming data,
for which timeliness is a key consideration. As it is pro-
hibitive to retrain the full model online, various works have
developed incremental learning strategies for neighbor-based
[13], graph-based [9], probabilistic [3] and MF [4, 5, 17, 27]
methods. For MF, diﬀerent learners have been studied for
online updating, including SGD [5, 27], RCD [4] and dual-
averaging [17]. To our knowledge, this work is the ﬁrst at-
tempt to exploit the ALS technique for online learning.

3. PRELIMINARIES

We ﬁrst introduce the whole-data based MF method for
learning from implicit data, highlighting the ineﬃciency is-
sue of the conventional ALS solution [12, 21]. Then we
describe eALS, an element-wise ALS learner [26] that can
reduce the time complexity to linearity w.r.t. number of
factors. Although the learner is generic in optimizing MF
with all kinds of weighting strategies, the form introduced
is costly in accounting for all missing data and is thus unre-
alistic for practical use. This defect motivates us to further
develop eALS to make it suitable for learning from implicit
feedback (details in Section 4.2).

3.1 MF Method for Implicit Feedback

We start by introducing some basic notation. For a user–
item interaction matrix R ∈ RM ×N , M and N denote the
number of users and items, respectively; R denotes the set
of user–item pairs whose values are non-zero. We reserve the
index u to denote a user and i to denote an item. Vector pu
denotes the latent feature vector for u, and set Ru denotes
the set of items that are interacted by u; similar notations
for qi and Ri. Matrices P ∈ RM ×K and Q ∈ RN ×K denote
the latent factor matrix for users and items.

Matrix factorization maps both users and items into a
joint latent feature space of K dimension such that interac-
tions are modeled as inner products in that space. Mathe-
matically, each entry rui of R is estimated as:

ˆrui =< pu, qi >= pT

u qi.

(1)

The item recommendation problem is formulated as estimat-
ing the scoring function ˆrui, which is used to rank items.
Note that this basic model subsumes the biased MF [14],
commonly used in modeling explicit ratings:

ˆrui = bu + bi+ < pB

u , qB

i >,

where bu (bi) captures the bias of user u (item i) in giving
(receiving) ratings. To recover it, set pu ← [pB
u , bu, 1] and

qi ← [qB
i , 1, bi]. As such, we adopt the basic MF model
to make notations simple and also to enable a fair compari-
son with baselines [4, 12] that also complied with the basic
model.

To learn model parameters, Hu et al. [12] introduced a
weighted regression function, which associates a conﬁdence
to each prediction in the implicit feedback matrix R:

J =

M
(cid:88)

N
(cid:88)

u=1

i=1

wui(rui − ˆrui)2 + λ(

||pu||2 +

||qi||2), (2)

M
(cid:88)

u=1

N
(cid:88)

i=1

where wui denotes the weight of entry rui and we use W =
[wui]M ×N to represent the weight matrix. λ controls the
strength of regularization, which is usually an L2 norm to
prevent overﬁtting. Note that in implicit feedback learning,
missing entries are usually assigned to a zero rui value but
non-zero wui weight, both crucial to performance.

3.2 Optimization by ALS

Alternating Least Square (ALS) is a popular approach
to optimize regression models such as MF and graph reg-
ularization [10]. It works by iteratively optimizing one pa-
rameter, while leaving the others ﬁxed. The prerequisite of
ALS is that the optimization sub-problem can be analyti-
cally solved. Here, we describe how Hu’s work [12] solves
this problem.

First, minimizing J with respect to user latent vector pu

is equivalent to minimizing:

Ju = ||Wu(ru − Qpu)||2 + λ||pu||2,
where Wu is a N × N diagonal matrix with W u
minimum is where the ﬁrst-order derivative is 0:

ii = wui. The

∂Ju
∂pu

= 2QT WuQpu − 2QT Wuru + 2λpu = 0

(3)

⇒ pu = (QT WuQ + λI)−1QT Wuru,

where I denotes the identity matrix. This analytical solution
is also known as the ridge regression [23]. Following the same
process, we can get the solution for qi.

3.2.1 Efﬁciency Issue with ALS

As we can see, in order to update a latent vector, in-
verting a K × K matrix is inevitable. Matrix inversion
is an expensive operation, usually assumed O(K 3) in time
complexity [12]. As such, updating one user latent vector
takes time O(K 3 + N K 2). Thus, the overall time complex-
ity of one iteration that updates all model parameters once
is O((M + N )K 3 + M N K 2). Clearly, this high complexity
makes the algorithm impractical to run on large-scale data,
where there can be millions of users and items and billions
of interactions.

Speed-up with Uniform Weighting. To reduce the
high time complexity, Hu et al. [12] applied a uniform weight
to missing entries; i.e., assuming that all zero entries in R
have a same weight w0. Through this simpliﬁcation, they
can speed up the computation with memoization:

QT WuQ = w0QT Q + QT (Wu − W0)Q,

(4)

where W0 is a diagonal matrix that each diagonal element is
w0. As QT Q is independent of u, it can be pre-computed for
updating all user latent vectors. Considering the fact that
Wu − W0 only has |Ru| non-zero entries, we can compute

Eq. (4) in O(|Ru|K 2) time. Thus, the time complexity of
ALS is reduced to O((M + N )K 3 + |R|K 2).

Even so, we argue that the O((M + N )K 3) term can be
a major cost when (M + N )K ≥ |R|.
In addition, the
O(|R|K 2) part is still much higher than in SGD [25], which
only requires O(|R|K) time. As a result, even with the ac-
celeration, ALS is still prohibitive for running on large data,
where large K is crucial as it can lead to better generaliz-
ability and thus better prediction performance. Moreover,
the uniform weighting assumption is usually invalid in real
applications and adversely degrades model’s predictiveness.
This thus motivates us to design an eﬃcient implicit MF
method not subject to uniform-weights.

3.3 Generic Element-wise ALS Learner

The bottleneck of the previous ALS solution lies in the ma-
trix inversion operation, which is due to the design that up-
dates the latent vector for a user (item) as a whole. As such,
it is natural to optimize parameters at the element level —
optimizing each coordinate of the latent vector, while leav-
ing the others ﬁxed [26]. To achieve this, we ﬁrst get the
derivative of objective function Eq. (2) with respect to puf :

∂J
∂puf

N
(cid:88)

i=1

= −2

(rui − ˆrf

ui)wuiqif + 2puf

wuiq2

if + 2λpuf ,

N
(cid:88)

i=1

where ˆrf
ui = ˆrui − puf qif , i.e., the prediction without the
component of latent factor f . By setting this derivative to
0, we obtain the solution of puf :

Similarly, we can get the solver for an item latent factor:

puf =

(cid:80)N

i=1(rui − ˆrf
(cid:80)N
i=1 wuiq2

ui)wuiqif
if + λ

.

qif =

(cid:80)M

u=1(rui − ˆrf
(cid:80)M
i=1 wuip2

ui)wuipuf
uf + λ

.

(5)

(6)

Given the closed-form solution that optimizes one param-
eter with other ﬁxed, the algorithm iteratively executes it
for all model parameters until a joint optimum is reached.
Due to the non-convexity of the objective function, critical
points where gradients vanish can be local minima.

Time Complexity. As can be seen, by performing opti-
mization at the element level, the expensive matrix inversion
can be avoided. A raw implementation takes O(M N K 2)
time for one iteration, directly speeding up ALS by eliminat-
ing the O(K 3) term. Moreover, by pre-computing ˆrui [26],
we can calculate ˆrf
ui in O(1) time rather than O(K). As
such, the complexity can be further reduced to O(M N K),
which is the same magnitude with evaluating all the user–
item predictions.

4. OUR IMPLICIT MF METHOD

We ﬁrst propose an item-oriented weighting scheme on
the missing data, and follow with a popularity-aware weight-
ing strategy, which is arguably more eﬀective than the uni-
form weighting for the recommendation task. Then, we de-
velop a fast eALS algorithm to optimize the objective func-
tion that signiﬁcantly reduces learning complexity compar-
ing with the conventional ALS [12] and generic element-wise
ALS learner [26]. Lastly, we discuss how to adjust the learn-
ing algorithm for real-time online learning.

(7)

the observed data part:

4.1

Item-Oriented Weighting on Missing Data
Due to the large space of items, the missing entries for a
user are a mixture of negative and unknown feedback. In
specifying the weight wui of missing entries, it is desired to
assign a higher weight to the negative feedback. However, it
is a well-known diﬃculty to diﬀerentiate the two cases. In
addition, as the interaction matrix R is usually large and
sparse, it will be too consuming to store each zero entry an
individualized weight. To this end, existing works [4, 12,
23, 30, 31] have applied a simple uniform weight on missing
entries, which are, however, suboptimal and non-extendable
for real applications.

Considering the ease of content providers in accessing neg-
ative information of the item side (e.g., which items have
been promoted to users but receive little interaction), we
believe it is more realistic to weight missing data based on
some item property. To capture this, we devise a more ﬁne-
grained objective function as follows:

(cid:88)

L =

wui(rui − ˆrui)2 +

(u,i)∈R

M
(cid:88)

u=1

+ λ(

||pu||2 +

||qi||2),

N
(cid:88)

i=1

M
(cid:88)

(cid:88)

u=1

i /∈Ru

ci ˆr2
ui

where ci denotes the conﬁdence that item i missed by users
is a true negative assessment, which can serve as a means to
encode domain knowledge from practitioners. It is clear that
the ﬁrst term denotes the prediction error of the observed
entries, which has been widely adopted in modeling explicit
ratings [15, 27]. The second term accounts for the miss-
ing data, which acts as the role of negative instances and is
crucial for recommendation from implicit feedback [12, 25].
Next, we present a domain-independent strategy to deter-
mine ci by leveraging a ubiquitous feature of modern Web
2.0 systems.

4.1.1 Popularity-aware Weighting Strategy

Existing visual interfaces of many Web 2.0 systems show-
case popular items in their recommendations. All other fac-
tors being equal, popular items are more likely to be known
by users in general [10], and thus it is reasonable to think
that a miss on a popular item is more probable to be truly
irrelevant (as opposed to unknown) to the user. To account
for this eﬀect, we parametrize ci based on item’s popularity:

ci = c0

f α
i
j=1 f α
j

,

(cid:80)N

(8)

where fi denotes the popularity of item i, given by its fre-
quency in the implicit feedback data: |Ri|/ (cid:80)N
j=1 |Rj|, and
c0 determines the overall weight of missing data. Exponent
α controls the signiﬁcance level of popular items over un-
popular ones — when α > 1 the weights of popular items
are promoted to strengthen the diﬀerence against unpop-
ular ones; while setting α within the lower range of (0, 1)
suppresses the weight of popular items and has a smoothing
eﬀect. We empirically ﬁnd α = 0.5 usually leads to good
results. Note that the uniform weighting is a special case by
setting α to 0 with w0 = c0/N .

Relationship to Negative Sampling. Our proposed
popularity-aware weighting strategy has the same intuition
with Rendle’s popularity-based oversampling [24] for learn-
ing BPR, which basically samples popular items as nega-

tive feedback with a higher probability. However, [24] em-
pirically shows the oversampling method underperforms the
basic uniform sampler. We suspect the reason comes from
the SGD learner, which will result in more gradient steps
on popular items, due to oversampling. As a result, popular
items may be over-trained locally at the expense of less pop-
ular items which would then be under-trained. To resolve
this, tricks like subsampling frequent items [19] and adaptive
learning rates like Adagrad [6] have been adopted in other
domains. As the focus of this paper is on whole-data based
implicit MF, we do not further explore the details of SGD. It
is worth pointing out that our proposed eALS learner avoids
these learning issues by an exact optimization on each model
parameter.

4.2 Fast eALS Learning Algorithm

We can speed up learning by avoiding the massive re-
peated computations introduced by the weighted missing
data. We detail the derivation process for puf ; where the
counterpart for qif is achieved likewise.

First, we rewrite the puf update rule Eq. (5) by separating

puf =

(cid:80)

i∈Ru
(cid:80)

(rui − ˆrf

ui)wuiqif − (cid:80)
if + (cid:80)

i /∈Ru

ˆrf
uiciqif
if + λ

i /∈Ru
ciq2

wuiq2

.

i∈Ru

Clearly, the computational bottleneck lies in the summa-
tion over missing data portion, which requires a traversal of
the whole negative space. We ﬁrst focus on the numerator:

(cid:88)

i /∈Ru

ˆrf
uiciqif =

ciqif

pukqik −

ˆrf
uiciqif

N
(cid:88)

i=1

(cid:88)

k(cid:54)=f

(cid:88)

=

puk

N
(cid:88)

k(cid:54)=f

i=1

(cid:88)

i∈Ru

(cid:88)

i∈Ru

ciqif qik −

ˆrf
uiciqif .

(9)

By this reformulation, we can see that the major computa-
tion — the (cid:80)N
i=1 ciqif qik term that iterates over all items
— is independent of u. However, a na¨ıve implementation
repeatedly computes it unnecessarily, when updating the la-
tent factors for diﬀerent users. Clearly, we can achieve a
signiﬁcant speed-up by memoizing it.

We deﬁne the Sq cache as Sq = (cid:80)N

i , which can
be pre-computed and used in updating the latent factors for
all users. Then, Eq. (9) can be evaluated as:

i=1 ciqiqT

(cid:88)

i /∈Ru

ˆrf
uiciqif =

puksq

f k −

ˆrf
uiciqif ,

(10)

(cid:88)

k(cid:54)=f

(cid:88)

i∈Ru

which can be done in O(K + |Ru|) time.

Similarly, we can apply the cache to speed up the calcu-

lation of denominator:

(cid:88)

i /∈Ru

N
(cid:88)

i=1

(cid:88)

i∈Ru

ciq2

if =

ciq2

if −

ciq2

if = sq

f f −

ciq2

if . (11)

(cid:88)

i∈Ru

To summarize the above memoization strategy, we give

the update rule for puf with the use of Sq cache:

puf =

(cid:80)

i∈Ru

[wuirui − (wui − ci)ˆrf
(wui − ci)q2

(cid:80)

ui]qif − (cid:80)
if + sq
f f + λ

i∈Ru

k(cid:54)=f puksq

kf

.

(12)

Algorithm 1: Fast eALS Learning algorithm.

Input: R, K, λ, W and item conﬁdence vector c;
Output: Latent feature matrix P and Q;

1 Randomly initialize P and Q ;
2 for (u, i)∈ R do ˆrui ← Eq. (1) ;
3 while Stopping criteria is not met do

(cid:46) O(|R|K)

// Update user factors
Sq = (cid:80)N
i=1 ciqiqT
;
for u ← 1 to M do

i

(cid:46) O(M K 2)
(cid:46) O(M K 2 + |R|K)

for f ← 1 to K do
for i ∈ Ru do ˆrf
puf ← Eq. (12) ;
for i ∈ Ru do ˆrui ← ˆrf

ui ← ˆrui − puf qif ;

(cid:46) O(K + |Ru|)

ui + puf qif ;

end

end
// Update item factors
Sp ← PT P ;
for i ← 1 to N do

(cid:46) O(N K 2)
(cid:46) O(N K 2 + |R|K)

for f ← 1 to K do
for u ∈ Ri do ˆrf
qif ← Eq. (13) ;
for u ∈ Ri do ˆrui ← ˆrf

ui ← ˆrui − puf qif ;

(cid:46) O(K + |Ri|)

ui + puf qif ;

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

end

end

19
20 end
21 return P and Q

Table 1: Time complexity of implicit MF methods.

Time Complexity
Method
O((M + N )K 3 + |R|K 2)
ALS (Hu et al.[12])
O(|R|K)
BPR (Rendle et al.[25])
O(K 3 + (M + N )K 2 + |R|K)
IALS1 (Pil´aszy et al.[23])
ii-SVD (Volkovs et al.[31]) O((M + N )K 2 + M N log K)
RCD (Devooght et al.[4])
eALS (Algorithm 1)

O((M + N )K 2 + |R|K)
O((M + N )K 2 + |R|K)

|R| denotes the number of non-zeros in user–item matrix R.

Similarly, we can derive the update rule for qif :

qif =

(cid:80)

u∈Ri

[wuirui − (wui − ci)ˆrf
(wui − ci)p2

(cid:80)

(cid:80)

ui]puf − ci
uf + cisp

f f + λ

u∈Ri

k(cid:54)=f qiksp

kf

,

(13)
kf denotes the (k, f )th element of the Sp cache, de-

where sp
ﬁned as Sp = PT P.

Algorithm 1 summarizes the accelerated algorithm for our
element-wise ALS learner, or eALS. For convergence, one
can either monitor the value of objective function on train-
ing set or check the prediction performance on a hold-out
validation data.

4.2.1 Discussion

Time Complexity.

In Algorithm 1, updating a user
latent factor takes O(K + |Ru|) time. Thus, one eALS iter-
ation takes O((M +N )K 2 +|R|K) time. Table 1 summarizes
the time complexity (of one iteration or epoch) of other MF
algorithms that are designed for implicit feedback.

Comparing with the vector-wise ALS [12, 21], our element-
wise ALS learner is K times faster. In addition, our pro-
posed eALS has the same time complexity with RCD [4],

being faster than ii-SVD [31], another recent solution. RCD
is a state-of-the-art learner for whole-data based MF, which
performs a gradient descent step on a randomly chosen la-
tent vector. Since it requires a good learning rate, the work
[4] adaptively determines it by a line search in each gradient
step, which essentially chooses the learning rate that leads to
the steepest descent among pre-deﬁned candidates. A major
advantage of eALS has over RCD is that it avoids the need
for a learning rate by an exact optimization in each param-
eter update, arguably more eﬀective and easier to use than
RCD. The most eﬃcient algorithm is BPR, which applies
the SGD learner on sampled, partial missing data only.

Computing the Objective Function. Evaluating the
objective function is important to check the convergence of
iterations and also to verify the correctness of implementa-
tion. A direct calculation takes O(M N K) time, requiring a
full estimation on the R matrix. Fortunately, with the item-
oriented weighting, we can similarly exploit the sparseness
of R for acceleration. To achieve this, we reformulate the
loss of the missing data part that causes the major cost:

M
(cid:88)

(cid:88)

u=1

i /∈Ru

ci ˆr2

ui =

u Sqpu −
pT

M
(cid:88)

u=1

(cid:88)

ci ˆr2

ui.

(u,i)∈R

(14)

By reusing Sq and the prediction cache ˆrui, we can calculate
the objective function in O(|R| + M K 2) time, much faster
than with direct calculation.

Parallel Learning. The iterations of eALS can be easily
parallelized. First, computing the S caches (line 4 and 12)
is the standard matrix multiplication operation, for which
modern matrix toolkits provide very eﬃcient and parallelized
implementation. Second, in updating the latent vectors for
diﬀerent users (line 5-11), the shared parameters are either
independent with each other (i.e., ˆrui) or remaining un-
changed (i.e., Sq). This nice property means that an exact
parallel solution can be obtained by separating the updates
by users; that is, letting diﬀerent workers update the model
parameters for disjoint sets of users. The same parallelism
can also be achieved in updating item latent vectors.

This is an advantage over the commonly-used SGD learner,
which is a stochastic method that updates model parame-
ters given a training instance.
In SGD, diﬀerent gradient
steps can inﬂuence with each other and there is no exact
way to separate the updates for workers. Thus, sophisti-
cated strategies are required to control the possible losses
introduced by parallelization [7]. Our proposed eALS solu-
tion optimizes by coordinate descent where in each step a
dedicated parameter is updated, making the algorithm em-
barrassingly parallel without any approximate loss.

4.3 Online Update

In practice, after a recommender model is trained oﬄine
on historical data, it will be used online and will need to
adapt to best serve users. Here, we consider the online
learning scenario that refreshes model parameters given a
new user–item interaction.

Incremental Updating. Let ˆP and ˆQ denote the model
parameters learnt from oﬄine training, and (u, i) denotes
the new interaction streamed in. To approximate the model
parameters in accounting for the new interaction, we per-
form optimization steps for pu and qi only. The underlying
assumption is that the new interaction should not change ˆP
and ˆQ too much from a global perspective, while it should

Algorithm 2: Online Incremental Updates for eALS.
Input: ˆP, ˆQ, new interaction (u, i) and its weight wnew
Output: Refreshed parameters P and Q;

1 P ← ˆP; Q ← ˆQ; ;
2 If u is a new user do Randomly initialize pu ;
3 If i is a new item do Randomly initialize qi ;
4 rui ← 1;
5 while Stopping criteria is not met do

ˆrui ← Eq. (1); wui ← wnew;

6

7

8

// Line 6-10 of Algorithm 1
update user(u); ;
update cache(u, Sp); ;
// Line 14-18 of Algorithm 1
update item(i); ;
update cache(i, Sq); ;

(cid:46) O(K 2 + |Ru|K)
(cid:46) O(K 2)

(cid:46) O(K 2 + |Ri|K)
(cid:46) O(K 2)

9
10 end
11 return P and Q

change the local features for u and i signiﬁcantly. Particu-
larly, when u is a new user, executing the local updates will
force pu close to qi, which meets the expectation of latent
factor model. The new item case is similar.

Algorithm 2 summarizes the incremental learning strat-
egy for eALS. For the stopping criteria, our empirical study
shows that one iteration is usually suﬃcient to get good re-
sults. Moreover, it is important to note that after updating
a latent vector, we need to update the S cache accordingly.
Weight of New Interactions. In an online system, new
interactions are more reﬂective of a user’s short-term inter-
est. Comparing to the historical interactions used in oﬄine
training, fresh data should be assigned a higher weight for
predicting user’s future action. We assign a weight wnew to
each new interaction (line 4 of Algorithm 2) as a tunable pa-
rameter. Later in Section 5.3, we investigate how the setting
of this parameter impacts online learning performance.

Time Complexity. The incremental update for a new
interaction (u, i) can be done in O(K 2+(|Ru|+|Ri|)K) time.
It is worth noting that the cost depends on the number of
observed interactions for u and i, while being independent
with number of total interactions, users and items. This lo-
calized complexity make the online learning algorithm suit-
able to deployment in industrial use, as the complex software
stack that deals with data dependencies [29] can be avoided.

5. EXPERIMENTS

We begin by introducing the experimental settings. Then
we perform an empirical study with the traditional oﬄine
protocol, followed by a more realistic online protocol.

5.1 Experimental Settings

Datasets. We evaluate on two publicly accessible datasets:
Yelp1 and Amazon Movies2. We transform the review dataset
into implicit data, where each entry is marked as 0/1 indi-
cating whether the user reviewed the item. Since the high
sparsity of the original datasets makes it diﬃcult to evaluate
recommendation algorithms (e.g., over half users have only
one review), we follow the common practice [25] to ﬁlter out

1We used the Yelp Challenge dataset downloaded on Octo-
ber 2015 that contained 1.6 million reviews: http://www.yelp.
com/dataset challenge
2

http://snap.stanford.edu/data/web-Amazon-links.html

Table 2: Statistics of the evaluation datasets.
Dataset Review# Item# User# Sparsity
Yelp
Amazon

731,671
5,020,705

99.89%
99.94 %

25,677
117,176

25,815
75,389

users and items with less than 10 interactions. Table 2 sum-
marizes the statistics of the ﬁltered datasets.

Methodology. We evaluate using two protocols:
- Oﬄine Protocol. We adopt the leave-one-out evalu-
ation, where the latest interaction of each user is held out
for prediction and the models are trained on the remain-
ing data. Although it is a widely used evaluation protocol
in the literature [9, 25], we point out that it is an artiﬁ-
cial split that does not correspond to the real recommenda-
tion scenario. In addition, the new users problem is averted
in this evaluation, as each test user has a training history.
Thus this protocol only evaluates an algorithm’s capability
in providing one-shot recommendation for existing users by
leveraging the static history data.

- Online Protocol. To create a more realistic recom-
mendation scenario, we simulate the dynamic data stream.
We ﬁrst sort all interactions in chronological order, training
models on the ﬁrst 90% of the interactions and holding out
the last 10% for testing. In the testing phase, given a test in-
teraction (i.e., a user–item pair) from the hold-out data, the
model ﬁrst recommends a ranked list of items to the user;
the performance is judged based on the ranked list. Then
the test interaction is fed into the model for an incremental
update. Note that with the global split by time, 14% and
57% test interactions are from new users for the Yelp and
Amazon dataset, respectively. Overall this protocol evalu-
ates an online learning algorithm’s eﬀectiveness in digesting
the dynamic new data.

To assess the ranked list with the ground-truth (GT) item
that user actually consumed, we adopt Hit Ratio (HR) and
Normalized Discounted Cumulative Gain (NDCG). We trun-
cate the ranked list at 100 for both metrics. HR measures
whether the ground truth item is present on the ranked list,
while NDCG accounts for the position of hit [9]. We report
the score averaged by all test interactions.

Baselines. We compare with the following methods:
- ALS [12]. This is the conventional ALS method that
optimizes the whole-data based MF. Due to the high time
complexity, this method is infeasible in a real-time dynamic
updating scenario, so we only evaluate it with the oﬄine
protocol.

- RCD [4]. This is the state-of-the-art implicit MF method
that has the same time complexity with eALS and is suit-
able for online learning. For the line search parameters, we
use the suggested values in the authors’ implementation3.

- BPR [25]. This is a sample-based method that opti-
mizes the pair-wise ranking between the positive and nega-
tive samples. It learns by SGD, which can be adjusted to
online incremental learning by [27]. We use a ﬁxed learning
rate, varying it and reporting the best performance.

Parameter Settings. For the weight of observed in-
teractions, we set it uniformly as 1, a default setting by
previous works [4, 25]. For regularization, we set λ as 0.01
for all methods for a fair comparison. All methods are im-
plemented in Java and running on the same machine (Intel

3

https://github.com/rdevooght/MF-with-prior-and-updates

(a) eALS vs. c0 (α = 0)

(b) eALS vs. α (c0 = 512)

(c) eALS vs. c0 (α = 0)

(d) eALS vs. α (c0 = 64)

Figure 1: Impact of weighting parameters c0 and α on eALS’s performance evaluated by oﬄine protocol.

(a) Iterations vs. HR

(b) Iterations vs. NDCG

(c) Iterations vs. HR

(d) Iterations vs. NDCG

Figure 2: Prediction accuracy of three whole-data based MF methods in each iteration (K = 128).

Xeon 2.67GHz CPU and 24GB RAM) in a single-thread for
a fair comparison on eﬃciency. As the ﬁndings are consis-
tent across the number of factors K, without any particular
outlier, we only show the results of K = 128, a relatively
large number that maintains good accuracy.

5.2 Ofﬂine Protocol

We ﬁrst study how does the weighting scheme on missing
data impact eALS’s performance. Then we compare with
the whole-data based implicit MF methods ALS and RCD,
as well as the sample-based ranking method BPR.

5.2.1 Weight of Missing Data

In Section 4.1, we propose an item popularity-aware weight-
ing strategy, which has two parameters: c0 determines the
overall weight of missing data and α controls the weight dis-
tribution. First, we set a uniform weight distribution (i.e.,
α = 0), varying c0 to study how does the weight of miss-
ing data impact the performance. For Yelp (Figure 1a),
the peak performance is achieved when c0 is around 512,
corresponding to that the weight of each zero entry is 0.02
(w0 = c0/N ); similarly for Amazon (Figure 1c), the optimal
c0 is around 64, corresponding to w0 = 0.0001. When c0
becomes smaller (where w0 is close to 0), the performance
degrades signiﬁcantly. This highlights the necessity of ac-
counting for the missing data when modeling implicit feed-
back for item recommendation. Moreover, when c0 is set
too large, the performance also suﬀers. Based on this ob-
servation, we believe the traditional SVD technique [2] that
treats all entries equally weighted will be suboptimal here.
Then, we set c0 to the best value (in the case of α =
0), varying α to check the performance change. As can be
seen from Figure 1b and 1d, the performance of eALS is
gradually improved with the increase of α, and the best
result is reached around 0.4. We further conducted the one-
sample paired t-test, verifying that the improvements are
statistically signiﬁcant (p-value < 0.01) for both metrics on
the two datasets. This indicates the eﬀectiveness of our
popularity-biased weighting strategy. Moreover, when α is

Figure 3: NDCG of whole-data based MF across K.

larger than 0.5, the performance starts to drop signiﬁcantly.
This reveals the drawback of over-weighting popular items
as negative instances, thus the importance of accounting for
less popular items with a proper weight.

In the following experiments, we ﬁx c0 and α according to
the best performance evaluated by HR, i.e., c0 = 512, α =
0.4 for Yelp and c0 = 64, α = 0.5 for Amazon.

5.2.2 Compare Whole-data based MF Methods

We performed the same grid search of w0 for RCD and

ALS and reported the best performance.

Convergence. Figure 2 shows the prediction accuracy
with respect to number of iterations. First, we see that
eALS achieves the best performance after converge. All
improvements are statistically signiﬁcant evidenced by the
one-sample paired t-test (p < 0.01). We believe the beneﬁts
mainly come from the popularity-aware objective function,
as both ALS and RCD apply a uniform weighting on the un-
knowns. Second, eALS and ALS converge faster than RCD.
We think the reason is that (e)ALS updates a parameter to
minimize the objective function of the current status, while
RCD updates towards the direction of the negative gradient,
which can be suboptimal. On Amazon, RCD shows high but
turbulent NDCG in early iterations, while the low hit ratio
and later iterations indicate the high NDCG is unstable.
Finally, we point out that in optimizing the same objective
function, ALS outperforms RCD in most cases, demonstrat-
ing the advantage of ALS over the gradient descent learner.

(a) Iterations vs. HR

(b) Iterations vs. NDCG

(c) Iterations vs. HR

(d) Iterations vs. NDCG

Figure 4: Accuracy and convergence comparison of eALS with BPR of diﬀerent learning rate.

Accuracy vs. Number of Factors. Figure 3 shows
the prediction accuracy with varying number of factors K.
We only show the evaluation by NDCG as HR admits the
same trend. First, eALS consistently outperforms ALS and
RCD across K, demonstrating the eﬀectiveness of our eALS
method (n.b. although the three methods seem to perform
on par for Amazon at K = 128, their diﬀerence can be
clearly seen in Figure 2d). Second, all methods can be im-
proved signiﬁcantly with a larger K. Although a large K
might have the risk of overﬁtting, it can increase model’s
representation ability thus better prediction. Especially for
large datasets that can have millions of users and billions
of interactions, a large K is particularly important for the
accuracy of MF methods.

Eﬃciency. Analytically, ALS’s time complexity is O((M +
N )K 3 + |R|K 2), while eALS and RCD are K times faster.
To compare their eﬃciency empirically, we show the actual
training time per iteration in Table 3.

Table 3: Training time per iteration of diﬀerent
whole-based MF methods with varying K.

Yelp
eALS RCD ALS
10s
1s
46s
3s
221s
10s
23m
0.9m
2.5h
2m
25.4h
11m

K
32
64
128
256
512
1024
s, m, and h denote seconds, minutes and hours, respectively.

Amazon
eALS RCD ALS
74s
10s
4.8m
17s
21m
42s
2h
2.8m
11.6h
9m
74h
48m

9s
23s
72s
4m
12m
54m

1s
4s
13s
1m
2m
7m

As can be seen, with the increase of K, ALS takes much
longer time than eALS and RCD. Speciﬁcally, when K is
512, ALS requires 11.6 hours for one iteration on Amazon,
while eALS only takes 12 minutes. Although eALS does
not empirically show K times faster than ALS due to the
more eﬃcient matrix inversion implementation (we used the
fastest known algorithm [1] with time complexity around
O(K 2.376)), the speed-up is already very signiﬁcant. More-
over, as RCD and eALS have the same analytical time com-
plexity, their actual running time are in the same magnitude;
the minor diﬀerence can be caused by some implementation
details, such as the data structures and caches used.

5.2.3

eALS vs. BPR (sample-based)

Figure 4 plots the performance of BPR with diﬀerent
learning rates4 in each iteration. Note that we only run
eALS for 100 iterations, which are enough for eALS to con-
verge. First, it is clear that BPR’s performance is subjected
4We have also tried other intermediate values of learning
rates, and the ﬁndings are consistent. Thus, to make the
ﬁgure more clear, we only show three selected values.

to the choice of learning rate — Figure 4a and 4b show that
a higher learning rate leads to a faster convergence, while the
ﬁnal accuracy may be suﬀered. Second, we see that eALS
signiﬁcantly outperforms BPR on the Yelp dataset evaluated
by both measures (p < 0.001). For Amazon, eALS obtains
a much higher hit ratio but a lower NDCG score, indicat-
ing that most hits occur at a relatively low ranks for eALS.
Comparing with the performance of other whole-based MF
methods ALS and RCD (Figure 2), we draw the conclusion
that BPR is a weak performer in terms of the prediction
recall, while being a strong performer in terms of the preci-
sion at top ranks. We think BPR’s strength in ranking top
items is due to its optimization objective, which is a pair-
wise ranking function tailored for ranking correct item high.
In contrast, the regression-based objective is not directly op-
timized for ranking; instead, by account for all missing data
in regression, it better predicts user’s preference on uncon-
sumed items, leading to a better recall. This is consistent
with [2]’s ﬁnding in evaluating top-K recommendation.

We notice that BPR shows unusual NDCG spike in early
iterations on the Amazon dataset, however the performance
is unstable and goes down with more iterations. The same
phenomenon was also observed for another gradient descent
method RCD on the same dataset (see Figure 2d). We hy-
pothesize that it might be caused by some regularities in
the data. For example, we ﬁnd some Amazon users review
on a movie multiple times5. In early iterations, BPR ranks
these repeated items high, leading to a high but unstable
NDCG score. There might be other reasons responsible for
this, and we do not further explore here.

5.3 Online Protocol

In the evaluation of online protocol, we hold out the latest
10% interactions as the test set, training all methods on the
remaining 90% data with the best parameter settings evi-
denced by the oﬄine evaluation. We ﬁrst study the number
of online iterations required for eALS to converge. Then we
show how does the weight of new interactions impact the
performance. Lastly, we compare with dynamic MF meth-
ods RCD and BPR in the online learning scenario.

5.3.1 Number of Online Iterations

Figure 6 shows how does eALS’s accuracy change with
number of online iterations. Results at the 0-th iteration
benchmark the performance of the oﬄine trained model, as
no incremental update is performed. First, we can see that
the oﬄine trained model performs very poorly, highlight-

5Due to user’s repeat consumption behaviours, we do not
exclude training items when generating recommend list.

(a) Test # vs. HR

(b) Test # vs. NDCG

(c) Test # vs. HR

(d) Test # vs. NDCG

Figure 5: Performance evolution of eALS and other dynamic MF methods in online learning.

ing the importance of refreshing recommender model for an
online system with dynamic data. Second, we ﬁnd most
performance gain comes from the ﬁrst iteration, and more
iterations do not further improve. This is due to the fact
that only the local features regarding to the new interaction
are updated, and one eALS step on a latent factor can ﬁnd
the optimal solution with others ﬁxed. Thus, one iteration
is enough for eALS to learn from a new interaction incre-
mentally, making eALS very eﬃcient for learning online.

Figure 6: Impact of online iterations on eALS.

We have also investigated number of online iterations re-
quired for baselines RCD and BPR. RCD shows the same
trend that good prediction is obtained in the ﬁrst iteration.
While BPR requires more iterations, usually 5-10 iterations
to get a peak performance and more iterations will adversely
hurt the performance due to the local over-training.

5.3.2 Weight of New Interactions

To evaluate how does the weight of new interactions ef-
fect the online learning algorithms, we also apply the same
weight wnew on RCD. Note that the original RCD paper [4]
does not consider the weight of interactions; we encode wnew
the same way with eALS, revising the RCD learner to opti-
mize the weighted regression function.

Figure 7: Impact of wnew on eALS and RCD in online
learning evaluated by NDCG.

Figure 7 shows the performance evaluated by NDCG (re-
sults of HR show the same trend thus omitted for space).
Setting wnew to 1 signiﬁes that new interaction is assigned a
same weight with the old training interaction. As expected,
with a modest increasing on wnew, the prediction of both

models is gradually improved, demonstrating the usefulness
of strengthening user’s short-term interest. The peak per-
formance is obtained around 4, where eALS shows better
prediction than RCD. Overly increasing wnew will adversely
hurt the performance, admitting the utility of user’s histor-
ical data used in oﬄine training. Overall, this experiment
indicates the importance of balancing user’s short-term and
long-term interest for quality recommendation.

5.3.3 Performance Comparison

With the simulated data stream, we show the performance
evolution with respect to number of test instances in Figure
5. First, eALS consistently outperforms RCD and BPR evi-
denced by both measures, and one-sample paired t-test ver-
iﬁes that all improvements are statistically signiﬁcant with
p < 0.001. BPR betters RCD for Yelp, while underper-
forms for Amazon. Second, we observe the trend that the
performance of dynamic learning ﬁrst decreases, and then
increases before becoming stable. This is caused by the new
users problem — when there are few feedback for a user, the
model can not personalize the user’s preference eﬀectively;
with more feedbacks streaming in, the model can adapt itself
to improve the preference modeling accordingly. To show
this, we further breakdown the results of eALS by number
of past interactions of test user in Figure 8.

Figure 8: Results breakdown of eALS by # of past
interactions of test user. Note: Interaction # > 0
denotes the performance for non-cold-start users.

It is clear that when there are no historical feedback for
a test user (i.e., user cold-start cases), the performance is
very poor — no better than random. After the ﬁrst interac-
tion streams in, the prediction is signiﬁcantly improved; and
with more interactions, the performance is further improved.
This highlights the importance of incorporating instanta-
neous user feedback into the model, especially for cold-start
or sparse users that have few history in training.

6. CONCLUSION AND FUTURE WORK

We study the problem of learning MF models from im-
plicit feedback. In contrast to previous work that applied

a uniform weight on missing data, we propose to weight
missing data based on the popularity of items. To address
the key eﬃciency challenge in optimization, we develop a
new learning algorithm — eALS — which eﬀectively learns
parameters by performing coordinate descent with memo-
ization. For online learning, we devise an incremental up-
date strategy for eALS to adapt dynamic data in real-time.
Experiments with both oﬄine and online protocols demon-
strate promising results. Importantly, our work makes MF
more practical to use for modeling implicit data, along two
dimensions. First, we investigate a new paradigm to deal
with missing data which can easily incorporate prior do-
main knowledge. Second, eALS is embarrassingly parallel,
making it attractive for large-scale industrial deployment.

We plan to study the optimal weighting strategy for online
data as a way to explore user’s short-term interest. Along
the technical line, we explored the element-wise ALS learner
in its basic MF form and solved the eﬃciency challenge in
handling missing data. To make our method more applica-
ble to real-world settings, we plan to encode side information
such as user social contexts [8] and reviews [9] by extending
eALS to more generic models, such as collective factoriza-
tion [11] and Factorization machines [26]. In addition, we
will study binary coding for MF on implicit data, since a
recent advance [32] has shown that discrete latent factors
are beneﬁcial to collaborative ﬁltering for explicit ratings.

The strength of eALS can be applied to other domains,
owing to the universality of factorizing sparse data matrices.
For example, recent advances in natural language process-
ing [16] have shown the connection between neural word em-
beddings and MF on the word–context matrix. This bridge
nicely motivates several proposals to use MF to learn word
embeddings; however, when it comes to handling missing
data, they have either ignored [22] or equally weighted the
missing entries, similar to traditional SVD [16]. It will be in-
teresting to see whether eALS will also improve these tasks.
Acknowledgement
The authors would like to thank the additional discussion
and help from Steﬀen Rendle, Bhargav Kanagal, Immanuel
Bayer, Tao Chen, Ming Gao and Jovian Lin.

7. REFERENCES
[1] D. Coppersmith and S. Winograd. Matrix multiplication

via arithmetic progressions. J. Symb. Comput.,
9(3):251–280, 1990.

[2] P. Cremonesi, Y. Koren, and R. Turrin. Performance of

recommender algorithms on top-n recommendation tasks.
In RecSys 2010, pages 39–46.

[3] A. S. Das, M. Datar, A. Garg, and S. Rajaram. Google

news personalization: Scalable online collaborative ﬁltering.
In WWW 2007, pages 271–280.

[4] R. Devooght, N. Kourtellis, and A. Mantrach. Dynamic
matrix factorization with priors on unknown values. In
KDD 2015, pages 189–198.

[5] E. Diaz-Aviles, L. Drumond, L. Schmidt-Thieme, and

W. Nejdl. Real-time top-n recommendation in social
streams. In RecSys 2012, pages 59–66.

[6] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient

methods for online learning and stochastic optimization. J.
Mach. Learn. Res., 12:2121–2159, 2011.

[7] R. Gemulla, E. Nijkamp, P. J. Haas, and Y. Sismanis.

Large-scale matrix factorization with distributed stochastic
gradient descent. In KDD 2011, pages 69–77.

[8] X. Geng, H. Zhang, J. Bian, and T.-S. Chua. Learning

image and user features for recommendation in social
networks. In ICCV 2015, pages 4274–4282.

[9] X. He, T. Chen, M.-Y. Kan, and X. Chen. Trirank:

Review-aware explainable recommendation by modeling
aspects. In CIKM 2015, pages 1661–1670.

[10] X. He, M. Gao, M.-Y. Kan, Y. Liu, and K. Sugiyama.

Predicting the popularity of web 2.0 items based on user
comments. In SIGIR 2014, pages 233–242.

[11] X. He, M.-Y. Kan, P. Xie, and X. Chen. Comment-based
multi-view clustering of web 2.0 items. In Proc. of WWW
’14, pages 771–782, 2014.

[12] Y. Hu, Y. Koren, and C. Volinsky. Collaborative ﬁltering
for implicit feedback datasets. In ICDM 2008, pages
263–272.

[13] Y. Huang, B. Cui, W. Zhang, J. Jiang, and Y. Xu.

Tencentrec: Real-time stream recommendation in practice.
In SIGMOD 2015, pages 227–238.

[14] Y. Koren. Collaborative ﬁltering with temporal dynamics.

In KDD 2009, pages 447–456.

[15] Y. Koren and R. Bell. Advances in collaborative ﬁltering.
In Recommender systems handbook, pages 145–186.
Springer, 2011.

[16] O. Levy and Y. Goldberg. Neural word embedding as
implicit matrix factorization. In NIPS 2014, pages
2177–2185.

[17] G. Ling, H. Yang, I. King, and M. R. Lyu. Online learning
for collaborative ﬁltering. In IJCNN 2012, pages 1–8.
[18] B. M. Marlin, R. S. Zemel, S. Roweis, and M. Slaney.
Collaborative ﬁltering and the missing at random
assumption. In UAI 2007, pages 267–276.

[19] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and

J. Dean. Distributed representations of words and phrases
and their compositionality. In NIPS 2013, pages 3111–3119.

[20] R. Pan and M. Scholz. Mind the gaps: Weighting the

unknown in large-scale one-class collaborative ﬁltering. In
KDD 2009, pages 667–676.

[21] R. Pan, Y. Zhou, B. Cao, N. Liu, R. Lukose, M. Scholz,
and Q. Yang. One-class collaborative ﬁltering. In ICDM
2008, pages 502–511.

[22] J. Pennington, R. Socher, and C. D. Manning. Glove:

Global vectors for word representation. In EMNLP 2014,
pages 1532–1543.

[23] I. Pil´aszy, D. Zibriczky, and D. Tikk. Fast als-based matrix
factorization for explicit and implicit feedback datasets. In
RecSys 2010, pages 71–78.

[24] S. Rendle and C. Freudenthaler. Improving pairwise

learning for item recommendation from implicit feedback.
In WSDM 2014, pages 273–282.

[25] S. Rendle, C. Freudenthaler, Z. Gantner, and

L. Schmidt-Thieme. Bpr: Bayesian personalized ranking
from implicit feedback. In UAI 2009, pages 452–461.

[26] S. Rendle, Z. Gantner, C. Freudenthaler, and

L. Schmidt-Thieme. Fast context-aware recommendations
with factorization machines. In SIGIR 2011, pages 635–644.

[27] S. Rendle and L. Schmidt-Thieme. Online-updating

regularized kernel matrix factorization models for large
scale recommender systems. In RecSys 2008, pages 251–258.

[28] P. Richt´arik and M. Tak´aˇc. Iteration complexity of

randomized block-coordinate descent methods for
minimizing a composite function. Math. Prog., 2014.
[29] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips,

D. Ebner, V. Chaudhary, and M. Young. Machine learning:
The high interest credit card of technical debt. In SE4ML
(NIPS 2014 Workshop), 2014.

[30] H. Steck. Training and testing of recommender systems on
data missing not at random. In KDD 2010, pages 713–722.

[31] M. Volkovs and G. W. Yu. Eﬀective latent models for

binary feedback in recommender systems. In SIGIR 2015,
pages 313–322.

[32] H. Zhang, F. Shen, W. Liu, X. He, H. Luan, and T.-S.
Chua. Discrete collaborative ﬁltering. In SIGIR 2016.

