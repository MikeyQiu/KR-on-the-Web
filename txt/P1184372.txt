6
1
0
2
 
p
e
S
 
1
 
 
]

G
L
.
s
c
[
 
 
5
v
9
0
3
6
0
.
1
1
5
1
:
v
i
X
r
a

Workshop track - ICLR 2016

SPATIO-TEMPORAL VIDEO AUTOENCODER WITH
DIFFERENTIABLE MEMORY

Viorica P˘atr˘aucean, Ankur Handa & Roberto Cipolla
Department of Engineering
University of Cambridge, UK
{vp344,ah781,rc10001}@cam.ac.uk

ABSTRACT

We describe a new spatio-temporal video autoencoder, based on a classic spatial
image autoencoder and a novel nested temporal autoencoder. The temporal en-
coder is represented by a differentiable visual memory composed of convolutional
long short-term memory (LSTM) cells that integrate changes over time. Here we
target motion changes and use as temporal decoder a robust optical ﬂow prediction
module together with an image sampler serving as built-in feedback loop. The ar-
chitecture is end-to-end differentiable. At each time step, the system receives as
input a video frame, predicts the optical ﬂow based on the current observation
and the LSTM memory state as a dense transformation map, and applies it to
the current frame to generate the next frame. By minimising the reconstruction
error between the predicted next frame and the corresponding ground truth next
frame, we train the whole system to extract features useful for motion estimation
without any supervision effort. We present one direct application of the proposed
framework in weakly-supervised semantic segmentation of videos through label
propagation using optical ﬂow.

1

INTRODUCTION

High-level understanding of video sequences is crucial for any autonomous intelligent agent, e.g. se-
mantic segmentation is indispensable for navigation, or object detection skills condition any form
of interaction with objects in a scene. The recent success of convolutional neural networks in tack-
ling these high-level tasks for static images opens up the path for numerous applications. However,
transferring the capabilities of these systems to tasks involving video sequences is not trivial, on the
one hand due to the lack of video labelled data, and on the other hand due to convnets’ inability of
exploiting temporal redundancy present in videos. Motivated by these two shortcomings, we focus
on reducing the supervision effort required to train recurrent neural networks, which are known for
their ability to handle sequential input data Williams & Zipser (1995); Hochreiter et al. (2000).

In particular, we describe a spatio-temporal video autoencoder integrating a differentiable short-term
memory module whose (unsupervised) training is geared towards motion estimation and prediction
Horn & Schunck (1994). This choice has biological inspiration. The human brain has a complex
system of visual memory modules, including iconic memory, visual short-term memory (VSTM),
and long-term memory Hollingworth (2004). Among them, VSTM is responsible mainly for under-
standing visual changes (movement, light changes) in dynamic environments, by integrating visual
stimuli over periods of time Phillips (1974); Magnussen (2000). Moreover, the fact that infants are
able to handle occlusion, containment, and covering events by the age of 2.5 months Baillargeon
(2004) could suggest that the primary skills acquired by VSTM are related to extracting features
useful for motion understanding. These features in turn could generate objectness awareness based
on the simple principle that points moving together belong to the same object. Understanding ob-
jectness is crucial for high-level tasks such as semantic segmentation or action recognition Alexe
et al. (2012). In this spirit, our approach is similar to the recent work of Agrawal et al. (2015), who
show that the features learnt by exploiting (freely-available) ego-motion information as supervision
data are as good as features extracted with human-labelled supervision data. We believe that our
work is complementary to their approach and integrating them could lead to an artiﬁcial agent with
enhanced vision capabilities.

1

Workshop track - ICLR 2016

Our implementation draws inspiration from standard video encoders and compression schemes, and
suggests that deep video autoencoders should differ conceptually from spatial image autoencoders.
A video autoencoder need not reproduce by heart an entire video sequence. Instead, it should be
able to encode the signiﬁcant differences that would allow it to reconstruct a frame given a previous
frame. To this end, we use a classic convolutional image encoder – decoder with a nested memory
module composed of convolutional LSTM cells, acting as temporal encoder. Since we focus on
learning features for motion prediction, we use as temporal decoder a robust optical ﬂow prediction
module together with an image sampler, which provides immediate feedback on the predicted ﬂow
map. At each time step, the system receives as input a video frame, predicts the optical ﬂow based on
the current frame and the LSTM content as a dense transformation map, and applies it to the current
frame to predict the next frame. By minimising the reconstruction error between the predicted next
frame and the ground truth next frame, we are able to train the whole system for motion prediction
without any supervision effort. Other modules handling other types of variations like light changes
could be added in parallel, inspired by neuroscience ﬁndings which suggest that VSTM is composed
of a series of different modules specialised in handling the low-level processes triggered by visual
changes, all of them connected to a shared memory module Magnussen (2000). Note that at the
hardware level, this variations-centred reasoning is similar to event-based cameras Boahen (2005),
which have started to make an impact in robotic applications Kim et al. (2014).

Summary of contributions: We propose a spatio-temporal version of LSTM cells to serve as a basic
form of visual short-term memory, and make available an end-to-end differentiable architecture
with built-in feedback loop, that allows effortless training and experimentation with the goal of
understanding the role of such an artiﬁcial visual short-term memory in low-level visual processes
and its implications in high-level visual tasks. We present one direct application of the proposed
framework in weakly-supervised semantic segmentation of videos through label propagation using
optical ﬂow. All our code (Torch implementation) is available online Con.

2 RELATED WORK

Architectures based on LSTM cells Hochreiter & Schmidhuber (1997) have been very successful in
various tasks involving one-dimensional temporal sequences: speech recognition Sak et al. (2014),
machine translation Sutskever et al. (2014), music composition Eck & Schmidhuber, due to their
ability to preserve information over long periods of time. Multi-dimensional LSTM networks have
been proposed to deal with (2D) images Graves et al. (2007) or (3D) volumetric data Stollenga et al.
(2015), treating the data as spatial sequences. Since in our work we aim at building a visual short-
term memory, customised LSTM cells that deal with temporal sequences of spatial data represent a
natural choice.

Recently, Srivastava et al. (2015) proposed an LSTM-based video autoencoder, which aims at gen-
erating past and future frames in a sequence, in an unsupervised manner. However, their LSTM
implementation treats the input as sequences of vectors by ﬂattening the frames or by using 1D
frame representations produced after the last fully-connected layer of a convolutional neural net-
work. This results in a large number of parameters, since the activations of the LSTM cells use
fully-connected linear operations, which are not necessarily useful for 2D images, since natural im-
age statistics indicate only local correlations. Another difference is in the architecture setup. We are
not interested in training a black-box to produce past and future frames in the sequence. Instead we
aim at a transparent setup, and train a more generic memory module together with specialised mod-
ules able to decode the memory content and on which appropriate constraints (sparsity, smoothness)
can be imposed.

Our approach is partially related to optical ﬂow estimation works like DeepFlow Weinzaepfel et al.
(2013) and FlowNet Fischer et al. (2015). However, these methods use purely supervised training to
establish matches between consecutive pairs of frames in a sequence, and then apply a variational
smoothing. Our architecture is end-to-end differentiable and integrates a smoothness penalty to
ensure that nearby pixels follow a locally smooth motion, and requires no labelled training data.

As mentioned in the previous section, our work is similar in spirit to Agrawal et al. (2015), by
establishing a direct link between vision and motion, in an attempt to reduce supervision effort for
high-level scene understanding tasks. However, instead of relying on data provided by an inertial
measurement unit from which we can estimate only a global transformation representing the ego-

2

Workshop track - ICLR 2016

Figure 1: Spatio-temporal video autoencoder.

motion, we predict a dense ﬂow map by integrating visual stimuli over time using a memory module,
and then we use a built-in feedback loop to assess the prediction. The dense ﬂow map is useful to
explain dynamic environments, where different objects can follow different motion models, hence a
global transformation is not sufﬁcient Menze & Geiger (2015).

3 ARCHITECTURE

Our architecture consists of a temporal autoencoder nested into a spatial autoencoder (see Figure 1).
At each time step, the network takes as input a video frame Yt of size H × W , and generates an
output of the same size, representing the predicted next frame, ˜Yt+1. In the following, we describe
each of the modules in detail.

3.1 SPATIAL AUTOENCODER E AND D

The spatial autoencoder is a classic convolutional encoder – decoder architecture. The encoder E
contains one convolutional layer, followed by tanh non-linearity and a spatial max-pooling with
subsampling layer. The decoder D mirrors the encoder, except for the non-linearity layer, and uses
nearest-neighbour spatial upsampling to bring the output back to the size of the original input. After
E−→ xt, the size of the feature maps xt is d × h × w,
the forward pass through the spatial encoder Yt
d being the number of features, and h and w the height and width after downsampling, respectively.

3.2 TEMPORAL AUTOENCODER

The goal of the temporal autoencoder is to capture signiﬁcant changes due to motion (ego-motion
or movement of the objects in the scene), that would allow it to predict the visual future, knowing
the past and the present.
In a classic spatial autoencoder Masci et al. (2011), the encoder and
decoder learn proprietary feature spaces that allow an optimal decomposition of the input using
some form of regularisation to prevent learning a trivial mapping. The encoder decides freely upon
a decomposition based on its current feature space, and the decoder constrains the learning of its
own feature space to satisfy this decomposition and to reconstruct the input, using usually operations
very similar to the encoder, and having the same number of degrees of freedom. Differently from
this, the proposed temporal autoencoder has a decoder with a small number of trainable parameters,
whose role is mainly to provide immediate feedback to the encoder, but without the capacity of
amending encoder’s mistakes like in the spatial case. In optimisation terms, the error during learning
is attributed mainly to the encoder, which is now more constrained to produce sensible feature maps.

3.2.1 MEMORY MODULE LSTM

The core of the proposed architecture is the memory module playing the role of a temporal encoder.
We aim at building a basic visual short-term memory, which preserves locality and layout ensuring
a fast access and bypassing more complicated addressing mechanisms like those used by Neural
Turing Machines Graves et al. (2014). To this end, we use customised spatio-temporal LSTM cells

3

Workshop track - ICLR 2016

with the same layout as the input. At each time step t, the LSTM module receives as input a new
video frame after projection in the spatial feature space. This is used together with the memory
content and output of the previous step t − 1 to compute the new memory activations.

Classic LSTM cells operate over sequences of (one-dimensional) vectors and perform biased linear
(fully-connected) transformations, followed by non-linearities to compute gate and cell activations.
In our case, to deal with the spatial and local nature of the video frames, we replace the fully-
connected transformations with spatial local convolutions. Therefore, the activations of a spatio-
temporal convolutional LSTM cell at time t are given by:






it = σ(xt ∗ wxi + ht−1 ∗ whi + wibias)
ft = σ(xt ∗ wxf + ht−1 ∗ whf + wf bias)
˜ct = tanh(xt ∗ wx˜c + ht−1 ∗ wh˜c + w˜cbias)
ct = ˜ct (cid:12) it + ct−1 (cid:12) ft
ot = σ(xt ∗ wxo + ht−1 ∗ who + wobias)
ht = ot (cid:12) tanh(ct)

(1)

where xt represents the input at time t, i.e. the feature maps of the frame t; it, ft, ˜ct, and ot represent
the input, forget, cell, and output gates, respectively; ct, ct−1, ht, and ht−1 are the memory and
output activations at time t and t − 1, respectively; σ and tanh are the sigmoid and hyperbolic
tangent non-linearities; ∗ represents the convolution operation, and (cid:12) the Hadamard product. For
input feature maps of size d × h × w, the LSTM module outputs a memory map of size dm × h × w,
where dm is the number of temporal features learnt by the memory. The recurrent connections
operate only over the temporal dimension, and use local convolutions to capture spatial context,
unlike multi-dimensional LSTM versions that use spatial recurrent connections Graves et al. (2007);
Stollenga et al. (2015). Note that a similar convolutional LSTM implementation was recently used
in Shi et al. (2015) for precipitation nowcasting.

3.2.2 OPTICAL FLOW PREDICTION Θ WITH HUBER PENALTY H

The optical ﬂow prediction module generates a dense transformation map T , having the same height
and width as the memory output, with one 2D ﬂow vector per pixel, representing the displacement in
x and y directions due to motion between consecutive frames. T allows predicting the next frame by
warping the current frame. We use two convolutional layers with relatively large kernels (15 × 15)
to regress from the memory feature space to the space of ﬂow vectors. Large kernels are needed
since the magnitude of the predicted optical ﬂow is limited by the size of the ﬁlters. To ensure local
smoothness, we need to penalise the local gradient of the ﬂow map (cid:79)T . We add a penalty module H
whose role is to forward its input unchanged during the forward pass, and to inject non-smoothness
error gradient during the backward pass, towards the modules that precede it in the architecture
and that might have contributed to the error. We use Huber loss as penalty, with its corresponding
derivative (2), due to its edge-preserving capability Werlberger et al. (2009). The gradient of the ﬂow
map is obtained by convolving the map with a ﬁxed (non-trainable) 2 × 3 × 3 ﬁlter, corresponding
to a 5-point stencil, and 0 bias.

Hδ(aij) =

(cid:26) 1

2 a2
ij,
δ(|aij| − 1

for |aij| ≤ δ

2 δ), otherwise

, ∇Hδ(aij) =

(cid:26) aij

δsign(aij), otherwise

for|aij| ≤ δ,

(2)

Here, aij represent the elements of (cid:79)T . In our experiments, we used δ = 10−3.

3.2.3 GRID GENERATOR GG AND IMAGE SAMPLER S

The grid generator GG and the image sampler S output the predicted feature maps ˜xt+1 of the next
frame after warping the current feature maps xt with the ﬂow map produced by the Θ module. We
use similar differentiable grid generator and image sampler as Spatial Transformer Network (STN)
Jaderberg et al. (2015) (implementation available online (Moodstocks, 2015)). The output of S, of
size d × h × w, is considered as a ﬁxed h × w grid, holding at each (xo, yo) position a feature map
entry of size 1×1×d. We modiﬁed the grid generator to accept one transformation per pixel, instead
of a single transformation for the entire image as in STN. Given the ﬂow map T , GG computes for
each element in the grid the source position (xs, ys) in the input feature map from where S needs to

4

Workshop track - ICLR 2016

sample to ﬁll in the position (xo, yo):

(cid:19)

(cid:18) xs
ys

= T (xo, yo)

, T (·, ·) =

(3)

(cid:33)

(cid:32) xo
yo
1

(cid:18) 1
0

(cid:19)

.

0
1

tx
ty

The Θ module outputs two parameters (tx, ty) for each pixel. The forward pass of GG is given
by equation (3). In the backward pass, GG simply backpropagates the derivative of equation (3)
w.r.t. the input parameters (tx, ty).

3.2.4 LOSS FUNCTION

Training the network comes down to minimising the reconstruction error between the predicted next
frame and the ground truth next frame, with Huber penalty gradient injected during backpropagation
on the optical ﬂow map:

Lt = (cid:107) ˜Yt+1 − Yt+1(cid:107)2

2 + wH H((cid:79)T ),

(4)

where wH is a hard-coded parameter used to weight the smoothness constraints w.r.t. the data term;
in our experiments wH = 10−2, which is a common value in optical ﬂow works.

3.3 NETWORK PARAMETERS

The proposed network has 1,035,067 trainable parameters. The spatial encoder and decoder have 16
ﬁlters each, size 7 × 7. The memory module (LSTM ) has 64 ﬁlters, size 7 × 7, and the optical ﬂow
regressor Θ has 2 convolutional layers, each with 2 ﬁlters of size 15 × 15, and a 1 × 1 convolutional
layer. The other modules: GG, H, and S have no trainable parameters.

4 TRAINING

The training was done using rmsprop, with a learning rate starting at 10−4 and decaying by 0.9 after
every 5 epochs. The initialisation of the convolutional layers except those involved in the memory
module was done using xavier method Glorot & Bengio (2010). The parameters of the memory
module, except the biases, were initialised from a uniform distribution U(−0.08, 0.08). The biases
of the forget gates are initialised to 1; for the other gates we set the biases to 0. During training,
before each parameter update, we clip the gradients to lie in a predeﬁned range, to avoid numerical
issues Graves (2013).

Our implementation is based on Torch library Collobert et al. and extends the rnn package L´eonard
et al. (2015). All our code is available online Con. The training was done on an NVIDIA K40 GPU
(12G memory).

5 EXPERIMENTS

As a sanity check to conﬁrm the ability of the grid generator GG and image sampler S to generate
the next frame given the current frame and the correct optical ﬂow map, we ran simple warping tests
using Sintel dataset Butler et al. (2012), by isolating the two modules from the rest of the architec-
ture. Figure 2 shows an example of warping. Note that since the ﬂow displacement is signiﬁcant in
this dataset, the sampling result can contain artefacts, especially near boundaries, caused by occlu-
sions. The average per-pixel error induced by the sampler on this dataset is 0.004 (for pixel values in
[0, 1]). However, this test was done by sampling directly from the input frames, no spatial encoder
– decoder was used. In our architecture, these artefacts will be washed out to a certain extent by the
convolutional decoder. The optical ﬂow throughout this section is displayed using the colour code
from Baker et al. (2011), illustrated in Figure 2, top-right corner.

5

Workshop track - ICLR 2016

Figure 2: Warping performed by the GG and S modules, given a video frame and the ground
truth optical ﬂow map. First line: input frame and ground truth optical ﬂow map displayed using the
colour code shown in the top-right corner of the image; the colour encodes the direction of movement
and the colour intensity reﬂects the magnitude; darker shades correspond to higher magnitudes Baker
et al. (2011). Second line: ground truth next frame and sampled next frame; note the artifacts close
to the boundaries.

Figure 3: Samples of images from moving MNIST dataset.

Architecture
AE-Conv
AE-fcLSTM
AE-ConvLSTM
AE-ConvLSTM-ﬂow

Parameters Test error

109,073
33,623,649
1,256,305
1,035,067

0.0948
0.0650
0.0641
0.0439

Table 1: Number of parameters for the different architectures and their per-pixel average error on
moving MNIST.

5.1 UNSUPERVISED EXPERIMENTS

To assess qualitatively and quantitatively the behaviour of the proposed architecture and of its com-
ponents, we ran unsupervised experiments on synthetic and real datasets1.

Synthetic dataset. Moving MNIST dataset Srivastava et al. (2015) consists of sequences of 20
frames each, obtained by moving (translating) MNIST digit images inside a square of size 64 × 64,
using uniform random sampling to obtain direction and velocity; the sequences can contain several
overlapping digits in one frame. We generated 10k sequences for training and 3k sequences for
validation (see Figure 3 for sample images).

To highlight the advantages of the proposed convolutional LSTM units (denoted as ConvLSTM) and
of the overall architecture (denoted as AE-ConvLSTM-ﬂow), we ran quantitative experiments us-

1Existing datasets in the optical ﬂow community do not represent suitable training and/or testing data be-
cause they contain ﬂow maps for only pairs of images Fischer et al. (2015), or for very few short video se-
quences (6 sequences of 2 or 8 frames in Middlebury dataset Baker et al. (2011)). Sintel dataset Butler et al.
(2012) contains indeed ground truth optical ﬂow for longer video sequences. However, it exhibits a complex
combination of camera self-motion and moving objects, with a relatively low frame rate, leading to very large
(non-realistic) displacements between consecutive frames. Note also that the self-motion makes the entire
scene to move in terms of optical ﬂow, making the prediction very hard and, in the same time, not useful for the
purposes of our work: if all the points in the scene move, it is difﬁcult to delineate objects.

6

Workshop track - ICLR 2016

Figure 4: Baseline (autoencoder) next frame predictors. C = convolutional, T = tanh, P = pooling,
U = unpooling, R = reshape from 2D feature maps to 1D (ﬂattened) vectors and vice-versa.

ing different architectures, illustrated in Figure 4. AE-Conv is a simple convolutional autoencoder,
which takes as input the frames of a video sequence considered as independent channels, and out-
puts the predicted next frame. It contains a similar encoder and decoder as AE-ConvLSTM-ﬂow,
but replaces the (recurrent) memory module and optical ﬂow module with simple convolution-tanh
layers. AE-ConvLSTM is a similar setup, but uses convolutional LSTM blocks as temporal encoder
and decoder respectively; AE-fcLSTM replaces the convolutional LSTM blocks with classic fully-
connected LSTM blocks, resembling the architecture of Srivastava et al. (2015), but predicting only
one frame in the future. Table 1 summarises the number of parameters for each architecture. Note
that, although the depth of all the architectures could be considered similar, the differences in the
number of parameters are signiﬁcant: all the architectures using LSTM blocks have more parameters
than the simple convolutional autoencoder due to the gating layers; among these, the fully-connected
model is considerably larger. AE-ConvLSTM-ﬂow has less parameters than AE-ConvLSTM due to
the fact that the LSTM temporal decoder in the latter is replaced by simple convolutional regression
layers in the former.

The loss function for this experiment was the binary cross-entropy Srivastava et al. (2015), and the
sequences are fed in as binary data. Table 1 presents the test errors and Figure 5 shows qualitative
results. All the architectures using LSTM blocks report lower errors than AE-Conv, showing the im-
portance of exploiting the temporal dependency between frames. Although the results of AE-Conv
seem qualitatively better than AE-fcLSTM, they are very blurred, leading to the increased error.
The architectures using the proposed convolutional LSTM units are more efﬁcient than the classic
fully-connected units, since the former preserve the spatial information; this can be observed in both
the quantitative and the qualitative results. Importantly, AE-ConvLSTM-ﬂow produces boundaries
with better continuity than AE-ConvLSTM, suggesting that it acquires a basic form of objectness
awareness, i.e. it learns to reason about objects as a whole due to their motion. Equally, these re-
sults show that the optical ﬂow maps produced by our network correspond to a basic form of image
segmentation into objects identiﬁed by their movement. Hence, this setup could potentially be more
useful for supervised video segmentation than a classic video autoencoder.

Real dataset. We train our architecture on real videos extracted from HMDB-51 dataset Kuehne
et al. (2011), with about 107k frames in total (1.1 hours), and present qualitative results of the
predicted next frame and the optical ﬂow maps on test sequences extracted from PROST Santner
et al. (2010) and ViSOR Vezzani & Cucchiara (2010) datasets. Figure 6 illustrates results obtained
on different test sequences and videos showing the optical ﬂow for entire test sequences are included
in the supplemental material. The results indicate that the moving elements of the scene are identiﬁed
by the network. However, the accuracy of the estimated ﬂow is not comparable to supervised setups
for optical ﬂow estimation. But this is expected since in our case we predict the ﬂow map that links
the already seen frame t to the unseen frame t + 1, whereas classic optical ﬂow methods estimate the
optical ﬂow between two already seen frames. Intuitively, we expect our network to extract more
meaningful features than a classic supervised setup, since the problem is more challenging, at least
for articulated objects.

7

Workshop track - ICLR 2016

Figure 5: Qualitative results on moving MNIST test dataset. First four columns: the last frames (out
of 10 in total) fed into the network (from t − 3 to t); 5th column: the ground truth next frame (t + 1);
6th column: AE-Conv; 7th column: AE-fcLSTM; 8th column: AE-ConvLSTM; last two columns:
AE-ConvLSTM-ﬂow predicted next frame and optical ﬂow.

5.2 APPLICATION TO WEAKLY-SUPERVISED VIDEO SEMANTIC SEGMENTATION

Almost no labelled dataset of real videos exists for semantic segmentation2, limiting the applicabil-
ity of deep architectures for the task. There exist, however, datasets that contain video sequences
with one labelled frame per sequence Brostow et al. (2008); Nathan Silberman & Fergus (2012). We
take advantage of our autoencoder to enable weakly-supervised video segmentation. The proposed
framework, denoted SegNet-ﬂow, is sketched in Figure 7. We use AE-ConvLSTM-ﬂow together
with a per-frame semantic segmentation module, SegNet3 Badrinarayanan et al. (2015). Using the
ﬂow maps produced by AE-ConvLSTM-ﬂow, we warp (propagate) SegNet’s predictions until reach-
ing the time step for which ground truth labels are available. The label propagation is performed by
a customised recurrent module, which integrates a merging block M and a sampler S similar to the
one used in AE-ConvLSTM-ﬂow. M is a recurrent convolutional module, which receives as input
SegNet’s basic predictions Lb
t at time t, and the warped predictions Lw
t−1 from time t − 1 generated
by S, and produces the ﬁnal label predictions ˜Lt for time t. For the time steps where ground truth
t are available, we minimise the cross-entropy segmentation error between ˜Lt and Lgt
labels Lgt
t .

2SceneNet Handa et al. (2015) is a recent synthetic dataset allowing to obtain synthetic labelled videos by
rendering labelled synthetic 3D scenes. However, the models lack texture. Sun3D Xiao et al. (2013) contains
video sequences with labels obtained by label propagation. However, the quality of the labelling is quite poor.
3Although not state-of-the-art for image semantic segmentation, we chose to use SegNet in our experiment
due to its reduced memory requirements. This is the only setup allowing us to train the entire network on one
GPU with 12G memory.

8

Workshop track - ICLR 2016

Figure 6: Qualitative results on real videos. The ﬁrst three columns show the last frames (out of 12
in total) fed into the network (from t − 2 to t), the t + 1 column shows the ground truth next frame,
and the last two columns show the predicted next frame and the predicted optical ﬂow.

This error term is added to the existing reconstruction error term. In this way, the two tasks, ﬂow
prediction and semantic segmentation, are learnt jointly with a minimum of supervision required.

Figure 7: SegNet-ﬂow: weakly-supervised video segmentation through label propagation using op-
tical ﬂow.

We trained our architecture on Camvid dataset Brostow et al. (2008), which consists of road scene
RGB videos, with about 10k training frames in total, among which 468 are labelled, and about 7k
testing frames, among which 233 are labelled. However, due to memory limitations, we were not
able to use all the (unlabelled) available frames, but only sequences of 6 frames around the labelled
ones. Hence about 2.5k frames were used for training. Additionally, we downsampled the videos
to 240 × 180 pixels per frame4. We use the 12 classes labelling, similar to Badrinarayanan et al.

4Note that the results in Table 2 for basic SegNet are different from those reported in Badrinarayanan et al.

(2015), due to this downsampling.

9

Workshop track - ICLR 2016

Network
SegNet
SegNet-ﬂow

y
k
S
89.2
92.9

g
n
i
d
l
i
u
B
65.4
73.4

e
l
o
P
22.6
3.22

d
a
o
R
92.0
97.2

k
l
a
w
e
d
i
S
70.7
62.9

n
o
i
t
a
t
e
g
e
V
44.8
52.3

n
g
i
s

c
ﬁ
f
a
r
T
14.9
2.37

e
c
n
e
F
5.38
2.35

r
a
C
60.5
63.3

n
a
i
r
t
s
e
d
e
P
25.6
14.9

t
s
i
l
c
y
C
2.73
1.70

.
g
v
A
s
s
a
l
C
44.9
42.4

.
g
v
A

l
a
b
o
l
G
75.3
76.9

Table 2: Per-class and global average segmentation accuracy.

(2015). Table 2 summarises the results of basic SegNet on the test set compared to our setup, and
Figure 8 shows qualitative results. It can be observed that the quality of the segmentation increases
considerably for the large classes, but diminishes for the small thin structures. We believe the main
reason for this is the relatively small size of the training set. SegNet-ﬂow has about one million
parameters more than the basic SegNet, hence the former model is less constrained. Equally, the
fact that the ﬂow estimation is performed on a downsampled version of the input (after the spatial
encoder) could lead to a poor ﬂow estimation for small structures. Nevertheless, we were able to
train end-to-end an architecture performing joint ﬂow estimation and semantic segmentation from
videos. Understanding better and improving this framework constitute an exciting direction of future
work.

Figure 8: Qualitative results on Camvid. 1st column: input frame, 2nd column: ground truth, 3rd
column: SegNet result, 4th column: SegNet-ﬂow result. The ﬂow constraints present in SegNet-
ﬂow make a signiﬁcant difference in smoothing the segmentation results produced by SegNet. We
expect this to hold irrespective of the segmentation module used.

6 CONCLUSION AND FUTURE WORK

We proposed an original spatio-termporal video autoencoder based on an end-to-end differentiable
architecture that allows unsupervised training for motion prediction. The core of the architecture

10

Workshop track - ICLR 2016

is a module implementing a convolutional version of long short-term memory (LSTM) cells to be
used as a form of artiﬁcial visual short-term memory. We showed that the proposed convolutional
LSTM module performs better than existing classic autoencoders and than fully-connected LSTM
implementations, while having a reduced number of parameters. The usefulness of the overall setup
was illustrated in the task of weakly-supervised video semantic segmentation.

We believe that our work can open up the path to a number of exciting directions. Due to the
built-in feedback loop, various experiments can be carried out effortlessly, to develop further the
basic memory module that we proposed.
In particular, investigating on the size of the memory
and its resolution/downsampling could shed some light into the causes of some geometric optical
illusions; e.g. when storing in memory an overly-downsampled version of the visual input, this could
affect the capacity of correctly perceiving the visual stimuli, in accordance with Shannon-Nyquist
sampling theorem Shannon (1949). Equally, we hypothesise that our convolutional version of LSTM
cells can lead to ambiguous memory activations, i.e. the same activation would be produced when
presenting a temporally moving boundary or a spatially repeated boundary. This could imitate the
illusory motion experienced by biological VSTMs, i.e. static repeated patterns that produce a false
perception of movement Conway et al. (2005).

A necessary future development is the integration of the memory module with an attention mecha-
nism and a form of long-term memory module, to enable a complete memory system, indispensable
for supervised tasks.

Last but not least, the proposed architecture composed of memory module and built-in feedback
loop could be applied for static images as a compression mechanism, similar to the inspirational
work on jigsaw video compression Kannan et al. (2007). The memory addressing in that case could
use a content-based mechanism similar to NTM Graves et al. (2014).

ACKNOWLEDGMENTS

We are greatly indebted to the Torch community for their efforts to maintain this great library, and
especially to Nicholas L´eonard for his helpful advice. We are grateful to CSIC–University of Cam-
bridge for funding this work (grant number EP/L010917/1).

REFERENCES

Convlstm. https://github.com/viorik/ConvLSTM.

Pulkit Agrawal, Jo˜ao Carreira, and Jitendra Malik.

Learning to see by moving.

CoRR,

abs/1505.01596, 2015. URL http://arxiv.org/abs/1505.01596.

B. Alexe, T. Deselaers, and V. Ferrari. Measuring the objectness of image windows. IEEE Trans.

Pattern Anal. Mach. Intell., 34(11), 2012. doi: 10.3758/BF03203943.

Vijay Badrinarayanan, Ankur Handa, and Roberto Cipolla. Segnet: A deep convolutional encoder-
decoder architecture for robust semantic pixel-wise labelling. CoRR, abs/1505.07293, 2015.

Rene´e Baillargeon. Infants’ physical world. American Psychological Society, 13(3), 2004.

Simon Baker, Daniel Scharstein, J. P. Lewis, Stefan Roth, Michael J. Black, and Richard Szeliski. A
database and evaluation methodology for optical ﬂow. Int. J. Comput. Vision, 92(1):1–31, 2011.

K. Boahen. Neuromorphic microchips. Scientiﬁc American, 292(5), 2005.

Gabriel J. Brostow, Jamie Shotton, Julien Fauqueur, and Roberto Cipolla. Segmentation and recog-

nition using structure from motion point clouds. In ECCV, pp. 44–57, 2008.

D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A naturalistic open source movie for optical

ﬂow evaluation. In European Conf. on Computer Vision (ECCV), pp. 611–625, 2012.

Ronan Collobert, Koray Kavukcuoglu, and Clment Farabet. Torch7: A matlab-like environment for

machine learning.

11

Workshop track - ICLR 2016

Bevil R. Conway, Akiyoshi Kitaoka, Arash Yazdanbakhsh, Christopher C. Pack, and Margaret S.
Livingstone. Neural basis for a powerful static motion illusion. The Journal of Neuroscience, 25
(23):5651–5656, 2005.

Douglas Eck and J¨urgen Schmidhuber. A ﬁrst look at music composition using lstm recurrent neural

networks. Technical Report IDSIA-07-02, IDSIA / USI-SUPSI.

Philipp Fischer, Alexey Dosovitskiy, Eddy Ilg, Philip H¨ausser, Caner Hazirbas, Vladimir Golkov,
Patrick van der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical ﬂow with
convolutional networks. CoRR, abs/1504.06852, 2015. URL http://arxiv.org/abs/
1504.06852.

Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural

networks. In AISTATS, 2010.

Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013.

Alex Graves, Santiago Fern´andez, and J¨urgen Schmidhuber. Multi-dimensional recurrent neural

networks. In Artiﬁcial Neural Networks, volume 4668, pp. 549–558. 2007.

Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs/1410.5401,

2014.

A. Handa, V. P˘atr˘aucean, V. Badrinarayanan, S. Stent, and R. Cipolla. SceneNet: Understanding

real world indoor scenes with synthetic data. arXiv preprint arXiv:1511.07041, 2015.

Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–
1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http://dx.
doi.org/10.1162/neco.1997.9.8.1735.

Sepp Hochreiter, Fakultat F. Informatik, Yoshua Bengio, Paolo Frasconi, and Jurgen Schmidhuber.
Gradient Flow in Recurrent Nets: the Difﬁculty of Learning Long-Term Dependencies. In Field
Guide to Dynamical Recurrent Networks. 2000.

A. Hollingworth. Constructing visual representations of natural scenes:

the roles of short- and

long-term visual memory. J Exp Psychol Hum Percept Perform., 30(3), 2004.

Berthold K. P. Horn and B. G. Schunck. Artiﬁcial intelligence in perspective. chapter Determining

Optical Flow: A Retrospective, pp. 81–87. 1994.

Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial transformer
networks. CoRR, abs/1506.02025, 2015. URL http://arxiv.org/abs/1506.02025.

Anitha Kannan, John Winn, and Carsten Rother. Clustering appearance and shape by learning

jigsaws. In Advances in Neural Information Processing Systems, pp. 657–664. 2007.

Hanme Kim, Ankur Handa, Ryad Benosman, Sio-Ho¨ı Ieng, and Andrew J. Davison. Simultaneous
mosaicing and tracking with an event camera. In British Machine Vision Conference (BMVC).
BMVC 2014, 2014.

H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. HMDB: a large video database for
human motion recognition. In Proceedings of the International Conference on Computer Vision
(ICCV), 2011.

Nicholas L´eonard, Sagar Waghmare, Yang Wang, and Jin-Hwa Kim. rnn : Recurrent library for

torch. CoRR, abs/1511.07889, 2015.

Svein Magnussen. Low-level memory processes in vision. Trends in Neurosciences, 23(6):247–251,
2000. ISSN 0166-2236. doi: http://dx.doi.org/10.1016/S0166-2236(00)01569-1. URL http:
//www.sciencedirect.com/science/article/pii/S0166223600015691.

Jonathan Masci, Ueli Meier, Dan Cirean, and Jrgen Schmidhuber. Stacked convolutional auto-
encoders for hierarchical feature extraction. In Artiﬁcial Neural Networks and Machine Learning,
volume 6791, pp. 52–59. 2011.

12

Workshop track - ICLR 2016

Moritz Menze and Andreas Geiger. Object scene ﬂow for autonomous vehicles. In Proceedings of

the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3061–3070, 2015.

Moodstocks.

Open Source Implementation of Spatial Transformer Networks.

URL

https://github.com/qassemoquab/stnbhwd, 2015.

Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support

inference from rgbd images. In ECCV, 2012.

W.A. Phillips. On the distinction between sensory storage and short-term visual memory. Perception
ISSN 0031-5117. doi: 10.3758/BF03203943. URL

& Psychophysics, 16(2):283–290, 1974.
http://dx.doi.org/10.3758/BF03203943.

Hasim Sak, Andrew W. Senior, and Franc¸oise Beaufays. Long short-term memory based recur-
rent neural network architectures for large vocabulary speech recognition. CoRR, abs/1402.1128,
2014. URL http://arxiv.org/abs/1402.1128.

Jakob Santner, Christian Leistner, Amir Saffari, Thomas Pock, and Horst Bischof. PROST Parallel
Robust Online Simple Tracking. In IEEE Computer Society Conference on Computer Vision and
Pattern Recognition (CVPR), San Francisco, CA, USA, 2010.

C.E. Shannon. Communication in the presence of noise. Proceedings of the IRE, 37(1):10–21, Jan

1949. ISSN 0096-8390. doi: 10.1109/JRPROC.1949.232969.

Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo.
Convolutional LSTM network: A machine learning approach for precipitation nowcasting. CoRR,
abs/1506.04214, 2015.

Nitish Srivastava, Elman Mansimov, and Ruslan Salakhutdinov. Unsupervised learning of video

representations using lstms. ICML, 2015.

Marijn Stollenga, Wonmin Byeon, Marcus Liwicki, and J¨urgen Schmidhuber. Parallel multi-
dimensional lstm, with application to fast biomedical volumetric image segmentation. CoRR,
abs/1506.07452, 2015. URL http://arxiv.org/abs/1506.07452.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks.

NIPS, 2014.

Roberto Vezzani and Rita Cucchiara. Video surveillance online repository (visor): an integrated

framework. Multimedia Tools and Applications, 50(2):359–380, 2010.

P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid. Deepﬂow: Large displacement optical
ﬂow with deep matching. In Computer Vision (ICCV), 2013 IEEE International Conference on,
pp. 1385–1392, Dec 2013. doi: 10.1109/ICCV.2013.175.

Manuel Werlberger, Werner Trobin, Thomas Pock, Andreas Wedel, Daniel Cremers, and Horst
Bischof. Anisotropic Huber-L1 optical ﬂow. In Proceedings of the British Machine Vision Con-
ference (BMVC), London, UK, September 2009.

Ronald J. Williams and David Zipser. Backpropagation. chapter Gradient-based Learning Algo-

rithms for Recurrent Networks and Their Computational Complexity, pp. 433–486. 1995.

J. Xiao, A. Owens, and A. Torralba. Sun3d: A database of big spaces reconstructed using sfm and

object labels. In ICCV, pp. 1625–1632, 2013.

13

