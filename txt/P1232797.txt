7
1
0
2
 
y
a
M
 
8
2
 
 
]

G
L
.
s
c
[
 
 
4
v
8
1
5
6
0
.
4
0
6
1
:
v
i
X
r
a

Journal of Machine Learning Research x (2016) y-z

Submitted a/b; Published c/d

Approximation Vector Machines
for Large-scale Online Learning

Trung Le∗
Centre for Pattern Recognition and Data Analytics, Australia

trung.l@deakin.edu.au

Tu Dinh Nguyen
Centre for Pattern Recognition and Data Analytics, Australia

tu.nguyen@deakin.edu.au

Vu Nguyen
Centre for Pattern Recognition and Data Analytics, Australia

v.nguyen@deakin.edu.au

Dinh Phung
Centre for Pattern Recognition and Data Analytics, Australia

dinh.phung@deakin.edu.au

Editor: Koby Crammer

Abstract

One of the most challenging problems in kernel online learning is to bound the model size
and to promote model sparsity. Sparse models not only improve computation and memory
usage, but also enhance the generalization capacity – a principle that concurs with the law
of parsimony. However, inappropriate sparsity modeling may also signiﬁcantly degrade the
performance. In this paper, we propose Approximation Vector Machine (AVM), a model
that can simultaneously encourage sparsity and safeguard its risk in compromising the per-
formance. In an online setting context, when an incoming instance arrives, we approximate
this instance by one of its neighbors whose distance to it is less than a predeﬁned threshold.
Our key intuition is that since the newly seen instance is expressed by its nearby neigh-
bor the optimal performance can be analytically formulated and maintained. We develop
theoretical foundations to support this intuition and further establish an analysis for the
common loss functions including Hinge, smooth Hinge, and Logistic (i.e., for the classiﬁ-
cation task) and ℓ1, ℓ2, and ε-insensitive (i.e., for the regression task) to characterize the
gap between the approximation and optimal solutions. This gap crucially depends on two
key factors including the frequency of approximation (i.e., how frequent the approximation
operation takes place) and the predeﬁned threshold. We conducted extensive experiments
for classiﬁcation and regression tasks in batch and online modes using several benchmark
datasets. The quantitative results show that our proposed AVM obtained comparable pre-
dictive performances with current state-of-the-art methods while simultaneously achieving
signiﬁcant computational speed-up due to the ability of the proposed AVM in maintaining
the model size.

Keywords: kernel, online learning, large-scale machine learning, sparsity, big data, core
set, stochastic gradient descent, convergence analysis

∗. Part of this work was performed while the author was aﬃliated with the HCM University of Education.

2016 Trung Le, Tu Dinh Nguyen, Vu Nguyen and Dinh Phung.

c
(cid:13)

Le et al

1. Introduction

In modern machine learning systems, data usually arrive continuously in stream. To enable
eﬃcient computation and to eﬀectively handle memory resource, the system should be able
to adapt according to incoming data. Online learning represents a family of eﬃcient and
scalable learning algorithms for building a predictive model incrementally from a sequence
In contrast to the conventional
of data examples (Rosenblatt, 1958; Zinkevich, 2003).
learning algorithms (Joachims, 1999; Chang and Lin, 2011), which usually require a costly
procedure to retrain the entire dataset when a new instance arrives, online learning aims to
utilize the new incoming instances to improve the model given the knowledge of the correct
answers to previous processed data (and possibly additional available information), making
them suitable for large-scale online applications wherein data usually arrive sequentially
and evolve rapidly.

The seminal

line of work in online learning, referred to as linear online learning
(Rosenblatt, 1958; Crammer et al., 2006; Dredze et al., 2008), aims at learning a linear pre-
dictor in the input space. The crucial limitation of this approach lies in its over-simpliﬁed
linear modeling choice and consequently may fail to capture non-linearity commonly seen
in many real-world applications. This motivated the works in kernel-based online learning
(Freund and Schapire, 1999; Kivinen et al., 2004) in which a linear model in the feature
space corresponding with a nonlinear model in the input space, hence allows one to cope
with a variety of data distributions.

One common issue with kernel-based online learning approach, also known as the curse
of kernelization, is that the model size (i.e., the number of vectors with non-zero coeﬃcients)
may grow linearly with the data size accumulated over time, hence causing computational
problem and potential memory overﬂow (Steinwart, 2003; Wang et al., 2012). Therefore
in practice, one might prefer kernel-based online learning methods with guaranty on a
limited and bounded model size.
In addition, enhancing model sparsity is also of great
interest to practitioners since this allows the generalization capacity to be improved; and
in many cases leading to a faster computation. However, encouraging sparsity needs to be
done with care since an inappropriate sparsity-encouraging mechanism may compromise the
performance. To address the curse of kernelization, budgeted approaches (Crammer et al.,
2004; Dekel et al., 2005; Cavallanti et al., 2007; Wang and Vucetic, 2010; Wang et al., 2012;
Le et al., 2016a,c) limits the model size to a predeﬁned budget B. Speciﬁcally, when the
current model size exceeds this budget, a budget maintenance strategy (e.g., removal, pro-
jection, or merging) is triggered to recover the model size back to the budget B. In these
approaches, determining a suitable value for the predeﬁned budget in a principled way is
important, but challenging, since setting a small budget makes the learning faster but may
suﬀer from underﬁtting, whereas a large budget makes the model ﬁt better to data but may
dramatically slow down the training process. An alternative way to address the curse of
kernelization is to use random features (Rahimi and Recht, 2007) to approximate a kernel
function (Ming et al., 2014; Lu et al., 2015; Le et al., 2016b). For example, Lu et al. (2015)
proposed to transform data from the input space to the random-feature space, and then
performed SGD in the feature space. However, in order for this approach to achieve good
kernel approximation, excessive number of random features is required which could lead to
a serious computational issue. To reduce the impact number of random features, (Le et al.,

2

Approximation Vector Machines

2016b) proposed to distribute the model in dual space including the original feature space
and the random feature space that approximates the ﬁrst space.

Figure 1: An illustration of the hypersphere coverage for 1, 000 data samples which locate in
3D space. We cover this dataset using hyperspheres with the diameter δ = 7.0, resulting in
20 hypersphere cells as shown in the ﬁgure (cf. Sections (6.3,9)). All data samples in a same
cell are approximated by a core point in this cell. The model size is therefore signiﬁcantly
reduced from 1,000 to 20.1

In this paper, we propose Approximation Vector Machine (AVM) to simultaneously
encourage model sparsity2 while preserving the model performance. Our model size is
theoretically proven to be bounded regardless of the data distribution and data arrival
order. To promote sparsity, we introduce the notion of δ-coverage which partitions the data
space into overlapped cells whose diameters are deﬁned by δ (cf. Figure 1). This coverage
can be constructed in advance or on the ﬂy. Our experiment on the real datasets shows
that the coverage can impressively boost sparsity; for example with dataset KDDCup99 of
4, 408, 589 instances, our model size is 115 with δ = 3 (i.e., only 115 cells are required); with
dataset airlines of 5, 336, 471 instances, our model size is 388 with δ = 1.

In an online setting context, when an incoming instance arrives, it can be approximated
with the corresponding core point in the cell that contains it. Our intuitive reason is that
when an instance is approximated by an its nearby core point, the performance would be
largely preserved. We further developed rigorous theory to support this intuitive reason.

1. In fact, we used a subset of the dataset a9a which has 123 features. We then project all data points onto
3D using t-SNE. We note that the t-SNE does not do clustering, it only reduces the dimensionality into
3D for visualization while trying to preserve the local properties of the data.

2. Model sparsity can be computed as the ratio of the model size and the number of vectors received so far.

3

Le et al

In particular, our convergence analysis (covers six popular loss functions, namely Hinge,
smooth Hinge, and Logistic for classiﬁcation task and ℓ2, ℓ1, and ε-insensitive for regres-
sion task) explicitly characterizes the gap between the approximate and optimal solutions.
The analysis shows that this gap crucially depends on two key factors including the cell
diameter δ and the approximation process. In addition, the cell parameter δ can be used
to eﬃciently control the trade-oﬀ between sparsity level and the model performance. We
conducted extensive experiments to validate the proposed method on a variety of learning
tasks, including classiﬁcation in batch mode, classiﬁcation and regression in online mode on
several benchmark large-scale datasets. The experimental results demonstrate that our pro-
posed method maintains a comparable predictive performance while simultaneously achiev-
ing an order of magnitude speed-up in computation comparing with the baselines due to
its capacity in maintaining model size. We would like to emphasize at the outset that un-
like budgeted algorithms (e.g., (Crammer et al., 2004; Dekel et al., 2005; Cavallanti et al.,
2007; Wang and Vucetic, 2010; Wang et al., 2012; Le et al., 2016a,c)), our proposed method
is nonparametric in the sense that the number of core sets grow with data on demand, hence
care should be exercised in practical implementation.
The rest of this paper is organized as follows.

In Section 2, we review works mostly
In Section 3, we present the primal and dual forms of Support Vector
related to ours.
Machine (SVM) as they are important background for our work. Section 4 formulates the
proposed problem. In Section 5, we discuss the standard SGD for kernel online learning
with an emphasis on the curse of kernelization. Section 6 presents our proposed AVM with
full technical details. Section 7 devotes to study the suitability of loss functions followed by
Section 8 where we extend the framework to multi-class setting. Finally, in Section 9, we
conduct extensive experiments on several benchmark datasets and then discuss experimental
results as well as their implications. In addition, all supporting proof is provided in the
appendix sections.

2. Related Work

One common goal of online kernel learning is to bound the model size and to encourage
sparsity. Generally, research in this direction can be broadly reviewed into the following
themes.

Budgeted Online Learning. This approach limits the model size to a predeﬁned budget
B. When the model size exceeds the budget, a budget maintenance strategy is triggered to
decrement the model size by one. Three popular budget maintenance strategies are removal,
projection, and merging. In the removal strategy, the most redundant support vector is sim-
ply eliminated. In the projection strategy, the information of the most redundant support
vector is conserved through its projection onto the linear span of the remaining support
vectors. The merging strategy ﬁrst selects two vectors, and then merges them into one
before discarding them. Forgetron (Dekel et al., 2005) is the ﬁrst budgeted online learning
method that employs the removal strategy for the budget maintenance. At each iteration,
if the classiﬁer makes a mistake, it conducts a three-step update: (i) running the standard
Perceptron (Rosenblatt, 1958) update; (ii) shrinking the coeﬃcients of support vectors with
a scaling factor; and (iii) removing the support vector with the smallest coeﬃcient. Ran-
domized Budget Perceptron (RBP) (Cavallanti et al., 2007) randomly removes a support

4

Approximation Vector Machines

vector when the model size overﬂows the budget. Budget Perceptron (Crammer et al.,
2004) and Budgeted Passive Aggressive (BPA-S)(Wang and Vucetic, 2010) attempt to dis-
card the most redundant support vector (SV). Orabona et al. (2009) used the projection
to automatically discover the model size. The new vector is added to the support set if its
projection onto the linear span of others in the feature space exceeds a predeﬁned thresh-
old, or otherwise its information is kept through the projection. Other works involving
the projection strategy include Budgeted Passive Aggressive Nearest Neighbor (BPA-NN)
(Wang and Vucetic, 2010; Wang et al., 2012). The merging strategy was used in some works
(Wang and Vucetic, 2009; Wang et al., 2012).

Random Features. The idea of random features was proposed in (Rahimi and Recht,
2007). Its aim is to approximate a shift-invariant kernel using the harmonic functions. In
the context of online kernel learning, the problem of model size vanishes since we can store
the model directly in the random features. However, the arising question is to determine the
appropriate number of random features D to suﬃciently approximate the real kernel while
keeping this dimension as small as possible for an eﬃcient computation. Ming et al. (2014)
investigated the number of random features in the online kernel learning context. Recently,
Lu et al. (2015) proposed to run stochastic gradient descent (SGD) in the random feature
space rather than that in the real feature space. The theory accompanied with this work
shows that with a high conﬁdence level, SGD in the random feature space can suﬃciently
approximate that in the real kernel space. Nonetheless, in order to achieve good kernel
approximation in this approach, excessive number of random features is required, possibly
leading to a serious computational issue. To reduce the impact of the number of random
features to learning performance, (Le et al., 2016b) proposed to store core vectors in the
original feature space, whilst storing remaining vectors in the random feature space that
suﬃciently approximates the ﬁrst space.

Core Set. This approach utilizes a core set to represent the model. This core set can
be constructed on the ﬂy or in advance. Notable works consist of the Core Vector Machine
(CVM) (Tsang et al., 2005) and its simpliﬁed version, the Ball Vector Machine (BVM)
(Tsang et al., 2007). The CVM was based on the achievement in computational geometry
(Badoiu and Clarkson, 2002) to reformulate a variation of ℓ2-SVM as a problem of ﬁnding
minimal enclosing ball (MEB) and the core set includes the points lying furthest away the
current centre of the current MEB. Our work can be categorized into this line of thinking.
However, our work is completely diﬀerent to (Tsang et al., 2005, 2007) in the mechanism
to determine the core set and update the model. In addition, the works of (Tsang et al.,
2005, 2007) are not applicable for the online learning.

3. Primal and Dual Forms of Support Vector Machine

Support Vector Machine (SVM) (Cortes and Vapnik, 1995) represents one of the state-of-
the-art methods for classiﬁcation. Given a training set
, the
data instances are mapped to a feature space using the transformation Φ (.), and then SVM
aims to learn an optimal hyperplane in the feature space such that the margin, the distance
from the closest data instance to the hyperplane, is maximized. The optimization problem

(x1, y1) , . . . , (xN , yN )
}
{

=

D

5

Le et al

of SVM can be formulated as follows

λ
2 k

w

2 +
k

1
N

min
w,b  
wT

(cid:17)
0, i = 1, ..., N

(cid:16)
ξi ≥

N

ξi

!

Xi=1
1

≥

−

s.t. : yi

Φ (xi) + b

ξi, i = 1, ..., N

(1)

where λ > 0 is the regularization parameter, Φ (.) is the transformation from the input
space to the feature space, and ξ = [ξi]N

i=1 is the vector of slack variables.

Using Karush-Kuhn-Tucker theorem, the above optimization problem is transformed to

the dual form as follows

1
min
2
α
s.t. : yTα = 0

(cid:18)

αT

Qα

eTα

−

(cid:19)

0

αi ≤

≤

1
λN

, i = 1, ..., N

where Q = [yiyjK (xi, xj)]N
function, e = [1]N

×

1 is the vector of all 1, and y = [yi]

T
i=1,...,N.

T
i,j=1 is the Gram matrix, K (x, x′) = Φ (x)

Φ (x′) is a kernel

The dual optimization problem can be solved using the solvers (Joachims, 1999;
Chang and Lin, 2011). However, the computational complexity of the solvers is over-
quadratic (Shalev-Shwartz and Srebro, 2008) and the dual form does not appeal to the
online learning setting. To scale up SVM and make it appealing to the online learning, we
rewrite the constrained optimization problem in Eq. (1) in the primal form as follows

λ
2 k

w

2 +

k

1
N

minw

 

l (w; xi, yi)

!

N

Xi=1

(2)

where l (w; x, y) = max

0, 1

ywTΦ (x)

3 is Hinge loss.

−

(cid:0)

In our current interest, the advantages of formulating the optimization problem of SVM
in the primal form as in Eq. (2) are at least two-fold. First, it encourages the application of
SGD-based method to propose a solution for the online learning context. Second, it allows
us to extend Hinge loss to any appropriate loss functions (cf. Section 7) to enrich a wider
class of problems that can be addressed.

(cid:1)

3. We can eliminate the bias b by simply adjusting the kernel.

6

Approximation Vector Machines

4. Problem Setting

We consider two following optimization problems for batch and online settings respectively
in Eqs. (3) and (4)

minw f (w) , λ
2 k
, λ

w

2 + E
k

w

2 +

2 k

k

1
N

∼

N

Xi=1

(x,y)

PN [l (w; x, y)]

l (w; xi, yi)

(3)

(4)

minw f (w) , λ

w

2 + E

(x,y)

PX ,Y [l (w; x, y)]

∼

2 k

X
=

,
X
and the label domain

k
where l (w; x, y) is a convex loss function, P
is the joint distribution of (x, y) over
X × Y
Y
, and PN speciﬁes the empirical distribution
with the data domain
Y
. Furthermore, we assume that the convex
(x1, y1) , . . . , (xN , yN )
over the training set
}
{
loss function l (w; x, y) satisﬁes the following property: there exists two positive numbers
w, x, y. As demonstrated in Section
A and B such that
7, this condition is valid for all common loss functions. Hereafter, for given any function
(w0) to denote the gradient (or any sub-gradient) of g (.) w.r.t
g(w), we use the notation g
w evaluated at w0.

1/2 + B,

(w; x, y)

w
k

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

≤

D

A

∀

k

l

′

′

′

It is clear that given a ﬁxed w,

there exists a random variable g such that

E [g
P
,

(w).

In fact, we can specify g = λw + l

w] = f
∼
or PN . We assume that a positive semi-deﬁnite (p.s.d.) and isotropic (iso.) kernel
R

(w; xt, yt) where (xt, yt)

(Rasmussen and Williams, 2005) is used, i.e., K

, where k :

x, x

= k

|
Y

x

x

X

2

′

′

′

is an appropriate function. Let Φ (.) be the feature map corresponding the kernel (i.e.,
). To simplify the convergence analysis, without loss of general-
K

= Φ (x)

x, x

Φ

x

T

′

′

(cid:16)

(cid:17)

(cid:17)

(cid:16)

2 = K (x, x) = 1,
(cid:17)
ity we further assume that
k
solution of optimization problem in Eq. (3) or (4) by w∗, that is, w∗ = argminw f (w).

. Finally, we denote the optimal

Φ (x)
k

∈ X

(cid:16)

x

∀

−

(cid:18)(cid:13)
(cid:13)
(cid:13)

(cid:19)

(cid:13)
(cid:13)
(cid:13)

X →

5. Stochastic Gradient Descent Method

′

t

i Φ (xi). The vector xi (1

We introduce the standard kernel stochastic gradient descent (SGD) in Algorithm 1 wherein
the standard learning rate ηt = 1
λt is used (Shalev-Shwartz et al., 2007, 2011). Let αt be a
(wt; xt, yt) = αtΦ (xt) (we note that this scalar exists for all common loss
scalar such that l
functions as presented in Section 7). It is apparent that at the iteration t the model wt has
i=1 α(t)
the form of wt =
t) is said to be a support vector
i
if its coeﬃcient α(t)
is nonzero. The model is represented through the support vectors,
P
i
α(t)
and hence we can deﬁne the model size to be
0 and model sparsity as the ratio
α(t)
0 /t). Since it is likely that αt is nonzero
between the current model size and t (i.e.,
(cid:13)
(cid:13)
(e.g., with Hinge loss, it happens if xt lies in the margins of the current hyperplane), the
(cid:13)
(cid:13)
standard kernel SGD algorithm is vulnerable to the curse of kernelization, that is, the model
size, is almost linearly grown with the data size accumulated over time (Steinwart, 2003).
Consequently, the computation gradually becomes slower or even infeasible when the data
size grows rapidly.

(cid:13)
(cid:13)

(cid:13)
(cid:13)

≤

≤

7

Le et al

T

)
·

Φ (

Algorithm 1 Stochastic Gradient Descent algorithm.
Input: λ, p.s.d. kernel K (., .) = Φ (
)
·
1: w1 = 0
2: for t = 1, 2, . . . T do
Receive (xt, yt)
3:
ηt = 1
λt
gt = λwt + l
6: wt+1 = wt −
7: end for
Output: wT = 1
T

(wt; xt, yt) = λwt + αtΦ (xt)
ηtgt = t
−

T
t=1 wt or wT +1

t wt −

ηtαtΦ (xt)

4:

5:

1

′

P

//(xt, yt)

P

,

X

Y

∼

or PN

6. Approximation Vector Machines for Large-scale Online Learning

In this section, we introduce our proposed Approximation Vector Machine (AVM) for online
learning. The main idea is that we employ an overlapping partition of suﬃciently small
cells to cover the data domain, i.e.,
); when an instance arrives, we approximate
X
this instance by a corresponding core point in the cell that contains this instance. Our
intuition behind this approximation procedure is that since the instance is approximated
by its neighbor, the performance would not be signiﬁcantly compromised while gaining
signiﬁcant speedup. We start this section with the deﬁnition of δ-coverage, its properties
and connection with the feature space. We then present AVM and the convergence analysis.

or Φ (

X

6.1 δ-coverage over a domain

To facilitate our technical development in sequel, we introduce the notion of δ-coverage in
this subsection. We ﬁrst start with the usual deﬁnition of a diameter for a set.

Deﬁnition 1. (diameter ) Given a set A, the diameter of this set is deﬁned as D (A) =
sup
x,x′

. This is the maximal pairwise distance between any two points in A.

−

x

||

′

x
A||
∈

Next, given a domain

(e.g., the data domain, input space) we introduce the concept

of δ-coverage for

using a collection of sets.

X

X

Deﬁnition 2. (δ-coverage) The collection of sets
the domain
iﬀ
discrete) and each element Pi ∈ P
set I is ﬁnite, the collection

I Pi and D (Pi)
∈

X ⊂ ∪i

X

δ,

∈

∀

I is said to be an δ-coverage of
P
∈
I where I is the index set (not necessarily
is further referred to as a cell. Furthermore if the index

= (Pi)i

≤

i

is called a ﬁnite δ-coverage.

P
Deﬁnition 3. (core set, core point) Given an δ-coverage

, for each i

X
∈
ci (s) is called the core set
as a core point.

I over a given domain
∈
I, we select an arbitrary point ci from the cell Pi, then the collection of all
is further referred to

of the δ-coverage

P
. Each point ci ∈ C

= (Pi)i

P

C

We show that these deﬁnitions can be also extended to the feature space with the

mapping Φ and kernel K via the following theorem.

8

Approximation Vector Machines

Theorem 4. Assume that the p.s.d. and isotropic kernel K(x, x
, where
k (.) is a monotonically continuous decreasing function with k (0) = 1, is examined and
Φ (.) is its induced feature map.
then
X
k (δ2))
Φ (
Pi))i
is a monotonically increasing function and lim
0
→

(cid:17)
I is an δ-coverage of the domain
∈
2 (1

P
I is also an δΦ-coverage of the domain Φ (
∈

), where δΦ =

= (Pi)i

δΦ = 0.

) = (Φ (

) = k

p

−

−

X

P

If

(cid:16)

x

x

||

δ

′

2
||

′

In particular, the Gaussian kernel given by K(x, x

) = exp(

γ

x

−
γδ2)). Theorem 4 further reveals that the image of an
iso. kernel and δΦ =
δ-coverage in the input space is an δΦ-coverage in the feature space and when the diameter
δ approaches 0, so does the induced diameter δΦ. For readability, the proof of this theorem
is provided in Appendix A.

exp (

2 (1

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

p

−

−

−

) is a p.s.d. and

′

2

′

x

We have further developed methods and algorithms to eﬃciently construct δ-coverage,

however to maintain the readability, we defer this construction to Section 6.3.

6.2 Approximation Vector Machines

We now present our proposed Approximation Vector Machine (AVM) for online learning. In
an online setting, instances arise on the ﬂy and we need an eﬃcient approach to incorporate
incoming instances into the learner. Diﬀerent from the existing works (cf. Section 2), our
approach is to construct an δ-coverage
, and for each
= (Pi)i
incoming instance x we ﬁnd the cell Pi that contains this instance and approximate this
can either be constructed
instance by a core point ci ∈
in advance or on the ﬂy as presented in Section 6.3.

I over the input domain
∈

Pi. The coverage

and core set

X

P

P

C

′

∀

In Algorithm 2, when receiving an incoming instance (xt, yt), we compute the scalar
(wt; xt, yt) (cf. Section 7) in Step 5. Furthermore at Step 7 we
αt such that αtΦ (xt) = l
introduce a Bernoulli random variable Zt to govern the approximation procedure. This
random variable could be either statistically independent or dependent with the incoming
instances and the current model. In Section 9.2, we report on diﬀerent settings for Zt and
how they inﬂuence the model size and learning performance. Our ﬁndings at the outset
is that, the naive setting with P (Zt = 1) = 1,
t (i.e., always performing approximation)
returns the sparsest model while obtaining comparable learning performance comparing
with the other settings. Moreover, as shown in Steps 9 and 11, we only approximate the
incoming data instance by the corresponding core point (i.e., cit) if Zt = 1. In addition, if
Zt = 1, we ﬁnd a cell that contains this instance in Step 8. It is worth noting that the δ-
coverage and the cells are constructed on the ﬂy along with the data arrival (cf. Algorithms
3 and 4). In other words, the incoming data instance might belong to an existing cell or a
new cell that has the incoming instance as its core point is created.
wtk

1 in the case of ℓ2 loss, if λ
1
≤
1/2,
then we project wt −
ηtht onto the hypersphere with centre origin and radius ymaxλ−
. Since it can be shown that with ℓ2 loss the optimal solution w∗ lies
0, ymaxλ−
i.e.,
B
1/2
0, ymaxλ−
in
(cf. Theorem 23 in Appendix C), this operation could possibly result in
(cid:1)
(cid:0)
a faster convergence. In addition, by reusing the previous information, this operation can
be eﬃciently implemented. Finally, we note that with ℓ2 loss and λ > 1, we do not need to
wtk
perform a projection to bound
k

since according to Theorem 25 in Appendix C,

Furthermore to ensure that

is bounded for all t

wtk

1/2

≥

B

k

k

(cid:0)

(cid:1)

9

Le et al

is bounded by ymax
1 . Here it is worth noting that we have deﬁned ymax = maxy
−
this notation is only used in the analysis for the regression task with the ℓ2 loss.

λ

y
∈Y |

|

and

), δ-coverage
Φ (
·

P

= (Pi)i

I
∈

//(xt, yt)

P

,

X

Y

∼

or PN

//cf. Section 7

//do approximation

Algorithm 2 Approximation Vector Machine.
T
Input: λ, p.s.d. & iso. K (., .) = Φ (
)
·
1: w1 = 0
2: for t = 1, . . . , T do
Receive (xt, yt)
3:
ηt = 1
4:
λt
l
Sample a Bernoulli random variable Zt
if Zt = 1 then
Find it ∈
ht = λwt + αtΦ (cit)

I such that xt ∈

(wt; xt, yt) = αtΦ (xt)

Pit

9:

5:

7:

6:

8:

′

10:

11:

12:
13:

14:

15:

16:

else

ht = λwt + αtΦ (xt)

end if
if ℓ2 loss is used and λ

wt+1 =

else

B

Q

wt+1 = wt −

ηtht

end if
17:
18: end for
Output: wT = PT

wt

t=1
T

or wT +1

1 then

≤
(0,ymaxλ−1/2) (wt −

ηtht)

In what follows, we present the theoretical results for our proposed AVM including the
convergence analysis for a general convex or smooth loss function and the upper bound
of the model size under the assumption that the incoming instances are drawn from an
arbitrary distribution and arrive in a random order.

6.2.1 Analysis for Generic Convex Loss Function

We start with the theoretical analysis for Algorithm 2. The decision of approximation (i.e.,
the random variable Zt) could be statistically independent or dependent with the current
model parameter wt and the incoming instance (xt, yt). For example, one can propose an
algorithm in which the decision of approximation is performed iﬀ the conﬁdence level of the
incoming instance w.r.t the current model is greater than 1, i.e., ytwT
1. We shall
develop our theory to take into account all possible cases.

t Φ (xt)

≥

Theorem 5 below establishes an upper bound on the regret under the possible assump-
tions of the statistical relationship among the decision of approximation, the data distribu-
tion, and the current model. Based on Theorem 5, in Theorem 8 we further establish an
inequality for the error incurred by a single-point output with a high conﬁdence level.

Theorem 5. Consider the running of Algorithm 2 where (xt, yt) is uniformly sampled from
the training set
,

or the joint distribution P

, the following statements hold

D

X

Y

10

Approximation Vector Machines

T

t=1
X

T

Xt=1

i) If Zt and wt are independent for all t (i.e., the decision of approximation only depends

on the data distribution) then

E [f (wT )

f (w∗)]

−

H (log (T ) + 1)
2λT

+

δΦM 1/2W 1/2
T

≤

P (Zt = 1)1/2

where H, M, W are positive constants.

ii) If Zt is independent with both (xt, yt) and wt for all t (i.e., the decision of approxi-

mation is independent with the current hyperplane and the data distribution) then

E [f (wT )

f (w∗)]

−

H (log (T ) + 1)
2λT

+

δΦM 1/2W 1/2
T

≤

P (Zt = 1)

iii) In general, we always have

E [f (wT )

f (w∗)]

−

H (log (T ) + 1)
2λT

≤

+ δΦM 1/2W 1/2

Remark 6. Theorem 5 consists of the standard convergence analysis. In particular, if the
approximation procedure is never performed, i.e., P (Zt = 1) = 0,
t, we have the regret
H(log(T )+1)
bound E [f (wT )
2λT
Remark 7. Theorem 5 further indicates that there exists an error gap between the opti-
mal and the approximate solutions. When δ decreases to 0, this gap also decreases to 0.
Speciﬁcally, when δ = 0 (so does δΦ), any incoming instance is approximated by itself and
consequently, the gap is exactly 0.

f (w∗)]

≤

−

∀

.

Theorem 8. Let us deﬁne the gap by dT , which is δΦM 1/2W 1/2
P (Zt = 1)1/2(if Zt is
independent with wt), δΦM 1/2W 1/2
P (Zt = 1) (if Zt is independent with (xt, yt) and
P
wt), or δΦM 1/2W 1/2. Let r be any number randomly picked from
. With the
probability at least (1

δ), the following statement holds

1, 2, . . . , T
{

T
t=1

T
t=1

P

}

T

T

−

f (wr)

f (w∗)

−

H (log (T ) + 1)
2λT

≤

+ dT + ∆T

log

1
2

r

1
δ

where ∆T = max
T
t
≤

≤

1

(f (wt)

f (w∗)).

−

We now present the convergence analysis for the case when we output the α-suﬃx

average result as proposed in (Rakhlin et al., 2012). With 0 < α < 1, let us denote

where we assume that the fractional indices are rounded to their ceiling values.

Theorem 9 establishes an upper bound on the regret for the α-suﬃx average case,
followed by Theorem 10 which establishes an inequality for the error incurred by a α-suﬃx
average output with a high conﬁdence level.

wα

T =

1
αT

T

wt

Xt=(1
−

α)T +1

11

Le et al

Theorem 9. Consider the running of Algorithm 2 where (xt, yt) is uniformly sampled from
the training set
,
Y
i) If Zt and wt are independent for all t (i.e., the decision of approximation only depends

or the joint distribution P

, the following statements hold

D

X

on the data distribution) then

E [f (wα
T )

f (w∗)]

−

λ (1

α)

−
2α

W α

T +

δΦM 1/2W 1/2
αT

≤

P (Zt = 1)1/2+

H log (1/ (1
2λαT

−

α))

Xt=(1
−

α)T +1

T

T

Xt=(1
−

α)T +1

where H, M, W are positive constants and W α

T = E
ii) If Zt is independent with both (xt, yt) and wt for all t (i.e., the decision of approxi-

α)T +1 −

w(1

w∗

−

.

i

mation is independent with the current hyperplane and the data distribution) then

h(cid:13)
(cid:13)

2

(cid:13)
(cid:13)

E [f (wα
T )

f (w∗)]

−

λ (1

α)

−
2α

W α

T +

δΦM 1/2W 1/2
αT

≤

P (Zt = 1)+

H log (1/ (1
2λαT

−

α))

iii) In general, we always have

E [f (wα
T )

f (w∗)]

−

λ (1

α)

−
2α

≤

W α

T + δΦM 1/2W 1/2 +

H log (1/ (1
2λαT

−

α))

T + δΦM 1/2W 1/2

Theorem 10. Let us once again deﬁne the induced gap by dT , which is respec-
tively λ(1
α)
P (Zt = 1)1/2(if Zt is independent with wt),
2α W α
−
T + δΦM 1/2W 1/2
α)
λ(1
T
2α W α
is independent with (xt, yt) and
−
P
t=(1
α)T +1
αT
−
wt), or λ(1
α)
T + δΦM 1/2W 1/2.
2α W α
Let r be any number randomly picked from
−
δ), the following statement holds
α) T + 1, 2, . . . , T
(1
{

P
. With the probability at least (1

α)T +1
−
P (Zt = 1) (if Zt

T
t=(1

−

−

αT

}

f (wr)

f (w∗)

−

H log (1/ (1
2λαT

−

≤

α))

+ dT + ∆α
T

1
2

r

log

1
δ

where ∆α

T =

max
α)T +1
≤

(1

−

T

t
≤

(f (wt)

f (w∗)).

−

Remark 11. Theorems 8 and 10 concern with the theoretical warranty if rendering any
single-point output wr rather than the average outputs. The upper bound gained in
Theorem 10 is tighter than that gained in Theorem 8 in the sense that the quantity
H log(1/(1
2λαT

δ decreases faster and may decrease to 0 when T

given a

α))

+

1

→

∞

+ ∆α
−
T
conﬁdence level 1

2 log 1
δ.
q
−

6.2.2 Analysis for Smooth Loss Function

Deﬁnition 12. A loss function l (w; x, y) is said to be µ-strongly smooth w.r.t a norm
iﬀ for all u, v and (x, y) the following condition satisﬁes

.
k
k

l (v; x, y)

l (u; x, y) + l

(u; x, y)

(v

u) +

′

T

µ
2 k

v

u

2
k

−

−

Another equivalent deﬁnition of µ-strongly smooth function is

≤

(cid:13)
(cid:13)
(cid:13)

′

l

(u; x, y)

′

l

(v; x, y)

−

µ

v

k

u

k

−

≤

∗

(cid:13)
(cid:13)
(cid:13)

12

Approximation Vector Machines

where
.
k∗
k
It is well-known that

is used to represent the dual norm of the norm

.
.
k

k

ℓ2 loss is 1-strongly smooth w.r.t

k2.
.
Logistic loss is 1-strongly smooth w.r.t

k

k2.
.
k

•

•

•

τ -smooth Hinge loss (Shalev-Shwartz and Zhang, 2013) is 1
k2.
.

k

τ -strongly smooth w.r.t

2 + 1, λ

Theorem 13. Assume that ℓ2, Logistic, or τ -smooth Hinge loss is used, let us denote
L = λ
1 respectively. Let us deﬁne the gap by dT as in Theorem 10.
Let r be any number randomly picked from
. With the probability
α) T + 1, 2, . . . , T
at least (1

(1
{
δ), the following statement holds

2 + 1, or λ

2 + τ −

−

}

−

f (wr)

f (w∗)

−

H log (1/ (1
2λαT

−

≤

α))

+ dT +

LM α
T
2 r

1
2

log

1
δ

where M α

T =

max
α)T +1
≤

(1

−

T k

t
≤

wt −

.

w∗k

Remark 14. Theorem 13 extends Theorem 10 for the case of smooth loss function. This
allows the gap H log(1/(1
δ to be quantiﬁed more precisely regarding
−
2λαT
the discrepancy in the model itself rather than that in the objective function. The gap
H log(1/(1
2λαT

δ could possibly decrease rapidly when T approaches +

+ LM α

+ LM α

2 log 1

2 log 1

q

α))

α))

−

2

2

1

1

.

T

T

q

Algorithms
Forgetron (Dekel et al., 2005)
PA-I, II (Crammer et al., 2006)
Randomized Budget Perceptron (Cavallanti et al., 2007)
Projection (Orabona et al., 2009)

Kernelized Pegasos (Shalev-Shwartz et al., 2011)

Budgeted SGD (Wang et al., 2012)

Fourier OGD (Lu et al., 2015)

Nystrom OGD (Lu et al., 2015)

AVM (average output)

AVM (α-suﬃx average output)

∞

Budget
MB
NB
NB
AB

NB

MB

MB

MB

AB

AB

Regret
NA
NA
NA
NA
log(T )
T
log(T )
T
1
√T
1
√T
log(T )
(cid:16)
(cid:17)
T
1
T

O
(cid:16)

(cid:16)

(cid:17)

(cid:16)

(cid:16)
O

O

(cid:17)

(cid:17)

(cid:17)

O

O

O

(cid:0)

(cid:1)

Table 1: Comparison on the regret bounds and the budget sizes of the kernel online algo-
rithms. On the column of budget size, NB stands for Not Bound (i.e., the model size is not
bounded and learning method is vulnerable to the curse of kernelization), MB stands for
Manual Bound (i.e., the model size is manually bounded by a predeﬁned budget), and AB
is an abbreviation of Automatic Bound (i.e., the model size is automatically bounded and
this model size is automatically inferred).

13

Le et al

To end this section, we present the regret bound and the obtained budget size for our
AVM(s) together with those of algorithms listed in Table 1. We note that some early
works on online kernel learning mainly focused on the mistake rate and did not present any
theoretical results regarding the regret bounds.

6.2.3 Upper Bound of Model Size

In what follows, we present the theoretical results regarding the model size and sparsity
level of our proposed AVM. Theorem 15 shows that AVM oﬀers a high level of freedom to
control the model size. Especially, if we use the always-on setting (i.e., P (Zt = 1) = 1,
t),
the model size is bounded regardless of the data distribution and data arrival order.
Theorem 15. Let us denote P (Zt = 1) = pt, P (Zt = 0) = qt, and the number of cells
generated after the iteration t by Mt. If we deﬁne the model size, i.e., the size of support
set, after the iteration t by St, the following statement holds

∀

T

T

T

≤

1]

Mt

qt +

t=1
X

E [ST ]

ptE [Mt −
Specially, if we use some speciﬁc settings for pt, we can bound the model size E [St] accord-
ingly as follows
i) If pt = 1,
i.e., its number of cells.
β
ii) If pt = max
t

speciﬁes the size of the partition

t then E [ST ]

qt + E [MT ]

E [MT ]

, where

≤ |P|

t=1
X

t=1
X

0, 1

|P|

≤

≤

P

∀

−

,

,

−

−

(cid:17)
β
tρ
β
tρ

(cid:17)

(cid:16)
0, 1

∀
,

≤

β (log (T ) + 1) + E [MT ].
βT 1−ρ

t then E [ST ]
t, where 0 < ρ < 1, then E [ST ]
∀
t, where ρ > 1, then E [ST ]

≤

1

−
βζ (ρ) + E [MT ]

ρ + E [MT ].

iv) If pt = max
where ζ (.) is ζ- Riemann function deﬁned by the integral ζ (s) = 1
Γ(s)

≤

−

(cid:16)

(cid:17)

∀

,

(cid:16)
iii) If pt = max

0, 1

βζ (ρ) +

,
|P|

+
∞0

≤
ts−1
es

−

1 dt.

R

0, 1

, where ρ > 1 or ρ = +

Remark 16. We use two parameters β and ρ to ﬂexibly control the rate of approximation pt.
It is evident that when β increases, the rate of approximation decreases and consequently
the model size and accuracy increase. On the other hand, when ρ increases, the rate of
approximation increases as well and it follows that the model size and accuracy decreases.
We conducted experiment to investigate how the variation of these two parameters inﬂuence
the model size and accuracy (cf. Section 9.2).
Remark 17. The items i) and iv) in Theorem 15 indicate that if P (Zt = 1) = pt =
β
max
(by
tρ
) = 0). In fact, the tight upper bound is βζ (ρ) + E [MT ], where
convention we deﬁne ζ (+
MT is the number of unique cells used so far. It is empirically proven that MT could be
very small comparing with T and
T )
are all lain in the core set, if we output the average wT = PT
wα
Remark 18. The items ii) and iii) in Theorem 15 indicate that if P (Zt = 1) = pt =
max
1 then although the model size is not bounded, it would
slowly increase comparing with T , i.e., log (T ) or T 1

α)T +1 wT , the model size is still bounded.

. In addition, since all support sets of wt (1

, then the model size is bounded by βζ (ρ)+

ρ when ρ is around 1.

or α-suﬃx average

, where 0 < ρ

T = 1
αT

T
t=(1

0, 1

t=1
T

|P|

|P|

P

∞

∞

β
tρ

wt

≤

≤

−

−

≤

(cid:16)

(cid:17)

−

t

(cid:16)

(cid:17)

−

14

Approximation Vector Machines

6.3 Construction of δ-Coverage

In this section, we return to the construction of δ-coverage deﬁned in Section 6.1 and present
two methods to construct a ﬁnite δ-coverage. The ﬁrst method employs hypersphere cells
(cf. Algorithm 3) whereas the second method utilizes the hyperrectangle cells (cf. Algorithm
4). In these two methods, the cells in coverage are constructed on the ﬂy when the incoming
instances arrive. Both are theoretically proven to be a ﬁnite coverage.

Algorithm 3 Constructing hypersphere δ-coverage.

Algorithm 4 Constructing hyperrectangle δ-coverage.

=

5:

6:

1:
∅
P
2: n = 0
3: for t = 1, 2, . . . do
Receive (xt, yt)
4:
it = argmink
if
xt −
k
n = n + 1
cn = xt
it = n
=
P
end if
11:
12: end for

n k
cit k ≥

P ∪

[
B

10:

9:

8:

7:

≤

ckk
xt −
δ/2 then

(cn, δ/2)]

< a then

∅

P

=
1:
2: a = δ/√d
3: n = 0
4: for t = 1, 2, . . . do
Receive (xt, yt)
5:
it = 0
6:
for i = 1 to n do
xt −
k
it = i
break

cik∞

if

7:

8:

9:

10:

11:

12:

end if
end for
if

14:

15:

13:

it = 0 then
n = n + 1
cn = xt
it = n
=
P
end if
18:
19: end for

16:
17:

[
R

P ∪

(cn, a)]

Algorithm 3 employs a collection of open hypersphere cell
< R

(c, R), which is deﬁned
, to cover the data domain. Similar to Algorithm 3,

(c, R) =

Rd :

B

x

x

as

B

k

−

c
k

∈

(cid:8)

(cid:9)

15

Le et al

x
k

c
k∞

Algorithm 4 uses a collection of open hyperrectangle
, to cover the data domain.

Rd :

< a

x

R

(c, a), which is given by

(c, a) =

R

−

∈
Both Algorithms 3 and 4 are constructed in the common spirit: if the incoming instance
(cid:8)
(xt, yt) is outside all current cells, a new cell whose centre or vertex is this instance is
generated. It is noteworthy that the variable it in these two algorithms speciﬁes the cell
that contains the new incoming instance and is the same as itself in Algorithm 2.

(cid:9)

Theorem 19 establishes that regardless of the data distribution and data arrival order,
Algorithms 3 and 4 always generate a ﬁnite δ-coverage which implies a bound on the model
size of AVM. It is noteworthy at this point that in some scenarios of data arrival, Algo-
. However, since the
rithms 3 and 4 might not generate a coverage for the entire space
(ci, δ), without loss
(ci, δ) and
generated sequence
of generality we can restrict
(ci, δ)
∪i B
or

xt}t cannot be outside the set
∪i R
∪i B

∪i B
(ci, δ) by assuming that

X
∪i R

(ci, δ) or

to

=

=

X

X

{

(ci, δ).

X

∪i R

Theorem 19. Let us consider the coverages formed by the running of Algorithms 3 and 4.
is compact (i.e., close and bounded) then these coverages are all ﬁnite
If the data domain
δ-coverages whose sizes are all dependent on the data domain
and independent with the
sequence of incoming data instances (xt, yt) received.

X

X

Remark 20. Theorem 19 also reveals that regardless of the data arrival order, the model size
of AVM is always bounded (cf. Remark 17). Referring to the work of (Cucker and Smale,

. However with many possible
2002), it is known that this model size cannot exceed
data arrival orders, the number of active cells or the model size of AVM is signiﬁcantly
smaller than the aforementioned theoretical bound.

(cid:17)

(cid:16)

X

d

)

4D(
δ

6.4 Complexity Analysis

(cid:0)

d2Mt

We now present the computational complexity of our AVM(s) with the hypersphere δ-
coverage at the iteration t. The cost to ﬁnd the hypersphere cell in Step 5 of Algorithm
. The cost to calculate αt in Step 6 of Algorithm 2 is O (St) if we consider
2 is O
the kernel operation as a unit operation. If ℓ2 loss is used and λ
1, we need to do a pro-
(cid:1)
jection onto the hypersphere
which requires the evaluation of the length
of the vector wt −
) which costs St unit operations using incremen-
ηtht (i.e.,
(cid:1)
tal implementation. Therefore, the computational operation at the iteration t of AVM(s)
(since
or O
is either O
Mt ≤

0, ymaxλ−
ηthtk
St

B
wt −
(cid:0)
k
d2 + 1

d2Mt + St + St

d2Mt + St

d2 + 2

= O

= O

St).

St

(cid:0)(cid:0)

(cid:0)(cid:0)

1/2

≤

(cid:0)

(cid:1)

(cid:1)

(cid:0)

(cid:1)

(cid:1)

(cid:1)

(cid:1)

7. Suitability of Loss Functions

We introduce six types of loss functions that can be used in our proposed algorithm, namely
insensitive, and τ -smooth Hinge. We verify that these loss func-
Hinge, Logistic, ℓ2, ℓ1, ε
−
1/2 + B for some
(w; x, y)
tions satisfying the necessary condition, that is,
k
appropriate positive numbers A, B (this is required for our problem formulation presented
in Section 4).

w

≤

A

k

l

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

′

For comprehensibility, without loss of generality, we assume that
x

= K (x, x)1/2 =
. At the outset of this section, it is noteworthy that for classiﬁcation task (i.e.,

Φ (x)
k
k

1,

∀

∈ X

16

Approximation Vector Machines

Hinge, Logistic, and τ -smooth Hinge cases), the label y is either
implies

= y2 = 1.

−

1 or 1 which instantly

y
|

|

Hinge loss

•

l (w; x, y) = max

0, 1

′

l

(w; x, y) =

n
ywTΦ(x)

I
{

−

≤

1
}

ywT

Φ (x)

−

yΦ (x)

o

where IS is the indicator function which renders 1 if the logical statement S is true
and 0 otherwise.

Therefore, by choosing A = 0, B = 1 we have

′

l

(w; x, y)

Φ (x)

1 = A

≤ k

k ≤

w

k

k

1/2 + B

ℓ2 loss

•

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

1/2 + B for
In this case, at the outset we cannot verify that
k
all w, x, y. However, to support the proposed theory, we only need to check that

(w; x, y)

w

≤

A

k

l

′

1/2 + B for all t

1. We derive as follows

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

′

l

(wt; x, y)

A

wtk
k

≤

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

l (w; x, y) =

′

l

(w; x, y) =

≥

y

1
2
(cid:16)
wT
(cid:16)

wT

Φ (x)

2

−
Φ (x)

(cid:17)
Φ (x)

y

−

(cid:17)

′

l

(wt; x, y)

=

(cid:13)
(cid:13)
(cid:13)

wT
|

t Φ (x) + y
wtk

k k

Φ (x)

wT

Φ (x)

| k
+ ymax ≤

k ≤ |
A

k

t Φ (x)
wtk

|
1/2 + B

+ ymax

(cid:13)
(cid:13)
(cid:13)

≤ k
y1/2
maxλ−
y1/2
max (λ

(

1/4

−

where B = ymax and A =

if λ

1

≤
otherwise

.

1/2

1)−

Here we note that we make use of the fact that
(cf. Theorem 25 in Appendix C) and
Algorithm 2 ).

wtk ≤

k

wtk ≤
k
ymaxλ−

ymax (λ

1 if λ > 1
−
1/2 otherwise (cf. Line 12 in

1)−

ℓ1 loss

•

Therefore, by choosing A = 0, B = 1 we have

l (w; x, y) =

−
(w; x, y) = sign

′

l

y
|

wT
wT

Φ (x)

|
Φ (x)

y

Φ (x)

−

(cid:17)

(cid:16)

(w; x, y)

=

Φ (x)

1 = A

k

k ≤

1/2 + B

w

k

k

′

l

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

17

Le et al

Logistic loss

•

l (w; x, y) = log

′

l

(w; x, y) = −

Φ (x)

1 + exp

ywT
−
ywTΦ (x)
(cid:16)
y exp
−
ywTΦ (x)) + 1
exp (
(cid:0)
−

(cid:16)

(cid:1)

Φ (x)

(cid:17)(cid:17)

Therefore, by choosing A = 0, B = 1 we have

′

l

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(w; x, y)

<

Φ (x)

1 = A

k

k ≤

1/2 + B

w

k

k

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

•

•

ε-insensitive loss

l (w; x, y) = max

′

l

(w; x, y) = I

wT

Φ (x)

ε

0,

y
|

−

n
wTΦ(x)
|

y

−

{|

sign

>ε

}

| −
wT
(cid:16)

o
Φ (x)

y

Φ (x)

−

(cid:17)

Therefore, by choosing A = 0, B = 1 we have

′

l

(w; x, y)

Φ (x)

1 = A

≤ k

k ≤

w

k

k

1/2 + B

τ -smooth Hinge loss (Shalev-Shwartz and Zhang, 2013)

−

0
l (w; x, y) = 
1

1
2τ
I

−
{
1I
+ τ −
1

(w; x, y) =

′

l

ywTΦ (x)
1

τ
2
−
ywTΦ (x)

2

−
(cid:0)
ywTΦ(x)<1

τ

−

}
ywTΦ(x)

yΦ (x)
(cid:1)
ywT

1

≤

τ

−

≤

if ywTΦ (x) > 1
if ywTΦ (x) < 1
otherwise

τ

−

Φ (x)

1

yΦ (x)

−

(cid:17)

(cid:16)
Therefore, by choosing A = 0, B = 2, we have

′

l

(w; x, y)

(cid:13)
(cid:13)
(cid:13)

y
≤ |
= A

| k
w

Φ (x)
k
1/2 + B
k

k

(cid:13)
(cid:13)
(cid:13)

1

+ τ −

y
|

Φ (x)
k

τ

| k

≤

2

8. Multiclass Setting

In this section, we show that our proposed framework could also easily extend to the multi-
class setting. We base on the work of (Crammer and Singer, 2002) for multiclass classiﬁca-
tion to formulate the optimization problem in multi-class setting as

f (W ) , λ
2 k

W

2
2,2 +
k

1
N

min
W  

yiΦ (xi)

l

wT
(cid:16)

wT

−

ziΦ (xi)
(cid:17)

!

N

Xi=1

18

where we have deﬁned

Approximation Vector Machines

zi = argmax

wT

j Φ (xi) ,

j6=yi

W = [w1, w2, . . . , wm] ,

m

W
k

2
2,2 =
k

2 ,

wjk
k

Xj=1
max (0, 1
a) Hinge loss
−
a)
log (1 + e−

Logistic loss

(

l (a) =

w(t+1)
j

′

′

ηtl
j −
j + ηtl

1

1

t
−
t
−
t
−

t w(t)
t w(t)
t w(t)

1

j

= 


(a) Φ (xt)
(a) Φ (xt)

if j = yt
if j = zt
otherwise

For the exact update, at the t-th iteration, we receive the instance (xt, yt) and modify

W as follows

where a = wT

I
−
{
The algorithm for Approximation Vector Machine with multiclass setting proceeds as


ztΦ (xt) and l

1/ (1 + ea).

ytΦ (xt)

(a) =

wT

a<1

or

−

−

}

′

in Algorithm 5.

Algorithm 5 Multiclass Approximation Vector Machine.
Input: λ, p.s.d. & iso. kernel K (., .), δ-coverage
= (Pi)i
1: W1 = 0
2: for t = 1, . . . , T do
Receive (xt, yt)
3:
a = wT
max
ytΦ (xt)
−
j
=yt
1
t W (t)
−

j Φ (xt)

wT

P

4:

5: W (t+1) = t
6:

I
∈

Sample a Bernoulli random variable Zt
if Zt = 1 then
Find it ∈
w(t+1)
yt
w(t+1)
zt

I such that xt ∈
ηtl
yt −
zt + ηtl

Pit
(a) Φ (cit )
(a) Φ (cit )

= w(t+1)
= w(t+1)

′

′

//(xt, yt)

P

,

X

Y

∼

or PN

//do approximation

7:

8:

9:

10:

11:

else

13:

12:

w(t+1)
yt
w(t+1)
zt
end if
14:
15: end for
Output W

(T )

9. Experiments

= w(t+1)
= w(t+1)

ηtl
yt −
zt + ηtl

′

′

(a) Φ (xt)
(a) Φ (xt)

= PT

t=1 W (t)
T

or W (t+1)

In this section, we conduct comprehensive experiments to quantitatively evaluate the capac-
ity and scalability of our proposed Approximation Vector Machine (AVM) on classiﬁcation
and regression tasks under three diﬀerent settings:

19

Le et al

•

•

Batch classiﬁcation 4: the regular binary and multiclass classiﬁcation tasks that follow
a standard validation setup, wherein each dataset is partitioned into training set and
testing set. The models are trained on the training part, and then their discriminative
capabilities are veriﬁed on the testing part using classiﬁcation accuracy measure. The
computational costs are commonly measured based on the training time.

Online classiﬁcation: the binary and multiclass classiﬁcation tasks that follow a purely
online learning setup, wherein there is no division of training and testing sets as in
batch setting. The algorithms sequentially receive and process a single data sample
turn-by-turn. When an individual data point comes, the models perform prediction
to compute the mistake rate ﬁrst, then use the feature and label information of such
data point to continue their learning procedures. Their predictive performances and
computational costs are measured basing on the average of mistake rate and execution
time, respectively, accumulated in the learning progress on the entire dataset.

•

Online regression: the regression task that follows the same setting of online classiﬁ-
cation, except the predictive performances are measured based on the regression error
rate accumulated in the learning progress on the entire dataset.

Our main goal is to examine the scalability, classiﬁcation and regression capabilities of
AVMs by directly comparing with those of several recent state-of-the-art batch and online
learning approaches using a number of datasets with a wide range of sizes. Our models are
implemented in Python with Numpy package. The source code and experimental scripts are
published for reproducibility5. In what follows, we present the data statistics, experimental
setup, results and our observations.

9.1 Data statistics and experimental setup

We use 11 datasets whose statistics are summarized in Table 2. The datasets are selected in
a diverse array of sizes in order to clearly expose the diﬀerences among scalable capabilities
of the models. Five of which (year, covtype, poker, KDDCup99, airlines) are large-scale
datasets with hundreds of thousands and millions of data points, whilst the rest are ordinal-
size databases. Except the airlines, all of the datasets can be downloaded from LIBSVM6
and UCI7 websites.

The airlines dataset is provided by American Statistical Association (ASA8). The dataset
contains information of all commercial ﬂights in the US from October 1987 to April 2008.
The aim is to predict whether a ﬂight will be delayed or not and how long in minutes the
ﬂight will be delayed in terms of departure time. The departure delay time is provided in
the ﬂight database. A ﬂight is considered delayed if its delay time is above 15 minutes, and
non-delayed otherwise. The average delay of a ﬂight in 2008 was of 56.3 minutes. Following
the procedure of (Hensman et al., 2013), we further process the data in two steps. First, we
join the data with the information of individual planes basing on their tail numbers in order

4. This setting is also known as oﬄine classiﬁcation.
5. https://github.com/tund/avm
6. https://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/
7. https://archive.ics.uci.edu/ml/datasets.html
8. The data can be downloaded from http://stat-computing.org/dataexpo/2009/.

20

Approximation Vector Machines

Source
UCI

Dataset #training #testing #features #classes
2
2 LIBSVM
2 LIBSVM
2 LIBSVM
7 LIBSVM

16, 281
14, 951
271, 617
91, 701
58, 101
1, 000, 000
489, 842
592, 942

32, 561
49, 749
59, 535
49, 990
522, 911
25, 010
4, 408, 589
5, 336, 471

123
a9a
300
w8a
8
cod-rna
22
ijcnn1
54
covtype
10
poker
41
KDDCup99
airlines
8
Dataset #training #testing #features
9
384
90
8

UCI
UCI
ASA
Source
UCI
UCI
UCI
ASA

10
23
2
value
[0, 1]
[0, 1]
[0, 1]
R+

45, 730
53, 500
515, 345
5, 929, 413

casp
slice
year
airlines

–
–
–
–

Table 2: Data statistics. #training: number of training samples; #testing: number of
testing samples.

to obtain the manufacture year. This additional information is provided as a supplemental
data source on ASA website. We then extract 8 features of many available ﬁelds: the age of
the aircraft (computed based on the manufacture year), journey distance, airtime, scheduled
departure time, scheduled arrival time, month, day of week and month. All features are
normalized into the range [0, 1].

In batch classiﬁcation experiments, we follow the original divisions of training and testing
sets in LIBSVM and UCI sites wherever available. For KDDCup99, covtype and airlines
datasets, we split the data into 90% for training and 10% for testing. In online classiﬁcation
and regression tasks, we either use the entire datasets or concatenate training and testing
parts into one. The online learning algorithms are then trained in a single pass through
the data. In both batch and online settings, for each dataset, the models perform 10 runs
on diﬀerent random permutations of the training data samples. Their prediction results
and time costs are then reported by taking the average with the standard deviation of the
results over these runs.

For comparison, we employ some baseline methods that will be described in the fol-
lowing sections. Their C++ implementations with Matlab interfaces are published as a
part of LIBSVM, BudgetedSVM9 and LSOKL10 toolboxes. Throughout the experiments,

we utilize RBF kernel, i.e., K

′

x, x

for all algorithms including

(cid:18)
ours. We use hypersphere strategy to construct the δ-coverage (cf. Section 6.3), due to
its better performance than that of hyperrectangle approach during model evaluation. All
experiments are conducted using a Windows machine with 3.46GHz Xeon processor and
96GB RAM.

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:19)

(cid:17)

(cid:16)

= exp

γ

x

−

−

2

′

x

9. http://www.dabi.temple.edu/budgetedsvm/index.html

10. http://lsokl.stevenhoi.com/

21

Le et al

9.2 Model evaluation on the eﬀect of hyperparameters

In the ﬁrst experiment, we investigate the eﬀect of hyperparameters, i.e., δ-coverage di-
ameter, sampling parameters β and ρ (cf. Section 6.2.3) on the performance of AVMs.
Particularly, we conduct an initial analysis to quantitatively evaluate the sensitivity of
these hyperparameters and their impact on the predictive accuracy and model size. This
analysis provides a heuristic approach to ﬁnd the best setting of hyperparameters. Here the
AVM with Hinge loss is trained following the online classiﬁcation scheme using two datasets
a9a and cod-rna.

To ﬁnd the plausible range of coverage diameter, we use a heuristic approach as follows.
First we compute the mean and standard deviation of pairwise Euclidean distances between
any two data samples. Treating the mean as the radius, the coverage diameter is then varied
around twice of this mean bounded by twice of the standard deviation. Fig. 2a and Fig. 3a
report the average mistake rates and model sizes of AVMs with respect to (w.r.t) these
values for datasets a9a and cod-rna, respectively. Here we set β = 0 and ρ = 1.0. There is
a consistent pattern in both ﬁgures: the classiﬁcation errors increase for larger δ whilst the
model sizes decrease. This represents the trade-oﬀ between model performance and model
size via the model coverage. To balance the performance and model size, in these cases, we
can choose δ = 7.0 for a9a data and δ = 1.0 for cod-rna data.

(a) The eﬀect of δ-coverage diameter on the mis-
take rate and model size.

(b) The eﬀect of β and ρ on the classiﬁcation mistake
rate. β = 0 means always approximating.

Figure 2: Performance evaluation of AVM with Hinge loss trained using a9a dataset with
diﬀerent values of hyperparameters.

Fixing the coverage diameters, we vary β and ρ in 10 values monotonically increasing
from 0 to 10 and from 0.5 to 1.5, respectively, to evaluate the classiﬁcation performance. The
smaller β and larger ρ indicate that the machine approximates the new incoming data more
frequently, resulting in less powerful prediction capability. This can be observed in Fig. 2b
and Fig. 3b, which depict the average mistake rates in 3D as a function of these values for
dataset a9a and cod-rna. Here β = 0 means that the model always performs approximation
without respect to the value of ρ. From these visualizations, we found that the AVM with
always-on approximation mode still can achieve fairly comparable classiﬁcation results.
Thus we set β = 0 for all following experiments.

22

Approximation Vector Machines

(a) The eﬀect of δ-coverage diameter on the mis-
take rate and model size.

(b) The eﬀect of β and ρ on the classiﬁcation mistake
rate. β = 0 means always approximating.

Figure 3: Performance evaluation of AVM with Hinge loss trained using cod-rna dataset
with diﬀerent values of hyperparameters.

9.3 Batch classiﬁcation

We now examine the performances of AVMs in classiﬁcation task following batch mode.
We use eight datasets: a9a, w8a, cod-rna, KDDCup99, ijcnn1, covtype, poker and airlines
(delayed and non-delayed labels). We create two versions of our approach: AVM with Hinge
loss (AVM-Hinge) and AVM with Logistic loss (AVM-Logit). It is noteworthy that the Hinge
loss is not a smooth function with undeﬁned gradient at the point that the classiﬁcation
conﬁdence yf (x) = 1. Following the sub-gradient deﬁnition, in our experiment, we compute
the gradient given the condition that yf (x) < 1, and set it to 0 otherwise.

Baselines. For discriminative performance comparison, we recruit the following state-of-
the-art baselines to train kernel SVMs for classiﬁcation in batch mode:

•

•

•

•

•

•

LIBSVM: one of the most widely-used and state-of-the-art implementations for batch
kernel SVM solver (Chang and Lin, 2011). We use the one-vs-all approach as the
default setting for the multiclass tasks;

LLSVM: low-rank linearization SVM algorithm that approximates kernel SVM op-
timization by a linear SVM using low-rank decomposition of the kernel matrix
(Zhang et al., 2012);

BSGD-M: budgeted stochastic gradient descent algorithm which extends the Pegasos
algorithm (Shalev-Shwartz et al., 2011) by introducing a merging strategy for support
vector budget maintenance (Wang et al., 2012);

BSGD-R: budgeted stochastic gradient descent algorithm which extends the Pegasos
algorithm (Shalev-Shwartz et al., 2011) by introducing a removal strategy for support
vector budget maintenance (Wang et al., 2012);

FOGD: Fourier online gradient descent algorithm that applies the random Fourier
features for approximating kernel functions (Lu et al., 2015);

NOGD: Nystrom online gradient descent (NOGD) algorithm that applies the Nystrom
method to approximate large kernel matrices (Lu et al., 2015).

23

Le et al

Hyperparameters setting. There are a number of diﬀerent hyperparameters for all
methods. Each method requires a diﬀerent set of hyperparameters, e.g., the regularization
parameters (C in LIBSVM, λ in Pegasos and AVM), the learning rates (η in FOGD and
NOGD), the coverage diameter (δ in AVM) and the RBF kernel width (γ in all methods).
Thus, for a fair comparison, these hyperparameters are speciﬁed using cross-validation on
training subset.

Particularly, we further partition the training set into 80% for learning and 20% for val-
idation. For large-scale databases, we use only 1% of training set, so that the searching can
ﬁnish within an acceptable time budget. The hyperparameters are varied in certain ranges
and selected for the best performance on the validation set. The ranges are given as fol-
8, 2−
2−4/N, 2−2/N, ..., 216/N
,
lows: C
η
where N is the number of data points. The
(cid:9)
}
coverage diameter δ of AVM is selected following the approach described in Section 9.2. For
the budget size B in NOGD and Pegasos algorithm, and the feature dimension D in FOGD
for each dataset, we use identical values to those used in Section 7.1.1 of (Lu et al., 2015).

16.0, 8.0, 4.0, 2.0, 0.2, 0.02, 0.002, 0.0002

2, 20, 22, 24, 28

3, ..., 215

, γ
}

5, 2−

4, 2−

∈ {

∈ {

, λ

2−

2−

∈

∈

(cid:8)

(cid:9)

(cid:8)

Results. The classiﬁcation results, training and testing time costs are reported in Table 3.
Overall, the batch algorithms achieve the highest classiﬁcation accuracies whilst those of
online algorithms are lower but fairly competitive. The online learning models, however,
are much sparser, resulting in a substantial speed-up, in which the training time costs and
model sizes of AVMs are smallest with orders of magnitude lower than those of the standard
batch methods. More speciﬁcally, the LIBSVM outperforms the other approaches in most
of datasets, on which its training phase ﬁnishes within the time limit (i.e., two hours),
except for the ijcnn1 data wherein its testing score is less accurate but very close to that
of BSGD-M. The LLSVM achieves good results which are slightly lower than those of the
state-of-the-art batch kernel algorithm. The method, however, does not support multiclass
classiﬁcation. These two batch algorithms – LIBSVM and LLSVM could not be trained
within the allowable amount of time on large-scale datasets (e.g., airlines), thus are not
scalable.

Furthermore, six online algorithms in general have signiﬁcant advantages against the
batch methods in computational eﬃciency, especially when running on large-scale datasets.
Among these algorithms, the BSGD-M (Pegasos+merging) obtains the highest classiﬁcation
scores, but suﬀers from a high computational cost. This can be seen in almost all datasets,
especially for the airlines dataset on which its learning exceeds the time limit. The slow
B2
training of BSGD-M is caused by the merging step with computational complexity
(B is the budget size). By contrast, the BSGD-R (Pegasos+removal) runs faster than the
(cid:1)
merging approach, but suﬀers from very high inaccurate results due to its naive budget
maintenance strategy, that simply discards the most redundant support vector which may
contain important information.

O

(cid:0)

24

Approximation Vector Machines

|

|

|

S

B

Dataset [δ

Table 3: Classiﬁcation performance of our AVMs and the baselines in batch mode. The
notation [δ
D], next to the dataset name, denotes the diameter δ, the model size S
of AVM-based models, the budget size B of budgeted algorithms, and the number of random
features D of FOGD, respectively. The accuracy is reported in percent (%), the training
time and testing time are in second. The best performance is in bold. It is noteworthy that
the LLSVM does not support multiclass classiﬁcation and we terminate all runs exceeding
the limit of two hours, therefore some results are unavailable.
a9a [7.0
Train
84.57
50.73
232.59
90.48
15.99
82.40
4.96
5.35

|
|
Algorithm
LIBSVM
LLSVM
BSGD-M
BSGD-R
FOGD
NOGD
AVM-Hinge
AVM-Logit

4, 000]
Accuracy
84.92
83.00
0.16
3.38
5.05
2.18
0.50
0.34

135
|
Test
22.23
8.73
2.88
2.72
2.87
0.60
0.25
0.25

131
|
Test
2.95
10.41
5.16
4.98
3.55
0.65
0.52
0.52

98.17
97.10
97.92
98.06
96.87
96.96

84.76
80.26
81.15
82.33
83.55
83.83

1, 000

1, 000

D]

B

S

|

|

|

|

Dataset [δ

S

B

D]

cod-rna [1.0

436

400

|

500
|
Test
11.17
54.22
6.13
7.07
10.10
3.68
2.71
2.67

1, 000

|

97.69
90.90
90.64
90.43
91.14
91.19

1, 000

|

±
±
±
±
±
±
1, 600]
|
Accuracy
96.39
94.16
0.21
0.11
4.20
3.35
1.16
2.11

95.67
66.83
92.65
91.83
94.38
93.10

w8a [13.0
Train
50.96
92.19
264.70
253.30
32.16
374.87
11.84
12.54
ijcnn1 [1.0
Train
38.63
40.62
93.05
41.70
7.31
21.58
6.47
6.86
poker [12.0
Train
40.03
–
414.09
35.76
9.61
118.54
3.86
3.36

±
±
±
±
±
±
1, 600]
|
Accuracy
–
–
0.16
1.69
5.85
2.96
0.37
0.34
400] airlines [1.0

±
±
±
±
±
±

393
|
Test
932.58
–
123.57
102.84
101.29
36.84
8.21
7.54

72.26
61.09
59.34
68.20
64.31
64.42

Train
–
–
–
4,741.68
1,085.73
3,112.08
586.90
642.23

|
Test
–
–
–
29.98
861.52
18.53
6.55
6.10

|

200
|
Accuracy
99.91
–
0.00
2.26
0.11
0.02
0.05
0.06

99.73
39.81
99.75
99.80
99.82
99.72

±
±
±
±
±
±

25

4, 000]
|
Accuracy
99.06
98.64
0.07
±
0.04
±
0.38
±
0.18
±
0.28
±
0.00
±
4, 000]
|
Accuracy
97.35
96.99
0.11
±
0.18
±
0.07
±
1.22
±
0.71
±
0.95
±
4, 000]
|
Accuracy
57.91
–
0.22
1.05
5.00
0.27
0.13
0.17

±
±
±
±
±
±
4, 000]
Accuracy
–
–
–
0.06
0.21
0.20
0.00
0.00

±
±
±
±
±

|

54.10
52.14
46.62
54.65
55.49
55.60

80.27
80.37
74.83
80.72
80.72

Dataset [δ

S

B

59

400

|

|

|

|

|
|
Algorithm
LIBSVM
LLSVM
BSGD-M
BSGD-R
FOGD
NOGD
AVM-Hinge
AVM-Logit

|
|
Algorithm
LIBSVM
LLSVM
BSGD-M
BSGD-R
FOGD
NOGD
AVM-Hinge
AVM-Logit

|
|
Algorithm
LIBSVM
LLSVM
BSGD-M
BSGD-R
FOGD
NOGD
AVM-Hinge
AVM-Logit

Train
114.90
20.17
90.62
19.31
7.62
9.81
6.52
7.03

|
Test
85.34
19.38
5.66
5.48
11.95
3.24
2.69
2.86

D]

covtype [3.0
|
Train
Test
–
–
–
–
2,413.15
3.75
418.68
3.02
69.94
2.45
679.50
0.76
60.27
0.26
0.22
61.92
D] KDDCup99 [3.0
Test
Train
661.04
4,380.58
–
–
21.25
2,680.58
14.33
1,644.25
22.73
706.20
3.11
3,726.21
2.75
554.42
2.80
576.76

Dataset [δ

S

B

115

|

388

1, 000

|

Le et al

In terms of predictive performance, our proposed methods outperform the recent ad-
vanced online learning algorithms – FOGD and NOGD in most scenarios. The AVM-based
models are able to achieve slightly less accurate but fairly comparable results compared
with those of the state-of-the-art LIBSVM algorithm. In terms of sparsity and speed, the
AVMs are the fastest ones in the training and testing phases in all cases thanks to their
remarkable smaller model sizes. The diﬀerence between the training speed of our AVMs
and that of two approaches varies across datasets. The gap is more signiﬁcant for datasets
with higher dimensional feature spaces. This is expected because the procedure to compute
random features for each data point of FOGD involves sin and cos operators which are
costly. These facts indicate that our proposed online kernel learning algorithms are both
eﬃcient and eﬀective in solving large-scale kernel classiﬁcation problems. Thus we believe
that the AVM is the fast alternative to the existing SVM solvers for large-scale classiﬁcation
tasks.

Finally, comparing two versions of AVMs, it can be seen that the discriminative perfor-
mances of AVM with Logistic loss are better than those of AVM with Hinge loss in most
of datasets. This is because the Logistic function is smoother than the Hinge function,
whilst the Hinge loss encourages sparsity of the model. The AVM-Logit, however, contains
additional exponential operators, resulting in worse training time.

9.4 Online classiﬁcation

The next experiment investigates the performance of the AVMs in online classiﬁcation task
where individual data point continuously come turn-by-turn in a stream. Here we also
use eight datasets and two versions of our approach: AVM with Hinge loss (AVM-Hinge)
and AVM with Logistic loss (AVM-Logit) which are used in batch classiﬁcation setting (cf.
Section 9.3).

Baselines. We recruit the two widely-used algorithms – Perceptron and OGD for regular
online kernel classiﬁcation without budget maintenance and 8 state-of-the-art budget online
kernel learning methods as follows:

Perceptron:
(Freund and Schapire, 1999);

the kernelized variant without budget of Perceptron algorithm

OGD: the kernelized variant without budget of online gradient descent (Kivinen et al.,
2004).

RBP: a budgeted Perceptron algorithm using random support vector removal strategy
(Cavallanti et al., 2007);

Forgetron: a kernel-based Perceptron maintaining a ﬁxed budget by discarding oldest
support vectors (Dekel et al., 2005);

Projectron: a Projectron algorithm using the projection strategy (Orabona et al.,
2009);

Projectron++: the aggressive version of Projectron algorithm (Orabona et al., 2009);

•

•

•

•

•

•

26

Approximation Vector Machines

BPAS: a budgeted variant of Passive-Aggressive algorithm with simple SV removal
strategy (Wang and Vucetic, 2010);

BOGD: a budgeted variant of online gradient descent algorithm using simple SV
removal strategy (Zhao et al., 2012);

FOGD and NOGD: described in Section 9.3.

•

•

•

Hyperparameters setting. For each method learning on each dataset, we follow the
same hyperparameter setting which is optimized in the batch classiﬁcation task. For time
eﬃciency, we only include the fast algorithms FOGD, NOGD and AVMs for the experiments
on large-scale datasets. The other methods would exceed the time limit when running on
such data.

Results. Fig. 4 and Fig. 5 shows the relative performance convergence w.r.t classiﬁcation
error and computation cost of the AVMs in comparison with those of the baselines. Com-
bining these two ﬁgures, we compare the average mistake rate and running time in Fig. 6.
Table 4 reports the ﬁnal average results in detailed numbers after the methods see all data
samples. It is worthy to note that for the four biggest datasets (KDDCup99, covtype, poker,
airlines) that consist of millions data points, we exclude the non-budgeted online learning
algorithm because of their substantially expensive time costs. From these results, we can
draw some observations as follows.

First of all, as can be seen from Fig. 4, there are three groups of algorithms that have
diﬀerent learning progresses in terms of classiﬁcation mistake rate. The ﬁrst group includes
the BOGD, Projectron and Forgetron that have the error rates ﬂuctuating at the beginning,
but then being stable till the end. In the meantime, the rates of the models in the second
group, including Perceptron, OGD, RBP, Projectron++ and BPAS, quickly saturate at a
plateau after these methods see a few portions, i.e., one-tenth to two-tenth, of the data. By
contrast, the last group includes the recent online learning approaches – FOGD, NOGD, and
our proposed ones – AVM-Hinge, AVM-Logit, that regularly perform better as more data
points come. Exceptionally, for the dataset w8a, the classiﬁcation errors of the methods in
the ﬁrst group keep increasing after seeing four-tenth of the data, whilst those of the last
group are unexpectedly worse.

Second, Fig. 6 plots average mistake rate against computational cost, which shows sim-
ilar patterns as in the our ﬁrst observation. In addition, it can be seen from Fig. 5 that
all algorithms have normal learning pace in which the execution time is accumulated over
the learning procedure. Only the Projectron++ is slow at the beginning but then performs
faster after receiving more data.

According to ﬁnal results summarized in Table 4, the budgeted online approaches show
eﬃcacies with substantially faster computation than the ones without budgets. This is
more obvious for larger datasets wherein the execution time costs of our proposed models
are several orders of magnitude lower than those of regular online algorithms. This is
because the coverage scheme of AVMs impressively boost their model sparsities, e.g., using
δ = 3 resulting in 115 core points for dataset KDDCup99 consisting of 4, 408, 589 instances,
and using δ = 1 resulting in 388 core points for dataset airlines containing 5, 336, 471 data
samples.

27

0.24

0.23

0.22

0.21

0.20

0.19

0.18

0.17

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.16
0

0.24

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.23

0.22

0.21

0.20

0.19

0.18

0.17

0.40

0.35

0.30

0.25

0.20

0.15

0.10

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.05

0. 0

0.0060

0.0055

0.0050

0.0045

0.0040

0.0035

0.0030

0.0025

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.54

0.52

0.50

0.48

0.46

0.44

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

Perceptron
OGD

RBP
Forgetron

Projectron
Projectron++

FOGD
NOGD

AVM-hinge
AVM-logit

1

2

4

Number of samples

5
104

×

BPAS
BOGD

3

0.16

0

1

2

3

Number of samples

4

5
104

×

1

2

3

4

5

6

Number of samples

7
104

×

(a) a9a

(b) w8a

0. 5

1. 0

1. 5

2. 0

2. 5

3. 0

0. 2

0. 4

0. 6

0. 8

1. 0

1. 2

Number of samples

(c) cod-rna

3. 5

105

×

0.08

0. 0

Number of samples

(d) ijcnn1

1. 4

105

×

0.0020

0

1

2

3

Number of samples

4

5
106

×

0.34

0

1

2

3
Number of samples

4

5

6
105

×

(e) KDDCup99

(f) covtype

0.42

0. 0

0. 2

0. 4

0. 6
Number of samples

0. 8

1. 0

1. 2

106

×

0.19

0

1

2

3
Number of samples

4

5

6
106

×

(g) poker

(h) airlines

Figure 4: Convergence evaluation of online classiﬁcation tasks: the average rate of mistakes
as a function of the number of samples seen by the models. (Best viewed in colors).

Le et al

0.060

0.055

0.050

0.045

0.040

0.035

0.030

0.025

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.020

0

0.17

0.16

0.15

0.14

0.13

0.12

0.11

0.10

0.09

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.46

0.44

0.42

0.40

0.38

0.36

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.26

0.25

0.24

0.23

0.22

0.21

0.20

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

28

Approximation Vector Machines

Perceptron
OGD

RBP
Forgetron

Projectron
Projectron++

FOGD
NOGD

AVM-hinge
AVM-logit

1

2

4

Number of samples

5
104

×

BPAS
BOGD

3

0.5

0

1

2

3

Number of samples

4

5
104

×

1

2

3

4

5

6

Number of samples

7
104

×

(a) a9a

(b) w8a

0.5

0. 0

0. 5

1. 0

1. 5

2. 0

2. 5

3. 0

0. 2

0. 4

0. 6

0. 8

1. 0

1. 2

3. 5

105

×

0.0

0. 0

Number of samples

(d) ijcnn1

1. 4

105

×

Number of samples

(c) cod-rna

1.5

0

1

2

3

Number of samples

4

5
106

×

2

3
Number of samples

4

5

6
105

×

0.5

0

1

(e) KDDCup99

(f) covtype

0.24

0.23

0.22

0.21

0.20

0.19

0.18

0.17

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.16
0

3.5

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

3.0

2.5

2.0

1.5

1.0

0.5

0.0

3.5

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

3.0

2.5

2.0

1.5

1.0

4.0

3.5

3.0

2.5

2.0

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

4.0

3.5

3.0

2.5

2.0

1.5

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

1.0

0. 0

0. 2

0. 4

0. 6
Number of samples

0. 8

1. 0

1. 2

106

×

0

1

2

3
Number of samples

4

5

6
106

×

(g) poker

(h) airlines

Figure 5: Convergence evaluation of online classiﬁcation task: the average time costs (sec-
onds shown in the logarithm with base 10) as a function of the number of samples seen by
the models. (Best viewed in colors).

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

3.5

3.0

2.5

2.0

1.5

1.0

0.5

0.0

0.5

0

3.0

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

2.5

2.0

1.5

1.0

0.5

3.0

2.5

2.0

1.5

1.0

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

3.5

3.0

2.5

2.0

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

29

0.24

0.23

0.22

0.21

0.20

0.19

0.18

0.17

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.16
0

0.24

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.23

0.22

0.21

0.20

0.19

0.18

0.17

0.16

0.40

0.35

0.30

0.25

0.20

0.15

0.10

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.0060

0.0055

0.0050

0.0045

0.0040

0.0035

0.0030

0.0025

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.0020

1.5

0.54

0.52

0.50

0.48

0.46

0.44

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

Perceptron
OGD

RBP
Forgetron

Projectron
Projectron++

FOGD
NOGD

AVM-hinge
AVM-logit

1

2

4

Number of samples

5
104

×

BPAS
BOGD

3

0.5

0.0

0.5

2.5

3.0

3.5

0.5

0.0

0.5

2.5

3.0

3.5

1.0

1.5
Average time cost (log10t)

2.0

(a) a9a

1.0

1.5
Average time cost (log10t)

2.0

(b) w8a

0.05

0.5

1.0

3.0

3.5

0.08

0.0

0.5

1.5
2.5
2.0
Average time cost (log10t)

(c) cod-rna

1.0
2.0
1.5
Average time cost (log10t)

2.5

3.0

(d) ijcnn1

2.0

2.5

3.0

3.5

4.0

1.0

1.5

2.0

2.5

3.0

0.34

0.5

Average time cost (log10t)

(e) KDDCup99

Average time cost (log10t)

(f) covtype

0.42

1.0

1.5

2.0
3.0
2.5
Average time cost (log10t)

(g) poker

3.5

4.0

2.0

2.5

3.0
Average time cost (log10t)

3.5

(h) airlines

Figure 6: Average mistake rate vs. time cost for online classiﬁcation. The average time
(seconds) is shown in the logarithm with base 10. (Best viewed in colors).

Le et al

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.060

0.055

0.050

0.045

0.040

0.035

0.030

0.025

0.020

0.17

0.16

0.15

0.14

0.13

0.12

0.11

0.10

0.09

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.46

0.44

0.42

0.40

0.38

0.36

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.26

0.25

0.24

0.23

0.22

0.21

0.20

0.19

30

Approximation Vector Machines

Table 4: Classiﬁcation performance of our proposed methods and the baselines in online
mode. Note that δ, B and D are set to be the same as in batch classiﬁcation tasks (cf.,
Section 9.3). The mistake rate is reported in percent (%) and the execution time is in
second. The best performance is in bold.

a9a [142]

w8a [131]

9.79
7.81
26.02
28.56
11.16

±
±
±
±
±
±
±
±
±
±
±
±

±
±
±
±
±
±
±
±
±
±
±
±

3.51
2.54
4.02
3.96
4.76
3.08
2.37
3.16
3.52
2.55
4.62
5.80

21.05
16.50
23.76
23.15
21.86
19.47
19.09
22.14
20.11
16.55
17.46
17.33

Dataset [S]
Algorithm Mistake Rate
Time Mistake Rate
0.03
0.12
Perceptron
976.79
0.06 2,539.46
0.03
OGD
0.07
118.25
0.21
RBP
0.10
109.71
0.34
Forgetron
1.13
122.08
1.73
Projectron
0.63
449.20
2.22
Projectron++
0.02
95.81
0.17
BPAS
0.08
96.11
0.25
BOGD
0.05
13.79
0.10
FOGD
0.05
99.54
0.07
NOGD
8.74
0.78
0.12
AVM-Hinge
0.02
9.31
0.16
AVM-Logit
Dataset [S]
ijcnn1 [500]
cod-rna [436]
Algorithm Mistake Rate
0.04
Perceptron
±
0.03
OGD
±
0.39
RBP
±
2.22
Forgetron
±
3.61
Projectron
±
15.60
Projectron++
±
11.97
0.09
BPAS
38.13
0.11
BOGD
7.15
0.03
FOGD
7.83
0.06
NOGD
5.61
0.17
AVM-Hinge
0.20
6.01
AVM-Logit
Dataset [S]
KDDCup99 [115]
Algorithm
FOGD
NOGD
AVM-Hinge
AVM-Logit
Dataset [S]
Algorithm Mistake Rate
0.04
0.16
0.09
0.07

Time Mistake Rate
0.09
0.06
0.21
0.26
0.23
0.09
0.05
0.18
0.03
0.08
0.18
0.20
covtype [59]

Time Mistake rate
Mistake rate
0.05
620.95
0.00
0.07
0.00 4,009.03
0.16
540.65
0.07
503.34
0.16
0.03
airlines [388]
poker [393]

Time Mistake Rate
0.01
928.89
0.01
4,920.33
0.00
122.59
0.00
124.86

1,393.56
2,804.01
85.84
102.64
97.38
1,799.93
92.08
104.60
53.45
105.18
40.89
45.67

12.85
10.39
15.54
16.17
12.98
9.97
10.68
10.87
9.41
10.43
8.01
8.07

FOGD
NOGD
AVM-Hinge
AVM-Logit

±
±
±
±
±
±
±
±
±
±
±
±

40.45
34.72
36.11
35.92

52.28
44.90
43.85
43.97

20.98
25.56
19.28
19.28

0.35
0.23
0.31
0.28

±
±
±
±
±
±

±
±
±
±

±
±
±
±

17.97

±
±
±
±

±
±
±
±

Time
691.80
1,290.13
544.83
557.75
572.20
1321.93
681.46
589.47
26.40
585.23
16.89
17.86

Time
727.90
960.44
54.29
60.54
59.37
749.70
55.44
55.99
25.93
59.36
23.26
23.36

Time
223.20
838.47
51.12
53.51

Time
1,270.75
3,553.50
733.72
766.19

31

Le et al

For classiﬁcation capability, the non-budgeted methods only surpass the budgeted ones
for the smallest dataset, that is, the OGD obtains the best performance for a9a data. This
again demonstrates the importance of exploring budget online kernel learning algorithms.
Between the two non-budgeted algorithms, the OGD achieves considerably better error
rates than the Perceptron. The method, however, must perform much more expensive
updates, resulting in a signiﬁcantly larger number of support vectors and signiﬁcantly higher
computational time costs. This represents the trade-oﬀ between classiﬁcation accuracy and
computational complexity of the OGD.

Furthermore, comparing the performance of diﬀerent existing budgeted online kernel
learning algorithms, the AVM-Hinge and AVM-Logit outperform others in both discrimi-
native performance and computation eﬃciency for almost all datasets. In particular, the
AVM-based methods achieve the best mistake rates – 5.61
0.09,
19.28
0.00 for the cod-rna, ijcnn1, poker and airlines data, that are, respectively, 27.5%,
17.5%, 2.4%, 8.8% lower than the error rates of the second best models – two recent ap-
proaches FOGD and NOGD. On the other hand, the computation costs of the AVMs are
signiﬁcantly lower with large margins of hundreds of percents for large-scale databases cov-
type, poker, and airlines as shown in Table 4.

0.18, 43.85

0.17, 8.01

±

±

±

±

In all experiments, our proposed method produces the model sizes that are much smaller
than the budget sizes of baseline methods. Thus we further investigate the performance
of the budgeted baselines by varying the budget size B, and compare with our AVM with
It can be seen
Hinge loss. Fig. 7 shows our analysis on two datasets a9a and cod-rna.
that the larger B helps model obtain better classiﬁcation results, but hurts their running
speed. For both datasets, the budgeted baselines with larger budget sizes still fail to beat
the predictive performance of AVM. On the other hand, the baselines with smaller budget
sizes run faster than the AVM on cod-rna dataset, but slower on a9a dataset.

B = 50
B = 100
B = 150
B = 250
B = 436
B = 1000

RBP
Forgetron
Projectron
BPAS
BOGD

)

%

(
 
e
t
a
r
 
e
k
a
t
s
i
M

32

30

28

26

24

22

20

18

)
s
4
7
.
8
 
,
2
4
1
=
B
(
 

M
V
A

B = 50
B = 100
B = 142
B = 250
B = 500
B = 1000

RBP
Forgetron
Projectron
BPAS
BOGD

40

35

30

25

20

15

10

)

%

(
 
e
t
a
r
 
e
k
a
t
s
i
M

5
30

)
s
9
8
.
0
4
 
,
6
3
4
=
B
(
 

M
V
A

0

20

40

100

120

140

40

50

60

90

100

110

120

AVM (B=142, 17.46%)

60

80
Time (s)

(a) a9a

AVM (B=436, 5.61%)

70
80
Time (s)

(b) cod-rna

Figure 7: Predictive and wall-clock performance on two datasets: a9a and cod-rna of
budgeted methods when the budget size B is varied. (Best viewed in colors).

Finally, two versions of AVMs demonstrate similar discriminative performances and
computational complexities wherein the AVM-Logit is slightly slower due to the additional
exponential operators as also seen in batch classiﬁcation task. All aforementioned observa-
tions validate the eﬀectiveness and eﬃciency of our proposed technique. Thus, we believe

32

Approximation Vector Machines

that our approximation machine is a promising technique for building scalable online kernel
learning algorithms for large-scale classiﬁcation tasks.

9.5 Online regression

The last experiment addresses the online regression problem to evaluate the capabilities of
our approach with three proposed loss functions – ℓ1,ℓ2 and ε-insensitive losses as described
in Section 7. Incorporating these loss functions creates three versions: AVM-ε, AVM-ℓ1 and
AVM-ℓ2. We use four datasets: casp, slice, year and airlines (delay minutes) with a wide
range of sizes for this task. We recruit six baselines: RBP, Forgetron, Projectron, BOGD,
FOGD and NOGD (cf. more detailed description in Section 9.4).

Hyperparameters setting. We adopt the same hyperparameter searching procedure
for batch classiﬁcation task as in Section 9.3. Furthermore, for the budget size B and
the feature dimension D in FOGD, we follow the same strategy used in Section 7.1.1 of
(Lu et al., 2015). More speciﬁcally, these hyperparameters are separately set for diﬀerent
datasets as reported in Table 5. They are chosen such that they are roughly proportional to
the number of support vectors produced by the batch SVM algorithm in LIBSVM running
on a small subset. The aim is to achieve competitive accuracy using a relatively larger
budget size for tackling more challenging regression tasks.

Results. Fig. 8a and Fig. 8b shows the relative performance convergence w.r.t regression
error (root mean square root - RMSE) and computation cost (seconds) of the AVMs in
comparison with those of the baselines. Combining these two ﬁgures, we compare the
average error and running time in Fig. 9. Table 5 reports the ﬁnal average results in
detailed numbers after the methods traverse all data samples. From these results, we can
draw some observations as follows.

First of all, as can be seen from Fig. 8a, there are several diﬀerent learning behaviors
w.r.t regression loss, of the methods training on individual datasets. All algorithms, in
general, reach their regression error plateaus very quickly as observed in the datasets year
and airlines where they converge at certain points from the initiation of the learning. On
the other hand, for casp and slice databases, the AVM-based models regularly obtain better
performance, that is, their average RMSE scores keep reducing when receiving more data,
except in slice data, the regression performance of AVM-ℓ2 are almost unchanged during
the learning. Note that, for these two datasets, the learning curve of AVM-ε coincides, thus
is overplotted by that of AVM-ℓ1, resulting in its no-show in the ﬁgure. Interestingly, the
errors of RBP and Forgetron slightly increase throughout their online learning in these two
cases.

Second, Fig. 9 plots average error against computational cost, which shows similar learn-
ing behaviors as in the our ﬁrst observation. The computational cost progresses are simple
and more obvious to comprehend than the regression progresses. As illustrated in Fig. 8b,
all algorithms have nicely plausible execution time curves in which the time is accumulated
over the learning procedure.

33

RBP

Forgetron

Projectron

BOGD

FOGD

NOGD

AVM-eps

AVM-L1

AVM-L2

1

1

2

3

2
4

Number of samples

5
Number of samples
104

×

0.05

3

0

1

4

2

3
Number of samples

4

5
104

5
×

6
104

×

casp

slice

1. 0

1. 5

2. 0

2. 5

3. 0

3. 5

4. 0

4. 5

5. 0

Number of samples

105

×

2

3
Number of samples

4

5

6
106

×

30

0

1

year

airlines

(a) The average RMSE as a function of the number of samples seen by the models.

1

2

3

Number of samples

4

5
104

×

1

2

3

Number of samples

4

5
104

×

casp

slice

0.40

0.40

E
S
M
R
 
e
g
a
r
e
v
A

0.35
E
S
M
R
 
e
g
a
r
0.30
e
v
A

0.35

0.30

0.25

0.25
0

0

0.26

0.24

E
S
M
R
 
e
g
a
r
e
v
A

0.22

0.20

0.18

0.16

0.14

0.12

0. 5

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

1.0

0.5

0.0

0.5

0

3.0

2.5

2.0

1.5

1.0

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

0.5

0. 5

1. 0

1. 5

2. 0

2. 5

3. 0

3. 5

4. 0

4. 5

5. 0

Number of samples

105

×

2

3
Number of samples

4

5

6
106

×

1.5

0

1

year

airlines

(b) The average time costs (seconds in logarithm of 10) as a function of the number of samples seen by
the models.

Figure 8: Convergence evaluation of online regresion. (Best viewed in colors).

Le et al

0.30

0.25

0.20

0.15

0.10

E
S
M
R
 
e
g
a
r
e
v
A

55

50

45

40

35

E
S
M
R
 
e
g
a
r
e
v
A

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

1.0

0.5

0.0

0.5

0

4.0

3.5

3.0

2.5

2.0

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

34

Approximation Vector Machines

Table 5: Online regression results of 6 baselines and 3 versions of our AVMs. The notation
[δ; S; B; D] denotes the same meanings as those in Table 3. The regression loss is measured
using root mean squared error (RMSE) and the execution time is reported in second. The
best performance is in bold.
Dataset

|

|

S

B

D]
[δ
|
Algorithm
RBP
Forgetron
Projectron
BOGD
FOGD
NOGD
AVM-ε
AVM-ℓ1
AVM-ℓ2
Dataset

D]

|

|

S

B

[δ
|
Algorithm
RBP
Forgetron
Projectron
BOGD
FOGD
NOGD
AVM-ε
AVM-ℓ1
AVM-ℓ2

|

|

166

[4.0

casp
400
|
RMSE
0.0012
0.3195
±
0.0008
0.3174
±
0.0002
0.2688
±
0.0002
0.2858
±
0.0014
0.3775
±
0.0001
0.2512
±
0.0329
0.3165
±
0.0330
0.3166
±
0.0280
0.3274
±
year
400
|
RMSE
0.0002
0.1881
±
0.0004
0.1877
±
0.0003
0.1390
±
0.0000
0.2009
±
0.0002
0.1581
±
0.0005
0.1375
±
0.0002
0.1286
±
0.0003
0.1232
±
0.0001
0.2420
±

[60.0

67

|

|

2, 000]

|

27

[16.0

Time
7.15
10.14
8.48
6.20
5.83
6.99
3.53
3.44
3.31

slice
1, 000
|
RMSE
0.0006
0.1154
±
0.0004
0.1131
±
0.0002
0.0770
±
0.0001
0.1723
±
0.0009
0.1440
±
0.0002
0.0873
±
0.0137
0.2013
±
0.0138
0.2013
±
0.0002
0.2590
±
airlines
1, 000
|
RMSE
Time
0.0010
36.5068
605.42
±
0.0003
36.5065
904.09
±
0.0009
36.1365
605.19
±
0.0010
35.7346
596.10
±
0.0120
53.1638
76.70
±
0.0013
607.37 34.7421
±
0.0914
36.0901
48.01
±
0.0192
36.3632
47.29
±
46.63
0.0192
35.1128
±

[1.0

388

|

1, 600]

|

|

3, 000]

Time
810.14
1,069.15
814.37
816.16
20.65
812.69
7.07
7.13
6.88

2, 000]

Time
3,418.89
5,774.47
3,834.19
3,058.96
646.15
3,324.38
638.60
621.57
633.27

According to ﬁnal results summarized in Table 5, our proposed models enjoy a signiﬁcant
advantage in computational eﬃcacy whilst achieve better (for year dataset) or competitive
regression results with other methods. The AVM, again, secures the best performance in
terms of model sparsity. Among the baselines, the FOGD is the fastest, that is, its time costs
can be considered to compare with those of our methods, but its regression performances
are worse. The remaining algorithms usually obtain better results, but is traded oﬀ by
the sacriﬁce of scalability. This, once again, veriﬁes the eﬀectiveness and eﬃciency of our
proposed techniques. We believe that the AVM is a promising machine to perform online
regression task for large-scale datasets.

Finally, comparing the capability of three AVM’s variants, all models demonstrate sim-
ilar computational complexities wherein the AVM-ℓ2 is slightly faster due to its simpler
operator in computing the gradient as derived in Section 7. However, its regression errors
are higher than two other methods – AVM-ε and AVM-ℓ1.

35

RBP

Forgetron

Projectron

BOGD

FOGD

NOGD

AVM-eps

AVM-L1

AVM-L2

1
0.0
Average time cost (log10t)

0.5

2

1.0
Number of samples

0.05

3
0.5

0.0

0.5

4

1.0

1.5
Average time cost (log10t)

2.0

2.5

5
3.0
104

×

3.5

casp

slice

0.40

0.40

E
S
M
R
 
e
g
a
r
e
v
A

0.35
E
S
M
R
 
e
g
a
r
0.30
e
v
A

0.35

0.30

0.25

0.25

0

0.5

0.26

0.24

0.22

0.20

0.18

0.16

0.14

E
S
M
R
 
e
g
a
r
e
v
A

0.12

0.5

1.0

1.5

2.0

2.5

3.0

2.0

2.5

3.0

3.5

4.0

Average time cost (log10t)

year

Average time cost (log10t)

airlines

Figure 9: Average RMSE vs. time cost for online regression. The average time (seconds) is
shown in the logarithm with base 10. (Best viewed in colors).

10. Conclusion

In this paper, we have proposed Approximation Vector Machine (AVM) for large-scale
online learning. The AVM is theoretically proven to have bounded and sparse model size
while not hurting the predictive performance. We have validated our proposed method
on several benchmark datasets. The experimental results show that the proposed AVM
obtains a comparable predictive performance while simultaneously achieving an impressive
model size and a computational speed-up compared with those of the baselines. Our future
works are to apply AVM to the context of semi-supervised learning, anomaly detection, and
support vector clustering.

Acknowledgment

We gratefully thank the editor and the anonymous reviewers for their valuable comments
and thorough inspection of the article. This work was partially supported by the Australian
Research Council (ARC) under the Discovery Project DP160109394.

Le et al

0.30

0.25

0.20

0.15

0.10

E
S
M
R
 
e
g
a
r
e
v
A

55

50

45

40

35

E
S
M
R
 
e
g
a
r
e
v
A

30

1.5

36

Approximation Vector Machines

Appendix A. Proofs Regarding δ-Coverage

Proof of Theorem 4
x

Assume that

x

′

δ, then we have

−

2

≤

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
′
x

Φ (x)

Φ

−

(cid:13)
(cid:13)
(cid:13)

= K (x, x) + K

′

′

x

, x

2K

′

x, x

−

(cid:17)

(cid:16)

(cid:17)

= 2

1

k

−

(cid:18)

(cid:16)

(cid:17)(cid:13)
(cid:13)
(cid:13)
≤

2

1

k

δ2

−

(cid:16)
= δ2
Φ

′

x

x

−

2

(cid:13)
(cid:13)
(cid:13)

(cid:19)(cid:19)

(cid:18)(cid:13)
(cid:13)
(cid:13)

(cid:0)
Furthermore, we have

(cid:0)

(cid:1)(cid:1)

lim
0
δ
→

δΦ = 21/2 lim
0
→

δ

k

δ2

1

−

(cid:0)

(cid:0)

(cid:1)(cid:1)

1/2

= 21/2 (1

k (0))1/2 = 0

−

Finally, since Gaussian kernel is a special radial kernel with k (t) = exp (

γt), we

−

obtain the ﬁnal conclusion.

Proof of Theorem 19

Since the proof is similar for the hyperrectangle cell case, we present the proof for
=
. From the
z
we must be able to extract a
. From the construction of

the hypersphere case. Let us consider the open coverage
compactness of the data domain
, it apparent that from
ﬁnite subcoverage of size m, that is,
the coverage

U
U
m
i=1 ⊂ U

Um =

zi, δ
2

z, δ
2

(cid:1)(cid:9)

∈X

X

B

B

(cid:8)

(cid:0)

in Algorithm 3, we know that
(cid:8)

(cid:0)

P

(cid:1)(cid:9)

ci −
k

cjk

> δ/2 if i

= j

Hence, each open sphere in the ﬁnite coverage

It means that the cardinality of

Um is able to contain at most one core
must be less than or equal m, that is,

P

point of
m.

|P| ≤

.

P

37

Le et al

Appendix B. Proofs Regarding Convergence Analysis

= (Pi)i

Given a ﬁnite δ-coverage
I , when receiving an
P
∈
incoming instance (xt, yt) we approximate (xt, yt) by (cit, yt) with cit is a core point whose
Pit. We use a Bernoulli random variable Zt to control if the
cell contains xt, that is, xt ∈
approximation is performed or not, that is, Zt = 1 indicates the approximation is performed.
Let us deﬁne gt = λwt + l

(wt; xt, yt) = λwt + αtΦ (xt). We have the following

I with the core set
∈

= (ci)i

C

′

ht = gt + Zt∆t

where ∆t = αt (Φ (cit)

Φ (xt)).

The update rule becomes

−

where S = RD (i.e., the feature space) or

B
Lemma 21. The following statements hold

(cid:0)

wt+1 =

(wt −

ηtht)

YS

0, ymaxλ−

1/2

.

(cid:1)

i) There exist two positive constants P and M such that E

for all t.
ii) E

iii) E

iv) E

Proof

L =

A√P + B

for all t.

2

2

≤

′

l

(wt; xt, yt)
(cid:13)
(cid:13)
G =
(cid:13)

2

2

2

i

≤

(cid:21)
(cid:16)
λP + A√P + B

(cid:20)(cid:13)
(cid:13)
gtk
(cid:13)
k
h
htk
k
h
i) We prove by induction that E
2

√G + δΦ
(cid:16)

(cid:17)
for all t.
2

(cid:17)
A√P + B

H =

(cid:17)(cid:17)

≤

(cid:16)

(cid:16)

i

for all t.

(δΦ+1)A+√(δΦ+1)2A2+4Bλ(δΦ+1)
2λ

(cid:18)
using Minkowski inequality, we have

(cid:19)

2

wtk

k
h

≤

i

P 2 and E

α2
t

M

≤

(cid:2)

(cid:3)

2

wtk
k

h

i

≤

P 2 where P

=

for all t. Assume that the claim is holding for t,

E

r

k
h

2

wt+1k

i

E

≤ v
u
u
u
t
t
−
t

≤




1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

YS

E

2

wtk
k

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
+ ηt


E

(wt −

ηtht)



≤

r

E

k
h

wt −

ηthtk

2

i

r

p

h
E [
wtk
k
λt

δΦ

E

+

r

A√P + B

t

1

−
t

≤

E

2

wtk
k

A

+

i

i

r

r

h

h

+

+

(t

1) P
−
t

(t

1) P
−
t

≤

≤

A√P + B
λt
(δΦ + 1)

(cid:16)
λt

38

l′ (wt; xt, yt)
k
k

2

+ ηt

] + B

+

i
δΦ

E

r
α2
t

q

λt

(cid:2)

(cid:3)

E

2

∆tk
k

h

i

l′ (wt; xt, yt)
k

2
k

i

h

(cid:17)

λt

= P

Approximation Vector Machines

Note that we have used

P , and u = P 1/2 = (δΦ+1)A+√(δΦ+1)2A2+4Bλ(δΦ+1)

2

= α2

′

l

(cid:13)
(cid:13)
(cid:13)

(wt; xt, yt)
(cid:13)
(cid:13)
(cid:13)

2λ

t K (xt, xt) = α2

t , E [
wtk
k
is the solution of the quadratic equation

wtk
k

r

≤

E

≤

h

i

]

2

u2

−

(δΦ + 1) Au
λ

−

(δΦ + 1) B
λ

= 0

The proof of E

insensitive
losses. In these cases, we simply choose M = max (ymax, 1)2. We only need to consider the
ℓ2-loss case. In particular, we have

M is trivial for the case of Hinge, ℓ1, Logistic, ε

−

≤

(cid:3)

(cid:2)

α2
t

α2

t =

t Φ (xt)

yt

2

2

wT

t Φ (xt)

2

+ y2

max

2

wtk
k

k

≤

−

≤
(cid:17)
2 + y2
Φ (xt)
k

(cid:18)(cid:16)
max

wT
(cid:16)
2

(cid:16)

(cid:19)

2 + y2

max

(cid:17)
wtk

k

(cid:17)

2

≤

(cid:17)

(cid:16)

ii) We have the following

(cid:2)

(cid:3)

(cid:0)

(cid:1)

E

α2
t

2

P 2 + y2

max

= M

≤

E

l′ (wt; xt, yt)
k
k

2

r

h

E

A

wtk

k

≤ s

i

(cid:20)(cid:16)

2

1/2 + B

A

≤

E [
k

wtk

] + B

A√P + B

≤

(cid:21)

(cid:17)

p

Note that we have used the inequality E [
k

wtk
]

≤

iii) Using Minkowski inequality, we yield

E

r

k
h

2

wtk

≤

i

P .

2

E

gtk
iv) We have the following

r

r

≤

λ

k

h

i

h

E

2

wtk

k

i

r

k
h

+

E

l′ (wt; xt, yt)

λP + A√P + B

2
k

≤

i

E

r

k
h

2

htk

≤ r

i

E

2

gtk
k

+ δΦ

E

α2
t

h
√G + δΦ

i
E

≤

q
(cid:2)
2
l′ (wt; xt, yt)
k

(cid:3)

r

k
h

= √G + δΦ

A√P + B

i

(cid:16)

(cid:17)

Lemma 22. There exists a positive constant W such that E

Proof We ﬁrst remind the deﬁnitions of the relevant quantities

2

wt −

w∗k

k
h

≤

i

W for all t.

′

gt = λwt + l
ht = gt + Zt∆t where ∆t = αt (Φ (cit)

(wt; xt, yt) = λwt + αtΦ (xt)

Φ (xt))

−

39

Le et al

We now prove by induction in t. We derive as follows

wt+1 −

k

w∗

2 =

k

(wt −
w∗

ηtht)

w∗

−

+ η2

htk

t k

2

k

wt −
wt −

2
k

w∗

ηtht −
w∗, gt + Zt∆ti

≤ k

2ηt h

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
−
(cid:13)

where S = RD or

0, ymaxλ−

Taking conditional expectation w.r.t wt, we gain

B

(cid:0)

=

(cid:13)
YS
(cid:13)
(cid:13)
wt −
(cid:13)
k
(cid:13)
1/2
.

(cid:1)

w∗

2
k

E

wt+1 −
k

w∗

2
k

h

E

≤
+ η2
t

i

E

≤

k
h

wt −
k
h
E
htk
w∗

k
h
wt −

2

i
2
k

i
2ηt

−

wt −

′

w∗, f

(wt)

D
2ηtλ

wt −
k

w∗

E
2 + η2
t
k

−

i
Here we note that we have used

following derivation

wt −

D

′

w∗, f

(wt)

λ

≥

k

E

−
E

2ηt h
htk

k
h
wt −

wt −

2

−

i
w∗k

w∗, Zt∆ti
wt −
2ηt h

w∗, Zt∆ti

2. It comes from the

f (w∗)

f (wt)

−

≥

D

′

f

(wt) , w∗

wt

+

−

E

λ
2 k

wt −

w∗

2
k

(wt) , wt −

w∗

′

f

D

f (wt)

f (w∗) +

E

≥

≥

−
wt −

λ

k

w∗

2
k

Taking expectation again, we gain

λ
2 k

wt −

w∗

′

f

2
k
≥
thanks to

D

(w∗) , wt −
f

′

E
w∗
(w∗) , wt −

w∗

+ λ

w∗

2

k

k

wt −
0

≥

E

D

E

wt+1 −
k

w∗

2
k

≤

h

i
M 1/2δφ+(M δ2

2λ

t

2

E

wt −
−
t
k
φ+2H)1/2

h

2

Choosing W =

i
, we gain if E

w∗

2
k

+

H
λ2t2 +

2W 1/2M 1/2δΦ
λt

E

w∗k
wt+1 −
k
of the equation

h

2

i

≤

(cid:18)
W . The reason is that W =

(cid:19)

wt −
k
h
φ+2H)1/2
M 1/2δφ+(M δ2

w∗k

2

2

W then

≤

i
is the solution

2λ

(cid:18)

(cid:19)

W =

W +

t

2

−
t

H
λ2t

+

2W 1/2M 1/2δΦ
λt

or 2W

2W 1/2M 1/2δΦ
λ

H
λ2 = 0

−

−

. Hence, if E

wt −

k

w∗k

2

≤

W , we arrive at

h

i
wt+1 −

E

k
h

w∗

2
k

≤

i

t

2

−
t

W +

H
λ2t

+

2W 1/2M 1/2δΦ
λt

= W

40

Approximation Vector Machines

We now show the proof of Theorem 5.

Proof of Theorem 5

We ﬁrst remind the deﬁnitions of the relevant quantities

′

gt = λwt + l
ht = gt + Zt∆t where ∆t = αt (Φ (cit)

(wt; xt, yt) = λwt + αtΦ (xt)

Φ (xt))

−

We then derive as follows

wt+1 −

k

w∗

2 =

k

(wt −
w∗

(cid:13)
YS
(cid:13)
(cid:13)
wt −
(cid:13)
k
(cid:13)

=

ηtht)

w∗

−

+ η2

htk

t k

2

k

wt −
wt −

2
k

w∗

ηtht −
w∗, gt + Zt∆ti

≤ k

2ηt h

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
−
(cid:13)

where S = RD or

0, ymaxλ−

1/2

.

B

(cid:0)

(cid:1)

wt −
h

w∗, gti ≤

wt −
k

w∗k

2

wt+1 −

w∗k

2

+

− k
2ηt

ηt
2 k

2

htk

wt −

w∗, Zt∆ti

− h

Taking conditional expectation w.r.t wt, we obtain

′

w∗, f

(wt)

wt −

D

E

ηt
2

h

E

E

≤

+

wt −
k

w∗k

2

2

htk
k

h

− h

i

E

−
i
2ηt
wt −

wt+1 −
k

w∗k

2

h
w∗, E [Zt∆t]
i

i

f (wt)

f (w∗) +

−

λ
2 k

wt −

w∗

2
k

E

ηt
2

h

E

≤

+

wt −
k

w∗k

2

2

htk

k
h

− h

i

E

−
i
2ηt
wt −

w∗k

wt+1 −

k
h
w∗, E [Zt∆t]
i

2

i

Taking expectation again, we achieve

E [f (wt)

f (w∗)]

−

λ
2
ηt
2

(t

E

≤

+

1) E

−

2

htk
k

h

i

w∗

2
wt −
k
E [
wt −
h

k
h
−

tE

λ
2
−
h
i
w∗, Zt∆ti
]

wt+1 −
k

w∗

2

k

i

i) If Zt is independent with wt, we derive as

41

Le et al

E [f (wt)

f (w∗)]

−

≤
+

λ
2
ηt
2
λ
2
≤
+ E

wt −
k
h
E [
−
h
wt −
k
w∗

2

1) E

2

htk
k
i
1) E

−

(t

E

h

(t

−
Z 2

t k

h
wt −
1) E

h

h
(t

1/2 E

−
Z 2
t

λ
2
≤
+ E

k
i
wt −
k
wt −
k
h
(cid:3)
wt −
≤
−
k
+ P (Zt = 1)1/2 E

1) E

(cid:2)
(t

λ
2

h

tE

w∗

λ
2
wt+1 −
2
−
k
k
i
h
w∗) , ∆ti
Zt (wt −
]
λ
tE
w∗
2
−
∆tk
k
λ
2

wt+1 −
k
h
1/2

h
2
k

i
tE

2
k

w∗

1/2

E

i

2

−
1/2

i

w∗

i
2
k
2
k
wt −
k

w∗

i
w∗

−
2

k

E

2

1/2

wt+1 −
k
h
∆tk
k
h
tE

i
wt+1 −
k
h
E
∆tk
k

2

λ
2
1/2

i

h

i

h

w∗

2

k

i

i

i

w∗

2

k

+

E

ηt
2

2

htk
k

w∗

2

k

+

E

ηt
2

2

htk
k

h

h

+

H

ηt
2

w∗

2

k

1/2

i

i

i

(5)

Taking sum over 1, 2, . . . , T and using the inequalities in Lemmas 21 and 22, we yield

E [f (wt)]

T f (w∗)

P (Zt = 1)1/2 E

wt −
k

w∗

2

k

T

Xt=1

−

−

H
2λ

≤

H
2λ

≤

T

T

Xt=1
T

Xt=1
T

1
t

+

1
t

+

Xt=1

Xt=1

T E [f (wT )

f (w∗)]

P (Zt = 1)1/2 E

wt −
k

w∗

2
k

1/2

E

1/2

(6)

2

∆tk
k

i

h

i

1/2

E

1/2

2

∆tk

k
h

i

i

h

h

E [f (wT )

f (w∗)]

−

H (log (T ) + 1)
2λT

+

δΦM 1/2W 1/2
T

≤

P (Zt = 1)1/2

T

t=1
X

ii) If Zt is independent with wt and (xt, yt), we derive as

E [f (wt)

f (w∗)]

−

≤
+

λ
2
ηt
2
λ
2
ηt
2
λ
2

−

−

(t

H

(t

1) E
wt −
k
h
E [Zt h
wt −
1) E
wt −

2

w∗

−
k
i
w∗, ∆ti
]
w∗

2

tE

λ
2

h

wt+1 −
k

w∗

2
k

i

tE

wt+1 −
k

w∗

2
k

−

−

H

≤
+

k
h
E [Zt] E [
wt −
h
wt −
w∗
wt −
k
Taking sum over 1, 2, . . . , T and using the inequalities in Lemmas 21 and 22, we yield

≤
−
h
+ P (Zt = 1) E

wt+1 −
k
h
∆tk

i
2
k

1) E

ηt
2

k
h

2
k

w∗

w∗

tE

1/2

H

+

(t

E

k

i

i

i

h

h

i

2

2

(7)

λ
2
−
k
i
w∗, ∆ti
]
λ
2
−
1/2

k

42

Approximation Vector Machines

E [f (wt)]

T f (w∗)

+

P (Zt = 1) E

T

Xt=1

−

−

H
2λ

≤

H
2λ

≤

T

T

1
t

1
t

Xt=1
T

Xt=1
T

Xt=1

Xt=1

T E [f (wT )

f (w∗)]

+

P (Zt = 1) E

wt −
k

w∗

2
k

1/2

E

1/2

(8)

h

k
h

wt −

w∗

2

k

1/2

E

2

∆tk

k
h

2

∆tk

k

h

i

i

1/2

E [f (wT )

f (w∗)]

−

H (log (T ) + 1)
2λT

+

δΦM 1/2W 1/2
T

≤

P (Zt = 1)

i

i

T

Xt=1

iii) In general, we derive as

E [f (wt)

f (w∗)]

−

H (log (T ) + 1)
2λT

≤

+ δΦM 1/2W 1/2

≤
+

λ
2
ηt
2
λ
2
ηt
2

−

−

(t

H

(t

1) E
wt −
k
h
E [Zt h
wt −
1) E
wt −
w∗

w∗

2
−
k
i
w∗, ∆ti
]
w∗

2
k
i
1/2

λ
2

λ
2

tE

wt+1 −
k

w∗

2

k

h

i

≤

k
h
wt −
k
Taking sum over 1, 2, . . . , T and using the inequalities in Lemmas 21 and 22, we yield

−
H + E

2
k

1/2

+

−

E

k

h

i

i

h

i

tE

wt+1 −
k
h
2
∆tk
k

2

w∗

(9)

E [f (wt)]

T f (w∗)

H
2λ

≤

T

T

1
t

+

E

wt −
k

w∗

2
k

T

t=1
X

t=1
X
T

H
2λ

≤

1
t

+

h

t=1
X
T

E

t=1
X

t=1
X

h

wt −

k

w∗

2

k

2

∆tk
k

i

h

i

1/2

E

2

∆tk

k
h

1/2

(10)

i

1/2

E

i

1/2

T E [f (wT )

f (w∗)]

−

−

E [f (wT )

f (w∗)]

−

H (log (T ) + 1)
2λT

≤

+ δΦM 1/2W 1/2

Proof of Theorem 15

Let us denote the model size, i.e., the number of vectors in support set, after the iteration
t by St. We also deﬁne Nt by the binary random variable which speciﬁes whether the
incoming instance (xt, yt) locates in a new cell of the coverage, that is, Nt = 1 indicating
the current cell Pit is a new cell. We assume that Zt is independent with (xt, yt) and so does
with Nt. Since a new instance is added to the support set if either a new cell is discovered
or the old cell is found but approximation is not performed, we reach the following

St ≤

−

St

1 + Nt + (1

Zt) (1

Nt)

−

−

43

Le et al

Taking expectation, we obtain

E [St]

≤

≤

≤
E [St]

E [St
−
E [St
−
E [St
−
E [St
−

1] + E [Nt] + (1
1] + E [Nt] + (1
−
1] + E [Nt] + qt (1

−

E [Zt]) (1
pt) (1

−
E [Nt])

E [Nt])

−
E [Nt])

−

E [Nt] + qt (1
Summing over the above when t = 1, . . . , T , we have

1]

≤

−

E [Nt])

−

T

T

t=1
X

E [ST ]

E [Nt] +

qt (1

E [Nt]) =

qt +

ptE [Nt]

T

T

t=1
X
T

≤

≤

−

T

T

t=1
X
E [Nt]

≤

qt +

t=1
X
qt + E [MT ]

Xt=1
where we have denoted P (Zt = 1) = pt, P (Zt = 0) = qt, and MT =
number of cells discovered so far.

Xt=1

Xt=1

We consider some speciﬁc cases and investigate the model size E [ST ] in these cases.
i) pt = P (Zt = 1) = 1,

t, that is, we always do approximation. From Eq. (11), we

P

obtain

∀

(11)

T
t=1 Nt indicates the

ii) pt = P (Zt = 1) = max

0, 1

E [ST ]
β
t

,

−

≤

∀

E [MT ]

≤ |P|

t. It follows that

(cid:16)
qt = 1

(cid:17)
pt ≤

1

−

β
t

1

−

=

β
t

(cid:19)

−

(cid:18)

From Eq. (11), we gain

E [ST ]

β

+ E [MT ]

β

1 +

+ E [MT ]

T

1
t

t=1
X

(cid:18)
β (log T + 1) + E [MT ]

≤

≤

≤

T

1
t

dt

(cid:19)

1

Z

iii) pt = P (Zt = 1) = max

0, 1

,

t where 0 < ρ < 1. It follows that

β
tρ

−

(cid:16)
qt = 1

(cid:17)
pt ≤

−

β
tρ

1

−

=

β
tρ

(cid:19)

−

(cid:18)

From Eq. (11), we gain

E [ST ]

β

≤

1
tρ + E [MT ]

≤

T

Xt=1

β

1 +

t−

ρdt

+ E [MT ]

+ E [MT ]

T

1

Z

(cid:19)

ρ

βT 1
1

−
ρ

−

≤

iv) pt = P (Zt = 1) = max

0, 1

,

t where ρ > 1. It follows that

(cid:18)

β
tρ

−

(cid:16)
qt = 1

(cid:17)
pt ≤

−

β
tρ

1

−

=

β
tρ

(cid:19)

−

(cid:18)

44

∀

1

∀

1

Approximation Vector Machines

From Eq. (11), we gain

E [ST ]

β

≤

1
tρ + E [MT ]

≤

βζ (ρ) + E [MT ]

βζ (ρ) +

≤

|P|

T

t=1
X

where ζ (.) is ζ- Riemann function deﬁned by the integral ζ (s) = 1
Γ(s)

+
∞0

ts−1
es

1 dt.

−

We now show the proof of Theorem 8. To realize this proof, we use the famous inequality,
namely Hoeﬀding which for completeness we state below.

R

bi for each i

Theorem. (Hoeﬀding inequality) Let the independent variables X1, . . . , Xn where ai ≤
i=1 Xi and ∆i = bi −
Xi ≤
2ε2
i=1 ∆2
i
(cid:17)
2ε2
i=1 ∆2
i

ai. The following hold

∈
E [S] > ε)

[n]. Let S =

−
ii) P (
S
|

−
(cid:16)
2 exp

≤
> ε)

E [S]
|

i) P (S

exp

P
Pn

Pn

−

−

≤

n

.

Proof of Theorem 8

From Eqs. (6, 8, 10), we achieve

(cid:16)

(cid:17)

1
T

T

t=1
X

−

T

1
T

t=1
X

E [f (wt)]

f (w∗)

−

H (log (T ) + 1)
2λT

≤

+ dT

Let us denote X = f (wr)

f (w∗), where r is uniformly sampled from

1, 2, . . . , T
{

.

}

We have

Er [X] =

E [f (wt)]

f (w∗)

−

H (log (T ) + 1)
2λT

≤

+ dT

It follows that

E [X] = E

(xt,yt)T

t=1

[Er [X]]

H (log (T ) + 1)
2λT

≤

+ dT

Let us denote ∆T = max
T
t
≤

≤

1

−

(f (wt)

f (w∗)) which implies that 0 < f (wr)

f (w∗) <

−

∆T . Applying Hoeﬀding inequality for the random variable X, we gain

P (X

E [X] > ε)

exp

−

≤

2ε2
∆2

T (cid:19)

−

(cid:18)

P

X

H (log (T ) + 1)
2λT

−

−

dT > ε

exp

≤

(cid:19)

−

(cid:18)

H (log (T ) + 1)
2λT

≤

+ dT + ε

> 1

exp

(cid:19)

−

−

(cid:18)

T (cid:19)

2ε2
∆2

T (cid:19)
2ε2
∆2

(cid:18)

X

P

(cid:18)

2ε2
∆2
T

−

(cid:16)

(cid:17)

Choosing δ = exp

or ε = ∆T

1

2 log 1

δ , then with the probability at least 1

δ,

−

we have

f (wr)

f (w∗)

−

H (log (T ) + 1)
2λT

≤

+ dT + ∆T

log

1
2

r

1
δ

q

45

Le et al

Proof of Theorem 9
T = E
We denote W α
i) If Zt is independent with wt, taking sum in Eq. (5) when t = (1

. Our proof proceeds as follows.

α)T +1 −

w(1

w∗

−

2

i

h(cid:13)
(cid:13)

α) T + 1, . . . , T ,

−

we gain

T

Xt=(1
−

α)T +1

E [f (wt)]

αT f (w∗)

−

λ (1

α) T

W α

T +

H
2λ

1/2

E

2

∆tk
k

h

i

1/2

(12)

(cid:13)
(cid:13)

−
2

≤

+

T

Xt=1
−
2

P (Zt = 1)1/2 E

λ (1

α) T

W α

T +

≤

T

1
t

Xt=(1
−

α)T +1

wt −
k
H log (1/ (1

h

w∗

2
k

i
α))

−

2λ

T

+ δΦM 1/2W 1/2

P (Zt = 1)1/2

T
t=(1

α)T +1

−

α)T +1

Xt=(1
−
log (1/ (1

1
t ≤

P

λ (1

α)

−
2α

W α

T +

δΦM 1/2W 1/2
αT

≤

+

H log (1/ (1
2λαT

−

α))

α)).

−

T

Xt=(1
−

α)T +1

E [f (wα
T )

f (w∗)]

−

P (Zt = 1)1/2

where we have used the inequality

ii) If Zt is independent with wt and (xt, yt), taking sum in Eq. (7) when t = (1

α) T +

−

1, . . . , T , we gain

T

Xt=(1
−

α)T +1

E [f (wt)]

αT f (w∗)

−

λ (1

α) T

W α

T +

H
2λ

T

1
t

Xt=(1
−

α)T +1

−
2

T

≤

+

Xt=(1
−

α)T +1

P (Zt = 1) E

wt −
k

w∗

2
k

1/2

E

1/2

2

∆tk
k

h

i

h

i
(13)

λ (1

α) T

H log (1/ (1

α))

≤

−
2

W α

T +

−

2λ

+ δΦM 1/2W 1/2

P (Zt = 1)

T

Xt=(1
−

α)T +1

E [f (wα
T )

f (w∗)]

−

λ (1

α)

−
2α

W α

T +

δΦM 1/2W 1/2
αT

T

P (Zt = 1)

Xt=(1
−

α)T +1

≤

+

H log (1/ (1
2λαT

−

α))

46

Approximation Vector Machines

iii) In general, taking sum in Eq. (9) when t = (1

α) T + 1, . . . , T , we gain

−

T

Xt=(1
−

α)T +1

E [f (wt)]

αT f (w∗)

−

λ (1

α) T

W α

T +

H
2λ

T

1
t

α)T +1

Xt=(1
−
1/2

−
2

T

≤

+

λ (1

Xt=(1
α)T +1
−
α) T
−
2

≤

W α

T +

E

wt −
k

w∗

2
k

h

i
H log (1/ (1

1/2

i

E

2

∆tk
k

h
α))

−

2λ

+ δΦM 1/2W 1/2αT

(14)

E [f (wα
T )

f (w∗)]

−

λ (1

α)

−
2α

≤

W α

T + δΦM 1/2W 1/2 +

H log (1/ (1
2λαT

−

α))

The proof of this theorem is similar to that of Theorem 8 which relies on Hoeﬀding

Proof of Theorem 10

inequality.

From Eqs. (12, 13, 14), we achieve

T

1
αT

Xt=(1
−

α)T +1

E [f (wt)]

f (w∗)

−

H log (1/ (1
2λαT

−

≤

α))

+ dT

Let us denote X = f (wr)

f (w∗), where r is uniformly sampled from

(1
{

−

α) T + 1, 2, . . . , T

. We have

−

Er [X] =

}

1
αT

T

Xt=(1
−

α)T +1

It follows that

E [f (wt)]

f (w∗)

−

H log (1/ (1
2λαT

−

≤

α))

+ dT

E [X] = E

(xt,yt)T

t=1

[Er [X]]

H log (1/ (1
2λαT

−

≤

α))

+ dT

Let us denote ∆α

T =

f (w∗) < ∆α

−
T . Applying Hoeﬀding inequality for the random variable X, we gain

t
≤

(1

−

T

−

(f (wt)

f (w∗)) which implies that 0 < f (wr)

max
α)T +1
≤

P (X

E [X] > ε)

exp

−

≤

 −

2ε2
∆α
T

2

!

H log (1/ (1
2λαT

−

−

α))

−

dT > ε

P

X

(cid:18)

(cid:1)
exp

(cid:0)

≤

(cid:19)

2ε2
∆α
T

2

!

 −

(cid:0)

(cid:1)

47

Le et al

H log (1/ (1
2λαT

−

≤

α))

P

X

(cid:18)

+ dT + ε

> 1

exp

(cid:19)

−

 −

2ε2
∆α
T

2

!

Choosing δ = exp

2ε2
T )2
(∆α

(cid:19)

−

(cid:18)

1

δ, we have

−

q

or ε = ∆α
T

1

2 log 1

(cid:1)
δ , then with the probability at least

(cid:0)

f (wr)

f (w∗)

−

H log (1/ (1
2λαT

−

≤

α))

+ dT + ∆α
T

1
2

r

log

1
δ

Proof of Theorem 13

It is apparent that f (w) is L-strongly smooth w.r.t

f (wr)

f (w∗)

−

≤

′

f

(w∗)

T

(wr −

w∗) +

It follows that ∆α

1
2 LM α

T . Hence we gain the conclusion.

T ≤

k2. Therefore, we have
.
k
L
2 k

wr −

w∗

w∗

≤

k

2

2
k

L
2 k

wr −

48

Approximation Vector Machines

Appendix C. Proofs of Bound for L2 Loss

We now consider the upper bound of
in the case that ℓ2 loss is being used for regression
problem. Concretely, we have the following theorem whose proof is similar to that of
Theorem 1 in (Shalev-Shwartz et al., 2007, 2011).

w∗k
k

Theorem 23. If w∗ = argminw
ymaxλ−

1/2.

(cid:16)

w

λ
2 k

2 + 1
N
k

N
i=1

yi −

wTΦ (xi)

2

then

w∗k ≤

k

P

(cid:0)

(cid:17)

(cid:1)

Proof Let us consider the equivalent constrains optimization problem

λ
2 k

min
w,ξ  
s.t.: ξi = yi −

w

k
wT

2 +

1
N

Φ (xi) ,

N

Xi=1
i
∀

ξ2
i

!

The Lagrange function is of the following form

(w, ξ, α) =

w2

+

L

λ
2

N

N

ξ2
i +

1
N

Xi=1

Xi=1

(cid:16)

αi

yi −

wT

Φ (xi)

ξi

−

(cid:17)

Setting the derivatives to 0, we gain

= λw

w

∇

L

αiΦ (xi) = 0

w = λ−

1

αiΦ (xi)

→

ξi −
Substituting the above to the Lagrange function, we gain the dual form

αi = 0

∇ξiL

ξi =

→

=

N

Xi=1

N αi
2

(α) =

W

λ
2 k

w

2 +
k

−

yiαi −

N
4

N

Xi=1

N

α2
i

Xi=1
N

2

αiΦ (xi)

+

yiαi −

N
4

N

α2
i

(cid:13)
(cid:13)
(cid:13)
Let us denote (w∗, ξ∗) and α∗ be the primal and dual solutions, respectively. Since the
(cid:13)
(cid:13)

Xi=1

Xi=1

Xi=1

strong duality holds, we have

λ
2 k

w∗

2 +

k

1
N

2
i =
ξ∗

λ
2 k

w∗

2 +
k

−

yiα∗i −

N
4

N

Xi=1

2
α∗
i

N

Xi=1

λ

w∗
k

2 =
k

N

yiα∗i −

N
4

2
α∗
i −

yiα∗i −

≤

Xi=1 (cid:18)

≤

(cid:19)

Xi=1

Xi=1
N
4

2
α∗
i

49

1
N

N

N

2
ξ∗
i

Xi=1
y2
i
N ≤

y2
max

(cid:13)
(cid:13)

(cid:13)
(cid:13)

N

−

Xi=1

2
N

=

−

1
2λ (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

N

Xi=1

N

Xi=1
N

Le et al

We note that we have used g (α∗i ) = yiα∗i −

conclusion.

N
2
4 α∗
i ≤

g

2yi
N

(cid:16)

(cid:17)

= y2

i

N . Hence, we gain the

Lemma 24. Assume that ℓ2 loss is using, the following statement holds

wT +1k ≤
k

1

λ−

ymax +

 

1
T

T

t=1
X

wtk!

k

where ymax = max
∈Y

y

y
|

.
|

Proof We have the following

wt+1 =

S

S

(Q
Q

(cid:0)
(cid:0)

1

t
−
t
−

t wt −
t wt −

1

ηtαtΦ (xt)
ηtαtΦ (cit)
(cid:1)
(cid:1)

if Zt = 0
otherwise

It follows that

wt+1k ≤
k

t

1

−
t

wtk
k

+

1
λt |

αt|

since

Φ (xt)
k

k

=

Φ (cit )
k
k

= 1

It happens that l

(wt; xt, yt) = αtΦ (xt). Hence, we gain

′

=

αt|
|
It implies that

yt −
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(t

wtk
wt+1k ≤
k
k
Taking sum when t = 1, 2, . . . , T , we achieve

1)

−

t

+ λ−

1 (ymax +

wtk
k

)

wT

t Φ (xt)

ymax +

wtk k

k

Φ (xt)

k ≤

ymax +

wtk
k

≤

T

wT +1k ≤

k

1

λ−

T ymax +

 

wtk!

k

wT +1k ≤
k

1

λ−

ymax +

 

1
T

wtk!

k

T

t=1
X

T

t=1
X

(15)

Theorem 25. If λ > 1 then

wT +1k ≤
k
Proof First we consider the sequence
and s1 = 0. It is easy to ﬁnd the formula of this sequence as

−

1

{

(cid:1)

(cid:0)

−

< ymax
λ
−

ymax
1
λ

1
λT

1 for all T .

sT }T which is identiﬁed as sT +1 = λ−

1 (ymax + sT )

sT +1 −

ymax
1
λ

−

= λ−

1

sT −

(cid:18)

ymax
1
λ

−

(cid:19)

= . . . = λ−

T

s1 −

(cid:18)

ymax
1
λ

−

(cid:19)

= −

λ−
λ

T ymax
1

−

50

Approximation Vector Machines

We prove by induction by T that

sT for all T . It is obvious that

= s1 = 0.

Assume that

wtk ≤
k

st for t

≤

T , we verify it for T + 1. Indeed, we have

w1k
k

sT +1 =

ymax
λ
1
−
wT k ≤

k

1

−

(cid:18)

1
λT

(cid:19)

wT +1k ≤
k

λ−

λ−

≤

T

1

ymax +

1
T
 
1 (ymax + sT ) = sT +1

Xt=1

k

wtk! ≤

1

λ−

ymax +

 

1
T

T

Xt=1

st

!

In addition, from Eq. (15) in case that λ
have the following theorem.

≤

1 we cannot bound

. Concretely, we

wT +1k

k

Theorem 26. If
zT }T is a sequence such that zT +1 = λ−
{
1 this sequence is not upper-bounded.
then in case that λ
≤

(cid:16)

1

ymax + 1
T

T
t=1 zt

with z1 = 0

P

(cid:17)

Let us denote st = y−

1
maxzt.

It is obvious that s1 = 0 and sT +1 =

. We now prove by induction by T that

Proof
1
λ−

(cid:16)

1 + 1
T

T
t=1 st

P

(cid:17)

With T = 2, we have s2 = λ−

1. Assume that this holds for all 2

t

T , we verify

≤

≤

it for T + 1.

for all T

2

≥

1
t

T

1

−

t=1
X

sT ≥

1

≥

sT +1 = λ−

1

1 +

T

1
T

T

t
−

1 +

st

! ≥

T

1

−

1 +

t=1
X
1
1
n ≥

T

st

1
T

T

t=1
X
t

−
T t

 

1
T

T

1 +

1 +

≥

≥

t=1
n=1
X
X
1
t −

1
T

Xt=1 (cid:18)

=

(cid:19)

T

t=1
X
1
t

Xt=1
T
t=1

P

The ﬁnal conclusion is obvious from the fact

1
t > log (T + 1) and hence can

exceed any positive number.

51

Le et al

References

M. Badoiu and K. L. Clarkson. Optimal core-sets for balls.

In In Proc. of DIMACS

Workshop on Computational Geometry, 2002.

G. Cavallanti, N. Cesa-Bianchi, and C. Gentile. Tracking the best hyperplane with a simple

budget perceptron. Machine Learning, 69(2-3):143–167, 2007.

C.-C. Chang and C.-J. Lin. Libsvm: A library for support vector machines. ACM Trans.

Intell. Syst. Technol., 2(3):27:1–27:27, May 2011. ISSN 2157-6904.

C. Cortes and V. Vapnik. Support-vector networks. Machine learning, 20(3):273–297, 1995.

K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based

vector machines. J. Mach. Learn. Res., 2:265–292, March 2002. ISSN 1532-4435.

K. Crammer, J. Kandola, and Y. Singer. Online classiﬁcation on a budget. In Advances in

Neural Information Processing Systems 16. MIT Press, 2004.

K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. Online passive-

aggressive algorithms. J. Mach. Learn. Res., 7:551–585, 2006.

F. Cucker and S. Smale. On the mathematical foundations of learning. Bulletin of the

American Mathematical Society, 39:1–49, 2002.

O. Dekel, S. Shalev-Shwartz, and Y. Singer. The forgetron: A kernel-based perceptron on
a ﬁxed budget. In Advances in Neural Information Processing Systems, pages 259–266,
2005.

M. Dredze, K. Crammer, and F. Pereira. Conﬁdence-weighted linear classiﬁcation.

In
Proceedings of the 25th International Conference on Machine Learning, ICML ’08, pages
264–271, New York, NY, USA, 2008. ACM.

Y. Freund and R. E. Schapire. Large margin classiﬁcation using the perceptron algorithm.

Mach. Learn., 37(3):277–296, December 1999.

J. Hensman, N. Fusi, and N. D Lawrence. Gaussian processes for big data. In Uncertainty

in Artiﬁcial Intelligence, page 282. Citeseer, 2013.

T. Joachims. Advances in kernel methods. chapter Making Large-scale Support Vector
Machine Learning Practical, pages 169–184. MIT Press, Cambridge, MA, USA, 1999.
ISBN 0-262-19416-3.

J. Kivinen, A. J. Smola, and R. C. Williamson. Online Learning with Kernels.

IEEE

Transactions on Signal Processing, 52:2165–2176, August 2004.

T. Le, P. Duong, M. Dinh, D. T Nguyen, V. Nguyen, and D. Phung. Budgeted semi-
supervised support vector machine. In The 32th Conference on Uncertainty in Artiﬁcial
Intelligence, June 2016a.

52

Approximation Vector Machines

T. Le, T. D. Nguyen, V. Nguyen, and D. Phung. Dual space gradient descent for on-
line learning. In Advances in Neural Information Processing (NIPS), pages 4583–4591,
December 2016b.

T. Le, V. Nguyen, T. D. Nguyen, and Dinh Phung. Nonparametric budgeted stochastic
In The 19th International Conference on Artiﬁcial Intelligence and

gradient descent.
Statistics, pages 654–572, May 2016c.

J. Lu, S. C.H. Hoi, J. Wang, P. Zhao, and Z.-Y. Liu. Large scale online kernel learning. J.

Mach. Learn. Res., 2015.

L. Ming, W. Shifeng, and Z. Changshui. On the sample complexity of random fourier
features for online learning: How many random fourier features do we need? ACM
Trans. Knowl. Discov. Data, 8(3):13:1–13:19, June 2014. ISSN 1556-4681.

F. Orabona, J. Keshet, and B. Caputo. Bounded kernel-based online learning. J. Mach.

Learn. Res., 10:2643–2666, December 2009. ISSN 1532-4435.

A. Rahimi and B. Recht. Random features for large-scale kernel machines. In In Neural

Infomration Processing Systems, 2007.

A. Rakhlin, O. Shamir, and K. Sridharan. Making gradient descent optimal for strongly con-
vex stochastic optimization. In International Conference on Machine Learning (ICML-
12), pages 449–456, 2012.

C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning (Adaptive

Computation and Machine Learning). The MIT Press, 2005. ISBN 026218253X.

F. Rosenblatt. The perceptron: A probabilistic model for information storage and organi-

zation in the brain. Psychological Review, 65(6):386–408, 1958.

S. Shalev-Shwartz and N. Srebro. Svm optimization:

inverse dependence on training set
In Proceedings of the 25th international conference on Machine learning, pages

size.
928–935. ACM, 2008.

S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized

loss. Journal of Machine Learning Research, 14(1):567–599, 2013.

S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver
for svm. In Proceedings of the 24th International Conference on Machine Learning, ICML
’07, pages 807–814, New York, NY, USA, 2007. ACM. ISBN 978-1-59593-793-3.

S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: Primal estimated sub-

gradient solver for svm. Mathematical programming, 127(1):3–30, 2011.

I. Steinwart. Sparseness of support vector machines. J. Mach. Learn. Res., 4:1071–1105,

December 2003. ISSN 1532-4435.

I. W. Tsang, J. T. Kwok, P. Cheung, and N. Cristianini. Core vector machines: Fast svm
training on very large data sets. Journal of Machine Learning Research, 6:363–392, 2005.

53

Le et al

I. W. Tsang, A. Kocsor, and J. T. Kwok. Simpler core vector machines with enclosing
balls. In Proceedings of the 24th International Conference on Machine Learning, ICML
’07, pages 911–918, 2007.

Z. Wang and S. Vucetic. Twin vector machines for online learning on a budget. In Proceed-

ings of the SIAM International Conference on Data Mining, pages 906–917, 2009.

Z. Wang and S. Vucetic. Online passive-aggressive algorithms on a budget. In AISTATS,

volume 9, pages 908–915, 2010.

Z. Wang, K. Crammer, and S. Vucetic. Breaking the curse of kernelization: Budgeted
stochastic gradient descent for large-scale svm training. J. Mach. Learn. Res., 13(1):
3103–3131, 2012.

K. Zhang, L. Lan, Z. Wang, and F. Moerchen. Scaling up kernel svm on limited resources:
A low-rank linearization approach. In International Conference on Artiﬁcial Intelligence
and Statistics, pages 1425–1434, 2012.

P. Zhao, J. Wang, P. Wu, R. Jin, and S. C. H. Hoi. Fast bounded online gradient descent

algorithms for scalable kernel-based online learning. CoRR, 2012.

M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In
Machine Learning, Proceedings of the Twentieth International Conference (ICML 2003,
pages 928–936, 2003.

54

7
1
0
2
 
y
a
M
 
8
2
 
 
]

G
L
.
s
c
[
 
 
4
v
8
1
5
6
0
.
4
0
6
1
:
v
i
X
r
a

Journal of Machine Learning Research x (2016) y-z

Submitted a/b; Published c/d

Approximation Vector Machines
for Large-scale Online Learning

Trung Le∗
Centre for Pattern Recognition and Data Analytics, Australia

trung.l@deakin.edu.au

Tu Dinh Nguyen
Centre for Pattern Recognition and Data Analytics, Australia

tu.nguyen@deakin.edu.au

Vu Nguyen
Centre for Pattern Recognition and Data Analytics, Australia

v.nguyen@deakin.edu.au

Dinh Phung
Centre for Pattern Recognition and Data Analytics, Australia

dinh.phung@deakin.edu.au

Editor: Koby Crammer

Abstract

One of the most challenging problems in kernel online learning is to bound the model size
and to promote model sparsity. Sparse models not only improve computation and memory
usage, but also enhance the generalization capacity – a principle that concurs with the law
of parsimony. However, inappropriate sparsity modeling may also signiﬁcantly degrade the
performance. In this paper, we propose Approximation Vector Machine (AVM), a model
that can simultaneously encourage sparsity and safeguard its risk in compromising the per-
formance. In an online setting context, when an incoming instance arrives, we approximate
this instance by one of its neighbors whose distance to it is less than a predeﬁned threshold.
Our key intuition is that since the newly seen instance is expressed by its nearby neigh-
bor the optimal performance can be analytically formulated and maintained. We develop
theoretical foundations to support this intuition and further establish an analysis for the
common loss functions including Hinge, smooth Hinge, and Logistic (i.e., for the classiﬁ-
cation task) and ℓ1, ℓ2, and ε-insensitive (i.e., for the regression task) to characterize the
gap between the approximation and optimal solutions. This gap crucially depends on two
key factors including the frequency of approximation (i.e., how frequent the approximation
operation takes place) and the predeﬁned threshold. We conducted extensive experiments
for classiﬁcation and regression tasks in batch and online modes using several benchmark
datasets. The quantitative results show that our proposed AVM obtained comparable pre-
dictive performances with current state-of-the-art methods while simultaneously achieving
signiﬁcant computational speed-up due to the ability of the proposed AVM in maintaining
the model size.

Keywords: kernel, online learning, large-scale machine learning, sparsity, big data, core
set, stochastic gradient descent, convergence analysis

∗. Part of this work was performed while the author was aﬃliated with the HCM University of Education.

2016 Trung Le, Tu Dinh Nguyen, Vu Nguyen and Dinh Phung.

c
(cid:13)

Le et al

1. Introduction

In modern machine learning systems, data usually arrive continuously in stream. To enable
eﬃcient computation and to eﬀectively handle memory resource, the system should be able
to adapt according to incoming data. Online learning represents a family of eﬃcient and
scalable learning algorithms for building a predictive model incrementally from a sequence
In contrast to the conventional
of data examples (Rosenblatt, 1958; Zinkevich, 2003).
learning algorithms (Joachims, 1999; Chang and Lin, 2011), which usually require a costly
procedure to retrain the entire dataset when a new instance arrives, online learning aims to
utilize the new incoming instances to improve the model given the knowledge of the correct
answers to previous processed data (and possibly additional available information), making
them suitable for large-scale online applications wherein data usually arrive sequentially
and evolve rapidly.

The seminal

line of work in online learning, referred to as linear online learning
(Rosenblatt, 1958; Crammer et al., 2006; Dredze et al., 2008), aims at learning a linear pre-
dictor in the input space. The crucial limitation of this approach lies in its over-simpliﬁed
linear modeling choice and consequently may fail to capture non-linearity commonly seen
in many real-world applications. This motivated the works in kernel-based online learning
(Freund and Schapire, 1999; Kivinen et al., 2004) in which a linear model in the feature
space corresponding with a nonlinear model in the input space, hence allows one to cope
with a variety of data distributions.

One common issue with kernel-based online learning approach, also known as the curse
of kernelization, is that the model size (i.e., the number of vectors with non-zero coeﬃcients)
may grow linearly with the data size accumulated over time, hence causing computational
problem and potential memory overﬂow (Steinwart, 2003; Wang et al., 2012). Therefore
in practice, one might prefer kernel-based online learning methods with guaranty on a
limited and bounded model size.
In addition, enhancing model sparsity is also of great
interest to practitioners since this allows the generalization capacity to be improved; and
in many cases leading to a faster computation. However, encouraging sparsity needs to be
done with care since an inappropriate sparsity-encouraging mechanism may compromise the
performance. To address the curse of kernelization, budgeted approaches (Crammer et al.,
2004; Dekel et al., 2005; Cavallanti et al., 2007; Wang and Vucetic, 2010; Wang et al., 2012;
Le et al., 2016a,c) limits the model size to a predeﬁned budget B. Speciﬁcally, when the
current model size exceeds this budget, a budget maintenance strategy (e.g., removal, pro-
jection, or merging) is triggered to recover the model size back to the budget B. In these
approaches, determining a suitable value for the predeﬁned budget in a principled way is
important, but challenging, since setting a small budget makes the learning faster but may
suﬀer from underﬁtting, whereas a large budget makes the model ﬁt better to data but may
dramatically slow down the training process. An alternative way to address the curse of
kernelization is to use random features (Rahimi and Recht, 2007) to approximate a kernel
function (Ming et al., 2014; Lu et al., 2015; Le et al., 2016b). For example, Lu et al. (2015)
proposed to transform data from the input space to the random-feature space, and then
performed SGD in the feature space. However, in order for this approach to achieve good
kernel approximation, excessive number of random features is required which could lead to
a serious computational issue. To reduce the impact number of random features, (Le et al.,

2

Approximation Vector Machines

2016b) proposed to distribute the model in dual space including the original feature space
and the random feature space that approximates the ﬁrst space.

Figure 1: An illustration of the hypersphere coverage for 1, 000 data samples which locate in
3D space. We cover this dataset using hyperspheres with the diameter δ = 7.0, resulting in
20 hypersphere cells as shown in the ﬁgure (cf. Sections (6.3,9)). All data samples in a same
cell are approximated by a core point in this cell. The model size is therefore signiﬁcantly
reduced from 1,000 to 20.1

In this paper, we propose Approximation Vector Machine (AVM) to simultaneously
encourage model sparsity2 while preserving the model performance. Our model size is
theoretically proven to be bounded regardless of the data distribution and data arrival
order. To promote sparsity, we introduce the notion of δ-coverage which partitions the data
space into overlapped cells whose diameters are deﬁned by δ (cf. Figure 1). This coverage
can be constructed in advance or on the ﬂy. Our experiment on the real datasets shows
that the coverage can impressively boost sparsity; for example with dataset KDDCup99 of
4, 408, 589 instances, our model size is 115 with δ = 3 (i.e., only 115 cells are required); with
dataset airlines of 5, 336, 471 instances, our model size is 388 with δ = 1.

In an online setting context, when an incoming instance arrives, it can be approximated
with the corresponding core point in the cell that contains it. Our intuitive reason is that
when an instance is approximated by an its nearby core point, the performance would be
largely preserved. We further developed rigorous theory to support this intuitive reason.

1. In fact, we used a subset of the dataset a9a which has 123 features. We then project all data points onto
3D using t-SNE. We note that the t-SNE does not do clustering, it only reduces the dimensionality into
3D for visualization while trying to preserve the local properties of the data.

2. Model sparsity can be computed as the ratio of the model size and the number of vectors received so far.

3

Le et al

In particular, our convergence analysis (covers six popular loss functions, namely Hinge,
smooth Hinge, and Logistic for classiﬁcation task and ℓ2, ℓ1, and ε-insensitive for regres-
sion task) explicitly characterizes the gap between the approximate and optimal solutions.
The analysis shows that this gap crucially depends on two key factors including the cell
diameter δ and the approximation process. In addition, the cell parameter δ can be used
to eﬃciently control the trade-oﬀ between sparsity level and the model performance. We
conducted extensive experiments to validate the proposed method on a variety of learning
tasks, including classiﬁcation in batch mode, classiﬁcation and regression in online mode on
several benchmark large-scale datasets. The experimental results demonstrate that our pro-
posed method maintains a comparable predictive performance while simultaneously achiev-
ing an order of magnitude speed-up in computation comparing with the baselines due to
its capacity in maintaining model size. We would like to emphasize at the outset that un-
like budgeted algorithms (e.g., (Crammer et al., 2004; Dekel et al., 2005; Cavallanti et al.,
2007; Wang and Vucetic, 2010; Wang et al., 2012; Le et al., 2016a,c)), our proposed method
is nonparametric in the sense that the number of core sets grow with data on demand, hence
care should be exercised in practical implementation.
The rest of this paper is organized as follows.

In Section 2, we review works mostly
In Section 3, we present the primal and dual forms of Support Vector
related to ours.
Machine (SVM) as they are important background for our work. Section 4 formulates the
proposed problem. In Section 5, we discuss the standard SGD for kernel online learning
with an emphasis on the curse of kernelization. Section 6 presents our proposed AVM with
full technical details. Section 7 devotes to study the suitability of loss functions followed by
Section 8 where we extend the framework to multi-class setting. Finally, in Section 9, we
conduct extensive experiments on several benchmark datasets and then discuss experimental
results as well as their implications. In addition, all supporting proof is provided in the
appendix sections.

2. Related Work

One common goal of online kernel learning is to bound the model size and to encourage
sparsity. Generally, research in this direction can be broadly reviewed into the following
themes.

Budgeted Online Learning. This approach limits the model size to a predeﬁned budget
B. When the model size exceeds the budget, a budget maintenance strategy is triggered to
decrement the model size by one. Three popular budget maintenance strategies are removal,
projection, and merging. In the removal strategy, the most redundant support vector is sim-
ply eliminated. In the projection strategy, the information of the most redundant support
vector is conserved through its projection onto the linear span of the remaining support
vectors. The merging strategy ﬁrst selects two vectors, and then merges them into one
before discarding them. Forgetron (Dekel et al., 2005) is the ﬁrst budgeted online learning
method that employs the removal strategy for the budget maintenance. At each iteration,
if the classiﬁer makes a mistake, it conducts a three-step update: (i) running the standard
Perceptron (Rosenblatt, 1958) update; (ii) shrinking the coeﬃcients of support vectors with
a scaling factor; and (iii) removing the support vector with the smallest coeﬃcient. Ran-
domized Budget Perceptron (RBP) (Cavallanti et al., 2007) randomly removes a support

4

Approximation Vector Machines

vector when the model size overﬂows the budget. Budget Perceptron (Crammer et al.,
2004) and Budgeted Passive Aggressive (BPA-S)(Wang and Vucetic, 2010) attempt to dis-
card the most redundant support vector (SV). Orabona et al. (2009) used the projection
to automatically discover the model size. The new vector is added to the support set if its
projection onto the linear span of others in the feature space exceeds a predeﬁned thresh-
old, or otherwise its information is kept through the projection. Other works involving
the projection strategy include Budgeted Passive Aggressive Nearest Neighbor (BPA-NN)
(Wang and Vucetic, 2010; Wang et al., 2012). The merging strategy was used in some works
(Wang and Vucetic, 2009; Wang et al., 2012).

Random Features. The idea of random features was proposed in (Rahimi and Recht,
2007). Its aim is to approximate a shift-invariant kernel using the harmonic functions. In
the context of online kernel learning, the problem of model size vanishes since we can store
the model directly in the random features. However, the arising question is to determine the
appropriate number of random features D to suﬃciently approximate the real kernel while
keeping this dimension as small as possible for an eﬃcient computation. Ming et al. (2014)
investigated the number of random features in the online kernel learning context. Recently,
Lu et al. (2015) proposed to run stochastic gradient descent (SGD) in the random feature
space rather than that in the real feature space. The theory accompanied with this work
shows that with a high conﬁdence level, SGD in the random feature space can suﬃciently
approximate that in the real kernel space. Nonetheless, in order to achieve good kernel
approximation in this approach, excessive number of random features is required, possibly
leading to a serious computational issue. To reduce the impact of the number of random
features to learning performance, (Le et al., 2016b) proposed to store core vectors in the
original feature space, whilst storing remaining vectors in the random feature space that
suﬃciently approximates the ﬁrst space.

Core Set. This approach utilizes a core set to represent the model. This core set can
be constructed on the ﬂy or in advance. Notable works consist of the Core Vector Machine
(CVM) (Tsang et al., 2005) and its simpliﬁed version, the Ball Vector Machine (BVM)
(Tsang et al., 2007). The CVM was based on the achievement in computational geometry
(Badoiu and Clarkson, 2002) to reformulate a variation of ℓ2-SVM as a problem of ﬁnding
minimal enclosing ball (MEB) and the core set includes the points lying furthest away the
current centre of the current MEB. Our work can be categorized into this line of thinking.
However, our work is completely diﬀerent to (Tsang et al., 2005, 2007) in the mechanism
to determine the core set and update the model. In addition, the works of (Tsang et al.,
2005, 2007) are not applicable for the online learning.

3. Primal and Dual Forms of Support Vector Machine

Support Vector Machine (SVM) (Cortes and Vapnik, 1995) represents one of the state-of-
the-art methods for classiﬁcation. Given a training set
, the
data instances are mapped to a feature space using the transformation Φ (.), and then SVM
aims to learn an optimal hyperplane in the feature space such that the margin, the distance
from the closest data instance to the hyperplane, is maximized. The optimization problem

(x1, y1) , . . . , (xN , yN )
}
{

=

D

5

Le et al

of SVM can be formulated as follows

λ
2 k

w

2 +
k

1
N

min
w,b  
wT

(cid:17)
0, i = 1, ..., N

(cid:16)
ξi ≥

N

ξi

!

Xi=1
1

≥

−

s.t. : yi

Φ (xi) + b

ξi, i = 1, ..., N

(1)

where λ > 0 is the regularization parameter, Φ (.) is the transformation from the input
space to the feature space, and ξ = [ξi]N

i=1 is the vector of slack variables.

Using Karush-Kuhn-Tucker theorem, the above optimization problem is transformed to

the dual form as follows

1
min
2
α
s.t. : yTα = 0

(cid:18)

αT

Qα

eTα

−

(cid:19)

0

αi ≤

≤

1
λN

, i = 1, ..., N

where Q = [yiyjK (xi, xj)]N
function, e = [1]N

×

1 is the vector of all 1, and y = [yi]

T
i=1,...,N.

T
i,j=1 is the Gram matrix, K (x, x′) = Φ (x)

Φ (x′) is a kernel

The dual optimization problem can be solved using the solvers (Joachims, 1999;
Chang and Lin, 2011). However, the computational complexity of the solvers is over-
quadratic (Shalev-Shwartz and Srebro, 2008) and the dual form does not appeal to the
online learning setting. To scale up SVM and make it appealing to the online learning, we
rewrite the constrained optimization problem in Eq. (1) in the primal form as follows

λ
2 k

w

2 +

k

1
N

minw

 

l (w; xi, yi)

!

N

Xi=1

(2)

where l (w; x, y) = max

0, 1

ywTΦ (x)

3 is Hinge loss.

−

(cid:0)

In our current interest, the advantages of formulating the optimization problem of SVM
in the primal form as in Eq. (2) are at least two-fold. First, it encourages the application of
SGD-based method to propose a solution for the online learning context. Second, it allows
us to extend Hinge loss to any appropriate loss functions (cf. Section 7) to enrich a wider
class of problems that can be addressed.

(cid:1)

3. We can eliminate the bias b by simply adjusting the kernel.

6

Approximation Vector Machines

4. Problem Setting

We consider two following optimization problems for batch and online settings respectively
in Eqs. (3) and (4)

minw f (w) , λ
2 k
, λ

w

2 + E
k

w

2 +

2 k

k

1
N

∼

N

Xi=1

(x,y)

PN [l (w; x, y)]

l (w; xi, yi)

(3)

(4)

minw f (w) , λ

w

2 + E

(x,y)

PX ,Y [l (w; x, y)]

∼

2 k

X
=

,
X
and the label domain

k
where l (w; x, y) is a convex loss function, P
is the joint distribution of (x, y) over
X × Y
Y
, and PN speciﬁes the empirical distribution
with the data domain
Y
. Furthermore, we assume that the convex
(x1, y1) , . . . , (xN , yN )
over the training set
}
{
loss function l (w; x, y) satisﬁes the following property: there exists two positive numbers
w, x, y. As demonstrated in Section
A and B such that
7, this condition is valid for all common loss functions. Hereafter, for given any function
(w0) to denote the gradient (or any sub-gradient) of g (.) w.r.t
g(w), we use the notation g
w evaluated at w0.

1/2 + B,

(w; x, y)

w
k

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

≤

D

A

∀

k

l

′

′

′

It is clear that given a ﬁxed w,

there exists a random variable g such that

E [g
P
,

(w).

In fact, we can specify g = λw + l

w] = f
∼
or PN . We assume that a positive semi-deﬁnite (p.s.d.) and isotropic (iso.) kernel
R

(w; xt, yt) where (xt, yt)

(Rasmussen and Williams, 2005) is used, i.e., K

, where k :

x, x

= k

|
Y

x

x

X

2

′

′

′

is an appropriate function. Let Φ (.) be the feature map corresponding the kernel (i.e.,
). To simplify the convergence analysis, without loss of general-
K

= Φ (x)

x, x

Φ

x

T

′

′

(cid:16)

(cid:17)

(cid:17)

(cid:16)

2 = K (x, x) = 1,
(cid:17)
ity we further assume that
k
solution of optimization problem in Eq. (3) or (4) by w∗, that is, w∗ = argminw f (w).

. Finally, we denote the optimal

Φ (x)
k

∈ X

(cid:16)

x

∀

−

(cid:18)(cid:13)
(cid:13)
(cid:13)

(cid:19)

(cid:13)
(cid:13)
(cid:13)

X →

5. Stochastic Gradient Descent Method

′

t

i Φ (xi). The vector xi (1

We introduce the standard kernel stochastic gradient descent (SGD) in Algorithm 1 wherein
the standard learning rate ηt = 1
λt is used (Shalev-Shwartz et al., 2007, 2011). Let αt be a
(wt; xt, yt) = αtΦ (xt) (we note that this scalar exists for all common loss
scalar such that l
functions as presented in Section 7). It is apparent that at the iteration t the model wt has
i=1 α(t)
the form of wt =
t) is said to be a support vector
i
if its coeﬃcient α(t)
is nonzero. The model is represented through the support vectors,
P
i
α(t)
and hence we can deﬁne the model size to be
0 and model sparsity as the ratio
α(t)
0 /t). Since it is likely that αt is nonzero
between the current model size and t (i.e.,
(cid:13)
(cid:13)
(e.g., with Hinge loss, it happens if xt lies in the margins of the current hyperplane), the
(cid:13)
(cid:13)
standard kernel SGD algorithm is vulnerable to the curse of kernelization, that is, the model
size, is almost linearly grown with the data size accumulated over time (Steinwart, 2003).
Consequently, the computation gradually becomes slower or even infeasible when the data
size grows rapidly.

(cid:13)
(cid:13)

(cid:13)
(cid:13)

≤

≤

7

Le et al

T

)
·

Φ (

Algorithm 1 Stochastic Gradient Descent algorithm.
Input: λ, p.s.d. kernel K (., .) = Φ (
)
·
1: w1 = 0
2: for t = 1, 2, . . . T do
Receive (xt, yt)
3:
ηt = 1
λt
gt = λwt + l
6: wt+1 = wt −
7: end for
Output: wT = 1
T

(wt; xt, yt) = λwt + αtΦ (xt)
ηtgt = t
−

T
t=1 wt or wT +1

t wt −

ηtαtΦ (xt)

5:

4:

1

′

P

//(xt, yt)

P

,

X

Y

∼

or PN

6. Approximation Vector Machines for Large-scale Online Learning

In this section, we introduce our proposed Approximation Vector Machine (AVM) for online
learning. The main idea is that we employ an overlapping partition of suﬃciently small
cells to cover the data domain, i.e.,
); when an instance arrives, we approximate
X
this instance by a corresponding core point in the cell that contains this instance. Our
intuition behind this approximation procedure is that since the instance is approximated
by its neighbor, the performance would not be signiﬁcantly compromised while gaining
signiﬁcant speedup. We start this section with the deﬁnition of δ-coverage, its properties
and connection with the feature space. We then present AVM and the convergence analysis.

or Φ (

X

6.1 δ-coverage over a domain

To facilitate our technical development in sequel, we introduce the notion of δ-coverage in
this subsection. We ﬁrst start with the usual deﬁnition of a diameter for a set.

Deﬁnition 1. (diameter ) Given a set A, the diameter of this set is deﬁned as D (A) =
sup
x,x′

. This is the maximal pairwise distance between any two points in A.

−

x

||

′

x
A||
∈

Next, given a domain

(e.g., the data domain, input space) we introduce the concept

of δ-coverage for

using a collection of sets.

X

X

Deﬁnition 2. (δ-coverage) The collection of sets
the domain
iﬀ
discrete) and each element Pi ∈ P
set I is ﬁnite, the collection

I Pi and D (Pi)
∈

X ⊂ ∪i

≤

X

δ,

I is said to be an δ-coverage of
P
∈
I where I is the index set (not necessarily
is further referred to as a cell. Furthermore if the index

= (Pi)i

∈

∀

i

is called a ﬁnite δ-coverage.

P
Deﬁnition 3. (core set, core point) Given an δ-coverage

, for each i

X
∈
ci (s) is called the core set
as a core point.

I over a given domain
∈
I, we select an arbitrary point ci from the cell Pi, then the collection of all
is further referred to

of the δ-coverage

P
. Each point ci ∈ C

= (Pi)i

P

C

We show that these deﬁnitions can be also extended to the feature space with the

mapping Φ and kernel K via the following theorem.

8

Approximation Vector Machines

Theorem 4. Assume that the p.s.d. and isotropic kernel K(x, x
, where
k (.) is a monotonically continuous decreasing function with k (0) = 1, is examined and
Φ (.) is its induced feature map.
then
X
k (δ2))
Φ (
Pi))i
is a monotonically increasing function and lim
0
→

(cid:17)
I is an δ-coverage of the domain
∈
2 (1

P
I is also an δΦ-coverage of the domain Φ (
∈

), where δΦ =

= (Pi)i

δΦ = 0.

) = (Φ (

) = k

p

−

−

X

P

If

(cid:16)

x

x

||

δ

′

2
||

′

In particular, the Gaussian kernel given by K(x, x

) = exp(

γ

x

−
γδ2)). Theorem 4 further reveals that the image of an
iso. kernel and δΦ =
δ-coverage in the input space is an δΦ-coverage in the feature space and when the diameter
δ approaches 0, so does the induced diameter δΦ. For readability, the proof of this theorem
is provided in Appendix A.

exp (

2 (1

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

p

−

−

−

) is a p.s.d. and

′

2

′

x

We have further developed methods and algorithms to eﬃciently construct δ-coverage,

however to maintain the readability, we defer this construction to Section 6.3.

6.2 Approximation Vector Machines

We now present our proposed Approximation Vector Machine (AVM) for online learning. In
an online setting, instances arise on the ﬂy and we need an eﬃcient approach to incorporate
incoming instances into the learner. Diﬀerent from the existing works (cf. Section 2), our
approach is to construct an δ-coverage
, and for each
= (Pi)i
incoming instance x we ﬁnd the cell Pi that contains this instance and approximate this
can either be constructed
instance by a core point ci ∈
in advance or on the ﬂy as presented in Section 6.3.

I over the input domain
∈

Pi. The coverage

and core set

X

P

P

C

′

∀

In Algorithm 2, when receiving an incoming instance (xt, yt), we compute the scalar
(wt; xt, yt) (cf. Section 7) in Step 5. Furthermore at Step 7 we
αt such that αtΦ (xt) = l
introduce a Bernoulli random variable Zt to govern the approximation procedure. This
random variable could be either statistically independent or dependent with the incoming
instances and the current model. In Section 9.2, we report on diﬀerent settings for Zt and
how they inﬂuence the model size and learning performance. Our ﬁndings at the outset
is that, the naive setting with P (Zt = 1) = 1,
t (i.e., always performing approximation)
returns the sparsest model while obtaining comparable learning performance comparing
with the other settings. Moreover, as shown in Steps 9 and 11, we only approximate the
incoming data instance by the corresponding core point (i.e., cit) if Zt = 1. In addition, if
Zt = 1, we ﬁnd a cell that contains this instance in Step 8. It is worth noting that the δ-
coverage and the cells are constructed on the ﬂy along with the data arrival (cf. Algorithms
3 and 4). In other words, the incoming data instance might belong to an existing cell or a
new cell that has the incoming instance as its core point is created.
wtk

1 in the case of ℓ2 loss, if λ
1
≤
1/2,
then we project wt −
ηtht onto the hypersphere with centre origin and radius ymaxλ−
. Since it can be shown that with ℓ2 loss the optimal solution w∗ lies
0, ymaxλ−
i.e.,
B
1/2
0, ymaxλ−
in
(cf. Theorem 23 in Appendix C), this operation could possibly result in
(cid:1)
(cid:0)
a faster convergence. In addition, by reusing the previous information, this operation can
be eﬃciently implemented. Finally, we note that with ℓ2 loss and λ > 1, we do not need to
wtk
perform a projection to bound
k

since according to Theorem 25 in Appendix C,

Furthermore to ensure that

is bounded for all t

wtk

1/2

≥

B

k

k

(cid:1)

(cid:0)

9

Le et al

is bounded by ymax
1 . Here it is worth noting that we have deﬁned ymax = maxy
−
this notation is only used in the analysis for the regression task with the ℓ2 loss.

λ

y
∈Y |

|

and

), δ-coverage
Φ (
·

P

= (Pi)i

I
∈

//(xt, yt)

P

,

X

Y

∼

or PN

//cf. Section 7

//do approximation

Algorithm 2 Approximation Vector Machine.
T
Input: λ, p.s.d. & iso. K (., .) = Φ (
)
·
1: w1 = 0
2: for t = 1, . . . , T do
Receive (xt, yt)
3:
ηt = 1
4:
λt
l
Sample a Bernoulli random variable Zt
if Zt = 1 then
Find it ∈
ht = λwt + αtΦ (cit)

I such that xt ∈

(wt; xt, yt) = αtΦ (xt)

Pit

9:

6:

7:

8:

5:

′

10:

11:

12:
13:

14:

15:

16:

else

ht = λwt + αtΦ (xt)

end if
if ℓ2 loss is used and λ

wt+1 =

else

B

Q

wt+1 = wt −

ηtht

end if
17:
18: end for
Output: wT = PT

wt

t=1
T

or wT +1

1 then

≤
(0,ymaxλ−1/2) (wt −

ηtht)

In what follows, we present the theoretical results for our proposed AVM including the
convergence analysis for a general convex or smooth loss function and the upper bound
of the model size under the assumption that the incoming instances are drawn from an
arbitrary distribution and arrive in a random order.

6.2.1 Analysis for Generic Convex Loss Function

We start with the theoretical analysis for Algorithm 2. The decision of approximation (i.e.,
the random variable Zt) could be statistically independent or dependent with the current
model parameter wt and the incoming instance (xt, yt). For example, one can propose an
algorithm in which the decision of approximation is performed iﬀ the conﬁdence level of the
incoming instance w.r.t the current model is greater than 1, i.e., ytwT
1. We shall
develop our theory to take into account all possible cases.

t Φ (xt)

≥

Theorem 5 below establishes an upper bound on the regret under the possible assump-
tions of the statistical relationship among the decision of approximation, the data distribu-
tion, and the current model. Based on Theorem 5, in Theorem 8 we further establish an
inequality for the error incurred by a single-point output with a high conﬁdence level.

Theorem 5. Consider the running of Algorithm 2 where (xt, yt) is uniformly sampled from
the training set
,

or the joint distribution P

, the following statements hold

D

X

Y

10

Approximation Vector Machines

T

t=1
X

T

Xt=1

i) If Zt and wt are independent for all t (i.e., the decision of approximation only depends

on the data distribution) then

E [f (wT )

f (w∗)]

−

H (log (T ) + 1)
2λT

+

δΦM 1/2W 1/2
T

≤

P (Zt = 1)1/2

where H, M, W are positive constants.

ii) If Zt is independent with both (xt, yt) and wt for all t (i.e., the decision of approxi-

mation is independent with the current hyperplane and the data distribution) then

E [f (wT )

f (w∗)]

−

H (log (T ) + 1)
2λT

+

δΦM 1/2W 1/2
T

≤

P (Zt = 1)

iii) In general, we always have

E [f (wT )

f (w∗)]

−

H (log (T ) + 1)
2λT

≤

+ δΦM 1/2W 1/2

Remark 6. Theorem 5 consists of the standard convergence analysis. In particular, if the
approximation procedure is never performed, i.e., P (Zt = 1) = 0,
t, we have the regret
H(log(T )+1)
bound E [f (wT )
2λT
Remark 7. Theorem 5 further indicates that there exists an error gap between the opti-
mal and the approximate solutions. When δ decreases to 0, this gap also decreases to 0.
Speciﬁcally, when δ = 0 (so does δΦ), any incoming instance is approximated by itself and
consequently, the gap is exactly 0.

f (w∗)]

≤

−

∀

.

Theorem 8. Let us deﬁne the gap by dT , which is δΦM 1/2W 1/2
P (Zt = 1)1/2(if Zt is
independent with wt), δΦM 1/2W 1/2
P (Zt = 1) (if Zt is independent with (xt, yt) and
P
wt), or δΦM 1/2W 1/2. Let r be any number randomly picked from
. With the
probability at least (1

δ), the following statement holds

1, 2, . . . , T
{

T
t=1

T
t=1

P

}

T

T

−

f (wr)

f (w∗)

−

H (log (T ) + 1)
2λT

≤

+ dT + ∆T

log

1
2

r

1
δ

where ∆T = max
T
t
≤

≤

1

(f (wt)

f (w∗)).

−

We now present the convergence analysis for the case when we output the α-suﬃx

average result as proposed in (Rakhlin et al., 2012). With 0 < α < 1, let us denote

where we assume that the fractional indices are rounded to their ceiling values.

Theorem 9 establishes an upper bound on the regret for the α-suﬃx average case,
followed by Theorem 10 which establishes an inequality for the error incurred by a α-suﬃx
average output with a high conﬁdence level.

wα

T =

1
αT

T

wt

Xt=(1
−

α)T +1

11

Le et al

Theorem 9. Consider the running of Algorithm 2 where (xt, yt) is uniformly sampled from
the training set
,
Y
i) If Zt and wt are independent for all t (i.e., the decision of approximation only depends

or the joint distribution P

, the following statements hold

D

X

on the data distribution) then

E [f (wα
T )

f (w∗)]

−

λ (1

α)

−
2α

W α

T +

δΦM 1/2W 1/2
αT

≤

P (Zt = 1)1/2+

H log (1/ (1
2λαT

−

α))

Xt=(1
−

α)T +1

T

T

Xt=(1
−

α)T +1

where H, M, W are positive constants and W α

T = E
ii) If Zt is independent with both (xt, yt) and wt for all t (i.e., the decision of approxi-

α)T +1 −

w(1

w∗

−

.

i

mation is independent with the current hyperplane and the data distribution) then

h(cid:13)
(cid:13)

2

(cid:13)
(cid:13)

E [f (wα
T )

f (w∗)]

−

λ (1

α)

−
2α

W α

T +

δΦM 1/2W 1/2
αT

≤

P (Zt = 1)+

H log (1/ (1
2λαT

−

α))

iii) In general, we always have

E [f (wα
T )

f (w∗)]

−

λ (1

α)

−
2α

≤

W α

T + δΦM 1/2W 1/2 +

H log (1/ (1
2λαT

−

α))

T + δΦM 1/2W 1/2

Theorem 10. Let us once again deﬁne the induced gap by dT , which is respec-
tively λ(1
α)
P (Zt = 1)1/2(if Zt is independent with wt),
2α W α
−
T + δΦM 1/2W 1/2
α)
λ(1
T
2α W α
is independent with (xt, yt) and
−
P
t=(1
α)T +1
αT
−
wt), or λ(1
α)
T + δΦM 1/2W 1/2.
2α W α
Let r be any number randomly picked from
−
δ), the following statement holds
α) T + 1, 2, . . . , T
(1
{

P
. With the probability at least (1

α)T +1
−
P (Zt = 1) (if Zt

T
t=(1

−

−

αT

}

f (wr)

f (w∗)

−

H log (1/ (1
2λαT

−

≤

α))

+ dT + ∆α
T

1
2

r

log

1
δ

where ∆α

T =

max
α)T +1
≤

(1

−

T

t
≤

(f (wt)

f (w∗)).

−

Remark 11. Theorems 8 and 10 concern with the theoretical warranty if rendering any
single-point output wr rather than the average outputs. The upper bound gained in
Theorem 10 is tighter than that gained in Theorem 8 in the sense that the quantity
H log(1/(1
2λαT

δ decreases faster and may decrease to 0 when T

given a

α))

+

→

∞

+ ∆α
−
T
conﬁdence level 1

1

2 log 1
δ.
q
−

6.2.2 Analysis for Smooth Loss Function

Deﬁnition 12. A loss function l (w; x, y) is said to be µ-strongly smooth w.r.t a norm
iﬀ for all u, v and (x, y) the following condition satisﬁes

.
k
k

l (v; x, y)

l (u; x, y) + l

(u; x, y)

(v

u) +

′

T

µ
2 k

v

u

2
k

−

−

Another equivalent deﬁnition of µ-strongly smooth function is

≤

(cid:13)
(cid:13)
(cid:13)

′

l

(u; x, y)

′

l

(v; x, y)

−

µ

v

k

u

k

−

≤

∗

(cid:13)
(cid:13)
(cid:13)

12

Approximation Vector Machines

where
.
k∗
k
It is well-known that

is used to represent the dual norm of the norm

.
.
k

k

ℓ2 loss is 1-strongly smooth w.r.t

k2.
.
Logistic loss is 1-strongly smooth w.r.t

k

k2.
.
k

•

•

•

τ -smooth Hinge loss (Shalev-Shwartz and Zhang, 2013) is 1
k2.
.

k

τ -strongly smooth w.r.t

2 + 1, λ

Theorem 13. Assume that ℓ2, Logistic, or τ -smooth Hinge loss is used, let us denote
L = λ
1 respectively. Let us deﬁne the gap by dT as in Theorem 10.
Let r be any number randomly picked from
. With the probability
α) T + 1, 2, . . . , T
at least (1

(1
{
δ), the following statement holds

2 + 1, or λ

2 + τ −

−

}

−

f (wr)

f (w∗)

−

H log (1/ (1
2λαT

−

≤

α))

+ dT +

LM α
T
2 r

1
2

log

1
δ

where M α

T =

max
α)T +1
≤

(1

−

T k

t
≤

wt −

.

w∗k

Remark 14. Theorem 13 extends Theorem 10 for the case of smooth loss function. This
allows the gap H log(1/(1
δ to be quantiﬁed more precisely regarding
−
2λαT
the discrepancy in the model itself rather than that in the objective function. The gap
H log(1/(1
2λαT

δ could possibly decrease rapidly when T approaches +

+ LM α

+ LM α

2 log 1

2 log 1

q

α))

α))

−

2

2

1

1

.

T

T

q

Algorithms
Forgetron (Dekel et al., 2005)
PA-I, II (Crammer et al., 2006)
Randomized Budget Perceptron (Cavallanti et al., 2007)
Projection (Orabona et al., 2009)

Kernelized Pegasos (Shalev-Shwartz et al., 2011)

Budgeted SGD (Wang et al., 2012)

Fourier OGD (Lu et al., 2015)

Nystrom OGD (Lu et al., 2015)

AVM (average output)

AVM (α-suﬃx average output)

∞

Budget
MB
NB
NB
AB

NB

MB

MB

MB

AB

AB

Regret
NA
NA
NA
NA
log(T )
T
log(T )
T
1
√T
1
√T
log(T )
(cid:16)
(cid:17)
T
1
T

O
(cid:16)

(cid:16)

(cid:17)

(cid:16)

(cid:16)
O

O

(cid:17)

(cid:17)

(cid:17)

O

O

O

(cid:0)

(cid:1)

Table 1: Comparison on the regret bounds and the budget sizes of the kernel online algo-
rithms. On the column of budget size, NB stands for Not Bound (i.e., the model size is not
bounded and learning method is vulnerable to the curse of kernelization), MB stands for
Manual Bound (i.e., the model size is manually bounded by a predeﬁned budget), and AB
is an abbreviation of Automatic Bound (i.e., the model size is automatically bounded and
this model size is automatically inferred).

13

Le et al

To end this section, we present the regret bound and the obtained budget size for our
AVM(s) together with those of algorithms listed in Table 1. We note that some early
works on online kernel learning mainly focused on the mistake rate and did not present any
theoretical results regarding the regret bounds.

6.2.3 Upper Bound of Model Size

In what follows, we present the theoretical results regarding the model size and sparsity
level of our proposed AVM. Theorem 15 shows that AVM oﬀers a high level of freedom to
control the model size. Especially, if we use the always-on setting (i.e., P (Zt = 1) = 1,
t),
the model size is bounded regardless of the data distribution and data arrival order.
Theorem 15. Let us denote P (Zt = 1) = pt, P (Zt = 0) = qt, and the number of cells
generated after the iteration t by Mt. If we deﬁne the model size, i.e., the size of support
set, after the iteration t by St, the following statement holds

∀

T

T

T

≤

1]

Mt

qt +

t=1
X

E [ST ]

ptE [Mt −
Specially, if we use some speciﬁc settings for pt, we can bound the model size E [St] accord-
ingly as follows
i) If pt = 1,
i.e., its number of cells.
β
ii) If pt = max
t

speciﬁes the size of the partition

t then E [ST ]

qt + E [MT ]

E [MT ]

, where

≤ |P|

t=1
X

t=1
X

0, 1

|P|

≤

≤

P

∀

−

,

,

−

−

(cid:17)
β
tρ
β
tρ

(cid:17)

(cid:16)
0, 1

∀
,

≤

β (log (T ) + 1) + E [MT ].
βT 1−ρ

t then E [ST ]
t, where 0 < ρ < 1, then E [ST ]
∀
t, where ρ > 1, then E [ST ]

≤

1

−
βζ (ρ) + E [MT ]

ρ + E [MT ].

iv) If pt = max
where ζ (.) is ζ- Riemann function deﬁned by the integral ζ (s) = 1
Γ(s)

≤

−

(cid:17)

(cid:16)

∀

,

(cid:16)
iii) If pt = max

0, 1

βζ (ρ) +

,
|P|

+
∞0

≤
ts−1
es

−

1 dt.

R

0, 1

, where ρ > 1 or ρ = +

Remark 16. We use two parameters β and ρ to ﬂexibly control the rate of approximation pt.
It is evident that when β increases, the rate of approximation decreases and consequently
the model size and accuracy increase. On the other hand, when ρ increases, the rate of
approximation increases as well and it follows that the model size and accuracy decreases.
We conducted experiment to investigate how the variation of these two parameters inﬂuence
the model size and accuracy (cf. Section 9.2).
Remark 17. The items i) and iv) in Theorem 15 indicate that if P (Zt = 1) = pt =
β
max
(by
tρ
) = 0). In fact, the tight upper bound is βζ (ρ) + E [MT ], where
convention we deﬁne ζ (+
MT is the number of unique cells used so far. It is empirically proven that MT could be
very small comparing with T and
T )
are all lain in the core set, if we output the average wT = PT
wα
Remark 18. The items ii) and iii) in Theorem 15 indicate that if P (Zt = 1) = pt =
max
1 then although the model size is not bounded, it would
slowly increase comparing with T , i.e., log (T ) or T 1

α)T +1 wT , the model size is still bounded.

. In addition, since all support sets of wt (1

, then the model size is bounded by βζ (ρ)+

ρ when ρ is around 1.

or α-suﬃx average

, where 0 < ρ

T = 1
αT

T
t=(1

0, 1

t=1
T

|P|

|P|

P

∞

∞

β
tρ

wt

≤

≤

≤

−

−

(cid:16)

(cid:17)

−

t

(cid:16)

(cid:17)

−

14

Approximation Vector Machines

6.3 Construction of δ-Coverage

In this section, we return to the construction of δ-coverage deﬁned in Section 6.1 and present
two methods to construct a ﬁnite δ-coverage. The ﬁrst method employs hypersphere cells
(cf. Algorithm 3) whereas the second method utilizes the hyperrectangle cells (cf. Algorithm
4). In these two methods, the cells in coverage are constructed on the ﬂy when the incoming
instances arrive. Both are theoretically proven to be a ﬁnite coverage.

Algorithm 3 Constructing hypersphere δ-coverage.

Algorithm 4 Constructing hyperrectangle δ-coverage.

=

5:

6:

1:
∅
P
2: n = 0
3: for t = 1, 2, . . . do
Receive (xt, yt)
4:
it = argmink
if
xt −
k
n = n + 1
cn = xt
it = n
=
P
end if
11:
12: end for

n k
cit k ≥

P ∪

[
B

10:

7:

9:

8:

≤

ckk
xt −
δ/2 then

(cn, δ/2)]

< a then

∅

P

=
1:
2: a = δ/√d
3: n = 0
4: for t = 1, 2, . . . do
Receive (xt, yt)
5:
it = 0
6:
for i = 1 to n do
xt −
k
it = i
break

cik∞

if

7:

8:

9:

10:

11:

12:

end if
end for
if

14:

15:

13:

it = 0 then
n = n + 1
cn = xt
it = n
=
P
end if
18:
19: end for

16:
17:

[
R

P ∪

(cn, a)]

Algorithm 3 employs a collection of open hypersphere cell
< R

(c, R), which is deﬁned
, to cover the data domain. Similar to Algorithm 3,

(c, R) =

Rd :

B

x

x

as

B

k

−

c
k

∈

(cid:8)

(cid:9)

15

Le et al

x
k

c
k∞

Algorithm 4 uses a collection of open hyperrectangle
, to cover the data domain.

Rd :

< a

x

R

(c, a), which is given by

(c, a) =

R

−

∈
Both Algorithms 3 and 4 are constructed in the common spirit: if the incoming instance
(cid:8)
(xt, yt) is outside all current cells, a new cell whose centre or vertex is this instance is
generated. It is noteworthy that the variable it in these two algorithms speciﬁes the cell
that contains the new incoming instance and is the same as itself in Algorithm 2.

(cid:9)

Theorem 19 establishes that regardless of the data distribution and data arrival order,
Algorithms 3 and 4 always generate a ﬁnite δ-coverage which implies a bound on the model
size of AVM. It is noteworthy at this point that in some scenarios of data arrival, Algo-
. However, since the
rithms 3 and 4 might not generate a coverage for the entire space
(ci, δ), without loss
(ci, δ) and
generated sequence
of generality we can restrict
(ci, δ)
∪i B
or

xt}t cannot be outside the set
∪i R
∪i B

∪i B
(ci, δ) by assuming that

X
∪i R

(ci, δ) or

to

=

=

X

X

{

(ci, δ).

X

∪i R

Theorem 19. Let us consider the coverages formed by the running of Algorithms 3 and 4.
is compact (i.e., close and bounded) then these coverages are all ﬁnite
If the data domain
δ-coverages whose sizes are all dependent on the data domain
and independent with the
sequence of incoming data instances (xt, yt) received.

X

X

Remark 20. Theorem 19 also reveals that regardless of the data arrival order, the model size
of AVM is always bounded (cf. Remark 17). Referring to the work of (Cucker and Smale,

. However with many possible
2002), it is known that this model size cannot exceed
data arrival orders, the number of active cells or the model size of AVM is signiﬁcantly
smaller than the aforementioned theoretical bound.

(cid:16)

(cid:17)

X

d

)

4D(
δ

6.4 Complexity Analysis

(cid:0)

d2Mt

We now present the computational complexity of our AVM(s) with the hypersphere δ-
coverage at the iteration t. The cost to ﬁnd the hypersphere cell in Step 5 of Algorithm
. The cost to calculate αt in Step 6 of Algorithm 2 is O (St) if we consider
2 is O
the kernel operation as a unit operation. If ℓ2 loss is used and λ
1, we need to do a pro-
(cid:1)
jection onto the hypersphere
which requires the evaluation of the length
of the vector wt −
) which costs St unit operations using incremen-
ηtht (i.e.,
(cid:1)
tal implementation. Therefore, the computational operation at the iteration t of AVM(s)
(since
or O
is either O
Mt ≤

0, ymaxλ−
ηthtk
St

B
wt −
(cid:0)
k
d2 + 1

d2Mt + St + St

d2Mt + St

d2 + 2

= O

= O

St).

St

(cid:0)(cid:0)

(cid:0)(cid:0)

1/2

≤

(cid:0)

(cid:1)

(cid:0)

(cid:1)

(cid:1)

(cid:1)

(cid:1)

(cid:1)

7. Suitability of Loss Functions

We introduce six types of loss functions that can be used in our proposed algorithm, namely
insensitive, and τ -smooth Hinge. We verify that these loss func-
Hinge, Logistic, ℓ2, ℓ1, ε
−
1/2 + B for some
(w; x, y)
tions satisfying the necessary condition, that is,
k
appropriate positive numbers A, B (this is required for our problem formulation presented
in Section 4).

w

≤

A

k

l

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

′

For comprehensibility, without loss of generality, we assume that
x

= K (x, x)1/2 =
. At the outset of this section, it is noteworthy that for classiﬁcation task (i.e.,

Φ (x)
k
k

1,

∀

∈ X

16

Approximation Vector Machines

Hinge, Logistic, and τ -smooth Hinge cases), the label y is either
implies

= y2 = 1.

−

1 or 1 which instantly

y
|

|

Hinge loss

•

l (w; x, y) = max

0, 1

′

l

(w; x, y) =

n
ywTΦ(x)

I
{

−

≤

1
}

ywT

Φ (x)

−

yΦ (x)

o

where IS is the indicator function which renders 1 if the logical statement S is true
and 0 otherwise.

Therefore, by choosing A = 0, B = 1 we have

′

l

(w; x, y)

Φ (x)

1 = A

≤ k

k ≤

w

k

k

1/2 + B

ℓ2 loss

•

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

1/2 + B for
In this case, at the outset we cannot verify that
k
all w, x, y. However, to support the proposed theory, we only need to check that

(w; x, y)

w

≤

A

k

l

′

1/2 + B for all t

1. We derive as follows

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

′

l

(wt; x, y)

A

wtk
k

≤

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

l (w; x, y) =

′

l

(w; x, y) =

≥

y

1
2
(cid:16)
wT
(cid:16)

wT

Φ (x)

2

−
Φ (x)

(cid:17)
Φ (x)

y

−

(cid:17)

′

l

(wt; x, y)

=

(cid:13)
(cid:13)
(cid:13)

wT
|

t Φ (x) + y
wtk

k k

Φ (x)

wT

Φ (x)

| k
+ ymax ≤

k ≤ |
A

k

t Φ (x)
wtk

|
1/2 + B

+ ymax

(cid:13)
(cid:13)
(cid:13)

≤ k
y1/2
maxλ−
y1/2
max (λ

(

1/4

−

where B = ymax and A =

if λ

1

≤
otherwise

.

1/2

1)−

Here we note that we make use of the fact that
(cf. Theorem 25 in Appendix C) and
Algorithm 2 ).

wtk ≤

k

wtk ≤
k
ymaxλ−

ymax (λ

1 if λ > 1
−
1/2 otherwise (cf. Line 12 in

1)−

ℓ1 loss

•

Therefore, by choosing A = 0, B = 1 we have

l (w; x, y) =

−
(w; x, y) = sign

′

l

y
|

wT
wT

Φ (x)

|
Φ (x)

y

Φ (x)

−

(cid:17)

(cid:16)

(w; x, y)

=

Φ (x)

1 = A

k

k ≤

1/2 + B

w

k

k

′

l

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

17

Le et al

Logistic loss

•

l (w; x, y) = log

′

l

(w; x, y) = −

Φ (x)

1 + exp

ywT
−
ywTΦ (x)
(cid:16)
y exp
−
ywTΦ (x)) + 1
exp (
(cid:0)
−

(cid:16)

(cid:1)

Φ (x)

(cid:17)(cid:17)

Therefore, by choosing A = 0, B = 1 we have

′

l

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(w; x, y)

<

Φ (x)

1 = A

k

k ≤

1/2 + B

w

k

k

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

•

•

ε-insensitive loss

l (w; x, y) = max

′

l

(w; x, y) = I

wT

Φ (x)

ε

0,

y
|

−

n
wTΦ(x)
|

y

−

{|

sign

>ε

}

| −
wT
(cid:16)

o
Φ (x)

y

Φ (x)

−

(cid:17)

Therefore, by choosing A = 0, B = 1 we have

′

l

(w; x, y)

Φ (x)

1 = A

≤ k

k ≤

w

k

k

1/2 + B

τ -smooth Hinge loss (Shalev-Shwartz and Zhang, 2013)

−

0
l (w; x, y) = 
1

1
2τ
I

−
{
1I
+ τ −
1

(w; x, y) =

′

l

ywTΦ (x)
1

τ
2
−
ywTΦ (x)

2

−
(cid:0)
ywTΦ(x)<1

τ

−

}
ywTΦ(x)

yΦ (x)
(cid:1)
ywT

1

≤

τ

−

≤

if ywTΦ (x) > 1
if ywTΦ (x) < 1
otherwise

τ

−

Φ (x)

1

yΦ (x)

−

(cid:17)

(cid:16)
Therefore, by choosing A = 0, B = 2, we have

′

l

(w; x, y)

(cid:13)
(cid:13)
(cid:13)

y
≤ |
= A

| k
w

Φ (x)
k
1/2 + B
k

k

(cid:13)
(cid:13)
(cid:13)

1

+ τ −

y
|

Φ (x)
k

τ

| k

≤

2

8. Multiclass Setting

In this section, we show that our proposed framework could also easily extend to the multi-
class setting. We base on the work of (Crammer and Singer, 2002) for multiclass classiﬁca-
tion to formulate the optimization problem in multi-class setting as

f (W ) , λ
2 k

W

2
2,2 +
k

1
N

min
W  

yiΦ (xi)

l

wT
(cid:16)

wT

−

ziΦ (xi)
(cid:17)

!

N

Xi=1

18

where we have deﬁned

Approximation Vector Machines

zi = argmax

wT

j Φ (xi) ,

j6=yi

W = [w1, w2, . . . , wm] ,

m

W
k

2
2,2 =
k

2 ,

wjk
k

Xj=1
max (0, 1
a) Hinge loss
−
a)
log (1 + e−

Logistic loss

(

l (a) =

w(t+1)
j

′

′

ηtl
j −
j + ηtl

1

1

t
−
t
−
t
−

t w(t)
t w(t)
t w(t)

1

j

= 


(a) Φ (xt)
(a) Φ (xt)

if j = yt
if j = zt
otherwise

For the exact update, at the t-th iteration, we receive the instance (xt, yt) and modify

W as follows

where a = wT

I
−
{
The algorithm for Approximation Vector Machine with multiclass setting proceeds as


ztΦ (xt) and l

1/ (1 + ea).

ytΦ (xt)

(a) =

wT

a<1

or

−

−

}

′

in Algorithm 5.

Algorithm 5 Multiclass Approximation Vector Machine.
Input: λ, p.s.d. & iso. kernel K (., .), δ-coverage
= (Pi)i
1: W1 = 0
2: for t = 1, . . . , T do
Receive (xt, yt)
3:
a = wT
max
ytΦ (xt)
−
j
=yt
1
t W (t)
−

j Φ (xt)

wT

P

4:

5: W (t+1) = t
6:

I
∈

Sample a Bernoulli random variable Zt
if Zt = 1 then
Find it ∈
w(t+1)
yt
w(t+1)
zt

I such that xt ∈
ηtl
yt −
zt + ηtl

Pit
(a) Φ (cit )
(a) Φ (cit )

= w(t+1)
= w(t+1)

′

′

//(xt, yt)

P

,

X

Y

∼

or PN

//do approximation

7:

8:

9:

10:

11:

else

13:

12:

w(t+1)
yt
w(t+1)
zt
end if
14:
15: end for
Output W

(T )

9. Experiments

= w(t+1)
= w(t+1)

ηtl
yt −
zt + ηtl

′

′

(a) Φ (xt)
(a) Φ (xt)

= PT

t=1 W (t)
T

or W (t+1)

In this section, we conduct comprehensive experiments to quantitatively evaluate the capac-
ity and scalability of our proposed Approximation Vector Machine (AVM) on classiﬁcation
and regression tasks under three diﬀerent settings:

19

Le et al

•

•

Batch classiﬁcation 4: the regular binary and multiclass classiﬁcation tasks that follow
a standard validation setup, wherein each dataset is partitioned into training set and
testing set. The models are trained on the training part, and then their discriminative
capabilities are veriﬁed on the testing part using classiﬁcation accuracy measure. The
computational costs are commonly measured based on the training time.

Online classiﬁcation: the binary and multiclass classiﬁcation tasks that follow a purely
online learning setup, wherein there is no division of training and testing sets as in
batch setting. The algorithms sequentially receive and process a single data sample
turn-by-turn. When an individual data point comes, the models perform prediction
to compute the mistake rate ﬁrst, then use the feature and label information of such
data point to continue their learning procedures. Their predictive performances and
computational costs are measured basing on the average of mistake rate and execution
time, respectively, accumulated in the learning progress on the entire dataset.

•

Online regression: the regression task that follows the same setting of online classiﬁ-
cation, except the predictive performances are measured based on the regression error
rate accumulated in the learning progress on the entire dataset.

Our main goal is to examine the scalability, classiﬁcation and regression capabilities of
AVMs by directly comparing with those of several recent state-of-the-art batch and online
learning approaches using a number of datasets with a wide range of sizes. Our models are
implemented in Python with Numpy package. The source code and experimental scripts are
published for reproducibility5. In what follows, we present the data statistics, experimental
setup, results and our observations.

9.1 Data statistics and experimental setup

We use 11 datasets whose statistics are summarized in Table 2. The datasets are selected in
a diverse array of sizes in order to clearly expose the diﬀerences among scalable capabilities
of the models. Five of which (year, covtype, poker, KDDCup99, airlines) are large-scale
datasets with hundreds of thousands and millions of data points, whilst the rest are ordinal-
size databases. Except the airlines, all of the datasets can be downloaded from LIBSVM6
and UCI7 websites.

The airlines dataset is provided by American Statistical Association (ASA8). The dataset
contains information of all commercial ﬂights in the US from October 1987 to April 2008.
The aim is to predict whether a ﬂight will be delayed or not and how long in minutes the
ﬂight will be delayed in terms of departure time. The departure delay time is provided in
the ﬂight database. A ﬂight is considered delayed if its delay time is above 15 minutes, and
non-delayed otherwise. The average delay of a ﬂight in 2008 was of 56.3 minutes. Following
the procedure of (Hensman et al., 2013), we further process the data in two steps. First, we
join the data with the information of individual planes basing on their tail numbers in order

4. This setting is also known as oﬄine classiﬁcation.
5. https://github.com/tund/avm
6. https://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/
7. https://archive.ics.uci.edu/ml/datasets.html
8. The data can be downloaded from http://stat-computing.org/dataexpo/2009/.

20

Approximation Vector Machines

Source
UCI

Dataset #training #testing #features #classes
2
2 LIBSVM
2 LIBSVM
2 LIBSVM
7 LIBSVM

16, 281
14, 951
271, 617
91, 701
58, 101
1, 000, 000
489, 842
592, 942

32, 561
49, 749
59, 535
49, 990
522, 911
25, 010
4, 408, 589
5, 336, 471

123
a9a
300
w8a
8
cod-rna
22
ijcnn1
54
covtype
10
poker
41
KDDCup99
airlines
8
Dataset #training #testing #features
9
384
90
8

UCI
UCI
ASA
Source
UCI
UCI
UCI
ASA

10
23
2
value
[0, 1]
[0, 1]
[0, 1]
R+

45, 730
53, 500
515, 345
5, 929, 413

casp
slice
year
airlines

–
–
–
–

Table 2: Data statistics. #training: number of training samples; #testing: number of
testing samples.

to obtain the manufacture year. This additional information is provided as a supplemental
data source on ASA website. We then extract 8 features of many available ﬁelds: the age of
the aircraft (computed based on the manufacture year), journey distance, airtime, scheduled
departure time, scheduled arrival time, month, day of week and month. All features are
normalized into the range [0, 1].

In batch classiﬁcation experiments, we follow the original divisions of training and testing
sets in LIBSVM and UCI sites wherever available. For KDDCup99, covtype and airlines
datasets, we split the data into 90% for training and 10% for testing. In online classiﬁcation
and regression tasks, we either use the entire datasets or concatenate training and testing
parts into one. The online learning algorithms are then trained in a single pass through
the data. In both batch and online settings, for each dataset, the models perform 10 runs
on diﬀerent random permutations of the training data samples. Their prediction results
and time costs are then reported by taking the average with the standard deviation of the
results over these runs.

For comparison, we employ some baseline methods that will be described in the fol-
lowing sections. Their C++ implementations with Matlab interfaces are published as a
part of LIBSVM, BudgetedSVM9 and LSOKL10 toolboxes. Throughout the experiments,

we utilize RBF kernel, i.e., K

′

x, x

for all algorithms including

(cid:18)
ours. We use hypersphere strategy to construct the δ-coverage (cf. Section 6.3), due to
its better performance than that of hyperrectangle approach during model evaluation. All
experiments are conducted using a Windows machine with 3.46GHz Xeon processor and
96GB RAM.

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:19)

(cid:17)

(cid:16)

= exp

γ

x

−

−

2

′

x

9. http://www.dabi.temple.edu/budgetedsvm/index.html

10. http://lsokl.stevenhoi.com/

21

Le et al

9.2 Model evaluation on the eﬀect of hyperparameters

In the ﬁrst experiment, we investigate the eﬀect of hyperparameters, i.e., δ-coverage di-
ameter, sampling parameters β and ρ (cf. Section 6.2.3) on the performance of AVMs.
Particularly, we conduct an initial analysis to quantitatively evaluate the sensitivity of
these hyperparameters and their impact on the predictive accuracy and model size. This
analysis provides a heuristic approach to ﬁnd the best setting of hyperparameters. Here the
AVM with Hinge loss is trained following the online classiﬁcation scheme using two datasets
a9a and cod-rna.

To ﬁnd the plausible range of coverage diameter, we use a heuristic approach as follows.
First we compute the mean and standard deviation of pairwise Euclidean distances between
any two data samples. Treating the mean as the radius, the coverage diameter is then varied
around twice of this mean bounded by twice of the standard deviation. Fig. 2a and Fig. 3a
report the average mistake rates and model sizes of AVMs with respect to (w.r.t) these
values for datasets a9a and cod-rna, respectively. Here we set β = 0 and ρ = 1.0. There is
a consistent pattern in both ﬁgures: the classiﬁcation errors increase for larger δ whilst the
model sizes decrease. This represents the trade-oﬀ between model performance and model
size via the model coverage. To balance the performance and model size, in these cases, we
can choose δ = 7.0 for a9a data and δ = 1.0 for cod-rna data.

(a) The eﬀect of δ-coverage diameter on the mis-
take rate and model size.

(b) The eﬀect of β and ρ on the classiﬁcation mistake
rate. β = 0 means always approximating.

Figure 2: Performance evaluation of AVM with Hinge loss trained using a9a dataset with
diﬀerent values of hyperparameters.

Fixing the coverage diameters, we vary β and ρ in 10 values monotonically increasing
from 0 to 10 and from 0.5 to 1.5, respectively, to evaluate the classiﬁcation performance. The
smaller β and larger ρ indicate that the machine approximates the new incoming data more
frequently, resulting in less powerful prediction capability. This can be observed in Fig. 2b
and Fig. 3b, which depict the average mistake rates in 3D as a function of these values for
dataset a9a and cod-rna. Here β = 0 means that the model always performs approximation
without respect to the value of ρ. From these visualizations, we found that the AVM with
always-on approximation mode still can achieve fairly comparable classiﬁcation results.
Thus we set β = 0 for all following experiments.

22

Approximation Vector Machines

(a) The eﬀect of δ-coverage diameter on the mis-
take rate and model size.

(b) The eﬀect of β and ρ on the classiﬁcation mistake
rate. β = 0 means always approximating.

Figure 3: Performance evaluation of AVM with Hinge loss trained using cod-rna dataset
with diﬀerent values of hyperparameters.

9.3 Batch classiﬁcation

We now examine the performances of AVMs in classiﬁcation task following batch mode.
We use eight datasets: a9a, w8a, cod-rna, KDDCup99, ijcnn1, covtype, poker and airlines
(delayed and non-delayed labels). We create two versions of our approach: AVM with Hinge
loss (AVM-Hinge) and AVM with Logistic loss (AVM-Logit). It is noteworthy that the Hinge
loss is not a smooth function with undeﬁned gradient at the point that the classiﬁcation
conﬁdence yf (x) = 1. Following the sub-gradient deﬁnition, in our experiment, we compute
the gradient given the condition that yf (x) < 1, and set it to 0 otherwise.

Baselines. For discriminative performance comparison, we recruit the following state-of-
the-art baselines to train kernel SVMs for classiﬁcation in batch mode:

•

•

•

•

•

•

LIBSVM: one of the most widely-used and state-of-the-art implementations for batch
kernel SVM solver (Chang and Lin, 2011). We use the one-vs-all approach as the
default setting for the multiclass tasks;

LLSVM: low-rank linearization SVM algorithm that approximates kernel SVM op-
timization by a linear SVM using low-rank decomposition of the kernel matrix
(Zhang et al., 2012);

BSGD-M: budgeted stochastic gradient descent algorithm which extends the Pegasos
algorithm (Shalev-Shwartz et al., 2011) by introducing a merging strategy for support
vector budget maintenance (Wang et al., 2012);

BSGD-R: budgeted stochastic gradient descent algorithm which extends the Pegasos
algorithm (Shalev-Shwartz et al., 2011) by introducing a removal strategy for support
vector budget maintenance (Wang et al., 2012);

FOGD: Fourier online gradient descent algorithm that applies the random Fourier
features for approximating kernel functions (Lu et al., 2015);

NOGD: Nystrom online gradient descent (NOGD) algorithm that applies the Nystrom
method to approximate large kernel matrices (Lu et al., 2015).

23

Le et al

Hyperparameters setting. There are a number of diﬀerent hyperparameters for all
methods. Each method requires a diﬀerent set of hyperparameters, e.g., the regularization
parameters (C in LIBSVM, λ in Pegasos and AVM), the learning rates (η in FOGD and
NOGD), the coverage diameter (δ in AVM) and the RBF kernel width (γ in all methods).
Thus, for a fair comparison, these hyperparameters are speciﬁed using cross-validation on
training subset.

Particularly, we further partition the training set into 80% for learning and 20% for val-
idation. For large-scale databases, we use only 1% of training set, so that the searching can
ﬁnish within an acceptable time budget. The hyperparameters are varied in certain ranges
and selected for the best performance on the validation set. The ranges are given as fol-
8, 2−
2−4/N, 2−2/N, ..., 216/N
,
lows: C
η
where N is the number of data points. The
(cid:9)
}
coverage diameter δ of AVM is selected following the approach described in Section 9.2. For
the budget size B in NOGD and Pegasos algorithm, and the feature dimension D in FOGD
for each dataset, we use identical values to those used in Section 7.1.1 of (Lu et al., 2015).

16.0, 8.0, 4.0, 2.0, 0.2, 0.02, 0.002, 0.0002

2, 20, 22, 24, 28

3, ..., 215

, γ
}

5, 2−

4, 2−

∈ {

∈ {

, λ

2−

2−

∈

∈

(cid:8)

(cid:8)

(cid:9)

Results. The classiﬁcation results, training and testing time costs are reported in Table 3.
Overall, the batch algorithms achieve the highest classiﬁcation accuracies whilst those of
online algorithms are lower but fairly competitive. The online learning models, however,
are much sparser, resulting in a substantial speed-up, in which the training time costs and
model sizes of AVMs are smallest with orders of magnitude lower than those of the standard
batch methods. More speciﬁcally, the LIBSVM outperforms the other approaches in most
of datasets, on which its training phase ﬁnishes within the time limit (i.e., two hours),
except for the ijcnn1 data wherein its testing score is less accurate but very close to that
of BSGD-M. The LLSVM achieves good results which are slightly lower than those of the
state-of-the-art batch kernel algorithm. The method, however, does not support multiclass
classiﬁcation. These two batch algorithms – LIBSVM and LLSVM could not be trained
within the allowable amount of time on large-scale datasets (e.g., airlines), thus are not
scalable.

Furthermore, six online algorithms in general have signiﬁcant advantages against the
batch methods in computational eﬃciency, especially when running on large-scale datasets.
Among these algorithms, the BSGD-M (Pegasos+merging) obtains the highest classiﬁcation
scores, but suﬀers from a high computational cost. This can be seen in almost all datasets,
especially for the airlines dataset on which its learning exceeds the time limit. The slow
B2
training of BSGD-M is caused by the merging step with computational complexity
(B is the budget size). By contrast, the BSGD-R (Pegasos+removal) runs faster than the
(cid:1)
merging approach, but suﬀers from very high inaccurate results due to its naive budget
maintenance strategy, that simply discards the most redundant support vector which may
contain important information.

O

(cid:0)

24

Approximation Vector Machines

|

|

|

S

B

Dataset [δ

Table 3: Classiﬁcation performance of our AVMs and the baselines in batch mode. The
notation [δ
D], next to the dataset name, denotes the diameter δ, the model size S
of AVM-based models, the budget size B of budgeted algorithms, and the number of random
features D of FOGD, respectively. The accuracy is reported in percent (%), the training
time and testing time are in second. The best performance is in bold. It is noteworthy that
the LLSVM does not support multiclass classiﬁcation and we terminate all runs exceeding
the limit of two hours, therefore some results are unavailable.
a9a [7.0
Train
84.57
50.73
232.59
90.48
15.99
82.40
4.96
5.35

|
|
Algorithm
LIBSVM
LLSVM
BSGD-M
BSGD-R
FOGD
NOGD
AVM-Hinge
AVM-Logit

4, 000]
Accuracy
84.92
83.00
0.16
3.38
5.05
2.18
0.50
0.34

135
|
Test
22.23
8.73
2.88
2.72
2.87
0.60
0.25
0.25

131
|
Test
2.95
10.41
5.16
4.98
3.55
0.65
0.52
0.52

98.17
97.10
97.92
98.06
96.87
96.96

84.76
80.26
81.15
82.33
83.55
83.83

1, 000

1, 000

D]

B

S

|

|

|

|

Dataset [δ

S

B

D]

cod-rna [1.0

436

400

|

500
|
Test
11.17
54.22
6.13
7.07
10.10
3.68
2.71
2.67

1, 000

|

97.69
90.90
90.64
90.43
91.14
91.19

1, 000

|

±
±
±
±
±
±
1, 600]
|
Accuracy
96.39
94.16
0.21
0.11
4.20
3.35
1.16
2.11

95.67
66.83
92.65
91.83
94.38
93.10

w8a [13.0
Train
50.96
92.19
264.70
253.30
32.16
374.87
11.84
12.54
ijcnn1 [1.0
Train
38.63
40.62
93.05
41.70
7.31
21.58
6.47
6.86
poker [12.0
Train
40.03
–
414.09
35.76
9.61
118.54
3.86
3.36

±
±
±
±
±
±
1, 600]
|
Accuracy
–
–
0.16
1.69
5.85
2.96
0.37
0.34
400] airlines [1.0

±
±
±
±
±
±

393
|
Test
932.58
–
123.57
102.84
101.29
36.84
8.21
7.54

72.26
61.09
59.34
68.20
64.31
64.42

Train
–
–
–
4,741.68
1,085.73
3,112.08
586.90
642.23

|
Test
–
–
–
29.98
861.52
18.53
6.55
6.10

|

200
|
Accuracy
99.91
–
0.00
2.26
0.11
0.02
0.05
0.06

99.73
39.81
99.75
99.80
99.82
99.72

±
±
±
±
±
±

25

4, 000]
|
Accuracy
99.06
98.64
0.07
±
0.04
±
0.38
±
0.18
±
0.28
±
0.00
±
4, 000]
|
Accuracy
97.35
96.99
0.11
±
0.18
±
0.07
±
1.22
±
0.71
±
0.95
±
4, 000]
|
Accuracy
57.91
–
0.22
1.05
5.00
0.27
0.13
0.17

±
±
±
±
±
±
4, 000]
Accuracy
–
–
–
0.06
0.21
0.20
0.00
0.00

±
±
±
±
±

|

54.10
52.14
46.62
54.65
55.49
55.60

80.27
80.37
74.83
80.72
80.72

Dataset [δ

S

B

59

400

|

|

|

|

|
|
Algorithm
LIBSVM
LLSVM
BSGD-M
BSGD-R
FOGD
NOGD
AVM-Hinge
AVM-Logit

|
|
Algorithm
LIBSVM
LLSVM
BSGD-M
BSGD-R
FOGD
NOGD
AVM-Hinge
AVM-Logit

|
|
Algorithm
LIBSVM
LLSVM
BSGD-M
BSGD-R
FOGD
NOGD
AVM-Hinge
AVM-Logit

Train
114.90
20.17
90.62
19.31
7.62
9.81
6.52
7.03

|
Test
85.34
19.38
5.66
5.48
11.95
3.24
2.69
2.86

D]

covtype [3.0
|
Train
Test
–
–
–
–
2,413.15
3.75
418.68
3.02
69.94
2.45
679.50
0.76
60.27
0.26
0.22
61.92
D] KDDCup99 [3.0
Test
Train
661.04
4,380.58
–
–
21.25
2,680.58
14.33
1,644.25
22.73
706.20
3.11
3,726.21
2.75
554.42
2.80
576.76

Dataset [δ

S

B

115

|

388

1, 000

|

Le et al

In terms of predictive performance, our proposed methods outperform the recent ad-
vanced online learning algorithms – FOGD and NOGD in most scenarios. The AVM-based
models are able to achieve slightly less accurate but fairly comparable results compared
with those of the state-of-the-art LIBSVM algorithm. In terms of sparsity and speed, the
AVMs are the fastest ones in the training and testing phases in all cases thanks to their
remarkable smaller model sizes. The diﬀerence between the training speed of our AVMs
and that of two approaches varies across datasets. The gap is more signiﬁcant for datasets
with higher dimensional feature spaces. This is expected because the procedure to compute
random features for each data point of FOGD involves sin and cos operators which are
costly. These facts indicate that our proposed online kernel learning algorithms are both
eﬃcient and eﬀective in solving large-scale kernel classiﬁcation problems. Thus we believe
that the AVM is the fast alternative to the existing SVM solvers for large-scale classiﬁcation
tasks.

Finally, comparing two versions of AVMs, it can be seen that the discriminative perfor-
mances of AVM with Logistic loss are better than those of AVM with Hinge loss in most
of datasets. This is because the Logistic function is smoother than the Hinge function,
whilst the Hinge loss encourages sparsity of the model. The AVM-Logit, however, contains
additional exponential operators, resulting in worse training time.

9.4 Online classiﬁcation

The next experiment investigates the performance of the AVMs in online classiﬁcation task
where individual data point continuously come turn-by-turn in a stream. Here we also
use eight datasets and two versions of our approach: AVM with Hinge loss (AVM-Hinge)
and AVM with Logistic loss (AVM-Logit) which are used in batch classiﬁcation setting (cf.
Section 9.3).

Baselines. We recruit the two widely-used algorithms – Perceptron and OGD for regular
online kernel classiﬁcation without budget maintenance and 8 state-of-the-art budget online
kernel learning methods as follows:

Perceptron:
(Freund and Schapire, 1999);

the kernelized variant without budget of Perceptron algorithm

OGD: the kernelized variant without budget of online gradient descent (Kivinen et al.,
2004).

RBP: a budgeted Perceptron algorithm using random support vector removal strategy
(Cavallanti et al., 2007);

Forgetron: a kernel-based Perceptron maintaining a ﬁxed budget by discarding oldest
support vectors (Dekel et al., 2005);

Projectron: a Projectron algorithm using the projection strategy (Orabona et al.,
2009);

Projectron++: the aggressive version of Projectron algorithm (Orabona et al., 2009);

•

•

•

•

•

•

26

Approximation Vector Machines

BPAS: a budgeted variant of Passive-Aggressive algorithm with simple SV removal
strategy (Wang and Vucetic, 2010);

BOGD: a budgeted variant of online gradient descent algorithm using simple SV
removal strategy (Zhao et al., 2012);

FOGD and NOGD: described in Section 9.3.

•

•

•

Hyperparameters setting. For each method learning on each dataset, we follow the
same hyperparameter setting which is optimized in the batch classiﬁcation task. For time
eﬃciency, we only include the fast algorithms FOGD, NOGD and AVMs for the experiments
on large-scale datasets. The other methods would exceed the time limit when running on
such data.

Results. Fig. 4 and Fig. 5 shows the relative performance convergence w.r.t classiﬁcation
error and computation cost of the AVMs in comparison with those of the baselines. Com-
bining these two ﬁgures, we compare the average mistake rate and running time in Fig. 6.
Table 4 reports the ﬁnal average results in detailed numbers after the methods see all data
samples. It is worthy to note that for the four biggest datasets (KDDCup99, covtype, poker,
airlines) that consist of millions data points, we exclude the non-budgeted online learning
algorithm because of their substantially expensive time costs. From these results, we can
draw some observations as follows.

First of all, as can be seen from Fig. 4, there are three groups of algorithms that have
diﬀerent learning progresses in terms of classiﬁcation mistake rate. The ﬁrst group includes
the BOGD, Projectron and Forgetron that have the error rates ﬂuctuating at the beginning,
but then being stable till the end. In the meantime, the rates of the models in the second
group, including Perceptron, OGD, RBP, Projectron++ and BPAS, quickly saturate at a
plateau after these methods see a few portions, i.e., one-tenth to two-tenth, of the data. By
contrast, the last group includes the recent online learning approaches – FOGD, NOGD, and
our proposed ones – AVM-Hinge, AVM-Logit, that regularly perform better as more data
points come. Exceptionally, for the dataset w8a, the classiﬁcation errors of the methods in
the ﬁrst group keep increasing after seeing four-tenth of the data, whilst those of the last
group are unexpectedly worse.

Second, Fig. 6 plots average mistake rate against computational cost, which shows sim-
ilar patterns as in the our ﬁrst observation. In addition, it can be seen from Fig. 5 that
all algorithms have normal learning pace in which the execution time is accumulated over
the learning procedure. Only the Projectron++ is slow at the beginning but then performs
faster after receiving more data.

According to ﬁnal results summarized in Table 4, the budgeted online approaches show
eﬃcacies with substantially faster computation than the ones without budgets. This is
more obvious for larger datasets wherein the execution time costs of our proposed models
are several orders of magnitude lower than those of regular online algorithms. This is
because the coverage scheme of AVMs impressively boost their model sparsities, e.g., using
δ = 3 resulting in 115 core points for dataset KDDCup99 consisting of 4, 408, 589 instances,
and using δ = 1 resulting in 388 core points for dataset airlines containing 5, 336, 471 data
samples.

27

0.24

0.23

0.22

0.21

0.20

0.19

0.18

0.17

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.16
0

0.24

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.23

0.22

0.21

0.20

0.19

0.18

0.17

0.40

0.35

0.30

0.25

0.20

0.15

0.10

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.05

0. 0

0.0060

0.0055

0.0050

0.0045

0.0040

0.0035

0.0030

0.0025

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.54

0.52

0.50

0.48

0.46

0.44

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

Perceptron
OGD

RBP
Forgetron

Projectron
Projectron++

FOGD
NOGD

AVM-hinge
AVM-logit

1

2

4

Number of samples

5
104

×

BPAS
BOGD

3

0.16

0

1

2

3

Number of samples

4

5
104

×

1

2

3

4

5

6

Number of samples

7
104

×

(a) a9a

(b) w8a

0. 5

1. 0

1. 5

2. 0

2. 5

3. 0

0. 2

0. 4

0. 6

0. 8

1. 0

1. 2

Number of samples

(c) cod-rna

3. 5

105

×

0.08

0. 0

Number of samples

(d) ijcnn1

1. 4

105

×

0.0020

0

1

2

3

Number of samples

4

5
106

×

0.34

0

1

2

3
Number of samples

4

5

6
105

×

(e) KDDCup99

(f) covtype

0.42

0. 0

0. 2

0. 4

0. 6
Number of samples

0. 8

1. 0

1. 2

106

×

0.19

0

1

2

3
Number of samples

4

5

6
106

×

(g) poker

(h) airlines

Figure 4: Convergence evaluation of online classiﬁcation tasks: the average rate of mistakes
as a function of the number of samples seen by the models. (Best viewed in colors).

Le et al

0.060

0.055

0.050

0.045

0.040

0.035

0.030

0.025

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.020

0

0.17

0.16

0.15

0.14

0.13

0.12

0.11

0.10

0.09

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.46

0.44

0.42

0.40

0.38

0.36

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.26

0.25

0.24

0.23

0.22

0.21

0.20

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

28

Approximation Vector Machines

Perceptron
OGD

RBP
Forgetron

Projectron
Projectron++

FOGD
NOGD

AVM-hinge
AVM-logit

1

2

4

Number of samples

5
104

×

BPAS
BOGD

3

0.5

0

1

2

3

Number of samples

4

5
104

×

1

2

3

4

5

6

Number of samples

7
104

×

(a) a9a

(b) w8a

0.5

0. 0

0. 5

1. 0

1. 5

2. 0

2. 5

3. 0

0. 2

0. 4

0. 6

0. 8

1. 0

1. 2

3. 5

105

×

0.0

0. 0

Number of samples

(d) ijcnn1

1. 4

105

×

Number of samples

(c) cod-rna

1.5

0

1

2

3

Number of samples

4

5
106

×

2

3
Number of samples

4

5

6
105

×

0.5

0

1

(e) KDDCup99

(f) covtype

0.24

0.23

0.22

0.21

0.20

0.19

0.18

0.17

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.16
0

3.5

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

3.0

2.5

2.0

1.5

1.0

0.5

0.0

3.5

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

3.0

2.5

2.0

1.5

1.0

4.0

3.5

3.0

2.5

2.0

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

4.0

3.5

3.0

2.5

2.0

1.5

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

1.0

0. 0

0. 2

0. 4

0. 6
Number of samples

0. 8

1. 0

1. 2

106

×

0

1

2

3
Number of samples

4

5

6
106

×

(g) poker

(h) airlines

Figure 5: Convergence evaluation of online classiﬁcation task: the average time costs (sec-
onds shown in the logarithm with base 10) as a function of the number of samples seen by
the models. (Best viewed in colors).

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

3.5

3.0

2.5

2.0

1.5

1.0

0.5

0.0

0.5

0

3.0

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

2.5

2.0

1.5

1.0

0.5

3.0

2.5

2.0

1.5

1.0

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

3.5

3.0

2.5

2.0

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

29

0.24

0.23

0.22

0.21

0.20

0.19

0.18

0.17

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.16
0

0.24

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.23

0.22

0.21

0.20

0.19

0.18

0.17

0.16

0.40

0.35

0.30

0.25

0.20

0.15

0.10

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.0060

0.0055

0.0050

0.0045

0.0040

0.0035

0.0030

0.0025

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.0020

1.5

0.54

0.52

0.50

0.48

0.46

0.44

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

Perceptron
OGD

RBP
Forgetron

Projectron
Projectron++

FOGD
NOGD

AVM-hinge
AVM-logit

1

2

4

Number of samples

5
104

×

BPAS
BOGD

3

0.5

0.0

0.5

2.5

3.0

3.5

0.5

0.0

0.5

2.5

3.0

3.5

1.0

1.5
Average time cost (log10t)

2.0

(a) a9a

1.0

1.5
Average time cost (log10t)

2.0

(b) w8a

0.05

0.5

1.0

3.0

3.5

0.08

0.0

0.5

1.5
2.5
2.0
Average time cost (log10t)

(c) cod-rna

1.0
2.0
1.5
Average time cost (log10t)

2.5

3.0

(d) ijcnn1

2.0

2.5

3.0

3.5

4.0

1.0

1.5

2.0

2.5

3.0

0.34

0.5

Average time cost (log10t)

(e) KDDCup99

Average time cost (log10t)

(f) covtype

0.42

1.0

1.5

2.0
3.0
2.5
Average time cost (log10t)

(g) poker

3.5

4.0

2.0

2.5

3.0
Average time cost (log10t)

3.5

(h) airlines

Figure 6: Average mistake rate vs. time cost for online classiﬁcation. The average time
(seconds) is shown in the logarithm with base 10. (Best viewed in colors).

Le et al

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.060

0.055

0.050

0.045

0.040

0.035

0.030

0.025

0.020

0.17

0.16

0.15

0.14

0.13

0.12

0.11

0.10

0.09

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.46

0.44

0.42

0.40

0.38

0.36

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

s
e
k
a
t
s
i
m

 
f
o
 
e
t
a
r
 
e
g
a
r
e
v
A

0.26

0.25

0.24

0.23

0.22

0.21

0.20

0.19

30

Approximation Vector Machines

Table 4: Classiﬁcation performance of our proposed methods and the baselines in online
mode. Note that δ, B and D are set to be the same as in batch classiﬁcation tasks (cf.,
Section 9.3). The mistake rate is reported in percent (%) and the execution time is in
second. The best performance is in bold.

a9a [142]

w8a [131]

9.79
7.81
26.02
28.56
11.16

±
±
±
±
±
±
±
±
±
±
±
±

±
±
±
±
±
±
±
±
±
±
±
±

3.51
2.54
4.02
3.96
4.76
3.08
2.37
3.16
3.52
2.55
4.62
5.80

21.05
16.50
23.76
23.15
21.86
19.47
19.09
22.14
20.11
16.55
17.46
17.33

Dataset [S]
Algorithm Mistake Rate
Time Mistake Rate
0.03
0.12
Perceptron
976.79
0.06 2,539.46
0.03
OGD
0.07
118.25
0.21
RBP
0.10
109.71
0.34
Forgetron
1.13
122.08
1.73
Projectron
0.63
449.20
2.22
Projectron++
0.02
95.81
0.17
BPAS
0.08
96.11
0.25
BOGD
0.05
13.79
0.10
FOGD
0.05
99.54
0.07
NOGD
8.74
0.78
0.12
AVM-Hinge
0.02
9.31
0.16
AVM-Logit
Dataset [S]
ijcnn1 [500]
cod-rna [436]
Algorithm Mistake Rate
0.04
Perceptron
±
0.03
OGD
±
0.39
RBP
±
2.22
Forgetron
±
3.61
Projectron
±
15.60
Projectron++
±
11.97
0.09
BPAS
38.13
0.11
BOGD
7.15
0.03
FOGD
7.83
0.06
NOGD
5.61
0.17
AVM-Hinge
0.20
6.01
AVM-Logit
Dataset [S]
KDDCup99 [115]
Algorithm
FOGD
NOGD
AVM-Hinge
AVM-Logit
Dataset [S]
Algorithm Mistake Rate
0.04
0.16
0.09
0.07

Time Mistake Rate
0.09
0.06
0.21
0.26
0.23
0.09
0.05
0.18
0.03
0.08
0.18
0.20
covtype [59]

Time Mistake rate
Mistake rate
0.05
620.95
0.00
0.07
0.00 4,009.03
0.16
540.65
0.07
503.34
0.16
0.03
airlines [388]
poker [393]

Time Mistake Rate
0.01
928.89
0.01
4,920.33
0.00
122.59
0.00
124.86

1,393.56
2,804.01
85.84
102.64
97.38
1,799.93
92.08
104.60
53.45
105.18
40.89
45.67

12.85
10.39
15.54
16.17
12.98
9.97
10.68
10.87
9.41
10.43
8.01
8.07

FOGD
NOGD
AVM-Hinge
AVM-Logit

±
±
±
±
±
±
±
±
±
±
±
±

40.45
34.72
36.11
35.92

52.28
44.90
43.85
43.97

20.98
25.56
19.28
19.28

0.35
0.23
0.31
0.28

±
±
±
±
±
±

±
±
±
±

±
±
±
±

17.97

±
±
±
±

±
±
±
±

Time
691.80
1,290.13
544.83
557.75
572.20
1321.93
681.46
589.47
26.40
585.23
16.89
17.86

Time
727.90
960.44
54.29
60.54
59.37
749.70
55.44
55.99
25.93
59.36
23.26
23.36

Time
223.20
838.47
51.12
53.51

Time
1,270.75
3,553.50
733.72
766.19

31

Le et al

For classiﬁcation capability, the non-budgeted methods only surpass the budgeted ones
for the smallest dataset, that is, the OGD obtains the best performance for a9a data. This
again demonstrates the importance of exploring budget online kernel learning algorithms.
Between the two non-budgeted algorithms, the OGD achieves considerably better error
rates than the Perceptron. The method, however, must perform much more expensive
updates, resulting in a signiﬁcantly larger number of support vectors and signiﬁcantly higher
computational time costs. This represents the trade-oﬀ between classiﬁcation accuracy and
computational complexity of the OGD.

Furthermore, comparing the performance of diﬀerent existing budgeted online kernel
learning algorithms, the AVM-Hinge and AVM-Logit outperform others in both discrimi-
native performance and computation eﬃciency for almost all datasets. In particular, the
AVM-based methods achieve the best mistake rates – 5.61
0.09,
19.28
0.00 for the cod-rna, ijcnn1, poker and airlines data, that are, respectively, 27.5%,
17.5%, 2.4%, 8.8% lower than the error rates of the second best models – two recent ap-
proaches FOGD and NOGD. On the other hand, the computation costs of the AVMs are
signiﬁcantly lower with large margins of hundreds of percents for large-scale databases cov-
type, poker, and airlines as shown in Table 4.

0.18, 43.85

0.17, 8.01

±

±

±

±

In all experiments, our proposed method produces the model sizes that are much smaller
than the budget sizes of baseline methods. Thus we further investigate the performance
of the budgeted baselines by varying the budget size B, and compare with our AVM with
It can be seen
Hinge loss. Fig. 7 shows our analysis on two datasets a9a and cod-rna.
that the larger B helps model obtain better classiﬁcation results, but hurts their running
speed. For both datasets, the budgeted baselines with larger budget sizes still fail to beat
the predictive performance of AVM. On the other hand, the baselines with smaller budget
sizes run faster than the AVM on cod-rna dataset, but slower on a9a dataset.

B = 50
B = 100
B = 150
B = 250
B = 436
B = 1000

RBP
Forgetron
Projectron
BPAS
BOGD

)

%

(
 
e
t
a
r
 
e
k
a
t
s
i
M

32

30

28

26

24

22

20

18

)
s
4
7
.
8
 
,
2
4
1
=
B
(
 

M
V
A

B = 50
B = 100
B = 142
B = 250
B = 500
B = 1000

RBP
Forgetron
Projectron
BPAS
BOGD

40

35

30

25

20

15

10

)

%

(
 
e
t
a
r
 
e
k
a
t
s
i
M

5
30

)
s
9
8
.
0
4
 
,
6
3
4
=
B
(
 

M
V
A

0

20

40

100

120

140

40

50

60

90

100

110

120

AVM (B=142, 17.46%)

60

80
Time (s)

(a) a9a

AVM (B=436, 5.61%)

70
80
Time (s)

(b) cod-rna

Figure 7: Predictive and wall-clock performance on two datasets: a9a and cod-rna of
budgeted methods when the budget size B is varied. (Best viewed in colors).

Finally, two versions of AVMs demonstrate similar discriminative performances and
computational complexities wherein the AVM-Logit is slightly slower due to the additional
exponential operators as also seen in batch classiﬁcation task. All aforementioned observa-
tions validate the eﬀectiveness and eﬃciency of our proposed technique. Thus, we believe

32

Approximation Vector Machines

that our approximation machine is a promising technique for building scalable online kernel
learning algorithms for large-scale classiﬁcation tasks.

9.5 Online regression

The last experiment addresses the online regression problem to evaluate the capabilities of
our approach with three proposed loss functions – ℓ1,ℓ2 and ε-insensitive losses as described
in Section 7. Incorporating these loss functions creates three versions: AVM-ε, AVM-ℓ1 and
AVM-ℓ2. We use four datasets: casp, slice, year and airlines (delay minutes) with a wide
range of sizes for this task. We recruit six baselines: RBP, Forgetron, Projectron, BOGD,
FOGD and NOGD (cf. more detailed description in Section 9.4).

Hyperparameters setting. We adopt the same hyperparameter searching procedure
for batch classiﬁcation task as in Section 9.3. Furthermore, for the budget size B and
the feature dimension D in FOGD, we follow the same strategy used in Section 7.1.1 of
(Lu et al., 2015). More speciﬁcally, these hyperparameters are separately set for diﬀerent
datasets as reported in Table 5. They are chosen such that they are roughly proportional to
the number of support vectors produced by the batch SVM algorithm in LIBSVM running
on a small subset. The aim is to achieve competitive accuracy using a relatively larger
budget size for tackling more challenging regression tasks.

Results. Fig. 8a and Fig. 8b shows the relative performance convergence w.r.t regression
error (root mean square root - RMSE) and computation cost (seconds) of the AVMs in
comparison with those of the baselines. Combining these two ﬁgures, we compare the
average error and running time in Fig. 9. Table 5 reports the ﬁnal average results in
detailed numbers after the methods traverse all data samples. From these results, we can
draw some observations as follows.

First of all, as can be seen from Fig. 8a, there are several diﬀerent learning behaviors
w.r.t regression loss, of the methods training on individual datasets. All algorithms, in
general, reach their regression error plateaus very quickly as observed in the datasets year
and airlines where they converge at certain points from the initiation of the learning. On
the other hand, for casp and slice databases, the AVM-based models regularly obtain better
performance, that is, their average RMSE scores keep reducing when receiving more data,
except in slice data, the regression performance of AVM-ℓ2 are almost unchanged during
the learning. Note that, for these two datasets, the learning curve of AVM-ε coincides, thus
is overplotted by that of AVM-ℓ1, resulting in its no-show in the ﬁgure. Interestingly, the
errors of RBP and Forgetron slightly increase throughout their online learning in these two
cases.

Second, Fig. 9 plots average error against computational cost, which shows similar learn-
ing behaviors as in the our ﬁrst observation. The computational cost progresses are simple
and more obvious to comprehend than the regression progresses. As illustrated in Fig. 8b,
all algorithms have nicely plausible execution time curves in which the time is accumulated
over the learning procedure.

33

RBP

Forgetron

Projectron

BOGD

FOGD

NOGD

AVM-eps

AVM-L1

AVM-L2

1

1

2

3

2
4

Number of samples

5
Number of samples
104

×

0.05

3

0

1

4

2

3
Number of samples

4

5
104

5
×

6
104

×

casp

slice

1. 0

1. 5

2. 0

2. 5

3. 0

3. 5

4. 0

4. 5

5. 0

Number of samples

105

×

2

3
Number of samples

4

5

6
106

×

30

0

1

year

airlines

(a) The average RMSE as a function of the number of samples seen by the models.

1

2

3

Number of samples

4

5
104

×

1

2

3

Number of samples

4

5
104

×

casp

slice

0.40

0.40

E
S
M
R
 
e
g
a
r
e
v
A

0.35
E
S
M
R
 
e
g
a
r
0.30
e
v
A

0.35

0.30

0.25

0.25
0

0

0.26

0.24

E
S
M
R
 
e
g
a
r
e
v
A

0.22

0.20

0.18

0.16

0.14

0.12

0. 5

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

1.0

0.5

0.0

0.5

0

3.0

2.5

2.0

1.5

1.0

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

0.5

0. 5

1. 0

1. 5

2. 0

2. 5

3. 0

3. 5

4. 0

4. 5

5. 0

Number of samples

105

×

2

3
Number of samples

4

5

6
106

×

1.5

0

1

year

airlines

(b) The average time costs (seconds in logarithm of 10) as a function of the number of samples seen by
the models.

Figure 8: Convergence evaluation of online regresion. (Best viewed in colors).

Le et al

0.30

0.25

0.20

0.15

0.10

E
S
M
R
 
e
g
a
r
e
v
A

55

50

45

40

35

E
S
M
R
 
e
g
a
r
e
v
A

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

1.0

0.5

0.0

0.5

0

4.0

3.5

3.0

2.5

2.0

)
t
0
1
g
o
l
(
 
t
s
o
c
 
e
m

i
t
 
e
g
a
r
e
v
A

34

Approximation Vector Machines

Table 5: Online regression results of 6 baselines and 3 versions of our AVMs. The notation
[δ; S; B; D] denotes the same meanings as those in Table 3. The regression loss is measured
using root mean squared error (RMSE) and the execution time is reported in second. The
best performance is in bold.
Dataset

|

|

S

B

D]
[δ
|
Algorithm
RBP
Forgetron
Projectron
BOGD
FOGD
NOGD
AVM-ε
AVM-ℓ1
AVM-ℓ2
Dataset

D]

|

|

S

B

[δ
|
Algorithm
RBP
Forgetron
Projectron
BOGD
FOGD
NOGD
AVM-ε
AVM-ℓ1
AVM-ℓ2

|

|

166

[4.0

casp
400
|
RMSE
0.0012
0.3195
±
0.0008
0.3174
±
0.0002
0.2688
±
0.0002
0.2858
±
0.0014
0.3775
±
0.0001
0.2512
±
0.0329
0.3165
±
0.0330
0.3166
±
0.0280
0.3274
±
year
400
|
RMSE
0.0002
0.1881
±
0.0004
0.1877
±
0.0003
0.1390
±
0.0000
0.2009
±
0.0002
0.1581
±
0.0005
0.1375
±
0.0002
0.1286
±
0.0003
0.1232
±
0.0001
0.2420
±

[60.0

67

|

|

2, 000]

|

27

[16.0

Time
7.15
10.14
8.48
6.20
5.83
6.99
3.53
3.44
3.31

slice
1, 000
|
RMSE
0.0006
0.1154
±
0.0004
0.1131
±
0.0002
0.0770
±
0.0001
0.1723
±
0.0009
0.1440
±
0.0002
0.0873
±
0.0137
0.2013
±
0.0138
0.2013
±
0.0002
0.2590
±
airlines
1, 000
|
RMSE
Time
0.0010
36.5068
605.42
±
0.0003
36.5065
904.09
±
0.0009
36.1365
605.19
±
0.0010
35.7346
596.10
±
0.0120
53.1638
76.70
±
0.0013
607.37 34.7421
±
0.0914
36.0901
48.01
±
0.0192
36.3632
47.29
±
46.63
0.0192
35.1128
±

[1.0

388

|

1, 600]

|

|

3, 000]

Time
810.14
1,069.15
814.37
816.16
20.65
812.69
7.07
7.13
6.88

2, 000]

Time
3,418.89
5,774.47
3,834.19
3,058.96
646.15
3,324.38
638.60
621.57
633.27

According to ﬁnal results summarized in Table 5, our proposed models enjoy a signiﬁcant
advantage in computational eﬃcacy whilst achieve better (for year dataset) or competitive
regression results with other methods. The AVM, again, secures the best performance in
terms of model sparsity. Among the baselines, the FOGD is the fastest, that is, its time costs
can be considered to compare with those of our methods, but its regression performances
are worse. The remaining algorithms usually obtain better results, but is traded oﬀ by
the sacriﬁce of scalability. This, once again, veriﬁes the eﬀectiveness and eﬃciency of our
proposed techniques. We believe that the AVM is a promising machine to perform online
regression task for large-scale datasets.

Finally, comparing the capability of three AVM’s variants, all models demonstrate sim-
ilar computational complexities wherein the AVM-ℓ2 is slightly faster due to its simpler
operator in computing the gradient as derived in Section 7. However, its regression errors
are higher than two other methods – AVM-ε and AVM-ℓ1.

35

RBP

Forgetron

Projectron

BOGD

FOGD

NOGD

AVM-eps

AVM-L1

AVM-L2

1
0.0
Average time cost (log10t)

0.5

2

1.0
Number of samples

0.05

3
0.5

0.0

0.5

4

1.0

1.5
Average time cost (log10t)

2.0

2.5

5
3.0
104

×

3.5

casp

slice

0.40

0.40

E
S
M
R
 
e
g
a
r
e
v
A

0.35
E
S
M
R
 
e
g
a
r
0.30
e
v
A

0.35

0.30

0.25

0.25

0

0.5

0.26

0.24

0.22

0.20

0.18

0.16

0.14

E
S
M
R
 
e
g
a
r
e
v
A

0.12

0.5

1.0

1.5

2.0

2.5

3.0

2.0

2.5

3.0

3.5

4.0

Average time cost (log10t)

year

Average time cost (log10t)

airlines

Figure 9: Average RMSE vs. time cost for online regression. The average time (seconds) is
shown in the logarithm with base 10. (Best viewed in colors).

10. Conclusion

In this paper, we have proposed Approximation Vector Machine (AVM) for large-scale
online learning. The AVM is theoretically proven to have bounded and sparse model size
while not hurting the predictive performance. We have validated our proposed method
on several benchmark datasets. The experimental results show that the proposed AVM
obtains a comparable predictive performance while simultaneously achieving an impressive
model size and a computational speed-up compared with those of the baselines. Our future
works are to apply AVM to the context of semi-supervised learning, anomaly detection, and
support vector clustering.

Acknowledgment

We gratefully thank the editor and the anonymous reviewers for their valuable comments
and thorough inspection of the article. This work was partially supported by the Australian
Research Council (ARC) under the Discovery Project DP160109394.

Le et al

0.30

0.25

0.20

0.15

0.10

E
S
M
R
 
e
g
a
r
e
v
A

55

50

45

40

35

E
S
M
R
 
e
g
a
r
e
v
A

30

1.5

36

Approximation Vector Machines

Appendix A. Proofs Regarding δ-Coverage

Proof of Theorem 4
x

Assume that

x

′

δ, then we have

−

2

≤

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
′
x

Φ (x)

Φ

−

(cid:13)
(cid:13)
(cid:13)

= K (x, x) + K

′

′

x

, x

2K

′

x, x

−

(cid:17)

(cid:16)

(cid:17)

= 2

1

k

−

(cid:18)

(cid:16)

(cid:17)(cid:13)
(cid:13)
(cid:13)
≤

2

1

k

δ2

−

(cid:16)
= δ2
Φ

′

x

x

−

2

(cid:13)
(cid:13)
(cid:13)

(cid:19)(cid:19)

(cid:18)(cid:13)
(cid:13)
(cid:13)

(cid:0)
Furthermore, we have

(cid:0)

(cid:1)(cid:1)

lim
0
δ
→

δΦ = 21/2 lim
0
→

δ

k

δ2

1

−

(cid:0)

(cid:0)

(cid:1)(cid:1)

1/2

= 21/2 (1

k (0))1/2 = 0

−

Finally, since Gaussian kernel is a special radial kernel with k (t) = exp (

γt), we

−

obtain the ﬁnal conclusion.

Proof of Theorem 19

Since the proof is similar for the hyperrectangle cell case, we present the proof for
=
. From the
z
we must be able to extract a
. From the construction of

the hypersphere case. Let us consider the open coverage
compactness of the data domain
, it apparent that from
ﬁnite subcoverage of size m, that is,
the coverage

U
U
m
i=1 ⊂ U

Um =

zi, δ
2

z, δ
2

(cid:1)(cid:9)

∈X

X

B

B

(cid:8)

(cid:0)

in Algorithm 3, we know that
(cid:8)

(cid:0)

P

(cid:1)(cid:9)

ci −
k

cjk

> δ/2 if i

= j

Hence, each open sphere in the ﬁnite coverage

It means that the cardinality of

Um is able to contain at most one core
must be less than or equal m, that is,

P

point of
m.

|P| ≤

.

P

37

Le et al

Appendix B. Proofs Regarding Convergence Analysis

= (Pi)i

Given a ﬁnite δ-coverage
I , when receiving an
P
∈
incoming instance (xt, yt) we approximate (xt, yt) by (cit, yt) with cit is a core point whose
Pit. We use a Bernoulli random variable Zt to control if the
cell contains xt, that is, xt ∈
approximation is performed or not, that is, Zt = 1 indicates the approximation is performed.
Let us deﬁne gt = λwt + l

(wt; xt, yt) = λwt + αtΦ (xt). We have the following

I with the core set
∈

= (ci)i

C

′

ht = gt + Zt∆t

where ∆t = αt (Φ (cit)

Φ (xt)).

The update rule becomes

−

where S = RD (i.e., the feature space) or

B
Lemma 21. The following statements hold

(cid:0)

wt+1 =

(wt −

ηtht)

YS

0, ymaxλ−

1/2

.

(cid:1)

i) There exist two positive constants P and M such that E

for all t.
ii) E

iii) E

iv) E

Proof

L =

A√P + B

for all t.

2

2

≤

′

l

(wt; xt, yt)
(cid:13)
(cid:13)
G =
(cid:13)

2

2

2

i

≤

(cid:21)
(cid:16)
λP + A√P + B

(cid:20)(cid:13)
(cid:13)
gtk
(cid:13)
k
h
htk
k
h
i) We prove by induction that E
2

√G + δΦ
(cid:16)

(cid:17)
for all t.
2

(cid:17)
A√P + B

H =

(cid:17)(cid:17)

≤

(cid:16)

(cid:16)

i

for all t.

(δΦ+1)A+√(δΦ+1)2A2+4Bλ(δΦ+1)
2λ

(cid:18)
using Minkowski inequality, we have

(cid:19)

2

wtk

k
h

≤

i

P 2 and E

α2
t

M

≤

(cid:2)

(cid:3)

2

wtk
k

h

i

≤

P 2 where P

=

for all t. Assume that the claim is holding for t,

E

r

k
h

2

wt+1k

i

E

≤ v
u
u
u
t
t
−
t

≤




1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

YS

E

2

wtk
k

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
+ ηt


E

(wt −

ηtht)



≤

r

E

k
h

wt −

ηthtk

2

i

r

p

h
E [
wtk
k
λt

δΦ

E

+

r

A√P + B

t

1

−
t

≤

E

2

wtk
k

A

+

i

i

r

r

h

h

+

+

(t

1) P
−
t

(t

1) P
−
t

≤

≤

A√P + B
λt
(δΦ + 1)

(cid:16)
λt

38

l′ (wt; xt, yt)
k
k

2

+ ηt

] + B

+

i
δΦ

E

r
α2
t

q

λt

(cid:2)

(cid:3)

E

2

∆tk
k

h

i

l′ (wt; xt, yt)
k

2
k

i

h

(cid:17)

λt

= P

Approximation Vector Machines

Note that we have used

P , and u = P 1/2 = (δΦ+1)A+√(δΦ+1)2A2+4Bλ(δΦ+1)

2

= α2

′

l

(cid:13)
(cid:13)
(cid:13)

(wt; xt, yt)
(cid:13)
(cid:13)
(cid:13)

2λ

t K (xt, xt) = α2

t , E [
wtk
k
is the solution of the quadratic equation

wtk
k

r

≤

E

≤

h

i

]

2

u2

−

(δΦ + 1) Au
λ

−

(δΦ + 1) B
λ

= 0

The proof of E

insensitive
losses. In these cases, we simply choose M = max (ymax, 1)2. We only need to consider the
ℓ2-loss case. In particular, we have

M is trivial for the case of Hinge, ℓ1, Logistic, ε

−

≤

(cid:2)

(cid:3)

α2
t

α2

t =

t Φ (xt)

yt

2

2

wT

t Φ (xt)

2

+ y2

max

2

wtk
k

k

≤

−

≤
(cid:17)
2 + y2
Φ (xt)
k

(cid:18)(cid:16)
max

wT
(cid:16)
2

(cid:16)

(cid:19)

2 + y2

max

(cid:17)
wtk

k

(cid:17)

2

≤

(cid:17)

(cid:16)

ii) We have the following

(cid:2)

(cid:3)

(cid:0)

(cid:1)

E

α2
t

2

P 2 + y2

max

= M

≤

E

l′ (wt; xt, yt)
k
k

2

r

h

E

A

wtk

k

≤ s

i

(cid:20)(cid:16)

2

1/2 + B

A

≤

E [
k

wtk

] + B

A√P + B

≤

(cid:21)

(cid:17)

p

Note that we have used the inequality E [
k

wtk
]

≤

iii) Using Minkowski inequality, we yield

E

r

k
h

2

wtk

≤

i

P .

2

E

gtk
iv) We have the following

r

r

≤

λ

k

i

h

h

E

2

wtk

k

i

r

k
h

+

E

l′ (wt; xt, yt)

λP + A√P + B

2
k

≤

i

E

r

k
h

2

htk

≤ r

i

E

2

gtk
k

+ δΦ

E

α2
t

h
√G + δΦ

i
E

≤

q
(cid:2)
2
l′ (wt; xt, yt)
k

(cid:3)

r

k
h

= √G + δΦ

A√P + B

i

(cid:16)

(cid:17)

Lemma 22. There exists a positive constant W such that E

Proof We ﬁrst remind the deﬁnitions of the relevant quantities

2

wt −

w∗k

k
h

≤

i

W for all t.

′

gt = λwt + l
ht = gt + Zt∆t where ∆t = αt (Φ (cit)

(wt; xt, yt) = λwt + αtΦ (xt)

Φ (xt))

−

39

Le et al

We now prove by induction in t. We derive as follows

wt+1 −

k

w∗

2 =

k

(wt −
w∗

ηtht)

w∗

−

+ η2

htk

t k

2

k

wt −
wt −

2
k

w∗

ηtht −
w∗, gt + Zt∆ti

≤ k

2ηt h

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
−
(cid:13)

where S = RD or

0, ymaxλ−

Taking conditional expectation w.r.t wt, we gain

B

(cid:0)

=

(cid:13)
YS
(cid:13)
(cid:13)
wt −
(cid:13)
k
(cid:13)
1/2
.

(cid:1)

w∗

2
k

E

wt+1 −
k

w∗

2
k

h

E

≤
+ η2
t

i

E

≤

k
h

wt −
k
h
E
htk
w∗

k
h
wt −

2

i
2
k

i
2ηt

−

wt −

′

w∗, f

(wt)

D
2ηtλ

wt −
k

w∗

E
2 + η2
t
k

−

i
Here we note that we have used

following derivation

wt −

D

′

w∗, f

(wt)

λ

≥

k

E

−
E

2ηt h
htk

k
h
wt −

wt −

2

−

i
w∗k

w∗, Zt∆ti
wt −
2ηt h

w∗, Zt∆ti

2. It comes from the

f (w∗)

f (wt)

−

≥

D

′

f

(wt) , w∗

wt

+

−

E

λ
2 k

wt −

w∗

2
k

(wt) , wt −

w∗

′

f

D

f (wt)

f (w∗) +

E

≥

≥

−
wt −

λ

k

w∗

2
k

Taking expectation again, we gain

λ
2 k

wt −

w∗

′

f

2
k
≥
thanks to

D

(w∗) , wt −
f

′

E
w∗
(w∗) , wt −

w∗

+ λ

w∗

2

k

k

wt −
0

≥

E

D

E

wt+1 −
k

w∗

2
k

≤

h

i
M 1/2δφ+(M δ2

2λ

t

2

E

wt −
−
t
k
φ+2H)1/2

h

2

Choosing W =

i
, we gain if E

w∗

2
k

+

H
λ2t2 +

2W 1/2M 1/2δΦ
λt

E

w∗k
wt+1 −
k
of the equation

h

2

i

≤

(cid:18)
W . The reason is that W =

(cid:19)

wt −
k
h
φ+2H)1/2
M 1/2δφ+(M δ2

w∗k

2

2

W then

≤

i
is the solution

2λ

(cid:18)

(cid:19)

W =

W +

t

2

−
t

H
λ2t

+

2W 1/2M 1/2δΦ
λt

or 2W

2W 1/2M 1/2δΦ
λ

H
λ2 = 0

−

−

. Hence, if E

wt −

k

w∗k

2

≤

W , we arrive at

h

i
wt+1 −

E

k
h

w∗

2
k

≤

i

t

2

−
t

W +

H
λ2t

+

2W 1/2M 1/2δΦ
λt

= W

40

Approximation Vector Machines

We now show the proof of Theorem 5.

Proof of Theorem 5

We ﬁrst remind the deﬁnitions of the relevant quantities

′

gt = λwt + l
ht = gt + Zt∆t where ∆t = αt (Φ (cit)

(wt; xt, yt) = λwt + αtΦ (xt)

Φ (xt))

−

We then derive as follows

wt+1 −

k

w∗

2 =

k

(wt −
w∗

(cid:13)
YS
(cid:13)
(cid:13)
wt −
(cid:13)
k
(cid:13)

=

ηtht)

w∗

−

+ η2

htk

t k

2

k

wt −
wt −

2
k

w∗

ηtht −
w∗, gt + Zt∆ti

≤ k

2ηt h

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
−
(cid:13)

where S = RD or

0, ymaxλ−

1/2

.

B

(cid:0)

(cid:1)

wt −
h

w∗, gti ≤

wt −
k

w∗k

2

wt+1 −

w∗k

2

+

− k
2ηt

ηt
2 k

2

htk

wt −

w∗, Zt∆ti

− h

Taking conditional expectation w.r.t wt, we obtain

′

w∗, f

(wt)

wt −

D

E

ηt
2

h

E

E

≤

+

wt −
k

w∗k

2

2

htk
k

h

− h

i

E

−
i
2ηt
wt −

wt+1 −
k

w∗k

2

h
w∗, E [Zt∆t]
i

i

f (wt)

f (w∗) +

−

λ
2 k

wt −

w∗

2
k

E

ηt
2

h

E

≤

+

wt −
k

w∗k

2

2

htk

k
h

− h

i

E

−
i
2ηt
wt −

w∗k

wt+1 −

k
h
w∗, E [Zt∆t]
i

2

i

Taking expectation again, we achieve

E [f (wt)

f (w∗)]

−

λ
2
ηt
2

(t

E

≤

+

1) E

−

2

htk
k

h

i

w∗

2
wt −
k
E [
wt −
h

k
h
−

tE

λ
2
−
h
i
w∗, Zt∆ti
]

wt+1 −
k

w∗

2

k

i

i) If Zt is independent with wt, we derive as

41

Le et al

E [f (wt)

f (w∗)]

−

≤
+

λ
2
ηt
2
λ
2
≤
+ E

wt −
k
h
E [
−
h
wt −
k
w∗

2

1) E

2

htk
k
i
1) E

−

(t

E

h

(t

−
Z 2

t k

h
wt −
1) E

h

h
(t

1/2 E

−
Z 2
t

λ
2
≤
+ E

k
i
wt −
k
wt −
k
h
(cid:3)
wt −
≤
−
k
+ P (Zt = 1)1/2 E

1) E

(cid:2)
(t

λ
2

h

tE

w∗

λ
2
wt+1 −
2
−
k
k
i
h
w∗) , ∆ti
Zt (wt −
]
λ
tE
w∗
2
−
∆tk
k
λ
2

wt+1 −
k
h
1/2

h
2
k

i
tE

2
k

w∗

1/2

E

i

2

−
1/2

i

w∗

i
2
k
2
k
wt −
k

w∗

i
w∗

−
2

k

E

2

1/2

wt+1 −
k
h
∆tk
k
h
tE

i
wt+1 −
k
h
E
∆tk
k

2

λ
2
1/2

i

h

i

h

w∗

2

k

i

i

i

w∗

2

k

+

E

ηt
2

2

htk
k

w∗

2

k

+

E

ηt
2

2

htk
k

h

h

+

H

ηt
2

w∗

2

k

1/2

i

i

i

(5)

Taking sum over 1, 2, . . . , T and using the inequalities in Lemmas 21 and 22, we yield

E [f (wt)]

T f (w∗)

P (Zt = 1)1/2 E

wt −
k

w∗

2

k

T

Xt=1

−

−

H
2λ

≤

H
2λ

≤

T

T

Xt=1
T

Xt=1
T

1
t

+

1
t

+

Xt=1

Xt=1

T E [f (wT )

f (w∗)]

P (Zt = 1)1/2 E

wt −
k

w∗

2
k

1/2

E

1/2

(6)

2

∆tk
k

i

h

i

1/2

E

1/2

2

∆tk

k
h

i

i

h

h

E [f (wT )

f (w∗)]

−

H (log (T ) + 1)
2λT

+

δΦM 1/2W 1/2
T

≤

P (Zt = 1)1/2

T

t=1
X

ii) If Zt is independent with wt and (xt, yt), we derive as

E [f (wt)

f (w∗)]

−

≤
+

λ
2
ηt
2
λ
2
ηt
2
λ
2

−

−

(t

H

(t

1) E
wt −
k
h
E [Zt h
wt −
1) E
wt −

2

w∗

−
k
i
w∗, ∆ti
]
w∗

tE

λ
2

h

wt+1 −
k

w∗

2
k

i

tE

wt+1 −
k

w∗

2
k

−

−

H

≤
+

k
h
E [Zt] E [
wt −
h
wt −
w∗
wt −
k
Taking sum over 1, 2, . . . , T and using the inequalities in Lemmas 21 and 22, we yield

≤
−
h
+ P (Zt = 1) E

wt+1 −
k
h
∆tk

i
2
k

1) E

ηt
2

k
h

2
k

w∗

w∗

tE

1/2

H

+

(t

E

k

i

i

i

h

h

i

2

(7)

2

λ
2
−
k
i
w∗, ∆ti
]
λ
2
−
1/2

k

2

42

Approximation Vector Machines

E [f (wt)]

T f (w∗)

+

P (Zt = 1) E

T

Xt=1

−

−

H
2λ

≤

H
2λ

≤

T

T

1
t

1
t

Xt=1
T

Xt=1
T

Xt=1

Xt=1

T E [f (wT )

f (w∗)]

+

P (Zt = 1) E

wt −
k

w∗

2
k

1/2

E

1/2

(8)

h

k
h

wt −

w∗

2

k

1/2

E

2

∆tk

k
h

2

∆tk

k

h

i

i

1/2

E [f (wT )

f (w∗)]

−

H (log (T ) + 1)
2λT

+

δΦM 1/2W 1/2
T

≤

P (Zt = 1)

i

i

T

Xt=1

iii) In general, we derive as

E [f (wt)

f (w∗)]

−

H (log (T ) + 1)
2λT

≤

+ δΦM 1/2W 1/2

≤
+

λ
2
ηt
2
λ
2
ηt
2

−

−

(t

H

(t

1) E
wt −
k
h
E [Zt h
wt −
1) E
wt −
w∗

w∗

2
−
k
i
w∗, ∆ti
]
w∗

2
k
i
1/2

λ
2

λ
2

tE

wt+1 −
k

w∗

2

k

h

i

≤

k
h
wt −
k
Taking sum over 1, 2, . . . , T and using the inequalities in Lemmas 21 and 22, we yield

−
H + E

2
k

1/2

+

−

E

k

h

i

i

i

h

tE

wt+1 −
k
h
2
∆tk
k

2

w∗

(9)

E [f (wt)]

T f (w∗)

H
2λ

≤

T

T

1
t

+

E

wt −
k

w∗

2
k

T

t=1
X

t=1
X
T

H
2λ

≤

1
t

+

h

t=1
X
T

E

t=1
X

t=1
X

h

wt −

k

w∗

2

k

2

∆tk
k

i

h

i

1/2

E

2

∆tk

k
h

1/2

(10)

i

1/2

E

i

1/2

T E [f (wT )

f (w∗)]

−

−

E [f (wT )

f (w∗)]

−

H (log (T ) + 1)
2λT

≤

+ δΦM 1/2W 1/2

Proof of Theorem 15

Let us denote the model size, i.e., the number of vectors in support set, after the iteration
t by St. We also deﬁne Nt by the binary random variable which speciﬁes whether the
incoming instance (xt, yt) locates in a new cell of the coverage, that is, Nt = 1 indicating
the current cell Pit is a new cell. We assume that Zt is independent with (xt, yt) and so does
with Nt. Since a new instance is added to the support set if either a new cell is discovered
or the old cell is found but approximation is not performed, we reach the following

St ≤

−

St

1 + Nt + (1

Zt) (1

Nt)

−

−

43

Le et al

Taking expectation, we obtain

E [St]

≤

≤

≤
E [St]

E [St
−
E [St
−
E [St
−
E [St
−

1] + E [Nt] + (1
1] + E [Nt] + (1
−
1] + E [Nt] + qt (1

−

E [Zt]) (1
pt) (1

−
E [Nt])

E [Nt])

−
E [Nt])

−

E [Nt] + qt (1
Summing over the above when t = 1, . . . , T , we have

1]

≤

−

E [Nt])

−

T

T

t=1
X

E [ST ]

E [Nt] +

qt (1

E [Nt]) =

qt +

ptE [Nt]

T

T

t=1
X
T

≤

≤

−

T

T

t=1
X
E [Nt]

≤

qt +

t=1
X
qt + E [MT ]

Xt=1
where we have denoted P (Zt = 1) = pt, P (Zt = 0) = qt, and MT =
number of cells discovered so far.

Xt=1

Xt=1

We consider some speciﬁc cases and investigate the model size E [ST ] in these cases.
i) pt = P (Zt = 1) = 1,

t, that is, we always do approximation. From Eq. (11), we

P

obtain

∀

(11)

T
t=1 Nt indicates the

ii) pt = P (Zt = 1) = max

0, 1

E [ST ]
β
t

,

−

≤

∀

E [MT ]

≤ |P|

t. It follows that

(cid:16)
qt = 1

(cid:17)
pt ≤

1

−

β
t

1

−

=

β
t

(cid:19)

−

(cid:18)

From Eq. (11), we gain

E [ST ]

β

+ E [MT ]

β

1 +

+ E [MT ]

T

1
t

t=1
X

(cid:18)
β (log T + 1) + E [MT ]

≤

≤

≤

T

1
t

dt

(cid:19)

1

Z

iii) pt = P (Zt = 1) = max

0, 1

,

t where 0 < ρ < 1. It follows that

β
tρ

−

(cid:16)
qt = 1

(cid:17)
pt ≤

−

β
tρ

1

−

=

β
tρ

(cid:19)

−

(cid:18)

From Eq. (11), we gain

E [ST ]

β

≤

1
tρ + E [MT ]

≤

T

Xt=1

β

1 +

t−

ρdt

+ E [MT ]

+ E [MT ]

T

1

Z

(cid:19)

ρ

βT 1
1

−
ρ

−

≤

iv) pt = P (Zt = 1) = max

0, 1

,

t where ρ > 1. It follows that

(cid:18)

β
tρ

−

(cid:16)
qt = 1

(cid:17)
pt ≤

−

β
tρ

1

−

=

β
tρ

(cid:19)

−

(cid:18)

44

∀

1

∀

1

Approximation Vector Machines

From Eq. (11), we gain

E [ST ]

β

≤

1
tρ + E [MT ]

≤

βζ (ρ) + E [MT ]

βζ (ρ) +

≤

|P|

T

t=1
X

where ζ (.) is ζ- Riemann function deﬁned by the integral ζ (s) = 1
Γ(s)

+
∞0

ts−1
es

1 dt.

−

We now show the proof of Theorem 8. To realize this proof, we use the famous inequality,
namely Hoeﬀding which for completeness we state below.

R

bi for each i

Theorem. (Hoeﬀding inequality) Let the independent variables X1, . . . , Xn where ai ≤
i=1 Xi and ∆i = bi −
Xi ≤
2ε2
i=1 ∆2
i
(cid:17)
2ε2
i=1 ∆2
i

ai. The following hold

∈
E [S] > ε)

[n]. Let S =

−
ii) P (
S
|

−
(cid:16)
2 exp

≤
> ε)

E [S]
|

i) P (S

exp

P
Pn

Pn

−

≤

−

n

.

Proof of Theorem 8

From Eqs. (6, 8, 10), we achieve

(cid:16)

(cid:17)

1
T

T

t=1
X

−

T

1
T

t=1
X

E [f (wt)]

f (w∗)

−

H (log (T ) + 1)
2λT

≤

+ dT

Let us denote X = f (wr)

f (w∗), where r is uniformly sampled from

1, 2, . . . , T
{

.

}

We have

Er [X] =

E [f (wt)]

f (w∗)

−

H (log (T ) + 1)
2λT

≤

+ dT

It follows that

E [X] = E

(xt,yt)T

t=1

[Er [X]]

H (log (T ) + 1)
2λT

≤

+ dT

Let us denote ∆T = max
T
t
≤

≤

1

−

(f (wt)

f (w∗)) which implies that 0 < f (wr)

f (w∗) <

−

∆T . Applying Hoeﬀding inequality for the random variable X, we gain

P (X

E [X] > ε)

exp

−

≤

2ε2
∆2

T (cid:19)

−

(cid:18)

P

X

H (log (T ) + 1)
2λT

−

−

dT > ε

exp

≤

(cid:19)

−

(cid:18)

H (log (T ) + 1)
2λT

≤

+ dT + ε

> 1

exp

(cid:19)

−

−

(cid:18)

T (cid:19)

2ε2
∆2

T (cid:19)
2ε2
∆2

(cid:18)

X

P

(cid:18)

2ε2
∆2
T

−

(cid:16)

(cid:17)

Choosing δ = exp

or ε = ∆T

1

2 log 1

δ , then with the probability at least 1

δ,

−

we have

f (wr)

f (w∗)

−

H (log (T ) + 1)
2λT

≤

+ dT + ∆T

log

1
2

r

1
δ

q

45

Le et al

Proof of Theorem 9
T = E
We denote W α
i) If Zt is independent with wt, taking sum in Eq. (5) when t = (1

. Our proof proceeds as follows.

α)T +1 −

w(1

w∗

−

2

i

h(cid:13)
(cid:13)

α) T + 1, . . . , T ,

−

we gain

T

Xt=(1
−

α)T +1

E [f (wt)]

αT f (w∗)

−

λ (1

α) T

W α

T +

H
2λ

1/2

E

2

∆tk
k

h

i

1/2

(12)

(cid:13)
(cid:13)

−
2

≤

+

T

Xt=1
−
2

P (Zt = 1)1/2 E

λ (1

α) T

W α

T +

≤

T

1
t

Xt=(1
−

α)T +1

wt −
k
H log (1/ (1

h

w∗

2
k

i
α))

−

2λ

T

+ δΦM 1/2W 1/2

P (Zt = 1)1/2

T
t=(1

α)T +1

−

α)T +1

Xt=(1
−
log (1/ (1

1
t ≤

P

λ (1

α)

−
2α

W α

T +

δΦM 1/2W 1/2
αT

≤

+

H log (1/ (1
2λαT

−

α))

α)).

−

T

Xt=(1
−

α)T +1

E [f (wα
T )

f (w∗)]

−

P (Zt = 1)1/2

where we have used the inequality

ii) If Zt is independent with wt and (xt, yt), taking sum in Eq. (7) when t = (1

α) T +

−

1, . . . , T , we gain

T

Xt=(1
−

α)T +1

E [f (wt)]

αT f (w∗)

−

λ (1

α) T

W α

T +

H
2λ

T

1
t

Xt=(1
−

α)T +1

−
2

T

≤

+

Xt=(1
−

α)T +1

P (Zt = 1) E

wt −
k

w∗

2
k

1/2

E

1/2

2

∆tk
k

h

i

h

i
(13)

λ (1

α) T

H log (1/ (1

α))

≤

−
2

W α

T +

−

2λ

+ δΦM 1/2W 1/2

P (Zt = 1)

T

Xt=(1
−

α)T +1

E [f (wα
T )

f (w∗)]

−

λ (1

α)

−
2α

W α

T +

δΦM 1/2W 1/2
αT

T

P (Zt = 1)

Xt=(1
−

α)T +1

≤

+

H log (1/ (1
2λαT

−

α))

46

Approximation Vector Machines

iii) In general, taking sum in Eq. (9) when t = (1

α) T + 1, . . . , T , we gain

−

T

Xt=(1
−

α)T +1

E [f (wt)]

αT f (w∗)

−

λ (1

α) T

W α

T +

H
2λ

T

1
t

α)T +1

Xt=(1
−
1/2

−
2

T

≤

+

λ (1

Xt=(1
α)T +1
−
α) T
−
2

≤

W α

T +

E

wt −
k

w∗

2
k

h

i
H log (1/ (1

1/2

i

E

2

∆tk
k

h
α))

−

2λ

+ δΦM 1/2W 1/2αT

(14)

E [f (wα
T )

f (w∗)]

−

λ (1

α)

−
2α

≤

W α

T + δΦM 1/2W 1/2 +

H log (1/ (1
2λαT

−

α))

The proof of this theorem is similar to that of Theorem 8 which relies on Hoeﬀding

Proof of Theorem 10

inequality.

From Eqs. (12, 13, 14), we achieve

T

1
αT

Xt=(1
−

α)T +1

E [f (wt)]

f (w∗)

−

H log (1/ (1
2λαT

−

≤

α))

+ dT

Let us denote X = f (wr)

f (w∗), where r is uniformly sampled from

(1
{

−

α) T + 1, 2, . . . , T

. We have

−

Er [X] =

}

1
αT

T

Xt=(1
−

α)T +1

It follows that

E [f (wt)]

f (w∗)

−

H log (1/ (1
2λαT

−

≤

α))

+ dT

E [X] = E

(xt,yt)T

t=1

[Er [X]]

H log (1/ (1
2λαT

−

≤

α))

+ dT

Let us denote ∆α

T =

f (w∗) < ∆α

−
T . Applying Hoeﬀding inequality for the random variable X, we gain

t
≤

(1

−

T

−

(f (wt)

f (w∗)) which implies that 0 < f (wr)

max
α)T +1
≤

P (X

E [X] > ε)

exp

−

≤

 −

2ε2
∆α
T

2

!

H log (1/ (1
2λαT

−

−

α))

−

dT > ε

P

X

(cid:18)

(cid:1)
exp

(cid:0)

≤

(cid:19)

2ε2
∆α
T

2

!

 −

(cid:0)

(cid:1)

47

Le et al

H log (1/ (1
2λαT

−

≤

α))

P

X

(cid:18)

+ dT + ε

> 1

exp

(cid:19)

−

 −

2ε2
∆α
T

2

!

Choosing δ = exp

2ε2
T )2
(∆α

(cid:19)

−

(cid:18)

1

δ, we have

−

q

or ε = ∆α
T

1

2 log 1

(cid:1)
δ , then with the probability at least

(cid:0)

f (wr)

f (w∗)

−

H log (1/ (1
2λαT

−

≤

α))

+ dT + ∆α
T

1
2

r

log

1
δ

Proof of Theorem 13

It is apparent that f (w) is L-strongly smooth w.r.t

f (wr)

f (w∗)

−

≤

′

f

(w∗)

T

(wr −

w∗) +

It follows that ∆α

1
2 LM α

T . Hence we gain the conclusion.

T ≤

k2. Therefore, we have
.
k
L
2 k

wr −

w∗

w∗

≤

k

2

2
k

L
2 k

wr −

48

Approximation Vector Machines

Appendix C. Proofs of Bound for L2 Loss

We now consider the upper bound of
in the case that ℓ2 loss is being used for regression
problem. Concretely, we have the following theorem whose proof is similar to that of
Theorem 1 in (Shalev-Shwartz et al., 2007, 2011).

w∗k
k

Theorem 23. If w∗ = argminw
ymaxλ−

1/2.

(cid:16)

w

λ
2 k

2 + 1
N
k

N
i=1

yi −

wTΦ (xi)

2

then

w∗k ≤

k

P

(cid:0)

(cid:17)

(cid:1)

Proof Let us consider the equivalent constrains optimization problem

λ
2 k

min
w,ξ  
s.t.: ξi = yi −

w

k
wT

2 +

1
N

Φ (xi) ,

N

Xi=1
i
∀

ξ2
i

!

The Lagrange function is of the following form

(w, ξ, α) =

w2

+

L

λ
2

N

N

ξ2
i +

1
N

Xi=1

Xi=1

(cid:16)

αi

yi −

wT

Φ (xi)

ξi

−

(cid:17)

Setting the derivatives to 0, we gain

= λw

w

∇

L

αiΦ (xi) = 0

w = λ−

1

αiΦ (xi)

→

ξi −
Substituting the above to the Lagrange function, we gain the dual form

αi = 0

∇ξiL

ξi =

→

=

N

Xi=1

N αi
2

(α) =

W

λ
2 k

w

2 +
k

−

yiαi −

N
4

N

Xi=1

N

α2
i

Xi=1
N

2

αiΦ (xi)

+

yiαi −

N
4

N

α2
i

(cid:13)
(cid:13)
(cid:13)
Let us denote (w∗, ξ∗) and α∗ be the primal and dual solutions, respectively. Since the
(cid:13)
(cid:13)

Xi=1

Xi=1

Xi=1

strong duality holds, we have

λ
2 k

w∗

2 +

k

1
N

2
i =
ξ∗

λ
2 k

w∗

2 +
k

−

yiα∗i −

N
4

N

Xi=1

2
α∗
i

N

Xi=1

λ

w∗
k

2 =
k

N

yiα∗i −

N
4

2
α∗
i −

yiα∗i −

≤

Xi=1 (cid:18)

≤

(cid:19)

Xi=1

Xi=1
N
4

2
α∗
i

49

1
N

N

N

2
ξ∗
i

Xi=1
y2
i
N ≤

y2
max

(cid:13)
(cid:13)

(cid:13)
(cid:13)

N

−

Xi=1

2
N

=

−

1
2λ (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

N

Xi=1

N

Xi=1
N

Le et al

We note that we have used g (α∗i ) = yiα∗i −

conclusion.

N
2
4 α∗
i ≤

g

2yi
N

(cid:16)

(cid:17)

= y2

i

N . Hence, we gain the

Lemma 24. Assume that ℓ2 loss is using, the following statement holds

wT +1k ≤
k

1

λ−

ymax +

 

1
T

T

t=1
X

wtk!

k

where ymax = max
∈Y

y

y
|

.
|

Proof We have the following

wt+1 =

S

S

(Q
Q

(cid:0)
(cid:0)

1

t
−
t
−

t wt −
t wt −

1

ηtαtΦ (xt)
ηtαtΦ (cit)
(cid:1)
(cid:1)

if Zt = 0
otherwise

It follows that

wt+1k ≤
k

t

1

−
t

wtk
k

+

1
λt |

αt|

since

Φ (xt)
k

k

=

Φ (cit )
k
k

= 1

It happens that l

(wt; xt, yt) = αtΦ (xt). Hence, we gain

′

=

αt|
|
It implies that

yt −
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(t

wtk
wt+1k ≤
k
k
Taking sum when t = 1, 2, . . . , T , we achieve

1)

−

t

+ λ−

1 (ymax +

wtk
k

)

wT

t Φ (xt)

ymax +

wtk k

k

Φ (xt)

k ≤

ymax +

wtk
k

≤

T

wT +1k ≤

k

1

λ−

T ymax +

 

wtk!

k

wT +1k ≤
k

1

λ−

ymax +

 

1
T

wtk!

k

T

t=1
X

T

t=1
X

(15)

Theorem 25. If λ > 1 then

wT +1k ≤
k
Proof First we consider the sequence
and s1 = 0. It is easy to ﬁnd the formula of this sequence as

−

1

{

(cid:1)

(cid:0)

−

< ymax
λ
−

ymax
1
λ

1
λT

1 for all T .

sT }T which is identiﬁed as sT +1 = λ−

1 (ymax + sT )

sT +1 −

ymax
1
λ

−

= λ−

1

sT −

(cid:18)

ymax
1
λ

−

(cid:19)

= . . . = λ−

T

s1 −

(cid:18)

ymax
1
λ

−

(cid:19)

= −

λ−
λ

T ymax
1

−

50

Approximation Vector Machines

We prove by induction by T that

sT for all T . It is obvious that

= s1 = 0.

Assume that

wtk ≤
k

st for t

≤

T , we verify it for T + 1. Indeed, we have

w1k
k

sT +1 =

ymax
λ
1
−
wT k ≤

k

1

−

(cid:18)

1
λT

(cid:19)

wT +1k ≤
k

λ−

λ−

≤

T

1

ymax +

1
T
 
1 (ymax + sT ) = sT +1

Xt=1

k

wtk! ≤

1

λ−

ymax +

 

1
T

T

Xt=1

st

!

In addition, from Eq. (15) in case that λ
have the following theorem.

≤

1 we cannot bound

. Concretely, we

wT +1k

k

Theorem 26. If
zT }T is a sequence such that zT +1 = λ−
{
1 this sequence is not upper-bounded.
then in case that λ
≤

(cid:16)

1

ymax + 1
T

T
t=1 zt

with z1 = 0

P

(cid:17)

Let us denote st = y−

1
maxzt.

It is obvious that s1 = 0 and sT +1 =

. We now prove by induction by T that

Proof
1
λ−

(cid:16)

1 + 1
T

T
t=1 st

P

(cid:17)

With T = 2, we have s2 = λ−

1. Assume that this holds for all 2

t

T , we verify

≤

≤

it for T + 1.

for all T

2

≥

1
t

T

1

−

t=1
X

sT ≥

1

≥

sT +1 = λ−

1

1 +

T

1
T

T

t
−

1 +

st

! ≥

T

1

−

1 +

t=1
X
1
1
n ≥

T

st

1
T

T

t=1
X
t

−
T t

 

1
T

T

1 +

1 +

≥

≥

t=1
n=1
X
X
1
t −

1
T

Xt=1 (cid:18)

=

(cid:19)

T

t=1
X
1
t

Xt=1
T
t=1

P

The ﬁnal conclusion is obvious from the fact

1
t > log (T + 1) and hence can

exceed any positive number.

51

Le et al

References

M. Badoiu and K. L. Clarkson. Optimal core-sets for balls.

In In Proc. of DIMACS

Workshop on Computational Geometry, 2002.

G. Cavallanti, N. Cesa-Bianchi, and C. Gentile. Tracking the best hyperplane with a simple

budget perceptron. Machine Learning, 69(2-3):143–167, 2007.

C.-C. Chang and C.-J. Lin. Libsvm: A library for support vector machines. ACM Trans.

Intell. Syst. Technol., 2(3):27:1–27:27, May 2011. ISSN 2157-6904.

C. Cortes and V. Vapnik. Support-vector networks. Machine learning, 20(3):273–297, 1995.

K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based

vector machines. J. Mach. Learn. Res., 2:265–292, March 2002. ISSN 1532-4435.

K. Crammer, J. Kandola, and Y. Singer. Online classiﬁcation on a budget. In Advances in

Neural Information Processing Systems 16. MIT Press, 2004.

K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. Online passive-

aggressive algorithms. J. Mach. Learn. Res., 7:551–585, 2006.

F. Cucker and S. Smale. On the mathematical foundations of learning. Bulletin of the

American Mathematical Society, 39:1–49, 2002.

O. Dekel, S. Shalev-Shwartz, and Y. Singer. The forgetron: A kernel-based perceptron on
a ﬁxed budget. In Advances in Neural Information Processing Systems, pages 259–266,
2005.

M. Dredze, K. Crammer, and F. Pereira. Conﬁdence-weighted linear classiﬁcation.

In
Proceedings of the 25th International Conference on Machine Learning, ICML ’08, pages
264–271, New York, NY, USA, 2008. ACM.

Y. Freund and R. E. Schapire. Large margin classiﬁcation using the perceptron algorithm.

Mach. Learn., 37(3):277–296, December 1999.

J. Hensman, N. Fusi, and N. D Lawrence. Gaussian processes for big data. In Uncertainty

in Artiﬁcial Intelligence, page 282. Citeseer, 2013.

T. Joachims. Advances in kernel methods. chapter Making Large-scale Support Vector
Machine Learning Practical, pages 169–184. MIT Press, Cambridge, MA, USA, 1999.
ISBN 0-262-19416-3.

J. Kivinen, A. J. Smola, and R. C. Williamson. Online Learning with Kernels.

IEEE

Transactions on Signal Processing, 52:2165–2176, August 2004.

T. Le, P. Duong, M. Dinh, D. T Nguyen, V. Nguyen, and D. Phung. Budgeted semi-
supervised support vector machine. In The 32th Conference on Uncertainty in Artiﬁcial
Intelligence, June 2016a.

52

Approximation Vector Machines

T. Le, T. D. Nguyen, V. Nguyen, and D. Phung. Dual space gradient descent for on-
line learning. In Advances in Neural Information Processing (NIPS), pages 4583–4591,
December 2016b.

T. Le, V. Nguyen, T. D. Nguyen, and Dinh Phung. Nonparametric budgeted stochastic
In The 19th International Conference on Artiﬁcial Intelligence and

gradient descent.
Statistics, pages 654–572, May 2016c.

J. Lu, S. C.H. Hoi, J. Wang, P. Zhao, and Z.-Y. Liu. Large scale online kernel learning. J.

Mach. Learn. Res., 2015.

L. Ming, W. Shifeng, and Z. Changshui. On the sample complexity of random fourier
features for online learning: How many random fourier features do we need? ACM
Trans. Knowl. Discov. Data, 8(3):13:1–13:19, June 2014. ISSN 1556-4681.

F. Orabona, J. Keshet, and B. Caputo. Bounded kernel-based online learning. J. Mach.

Learn. Res., 10:2643–2666, December 2009. ISSN 1532-4435.

A. Rahimi and B. Recht. Random features for large-scale kernel machines. In In Neural

Infomration Processing Systems, 2007.

A. Rakhlin, O. Shamir, and K. Sridharan. Making gradient descent optimal for strongly con-
vex stochastic optimization. In International Conference on Machine Learning (ICML-
12), pages 449–456, 2012.

C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning (Adaptive

Computation and Machine Learning). The MIT Press, 2005. ISBN 026218253X.

F. Rosenblatt. The perceptron: A probabilistic model for information storage and organi-

zation in the brain. Psychological Review, 65(6):386–408, 1958.

S. Shalev-Shwartz and N. Srebro. Svm optimization:

inverse dependence on training set
In Proceedings of the 25th international conference on Machine learning, pages

size.
928–935. ACM, 2008.

S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized

loss. Journal of Machine Learning Research, 14(1):567–599, 2013.

S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver
for svm. In Proceedings of the 24th International Conference on Machine Learning, ICML
’07, pages 807–814, New York, NY, USA, 2007. ACM. ISBN 978-1-59593-793-3.

S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: Primal estimated sub-

gradient solver for svm. Mathematical programming, 127(1):3–30, 2011.

I. Steinwart. Sparseness of support vector machines. J. Mach. Learn. Res., 4:1071–1105,

December 2003. ISSN 1532-4435.

I. W. Tsang, J. T. Kwok, P. Cheung, and N. Cristianini. Core vector machines: Fast svm
training on very large data sets. Journal of Machine Learning Research, 6:363–392, 2005.

53

Le et al

I. W. Tsang, A. Kocsor, and J. T. Kwok. Simpler core vector machines with enclosing
balls. In Proceedings of the 24th International Conference on Machine Learning, ICML
’07, pages 911–918, 2007.

Z. Wang and S. Vucetic. Twin vector machines for online learning on a budget. In Proceed-

ings of the SIAM International Conference on Data Mining, pages 906–917, 2009.

Z. Wang and S. Vucetic. Online passive-aggressive algorithms on a budget. In AISTATS,

volume 9, pages 908–915, 2010.

Z. Wang, K. Crammer, and S. Vucetic. Breaking the curse of kernelization: Budgeted
stochastic gradient descent for large-scale svm training. J. Mach. Learn. Res., 13(1):
3103–3131, 2012.

K. Zhang, L. Lan, Z. Wang, and F. Moerchen. Scaling up kernel svm on limited resources:
A low-rank linearization approach. In International Conference on Artiﬁcial Intelligence
and Statistics, pages 1425–1434, 2012.

P. Zhao, J. Wang, P. Wu, R. Jin, and S. C. H. Hoi. Fast bounded online gradient descent

algorithms for scalable kernel-based online learning. CoRR, 2012.

M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In
Machine Learning, Proceedings of the Twentieth International Conference (ICML 2003,
pages 928–936, 2003.

54

