7
1
0
2
 
b
e
F
 
9
 
 
]

R

I
.
s
c
[
 
 
1
v
7
1
8
2
0
.
2
0
7
1
:
v
i
X
r
a

Graph Based Relational Features for Collective
Classiﬁcation

Immanuel Bayer, Uwe Nagel, and Steﬀen Rendle(cid:63)

University of Konstanz
78457 Konstanz, Germany
{immanuel.bayer,uwe.nagel,steffen.rendle}@uni-konstanz.de

Abstract. Statistical Relational Learning (SRL) methods have shown
that classiﬁcation accuracy can be improved by integrating relations be-
tween samples. Techniques such as iterative classiﬁcation or relaxation
labeling achieve this by propagating information between related sam-
ples during the inference process. When only a few samples are labeled
and connections between samples are sparse, collective inference methods
have shown large improvements over standard feature-based ML methods.
However, in contrast to feature based ML, collective inference methods
require complex inference procedures and often depend on the strong as-
sumption of label consistency among related samples. In this paper, we
introduce new relational features for standard ML methods by extract-
ing information from direct and indirect relations. We show empirically
on three standard benchmark datasets that our relational features yield
results comparable to collective inference methods. Finally we show that
our proposal outperforms these methods when additional information is
available.

1

Introduction

Statistical relational learning (SRL) methods are used when samples are con-
nected by one or more relation. These relations are helpful in tasks like scientiﬁc
article classiﬁcation where patterns such as “connected samples have similar la-
bels” are very predictive. Feature based ML methods in contrast often assume
that samples are independently, identically distributed (iid). This approach is
well established, allows for eﬃcient parameter estimation and simpliﬁed predic-
tion but ignores the relational information available in SRL settings. Recently,
a number of methods have been proposed [11, 13, 18, 31] that signiﬁcantly im-
proves over classical methods by using joint inference. Exact joint inference has
high runtime complexity which often requires approximate solutions [30]. These
approximated joint inference techniques introduce new diﬃculties such as the
need for specialized implementations that are expensive to run and diﬃcult to
tune [30].

In this paper we propose to transfer the relational information into classical
features. This allows a straightforward combination of relational and classical

(cid:63) Current aﬃliation: Google Inc.

(attribute) information. It also renders traditional, feature based ML methods
competitive in settings where relational information is available and allows to
leverage the large body of classical ML methods and their scalable algorithms.
We use three standard collective classiﬁcation (CC) benchmark datasets to
show that classical ML with relational features are strong competitors for state
of the art SRL methods on this task. Note that on these datasets, collective
classiﬁcation are considered the best performing methods in the current litera-
ture [7, 14]. In particular, we make the following contributions:

– We discuss how joint inference could be avoided by extending the sample

description with relational features (Section 2.1).

– We extend relational features to indirect relations (Section 2.2). This is new
and crucial to achive high accuracy when only few samples are labeled (Sec-
tion 4.3).

– We introduce a new cluster based relational feature (Section 2.2) that pro-

vides strong results and is cheap to compute.

– We show that our approach improves state of the art collective classiﬁcation

even in network only settings (Section 4.2).

2 Problem Setting

We start by giving the necessary deﬁnitions with the traditional setting of sam-
ples D = {(xi, yi)}N
i=1 where yi is the class label and xi is a feature vector
describing sample i. We assume that for the ﬁrst u samples (i ∈ {1, . . . , u}), the
class label yi is known and for the samples with index i > u, the class label
is unknown. Relations among samples are represented by weighted, symmetric
adjacency matrices Rk ∈ Rn×n and the complete relational information is de-
noted as R = {Rk}K
k=1. While in the general case, each of the k relations could
be complete, i.e. provide some similarity between each pair of samples, we ex-
plicitly consider the case of sparse, unweighted relations where only a minority
of node pairs are connected by an edge. We start with a statistical argument to
motivate the representation of relational information in a way that is compatible
with the iid assumption.

2.1 Preserving IID

Many machine learning algorithms are based on the maximum likelihood prin-
cipal to learn the optimal value θ∗ for model parameters given a dataset D
(c.f. [17])

θ∗ := arg max

p(θ|D) = arg max

p(D|θ)

θ

θ

where l(θ) := p(D|θ) is called the likelihood. A very common assumption in
many ML approaches is that the samples in the dataset D are independent and
identically distributed (iid). This assumption simpliﬁes the likelihood to

p(D|θ)

iid
∝

p(yi|xi, θ).

N
(cid:89)

i=1

One of the central arguments of relational learning is that examples are not iid. In
particular for any pair of examples (xi, yi) and (xj, yj) conditional independence
does not hold

p((xi, yi), (xj, yj)|θ) (cid:54)∝ p(yi|xi, θ) p(yj|xj, θ).

Note that in this formulation the relational information R is completely ne-
glected. However if R is used, it can render the probabilities independent

p((xi, yi), (xj, yj), R|θ)

∝ p(yi|xi, R, θ) p(yj|xj, R, θ).

(1)

This formulation is close to standard non-relational ML with iid of samples.

Standard ML algorithms assume that all information about a sample can
be encoded in a (usually real-valued) feature vector x. Let us assume that the
inﬂuence of R on sample i can be described through a ﬁnite number of real
valued variables xr
i relational features. To simplify notation, we can
deﬁne ˜xi as the extended feature vector of an example i that combines both non-
relational features xi and relational features xr
i . In total, this allows to rewrite
Equation 1

i . We call xr

p(D|θ)

iid
∝

p(yi|˜xi, θ).

N
(cid:89)

i=1

Note that due to the relational information in ˜xi, the iid assumption can be
preserved. In the remainder of this section, we discuss several ways to generate
relational features.

2.2 Graph Based Relational Features

Samples can be linked through multiple relations each of which can be described
as a graph. Representing each relation by an independent set of features allows
us to integrate an arbitrary number of relations per problem into a standard
feature matrix. All of the proposed features have in common that the encoded
relational information does not only consist of direct relations but in addition
captures indirect relations which we found to be the key to their performance.

Neighbor Ids Encoding the direct neighbors of a sample i in relation Rk can
be achieved by treating each sample as a categorical variable which is true when
samples are connected and false otherwise [1, 24, 25]. As this information is very
local and yields limited information about i’s position in Rk, we extend it by
additionally including indirect neighbors at various distances to i. Distance refers
to the number of edges dk(i, j) on a shortest path connecting i and j in Rk. In
particular, for a (small) set of distances d ≥ 1 we describe each relation Rk
by the distance-d neighborhood matrices Dd
i,j = 1,
if dk(i, j) = d and 0 otherwise. We illustrate the derivation of this feature in
Figure 1.

k ∈ {0, 1}n×n with (cid:0)Dd

(cid:1)

k

Fig. 1. Feature matrix for
the node
neighborhoods of a relation. The edges of
the original relation are depicted as gray
lines, while connections in distance two
and three are shown as blue/short dashed
and red/long dashed curves.

Aggregated Neighbor Attributes Relational position can also be described
by individual features of direct and indirect neighbors [20, 21]. As before we
extend the idea by calculating individual features at various distances. For a
categorical attribute with categories 1, . . . , c, we deﬁne an n × c matrix L with
Li,j = 1 if xi is labeled as j and Li,j = 0 otherwise. Then the count matrix
C d
kL can be derived as the projection of the corresponding neighborhood
matrix to the label matrix L such that (C d
k )i,j yields the number of category j
nodes in distance d of sample i. We denote this feature by neighbor class counts
(NCC) and provide an illustration in Figure 2. An additional row normalization
yields a probability matrix for the class labels in distance d, which we will denote
by neighbor class probabilities (NCP).

k =: Dd

Fig. 2. Aggregation for attribute counts
on distances one, two and three in the re-
lation Rk. Labels are given in text and
as color (1-blue, 2-red). Sample 1 has a
single node of each label as direct neigh-
bors which is reﬂected in the ﬁrst two
columns(C 1
k) of its row, while the next two
columns (C 2
k) encode the single 2-labeled
node in distance 2. Note that unlabeled
nodes (white) are ignored in the features.

Random Walk Similarity While the features described above are based
strictly on shortest paths, random walks with restart (rwr) incorporate a dif-
ferent notion of connectivity. They have been proposed as a similarity measure
in the context of auto-captioning of images [22].

The similarity between two nodes is measured as the probability of a random
walk connecting them, i.e. the probability of the random walk process visiting
one node when started from the other. To control for locality this includes a
restart probability: in each step the walk will jump back to the starting point
with probability r. This can be modeled as

pi = (1 − r)W pi + rei

(2)

where the column vector pi of matrix P describes the steady-state probability
distribution over all samples for walks starting at i. W is the matrix encoding

transition probabilities, i.e. a L1 row normalized version of Rk, ei is the vector of
zeros and unit value at position i and the parameter r is the restart probability.
P can be determined as the solution of a linear system or approximated eﬃ-
ciently [32], leaving r as free parameter. We derive ˆP from P by column wise L2
normalization and use analogous to the neighbor features row i of ˆP as features
for sample i.

Fig. 3. An example for encoding a clus-
tering of a relation as a feature vector.
Memberships of samples in clusters are
represented as binary features.

Clustering Memberships Clustering methods can be used to identify groups
of similar samples. Clustering features encode this information by representing
this group membership. Given a clustering of the graph representing relation
Rk into c clusters, we obtain an n × c feature matrix M c
k)i,m = 1 if
sample i belongs to cluster m and zero otherwise. Since a single clustering yields
limited information about the dense groups in the graph, we create features for
various clusterings, i.e. diﬀerent c. We limit c to c = 2j subject to 2 ≤ c ≤ n
which limits the number of clusterings to (cid:98)log2(n)(cid:99) while also providing a wide
range of cluster sizes. This results in O(n) features per relation that are very
sparse with only O(log(n)) non-zero features for per sample. The clusters can be
calculated with negligible runtime using the METIS clustering framework1 [8].
Note that the dense subgroups identiﬁed in the clusterings can be directly related
to the homophily assumption often exploited in relational learning.

k with (M c

3 Related Work

We build on two main categories of related work. The ﬁrst, in Section 3.1 uses
features derived from the network structure to improve iid based inference. The
second, discussed in Section 3.2 is work that views collective classiﬁcation as
a joint inference problem, simultaneously inferring the class label on every in-
stance. The challenges speciﬁc to problems with few labeled data points have
received special attention [4, 5, 14, 29] and helped us to understand the impor-
tance of indirect relations.

3.1 Relational Features

Relational features can be combined with collective inference or directly used
with standard ML methods as we argue in Section 2.1. Models such as Relational

1 We used the implementation available at http://glaros.dtc.umn.edu/gkhome/

views/metis with default parameters.

Probabilistic Trees [20], Relational Bayes Classiﬁer [21] and the Link Based
Classiﬁer (LBC, [11]) concentrate primarily on the aggregation of attributes
of connected samples. Others use rows from the (weighted) adjacency matrix
as basis for feature construction [1, 24, 25]. We were especially inspired from
the suggestion to extend the neighborhood of samples with few neighbors with
distance-two neighborhoods [26] or ghost edges [5]. In contrast to previous work
we keep the information from various neighborhood distances separated and
introduce the concept of multiple indirect relations.

3.2 Collective Inference

Full relational models such as Markov Logic Networks (MLN, [28]) or the Prob-
abilistic Relational Model [31] can be used for CC [3]. We refer to Sen et al. [30]
for a comprehensive overview of collective inference based CC algorithms. Their
strength in high label autocorrelation settings and the problem of error prop-
agation has been examined [7, 33] and improved inference schemes have been
proposed [15]. Recently, stacking of non relational model has been introduced [9]
as a fast approximation of Relational Dependency Networks [19].

4 Evaluation

In our experiments2 we investigate the following three questions:

1. Are classical ML methods with relational features competitive to MLN and

Collective Classiﬁcation approaches.

2. What are the main ingredients that make relational features eﬀective.
3. Does the combination of relational and attribute information improve re-

sults?

4.1 Experimental Setup

Datasets We use three standard benchmark SRL datasets. The Cora and Cite-
Seer scientiﬁc paper collections have been used in diﬀerent versions, we chose
the versions3 presented in [30] and the IMDb dataset4. Both, Cora and CiteSeer
include text features in form of bag of words (bow) representations. We give
some statistics of these datasets in Table 1.
Benchmark Models As baseline models we use the well established relational
learning methods wvRN [12, 13], nLB [11] and MLN [3]. We chose relaxation
labeling [2] as the collective inference method for wvRN and nLB as it has
been shown to outperform Gibbs sampling and iterative classiﬁcation on our
datasets [14]. For MLN we used the rules HasWord(p,+w)=>Topic(p,+t) and

2 Our code is available from https://github.com/ibayer/PAKDD2015
3 http://linqs.cs.umd.edu/projects/projects/lbc/index.html
4 http://netkit-srl.sourceforge.net/data.html

Nodes Links Classes |Dictionary| avg. Degree
2708 5278
Cora
CiteSeer
3312 4660
IMDb (all) 1377 46124

3.8981
2.8140
66.9920

1433
3703
-

7
6
2

Table 1. Summary statistics of the datasets.

Topic(p,t)^Link(p,p’)=>Topic(p’,t) together with discriminative weight learn-
ing and MC-SAT inference as recommended for Cora and Citeseer in a previous
study [3].
Measures & Protocol We follow [14] and remove samples that are not con-
nected to any other sample (singletons) in all experiments. Each experiment is
repeated 10 times with class-balanced splits into ratios of 0.1, 0.2, . . . , 0.9 for the
train-test set (shown as percentage in the ﬁgures). The MLN experiments are
only repeated 3 times due to their extremely high evaluation time. We calculate
multi-class accuracies by micro averaging and plot them on the y-axis of each
ﬁgure in this section. We used the netkit-srl framework [14] in version (1.4.0)5
to evaluate the wvRN and nLB classiﬁers and the Alchemy 2.0 framework6 to
evaluate the MLN model. The graph clusterings were obtained using METIS [8]
version 5.1.0. Relational features are learned with an L2 penalized logistic re-
gression model7 included in scikit-learn [23] version 0.14.01. The penalty hyper-
parameter C is optimized via grid search over {0.001, 0.01, 0.1, 1, 10, 100, 1000}
on the training set.

4.2 Comparing Relational Features to SRL

This section examines whether feature based relational models (without collec-
tive inference) are able to compete with the prediction quality of specialized
relational learning methods. Consequently, the benchmark is a task where only
relational data is available. In the ﬁrst experiment, we compare wvRN and nLB
with two logistic regression models that use only our relational features. The ﬁrst
relational feature model (rwr) is based on a random walk. The second model uses
both, neighborhood and aggregated (NCP) features with distances 1,2,3. We ex-
clude distances higher than three, since almost every node can be reached over
three edges from every other node and therefore further distances do not provide
additional information.

Figure 4 illustrates two problems of SRL models: (i) nLB performs poorly8
when labels are sparse and (ii) wvRN is sensitive to violations of its built in
assumptions – i.e. if label consistency among neighbors is not met, as with the
IMDb dataset.

5 http://netkit-srl.sourceforge.net/
6 https://code.google.com/p/alchemy-2/
7 We use a one-vs-all scheme for multiclass classiﬁcation.
8 This has been attributed to a lack of training data [14].

Fig. 4. Comparison between two SRL methods (nLB, wvRN) with relaxation labeling
and two relational feature based models on network only data. The dashed black lines
indicate base accuracy.

The relational feature based models show a very consistent performance not
much aﬀected by the number of labeled samples. The results of the neighbor and
NCP feature combination on IMDb illustrate the ﬂexibility of relational features.

4.3 Engineering Relational Features

We now examine the diﬀerent relational features and the inﬂuence of their pa-
rameters. Two questions will be addressed: (i) How important are indirect rela-
tions? (ii) Which of the proposed relational features lead to the best results? All
relational features that we consider can incorporate information about indirect
neighbors. Each method has parameters that adjusts the locality of the resulting
features. We ﬁrst examine the eﬀect of including indirect neighbors (Figure 5)
and the importance of unlabeled neighbors (Figure 6) for relational neighbor
features. The inﬂuence of the restart parameter r (c.f. Equation 2) for the rwr
features can be seen in Figure 7 and an informative subset of results for various
numbers of clusters is shown in Figure 8. The results suggest that the inclu-
sion of indirect neighbors in the relational features is beneﬁcial independently of
whether they are used directly or for aggregation. Figure 6 shows that unlabeled
neighbors contribute signiﬁcantly to the overall performance. Together this an-
swers our ﬁrst question: unlabeled samples and indirect neighborhood relations
are essential ingredients for relational features.

4.4 Combining Relational and Local Information

In the following, we examine the eﬀect of adding local attributes. Figure 9 shows
results with neighborhood count features (NCC) of distances 1,2,3. Interestingly,
the bag of words model performs better than network only models on Citeseer
but worse on Cora. Combining relational and local attributes on the other hand,
improves results in both cases. The ﬁgure further shows that our features out-
perform MLN on both datasets. In summary, our experiments suggest that the
combination of relational features and attributes is beneﬁcial even with a simple
model such as logistic regression.

Fig. 5. Inﬂuence of the distance parameter for label dependent and independent re-
lational features. Including information from indirect neighbors improves results espe-
cially if few samples are labeled.

4.5 Discussion

Our experiments indicate that relational feature based models compare well to
specialized relational learners even in network only and sparse labeling settings.
This has been veriﬁed on three standard SRL benchmark datasets and with
three state of the art SRL methods for comparison. The inclusion of indirect
neighbors has proven extremely important, especially in sparse label settings.
We have further shown that the combination of relational features and local
attributes is both straightforward and has the potential to improve considerably
over both, feature only and network only models.

Note, that our relational features can lead to very high dimensional repre-
sentations. Such feature spaces are, however, common in recommender systems,
click-through rate prediction and websearch where regularized logistic regression
has been shown to be very eﬀective [6,10]. In addition, we use a standard imple-
mentation of logistic regression and can consequently employ scalable versions
that can be trained with billions of samples of high dimensions [16, 27]. We are
further not committed to the logistic regression model as our features could be
used as input for arbitrary vector space models.

Fig. 6. Comparison of using only labeled vs all neighbors to construct relational neigh-
bor features (NCC). Including unlabeled neighbors improves the results in all settings.

Fig. 7. Inﬂuence of various restart values on the prediction accuracy. While as a general
trend larger values of r (c.f. Equation 2) lead to better results, the value r = 0.9 yields
consistently good results.

5 Conclusion

We have shown that dependencies between samples can be exploited using rela-
tional feature engineering. Our method allows to combine relational information
from various sources with attributes attached to individual samples. We tested
this on standard SRL benchmark datasets, showing that even on network only
data our features are competitive to specialized relational learning models. In
addition, our features can outperform them when additional information is avail-
able. Note that in contrast to the SRL methods, our proposal achieves these re-
sults without collective inference. While we restricted our experiments to logistic
regression as prediction model, the proposed features could be used as input to
any other feature based learning algorithm such as SVM, neural networks or ran-
dom forests. Extending the use of relational features to multi relational datasets
would be straight forward and a interesting direction for further research.

Acknowledgments. This work was supported by the DFG under grants Re
3311/2-1 and Br 2158/6-1.

Fig. 8. Examination of various clusterings of the dependency network and their usage
as relational feature. The optimal number of clusters strongly depends on the dataset
and ratio of labeled samples. The combination of all clusterings, however, is consistently
superior to individual clusterings.

Fig. 9. Comparison of attribute only
(bow), network only (NCC), MLN and
our combination of relational
features
with local attributes.

References

1. Bernstein, A., Clearwater, S., Provost, F.: The relational vector-space model and

industry classiﬁcation. In: IJCAI workshop. Volume 266. (2003)

2. Chakrabarti, S., Dom, B., Indyk, P.: Enhanced hypertext categorization using

hyperlinks. ACM SIGMOD Record 27(2) (1998) 307–318

3. Crane, R., McDowell, L.: Investigating markov logic networks for collective classi-

ﬁcation. In: ICAART (1). (2012) 5–15

4. Gallagher, B., Eliassi-Rad, T.: Leveraging label-independent features for classiﬁ-
cation in sparsely labeled networks: An empirical study. In Giles, L., Smith, M.,
Yen, J., Zhang, H., eds.: ASONAM. Springer (2010) 1–19

5. Gallagher, B., Tong, H., Eliassi-Rad, T., Faloutsos, C.: Using ghost edges for

classiﬁcation in sparsely labeled networks. In: KDD. (2008) 256–264

6. Graepel, T., Candela, J.Q., Borchert, T., Herbrich, R.: Web-scale bayesian click-
through rate prediction for sponsored search advertising in microsoft’s bing search
engine. In: ICML. (2010) 13–20

7. Jensen, D., Neville, J., Gallagher, B.: Why collective inference improves relational

classiﬁcation. In: KDD. (2004) 593–598

8. Karypis, G., Kumar, V.: A fast and high quality multilevel scheme for partitioning
irregular graphs. SIAM Journal on Scientiﬁc Computing 20(1) (1998) 359–392
9. Kou, Z., Cohen, W.W.: Stacked graphical models for eﬃcient inference in markov

random ﬁelds. In: SDM, SIAM (2007) 533–538

10. Liu, J., Chen, J., Ye, J.: Large-scale sparse logistic regression. In: KDD, ACM

11. Lu, Q., Getoor, L.: Link-based classiﬁcation. In: ICML. Volume 3. (2003) 496–503
12. Macskassy, S.A.: Improving learning in networked data by combining explicit and

mined links. In: AAAI. Volume 22. (2007) 590–595

13. Macskassy, S.A., Provost, F.: A simple relational classiﬁer. In: KDD-Workshop.

(2009) 547–556

(2003) 64–76

14. Macskassy, S.A., Provost, F.: Classiﬁcation in networked data: A toolkit and a

univariate case study. JMLR 8 (2007) 935–983

15. McDowell, L.K., Gupta, K.M., Aha, D.W.: Cautious collective classiﬁcation. JMLR

10 (2009) 2777–2836

16. Mukherjee, I., Canini, K., Frongillo, R., Singer, Y.: Parallel boosting with momen-

tum. In: ECML PKDD. Springer (2013) 17–32

17. Murphy, K.P.: Machine learning: A probabilistic perspective. The MIT Press

(2012)

18. Neville, J., Jensen, D.: Iterative classiﬁcation in relational data. In: Proc. AAAI-
2000 Workshop on Learning Statistical Models from Relational Data. (2000) 13–20
19. Neville, J., Jensen, D.: Collective classiﬁcation with relational dependency net-

20. Neville, J., Jensen, D., Friedland, L., Hay, M.: Learning relational probability trees.

works. In: UAI. (2003) 77–91

In: KDD. (2003) 625–630

21. Neville, J., Jensen, D., Gallagher, B.: Simple estimators for relational bayesian

classiﬁers. In: ICDM. (2003) 609–612

22. Pan, J.Y., Yang, H.J., Faloutsos, C., Duygulu, P.: Automatic multimedia cross-

modal correlation discovery. In: KDD, ACM (2004) 653–658

23. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O.,
Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A.,
Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, E.: Scikit-learn: Machine
learning in Python. JMLR 12 (2011) 2825–2830

24. Perlich, C., Provost, F.: Distribution-based aggregation for relational learning with

identiﬁer attributes. Machine Learning 62(1) (2006) 65–105

25. Perlich, C., Provost, F.: Aggregation-based feature invention and relational concept

classes. In: KDD, ACM (2003) 167–176

26. Preisach, C., Schmidt-Thieme, L.: Relational ensemble classiﬁcation. In: ICDM.

27. Rendle, S.: Scaling factorization machines to relational data. In: VLDB. Volume 6.,

(2006) 499–509

VLDB Endowment (2013) 337–348

(2006) 107–136

ﬁcation models. JMLR (2007)

28. Richardson, M., Domingos, P.: Markov logic networks. Machine Learning 62

29. Saar-Tsechansky, M., Provost, F.: Handling missing values when applying classi-

30. Sen, P., Namata, G.M., Bilgic, M., Getoor, L., Gallagher, B., Eliassi-Rad, T.:

Collective classiﬁcation in network data. AI Magazine 29(3) (2008) 93–106

31. Taskar, B., Segal, E., Koller, D.: Probabilistic classiﬁcation and clustering in rela-

tional data. In: IJCAI. Volume 17. (2001) 870–878

32. Tong, H., Faloutsos, C., Pan, J.Y.: Fast random walk with restart and its appli-

cations. In: ICDM. (2006) 613–622

33. Xiang, R., Neville, J.: Understanding propagation error and its eﬀect on collective

classiﬁcation. In: ICDM, IEEE (2011) 834–843

7
1
0
2
 
b
e
F
 
9
 
 
]

R

I
.
s
c
[
 
 
1
v
7
1
8
2
0
.
2
0
7
1
:
v
i
X
r
a

Graph Based Relational Features for Collective
Classiﬁcation

Immanuel Bayer, Uwe Nagel, and Steﬀen Rendle(cid:63)

University of Konstanz
78457 Konstanz, Germany
{immanuel.bayer,uwe.nagel,steffen.rendle}@uni-konstanz.de

Abstract. Statistical Relational Learning (SRL) methods have shown
that classiﬁcation accuracy can be improved by integrating relations be-
tween samples. Techniques such as iterative classiﬁcation or relaxation
labeling achieve this by propagating information between related sam-
ples during the inference process. When only a few samples are labeled
and connections between samples are sparse, collective inference methods
have shown large improvements over standard feature-based ML methods.
However, in contrast to feature based ML, collective inference methods
require complex inference procedures and often depend on the strong as-
sumption of label consistency among related samples. In this paper, we
introduce new relational features for standard ML methods by extract-
ing information from direct and indirect relations. We show empirically
on three standard benchmark datasets that our relational features yield
results comparable to collective inference methods. Finally we show that
our proposal outperforms these methods when additional information is
available.

1

Introduction

Statistical relational learning (SRL) methods are used when samples are con-
nected by one or more relation. These relations are helpful in tasks like scientiﬁc
article classiﬁcation where patterns such as “connected samples have similar la-
bels” are very predictive. Feature based ML methods in contrast often assume
that samples are independently, identically distributed (iid). This approach is
well established, allows for eﬃcient parameter estimation and simpliﬁed predic-
tion but ignores the relational information available in SRL settings. Recently,
a number of methods have been proposed [11, 13, 18, 31] that signiﬁcantly im-
proves over classical methods by using joint inference. Exact joint inference has
high runtime complexity which often requires approximate solutions [30]. These
approximated joint inference techniques introduce new diﬃculties such as the
need for specialized implementations that are expensive to run and diﬃcult to
tune [30].

In this paper we propose to transfer the relational information into classical
features. This allows a straightforward combination of relational and classical

(cid:63) Current aﬃliation: Google Inc.

(attribute) information. It also renders traditional, feature based ML methods
competitive in settings where relational information is available and allows to
leverage the large body of classical ML methods and their scalable algorithms.
We use three standard collective classiﬁcation (CC) benchmark datasets to
show that classical ML with relational features are strong competitors for state
of the art SRL methods on this task. Note that on these datasets, collective
classiﬁcation are considered the best performing methods in the current litera-
ture [7, 14]. In particular, we make the following contributions:

– We discuss how joint inference could be avoided by extending the sample

description with relational features (Section 2.1).

– We extend relational features to indirect relations (Section 2.2). This is new
and crucial to achive high accuracy when only few samples are labeled (Sec-
tion 4.3).

– We introduce a new cluster based relational feature (Section 2.2) that pro-

vides strong results and is cheap to compute.

– We show that our approach improves state of the art collective classiﬁcation

even in network only settings (Section 4.2).

2 Problem Setting

We start by giving the necessary deﬁnitions with the traditional setting of sam-
ples D = {(xi, yi)}N
i=1 where yi is the class label and xi is a feature vector
describing sample i. We assume that for the ﬁrst u samples (i ∈ {1, . . . , u}), the
class label yi is known and for the samples with index i > u, the class label
is unknown. Relations among samples are represented by weighted, symmetric
adjacency matrices Rk ∈ Rn×n and the complete relational information is de-
noted as R = {Rk}K
k=1. While in the general case, each of the k relations could
be complete, i.e. provide some similarity between each pair of samples, we ex-
plicitly consider the case of sparse, unweighted relations where only a minority
of node pairs are connected by an edge. We start with a statistical argument to
motivate the representation of relational information in a way that is compatible
with the iid assumption.

2.1 Preserving IID

Many machine learning algorithms are based on the maximum likelihood prin-
cipal to learn the optimal value θ∗ for model parameters given a dataset D
(c.f. [17])

θ∗ := arg max

p(θ|D) = arg max

p(D|θ)

θ

θ

where l(θ) := p(D|θ) is called the likelihood. A very common assumption in
many ML approaches is that the samples in the dataset D are independent and
identically distributed (iid). This assumption simpliﬁes the likelihood to

p(D|θ)

iid
∝

p(yi|xi, θ).

N
(cid:89)

i=1

One of the central arguments of relational learning is that examples are not iid. In
particular for any pair of examples (xi, yi) and (xj, yj) conditional independence
does not hold

p((xi, yi), (xj, yj)|θ) (cid:54)∝ p(yi|xi, θ) p(yj|xj, θ).

Note that in this formulation the relational information R is completely ne-
glected. However if R is used, it can render the probabilities independent

p((xi, yi), (xj, yj), R|θ)

∝ p(yi|xi, R, θ) p(yj|xj, R, θ).

(1)

This formulation is close to standard non-relational ML with iid of samples.

Standard ML algorithms assume that all information about a sample can
be encoded in a (usually real-valued) feature vector x. Let us assume that the
inﬂuence of R on sample i can be described through a ﬁnite number of real
valued variables xr
i relational features. To simplify notation, we can
deﬁne ˜xi as the extended feature vector of an example i that combines both non-
relational features xi and relational features xr
i . In total, this allows to rewrite
Equation 1

i . We call xr

p(D|θ)

iid
∝

p(yi|˜xi, θ).

N
(cid:89)

i=1

Note that due to the relational information in ˜xi, the iid assumption can be
preserved. In the remainder of this section, we discuss several ways to generate
relational features.

2.2 Graph Based Relational Features

Samples can be linked through multiple relations each of which can be described
as a graph. Representing each relation by an independent set of features allows
us to integrate an arbitrary number of relations per problem into a standard
feature matrix. All of the proposed features have in common that the encoded
relational information does not only consist of direct relations but in addition
captures indirect relations which we found to be the key to their performance.

Neighbor Ids Encoding the direct neighbors of a sample i in relation Rk can
be achieved by treating each sample as a categorical variable which is true when
samples are connected and false otherwise [1, 24, 25]. As this information is very
local and yields limited information about i’s position in Rk, we extend it by
additionally including indirect neighbors at various distances to i. Distance refers
to the number of edges dk(i, j) on a shortest path connecting i and j in Rk. In
particular, for a (small) set of distances d ≥ 1 we describe each relation Rk
by the distance-d neighborhood matrices Dd
i,j = 1,
if dk(i, j) = d and 0 otherwise. We illustrate the derivation of this feature in
Figure 1.

k ∈ {0, 1}n×n with (cid:0)Dd

(cid:1)

k

Fig. 1. Feature matrix for
the node
neighborhoods of a relation. The edges of
the original relation are depicted as gray
lines, while connections in distance two
and three are shown as blue/short dashed
and red/long dashed curves.

Aggregated Neighbor Attributes Relational position can also be described
by individual features of direct and indirect neighbors [20, 21]. As before we
extend the idea by calculating individual features at various distances. For a
categorical attribute with categories 1, . . . , c, we deﬁne an n × c matrix L with
Li,j = 1 if xi is labeled as j and Li,j = 0 otherwise. Then the count matrix
C d
kL can be derived as the projection of the corresponding neighborhood
matrix to the label matrix L such that (C d
k )i,j yields the number of category j
nodes in distance d of sample i. We denote this feature by neighbor class counts
(NCC) and provide an illustration in Figure 2. An additional row normalization
yields a probability matrix for the class labels in distance d, which we will denote
by neighbor class probabilities (NCP).

k =: Dd

Fig. 2. Aggregation for attribute counts
on distances one, two and three in the re-
lation Rk. Labels are given in text and
as color (1-blue, 2-red). Sample 1 has a
single node of each label as direct neigh-
bors which is reﬂected in the ﬁrst two
columns(C 1
k) of its row, while the next two
columns (C 2
k) encode the single 2-labeled
node in distance 2. Note that unlabeled
nodes (white) are ignored in the features.

Random Walk Similarity While the features described above are based
strictly on shortest paths, random walks with restart (rwr) incorporate a dif-
ferent notion of connectivity. They have been proposed as a similarity measure
in the context of auto-captioning of images [22].

The similarity between two nodes is measured as the probability of a random
walk connecting them, i.e. the probability of the random walk process visiting
one node when started from the other. To control for locality this includes a
restart probability: in each step the walk will jump back to the starting point
with probability r. This can be modeled as

pi = (1 − r)W pi + rei

(2)

where the column vector pi of matrix P describes the steady-state probability
distribution over all samples for walks starting at i. W is the matrix encoding

transition probabilities, i.e. a L1 row normalized version of Rk, ei is the vector of
zeros and unit value at position i and the parameter r is the restart probability.
P can be determined as the solution of a linear system or approximated eﬃ-
ciently [32], leaving r as free parameter. We derive ˆP from P by column wise L2
normalization and use analogous to the neighbor features row i of ˆP as features
for sample i.

Fig. 3. An example for encoding a clus-
tering of a relation as a feature vector.
Memberships of samples in clusters are
represented as binary features.

Clustering Memberships Clustering methods can be used to identify groups
of similar samples. Clustering features encode this information by representing
this group membership. Given a clustering of the graph representing relation
Rk into c clusters, we obtain an n × c feature matrix M c
k)i,m = 1 if
sample i belongs to cluster m and zero otherwise. Since a single clustering yields
limited information about the dense groups in the graph, we create features for
various clusterings, i.e. diﬀerent c. We limit c to c = 2j subject to 2 ≤ c ≤ n
which limits the number of clusterings to (cid:98)log2(n)(cid:99) while also providing a wide
range of cluster sizes. This results in O(n) features per relation that are very
sparse with only O(log(n)) non-zero features for per sample. The clusters can be
calculated with negligible runtime using the METIS clustering framework1 [8].
Note that the dense subgroups identiﬁed in the clusterings can be directly related
to the homophily assumption often exploited in relational learning.

k with (M c

3 Related Work

We build on two main categories of related work. The ﬁrst, in Section 3.1 uses
features derived from the network structure to improve iid based inference. The
second, discussed in Section 3.2 is work that views collective classiﬁcation as
a joint inference problem, simultaneously inferring the class label on every in-
stance. The challenges speciﬁc to problems with few labeled data points have
received special attention [4, 5, 14, 29] and helped us to understand the impor-
tance of indirect relations.

3.1 Relational Features

Relational features can be combined with collective inference or directly used
with standard ML methods as we argue in Section 2.1. Models such as Relational

1 We used the implementation available at http://glaros.dtc.umn.edu/gkhome/

views/metis with default parameters.

Probabilistic Trees [20], Relational Bayes Classiﬁer [21] and the Link Based
Classiﬁer (LBC, [11]) concentrate primarily on the aggregation of attributes
of connected samples. Others use rows from the (weighted) adjacency matrix
as basis for feature construction [1, 24, 25]. We were especially inspired from
the suggestion to extend the neighborhood of samples with few neighbors with
distance-two neighborhoods [26] or ghost edges [5]. In contrast to previous work
we keep the information from various neighborhood distances separated and
introduce the concept of multiple indirect relations.

3.2 Collective Inference

Full relational models such as Markov Logic Networks (MLN, [28]) or the Prob-
abilistic Relational Model [31] can be used for CC [3]. We refer to Sen et al. [30]
for a comprehensive overview of collective inference based CC algorithms. Their
strength in high label autocorrelation settings and the problem of error prop-
agation has been examined [7, 33] and improved inference schemes have been
proposed [15]. Recently, stacking of non relational model has been introduced [9]
as a fast approximation of Relational Dependency Networks [19].

4 Evaluation

In our experiments2 we investigate the following three questions:

1. Are classical ML methods with relational features competitive to MLN and

Collective Classiﬁcation approaches.

2. What are the main ingredients that make relational features eﬀective.
3. Does the combination of relational and attribute information improve re-

sults?

4.1 Experimental Setup

Datasets We use three standard benchmark SRL datasets. The Cora and Cite-
Seer scientiﬁc paper collections have been used in diﬀerent versions, we chose
the versions3 presented in [30] and the IMDb dataset4. Both, Cora and CiteSeer
include text features in form of bag of words (bow) representations. We give
some statistics of these datasets in Table 1.
Benchmark Models As baseline models we use the well established relational
learning methods wvRN [12, 13], nLB [11] and MLN [3]. We chose relaxation
labeling [2] as the collective inference method for wvRN and nLB as it has
been shown to outperform Gibbs sampling and iterative classiﬁcation on our
datasets [14]. For MLN we used the rules HasWord(p,+w)=>Topic(p,+t) and

2 Our code is available from https://github.com/ibayer/PAKDD2015
3 http://linqs.cs.umd.edu/projects/projects/lbc/index.html
4 http://netkit-srl.sourceforge.net/data.html

Nodes Links Classes |Dictionary| avg. Degree
2708 5278
Cora
CiteSeer
3312 4660
IMDb (all) 1377 46124

3.8981
2.8140
66.9920

1433
3703
-

7
6
2

Table 1. Summary statistics of the datasets.

Topic(p,t)^Link(p,p’)=>Topic(p’,t) together with discriminative weight learn-
ing and MC-SAT inference as recommended for Cora and Citeseer in a previous
study [3].
Measures & Protocol We follow [14] and remove samples that are not con-
nected to any other sample (singletons) in all experiments. Each experiment is
repeated 10 times with class-balanced splits into ratios of 0.1, 0.2, . . . , 0.9 for the
train-test set (shown as percentage in the ﬁgures). The MLN experiments are
only repeated 3 times due to their extremely high evaluation time. We calculate
multi-class accuracies by micro averaging and plot them on the y-axis of each
ﬁgure in this section. We used the netkit-srl framework [14] in version (1.4.0)5
to evaluate the wvRN and nLB classiﬁers and the Alchemy 2.0 framework6 to
evaluate the MLN model. The graph clusterings were obtained using METIS [8]
version 5.1.0. Relational features are learned with an L2 penalized logistic re-
gression model7 included in scikit-learn [23] version 0.14.01. The penalty hyper-
parameter C is optimized via grid search over {0.001, 0.01, 0.1, 1, 10, 100, 1000}
on the training set.

4.2 Comparing Relational Features to SRL

This section examines whether feature based relational models (without collec-
tive inference) are able to compete with the prediction quality of specialized
relational learning methods. Consequently, the benchmark is a task where only
relational data is available. In the ﬁrst experiment, we compare wvRN and nLB
with two logistic regression models that use only our relational features. The ﬁrst
relational feature model (rwr) is based on a random walk. The second model uses
both, neighborhood and aggregated (NCP) features with distances 1,2,3. We ex-
clude distances higher than three, since almost every node can be reached over
three edges from every other node and therefore further distances do not provide
additional information.

Figure 4 illustrates two problems of SRL models: (i) nLB performs poorly8
when labels are sparse and (ii) wvRN is sensitive to violations of its built in
assumptions – i.e. if label consistency among neighbors is not met, as with the
IMDb dataset.

5 http://netkit-srl.sourceforge.net/
6 https://code.google.com/p/alchemy-2/
7 We use a one-vs-all scheme for multiclass classiﬁcation.
8 This has been attributed to a lack of training data [14].

Fig. 4. Comparison between two SRL methods (nLB, wvRN) with relaxation labeling
and two relational feature based models on network only data. The dashed black lines
indicate base accuracy.

The relational feature based models show a very consistent performance not
much aﬀected by the number of labeled samples. The results of the neighbor and
NCP feature combination on IMDb illustrate the ﬂexibility of relational features.

4.3 Engineering Relational Features

We now examine the diﬀerent relational features and the inﬂuence of their pa-
rameters. Two questions will be addressed: (i) How important are indirect rela-
tions? (ii) Which of the proposed relational features lead to the best results? All
relational features that we consider can incorporate information about indirect
neighbors. Each method has parameters that adjusts the locality of the resulting
features. We ﬁrst examine the eﬀect of including indirect neighbors (Figure 5)
and the importance of unlabeled neighbors (Figure 6) for relational neighbor
features. The inﬂuence of the restart parameter r (c.f. Equation 2) for the rwr
features can be seen in Figure 7 and an informative subset of results for various
numbers of clusters is shown in Figure 8. The results suggest that the inclu-
sion of indirect neighbors in the relational features is beneﬁcial independently of
whether they are used directly or for aggregation. Figure 6 shows that unlabeled
neighbors contribute signiﬁcantly to the overall performance. Together this an-
swers our ﬁrst question: unlabeled samples and indirect neighborhood relations
are essential ingredients for relational features.

4.4 Combining Relational and Local Information

In the following, we examine the eﬀect of adding local attributes. Figure 9 shows
results with neighborhood count features (NCC) of distances 1,2,3. Interestingly,
the bag of words model performs better than network only models on Citeseer
but worse on Cora. Combining relational and local attributes on the other hand,
improves results in both cases. The ﬁgure further shows that our features out-
perform MLN on both datasets. In summary, our experiments suggest that the
combination of relational features and attributes is beneﬁcial even with a simple
model such as logistic regression.

Fig. 5. Inﬂuence of the distance parameter for label dependent and independent re-
lational features. Including information from indirect neighbors improves results espe-
cially if few samples are labeled.

4.5 Discussion

Our experiments indicate that relational feature based models compare well to
specialized relational learners even in network only and sparse labeling settings.
This has been veriﬁed on three standard SRL benchmark datasets and with
three state of the art SRL methods for comparison. The inclusion of indirect
neighbors has proven extremely important, especially in sparse label settings.
We have further shown that the combination of relational features and local
attributes is both straightforward and has the potential to improve considerably
over both, feature only and network only models.

Note, that our relational features can lead to very high dimensional repre-
sentations. Such feature spaces are, however, common in recommender systems,
click-through rate prediction and websearch where regularized logistic regression
has been shown to be very eﬀective [6,10]. In addition, we use a standard imple-
mentation of logistic regression and can consequently employ scalable versions
that can be trained with billions of samples of high dimensions [16, 27]. We are
further not committed to the logistic regression model as our features could be
used as input for arbitrary vector space models.

Fig. 6. Comparison of using only labeled vs all neighbors to construct relational neigh-
bor features (NCC). Including unlabeled neighbors improves the results in all settings.

Fig. 7. Inﬂuence of various restart values on the prediction accuracy. While as a general
trend larger values of r (c.f. Equation 2) lead to better results, the value r = 0.9 yields
consistently good results.

5 Conclusion

We have shown that dependencies between samples can be exploited using rela-
tional feature engineering. Our method allows to combine relational information
from various sources with attributes attached to individual samples. We tested
this on standard SRL benchmark datasets, showing that even on network only
data our features are competitive to specialized relational learning models. In
addition, our features can outperform them when additional information is avail-
able. Note that in contrast to the SRL methods, our proposal achieves these re-
sults without collective inference. While we restricted our experiments to logistic
regression as prediction model, the proposed features could be used as input to
any other feature based learning algorithm such as SVM, neural networks or ran-
dom forests. Extending the use of relational features to multi relational datasets
would be straight forward and a interesting direction for further research.

Acknowledgments. This work was supported by the DFG under grants Re
3311/2-1 and Br 2158/6-1.

Fig. 8. Examination of various clusterings of the dependency network and their usage
as relational feature. The optimal number of clusters strongly depends on the dataset
and ratio of labeled samples. The combination of all clusterings, however, is consistently
superior to individual clusterings.

Fig. 9. Comparison of attribute only
(bow), network only (NCC), MLN and
our combination of relational
features
with local attributes.

References

1. Bernstein, A., Clearwater, S., Provost, F.: The relational vector-space model and

industry classiﬁcation. In: IJCAI workshop. Volume 266. (2003)

2. Chakrabarti, S., Dom, B., Indyk, P.: Enhanced hypertext categorization using

hyperlinks. ACM SIGMOD Record 27(2) (1998) 307–318

3. Crane, R., McDowell, L.: Investigating markov logic networks for collective classi-

ﬁcation. In: ICAART (1). (2012) 5–15

4. Gallagher, B., Eliassi-Rad, T.: Leveraging label-independent features for classiﬁ-
cation in sparsely labeled networks: An empirical study. In Giles, L., Smith, M.,
Yen, J., Zhang, H., eds.: ASONAM. Springer (2010) 1–19

5. Gallagher, B., Tong, H., Eliassi-Rad, T., Faloutsos, C.: Using ghost edges for

classiﬁcation in sparsely labeled networks. In: KDD. (2008) 256–264

6. Graepel, T., Candela, J.Q., Borchert, T., Herbrich, R.: Web-scale bayesian click-
through rate prediction for sponsored search advertising in microsoft’s bing search
engine. In: ICML. (2010) 13–20

7. Jensen, D., Neville, J., Gallagher, B.: Why collective inference improves relational

classiﬁcation. In: KDD. (2004) 593–598

8. Karypis, G., Kumar, V.: A fast and high quality multilevel scheme for partitioning
irregular graphs. SIAM Journal on Scientiﬁc Computing 20(1) (1998) 359–392
9. Kou, Z., Cohen, W.W.: Stacked graphical models for eﬃcient inference in markov

random ﬁelds. In: SDM, SIAM (2007) 533–538

10. Liu, J., Chen, J., Ye, J.: Large-scale sparse logistic regression. In: KDD, ACM

11. Lu, Q., Getoor, L.: Link-based classiﬁcation. In: ICML. Volume 3. (2003) 496–503
12. Macskassy, S.A.: Improving learning in networked data by combining explicit and

mined links. In: AAAI. Volume 22. (2007) 590–595

13. Macskassy, S.A., Provost, F.: A simple relational classiﬁer. In: KDD-Workshop.

(2009) 547–556

(2003) 64–76

14. Macskassy, S.A., Provost, F.: Classiﬁcation in networked data: A toolkit and a

univariate case study. JMLR 8 (2007) 935–983

15. McDowell, L.K., Gupta, K.M., Aha, D.W.: Cautious collective classiﬁcation. JMLR

10 (2009) 2777–2836

16. Mukherjee, I., Canini, K., Frongillo, R., Singer, Y.: Parallel boosting with momen-

tum. In: ECML PKDD. Springer (2013) 17–32

17. Murphy, K.P.: Machine learning: A probabilistic perspective. The MIT Press

(2012)

18. Neville, J., Jensen, D.: Iterative classiﬁcation in relational data. In: Proc. AAAI-
2000 Workshop on Learning Statistical Models from Relational Data. (2000) 13–20
19. Neville, J., Jensen, D.: Collective classiﬁcation with relational dependency net-

20. Neville, J., Jensen, D., Friedland, L., Hay, M.: Learning relational probability trees.

works. In: UAI. (2003) 77–91

In: KDD. (2003) 625–630

21. Neville, J., Jensen, D., Gallagher, B.: Simple estimators for relational bayesian

classiﬁers. In: ICDM. (2003) 609–612

22. Pan, J.Y., Yang, H.J., Faloutsos, C., Duygulu, P.: Automatic multimedia cross-

modal correlation discovery. In: KDD, ACM (2004) 653–658

23. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O.,
Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A.,
Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, E.: Scikit-learn: Machine
learning in Python. JMLR 12 (2011) 2825–2830

24. Perlich, C., Provost, F.: Distribution-based aggregation for relational learning with

identiﬁer attributes. Machine Learning 62(1) (2006) 65–105

25. Perlich, C., Provost, F.: Aggregation-based feature invention and relational concept

classes. In: KDD, ACM (2003) 167–176

26. Preisach, C., Schmidt-Thieme, L.: Relational ensemble classiﬁcation. In: ICDM.

27. Rendle, S.: Scaling factorization machines to relational data. In: VLDB. Volume 6.,

(2006) 499–509

VLDB Endowment (2013) 337–348

(2006) 107–136

ﬁcation models. JMLR (2007)

28. Richardson, M., Domingos, P.: Markov logic networks. Machine Learning 62

29. Saar-Tsechansky, M., Provost, F.: Handling missing values when applying classi-

30. Sen, P., Namata, G.M., Bilgic, M., Getoor, L., Gallagher, B., Eliassi-Rad, T.:

Collective classiﬁcation in network data. AI Magazine 29(3) (2008) 93–106

31. Taskar, B., Segal, E., Koller, D.: Probabilistic classiﬁcation and clustering in rela-

tional data. In: IJCAI. Volume 17. (2001) 870–878

32. Tong, H., Faloutsos, C., Pan, J.Y.: Fast random walk with restart and its appli-

cations. In: ICDM. (2006) 613–622

33. Xiang, R., Neville, J.: Understanding propagation error and its eﬀect on collective

classiﬁcation. In: ICDM, IEEE (2011) 834–843

