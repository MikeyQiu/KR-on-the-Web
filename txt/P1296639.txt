9
1
0
2
 
n
u
J
 
9
1
 
 
]
L
M

.
t
a
t
s
[
 
 
6
v
2
2
9
3
0
.
6
0
7
1
:
v
i
X
r
a

Analyzing the Robustness of Nearest Neighbors to
Adversarial Examples

Yizhen Wang
University of California, San Diego
yiw248@eng.ucsd.edu

Somesh Jha
University of Wisconsin-Madison
jha@cs.wisc.edu

Kamalika Chaudhuri
University of California, San Diego
kamalika@cs.eng.ucsd.edu

Abstract

Motivated by safety-critical applications, test-time attacks on classiﬁers via adversar-
ial examples has recently received a great deal of attention. However, there is a general
lack of understanding on why adversarial examples arise; whether they originate due to
inherent properties of data or due to lack of training samples remains ill-understood.
In this work, we introduce a theoretical framework analogous to bias-variance theory
for understanding these eﬀects. We use our framework to analyze the robustness of
a canonical non-parametric classiﬁer – the k-nearest neighbors. Our analysis shows
that its robustness properties depend critically on the value of k – the classiﬁer may
be inherently non-robust for small k, but its robustness approaches that of the Bayes
Optimal classiﬁer for fast-growing k. We propose a novel modiﬁed 1-nearest neighbor
classiﬁer, and guarantee its robustness in the large sample limit. Our experiments 1
suggest that this classiﬁer may have good robustness properties even for reasonable
data set sizes.

1

Introduction

Machine learning is increasingly applied in security-critical domains such as automotive
systems, healthcare, ﬁnance and robotics. To ensure safe deployment in these applications,
there is an increasing need to design machine-learning algorithms that are robust in the
presence of adversarial attacks.

A realistic attack paradigm that has received a lot of recent attention [12, 26, 28, 25] is
test-time attacks via adversarial examples. Here, an adversary has the ability to provide
modiﬁed test inputs to an already-trained classiﬁer, but cannot modify the training process
in any way. Their goal is to perturb legitimate test inputs by a “small amount” in order to
force the classiﬁer to report an incorrect label. An example is an adversary that replaces a
stop sign by a slightly defaced version in order to force an autonomous vehicle to recognize
it as an yield sign. This attack is undetectable to the human eye if the perturbation is small
enough.

1Code available at: https://github.com/EricYizhenWang/robust_nn_icml

1

Prior work has considered adversarial examples in the context of linear classiﬁers [19],
kernel SVMs [3] and neural networks [28, 12, 25, 26, 22]. However, most of this work has
either been empirical, or has focussed on developing theoretically motivated attacks and
defenses. Consequently, there is a general lack of understanding on why adversarial examples
arise; whether they originate due to inherent properties of data or due to lack of training
samples remains ill-understood.

This work develops a theoretical framework for robust learning in order to understand the
eﬀects of distributional properties and ﬁnite samples on robustness. Building on traditional
bias-variance theory [10], we posit that a classiﬁcation algorithm may be robust to adversarial
examples due to three reasons. First, it may be distributionally robust, in the sense that the
output classiﬁer is robust as the number of training samples grow to inﬁnity. Second, even
the output of a distributionally robust classiﬁcation algorithm may be vulnerable due to too
few training samples – this is characterized by ﬁnite sample robustness. Finally, diﬀerent
training algorithms might result in classiﬁers with diﬀerent degrees of robustness, which we
call algorithmic robustness. These quantities are analogous to bias, variance and algorithmic
eﬀects respectively.

Next, we analyze a simple non-parametric classiﬁcation algorithm: k-nearest neighbors in
our framework. Our analysis demonstrates that large sample robustness properties of this
algorithm depend very much on k.

Speciﬁcally, we identify two distinct regimes for k with vastly diﬀerent robustness
properties. When k is constant, we show that k-nearest neighbors has zero robustness in
the large sample limit in regions where p(y = 1|x) lies in (0, 1). This is in contrast with
dn log n), where d is the
accuracy, which may be quite high in these regions. For k = Ω(
data dimension and n is the sample size, we show that the robustness region of k-nearest
neighbors approaches that of the Bayes Optimal classiﬁer in the large sample limit. This is
again in contrast with accuracy, where convergence to the Bayes Optimal accuracy is known
for a much slower growing k [7, 5].
√

dn log n) is too high to use in practice with nearest neighbors, we next
propose a novel robust version of the 1-nearest neighbor classiﬁer that operates on a modiﬁed
training set. We provably show that in the large sample limit, this algorithm has superior
robustness to standard 1-nearest neighbors for data distributions with certain properties.

Since k = Ω(

√

Finally, we validate our theoretical results by empirically evaluating our algorithm on
three datasets against several popular attacks. Our experiments demonstrate that our
algorithm performs better than or about as well as both standard 1-nearest neighbors and
nearest neighbors with adversarial training – a popular and eﬀective defense mechanism.
This suggests that although our performance guarantees hold in the large sample limit, our
algorithm may have good robustness properties even for realistic training data sizes.

1.1 Related Work

Adversarial examples have recently received a great deal of attention [12, 3, 26, 28, 25].
Most of the work, however, has been empirical, and has focussed on developing increasingly
sophisticated attacks and defenses.

1.1.1 Related Work on Adversarial Examples

Prior theoretical work on adversarial examples falls into two categories – analysis and theory-
inspired defenses. Work on analysis includes [9], which analyzes the robustness of linear and

2

quadratic classiﬁers under random and semi-random perturbations. [13] provides robustness
guarantees on linear and kernel classiﬁers trained on a given data set. [11] shows that linear
classiﬁers for high dimensional datasets may have inherent robustness-accuracy trade-oﬀs.
Work on theory-inspired defenses include [20, 15, 1], who provide defense mechanisms for
adversarial examples in neural networks that are relaxations of certain principled optimization
objectives. [14] shows how to use program veriﬁcation to certify robustness of neural networks
around given inputs for small neural networks.

Our work diﬀers from these in two important ways. First, unlike most prior work which
looks at a given training dataset, we consider eﬀects of the data distribution and number of
samples, and analyze robustness properties in the large sample limit. Second, unlike prior
work which largely focuses on parametric methods such as neural networks, our focus is on a
canonical non-parametric method – the nearest neighbors classiﬁer.

1.1.2 Related Work on Nearest Neighbors

There has been a body of work on the convergence and consistency of nearest-neighbor
classiﬁers and their many variants [6, 27, 17, 8, 5, 16]; all these works however consider
accuracy and not robustness.

In the asymptotic regime, [6] shows that the accuracy of 1-nearest neighbors converges in
the large sample limit to 1 − 2R∗(1 − R∗) where R∗ is the error rate of the Bayes Optimal
classiﬁer. This implies that even 1-nearest neighbor may achieve relatively high accuracy
even when p(y = 1|x) is not 0 or 1. In contrast, we show that 1-nearest neighbor is inherently
non-robust when p(y = 1|x) ∈ (0, 1) under some continuity conditions.

For larger k, the accuracy of k-nearest neighbors is known to converge to that of the
Bayes Optimal classiﬁer if kn → ∞ and kn/n → 0 as the sample size n → ∞. We show that
the robustness also converges to that of the Bayes Optimal classiﬁer when kn grows at a
much higher rate – fast enough to ensure uniform convergence. Whether this high rate is
necessary remains an intriguing open question.

Finite sample rates on the accuracy of nearest neighbors are known to depend heavily on
properties of the data distribution, and there is no distribution free rate as in parametric
methods [8].
[5] provides a clean characterization of the ﬁnite sample rates of nearest
neighbors as a function of natural interiors of the classes. Here we build on their results
by deﬁning a stricter, more robust version of interiors and providing bounds as functions of
these new robust quantities.

1.1.3 Other Related Work

[2] provides a method for generating adversarial examples for nearest neighbors, and shows
that the eﬀectiveness of attacks grow with intrinsic dimensionality. Finally, [24, 25] provides
black-box attacks on substitute classiﬁers; their experiments show that attacks from other
types of substitute classiﬁers are not successful on nearest neighbors; our experiments
corroborate these results.

3

2 The Setting and Deﬁnitions

2.1 The Basic Setup

We consider test-time attacks in a white box setting, where the adversary has full knowledge
of the training process – namely, the type of classiﬁer used, the training data and any
parameters – but cannot modify training in any way.

Given an input x, the adversary’s goal is to perturb it so as to force the trained classiﬁer
f to report a diﬀerent label than f (x). The amount of perturbation is measured by an
application-speciﬁc metric d, and is constrained to be within a radius r. Our analysis can
be extended to any metric, but for this paper we assume that d is the Euclidean distance
for mathematical simplicity; we also focus on binary classiﬁcation, and leave extensions to
multiclass for future work.

Finally, we assume that unlabeled instances are drawn from an instance space X , and
their labels are drawn from the label space {0, 1}. There is an underlying data distribution
D that generates labeled examples; the marginal over X of D is µ and the conditional
distribution of labels given x is denoted by η.

2.2 Robustness and astuteness

We begin by deﬁning robustness, which for a classiﬁer f at input x is measured by the
robustness radius.

Deﬁnition 2.1 (Robustness Radius). The robustness radius of a classiﬁer f at an instance
x ∈ X , denoted by ρ(f, x), is the shortest distance between x and an input x(cid:48) to which f
assigns a label diﬀerent from f (x):

ρ(f, x) = inf
r

{∃x(cid:48) ∈ X ∩ B(x, r) s.t f (x) (cid:54)= f (x(cid:48))}

Observe that the robustness radius measures a classiﬁer’s local robustness. A classiﬁer
f with robustness radius r at x guarantees that no adversarial example of x with norm
of perturbation less than r can be created using any attack method. A plausible way
to extend this into a global notion is to require a lower bound on the robustness radius
everywhere; however, only the constant classiﬁer will satisfy this condition. Instead, we
consider robustness around meaningful instances, that we model as examples drawn from
the underlying data distribution.

Deﬁnition 2.2 (Robustness with respect to a Distribution). The robustness of a classiﬁer
f at radius r with respect to a distribution µ over the instance space X , denoted by R(f, r, µ),
is the fraction of instances drawn from µ for which the robustness radius is greater than or
equal to r.

R(f, r, µ) = Pr
x∼µ

(ρ(f, x) ≥ r)

Finally, observe that we are interested in classiﬁers that are both robust and accurate.
This leads to the notion of astuteness, which measures the fraction of instances on which a
classiﬁer is both accurate and robust.

Deﬁnition 2.3 (astuteness). The astuteness of a classiﬁer f with respect to a data distribu-
tion D and a radius r is the fraction of examples on which it is accurate and has robustness
radius at least r; formally,

AstD(f, r) = Pr

(ρ(f, x) ≥ r, f (x) = y),

(x,y)∼D

4

Observe that astuteness is analogous to classiﬁcation accuracy, and we argue that it is
a more appropriate metric if we are concerned with both robustness and accuracy. Unlike
accuracy, astuteness cannot be directly empirically measured unless we have a way to certify
a lower bound on the robustness radius. In this work, we will prove bounds on the astuteness
of classiﬁers, and in our experiments, we will approximate it by measuring resistance to
standard attacks.

2.3 Sources of Robustness

There are three plausible reasons why classiﬁers lack robustness – distributional, ﬁnite sample
and algorithmic. These sources are analogous to bias, variance, and algorithmic eﬀects
respectively in standard bias-variance theory.

Distributional robustness measures the eﬀect of the data distribution on robustness
when an inﬁnitely large number of samples are used to train the classiﬁer. Formally, if
Sn is a training sample of size n drawn from D and A(Sn, ·) is a classiﬁer obtained by
applying the training procedure A on Sn, then the distributional robustness at radius r is
limn→∞ ESn∼D[R(A(Sn, ·), r, µ)].

In contrast, for ﬁnite sample robustness, we characterize the behaviour of R(A(Sn, ·), r, µ)
for ﬁnite n – usually by putting high probability bounds over the training set. Thus, ﬁnite
sample robustness depends on the training set size n, and quantiﬁes how it changes with
sample size. Finally, robustness also depends on the training algorithm itself; for example,
some variants of nearest neighbors may have higher robustness than nearest neighbors itself.

2.4 Nearest Neighbor and Bayes Optimal Classiﬁers

Given a training set Sn = {(X1, Y1), . . . , (Xn, Yn)} and a test example x, we use the notation
X (i)(x) to denote the i-th nearest neighbor of x in Sn, and Y (i)(x) to denote the label of
X (i)(x).

Given a test example x, the k-nearest neighbor classiﬁer Ak(Sn, x) outputs:

= 1,

= 0,

if Y (1)(x) + . . . + Y (k)(x) ≥ k/2
otherwise.

The Bayes optimal classiﬁer g over a data distribution D has the following classiﬁcation

rule:

g(x) =

(cid:26) 1
0

if η(x) = Pr(y = 1|x) ≥ 1/2;
otherwise.

(1)

3 Robustness of Nearest Neighbors

How robust is the k-nearest neighbor classiﬁer? We show that it depends on the value of k.
Speciﬁcally, we identify two distinct regimes – constant k and k = Ω(
dn log n) where d is
the data dimension – and show that nearest neighbors has diﬀerent robustness properties in
the two.

√

3.1 Low k Regime

In this region, k is a constant that does not depend on the training set size n. Provided
certain regularity conditions hold, we show that k-nearest neighbors is inherently non-robust

5

in this regime unless η(x) ∈ {0, 1} – in the sense that the distributional robustness becomes
0 in the large sample limit.

Theorem 3.1. Let x ∈ X ∩ supp(µ) such that (a) µ is absolutely continuous with respect
to the Lebesgue measure (b) η(x) ∈ (0, 1) (c) η is continuous with respect to the Euclidean
metric in a neighborhood of x. Then, for ﬁxed k, ρ(Ak(Sn, ·), x) converges in probability to
0.

Remarks. Observe that Theorem 3.1 implies that the distributional robustness (and hence
astuteness) in a region where η(x) ∈ (0, 1) is 0. This is in contrast with accuracy; for 1-NN,
the accuracy converges to 1 − 2R∗(1 − R∗) as n → ∞, where R∗ is the error rate of the
Bayes Optimal classiﬁer, and thus may be quite high.

The proof of Theorem 3.1 in the Appendix shows that the absolute continuity of µ with
respect to the Lebesgue measure is not strictly necessary; absolute continuity with respect
to an embedded manifold will give the same result, but will result in a more complex proof.
In the Appendix A (Theorem A.2), we show that k-nearest neighbor is astute in the

interior of the region where η ∈ {0, 1}, and provide ﬁnite sample rates for this case.

3.2 High k Regime

√

Prior work has shown that in the large sample limit, the accuracy of the nearest neighbor
classiﬁers converge to the Bayes Optimal, provided k is set properly. We next show that
if k is Ω(
dn log n), the regions of robustness and the astuteness of the k nearest neighbor
classiﬁers also approach the corresponding quantities for the Bayes Optimal classiﬁer as
n → ∞. Thus, if the Bayes Optimal classiﬁer is robust, then so is k-nearest neighbors in the
large sample limit.

The main intuition is that k = Ω(

dn log n) is large enough for uniform convergence –
where, with high probability, all Euclidean balls with k examples have the property that the
empirical averages of their labels are close to their expectations. This guarantees that for
any x, the k-nearest neighbor reports the same label as the Bayes Optimal classiﬁer for all
x(cid:48) close to x. Thus, if the Bayes Optimal classiﬁer is robust, so is nearest neighbors.

√

3.2.1 Deﬁnitions

We begin with some deﬁnitions that we can use to characterize the robustness of the Bayes
Optimal classiﬁer. Following [5], we use the notation Bo(x, r) to denote an open ball and
B(x, r) to denote a closed ball of radius r around x. We deﬁne the probability radius of a
ball around x as:

We next deﬁne the r-robust (p, ∆)-strict interiors as follows:

rp(x) = inf{r | µ(B(x, r)) ≥ p}

X +

r,∆,p = {x ∈ supp(µ) | ∀x(cid:48) ∈ Bo(x, r),

∀x(cid:48)(cid:48) ∈ B(x(cid:48), rp(x(cid:48))), η(x(cid:48)(cid:48)) > 1/2 + ∆}

X −

r,∆,p = {x ∈ supp(µ) | ∀x(cid:48) ∈ Bo(x, r),

∀x(cid:48)(cid:48) ∈ B(x(cid:48), rp(x(cid:48))), η(x(cid:48)(cid:48)) < 1/2 − ∆}

What is the signiﬁcance of these interiors? Let x(cid:48) be an instance such that all x(cid:48)(cid:48) ∈
n , then the k points x(cid:48)(cid:48) closest to x(cid:48) have

B(x(cid:48), rp(x(cid:48))) have η(x(cid:48)(cid:48)) > 1/2 + ∆. If p ≈ k

6

η(x(cid:48)(cid:48)) > 1/2 + ∆. Provided the average of the labels of these points is close to expectation,
which happens when k is large relative to 1/∆, k-nearest neighbor outputs label 1 on x(cid:48).
When x is in the r-robust (p, ∆)-strict interior region X +
r,∆,p, this is true for all x(cid:48) within
distance r of x, which means that k-nearest neighbors will be robust at x. Thus, the r-robust
(p, ∆)-strict interior is the region where we natually expect k-nearest neighbor to have
robustness radius r, when k is large relative to 1

∆ and p ≈ k
n .

Readers familiar with [5] will observe that the set of all x(cid:48) for which ∀x(cid:48)(cid:48) ∈ B(x(cid:48), rp(x(cid:48))), η(x(cid:48)(cid:48)) >

1/2 + ∆ forms a stricter version of the (p, ∆)-interiors of the 1 region that was deﬁned in
this work; these x(cid:48) also represent the region where k-nearest neighbors are accurate when
k ≈ max(np, 1/∆2). The r-robust (p, ∆)-strict interior is thus a somewhat stricter and more
robust version of this deﬁnition.

3.2.2 Main Results

We begin by characterizing where the Bayes Optimal classiﬁer is robust.

Theorem 3.2. The Bayes Optimal classiﬁer has robustness radius r at x ∈ X +
Moreover, its astuteness is E[η(x)1(x ∈ X +

r,0,0)] + E[(1 − η(x))1(x ∈ X −

r,0,0)].

r,0,0 ∪ X −

r,0,0.

The proof is in the Appendix, along with analogous results for astuteness. The following
theorem, along with a similar result for astuteness, proved in the Appendix, characterizes
robustness in the large k regime.

√

Theorem 3.3. For any n, pick a δ and a ∆n → 0. There exist constant C1 and C2 such that
if kn ≥ C1
n (1 + C2
≥ 1 − 3δ, kn-NN has robustness radius r in x ∈ X +

(cid:113) d log n+log(1/δ)
kn
∪ X −

), then, with probability

, and pn ≥ kn

dn log n+n log(1/δn)

∆n

.

r,∆n,pn

r,∆n,pn

Remarks. Some remarks are in order. First, observe that as n → ∞, ∆n and pn tend
to 0; thus, provided certain continuity conditions hold, X +
approaches
X +

r,∆n,pn
r,0,0, the robustness region of the Bayes Optimal classiﬁer.

r,0,0 ∪ X −

∪ X −

r,∆n,pn

Second, observe that as r-robust strict interiors extend the deﬁnition of interiors in [5],
Theorem 3.3 is a robustness analogue of Theorem 5 in this work. Unlike the latter, Theo-
rem 3.3 has a more stringent requirement on k. Whether this is necessary is left as an open
question for future work.

4 A Robust 1-NN Algorithm

√

Section 3 shows that nearest neighbors is robust for k as large as Ω(
dn log n). However,
this k is too high to use in practice – high values of k require even higher sample sizes [5],
and lead to higher running times. Thus a natural question is whether we can ﬁnd a more
robust version of the algorithm for smaller k. In this section, we provide a more robust
version of 1-nearest neighbors, and analytically demonstrate its robustness.

Our algorithm is motivated by the observation that 1-nearest neighbor is robust when
oppositely labeled points are far apart, and when test points lie close to training data. Most
training datasets however contain nearby points that are oppositely labeled; thus, we propose
to remove a subset of training points to enforce this property.

Which points should we remove? A plausible approach is to keep the largest subset where
oppositely labeled points are far apart; however, this subset has poor stability properties

7

even for large n. Therefore, we propose to keep all points x such that: (a) we are highly
conﬁdent about the label of x and its nearby points and (b) all points close to x have the
same label. Given that all such x are kept, we remove as few points as possible, and execute
nearest neighbors on the remaining dataset.

The following deﬁnition characterizes data where oppositely labeled points are far apart.

Deﬁnition 4.1 (r-separated set). A set A = {(x1, y1), . . . , (xm, ym)} of labeled examples is
said to be r-separated if for all pairs (xi, yi), (xj, yj) ∈ A, (cid:107)xi − xj(cid:107) ≤ r implies yi = yj.

The full algorithm is described in Algorithm 1 and Algorithm 2. Given conﬁdence
parameters ∆ and δ, Algorithm 2 returns a 0/1 label when this label agrees with the average
of kn points closest to x; otherwise, it returns ⊥. kn is chosen such that with probability
≥ 1 − δ, the empirical majority of kn labels agrees with the majority in expectation, provided
the latter is at least ∆ away from 1
2 .

Algorithm 2 is used to determine whether an xi should be kept. Let f (xi) be the output
of Algorithm 2 on xi. If yi = f (xi) and if for all xj ∈ B(xi, r), f (xi) = f (xj) = yi, then
we mark xi as red. Finally, we compute the largest r-separated subset of the training data
that includes all the red points; this reduces to a constrained matching problem as in [16].
The resulting set, returned by Algorithm 1, is our new training set. We observe that this
set is r-separated from Lemma B.2 in the Appendix, and thus oppositely labeled points are
far apart. Moreover, we keep all (xi, yi) when we are conﬁdent about the label of xi and
its nearby points. Observe that our ﬁnal procedure is a 1-NN algorithm, even though kn
neighbors are used to determine if a point should be retained in the training set.

Algorithm 1 Robust_1NN(Sn, r, ∆, δ, x)

f (xi) = Conﬁdent-Label(Sn, ∆, δ, xi)

for (xi, yi) ∈ Sn do

end for
SRED = ∅
for (xi, yi) ∈ Sn do

if f (xi) = yi and f (xi) = f (xj) for all xj such that (cid:107)xi − xj(cid:107) ≤ r and (xj, yj) ∈ Sn
then

SRED = SRED

(cid:83){(xi, yi)}

end if
end for
Let S(cid:48) be the largest 2r-separated subset of Sn that contains all points in SRED.
return new training set S(cid:48)

Algorithm 2 Conﬁdent-Label(Sn, ∆, δ, x)

kn = 3 log(2n/δ)/∆2
¯y = (1/kn) (cid:80)kn
2 − ∆, 1
if ¯y ∈ [ 1

i=1 Y (i)(x)
2 + ∆] then

return ⊥

else

end if

return 1

2 sgn(¯y − 1

2 ) + 1

2

8

4.1 Performance Guarantees

The following theorem establishes performance guarantees for Algorithm 1.

Theorem 4.2. Pick a ∆n and δ, and set kn = 3 log(2n/δ)/∆2
n. Pick a margin parameter
τ . Then, there exist constants C and C0 such that the following hold. If we set pn =
kn
n (1 + C

(cid:113) d log n+log(1/δ)
kn

), and deﬁne the set:

XR =

(cid:40)

(cid:12)
(cid:12)x ∈ X +
(cid:12)
x

r+τ,∆n,pn

r+τ,∆n,pn

∪ X −

,

µ(B(x, τ )) ≥

(d log n + log(1/δ))

2C0
n

(cid:41)

Then, with probability ≥ 1 − 2δ over the training set, Algorithm 1 run with parameters r, ∆n
and δ has robustness radius at least r − 2τ on XR.

Remarks. The proof is in the Appendix, along with an analogous result for astuteness.
Observe that XR is roughly the high density subset of the r + τ -robust strict interior
X +
2 + ∆n or less than
r+τ,∆n,pn
1
2 − ∆n in this region, as opposed to 0 or 1, this is an improvement over standard nearest
neighbors when the data distribution has a large high density region that intersects with the
interiors.

. Since η(x) is constrained to be greater than 1

r+τ,∆n,pn

∪ X −

A second observation is that as τ is an arbitrary constant, we can set to it be quite small
and still satisfy the condition on µ(B(x, τ )) for a large fraction of x’s when n is very large.
This means that in the large sample limit, r − 2τ may be close to r and XR may be close to
the high density subset of X +

for a lot of smooth distributions.

∪ X −

r,∆n,pn

r,∆n,pn

5 Experiments

The results in Section 4 assume large sample limits. Thus, a natural question is how well
Algorithm 1 performs with more reasonable amounts of training data. We now empirically
investigate this question.

Since there are no general methods that certify robustness at an input, we assess
robustness by measuring how our algorithm performs against a suite of standard attack
methods. Speciﬁcally, we consider the following questions:

1. 1. How does our algorithm perform against popular white box and black box attacks

compared with standard baselines?

2. 2. How is performance aﬀected when we change the training set size relative to the

data dimension?

These questions are considered in the context of three datasets with varying training set
sizes relative to the dimension, as well as two standard white box attacks and black box
attacks with two kinds of substitute classiﬁers.

9

Figure. 1: White Box Attacks: Plot of classiﬁcation accuracy on adversarial examples v.s.
attack radius. Top row: Direct Attack. Bottom row: Kernel Substitute Attack. Left to right:
1) Halfmoon, 2) MNIST 1v 7 and 3) Abalone.

Figure. 2: Black Box Attacks: Plot of classiﬁcation accuracy on adversarial examples v.s.
attack radius. Top to Bottom: 1) kernel substitute, 2) neural net substitute. Left to right:
1) Halfmoon, 2) MNIST 1 v.s. 7 and 3) Abalone.

5.1 Methodology

5.1.1 Data

We use three datasets – Halfmoon, MNIST 1v7 and Abalone – with diﬀering data sizes
relative to dimension. Halfmoon is a popular 2-dimensional synthetic data set for non-linear
classiﬁcation. We use a training set of size 2000 and a test set of size 1000 generated with
standard deviation σ = 0.2. The MNIST 1v7 data set is a subset of the 784-dimensional
MNIST data. For training, we use 1000 images each of Digit 1 and 7, and for test, 500 images
of each digit. Finally, for the Abalone dataset [18], our classiﬁcation task is to distinguish
whether an abalone is older than 12.5 years based on 7 physical measurements. For training,
we use 500 and for test, 100 samples. In addition, a validation set with the same size as the

10

test set is generated for each experiment for parameter tuning.

5.1.2 Baselines

We compare Algorithm 1, denoted by RobustNN, against three baselines. The ﬁrst is
the standard 1-nearest neighbor algorithm, denoted by StandardNN. We use two forms
of adversarially-trained nearest neighbors - ATNN and ATNN-all. Let S be the training
set used by standard nearest neighbors. In ATNN, we augment S by creating, for each
(x, y) ∈ S, an adversarial example xadv using the attack method in the experiment, and
adding (xadv, y). The ATNN classiﬁer is 1-nearest neighbor on this augmented data. In
ATNN-all, for each (x, y) ∈ S, we create adversarial examples using all the attack methods
in the experiment, and add them all to S. ATNN-all is the nearest neighbor classiﬁer on this
augmented data. For example, for white box Direct Attacks in Section 5.2, ATNN includes
adversarial examples generated by the Direct Attack, and ATNN-all includes adversarial
examples generated by both Direct and Kernel Substitute Attacks.

Observe that all algorithms except StandardNN have parameters to tune. RobustNN
has three input parameters – ∆, δ and a defense radius r which is an approximation to the
robustness radius. For simplicity, we set ∆ = 0.45, δ = 0.1 and tune r on the validation set;
this can be viewed as tuning the parameter τ in Theorem 4.2. For ATNN and ATNN-all, the
methods that generate the augmenting adversarial examples need a perturbation magnitude
r; we call this the defense radius. To be fair to all algorithms, we tune the defense radius
for each. We consider the adversary with the highest attack perturbation magnitude in the
experiment, and select the defense radius that yields the highest validation accuracy against
this adversary.

5.2 White-box Attacks and Results

To evaluate the robustness of Algorithm 1, we use two standard classes of attacks – white
box and black box. For white-box attacks, the adversary knows all details about the classiﬁer
under attack, including its training data, the training algorithm and any hyperparameters.

5.2.1 Attack Methods

We consider two white-box attacks – direct attack [2] and Kernel Substitute Attack [24].
Direct Attack. This attack takes as input a test example x, an attack radius r, and a
training dataset S (which may be an augmented or reduced dataset). It ﬁnds an x(cid:48) ∈ S that is
closest to x but has a diﬀerent label, and returns the adversarial example xadv = x+r x−x(cid:48)
.
||x−x(cid:48)||2
Kernel Substitute Attack. This method attacks a substitute kernel classiﬁer trained on
the same training set. For a test input (cid:126)x, a set of training points Z with one-hot labels Y , a
kernel classiﬁer f predicts the class probability as:

The adversary trains a kernel classiﬁer on the training set of the corresponding nearest
neighbors, and then generates adversarial examples against this kernel classiﬁer. The
advantage is that the prediction of the kernel classiﬁer is diﬀerentiable, which allows the
use of standard gradient-based attack methods. For our experiments, we use the popular

f : (cid:126)x →

2/c(cid:105)

(cid:104)

e−||(cid:126)x−(cid:126)z||2
(cid:80)

(cid:126)z∈X e−||(cid:126)x−(cid:126)z||2

(cid:126)z∈X
2/c

· Y

11

fast-gradient-sign method (FSGM). The parameter c is tuned to yield the most eﬀective
attack, and is set to 0.1 for Halfmoon and MNIST, and 0.01 for Abalone.

5.2.2 Results

Figure 1 shows the results. We see that RobustNN outperforms all baselines for Halfmoon
and Abalone for all attack radii. For MNIST, for low attack radii, RobustNN’s classiﬁcation
accuracy is slightly lower than the others, while it outperforms the others for large attack
radii. Additionally, as is to be expected, the Direct Attack results in lower general accuracy
than the Kernel Substitute Attack.

These results suggest that our algorithm mostly outperforms the baselines StandardNN,
ATNN and ATNN-all. As predicted by theory, the performance gain is higher when the
training set size is large relative to the dimension – which is the setting where nearest
neighbors work well in general. It has superior performance for Halfmoon and Abalone,
where the training set size is large to medium relative to dimension. In contrast, in the
sparse dataset MNIST, our algorithm has slightly lower classiﬁcation accuracy for small
attack radii, and higher otherwise.

5.3 Black-box Attacks and Results

[25] has observed that some defense methods that work by masking gradients remain highly
amenable to black box attacks. In this attack, the adversary is unaware of the target classiﬁer’s
nature, parameters or training data, but has access to a seed dataset drawn from the same
distribution which they use to train and attack a substitute classiﬁer. To establish robustness
properties of Algorithm 1, we therefore validate it against black box attacks based on two
types of substitute classiﬁers.

5.3.1 Attack Methods

We use two types of substitute classiﬁers – kernel classiﬁers and neural networks. The
adversary trains the substitute classiﬁer using the method of [25] and uses the adversarial
examples against the substitute to attack the target classiﬁer.

Kernel Classiﬁer. The kernel classiﬁer substitute is the same as the one in Section 5.2,
but trained using the seed data and the method of [25].
Neural Networks. The neural network for MNIST is the ConvNet in [23]’s tutorial. For
Halfmoon and Abalone, the network is a multi-layer perceptron with 2 hidden layers.
Procedure. To train the substitute classiﬁer, the adversary uses the method of [24] to
augment the seed data for two rounds; labels are obtained by querying the target classiﬁer.
Adversarial examples against the substitutes are created by FGSM, following [24]. As a
sanity check, we verify the performance of the substitute classiﬁers on the original training
and test sets. Details are in Table 1 in the Appendix. Sanity checks on the performance of
the substitute classiﬁers are presented in Table 1 in the Appendix.

5.3.2 Results

Figure 2 shows the results. For all algorithms, black box attacks are less eﬀective than white
box, which corroborates the results of [24], who observed that black-box attacks are less
successful against nearest neighbors. We also ﬁnd that the kernel substitute attack is more
eﬀective than the neural network substitute, which is expected as kernel classiﬁers have

12

similar structure to nearest neighbors. Finally, for Halfmoon and Abalone, our algorithm
outperforms the baselines for both attacks; however, for MNIST neural network substitute,
our algorithm does not perform as well for small attack radii. This again conﬁrms the
theoretical predictions that our algorithm’s performance is better when the training set is
large relative to the data dimension – the setting in which nearest neighbors work well in
general.

5.4 Discussion

The results show that our algorithm performs either better than or about the same as
standard baselines against popular white box and black box attacks. As expected from
our theoretical results, it performs better for denser datasets which have high or medium
amounts of training data relative to the dimension, and either slightly worse or better for
sparser datasets, depending on the attack radius. Since non-parametric methods such as
nearest neighbors are mostly used for dense data, this suggests that our algorithm has good
robustness properties even with reasonable amounts of training data.

6 Conclusion

We introduce a novel theoretical framework for learning robust to adversarial examples, and
introduce notions of distributional and ﬁnite-sample robustness. We use these notions to
analyze a non-parametric classiﬁer, k-nearest neighbors, and introduce a novel modiﬁed
1-nearest neighbor algorithm that has good robustness properties in the large sample limit.
Experiments show that these properties are still retained for reasonable data sizes.

Many open questions remain. The ﬁrst is to close the gap in analysis of k-nearest
neighbors for k in between our two regimes. The second is to develop nearest neighbor
algorithms with better robustness guarantees. Finally, we believe that our work is a ﬁrst
step towards a comprehensive analysis of how the size of training data aﬀects robustness;
we believe that an important line of future work is to carry out similar analyses for other
supervised learning methods.

Acknowledgement

We thank NSF under IIS 1253942 for support. This work was also partially supported by
ARO under contract number W911NF-1-0405. We thank all anonymous reviewers for their
constructive comments.

We also thank Rabanus Derr from University of Tübingen for pointing about a minor

mistake in the proof of Lemma B.3. The proof is ﬁxed in the latest version.

References

[1] John Duchi Aman Sinha, Hongseok Namkoong. Certiﬁable distributional robustness with
principled adversarial training. International Conference on Learning Representations,
2018.

13

[2] Laurent Amsaleg, James Bailey, Sarah Erfani, Teddy Furon, Michael E Houle, Miloš
Radovanović, and Nguyen Xuan Vinh. The vulnerability of learning to adversarial
perturbation increases with intrinsic dimensionality. 2016.

[3] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndić, Pavel
Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at
test time. In Joint European Conference on Machine Learning and Knowledge Discovery
in Databases, pages 387–402. Springer, 2013.

[4] Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for the cluster tree.
In J. D. Laﬀerty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta,
editors, Advances in Neural Information Processing Systems 23, pages 343–351. Curran
Associates, Inc., 2010.

[5] Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for nearest neighbor
classiﬁcation. In Advances in Neural Information Processing Systems, pages 3437–3445,
2014.

[6] T. Cover and P.E. Hart. Nearest neighbor pattern classiﬁcation. IEEE Transactions on

Information Theory, 13:21–27, 1967.

[7] Luc Devroye, Laszlo Gyorﬁ, Adam Krzyzak, and Gabor Lugosi. On the strong universal
consistency of nearest neighbor regression function estimates. The Annals of Statistics,
pages 1371–1385, 1994.

[8] Luc P Devroye and Terry J Wagner. The strong uniform consistency of nearest neighbor

density estimates. The Annals of Statistics, pages 536–540, 1977.

[9] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of
classiﬁers: from adversarial to random noise. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems
29, pages 1632–1640. Curran Associates, Inc., 2016.

[10] Jerome Friedman, Trevor Hastie, Robert Tibshirani, et al. Additive logistic regression:
a statistical view of boosting (with discussion and a rejoinder by the authors). The
annals of statistics, 28(2):337–407, 2000.

[11] Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu, Martin
Wattenberg, and Ian Goodfellow. Adversarial spheres. arXiv preprint arXiv:1801.02774,
2018.

[12] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing

adversarial examples. arXiv preprint arXiv:1412.6572, 2014.

[13] Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness
of a classiﬁer against adversarial manipulation. In Advances in Neural Information
Processing Systems, pages 2263–2273, 2017.

[14] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer.
Towards proving the adversarial robustness of deep neural networks. arXiv preprint
arXiv:1709.02802, 2017.

14

[15] J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the

convex outer adversarial polytope. arXiv preprint arXiv:1711.00851, 2017.

[16] Aryeh Kontorovich and Roi Weiss. A bayes consistent 1-nn classiﬁer. In Artiﬁcial

Intelligence and Statistics Conference, 2015.

[17] S. Kulkarni and S. Posner. Rates of convergence of nearest neighbor estimation under
arbitrary sampling. IEEE Transactions on Information Theory, 41(4):1028–1039, 1995.

[18] M. Lichman. UCI machine learning repository, 2013.

[19] Daniel Lowd and Christopher Meek. Adversarial learning. In Proceedings of the eleventh
ACM SIGKDD international conference on Knowledge discovery in data mining, pages
641–647. ACM, 2005.

[20] Aleksander Mądry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian
Vladu. Towards deep learning models resistant to adversarial attacks. stat, 1050:9, 2017.

[21] Michael Mitzenmacher and Eli Upfal. Probability and computing: Randomized algorithms

and probabilistic analysis. Cambridge university press, 2005.

[22] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a
simple and accurate method to fool deep neural networks. IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2016.

[23] Nicolas Papernot, Nicholas Carlini, Ian Goodfellow, Reuben Feinman, Fartash Faghri,
Alexander Matyasko, Karen Hambardzumyan, Yi-Lin Juang, Alexey Kurakin, Ryan
Sheatsley, Abhibhav Garg, and Yen-Chen Lin. cleverhans v2.0.0: an adversarial machine
learning library. arXiv preprint arXiv:1610.00768, 2017.

[24] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine
learning: from phenomena to black-box attacks using adversarial samples. arXiv preprint
arXiv:1605.07277, 2016.

[25] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Berkay Celik, and
Ananthram Swami. Practical black-box attacks against deep learning systems using
adversarial examples. In Proceedings of the 2017 ACM Asia Conference on Computer
and Communications Security, 2017.

[26] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik,
and Ananthram Swami. The limitations of deep learning in adversarial settings. In
Proceedings of the 1st IEEE European Symposium on Security and Privacy. arXiv
preprint arXiv:1511.07528, 2016.

[27] C. Stone. Consistent nonparametric regression. Annals of Statistics, 5:595–645, 1977.

[28] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint
arXiv:1312.6199, 2013.

15

A Proofs from Section 3

A.1 Proofs for Constant k

Proof. (Of Theorem 3.1) To show convergence in probability, we need to show that for all
(cid:15), δ > 0, there exists an n((cid:15), δ) such that Pr(ρ(Ak(Sn, ·), x) ≥ (cid:15)) ≤ δ for n ≥ n0((cid:15), δ).

The proof will again proceed in two stages. First, we show in Lemma A.1 that if the
conditions in the statement of Theorem 3.1 hold, then there exists some n((cid:15), δ) such that
for n ≥ n((cid:15), δ), with probability at least 1 − δ, there exists two points x+ and x− in B(x, (cid:15))
such that (a) all k nearest neighbors of x+ have label 1, (b) all k nearest neighbors of x−
have label 0, and (c) x+ (cid:54)= x−.

Next we show that if the event stated above happens, then ρ(Ak(Sn, ·), x) ≤ (cid:15). This is
because Ak(Sn, x+) = 1 and Ak(Sn, x−) = 0. No matter what Ak(Sn, x) is, we can always
ﬁnd a point x(cid:48) that lies in {x+, x−} ⊂ B(x, (cid:15)) such that the prediction at x(cid:48) is diﬀerent from
Ak(Sn, x).

Lemma A.1. If the conditions in the statement of Theorem 3.1 hold, then there exists
some n((cid:15), δ) such that for n ≥ n((cid:15), δ), with probability at least 1 − δ, there are two points x+
and x− in B(x, (cid:15)) such that (a) all k nearest neighbors of x+ have label 1, (b) all k nearest
neighbors of x− have label 0, and (c) x+ (cid:54)= x−.

Proof. (Of Lemma A.1) The proof consists of two major components. First, for large enough
n, with high probability there are many disjoint balls in the neighborhood of x such that
each ball contains at least k points in Sn. Second, with high probability among these balls,
there exists a ball such that the k neareast neighbors of its center all have label 1. Similarly,
there exists a ball such that the k nearest neighbor of its center all have label 0.

Since µ is absolutely continuous with respect to Lebesgue measure in the neighbor-
bood of x and η is continuous, then for any m ∈ Z+, we can always ﬁnd m balls
B(x1, r1), · · · , B(xm, rm) such that (a) all m balls are disjoint, and (b) for all i ∈ {1, · · · , m},
we have xi ∈ B(x, (cid:15)), µ(B(xi, ri)) > 0 and η(x) ∈ (0, 1) for x ∈ B(xi, ri). For simplicity, we
(cid:84) Sn. Also, let
use Bi to denote B(xi, ri) and ci(n) to denote the number of points in Bi
µmin = mini∈{1,··· ,m} µ(Bi). Then by Hoeﬀding’s inequality, for each ball Bi and for any
n > k+1
µmin

,

Pr[ci(n) < k] ≤ exp(−2nµ2

min/(k + 1)2),

where the randomness comes from drawing sample Sn. Then taking the union bound over
all m balls, we have

Pr[∃i ∈ {1, · · · , m} such that ci(n) < k] ≤ m exp(−2nµ2
(cid:16) k+1
µmin

, [log m−log(δ/3)](k+1)2
which implies that when n > max
µ2
1 − δ/3, each of B1, · · · , Bm contains at least k points in Sn.

(cid:17)

min

min/(k + 1)2),

(2)

, with probability at least

An important consequence of the above result is that with probability at least 1 − δ/3,
the set of k nearest neighbors of each center xi of Bi is completely diﬀerent from another
center xj’s, so the labels of xi’s k nearest neighbors are independent of the labels of xj’s k
nearest neighbors.

Now let ηmin,+ = minx∈B1

(cid:83)··· (cid:83) Bm(1 − η(x)). Both
ηmin,+ and ηmin,− are greater than 0 by the construction requirements of B1, · · · , Bm. For
any xi,

(cid:83)··· (cid:83) Bm η(x) and ηmin,− = minx∈B1

Pr[xi’s k nearest neighbors all have label 1] ≥ ηk

min,+

16

Then,

Pr[∃i ∈ {1, · · · , m}s.t. xi’s k nearest neighbor all have label 1] ≥ 1 − (1 − ηk

min,+)m,

(3)

which implies when m ≥
s.t. its k nearest neighbors all have label 1. This xi is our x+.

min,+) , with probability at least 1 − δ/3, there exists an xi

log(1−ηk

log δ/3

Similarly,

Pr[∃i ∈ {1, · · · , m} s.t. xi’s k nearest neighbor all have label 0] ≥ 1 − (1 − ηk

min,−)m, (4)

log δ/3

and when m ≥
nearest neighbors all have label 0. This xi is our x−.
Combining the results above, we show that for

log(1−ηk

min,−) , with probability at least 1 − δ/3, there exists an xi s.t. its k

n > max

(cid:18) k + 1
µmin

,

[log m − log(δ/3)](k + 1)2
µ2

min

(cid:32)

m ≥ max

log δ/3
log(1 − ηk

min,+)

,

log δ/3
log(1 − ηk

min,−)

(cid:19)

(cid:33)

,

,

with probability at least 1 − δ, the statement in Lemma A.1 is satisﬁed.

A.2 Theorem and proof for k-nn robustness lower bound.

Theorem 3.1 shows that k-NN is inherently non-robust in the low k regime if η(x) ∈ (0, 1). On
the contrary, k-NN can be robust at x if η(x) ∈ {0, 1}. We deﬁne the r-robust (p, ∆)-interior
as follows:

ˆX +

r,∆,p = {x ∈ supp(µ)|∀x(cid:48) ∈ Bo(x, r),

∀x(cid:48)(cid:48) ∈ B(x(cid:48), rp(x(cid:48))), η(x(cid:48)(cid:48)) ≥ 1/2 + ∆}

ˆX −

r,∆,p = {x ∈ supp(µ)|∀x(cid:48) ∈ Bo(x, r),

∀x(cid:48)(cid:48) ∈ B(x(cid:48), rp(x(cid:48))), η(x(cid:48)(cid:48)) ≤ 1/2 − ∆}

The deﬁnition is similar to the strict r-robust (p, ∆)-interior in Section 4, except replacing <
and > with ≤ and ≥. Theorem A.2 show that k-NN is robust at radius r in the r-robust
(1/2, p)-interior with high high probability. Corollary A.3 shows the ﬁnite sample rate of the
robustness lowerbound.

Theorem A.2. Let x ∈ X ∩ supp(µ) such that (a) µ is absolutely continuous with respect
to the Lebesgue measure (b) η(x) ∈ {0, 1}. Then, for ﬁxed k, there exists an n0 such that for
n ≥ n0,

for all x in ˆX +

r,1/2,p

(cid:83) ˆX −

r,1/2,p for all p > 0, δ > 0.

Pr[ρ(Ak(Sn, ·), x) ≥ r] ≥ 1 − δ

In addition, with probability at least 1 − δ, the astuteness of the k-NN classiﬁer is at least:

E(1(X ∈ ˆX +

r,1/2,p

(cid:91) ˆX −

r,1/2,p))

17

Proof. The k-NN classiﬁer Ak(Sn, ·) is robust at radius r at x if for every x(cid:48) ∈ Bo(x, r), a)
there are k training points in B(x(cid:48), rp(x(cid:48))), and b) more than (cid:98)k/2(cid:99) of them have the same
label as Ak(Sn, x). Without loss of generality, we look at a point x ∈ ˆX +
r,1/2,p. The second
condition is satisﬁed since η(x) = 1 for all training points in B(x(cid:48), rp(x(cid:48))) by the deﬁnition
of ˆX +

r,1/2,p.
It remains to check the ﬁrst condition. Let B be a ball in Rd and n(B) be the number of
training points in B. Lemma 16 of [4] suggests that with probability at least 1 − δ, for all B
in Rd,

µ(B) ≥

+

d log n + log

+

k

d log n + log

(5)

k
n

(cid:32)

Co
n

(cid:115)

(cid:18)

1
δ

(cid:19)(cid:33)

1
δ

implies n(B) ≥ k, where Co is a constant term. Let B = B(x(cid:48), rp(x(cid:48))). By the deﬁnition of
rp, µ(B) ≥ p > 0. Then as n → ∞, Inequality 5 will eventually be satisﬁed, which implies B
contains at least k training points. The ﬁrst condition is then met.
The astuteness result follows because Ak(Sn, x) = y = 1 in ˆX +
r,1/2,p with probability 1.

r,1/2,p and Ak(Sn, x) = y = 0

in ˆX −

Corollary A.3. For n ≥ max(104, c4

d,k,δ/[(k + 1)2p2]) where

cd,k,δ = 4(d + 1) + (cid:112)16(d + 1)2 + 8(ln(8/δ) + k + 1)

, with probability at least 1 − 2δ, ρ(Ak(Sn, x)) ≥ r for all x in ˆX +
p > 0, δ > 0.

r,1/2,p

(cid:83) ˆX −

r,1/2,p and for all

In addition, with probability at least 1 − 2δ, the astuteness of the k-NN classiﬁer is at

least:

E(1(X ∈ ˆX +

r,1/2,p

(cid:91) ˆX −

r,1/2,p))

Proof. Without loss of generality, we look at a point x ∈ ˆX +
r,1/2,p. Let B = B(x(cid:48), rp(x(cid:48))),
J(B) = E(Y · 1(X ∈ B)) and ˆJ(B) be the empirical estimation of J(B). Notice that ˆJ(B)n
is the number of training points in B, because η(x) = 1 for all x ∈ B by the deﬁnition of
r-robust (1/2, p)-interior. It remains to ﬁnd a threshold n such that for all n(cid:48) > n,

By Lemma A.5, with probability 1 − 2δ,

ˆJ(B) ≥ (k + 1)/n(cid:48)

ˆJ(B) ≥ p − 2βn

p − 2β2
n

√

for all B ∈ Rd.

Therefore it suﬃces to ﬁnd a threshold n that satisﬁes

√

p − 2βn

p − 2β2

n ≥ (k + 1)/n,

where βn = (cid:112)(4/n)((d + 1) ln 2n + ln(8/δ)).
Solving this quadratic inequality yields

√

−

p + (cid:112)3p + (k + 1)/n

,

βn ≤

2

18

(6)

(7)

(8)

(9)

(10)

(11)

(12)

(13)

which can be re-written as

√

(8/

n)[(d + 1) ln(2n) + ln(8/δ) + (k + 1)/8] ≤ (cid:112)(k + 1)p

by substituting the expression for βn. This inequality does not admit an analytic solution.
Nevertheless, we observe that n1/4 ≥ ln(2n) for all n ≥ 104. Therefore it suﬃces to ﬁnd an
n ≥ 104 such that

√

(8/

n)[(d + 1)n1/4 + ln(8/δ) + (k + 1)/8] ≤ (cid:112)(k + 1)p.

Let m = n1/4. Inequality 11 can be re-written as

(cid:112)(k + 1)pm2 − 8(d + 1)m − (8 ln(8/δ) + (k + 1)) ≥ 0.

Solving this quadratic inequality with respct to m gives

m ≥

4(d + 1) + (cid:112)16(d + 1)2 + 8(ln(8/δ) + k + 1)
(cid:112)(k + 1)p

.

Letting

cd,k,δ = 4(d + 1) + (cid:112)16(d + 1)2 + 8(ln(8/δ) + k + 1)

, we ﬁnd a desired threshold

n = max(104, m4) ≥ max(104, c4

d,k,δ/[(k + 1)2p2]).

(14)

The astuteness result follows in a similar way to Theorem A.2.

A.3 Proofs for High k

A.3.1 Robustness of the Bayes Optimal Classiﬁer

Proof. (Of Theorem 3.2) Suppose x ∈ X +
r,0,0. Then, g(x) = 1. Consider any x(cid:48) ∈ Bo(x, r);
by deﬁnition, η(x(cid:48)) > 1/2, which implies that g(x(cid:48)) = 1 as well. Thus, ρ(g, x) ≥ r. The other
case (x ∈ X −

r,0,0) is symmetric.

Consider an x ∈ X +

r,0,0 (the other case is symmetric). We just showed that g has
robustness radius ≥ r at x. Moreover, p(y = 1 = g(x)|x) = η(x); therefore, g predicts the
correct label at x with probability η(x). The theorem follows by integrating over all x in
X +

r,0,0 ∪ X −

r,0,0.

A.3.2 Robustness of k-Nearest Neighbor

We begin by stating and proving a more technical version of Theorem 3.3.

Theorem A.4. For any n and data dimension d, deﬁne:

C0
n

(cid:114)

an =

(d log n + log(1/δ))

d log n + log(1/δ)
n

bn = C0
βn = (cid:112)(4/n)((d + 1) ln 2n + ln(8/δ))

19

where C0 is the constant in Theorem 15 of [4]. Now, pick kn and ∆n so that ∆n → 0 and
the following condition is satisﬁed:

and set

kn
n

≥

2βn + bn + (cid:112)(2βn + bn)2 + 2∆n(2β2

n + an)

∆n

pn =

(cid:16)

+

C0
n

d log n + log(1/δ)

kn
n
(cid:17)
+(cid:112)kn(d log n + log(1/δ)

Then, with probability ≥ 1 − 3δ, kn-NN has robustness radius r at all x ∈ X +
. In addition, with probability ≥ 1 − δ, the astuteness of kn-NN is at least:

r,∆n,pn

∪

X −

r,∆n,pn

E[η(X) · 1(X ∈ X +

)] + E(1 − η(X)) · 1(X ∈ X −

r,∆n,pn

r,∆n,pn

)]

Before we prove Theorem A.4, we need some deﬁnitions and lemmas.
For any Euclidean ball B in Rd, deﬁne J(B) = E[Y · 1(X ∈ B)] and ˆJ(B) as the

corresponding empirical quantity.

Lemma A.5. With probability ≥ 1 − 2δ, for all balls B in Rd, we have:

|J(B) − ˆJ(B)| ≤ 2β2

n + 2βn min((cid:112)J(B),

(cid:113)

ˆJ(B)),

where βn = (cid:112)(4/n)((d + 1) ln 2n + ln(8/δ)).
Proof. (Of Lemma A.5) Consider the two functions: h+
h−
B(x, y) = 1(y = −1, x ∈ B). From Lemma A.6, both h+
VC dimension at most d + 1. Additionally, J(B) = E[h+
of [4], along with an union bound gives the lemma.

B(x, y) = 1(y = 1, x ∈ B) and
B and h−
B are 0/1 functions with
B] − E[h−
B]. Applying Theorem 15

Lemma A.6. For an Euclidean ball B in Rd, deﬁne the function h+
as:

B : Rd×{−1, +1} → {0, 1}

h+
B(x, y) = 1(y = 1, x ∈ B)

and let HB = {h+
most d + 1.

B} be the class of all such functions. Then the VC-dimension of HB is at

Proof. (Of Lemma A.6) Let U be a set of d + 2 points in Rd; as the VC dimension of balls
in Rd is d + 1, U cannot be shattered by balls in Rd. Let UL = {(x, y)|x ∈ U } be a labeling
of U that cannot be achieved by any ball (with pluses inside and minuses outside); the
corresponding d + 1-dimensional points cannot be labeled accordingly by h+
B. Since U is
an arbitrary set of d + 2 points, this implies that any set of d + 2 points in Rd × {−1, +1}
cannot be shattered by HB. The lemma follows.

Lemma A.7. Let δp = C0
n
bility ≥ 1−δ, for all x, (cid:107)x−X(k+1)(x)(cid:107) ≤ rk/n+δp (x), and µ(B(x, (cid:107)x−X(k+1)(x)(cid:107))) ≥ k

. Then, with proba-
n −δp.

(cid:17)
d log n + log(1/δ) + (cid:112)k(d log n + log(1/δ)

(cid:16)

20

Figure. 3: Visualization of the halfmoon dataset. 1) Training sample of size n = 2000,
2) subset selected by Robust_1NN with defense radius r = 0.1, 3) subset selected by
Robust_1NN with defense radius r = 0.2.

Proof. (Of Lemma A.7) Observe that by deﬁnition for any x, rp is the smallest r such that
µ(B(x, rp(x)) ≥ p. The rest of the proof follows from Lemma 16 of [4].

Proof. (Of Theorem A.4)

From Lemma A.7, by uniform convergence of ˆµ, with probability ≥ 1 − δ, for all x(cid:48),
(cid:107)x(cid:48) − X (kn)(x(cid:48))(cid:107) ≤ rpn (x(cid:48)) and µ(B(x, (cid:107)x − X (kn)(x)(cid:107))) ≥ kn
,
r,∆n,pn
this implies that for all ˜x ∈ B(x(cid:48), X (kn)(x(cid:48))), η(˜x) ≥ 1/2 + ∆. Therefore, for such an
x(cid:48), J(B(x(cid:48), X (kn)(x(cid:48)))) ≥ ( 1
2 + ∆n)µ(B(x(cid:48), X (kn)(x(cid:48)))) ≥ ( 1
2 + ∆n)(kn/n − δp). Since for
B(x(cid:48), X (kn)(x(cid:48))), ˆµ(B(x(cid:48), X (kn)(x(cid:48)))) = kn
n . Thus we can apply Lemma A.5
to conclude that

n , min( ˆJ, J) ≤ k

If x(cid:48) ∈ X +

n − δp.

ˆJ(B) > J(B) − 2β2

n − 2βn

(cid:112)kn/n >

kn
2n

,

which implies that ˆY (B) = 1
kn
follows.

(cid:80)kn

i=1 Y (i)(x) = n
kn

ˆJ(B) > 1

2 . The ﬁrst part of the theorem

For the second part, observe that for an x ∈ X +

probability η(x) and for an x ∈ X −
Combining this with the ﬁrst part completes the proof.

r,∆n,pn

, the label Y is equal to +1 with
, the label Y is equal to −1 with probability 1 − η(x).

r,∆n,pn

B Proofs from Section 4

We begin with a statement of Chernoﬀ Bounds that we use in our calculations.

Theorem B.1.

[21] Let Xi be a 0/1 random variable and let X = 1
m

(cid:80)m

i=1 Xi. Then,

Pr(|X − E[X]| ≥ δ) ≤ e−mδ2/2 + e−mδ2/3 ≤ 2e−mδ2/3

Lemma B.2. Suppose we run Algorithm 1 with parameter r. Then, the points marked as
red by the algorithm form an r-separated subset of the training set.

21

Figure. 4: Adversarial examples of MNIST digit 1 images created by diﬀerent attack methods.
Top row: clean digit 1 test images. Middle row from left to right: 1) direct attack, 2)
white-box kernel attack. Bottom row from left to right: 1) black-box kernel attack, 2)
black-box neural net substitute attack.

Proof. Let f (xi) denote the output of Algorithm 2 on xi. If (xi, 1) is a Red point, then
f (xi) = 1 = f (xj) for all xj ∈ B(x, r); therefore, (xj, −1) cannot be marked as Red by the
algorithm as f (xj) (cid:54)= yj. The other case, where (xi, −1) is a Red point is similar.

Lemma B.3. Let x ∈ X such that Algorithm 1 ﬁnds a Red xi within Bo(x, τ ). Then,
Algorithm 1 has robustness radius at least r − 2τ at x.

Proof. For all x(cid:48) ∈ B(x, τ ), we have:

(cid:107)x(cid:48) − xi(cid:107) ≤ (cid:107)x − xi(cid:107) + (cid:107)x − x(cid:48)(cid:107) < 2τ

Since xi is a Red point, from Lemma B.2, any xj in training set output by Algorithm 1 with
yj (cid:54)= yi must have the property that (cid:107)xi − xj(cid:107) > 2r. Therefore,

(cid:107)x(cid:48) − xj(cid:107) ≥ (cid:107)xi − xj(cid:107) − (cid:107)x(cid:48) − xi(cid:107) > 2r − 2τ

Therefore, Algorithm 1 will assign x(cid:48) the label yi. The lemma follows.

Lemma B.4. Let B be a ball such that: (a) for all x ∈ B, η(x) > 1
µ(B) ≥ 2C0
one xi such that xi ∈ |B ∩ Xn| and yi = 1.

2 + ∆ and (b)
n (d log n + log(1/δ)). Then, with probability ≥ 1 − δ, all such balls have at least

Proof. Observe that J(B) ≥ C0
that ˆJ(B) > 0, which gives the theorem.

n (d log n + log(1/δ)). Applying Theorem 16 of [4], this implies

Lemma B.5. Fix ∆ and δ, and let kn = 3 log(2n/δ)

. Additionally, let

∆2

pn =

kn
n

+

C0
n

(d log n + log(1/δ) + (cid:112)kn(d log n + log(1/δ)),

22

Table 1: An evaluation of the black-box substitute classiﬁer. Each black-box substitute is
evaluated by: 1) its accuracy on the its training set, 2) its accuracy on the test set, and 3) the
percentage of predictions agreeing with the target classiﬁer on the test set. A combination of
high test accuracy and consistency with the original classiﬁer indicates the black-box model
emulates the target classiﬁer well.

Abalone

target f % training % test % testf
accuracy accuracy same as f
61.3%
62.5%
61.4%
63.5%
68.9%
64.1%
68.4%
65.0%

StandardNN 100%
100%
RobustNN
100%
ATNN
100%
ATNN-All
StandardNN 69.1%
87.2%
RobustNN
68.8%
ATNN
66.5%
ATNN-All
Halfmoon

72.6%
90.9%
73.7%
73.5%
68.6%
86.9%
68.4%
66.6%

target f % training % test % test

StandardNN 95.9%
97.7%
RobustNN
96.4%
ATNN
97.6%
ATNN-All
StandardNN 94.5%
94.2%
RobustNN
95.3%
ATNN
96.9%
ATNN-All

accuracy accuracy same as f
95.6%
94.9%
95.1%
96.8%
94.0%
90.5%
94.2%
96.2%

95.5%
97.6%
96.0%
97.3%
94.4%
94.1%
95.2%
96.5%

MNIST 1v7

target f % training % test % test

StandardNN 100%
100%
RobustNN
100%
ATNN
100%
ATNN-All
StandardNN 99.9%
99.8%
RobustNN
100%
ATNN
99.7%
ATNN-All

accuracy accuracy same as f
98.9%
95.4%
98.9%
98.7%
98.9%
94.8%
98.8%
98.9%

99.3%
97.6%
99.3%
99.3%
99.1%
98.7%
99.2%
99.3%

Kernel

Neural
Nets

Kernel

Neural
Nets

Kernel

Neural
Nets

where C0 is the constant in Theorem 15 of [4]. Deﬁne:

SRED = {(xi, yi) ∈ Sn|xi ∈ X +
r,∆,pn
(cid:19)
1
1
2
2

η(xi) −

yi =

sgn

(cid:18)

+

1
2

}

∪ X −

r,∆,p,

Then, with probability ≥ 1 − δ, all (xi, yi) ∈ SRED are marked as Red by Algorithm 1 run
with parameters r, ∆ and δ.

23

Proof. Consider a (xi, yi) ∈ SRED such that xi ∈ Xn∩X +
, and consider any (xj, yj) ∈ Sn
such that xj ∈ B(xi, r). From Lemma A.7, for all such xj, (cid:107)xj − X (kn)(xj)(cid:107) ≤ rpn (xj); this
means that all kn-nearest neighbors x(cid:48)(cid:48) of such an xj have η(x(cid:48)(cid:48)) > 1

r,∆,pn

Therefore, E[(cid:80)kn

l=1 Y (l)(xj)] ≥ kn(1/2 + ∆); by Theorem B.1, this means that for a
speciﬁc xj, Pr((cid:80)kn
l=1 Y (l)(xj) < 1/2) ≤ 2e−kn∆2/3, which is ≤ δ/n from our choice of kn.
By an union bound over all such xj, with probability ≥ 1 − δ, we see that Algorithm 2
reports the label g(xi) on all such xi, which is the same as yi by the deﬁnition of interiors;
xi therefore gets marked as Red.

2 + ∆.

Finally, we are ready to prove the main theorem of this section, which is a slightly more

technical form of Theorem 4.2.

Theorem B.6. Fix a ∆n, and pick kn and pn as in Lemma B.5. Suppose we run Algorithm 1
with parameters r, ∆n and δ. Consider the set:

XR =

(cid:40)

(cid:12)
(cid:12)x ∈ X +
(cid:12)

x

r+τ,∆n,pn

r+τ,∆n,pn

∪ X −

,

µ(B(x, τ )) ≥

(d log n + log(1/δ))

,

2C0
n

(cid:41)

where C0 is the constant in Theorem 15 of [4]. Then, with probability ≥ 1 − 2δ over the
training set, Algorithm 1 has robustness radius ≥ r − 2τ on XR. Additionally, its astuteness
at radius r − 2τ is at least E[η(X) · 1(X ∈ X +
)].

)] + E[(1 − η(X)) · 1(X ∈ X −

r+τ,∆n,pn

r+τ,∆n,pn

Proof. Due to the condition on µ(B(x, τ )), from Lemma B.4, with probability ≥ 1 − δ, all
x ∈ XR have the property that there exists a (xi, yi) in Sn such that yi = g(xi) and xi ∈
B(x, τ ). Without loss of generality, suppose that x ∈ X +
, so that η(x) > 1/2 + ∆n.
Then, from the properties of r-robust interiors, this xi ∈ X +
.

r+τ,∆n,pn

From Lemma B.5, with probability ≥ 1 − δ, this (xi, yi) is marked Red by Algorithm 1
run with parameters r, ∆n and δ. The theorem now follows from an union bound and
Lemma B.3.

r,∆n,pn

C Experiment Visualization and Validation

First, we show adversarial examples created by diﬀerent attacks on the MNIST dataset in
order to illustrate characteristics of each attack. Next, we show the subset of training points
selected by Algorithm 1 on the halfmoon dataset. The visualization illustrates the intuition
behind Algorithm 1 and also validates its implementation. Finally, we validate how eﬀective
the black-box subsitute classiﬁers emulate the target classiﬁer.

C.1 Adversarial Examples Created by Diﬀerent Attacks

Figure 4 shows adversarial examples created on MNIST digit 1 images with attack radius
r = 3. First, we observe that the perturbations added by direct attack, white-box kernel
attack and black-box kernel attack are clearly targeted: either a faint horizontal stroke or a
shadow of digit 7 are added to the original image. The perturbation budget is used on "key"
pixels that distinguish digit 1 and digit 7, therefore the attack is eﬀective. On the contrary,

24

black-box attacks with neural nets substitute adds perturbation to a large number of pixels.
While such perturbation often fools a neural net classiﬁer, it is not eﬀective against nearest
neighbors. Consider a pixel that is dark in most digit 1 and digit 7 training images; adding
brightness to this pixel increases the distance between the test image to training images from
both classes, therefore may not change the nearest neighbor to the test image.

Figure 4 also illustrates the break-down attack radius of visual similarity. At r = 3,
the true class of adversarial examples created by eﬀective attacks becomes ambiguous even
to humans. Our defense is successful as the Robust_1NN classiﬁers still have non-trivial
classiﬁcation accuracy at such attack radius. Meanwhile, we should not expect robustness
against even larger attack radius since the adversarial examples at r = 3 are already close to
the boundary of human perception.

C.2 Training Subset Selected by Robust_1NN

Figure 3 shows the training set selected by Robust_1NN on a halfmoon training set of size
2000. On the original training set, we see a noisy region between the two halfmoons where
both red and blue points appear. Robust_1NN cleans training points in this region so as to
create a gap between the red and blue halfmoons, and the gap width increases with defense
radius r.

C.3 Performance of Black-box Attack Substitutes

We validate the black-box substitute training process by checking the substitute’s accuracy
on its training set, the clean test set and the percentage of predictions agreeing with the
target classiﬁer on the clean test set. The results are shown in Table 1. For the halfmoon and
MNIST dataset, the substitute classiﬁers both achieve high accuracy on both the training
and test sets, and are also consistent with the target classiﬁer on the test set. The subsitutute
classiﬁers do not emulate the target classiﬁer on the Abalone dataset as close as on the other
two datasets due to the high noise level in the Abalone dataset. Nonetheless, the substitute
classiﬁer still achieve test time accuracy comparable to the target classiﬁer.

25

9
1
0
2
 
n
u
J
 
9
1
 
 
]
L
M

.
t
a
t
s
[
 
 
6
v
2
2
9
3
0
.
6
0
7
1
:
v
i
X
r
a

Analyzing the Robustness of Nearest Neighbors to
Adversarial Examples

Yizhen Wang
University of California, San Diego
yiw248@eng.ucsd.edu

Somesh Jha
University of Wisconsin-Madison
jha@cs.wisc.edu

Kamalika Chaudhuri
University of California, San Diego
kamalika@cs.eng.ucsd.edu

Abstract

Motivated by safety-critical applications, test-time attacks on classiﬁers via adversar-
ial examples has recently received a great deal of attention. However, there is a general
lack of understanding on why adversarial examples arise; whether they originate due to
inherent properties of data or due to lack of training samples remains ill-understood.
In this work, we introduce a theoretical framework analogous to bias-variance theory
for understanding these eﬀects. We use our framework to analyze the robustness of
a canonical non-parametric classiﬁer – the k-nearest neighbors. Our analysis shows
that its robustness properties depend critically on the value of k – the classiﬁer may
be inherently non-robust for small k, but its robustness approaches that of the Bayes
Optimal classiﬁer for fast-growing k. We propose a novel modiﬁed 1-nearest neighbor
classiﬁer, and guarantee its robustness in the large sample limit. Our experiments 1
suggest that this classiﬁer may have good robustness properties even for reasonable
data set sizes.

1

Introduction

Machine learning is increasingly applied in security-critical domains such as automotive
systems, healthcare, ﬁnance and robotics. To ensure safe deployment in these applications,
there is an increasing need to design machine-learning algorithms that are robust in the
presence of adversarial attacks.

A realistic attack paradigm that has received a lot of recent attention [12, 26, 28, 25] is
test-time attacks via adversarial examples. Here, an adversary has the ability to provide
modiﬁed test inputs to an already-trained classiﬁer, but cannot modify the training process
in any way. Their goal is to perturb legitimate test inputs by a “small amount” in order to
force the classiﬁer to report an incorrect label. An example is an adversary that replaces a
stop sign by a slightly defaced version in order to force an autonomous vehicle to recognize
it as an yield sign. This attack is undetectable to the human eye if the perturbation is small
enough.

1Code available at: https://github.com/EricYizhenWang/robust_nn_icml

1

Prior work has considered adversarial examples in the context of linear classiﬁers [19],
kernel SVMs [3] and neural networks [28, 12, 25, 26, 22]. However, most of this work has
either been empirical, or has focussed on developing theoretically motivated attacks and
defenses. Consequently, there is a general lack of understanding on why adversarial examples
arise; whether they originate due to inherent properties of data or due to lack of training
samples remains ill-understood.

This work develops a theoretical framework for robust learning in order to understand the
eﬀects of distributional properties and ﬁnite samples on robustness. Building on traditional
bias-variance theory [10], we posit that a classiﬁcation algorithm may be robust to adversarial
examples due to three reasons. First, it may be distributionally robust, in the sense that the
output classiﬁer is robust as the number of training samples grow to inﬁnity. Second, even
the output of a distributionally robust classiﬁcation algorithm may be vulnerable due to too
few training samples – this is characterized by ﬁnite sample robustness. Finally, diﬀerent
training algorithms might result in classiﬁers with diﬀerent degrees of robustness, which we
call algorithmic robustness. These quantities are analogous to bias, variance and algorithmic
eﬀects respectively.

Next, we analyze a simple non-parametric classiﬁcation algorithm: k-nearest neighbors in
our framework. Our analysis demonstrates that large sample robustness properties of this
algorithm depend very much on k.

Speciﬁcally, we identify two distinct regimes for k with vastly diﬀerent robustness
properties. When k is constant, we show that k-nearest neighbors has zero robustness in
the large sample limit in regions where p(y = 1|x) lies in (0, 1). This is in contrast with
dn log n), where d is the
accuracy, which may be quite high in these regions. For k = Ω(
data dimension and n is the sample size, we show that the robustness region of k-nearest
neighbors approaches that of the Bayes Optimal classiﬁer in the large sample limit. This is
again in contrast with accuracy, where convergence to the Bayes Optimal accuracy is known
for a much slower growing k [7, 5].
√

dn log n) is too high to use in practice with nearest neighbors, we next
propose a novel robust version of the 1-nearest neighbor classiﬁer that operates on a modiﬁed
training set. We provably show that in the large sample limit, this algorithm has superior
robustness to standard 1-nearest neighbors for data distributions with certain properties.

Since k = Ω(

√

Finally, we validate our theoretical results by empirically evaluating our algorithm on
three datasets against several popular attacks. Our experiments demonstrate that our
algorithm performs better than or about as well as both standard 1-nearest neighbors and
nearest neighbors with adversarial training – a popular and eﬀective defense mechanism.
This suggests that although our performance guarantees hold in the large sample limit, our
algorithm may have good robustness properties even for realistic training data sizes.

1.1 Related Work

Adversarial examples have recently received a great deal of attention [12, 3, 26, 28, 25].
Most of the work, however, has been empirical, and has focussed on developing increasingly
sophisticated attacks and defenses.

1.1.1 Related Work on Adversarial Examples

Prior theoretical work on adversarial examples falls into two categories – analysis and theory-
inspired defenses. Work on analysis includes [9], which analyzes the robustness of linear and

2

quadratic classiﬁers under random and semi-random perturbations. [13] provides robustness
guarantees on linear and kernel classiﬁers trained on a given data set. [11] shows that linear
classiﬁers for high dimensional datasets may have inherent robustness-accuracy trade-oﬀs.
Work on theory-inspired defenses include [20, 15, 1], who provide defense mechanisms for
adversarial examples in neural networks that are relaxations of certain principled optimization
objectives. [14] shows how to use program veriﬁcation to certify robustness of neural networks
around given inputs for small neural networks.

Our work diﬀers from these in two important ways. First, unlike most prior work which
looks at a given training dataset, we consider eﬀects of the data distribution and number of
samples, and analyze robustness properties in the large sample limit. Second, unlike prior
work which largely focuses on parametric methods such as neural networks, our focus is on a
canonical non-parametric method – the nearest neighbors classiﬁer.

1.1.2 Related Work on Nearest Neighbors

There has been a body of work on the convergence and consistency of nearest-neighbor
classiﬁers and their many variants [6, 27, 17, 8, 5, 16]; all these works however consider
accuracy and not robustness.

In the asymptotic regime, [6] shows that the accuracy of 1-nearest neighbors converges in
the large sample limit to 1 − 2R∗(1 − R∗) where R∗ is the error rate of the Bayes Optimal
classiﬁer. This implies that even 1-nearest neighbor may achieve relatively high accuracy
even when p(y = 1|x) is not 0 or 1. In contrast, we show that 1-nearest neighbor is inherently
non-robust when p(y = 1|x) ∈ (0, 1) under some continuity conditions.

For larger k, the accuracy of k-nearest neighbors is known to converge to that of the
Bayes Optimal classiﬁer if kn → ∞ and kn/n → 0 as the sample size n → ∞. We show that
the robustness also converges to that of the Bayes Optimal classiﬁer when kn grows at a
much higher rate – fast enough to ensure uniform convergence. Whether this high rate is
necessary remains an intriguing open question.

Finite sample rates on the accuracy of nearest neighbors are known to depend heavily on
properties of the data distribution, and there is no distribution free rate as in parametric
methods [8].
[5] provides a clean characterization of the ﬁnite sample rates of nearest
neighbors as a function of natural interiors of the classes. Here we build on their results
by deﬁning a stricter, more robust version of interiors and providing bounds as functions of
these new robust quantities.

1.1.3 Other Related Work

[2] provides a method for generating adversarial examples for nearest neighbors, and shows
that the eﬀectiveness of attacks grow with intrinsic dimensionality. Finally, [24, 25] provides
black-box attacks on substitute classiﬁers; their experiments show that attacks from other
types of substitute classiﬁers are not successful on nearest neighbors; our experiments
corroborate these results.

3

2 The Setting and Deﬁnitions

2.1 The Basic Setup

We consider test-time attacks in a white box setting, where the adversary has full knowledge
of the training process – namely, the type of classiﬁer used, the training data and any
parameters – but cannot modify training in any way.

Given an input x, the adversary’s goal is to perturb it so as to force the trained classiﬁer
f to report a diﬀerent label than f (x). The amount of perturbation is measured by an
application-speciﬁc metric d, and is constrained to be within a radius r. Our analysis can
be extended to any metric, but for this paper we assume that d is the Euclidean distance
for mathematical simplicity; we also focus on binary classiﬁcation, and leave extensions to
multiclass for future work.

Finally, we assume that unlabeled instances are drawn from an instance space X , and
their labels are drawn from the label space {0, 1}. There is an underlying data distribution
D that generates labeled examples; the marginal over X of D is µ and the conditional
distribution of labels given x is denoted by η.

2.2 Robustness and astuteness

We begin by deﬁning robustness, which for a classiﬁer f at input x is measured by the
robustness radius.

Deﬁnition 2.1 (Robustness Radius). The robustness radius of a classiﬁer f at an instance
x ∈ X , denoted by ρ(f, x), is the shortest distance between x and an input x(cid:48) to which f
assigns a label diﬀerent from f (x):

ρ(f, x) = inf
r

{∃x(cid:48) ∈ X ∩ B(x, r) s.t f (x) (cid:54)= f (x(cid:48))}

Observe that the robustness radius measures a classiﬁer’s local robustness. A classiﬁer
f with robustness radius r at x guarantees that no adversarial example of x with norm
of perturbation less than r can be created using any attack method. A plausible way
to extend this into a global notion is to require a lower bound on the robustness radius
everywhere; however, only the constant classiﬁer will satisfy this condition. Instead, we
consider robustness around meaningful instances, that we model as examples drawn from
the underlying data distribution.

Deﬁnition 2.2 (Robustness with respect to a Distribution). The robustness of a classiﬁer
f at radius r with respect to a distribution µ over the instance space X , denoted by R(f, r, µ),
is the fraction of instances drawn from µ for which the robustness radius is greater than or
equal to r.

R(f, r, µ) = Pr
x∼µ

(ρ(f, x) ≥ r)

Finally, observe that we are interested in classiﬁers that are both robust and accurate.
This leads to the notion of astuteness, which measures the fraction of instances on which a
classiﬁer is both accurate and robust.

Deﬁnition 2.3 (astuteness). The astuteness of a classiﬁer f with respect to a data distribu-
tion D and a radius r is the fraction of examples on which it is accurate and has robustness
radius at least r; formally,

AstD(f, r) = Pr

(ρ(f, x) ≥ r, f (x) = y),

(x,y)∼D

4

Observe that astuteness is analogous to classiﬁcation accuracy, and we argue that it is
a more appropriate metric if we are concerned with both robustness and accuracy. Unlike
accuracy, astuteness cannot be directly empirically measured unless we have a way to certify
a lower bound on the robustness radius. In this work, we will prove bounds on the astuteness
of classiﬁers, and in our experiments, we will approximate it by measuring resistance to
standard attacks.

2.3 Sources of Robustness

There are three plausible reasons why classiﬁers lack robustness – distributional, ﬁnite sample
and algorithmic. These sources are analogous to bias, variance, and algorithmic eﬀects
respectively in standard bias-variance theory.

Distributional robustness measures the eﬀect of the data distribution on robustness
when an inﬁnitely large number of samples are used to train the classiﬁer. Formally, if
Sn is a training sample of size n drawn from D and A(Sn, ·) is a classiﬁer obtained by
applying the training procedure A on Sn, then the distributional robustness at radius r is
limn→∞ ESn∼D[R(A(Sn, ·), r, µ)].

In contrast, for ﬁnite sample robustness, we characterize the behaviour of R(A(Sn, ·), r, µ)
for ﬁnite n – usually by putting high probability bounds over the training set. Thus, ﬁnite
sample robustness depends on the training set size n, and quantiﬁes how it changes with
sample size. Finally, robustness also depends on the training algorithm itself; for example,
some variants of nearest neighbors may have higher robustness than nearest neighbors itself.

2.4 Nearest Neighbor and Bayes Optimal Classiﬁers

Given a training set Sn = {(X1, Y1), . . . , (Xn, Yn)} and a test example x, we use the notation
X (i)(x) to denote the i-th nearest neighbor of x in Sn, and Y (i)(x) to denote the label of
X (i)(x).

Given a test example x, the k-nearest neighbor classiﬁer Ak(Sn, x) outputs:

= 1,

= 0,

if Y (1)(x) + . . . + Y (k)(x) ≥ k/2
otherwise.

The Bayes optimal classiﬁer g over a data distribution D has the following classiﬁcation

rule:

g(x) =

(cid:26) 1
0

if η(x) = Pr(y = 1|x) ≥ 1/2;
otherwise.

(1)

3 Robustness of Nearest Neighbors

How robust is the k-nearest neighbor classiﬁer? We show that it depends on the value of k.
Speciﬁcally, we identify two distinct regimes – constant k and k = Ω(
dn log n) where d is
the data dimension – and show that nearest neighbors has diﬀerent robustness properties in
the two.

√

3.1 Low k Regime

In this region, k is a constant that does not depend on the training set size n. Provided
certain regularity conditions hold, we show that k-nearest neighbors is inherently non-robust

5

in this regime unless η(x) ∈ {0, 1} – in the sense that the distributional robustness becomes
0 in the large sample limit.

Theorem 3.1. Let x ∈ X ∩ supp(µ) such that (a) µ is absolutely continuous with respect
to the Lebesgue measure (b) η(x) ∈ (0, 1) (c) η is continuous with respect to the Euclidean
metric in a neighborhood of x. Then, for ﬁxed k, ρ(Ak(Sn, ·), x) converges in probability to
0.

Remarks. Observe that Theorem 3.1 implies that the distributional robustness (and hence
astuteness) in a region where η(x) ∈ (0, 1) is 0. This is in contrast with accuracy; for 1-NN,
the accuracy converges to 1 − 2R∗(1 − R∗) as n → ∞, where R∗ is the error rate of the
Bayes Optimal classiﬁer, and thus may be quite high.

The proof of Theorem 3.1 in the Appendix shows that the absolute continuity of µ with
respect to the Lebesgue measure is not strictly necessary; absolute continuity with respect
to an embedded manifold will give the same result, but will result in a more complex proof.
In the Appendix A (Theorem A.2), we show that k-nearest neighbor is astute in the

interior of the region where η ∈ {0, 1}, and provide ﬁnite sample rates for this case.

3.2 High k Regime

√

Prior work has shown that in the large sample limit, the accuracy of the nearest neighbor
classiﬁers converge to the Bayes Optimal, provided k is set properly. We next show that
if k is Ω(
dn log n), the regions of robustness and the astuteness of the k nearest neighbor
classiﬁers also approach the corresponding quantities for the Bayes Optimal classiﬁer as
n → ∞. Thus, if the Bayes Optimal classiﬁer is robust, then so is k-nearest neighbors in the
large sample limit.

The main intuition is that k = Ω(

dn log n) is large enough for uniform convergence –
where, with high probability, all Euclidean balls with k examples have the property that the
empirical averages of their labels are close to their expectations. This guarantees that for
any x, the k-nearest neighbor reports the same label as the Bayes Optimal classiﬁer for all
x(cid:48) close to x. Thus, if the Bayes Optimal classiﬁer is robust, so is nearest neighbors.

√

3.2.1 Deﬁnitions

We begin with some deﬁnitions that we can use to characterize the robustness of the Bayes
Optimal classiﬁer. Following [5], we use the notation Bo(x, r) to denote an open ball and
B(x, r) to denote a closed ball of radius r around x. We deﬁne the probability radius of a
ball around x as:

We next deﬁne the r-robust (p, ∆)-strict interiors as follows:

rp(x) = inf{r | µ(B(x, r)) ≥ p}

X +

r,∆,p = {x ∈ supp(µ) | ∀x(cid:48) ∈ Bo(x, r),

∀x(cid:48)(cid:48) ∈ B(x(cid:48), rp(x(cid:48))), η(x(cid:48)(cid:48)) > 1/2 + ∆}

X −

r,∆,p = {x ∈ supp(µ) | ∀x(cid:48) ∈ Bo(x, r),

∀x(cid:48)(cid:48) ∈ B(x(cid:48), rp(x(cid:48))), η(x(cid:48)(cid:48)) < 1/2 − ∆}

What is the signiﬁcance of these interiors? Let x(cid:48) be an instance such that all x(cid:48)(cid:48) ∈
n , then the k points x(cid:48)(cid:48) closest to x(cid:48) have

B(x(cid:48), rp(x(cid:48))) have η(x(cid:48)(cid:48)) > 1/2 + ∆. If p ≈ k

6

η(x(cid:48)(cid:48)) > 1/2 + ∆. Provided the average of the labels of these points is close to expectation,
which happens when k is large relative to 1/∆, k-nearest neighbor outputs label 1 on x(cid:48).
When x is in the r-robust (p, ∆)-strict interior region X +
r,∆,p, this is true for all x(cid:48) within
distance r of x, which means that k-nearest neighbors will be robust at x. Thus, the r-robust
(p, ∆)-strict interior is the region where we natually expect k-nearest neighbor to have
robustness radius r, when k is large relative to 1

∆ and p ≈ k
n .

Readers familiar with [5] will observe that the set of all x(cid:48) for which ∀x(cid:48)(cid:48) ∈ B(x(cid:48), rp(x(cid:48))), η(x(cid:48)(cid:48)) >

1/2 + ∆ forms a stricter version of the (p, ∆)-interiors of the 1 region that was deﬁned in
this work; these x(cid:48) also represent the region where k-nearest neighbors are accurate when
k ≈ max(np, 1/∆2). The r-robust (p, ∆)-strict interior is thus a somewhat stricter and more
robust version of this deﬁnition.

3.2.2 Main Results

We begin by characterizing where the Bayes Optimal classiﬁer is robust.

Theorem 3.2. The Bayes Optimal classiﬁer has robustness radius r at x ∈ X +
Moreover, its astuteness is E[η(x)1(x ∈ X +

r,0,0)] + E[(1 − η(x))1(x ∈ X −

r,0,0)].

r,0,0 ∪ X −

r,0,0.

The proof is in the Appendix, along with analogous results for astuteness. The following
theorem, along with a similar result for astuteness, proved in the Appendix, characterizes
robustness in the large k regime.

√

Theorem 3.3. For any n, pick a δ and a ∆n → 0. There exist constant C1 and C2 such that
if kn ≥ C1
n (1 + C2
≥ 1 − 3δ, kn-NN has robustness radius r in x ∈ X +

(cid:113) d log n+log(1/δ)
kn
∪ X −

), then, with probability

, and pn ≥ kn

dn log n+n log(1/δn)

∆n

.

r,∆n,pn

r,∆n,pn

Remarks. Some remarks are in order. First, observe that as n → ∞, ∆n and pn tend
to 0; thus, provided certain continuity conditions hold, X +
approaches
X +

r,∆n,pn
r,0,0, the robustness region of the Bayes Optimal classiﬁer.

r,0,0 ∪ X −

∪ X −

r,∆n,pn

Second, observe that as r-robust strict interiors extend the deﬁnition of interiors in [5],
Theorem 3.3 is a robustness analogue of Theorem 5 in this work. Unlike the latter, Theo-
rem 3.3 has a more stringent requirement on k. Whether this is necessary is left as an open
question for future work.

4 A Robust 1-NN Algorithm

√

Section 3 shows that nearest neighbors is robust for k as large as Ω(
dn log n). However,
this k is too high to use in practice – high values of k require even higher sample sizes [5],
and lead to higher running times. Thus a natural question is whether we can ﬁnd a more
robust version of the algorithm for smaller k. In this section, we provide a more robust
version of 1-nearest neighbors, and analytically demonstrate its robustness.

Our algorithm is motivated by the observation that 1-nearest neighbor is robust when
oppositely labeled points are far apart, and when test points lie close to training data. Most
training datasets however contain nearby points that are oppositely labeled; thus, we propose
to remove a subset of training points to enforce this property.

Which points should we remove? A plausible approach is to keep the largest subset where
oppositely labeled points are far apart; however, this subset has poor stability properties

7

even for large n. Therefore, we propose to keep all points x such that: (a) we are highly
conﬁdent about the label of x and its nearby points and (b) all points close to x have the
same label. Given that all such x are kept, we remove as few points as possible, and execute
nearest neighbors on the remaining dataset.

The following deﬁnition characterizes data where oppositely labeled points are far apart.

Deﬁnition 4.1 (r-separated set). A set A = {(x1, y1), . . . , (xm, ym)} of labeled examples is
said to be r-separated if for all pairs (xi, yi), (xj, yj) ∈ A, (cid:107)xi − xj(cid:107) ≤ r implies yi = yj.

The full algorithm is described in Algorithm 1 and Algorithm 2. Given conﬁdence
parameters ∆ and δ, Algorithm 2 returns a 0/1 label when this label agrees with the average
of kn points closest to x; otherwise, it returns ⊥. kn is chosen such that with probability
≥ 1 − δ, the empirical majority of kn labels agrees with the majority in expectation, provided
the latter is at least ∆ away from 1
2 .

Algorithm 2 is used to determine whether an xi should be kept. Let f (xi) be the output
of Algorithm 2 on xi. If yi = f (xi) and if for all xj ∈ B(xi, r), f (xi) = f (xj) = yi, then
we mark xi as red. Finally, we compute the largest r-separated subset of the training data
that includes all the red points; this reduces to a constrained matching problem as in [16].
The resulting set, returned by Algorithm 1, is our new training set. We observe that this
set is r-separated from Lemma B.2 in the Appendix, and thus oppositely labeled points are
far apart. Moreover, we keep all (xi, yi) when we are conﬁdent about the label of xi and
its nearby points. Observe that our ﬁnal procedure is a 1-NN algorithm, even though kn
neighbors are used to determine if a point should be retained in the training set.

Algorithm 1 Robust_1NN(Sn, r, ∆, δ, x)

f (xi) = Conﬁdent-Label(Sn, ∆, δ, xi)

for (xi, yi) ∈ Sn do

end for
SRED = ∅
for (xi, yi) ∈ Sn do

if f (xi) = yi and f (xi) = f (xj) for all xj such that (cid:107)xi − xj(cid:107) ≤ r and (xj, yj) ∈ Sn
then

SRED = SRED

(cid:83){(xi, yi)}

end if
end for
Let S(cid:48) be the largest 2r-separated subset of Sn that contains all points in SRED.
return new training set S(cid:48)

Algorithm 2 Conﬁdent-Label(Sn, ∆, δ, x)

kn = 3 log(2n/δ)/∆2
¯y = (1/kn) (cid:80)kn
2 − ∆, 1
if ¯y ∈ [ 1

i=1 Y (i)(x)
2 + ∆] then

return ⊥

else

end if

return 1

2 sgn(¯y − 1

2 ) + 1

2

8

4.1 Performance Guarantees

The following theorem establishes performance guarantees for Algorithm 1.

Theorem 4.2. Pick a ∆n and δ, and set kn = 3 log(2n/δ)/∆2
n. Pick a margin parameter
τ . Then, there exist constants C and C0 such that the following hold. If we set pn =
kn
n (1 + C

(cid:113) d log n+log(1/δ)
kn

), and deﬁne the set:

XR =

(cid:40)

(cid:12)
(cid:12)x ∈ X +
(cid:12)
x

r+τ,∆n,pn

r+τ,∆n,pn

∪ X −

,

µ(B(x, τ )) ≥

(d log n + log(1/δ))

2C0
n

(cid:41)

Then, with probability ≥ 1 − 2δ over the training set, Algorithm 1 run with parameters r, ∆n
and δ has robustness radius at least r − 2τ on XR.

Remarks. The proof is in the Appendix, along with an analogous result for astuteness.
Observe that XR is roughly the high density subset of the r + τ -robust strict interior
X +
2 + ∆n or less than
r+τ,∆n,pn
1
2 − ∆n in this region, as opposed to 0 or 1, this is an improvement over standard nearest
neighbors when the data distribution has a large high density region that intersects with the
interiors.

. Since η(x) is constrained to be greater than 1

r+τ,∆n,pn

∪ X −

A second observation is that as τ is an arbitrary constant, we can set to it be quite small
and still satisfy the condition on µ(B(x, τ )) for a large fraction of x’s when n is very large.
This means that in the large sample limit, r − 2τ may be close to r and XR may be close to
the high density subset of X +

for a lot of smooth distributions.

∪ X −

r,∆n,pn

r,∆n,pn

5 Experiments

The results in Section 4 assume large sample limits. Thus, a natural question is how well
Algorithm 1 performs with more reasonable amounts of training data. We now empirically
investigate this question.

Since there are no general methods that certify robustness at an input, we assess
robustness by measuring how our algorithm performs against a suite of standard attack
methods. Speciﬁcally, we consider the following questions:

1. 1. How does our algorithm perform against popular white box and black box attacks

compared with standard baselines?

2. 2. How is performance aﬀected when we change the training set size relative to the

data dimension?

These questions are considered in the context of three datasets with varying training set
sizes relative to the dimension, as well as two standard white box attacks and black box
attacks with two kinds of substitute classiﬁers.

9

Figure. 1: White Box Attacks: Plot of classiﬁcation accuracy on adversarial examples v.s.
attack radius. Top row: Direct Attack. Bottom row: Kernel Substitute Attack. Left to right:
1) Halfmoon, 2) MNIST 1v 7 and 3) Abalone.

Figure. 2: Black Box Attacks: Plot of classiﬁcation accuracy on adversarial examples v.s.
attack radius. Top to Bottom: 1) kernel substitute, 2) neural net substitute. Left to right:
1) Halfmoon, 2) MNIST 1 v.s. 7 and 3) Abalone.

5.1 Methodology

5.1.1 Data

We use three datasets – Halfmoon, MNIST 1v7 and Abalone – with diﬀering data sizes
relative to dimension. Halfmoon is a popular 2-dimensional synthetic data set for non-linear
classiﬁcation. We use a training set of size 2000 and a test set of size 1000 generated with
standard deviation σ = 0.2. The MNIST 1v7 data set is a subset of the 784-dimensional
MNIST data. For training, we use 1000 images each of Digit 1 and 7, and for test, 500 images
of each digit. Finally, for the Abalone dataset [18], our classiﬁcation task is to distinguish
whether an abalone is older than 12.5 years based on 7 physical measurements. For training,
we use 500 and for test, 100 samples. In addition, a validation set with the same size as the

10

test set is generated for each experiment for parameter tuning.

5.1.2 Baselines

We compare Algorithm 1, denoted by RobustNN, against three baselines. The ﬁrst is
the standard 1-nearest neighbor algorithm, denoted by StandardNN. We use two forms
of adversarially-trained nearest neighbors - ATNN and ATNN-all. Let S be the training
set used by standard nearest neighbors. In ATNN, we augment S by creating, for each
(x, y) ∈ S, an adversarial example xadv using the attack method in the experiment, and
adding (xadv, y). The ATNN classiﬁer is 1-nearest neighbor on this augmented data. In
ATNN-all, for each (x, y) ∈ S, we create adversarial examples using all the attack methods
in the experiment, and add them all to S. ATNN-all is the nearest neighbor classiﬁer on this
augmented data. For example, for white box Direct Attacks in Section 5.2, ATNN includes
adversarial examples generated by the Direct Attack, and ATNN-all includes adversarial
examples generated by both Direct and Kernel Substitute Attacks.

Observe that all algorithms except StandardNN have parameters to tune. RobustNN
has three input parameters – ∆, δ and a defense radius r which is an approximation to the
robustness radius. For simplicity, we set ∆ = 0.45, δ = 0.1 and tune r on the validation set;
this can be viewed as tuning the parameter τ in Theorem 4.2. For ATNN and ATNN-all, the
methods that generate the augmenting adversarial examples need a perturbation magnitude
r; we call this the defense radius. To be fair to all algorithms, we tune the defense radius
for each. We consider the adversary with the highest attack perturbation magnitude in the
experiment, and select the defense radius that yields the highest validation accuracy against
this adversary.

5.2 White-box Attacks and Results

To evaluate the robustness of Algorithm 1, we use two standard classes of attacks – white
box and black box. For white-box attacks, the adversary knows all details about the classiﬁer
under attack, including its training data, the training algorithm and any hyperparameters.

5.2.1 Attack Methods

We consider two white-box attacks – direct attack [2] and Kernel Substitute Attack [24].
Direct Attack. This attack takes as input a test example x, an attack radius r, and a
training dataset S (which may be an augmented or reduced dataset). It ﬁnds an x(cid:48) ∈ S that is
closest to x but has a diﬀerent label, and returns the adversarial example xadv = x+r x−x(cid:48)
.
||x−x(cid:48)||2
Kernel Substitute Attack. This method attacks a substitute kernel classiﬁer trained on
the same training set. For a test input (cid:126)x, a set of training points Z with one-hot labels Y , a
kernel classiﬁer f predicts the class probability as:

The adversary trains a kernel classiﬁer on the training set of the corresponding nearest
neighbors, and then generates adversarial examples against this kernel classiﬁer. The
advantage is that the prediction of the kernel classiﬁer is diﬀerentiable, which allows the
use of standard gradient-based attack methods. For our experiments, we use the popular

f : (cid:126)x →

2/c(cid:105)

(cid:104)

e−||(cid:126)x−(cid:126)z||2
(cid:80)

(cid:126)z∈X e−||(cid:126)x−(cid:126)z||2

(cid:126)z∈X
2/c

· Y

11

fast-gradient-sign method (FSGM). The parameter c is tuned to yield the most eﬀective
attack, and is set to 0.1 for Halfmoon and MNIST, and 0.01 for Abalone.

5.2.2 Results

Figure 1 shows the results. We see that RobustNN outperforms all baselines for Halfmoon
and Abalone for all attack radii. For MNIST, for low attack radii, RobustNN’s classiﬁcation
accuracy is slightly lower than the others, while it outperforms the others for large attack
radii. Additionally, as is to be expected, the Direct Attack results in lower general accuracy
than the Kernel Substitute Attack.

These results suggest that our algorithm mostly outperforms the baselines StandardNN,
ATNN and ATNN-all. As predicted by theory, the performance gain is higher when the
training set size is large relative to the dimension – which is the setting where nearest
neighbors work well in general. It has superior performance for Halfmoon and Abalone,
where the training set size is large to medium relative to dimension. In contrast, in the
sparse dataset MNIST, our algorithm has slightly lower classiﬁcation accuracy for small
attack radii, and higher otherwise.

5.3 Black-box Attacks and Results

[25] has observed that some defense methods that work by masking gradients remain highly
amenable to black box attacks. In this attack, the adversary is unaware of the target classiﬁer’s
nature, parameters or training data, but has access to a seed dataset drawn from the same
distribution which they use to train and attack a substitute classiﬁer. To establish robustness
properties of Algorithm 1, we therefore validate it against black box attacks based on two
types of substitute classiﬁers.

5.3.1 Attack Methods

We use two types of substitute classiﬁers – kernel classiﬁers and neural networks. The
adversary trains the substitute classiﬁer using the method of [25] and uses the adversarial
examples against the substitute to attack the target classiﬁer.

Kernel Classiﬁer. The kernel classiﬁer substitute is the same as the one in Section 5.2,
but trained using the seed data and the method of [25].
Neural Networks. The neural network for MNIST is the ConvNet in [23]’s tutorial. For
Halfmoon and Abalone, the network is a multi-layer perceptron with 2 hidden layers.
Procedure. To train the substitute classiﬁer, the adversary uses the method of [24] to
augment the seed data for two rounds; labels are obtained by querying the target classiﬁer.
Adversarial examples against the substitutes are created by FGSM, following [24]. As a
sanity check, we verify the performance of the substitute classiﬁers on the original training
and test sets. Details are in Table 1 in the Appendix. Sanity checks on the performance of
the substitute classiﬁers are presented in Table 1 in the Appendix.

5.3.2 Results

Figure 2 shows the results. For all algorithms, black box attacks are less eﬀective than white
box, which corroborates the results of [24], who observed that black-box attacks are less
successful against nearest neighbors. We also ﬁnd that the kernel substitute attack is more
eﬀective than the neural network substitute, which is expected as kernel classiﬁers have

12

similar structure to nearest neighbors. Finally, for Halfmoon and Abalone, our algorithm
outperforms the baselines for both attacks; however, for MNIST neural network substitute,
our algorithm does not perform as well for small attack radii. This again conﬁrms the
theoretical predictions that our algorithm’s performance is better when the training set is
large relative to the data dimension – the setting in which nearest neighbors work well in
general.

5.4 Discussion

The results show that our algorithm performs either better than or about the same as
standard baselines against popular white box and black box attacks. As expected from
our theoretical results, it performs better for denser datasets which have high or medium
amounts of training data relative to the dimension, and either slightly worse or better for
sparser datasets, depending on the attack radius. Since non-parametric methods such as
nearest neighbors are mostly used for dense data, this suggests that our algorithm has good
robustness properties even with reasonable amounts of training data.

6 Conclusion

We introduce a novel theoretical framework for learning robust to adversarial examples, and
introduce notions of distributional and ﬁnite-sample robustness. We use these notions to
analyze a non-parametric classiﬁer, k-nearest neighbors, and introduce a novel modiﬁed
1-nearest neighbor algorithm that has good robustness properties in the large sample limit.
Experiments show that these properties are still retained for reasonable data sizes.

Many open questions remain. The ﬁrst is to close the gap in analysis of k-nearest
neighbors for k in between our two regimes. The second is to develop nearest neighbor
algorithms with better robustness guarantees. Finally, we believe that our work is a ﬁrst
step towards a comprehensive analysis of how the size of training data aﬀects robustness;
we believe that an important line of future work is to carry out similar analyses for other
supervised learning methods.

Acknowledgement

We thank NSF under IIS 1253942 for support. This work was also partially supported by
ARO under contract number W911NF-1-0405. We thank all anonymous reviewers for their
constructive comments.

We also thank Rabanus Derr from University of Tübingen for pointing about a minor

mistake in the proof of Lemma B.3. The proof is ﬁxed in the latest version.

References

[1] John Duchi Aman Sinha, Hongseok Namkoong. Certiﬁable distributional robustness with
principled adversarial training. International Conference on Learning Representations,
2018.

13

[2] Laurent Amsaleg, James Bailey, Sarah Erfani, Teddy Furon, Michael E Houle, Miloš
Radovanović, and Nguyen Xuan Vinh. The vulnerability of learning to adversarial
perturbation increases with intrinsic dimensionality. 2016.

[3] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndić, Pavel
Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at
test time. In Joint European Conference on Machine Learning and Knowledge Discovery
in Databases, pages 387–402. Springer, 2013.

[4] Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for the cluster tree.
In J. D. Laﬀerty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta,
editors, Advances in Neural Information Processing Systems 23, pages 343–351. Curran
Associates, Inc., 2010.

[5] Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for nearest neighbor
classiﬁcation. In Advances in Neural Information Processing Systems, pages 3437–3445,
2014.

[6] T. Cover and P.E. Hart. Nearest neighbor pattern classiﬁcation. IEEE Transactions on

Information Theory, 13:21–27, 1967.

[7] Luc Devroye, Laszlo Gyorﬁ, Adam Krzyzak, and Gabor Lugosi. On the strong universal
consistency of nearest neighbor regression function estimates. The Annals of Statistics,
pages 1371–1385, 1994.

[8] Luc P Devroye and Terry J Wagner. The strong uniform consistency of nearest neighbor

density estimates. The Annals of Statistics, pages 536–540, 1977.

[9] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of
classiﬁers: from adversarial to random noise. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems
29, pages 1632–1640. Curran Associates, Inc., 2016.

[10] Jerome Friedman, Trevor Hastie, Robert Tibshirani, et al. Additive logistic regression:
a statistical view of boosting (with discussion and a rejoinder by the authors). The
annals of statistics, 28(2):337–407, 2000.

[11] Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu, Martin
Wattenberg, and Ian Goodfellow. Adversarial spheres. arXiv preprint arXiv:1801.02774,
2018.

[12] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing

adversarial examples. arXiv preprint arXiv:1412.6572, 2014.

[13] Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness
of a classiﬁer against adversarial manipulation. In Advances in Neural Information
Processing Systems, pages 2263–2273, 2017.

[14] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer.
Towards proving the adversarial robustness of deep neural networks. arXiv preprint
arXiv:1709.02802, 2017.

14

[15] J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the

convex outer adversarial polytope. arXiv preprint arXiv:1711.00851, 2017.

[16] Aryeh Kontorovich and Roi Weiss. A bayes consistent 1-nn classiﬁer. In Artiﬁcial

Intelligence and Statistics Conference, 2015.

[17] S. Kulkarni and S. Posner. Rates of convergence of nearest neighbor estimation under
arbitrary sampling. IEEE Transactions on Information Theory, 41(4):1028–1039, 1995.

[18] M. Lichman. UCI machine learning repository, 2013.

[19] Daniel Lowd and Christopher Meek. Adversarial learning. In Proceedings of the eleventh
ACM SIGKDD international conference on Knowledge discovery in data mining, pages
641–647. ACM, 2005.

[20] Aleksander Mądry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian
Vladu. Towards deep learning models resistant to adversarial attacks. stat, 1050:9, 2017.

[21] Michael Mitzenmacher and Eli Upfal. Probability and computing: Randomized algorithms

and probabilistic analysis. Cambridge university press, 2005.

[22] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a
simple and accurate method to fool deep neural networks. IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2016.

[23] Nicolas Papernot, Nicholas Carlini, Ian Goodfellow, Reuben Feinman, Fartash Faghri,
Alexander Matyasko, Karen Hambardzumyan, Yi-Lin Juang, Alexey Kurakin, Ryan
Sheatsley, Abhibhav Garg, and Yen-Chen Lin. cleverhans v2.0.0: an adversarial machine
learning library. arXiv preprint arXiv:1610.00768, 2017.

[24] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine
learning: from phenomena to black-box attacks using adversarial samples. arXiv preprint
arXiv:1605.07277, 2016.

[25] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Berkay Celik, and
Ananthram Swami. Practical black-box attacks against deep learning systems using
adversarial examples. In Proceedings of the 2017 ACM Asia Conference on Computer
and Communications Security, 2017.

[26] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik,
and Ananthram Swami. The limitations of deep learning in adversarial settings. In
Proceedings of the 1st IEEE European Symposium on Security and Privacy. arXiv
preprint arXiv:1511.07528, 2016.

[27] C. Stone. Consistent nonparametric regression. Annals of Statistics, 5:595–645, 1977.

[28] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint
arXiv:1312.6199, 2013.

15

A Proofs from Section 3

A.1 Proofs for Constant k

Proof. (Of Theorem 3.1) To show convergence in probability, we need to show that for all
(cid:15), δ > 0, there exists an n((cid:15), δ) such that Pr(ρ(Ak(Sn, ·), x) ≥ (cid:15)) ≤ δ for n ≥ n0((cid:15), δ).

The proof will again proceed in two stages. First, we show in Lemma A.1 that if the
conditions in the statement of Theorem 3.1 hold, then there exists some n((cid:15), δ) such that
for n ≥ n((cid:15), δ), with probability at least 1 − δ, there exists two points x+ and x− in B(x, (cid:15))
such that (a) all k nearest neighbors of x+ have label 1, (b) all k nearest neighbors of x−
have label 0, and (c) x+ (cid:54)= x−.

Next we show that if the event stated above happens, then ρ(Ak(Sn, ·), x) ≤ (cid:15). This is
because Ak(Sn, x+) = 1 and Ak(Sn, x−) = 0. No matter what Ak(Sn, x) is, we can always
ﬁnd a point x(cid:48) that lies in {x+, x−} ⊂ B(x, (cid:15)) such that the prediction at x(cid:48) is diﬀerent from
Ak(Sn, x).

Lemma A.1. If the conditions in the statement of Theorem 3.1 hold, then there exists
some n((cid:15), δ) such that for n ≥ n((cid:15), δ), with probability at least 1 − δ, there are two points x+
and x− in B(x, (cid:15)) such that (a) all k nearest neighbors of x+ have label 1, (b) all k nearest
neighbors of x− have label 0, and (c) x+ (cid:54)= x−.

Proof. (Of Lemma A.1) The proof consists of two major components. First, for large enough
n, with high probability there are many disjoint balls in the neighborhood of x such that
each ball contains at least k points in Sn. Second, with high probability among these balls,
there exists a ball such that the k neareast neighbors of its center all have label 1. Similarly,
there exists a ball such that the k nearest neighbor of its center all have label 0.

Since µ is absolutely continuous with respect to Lebesgue measure in the neighbor-
bood of x and η is continuous, then for any m ∈ Z+, we can always ﬁnd m balls
B(x1, r1), · · · , B(xm, rm) such that (a) all m balls are disjoint, and (b) for all i ∈ {1, · · · , m},
we have xi ∈ B(x, (cid:15)), µ(B(xi, ri)) > 0 and η(x) ∈ (0, 1) for x ∈ B(xi, ri). For simplicity, we
(cid:84) Sn. Also, let
use Bi to denote B(xi, ri) and ci(n) to denote the number of points in Bi
µmin = mini∈{1,··· ,m} µ(Bi). Then by Hoeﬀding’s inequality, for each ball Bi and for any
n > k+1
µmin

,

Pr[ci(n) < k] ≤ exp(−2nµ2

min/(k + 1)2),

where the randomness comes from drawing sample Sn. Then taking the union bound over
all m balls, we have

Pr[∃i ∈ {1, · · · , m} such that ci(n) < k] ≤ m exp(−2nµ2
(cid:16) k+1
µmin

, [log m−log(δ/3)](k+1)2
which implies that when n > max
µ2
1 − δ/3, each of B1, · · · , Bm contains at least k points in Sn.

(cid:17)

min

min/(k + 1)2),

(2)

, with probability at least

An important consequence of the above result is that with probability at least 1 − δ/3,
the set of k nearest neighbors of each center xi of Bi is completely diﬀerent from another
center xj’s, so the labels of xi’s k nearest neighbors are independent of the labels of xj’s k
nearest neighbors.

Now let ηmin,+ = minx∈B1

(cid:83)··· (cid:83) Bm(1 − η(x)). Both
ηmin,+ and ηmin,− are greater than 0 by the construction requirements of B1, · · · , Bm. For
any xi,

(cid:83)··· (cid:83) Bm η(x) and ηmin,− = minx∈B1

Pr[xi’s k nearest neighbors all have label 1] ≥ ηk

min,+

16

Then,

Pr[∃i ∈ {1, · · · , m}s.t. xi’s k nearest neighbor all have label 1] ≥ 1 − (1 − ηk

min,+)m,

(3)

which implies when m ≥
s.t. its k nearest neighbors all have label 1. This xi is our x+.

min,+) , with probability at least 1 − δ/3, there exists an xi

log(1−ηk

log δ/3

Similarly,

Pr[∃i ∈ {1, · · · , m} s.t. xi’s k nearest neighbor all have label 0] ≥ 1 − (1 − ηk

min,−)m, (4)

log δ/3

and when m ≥
nearest neighbors all have label 0. This xi is our x−.
Combining the results above, we show that for

log(1−ηk

min,−) , with probability at least 1 − δ/3, there exists an xi s.t. its k

n > max

(cid:18) k + 1
µmin

,

[log m − log(δ/3)](k + 1)2
µ2

min

(cid:32)

m ≥ max

log δ/3
log(1 − ηk

min,+)

,

log δ/3
log(1 − ηk

min,−)

(cid:19)

(cid:33)

,

,

with probability at least 1 − δ, the statement in Lemma A.1 is satisﬁed.

A.2 Theorem and proof for k-nn robustness lower bound.

Theorem 3.1 shows that k-NN is inherently non-robust in the low k regime if η(x) ∈ (0, 1). On
the contrary, k-NN can be robust at x if η(x) ∈ {0, 1}. We deﬁne the r-robust (p, ∆)-interior
as follows:

ˆX +

r,∆,p = {x ∈ supp(µ)|∀x(cid:48) ∈ Bo(x, r),

∀x(cid:48)(cid:48) ∈ B(x(cid:48), rp(x(cid:48))), η(x(cid:48)(cid:48)) ≥ 1/2 + ∆}

ˆX −

r,∆,p = {x ∈ supp(µ)|∀x(cid:48) ∈ Bo(x, r),

∀x(cid:48)(cid:48) ∈ B(x(cid:48), rp(x(cid:48))), η(x(cid:48)(cid:48)) ≤ 1/2 − ∆}

The deﬁnition is similar to the strict r-robust (p, ∆)-interior in Section 4, except replacing <
and > with ≤ and ≥. Theorem A.2 show that k-NN is robust at radius r in the r-robust
(1/2, p)-interior with high high probability. Corollary A.3 shows the ﬁnite sample rate of the
robustness lowerbound.

Theorem A.2. Let x ∈ X ∩ supp(µ) such that (a) µ is absolutely continuous with respect
to the Lebesgue measure (b) η(x) ∈ {0, 1}. Then, for ﬁxed k, there exists an n0 such that for
n ≥ n0,

for all x in ˆX +

r,1/2,p

(cid:83) ˆX −

r,1/2,p for all p > 0, δ > 0.

Pr[ρ(Ak(Sn, ·), x) ≥ r] ≥ 1 − δ

In addition, with probability at least 1 − δ, the astuteness of the k-NN classiﬁer is at least:

E(1(X ∈ ˆX +

r,1/2,p

(cid:91) ˆX −

r,1/2,p))

17

Proof. The k-NN classiﬁer Ak(Sn, ·) is robust at radius r at x if for every x(cid:48) ∈ Bo(x, r), a)
there are k training points in B(x(cid:48), rp(x(cid:48))), and b) more than (cid:98)k/2(cid:99) of them have the same
label as Ak(Sn, x). Without loss of generality, we look at a point x ∈ ˆX +
r,1/2,p. The second
condition is satisﬁed since η(x) = 1 for all training points in B(x(cid:48), rp(x(cid:48))) by the deﬁnition
of ˆX +

r,1/2,p.
It remains to check the ﬁrst condition. Let B be a ball in Rd and n(B) be the number of
training points in B. Lemma 16 of [4] suggests that with probability at least 1 − δ, for all B
in Rd,

µ(B) ≥

+

d log n + log

+

k

d log n + log

(5)

k
n

(cid:32)

Co
n

(cid:115)

(cid:18)

1
δ

(cid:19)(cid:33)

1
δ

implies n(B) ≥ k, where Co is a constant term. Let B = B(x(cid:48), rp(x(cid:48))). By the deﬁnition of
rp, µ(B) ≥ p > 0. Then as n → ∞, Inequality 5 will eventually be satisﬁed, which implies B
contains at least k training points. The ﬁrst condition is then met.
The astuteness result follows because Ak(Sn, x) = y = 1 in ˆX +
r,1/2,p with probability 1.

r,1/2,p and Ak(Sn, x) = y = 0

in ˆX −

Corollary A.3. For n ≥ max(104, c4

d,k,δ/[(k + 1)2p2]) where

cd,k,δ = 4(d + 1) + (cid:112)16(d + 1)2 + 8(ln(8/δ) + k + 1)

, with probability at least 1 − 2δ, ρ(Ak(Sn, x)) ≥ r for all x in ˆX +
p > 0, δ > 0.

r,1/2,p

(cid:83) ˆX −

r,1/2,p and for all

In addition, with probability at least 1 − 2δ, the astuteness of the k-NN classiﬁer is at

least:

E(1(X ∈ ˆX +

r,1/2,p

(cid:91) ˆX −

r,1/2,p))

Proof. Without loss of generality, we look at a point x ∈ ˆX +
r,1/2,p. Let B = B(x(cid:48), rp(x(cid:48))),
J(B) = E(Y · 1(X ∈ B)) and ˆJ(B) be the empirical estimation of J(B). Notice that ˆJ(B)n
is the number of training points in B, because η(x) = 1 for all x ∈ B by the deﬁnition of
r-robust (1/2, p)-interior. It remains to ﬁnd a threshold n such that for all n(cid:48) > n,

By Lemma A.5, with probability 1 − 2δ,

ˆJ(B) ≥ (k + 1)/n(cid:48)

ˆJ(B) ≥ p − 2βn

p − 2β2
n

√

for all B ∈ Rd.

Therefore it suﬃces to ﬁnd a threshold n that satisﬁes

√

p − 2βn

p − 2β2

n ≥ (k + 1)/n,

where βn = (cid:112)(4/n)((d + 1) ln 2n + ln(8/δ)).
Solving this quadratic inequality yields

√

−

p + (cid:112)3p + (k + 1)/n

,

βn ≤

2

18

(6)

(7)

(8)

(9)

(10)

(11)

(12)

(13)

which can be re-written as

√

(8/

n)[(d + 1) ln(2n) + ln(8/δ) + (k + 1)/8] ≤ (cid:112)(k + 1)p

by substituting the expression for βn. This inequality does not admit an analytic solution.
Nevertheless, we observe that n1/4 ≥ ln(2n) for all n ≥ 104. Therefore it suﬃces to ﬁnd an
n ≥ 104 such that

√

(8/

n)[(d + 1)n1/4 + ln(8/δ) + (k + 1)/8] ≤ (cid:112)(k + 1)p.

Let m = n1/4. Inequality 11 can be re-written as

(cid:112)(k + 1)pm2 − 8(d + 1)m − (8 ln(8/δ) + (k + 1)) ≥ 0.

Solving this quadratic inequality with respct to m gives

m ≥

4(d + 1) + (cid:112)16(d + 1)2 + 8(ln(8/δ) + k + 1)
(cid:112)(k + 1)p

.

Letting

cd,k,δ = 4(d + 1) + (cid:112)16(d + 1)2 + 8(ln(8/δ) + k + 1)

, we ﬁnd a desired threshold

n = max(104, m4) ≥ max(104, c4

d,k,δ/[(k + 1)2p2]).

(14)

The astuteness result follows in a similar way to Theorem A.2.

A.3 Proofs for High k

A.3.1 Robustness of the Bayes Optimal Classiﬁer

Proof. (Of Theorem 3.2) Suppose x ∈ X +
r,0,0. Then, g(x) = 1. Consider any x(cid:48) ∈ Bo(x, r);
by deﬁnition, η(x(cid:48)) > 1/2, which implies that g(x(cid:48)) = 1 as well. Thus, ρ(g, x) ≥ r. The other
case (x ∈ X −

r,0,0) is symmetric.

Consider an x ∈ X +

r,0,0 (the other case is symmetric). We just showed that g has
robustness radius ≥ r at x. Moreover, p(y = 1 = g(x)|x) = η(x); therefore, g predicts the
correct label at x with probability η(x). The theorem follows by integrating over all x in
X +

r,0,0 ∪ X −

r,0,0.

A.3.2 Robustness of k-Nearest Neighbor

We begin by stating and proving a more technical version of Theorem 3.3.

Theorem A.4. For any n and data dimension d, deﬁne:

C0
n

(cid:114)

an =

(d log n + log(1/δ))

d log n + log(1/δ)
n

bn = C0
βn = (cid:112)(4/n)((d + 1) ln 2n + ln(8/δ))

19

where C0 is the constant in Theorem 15 of [4]. Now, pick kn and ∆n so that ∆n → 0 and
the following condition is satisﬁed:

and set

kn
n

≥

2βn + bn + (cid:112)(2βn + bn)2 + 2∆n(2β2

n + an)

∆n

pn =

(cid:16)

+

C0
n

d log n + log(1/δ)

kn
n
(cid:17)
+(cid:112)kn(d log n + log(1/δ)

Then, with probability ≥ 1 − 3δ, kn-NN has robustness radius r at all x ∈ X +
. In addition, with probability ≥ 1 − δ, the astuteness of kn-NN is at least:

r,∆n,pn

∪

X −

r,∆n,pn

E[η(X) · 1(X ∈ X +

)] + E(1 − η(X)) · 1(X ∈ X −

r,∆n,pn

r,∆n,pn

)]

Before we prove Theorem A.4, we need some deﬁnitions and lemmas.
For any Euclidean ball B in Rd, deﬁne J(B) = E[Y · 1(X ∈ B)] and ˆJ(B) as the

corresponding empirical quantity.

Lemma A.5. With probability ≥ 1 − 2δ, for all balls B in Rd, we have:

|J(B) − ˆJ(B)| ≤ 2β2

n + 2βn min((cid:112)J(B),

(cid:113)

ˆJ(B)),

where βn = (cid:112)(4/n)((d + 1) ln 2n + ln(8/δ)).
Proof. (Of Lemma A.5) Consider the two functions: h+
h−
B(x, y) = 1(y = −1, x ∈ B). From Lemma A.6, both h+
VC dimension at most d + 1. Additionally, J(B) = E[h+
of [4], along with an union bound gives the lemma.

B(x, y) = 1(y = 1, x ∈ B) and
B and h−
B are 0/1 functions with
B] − E[h−
B]. Applying Theorem 15

Lemma A.6. For an Euclidean ball B in Rd, deﬁne the function h+
as:

B : Rd×{−1, +1} → {0, 1}

h+
B(x, y) = 1(y = 1, x ∈ B)

and let HB = {h+
most d + 1.

B} be the class of all such functions. Then the VC-dimension of HB is at

Proof. (Of Lemma A.6) Let U be a set of d + 2 points in Rd; as the VC dimension of balls
in Rd is d + 1, U cannot be shattered by balls in Rd. Let UL = {(x, y)|x ∈ U } be a labeling
of U that cannot be achieved by any ball (with pluses inside and minuses outside); the
corresponding d + 1-dimensional points cannot be labeled accordingly by h+
B. Since U is
an arbitrary set of d + 2 points, this implies that any set of d + 2 points in Rd × {−1, +1}
cannot be shattered by HB. The lemma follows.

Lemma A.7. Let δp = C0
n
bility ≥ 1−δ, for all x, (cid:107)x−X(k+1)(x)(cid:107) ≤ rk/n+δp (x), and µ(B(x, (cid:107)x−X(k+1)(x)(cid:107))) ≥ k

. Then, with proba-
n −δp.

(cid:17)
d log n + log(1/δ) + (cid:112)k(d log n + log(1/δ)

(cid:16)

20

Figure. 3: Visualization of the halfmoon dataset. 1) Training sample of size n = 2000,
2) subset selected by Robust_1NN with defense radius r = 0.1, 3) subset selected by
Robust_1NN with defense radius r = 0.2.

Proof. (Of Lemma A.7) Observe that by deﬁnition for any x, rp is the smallest r such that
µ(B(x, rp(x)) ≥ p. The rest of the proof follows from Lemma 16 of [4].

Proof. (Of Theorem A.4)

From Lemma A.7, by uniform convergence of ˆµ, with probability ≥ 1 − δ, for all x(cid:48),
(cid:107)x(cid:48) − X (kn)(x(cid:48))(cid:107) ≤ rpn (x(cid:48)) and µ(B(x, (cid:107)x − X (kn)(x)(cid:107))) ≥ kn
,
r,∆n,pn
this implies that for all ˜x ∈ B(x(cid:48), X (kn)(x(cid:48))), η(˜x) ≥ 1/2 + ∆. Therefore, for such an
x(cid:48), J(B(x(cid:48), X (kn)(x(cid:48)))) ≥ ( 1
2 + ∆n)µ(B(x(cid:48), X (kn)(x(cid:48)))) ≥ ( 1
2 + ∆n)(kn/n − δp). Since for
B(x(cid:48), X (kn)(x(cid:48))), ˆµ(B(x(cid:48), X (kn)(x(cid:48)))) = kn
n . Thus we can apply Lemma A.5
to conclude that

n , min( ˆJ, J) ≤ k

If x(cid:48) ∈ X +

n − δp.

ˆJ(B) > J(B) − 2β2

n − 2βn

(cid:112)kn/n >

kn
2n

,

which implies that ˆY (B) = 1
kn
follows.

(cid:80)kn

i=1 Y (i)(x) = n
kn

ˆJ(B) > 1

2 . The ﬁrst part of the theorem

For the second part, observe that for an x ∈ X +

probability η(x) and for an x ∈ X −
Combining this with the ﬁrst part completes the proof.

r,∆n,pn

, the label Y is equal to +1 with
, the label Y is equal to −1 with probability 1 − η(x).

r,∆n,pn

B Proofs from Section 4

We begin with a statement of Chernoﬀ Bounds that we use in our calculations.

Theorem B.1.

[21] Let Xi be a 0/1 random variable and let X = 1
m

(cid:80)m

i=1 Xi. Then,

Pr(|X − E[X]| ≥ δ) ≤ e−mδ2/2 + e−mδ2/3 ≤ 2e−mδ2/3

Lemma B.2. Suppose we run Algorithm 1 with parameter r. Then, the points marked as
red by the algorithm form an r-separated subset of the training set.

21

Figure. 4: Adversarial examples of MNIST digit 1 images created by diﬀerent attack methods.
Top row: clean digit 1 test images. Middle row from left to right: 1) direct attack, 2)
white-box kernel attack. Bottom row from left to right: 1) black-box kernel attack, 2)
black-box neural net substitute attack.

Proof. Let f (xi) denote the output of Algorithm 2 on xi. If (xi, 1) is a Red point, then
f (xi) = 1 = f (xj) for all xj ∈ B(x, r); therefore, (xj, −1) cannot be marked as Red by the
algorithm as f (xj) (cid:54)= yj. The other case, where (xi, −1) is a Red point is similar.

Lemma B.3. Let x ∈ X such that Algorithm 1 ﬁnds a Red xi within Bo(x, τ ). Then,
Algorithm 1 has robustness radius at least r − 2τ at x.

Proof. For all x(cid:48) ∈ B(x, τ ), we have:

(cid:107)x(cid:48) − xi(cid:107) ≤ (cid:107)x − xi(cid:107) + (cid:107)x − x(cid:48)(cid:107) < 2τ

Since xi is a Red point, from Lemma B.2, any xj in training set output by Algorithm 1 with
yj (cid:54)= yi must have the property that (cid:107)xi − xj(cid:107) > 2r. Therefore,

(cid:107)x(cid:48) − xj(cid:107) ≥ (cid:107)xi − xj(cid:107) − (cid:107)x(cid:48) − xi(cid:107) > 2r − 2τ

Therefore, Algorithm 1 will assign x(cid:48) the label yi. The lemma follows.

Lemma B.4. Let B be a ball such that: (a) for all x ∈ B, η(x) > 1
µ(B) ≥ 2C0
one xi such that xi ∈ |B ∩ Xn| and yi = 1.

2 + ∆ and (b)
n (d log n + log(1/δ)). Then, with probability ≥ 1 − δ, all such balls have at least

Proof. Observe that J(B) ≥ C0
that ˆJ(B) > 0, which gives the theorem.

n (d log n + log(1/δ)). Applying Theorem 16 of [4], this implies

Lemma B.5. Fix ∆ and δ, and let kn = 3 log(2n/δ)

. Additionally, let

∆2

pn =

kn
n

+

C0
n

(d log n + log(1/δ) + (cid:112)kn(d log n + log(1/δ)),

22

Table 1: An evaluation of the black-box substitute classiﬁer. Each black-box substitute is
evaluated by: 1) its accuracy on the its training set, 2) its accuracy on the test set, and 3) the
percentage of predictions agreeing with the target classiﬁer on the test set. A combination of
high test accuracy and consistency with the original classiﬁer indicates the black-box model
emulates the target classiﬁer well.

Abalone

target f % training % test % testf
accuracy accuracy same as f
61.3%
62.5%
61.4%
63.5%
68.9%
64.1%
68.4%
65.0%

StandardNN 100%
100%
RobustNN
100%
ATNN
100%
ATNN-All
StandardNN 69.1%
87.2%
RobustNN
68.8%
ATNN
66.5%
ATNN-All
Halfmoon

72.6%
90.9%
73.7%
73.5%
68.6%
86.9%
68.4%
66.6%

target f % training % test % test

StandardNN 95.9%
97.7%
RobustNN
96.4%
ATNN
97.6%
ATNN-All
StandardNN 94.5%
94.2%
RobustNN
95.3%
ATNN
96.9%
ATNN-All

accuracy accuracy same as f
95.6%
94.9%
95.1%
96.8%
94.0%
90.5%
94.2%
96.2%

95.5%
97.6%
96.0%
97.3%
94.4%
94.1%
95.2%
96.5%

MNIST 1v7

target f % training % test % test

StandardNN 100%
100%
RobustNN
100%
ATNN
100%
ATNN-All
StandardNN 99.9%
99.8%
RobustNN
100%
ATNN
99.7%
ATNN-All

accuracy accuracy same as f
98.9%
95.4%
98.9%
98.7%
98.9%
94.8%
98.8%
98.9%

99.3%
97.6%
99.3%
99.3%
99.1%
98.7%
99.2%
99.3%

Kernel

Neural
Nets

Kernel

Neural
Nets

Kernel

Neural
Nets

where C0 is the constant in Theorem 15 of [4]. Deﬁne:

SRED = {(xi, yi) ∈ Sn|xi ∈ X +
r,∆,pn
(cid:19)
1
1
2
2

η(xi) −

yi =

sgn

(cid:18)

+

1
2

}

∪ X −

r,∆,p,

Then, with probability ≥ 1 − δ, all (xi, yi) ∈ SRED are marked as Red by Algorithm 1 run
with parameters r, ∆ and δ.

23

Proof. Consider a (xi, yi) ∈ SRED such that xi ∈ Xn∩X +
, and consider any (xj, yj) ∈ Sn
such that xj ∈ B(xi, r). From Lemma A.7, for all such xj, (cid:107)xj − X (kn)(xj)(cid:107) ≤ rpn (xj); this
means that all kn-nearest neighbors x(cid:48)(cid:48) of such an xj have η(x(cid:48)(cid:48)) > 1

r,∆,pn

Therefore, E[(cid:80)kn

l=1 Y (l)(xj)] ≥ kn(1/2 + ∆); by Theorem B.1, this means that for a
speciﬁc xj, Pr((cid:80)kn
l=1 Y (l)(xj) < 1/2) ≤ 2e−kn∆2/3, which is ≤ δ/n from our choice of kn.
By an union bound over all such xj, with probability ≥ 1 − δ, we see that Algorithm 2
reports the label g(xi) on all such xi, which is the same as yi by the deﬁnition of interiors;
xi therefore gets marked as Red.

2 + ∆.

Finally, we are ready to prove the main theorem of this section, which is a slightly more

technical form of Theorem 4.2.

Theorem B.6. Fix a ∆n, and pick kn and pn as in Lemma B.5. Suppose we run Algorithm 1
with parameters r, ∆n and δ. Consider the set:

XR =

(cid:40)

(cid:12)
(cid:12)x ∈ X +
(cid:12)

x

r+τ,∆n,pn

r+τ,∆n,pn

∪ X −

,

µ(B(x, τ )) ≥

(d log n + log(1/δ))

,

2C0
n

(cid:41)

where C0 is the constant in Theorem 15 of [4]. Then, with probability ≥ 1 − 2δ over the
training set, Algorithm 1 has robustness radius ≥ r − 2τ on XR. Additionally, its astuteness
at radius r − 2τ is at least E[η(X) · 1(X ∈ X +
)].

)] + E[(1 − η(X)) · 1(X ∈ X −

r+τ,∆n,pn

r+τ,∆n,pn

Proof. Due to the condition on µ(B(x, τ )), from Lemma B.4, with probability ≥ 1 − δ, all
x ∈ XR have the property that there exists a (xi, yi) in Sn such that yi = g(xi) and xi ∈
B(x, τ ). Without loss of generality, suppose that x ∈ X +
, so that η(x) > 1/2 + ∆n.
Then, from the properties of r-robust interiors, this xi ∈ X +
.

r+τ,∆n,pn

From Lemma B.5, with probability ≥ 1 − δ, this (xi, yi) is marked Red by Algorithm 1
run with parameters r, ∆n and δ. The theorem now follows from an union bound and
Lemma B.3.

r,∆n,pn

C Experiment Visualization and Validation

First, we show adversarial examples created by diﬀerent attacks on the MNIST dataset in
order to illustrate characteristics of each attack. Next, we show the subset of training points
selected by Algorithm 1 on the halfmoon dataset. The visualization illustrates the intuition
behind Algorithm 1 and also validates its implementation. Finally, we validate how eﬀective
the black-box subsitute classiﬁers emulate the target classiﬁer.

C.1 Adversarial Examples Created by Diﬀerent Attacks

Figure 4 shows adversarial examples created on MNIST digit 1 images with attack radius
r = 3. First, we observe that the perturbations added by direct attack, white-box kernel
attack and black-box kernel attack are clearly targeted: either a faint horizontal stroke or a
shadow of digit 7 are added to the original image. The perturbation budget is used on "key"
pixels that distinguish digit 1 and digit 7, therefore the attack is eﬀective. On the contrary,

24

black-box attacks with neural nets substitute adds perturbation to a large number of pixels.
While such perturbation often fools a neural net classiﬁer, it is not eﬀective against nearest
neighbors. Consider a pixel that is dark in most digit 1 and digit 7 training images; adding
brightness to this pixel increases the distance between the test image to training images from
both classes, therefore may not change the nearest neighbor to the test image.

Figure 4 also illustrates the break-down attack radius of visual similarity. At r = 3,
the true class of adversarial examples created by eﬀective attacks becomes ambiguous even
to humans. Our defense is successful as the Robust_1NN classiﬁers still have non-trivial
classiﬁcation accuracy at such attack radius. Meanwhile, we should not expect robustness
against even larger attack radius since the adversarial examples at r = 3 are already close to
the boundary of human perception.

C.2 Training Subset Selected by Robust_1NN

Figure 3 shows the training set selected by Robust_1NN on a halfmoon training set of size
2000. On the original training set, we see a noisy region between the two halfmoons where
both red and blue points appear. Robust_1NN cleans training points in this region so as to
create a gap between the red and blue halfmoons, and the gap width increases with defense
radius r.

C.3 Performance of Black-box Attack Substitutes

We validate the black-box substitute training process by checking the substitute’s accuracy
on its training set, the clean test set and the percentage of predictions agreeing with the
target classiﬁer on the clean test set. The results are shown in Table 1. For the halfmoon and
MNIST dataset, the substitute classiﬁers both achieve high accuracy on both the training
and test sets, and are also consistent with the target classiﬁer on the test set. The subsitutute
classiﬁers do not emulate the target classiﬁer on the Abalone dataset as close as on the other
two datasets due to the high noise level in the Abalone dataset. Nonetheless, the substitute
classiﬁer still achieve test time accuracy comparable to the target classiﬁer.

25

9
1
0
2
 
n
u
J
 
9
1
 
 
]
L
M

.
t
a
t
s
[
 
 
6
v
2
2
9
3
0
.
6
0
7
1
:
v
i
X
r
a

Analyzing the Robustness of Nearest Neighbors to
Adversarial Examples

Yizhen Wang
University of California, San Diego
yiw248@eng.ucsd.edu

Somesh Jha
University of Wisconsin-Madison
jha@cs.wisc.edu

Kamalika Chaudhuri
University of California, San Diego
kamalika@cs.eng.ucsd.edu

Abstract

Motivated by safety-critical applications, test-time attacks on classiﬁers via adversar-
ial examples has recently received a great deal of attention. However, there is a general
lack of understanding on why adversarial examples arise; whether they originate due to
inherent properties of data or due to lack of training samples remains ill-understood.
In this work, we introduce a theoretical framework analogous to bias-variance theory
for understanding these eﬀects. We use our framework to analyze the robustness of
a canonical non-parametric classiﬁer – the k-nearest neighbors. Our analysis shows
that its robustness properties depend critically on the value of k – the classiﬁer may
be inherently non-robust for small k, but its robustness approaches that of the Bayes
Optimal classiﬁer for fast-growing k. We propose a novel modiﬁed 1-nearest neighbor
classiﬁer, and guarantee its robustness in the large sample limit. Our experiments 1
suggest that this classiﬁer may have good robustness properties even for reasonable
data set sizes.

1

Introduction

Machine learning is increasingly applied in security-critical domains such as automotive
systems, healthcare, ﬁnance and robotics. To ensure safe deployment in these applications,
there is an increasing need to design machine-learning algorithms that are robust in the
presence of adversarial attacks.

A realistic attack paradigm that has received a lot of recent attention [12, 26, 28, 25] is
test-time attacks via adversarial examples. Here, an adversary has the ability to provide
modiﬁed test inputs to an already-trained classiﬁer, but cannot modify the training process
in any way. Their goal is to perturb legitimate test inputs by a “small amount” in order to
force the classiﬁer to report an incorrect label. An example is an adversary that replaces a
stop sign by a slightly defaced version in order to force an autonomous vehicle to recognize
it as an yield sign. This attack is undetectable to the human eye if the perturbation is small
enough.

1Code available at: https://github.com/EricYizhenWang/robust_nn_icml

1

Prior work has considered adversarial examples in the context of linear classiﬁers [19],
kernel SVMs [3] and neural networks [28, 12, 25, 26, 22]. However, most of this work has
either been empirical, or has focussed on developing theoretically motivated attacks and
defenses. Consequently, there is a general lack of understanding on why adversarial examples
arise; whether they originate due to inherent properties of data or due to lack of training
samples remains ill-understood.

This work develops a theoretical framework for robust learning in order to understand the
eﬀects of distributional properties and ﬁnite samples on robustness. Building on traditional
bias-variance theory [10], we posit that a classiﬁcation algorithm may be robust to adversarial
examples due to three reasons. First, it may be distributionally robust, in the sense that the
output classiﬁer is robust as the number of training samples grow to inﬁnity. Second, even
the output of a distributionally robust classiﬁcation algorithm may be vulnerable due to too
few training samples – this is characterized by ﬁnite sample robustness. Finally, diﬀerent
training algorithms might result in classiﬁers with diﬀerent degrees of robustness, which we
call algorithmic robustness. These quantities are analogous to bias, variance and algorithmic
eﬀects respectively.

Next, we analyze a simple non-parametric classiﬁcation algorithm: k-nearest neighbors in
our framework. Our analysis demonstrates that large sample robustness properties of this
algorithm depend very much on k.

Speciﬁcally, we identify two distinct regimes for k with vastly diﬀerent robustness
properties. When k is constant, we show that k-nearest neighbors has zero robustness in
the large sample limit in regions where p(y = 1|x) lies in (0, 1). This is in contrast with
dn log n), where d is the
accuracy, which may be quite high in these regions. For k = Ω(
data dimension and n is the sample size, we show that the robustness region of k-nearest
neighbors approaches that of the Bayes Optimal classiﬁer in the large sample limit. This is
again in contrast with accuracy, where convergence to the Bayes Optimal accuracy is known
for a much slower growing k [7, 5].
√

dn log n) is too high to use in practice with nearest neighbors, we next
propose a novel robust version of the 1-nearest neighbor classiﬁer that operates on a modiﬁed
training set. We provably show that in the large sample limit, this algorithm has superior
robustness to standard 1-nearest neighbors for data distributions with certain properties.

Since k = Ω(

√

Finally, we validate our theoretical results by empirically evaluating our algorithm on
three datasets against several popular attacks. Our experiments demonstrate that our
algorithm performs better than or about as well as both standard 1-nearest neighbors and
nearest neighbors with adversarial training – a popular and eﬀective defense mechanism.
This suggests that although our performance guarantees hold in the large sample limit, our
algorithm may have good robustness properties even for realistic training data sizes.

1.1 Related Work

Adversarial examples have recently received a great deal of attention [12, 3, 26, 28, 25].
Most of the work, however, has been empirical, and has focussed on developing increasingly
sophisticated attacks and defenses.

1.1.1 Related Work on Adversarial Examples

Prior theoretical work on adversarial examples falls into two categories – analysis and theory-
inspired defenses. Work on analysis includes [9], which analyzes the robustness of linear and

2

quadratic classiﬁers under random and semi-random perturbations. [13] provides robustness
guarantees on linear and kernel classiﬁers trained on a given data set. [11] shows that linear
classiﬁers for high dimensional datasets may have inherent robustness-accuracy trade-oﬀs.
Work on theory-inspired defenses include [20, 15, 1], who provide defense mechanisms for
adversarial examples in neural networks that are relaxations of certain principled optimization
objectives. [14] shows how to use program veriﬁcation to certify robustness of neural networks
around given inputs for small neural networks.

Our work diﬀers from these in two important ways. First, unlike most prior work which
looks at a given training dataset, we consider eﬀects of the data distribution and number of
samples, and analyze robustness properties in the large sample limit. Second, unlike prior
work which largely focuses on parametric methods such as neural networks, our focus is on a
canonical non-parametric method – the nearest neighbors classiﬁer.

1.1.2 Related Work on Nearest Neighbors

There has been a body of work on the convergence and consistency of nearest-neighbor
classiﬁers and their many variants [6, 27, 17, 8, 5, 16]; all these works however consider
accuracy and not robustness.

In the asymptotic regime, [6] shows that the accuracy of 1-nearest neighbors converges in
the large sample limit to 1 − 2R∗(1 − R∗) where R∗ is the error rate of the Bayes Optimal
classiﬁer. This implies that even 1-nearest neighbor may achieve relatively high accuracy
even when p(y = 1|x) is not 0 or 1. In contrast, we show that 1-nearest neighbor is inherently
non-robust when p(y = 1|x) ∈ (0, 1) under some continuity conditions.

For larger k, the accuracy of k-nearest neighbors is known to converge to that of the
Bayes Optimal classiﬁer if kn → ∞ and kn/n → 0 as the sample size n → ∞. We show that
the robustness also converges to that of the Bayes Optimal classiﬁer when kn grows at a
much higher rate – fast enough to ensure uniform convergence. Whether this high rate is
necessary remains an intriguing open question.

Finite sample rates on the accuracy of nearest neighbors are known to depend heavily on
properties of the data distribution, and there is no distribution free rate as in parametric
methods [8].
[5] provides a clean characterization of the ﬁnite sample rates of nearest
neighbors as a function of natural interiors of the classes. Here we build on their results
by deﬁning a stricter, more robust version of interiors and providing bounds as functions of
these new robust quantities.

1.1.3 Other Related Work

[2] provides a method for generating adversarial examples for nearest neighbors, and shows
that the eﬀectiveness of attacks grow with intrinsic dimensionality. Finally, [24, 25] provides
black-box attacks on substitute classiﬁers; their experiments show that attacks from other
types of substitute classiﬁers are not successful on nearest neighbors; our experiments
corroborate these results.

3

2 The Setting and Deﬁnitions

2.1 The Basic Setup

We consider test-time attacks in a white box setting, where the adversary has full knowledge
of the training process – namely, the type of classiﬁer used, the training data and any
parameters – but cannot modify training in any way.

Given an input x, the adversary’s goal is to perturb it so as to force the trained classiﬁer
f to report a diﬀerent label than f (x). The amount of perturbation is measured by an
application-speciﬁc metric d, and is constrained to be within a radius r. Our analysis can
be extended to any metric, but for this paper we assume that d is the Euclidean distance
for mathematical simplicity; we also focus on binary classiﬁcation, and leave extensions to
multiclass for future work.

Finally, we assume that unlabeled instances are drawn from an instance space X , and
their labels are drawn from the label space {0, 1}. There is an underlying data distribution
D that generates labeled examples; the marginal over X of D is µ and the conditional
distribution of labels given x is denoted by η.

2.2 Robustness and astuteness

We begin by deﬁning robustness, which for a classiﬁer f at input x is measured by the
robustness radius.

Deﬁnition 2.1 (Robustness Radius). The robustness radius of a classiﬁer f at an instance
x ∈ X , denoted by ρ(f, x), is the shortest distance between x and an input x(cid:48) to which f
assigns a label diﬀerent from f (x):

ρ(f, x) = inf
r

{∃x(cid:48) ∈ X ∩ B(x, r) s.t f (x) (cid:54)= f (x(cid:48))}

Observe that the robustness radius measures a classiﬁer’s local robustness. A classiﬁer
f with robustness radius r at x guarantees that no adversarial example of x with norm
of perturbation less than r can be created using any attack method. A plausible way
to extend this into a global notion is to require a lower bound on the robustness radius
everywhere; however, only the constant classiﬁer will satisfy this condition. Instead, we
consider robustness around meaningful instances, that we model as examples drawn from
the underlying data distribution.

Deﬁnition 2.2 (Robustness with respect to a Distribution). The robustness of a classiﬁer
f at radius r with respect to a distribution µ over the instance space X , denoted by R(f, r, µ),
is the fraction of instances drawn from µ for which the robustness radius is greater than or
equal to r.

R(f, r, µ) = Pr
x∼µ

(ρ(f, x) ≥ r)

Finally, observe that we are interested in classiﬁers that are both robust and accurate.
This leads to the notion of astuteness, which measures the fraction of instances on which a
classiﬁer is both accurate and robust.

Deﬁnition 2.3 (astuteness). The astuteness of a classiﬁer f with respect to a data distribu-
tion D and a radius r is the fraction of examples on which it is accurate and has robustness
radius at least r; formally,

AstD(f, r) = Pr

(ρ(f, x) ≥ r, f (x) = y),

(x,y)∼D

4

Observe that astuteness is analogous to classiﬁcation accuracy, and we argue that it is
a more appropriate metric if we are concerned with both robustness and accuracy. Unlike
accuracy, astuteness cannot be directly empirically measured unless we have a way to certify
a lower bound on the robustness radius. In this work, we will prove bounds on the astuteness
of classiﬁers, and in our experiments, we will approximate it by measuring resistance to
standard attacks.

2.3 Sources of Robustness

There are three plausible reasons why classiﬁers lack robustness – distributional, ﬁnite sample
and algorithmic. These sources are analogous to bias, variance, and algorithmic eﬀects
respectively in standard bias-variance theory.

Distributional robustness measures the eﬀect of the data distribution on robustness
when an inﬁnitely large number of samples are used to train the classiﬁer. Formally, if
Sn is a training sample of size n drawn from D and A(Sn, ·) is a classiﬁer obtained by
applying the training procedure A on Sn, then the distributional robustness at radius r is
limn→∞ ESn∼D[R(A(Sn, ·), r, µ)].

In contrast, for ﬁnite sample robustness, we characterize the behaviour of R(A(Sn, ·), r, µ)
for ﬁnite n – usually by putting high probability bounds over the training set. Thus, ﬁnite
sample robustness depends on the training set size n, and quantiﬁes how it changes with
sample size. Finally, robustness also depends on the training algorithm itself; for example,
some variants of nearest neighbors may have higher robustness than nearest neighbors itself.

2.4 Nearest Neighbor and Bayes Optimal Classiﬁers

Given a training set Sn = {(X1, Y1), . . . , (Xn, Yn)} and a test example x, we use the notation
X (i)(x) to denote the i-th nearest neighbor of x in Sn, and Y (i)(x) to denote the label of
X (i)(x).

Given a test example x, the k-nearest neighbor classiﬁer Ak(Sn, x) outputs:

= 1,

= 0,

if Y (1)(x) + . . . + Y (k)(x) ≥ k/2
otherwise.

The Bayes optimal classiﬁer g over a data distribution D has the following classiﬁcation

rule:

g(x) =

(cid:26) 1
0

if η(x) = Pr(y = 1|x) ≥ 1/2;
otherwise.

(1)

3 Robustness of Nearest Neighbors

How robust is the k-nearest neighbor classiﬁer? We show that it depends on the value of k.
Speciﬁcally, we identify two distinct regimes – constant k and k = Ω(
dn log n) where d is
the data dimension – and show that nearest neighbors has diﬀerent robustness properties in
the two.

√

3.1 Low k Regime

In this region, k is a constant that does not depend on the training set size n. Provided
certain regularity conditions hold, we show that k-nearest neighbors is inherently non-robust

5

in this regime unless η(x) ∈ {0, 1} – in the sense that the distributional robustness becomes
0 in the large sample limit.

Theorem 3.1. Let x ∈ X ∩ supp(µ) such that (a) µ is absolutely continuous with respect
to the Lebesgue measure (b) η(x) ∈ (0, 1) (c) η is continuous with respect to the Euclidean
metric in a neighborhood of x. Then, for ﬁxed k, ρ(Ak(Sn, ·), x) converges in probability to
0.

Remarks. Observe that Theorem 3.1 implies that the distributional robustness (and hence
astuteness) in a region where η(x) ∈ (0, 1) is 0. This is in contrast with accuracy; for 1-NN,
the accuracy converges to 1 − 2R∗(1 − R∗) as n → ∞, where R∗ is the error rate of the
Bayes Optimal classiﬁer, and thus may be quite high.

The proof of Theorem 3.1 in the Appendix shows that the absolute continuity of µ with
respect to the Lebesgue measure is not strictly necessary; absolute continuity with respect
to an embedded manifold will give the same result, but will result in a more complex proof.
In the Appendix A (Theorem A.2), we show that k-nearest neighbor is astute in the

interior of the region where η ∈ {0, 1}, and provide ﬁnite sample rates for this case.

3.2 High k Regime

√

Prior work has shown that in the large sample limit, the accuracy of the nearest neighbor
classiﬁers converge to the Bayes Optimal, provided k is set properly. We next show that
if k is Ω(
dn log n), the regions of robustness and the astuteness of the k nearest neighbor
classiﬁers also approach the corresponding quantities for the Bayes Optimal classiﬁer as
n → ∞. Thus, if the Bayes Optimal classiﬁer is robust, then so is k-nearest neighbors in the
large sample limit.

The main intuition is that k = Ω(

dn log n) is large enough for uniform convergence –
where, with high probability, all Euclidean balls with k examples have the property that the
empirical averages of their labels are close to their expectations. This guarantees that for
any x, the k-nearest neighbor reports the same label as the Bayes Optimal classiﬁer for all
x(cid:48) close to x. Thus, if the Bayes Optimal classiﬁer is robust, so is nearest neighbors.

√

3.2.1 Deﬁnitions

We begin with some deﬁnitions that we can use to characterize the robustness of the Bayes
Optimal classiﬁer. Following [5], we use the notation Bo(x, r) to denote an open ball and
B(x, r) to denote a closed ball of radius r around x. We deﬁne the probability radius of a
ball around x as:

We next deﬁne the r-robust (p, ∆)-strict interiors as follows:

rp(x) = inf{r | µ(B(x, r)) ≥ p}

X +

r,∆,p = {x ∈ supp(µ) | ∀x(cid:48) ∈ Bo(x, r),

∀x(cid:48)(cid:48) ∈ B(x(cid:48), rp(x(cid:48))), η(x(cid:48)(cid:48)) > 1/2 + ∆}

X −

r,∆,p = {x ∈ supp(µ) | ∀x(cid:48) ∈ Bo(x, r),

∀x(cid:48)(cid:48) ∈ B(x(cid:48), rp(x(cid:48))), η(x(cid:48)(cid:48)) < 1/2 − ∆}

What is the signiﬁcance of these interiors? Let x(cid:48) be an instance such that all x(cid:48)(cid:48) ∈
n , then the k points x(cid:48)(cid:48) closest to x(cid:48) have

B(x(cid:48), rp(x(cid:48))) have η(x(cid:48)(cid:48)) > 1/2 + ∆. If p ≈ k

6

η(x(cid:48)(cid:48)) > 1/2 + ∆. Provided the average of the labels of these points is close to expectation,
which happens when k is large relative to 1/∆, k-nearest neighbor outputs label 1 on x(cid:48).
When x is in the r-robust (p, ∆)-strict interior region X +
r,∆,p, this is true for all x(cid:48) within
distance r of x, which means that k-nearest neighbors will be robust at x. Thus, the r-robust
(p, ∆)-strict interior is the region where we natually expect k-nearest neighbor to have
robustness radius r, when k is large relative to 1

∆ and p ≈ k
n .

Readers familiar with [5] will observe that the set of all x(cid:48) for which ∀x(cid:48)(cid:48) ∈ B(x(cid:48), rp(x(cid:48))), η(x(cid:48)(cid:48)) >

1/2 + ∆ forms a stricter version of the (p, ∆)-interiors of the 1 region that was deﬁned in
this work; these x(cid:48) also represent the region where k-nearest neighbors are accurate when
k ≈ max(np, 1/∆2). The r-robust (p, ∆)-strict interior is thus a somewhat stricter and more
robust version of this deﬁnition.

3.2.2 Main Results

We begin by characterizing where the Bayes Optimal classiﬁer is robust.

Theorem 3.2. The Bayes Optimal classiﬁer has robustness radius r at x ∈ X +
Moreover, its astuteness is E[η(x)1(x ∈ X +

r,0,0)] + E[(1 − η(x))1(x ∈ X −

r,0,0)].

r,0,0 ∪ X −

r,0,0.

The proof is in the Appendix, along with analogous results for astuteness. The following
theorem, along with a similar result for astuteness, proved in the Appendix, characterizes
robustness in the large k regime.

√

Theorem 3.3. For any n, pick a δ and a ∆n → 0. There exist constant C1 and C2 such that
if kn ≥ C1
n (1 + C2
≥ 1 − 3δ, kn-NN has robustness radius r in x ∈ X +

(cid:113) d log n+log(1/δ)
kn
∪ X −

), then, with probability

, and pn ≥ kn

dn log n+n log(1/δn)

∆n

.

r,∆n,pn

r,∆n,pn

Remarks. Some remarks are in order. First, observe that as n → ∞, ∆n and pn tend
to 0; thus, provided certain continuity conditions hold, X +
approaches
X +

r,∆n,pn
r,0,0, the robustness region of the Bayes Optimal classiﬁer.

r,0,0 ∪ X −

∪ X −

r,∆n,pn

Second, observe that as r-robust strict interiors extend the deﬁnition of interiors in [5],
Theorem 3.3 is a robustness analogue of Theorem 5 in this work. Unlike the latter, Theo-
rem 3.3 has a more stringent requirement on k. Whether this is necessary is left as an open
question for future work.

4 A Robust 1-NN Algorithm

√

Section 3 shows that nearest neighbors is robust for k as large as Ω(
dn log n). However,
this k is too high to use in practice – high values of k require even higher sample sizes [5],
and lead to higher running times. Thus a natural question is whether we can ﬁnd a more
robust version of the algorithm for smaller k. In this section, we provide a more robust
version of 1-nearest neighbors, and analytically demonstrate its robustness.

Our algorithm is motivated by the observation that 1-nearest neighbor is robust when
oppositely labeled points are far apart, and when test points lie close to training data. Most
training datasets however contain nearby points that are oppositely labeled; thus, we propose
to remove a subset of training points to enforce this property.

Which points should we remove? A plausible approach is to keep the largest subset where
oppositely labeled points are far apart; however, this subset has poor stability properties

7

even for large n. Therefore, we propose to keep all points x such that: (a) we are highly
conﬁdent about the label of x and its nearby points and (b) all points close to x have the
same label. Given that all such x are kept, we remove as few points as possible, and execute
nearest neighbors on the remaining dataset.

The following deﬁnition characterizes data where oppositely labeled points are far apart.

Deﬁnition 4.1 (r-separated set). A set A = {(x1, y1), . . . , (xm, ym)} of labeled examples is
said to be r-separated if for all pairs (xi, yi), (xj, yj) ∈ A, (cid:107)xi − xj(cid:107) ≤ r implies yi = yj.

The full algorithm is described in Algorithm 1 and Algorithm 2. Given conﬁdence
parameters ∆ and δ, Algorithm 2 returns a 0/1 label when this label agrees with the average
of kn points closest to x; otherwise, it returns ⊥. kn is chosen such that with probability
≥ 1 − δ, the empirical majority of kn labels agrees with the majority in expectation, provided
the latter is at least ∆ away from 1
2 .

Algorithm 2 is used to determine whether an xi should be kept. Let f (xi) be the output
of Algorithm 2 on xi. If yi = f (xi) and if for all xj ∈ B(xi, r), f (xi) = f (xj) = yi, then
we mark xi as red. Finally, we compute the largest r-separated subset of the training data
that includes all the red points; this reduces to a constrained matching problem as in [16].
The resulting set, returned by Algorithm 1, is our new training set. We observe that this
set is r-separated from Lemma B.2 in the Appendix, and thus oppositely labeled points are
far apart. Moreover, we keep all (xi, yi) when we are conﬁdent about the label of xi and
its nearby points. Observe that our ﬁnal procedure is a 1-NN algorithm, even though kn
neighbors are used to determine if a point should be retained in the training set.

Algorithm 1 Robust_1NN(Sn, r, ∆, δ, x)

f (xi) = Conﬁdent-Label(Sn, ∆, δ, xi)

for (xi, yi) ∈ Sn do

end for
SRED = ∅
for (xi, yi) ∈ Sn do

if f (xi) = yi and f (xi) = f (xj) for all xj such that (cid:107)xi − xj(cid:107) ≤ r and (xj, yj) ∈ Sn
then

SRED = SRED

(cid:83){(xi, yi)}

end if
end for
Let S(cid:48) be the largest 2r-separated subset of Sn that contains all points in SRED.
return new training set S(cid:48)

Algorithm 2 Conﬁdent-Label(Sn, ∆, δ, x)

kn = 3 log(2n/δ)/∆2
¯y = (1/kn) (cid:80)kn
2 − ∆, 1
if ¯y ∈ [ 1

i=1 Y (i)(x)
2 + ∆] then

return ⊥

else

end if

return 1

2 sgn(¯y − 1

2 ) + 1

2

8

4.1 Performance Guarantees

The following theorem establishes performance guarantees for Algorithm 1.

Theorem 4.2. Pick a ∆n and δ, and set kn = 3 log(2n/δ)/∆2
n. Pick a margin parameter
τ . Then, there exist constants C and C0 such that the following hold. If we set pn =
kn
n (1 + C

(cid:113) d log n+log(1/δ)
kn

), and deﬁne the set:

XR =

(cid:40)

(cid:12)
(cid:12)x ∈ X +
(cid:12)
x

r+τ,∆n,pn

r+τ,∆n,pn

∪ X −

,

µ(B(x, τ )) ≥

(d log n + log(1/δ))

2C0
n

(cid:41)

Then, with probability ≥ 1 − 2δ over the training set, Algorithm 1 run with parameters r, ∆n
and δ has robustness radius at least r − 2τ on XR.

Remarks. The proof is in the Appendix, along with an analogous result for astuteness.
Observe that XR is roughly the high density subset of the r + τ -robust strict interior
X +
2 + ∆n or less than
r+τ,∆n,pn
1
2 − ∆n in this region, as opposed to 0 or 1, this is an improvement over standard nearest
neighbors when the data distribution has a large high density region that intersects with the
interiors.

. Since η(x) is constrained to be greater than 1

r+τ,∆n,pn

∪ X −

A second observation is that as τ is an arbitrary constant, we can set to it be quite small
and still satisfy the condition on µ(B(x, τ )) for a large fraction of x’s when n is very large.
This means that in the large sample limit, r − 2τ may be close to r and XR may be close to
the high density subset of X +

for a lot of smooth distributions.

∪ X −

r,∆n,pn

r,∆n,pn

5 Experiments

The results in Section 4 assume large sample limits. Thus, a natural question is how well
Algorithm 1 performs with more reasonable amounts of training data. We now empirically
investigate this question.

Since there are no general methods that certify robustness at an input, we assess
robustness by measuring how our algorithm performs against a suite of standard attack
methods. Speciﬁcally, we consider the following questions:

1. 1. How does our algorithm perform against popular white box and black box attacks

compared with standard baselines?

2. 2. How is performance aﬀected when we change the training set size relative to the

data dimension?

These questions are considered in the context of three datasets with varying training set
sizes relative to the dimension, as well as two standard white box attacks and black box
attacks with two kinds of substitute classiﬁers.

9

Figure. 1: White Box Attacks: Plot of classiﬁcation accuracy on adversarial examples v.s.
attack radius. Top row: Direct Attack. Bottom row: Kernel Substitute Attack. Left to right:
1) Halfmoon, 2) MNIST 1v 7 and 3) Abalone.

Figure. 2: Black Box Attacks: Plot of classiﬁcation accuracy on adversarial examples v.s.
attack radius. Top to Bottom: 1) kernel substitute, 2) neural net substitute. Left to right:
1) Halfmoon, 2) MNIST 1 v.s. 7 and 3) Abalone.

5.1 Methodology

5.1.1 Data

We use three datasets – Halfmoon, MNIST 1v7 and Abalone – with diﬀering data sizes
relative to dimension. Halfmoon is a popular 2-dimensional synthetic data set for non-linear
classiﬁcation. We use a training set of size 2000 and a test set of size 1000 generated with
standard deviation σ = 0.2. The MNIST 1v7 data set is a subset of the 784-dimensional
MNIST data. For training, we use 1000 images each of Digit 1 and 7, and for test, 500 images
of each digit. Finally, for the Abalone dataset [18], our classiﬁcation task is to distinguish
whether an abalone is older than 12.5 years based on 7 physical measurements. For training,
we use 500 and for test, 100 samples. In addition, a validation set with the same size as the

10

test set is generated for each experiment for parameter tuning.

5.1.2 Baselines

We compare Algorithm 1, denoted by RobustNN, against three baselines. The ﬁrst is
the standard 1-nearest neighbor algorithm, denoted by StandardNN. We use two forms
of adversarially-trained nearest neighbors - ATNN and ATNN-all. Let S be the training
set used by standard nearest neighbors. In ATNN, we augment S by creating, for each
(x, y) ∈ S, an adversarial example xadv using the attack method in the experiment, and
adding (xadv, y). The ATNN classiﬁer is 1-nearest neighbor on this augmented data. In
ATNN-all, for each (x, y) ∈ S, we create adversarial examples using all the attack methods
in the experiment, and add them all to S. ATNN-all is the nearest neighbor classiﬁer on this
augmented data. For example, for white box Direct Attacks in Section 5.2, ATNN includes
adversarial examples generated by the Direct Attack, and ATNN-all includes adversarial
examples generated by both Direct and Kernel Substitute Attacks.

Observe that all algorithms except StandardNN have parameters to tune. RobustNN
has three input parameters – ∆, δ and a defense radius r which is an approximation to the
robustness radius. For simplicity, we set ∆ = 0.45, δ = 0.1 and tune r on the validation set;
this can be viewed as tuning the parameter τ in Theorem 4.2. For ATNN and ATNN-all, the
methods that generate the augmenting adversarial examples need a perturbation magnitude
r; we call this the defense radius. To be fair to all algorithms, we tune the defense radius
for each. We consider the adversary with the highest attack perturbation magnitude in the
experiment, and select the defense radius that yields the highest validation accuracy against
this adversary.

5.2 White-box Attacks and Results

To evaluate the robustness of Algorithm 1, we use two standard classes of attacks – white
box and black box. For white-box attacks, the adversary knows all details about the classiﬁer
under attack, including its training data, the training algorithm and any hyperparameters.

5.2.1 Attack Methods

We consider two white-box attacks – direct attack [2] and Kernel Substitute Attack [24].
Direct Attack. This attack takes as input a test example x, an attack radius r, and a
training dataset S (which may be an augmented or reduced dataset). It ﬁnds an x(cid:48) ∈ S that is
closest to x but has a diﬀerent label, and returns the adversarial example xadv = x+r x−x(cid:48)
.
||x−x(cid:48)||2
Kernel Substitute Attack. This method attacks a substitute kernel classiﬁer trained on
the same training set. For a test input (cid:126)x, a set of training points Z with one-hot labels Y , a
kernel classiﬁer f predicts the class probability as:

The adversary trains a kernel classiﬁer on the training set of the corresponding nearest
neighbors, and then generates adversarial examples against this kernel classiﬁer. The
advantage is that the prediction of the kernel classiﬁer is diﬀerentiable, which allows the
use of standard gradient-based attack methods. For our experiments, we use the popular

f : (cid:126)x →

2/c(cid:105)

(cid:104)

e−||(cid:126)x−(cid:126)z||2
(cid:80)

(cid:126)z∈X e−||(cid:126)x−(cid:126)z||2

(cid:126)z∈X
2/c

· Y

11

fast-gradient-sign method (FSGM). The parameter c is tuned to yield the most eﬀective
attack, and is set to 0.1 for Halfmoon and MNIST, and 0.01 for Abalone.

5.2.2 Results

Figure 1 shows the results. We see that RobustNN outperforms all baselines for Halfmoon
and Abalone for all attack radii. For MNIST, for low attack radii, RobustNN’s classiﬁcation
accuracy is slightly lower than the others, while it outperforms the others for large attack
radii. Additionally, as is to be expected, the Direct Attack results in lower general accuracy
than the Kernel Substitute Attack.

These results suggest that our algorithm mostly outperforms the baselines StandardNN,
ATNN and ATNN-all. As predicted by theory, the performance gain is higher when the
training set size is large relative to the dimension – which is the setting where nearest
neighbors work well in general. It has superior performance for Halfmoon and Abalone,
where the training set size is large to medium relative to dimension. In contrast, in the
sparse dataset MNIST, our algorithm has slightly lower classiﬁcation accuracy for small
attack radii, and higher otherwise.

5.3 Black-box Attacks and Results

[25] has observed that some defense methods that work by masking gradients remain highly
amenable to black box attacks. In this attack, the adversary is unaware of the target classiﬁer’s
nature, parameters or training data, but has access to a seed dataset drawn from the same
distribution which they use to train and attack a substitute classiﬁer. To establish robustness
properties of Algorithm 1, we therefore validate it against black box attacks based on two
types of substitute classiﬁers.

5.3.1 Attack Methods

We use two types of substitute classiﬁers – kernel classiﬁers and neural networks. The
adversary trains the substitute classiﬁer using the method of [25] and uses the adversarial
examples against the substitute to attack the target classiﬁer.

Kernel Classiﬁer. The kernel classiﬁer substitute is the same as the one in Section 5.2,
but trained using the seed data and the method of [25].
Neural Networks. The neural network for MNIST is the ConvNet in [23]’s tutorial. For
Halfmoon and Abalone, the network is a multi-layer perceptron with 2 hidden layers.
Procedure. To train the substitute classiﬁer, the adversary uses the method of [24] to
augment the seed data for two rounds; labels are obtained by querying the target classiﬁer.
Adversarial examples against the substitutes are created by FGSM, following [24]. As a
sanity check, we verify the performance of the substitute classiﬁers on the original training
and test sets. Details are in Table 1 in the Appendix. Sanity checks on the performance of
the substitute classiﬁers are presented in Table 1 in the Appendix.

5.3.2 Results

Figure 2 shows the results. For all algorithms, black box attacks are less eﬀective than white
box, which corroborates the results of [24], who observed that black-box attacks are less
successful against nearest neighbors. We also ﬁnd that the kernel substitute attack is more
eﬀective than the neural network substitute, which is expected as kernel classiﬁers have

12

similar structure to nearest neighbors. Finally, for Halfmoon and Abalone, our algorithm
outperforms the baselines for both attacks; however, for MNIST neural network substitute,
our algorithm does not perform as well for small attack radii. This again conﬁrms the
theoretical predictions that our algorithm’s performance is better when the training set is
large relative to the data dimension – the setting in which nearest neighbors work well in
general.

5.4 Discussion

The results show that our algorithm performs either better than or about the same as
standard baselines against popular white box and black box attacks. As expected from
our theoretical results, it performs better for denser datasets which have high or medium
amounts of training data relative to the dimension, and either slightly worse or better for
sparser datasets, depending on the attack radius. Since non-parametric methods such as
nearest neighbors are mostly used for dense data, this suggests that our algorithm has good
robustness properties even with reasonable amounts of training data.

6 Conclusion

We introduce a novel theoretical framework for learning robust to adversarial examples, and
introduce notions of distributional and ﬁnite-sample robustness. We use these notions to
analyze a non-parametric classiﬁer, k-nearest neighbors, and introduce a novel modiﬁed
1-nearest neighbor algorithm that has good robustness properties in the large sample limit.
Experiments show that these properties are still retained for reasonable data sizes.

Many open questions remain. The ﬁrst is to close the gap in analysis of k-nearest
neighbors for k in between our two regimes. The second is to develop nearest neighbor
algorithms with better robustness guarantees. Finally, we believe that our work is a ﬁrst
step towards a comprehensive analysis of how the size of training data aﬀects robustness;
we believe that an important line of future work is to carry out similar analyses for other
supervised learning methods.

Acknowledgement

We thank NSF under IIS 1253942 for support. This work was also partially supported by
ARO under contract number W911NF-1-0405. We thank all anonymous reviewers for their
constructive comments.

We also thank Rabanus Derr from University of Tübingen for pointing about a minor

mistake in the proof of Lemma B.3. The proof is ﬁxed in the latest version.

References

[1] John Duchi Aman Sinha, Hongseok Namkoong. Certiﬁable distributional robustness with
principled adversarial training. International Conference on Learning Representations,
2018.

13

[2] Laurent Amsaleg, James Bailey, Sarah Erfani, Teddy Furon, Michael E Houle, Miloš
Radovanović, and Nguyen Xuan Vinh. The vulnerability of learning to adversarial
perturbation increases with intrinsic dimensionality. 2016.

[3] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndić, Pavel
Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at
test time. In Joint European Conference on Machine Learning and Knowledge Discovery
in Databases, pages 387–402. Springer, 2013.

[4] Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for the cluster tree.
In J. D. Laﬀerty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta,
editors, Advances in Neural Information Processing Systems 23, pages 343–351. Curran
Associates, Inc., 2010.

[5] Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for nearest neighbor
classiﬁcation. In Advances in Neural Information Processing Systems, pages 3437–3445,
2014.

[6] T. Cover and P.E. Hart. Nearest neighbor pattern classiﬁcation. IEEE Transactions on

Information Theory, 13:21–27, 1967.

[7] Luc Devroye, Laszlo Gyorﬁ, Adam Krzyzak, and Gabor Lugosi. On the strong universal
consistency of nearest neighbor regression function estimates. The Annals of Statistics,
pages 1371–1385, 1994.

[8] Luc P Devroye and Terry J Wagner. The strong uniform consistency of nearest neighbor

density estimates. The Annals of Statistics, pages 536–540, 1977.

[9] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of
classiﬁers: from adversarial to random noise. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems
29, pages 1632–1640. Curran Associates, Inc., 2016.

[10] Jerome Friedman, Trevor Hastie, Robert Tibshirani, et al. Additive logistic regression:
a statistical view of boosting (with discussion and a rejoinder by the authors). The
annals of statistics, 28(2):337–407, 2000.

[11] Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu, Martin
Wattenberg, and Ian Goodfellow. Adversarial spheres. arXiv preprint arXiv:1801.02774,
2018.

[12] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing

adversarial examples. arXiv preprint arXiv:1412.6572, 2014.

[13] Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness
of a classiﬁer against adversarial manipulation. In Advances in Neural Information
Processing Systems, pages 2263–2273, 2017.

[14] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer.
Towards proving the adversarial robustness of deep neural networks. arXiv preprint
arXiv:1709.02802, 2017.

14

[15] J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the

convex outer adversarial polytope. arXiv preprint arXiv:1711.00851, 2017.

[16] Aryeh Kontorovich and Roi Weiss. A bayes consistent 1-nn classiﬁer. In Artiﬁcial

Intelligence and Statistics Conference, 2015.

[17] S. Kulkarni and S. Posner. Rates of convergence of nearest neighbor estimation under
arbitrary sampling. IEEE Transactions on Information Theory, 41(4):1028–1039, 1995.

[18] M. Lichman. UCI machine learning repository, 2013.

[19] Daniel Lowd and Christopher Meek. Adversarial learning. In Proceedings of the eleventh
ACM SIGKDD international conference on Knowledge discovery in data mining, pages
641–647. ACM, 2005.

[20] Aleksander Mądry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian
Vladu. Towards deep learning models resistant to adversarial attacks. stat, 1050:9, 2017.

[21] Michael Mitzenmacher and Eli Upfal. Probability and computing: Randomized algorithms

and probabilistic analysis. Cambridge university press, 2005.

[22] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a
simple and accurate method to fool deep neural networks. IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2016.

[23] Nicolas Papernot, Nicholas Carlini, Ian Goodfellow, Reuben Feinman, Fartash Faghri,
Alexander Matyasko, Karen Hambardzumyan, Yi-Lin Juang, Alexey Kurakin, Ryan
Sheatsley, Abhibhav Garg, and Yen-Chen Lin. cleverhans v2.0.0: an adversarial machine
learning library. arXiv preprint arXiv:1610.00768, 2017.

[24] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine
learning: from phenomena to black-box attacks using adversarial samples. arXiv preprint
arXiv:1605.07277, 2016.

[25] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Berkay Celik, and
Ananthram Swami. Practical black-box attacks against deep learning systems using
adversarial examples. In Proceedings of the 2017 ACM Asia Conference on Computer
and Communications Security, 2017.

[26] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik,
and Ananthram Swami. The limitations of deep learning in adversarial settings. In
Proceedings of the 1st IEEE European Symposium on Security and Privacy. arXiv
preprint arXiv:1511.07528, 2016.

[27] C. Stone. Consistent nonparametric regression. Annals of Statistics, 5:595–645, 1977.

[28] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint
arXiv:1312.6199, 2013.

15

A Proofs from Section 3

A.1 Proofs for Constant k

Proof. (Of Theorem 3.1) To show convergence in probability, we need to show that for all
(cid:15), δ > 0, there exists an n((cid:15), δ) such that Pr(ρ(Ak(Sn, ·), x) ≥ (cid:15)) ≤ δ for n ≥ n0((cid:15), δ).

The proof will again proceed in two stages. First, we show in Lemma A.1 that if the
conditions in the statement of Theorem 3.1 hold, then there exists some n((cid:15), δ) such that
for n ≥ n((cid:15), δ), with probability at least 1 − δ, there exists two points x+ and x− in B(x, (cid:15))
such that (a) all k nearest neighbors of x+ have label 1, (b) all k nearest neighbors of x−
have label 0, and (c) x+ (cid:54)= x−.

Next we show that if the event stated above happens, then ρ(Ak(Sn, ·), x) ≤ (cid:15). This is
because Ak(Sn, x+) = 1 and Ak(Sn, x−) = 0. No matter what Ak(Sn, x) is, we can always
ﬁnd a point x(cid:48) that lies in {x+, x−} ⊂ B(x, (cid:15)) such that the prediction at x(cid:48) is diﬀerent from
Ak(Sn, x).

Lemma A.1. If the conditions in the statement of Theorem 3.1 hold, then there exists
some n((cid:15), δ) such that for n ≥ n((cid:15), δ), with probability at least 1 − δ, there are two points x+
and x− in B(x, (cid:15)) such that (a) all k nearest neighbors of x+ have label 1, (b) all k nearest
neighbors of x− have label 0, and (c) x+ (cid:54)= x−.

Proof. (Of Lemma A.1) The proof consists of two major components. First, for large enough
n, with high probability there are many disjoint balls in the neighborhood of x such that
each ball contains at least k points in Sn. Second, with high probability among these balls,
there exists a ball such that the k neareast neighbors of its center all have label 1. Similarly,
there exists a ball such that the k nearest neighbor of its center all have label 0.

Since µ is absolutely continuous with respect to Lebesgue measure in the neighbor-
bood of x and η is continuous, then for any m ∈ Z+, we can always ﬁnd m balls
B(x1, r1), · · · , B(xm, rm) such that (a) all m balls are disjoint, and (b) for all i ∈ {1, · · · , m},
we have xi ∈ B(x, (cid:15)), µ(B(xi, ri)) > 0 and η(x) ∈ (0, 1) for x ∈ B(xi, ri). For simplicity, we
(cid:84) Sn. Also, let
use Bi to denote B(xi, ri) and ci(n) to denote the number of points in Bi
µmin = mini∈{1,··· ,m} µ(Bi). Then by Hoeﬀding’s inequality, for each ball Bi and for any
n > k+1
µmin

,

Pr[ci(n) < k] ≤ exp(−2nµ2

min/(k + 1)2),

where the randomness comes from drawing sample Sn. Then taking the union bound over
all m balls, we have

Pr[∃i ∈ {1, · · · , m} such that ci(n) < k] ≤ m exp(−2nµ2
(cid:16) k+1
µmin

, [log m−log(δ/3)](k+1)2
which implies that when n > max
µ2
1 − δ/3, each of B1, · · · , Bm contains at least k points in Sn.

(cid:17)

min

min/(k + 1)2),

(2)

, with probability at least

An important consequence of the above result is that with probability at least 1 − δ/3,
the set of k nearest neighbors of each center xi of Bi is completely diﬀerent from another
center xj’s, so the labels of xi’s k nearest neighbors are independent of the labels of xj’s k
nearest neighbors.

Now let ηmin,+ = minx∈B1

(cid:83)··· (cid:83) Bm(1 − η(x)). Both
ηmin,+ and ηmin,− are greater than 0 by the construction requirements of B1, · · · , Bm. For
any xi,

(cid:83)··· (cid:83) Bm η(x) and ηmin,− = minx∈B1

Pr[xi’s k nearest neighbors all have label 1] ≥ ηk

min,+

16

Then,

Pr[∃i ∈ {1, · · · , m}s.t. xi’s k nearest neighbor all have label 1] ≥ 1 − (1 − ηk

min,+)m,

(3)

which implies when m ≥
s.t. its k nearest neighbors all have label 1. This xi is our x+.

min,+) , with probability at least 1 − δ/3, there exists an xi

log(1−ηk

log δ/3

Similarly,

Pr[∃i ∈ {1, · · · , m} s.t. xi’s k nearest neighbor all have label 0] ≥ 1 − (1 − ηk

min,−)m, (4)

log δ/3

and when m ≥
nearest neighbors all have label 0. This xi is our x−.
Combining the results above, we show that for

log(1−ηk

min,−) , with probability at least 1 − δ/3, there exists an xi s.t. its k

n > max

(cid:18) k + 1
µmin

,

[log m − log(δ/3)](k + 1)2
µ2

min

(cid:32)

m ≥ max

log δ/3
log(1 − ηk

min,+)

,

log δ/3
log(1 − ηk

min,−)

(cid:19)

(cid:33)

,

,

with probability at least 1 − δ, the statement in Lemma A.1 is satisﬁed.

A.2 Theorem and proof for k-nn robustness lower bound.

Theorem 3.1 shows that k-NN is inherently non-robust in the low k regime if η(x) ∈ (0, 1). On
the contrary, k-NN can be robust at x if η(x) ∈ {0, 1}. We deﬁne the r-robust (p, ∆)-interior
as follows:

ˆX +

r,∆,p = {x ∈ supp(µ)|∀x(cid:48) ∈ Bo(x, r),

∀x(cid:48)(cid:48) ∈ B(x(cid:48), rp(x(cid:48))), η(x(cid:48)(cid:48)) ≥ 1/2 + ∆}

ˆX −

r,∆,p = {x ∈ supp(µ)|∀x(cid:48) ∈ Bo(x, r),

∀x(cid:48)(cid:48) ∈ B(x(cid:48), rp(x(cid:48))), η(x(cid:48)(cid:48)) ≤ 1/2 − ∆}

The deﬁnition is similar to the strict r-robust (p, ∆)-interior in Section 4, except replacing <
and > with ≤ and ≥. Theorem A.2 show that k-NN is robust at radius r in the r-robust
(1/2, p)-interior with high high probability. Corollary A.3 shows the ﬁnite sample rate of the
robustness lowerbound.

Theorem A.2. Let x ∈ X ∩ supp(µ) such that (a) µ is absolutely continuous with respect
to the Lebesgue measure (b) η(x) ∈ {0, 1}. Then, for ﬁxed k, there exists an n0 such that for
n ≥ n0,

for all x in ˆX +

r,1/2,p

(cid:83) ˆX −

r,1/2,p for all p > 0, δ > 0.

Pr[ρ(Ak(Sn, ·), x) ≥ r] ≥ 1 − δ

In addition, with probability at least 1 − δ, the astuteness of the k-NN classiﬁer is at least:

E(1(X ∈ ˆX +

r,1/2,p

(cid:91) ˆX −

r,1/2,p))

17

Proof. The k-NN classiﬁer Ak(Sn, ·) is robust at radius r at x if for every x(cid:48) ∈ Bo(x, r), a)
there are k training points in B(x(cid:48), rp(x(cid:48))), and b) more than (cid:98)k/2(cid:99) of them have the same
label as Ak(Sn, x). Without loss of generality, we look at a point x ∈ ˆX +
r,1/2,p. The second
condition is satisﬁed since η(x) = 1 for all training points in B(x(cid:48), rp(x(cid:48))) by the deﬁnition
of ˆX +

r,1/2,p.
It remains to check the ﬁrst condition. Let B be a ball in Rd and n(B) be the number of
training points in B. Lemma 16 of [4] suggests that with probability at least 1 − δ, for all B
in Rd,

µ(B) ≥

+

d log n + log

+

k

d log n + log

(5)

k
n

(cid:32)

Co
n

(cid:115)

(cid:18)

1
δ

(cid:19)(cid:33)

1
δ

implies n(B) ≥ k, where Co is a constant term. Let B = B(x(cid:48), rp(x(cid:48))). By the deﬁnition of
rp, µ(B) ≥ p > 0. Then as n → ∞, Inequality 5 will eventually be satisﬁed, which implies B
contains at least k training points. The ﬁrst condition is then met.
The astuteness result follows because Ak(Sn, x) = y = 1 in ˆX +
r,1/2,p with probability 1.

r,1/2,p and Ak(Sn, x) = y = 0

in ˆX −

Corollary A.3. For n ≥ max(104, c4

d,k,δ/[(k + 1)2p2]) where

cd,k,δ = 4(d + 1) + (cid:112)16(d + 1)2 + 8(ln(8/δ) + k + 1)

, with probability at least 1 − 2δ, ρ(Ak(Sn, x)) ≥ r for all x in ˆX +
p > 0, δ > 0.

r,1/2,p

(cid:83) ˆX −

r,1/2,p and for all

In addition, with probability at least 1 − 2δ, the astuteness of the k-NN classiﬁer is at

least:

E(1(X ∈ ˆX +

r,1/2,p

(cid:91) ˆX −

r,1/2,p))

Proof. Without loss of generality, we look at a point x ∈ ˆX +
r,1/2,p. Let B = B(x(cid:48), rp(x(cid:48))),
J(B) = E(Y · 1(X ∈ B)) and ˆJ(B) be the empirical estimation of J(B). Notice that ˆJ(B)n
is the number of training points in B, because η(x) = 1 for all x ∈ B by the deﬁnition of
r-robust (1/2, p)-interior. It remains to ﬁnd a threshold n such that for all n(cid:48) > n,

By Lemma A.5, with probability 1 − 2δ,

ˆJ(B) ≥ (k + 1)/n(cid:48)

ˆJ(B) ≥ p − 2βn

p − 2β2
n

√

for all B ∈ Rd.

Therefore it suﬃces to ﬁnd a threshold n that satisﬁes

√

p − 2βn

p − 2β2

n ≥ (k + 1)/n,

where βn = (cid:112)(4/n)((d + 1) ln 2n + ln(8/δ)).
Solving this quadratic inequality yields

√

−

p + (cid:112)3p + (k + 1)/n

,

βn ≤

2

18

(6)

(7)

(8)

(9)

(10)

(11)

(12)

(13)

which can be re-written as

√

(8/

n)[(d + 1) ln(2n) + ln(8/δ) + (k + 1)/8] ≤ (cid:112)(k + 1)p

by substituting the expression for βn. This inequality does not admit an analytic solution.
Nevertheless, we observe that n1/4 ≥ ln(2n) for all n ≥ 104. Therefore it suﬃces to ﬁnd an
n ≥ 104 such that

√

(8/

n)[(d + 1)n1/4 + ln(8/δ) + (k + 1)/8] ≤ (cid:112)(k + 1)p.

Let m = n1/4. Inequality 11 can be re-written as

(cid:112)(k + 1)pm2 − 8(d + 1)m − (8 ln(8/δ) + (k + 1)) ≥ 0.

Solving this quadratic inequality with respct to m gives

m ≥

4(d + 1) + (cid:112)16(d + 1)2 + 8(ln(8/δ) + k + 1)
(cid:112)(k + 1)p

.

Letting

cd,k,δ = 4(d + 1) + (cid:112)16(d + 1)2 + 8(ln(8/δ) + k + 1)

, we ﬁnd a desired threshold

n = max(104, m4) ≥ max(104, c4

d,k,δ/[(k + 1)2p2]).

(14)

The astuteness result follows in a similar way to Theorem A.2.

A.3 Proofs for High k

A.3.1 Robustness of the Bayes Optimal Classiﬁer

Proof. (Of Theorem 3.2) Suppose x ∈ X +
r,0,0. Then, g(x) = 1. Consider any x(cid:48) ∈ Bo(x, r);
by deﬁnition, η(x(cid:48)) > 1/2, which implies that g(x(cid:48)) = 1 as well. Thus, ρ(g, x) ≥ r. The other
case (x ∈ X −

r,0,0) is symmetric.

Consider an x ∈ X +

r,0,0 (the other case is symmetric). We just showed that g has
robustness radius ≥ r at x. Moreover, p(y = 1 = g(x)|x) = η(x); therefore, g predicts the
correct label at x with probability η(x). The theorem follows by integrating over all x in
X +

r,0,0 ∪ X −

r,0,0.

A.3.2 Robustness of k-Nearest Neighbor

We begin by stating and proving a more technical version of Theorem 3.3.

Theorem A.4. For any n and data dimension d, deﬁne:

C0
n

(cid:114)

an =

(d log n + log(1/δ))

d log n + log(1/δ)
n

bn = C0
βn = (cid:112)(4/n)((d + 1) ln 2n + ln(8/δ))

19

where C0 is the constant in Theorem 15 of [4]. Now, pick kn and ∆n so that ∆n → 0 and
the following condition is satisﬁed:

and set

kn
n

≥

2βn + bn + (cid:112)(2βn + bn)2 + 2∆n(2β2

n + an)

∆n

pn =

(cid:16)

+

C0
n

d log n + log(1/δ)

kn
n
(cid:17)
+(cid:112)kn(d log n + log(1/δ)

Then, with probability ≥ 1 − 3δ, kn-NN has robustness radius r at all x ∈ X +
. In addition, with probability ≥ 1 − δ, the astuteness of kn-NN is at least:

r,∆n,pn

∪

X −

r,∆n,pn

E[η(X) · 1(X ∈ X +

)] + E(1 − η(X)) · 1(X ∈ X −

r,∆n,pn

r,∆n,pn

)]

Before we prove Theorem A.4, we need some deﬁnitions and lemmas.
For any Euclidean ball B in Rd, deﬁne J(B) = E[Y · 1(X ∈ B)] and ˆJ(B) as the

corresponding empirical quantity.

Lemma A.5. With probability ≥ 1 − 2δ, for all balls B in Rd, we have:

|J(B) − ˆJ(B)| ≤ 2β2

n + 2βn min((cid:112)J(B),

(cid:113)

ˆJ(B)),

where βn = (cid:112)(4/n)((d + 1) ln 2n + ln(8/δ)).
Proof. (Of Lemma A.5) Consider the two functions: h+
h−
B(x, y) = 1(y = −1, x ∈ B). From Lemma A.6, both h+
VC dimension at most d + 1. Additionally, J(B) = E[h+
of [4], along with an union bound gives the lemma.

B(x, y) = 1(y = 1, x ∈ B) and
B and h−
B are 0/1 functions with
B] − E[h−
B]. Applying Theorem 15

Lemma A.6. For an Euclidean ball B in Rd, deﬁne the function h+
as:

B : Rd×{−1, +1} → {0, 1}

h+
B(x, y) = 1(y = 1, x ∈ B)

and let HB = {h+
most d + 1.

B} be the class of all such functions. Then the VC-dimension of HB is at

Proof. (Of Lemma A.6) Let U be a set of d + 2 points in Rd; as the VC dimension of balls
in Rd is d + 1, U cannot be shattered by balls in Rd. Let UL = {(x, y)|x ∈ U } be a labeling
of U that cannot be achieved by any ball (with pluses inside and minuses outside); the
corresponding d + 1-dimensional points cannot be labeled accordingly by h+
B. Since U is
an arbitrary set of d + 2 points, this implies that any set of d + 2 points in Rd × {−1, +1}
cannot be shattered by HB. The lemma follows.

Lemma A.7. Let δp = C0
n
bility ≥ 1−δ, for all x, (cid:107)x−X(k+1)(x)(cid:107) ≤ rk/n+δp (x), and µ(B(x, (cid:107)x−X(k+1)(x)(cid:107))) ≥ k

. Then, with proba-
n −δp.

(cid:17)
d log n + log(1/δ) + (cid:112)k(d log n + log(1/δ)

(cid:16)

20

Figure. 3: Visualization of the halfmoon dataset. 1) Training sample of size n = 2000,
2) subset selected by Robust_1NN with defense radius r = 0.1, 3) subset selected by
Robust_1NN with defense radius r = 0.2.

Proof. (Of Lemma A.7) Observe that by deﬁnition for any x, rp is the smallest r such that
µ(B(x, rp(x)) ≥ p. The rest of the proof follows from Lemma 16 of [4].

Proof. (Of Theorem A.4)

From Lemma A.7, by uniform convergence of ˆµ, with probability ≥ 1 − δ, for all x(cid:48),
(cid:107)x(cid:48) − X (kn)(x(cid:48))(cid:107) ≤ rpn (x(cid:48)) and µ(B(x, (cid:107)x − X (kn)(x)(cid:107))) ≥ kn
,
r,∆n,pn
this implies that for all ˜x ∈ B(x(cid:48), X (kn)(x(cid:48))), η(˜x) ≥ 1/2 + ∆. Therefore, for such an
x(cid:48), J(B(x(cid:48), X (kn)(x(cid:48)))) ≥ ( 1
2 + ∆n)µ(B(x(cid:48), X (kn)(x(cid:48)))) ≥ ( 1
2 + ∆n)(kn/n − δp). Since for
B(x(cid:48), X (kn)(x(cid:48))), ˆµ(B(x(cid:48), X (kn)(x(cid:48)))) = kn
n . Thus we can apply Lemma A.5
to conclude that

n , min( ˆJ, J) ≤ k

If x(cid:48) ∈ X +

n − δp.

ˆJ(B) > J(B) − 2β2

n − 2βn

(cid:112)kn/n >

kn
2n

,

which implies that ˆY (B) = 1
kn
follows.

(cid:80)kn

i=1 Y (i)(x) = n
kn

ˆJ(B) > 1

2 . The ﬁrst part of the theorem

For the second part, observe that for an x ∈ X +

probability η(x) and for an x ∈ X −
Combining this with the ﬁrst part completes the proof.

r,∆n,pn

, the label Y is equal to +1 with
, the label Y is equal to −1 with probability 1 − η(x).

r,∆n,pn

B Proofs from Section 4

We begin with a statement of Chernoﬀ Bounds that we use in our calculations.

Theorem B.1.

[21] Let Xi be a 0/1 random variable and let X = 1
m

(cid:80)m

i=1 Xi. Then,

Pr(|X − E[X]| ≥ δ) ≤ e−mδ2/2 + e−mδ2/3 ≤ 2e−mδ2/3

Lemma B.2. Suppose we run Algorithm 1 with parameter r. Then, the points marked as
red by the algorithm form an r-separated subset of the training set.

21

Figure. 4: Adversarial examples of MNIST digit 1 images created by diﬀerent attack methods.
Top row: clean digit 1 test images. Middle row from left to right: 1) direct attack, 2)
white-box kernel attack. Bottom row from left to right: 1) black-box kernel attack, 2)
black-box neural net substitute attack.

Proof. Let f (xi) denote the output of Algorithm 2 on xi. If (xi, 1) is a Red point, then
f (xi) = 1 = f (xj) for all xj ∈ B(x, r); therefore, (xj, −1) cannot be marked as Red by the
algorithm as f (xj) (cid:54)= yj. The other case, where (xi, −1) is a Red point is similar.

Lemma B.3. Let x ∈ X such that Algorithm 1 ﬁnds a Red xi within Bo(x, τ ). Then,
Algorithm 1 has robustness radius at least r − 2τ at x.

Proof. For all x(cid:48) ∈ B(x, τ ), we have:

(cid:107)x(cid:48) − xi(cid:107) ≤ (cid:107)x − xi(cid:107) + (cid:107)x − x(cid:48)(cid:107) < 2τ

Since xi is a Red point, from Lemma B.2, any xj in training set output by Algorithm 1 with
yj (cid:54)= yi must have the property that (cid:107)xi − xj(cid:107) > 2r. Therefore,

(cid:107)x(cid:48) − xj(cid:107) ≥ (cid:107)xi − xj(cid:107) − (cid:107)x(cid:48) − xi(cid:107) > 2r − 2τ

Therefore, Algorithm 1 will assign x(cid:48) the label yi. The lemma follows.

Lemma B.4. Let B be a ball such that: (a) for all x ∈ B, η(x) > 1
µ(B) ≥ 2C0
one xi such that xi ∈ |B ∩ Xn| and yi = 1.

2 + ∆ and (b)
n (d log n + log(1/δ)). Then, with probability ≥ 1 − δ, all such balls have at least

Proof. Observe that J(B) ≥ C0
that ˆJ(B) > 0, which gives the theorem.

n (d log n + log(1/δ)). Applying Theorem 16 of [4], this implies

Lemma B.5. Fix ∆ and δ, and let kn = 3 log(2n/δ)

. Additionally, let

∆2

pn =

kn
n

+

C0
n

(d log n + log(1/δ) + (cid:112)kn(d log n + log(1/δ)),

22

Table 1: An evaluation of the black-box substitute classiﬁer. Each black-box substitute is
evaluated by: 1) its accuracy on the its training set, 2) its accuracy on the test set, and 3) the
percentage of predictions agreeing with the target classiﬁer on the test set. A combination of
high test accuracy and consistency with the original classiﬁer indicates the black-box model
emulates the target classiﬁer well.

Abalone

target f % training % test % testf
accuracy accuracy same as f
61.3%
62.5%
61.4%
63.5%
68.9%
64.1%
68.4%
65.0%

StandardNN 100%
100%
RobustNN
100%
ATNN
100%
ATNN-All
StandardNN 69.1%
87.2%
RobustNN
68.8%
ATNN
66.5%
ATNN-All
Halfmoon

72.6%
90.9%
73.7%
73.5%
68.6%
86.9%
68.4%
66.6%

target f % training % test % test

StandardNN 95.9%
97.7%
RobustNN
96.4%
ATNN
97.6%
ATNN-All
StandardNN 94.5%
94.2%
RobustNN
95.3%
ATNN
96.9%
ATNN-All

accuracy accuracy same as f
95.6%
94.9%
95.1%
96.8%
94.0%
90.5%
94.2%
96.2%

95.5%
97.6%
96.0%
97.3%
94.4%
94.1%
95.2%
96.5%

MNIST 1v7

target f % training % test % test

StandardNN 100%
100%
RobustNN
100%
ATNN
100%
ATNN-All
StandardNN 99.9%
99.8%
RobustNN
100%
ATNN
99.7%
ATNN-All

accuracy accuracy same as f
98.9%
95.4%
98.9%
98.7%
98.9%
94.8%
98.8%
98.9%

99.3%
97.6%
99.3%
99.3%
99.1%
98.7%
99.2%
99.3%

Kernel

Neural
Nets

Kernel

Neural
Nets

Kernel

Neural
Nets

where C0 is the constant in Theorem 15 of [4]. Deﬁne:

SRED = {(xi, yi) ∈ Sn|xi ∈ X +
r,∆,pn
(cid:19)
1
1
2
2

η(xi) −

yi =

sgn

(cid:18)

+

1
2

}

∪ X −

r,∆,p,

Then, with probability ≥ 1 − δ, all (xi, yi) ∈ SRED are marked as Red by Algorithm 1 run
with parameters r, ∆ and δ.

23

Proof. Consider a (xi, yi) ∈ SRED such that xi ∈ Xn∩X +
, and consider any (xj, yj) ∈ Sn
such that xj ∈ B(xi, r). From Lemma A.7, for all such xj, (cid:107)xj − X (kn)(xj)(cid:107) ≤ rpn (xj); this
means that all kn-nearest neighbors x(cid:48)(cid:48) of such an xj have η(x(cid:48)(cid:48)) > 1

r,∆,pn

Therefore, E[(cid:80)kn

l=1 Y (l)(xj)] ≥ kn(1/2 + ∆); by Theorem B.1, this means that for a
speciﬁc xj, Pr((cid:80)kn
l=1 Y (l)(xj) < 1/2) ≤ 2e−kn∆2/3, which is ≤ δ/n from our choice of kn.
By an union bound over all such xj, with probability ≥ 1 − δ, we see that Algorithm 2
reports the label g(xi) on all such xi, which is the same as yi by the deﬁnition of interiors;
xi therefore gets marked as Red.

2 + ∆.

Finally, we are ready to prove the main theorem of this section, which is a slightly more

technical form of Theorem 4.2.

Theorem B.6. Fix a ∆n, and pick kn and pn as in Lemma B.5. Suppose we run Algorithm 1
with parameters r, ∆n and δ. Consider the set:

XR =

(cid:40)

(cid:12)
(cid:12)x ∈ X +
(cid:12)

x

r+τ,∆n,pn

r+τ,∆n,pn

∪ X −

,

µ(B(x, τ )) ≥

(d log n + log(1/δ))

,

2C0
n

(cid:41)

where C0 is the constant in Theorem 15 of [4]. Then, with probability ≥ 1 − 2δ over the
training set, Algorithm 1 has robustness radius ≥ r − 2τ on XR. Additionally, its astuteness
at radius r − 2τ is at least E[η(X) · 1(X ∈ X +
)].

)] + E[(1 − η(X)) · 1(X ∈ X −

r+τ,∆n,pn

r+τ,∆n,pn

Proof. Due to the condition on µ(B(x, τ )), from Lemma B.4, with probability ≥ 1 − δ, all
x ∈ XR have the property that there exists a (xi, yi) in Sn such that yi = g(xi) and xi ∈
B(x, τ ). Without loss of generality, suppose that x ∈ X +
, so that η(x) > 1/2 + ∆n.
Then, from the properties of r-robust interiors, this xi ∈ X +
.

r+τ,∆n,pn

From Lemma B.5, with probability ≥ 1 − δ, this (xi, yi) is marked Red by Algorithm 1
run with parameters r, ∆n and δ. The theorem now follows from an union bound and
Lemma B.3.

r,∆n,pn

C Experiment Visualization and Validation

First, we show adversarial examples created by diﬀerent attacks on the MNIST dataset in
order to illustrate characteristics of each attack. Next, we show the subset of training points
selected by Algorithm 1 on the halfmoon dataset. The visualization illustrates the intuition
behind Algorithm 1 and also validates its implementation. Finally, we validate how eﬀective
the black-box subsitute classiﬁers emulate the target classiﬁer.

C.1 Adversarial Examples Created by Diﬀerent Attacks

Figure 4 shows adversarial examples created on MNIST digit 1 images with attack radius
r = 3. First, we observe that the perturbations added by direct attack, white-box kernel
attack and black-box kernel attack are clearly targeted: either a faint horizontal stroke or a
shadow of digit 7 are added to the original image. The perturbation budget is used on "key"
pixels that distinguish digit 1 and digit 7, therefore the attack is eﬀective. On the contrary,

24

black-box attacks with neural nets substitute adds perturbation to a large number of pixels.
While such perturbation often fools a neural net classiﬁer, it is not eﬀective against nearest
neighbors. Consider a pixel that is dark in most digit 1 and digit 7 training images; adding
brightness to this pixel increases the distance between the test image to training images from
both classes, therefore may not change the nearest neighbor to the test image.

Figure 4 also illustrates the break-down attack radius of visual similarity. At r = 3,
the true class of adversarial examples created by eﬀective attacks becomes ambiguous even
to humans. Our defense is successful as the Robust_1NN classiﬁers still have non-trivial
classiﬁcation accuracy at such attack radius. Meanwhile, we should not expect robustness
against even larger attack radius since the adversarial examples at r = 3 are already close to
the boundary of human perception.

C.2 Training Subset Selected by Robust_1NN

Figure 3 shows the training set selected by Robust_1NN on a halfmoon training set of size
2000. On the original training set, we see a noisy region between the two halfmoons where
both red and blue points appear. Robust_1NN cleans training points in this region so as to
create a gap between the red and blue halfmoons, and the gap width increases with defense
radius r.

C.3 Performance of Black-box Attack Substitutes

We validate the black-box substitute training process by checking the substitute’s accuracy
on its training set, the clean test set and the percentage of predictions agreeing with the
target classiﬁer on the clean test set. The results are shown in Table 1. For the halfmoon and
MNIST dataset, the substitute classiﬁers both achieve high accuracy on both the training
and test sets, and are also consistent with the target classiﬁer on the test set. The subsitutute
classiﬁers do not emulate the target classiﬁer on the Abalone dataset as close as on the other
two datasets due to the high noise level in the Abalone dataset. Nonetheless, the substitute
classiﬁer still achieve test time accuracy comparable to the target classiﬁer.

25

9
1
0
2
 
n
u
J
 
9
1
 
 
]
L
M

.
t
a
t
s
[
 
 
6
v
2
2
9
3
0
.
6
0
7
1
:
v
i
X
r
a

Analyzing the Robustness of Nearest Neighbors to
Adversarial Examples

Yizhen Wang
University of California, San Diego
yiw248@eng.ucsd.edu

Somesh Jha
University of Wisconsin-Madison
jha@cs.wisc.edu

Kamalika Chaudhuri
University of California, San Diego
kamalika@cs.eng.ucsd.edu

Abstract

Motivated by safety-critical applications, test-time attacks on classiﬁers via adversar-
ial examples has recently received a great deal of attention. However, there is a general
lack of understanding on why adversarial examples arise; whether they originate due to
inherent properties of data or due to lack of training samples remains ill-understood.
In this work, we introduce a theoretical framework analogous to bias-variance theory
for understanding these eﬀects. We use our framework to analyze the robustness of
a canonical non-parametric classiﬁer – the k-nearest neighbors. Our analysis shows
that its robustness properties depend critically on the value of k – the classiﬁer may
be inherently non-robust for small k, but its robustness approaches that of the Bayes
Optimal classiﬁer for fast-growing k. We propose a novel modiﬁed 1-nearest neighbor
classiﬁer, and guarantee its robustness in the large sample limit. Our experiments 1
suggest that this classiﬁer may have good robustness properties even for reasonable
data set sizes.

1

Introduction

Machine learning is increasingly applied in security-critical domains such as automotive
systems, healthcare, ﬁnance and robotics. To ensure safe deployment in these applications,
there is an increasing need to design machine-learning algorithms that are robust in the
presence of adversarial attacks.

A realistic attack paradigm that has received a lot of recent attention [12, 26, 28, 25] is
test-time attacks via adversarial examples. Here, an adversary has the ability to provide
modiﬁed test inputs to an already-trained classiﬁer, but cannot modify the training process
in any way. Their goal is to perturb legitimate test inputs by a “small amount” in order to
force the classiﬁer to report an incorrect label. An example is an adversary that replaces a
stop sign by a slightly defaced version in order to force an autonomous vehicle to recognize
it as an yield sign. This attack is undetectable to the human eye if the perturbation is small
enough.

1Code available at: https://github.com/EricYizhenWang/robust_nn_icml

1

Prior work has considered adversarial examples in the context of linear classiﬁers [19],
kernel SVMs [3] and neural networks [28, 12, 25, 26, 22]. However, most of this work has
either been empirical, or has focussed on developing theoretically motivated attacks and
defenses. Consequently, there is a general lack of understanding on why adversarial examples
arise; whether they originate due to inherent properties of data or due to lack of training
samples remains ill-understood.

This work develops a theoretical framework for robust learning in order to understand the
eﬀects of distributional properties and ﬁnite samples on robustness. Building on traditional
bias-variance theory [10], we posit that a classiﬁcation algorithm may be robust to adversarial
examples due to three reasons. First, it may be distributionally robust, in the sense that the
output classiﬁer is robust as the number of training samples grow to inﬁnity. Second, even
the output of a distributionally robust classiﬁcation algorithm may be vulnerable due to too
few training samples – this is characterized by ﬁnite sample robustness. Finally, diﬀerent
training algorithms might result in classiﬁers with diﬀerent degrees of robustness, which we
call algorithmic robustness. These quantities are analogous to bias, variance and algorithmic
eﬀects respectively.

Next, we analyze a simple non-parametric classiﬁcation algorithm: k-nearest neighbors in
our framework. Our analysis demonstrates that large sample robustness properties of this
algorithm depend very much on k.

Speciﬁcally, we identify two distinct regimes for k with vastly diﬀerent robustness
properties. When k is constant, we show that k-nearest neighbors has zero robustness in
the large sample limit in regions where p(y = 1|x) lies in (0, 1). This is in contrast with
dn log n), where d is the
accuracy, which may be quite high in these regions. For k = Ω(
data dimension and n is the sample size, we show that the robustness region of k-nearest
neighbors approaches that of the Bayes Optimal classiﬁer in the large sample limit. This is
again in contrast with accuracy, where convergence to the Bayes Optimal accuracy is known
for a much slower growing k [7, 5].
√

dn log n) is too high to use in practice with nearest neighbors, we next
propose a novel robust version of the 1-nearest neighbor classiﬁer that operates on a modiﬁed
training set. We provably show that in the large sample limit, this algorithm has superior
robustness to standard 1-nearest neighbors for data distributions with certain properties.

Since k = Ω(

√

Finally, we validate our theoretical results by empirically evaluating our algorithm on
three datasets against several popular attacks. Our experiments demonstrate that our
algorithm performs better than or about as well as both standard 1-nearest neighbors and
nearest neighbors with adversarial training – a popular and eﬀective defense mechanism.
This suggests that although our performance guarantees hold in the large sample limit, our
algorithm may have good robustness properties even for realistic training data sizes.

1.1 Related Work

Adversarial examples have recently received a great deal of attention [12, 3, 26, 28, 25].
Most of the work, however, has been empirical, and has focussed on developing increasingly
sophisticated attacks and defenses.

1.1.1 Related Work on Adversarial Examples

Prior theoretical work on adversarial examples falls into two categories – analysis and theory-
inspired defenses. Work on analysis includes [9], which analyzes the robustness of linear and

2

quadratic classiﬁers under random and semi-random perturbations. [13] provides robustness
guarantees on linear and kernel classiﬁers trained on a given data set. [11] shows that linear
classiﬁers for high dimensional datasets may have inherent robustness-accuracy trade-oﬀs.
Work on theory-inspired defenses include [20, 15, 1], who provide defense mechanisms for
adversarial examples in neural networks that are relaxations of certain principled optimization
objectives. [14] shows how to use program veriﬁcation to certify robustness of neural networks
around given inputs for small neural networks.

Our work diﬀers from these in two important ways. First, unlike most prior work which
looks at a given training dataset, we consider eﬀects of the data distribution and number of
samples, and analyze robustness properties in the large sample limit. Second, unlike prior
work which largely focuses on parametric methods such as neural networks, our focus is on a
canonical non-parametric method – the nearest neighbors classiﬁer.

1.1.2 Related Work on Nearest Neighbors

There has been a body of work on the convergence and consistency of nearest-neighbor
classiﬁers and their many variants [6, 27, 17, 8, 5, 16]; all these works however consider
accuracy and not robustness.

In the asymptotic regime, [6] shows that the accuracy of 1-nearest neighbors converges in
the large sample limit to 1 − 2R∗(1 − R∗) where R∗ is the error rate of the Bayes Optimal
classiﬁer. This implies that even 1-nearest neighbor may achieve relatively high accuracy
even when p(y = 1|x) is not 0 or 1. In contrast, we show that 1-nearest neighbor is inherently
non-robust when p(y = 1|x) ∈ (0, 1) under some continuity conditions.

For larger k, the accuracy of k-nearest neighbors is known to converge to that of the
Bayes Optimal classiﬁer if kn → ∞ and kn/n → 0 as the sample size n → ∞. We show that
the robustness also converges to that of the Bayes Optimal classiﬁer when kn grows at a
much higher rate – fast enough to ensure uniform convergence. Whether this high rate is
necessary remains an intriguing open question.

Finite sample rates on the accuracy of nearest neighbors are known to depend heavily on
properties of the data distribution, and there is no distribution free rate as in parametric
methods [8].
[5] provides a clean characterization of the ﬁnite sample rates of nearest
neighbors as a function of natural interiors of the classes. Here we build on their results
by deﬁning a stricter, more robust version of interiors and providing bounds as functions of
these new robust quantities.

1.1.3 Other Related Work

[2] provides a method for generating adversarial examples for nearest neighbors, and shows
that the eﬀectiveness of attacks grow with intrinsic dimensionality. Finally, [24, 25] provides
black-box attacks on substitute classiﬁers; their experiments show that attacks from other
types of substitute classiﬁers are not successful on nearest neighbors; our experiments
corroborate these results.

3

2 The Setting and Deﬁnitions

2.1 The Basic Setup

We consider test-time attacks in a white box setting, where the adversary has full knowledge
of the training process – namely, the type of classiﬁer used, the training data and any
parameters – but cannot modify training in any way.

Given an input x, the adversary’s goal is to perturb it so as to force the trained classiﬁer
f to report a diﬀerent label than f (x). The amount of perturbation is measured by an
application-speciﬁc metric d, and is constrained to be within a radius r. Our analysis can
be extended to any metric, but for this paper we assume that d is the Euclidean distance
for mathematical simplicity; we also focus on binary classiﬁcation, and leave extensions to
multiclass for future work.

Finally, we assume that unlabeled instances are drawn from an instance space X , and
their labels are drawn from the label space {0, 1}. There is an underlying data distribution
D that generates labeled examples; the marginal over X of D is µ and the conditional
distribution of labels given x is denoted by η.

2.2 Robustness and astuteness

We begin by deﬁning robustness, which for a classiﬁer f at input x is measured by the
robustness radius.

Deﬁnition 2.1 (Robustness Radius). The robustness radius of a classiﬁer f at an instance
x ∈ X , denoted by ρ(f, x), is the shortest distance between x and an input x(cid:48) to which f
assigns a label diﬀerent from f (x):

ρ(f, x) = inf
r

{∃x(cid:48) ∈ X ∩ B(x, r) s.t f (x) (cid:54)= f (x(cid:48))}

Observe that the robustness radius measures a classiﬁer’s local robustness. A classiﬁer
f with robustness radius r at x guarantees that no adversarial example of x with norm
of perturbation less than r can be created using any attack method. A plausible way
to extend this into a global notion is to require a lower bound on the robustness radius
everywhere; however, only the constant classiﬁer will satisfy this condition. Instead, we
consider robustness around meaningful instances, that we model as examples drawn from
the underlying data distribution.

Deﬁnition 2.2 (Robustness with respect to a Distribution). The robustness of a classiﬁer
f at radius r with respect to a distribution µ over the instance space X , denoted by R(f, r, µ),
is the fraction of instances drawn from µ for which the robustness radius is greater than or
equal to r.

R(f, r, µ) = Pr
x∼µ

(ρ(f, x) ≥ r)

Finally, observe that we are interested in classiﬁers that are both robust and accurate.
This leads to the notion of astuteness, which measures the fraction of instances on which a
classiﬁer is both accurate and robust.

Deﬁnition 2.3 (astuteness). The astuteness of a classiﬁer f with respect to a data distribu-
tion D and a radius r is the fraction of examples on which it is accurate and has robustness
radius at least r; formally,

AstD(f, r) = Pr

(ρ(f, x) ≥ r, f (x) = y),

(x,y)∼D

4

Observe that astuteness is analogous to classiﬁcation accuracy, and we argue that it is
a more appropriate metric if we are concerned with both robustness and accuracy. Unlike
accuracy, astuteness cannot be directly empirically measured unless we have a way to certify
a lower bound on the robustness radius. In this work, we will prove bounds on the astuteness
of classiﬁers, and in our experiments, we will approximate it by measuring resistance to
standard attacks.

2.3 Sources of Robustness

There are three plausible reasons why classiﬁers lack robustness – distributional, ﬁnite sample
and algorithmic. These sources are analogous to bias, variance, and algorithmic eﬀects
respectively in standard bias-variance theory.

Distributional robustness measures the eﬀect of the data distribution on robustness
when an inﬁnitely large number of samples are used to train the classiﬁer. Formally, if
Sn is a training sample of size n drawn from D and A(Sn, ·) is a classiﬁer obtained by
applying the training procedure A on Sn, then the distributional robustness at radius r is
limn→∞ ESn∼D[R(A(Sn, ·), r, µ)].

In contrast, for ﬁnite sample robustness, we characterize the behaviour of R(A(Sn, ·), r, µ)
for ﬁnite n – usually by putting high probability bounds over the training set. Thus, ﬁnite
sample robustness depends on the training set size n, and quantiﬁes how it changes with
sample size. Finally, robustness also depends on the training algorithm itself; for example,
some variants of nearest neighbors may have higher robustness than nearest neighbors itself.

2.4 Nearest Neighbor and Bayes Optimal Classiﬁers

Given a training set Sn = {(X1, Y1), . . . , (Xn, Yn)} and a test example x, we use the notation
X (i)(x) to denote the i-th nearest neighbor of x in Sn, and Y (i)(x) to denote the label of
X (i)(x).

Given a test example x, the k-nearest neighbor classiﬁer Ak(Sn, x) outputs:

= 1,

= 0,

if Y (1)(x) + . . . + Y (k)(x) ≥ k/2
otherwise.

The Bayes optimal classiﬁer g over a data distribution D has the following classiﬁcation

rule:

g(x) =

(cid:26) 1
0

if η(x) = Pr(y = 1|x) ≥ 1/2;
otherwise.

(1)

3 Robustness of Nearest Neighbors

How robust is the k-nearest neighbor classiﬁer? We show that it depends on the value of k.
Speciﬁcally, we identify two distinct regimes – constant k and k = Ω(
dn log n) where d is
the data dimension – and show that nearest neighbors has diﬀerent robustness properties in
the two.

√

3.1 Low k Regime

In this region, k is a constant that does not depend on the training set size n. Provided
certain regularity conditions hold, we show that k-nearest neighbors is inherently non-robust

5

in this regime unless η(x) ∈ {0, 1} – in the sense that the distributional robustness becomes
0 in the large sample limit.

Theorem 3.1. Let x ∈ X ∩ supp(µ) such that (a) µ is absolutely continuous with respect
to the Lebesgue measure (b) η(x) ∈ (0, 1) (c) η is continuous with respect to the Euclidean
metric in a neighborhood of x. Then, for ﬁxed k, ρ(Ak(Sn, ·), x) converges in probability to
0.

Remarks. Observe that Theorem 3.1 implies that the distributional robustness (and hence
astuteness) in a region where η(x) ∈ (0, 1) is 0. This is in contrast with accuracy; for 1-NN,
the accuracy converges to 1 − 2R∗(1 − R∗) as n → ∞, where R∗ is the error rate of the
Bayes Optimal classiﬁer, and thus may be quite high.

The proof of Theorem 3.1 in the Appendix shows that the absolute continuity of µ with
respect to the Lebesgue measure is not strictly necessary; absolute continuity with respect
to an embedded manifold will give the same result, but will result in a more complex proof.
In the Appendix A (Theorem A.2), we show that k-nearest neighbor is astute in the

interior of the region where η ∈ {0, 1}, and provide ﬁnite sample rates for this case.

3.2 High k Regime

√

Prior work has shown that in the large sample limit, the accuracy of the nearest neighbor
classiﬁers converge to the Bayes Optimal, provided k is set properly. We next show that
if k is Ω(
dn log n), the regions of robustness and the astuteness of the k nearest neighbor
classiﬁers also approach the corresponding quantities for the Bayes Optimal classiﬁer as
n → ∞. Thus, if the Bayes Optimal classiﬁer is robust, then so is k-nearest neighbors in the
large sample limit.

The main intuition is that k = Ω(

dn log n) is large enough for uniform convergence –
where, with high probability, all Euclidean balls with k examples have the property that the
empirical averages of their labels are close to their expectations. This guarantees that for
any x, the k-nearest neighbor reports the same label as the Bayes Optimal classiﬁer for all
x(cid:48) close to x. Thus, if the Bayes Optimal classiﬁer is robust, so is nearest neighbors.

√

3.2.1 Deﬁnitions

We begin with some deﬁnitions that we can use to characterize the robustness of the Bayes
Optimal classiﬁer. Following [5], we use the notation Bo(x, r) to denote an open ball and
B(x, r) to denote a closed ball of radius r around x. We deﬁne the probability radius of a
ball around x as:

We next deﬁne the r-robust (p, ∆)-strict interiors as follows:

rp(x) = inf{r | µ(B(x, r)) ≥ p}

X +

r,∆,p = {x ∈ supp(µ) | ∀x(cid:48) ∈ Bo(x, r),

∀x(cid:48)(cid:48) ∈ B(x(cid:48), rp(x(cid:48))), η(x(cid:48)(cid:48)) > 1/2 + ∆}

X −

r,∆,p = {x ∈ supp(µ) | ∀x(cid:48) ∈ Bo(x, r),

∀x(cid:48)(cid:48) ∈ B(x(cid:48), rp(x(cid:48))), η(x(cid:48)(cid:48)) < 1/2 − ∆}

What is the signiﬁcance of these interiors? Let x(cid:48) be an instance such that all x(cid:48)(cid:48) ∈
n , then the k points x(cid:48)(cid:48) closest to x(cid:48) have

B(x(cid:48), rp(x(cid:48))) have η(x(cid:48)(cid:48)) > 1/2 + ∆. If p ≈ k

6

η(x(cid:48)(cid:48)) > 1/2 + ∆. Provided the average of the labels of these points is close to expectation,
which happens when k is large relative to 1/∆, k-nearest neighbor outputs label 1 on x(cid:48).
When x is in the r-robust (p, ∆)-strict interior region X +
r,∆,p, this is true for all x(cid:48) within
distance r of x, which means that k-nearest neighbors will be robust at x. Thus, the r-robust
(p, ∆)-strict interior is the region where we natually expect k-nearest neighbor to have
robustness radius r, when k is large relative to 1

∆ and p ≈ k
n .

Readers familiar with [5] will observe that the set of all x(cid:48) for which ∀x(cid:48)(cid:48) ∈ B(x(cid:48), rp(x(cid:48))), η(x(cid:48)(cid:48)) >

1/2 + ∆ forms a stricter version of the (p, ∆)-interiors of the 1 region that was deﬁned in
this work; these x(cid:48) also represent the region where k-nearest neighbors are accurate when
k ≈ max(np, 1/∆2). The r-robust (p, ∆)-strict interior is thus a somewhat stricter and more
robust version of this deﬁnition.

3.2.2 Main Results

We begin by characterizing where the Bayes Optimal classiﬁer is robust.

Theorem 3.2. The Bayes Optimal classiﬁer has robustness radius r at x ∈ X +
Moreover, its astuteness is E[η(x)1(x ∈ X +

r,0,0)] + E[(1 − η(x))1(x ∈ X −

r,0,0)].

r,0,0 ∪ X −

r,0,0.

The proof is in the Appendix, along with analogous results for astuteness. The following
theorem, along with a similar result for astuteness, proved in the Appendix, characterizes
robustness in the large k regime.

√

Theorem 3.3. For any n, pick a δ and a ∆n → 0. There exist constant C1 and C2 such that
if kn ≥ C1
n (1 + C2
≥ 1 − 3δ, kn-NN has robustness radius r in x ∈ X +

(cid:113) d log n+log(1/δ)
kn
∪ X −

), then, with probability

, and pn ≥ kn

dn log n+n log(1/δn)

∆n

.

r,∆n,pn

r,∆n,pn

Remarks. Some remarks are in order. First, observe that as n → ∞, ∆n and pn tend
to 0; thus, provided certain continuity conditions hold, X +
approaches
X +

r,∆n,pn
r,0,0, the robustness region of the Bayes Optimal classiﬁer.

r,0,0 ∪ X −

∪ X −

r,∆n,pn

Second, observe that as r-robust strict interiors extend the deﬁnition of interiors in [5],
Theorem 3.3 is a robustness analogue of Theorem 5 in this work. Unlike the latter, Theo-
rem 3.3 has a more stringent requirement on k. Whether this is necessary is left as an open
question for future work.

4 A Robust 1-NN Algorithm

√

Section 3 shows that nearest neighbors is robust for k as large as Ω(
dn log n). However,
this k is too high to use in practice – high values of k require even higher sample sizes [5],
and lead to higher running times. Thus a natural question is whether we can ﬁnd a more
robust version of the algorithm for smaller k. In this section, we provide a more robust
version of 1-nearest neighbors, and analytically demonstrate its robustness.

Our algorithm is motivated by the observation that 1-nearest neighbor is robust when
oppositely labeled points are far apart, and when test points lie close to training data. Most
training datasets however contain nearby points that are oppositely labeled; thus, we propose
to remove a subset of training points to enforce this property.

Which points should we remove? A plausible approach is to keep the largest subset where
oppositely labeled points are far apart; however, this subset has poor stability properties

7

even for large n. Therefore, we propose to keep all points x such that: (a) we are highly
conﬁdent about the label of x and its nearby points and (b) all points close to x have the
same label. Given that all such x are kept, we remove as few points as possible, and execute
nearest neighbors on the remaining dataset.

The following deﬁnition characterizes data where oppositely labeled points are far apart.

Deﬁnition 4.1 (r-separated set). A set A = {(x1, y1), . . . , (xm, ym)} of labeled examples is
said to be r-separated if for all pairs (xi, yi), (xj, yj) ∈ A, (cid:107)xi − xj(cid:107) ≤ r implies yi = yj.

The full algorithm is described in Algorithm 1 and Algorithm 2. Given conﬁdence
parameters ∆ and δ, Algorithm 2 returns a 0/1 label when this label agrees with the average
of kn points closest to x; otherwise, it returns ⊥. kn is chosen such that with probability
≥ 1 − δ, the empirical majority of kn labels agrees with the majority in expectation, provided
the latter is at least ∆ away from 1
2 .

Algorithm 2 is used to determine whether an xi should be kept. Let f (xi) be the output
of Algorithm 2 on xi. If yi = f (xi) and if for all xj ∈ B(xi, r), f (xi) = f (xj) = yi, then
we mark xi as red. Finally, we compute the largest r-separated subset of the training data
that includes all the red points; this reduces to a constrained matching problem as in [16].
The resulting set, returned by Algorithm 1, is our new training set. We observe that this
set is r-separated from Lemma B.2 in the Appendix, and thus oppositely labeled points are
far apart. Moreover, we keep all (xi, yi) when we are conﬁdent about the label of xi and
its nearby points. Observe that our ﬁnal procedure is a 1-NN algorithm, even though kn
neighbors are used to determine if a point should be retained in the training set.

Algorithm 1 Robust_1NN(Sn, r, ∆, δ, x)

f (xi) = Conﬁdent-Label(Sn, ∆, δ, xi)

for (xi, yi) ∈ Sn do

end for
SRED = ∅
for (xi, yi) ∈ Sn do

if f (xi) = yi and f (xi) = f (xj) for all xj such that (cid:107)xi − xj(cid:107) ≤ r and (xj, yj) ∈ Sn
then

SRED = SRED

(cid:83){(xi, yi)}

end if
end for
Let S(cid:48) be the largest 2r-separated subset of Sn that contains all points in SRED.
return new training set S(cid:48)

Algorithm 2 Conﬁdent-Label(Sn, ∆, δ, x)

kn = 3 log(2n/δ)/∆2
¯y = (1/kn) (cid:80)kn
2 − ∆, 1
if ¯y ∈ [ 1

i=1 Y (i)(x)
2 + ∆] then

return ⊥

else

end if

return 1

2 sgn(¯y − 1

2 ) + 1

2

8

4.1 Performance Guarantees

The following theorem establishes performance guarantees for Algorithm 1.

Theorem 4.2. Pick a ∆n and δ, and set kn = 3 log(2n/δ)/∆2
n. Pick a margin parameter
τ . Then, there exist constants C and C0 such that the following hold. If we set pn =
kn
n (1 + C

(cid:113) d log n+log(1/δ)
kn

), and deﬁne the set:

XR =

(cid:40)

(cid:12)
(cid:12)x ∈ X +
(cid:12)
x

r+τ,∆n,pn

r+τ,∆n,pn

∪ X −

,

µ(B(x, τ )) ≥

(d log n + log(1/δ))

2C0
n

(cid:41)

Then, with probability ≥ 1 − 2δ over the training set, Algorithm 1 run with parameters r, ∆n
and δ has robustness radius at least r − 2τ on XR.

Remarks. The proof is in the Appendix, along with an analogous result for astuteness.
Observe that XR is roughly the high density subset of the r + τ -robust strict interior
X +
2 + ∆n or less than
r+τ,∆n,pn
1
2 − ∆n in this region, as opposed to 0 or 1, this is an improvement over standard nearest
neighbors when the data distribution has a large high density region that intersects with the
interiors.

. Since η(x) is constrained to be greater than 1

r+τ,∆n,pn

∪ X −

A second observation is that as τ is an arbitrary constant, we can set to it be quite small
and still satisfy the condition on µ(B(x, τ )) for a large fraction of x’s when n is very large.
This means that in the large sample limit, r − 2τ may be close to r and XR may be close to
the high density subset of X +

for a lot of smooth distributions.

∪ X −

r,∆n,pn

r,∆n,pn

5 Experiments

The results in Section 4 assume large sample limits. Thus, a natural question is how well
Algorithm 1 performs with more reasonable amounts of training data. We now empirically
investigate this question.

Since there are no general methods that certify robustness at an input, we assess
robustness by measuring how our algorithm performs against a suite of standard attack
methods. Speciﬁcally, we consider the following questions:

1. 1. How does our algorithm perform against popular white box and black box attacks

compared with standard baselines?

2. 2. How is performance aﬀected when we change the training set size relative to the

data dimension?

These questions are considered in the context of three datasets with varying training set
sizes relative to the dimension, as well as two standard white box attacks and black box
attacks with two kinds of substitute classiﬁers.

9

Figure. 1: White Box Attacks: Plot of classiﬁcation accuracy on adversarial examples v.s.
attack radius. Top row: Direct Attack. Bottom row: Kernel Substitute Attack. Left to right:
1) Halfmoon, 2) MNIST 1v 7 and 3) Abalone.

Figure. 2: Black Box Attacks: Plot of classiﬁcation accuracy on adversarial examples v.s.
attack radius. Top to Bottom: 1) kernel substitute, 2) neural net substitute. Left to right:
1) Halfmoon, 2) MNIST 1 v.s. 7 and 3) Abalone.

5.1 Methodology

5.1.1 Data

We use three datasets – Halfmoon, MNIST 1v7 and Abalone – with diﬀering data sizes
relative to dimension. Halfmoon is a popular 2-dimensional synthetic data set for non-linear
classiﬁcation. We use a training set of size 2000 and a test set of size 1000 generated with
standard deviation σ = 0.2. The MNIST 1v7 data set is a subset of the 784-dimensional
MNIST data. For training, we use 1000 images each of Digit 1 and 7, and for test, 500 images
of each digit. Finally, for the Abalone dataset [18], our classiﬁcation task is to distinguish
whether an abalone is older than 12.5 years based on 7 physical measurements. For training,
we use 500 and for test, 100 samples. In addition, a validation set with the same size as the

10

test set is generated for each experiment for parameter tuning.

5.1.2 Baselines

We compare Algorithm 1, denoted by RobustNN, against three baselines. The ﬁrst is
the standard 1-nearest neighbor algorithm, denoted by StandardNN. We use two forms
of adversarially-trained nearest neighbors - ATNN and ATNN-all. Let S be the training
set used by standard nearest neighbors. In ATNN, we augment S by creating, for each
(x, y) ∈ S, an adversarial example xadv using the attack method in the experiment, and
adding (xadv, y). The ATNN classiﬁer is 1-nearest neighbor on this augmented data. In
ATNN-all, for each (x, y) ∈ S, we create adversarial examples using all the attack methods
in the experiment, and add them all to S. ATNN-all is the nearest neighbor classiﬁer on this
augmented data. For example, for white box Direct Attacks in Section 5.2, ATNN includes
adversarial examples generated by the Direct Attack, and ATNN-all includes adversarial
examples generated by both Direct and Kernel Substitute Attacks.

Observe that all algorithms except StandardNN have parameters to tune. RobustNN
has three input parameters – ∆, δ and a defense radius r which is an approximation to the
robustness radius. For simplicity, we set ∆ = 0.45, δ = 0.1 and tune r on the validation set;
this can be viewed as tuning the parameter τ in Theorem 4.2. For ATNN and ATNN-all, the
methods that generate the augmenting adversarial examples need a perturbation magnitude
r; we call this the defense radius. To be fair to all algorithms, we tune the defense radius
for each. We consider the adversary with the highest attack perturbation magnitude in the
experiment, and select the defense radius that yields the highest validation accuracy against
this adversary.

5.2 White-box Attacks and Results

To evaluate the robustness of Algorithm 1, we use two standard classes of attacks – white
box and black box. For white-box attacks, the adversary knows all details about the classiﬁer
under attack, including its training data, the training algorithm and any hyperparameters.

5.2.1 Attack Methods

We consider two white-box attacks – direct attack [2] and Kernel Substitute Attack [24].
Direct Attack. This attack takes as input a test example x, an attack radius r, and a
training dataset S (which may be an augmented or reduced dataset). It ﬁnds an x(cid:48) ∈ S that is
closest to x but has a diﬀerent label, and returns the adversarial example xadv = x+r x−x(cid:48)
.
||x−x(cid:48)||2
Kernel Substitute Attack. This method attacks a substitute kernel classiﬁer trained on
the same training set. For a test input (cid:126)x, a set of training points Z with one-hot labels Y , a
kernel classiﬁer f predicts the class probability as:

The adversary trains a kernel classiﬁer on the training set of the corresponding nearest
neighbors, and then generates adversarial examples against this kernel classiﬁer. The
advantage is that the prediction of the kernel classiﬁer is diﬀerentiable, which allows the
use of standard gradient-based attack methods. For our experiments, we use the popular

f : (cid:126)x →

2/c(cid:105)

(cid:104)

e−||(cid:126)x−(cid:126)z||2
(cid:80)

(cid:126)z∈X e−||(cid:126)x−(cid:126)z||2

(cid:126)z∈X
2/c

· Y

11

fast-gradient-sign method (FSGM). The parameter c is tuned to yield the most eﬀective
attack, and is set to 0.1 for Halfmoon and MNIST, and 0.01 for Abalone.

5.2.2 Results

Figure 1 shows the results. We see that RobustNN outperforms all baselines for Halfmoon
and Abalone for all attack radii. For MNIST, for low attack radii, RobustNN’s classiﬁcation
accuracy is slightly lower than the others, while it outperforms the others for large attack
radii. Additionally, as is to be expected, the Direct Attack results in lower general accuracy
than the Kernel Substitute Attack.

These results suggest that our algorithm mostly outperforms the baselines StandardNN,
ATNN and ATNN-all. As predicted by theory, the performance gain is higher when the
training set size is large relative to the dimension – which is the setting where nearest
neighbors work well in general. It has superior performance for Halfmoon and Abalone,
where the training set size is large to medium relative to dimension. In contrast, in the
sparse dataset MNIST, our algorithm has slightly lower classiﬁcation accuracy for small
attack radii, and higher otherwise.

5.3 Black-box Attacks and Results

[25] has observed that some defense methods that work by masking gradients remain highly
amenable to black box attacks. In this attack, the adversary is unaware of the target classiﬁer’s
nature, parameters or training data, but has access to a seed dataset drawn from the same
distribution which they use to train and attack a substitute classiﬁer. To establish robustness
properties of Algorithm 1, we therefore validate it against black box attacks based on two
types of substitute classiﬁers.

5.3.1 Attack Methods

We use two types of substitute classiﬁers – kernel classiﬁers and neural networks. The
adversary trains the substitute classiﬁer using the method of [25] and uses the adversarial
examples against the substitute to attack the target classiﬁer.

Kernel Classiﬁer. The kernel classiﬁer substitute is the same as the one in Section 5.2,
but trained using the seed data and the method of [25].
Neural Networks. The neural network for MNIST is the ConvNet in [23]’s tutorial. For
Halfmoon and Abalone, the network is a multi-layer perceptron with 2 hidden layers.
Procedure. To train the substitute classiﬁer, the adversary uses the method of [24] to
augment the seed data for two rounds; labels are obtained by querying the target classiﬁer.
Adversarial examples against the substitutes are created by FGSM, following [24]. As a
sanity check, we verify the performance of the substitute classiﬁers on the original training
and test sets. Details are in Table 1 in the Appendix. Sanity checks on the performance of
the substitute classiﬁers are presented in Table 1 in the Appendix.

5.3.2 Results

Figure 2 shows the results. For all algorithms, black box attacks are less eﬀective than white
box, which corroborates the results of [24], who observed that black-box attacks are less
successful against nearest neighbors. We also ﬁnd that the kernel substitute attack is more
eﬀective than the neural network substitute, which is expected as kernel classiﬁers have

12

similar structure to nearest neighbors. Finally, for Halfmoon and Abalone, our algorithm
outperforms the baselines for both attacks; however, for MNIST neural network substitute,
our algorithm does not perform as well for small attack radii. This again conﬁrms the
theoretical predictions that our algorithm’s performance is better when the training set is
large relative to the data dimension – the setting in which nearest neighbors work well in
general.

5.4 Discussion

The results show that our algorithm performs either better than or about the same as
standard baselines against popular white box and black box attacks. As expected from
our theoretical results, it performs better for denser datasets which have high or medium
amounts of training data relative to the dimension, and either slightly worse or better for
sparser datasets, depending on the attack radius. Since non-parametric methods such as
nearest neighbors are mostly used for dense data, this suggests that our algorithm has good
robustness properties even with reasonable amounts of training data.

6 Conclusion

We introduce a novel theoretical framework for learning robust to adversarial examples, and
introduce notions of distributional and ﬁnite-sample robustness. We use these notions to
analyze a non-parametric classiﬁer, k-nearest neighbors, and introduce a novel modiﬁed
1-nearest neighbor algorithm that has good robustness properties in the large sample limit.
Experiments show that these properties are still retained for reasonable data sizes.

Many open questions remain. The ﬁrst is to close the gap in analysis of k-nearest
neighbors for k in between our two regimes. The second is to develop nearest neighbor
algorithms with better robustness guarantees. Finally, we believe that our work is a ﬁrst
step towards a comprehensive analysis of how the size of training data aﬀects robustness;
we believe that an important line of future work is to carry out similar analyses for other
supervised learning methods.

Acknowledgement

We thank NSF under IIS 1253942 for support. This work was also partially supported by
ARO under contract number W911NF-1-0405. We thank all anonymous reviewers for their
constructive comments.

We also thank Rabanus Derr from University of Tübingen for pointing about a minor

mistake in the proof of Lemma B.3. The proof is ﬁxed in the latest version.

References

[1] John Duchi Aman Sinha, Hongseok Namkoong. Certiﬁable distributional robustness with
principled adversarial training. International Conference on Learning Representations,
2018.

13

[2] Laurent Amsaleg, James Bailey, Sarah Erfani, Teddy Furon, Michael E Houle, Miloš
Radovanović, and Nguyen Xuan Vinh. The vulnerability of learning to adversarial
perturbation increases with intrinsic dimensionality. 2016.

[3] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndić, Pavel
Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at
test time. In Joint European Conference on Machine Learning and Knowledge Discovery
in Databases, pages 387–402. Springer, 2013.

[4] Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for the cluster tree.
In J. D. Laﬀerty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta,
editors, Advances in Neural Information Processing Systems 23, pages 343–351. Curran
Associates, Inc., 2010.

[5] Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for nearest neighbor
classiﬁcation. In Advances in Neural Information Processing Systems, pages 3437–3445,
2014.

[6] T. Cover and P.E. Hart. Nearest neighbor pattern classiﬁcation. IEEE Transactions on

Information Theory, 13:21–27, 1967.

[7] Luc Devroye, Laszlo Gyorﬁ, Adam Krzyzak, and Gabor Lugosi. On the strong universal
consistency of nearest neighbor regression function estimates. The Annals of Statistics,
pages 1371–1385, 1994.

[8] Luc P Devroye and Terry J Wagner. The strong uniform consistency of nearest neighbor

density estimates. The Annals of Statistics, pages 536–540, 1977.

[9] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of
classiﬁers: from adversarial to random noise. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems
29, pages 1632–1640. Curran Associates, Inc., 2016.

[10] Jerome Friedman, Trevor Hastie, Robert Tibshirani, et al. Additive logistic regression:
a statistical view of boosting (with discussion and a rejoinder by the authors). The
annals of statistics, 28(2):337–407, 2000.

[11] Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu, Martin
Wattenberg, and Ian Goodfellow. Adversarial spheres. arXiv preprint arXiv:1801.02774,
2018.

[12] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing

adversarial examples. arXiv preprint arXiv:1412.6572, 2014.

[13] Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness
of a classiﬁer against adversarial manipulation. In Advances in Neural Information
Processing Systems, pages 2263–2273, 2017.

[14] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer.
Towards proving the adversarial robustness of deep neural networks. arXiv preprint
arXiv:1709.02802, 2017.

14

[15] J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the

convex outer adversarial polytope. arXiv preprint arXiv:1711.00851, 2017.

[16] Aryeh Kontorovich and Roi Weiss. A bayes consistent 1-nn classiﬁer. In Artiﬁcial

Intelligence and Statistics Conference, 2015.

[17] S. Kulkarni and S. Posner. Rates of convergence of nearest neighbor estimation under
arbitrary sampling. IEEE Transactions on Information Theory, 41(4):1028–1039, 1995.

[18] M. Lichman. UCI machine learning repository, 2013.

[19] Daniel Lowd and Christopher Meek. Adversarial learning. In Proceedings of the eleventh
ACM SIGKDD international conference on Knowledge discovery in data mining, pages
641–647. ACM, 2005.

[20] Aleksander Mądry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian
Vladu. Towards deep learning models resistant to adversarial attacks. stat, 1050:9, 2017.

[21] Michael Mitzenmacher and Eli Upfal. Probability and computing: Randomized algorithms

and probabilistic analysis. Cambridge university press, 2005.

[22] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a
simple and accurate method to fool deep neural networks. IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2016.

[23] Nicolas Papernot, Nicholas Carlini, Ian Goodfellow, Reuben Feinman, Fartash Faghri,
Alexander Matyasko, Karen Hambardzumyan, Yi-Lin Juang, Alexey Kurakin, Ryan
Sheatsley, Abhibhav Garg, and Yen-Chen Lin. cleverhans v2.0.0: an adversarial machine
learning library. arXiv preprint arXiv:1610.00768, 2017.

[24] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine
learning: from phenomena to black-box attacks using adversarial samples. arXiv preprint
arXiv:1605.07277, 2016.

[25] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Berkay Celik, and
Ananthram Swami. Practical black-box attacks against deep learning systems using
adversarial examples. In Proceedings of the 2017 ACM Asia Conference on Computer
and Communications Security, 2017.

[26] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik,
and Ananthram Swami. The limitations of deep learning in adversarial settings. In
Proceedings of the 1st IEEE European Symposium on Security and Privacy. arXiv
preprint arXiv:1511.07528, 2016.

[27] C. Stone. Consistent nonparametric regression. Annals of Statistics, 5:595–645, 1977.

[28] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint
arXiv:1312.6199, 2013.

15

A Proofs from Section 3

A.1 Proofs for Constant k

Proof. (Of Theorem 3.1) To show convergence in probability, we need to show that for all
(cid:15), δ > 0, there exists an n((cid:15), δ) such that Pr(ρ(Ak(Sn, ·), x) ≥ (cid:15)) ≤ δ for n ≥ n0((cid:15), δ).

The proof will again proceed in two stages. First, we show in Lemma A.1 that if the
conditions in the statement of Theorem 3.1 hold, then there exists some n((cid:15), δ) such that
for n ≥ n((cid:15), δ), with probability at least 1 − δ, there exists two points x+ and x− in B(x, (cid:15))
such that (a) all k nearest neighbors of x+ have label 1, (b) all k nearest neighbors of x−
have label 0, and (c) x+ (cid:54)= x−.

Next we show that if the event stated above happens, then ρ(Ak(Sn, ·), x) ≤ (cid:15). This is
because Ak(Sn, x+) = 1 and Ak(Sn, x−) = 0. No matter what Ak(Sn, x) is, we can always
ﬁnd a point x(cid:48) that lies in {x+, x−} ⊂ B(x, (cid:15)) such that the prediction at x(cid:48) is diﬀerent from
Ak(Sn, x).

Lemma A.1. If the conditions in the statement of Theorem 3.1 hold, then there exists
some n((cid:15), δ) such that for n ≥ n((cid:15), δ), with probability at least 1 − δ, there are two points x+
and x− in B(x, (cid:15)) such that (a) all k nearest neighbors of x+ have label 1, (b) all k nearest
neighbors of x− have label 0, and (c) x+ (cid:54)= x−.

Proof. (Of Lemma A.1) The proof consists of two major components. First, for large enough
n, with high probability there are many disjoint balls in the neighborhood of x such that
each ball contains at least k points in Sn. Second, with high probability among these balls,
there exists a ball such that the k neareast neighbors of its center all have label 1. Similarly,
there exists a ball such that the k nearest neighbor of its center all have label 0.

Since µ is absolutely continuous with respect to Lebesgue measure in the neighbor-
bood of x and η is continuous, then for any m ∈ Z+, we can always ﬁnd m balls
B(x1, r1), · · · , B(xm, rm) such that (a) all m balls are disjoint, and (b) for all i ∈ {1, · · · , m},
we have xi ∈ B(x, (cid:15)), µ(B(xi, ri)) > 0 and η(x) ∈ (0, 1) for x ∈ B(xi, ri). For simplicity, we
(cid:84) Sn. Also, let
use Bi to denote B(xi, ri) and ci(n) to denote the number of points in Bi
µmin = mini∈{1,··· ,m} µ(Bi). Then by Hoeﬀding’s inequality, for each ball Bi and for any
n > k+1
µmin

,

Pr[ci(n) < k] ≤ exp(−2nµ2

min/(k + 1)2),

where the randomness comes from drawing sample Sn. Then taking the union bound over
all m balls, we have

Pr[∃i ∈ {1, · · · , m} such that ci(n) < k] ≤ m exp(−2nµ2
(cid:16) k+1
µmin

, [log m−log(δ/3)](k+1)2
which implies that when n > max
µ2
1 − δ/3, each of B1, · · · , Bm contains at least k points in Sn.

(cid:17)

min

min/(k + 1)2),

(2)

, with probability at least

An important consequence of the above result is that with probability at least 1 − δ/3,
the set of k nearest neighbors of each center xi of Bi is completely diﬀerent from another
center xj’s, so the labels of xi’s k nearest neighbors are independent of the labels of xj’s k
nearest neighbors.

Now let ηmin,+ = minx∈B1

(cid:83)··· (cid:83) Bm(1 − η(x)). Both
ηmin,+ and ηmin,− are greater than 0 by the construction requirements of B1, · · · , Bm. For
any xi,

(cid:83)··· (cid:83) Bm η(x) and ηmin,− = minx∈B1

Pr[xi’s k nearest neighbors all have label 1] ≥ ηk

min,+

16

Then,

Pr[∃i ∈ {1, · · · , m}s.t. xi’s k nearest neighbor all have label 1] ≥ 1 − (1 − ηk

min,+)m,

(3)

which implies when m ≥
s.t. its k nearest neighbors all have label 1. This xi is our x+.

min,+) , with probability at least 1 − δ/3, there exists an xi

log(1−ηk

log δ/3

Similarly,

Pr[∃i ∈ {1, · · · , m} s.t. xi’s k nearest neighbor all have label 0] ≥ 1 − (1 − ηk

min,−)m, (4)

log δ/3

and when m ≥
nearest neighbors all have label 0. This xi is our x−.
Combining the results above, we show that for

log(1−ηk

min,−) , with probability at least 1 − δ/3, there exists an xi s.t. its k

n > max

(cid:18) k + 1
µmin

,

[log m − log(δ/3)](k + 1)2
µ2

min

(cid:32)

m ≥ max

log δ/3
log(1 − ηk

min,+)

,

log δ/3
log(1 − ηk

min,−)

(cid:19)

(cid:33)

,

,

with probability at least 1 − δ, the statement in Lemma A.1 is satisﬁed.

A.2 Theorem and proof for k-nn robustness lower bound.

Theorem 3.1 shows that k-NN is inherently non-robust in the low k regime if η(x) ∈ (0, 1). On
the contrary, k-NN can be robust at x if η(x) ∈ {0, 1}. We deﬁne the r-robust (p, ∆)-interior
as follows:

ˆX +

r,∆,p = {x ∈ supp(µ)|∀x(cid:48) ∈ Bo(x, r),

∀x(cid:48)(cid:48) ∈ B(x(cid:48), rp(x(cid:48))), η(x(cid:48)(cid:48)) ≥ 1/2 + ∆}

ˆX −

r,∆,p = {x ∈ supp(µ)|∀x(cid:48) ∈ Bo(x, r),

∀x(cid:48)(cid:48) ∈ B(x(cid:48), rp(x(cid:48))), η(x(cid:48)(cid:48)) ≤ 1/2 − ∆}

The deﬁnition is similar to the strict r-robust (p, ∆)-interior in Section 4, except replacing <
and > with ≤ and ≥. Theorem A.2 show that k-NN is robust at radius r in the r-robust
(1/2, p)-interior with high high probability. Corollary A.3 shows the ﬁnite sample rate of the
robustness lowerbound.

Theorem A.2. Let x ∈ X ∩ supp(µ) such that (a) µ is absolutely continuous with respect
to the Lebesgue measure (b) η(x) ∈ {0, 1}. Then, for ﬁxed k, there exists an n0 such that for
n ≥ n0,

for all x in ˆX +

r,1/2,p

(cid:83) ˆX −

r,1/2,p for all p > 0, δ > 0.

Pr[ρ(Ak(Sn, ·), x) ≥ r] ≥ 1 − δ

In addition, with probability at least 1 − δ, the astuteness of the k-NN classiﬁer is at least:

E(1(X ∈ ˆX +

r,1/2,p

(cid:91) ˆX −

r,1/2,p))

17

Proof. The k-NN classiﬁer Ak(Sn, ·) is robust at radius r at x if for every x(cid:48) ∈ Bo(x, r), a)
there are k training points in B(x(cid:48), rp(x(cid:48))), and b) more than (cid:98)k/2(cid:99) of them have the same
label as Ak(Sn, x). Without loss of generality, we look at a point x ∈ ˆX +
r,1/2,p. The second
condition is satisﬁed since η(x) = 1 for all training points in B(x(cid:48), rp(x(cid:48))) by the deﬁnition
of ˆX +

r,1/2,p.
It remains to check the ﬁrst condition. Let B be a ball in Rd and n(B) be the number of
training points in B. Lemma 16 of [4] suggests that with probability at least 1 − δ, for all B
in Rd,

µ(B) ≥

+

d log n + log

+

k

d log n + log

(5)

k
n

(cid:32)

Co
n

(cid:115)

(cid:18)

1
δ

(cid:19)(cid:33)

1
δ

implies n(B) ≥ k, where Co is a constant term. Let B = B(x(cid:48), rp(x(cid:48))). By the deﬁnition of
rp, µ(B) ≥ p > 0. Then as n → ∞, Inequality 5 will eventually be satisﬁed, which implies B
contains at least k training points. The ﬁrst condition is then met.
The astuteness result follows because Ak(Sn, x) = y = 1 in ˆX +
r,1/2,p with probability 1.

r,1/2,p and Ak(Sn, x) = y = 0

in ˆX −

Corollary A.3. For n ≥ max(104, c4

d,k,δ/[(k + 1)2p2]) where

cd,k,δ = 4(d + 1) + (cid:112)16(d + 1)2 + 8(ln(8/δ) + k + 1)

, with probability at least 1 − 2δ, ρ(Ak(Sn, x)) ≥ r for all x in ˆX +
p > 0, δ > 0.

r,1/2,p

(cid:83) ˆX −

r,1/2,p and for all

In addition, with probability at least 1 − 2δ, the astuteness of the k-NN classiﬁer is at

least:

E(1(X ∈ ˆX +

r,1/2,p

(cid:91) ˆX −

r,1/2,p))

Proof. Without loss of generality, we look at a point x ∈ ˆX +
r,1/2,p. Let B = B(x(cid:48), rp(x(cid:48))),
J(B) = E(Y · 1(X ∈ B)) and ˆJ(B) be the empirical estimation of J(B). Notice that ˆJ(B)n
is the number of training points in B, because η(x) = 1 for all x ∈ B by the deﬁnition of
r-robust (1/2, p)-interior. It remains to ﬁnd a threshold n such that for all n(cid:48) > n,

By Lemma A.5, with probability 1 − 2δ,

ˆJ(B) ≥ (k + 1)/n(cid:48)

ˆJ(B) ≥ p − 2βn

p − 2β2
n

√

for all B ∈ Rd.

Therefore it suﬃces to ﬁnd a threshold n that satisﬁes

√

p − 2βn

p − 2β2

n ≥ (k + 1)/n,

where βn = (cid:112)(4/n)((d + 1) ln 2n + ln(8/δ)).
Solving this quadratic inequality yields

√

−

p + (cid:112)3p + (k + 1)/n

,

βn ≤

2

18

(6)

(7)

(8)

(9)

(10)

(11)

(12)

(13)

which can be re-written as

√

(8/

n)[(d + 1) ln(2n) + ln(8/δ) + (k + 1)/8] ≤ (cid:112)(k + 1)p

by substituting the expression for βn. This inequality does not admit an analytic solution.
Nevertheless, we observe that n1/4 ≥ ln(2n) for all n ≥ 104. Therefore it suﬃces to ﬁnd an
n ≥ 104 such that

√

(8/

n)[(d + 1)n1/4 + ln(8/δ) + (k + 1)/8] ≤ (cid:112)(k + 1)p.

Let m = n1/4. Inequality 11 can be re-written as

(cid:112)(k + 1)pm2 − 8(d + 1)m − (8 ln(8/δ) + (k + 1)) ≥ 0.

Solving this quadratic inequality with respct to m gives

m ≥

4(d + 1) + (cid:112)16(d + 1)2 + 8(ln(8/δ) + k + 1)
(cid:112)(k + 1)p

.

Letting

cd,k,δ = 4(d + 1) + (cid:112)16(d + 1)2 + 8(ln(8/δ) + k + 1)

, we ﬁnd a desired threshold

n = max(104, m4) ≥ max(104, c4

d,k,δ/[(k + 1)2p2]).

(14)

The astuteness result follows in a similar way to Theorem A.2.

A.3 Proofs for High k

A.3.1 Robustness of the Bayes Optimal Classiﬁer

Proof. (Of Theorem 3.2) Suppose x ∈ X +
r,0,0. Then, g(x) = 1. Consider any x(cid:48) ∈ Bo(x, r);
by deﬁnition, η(x(cid:48)) > 1/2, which implies that g(x(cid:48)) = 1 as well. Thus, ρ(g, x) ≥ r. The other
case (x ∈ X −

r,0,0) is symmetric.

Consider an x ∈ X +

r,0,0 (the other case is symmetric). We just showed that g has
robustness radius ≥ r at x. Moreover, p(y = 1 = g(x)|x) = η(x); therefore, g predicts the
correct label at x with probability η(x). The theorem follows by integrating over all x in
X +

r,0,0 ∪ X −

r,0,0.

A.3.2 Robustness of k-Nearest Neighbor

We begin by stating and proving a more technical version of Theorem 3.3.

Theorem A.4. For any n and data dimension d, deﬁne:

C0
n

(cid:114)

an =

(d log n + log(1/δ))

d log n + log(1/δ)
n

bn = C0
βn = (cid:112)(4/n)((d + 1) ln 2n + ln(8/δ))

19

where C0 is the constant in Theorem 15 of [4]. Now, pick kn and ∆n so that ∆n → 0 and
the following condition is satisﬁed:

and set

kn
n

≥

2βn + bn + (cid:112)(2βn + bn)2 + 2∆n(2β2

n + an)

∆n

pn =

(cid:16)

+

C0
n

d log n + log(1/δ)

kn
n
(cid:17)
+(cid:112)kn(d log n + log(1/δ)

Then, with probability ≥ 1 − 3δ, kn-NN has robustness radius r at all x ∈ X +
. In addition, with probability ≥ 1 − δ, the astuteness of kn-NN is at least:

r,∆n,pn

∪

X −

r,∆n,pn

E[η(X) · 1(X ∈ X +

)] + E(1 − η(X)) · 1(X ∈ X −

r,∆n,pn

r,∆n,pn

)]

Before we prove Theorem A.4, we need some deﬁnitions and lemmas.
For any Euclidean ball B in Rd, deﬁne J(B) = E[Y · 1(X ∈ B)] and ˆJ(B) as the

corresponding empirical quantity.

Lemma A.5. With probability ≥ 1 − 2δ, for all balls B in Rd, we have:

|J(B) − ˆJ(B)| ≤ 2β2

n + 2βn min((cid:112)J(B),

(cid:113)

ˆJ(B)),

where βn = (cid:112)(4/n)((d + 1) ln 2n + ln(8/δ)).
Proof. (Of Lemma A.5) Consider the two functions: h+
h−
B(x, y) = 1(y = −1, x ∈ B). From Lemma A.6, both h+
VC dimension at most d + 1. Additionally, J(B) = E[h+
of [4], along with an union bound gives the lemma.

B(x, y) = 1(y = 1, x ∈ B) and
B and h−
B are 0/1 functions with
B] − E[h−
B]. Applying Theorem 15

Lemma A.6. For an Euclidean ball B in Rd, deﬁne the function h+
as:

B : Rd×{−1, +1} → {0, 1}

h+
B(x, y) = 1(y = 1, x ∈ B)

and let HB = {h+
most d + 1.

B} be the class of all such functions. Then the VC-dimension of HB is at

Proof. (Of Lemma A.6) Let U be a set of d + 2 points in Rd; as the VC dimension of balls
in Rd is d + 1, U cannot be shattered by balls in Rd. Let UL = {(x, y)|x ∈ U } be a labeling
of U that cannot be achieved by any ball (with pluses inside and minuses outside); the
corresponding d + 1-dimensional points cannot be labeled accordingly by h+
B. Since U is
an arbitrary set of d + 2 points, this implies that any set of d + 2 points in Rd × {−1, +1}
cannot be shattered by HB. The lemma follows.

Lemma A.7. Let δp = C0
n
bility ≥ 1−δ, for all x, (cid:107)x−X(k+1)(x)(cid:107) ≤ rk/n+δp (x), and µ(B(x, (cid:107)x−X(k+1)(x)(cid:107))) ≥ k

. Then, with proba-
n −δp.

(cid:17)
d log n + log(1/δ) + (cid:112)k(d log n + log(1/δ)

(cid:16)

20

Figure. 3: Visualization of the halfmoon dataset. 1) Training sample of size n = 2000,
2) subset selected by Robust_1NN with defense radius r = 0.1, 3) subset selected by
Robust_1NN with defense radius r = 0.2.

Proof. (Of Lemma A.7) Observe that by deﬁnition for any x, rp is the smallest r such that
µ(B(x, rp(x)) ≥ p. The rest of the proof follows from Lemma 16 of [4].

Proof. (Of Theorem A.4)

From Lemma A.7, by uniform convergence of ˆµ, with probability ≥ 1 − δ, for all x(cid:48),
(cid:107)x(cid:48) − X (kn)(x(cid:48))(cid:107) ≤ rpn (x(cid:48)) and µ(B(x, (cid:107)x − X (kn)(x)(cid:107))) ≥ kn
,
r,∆n,pn
this implies that for all ˜x ∈ B(x(cid:48), X (kn)(x(cid:48))), η(˜x) ≥ 1/2 + ∆. Therefore, for such an
x(cid:48), J(B(x(cid:48), X (kn)(x(cid:48)))) ≥ ( 1
2 + ∆n)µ(B(x(cid:48), X (kn)(x(cid:48)))) ≥ ( 1
2 + ∆n)(kn/n − δp). Since for
B(x(cid:48), X (kn)(x(cid:48))), ˆµ(B(x(cid:48), X (kn)(x(cid:48)))) = kn
n . Thus we can apply Lemma A.5
to conclude that

n , min( ˆJ, J) ≤ k

If x(cid:48) ∈ X +

n − δp.

ˆJ(B) > J(B) − 2β2

n − 2βn

(cid:112)kn/n >

kn
2n

,

which implies that ˆY (B) = 1
kn
follows.

(cid:80)kn

i=1 Y (i)(x) = n
kn

ˆJ(B) > 1

2 . The ﬁrst part of the theorem

For the second part, observe that for an x ∈ X +

probability η(x) and for an x ∈ X −
Combining this with the ﬁrst part completes the proof.

r,∆n,pn

, the label Y is equal to +1 with
, the label Y is equal to −1 with probability 1 − η(x).

r,∆n,pn

B Proofs from Section 4

We begin with a statement of Chernoﬀ Bounds that we use in our calculations.

Theorem B.1.

[21] Let Xi be a 0/1 random variable and let X = 1
m

(cid:80)m

i=1 Xi. Then,

Pr(|X − E[X]| ≥ δ) ≤ e−mδ2/2 + e−mδ2/3 ≤ 2e−mδ2/3

Lemma B.2. Suppose we run Algorithm 1 with parameter r. Then, the points marked as
red by the algorithm form an r-separated subset of the training set.

21

Figure. 4: Adversarial examples of MNIST digit 1 images created by diﬀerent attack methods.
Top row: clean digit 1 test images. Middle row from left to right: 1) direct attack, 2)
white-box kernel attack. Bottom row from left to right: 1) black-box kernel attack, 2)
black-box neural net substitute attack.

Proof. Let f (xi) denote the output of Algorithm 2 on xi. If (xi, 1) is a Red point, then
f (xi) = 1 = f (xj) for all xj ∈ B(x, r); therefore, (xj, −1) cannot be marked as Red by the
algorithm as f (xj) (cid:54)= yj. The other case, where (xi, −1) is a Red point is similar.

Lemma B.3. Let x ∈ X such that Algorithm 1 ﬁnds a Red xi within Bo(x, τ ). Then,
Algorithm 1 has robustness radius at least r − 2τ at x.

Proof. For all x(cid:48) ∈ B(x, τ ), we have:

(cid:107)x(cid:48) − xi(cid:107) ≤ (cid:107)x − xi(cid:107) + (cid:107)x − x(cid:48)(cid:107) < 2τ

Since xi is a Red point, from Lemma B.2, any xj in training set output by Algorithm 1 with
yj (cid:54)= yi must have the property that (cid:107)xi − xj(cid:107) > 2r. Therefore,

(cid:107)x(cid:48) − xj(cid:107) ≥ (cid:107)xi − xj(cid:107) − (cid:107)x(cid:48) − xi(cid:107) > 2r − 2τ

Therefore, Algorithm 1 will assign x(cid:48) the label yi. The lemma follows.

Lemma B.4. Let B be a ball such that: (a) for all x ∈ B, η(x) > 1
µ(B) ≥ 2C0
one xi such that xi ∈ |B ∩ Xn| and yi = 1.

2 + ∆ and (b)
n (d log n + log(1/δ)). Then, with probability ≥ 1 − δ, all such balls have at least

Proof. Observe that J(B) ≥ C0
that ˆJ(B) > 0, which gives the theorem.

n (d log n + log(1/δ)). Applying Theorem 16 of [4], this implies

Lemma B.5. Fix ∆ and δ, and let kn = 3 log(2n/δ)

. Additionally, let

∆2

pn =

kn
n

+

C0
n

(d log n + log(1/δ) + (cid:112)kn(d log n + log(1/δ)),

22

Table 1: An evaluation of the black-box substitute classiﬁer. Each black-box substitute is
evaluated by: 1) its accuracy on the its training set, 2) its accuracy on the test set, and 3) the
percentage of predictions agreeing with the target classiﬁer on the test set. A combination of
high test accuracy and consistency with the original classiﬁer indicates the black-box model
emulates the target classiﬁer well.

Abalone

target f % training % test % testf
accuracy accuracy same as f
61.3%
62.5%
61.4%
63.5%
68.9%
64.1%
68.4%
65.0%

StandardNN 100%
100%
RobustNN
100%
ATNN
100%
ATNN-All
StandardNN 69.1%
87.2%
RobustNN
68.8%
ATNN
66.5%
ATNN-All
Halfmoon

72.6%
90.9%
73.7%
73.5%
68.6%
86.9%
68.4%
66.6%

target f % training % test % test

StandardNN 95.9%
97.7%
RobustNN
96.4%
ATNN
97.6%
ATNN-All
StandardNN 94.5%
94.2%
RobustNN
95.3%
ATNN
96.9%
ATNN-All

accuracy accuracy same as f
95.6%
94.9%
95.1%
96.8%
94.0%
90.5%
94.2%
96.2%

95.5%
97.6%
96.0%
97.3%
94.4%
94.1%
95.2%
96.5%

MNIST 1v7

target f % training % test % test

StandardNN 100%
100%
RobustNN
100%
ATNN
100%
ATNN-All
StandardNN 99.9%
99.8%
RobustNN
100%
ATNN
99.7%
ATNN-All

accuracy accuracy same as f
98.9%
95.4%
98.9%
98.7%
98.9%
94.8%
98.8%
98.9%

99.3%
97.6%
99.3%
99.3%
99.1%
98.7%
99.2%
99.3%

Kernel

Neural
Nets

Kernel

Neural
Nets

Kernel

Neural
Nets

where C0 is the constant in Theorem 15 of [4]. Deﬁne:

SRED = {(xi, yi) ∈ Sn|xi ∈ X +
r,∆,pn
(cid:19)
1
1
2
2

η(xi) −

yi =

sgn

(cid:18)

+

1
2

}

∪ X −

r,∆,p,

Then, with probability ≥ 1 − δ, all (xi, yi) ∈ SRED are marked as Red by Algorithm 1 run
with parameters r, ∆ and δ.

23

Proof. Consider a (xi, yi) ∈ SRED such that xi ∈ Xn∩X +
, and consider any (xj, yj) ∈ Sn
such that xj ∈ B(xi, r). From Lemma A.7, for all such xj, (cid:107)xj − X (kn)(xj)(cid:107) ≤ rpn (xj); this
means that all kn-nearest neighbors x(cid:48)(cid:48) of such an xj have η(x(cid:48)(cid:48)) > 1

r,∆,pn

Therefore, E[(cid:80)kn

l=1 Y (l)(xj)] ≥ kn(1/2 + ∆); by Theorem B.1, this means that for a
speciﬁc xj, Pr((cid:80)kn
l=1 Y (l)(xj) < 1/2) ≤ 2e−kn∆2/3, which is ≤ δ/n from our choice of kn.
By an union bound over all such xj, with probability ≥ 1 − δ, we see that Algorithm 2
reports the label g(xi) on all such xi, which is the same as yi by the deﬁnition of interiors;
xi therefore gets marked as Red.

2 + ∆.

Finally, we are ready to prove the main theorem of this section, which is a slightly more

technical form of Theorem 4.2.

Theorem B.6. Fix a ∆n, and pick kn and pn as in Lemma B.5. Suppose we run Algorithm 1
with parameters r, ∆n and δ. Consider the set:

XR =

(cid:40)

(cid:12)
(cid:12)x ∈ X +
(cid:12)

x

r+τ,∆n,pn

r+τ,∆n,pn

∪ X −

,

µ(B(x, τ )) ≥

(d log n + log(1/δ))

,

2C0
n

(cid:41)

where C0 is the constant in Theorem 15 of [4]. Then, with probability ≥ 1 − 2δ over the
training set, Algorithm 1 has robustness radius ≥ r − 2τ on XR. Additionally, its astuteness
at radius r − 2τ is at least E[η(X) · 1(X ∈ X +
)].

)] + E[(1 − η(X)) · 1(X ∈ X −

r+τ,∆n,pn

r+τ,∆n,pn

Proof. Due to the condition on µ(B(x, τ )), from Lemma B.4, with probability ≥ 1 − δ, all
x ∈ XR have the property that there exists a (xi, yi) in Sn such that yi = g(xi) and xi ∈
B(x, τ ). Without loss of generality, suppose that x ∈ X +
, so that η(x) > 1/2 + ∆n.
Then, from the properties of r-robust interiors, this xi ∈ X +
.

r+τ,∆n,pn

From Lemma B.5, with probability ≥ 1 − δ, this (xi, yi) is marked Red by Algorithm 1
run with parameters r, ∆n and δ. The theorem now follows from an union bound and
Lemma B.3.

r,∆n,pn

C Experiment Visualization and Validation

First, we show adversarial examples created by diﬀerent attacks on the MNIST dataset in
order to illustrate characteristics of each attack. Next, we show the subset of training points
selected by Algorithm 1 on the halfmoon dataset. The visualization illustrates the intuition
behind Algorithm 1 and also validates its implementation. Finally, we validate how eﬀective
the black-box subsitute classiﬁers emulate the target classiﬁer.

C.1 Adversarial Examples Created by Diﬀerent Attacks

Figure 4 shows adversarial examples created on MNIST digit 1 images with attack radius
r = 3. First, we observe that the perturbations added by direct attack, white-box kernel
attack and black-box kernel attack are clearly targeted: either a faint horizontal stroke or a
shadow of digit 7 are added to the original image. The perturbation budget is used on "key"
pixels that distinguish digit 1 and digit 7, therefore the attack is eﬀective. On the contrary,

24

black-box attacks with neural nets substitute adds perturbation to a large number of pixels.
While such perturbation often fools a neural net classiﬁer, it is not eﬀective against nearest
neighbors. Consider a pixel that is dark in most digit 1 and digit 7 training images; adding
brightness to this pixel increases the distance between the test image to training images from
both classes, therefore may not change the nearest neighbor to the test image.

Figure 4 also illustrates the break-down attack radius of visual similarity. At r = 3,
the true class of adversarial examples created by eﬀective attacks becomes ambiguous even
to humans. Our defense is successful as the Robust_1NN classiﬁers still have non-trivial
classiﬁcation accuracy at such attack radius. Meanwhile, we should not expect robustness
against even larger attack radius since the adversarial examples at r = 3 are already close to
the boundary of human perception.

C.2 Training Subset Selected by Robust_1NN

Figure 3 shows the training set selected by Robust_1NN on a halfmoon training set of size
2000. On the original training set, we see a noisy region between the two halfmoons where
both red and blue points appear. Robust_1NN cleans training points in this region so as to
create a gap between the red and blue halfmoons, and the gap width increases with defense
radius r.

C.3 Performance of Black-box Attack Substitutes

We validate the black-box substitute training process by checking the substitute’s accuracy
on its training set, the clean test set and the percentage of predictions agreeing with the
target classiﬁer on the clean test set. The results are shown in Table 1. For the halfmoon and
MNIST dataset, the substitute classiﬁers both achieve high accuracy on both the training
and test sets, and are also consistent with the target classiﬁer on the test set. The subsitutute
classiﬁers do not emulate the target classiﬁer on the Abalone dataset as close as on the other
two datasets due to the high noise level in the Abalone dataset. Nonetheless, the substitute
classiﬁer still achieve test time accuracy comparable to the target classiﬁer.

25

