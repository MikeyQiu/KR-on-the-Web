Recursive nonlinear-system identiﬁcation using
latent variables

Per Mattsson, Dave Zachariah, Petre Stoica

1

8
1
0
2
 
y
a
M
 
5
2

 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
6
6
3
4
0
.
6
0
6
1
:
v
i
X
r
a

Abstract—In this paper we develop a method for learning
nonlinear system models with multiple outputs and inputs.
We begin by modelling the errors of a nominal predictor of
the system using a latent variable framework. Then using the
maximum likelihood principle we derive a criterion for learning
the model. The resulting optimization problem is tackled using a
majorization-minimization approach. Finally, we develop a con-
vex majorization technique and show that it enables a recursive
identiﬁcation method. The method learns parsimonious predictive
models and is tested on both synthetic and real nonlinear systems.

I. INTRODUCTION

In this paper we consider the problem of learning a non-
linear dynamical system model with multiple outputs y(t)
and multiple inputs u(t) (when they exist). Generally this
identiﬁcation problem can be tackled using different model
structures, with the class of linear models being arguably the
most well studied in engineering, statistics and econometrics
[2], [6], [7], [15], [27].

Linear models are often used even when the system is
known to be nonlinear [10], [25]. However certain nonlinear-
ities, such as saturations, cannot always be neglected. In such
cases using block-oriented models is a popular approach to
capture static nonlinearities [11]. Recently, such models have
been given semiparametric formulations and identiﬁed using
machine learning methods, cf. [22], [23]. To model nonlinear
dynamics a common approach is to use NARMAX models [5],
[26].

In this paper we are interested in recursive identiﬁcation
methods [18]. In cases where the model structure is linear
in the parameters, recursive least-squares can be applied.
For certain models with nonlinear parameters, the extended
recursive least-squares has been used [8]. Another popular ap-
proach is the recursive prediction error method which has been
developed, e.g., for Wiener models, Hammerstein models, and
polynomial state-space models [19], [31], [33].

Nonparametric models are often based on weighted sums of
the observed data [24]. The weights vary for each predicted
output and the number of weights increases with each observed
datapoint. The weights are typically obtained in a batch
manner; in [1], [4] they are computed recursively but must
be recomputed for each new prediction of the output.

For many nonlinear systems, however, linear models work
well as an initial approximation. The strategy in [21] exploits
this fact by ﬁrst ﬁnding the best linear approximation using a

This work has been partly supported by the Swedish Research Council
(VR) under contracts 621-2014-5874 and 2016-06079. Per Mattsson is with
University of G¨avle. Dave Zachariah and Petre Stoica are with Uppsala
University.

frequency domain approach. Then, starting from this approxi-
mation, a nonlinear polynomial state-space model is ﬁtted by
solving a nonconvex problem. This two-step method cannot be
readily implemented recursively and it requires input signals
with appropriate frequency domain properties.

In this paper, we start from a nominal model structure.
This class can be based on insights about the system, e.g.
that linear model structures can approximate a system around
an operating point. Given a record of past outputs, y(t) and
inputs u(t), that is,

Dt ,

(cid:8)

(y(1), u(1)) , . . . , (y(t), u(t))

,

(cid:9)

a nominal model yields a predicted output y0(t + 1) which
differs from the output y(t + 1). The resulting prediction
error is denoted ε(t + 1) [16]. By characterizing the nominal
prediction errors in a data-driven manner, we aim to develop
a reﬁned predictor model of the system. Thus we integrate
classic and data-driven system modeling approaches in a
natural way.

The general model class and problem formulation are intro-
duced in Section II. Then in Section III we apply the principle
of maximum likelihood to derive a statistically motivated
learning criterion. In Section IV this nonconvex criterion is
minimized using a majorization-minimization approach that
gives rise to a convex user-parameter free method. We derive
a computionally efﬁcient recursive algorithm for solving the
convex problem, which can be applied to large datasets as well
as online learning scenarios. In Section V, we evaluate the
proposed method using both synthetic and real data examples.
In a nutshell, the contribution of the paper is a modelling
approach and identiﬁcation method for nonlinear multiple
input-multiple output systems that:

• explicitly separates modeling based on application-
speciﬁc insights from general data-driven modelling,
• circumvents the choice of regularization parameters and

initialization points,

• learns parsimonious predictor models,
• admits a computationally efﬁcient implementation.

Notation: Ei,j denotes the ijth standard basis matrix.
and
denote the Kronecker and Hadamard products, respectively.
) is the vectorization operation.
W =
X⊤WX
0, denote ℓ2-, ℓ1- and weighted
, where W
}
norms, respectively. The Moore-Penrose pseudoinverse of X
p
is denoted X†.

⊙
vec(
·
tr
{

x
k1 and

x
k2,

X
k

⊗

≻

k

k

k

Remark 1. An implementation of the proposed method is
available at https://github.com/magni84/lava.

II. PROBLEM FORMULATION

Given

Dt−1, the ny-dimensional output of a system can

always be written as

where y0(t) is any one-step-ahead predictor based on a
nominal model. Here we consider nominal models on the form

y(t) = y0(t) + ε(t),

y0(t) = Θϕ(t),

(1)

(2)

1 vector ϕ(t) is a given function of

where the p
Θ denotes the unknown parameters.
Remark 2. A typical example of ϕ(t) is

×

Dt−1 and

−

−

1)

1)

· · ·

y⊤(t

na) u⊤(t

ϕ(t) = [y⊤(t

nb) 1]⊤,
(3)
in which case the nominal predictor is linear in the data and
therefore captures the linear system dynamics. Nonlinearities
can be incorporated if such are known about the system, in
which case ϕ(t) will be nonlinear in the data.

u⊤(t

· · ·

−

−

The popular ARX model structure, for instance, can be cast
into the framework (1) and (2) by assuming that the nominal
prediction error ε(t) is a white noise process [15], [27]. For
certain systems, (2) may accurately describe the dynamics
of the system around its operation point and consequently
the white noise assumption on ε(t) may be a reasonable
approximation. However, this ceases to be the case even for
systems with weak nonlinearities, cf. [10].

Next, we develop a data-driven model of the prediction
Dt−1. Specif-

errors ε(t) in (1), conditioned on the past data
ically, we assume the conditional model

(4)

ε(t)

(Zγ(t), Σ),

| Dt−1 ∼ N
where Z is an ny ×
q matrix of unknown latent variables, Σ
1 vector γ(t)
is an unknown covariance matrix, and the q
Dt−1. This is a fairly general model
is any given function of
structure that can capture correlated data-dependent nominal
prediction errors.

×

≡

Note that when Z

0, the prediction errors are temporally
white and the nominal model (2) captures all relevant system
dynamics. The latent variable is modeled as random here.
Before data collection, we assume Z to have mean 0 as we
have no reason to depart from the nominal model assumptions
until after observing data. Using a Gaussian distribution, we
thus get

vec(Z)

(0, D),

∼ N

where D is an unknown covariance matrix.

Our goal here is to identify a reﬁned predictor of the form

(5)

(6)

ˆy(t) =

Θϕ(t)

+

Zγ(t)
,

ˆy0(t)
b
| {z }

b
ˆε(t)
| {z }
from a data set
Dt−1, by maximizing the likelihood function.
The ﬁrst term is an estimate of the nominal predictor model
while the second term tries to capture structure in the data
that is not taken into account by the nominal model. Note that
Z is sparse we obtain a parsimonious predictor model.
when

where

b

2

Remark 3. The model (1)-(4) implies that we can write the
output in the equivalent form

y(t) = Θϕ(t) + Zγ(t) + v(t),

where v(t) is a white process with covariance Σ. In order
to formulate a ﬂexible data-driven error model (4), we over-
parametrize it using a high-dimensional γ(t). In this case,
regularization of Z is desirable, which is achieved by (5). Note
that D and Σ are both unknown. Estimating these covariance
matrices corresponds to using a data-adaptive regularization,
as we show in subsequent sections.

Remark 4. The nonlinear function γ(t) of
Dt−1 can be seen
as a basis expansion which is chosen to yield a ﬂexible
model structure of the errors. In the examples below we will
use the Laplace operator basis functions [28]. Other possible
choices include the polynomial basis functions, Fourier basis
functions, wavelets, etc. [15], [26], [32].
Remark 5. In (6), ˆy(t) is a one-step-ahead predictor. However,
the framework can be readily applied to k-step-ahead predic-
tion where ϕ(t) and γ(t) depend on y(1), . . . , y(t

k).

−

III. LATENT VARIABLE FRAMEWORK

Given a record of N data samples,

DN , our goal is to
estimate Θ and Z to form the reﬁned predictor (6). In Section
III-A, we employ the maximum likelihood approach based
Θ, D, Σ), which requires the
on the likelihood function p(Y
|
estimation of nuisance parameters D and Σ. For notational
simplicity, we write the parameters as Ω =
and in
Section III-B we show how an estimator of Z is obtained as
a function of Ω and

Θ, D, Σ

{

}

DN .

A. Parameter estimation

We write the output samples in matrix form as

Y =

y(1)

y(N )

Rny×N .

· · ·

(cid:2)

∈

(cid:3)

In order to obtain maximum likelihood estimates of Ω, we
ﬁrst derive the likelihood function by marginalizing out the
latent variables from the data distribution:

p(Y

Ω) =

p(Y

Ω, Z)p(Z)dZ,

|

Z

|

(7)

where the data distribution p(Y
(4) and (5), respectively.

|

Ω, Z) and p(Z) are given by

To obtain a closed-form expression of (7), we begin by

constructing the regression matrices

Φ =
Γ =

ϕ(1)
(cid:2)
γ(1)
(cid:2)

· · ·

∈

Rp×N ,
Rq×N .

ϕ(N )
γ(N )
(cid:3)

(cid:3)

· · ·
It is shown in Appendix A that (7) can be written as

∈

p(Y

Ω) =

|

1

(2π)N ny

R

|

|

exp

1
2 k

y

(cid:18)−

Fθ

2
R−1

k

,

(cid:19)

−

(8)

y = vec(Y),

θ = vec(Θ),

z = vec(Z),

(9)

are vectorized variables, and

Iny , G = Γ⊤

F = Φ⊤
⊗
R , GDG⊤ + IN ⊗

Σ.

Iny ,

⊗

(10)

(11)

Note that (8) is not a Gaussian distribution in general, since
R may be a function of Y. It follows that maximizing (8) is
equivalent to solving

Given the overparameterized error model (4), it is natural
to initialize at points in the parameter space which correspond
to the nominal predictor model structure (2). That is,

Ω0 =

Θ0, 0, Σ0}

,

where Σ0 ≻

0,

(19)

at which

Z

{
0.

≡

b

min
Ω

V (Ω),

(12)

A. Convex majorization

3

where

V (Ω) ,

y

k
Fθ = vec(Y

Fθ

2
R−1 + ln

R

k

|
−
ΘΦ) = vec([ε(1)

|

and y
−
nothing but the vector of nominal prediction errors.

· · ·

−

(13)

ε(N )]) is

B. Latent variable estimation

Next, we turn to the latent variable Z which is used to
model the nominal prediction error ε(t) in (4). As we show
in Appendix A, the conditional distribution of Z is Gaussian
and can be written as

Ω, Y) =

p(Z
|

with conditional mean

p

1
(2π)ny q

Σz|

|

exp

1
2 k

z

(cid:18)−

ζ

2
Σ

k

−1
z (cid:19)

−

, (14)

|

|

ln

R

|

| ≤

(15)

(16)

ζ = DG⊤R−1(y

Fθ),

−

and covariance matrix

Σz =

D−1 + G⊤(IN ⊗
(cid:0)

Σ−1)G
(cid:1)

−1

.

An estimate
(vectorized) mean (15) at the optimal estimate
via (12).

Z is then given by evaluating the conditional
Ω obtained

b

b

IV. MAJORIZATION-MINIMIZATION APPROACH

The quantities in the reﬁned predictor (6) are readily ob-
tained from the solution of (12). In general, V (Ω) may have
local minima and (12) must be tackled using computationally
efﬁcient iterative methods to ﬁnd the optimum. The obtained
estimates will
then depend on the choice of initial point
Ω0. Such methods includes the majorization-minimization
approach [12], [35], which in turn include Expectation Maxi-
mization [9] as a special case.

The majorization-minimization approach is based on ﬁnding
Ω) with the following properties:

a majorizing function V ′(Ω

|
V ′(Ω
e

V (Ω)

≤

Ω

∀

Ω),

|

e

For a parsimonious parameterization and computationally
advantageous formulation, we consider a diagonal structure of
the covariance matrices in (4), i.e., we let

Σ = diag(σ1, σ2, . . . , σny ),

(20)

and we let Di = diag(di,1, . . . , di,q) denote the covariance
matrix corresponding to the ith row of Z, so that

D =

Di ⊗

Ei,i.

ny

Xi=1

(21)

We begin by majorizing (13) via linearization of the concave
term ln

R

:

ln

R

|
+ tr
e
{

R−1(IN ⊗
tr
| −
{
R−1(IN ⊗
Σ)
e
}
e

Σ)

+ tr
e

{

} −
G⊤

R−1G

{

G⊤
tr
R−1GD
e
}

,

D
}

e
(22)

e

e

D and

R is obtained by inserting

Σ are arbitrary diagonal covariance matrices
where
Σ into (11). The right-
and
hand side of the inequality above is a majorizing tangent plane
e
, see Appendix B. The use of (22) yields a convex
to ln
|
majorizing function of V (Ω) in (12):

D and

R

e

e

e

|

|

e

y

Ω) =

V ′(Ω

k
−
+ tr

2
Fθ
R−1 + tr
k
R−1GD
G⊤
}
e
R−1(IN ⊗
D
where
}
is a constant. To derive a computationally efﬁcient algorithm
e
e
for minimizing (23), the following theorem is useful:

R−1(IN ⊗
{
K,
+
e

K = ln

e
} −

R−1G

Σ)
}

{
tr

(23)

G⊤

| −

Σ)

R

tr

e

e

e

e

{

{

|

Theorem 1. The majorizing function (23) can also be written
as

where

V ′(Ω

Z,

Ω) =

|

e

(17)

V ′(Ω

Ω) = min

V ′(Ω

Z,

Ω)

Z

|

(24)

|

e

e

Y

k
+ tr

2
ΘΦ
ZΓ
Σ−1 +
k
k
−
−
R−1(IN ⊗
Σ)
+ tr
}
e

{

2
vec(Z)
D−1
k
R−1GD
G⊤
}

{

e

K.
+
(25)
e

with equality when Ω =
Ω. The key is to ﬁnd a majorizing
function that is simpler to minimize than V (Ω). Let Ωk+1
denote the minimizer of V ′(Ω

e

V (Ωk+1)

V ′(Ωk+1|

Ωk)

≤

≤
This property leads directly to an iterative scheme that de-
creases V (Ω) monotonically, starting from an initial estimate
Ω0.

Ωk) = V (Ωk).

(18)

|

Ωk). Then
V ′(Ωk|

The minimizing Z is given by the conditional mean (15).

Proof. The problem in (24) has a minimizing Z which, after
vectorization, equals ζ in (15). Inserting the minimizing Z into
(25) and using (41) yields (23).

Remark 6. The augmented form in (23), enables us to solve
for the nuisance parameters D and Σ in closed-form and also
yields the conditional mean

Z as a by-product.

b

4

To prepare for the minimization of the function (25) we
write the matrix quantities using variables that denote the ith
rows of the following matrices:

...
y⊤
i
...

...
θ⊤
i
...

...
z⊤
i
...

Y = 










, Θ = 










, Z = 










, Γ = 




Theorem 2. After concentrating out the nuisance parameters,
the minimizing arguments Θ and Z of the function (25) are
obtained by solving the following convex problem:

...
γ⊤
i
...

.







ny

min
Θ,Z

Xi=1 (cid:16)k

where

yi −

Φ⊤θi −

Γ⊤zik2 +

wi ⊙

k

zik1

(cid:17)

(26)

wi =

wi,1

(cid:2)
Ri = Γ

DiΓ⊤ +

σiIN .

· · ·

⊤

wi,q

(cid:3)

, wi,j = v
u
u
t

γ⊤
j
tr

R−1
i γ j
R−1
e
i }
{

e

(27)

(28)

B. Recursive computation

We now show that the convex problem (26) can be solved

recursively, for each new sample y(t) and u(t).

Θ: If we ﬁx Z and only solve for Θ, the

1) Computing
solution is given by
b

Θ = Θ

ZH⊤

−

(31)

where

b
Θ = YΦ†

and H⊤ = ΓΦ†.

Note that both Θ and H are independent of Z, and that they
can be computed for each sample t using a standard recursive
least-squares (LS) algorithm:

Θ(t) = Θ(t
H(t) = H(t

1) + (y(t)
−
1) + P(t)ϕ(t)(γ ⊤(t)

Θ(t

−

−

−

1)ϕ(t))ϕ⊤(t)P(t) (32)

ϕ⊤(t)H(t

1))

−

P(t) = P(t

1)

−

−

−

1 + ϕ⊤(t)P(t

P(t

1)ϕ(t)ϕ⊤(t)P(t

1)

.

−
1)ϕ(t)

(33)

(34)

−

−

e

e

e

The closed-form expression for the minimizing nuisance pa-
rameters (20) and (21) are given by
Φ⊤θi −
Γ⊤zik2
R−1
tr
i }
{

zi,j|
R−1

, ˆdi,j =

yi −

ˆσi = k

|
γ ⊤
j

i γ j

(29)

.

q

q

Proof. See Appendix C.

e

e

Remark 10. Natural initial values are Θ(0) = 0 and H(0) =
0. The matrix Φ† equals Φ⊤(ΦΦ⊤)−1 when t
p samples
yield a full-rank matrix Φ. The matrix P(t) converges to
(ΦΦ⊤)−1. A common choice for the initial value of P(t)
is P(0) = cI, where a larger constant c > 0 leads to a faster
convergence of (34), cf. [27], [29].

≥

2) Computing

Z: Using (31), we concentrate out Θ from

Remark 7. Problem (26) contains a data-adaptive regularizing
term which typically leads to parsimonious estimates of Z, cf.
[30].

Remark 8. Majorizing at a nominal predictor model, i.e.
Σ
Θ, 0,
{
}
weights are given by

as discussed above, yields

Ω =
σiIN and the

Ri =

e

where

e

e

e

e

wi,j = k

γjk2
√N

.

Then problem (26) and consequently the minimization of (24)
becomes invariant with respect to

Σ.

e

The iterative scheme (18) is executed by initializing at
Ω := Ω0 and solving (26). The procedure is then repeated
by updating the majorization point using the new estimate
e
Ω. It follows that the estimates will converge to a local
Ω :=
minima of (13). The following theorem establishes that the
e
local minima found, and thus the resulting predictor (6), is
invariant to Ω0 in the form (19).

b

Theorem 3. All initial points Ω0 in the form (19) result in the
Θk,
Zk) of (26), for all k > 0.
same sequence of minimizers (
Σk) converges to a unique point
Moreover, the sequence (
when k

Dk,

b

b

.
→ ∞

b

b

Proof. See Appendix D.

Remark 9. As a result we may initialize the scheme (18)
at Ω0 =
. This obviates the need for carefully
selecting an initialization point, which would be needed in
e.g. the Expectation Maximization algorithm.

0, 0, Iny }

{

(26) to obtain

b

ny

V ′(Z) =

Xi=1 (cid:16)k

ξi − (cid:16)

Γ⊤

Φ⊤H

−

zik2 +

wi ⊙

k

zik1

(cid:17)

(cid:17)

ξi = yi −

Φ⊤θi.

(30)

In Appendix E it is shown how the minimum of V ′(Z) can be
found via cyclic minimization with respect to the elements of
Z, similar to what has been done in [36] in a simpler case. This
iterative procedure is implemented using recursively computed
quantities and produces an estimate

Z(t) at sample t.

3) Summary of the algorithm: The algorithm computes
Z(t) recursively by means of the following steps

b

Θ(t) and
at each sample t:
b

b

i) Compute Θ(t) and H(t), using (32)-(34).
ii) Compute

Z(t) via the cyclic minimization of V ′(Z).

Cycle through all elements L times.
Z(t)H⊤(t)

b
Θ(t) = Θ(t)

iii) Compute

−

Θ(0) = 0 and

Z(0) = 0. In
The estimates are initialized as
b
b
practice, small L works well since we cycle L times through
all elements of Z for each new data sample. The computational
details are given in Algorithm 1 in Appendix E, which can be
readily implemented e.g. in MATLAB.

b

b

V. NUMERICAL EXPERIMENTS

In this section we evaluate the proposed method and com-

pare it with two alternative identiﬁcation methods.

A. Identiﬁcation methods and experimental setup

B. Performance metric

The numerical experiments were conducted as follows.
Three methods have been used: LS identiﬁcation of afﬁne
ARX, NARX using wavelet networks (WAVE for short), and the
latent variable method (LAVA) presented in this paper. From
our numerical experiments we found that performing even
only one iteration of the majorization-minimization algorithm
produces good results, and doing so leads to a computationally
efﬁcient recursive implementation (which we denote LAVA-R
for recursive).

For each method the function ϕ(t) is taken as the linear
regressor in (3). Then the dimension of ϕ(t) equals p =
nyna + nunb + 1. For afﬁne ARX, the model is given by

ˆyARX (t) = Θϕ(t),

where Θ is estimated using recursive least squares [27]. Note
that in LAVA-R, Θ is computed as a byproduct (32).

For the wavelet network, nlarx in the System Identiﬁ-
cation Toolbox for Matlab was used, with the number of
nonlinear units automatically detected [17].

{

Ω0 =

Θ,
0, 0, Iny }
b
b

For LAVA-R, the model is given by (6) and

Z are found
. The
by the minimization of (26) using
minimization is performed using the recursive algorithm in
Section IV-B3 with L = 5. The nonlinear function γ(t) of
the data
Dt−1 can be chosen to be a set of basis functions
evaluated at ϕ(t). Then Zγ(t) can be seen as a truncated
basis expansion of some nonlinear function. In the numerical
examples, γ(t) uses the Laplace basis expansion due to its
good approximation properties [28]. Each element
in the
expansion is given by

e

γk1,...,kp−1(t) =

p−1

Yi=1

1
√ℓi

sin

πki(ϕi(t) + ℓi)
2ℓi

,

(cid:19)

(cid:18)

(35)

γ(t) = [γ1,...,1(t)

γp−1,...,p−1(t)]⊤

· · ·
equals q = M p−1, where M is a user parameter which
determines the resolution of the basis.

Finally, an important part of the identiﬁcation setup is the
choice of input signal. For a nonlinear system it is important
to excite the system both in frequency and in amplitude. For
linear models a commonly used input signal is a pseudoran-
dom binary sequence (PRBS), which is a signal that shifts
between two levels in a certain fashion. One reason for using
PRBS is that it has good correlation properties [27]. Hence,
PRBS excites the system well in frequency, but poorly in
amplitude. A remedy to the poor amplitude excitation is to
multiply each interval of constant signal level with a random
factor that is uniformly distributed on some interval [0, A], cf.
[34]. Hence, if the PRBS takes the values
1 and 1, then
the resulting sequence will contain constant intervals with
random amplitudes between
A and A. We denote such a
−
random amplitude sequence RS(A) where A is the maximum
amplitude.

−

5

For the examples considered here the system does not
necessarily belong to the model class, and thus there is no true
parameter vector to compare with the estimated parameters.
Hence, the different methods will instead be evaluated with
respect to the simulated model output ˆys(t). For LAVA-R
ϕ(t) = [ˆy⊤
Θ
ys(t) =
b
and
b

γ(t) is computed as (35) with ϕ(t) replaced by ˆϕ(t).
The performance can then be evaluated using the root mean

s (t
−
ϕ(t) +

na) u⊤(t

· · ·
γ(t)

u⊤(t

s (t

ˆy⊤

· · ·

1)

1)

−

−

−

Z

b

b

b

b

squared error (RMSE) for each output channel i,

b

nb) 1]⊤.

RMSEi = v
u
u
t

1
T

T

Xt=1

E [

yi(t)

k

ˆys,i(t)
k

2
2].

−

The expectations are computed using 100 Monte Carlo simu-
lations on validation data.

it

For the dataset collected from a real system,

is not
possible to evaluate the expectation in the RMSE formula.
For such sets we use the ﬁt of the data, i.e.,
ˆys,ik2
¯yi1

−
where ˆys,i contains the simulated outputs for channel i, ¯yi is
the empirical mean of yi and 1 is a vector of ones. Hence,
FIT compares the simulated output errors with those obtained
using the empirical mean as the model output.

yi −
k
yi −
k

FITi = 100

k2 (cid:19)

1
(cid:18)

,

C. System with saturation

Consider the following state-space model,

x1(t + 1) = sat2[0.9x1(t) + 0.1u1(t)]
x2(t + 1) = 0.08x1(t) + 0.9x2(t) + 0.6u2(t)

y(t) = x(t) + e(t).

⊤ and

x1(t) x2(t)
(cid:3)

(cid:2)

sata(x) =

if
x
sign(x)a if

(cid:26)

< a
a

x
|
x
| ≥

|
|

.

(36)

(37)
(38)

·

A block-diagram for the above system is shown in Fig. 1.
The measurement noise e(t) was chosen as a white Gaussian
process with covariance matrix σI where σ = 2.5

10−3.

Data was collected from the system using an RS(A) input
signal for several different amplitudes A. The identiﬁcation
was performed using na = 1, nb = 1, M = 4, and N =
1000 data samples. This means that p = 5 and q = 256, and
therefore there are 10 parameters in Θ and 512 in Z.

Note that, for sufﬁciently low amplitudes A, x1(t) will be
smaller than the saturation level a = 2 for all t, and thus
the system will behave as a linear system. However, when A
increases, the saturation will affect the system output more and
more. The RMSE was computed for eight different amplitudes
A, and the result is shown in Fig. 2. It can be seen that for
small amplitudes, when the system is effectively linear, the
ARX model gives a marginally better result than LAVA-R and
WAVE. However, as the amplitude is increased, the nonlinear
effects become more important, and LAVA-R outperforms both
WAVE and ARX models.

where ℓi are the boundaries of the inputs and outputs for each
channel and ki = 1, . . . , M are the indices for each element
of γ(t). Then the dimension of

where x(t) =

6

u1(t)

Saturated

u2(t)

Linear

e1(t)

e2(t)

+

+

y1(t)

y2(t)

Fig. 1. A block diagram of the system used in Example V-C.

TABLE I
FIT FOR EXAMPLE V-D.

Upper tank
Lower tank

LAVA-R WAVE
91.6%
90.8%

ARX
79.2% 84.9%
76.9% 78.6%

TABLE II
FIT FOR EXAMPLE V-E.

LAVA-R WAVE
83.2%

ARX
78.2% 73.1%

FIT

1.5

2

2.5

3

3.5

4

4.5

1.5

2

2.5

3

3.5

4

4.5

Maximum amplitude, A

Fig. 2. The RMSE for Example (V-C) computed for different input ampli-
tudes, using LAVA-R (solid), afﬁne ARX (dashed) and WAVE (dash-dotted).

D. Water tank

In this example a real cascade tank process is studied. It
consists of two tanks mounted on top of each other, with
free outlets. The top tank is fed with water by a pump. The
input signal is the voltage applied to the pump, and the output
consists of the water level in the two tanks. The setup is
described in more detail in [34]. The data set consists of 2500
samples collected every ﬁve seconds. The ﬁrst 1250 samples
where used for identiﬁcation, and the last 1250 samples for
validation.

and 1458 parameters in Z. LAVA-R found a model with only
37 nonzero parameters in Z, and the simulated output together
with the measured output are shown in Fig. 3. The FIT values,
computed on the validation data are shown in Table I. It can be
seen that an afﬁne ARX model gives a good ﬁt, but also that
using LAVA-R the FIT measure can be improved signiﬁcantly.
In this example, WAVE did not perform very well.

E. Pick-and-place machine

In the ﬁnal example, a real pick-and-place machine is stud-
ied. This machine is used to place electronic components on
a circuit board, and is described in detail in [13]. This system
exhibits saturation, different modes, and other nonlinearities.
The data used here are from a real physical process, and were
also used in e.g. [3], [14], [20]. The data set consists of a 15s
recording of the single input u(t) and the vertical position of
the mounting head y(t). The data was sampled at 50 Hz, and
the ﬁrst 8s (N = 400) were used for identiﬁcation and the last
7s for validation.

The identiﬁcation was performed using na = 2, nb = 2 and
M = 6. For the SISO system considered here, this results in
5 parameters in Θ and 1296 parameters in Z. LAVA-R found
a model with 33 of the parameters in Z being nonzero, the
output of which is shown in Fig. 4.

25

20

15

10

t
u
p
t
u
O

5

0

0

0

500

1000

1500

2000

2500

5

10

15

Time [s]

0

500

2000

2500

1000
Number of samples

1500

Fig. 3. The output in Example V-D (blue), plotted together with the output
of the model identiﬁed by LAVA-R (red). The system was identiﬁed using the
ﬁrst 1250 data samples. The validation set consisted of the remaining 1250
samples.

Fig. 4. The output in Example V-E (blue), plotted together with the output
of the model identiﬁed by LAVA-R (red). The system was identiﬁed using
the ﬁrst part of the data, while the validation set consisted of the remaining
samples indicated after the dashed line.

The FIT values, computed on the validation data, for LAVA-
R, WAVE and afﬁne ARX are shown in Table II. LAVA-R
outperforms NARX using wavelet networks, and both are better
than ARX.

VI. CONCLUSION

The identiﬁcation was performed using na = 2, nb = 2 and
M = 3. With two outputs, this results in 14 parameters in Θ

We have developed a method for learning nonlinear systems
with multiple outputs and inputs. We began by modelling the

0.5

0.4

0.2

0.1

1

0.3

E
S
M
R

0

1

0.25

0.2

0.15

0.1

2

E
S
M
R

0.05

1

k
n
a
t
 
r
e
p
p
U

k
n
a
t
 
r
e
w
o
L

8

6

4

2

0

8

6

4

2

0

7

errors of a nominal predictor using a latent variable formu-
lation. The nominal predictor could for instance be a linear
approximation of the system but could also include known
nonlinearities. A learning criterion was derived based on the
principle of maximum likelihood, which obviates the tuning
of regularization parameters. The criterion is then minimized
using a majorization-minimization approach. Speciﬁcally, we
derived a convex user-parameter free formulation, which led
to a computationally efﬁcient recursive algorithm that can be
applied to large datasets as well as online learning problems.
The method introduced in this paper learns parsimonious
predictor models and captures nonlinear system dynamics.
This was illustrated via synthetic as well as real data examples.
As shown in these examples a recursive implementation of
the proposed method was capable of outperforming a batch
method using a NARX model with a wavelet network.

APPENDIX A
DERIVATION OF THE DISTRIBUTIONS (8) AND (14)

We start by computing p(Y

Ω) given in (7). The function

|
Ω, Z) can be found from (1)–(4) and the chain rule:

p(Y

|

p(Y

Ω, Z) =

pε(y(t)

Θϕ(t)

−

|Dt−1, Z),

(39)

|

N

Yt=1

where we have neglected initial conditions [27]. Since

it can be seen that

p(Y

Ω) =

|

1
(2π)N ny

R

exp

1
2 k

y

(cid:18)−

Fθ

2
R−1

k

(cid:19)

−

,

(43)

|
which proves (8). To obtain an expression for p(Z
|
simply insert (42) and (43) into Bayes’ rule to get (14).

p

|

Ω, Y)

APPENDIX B
DERIVATION OF THE MAJORIZING TANGENT PLANE (22)

The ﬁrst-order Taylor expansion of the log-determinant can

be written as

ln

R

|

| ≃

ln

R

|

|
+ (∂d ln
e

+ (∂σ ln
R

R
)
|R=
|
|
eR)(d

eR(σ
˜d)

σ)

−

|R=
where σ is the vector of diagonal elements in Σ and d contains
the diagonal elements in D.

−

|

e

For the derivatives with respect to d we have
R−1 ∂R
(cid:18)

∂
∂di,j

∂di,j (cid:19)

= tr

= tr

R

ln

(cid:18)

|

|

G⊤R−1G

∂D
∂di,j (cid:19)

.

Note that

so Hence

ny

q

Xi=1

Xj=1

di,j

∂D
∂di,j

= D

pε(y(t)

Θϕ(t)

−
1
2 k

exp

(cid:18)−

y(t)

−

|Dt−1, Z)
Θϕ(t)

∝
Zγ(t)
k

−

2
Σ−1

,

(cid:19)

∂d ln

R

|R=
In the same way

|

eR(d

−

˜d) = tr

G⊤

R−1G(D

(cid:16)

e

D)

.

(cid:17)

−

e

it follows that

p(Y

|

Ω, Z) =
1

(2π)nyN

Σ

N

|

|

p

and thus,

exp

1
2 k

Y

−

(cid:18)−

ΘΦ

ZΓ

−

2
Σ−1

.

(cid:19)

k

Using the vectorized variable in (9)-(10) we can see that

vec(ΘΦ) = Fθ and

vec(ZΓ) = Gz.

(40)

Since ln

∂σ ln

R

|
R

|

|
K + tr

ln

R

|

| ≤

where

eR(σ

˜σ) = tr

|R=
−
(cid:16)
is concave in σ and d, it follows that

R−1(IN ⊗
e

(Σ

−

Σ)

.

(cid:17)

e

G⊤

R−1GD

+ tr

(cid:17)

(cid:16)

(cid:16)

e

e

e

K = ln

R

tr

G⊤

R−1G

D

tr

|

| −

(cid:16)

(cid:17) −

e

e

e
APPENDIX C
PROOF OF THEOREM 2

It follows from (21) that

Σ)

(44)

(cid:17)

R−1(IN ⊗
e
R−1(IN ⊗
e

(cid:16)

Σ)

.

(cid:17)

e

Y

ΘΦ

2
2
IN ⊗Σ−1.
Σ−1 =
Next, we note that the following useful equality holds:

Gz
k

ZΓ

Fθ

−

−

−

−

y

k

k

k

y

k

−

Fθ

Gz
k

−

2
IN ⊗Σ−1 +
y

k

z
k
Fθ

2
D−1 =
2
R−1 +

k

−
where R is given by (11), ζ by (15), and Σz by (16). To see
that the equality holds, expand the norms on both sides of (41)
and apply the matrix inversion lemma.
The sought-after distribution p(Y

Ω) is given by (7). By

−

k

k

k

z

ζ

2
−1
Σ
z

(41)

using (41) it follows that

|

where Ri = Γ⊤DiΓ + σiIN . Hence,

R =

(Ri ⊗

Ei,i)

ny

Xi=1

ny

R−1 =

R−1

i ⊗

Ei,i

.

(cid:1)

Xi=1 (cid:0)

Thus, we can rewrite (25) as (to within an additive constant):

p(Y

|

Ω, Z)p(Z)
1
2 k

exp(

−

∝
y

Fθ

2
R−1) exp(

1
2 k

z

−
with the normalization constant ((2π)ny (N +q)
Noting that

−

−

k

ζ

k
Σ

|

|

2
−1
Σ
z

).

(42)

N

D
|

|

)−1/2.

V ′(Ω

Z,

Ω) =

|

e

ny

Xi=1(cid:16)

1
σi k

exp

1
2 k

z

(cid:18)−

Z

ζ

2
Σ

k

−1
z (cid:19)

−

dZ =

(2π)nyq

Σz|

|

p

where ¯yi = yi −

Φ⊤θi −

zik

k

2
−1
D
i

+

2
2 +

¯yik
R−1
i

e
Γ⊤zi.

σi tr(

) + tr(Γ

R−1

i Γ⊤Di)
(cid:17)

.

(45)

e

8

We next derive analytical expressions for the Σ and D that

minimize V ′(Ω

Z,

|
V ′(Ω

∂
∂σi

Ω). Note that
1
σ2
i k

e
Z,
|

Ω) =

2
2 + tr(

R−1
i

),

¯yik

−
and setting the derivative to zero yields the estimate (29). In the
same way it can be seen that the minimum of di,j is attained
at (29).

e

e

Inserting ˆσi and ˆdi,j into (45), we see that we can ﬁnd the

minimizing Θ and Z by minimizing

APPENDIX E
DERIVATION OF THE PROPOSED RECURSIVE ALGORITHM
In order to minimize V ′(Z) we use a cyclic algorithm. That
is, we minimize with respect to one component at a time.
We follow an approach similar to that in [36], with the main
difference being that here we consider arbitrary nonnegative
weights wi.

Note that minimization of V ′(Z) with respect to zi,j is

equivalent to minimizing

tr(

R−1
i

)
k

yi −

Φ⊤θi −

Γ⊤zik2+

where

V ′(zi,j) =

˜ξi,j −

k

cj zi,jk2 + wi,j |

zi,j|

ny

2

Xi=1(cid:16)q
q

e
γ⊤
zi,j|q
j

|

Xj=1

R−1

i γj

.
(cid:17)

e

Since term i in the above sum is invariant with respect to θk
and zk for k
= i, we can divide term i by 2
), and
see that minimizing the criterion above is equivalent to (26).

R−1
i

tr(

q

e

APPENDIX D
PROOF OF THEOREM 3

0, 0, Iny }

Initializing (18) at Ω0 =

Θ0, 0, Σ0}
where Σ0 = diag(σ(0)
ny ), produces two sequences
denoted Ωk =
and Ωk =
for
k > 0, respectively. This results also in sequences Zk and Zk.
The theorem states that:

{
1 , . . . , σ(0)
Θk, Dk, Σk}

{
Θk, Dk, Σk}

and Ω0 =

{

{

Θk = Θk
Dk →

and Zk = Zk

(46)

Dk −

0 and Σk −
We now show the stronger result that the covariance matrices
converge as

Σk →

(47)

0.

,

i σ(k)

(k)
i = c(k)
i D
i
i = (σ(0)

i = c(k)
σ(k)
2k . Note that c(k)

D(k)
where c(k)
. Hence
(48) implies (47). We prove (48) and (46) by induction. That
(48) and (46) holds for k = 1 follows directly from Theorem
2. Now assume that (48) holds for some k

∀
1 as k

i →

k > 0,

1. Let

→ ∞

(48)

)

,

i

i

1

(k)

Ri = ΓD
Ri = ΓD(k)

i Γ⊤ + σ(k)
i Γ⊤ + σ(k)

i

i IN ,

IN = c(k)

i Ri,

≥

e

where the last equality follows by the assumption in (48).
Therefore the weights used to estimate Θk+1 and Zk+1 are
the same as those used to estimate Θk+1, Zk+1:

wi,j = v
u
u
t

R−1
γ⊤
i γj
j
R−1
)
tr(
e
i

γ⊤

−1
j R
i γj
−1
tr(R
)
i

,

= v
u
u
t

so we can conclude that Θk+1 = Θk+1 and Zk+1 = Zk+1.
The estimate Dk+1 is given by

e

d(k+1)
i,j

=

zi,j|
R−1

|
γ⊤
j

i γj

q

c(k)
i

|
γ⊤R

zi,j|
γ

−1

= q

q

= c(k+1)
i

(k+1)
i,j

d

(k+1)
e
= c(k+1)
so D(k+1)
D
i
i
i
= c(k+1)
seen that σ(k+1)
i
(46) are true for all k > 0 and Theorem 3 follows.

, and in the same way it can be
. Hence by induction (48) and

σ(k+1)
i

i

˜ξi,j = ξi − Xk6=j

ckzi,k,

cj = [Γ⊤

Φ⊤H]j.

−

As in [36] it can be shown that the sign of the optimal ˆzi,j
˜ξi,j). Hence we only have to
is given by sign(ˆzi,j) = sign(c⊤
j
ﬁnd the absolute value ri,j =
zi,j|
cjk
k
It is then straightforward to verify that the minimization of
V ′(zi,j) is equivalent to minimizing
V ′(ri,j ) = (αi,j + βjr2

2gi,jri,j )1/2 + wi,jri,j ,

gi,j = c⊤
j

˜ξi,jk

αi,j =

˜ξi,j.

βj =

. Let

2
2,

2
2,

k

|

i,j −

over all ri,j ≥
the Cauchy-Schwartz inequality it follows that

0, and then setting ˆzi,j = sign(gi,j)ˆri,j . From

αi,jβi,j ≥

g2
i,j.

Using this inequality it was shown in [36] that V ′(ri,j ) is
a convex function. The derivative of V ′(ri,j ) is given by
(dropping the subindices for now),

dV ′
dr

=

βr
g
r + α)1/2 + w.
− |
g
2
|
|
Since V ′(r) is convex it follows that it is minimized by r = 0
if and only if dV ′(0)/dr

0, i.e., if and only if

(βr2

(49)

−

|

≥
αw2

g2.

(50)

(51)

≥
Next we study the case when g2 > αw2. It then follows from
(49) that the stationary points of V ′(r) satisfy

(βr

g

) =

− |

|

−

w(βr2

2

g

|

|

−

r + α)1/2

Solving this equation for r we get the stationary point

ˆr = |

g
|
β −

w

β

β

w2

p

αβ

g2.

−

−
Hence we can conclude that the minimizer of V ′(zi,j) is given
by

p

ˆzi,j =

(cid:26)

sign(gi,j )ˆri,j
0

if αi,j w2
otherwise

i,j < g2
i,j

.

Φ⊤H)⊤(Γ⊤

Next we show how to obtain this estimate using only recur-
sively computed quantities. Let
T = (Γ⊤
ξik
κi =
k
ρi = (Γ⊤
−
ξi −
ηi =
k
ζi = (Γ⊤
−

Φ⊤H)⊤ξi
(Γ⊤
−
Φ⊤H)(ξi −

Φ⊤H)zik
2
2
(Γ⊤

Φ⊤H)zi)

Φ⊤H)

−
2
2

(54)

(53)

(55)

(52)

(56)

−

−

Then it is straightforward to show that

i,j + 2ζi,jzi,j

αi,j = ηi + βjz2
βj = Tj,j
gi,j = ζi,j + βjzi,j.

Also deﬁne Ψa,b(t) recursively, for any two vector-valued
signals a(t), b(t), as

Ψa,b(0) = 0

Ψa,b(t + 1) = Ψa,b(t) + a(t)b⊤(t).

(57)

(58)

Note that Ψa,b(t) = (Ψb,a(t))⊤. It can be veriﬁed that
all quantities (52)–(56), and thus ˆzi,j, can be coputed from
Ψ·,·(N ).

The full algorithm for updating the needed quantities, in-
cluding the update of Θ and H, is summarized in Algorithm
1. Note that the iterations of the outer for-loop can be executed
in parallel.

Algorithm 1 : Recursive solution to (26)
1: Input: y(t), ϕ(t), γ(t), ˇΘ and ˇZ
2: Update P(t), Θ(t) and H(t) according to (32)-(34).
3: Update Ψϕ,ϕ(t), Ψγ,γ(t), Ψy,y(t), Ψϕ,γ(t), Ψϕ,y(t)

and Ψγ,y(t) according to (58) .
4: T = Ψγ,γ
Ψγ,ϕH
−
5: for i = 1, . . . , ny do
⊤
κ = Ψy,y
i,i + ˜θ
6:
ρ = [Ψγ,y]i −
2ρ⊤ˇzi + ˇz⊤
η = κ
−
Tˇzi
ζ = ρ
−
repeat

−
i Ψϕ,ϕ˜θi −
Ψγ,ϕ˜θi −
i Tˇzi.

8:
9:

7:

H⊤Ψϕ,γ + H⊤Ψϕ,ϕH.

⊤

i [Ψϕ,y]i.

2˜θ
H⊤[Ψϕ,γ]i + H⊤Ψϕ,ϕ˜θi.

10:

11:
12:

13:

14:

15:

16:

17:

18:

19:
20:

for j = 1, . . . , q do
α = η + Tj,j ˇz2
g = ζj + Tj,j ˇzi,j.
β = Tj,j.
ˆr = |g|

i,j + 2ζj ˇzi,j.

i,j p

β −

wi,j
β√β−w2
sign(g)ˆr
ˆzi,j =
0
η := η + Tj,j (ˇzi,j −
ζ := ζ + [T]j(ˇzi,j −
ˇzi,j := ˆzi,j.

(cid:26)

g2.
αβ
−
if αw2
i,j < g2
otherwise
ˆzi,j) + 2(ˇzi,j −
ˆzi,j).

ˆzi,j)ζj.

until number of iterations equals L.

21:
22: end for
Z = ˇZ.
23:
Θ = Θ
24:
b
25: Output:
b

end for

ZH.
−
Z.
Θ,
b
b

b

REFERENCES

[1] E. W. Bai and Y. Liu. Recursive direct weight optimization in nonlinear
Automatic

system identiﬁcation: A minimal probability approach.
Control, IEEE Transactions on, 52(7):1218–1231, 2007.

[2] D. Barber. Bayesian reasoning and machine learning. Cambridge

University Press, 2012.

9

[4] H. Bijl, J. W. van Wingerden, T. Sch¨on, and M. Verhaegen. Online
sparse gaussian process regression using ﬁtc and pitc approximations.
IFAC-PapersOnLine, 48(28):703–708, 2015.

[5] S. Billings. Nonlinear System Identiﬁcation: NARMAX Methods in the

Time, Frequency, and Spatio-Temporal Domains. Wiley, 2013.

[6] C. M. Bishop. Pattern Recognition and Machine Learning. Springer,

2006.

[7] G. E. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung. Time Series

Analysis: Forecasting and Control. John Wiley & Sons, 2015.

[8] H. Chen. Extended recursive least squares algorithm for nonlinear
stochastic systems. In American Control Conference, 2004. Proceedings
of the 2004, volume 5, pages 4758–4763. IEEE, 2004.

[9] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood
the royal

from incomplete data via the em algorithm.
statistical society. Series B (methodological), pages 1–38, 1977.
[10] M. Enqvist. Linear models of nonlinear systems. PhD thesis, Link¨oping

Journal of

University, Link¨oping, Sweden, 2005.

[11] F. Giri and E. W. Bai. Block-oriented nonlinear system identiﬁcation,

[12] D. R. Hunter and K. Lange. A tutorial on MM algorithms. The American

volume 1. Springer, 2010.

Statistician, 58(1):30–37, 2004.

[13] A. Juloski, W. Heemels, and G. Ferrari-Trecate. Data-based hybrid mod-
elling of the component placement process in pick-and-place machines.
Control Engineering Practice, 12(10):1241–1252, 2004.

[14] A. L. Juloski, S. Paoletti, and J. Roll. Recent

techniques for the
identiﬁcation of piecewise afﬁne and hybrid systems.
In L. Menini,
L. Zaccarian, and C. Abdallah, editors, Current Trends in Nonlinear
Systems and Control, pages 79–99. Birkh¨auser Boston, 2006.

[15] L. Ljung. System Identiﬁcation: Theory for the User. Pearson Education,

1998.

[16] L. Ljung. Model validation and model error modeling. In The ˚Astr¨om

Symposium on Control, pages 15–42. Studentlitteratur, 1999.

[17] L. Ljung. System identiﬁcation toolbox for use with MATLAB. The

MathWorks, Inc., 2007.
[18] L. Ljung and T. S¨oderstr¨om.

Theory and Practice of Recursive

Identiﬁcation. MIT Press, Cambridge, MA, 1983.

[19] P. Mattsson and T. Wigren. Convergence analysis for recursive Ham-

merstein identiﬁcation. Automatica, 71:179–186, 2016.

[20] H. Ohlsson and L. Ljung. Identiﬁcation of switched linear regression
models using sum-of-norms regularization. Automatica, 49(4):1045–
1050, 2013.

[21] J. Paduart, L. Lauwers, J. Swevers, K. Smolders, Johan Schoukens,
and Rik Pintelon. Identiﬁcation of nonlinear systems using polynomial
nonlinear state space models. Automatica, 46(4):647 – 656, 2010.
[22] G. Pillonetto. Consistent identiﬁcation of wiener systems: A machine

learning viewpoint. Automatica, 49(9):2704–2712, 2013.

[23] G. Pillonetto, F. Dinuzzo, T. Chen, G. De Nicolao, and Lennart Ljung.
Kernel methods in system identiﬁcation, machine learning and function
estimation: A survey. Automatica, 50(3):657–682, 2014.

[24] J. Roll, A. Nazin, and L. Ljung. Nonlinear system identiﬁcation via

direct weight optimization. Automatica, 41(3):475–490, 2005.

[25] J. Schoukens, M. Vaes, and R. Pintelon. Linear system identiﬁcation in
a nonlinear setting: Nonparametric analysis of the nonlinear distortions
and their impact on the best linear approximation. IEEE Control Systems,
36(3):38–69, 2016.

[26] J. Sj¨oberg, Q. Zhang, L. Ljung, A. Benveniste, Bernard Delyon, Pierre-
Yves Glorennec, H˚akan Hjalmarsson, and Anatoli Juditsky. Nonlinear
black-box modeling in system identiﬁcation: a uniﬁed overview. Auto-
matica, 31(12):1691 – 1724, 1995.

[27] T. S¨oderstr¨om and P. Stoica. System identiﬁcation. Prentice-Hall, Inc.,

1988.

[28] A. Solin and S. S¨arkk¨a. Hilbert space methods for reduced-rank
Gaussian process regression, 2014. arXiv preprint arXiv:1401.5508.

[29] P. Stoica and P.

˚Ahgren. Exact initialization of the recursive least-
squares algorithm. International Journal of Adaptive Control and Signal
Processing, 16(3):219–230, 2002.

[30] P. Stoica, D. Zachariah, and J. Li. Weighted SPICE: A unifying approach
for hyperparameter-free sparse estimation. Digital Signal Processing,
33:1–12, 2014.

[31] S. Tayamon, T. Wigren, and J. Schoukens. Convergence analysis and
experiments using an RPEM based on nonlinear ODEs and midpoint
In Decision and Control (CDC), 2012 IEEE 51st Annual
integration.
Conference on, pages 2858–2865. IEEE, 2012.

[3] A. Bemporad, A. Garulli, S. Paoletti, and A. Vicino. A bounded-
IEEE Trans.

error approach to piecewise afﬁne system identiﬁcation.
Automatic Control, 50(10):1567–1580, Oct 2005.

[32] P. Van den Hof and B. Ninness. System identiﬁcation with generalized
In Modelling and Identiﬁcation with

orthonormal basis functions.
Rational Orthogonal Basis Functions, pages 61–102. Springer, 2005.

[33] T. Wigren. Recursive prediction error identiﬁcation using the nonlinear

Wiener model. Automatica, 29(4):1011 – 1025, 1993.

[34] T. Wigren. Recursive prediction error identiﬁcation and scaling of non-
linear state space models using a restricted black box parameterization.
Automatica, 42(1):159 – 168, 2006.

[35] T. T. Wu and K. Lange. The mm alternative to em. Statistical Science,

[36] D. Zachariah and P. Stoica. Online hyperparameter-free sparse estima-
IEEE Trans. Signal Processing, 63(13):3348–3359, July

25(4):492–505, 2010.

tion method.
2015.

10

Recursive nonlinear-system identiﬁcation using
latent variables

Per Mattsson, Dave Zachariah, Petre Stoica

1

8
1
0
2
 
y
a
M
 
5
2

 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
6
6
3
4
0
.
6
0
6
1
:
v
i
X
r
a

Abstract—In this paper we develop a method for learning
nonlinear system models with multiple outputs and inputs.
We begin by modelling the errors of a nominal predictor of
the system using a latent variable framework. Then using the
maximum likelihood principle we derive a criterion for learning
the model. The resulting optimization problem is tackled using a
majorization-minimization approach. Finally, we develop a con-
vex majorization technique and show that it enables a recursive
identiﬁcation method. The method learns parsimonious predictive
models and is tested on both synthetic and real nonlinear systems.

I. INTRODUCTION

In this paper we consider the problem of learning a non-
linear dynamical system model with multiple outputs y(t)
and multiple inputs u(t) (when they exist). Generally this
identiﬁcation problem can be tackled using different model
structures, with the class of linear models being arguably the
most well studied in engineering, statistics and econometrics
[2], [6], [7], [15], [27].

Linear models are often used even when the system is
known to be nonlinear [10], [25]. However certain nonlinear-
ities, such as saturations, cannot always be neglected. In such
cases using block-oriented models is a popular approach to
capture static nonlinearities [11]. Recently, such models have
been given semiparametric formulations and identiﬁed using
machine learning methods, cf. [22], [23]. To model nonlinear
dynamics a common approach is to use NARMAX models [5],
[26].

In this paper we are interested in recursive identiﬁcation
methods [18]. In cases where the model structure is linear
in the parameters, recursive least-squares can be applied.
For certain models with nonlinear parameters, the extended
recursive least-squares has been used [8]. Another popular ap-
proach is the recursive prediction error method which has been
developed, e.g., for Wiener models, Hammerstein models, and
polynomial state-space models [19], [31], [33].

Nonparametric models are often based on weighted sums of
the observed data [24]. The weights vary for each predicted
output and the number of weights increases with each observed
datapoint. The weights are typically obtained in a batch
manner; in [1], [4] they are computed recursively but must
be recomputed for each new prediction of the output.

For many nonlinear systems, however, linear models work
well as an initial approximation. The strategy in [21] exploits
this fact by ﬁrst ﬁnding the best linear approximation using a

This work has been partly supported by the Swedish Research Council
(VR) under contracts 621-2014-5874 and 2016-06079. Per Mattsson is with
University of G¨avle. Dave Zachariah and Petre Stoica are with Uppsala
University.

frequency domain approach. Then, starting from this approxi-
mation, a nonlinear polynomial state-space model is ﬁtted by
solving a nonconvex problem. This two-step method cannot be
readily implemented recursively and it requires input signals
with appropriate frequency domain properties.

In this paper, we start from a nominal model structure.
This class can be based on insights about the system, e.g.
that linear model structures can approximate a system around
an operating point. Given a record of past outputs, y(t) and
inputs u(t), that is,

Dt ,

(cid:8)

(y(1), u(1)) , . . . , (y(t), u(t))

,

(cid:9)

a nominal model yields a predicted output y0(t + 1) which
differs from the output y(t + 1). The resulting prediction
error is denoted ε(t + 1) [16]. By characterizing the nominal
prediction errors in a data-driven manner, we aim to develop
a reﬁned predictor model of the system. Thus we integrate
classic and data-driven system modeling approaches in a
natural way.

The general model class and problem formulation are intro-
duced in Section II. Then in Section III we apply the principle
of maximum likelihood to derive a statistically motivated
learning criterion. In Section IV this nonconvex criterion is
minimized using a majorization-minimization approach that
gives rise to a convex user-parameter free method. We derive
a computionally efﬁcient recursive algorithm for solving the
convex problem, which can be applied to large datasets as well
as online learning scenarios. In Section V, we evaluate the
proposed method using both synthetic and real data examples.
In a nutshell, the contribution of the paper is a modelling
approach and identiﬁcation method for nonlinear multiple
input-multiple output systems that:

• explicitly separates modeling based on application-
speciﬁc insights from general data-driven modelling,
• circumvents the choice of regularization parameters and

initialization points,

• learns parsimonious predictor models,
• admits a computationally efﬁcient implementation.

Notation: Ei,j denotes the ijth standard basis matrix.
and
denote the Kronecker and Hadamard products, respectively.
) is the vectorization operation.
W =
X⊤WX
0, denote ℓ2-, ℓ1- and weighted
, where W
}
norms, respectively. The Moore-Penrose pseudoinverse of X
p
is denoted X†.

⊙
vec(
·
tr
{

x
k1 and

x
k2,

X
k

⊗

≻

k

k

k

Remark 1. An implementation of the proposed method is
available at https://github.com/magni84/lava.

II. PROBLEM FORMULATION

Given

Dt−1, the ny-dimensional output of a system can

always be written as

where y0(t) is any one-step-ahead predictor based on a
nominal model. Here we consider nominal models on the form

y(t) = y0(t) + ε(t),

y0(t) = Θϕ(t),

(1)

(2)

1 vector ϕ(t) is a given function of

where the p
Θ denotes the unknown parameters.
Remark 2. A typical example of ϕ(t) is

×

Dt−1 and

−

−

1)

1)

· · ·

y⊤(t

na) u⊤(t

ϕ(t) = [y⊤(t

nb) 1]⊤,
(3)
in which case the nominal predictor is linear in the data and
therefore captures the linear system dynamics. Nonlinearities
can be incorporated if such are known about the system, in
which case ϕ(t) will be nonlinear in the data.

u⊤(t

· · ·

−

−

The popular ARX model structure, for instance, can be cast
into the framework (1) and (2) by assuming that the nominal
prediction error ε(t) is a white noise process [15], [27]. For
certain systems, (2) may accurately describe the dynamics
of the system around its operation point and consequently
the white noise assumption on ε(t) may be a reasonable
approximation. However, this ceases to be the case even for
systems with weak nonlinearities, cf. [10].

Next, we develop a data-driven model of the prediction
Dt−1. Specif-

errors ε(t) in (1), conditioned on the past data
ically, we assume the conditional model

(4)

ε(t)

(Zγ(t), Σ),

| Dt−1 ∼ N
where Z is an ny ×
q matrix of unknown latent variables, Σ
1 vector γ(t)
is an unknown covariance matrix, and the q
Dt−1. This is a fairly general model
is any given function of
structure that can capture correlated data-dependent nominal
prediction errors.

×

≡

Note that when Z

0, the prediction errors are temporally
white and the nominal model (2) captures all relevant system
dynamics. The latent variable is modeled as random here.
Before data collection, we assume Z to have mean 0 as we
have no reason to depart from the nominal model assumptions
until after observing data. Using a Gaussian distribution, we
thus get

vec(Z)

(0, D),

∼ N

where D is an unknown covariance matrix.

Our goal here is to identify a reﬁned predictor of the form

(5)

(6)

ˆy(t) =

Θϕ(t)

+

Zγ(t)
,

ˆy0(t)
b
| {z }

b
ˆε(t)
| {z }
from a data set
Dt−1, by maximizing the likelihood function.
The ﬁrst term is an estimate of the nominal predictor model
while the second term tries to capture structure in the data
that is not taken into account by the nominal model. Note that
Z is sparse we obtain a parsimonious predictor model.
when

where

b

2

Remark 3. The model (1)-(4) implies that we can write the
output in the equivalent form

y(t) = Θϕ(t) + Zγ(t) + v(t),

where v(t) is a white process with covariance Σ. In order
to formulate a ﬂexible data-driven error model (4), we over-
parametrize it using a high-dimensional γ(t). In this case,
regularization of Z is desirable, which is achieved by (5). Note
that D and Σ are both unknown. Estimating these covariance
matrices corresponds to using a data-adaptive regularization,
as we show in subsequent sections.

Remark 4. The nonlinear function γ(t) of
Dt−1 can be seen
as a basis expansion which is chosen to yield a ﬂexible
model structure of the errors. In the examples below we will
use the Laplace operator basis functions [28]. Other possible
choices include the polynomial basis functions, Fourier basis
functions, wavelets, etc. [15], [26], [32].
Remark 5. In (6), ˆy(t) is a one-step-ahead predictor. However,
the framework can be readily applied to k-step-ahead predic-
tion where ϕ(t) and γ(t) depend on y(1), . . . , y(t

k).

−

III. LATENT VARIABLE FRAMEWORK

Given a record of N data samples,

DN , our goal is to
estimate Θ and Z to form the reﬁned predictor (6). In Section
III-A, we employ the maximum likelihood approach based
Θ, D, Σ), which requires the
on the likelihood function p(Y
|
estimation of nuisance parameters D and Σ. For notational
simplicity, we write the parameters as Ω =
and in
Section III-B we show how an estimator of Z is obtained as
a function of Ω and

Θ, D, Σ

{

}

DN .

A. Parameter estimation

We write the output samples in matrix form as

Y =

y(1)

y(N )

Rny×N .

· · ·

(cid:2)

∈

(cid:3)

In order to obtain maximum likelihood estimates of Ω, we
ﬁrst derive the likelihood function by marginalizing out the
latent variables from the data distribution:

p(Y

Ω) =

p(Y

Ω, Z)p(Z)dZ,

|

Z

|

(7)

where the data distribution p(Y
(4) and (5), respectively.

|

Ω, Z) and p(Z) are given by

To obtain a closed-form expression of (7), we begin by

constructing the regression matrices

Φ =
Γ =

ϕ(1)
(cid:2)
γ(1)
(cid:2)

· · ·

∈

Rp×N ,
Rq×N .

ϕ(N )
γ(N )
(cid:3)

(cid:3)

· · ·
It is shown in Appendix A that (7) can be written as

∈

p(Y

Ω) =

|

1

(2π)N ny

R

|

|

exp

1
2 k

y

(cid:18)−

Fθ

2
R−1

k

,

(cid:19)

−

(8)

y = vec(Y),

θ = vec(Θ),

z = vec(Z),

(9)

are vectorized variables, and

Iny , G = Γ⊤

F = Φ⊤
⊗
R , GDG⊤ + IN ⊗

Σ.

Iny ,

⊗

(10)

(11)

Note that (8) is not a Gaussian distribution in general, since
R may be a function of Y. It follows that maximizing (8) is
equivalent to solving

Given the overparameterized error model (4), it is natural
to initialize at points in the parameter space which correspond
to the nominal predictor model structure (2). That is,

Ω0 =

Θ0, 0, Σ0}

,

where Σ0 ≻

0,

(19)

at which

Z

{
0.

≡

b

min
Ω

V (Ω),

(12)

A. Convex majorization

3

where

V (Ω) ,

y

k
Fθ = vec(Y

Fθ

2
R−1 + ln

R

k

|
−
ΘΦ) = vec([ε(1)

|

and y
−
nothing but the vector of nominal prediction errors.

· · ·

−

(13)

ε(N )]) is

B. Latent variable estimation

Next, we turn to the latent variable Z which is used to
model the nominal prediction error ε(t) in (4). As we show
in Appendix A, the conditional distribution of Z is Gaussian
and can be written as

Ω, Y) =

p(Z
|

with conditional mean

p

1
(2π)ny q

Σz|

|

exp

1
2 k

z

(cid:18)−

ζ

2
Σ

k

−1
z (cid:19)

−

, (14)

|

|

ln

R

|

| ≤

(15)

(16)

ζ = DG⊤R−1(y

Fθ),

−

and covariance matrix

Σz =

D−1 + G⊤(IN ⊗
(cid:0)

Σ−1)G
(cid:1)

−1

.

An estimate
(vectorized) mean (15) at the optimal estimate
via (12).

Z is then given by evaluating the conditional
Ω obtained

b

b

IV. MAJORIZATION-MINIMIZATION APPROACH

The quantities in the reﬁned predictor (6) are readily ob-
tained from the solution of (12). In general, V (Ω) may have
local minima and (12) must be tackled using computationally
efﬁcient iterative methods to ﬁnd the optimum. The obtained
estimates will
then depend on the choice of initial point
Ω0. Such methods includes the majorization-minimization
approach [12], [35], which in turn include Expectation Maxi-
mization [9] as a special case.

The majorization-minimization approach is based on ﬁnding
Ω) with the following properties:

a majorizing function V ′(Ω

|
V ′(Ω
e

V (Ω)

≤

Ω

∀

Ω),

|

e

For a parsimonious parameterization and computationally
advantageous formulation, we consider a diagonal structure of
the covariance matrices in (4), i.e., we let

Σ = diag(σ1, σ2, . . . , σny ),

(20)

and we let Di = diag(di,1, . . . , di,q) denote the covariance
matrix corresponding to the ith row of Z, so that

D =

Di ⊗

Ei,i.

ny

Xi=1

(21)

We begin by majorizing (13) via linearization of the concave
term ln

R

:

ln

R

|
+ tr
e
{

R−1(IN ⊗
tr
| −
{
R−1(IN ⊗
Σ)
e
}
e

Σ)

+ tr
e

{

} −
G⊤

R−1G

{

G⊤
tr
R−1GD
e
}

,

D
}

e
(22)

e

e

D and

R is obtained by inserting

Σ are arbitrary diagonal covariance matrices
where
Σ into (11). The right-
and
hand side of the inequality above is a majorizing tangent plane
e
, see Appendix B. The use of (22) yields a convex
to ln
|
majorizing function of V (Ω) in (12):

D and

R

e

e

e

|

|

e

y

Ω) =

V ′(Ω

k
−
+ tr

2
Fθ
R−1 + tr
k
R−1GD
G⊤
}
e
R−1(IN ⊗
D
where
}
is a constant. To derive a computationally efﬁcient algorithm
e
e
for minimizing (23), the following theorem is useful:

R−1(IN ⊗
{
K,
+
e

K = ln

e
} −

R−1G

Σ)
}

{
tr

(23)

G⊤

| −

Σ)

R

tr

e

e

e

e

{

{

|

Theorem 1. The majorizing function (23) can also be written
as

where

V ′(Ω

Z,

Ω) =

|

e

(17)

V ′(Ω

Ω) = min

V ′(Ω

Z,

Ω)

Z

|

(24)

|

e

e

Y

k
+ tr

2
ΘΦ
ZΓ
Σ−1 +
k
k
−
−
R−1(IN ⊗
Σ)
+ tr
}
e

{

2
vec(Z)
D−1
k
R−1GD
G⊤
}

{

e

K.
+
(25)
e

with equality when Ω =
Ω. The key is to ﬁnd a majorizing
function that is simpler to minimize than V (Ω). Let Ωk+1
denote the minimizer of V ′(Ω

e

V (Ωk+1)

V ′(Ωk+1|

Ωk)

≤

≤
This property leads directly to an iterative scheme that de-
creases V (Ω) monotonically, starting from an initial estimate
Ω0.

Ωk) = V (Ωk).

(18)

|

Ωk). Then
V ′(Ωk|

The minimizing Z is given by the conditional mean (15).

Proof. The problem in (24) has a minimizing Z which, after
vectorization, equals ζ in (15). Inserting the minimizing Z into
(25) and using (41) yields (23).

Remark 6. The augmented form in (23), enables us to solve
for the nuisance parameters D and Σ in closed-form and also
yields the conditional mean

Z as a by-product.

b

4

To prepare for the minimization of the function (25) we
write the matrix quantities using variables that denote the ith
rows of the following matrices:

...
y⊤
i
...

...
θ⊤
i
...

...
z⊤
i
...

Y = 










, Θ = 










, Z = 










, Γ = 




Theorem 2. After concentrating out the nuisance parameters,
the minimizing arguments Θ and Z of the function (25) are
obtained by solving the following convex problem:

...
γ⊤
i
...

.







ny

min
Θ,Z

Xi=1 (cid:16)k

where

yi −

Φ⊤θi −

Γ⊤zik2 +

wi ⊙

k

zik1

(cid:17)

(26)

wi =

wi,1

(cid:2)
Ri = Γ

DiΓ⊤ +

σiIN .

· · ·

⊤

wi,q

(cid:3)

, wi,j = v
u
u
t

γ⊤
j
tr

R−1
i γ j
R−1
e
i }
{

e

(27)

(28)

B. Recursive computation

We now show that the convex problem (26) can be solved

recursively, for each new sample y(t) and u(t).

Θ: If we ﬁx Z and only solve for Θ, the

1) Computing
solution is given by
b

Θ = Θ

ZH⊤

−

(31)

where

b
Θ = YΦ†

and H⊤ = ΓΦ†.

Note that both Θ and H are independent of Z, and that they
can be computed for each sample t using a standard recursive
least-squares (LS) algorithm:

Θ(t) = Θ(t
H(t) = H(t

1) + (y(t)
−
1) + P(t)ϕ(t)(γ ⊤(t)

Θ(t

−

−

−

1)ϕ(t))ϕ⊤(t)P(t) (32)

ϕ⊤(t)H(t

1))

−

P(t) = P(t

1)

−

−

−

1 + ϕ⊤(t)P(t

P(t

1)ϕ(t)ϕ⊤(t)P(t

1)

.

−
1)ϕ(t)

(33)

(34)

−

−

e

e

e

The closed-form expression for the minimizing nuisance pa-
rameters (20) and (21) are given by
Φ⊤θi −
Γ⊤zik2
R−1
tr
i }
{

zi,j|
R−1

, ˆdi,j =

yi −

ˆσi = k

|
γ ⊤
j

i γ j

(29)

.

q

q

Proof. See Appendix C.

e

e

Remark 10. Natural initial values are Θ(0) = 0 and H(0) =
0. The matrix Φ† equals Φ⊤(ΦΦ⊤)−1 when t
p samples
yield a full-rank matrix Φ. The matrix P(t) converges to
(ΦΦ⊤)−1. A common choice for the initial value of P(t)
is P(0) = cI, where a larger constant c > 0 leads to a faster
convergence of (34), cf. [27], [29].

≥

2) Computing

Z: Using (31), we concentrate out Θ from

Remark 7. Problem (26) contains a data-adaptive regularizing
term which typically leads to parsimonious estimates of Z, cf.
[30].

Remark 8. Majorizing at a nominal predictor model, i.e.
Σ
Θ, 0,
{
}
weights are given by

as discussed above, yields

Ω =
σiIN and the

Ri =

e

where

e

e

e

e

wi,j = k

γjk2
√N

.

Then problem (26) and consequently the minimization of (24)
becomes invariant with respect to

Σ.

e

The iterative scheme (18) is executed by initializing at
Ω := Ω0 and solving (26). The procedure is then repeated
by updating the majorization point using the new estimate
e
Ω. It follows that the estimates will converge to a local
Ω :=
minima of (13). The following theorem establishes that the
e
local minima found, and thus the resulting predictor (6), is
invariant to Ω0 in the form (19).

b

Theorem 3. All initial points Ω0 in the form (19) result in the
Θk,
Zk) of (26), for all k > 0.
same sequence of minimizers (
Σk) converges to a unique point
Moreover, the sequence (
when k

Dk,

b

b

.
→ ∞

b

b

Proof. See Appendix D.

Remark 9. As a result we may initialize the scheme (18)
at Ω0 =
. This obviates the need for carefully
selecting an initialization point, which would be needed in
e.g. the Expectation Maximization algorithm.

0, 0, Iny }

{

(26) to obtain

b

ny

V ′(Z) =

Xi=1 (cid:16)k

ξi − (cid:16)

Γ⊤

Φ⊤H

−

zik2 +

wi ⊙

k

zik1

(cid:17)

(cid:17)

ξi = yi −

Φ⊤θi.

(30)

In Appendix E it is shown how the minimum of V ′(Z) can be
found via cyclic minimization with respect to the elements of
Z, similar to what has been done in [36] in a simpler case. This
iterative procedure is implemented using recursively computed
quantities and produces an estimate

Z(t) at sample t.

3) Summary of the algorithm: The algorithm computes
Z(t) recursively by means of the following steps

b

Θ(t) and
at each sample t:
b

b

i) Compute Θ(t) and H(t), using (32)-(34).
ii) Compute

Z(t) via the cyclic minimization of V ′(Z).

Cycle through all elements L times.
Z(t)H⊤(t)

b
Θ(t) = Θ(t)

iii) Compute

−

Θ(0) = 0 and

Z(0) = 0. In
The estimates are initialized as
b
b
practice, small L works well since we cycle L times through
all elements of Z for each new data sample. The computational
details are given in Algorithm 1 in Appendix E, which can be
readily implemented e.g. in MATLAB.

b

b

V. NUMERICAL EXPERIMENTS

In this section we evaluate the proposed method and com-

pare it with two alternative identiﬁcation methods.

A. Identiﬁcation methods and experimental setup

B. Performance metric

The numerical experiments were conducted as follows.
Three methods have been used: LS identiﬁcation of afﬁne
ARX, NARX using wavelet networks (WAVE for short), and the
latent variable method (LAVA) presented in this paper. From
our numerical experiments we found that performing even
only one iteration of the majorization-minimization algorithm
produces good results, and doing so leads to a computationally
efﬁcient recursive implementation (which we denote LAVA-R
for recursive).

For each method the function ϕ(t) is taken as the linear
regressor in (3). Then the dimension of ϕ(t) equals p =
nyna + nunb + 1. For afﬁne ARX, the model is given by

ˆyARX (t) = Θϕ(t),

where Θ is estimated using recursive least squares [27]. Note
that in LAVA-R, Θ is computed as a byproduct (32).

For the wavelet network, nlarx in the System Identiﬁ-
cation Toolbox for Matlab was used, with the number of
nonlinear units automatically detected [17].

{

Ω0 =

Θ,
0, 0, Iny }
b
b

For LAVA-R, the model is given by (6) and

Z are found
. The
by the minimization of (26) using
minimization is performed using the recursive algorithm in
Section IV-B3 with L = 5. The nonlinear function γ(t) of
the data
Dt−1 can be chosen to be a set of basis functions
evaluated at ϕ(t). Then Zγ(t) can be seen as a truncated
basis expansion of some nonlinear function. In the numerical
examples, γ(t) uses the Laplace basis expansion due to its
good approximation properties [28]. Each element
in the
expansion is given by

e

γk1,...,kp−1(t) =

p−1

Yi=1

1
√ℓi

sin

πki(ϕi(t) + ℓi)
2ℓi

,

(cid:19)

(cid:18)

(35)

γ(t) = [γ1,...,1(t)

γp−1,...,p−1(t)]⊤

· · ·
equals q = M p−1, where M is a user parameter which
determines the resolution of the basis.

Finally, an important part of the identiﬁcation setup is the
choice of input signal. For a nonlinear system it is important
to excite the system both in frequency and in amplitude. For
linear models a commonly used input signal is a pseudoran-
dom binary sequence (PRBS), which is a signal that shifts
between two levels in a certain fashion. One reason for using
PRBS is that it has good correlation properties [27]. Hence,
PRBS excites the system well in frequency, but poorly in
amplitude. A remedy to the poor amplitude excitation is to
multiply each interval of constant signal level with a random
factor that is uniformly distributed on some interval [0, A], cf.
[34]. Hence, if the PRBS takes the values
1 and 1, then
the resulting sequence will contain constant intervals with
random amplitudes between
A and A. We denote such a
−
random amplitude sequence RS(A) where A is the maximum
amplitude.

−

5

For the examples considered here the system does not
necessarily belong to the model class, and thus there is no true
parameter vector to compare with the estimated parameters.
Hence, the different methods will instead be evaluated with
respect to the simulated model output ˆys(t). For LAVA-R
ϕ(t) = [ˆy⊤
Θ
ys(t) =
b
and
b

γ(t) is computed as (35) with ϕ(t) replaced by ˆϕ(t).
The performance can then be evaluated using the root mean

s (t
−
ϕ(t) +

na) u⊤(t

· · ·
γ(t)

u⊤(t

s (t

ˆy⊤

· · ·

1)

1)

−

−

−

Z

b

b

b

b

squared error (RMSE) for each output channel i,

b

nb) 1]⊤.

RMSEi = v
u
u
t

1
T

T

Xt=1

E [

yi(t)

k

ˆys,i(t)
k

2
2].

−

The expectations are computed using 100 Monte Carlo simu-
lations on validation data.

it

For the dataset collected from a real system,

is not
possible to evaluate the expectation in the RMSE formula.
For such sets we use the ﬁt of the data, i.e.,
ˆys,ik2
¯yi1

−
where ˆys,i contains the simulated outputs for channel i, ¯yi is
the empirical mean of yi and 1 is a vector of ones. Hence,
FIT compares the simulated output errors with those obtained
using the empirical mean as the model output.

yi −
k
yi −
k

FITi = 100

k2 (cid:19)

1
(cid:18)

,

C. System with saturation

Consider the following state-space model,

x1(t + 1) = sat2[0.9x1(t) + 0.1u1(t)]
x2(t + 1) = 0.08x1(t) + 0.9x2(t) + 0.6u2(t)

y(t) = x(t) + e(t).

⊤ and

x1(t) x2(t)
(cid:3)

(cid:2)

sata(x) =

if
x
sign(x)a if

(cid:26)

< a
a

x
|
x
| ≥

|
|

.

(36)

(37)
(38)

·

A block-diagram for the above system is shown in Fig. 1.
The measurement noise e(t) was chosen as a white Gaussian
process with covariance matrix σI where σ = 2.5

10−3.

Data was collected from the system using an RS(A) input
signal for several different amplitudes A. The identiﬁcation
was performed using na = 1, nb = 1, M = 4, and N =
1000 data samples. This means that p = 5 and q = 256, and
therefore there are 10 parameters in Θ and 512 in Z.

Note that, for sufﬁciently low amplitudes A, x1(t) will be
smaller than the saturation level a = 2 for all t, and thus
the system will behave as a linear system. However, when A
increases, the saturation will affect the system output more and
more. The RMSE was computed for eight different amplitudes
A, and the result is shown in Fig. 2. It can be seen that for
small amplitudes, when the system is effectively linear, the
ARX model gives a marginally better result than LAVA-R and
WAVE. However, as the amplitude is increased, the nonlinear
effects become more important, and LAVA-R outperforms both
WAVE and ARX models.

where ℓi are the boundaries of the inputs and outputs for each
channel and ki = 1, . . . , M are the indices for each element
of γ(t). Then the dimension of

where x(t) =

6

u1(t)

Saturated

u2(t)

Linear

e1(t)

e2(t)

+

+

y1(t)

y2(t)

Fig. 1. A block diagram of the system used in Example V-C.

TABLE I
FIT FOR EXAMPLE V-D.

Upper tank
Lower tank

LAVA-R WAVE
91.6%
90.8%

ARX
79.2% 84.9%
76.9% 78.6%

TABLE II
FIT FOR EXAMPLE V-E.

LAVA-R WAVE
83.2%

ARX
78.2% 73.1%

FIT

1.5

2

2.5

3

3.5

4

4.5

1.5

2

2.5

3

3.5

4

4.5

Maximum amplitude, A

Fig. 2. The RMSE for Example (V-C) computed for different input ampli-
tudes, using LAVA-R (solid), afﬁne ARX (dashed) and WAVE (dash-dotted).

D. Water tank

In this example a real cascade tank process is studied. It
consists of two tanks mounted on top of each other, with
free outlets. The top tank is fed with water by a pump. The
input signal is the voltage applied to the pump, and the output
consists of the water level in the two tanks. The setup is
described in more detail in [34]. The data set consists of 2500
samples collected every ﬁve seconds. The ﬁrst 1250 samples
where used for identiﬁcation, and the last 1250 samples for
validation.

and 1458 parameters in Z. LAVA-R found a model with only
37 nonzero parameters in Z, and the simulated output together
with the measured output are shown in Fig. 3. The FIT values,
computed on the validation data are shown in Table I. It can be
seen that an afﬁne ARX model gives a good ﬁt, but also that
using LAVA-R the FIT measure can be improved signiﬁcantly.
In this example, WAVE did not perform very well.

E. Pick-and-place machine

In the ﬁnal example, a real pick-and-place machine is stud-
ied. This machine is used to place electronic components on
a circuit board, and is described in detail in [13]. This system
exhibits saturation, different modes, and other nonlinearities.
The data used here are from a real physical process, and were
also used in e.g. [3], [14], [20]. The data set consists of a 15s
recording of the single input u(t) and the vertical position of
the mounting head y(t). The data was sampled at 50 Hz, and
the ﬁrst 8s (N = 400) were used for identiﬁcation and the last
7s for validation.

The identiﬁcation was performed using na = 2, nb = 2 and
M = 6. For the SISO system considered here, this results in
5 parameters in Θ and 1296 parameters in Z. LAVA-R found
a model with 33 of the parameters in Z being nonzero, the
output of which is shown in Fig. 4.

25

20

15

10

t
u
p
t
u
O

5

0

0

0

500

1000

1500

2000

2500

5

10

15

Time [s]

0

500

2000

2500

1000
Number of samples

1500

Fig. 3. The output in Example V-D (blue), plotted together with the output
of the model identiﬁed by LAVA-R (red). The system was identiﬁed using the
ﬁrst 1250 data samples. The validation set consisted of the remaining 1250
samples.

Fig. 4. The output in Example V-E (blue), plotted together with the output
of the model identiﬁed by LAVA-R (red). The system was identiﬁed using
the ﬁrst part of the data, while the validation set consisted of the remaining
samples indicated after the dashed line.

The FIT values, computed on the validation data, for LAVA-
R, WAVE and afﬁne ARX are shown in Table II. LAVA-R
outperforms NARX using wavelet networks, and both are better
than ARX.

VI. CONCLUSION

The identiﬁcation was performed using na = 2, nb = 2 and
M = 3. With two outputs, this results in 14 parameters in Θ

We have developed a method for learning nonlinear systems
with multiple outputs and inputs. We began by modelling the

0.5

0.4

0.2

0.1

1

0.3

E
S
M
R

0

1

0.25

0.2

0.15

0.1

2

E
S
M
R

0.05

1

k
n
a
t
 
r
e
p
p
U

k
n
a
t
 
r
e
w
o
L

8

6

4

2

0

8

6

4

2

0

7

errors of a nominal predictor using a latent variable formu-
lation. The nominal predictor could for instance be a linear
approximation of the system but could also include known
nonlinearities. A learning criterion was derived based on the
principle of maximum likelihood, which obviates the tuning
of regularization parameters. The criterion is then minimized
using a majorization-minimization approach. Speciﬁcally, we
derived a convex user-parameter free formulation, which led
to a computationally efﬁcient recursive algorithm that can be
applied to large datasets as well as online learning problems.
The method introduced in this paper learns parsimonious
predictor models and captures nonlinear system dynamics.
This was illustrated via synthetic as well as real data examples.
As shown in these examples a recursive implementation of
the proposed method was capable of outperforming a batch
method using a NARX model with a wavelet network.

APPENDIX A
DERIVATION OF THE DISTRIBUTIONS (8) AND (14)

We start by computing p(Y

Ω) given in (7). The function

|
Ω, Z) can be found from (1)–(4) and the chain rule:

p(Y

|

p(Y

Ω, Z) =

pε(y(t)

Θϕ(t)

−

|Dt−1, Z),

(39)

|

N

Yt=1

where we have neglected initial conditions [27]. Since

it can be seen that

p(Y

Ω) =

|

1
(2π)N ny

R

exp

1
2 k

y

(cid:18)−

Fθ

2
R−1

k

(cid:19)

−

,

(43)

|
which proves (8). To obtain an expression for p(Z
|
simply insert (42) and (43) into Bayes’ rule to get (14).

p

|

Ω, Y)

APPENDIX B
DERIVATION OF THE MAJORIZING TANGENT PLANE (22)

The ﬁrst-order Taylor expansion of the log-determinant can

be written as

ln

R

|

| ≃

ln

R

|

|
+ (∂d ln
e

+ (∂σ ln
R

R
)
|R=
|
|
eR)(d

eR(σ
˜d)

σ)

−

|R=
where σ is the vector of diagonal elements in Σ and d contains
the diagonal elements in D.

−

|

e

For the derivatives with respect to d we have
R−1 ∂R
(cid:18)

∂
∂di,j

∂di,j (cid:19)

= tr

= tr

R

ln

(cid:18)

|

|

G⊤R−1G

∂D
∂di,j (cid:19)

.

Note that

so Hence

ny

q

Xi=1

Xj=1

di,j

∂D
∂di,j

= D

pε(y(t)

Θϕ(t)

−
1
2 k

exp

(cid:18)−

y(t)

−

|Dt−1, Z)
Θϕ(t)

∝
Zγ(t)
k

−

2
Σ−1

,

(cid:19)

∂d ln

R

|R=
In the same way

|

eR(d

−

˜d) = tr

G⊤

R−1G(D

(cid:16)

e

D)

.

(cid:17)

−

e

it follows that

p(Y

|

Ω, Z) =
1

(2π)nyN

Σ

N

|

|

p

and thus,

exp

1
2 k

Y

−

(cid:18)−

ΘΦ

ZΓ

−

2
Σ−1

.

(cid:19)

k

Using the vectorized variable in (9)-(10) we can see that

vec(ΘΦ) = Fθ and

vec(ZΓ) = Gz.

(40)

Since ln

∂σ ln

R

|
R

|

|
K + tr

ln

R

|

| ≤

where

eR(σ

˜σ) = tr

|R=
−
(cid:16)
is concave in σ and d, it follows that

R−1(IN ⊗
e

(Σ

−

Σ)

.

(cid:17)

e

G⊤

R−1GD

+ tr

(cid:17)

(cid:16)

(cid:16)

e

e

e

K = ln

R

tr

G⊤

R−1G

D

tr

|

| −

(cid:16)

(cid:17) −

e

e

e
APPENDIX C
PROOF OF THEOREM 2

It follows from (21) that

Σ)

(44)

(cid:17)

R−1(IN ⊗
e
R−1(IN ⊗
e

(cid:16)

Σ)

.

(cid:17)

e

Y

ΘΦ

2
2
IN ⊗Σ−1.
Σ−1 =
Next, we note that the following useful equality holds:

Gz
k

ZΓ

Fθ

−

−

−

−

y

k

k

k

y

k

−

Fθ

Gz
k

−

2
IN ⊗Σ−1 +
y

k

z
k
Fθ

2
D−1 =
2
R−1 +

k

−
where R is given by (11), ζ by (15), and Σz by (16). To see
that the equality holds, expand the norms on both sides of (41)
and apply the matrix inversion lemma.
The sought-after distribution p(Y

Ω) is given by (7). By

−

k

k

k

z

ζ

2
−1
Σ
z

(41)

using (41) it follows that

|

where Ri = Γ⊤DiΓ + σiIN . Hence,

R =

(Ri ⊗

Ei,i)

ny

Xi=1

ny

R−1 =

R−1

i ⊗

Ei,i

.

(cid:1)

Xi=1 (cid:0)

Thus, we can rewrite (25) as (to within an additive constant):

p(Y

|

Ω, Z)p(Z)
1
2 k

exp(

−

∝
y

Fθ

2
R−1) exp(

1
2 k

z

−
with the normalization constant ((2π)ny (N +q)
Noting that

−

−

k

ζ

k
Σ

|

|

2
−1
Σ
z

).

(42)

N

D
|

|

)−1/2.

V ′(Ω

Z,

Ω) =

|

e

ny

Xi=1(cid:16)

1
σi k

exp

1
2 k

z

(cid:18)−

Z

ζ

2
Σ

k

−1
z (cid:19)

−

dZ =

(2π)nyq

Σz|

|

p

where ¯yi = yi −

Φ⊤θi −

zik

k

2
−1
D
i

+

2
2 +

¯yik
R−1
i

e
Γ⊤zi.

σi tr(

) + tr(Γ

R−1

i Γ⊤Di)
(cid:17)

.

(45)

e

8

We next derive analytical expressions for the Σ and D that

minimize V ′(Ω

Z,

|
V ′(Ω

∂
∂σi

Ω). Note that
1
σ2
i k

e
Z,
|

Ω) =

2
2 + tr(

R−1
i

),

¯yik

−
and setting the derivative to zero yields the estimate (29). In the
same way it can be seen that the minimum of di,j is attained
at (29).

e

e

Inserting ˆσi and ˆdi,j into (45), we see that we can ﬁnd the

minimizing Θ and Z by minimizing

APPENDIX E
DERIVATION OF THE PROPOSED RECURSIVE ALGORITHM
In order to minimize V ′(Z) we use a cyclic algorithm. That
is, we minimize with respect to one component at a time.
We follow an approach similar to that in [36], with the main
difference being that here we consider arbitrary nonnegative
weights wi.

Note that minimization of V ′(Z) with respect to zi,j is

equivalent to minimizing

tr(

R−1
i

)
k

yi −

Φ⊤θi −

Γ⊤zik2+

where

V ′(zi,j) =

˜ξi,j −

k

cj zi,jk2 + wi,j |

zi,j|

ny

2

Xi=1(cid:16)q
q

e
γ⊤
zi,j|q
j

|

Xj=1

R−1

i γj

.
(cid:17)

e

Since term i in the above sum is invariant with respect to θk
and zk for k
= i, we can divide term i by 2
), and
see that minimizing the criterion above is equivalent to (26).

R−1
i

tr(

q

e

APPENDIX D
PROOF OF THEOREM 3

0, 0, Iny }

Initializing (18) at Ω0 =

Θ0, 0, Σ0}
where Σ0 = diag(σ(0)
ny ), produces two sequences
denoted Ωk =
and Ωk =
for
k > 0, respectively. This results also in sequences Zk and Zk.
The theorem states that:

{
1 , . . . , σ(0)
Θk, Dk, Σk}

{
Θk, Dk, Σk}

and Ω0 =

{

{

Θk = Θk
Dk →

and Zk = Zk

(46)

Dk −

0 and Σk −
We now show the stronger result that the covariance matrices
converge as

Σk →

(47)

0.

,

i σ(k)

(k)
i = c(k)
i D
i
i = (σ(0)

i = c(k)
σ(k)
2k . Note that c(k)

D(k)
where c(k)
. Hence
(48) implies (47). We prove (48) and (46) by induction. That
(48) and (46) holds for k = 1 follows directly from Theorem
2. Now assume that (48) holds for some k

∀
1 as k

i →

k > 0,

1. Let

→ ∞

(48)

)

,

i

i

1

(k)

Ri = ΓD
Ri = ΓD(k)

i Γ⊤ + σ(k)
i Γ⊤ + σ(k)

i

i IN ,

IN = c(k)

i Ri,

≥

e

where the last equality follows by the assumption in (48).
Therefore the weights used to estimate Θk+1 and Zk+1 are
the same as those used to estimate Θk+1, Zk+1:

wi,j = v
u
u
t

R−1
γ⊤
i γj
j
R−1
)
tr(
e
i

γ⊤

−1
j R
i γj
−1
tr(R
)
i

,

= v
u
u
t

so we can conclude that Θk+1 = Θk+1 and Zk+1 = Zk+1.
The estimate Dk+1 is given by

e

d(k+1)
i,j

=

zi,j|
R−1

|
γ⊤
j

i γj

q

c(k)
i

|
γ⊤R

zi,j|
γ

−1

= q

q

= c(k+1)
i

(k+1)
i,j

d

(k+1)
e
= c(k+1)
so D(k+1)
D
i
i
i
= c(k+1)
seen that σ(k+1)
i
(46) are true for all k > 0 and Theorem 3 follows.

, and in the same way it can be
. Hence by induction (48) and

σ(k+1)
i

i

˜ξi,j = ξi − Xk6=j

ckzi,k,

cj = [Γ⊤

Φ⊤H]j.

−

As in [36] it can be shown that the sign of the optimal ˆzi,j
˜ξi,j). Hence we only have to
is given by sign(ˆzi,j) = sign(c⊤
j
ﬁnd the absolute value ri,j =
zi,j|
cjk
k
It is then straightforward to verify that the minimization of
V ′(zi,j) is equivalent to minimizing
V ′(ri,j ) = (αi,j + βjr2

2gi,jri,j )1/2 + wi,jri,j ,

gi,j = c⊤
j

˜ξi,jk

αi,j =

˜ξi,j.

βj =

. Let

2
2,

2
2,

k

|

i,j −

over all ri,j ≥
the Cauchy-Schwartz inequality it follows that

0, and then setting ˆzi,j = sign(gi,j)ˆri,j . From

αi,jβi,j ≥

g2
i,j.

Using this inequality it was shown in [36] that V ′(ri,j ) is
a convex function. The derivative of V ′(ri,j ) is given by
(dropping the subindices for now),

dV ′
dr

=

βr
g
r + α)1/2 + w.
− |
g
2
|
|
Since V ′(r) is convex it follows that it is minimized by r = 0
if and only if dV ′(0)/dr

0, i.e., if and only if

(βr2

(49)

−

|

≥
αw2

g2.

(50)

(51)

≥
Next we study the case when g2 > αw2. It then follows from
(49) that the stationary points of V ′(r) satisfy

(βr

g

) =

− |

|

−

w(βr2

2

g

|

|

−

r + α)1/2

Solving this equation for r we get the stationary point

ˆr = |

g
|
β −

w

β

β

w2

p

αβ

g2.

−

−
Hence we can conclude that the minimizer of V ′(zi,j) is given
by

p

ˆzi,j =

(cid:26)

sign(gi,j )ˆri,j
0

if αi,j w2
otherwise

i,j < g2
i,j

.

Φ⊤H)⊤(Γ⊤

Next we show how to obtain this estimate using only recur-
sively computed quantities. Let
T = (Γ⊤
ξik
κi =
k
ρi = (Γ⊤
−
ξi −
ηi =
k
ζi = (Γ⊤
−

Φ⊤H)⊤ξi
(Γ⊤
−
Φ⊤H)(ξi −

Φ⊤H)zik
2
2
(Γ⊤

Φ⊤H)zi)

Φ⊤H)

−
2
2

(54)

(53)

(55)

(52)

(56)

−

−

Then it is straightforward to show that

i,j + 2ζi,jzi,j

αi,j = ηi + βjz2
βj = Tj,j
gi,j = ζi,j + βjzi,j.

Also deﬁne Ψa,b(t) recursively, for any two vector-valued
signals a(t), b(t), as

Ψa,b(0) = 0

Ψa,b(t + 1) = Ψa,b(t) + a(t)b⊤(t).

(57)

(58)

Note that Ψa,b(t) = (Ψb,a(t))⊤. It can be veriﬁed that
all quantities (52)–(56), and thus ˆzi,j, can be coputed from
Ψ·,·(N ).

The full algorithm for updating the needed quantities, in-
cluding the update of Θ and H, is summarized in Algorithm
1. Note that the iterations of the outer for-loop can be executed
in parallel.

Algorithm 1 : Recursive solution to (26)
1: Input: y(t), ϕ(t), γ(t), ˇΘ and ˇZ
2: Update P(t), Θ(t) and H(t) according to (32)-(34).
3: Update Ψϕ,ϕ(t), Ψγ,γ(t), Ψy,y(t), Ψϕ,γ(t), Ψϕ,y(t)

and Ψγ,y(t) according to (58) .
4: T = Ψγ,γ
Ψγ,ϕH
−
5: for i = 1, . . . , ny do
⊤
κ = Ψy,y
i,i + ˜θ
6:
ρ = [Ψγ,y]i −
2ρ⊤ˇzi + ˇz⊤
η = κ
−
Tˇzi
ζ = ρ
−
repeat

−
i Ψϕ,ϕ˜θi −
Ψγ,ϕ˜θi −
i Tˇzi.

8:
9:

7:

H⊤Ψϕ,γ + H⊤Ψϕ,ϕH.

⊤

i [Ψϕ,y]i.

2˜θ
H⊤[Ψϕ,γ]i + H⊤Ψϕ,ϕ˜θi.

10:

11:
12:

13:

14:

15:

16:

17:

18:

19:
20:

for j = 1, . . . , q do
α = η + Tj,j ˇz2
g = ζj + Tj,j ˇzi,j.
β = Tj,j.
ˆr = |g|

i,j + 2ζj ˇzi,j.

i,j p

β −

wi,j
β√β−w2
sign(g)ˆr
ˆzi,j =
0
η := η + Tj,j (ˇzi,j −
ζ := ζ + [T]j(ˇzi,j −
ˇzi,j := ˆzi,j.

(cid:26)

g2.
αβ
−
if αw2
i,j < g2
otherwise
ˆzi,j) + 2(ˇzi,j −
ˆzi,j).

ˆzi,j)ζj.

until number of iterations equals L.

21:
22: end for
Z = ˇZ.
23:
Θ = Θ
24:
b
25: Output:
b

end for

ZH.
−
Z.
Θ,
b
b

b

REFERENCES

[1] E. W. Bai and Y. Liu. Recursive direct weight optimization in nonlinear
Automatic

system identiﬁcation: A minimal probability approach.
Control, IEEE Transactions on, 52(7):1218–1231, 2007.

[2] D. Barber. Bayesian reasoning and machine learning. Cambridge

University Press, 2012.

9

[4] H. Bijl, J. W. van Wingerden, T. Sch¨on, and M. Verhaegen. Online
sparse gaussian process regression using ﬁtc and pitc approximations.
IFAC-PapersOnLine, 48(28):703–708, 2015.

[5] S. Billings. Nonlinear System Identiﬁcation: NARMAX Methods in the

Time, Frequency, and Spatio-Temporal Domains. Wiley, 2013.

[6] C. M. Bishop. Pattern Recognition and Machine Learning. Springer,

2006.

[7] G. E. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung. Time Series

Analysis: Forecasting and Control. John Wiley & Sons, 2015.

[8] H. Chen. Extended recursive least squares algorithm for nonlinear
stochastic systems. In American Control Conference, 2004. Proceedings
of the 2004, volume 5, pages 4758–4763. IEEE, 2004.

[9] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood
the royal

from incomplete data via the em algorithm.
statistical society. Series B (methodological), pages 1–38, 1977.
[10] M. Enqvist. Linear models of nonlinear systems. PhD thesis, Link¨oping

Journal of

University, Link¨oping, Sweden, 2005.

[11] F. Giri and E. W. Bai. Block-oriented nonlinear system identiﬁcation,

[12] D. R. Hunter and K. Lange. A tutorial on MM algorithms. The American

volume 1. Springer, 2010.

Statistician, 58(1):30–37, 2004.

[13] A. Juloski, W. Heemels, and G. Ferrari-Trecate. Data-based hybrid mod-
elling of the component placement process in pick-and-place machines.
Control Engineering Practice, 12(10):1241–1252, 2004.

[14] A. L. Juloski, S. Paoletti, and J. Roll. Recent

techniques for the
identiﬁcation of piecewise afﬁne and hybrid systems.
In L. Menini,
L. Zaccarian, and C. Abdallah, editors, Current Trends in Nonlinear
Systems and Control, pages 79–99. Birkh¨auser Boston, 2006.

[15] L. Ljung. System Identiﬁcation: Theory for the User. Pearson Education,

1998.

[16] L. Ljung. Model validation and model error modeling. In The ˚Astr¨om

Symposium on Control, pages 15–42. Studentlitteratur, 1999.

[17] L. Ljung. System identiﬁcation toolbox for use with MATLAB. The

MathWorks, Inc., 2007.
[18] L. Ljung and T. S¨oderstr¨om.

Theory and Practice of Recursive

Identiﬁcation. MIT Press, Cambridge, MA, 1983.

[19] P. Mattsson and T. Wigren. Convergence analysis for recursive Ham-

merstein identiﬁcation. Automatica, 71:179–186, 2016.

[20] H. Ohlsson and L. Ljung. Identiﬁcation of switched linear regression
models using sum-of-norms regularization. Automatica, 49(4):1045–
1050, 2013.

[21] J. Paduart, L. Lauwers, J. Swevers, K. Smolders, Johan Schoukens,
and Rik Pintelon. Identiﬁcation of nonlinear systems using polynomial
nonlinear state space models. Automatica, 46(4):647 – 656, 2010.
[22] G. Pillonetto. Consistent identiﬁcation of wiener systems: A machine

learning viewpoint. Automatica, 49(9):2704–2712, 2013.

[23] G. Pillonetto, F. Dinuzzo, T. Chen, G. De Nicolao, and Lennart Ljung.
Kernel methods in system identiﬁcation, machine learning and function
estimation: A survey. Automatica, 50(3):657–682, 2014.

[24] J. Roll, A. Nazin, and L. Ljung. Nonlinear system identiﬁcation via

direct weight optimization. Automatica, 41(3):475–490, 2005.

[25] J. Schoukens, M. Vaes, and R. Pintelon. Linear system identiﬁcation in
a nonlinear setting: Nonparametric analysis of the nonlinear distortions
and their impact on the best linear approximation. IEEE Control Systems,
36(3):38–69, 2016.

[26] J. Sj¨oberg, Q. Zhang, L. Ljung, A. Benveniste, Bernard Delyon, Pierre-
Yves Glorennec, H˚akan Hjalmarsson, and Anatoli Juditsky. Nonlinear
black-box modeling in system identiﬁcation: a uniﬁed overview. Auto-
matica, 31(12):1691 – 1724, 1995.

[27] T. S¨oderstr¨om and P. Stoica. System identiﬁcation. Prentice-Hall, Inc.,

1988.

[28] A. Solin and S. S¨arkk¨a. Hilbert space methods for reduced-rank
Gaussian process regression, 2014. arXiv preprint arXiv:1401.5508.

[29] P. Stoica and P.

˚Ahgren. Exact initialization of the recursive least-
squares algorithm. International Journal of Adaptive Control and Signal
Processing, 16(3):219–230, 2002.

[30] P. Stoica, D. Zachariah, and J. Li. Weighted SPICE: A unifying approach
for hyperparameter-free sparse estimation. Digital Signal Processing,
33:1–12, 2014.

[31] S. Tayamon, T. Wigren, and J. Schoukens. Convergence analysis and
experiments using an RPEM based on nonlinear ODEs and midpoint
In Decision and Control (CDC), 2012 IEEE 51st Annual
integration.
Conference on, pages 2858–2865. IEEE, 2012.

[3] A. Bemporad, A. Garulli, S. Paoletti, and A. Vicino. A bounded-
IEEE Trans.

error approach to piecewise afﬁne system identiﬁcation.
Automatic Control, 50(10):1567–1580, Oct 2005.

[32] P. Van den Hof and B. Ninness. System identiﬁcation with generalized
In Modelling and Identiﬁcation with

orthonormal basis functions.
Rational Orthogonal Basis Functions, pages 61–102. Springer, 2005.

[33] T. Wigren. Recursive prediction error identiﬁcation using the nonlinear

Wiener model. Automatica, 29(4):1011 – 1025, 1993.

[34] T. Wigren. Recursive prediction error identiﬁcation and scaling of non-
linear state space models using a restricted black box parameterization.
Automatica, 42(1):159 – 168, 2006.

[35] T. T. Wu and K. Lange. The mm alternative to em. Statistical Science,

[36] D. Zachariah and P. Stoica. Online hyperparameter-free sparse estima-
IEEE Trans. Signal Processing, 63(13):3348–3359, July

25(4):492–505, 2010.

tion method.
2015.

10

Recursive nonlinear-system identiﬁcation using
latent variables

Per Mattsson, Dave Zachariah, Petre Stoica

1

8
1
0
2
 
y
a
M
 
5
2

 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
6
6
3
4
0
.
6
0
6
1
:
v
i
X
r
a

Abstract—In this paper we develop a method for learning
nonlinear system models with multiple outputs and inputs.
We begin by modelling the errors of a nominal predictor of
the system using a latent variable framework. Then using the
maximum likelihood principle we derive a criterion for learning
the model. The resulting optimization problem is tackled using a
majorization-minimization approach. Finally, we develop a con-
vex majorization technique and show that it enables a recursive
identiﬁcation method. The method learns parsimonious predictive
models and is tested on both synthetic and real nonlinear systems.

I. INTRODUCTION

In this paper we consider the problem of learning a non-
linear dynamical system model with multiple outputs y(t)
and multiple inputs u(t) (when they exist). Generally this
identiﬁcation problem can be tackled using different model
structures, with the class of linear models being arguably the
most well studied in engineering, statistics and econometrics
[2], [6], [7], [15], [27].

Linear models are often used even when the system is
known to be nonlinear [10], [25]. However certain nonlinear-
ities, such as saturations, cannot always be neglected. In such
cases using block-oriented models is a popular approach to
capture static nonlinearities [11]. Recently, such models have
been given semiparametric formulations and identiﬁed using
machine learning methods, cf. [22], [23]. To model nonlinear
dynamics a common approach is to use NARMAX models [5],
[26].

In this paper we are interested in recursive identiﬁcation
methods [18]. In cases where the model structure is linear
in the parameters, recursive least-squares can be applied.
For certain models with nonlinear parameters, the extended
recursive least-squares has been used [8]. Another popular ap-
proach is the recursive prediction error method which has been
developed, e.g., for Wiener models, Hammerstein models, and
polynomial state-space models [19], [31], [33].

Nonparametric models are often based on weighted sums of
the observed data [24]. The weights vary for each predicted
output and the number of weights increases with each observed
datapoint. The weights are typically obtained in a batch
manner; in [1], [4] they are computed recursively but must
be recomputed for each new prediction of the output.

For many nonlinear systems, however, linear models work
well as an initial approximation. The strategy in [21] exploits
this fact by ﬁrst ﬁnding the best linear approximation using a

This work has been partly supported by the Swedish Research Council
(VR) under contracts 621-2014-5874 and 2016-06079. Per Mattsson is with
University of G¨avle. Dave Zachariah and Petre Stoica are with Uppsala
University.

frequency domain approach. Then, starting from this approxi-
mation, a nonlinear polynomial state-space model is ﬁtted by
solving a nonconvex problem. This two-step method cannot be
readily implemented recursively and it requires input signals
with appropriate frequency domain properties.

In this paper, we start from a nominal model structure.
This class can be based on insights about the system, e.g.
that linear model structures can approximate a system around
an operating point. Given a record of past outputs, y(t) and
inputs u(t), that is,

Dt ,

(cid:8)

(y(1), u(1)) , . . . , (y(t), u(t))

,

(cid:9)

a nominal model yields a predicted output y0(t + 1) which
differs from the output y(t + 1). The resulting prediction
error is denoted ε(t + 1) [16]. By characterizing the nominal
prediction errors in a data-driven manner, we aim to develop
a reﬁned predictor model of the system. Thus we integrate
classic and data-driven system modeling approaches in a
natural way.

The general model class and problem formulation are intro-
duced in Section II. Then in Section III we apply the principle
of maximum likelihood to derive a statistically motivated
learning criterion. In Section IV this nonconvex criterion is
minimized using a majorization-minimization approach that
gives rise to a convex user-parameter free method. We derive
a computionally efﬁcient recursive algorithm for solving the
convex problem, which can be applied to large datasets as well
as online learning scenarios. In Section V, we evaluate the
proposed method using both synthetic and real data examples.
In a nutshell, the contribution of the paper is a modelling
approach and identiﬁcation method for nonlinear multiple
input-multiple output systems that:

• explicitly separates modeling based on application-
speciﬁc insights from general data-driven modelling,
• circumvents the choice of regularization parameters and

initialization points,

• learns parsimonious predictor models,
• admits a computationally efﬁcient implementation.

Notation: Ei,j denotes the ijth standard basis matrix.
and
denote the Kronecker and Hadamard products, respectively.
) is the vectorization operation.
W =
X⊤WX
0, denote ℓ2-, ℓ1- and weighted
, where W
}
norms, respectively. The Moore-Penrose pseudoinverse of X
p
is denoted X†.

⊙
vec(
·
tr
{

x
k1 and

x
k2,

X
k

⊗

≻

k

k

k

Remark 1. An implementation of the proposed method is
available at https://github.com/magni84/lava.

II. PROBLEM FORMULATION

Given

Dt−1, the ny-dimensional output of a system can

always be written as

where y0(t) is any one-step-ahead predictor based on a
nominal model. Here we consider nominal models on the form

y(t) = y0(t) + ε(t),

y0(t) = Θϕ(t),

(1)

(2)

1 vector ϕ(t) is a given function of

where the p
Θ denotes the unknown parameters.
Remark 2. A typical example of ϕ(t) is

×

Dt−1 and

−

−

1)

1)

· · ·

y⊤(t

na) u⊤(t

ϕ(t) = [y⊤(t

nb) 1]⊤,
(3)
in which case the nominal predictor is linear in the data and
therefore captures the linear system dynamics. Nonlinearities
can be incorporated if such are known about the system, in
which case ϕ(t) will be nonlinear in the data.

u⊤(t

· · ·

−

−

The popular ARX model structure, for instance, can be cast
into the framework (1) and (2) by assuming that the nominal
prediction error ε(t) is a white noise process [15], [27]. For
certain systems, (2) may accurately describe the dynamics
of the system around its operation point and consequently
the white noise assumption on ε(t) may be a reasonable
approximation. However, this ceases to be the case even for
systems with weak nonlinearities, cf. [10].

Next, we develop a data-driven model of the prediction
Dt−1. Specif-

errors ε(t) in (1), conditioned on the past data
ically, we assume the conditional model

(4)

ε(t)

(Zγ(t), Σ),

| Dt−1 ∼ N
where Z is an ny ×
q matrix of unknown latent variables, Σ
1 vector γ(t)
is an unknown covariance matrix, and the q
Dt−1. This is a fairly general model
is any given function of
structure that can capture correlated data-dependent nominal
prediction errors.

×

≡

Note that when Z

0, the prediction errors are temporally
white and the nominal model (2) captures all relevant system
dynamics. The latent variable is modeled as random here.
Before data collection, we assume Z to have mean 0 as we
have no reason to depart from the nominal model assumptions
until after observing data. Using a Gaussian distribution, we
thus get

vec(Z)

(0, D),

∼ N

where D is an unknown covariance matrix.

Our goal here is to identify a reﬁned predictor of the form

(5)

(6)

ˆy(t) =

Θϕ(t)

+

Zγ(t)
,

ˆy0(t)
b
| {z }

b
ˆε(t)
| {z }
from a data set
Dt−1, by maximizing the likelihood function.
The ﬁrst term is an estimate of the nominal predictor model
while the second term tries to capture structure in the data
that is not taken into account by the nominal model. Note that
Z is sparse we obtain a parsimonious predictor model.
when

where

b

2

Remark 3. The model (1)-(4) implies that we can write the
output in the equivalent form

y(t) = Θϕ(t) + Zγ(t) + v(t),

where v(t) is a white process with covariance Σ. In order
to formulate a ﬂexible data-driven error model (4), we over-
parametrize it using a high-dimensional γ(t). In this case,
regularization of Z is desirable, which is achieved by (5). Note
that D and Σ are both unknown. Estimating these covariance
matrices corresponds to using a data-adaptive regularization,
as we show in subsequent sections.

Remark 4. The nonlinear function γ(t) of
Dt−1 can be seen
as a basis expansion which is chosen to yield a ﬂexible
model structure of the errors. In the examples below we will
use the Laplace operator basis functions [28]. Other possible
choices include the polynomial basis functions, Fourier basis
functions, wavelets, etc. [15], [26], [32].
Remark 5. In (6), ˆy(t) is a one-step-ahead predictor. However,
the framework can be readily applied to k-step-ahead predic-
tion where ϕ(t) and γ(t) depend on y(1), . . . , y(t

k).

−

III. LATENT VARIABLE FRAMEWORK

Given a record of N data samples,

DN , our goal is to
estimate Θ and Z to form the reﬁned predictor (6). In Section
III-A, we employ the maximum likelihood approach based
Θ, D, Σ), which requires the
on the likelihood function p(Y
|
estimation of nuisance parameters D and Σ. For notational
simplicity, we write the parameters as Ω =
and in
Section III-B we show how an estimator of Z is obtained as
a function of Ω and

Θ, D, Σ

{

}

DN .

A. Parameter estimation

We write the output samples in matrix form as

Y =

y(1)

y(N )

Rny×N .

· · ·

(cid:2)

∈

(cid:3)

In order to obtain maximum likelihood estimates of Ω, we
ﬁrst derive the likelihood function by marginalizing out the
latent variables from the data distribution:

p(Y

Ω) =

p(Y

Ω, Z)p(Z)dZ,

|

Z

|

(7)

where the data distribution p(Y
(4) and (5), respectively.

|

Ω, Z) and p(Z) are given by

To obtain a closed-form expression of (7), we begin by

constructing the regression matrices

Φ =
Γ =

ϕ(1)
(cid:2)
γ(1)
(cid:2)

· · ·

∈

Rp×N ,
Rq×N .

ϕ(N )
γ(N )
(cid:3)

(cid:3)

· · ·
It is shown in Appendix A that (7) can be written as

∈

p(Y

Ω) =

|

1

(2π)N ny

R

|

|

exp

1
2 k

y

(cid:18)−

Fθ

2
R−1

k

,

(cid:19)

−

(8)

y = vec(Y),

θ = vec(Θ),

z = vec(Z),

(9)

are vectorized variables, and

Iny , G = Γ⊤

F = Φ⊤
⊗
R , GDG⊤ + IN ⊗

Σ.

Iny ,

⊗

(10)

(11)

Note that (8) is not a Gaussian distribution in general, since
R may be a function of Y. It follows that maximizing (8) is
equivalent to solving

Given the overparameterized error model (4), it is natural
to initialize at points in the parameter space which correspond
to the nominal predictor model structure (2). That is,

Ω0 =

Θ0, 0, Σ0}

,

where Σ0 ≻

0,

(19)

at which

Z

{
0.

≡

b

min
Ω

V (Ω),

(12)

A. Convex majorization

3

where

V (Ω) ,

y

k
Fθ = vec(Y

Fθ

2
R−1 + ln

R

k

|
−
ΘΦ) = vec([ε(1)

|

and y
−
nothing but the vector of nominal prediction errors.

· · ·

−

(13)

ε(N )]) is

B. Latent variable estimation

Next, we turn to the latent variable Z which is used to
model the nominal prediction error ε(t) in (4). As we show
in Appendix A, the conditional distribution of Z is Gaussian
and can be written as

Ω, Y) =

p(Z
|

with conditional mean

p

1
(2π)ny q

Σz|

|

exp

1
2 k

z

(cid:18)−

ζ

2
Σ

k

−1
z (cid:19)

−

, (14)

|

|

ln

R

|

| ≤

(15)

(16)

ζ = DG⊤R−1(y

Fθ),

−

and covariance matrix

Σz =

D−1 + G⊤(IN ⊗
(cid:0)

Σ−1)G
(cid:1)

−1

.

An estimate
(vectorized) mean (15) at the optimal estimate
via (12).

Z is then given by evaluating the conditional
Ω obtained

b

b

IV. MAJORIZATION-MINIMIZATION APPROACH

The quantities in the reﬁned predictor (6) are readily ob-
tained from the solution of (12). In general, V (Ω) may have
local minima and (12) must be tackled using computationally
efﬁcient iterative methods to ﬁnd the optimum. The obtained
estimates will
then depend on the choice of initial point
Ω0. Such methods includes the majorization-minimization
approach [12], [35], which in turn include Expectation Maxi-
mization [9] as a special case.

The majorization-minimization approach is based on ﬁnding
Ω) with the following properties:

a majorizing function V ′(Ω

|
V ′(Ω
e

V (Ω)

≤

Ω

∀

Ω),

|

e

For a parsimonious parameterization and computationally
advantageous formulation, we consider a diagonal structure of
the covariance matrices in (4), i.e., we let

Σ = diag(σ1, σ2, . . . , σny ),

(20)

and we let Di = diag(di,1, . . . , di,q) denote the covariance
matrix corresponding to the ith row of Z, so that

D =

Di ⊗

Ei,i.

ny

Xi=1

(21)

We begin by majorizing (13) via linearization of the concave
term ln

R

:

ln

R

|
+ tr
e
{

R−1(IN ⊗
tr
| −
{
R−1(IN ⊗
Σ)
e
}
e

Σ)

+ tr
e

{

} −
G⊤

R−1G

{

G⊤
tr
R−1GD
e
}

,

D
}

e
(22)

e

e

D and

R is obtained by inserting

Σ are arbitrary diagonal covariance matrices
where
Σ into (11). The right-
and
hand side of the inequality above is a majorizing tangent plane
e
, see Appendix B. The use of (22) yields a convex
to ln
|
majorizing function of V (Ω) in (12):

D and

R

e

e

e

|

|

e

y

Ω) =

V ′(Ω

k
−
+ tr

2
Fθ
R−1 + tr
k
R−1GD
G⊤
}
e
R−1(IN ⊗
D
where
}
is a constant. To derive a computationally efﬁcient algorithm
e
e
for minimizing (23), the following theorem is useful:

R−1(IN ⊗
{
K,
+
e

K = ln

e
} −

R−1G

Σ)
}

{
tr

(23)

G⊤

| −

Σ)

R

tr

e

e

e

e

{

{

|

Theorem 1. The majorizing function (23) can also be written
as

where

V ′(Ω

Z,

Ω) =

|

e

(17)

V ′(Ω

Ω) = min

V ′(Ω

Z,

Ω)

Z

|

(24)

|

e

e

Y

k
+ tr

2
ΘΦ
ZΓ
Σ−1 +
k
k
−
−
R−1(IN ⊗
Σ)
+ tr
}
e

{

2
vec(Z)
D−1
k
R−1GD
G⊤
}

{

e

K.
+
(25)
e

with equality when Ω =
Ω. The key is to ﬁnd a majorizing
function that is simpler to minimize than V (Ω). Let Ωk+1
denote the minimizer of V ′(Ω

e

V (Ωk+1)

V ′(Ωk+1|

Ωk)

≤

≤
This property leads directly to an iterative scheme that de-
creases V (Ω) monotonically, starting from an initial estimate
Ω0.

Ωk) = V (Ωk).

(18)

|

Ωk). Then
V ′(Ωk|

The minimizing Z is given by the conditional mean (15).

Proof. The problem in (24) has a minimizing Z which, after
vectorization, equals ζ in (15). Inserting the minimizing Z into
(25) and using (41) yields (23).

Remark 6. The augmented form in (23), enables us to solve
for the nuisance parameters D and Σ in closed-form and also
yields the conditional mean

Z as a by-product.

b

4

To prepare for the minimization of the function (25) we
write the matrix quantities using variables that denote the ith
rows of the following matrices:

...
y⊤
i
...

...
θ⊤
i
...

...
z⊤
i
...

Y = 










, Θ = 










, Z = 










, Γ = 




Theorem 2. After concentrating out the nuisance parameters,
the minimizing arguments Θ and Z of the function (25) are
obtained by solving the following convex problem:

...
γ⊤
i
...

.







ny

min
Θ,Z

Xi=1 (cid:16)k

where

yi −

Φ⊤θi −

Γ⊤zik2 +

wi ⊙

k

zik1

(cid:17)

(26)

wi =

wi,1

(cid:2)
Ri = Γ

DiΓ⊤ +

σiIN .

· · ·

⊤

wi,q

(cid:3)

, wi,j = v
u
u
t

γ⊤
j
tr

R−1
i γ j
R−1
e
i }
{

e

(27)

(28)

B. Recursive computation

We now show that the convex problem (26) can be solved

recursively, for each new sample y(t) and u(t).

Θ: If we ﬁx Z and only solve for Θ, the

1) Computing
solution is given by
b

Θ = Θ

ZH⊤

−

(31)

where

b
Θ = YΦ†

and H⊤ = ΓΦ†.

Note that both Θ and H are independent of Z, and that they
can be computed for each sample t using a standard recursive
least-squares (LS) algorithm:

Θ(t) = Θ(t
H(t) = H(t

1) + (y(t)
−
1) + P(t)ϕ(t)(γ ⊤(t)

Θ(t

−

−

−

1)ϕ(t))ϕ⊤(t)P(t) (32)

ϕ⊤(t)H(t

1))

−

P(t) = P(t

1)

−

−

−

1 + ϕ⊤(t)P(t

P(t

1)ϕ(t)ϕ⊤(t)P(t

1)

.

−
1)ϕ(t)

(33)

(34)

−

−

e

e

e

The closed-form expression for the minimizing nuisance pa-
rameters (20) and (21) are given by
Φ⊤θi −
Γ⊤zik2
R−1
tr
i }
{

zi,j|
R−1

, ˆdi,j =

yi −

ˆσi = k

|
γ ⊤
j

i γ j

(29)

.

q

q

Proof. See Appendix C.

e

e

Remark 10. Natural initial values are Θ(0) = 0 and H(0) =
0. The matrix Φ† equals Φ⊤(ΦΦ⊤)−1 when t
p samples
yield a full-rank matrix Φ. The matrix P(t) converges to
(ΦΦ⊤)−1. A common choice for the initial value of P(t)
is P(0) = cI, where a larger constant c > 0 leads to a faster
convergence of (34), cf. [27], [29].

≥

2) Computing

Z: Using (31), we concentrate out Θ from

Remark 7. Problem (26) contains a data-adaptive regularizing
term which typically leads to parsimonious estimates of Z, cf.
[30].

Remark 8. Majorizing at a nominal predictor model, i.e.
Σ
Θ, 0,
{
}
weights are given by

as discussed above, yields

Ω =
σiIN and the

Ri =

e

where

e

e

e

e

wi,j = k

γjk2
√N

.

Then problem (26) and consequently the minimization of (24)
becomes invariant with respect to

Σ.

e

The iterative scheme (18) is executed by initializing at
Ω := Ω0 and solving (26). The procedure is then repeated
by updating the majorization point using the new estimate
e
Ω. It follows that the estimates will converge to a local
Ω :=
minima of (13). The following theorem establishes that the
e
local minima found, and thus the resulting predictor (6), is
invariant to Ω0 in the form (19).

b

Theorem 3. All initial points Ω0 in the form (19) result in the
Θk,
Zk) of (26), for all k > 0.
same sequence of minimizers (
Σk) converges to a unique point
Moreover, the sequence (
when k

Dk,

b

b

.
→ ∞

b

b

Proof. See Appendix D.

Remark 9. As a result we may initialize the scheme (18)
at Ω0 =
. This obviates the need for carefully
selecting an initialization point, which would be needed in
e.g. the Expectation Maximization algorithm.

0, 0, Iny }

{

(26) to obtain

b

ny

V ′(Z) =

Xi=1 (cid:16)k

ξi − (cid:16)

Γ⊤

Φ⊤H

−

zik2 +

wi ⊙

k

zik1

(cid:17)

(cid:17)

ξi = yi −

Φ⊤θi.

(30)

In Appendix E it is shown how the minimum of V ′(Z) can be
found via cyclic minimization with respect to the elements of
Z, similar to what has been done in [36] in a simpler case. This
iterative procedure is implemented using recursively computed
quantities and produces an estimate

Z(t) at sample t.

3) Summary of the algorithm: The algorithm computes
Z(t) recursively by means of the following steps

b

Θ(t) and
at each sample t:
b

b

i) Compute Θ(t) and H(t), using (32)-(34).
ii) Compute

Z(t) via the cyclic minimization of V ′(Z).

Cycle through all elements L times.
Z(t)H⊤(t)

b
Θ(t) = Θ(t)

iii) Compute

−

Θ(0) = 0 and

Z(0) = 0. In
The estimates are initialized as
b
b
practice, small L works well since we cycle L times through
all elements of Z for each new data sample. The computational
details are given in Algorithm 1 in Appendix E, which can be
readily implemented e.g. in MATLAB.

b

b

V. NUMERICAL EXPERIMENTS

In this section we evaluate the proposed method and com-

pare it with two alternative identiﬁcation methods.

A. Identiﬁcation methods and experimental setup

B. Performance metric

The numerical experiments were conducted as follows.
Three methods have been used: LS identiﬁcation of afﬁne
ARX, NARX using wavelet networks (WAVE for short), and the
latent variable method (LAVA) presented in this paper. From
our numerical experiments we found that performing even
only one iteration of the majorization-minimization algorithm
produces good results, and doing so leads to a computationally
efﬁcient recursive implementation (which we denote LAVA-R
for recursive).

For each method the function ϕ(t) is taken as the linear
regressor in (3). Then the dimension of ϕ(t) equals p =
nyna + nunb + 1. For afﬁne ARX, the model is given by

ˆyARX (t) = Θϕ(t),

where Θ is estimated using recursive least squares [27]. Note
that in LAVA-R, Θ is computed as a byproduct (32).

For the wavelet network, nlarx in the System Identiﬁ-
cation Toolbox for Matlab was used, with the number of
nonlinear units automatically detected [17].

{

Ω0 =

Θ,
0, 0, Iny }
b
b

For LAVA-R, the model is given by (6) and

Z are found
. The
by the minimization of (26) using
minimization is performed using the recursive algorithm in
Section IV-B3 with L = 5. The nonlinear function γ(t) of
the data
Dt−1 can be chosen to be a set of basis functions
evaluated at ϕ(t). Then Zγ(t) can be seen as a truncated
basis expansion of some nonlinear function. In the numerical
examples, γ(t) uses the Laplace basis expansion due to its
good approximation properties [28]. Each element
in the
expansion is given by

e

γk1,...,kp−1(t) =

p−1

Yi=1

1
√ℓi

sin

πki(ϕi(t) + ℓi)
2ℓi

,

(cid:19)

(cid:18)

(35)

γ(t) = [γ1,...,1(t)

γp−1,...,p−1(t)]⊤

· · ·
equals q = M p−1, where M is a user parameter which
determines the resolution of the basis.

Finally, an important part of the identiﬁcation setup is the
choice of input signal. For a nonlinear system it is important
to excite the system both in frequency and in amplitude. For
linear models a commonly used input signal is a pseudoran-
dom binary sequence (PRBS), which is a signal that shifts
between two levels in a certain fashion. One reason for using
PRBS is that it has good correlation properties [27]. Hence,
PRBS excites the system well in frequency, but poorly in
amplitude. A remedy to the poor amplitude excitation is to
multiply each interval of constant signal level with a random
factor that is uniformly distributed on some interval [0, A], cf.
[34]. Hence, if the PRBS takes the values
1 and 1, then
the resulting sequence will contain constant intervals with
random amplitudes between
A and A. We denote such a
−
random amplitude sequence RS(A) where A is the maximum
amplitude.

−

5

For the examples considered here the system does not
necessarily belong to the model class, and thus there is no true
parameter vector to compare with the estimated parameters.
Hence, the different methods will instead be evaluated with
respect to the simulated model output ˆys(t). For LAVA-R
ϕ(t) = [ˆy⊤
Θ
ys(t) =
b
and
b

γ(t) is computed as (35) with ϕ(t) replaced by ˆϕ(t).
The performance can then be evaluated using the root mean

s (t
−
ϕ(t) +

na) u⊤(t

· · ·
γ(t)

u⊤(t

s (t

ˆy⊤

· · ·

1)

1)

−

−

−

Z

b

b

b

b

squared error (RMSE) for each output channel i,

b

nb) 1]⊤.

RMSEi = v
u
u
t

1
T

T

Xt=1

E [

yi(t)

k

ˆys,i(t)
k

2
2].

−

The expectations are computed using 100 Monte Carlo simu-
lations on validation data.

it

For the dataset collected from a real system,

is not
possible to evaluate the expectation in the RMSE formula.
For such sets we use the ﬁt of the data, i.e.,
ˆys,ik2
¯yi1

−
where ˆys,i contains the simulated outputs for channel i, ¯yi is
the empirical mean of yi and 1 is a vector of ones. Hence,
FIT compares the simulated output errors with those obtained
using the empirical mean as the model output.

yi −
k
yi −
k

FITi = 100

k2 (cid:19)

1
(cid:18)

,

C. System with saturation

Consider the following state-space model,

x1(t + 1) = sat2[0.9x1(t) + 0.1u1(t)]
x2(t + 1) = 0.08x1(t) + 0.9x2(t) + 0.6u2(t)

y(t) = x(t) + e(t).

⊤ and

x1(t) x2(t)
(cid:3)

(cid:2)

sata(x) =

if
x
sign(x)a if

(cid:26)

< a
a

x
|
x
| ≥

|
|

.

(36)

(37)
(38)

·

A block-diagram for the above system is shown in Fig. 1.
The measurement noise e(t) was chosen as a white Gaussian
process with covariance matrix σI where σ = 2.5

10−3.

Data was collected from the system using an RS(A) input
signal for several different amplitudes A. The identiﬁcation
was performed using na = 1, nb = 1, M = 4, and N =
1000 data samples. This means that p = 5 and q = 256, and
therefore there are 10 parameters in Θ and 512 in Z.

Note that, for sufﬁciently low amplitudes A, x1(t) will be
smaller than the saturation level a = 2 for all t, and thus
the system will behave as a linear system. However, when A
increases, the saturation will affect the system output more and
more. The RMSE was computed for eight different amplitudes
A, and the result is shown in Fig. 2. It can be seen that for
small amplitudes, when the system is effectively linear, the
ARX model gives a marginally better result than LAVA-R and
WAVE. However, as the amplitude is increased, the nonlinear
effects become more important, and LAVA-R outperforms both
WAVE and ARX models.

where ℓi are the boundaries of the inputs and outputs for each
channel and ki = 1, . . . , M are the indices for each element
of γ(t). Then the dimension of

where x(t) =

6

u1(t)

Saturated

u2(t)

Linear

e1(t)

e2(t)

+

+

y1(t)

y2(t)

Fig. 1. A block diagram of the system used in Example V-C.

TABLE I
FIT FOR EXAMPLE V-D.

Upper tank
Lower tank

LAVA-R WAVE
91.6%
90.8%

ARX
79.2% 84.9%
76.9% 78.6%

TABLE II
FIT FOR EXAMPLE V-E.

LAVA-R WAVE
83.2%

ARX
78.2% 73.1%

FIT

1.5

2

2.5

3

3.5

4

4.5

1.5

2

2.5

3

3.5

4

4.5

Maximum amplitude, A

Fig. 2. The RMSE for Example (V-C) computed for different input ampli-
tudes, using LAVA-R (solid), afﬁne ARX (dashed) and WAVE (dash-dotted).

D. Water tank

In this example a real cascade tank process is studied. It
consists of two tanks mounted on top of each other, with
free outlets. The top tank is fed with water by a pump. The
input signal is the voltage applied to the pump, and the output
consists of the water level in the two tanks. The setup is
described in more detail in [34]. The data set consists of 2500
samples collected every ﬁve seconds. The ﬁrst 1250 samples
where used for identiﬁcation, and the last 1250 samples for
validation.

and 1458 parameters in Z. LAVA-R found a model with only
37 nonzero parameters in Z, and the simulated output together
with the measured output are shown in Fig. 3. The FIT values,
computed on the validation data are shown in Table I. It can be
seen that an afﬁne ARX model gives a good ﬁt, but also that
using LAVA-R the FIT measure can be improved signiﬁcantly.
In this example, WAVE did not perform very well.

E. Pick-and-place machine

In the ﬁnal example, a real pick-and-place machine is stud-
ied. This machine is used to place electronic components on
a circuit board, and is described in detail in [13]. This system
exhibits saturation, different modes, and other nonlinearities.
The data used here are from a real physical process, and were
also used in e.g. [3], [14], [20]. The data set consists of a 15s
recording of the single input u(t) and the vertical position of
the mounting head y(t). The data was sampled at 50 Hz, and
the ﬁrst 8s (N = 400) were used for identiﬁcation and the last
7s for validation.

The identiﬁcation was performed using na = 2, nb = 2 and
M = 6. For the SISO system considered here, this results in
5 parameters in Θ and 1296 parameters in Z. LAVA-R found
a model with 33 of the parameters in Z being nonzero, the
output of which is shown in Fig. 4.

25

20

15

10

t
u
p
t
u
O

5

0

0

0

500

1000

1500

2000

2500

5

10

15

Time [s]

0

500

2000

2500

1000
Number of samples

1500

Fig. 3. The output in Example V-D (blue), plotted together with the output
of the model identiﬁed by LAVA-R (red). The system was identiﬁed using the
ﬁrst 1250 data samples. The validation set consisted of the remaining 1250
samples.

Fig. 4. The output in Example V-E (blue), plotted together with the output
of the model identiﬁed by LAVA-R (red). The system was identiﬁed using
the ﬁrst part of the data, while the validation set consisted of the remaining
samples indicated after the dashed line.

The FIT values, computed on the validation data, for LAVA-
R, WAVE and afﬁne ARX are shown in Table II. LAVA-R
outperforms NARX using wavelet networks, and both are better
than ARX.

VI. CONCLUSION

The identiﬁcation was performed using na = 2, nb = 2 and
M = 3. With two outputs, this results in 14 parameters in Θ

We have developed a method for learning nonlinear systems
with multiple outputs and inputs. We began by modelling the

0.5

0.4

0.2

0.1

1

0.3

E
S
M
R

0

1

0.25

0.2

0.15

0.1

2

E
S
M
R

0.05

1

k
n
a
t
 
r
e
p
p
U

k
n
a
t
 
r
e
w
o
L

8

6

4

2

0

8

6

4

2

0

7

errors of a nominal predictor using a latent variable formu-
lation. The nominal predictor could for instance be a linear
approximation of the system but could also include known
nonlinearities. A learning criterion was derived based on the
principle of maximum likelihood, which obviates the tuning
of regularization parameters. The criterion is then minimized
using a majorization-minimization approach. Speciﬁcally, we
derived a convex user-parameter free formulation, which led
to a computationally efﬁcient recursive algorithm that can be
applied to large datasets as well as online learning problems.
The method introduced in this paper learns parsimonious
predictor models and captures nonlinear system dynamics.
This was illustrated via synthetic as well as real data examples.
As shown in these examples a recursive implementation of
the proposed method was capable of outperforming a batch
method using a NARX model with a wavelet network.

APPENDIX A
DERIVATION OF THE DISTRIBUTIONS (8) AND (14)

We start by computing p(Y

Ω) given in (7). The function

|
Ω, Z) can be found from (1)–(4) and the chain rule:

p(Y

|

p(Y

Ω, Z) =

pε(y(t)

Θϕ(t)

−

|Dt−1, Z),

(39)

|

N

Yt=1

where we have neglected initial conditions [27]. Since

it can be seen that

p(Y

Ω) =

|

1
(2π)N ny

R

exp

1
2 k

y

(cid:18)−

Fθ

2
R−1

k

(cid:19)

−

,

(43)

|
which proves (8). To obtain an expression for p(Z
|
simply insert (42) and (43) into Bayes’ rule to get (14).

p

|

Ω, Y)

APPENDIX B
DERIVATION OF THE MAJORIZING TANGENT PLANE (22)

The ﬁrst-order Taylor expansion of the log-determinant can

be written as

ln

R

|

| ≃

ln

R

|

|
+ (∂d ln
e

+ (∂σ ln
R

R
)
|R=
|
|
eR)(d

eR(σ
˜d)

σ)

−

|R=
where σ is the vector of diagonal elements in Σ and d contains
the diagonal elements in D.

−

|

e

For the derivatives with respect to d we have
R−1 ∂R
(cid:18)

∂
∂di,j

∂di,j (cid:19)

= tr

= tr

R

ln

(cid:18)

|

|

G⊤R−1G

∂D
∂di,j (cid:19)

.

Note that

so Hence

ny

q

Xi=1

Xj=1

di,j

∂D
∂di,j

= D

pε(y(t)

Θϕ(t)

−
1
2 k

exp

(cid:18)−

y(t)

−

|Dt−1, Z)
Θϕ(t)

∝
Zγ(t)
k

−

2
Σ−1

,

(cid:19)

∂d ln

R

|R=
In the same way

|

eR(d

−

˜d) = tr

G⊤

R−1G(D

(cid:16)

e

D)

.

(cid:17)

−

e

it follows that

p(Y

|

Ω, Z) =
1

(2π)nyN

Σ

N

|

|

p

and thus,

exp

1
2 k

Y

−

(cid:18)−

ΘΦ

ZΓ

−

2
Σ−1

.

(cid:19)

k

Using the vectorized variable in (9)-(10) we can see that

vec(ΘΦ) = Fθ and

vec(ZΓ) = Gz.

(40)

Since ln

∂σ ln

R

|
R

|

|
K + tr

ln

R

|

| ≤

where

eR(σ

˜σ) = tr

|R=
−
(cid:16)
is concave in σ and d, it follows that

R−1(IN ⊗
e

(Σ

−

Σ)

.

(cid:17)

e

G⊤

R−1GD

+ tr

(cid:17)

(cid:16)

(cid:16)

e

e

e

K = ln

R

tr

G⊤

R−1G

D

tr

|

| −

(cid:16)

(cid:17) −

e

e

e
APPENDIX C
PROOF OF THEOREM 2

It follows from (21) that

Σ)

(44)

(cid:17)

R−1(IN ⊗
e
R−1(IN ⊗
e

(cid:16)

Σ)

.

(cid:17)

e

Y

ΘΦ

2
2
IN ⊗Σ−1.
Σ−1 =
Next, we note that the following useful equality holds:

Gz
k

ZΓ

Fθ

−

−

−

−

y

k

k

k

y

k

−

Fθ

Gz
k

−

2
IN ⊗Σ−1 +
y

k

z
k
Fθ

2
D−1 =
2
R−1 +

k

−
where R is given by (11), ζ by (15), and Σz by (16). To see
that the equality holds, expand the norms on both sides of (41)
and apply the matrix inversion lemma.
The sought-after distribution p(Y

Ω) is given by (7). By

−

k

k

k

z

ζ

2
−1
Σ
z

(41)

using (41) it follows that

|

where Ri = Γ⊤DiΓ + σiIN . Hence,

R =

(Ri ⊗

Ei,i)

ny

Xi=1

ny

R−1 =

R−1

i ⊗

Ei,i

.

(cid:1)

Xi=1 (cid:0)

Thus, we can rewrite (25) as (to within an additive constant):

p(Y

|

Ω, Z)p(Z)
1
2 k

exp(

−

∝
y

Fθ

2
R−1) exp(

1
2 k

z

−
with the normalization constant ((2π)ny (N +q)
Noting that

−

−

k

ζ

k
Σ

|

|

2
−1
Σ
z

).

(42)

N

D
|

|

)−1/2.

V ′(Ω

Z,

Ω) =

|

e

ny

Xi=1(cid:16)

1
σi k

exp

1
2 k

z

(cid:18)−

Z

ζ

2
Σ

k

−1
z (cid:19)

−

dZ =

(2π)nyq

Σz|

|

p

where ¯yi = yi −

Φ⊤θi −

zik

k

2
−1
D
i

+

2
2 +

¯yik
R−1
i

e
Γ⊤zi.

σi tr(

) + tr(Γ

R−1

i Γ⊤Di)
(cid:17)

.

(45)

e

8

We next derive analytical expressions for the Σ and D that

minimize V ′(Ω

Z,

|
V ′(Ω

∂
∂σi

Ω). Note that
1
σ2
i k

e
Z,
|

Ω) =

2
2 + tr(

R−1
i

),

¯yik

−
and setting the derivative to zero yields the estimate (29). In the
same way it can be seen that the minimum of di,j is attained
at (29).

e

e

Inserting ˆσi and ˆdi,j into (45), we see that we can ﬁnd the

minimizing Θ and Z by minimizing

APPENDIX E
DERIVATION OF THE PROPOSED RECURSIVE ALGORITHM
In order to minimize V ′(Z) we use a cyclic algorithm. That
is, we minimize with respect to one component at a time.
We follow an approach similar to that in [36], with the main
difference being that here we consider arbitrary nonnegative
weights wi.

Note that minimization of V ′(Z) with respect to zi,j is

equivalent to minimizing

tr(

R−1
i

)
k

yi −

Φ⊤θi −

Γ⊤zik2+

where

V ′(zi,j) =

˜ξi,j −

k

cj zi,jk2 + wi,j |

zi,j|

ny

2

Xi=1(cid:16)q
q

e
γ⊤
zi,j|q
j

|

Xj=1

R−1

i γj

.
(cid:17)

e

Since term i in the above sum is invariant with respect to θk
and zk for k
= i, we can divide term i by 2
), and
see that minimizing the criterion above is equivalent to (26).

R−1
i

tr(

q

e

APPENDIX D
PROOF OF THEOREM 3

0, 0, Iny }

Initializing (18) at Ω0 =

Θ0, 0, Σ0}
where Σ0 = diag(σ(0)
ny ), produces two sequences
denoted Ωk =
and Ωk =
for
k > 0, respectively. This results also in sequences Zk and Zk.
The theorem states that:

{
1 , . . . , σ(0)
Θk, Dk, Σk}

{
Θk, Dk, Σk}

and Ω0 =

{

{

Θk = Θk
Dk →

and Zk = Zk

(46)

Dk −

0 and Σk −
We now show the stronger result that the covariance matrices
converge as

Σk →

(47)

0.

,

i σ(k)

(k)
i = c(k)
i D
i
i = (σ(0)

i = c(k)
σ(k)
2k . Note that c(k)

D(k)
where c(k)
. Hence
(48) implies (47). We prove (48) and (46) by induction. That
(48) and (46) holds for k = 1 follows directly from Theorem
2. Now assume that (48) holds for some k

∀
1 as k

i →

k > 0,

1. Let

→ ∞

(48)

)

,

i

i

1

(k)

Ri = ΓD
Ri = ΓD(k)

i Γ⊤ + σ(k)
i Γ⊤ + σ(k)

i

i IN ,

IN = c(k)

i Ri,

≥

e

where the last equality follows by the assumption in (48).
Therefore the weights used to estimate Θk+1 and Zk+1 are
the same as those used to estimate Θk+1, Zk+1:

wi,j = v
u
u
t

R−1
γ⊤
i γj
j
R−1
)
tr(
e
i

γ⊤

−1
j R
i γj
−1
tr(R
)
i

,

= v
u
u
t

so we can conclude that Θk+1 = Θk+1 and Zk+1 = Zk+1.
The estimate Dk+1 is given by

e

d(k+1)
i,j

=

zi,j|
R−1

|
γ⊤
j

i γj

q

c(k)
i

|
γ⊤R

zi,j|
γ

−1

= q

q

= c(k+1)
i

(k+1)
i,j

d

(k+1)
e
= c(k+1)
so D(k+1)
D
i
i
i
= c(k+1)
seen that σ(k+1)
i
(46) are true for all k > 0 and Theorem 3 follows.

, and in the same way it can be
. Hence by induction (48) and

σ(k+1)
i

i

˜ξi,j = ξi − Xk6=j

ckzi,k,

cj = [Γ⊤

Φ⊤H]j.

−

As in [36] it can be shown that the sign of the optimal ˆzi,j
˜ξi,j). Hence we only have to
is given by sign(ˆzi,j) = sign(c⊤
j
ﬁnd the absolute value ri,j =
zi,j|
cjk
k
It is then straightforward to verify that the minimization of
V ′(zi,j) is equivalent to minimizing
V ′(ri,j ) = (αi,j + βjr2

2gi,jri,j )1/2 + wi,jri,j ,

gi,j = c⊤
j

˜ξi,jk

αi,j =

˜ξi,j.

βj =

. Let

2
2,

2
2,

k

|

i,j −

over all ri,j ≥
the Cauchy-Schwartz inequality it follows that

0, and then setting ˆzi,j = sign(gi,j)ˆri,j . From

αi,jβi,j ≥

g2
i,j.

Using this inequality it was shown in [36] that V ′(ri,j ) is
a convex function. The derivative of V ′(ri,j ) is given by
(dropping the subindices for now),

dV ′
dr

=

βr
g
r + α)1/2 + w.
− |
g
2
|
|
Since V ′(r) is convex it follows that it is minimized by r = 0
if and only if dV ′(0)/dr

0, i.e., if and only if

(βr2

(49)

−

|

≥
αw2

g2.

(50)

(51)

≥
Next we study the case when g2 > αw2. It then follows from
(49) that the stationary points of V ′(r) satisfy

(βr

g

) =

− |

|

−

w(βr2

2

g

|

|

−

r + α)1/2

Solving this equation for r we get the stationary point

ˆr = |

g
|
β −

w

β

β

w2

p

αβ

g2.

−

−
Hence we can conclude that the minimizer of V ′(zi,j) is given
by

p

ˆzi,j =

(cid:26)

sign(gi,j )ˆri,j
0

if αi,j w2
otherwise

i,j < g2
i,j

.

Φ⊤H)⊤(Γ⊤

Next we show how to obtain this estimate using only recur-
sively computed quantities. Let
T = (Γ⊤
ξik
κi =
k
ρi = (Γ⊤
−
ξi −
ηi =
k
ζi = (Γ⊤
−

Φ⊤H)⊤ξi
(Γ⊤
−
Φ⊤H)(ξi −

Φ⊤H)zik
2
2
(Γ⊤

Φ⊤H)zi)

Φ⊤H)

−
2
2

(54)

(53)

(55)

(52)

(56)

−

−

Then it is straightforward to show that

i,j + 2ζi,jzi,j

αi,j = ηi + βjz2
βj = Tj,j
gi,j = ζi,j + βjzi,j.

Also deﬁne Ψa,b(t) recursively, for any two vector-valued
signals a(t), b(t), as

Ψa,b(0) = 0

Ψa,b(t + 1) = Ψa,b(t) + a(t)b⊤(t).

(57)

(58)

Note that Ψa,b(t) = (Ψb,a(t))⊤. It can be veriﬁed that
all quantities (52)–(56), and thus ˆzi,j, can be coputed from
Ψ·,·(N ).

The full algorithm for updating the needed quantities, in-
cluding the update of Θ and H, is summarized in Algorithm
1. Note that the iterations of the outer for-loop can be executed
in parallel.

Algorithm 1 : Recursive solution to (26)
1: Input: y(t), ϕ(t), γ(t), ˇΘ and ˇZ
2: Update P(t), Θ(t) and H(t) according to (32)-(34).
3: Update Ψϕ,ϕ(t), Ψγ,γ(t), Ψy,y(t), Ψϕ,γ(t), Ψϕ,y(t)

and Ψγ,y(t) according to (58) .
4: T = Ψγ,γ
Ψγ,ϕH
−
5: for i = 1, . . . , ny do
⊤
κ = Ψy,y
i,i + ˜θ
6:
ρ = [Ψγ,y]i −
2ρ⊤ˇzi + ˇz⊤
η = κ
−
Tˇzi
ζ = ρ
−
repeat

−
i Ψϕ,ϕ˜θi −
Ψγ,ϕ˜θi −
i Tˇzi.

8:
9:

7:

H⊤Ψϕ,γ + H⊤Ψϕ,ϕH.

⊤

i [Ψϕ,y]i.

2˜θ
H⊤[Ψϕ,γ]i + H⊤Ψϕ,ϕ˜θi.

10:

11:
12:

13:

14:

15:

16:

17:

18:

19:
20:

for j = 1, . . . , q do
α = η + Tj,j ˇz2
g = ζj + Tj,j ˇzi,j.
β = Tj,j.
ˆr = |g|

i,j + 2ζj ˇzi,j.

i,j p

β −

wi,j
β√β−w2
sign(g)ˆr
ˆzi,j =
0
η := η + Tj,j (ˇzi,j −
ζ := ζ + [T]j(ˇzi,j −
ˇzi,j := ˆzi,j.

(cid:26)

g2.
αβ
−
if αw2
i,j < g2
otherwise
ˆzi,j) + 2(ˇzi,j −
ˆzi,j).

ˆzi,j)ζj.

until number of iterations equals L.

21:
22: end for
Z = ˇZ.
23:
Θ = Θ
24:
b
25: Output:
b

end for

ZH.
−
Z.
Θ,
b
b

b

REFERENCES

[1] E. W. Bai and Y. Liu. Recursive direct weight optimization in nonlinear
Automatic

system identiﬁcation: A minimal probability approach.
Control, IEEE Transactions on, 52(7):1218–1231, 2007.

[2] D. Barber. Bayesian reasoning and machine learning. Cambridge

University Press, 2012.

9

[4] H. Bijl, J. W. van Wingerden, T. Sch¨on, and M. Verhaegen. Online
sparse gaussian process regression using ﬁtc and pitc approximations.
IFAC-PapersOnLine, 48(28):703–708, 2015.

[5] S. Billings. Nonlinear System Identiﬁcation: NARMAX Methods in the

Time, Frequency, and Spatio-Temporal Domains. Wiley, 2013.

[6] C. M. Bishop. Pattern Recognition and Machine Learning. Springer,

2006.

[7] G. E. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung. Time Series

Analysis: Forecasting and Control. John Wiley & Sons, 2015.

[8] H. Chen. Extended recursive least squares algorithm for nonlinear
stochastic systems. In American Control Conference, 2004. Proceedings
of the 2004, volume 5, pages 4758–4763. IEEE, 2004.

[9] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood
the royal

from incomplete data via the em algorithm.
statistical society. Series B (methodological), pages 1–38, 1977.
[10] M. Enqvist. Linear models of nonlinear systems. PhD thesis, Link¨oping

Journal of

University, Link¨oping, Sweden, 2005.

[11] F. Giri and E. W. Bai. Block-oriented nonlinear system identiﬁcation,

[12] D. R. Hunter and K. Lange. A tutorial on MM algorithms. The American

volume 1. Springer, 2010.

Statistician, 58(1):30–37, 2004.

[13] A. Juloski, W. Heemels, and G. Ferrari-Trecate. Data-based hybrid mod-
elling of the component placement process in pick-and-place machines.
Control Engineering Practice, 12(10):1241–1252, 2004.

[14] A. L. Juloski, S. Paoletti, and J. Roll. Recent

techniques for the
identiﬁcation of piecewise afﬁne and hybrid systems.
In L. Menini,
L. Zaccarian, and C. Abdallah, editors, Current Trends in Nonlinear
Systems and Control, pages 79–99. Birkh¨auser Boston, 2006.

[15] L. Ljung. System Identiﬁcation: Theory for the User. Pearson Education,

1998.

[16] L. Ljung. Model validation and model error modeling. In The ˚Astr¨om

Symposium on Control, pages 15–42. Studentlitteratur, 1999.

[17] L. Ljung. System identiﬁcation toolbox for use with MATLAB. The

MathWorks, Inc., 2007.
[18] L. Ljung and T. S¨oderstr¨om.

Theory and Practice of Recursive

Identiﬁcation. MIT Press, Cambridge, MA, 1983.

[19] P. Mattsson and T. Wigren. Convergence analysis for recursive Ham-

merstein identiﬁcation. Automatica, 71:179–186, 2016.

[20] H. Ohlsson and L. Ljung. Identiﬁcation of switched linear regression
models using sum-of-norms regularization. Automatica, 49(4):1045–
1050, 2013.

[21] J. Paduart, L. Lauwers, J. Swevers, K. Smolders, Johan Schoukens,
and Rik Pintelon. Identiﬁcation of nonlinear systems using polynomial
nonlinear state space models. Automatica, 46(4):647 – 656, 2010.
[22] G. Pillonetto. Consistent identiﬁcation of wiener systems: A machine

learning viewpoint. Automatica, 49(9):2704–2712, 2013.

[23] G. Pillonetto, F. Dinuzzo, T. Chen, G. De Nicolao, and Lennart Ljung.
Kernel methods in system identiﬁcation, machine learning and function
estimation: A survey. Automatica, 50(3):657–682, 2014.

[24] J. Roll, A. Nazin, and L. Ljung. Nonlinear system identiﬁcation via

direct weight optimization. Automatica, 41(3):475–490, 2005.

[25] J. Schoukens, M. Vaes, and R. Pintelon. Linear system identiﬁcation in
a nonlinear setting: Nonparametric analysis of the nonlinear distortions
and their impact on the best linear approximation. IEEE Control Systems,
36(3):38–69, 2016.

[26] J. Sj¨oberg, Q. Zhang, L. Ljung, A. Benveniste, Bernard Delyon, Pierre-
Yves Glorennec, H˚akan Hjalmarsson, and Anatoli Juditsky. Nonlinear
black-box modeling in system identiﬁcation: a uniﬁed overview. Auto-
matica, 31(12):1691 – 1724, 1995.

[27] T. S¨oderstr¨om and P. Stoica. System identiﬁcation. Prentice-Hall, Inc.,

1988.

[28] A. Solin and S. S¨arkk¨a. Hilbert space methods for reduced-rank
Gaussian process regression, 2014. arXiv preprint arXiv:1401.5508.

[29] P. Stoica and P.

˚Ahgren. Exact initialization of the recursive least-
squares algorithm. International Journal of Adaptive Control and Signal
Processing, 16(3):219–230, 2002.

[30] P. Stoica, D. Zachariah, and J. Li. Weighted SPICE: A unifying approach
for hyperparameter-free sparse estimation. Digital Signal Processing,
33:1–12, 2014.

[31] S. Tayamon, T. Wigren, and J. Schoukens. Convergence analysis and
experiments using an RPEM based on nonlinear ODEs and midpoint
In Decision and Control (CDC), 2012 IEEE 51st Annual
integration.
Conference on, pages 2858–2865. IEEE, 2012.

[3] A. Bemporad, A. Garulli, S. Paoletti, and A. Vicino. A bounded-
IEEE Trans.

error approach to piecewise afﬁne system identiﬁcation.
Automatic Control, 50(10):1567–1580, Oct 2005.

[32] P. Van den Hof and B. Ninness. System identiﬁcation with generalized
In Modelling and Identiﬁcation with

orthonormal basis functions.
Rational Orthogonal Basis Functions, pages 61–102. Springer, 2005.

[33] T. Wigren. Recursive prediction error identiﬁcation using the nonlinear

Wiener model. Automatica, 29(4):1011 – 1025, 1993.

[34] T. Wigren. Recursive prediction error identiﬁcation and scaling of non-
linear state space models using a restricted black box parameterization.
Automatica, 42(1):159 – 168, 2006.

[35] T. T. Wu and K. Lange. The mm alternative to em. Statistical Science,

[36] D. Zachariah and P. Stoica. Online hyperparameter-free sparse estima-
IEEE Trans. Signal Processing, 63(13):3348–3359, July

25(4):492–505, 2010.

tion method.
2015.

10

