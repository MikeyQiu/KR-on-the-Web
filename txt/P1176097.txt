Top-down Tree Long Short-Term Memory Networks

Xingxing Zhang, Liang Lu and Mirella Lapata
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
{x.zhang,liang.lu}@ed.ac.uk,mlap@inf.ed.ac.uk

6
1
0
2
 
r
p
A
 
3
 
 
]
L
C
.
s
c
[
 
 
3
v
0
6
0
0
0
.
1
1
5
1
:
v
i
X
r
a

Abstract

Long Short-Term Memory (LSTM) networks,
a type of recurrent neural network with a
more complex computational unit, have been
successfully applied to a variety of sequence
modeling tasks. In this paper we develop Tree
Long Short-Term Memory (TREELSTM), a
neural network model based on LSTM, which
is designed to predict a tree rather than a lin-
ear sequence. TREELSTM deﬁnes the prob-
ability of a sentence by estimating the gener-
ation probability of its dependency tree. At
each time step, a node is generated based
on the representation of the generated sub-
tree. We further enhance the modeling power
of TREELSTM by explicitly representing the
correlations between left and right depen-
dents. Application of our model to the MSR
sentence completion challenge achieves re-
sults beyond the current state of the art. We
also report results on dependency parsing
reranking achieving competitive performance.

1

Introduction

Neural language models have been gaining increas-
ing attention as a competitive alternative to n-grams.
The main idea is to represent each word using a
real-valued feature vector capturing the contexts in
which it occurs. The conditional probability of the
next word is then modeled as a smooth function of
the feature vectors of the preceding words and the
In essence, similar representations are
next word.
learned for words found in similar contexts result-
ing in similar predictions for the next word. Previ-
ous approaches have mainly employed feed-forward

(Bengio et al., 2003; Mnih and Hinton, 2007) and
recurrent neural networks (Mikolov et al., 2010;
Mikolov, 2012) in order to map the feature vec-
tors of the context words to the distribution for the
next word. Recently, RNNs with Long Short-Term
Memory (LSTM) units (Hochreiter and Schmidhu-
ber, 1997; Hochreiter, 1998) have emerged as a pop-
ular architecture due to their strong ability to capture
long-term dependencies. LSTMs have been success-
fully applied to a variety of tasks ranging from ma-
chine translation (Sutskever et al., 2014), to speech
recognition (Graves et al., 2013), and image descrip-
tion generation (Vinyals et al., 2015).

Despite superior performance in many applica-
tions, neural language models essentially predict se-
quences of words. Many NLP tasks, however, ex-
ploit syntactic information operating over tree struc-
tures (e.g., dependency or constituent trees). In this
paper we develop a novel neural network model
which combines the advantages of the LSTM archi-
tecture and syntactic structure. Our model estimates
the probability of a sentence by estimating the gen-
eration probability of its dependency tree. Instead
of explicitly encoding tree structure as a set of fea-
tures, we use four LSTM networks to model four
types of dependency edges which altogether specify
how the tree is built. At each time step, one LSTM is
activated which predicts the next word conditioned
on the sub-tree generated so far. To learn the repre-
sentations of the conditioned sub-tree, we force the
four LSTMs to share their hidden layers. Our model
is also capable of generating trees just by sampling
from a trained model and can be seamlessly inte-
grated with text generation applications.

Our approach is related to but ultimately differ-
ent from recursive neural networks (Pollack, 1990)
a class of models which operate on structured in-
puts. Given a (binary) parse tree, they recursively
generate parent representations in a bottom-up fash-
ion, by combining tokens to produce representations
for phrases, and eventually the whole sentence. The
learned representations can be then used in classi-
ﬁcation tasks such as sentiment analysis (Socher et
al., 2011b) and paraphrase detection (Socher et al.,
2011a). Tai et al. (2015) learn distributed representa-
tions over syntactic trees by generalizing the LSTM
architecture to tree-structured network topologies.
The key feature of our model is not so much that
it can learn semantic representations of phrases or
sentences, but its ability to predict tree structure and
estimate its probability.

Syntactic language models have a long history
in NLP dating back to Chelba and Jelinek (2000)
(see also Roark (2001) and Charniak (2001)). These
models differ in how grammar structures in a parsing
tree are used when predicting the next word. Other
work develops dependency-based language models
for speciﬁc applications such as machine translation
(Shen et al., 2008; Zhang, 2009; Sennrich, 2015),
speech recognition (Chelba et al., 1997) or sentence
completion (Gubbins and Vlachos, 2013). All in-
stances of these models apply Markov assumptions
on the dependency tree, and adopt standard n-gram
smoothing methods for reliable parameter estima-
tion. Emami et al. (2003) and Sennrich (2015) esti-
mate the parameters of a structured language model
using feed-forward neural networks (Bengio et al.,
2003). Mirowski and Vlachos (2015) re-implement
the model of Gubbins and Vlachos (2013) with
RNNs. They view sentences as sequences of words
over a tree. While they ignore the tree structures
themselves, we model them explicitly.

Our model shares with other structured-based lan-
guage models the ability to take dependency infor-
mation into account. It differs in the following re-
spects: (a) it does not artiﬁcially restrict the depth
of the dependencies it considers and can thus be
viewed as an inﬁnite order dependency language
model; (b) it not only estimates the probability of a
string but is also capable of generating dependency
trees; (c) ﬁnally, contrary to previous dependency-
based language models which encode syntactic in-

formation as features, our model takes tree structure
into account more directly via representing different
types of dependency edges explicitly using LSTMs.
Therefore, there is no need to manually determine
which dependency tree features should be used or
how large the feature embeddings should be.

We evaluate our model on the MSR sentence com-
pletion challenge, a benchmark language modeling
dataset. Our results outperform the best published
results on this dataset. Since our model is a general
tree estimator, we also use it to rerank the top K de-
pendency trees from the (second order) MSTPasrser
and obtain performance on par with recently pro-
posed dependency parsers.

2 Tree Long Short-Term Memory

Networks

We seek to estimate the probability of a sentence by
estimating the generation probability of its depen-
dency tree. Syntactic information in our model is
represented in the form of dependency paths. In the
following, we ﬁrst describe our deﬁnition of depen-
dency path and based on it explain how the proba-
bility of a sentence is estimated.

2.1 Dependency Path

Generally speaking, a dependency path is the path
between ROOT and w consisting of the nodes on
the path and the edges connecting them. To rep-
resent dependency paths, we introduce four types
of edges which essentially deﬁne the “shape” of a
dependency tree. Let w0 denote a node in a tree
and w1, w2, . . . , wn its left dependents. As shown in
Figure 1, LEFT edge is the edge between w0 and
its ﬁrst left dependent denoted as (w0, w1). Let wk
(with 1 < k ≤ n) denote a non-ﬁrst left dependent
of w0. The edge from wk−1 to wk is a NX-LEFT
edge (NX stands for NEXT), where wk−1 is the right
adjacent sibling of wk. Note that the NX-LEFT edge
(wk−1, wk) replaces edge (w0, wk) (illustrated with a
dashed line in Figure 1) in the original dependency
tree. The modiﬁcation allows information to ﬂow
from w0 to wk through w1, . . . , wk−1 rather than di-
rectly from w0 to wk. RIGHT and NX-RIGHT edges
are deﬁned analogously for right dependents.

Given these four types of edges, dependency
paths (denoted as D(w)) can be deﬁned as follows

w0

wn

wk

wk−1

w1

LEFT

NX-LEFT

Figure 1: LEFT and NX-LEFT edges. Dotted line between
w1 and wk−1 (also between wk and wn) indicate that there may
be ≥ 0 nodes inbetween.

bearing in mind that the ﬁrst right dependent of
ROOT is its only dependent and that wp denotes the
parent of w. We use (. . . ) to denote a sequence,
where () is an empty sequence and (cid:107) is an operator
for concatenating two sequences.

(1) if w is ROOT, then D(w) = ()
(2) if w is a left dependent of wp
(a) if w is the ﬁrst

left dependent,

then

D(w) = D(wp)(cid:107)((cid:104)wp, LEFT(cid:105))

(b) if w is not the ﬁrst left dependent and ws is

its right adjacent sibling, then
D(w) = D(ws)(cid:107)((cid:104)ws, NX-LEFT(cid:105))

(3) if w is a right dependent of wp

(a) if w is the ﬁrst right dependent,
D(w) = D(wp)(cid:107)((cid:104)wp, RIGHT(cid:105))

then

(b) if w is not the ﬁrst right dependent and ws

is its left adjacent sibling, then
D(w) = D(ws)(cid:107)((cid:104)ws, NX-RIGHT(cid:105))

A dependency tree can be represented by the set of
its dependency paths which in turn can be used to
reconstruct the original tree.1

for

the ﬁrst

the moment

Dependency paths

two levels
the tree in Figure 2 are as follows (ig-
of
the subscripts which
noring for
D(sold) =
we explain in the next section).
((cid:104)ROOT, RIGHT(cid:105)) (see deﬁnitions (1) and (3a)),
D(year) = D(sold)(cid:107)((cid:104)sold, LEFT(cid:105))
(2a)),
D(manufacturer) = D(year)(cid:107)((cid:104)year, NX-LEFT(cid:105))
(see (2b)), D(cars) = D(sold)(cid:107)((cid:104)sold, RIGHT(cid:105))
(see (3a)), D(in) = D(cars)(cid:107)((cid:104)cars, NX-RIGHT(cid:105))
(according to (3b)).

(see

2.2 Tree Probability

ROOT

sold1

manufacturer3

year2

cars4

in5

The9

luxury8 auto7

last6

1,21410

U.S.11

the12

Figure 2: Dependency tree of the sentence The luxury auto

manufacturer last year sold 1,214 cars in the U.S. Subscripts

indicate breadth-ﬁrst traversal. ROOT has only one dependent

(i.e., sold ) which we view as its ﬁrst right dependent.

its corresponding tree T , P(S|T ). We view the prob-
ability computation of a dependency tree as a gener-
ation process. Speciﬁcally, we assume dependency
trees are constructed top-down, in a breadth-ﬁrst
manner. Generation starts at the ROOT node. For
each node at each level, ﬁrst its left dependents are
generated from closest to farthest and then the right
dependents (again from closest to farthest). The
same process is applied to the next node at the same
level or a node at the next level. Figure 2 shows the
breadth-ﬁrst traversal of a dependency tree.

Under the assumption that each word w in a de-
pendency tree is only conditioned on its dependency
path, the probability of a sentence S given its depen-
dency tree T is:

P(S|T ) = ∏

P(w|D(w))

(1)

w∈BFS(T )\ROOT

where D(w) is the dependency path of w. Note that
each word w is visited according to its breadth-ﬁrst
search order (BFS(T)) and the probability of ROOT
is ignored since every tree has one. The role of
ROOT in a dependency tree is the same as the begin
of sentence token (BOS) in a sentence. When com-
puting P(S|T ) (or P(S)), the probability of ROOT (or
BOS) is ignored (we assume it always exists), but is
used to predict other words. We explain in the next
section how TREELSTM estimates P(w|D(w)).

The core problem in syntax-based language model-
ing is to estimate the probability of sentence S given

2.3 Tree LSTMs

1Throughout this paper we assume all dependency trees are

projective.

A dependency path D(w) is subtree which we de-
note as a sequence of (cid:104)word, edge-type(cid:105) tuples. Our

w0

w0

w3

w2

w1

w4

w5

w6

Generated by four LSTMs

with tied We and tied Who

w2

w1

w0

G

E

N

-

L

G E N-R

w0

w4

w5

GEN-NX-L

GEN-NX-L

GEN-NX-R

GEN-NX-R

w2

w1

w3
w6
Figure 3: Generation process of left (w1, w2, w3) and right
(w4, w5, w6) dependents of tree node wo (top) using four LSTMs
(GEN-L, GEN-R, GEN-NX-L and GEN-NX-R). The model can

w4

w5

handle an arbitrary number of dependents due to GEN-NX-L

and GEN-NX-R.

innovation is to learn the representation of D(w) us-
ing four LSTMs. The four LSTMs (GEN-L, GEN-
R, GEN-NX-L and GEN-NX-R) are used to repre-
sent the four types of edges (LEFT, RIGHT, NX-
LEFT and NX-RIGHT) introduced earlier. GEN,
NX, L and R are shorthands for GENERATE, NEXT,
LEFT and RIGHT. At each time step, an LSTM is
chosen according to an edge-type; then the LSTM
takes a word as input and predicts/generates its de-
pendent or sibling. This process can be also viewed
as adding an edge and a node to a tree. Speciﬁ-
cally, LSTMs GEN-L and GEN-R are used to gen-
erate the ﬁrst left and right dependent of a node
(w1 and w4 in Figure 3). So, these two LSTMs
are responsible for going deeper in a tree. While
GEN-NX-L and GEN-NX-R generate the remain-
ing left/right dependents and therefore go wider in
a tree. As shown in Figure 3, w2 and w3 are gener-
ated by GEN-NX-L, whereas w5 and w6 are gener-
ated by GEN-NX-R. Note that the model can handle
any number of left or right dependents by applying
GEN-NX-L or GEN-NX-R multiple times.

We assume time steps correspond to the steps
taken by the breadth-ﬁrst traversal of the depen-
dency tree and the sentence has length n. At
time step t (1 ≤ t ≤ n), let (cid:104)wt(cid:48), zt(cid:105) denote the last
Subscripts t and t(cid:48) denote the
tuple in D(wt).
breadth-ﬁrst search order of wt and wt(cid:48), respectively.
zt ∈ {LEFT, RIGHT, NX-LEFT, NX-RIGHT} is the
edge type (see the deﬁnitions in Section 2.1). Let
We ∈ Rs×|V | denote the word embedding matrix and

Who ∈ R|V |×d the output matrix of our model, where
|V | is the vocabulary size, s the word embedding size
and d the hidden unit size. We use tied We and tied
Who for the four LSTMs to reduce the number of pa-
rameters in our model. The four LSTMs also share
their hidden states. Let H ∈ Rd×(n+1) denote the
shared hidden states of all time steps and e(wt) the
one-hot vector of wt. Then, H[:,t] represents D(wt)
at time step t, and the computation2 is:

xt = We · e(wt(cid:48))
ht = LSTMzt (xt, H[:,t(cid:48)])

H[:,t] = ht

yt = Who · ht

(2a)

(2b)

(2c)

(2d)

where the initial hidden state H[:, 0] is initialized to
a vector of small values such as 0.01. According to
Equation (2b), the model selects an LSTM based on
edge type zt. We describe the details of LSTMzt in
the next paragraph. The probability of wt given its
dependency path D(wt) is estimated by a softmax
function:

P(wt|D(wt)) =

(3)

exp(yt,wt )
|V |
k(cid:48)=1 exp(yt,k(cid:48))

∑

We must point out that although we use four jointly
trained LSTMs to encode the hidden states, the train-
ing and inference complexity of our model is no dif-
ferent from a regular LSTM, since at each time step
only one LSTM is working.

We implement LSTMz in Equation (2b) using a
deep LSTM (to simplify notation, from now on we
write z instead of zt). The inputs at time step t
are xt and ht(cid:48) (the hidden state of an earlier time
step t(cid:48)) and the output is ht (the hidden state of cur-
rent time step). Let L denote the layer number of
LSTMz and ˆhl
t the internal hidden state of the l-th
layer of the LSTMz at time step t, where xt is ˆh0
t and
ht(cid:48) is ˆhL
t(cid:48). The LSTM architecture introduces mul-
tiplicative gates and memory cells ˆcl
t (at l-th layer)
in order to address the vanishing gradient problem
which makes it difﬁcult for the standard RNN model
to learn long-distance correlations in a sequence.
Here, ˆcl
t is a linear combination of the current input
signal ut and an earlier memory cell ˆcl
t(cid:48). How much
input information ut will ﬂow into ˆcl
t is controlled

2We ignore all bias terms for notational simplicity.

w2

w1

w0

G

E

N

-

L

G E N-R

w0

w4

w5

GEN-NX-L

GEN-NX-L

GEN-NX-R

GEN-NX-R

LD

LD

w0

w4

w5

w6

w3

w2

w1

Figure 4: Generation of left and right dependents of node w0
according to LDTREELSTM.

by input gate it and how much of the earlier mem-
ory cell ˆcl
t(cid:48) will be forgotten is controlled by forget
gate ft. This process is computed as follows:

ux · ˆhl−1

ut = tanh(Wz,l
it = σ(Wz,l
ft = σ(Wz,l
t = ft (cid:12) ˆcl
ˆcl

ix · ˆhl−1
f x · ˆhl−1
t(cid:48) + it (cid:12) ut

t + Wz,l
uh · ˆhl
t(cid:48))
ih · ˆhl
t(cid:48))
f h · ˆhl
t(cid:48))

t + Wz,l
t + Wz,l

(4a)

(4b)

(4c)

(4d)

ux ∈ Rd×d (Wz,l

uh ∈ Rd×d are weight matrices for ut, Wz,l
ih are weight matrices for it and Wz,l

where Wz,l
ux ∈ Rd×s when l = 1) and
Wz,l
ix and
f x, and Wz,l
Wz,l
f h
are weight matrices for ft. σ is a sigmoid function
and (cid:12) the element-wise product.

Output gate ot controls how much information of

the cell ˆcl

t can be seen by other modules:

t + Wz,l
ox · ˆhl−1
ot = σ(Wz,l
ˆhl
t = ot (cid:12) tanh(ˆcl
t )

oh · ˆhl
t(cid:48))

(5a)

(5b)

Application of the above process to all layers L, will
yield ˆhL
t , which is ht. Note that in implementation,
all ˆcl
t (1 ≤ l ≤ L) at time step t are stored,
although we only care about ˆhL
t (ht).

t and ˆhl

2.4 Left Dependent Tree LSTMs

TREELSTM computes P(w|D(w)) based on the de-
pendency path D(w), which ignores the interaction
between left and right dependents on the same level.
In many cases, TREELSTM will use a verb to pre-
dict its object directly without knowing its subject.
For example, in Figure 2, TREELSTM uses (cid:104)ROOT,
RIGHT(cid:105) and (cid:104) sold, RIGHT (cid:105) to predict cars. This in-
formation is unfortunately not speciﬁc to cars (many
things can be sold, e.g., chocolates, candy). Consid-
ering manufacturer, the left dependent of sold would
help predict cars more accurately.

In order to jointly take left and right dependents
into account, we employ yet another LSTM, which
goes from the furthest left dependent to the closest
left dependent (LD is a shorthand for left depen-
dent). As shown in Figure 4, LD LSTM learns the
representation of all left dependents of a node w0;
this representation is then used to predict the ﬁrst
right dependent of the same node. Non-ﬁrst right de-
pendents can also leverage the representation of left
dependents, since this information is injected into
the hidden state of the ﬁrst right dependent and can
percolate all the way. Note that in order to retain the
generation capability of our model (Section 3.4), we
only allow right dependents to leverage left depen-
dents (they are generated before right dependents).

The computation of the LDTREELSTM is al-
the same as in TREELSTM except when
most
zt = GEN-R.
let vt be the cor-
responding left dependent sequence with length
K (vt = (w3, w2, w1) in Figure 4). Then, the hidden
state (qk) of vt at each time step k is:

In this case,

mk = We · e(vt,k)
qk = LSTMLD(mk, qk−1)

(6a)

(6b)

where qK is the representation for all left depen-
dents. Then, the computation of the current hid-
den state becomes (see Equation (2) for the original
computation):

rt =

(cid:21)
(cid:20)We · e(wt(cid:48))
qK

ht = LSTMGEN-R(rt, H[:,t(cid:48)])

(7a)

(7b)

where qK serves as additional input for LSTMGEN-R.
All other computational details are the same as in
TreeLSTM (see Section 2.3).

2.5 Model Training

On small scale datasets we employ Negative Log-
likelihood (NLL) as our training objective for both
TREELSTM and LDTREELSTM:

L NLL(θ) = −

log P(S|T )

(8)

1
|S | ∑

S∈S

where S is a sentence in the training set S , T is the
dependency tree of S and P(S|T ) is deﬁned as in
Equation (1).

On large scale datasets (e.g., with vocabulary
size of 65K), computing the output layer activa-
tions and the softmax function with NLL would
become prohibitively expensive.
Instead, we em-
ploy Noise Contrastive Estimation (NCE; Gutmann
and Hyv¨arinen (2012), Mnih and Teh (2012)) which
treats the normalization term ˆZ in ˆP(w|D(wt)) =
exp(Who[w,:]·ht )
as constant. The intuition behind NCE
ˆZ
is to discriminate between samples from a data dis-
tribution ˆP(w|D(wt)) and a known noise distribu-
tion Pn(w) via binary logistic regression. Assuming
that noise words are k times more frequent than real
words in the training set (Mnih and Teh, 2012), then
the probability of a word w being from our model
Pd(w,D(wt)) is
. We apply NCE to
large vocabulary models with the following training
objective:

ˆP(w|D(wt ))
ˆP(w|D(wt ))+kPn(w)

L NCE(θ) = −

log Pd(wt,D(wt))

(cid:18)

1
|S | ∑

T ∈S

|T |
∑
t=1

+

log[1 − Pd( ˜wt, j,D(wt))]

(cid:19)

k
∑
j=1

where ˜wt, j is a word sampled from the noise distri-
bution Pn(w). We use smoothed unigram frequen-
cies (exponentiating by 0.75) as the noise distribu-
tion Pn(w) (Mikolov et al., 2013b). We initialize
ln ˆZ = 9 as suggested in Chen et al. (2015), but in-
stead of keeping it ﬁxed we also learn ˆZ during train-
ing (Vaswani et al., 2013). We set k = 20.

3 Experiments

We assess the performance of our model on two
tasks: the Microsoft Research (MSR) sentence com-
pletion challenge (Zweig and Burges, 2012), and de-
pendency parsing reranking. We also demonstrate
the tree generation capability of our models. In the
following, we ﬁrst present details on model train-
ing and then present our results. We implemented
our models using the Torch library (Collobert et
al., 2011) and our code is available at https://
github.com/XingxingZhang/td-treelstm.

3.1 Training Details

We trained our model with back propagation
through time (Rumelhart et al., 1988) on an Nvidia

GPU Card with a mini-batch size of 64. The ob-
jective (NLL or NCE) was minimized by stochastic
gradient descent. Model parameters were uniformly
initialized in [−0.1, 0.1]. We used the NCE objec-
tive on the MSR sentence completion task (due to
the large size of this dataset) and the NLL objec-
tive on dependency parsing reranking. We used an
initial learning rate of 1.0 for all experiments and
when there was no signiﬁcant improvement in log-
likelihood on the validation set, the learning rate was
divided by 2 per epoch until convergence (Mikolov
et al., 2010). To alleviate the exploding gradients
problem, we rescaled the gradient g when the gradi-
ent norm ||g|| > 5 and set g = 5g
||g|| (Pascanu et al.,
2013; Sutskever et al., 2014). Dropout (Srivastava
et al., 2014) was applied to the 2-layer TREELSTM
and LDTREELSTM models. The word embedding
size was set to s = d/2 where d is the hidden unit
size.

3.2 Microsoft Sentence Completion Challenge

The task in the MSR Sentence Completion Chal-
lenge (Zweig and Burges, 2012) is to select the
correct missing word for 1,040 SAT-style test sen-
tences when presented with ﬁve candidate comple-
tions. The training set contains 522 novels from
the Project Gutenberg which we preprocessed as fol-
lows. After removing headers and footers from the
ﬁles, we tokenized and parsed the dataset into de-
pendency trees with the Stanford Core NLP toolkit
(Manning et al., 2014). The resulting training set
contained 49M words. We converted all words to
lower case and replaced those occurring ﬁve times
or less with UNK. The resulting vocabulary size
was 65,346 words. We randomly sampled 4,000
sentences from the training set as our validation set.
The literature describes two main approaches to
the sentence completion task based on word vectors
and language models. In vector-based approaches,
all words in the sentence and the ﬁve candidate
words are represented by a vector; the candidate
which has the highest average similarity with the
sentence words is selected as the answer. For lan-
guage model-based methods, the LM computes the
probability of a test sentence with each of the ﬁve
candidate words, and picks the candidate comple-
tion which gives the highest probability. Our model
belongs to this class of models.

|θ|

Accuracy

d

—
640
600

Model
Word Vector based Models
LSA
Skip-gram
IVLBL
Language Models
KN5
UDepNgram
LDepNgram
RNN
RNNME
depRNN+3gram
ldepRNN+4gram
LBL
LSTM
LSTM
LSTM
Bidirectional LSTM
Bidirectional LSTM
Bidirectional LSTM
Model Combinations
RNNMEs
—
Skip-gram + RNNMEs —
Our Models
TREELSTM
LDTREELSTM
TREELSTM
LDTREELSTM

—
—
—
300
300
100
200
300
300
400
450
200
300
400

300
300
400
400

—
102M
96.0M

—
—
—
48.1M
1120M
1014M
1029M
48.0M
29.9M
40.2M
45.3M
33.2M
50.1M
67.3M

—
—

31.6M
32.5M
43.1M
44.7M

49.0
48.0
55.5

40.0
48.3
50.0
45.0
49.3
53.5
50.7
54.7
55.00
57.02
55.96
48.46
49.90
48.65

55.4
58.9

55.29
57.79
56.73
60.67

Table 1: Model accuracy on the MSR sentence completion task.

The results of KN5, RNNME and RNNMEs are reported in

Mikolov (2012), LSA and RNN in Zweig et al. (2012), UDep-

Ngram and LDepNgram in Gubbins and Vlachos (2013), de-

pRNN+3gram and depRNN+4gram in Mirowski and Vlachos

(2015), LBL in Mnih and Teh (2012), Skip-gram and Skip-

gram+RNNMEs in Mikolov et al. (2013a), and IVLBL in Mnih
and Kavukcuoglu (2013); d is the hidden size and |θ| the num-
ber of parameters in a model.

Table 1 presents a summary of our results to-
gether with previoulsy published results. The best
performing word vector model is IVLBL (Mnih and
Kavukcuoglu, 2013) with an accuracy of 55.5, while
the best performing single language model is LBL
(Mnih and Teh, 2012) with an accuracy of 54.7.
Both approaches are based on the log-bilinear lan-
guage model (Mnih and Hinton, 2007). A combi-
nation of several recurrent neural networks and the
skip-gram model holds the state of the art with an
accuracy of 58.9 (Mikolov et al., 2013b). To fairly
compare with existing models, we restrict the layer

Test

Parser

Development
UAS
MSTParser-2nd
92.20
TREELSTM
92.51
92.64
TREELSTM*
LDTREELSTM 92.66
92.00
NN parser*
93.20
S-LSTM*

LAS
88.44
88.53
88.69
88.69
89.60
90.90
Table 2: Performance of TREELSTM and LDTREELSTM on

LAS UAS
91.63
88.78
91.79
89.07
91.97
89.09
91.99
89.14
91.80
89.70
93.10
90.90

reranking the top dependency trees produced by the 2nd order

MSTParser (McDonald and Pereira, 2006). Results for the NN

and S-LSTM parsers are reported in Chen and Manning (2014)

and Dyer et al. (2015), respectively. * indicates that the model

is initialized with pre-trained word vectors.

size of our models to 1. We observe that LDTREEL-
STM consistently outperforms TREELSTM, which
indicates the importance of modeling the interac-
In fact,
tion between left and right dependents.
LDTREELSTM (d = 400) achieves a new state-of-
the-art on this task, despite being a single model.
We also implement LSTM and bidirectional LSTM
language models.3 An LSTM with d = 400 out-
performs its smaller counterpart (d = 300), however
performance decreases with d = 450. The bidirec-
tional LSTM is worse than the LSTM (see Mnih
and Teh (2012) for a similar observation). The
best performing LSTM is worse than a LDTREEL-
STM (d = 300). The input and output embeddings
(We and Who) dominate the number of parame-
ters in all neural models except for RNNME, de-
pRNN+3gram and ldepRNN+4gram, which include
a ME model that contains 1 billion sparse n-gram
features (Mikolov, 2012; Mirowski and Vlachos,
2015). The number of parameters in TREELSTM
and LDTREELSTM is not much larger compared to
LSTM due to the tied We and Who matrices.

3.3 Dependency Parsing

In this section we demonstrate that our model can
be also used for parse reranking. This is not possi-
ble for sequence-based language models since they
cannot estimate the probability of a tree. We use
our models to rerank the top K dependency trees
produced by the second order MSTParser (McDon-

3LSTMs and BiLSTMs were also trained with NCE
(s = d/2; hyperparameters were tuned on the development set).

ald and Pereira, 2006).4 We follow closely the ex-
perimental setup of Chen and Manning (2014) and
Dyer et al. (2015). Speciﬁcally, we trained TREEL-
STM and LDTREELSTM on Penn Treebank sec-
tions 2–21. We used section 22 for development and
section 23 for testing. We adopted the Stanford ba-
sic dependency representations (De Marneffe et al.,
2006); part-of-speech tags were predicted with the
Stanford Tagger (Toutanova et al., 2003). We trained
TREELSTM and LDTREELSTM as language mod-
els (singletons were replaced with UNK) and did
not use any POS tags, dependency labels or com-
position features, whereas these features are used in
Chen and Manning (2014) and Dyer et al. (2015).
We tuned d, the number of layers, and K on the de-
velopment set.

Table 2 reports unlabeled attachment scores
(UAS) and labeled attachment scores (LAS) for
the MSTParser, TREELSTM (d = 300, 1 layer,
K = 2), and LDTREELSTM (d = 200, 2 layers,
K = 4). We also include the performance of two
neural network-based dependency parsers; Chen and
Manning (2014) use a neural network classiﬁer to
predict the correct transition (NN parser); Dyer et
al. (2015) also implement a transition-based depen-
dency parser using LSTMs to represent the contents
of the stack and buffer in a continuous space. As can
be seen, both TREELSTM and LDTREELSTM out-
perform the baseline MSTParser, with LDTREEL-
STM performing best. We also initialized the word
embedding matrix We with pre-trained GLOVE vec-
tors (Pennington et al., 2014). We obtained a slight
improvement over TREELSTM (TREELSTM* in
Table 2; d = 200, 2 layer, K = 4) but no im-
provement over LDTREELSTM. Finally, notice that
LDTREELSTM is slightly better than the NN parser
in terms of UAS but worse than the S-LSTM parser.
In the future, we would like to extend our model so
that it takes labeled dependency information into ac-
count.

3.4 Tree Generation

This section demonstrates how to use a trained
LDTREELSTM to generate tree samples. The gen-
eration starts at the ROOT node. At each time step t,
for each node wt, we add a new edge and node to

Figure 5: Generated dependency trees with LDTREELSTM

trained on the PTB.

the tree. Unfortunately during generation, we do not
know which type of edge to add. We therefore use
four binary classiﬁers (ADD-LEFT, ADD-RIGHT,
ADD-NX-LEFT and ADD-NX-RIGHT) to predict
whether we should add a LEFT, RIGHT, NX-LEFT
or NX-RIGHT edge.5 Then when a classiﬁer pre-
dicts true, we use the corresponding LSTM to gener-
ate a new node by sampling from the predicted word
distribution in Equation (3). The four classiﬁers take
the previous hidden state H[:,t(cid:48)] and the output em-
bedding of the current node Who · e(wt) as features.6
Speciﬁcally, we use a trained LDTREELSTM to
go through the training corpus and generate hidden
states and embeddings as input features; the corre-
sponding class labels (true and false) are “read off”
the training dependency trees. We use two-layer rec-
tiﬁer networks (Glorot et al., 2011) as the four clas-
siﬁers with a hidden size of 300. We use the same
LDTREELSTM model as in Section 3.3 to gener-
ate dependency trees. The classiﬁers were trained
using AdaGrad (Duchi et al., 2011) with a learning
rate of 0.01. The accuracies of ADD-LEFT, ADD-
RIGHT, ADD-NX-LEFT and ADD-NX-RIGHT are

5It is possible to get rid of the four classiﬁers by adding
START/STOP symbols when generating left and right depen-
dents as in (Eisner, 1996). We refrained from doing this for
computational reasons. For a sentence with N words, this ap-
proach will lead to 2N additional START/STOP symbols (with
one START and one STOP symbol for each word). Conse-
quently, the computational cost and memory consumption dur-
ing training will be three times as much rendering our model
less scalable.

6The input embeddings have lower dimensions and therefore

4http://www.seas.upenn.edu/ strctlrn/MSTParser

result in slightly worse classiﬁers.

94.3%, 92.6%, 93.4% and 96.0%, respectively. Fig-
ure 5 shows examples of generated trees.

4 Conclusions

In this paper we developed TREELSTM (and
LDTREELSTM), a neural network model architec-
ture, which is designed to predict tree structures
rather than linear sequences. Experimental results
on the MSR sentence completion task show that
LDTREELSTM is superior to sequential LSTMs.
Dependency parsing reranking experiments high-
light our model’s potential for dependency pars-
ing. Finally, the ability of our model to gener-
ate dependency trees holds promise for text gen-
eration applications such as sentence compression
and simpliﬁcation (Filippova et al., 2015). Although
our experiments have focused exclusively on depen-
dency trees, there is nothing inherent in our formu-
lation that disallows its application to other types of
tree structure such as constituent trees or even tax-
onomies.

Acknowledgments

We would like to thank Adam Lopez, Frank Keller,
Iain Murray, Li Dong, Brian Roark, and the NAACL
reviewers for their valuable feedback. Xingxing
Zhang gratefully acknowledges the ﬁnancial sup-
port of the China Scholarship Council (CSC). Liang
Lu is funded by the UK EPSRC Programme Grant
EP/I031022/1, Natural Speech Technology (NST).

References

[Bengio et al.2003] Yoshua Bengio, R´ejean Ducharme,
Pascal Vincent, and Christian Janvin. 2003. A neural
probabilistic language model. The Journal of Machine
Learning Research, 3:1137–1155.

[Charniak2001] Eugene Charniak.

Immediate-
head parsing for language models. In Proceedings of
the 39th Annual Meeting on Association for Compu-
tational Linguistics, pages 124–131. Association for
Computational Linguistics.

2001.

[Chelba and Jelinek2000] Ciprian Chelba and Frederick
Jelinek. 2000. Structured language modeling. Com-
puter Speech and Language, 14(4):283–332.

[Chelba et al.1997] Ciprian Chelba, David Engle, Freder-
ick Jelinek, Victor Jimenez, Sanjeev Khudanpur, Lidia
Mangu, Harry Printz, Eric Ristad, Ronald Rosenfeld,

Andreas Stolcke, et al.
formance of a dependency language model.
ROSPEECH. Citeseer.

1997. Structure and per-
In EU-

[Chen and Manning2014] Danqi Chen and Christopher
2014. A fast and accurate dependency
Manning.
In Proceedings of
parser using neural networks.
the 2014 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 740–750,
Doha, Qatar, October. Association for Computational
Linguistics.

[Chen et al.2015] X Chen, X Liu, MJF Gales, and
PC Woodland. 2015. Recurrent neural network lan-
guage model training with noise contrastive estimation
for speech recognition. In In 40th IEEE International
Conference on Accoustics, Speech and Signal Process-
ing, pages 5401–5405, Brisbane, Australia.

[Collobert et al.2011] Ronan

Collobert,

Koray
Kavukcuoglu, and Cl´ement Farabet. 2011. Torch7:
A matlab-like environment for machine learning.
In
BigLearn, NIPS Workshop, number EPFL-CONF-
192376.

[De Marneffe et al.2006] Marie-Catherine De Marneffe,
Bill MacCartney, Christopher D Manning, et al. 2006.
Generating typed dependency parses from phrase
structure parses. In Proceedings of LREC, volume 6,
pages 449–454.

[Duchi et al.2011] John Duchi, Elad Hazan, and Yoram
Singer. 2011. Adaptive subgradient methods for on-
line learning and stochastic optimization. The Journal
of Machine Learning Research, 12:2121–2159.

[Dyer et al.2015] Chris Dyer, Miguel Ballesteros, Wang
Ling, Austin Matthews, and Noah A. Smith. 2015.
Transition-based dependency parsing with stack long
In Proceedings of the 53rd An-
short-term memory.
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long Pa-
pers), pages 334–343, Beijing, China, July. Associa-
tion for Computational Linguistics.

[Eisner1996] Jason M Eisner. 1996. Three new prob-
abilistic models for dependency parsing: An explo-
ration. In Proceedings of the 16th conference on Com-
putational linguistics-Volume 1, pages 340–345. Asso-
ciation for Computational Linguistics.

[Emami et al.2003] Ahmad Emami, Peng Xu, and Fred-
erick Jelinek. 2003. Using a connectionist model in
In Proceedings
a syntactical based language model.
of the IEEE International Conference on Acoustics,
Speech, and Signal Processing, pages 372–375, Hong
Kong, China.

[Filippova et al.2015] Katja Filippova, Enrique Alfon-
seca, Carlos A Colmenares, Lukasz Kaiser, and Oriol
Vinyals. 2015. Sentence compression by deletion
with lstms. In EMNLP, pages 360–368.

[Glorot et al.2011] Xavier Glorot, Antoine Bordes, and
Yoshua Bengio. 2011. Deep sparse rectiﬁer neural
networks. In International Conference on Artiﬁcial In-
telligence and Statistics, pages 315–323.

[Graves et al.2013] Alan Graves, Abdel-rahman Mo-
hamed, and Geoffrey Hinton. 2013. Speech recogni-
tion with deep recurrent neural networks. In Acoustics,
Speech and Signal Processing (ICASSP), 2013 IEEE
International Conference on, pages 6645–6649. IEEE.
[Gubbins and Vlachos2013] Joseph Gubbins and Andreas
Vlachos. 2013. Dependency language models for sen-
tence completion. In EMNLP, pages 1405–1410, Seat-
tle, Washington, USA, October. Association for Com-
putational Linguistics.

[Gutmann and Hyv¨arinen2012] Michael U Gutmann and
Aapo Hyv¨arinen. 2012. Noise-contrastive estimation
of unnormalized statistical models, with applications
to natural image statistics. The Journal of Machine
Learning Research, 13(1):307–361.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural computation, 9(8):1735–1780.

[Hochreiter1998] Sepp Hochreiter. 1998. Vanishing gra-
dient problem during learning recurrent neural nets
and problem solutions. International Journal of Un-
certainty, Fuzziness and Knowledge-based Systems,
6(2):107–116.

[Manning et al.2014] Christopher D Manning, Mihai Sur-
deanu, John Bauer, Jenny Finkel, Steven J Bethard,
and David McClosky. 2014. The stanford corenlp
In Proceedings
natural language processing toolkit.
of 52nd Annual Meeting of the Association for Com-
putational Linguistics: System Demonstrations, pages
55–60.

[McDonald and Pereira2006] Ryan T McDonald and Fer-
nando CN Pereira. 2006. Online learning of approxi-
mate dependency parsing algorithms. In EACL.

[Mikolov et al.2010] Tomas Mikolov, Martin Karaﬁ´at,
Lukas Burget, Jan Cernock`y, and Sanjeev Khudan-
pur. 2010. Recurrent neural network based language
model. In INTERSPEECH 2010, 11th Annual Confer-
ence of the International Speech Communication As-
sociation, Makuhari, Chiba, Japan, September 26-30,
2010, pages 1045–1048.

[Mikolov et al.2013a] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013a. Efﬁcient esti-
In
mation of word representations in vector space.
Proceedings of the 2013 International Conference on
Learning Representations, Scottsdale, Arizona, USA.
Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013b.
Distributed representations of words and phrases and
In Advances in Neural Infor-
their compositionality.
mation Processing Systems 26, pages 3111–3119.

[Mikolov et al.2013b] Tomas Mikolov,

[Mikolov2012] Tomas Mikolov. 2012. Statistical Lan-
guage Models based on Neural Networks. Ph.D. the-
sis, Brno University of Technology.

[Mirowski and Vlachos2015] Piotr Mirowski and An-
dreas Vlachos. 2015. Dependency recurrent neural
In ACL,
language models for sentence completion.
pages 511–517, Beijing, China, July. Association for
Computational Linguistics.

[Mnih and Hinton2007] Andriy Mnih and Geoffrey Hin-
ton. 2007. Three new graphical models for statistical
In Proceedings of the 24th In-
language modelling.
ternational Conference on Machine Learning, pages
641–648.

[Mnih and Kavukcuoglu2013] Andriy Mnih and Koray
Kavukcuoglu. 2013. Learning word embeddings efﬁ-
ciently with noise-contrastive estimation. In Advances
in Neural Information Processing Systems 26, pages
2265–2273.

[Mnih and Teh2012] Andriy Mnih and Yee Whye Teh.
2012. A fast and simple algorithm for training neural
probabilistic language models. In Proceedings of the
29th International Conference on Machine Learning,
pages 1751–1758, Edinburgh, Scotland.

[Pascanu et al.2013] Razvan Pascanu, Tomas Mikolov,
and Yoshua Bengio. 2013. On the difﬁculty of train-
ing recurrent neural networks. In Proceedings of the
31st International Conference on Machine Learning,
pages 1310–1318, Atlanta, Georgia, USA.
Pennington,

Richard
Socher, and Christopher D Manning. 2014. Glove:
EMNLP,
Global vectors for word representation.
12:1532–1543.

[Pennington et al.2014] Jeffrey

[Pollack1990] Jordan B. Pollack. 1990. Recursive dis-
tributed representations. Artiﬁcial Intelligence, 1–
2(46):77–105.

[Roark2001] Brian Roark. 2001. Probabilistic top-down
parsing and language modeling. Computational lin-
guistics, 27(2):249–276.

[Rumelhart et al.1988] David E Rumelhart, Geoffrey E
Hinton, and Ronald J Williams. 1988. Learning repre-
sentations by back-propagating errors. Cognitive mod-
eling, 5:3.

[Sennrich2015] Rico Sennrich. 2015. Modelling and op-
timizing on syntactic n-grams for statistical machine
translation. Transactions of the Association for Com-
putational Linguistics, 3:169–182.

[Shen et al.2008] Libin Shen,

Jinxi Xu,

and Ralph
Weischedel. 2008. A new string-to-dependency ma-
chine translation algorithm with a target dependency
In Proceedings of ACL-08: HLT,
language model.
pages 577–585, Columbus, Ohio, USA.

[Socher et al.2011a] Richard Socher, Eric H. Huang, Jef-
frey Pennington, Christopher D. Manning, and An-
drew Ng. 2011a. Dynamic pooling and unfolding

Model? On the Future of Language Modeling for HLT,
pages 29–36, Montr´eal, Canada.

[Zweig et al.2012] Geoffrey Zweig, John C Platt, Christo-
pher Meek, Christopher JC Burges, Ainur Yessenalina,
2012. Computational approaches
and Qiang Liu.
In Proceedings of the 50th
to sentence completion.
Annual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 601–610.
Association for Computational Linguistics.

In
recursive autoencoders for paraphrase detection.
Advances in Neural Information Processing Systems,
pages 801–809.

[Socher et al.2011b] Richard Socher, Jeffrey Pennington,
Eric H. Huang, Andrew Y. Ng, and Christopher D.
Manning. 2011b. Semi-supervised recursive autoen-
coders for predicting sentiment distributions. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 151–161,
Edinburgh, Scotland, UK.

[Srivastava et al.2014] Nitish Srivastava, Geoffrey Hin-
ton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: A simple way to pre-
vent neural networks from overﬁtting. The Journal of
Machine Learning Research, 15(1):1929–1958.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and
Quoc VV Le. 2014. Sequence to sequence learning
with neural networks. In Advances in Neural Informa-
tion Processing Systems, pages 3104–3112.

[Tai et al.2015] Kai Sheng Tai, Richard Socher, and
Christopher D. Manning. 2015.
Improved semantic
representations from tree-structured long short-term
memory networks. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguis-
tics and the 7th International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers),
pages 1556–1566, Beijing, China, July. Association
for Computational Linguistics.

[Toutanova et al.2003] Kristina Toutanova, Dan Klein,
Christopher D Manning, and Yoram Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic de-
pendency network. In Proceedings of the 2003 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology-Volume 1, pages 173–180. Associa-
tion for Computational Linguistics.

[Vaswani et al.2013] Ashish Vaswani, Yinggong Zhao,
Victoria Fossum, and David Chiang. 2013. Decod-
ing with large-scale neural language models improves
translation. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
pages 1387–1392, Seattle, Washington, USA.

[Vinyals et al.2015] Oriol Vinyals, Alexander Toshev,
Samy Bengio, and Dumitru Erhan. 2015. Show and
tell: A neural image caption generator. In The IEEE
Conference on Computer Vision and Pattern Recogni-
tion, Boston, Massachusetts, USA.

[Zhang2009] Ying Zhang. 2009. Structured language
models for statistical machine translation. Ph.D. the-
sis, Johns Hopkins University.

[Zweig and Burges2012] Geoffrey Zweig and Chris J.C.
Burges. 2012. A challenge set for advancing language
In Proceedings of the NAACL-HLT 2012
modeling.
Workshop: Will We Ever Really Replace the N-gram

Top-down Tree Long Short-Term Memory Networks

Xingxing Zhang, Liang Lu and Mirella Lapata
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
{x.zhang,liang.lu}@ed.ac.uk,mlap@inf.ed.ac.uk

6
1
0
2
 
r
p
A
 
3
 
 
]
L
C
.
s
c
[
 
 
3
v
0
6
0
0
0
.
1
1
5
1
:
v
i
X
r
a

Abstract

Long Short-Term Memory (LSTM) networks,
a type of recurrent neural network with a
more complex computational unit, have been
successfully applied to a variety of sequence
modeling tasks. In this paper we develop Tree
Long Short-Term Memory (TREELSTM), a
neural network model based on LSTM, which
is designed to predict a tree rather than a lin-
ear sequence. TREELSTM deﬁnes the prob-
ability of a sentence by estimating the gener-
ation probability of its dependency tree. At
each time step, a node is generated based
on the representation of the generated sub-
tree. We further enhance the modeling power
of TREELSTM by explicitly representing the
correlations between left and right depen-
dents. Application of our model to the MSR
sentence completion challenge achieves re-
sults beyond the current state of the art. We
also report results on dependency parsing
reranking achieving competitive performance.

1

Introduction

Neural language models have been gaining increas-
ing attention as a competitive alternative to n-grams.
The main idea is to represent each word using a
real-valued feature vector capturing the contexts in
which it occurs. The conditional probability of the
next word is then modeled as a smooth function of
the feature vectors of the preceding words and the
In essence, similar representations are
next word.
learned for words found in similar contexts result-
ing in similar predictions for the next word. Previ-
ous approaches have mainly employed feed-forward

(Bengio et al., 2003; Mnih and Hinton, 2007) and
recurrent neural networks (Mikolov et al., 2010;
Mikolov, 2012) in order to map the feature vec-
tors of the context words to the distribution for the
next word. Recently, RNNs with Long Short-Term
Memory (LSTM) units (Hochreiter and Schmidhu-
ber, 1997; Hochreiter, 1998) have emerged as a pop-
ular architecture due to their strong ability to capture
long-term dependencies. LSTMs have been success-
fully applied to a variety of tasks ranging from ma-
chine translation (Sutskever et al., 2014), to speech
recognition (Graves et al., 2013), and image descrip-
tion generation (Vinyals et al., 2015).

Despite superior performance in many applica-
tions, neural language models essentially predict se-
quences of words. Many NLP tasks, however, ex-
ploit syntactic information operating over tree struc-
tures (e.g., dependency or constituent trees). In this
paper we develop a novel neural network model
which combines the advantages of the LSTM archi-
tecture and syntactic structure. Our model estimates
the probability of a sentence by estimating the gen-
eration probability of its dependency tree. Instead
of explicitly encoding tree structure as a set of fea-
tures, we use four LSTM networks to model four
types of dependency edges which altogether specify
how the tree is built. At each time step, one LSTM is
activated which predicts the next word conditioned
on the sub-tree generated so far. To learn the repre-
sentations of the conditioned sub-tree, we force the
four LSTMs to share their hidden layers. Our model
is also capable of generating trees just by sampling
from a trained model and can be seamlessly inte-
grated with text generation applications.

Our approach is related to but ultimately differ-
ent from recursive neural networks (Pollack, 1990)
a class of models which operate on structured in-
puts. Given a (binary) parse tree, they recursively
generate parent representations in a bottom-up fash-
ion, by combining tokens to produce representations
for phrases, and eventually the whole sentence. The
learned representations can be then used in classi-
ﬁcation tasks such as sentiment analysis (Socher et
al., 2011b) and paraphrase detection (Socher et al.,
2011a). Tai et al. (2015) learn distributed representa-
tions over syntactic trees by generalizing the LSTM
architecture to tree-structured network topologies.
The key feature of our model is not so much that
it can learn semantic representations of phrases or
sentences, but its ability to predict tree structure and
estimate its probability.

Syntactic language models have a long history
in NLP dating back to Chelba and Jelinek (2000)
(see also Roark (2001) and Charniak (2001)). These
models differ in how grammar structures in a parsing
tree are used when predicting the next word. Other
work develops dependency-based language models
for speciﬁc applications such as machine translation
(Shen et al., 2008; Zhang, 2009; Sennrich, 2015),
speech recognition (Chelba et al., 1997) or sentence
completion (Gubbins and Vlachos, 2013). All in-
stances of these models apply Markov assumptions
on the dependency tree, and adopt standard n-gram
smoothing methods for reliable parameter estima-
tion. Emami et al. (2003) and Sennrich (2015) esti-
mate the parameters of a structured language model
using feed-forward neural networks (Bengio et al.,
2003). Mirowski and Vlachos (2015) re-implement
the model of Gubbins and Vlachos (2013) with
RNNs. They view sentences as sequences of words
over a tree. While they ignore the tree structures
themselves, we model them explicitly.

Our model shares with other structured-based lan-
guage models the ability to take dependency infor-
mation into account. It differs in the following re-
spects: (a) it does not artiﬁcially restrict the depth
of the dependencies it considers and can thus be
viewed as an inﬁnite order dependency language
model; (b) it not only estimates the probability of a
string but is also capable of generating dependency
trees; (c) ﬁnally, contrary to previous dependency-
based language models which encode syntactic in-

formation as features, our model takes tree structure
into account more directly via representing different
types of dependency edges explicitly using LSTMs.
Therefore, there is no need to manually determine
which dependency tree features should be used or
how large the feature embeddings should be.

We evaluate our model on the MSR sentence com-
pletion challenge, a benchmark language modeling
dataset. Our results outperform the best published
results on this dataset. Since our model is a general
tree estimator, we also use it to rerank the top K de-
pendency trees from the (second order) MSTPasrser
and obtain performance on par with recently pro-
posed dependency parsers.

2 Tree Long Short-Term Memory

Networks

We seek to estimate the probability of a sentence by
estimating the generation probability of its depen-
dency tree. Syntactic information in our model is
represented in the form of dependency paths. In the
following, we ﬁrst describe our deﬁnition of depen-
dency path and based on it explain how the proba-
bility of a sentence is estimated.

2.1 Dependency Path

Generally speaking, a dependency path is the path
between ROOT and w consisting of the nodes on
the path and the edges connecting them. To rep-
resent dependency paths, we introduce four types
of edges which essentially deﬁne the “shape” of a
dependency tree. Let w0 denote a node in a tree
and w1, w2, . . . , wn its left dependents. As shown in
Figure 1, LEFT edge is the edge between w0 and
its ﬁrst left dependent denoted as (w0, w1). Let wk
(with 1 < k ≤ n) denote a non-ﬁrst left dependent
of w0. The edge from wk−1 to wk is a NX-LEFT
edge (NX stands for NEXT), where wk−1 is the right
adjacent sibling of wk. Note that the NX-LEFT edge
(wk−1, wk) replaces edge (w0, wk) (illustrated with a
dashed line in Figure 1) in the original dependency
tree. The modiﬁcation allows information to ﬂow
from w0 to wk through w1, . . . , wk−1 rather than di-
rectly from w0 to wk. RIGHT and NX-RIGHT edges
are deﬁned analogously for right dependents.

Given these four types of edges, dependency
paths (denoted as D(w)) can be deﬁned as follows

w0

wn

wk

wk−1

w1

LEFT

NX-LEFT

Figure 1: LEFT and NX-LEFT edges. Dotted line between
w1 and wk−1 (also between wk and wn) indicate that there may
be ≥ 0 nodes inbetween.

bearing in mind that the ﬁrst right dependent of
ROOT is its only dependent and that wp denotes the
parent of w. We use (. . . ) to denote a sequence,
where () is an empty sequence and (cid:107) is an operator
for concatenating two sequences.

(1) if w is ROOT, then D(w) = ()
(2) if w is a left dependent of wp
(a) if w is the ﬁrst

left dependent,

then

D(w) = D(wp)(cid:107)((cid:104)wp, LEFT(cid:105))

(b) if w is not the ﬁrst left dependent and ws is

its right adjacent sibling, then
D(w) = D(ws)(cid:107)((cid:104)ws, NX-LEFT(cid:105))

(3) if w is a right dependent of wp

(a) if w is the ﬁrst right dependent,
D(w) = D(wp)(cid:107)((cid:104)wp, RIGHT(cid:105))

then

(b) if w is not the ﬁrst right dependent and ws

is its left adjacent sibling, then
D(w) = D(ws)(cid:107)((cid:104)ws, NX-RIGHT(cid:105))

A dependency tree can be represented by the set of
its dependency paths which in turn can be used to
reconstruct the original tree.1

for

the ﬁrst

the moment

Dependency paths

two levels
the tree in Figure 2 are as follows (ig-
of
the subscripts which
noring for
D(sold) =
we explain in the next section).
((cid:104)ROOT, RIGHT(cid:105)) (see deﬁnitions (1) and (3a)),
D(year) = D(sold)(cid:107)((cid:104)sold, LEFT(cid:105))
(2a)),
D(manufacturer) = D(year)(cid:107)((cid:104)year, NX-LEFT(cid:105))
(see (2b)), D(cars) = D(sold)(cid:107)((cid:104)sold, RIGHT(cid:105))
(see (3a)), D(in) = D(cars)(cid:107)((cid:104)cars, NX-RIGHT(cid:105))
(according to (3b)).

(see

2.2 Tree Probability

ROOT

sold1

manufacturer3

year2

cars4

in5

The9

luxury8 auto7

last6

1,21410

U.S.11

the12

Figure 2: Dependency tree of the sentence The luxury auto

manufacturer last year sold 1,214 cars in the U.S. Subscripts

indicate breadth-ﬁrst traversal. ROOT has only one dependent

(i.e., sold ) which we view as its ﬁrst right dependent.

its corresponding tree T , P(S|T ). We view the prob-
ability computation of a dependency tree as a gener-
ation process. Speciﬁcally, we assume dependency
trees are constructed top-down, in a breadth-ﬁrst
manner. Generation starts at the ROOT node. For
each node at each level, ﬁrst its left dependents are
generated from closest to farthest and then the right
dependents (again from closest to farthest). The
same process is applied to the next node at the same
level or a node at the next level. Figure 2 shows the
breadth-ﬁrst traversal of a dependency tree.

Under the assumption that each word w in a de-
pendency tree is only conditioned on its dependency
path, the probability of a sentence S given its depen-
dency tree T is:

P(S|T ) = ∏

P(w|D(w))

(1)

w∈BFS(T )\ROOT

where D(w) is the dependency path of w. Note that
each word w is visited according to its breadth-ﬁrst
search order (BFS(T)) and the probability of ROOT
is ignored since every tree has one. The role of
ROOT in a dependency tree is the same as the begin
of sentence token (BOS) in a sentence. When com-
puting P(S|T ) (or P(S)), the probability of ROOT (or
BOS) is ignored (we assume it always exists), but is
used to predict other words. We explain in the next
section how TREELSTM estimates P(w|D(w)).

The core problem in syntax-based language model-
ing is to estimate the probability of sentence S given

2.3 Tree LSTMs

1Throughout this paper we assume all dependency trees are

projective.

A dependency path D(w) is subtree which we de-
note as a sequence of (cid:104)word, edge-type(cid:105) tuples. Our

w0

w0

w3

w2

w1

w4

w5

w6

Generated by four LSTMs

with tied We and tied Who

w2

w1

w0

G

E

N

-

L

G E N-R

w0

w4

w5

GEN-NX-L

GEN-NX-L

GEN-NX-R

GEN-NX-R

w2

w1

w3
w6
Figure 3: Generation process of left (w1, w2, w3) and right
(w4, w5, w6) dependents of tree node wo (top) using four LSTMs
(GEN-L, GEN-R, GEN-NX-L and GEN-NX-R). The model can

w4

w5

handle an arbitrary number of dependents due to GEN-NX-L

and GEN-NX-R.

innovation is to learn the representation of D(w) us-
ing four LSTMs. The four LSTMs (GEN-L, GEN-
R, GEN-NX-L and GEN-NX-R) are used to repre-
sent the four types of edges (LEFT, RIGHT, NX-
LEFT and NX-RIGHT) introduced earlier. GEN,
NX, L and R are shorthands for GENERATE, NEXT,
LEFT and RIGHT. At each time step, an LSTM is
chosen according to an edge-type; then the LSTM
takes a word as input and predicts/generates its de-
pendent or sibling. This process can be also viewed
as adding an edge and a node to a tree. Speciﬁ-
cally, LSTMs GEN-L and GEN-R are used to gen-
erate the ﬁrst left and right dependent of a node
(w1 and w4 in Figure 3). So, these two LSTMs
are responsible for going deeper in a tree. While
GEN-NX-L and GEN-NX-R generate the remain-
ing left/right dependents and therefore go wider in
a tree. As shown in Figure 3, w2 and w3 are gener-
ated by GEN-NX-L, whereas w5 and w6 are gener-
ated by GEN-NX-R. Note that the model can handle
any number of left or right dependents by applying
GEN-NX-L or GEN-NX-R multiple times.

We assume time steps correspond to the steps
taken by the breadth-ﬁrst traversal of the depen-
dency tree and the sentence has length n. At
time step t (1 ≤ t ≤ n), let (cid:104)wt(cid:48), zt(cid:105) denote the last
Subscripts t and t(cid:48) denote the
tuple in D(wt).
breadth-ﬁrst search order of wt and wt(cid:48), respectively.
zt ∈ {LEFT, RIGHT, NX-LEFT, NX-RIGHT} is the
edge type (see the deﬁnitions in Section 2.1). Let
We ∈ Rs×|V | denote the word embedding matrix and

Who ∈ R|V |×d the output matrix of our model, where
|V | is the vocabulary size, s the word embedding size
and d the hidden unit size. We use tied We and tied
Who for the four LSTMs to reduce the number of pa-
rameters in our model. The four LSTMs also share
their hidden states. Let H ∈ Rd×(n+1) denote the
shared hidden states of all time steps and e(wt) the
one-hot vector of wt. Then, H[:,t] represents D(wt)
at time step t, and the computation2 is:

xt = We · e(wt(cid:48))
ht = LSTMzt (xt, H[:,t(cid:48)])

H[:,t] = ht

yt = Who · ht

(2a)

(2b)

(2c)

(2d)

where the initial hidden state H[:, 0] is initialized to
a vector of small values such as 0.01. According to
Equation (2b), the model selects an LSTM based on
edge type zt. We describe the details of LSTMzt in
the next paragraph. The probability of wt given its
dependency path D(wt) is estimated by a softmax
function:

P(wt|D(wt)) =

(3)

exp(yt,wt )
|V |
k(cid:48)=1 exp(yt,k(cid:48))

∑

We must point out that although we use four jointly
trained LSTMs to encode the hidden states, the train-
ing and inference complexity of our model is no dif-
ferent from a regular LSTM, since at each time step
only one LSTM is working.

We implement LSTMz in Equation (2b) using a
deep LSTM (to simplify notation, from now on we
write z instead of zt). The inputs at time step t
are xt and ht(cid:48) (the hidden state of an earlier time
step t(cid:48)) and the output is ht (the hidden state of cur-
rent time step). Let L denote the layer number of
LSTMz and ˆhl
t the internal hidden state of the l-th
layer of the LSTMz at time step t, where xt is ˆh0
t and
ht(cid:48) is ˆhL
t(cid:48). The LSTM architecture introduces mul-
tiplicative gates and memory cells ˆcl
t (at l-th layer)
in order to address the vanishing gradient problem
which makes it difﬁcult for the standard RNN model
to learn long-distance correlations in a sequence.
Here, ˆcl
t is a linear combination of the current input
signal ut and an earlier memory cell ˆcl
t(cid:48). How much
input information ut will ﬂow into ˆcl
t is controlled

2We ignore all bias terms for notational simplicity.

w2

w1

w0

G

E

N

-

L

G E N-R

w0

w4

w5

GEN-NX-L

GEN-NX-L

GEN-NX-R

GEN-NX-R

LD

LD

w0

w4

w5

w6

w3

w2

w1

Figure 4: Generation of left and right dependents of node w0
according to LDTREELSTM.

by input gate it and how much of the earlier mem-
ory cell ˆcl
t(cid:48) will be forgotten is controlled by forget
gate ft. This process is computed as follows:

ux · ˆhl−1

ut = tanh(Wz,l
it = σ(Wz,l
ft = σ(Wz,l
t = ft (cid:12) ˆcl
ˆcl

ix · ˆhl−1
f x · ˆhl−1
t(cid:48) + it (cid:12) ut

t + Wz,l
uh · ˆhl
t(cid:48))
ih · ˆhl
t(cid:48))
f h · ˆhl
t(cid:48))

t + Wz,l
t + Wz,l

(4a)

(4b)

(4c)

(4d)

ux ∈ Rd×d (Wz,l

uh ∈ Rd×d are weight matrices for ut, Wz,l
ih are weight matrices for it and Wz,l

where Wz,l
ux ∈ Rd×s when l = 1) and
Wz,l
ix and
f x, and Wz,l
Wz,l
f h
are weight matrices for ft. σ is a sigmoid function
and (cid:12) the element-wise product.

Output gate ot controls how much information of

the cell ˆcl

t can be seen by other modules:

t + Wz,l
ox · ˆhl−1
ot = σ(Wz,l
ˆhl
t = ot (cid:12) tanh(ˆcl
t )

oh · ˆhl
t(cid:48))

(5a)

(5b)

Application of the above process to all layers L, will
yield ˆhL
t , which is ht. Note that in implementation,
all ˆcl
t (1 ≤ l ≤ L) at time step t are stored,
although we only care about ˆhL
t (ht).

t and ˆhl

2.4 Left Dependent Tree LSTMs

TREELSTM computes P(w|D(w)) based on the de-
pendency path D(w), which ignores the interaction
between left and right dependents on the same level.
In many cases, TREELSTM will use a verb to pre-
dict its object directly without knowing its subject.
For example, in Figure 2, TREELSTM uses (cid:104)ROOT,
RIGHT(cid:105) and (cid:104) sold, RIGHT (cid:105) to predict cars. This in-
formation is unfortunately not speciﬁc to cars (many
things can be sold, e.g., chocolates, candy). Consid-
ering manufacturer, the left dependent of sold would
help predict cars more accurately.

In order to jointly take left and right dependents
into account, we employ yet another LSTM, which
goes from the furthest left dependent to the closest
left dependent (LD is a shorthand for left depen-
dent). As shown in Figure 4, LD LSTM learns the
representation of all left dependents of a node w0;
this representation is then used to predict the ﬁrst
right dependent of the same node. Non-ﬁrst right de-
pendents can also leverage the representation of left
dependents, since this information is injected into
the hidden state of the ﬁrst right dependent and can
percolate all the way. Note that in order to retain the
generation capability of our model (Section 3.4), we
only allow right dependents to leverage left depen-
dents (they are generated before right dependents).

The computation of the LDTREELSTM is al-
the same as in TREELSTM except when
most
zt = GEN-R.
let vt be the cor-
responding left dependent sequence with length
K (vt = (w3, w2, w1) in Figure 4). Then, the hidden
state (qk) of vt at each time step k is:

In this case,

mk = We · e(vt,k)
qk = LSTMLD(mk, qk−1)

(6a)

(6b)

where qK is the representation for all left depen-
dents. Then, the computation of the current hid-
den state becomes (see Equation (2) for the original
computation):

rt =

(cid:21)
(cid:20)We · e(wt(cid:48))
qK

ht = LSTMGEN-R(rt, H[:,t(cid:48)])

(7a)

(7b)

where qK serves as additional input for LSTMGEN-R.
All other computational details are the same as in
TreeLSTM (see Section 2.3).

2.5 Model Training

On small scale datasets we employ Negative Log-
likelihood (NLL) as our training objective for both
TREELSTM and LDTREELSTM:

L NLL(θ) = −

log P(S|T )

(8)

1
|S | ∑

S∈S

where S is a sentence in the training set S , T is the
dependency tree of S and P(S|T ) is deﬁned as in
Equation (1).

On large scale datasets (e.g., with vocabulary
size of 65K), computing the output layer activa-
tions and the softmax function with NLL would
become prohibitively expensive.
Instead, we em-
ploy Noise Contrastive Estimation (NCE; Gutmann
and Hyv¨arinen (2012), Mnih and Teh (2012)) which
treats the normalization term ˆZ in ˆP(w|D(wt)) =
exp(Who[w,:]·ht )
as constant. The intuition behind NCE
ˆZ
is to discriminate between samples from a data dis-
tribution ˆP(w|D(wt)) and a known noise distribu-
tion Pn(w) via binary logistic regression. Assuming
that noise words are k times more frequent than real
words in the training set (Mnih and Teh, 2012), then
the probability of a word w being from our model
Pd(w,D(wt)) is
. We apply NCE to
large vocabulary models with the following training
objective:

ˆP(w|D(wt ))
ˆP(w|D(wt ))+kPn(w)

L NCE(θ) = −

log Pd(wt,D(wt))

(cid:18)

1
|S | ∑

T ∈S

|T |
∑
t=1

+

log[1 − Pd( ˜wt, j,D(wt))]

(cid:19)

k
∑
j=1

where ˜wt, j is a word sampled from the noise distri-
bution Pn(w). We use smoothed unigram frequen-
cies (exponentiating by 0.75) as the noise distribu-
tion Pn(w) (Mikolov et al., 2013b). We initialize
ln ˆZ = 9 as suggested in Chen et al. (2015), but in-
stead of keeping it ﬁxed we also learn ˆZ during train-
ing (Vaswani et al., 2013). We set k = 20.

3 Experiments

We assess the performance of our model on two
tasks: the Microsoft Research (MSR) sentence com-
pletion challenge (Zweig and Burges, 2012), and de-
pendency parsing reranking. We also demonstrate
the tree generation capability of our models. In the
following, we ﬁrst present details on model train-
ing and then present our results. We implemented
our models using the Torch library (Collobert et
al., 2011) and our code is available at https://
github.com/XingxingZhang/td-treelstm.

3.1 Training Details

We trained our model with back propagation
through time (Rumelhart et al., 1988) on an Nvidia

GPU Card with a mini-batch size of 64. The ob-
jective (NLL or NCE) was minimized by stochastic
gradient descent. Model parameters were uniformly
initialized in [−0.1, 0.1]. We used the NCE objec-
tive on the MSR sentence completion task (due to
the large size of this dataset) and the NLL objec-
tive on dependency parsing reranking. We used an
initial learning rate of 1.0 for all experiments and
when there was no signiﬁcant improvement in log-
likelihood on the validation set, the learning rate was
divided by 2 per epoch until convergence (Mikolov
et al., 2010). To alleviate the exploding gradients
problem, we rescaled the gradient g when the gradi-
ent norm ||g|| > 5 and set g = 5g
||g|| (Pascanu et al.,
2013; Sutskever et al., 2014). Dropout (Srivastava
et al., 2014) was applied to the 2-layer TREELSTM
and LDTREELSTM models. The word embedding
size was set to s = d/2 where d is the hidden unit
size.

3.2 Microsoft Sentence Completion Challenge

The task in the MSR Sentence Completion Chal-
lenge (Zweig and Burges, 2012) is to select the
correct missing word for 1,040 SAT-style test sen-
tences when presented with ﬁve candidate comple-
tions. The training set contains 522 novels from
the Project Gutenberg which we preprocessed as fol-
lows. After removing headers and footers from the
ﬁles, we tokenized and parsed the dataset into de-
pendency trees with the Stanford Core NLP toolkit
(Manning et al., 2014). The resulting training set
contained 49M words. We converted all words to
lower case and replaced those occurring ﬁve times
or less with UNK. The resulting vocabulary size
was 65,346 words. We randomly sampled 4,000
sentences from the training set as our validation set.
The literature describes two main approaches to
the sentence completion task based on word vectors
and language models. In vector-based approaches,
all words in the sentence and the ﬁve candidate
words are represented by a vector; the candidate
which has the highest average similarity with the
sentence words is selected as the answer. For lan-
guage model-based methods, the LM computes the
probability of a test sentence with each of the ﬁve
candidate words, and picks the candidate comple-
tion which gives the highest probability. Our model
belongs to this class of models.

|θ|

Accuracy

d

—
640
600

Model
Word Vector based Models
LSA
Skip-gram
IVLBL
Language Models
KN5
UDepNgram
LDepNgram
RNN
RNNME
depRNN+3gram
ldepRNN+4gram
LBL
LSTM
LSTM
LSTM
Bidirectional LSTM
Bidirectional LSTM
Bidirectional LSTM
Model Combinations
RNNMEs
—
Skip-gram + RNNMEs —
Our Models
TREELSTM
LDTREELSTM
TREELSTM
LDTREELSTM

—
—
—
300
300
100
200
300
300
400
450
200
300
400

300
300
400
400

—
102M
96.0M

—
—
—
48.1M
1120M
1014M
1029M
48.0M
29.9M
40.2M
45.3M
33.2M
50.1M
67.3M

—
—

31.6M
32.5M
43.1M
44.7M

49.0
48.0
55.5

40.0
48.3
50.0
45.0
49.3
53.5
50.7
54.7
55.00
57.02
55.96
48.46
49.90
48.65

55.4
58.9

55.29
57.79
56.73
60.67

Table 1: Model accuracy on the MSR sentence completion task.

The results of KN5, RNNME and RNNMEs are reported in

Mikolov (2012), LSA and RNN in Zweig et al. (2012), UDep-

Ngram and LDepNgram in Gubbins and Vlachos (2013), de-

pRNN+3gram and depRNN+4gram in Mirowski and Vlachos

(2015), LBL in Mnih and Teh (2012), Skip-gram and Skip-

gram+RNNMEs in Mikolov et al. (2013a), and IVLBL in Mnih
and Kavukcuoglu (2013); d is the hidden size and |θ| the num-
ber of parameters in a model.

Table 1 presents a summary of our results to-
gether with previoulsy published results. The best
performing word vector model is IVLBL (Mnih and
Kavukcuoglu, 2013) with an accuracy of 55.5, while
the best performing single language model is LBL
(Mnih and Teh, 2012) with an accuracy of 54.7.
Both approaches are based on the log-bilinear lan-
guage model (Mnih and Hinton, 2007). A combi-
nation of several recurrent neural networks and the
skip-gram model holds the state of the art with an
accuracy of 58.9 (Mikolov et al., 2013b). To fairly
compare with existing models, we restrict the layer

Test

Parser

Development
UAS
MSTParser-2nd
92.20
TREELSTM
92.51
92.64
TREELSTM*
LDTREELSTM 92.66
92.00
NN parser*
93.20
S-LSTM*

LAS
88.44
88.53
88.69
88.69
89.60
90.90
Table 2: Performance of TREELSTM and LDTREELSTM on

LAS UAS
91.63
88.78
91.79
89.07
91.97
89.09
91.99
89.14
91.80
89.70
93.10
90.90

reranking the top dependency trees produced by the 2nd order

MSTParser (McDonald and Pereira, 2006). Results for the NN

and S-LSTM parsers are reported in Chen and Manning (2014)

and Dyer et al. (2015), respectively. * indicates that the model

is initialized with pre-trained word vectors.

size of our models to 1. We observe that LDTREEL-
STM consistently outperforms TREELSTM, which
indicates the importance of modeling the interac-
In fact,
tion between left and right dependents.
LDTREELSTM (d = 400) achieves a new state-of-
the-art on this task, despite being a single model.
We also implement LSTM and bidirectional LSTM
language models.3 An LSTM with d = 400 out-
performs its smaller counterpart (d = 300), however
performance decreases with d = 450. The bidirec-
tional LSTM is worse than the LSTM (see Mnih
and Teh (2012) for a similar observation). The
best performing LSTM is worse than a LDTREEL-
STM (d = 300). The input and output embeddings
(We and Who) dominate the number of parame-
ters in all neural models except for RNNME, de-
pRNN+3gram and ldepRNN+4gram, which include
a ME model that contains 1 billion sparse n-gram
features (Mikolov, 2012; Mirowski and Vlachos,
2015). The number of parameters in TREELSTM
and LDTREELSTM is not much larger compared to
LSTM due to the tied We and Who matrices.

3.3 Dependency Parsing

In this section we demonstrate that our model can
be also used for parse reranking. This is not possi-
ble for sequence-based language models since they
cannot estimate the probability of a tree. We use
our models to rerank the top K dependency trees
produced by the second order MSTParser (McDon-

3LSTMs and BiLSTMs were also trained with NCE
(s = d/2; hyperparameters were tuned on the development set).

ald and Pereira, 2006).4 We follow closely the ex-
perimental setup of Chen and Manning (2014) and
Dyer et al. (2015). Speciﬁcally, we trained TREEL-
STM and LDTREELSTM on Penn Treebank sec-
tions 2–21. We used section 22 for development and
section 23 for testing. We adopted the Stanford ba-
sic dependency representations (De Marneffe et al.,
2006); part-of-speech tags were predicted with the
Stanford Tagger (Toutanova et al., 2003). We trained
TREELSTM and LDTREELSTM as language mod-
els (singletons were replaced with UNK) and did
not use any POS tags, dependency labels or com-
position features, whereas these features are used in
Chen and Manning (2014) and Dyer et al. (2015).
We tuned d, the number of layers, and K on the de-
velopment set.

Table 2 reports unlabeled attachment scores
(UAS) and labeled attachment scores (LAS) for
the MSTParser, TREELSTM (d = 300, 1 layer,
K = 2), and LDTREELSTM (d = 200, 2 layers,
K = 4). We also include the performance of two
neural network-based dependency parsers; Chen and
Manning (2014) use a neural network classiﬁer to
predict the correct transition (NN parser); Dyer et
al. (2015) also implement a transition-based depen-
dency parser using LSTMs to represent the contents
of the stack and buffer in a continuous space. As can
be seen, both TREELSTM and LDTREELSTM out-
perform the baseline MSTParser, with LDTREEL-
STM performing best. We also initialized the word
embedding matrix We with pre-trained GLOVE vec-
tors (Pennington et al., 2014). We obtained a slight
improvement over TREELSTM (TREELSTM* in
Table 2; d = 200, 2 layer, K = 4) but no im-
provement over LDTREELSTM. Finally, notice that
LDTREELSTM is slightly better than the NN parser
in terms of UAS but worse than the S-LSTM parser.
In the future, we would like to extend our model so
that it takes labeled dependency information into ac-
count.

3.4 Tree Generation

This section demonstrates how to use a trained
LDTREELSTM to generate tree samples. The gen-
eration starts at the ROOT node. At each time step t,
for each node wt, we add a new edge and node to

Figure 5: Generated dependency trees with LDTREELSTM

trained on the PTB.

the tree. Unfortunately during generation, we do not
know which type of edge to add. We therefore use
four binary classiﬁers (ADD-LEFT, ADD-RIGHT,
ADD-NX-LEFT and ADD-NX-RIGHT) to predict
whether we should add a LEFT, RIGHT, NX-LEFT
or NX-RIGHT edge.5 Then when a classiﬁer pre-
dicts true, we use the corresponding LSTM to gener-
ate a new node by sampling from the predicted word
distribution in Equation (3). The four classiﬁers take
the previous hidden state H[:,t(cid:48)] and the output em-
bedding of the current node Who · e(wt) as features.6
Speciﬁcally, we use a trained LDTREELSTM to
go through the training corpus and generate hidden
states and embeddings as input features; the corre-
sponding class labels (true and false) are “read off”
the training dependency trees. We use two-layer rec-
tiﬁer networks (Glorot et al., 2011) as the four clas-
siﬁers with a hidden size of 300. We use the same
LDTREELSTM model as in Section 3.3 to gener-
ate dependency trees. The classiﬁers were trained
using AdaGrad (Duchi et al., 2011) with a learning
rate of 0.01. The accuracies of ADD-LEFT, ADD-
RIGHT, ADD-NX-LEFT and ADD-NX-RIGHT are

5It is possible to get rid of the four classiﬁers by adding
START/STOP symbols when generating left and right depen-
dents as in (Eisner, 1996). We refrained from doing this for
computational reasons. For a sentence with N words, this ap-
proach will lead to 2N additional START/STOP symbols (with
one START and one STOP symbol for each word). Conse-
quently, the computational cost and memory consumption dur-
ing training will be three times as much rendering our model
less scalable.

6The input embeddings have lower dimensions and therefore

4http://www.seas.upenn.edu/ strctlrn/MSTParser

result in slightly worse classiﬁers.

94.3%, 92.6%, 93.4% and 96.0%, respectively. Fig-
ure 5 shows examples of generated trees.

4 Conclusions

In this paper we developed TREELSTM (and
LDTREELSTM), a neural network model architec-
ture, which is designed to predict tree structures
rather than linear sequences. Experimental results
on the MSR sentence completion task show that
LDTREELSTM is superior to sequential LSTMs.
Dependency parsing reranking experiments high-
light our model’s potential for dependency pars-
ing. Finally, the ability of our model to gener-
ate dependency trees holds promise for text gen-
eration applications such as sentence compression
and simpliﬁcation (Filippova et al., 2015). Although
our experiments have focused exclusively on depen-
dency trees, there is nothing inherent in our formu-
lation that disallows its application to other types of
tree structure such as constituent trees or even tax-
onomies.

Acknowledgments

We would like to thank Adam Lopez, Frank Keller,
Iain Murray, Li Dong, Brian Roark, and the NAACL
reviewers for their valuable feedback. Xingxing
Zhang gratefully acknowledges the ﬁnancial sup-
port of the China Scholarship Council (CSC). Liang
Lu is funded by the UK EPSRC Programme Grant
EP/I031022/1, Natural Speech Technology (NST).

References

[Bengio et al.2003] Yoshua Bengio, R´ejean Ducharme,
Pascal Vincent, and Christian Janvin. 2003. A neural
probabilistic language model. The Journal of Machine
Learning Research, 3:1137–1155.

[Charniak2001] Eugene Charniak.

Immediate-
head parsing for language models. In Proceedings of
the 39th Annual Meeting on Association for Compu-
tational Linguistics, pages 124–131. Association for
Computational Linguistics.

2001.

[Chelba and Jelinek2000] Ciprian Chelba and Frederick
Jelinek. 2000. Structured language modeling. Com-
puter Speech and Language, 14(4):283–332.

[Chelba et al.1997] Ciprian Chelba, David Engle, Freder-
ick Jelinek, Victor Jimenez, Sanjeev Khudanpur, Lidia
Mangu, Harry Printz, Eric Ristad, Ronald Rosenfeld,

Andreas Stolcke, et al.
formance of a dependency language model.
ROSPEECH. Citeseer.

1997. Structure and per-
In EU-

[Chen and Manning2014] Danqi Chen and Christopher
2014. A fast and accurate dependency
Manning.
In Proceedings of
parser using neural networks.
the 2014 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 740–750,
Doha, Qatar, October. Association for Computational
Linguistics.

[Chen et al.2015] X Chen, X Liu, MJF Gales, and
PC Woodland. 2015. Recurrent neural network lan-
guage model training with noise contrastive estimation
for speech recognition. In In 40th IEEE International
Conference on Accoustics, Speech and Signal Process-
ing, pages 5401–5405, Brisbane, Australia.

[Collobert et al.2011] Ronan

Collobert,

Koray
Kavukcuoglu, and Cl´ement Farabet. 2011. Torch7:
A matlab-like environment for machine learning.
In
BigLearn, NIPS Workshop, number EPFL-CONF-
192376.

[De Marneffe et al.2006] Marie-Catherine De Marneffe,
Bill MacCartney, Christopher D Manning, et al. 2006.
Generating typed dependency parses from phrase
structure parses. In Proceedings of LREC, volume 6,
pages 449–454.

[Duchi et al.2011] John Duchi, Elad Hazan, and Yoram
Singer. 2011. Adaptive subgradient methods for on-
line learning and stochastic optimization. The Journal
of Machine Learning Research, 12:2121–2159.

[Dyer et al.2015] Chris Dyer, Miguel Ballesteros, Wang
Ling, Austin Matthews, and Noah A. Smith. 2015.
Transition-based dependency parsing with stack long
In Proceedings of the 53rd An-
short-term memory.
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long Pa-
pers), pages 334–343, Beijing, China, July. Associa-
tion for Computational Linguistics.

[Eisner1996] Jason M Eisner. 1996. Three new prob-
abilistic models for dependency parsing: An explo-
ration. In Proceedings of the 16th conference on Com-
putational linguistics-Volume 1, pages 340–345. Asso-
ciation for Computational Linguistics.

[Emami et al.2003] Ahmad Emami, Peng Xu, and Fred-
erick Jelinek. 2003. Using a connectionist model in
In Proceedings
a syntactical based language model.
of the IEEE International Conference on Acoustics,
Speech, and Signal Processing, pages 372–375, Hong
Kong, China.

[Filippova et al.2015] Katja Filippova, Enrique Alfon-
seca, Carlos A Colmenares, Lukasz Kaiser, and Oriol
Vinyals. 2015. Sentence compression by deletion
with lstms. In EMNLP, pages 360–368.

[Glorot et al.2011] Xavier Glorot, Antoine Bordes, and
Yoshua Bengio. 2011. Deep sparse rectiﬁer neural
networks. In International Conference on Artiﬁcial In-
telligence and Statistics, pages 315–323.

[Graves et al.2013] Alan Graves, Abdel-rahman Mo-
hamed, and Geoffrey Hinton. 2013. Speech recogni-
tion with deep recurrent neural networks. In Acoustics,
Speech and Signal Processing (ICASSP), 2013 IEEE
International Conference on, pages 6645–6649. IEEE.
[Gubbins and Vlachos2013] Joseph Gubbins and Andreas
Vlachos. 2013. Dependency language models for sen-
tence completion. In EMNLP, pages 1405–1410, Seat-
tle, Washington, USA, October. Association for Com-
putational Linguistics.

[Gutmann and Hyv¨arinen2012] Michael U Gutmann and
Aapo Hyv¨arinen. 2012. Noise-contrastive estimation
of unnormalized statistical models, with applications
to natural image statistics. The Journal of Machine
Learning Research, 13(1):307–361.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural computation, 9(8):1735–1780.

[Hochreiter1998] Sepp Hochreiter. 1998. Vanishing gra-
dient problem during learning recurrent neural nets
and problem solutions. International Journal of Un-
certainty, Fuzziness and Knowledge-based Systems,
6(2):107–116.

[Manning et al.2014] Christopher D Manning, Mihai Sur-
deanu, John Bauer, Jenny Finkel, Steven J Bethard,
and David McClosky. 2014. The stanford corenlp
In Proceedings
natural language processing toolkit.
of 52nd Annual Meeting of the Association for Com-
putational Linguistics: System Demonstrations, pages
55–60.

[McDonald and Pereira2006] Ryan T McDonald and Fer-
nando CN Pereira. 2006. Online learning of approxi-
mate dependency parsing algorithms. In EACL.

[Mikolov et al.2010] Tomas Mikolov, Martin Karaﬁ´at,
Lukas Burget, Jan Cernock`y, and Sanjeev Khudan-
pur. 2010. Recurrent neural network based language
model. In INTERSPEECH 2010, 11th Annual Confer-
ence of the International Speech Communication As-
sociation, Makuhari, Chiba, Japan, September 26-30,
2010, pages 1045–1048.

[Mikolov et al.2013a] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013a. Efﬁcient esti-
In
mation of word representations in vector space.
Proceedings of the 2013 International Conference on
Learning Representations, Scottsdale, Arizona, USA.
Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013b.
Distributed representations of words and phrases and
In Advances in Neural Infor-
their compositionality.
mation Processing Systems 26, pages 3111–3119.

[Mikolov et al.2013b] Tomas Mikolov,

[Mikolov2012] Tomas Mikolov. 2012. Statistical Lan-
guage Models based on Neural Networks. Ph.D. the-
sis, Brno University of Technology.

[Mirowski and Vlachos2015] Piotr Mirowski and An-
dreas Vlachos. 2015. Dependency recurrent neural
In ACL,
language models for sentence completion.
pages 511–517, Beijing, China, July. Association for
Computational Linguistics.

[Mnih and Hinton2007] Andriy Mnih and Geoffrey Hin-
ton. 2007. Three new graphical models for statistical
In Proceedings of the 24th In-
language modelling.
ternational Conference on Machine Learning, pages
641–648.

[Mnih and Kavukcuoglu2013] Andriy Mnih and Koray
Kavukcuoglu. 2013. Learning word embeddings efﬁ-
ciently with noise-contrastive estimation. In Advances
in Neural Information Processing Systems 26, pages
2265–2273.

[Mnih and Teh2012] Andriy Mnih and Yee Whye Teh.
2012. A fast and simple algorithm for training neural
probabilistic language models. In Proceedings of the
29th International Conference on Machine Learning,
pages 1751–1758, Edinburgh, Scotland.

[Pascanu et al.2013] Razvan Pascanu, Tomas Mikolov,
and Yoshua Bengio. 2013. On the difﬁculty of train-
ing recurrent neural networks. In Proceedings of the
31st International Conference on Machine Learning,
pages 1310–1318, Atlanta, Georgia, USA.
Pennington,

Richard
Socher, and Christopher D Manning. 2014. Glove:
EMNLP,
Global vectors for word representation.
12:1532–1543.

[Pennington et al.2014] Jeffrey

[Pollack1990] Jordan B. Pollack. 1990. Recursive dis-
tributed representations. Artiﬁcial Intelligence, 1–
2(46):77–105.

[Roark2001] Brian Roark. 2001. Probabilistic top-down
parsing and language modeling. Computational lin-
guistics, 27(2):249–276.

[Rumelhart et al.1988] David E Rumelhart, Geoffrey E
Hinton, and Ronald J Williams. 1988. Learning repre-
sentations by back-propagating errors. Cognitive mod-
eling, 5:3.

[Sennrich2015] Rico Sennrich. 2015. Modelling and op-
timizing on syntactic n-grams for statistical machine
translation. Transactions of the Association for Com-
putational Linguistics, 3:169–182.

[Shen et al.2008] Libin Shen,

Jinxi Xu,

and Ralph
Weischedel. 2008. A new string-to-dependency ma-
chine translation algorithm with a target dependency
In Proceedings of ACL-08: HLT,
language model.
pages 577–585, Columbus, Ohio, USA.

[Socher et al.2011a] Richard Socher, Eric H. Huang, Jef-
frey Pennington, Christopher D. Manning, and An-
drew Ng. 2011a. Dynamic pooling and unfolding

Model? On the Future of Language Modeling for HLT,
pages 29–36, Montr´eal, Canada.

[Zweig et al.2012] Geoffrey Zweig, John C Platt, Christo-
pher Meek, Christopher JC Burges, Ainur Yessenalina,
2012. Computational approaches
and Qiang Liu.
In Proceedings of the 50th
to sentence completion.
Annual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 601–610.
Association for Computational Linguistics.

In
recursive autoencoders for paraphrase detection.
Advances in Neural Information Processing Systems,
pages 801–809.

[Socher et al.2011b] Richard Socher, Jeffrey Pennington,
Eric H. Huang, Andrew Y. Ng, and Christopher D.
Manning. 2011b. Semi-supervised recursive autoen-
coders for predicting sentiment distributions. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 151–161,
Edinburgh, Scotland, UK.

[Srivastava et al.2014] Nitish Srivastava, Geoffrey Hin-
ton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: A simple way to pre-
vent neural networks from overﬁtting. The Journal of
Machine Learning Research, 15(1):1929–1958.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and
Quoc VV Le. 2014. Sequence to sequence learning
with neural networks. In Advances in Neural Informa-
tion Processing Systems, pages 3104–3112.

[Tai et al.2015] Kai Sheng Tai, Richard Socher, and
Christopher D. Manning. 2015.
Improved semantic
representations from tree-structured long short-term
memory networks. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguis-
tics and the 7th International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers),
pages 1556–1566, Beijing, China, July. Association
for Computational Linguistics.

[Toutanova et al.2003] Kristina Toutanova, Dan Klein,
Christopher D Manning, and Yoram Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic de-
pendency network. In Proceedings of the 2003 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology-Volume 1, pages 173–180. Associa-
tion for Computational Linguistics.

[Vaswani et al.2013] Ashish Vaswani, Yinggong Zhao,
Victoria Fossum, and David Chiang. 2013. Decod-
ing with large-scale neural language models improves
translation. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
pages 1387–1392, Seattle, Washington, USA.

[Vinyals et al.2015] Oriol Vinyals, Alexander Toshev,
Samy Bengio, and Dumitru Erhan. 2015. Show and
tell: A neural image caption generator. In The IEEE
Conference on Computer Vision and Pattern Recogni-
tion, Boston, Massachusetts, USA.

[Zhang2009] Ying Zhang. 2009. Structured language
models for statistical machine translation. Ph.D. the-
sis, Johns Hopkins University.

[Zweig and Burges2012] Geoffrey Zweig and Chris J.C.
Burges. 2012. A challenge set for advancing language
In Proceedings of the NAACL-HLT 2012
modeling.
Workshop: Will We Ever Really Replace the N-gram

Top-down Tree Long Short-Term Memory Networks

Xingxing Zhang, Liang Lu and Mirella Lapata
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
{x.zhang,liang.lu}@ed.ac.uk,mlap@inf.ed.ac.uk

6
1
0
2
 
r
p
A
 
3
 
 
]
L
C
.
s
c
[
 
 
3
v
0
6
0
0
0
.
1
1
5
1
:
v
i
X
r
a

Abstract

Long Short-Term Memory (LSTM) networks,
a type of recurrent neural network with a
more complex computational unit, have been
successfully applied to a variety of sequence
modeling tasks. In this paper we develop Tree
Long Short-Term Memory (TREELSTM), a
neural network model based on LSTM, which
is designed to predict a tree rather than a lin-
ear sequence. TREELSTM deﬁnes the prob-
ability of a sentence by estimating the gener-
ation probability of its dependency tree. At
each time step, a node is generated based
on the representation of the generated sub-
tree. We further enhance the modeling power
of TREELSTM by explicitly representing the
correlations between left and right depen-
dents. Application of our model to the MSR
sentence completion challenge achieves re-
sults beyond the current state of the art. We
also report results on dependency parsing
reranking achieving competitive performance.

1

Introduction

Neural language models have been gaining increas-
ing attention as a competitive alternative to n-grams.
The main idea is to represent each word using a
real-valued feature vector capturing the contexts in
which it occurs. The conditional probability of the
next word is then modeled as a smooth function of
the feature vectors of the preceding words and the
In essence, similar representations are
next word.
learned for words found in similar contexts result-
ing in similar predictions for the next word. Previ-
ous approaches have mainly employed feed-forward

(Bengio et al., 2003; Mnih and Hinton, 2007) and
recurrent neural networks (Mikolov et al., 2010;
Mikolov, 2012) in order to map the feature vec-
tors of the context words to the distribution for the
next word. Recently, RNNs with Long Short-Term
Memory (LSTM) units (Hochreiter and Schmidhu-
ber, 1997; Hochreiter, 1998) have emerged as a pop-
ular architecture due to their strong ability to capture
long-term dependencies. LSTMs have been success-
fully applied to a variety of tasks ranging from ma-
chine translation (Sutskever et al., 2014), to speech
recognition (Graves et al., 2013), and image descrip-
tion generation (Vinyals et al., 2015).

Despite superior performance in many applica-
tions, neural language models essentially predict se-
quences of words. Many NLP tasks, however, ex-
ploit syntactic information operating over tree struc-
tures (e.g., dependency or constituent trees). In this
paper we develop a novel neural network model
which combines the advantages of the LSTM archi-
tecture and syntactic structure. Our model estimates
the probability of a sentence by estimating the gen-
eration probability of its dependency tree. Instead
of explicitly encoding tree structure as a set of fea-
tures, we use four LSTM networks to model four
types of dependency edges which altogether specify
how the tree is built. At each time step, one LSTM is
activated which predicts the next word conditioned
on the sub-tree generated so far. To learn the repre-
sentations of the conditioned sub-tree, we force the
four LSTMs to share their hidden layers. Our model
is also capable of generating trees just by sampling
from a trained model and can be seamlessly inte-
grated with text generation applications.

Our approach is related to but ultimately differ-
ent from recursive neural networks (Pollack, 1990)
a class of models which operate on structured in-
puts. Given a (binary) parse tree, they recursively
generate parent representations in a bottom-up fash-
ion, by combining tokens to produce representations
for phrases, and eventually the whole sentence. The
learned representations can be then used in classi-
ﬁcation tasks such as sentiment analysis (Socher et
al., 2011b) and paraphrase detection (Socher et al.,
2011a). Tai et al. (2015) learn distributed representa-
tions over syntactic trees by generalizing the LSTM
architecture to tree-structured network topologies.
The key feature of our model is not so much that
it can learn semantic representations of phrases or
sentences, but its ability to predict tree structure and
estimate its probability.

Syntactic language models have a long history
in NLP dating back to Chelba and Jelinek (2000)
(see also Roark (2001) and Charniak (2001)). These
models differ in how grammar structures in a parsing
tree are used when predicting the next word. Other
work develops dependency-based language models
for speciﬁc applications such as machine translation
(Shen et al., 2008; Zhang, 2009; Sennrich, 2015),
speech recognition (Chelba et al., 1997) or sentence
completion (Gubbins and Vlachos, 2013). All in-
stances of these models apply Markov assumptions
on the dependency tree, and adopt standard n-gram
smoothing methods for reliable parameter estima-
tion. Emami et al. (2003) and Sennrich (2015) esti-
mate the parameters of a structured language model
using feed-forward neural networks (Bengio et al.,
2003). Mirowski and Vlachos (2015) re-implement
the model of Gubbins and Vlachos (2013) with
RNNs. They view sentences as sequences of words
over a tree. While they ignore the tree structures
themselves, we model them explicitly.

Our model shares with other structured-based lan-
guage models the ability to take dependency infor-
mation into account. It differs in the following re-
spects: (a) it does not artiﬁcially restrict the depth
of the dependencies it considers and can thus be
viewed as an inﬁnite order dependency language
model; (b) it not only estimates the probability of a
string but is also capable of generating dependency
trees; (c) ﬁnally, contrary to previous dependency-
based language models which encode syntactic in-

formation as features, our model takes tree structure
into account more directly via representing different
types of dependency edges explicitly using LSTMs.
Therefore, there is no need to manually determine
which dependency tree features should be used or
how large the feature embeddings should be.

We evaluate our model on the MSR sentence com-
pletion challenge, a benchmark language modeling
dataset. Our results outperform the best published
results on this dataset. Since our model is a general
tree estimator, we also use it to rerank the top K de-
pendency trees from the (second order) MSTPasrser
and obtain performance on par with recently pro-
posed dependency parsers.

2 Tree Long Short-Term Memory

Networks

We seek to estimate the probability of a sentence by
estimating the generation probability of its depen-
dency tree. Syntactic information in our model is
represented in the form of dependency paths. In the
following, we ﬁrst describe our deﬁnition of depen-
dency path and based on it explain how the proba-
bility of a sentence is estimated.

2.1 Dependency Path

Generally speaking, a dependency path is the path
between ROOT and w consisting of the nodes on
the path and the edges connecting them. To rep-
resent dependency paths, we introduce four types
of edges which essentially deﬁne the “shape” of a
dependency tree. Let w0 denote a node in a tree
and w1, w2, . . . , wn its left dependents. As shown in
Figure 1, LEFT edge is the edge between w0 and
its ﬁrst left dependent denoted as (w0, w1). Let wk
(with 1 < k ≤ n) denote a non-ﬁrst left dependent
of w0. The edge from wk−1 to wk is a NX-LEFT
edge (NX stands for NEXT), where wk−1 is the right
adjacent sibling of wk. Note that the NX-LEFT edge
(wk−1, wk) replaces edge (w0, wk) (illustrated with a
dashed line in Figure 1) in the original dependency
tree. The modiﬁcation allows information to ﬂow
from w0 to wk through w1, . . . , wk−1 rather than di-
rectly from w0 to wk. RIGHT and NX-RIGHT edges
are deﬁned analogously for right dependents.

Given these four types of edges, dependency
paths (denoted as D(w)) can be deﬁned as follows

w0

wn

wk

wk−1

w1

LEFT

NX-LEFT

Figure 1: LEFT and NX-LEFT edges. Dotted line between
w1 and wk−1 (also between wk and wn) indicate that there may
be ≥ 0 nodes inbetween.

bearing in mind that the ﬁrst right dependent of
ROOT is its only dependent and that wp denotes the
parent of w. We use (. . . ) to denote a sequence,
where () is an empty sequence and (cid:107) is an operator
for concatenating two sequences.

(1) if w is ROOT, then D(w) = ()
(2) if w is a left dependent of wp
(a) if w is the ﬁrst

left dependent,

then

D(w) = D(wp)(cid:107)((cid:104)wp, LEFT(cid:105))

(b) if w is not the ﬁrst left dependent and ws is

its right adjacent sibling, then
D(w) = D(ws)(cid:107)((cid:104)ws, NX-LEFT(cid:105))

(3) if w is a right dependent of wp

(a) if w is the ﬁrst right dependent,
D(w) = D(wp)(cid:107)((cid:104)wp, RIGHT(cid:105))

then

(b) if w is not the ﬁrst right dependent and ws

is its left adjacent sibling, then
D(w) = D(ws)(cid:107)((cid:104)ws, NX-RIGHT(cid:105))

A dependency tree can be represented by the set of
its dependency paths which in turn can be used to
reconstruct the original tree.1

for

the ﬁrst

the moment

Dependency paths

two levels
the tree in Figure 2 are as follows (ig-
of
the subscripts which
noring for
D(sold) =
we explain in the next section).
((cid:104)ROOT, RIGHT(cid:105)) (see deﬁnitions (1) and (3a)),
D(year) = D(sold)(cid:107)((cid:104)sold, LEFT(cid:105))
(2a)),
D(manufacturer) = D(year)(cid:107)((cid:104)year, NX-LEFT(cid:105))
(see (2b)), D(cars) = D(sold)(cid:107)((cid:104)sold, RIGHT(cid:105))
(see (3a)), D(in) = D(cars)(cid:107)((cid:104)cars, NX-RIGHT(cid:105))
(according to (3b)).

(see

2.2 Tree Probability

ROOT

sold1

manufacturer3

year2

cars4

in5

The9

luxury8 auto7

last6

1,21410

U.S.11

the12

Figure 2: Dependency tree of the sentence The luxury auto

manufacturer last year sold 1,214 cars in the U.S. Subscripts

indicate breadth-ﬁrst traversal. ROOT has only one dependent

(i.e., sold ) which we view as its ﬁrst right dependent.

its corresponding tree T , P(S|T ). We view the prob-
ability computation of a dependency tree as a gener-
ation process. Speciﬁcally, we assume dependency
trees are constructed top-down, in a breadth-ﬁrst
manner. Generation starts at the ROOT node. For
each node at each level, ﬁrst its left dependents are
generated from closest to farthest and then the right
dependents (again from closest to farthest). The
same process is applied to the next node at the same
level or a node at the next level. Figure 2 shows the
breadth-ﬁrst traversal of a dependency tree.

Under the assumption that each word w in a de-
pendency tree is only conditioned on its dependency
path, the probability of a sentence S given its depen-
dency tree T is:

P(S|T ) = ∏

P(w|D(w))

(1)

w∈BFS(T )\ROOT

where D(w) is the dependency path of w. Note that
each word w is visited according to its breadth-ﬁrst
search order (BFS(T)) and the probability of ROOT
is ignored since every tree has one. The role of
ROOT in a dependency tree is the same as the begin
of sentence token (BOS) in a sentence. When com-
puting P(S|T ) (or P(S)), the probability of ROOT (or
BOS) is ignored (we assume it always exists), but is
used to predict other words. We explain in the next
section how TREELSTM estimates P(w|D(w)).

The core problem in syntax-based language model-
ing is to estimate the probability of sentence S given

2.3 Tree LSTMs

1Throughout this paper we assume all dependency trees are

projective.

A dependency path D(w) is subtree which we de-
note as a sequence of (cid:104)word, edge-type(cid:105) tuples. Our

w0

w0

w3

w2

w1

w4

w5

w6

Generated by four LSTMs

with tied We and tied Who

w2

w1

w0

G

E

N

-

L

G E N-R

w0

w4

w5

GEN-NX-L

GEN-NX-L

GEN-NX-R

GEN-NX-R

w2

w1

w3
w6
Figure 3: Generation process of left (w1, w2, w3) and right
(w4, w5, w6) dependents of tree node wo (top) using four LSTMs
(GEN-L, GEN-R, GEN-NX-L and GEN-NX-R). The model can

w4

w5

handle an arbitrary number of dependents due to GEN-NX-L

and GEN-NX-R.

innovation is to learn the representation of D(w) us-
ing four LSTMs. The four LSTMs (GEN-L, GEN-
R, GEN-NX-L and GEN-NX-R) are used to repre-
sent the four types of edges (LEFT, RIGHT, NX-
LEFT and NX-RIGHT) introduced earlier. GEN,
NX, L and R are shorthands for GENERATE, NEXT,
LEFT and RIGHT. At each time step, an LSTM is
chosen according to an edge-type; then the LSTM
takes a word as input and predicts/generates its de-
pendent or sibling. This process can be also viewed
as adding an edge and a node to a tree. Speciﬁ-
cally, LSTMs GEN-L and GEN-R are used to gen-
erate the ﬁrst left and right dependent of a node
(w1 and w4 in Figure 3). So, these two LSTMs
are responsible for going deeper in a tree. While
GEN-NX-L and GEN-NX-R generate the remain-
ing left/right dependents and therefore go wider in
a tree. As shown in Figure 3, w2 and w3 are gener-
ated by GEN-NX-L, whereas w5 and w6 are gener-
ated by GEN-NX-R. Note that the model can handle
any number of left or right dependents by applying
GEN-NX-L or GEN-NX-R multiple times.

We assume time steps correspond to the steps
taken by the breadth-ﬁrst traversal of the depen-
dency tree and the sentence has length n. At
time step t (1 ≤ t ≤ n), let (cid:104)wt(cid:48), zt(cid:105) denote the last
Subscripts t and t(cid:48) denote the
tuple in D(wt).
breadth-ﬁrst search order of wt and wt(cid:48), respectively.
zt ∈ {LEFT, RIGHT, NX-LEFT, NX-RIGHT} is the
edge type (see the deﬁnitions in Section 2.1). Let
We ∈ Rs×|V | denote the word embedding matrix and

Who ∈ R|V |×d the output matrix of our model, where
|V | is the vocabulary size, s the word embedding size
and d the hidden unit size. We use tied We and tied
Who for the four LSTMs to reduce the number of pa-
rameters in our model. The four LSTMs also share
their hidden states. Let H ∈ Rd×(n+1) denote the
shared hidden states of all time steps and e(wt) the
one-hot vector of wt. Then, H[:,t] represents D(wt)
at time step t, and the computation2 is:

xt = We · e(wt(cid:48))
ht = LSTMzt (xt, H[:,t(cid:48)])

H[:,t] = ht

yt = Who · ht

(2a)

(2b)

(2c)

(2d)

where the initial hidden state H[:, 0] is initialized to
a vector of small values such as 0.01. According to
Equation (2b), the model selects an LSTM based on
edge type zt. We describe the details of LSTMzt in
the next paragraph. The probability of wt given its
dependency path D(wt) is estimated by a softmax
function:

P(wt|D(wt)) =

(3)

exp(yt,wt )
|V |
k(cid:48)=1 exp(yt,k(cid:48))

∑

We must point out that although we use four jointly
trained LSTMs to encode the hidden states, the train-
ing and inference complexity of our model is no dif-
ferent from a regular LSTM, since at each time step
only one LSTM is working.

We implement LSTMz in Equation (2b) using a
deep LSTM (to simplify notation, from now on we
write z instead of zt). The inputs at time step t
are xt and ht(cid:48) (the hidden state of an earlier time
step t(cid:48)) and the output is ht (the hidden state of cur-
rent time step). Let L denote the layer number of
LSTMz and ˆhl
t the internal hidden state of the l-th
layer of the LSTMz at time step t, where xt is ˆh0
t and
ht(cid:48) is ˆhL
t(cid:48). The LSTM architecture introduces mul-
tiplicative gates and memory cells ˆcl
t (at l-th layer)
in order to address the vanishing gradient problem
which makes it difﬁcult for the standard RNN model
to learn long-distance correlations in a sequence.
Here, ˆcl
t is a linear combination of the current input
signal ut and an earlier memory cell ˆcl
t(cid:48). How much
input information ut will ﬂow into ˆcl
t is controlled

2We ignore all bias terms for notational simplicity.

w2

w1

w0

G

E

N

-

L

G E N-R

w0

w4

w5

GEN-NX-L

GEN-NX-L

GEN-NX-R

GEN-NX-R

LD

LD

w0

w4

w5

w6

w3

w2

w1

Figure 4: Generation of left and right dependents of node w0
according to LDTREELSTM.

by input gate it and how much of the earlier mem-
ory cell ˆcl
t(cid:48) will be forgotten is controlled by forget
gate ft. This process is computed as follows:

ux · ˆhl−1

ut = tanh(Wz,l
it = σ(Wz,l
ft = σ(Wz,l
t = ft (cid:12) ˆcl
ˆcl

ix · ˆhl−1
f x · ˆhl−1
t(cid:48) + it (cid:12) ut

t + Wz,l
uh · ˆhl
t(cid:48))
ih · ˆhl
t(cid:48))
f h · ˆhl
t(cid:48))

t + Wz,l
t + Wz,l

(4a)

(4b)

(4c)

(4d)

ux ∈ Rd×d (Wz,l

uh ∈ Rd×d are weight matrices for ut, Wz,l
ih are weight matrices for it and Wz,l

where Wz,l
ux ∈ Rd×s when l = 1) and
Wz,l
ix and
f x, and Wz,l
Wz,l
f h
are weight matrices for ft. σ is a sigmoid function
and (cid:12) the element-wise product.

Output gate ot controls how much information of

the cell ˆcl

t can be seen by other modules:

t + Wz,l
ox · ˆhl−1
ot = σ(Wz,l
ˆhl
t = ot (cid:12) tanh(ˆcl
t )

oh · ˆhl
t(cid:48))

(5a)

(5b)

Application of the above process to all layers L, will
yield ˆhL
t , which is ht. Note that in implementation,
all ˆcl
t (1 ≤ l ≤ L) at time step t are stored,
although we only care about ˆhL
t (ht).

t and ˆhl

2.4 Left Dependent Tree LSTMs

TREELSTM computes P(w|D(w)) based on the de-
pendency path D(w), which ignores the interaction
between left and right dependents on the same level.
In many cases, TREELSTM will use a verb to pre-
dict its object directly without knowing its subject.
For example, in Figure 2, TREELSTM uses (cid:104)ROOT,
RIGHT(cid:105) and (cid:104) sold, RIGHT (cid:105) to predict cars. This in-
formation is unfortunately not speciﬁc to cars (many
things can be sold, e.g., chocolates, candy). Consid-
ering manufacturer, the left dependent of sold would
help predict cars more accurately.

In order to jointly take left and right dependents
into account, we employ yet another LSTM, which
goes from the furthest left dependent to the closest
left dependent (LD is a shorthand for left depen-
dent). As shown in Figure 4, LD LSTM learns the
representation of all left dependents of a node w0;
this representation is then used to predict the ﬁrst
right dependent of the same node. Non-ﬁrst right de-
pendents can also leverage the representation of left
dependents, since this information is injected into
the hidden state of the ﬁrst right dependent and can
percolate all the way. Note that in order to retain the
generation capability of our model (Section 3.4), we
only allow right dependents to leverage left depen-
dents (they are generated before right dependents).

The computation of the LDTREELSTM is al-
the same as in TREELSTM except when
most
zt = GEN-R.
let vt be the cor-
responding left dependent sequence with length
K (vt = (w3, w2, w1) in Figure 4). Then, the hidden
state (qk) of vt at each time step k is:

In this case,

mk = We · e(vt,k)
qk = LSTMLD(mk, qk−1)

(6a)

(6b)

where qK is the representation for all left depen-
dents. Then, the computation of the current hid-
den state becomes (see Equation (2) for the original
computation):

rt =

(cid:21)
(cid:20)We · e(wt(cid:48))
qK

ht = LSTMGEN-R(rt, H[:,t(cid:48)])

(7a)

(7b)

where qK serves as additional input for LSTMGEN-R.
All other computational details are the same as in
TreeLSTM (see Section 2.3).

2.5 Model Training

On small scale datasets we employ Negative Log-
likelihood (NLL) as our training objective for both
TREELSTM and LDTREELSTM:

L NLL(θ) = −

log P(S|T )

(8)

1
|S | ∑

S∈S

where S is a sentence in the training set S , T is the
dependency tree of S and P(S|T ) is deﬁned as in
Equation (1).

On large scale datasets (e.g., with vocabulary
size of 65K), computing the output layer activa-
tions and the softmax function with NLL would
become prohibitively expensive.
Instead, we em-
ploy Noise Contrastive Estimation (NCE; Gutmann
and Hyv¨arinen (2012), Mnih and Teh (2012)) which
treats the normalization term ˆZ in ˆP(w|D(wt)) =
exp(Who[w,:]·ht )
as constant. The intuition behind NCE
ˆZ
is to discriminate between samples from a data dis-
tribution ˆP(w|D(wt)) and a known noise distribu-
tion Pn(w) via binary logistic regression. Assuming
that noise words are k times more frequent than real
words in the training set (Mnih and Teh, 2012), then
the probability of a word w being from our model
Pd(w,D(wt)) is
. We apply NCE to
large vocabulary models with the following training
objective:

ˆP(w|D(wt ))
ˆP(w|D(wt ))+kPn(w)

L NCE(θ) = −

log Pd(wt,D(wt))

(cid:18)

1
|S | ∑

T ∈S

|T |
∑
t=1

+

log[1 − Pd( ˜wt, j,D(wt))]

(cid:19)

k
∑
j=1

where ˜wt, j is a word sampled from the noise distri-
bution Pn(w). We use smoothed unigram frequen-
cies (exponentiating by 0.75) as the noise distribu-
tion Pn(w) (Mikolov et al., 2013b). We initialize
ln ˆZ = 9 as suggested in Chen et al. (2015), but in-
stead of keeping it ﬁxed we also learn ˆZ during train-
ing (Vaswani et al., 2013). We set k = 20.

3 Experiments

We assess the performance of our model on two
tasks: the Microsoft Research (MSR) sentence com-
pletion challenge (Zweig and Burges, 2012), and de-
pendency parsing reranking. We also demonstrate
the tree generation capability of our models. In the
following, we ﬁrst present details on model train-
ing and then present our results. We implemented
our models using the Torch library (Collobert et
al., 2011) and our code is available at https://
github.com/XingxingZhang/td-treelstm.

3.1 Training Details

We trained our model with back propagation
through time (Rumelhart et al., 1988) on an Nvidia

GPU Card with a mini-batch size of 64. The ob-
jective (NLL or NCE) was minimized by stochastic
gradient descent. Model parameters were uniformly
initialized in [−0.1, 0.1]. We used the NCE objec-
tive on the MSR sentence completion task (due to
the large size of this dataset) and the NLL objec-
tive on dependency parsing reranking. We used an
initial learning rate of 1.0 for all experiments and
when there was no signiﬁcant improvement in log-
likelihood on the validation set, the learning rate was
divided by 2 per epoch until convergence (Mikolov
et al., 2010). To alleviate the exploding gradients
problem, we rescaled the gradient g when the gradi-
ent norm ||g|| > 5 and set g = 5g
||g|| (Pascanu et al.,
2013; Sutskever et al., 2014). Dropout (Srivastava
et al., 2014) was applied to the 2-layer TREELSTM
and LDTREELSTM models. The word embedding
size was set to s = d/2 where d is the hidden unit
size.

3.2 Microsoft Sentence Completion Challenge

The task in the MSR Sentence Completion Chal-
lenge (Zweig and Burges, 2012) is to select the
correct missing word for 1,040 SAT-style test sen-
tences when presented with ﬁve candidate comple-
tions. The training set contains 522 novels from
the Project Gutenberg which we preprocessed as fol-
lows. After removing headers and footers from the
ﬁles, we tokenized and parsed the dataset into de-
pendency trees with the Stanford Core NLP toolkit
(Manning et al., 2014). The resulting training set
contained 49M words. We converted all words to
lower case and replaced those occurring ﬁve times
or less with UNK. The resulting vocabulary size
was 65,346 words. We randomly sampled 4,000
sentences from the training set as our validation set.
The literature describes two main approaches to
the sentence completion task based on word vectors
and language models. In vector-based approaches,
all words in the sentence and the ﬁve candidate
words are represented by a vector; the candidate
which has the highest average similarity with the
sentence words is selected as the answer. For lan-
guage model-based methods, the LM computes the
probability of a test sentence with each of the ﬁve
candidate words, and picks the candidate comple-
tion which gives the highest probability. Our model
belongs to this class of models.

|θ|

Accuracy

d

—
640
600

Model
Word Vector based Models
LSA
Skip-gram
IVLBL
Language Models
KN5
UDepNgram
LDepNgram
RNN
RNNME
depRNN+3gram
ldepRNN+4gram
LBL
LSTM
LSTM
LSTM
Bidirectional LSTM
Bidirectional LSTM
Bidirectional LSTM
Model Combinations
RNNMEs
—
Skip-gram + RNNMEs —
Our Models
TREELSTM
LDTREELSTM
TREELSTM
LDTREELSTM

—
—
—
300
300
100
200
300
300
400
450
200
300
400

300
300
400
400

—
102M
96.0M

—
—
—
48.1M
1120M
1014M
1029M
48.0M
29.9M
40.2M
45.3M
33.2M
50.1M
67.3M

—
—

31.6M
32.5M
43.1M
44.7M

49.0
48.0
55.5

40.0
48.3
50.0
45.0
49.3
53.5
50.7
54.7
55.00
57.02
55.96
48.46
49.90
48.65

55.4
58.9

55.29
57.79
56.73
60.67

Table 1: Model accuracy on the MSR sentence completion task.

The results of KN5, RNNME and RNNMEs are reported in

Mikolov (2012), LSA and RNN in Zweig et al. (2012), UDep-

Ngram and LDepNgram in Gubbins and Vlachos (2013), de-

pRNN+3gram and depRNN+4gram in Mirowski and Vlachos

(2015), LBL in Mnih and Teh (2012), Skip-gram and Skip-

gram+RNNMEs in Mikolov et al. (2013a), and IVLBL in Mnih
and Kavukcuoglu (2013); d is the hidden size and |θ| the num-
ber of parameters in a model.

Table 1 presents a summary of our results to-
gether with previoulsy published results. The best
performing word vector model is IVLBL (Mnih and
Kavukcuoglu, 2013) with an accuracy of 55.5, while
the best performing single language model is LBL
(Mnih and Teh, 2012) with an accuracy of 54.7.
Both approaches are based on the log-bilinear lan-
guage model (Mnih and Hinton, 2007). A combi-
nation of several recurrent neural networks and the
skip-gram model holds the state of the art with an
accuracy of 58.9 (Mikolov et al., 2013b). To fairly
compare with existing models, we restrict the layer

Test

Parser

Development
UAS
MSTParser-2nd
92.20
TREELSTM
92.51
92.64
TREELSTM*
LDTREELSTM 92.66
92.00
NN parser*
93.20
S-LSTM*

LAS
88.44
88.53
88.69
88.69
89.60
90.90
Table 2: Performance of TREELSTM and LDTREELSTM on

LAS UAS
91.63
88.78
91.79
89.07
91.97
89.09
91.99
89.14
91.80
89.70
93.10
90.90

reranking the top dependency trees produced by the 2nd order

MSTParser (McDonald and Pereira, 2006). Results for the NN

and S-LSTM parsers are reported in Chen and Manning (2014)

and Dyer et al. (2015), respectively. * indicates that the model

is initialized with pre-trained word vectors.

size of our models to 1. We observe that LDTREEL-
STM consistently outperforms TREELSTM, which
indicates the importance of modeling the interac-
In fact,
tion between left and right dependents.
LDTREELSTM (d = 400) achieves a new state-of-
the-art on this task, despite being a single model.
We also implement LSTM and bidirectional LSTM
language models.3 An LSTM with d = 400 out-
performs its smaller counterpart (d = 300), however
performance decreases with d = 450. The bidirec-
tional LSTM is worse than the LSTM (see Mnih
and Teh (2012) for a similar observation). The
best performing LSTM is worse than a LDTREEL-
STM (d = 300). The input and output embeddings
(We and Who) dominate the number of parame-
ters in all neural models except for RNNME, de-
pRNN+3gram and ldepRNN+4gram, which include
a ME model that contains 1 billion sparse n-gram
features (Mikolov, 2012; Mirowski and Vlachos,
2015). The number of parameters in TREELSTM
and LDTREELSTM is not much larger compared to
LSTM due to the tied We and Who matrices.

3.3 Dependency Parsing

In this section we demonstrate that our model can
be also used for parse reranking. This is not possi-
ble for sequence-based language models since they
cannot estimate the probability of a tree. We use
our models to rerank the top K dependency trees
produced by the second order MSTParser (McDon-

3LSTMs and BiLSTMs were also trained with NCE
(s = d/2; hyperparameters were tuned on the development set).

ald and Pereira, 2006).4 We follow closely the ex-
perimental setup of Chen and Manning (2014) and
Dyer et al. (2015). Speciﬁcally, we trained TREEL-
STM and LDTREELSTM on Penn Treebank sec-
tions 2–21. We used section 22 for development and
section 23 for testing. We adopted the Stanford ba-
sic dependency representations (De Marneffe et al.,
2006); part-of-speech tags were predicted with the
Stanford Tagger (Toutanova et al., 2003). We trained
TREELSTM and LDTREELSTM as language mod-
els (singletons were replaced with UNK) and did
not use any POS tags, dependency labels or com-
position features, whereas these features are used in
Chen and Manning (2014) and Dyer et al. (2015).
We tuned d, the number of layers, and K on the de-
velopment set.

Table 2 reports unlabeled attachment scores
(UAS) and labeled attachment scores (LAS) for
the MSTParser, TREELSTM (d = 300, 1 layer,
K = 2), and LDTREELSTM (d = 200, 2 layers,
K = 4). We also include the performance of two
neural network-based dependency parsers; Chen and
Manning (2014) use a neural network classiﬁer to
predict the correct transition (NN parser); Dyer et
al. (2015) also implement a transition-based depen-
dency parser using LSTMs to represent the contents
of the stack and buffer in a continuous space. As can
be seen, both TREELSTM and LDTREELSTM out-
perform the baseline MSTParser, with LDTREEL-
STM performing best. We also initialized the word
embedding matrix We with pre-trained GLOVE vec-
tors (Pennington et al., 2014). We obtained a slight
improvement over TREELSTM (TREELSTM* in
Table 2; d = 200, 2 layer, K = 4) but no im-
provement over LDTREELSTM. Finally, notice that
LDTREELSTM is slightly better than the NN parser
in terms of UAS but worse than the S-LSTM parser.
In the future, we would like to extend our model so
that it takes labeled dependency information into ac-
count.

3.4 Tree Generation

This section demonstrates how to use a trained
LDTREELSTM to generate tree samples. The gen-
eration starts at the ROOT node. At each time step t,
for each node wt, we add a new edge and node to

Figure 5: Generated dependency trees with LDTREELSTM

trained on the PTB.

the tree. Unfortunately during generation, we do not
know which type of edge to add. We therefore use
four binary classiﬁers (ADD-LEFT, ADD-RIGHT,
ADD-NX-LEFT and ADD-NX-RIGHT) to predict
whether we should add a LEFT, RIGHT, NX-LEFT
or NX-RIGHT edge.5 Then when a classiﬁer pre-
dicts true, we use the corresponding LSTM to gener-
ate a new node by sampling from the predicted word
distribution in Equation (3). The four classiﬁers take
the previous hidden state H[:,t(cid:48)] and the output em-
bedding of the current node Who · e(wt) as features.6
Speciﬁcally, we use a trained LDTREELSTM to
go through the training corpus and generate hidden
states and embeddings as input features; the corre-
sponding class labels (true and false) are “read off”
the training dependency trees. We use two-layer rec-
tiﬁer networks (Glorot et al., 2011) as the four clas-
siﬁers with a hidden size of 300. We use the same
LDTREELSTM model as in Section 3.3 to gener-
ate dependency trees. The classiﬁers were trained
using AdaGrad (Duchi et al., 2011) with a learning
rate of 0.01. The accuracies of ADD-LEFT, ADD-
RIGHT, ADD-NX-LEFT and ADD-NX-RIGHT are

5It is possible to get rid of the four classiﬁers by adding
START/STOP symbols when generating left and right depen-
dents as in (Eisner, 1996). We refrained from doing this for
computational reasons. For a sentence with N words, this ap-
proach will lead to 2N additional START/STOP symbols (with
one START and one STOP symbol for each word). Conse-
quently, the computational cost and memory consumption dur-
ing training will be three times as much rendering our model
less scalable.

6The input embeddings have lower dimensions and therefore

4http://www.seas.upenn.edu/ strctlrn/MSTParser

result in slightly worse classiﬁers.

94.3%, 92.6%, 93.4% and 96.0%, respectively. Fig-
ure 5 shows examples of generated trees.

4 Conclusions

In this paper we developed TREELSTM (and
LDTREELSTM), a neural network model architec-
ture, which is designed to predict tree structures
rather than linear sequences. Experimental results
on the MSR sentence completion task show that
LDTREELSTM is superior to sequential LSTMs.
Dependency parsing reranking experiments high-
light our model’s potential for dependency pars-
ing. Finally, the ability of our model to gener-
ate dependency trees holds promise for text gen-
eration applications such as sentence compression
and simpliﬁcation (Filippova et al., 2015). Although
our experiments have focused exclusively on depen-
dency trees, there is nothing inherent in our formu-
lation that disallows its application to other types of
tree structure such as constituent trees or even tax-
onomies.

Acknowledgments

We would like to thank Adam Lopez, Frank Keller,
Iain Murray, Li Dong, Brian Roark, and the NAACL
reviewers for their valuable feedback. Xingxing
Zhang gratefully acknowledges the ﬁnancial sup-
port of the China Scholarship Council (CSC). Liang
Lu is funded by the UK EPSRC Programme Grant
EP/I031022/1, Natural Speech Technology (NST).

References

[Bengio et al.2003] Yoshua Bengio, R´ejean Ducharme,
Pascal Vincent, and Christian Janvin. 2003. A neural
probabilistic language model. The Journal of Machine
Learning Research, 3:1137–1155.

[Charniak2001] Eugene Charniak.

Immediate-
head parsing for language models. In Proceedings of
the 39th Annual Meeting on Association for Compu-
tational Linguistics, pages 124–131. Association for
Computational Linguistics.

2001.

[Chelba and Jelinek2000] Ciprian Chelba and Frederick
Jelinek. 2000. Structured language modeling. Com-
puter Speech and Language, 14(4):283–332.

[Chelba et al.1997] Ciprian Chelba, David Engle, Freder-
ick Jelinek, Victor Jimenez, Sanjeev Khudanpur, Lidia
Mangu, Harry Printz, Eric Ristad, Ronald Rosenfeld,

Andreas Stolcke, et al.
formance of a dependency language model.
ROSPEECH. Citeseer.

1997. Structure and per-
In EU-

[Chen and Manning2014] Danqi Chen and Christopher
2014. A fast and accurate dependency
Manning.
In Proceedings of
parser using neural networks.
the 2014 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 740–750,
Doha, Qatar, October. Association for Computational
Linguistics.

[Chen et al.2015] X Chen, X Liu, MJF Gales, and
PC Woodland. 2015. Recurrent neural network lan-
guage model training with noise contrastive estimation
for speech recognition. In In 40th IEEE International
Conference on Accoustics, Speech and Signal Process-
ing, pages 5401–5405, Brisbane, Australia.

[Collobert et al.2011] Ronan

Collobert,

Koray
Kavukcuoglu, and Cl´ement Farabet. 2011. Torch7:
A matlab-like environment for machine learning.
In
BigLearn, NIPS Workshop, number EPFL-CONF-
192376.

[De Marneffe et al.2006] Marie-Catherine De Marneffe,
Bill MacCartney, Christopher D Manning, et al. 2006.
Generating typed dependency parses from phrase
structure parses. In Proceedings of LREC, volume 6,
pages 449–454.

[Duchi et al.2011] John Duchi, Elad Hazan, and Yoram
Singer. 2011. Adaptive subgradient methods for on-
line learning and stochastic optimization. The Journal
of Machine Learning Research, 12:2121–2159.

[Dyer et al.2015] Chris Dyer, Miguel Ballesteros, Wang
Ling, Austin Matthews, and Noah A. Smith. 2015.
Transition-based dependency parsing with stack long
In Proceedings of the 53rd An-
short-term memory.
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long Pa-
pers), pages 334–343, Beijing, China, July. Associa-
tion for Computational Linguistics.

[Eisner1996] Jason M Eisner. 1996. Three new prob-
abilistic models for dependency parsing: An explo-
ration. In Proceedings of the 16th conference on Com-
putational linguistics-Volume 1, pages 340–345. Asso-
ciation for Computational Linguistics.

[Emami et al.2003] Ahmad Emami, Peng Xu, and Fred-
erick Jelinek. 2003. Using a connectionist model in
In Proceedings
a syntactical based language model.
of the IEEE International Conference on Acoustics,
Speech, and Signal Processing, pages 372–375, Hong
Kong, China.

[Filippova et al.2015] Katja Filippova, Enrique Alfon-
seca, Carlos A Colmenares, Lukasz Kaiser, and Oriol
Vinyals. 2015. Sentence compression by deletion
with lstms. In EMNLP, pages 360–368.

[Glorot et al.2011] Xavier Glorot, Antoine Bordes, and
Yoshua Bengio. 2011. Deep sparse rectiﬁer neural
networks. In International Conference on Artiﬁcial In-
telligence and Statistics, pages 315–323.

[Graves et al.2013] Alan Graves, Abdel-rahman Mo-
hamed, and Geoffrey Hinton. 2013. Speech recogni-
tion with deep recurrent neural networks. In Acoustics,
Speech and Signal Processing (ICASSP), 2013 IEEE
International Conference on, pages 6645–6649. IEEE.
[Gubbins and Vlachos2013] Joseph Gubbins and Andreas
Vlachos. 2013. Dependency language models for sen-
tence completion. In EMNLP, pages 1405–1410, Seat-
tle, Washington, USA, October. Association for Com-
putational Linguistics.

[Gutmann and Hyv¨arinen2012] Michael U Gutmann and
Aapo Hyv¨arinen. 2012. Noise-contrastive estimation
of unnormalized statistical models, with applications
to natural image statistics. The Journal of Machine
Learning Research, 13(1):307–361.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural computation, 9(8):1735–1780.

[Hochreiter1998] Sepp Hochreiter. 1998. Vanishing gra-
dient problem during learning recurrent neural nets
and problem solutions. International Journal of Un-
certainty, Fuzziness and Knowledge-based Systems,
6(2):107–116.

[Manning et al.2014] Christopher D Manning, Mihai Sur-
deanu, John Bauer, Jenny Finkel, Steven J Bethard,
and David McClosky. 2014. The stanford corenlp
In Proceedings
natural language processing toolkit.
of 52nd Annual Meeting of the Association for Com-
putational Linguistics: System Demonstrations, pages
55–60.

[McDonald and Pereira2006] Ryan T McDonald and Fer-
nando CN Pereira. 2006. Online learning of approxi-
mate dependency parsing algorithms. In EACL.

[Mikolov et al.2010] Tomas Mikolov, Martin Karaﬁ´at,
Lukas Burget, Jan Cernock`y, and Sanjeev Khudan-
pur. 2010. Recurrent neural network based language
model. In INTERSPEECH 2010, 11th Annual Confer-
ence of the International Speech Communication As-
sociation, Makuhari, Chiba, Japan, September 26-30,
2010, pages 1045–1048.

[Mikolov et al.2013a] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013a. Efﬁcient esti-
In
mation of word representations in vector space.
Proceedings of the 2013 International Conference on
Learning Representations, Scottsdale, Arizona, USA.
Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013b.
Distributed representations of words and phrases and
In Advances in Neural Infor-
their compositionality.
mation Processing Systems 26, pages 3111–3119.

[Mikolov et al.2013b] Tomas Mikolov,

[Mikolov2012] Tomas Mikolov. 2012. Statistical Lan-
guage Models based on Neural Networks. Ph.D. the-
sis, Brno University of Technology.

[Mirowski and Vlachos2015] Piotr Mirowski and An-
dreas Vlachos. 2015. Dependency recurrent neural
In ACL,
language models for sentence completion.
pages 511–517, Beijing, China, July. Association for
Computational Linguistics.

[Mnih and Hinton2007] Andriy Mnih and Geoffrey Hin-
ton. 2007. Three new graphical models for statistical
In Proceedings of the 24th In-
language modelling.
ternational Conference on Machine Learning, pages
641–648.

[Mnih and Kavukcuoglu2013] Andriy Mnih and Koray
Kavukcuoglu. 2013. Learning word embeddings efﬁ-
ciently with noise-contrastive estimation. In Advances
in Neural Information Processing Systems 26, pages
2265–2273.

[Mnih and Teh2012] Andriy Mnih and Yee Whye Teh.
2012. A fast and simple algorithm for training neural
probabilistic language models. In Proceedings of the
29th International Conference on Machine Learning,
pages 1751–1758, Edinburgh, Scotland.

[Pascanu et al.2013] Razvan Pascanu, Tomas Mikolov,
and Yoshua Bengio. 2013. On the difﬁculty of train-
ing recurrent neural networks. In Proceedings of the
31st International Conference on Machine Learning,
pages 1310–1318, Atlanta, Georgia, USA.
Pennington,

Richard
Socher, and Christopher D Manning. 2014. Glove:
EMNLP,
Global vectors for word representation.
12:1532–1543.

[Pennington et al.2014] Jeffrey

[Pollack1990] Jordan B. Pollack. 1990. Recursive dis-
tributed representations. Artiﬁcial Intelligence, 1–
2(46):77–105.

[Roark2001] Brian Roark. 2001. Probabilistic top-down
parsing and language modeling. Computational lin-
guistics, 27(2):249–276.

[Rumelhart et al.1988] David E Rumelhart, Geoffrey E
Hinton, and Ronald J Williams. 1988. Learning repre-
sentations by back-propagating errors. Cognitive mod-
eling, 5:3.

[Sennrich2015] Rico Sennrich. 2015. Modelling and op-
timizing on syntactic n-grams for statistical machine
translation. Transactions of the Association for Com-
putational Linguistics, 3:169–182.

[Shen et al.2008] Libin Shen,

Jinxi Xu,

and Ralph
Weischedel. 2008. A new string-to-dependency ma-
chine translation algorithm with a target dependency
In Proceedings of ACL-08: HLT,
language model.
pages 577–585, Columbus, Ohio, USA.

[Socher et al.2011a] Richard Socher, Eric H. Huang, Jef-
frey Pennington, Christopher D. Manning, and An-
drew Ng. 2011a. Dynamic pooling and unfolding

Model? On the Future of Language Modeling for HLT,
pages 29–36, Montr´eal, Canada.

[Zweig et al.2012] Geoffrey Zweig, John C Platt, Christo-
pher Meek, Christopher JC Burges, Ainur Yessenalina,
2012. Computational approaches
and Qiang Liu.
In Proceedings of the 50th
to sentence completion.
Annual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 601–610.
Association for Computational Linguistics.

In
recursive autoencoders for paraphrase detection.
Advances in Neural Information Processing Systems,
pages 801–809.

[Socher et al.2011b] Richard Socher, Jeffrey Pennington,
Eric H. Huang, Andrew Y. Ng, and Christopher D.
Manning. 2011b. Semi-supervised recursive autoen-
coders for predicting sentiment distributions. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 151–161,
Edinburgh, Scotland, UK.

[Srivastava et al.2014] Nitish Srivastava, Geoffrey Hin-
ton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: A simple way to pre-
vent neural networks from overﬁtting. The Journal of
Machine Learning Research, 15(1):1929–1958.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and
Quoc VV Le. 2014. Sequence to sequence learning
with neural networks. In Advances in Neural Informa-
tion Processing Systems, pages 3104–3112.

[Tai et al.2015] Kai Sheng Tai, Richard Socher, and
Christopher D. Manning. 2015.
Improved semantic
representations from tree-structured long short-term
memory networks. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguis-
tics and the 7th International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers),
pages 1556–1566, Beijing, China, July. Association
for Computational Linguistics.

[Toutanova et al.2003] Kristina Toutanova, Dan Klein,
Christopher D Manning, and Yoram Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic de-
pendency network. In Proceedings of the 2003 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology-Volume 1, pages 173–180. Associa-
tion for Computational Linguistics.

[Vaswani et al.2013] Ashish Vaswani, Yinggong Zhao,
Victoria Fossum, and David Chiang. 2013. Decod-
ing with large-scale neural language models improves
translation. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
pages 1387–1392, Seattle, Washington, USA.

[Vinyals et al.2015] Oriol Vinyals, Alexander Toshev,
Samy Bengio, and Dumitru Erhan. 2015. Show and
tell: A neural image caption generator. In The IEEE
Conference on Computer Vision and Pattern Recogni-
tion, Boston, Massachusetts, USA.

[Zhang2009] Ying Zhang. 2009. Structured language
models for statistical machine translation. Ph.D. the-
sis, Johns Hopkins University.

[Zweig and Burges2012] Geoffrey Zweig and Chris J.C.
Burges. 2012. A challenge set for advancing language
In Proceedings of the NAACL-HLT 2012
modeling.
Workshop: Will We Ever Really Replace the N-gram

Top-down Tree Long Short-Term Memory Networks

Xingxing Zhang, Liang Lu and Mirella Lapata
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
{x.zhang,liang.lu}@ed.ac.uk,mlap@inf.ed.ac.uk

6
1
0
2
 
r
p
A
 
3
 
 
]
L
C
.
s
c
[
 
 
3
v
0
6
0
0
0
.
1
1
5
1
:
v
i
X
r
a

Abstract

Long Short-Term Memory (LSTM) networks,
a type of recurrent neural network with a
more complex computational unit, have been
successfully applied to a variety of sequence
modeling tasks. In this paper we develop Tree
Long Short-Term Memory (TREELSTM), a
neural network model based on LSTM, which
is designed to predict a tree rather than a lin-
ear sequence. TREELSTM deﬁnes the prob-
ability of a sentence by estimating the gener-
ation probability of its dependency tree. At
each time step, a node is generated based
on the representation of the generated sub-
tree. We further enhance the modeling power
of TREELSTM by explicitly representing the
correlations between left and right depen-
dents. Application of our model to the MSR
sentence completion challenge achieves re-
sults beyond the current state of the art. We
also report results on dependency parsing
reranking achieving competitive performance.

1

Introduction

Neural language models have been gaining increas-
ing attention as a competitive alternative to n-grams.
The main idea is to represent each word using a
real-valued feature vector capturing the contexts in
which it occurs. The conditional probability of the
next word is then modeled as a smooth function of
the feature vectors of the preceding words and the
In essence, similar representations are
next word.
learned for words found in similar contexts result-
ing in similar predictions for the next word. Previ-
ous approaches have mainly employed feed-forward

(Bengio et al., 2003; Mnih and Hinton, 2007) and
recurrent neural networks (Mikolov et al., 2010;
Mikolov, 2012) in order to map the feature vec-
tors of the context words to the distribution for the
next word. Recently, RNNs with Long Short-Term
Memory (LSTM) units (Hochreiter and Schmidhu-
ber, 1997; Hochreiter, 1998) have emerged as a pop-
ular architecture due to their strong ability to capture
long-term dependencies. LSTMs have been success-
fully applied to a variety of tasks ranging from ma-
chine translation (Sutskever et al., 2014), to speech
recognition (Graves et al., 2013), and image descrip-
tion generation (Vinyals et al., 2015).

Despite superior performance in many applica-
tions, neural language models essentially predict se-
quences of words. Many NLP tasks, however, ex-
ploit syntactic information operating over tree struc-
tures (e.g., dependency or constituent trees). In this
paper we develop a novel neural network model
which combines the advantages of the LSTM archi-
tecture and syntactic structure. Our model estimates
the probability of a sentence by estimating the gen-
eration probability of its dependency tree. Instead
of explicitly encoding tree structure as a set of fea-
tures, we use four LSTM networks to model four
types of dependency edges which altogether specify
how the tree is built. At each time step, one LSTM is
activated which predicts the next word conditioned
on the sub-tree generated so far. To learn the repre-
sentations of the conditioned sub-tree, we force the
four LSTMs to share their hidden layers. Our model
is also capable of generating trees just by sampling
from a trained model and can be seamlessly inte-
grated with text generation applications.

Our approach is related to but ultimately differ-
ent from recursive neural networks (Pollack, 1990)
a class of models which operate on structured in-
puts. Given a (binary) parse tree, they recursively
generate parent representations in a bottom-up fash-
ion, by combining tokens to produce representations
for phrases, and eventually the whole sentence. The
learned representations can be then used in classi-
ﬁcation tasks such as sentiment analysis (Socher et
al., 2011b) and paraphrase detection (Socher et al.,
2011a). Tai et al. (2015) learn distributed representa-
tions over syntactic trees by generalizing the LSTM
architecture to tree-structured network topologies.
The key feature of our model is not so much that
it can learn semantic representations of phrases or
sentences, but its ability to predict tree structure and
estimate its probability.

Syntactic language models have a long history
in NLP dating back to Chelba and Jelinek (2000)
(see also Roark (2001) and Charniak (2001)). These
models differ in how grammar structures in a parsing
tree are used when predicting the next word. Other
work develops dependency-based language models
for speciﬁc applications such as machine translation
(Shen et al., 2008; Zhang, 2009; Sennrich, 2015),
speech recognition (Chelba et al., 1997) or sentence
completion (Gubbins and Vlachos, 2013). All in-
stances of these models apply Markov assumptions
on the dependency tree, and adopt standard n-gram
smoothing methods for reliable parameter estima-
tion. Emami et al. (2003) and Sennrich (2015) esti-
mate the parameters of a structured language model
using feed-forward neural networks (Bengio et al.,
2003). Mirowski and Vlachos (2015) re-implement
the model of Gubbins and Vlachos (2013) with
RNNs. They view sentences as sequences of words
over a tree. While they ignore the tree structures
themselves, we model them explicitly.

Our model shares with other structured-based lan-
guage models the ability to take dependency infor-
mation into account. It differs in the following re-
spects: (a) it does not artiﬁcially restrict the depth
of the dependencies it considers and can thus be
viewed as an inﬁnite order dependency language
model; (b) it not only estimates the probability of a
string but is also capable of generating dependency
trees; (c) ﬁnally, contrary to previous dependency-
based language models which encode syntactic in-

formation as features, our model takes tree structure
into account more directly via representing different
types of dependency edges explicitly using LSTMs.
Therefore, there is no need to manually determine
which dependency tree features should be used or
how large the feature embeddings should be.

We evaluate our model on the MSR sentence com-
pletion challenge, a benchmark language modeling
dataset. Our results outperform the best published
results on this dataset. Since our model is a general
tree estimator, we also use it to rerank the top K de-
pendency trees from the (second order) MSTPasrser
and obtain performance on par with recently pro-
posed dependency parsers.

2 Tree Long Short-Term Memory

Networks

We seek to estimate the probability of a sentence by
estimating the generation probability of its depen-
dency tree. Syntactic information in our model is
represented in the form of dependency paths. In the
following, we ﬁrst describe our deﬁnition of depen-
dency path and based on it explain how the proba-
bility of a sentence is estimated.

2.1 Dependency Path

Generally speaking, a dependency path is the path
between ROOT and w consisting of the nodes on
the path and the edges connecting them. To rep-
resent dependency paths, we introduce four types
of edges which essentially deﬁne the “shape” of a
dependency tree. Let w0 denote a node in a tree
and w1, w2, . . . , wn its left dependents. As shown in
Figure 1, LEFT edge is the edge between w0 and
its ﬁrst left dependent denoted as (w0, w1). Let wk
(with 1 < k ≤ n) denote a non-ﬁrst left dependent
of w0. The edge from wk−1 to wk is a NX-LEFT
edge (NX stands for NEXT), where wk−1 is the right
adjacent sibling of wk. Note that the NX-LEFT edge
(wk−1, wk) replaces edge (w0, wk) (illustrated with a
dashed line in Figure 1) in the original dependency
tree. The modiﬁcation allows information to ﬂow
from w0 to wk through w1, . . . , wk−1 rather than di-
rectly from w0 to wk. RIGHT and NX-RIGHT edges
are deﬁned analogously for right dependents.

Given these four types of edges, dependency
paths (denoted as D(w)) can be deﬁned as follows

w0

wn

wk

wk−1

w1

LEFT

NX-LEFT

Figure 1: LEFT and NX-LEFT edges. Dotted line between
w1 and wk−1 (also between wk and wn) indicate that there may
be ≥ 0 nodes inbetween.

bearing in mind that the ﬁrst right dependent of
ROOT is its only dependent and that wp denotes the
parent of w. We use (. . . ) to denote a sequence,
where () is an empty sequence and (cid:107) is an operator
for concatenating two sequences.

(1) if w is ROOT, then D(w) = ()
(2) if w is a left dependent of wp
(a) if w is the ﬁrst

left dependent,

then

D(w) = D(wp)(cid:107)((cid:104)wp, LEFT(cid:105))

(b) if w is not the ﬁrst left dependent and ws is

its right adjacent sibling, then
D(w) = D(ws)(cid:107)((cid:104)ws, NX-LEFT(cid:105))

(3) if w is a right dependent of wp

(a) if w is the ﬁrst right dependent,
D(w) = D(wp)(cid:107)((cid:104)wp, RIGHT(cid:105))

then

(b) if w is not the ﬁrst right dependent and ws

is its left adjacent sibling, then
D(w) = D(ws)(cid:107)((cid:104)ws, NX-RIGHT(cid:105))

A dependency tree can be represented by the set of
its dependency paths which in turn can be used to
reconstruct the original tree.1

for

the ﬁrst

the moment

Dependency paths

two levels
the tree in Figure 2 are as follows (ig-
of
the subscripts which
noring for
D(sold) =
we explain in the next section).
((cid:104)ROOT, RIGHT(cid:105)) (see deﬁnitions (1) and (3a)),
D(year) = D(sold)(cid:107)((cid:104)sold, LEFT(cid:105))
(2a)),
D(manufacturer) = D(year)(cid:107)((cid:104)year, NX-LEFT(cid:105))
(see (2b)), D(cars) = D(sold)(cid:107)((cid:104)sold, RIGHT(cid:105))
(see (3a)), D(in) = D(cars)(cid:107)((cid:104)cars, NX-RIGHT(cid:105))
(according to (3b)).

(see

2.2 Tree Probability

ROOT

sold1

manufacturer3

year2

cars4

in5

The9

luxury8 auto7

last6

1,21410

U.S.11

the12

Figure 2: Dependency tree of the sentence The luxury auto

manufacturer last year sold 1,214 cars in the U.S. Subscripts

indicate breadth-ﬁrst traversal. ROOT has only one dependent

(i.e., sold ) which we view as its ﬁrst right dependent.

its corresponding tree T , P(S|T ). We view the prob-
ability computation of a dependency tree as a gener-
ation process. Speciﬁcally, we assume dependency
trees are constructed top-down, in a breadth-ﬁrst
manner. Generation starts at the ROOT node. For
each node at each level, ﬁrst its left dependents are
generated from closest to farthest and then the right
dependents (again from closest to farthest). The
same process is applied to the next node at the same
level or a node at the next level. Figure 2 shows the
breadth-ﬁrst traversal of a dependency tree.

Under the assumption that each word w in a de-
pendency tree is only conditioned on its dependency
path, the probability of a sentence S given its depen-
dency tree T is:

P(S|T ) = ∏

P(w|D(w))

(1)

w∈BFS(T )\ROOT

where D(w) is the dependency path of w. Note that
each word w is visited according to its breadth-ﬁrst
search order (BFS(T)) and the probability of ROOT
is ignored since every tree has one. The role of
ROOT in a dependency tree is the same as the begin
of sentence token (BOS) in a sentence. When com-
puting P(S|T ) (or P(S)), the probability of ROOT (or
BOS) is ignored (we assume it always exists), but is
used to predict other words. We explain in the next
section how TREELSTM estimates P(w|D(w)).

The core problem in syntax-based language model-
ing is to estimate the probability of sentence S given

2.3 Tree LSTMs

1Throughout this paper we assume all dependency trees are

projective.

A dependency path D(w) is subtree which we de-
note as a sequence of (cid:104)word, edge-type(cid:105) tuples. Our

w0

w0

w3

w2

w1

w4

w5

w6

Generated by four LSTMs

with tied We and tied Who

w2

w1

w0

G

E

N

-

L

G E N-R

w0

w4

w5

GEN-NX-L

GEN-NX-L

GEN-NX-R

GEN-NX-R

w1

w2

w3
w6
Figure 3: Generation process of left (w1, w2, w3) and right
(w4, w5, w6) dependents of tree node wo (top) using four LSTMs
(GEN-L, GEN-R, GEN-NX-L and GEN-NX-R). The model can

w4

w5

handle an arbitrary number of dependents due to GEN-NX-L

and GEN-NX-R.

innovation is to learn the representation of D(w) us-
ing four LSTMs. The four LSTMs (GEN-L, GEN-
R, GEN-NX-L and GEN-NX-R) are used to repre-
sent the four types of edges (LEFT, RIGHT, NX-
LEFT and NX-RIGHT) introduced earlier. GEN,
NX, L and R are shorthands for GENERATE, NEXT,
LEFT and RIGHT. At each time step, an LSTM is
chosen according to an edge-type; then the LSTM
takes a word as input and predicts/generates its de-
pendent or sibling. This process can be also viewed
as adding an edge and a node to a tree. Speciﬁ-
cally, LSTMs GEN-L and GEN-R are used to gen-
erate the ﬁrst left and right dependent of a node
(w1 and w4 in Figure 3). So, these two LSTMs
are responsible for going deeper in a tree. While
GEN-NX-L and GEN-NX-R generate the remain-
ing left/right dependents and therefore go wider in
a tree. As shown in Figure 3, w2 and w3 are gener-
ated by GEN-NX-L, whereas w5 and w6 are gener-
ated by GEN-NX-R. Note that the model can handle
any number of left or right dependents by applying
GEN-NX-L or GEN-NX-R multiple times.

We assume time steps correspond to the steps
taken by the breadth-ﬁrst traversal of the depen-
dency tree and the sentence has length n. At
time step t (1 ≤ t ≤ n), let (cid:104)wt(cid:48), zt(cid:105) denote the last
Subscripts t and t(cid:48) denote the
tuple in D(wt).
breadth-ﬁrst search order of wt and wt(cid:48), respectively.
zt ∈ {LEFT, RIGHT, NX-LEFT, NX-RIGHT} is the
edge type (see the deﬁnitions in Section 2.1). Let
We ∈ Rs×|V | denote the word embedding matrix and

Who ∈ R|V |×d the output matrix of our model, where
|V | is the vocabulary size, s the word embedding size
and d the hidden unit size. We use tied We and tied
Who for the four LSTMs to reduce the number of pa-
rameters in our model. The four LSTMs also share
their hidden states. Let H ∈ Rd×(n+1) denote the
shared hidden states of all time steps and e(wt) the
one-hot vector of wt. Then, H[:,t] represents D(wt)
at time step t, and the computation2 is:

xt = We · e(wt(cid:48))
ht = LSTMzt (xt, H[:,t(cid:48)])

H[:,t] = ht

yt = Who · ht

(2a)

(2b)

(2c)

(2d)

where the initial hidden state H[:, 0] is initialized to
a vector of small values such as 0.01. According to
Equation (2b), the model selects an LSTM based on
edge type zt. We describe the details of LSTMzt in
the next paragraph. The probability of wt given its
dependency path D(wt) is estimated by a softmax
function:

P(wt|D(wt)) =

(3)

exp(yt,wt )
|V |
k(cid:48)=1 exp(yt,k(cid:48))

∑

We must point out that although we use four jointly
trained LSTMs to encode the hidden states, the train-
ing and inference complexity of our model is no dif-
ferent from a regular LSTM, since at each time step
only one LSTM is working.

We implement LSTMz in Equation (2b) using a
deep LSTM (to simplify notation, from now on we
write z instead of zt). The inputs at time step t
are xt and ht(cid:48) (the hidden state of an earlier time
step t(cid:48)) and the output is ht (the hidden state of cur-
rent time step). Let L denote the layer number of
LSTMz and ˆhl
t the internal hidden state of the l-th
layer of the LSTMz at time step t, where xt is ˆh0
t and
ht(cid:48) is ˆhL
t(cid:48). The LSTM architecture introduces mul-
tiplicative gates and memory cells ˆcl
t (at l-th layer)
in order to address the vanishing gradient problem
which makes it difﬁcult for the standard RNN model
to learn long-distance correlations in a sequence.
Here, ˆcl
t is a linear combination of the current input
signal ut and an earlier memory cell ˆcl
t(cid:48). How much
input information ut will ﬂow into ˆcl
t is controlled

2We ignore all bias terms for notational simplicity.

w2

w1

w0

G

E

N

-

L

G E N-R

w0

w4

w5

GEN-NX-L

GEN-NX-L

GEN-NX-R

GEN-NX-R

LD

LD

w0

w4

w5

w6

w3

w2

w1

Figure 4: Generation of left and right dependents of node w0
according to LDTREELSTM.

by input gate it and how much of the earlier mem-
ory cell ˆcl
t(cid:48) will be forgotten is controlled by forget
gate ft. This process is computed as follows:

ux · ˆhl−1

ut = tanh(Wz,l
it = σ(Wz,l
ft = σ(Wz,l
t = ft (cid:12) ˆcl
ˆcl

ix · ˆhl−1
f x · ˆhl−1
t(cid:48) + it (cid:12) ut

t + Wz,l
uh · ˆhl
t(cid:48))
ih · ˆhl
t(cid:48))
f h · ˆhl
t(cid:48))

t + Wz,l
t + Wz,l

(4a)

(4b)

(4c)

(4d)

ux ∈ Rd×d (Wz,l

uh ∈ Rd×d are weight matrices for ut, Wz,l
ih are weight matrices for it and Wz,l

where Wz,l
ux ∈ Rd×s when l = 1) and
Wz,l
ix and
f x, and Wz,l
Wz,l
f h
are weight matrices for ft. σ is a sigmoid function
and (cid:12) the element-wise product.

Output gate ot controls how much information of

the cell ˆcl

t can be seen by other modules:

t + Wz,l
ox · ˆhl−1
ot = σ(Wz,l
ˆhl
t = ot (cid:12) tanh(ˆcl
t )

oh · ˆhl
t(cid:48))

(5a)

(5b)

Application of the above process to all layers L, will
yield ˆhL
t , which is ht. Note that in implementation,
all ˆcl
t (1 ≤ l ≤ L) at time step t are stored,
although we only care about ˆhL
t (ht).

t and ˆhl

2.4 Left Dependent Tree LSTMs

TREELSTM computes P(w|D(w)) based on the de-
pendency path D(w), which ignores the interaction
between left and right dependents on the same level.
In many cases, TREELSTM will use a verb to pre-
dict its object directly without knowing its subject.
For example, in Figure 2, TREELSTM uses (cid:104)ROOT,
RIGHT(cid:105) and (cid:104) sold, RIGHT (cid:105) to predict cars. This in-
formation is unfortunately not speciﬁc to cars (many
things can be sold, e.g., chocolates, candy). Consid-
ering manufacturer, the left dependent of sold would
help predict cars more accurately.

In order to jointly take left and right dependents
into account, we employ yet another LSTM, which
goes from the furthest left dependent to the closest
left dependent (LD is a shorthand for left depen-
dent). As shown in Figure 4, LD LSTM learns the
representation of all left dependents of a node w0;
this representation is then used to predict the ﬁrst
right dependent of the same node. Non-ﬁrst right de-
pendents can also leverage the representation of left
dependents, since this information is injected into
the hidden state of the ﬁrst right dependent and can
percolate all the way. Note that in order to retain the
generation capability of our model (Section 3.4), we
only allow right dependents to leverage left depen-
dents (they are generated before right dependents).

The computation of the LDTREELSTM is al-
the same as in TREELSTM except when
most
zt = GEN-R.
let vt be the cor-
responding left dependent sequence with length
K (vt = (w3, w2, w1) in Figure 4). Then, the hidden
state (qk) of vt at each time step k is:

In this case,

mk = We · e(vt,k)
qk = LSTMLD(mk, qk−1)

(6a)

(6b)

where qK is the representation for all left depen-
dents. Then, the computation of the current hid-
den state becomes (see Equation (2) for the original
computation):

rt =

(cid:21)
(cid:20)We · e(wt(cid:48))
qK

ht = LSTMGEN-R(rt, H[:,t(cid:48)])

(7a)

(7b)

where qK serves as additional input for LSTMGEN-R.
All other computational details are the same as in
TreeLSTM (see Section 2.3).

2.5 Model Training

On small scale datasets we employ Negative Log-
likelihood (NLL) as our training objective for both
TREELSTM and LDTREELSTM:

L NLL(θ) = −

log P(S|T )

(8)

1
|S | ∑

S∈S

where S is a sentence in the training set S , T is the
dependency tree of S and P(S|T ) is deﬁned as in
Equation (1).

On large scale datasets (e.g., with vocabulary
size of 65K), computing the output layer activa-
tions and the softmax function with NLL would
become prohibitively expensive.
Instead, we em-
ploy Noise Contrastive Estimation (NCE; Gutmann
and Hyv¨arinen (2012), Mnih and Teh (2012)) which
treats the normalization term ˆZ in ˆP(w|D(wt)) =
exp(Who[w,:]·ht )
as constant. The intuition behind NCE
ˆZ
is to discriminate between samples from a data dis-
tribution ˆP(w|D(wt)) and a known noise distribu-
tion Pn(w) via binary logistic regression. Assuming
that noise words are k times more frequent than real
words in the training set (Mnih and Teh, 2012), then
the probability of a word w being from our model
Pd(w,D(wt)) is
. We apply NCE to
large vocabulary models with the following training
objective:

ˆP(w|D(wt ))
ˆP(w|D(wt ))+kPn(w)

L NCE(θ) = −

log Pd(wt,D(wt))

(cid:18)

1
|S | ∑

T ∈S

|T |
∑
t=1

+

log[1 − Pd( ˜wt, j,D(wt))]

(cid:19)

k
∑
j=1

where ˜wt, j is a word sampled from the noise distri-
bution Pn(w). We use smoothed unigram frequen-
cies (exponentiating by 0.75) as the noise distribu-
tion Pn(w) (Mikolov et al., 2013b). We initialize
ln ˆZ = 9 as suggested in Chen et al. (2015), but in-
stead of keeping it ﬁxed we also learn ˆZ during train-
ing (Vaswani et al., 2013). We set k = 20.

3 Experiments

We assess the performance of our model on two
tasks: the Microsoft Research (MSR) sentence com-
pletion challenge (Zweig and Burges, 2012), and de-
pendency parsing reranking. We also demonstrate
the tree generation capability of our models. In the
following, we ﬁrst present details on model train-
ing and then present our results. We implemented
our models using the Torch library (Collobert et
al., 2011) and our code is available at https://
github.com/XingxingZhang/td-treelstm.

3.1 Training Details

We trained our model with back propagation
through time (Rumelhart et al., 1988) on an Nvidia

GPU Card with a mini-batch size of 64. The ob-
jective (NLL or NCE) was minimized by stochastic
gradient descent. Model parameters were uniformly
initialized in [−0.1, 0.1]. We used the NCE objec-
tive on the MSR sentence completion task (due to
the large size of this dataset) and the NLL objec-
tive on dependency parsing reranking. We used an
initial learning rate of 1.0 for all experiments and
when there was no signiﬁcant improvement in log-
likelihood on the validation set, the learning rate was
divided by 2 per epoch until convergence (Mikolov
et al., 2010). To alleviate the exploding gradients
problem, we rescaled the gradient g when the gradi-
ent norm ||g|| > 5 and set g = 5g
||g|| (Pascanu et al.,
2013; Sutskever et al., 2014). Dropout (Srivastava
et al., 2014) was applied to the 2-layer TREELSTM
and LDTREELSTM models. The word embedding
size was set to s = d/2 where d is the hidden unit
size.

3.2 Microsoft Sentence Completion Challenge

The task in the MSR Sentence Completion Chal-
lenge (Zweig and Burges, 2012) is to select the
correct missing word for 1,040 SAT-style test sen-
tences when presented with ﬁve candidate comple-
tions. The training set contains 522 novels from
the Project Gutenberg which we preprocessed as fol-
lows. After removing headers and footers from the
ﬁles, we tokenized and parsed the dataset into de-
pendency trees with the Stanford Core NLP toolkit
(Manning et al., 2014). The resulting training set
contained 49M words. We converted all words to
lower case and replaced those occurring ﬁve times
or less with UNK. The resulting vocabulary size
was 65,346 words. We randomly sampled 4,000
sentences from the training set as our validation set.
The literature describes two main approaches to
the sentence completion task based on word vectors
and language models. In vector-based approaches,
all words in the sentence and the ﬁve candidate
words are represented by a vector; the candidate
which has the highest average similarity with the
sentence words is selected as the answer. For lan-
guage model-based methods, the LM computes the
probability of a test sentence with each of the ﬁve
candidate words, and picks the candidate comple-
tion which gives the highest probability. Our model
belongs to this class of models.

|θ|

Accuracy

d

—
640
600

Model
Word Vector based Models
LSA
Skip-gram
IVLBL
Language Models
KN5
UDepNgram
LDepNgram
RNN
RNNME
depRNN+3gram
ldepRNN+4gram
LBL
LSTM
LSTM
LSTM
Bidirectional LSTM
Bidirectional LSTM
Bidirectional LSTM
Model Combinations
RNNMEs
—
Skip-gram + RNNMEs —
Our Models
TREELSTM
LDTREELSTM
TREELSTM
LDTREELSTM

—
—
—
300
300
100
200
300
300
400
450
200
300
400

300
300
400
400

—
102M
96.0M

—
—
—
48.1M
1120M
1014M
1029M
48.0M
29.9M
40.2M
45.3M
33.2M
50.1M
67.3M

—
—

31.6M
32.5M
43.1M
44.7M

49.0
48.0
55.5

40.0
48.3
50.0
45.0
49.3
53.5
50.7
54.7
55.00
57.02
55.96
48.46
49.90
48.65

55.4
58.9

55.29
57.79
56.73
60.67

Table 1: Model accuracy on the MSR sentence completion task.

The results of KN5, RNNME and RNNMEs are reported in

Mikolov (2012), LSA and RNN in Zweig et al. (2012), UDep-

Ngram and LDepNgram in Gubbins and Vlachos (2013), de-

pRNN+3gram and depRNN+4gram in Mirowski and Vlachos

(2015), LBL in Mnih and Teh (2012), Skip-gram and Skip-

gram+RNNMEs in Mikolov et al. (2013a), and IVLBL in Mnih
and Kavukcuoglu (2013); d is the hidden size and |θ| the num-
ber of parameters in a model.

Table 1 presents a summary of our results to-
gether with previoulsy published results. The best
performing word vector model is IVLBL (Mnih and
Kavukcuoglu, 2013) with an accuracy of 55.5, while
the best performing single language model is LBL
(Mnih and Teh, 2012) with an accuracy of 54.7.
Both approaches are based on the log-bilinear lan-
guage model (Mnih and Hinton, 2007). A combi-
nation of several recurrent neural networks and the
skip-gram model holds the state of the art with an
accuracy of 58.9 (Mikolov et al., 2013b). To fairly
compare with existing models, we restrict the layer

Test

Parser

Development
UAS
MSTParser-2nd
92.20
TREELSTM
92.51
92.64
TREELSTM*
LDTREELSTM 92.66
92.00
NN parser*
93.20
S-LSTM*

LAS
88.44
88.53
88.69
88.69
89.60
90.90
Table 2: Performance of TREELSTM and LDTREELSTM on

LAS UAS
91.63
88.78
91.79
89.07
91.97
89.09
91.99
89.14
91.80
89.70
93.10
90.90

reranking the top dependency trees produced by the 2nd order

MSTParser (McDonald and Pereira, 2006). Results for the NN

and S-LSTM parsers are reported in Chen and Manning (2014)

and Dyer et al. (2015), respectively. * indicates that the model

is initialized with pre-trained word vectors.

size of our models to 1. We observe that LDTREEL-
STM consistently outperforms TREELSTM, which
indicates the importance of modeling the interac-
In fact,
tion between left and right dependents.
LDTREELSTM (d = 400) achieves a new state-of-
the-art on this task, despite being a single model.
We also implement LSTM and bidirectional LSTM
language models.3 An LSTM with d = 400 out-
performs its smaller counterpart (d = 300), however
performance decreases with d = 450. The bidirec-
tional LSTM is worse than the LSTM (see Mnih
and Teh (2012) for a similar observation). The
best performing LSTM is worse than a LDTREEL-
STM (d = 300). The input and output embeddings
(We and Who) dominate the number of parame-
ters in all neural models except for RNNME, de-
pRNN+3gram and ldepRNN+4gram, which include
a ME model that contains 1 billion sparse n-gram
features (Mikolov, 2012; Mirowski and Vlachos,
2015). The number of parameters in TREELSTM
and LDTREELSTM is not much larger compared to
LSTM due to the tied We and Who matrices.

3.3 Dependency Parsing

In this section we demonstrate that our model can
be also used for parse reranking. This is not possi-
ble for sequence-based language models since they
cannot estimate the probability of a tree. We use
our models to rerank the top K dependency trees
produced by the second order MSTParser (McDon-

3LSTMs and BiLSTMs were also trained with NCE
(s = d/2; hyperparameters were tuned on the development set).

ald and Pereira, 2006).4 We follow closely the ex-
perimental setup of Chen and Manning (2014) and
Dyer et al. (2015). Speciﬁcally, we trained TREEL-
STM and LDTREELSTM on Penn Treebank sec-
tions 2–21. We used section 22 for development and
section 23 for testing. We adopted the Stanford ba-
sic dependency representations (De Marneffe et al.,
2006); part-of-speech tags were predicted with the
Stanford Tagger (Toutanova et al., 2003). We trained
TREELSTM and LDTREELSTM as language mod-
els (singletons were replaced with UNK) and did
not use any POS tags, dependency labels or com-
position features, whereas these features are used in
Chen and Manning (2014) and Dyer et al. (2015).
We tuned d, the number of layers, and K on the de-
velopment set.

Table 2 reports unlabeled attachment scores
(UAS) and labeled attachment scores (LAS) for
the MSTParser, TREELSTM (d = 300, 1 layer,
K = 2), and LDTREELSTM (d = 200, 2 layers,
K = 4). We also include the performance of two
neural network-based dependency parsers; Chen and
Manning (2014) use a neural network classiﬁer to
predict the correct transition (NN parser); Dyer et
al. (2015) also implement a transition-based depen-
dency parser using LSTMs to represent the contents
of the stack and buffer in a continuous space. As can
be seen, both TREELSTM and LDTREELSTM out-
perform the baseline MSTParser, with LDTREEL-
STM performing best. We also initialized the word
embedding matrix We with pre-trained GLOVE vec-
tors (Pennington et al., 2014). We obtained a slight
improvement over TREELSTM (TREELSTM* in
Table 2; d = 200, 2 layer, K = 4) but no im-
provement over LDTREELSTM. Finally, notice that
LDTREELSTM is slightly better than the NN parser
in terms of UAS but worse than the S-LSTM parser.
In the future, we would like to extend our model so
that it takes labeled dependency information into ac-
count.

3.4 Tree Generation

This section demonstrates how to use a trained
LDTREELSTM to generate tree samples. The gen-
eration starts at the ROOT node. At each time step t,
for each node wt, we add a new edge and node to

Figure 5: Generated dependency trees with LDTREELSTM

trained on the PTB.

the tree. Unfortunately during generation, we do not
know which type of edge to add. We therefore use
four binary classiﬁers (ADD-LEFT, ADD-RIGHT,
ADD-NX-LEFT and ADD-NX-RIGHT) to predict
whether we should add a LEFT, RIGHT, NX-LEFT
or NX-RIGHT edge.5 Then when a classiﬁer pre-
dicts true, we use the corresponding LSTM to gener-
ate a new node by sampling from the predicted word
distribution in Equation (3). The four classiﬁers take
the previous hidden state H[:,t(cid:48)] and the output em-
bedding of the current node Who · e(wt) as features.6
Speciﬁcally, we use a trained LDTREELSTM to
go through the training corpus and generate hidden
states and embeddings as input features; the corre-
sponding class labels (true and false) are “read off”
the training dependency trees. We use two-layer rec-
tiﬁer networks (Glorot et al., 2011) as the four clas-
siﬁers with a hidden size of 300. We use the same
LDTREELSTM model as in Section 3.3 to gener-
ate dependency trees. The classiﬁers were trained
using AdaGrad (Duchi et al., 2011) with a learning
rate of 0.01. The accuracies of ADD-LEFT, ADD-
RIGHT, ADD-NX-LEFT and ADD-NX-RIGHT are

5It is possible to get rid of the four classiﬁers by adding
START/STOP symbols when generating left and right depen-
dents as in (Eisner, 1996). We refrained from doing this for
computational reasons. For a sentence with N words, this ap-
proach will lead to 2N additional START/STOP symbols (with
one START and one STOP symbol for each word). Conse-
quently, the computational cost and memory consumption dur-
ing training will be three times as much rendering our model
less scalable.

6The input embeddings have lower dimensions and therefore

4http://www.seas.upenn.edu/ strctlrn/MSTParser

result in slightly worse classiﬁers.

94.3%, 92.6%, 93.4% and 96.0%, respectively. Fig-
ure 5 shows examples of generated trees.

4 Conclusions

In this paper we developed TREELSTM (and
LDTREELSTM), a neural network model architec-
ture, which is designed to predict tree structures
rather than linear sequences. Experimental results
on the MSR sentence completion task show that
LDTREELSTM is superior to sequential LSTMs.
Dependency parsing reranking experiments high-
light our model’s potential for dependency pars-
ing. Finally, the ability of our model to gener-
ate dependency trees holds promise for text gen-
eration applications such as sentence compression
and simpliﬁcation (Filippova et al., 2015). Although
our experiments have focused exclusively on depen-
dency trees, there is nothing inherent in our formu-
lation that disallows its application to other types of
tree structure such as constituent trees or even tax-
onomies.

Acknowledgments

We would like to thank Adam Lopez, Frank Keller,
Iain Murray, Li Dong, Brian Roark, and the NAACL
reviewers for their valuable feedback. Xingxing
Zhang gratefully acknowledges the ﬁnancial sup-
port of the China Scholarship Council (CSC). Liang
Lu is funded by the UK EPSRC Programme Grant
EP/I031022/1, Natural Speech Technology (NST).

References

[Bengio et al.2003] Yoshua Bengio, R´ejean Ducharme,
Pascal Vincent, and Christian Janvin. 2003. A neural
probabilistic language model. The Journal of Machine
Learning Research, 3:1137–1155.

[Charniak2001] Eugene Charniak.

Immediate-
head parsing for language models. In Proceedings of
the 39th Annual Meeting on Association for Compu-
tational Linguistics, pages 124–131. Association for
Computational Linguistics.

2001.

[Chelba and Jelinek2000] Ciprian Chelba and Frederick
Jelinek. 2000. Structured language modeling. Com-
puter Speech and Language, 14(4):283–332.

[Chelba et al.1997] Ciprian Chelba, David Engle, Freder-
ick Jelinek, Victor Jimenez, Sanjeev Khudanpur, Lidia
Mangu, Harry Printz, Eric Ristad, Ronald Rosenfeld,

Andreas Stolcke, et al.
formance of a dependency language model.
ROSPEECH. Citeseer.

1997. Structure and per-
In EU-

[Chen and Manning2014] Danqi Chen and Christopher
2014. A fast and accurate dependency
Manning.
In Proceedings of
parser using neural networks.
the 2014 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 740–750,
Doha, Qatar, October. Association for Computational
Linguistics.

[Chen et al.2015] X Chen, X Liu, MJF Gales, and
PC Woodland. 2015. Recurrent neural network lan-
guage model training with noise contrastive estimation
for speech recognition. In In 40th IEEE International
Conference on Accoustics, Speech and Signal Process-
ing, pages 5401–5405, Brisbane, Australia.

[Collobert et al.2011] Ronan

Collobert,

Koray
Kavukcuoglu, and Cl´ement Farabet. 2011. Torch7:
A matlab-like environment for machine learning.
In
BigLearn, NIPS Workshop, number EPFL-CONF-
192376.

[De Marneffe et al.2006] Marie-Catherine De Marneffe,
Bill MacCartney, Christopher D Manning, et al. 2006.
Generating typed dependency parses from phrase
structure parses. In Proceedings of LREC, volume 6,
pages 449–454.

[Duchi et al.2011] John Duchi, Elad Hazan, and Yoram
Singer. 2011. Adaptive subgradient methods for on-
line learning and stochastic optimization. The Journal
of Machine Learning Research, 12:2121–2159.

[Dyer et al.2015] Chris Dyer, Miguel Ballesteros, Wang
Ling, Austin Matthews, and Noah A. Smith. 2015.
Transition-based dependency parsing with stack long
In Proceedings of the 53rd An-
short-term memory.
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long Pa-
pers), pages 334–343, Beijing, China, July. Associa-
tion for Computational Linguistics.

[Eisner1996] Jason M Eisner. 1996. Three new prob-
abilistic models for dependency parsing: An explo-
ration. In Proceedings of the 16th conference on Com-
putational linguistics-Volume 1, pages 340–345. Asso-
ciation for Computational Linguistics.

[Emami et al.2003] Ahmad Emami, Peng Xu, and Fred-
erick Jelinek. 2003. Using a connectionist model in
In Proceedings
a syntactical based language model.
of the IEEE International Conference on Acoustics,
Speech, and Signal Processing, pages 372–375, Hong
Kong, China.

[Filippova et al.2015] Katja Filippova, Enrique Alfon-
seca, Carlos A Colmenares, Lukasz Kaiser, and Oriol
Vinyals. 2015. Sentence compression by deletion
with lstms. In EMNLP, pages 360–368.

[Glorot et al.2011] Xavier Glorot, Antoine Bordes, and
Yoshua Bengio. 2011. Deep sparse rectiﬁer neural
networks. In International Conference on Artiﬁcial In-
telligence and Statistics, pages 315–323.

[Graves et al.2013] Alan Graves, Abdel-rahman Mo-
hamed, and Geoffrey Hinton. 2013. Speech recogni-
tion with deep recurrent neural networks. In Acoustics,
Speech and Signal Processing (ICASSP), 2013 IEEE
International Conference on, pages 6645–6649. IEEE.
[Gubbins and Vlachos2013] Joseph Gubbins and Andreas
Vlachos. 2013. Dependency language models for sen-
tence completion. In EMNLP, pages 1405–1410, Seat-
tle, Washington, USA, October. Association for Com-
putational Linguistics.

[Gutmann and Hyv¨arinen2012] Michael U Gutmann and
Aapo Hyv¨arinen. 2012. Noise-contrastive estimation
of unnormalized statistical models, with applications
to natural image statistics. The Journal of Machine
Learning Research, 13(1):307–361.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural computation, 9(8):1735–1780.

[Hochreiter1998] Sepp Hochreiter. 1998. Vanishing gra-
dient problem during learning recurrent neural nets
and problem solutions. International Journal of Un-
certainty, Fuzziness and Knowledge-based Systems,
6(2):107–116.

[Manning et al.2014] Christopher D Manning, Mihai Sur-
deanu, John Bauer, Jenny Finkel, Steven J Bethard,
and David McClosky. 2014. The stanford corenlp
In Proceedings
natural language processing toolkit.
of 52nd Annual Meeting of the Association for Com-
putational Linguistics: System Demonstrations, pages
55–60.

[McDonald and Pereira2006] Ryan T McDonald and Fer-
nando CN Pereira. 2006. Online learning of approxi-
mate dependency parsing algorithms. In EACL.

[Mikolov et al.2010] Tomas Mikolov, Martin Karaﬁ´at,
Lukas Burget, Jan Cernock`y, and Sanjeev Khudan-
pur. 2010. Recurrent neural network based language
model. In INTERSPEECH 2010, 11th Annual Confer-
ence of the International Speech Communication As-
sociation, Makuhari, Chiba, Japan, September 26-30,
2010, pages 1045–1048.

[Mikolov et al.2013a] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013a. Efﬁcient esti-
In
mation of word representations in vector space.
Proceedings of the 2013 International Conference on
Learning Representations, Scottsdale, Arizona, USA.
Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013b.
Distributed representations of words and phrases and
In Advances in Neural Infor-
their compositionality.
mation Processing Systems 26, pages 3111–3119.

[Mikolov et al.2013b] Tomas Mikolov,

[Mikolov2012] Tomas Mikolov. 2012. Statistical Lan-
guage Models based on Neural Networks. Ph.D. the-
sis, Brno University of Technology.

[Mirowski and Vlachos2015] Piotr Mirowski and An-
dreas Vlachos. 2015. Dependency recurrent neural
In ACL,
language models for sentence completion.
pages 511–517, Beijing, China, July. Association for
Computational Linguistics.

[Mnih and Hinton2007] Andriy Mnih and Geoffrey Hin-
ton. 2007. Three new graphical models for statistical
In Proceedings of the 24th In-
language modelling.
ternational Conference on Machine Learning, pages
641–648.

[Mnih and Kavukcuoglu2013] Andriy Mnih and Koray
Kavukcuoglu. 2013. Learning word embeddings efﬁ-
ciently with noise-contrastive estimation. In Advances
in Neural Information Processing Systems 26, pages
2265–2273.

[Mnih and Teh2012] Andriy Mnih and Yee Whye Teh.
2012. A fast and simple algorithm for training neural
probabilistic language models. In Proceedings of the
29th International Conference on Machine Learning,
pages 1751–1758, Edinburgh, Scotland.

[Pascanu et al.2013] Razvan Pascanu, Tomas Mikolov,
and Yoshua Bengio. 2013. On the difﬁculty of train-
ing recurrent neural networks. In Proceedings of the
31st International Conference on Machine Learning,
pages 1310–1318, Atlanta, Georgia, USA.
Pennington,

Richard
Socher, and Christopher D Manning. 2014. Glove:
EMNLP,
Global vectors for word representation.
12:1532–1543.

[Pennington et al.2014] Jeffrey

[Pollack1990] Jordan B. Pollack. 1990. Recursive dis-
tributed representations. Artiﬁcial Intelligence, 1–
2(46):77–105.

[Roark2001] Brian Roark. 2001. Probabilistic top-down
parsing and language modeling. Computational lin-
guistics, 27(2):249–276.

[Rumelhart et al.1988] David E Rumelhart, Geoffrey E
Hinton, and Ronald J Williams. 1988. Learning repre-
sentations by back-propagating errors. Cognitive mod-
eling, 5:3.

[Sennrich2015] Rico Sennrich. 2015. Modelling and op-
timizing on syntactic n-grams for statistical machine
translation. Transactions of the Association for Com-
putational Linguistics, 3:169–182.

[Shen et al.2008] Libin Shen,

Jinxi Xu,

and Ralph
Weischedel. 2008. A new string-to-dependency ma-
chine translation algorithm with a target dependency
In Proceedings of ACL-08: HLT,
language model.
pages 577–585, Columbus, Ohio, USA.

[Socher et al.2011a] Richard Socher, Eric H. Huang, Jef-
frey Pennington, Christopher D. Manning, and An-
drew Ng. 2011a. Dynamic pooling and unfolding

Model? On the Future of Language Modeling for HLT,
pages 29–36, Montr´eal, Canada.

[Zweig et al.2012] Geoffrey Zweig, John C Platt, Christo-
pher Meek, Christopher JC Burges, Ainur Yessenalina,
2012. Computational approaches
and Qiang Liu.
In Proceedings of the 50th
to sentence completion.
Annual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 601–610.
Association for Computational Linguistics.

In
recursive autoencoders for paraphrase detection.
Advances in Neural Information Processing Systems,
pages 801–809.

[Socher et al.2011b] Richard Socher, Jeffrey Pennington,
Eric H. Huang, Andrew Y. Ng, and Christopher D.
Manning. 2011b. Semi-supervised recursive autoen-
coders for predicting sentiment distributions. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 151–161,
Edinburgh, Scotland, UK.

[Srivastava et al.2014] Nitish Srivastava, Geoffrey Hin-
ton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: A simple way to pre-
vent neural networks from overﬁtting. The Journal of
Machine Learning Research, 15(1):1929–1958.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and
Quoc VV Le. 2014. Sequence to sequence learning
with neural networks. In Advances in Neural Informa-
tion Processing Systems, pages 3104–3112.

[Tai et al.2015] Kai Sheng Tai, Richard Socher, and
Christopher D. Manning. 2015.
Improved semantic
representations from tree-structured long short-term
memory networks. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguis-
tics and the 7th International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers),
pages 1556–1566, Beijing, China, July. Association
for Computational Linguistics.

[Toutanova et al.2003] Kristina Toutanova, Dan Klein,
Christopher D Manning, and Yoram Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic de-
pendency network. In Proceedings of the 2003 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology-Volume 1, pages 173–180. Associa-
tion for Computational Linguistics.

[Vaswani et al.2013] Ashish Vaswani, Yinggong Zhao,
Victoria Fossum, and David Chiang. 2013. Decod-
ing with large-scale neural language models improves
translation. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
pages 1387–1392, Seattle, Washington, USA.

[Vinyals et al.2015] Oriol Vinyals, Alexander Toshev,
Samy Bengio, and Dumitru Erhan. 2015. Show and
tell: A neural image caption generator. In The IEEE
Conference on Computer Vision and Pattern Recogni-
tion, Boston, Massachusetts, USA.

[Zhang2009] Ying Zhang. 2009. Structured language
models for statistical machine translation. Ph.D. the-
sis, Johns Hopkins University.

[Zweig and Burges2012] Geoffrey Zweig and Chris J.C.
Burges. 2012. A challenge set for advancing language
In Proceedings of the NAACL-HLT 2012
modeling.
Workshop: Will We Ever Really Replace the N-gram

Top-down Tree Long Short-Term Memory Networks

Xingxing Zhang, Liang Lu and Mirella Lapata
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
{x.zhang,liang.lu}@ed.ac.uk,mlap@inf.ed.ac.uk

6
1
0
2
 
r
p
A
 
3
 
 
]
L
C
.
s
c
[
 
 
3
v
0
6
0
0
0
.
1
1
5
1
:
v
i
X
r
a

Abstract

Long Short-Term Memory (LSTM) networks,
a type of recurrent neural network with a
more complex computational unit, have been
successfully applied to a variety of sequence
modeling tasks. In this paper we develop Tree
Long Short-Term Memory (TREELSTM), a
neural network model based on LSTM, which
is designed to predict a tree rather than a lin-
ear sequence. TREELSTM deﬁnes the prob-
ability of a sentence by estimating the gener-
ation probability of its dependency tree. At
each time step, a node is generated based
on the representation of the generated sub-
tree. We further enhance the modeling power
of TREELSTM by explicitly representing the
correlations between left and right depen-
dents. Application of our model to the MSR
sentence completion challenge achieves re-
sults beyond the current state of the art. We
also report results on dependency parsing
reranking achieving competitive performance.

1

Introduction

Neural language models have been gaining increas-
ing attention as a competitive alternative to n-grams.
The main idea is to represent each word using a
real-valued feature vector capturing the contexts in
which it occurs. The conditional probability of the
next word is then modeled as a smooth function of
the feature vectors of the preceding words and the
In essence, similar representations are
next word.
learned for words found in similar contexts result-
ing in similar predictions for the next word. Previ-
ous approaches have mainly employed feed-forward

(Bengio et al., 2003; Mnih and Hinton, 2007) and
recurrent neural networks (Mikolov et al., 2010;
Mikolov, 2012) in order to map the feature vec-
tors of the context words to the distribution for the
next word. Recently, RNNs with Long Short-Term
Memory (LSTM) units (Hochreiter and Schmidhu-
ber, 1997; Hochreiter, 1998) have emerged as a pop-
ular architecture due to their strong ability to capture
long-term dependencies. LSTMs have been success-
fully applied to a variety of tasks ranging from ma-
chine translation (Sutskever et al., 2014), to speech
recognition (Graves et al., 2013), and image descrip-
tion generation (Vinyals et al., 2015).

Despite superior performance in many applica-
tions, neural language models essentially predict se-
quences of words. Many NLP tasks, however, ex-
ploit syntactic information operating over tree struc-
tures (e.g., dependency or constituent trees). In this
paper we develop a novel neural network model
which combines the advantages of the LSTM archi-
tecture and syntactic structure. Our model estimates
the probability of a sentence by estimating the gen-
eration probability of its dependency tree. Instead
of explicitly encoding tree structure as a set of fea-
tures, we use four LSTM networks to model four
types of dependency edges which altogether specify
how the tree is built. At each time step, one LSTM is
activated which predicts the next word conditioned
on the sub-tree generated so far. To learn the repre-
sentations of the conditioned sub-tree, we force the
four LSTMs to share their hidden layers. Our model
is also capable of generating trees just by sampling
from a trained model and can be seamlessly inte-
grated with text generation applications.

Our approach is related to but ultimately differ-
ent from recursive neural networks (Pollack, 1990)
a class of models which operate on structured in-
puts. Given a (binary) parse tree, they recursively
generate parent representations in a bottom-up fash-
ion, by combining tokens to produce representations
for phrases, and eventually the whole sentence. The
learned representations can be then used in classi-
ﬁcation tasks such as sentiment analysis (Socher et
al., 2011b) and paraphrase detection (Socher et al.,
2011a). Tai et al. (2015) learn distributed representa-
tions over syntactic trees by generalizing the LSTM
architecture to tree-structured network topologies.
The key feature of our model is not so much that
it can learn semantic representations of phrases or
sentences, but its ability to predict tree structure and
estimate its probability.

Syntactic language models have a long history
in NLP dating back to Chelba and Jelinek (2000)
(see also Roark (2001) and Charniak (2001)). These
models differ in how grammar structures in a parsing
tree are used when predicting the next word. Other
work develops dependency-based language models
for speciﬁc applications such as machine translation
(Shen et al., 2008; Zhang, 2009; Sennrich, 2015),
speech recognition (Chelba et al., 1997) or sentence
completion (Gubbins and Vlachos, 2013). All in-
stances of these models apply Markov assumptions
on the dependency tree, and adopt standard n-gram
smoothing methods for reliable parameter estima-
tion. Emami et al. (2003) and Sennrich (2015) esti-
mate the parameters of a structured language model
using feed-forward neural networks (Bengio et al.,
2003). Mirowski and Vlachos (2015) re-implement
the model of Gubbins and Vlachos (2013) with
RNNs. They view sentences as sequences of words
over a tree. While they ignore the tree structures
themselves, we model them explicitly.

Our model shares with other structured-based lan-
guage models the ability to take dependency infor-
mation into account. It differs in the following re-
spects: (a) it does not artiﬁcially restrict the depth
of the dependencies it considers and can thus be
viewed as an inﬁnite order dependency language
model; (b) it not only estimates the probability of a
string but is also capable of generating dependency
trees; (c) ﬁnally, contrary to previous dependency-
based language models which encode syntactic in-

formation as features, our model takes tree structure
into account more directly via representing different
types of dependency edges explicitly using LSTMs.
Therefore, there is no need to manually determine
which dependency tree features should be used or
how large the feature embeddings should be.

We evaluate our model on the MSR sentence com-
pletion challenge, a benchmark language modeling
dataset. Our results outperform the best published
results on this dataset. Since our model is a general
tree estimator, we also use it to rerank the top K de-
pendency trees from the (second order) MSTPasrser
and obtain performance on par with recently pro-
posed dependency parsers.

2 Tree Long Short-Term Memory

Networks

We seek to estimate the probability of a sentence by
estimating the generation probability of its depen-
dency tree. Syntactic information in our model is
represented in the form of dependency paths. In the
following, we ﬁrst describe our deﬁnition of depen-
dency path and based on it explain how the proba-
bility of a sentence is estimated.

2.1 Dependency Path

Generally speaking, a dependency path is the path
between ROOT and w consisting of the nodes on
the path and the edges connecting them. To rep-
resent dependency paths, we introduce four types
of edges which essentially deﬁne the “shape” of a
dependency tree. Let w0 denote a node in a tree
and w1, w2, . . . , wn its left dependents. As shown in
Figure 1, LEFT edge is the edge between w0 and
its ﬁrst left dependent denoted as (w0, w1). Let wk
(with 1 < k ≤ n) denote a non-ﬁrst left dependent
of w0. The edge from wk−1 to wk is a NX-LEFT
edge (NX stands for NEXT), where wk−1 is the right
adjacent sibling of wk. Note that the NX-LEFT edge
(wk−1, wk) replaces edge (w0, wk) (illustrated with a
dashed line in Figure 1) in the original dependency
tree. The modiﬁcation allows information to ﬂow
from w0 to wk through w1, . . . , wk−1 rather than di-
rectly from w0 to wk. RIGHT and NX-RIGHT edges
are deﬁned analogously for right dependents.

Given these four types of edges, dependency
paths (denoted as D(w)) can be deﬁned as follows

w0

wn

wk

wk−1

w1

LEFT

NX-LEFT

Figure 1: LEFT and NX-LEFT edges. Dotted line between
w1 and wk−1 (also between wk and wn) indicate that there may
be ≥ 0 nodes inbetween.

bearing in mind that the ﬁrst right dependent of
ROOT is its only dependent and that wp denotes the
parent of w. We use (. . . ) to denote a sequence,
where () is an empty sequence and (cid:107) is an operator
for concatenating two sequences.

(1) if w is ROOT, then D(w) = ()
(2) if w is a left dependent of wp
(a) if w is the ﬁrst

left dependent,

then

D(w) = D(wp)(cid:107)((cid:104)wp, LEFT(cid:105))

(b) if w is not the ﬁrst left dependent and ws is

its right adjacent sibling, then
D(w) = D(ws)(cid:107)((cid:104)ws, NX-LEFT(cid:105))

(3) if w is a right dependent of wp

(a) if w is the ﬁrst right dependent,
D(w) = D(wp)(cid:107)((cid:104)wp, RIGHT(cid:105))

then

(b) if w is not the ﬁrst right dependent and ws

is its left adjacent sibling, then
D(w) = D(ws)(cid:107)((cid:104)ws, NX-RIGHT(cid:105))

A dependency tree can be represented by the set of
its dependency paths which in turn can be used to
reconstruct the original tree.1

for

the ﬁrst

the moment

Dependency paths

two levels
the tree in Figure 2 are as follows (ig-
of
the subscripts which
noring for
D(sold) =
we explain in the next section).
((cid:104)ROOT, RIGHT(cid:105)) (see deﬁnitions (1) and (3a)),
D(year) = D(sold)(cid:107)((cid:104)sold, LEFT(cid:105))
(2a)),
D(manufacturer) = D(year)(cid:107)((cid:104)year, NX-LEFT(cid:105))
(see (2b)), D(cars) = D(sold)(cid:107)((cid:104)sold, RIGHT(cid:105))
(see (3a)), D(in) = D(cars)(cid:107)((cid:104)cars, NX-RIGHT(cid:105))
(according to (3b)).

(see

2.2 Tree Probability

ROOT

sold1

manufacturer3

year2

cars4

in5

The9

luxury8 auto7

last6

1,21410

U.S.11

the12

Figure 2: Dependency tree of the sentence The luxury auto

manufacturer last year sold 1,214 cars in the U.S. Subscripts

indicate breadth-ﬁrst traversal. ROOT has only one dependent

(i.e., sold ) which we view as its ﬁrst right dependent.

its corresponding tree T , P(S|T ). We view the prob-
ability computation of a dependency tree as a gener-
ation process. Speciﬁcally, we assume dependency
trees are constructed top-down, in a breadth-ﬁrst
manner. Generation starts at the ROOT node. For
each node at each level, ﬁrst its left dependents are
generated from closest to farthest and then the right
dependents (again from closest to farthest). The
same process is applied to the next node at the same
level or a node at the next level. Figure 2 shows the
breadth-ﬁrst traversal of a dependency tree.

Under the assumption that each word w in a de-
pendency tree is only conditioned on its dependency
path, the probability of a sentence S given its depen-
dency tree T is:

P(S|T ) = ∏

P(w|D(w))

(1)

w∈BFS(T )\ROOT

where D(w) is the dependency path of w. Note that
each word w is visited according to its breadth-ﬁrst
search order (BFS(T)) and the probability of ROOT
is ignored since every tree has one. The role of
ROOT in a dependency tree is the same as the begin
of sentence token (BOS) in a sentence. When com-
puting P(S|T ) (or P(S)), the probability of ROOT (or
BOS) is ignored (we assume it always exists), but is
used to predict other words. We explain in the next
section how TREELSTM estimates P(w|D(w)).

The core problem in syntax-based language model-
ing is to estimate the probability of sentence S given

2.3 Tree LSTMs

1Throughout this paper we assume all dependency trees are

projective.

A dependency path D(w) is subtree which we de-
note as a sequence of (cid:104)word, edge-type(cid:105) tuples. Our

w0

w0

w3

w2

w1

w4

w5

w6

Generated by four LSTMs

with tied We and tied Who

w2

w1

w0

G

E

N

-

L

G E N-R

w0

w4

w5

GEN-NX-L

GEN-NX-L

GEN-NX-R

GEN-NX-R

w2

w1

w3
w6
Figure 3: Generation process of left (w1, w2, w3) and right
(w4, w5, w6) dependents of tree node wo (top) using four LSTMs
(GEN-L, GEN-R, GEN-NX-L and GEN-NX-R). The model can

w4

w5

handle an arbitrary number of dependents due to GEN-NX-L

and GEN-NX-R.

innovation is to learn the representation of D(w) us-
ing four LSTMs. The four LSTMs (GEN-L, GEN-
R, GEN-NX-L and GEN-NX-R) are used to repre-
sent the four types of edges (LEFT, RIGHT, NX-
LEFT and NX-RIGHT) introduced earlier. GEN,
NX, L and R are shorthands for GENERATE, NEXT,
LEFT and RIGHT. At each time step, an LSTM is
chosen according to an edge-type; then the LSTM
takes a word as input and predicts/generates its de-
pendent or sibling. This process can be also viewed
as adding an edge and a node to a tree. Speciﬁ-
cally, LSTMs GEN-L and GEN-R are used to gen-
erate the ﬁrst left and right dependent of a node
(w1 and w4 in Figure 3). So, these two LSTMs
are responsible for going deeper in a tree. While
GEN-NX-L and GEN-NX-R generate the remain-
ing left/right dependents and therefore go wider in
a tree. As shown in Figure 3, w2 and w3 are gener-
ated by GEN-NX-L, whereas w5 and w6 are gener-
ated by GEN-NX-R. Note that the model can handle
any number of left or right dependents by applying
GEN-NX-L or GEN-NX-R multiple times.

We assume time steps correspond to the steps
taken by the breadth-ﬁrst traversal of the depen-
dency tree and the sentence has length n. At
time step t (1 ≤ t ≤ n), let (cid:104)wt(cid:48), zt(cid:105) denote the last
Subscripts t and t(cid:48) denote the
tuple in D(wt).
breadth-ﬁrst search order of wt and wt(cid:48), respectively.
zt ∈ {LEFT, RIGHT, NX-LEFT, NX-RIGHT} is the
edge type (see the deﬁnitions in Section 2.1). Let
We ∈ Rs×|V | denote the word embedding matrix and

Who ∈ R|V |×d the output matrix of our model, where
|V | is the vocabulary size, s the word embedding size
and d the hidden unit size. We use tied We and tied
Who for the four LSTMs to reduce the number of pa-
rameters in our model. The four LSTMs also share
their hidden states. Let H ∈ Rd×(n+1) denote the
shared hidden states of all time steps and e(wt) the
one-hot vector of wt. Then, H[:,t] represents D(wt)
at time step t, and the computation2 is:

xt = We · e(wt(cid:48))
ht = LSTMzt (xt, H[:,t(cid:48)])

H[:,t] = ht

yt = Who · ht

(2a)

(2b)

(2c)

(2d)

where the initial hidden state H[:, 0] is initialized to
a vector of small values such as 0.01. According to
Equation (2b), the model selects an LSTM based on
edge type zt. We describe the details of LSTMzt in
the next paragraph. The probability of wt given its
dependency path D(wt) is estimated by a softmax
function:

P(wt|D(wt)) =

(3)

exp(yt,wt )
|V |
k(cid:48)=1 exp(yt,k(cid:48))

∑

We must point out that although we use four jointly
trained LSTMs to encode the hidden states, the train-
ing and inference complexity of our model is no dif-
ferent from a regular LSTM, since at each time step
only one LSTM is working.

We implement LSTMz in Equation (2b) using a
deep LSTM (to simplify notation, from now on we
write z instead of zt). The inputs at time step t
are xt and ht(cid:48) (the hidden state of an earlier time
step t(cid:48)) and the output is ht (the hidden state of cur-
rent time step). Let L denote the layer number of
LSTMz and ˆhl
t the internal hidden state of the l-th
layer of the LSTMz at time step t, where xt is ˆh0
t and
ht(cid:48) is ˆhL
t(cid:48). The LSTM architecture introduces mul-
tiplicative gates and memory cells ˆcl
t (at l-th layer)
in order to address the vanishing gradient problem
which makes it difﬁcult for the standard RNN model
to learn long-distance correlations in a sequence.
Here, ˆcl
t is a linear combination of the current input
signal ut and an earlier memory cell ˆcl
t(cid:48). How much
input information ut will ﬂow into ˆcl
t is controlled

2We ignore all bias terms for notational simplicity.

w2

w1

w0

G

E

N

-

L

G E N-R

w0

w4

w5

GEN-NX-L

GEN-NX-L

GEN-NX-R

GEN-NX-R

LD

LD

w0

w4

w5

w6

w3

w2

w1

Figure 4: Generation of left and right dependents of node w0
according to LDTREELSTM.

by input gate it and how much of the earlier mem-
ory cell ˆcl
t(cid:48) will be forgotten is controlled by forget
gate ft. This process is computed as follows:

ux · ˆhl−1

ut = tanh(Wz,l
it = σ(Wz,l
ft = σ(Wz,l
t = ft (cid:12) ˆcl
ˆcl

ix · ˆhl−1
f x · ˆhl−1
t(cid:48) + it (cid:12) ut

t + Wz,l
uh · ˆhl
t(cid:48))
ih · ˆhl
t(cid:48))
f h · ˆhl
t(cid:48))

t + Wz,l
t + Wz,l

(4a)

(4b)

(4c)

(4d)

ux ∈ Rd×d (Wz,l

uh ∈ Rd×d are weight matrices for ut, Wz,l
ih are weight matrices for it and Wz,l

where Wz,l
ux ∈ Rd×s when l = 1) and
Wz,l
ix and
f x, and Wz,l
Wz,l
f h
are weight matrices for ft. σ is a sigmoid function
and (cid:12) the element-wise product.

Output gate ot controls how much information of

the cell ˆcl

t can be seen by other modules:

t + Wz,l
ox · ˆhl−1
ot = σ(Wz,l
ˆhl
t = ot (cid:12) tanh(ˆcl
t )

oh · ˆhl
t(cid:48))

(5a)

(5b)

Application of the above process to all layers L, will
yield ˆhL
t , which is ht. Note that in implementation,
all ˆcl
t (1 ≤ l ≤ L) at time step t are stored,
although we only care about ˆhL
t (ht).

t and ˆhl

2.4 Left Dependent Tree LSTMs

TREELSTM computes P(w|D(w)) based on the de-
pendency path D(w), which ignores the interaction
between left and right dependents on the same level.
In many cases, TREELSTM will use a verb to pre-
dict its object directly without knowing its subject.
For example, in Figure 2, TREELSTM uses (cid:104)ROOT,
RIGHT(cid:105) and (cid:104) sold, RIGHT (cid:105) to predict cars. This in-
formation is unfortunately not speciﬁc to cars (many
things can be sold, e.g., chocolates, candy). Consid-
ering manufacturer, the left dependent of sold would
help predict cars more accurately.

In order to jointly take left and right dependents
into account, we employ yet another LSTM, which
goes from the furthest left dependent to the closest
left dependent (LD is a shorthand for left depen-
dent). As shown in Figure 4, LD LSTM learns the
representation of all left dependents of a node w0;
this representation is then used to predict the ﬁrst
right dependent of the same node. Non-ﬁrst right de-
pendents can also leverage the representation of left
dependents, since this information is injected into
the hidden state of the ﬁrst right dependent and can
percolate all the way. Note that in order to retain the
generation capability of our model (Section 3.4), we
only allow right dependents to leverage left depen-
dents (they are generated before right dependents).

The computation of the LDTREELSTM is al-
the same as in TREELSTM except when
most
zt = GEN-R.
let vt be the cor-
responding left dependent sequence with length
K (vt = (w3, w2, w1) in Figure 4). Then, the hidden
state (qk) of vt at each time step k is:

In this case,

mk = We · e(vt,k)
qk = LSTMLD(mk, qk−1)

(6a)

(6b)

where qK is the representation for all left depen-
dents. Then, the computation of the current hid-
den state becomes (see Equation (2) for the original
computation):

rt =

(cid:21)
(cid:20)We · e(wt(cid:48))
qK

ht = LSTMGEN-R(rt, H[:,t(cid:48)])

(7a)

(7b)

where qK serves as additional input for LSTMGEN-R.
All other computational details are the same as in
TreeLSTM (see Section 2.3).

2.5 Model Training

On small scale datasets we employ Negative Log-
likelihood (NLL) as our training objective for both
TREELSTM and LDTREELSTM:

L NLL(θ) = −

log P(S|T )

(8)

1
|S | ∑

S∈S

where S is a sentence in the training set S , T is the
dependency tree of S and P(S|T ) is deﬁned as in
Equation (1).

On large scale datasets (e.g., with vocabulary
size of 65K), computing the output layer activa-
tions and the softmax function with NLL would
become prohibitively expensive.
Instead, we em-
ploy Noise Contrastive Estimation (NCE; Gutmann
and Hyv¨arinen (2012), Mnih and Teh (2012)) which
treats the normalization term ˆZ in ˆP(w|D(wt)) =
exp(Who[w,:]·ht )
as constant. The intuition behind NCE
ˆZ
is to discriminate between samples from a data dis-
tribution ˆP(w|D(wt)) and a known noise distribu-
tion Pn(w) via binary logistic regression. Assuming
that noise words are k times more frequent than real
words in the training set (Mnih and Teh, 2012), then
the probability of a word w being from our model
Pd(w,D(wt)) is
. We apply NCE to
large vocabulary models with the following training
objective:

ˆP(w|D(wt ))
ˆP(w|D(wt ))+kPn(w)

L NCE(θ) = −

log Pd(wt,D(wt))

(cid:18)

1
|S | ∑

T ∈S

|T |
∑
t=1

+

log[1 − Pd( ˜wt, j,D(wt))]

(cid:19)

k
∑
j=1

where ˜wt, j is a word sampled from the noise distri-
bution Pn(w). We use smoothed unigram frequen-
cies (exponentiating by 0.75) as the noise distribu-
tion Pn(w) (Mikolov et al., 2013b). We initialize
ln ˆZ = 9 as suggested in Chen et al. (2015), but in-
stead of keeping it ﬁxed we also learn ˆZ during train-
ing (Vaswani et al., 2013). We set k = 20.

3 Experiments

We assess the performance of our model on two
tasks: the Microsoft Research (MSR) sentence com-
pletion challenge (Zweig and Burges, 2012), and de-
pendency parsing reranking. We also demonstrate
the tree generation capability of our models. In the
following, we ﬁrst present details on model train-
ing and then present our results. We implemented
our models using the Torch library (Collobert et
al., 2011) and our code is available at https://
github.com/XingxingZhang/td-treelstm.

3.1 Training Details

We trained our model with back propagation
through time (Rumelhart et al., 1988) on an Nvidia

GPU Card with a mini-batch size of 64. The ob-
jective (NLL or NCE) was minimized by stochastic
gradient descent. Model parameters were uniformly
initialized in [−0.1, 0.1]. We used the NCE objec-
tive on the MSR sentence completion task (due to
the large size of this dataset) and the NLL objec-
tive on dependency parsing reranking. We used an
initial learning rate of 1.0 for all experiments and
when there was no signiﬁcant improvement in log-
likelihood on the validation set, the learning rate was
divided by 2 per epoch until convergence (Mikolov
et al., 2010). To alleviate the exploding gradients
problem, we rescaled the gradient g when the gradi-
ent norm ||g|| > 5 and set g = 5g
||g|| (Pascanu et al.,
2013; Sutskever et al., 2014). Dropout (Srivastava
et al., 2014) was applied to the 2-layer TREELSTM
and LDTREELSTM models. The word embedding
size was set to s = d/2 where d is the hidden unit
size.

3.2 Microsoft Sentence Completion Challenge

The task in the MSR Sentence Completion Chal-
lenge (Zweig and Burges, 2012) is to select the
correct missing word for 1,040 SAT-style test sen-
tences when presented with ﬁve candidate comple-
tions. The training set contains 522 novels from
the Project Gutenberg which we preprocessed as fol-
lows. After removing headers and footers from the
ﬁles, we tokenized and parsed the dataset into de-
pendency trees with the Stanford Core NLP toolkit
(Manning et al., 2014). The resulting training set
contained 49M words. We converted all words to
lower case and replaced those occurring ﬁve times
or less with UNK. The resulting vocabulary size
was 65,346 words. We randomly sampled 4,000
sentences from the training set as our validation set.
The literature describes two main approaches to
the sentence completion task based on word vectors
and language models. In vector-based approaches,
all words in the sentence and the ﬁve candidate
words are represented by a vector; the candidate
which has the highest average similarity with the
sentence words is selected as the answer. For lan-
guage model-based methods, the LM computes the
probability of a test sentence with each of the ﬁve
candidate words, and picks the candidate comple-
tion which gives the highest probability. Our model
belongs to this class of models.

|θ|

Accuracy

d

—
640
600

Model
Word Vector based Models
LSA
Skip-gram
IVLBL
Language Models
KN5
UDepNgram
LDepNgram
RNN
RNNME
depRNN+3gram
ldepRNN+4gram
LBL
LSTM
LSTM
LSTM
Bidirectional LSTM
Bidirectional LSTM
Bidirectional LSTM
Model Combinations
RNNMEs
—
Skip-gram + RNNMEs —
Our Models
TREELSTM
LDTREELSTM
TREELSTM
LDTREELSTM

—
—
—
300
300
100
200
300
300
400
450
200
300
400

300
300
400
400

—
102M
96.0M

—
—
—
48.1M
1120M
1014M
1029M
48.0M
29.9M
40.2M
45.3M
33.2M
50.1M
67.3M

—
—

31.6M
32.5M
43.1M
44.7M

49.0
48.0
55.5

40.0
48.3
50.0
45.0
49.3
53.5
50.7
54.7
55.00
57.02
55.96
48.46
49.90
48.65

55.4
58.9

55.29
57.79
56.73
60.67

Table 1: Model accuracy on the MSR sentence completion task.

The results of KN5, RNNME and RNNMEs are reported in

Mikolov (2012), LSA and RNN in Zweig et al. (2012), UDep-

Ngram and LDepNgram in Gubbins and Vlachos (2013), de-

pRNN+3gram and depRNN+4gram in Mirowski and Vlachos

(2015), LBL in Mnih and Teh (2012), Skip-gram and Skip-

gram+RNNMEs in Mikolov et al. (2013a), and IVLBL in Mnih
and Kavukcuoglu (2013); d is the hidden size and |θ| the num-
ber of parameters in a model.

Table 1 presents a summary of our results to-
gether with previoulsy published results. The best
performing word vector model is IVLBL (Mnih and
Kavukcuoglu, 2013) with an accuracy of 55.5, while
the best performing single language model is LBL
(Mnih and Teh, 2012) with an accuracy of 54.7.
Both approaches are based on the log-bilinear lan-
guage model (Mnih and Hinton, 2007). A combi-
nation of several recurrent neural networks and the
skip-gram model holds the state of the art with an
accuracy of 58.9 (Mikolov et al., 2013b). To fairly
compare with existing models, we restrict the layer

Test

Parser

Development
UAS
MSTParser-2nd
92.20
TREELSTM
92.51
92.64
TREELSTM*
LDTREELSTM 92.66
92.00
NN parser*
93.20
S-LSTM*

LAS
88.44
88.53
88.69
88.69
89.60
90.90
Table 2: Performance of TREELSTM and LDTREELSTM on

LAS UAS
91.63
88.78
91.79
89.07
91.97
89.09
91.99
89.14
91.80
89.70
93.10
90.90

reranking the top dependency trees produced by the 2nd order

MSTParser (McDonald and Pereira, 2006). Results for the NN

and S-LSTM parsers are reported in Chen and Manning (2014)

and Dyer et al. (2015), respectively. * indicates that the model

is initialized with pre-trained word vectors.

size of our models to 1. We observe that LDTREEL-
STM consistently outperforms TREELSTM, which
indicates the importance of modeling the interac-
In fact,
tion between left and right dependents.
LDTREELSTM (d = 400) achieves a new state-of-
the-art on this task, despite being a single model.
We also implement LSTM and bidirectional LSTM
language models.3 An LSTM with d = 400 out-
performs its smaller counterpart (d = 300), however
performance decreases with d = 450. The bidirec-
tional LSTM is worse than the LSTM (see Mnih
and Teh (2012) for a similar observation). The
best performing LSTM is worse than a LDTREEL-
STM (d = 300). The input and output embeddings
(We and Who) dominate the number of parame-
ters in all neural models except for RNNME, de-
pRNN+3gram and ldepRNN+4gram, which include
a ME model that contains 1 billion sparse n-gram
features (Mikolov, 2012; Mirowski and Vlachos,
2015). The number of parameters in TREELSTM
and LDTREELSTM is not much larger compared to
LSTM due to the tied We and Who matrices.

3.3 Dependency Parsing

In this section we demonstrate that our model can
be also used for parse reranking. This is not possi-
ble for sequence-based language models since they
cannot estimate the probability of a tree. We use
our models to rerank the top K dependency trees
produced by the second order MSTParser (McDon-

3LSTMs and BiLSTMs were also trained with NCE
(s = d/2; hyperparameters were tuned on the development set).

ald and Pereira, 2006).4 We follow closely the ex-
perimental setup of Chen and Manning (2014) and
Dyer et al. (2015). Speciﬁcally, we trained TREEL-
STM and LDTREELSTM on Penn Treebank sec-
tions 2–21. We used section 22 for development and
section 23 for testing. We adopted the Stanford ba-
sic dependency representations (De Marneffe et al.,
2006); part-of-speech tags were predicted with the
Stanford Tagger (Toutanova et al., 2003). We trained
TREELSTM and LDTREELSTM as language mod-
els (singletons were replaced with UNK) and did
not use any POS tags, dependency labels or com-
position features, whereas these features are used in
Chen and Manning (2014) and Dyer et al. (2015).
We tuned d, the number of layers, and K on the de-
velopment set.

Table 2 reports unlabeled attachment scores
(UAS) and labeled attachment scores (LAS) for
the MSTParser, TREELSTM (d = 300, 1 layer,
K = 2), and LDTREELSTM (d = 200, 2 layers,
K = 4). We also include the performance of two
neural network-based dependency parsers; Chen and
Manning (2014) use a neural network classiﬁer to
predict the correct transition (NN parser); Dyer et
al. (2015) also implement a transition-based depen-
dency parser using LSTMs to represent the contents
of the stack and buffer in a continuous space. As can
be seen, both TREELSTM and LDTREELSTM out-
perform the baseline MSTParser, with LDTREEL-
STM performing best. We also initialized the word
embedding matrix We with pre-trained GLOVE vec-
tors (Pennington et al., 2014). We obtained a slight
improvement over TREELSTM (TREELSTM* in
Table 2; d = 200, 2 layer, K = 4) but no im-
provement over LDTREELSTM. Finally, notice that
LDTREELSTM is slightly better than the NN parser
in terms of UAS but worse than the S-LSTM parser.
In the future, we would like to extend our model so
that it takes labeled dependency information into ac-
count.

3.4 Tree Generation

This section demonstrates how to use a trained
LDTREELSTM to generate tree samples. The gen-
eration starts at the ROOT node. At each time step t,
for each node wt, we add a new edge and node to

Figure 5: Generated dependency trees with LDTREELSTM

trained on the PTB.

the tree. Unfortunately during generation, we do not
know which type of edge to add. We therefore use
four binary classiﬁers (ADD-LEFT, ADD-RIGHT,
ADD-NX-LEFT and ADD-NX-RIGHT) to predict
whether we should add a LEFT, RIGHT, NX-LEFT
or NX-RIGHT edge.5 Then when a classiﬁer pre-
dicts true, we use the corresponding LSTM to gener-
ate a new node by sampling from the predicted word
distribution in Equation (3). The four classiﬁers take
the previous hidden state H[:,t(cid:48)] and the output em-
bedding of the current node Who · e(wt) as features.6
Speciﬁcally, we use a trained LDTREELSTM to
go through the training corpus and generate hidden
states and embeddings as input features; the corre-
sponding class labels (true and false) are “read off”
the training dependency trees. We use two-layer rec-
tiﬁer networks (Glorot et al., 2011) as the four clas-
siﬁers with a hidden size of 300. We use the same
LDTREELSTM model as in Section 3.3 to gener-
ate dependency trees. The classiﬁers were trained
using AdaGrad (Duchi et al., 2011) with a learning
rate of 0.01. The accuracies of ADD-LEFT, ADD-
RIGHT, ADD-NX-LEFT and ADD-NX-RIGHT are

5It is possible to get rid of the four classiﬁers by adding
START/STOP symbols when generating left and right depen-
dents as in (Eisner, 1996). We refrained from doing this for
computational reasons. For a sentence with N words, this ap-
proach will lead to 2N additional START/STOP symbols (with
one START and one STOP symbol for each word). Conse-
quently, the computational cost and memory consumption dur-
ing training will be three times as much rendering our model
less scalable.

6The input embeddings have lower dimensions and therefore

4http://www.seas.upenn.edu/ strctlrn/MSTParser

result in slightly worse classiﬁers.

94.3%, 92.6%, 93.4% and 96.0%, respectively. Fig-
ure 5 shows examples of generated trees.

4 Conclusions

In this paper we developed TREELSTM (and
LDTREELSTM), a neural network model architec-
ture, which is designed to predict tree structures
rather than linear sequences. Experimental results
on the MSR sentence completion task show that
LDTREELSTM is superior to sequential LSTMs.
Dependency parsing reranking experiments high-
light our model’s potential for dependency pars-
ing. Finally, the ability of our model to gener-
ate dependency trees holds promise for text gen-
eration applications such as sentence compression
and simpliﬁcation (Filippova et al., 2015). Although
our experiments have focused exclusively on depen-
dency trees, there is nothing inherent in our formu-
lation that disallows its application to other types of
tree structure such as constituent trees or even tax-
onomies.

Acknowledgments

We would like to thank Adam Lopez, Frank Keller,
Iain Murray, Li Dong, Brian Roark, and the NAACL
reviewers for their valuable feedback. Xingxing
Zhang gratefully acknowledges the ﬁnancial sup-
port of the China Scholarship Council (CSC). Liang
Lu is funded by the UK EPSRC Programme Grant
EP/I031022/1, Natural Speech Technology (NST).

References

[Bengio et al.2003] Yoshua Bengio, R´ejean Ducharme,
Pascal Vincent, and Christian Janvin. 2003. A neural
probabilistic language model. The Journal of Machine
Learning Research, 3:1137–1155.

[Charniak2001] Eugene Charniak.

Immediate-
head parsing for language models. In Proceedings of
the 39th Annual Meeting on Association for Compu-
tational Linguistics, pages 124–131. Association for
Computational Linguistics.

2001.

[Chelba and Jelinek2000] Ciprian Chelba and Frederick
Jelinek. 2000. Structured language modeling. Com-
puter Speech and Language, 14(4):283–332.

[Chelba et al.1997] Ciprian Chelba, David Engle, Freder-
ick Jelinek, Victor Jimenez, Sanjeev Khudanpur, Lidia
Mangu, Harry Printz, Eric Ristad, Ronald Rosenfeld,

Andreas Stolcke, et al.
formance of a dependency language model.
ROSPEECH. Citeseer.

1997. Structure and per-
In EU-

[Chen and Manning2014] Danqi Chen and Christopher
2014. A fast and accurate dependency
Manning.
In Proceedings of
parser using neural networks.
the 2014 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 740–750,
Doha, Qatar, October. Association for Computational
Linguistics.

[Chen et al.2015] X Chen, X Liu, MJF Gales, and
PC Woodland. 2015. Recurrent neural network lan-
guage model training with noise contrastive estimation
for speech recognition. In In 40th IEEE International
Conference on Accoustics, Speech and Signal Process-
ing, pages 5401–5405, Brisbane, Australia.

[Collobert et al.2011] Ronan

Collobert,

Koray
Kavukcuoglu, and Cl´ement Farabet. 2011. Torch7:
A matlab-like environment for machine learning.
In
BigLearn, NIPS Workshop, number EPFL-CONF-
192376.

[De Marneffe et al.2006] Marie-Catherine De Marneffe,
Bill MacCartney, Christopher D Manning, et al. 2006.
Generating typed dependency parses from phrase
structure parses. In Proceedings of LREC, volume 6,
pages 449–454.

[Duchi et al.2011] John Duchi, Elad Hazan, and Yoram
Singer. 2011. Adaptive subgradient methods for on-
line learning and stochastic optimization. The Journal
of Machine Learning Research, 12:2121–2159.

[Dyer et al.2015] Chris Dyer, Miguel Ballesteros, Wang
Ling, Austin Matthews, and Noah A. Smith. 2015.
Transition-based dependency parsing with stack long
In Proceedings of the 53rd An-
short-term memory.
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long Pa-
pers), pages 334–343, Beijing, China, July. Associa-
tion for Computational Linguistics.

[Eisner1996] Jason M Eisner. 1996. Three new prob-
abilistic models for dependency parsing: An explo-
ration. In Proceedings of the 16th conference on Com-
putational linguistics-Volume 1, pages 340–345. Asso-
ciation for Computational Linguistics.

[Emami et al.2003] Ahmad Emami, Peng Xu, and Fred-
erick Jelinek. 2003. Using a connectionist model in
In Proceedings
a syntactical based language model.
of the IEEE International Conference on Acoustics,
Speech, and Signal Processing, pages 372–375, Hong
Kong, China.

[Filippova et al.2015] Katja Filippova, Enrique Alfon-
seca, Carlos A Colmenares, Lukasz Kaiser, and Oriol
Vinyals. 2015. Sentence compression by deletion
with lstms. In EMNLP, pages 360–368.

[Glorot et al.2011] Xavier Glorot, Antoine Bordes, and
Yoshua Bengio. 2011. Deep sparse rectiﬁer neural
networks. In International Conference on Artiﬁcial In-
telligence and Statistics, pages 315–323.

[Graves et al.2013] Alan Graves, Abdel-rahman Mo-
hamed, and Geoffrey Hinton. 2013. Speech recogni-
tion with deep recurrent neural networks. In Acoustics,
Speech and Signal Processing (ICASSP), 2013 IEEE
International Conference on, pages 6645–6649. IEEE.
[Gubbins and Vlachos2013] Joseph Gubbins and Andreas
Vlachos. 2013. Dependency language models for sen-
tence completion. In EMNLP, pages 1405–1410, Seat-
tle, Washington, USA, October. Association for Com-
putational Linguistics.

[Gutmann and Hyv¨arinen2012] Michael U Gutmann and
Aapo Hyv¨arinen. 2012. Noise-contrastive estimation
of unnormalized statistical models, with applications
to natural image statistics. The Journal of Machine
Learning Research, 13(1):307–361.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural computation, 9(8):1735–1780.

[Hochreiter1998] Sepp Hochreiter. 1998. Vanishing gra-
dient problem during learning recurrent neural nets
and problem solutions. International Journal of Un-
certainty, Fuzziness and Knowledge-based Systems,
6(2):107–116.

[Manning et al.2014] Christopher D Manning, Mihai Sur-
deanu, John Bauer, Jenny Finkel, Steven J Bethard,
and David McClosky. 2014. The stanford corenlp
In Proceedings
natural language processing toolkit.
of 52nd Annual Meeting of the Association for Com-
putational Linguistics: System Demonstrations, pages
55–60.

[McDonald and Pereira2006] Ryan T McDonald and Fer-
nando CN Pereira. 2006. Online learning of approxi-
mate dependency parsing algorithms. In EACL.

[Mikolov et al.2010] Tomas Mikolov, Martin Karaﬁ´at,
Lukas Burget, Jan Cernock`y, and Sanjeev Khudan-
pur. 2010. Recurrent neural network based language
model. In INTERSPEECH 2010, 11th Annual Confer-
ence of the International Speech Communication As-
sociation, Makuhari, Chiba, Japan, September 26-30,
2010, pages 1045–1048.

[Mikolov et al.2013a] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013a. Efﬁcient esti-
In
mation of word representations in vector space.
Proceedings of the 2013 International Conference on
Learning Representations, Scottsdale, Arizona, USA.
Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013b.
Distributed representations of words and phrases and
In Advances in Neural Infor-
their compositionality.
mation Processing Systems 26, pages 3111–3119.

[Mikolov et al.2013b] Tomas Mikolov,

[Mikolov2012] Tomas Mikolov. 2012. Statistical Lan-
guage Models based on Neural Networks. Ph.D. the-
sis, Brno University of Technology.

[Mirowski and Vlachos2015] Piotr Mirowski and An-
dreas Vlachos. 2015. Dependency recurrent neural
In ACL,
language models for sentence completion.
pages 511–517, Beijing, China, July. Association for
Computational Linguistics.

[Mnih and Hinton2007] Andriy Mnih and Geoffrey Hin-
ton. 2007. Three new graphical models for statistical
In Proceedings of the 24th In-
language modelling.
ternational Conference on Machine Learning, pages
641–648.

[Mnih and Kavukcuoglu2013] Andriy Mnih and Koray
Kavukcuoglu. 2013. Learning word embeddings efﬁ-
ciently with noise-contrastive estimation. In Advances
in Neural Information Processing Systems 26, pages
2265–2273.

[Mnih and Teh2012] Andriy Mnih and Yee Whye Teh.
2012. A fast and simple algorithm for training neural
probabilistic language models. In Proceedings of the
29th International Conference on Machine Learning,
pages 1751–1758, Edinburgh, Scotland.

[Pascanu et al.2013] Razvan Pascanu, Tomas Mikolov,
and Yoshua Bengio. 2013. On the difﬁculty of train-
ing recurrent neural networks. In Proceedings of the
31st International Conference on Machine Learning,
pages 1310–1318, Atlanta, Georgia, USA.
Pennington,

Richard
Socher, and Christopher D Manning. 2014. Glove:
EMNLP,
Global vectors for word representation.
12:1532–1543.

[Pennington et al.2014] Jeffrey

[Pollack1990] Jordan B. Pollack. 1990. Recursive dis-
tributed representations. Artiﬁcial Intelligence, 1–
2(46):77–105.

[Roark2001] Brian Roark. 2001. Probabilistic top-down
parsing and language modeling. Computational lin-
guistics, 27(2):249–276.

[Rumelhart et al.1988] David E Rumelhart, Geoffrey E
Hinton, and Ronald J Williams. 1988. Learning repre-
sentations by back-propagating errors. Cognitive mod-
eling, 5:3.

[Sennrich2015] Rico Sennrich. 2015. Modelling and op-
timizing on syntactic n-grams for statistical machine
translation. Transactions of the Association for Com-
putational Linguistics, 3:169–182.

[Shen et al.2008] Libin Shen,

Jinxi Xu,

and Ralph
Weischedel. 2008. A new string-to-dependency ma-
chine translation algorithm with a target dependency
In Proceedings of ACL-08: HLT,
language model.
pages 577–585, Columbus, Ohio, USA.

[Socher et al.2011a] Richard Socher, Eric H. Huang, Jef-
frey Pennington, Christopher D. Manning, and An-
drew Ng. 2011a. Dynamic pooling and unfolding

Model? On the Future of Language Modeling for HLT,
pages 29–36, Montr´eal, Canada.

[Zweig et al.2012] Geoffrey Zweig, John C Platt, Christo-
pher Meek, Christopher JC Burges, Ainur Yessenalina,
2012. Computational approaches
and Qiang Liu.
In Proceedings of the 50th
to sentence completion.
Annual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 601–610.
Association for Computational Linguistics.

In
recursive autoencoders for paraphrase detection.
Advances in Neural Information Processing Systems,
pages 801–809.

[Socher et al.2011b] Richard Socher, Jeffrey Pennington,
Eric H. Huang, Andrew Y. Ng, and Christopher D.
Manning. 2011b. Semi-supervised recursive autoen-
coders for predicting sentiment distributions. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 151–161,
Edinburgh, Scotland, UK.

[Srivastava et al.2014] Nitish Srivastava, Geoffrey Hin-
ton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: A simple way to pre-
vent neural networks from overﬁtting. The Journal of
Machine Learning Research, 15(1):1929–1958.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and
Quoc VV Le. 2014. Sequence to sequence learning
with neural networks. In Advances in Neural Informa-
tion Processing Systems, pages 3104–3112.

[Tai et al.2015] Kai Sheng Tai, Richard Socher, and
Christopher D. Manning. 2015.
Improved semantic
representations from tree-structured long short-term
memory networks. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguis-
tics and the 7th International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers),
pages 1556–1566, Beijing, China, July. Association
for Computational Linguistics.

[Toutanova et al.2003] Kristina Toutanova, Dan Klein,
Christopher D Manning, and Yoram Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic de-
pendency network. In Proceedings of the 2003 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology-Volume 1, pages 173–180. Associa-
tion for Computational Linguistics.

[Vaswani et al.2013] Ashish Vaswani, Yinggong Zhao,
Victoria Fossum, and David Chiang. 2013. Decod-
ing with large-scale neural language models improves
translation. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
pages 1387–1392, Seattle, Washington, USA.

[Vinyals et al.2015] Oriol Vinyals, Alexander Toshev,
Samy Bengio, and Dumitru Erhan. 2015. Show and
tell: A neural image caption generator. In The IEEE
Conference on Computer Vision and Pattern Recogni-
tion, Boston, Massachusetts, USA.

[Zhang2009] Ying Zhang. 2009. Structured language
models for statistical machine translation. Ph.D. the-
sis, Johns Hopkins University.

[Zweig and Burges2012] Geoffrey Zweig and Chris J.C.
Burges. 2012. A challenge set for advancing language
In Proceedings of the NAACL-HLT 2012
modeling.
Workshop: Will We Ever Really Replace the N-gram

Top-down Tree Long Short-Term Memory Networks

Xingxing Zhang, Liang Lu and Mirella Lapata
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
{x.zhang,liang.lu}@ed.ac.uk,mlap@inf.ed.ac.uk

6
1
0
2
 
r
p
A
 
3
 
 
]
L
C
.
s
c
[
 
 
3
v
0
6
0
0
0
.
1
1
5
1
:
v
i
X
r
a

Abstract

Long Short-Term Memory (LSTM) networks,
a type of recurrent neural network with a
more complex computational unit, have been
successfully applied to a variety of sequence
modeling tasks. In this paper we develop Tree
Long Short-Term Memory (TREELSTM), a
neural network model based on LSTM, which
is designed to predict a tree rather than a lin-
ear sequence. TREELSTM deﬁnes the prob-
ability of a sentence by estimating the gener-
ation probability of its dependency tree. At
each time step, a node is generated based
on the representation of the generated sub-
tree. We further enhance the modeling power
of TREELSTM by explicitly representing the
correlations between left and right depen-
dents. Application of our model to the MSR
sentence completion challenge achieves re-
sults beyond the current state of the art. We
also report results on dependency parsing
reranking achieving competitive performance.

1

Introduction

Neural language models have been gaining increas-
ing attention as a competitive alternative to n-grams.
The main idea is to represent each word using a
real-valued feature vector capturing the contexts in
which it occurs. The conditional probability of the
next word is then modeled as a smooth function of
the feature vectors of the preceding words and the
In essence, similar representations are
next word.
learned for words found in similar contexts result-
ing in similar predictions for the next word. Previ-
ous approaches have mainly employed feed-forward

(Bengio et al., 2003; Mnih and Hinton, 2007) and
recurrent neural networks (Mikolov et al., 2010;
Mikolov, 2012) in order to map the feature vec-
tors of the context words to the distribution for the
next word. Recently, RNNs with Long Short-Term
Memory (LSTM) units (Hochreiter and Schmidhu-
ber, 1997; Hochreiter, 1998) have emerged as a pop-
ular architecture due to their strong ability to capture
long-term dependencies. LSTMs have been success-
fully applied to a variety of tasks ranging from ma-
chine translation (Sutskever et al., 2014), to speech
recognition (Graves et al., 2013), and image descrip-
tion generation (Vinyals et al., 2015).

Despite superior performance in many applica-
tions, neural language models essentially predict se-
quences of words. Many NLP tasks, however, ex-
ploit syntactic information operating over tree struc-
tures (e.g., dependency or constituent trees). In this
paper we develop a novel neural network model
which combines the advantages of the LSTM archi-
tecture and syntactic structure. Our model estimates
the probability of a sentence by estimating the gen-
eration probability of its dependency tree. Instead
of explicitly encoding tree structure as a set of fea-
tures, we use four LSTM networks to model four
types of dependency edges which altogether specify
how the tree is built. At each time step, one LSTM is
activated which predicts the next word conditioned
on the sub-tree generated so far. To learn the repre-
sentations of the conditioned sub-tree, we force the
four LSTMs to share their hidden layers. Our model
is also capable of generating trees just by sampling
from a trained model and can be seamlessly inte-
grated with text generation applications.

Our approach is related to but ultimately differ-
ent from recursive neural networks (Pollack, 1990)
a class of models which operate on structured in-
puts. Given a (binary) parse tree, they recursively
generate parent representations in a bottom-up fash-
ion, by combining tokens to produce representations
for phrases, and eventually the whole sentence. The
learned representations can be then used in classi-
ﬁcation tasks such as sentiment analysis (Socher et
al., 2011b) and paraphrase detection (Socher et al.,
2011a). Tai et al. (2015) learn distributed representa-
tions over syntactic trees by generalizing the LSTM
architecture to tree-structured network topologies.
The key feature of our model is not so much that
it can learn semantic representations of phrases or
sentences, but its ability to predict tree structure and
estimate its probability.

Syntactic language models have a long history
in NLP dating back to Chelba and Jelinek (2000)
(see also Roark (2001) and Charniak (2001)). These
models differ in how grammar structures in a parsing
tree are used when predicting the next word. Other
work develops dependency-based language models
for speciﬁc applications such as machine translation
(Shen et al., 2008; Zhang, 2009; Sennrich, 2015),
speech recognition (Chelba et al., 1997) or sentence
completion (Gubbins and Vlachos, 2013). All in-
stances of these models apply Markov assumptions
on the dependency tree, and adopt standard n-gram
smoothing methods for reliable parameter estima-
tion. Emami et al. (2003) and Sennrich (2015) esti-
mate the parameters of a structured language model
using feed-forward neural networks (Bengio et al.,
2003). Mirowski and Vlachos (2015) re-implement
the model of Gubbins and Vlachos (2013) with
RNNs. They view sentences as sequences of words
over a tree. While they ignore the tree structures
themselves, we model them explicitly.

Our model shares with other structured-based lan-
guage models the ability to take dependency infor-
mation into account. It differs in the following re-
spects: (a) it does not artiﬁcially restrict the depth
of the dependencies it considers and can thus be
viewed as an inﬁnite order dependency language
model; (b) it not only estimates the probability of a
string but is also capable of generating dependency
trees; (c) ﬁnally, contrary to previous dependency-
based language models which encode syntactic in-

formation as features, our model takes tree structure
into account more directly via representing different
types of dependency edges explicitly using LSTMs.
Therefore, there is no need to manually determine
which dependency tree features should be used or
how large the feature embeddings should be.

We evaluate our model on the MSR sentence com-
pletion challenge, a benchmark language modeling
dataset. Our results outperform the best published
results on this dataset. Since our model is a general
tree estimator, we also use it to rerank the top K de-
pendency trees from the (second order) MSTPasrser
and obtain performance on par with recently pro-
posed dependency parsers.

2 Tree Long Short-Term Memory

Networks

We seek to estimate the probability of a sentence by
estimating the generation probability of its depen-
dency tree. Syntactic information in our model is
represented in the form of dependency paths. In the
following, we ﬁrst describe our deﬁnition of depen-
dency path and based on it explain how the proba-
bility of a sentence is estimated.

2.1 Dependency Path

Generally speaking, a dependency path is the path
between ROOT and w consisting of the nodes on
the path and the edges connecting them. To rep-
resent dependency paths, we introduce four types
of edges which essentially deﬁne the “shape” of a
dependency tree. Let w0 denote a node in a tree
and w1, w2, . . . , wn its left dependents. As shown in
Figure 1, LEFT edge is the edge between w0 and
its ﬁrst left dependent denoted as (w0, w1). Let wk
(with 1 < k ≤ n) denote a non-ﬁrst left dependent
of w0. The edge from wk−1 to wk is a NX-LEFT
edge (NX stands for NEXT), where wk−1 is the right
adjacent sibling of wk. Note that the NX-LEFT edge
(wk−1, wk) replaces edge (w0, wk) (illustrated with a
dashed line in Figure 1) in the original dependency
tree. The modiﬁcation allows information to ﬂow
from w0 to wk through w1, . . . , wk−1 rather than di-
rectly from w0 to wk. RIGHT and NX-RIGHT edges
are deﬁned analogously for right dependents.

Given these four types of edges, dependency
paths (denoted as D(w)) can be deﬁned as follows

w0

wn

wk

wk−1

w1

LEFT

NX-LEFT

Figure 1: LEFT and NX-LEFT edges. Dotted line between
w1 and wk−1 (also between wk and wn) indicate that there may
be ≥ 0 nodes inbetween.

bearing in mind that the ﬁrst right dependent of
ROOT is its only dependent and that wp denotes the
parent of w. We use (. . . ) to denote a sequence,
where () is an empty sequence and (cid:107) is an operator
for concatenating two sequences.

(1) if w is ROOT, then D(w) = ()
(2) if w is a left dependent of wp
(a) if w is the ﬁrst

left dependent,

then

D(w) = D(wp)(cid:107)((cid:104)wp, LEFT(cid:105))

(b) if w is not the ﬁrst left dependent and ws is

its right adjacent sibling, then
D(w) = D(ws)(cid:107)((cid:104)ws, NX-LEFT(cid:105))

(3) if w is a right dependent of wp

(a) if w is the ﬁrst right dependent,
D(w) = D(wp)(cid:107)((cid:104)wp, RIGHT(cid:105))

then

(b) if w is not the ﬁrst right dependent and ws

is its left adjacent sibling, then
D(w) = D(ws)(cid:107)((cid:104)ws, NX-RIGHT(cid:105))

A dependency tree can be represented by the set of
its dependency paths which in turn can be used to
reconstruct the original tree.1

for

the ﬁrst

the moment

Dependency paths

two levels
the tree in Figure 2 are as follows (ig-
of
the subscripts which
noring for
D(sold) =
we explain in the next section).
((cid:104)ROOT, RIGHT(cid:105)) (see deﬁnitions (1) and (3a)),
D(year) = D(sold)(cid:107)((cid:104)sold, LEFT(cid:105))
(2a)),
D(manufacturer) = D(year)(cid:107)((cid:104)year, NX-LEFT(cid:105))
(see (2b)), D(cars) = D(sold)(cid:107)((cid:104)sold, RIGHT(cid:105))
(see (3a)), D(in) = D(cars)(cid:107)((cid:104)cars, NX-RIGHT(cid:105))
(according to (3b)).

(see

2.2 Tree Probability

ROOT

sold1

manufacturer3

year2

cars4

in5

The9

luxury8 auto7

last6

1,21410

U.S.11

the12

Figure 2: Dependency tree of the sentence The luxury auto

manufacturer last year sold 1,214 cars in the U.S. Subscripts

indicate breadth-ﬁrst traversal. ROOT has only one dependent

(i.e., sold ) which we view as its ﬁrst right dependent.

its corresponding tree T , P(S|T ). We view the prob-
ability computation of a dependency tree as a gener-
ation process. Speciﬁcally, we assume dependency
trees are constructed top-down, in a breadth-ﬁrst
manner. Generation starts at the ROOT node. For
each node at each level, ﬁrst its left dependents are
generated from closest to farthest and then the right
dependents (again from closest to farthest). The
same process is applied to the next node at the same
level or a node at the next level. Figure 2 shows the
breadth-ﬁrst traversal of a dependency tree.

Under the assumption that each word w in a de-
pendency tree is only conditioned on its dependency
path, the probability of a sentence S given its depen-
dency tree T is:

P(S|T ) = ∏

P(w|D(w))

(1)

w∈BFS(T )\ROOT

where D(w) is the dependency path of w. Note that
each word w is visited according to its breadth-ﬁrst
search order (BFS(T)) and the probability of ROOT
is ignored since every tree has one. The role of
ROOT in a dependency tree is the same as the begin
of sentence token (BOS) in a sentence. When com-
puting P(S|T ) (or P(S)), the probability of ROOT (or
BOS) is ignored (we assume it always exists), but is
used to predict other words. We explain in the next
section how TREELSTM estimates P(w|D(w)).

The core problem in syntax-based language model-
ing is to estimate the probability of sentence S given

2.3 Tree LSTMs

1Throughout this paper we assume all dependency trees are

projective.

A dependency path D(w) is subtree which we de-
note as a sequence of (cid:104)word, edge-type(cid:105) tuples. Our

w0

w0

w3

w2

w1

w4

w5

w6

Generated by four LSTMs

with tied We and tied Who

w2

w1

w0

G

E

N

-

L

G E N-R

w0

w4

w5

GEN-NX-L

GEN-NX-L

GEN-NX-R

GEN-NX-R

w2

w1

w3
w6
Figure 3: Generation process of left (w1, w2, w3) and right
(w4, w5, w6) dependents of tree node wo (top) using four LSTMs
(GEN-L, GEN-R, GEN-NX-L and GEN-NX-R). The model can

w4

w5

handle an arbitrary number of dependents due to GEN-NX-L

and GEN-NX-R.

innovation is to learn the representation of D(w) us-
ing four LSTMs. The four LSTMs (GEN-L, GEN-
R, GEN-NX-L and GEN-NX-R) are used to repre-
sent the four types of edges (LEFT, RIGHT, NX-
LEFT and NX-RIGHT) introduced earlier. GEN,
NX, L and R are shorthands for GENERATE, NEXT,
LEFT and RIGHT. At each time step, an LSTM is
chosen according to an edge-type; then the LSTM
takes a word as input and predicts/generates its de-
pendent or sibling. This process can be also viewed
as adding an edge and a node to a tree. Speciﬁ-
cally, LSTMs GEN-L and GEN-R are used to gen-
erate the ﬁrst left and right dependent of a node
(w1 and w4 in Figure 3). So, these two LSTMs
are responsible for going deeper in a tree. While
GEN-NX-L and GEN-NX-R generate the remain-
ing left/right dependents and therefore go wider in
a tree. As shown in Figure 3, w2 and w3 are gener-
ated by GEN-NX-L, whereas w5 and w6 are gener-
ated by GEN-NX-R. Note that the model can handle
any number of left or right dependents by applying
GEN-NX-L or GEN-NX-R multiple times.

We assume time steps correspond to the steps
taken by the breadth-ﬁrst traversal of the depen-
dency tree and the sentence has length n. At
time step t (1 ≤ t ≤ n), let (cid:104)wt(cid:48), zt(cid:105) denote the last
Subscripts t and t(cid:48) denote the
tuple in D(wt).
breadth-ﬁrst search order of wt and wt(cid:48), respectively.
zt ∈ {LEFT, RIGHT, NX-LEFT, NX-RIGHT} is the
edge type (see the deﬁnitions in Section 2.1). Let
We ∈ Rs×|V | denote the word embedding matrix and

Who ∈ R|V |×d the output matrix of our model, where
|V | is the vocabulary size, s the word embedding size
and d the hidden unit size. We use tied We and tied
Who for the four LSTMs to reduce the number of pa-
rameters in our model. The four LSTMs also share
their hidden states. Let H ∈ Rd×(n+1) denote the
shared hidden states of all time steps and e(wt) the
one-hot vector of wt. Then, H[:,t] represents D(wt)
at time step t, and the computation2 is:

xt = We · e(wt(cid:48))
ht = LSTMzt (xt, H[:,t(cid:48)])

H[:,t] = ht

yt = Who · ht

(2a)

(2b)

(2c)

(2d)

where the initial hidden state H[:, 0] is initialized to
a vector of small values such as 0.01. According to
Equation (2b), the model selects an LSTM based on
edge type zt. We describe the details of LSTMzt in
the next paragraph. The probability of wt given its
dependency path D(wt) is estimated by a softmax
function:

P(wt|D(wt)) =

(3)

exp(yt,wt )
|V |
k(cid:48)=1 exp(yt,k(cid:48))

∑

We must point out that although we use four jointly
trained LSTMs to encode the hidden states, the train-
ing and inference complexity of our model is no dif-
ferent from a regular LSTM, since at each time step
only one LSTM is working.

We implement LSTMz in Equation (2b) using a
deep LSTM (to simplify notation, from now on we
write z instead of zt). The inputs at time step t
are xt and ht(cid:48) (the hidden state of an earlier time
step t(cid:48)) and the output is ht (the hidden state of cur-
rent time step). Let L denote the layer number of
LSTMz and ˆhl
t the internal hidden state of the l-th
layer of the LSTMz at time step t, where xt is ˆh0
t and
ht(cid:48) is ˆhL
t(cid:48). The LSTM architecture introduces mul-
tiplicative gates and memory cells ˆcl
t (at l-th layer)
in order to address the vanishing gradient problem
which makes it difﬁcult for the standard RNN model
to learn long-distance correlations in a sequence.
Here, ˆcl
t is a linear combination of the current input
signal ut and an earlier memory cell ˆcl
t(cid:48). How much
input information ut will ﬂow into ˆcl
t is controlled

2We ignore all bias terms for notational simplicity.

w2

w1

w0

G

E

N

-

L

G E N-R

w0

w4

w5

GEN-NX-L

GEN-NX-L

GEN-NX-R

GEN-NX-R

LD

LD

w0

w4

w5

w6

w3

w2

w1

Figure 4: Generation of left and right dependents of node w0
according to LDTREELSTM.

by input gate it and how much of the earlier mem-
ory cell ˆcl
t(cid:48) will be forgotten is controlled by forget
gate ft. This process is computed as follows:

ux · ˆhl−1

ut = tanh(Wz,l
it = σ(Wz,l
ft = σ(Wz,l
t = ft (cid:12) ˆcl
ˆcl

ix · ˆhl−1
f x · ˆhl−1
t(cid:48) + it (cid:12) ut

t + Wz,l
uh · ˆhl
t(cid:48))
ih · ˆhl
t(cid:48))
f h · ˆhl
t(cid:48))

t + Wz,l
t + Wz,l

(4a)

(4b)

(4c)

(4d)

ux ∈ Rd×d (Wz,l

uh ∈ Rd×d are weight matrices for ut, Wz,l
ih are weight matrices for it and Wz,l

where Wz,l
ux ∈ Rd×s when l = 1) and
Wz,l
ix and
f x, and Wz,l
Wz,l
f h
are weight matrices for ft. σ is a sigmoid function
and (cid:12) the element-wise product.

Output gate ot controls how much information of

the cell ˆcl

t can be seen by other modules:

t + Wz,l
ox · ˆhl−1
ot = σ(Wz,l
ˆhl
t = ot (cid:12) tanh(ˆcl
t )

oh · ˆhl
t(cid:48))

(5a)

(5b)

Application of the above process to all layers L, will
yield ˆhL
t , which is ht. Note that in implementation,
all ˆcl
t (1 ≤ l ≤ L) at time step t are stored,
although we only care about ˆhL
t (ht).

t and ˆhl

2.4 Left Dependent Tree LSTMs

TREELSTM computes P(w|D(w)) based on the de-
pendency path D(w), which ignores the interaction
between left and right dependents on the same level.
In many cases, TREELSTM will use a verb to pre-
dict its object directly without knowing its subject.
For example, in Figure 2, TREELSTM uses (cid:104)ROOT,
RIGHT(cid:105) and (cid:104) sold, RIGHT (cid:105) to predict cars. This in-
formation is unfortunately not speciﬁc to cars (many
things can be sold, e.g., chocolates, candy). Consid-
ering manufacturer, the left dependent of sold would
help predict cars more accurately.

In order to jointly take left and right dependents
into account, we employ yet another LSTM, which
goes from the furthest left dependent to the closest
left dependent (LD is a shorthand for left depen-
dent). As shown in Figure 4, LD LSTM learns the
representation of all left dependents of a node w0;
this representation is then used to predict the ﬁrst
right dependent of the same node. Non-ﬁrst right de-
pendents can also leverage the representation of left
dependents, since this information is injected into
the hidden state of the ﬁrst right dependent and can
percolate all the way. Note that in order to retain the
generation capability of our model (Section 3.4), we
only allow right dependents to leverage left depen-
dents (they are generated before right dependents).

The computation of the LDTREELSTM is al-
the same as in TREELSTM except when
most
zt = GEN-R.
let vt be the cor-
responding left dependent sequence with length
K (vt = (w3, w2, w1) in Figure 4). Then, the hidden
state (qk) of vt at each time step k is:

In this case,

mk = We · e(vt,k)
qk = LSTMLD(mk, qk−1)

(6a)

(6b)

where qK is the representation for all left depen-
dents. Then, the computation of the current hid-
den state becomes (see Equation (2) for the original
computation):

rt =

(cid:21)
(cid:20)We · e(wt(cid:48))
qK

ht = LSTMGEN-R(rt, H[:,t(cid:48)])

(7a)

(7b)

where qK serves as additional input for LSTMGEN-R.
All other computational details are the same as in
TreeLSTM (see Section 2.3).

2.5 Model Training

On small scale datasets we employ Negative Log-
likelihood (NLL) as our training objective for both
TREELSTM and LDTREELSTM:

L NLL(θ) = −

log P(S|T )

(8)

1
|S | ∑

S∈S

where S is a sentence in the training set S , T is the
dependency tree of S and P(S|T ) is deﬁned as in
Equation (1).

On large scale datasets (e.g., with vocabulary
size of 65K), computing the output layer activa-
tions and the softmax function with NLL would
become prohibitively expensive.
Instead, we em-
ploy Noise Contrastive Estimation (NCE; Gutmann
and Hyv¨arinen (2012), Mnih and Teh (2012)) which
treats the normalization term ˆZ in ˆP(w|D(wt)) =
exp(Who[w,:]·ht )
as constant. The intuition behind NCE
ˆZ
is to discriminate between samples from a data dis-
tribution ˆP(w|D(wt)) and a known noise distribu-
tion Pn(w) via binary logistic regression. Assuming
that noise words are k times more frequent than real
words in the training set (Mnih and Teh, 2012), then
the probability of a word w being from our model
Pd(w,D(wt)) is
. We apply NCE to
large vocabulary models with the following training
objective:

ˆP(w|D(wt ))
ˆP(w|D(wt ))+kPn(w)

L NCE(θ) = −

log Pd(wt,D(wt))

(cid:18)

1
|S | ∑

T ∈S

|T |
∑
t=1

+

log[1 − Pd( ˜wt, j,D(wt))]

(cid:19)

k
∑
j=1

where ˜wt, j is a word sampled from the noise distri-
bution Pn(w). We use smoothed unigram frequen-
cies (exponentiating by 0.75) as the noise distribu-
tion Pn(w) (Mikolov et al., 2013b). We initialize
ln ˆZ = 9 as suggested in Chen et al. (2015), but in-
stead of keeping it ﬁxed we also learn ˆZ during train-
ing (Vaswani et al., 2013). We set k = 20.

3 Experiments

We assess the performance of our model on two
tasks: the Microsoft Research (MSR) sentence com-
pletion challenge (Zweig and Burges, 2012), and de-
pendency parsing reranking. We also demonstrate
the tree generation capability of our models. In the
following, we ﬁrst present details on model train-
ing and then present our results. We implemented
our models using the Torch library (Collobert et
al., 2011) and our code is available at https://
github.com/XingxingZhang/td-treelstm.

3.1 Training Details

We trained our model with back propagation
through time (Rumelhart et al., 1988) on an Nvidia

GPU Card with a mini-batch size of 64. The ob-
jective (NLL or NCE) was minimized by stochastic
gradient descent. Model parameters were uniformly
initialized in [−0.1, 0.1]. We used the NCE objec-
tive on the MSR sentence completion task (due to
the large size of this dataset) and the NLL objec-
tive on dependency parsing reranking. We used an
initial learning rate of 1.0 for all experiments and
when there was no signiﬁcant improvement in log-
likelihood on the validation set, the learning rate was
divided by 2 per epoch until convergence (Mikolov
et al., 2010). To alleviate the exploding gradients
problem, we rescaled the gradient g when the gradi-
ent norm ||g|| > 5 and set g = 5g
||g|| (Pascanu et al.,
2013; Sutskever et al., 2014). Dropout (Srivastava
et al., 2014) was applied to the 2-layer TREELSTM
and LDTREELSTM models. The word embedding
size was set to s = d/2 where d is the hidden unit
size.

3.2 Microsoft Sentence Completion Challenge

The task in the MSR Sentence Completion Chal-
lenge (Zweig and Burges, 2012) is to select the
correct missing word for 1,040 SAT-style test sen-
tences when presented with ﬁve candidate comple-
tions. The training set contains 522 novels from
the Project Gutenberg which we preprocessed as fol-
lows. After removing headers and footers from the
ﬁles, we tokenized and parsed the dataset into de-
pendency trees with the Stanford Core NLP toolkit
(Manning et al., 2014). The resulting training set
contained 49M words. We converted all words to
lower case and replaced those occurring ﬁve times
or less with UNK. The resulting vocabulary size
was 65,346 words. We randomly sampled 4,000
sentences from the training set as our validation set.
The literature describes two main approaches to
the sentence completion task based on word vectors
and language models. In vector-based approaches,
all words in the sentence and the ﬁve candidate
words are represented by a vector; the candidate
which has the highest average similarity with the
sentence words is selected as the answer. For lan-
guage model-based methods, the LM computes the
probability of a test sentence with each of the ﬁve
candidate words, and picks the candidate comple-
tion which gives the highest probability. Our model
belongs to this class of models.

|θ|

Accuracy

d

—
640
600

Model
Word Vector based Models
LSA
Skip-gram
IVLBL
Language Models
KN5
UDepNgram
LDepNgram
RNN
RNNME
depRNN+3gram
ldepRNN+4gram
LBL
LSTM
LSTM
LSTM
Bidirectional LSTM
Bidirectional LSTM
Bidirectional LSTM
Model Combinations
RNNMEs
—
Skip-gram + RNNMEs —
Our Models
TREELSTM
LDTREELSTM
TREELSTM
LDTREELSTM

—
—
—
300
300
100
200
300
300
400
450
200
300
400

300
300
400
400

—
102M
96.0M

—
—
—
48.1M
1120M
1014M
1029M
48.0M
29.9M
40.2M
45.3M
33.2M
50.1M
67.3M

—
—

31.6M
32.5M
43.1M
44.7M

49.0
48.0
55.5

40.0
48.3
50.0
45.0
49.3
53.5
50.7
54.7
55.00
57.02
55.96
48.46
49.90
48.65

55.4
58.9

55.29
57.79
56.73
60.67

Table 1: Model accuracy on the MSR sentence completion task.

The results of KN5, RNNME and RNNMEs are reported in

Mikolov (2012), LSA and RNN in Zweig et al. (2012), UDep-

Ngram and LDepNgram in Gubbins and Vlachos (2013), de-

pRNN+3gram and depRNN+4gram in Mirowski and Vlachos

(2015), LBL in Mnih and Teh (2012), Skip-gram and Skip-

gram+RNNMEs in Mikolov et al. (2013a), and IVLBL in Mnih
and Kavukcuoglu (2013); d is the hidden size and |θ| the num-
ber of parameters in a model.

Table 1 presents a summary of our results to-
gether with previoulsy published results. The best
performing word vector model is IVLBL (Mnih and
Kavukcuoglu, 2013) with an accuracy of 55.5, while
the best performing single language model is LBL
(Mnih and Teh, 2012) with an accuracy of 54.7.
Both approaches are based on the log-bilinear lan-
guage model (Mnih and Hinton, 2007). A combi-
nation of several recurrent neural networks and the
skip-gram model holds the state of the art with an
accuracy of 58.9 (Mikolov et al., 2013b). To fairly
compare with existing models, we restrict the layer

Test

Parser

Development
UAS
MSTParser-2nd
92.20
TREELSTM
92.51
92.64
TREELSTM*
LDTREELSTM 92.66
92.00
NN parser*
93.20
S-LSTM*

LAS
88.44
88.53
88.69
88.69
89.60
90.90
Table 2: Performance of TREELSTM and LDTREELSTM on

LAS UAS
91.63
88.78
91.79
89.07
91.97
89.09
91.99
89.14
91.80
89.70
93.10
90.90

reranking the top dependency trees produced by the 2nd order

MSTParser (McDonald and Pereira, 2006). Results for the NN

and S-LSTM parsers are reported in Chen and Manning (2014)

and Dyer et al. (2015), respectively. * indicates that the model

is initialized with pre-trained word vectors.

size of our models to 1. We observe that LDTREEL-
STM consistently outperforms TREELSTM, which
indicates the importance of modeling the interac-
In fact,
tion between left and right dependents.
LDTREELSTM (d = 400) achieves a new state-of-
the-art on this task, despite being a single model.
We also implement LSTM and bidirectional LSTM
language models.3 An LSTM with d = 400 out-
performs its smaller counterpart (d = 300), however
performance decreases with d = 450. The bidirec-
tional LSTM is worse than the LSTM (see Mnih
and Teh (2012) for a similar observation). The
best performing LSTM is worse than a LDTREEL-
STM (d = 300). The input and output embeddings
(We and Who) dominate the number of parame-
ters in all neural models except for RNNME, de-
pRNN+3gram and ldepRNN+4gram, which include
a ME model that contains 1 billion sparse n-gram
features (Mikolov, 2012; Mirowski and Vlachos,
2015). The number of parameters in TREELSTM
and LDTREELSTM is not much larger compared to
LSTM due to the tied We and Who matrices.

3.3 Dependency Parsing

In this section we demonstrate that our model can
be also used for parse reranking. This is not possi-
ble for sequence-based language models since they
cannot estimate the probability of a tree. We use
our models to rerank the top K dependency trees
produced by the second order MSTParser (McDon-

3LSTMs and BiLSTMs were also trained with NCE
(s = d/2; hyperparameters were tuned on the development set).

ald and Pereira, 2006).4 We follow closely the ex-
perimental setup of Chen and Manning (2014) and
Dyer et al. (2015). Speciﬁcally, we trained TREEL-
STM and LDTREELSTM on Penn Treebank sec-
tions 2–21. We used section 22 for development and
section 23 for testing. We adopted the Stanford ba-
sic dependency representations (De Marneffe et al.,
2006); part-of-speech tags were predicted with the
Stanford Tagger (Toutanova et al., 2003). We trained
TREELSTM and LDTREELSTM as language mod-
els (singletons were replaced with UNK) and did
not use any POS tags, dependency labels or com-
position features, whereas these features are used in
Chen and Manning (2014) and Dyer et al. (2015).
We tuned d, the number of layers, and K on the de-
velopment set.

Table 2 reports unlabeled attachment scores
(UAS) and labeled attachment scores (LAS) for
the MSTParser, TREELSTM (d = 300, 1 layer,
K = 2), and LDTREELSTM (d = 200, 2 layers,
K = 4). We also include the performance of two
neural network-based dependency parsers; Chen and
Manning (2014) use a neural network classiﬁer to
predict the correct transition (NN parser); Dyer et
al. (2015) also implement a transition-based depen-
dency parser using LSTMs to represent the contents
of the stack and buffer in a continuous space. As can
be seen, both TREELSTM and LDTREELSTM out-
perform the baseline MSTParser, with LDTREEL-
STM performing best. We also initialized the word
embedding matrix We with pre-trained GLOVE vec-
tors (Pennington et al., 2014). We obtained a slight
improvement over TREELSTM (TREELSTM* in
Table 2; d = 200, 2 layer, K = 4) but no im-
provement over LDTREELSTM. Finally, notice that
LDTREELSTM is slightly better than the NN parser
in terms of UAS but worse than the S-LSTM parser.
In the future, we would like to extend our model so
that it takes labeled dependency information into ac-
count.

3.4 Tree Generation

This section demonstrates how to use a trained
LDTREELSTM to generate tree samples. The gen-
eration starts at the ROOT node. At each time step t,
for each node wt, we add a new edge and node to

Figure 5: Generated dependency trees with LDTREELSTM

trained on the PTB.

the tree. Unfortunately during generation, we do not
know which type of edge to add. We therefore use
four binary classiﬁers (ADD-LEFT, ADD-RIGHT,
ADD-NX-LEFT and ADD-NX-RIGHT) to predict
whether we should add a LEFT, RIGHT, NX-LEFT
or NX-RIGHT edge.5 Then when a classiﬁer pre-
dicts true, we use the corresponding LSTM to gener-
ate a new node by sampling from the predicted word
distribution in Equation (3). The four classiﬁers take
the previous hidden state H[:,t(cid:48)] and the output em-
bedding of the current node Who · e(wt) as features.6
Speciﬁcally, we use a trained LDTREELSTM to
go through the training corpus and generate hidden
states and embeddings as input features; the corre-
sponding class labels (true and false) are “read off”
the training dependency trees. We use two-layer rec-
tiﬁer networks (Glorot et al., 2011) as the four clas-
siﬁers with a hidden size of 300. We use the same
LDTREELSTM model as in Section 3.3 to gener-
ate dependency trees. The classiﬁers were trained
using AdaGrad (Duchi et al., 2011) with a learning
rate of 0.01. The accuracies of ADD-LEFT, ADD-
RIGHT, ADD-NX-LEFT and ADD-NX-RIGHT are

5It is possible to get rid of the four classiﬁers by adding
START/STOP symbols when generating left and right depen-
dents as in (Eisner, 1996). We refrained from doing this for
computational reasons. For a sentence with N words, this ap-
proach will lead to 2N additional START/STOP symbols (with
one START and one STOP symbol for each word). Conse-
quently, the computational cost and memory consumption dur-
ing training will be three times as much rendering our model
less scalable.

6The input embeddings have lower dimensions and therefore

4http://www.seas.upenn.edu/ strctlrn/MSTParser

result in slightly worse classiﬁers.

94.3%, 92.6%, 93.4% and 96.0%, respectively. Fig-
ure 5 shows examples of generated trees.

4 Conclusions

In this paper we developed TREELSTM (and
LDTREELSTM), a neural network model architec-
ture, which is designed to predict tree structures
rather than linear sequences. Experimental results
on the MSR sentence completion task show that
LDTREELSTM is superior to sequential LSTMs.
Dependency parsing reranking experiments high-
light our model’s potential for dependency pars-
ing. Finally, the ability of our model to gener-
ate dependency trees holds promise for text gen-
eration applications such as sentence compression
and simpliﬁcation (Filippova et al., 2015). Although
our experiments have focused exclusively on depen-
dency trees, there is nothing inherent in our formu-
lation that disallows its application to other types of
tree structure such as constituent trees or even tax-
onomies.

Acknowledgments

We would like to thank Adam Lopez, Frank Keller,
Iain Murray, Li Dong, Brian Roark, and the NAACL
reviewers for their valuable feedback. Xingxing
Zhang gratefully acknowledges the ﬁnancial sup-
port of the China Scholarship Council (CSC). Liang
Lu is funded by the UK EPSRC Programme Grant
EP/I031022/1, Natural Speech Technology (NST).

References

[Bengio et al.2003] Yoshua Bengio, R´ejean Ducharme,
Pascal Vincent, and Christian Janvin. 2003. A neural
probabilistic language model. The Journal of Machine
Learning Research, 3:1137–1155.

[Charniak2001] Eugene Charniak.

Immediate-
head parsing for language models. In Proceedings of
the 39th Annual Meeting on Association for Compu-
tational Linguistics, pages 124–131. Association for
Computational Linguistics.

2001.

[Chelba and Jelinek2000] Ciprian Chelba and Frederick
Jelinek. 2000. Structured language modeling. Com-
puter Speech and Language, 14(4):283–332.

[Chelba et al.1997] Ciprian Chelba, David Engle, Freder-
ick Jelinek, Victor Jimenez, Sanjeev Khudanpur, Lidia
Mangu, Harry Printz, Eric Ristad, Ronald Rosenfeld,

Andreas Stolcke, et al.
formance of a dependency language model.
ROSPEECH. Citeseer.

1997. Structure and per-
In EU-

[Chen and Manning2014] Danqi Chen and Christopher
2014. A fast and accurate dependency
Manning.
In Proceedings of
parser using neural networks.
the 2014 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 740–750,
Doha, Qatar, October. Association for Computational
Linguistics.

[Chen et al.2015] X Chen, X Liu, MJF Gales, and
PC Woodland. 2015. Recurrent neural network lan-
guage model training with noise contrastive estimation
for speech recognition. In In 40th IEEE International
Conference on Accoustics, Speech and Signal Process-
ing, pages 5401–5405, Brisbane, Australia.

[Collobert et al.2011] Ronan

Collobert,

Koray
Kavukcuoglu, and Cl´ement Farabet. 2011. Torch7:
A matlab-like environment for machine learning.
In
BigLearn, NIPS Workshop, number EPFL-CONF-
192376.

[De Marneffe et al.2006] Marie-Catherine De Marneffe,
Bill MacCartney, Christopher D Manning, et al. 2006.
Generating typed dependency parses from phrase
structure parses. In Proceedings of LREC, volume 6,
pages 449–454.

[Duchi et al.2011] John Duchi, Elad Hazan, and Yoram
Singer. 2011. Adaptive subgradient methods for on-
line learning and stochastic optimization. The Journal
of Machine Learning Research, 12:2121–2159.

[Dyer et al.2015] Chris Dyer, Miguel Ballesteros, Wang
Ling, Austin Matthews, and Noah A. Smith. 2015.
Transition-based dependency parsing with stack long
In Proceedings of the 53rd An-
short-term memory.
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long Pa-
pers), pages 334–343, Beijing, China, July. Associa-
tion for Computational Linguistics.

[Eisner1996] Jason M Eisner. 1996. Three new prob-
abilistic models for dependency parsing: An explo-
ration. In Proceedings of the 16th conference on Com-
putational linguistics-Volume 1, pages 340–345. Asso-
ciation for Computational Linguistics.

[Emami et al.2003] Ahmad Emami, Peng Xu, and Fred-
erick Jelinek. 2003. Using a connectionist model in
In Proceedings
a syntactical based language model.
of the IEEE International Conference on Acoustics,
Speech, and Signal Processing, pages 372–375, Hong
Kong, China.

[Filippova et al.2015] Katja Filippova, Enrique Alfon-
seca, Carlos A Colmenares, Lukasz Kaiser, and Oriol
Vinyals. 2015. Sentence compression by deletion
with lstms. In EMNLP, pages 360–368.

[Glorot et al.2011] Xavier Glorot, Antoine Bordes, and
Yoshua Bengio. 2011. Deep sparse rectiﬁer neural
networks. In International Conference on Artiﬁcial In-
telligence and Statistics, pages 315–323.

[Graves et al.2013] Alan Graves, Abdel-rahman Mo-
hamed, and Geoffrey Hinton. 2013. Speech recogni-
tion with deep recurrent neural networks. In Acoustics,
Speech and Signal Processing (ICASSP), 2013 IEEE
International Conference on, pages 6645–6649. IEEE.
[Gubbins and Vlachos2013] Joseph Gubbins and Andreas
Vlachos. 2013. Dependency language models for sen-
tence completion. In EMNLP, pages 1405–1410, Seat-
tle, Washington, USA, October. Association for Com-
putational Linguistics.

[Gutmann and Hyv¨arinen2012] Michael U Gutmann and
Aapo Hyv¨arinen. 2012. Noise-contrastive estimation
of unnormalized statistical models, with applications
to natural image statistics. The Journal of Machine
Learning Research, 13(1):307–361.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural computation, 9(8):1735–1780.

[Hochreiter1998] Sepp Hochreiter. 1998. Vanishing gra-
dient problem during learning recurrent neural nets
and problem solutions. International Journal of Un-
certainty, Fuzziness and Knowledge-based Systems,
6(2):107–116.

[Manning et al.2014] Christopher D Manning, Mihai Sur-
deanu, John Bauer, Jenny Finkel, Steven J Bethard,
and David McClosky. 2014. The stanford corenlp
In Proceedings
natural language processing toolkit.
of 52nd Annual Meeting of the Association for Com-
putational Linguistics: System Demonstrations, pages
55–60.

[McDonald and Pereira2006] Ryan T McDonald and Fer-
nando CN Pereira. 2006. Online learning of approxi-
mate dependency parsing algorithms. In EACL.

[Mikolov et al.2010] Tomas Mikolov, Martin Karaﬁ´at,
Lukas Burget, Jan Cernock`y, and Sanjeev Khudan-
pur. 2010. Recurrent neural network based language
model. In INTERSPEECH 2010, 11th Annual Confer-
ence of the International Speech Communication As-
sociation, Makuhari, Chiba, Japan, September 26-30,
2010, pages 1045–1048.

[Mikolov et al.2013a] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013a. Efﬁcient esti-
In
mation of word representations in vector space.
Proceedings of the 2013 International Conference on
Learning Representations, Scottsdale, Arizona, USA.
Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013b.
Distributed representations of words and phrases and
In Advances in Neural Infor-
their compositionality.
mation Processing Systems 26, pages 3111–3119.

[Mikolov et al.2013b] Tomas Mikolov,

[Mikolov2012] Tomas Mikolov. 2012. Statistical Lan-
guage Models based on Neural Networks. Ph.D. the-
sis, Brno University of Technology.

[Mirowski and Vlachos2015] Piotr Mirowski and An-
dreas Vlachos. 2015. Dependency recurrent neural
In ACL,
language models for sentence completion.
pages 511–517, Beijing, China, July. Association for
Computational Linguistics.

[Mnih and Hinton2007] Andriy Mnih and Geoffrey Hin-
ton. 2007. Three new graphical models for statistical
In Proceedings of the 24th In-
language modelling.
ternational Conference on Machine Learning, pages
641–648.

[Mnih and Kavukcuoglu2013] Andriy Mnih and Koray
Kavukcuoglu. 2013. Learning word embeddings efﬁ-
ciently with noise-contrastive estimation. In Advances
in Neural Information Processing Systems 26, pages
2265–2273.

[Mnih and Teh2012] Andriy Mnih and Yee Whye Teh.
2012. A fast and simple algorithm for training neural
probabilistic language models. In Proceedings of the
29th International Conference on Machine Learning,
pages 1751–1758, Edinburgh, Scotland.

[Pascanu et al.2013] Razvan Pascanu, Tomas Mikolov,
and Yoshua Bengio. 2013. On the difﬁculty of train-
ing recurrent neural networks. In Proceedings of the
31st International Conference on Machine Learning,
pages 1310–1318, Atlanta, Georgia, USA.
Pennington,

Richard
Socher, and Christopher D Manning. 2014. Glove:
EMNLP,
Global vectors for word representation.
12:1532–1543.

[Pennington et al.2014] Jeffrey

[Pollack1990] Jordan B. Pollack. 1990. Recursive dis-
tributed representations. Artiﬁcial Intelligence, 1–
2(46):77–105.

[Roark2001] Brian Roark. 2001. Probabilistic top-down
parsing and language modeling. Computational lin-
guistics, 27(2):249–276.

[Rumelhart et al.1988] David E Rumelhart, Geoffrey E
Hinton, and Ronald J Williams. 1988. Learning repre-
sentations by back-propagating errors. Cognitive mod-
eling, 5:3.

[Sennrich2015] Rico Sennrich. 2015. Modelling and op-
timizing on syntactic n-grams for statistical machine
translation. Transactions of the Association for Com-
putational Linguistics, 3:169–182.

[Shen et al.2008] Libin Shen,

Jinxi Xu,

and Ralph
Weischedel. 2008. A new string-to-dependency ma-
chine translation algorithm with a target dependency
In Proceedings of ACL-08: HLT,
language model.
pages 577–585, Columbus, Ohio, USA.

[Socher et al.2011a] Richard Socher, Eric H. Huang, Jef-
frey Pennington, Christopher D. Manning, and An-
drew Ng. 2011a. Dynamic pooling and unfolding

Model? On the Future of Language Modeling for HLT,
pages 29–36, Montr´eal, Canada.

[Zweig et al.2012] Geoffrey Zweig, John C Platt, Christo-
pher Meek, Christopher JC Burges, Ainur Yessenalina,
2012. Computational approaches
and Qiang Liu.
In Proceedings of the 50th
to sentence completion.
Annual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 601–610.
Association for Computational Linguistics.

In
recursive autoencoders for paraphrase detection.
Advances in Neural Information Processing Systems,
pages 801–809.

[Socher et al.2011b] Richard Socher, Jeffrey Pennington,
Eric H. Huang, Andrew Y. Ng, and Christopher D.
Manning. 2011b. Semi-supervised recursive autoen-
coders for predicting sentiment distributions. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 151–161,
Edinburgh, Scotland, UK.

[Srivastava et al.2014] Nitish Srivastava, Geoffrey Hin-
ton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: A simple way to pre-
vent neural networks from overﬁtting. The Journal of
Machine Learning Research, 15(1):1929–1958.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and
Quoc VV Le. 2014. Sequence to sequence learning
with neural networks. In Advances in Neural Informa-
tion Processing Systems, pages 3104–3112.

[Tai et al.2015] Kai Sheng Tai, Richard Socher, and
Christopher D. Manning. 2015.
Improved semantic
representations from tree-structured long short-term
memory networks. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguis-
tics and the 7th International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers),
pages 1556–1566, Beijing, China, July. Association
for Computational Linguistics.

[Toutanova et al.2003] Kristina Toutanova, Dan Klein,
Christopher D Manning, and Yoram Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic de-
pendency network. In Proceedings of the 2003 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology-Volume 1, pages 173–180. Associa-
tion for Computational Linguistics.

[Vaswani et al.2013] Ashish Vaswani, Yinggong Zhao,
Victoria Fossum, and David Chiang. 2013. Decod-
ing with large-scale neural language models improves
translation. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
pages 1387–1392, Seattle, Washington, USA.

[Vinyals et al.2015] Oriol Vinyals, Alexander Toshev,
Samy Bengio, and Dumitru Erhan. 2015. Show and
tell: A neural image caption generator. In The IEEE
Conference on Computer Vision and Pattern Recogni-
tion, Boston, Massachusetts, USA.

[Zhang2009] Ying Zhang. 2009. Structured language
models for statistical machine translation. Ph.D. the-
sis, Johns Hopkins University.

[Zweig and Burges2012] Geoffrey Zweig and Chris J.C.
Burges. 2012. A challenge set for advancing language
In Proceedings of the NAACL-HLT 2012
modeling.
Workshop: Will We Ever Really Replace the N-gram

Top-down Tree Long Short-Term Memory Networks

Xingxing Zhang, Liang Lu and Mirella Lapata
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
{x.zhang,liang.lu}@ed.ac.uk,mlap@inf.ed.ac.uk

6
1
0
2
 
r
p
A
 
3
 
 
]
L
C
.
s
c
[
 
 
3
v
0
6
0
0
0
.
1
1
5
1
:
v
i
X
r
a

Abstract

Long Short-Term Memory (LSTM) networks,
a type of recurrent neural network with a
more complex computational unit, have been
successfully applied to a variety of sequence
modeling tasks. In this paper we develop Tree
Long Short-Term Memory (TREELSTM), a
neural network model based on LSTM, which
is designed to predict a tree rather than a lin-
ear sequence. TREELSTM deﬁnes the prob-
ability of a sentence by estimating the gener-
ation probability of its dependency tree. At
each time step, a node is generated based
on the representation of the generated sub-
tree. We further enhance the modeling power
of TREELSTM by explicitly representing the
correlations between left and right depen-
dents. Application of our model to the MSR
sentence completion challenge achieves re-
sults beyond the current state of the art. We
also report results on dependency parsing
reranking achieving competitive performance.

1

Introduction

Neural language models have been gaining increas-
ing attention as a competitive alternative to n-grams.
The main idea is to represent each word using a
real-valued feature vector capturing the contexts in
which it occurs. The conditional probability of the
next word is then modeled as a smooth function of
the feature vectors of the preceding words and the
In essence, similar representations are
next word.
learned for words found in similar contexts result-
ing in similar predictions for the next word. Previ-
ous approaches have mainly employed feed-forward

(Bengio et al., 2003; Mnih and Hinton, 2007) and
recurrent neural networks (Mikolov et al., 2010;
Mikolov, 2012) in order to map the feature vec-
tors of the context words to the distribution for the
next word. Recently, RNNs with Long Short-Term
Memory (LSTM) units (Hochreiter and Schmidhu-
ber, 1997; Hochreiter, 1998) have emerged as a pop-
ular architecture due to their strong ability to capture
long-term dependencies. LSTMs have been success-
fully applied to a variety of tasks ranging from ma-
chine translation (Sutskever et al., 2014), to speech
recognition (Graves et al., 2013), and image descrip-
tion generation (Vinyals et al., 2015).

Despite superior performance in many applica-
tions, neural language models essentially predict se-
quences of words. Many NLP tasks, however, ex-
ploit syntactic information operating over tree struc-
tures (e.g., dependency or constituent trees). In this
paper we develop a novel neural network model
which combines the advantages of the LSTM archi-
tecture and syntactic structure. Our model estimates
the probability of a sentence by estimating the gen-
eration probability of its dependency tree. Instead
of explicitly encoding tree structure as a set of fea-
tures, we use four LSTM networks to model four
types of dependency edges which altogether specify
how the tree is built. At each time step, one LSTM is
activated which predicts the next word conditioned
on the sub-tree generated so far. To learn the repre-
sentations of the conditioned sub-tree, we force the
four LSTMs to share their hidden layers. Our model
is also capable of generating trees just by sampling
from a trained model and can be seamlessly inte-
grated with text generation applications.

Our approach is related to but ultimately differ-
ent from recursive neural networks (Pollack, 1990)
a class of models which operate on structured in-
puts. Given a (binary) parse tree, they recursively
generate parent representations in a bottom-up fash-
ion, by combining tokens to produce representations
for phrases, and eventually the whole sentence. The
learned representations can be then used in classi-
ﬁcation tasks such as sentiment analysis (Socher et
al., 2011b) and paraphrase detection (Socher et al.,
2011a). Tai et al. (2015) learn distributed representa-
tions over syntactic trees by generalizing the LSTM
architecture to tree-structured network topologies.
The key feature of our model is not so much that
it can learn semantic representations of phrases or
sentences, but its ability to predict tree structure and
estimate its probability.

Syntactic language models have a long history
in NLP dating back to Chelba and Jelinek (2000)
(see also Roark (2001) and Charniak (2001)). These
models differ in how grammar structures in a parsing
tree are used when predicting the next word. Other
work develops dependency-based language models
for speciﬁc applications such as machine translation
(Shen et al., 2008; Zhang, 2009; Sennrich, 2015),
speech recognition (Chelba et al., 1997) or sentence
completion (Gubbins and Vlachos, 2013). All in-
stances of these models apply Markov assumptions
on the dependency tree, and adopt standard n-gram
smoothing methods for reliable parameter estima-
tion. Emami et al. (2003) and Sennrich (2015) esti-
mate the parameters of a structured language model
using feed-forward neural networks (Bengio et al.,
2003). Mirowski and Vlachos (2015) re-implement
the model of Gubbins and Vlachos (2013) with
RNNs. They view sentences as sequences of words
over a tree. While they ignore the tree structures
themselves, we model them explicitly.

Our model shares with other structured-based lan-
guage models the ability to take dependency infor-
mation into account. It differs in the following re-
spects: (a) it does not artiﬁcially restrict the depth
of the dependencies it considers and can thus be
viewed as an inﬁnite order dependency language
model; (b) it not only estimates the probability of a
string but is also capable of generating dependency
trees; (c) ﬁnally, contrary to previous dependency-
based language models which encode syntactic in-

formation as features, our model takes tree structure
into account more directly via representing different
types of dependency edges explicitly using LSTMs.
Therefore, there is no need to manually determine
which dependency tree features should be used or
how large the feature embeddings should be.

We evaluate our model on the MSR sentence com-
pletion challenge, a benchmark language modeling
dataset. Our results outperform the best published
results on this dataset. Since our model is a general
tree estimator, we also use it to rerank the top K de-
pendency trees from the (second order) MSTPasrser
and obtain performance on par with recently pro-
posed dependency parsers.

2 Tree Long Short-Term Memory

Networks

We seek to estimate the probability of a sentence by
estimating the generation probability of its depen-
dency tree. Syntactic information in our model is
represented in the form of dependency paths. In the
following, we ﬁrst describe our deﬁnition of depen-
dency path and based on it explain how the proba-
bility of a sentence is estimated.

2.1 Dependency Path

Generally speaking, a dependency path is the path
between ROOT and w consisting of the nodes on
the path and the edges connecting them. To rep-
resent dependency paths, we introduce four types
of edges which essentially deﬁne the “shape” of a
dependency tree. Let w0 denote a node in a tree
and w1, w2, . . . , wn its left dependents. As shown in
Figure 1, LEFT edge is the edge between w0 and
its ﬁrst left dependent denoted as (w0, w1). Let wk
(with 1 < k ≤ n) denote a non-ﬁrst left dependent
of w0. The edge from wk−1 to wk is a NX-LEFT
edge (NX stands for NEXT), where wk−1 is the right
adjacent sibling of wk. Note that the NX-LEFT edge
(wk−1, wk) replaces edge (w0, wk) (illustrated with a
dashed line in Figure 1) in the original dependency
tree. The modiﬁcation allows information to ﬂow
from w0 to wk through w1, . . . , wk−1 rather than di-
rectly from w0 to wk. RIGHT and NX-RIGHT edges
are deﬁned analogously for right dependents.

Given these four types of edges, dependency
paths (denoted as D(w)) can be deﬁned as follows

w0

wn

wk

wk−1

w1

LEFT

NX-LEFT

Figure 1: LEFT and NX-LEFT edges. Dotted line between
w1 and wk−1 (also between wk and wn) indicate that there may
be ≥ 0 nodes inbetween.

bearing in mind that the ﬁrst right dependent of
ROOT is its only dependent and that wp denotes the
parent of w. We use (. . . ) to denote a sequence,
where () is an empty sequence and (cid:107) is an operator
for concatenating two sequences.

(1) if w is ROOT, then D(w) = ()
(2) if w is a left dependent of wp
(a) if w is the ﬁrst

left dependent,

then

D(w) = D(wp)(cid:107)((cid:104)wp, LEFT(cid:105))

(b) if w is not the ﬁrst left dependent and ws is

its right adjacent sibling, then
D(w) = D(ws)(cid:107)((cid:104)ws, NX-LEFT(cid:105))

(3) if w is a right dependent of wp

(a) if w is the ﬁrst right dependent,
D(w) = D(wp)(cid:107)((cid:104)wp, RIGHT(cid:105))

then

(b) if w is not the ﬁrst right dependent and ws

is its left adjacent sibling, then
D(w) = D(ws)(cid:107)((cid:104)ws, NX-RIGHT(cid:105))

A dependency tree can be represented by the set of
its dependency paths which in turn can be used to
reconstruct the original tree.1

for

the ﬁrst

the moment

Dependency paths

two levels
the tree in Figure 2 are as follows (ig-
of
the subscripts which
noring for
D(sold) =
we explain in the next section).
((cid:104)ROOT, RIGHT(cid:105)) (see deﬁnitions (1) and (3a)),
D(year) = D(sold)(cid:107)((cid:104)sold, LEFT(cid:105))
(2a)),
D(manufacturer) = D(year)(cid:107)((cid:104)year, NX-LEFT(cid:105))
(see (2b)), D(cars) = D(sold)(cid:107)((cid:104)sold, RIGHT(cid:105))
(see (3a)), D(in) = D(cars)(cid:107)((cid:104)cars, NX-RIGHT(cid:105))
(according to (3b)).

(see

2.2 Tree Probability

ROOT

sold1

manufacturer3

year2

cars4

in5

The9

luxury8 auto7

last6

1,21410

U.S.11

the12

Figure 2: Dependency tree of the sentence The luxury auto

manufacturer last year sold 1,214 cars in the U.S. Subscripts

indicate breadth-ﬁrst traversal. ROOT has only one dependent

(i.e., sold ) which we view as its ﬁrst right dependent.

its corresponding tree T , P(S|T ). We view the prob-
ability computation of a dependency tree as a gener-
ation process. Speciﬁcally, we assume dependency
trees are constructed top-down, in a breadth-ﬁrst
manner. Generation starts at the ROOT node. For
each node at each level, ﬁrst its left dependents are
generated from closest to farthest and then the right
dependents (again from closest to farthest). The
same process is applied to the next node at the same
level or a node at the next level. Figure 2 shows the
breadth-ﬁrst traversal of a dependency tree.

Under the assumption that each word w in a de-
pendency tree is only conditioned on its dependency
path, the probability of a sentence S given its depen-
dency tree T is:

P(S|T ) = ∏

P(w|D(w))

(1)

w∈BFS(T )\ROOT

where D(w) is the dependency path of w. Note that
each word w is visited according to its breadth-ﬁrst
search order (BFS(T)) and the probability of ROOT
is ignored since every tree has one. The role of
ROOT in a dependency tree is the same as the begin
of sentence token (BOS) in a sentence. When com-
puting P(S|T ) (or P(S)), the probability of ROOT (or
BOS) is ignored (we assume it always exists), but is
used to predict other words. We explain in the next
section how TREELSTM estimates P(w|D(w)).

The core problem in syntax-based language model-
ing is to estimate the probability of sentence S given

2.3 Tree LSTMs

1Throughout this paper we assume all dependency trees are

projective.

A dependency path D(w) is subtree which we de-
note as a sequence of (cid:104)word, edge-type(cid:105) tuples. Our

w0

w0

w3

w2

w1

w4

w5

w6

Generated by four LSTMs

with tied We and tied Who

w2

w1

w0

G

E

N

-

L

G E N-R

w0

w4

w5

GEN-NX-L

GEN-NX-L

GEN-NX-R

GEN-NX-R

w2

w1

w3
w6
Figure 3: Generation process of left (w1, w2, w3) and right
(w4, w5, w6) dependents of tree node wo (top) using four LSTMs
(GEN-L, GEN-R, GEN-NX-L and GEN-NX-R). The model can

w4

w5

handle an arbitrary number of dependents due to GEN-NX-L

and GEN-NX-R.

innovation is to learn the representation of D(w) us-
ing four LSTMs. The four LSTMs (GEN-L, GEN-
R, GEN-NX-L and GEN-NX-R) are used to repre-
sent the four types of edges (LEFT, RIGHT, NX-
LEFT and NX-RIGHT) introduced earlier. GEN,
NX, L and R are shorthands for GENERATE, NEXT,
LEFT and RIGHT. At each time step, an LSTM is
chosen according to an edge-type; then the LSTM
takes a word as input and predicts/generates its de-
pendent or sibling. This process can be also viewed
as adding an edge and a node to a tree. Speciﬁ-
cally, LSTMs GEN-L and GEN-R are used to gen-
erate the ﬁrst left and right dependent of a node
(w1 and w4 in Figure 3). So, these two LSTMs
are responsible for going deeper in a tree. While
GEN-NX-L and GEN-NX-R generate the remain-
ing left/right dependents and therefore go wider in
a tree. As shown in Figure 3, w2 and w3 are gener-
ated by GEN-NX-L, whereas w5 and w6 are gener-
ated by GEN-NX-R. Note that the model can handle
any number of left or right dependents by applying
GEN-NX-L or GEN-NX-R multiple times.

We assume time steps correspond to the steps
taken by the breadth-ﬁrst traversal of the depen-
dency tree and the sentence has length n. At
time step t (1 ≤ t ≤ n), let (cid:104)wt(cid:48), zt(cid:105) denote the last
Subscripts t and t(cid:48) denote the
tuple in D(wt).
breadth-ﬁrst search order of wt and wt(cid:48), respectively.
zt ∈ {LEFT, RIGHT, NX-LEFT, NX-RIGHT} is the
edge type (see the deﬁnitions in Section 2.1). Let
We ∈ Rs×|V | denote the word embedding matrix and

Who ∈ R|V |×d the output matrix of our model, where
|V | is the vocabulary size, s the word embedding size
and d the hidden unit size. We use tied We and tied
Who for the four LSTMs to reduce the number of pa-
rameters in our model. The four LSTMs also share
their hidden states. Let H ∈ Rd×(n+1) denote the
shared hidden states of all time steps and e(wt) the
one-hot vector of wt. Then, H[:,t] represents D(wt)
at time step t, and the computation2 is:

xt = We · e(wt(cid:48))
ht = LSTMzt (xt, H[:,t(cid:48)])

H[:,t] = ht

yt = Who · ht

(2a)

(2b)

(2c)

(2d)

where the initial hidden state H[:, 0] is initialized to
a vector of small values such as 0.01. According to
Equation (2b), the model selects an LSTM based on
edge type zt. We describe the details of LSTMzt in
the next paragraph. The probability of wt given its
dependency path D(wt) is estimated by a softmax
function:

P(wt|D(wt)) =

(3)

exp(yt,wt )
|V |
k(cid:48)=1 exp(yt,k(cid:48))

∑

We must point out that although we use four jointly
trained LSTMs to encode the hidden states, the train-
ing and inference complexity of our model is no dif-
ferent from a regular LSTM, since at each time step
only one LSTM is working.

We implement LSTMz in Equation (2b) using a
deep LSTM (to simplify notation, from now on we
write z instead of zt). The inputs at time step t
are xt and ht(cid:48) (the hidden state of an earlier time
step t(cid:48)) and the output is ht (the hidden state of cur-
rent time step). Let L denote the layer number of
LSTMz and ˆhl
t the internal hidden state of the l-th
layer of the LSTMz at time step t, where xt is ˆh0
t and
ht(cid:48) is ˆhL
t(cid:48). The LSTM architecture introduces mul-
tiplicative gates and memory cells ˆcl
t (at l-th layer)
in order to address the vanishing gradient problem
which makes it difﬁcult for the standard RNN model
to learn long-distance correlations in a sequence.
Here, ˆcl
t is a linear combination of the current input
signal ut and an earlier memory cell ˆcl
t(cid:48). How much
input information ut will ﬂow into ˆcl
t is controlled

2We ignore all bias terms for notational simplicity.

w2

w1

w0

G

E

N

-

L

G E N-R

w0

w4

w5

GEN-NX-L

GEN-NX-L

GEN-NX-R

GEN-NX-R

LD

LD

w0

w4

w5

w6

w3

w2

w1

Figure 4: Generation of left and right dependents of node w0
according to LDTREELSTM.

by input gate it and how much of the earlier mem-
ory cell ˆcl
t(cid:48) will be forgotten is controlled by forget
gate ft. This process is computed as follows:

ux · ˆhl−1

ut = tanh(Wz,l
it = σ(Wz,l
ft = σ(Wz,l
t = ft (cid:12) ˆcl
ˆcl

ix · ˆhl−1
f x · ˆhl−1
t(cid:48) + it (cid:12) ut

t + Wz,l
uh · ˆhl
t(cid:48))
ih · ˆhl
t(cid:48))
f h · ˆhl
t(cid:48))

t + Wz,l
t + Wz,l

(4a)

(4b)

(4c)

(4d)

ux ∈ Rd×d (Wz,l

uh ∈ Rd×d are weight matrices for ut, Wz,l
ih are weight matrices for it and Wz,l

where Wz,l
ux ∈ Rd×s when l = 1) and
Wz,l
ix and
f x, and Wz,l
Wz,l
f h
are weight matrices for ft. σ is a sigmoid function
and (cid:12) the element-wise product.

Output gate ot controls how much information of

the cell ˆcl

t can be seen by other modules:

t + Wz,l
ox · ˆhl−1
ot = σ(Wz,l
ˆhl
t = ot (cid:12) tanh(ˆcl
t )

oh · ˆhl
t(cid:48))

(5a)

(5b)

Application of the above process to all layers L, will
yield ˆhL
t , which is ht. Note that in implementation,
all ˆcl
t (1 ≤ l ≤ L) at time step t are stored,
although we only care about ˆhL
t (ht).

t and ˆhl

2.4 Left Dependent Tree LSTMs

TREELSTM computes P(w|D(w)) based on the de-
pendency path D(w), which ignores the interaction
between left and right dependents on the same level.
In many cases, TREELSTM will use a verb to pre-
dict its object directly without knowing its subject.
For example, in Figure 2, TREELSTM uses (cid:104)ROOT,
RIGHT(cid:105) and (cid:104) sold, RIGHT (cid:105) to predict cars. This in-
formation is unfortunately not speciﬁc to cars (many
things can be sold, e.g., chocolates, candy). Consid-
ering manufacturer, the left dependent of sold would
help predict cars more accurately.

In order to jointly take left and right dependents
into account, we employ yet another LSTM, which
goes from the furthest left dependent to the closest
left dependent (LD is a shorthand for left depen-
dent). As shown in Figure 4, LD LSTM learns the
representation of all left dependents of a node w0;
this representation is then used to predict the ﬁrst
right dependent of the same node. Non-ﬁrst right de-
pendents can also leverage the representation of left
dependents, since this information is injected into
the hidden state of the ﬁrst right dependent and can
percolate all the way. Note that in order to retain the
generation capability of our model (Section 3.4), we
only allow right dependents to leverage left depen-
dents (they are generated before right dependents).

The computation of the LDTREELSTM is al-
the same as in TREELSTM except when
most
zt = GEN-R.
let vt be the cor-
responding left dependent sequence with length
K (vt = (w3, w2, w1) in Figure 4). Then, the hidden
state (qk) of vt at each time step k is:

In this case,

mk = We · e(vt,k)
qk = LSTMLD(mk, qk−1)

(6a)

(6b)

where qK is the representation for all left depen-
dents. Then, the computation of the current hid-
den state becomes (see Equation (2) for the original
computation):

rt =

(cid:21)
(cid:20)We · e(wt(cid:48))
qK

ht = LSTMGEN-R(rt, H[:,t(cid:48)])

(7a)

(7b)

where qK serves as additional input for LSTMGEN-R.
All other computational details are the same as in
TreeLSTM (see Section 2.3).

2.5 Model Training

On small scale datasets we employ Negative Log-
likelihood (NLL) as our training objective for both
TREELSTM and LDTREELSTM:

L NLL(θ) = −

log P(S|T )

(8)

1
|S | ∑

S∈S

where S is a sentence in the training set S , T is the
dependency tree of S and P(S|T ) is deﬁned as in
Equation (1).

On large scale datasets (e.g., with vocabulary
size of 65K), computing the output layer activa-
tions and the softmax function with NLL would
become prohibitively expensive.
Instead, we em-
ploy Noise Contrastive Estimation (NCE; Gutmann
and Hyv¨arinen (2012), Mnih and Teh (2012)) which
treats the normalization term ˆZ in ˆP(w|D(wt)) =
exp(Who[w,:]·ht )
as constant. The intuition behind NCE
ˆZ
is to discriminate between samples from a data dis-
tribution ˆP(w|D(wt)) and a known noise distribu-
tion Pn(w) via binary logistic regression. Assuming
that noise words are k times more frequent than real
words in the training set (Mnih and Teh, 2012), then
the probability of a word w being from our model
Pd(w,D(wt)) is
. We apply NCE to
large vocabulary models with the following training
objective:

ˆP(w|D(wt ))
ˆP(w|D(wt ))+kPn(w)

L NCE(θ) = −

log Pd(wt,D(wt))

(cid:18)

1
|S | ∑

T ∈S

|T |
∑
t=1

+

log[1 − Pd( ˜wt, j,D(wt))]

(cid:19)

k
∑
j=1

where ˜wt, j is a word sampled from the noise distri-
bution Pn(w). We use smoothed unigram frequen-
cies (exponentiating by 0.75) as the noise distribu-
tion Pn(w) (Mikolov et al., 2013b). We initialize
ln ˆZ = 9 as suggested in Chen et al. (2015), but in-
stead of keeping it ﬁxed we also learn ˆZ during train-
ing (Vaswani et al., 2013). We set k = 20.

3 Experiments

We assess the performance of our model on two
tasks: the Microsoft Research (MSR) sentence com-
pletion challenge (Zweig and Burges, 2012), and de-
pendency parsing reranking. We also demonstrate
the tree generation capability of our models. In the
following, we ﬁrst present details on model train-
ing and then present our results. We implemented
our models using the Torch library (Collobert et
al., 2011) and our code is available at https://
github.com/XingxingZhang/td-treelstm.

3.1 Training Details

We trained our model with back propagation
through time (Rumelhart et al., 1988) on an Nvidia

GPU Card with a mini-batch size of 64. The ob-
jective (NLL or NCE) was minimized by stochastic
gradient descent. Model parameters were uniformly
initialized in [−0.1, 0.1]. We used the NCE objec-
tive on the MSR sentence completion task (due to
the large size of this dataset) and the NLL objec-
tive on dependency parsing reranking. We used an
initial learning rate of 1.0 for all experiments and
when there was no signiﬁcant improvement in log-
likelihood on the validation set, the learning rate was
divided by 2 per epoch until convergence (Mikolov
et al., 2010). To alleviate the exploding gradients
problem, we rescaled the gradient g when the gradi-
ent norm ||g|| > 5 and set g = 5g
||g|| (Pascanu et al.,
2013; Sutskever et al., 2014). Dropout (Srivastava
et al., 2014) was applied to the 2-layer TREELSTM
and LDTREELSTM models. The word embedding
size was set to s = d/2 where d is the hidden unit
size.

3.2 Microsoft Sentence Completion Challenge

The task in the MSR Sentence Completion Chal-
lenge (Zweig and Burges, 2012) is to select the
correct missing word for 1,040 SAT-style test sen-
tences when presented with ﬁve candidate comple-
tions. The training set contains 522 novels from
the Project Gutenberg which we preprocessed as fol-
lows. After removing headers and footers from the
ﬁles, we tokenized and parsed the dataset into de-
pendency trees with the Stanford Core NLP toolkit
(Manning et al., 2014). The resulting training set
contained 49M words. We converted all words to
lower case and replaced those occurring ﬁve times
or less with UNK. The resulting vocabulary size
was 65,346 words. We randomly sampled 4,000
sentences from the training set as our validation set.
The literature describes two main approaches to
the sentence completion task based on word vectors
and language models. In vector-based approaches,
all words in the sentence and the ﬁve candidate
words are represented by a vector; the candidate
which has the highest average similarity with the
sentence words is selected as the answer. For lan-
guage model-based methods, the LM computes the
probability of a test sentence with each of the ﬁve
candidate words, and picks the candidate comple-
tion which gives the highest probability. Our model
belongs to this class of models.

|θ|

Accuracy

d

—
640
600

Model
Word Vector based Models
LSA
Skip-gram
IVLBL
Language Models
KN5
UDepNgram
LDepNgram
RNN
RNNME
depRNN+3gram
ldepRNN+4gram
LBL
LSTM
LSTM
LSTM
Bidirectional LSTM
Bidirectional LSTM
Bidirectional LSTM
Model Combinations
RNNMEs
—
Skip-gram + RNNMEs —
Our Models
TREELSTM
LDTREELSTM
TREELSTM
LDTREELSTM

—
—
—
300
300
100
200
300
300
400
450
200
300
400

300
300
400
400

—
102M
96.0M

—
—
—
48.1M
1120M
1014M
1029M
48.0M
29.9M
40.2M
45.3M
33.2M
50.1M
67.3M

—
—

31.6M
32.5M
43.1M
44.7M

49.0
48.0
55.5

40.0
48.3
50.0
45.0
49.3
53.5
50.7
54.7
55.00
57.02
55.96
48.46
49.90
48.65

55.4
58.9

55.29
57.79
56.73
60.67

Table 1: Model accuracy on the MSR sentence completion task.

The results of KN5, RNNME and RNNMEs are reported in

Mikolov (2012), LSA and RNN in Zweig et al. (2012), UDep-

Ngram and LDepNgram in Gubbins and Vlachos (2013), de-

pRNN+3gram and depRNN+4gram in Mirowski and Vlachos

(2015), LBL in Mnih and Teh (2012), Skip-gram and Skip-

gram+RNNMEs in Mikolov et al. (2013a), and IVLBL in Mnih
and Kavukcuoglu (2013); d is the hidden size and |θ| the num-
ber of parameters in a model.

Table 1 presents a summary of our results to-
gether with previoulsy published results. The best
performing word vector model is IVLBL (Mnih and
Kavukcuoglu, 2013) with an accuracy of 55.5, while
the best performing single language model is LBL
(Mnih and Teh, 2012) with an accuracy of 54.7.
Both approaches are based on the log-bilinear lan-
guage model (Mnih and Hinton, 2007). A combi-
nation of several recurrent neural networks and the
skip-gram model holds the state of the art with an
accuracy of 58.9 (Mikolov et al., 2013b). To fairly
compare with existing models, we restrict the layer

Test

Parser

Development
UAS
MSTParser-2nd
92.20
TREELSTM
92.51
92.64
TREELSTM*
LDTREELSTM 92.66
92.00
NN parser*
93.20
S-LSTM*

LAS
88.44
88.53
88.69
88.69
89.60
90.90
Table 2: Performance of TREELSTM and LDTREELSTM on

LAS UAS
91.63
88.78
91.79
89.07
91.97
89.09
91.99
89.14
91.80
89.70
93.10
90.90

reranking the top dependency trees produced by the 2nd order

MSTParser (McDonald and Pereira, 2006). Results for the NN

and S-LSTM parsers are reported in Chen and Manning (2014)

and Dyer et al. (2015), respectively. * indicates that the model

is initialized with pre-trained word vectors.

size of our models to 1. We observe that LDTREEL-
STM consistently outperforms TREELSTM, which
indicates the importance of modeling the interac-
In fact,
tion between left and right dependents.
LDTREELSTM (d = 400) achieves a new state-of-
the-art on this task, despite being a single model.
We also implement LSTM and bidirectional LSTM
language models.3 An LSTM with d = 400 out-
performs its smaller counterpart (d = 300), however
performance decreases with d = 450. The bidirec-
tional LSTM is worse than the LSTM (see Mnih
and Teh (2012) for a similar observation). The
best performing LSTM is worse than a LDTREEL-
STM (d = 300). The input and output embeddings
(We and Who) dominate the number of parame-
ters in all neural models except for RNNME, de-
pRNN+3gram and ldepRNN+4gram, which include
a ME model that contains 1 billion sparse n-gram
features (Mikolov, 2012; Mirowski and Vlachos,
2015). The number of parameters in TREELSTM
and LDTREELSTM is not much larger compared to
LSTM due to the tied We and Who matrices.

3.3 Dependency Parsing

In this section we demonstrate that our model can
be also used for parse reranking. This is not possi-
ble for sequence-based language models since they
cannot estimate the probability of a tree. We use
our models to rerank the top K dependency trees
produced by the second order MSTParser (McDon-

3LSTMs and BiLSTMs were also trained with NCE
(s = d/2; hyperparameters were tuned on the development set).

ald and Pereira, 2006).4 We follow closely the ex-
perimental setup of Chen and Manning (2014) and
Dyer et al. (2015). Speciﬁcally, we trained TREEL-
STM and LDTREELSTM on Penn Treebank sec-
tions 2–21. We used section 22 for development and
section 23 for testing. We adopted the Stanford ba-
sic dependency representations (De Marneffe et al.,
2006); part-of-speech tags were predicted with the
Stanford Tagger (Toutanova et al., 2003). We trained
TREELSTM and LDTREELSTM as language mod-
els (singletons were replaced with UNK) and did
not use any POS tags, dependency labels or com-
position features, whereas these features are used in
Chen and Manning (2014) and Dyer et al. (2015).
We tuned d, the number of layers, and K on the de-
velopment set.

Table 2 reports unlabeled attachment scores
(UAS) and labeled attachment scores (LAS) for
the MSTParser, TREELSTM (d = 300, 1 layer,
K = 2), and LDTREELSTM (d = 200, 2 layers,
K = 4). We also include the performance of two
neural network-based dependency parsers; Chen and
Manning (2014) use a neural network classiﬁer to
predict the correct transition (NN parser); Dyer et
al. (2015) also implement a transition-based depen-
dency parser using LSTMs to represent the contents
of the stack and buffer in a continuous space. As can
be seen, both TREELSTM and LDTREELSTM out-
perform the baseline MSTParser, with LDTREEL-
STM performing best. We also initialized the word
embedding matrix We with pre-trained GLOVE vec-
tors (Pennington et al., 2014). We obtained a slight
improvement over TREELSTM (TREELSTM* in
Table 2; d = 200, 2 layer, K = 4) but no im-
provement over LDTREELSTM. Finally, notice that
LDTREELSTM is slightly better than the NN parser
in terms of UAS but worse than the S-LSTM parser.
In the future, we would like to extend our model so
that it takes labeled dependency information into ac-
count.

3.4 Tree Generation

This section demonstrates how to use a trained
LDTREELSTM to generate tree samples. The gen-
eration starts at the ROOT node. At each time step t,
for each node wt, we add a new edge and node to

Figure 5: Generated dependency trees with LDTREELSTM

trained on the PTB.

the tree. Unfortunately during generation, we do not
know which type of edge to add. We therefore use
four binary classiﬁers (ADD-LEFT, ADD-RIGHT,
ADD-NX-LEFT and ADD-NX-RIGHT) to predict
whether we should add a LEFT, RIGHT, NX-LEFT
or NX-RIGHT edge.5 Then when a classiﬁer pre-
dicts true, we use the corresponding LSTM to gener-
ate a new node by sampling from the predicted word
distribution in Equation (3). The four classiﬁers take
the previous hidden state H[:,t(cid:48)] and the output em-
bedding of the current node Who · e(wt) as features.6
Speciﬁcally, we use a trained LDTREELSTM to
go through the training corpus and generate hidden
states and embeddings as input features; the corre-
sponding class labels (true and false) are “read off”
the training dependency trees. We use two-layer rec-
tiﬁer networks (Glorot et al., 2011) as the four clas-
siﬁers with a hidden size of 300. We use the same
LDTREELSTM model as in Section 3.3 to gener-
ate dependency trees. The classiﬁers were trained
using AdaGrad (Duchi et al., 2011) with a learning
rate of 0.01. The accuracies of ADD-LEFT, ADD-
RIGHT, ADD-NX-LEFT and ADD-NX-RIGHT are

5It is possible to get rid of the four classiﬁers by adding
START/STOP symbols when generating left and right depen-
dents as in (Eisner, 1996). We refrained from doing this for
computational reasons. For a sentence with N words, this ap-
proach will lead to 2N additional START/STOP symbols (with
one START and one STOP symbol for each word). Conse-
quently, the computational cost and memory consumption dur-
ing training will be three times as much rendering our model
less scalable.

6The input embeddings have lower dimensions and therefore

4http://www.seas.upenn.edu/ strctlrn/MSTParser

result in slightly worse classiﬁers.

94.3%, 92.6%, 93.4% and 96.0%, respectively. Fig-
ure 5 shows examples of generated trees.

4 Conclusions

In this paper we developed TREELSTM (and
LDTREELSTM), a neural network model architec-
ture, which is designed to predict tree structures
rather than linear sequences. Experimental results
on the MSR sentence completion task show that
LDTREELSTM is superior to sequential LSTMs.
Dependency parsing reranking experiments high-
light our model’s potential for dependency pars-
ing. Finally, the ability of our model to gener-
ate dependency trees holds promise for text gen-
eration applications such as sentence compression
and simpliﬁcation (Filippova et al., 2015). Although
our experiments have focused exclusively on depen-
dency trees, there is nothing inherent in our formu-
lation that disallows its application to other types of
tree structure such as constituent trees or even tax-
onomies.

Acknowledgments

We would like to thank Adam Lopez, Frank Keller,
Iain Murray, Li Dong, Brian Roark, and the NAACL
reviewers for their valuable feedback. Xingxing
Zhang gratefully acknowledges the ﬁnancial sup-
port of the China Scholarship Council (CSC). Liang
Lu is funded by the UK EPSRC Programme Grant
EP/I031022/1, Natural Speech Technology (NST).

References

[Bengio et al.2003] Yoshua Bengio, R´ejean Ducharme,
Pascal Vincent, and Christian Janvin. 2003. A neural
probabilistic language model. The Journal of Machine
Learning Research, 3:1137–1155.

[Charniak2001] Eugene Charniak.

Immediate-
head parsing for language models. In Proceedings of
the 39th Annual Meeting on Association for Compu-
tational Linguistics, pages 124–131. Association for
Computational Linguistics.

2001.

[Chelba and Jelinek2000] Ciprian Chelba and Frederick
Jelinek. 2000. Structured language modeling. Com-
puter Speech and Language, 14(4):283–332.

[Chelba et al.1997] Ciprian Chelba, David Engle, Freder-
ick Jelinek, Victor Jimenez, Sanjeev Khudanpur, Lidia
Mangu, Harry Printz, Eric Ristad, Ronald Rosenfeld,

Andreas Stolcke, et al.
formance of a dependency language model.
ROSPEECH. Citeseer.

1997. Structure and per-
In EU-

[Chen and Manning2014] Danqi Chen and Christopher
2014. A fast and accurate dependency
Manning.
In Proceedings of
parser using neural networks.
the 2014 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 740–750,
Doha, Qatar, October. Association for Computational
Linguistics.

[Chen et al.2015] X Chen, X Liu, MJF Gales, and
PC Woodland. 2015. Recurrent neural network lan-
guage model training with noise contrastive estimation
for speech recognition. In In 40th IEEE International
Conference on Accoustics, Speech and Signal Process-
ing, pages 5401–5405, Brisbane, Australia.

[Collobert et al.2011] Ronan

Collobert,

Koray
Kavukcuoglu, and Cl´ement Farabet. 2011. Torch7:
A matlab-like environment for machine learning.
In
BigLearn, NIPS Workshop, number EPFL-CONF-
192376.

[De Marneffe et al.2006] Marie-Catherine De Marneffe,
Bill MacCartney, Christopher D Manning, et al. 2006.
Generating typed dependency parses from phrase
structure parses. In Proceedings of LREC, volume 6,
pages 449–454.

[Duchi et al.2011] John Duchi, Elad Hazan, and Yoram
Singer. 2011. Adaptive subgradient methods for on-
line learning and stochastic optimization. The Journal
of Machine Learning Research, 12:2121–2159.

[Dyer et al.2015] Chris Dyer, Miguel Ballesteros, Wang
Ling, Austin Matthews, and Noah A. Smith. 2015.
Transition-based dependency parsing with stack long
In Proceedings of the 53rd An-
short-term memory.
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long Pa-
pers), pages 334–343, Beijing, China, July. Associa-
tion for Computational Linguistics.

[Eisner1996] Jason M Eisner. 1996. Three new prob-
abilistic models for dependency parsing: An explo-
ration. In Proceedings of the 16th conference on Com-
putational linguistics-Volume 1, pages 340–345. Asso-
ciation for Computational Linguistics.

[Emami et al.2003] Ahmad Emami, Peng Xu, and Fred-
erick Jelinek. 2003. Using a connectionist model in
In Proceedings
a syntactical based language model.
of the IEEE International Conference on Acoustics,
Speech, and Signal Processing, pages 372–375, Hong
Kong, China.

[Filippova et al.2015] Katja Filippova, Enrique Alfon-
seca, Carlos A Colmenares, Lukasz Kaiser, and Oriol
Vinyals. 2015. Sentence compression by deletion
with lstms. In EMNLP, pages 360–368.

[Glorot et al.2011] Xavier Glorot, Antoine Bordes, and
Yoshua Bengio. 2011. Deep sparse rectiﬁer neural
networks. In International Conference on Artiﬁcial In-
telligence and Statistics, pages 315–323.

[Graves et al.2013] Alan Graves, Abdel-rahman Mo-
hamed, and Geoffrey Hinton. 2013. Speech recogni-
tion with deep recurrent neural networks. In Acoustics,
Speech and Signal Processing (ICASSP), 2013 IEEE
International Conference on, pages 6645–6649. IEEE.
[Gubbins and Vlachos2013] Joseph Gubbins and Andreas
Vlachos. 2013. Dependency language models for sen-
tence completion. In EMNLP, pages 1405–1410, Seat-
tle, Washington, USA, October. Association for Com-
putational Linguistics.

[Gutmann and Hyv¨arinen2012] Michael U Gutmann and
Aapo Hyv¨arinen. 2012. Noise-contrastive estimation
of unnormalized statistical models, with applications
to natural image statistics. The Journal of Machine
Learning Research, 13(1):307–361.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural computation, 9(8):1735–1780.

[Hochreiter1998] Sepp Hochreiter. 1998. Vanishing gra-
dient problem during learning recurrent neural nets
and problem solutions. International Journal of Un-
certainty, Fuzziness and Knowledge-based Systems,
6(2):107–116.

[Manning et al.2014] Christopher D Manning, Mihai Sur-
deanu, John Bauer, Jenny Finkel, Steven J Bethard,
and David McClosky. 2014. The stanford corenlp
In Proceedings
natural language processing toolkit.
of 52nd Annual Meeting of the Association for Com-
putational Linguistics: System Demonstrations, pages
55–60.

[McDonald and Pereira2006] Ryan T McDonald and Fer-
nando CN Pereira. 2006. Online learning of approxi-
mate dependency parsing algorithms. In EACL.

[Mikolov et al.2010] Tomas Mikolov, Martin Karaﬁ´at,
Lukas Burget, Jan Cernock`y, and Sanjeev Khudan-
pur. 2010. Recurrent neural network based language
model. In INTERSPEECH 2010, 11th Annual Confer-
ence of the International Speech Communication As-
sociation, Makuhari, Chiba, Japan, September 26-30,
2010, pages 1045–1048.

[Mikolov et al.2013a] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013a. Efﬁcient esti-
In
mation of word representations in vector space.
Proceedings of the 2013 International Conference on
Learning Representations, Scottsdale, Arizona, USA.
Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013b.
Distributed representations of words and phrases and
In Advances in Neural Infor-
their compositionality.
mation Processing Systems 26, pages 3111–3119.

[Mikolov et al.2013b] Tomas Mikolov,

[Mikolov2012] Tomas Mikolov. 2012. Statistical Lan-
guage Models based on Neural Networks. Ph.D. the-
sis, Brno University of Technology.

[Mirowski and Vlachos2015] Piotr Mirowski and An-
dreas Vlachos. 2015. Dependency recurrent neural
In ACL,
language models for sentence completion.
pages 511–517, Beijing, China, July. Association for
Computational Linguistics.

[Mnih and Hinton2007] Andriy Mnih and Geoffrey Hin-
ton. 2007. Three new graphical models for statistical
In Proceedings of the 24th In-
language modelling.
ternational Conference on Machine Learning, pages
641–648.

[Mnih and Kavukcuoglu2013] Andriy Mnih and Koray
Kavukcuoglu. 2013. Learning word embeddings efﬁ-
ciently with noise-contrastive estimation. In Advances
in Neural Information Processing Systems 26, pages
2265–2273.

[Mnih and Teh2012] Andriy Mnih and Yee Whye Teh.
2012. A fast and simple algorithm for training neural
probabilistic language models. In Proceedings of the
29th International Conference on Machine Learning,
pages 1751–1758, Edinburgh, Scotland.

[Pascanu et al.2013] Razvan Pascanu, Tomas Mikolov,
and Yoshua Bengio. 2013. On the difﬁculty of train-
ing recurrent neural networks. In Proceedings of the
31st International Conference on Machine Learning,
pages 1310–1318, Atlanta, Georgia, USA.
Pennington,

Richard
Socher, and Christopher D Manning. 2014. Glove:
EMNLP,
Global vectors for word representation.
12:1532–1543.

[Pennington et al.2014] Jeffrey

[Pollack1990] Jordan B. Pollack. 1990. Recursive dis-
tributed representations. Artiﬁcial Intelligence, 1–
2(46):77–105.

[Roark2001] Brian Roark. 2001. Probabilistic top-down
parsing and language modeling. Computational lin-
guistics, 27(2):249–276.

[Rumelhart et al.1988] David E Rumelhart, Geoffrey E
Hinton, and Ronald J Williams. 1988. Learning repre-
sentations by back-propagating errors. Cognitive mod-
eling, 5:3.

[Sennrich2015] Rico Sennrich. 2015. Modelling and op-
timizing on syntactic n-grams for statistical machine
translation. Transactions of the Association for Com-
putational Linguistics, 3:169–182.

[Shen et al.2008] Libin Shen,

Jinxi Xu,

and Ralph
Weischedel. 2008. A new string-to-dependency ma-
chine translation algorithm with a target dependency
In Proceedings of ACL-08: HLT,
language model.
pages 577–585, Columbus, Ohio, USA.

[Socher et al.2011a] Richard Socher, Eric H. Huang, Jef-
frey Pennington, Christopher D. Manning, and An-
drew Ng. 2011a. Dynamic pooling and unfolding

Model? On the Future of Language Modeling for HLT,
pages 29–36, Montr´eal, Canada.

[Zweig et al.2012] Geoffrey Zweig, John C Platt, Christo-
pher Meek, Christopher JC Burges, Ainur Yessenalina,
2012. Computational approaches
and Qiang Liu.
In Proceedings of the 50th
to sentence completion.
Annual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 601–610.
Association for Computational Linguistics.

In
recursive autoencoders for paraphrase detection.
Advances in Neural Information Processing Systems,
pages 801–809.

[Socher et al.2011b] Richard Socher, Jeffrey Pennington,
Eric H. Huang, Andrew Y. Ng, and Christopher D.
Manning. 2011b. Semi-supervised recursive autoen-
coders for predicting sentiment distributions. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 151–161,
Edinburgh, Scotland, UK.

[Srivastava et al.2014] Nitish Srivastava, Geoffrey Hin-
ton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: A simple way to pre-
vent neural networks from overﬁtting. The Journal of
Machine Learning Research, 15(1):1929–1958.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and
Quoc VV Le. 2014. Sequence to sequence learning
with neural networks. In Advances in Neural Informa-
tion Processing Systems, pages 3104–3112.

[Tai et al.2015] Kai Sheng Tai, Richard Socher, and
Christopher D. Manning. 2015.
Improved semantic
representations from tree-structured long short-term
memory networks. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguis-
tics and the 7th International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers),
pages 1556–1566, Beijing, China, July. Association
for Computational Linguistics.

[Toutanova et al.2003] Kristina Toutanova, Dan Klein,
Christopher D Manning, and Yoram Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic de-
pendency network. In Proceedings of the 2003 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology-Volume 1, pages 173–180. Associa-
tion for Computational Linguistics.

[Vaswani et al.2013] Ashish Vaswani, Yinggong Zhao,
Victoria Fossum, and David Chiang. 2013. Decod-
ing with large-scale neural language models improves
translation. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
pages 1387–1392, Seattle, Washington, USA.

[Vinyals et al.2015] Oriol Vinyals, Alexander Toshev,
Samy Bengio, and Dumitru Erhan. 2015. Show and
tell: A neural image caption generator. In The IEEE
Conference on Computer Vision and Pattern Recogni-
tion, Boston, Massachusetts, USA.

[Zhang2009] Ying Zhang. 2009. Structured language
models for statistical machine translation. Ph.D. the-
sis, Johns Hopkins University.

[Zweig and Burges2012] Geoffrey Zweig and Chris J.C.
Burges. 2012. A challenge set for advancing language
In Proceedings of the NAACL-HLT 2012
modeling.
Workshop: Will We Ever Really Replace the N-gram

