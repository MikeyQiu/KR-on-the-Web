Switch-LSTMs for Multi-Criteria Chinese Word Segmentation

Jingjing Gong∗, Xinchi Chen∗, Tao Gui, Xipeng Qiu†
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
School of Computer Science, Fudan University
Shanghai Insitute of Intelligent Electroics & Systems
{jjgong, xinchichen13, tgui16, xpqiu}@fudan.edu.cn

8
1
0
2
 
c
e
D
 
9
1
 
 
]
L
C
.
s
c
[
 
 
1
v
3
3
0
8
0
.
2
1
8
1
:
v
i
X
r
a

Abstract

Multi-criteria Chinese word segmentation is a promising but
challenging task, which exploits several different segmenta-
tion criteria and mines their common underlying knowledge.
In this paper, we propose a ﬂexible multi-criteria learning for
Chinese word segmentation. Usually, a segmentation crite-
rion could be decomposed into multiple sub-criteria, which
are shareable with other segmentation criteria. The process
of word segmentation is a routing among these sub-criteria.
From this perspective, we present Switch-LSTMs to segment
words, which consist of several long short-term memory neu-
ral networks (LSTM), and a switcher to automatically switch
the routing among these LSTMs. With these auto-switched
LSTMs, our model provides a more ﬂexible solution for
multi-criteria CWS, which is also easy to transfer the learned
knowledge to new criteria. Experiments show that our model
obtains signiﬁcant improvements on eight corpora with het-
erogeneous segmentation criteria, compared to the previous
method and single-criterion learning.

Introduction
Chinese word segmentation (CWS) is a preliminary and im-
portant task for Chinese natural language processing (NLP).
Currently, the state-of-the-art CWS methods are based on
supervised machine learning algorithms and greatly rely on
a large-scale annotated corpus whose cost is extremely ex-
pensive. Although several CWS corpora have been built with
great efforts, their segmentation criteria are different. Since
these segmentation criteria are from different linguistic per-
spectives, it remains to be a challenging problem on how to
effectively utilize these resources.

Most existing methods focus on improving the perfor-
mance of each single segmentation criterion. Recently, deep
learning methods have been widely used in segmenting Chi-
nese words and can effectively reduce the efforts of fea-
ture engineering (Zheng, Chen, and Xu 2013; Pei, Ge, and
Baobao 2014; Chen et al. 2015a; 2015b; Cai and Zhao 2016;
Zhang, Zhang, and Fu 2016; Yao and Huang 2016; Gong et
al. 2017). However, it is a waste of resources if we fail to
fully exploit all the corpora with different criteria.

∗ J. Gong and X. Chen contributed equally to this paper.
† Corresponding author.

Copyright c(cid:13) 2019, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Corpora
CTB
PKU
MSRA

Lin Dan
林丹

won
拿了

the championship
总冠军

林 丹 拿了 总
拿了 总

林丹

冠军
冠军

Table 1: Illustration of the different segmentation criteria,
which partially share some underlying sub-criteria.

Chen et al. (2017) proposed a multi-criteria learning
framework for CWS. Speciﬁcally, they regard each segmen-
tation criterion as a single task under the framework of
multi-task learning (Caruana 1997), where a shared layer
is used to extract the criteria-invariant features, and a pri-
vate layer is used to extract the criteria-speciﬁc features.
However, it is unnecessary to use a speciﬁc private layer
for each criterion. These different criterions often have par-
tial overlaps. As shown in Table 1, given a sentence “林丹
拿了总冠军(Lin Dan won the championship)”, the three
commonly-used corpora, PKU’s People’s Daily (PKU) (Yu
et al. 2001), Penn Chinese Treebank (CTB) (Fei 2000) and
MSRA (Emerson 2005), use different segmentation criteria.
The segmentation of “林丹(Lin Dan)” is the same as in PKU
and MSRA criteria, and the segmentation of “总|冠军(the
championship)” is the same as in CTB and MSRA criteria.
All these three criteria have same segmentation for the word
“拿了(won)”.

Inspired by the above example, we propose a more ﬂexi-
ble model for multi-criteria CWS. Each segmentation crite-
rion can be separated into several sub-criteria, each of which
is implemented by a long short-term memory neural net-
work (LSTM) (Hochreiter and Schmidhuber 1997). When
segmenting a sentence, these sub-criteria are automatically
switched for different words. Therefore, speciﬁc segmenta-
tion criteria can be regarded as a combination of these sub-
criteria.

Speciﬁcally, our model consists of several LSTM cells,
each of which represents a segmentation sub-criterion, and a
controller, called switcher, to switch the routing among the
different LSTMs. These sub-criteria could be shared among
different segmentation criteria.

The major merits of our proposed approach are simplicity
and ﬂexibility. Existing work on multi-task learning usually
has private layers for each task, parameter, and scale of the

(a) Single crite-
rion CWS

(b) Multi-criteria CWS with MTL

(c) Multi-criteria CWS
with Switch-LSTMs

Figure 1: Architectures of single-criterion and multi-criteria Chinese word segmentation. Parameters of components in orange
are shared, and those in green are private.

computational graph will linearly grow along with the num-
ber of tasks. Also, private layers will reduce the ﬂexibility
of the model, for example when a new task comes in, ap-
proaches with private layers will need to stop training and
reconstruct the whole graph. While in our proposed method,
all layers can be considered as the shared layer, which will
perfectly avoid the growing number of parameter and graph
scale. And more interestingly, we can pre-allocate many pre-
served task embeddings with negligible overhead. With ex-
tra task embeddings, we can effortlessly hot-plug a new task
into the in-training model.

Finally, we experiment on eight Chinese word segmenta-
tion datasets. Experiments show that our models are effec-
tive to improve the performance of CWS.

Background

Single Criterion CWS with LSTM-CRF
Framework

Chinese word segmentation task could be viewed as a char-
acter based sequence labeling problem (Zheng, Chen, and
Xu 2013; Pei, Ge, and Baobao 2014; Chen et al. 2015a;
2015b). Speciﬁcally, each character in a sentence X =
{x1, . . . , xn} is labelled as one of L = {B, M, E, S}, in-
dicating the begin, middle, end of a word, or a word with
single character. The aim of CWS task is to ﬁgure out the
ground truth of labels Y ∗ = {y∗

1, . . . , y∗

n}:

Y ∗ = arg max

p(Y |X).

Y ∈Ln

The popular architecture of neural CWS could be char-
acterized by three components: (1) a character embedding
layer; (2) feature extraction layers consisting of several clas-
sical neural networks and (3) a CRF tag inference layer. Fig-
ure 1a illustrates the general architecture of CWS.

Embedding layer
In neural models, the ﬁrst step is to map
discrete language symbols into distributed embedding space.
Formally, each character xt is mapped as ext ∈ Rde ⊂ E,
where de is a hyper-parameter indicating the size of charac-
ter embedding, and E is the embedding matrix.

Encoding layers Usually a bi-directional LSTM (Bi-
LSTMs) (Hochreiter and Schmidhuber 1997) is a prevalent
choice for the encoding layer. It could incorporate informa-
tion from both sides of sequence

ht = Bi-LSTMs(ext,

−→
h t−1,

←−
h t+1, θ),

(2)

−→
h t and

←−
h t are the hidden states at step t of the for-
where
ward and backward LSTMs respectively. θ denotes all the
parameters in Bi-LSTMs layer.

Inference Layer The extracted features are then sent to
conditional random ﬁelds (CRF) (Lafferty, McCallum, and
Pereira 2001) layer for tag inference. In CRF layer, p(Y |X)
in Eq (1) could be formalized as:

p(Y |X) =

Ψ(Y |X)
Y (cid:48)∈Ln Ψ(Y (cid:48)|X)

.

(cid:80)

Here, Ψ(Y |X) is the potential function. In ﬁrst order linear
chain CRFs, we have:

n
(cid:89)

t=2

Ψ(Y |X) =

ψ(X, t, yt−1, yt),

ψ(x, t, y(cid:48), y) = exp(δ(X, t)y + by(cid:48)y),

(3)

(4)

(5)

(6)

where by(cid:48)y ∈ R is trainable parameters respective to label
pair (y(cid:48), y). Score function δ(X, t) ∈ R|L| calculates scores
of each label for tagging the t-th character:

(1)

δ(X, t) = W(cid:62)

δ ht + bδ,

where ht is the hidden state of Bi-LSTMs at step t; Wδ ∈
Rdh×|L| and bδ ∈ R|L| are trainable parameters.

Multi-Criteria CWS using Multi-Task Learning
Annotations in Chinese word segmentation are valuable and
expensive, Chen et al. (2017) tried to train on Chinese word
segmentation annotations in multiple heterogeneous criteria
to improve the performance. The multi-task learning frame-
work is a suitable way to exploit the shared information
among these different criteria as shown in Figure 1b.

Figure 2: Switch-LSTMs for multi-criteria Chinese word segmentation. at denote switches taking ht−1, xt and task ID m as
inputs, and output distributions over k-way LSTMs.

Formally, assuming that there are M corpora with het-
erogeneous segmentation criteria, we refer Dm as corpus m
with Nm samples:

Dm = {(X (m)
(7)
i denote the i-th sentence and the corre-

, Y (m)
i

)}Nm
i=1,

i

where X m
sponding label in corpus m respectively.

i and Y m

To exploit information across multiple datasets, the fea-
ture layer additionally introduce an shared Bi-LSTMs, to-
gether with the original private Bi-LSTMs as in Eq (2). Con-
cretely, for corpus m, the hidden states of shared layer and
private layer are calculated as:

h(s)
h(m)

t = Bi-LSTMs(ext,

t = Bi-LSTMs(ext,

−→
h (s)
t−1,
−→
h (m)
t−1,

←−
h (s)
←−
h (m)

t+1; θ(s)
h ),
t+1; θ(m)
).

h

(8)

(9)

The score function δ(X, t) in Eq (6) is replaced by:

δ(X, t) = g(h(s)

, h(m)
i

; θ(m)
g

),

i
where g is a feed-forward neural network, taking shared and
private features as inputs.

(10)

Objective The objective is to maximize the log likelihood
of true labels on all the corpora:

M
(cid:88)

Nm(cid:88)

m=1

i=1

Jseg(Θm, Θs) =

log p(Y (m)

i

|X (m)
i

; Θm, Θs),

where Θm = {θ(m)
, θ(m)
g
private and shared parameters respectively.

} and Θs = {E, θ(s)

h

(11)
h } denote all the

Switch-LSTMs for Multi-Criteria CWS
Multi-task learning framework separates its parameters into
private ones and shared ones. There are two main draw-
backs. First, the model architecture should be manually de-
signed. Second, it is very difﬁcult to generalize to a new tiny

dataset. Unlike the multi-task learning framework, switch
LSTM doesn’t have any private parameters as shown in
Figure 1c. Figure 2 gives the architecture of the proposed
switch-LSTMs.

Switch-LSTMs
Switch-LSTMs consist of K independent LSTM cells and
a switcher. At each time step t, switch-LSTMs will switch
to one of K LSTMc cells according to the switcher state
at ∈ RK. Thus, the switch-LSTMs could be formalized as:

st,k = LSTM(ext, ht−1; θ(s)
at,k = Switch(ext, st,k, em), ∀k ∈ [1, K]
(cid:44) softmax(W[ext, st,k, em]),

k ), ∀k ∈ [1, K]

ht =

at,kst,k,

K
(cid:88)

k=1

(12)

(13)

(14)

1 , θ(s)

where θ(s) = {θ(s)
2 , · · · , θ(s)
K } denotes the parameter
of the corresponding LSTM. Since the switcher picks layers
according to the task property as well, we introduce a task
embedding em ⊂ Em for task ID m.

We abbreviate the above equations of switch-LSTMs to

Similar

ht = Switch-LSTMs(ext, ht−1; θ(s)).
to Bi-LSTMs, we use bi-directional switch
LSTMs (Bi-Switch-LSTMs) for multi-criteria Chinese word
segmentation. The feature ht extracted by Bi-Switch-
LSTMs could be formalized as:

(15)

ht = Bi-Switch-LSTMs(exi,

−→
h t−1,

←−
h t+1),

(16)

−→
h t and

←−
h t are forward and backward adaptive
where
LSTMs respectively as in Eq (15). Thus, the forward and
−→
A = {−→a 1, · · · , −→a n} and
backward switch gate statuses are
←−
A = {←−a 1, · · · , ←−a n} respectively.

5
0
n
a
h
g
i
S

8
0
n
a
h
g
i
S

Corpora

MSRA

AS

PKU

CTB

CKIP

CITYU

NCC

SXU

Train
Test
Train
Test
Train
Test
Train
Test
Train
Test
Train
Test
Train
Test
Train
Test

# of Tokens
2.4M
0.1M
5.4M
0.1M
1.1M
0.2M
0.6M
0.1M
0.7M
0.1M
1.1M
0.2M
0.5M
0.1M
0.5M
0.1M

# of Chars Dict Size Char Types
5.2K
88.1K
2.8K
12.9K
6.1K
141.3K
3.7K
18.8K
4.7K
55.2K
3.4K
17.6K
4.2K
42.2K
2.6K
9.8K
4.7K
48.1K
3.5K
15.3K
4.4K
43.6K
3.4K
17.8K
5.0K
45.2K
3.6K
17.5K
4.2K
32.5K
2.8K
12.4K

4.1M
0.2M
8.4M
0.2M
1.8M
0.3M
1.1M
0.1M
1.1M
0.1M
1.8M
0.3M
0.8M
0.2M
0.9M
0.2M

# of Sents OOV Rate
-
2.60%
-
4.30%
-
3.33%
-
5.55%
-
7.41%
-
8.23%
-
4.74%
-
5.12%

86.9K
4.0K
709.0K
14.4K
47.3K
6.4K
23.4K
2.1K
94.2K
10.9K
36.2K
6.7K
18.9K
3.6K
17.1K
3.7K

Table 2: Details of the eight datasets.

The score function δ(X, t) in Eq (6) is replaced by:

δ(X, t) =

−→a t,i

−→
Wi

−→
h t +

←−a t,i

←−
Wi

←−
h t,

(17)

K
(cid:88)

i=1

K
(cid:88)

i=1

conducted upon pre-trained character embedding with bi-
gram feature, embeddings are pre-trained with word2vec
toolkit (Mikolov et al. 2013) on Chinese Wikipedia corpus.
Character embeddings are shared across tasks.

where + denotes the element-wise addition operation,
−→
Wi are parameters for ith LSTM cells in left and

←−
Wi and
right Switch-LSTMs respectively.

Training

Objective for Multi-task Learning
The objective is to maximize the log likelihood Jseg(Θ):

Jseg(Θ) =

log p(Y (m)

i

|X (m)
i

, m; Θ),

(18)

M
(cid:88)

Nm(cid:88)

m=1

i=1

where Θ = {E, θ(s), Em, W,
able parameters.

−→
W,

←−
W} are all shared train-

Datasets
We experiment on eight CWS datasets from SIGHAN2005
(Emerson 2005) and SIGHAN2008 (Jin and Chen 2008). Ta-
ble 2 gives the details of the eight datasets. AS, CITYU and
CKIP are in traditional Chinese. MSRA, PKU, CTB, NCC
and SXU are in simpliﬁed Chinese. We randomly pick 10%
instances from training set as the development set for all the
datasets.

Experimental Conﬁguration
The character embedding size de is set to 100, task embed-
ding size is set to 20, the hidden size dh for our proposed
Switch-LSTMs are set to 100, the number of choices K in
K-way switch is set to one of {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}. As
a common approach to alleviate overﬁtting, we dropout our
embedding with a probability of 0.2. Other than embedding,
we use Xavier uniform initializer for all trainable parameters
in our model. All traditional Chinese characters are mapped
to the simpliﬁed Chinese character, and all experiments are

For each training step, we sample 6 tasks from the task pool,
each with a batch size of 128, the probability of being sam-
pled is proportional to the total number of samples in the
corresponding task. Then we feed 6 batches of data into 6
computational graphs and then update the global parame-
ter synchronously. For parameter selection, we keep track
of parameters that perform the best on the development set.
The training process is terminated after 7 epochs without
improvement on development set, and note that we use an
averaged F scores over all tasks to measure how good our
model is.

Switch-LSTMs could work on both single-criterion learning
and multi-criteria learning scenarios. Table 3 gives the over-
all results of the proposed model. Here, Switch-LSTMs have
4-way switches. When applying Switch-LSTMs to single-
criterion learning scenario, we omit the em term in Eq (13)
and the action at is only related to the current state st and
the input ext+1.

In single-criterion learning scenario, we compare Switch-
LSTMs with prevalent LSTM and Bi-LSTMs model. As we
can see, Bi-LSTMs outperform LSTM and Switch-LSTMs
outperform previous LSTM and Bi-LSTMs models. Quan-
titatively speaking, Switch-LSTMs obtain 94.76 in average
F-value, while LSTM and Bi-LSTMs obtain 93.97 and 94.14
in average F-value respectively. It shows that the capability
of Switch-LSTMs is greater than conventional LSTM.

In multi-criteria learning scenario, we compare Switch-
LSTMs with Bi-LSTMs and multi-task learning frame-
work (MTL) proposed by Chen et al. (2017). Notably, Bi-
LSTMs is a special case of Switch-LSTM with K set to 1

Experiments

Overall Results

Models

MSRA

AS

PKU

CTB

CKIP

CITYU

NCC

SXU

Avg.

Single-Criterion Learning

LSTM

Bi-LSTMs
(Chen et al. 2017)

Stacked Bi-LSTM
(Chen et al. 2017)

Switch-LSTMs
This Work

Multi-Criteria Learning

Bi-LSTMs

Multi-Task Framework
(Chen et al. 2017)

Switch-LSTMs
This Work

P
R
F
OOV
P
R
F
OOV
P
R
F
OOV
P
R
F
OOV

P
R
F
OOV
P
R
F
OOV
P
R
F
OOV

95.13
95.55
95.34
63.60
95.70
95.99
95.84
66.28
95.69
95.81
95.75
65.55
96.07
96.86
96.46
69.90

94.64
93.20
93.91
65.60
95.76
95.89
95.82
70.72
97.69
97.87
97.78
64.20

93.66
94.71
94.18
69.83
93.64
94.77
94.20
70.07
93.89
94.54
94.22
71.50
93.83
95.21
94.51
77.80

93.54
94.06
93.80
89.20
93.99
95.07
94.53
72.59
94.42
96.03
95.22
77.33

93.96
92.65
93.30
66.34
93.67
92.93
93.30
66.09
94.10
92.66
93.37
67.92
95.92
95.56
95.74
72.70

93.24
91.94
92.59
64.90
94.95
93.48
94.21
73.12
96.24
96.05
96.15
69.88

95.36
85.52
95.44
76.34
95.19
95.42
95.30
76.47
95.20
95.40
95.30
75.44
97.13
97.05
97.09
81.80

92.87
91.75
92.31
75.40
95.85
96.11
95.98
81.21
97.09
97.43
97.26
83.89

91.85
93.34
92.59
68.67
92.44
93.69
93.06
72.12
92.40
93.39
92.89
70.50
92.02
93.76
92.88
71.60

93.26
93.41
93.33
80.00
93.50
94.58
94.04
76.56
94.53
95.45
94.99
77.69

94.01
94.00
94.00
65.48
94.00
94.15
94.07
65.79
94.13
93.99
94.06
66.35
93.69
93.73
93.71
59.80

91.41
90.64
91.02
74.80
95.56
95.62
95.59
82.14
95.85
96.59
96.22
73.58

91.45
92.22
91.83
56.28
91.86
92.47
92.17
59.11
91.81
92.62
92.21
57.39
91.81
92.43
92.12
55.50

89.30
88.04
88.66
64.00
92.17
92.96
92.57
60.83
94.07
94.17
94.12
69.76

95.02
95.05
95.04
69.46
95.11
95.23
95.17
71.27
94.99
95.37
95.18
69.69
95.02
96.13
95.57
67.30

92.61
92.42
92.51
68.50
96.10
96.13
96.12
77.56
96.88
97.62
97.25
78.69

93.81
92.88
93.97
67.00
93.95
94.33
94.14
68.40
94.03
94.22
94.12
68.04
94.44
95.09
94.76
69.55

92.61
91.93
92.27
72.80
94.74
94.98
94.86
74.34
95.85
96.40
96.12
74.38

Table 3: Results of the proposed model on the test sets of eight CWS datasets. There are two blocks. The ﬁrst block consists of
single-criterion learning models. LSTM and Bi-LSTMs are baselines and the results on them are reported in Chen et al. (2017).
The second block consists of the multi-criteria learning model. Multi-task framework for multi-criterion Chinese word seg-
mentation is proposed by Chen et al. (2017). Here, P, R, F, OOV indicate the precision, recall, F value and OOV recall rate
respectively. The maximum F values are highlighted for each dataset.

(single-criterion learning). By concatenating all datasets, Bi-
LSTMs performs poorly in multi-criteria learning scenario
(the worst). Experimental results show that Switch-LSTMs
outperform both Bi-LSTMs and multi-task learning frame-
work on all the corpora. In average, Switch-LSTMs boost
about +1% (96.12 in F-value) compared to multi-task learn-
ing framework (94.86 in F-value), and boosts +3.85% com-
pared to Bi-LSTMs model (92.27 in F-value).

We could also observe that the performance beneﬁts from
multi-criteria learning, since, in this case, the model could
learn extra helpful information from other corpora. Con-
cretely, in average F-value, Switch-LSTMs for multi-criteria
learning boosts +1.36% (96.12 in F-value) compared to
Switch-LSTMs for single-criterion learning (94.76 in F-
value).

Model Selection

Figure 4 shows the relationship between switch number and
performance in the multi-criteria learning scenario. As we
can see, models with more than 2 switches are better than
1-switch-LSTM with a considerable margin, and the case
with 4-way switches is slightly better than other settings. So

we employ 4-way Switch-LSTMs for the following experi-
ments. 1-way Switch-LSTMs are the traditional LSTM. So,
LSTM could be viewed as a special case of the proposed
Switch-LSTMs.

Scale of Parameter Set

Table 4 gives the results of multi-task framework and
Switch-LSTMs on the test sets of eight datasets for multi-
criteria learning. For 8 datasets, the multi-task framework
contains 8 private Bi-LSTMs and 1 shared Bi-LSTMs,
whereas Switch-LSTMs do not have any private parame-
ters, consisting of K LSTM cells associated with one switch
for control. As we could observe, the parameter set size of
multi-task framework is 25K, while the parameter set size of
Switch-LSTMs ranges from 4K to 36K with respect to var-
ious number of switches. However, as mentioned, Switch-
LSTMs perform great when we have more than 2 switches.
Concretely, 2-way Switch-LSTMs obtain 95.53 on F-value
averagely, outperforming the multi-task framework (94.86
on F-value). But the parameter set size of 2-way Switch-
LSTMs is only 7K. Therefore, Switch-LSTMs could out-
perform multi-task framework with fewer parameters.

Models
Multi-Task Framework

Switch-LSTMs
# of switches

# of Param. MSRA
95.82
93.91
95.74
96.70
96.71
96.47
96.52
96.44
96.38
96.46
96.88

25K
4K
7K
11K
15K
18K
22K
25K
29K
33K
36K

AS
94.53
93.80
94.61
94.94
95.03
95.84
94.93
94.80
95.12
94.98
94.80

PKU CTB CKIP CITYU NCC
92.57
94.04
94.21
88.66
93.33
92.59
93.16
95.33
95.82
92.94
94.47
95.70
93.37
95.57
96.22
93.50
95.15
95.89
93.64
95.23
95.79
93.47
95.44
96.23
93.30
94.84
96.22
93.56
94.66
95.91
93.18
94.63
96.00

95.59
91.02
95.48
95.04
95.38
95.33
94.50
95.15
95.31
95.27
94.82

95.98
92.31
97.17
97.21
97.55
97.63
97.54
97.60
97.51
97.47
97.80

SXU Avg.
94.86
96.12
92.51
92.27
96.91
95.53
95.45
96.57
95.84
96.91
96.73
95.82
95.50
95.85
96.96
95.76
95.63
96.39
97.25
95.69
95.55
96.29

1
2
3
4
5
6
7
8
9
10

Table 4: Results of multi-task framework and Switch-LSTMs with various numbers of switches on the test sets of eight datasets
on multi-criteria learning scenario. Only F-values are reported. # of parameters only takes parameters of the model into account
(input embedding matrix E is not included).

Visualization
To better understand how switches affect and contribute to
the performance, we visualize the statuses of switches for
eight datasets as shown in Figure 3. Concretely, each switch
gives a distribution over k-way LSTM on all step t for all
the instances. Thus, we calculate the global switch distribu-
tion by a normalization operation for each dataset. As we
observe, switch distributions are diverse, and datasets with
similar criteria obtain similar switch distributions. Due to
space limitation, we only plot the switch statuses of forward-
ing Switch-LSTMs.

Effects of Switch
To evaluate how the inputs of switch affect the performance,
we do some input ablation experiments. As shown in Ta-
ble 5, all the inputs of switch contribute to the model per-

MSRA

CKIP

AS

CITYU

PKU

NCC

CTB

SXU

F-value(%)

96

94

92

90

Prob

0.5

0.4

0.3

0.2

0.1

1.5

2

2.5

3

3.5

4

k-way

Figure 3: Distributions of switch statuses of forward Switch-
LSTMs on test sets of eight datasets for multi-criteria learn-
ing. X-axis denotes every individual switch state, and Y-axis
is the corresponding probability that state occurs.

MSRA

PKU

CKIP

NCC

Avg

AS

CTB

CITYU

SXU

# of switches

2

4

6

8

10

Figure 4: Results (on F-value) of Switch-LSTMs with var-
ious numbers of switches on development sets of eight
datasets on multi-criteria learning scenario. X-axis denotes
the # of switches that the model employs, and Y-axis reports
the corresponding results.

formance, and the task embedding em is the most effective
part. Concretely, without task embedding em, the average
F-value over eight datasets drops by 3.33% compared to the
full model (95.84 on F-value). Besides, states st are also cru-
cial to switch. The performance further drops by 0.81% in
average F-value (from 92.51 to 91.70) by additionally ablat-
ing state information st.

We also tried two switch strategies to see how the switch
affect the performance: randomly switch in the training
phase and randomly switch in the testing phase. Former one
always randomly picks up a switch status at each time step
during training, and when at test phase, a normal switch is
employed. The later one train the switch in a normal way
but randomly picks up a switch status at each time step at the
testing phase. By randomly pick up a switch at each time, we
could test that if the model works well without the switch.
As we observe, random strategies on switch lead to poor
performance (performance drops by 3% on F-value). It im-

Models
w/o states st
w/o input embeddings exi
w/o task embedding em
w/o exi & em
w/o st & em
Randomly switch in training
Randomly switch in testing
All (Full model)

MSRA
95.68
96.28
93.36
93.26
93.39
92.64
85.56
96.71

AS
95.03
94.82
93.49
93.50
91.97
92.90
93.63
95.03

PKU
95.71
95.98
92.90
92.60
91.66
92.84
90.16
96.22

CTB
97.60
97.18
92.65
93.31
92.51
93.51
97.16
97.55

CKIP CITYU NCC
93.34
94.84
94.73
93.46
95.14
94.91
89.25
91.61
93.81
88.74
91.74
94.23
88.43
91.22
92.09
89.45
91.63
94.20
85.56
93.22
93.53
95.38
95.57
93.37

SXU
96.58
96.54
93.02
93.10
92.33
93.05
91.49
96.91

Avg.
95.44
95.54
92.51
92.56
91.70
92.53
91.29
95.84

Table 5: Results of the proposed model with different switch conﬁgurations on test sets of eight CWS datasets for multi-criteria
learning. Only F-values are reported. Term “w/o” denotes “without”. The maximum F values are highlighted for each dataset.

300

100

# of instances Models MSRA
75.71
89.08
81.50
88.99
83.52
89.17
83.55
89.41
85.75
89.04

Single
Transfer
Single
Transfer
Single
Transfer
Single
Transfer
Single
Transfer

1000

500

700

AS
68.51
91.93
72.77
92.05
75.29
92.23
76.29
92.10
78.78
92.35

PKU
78.87
91.50
83.95
91.97
86.02
91.90
87.70
91.74
88.75
92.51

CTB
79.65
92.08
88.46
91.92
90.33
94.94
91.59
95.49
91.82
95.39

CKIP CITYU NCC
71.75
77.18
65.59
88.97
93.62
94.02
78.20
82.12
73.76
89.94
93.43
94.52
79.89
86.43
75.12
89.98
93.72
94.61
81.72
87.97
77.76
93.87
90.12
94.49
82.52
88.70
78.51
90.45
94.64
93.82

SXU
81.29
93.34
85.97
94.03
87.12
94.19
88.33
93.67
90.09
93.85

Avg.
74.82
91.82
80.84
92.11
82.97
92.59
75.25
92.61
76.30
92.76

Table 6: Results of transfer learning of the proposed model on test sets of eight CWS datasets for single-criteria learning. Only
F-values are reported. “# of instances” denotes how many instances involved for training. Single model is the conventional
Bi-LSTM model. “Transfer” denotes the proposed Switch-LSTMs by ﬁxing all the parameters learned from other 7 datasets
except the new involved task embedding.

plies that a normal switch is crucial to our model, and switch
mechanism contributes a lot in boosting performance.

Knowledge Transfer
Switch-LSTMs could also be easily transferred to other new
datasets. To evaluate the transfer capability of our model,
we leave one dataset out and train Switch-LSTMs on other 7
datasets. Then, we ﬁxed all the parameters except the newly
introduced task embedding when training on instances of the
leave out dataset. As shown in Table 6, Switch-LSTMs could
obtain excellent performance when only 100, 300. 500, 700,
1000 training instances are available. The single model (con-
ventional LSTM) cannot learn from such few instances. For
instance, when we train with 1000 training examples, the
single model only obtains 76.30 in average F-value, whereas
Switch-LSTMs could obtain 92.76 (boosts 16.46 averagely).
It shows that Switch-LSTMs could adapt to a new crite-
rion by only learning a new task embedding, and the newly
learned task embedding leads to a new switch strategy for
the new criterion.

Related Work
It is a common practice for utilizing annotated data from dif-
ferent but similar domain to boost the performance for each
task. Many efforts have been made to better utilizing the ho-
mogeneous factor in various tasks to help improve multiple
tasks especially those barren tasks with few examples.

Recently, some efforts have been made to transfer knowl-
edge between NLP Tasks. Zoph and Knight; Johnson et
al. (2016; 2017) have been jointly training translation mod-
els from and to different languages, it is achieved simply by
jointly train encoder or both encoder and decoder. (Jiang,
Huang, and Liu 2009; Sun and Wan 2012; Qiu, Zhao, and
Huang 2013; Li et al. 2015; 2016; Chen, Zhang, and Liu
2016) adopted the stack-based model to take advantage of
annotated data from multiple sources, and show that tasks
can indeed help improve each other.

Chen, Zhang, and Liu (2016) adopted two neural mod-
els based on stacking framework and multi-view frame-
work respectively, which boosts POS-tagging performance
by utilizing corpora in heterogeneous annotations. Chen et
al. (2017) have proposed a multi-criteria learning framework
for CWS. Using a similar framework as in Caruana (1997),
there are private layers for each task to extract criteria-
speciﬁc features, and a shared layer for the purpose of trans-
ferring information between tasks, to avoid negative transfer,
they pose an adversarial loss on the shared layer to impose
source indistinguishability thus make it criteria-invariant.

Conclusions

In this paper, we propose a ﬂexible model, called Switch-
LSTMs, for multi-criteria CWS, which can improve the per-
formance of every single criterion by fully exploiting the un-
derlying shared sub-criteria across multiple heterogeneous

criteria. Experiments on eight corpora show the effective-
ness of our proposed model. In future works, we are plan-
ning to experiment Switch-LSTMs on other multi-task prob-
lems such as transferring information between different doc-
ument categorization datasets and further investigate a dis-
crete version of Switch-LSTM via reinforcement learning.

Acknowledgments
We would like to thank the anonymous reviewers for
their valuable comments. The research work is supported
by Shanghai Municipal Science and Technology Commis-
sion (No. 17JC1404100 and 16JC1420401), and National
Natural Science Foundation of China (No. 61672162 and
61751201).

References
Cai, D., and Zhao, H. 2016. Neural word segmentation
learning for chinese. In Proceedings of Annual Meeting of
the Association for Computational Linguistics, ACL.
Caruana, R. 1997. Multitask learning. Machine learning.
Chen, X.; Qiu, X.; Zhu, C.; and Huang, X. 2015a. Gated
recursive neural network for chinese word segmentation. In
Proceedings of Annual Meeting of the Association for Com-
putational Linguistics, ACL.
Chen, X.; Qiu, X.; Zhu, C.; Liu, P.; and Huang, X. 2015b.
Long short-term memory neural networks for chinese word
segmentation. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, EMNLP.
Chen, X.; Shi, Z.; Qiu, X.; and Huang, X. 2017. Adversar-
ial multi-criteria learning for chinese word segmentation. In
Proceedings of Annual Meeting of the Association for Com-
putational Linguistics, ACL.
Chen, H.; Zhang, Y.; and Liu, Q. 2016. Neural network
for heterogeneous annotations. Proceedings of the Confer-
ence on Empirical Methods in Natural Language Process-
ing, EMNLP.
Emerson, T. 2005. The second international chinese word
segmentation bakeoff. In Proceedings of the fourth SIGHAN
workshop on Chinese language Processing.
Fei, X. 2000. The part-of-speech tagging guidelines for the
penn chinese treebank (3.0). URL: http://www. cis. upenn.
edu/˜ chinese/segguide. 3rd. ch. pdf.
Gong, C.; Li, Z.; Zhang, M.; and Jiang, X. 2017. Multi-
In Proceedings of
grained chinese word segmentation.
the Conference on Empirical Methods in Natural Language
Processing, EMNLP.
Hochreiter, S., and Schmidhuber, J. 1997. Long short-term
memory. Neural computation.
Jiang, W.; Huang, L.; and Liu, Q. 2009. Automatic adap-
tation of annotation standards: Chinese word segmentation
and POS tagging: a case study. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Language
Processing.
Jin, G., and Chen, X. 2008. The fourth international chinese
language processing bakeoff: Chinese word segmentation,

named entity recognition and chinese pos tagging. In Sixth
SIGHAN Workshop on Chinese Language Processing.
Johnson, M.; Schuster, M.; Le, Q. V.; Krikun, M.; Wu, Y.;
Chen, Z.; Thorat, N.; Vi´egas, F. B.; Wattenberg, M.; Cor-
rado, G.; Hughes, M.; and Dean, J. 2017. Google’s multilin-
gual neural machine translation system: Enabling zero-shot
translation. TACL.
Lafferty, J. D.; McCallum, A.; and Pereira, F. C. N. 2001.
Conditional random ﬁelds: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of the
Eighteenth International Conference on Machine Learning.
Li, Z.; Chao, J.; Zhang, M.; and Chen, W. 2015. Coupled se-
quence labeling on heterogeneous annotations: Pos tagging
as a case study. In Proceedings of the 53rd Annual Meeting
of the Association for Computational Linguistics and the 7th
International Joint Conference on Natural Language Pro-
cessing.
Li, Z.; Chao, J.; Zhang, M.; and Yang, J. 2016. Fast coupled
sequence labeling on heterogeneous annotations via context-
aware pruning. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, EMNLP.
Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and
Dean, J. 2013. Distributed representations of words and
phrases and their compositionality. In Advances in Neural
Information Processing Systems.
Pei, W.; Ge, T.; and Baobao, C. 2014. Maxmargin ten-
sor neural network for chinese word segmentation. In Pro-
ceedings of Annual Meeting of the Association for Compu-
tational Linguistics, ACL.
Qiu, X.; Zhao, J.; and Huang, X.
Joint chinese
word segmentation and pos tagging on heterogeneous anno-
tated corpora with multiple task learning. In Proceedings of
the Conference on Empirical Methods in Natural Language
Processing, EMNLP.
Sun, W., and Wan, X. 2012. Reducing approximation and
estimation errors for chinese lexical processing with hetero-
geneous annotations. In Proceedings of Annual Meeting of
the Association for Computational Linguistics, ACL.
Yao, Y., and Huang, Z. 2016. Bi-directional lstm recurrent
neural network for chinese word segmentation. In Interna-
tional Conference on Neural Information Processing.
Yu, S.; Lu, J.; Zhu, X.; Duan, H.; Kang, S.; Sun, H.; Wang,
H.; Zhao, Q.; and Zhan, W. 2001. Processing norms of
modern Chinese corpus. Technical report, Technical report.
Zhang, M.; Zhang, Y.; and Fu, G. 2016. Transition-based
neural word segmentation. Proceedings of Annual Meeting
of the Association for Computational Linguistics, ACL.
Zheng, X.; Chen, H.; and Xu, T. 2013. Deep learning for chi-
nese word segmentation and pos tagging. In Proceedings of
the Conference on Empirical Methods in Natural Language
Processing, EMNLP.
Zoph, B., and Knight, K. 2016. Multi-source neural trans-
lation. In NAACL HLT 2016, The 2016 Conference of the
North American Chapter of the Association for Compu-
tational Linguistics: Human Language Technologies, San
Diego California, USA, June 12-17, 2016.

2013.

