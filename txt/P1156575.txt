On First-Order Bounds, Variance and Gap-Dependent Bounds for
Adversarial Bandits

9
1
0
2
 
l
u
J
 
4
2
 
 
]

G
L
.
s
c
[
 
 
3
v
0
9
8
7
0
.
3
0
9
1
:
v
i
X
r
a

Roman Pogodin
Gatsby Computational Neuroscience Unit
University College London, London, UK
roman.pogodin.17@ucl.ac.uk

Tor Lattimore
DeepMind
London, UK
tor.lattimore@gmail.com

Abstract

We make three contributions to the theory
of k-armed adversarial bandits.
First, we
prove a ﬁrst-order bound for a modiﬁed variant
of the INF strategy by Audibert and Bubeck
[2009], without sacriﬁcing worst case opti-
mality or modifying the loss estimators.
Second, we provide a variance analysis
for algorithms based on follow the regu-
larised leader, showing that without adapta-
tion the variance of the regret is typically
Ω(n2) where n is the horizon.
Finally,
we study bounds that depend on the degree
of separation of the arms, generalising the
results by Cowan and Katehakis [2015] from
the stochastic setting to the adversarial and
improving the result of Seldin and Slivkins
[2014] by a factor of log(n)/ log log(n).

1 INTRODUCTION

The k-armed adversarial bandit is a sequential game
played over n rounds. At the start of the game the
adversary secretly chooses a sequence of losses (ℓt)n
t=1
[0, 1]k. In each round t the learner chooses
with ℓt ∈
a distribution Pt over the actions [k] =
.
}
[k] is sampled from Pt and the learner
An action At ∈
observes the loss ℓtAt. Like prior work we focus on
controlling the regret, which is

1, 2, . . . , k

{

n

ˆRn = max
[k]
i
∈

(ℓtAt −

ℓti) .

t=1
X
This quantity is a random variable, so the standard
objective is to bound ˆRn with high probability or its
expectation: Rn = E[ ˆRn].

We make three contributions, with the common objective
of furthering our understanding of the application of

follow the regularised leader (FTRL) to adversarial
bandit problems. Our ﬁrst contribution is a modiﬁcation
the INF policy by Audibert and Bubeck [2009]
of
in terms
in order to prove ﬁrst-order bounds (i.e.
of the loss of the best action) without sacriﬁcing
minimax optimality.
Then we turn our attention
to the variance of algorithms based on FTRL. Here
we prove that using the standard importance-weighted
estimators and a large class of potentials leads to a
variance of Ω(n2), which is the worst possible for
bounded losses. Finally, we investigate the asymptotic
performance of algorithms when there is a linear
separation between the losses of the arms. We improve
the result by Seldin and Slivkins [2014] by a factor of
log(n)/ log log(n) and generalise known results in the
stochastic setting by Cowan and Katehakis [2015] to the
adversarial one by constructing an algorithm for which
the regret grows arbitrarily slowly almost surely.

[2019]

Related work The literature on adversarial bandits is
enormous. See the books by Bubeck and Cesa-Bianchi
[2012] and Lattimore and Szepesv´ari
for a
comprehensive account. The common thread in the
three components of our analysis is adaptivity for
algorithms based on follow the regularised leader.
The INF policy that underlies much of our analysis
was introduced by Audibert and Bubeck [2009]. The
connection to mirror descent and follow the regu-
larised leader came later [Audibert and Bubeck, 2010,
Bubeck and Cesa-Bianchi, 2012], which greatly simpli-
ﬁed the analysis. The principle justiﬁcation for intro-
ducing this algorithm was to prove bounds on the
minimax regret. Remarkably, it was recently shown that
by introducing a non-adaptive decaying learning rate, the
algorithm retains minimax optimality while simultane-
ously achieving a near-optimal logarithmic regret in the
stochastic setting [Zimmert and Seldin, 2019]. Despite
its simplicity, the algorithm improves on the state-of-
the-art for this problem Bubeck and Slivkins [2012],
Seldin and Slivkins [2014], Seldin and Lugosi [2017].

p

See also the extension to the combinatorial semibandit
setting [Zimmert et al., 2019]. First-order bounds for
bandits were ﬁrst given by Allenberg et al. [2006],
who analysed a modiﬁcation of Exp3 [Auer et al.,
1995]. As far as we know, previous algorithms with
ﬁrst order bounds have not been minimax optimal
(Rn = O(√kn)):
the recent work by Neu [2015b]
kn(log(k) + 1)) expected regret, and
achieved O(
[Wei and Luo, 2018] had a O(√kn log n) bound. Both
papers used the idea of an adaptive learning rate similar
to our analysis. In the setting of gains rather than losses
Audibert and Bubeck [2010] have shown that by intro-
ducing biased estimators it is possible to prove a bound
of O(√kG∗) where G∗ is the maximum gain. Although
it is not obvious, we suspect the same idea could be
applied in our setting. We ﬁnd it interesting nevertheless
that the same affect is possible without modifying the
loss estimators. The aforementioned work also assumes
knowledge of G∗. Possibly our adaptive learning rates
could be used to make this algorithm anytime without a
doubling trick.

Although it is well known that straightforward appli-
cations of
follow the regularised leader or mirror
descent with importance-weighted estimators leads to
poor concentration of the regret, we suspect the severity
of the situation is not widely appreciated. As far as
the quadratic variance of Exp3 was only
we know,
derived recently [Lattimore and Szepesv´ari, 2019,
11].
There are, however, a number of works modifying the
importance-weighted estimators to prove high proba-
bility bounds Auer et al. [1995], Abernethy and Rakhlin
[2009], Neu [2015a] with matching lower bounds
by Gerchinovitz and Lattimore [2016].
Finally, we
note there are many kinds of adaptivity beyond ﬁrst-
order bounds.
For example sparsity and variance
[Bubeck et al., 2018, Hazan and Kale, 2011, and others].

§

2 NOTATION

×

k

}

∈

∈

Rd

1 =

Rd let diag (x)

k1 = 1
x

has domain dom(F ) =

1)-dimensional probability simplex is ∆d

−
. A convex function F : Rd
Rd : F (x)
x

d be the
Given a vector x
diagonal matrix with x along the diagonal. The interior
of a topological space X is interior(X) and its boundary
is ∂X. The standard basis vectors are e1, . . . , ed. The
(d
−
[0, 1]d :

∈
∪
.
∞}
{∞}
The Bregman divergence with respect to a differentiable
F is a function DF : dom(F )
]
∞
×
deﬁned by DF (x, y) = F (x)
.
y
F (y)
−
i
The Fenchel dual of F is F ∗ : Rd
deﬁned
by F ∗(u) = supx
i −
There are k arms and the horizon is n, which may or
may not be known. The losses are (ℓt)n
t=1 with ℓt ∈

x
{
R
→
=

→
F (y), x

dom(F )

x, u
h

∪ {∞}

− h∇
R

F (x).

[0,

→

−

∈

Rd

{

∈

}

P

P

t
s=1

At = i
{

t
[0, 1]k. We let Lt =
s=1 ℓs. The importance-weighted
estimator of ℓt is ˆℓt deﬁned by ˆℓti = 1
ℓti/Pti.
All algorithms proposed here ensure that Pti > 0 for
all t and i, so this quantity is always well deﬁned. Let
ˆℓs. Expectations are with respect to the
ˆLt =
randomness in the actions (At)n
t=1. Of course the learner
can only choose Pt based on information available at
the start of round t. Let
Ft = σ(A1, . . . , At). Then
and
Pt is
At = i
Ft
{
−
t
s=1 Asi be the number of times arm i is
Ti(t) =
played in the ﬁrst t rounds. Our standing assumption
is that the ﬁrst arm is optimal. All our algorithms are
symmetric, so this is purely for notational convenience.
Assumption 2.1. Lt1 = mini
∈

1-measurable. Let Ati = 1

[k] Lti.

P

}

3 FOLLOW THE REGULARISED

LEADER

Follow the regularized leader (FTRL) is a popular tool
for online optimization [Shalev-Shwartz, 2007, Hazan,
2016]. The basic algorithm depends on a sequence of
potential functions (Ft)∞t=1 where Ft : Rd
is convex and dom(Ft)
=
algorithm chooses the distribution

R
∪ {∞}
. In each round the

∆k

→

∩

∅

−

1

Pt = arg min
∆k−1 h
p

p, ˆLt
−

1i

∈

+ Ft(p) ,

[k] is sampled
which we assume exists. The action At ∈
from Pt. In many applications Ft = F is chosen in a
time independent way, with examples given in Table 1.
This has the disadvantage that F must be chosen in
advance in a way that depends on the horizon, which
may be unknown. This weakness can be overcome
by choosing Ft = F/ηt where (ηt)∞t=1 is a sequence
of learning rates, which may be chosen in advance or
adaptively in a data-dependent way.

A modiﬁcation that will prove useful is to let (
a sequence of subsets of ∆k

1 and deﬁne

−

At)∞t=1 be

Pt = arg min
t h
p

p, ˆLt
−

1i

∈A

+ Ft(p) .

−

The restriction to a subset of ∆k
1 can be useful to
control the gradients of Ft(Pt), which is sometimes
crucial. The following theorem provides a generic bound
for FTRL with changing potentials and constraint sets.
The result is reminiscent of many previous bounds for
FTRL, but a reference for this result seems elusive. Most
related is the generic analysis by Joulani et al. [2017],
which also provides the most comprehensive literature
summary.

Theorem 3.1. Assume
and (Ft)n+1

1
−
t=1 is a sequence of convex functions with

A1 ⊆ · · · ⊆ An+1 ⊆

∆k

dt = max
y

t+1

∈A

x

min
x
∈A

tk

y

k1 ,

−

gt = sup
x
∈A

tk∇

Ft(x)

k∞

We can rewrite the Φ-differences as

t=1
X

Potential

Negentropy

1/2-Tsallis

Log barrier

Deﬁnition
η Pk

1

2

i=1 pi(log(pi)
i=1 √pi
i=1 log(pi)

η Pk
η Pk

1

−

−

Alg.

1)

Exp3

−

INF

Table 1: Common potential functions

dom(Ft)

=

for all t. Deﬁne

∩ At 6

∅

and

vn =

dt(gt + (t

1)) .

−

n

t=1
X

Then the regret of FTRL is bounded by

Rn ≤

vn + E

Pt+1, ˆℓt

Pt −

DFt (Pt+1, Pt)

#

therefore

−

E

n

"

t=1 D
X

min
∈A
n

n+1

p

(cid:20)

+ E

+ E

"

t=1
X

(Fn+1(p) + n

p

k

−

e1k1)

−

F1(P1)
(cid:21)

(Ft(Pt+1)

Ft+1(Pt+1))

.

−

#

Proof. Let p

∈ An+1. Using the fact that ˆℓt is unbiased,

n

"

t=1
X
n

"

t=1
X

Rn = E

Pt −
h

e1, ˆℓti#

= E

Pt −
h

p, ˆℓti#

+ E

p
h

−

e1, ℓti#

.

n

"

t=1
X

The second sum is the approximation error, and by
Holder’s inequality,

p
h

e1, ℓti ≤ k

p

−

e1k1

−

ℓtk∞ ≤
k

n

p

k

e1k1 .

−

n

t=1
X

n

"

t=1
X
+ n

Pt −
h
p

k

−

Pt+1, ˆℓti
e1k1 .

n

+

t=1
X

Pt+1 −
h

p, ˆℓti#

Let Φt(q) = Ft(q) +
that Pt = arg minq

t
1
−
s=1h

q, ˆℓsi

, which is chosen so
t Φt(q). Then the second sum in
P

∈A

n

t=1
X

Therefore,

E

Rn ≤

(Φt+1(Pt+1)

Φt(Pt+1)

Ft+1(Pt+1) + Ft(Pt+1))

−

−

the above display equals

n

t=1
X

n

t=1
X

Φn+1(p) + Fn+1(p)

−

=

(Φt(Pt)

Φt(Pt+1))

−

+ Φn+1(Pn+1)
−
n

Φ1(P1)

Φn+1(p)

−

+ Fn+1(p) +

(Ft(Pt+1)

Ft+1(Pt+1))

−

−
−

Φt(Pt+1)
DΦt (Pt+1, Pt)

Φt(Pt)
=
Let δt = Pt+1 −
arg minq
ﬁrst-order optimality condition for Pt on
At,
]
Pti
≥

Φt(Pt), (Pt+1 −

− h∇
q

E [

δt)

h∇

t k

∈A

−

Φt(Pt), Pt+1 −
−

.
Pt+1k1. Then due to

Pti

0,

E [

Φt(Pt), Pt+1 −

]
Pti

h∇

≥

E

≥

"h∇

Ft(Pt), δti

+

E [
h∇
1
t
−

Φt(Pt), δti
]
ˆℓs, δti#

h

s=1
X

E

δtk1

"k

≥ −

Ft(Pt)

+

k∞

ℓsk∞!#
k

1

t
−

s=1
X

 k∇
1
t
−

dtgt −

dt

≥ −

ℓsk∞ ≥ −

k

dt(gt + (t

1)) ,

−

s=1
X
where we used Holder’s inequality, the deﬁnitions of dt
and gt, non-negativity of ˆℓs and that E ˆℓs = ℓs ∈
[0, 1].
It follows that

dt(gt + k(t

Φt(Pt+1)
−
∈ An+1 and Pn+1 is the minimiser of Φn+1 in

DΦt (Pt+1, Pt) .

1))

−

−

≤

Φt(Pt)

Since p
An+1, we have

Φn+1(Pn+1)

Φn+1(p)

0 .

−
Finally, noting that Φ1 = F1 and DΦt (Pt+1, Pt) =
DFt (Pt+1, Pt) we obtain

≤

n

Rn ≤

n

p

k

−

e1k1 +

dt(gt + (t

1))

−

t=1
X
Pt+1, ˆℓti −
F1(P1)]

n

"

+ E

Pt −
h
t=1
X
+ E [Fn+1(p)
n

−

DFt(Pt+1, Pt)

#

+ E

"

(Ft(Pt+1)

Ft+1(Pt+1))

,

−

#

t=1
X
from which the statement follows.

4 FIRST ORDER BOUNDS

1
ηt

k

i=1
X

η0

We now introduce the modiﬁcation of the INF strategy,
which takes inspiration from Wei and Luo [2018],
Zimmert and Seldin [2019], Zimmert et al. [2019]. The
new algorithm plays on the ‘chopped’ simplex, with the
magnitude of the cut dependent on the round,
At = ∆k

[1/t, 1]k .

(1)

∩

−

1

Then for a convex potential ft(p) with dom(ft)k
∆k

deﬁne a potential

=

1

−

∩

∅

Ft(p) =

ft(pi) ,

(2)

where the learning rate ηt is given by

ηt =

1 +

q

1
t
−
s=1

ˆℓ2
sAs(

∇

2(fs)(PsAs ))−

1

,

(3)

where η0 is positive constant to be tuned later.

P

The Hessian of the potential plays a fundamental role in
the regret, simplifying the derivation of a generic ﬁrst-
order bound:
Theorem 4.1. Suppose that
and there exist B, C

2ft is decreasing on (0, 1)

∇
0 such that

1
2ft(p) ≤

p2

≥
B, E

1
2ft(PtAt )

C ,

≤

P 2

(cid:20)

tAt ∇
∇
[n]. Assume additionally
for all p
∈
that there exist a non-negative constant h1 and a non-
negative function h2(n) such that

(0, 1) and t

∈

(cid:21)

vn + min
p

n+1

(Fn+1(p) +

p

k

−

e1k1n)

−

F1(P1)

+

(Ft(Pt+1)

Ft+1(Pt+1))

+ h2(n) ,

−

h1
ηn+1

≤

∈A
n

t=1
X

almost surely. Then the expected regret of FTRL with
η0 = √h1/21/4 simultaneously satisﬁes

h1
√2 (cid:19)

,

Rn ≤

× s

h1

√h1
25/4 B + h2(n) + 2√2Bh1 + 27/4
p
√h1
Bh2(n)
29/4 +
2
Cn
2

√h1
25/4 B + h2(n) + 27/4

BLn1
2

+ B2

1 +

1 +

h1

+

(cid:18)

.

r

p

Rn ≤
Remark 4.2. h1 and h2(n) reﬂect the approximation
error, non-stationarity of the potential ft and how
At. In a simple case with
sensitive it is to the changes in
, ft = f for all t, this is a standard bound for the
At =
sum of the potential differences. h1 can be a function
of n when the horizon n is known, as we choose the
learning rate based on it.

A

As an application of this general ﬁrst-order result, we
derive a worst-case optimal bound for a carefully chosen
mixture of the INF regularizer and the log-barrier:
Corollary 4.3. For η0 = k1/4

and

13
3√2

+ 3
√2q

ft(p) =

2√p

−

q

log p

−

√k log1+q max
{

3, t

}

and any q > 0 and n

3, the regret grows with n as

≥

Rn = O

kLn1 log1+q(n) + k2 log2(1+q) n + k log1+q(n)

,

(cid:18)q

with some constants proportional to 1/q.

(cid:19)

Corollary 4.4. For q = 1, η0 = k1/4

22/(3√2),

19k2 + 22k log2(n) + 2k log(n) + 6.5 log(n)

q

kLn1 + 19k3 + 2k2 log(n) + 11.2k2 log2(n) .

In the worst-case scenario the regret satisﬁes

Rn ≤

×

q

lim sup
n

→∞

Rn
√kn ≤

9.2.

Corollary 4.5. If the horizon n
using

1

At = ∆k

−

∩

≥

[1/n, 1]k, η0 = k1/4√3/21/4 and
log p
√k log n

2√p

−

−

ft(p) =

3 is known in advance,

results in
Rn ≤

k + 9.1k log n

+ 4.2

kLn1 log(n) + 2√k + 6k2 log2(n) ,

q

lim sup
n

→∞

Rn
√kn ≤

5.9 .

The proof of the last corollary simply repeats previous
statements, also using the stationarity of the constraint
set and ft(p). See Appendix B for more details.
Remark 4.6. Theorem 4.1 with known n reproduces
the result of [Wei and Luo, 2018] (note that they used
a slightly different algorithm and the learning rate
schedule): for the log-barrier potential ft(p) =
log p
k log n, such that the
we have B = C = 1 and h1(n)
worst-case regret is Rn = O(√kn log n).

−

∝

The proof of Theorem 4.1 follows from Theorem 3.1 and
the following lemmas:
Lemma 4.7. For a potential of the form Eq. (2) with
(0, 1),

2ft(p) that is monotonically decreasing on p

∇

n

t=1 D
X

Pt+1, ˆℓt

Pt −

−

E

∈

DFt(Pt+1, Pt)

n

ηt
2

≤

t=1
X

ℓ2
tAt
2ft(PtAt )

.

P 2

tAt ∇

Proof. Let t
[n] and suppose that Pt+1,At > PtAt .
Then using the fact that the loss estimators and Bregman
divergence are non-negative,

∈

The ﬁrst term in the last line is proportional to 1/ηn+1,
therefore using the deﬁnition of ηt, Jensen’s inequality
and ℓ2
ℓtAt, the regret can be bounded as

tAt ≤

Pt −
h

Pt+1, ˆℓti −
= (PtAt −
Now suppose that Pt+1,At ≤
2019, Theorem 26.5],

DFt (Pt+1, Pt)
Pt+1,At )ˆℓtAt ≤

Pt −

Pt+1, ˆℓti

≤ h
0 .

PtAt . By [Lattimore and Szepesv´ari,

Pt+1, ˆℓti −
Pt −
h
where z = αPt + (1
By deﬁnition
ˆℓti = 0 for i

∇
= At,

DFt (Pt+1, Pt)

1
2 k

2
(

ˆℓtk

∇

≤

2Ft(z))−1 ,

−
2Ft(z) = diag (

α)Pt+1 for some α

[0, 1].
2ft(z))/ηt and since

∈

∇

1
2 k

ˆℓtk

2

∇

2Ft(z)−1 =

tAt

ηt ˆℓ2
2ft(zAt ) ≤

2

ηt ˆℓ2
tAt
2ft(PtAt )

,

2

∇

2ft(p)
where we used the fact that zAt ≤
is decreasing. The result follows by substituting the
deﬁnition of ˆℓtAt and summing over t

[n].

∇

∇
PtAt and that

Lemma 4.8. Let (xt)n
for all t. Then

t=1 be a sequence with xt ∈

[0, B]

n

t=1
X

1 +

q

t
1
s=1 xs
−

≤

v
u
u
t

xt

P

4

1 +

xt + B .

The proof follows from a comparison to an integral and
is given in Appendix A.

∈

1
2

n

t=1
X

Proof of Theorem 4.1. Using the result of Theorem 3.1,
Lemma 4.7 and the assumption on the difference in the
potentials, we have

Rn ≤

h2(n) + E

h1
ηn+1

"

+

ηt
2

n

t=1
X

ℓ2
tAt
2ft(PtAt ) #

.

P 2

tAt ∇

As ℓtAt ≤

1, we can apply Lemma 4.8 with

xt =

P 2

tAt ∇

ℓ2
tAt
2ft(PtAt ) ≤

B .

It follows that ηt = η0/

1 +

1

t
s=1 xs, and thus
−

q

P

ℓ2
tAt
2ft(PtAt ) ≤

ηt
2

n

t=1
X

P 2

tAt ∇

n

1 +

1
2

P 2

tAt ∇

t=1
X

2η0v
u
u
t

ℓ2
tAt
2ft(PtAt )

+

B .

η0
2

Rn ≤

h2(n) +

B +

√2h1
η0

 

+ 2η0

!

1 +

× v
u
u
t

n

P 2

tAt ∇

"

t=1
X

ℓtAt
2ft(PtAt ) #

.

The ﬁrst bound in the theorem follows from

η0
2

E

1
2

n

E

P 2

tAt ∇

"

t=1
X

BE

≤

"

t=1
X

ℓtAt
2ft(PtAt ) #
n

(ℓtAt −

ℓt1 + ℓt1)

= BRn + BLn1

#

and then from choosing η0 = √h1/21/4 and solving the
resulting quadratic equation with respect to Rn.
For the second bound, we use ltAt ≤
of C, such that

1 and the deﬁnition

n

E

P 2

tAt ∇

"

t=1
X

ℓtAt
2ft(PtAt ) # ≤

Cn
2

.

To prove the corollaries, we need to bound h1, h2(n), B,
and C:

Lemma 4.9. The Hessian of the hybrid potential in
Corollary 4.3 is monotonically decreasing, and for n
3

1
2ft(p) ≤

p2

∇

√k log1+q n, E

Proof. For p

interior(∆k

1),

−

∈

1
2ft(PtAt )

P 2

(cid:20)

tAt ∇

≥

≤

(cid:21)

2√k ,

2ft(p) =

∇

1
2p3/2 +

1

p2√k log1+q max

3, t

{

}

is a decreasing function of p. It follows that for n

3

≥

1
2ft(p) ≤

p2

∇

√k log1+q n .

Moreover,

E

sup

t,Pt

t
∈A

P 2

(cid:20)

tAt ∇

1
2ft(PtAt )

(cid:21)

≤

t,Pt

∆k−1

sup

∈

E

"

2
PtAt #

= 2√k .

p

Lemma 4.10. Under the conditions of Corollary 4.3,

vn ≤

√k
4
ηn+1 (cid:18)
3
3.7√k
+

η0 q

1 + 9k3/2 log1+q(9k3/2)

+

2
q

+

5.5k
η0

(cid:19)

q
1 + 3√k log1+q 3 + 2k log n .

Proof. Due to the chopped simplex and the factorised
potential, we have (recall the deﬁnition in Theorem 3.1
and use Lemma A.1 for the last inequality)

F1(P1) and telescoping
Summing with the INF part of
shows that it contributes at most 2√k/ηn+1 to the sum.
For log-barrier, suppose αt/ηt ≤

αt+1/ηt+1. Then

−

αt
ηt

+

αt+1
ηt+1 (cid:19)

−

(cid:18)

k

i=1
X

log(Pt+1,i)

0 .

≤

Now suppose that αt/ηt > αt+1/ηt+1. For t
Pt+1 ∈ At+1,

3, as

≥

vn =

dt(gt + (t

1))

n

t=1
X

n

≤

≤

t=1
X
n

t=1
X

−

p

∈

p

∈

2k
t2

2k
t2

1
ηt

1
ηt

 

 

sup
[1/t,1] |∇

ft(p)
|

+ (t

1)

−

!

sup
[1/t,1] |∇

ft(p)

|!

+ 2k log n .

For p

[1/t, 1] the gradient is bounded as

∈

ft(p)

|∇

| ≤

1
√p

+

p√k log1+q max

3, t

1

t

}

{
.

√t +

≤

√k log1+q max

3, t

{

}

Therefore, the corresponding sum in vn converges. By
a straightforward calculation (as shown in Lemma A.2),
the Hessian is bounded as in Lemma 4.9),

αt
ηt

+

−

(cid:18)

log(Pt+1,i)

k

αt+1
ηt+1 (cid:19)
αt
ηt −

≤

i=1
X

(cid:18)
1
ηt (cid:18)

αt+1
ηt+1 (cid:19)
log(t + 1)
log1+q(t) −
√k
ηtt log1+q t

.

≤

≤

k log(t + 1)

1
logq(t)

√k

(cid:19)

Summing over t and noting that due to α1 = α2 = α3
the potential is unchanged,

n

−

t=1 (cid:18)
X

αt
ηt

n

k

+

αt+1
ηt+1 (cid:19)
√k
ηtt log1+q t ≤

i=1
X

≤

t=3
X

log wt+1,i

√k
ηn+1q

+

√k
3η3

,

vn ≤

√k
4
ηn+1 (cid:18)
3
3.7√k
+

η0 q

1 + 9k3/2 log1+q(9k3/2)

+

2
q

+

5.5k
η0

(cid:19)

q
1 + 3√k log1+q 3 + 2k log n .

where the last inequality follows from Lemma A.1,
which essentially compares the sum to the integral of
1/(t log1+q t) and uses that 1/ logq t
3. We
can further bound η3 as

1 for t

≤

≥

Lemma 4.11. Under the conditions of Corollary 4.3,

min
∈A

n+1

p

n

(Fn+1(p) +

p

k

−

e1k1n)

−

F1(P1)

+

(Ft(Pt+1)

Ft+1(Pt+1))

t=1
X
√k
ηn+1 (cid:18)

≤

−

1
q

(cid:19)

√k
3η0 q

3 +

+ k +

1 + 3√k log1+q 3 .

Proof. The potential is a mixture of the INF and the log-
2
i log pi with
barrier parts, Ft(p) =
ηt
−
αt = 1/(√k log1+q max

i √pi −
).
3, t
P
}
To control the contribution of the INF term, ﬁrst notice
that the INF part of Fn+1(p) is negative. Moreover,

αt
ηt

P

{

k

2
ηt

+

2
ηt+1 (cid:19)

−

(cid:18)

i=1
X

p

Pt+1,i ≤

2√k

1

ηt+1 −

(cid:18)

1
ηt (cid:19)

.

1
η3 ≤

1
η0 q

1 + 3√k log1+q 3

by using the fact that the Hessian is bounded (see the
proof of Lemma 4.9).

−

Finally, the log-barrier part of
F1(P1) is negative. The
log-barrier part of Fn+1(p) is bounded by √k/ηn+1 as
∈ An+1. Thus,
p
min
∈A

Fn+1(p) + n

e1k1

−

n+1

p

k

p

√k
ηn+1

≤

+ min
p
∈A

n+1

n

p

k

−

e1k1 =

√k
ηn+1

+

kn
n + 1

.

Combining the three bounds and using that kn/(n+1)
k concludes the proof.

≤

Proof of Corollary 4.3. From Lemma 4.9, Lemma 4.10

and Lemma 4.11, we ﬁnd

13
3

+

3
q

,

≥

(cid:19)
1 + 9k3/2 log1+q(9k3/2)

(cid:18)

B = √k log1+q n , C = 2√k , h1 = √k

h2(n) = 2k log n +

5.5.k
η0

q
1 + 3√k log1+q 3 + k .

+

4.1
η0

√k

q

+ 3
Now applying Theorem 4.1 with η0 = k1/4
√2q
completes the proof. Note that in the big-O notation, we
q
only kept the leading terms that grow with n.

13
3√2

Proof of Corollary 4.4. Starting from the end of the
previous proof, choosing q = 1 and upper-bounding the
numerical coefﬁcients, we obtain the corollary.

5 VARIANCE OF THE REGRET

is

just one measure of

The expected regret
the
performance of an algorithm. Algorithms with small
expected regret may suffer
from a large variance.
Since the adversarial model is often motivated on the
grounds of providing robustness, it would be unfortunate
if proposed algorithms suffered from high variance.
Recently, however, it was shown that the variance of
Exp3 without exploration is quadratic in the horizon
11], and a similar
[Lattimore and Szepesv´ari, 2019,
result holds for Thompson sampling in a Bayesian setting
[Bubeck and Sellke, 2019]. Here we generalise these
arguments to prove quadratic variance of the regret for
a class of algorithms based on FTRL with importance-
weighted loss estimators. This is the worst possible result
for bandits with bounded losses. The class of policies
covered by our theorem includes INF and Exp3, but not
FTRL with the log barrier. To keep things simple we
restrict ourselves to algorithms of the form

§

Pt = arg min
∆k−1 h
p

p, ˆLt
−

1i

+

∈

1
ηn

k

i=1
X

f (pi) ,

where f is convex and (ηn)∞n=1 is a sequence of learning
rates. Note that this corresponds to a sequence of
algorithms, each with a ﬁxed learning rate.

Assumption 5.1. The number of actions is k = 2 and f
is Legendre with (0, 1)

dom(f ) and 0

∂ dom(f ).

⊆

∈

is satisﬁed by all
The assumption on the potential
standard potentials for bandits on the probability
simplex, including those in Table 1. It allows us to write
p), which
Pt in a simple form. Let g(p) = f (p) + f (1

−

is convex and Legendre with dom(g) = (0, 1). Given
x

0,

arg min
[0,1]
p

∈

(px + g(p)) =

g∗(

x) ,

∇

−

∇

∇

g)−

1 =

→∞ ∇

∇
limx

→−∞ ∇

g∗ is nondecreasing and limx

where we used the fact that for Legendre functions the
gradient is invertible and (
g∗. That g
is Legendre with dom(g) = (0, 1) also ensures that
g∗(x) = 0 and
g∗(x) = 1. By symmetry, we also have
g∗(0) = 1/2. The point is that by the deﬁnition of
∇
FTRL, Pt1 =
1,2 −
∇
Theorem 5.2. Assume lim supn
anηn) <
for all a > 0. Then for all sufﬁciently large n there
∞
exists a bandit for which P( ˆRn ≥
c, where c > 0
is a constant that depends on the algorithm, but not the
horizon.

g∗(ηn( ˆLt
−

1,1)).
n

ˆLt
−

n/4)

g∗(

→∞

∇

≥

−

Corollary 5.3. Under the same conditions as Theorem 5.2
the variance of the regret is Var[ ˆRn] = Ω(n2).

1/2 for some a > 0.
Examples Suppose ηn = an−
Then the conditions of the theorem are satisﬁed when
f is the negentropy.
g∗ is the sigmoid
In this case
function and the corresponding algorithm is just Exp3.
When f (p) =

2√p and x

0, then

∇

g∗(x) =

∇

1
1
2 

1 +

− s

2√1 + x2
x4

2

−

−

x2

−



≤

4

(cid:0)

,

(cid:1)





−

g∗(

→∞ ∇

a√n)n = 1/a2. In
which satisﬁes lim supn
1/2)
this sense 1/2-Tsallis entropy with ηn = Θ(n−
just barely satisﬁes the conditions. The consequence
is that the minimax optimal INF policy proposed by
Audibert and Bubeck [2009] has quadratic variance. The
log barrier does not satisfy the conditions and we
speculate it is more stable.

Proof of Theorem 5.2. Assume for simplicity that 4 is a
[0, 1/2] be a constant to be tuned
factor of n. Let αn ∈
subsequently and consider a bandit deﬁned by

ℓt1 =

αn
0

(

if t
n/2
≤
otherwise .

ℓt2 =

0
1

(

if t
n/2
≤
otherwise .

g∗(

Clearly the ﬁrst arm is optimal. Let c1 > 0 be a
constant such that for all sufﬁciently large n it holds
that
c1/n, which is guaranteed to exist
by the assumptions in the theorem. Then deﬁne events
. On the event
Ft =
Fn the random regret satisﬁes

≤
As = 2, Ps1 ≤

t
s=n/2+1{
∩

nηn)

c1/n

∇

−

}

ˆRn ≥

n
2 −

αnn

2 ≥

n
4

.

(4)

The theorem follows by proving that P (Fn)
c for all
sufﬁciently large n and constant c > 0. The idea is to
show that the estimated loss for the optimal arm after the
ﬁrst n/2 rounds is large enough that the algorithm never
plays the optimal arm in the second half of the game with
constant probability.

≥

First half dynamics The choice of αn determines the
dynamics of the interaction between the algorithm and
environment in the ﬁrst n/2 rounds. Before the main
proof we establish some facts about this. Let α
[0, 1/2]
and deﬁne (ps(α))n
s=0 inductively by p0(α) = 1/2 and

∈

ps+1(α) =

g∗

∇

ηn

 −

s

u=0
X

α
pu(α) !

,

which is chosen so that Pt+1,1 = ps(α) whenever
n/2 and T1(t) = s. Here we used the fact that
t + 1
≤
ˆLt2 = 0 for t
n/2, which follows from the deﬁnition
≤
1
s
of the bandit. Let Qs(α) =
u=0 α/pu(α). Clearly
−
Q2(1/2) > 0 and Qs(0) = 0 for all s. Furthermore,
Qs(α) is increasing in both α and s and continuous in
α. Therefore there exists an α
(0, 1/2) such that
Q2(1/2)
Q3(α
≥
◦
). Using the fact that
Qs+1(α
◦

). Now suppose that Qs(1/2)

g∗ is increasing,

◦ ∈

P

≥

g∗ (

Qs+1(1/2) = Qs(1/2) +

1
2 ∇
ηnQs+1(α
≥
◦∇
◦
which by induction means that Qs(1/2)
all s

Qs+1(α
◦

) + α

g∗ (

−

−

2. Notice that ˆLt1 = Qs(αn) when T1(t

))−

≥

1 = Qs+2(α
◦
) for
1) = s.

Qs+1(α
◦

ηnQs(1/2))−

1

∇

−

≥

Second half dynamics Deﬁne threshold λn by

λn = n + n2/(2(n

c1))

2n ,

≤

−

where the latter inequality holds for all sufﬁciently large
n. Let E be the event E =
. We claim
λn}
that P (Fn |
c1/2). Suppose that t > n/2
E)
and E
∩

exp(
≥
−
Ft occurs. Then

ˆLn/2,1 ≥

{

ˆLt2 =

t

t

1
Ps2 ≤

1
c1/n ≤

1

n2

,

2(n

c1)

−

−

Xs=n/2+1

Xs=n/2+1
where the ﬁrst inequality follows from the deﬁnition of
Ft. Therefore, since ˆLt1 ≥
g∗(ηn( ˆLt2 −
Pt+1,1 =
∇
ηn
g∗

ˆLn/2,1 ≥
ˆLt1))
n2

λn,

(5)

λn

.

2(n

c1) −

−

≤

(cid:19)(cid:19)

≤ ∇
(cid:18)
(cid:18)
Hence P (Ft+1 |
Ft, E)
≥
implies that Pn/2+1,1 ≤
and hence by induction

c1
n

⊆

P (Fn |

E)

≥

n/2

c1
n

1

−

(cid:16)

(cid:17)

exp(

c1/2) .

(6)

≥

−

u : Qu(1/2)

Lower bounding P (E) By Eqs. (4) and (6) it sufﬁces
to prove that P (E) is larger than a constant for
sufﬁciently large n. Let s = min
,
λn}
{
g∗ for sufﬁciently large
which by our assumptions on
∇
n/2. Then Qs(α
n is at least s > 2 and at most s
)
≤
◦
Qs(1/2). By the intermediate
Qs
Qs(α) we may
value theorem and the continuity of α
, 1/2] such that Qs(αn) = λn. Now
choose αn ∈
introduce a sequence of independent geometric random
and E[Gu] =
variables (Gu)s
u=0 with Gu ∈ {
1/pu(α). Then by construction,

1(1/2) < λn ≤

1, 2, . . .
}

(α
◦

7→

≥

≤

−

P (T1(n/2)

s) = P

≥

s

1

−

 

u=0
X

Gu ≤

n
2 !

.

(7)

You should think of Gu as the number of rounds before
the algorithm plays action 1 for the uth time. Let

κ = min

m :

(

s

m

−

−

1

1

pu(αn) ≤

u=0
X

n
8 )

.

1

−

κ
s
−
u=0
n/16 or

Then either
1/ps
κ(αn)
Then there exists a constant c2 ≥
sufﬁciently large n,

1/pu(αn)
s
κ
−
u=0

≤
1
−

P
≥

P

−

n/16 in which case
n/16.
1/pu(αn)
≥
0 such that for

) ,

ps

κ(αn) =

g∗(

−

∇

−

ηnQs
s

−

1(αn))
−
1

κ

κ

=

g∗

∇

g∗

≤ ∇

−

−

ηn

 −
nηn
α
◦
16

−

u=0
X

αn
pu(αn) !

c2
n

.

≤

(cid:16)
Combining the two cases and choosing c2 ≥
guarantees that ps
n. Using the fact that s

16
c2/n for sufﬁciently large

≤
ps(αn) is decreasing,

κ(αn)

(cid:17)

−

2n

λn =

≥

pu(αn) ≥

7→

αn

s

1

−

u=0
X

s

1

−

u=s

X
−

κ

n
α
◦
c2

=

n

κα
◦
c2

.

Rearranging shows that κ is less than a constant that is
independent of n. By Markov’s inequality

s

κ

−

−

1

P

 

u=0
X

Gu ≥

P

≤

 

n
4 !
κ
s

1

−

−

u=0
X

Gu ≥

2

s

κ

−

−

1

1

pu(αn) ! ≤

u=0
X

1
2

.

(8)

s

κ

−

−

1

P

 

u=0
X

Gu <

n
4 ! ≥

1
2

.

1
c1/n shows that E

c1/n. Noting that Eq. (5)
Fn/2+1

−

Hence

Furthermore,

α
◦
1(αn) ≤

ps

−

s

1

−

ps

αn
1(αn) ≤
u=0
X
= Qs(αn) = λn ≤

−

αn
pu(αn)

2n .

Therefore, using again that s

ps(α) is decreasing,

7→

s

1

−

P

Gu ≤

n
4 !

 

u=s

κ
X
−
n/4
κ
n/4
κ

(cid:18)

(cid:18)

≥

≥

ps

1(αn)κ (1

κ(αn))n/4
−

κ

ps

−

−

α
◦
2n

(cid:19)

(cid:19) (cid:16)

−
c2
n

κ

1

−

n/4

κ

−

,

(cid:17)

(cid:16)
which for sufﬁciently large n is larger than a strictly
positive constant and the result follows by combining the
above with Eqs. (7) and (8).

(cid:17)

Remark 5.4. We believe the result continues to hold
for adaptive learning rates under the assumption that
for all a > 0. The proof
lim supt
becomes signiﬁcantly more delicate, however.

atηt) <

g∗(

→∞

∞

∇

−

t

6 LINEARLY SEPARABLE BANDITS

In this section we consider the case where the adversary
chooses an inﬁnite sequence of loss vectors (ℓt)∞t=1. The
main objective is to prove logarithmic (or better) regret
under the following assumption.

Assumption 6.1. There is a linear separation between
the optimal and suboptimal arms:

∆i = lim inf
n
→∞

(Lni −

Ln1)/n > 0 for all i > 1 .

Note that if (ℓt)∞t=1 are independent and identically
then the above holds
distributed random vectors,
almost surely whenever there is a unique optimal
arm. We provide two results in this setting. The
ﬁrst generalises a known result from stochastic bandits
that there exist algorithms for which the asymptotic
random regret grows arbitrarily slowly almost surely
[Cowan and Katehakis, 2015].
Theorem 6.2. For any nondecreasing function f : N
N with limn
such that lim supn

→
there exists an algorithm

∞
ˆRn/f (n) <

almost surely.

f (n) =

→∞

→∞

∞

The algorithm realising the bound in Theorem 6.2
explores uniformly at random on a set E for which
1 almost surely. The
lim supn
|
→∞ |
reader is warned that
the constants hidden by the
asymptotics are potentially quite enormous.

/f (n)

[n]

≤

E

∩

Of course this result says nothing about the expected
regret, which must be logarithmic for consistent
algorithms [Lai and Robbins, 1985].
The following
theorem improves on a result by Seldin and Slivkins
[2014] by a factor of log(n)/ log log(n).
Theorem 6.3. There exists an algorithm such that for
any adversarial bandit Rn = O(√kn). Furthermore,
under Assumption 6.1 it holds that

Rn
log(n)2 log log(n)

<

.

∞

lim sup
n

→∞

The algorithm is INF with enough forced exploration
that the loss estimators are guaranteed to be sufﬁciently
accurate to detect a linear separation.
The proofs
of Theorems 6.2 and 6.3 use standard concentration
results and are given in Appendix C and Appendix D
respectively.

7 OPEN QUESTIONS

n

P

Despite the relatively long history and extensive
research, many open questions exist about k-armed
adversarial bandits. Perhaps the most exciting ques-
tion is the existence/nature of a genuinely instance-
optimal algorithm. The work by Zimmert and Seldin
[2019] suggests the possibility of an algorithm for which
Rn = O(√kn) and Rn = O(
i:∆i>0 log(n)/∆i),
where ∆i = 1
ℓt1) is the empirical gap
t=1(ℓti −
n
In fact, one could hope for a little
between the arms.
P
For stochastic Bernoulli bandits with means
more.
(θi)k
i=1, the KL-UCB algorithm by Capp´e et al. [2013]
i:∆i>0 ∆i log(n)/d(θi, θ∗)) where
satisﬁes Rn = O(
d(θi, θ1) is the relative entropy between Bernoulli distri-
butions with bias θi and θ1 respectively. We are not
aware of a lower bound proving that such a result is not
n
possible for adversarial bandits with θi = 1
t=1 ℓti.
n
At present it is not clear whether or not our modi-
ﬁed algorithm from Corollary 4.3 retains the logarithmic
regret in the stochastic setting, both because we use an
adaptive learning rate and a hybrid potential. Finally,
it is known that sub-exponential tail bounds are incom-
patible with logarithmic regret in the stochastic setting
[Audibert et al., 2009], but by appropriately tuning the
conﬁdence intervals it is straightforward to prove the
variance is linear in n, which is optimal. Missing is
an adaptation of INF that enjoys (a) minimax regret, (b)
logarithmic regret in the stochastic setting and (c) linear
variance.

P

P

Acknowledgements

This work was supported by the Gatsby Charitable
Foundation. The authors thank Haipeng Luo for spotting
an error in the earlier version of the manuscript.

References

J. Abernethy and A. Rakhlin. Beating the adaptive bandit
with high probability. In 2009 Information Theory and
Applications Workshop, pages 280–289, 2009. 2

C. Allenberg, P. Auer, L. Gy¨orﬁ, and G. Ottucs´ak.
Hannan consistency in on-line learning in case of
unbounded losses under partial monitoring.
In
Proceedings of the 17th International Conference on
Algorithmic Learning Theory, ALT, pages 229–243,
Berlin, Heidelberg, 2006. Springer-Verlag. 2

J.-Y. Audibert and S. Bubeck. Minimax policies for
adversarial and stochastic bandits. In Proceedings of
Conference on Learning Theory (COLT), pages 217–
226, 2009. 1, 7

J.-Y. Audibert and S. Bubeck. Best arm identiﬁcation in
multi-armed bandits. In Proceedings of Conference on
Learning Theory (COLT), 2010. 1, 2

J.-Y. Audibert, R. Munos,

and Cs. Szepesv´ari.
Exploration-exploitation tradeoff using variance esti-
mates in multi-armed bandits. Theoretical Computer
Science, 410(19):1876–1902, 2009. 9

P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire.
Gambling in a rigged casino: The adversarial multi-
armed bandit problem. In Foundations of Computer
Science, 1995. Proceedings., 36th Annual Symposium
on, pages 322–331. IEEE, 1995. 2

A. Balsubramani.

iterated-
logarithm martingale concentration. arXiv preprint
arXiv:1405.2639, 2014. 13

Sharp ﬁnite-time

S. Bubeck and N. Cesa-Bianchi.

Regret Analysis
of Stochastic and Nonstochastic Multi-armed Bandit
Problems.
Foundations and Trends in Machine
Learning. Now Publishers Incorporated, 2012. 1

S. Bubeck and M. Sellke.

ysis of
arXiv:1902.00681, 2019. 7

thompson sampling.

First-order regret anal-
arXiv preprint

S. Bubeck and A. Slivkins. The best of both worlds:
In COLT, pages

Stochastic and adversarial bandits.
42.1–42.23, 2012. 1

S. Bubeck, M. Cohen, and Y. Li. Sparsity, variance
and curvature in multi-armed bandits.
In F. Janoos,
M. Mohri, and K. Sridharan, editors, Proceedings
of Algorithmic Learning Theory, volume 83 of
Proceedings of Machine Learning Research, pages
111–127. PMLR, 07–09 Apr 2018. 2

O. Capp´e, A. Garivier, O. Maillard, R. Munos, and
G. Stoltz. Kullback–Leibler upper conﬁdence bounds
The Annals of
for optimal sequential allocation.
Statistics, 41(3):1516–1541, 2013. 9

Y. S. Chow. On a strong law of large numbers for
martingales. The Annals of Mathematical Statistics,
38(2):610–610, 1967. 12

W. Cowan and M. N. Katehakis. Asymptotic behavior
of minimal-exploration allocation policies: Almost
sure, arbitrarily slow growing regret. arXiv preprint
arXiv:1505.02865, 2015. 1, 9

S. Gerchinovitz and T. Lattimore. Reﬁned lower bounds
for adversarial bandits. In D. D. Lee, M. Sugiyama,
U. V. Luxburg, I. Guyon, and R. Garnett, editors,
Advances in Neural Information Processing Systems
29, NIPS, pages 1198–1206. Curran Associates, Inc.,
2016. 2

E. Hazan.

Introduction to online convex optimization.
in Optimization, 2(3-4):

Foundations and Trends R
(cid:13)
157–325, 2016. 2

E. Hazan and S. Kale.

A simple multi-armed
bandit algorithm with optimal variation-bounded
regret.
In S. M. Kakade and U. von Luxburg,
editors, Proceedings of the 24th Annual Conference
on Learning Theory, volume 19 of Proceedings of
Machine Learning Research, pages 817–820. PMLR,
2011. 2

P. Joulani, A. Gy¨orgy, and Cs. Szepesv´ari. A modular
analysis of adaptive (non-)convex optimization: Opti-
mism, composite objectives, and variational bounds.
In ALT, 2017. 2

T. L. Lai and H. Robbins. Asymptotically efﬁcient
Advances in applied

adaptive allocation rules.
mathematics, 6(1):4–22, 1985. 9

T. Lattimore and Cs. Szepesv´ari. Bandit Algorithms.
Cambridge University Press (preprint), 2019. 1, 2, 5,
7

G. Neu. Explore no more: Improved high-probability
regret bounds for non-stochastic bandits. In C. Cortes,
N. D. Lawrence, D. D. Lee, M. Sugiyama, and
R. Garnett, editors, Advances in Neural Information
Processing Systems 28, NIPS, pages 3168–3176.
Curran Associates, Inc., 2015a. 2

G. Neu. First-order regret bounds for combinatorial
semi-bandits.
In P. Gr¨unwald, E. Hazan, and
S. Kale, editors, Proceedings of The 28th Conference
on Learning Theory, volume 40 of Proceedings of
Machine Learning Research, pages 1360–1375, Paris,
France, 03–06 Jul 2015b. PMLR. 2

Y. Seldin and G. Lugosi. An improved parametriza-
the EXP3++ algorithm for
tion and analysis of
stochastic and adversarial bandits.
In S. Kale and
O. Shamir, editors, Proceedings of the 2017 Confer-
ence on Learning Theory, volume 65 of Proceedings

of Machine Learning Research, pages 1743–1759,
Amsterdam, Netherlands, 07–10 Jul 2017. PMLR. 1

Y. Seldin and A. Slivkins. One practical algorithm
In E. P.
for both stochastic and adversarial bandits.
the
Xing and T. Jebara, editors, Proceedings of
31st International Conference on Machine Learning,
volume 32 of Proceedings of Machine Learning
Research, pages 1287–1295, Bejing, China, 22–24 Jun
2014. PMLR. 1, 9

S. Shalev-Shwartz. Online learning: Theory, algorithms,
and applications. PhD thesis, The Hebrew University
of Jerusalem, 2007. 2

C-Y. Wei and H. Luo. More adaptive algorithms
In S. Bubeck, V. Perchet,
for adversarial bandits.
and P. Rigollet, editors, Proceedings of
the 31st
Conference On Learning Theory, volume 75 of
Proceedings of Machine Learning Research, pages
1263–1291. PMLR, 06–09 Jul 2018. 2, 4

J. Zimmert and Y. Seldin.

An optimal algorithm
for stochastic and adversarial bandits.
In Kamalika
Chaudhuri and Masashi Sugiyama, editors, Proceed-
ings of Machine Learning Research, volume 89 of
Proceedings of Machine Learning Research, pages
467–475. PMLR, 16–18 Apr 2019. 1, 4, 9, 13

J. Zimmert, H. Luo,

and C-Y. Wei.

Beating
stochastic and adversarial semi-bandits optimally
and simultaneously.
In Kamalika Chaudhuri and
Ruslan Salakhutdinov, editors, Proceedings of the
36th International Conference on Machine Learning,
volume 97 of Proceedings of Machine Learning
Research, pages 7683–7692. PMLR, 09–15 Jun 2019.
2, 4

Lemma A.1. Let (xt)∞t=t◦ be a sequence of positive
non-decreasing elements, and f (x) be a continuous non-
increasing functions such that f (t) = xt, t

. Then

t

≥

◦

n

t=t◦
X

xt ≤

xt◦ +

f (t)dt.

n

t◦

Z

Proof. Follows from the geometric deﬁnition of the
Riemann integral.

Lemma A.2. If a non-increasing learning rate ηt is such
1 + t√k log1+q t/η0, then
that 1/ηt ≤

q

√t +

2k
ηtt2

n

t=1
X

(cid:18)
√k
ηn+1 (cid:18)
3.7√k

η0 q

≤

+

t

√k log1+q max

4
3

+

2
q

+

5.5k
η0

(cid:19)

q
1 + 3√k log1+q 3 .

} (cid:19)

3, t
{
1 + 9k3/2 log1+q(9k3/2)

Proof. Consider the ﬁrst part of the sum. Splitting it at
t
◦
2.7,

= 9k, applying Lemma A.1 and using

∞t=1 1/t3/2

≤

P

n

2k

ηtt3/2 ≤

t=1
X

5.4k
ηt◦

5.5k
η0

≤

≤

n

2k
ηtt3/2

t◦

2k
ηt◦ t3/2 +
2k
ηt◦ t3/2
◦

+

t=1
X
+

t=t◦
X
4k
ηn+1t1/2

◦

1 + 9k3/2 log1+q(9k3/2) +

4√k
3ηn+1

.

For the second sum, using the integral of 1/(x log1+q x)
from t
log 3 > 1,

in Lemma A.1 and log n

= 3 to

≥

1
2

1 +

+

1
3

(cid:19)

2√k
η3 (cid:18)

2√k

ηtt log1+q max

3, t
{

}

≤

n

+

≤

t=3
Z
3.7√k
η3

ηn+1t log1+q max
2√k
ηn+1q

+

.

3, t
{

}

q

∞
2√k

Combining the two completes the proof.

A TECHNICAL INEQUALITIES

Proof of Lemma 4.8. The result is immediate if
B. Otherwise let t
B
= min

t
−

t :

1

◦

{

s=1 xs ≥

. Then
P

}

n
t=1 xt <

1 +

t
1
s=1 xs
−

≤

B +

P
n

t=t◦
X

xt

.

1 + 1
2

t
s=1 xs

◦

n

t=1
X

xt

P

n

t=1
X

q

n

t=t◦
X

q

≤

p

Next let f (t) = x
t
⌉
⌈

and F (t) =

1 + 1
2

t
s=1 xs

≤

0
Z

xt

P

P

q
t
0 f (s)ds. Then
R
n

f (t)

dt

1 + F (t)/2

p
1
2

n

t=1
X

v
u
u
t

The result follows from the previous two displays.

4

1 + F (n)/2 = 4

1 +

xt .

B PROOF OF COROLLARY 4.5

Proof of Corollary 4.5. First, we repeat
Theorem 3.1 and note that for a time-independent

the proof of
At we

have vn = 0. Therefore, the regret is bounded as

E

Rn ≤

Pt+1, ˆℓt

Pt −

DFt (Pt+1, Pt)

#

−

E

n

"

t=1 D
X

min
∈A
n

n+1

p

(cid:20)

+ E

+ E

"

t=1
X

Fn+1(p) + k

−

F1(P1)
(cid:21)

(Ft(Pt+1)

Ft+1(Pt+1))

.

−

#

Now we repeat
h1, h2(n).
regularizer is unchanged.

the proof of Lemma 4.10 to ﬁnd
The argument for the INF term in the

For the log-barrier term, due to the non-decreasing
learning rate, at each step

(cid:18)

1
√kηt log n −

1
√kηt+1 log n (cid:19)
Therefore, the only contribution from the log-barrier is
from Fn+1(˜p)

√k/ηn+1.

log Pti)

(
−

0.

≤

≤

Consequently,

Fn+1(˜p)

F1(P1) +

(Ft(Pt+1)

Ft+1(Pt+1))

−

n

t=1
X

−

3√k
ηn+1

,

≤

and thus h1 = 3√k, h2 = k.

Repeating the calculation for the Hessian (essentially for
q = 0), we have that B = √k log n and C = 2√k.

Now using the general bound developed in Theorem 4.1
with η0 = k1/4√3/21/4, we obtain the statement of the
corollary.

C PROOF OF THEOREM 6.2

Deﬁne τ (m) = min
. We assume
{
without loss of generality that f (1) = 1 and that f grows
sufﬁciently slowly so that

t : f (t) = m

}

∈

Then in rounds t
over all actions. In rounds t /
∈
At = arg min
i
∈

E the algorithm explores uniformly
E the algorithm chooses
ˆθif (t
−

1) .

[k]

Let κ = max
can be decomposed by

t : ˆθm1 ≥

{

mini>1 ˆθmi}

. Then the regret

n

t=1
X

ˆRn =

(ℓtAt −

ℓt1)

≤

κ +

1

t
{

∈

E

.

}

The result follows by showing that κ is almost surely
ﬁnite and that

n

t=1
X

1

E

t
{

∈

} ≤

1 a.s.

(10)

lim sup
n
→∞

1
f (n)

n

t=1
X

To show Eq. (10),

P

τ (m)





t=1
X

≤

1

E

t
{

∈

} ≥

m + 1



∞

P (Ej ≤


τ (m))

j=m+1
X
By Borel-Cantelli and Eq. (9),

lim sup
m

→∞

τ (m)

t=1
X

1
m

n

Therefore

1
f (n)

lim sup
n

→∞

t=1
X

1

t
{

∈

E

}

∞

≤

j=m+1
X

τ (m)
τ (j)

.

1

E

t
{

∈

} ≤

1 a.s.

≤

lim sup
n
→∞ P

1

τ (f (n)+1)
t=1

t
{
f (τ (f (n) + 1))

E
}
1 ≤

∈
−

1 a.s.

For the ﬁrst part let Xmi = kAEmiℓEmi and
σ(E1, . . . , Em). Then
1] = E
E[Xmi | Gm
1

kAEmiℓEmAEm | Gm

τ (m)
(cid:2)

−

−

1

Gm =

ℓti1

t /
∈
{

(cid:3)
E1, . . . , Em

1}

−

=

τ (m)

m + 1

=

1
τ (m)

t=1
X

ℓti + O

−
τ (m)

t=1
X

m
τ (m)

.

(cid:19)

(cid:18)

∞

∞

τ (m)
τ (j)

<

,

∞

m=1
X

j=m+1
X
(Em)∞m=1 be

(9)

Now ﬁx an i > 1 and let ˆ∆m = Xmi −
Xm1. By the
1] = ∆i almost
previous display, limm
surely. Since ˆ∆m is bounded, Chow’s strong law of large
numbers for martingales [Chow, 1967] shows that

E[ ˆ∆m | Gm

→∞

−

Then let
an inﬁnite sequence of
random variables with Em uniformly distributed on
E1, . . .
1, . . . , τ (m)
}

E1, . . . , Em
{
be the set of steps on which exploration occurs and

. Let E =

} \ {

1}

{

−

ˆθmi =

ℓEjiAEj i .

k
m

m

j=1
X

lim
m
→∞

(ˆθmi −

ˆθm1) = lim
→∞

m

= ∆i + lim
→∞

m

1
m

m

j=1
X

( ˆ∆j −

m

ˆ∆j

1
m

j=1
X
E[ ˆ∆j | Gj

−

1]) = ∆i a.s.

The result follows because ∆i > 0 by assumption.

D PROOF OF THEOREM 6.3

2

Let F (p) =
of INF that chooses
P

−

k
i=1 √pi and consider the modiﬁcation

˜Pt = arg min
∆k−1 h
p
∈

p, ˆLt
−

1i

+

F (p)
ηt

γt) ˜Pt + γt1/k where (γt)∞t=1 and (ηt)∞t=1
and Pt = (1
are appropriately tuned sequences of exploration and
learning rates. Deﬁne

−

g(n) =

1
n2

n

t=1
X

1
γt

.

Lemma D.1. Suppose that ˆLt
−
1

1,i > ˆLt
−

1,1. Then

˜Pti ≤

η2
t (Lt

1,i −

−

Lt

−

1,1)2 .

Proof. Straightforward calculus shows that

,

˜Pti =

1
t (λ + ˆLt
η2
R is the unique value such that ˜Pt ∈
1,1 and the result follows.
−

ˆLt
−

1,i)2

−

∆k

1.

−

where λ
∈
Clearly λ >

Lemma D.2. Suppose that g(n) = o(1/ log(n)) and let
t : ˆLti −

ˆLt1 ≤

τi = max

t∆i/2

}

{

.

Then E[τi] <

.
∞

Proof. Deﬁne sequence of random variables by

Mt = ˆLti −
which is a martingale adapted to (
and

Lti + Lt1 −

ˆLt1 ,

Ft)∞t=1 with M0 = 0

Rn = E

≤

2E

Mt)2

| Ft]

E[(Mt+1 −
By a ﬁnite-time version of the law of the iterated
logarithm [Balsubramani, 2014] it holds with probability
at least 1

t+1,i + ˆℓ2
ˆℓ2
(cid:20)

δ that

Ft

t+1,1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

(cid:21)

4
γt

.

−

|

Mt|
t ≤

c

g(t) log

s

log(t)
δ

.

(cid:19)

(cid:18)

(11)

1 and

Then deﬁne random variable Λ to be the smallest value
such that Λ
≥
Mt|
t ≤

g(t) log (Λ log(t))

for all t .

c

|

p

≥

x)

1/x for all x

By Eq. (11), P (Λ
1. Let
R be a strictly decreasing function such that
h : R
1). Using the
h(n) and h(n) = o(log(n)−
g(n)
deﬁnition of Mt, Assumption 6.1 and by inverting the
above display,

→
≤

≥

≤

1

h−

τi ≤

c1
log(Λ)

(cid:18)

(cid:19)

+ c2 ,

where c1, c2 are constants that depend on the loss
sequence, but not the horizon. Hence

E[τi]

∞

P

1

h−

c1
log(Λ)

x

≥

−

c2

dx

(cid:18)
P (Λ

(cid:19)
(cid:18)
exp(c1h(x

(cid:19)
c2))) dx

≥

−

min
{

1, exp(

c1h(x

−

c2))
}

dx

−

≤

=

≤
<

0
Z

0
Z

∞

∞

0
Z

∞

.

Proof of Theorem 6.3. Suppose that ηt and γt are
deﬁned by

ηt =

1/t

γt =

log(t) log log(t)
t

.

p

That Rn = O(√nk) follows from the standard analysis
of INF with adaptive learning rates [Zimmert and Seldin,
2019] and the observation that the exploration only
contributes a lower order term of order

γt = o(√n) .

n

t=1
X

For the second part. Given i > 1 deﬁne random time

{

τi = max

t : ˆLt
}
−
Then let τ = maxi>1 τi. By Lemma D.1, for t
deﬁnition of the algorithm ensures that Pti ≤
for all i > 1. Decomposing the regret,

ˆLt
−

1,1 ≤

t∆i/2

1,i −

.

τ the
≥
4/(t∆2
i )

(ℓtAt −

ℓt1)

#

= E

Pt −
h

e1, ℓti#

+ E

e1, ℓti#

1/k

γth

−

˜Pt, ℓti#

n

"

t=1
X
n

"
n

t=1
X

γt

e1, ℓti#

+

e1, ℓti#

t=1
X
+ E[τ ] +

n

γt

t=1
X

γt + O(log(n))

γt + O(log(n)) .

n

"

t=1
X
n

= E

E

"

"

˜Pt −
h
t=1
X
n

˜Pt −
h
t=1
X
n

"

E

˜Pt −
h
t=τ +1
X
n
E[τ ] +

k

n

t=1
X
E[τi] +

i=2
X

t=1
X

≤

≤

≤

≤

The result follows from Lemma D.2 and the fact that

n

t=1
X

γt = O

log(n)2 log log(n)

.

(cid:0)

(cid:1)

