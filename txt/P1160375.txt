0
2
0
2
 
n
a
J
 
4
1
 
 
]
L
M

.
t
a
t
s
[
 
 
6
v
4
3
2
6
0
.
4
0
8
1
:
v
i
X
r
a

Clustering Analysis on Locally Asymptotically
Self-similar Processes with Known Number of
Clusters

Qidi Peng*, Nan Rao†, Ran Zhao‡

Abstract

We study the problems of clustering locally asymptotically self-similar stochastic
processes, when the true number of clusters is priorly known. A new covariance-
based dissimilarity measure is introduced, from which the so-called approximately
asymptotically consistent clustering algorithms are obtained. In a simulation study,
clustering data sampled from multifractional Brownian motions is performed to illus-
trate the approximated asymptotic consistency of the proposed algorithms.

Keywords: Clustering processes · covariance-based dissimilarity · local asymptotic
self-similarity · approximated asymptotic consistency
MSC (2010): 62-07 · 60G10 · 62M10

1

Introduction

Learning stochastic processes is an important area of machine learning, as there is a
considerable amount of machine learning problems that involve time as a component
(Cotofrei, 2002; Harms et al., 2002a; Jin et al., 2002a,b; Keogh and Kasetty, 2003). Due to
the nature of the time component, stochastic processes often possess path features (Lin
et al., 2002). This additional information brought by the time component makes machine
learning on stochastic processes more difﬁcult to handle than machine learning on the
other type objects. A number of new techniques are developed along with studying such
machine learning problems (Li et al., 1998; Bradley and Fayyad, 1998; Keogh et al., 2001;

*Institute of Mathematical Sciences, Claremont Graduate University, Claremont, CA 91711. Email:

†Institute of Mathematical Sciences, Claremont Graduate University, Claremont, CA 91711. Email:

qidi.peng@cgu.edu.

nan.rao@cgu.edu.

‡Institute of Mathematical Sceinces and Drucker School of Management, Claremont Graduate Univer-

sity, Claremont, CA 91711. Email: ran.zhao@cgu.edu.

1

Java and Perlman, 2002). In this paper, we study a particular type of learning problems
on stochastic processes: clustering. Among all machine learning tools, the cluster analysis
is a common technique for unsupervised learning. It aims to detect the hidden patterns
of a set of objects, through grouping the objects in the same cluster, if they are more sim-
ilar to each other than to those in other clusters. Compared to other machine learning
problems, clustering stochastic processes is more sensitive to dissimilarity measurement,
which heavily depends on the objects’ data type (Gan et al., 2007; Aghabozorgi et al.,
2015; Sardá-Espinosa, 2017). Clustering techniques may vary drastically depending on
whether the objects’ data types are vector, matrix or sequence, etc. Being a subset of clus-
tering problems, clustering stochastic processes has received growing attention in diverse
areas to discover patterns of data indexed by “time”. The stochastic process is a common
type of dynamic data that naturally arises in many different scenarios. They have been
broadly explored in information technology (Slonim et al., 2005; Jain et al., 1999), signal
and image processing (Yairi et al., 2001; Honda et al., 2002; Guha et al., 2003; Truppel
et al., 2003; Rubinstein et al., 2013), geology (Juozapaviˇcius and Rapsevicius, 2001; Harms
et al., 2002b), biology and medical research (Bar-Joseph et al., 2002; Damian et al., 2007;
Zhao et al., 2014; Jääskinen et al., 2014), robotics (Oates, 1999), and ﬁnance (Mantegna,
1999; Gavrilov et al., 2000; Tino et al., 2000; Fu et al., 2001; Pavlidis et al., 2006; Bastos
and Caiado, 2014; Ieva et al., 2016), etc. Unlike random vector type data, stochastic pro-
cesses type data are sampled from processes’ distributions, which possess not only ﬁnite
dimensional distribution features but also inﬁnite-length paths features, such as station-
arity, ergodicity, seasonality and Markov property.

In the problem of clustering stochastic processes, dimensionality of a process related to
time is extremely high, compared to dimensionality of commonly observed data. There-
fore new challenges arise if one applies the conventional approaches for cluster static data
on stochastic processes. usually become computationally forbidding (Sardá-Espinosa,
2017). We list at least three issues that may occur when performing these clustering algo-
rithms.

(1) Thanks to the large data volume and high dimension, the conventional approaches
for clustering static data (i.e. data who do not change with time) usually become
computationally forbidding (Sardá-Espinosa, 2017).

(2) Even when the sample observations are of relatively low dimension, the conventional
clustering approaches might suffer from over-ﬁtting issues. For instance, cluster-
ing stationary (or seasonal) processes using the K -means approach with Euclidean
distance between the sample paths, will result in large prediction mis-clustering
errors in model validation. This is because, if one does not take into account the
stationarity (or seasonality) of the process, it is then unable to reduce the noise on
the stationary mean and covariances (or the period) along that process.

(3) More interestingly, Keogh and Lin (2005) surprisingly proved that, without any other
information on the observed stochastic processes, clustering their sample paths (sub-
sequences) is “meaningless”, i.e. the output does not depend on the input data.

2

To overcome the above issues, we assume some key path features of the observed
stochastic processes are known in this framework (see Assumption (A ) in the next sec-
tion). This is not a strong constrain, for the reason that in many ﬁelds it is proved that
the observed paths are sampled from well-known process distributions. For example,
dynamics in ﬁnancial markets (equity returns and interest rates) can be described based
on Geometric Brownian motions (GBm); long-term dependent or self-similar phenomena
are often modeled by fractional Brownian motions (fBm). Contrary to the conventional
clustering approaches, clustering based on the paths features of the processes largely re-
moves the noise by capturing the observations’ paths features. Therefore, a nice dissimi-
larity measure should be the one that well characterizes the paths features. In this context,
“nice” refers to the property that the computational complexity and the prediction errors
caused by the over-ﬁtting issues are expected to be largely reduced. Moreover, subject
to known paths features, consistency of the clustering algorithm (Khaleghi et al., 2012,
2016) may be obtained. Among all the stochastic process features, we focus on character-
izing the property of ergodicity in this paper. However similar analysis can be made for
other patterns of process features such as seasonality, Markov property and martingale
property.

Ergodicity (Krengel, 1985) is a very typical feature possessed by a number of well-
known processes, which are applied to ﬁnancial time series analysis. It is tightly related to
other process features such as stationarity, long-term memory and self-similarity (Grazz-
ini, 2012; Samorodnitsky, 2004). Khaleghi et al. (2016) and Peng et al. (2019) showed that
both distribution ergodicity and covariance ergodicity lead to obtaining an asymptotically
consistent clustering algorithms for clustering processes. In this paper, we step further to
relax the condition of ergodicity to the “local asymptotic ergodicity” (Boufoussi et al.,
2008) and obtain the so-called “approximately asymptotically consistent algorithms” for
clustering processes having such path property. This setting presents such a large class of
processes that includes the well-known Lévy processes, some self-similar processes and
some multifractional processes (Boufoussi et al., 2008).

Each clustering stochastic processes problem involves handling data, deﬁning clus-
ter, measuring dissimilarities and ﬁnding groups efﬁciently, therefore we organize the
paper as follows. Section 2 is devoted to introducing a class of locally asymptotically self-
similar processes to which our clustering approaches apply. In Section 3, a covariance-
based dissimilarity measure is suggested and in Section 4 the approximately asymptoti-
cally consistent algorithms for clustering both ofﬂine and online datasets are designed. A
simulation study is performed in Section 5, where the algorithms are applied to cluster
multifractional Brownian motions (mBm), an excellent representative of the class of lo-
cally asymptotically self-similar processes. In Section 6, we perform cluster analysis over
the real world global ﬁnancial market data through applying our clustering algorithms
and provide economic implications. 7 concludes our ﬁndings.

3

2 A Class of Locally Asymptotically Self-similar Processes

Self-similarity is a process (path) feature. Self-similar processes are a class of processes
that are invariant in distribution under suitable scaling of time (Samorodnitsky and Taqqu,
1994; Embrechts and Maejima, 2000, 2002). These processes have been used to success-
fully model various time-scaling random phenomena observed in high frequency data,
especially in the geological data and ﬁnancial data.

Deﬁnition 1 (Self-similar process) A stochastic process {Y (H )
}t ≥0 (here the time indexes set is
not necessarily continuous) is self-similar with self-similarity index H ∈ (0, 1) if, for all n ∈ (cid:78) :=
{1, 2, . . .}, all t1, . . . , tn ≥ 0 and all c > 0,

t

(cid:179)

Y (H )
ct1

, . . . , Y (H )
c tn

(cid:180) law=

(cid:179)
c H Y (H )
t1

, . . . , c H Y (H )

tn

(cid:180)

,

law= denotes the equality in joint probability distribution of two random vectors.

where
When (cid:69)|Y (H )

t

| < +∞ for t ≥ 0, it follows from (1) that for any t ≥ 0,

(cid:179)

(cid:69)

(cid:180)

Y (H )
t

= c H (cid:69)

(cid:180)

(cid:179)
Y (H )
t /c

, for all c > 0,

therefore taking t = 0 at both hand sides of (2) yields

(cid:69)

(cid:180)

(cid:179)
Y (H )
0

= 0.

Self-similar processes are generally not distribution stationary but their increment pro-
cesses can be distribution stationary (any ﬁnite subset’s joint distribution is invariant
subject to time shift) or covariance stationary (its mean and covariance structure exist
and are invariant subject to time shift). From now on we restrict our setting to stochas-
tically continuous-time self-similar processes only (Embrechts and Maejima, 2002). i.e., a
process {X t }t ≥0 is stochastically continuous at t0 ≥ 0 if

(cid:80)(|X (t0 + h) − X (t0)| > ε) −−−−→

h→0+ 0, for any ε > 0.

This assumption is weaker than the almost sure continuity. The process {X t }t ≥0 is called
(stochastically) continuous-time over [0, +∞) if it is continuous at each t ≥ 0. For u > 0, we
call {Y (t )}t = {X (t +u)−X (t )}t the increment process (or simply the increments) of {X (t )}t . If
a continuous-time self-similar process’ all increment processes are covariance stationary,
its covariance structure can be explicitly given as below:
Theorem 1 Let (cid:169)X (H )
stationary increments. Then

t ≥0 be a self-similar process with index H ∈ (0, 1) and with covariance

(cid:170)

t

(cid:179)

(cid:69)

(cid:180)

X (H )
t

= 0, for all t ≥ 0,

and

(cid:179)

(cid:67)ov

X (H )
s

, X (H )
t

(cid:180)

=

V ar (X (H )
2

1

)

(cid:161)|s|2H + |t |2H − |s − t |2H (cid:162) , for any s, t ≥ 0.

4

(1)

(2)

(3)

(4)

(5)

Theorem 1 can be obtained by replacing the distribution stationary increments in The-
orem 1.2 in Embrechts and Maejima (2000) with covariance stationary increments. We
brieﬂy provide the proof below.
Proof 1 We ﬁrst prove (4). On one hand, by using the fact that the increments of {X (H )
covariance stationary and (3), we have

}t are

t

(cid:179)

(cid:69)

X (H )
mt

(cid:180)

= (cid:69)

X (H )

(k+1)t

− X (H )
kt

+ X (H )
0

(cid:180)

= m(cid:69)

(cid:180)

(cid:179)

X (H )
t

, for all m ∈ (cid:78).

(cid:33)

(cid:195)m−1
(cid:88)

(cid:179)

k=0

On the other hand, since {X (H )

t

}t is self-similar, we have
(cid:179)

(cid:180)

(cid:179)

(cid:180)

(cid:69)

X (H )
mt

= m H (cid:69)

X (H )
t

, for all m ∈ (cid:78).

Putting together (6), (7) and the fact that H < 1, we necessarily have (cid:69)(X (H )
is proved.

t

) = 0 for all t ≥ 0. (4)

For proving (5) we ﬁrst observe that, for s, t ≥ 0,

(cid:179)

(cid:69)

s X (H )
X (H )
t

(cid:180)

=

1
2

(cid:181)
(cid:69) (cid:161)X (H )
s

(cid:179)

(cid:162)2 + (cid:69)

X (H )
t

(cid:180)2

(cid:179)

− (cid:69)

X (H )
s

− X (H )
t

(cid:180)2(cid:182)

.

Next we can see from the facts that {X (H )
covariance stationary and (4), that

t

}t is self-similar with index H, that its increments are

(cid:69) (cid:161)X (H )
s
(cid:179)

(cid:162)2 = |s|2H (cid:69)
(cid:180)2

(cid:69)

− X (H )
X (H )
s
t
(cid:180)
(cid:179)
X (H )
|s−t |

= V ar

(cid:179)

(cid:180)2

X (H )
1
(cid:179)

= |s|2H V ar
(cid:180)

= V ar

X (H )
s

− X (H )
t
(cid:180)
(cid:179)
X (H )
1

= |s − t |2H V ar

, for s, t ≥ 0.

(cid:180)

(cid:179)

X (H )
1

, for s ≥ 0;

(6)

(7)

(8)

(9)

The covariance stationarity yields V ar (X (H )
is thus proved.

1

) < +∞. (5) then follows from (8) and (9). Theorem 1

We highlight that, contrary to Theorem 1.2 in Embrechts and Maejima (2000), the covari-
ance stationary increment process of (cid:169)X (H )
t in Theorem 1 is not necessarily distribution
stationary. This fact inspires us to relax the distribution stationarity of the processes to
the covariance stationarity in the following Assumption (A ). Below we introduce a nat-
ural extension of self-similar processes, the so-called locally asymptotically self-similar
processes (Boufoussi et al., 2008; Falconer, 2002, 2003).

(cid:170)

t

(cid:170)

Deﬁnition 2 (Locally asymptotically self-similar process) A continuous-time stochastic pro-
cess (cid:169)Z (H (t ))
t ≥0 with its index H (•) being a continuous function valued in (0, 1), is called lo-
cally asymptotically self-similar, if for each t ≥ 0, there exists a non-degenerate self-similar process
(cid:169)Y (H (t ))
u

t

(cid:170)
u≥0 with self-similarity index H (t ), such that
Z (H (t +τu))
t +τu

− Z (H (t ))
t

(cid:40)

(cid:41)

τH (t )

f.d.d.
−−−−→
τ→0+

(cid:169)Y (H (t ))
u

(cid:170)

u≥0 ,

(10)

u≥0

5

u

f.d.d.
−−−−→ is in the sense of all the ﬁnite dimensional distributions.

where the convergence
In (10), {Y (H (t ))
}u is called the tangent process of {Z (H (t ))
}t at t (Falconer, 2002, 2003). More-
over, it is shown (see Theorem 3.8 in Falconer (2003)) that, if {Y (H (t ))
}u is unique in law, it is
then self-similar with index H (t ) and it has distribution stationary increments. Then the
local asymptotic self-similarity generalizes the conventional self-similarity, in the sense
that, any non-degenerate self-similar process with distribution stationary increments is
locally asymptotically self-similar and its tangent process is itself. Further, in a weaker
sense, it is not difﬁcult to show the following:

u

t

Proposition 1 Let {Z (H )
}t ≥0 be a continuous-time self-similar process with self-similarity index
H ∈ (0, 1) and with covariance stationary increments. Then its tangent processes share equal mean
and covariance functions.

t

Proof 2 Since {Z (H )
exists a tangent process {Y (H )

t

u

}t ≥0 is locally asymptotically self-similar, by deﬁnition at each t ≥ 0 there

}u≥0 such that
(cid:40)

Z (H )
t +τu

− Z (H )
t
τH

(cid:41)

f .d .d .−−−−→
τ→0+

(cid:169)Y (H )
u

(cid:170)
u≥0 .

u≥0

Next we show {Y (H )
Since {Z (H )

t

increments

}u≥0’s mean and covariance structure are uniquely determined.

u
}t ≥0 has covariance stationary increments, for any u ≥ 0, τ > 0, deﬁne the scaled

Y (H )
u,τ :=

Z (H )
t +τu

− Z (H )
t
τH

.

}t ≥0 has covariance stationary increments, then using (4) in Theorem

Again by the fact that {Z (H )
1 we obtain

t

and by (5) in Theorem 1, we have for u1, u2 ≥ 0 and τ > 0,

(cid:69) (cid:161)Y (H )
u,τ

(cid:162) = 0, for all u ≥ 0, τ > 0,

(cid:67)ov (cid:161)Y (H )

u1,τ, Y (H )
u2,τ

(cid:162) = τ−2H (cid:67)ov

(cid:179)

t +τu1

Z (H )
(cid:180)

− Z (H )
t

, Z (H )

t +τu2

− Z (H )
t

(cid:180)

(cid:161)|u1|2H + |u2|2H − |u1 − u2|2H (cid:162) ,

V ar

(cid:179)

Z (H )
1

=

2

(11)

(12)

(13)

which is independent of τ.

It follows from (11), (12) and (13) that
(cid:69) (cid:161)Y (H )
(cid:69) (cid:161)Y (H )
u,τ
u
(cid:162) = lim
(cid:67)ov (cid:161)Y (H )
u1
τ→0+
(cid:179)

(cid:162) = lim
τ→0+
, Y (H )
u2
(cid:180)

(cid:162) = 0, for all u ≥ 0;
(cid:162)
(cid:67)ov (cid:161)Y (H )

u1,τ, Y (H )
u2,τ

V ar

Z (H )
1

=

2

(cid:161)|u1|2H + |u2|2H − |u1 − u2|2H (cid:162) , for u1, u2 ≥ 0.

(14)

6

This implies that all tangent processes of {Z (H )
}t ≥0 possess zero-mean and equal covariance func-
tions. By the way it is easy to derive from (14) that these tangent processes have covariance
stationary increments. Proposition 1 is proved.

t

We remark from Proposition 1 that the tangent processes of {Z (H )
}t ≥0 may not be unique
in law, but their ﬁnite-dimensional subsets have unique ﬁrst and second order moments.
Based on the above discussion, throughout this paper we assume that the observed
dataset are sampled from a known number (κ) of continuous-time processes satisfying
the following condition:

t

Assumption (A ): The processes are locally asymptotically self-similar; their tangent pro-
cesses’ increment processes are autocovariance ergodic.

Here the autocovariance-ergodicity means that the sample autocovariance functions
of the covariance stationary process converges in squared mean to the autocovaraince
functions of the process, i.e., a zero-mean (that is the case for the tangent processes’ in-
crements) continuous-time process {X (t )}t ≥0 is autocovariance ergodic if it is covariance
stationary and satisﬁes

1
T

(cid:90) T

0

L2((cid:80))
−−−−−→
T →+∞

X (t + τ)X (t ) dt

(cid:69) (X (u + τ)X (u)) , for all u > 0, τ ≥ 0,

(15)

L2((cid:80))−−−−−→
n→+∞ X denotes the mean squared convergence: (cid:69)|Xn − X |2 −−−−−→

n→+∞ 0. Note that

where Xn
the above convergence (15) yields

1
n − τ − 1

n−τ
(cid:88)

k=1

X (k)X (k + τ)

(cid:69)(X (1)X (1 + τ)), for τ ∈ (cid:78).

(16)

(cid:80)−−−−−→
n→+∞

Thus Assumption (A ) says that the observed processes’ tangent processes have covari-
ance stationary increments. The well-known examples of locally asymptotically self-
similar processes satisfying Assumption (A ) are fractional Brownian motions and mul-
tifractional Brownian motions (Mandelbrot and Van Ness, 1968; Peltier and Lévy-Véhel,
1995; Benassi et al., 1997).

The assumption of covariance stationarity inspires us to introduce a covariance-based
dissimilarity measure between the sample paths, in order to capture the level of differ-
ences between the two corresponding covariance stationary processes. Later we show
that the assumption of autocovariance-ergodicity is sufﬁcient for the clustering algo-
rithms to be approximately asymptotically consistent.

7

3 Clustering Stochastic Processes Satisfying Assumption

(A )

Ergodic Processes

3.1 Covariance-based Dissimilarity Measure between Autocovariance

Let Z be a process satisfying Assumption (A ). Denote by Y its tangent process (see (10))
and denote by X an increment process of Y , i.e., there is some u ≥ 0 such that X (t ) =
Y (t + u) − Y (u) for all t ≥ 0. Under Assumption (A ), X is autocovariance ergodic. Since
we will show that clustering distinct Z ’s is approximately asymptotically equivalent to
clustering the corresponding increment processes X ’s, then the dissimilarity measures of
Z ’s can be constructed based on those of the autocovariance ergodic processes X ’s.

From (4) we know that the autocovariance process X is zero-mean. Our ﬁrst main
result is then introduction to the following covariance-based dissimilarity measure between
autocovariance ergodic processes (Peng et al., 2019).

Deﬁnition 3 The covariance-based dissimilarity measure d between the discrete-time stochastic
processes X (1), X (2) (in fact X (1), X (2) denote two covariance structures, each class may contain
different process distributions) is deﬁned by

d (cid:161)X (1), X (2)(cid:162) :=

wm wl ρ

(cid:179)
(cid:67)ov(X (1)

l ...l +m−1), (cid:67)ov(X (2)

l ...l +m−1)

(cid:180)

,

(17)

+∞
(cid:88)

m,l =1

where:

• For any integers l ≥ 1, m ≥ 0, X (1)

l ...l +m−1 is the shortcut notation of the row vector

(cid:179)

X (1)
l

, . . . , X (1)

l +m−1

(cid:180)
.

• The distance ρ between 2 equal-sized covariance matrices M1, M2 is deﬁned to be the Frobe-
nius norm of M1 − M2. Recall that for a matrix AM ×N , its Frobenius norm is deﬁned by

(cid:107)AM ×N (cid:107)

F :=

(cid:118)
(cid:117)
(cid:117)
(cid:116)

M
(cid:88)

N
(cid:88)

i =1

j =1

a2
i j ,

where for each (i , j ) ∈ {1, . . . , M } × {1, . . . , N }, ai j denotes the (i , j )-coefﬁcient of AM ×N .

• The sequence of positive weights {w j } j ≥1 should be chosen such that d (cid:161)X (1), X (2)(cid:162) < +∞,
i.e., the series on the right-hand side of Eq. (17) is convergent. The choice of {w j } j will be
discussed in the forthcoming simulation study in Section 5.

Remark 1 It is important to note that Deﬁnition 3 only deﬁnes the dissimilarity measures be-
tween discrete-time stochastic processes (time series). This is the most common object studied in
the literature on clustering stochastic processes. Considering time series is sufﬁcient for practical
applications for at least 2 reasons. First, continuous-time path is not observable in practice. Sec-
ondly, any (stochastically) continuous-time process can be approximated by its discrete-time paths.
In what follows we will mean sample path by a ﬁnite-length subsequence of discretized stochastic
process.

8

Thanks to the autocovariance-ergodicity of the sample processes, the dissimilarity mea-
sure d can be estimated by the empirical dissimilarity measure (cid:98)d below:

( j )
n j ) for j = 1, 2, let
Deﬁnition 4 Given two processes’ discrete-time sample paths x j = (X
n = min{n1, n2}, then the empirical covariance-based dissimilarity measure between x1 and x2 is
given by

( j )
1 , . . . , X

(cid:98)d (x1, x2) :=

mn(cid:88)
m=1

n−m+1
(cid:88)

l =1

wm wl ρ

(cid:179)
ν(X (1)

l ...l +m−1), ν(X (2)

l ...l +m−1)

(cid:180)

,

(18)

where:

• mn (≤ n) is the largest dimension of the covariance matrix considered by (cid:98)d; in this frame-
work we take mn = (cid:98)log n(cid:99), i.e. the ﬂoor number of log n (Khaleghi et al., 2016; Peng et al.,
2019).

• For j = 1, 2, 1 ≤ l ≤ n and m ≤ n−l +1, ν(X

( j )
l ...l +m−1) denotes the empirical covariance matrix

of the process X ( j )’s path (X

( j )
l

, . . . , X

( j )
l +m−1), which is given below:

(cid:179)

ν

X

( j )
l ...l +m−1

(cid:180)

:=

(cid:80)n−m+1
i =l

(X

( j )
i

( j )

. . . X

i +m−1)T (X
n − m − l + 2

( j )
i

. . . X

( j )
i +m−1)

,

(19)

where (•)T denotes the transpose of a matrix.

Remark 2 Since X is autocovariance ergodic, every empirical covariance matrix ν(Xl ...l +m−1)
is a consistent estimator of the covariance matrix (cid:67)ov(Xl ...l +m−1) under Frobenius norm and in
probability, i.e.,

(cid:107)ν(Xl ...l +m−1) − (cid:67)ov(Xl ...l +m−1)(cid:107)

F

(cid:80)−−−−−→
n→+∞ 0, for any l ≥ 0.

(20)

Further, the fact that both d and (cid:98)d satisfy the triangle inequalities implies that (cid:98)d is a
consistent estimator of d. The proof is quite similar to that of Lemma 1 in Peng et al.
(2019), except that in the former statement the convergence holds in probability. These
ergodicity and triangle inequalities are the keys to demonstrate that our algorithms in the
next section are approximately asymptotically consistent. We list them in the following
remarks.

Remark 3 For every pair of paths

(cid:179)

x1 =

X (1)
1 , . . . , X (1)
n1

and

x2 =

X (2)
1 , . . . , X (2)
n2

(cid:180)

,

(cid:179)

(cid:180)

,

sampled from two autocovariance ergodic processes X (1) and X (2) respectively, we have

(cid:98)d (x1, x2)

(cid:80)−−−−−−−→
n1,n2→+∞ d (cid:161)X (1), X (2)(cid:162) ,

(21)

9

and

xi , X ( j )(cid:180)
(cid:179)
(cid:98)d

(cid:80)−−−−→
ni →∞ d (cid:161)X (1), X (2)(cid:162) , for i , j ∈ {1, 2},

(22)

where the dissimilarity measure (cid:98)d (cid:161)xi , X ( j )(cid:162) between the sample path xi and the stochastic process
X ( j ) is deﬁned to be

xi , X ( j )(cid:180)
(cid:179)
(cid:98)d

(cid:179)
:= (cid:98)d

xi ,

(cid:179)

X

( j )
1 , . . . , X

( j )
ni

(cid:180)(cid:180)

.

Remark 4 Thanks to their deﬁnitions, the triangle inequalities hold for the covariance-based dis-
similarity measure d in (17), as well as for its empirical estimates (cid:98)d in (18). Therefore for arbitrary
processes X (i ), i = 1, 2, 3 and arbitrary random vectors xi , i = 1, 2, 3 we have

d (cid:161)X (1), X (2)(cid:162) ≤ d (cid:161)X (1), X (3)(cid:162) + d (cid:161)X (3), X (2)(cid:162),
(cid:98)d (x1, x2) ≤ (cid:98)d (x1, x3) + (cid:98)d (x2, x3),
(cid:98)d (cid:161)x1, X (1)(cid:162) ≤ (cid:98)d (cid:161)x1, X (2)(cid:162) + d (cid:161)X (1), X (2)(cid:162).

In the next section we deﬁne a proper covariance-based dissimilarity measure between
locally asymptotically self-similar processes satisfying Assumption (A ), based on the dis-
similarity measure d.

3.2 Covariance-based Dissimilarity Measure between Locally Asymp-

totically Self-similar Processes

Now under Assumption (A ), we study the asymptotic relationship between the locally
asymptotically self-similar process {Z (H (t ))
}t in (10) and its tangent process’ increment pro-
cess. The following result reveals the relationship between local asymptotic self-similarity
and covariance stationarity.
Proposition 2 Let (cid:169)Z (H (t ))
tion (A ). For each h > 0,

t ≥0 be a locally asymptotically self-similar process satisfying Assump-

(cid:170)

t

t






Z (H (t +τ(u+h)))
t +τ(u+h)

− Z (H (t +τu))
t +τu

τH (t )






u≥0

(cid:110)

f.d.d.
−−−−→
τ→0+

(cid:111)

X (H (t ))
u,h

,

u≥0

(23)

where (cid:169)X (H (t ))

(cid:170)

u,h

u≥0 := (cid:169)Y (H (t ))
u+h

− Y (H (t ))
u

(cid:170)

u≥0 (see (10)) is an autocovariance ergodic process.

Proof 3 Let’s ﬁx h > 0 and pick any ﬁnite discrete time indexes set T ⊂ [0, +∞). Under Assump-
tion (A ), the f.d.d. convergence (10) holds. It then implies





Z (H (t +τ(u+h)))
t +τ(u+h)
τH (t )

− Z (H (t ))
t

Z (H (t +τu))
t +τu

− Z (H (t ))
t

,

τH (t )





u∈T

law−−−−→
τ→0+

(cid:179)
Y (H (t ))
u+h , Y (H (t ))

u

(cid:180)

,

u∈T

(24)

10

where we adopt the notation (au, bu)u∈{u1,...,uN } to denote the vector

It follows from (24) and the continuous mapping theorem that

(au1, bu1, au2, bu2, . . . , auN , buN ).





Z (H (t +τ(u+h)))
t +τ(u+h)
τH (t )

− Z (H (t ))
t

Z (H (t +τu))
t +τu

− Z (H (t ))
t

−

τH (t )





u∈T

law−−−−→
τ→0+

(cid:179)
Y (H (t ))
u+h

− Y (H (t ))
u

(cid:180)

.

u∈T

(25)

(23) then results from (25) and the fact that the choice of T is arbitrary. Under Assumption (A ),
(cid:169)X (H (t ))
u,h

(cid:170)
u is autocovariance ergodic, hence Proposition 2 is proved.

u := (cid:169)Y (H (t ))
u+h

− Y (H (t ))
u

(cid:170)

From a statistical point of view, the sequence at the left-hand side of (23) can not straight-
forwardly serve to estimate the distribution of the right-hand side {X H (t )
u,h }u, because the
functional index H (•) at the left-hand side is not observable in practice. To overcome this
inconvenience we note that (23) can be interpreted as: when τ is sufﬁciently small,

(cid:110)

Z (H (t +τ(u+h)))
t +τ(u+h)

− Z (H (t +τu))
t +τu

(cid:111)

f.d.d.≈

(cid:110)

τH (t )X (H (t ))

u,h

(cid:111)

,

u∈[0,K h]

u∈[0,K h]

where K is an arbitrary positive integer. Statistically, (26) says that: given a discrete-
time path Z (H (t1))
with ti = i h∆t for each i ∈ {1, . . . , n}, sampled from a locally
asymptotically self-similar process {Z (H (t ))
}t , its localized increment paths with time index
around ti , i.e.,

, . . . , Z (H (tn ))
tn

t1

t

(cid:179)

z(i ) :=

Z (H (ti +1))
ti +1

− Z (H (ti ))
ti

, . . . , Z (H (ti +1+K ))
ti +1+K

− Z (H (ti +K ))
ti +K

(cid:180)

,

is approximately distributed as an autocovariance ergodic increment process of the self-
similar process

. This fact drives us to deﬁne the empirical covariance-

(cid:110)
∆t H (ti )X (H (ti ))

(cid:111)

u,h

u∈[0,K h]

based dissimilarity measure between two paths of locally asymptotically self-similar pro-
cesses z1 and z2 as follows:

(26)

(27)

(28)

(cid:99)d ∗(z1, z2) :=

(cid:179)
(cid:98)d

1
L

L
(cid:88)

i =1

1 , z(i )
z(i )

2

(cid:180)

,

where:

• L ≤ n − K − 1 is a positive integer not depending on K ;

• z(i )

1 , z(i )
2 are the localized increment paths deﬁned as in (27). Heuristically speaking,
for i = 1, . . . , n − K − 1, (cid:98)d (z(i )
2 ) computes the “distance” between the 2 covariance
structures (of the increments of {Z H (t )
}t ) indexed by the time in the neighborhood
of ti , and (cid:99)d ∗(z1, z2) averages the above distances. It is worth noting that the value K
describes the “sample size” used to approximate each local distance ˆd. Therefore its
value should be picked neither too large nor too small and it can depend on n.

1 , z(i )

t

11

The following observation is straightforward.

Remark 5 Based on the deﬁnition (28) and Remark 2, (cid:99)d ∗ is also a (weakly) consistent estimator
of d (see Remark 3) and it also satisﬁes the triangle inequalities as in Remark 4.

Through employing the dissimilarity measure (cid:99)d ∗ on locally asymptotically self-similar
processes, we obtain the so-called “approximately asymptotically consistent algorithms”,
which are introduced in the following section.

4 Approximately Asymptotically Consistent Algorithms

4.1 Ofﬂine and Online Algorithms

Note that the covariance-based dissimilarity measure (cid:99)d ∗ deﬁned in (28) will aim to cluster
covariance structures, not process distributions, therefore the ground truth of clustering
should be based on covariance structures. We thus deﬁne the ground truth as follows
(Peng et al., 2019).
Deﬁnition 5 (Ground truth of covariance structures) Let G = (cid:169)G1, . . . ,Gκ
(cid:170) be a partitioning
of (cid:78) into κ disjoint sets Gk, k = 1, . . . , κ, such that the means and covariance structures of xi , i ∈ (cid:78)
are identical, if and only if i ∈ Gk for some k = 1, . . . , κ. Such G is called ground truth of covariance
structures. For N ≥ 1, we denote by G|N the restriction of G to the ﬁrst N sequences:

G|N = (cid:169)Gk ∩ {1, . . . , N } : k = 1, . . . , κ(cid:170).

The processes Z satisfying Assumption (A ) are generally not covariance stationary, how-
ever their tangent processes’ increments X are covariance stationary. In view of (23) and
(26), clustering these processes Z is equivalent to clustering X , based on the covariance
structure ground truth of the latter increments. Below we will introduce algorithms aim-
ing to approximate the covariance structure ground truth of X .

Depending on how the information is collected, the processes clustering problems
consist of dealing with two separate model settings: ofﬂine setting and online setting. In
the ofﬂine setting, the sample size and each path length are time-independent. However,
in the online setting, they may both grow with time. As stated in Khaleghi et al. (2016),
using the ofﬂine algorithm in the online setting by simply applying it to the entire data ob-
served at every time step, does not result in an asymptotically consistent algorithm. As a
result, we consider clustering ofﬂine and online datasets as 2 approaches and study them
separately. Hence the approximated asymptotic consistency will be described in Theorem
2 and Theorem 3 below, respectively for ofﬂine and online clustering algorithms. Our of-
ﬂine and online clustering algorithms below are obtained by replacing the dissimilarity
measures in Algorithms 1 and 2 in Peng et al. (2019) with (cid:99)d ∗.

For the ofﬂine setting, we cluster observed data using Algorithm 1 below. It is a 2-
point initialization centroid-based clustering approach. Under the distance (cid:99)d ∗ deﬁned in

12

(28), the algorithm picks the farthest two points to be the ﬁrst two cluster centers (Lines
1-2). Then iteratively, each next cluster center is chosen to be the point farthest to all
the previously assigned cluster centers (Lines 3-5). Finally the algorithm assigns each
remaining point to the nearest cluster (Lines 7-10).

Algorithm 1: Ofﬂine clustering

Input: sample paths S = {z1, . . . , zN }; number of clusters κ.
(cid:99)d ∗(zi , z j );

1 (c1, c2) ←−

argmax
(i , j )∈{1,...,N }2,i < j

2 C1 ←− {c1}; C2 ←− {c2};
3 for k = 3, . . . , κ do
ck ←− argmax
4
i =1,...,N

min
j =1,...,k−1

(cid:99)d ∗(zi , zc j );

5 end
6 Assign the remaining points to the nearest centers:
7 for i = 1, . . . , N do
(cid:110)
k ←− argmin
k∈{1,...,κ}

(cid:99)d ∗(zi , z j ) : j ∈ Ck

(cid:111)
;

8

Ck ←− Ck ∪ {i };

9
10 end

Output: The κ clusters f (S, κ, (cid:99)d ∗) = {C1, . . . ,Cκ}.

The strategy for clustering online data is presented in Algorithm 2 as follows. At
each time t, ﬁrst update the collection of sample paths S(t ) (Lines 1-2). Then for each
j = κ, . . . , N (t ), use Algorithm 1 to group the ﬁrst j paths in S(t ) into κ clusters (Lines 6-7);
for each cluster the center is selected as the point having the smallest index among that
cluster, and order these indexes increasingly (Line 8); calculate the minimum inter-cluster
distance γ j and update the normalization factor η (Line 9-11). Finally, every sample path
in S(t ) is assigned to the “nearest” cluster, based on the weighted combination of the
distances (given in Line 15) between this observation and the candidate cluster centers
obtained at each iteration on j (Lines 14-17).

4.2 Computational Complexity and Consistency of the Algorithms

We describe the computational complexity based on the number of computations of the
distance ρ. For Algorithm 1, the 2-point initialization requires N (N − 1)/2 times calcula-
tions of (cid:99)d ∗. From (28) we see that each calculation of (cid:99)d ∗ consists of nmin − K − 1 times
ˆd can be obtained through computing K − log K + 1 times
computations of ˆd. By (18),
distances ρ. Therefore total number of computations of ρ is not greater than N (N −
1)(nmin − K − 1)(K − log K + 1)/2. For Algorithm 2, since at each step j ∈ {κ, . . . , N − κ + 1}, Al-
gorithm 1 is run on j observations, the total number of ρ’s computations is then less than
(nmin − K − 1)(K − log K + 1) (cid:80)N −κ+1
j ( j − 1)/2. The computational complexity is acceptable

j =κ

13

; number of clusters κ; a sequence of

Algorithm 2: Online clustering
(cid:110)
S(t ) = {zt

Input: Sample paths
weights {β j } j .

1 for t = 1, . . . , ∞ do

1, . . . , zt

N (t )}

(cid:111)

t

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

(cid:110)
1, . . . , zt
zt

(cid:111)
;

N (t )

Obtain new paths: S(t ) ←−
Initialize the normalization factor: η ←− 0;
Initialize the κ clusters: Ck (t ) ←− (cid:59), k = 1, . . . , κ;
Generate N (t ) − κ + 1 candidate cluster centers:
for j = κ, . . . , N (t ) do
(cid:170) ←− Alg1(cid:161)(cid:169)zt
(cid:169)C
j
1, . . . , zt
1 , . . . ,C
j
k ) ←− sort(min (cid:169)i ∈ C
j
j
j
(c
1 , . . . , c
k
the one for cluster center, and sort them increasingly;
γ j ←−

k,k(cid:48)∈{1,...,κ},k(cid:54)=k(cid:48) (cid:99)d ∗(cid:161)zt

, zt
c

min

(cid:162);

j
κ

c

(cid:48)

j
k

j
k

w j ←− β j ; \\ w j is the weight used in (cid:99)d ∗;
η ←− η + w j γ j ; \\ Update the normalization factor;

end
Assign each point to one of the κ clusters:
for i = 1, . . . , N (t ) do
N (t )
(cid:80)
k ←− argmin
j =κ
k(cid:48)∈{1,...,κ}
Ck (t ) ←− Ck (t ) ∪ {i };

w j γ j (cid:99)d ∗(cid:161)zt

i , zt

(cid:162);

1
η

j
k

c

(cid:48)

end

17
18 end

Output: The κ clusters f (S(t ), κ, (cid:99)d ∗) = {C1(t ), . . . ,Cκ(t )}, t = 1, 2, . . . , ∞.

(cid:170), κ, (cid:99)d ∗(cid:162);
(cid:170) : k = 1, . . . , κ); \\ Choose the smallest index to be

in practice, and it is quite competitive to the existing algorithms for clustering stochastic
processes.

Now we introduce the notion of approximately asymptotic consistency. Fix a positive
integer K . Let Z (1), Z (2) be 2 locally asymptotically self-similar processes with respect
functional indexes H1(•), H2(•). Also let
1 , . . . , z(n−K −1)

2 , . . . , z(n−K −1)

(cid:170) and (cid:169)z(1)

(cid:169)z(1)

(cid:170),

1

2

be respectively their sample paths z1, z2’ increments, deﬁned as in (27). For j = 1, 2, we
deﬁne the normalized increments by taking the following linear transformation:

H (cid:161)z(i )
j

(cid:162) :=

z(i )
j
∆t H j (ti )

, for i = 1, . . . , n − K − 1.

(29)

14

(30)

(31)

(32)

Then using (23) we obtain

H (cid:161)z(i )
j

(cid:162) law−−−−→
∆t →0

(cid:179)

X

(H j (ti ))
0, j

, X

(H j (ti ))
h, j

, . . . , X

(H j (ti ))
K h, j

(cid:180)

,

(cid:179)

(H j (ti ))
0, j

(H j (ti ))
h, j

(cid:180)

(H j (ti ))
K h, j

X

, X

where
denotes a discrete-time path of the increment of a self-
similar process with self-similarity index H j (ti ). Fix L ≥ 1. For each empirical dissimilarity
measure (cid:99)d ∗(z1, z2), we correspondingly deﬁne

, . . . , X

(cid:102)d ∗(z1, z2) :=

1
L

L
(cid:88)

i =1

(cid:179)
H (z(i )
(cid:98)d

(cid:180)
1 ), H (z(i )
2 )

.

(cid:102)d ∗(z1, z2) is another dissimilarity measure between z1, z2, which has a tight relationship to
the distance (cid:99)d ∗ between their tangent processes’ increments. Indeed by using (30) and
the continuous mapping theorem, it is easy to derive the following result.

Proposition 3 For any N independent sample paths z1, z2, . . . , zN ,

(cid:179)
(cid:102)d ∗(zi , z j )

(cid:180)
i , j ∈{1,...,N },i (cid:54)= j

law−−−−→
∆t →0

(cid:179)

(cid:99)d ∗(xi , x j )

(cid:180)
i , j ∈{1,...,N },i (cid:54)= j

,

where x1, x2 are the increments of the tangent processes corresponding to z1, z2 respectively.
In particular when ∆t = 1, (cid:102)d ∗(z1, z2) = (cid:99)d ∗(z1, z2). In this sense (cid:102)d ∗(z1, z2) “approximates” (cid:99)d ∗.
(cid:102)d ∗ can not be observed in practice since the functional indexes of the locally asymptoti-
cally self-similar processes are supposed to be unknown. In what follows (cid:102)d ∗ only serves
to deﬁne the approximate asymptotic consistency. In this notion “approximate” means
the clustering locally asymptotically self-similar processes problem is “approximately”
equivalent to the clustering their tangent processes problem.

Now we state the consistency theorems. Through Theorems 2 and 3 below we show
that Algorithms 1 and 2 are both approximately asymptotically consistent. Their proofs
are inspired by the ones in Khaleghi et al. (2016) and Peng et al. (2019). However different
from the consistent theorems in Khaleghi et al. (2016) and Peng et al. (2019), the conver-
gences in Theorems 2 and 3 have a weaker sense, which are in probability, not almost
sure.

Theorem 2 Under Assumption (A ), Algorithm 1 is approximately asymptotically consistent for
if (cid:99)d ∗ is replaced with (cid:102)d ∗ in
clustering the ofﬂine sample paths S = {z1, . . . , zN }. This means:
Algorithm 1, then the output clusters converge to the covariance structure ground truths of the
(cid:48) = {x1, . . . , xN } in probability, as ∆t → 0 and
increments of the corresponding tangent processes S
nmin := min{n1, . . . , nN } → +∞. More formally,
(cid:179)

(cid:180)

lim

nmin→+∞ lim
∆t →0

(cid:80)

f (S, κ, (cid:102)d ∗) = GS(cid:48)

= 1,

(33)

where f is given in Algorithm 1 and GS(cid:48) denotes the ground truths of the covariance structures
that generate the set of paths S

(cid:48).

15

Proof 4 First, we show

(cid:179)

(cid:80)

f (S, κ, (cid:102)d ∗) = G

(cid:180)

−−−−→
∆t →0

(cid:179)

(cid:80)

(cid:48)

f (S

, κ, (cid:99)d ∗) = G

(cid:180)

,

where G = {C1, . . . ,Cκ} denotes any κ-partition of {1, . . . , N }. Since if the estimated clusters f (S, κ, (cid:102)d ∗) =
G, the samples in S that are generated by the same cluster in G are closer under (cid:102)d ∗ to each other
than to the rest of the samples. Hence we can write

(cid:179)

(cid:80)

(cid:180)

f (S, κ, (cid:102)d ∗) = G





(cid:91)




max
l ∈{1,...,κ}
i , j ∈Cl



ε>0

= (cid:80)







(cid:92)






(cid:102)d ∗(zi , z j ) < ε

k,k



min
(cid:48)∈{1,...,κ}, k(cid:54)=k
i ∈Ck , j ∈Ck

(cid:48)

(cid:48)

(cid:102)d ∗(zi , z j ) > ε







 .







It follows from (35) and (32) that

(cid:179)

(cid:80)

lim
∆t →0


f (S, κ, (cid:102)d ∗) = G


(cid:180)

= (cid:80)










(cid:91)

ε>0







(cid:179)

max
l ∈{1,...,κ}
i , j ∈Cl

= (cid:80)

(cid:48)

f (S

, κ, (cid:99)d ∗) = G

(cid:180)

,

(cid:99)d ∗(xi , x j ) < ε

(cid:92)

(cid:99)d ∗(xi , x j ) > ε











min
i ∈Ck , j ∈Ck
(cid:48)
(cid:48)∈{1,...,κ}
k,k
(cid:48)
k(cid:54)=k






















which proves (34).

Next we show that Algorithm 1 is asymptotically consistent on clustering S

(cid:48) under (cid:99)d ∗:

(cid:179)

(cid:80)

(cid:48)

f (S

, κ, (cid:99)d ∗) = GS(cid:48)

(cid:180)

−−−−−−→
nmin→∞ 1.

Denote by

Fix δ > 0. Let nmin denote the shortest path length in S

(cid:48):

GS(cid:48) := {G1, . . . ,Gκ}.

nmin := min {ni : i ∈ {1, . . . , N }} .
Denote by δmin the minimum non-zero dissimilarity measure between the processes with different
covariance structures:

δmin := min

(cid:110)

(cid:179)

d

X (k), X (k

(cid:48)

)(cid:180)

: k, k

(cid:48) ∈ {1, . . . , κ}, k (cid:54)= k

(cid:48)(cid:111)

.

Fix ε ∈ (0, δmin/4) and let δ > 0 be arbitrarily small. Since there are a ﬁnite number N of samples,
by Remark 3 there is n0 such that for nmi n > n0 we have

(34)

(35)

(36)

(37)

(38)



(cid:80)


 max
l ∈{1,...,κ}
i ∈Gl ∩{1,...,N }

(cid:99)d ∗ (cid:179)

xi , X (l )(cid:180)

> ε

< δ.






16

On one hand, by applying the triangle inequalities (see Remark 5), we obtain

max
l ∈{1,...,κ}
i , j ∈Gl ∩{1,...,N }

≤

max
l ∈{1,...,κ}
i , j ∈Gl ∩{1,...,N }

= 2 max
l ∈{1,...,κ}
i ∈Gl ∩{1,...,N }

(cid:99)d ∗(xi , x j )

(cid:99)d ∗ (cid:179)

xi , X (l )(cid:180)

(cid:99)d ∗ (cid:179)

xi , X (l )(cid:180)

.

+ max
l ∈{1,...,κ}
i , j ∈Gl ∩{1,...,N }

(cid:99)d ∗ (cid:179)

x j , X (l )(cid:180)

(39)

Then by (39) and the fact that 2ε < δmin/2, the following inclusion holds:






max
l ∈{1,...,κ}
i ∈Gl ∩{1,...,N }

(cid:99)d ∗ (cid:179)

xi , X (l )(cid:180)

≤ ε

⊂











max
l ∈{1,...,κ}
i , j ∈Gl ∩{1,...,N }

(cid:99)d ∗(xi , x j ) ≤ 2ε <

(40)

δmin
2






.

On the other hand, by applying the triangle inequalities (see Remark 5) and the fact that 2ε <
δmin/2, we also obtain: if

(cid:99)d ∗ (cid:179)

xi , X (l )(cid:180)

≤ ε,

max
l ∈{1,...,κ}
i ∈Gl ∩{1,...,N }

then

Equivalently,

(cid:99)d ∗(xi , x j )

k,k

min
(cid:48)∈{1,...,κ}, k(cid:54)=k
i ∈Gk ∩{1,...,N }
(cid:48) ∩{1,...,N }
j ∈Gk

(cid:48)

≥

k,k

min
(cid:48)∈{1,...,κ}, k(cid:54)=k
i ∈Gk ∩{1,...,N }
(cid:48) ∩{1,...,N }
j ∈Gk

(cid:48)

≥ δmin − 2ε >

δmin
2

.

(cid:110)

(cid:179)

d

X (k), X (k

(cid:48)

)(cid:180)

− (cid:99)d ∗ (cid:179)

xi , X (k)(cid:180)

− (cid:99)d ∗ (cid:179)

x j , X (k

(cid:48)

)(cid:180)(cid:111)






max
l ∈{1,...,κ}
i ∈Gl ∩{1,...,N }

(cid:99)d ∗ (cid:179)

xi , X (l )(cid:180)

≤ ε

⊂











k,k

min
(cid:48)∈{1,...,κ}, k(cid:54)=k
i ∈Gk ∩{1,...,N }
(cid:48) ∩{1,...,N }
j ∈Gk

(cid:48)

(cid:99)d ∗(xi , x j ) >

δmin
2

(41)






.

17

It follows from (40), (41) and (38) that for nmin > n0,




max
l ∈{1,...,κ}
i , j ∈Gl ∩{1,...,N }

(cid:99)d ∗(xi , x j ) ≥

δmin
2






(cid:91)




k,k

min
(cid:48)∈{1,...,κ}, k(cid:54)=k
i ∈Gk ∩{1,...,N }
(cid:48) ∩{1,...,N }
j ∈Gk

(cid:48)

(cid:99)d ∗(xi , x j ) ≤

δmin
2

(cid:99)d ∗ (cid:179)

xi , X (l )(cid:180)

> ε

< δ.






≤ (cid:80)


 max
l ∈{1,...,κ}
i ∈Gl ∩{1,...,N }

Note that (42) is equivalent to

δmin
2






(cid:92)






(cid:99)d ∗(xi , x j ) >

δmin
2

k,k

min
(cid:48)∈{1,...,κ}, k(cid:54)=k
i ∈Gk ∩{1,...,N }
(cid:48) ∩{1,...,N }
j ∈Gk

(cid:48)

max
l ∈{1,...,κ}
i , j ∈Gl ∩{1,...,N }

(cid:99)d ∗(xi , x j ) <

−−−−−−−→
nmin→+∞ 1.

(cid:80)























(cid:80)

(42)



























This tells that the sample paths in S that are generated by the same covariance structures are closer
to each other than to the rest of sample paths. Then by (42), for nmin > n0, each sample path should
be “close” enough to its cluster center, i.e.,

(cid:181)

(cid:80)

max
i ∈{1,...,N }

min
k∈{1,...,κ−1}

(cid:99)d ∗(xi , xck ) ≤

(cid:182)

δmin
2

< δ,

(43)

where the κ cluster centers’ indexes c1, . . . , cκ are determined by Algorithm 1 in the following way:

and

(c1, c2) := argmax

(cid:99)d ∗(xi , x j ),

i , j ∈{1,...,N }, i < j

ck := argmax
i ∈{1,...,N }

min
j ∈{1,...,k−1}

(cid:99)d ∗(xi , xc j ), k = 3, . . . , κ.

These c1, . . . , cκ are chosen to index sample paths generated by different process covariance struc-
tures. Then by (42), each remaining sample path will be assigned to the cluster center correspond-
ing to the sample path generated by the same process covariance structure. Finally (36) results
from (42) and (43); and (33) is proved by combining (34) and (36).

Below we state the consistency theorem concerning the online clustering algorithm.

Theorem 3 Under Assumption (A ), Algorithm 2 is approximately asymptotically consistent for
N (t )}, t = 1, 2, . . .. This means: if (cid:99)d ∗ is replaced
clustering the online sample paths S(t ) = {zt
1, . . . , zt
with (cid:102)d ∗ in Algorithm 2, for any integer N ≥ 1, the output clusters of the ﬁrst N paths in S(t )

S(t )|N := (cid:169)zt

1, . . . , zt
N

(cid:170) ,

18

converge to the covariance structure ground truths of the increments of the corresponding tangent
processes S

N } in probability, as ∆t → 0 and t → +∞. In other words,

(t )|N := {xt

1, . . . , xt

(cid:48)

(cid:179)

(cid:80)

lim
t →+∞

lim
∆t →0

f (S(t ), κ, (cid:102)d ∗)|N = GS(cid:48)(t )|N

= 1,

(cid:180)

where f (S(t ), κ, (cid:102)d ∗)|N denotes the clustering f (S(t ), κ, (cid:102)d ∗) restricted to the ﬁrst N sample paths in
S(t ). We also recall that GS(cid:48)(t )|N is the restriction of GS(cid:48)(t ) to the ﬁrst N sample paths {xt
N }
in S

(t ) (see Deﬁnition 5).

1, . . . , xt

(cid:48)

Proof 5 Let’s ﬁx N ≥ 1. First, similar to the derivation of (34) in the proof of Theorem 2, we can
obtain

(cid:179)

(cid:80)

f (S(t ), κ, (cid:102)d ∗)|N = GS(cid:48)(t )|N

(cid:180)

−−−−→
∆t →0

(cid:179)

(cid:80)

(cid:48)

f (S

(t ), κ, (cid:99)d ∗)|N = GS(cid:48)(t )|N

(cid:180)

.

Then it remains to prove

(cid:179)

(cid:80)

(cid:48)

f (S

(t ), κ, (cid:99)d ∗)|N = GS(cid:48)(t )|N

(cid:180)

−−−−→
t →+∞

1.

In what follows we prove (46).

Let δ > 0 be arbitrarily small. Fix ε ∈ (0, δmin/4), where δmin is deﬁned as in (37).
Denote by

δmax := max

(cid:110)

(cid:179)

d

X (k), X (k

(cid:48)

)(cid:180)

: k, k

(cid:48) ∈ {1, . . . , κ}

(cid:111)

.

For k ∈ {1, . . . , κ}, denote by sk the index of the ﬁrst path in S

(t ) sampled from X (k), i.e.,

(cid:48)

sk := min {i ∈ Gk ∩ {1, . . . , N (t )}} .

Note that sk does not depend on t but only on k. Then denote by

(cid:48)

For j ≥ 1 denote by S
m ≤ N (t ) and S
By using the fact that (cid:80)+∞

(cid:48)

(t )| j the ﬁrst j sample paths contained in S(t ). Then from (49) we see that,
(t )|m contains paths sampled from all κ distinct processes (covariance structures).

j =1 w j < +∞, we can ﬁnd a ﬁxed value J ≥ m such that

m := max

k∈{1,...,κ}

sk .

+∞
(cid:88)

j =J +1

w j ≤ ε.

Recall that in the online setting, the i th sample path’s length ni (t ) grows with time t. Therefore,
by Remark 5, for every j ∈ {1, . . . , J } there exists some T1( j ) > 0 such that

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)



(cid:80)

sup
t ≥T1( j )


 max
k∈{1,...,κ}
i ∈Gk ∩{1,..., j}

(cid:99)d ∗ (cid:179)

i , X (k)(cid:180)
xt

> ε

< δ.






19

(t )| j , κ, (cid:99)d ∗)
Since J ≥ m, by Theorem 2 for every j ∈ {m, . . . , J } there exists T2( j ) > 0 such that Alg1(S
is asymptotically consistent for all t ≥ T2( j ). Since N (t ) is increasing as t → +∞, there is T3 > 0
such that N (t ) > J for t ≥ T3. Let

(cid:48)

T := max

Ti ( j ), T3

.






max
i ∈{1,2}
j ∈{1,...,J }






From Algorithm 2 (Lines 9, 11) we see

ηt :=

w j γt

j , with γt

N (t )
(cid:88)

j =1

j := min
(cid:48)∈{1,...,κ}
(cid:48)
k(cid:54)=k

k,k

(cid:99)d ∗

(cid:181)
xt
c

, xt
c

j
k

j
k

(cid:48)

(cid:182)

.

(52)

Below we provide upper bounds in probability of ηt and γt
j .

Upper bound of γt

j : Similar to how (41) is derived, we use the triangle inequalities (Remark 5)

and (37) to obtain:

(cid:181)

(cid:80)

sup
t ≥T

min
j ∈{1,...,N (t )}


<

γt
j

(cid:182)

δmin
2

(cid:80)

≤ sup
t ≥T

min
j ∈{1,...,N (t )}
(cid:48)∈{1,...,κ}
k,k
(cid:48)
k(cid:54)=k









(cid:80)

≤ sup
t ≥T


 max
j ∈{1,...,N (t )}
k∈{1,...,κ}

(cid:181)

(cid:99)d ∗

xt
c

j
k

(cid:182)

, X (k)

>




 .

δmin
4

(cid:179)

(cid:181)
d

X (k), X (k

(cid:48)

)(cid:180)

− 2 (cid:99)d ∗

(cid:182)(cid:182)

, X (k)

<

(cid:181)
xt
c

j
k

δmin
2









(53)

Since the clusters are ordered in the order of appearance of the distinct process covariance
sk for all j ≥ m and k ∈ {1, . . . , κ}, where we recall that the index
structures, we have xt
c
sk is deﬁned in (48). It follows from (53), the fact that ε < δmin/4 and (51) that

= xt

j
k

(cid:181)

(cid:80)

sup
t ≥T

min
j ∈{1,...,N (t )}

<

γt
j

(cid:182)

δmin
2

(cid:80)

≤ sup
t ≥T


 max
j ∈{1,...,m}
k∈{1,...,κ}

(cid:99)d ∗

(cid:181)
xt
c

j
k



(cid:182)






, X (k)

> ε

< mδ.

(54)

20

For j ∈ {1, . . . , N (t )}, by (52), the triangle inequality, (47) and (54), we have

(cid:181)

(cid:179)

d

X (k), X (k

(cid:48)

)(cid:180)

+ 2 (cid:99)d ∗

(cid:182)(cid:182)

, X (k)

> δmax + 2ε

(cid:181)
xt
c

j
k









(cid:181)

(cid:80)

sup
t ≥T

max
j ∈{1,...,N (t )}


(cid:182)

γt
j

> δmax + 2ε

(cid:80)

≤ sup
t ≥T

max
j ∈{1,...,N (t )}
(cid:48)∈{1,...,κ}
k,k
(cid:48)
k(cid:54)=k









(cid:80)

≤ sup
t ≥T


 max
j ∈{1,...,m}
k∈{1,...,κ}

< mδ.

(cid:99)d ∗

(cid:181)
xt
c

j
k

(cid:182)

, X (k)

> ε






Upper bound of ηt : By (54) and the fact that (cid:80)N (t )

j =1 w j ≥ wm, we have

(cid:181)
ηt <

(cid:80)

(cid:182)

wmδmin
2

sup
t ≥T

(cid:181)

(cid:80)

≤ sup
t ≥T

min
j ∈{1,...,N (t )}

γt
j

(cid:195)

(cid:80)

≤ sup
t ≥T
δmin
2

<

(cid:182)

< mδ.

min
j ∈{1,...,N (t )}

γt
j

N (t )
(cid:88)

j =1

w j <

(cid:33)

wmδmin
2

Recall that N (t ) > J for t ≥ T . Therefore for every k ∈ {1, . . . , κ} we can write

1
ηt

N (t )
(cid:88)

j =1

w j γt

j (cid:99)d ∗

(cid:182)

, X (k)

=

(cid:181)

xt
c

j
k

1
ηt

m−1
(cid:88)

j =1

w j γt

j (cid:99)d ∗

(cid:182)

, X (k)

(cid:181)

xt
c

j
k

+

1
ηt

J
(cid:88)

j =m

w j γt

j (cid:99)d ∗

(cid:182)

, X (k)

+

(cid:181)
xt
c

j
k

1
ηt

N (t )
(cid:88)

j =J +1

w j γt

j (cid:99)d ∗

(cid:182)

, X (k)

.

(cid:181)
xt
c

j
k

(57)

Now we provide upper bounds in probability of the 3 terms on the right-hand side of (57).

Upper bound of the ﬁrst term: In view of (51) and the fact that (ηt )

−1 (cid:80)m−1

j =1 w j γt

j

≤ 1, we get

(55)

(56)

(58)

w j γt

j (cid:99)d ∗

, X (k)

> ε

(cid:181)

xt
c

j
k

(cid:181)

(cid:99)d ∗

xt
c

j
k

(cid:182)

(cid:182)

(cid:33)






, X (k)

> ε

(cid:80)

sup
t ≥T

m−1
(cid:88)

j =1

(cid:195)

1
ηt



(cid:80)

≤ sup
t ≥T


 max
j ∈{1,...,m−1}
k∈{1,...,κ}

≤ (m − 1)δ.

21

Upper bound of the second term: Recall that xt
c

Therefore, by (51) and the fact that (ηt )

= xt

j
k

sk for all j ∈ {m, . . . , J } and k ∈ {1, . . . , κ}.
≤ 1, for every k ∈ {1, . . . , κ} we have

−1 (cid:80)J

j =m w j γt

j

Upper bound of the third term: By (50), (56) and (55),

1
ηt

J
(cid:88)

j =m

(cid:195)
(cid:99)d ∗ (cid:179)

(cid:195)

(cid:80)

sup
t ≥T

(cid:80)

= sup
t ≥T

≤ sup
t ≥T

w j γt

j (cid:99)d ∗

(cid:182)

(cid:33)

, X (k)

> ε

(cid:181)

xt
c

j
k

xt
sk

, X (k)(cid:180) 1
ηt

(cid:33)

w j γt
j

> ε

J
(cid:88)

j =m
(cid:180)

(cid:80)

(cid:99)d ∗ (cid:179)
(cid:179)

xt
sk

, X (k)(cid:180)

> ε

< δ.

(cid:80)

sup
t ≥T

(cid:195)

1
ηt



N (t )
(cid:88)

j =J +1

w j γt

j (cid:99)d ∗

(cid:182)

, X (k)

>

(cid:181)

xt
c

j
k

2ε2(δmax + 2ε)
wmδmin

(cid:33)

(cid:80)

≤ sup
t ≥T


 max
j ∈{1,...,N (t )}
k∈{1,...,κ}

(cid:181)

(cid:99)d ∗

xt
c

j
k

(cid:182)

, X (k)

> ε






(cid:181)

(cid:182)

(cid:80)

+ sup
t ≥T

max
j ∈{1,...,N (t )}

γt
j

> δmax + 2ε

< 2mδ.

Combining (57), (58), (59) and (60) we obtain, for k ∈ {1, . . . , κ},

(cid:195)

1
ηt

N (t )
(cid:88)

j =1

(cid:80)

sup
t ≥T

w j γt

j (cid:99)d ∗

(cid:181)
xt
c

j
k

(cid:182)

, X (k)

> ε

(cid:181)
2 +

2ε2(δmax + 2ε)
wmδmin

(cid:182)(cid:33)

< 3mδ.

Now we explain how to use (61) to prove the asymptotic consistency of Algorithm 2. Consider
(cid:48) ∈ {1, . . . , κ}. On one hand, using the triangle inequalities, we get for
an index i ∈ Gk(cid:48) for some k
k ∈ {1, . . . , κ}, k (cid:54)= k

(cid:48),

1
ηt

N (t )
(cid:88)

j =1

w j γt

j (cid:99)d ∗

(cid:182)

≥

(cid:181)
i , xt
xt

c

j
k

N (t )
(cid:88)

j =1

1
ηt

(cid:33)

w j γt

j d

∗ (cid:179)

X (k), X (k

(cid:48)

)(cid:180)

(cid:195)

−

1
ηt

N (t )
(cid:88)

j =1

w j γt
j

(cid:99)d ∗ (cid:179)
i , X (k
xt

(cid:48)

)(cid:180)

−

1
ηt

w j γt

j (cid:99)d ∗

(cid:182)

, X (k)

(cid:181)

xt
c

j
k

N (t )
(cid:88)

j =1
(cid:182)

≥ δmin − (cid:99)d ∗ (cid:179)
i , X (k
xt

(cid:48)

)(cid:180)

+

1
ηt

N (t )
(cid:88)

j =1

w j γt

j (cid:99)d ∗

, X (k)

.

(cid:181)

xt
c

j
k

22

(59)

(60)

(61)

Then applying (51) and (61) we obtain

(cid:80)

sup
t ≥T

N (t )
(cid:88)

j =1

(cid:195)

1
ηt

(cid:195)

w j γt

j (cid:99)d ∗

(cid:182)

(cid:181)
i , xt
xt

c

j
k

(cid:181)

< δmin − ε

3 +

(cid:99)d ∗ (cid:179)
i , X (k
xt

(cid:48)

)(cid:180)

+

(cid:80)

(cid:179)

(cid:99)d ∗ (cid:179)
i , X (k
xt

(cid:48)

)(cid:180)

> ε

(cid:80)

≤ sup
t ≥T

≤ sup
t ≥T

w j γt

j (cid:99)d ∗

(cid:181)
xt
c

j
k

N (t )
(cid:88)

j =1

1
ηt

(cid:180)

(cid:182)(cid:33)

2ε2(δmax + 2ε)
wmδmin
(cid:181)
(cid:182)
3 +

> ε

, X (k)

(cid:182)(cid:33)

2ε2(δmax + 2ε)
wmδmin

(cid:80)

+ sup
t ≥T

(cid:195)

1
ηt

N (t )
(cid:88)

j =1

w j γt

j (cid:99)d ∗

(cid:181)
xt
c

j
k

(cid:182)

, X (k)

> ε

(cid:181)
2 +

2ε2(δmax + 2ε)
wmδmin

(cid:182)(cid:33)

< (3m + 1)δ.

On the other hand, from (51) we see for any N ≥ 1,



sup
t ≥T1(N )

(cid:80)


 max
k∈{1,...,κ}
i ∈Gk ∩{1,...,N }

(cid:99)d ∗ (cid:179)

i , X (k)(cid:180)
xt

> ε

< δ.






Using again the triangle inequalities, we get

w j γt

j (cid:99)d ∗

i , xt
xt

(cid:182)

c

j
k

(cid:48)

(cid:181)

(cid:33)

1
ηt

N (t )
(cid:88)

j =1

(cid:195)

≤

1
ηt

N (t )
(cid:88)

j =1

w j γt
j

(cid:99)d ∗ (cid:179)
i , X (k
xt

(cid:48)

)(cid:180)

+

1
ηt

N (t )
(cid:88)

j =1

w j γt

j (cid:99)d ∗

(cid:182)

(cid:48)

)

, X (k

(cid:181)

xt
c

j
k

(cid:48)

≤ (cid:99)d ∗ (cid:179)

i , X (k
xt

(cid:48)

)(cid:180)

+

1
ηt

N (t )
(cid:88)

j =1

w j γt

j (cid:99)d ∗

(cid:182)

(cid:48)

)

.

, X (k

(cid:181)
xt
c

j
k

(cid:48)

(cid:48)

Let T

:= max{T, T1(N )}. It results from (64), (63) and (61) that

(cid:181)

w j γt

j (cid:99)d ∗

i , xt
xt

(cid:182)

(cid:181)
3 +

> ε

(cid:48)

)(cid:180)

+

i , X (k
xt

w j γt

j (cid:99)d ∗

c

j
k

(cid:48)

N (t )
(cid:88)

j =1

1
ηt

(cid:180)

(cid:182)(cid:33)

2ε2(δmax + 2ε)
wmδmin
(cid:182)

(cid:48)

)

, X (k

(cid:181)
3 +

> ε

(cid:181)
xt
c

j
k

(cid:48)

(cid:80)

(cid:99)d ∗ (cid:179)
(cid:179)

i , X (k
xt

(cid:48)

)(cid:180)

> ε

(cid:182)(cid:33)

2ε2(δmax + 2ε)
wmδmin

(cid:80)

+ sup
t ≥T (cid:48)

(cid:195)

1
ηt

N (t )
(cid:88)

j =1

w j γt

j (cid:99)d ∗

(cid:181)
xt
c

j
k

(cid:48)

(cid:182)

(cid:48)

)

(cid:181)
2 +

> ε

, X (k

(cid:182)(cid:33)

2ε2(δmax + 2ε)
wmδmin

1
ηt

N (t )
(cid:88)

j =1

(cid:195)
(cid:99)d ∗ (cid:179)

(cid:195)

(cid:80)

sup
t ≥T (cid:48)

(cid:80)

≤ sup
t ≥T (cid:48)

≤ sup
t ≥T (cid:48)

< (3m + 1)δ.

23

(62)

(63)

(64)

(65)

Since δ and ε can be chosen arbitrarily small, it follows from (62) and (65) that

(cid:195)

(cid:80)

argmin
k∈{1,...,κ}

1
ηt

N (t )
(cid:88)

j =1

w j γ j (cid:99)d ∗

(cid:182)

(cid:181)
i , xt
xt

c

j
k

(cid:33)

(cid:48)

= k

−−−−→
t →+∞

1,

(66)

for all i ∈ {1, . . . , N }. (46) as well as Theorem 3 is proved.

5 Tests on Simulated Data: Clustering Multifractional Brow-

nian Motions

5.1 Efﬁciency Improvement: log

-transformation

∗

In this section, we show performance of the proposed clustering approaches (Algorithm
1) and (Algorithm 2) on clustering simulated multifractional Brownian motions (mBm).
MBm is a paradigmatic example of locally asymptotically self-similar processes. Its tan-
gent process is fractional Brownian motion (fBm), which is self-similar. Since the covari-
ance structure of the fBm is nonlinearly dependent on its self-similarity index, we can
∗
then apply the so-called log
-transformation to the covariance matrices of its increments,
in order to improve the efﬁciency of the clustering algorithms. More precisely, in our
clustering algorithms, we replace in (cid:99)d ∗ the coefﬁcients of all the covariance matrices and
∗
-transformation, i.e., for x ∈ (cid:82),
their estimators with their log

∗
log

(x) :=






log x,
− log(−x),
0,

if x > 0;
if x < 0;
if x = 0.

By applying such transformation, the observations assigned to any two clusters by the
covariance structure ground truths become well separated thus the clustering algorithms
become more efﬁcient. For more detail on this efﬁciency improvement approach we refer
the readers to Section 3 in Peng et al. (2019).

5.2 Simulation Methodology

Recall that an mBm {WH (t )(t )}t ≥0 is a zero-mean continuous-time Gaussian process, which
can be deﬁned via its covariance function (Benassi et al., 1997; Ayache et al., 2000; Stoev
and Taqqu, 2006): for s, t ≥ 0,

where

(cid:67)ov (cid:161)WH (t )(t ),WH (s)(s)(cid:162) := D(H (t ), H (s))

× (cid:161)t H (t )+H (s) + s H (t )+H (s) − |t − s|H (t )+H (s)(cid:162) ,

(cid:112)

D(t , s) :=

Γ(2t + 1)Γ(2s + 1) sin(πt ) sin(πs)
2Γ(t + s + 1) sin(π(t + s)/2)

.

24

(67)

(68)

It can be seen from Boufoussi et al. (2008) that the mBm is locally asymptotically self-
similar satisfying Assumption (A ). Its tangent process at t is an fBm {B (H (t ))(u)}u with
index H (t ):

(cid:189)WH (t +τu)(t + τu) − WH (t )(t )
τH (t )

(cid:190)

f.d.d.
−−−−−→
τ→0+ C H (t )

u

(cid:169)B (H (t ))(u)(cid:170)

u ,

where C H (t ) is a deterministic function only depending on H (t ).

We select Wood-Chan’s simulation method (Wood and Chan, 1994; Chan and Wood,
1998) to simulate the mBm paths, and use the implementation (MATLAB) of Wood-
Chan’s method in FracLab (version 2.2) by INRIA in our simulation study1. This method
outputs independent sample paths of the following form:

(69)

(70)

(cid:189)

WH (i /n)

(cid:182)(cid:190)

(cid:181) i
n

, for i = 0, 1, . . . , n,

where n ≥ 1 is an input parameter. Now we would select w j = 1/( j 2( j + 1)2) so that d (see
(17)) is a convergent series (well-deﬁned). To show this choice is reasonable we consider
the stochastic process

(cid:169)WH (i ∆) (i ∆)(cid:170) , for i = 0, 1, . . .,
(71)
where ∆ > 0 is some given mesh. For each t0 = 0, ∆, 2∆, . . ., the increments of the tangent
process (see the right-hand side of (26))

(cid:169)C H (t0)n

−H (t0)B (H (t0)) (i ∆)(cid:170) , for i = 0, 1, . . .

is given by: for i = 0, 1, . . .,

X (H (t0)) (i ∆) = C H (t0)n

−H (t0) (cid:161)B (H (t0)) ((i + 1)∆) − B (H (t0)) (i ∆)(cid:162) .

As increments of fBm, X (H (t0))(•) is autocovariance ergodic. Moreover for i , j = 0, 1, . . ., we
∗
(see (67)), the covariance function of fBm (see (5)) and
have, by using the deﬁnition of log
the fact that sups≥0 H (s) ≤ 1,

∗ (cid:161)(cid:67)ov (cid:161)X H (t0)) (i ∆) , X (H (t0)) (cid:161) j ∆(cid:162)(cid:162)(cid:162)

log

(cid:195)C 2

H (t0)

∆2H (t0)

∗

= log

2

(cid:179)(cid:175)
(cid:175)i − j − 1

(cid:175)
(cid:175)

2H (t0) + (cid:175)

(cid:175)i − j + 1

(cid:175)
(cid:175)

2H (t0) − 2

(cid:175)
(cid:175)i − j

(cid:175)
(cid:175)

(cid:33)

2H (t0)(cid:180)

= O (cid:161)log(|i − j | + 1)(cid:162) , as |i − j | → +∞.

(72)

From the deﬁnition of d in (17) and (72) we can see that, by taking w j = 1/( j 2( j + 1)2) and
using (72), for any t0, t

= 0, ∆, 2∆, . . .,

(cid:48)
0

(cid:179)

d

X (H (t0)), X (H (t

(cid:48)

0))(cid:180)

= O





+∞
(cid:88)

l ,m=1

(cid:80)l +m−1
log(|i − j | + 1)
i , j =l
l 2(l + 1)2m2(m + 1)2



 ,

1https://project.inria.fr/fraclab/download/overview/.

25

where,

+∞
(cid:88)

l ,m=1

(cid:80)l +m−1
log(|i − j | + 1)
i , j =l
l 2(l + 1)2m2(m + 1)2

= 2

+∞
(cid:88)

l ,m=1

(cid:80)m−1

k=1 (m − k) log(k + 1)
l 2(l + 1)2m2(m + 1)2

≤ 2

+∞
(cid:88)

l ,m=1

log m
l 2(l + 1)2(m + 1)2

< +∞.

Therefore we have shown that w j = 1/( j 2( j + 1)2) leads to that d is well-deﬁned.

5.3 Synthetic Datasets

To construct a collection of the mBm paths with distinct functional indexes H (•), we set
the function form of H (•) in each of the predetermined clusters. Two functional forms of
H (•) are selected for synthetic data study:

• Case 1 (Monotonic function): The general form is taken to be

H (t ) = 0.5 + h · t /Q,

t ∈ [0,Q],

(73)

where Q > 0 is a ﬁxed integer and different values of h correspond to difference clus-
ters. We then predetermine 5 clusters with various h’s to separate different clusters.
In this study we set Q = 100, h1 = −0.4, h2 = −0.2, h3 = 0, h4 = 0.2 and h5 = 0.4. The
trajectories of the 5 functional forms of H (•) in different clusters are illustrated in
the top graph of Figure 1.

• Case 2 (Periodic function): The general form is taken to be

H (t ) = 0.5 + h · sin(πt /Q),

t ∈ [0,Q],

(74)

where different values of h lead to different clusters. Speciﬁcally, we take Q = 100,
h1 = 0.4, h2 = 0.2, h3 = 0, h4 = −0.2 and h5 = −0.4. The trajectories of the correspond-
ing 5 functional forms of H (•) are illustrated in the top graph of Figure 2.

We demonstrate the approximated asymptotic consistency of the proposed algorithms
by conducting both ofﬂine and online clustering analysis. Denote the number of observed
data points in each time series by n(t ), and denote the number of time series paths by N (t ).
Under ofﬂine setting, the number of observed paths does not depend on time t, ow-

ever the lengths do. In order to construct ofﬂine datasets, we perform the follows:

1. For i = 1, . . . , 5, simulate 20 mBm paths in group i (corresponding to hi ), each path is
with length of 305. Then the total number of paths N = 100. To be more explicit we
denote by








x1,1
x2,1
...

S :=

x100,1 x100,2

x1,2
x2,2
...

26

x1,305
x2,305
...

· · ·
· · ·
...
· · · x100,305








,

(75)

where each row is an mBm discrete-time path. For i = 1, . . . , 5, the data from the i th
group are given as:






S(i ) :=

x20(i −1)+1,1 x20(i −1)+1,2

...
x20i ,1

...
x20i ,2

· · · x20(i −1)+1,305
...
· · ·

...
x20i ,305




 .

(76)

2. At each t = 1, . . . , 100, we suppose to observe the ﬁrst n(t ) = 3t +5 values of each path,

i.e.,

Sofﬂine(t ) =








x1,1
x2,1
...

x1,2
x2,2
...

x100,1 x100,2

x1,3t +5
x2,3t +5
...

· · ·
· · ·
...
· · · x100,3t +5








.

The online dataset does not require observed paths to be with equal length, and can
be regarded as some extension of the ofﬂine case. Introducing the online dataset aims at
mimicking the situation where new time series are observed as time goes. In our simula-
tion study, we use the following way to construct online datasets:

1. For i = 1, . . . , 5, simulate 20 mBm paths in group i (corresponding to hi ), each path is

with length of 305 (see (75) and (76)).

2. At each t = 1, . . . , 100 and i = 1, . . . , 5, we suppose to observe the following dataset in

the i th group:

S(i )
online

(t ) =








˜x1,1
˜x2,1
...
˜xNi (t ),1

˜x1,2
˜x2,2
...
˜xNi (t ),2

· · ·
· · ·
...
· · ·

· · ·
· · ·
...
˜xNi (t ),nNi (t )(t )

· · ·
· · ·

· · ·
˜x1,n2(t )

˜x1,n1(t )








,

where

• ˜xk,l ’s are the (k, l )-coefﬁcients in S(i ) given in (76).
• Ni (t ) := 6 + (cid:98)(t − 1)/10(cid:99) denotes the number of paths in the i th group. Here (cid:98)•(cid:99)
denotes the ﬂoor number. That is, starting from 6 paths in each group, 1 new
path will be added into each group as t increases by 10.

• nl (t ) := 3 (cid:161)t − (l − 6)

+(cid:162)+ + 5, with (•)
3 new values as t increases by 1.

+

:= max(•, 0). This means each path observes

Since at each time t, the covariance structure ground truth being known, we can then
evaluate the clustering performance in terms of the so-called misclassiﬁcation rates (Peng
et al., 2019). Heuristically speaking, the misclassiﬁcation rate is then calculated by aver-
aging the proportion of mis-clustered paths in each scenario.

27

5.4 Experimental Results

We demonstrate the asymptotic consistency of our clustering algorithms by computing
the misclassiﬁcation rates using simulated ofﬂine and online datasets. More details about
such misclassiﬁcation rate are provided in Section 4 of Peng et al. (2019).

Below we summarize the simulation study results.

Case 1 (Monotonic function):

When H (•)’s are chosen to be 5 monotonic functionals of the form (73) (see the top graph
in Figure 1), the bottom graph in Figure 1 illustrates the behavior of the misclassiﬁcation
rates corresponding to Algorithm 1 applied to ofﬂine data setting (solid line), and Al-
gorithm 2 applied to online data setting (dashed line). From this result we observe the
following:

(1) Both algorithms attempt to be consistent in their circumstances, as the time t in-
creases, in the sense that the corresponding misclassiﬁcation rates are decreasing
to 0.

(2) Clustering mBms are asymptotically equivalent to clustering their tangent processes’

increments.

(3) The online algorithm seems to have an overall better performance: its misclassiﬁca-
tion rates are 5%-10% lower than that of ofﬂine algorithm. The reason may be that
at early time steps the differences among the H (•)’s are not signiﬁcantly. Unlike the
ofﬂine clustering algorithm, the online one is ﬂexible enough to catch these small
differences.

Case 2 (Periodic function):

The same converging behaviors are found in case of periodic functional form of H (•)
as speciﬁed in (74). Their trajectories are illustrated in the top graph of Figure 2. The
clustering performance shown in the bottom graph of Figure 2 indicate the following:

(1) Both misclassiﬁcation rates of the clustering algorithms have generally a declining

trend as time increases.

(2) As the differences among the periodic function H (•)’s values go up and down, the

misclassiﬁcation rates go down and up accordingly.

(3) The online clustering algorithm has an overall worse performance than the ofﬂine
one. This may be because starting from t = 20 the differences among H (•)’s be-
come signiﬁcantly large.
In this situation ofﬂine clustering algorithm can better
catch these differences, since it has larger sample size (20 paths in each group) than
the online one.

28

Finally note that in the simulation study, for each pair of paths with length n(t ), we have
taken K = n(t ) − 2 and L = 1 in (cid:99)d ∗, however any other value of K could be taken. We have
provided easily readable and editable MATLAB codes of the proposed algorithms and
simulation study replications. All the codes used in this section can be found publicly
online2.

6 Real World Application: Clustering Global Financial Mar-

kets

6.1 Motivation

In this section, we motivate the application of the proposed clustering algorithms to real
world datasets through performing cluster analysis on global equity markets. In the past,
stock returns of countries in the same region are commonly believed to have more similar
patterns and higher correlation. The reason is obvious: economic entities within closer
geographical distance have potentially more trades and thus are inﬂuenced by similar
economic factors. However, more recent empirical evidences of ﬁnancial markets reveal
that globalization is breaking the geographical barrier and is creating common economic
factors. As a result, global economic clusters switch from “geographical centriods” to
“emerging/developed economics centriods”. That is, emerging markets demonstrate
more and more similar ﬁnancial market patterns and correlations (Demirer et al., 2018),
whereas developed economic entities share increasingly ﬁnancial market similarity (Ang
and Longstaff, 2013).

The idea of modeling stock returns by locally asymptotically self-similar processes
(mBm) is pioneered by Bianchi and Pianese (2008). This time-varying self-similar feature
of the ﬁnancial markets stochastic processes is further convinced by Bianchi et al. (2013)
and Peng and Zhao (2018). We consider data from global ﬁnancial markets to be perfect
underlying stochastic processes of our proposed clustering algorithms. We further exam-
ine the connection of global ﬁnancial markets by answering whether economic entities are
better co-behaved and clustered by geographical distribution or by development level.

6.2 Data and Methodology

Two asset classes from global ﬁnancial markets are used in our empirical cluster analysis.
The equity index return captures the upside (growth) characteristics and the sovereign CDS
spread captures the downside (credit risk) characteristics of underlying economic entities:

• Equity indexes returns: We cluster the global equity indexes according to the em-
pirical time-varying covariance structure of their performance, using Algorithms 1

2https://github.com/researchcoding/clustering_locally_asymptotically_self_

similar_processes/.

29

and 2 as purposed in this paper. The index constituents of MSCI ACWI (All Coun-
try World Index), selected as underlying stochastic processes. Each of the indexes
is a realized path representing the historical monthly total returns (with dividends)
of underlying economic entities. MSCI ACWI is the leading global equity market
index and covers about 85% of the market capitalization in each market3.

• Sovereign CDS spreads: We cluster the sovereign credit default swap (CDS) spreads
of the same economic entities. The sovereign CDS is an insurance-like product that
provides default protection on bonds and other debts issued by government, nation
or large economic entity. Its spread reﬂects the cost to insurer the exposure to the
possibility of a sovereign defaulting or restructuring. We select 5-year sovereign
CDS spread as the indicator of sovereign credit risk, and 5-year products usually
have the best liquidity on the market. The same economic entities as in equity index
analysis are selected. Our data source is Bloomberg and Markit.

Through empirical study it is proved that these indexes returns exhibit the “long mem-
ory” path feature hence they can be modeled by self-similar processes or more generally
by locally asymptotically self-similar processes such as fBms and mBms (see e.g. Bianchi
and Pianese (2008) Bianchi et al. (2013) and Peng and Zhao (2018)). Therefore similar to
∗
Section 5 we may cluster the increments of the indexes returns with the log
-transformed
dissimilarity measure. We select equity indexes and sovereign CDS spreads covering 23
developed economic entities and 24 emerging markets. The detailed constituents can
found at Table 1 or https://www.msci.com/acwi: Americas, EMEA (Europe, Middle
East and Africa), Paciﬁc and Asia.

We construct both ofﬂine and online datasets for monthly returns. For monthly return
of global equity indexes, ofﬂine dataset starts from June 2005 and includes the ﬁnancial
crisis period in 2007 and 2008 and ends on November 2019. We include the global stock
market crisis period to incorporate potential credit risk contagion effect on our clustering
analysis. The online dataset starts on January 1989, which covers 1997 Asian ﬁnancial
crisis, 2003 dot-com bubble and 2007 subprime mortgage crisis, and ends on November
2019. In ofﬂine setting, each stochastic path in the dataset has 174 time series observations,
and there are 47 paths to be clustered. In online setting, the longest time series have 371
observations and shortest time series have 174 observations. The online dataset begins
with 33 economic entities and ends with 47 paths.

For monthly average on sovereign CDS spreads, we remove Netherlands, Qatar, Sin-
gapore, Greece and United Arabic from the economic entity samples due to insufﬁcient
observation. Therefore, we end up with 42 economic entities with CDS spread clustering
analysis. The ofﬂine dateset starts from June 2005 and ends on November 2019, and each
economic entity has 174 observations.

3As of December 2018, as reported on https://www.msci.com/acwi.

30

.
)
x
e
d
n
I

d
l
r
o
W
y
r
t
n
u
o
C

l
l

A

(

I

W
C
A

I

C
S
M
e
h
t

n
i

s
t
e
k
r
a
m
S
D
C
n
g
i
e
r
e
v
o
s

d
n
a

y
t
i
u
q
e

j

r
o
a
m

f
o

s
e
i
r
o
g
e
t
a
c

e
h
T

:
1

e
l
b
a
T

-
o
e
g

e
h
T

.
s
a
e
r
a

r
o

s
e
i
r
t
n
u
o
c

g
n

i
g
r
e
m
e
m
o
r
f

s
t
e
k
r
a
m
4
2
d
n
a

,
s
e
i
t
i
t
n
e

c
i
m
o
n
o
c
e
d
e
p
o
l
e
v
e
d
m
o
r
f

s
t
e
k
r
a
m
3
2

e
r
a

e
r
e
h
T

e
r
a
*
h
t
i

w
s
t
e
k
r
a
M

.

a
i
s
A
d
n
a
c
ﬁ
i
c
a
P

,
)
a
c
i
r
f
A
d
n
a
t
s
a
E
e
l
d
d
M

i

,
e
p
o
r
u
E
(

A
E
M
E

,
s
a
c
i
r
e
m
A
s
n
i
a
t
n
o
c
g
n
i
r
e
t
s
u
l
c

l
a
c
i
h
p
a
r
g

.
a
t
a
d
S
D
C
n
g
i
e
r
e
v
o
s
g
n
i
s
s
i

m

s
t
e
k
r
a
M
g
n
i
g
r
e
m
E

s
t
e
k
r
a
M
d
e
p
o
l
e
v
e
D

a
i
s
A

a
c
i
r
f
A
&

i

t
s
a
E
e
l
d
d
M
&
e
p
o
r
u
E

s
a
c
i
r
e
m
A

c
ﬁ
i
c
a
P

i

t
s
a
E
e
l
d
d
M
&
e
p
o
r
u
E

s
a
c
i
r
e
m
A

)
d
n
a
l

n
i
a
M

(
a
n
h
C

i

c
i
l
b
u
p
e
R
h
c
e
z
C

a
i
d
n
I

a
i
s
e
n
o
d
n
I

a
e
r
o
K

a
i
s
y
a
l
a
M

n
a
t
s
i
k
a
P

s
e
n
p
p

i

i
l
i

h
P

n
a
w
i
a
T

d
n
a
l
i
a
h
T

*
e
c
e
e
r
G

y
r
a
g
n
u
H

d
n
a
l
o
P

a
i
s
s
u
R

y
e
k
r
u
T

t
p
y
g
E

a
c
i
r
f
A
h
t
u
o
S

*
r
a
t
a
Q

l
i
z
a
r
B

e
l
i
h
C

a
i
b
m
o
l
o
C

o
c
i
x
e
M

u
r
e
P

a
i
l
a
r
t
s
u
A

g
n
o
K
g
n
o
H

n
a
p
a
J

d
n
a
l
a
e
Z
w
e
N

*
e
r
o
p
a
g
n
i
S

a
d
a
n
a
C

A
S
U

31

*
s
e
t
a
r
i

m
E
b
a
r
A
d
e
t
i
n
U

*
s
d
n
a
l
r
e
h
t
e
N

a
i
r
t
s
u
A

m
u
i
g
l
e
B

k
r
a
m
n
e
D

d
n
a
l
n
i
F

e
c
n
a
r
F

y
n
a
m
r
e
G

d
n
a
l
e
r
I

l
e
a
r
s
I

y
l
a
t
I

y
a
w
r
o
N

l
a
g
u
t
r
o
P

n
i
a
p
S

n
e
d
e
w
S

d
n
a
l
r
e
z
t
i

w
S

m
o
d
g
n
i
K
d
e
t
i
n
U

.

i
w
c
a
/
m
o
c
.
i
c
s
m
.
w
w
w
/
/
:
s
p
t
t
h

.

n
o
i
t
a
c
o
l
l
a

t
e
k
r
a
m

)
x
e
d
n
I
d
l
r
o
W
y
r
t
n
u
o
C

l
l

A

(

I

W
C
A

I

C
S
M

:
e
c
r
u
o
S

6.3 Clustering Results

We compare the clustering outcomes of both ofﬂine and online datasets with separations
suggested by region (4 groups: Americas, Europe & Middle East, Paciﬁc and Asia) and
development level (2 groups: emerging markets and developed markets). The cluster-
ing factor (region or development level) that has lower misclassiﬁcation rate contributes
the partition of the economics entities the most. That is, we examine whether region or
development level differentiates the ﬁnancial markets behaviors.

Table 2 shows that the misclassiﬁcation rates for development levels are signiﬁcantly
and consistently lower than that of geographical region, for ofﬂine and online settings
and for both equity and credit markets. The clustering comparison seems to convince
that development level dominates the ﬁnancial market characteristics over geographical
distance for those underlying economic entities. The best results are clustering ofﬂine
dataset using ofﬂine algorithm and clustering online dataset using online algorithm on
dividing emerging markets and developed markets. There are 30% to 50% decrease on
misclassiﬁcation rates when clustering via development level than that via region.

Table 2: The misclassiﬁcation rates of clustering algorithms on datasets, comparing to
clusters suggested by geographical region and development levels. Panel A presents the
results from clustering equity indexes, and Panel B presents the results from clustering
sovereign CDS spreads.

Panel A

Ofﬂine Algorithm

Online Algorithm

Stock Returns Regions Emerging/Developed Regions Emerging/Developed

ofﬂine dataset
online dataset

61.70%
53.19%

29.79%
44.68%

55.32%
51.06%

36.17%
14.89%

Panel B

Ofﬂine Algorithm

Online Algorithm

CDS Spreads Regions Emerging/Developed Regions Emerging/Developed

ofﬂine dataset
online dataset

64.29%
54.76%

28.57%
47.62%

71.43%
59.52%

26.19%
26.19%

Table 3 presents misclassiﬁed economic entities from the cluster analysis on group-
ing emerging markets and developed markets. For equity indexes dataset, more mis-
classiﬁcation concentrates on clustering developed economic entities into emerging mar-
ket group. Stock index returns from Austria, Finland and Portugal markets are clus-
tered as from emerging markets by both ofﬂine and online algorithms. The misclassiﬁ-
cation within developed group is rather random. For sovereign CDS spreads, more mis-
classiﬁcation is on clustering emerging economic entities into developed market group.
Sovereign CDS spreads of Chile, China (Mainland), Czech, Korea, Malaysia, Mexico,

32

Poland and Thailand are consistently misclassiﬁed into developed markets, probably due
to their low sovereign credit risk.

From both equity and credit markets, we show that clustering ﬁnancial time series
using development level outperforms the clustering outcome using region. This empiri-
cal result further supports that economic globalization is breaking the geographic barrier
and enhances the comovement of ﬁnancial market behaviors by economics strengthens
and status.

7 Conclusions and Future Prospects

We introduce the problem of clustering locally asymptotically self-similar processes. A
new covariance-based dissimilarity measure is proposed to obtain approximately asymp-
totically consistent clustering algorithms for both ofﬂine and online settings. We have
shown that the recommended algorithms are competitive for at least three reasons:

(1) Given their ﬂexibility, our algorithms are applicable to clustering any distribution
stationary ergodic processes with ﬁnite variances; any autocovariance ergodic pro-
cesses; locally asymptotically self-similar processes whose tangent processes have
autocovariance ergodic increments. The multifractional Brownian motion (mBm) is
an excellent example of the latter process.

(2) Our algorithms are efﬁcient enough in terms of their computational complexity. Sim-
ulation study is performed on clustering mBm. The results show that both ofﬂine
and online algorithms are approximately asymptotically consistent.

(3) Our algorithms are successfully applied to cluster the real world ﬁnancial time series
(equity returns and sovereign CDS spreads) via development level and via regions.
The outcomes are self-consistent with the ﬁnancial markets behavior.

Finally we list the following open problems which could be left for future research.

(1) The clustering framework proposed in our paper only focuses on the cases where the
true number of clusters κ is known. The problem for which κ is supposed to be
unknown remains open.

(2) If we drop the Gaussianity assumption the class of stationary increments self-similar
processes becomes much larger. This will yield introduction to a more general class
of locally asymptotically self-similar processes, whose autocovariances do not ex-
ist. This class includes linear multifractional stable motion (Stoev and Taqqu, 2004,
2005) as a paradigmatic example. Cluster analysis on such stable processes will no-
doubt lead to a wide range of applications, especially when the process distributions
exhibit heavy-tailed phenomena. Neither the distribution dissimilarity measure in-
troduced in Khaleghi et al. (2016) nor the covariance-based dissimilarity measures
used in this paper would work in this case, hence new techniques are required to
cluster such processes.

33

Table 3: The misclassiﬁcation outcome using ofﬂine algorithm on ofﬂine dataset and on-
line algorithm on online dataset. Panel A reports the incorrectly categorized economics
entities from equity index return clustering, and Panel B reports the incorrectly catego-
rized economics entities from sovereign CDS spreads clustering. The algorithm divides
the whole dataset into two groups: emerging market and developed markets, respec-
tively. The incorrect outcome, where (i) entities from developed markets incorrectly clus-
ters in emerging market, or (ii) vice versa, are reported in the table.

Pabel A: Equity Indexes Returns

Group 1 (Emerging Markets)

Group 2 (Developed Markets)

Incorrect - Online
Austria
Finland
Portugal

Incorrect - Ofﬂine
Korea
Chile
Philippines
Malaysia
Mexico

Incorrect - Online
Czech Republic
Qatar
Peru
South Africa

Incorrect - Ofﬂine
Austria
Finland
Germany
Ireland
Italy
Norway
Portugal
Spain
New Zealand

Pabel B: Sovereign CDS Spreads

Group 1 (Emerging Markets)

Group 2 (Developed Markets)

Incorrect - Ofﬂine
Ireland
Italy
Portugal
Spain

Incorrect - Online
Ireland
Portugal

Incorrect - Ofﬂine
Chile

Incorrect - Online
Chile

China (Mainland) China (Mainland)
Czech Republic
Czech Republic
Hungary
Korea
Korea
Malaysia
Malaysia
Mexico
Mexico
Poland
Poland
Thailand
Thailand

34

References

Aghabozorgi, S., Shirkhorshidi, A. S., and Wah, T. Y. (2015). Time-series clustering – A

decade review. Information Systems, 53:16–38.

Ang, A. and Longstaff, F. A. (2013). Systemic sovereign credit risk: Lessons from the US

and Europe. Journal of Monetary Economics, 60(5):493–510.

Ayache, A., Cohen, S., and Lévy-Véhel, J. (2000). The covariance structure of multifrac-
In 2000 IEEE
tional brownian motion, with application to long range dependence.
International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.
00CH37100), volume 6, pages 3810–3813. IEEE.

Bar-Joseph, Z., Gerber, G., Gifford, D. K., Jaakkola, T. S., and Simon, I. (2002). A new ap-
proach to analyzing gene expression time series data. In Proceedings of the Sixth Annual
International Conference on Computational Biology, pages 39–48. ACM.

Bastos, J. A. and Caiado, J. (2014). Clustering ﬁnancial time series with variance ratio

statistics. Quantitative Finance, 14(12):2121–2133.

Benassi, A., Jaffard, S., and Roux, D. (1997). Elliptic Gaussian random processes. Revista

Matemática Iberoamericana, 13(1):19–90.

Bianchi, S., Pantanella, A., and Pianese, A. (2013). Modeling stock prices by multifrac-
tional Brownian motion: an improved estimation of the pointwise regularity. Quantita-
tive Finance, 13(8):1317–1330.

Bianchi, S. and Pianese, A. (2008). Multifractional properties of stock indices decomposed
International Journal of Theoretical and

by ﬁltering their pointwise Hölder regularity.
Applied Finance, 11(6):567–595.

Boufoussi, B., Dozzi, M., and Guerbaz, R. (2008). Path properties of a class of locally
asymptotically self-similar processes. Electronic Journal of Probability, 13(29):898–921.

Bradley, P. S. and Fayyad, U. M. (1998). Reﬁning initial points for k-means clustering. In

ICML, volume 98, pages 91–99.

COMPSTAT, pages 233–238.

Chan, G. and Wood, A. T. (1998). Simulation of multifractional Brownian motion.

In

Cotofrei, P. (2002). Statistical temporal rules. In Proceedings of the 15th Conference on Com-

putational Statistics–Short Communications and Posters.

Damian, D., Orešiˇc, M., Verheij, E., Meulman, J., Friedman, J., Adourian, A., Morel, N.,
Smilde, A., and van der Greef, J. (2007). Applications of a new subspace clustering
algorithm (cosa) in medical systems biology. Metabolomics, 3(1):69–77.

35

Demirer, R., Omay, T., Yuksel, A., and Yuksel, A. (2018). Global risk aversion and emerg-

ing market return comovements. Economics Letters, 173:118–121.

Embrechts, P. and Maejima, M. (2000). An introduction to the theory of self-similar
stochastic processes. International Journal of Modern Physics B, 14(12n13):1399–1420.

Embrechts, P. and Maejima, M. (2002). Selfsimilar Processes. Princeton Series in Applied

Mathematics.

Falconer, K. (2002). Tangent ﬁelds and the local structure of random ﬁelds.

Journal of

Theoretical Probability, 15(3):731–750.

Falconer, K. (2003). The local structure of random processes. Journal of the London Mathe-

matical Society, 67(3):657–672.

Fu, T.-c., Chung, F.-l., Ng, V., and Luk, R. (2001). Pattern discovery from stock time series
using self-organizing maps. In Workshop Notes of KDD2001 Workshop on Temporal Data
Mining, pages 26–29.

Gan, G., Ma, C., and Wu, J. (2007). Data clustering: Theory, Algorithms, and Applications,

volume 20. ASA-SIAM Series on Statistics and Applied Mathematics.

Gavrilov, M., Anguelov, D., Indyk, P., and Motwani, R. (2000). Mining the stock market:

Which measure is best? In Proc. of the 6th ACM SIGKDD, pages 487–496.

Grazzini, J. (2012). Analysis of the emergent properties: stationarity and ergodicity. Jour-

nal of Artiﬁcial Societies and Social Simulation, 15(2):7.

Guha, S., Meyerson, A., Mishra, N., Motwani, R., and O’Callaghan, L. (2003). Clustering
data streams: Theory and practice. IEEE Transactions on Knowledge and Data Engineering,
15(3):515–528.

Harms, S. K., Deogun, J., and Tadesse, T. (2002a). Discovering sequential association
rules with constraints and time lags in multiple sequences. In International Symposium
on Methodologies for Intelligent Systems, pages 432–441.

Harms, S. K., Goddard, S., Reichenbach, S. E., Waltman, W. J., and Tadesse, T. (2002b).
Data mining in a geospatial decision support system for drought risk management. In
Proceedings of the 1st National Conference on Digital Government, pages 9–16.

Honda, R., Wang, S., Kikuchi, T., and Konishi, O. (2002). Mining of moving objects from
time-series images and its application to satellite weather imagery. Journal of Intelligent
Information Systems, 19(1):79–93.

Ieva, F., Paganoni, A. M., and Tarabelloni, N. (2016). Covariance-based clustering in mul-
tivariate and functional data analysis. Journal of Machine Learning Research, 17:1–21.

36

Jääskinen, V., Parkkinen, V., Cheng, L., and Corander, J. (2014). Bayesian clustering of
DNA sequences using Markov chains and a stochastic partition model. Statistical Ap-
plications in Genetics and Molecular Biology, 13(1):105–121.

Jain, A. K., Murty, M. N., and Flynn, P. J. (1999). Data clustering: a review. ACM Computing

Surveys (CSUR), 31(3):264–323.

Java, A. and Perlman, E. S. (2002). Predictive mining of time series data. In Bulletin of the

American Astronomical Society, volume 34, page 741.

Jin, X., Lu, Y., and Shi, C. (2002a). Distribution discovery: Local analysis of temporal rules.

In Paciﬁc-Asia Conference on Knowledge Discovery and Data Mining, pages 469–480.

Jin, X., Wang, L., Lu, Y., and Shi, C. (2002b). Indexing and mining of the local patterns in
sequence database. In International Conference on Intelligent Data Engineering and Auto-
mated Learning, pages 68–73.

Juozapaviˇcius, A. and Rapsevicius, V. (2001). Clustering through decision tree construc-

tion in geology. Nonlinear Analysis: Modelling and Control, 6(2):29–41.

Keogh, E., Chakrabarti, K., Pazzani, M., and Mehrotra, S. (2001). Dimensionality reduc-
tion for fast similarity search in large time series databases. Knowledge and Information
Systems, 3(3):263–286.

Keogh, E. and Kasetty, S. (2003). On the need for time series data mining benchmarks:
a survey and empirical demonstration. Data Mining and Knowledge Discovery, 7(4):349–
371.

Keogh, E. and Lin, J. (2005). Clustering of time-series subsequences is meaningless: impli-
cations for previous and future research. Knowledge and Information Systems, 8(2):154–
177.

Khaleghi, A., Ryabko, D., Mary, J., and Preux, P. (2012). Online clustering of processes. In

Artiﬁcial Intelligence and Statistics, pages 601–609.

Khaleghi, A., Ryabko, D., Mary, J., and Preux, P. (2016). Consistent algorithms for cluster-

ing time series. Journal of Machine Learning Research, 17(3):1–32.

Krengel, U. (1985). Ergodic Theorems. Walter de Gruyter.

Li, C.-S., Yu, P. S., and Castelli, V. (1998). Malm: A framework for mining sequence
database at multiple abstraction levels. In Proceedings of the Seventh International Confer-
ence on Information and Knowledge Management, pages 267–272.

Lin, J., Keogh, E., Lonardi, S., and Patel, P. (2002). Finding motifs in time series.

In

Proceedings of the 2nd Workshop on Temporal Data Mining, pages 53–68.

37

Mandelbrot, B. B. and Van Ness, J. W. (1968). Fractional Brownian motions, fractional

noises and applications. SIAM Review, 10(4):422–437.

Mantegna, R. N. (1999). Hierarchical structure in ﬁnancial markets. The European Physical

Journal B-Condensed Matter and Complex Systems, 11(1):193–197.

Oates, T. (1999). Identifying distinctive subsequences in multivariate time series by clus-

tering. In KDD, volume 99, pages 322–326.

Pavlidis, N. G., Plagianakos, V. P., Tasoulis, D. K., and Vrahatis, M. N. (2006). Financial
forecasting through unsupervised clustering and neural networks. Operational Research,
6(2):103–127.

Peltier, R. F. and Lévy-Véhel, J. (1995). Multifractional Brownian motion: deﬁnition and
preliminary results. Technical Report 2645, Institut National de Recherche en Informatique
et en Automatique, INRIA, France.

Peng, Q., Rao, N., and Zhao, R. (2019). Covariance-based dissimilarity measures applied
to clustering wide-sense stationary ergodic processes. Machine Learning, 108(12):2159–
2195.

Peng, Q. and Zhao, R. (2018). A general class of multifractional processes and stock price

informativeness. Chaos, Solitons & Fractals, 115:248–267.

Rubinstein, M., Joulin, A., Kopf, J., and Liu, C. (2013). Unsupervised joint object discovery
In The IEEE Conference on Computer Vision and

and segmentation in internet images.
Pattern Recognition (CVPR), pages 1939–1946.

Samorodnitsky, G. (2004). Extreme value theory, ergodic theory and the boundary be-
tween short memory and long memory for stationary stable processes. The Annals of
Probability, 32(2):1438–1468.

Samorodnitsky, G. and Taqqu, M. S. (1994). Stable Non-Gaussian Random Processes: Stochas-

tic Models with Inﬁnite Variance. Chapman & Hall, New York.

Sardá-Espinosa, A. (2017). Comparing time-series clustering algorithms in R using the

dtwclust package. Vienna: R Development Core Team.

Slonim, N., Atwal, G. S., Tka ˇcik, G., and Bialek, W. (2005). Information-based clustering.

PNAS, 102(51):18297–18302.

Stoev, S. and Taqqu, M. S. (2004). Stochastic properties of the linear multifractional stable

motion. Advances in Applied Probability, 36(4):1085–1115.

Stoev, S. and Taqqu, M. S. (2005). Path properties of the linear multifractional stable

motion. Fractals, 13(02):157–178.

38

Stoev, S. A. and Taqqu, M. S. (2006). How rich is the class of multifractional Brownian

motions? Stochastic Processes and their Applications, 116(2):200–221.

Tino, P., Schittenkopf, C., and Dorffner, G. (2000). Temporal pattern recognition in noisy
non-stationary time series based on quantization into symbolic streams. lessons learned
from ﬁnancial volatility trading. SFB Adaptive Information Systems and Modelling in Eco-
nomics and Management Science.

Truppel, W., Keogh, E., and Lin, J. (2003). A hidden constraint when clustering streaming

time series. Technical report, UCR.

Wood, A. T. and Chan, G. (1994). Simulation of stationary Gaussian processes in [0, 1]d .

Journal of Computational and Graphical Statistics, 3(4):409–432.

Yairi, T., Kato, Y., and Hori, K. (2001). Fault detection by mining association rules from
house-keeping data. In Proceedings of the 6th International Symposium on Artiﬁcial Intelli-
gence, Robotics and Automation in Space, volume 18, page 21. Citeseer.

Zhao, W., Zou, W., and Chen, J. J. (2014). Topic modeling for cluster analysis of large

biological and medical datasets. BMC Bioinformatics, 15:S11.

39

Figure 1: The functional form of H (•) follows Eq.
(73): H (t ) = 0.5 + hi · t /100 with
t = 0, 1, . . . , 100. The top graph plots H (•) corresponding to 5 different clusters. The bot-
tom graph illustrates the misclassiﬁcation rates output by (i ) ofﬂine algorithm on ofﬂine
dataset (solid line) and (i i ) online algorithm on online dataset (dashed line). Both al-
∗
- transformed covariance-based dissimilarity
gorithms are performed based on the log
measure.

40

Figure 2: The functional form of H (•) follows Eq. (74): H (t ) = 0.5 + hi · sin(πt /100) with
t = 0, 1, . . . , 100. The top graph plots H (•) corresponding to 5 different clusters. The bot-
tom graph illustrates the misclassiﬁcation rates output by (i ) ofﬂine algorithm on ofﬂine
dataset (solid line) and (i i ) online algorithm on online dataset (dashed line). Both al-
∗
- transformed covariance-based dissimilarity
gorithms are performed based on the log
measure.

41

