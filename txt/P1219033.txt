9
1
0
2
 
l
u
J
 
1
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
9
4
0
9
0
.
1
0
8
1
:
v
i
X
r
a

Covariance-based Dissimilarity Measures Applied
to Clustering Wide-sense Stationary Ergodic
Processes

Qidi Peng∗

Nan Rao†

Ran Zhao‡

Abstract

We introduce a new unsupervised learning problem: clustering wide-
sense stationary ergodic stochastic processes. A covariance-based dis-
similarity measure together with asymptotically consistent algorithms is
designed for clustering oﬄine and online datasets, respectively. We also
suggest a formal criterion on the eﬃciency of dissimilarity measures, and
discuss of some approach to improve the eﬃciency of our clustering algo-
rithms, when they are applied to cluster particular type of processes, such
as self-similar processes with wide-sense stationary ergodic increments.
Clustering synthetic data and real-world data are provided as examples
of applications.

cluster analysis ¨ wide-sense stationary ergodic processes ¨

Keywords:
covariance-based dissimilarity measure ¨ self-similar processes
MCS (2010): 62-07 ¨ 60G10 ¨ 62M10

∗Institute of Mathematical Sciences, Claremont Graduate University, Claremont, CA

91711. Email: qidi.peng@cgu.edu.

nan.rao@sjtu.edu.cn.

†School of Mathematical Sciences, Shanghai Jiao Tong University, Shanghai, China. Email:

‡Institute of Mathematical Sceinces and Drucker School of Management, Claremont Grad-

uate University, Claremont, CA 91711. Email: ran.zhao@cgu.edu.

1

1

Introduction

Cluster analysis, as a core category of unsupervised learning techniques, allows
to discover hidden patterns in data where one does not know the true answer
upfront. Its goal is to assign a heterogeneous set of objects into non-overlapping
clusters, where in each cluster any two objects are more related to each other
than to objects in other clusters. Given its exploratory nature, clustering has
nowadays a number of applications in various ﬁelds of both industry and sci-
entiﬁc research, such as biological and medical research (Damian et al, 2007;
Zhao et al, 2014; J¨a¨askinen et al, 2014), information technology (Jain et al,
1999; Slonim et al, 2005), signal and image processing (Rubinstein et al, 2013),
geology (Juozapaviˇcius and Rapsevicius, 2001) and ﬁnance (Pavlidis et al, 2006;
Bastos and Caiado, 2014; Ieva et al, 2016). There exists a rich literature of clus-
ter analysis on random vectors, where the objects, waiting to be clustered, are
sampled from high-dimensional joint distributions. There is no shortage of such
clustering algorithms (Xu and Wunsch, 2005). However, stochastic processes are
quite a diﬀerent setting from random vectors, since their observations (sample
paths) are sampled from processes distributions. While the cluster analysis on
random vectors has developed aggressively, clustering on stochastic processes
receives much less attention. Today cluster analysis on stochastic processes
deserves increasingly intense study, thanks to their vital importance to many
applied areas, where the collected information are indexed by real time and are
especially long. Examples of these time-indexed information include biological
data, ﬁnancial data, marketing data, surface weather data, geological data and
video/audio data, etc.

Recall that in the setting of random vectors, a process of clustering often

consists of two steps:

Step 1 One suggests a suitable dissimilarity measure to describe the distance
between 2 objects, under which “two objects are close to each other”
becomes meaningful.

Step 2 One designs an enough accurate and computationally eﬃcient cluster-

ing function based on the above dissimilarity measure.

Clustering stochastic processes is performed in a similar way but new challenges
may arise in both Step 1 and Step 2. Intuitively, one can always apply existing
random vectors clustering approaches to cluster arbitrary stochastic processes,
such as non-hierarchical approaches (K-means clustering methods) and hierar-
chical approaches (agglomerative method, divisive method) (Hartigan, 1975),
based on “naive” dissimilarity measures (e.g., Euclidean distance, Manhattan
distance or Minkowski distance). However, one faces at least 2 potential risks
when applying the above approaches to clustering stochastic processes:

Risk 1 These approaches might suﬀer from their huge complexity costs, due
to the great length of their sample paths. As a result classical clustering
algorithms are often computationally forbidding (Ieva et al, 2016; Peng
and M¨uller, 2008).

2

Risk 2 These approaches might suﬀer from over-ﬁtting issues. For example,
clustering stationary or periodic processes based on Euclidean distance
between the paths, without considering their path properties will result in
“over ﬁtting, bad clusters” situation.

In summary, classical dissimilarity measures or clustering strategies would fail
in clustering stochastic processes.

Fortunately, the complexity cost and the over-ﬁtting errors of clustering
processes could be largely reduced, if one is aware of the fact that a stochastic
process often possesses ﬁne paths features (e.g., stationarity, Markov property,
self-similarity, sparsity, seasonality, etc.), which is unlike an arbitrary random
vector. An appropriate dissimilarity measure then should be chosen to be able
to capture these paths features. Clustering processes is then performed to group
any two sample paths into one group, if they are relatively close to each other
under that particular dissimilarity measure. Below are some examples provided
in the literature.

Peng and M¨uller (2008) proposed a dissimilarity measure between two special
sample paths of processes. In their setting it is supposed that, for each path only
sparse and irregularly spaced measurements with additional measurement errors
are available. Such features occur commonly in longitudinal studies and online
trading data. Based on this particular dissimilarity measure, classiﬁcation and
cluster analysis could be made. Ieva et al (2016) developed a new algorithm to
perform clustering of multivariate and functional data, based on a covariance-
based dissimilarity measure. Their attention is focused on the speciﬁc case of a
set of observations from two populations, whose probability distributions have
equal mean but diﬀer in terms of covariances. Khaleghi et al (2016) designed
consistent algorithms for clustering strict-sense stationary ergodic processes (see
the forthcoming Eq. (1.4) for the deﬁnition of strict-sense ergodicity), where
the dissimilarity measure is proposed as distance of process distributions. It is
worth noting that the consistency of their algorithms is guaranteed thanks to
the assumption of strict-sense ergodicity.

In this framework, we aim to design asymptotically consistent algorithms to
cluster a general class of stochastic processes, i.e., wide-sense stationary ergodic
processes (see Deﬁnition 1.1 below). Asymptotically consistent algorithms can
be obtained for this setting, since the covariance stationarity and ergodicity
allow the process to present some featured asymptotic behavior with respect to
their length, rather than to the total number of paths.

Deﬁnition 1.1 (Wide-sense stationary ergodic process). A stochastic process
X “ tXtutPT (the time indexes set T can be either R` “ r0, `8q or N “
t1, 2 . . .u) is called wide-sense stationary if its mean and covariance structure
are ﬁnite and time-invariant: EpXtq “ µ for any t P T , and for any subset
pXi1 , . . . , Xir q, its covariance matrix remains invariant subject to any time shift
h ą 0:

CovpXi1 , . . . , Xir q “ CovpXi1`h, . . . , Xir`hq.
Denote by γ the auto-covariance function of X. Then X is further called weakly

3

ergodic (or wide-sense ergodic) if it is ergodic for the mean and the second-order
moment:

• If X is a continuous-time process (e.g., T “ R`), then it satisﬁes for any

ż

s`h

1
h

s

Xu du

a.s.
ÝÝÝÝÑ
hÑ`8

µ,

pXu`τ ´ µqpXu ´ µq du

a.s.
ÝÝÝÝÑ
hÑ`8

γpτ q, for all τ P R`,

ÝÝÑ denotes the almost sure convergence (convergence with proba-

• If X is a discrete-time process (e.g., T “ N), then it satisﬁes for any

Xs ` Xs`1 ` . . . ` Xs`h
h ` 1

a.s.
ÝÝÝÝÝÝÝÝÑ
hPN, hÑ`8

µ,

s P R`,

and

ż

s`h

1
h

s

where a.s.
bility 1).

s P N Y t0u,

and

ř

s`h
u“spXu`τ ´ µqpXu ´ µq
h ` 1

a.s.
ÝÝÝÝÝÝÝÝÑ
hPN, hÑ`8

γpτ q, for all τ P N Y t0u.

Wide-sense stationarity and ergodicity are believed to be a very general

assumption, at least in the following senses:

1. The assumption that each process is generated by some mean and co-
variance structure is suﬃcient for capturing all features of a wide-sense
stationary ergodic process. In other words, our algorithms intend to clus-
ter means and auto-covariance functions, not process distributions.

2. Wide-sense stationary ergodic process partially extends the strict-sense
one. A ﬁnite-variance strict-sense stationary ergodic process (see Eq. (1.4)
for its deﬁnition) is also wide-sense stationary ergodic. However strict-
sense stationary ergodic stable processes are not wide-sense stationary,
because their variances explode (Cambanis et al, 1987; Samorodnitsky,
2004).

3. A Gaussian process can be fully identiﬁed only by its mean and covariance
structure. Then a wide-sense stationary ergodic Gaussian process is also
strict-sense stationary ergodic.

4. In the clustering problem, the dependency among the sample paths can

be arbitrary.

4

There is a long list of processes which are wide-sense stationary ergodic, but not
necessarily stationary in the strict sense. The examples of wide-sense stationary
processes below are not exhausted.

Example 1 Non-independent White Noise.

Let U be a random variable uniformly distributed over p0, 2πq and deﬁne

?

Zptq :“

2 cosptU q, for t P N.

The process Z “ tZptqutPN is then a white noise because it veriﬁes

EpZptqq “ 0, VarpZptqq “ 1 and CovpZpsq, Zptqq “ 0, for s ‰ t.

We claim that Z is wide-sense stationary ergodic, which can be obtained
by using the Kolmogorov’s strong law of large numbers, see e.g. Theorem
2.3.10 in Sen and Singer (1993). However Z is not strict-sense stationary
since

pZp1q, Zp2qq ‰ pZp2q, Zp3qq in law.

Indeed, it is easy to see that

`
0 ă E

Zp1q2Zp2q

˘

`
‰ E

˘
Zp2q2Zp3q

“ 0.

Example 2 Auto-regressive Models.

It is well-known that an auto-regressive model tY ptqut „ ARp1q in the
form:

Y ptq “ aY pt ´ 1q ` Zptq, |a| ă 1, a ‰ 0, for t P N

(1.1)

is wide-sense stationary ergodic. However it is not necessarily strict-sense
stationary ergodic, when the joint distributions of the white noise tZptqut
are not invariant with time-shifting (e.g., take tZptqut to be the white
noise in Example 1).

Example 3 Increment Process of Fractional Brownian Motion.

Let tBH ptqut be a fractional Brownian motion with Hurst index H P p0, 1q
(see Mandelbrot and van Ness (1968)). For each h ą 0, its increment pro-
cess tZ hptq :“ BH pt`hq´BH ptqut is ﬁnite-variance strict-sense stationary
ergodic (Magdziarz and Weron, 2011). As a result it is also wide-sense sta-
tionary ergodic. More detail will be discussed in Section 4.

Example 4 Increment Process of More General Gaussian Processes.

Peng (2012) introduced a general class of zero-mean Gaussian processes
X “ tXptqutPR having stationary increments.
Its variogram νptq :“
2´1EpXptq2q satisﬁes:

(1) There is a non-negative integer d such that ν is 2d-times continu-
ously diﬀerentiable over r´2, 2s, but not 2pd ` 1q-times continuously
diﬀerentiable over r´2, 2s.

5

(2) There are 2 real numbers c ‰ 0 and s0 P p0, 2q, such that for all

t P r´2, 2s,

νptq “ νp2dqp0q ` c|t|s0 ` rptq,

where the remainder rptq satisﬁes:

• rptq “ op|t|s0 q, as t Ñ 0.
• There are two real numbers c1 ą 0, ω ą s0 and an integer
q ą ω ` 1{2 such that r is q-times continuously diﬀerentiable
on r´2, 2szt0u and for all t P r´2, 2szt0u, we have

|rpqqptq| ď c1|t|ω´q.

It is shown that the process X extends fractional Brownian motion and it
also has wide-sense (and strict-sense) stationary ergodic increments when
d ` s0{2 P p0, 1q (see Proposition 3.1 in Peng (2012)).

The problem of clustering processes via their means and covariance structures
leads us to formulating our clustering targets in the following way.

Deﬁnition 1.2 (Ground-truth G of covariance structures). Let

(cid:32)
G1, . . . , Gκ

(

G “

be a partitioning of N “ t1, 2, . . .u into κ disjoint sets Gk, k “ 1, . . . , κ, such
that the means and covariance structures of xi, i P N are identical, if and only
if i P Gk for some k “ 1, . . . , κ. Such G is called ground-truth of covariance
structures. We also denote by G|N the restriction of G to the ﬁrst N sequences:

G|N “

(cid:32)
Gk X t1, . . . , N u : k “ 1, . . . , κ

(
.

Our clustering algorithms will aim to output the ground-truth partitioning
G, as the sample length grows. Before stating these algorithms, we introduce
the inspiring framework done by Khaleghi et al (2016).

1.1 Preliminary Results: Clustering Strict-sense Station-

ary Ergodic Processes

Khaleghi et al (2016) considered the problem of clustering strict-sense stationary
ergodic processes. The main fruit in Khaleghi et al (2016) is obtaining the so-
called asymptotically consistent algorithms to cluster processes of that type. We
brieﬂy state their work below. Depending on how the information is collected,
the stochastic processes clustering problems consist of dealing with two models:
oﬄine setting and online setting.

Oﬄine setting: The observations are assumed to be a ﬁnite number N of

paths:

´

¯

´

x1 “

X p1q

1 , . . . , X p1q
n1

, . . . , xN “

X pN q
1

, . . . , X pN q
nN

¯
.

6

Each path is generated by one of the κ diﬀerent unknown process dis-
tributions. In this case, an asymptotically consistent clustering function
should satisfy the following.

Deﬁnition 1.3 (Consistency: oﬄine setting). A clustering function f is
consistent for a set of sequences S if f pS, κq “ G. Moreover, denoting
n “ mintn1, . . . , nN u, f is called strongly asymptotically consistent in the
oﬄine sense if with probability 1 from some n on it is consistent on the
set S, i.e.,

¯

´

P

lim
nÑ8

f pS, κq “ G

“ 1.

It is called weakly asymptotically consistent if

Ppf pS, κq “ Gq “ 1.

lim
nÑ8

Online setting: In this setting the observations, having growing length and

number of scenarios with respect to time t, are denoted by
¯

´

x1 “

X p1q

1 , . . . , X p1q
n1

, . . . , xN ptq “

´
X pN ptqq
1

, . . . , X pN ptqq
nN ptq

¯
,

where the index function N ptq is non-decreasing with respect to t.
Then an asymptotically consistent online clustering function is deﬁned
below:

Deﬁnition 1.4 (Consistency: online setting). A clustering function is
strongly (RESP. weakly) asymptotically consistent in the online sense, if
for every N P N the clustering f pSptq, κq|N is strongly (RESP. weakly)
asymptotically consistent in the oﬄine sense, where f pSptq, κq|N is the
clustering f pSptq, κq restricted to the ﬁrst N sequences:

f pSptq, κq|N “ tf pSptq, κq X t1, . . . , N u : k “ 1, . . . , κu .

There is a detailed discussion on the comparison of oﬄine and online settings in
Khaleghi et al (2016), stating that these two settings have signiﬁcant diﬀerences,
since using the oﬄine algorithm in the online setting by simply applying it to the
entire data observed at every time step, does not result in an asymptotically
consistent algorithm. Therefore separately and independently studying these
two settings becomes necessary and meaningful.

As the main results in Khaleghi et al (2016), asymptotically consistent clus-
tering algorithms for both oﬄine and online settings are designed. They are
then successfully applied to clustering synthetic and real data sets.

Note that in the framework of Khaleghi et al (2016), a key step is intro-
duction to the so-called distributional distance (Gray, 1988): the distributional
distance between a pair of process distributions ρ1, ρ2 is deﬁned to be

dpρ1, ρ2q “

wmwl

|ρ1pBq ´ ρ2pBq| ,

(1.2)

8ÿ

ÿ

m,l“1

BPBm,l

where:

7

• The sets Bm,l, m, l ě 1 are obtained via the partitioning of Rm into cubes

of dimension m and volume 2´ml, starting at the origin.

• The sequence of weights twjujě1 is positive and decreasing to zero. More-
over it should be chosen such that the series in (1.2) is convergent. The
weights are often suggested to give precedence to earlier clusterings, pro-
tecting the clustering decisions from the presence of the newly observed
sample paths, whose corresponding distance estimates may not yet be ac-
curate. For instance, it is set to be wj “ 1{jpj ` 1q in Khaleghi et al
(2016).

Further, the distance between two sample paths x1, x2 of stochastic processes
is given by

mnÿ

lnÿ

ÿ

pdpx1, x2q “

m“1

l“1

BPBm,l

wmwl

|νpx1, Bq ´ νpx2, Bq|,

(1.3)

where:

• mn, ln (ď n) can be arbitrary sequences of positive integers increasing to

inﬁnity, as n Ñ 8.

• For a process path x “ pX1, . . . , Xnq, and an event B, νpx, Bq denotes
the average times that the event B occurs over n ´ mn ` 1 time intervals.
More precisely,

νpx, Bq :“

1
n ´ mn ` 1

n´mn`1ÿ

i“1

1tpXi, . . . , Xi`mn´1q P Bu.

The process distribution X from which x is sampled is called strictly ergodic if

´

P

lim
nÑ8

¯
νpx, Bq “ PpX P Bq

“ 1, for all B.

(1.4)

The assumption that the processes are ergodic leads to that pd is a strongly
consistent estimator of d:
´

¯

P

lim
nÑ8

pdpx1, x2q “ dpρ1, ρ2q

“ 1,

where ρ1, ρ2 are the process distributions corresponding to x1, x2, respectively.
Based on the distances d and their estimates pd, the asymptotically consis-
tent algorithms for clustering stationary ergodic processes in each of the oﬄine
and online settings are provided (see Algorithms 1, 2 and Theorems 11, 12 in
Khaleghi et al (2016)). Khaleghi et al (2016) also show that their methods
can be implemented eﬃciently: they are at most quadratic in each of their
arguments, and are linear (up to log terms) in some formulations.

8

1.2 Statistical Setting: Clustering Wide-sense Stationary

Ergodic Processes

Inspired by the framework of Khaleghi et al (2016), we consider the problem
of clustering wide-sense stationary ergodic processes. We ﬁrst introduce the
following covariance-based dissimilarity measure, which is one of the main con-
tributions of this paper.

Deﬁnition 1.5. (Covariance-based dissimilarity measure) The covariance-based
dissimilarity measure d˚ between a pair of processes X p1q, X p2q (in fact X p1q,
X p2q denote two covariance structures, each may contain diﬀerent process dis-
tributions) is deﬁned as follows:

`

˘

8ÿ

d˚

X p1q, X p2q

:“

wmwl

´´

`
E

X p1q

ˆM

m,l“1
˘

`

l...l`m´1

, Cov

X p1q

l...l`m´1

˘¯

´

`
E

X p2q

,

˘

`

, Cov

X p2q

l...l`m´1

l...l`m´1

˘¯¯

,

(1.5)

where:

• For j “ 1, 2, tX pjq

l ulPN denotes some path sampled from the process X pjq.
We assume that all possible observations of the process X pjq is a subset
l ulPN. For l1 ě l ě 1, we deﬁne the shortcut notation X pjq
of tX pjq
l...l1 :“
pX pjq
, . . . , X pjq
l

l1 q.

• The function M is deﬁned by: for any p1, p2, p3 P N, any 2 vectors v1, v2 P

Rp1 and any 2 matrices A1, A2 P Rp2ˆp3,

Mppv1, A1q, pv2, A2qq :“ |v1 ´ v2| ` ρ˚ pA1, A2q .

(1.6)

• The distance ρ˚ between 2 equal-sized matrices M1, M2 is deﬁned to be

ρ˚pM1, M2q :“ }M1 ´ M2}F ,

(1.7)

with } ¨ }F being the Frobenius norm:
for an arbitrary matrix M “ tMijui“1,...,m;j“1,...,n,

}M }F :“

|Mij|2.

g
f
f
e

mÿ

nÿ

i“1

j“1

Introduction to the matrices distance ρ˚ is inspired by Herdin et al (2005).
The matrices distance given in Herdin et al (2005) is used to measure the
distance between 2 correlation matrices. However, our distance ρ˚ is a
modiﬁcation of the one in the latter paper. Indeed, unlike Herdin et al
(2005), ρ˚ is a well-deﬁned metric distance, as it satisﬁes the triangle
inequalities.

9

• The sequence of positive weights twju is chosen such that d˚pX p1q, X p2qq
is ﬁnite. Observe that the distances | ¨ | and ρ˚ in (1.5) do not depend on
l, as a result we necessarily have

8ÿ

l“1

wl ă `8.

(1.8)

In practice a typical choice of weights we suggest is wj “ 1{jpj ` 1q, j “
1, 2, . . .. This is because, for most of the well-known covariance stationary
ergodic processes (causal ARM App, qq, increments of fractional Brownian
motions, etc.), their auto-covariance functions are absolutely summable:
denote by γX the auto-covariance function of tXtut,

`8ÿ

h“´8

|γX phq| ă `8.

(1.9)

`Slęzak (2017) pointed out that (1.9) is a suﬃcient condition for tXtu be-
ing mean-ergodic. However (1.9) does not necessarily imply that tXtu is
covariance-ergodic. It becomes a suﬃcient and necessary condition if tXtu
is Gaussian. Therefore subject to (1.9), taking wj “ 1{jpj ` 1q, we obtain
for any integer N ą 0,

Nÿ

ˇ
ˇ
ˇE

´
X p1q

wmwl

¯

´

l...l`m´1

´ E

X p2q

l...l`m´1

¯ˇ
ˇ
ˇ

m,l“1

Nÿ

“

m,l“1

?

`8ÿ

m,l“1

wmwl

m|µ1 ´ µ2| “ |µ1 ´ µ2|

Nÿ

m,l“1

?

1
mpm ` 1qlpl ` 1q

ď |µ1 ´ µ2|

?

1
mpm ` 1qlpl ` 1q

ă `8,

(1.10)

10

`
with µj “ E

X pjq
1

˘

, for j “ 1, 2; and

wmwlρ˚

Cov

´
X p1q

¯

´
X p2q

, Cov

l...l`m´1

l...l`m´1

¯¯

Nÿ

m,l“1

Nÿ

ď

“

ď

m,l“1

Nÿ

m,l“1

Nÿ

m,l“1
Nÿ

´

g
f
f
e2

g
f
f
e2

g
f
f
e2m

?

mÿ

mÿ

k1“1

k2“1

m´1ÿ

q“´pm´1q

m´1ÿ

wmwl

wmwl

wmwl

pγX p|k1 ´ k2|qq2

pm ´ |q|q pγX p|q|qq2

pγX p|q|qq2

q“´pm´1q

ď c

m,l“1

2m
mpm ` 1qlpl ` 1q
ř

where the constant c “
bining (1.10) and (1.11) leads to

ď c

`8ÿ

m,l“1

?

2m
mpm ` 1qlpl ` 1q

ă `8,(1.11)

8

q“´8 |CovpX1, X1`|q|q| ă `8. Therefore com-

`

˘

d˚

X p1q, X p2q

ă `8.

Hence d˚pX p1q, X p2qq in (1.5) is well-deﬁned.

In (1.5) and (1.6) we see that the behavior of the dissimilarity measure d˚ is
jointly explained by the Euclidean distance of means and the matrices distance
of covariance matrices. If the means of the processes X p1q and X p2q are priorly
known to be equal, the distance d˚ can be simpliﬁed to:

`

˘

8ÿ

d˚

X p1q, X p2q

“

´

`

wmwlρ˚

Cov

X p1q

l...l`m´1

˘

`

, Cov

X p2q

l...l`m´1

˘¯

.

(1.12)

m,l“1

Note that this dissimilarity measure can be applied on self-similar processes,
since they are all zero-mean (see Section 3).

Next we provide consistent estimator of d˚pX p1q, X p2qq. For 1 ď l ď n and
m ď n ´ l ` 1, deﬁne µ˚pXl...n, mq to be the empirical mean of a process X’s
sample path pXl, . . . , Xnq:

µ˚pXl...n, mq :“

pXi . . . Xi`m´1qT ,

(1.13)

1
n ´ m ´ l ` 2

n´m`1ÿ

i“l

and deﬁne ν˚pXl...n, mq to be the empirical covariance matrix of pXl, . . . , Xnq:

ν˚pXl...n, mq

:“

pXi . . . Xi`m´1qT pXi . . . Xi`m´1q

(1.14)

n´m`1ÿ

1
n ´ m ´ l ` 2
´µ˚pXl...n, mqµ˚pXl...n, mqT ,

i“l

11

where M T denotes the transpose of the matrix M .

Recall that the notion of wide-sense ergodicity is given in Deﬁnition 1.1. The
ergodicity theorem concerns what information can be derived from an average
over time about the ensemble average at each point of time. For the wide-sense
stationary ergodic process X, being either continuous-time or discrete-time, the
following statement holds: every empirical mean µ˚pXl...n, mq is a strongly con-
sistent estimator of the path mean EpXl...l`m´1q; and every empirical covariance
matrix ν˚pXl...n, mq is a strongly consistent estimator of the covariance matrix
CovpXl...l`m´1q under the Frobenius norm, i.e., for all m ě 1, we have

´

P

lim
nÑ8

¯
|µ˚pXl...n, mq ´ EpXl...l`m´1q| “ 0

“ 1

´

P

and

¯
}ν˚pXl...n, mq ´ CovpXl...l`m´1q}F “ 0
Next we introduce the empirical covariance-based dissimilarity measure xd˚, serv-
ing as a consistent estimator of the covariance-based dissimilarity measure d˚.

lim
nÑ8

“ 1.

Deﬁnition 1.6 (Empirical covariance-based dissimilarity measure). Given two
processes’ sample paths xj “ pX pjq
1 , . . . , X pjq
nj q, j “ 1, 2. Let n “ mintn1, n2u,
we deﬁne the empirical covariance-based dissimilarity measure between x1 and
x2 by

xd˚px1, x2q :“
´´

mnÿ

n´m`1ÿ

m“1

l“1

wmwl

ˆM

µ˚pX p1q

l...n, mq, ν˚pX p1q

l...n, mq

l...n, mq, ν˚pX p2q

l...n, mq

¯

´
µ˚pX p2q

,

¯¯
.(1.15)

The empirical covariance-based dissimilarity measure between a sample path xi
and a process X pjq (i, j P t1, 2u) is deﬁned by
mnÿ

n´m`1ÿ

xd˚pxi, X pjqq :“
´´

m“1

l“1

ˆM

µ˚pX piq

l...n, mq, ν˚pX piq

wmwl

´

´

¯
l...n, mq

,

E

X pjq

l...l`m´1

, Cov

¯

´
X pjq

l...l`m´1

¯¯¯

.

(1.16)

Unlike the dissimilarity measure d˚ which describes some distance between
stochastic processes, the empirical covriance-based dissimilarity measure is some
distance between two sample paths (ﬁnite-length vectors). We will show in the
forthcoming Lemma 1.8 that xd˚ is a consistent estimator of d˚.

Two observed sample paths possibly have distinct lengths n1, n2, therefore
in (1.15) we consider computing the distances between their subsequences of
length n “ mintn1, n2u.
In practice we usually take mn “ tlog nu, the ﬂoor
number of log n.

It is easy to verify that both d˚ and xd˚ satisfy the triangle inequalities,
thanks to the fact that both the Euclidean distance and ρ˚ satisfy the triangle
inequalities. More precisely, the following holds.

12

Remark 1.7. Thanks to (1.7) and the deﬁnitions of d˚ (see (1.5)) and xd˚
(see (1.15)), we see that the triangle inequality holds for the covariance-based
dissimilarity measure d˚, as well as for its empirical estimate xd˚. Therefore
for arbitrary processes X piq, i “ 1, 2, 3 and arbitrary ﬁnite-length sample paths
xi, i “ 1, 2, 3, we have

`

˘

`

X p1q, X p2q
d˚
xd˚px1, x2q ď
˘
xd˚
x1, X p1q

X p1q, X p3q

ď d˚
xd˚px1, x3q `
`
x1, X p2q
ď

` d˚
xd˚px2, x3q,
˘
` d˚

xd˚

`

`

˘

X p1q, X p2q

.

˘

`

˘

X p2q, X p3q

,

Remark 1.7 together with the fact that the processes are weakly ergodic,
leads to Lemma 1.8 below, which is the key to demonstrate that our clustering
algorithms in the forthcoming section are asymptotically consistent.

Lemma 1.8. Given two paths

´

¯

x1 “

X p1q

1 , . . . , X p1q
n1

and x2 “

1 , . . . , X p2q
n2

,

´
X p2q

¯

sampled from the wide-sense stationary ergodic processes X p1q and X p2q respec-
tively, we have

ˆ

P

lim
n1,n2Ñ8

´

¯˙

xd˚ px1, x2q “ d˚

X p1q, X p2q

“ 1

(1.17)

and

ˆ

P

¯

´
xi, X pjq

xd˚

lim
niÑ8

´

¯˙

“ d˚

X p1q, X p2q

“ 1, for i, j P t1, 2u, i ‰ j. (1.18)

Proof. We take n “ mintn1, n2u. To show (1.17) holds it suﬃces to prove that
for arbitrary ε ą 0, there is an integer N ą 0 such that for any n ě N , with
probability 1,

ˇ
ˇ
ˇ
ˇ
ˇxd˚ px1, x2q ´ d˚pX p1q, X p2qq
ˇ ă ε.

Deﬁne the sets of indexes
(
(cid:32)
pm, lq P N2 : m ď mn, l ď n ´ m ` 1

S1pnq “

and S2pnq “ N2zS1pnq.

To be more convenient we also denote by

´
X pjq

V

¯

´

´
X pjq

E

:“

l...l`m´1

l...l`m´1

¯

´

¯¯

, Cov

X pjq

l...l`m´1

(1.19)

and

´

pV

¯
l...n, m

´

´

¯
l...n, m

X pjq

X pjq
(1.20)
for pm, lq P N2 and j “ 1, 2. By using the deﬁnitions of d˚ (see (1.5)), of xd˚ (see
(1.15)) and the triangle inequality

l...n, m

X pjq

, ν˚

µ˚

:“

,

´

¯¯

ˇ
ˇ
ˇ
ˇ
ˇ ď

ai

ˇ
ˇ
ˇ
ˇ
ˇ

ÿ

iPI

ÿ

iPI

|ai|, for any indexes set I and any real numbers ai’s,

13

we obtainˇ
ˇ
ˇxd˚px1, x2q ´ d˚
ÿ

`

ˇ
ˇ
ˇ
ˇ
pm,lqPS1pnq

“

X p1q, X p2q

˘ˇ
ˇ
ˇ

´

´
pV pX p1q

wmwl

M

l...n, mq, pV pX p2q

l...n, mq

ÿ

´

´

wmwlM

V pX p1q

l...l`m´1q, V pX p2q

l...l`m´1q

¯ ˇ
ˇ
ˇ
ˇ

¯

¯

ď

´

ÿ

M

wmwl

pm,lqPS1pnqYS2pnq
´
pV pX p1q

ˇ
ˇ
ˇ
ˇ
pm,lqPS1pnq
´
l...l`m´1q, V pX p2q
V pX p1q
´
ÿ
V pX p1q

l...n, mq, pV pX p2q
l...n, mq
¯¯ ˇ
ˇ
ˇ
ˇ

l...l`m´1q

wmwlM

´M

`

ÿ

ď

pm,lqPS2pnq

ˇ
ˇ
ˇ
ˇM

´
pV pX p1q

wmwl

pm,lqPS1pnq
´

´M

l...n, mq, pV pX p2q
l...n, mq
¯ ˇ
ˇ
ˇ
ˇ

V pX p1q
l...l`m´1q, V pX p2q
´
ÿ
V pX p1q

l...l`m´1q

wmwlM

`

pm,lqPS2pnq

¯
l...l`m´1q

l...l`m´1q, V pX p2q
¯

l...l`m´1q, V pX p2q

¯
.(1.21)
l...l`m´1q

Next note that the metric M satisﬁes the following triangle inequality:

ˇ
ˇ
ˇ
ˇM

´
pV pX p1q
´

l...n, mq, pV pX p2q

ď M

pV pX p1q

l...n, mq, V pX p1q

´

¯
l...n, mq

´ M
¯
l...l`m´1q

` M

V pX p1q
l...l`m´1q, V pX p2q
´
pV pX p2q

l...n, mq, V pX p2q

l...l`m´1q

¯ ˇ
ˇ
ˇ
ˇ
¯
(1.22)
l...l`m´1q

.

It follows from (1.21) and (1.22) that
˘ˇ
ˇ
ˇ

ˇ
ˇ
ˇxd˚px1, x2q ´ d˚
ÿ

X p1q, X p2q
ˆ

`

ď

wmwl

M

´
pV pX p1q

pm,lqPS1pnq
´

`M

¯

l...n, mq, V pX p1q
¯ ˙

l...l`m´1q

pV pX p2q
l...n, mq, V pX p2q
ÿ

l...l`m´1q
´
V pX p1q

wmwlM

`

pm,lqPS2pnq

l...l`m´1q, V pX p2q

¯
.(1.23)
l...l`m´1q

Next we show that the right-hand side of (1.23) converges to 0 as n Ñ 8. First
observe that the weights twmumě1 have been chosen such that

wmwlM

´
V pX p1q

l...l`m´1q, V pX p2q

¯
l...l`m´1q

ă `8.

(1.24)

8ÿ

m,l“1

14

Then for arbitrary ﬁxed ε ą 0, we can ﬁnd an index J such that for n ě J,

ÿ

´
V pX p1q

wmwlM

l...l`m´1q, V pX p2q

¯
l...l`m´1q

ď

ε
3

.

pm,lqPS2pnq

(1.25)

Next, the weak ergodicity of the processes X p1q and X p2q implies that:
each pm, lq P N2, pV pX pjq
V pX pjq

for
l...n, mq (j “ 1, 2) is a strongly consistent estimator of

l...l`m´1q, under the metric M, i.e., with probability 1,
´
pV pX pjq

l...n, mq, V pX pjq

l...l`m´1q

M

¯

“ 0.

(1.26)

lim
nÑ8

Thanks to (1.26), for any pm, lq P S1pJq, there exists some Nm,l (which depends
on m, l) such that for all n ě Nm,l, we have, with probability 1,

´
pV pX pjq

M

l...n, mq, V pX pjq

l...l`m´1q

ď

¯

ε
3wmwl#S1pJq

, for j “ 1, 2,

(1.27)

where #A denotes the number of elements included in the set A. Denote by
Nm,l. Then observe that, for n ě maxtNJ , Ju,
NJ “ max

pm,lqPS1pJq

ÿ

wmwlM

l...l`m´1q, V pX p2q

l...l`m´1q

´

V pX p1q
´

¯

¯

wmwlM

V pX p1q

l...l`m´1q, V pX p2q

l...l`m´1q

.

(1.28)

pm,lqPS2pnq

ÿ

ď

pm,lqPS2pJq

It results from (1.23), (1.28), (1.27) and (1.25) that, for n ě maxtNJ , Ju,
˘ˇ
ˇ
ˇ

`

ˇ
ˇ
ˇxd˚px1, x2q ´ d˚
ÿ

X p1q, X p2q
´

¯
l...l`m´1q

pV pX p1q
l...n, mq, V pX p1q
´
pV pX p2q
l...n, mq, V pX p2q
´

¯

l...l`m´1q

wmwlM

V pX p1q

l...l`m´1q, V pX p2q

l...l`m´1q

¯

ď

wmwlM

pm,lqPS1pnq

ÿ

`

wmwlM

pm,lqPS1pnq

ÿ

`

pm,lqPS2pJq

ď

`

`

“ ε,

ε
3

ε
3

ε
3

which proves (1.17). The statement (1.18) can be proved analogously.

2 Asymptotically Consistent Clustering Algorithms

2.1 Oﬄine and Online Algorithms

In this section we introduce the asymptotically consistent algorithms for cluster-
ing oﬄine and online datasets respectively. We explain how the two algorithms

15

work, and prove that both algorithms are asymptotically consistent. It is worth
noting that the asymptotic consistency of our algorithms relies on the assump-
tion that the number of clusters κ is priorly known. The case for κ being
unknown has been studied in Khaleghi et al (2016) in the problem of cluster-
ing strictly stationary ergodic processes. However in the setting of wide-sense
stationary ergodic processes, this problem remains open.

Algorithm 1 below presents the pseudo-code for clustering oﬄine datasets.
It is a centroid-based clustering approach. One of its main features is that the
farthest 2-point initialization applies. The algorithm selects the ﬁrst two cluster
centers by picking the two “farthest” observations among all observations (Lines
1 - 3), under the empirical dissimilarity measure xd˚. Then each next cluster cen-
ter is chosen to be the observation farthest to all the previously assigned cluster
centers (Lines 4 - 6). Finally the algorithm assigns each remaining observation
to its nearest cluster (Lines 7-11).

Algorithm 1: Oﬄine clustering, with known κ

Input: sample paths S “ tx1, . . . , xN u; number κ of clusters;

1 pc1, c2q ÐÝ

weights wj, j “ 1, . . . , N ptq.
xd˚pxi, xjq;

argmax
pi,jqPt1,...,N u2,iăj

2 C1 ÐÝ tc1u;
3 C2 ÐÝ tc2u;
4 for k “ 3, . . . , κ do

5

ck ÐÝ argmax
i“1,...,N

min
j“1,...,k´1

xd˚pxi, xcj q;

6 end
7 Assign each remaining point to its nearest cluster center :
8 for i “ 1, . . . , N do
!
xd˚pxi, xjq : j P Ck

)
;

9

k ÐÝ argmin
kPt1,...,κu

Ck ÐÝ Ck Y tiu;

10
11 end

Output: The κ clusters tC1, C2, . . . , Cκu.
We point out that Algorithm 1 is diﬀerent from Algorithm 1 in Khaleghi

et al (2016) at two points:

1. As mentioned previously, our algorithm relies on the covariance-based dis-

similarity xd˚, in lieu of the process distributional distances.

2. Our algorithm suggests 2-point initialization, while Algorithm 1 in Khaleghi
et al (2016) randomly picks 1-point as the ﬁrst cluster center. The latter
initialization was proposed for use with k-means clustering by Katsavouni-
dis et al (1994). Algorithm 1 in Khaleghi et al (2016) requires κN distance
calculations, while our algorithm requires N pN ´ 1q{2 distances calcula-
tions. It is very important to point out that, to reduce the computational
complexity cost of our algorithm, it is ﬁne to replace our 2-point initializa-

16

tion with the one in Khaleghi et al (2016). However there are two reasons
based on which we recommend using our approach of initialization:

Reason 1 In the forthcoming Section 4.1, our empirical comparison to
Khaleghi et al (2016) shows that the 2-point initialization turns out
to be more accurate in clustering than the 1-point initialization.

ř

Reason 2 Concerning the complexity cost, we have the following loss
and earn: on one hand, the 2-point initialization requires more steps
of calculations than the 1-point initialization; on the other hand,
in our covariance-based dissimilarity measure xd˚ deﬁned in (1.15),
the matrices distance ρ˚ requires m2
n computations of Euclidean dis-
tances, while the distance
BPBm,l |νpx1, Bq´νpx2, Bq| given in (1.3)
requires at least n1 ` n2 ´ 2mn ` 2 computations of Euclidean dis-
tances (see Eq. (33) in Khaleghi et al (2016)). Note that we take
mn “ tlog nu (t¨u denotes the ﬂoor integer number) though this frame-
work. Therefore the computational complexity of the covariance-
based dissimilarity xd˚ makes the overall complexity of Algorithm 1
quite competitive to the algorithm in Khaleghi et al (2016), especially
when the paths lengths ni, i “ 1, . . . , n are relatively large, or when
the database of all distance values are at hand.

Next we present the clustering algorithm for online setting. As mentioned in
Khaleghi et al (2016), one regards recently-observed paths as unreliable ob-
servations, for which suﬃcient information has not yet been collected, and for
which the estimators of the covariance-based dissimilarity measures are not ac-
curate enough. Consequently, farthest-point initialization would not work in
this case; and clustering on all available data results in not only mis-clustering
unreliable paths, but also in clustering incorrectly those for which suﬃcient data
are already available. The strategy is presented in Algorithm 2 below: cluster-
ing based on a weighted combination of several clusterings, each obtained by
running the oﬄine algorithm (Algorithm 1) on diﬀerent portions of data.

More precisely, Algorithm 2 works as follows. Suppose the number of clusters
κ is known. At time t, a sample Sptq is observed (Lines 1 - 2), the algorithm
iterates over j “ κ, . . . , N ptq where at each iteration Algorithm 1 is utilized to
cluster the ﬁrst j paths in Sptq into κ clusters (Lines 6 - 7). For each cluster
its center is selected as the observation having the smallest index among that
cluster, and their indexes are ordered increasingly (Line 8). The minimum
inter-cluster distance γj (see Cesa-Bianchi and Lugosi (2006)) is calculated as
the minimum distance xd˚ between the κ cluster centers obtained at iteration j
(Line 9). Finally, every observation in Sptq is assigned to the nearest cluster,
based on the weighted combination of the distances between this observation

17

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

and the candidate cluster centers obtained at each iteration on j (Lines 14 - 17).

; number of clusters

Algorithm 2: Online clustering, with known κ

!

Input: sample paths

Sptq “ txt

1, . . . , xt
κ; weights βpjq, j “ 1, . . . , N ptq.
!

1 for t “ 1, . . . , 8 do

)

t

N ptqu

)
;

1, . . . , xt
xt

Obtain new sequences: Sptq ÐÝ

N ptq
Initialize the normalization factor : η ÐÝ 0;
Initialize the ﬁnal clusters: Ckptq ÐÝ H, k “ 1, . . . , κ;
Generate N ptq ´ κ ` 1 candidate cluster centers:
for j “ κ, . . . , N ptq do
`(cid:32)
(cid:32)
C j
1, . . . , xt
xt
1, . . . , C j
;
(cid:32)
(
κ
j
i P C j
pcj
1, . . . , cj
, k “ 1, . . . , κq;
`
˘
k
xt
, xt
γj ÐÝ
cj
cj
k1
k

ÐÝ Alg1
κq ÐÝ sortpmin
xd˚

min
k,k1Pt1,...,κu,k‰k1

˘
, κ

(

(

;

wj ÐÝ βpjq;
η ÐÝ η ` wjγj;

end
Assign each point to a cluster :
for i “ 1, . . . , N ptq do

k ÐÝ argmin
j“κ
k1Pt1,...,κu
Ckptq ÐÝ Ckptq Y tiu;

1
η

N ptqř

`

xd˚

wjγj

˘

;

i, xt
xt
cj
k1

end

17
18 end

Output: The κ clusters tC1ptq, . . . , Cκptqu, t “ 1, 2, . . . , 8.

In Algorithm 2, βpjq denotes a function indexed by j, which is the value
chosen for the weight wj. Remark that for online setting, our algorithm requires
the same number of distance calculations as in Algorithm 2 in Khaleghi et al
(2016). They are both bounded by OpN ptq2q. Using 2-point initialization, our
Algorithm 2 then takes advantage in the overall computational complexity cost.
Finally we note that both Algorithm 1 and Algorithm 2 require κ ě 2. When
κ is known, this restriction is not a practical issue.

2.2 Consistency and Computational Complexity of the

Algorithms

In this section we prove the asymptotic consistency of Algorithms 1 and 2. They
are stated in the 2 theorems below.

Theorem 2.1. Algorithm 1 is strongly asymptotically consistent (in the oﬄine
sense), provided that the true number κ of clusters is known, and each sequence
xi, i “ 1, . . . , N is sampled from some wide-sense stationary ergodic process.

Proof. Similar to the idea used in the proof of Theorem 11 in Khaleghi et al

18

(2016), to prove the consistency statement we will need Lemma 1.8 to show that
if the sample paths in S are long enough, the sample paths that are generated
by the same process covariance structure are “closer” to each other than to the
rest. Therefore, the sample paths chosen as cluster centers are each generated
by a diﬀerent covariance structure, and since the algorithm assigns the rest to
the closest clusters, the statement follows. More formally, let nmin denote the
shortest path length in S:

nmin :“ min tni : i “ 1, . . . , N u .

Denote by δmin the minimum non-zero covariance-based dissimilarity measure
between any 2 covariance structures:

!

´

¯

)

δmin :“ min

d˚

X pkq, X pk1q

: k, k1 P t1, . . . , κu, k ‰ k1

.

(2.1)

Fix ε P p0, δmin{4q. Since there are a ﬁnite number N of observations, by Lemma
1.8 there is n0 such that for nmin ě n0 we have

´

xd˚

¯

xi, X plq

ď ε,

max
lPt1,...,κu
iPGlXt1,...,N u

(2.2)

where Gl, l “ 1, . . . , κ denote the covariance structure ground-truth partitions
given by Deﬁnition 1.2.

On one hand, by using (2.2), the triangle inequality (see Remark 1.7) and

the fact that

max
iPI

pai ` biq ď max
iPI

ai ` max
iPI

bi

for any indexes set I and any real numbers ai’s and bi’s, we obtain

max
lPt1,...,κu
i,jPGlXt1,...,N u

xd˚ pxi, xjq

ď

“

max
lPt1,...,κu
i,jPGlXt1,...,N u
xd˚

max
lPt1,...,κu
iPGlXt1,...,N u

ď 2ε ă

δmin
2

.

¯

´
xi, X plq

xd˚

´

¯

xi, X plq

`

`

max
lPt1,...,κu
i,jPGlXt1,...,N u
xd˚

max
lPt1,...,κu
jPGlXt1,...,N u

¯

´
xj, X plq

xd˚

¯

´
xj, X plq

(2.3)

On the other hand, by using the triangle inequality (see Remark 1.7), (2.1) and

19

(2.2), we have for nmin ě n0,

xd˚pxi, xjq

min
k,k1Pt1,...,κu,k‰k1
iPGkXt1,...,N u
jPGk1 Xt1,...,N u

ě

min
k,k1Pt1,...,κu,k‰k1
iPGkXt1,...,N u
jPGk1 Xt1,...,N u

ě δmin ´ 2ε ą

δmin
2

.

!

´
X pkq, X pk1q

¯

d˚

¯

´
xi, X pkq

xd˚

´

´
xj, X pk1q

¯)

xd˚

´

(2.4)

(2.5)

In words, (2.3) together with (2.4) indicates that the sample paths in S that
are generated by the same covariance structure are closer to each other than to
the rest of sample paths. Then by (2.3) and (2.4), for nmin ě n0, we necessarily
have each sample path should be “close” enough to its cluster center, i.e.,

where the κ cluster centers’ indexes c1, . . . , cκ are given by Algorithm 1 as

max
i“1,...,N

min
k“1,...,κ´1

xd˚pxi, xck q ą

δmin
2

,

pc1, c2q :“ argmax

i,j“1,...,N, iăj

xd˚pxi, xjq,

and

ck :“ argmax
i“1,...,N

min
j“1,...,k´1

xd˚pxi, xcj q, k “ 3, . . . , κ.

Hence, the indexes c1, . . . , cκ will be chosen to index the sample paths gener-
ated by diﬀerent process covariance structures. Then by (2.3) and (2.4), each
remaining sample path will be assigned to the cluster center corresponding to
the sample path generated by the same process covariance structure. Finally
Theorem 2.1 results from (2.3), (2.4) and (2.5).

Theorem 2.2. Algorithm 2 is strongly asymptotically consistent (in the online
sense), provided the true number of clusters κ is known, and each sequence
xi, i P N is sampled from some wide-sense stationary ergodic process.

Proof. The idea of the proof is similar to that of Theorem 12 in Khaleghi et al
(2016). The main diﬀerences between the 2 proofs are made by the fact that
our covariance-based dissimilarity measure xd˚ is not bounded by some constant.
Although it is not mentioned in the pseudo-code Algorithm 2, the notations γj’s
and η are dependent of t, therefore we denote γt
j :“ γj and ηt :“ η through this
proof. In the ﬁrst step, by using the triangle inequality we can show that for

20

any t ą 0, any N P N,

sup
jPt1,...,N u
kPt1,...,κu

jPt1,...,N u
kPt1,...,κu

jPt1,...,N u
kPt1,...,κu

´

´

´

xd˚

¯

j, X pkq
xt

ď sup

´

´
X pkq, X pk1
j q

¯

d˚

´

¯¯

xd˚

`

j, X pk1
xt
j q

ď sup

d˚

X pkq, X pk1
j q

` sup

j, X pk1
xt
j q

jPt1,...,N u
kPt1,...,κu
¯

jPt1,...,N u
kPt1,...,κu

¯

` sup

jPt1,...,N u

´

´

xd˚

xd˚

¯

¯

“ sup

d˚

X pkq, X pk1
j q

j, X pk1
xt
j q

,

(2.6)

j is chosen such that xt

j is sampled from the process covariance

where for each j, k1
structure X pk1

j q. On one hand, let
!

´
X pkq, X pk1q

¯

d˚

δmax :“ max

: k, k1 P t1, . . . , κu, k ‰ k1

,

(2.7)

)

then the ﬁrst term on the right-hand side of (2.6) can be bounded by the constant
δmax, which neither depends on t nor on N :

´

¯

d˚

X pkq, X pk1
j q

ď δmax.

(2.8)

sup
jPt1,...,N u
kPt1,...,κu

On the other hand, since xt
(see Lemma 1.8), for j “ 1, . . . , N , with probability 1,

j is sampled from X pk1

j q, by using the weak ergodicity

´

¯

j, X pk1
xt
j q

“ 0.

xd˚

lim
tÑ8

This together with the fact that a convergent sequence is also bounded, leads
to, for each j P t1, . . . , N u, there is bj (not depending on t) such that

´

xd˚

j, X pk1
xt
j q

¯

ď bj, for all t ě 0.

Therefore the second term on the right-hand side of (2.6) can be bounded as:

´

xd˚

j, X pk1
xt
j q

¯

sup
jPt1,...,N u

ď maxtb1, . . . , bN u.

(2.9)

Let

BpN q :“ δmax ` maxtb1, . . . , bN u.

(2.10)

It is important to point out that BpN q depends only on N but not on t. It
follows from (2.6), (2.8), (2.9) and (2.10) that
¯

´

xd˚

j, X pkq
xt

ď BpN q.

(2.11)

sup
jPt1,...,N u
kPt1,...,κu

21

Let δmin be the one given in (2.1). Fix ε P p0, δmin{4q. By using (1.8), we can
choose some J ą 0 so that

8ÿ

j“J`1

wj ď ε.

Recall that in online setting, the ith sample path’s length niptq grows with time,
for each i. Therefore, by the wide-sense ergodicity (see Lemma 1.8), for every
j P t1, . . . , Ju there exists some T1pjq ą 0 such that for all t ě T1pjq we have

(2.12)

(2.13)

´

xd˚

¯

i, X pkq
xt

ď ε.

max
kPt1,...,κu
iPGkXt1,...,ju

For k “ 1, . . . , κ, deﬁne skpN ptqq to be the index of the ﬁrst path in Sptq sampled
from the covariance structure X pkq, i.e.,

skpN ptqq :“ min ti P Gk X t1, . . . , N ptquu .

(2.14)

Note that skpN ptqq depends only on N ptq. Then denote

mpN ptqq :“ max

skpN ptqq.

kPt1,...,κu

(2.15)

By Theorem 2.1 for every j P tmpN ptqq, . . . , Ju there exists some T2pjq such
that Alg1pSptq|j, κq is asymptotically consistent for all t ě T2pjq, where Sptq|j “
(cid:32)
xt
1, . . . , xt
denotes the subset of Sptq consisting of the ﬁrst j sample paths.
j
Let

(

T :“ max
i“1,2
jPt1,...,Ju

Tipjq.

Recall that, by the deﬁnition of mpN ptqq in (2.15), Sptq|mpN ptqq contains sample
paths from all κ distinct covariance structures. Therefore, similar to obtaining
(2.4), for all t ě T , we use the triangle inequality, (2.1) and (2.13) to obtain
˙

ˆ

xd˚

xt

, xt

cmpN ptqq
k

cmpN ptqq
k1

ˆ

´
X pkq, X pk1q

¯

d˚

´

ˆ

´

xd˚

xt

¯

ˆ

, X pkq

xd˚

`

xt

cmpN ptqq
k

, X pk1q

cmpN ptqq
k1

˙˙˙

(2.16)

min
k,k1Pt1,...,κu
k‰k1

ě

min
k,k1Pt1,...,κu
k‰k1

ě δmin ´ 2ε ě

δmin
2

.

N ptqÿ

j“1

From Algorithm 2 (see Lines 9, 11) we see

ηt :“

wjγt

j, with γt

j :“

´

xd˚

¯

.

xt
cj
k

, xt
cj
k1

min
k,k1Pt1,...,κu
k‰k1

22

(2.17)

(2.18)

(2.19)

(2.20)

Hence, by (2.16), for all t ě T ,

ηt ě

wmpN ptqqδmin
2

.

For j P tJ ` 1, . . . , N ptqu, by the triangle inequality and (2.11), we have for all
t ě T ,

¯

´

xd˚

xt
cj
k

, xt
cj
k1

γt
j “

min
k,k1Pt1,...,κu
k‰k1
´

ď

min
k,k1Pt1,...,κu
k‰k1

ď δmax ` 2BpN ptqq.

´
X pkq, X pk1q

¯

d˚

´

´
xd˚

`

¯

´

xd˚

`

, X pkq

xt
cj
k

, X pk1q

xt
cj
k1

¯¯¯

Denote by

then (2.18) can be interpreted as: for all t ě T ,

M pN ptqq :“ δmax ` 2BpN ptqq,

sup
jPtJ`1,...,N ptqu

γt
j ď M pN ptqq.

By (2.11), (2.17) and (2.19), for every k P t1, . . . , κu we obtain

1
ηt

N ptqÿ

j“1

´

¯

wjγt
j

xd˚

, X pkq

xt
cj
k

wjγt
j

xd˚

, X pkq

`

xt
cj
k

1
ηt

N ptqÿ

j“J`1

´

¯

wjγt
j

xd˚

, X pkq

xt
cj
k

¯

¯

wjγt
j

xd˚

, X pkq

`

xt
cj
k

2BpN ptqqM pN ptqq
wmpN ptqqδmin

N ptqÿ

j“J`1

wj

´

´

´

Jÿ

j“1
Jÿ

“

ď

“

1
ηt

1
ηt

1
ηt

j“1
mpN ptqq´1ÿ

wjγt
j

xd˚

´

¯

, X pkq

`

xt
cj
k

1
ηt

Jÿ

¯

wjγt
j

xd˚

, X pkq

xt
cj
k

j“mpN ptqq

j“1
2BpN ptqqM pN ptqqε
wmpN ptqqδmin

.

`

Next we provide upper bounds of the ﬁrst 2 items in the right-hand side of
(2.20). On one hand, by the deﬁnition of mpN ptq, the sample paths in Sptq|j
for j “ 1, . . . , mpN ptqq ´ 1 are generated by at most κ ´ 1 out of the κ process
covariance structures. Therefore for each j P t1, . . . , mpN ptqq´1u there exists at
least one pair of distinct cluster centers that are generated by the same process
covariance structure. Consequently, by (2.13) and the deﬁnition of ηt, for all
t ě T and k P t1, . . . , κu,

mpN ptqq´1ÿ

1
ηt

j“1

´

wjγt
j

xd˚

, X pkq

ď

xt
cj
k

ε
ηt

¯

mpN ptqq´1ÿ

j“1

wjγt

j ď ε.

(2.21)

23

On the other hand, since the clusters are ordered in the order of appearance of
the distinct covariance structures, we have xt
slpN ptqq for all j “ m, . . . , J
cj
l
and l “ 1, . . . , κ, where the index slpN ptqq is deﬁned in (2.14). Therefore, by
(2.13) and the deﬁnition of ηt, for all t ě T and every l “ 1, . . . , κ we have

“ xt

1
ηt

Jÿ

´

wjγt
j

xd˚

, X plq

xt
cj
l

¯

´

xd˚

“

j“mpN ptqq

slpN ptqq, X plq
xt

Jÿ

¯

1
ηt

j“mpN ptqq

wjγt

j ď ε.

Combining (2.20), (2.21), (2.22) and (2.13) we obtain, for t ě T ,
˙

ˆ

1
ηt

N ptqÿ

j“1

´

¯

wjγt
j

xd˚

, X pkq

xt
cj
k

ď ε

2 `

2BpN ptqqM pN ptqq
wmpN ptqqδmin

(2.22)

(2.23)

for all l “ 1, . . . , κ.

Now we explain how to use (2.23) to prove the asymptotic consistency of
Algorithm 2. Consider an index i P Gk1 for some k1 P t1, . . . , κu. Then on one
hand, using (2.21) and (2.22), we get for k P t1, . . . , κu, k ‰ k1,

1
ηt

N ptqÿ

j“1

wjγt
j

xd˚

´

¯

i, xt
xt
cj
k

ě

ě

1
ηt

1
ηt

N ptqÿ

j“1
N ptqÿ

j“1

´

¯

wjγt
j

xd˚

i, X pkq
xt

´

´

¯

wjγt
j

xd˚

, X pkq

xt
cj
k

´

´

´

¯¯

wjγt
j

d˚

X pkq, X pk1q

xd˚

´

i, X pk1q
xt

N ptqÿ

j“1

1
ηt

¯

N ptqÿ

´

¯

wjγt
j

xd˚

, X pkq

xt
cj
k

´

1
ηt

ˆ

ě δmin ´ 2ε

j“1
2BpN ptqqM pN ptqq
wmpN ptqqδmin
On the other hand, for any N P N, by using the wide-sense ergodicity, there is
T pN q such that for all t ě T pN q,

(2.24)

2 `

˙

.

Since ε can be arbitrarily chosen, it follows from (2.24) and (2.25) that

´

xd˚

¯

i, X pkq
xt

ď ε.

max
kPt1,...,κu
iPGkXt1,...,N u

argmin
kPt1,...,κu

1
ηt

N ptqÿ

j“1

wjγj

xd˚

´

¯

i, xt
xt
cj
k

“ k1

(2.25)

(2.26)

holds almost surely for all i “ 1, . . . , N and all t ě maxtT, T pN qu. Theorem 2.2
is proved.

24

The next part involves discussion of the complexity costs of the above two

algorithms.

1. For oﬄine setting, our Algorithm 1 requires N pN ´ 1q{2 calculations of
xd˚, against κN calculations of pd in the oﬄine algorithm in Khaleghi et al
(2016). In each xd˚, the matrices distance ρ˚ consists of m2
n calculations
of Euclidean distances. Then iterating over m, l in xd˚ we see that at most
Opnm3
nq computations of Euclidean distances, against Opnmn{| log s|q com-
putations of ˆd for the oﬄine algorithm in Khaleghi et al (2016), where

s “

min
i ‰X p2q
iPt1,...,n1u;jPt1,...,n2u

X p1q

j

ˇ
ˇ
ˇX p1q

i ´ X p2q

j

ˇ
ˇ
ˇ .

It is known that eﬃcient searching algorithm can be utilized to determine
s, with at most Opn logpnqq (n “ mintn1, n2u) computations. Therefore
our Algorithm 1 is computationally competitive to the one in Khaleghi
et al (2016).

2. For online setting, we can hold a similar discussion as in Khaleghi et al
(2016), Section 5.1. There it shows the computational complexity of
updates of xd˚ for both our Algorithm 2 and the online algorithm in
Khaleghi et al (2016) is at most OpN ptq2 ` N ptq log3 nptqq (here we take
mnptq “ tlog nptqu). Therefore the overall diﬀerence of computational com-
plexities between the 2 algorithms are reﬂected by the complexity of com-
puting xd˚ and pd (see Point 1).

2.3 Eﬃcient Dissimilarity Measure

Kleinberg (2003) presented a set of three simple properties that a good clustering
function should have: scale-invariance, richness and consistency. Further, he
demonstrated that there is no clustering function that satisﬁes these properties
at the meanwhile. He pointed out, as one particular example, that the centroid-
based clustering basically does not satisfy the above consistency property (note
that this is a diﬀerent concept from our asymptotic consistency). In this section
we show that, although the consistency property is not satisﬁed, there exists
some other criterion of eﬃciency of dissimilarity measure in a particular setting.
It is the so-called eﬃcient dissimilarity measure.

Deﬁnition 2.3 (Eﬃcient dissimilarity measure). Assume that the samples S “
txpξq : ξ P Hu (H Ă Rq for some q P N), meaning that all the paths xpξq are
indexed by a set of real-valued parameters ξ. Then a clustering function is called
eﬃcient if its dissimilarity measure d satisﬁes that, there exists c ą 0 so that
for any xpξ1q, xpξ2q P S,

dpxpξ1q, xpξ2qq “ c}ξ1 ´ ξ2},

where } ¨ } denotes some norm deﬁned over Rq.

25

Mathematically, eﬃcient dissimilarity measure is a metric induced by some
norm. Clustering processes based on eﬃcient dissimilarity measure will then be
equivalent to clustering under classical distances in Rq, such as Euclidean dis-
tance, Manhattan distance, or Minkowski distance. The latter setting has well-
known advantages in cluster analysis. For example, Euclidean distance performs
well when deployed to datasets that include compact or isolated clusters (Jain
and Mao, 1996; Jain et al, 1999); when the shape of clusters is hyper-rectangular
(Xu and Wunsch, 2005), Manhattan distance can be used; Minkowski distance,
including Euclidean and Manhattan distances as its particular cases, can be
utilized to solve clustering obstacles (Wilson and Martinez, 1997). There is a
rich literature on comparing the above three distances to each other through
discussing of their advantages and inconveniences. We refer to Hirkhorshidi et al
(2015) and the references therein.

In the next section we present an excellent example, to show how to improve
the eﬃciency of our consistent algorithms, for clustering self-similar processes
with wide-sense stationary ergodic increments.

3 Self-similar Processes and Logarithmic Trans-

formation

In this section we introduce a non-linear transformation of the covariance matri-
ces in xd˚, in order to improve the eﬃciency of clustering. This transformation is
based on logarithmic function. We use one example to explain how this transfor-
mation works. We show this transformation maps xd˚ to some covariance-based
dissimilarity measure similar to an eﬃcient one, when applied to clustering self-
similar processes.

Deﬁnition 3.1 (Self-similar process, see Samorodnitsky and Taqqu (1994)).
A process X pHq “ tX pHq
utPT (e.g., T “ R or Z) is self-similar with index
H P p0, 1q if, for all n P N, all t1, . . . , tn P T , and all c ‰ 0 such that cti P T
(i “ 1, . . . , n),
´
X pHq
t1

ct1 , . . . , |c|´H X pHq

´
|c|´H X pHq

, . . . , X pHq

¯
.

law
“

ctn

¯

tn

t

It can be shown that a self-similar process has necessarily zero mean and its
covariance structure is indexed by its self-similarity index H, in the following
way (Embrechts and Maejima, 2000).

(
tPT be a zero-mean self-similar process with in-
Theorem 3.2. Let
dex H P p0, 1q and with wide-sense stationary ergodic increments. Assume
E|X pHq
1

|2 ă `8, then for any s, t P T ,

(cid:32)
X pHq
t

´

¯

Cov

X pHq
s

, X pHq
t

“

`

|2

E|X pHq
1
2

|s|2H ` |t|2H ´ |s ´ t|2H

.

˘

The corollary below follows.

26

Corollary 3.3. Let tX pHq
and weakly stationary increments. Assume E|X pHq
enough, deﬁne the increment process Z pHq
such that s ´ t ě h, we have

utPT be a zero-mean self-similar process with index H
|2 ă `8. For h ą 0 small
s`h ´ X pHq
, then for s, t P T

1
psq “ X pHq

h

s

t

´
Z pHq
h

Cov

¯

psq, Z pHq

h

ptq

“

`

|2

E|X pHq
1
2

ps ´ t ´ hq2H ` ps ´ t ` hq2H ´ 2ps ´ tq2H
(3.1)

˘

.

Applying three times the mean value theorem to (3.1) leads to

´
Z pHq
h

Cov

¯

´

psq, Z pHq

h

ptq

“ HE|X pHq

|2

1

pvpHq
1

q2H´1 ´ pvpHq

q2H´1

2

h

“ Hp2H ´ 1qE|X pHq

|2pvpHqq2H´2h,

1

¯

(3.2)

for some vpHq
1
see that the item Cov

P ps´t, s´t`hq, vpHq
´

2
psq, Z pHq

h

¯

P ps´t´h, s´tq and vpHq P pvpHq

q. We
is a non-linear function of H. Next
´

, vpHq
1

ptq

¯¯

2

Z pHq
h

we would ﬁnd a function g such that g
is linearly
dependent of H. To this end we introduce the following log˚-transformation:
for x P R, deﬁne

ptq

h

psq, Z pHq

´
Z pHq
h

Cov

log˚pxq :“ sgnpxq log |x| “

$
&

%

logpxq
´ logp´xq
0

if x ą 0;
if x ă 0;
if x “ 0.

Introduction to log˚-transformation is driven by the following 2 motivations:

Motivation 1 The log˚ function transforms the current dissimilarity measure

to the one which “linearly” depends on its variable H.

Motivation 2 The value log˚pxq preserves the sign of x, which leads to the
consequence that larger distance between x, y yields larger distance be-
tween log˚pxq and log˚pyq.

Applying log˚-transformation to the covariances of Z pHq
tain

h

given in (3.2), we ob-

´

log˚

Cov

¯¯

psq, Z pHq

´
Z pHq
h
´
p2H ´ 2q log vpHq ` log h ` logpH|1 ´ 2H|VarpX pHq

ptq

h

¯

qq

.

1

“ sgnp2H ´ 1q

When vpHq and h are small the items log vpHq and log h are signiﬁcantly large
so logpH|1 ´ 2H|VarpX pHq
´
Z pHq
h

¯
´
p2H ´ 2q log vpHq ` log h

qq becomes negligible. Thus we can write

« sgnp2H ´ 1q

psq, Z pHq

log˚

Cov

ptq

¯¯

´

h

1

.

In conclusion,

27

´

´
Z pHq
h

psq, Z pHq

h

ptq

¯¯

• When H1, H2 P p0, 1{2s or H1, H2 P r1{2, 1q, the item log˚

Cov

is “approximately linear” on H P p0, 1{2s or on H P r1{2, 1q.
Using the approximation log vpH1q « log vpH2q for H1, H2 P p0, 1{2s or
H1, H2 P r1{2, 1q, we have

´

´

¯¯

´

log˚

Cov

Z pH1q
h

psq, Z pH1q
h

ptq

´ log˚

Cov

psq, Z pH2q
h

ptq

´
Z pH2q
h

« 2 sgnp2H1 ´ 1qpH1 ´ H2q log vpH1q.

´

´

• When H1 P p0, 1{2s and H2 P p1{2, 1q, log˚

Cov

Z pHq
h

psq, Z pHq

h

ptq

turns out to be relatively large, because we have

¯¯

´

´

´
Z pH1q
h

Cov

log˚

psq, Z pH1q
h
« ´p2H1 ´ 2q log vpH1q ´ p2H2 ´ 2q log vpH2q
)
!

´ log˚

ptq

Cov

ě 2p2 ´ H1 ´ H2q min

log vpH1q, log vpH2q

.

´
Z pH2q
h

psq, Z pH2q
h

ptq

¯¯

¯¯

¯¯

Taking advantage of the above facts we deﬁne the new empirical covariance-
based dissimilarity measure (based on the deﬁnition (1.12)) to be

yd˚˚pz1, z2q :“

mnÿ

n´m`1ÿ

wmwlρ˚

´
ν˚˚pZ pH1q

l...n , mq, ν˚˚pZ pH2q

¯
l...n , mq

,

m“1

l“1

where ν˚˚pZ pH1q
l...n , mq is the empirical covariance matrix of Z pH1q
l...n , mq,
with each of its coeﬃcients transformed by log˚: let M “ tMi,jui“1,...,m; j“1,...,n
be an arbitrary real-valued matrix, deﬁne
(
i“1,...,m; j“1,...,n .
¯
´
ν˚pZ pH1q

(cid:32)
log˚ Mij

Then we have

, ν˚pZ pH1q

ν˚˚pZ pH1q

log˚ M :“

l...n , mq :“ log˚

l...n , mq

h

.

Now given 2 wide-sense stationary ergodic processes X p1q, X p2q, we choose
twjujPN to satisfy

wmwlρ˚

¯
´
log˚pVl,l`m´1pX p1qqq, log˚pVl,l`m´1pX p2qq

ă `8,

(3.3)

8ÿ

m,l“1

where we denote by

Vl,l`m´1pX p1qq :“ Cov

X p1q
l

, . . . , X p1q

l`m´1

.

´

¯

Then deﬁne the log˚-transformation of the covariance-based dissimilarity mea-
sure to be

d˚˚pX p1q, X p2qq :“

wmwlρ˚

¯
´
log˚pVl,l`m´1pX p1qqq, log˚pVl,l`m´1pX p2qq

.

8ÿ

m,l“1

(3.4)

28

Using the fact that log˚ is continuous over Rzt0u and the weak ergodicity of
Z pHq
h

, we have the following version of ergodicity:

yd˚˚pz1, z2q

a.s.
ÝÝÝÑ
nÑ8

d˚˚

Z pH1q
h

, Z pH2q
h

`

˘

.

Unlike xd˚, the dissimilarity measure yd˚˚ is approximately linear with respect to
the self-similarity index H. Indeed, it is easy to see that

"

yd˚˚pz1, z2q „

|H1 ´ H2| ă 1,
2p2 ´ H1 ´ H2q ą 1,

for H1, H2 P p0, 1{2s or H1, H2 P r1{2, 1q;
for H1 P p0, 1{2q and H2 P r1{2, 1q,

(3.5)
where H1, H2 correspond to the self-similarity indexes of X pH1q, X pH2q respec-
In fact, from (3.5) we can say that yd˚˚ satisﬁes Deﬁnition 2.3 in the
tively.
wide sense:
it is approximately linearly dependent of |H1 ´ H2| when H1, H2
are in the same group out of p0, 1{2s and r1{2, 1q; it is approximately larger
than |H1 ´ H2| when H1, H2 are in diﬀerent groups out of p0, 1{2s and r1{2, 1q.
This fact allows our asymptotically consistent algorithms to be more eﬃcient
when clustering self-similar processes with weakly stationary increments, having
diﬀerent values of H. In Section 4.2 we provide an example of clustering us-
ing our consistent algorithms with and without the log˚-transformation, when
the observed paths are from a well-known self-similar process with stationary
increments – fractional Brownian motion.

4 Simulation and Empirical Study

This section is devoted to applying our clustering algorithms to several synthetic
data and real-world data.
It is worth noting that, in our statistical setting,
the auto-covariance functions are supposed to be unavailable, then the prior
choice of the weights wj presents some trade-oﬀ between the convergence of
the dissimilarity measure and practical application. On one hand, low rate of
convergence (e.g. wj “ 1{jpj ` 1q) risks to a divergent dissimilarity measure d˚
(see (1.5)). On the other hand, high rate of convergence (e.g., wj “ 1{j3pj `1q3)
will only make use of some ﬁrst observations in the sample paths. We believe
that the ﬁrst issue is a minor one in practice, because for most of the wide-sense
stationary ergodic processes (especially Gaussian) taking wj “ 1{jpj ` 1q can
lead to convergent d˚. Also, in practice, instead of (1.5) it is ﬁne to regard

´

¯

Nÿ

d˚

X p1q, X p2q

:“

wmwlρ˚

´

¯
Vl,l`m´1pX p1qq, Vl,l`m´1pX p2qq

,

m,l“1

for some N large enough.

Therefore, through this entire section we take wj “ 1{jpj ` 1q and mn “
tlog nu (recall that t¨u denotes the ﬂoor number) in the covariance-based dissim-
ilarity measure xd˚. Next we explain how to prepare oﬄine and online datasets
in this simulation study.

29

Oﬄine dataset simulation: For each scenario, we simulate 5 groups of sam-
ple paths, each consists of 10 paths with length N ptq “ 5t, for the time
steps t “ 1, 2, . . . , 50. Algorithm 1 is performed over 100 such scenarios,
and the misclassiﬁcation rate is calculated.

Online dataset simulation: For each scenario, we simulate 5 groups of sam-
ple paths. Let the total number of sample paths be N ptq “ 30`tpt´1q{10u
at each time step t. That is, there are 6 sample paths in each of the 5
groups when t “ 1. And the number of sample paths in each group
will increase by 1 once the time t increases by 10. For i “ 1, 2, . . ., the
ith sample path in each group has length niptq “ 5rt ´ pi ´ 6q`s, where
x` “ maxpx, 0q.

We then apply the proposed clustering algorithms to both oﬄine and online set-
tings, and determine their corresponding misclassiﬁcation rates. These misclas-
siﬁcation rates are utilized to intuitively illustrate the asymptotic consistency
of our clustering algorithms, or to compare the performances of our clustering
approaches to other ones. Recall that the misclassiﬁcation rate (i.e. mean clus-
tering error rate, see Section 6 in Khaleghi et al (2016)) is obtained by dividing
the number of misclassiﬁed paths by the total number of paths per scenario,
then average all these fractions:

ˆ

p :“ avg

# of misclassiﬁed sample paths
# of total sample paths collected

˙

.

More precisely, let pC1, . . . , Cκq denote the ground truth clusters of the N sample
paths x1, . . . , xN . We deﬁne the ground truth cluster labels by

Lk “ pk, . . . , kq
loooomoooon

, for k “ 1, . . . , κ.

#Ck times

Let pl1, . . . , lN q denote the cluster labels of px1, . . . , xN q output by some clus-
tering approach. Then the misclassiﬁcation rate p of this approach is computed
by

p “

min
σPSκ
pπ1,...,πN q“pLσp1q,...,Lσpκqq

Nř

1

i“1

tπi‰liu

,

N

(4.1)

where Sκ denotes the group of all possible permutations over the set t1, . . . , κu.
For example, in one scenario of 7 sample paths, if the ground truth cluster

labels of px1, . . . , x7q satisfy

pL1, L2, L3q “ pp1, 1q, p2q, p3, 3, 3, 3qq,

while the clustering algorithm output cluster labels corresponding to px1, . . . , x7q
are given by

pl1, . . . , l7q “ p2, 1, 1, 2, 3, 2, 1q ,

30

(4.1), the misclassiﬁcation rate is 4{7. This can be
then according to Eq.
explained as, at least 4 changes of labels are needed to let the output cluster
labels match that of the ground truth ones p1, 1, 3, 2, 2, 2, 2q:

l1 Ð 1; l3 Ð 3; l5 Ð 2; l7 Ð 2.

We provide the implementation of the misclassiﬁcation rate (see Eq. (4.1)) in
MATLAB publicly online as misclassify rate.m 1.

4.1 Clustering Non-Gaussian Discrete-time Stochastic Pro-

cesses

In Khaleghi et al (2016) a simulation study on a non-Gaussian strictly station-
ary ergodic discrete-time stochastic process (see also Shields (1996)) has been
performed. Since this process has ﬁnite covariance structure, it is also wide-
sense stationary ergodic. As a result we can test our clustering algorithms over
the same dataset and compare their performances to the ones in Khaleghi et al
(2016). Recall that this process tXtutPN is generated in the following way. Fix
some irrational-valued parameter α P p0, 1q.

Step 1. Draw a uniform random number r0 P r0, 1s.

Step 2. For each index i “ 1, 2, . . . , N :

Step 2.1. Deﬁne ri “ ri´1 ` α ´ tri´1 ` αu.

Step 2.2. Deﬁne Xi “

#

1
0

when ri ą 0.5,
otherwise.

We simulate 5 groups of sample paths tXtutPN indexed by the irrational values
α1 “ 0.31..., α2 “ 0.33..., α3 “ 0.35..., α4 “ 0.37..., α5 “ 0.39... (αi, i “
1, . . . , 5, each is simulated by a longdouble with a long mantissa, see Khaleghi
et al (2016)), respectively.

4.1.1 Oﬄine Dataset

We demonstrate the asymptotic consistency of Algorithm 1 by conducting oﬄine
clustering on the simulated oﬄine datasets of tXiuiPN.

The valid blue line in Fig. 1 illustrates the asymptotic consistency of Al-
gorithm 1 through the fact that its misclassiﬁcation rate decreases as time t
increases. Compared to the simulation study over the same dataset in Khaleghi
et al (2016), the misclassiﬁcation rate provided by our proposed algorithm con-
verges at a comparable speed (see Figure 2 in Khaleghi et al (2016)), even
though Algorithm 1 aims to cluster “covariance structures” but not “process
distributions”.

1https://github.com/researchcoding/clustering_stochastic_processes/blob/

master/misclassify_rate.m.

31

The dot-dashed red line in Fig. 1 presents the performance of Algorithm 2
and compares its misclassiﬁcation rates with the ones from Algorithm 1. Applied
to oﬄine dataset, the oﬄine algorithm’s misclassiﬁcation rates are consistently
lower than the online algorithm, i.e., the oﬄine dataset clustering algorithm
performs better than the online dataset clustering algorithm, when dealing with
oﬄine datasets.

Figure 1: The graph compares the misclassiﬁcation rates of Algorithm 1 and
Algorithm 2 applied to oﬄine dataset of non-Gaussian discrete-time processes.
100 runs are performed at each time step t to compute the misclassiﬁcation rate.

4.1.2 Online Dataset

In our simulated online datasets the number of sample paths and the length of
each sample path increase as t increases. This type of setting is mimicking the
situation such as modeling ﬁnancial asset prices, where new assets are launched
at each time step. The oﬄine and online clustering algorithms are applied at
each time t with 100 runs, their misclassiﬁcation rates at each time t are then
obtained.

Fig. 2 compares the misclassiﬁcation rates of oﬄine algorithm and online
algorithm applied to the online dataset described above. The periodical pattern,
that misclassiﬁcation rate increases per 10 time steps using oﬄine algorithm,
matches the timing of adding new observations. That is, the misclassiﬁcation
rate spikes whenever new observations are obtained. We observe that the mis-
classiﬁcation rate of the online algorithm is overall lower than that of oﬄine
algorithm in this dataset, reﬂecting the advantage of online algorithm against
the oﬄine one in the case where new observations are expected to occur. It is
worth pointing out that our online setting is diﬀerent from the one in Khaleghi
et al (2016), therefore the two clustering results are not comparable.

32

Finally, all the codes in MATLAB that reproduce the main conclusions in

this subsection can be found publicly online2.

Figure 2: The graph compares the misclassiﬁcation rates of Algorithm 1 and
Algorithm 2 applied to online dataset of non-Gaussian discrete-time processes.
100 runs are performed at each time step t to compute the misclassiﬁcation rate.

4.2 Clustering Fractional Brownian Motions

In this section, we present the performance of proposed oﬄine (Algorithm 1) and
online (Algorithm 2) methods, on a synthetic dataset sampled from continuous-
time Gaussian processes. The wide-sense stationary ergodic processes that we
choose are the ﬁrst order increment processes of fractional Brownian motions
(see Mandelbrot and van Ness (1968)). Denote by tBH ptqutě0 a fractional
Brownian motion with Hurst index H P p0, 1q. It is well-known that BH is a
zero-mean self-similar Gaussian process with self-similarity index H and with
covariance function

`

˘
BH psq, BH ptq

“

Cov

`

1
2

s2H ` t2H ´ |s ´ t|2H

, for s, t ě 0.

(4.2)

˘

Fix h ą 0, deﬁne its increment process (with time variation h) to be

Z pHq
h

ptq “ BH pt ` hq ´ BH ptq, for t ě 0.

is also called fractional Gaussian noise. Using the covariance function (4.2)

Z pHq
h
we obtain the auto-covariance function of Z pHq

below: for τ ě 0,

´

γpτ q “ Cov

Z pHq
h

psq, Z pHq

h

ps ` τ q

“

|τ ` h|2H ` |τ ´ h|2H ´ 2|τ |2H

.

˘

(4.3)

2https://github.com/researchcoding/clustering_WSSP_with_cov_distance.

¯

h

`

1
2

33

Recall that for stationary Gaussian processes such as Z pHq
, the strict ergodicity
can be fully expressed in the language of its auto-covariance function γ, i.e.,
the following result (Maruyama, 1970; `Slęzak, 2017) provides a suﬃcient and
necessary condition for a stationary Gaussian process to be strictly ergodic.

h

Theorem 4.1 (Strict ergodicity of Gaussian processes). A continuous-time
Gaussian stationary process X is strictly ergodic if and only if

ż

t

0

1
t

lim
tÑ8

|γX puq| du “ 0,

(4.4)

where γX denotes the auto-covariance function of X.

In view of (4.3) we can deduce that the auto-covariance function γ of Z pHq
is second-order

satisﬁes (4.4). This together with Theorem 4.1 yields that Z pHq
strict-sense stationary ergodic, so it is also wide-sense stationary ergodic.

h

h

To test our algorithms we simulate κ “ 5 groups of independent fractional
Brownian paths, with the ith group containing 10 paths as tBHip1{nq, . . . , BHippn´
1q{nq, BHip1qu, for the self-similarity indexes

H1 “ 0.3, H2 “ 0.4,

. . . , H5 “ 0.7.

Remark that clustering a zero-mean fractional Brownian motion BH is equiva-
lent to clustering its increments Z pHq
1{n ptq “ BH pt ` 1{nq ´ BH ptq. These total
number of 50 observed paths of Z pHq
1{n ptq, each of length 150, compose an oﬄine
dataset and an online one. The clustering algorithms are applied to the dataset
at each time step t. 100 runs are made to compute the misclassiﬁcation rates. we
use oﬄine (RESP. online) dataset clustering algorithm to cluster oﬄine (RESP.
online) dataset. The purpose is to compare the the algorithms with and without
log˚-transformations.

Fig. 3 presents the comparisons of 2 algorithms: one is using the dissimilar-
ity measure xd˚, the other one is using the dissimilarity measure yd˚˚, based on
the behavior of misclassiﬁcation rates as time increases. We conclude that, both
algorithms with and without the log˚-transformations are asymptotically consis-
tent. However in both oﬄine and online settings, the covariance-based dissimi-
larity measure algorithms with log˚-transformation (dashed red lines) have 30%
lower misclassiﬁcation rates on average than that of algorithms without log˚-
transformation (solid blue lines). This simulation study proves the necessity of
utilizing log˚-transformed covariance-based dissimilarity measure when the un-
derlying observations have nonlinear, especially power based, covariance-based
dissimilarity measure, such as observations sampled from self-similar processes.
The codes in MATLAB used in this subsection are provided publicly online3.

3https://github.com/researchcoding/clustering_stochastic_processes.

34

Figure 3: The top graph illustrates the misclassiﬁcation rates by oﬄine algo-
rithm applied to oﬄine datasets of increments of fractional Brownian motions.
The bottom graph plots misclassiﬁcation rates by online algorithm applied to
online datasets.

4.3 Clustering ARp1q Processes: Non Strict-sense Station-

ary Ergodic

To show that our algorithms can be applied to clustering non strict-sense sta-
tionary ergodic processes, we consider a simulation study on the non-Gaussian
ARp1q process tY ptqut deﬁned in Example 2, Eq. (1.1). We then conduct the

35

cluster analysis with κ “ 5, and specify the values of a in Eq. (1.1) as

a1 “ ´0.4, a2 “ ´0.15, a3 “ 0.1, a4 “ 0.35, a5 “ 0.6.

We mimic the procedure in Section 4.2 to generate the oﬄine and online datasets
of tXptqut. Fig. 4 illustrates the consistent converging property of oﬄine algo-
rithm and online algorithm under diﬀerent dataset settings.

All the codes in MATLAB that reproduce the main conclusions in this sub-

section can be found publicly online4.

4https://github.com/researchcoding/clustering_nonGaussian_processes.

36

Figure 4: The top graph plots the misclassiﬁcation rates of (log˚) covariance-
based dissimilarity measure along with the increase of time using oﬄine and
online algorithms on oﬄine dataset. The bottom graph shows misclassiﬁcation
rates with both algorithms on online dataset.

37

4.4 Application to the Real World: Clustering Global Eq-

uity Markets

4.4.1 Data and Methodology

In this section we apply the clustering algorithms to real-world datasets. The
application involves in dividing equity markets of major economic entities in
the world into diﬀerent subgroups. In ﬁnancial economics, researchers usually
cluster global equity markets according to either geographical region or the de-
velopment stage of the underlying economic entities. The reasoning of these
clustering methods is that entities with less geographical distance and closer
development level involve in more bilateral economic activities. Impacted by
similar economic factors, entities with less “distance” tend to have higher cor-
relation in stock market performance. This correlation then measures the level
of “comovement” of stock market indexes on global capital market.

However, the globalization is breaking the barriers of region and develop-
ment level. For instance, in 2016 China became the largest trader partner with
the U.S. (besides EU)5. China is not a regional neighbor of the U.S., and is
categorized as a developing country by World Bank, in opposite to the U.S. as
a developed country.

We cluster the equity markets in the world according to the empirical covari-
ance structure of their performance, using Algorithms 1 and 2 as purposed in
this paper. Then we compare our clustering results with the traditional cluster-
ing methodologies. The index constituents of MSCI ACWI (All Country World
Index) are selected as the sample data. Each of the observations is a sample
path representing the historical monthly returns of underlying economic enti-
ties. Through empirical study it is proved that these indexes returns exhibit
the “long memory” path feature hence they can be modeled by self-similar pro-
cesses such as fractional Brownian motions (see e.g. Comte and Renault (1998);
Bianchi and Pianese (2008)). Therefore similar to Section 4.2 we may cluster the
increments of the indexes returns with the log˚-transformed dissimilarity mea-
sure yd˚˚. MSCI ACWI is the leading global equity market index and has $3.2
billion in underlying market capitalization6. MSCI ACWI contains 23 devel-
oped markets, 24 emerging markets from 4 regions: Americas, EMEA (Europe,
Middle East and Africa), Paciﬁc and Asia. Table 1 lists all markets included in
this empirical study. We exclude Greece market due to its bankruptcy after the
global ﬁnancial crisis.

We construct both oﬄine and online datasets starting from diﬀerent dates.
For oﬄine dataset we let it start from Jan. 30, 2009 to exclude the ﬁnancial
crisis period in 2007 and 2008. This is because, under global stock market crisis,
the (downside) performance of equity market is contagious and thus blurs the
cluster analysis. The online dataset starts on Jan. 31, 1989, which covers 1997
Asian ﬁnancial crisis, 2003 dot-com bubble and 2007 subprime mortgage crisis.

5Source: U.S. Department of Commerce, Census Bureau, Economic Indicators Division.
6As of June 30, 2017, as reported on September 30, 2017 by eVestment, Morningstar and

Bloomberg.

38

Another key feature is that 14 markets are added to the MSCI ACWI index
(at diﬀerent time) since 1989, including 1 developed market and 13 emerging
markets. Therefore, the case where new time series are observed is handled in
online dataset.

4.4.2 Clustering Results

We compare the clustering outcomes of both oﬄine and online datasets with
separations suggested by region (4 groups) and development level (2 groups).
The factor with the lowest misclassiﬁcation rate is proved to be the correspond-
ing factor that contributes to increase covariance-based dissimilarity measure
the most. In other words, this corresponding factor leads to the clustering of
stock markets with the most signiﬁcant impact.

Table 2 shows that the misclassiﬁcation rates for development levels are sig-
niﬁcantly and consistently lower than that of geographical region, for both algo-
rithms (oﬄine and online algorithms) and datasets (oﬄine and online datasets).
The clustering results seem to infer that the geographical distance is less dom-
inating than the development level of underlying economic entities, when ana-
lyzing diﬀerent groups of equity markets.

The global minimum of the misclassiﬁcation rate occurs when we use online
algorithm on oﬄine dataset. Table 3 presents the detailed clustering outcome
under this circumstance. In each group, the correctly and incorrectly catego-
rized equity markets are listed respectively. For instance, China (Mainland)
market is correctly categorized along with other emerging market. Meanwhile
Austria market, though being developed market in MSCI ACWI, is categorized
to the group where most of the equity markets are emerging markets. The mis-
classiﬁed markets in the emerging group are Austria, Finland, Italy, Norway and
Spain markets. The misclassiﬁed markets in the developed group are Malaysia,
Philippines, Taiwan, Chile and Mexico markets. These empirical results thus
suggest that several capital markets have irregular post-crisis performance which
blurs the barrier between emerging and developed markets.

The contribution of this real-world dataset cluster analysis is two-fold. First,
we explored and determined the principal force that brings structural diﬀerence
in global capital markets, which potentially predicts the “comovement” pattern
of future index performance. Second, we provided new evidence on the impact
of globalization on breaking geographical barriers between economic entities.

5 Conclusion and Future Perspectives

Inspired by Khaleghi et al (2016), we introduce the problem of clustering wide-
sense stationary ergodic processes. A new covariance-based dissimilarity mea-
sure is proposed to obtain asymptotically consistent clustering algorithms for
both oﬄine and online settings. The recommended algorithms are competitive
for at least two reasons:

39

m
o
r
f

s
t
e
k
r
a
m
3
2

e
r
a

e
r
e
h
T

.
)
x
e
d
n
I

d

l
r
o

W
y
r
t
n
u
o
C

l
l

A
(

I

W
C
A

I

C
S
M

e
h
t

n

i

s
t
e
k
r
a
m
y
t
i

u
q
e

r
o
j
a
m

f
o

s
e
i
r
o
g
e
t
a
c

e
h
T

:
1

e
l
b
a
T

,
s
a
c
i
r
e
m
A
s
n
i
a
t
n
o
c

g
n
i
r
e
t
s
u
l
c

l
a
c
i
h
p
a
r
g
o
e
g

e
h
T

.
s
a
e
r
a

r
o

s
e
i
r
t
n
u
o
c

g
n

i
g
r
e
m
e
m
o
r
f

s
t
e
k
r
a
m
4
2

d
n
a

,
s
e
i
t
i
t
n
e

c
i
m
o
n
o
c
e

d
e
p
o
l
e
v
e
d

.
a
i
s
A
d
n
a

c
ﬁ

i
c
a
P

,
)
a
c
i
r
f
A
d
n
a

t
s
a
E
e
l
d
d
i
M

,
e
p
o
r
u
E
(
A
E
M
E

s
t
e
k
r
a
M
g
n
i
g
r
e
m
E

s
t
e
k
r
a
M
d
e
p
o
l
e
v
e
D

a
i
s
A

a
c
i
r
f
A
&
t
s
a
E
e
l
d
d
M
&
e
p
o
r
u
E

i

s
a
c
i
r
e
m
A

c
ﬁ

i
c
a
P

t
s
a
E
e
l
d
d
i
M
&
e
p
o
r
u
E

s
a
c
i
r
e
m
A

)
d
n
a
l
n
i
a
M

(

a
n
i
h
C

c
i
l
b
u
p
e
R
h
c
e
z
C

a
i
d
n
I

a
i
s
e
n
o
d
n
I

a
e
r
o
K

a
i
s
y
a
l
a
M

n
a
t
s
i
k
a
P

s
e
n
i
p
p
i
l
i
h
P

n
a
w
i
a
T

d
n
a
l
i
a
h
T

e
c
e
e
r
G

y
r
a
g
n
u
H

d
n
a
l
o
P

a
i
s
s
u
R

y
e
k
r
u
T

t
p
y
g
E

a
c
i
r
f
A
h
t
u
o
S

r
a
t
a
Q

s
e
t
a
r
i

m
E
b
a
r
A
d
e
t
i
n
U

l
i
z
a
r
B

e
l
i

h
C

a
i
b
m
o
l
o
C

o
c
i
x
e
M

u
r
e
P

a
i
l
a
r
t
s
u
A

g
n
o
K
g
n
o
H

n
a
p
a
J

d
n
a
l
a
e
Z
w
e
N

e
r
o
p
a
g
n
S

i

a
d
a
n
a
C

A
S
U

40

a
i
r
t
s
u
A

m
u
i
g
l
e
B

k
r
a
m
n
e
D

d
n
a
l
n
i
F

e
c
n
a
r
F

y
n
a
m
r
e
G

d
n
a
l
e
r
I

l
e
a
r
s
I

y
l
a
t
I

y
a
w
r
o
N

l
a
g
u
t
r
o
P

n
i
a
p
S

n
e
d
e
w
S

s
d
n
a
l
r
e
h
t
e
N

d
n
a
l
r
e
z
t
i
w
S

m
o
d
g
n
i
K
d
e
t
i
n
U

.
i
w
c
a
/
m
o
c
.
i
c
s
m
.
w
w
w
/
/
:
s
p
t
t
h

.
n
o
i
t
a
c
o
l
l
a

t
e
k
r
a
m

)
x
e
d
n
I

d
l
r
o

W
y
r
t
n
u
o
C

l
l

A
(

I

W
C
A

I
C
S
M

:
e
c
r
u
o
S

Table 2: The misclassiﬁcation rates of clustering algorithms on datasets, com-
paring to clusters suggested by geographical region and development levels.

oﬄine algorithm

online algorithm

region

development level

region

development level

oﬄine dataset
online dataset

63.04%
59.57%

28.26%
44.68%

60.87%
57.45%

23.91%
38.30%

Table 3: The clustering outcome of equity markets using oﬄine dataset (starting
from Jan. 30, 2009) and online algorithm. The algorithm divides the whole
dataset (excluding Greece) into two groups, and in each group the correctly and
correctly separated markets are listed, respectively.

Group 1 (Emerging Markets)

Group 2 (Developed Markets)

Incorrect
Malaysia
Philippines
Taiwan
Thailand
Chile
Mexico

Incorrect
Austria
Finland
Italy
Norway
Spain

Correct
China (Mainland)
India
Indonesia
Korea
Pakistan
Brazil
Colombia
PERU
Czech Republic
Hungary
Poland
Russia
Turkey
Egypt
South Africa
Qatar
United Arab Emirates

Correct
Belgium
Denmark
France
Germany
Ireland
Israel
Netherlands
Portugal
Sweden
Switzerland
United Kingdom
Australia
Hong Kong
Japan
New Zealand
Singapore
Canada
USA

41

1. Our algorithms are applicable to clustering a wide class of stochastic pro-
cesses, including any strict-sense stationary ergodic processes whose co-
variance structures are ﬁnite.

2. Our algorithms are eﬃcient enough in terms of their computational com-
plexity cost. In particular, a so-called log˚-transformation is introduced
to improve the eﬃciency of clustering, for self-similar processes.

The above advantages have been supported through the simulation study on
non-Gaussian discrete-time processes, fractional Brownian motions, non-Gaussian
non strict-sense stationary ergodic ARp1q processes, and a real-world applica-
tion: clustering global equity markets. The implementations in MATLAB of
our clustering algorithms are provided publicly online.

Finally we note that, the clustering framework proposed in our paper focuses
on the cases where the true number of clusters κ is known. The case for which κ
is unknown is still open and left to future research. Another interesting problem
is that, many stochastic processes are not wide-sense stationary but they get a
tight relationship with the wide-sense stationarity. For example, a self-similar
process does not necessarily have wide-sense stationary increments, but their
Lamperti transformations are strict-sense stationary (Lamperti, 1962); locally
asymptotically self-similar processes are generally not self-similar but their tan-
gent processes are self-similar (Boufoussi et al, 2008). Our cluster analysis sheds
light on clustering the above processes. These topics can be left for future re-
search.

References

Bastos J. A., Caiado J. (2014) Clustering ﬁnancial time series with variance

ratio statistics. Quantitative Finance 14(12):2121–2133.

Bianchi S., Pianese A. (2008) Multifractional properties of stock indices decom-
posed by ﬁltering their pointwise H¨older regularity. International Journal of
Theoretical and Applied Finance 11(06):567–595.

Boufoussi B., Dozzi M., Guerbaz R. (2008) Path properties of a class of lo-
cally asymptotically self similar processes. Electronic Journal of Probability
13(29):898–921.

Cambanis S., Hardin C. J., Weron A. (1987) Ergodic properties of stationary
stable processes. Stochastic Processes and their Applications 24(1):1–18.

Cesa-Bianchi N., Lugosi G. (2006) Prediction, Learning, and Games. Cambridge

University Press.

Comte F., Renault E. (1998) Long memory in continuous-time stochastic volatil-

ity models. Mathematical Finance 8(4):291–323.

42

Damian D., Oreˇsiˇc M., Verheij E., et al. (2007) Applications of a new subspace
clustering algorithm (COSA) in medical systems biology. Metabolomics
3(1):69–77.

Embrechts P., Maejima M. (2000) An introduction to the theory of self-
similar stochastic processes. International Journal of Modern Physics B
14(12):1399–1420.

Gray R. M. (1988) Probability, Random Processes, and Ergodic Properties.

Springer.

Hartigan J. A. (1975) Clustering Algorithms. John Wiley & Sons, Inc.

Herdin M., Czink N., Ozcelik H., Bonek E. (2005) Correlation matrix distance,
a meaningful measure for evaluation of non-stationary MIMO channels. In:
IEEE 61st Vehicular Technology Conference, 2005., vol 1, pp. 136–140.

Hirkhorshidi A. S., Aghabozorgi S., Wah T. Y. (2015) A comparison study on
similarity and dissimilarity measures in clustering continuous data. PLoS
ONE 10(12):e0144059.

Ieva F., Paganoni A. M., Tarabelloni N. (2016) Covariance-based clustering
in multivariate and functional data analysis. Journal of Machine Learning
Research 17:1–21.

J¨a¨askinen V., Parkkinen V., Cheng L., Corander J. (2014) Bayesian clustering
of DNA sequences using markov chains and a stochastic partition model.
Statistical Applications in Genetics and Molecular Biology 13(1):105–121.

Jain A. K., Mao J. (1996) A self-organizing network for hyperellipsoidal clus-

tering (HEC). IEEE Transactions on Neural Networks 7:16–29.

Jain A. K., Murty M. N., Flynn P. J. (1999) Data clustering: a review. ACM

Computing Surveys (CSUR) 31(3):264–323.

Juozapaviˇcius A., Rapsevicius V. (2001) Clustering through decision tree con-
struction in geology. Nonlinear Analysis: Modelling and Control 6(2):29–41.

Katsavounidis I., Kuo C. J., Zhang Z. (1994) A new initialization technique for
generalized Lloyd iteration. IEEE Signal Processing Letters 1(10):144–146.

Khaleghi A., Ryabko D., Mari J., Preux P. (2016) Consistent algorithms for
clustering time series. Journal of Machine Learning Research 17(3):1–32.

Kleinberg J. M. (2003) An impossibility theorem for clustering. In: Advances
in Neural Information Processing Systems (NIPS), vol 15, pp. 463–470.

Lamperti J. W. (1962) Semi-stable stochastic processes. Transactions of the

American Mathematical Society 104:62–78.

Magdziarz M., Weron A. (2011) Ergodic properties of anomalous diﬀusion pro-

cesses. Annals of Physics 326:2431–2443.

Mandelbrot B., van Ness J. W. (1968) Fractional Brownian motions, fractional

noises and applications. SIAM Review 10(4):422–437.

Maruyama G. (1970) Inﬁnitely divisible processes. Theory of Probability and

43

its Applications 15(1):1–22.

Pavlidis N. G., Plagianakos V. P., Tasoulis D. K., Vrahatis M. N. (2006) Fi-
nancial forecasting through unsupervised clustering and neural networks.
Operational Research 6(2):103–127.

Peng J., M¨uller H.-G. (2008) Distance-based clustering of sparsely observed
stochastic processes, with applications to online auctions. The Annals of
Applied Statistics 2(3):1056–1077.

Peng Q. (2012) Uniform H¨older exponent of a stationary increments Gaussian
process: estimation starting from average values. Statistics & Probability
Letters 81(8):1326–1335.

Rubinstein M., Joulin A., Kopf J., Liu C. (2013) Unsupervised joint object
discovery and segmentation in internet images. In: The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pp. 1939–1946.

Samorodnitsky G. (2004) Extreme value theory, ergodic theory and the bound-
ary between short memory and long memory for stationary stable processes.
The Annals of Probability 32(2):1438–1468.

Samorodnitsky G., Taqqu M. S. (1994) Stable Non-Gaussian Random Processes:
Stochastic Models with Inﬁnite Variance. Chapman & Hall, New York.

Sen P. K., Singer J. M. (1993) Large Sample Methods in Statistics. Chapman

& Hall, Inc.

Shields P. C. (1996) The Ergodic Theory of Discrete Sample Paths, Graduate

Studies in Mathematics, vol 13. American Mathematical Society.

`Slęzak J. (2017) Asymptotic behaviour of time averages for non-ergodic Gaus-

sian processes. Annals of Physics 383:285–311.

Slonim N., Atwal G. S., Tkaˇcik G., Bialek W. (2005) Information-based clus-

tering. PNAS 102(51):18297–18302.

Wilson D. R., Martinez T. R. (1997) Improved heterogeneous distance functions.

JAIR 6:1–34.

Xu R., Wunsch D. (2005) Survey of clustering algorithms. IEEE Transactions

on Neural Networks 16(3):645–678.

Zhao W., Zou W., Chen J. J. (2014) Topic modeling for cluster analysis of large

biological and medical datasets. BMC Bioinformatics 15:S11.

44

9
1
0
2
 
l
u
J
 
1
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
9
4
0
9
0
.
1
0
8
1
:
v
i
X
r
a

Covariance-based Dissimilarity Measures Applied
to Clustering Wide-sense Stationary Ergodic
Processes

Qidi Peng∗

Nan Rao†

Ran Zhao‡

Abstract

We introduce a new unsupervised learning problem: clustering wide-
sense stationary ergodic stochastic processes. A covariance-based dis-
similarity measure together with asymptotically consistent algorithms is
designed for clustering oﬄine and online datasets, respectively. We also
suggest a formal criterion on the eﬃciency of dissimilarity measures, and
discuss of some approach to improve the eﬃciency of our clustering algo-
rithms, when they are applied to cluster particular type of processes, such
as self-similar processes with wide-sense stationary ergodic increments.
Clustering synthetic data and real-world data are provided as examples
of applications.

cluster analysis ¨ wide-sense stationary ergodic processes ¨

Keywords:
covariance-based dissimilarity measure ¨ self-similar processes
MCS (2010): 62-07 ¨ 60G10 ¨ 62M10

∗Institute of Mathematical Sciences, Claremont Graduate University, Claremont, CA

91711. Email: qidi.peng@cgu.edu.

nan.rao@sjtu.edu.cn.

†School of Mathematical Sciences, Shanghai Jiao Tong University, Shanghai, China. Email:

‡Institute of Mathematical Sceinces and Drucker School of Management, Claremont Grad-

uate University, Claremont, CA 91711. Email: ran.zhao@cgu.edu.

1

1

Introduction

Cluster analysis, as a core category of unsupervised learning techniques, allows
to discover hidden patterns in data where one does not know the true answer
upfront. Its goal is to assign a heterogeneous set of objects into non-overlapping
clusters, where in each cluster any two objects are more related to each other
than to objects in other clusters. Given its exploratory nature, clustering has
nowadays a number of applications in various ﬁelds of both industry and sci-
entiﬁc research, such as biological and medical research (Damian et al, 2007;
Zhao et al, 2014; J¨a¨askinen et al, 2014), information technology (Jain et al,
1999; Slonim et al, 2005), signal and image processing (Rubinstein et al, 2013),
geology (Juozapaviˇcius and Rapsevicius, 2001) and ﬁnance (Pavlidis et al, 2006;
Bastos and Caiado, 2014; Ieva et al, 2016). There exists a rich literature of clus-
ter analysis on random vectors, where the objects, waiting to be clustered, are
sampled from high-dimensional joint distributions. There is no shortage of such
clustering algorithms (Xu and Wunsch, 2005). However, stochastic processes are
quite a diﬀerent setting from random vectors, since their observations (sample
paths) are sampled from processes distributions. While the cluster analysis on
random vectors has developed aggressively, clustering on stochastic processes
receives much less attention. Today cluster analysis on stochastic processes
deserves increasingly intense study, thanks to their vital importance to many
applied areas, where the collected information are indexed by real time and are
especially long. Examples of these time-indexed information include biological
data, ﬁnancial data, marketing data, surface weather data, geological data and
video/audio data, etc.

Recall that in the setting of random vectors, a process of clustering often

consists of two steps:

Step 1 One suggests a suitable dissimilarity measure to describe the distance
between 2 objects, under which “two objects are close to each other”
becomes meaningful.

Step 2 One designs an enough accurate and computationally eﬃcient cluster-

ing function based on the above dissimilarity measure.

Clustering stochastic processes is performed in a similar way but new challenges
may arise in both Step 1 and Step 2. Intuitively, one can always apply existing
random vectors clustering approaches to cluster arbitrary stochastic processes,
such as non-hierarchical approaches (K-means clustering methods) and hierar-
chical approaches (agglomerative method, divisive method) (Hartigan, 1975),
based on “naive” dissimilarity measures (e.g., Euclidean distance, Manhattan
distance or Minkowski distance). However, one faces at least 2 potential risks
when applying the above approaches to clustering stochastic processes:

Risk 1 These approaches might suﬀer from their huge complexity costs, due
to the great length of their sample paths. As a result classical clustering
algorithms are often computationally forbidding (Ieva et al, 2016; Peng
and M¨uller, 2008).

2

Risk 2 These approaches might suﬀer from over-ﬁtting issues. For example,
clustering stationary or periodic processes based on Euclidean distance
between the paths, without considering their path properties will result in
“over ﬁtting, bad clusters” situation.

In summary, classical dissimilarity measures or clustering strategies would fail
in clustering stochastic processes.

Fortunately, the complexity cost and the over-ﬁtting errors of clustering
processes could be largely reduced, if one is aware of the fact that a stochastic
process often possesses ﬁne paths features (e.g., stationarity, Markov property,
self-similarity, sparsity, seasonality, etc.), which is unlike an arbitrary random
vector. An appropriate dissimilarity measure then should be chosen to be able
to capture these paths features. Clustering processes is then performed to group
any two sample paths into one group, if they are relatively close to each other
under that particular dissimilarity measure. Below are some examples provided
in the literature.

Peng and M¨uller (2008) proposed a dissimilarity measure between two special
sample paths of processes. In their setting it is supposed that, for each path only
sparse and irregularly spaced measurements with additional measurement errors
are available. Such features occur commonly in longitudinal studies and online
trading data. Based on this particular dissimilarity measure, classiﬁcation and
cluster analysis could be made. Ieva et al (2016) developed a new algorithm to
perform clustering of multivariate and functional data, based on a covariance-
based dissimilarity measure. Their attention is focused on the speciﬁc case of a
set of observations from two populations, whose probability distributions have
equal mean but diﬀer in terms of covariances. Khaleghi et al (2016) designed
consistent algorithms for clustering strict-sense stationary ergodic processes (see
the forthcoming Eq. (1.4) for the deﬁnition of strict-sense ergodicity), where
the dissimilarity measure is proposed as distance of process distributions. It is
worth noting that the consistency of their algorithms is guaranteed thanks to
the assumption of strict-sense ergodicity.

In this framework, we aim to design asymptotically consistent algorithms to
cluster a general class of stochastic processes, i.e., wide-sense stationary ergodic
processes (see Deﬁnition 1.1 below). Asymptotically consistent algorithms can
be obtained for this setting, since the covariance stationarity and ergodicity
allow the process to present some featured asymptotic behavior with respect to
their length, rather than to the total number of paths.

Deﬁnition 1.1 (Wide-sense stationary ergodic process). A stochastic process
X “ tXtutPT (the time indexes set T can be either R` “ r0, `8q or N “
t1, 2 . . .u) is called wide-sense stationary if its mean and covariance structure
are ﬁnite and time-invariant: EpXtq “ µ for any t P T , and for any subset
pXi1 , . . . , Xir q, its covariance matrix remains invariant subject to any time shift
h ą 0:

CovpXi1 , . . . , Xir q “ CovpXi1`h, . . . , Xir`hq.
Denote by γ the auto-covariance function of X. Then X is further called weakly

3

ergodic (or wide-sense ergodic) if it is ergodic for the mean and the second-order
moment:

• If X is a continuous-time process (e.g., T “ R`), then it satisﬁes for any

ż

s`h

1
h

s

Xu du

a.s.
ÝÝÝÝÑ
hÑ`8

µ,

pXu`τ ´ µqpXu ´ µq du

a.s.
ÝÝÝÝÑ
hÑ`8

γpτ q, for all τ P R`,

ÝÝÑ denotes the almost sure convergence (convergence with proba-

• If X is a discrete-time process (e.g., T “ N), then it satisﬁes for any

Xs ` Xs`1 ` . . . ` Xs`h
h ` 1

a.s.
ÝÝÝÝÝÝÝÝÑ
hPN, hÑ`8

µ,

s P R`,

and

ż

s`h

1
h

s

where a.s.
bility 1).

s P N Y t0u,

and

ř

s`h
u“spXu`τ ´ µqpXu ´ µq
h ` 1

a.s.
ÝÝÝÝÝÝÝÝÑ
hPN, hÑ`8

γpτ q, for all τ P N Y t0u.

Wide-sense stationarity and ergodicity are believed to be a very general

assumption, at least in the following senses:

1. The assumption that each process is generated by some mean and co-
variance structure is suﬃcient for capturing all features of a wide-sense
stationary ergodic process. In other words, our algorithms intend to clus-
ter means and auto-covariance functions, not process distributions.

2. Wide-sense stationary ergodic process partially extends the strict-sense
one. A ﬁnite-variance strict-sense stationary ergodic process (see Eq. (1.4)
for its deﬁnition) is also wide-sense stationary ergodic. However strict-
sense stationary ergodic stable processes are not wide-sense stationary,
because their variances explode (Cambanis et al, 1987; Samorodnitsky,
2004).

3. A Gaussian process can be fully identiﬁed only by its mean and covariance
structure. Then a wide-sense stationary ergodic Gaussian process is also
strict-sense stationary ergodic.

4. In the clustering problem, the dependency among the sample paths can

be arbitrary.

4

There is a long list of processes which are wide-sense stationary ergodic, but not
necessarily stationary in the strict sense. The examples of wide-sense stationary
processes below are not exhausted.

Example 1 Non-independent White Noise.

Let U be a random variable uniformly distributed over p0, 2πq and deﬁne

?

Zptq :“

2 cosptU q, for t P N.

The process Z “ tZptqutPN is then a white noise because it veriﬁes

EpZptqq “ 0, VarpZptqq “ 1 and CovpZpsq, Zptqq “ 0, for s ‰ t.

We claim that Z is wide-sense stationary ergodic, which can be obtained
by using the Kolmogorov’s strong law of large numbers, see e.g. Theorem
2.3.10 in Sen and Singer (1993). However Z is not strict-sense stationary
since

pZp1q, Zp2qq ‰ pZp2q, Zp3qq in law.

Indeed, it is easy to see that

`
0 ă E

Zp1q2Zp2q

˘

`
‰ E

˘
Zp2q2Zp3q

“ 0.

Example 2 Auto-regressive Models.

It is well-known that an auto-regressive model tY ptqut „ ARp1q in the
form:

Y ptq “ aY pt ´ 1q ` Zptq, |a| ă 1, a ‰ 0, for t P N

(1.1)

is wide-sense stationary ergodic. However it is not necessarily strict-sense
stationary ergodic, when the joint distributions of the white noise tZptqut
are not invariant with time-shifting (e.g., take tZptqut to be the white
noise in Example 1).

Example 3 Increment Process of Fractional Brownian Motion.

Let tBH ptqut be a fractional Brownian motion with Hurst index H P p0, 1q
(see Mandelbrot and van Ness (1968)). For each h ą 0, its increment pro-
cess tZ hptq :“ BH pt`hq´BH ptqut is ﬁnite-variance strict-sense stationary
ergodic (Magdziarz and Weron, 2011). As a result it is also wide-sense sta-
tionary ergodic. More detail will be discussed in Section 4.

Example 4 Increment Process of More General Gaussian Processes.

Peng (2012) introduced a general class of zero-mean Gaussian processes
X “ tXptqutPR having stationary increments.
Its variogram νptq :“
2´1EpXptq2q satisﬁes:

(1) There is a non-negative integer d such that ν is 2d-times continu-
ously diﬀerentiable over r´2, 2s, but not 2pd ` 1q-times continuously
diﬀerentiable over r´2, 2s.

5

(2) There are 2 real numbers c ‰ 0 and s0 P p0, 2q, such that for all

t P r´2, 2s,

νptq “ νp2dqp0q ` c|t|s0 ` rptq,

where the remainder rptq satisﬁes:

• rptq “ op|t|s0 q, as t Ñ 0.
• There are two real numbers c1 ą 0, ω ą s0 and an integer
q ą ω ` 1{2 such that r is q-times continuously diﬀerentiable
on r´2, 2szt0u and for all t P r´2, 2szt0u, we have

|rpqqptq| ď c1|t|ω´q.

It is shown that the process X extends fractional Brownian motion and it
also has wide-sense (and strict-sense) stationary ergodic increments when
d ` s0{2 P p0, 1q (see Proposition 3.1 in Peng (2012)).

The problem of clustering processes via their means and covariance structures
leads us to formulating our clustering targets in the following way.

Deﬁnition 1.2 (Ground-truth G of covariance structures). Let

(cid:32)
G1, . . . , Gκ

(

G “

be a partitioning of N “ t1, 2, . . .u into κ disjoint sets Gk, k “ 1, . . . , κ, such
that the means and covariance structures of xi, i P N are identical, if and only
if i P Gk for some k “ 1, . . . , κ. Such G is called ground-truth of covariance
structures. We also denote by G|N the restriction of G to the ﬁrst N sequences:

G|N “

(cid:32)
Gk X t1, . . . , N u : k “ 1, . . . , κ

(
.

Our clustering algorithms will aim to output the ground-truth partitioning
G, as the sample length grows. Before stating these algorithms, we introduce
the inspiring framework done by Khaleghi et al (2016).

1.1 Preliminary Results: Clustering Strict-sense Station-

ary Ergodic Processes

Khaleghi et al (2016) considered the problem of clustering strict-sense stationary
ergodic processes. The main fruit in Khaleghi et al (2016) is obtaining the so-
called asymptotically consistent algorithms to cluster processes of that type. We
brieﬂy state their work below. Depending on how the information is collected,
the stochastic processes clustering problems consist of dealing with two models:
oﬄine setting and online setting.

Oﬄine setting: The observations are assumed to be a ﬁnite number N of

paths:

´

¯

´

x1 “

X p1q

1 , . . . , X p1q
n1

, . . . , xN “

X pN q
1

, . . . , X pN q
nN

¯
.

6

Each path is generated by one of the κ diﬀerent unknown process dis-
tributions. In this case, an asymptotically consistent clustering function
should satisfy the following.

Deﬁnition 1.3 (Consistency: oﬄine setting). A clustering function f is
consistent for a set of sequences S if f pS, κq “ G. Moreover, denoting
n “ mintn1, . . . , nN u, f is called strongly asymptotically consistent in the
oﬄine sense if with probability 1 from some n on it is consistent on the
set S, i.e.,

¯

´

P

lim
nÑ8

f pS, κq “ G

“ 1.

It is called weakly asymptotically consistent if

Ppf pS, κq “ Gq “ 1.

lim
nÑ8

Online setting: In this setting the observations, having growing length and

number of scenarios with respect to time t, are denoted by
¯

´

x1 “

X p1q

1 , . . . , X p1q
n1

, . . . , xN ptq “

´
X pN ptqq
1

, . . . , X pN ptqq
nN ptq

¯
,

where the index function N ptq is non-decreasing with respect to t.
Then an asymptotically consistent online clustering function is deﬁned
below:

Deﬁnition 1.4 (Consistency: online setting). A clustering function is
strongly (RESP. weakly) asymptotically consistent in the online sense, if
for every N P N the clustering f pSptq, κq|N is strongly (RESP. weakly)
asymptotically consistent in the oﬄine sense, where f pSptq, κq|N is the
clustering f pSptq, κq restricted to the ﬁrst N sequences:

f pSptq, κq|N “ tf pSptq, κq X t1, . . . , N u : k “ 1, . . . , κu .

There is a detailed discussion on the comparison of oﬄine and online settings in
Khaleghi et al (2016), stating that these two settings have signiﬁcant diﬀerences,
since using the oﬄine algorithm in the online setting by simply applying it to the
entire data observed at every time step, does not result in an asymptotically
consistent algorithm. Therefore separately and independently studying these
two settings becomes necessary and meaningful.

As the main results in Khaleghi et al (2016), asymptotically consistent clus-
tering algorithms for both oﬄine and online settings are designed. They are
then successfully applied to clustering synthetic and real data sets.

Note that in the framework of Khaleghi et al (2016), a key step is intro-
duction to the so-called distributional distance (Gray, 1988): the distributional
distance between a pair of process distributions ρ1, ρ2 is deﬁned to be

dpρ1, ρ2q “

wmwl

|ρ1pBq ´ ρ2pBq| ,

(1.2)

8ÿ

ÿ

m,l“1

BPBm,l

where:

7

• The sets Bm,l, m, l ě 1 are obtained via the partitioning of Rm into cubes

of dimension m and volume 2´ml, starting at the origin.

• The sequence of weights twjujě1 is positive and decreasing to zero. More-
over it should be chosen such that the series in (1.2) is convergent. The
weights are often suggested to give precedence to earlier clusterings, pro-
tecting the clustering decisions from the presence of the newly observed
sample paths, whose corresponding distance estimates may not yet be ac-
curate. For instance, it is set to be wj “ 1{jpj ` 1q in Khaleghi et al
(2016).

Further, the distance between two sample paths x1, x2 of stochastic processes
is given by

mnÿ

lnÿ

ÿ

pdpx1, x2q “

m“1

l“1

BPBm,l

wmwl

|νpx1, Bq ´ νpx2, Bq|,

(1.3)

where:

• mn, ln (ď n) can be arbitrary sequences of positive integers increasing to

inﬁnity, as n Ñ 8.

• For a process path x “ pX1, . . . , Xnq, and an event B, νpx, Bq denotes
the average times that the event B occurs over n ´ mn ` 1 time intervals.
More precisely,

νpx, Bq :“

1
n ´ mn ` 1

n´mn`1ÿ

i“1

1tpXi, . . . , Xi`mn´1q P Bu.

The process distribution X from which x is sampled is called strictly ergodic if

´

P

lim
nÑ8

¯
νpx, Bq “ PpX P Bq

“ 1, for all B.

(1.4)

The assumption that the processes are ergodic leads to that pd is a strongly
consistent estimator of d:
´

¯

P

lim
nÑ8

pdpx1, x2q “ dpρ1, ρ2q

“ 1,

where ρ1, ρ2 are the process distributions corresponding to x1, x2, respectively.
Based on the distances d and their estimates pd, the asymptotically consis-
tent algorithms for clustering stationary ergodic processes in each of the oﬄine
and online settings are provided (see Algorithms 1, 2 and Theorems 11, 12 in
Khaleghi et al (2016)). Khaleghi et al (2016) also show that their methods
can be implemented eﬃciently: they are at most quadratic in each of their
arguments, and are linear (up to log terms) in some formulations.

8

1.2 Statistical Setting: Clustering Wide-sense Stationary

Ergodic Processes

Inspired by the framework of Khaleghi et al (2016), we consider the problem
of clustering wide-sense stationary ergodic processes. We ﬁrst introduce the
following covariance-based dissimilarity measure, which is one of the main con-
tributions of this paper.

Deﬁnition 1.5. (Covariance-based dissimilarity measure) The covariance-based
dissimilarity measure d˚ between a pair of processes X p1q, X p2q (in fact X p1q,
X p2q denote two covariance structures, each may contain diﬀerent process dis-
tributions) is deﬁned as follows:

`

˘

8ÿ

d˚

X p1q, X p2q

:“

wmwl

´´

`
E

X p1q

ˆM

m,l“1
˘

`

l...l`m´1

, Cov

X p1q

l...l`m´1

˘¯

´

`
E

X p2q

,

˘

`

, Cov

X p2q

l...l`m´1

l...l`m´1

˘¯¯

,

(1.5)

where:

• For j “ 1, 2, tX pjq

l ulPN denotes some path sampled from the process X pjq.
We assume that all possible observations of the process X pjq is a subset
l ulPN. For l1 ě l ě 1, we deﬁne the shortcut notation X pjq
of tX pjq
l...l1 :“
pX pjq
, . . . , X pjq
l

l1 q.

• The function M is deﬁned by: for any p1, p2, p3 P N, any 2 vectors v1, v2 P

Rp1 and any 2 matrices A1, A2 P Rp2ˆp3,

Mppv1, A1q, pv2, A2qq :“ |v1 ´ v2| ` ρ˚ pA1, A2q .

(1.6)

• The distance ρ˚ between 2 equal-sized matrices M1, M2 is deﬁned to be

ρ˚pM1, M2q :“ }M1 ´ M2}F ,

(1.7)

with } ¨ }F being the Frobenius norm:
for an arbitrary matrix M “ tMijui“1,...,m;j“1,...,n,

}M }F :“

|Mij|2.

g
f
f
e

mÿ

nÿ

i“1

j“1

Introduction to the matrices distance ρ˚ is inspired by Herdin et al (2005).
The matrices distance given in Herdin et al (2005) is used to measure the
distance between 2 correlation matrices. However, our distance ρ˚ is a
modiﬁcation of the one in the latter paper. Indeed, unlike Herdin et al
(2005), ρ˚ is a well-deﬁned metric distance, as it satisﬁes the triangle
inequalities.

9

• The sequence of positive weights twju is chosen such that d˚pX p1q, X p2qq
is ﬁnite. Observe that the distances | ¨ | and ρ˚ in (1.5) do not depend on
l, as a result we necessarily have

8ÿ

l“1

wl ă `8.

(1.8)

In practice a typical choice of weights we suggest is wj “ 1{jpj ` 1q, j “
1, 2, . . .. This is because, for most of the well-known covariance stationary
ergodic processes (causal ARM App, qq, increments of fractional Brownian
motions, etc.), their auto-covariance functions are absolutely summable:
denote by γX the auto-covariance function of tXtut,

`8ÿ

h“´8

|γX phq| ă `8.

(1.9)

`Slęzak (2017) pointed out that (1.9) is a suﬃcient condition for tXtu be-
ing mean-ergodic. However (1.9) does not necessarily imply that tXtu is
covariance-ergodic. It becomes a suﬃcient and necessary condition if tXtu
is Gaussian. Therefore subject to (1.9), taking wj “ 1{jpj ` 1q, we obtain
for any integer N ą 0,

Nÿ

ˇ
ˇ
ˇE

´
X p1q

wmwl

¯

´

l...l`m´1

´ E

X p2q

l...l`m´1

¯ˇ
ˇ
ˇ

m,l“1

Nÿ

“

m,l“1

?

`8ÿ

m,l“1

wmwl

m|µ1 ´ µ2| “ |µ1 ´ µ2|

Nÿ

m,l“1

?

1
mpm ` 1qlpl ` 1q

ď |µ1 ´ µ2|

?

1
mpm ` 1qlpl ` 1q

ă `8,

(1.10)

10

`
with µj “ E

X pjq
1

˘

, for j “ 1, 2; and

wmwlρ˚

Cov

´
X p1q

¯

´
X p2q

, Cov

l...l`m´1

l...l`m´1

¯¯

Nÿ

m,l“1

Nÿ

ď

“

ď

m,l“1

Nÿ

m,l“1

Nÿ

m,l“1
Nÿ

´

g
f
f
e2

g
f
f
e2

g
f
f
e2m

?

mÿ

mÿ

k1“1

k2“1

m´1ÿ

q“´pm´1q

m´1ÿ

wmwl

wmwl

wmwl

pγX p|k1 ´ k2|qq2

pm ´ |q|q pγX p|q|qq2

pγX p|q|qq2

q“´pm´1q

ď c

m,l“1

2m
mpm ` 1qlpl ` 1q
ř

where the constant c “
bining (1.10) and (1.11) leads to

ď c

`8ÿ

m,l“1

?

2m
mpm ` 1qlpl ` 1q

ă `8,(1.11)

8

q“´8 |CovpX1, X1`|q|q| ă `8. Therefore com-

`

˘

d˚

X p1q, X p2q

ă `8.

Hence d˚pX p1q, X p2qq in (1.5) is well-deﬁned.

In (1.5) and (1.6) we see that the behavior of the dissimilarity measure d˚ is
jointly explained by the Euclidean distance of means and the matrices distance
of covariance matrices. If the means of the processes X p1q and X p2q are priorly
known to be equal, the distance d˚ can be simpliﬁed to:

`

˘

8ÿ

d˚

X p1q, X p2q

“

´

`

wmwlρ˚

Cov

X p1q

l...l`m´1

˘

`

, Cov

X p2q

l...l`m´1

˘¯

.

(1.12)

m,l“1

Note that this dissimilarity measure can be applied on self-similar processes,
since they are all zero-mean (see Section 3).

Next we provide consistent estimator of d˚pX p1q, X p2qq. For 1 ď l ď n and
m ď n ´ l ` 1, deﬁne µ˚pXl...n, mq to be the empirical mean of a process X’s
sample path pXl, . . . , Xnq:

µ˚pXl...n, mq :“

pXi . . . Xi`m´1qT ,

(1.13)

1
n ´ m ´ l ` 2

n´m`1ÿ

i“l

and deﬁne ν˚pXl...n, mq to be the empirical covariance matrix of pXl, . . . , Xnq:

ν˚pXl...n, mq

:“

pXi . . . Xi`m´1qT pXi . . . Xi`m´1q

(1.14)

n´m`1ÿ

1
n ´ m ´ l ` 2
´µ˚pXl...n, mqµ˚pXl...n, mqT ,

i“l

11

where M T denotes the transpose of the matrix M .

Recall that the notion of wide-sense ergodicity is given in Deﬁnition 1.1. The
ergodicity theorem concerns what information can be derived from an average
over time about the ensemble average at each point of time. For the wide-sense
stationary ergodic process X, being either continuous-time or discrete-time, the
following statement holds: every empirical mean µ˚pXl...n, mq is a strongly con-
sistent estimator of the path mean EpXl...l`m´1q; and every empirical covariance
matrix ν˚pXl...n, mq is a strongly consistent estimator of the covariance matrix
CovpXl...l`m´1q under the Frobenius norm, i.e., for all m ě 1, we have

´

P

lim
nÑ8

¯
|µ˚pXl...n, mq ´ EpXl...l`m´1q| “ 0

“ 1

´

P

and

¯
}ν˚pXl...n, mq ´ CovpXl...l`m´1q}F “ 0
Next we introduce the empirical covariance-based dissimilarity measure xd˚, serv-
ing as a consistent estimator of the covariance-based dissimilarity measure d˚.

lim
nÑ8

“ 1.

Deﬁnition 1.6 (Empirical covariance-based dissimilarity measure). Given two
processes’ sample paths xj “ pX pjq
1 , . . . , X pjq
nj q, j “ 1, 2. Let n “ mintn1, n2u,
we deﬁne the empirical covariance-based dissimilarity measure between x1 and
x2 by

xd˚px1, x2q :“
´´

mnÿ

n´m`1ÿ

m“1

l“1

wmwl

ˆM

µ˚pX p1q

l...n, mq, ν˚pX p1q

l...n, mq

l...n, mq, ν˚pX p2q

l...n, mq

¯

´
µ˚pX p2q

,

¯¯
.(1.15)

The empirical covariance-based dissimilarity measure between a sample path xi
and a process X pjq (i, j P t1, 2u) is deﬁned by
mnÿ

n´m`1ÿ

xd˚pxi, X pjqq :“
´´

m“1

l“1

ˆM

µ˚pX piq

l...n, mq, ν˚pX piq

wmwl

´

´

¯
l...n, mq

,

E

X pjq

l...l`m´1

, Cov

¯

´
X pjq

l...l`m´1

¯¯¯

.

(1.16)

Unlike the dissimilarity measure d˚ which describes some distance between
stochastic processes, the empirical covriance-based dissimilarity measure is some
distance between two sample paths (ﬁnite-length vectors). We will show in the
forthcoming Lemma 1.8 that xd˚ is a consistent estimator of d˚.

Two observed sample paths possibly have distinct lengths n1, n2, therefore
in (1.15) we consider computing the distances between their subsequences of
length n “ mintn1, n2u.
In practice we usually take mn “ tlog nu, the ﬂoor
number of log n.

It is easy to verify that both d˚ and xd˚ satisfy the triangle inequalities,
thanks to the fact that both the Euclidean distance and ρ˚ satisfy the triangle
inequalities. More precisely, the following holds.

12

Remark 1.7. Thanks to (1.7) and the deﬁnitions of d˚ (see (1.5)) and xd˚
(see (1.15)), we see that the triangle inequality holds for the covariance-based
dissimilarity measure d˚, as well as for its empirical estimate xd˚. Therefore
for arbitrary processes X piq, i “ 1, 2, 3 and arbitrary ﬁnite-length sample paths
xi, i “ 1, 2, 3, we have

`

˘

`

X p1q, X p2q
d˚
xd˚px1, x2q ď
˘
xd˚
x1, X p1q

X p1q, X p3q

ď d˚
xd˚px1, x3q `
`
x1, X p2q
ď

` d˚
xd˚px2, x3q,
˘
` d˚

xd˚

`

`

˘

X p1q, X p2q

.

˘

`

˘

X p2q, X p3q

,

Remark 1.7 together with the fact that the processes are weakly ergodic,
leads to Lemma 1.8 below, which is the key to demonstrate that our clustering
algorithms in the forthcoming section are asymptotically consistent.

Lemma 1.8. Given two paths

´

¯

x1 “

X p1q

1 , . . . , X p1q
n1

and x2 “

1 , . . . , X p2q
n2

,

´
X p2q

¯

sampled from the wide-sense stationary ergodic processes X p1q and X p2q respec-
tively, we have

ˆ

P

lim
n1,n2Ñ8

´

¯˙

xd˚ px1, x2q “ d˚

X p1q, X p2q

“ 1

(1.17)

and

ˆ

P

¯

´
xi, X pjq

xd˚

lim
niÑ8

´

¯˙

“ d˚

X p1q, X p2q

“ 1, for i, j P t1, 2u, i ‰ j. (1.18)

Proof. We take n “ mintn1, n2u. To show (1.17) holds it suﬃces to prove that
for arbitrary ε ą 0, there is an integer N ą 0 such that for any n ě N , with
probability 1,

ˇ
ˇ
ˇ
ˇ
ˇxd˚ px1, x2q ´ d˚pX p1q, X p2qq
ˇ ă ε.

Deﬁne the sets of indexes
(
(cid:32)
pm, lq P N2 : m ď mn, l ď n ´ m ` 1

S1pnq “

and S2pnq “ N2zS1pnq.

To be more convenient we also denote by

´
X pjq

V

¯

´

´
X pjq

E

:“

l...l`m´1

l...l`m´1

¯

´

¯¯

, Cov

X pjq

l...l`m´1

(1.19)

and

´

pV

¯
l...n, m

´

´

¯
l...n, m

X pjq

X pjq
(1.20)
for pm, lq P N2 and j “ 1, 2. By using the deﬁnitions of d˚ (see (1.5)), of xd˚ (see
(1.15)) and the triangle inequality

l...n, m

X pjq

, ν˚

µ˚

:“

,

´

¯¯

ˇ
ˇ
ˇ
ˇ
ˇ ď

ai

ˇ
ˇ
ˇ
ˇ
ˇ

ÿ

iPI

ÿ

iPI

|ai|, for any indexes set I and any real numbers ai’s,

13

we obtainˇ
ˇ
ˇxd˚px1, x2q ´ d˚
ÿ

`

ˇ
ˇ
ˇ
ˇ
pm,lqPS1pnq

“

X p1q, X p2q

˘ˇ
ˇ
ˇ

´

´
pV pX p1q

wmwl

M

l...n, mq, pV pX p2q

l...n, mq

ÿ

´

´

wmwlM

V pX p1q

l...l`m´1q, V pX p2q

l...l`m´1q

¯ ˇ
ˇ
ˇ
ˇ

¯

¯

ď

´

ÿ

M

wmwl

pm,lqPS1pnqYS2pnq
´
pV pX p1q

ˇ
ˇ
ˇ
ˇ
pm,lqPS1pnq
´
l...l`m´1q, V pX p2q
V pX p1q
´
ÿ
V pX p1q

l...n, mq, pV pX p2q
l...n, mq
¯¯ ˇ
ˇ
ˇ
ˇ

l...l`m´1q

wmwlM

´M

`

ÿ

ď

pm,lqPS2pnq

ˇ
ˇ
ˇ
ˇM

´
pV pX p1q

wmwl

pm,lqPS1pnq
´

´M

l...n, mq, pV pX p2q
l...n, mq
¯ ˇ
ˇ
ˇ
ˇ

V pX p1q
l...l`m´1q, V pX p2q
´
ÿ
V pX p1q

l...l`m´1q

wmwlM

`

pm,lqPS2pnq

¯
l...l`m´1q

l...l`m´1q, V pX p2q
¯

l...l`m´1q, V pX p2q

¯
.(1.21)
l...l`m´1q

Next note that the metric M satisﬁes the following triangle inequality:

ˇ
ˇ
ˇ
ˇM

´
pV pX p1q
´

l...n, mq, pV pX p2q

ď M

pV pX p1q

l...n, mq, V pX p1q

´

¯
l...n, mq

´ M
¯
l...l`m´1q

` M

V pX p1q
l...l`m´1q, V pX p2q
´
pV pX p2q

l...n, mq, V pX p2q

l...l`m´1q

¯ ˇ
ˇ
ˇ
ˇ
¯
(1.22)
l...l`m´1q

.

It follows from (1.21) and (1.22) that
˘ˇ
ˇ
ˇ

ˇ
ˇ
ˇxd˚px1, x2q ´ d˚
ÿ

X p1q, X p2q
ˆ

`

ď

wmwl

M

´
pV pX p1q

pm,lqPS1pnq
´

`M

¯

l...n, mq, V pX p1q
¯ ˙

l...l`m´1q

pV pX p2q
l...n, mq, V pX p2q
ÿ

l...l`m´1q
´
V pX p1q

wmwlM

`

pm,lqPS2pnq

l...l`m´1q, V pX p2q

¯
.(1.23)
l...l`m´1q

Next we show that the right-hand side of (1.23) converges to 0 as n Ñ 8. First
observe that the weights twmumě1 have been chosen such that

wmwlM

´
V pX p1q

l...l`m´1q, V pX p2q

¯
l...l`m´1q

ă `8.

(1.24)

8ÿ

m,l“1

14

Then for arbitrary ﬁxed ε ą 0, we can ﬁnd an index J such that for n ě J,

ÿ

´
V pX p1q

wmwlM

l...l`m´1q, V pX p2q

¯
l...l`m´1q

ď

ε
3

.

pm,lqPS2pnq

(1.25)

Next, the weak ergodicity of the processes X p1q and X p2q implies that:
each pm, lq P N2, pV pX pjq
V pX pjq

for
l...n, mq (j “ 1, 2) is a strongly consistent estimator of

l...l`m´1q, under the metric M, i.e., with probability 1,
´
pV pX pjq

l...n, mq, V pX pjq

l...l`m´1q

M

¯

“ 0.

(1.26)

lim
nÑ8

Thanks to (1.26), for any pm, lq P S1pJq, there exists some Nm,l (which depends
on m, l) such that for all n ě Nm,l, we have, with probability 1,

´
pV pX pjq

M

l...n, mq, V pX pjq

l...l`m´1q

ď

¯

ε
3wmwl#S1pJq

, for j “ 1, 2,

(1.27)

where #A denotes the number of elements included in the set A. Denote by
Nm,l. Then observe that, for n ě maxtNJ , Ju,
NJ “ max

pm,lqPS1pJq

ÿ

wmwlM

l...l`m´1q, V pX p2q

l...l`m´1q

´

V pX p1q
´

¯

¯

wmwlM

V pX p1q

l...l`m´1q, V pX p2q

l...l`m´1q

.

(1.28)

pm,lqPS2pnq

ÿ

ď

pm,lqPS2pJq

It results from (1.23), (1.28), (1.27) and (1.25) that, for n ě maxtNJ , Ju,
˘ˇ
ˇ
ˇ

`

ˇ
ˇ
ˇxd˚px1, x2q ´ d˚
ÿ

X p1q, X p2q
´

¯
l...l`m´1q

pV pX p1q
l...n, mq, V pX p1q
´
pV pX p2q
l...n, mq, V pX p2q
´

¯

l...l`m´1q

wmwlM

V pX p1q

l...l`m´1q, V pX p2q

l...l`m´1q

¯

ď

wmwlM

pm,lqPS1pnq

ÿ

`

wmwlM

pm,lqPS1pnq

ÿ

`

pm,lqPS2pJq

ď

`

`

“ ε,

ε
3

ε
3

ε
3

which proves (1.17). The statement (1.18) can be proved analogously.

2 Asymptotically Consistent Clustering Algorithms

2.1 Oﬄine and Online Algorithms

In this section we introduce the asymptotically consistent algorithms for cluster-
ing oﬄine and online datasets respectively. We explain how the two algorithms

15

work, and prove that both algorithms are asymptotically consistent. It is worth
noting that the asymptotic consistency of our algorithms relies on the assump-
tion that the number of clusters κ is priorly known. The case for κ being
unknown has been studied in Khaleghi et al (2016) in the problem of cluster-
ing strictly stationary ergodic processes. However in the setting of wide-sense
stationary ergodic processes, this problem remains open.

Algorithm 1 below presents the pseudo-code for clustering oﬄine datasets.
It is a centroid-based clustering approach. One of its main features is that the
farthest 2-point initialization applies. The algorithm selects the ﬁrst two cluster
centers by picking the two “farthest” observations among all observations (Lines
1 - 3), under the empirical dissimilarity measure xd˚. Then each next cluster cen-
ter is chosen to be the observation farthest to all the previously assigned cluster
centers (Lines 4 - 6). Finally the algorithm assigns each remaining observation
to its nearest cluster (Lines 7-11).

Algorithm 1: Oﬄine clustering, with known κ

Input: sample paths S “ tx1, . . . , xN u; number κ of clusters;

1 pc1, c2q ÐÝ

weights wj, j “ 1, . . . , N ptq.
xd˚pxi, xjq;

argmax
pi,jqPt1,...,N u2,iăj

2 C1 ÐÝ tc1u;
3 C2 ÐÝ tc2u;
4 for k “ 3, . . . , κ do

5

ck ÐÝ argmax
i“1,...,N

min
j“1,...,k´1

xd˚pxi, xcj q;

6 end
7 Assign each remaining point to its nearest cluster center :
8 for i “ 1, . . . , N do
!
xd˚pxi, xjq : j P Ck

)
;

9

k ÐÝ argmin
kPt1,...,κu

Ck ÐÝ Ck Y tiu;

10
11 end

Output: The κ clusters tC1, C2, . . . , Cκu.
We point out that Algorithm 1 is diﬀerent from Algorithm 1 in Khaleghi

et al (2016) at two points:

1. As mentioned previously, our algorithm relies on the covariance-based dis-

similarity xd˚, in lieu of the process distributional distances.

2. Our algorithm suggests 2-point initialization, while Algorithm 1 in Khaleghi
et al (2016) randomly picks 1-point as the ﬁrst cluster center. The latter
initialization was proposed for use with k-means clustering by Katsavouni-
dis et al (1994). Algorithm 1 in Khaleghi et al (2016) requires κN distance
calculations, while our algorithm requires N pN ´ 1q{2 distances calcula-
tions. It is very important to point out that, to reduce the computational
complexity cost of our algorithm, it is ﬁne to replace our 2-point initializa-

16

tion with the one in Khaleghi et al (2016). However there are two reasons
based on which we recommend using our approach of initialization:

Reason 1 In the forthcoming Section 4.1, our empirical comparison to
Khaleghi et al (2016) shows that the 2-point initialization turns out
to be more accurate in clustering than the 1-point initialization.

ř

Reason 2 Concerning the complexity cost, we have the following loss
and earn: on one hand, the 2-point initialization requires more steps
of calculations than the 1-point initialization; on the other hand,
in our covariance-based dissimilarity measure xd˚ deﬁned in (1.15),
the matrices distance ρ˚ requires m2
n computations of Euclidean dis-
tances, while the distance
BPBm,l |νpx1, Bq´νpx2, Bq| given in (1.3)
requires at least n1 ` n2 ´ 2mn ` 2 computations of Euclidean dis-
tances (see Eq. (33) in Khaleghi et al (2016)). Note that we take
mn “ tlog nu (t¨u denotes the ﬂoor integer number) though this frame-
work. Therefore the computational complexity of the covariance-
based dissimilarity xd˚ makes the overall complexity of Algorithm 1
quite competitive to the algorithm in Khaleghi et al (2016), especially
when the paths lengths ni, i “ 1, . . . , n are relatively large, or when
the database of all distance values are at hand.

Next we present the clustering algorithm for online setting. As mentioned in
Khaleghi et al (2016), one regards recently-observed paths as unreliable ob-
servations, for which suﬃcient information has not yet been collected, and for
which the estimators of the covariance-based dissimilarity measures are not ac-
curate enough. Consequently, farthest-point initialization would not work in
this case; and clustering on all available data results in not only mis-clustering
unreliable paths, but also in clustering incorrectly those for which suﬃcient data
are already available. The strategy is presented in Algorithm 2 below: cluster-
ing based on a weighted combination of several clusterings, each obtained by
running the oﬄine algorithm (Algorithm 1) on diﬀerent portions of data.

More precisely, Algorithm 2 works as follows. Suppose the number of clusters
κ is known. At time t, a sample Sptq is observed (Lines 1 - 2), the algorithm
iterates over j “ κ, . . . , N ptq where at each iteration Algorithm 1 is utilized to
cluster the ﬁrst j paths in Sptq into κ clusters (Lines 6 - 7). For each cluster
its center is selected as the observation having the smallest index among that
cluster, and their indexes are ordered increasingly (Line 8). The minimum
inter-cluster distance γj (see Cesa-Bianchi and Lugosi (2006)) is calculated as
the minimum distance xd˚ between the κ cluster centers obtained at iteration j
(Line 9). Finally, every observation in Sptq is assigned to the nearest cluster,
based on the weighted combination of the distances between this observation

17

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

and the candidate cluster centers obtained at each iteration on j (Lines 14 - 17).

; number of clusters

Algorithm 2: Online clustering, with known κ

!

Input: sample paths

Sptq “ txt

1, . . . , xt
κ; weights βpjq, j “ 1, . . . , N ptq.
!

1 for t “ 1, . . . , 8 do

)

t

N ptqu

)
;

1, . . . , xt
xt

Obtain new sequences: Sptq ÐÝ

N ptq
Initialize the normalization factor : η ÐÝ 0;
Initialize the ﬁnal clusters: Ckptq ÐÝ H, k “ 1, . . . , κ;
Generate N ptq ´ κ ` 1 candidate cluster centers:
for j “ κ, . . . , N ptq do
`(cid:32)
(cid:32)
C j
1, . . . , xt
xt
1, . . . , C j
;
(cid:32)
(
κ
j
i P C j
pcj
1, . . . , cj
, k “ 1, . . . , κq;
`
˘
k
xt
, xt
γj ÐÝ
cj
cj
k1
k

ÐÝ Alg1
κq ÐÝ sortpmin
xd˚

min
k,k1Pt1,...,κu,k‰k1

˘
, κ

(

(

;

wj ÐÝ βpjq;
η ÐÝ η ` wjγj;

end
Assign each point to a cluster :
for i “ 1, . . . , N ptq do

k ÐÝ argmin
j“κ
k1Pt1,...,κu
Ckptq ÐÝ Ckptq Y tiu;

1
η

N ptqř

`

xd˚

wjγj

˘

;

i, xt
xt
cj
k1

end

17
18 end

Output: The κ clusters tC1ptq, . . . , Cκptqu, t “ 1, 2, . . . , 8.

In Algorithm 2, βpjq denotes a function indexed by j, which is the value
chosen for the weight wj. Remark that for online setting, our algorithm requires
the same number of distance calculations as in Algorithm 2 in Khaleghi et al
(2016). They are both bounded by OpN ptq2q. Using 2-point initialization, our
Algorithm 2 then takes advantage in the overall computational complexity cost.
Finally we note that both Algorithm 1 and Algorithm 2 require κ ě 2. When
κ is known, this restriction is not a practical issue.

2.2 Consistency and Computational Complexity of the

Algorithms

In this section we prove the asymptotic consistency of Algorithms 1 and 2. They
are stated in the 2 theorems below.

Theorem 2.1. Algorithm 1 is strongly asymptotically consistent (in the oﬄine
sense), provided that the true number κ of clusters is known, and each sequence
xi, i “ 1, . . . , N is sampled from some wide-sense stationary ergodic process.

Proof. Similar to the idea used in the proof of Theorem 11 in Khaleghi et al

18

(2016), to prove the consistency statement we will need Lemma 1.8 to show that
if the sample paths in S are long enough, the sample paths that are generated
by the same process covariance structure are “closer” to each other than to the
rest. Therefore, the sample paths chosen as cluster centers are each generated
by a diﬀerent covariance structure, and since the algorithm assigns the rest to
the closest clusters, the statement follows. More formally, let nmin denote the
shortest path length in S:

nmin :“ min tni : i “ 1, . . . , N u .

Denote by δmin the minimum non-zero covariance-based dissimilarity measure
between any 2 covariance structures:

!

´

¯

)

δmin :“ min

d˚

X pkq, X pk1q

: k, k1 P t1, . . . , κu, k ‰ k1

.

(2.1)

Fix ε P p0, δmin{4q. Since there are a ﬁnite number N of observations, by Lemma
1.8 there is n0 such that for nmin ě n0 we have

´

xd˚

¯

xi, X plq

ď ε,

max
lPt1,...,κu
iPGlXt1,...,N u

(2.2)

where Gl, l “ 1, . . . , κ denote the covariance structure ground-truth partitions
given by Deﬁnition 1.2.

On one hand, by using (2.2), the triangle inequality (see Remark 1.7) and

the fact that

max
iPI

pai ` biq ď max
iPI

ai ` max
iPI

bi

for any indexes set I and any real numbers ai’s and bi’s, we obtain

max
lPt1,...,κu
i,jPGlXt1,...,N u

xd˚ pxi, xjq

ď

“

max
lPt1,...,κu
i,jPGlXt1,...,N u
xd˚

max
lPt1,...,κu
iPGlXt1,...,N u

ď 2ε ă

δmin
2

.

¯

´
xi, X plq

xd˚

´

¯

xi, X plq

`

`

max
lPt1,...,κu
i,jPGlXt1,...,N u
xd˚

max
lPt1,...,κu
jPGlXt1,...,N u

¯

´
xj, X plq

xd˚

¯

´
xj, X plq

(2.3)

On the other hand, by using the triangle inequality (see Remark 1.7), (2.1) and

19

(2.2), we have for nmin ě n0,

xd˚pxi, xjq

min
k,k1Pt1,...,κu,k‰k1
iPGkXt1,...,N u
jPGk1 Xt1,...,N u

ě

min
k,k1Pt1,...,κu,k‰k1
iPGkXt1,...,N u
jPGk1 Xt1,...,N u

ě δmin ´ 2ε ą

δmin
2

.

!

´
X pkq, X pk1q

¯

d˚

¯

´
xi, X pkq

´
xj, X pk1q

¯)

xd˚

´

xd˚

´

(2.4)

(2.5)

In words, (2.3) together with (2.4) indicates that the sample paths in S that
are generated by the same covariance structure are closer to each other than to
the rest of sample paths. Then by (2.3) and (2.4), for nmin ě n0, we necessarily
have each sample path should be “close” enough to its cluster center, i.e.,

where the κ cluster centers’ indexes c1, . . . , cκ are given by Algorithm 1 as

max
i“1,...,N

min
k“1,...,κ´1

xd˚pxi, xck q ą

δmin
2

,

pc1, c2q :“ argmax

i,j“1,...,N, iăj

xd˚pxi, xjq,

and

ck :“ argmax
i“1,...,N

min
j“1,...,k´1

xd˚pxi, xcj q, k “ 3, . . . , κ.

Hence, the indexes c1, . . . , cκ will be chosen to index the sample paths gener-
ated by diﬀerent process covariance structures. Then by (2.3) and (2.4), each
remaining sample path will be assigned to the cluster center corresponding to
the sample path generated by the same process covariance structure. Finally
Theorem 2.1 results from (2.3), (2.4) and (2.5).

Theorem 2.2. Algorithm 2 is strongly asymptotically consistent (in the online
sense), provided the true number of clusters κ is known, and each sequence
xi, i P N is sampled from some wide-sense stationary ergodic process.

Proof. The idea of the proof is similar to that of Theorem 12 in Khaleghi et al
(2016). The main diﬀerences between the 2 proofs are made by the fact that
our covariance-based dissimilarity measure xd˚ is not bounded by some constant.
Although it is not mentioned in the pseudo-code Algorithm 2, the notations γj’s
and η are dependent of t, therefore we denote γt
j :“ γj and ηt :“ η through this
proof. In the ﬁrst step, by using the triangle inequality we can show that for

20

any t ą 0, any N P N,

sup
jPt1,...,N u
kPt1,...,κu

jPt1,...,N u
kPt1,...,κu

jPt1,...,N u
kPt1,...,κu

´

´

´

xd˚

¯

j, X pkq
xt

ď sup

´

´
X pkq, X pk1
j q

¯

d˚

´

¯¯

xd˚

`

j, X pk1
xt
j q

ď sup

d˚

X pkq, X pk1
j q

` sup

j, X pk1
xt
j q

jPt1,...,N u
kPt1,...,κu
¯

jPt1,...,N u
kPt1,...,κu

¯

` sup

jPt1,...,N u

´

´

xd˚

xd˚

¯

¯

“ sup

d˚

X pkq, X pk1
j q

j, X pk1
xt
j q

,

(2.6)

j is chosen such that xt

j is sampled from the process covariance

where for each j, k1
structure X pk1

j q. On one hand, let
!

´
X pkq, X pk1q

¯

d˚

δmax :“ max

: k, k1 P t1, . . . , κu, k ‰ k1

,

(2.7)

)

then the ﬁrst term on the right-hand side of (2.6) can be bounded by the constant
δmax, which neither depends on t nor on N :

´

¯

d˚

X pkq, X pk1
j q

ď δmax.

(2.8)

sup
jPt1,...,N u
kPt1,...,κu

On the other hand, since xt
(see Lemma 1.8), for j “ 1, . . . , N , with probability 1,

j is sampled from X pk1

j q, by using the weak ergodicity

´

¯

j, X pk1
xt
j q

“ 0.

xd˚

lim
tÑ8

This together with the fact that a convergent sequence is also bounded, leads
to, for each j P t1, . . . , N u, there is bj (not depending on t) such that

´

xd˚

j, X pk1
xt
j q

¯

ď bj, for all t ě 0.

Therefore the second term on the right-hand side of (2.6) can be bounded as:

´

xd˚

j, X pk1
xt
j q

¯

sup
jPt1,...,N u

ď maxtb1, . . . , bN u.

(2.9)

Let

BpN q :“ δmax ` maxtb1, . . . , bN u.

(2.10)

It is important to point out that BpN q depends only on N but not on t. It
follows from (2.6), (2.8), (2.9) and (2.10) that
¯

´

xd˚

j, X pkq
xt

ď BpN q.

(2.11)

sup
jPt1,...,N u
kPt1,...,κu

21

Let δmin be the one given in (2.1). Fix ε P p0, δmin{4q. By using (1.8), we can
choose some J ą 0 so that

8ÿ

j“J`1

wj ď ε.

Recall that in online setting, the ith sample path’s length niptq grows with time,
for each i. Therefore, by the wide-sense ergodicity (see Lemma 1.8), for every
j P t1, . . . , Ju there exists some T1pjq ą 0 such that for all t ě T1pjq we have

(2.12)

(2.13)

´

xd˚

¯

i, X pkq
xt

ď ε.

max
kPt1,...,κu
iPGkXt1,...,ju

For k “ 1, . . . , κ, deﬁne skpN ptqq to be the index of the ﬁrst path in Sptq sampled
from the covariance structure X pkq, i.e.,

skpN ptqq :“ min ti P Gk X t1, . . . , N ptquu .

(2.14)

Note that skpN ptqq depends only on N ptq. Then denote

mpN ptqq :“ max

skpN ptqq.

kPt1,...,κu

(2.15)

By Theorem 2.1 for every j P tmpN ptqq, . . . , Ju there exists some T2pjq such
that Alg1pSptq|j, κq is asymptotically consistent for all t ě T2pjq, where Sptq|j “
(cid:32)
xt
1, . . . , xt
denotes the subset of Sptq consisting of the ﬁrst j sample paths.
j
Let

(

T :“ max
i“1,2
jPt1,...,Ju

Tipjq.

Recall that, by the deﬁnition of mpN ptqq in (2.15), Sptq|mpN ptqq contains sample
paths from all κ distinct covariance structures. Therefore, similar to obtaining
(2.4), for all t ě T , we use the triangle inequality, (2.1) and (2.13) to obtain
˙

ˆ

xd˚

xt

, xt

cmpN ptqq
k

cmpN ptqq
k1

ˆ

´
X pkq, X pk1q

¯

d˚

´

ˆ

´

xd˚

xt

¯

ˆ

, X pkq

xd˚

`

xt

cmpN ptqq
k

, X pk1q

cmpN ptqq
k1

˙˙˙

(2.16)

min
k,k1Pt1,...,κu
k‰k1

ě

min
k,k1Pt1,...,κu
k‰k1

ě δmin ´ 2ε ě

δmin
2

.

N ptqÿ

j“1

From Algorithm 2 (see Lines 9, 11) we see

ηt :“

wjγt

j, with γt

j :“

´

xd˚

¯

.

xt
cj
k

, xt
cj
k1

min
k,k1Pt1,...,κu
k‰k1

22

(2.17)

(2.18)

(2.19)

(2.20)

Hence, by (2.16), for all t ě T ,

ηt ě

wmpN ptqqδmin
2

.

For j P tJ ` 1, . . . , N ptqu, by the triangle inequality and (2.11), we have for all
t ě T ,

¯

´

xd˚

xt
cj
k

, xt
cj
k1

γt
j “

min
k,k1Pt1,...,κu
k‰k1
´

ď

min
k,k1Pt1,...,κu
k‰k1

ď δmax ` 2BpN ptqq.

´
X pkq, X pk1q

¯

d˚

´

´
xd˚

`

¯

´

xd˚

`

, X pkq

xt
cj
k

, X pk1q

xt
cj
k1

¯¯¯

Denote by

then (2.18) can be interpreted as: for all t ě T ,

M pN ptqq :“ δmax ` 2BpN ptqq,

sup
jPtJ`1,...,N ptqu

γt
j ď M pN ptqq.

By (2.11), (2.17) and (2.19), for every k P t1, . . . , κu we obtain

1
ηt

N ptqÿ

j“1

´

¯

wjγt
j

xd˚

, X pkq

xt
cj
k

wjγt
j

xd˚

, X pkq

`

xt
cj
k

1
ηt

N ptqÿ

j“J`1

´

¯

wjγt
j

xd˚

, X pkq

xt
cj
k

¯

¯

wjγt
j

xd˚

, X pkq

`

xt
cj
k

2BpN ptqqM pN ptqq
wmpN ptqqδmin

N ptqÿ

j“J`1

wj

´

´

´

Jÿ

j“1
Jÿ

“

ď

“

1
ηt

1
ηt

1
ηt

j“1
mpN ptqq´1ÿ

wjγt
j

xd˚

´

¯

, X pkq

`

xt
cj
k

1
ηt

Jÿ

¯

wjγt
j

xd˚

, X pkq

xt
cj
k

j“mpN ptqq

j“1
2BpN ptqqM pN ptqqε
wmpN ptqqδmin

.

`

Next we provide upper bounds of the ﬁrst 2 items in the right-hand side of
(2.20). On one hand, by the deﬁnition of mpN ptq, the sample paths in Sptq|j
for j “ 1, . . . , mpN ptqq ´ 1 are generated by at most κ ´ 1 out of the κ process
covariance structures. Therefore for each j P t1, . . . , mpN ptqq´1u there exists at
least one pair of distinct cluster centers that are generated by the same process
covariance structure. Consequently, by (2.13) and the deﬁnition of ηt, for all
t ě T and k P t1, . . . , κu,

mpN ptqq´1ÿ

1
ηt

j“1

´

wjγt
j

xd˚

, X pkq

ď

xt
cj
k

ε
ηt

¯

mpN ptqq´1ÿ

j“1

wjγt

j ď ε.

(2.21)

23

On the other hand, since the clusters are ordered in the order of appearance of
the distinct covariance structures, we have xt
slpN ptqq for all j “ m, . . . , J
cj
l
and l “ 1, . . . , κ, where the index slpN ptqq is deﬁned in (2.14). Therefore, by
(2.13) and the deﬁnition of ηt, for all t ě T and every l “ 1, . . . , κ we have

“ xt

1
ηt

Jÿ

´

wjγt
j

xd˚

, X plq

xt
cj
l

¯

´

xd˚

“

j“mpN ptqq

slpN ptqq, X plq
xt

Jÿ

¯

1
ηt

j“mpN ptqq

wjγt

j ď ε.

Combining (2.20), (2.21), (2.22) and (2.13) we obtain, for t ě T ,
˙

ˆ

1
ηt

N ptqÿ

j“1

´

¯

wjγt
j

xd˚

, X pkq

xt
cj
k

ď ε

2 `

2BpN ptqqM pN ptqq
wmpN ptqqδmin

(2.22)

(2.23)

for all l “ 1, . . . , κ.

Now we explain how to use (2.23) to prove the asymptotic consistency of
Algorithm 2. Consider an index i P Gk1 for some k1 P t1, . . . , κu. Then on one
hand, using (2.21) and (2.22), we get for k P t1, . . . , κu, k ‰ k1,

1
ηt

N ptqÿ

j“1

wjγt
j

xd˚

´

¯

i, xt
xt
cj
k

ě

ě

1
ηt

1
ηt

N ptqÿ

j“1
N ptqÿ

j“1

´

¯

wjγt
j

xd˚

i, X pkq
xt

´

´

¯

wjγt
j

xd˚

, X pkq

xt
cj
k

´

´

´

¯¯

wjγt
j

d˚

X pkq, X pk1q

xd˚

´

i, X pk1q
xt

N ptqÿ

j“1

1
ηt

¯

N ptqÿ

´

¯

wjγt
j

xd˚

, X pkq

xt
cj
k

´

1
ηt

ˆ

ě δmin ´ 2ε

j“1
2BpN ptqqM pN ptqq
wmpN ptqqδmin
On the other hand, for any N P N, by using the wide-sense ergodicity, there is
T pN q such that for all t ě T pN q,

(2.24)

2 `

˙

.

Since ε can be arbitrarily chosen, it follows from (2.24) and (2.25) that

´

xd˚

¯

i, X pkq
xt

ď ε.

max
kPt1,...,κu
iPGkXt1,...,N u

argmin
kPt1,...,κu

1
ηt

N ptqÿ

j“1

wjγj

xd˚

´

¯

i, xt
xt
cj
k

“ k1

(2.25)

(2.26)

holds almost surely for all i “ 1, . . . , N and all t ě maxtT, T pN qu. Theorem 2.2
is proved.

24

The next part involves discussion of the complexity costs of the above two

algorithms.

1. For oﬄine setting, our Algorithm 1 requires N pN ´ 1q{2 calculations of
xd˚, against κN calculations of pd in the oﬄine algorithm in Khaleghi et al
(2016). In each xd˚, the matrices distance ρ˚ consists of m2
n calculations
of Euclidean distances. Then iterating over m, l in xd˚ we see that at most
Opnm3
nq computations of Euclidean distances, against Opnmn{| log s|q com-
putations of ˆd for the oﬄine algorithm in Khaleghi et al (2016), where

s “

min
i ‰X p2q
iPt1,...,n1u;jPt1,...,n2u

X p1q

j

ˇ
ˇ
ˇX p1q

i ´ X p2q

j

ˇ
ˇ
ˇ .

It is known that eﬃcient searching algorithm can be utilized to determine
s, with at most Opn logpnqq (n “ mintn1, n2u) computations. Therefore
our Algorithm 1 is computationally competitive to the one in Khaleghi
et al (2016).

2. For online setting, we can hold a similar discussion as in Khaleghi et al
(2016), Section 5.1. There it shows the computational complexity of
updates of xd˚ for both our Algorithm 2 and the online algorithm in
Khaleghi et al (2016) is at most OpN ptq2 ` N ptq log3 nptqq (here we take
mnptq “ tlog nptqu). Therefore the overall diﬀerence of computational com-
plexities between the 2 algorithms are reﬂected by the complexity of com-
puting xd˚ and pd (see Point 1).

2.3 Eﬃcient Dissimilarity Measure

Kleinberg (2003) presented a set of three simple properties that a good clustering
function should have: scale-invariance, richness and consistency. Further, he
demonstrated that there is no clustering function that satisﬁes these properties
at the meanwhile. He pointed out, as one particular example, that the centroid-
based clustering basically does not satisfy the above consistency property (note
that this is a diﬀerent concept from our asymptotic consistency). In this section
we show that, although the consistency property is not satisﬁed, there exists
some other criterion of eﬃciency of dissimilarity measure in a particular setting.
It is the so-called eﬃcient dissimilarity measure.

Deﬁnition 2.3 (Eﬃcient dissimilarity measure). Assume that the samples S “
txpξq : ξ P Hu (H Ă Rq for some q P N), meaning that all the paths xpξq are
indexed by a set of real-valued parameters ξ. Then a clustering function is called
eﬃcient if its dissimilarity measure d satisﬁes that, there exists c ą 0 so that
for any xpξ1q, xpξ2q P S,

dpxpξ1q, xpξ2qq “ c}ξ1 ´ ξ2},

where } ¨ } denotes some norm deﬁned over Rq.

25

Mathematically, eﬃcient dissimilarity measure is a metric induced by some
norm. Clustering processes based on eﬃcient dissimilarity measure will then be
equivalent to clustering under classical distances in Rq, such as Euclidean dis-
tance, Manhattan distance, or Minkowski distance. The latter setting has well-
known advantages in cluster analysis. For example, Euclidean distance performs
well when deployed to datasets that include compact or isolated clusters (Jain
and Mao, 1996; Jain et al, 1999); when the shape of clusters is hyper-rectangular
(Xu and Wunsch, 2005), Manhattan distance can be used; Minkowski distance,
including Euclidean and Manhattan distances as its particular cases, can be
utilized to solve clustering obstacles (Wilson and Martinez, 1997). There is a
rich literature on comparing the above three distances to each other through
discussing of their advantages and inconveniences. We refer to Hirkhorshidi et al
(2015) and the references therein.

In the next section we present an excellent example, to show how to improve
the eﬃciency of our consistent algorithms, for clustering self-similar processes
with wide-sense stationary ergodic increments.

3 Self-similar Processes and Logarithmic Trans-

formation

In this section we introduce a non-linear transformation of the covariance matri-
ces in xd˚, in order to improve the eﬃciency of clustering. This transformation is
based on logarithmic function. We use one example to explain how this transfor-
mation works. We show this transformation maps xd˚ to some covariance-based
dissimilarity measure similar to an eﬃcient one, when applied to clustering self-
similar processes.

Deﬁnition 3.1 (Self-similar process, see Samorodnitsky and Taqqu (1994)).
A process X pHq “ tX pHq
utPT (e.g., T “ R or Z) is self-similar with index
H P p0, 1q if, for all n P N, all t1, . . . , tn P T , and all c ‰ 0 such that cti P T
(i “ 1, . . . , n),
´
X pHq
t1

ct1 , . . . , |c|´H X pHq

´
|c|´H X pHq

, . . . , X pHq

¯
.

law
“

ctn

¯

tn

t

It can be shown that a self-similar process has necessarily zero mean and its
covariance structure is indexed by its self-similarity index H, in the following
way (Embrechts and Maejima, 2000).

(
tPT be a zero-mean self-similar process with in-
Theorem 3.2. Let
dex H P p0, 1q and with wide-sense stationary ergodic increments. Assume
E|X pHq
1

|2 ă `8, then for any s, t P T ,

(cid:32)
X pHq
t

´

¯

Cov

X pHq
s

, X pHq
t

“

`

|2

E|X pHq
1
2

|s|2H ` |t|2H ´ |s ´ t|2H

.

˘

The corollary below follows.

26

Corollary 3.3. Let tX pHq
and weakly stationary increments. Assume E|X pHq
enough, deﬁne the increment process Z pHq
such that s ´ t ě h, we have

utPT be a zero-mean self-similar process with index H
|2 ă `8. For h ą 0 small
s`h ´ X pHq
, then for s, t P T

1
psq “ X pHq

h

s

t

´
Z pHq
h

Cov

¯

psq, Z pHq

h

ptq

“

`

|2

E|X pHq
1
2

ps ´ t ´ hq2H ` ps ´ t ` hq2H ´ 2ps ´ tq2H
(3.1)

˘

.

Applying three times the mean value theorem to (3.1) leads to

´
Z pHq
h

Cov

¯

´

psq, Z pHq

h

ptq

“ HE|X pHq

|2

1

pvpHq
1

q2H´1 ´ pvpHq

q2H´1

2

h

“ Hp2H ´ 1qE|X pHq

|2pvpHqq2H´2h,

1

¯

(3.2)

for some vpHq
1
see that the item Cov

P ps´t, s´t`hq, vpHq
´

2
psq, Z pHq

h

¯

P ps´t´h, s´tq and vpHq P pvpHq

q. We
is a non-linear function of H. Next
´

, vpHq
1

ptq

¯¯

2

Z pHq
h

we would ﬁnd a function g such that g
is linearly
dependent of H. To this end we introduce the following log˚-transformation:
for x P R, deﬁne

ptq

h

psq, Z pHq

´
Z pHq
h

Cov

log˚pxq :“ sgnpxq log |x| “

$
&

%

logpxq
´ logp´xq
0

if x ą 0;
if x ă 0;
if x “ 0.

Introduction to log˚-transformation is driven by the following 2 motivations:

Motivation 1 The log˚ function transforms the current dissimilarity measure

to the one which “linearly” depends on its variable H.

Motivation 2 The value log˚pxq preserves the sign of x, which leads to the
consequence that larger distance between x, y yields larger distance be-
tween log˚pxq and log˚pyq.

Applying log˚-transformation to the covariances of Z pHq
tain

h

given in (3.2), we ob-

´

log˚

Cov

¯¯

psq, Z pHq

´
Z pHq
h
´
p2H ´ 2q log vpHq ` log h ` logpH|1 ´ 2H|VarpX pHq

ptq

h

¯

qq

.

1

“ sgnp2H ´ 1q

When vpHq and h are small the items log vpHq and log h are signiﬁcantly large
so logpH|1 ´ 2H|VarpX pHq
´
Z pHq
h

¯
´
p2H ´ 2q log vpHq ` log h

qq becomes negligible. Thus we can write

« sgnp2H ´ 1q

psq, Z pHq

log˚

Cov

ptq

¯¯

´

h

1

.

In conclusion,

27

´

´
Z pHq
h

psq, Z pHq

h

ptq

¯¯

• When H1, H2 P p0, 1{2s or H1, H2 P r1{2, 1q, the item log˚

Cov

is “approximately linear” on H P p0, 1{2s or on H P r1{2, 1q.
Using the approximation log vpH1q « log vpH2q for H1, H2 P p0, 1{2s or
H1, H2 P r1{2, 1q, we have

´

´

¯¯

´

log˚

Cov

Z pH1q
h

psq, Z pH1q
h

ptq

´ log˚

Cov

psq, Z pH2q
h

ptq

´
Z pH2q
h

« 2 sgnp2H1 ´ 1qpH1 ´ H2q log vpH1q.

´

´

• When H1 P p0, 1{2s and H2 P p1{2, 1q, log˚

Cov

Z pHq
h

psq, Z pHq

h

ptq

turns out to be relatively large, because we have

¯¯

´

´

´
Z pH1q
h

Cov

log˚

psq, Z pH1q
h
« ´p2H1 ´ 2q log vpH1q ´ p2H2 ´ 2q log vpH2q
)
!

´ log˚

ptq

Cov

ě 2p2 ´ H1 ´ H2q min

log vpH1q, log vpH2q

.

´
Z pH2q
h

psq, Z pH2q
h

ptq

¯¯

¯¯

¯¯

Taking advantage of the above facts we deﬁne the new empirical covariance-
based dissimilarity measure (based on the deﬁnition (1.12)) to be

yd˚˚pz1, z2q :“

mnÿ

n´m`1ÿ

wmwlρ˚

´
ν˚˚pZ pH1q

l...n , mq, ν˚˚pZ pH2q

¯
l...n , mq

,

m“1

l“1

where ν˚˚pZ pH1q
l...n , mq is the empirical covariance matrix of Z pH1q
l...n , mq,
with each of its coeﬃcients transformed by log˚: let M “ tMi,jui“1,...,m; j“1,...,n
be an arbitrary real-valued matrix, deﬁne
(
i“1,...,m; j“1,...,n .
¯
´
ν˚pZ pH1q

(cid:32)
log˚ Mij

Then we have

, ν˚pZ pH1q

ν˚˚pZ pH1q

log˚ M :“

l...n , mq :“ log˚

l...n , mq

h

.

Now given 2 wide-sense stationary ergodic processes X p1q, X p2q, we choose
twjujPN to satisfy

wmwlρ˚

¯
´
log˚pVl,l`m´1pX p1qqq, log˚pVl,l`m´1pX p2qq

ă `8,

(3.3)

8ÿ

m,l“1

where we denote by

Vl,l`m´1pX p1qq :“ Cov

X p1q
l

, . . . , X p1q

l`m´1

.

´

¯

Then deﬁne the log˚-transformation of the covariance-based dissimilarity mea-
sure to be

d˚˚pX p1q, X p2qq :“

wmwlρ˚

¯
´
log˚pVl,l`m´1pX p1qqq, log˚pVl,l`m´1pX p2qq

.

8ÿ

m,l“1

(3.4)

28

Using the fact that log˚ is continuous over Rzt0u and the weak ergodicity of
Z pHq
h

, we have the following version of ergodicity:

yd˚˚pz1, z2q

a.s.
ÝÝÝÑ
nÑ8

d˚˚

Z pH1q
h

, Z pH2q
h

`

˘

.

Unlike xd˚, the dissimilarity measure yd˚˚ is approximately linear with respect to
the self-similarity index H. Indeed, it is easy to see that

"

yd˚˚pz1, z2q „

|H1 ´ H2| ă 1,
2p2 ´ H1 ´ H2q ą 1,

for H1, H2 P p0, 1{2s or H1, H2 P r1{2, 1q;
for H1 P p0, 1{2q and H2 P r1{2, 1q,

(3.5)
where H1, H2 correspond to the self-similarity indexes of X pH1q, X pH2q respec-
In fact, from (3.5) we can say that yd˚˚ satisﬁes Deﬁnition 2.3 in the
tively.
wide sense:
it is approximately linearly dependent of |H1 ´ H2| when H1, H2
are in the same group out of p0, 1{2s and r1{2, 1q; it is approximately larger
than |H1 ´ H2| when H1, H2 are in diﬀerent groups out of p0, 1{2s and r1{2, 1q.
This fact allows our asymptotically consistent algorithms to be more eﬃcient
when clustering self-similar processes with weakly stationary increments, having
diﬀerent values of H. In Section 4.2 we provide an example of clustering us-
ing our consistent algorithms with and without the log˚-transformation, when
the observed paths are from a well-known self-similar process with stationary
increments – fractional Brownian motion.

4 Simulation and Empirical Study

This section is devoted to applying our clustering algorithms to several synthetic
data and real-world data.
It is worth noting that, in our statistical setting,
the auto-covariance functions are supposed to be unavailable, then the prior
choice of the weights wj presents some trade-oﬀ between the convergence of
the dissimilarity measure and practical application. On one hand, low rate of
convergence (e.g. wj “ 1{jpj ` 1q) risks to a divergent dissimilarity measure d˚
(see (1.5)). On the other hand, high rate of convergence (e.g., wj “ 1{j3pj `1q3)
will only make use of some ﬁrst observations in the sample paths. We believe
that the ﬁrst issue is a minor one in practice, because for most of the wide-sense
stationary ergodic processes (especially Gaussian) taking wj “ 1{jpj ` 1q can
lead to convergent d˚. Also, in practice, instead of (1.5) it is ﬁne to regard

´

¯

Nÿ

d˚

X p1q, X p2q

:“

wmwlρ˚

´

¯
Vl,l`m´1pX p1qq, Vl,l`m´1pX p2qq

,

m,l“1

for some N large enough.

Therefore, through this entire section we take wj “ 1{jpj ` 1q and mn “
tlog nu (recall that t¨u denotes the ﬂoor number) in the covariance-based dissim-
ilarity measure xd˚. Next we explain how to prepare oﬄine and online datasets
in this simulation study.

29

Oﬄine dataset simulation: For each scenario, we simulate 5 groups of sam-
ple paths, each consists of 10 paths with length N ptq “ 5t, for the time
steps t “ 1, 2, . . . , 50. Algorithm 1 is performed over 100 such scenarios,
and the misclassiﬁcation rate is calculated.

Online dataset simulation: For each scenario, we simulate 5 groups of sam-
ple paths. Let the total number of sample paths be N ptq “ 30`tpt´1q{10u
at each time step t. That is, there are 6 sample paths in each of the 5
groups when t “ 1. And the number of sample paths in each group
will increase by 1 once the time t increases by 10. For i “ 1, 2, . . ., the
ith sample path in each group has length niptq “ 5rt ´ pi ´ 6q`s, where
x` “ maxpx, 0q.

We then apply the proposed clustering algorithms to both oﬄine and online set-
tings, and determine their corresponding misclassiﬁcation rates. These misclas-
siﬁcation rates are utilized to intuitively illustrate the asymptotic consistency
of our clustering algorithms, or to compare the performances of our clustering
approaches to other ones. Recall that the misclassiﬁcation rate (i.e. mean clus-
tering error rate, see Section 6 in Khaleghi et al (2016)) is obtained by dividing
the number of misclassiﬁed paths by the total number of paths per scenario,
then average all these fractions:

ˆ

p :“ avg

# of misclassiﬁed sample paths
# of total sample paths collected

˙

.

More precisely, let pC1, . . . , Cκq denote the ground truth clusters of the N sample
paths x1, . . . , xN . We deﬁne the ground truth cluster labels by

Lk “ pk, . . . , kq
loooomoooon

, for k “ 1, . . . , κ.

#Ck times

Let pl1, . . . , lN q denote the cluster labels of px1, . . . , xN q output by some clus-
tering approach. Then the misclassiﬁcation rate p of this approach is computed
by

p “

min
σPSκ
pπ1,...,πN q“pLσp1q,...,Lσpκqq

Nř

1

i“1

tπi‰liu

,

N

(4.1)

where Sκ denotes the group of all possible permutations over the set t1, . . . , κu.
For example, in one scenario of 7 sample paths, if the ground truth cluster

labels of px1, . . . , x7q satisfy

pL1, L2, L3q “ pp1, 1q, p2q, p3, 3, 3, 3qq,

while the clustering algorithm output cluster labels corresponding to px1, . . . , x7q
are given by

pl1, . . . , l7q “ p2, 1, 1, 2, 3, 2, 1q ,

30

(4.1), the misclassiﬁcation rate is 4{7. This can be
then according to Eq.
explained as, at least 4 changes of labels are needed to let the output cluster
labels match that of the ground truth ones p1, 1, 3, 2, 2, 2, 2q:

l1 Ð 1; l3 Ð 3; l5 Ð 2; l7 Ð 2.

We provide the implementation of the misclassiﬁcation rate (see Eq. (4.1)) in
MATLAB publicly online as misclassify rate.m 1.

4.1 Clustering Non-Gaussian Discrete-time Stochastic Pro-

cesses

In Khaleghi et al (2016) a simulation study on a non-Gaussian strictly station-
ary ergodic discrete-time stochastic process (see also Shields (1996)) has been
performed. Since this process has ﬁnite covariance structure, it is also wide-
sense stationary ergodic. As a result we can test our clustering algorithms over
the same dataset and compare their performances to the ones in Khaleghi et al
(2016). Recall that this process tXtutPN is generated in the following way. Fix
some irrational-valued parameter α P p0, 1q.

Step 1. Draw a uniform random number r0 P r0, 1s.

Step 2. For each index i “ 1, 2, . . . , N :

Step 2.1. Deﬁne ri “ ri´1 ` α ´ tri´1 ` αu.

Step 2.2. Deﬁne Xi “

#

1
0

when ri ą 0.5,
otherwise.

We simulate 5 groups of sample paths tXtutPN indexed by the irrational values
α1 “ 0.31..., α2 “ 0.33..., α3 “ 0.35..., α4 “ 0.37..., α5 “ 0.39... (αi, i “
1, . . . , 5, each is simulated by a longdouble with a long mantissa, see Khaleghi
et al (2016)), respectively.

4.1.1 Oﬄine Dataset

We demonstrate the asymptotic consistency of Algorithm 1 by conducting oﬄine
clustering on the simulated oﬄine datasets of tXiuiPN.

The valid blue line in Fig. 1 illustrates the asymptotic consistency of Al-
gorithm 1 through the fact that its misclassiﬁcation rate decreases as time t
increases. Compared to the simulation study over the same dataset in Khaleghi
et al (2016), the misclassiﬁcation rate provided by our proposed algorithm con-
verges at a comparable speed (see Figure 2 in Khaleghi et al (2016)), even
though Algorithm 1 aims to cluster “covariance structures” but not “process
distributions”.

1https://github.com/researchcoding/clustering_stochastic_processes/blob/

master/misclassify_rate.m.

31

The dot-dashed red line in Fig. 1 presents the performance of Algorithm 2
and compares its misclassiﬁcation rates with the ones from Algorithm 1. Applied
to oﬄine dataset, the oﬄine algorithm’s misclassiﬁcation rates are consistently
lower than the online algorithm, i.e., the oﬄine dataset clustering algorithm
performs better than the online dataset clustering algorithm, when dealing with
oﬄine datasets.

Figure 1: The graph compares the misclassiﬁcation rates of Algorithm 1 and
Algorithm 2 applied to oﬄine dataset of non-Gaussian discrete-time processes.
100 runs are performed at each time step t to compute the misclassiﬁcation rate.

4.1.2 Online Dataset

In our simulated online datasets the number of sample paths and the length of
each sample path increase as t increases. This type of setting is mimicking the
situation such as modeling ﬁnancial asset prices, where new assets are launched
at each time step. The oﬄine and online clustering algorithms are applied at
each time t with 100 runs, their misclassiﬁcation rates at each time t are then
obtained.

Fig. 2 compares the misclassiﬁcation rates of oﬄine algorithm and online
algorithm applied to the online dataset described above. The periodical pattern,
that misclassiﬁcation rate increases per 10 time steps using oﬄine algorithm,
matches the timing of adding new observations. That is, the misclassiﬁcation
rate spikes whenever new observations are obtained. We observe that the mis-
classiﬁcation rate of the online algorithm is overall lower than that of oﬄine
algorithm in this dataset, reﬂecting the advantage of online algorithm against
the oﬄine one in the case where new observations are expected to occur. It is
worth pointing out that our online setting is diﬀerent from the one in Khaleghi
et al (2016), therefore the two clustering results are not comparable.

32

Finally, all the codes in MATLAB that reproduce the main conclusions in

this subsection can be found publicly online2.

Figure 2: The graph compares the misclassiﬁcation rates of Algorithm 1 and
Algorithm 2 applied to online dataset of non-Gaussian discrete-time processes.
100 runs are performed at each time step t to compute the misclassiﬁcation rate.

4.2 Clustering Fractional Brownian Motions

In this section, we present the performance of proposed oﬄine (Algorithm 1) and
online (Algorithm 2) methods, on a synthetic dataset sampled from continuous-
time Gaussian processes. The wide-sense stationary ergodic processes that we
choose are the ﬁrst order increment processes of fractional Brownian motions
(see Mandelbrot and van Ness (1968)). Denote by tBH ptqutě0 a fractional
Brownian motion with Hurst index H P p0, 1q. It is well-known that BH is a
zero-mean self-similar Gaussian process with self-similarity index H and with
covariance function

`

˘
BH psq, BH ptq

“

Cov

`

1
2

s2H ` t2H ´ |s ´ t|2H

, for s, t ě 0.

(4.2)

˘

Fix h ą 0, deﬁne its increment process (with time variation h) to be

Z pHq
h

ptq “ BH pt ` hq ´ BH ptq, for t ě 0.

is also called fractional Gaussian noise. Using the covariance function (4.2)

Z pHq
h
we obtain the auto-covariance function of Z pHq

below: for τ ě 0,

´

γpτ q “ Cov

Z pHq
h

psq, Z pHq

h

ps ` τ q

“

|τ ` h|2H ` |τ ´ h|2H ´ 2|τ |2H

.

˘

(4.3)

2https://github.com/researchcoding/clustering_WSSP_with_cov_distance.

¯

h

`

1
2

33

Recall that for stationary Gaussian processes such as Z pHq
, the strict ergodicity
can be fully expressed in the language of its auto-covariance function γ, i.e.,
the following result (Maruyama, 1970; `Slęzak, 2017) provides a suﬃcient and
necessary condition for a stationary Gaussian process to be strictly ergodic.

h

Theorem 4.1 (Strict ergodicity of Gaussian processes). A continuous-time
Gaussian stationary process X is strictly ergodic if and only if

ż

t

0

1
t

lim
tÑ8

|γX puq| du “ 0,

(4.4)

where γX denotes the auto-covariance function of X.

In view of (4.3) we can deduce that the auto-covariance function γ of Z pHq
is second-order

satisﬁes (4.4). This together with Theorem 4.1 yields that Z pHq
strict-sense stationary ergodic, so it is also wide-sense stationary ergodic.

h

h

To test our algorithms we simulate κ “ 5 groups of independent fractional
Brownian paths, with the ith group containing 10 paths as tBHip1{nq, . . . , BHippn´
1q{nq, BHip1qu, for the self-similarity indexes

H1 “ 0.3, H2 “ 0.4,

. . . , H5 “ 0.7.

Remark that clustering a zero-mean fractional Brownian motion BH is equiva-
lent to clustering its increments Z pHq
1{n ptq “ BH pt ` 1{nq ´ BH ptq. These total
number of 50 observed paths of Z pHq
1{n ptq, each of length 150, compose an oﬄine
dataset and an online one. The clustering algorithms are applied to the dataset
at each time step t. 100 runs are made to compute the misclassiﬁcation rates. we
use oﬄine (RESP. online) dataset clustering algorithm to cluster oﬄine (RESP.
online) dataset. The purpose is to compare the the algorithms with and without
log˚-transformations.

Fig. 3 presents the comparisons of 2 algorithms: one is using the dissimilar-
ity measure xd˚, the other one is using the dissimilarity measure yd˚˚, based on
the behavior of misclassiﬁcation rates as time increases. We conclude that, both
algorithms with and without the log˚-transformations are asymptotically consis-
tent. However in both oﬄine and online settings, the covariance-based dissimi-
larity measure algorithms with log˚-transformation (dashed red lines) have 30%
lower misclassiﬁcation rates on average than that of algorithms without log˚-
transformation (solid blue lines). This simulation study proves the necessity of
utilizing log˚-transformed covariance-based dissimilarity measure when the un-
derlying observations have nonlinear, especially power based, covariance-based
dissimilarity measure, such as observations sampled from self-similar processes.
The codes in MATLAB used in this subsection are provided publicly online3.

3https://github.com/researchcoding/clustering_stochastic_processes.

34

Figure 3: The top graph illustrates the misclassiﬁcation rates by oﬄine algo-
rithm applied to oﬄine datasets of increments of fractional Brownian motions.
The bottom graph plots misclassiﬁcation rates by online algorithm applied to
online datasets.

4.3 Clustering ARp1q Processes: Non Strict-sense Station-

ary Ergodic

To show that our algorithms can be applied to clustering non strict-sense sta-
tionary ergodic processes, we consider a simulation study on the non-Gaussian
ARp1q process tY ptqut deﬁned in Example 2, Eq. (1.1). We then conduct the

35

cluster analysis with κ “ 5, and specify the values of a in Eq. (1.1) as

a1 “ ´0.4, a2 “ ´0.15, a3 “ 0.1, a4 “ 0.35, a5 “ 0.6.

We mimic the procedure in Section 4.2 to generate the oﬄine and online datasets
of tXptqut. Fig. 4 illustrates the consistent converging property of oﬄine algo-
rithm and online algorithm under diﬀerent dataset settings.

All the codes in MATLAB that reproduce the main conclusions in this sub-

section can be found publicly online4.

4https://github.com/researchcoding/clustering_nonGaussian_processes.

36

Figure 4: The top graph plots the misclassiﬁcation rates of (log˚) covariance-
based dissimilarity measure along with the increase of time using oﬄine and
online algorithms on oﬄine dataset. The bottom graph shows misclassiﬁcation
rates with both algorithms on online dataset.

37

4.4 Application to the Real World: Clustering Global Eq-

uity Markets

4.4.1 Data and Methodology

In this section we apply the clustering algorithms to real-world datasets. The
application involves in dividing equity markets of major economic entities in
the world into diﬀerent subgroups. In ﬁnancial economics, researchers usually
cluster global equity markets according to either geographical region or the de-
velopment stage of the underlying economic entities. The reasoning of these
clustering methods is that entities with less geographical distance and closer
development level involve in more bilateral economic activities. Impacted by
similar economic factors, entities with less “distance” tend to have higher cor-
relation in stock market performance. This correlation then measures the level
of “comovement” of stock market indexes on global capital market.

However, the globalization is breaking the barriers of region and develop-
ment level. For instance, in 2016 China became the largest trader partner with
the U.S. (besides EU)5. China is not a regional neighbor of the U.S., and is
categorized as a developing country by World Bank, in opposite to the U.S. as
a developed country.

We cluster the equity markets in the world according to the empirical covari-
ance structure of their performance, using Algorithms 1 and 2 as purposed in
this paper. Then we compare our clustering results with the traditional cluster-
ing methodologies. The index constituents of MSCI ACWI (All Country World
Index) are selected as the sample data. Each of the observations is a sample
path representing the historical monthly returns of underlying economic enti-
ties. Through empirical study it is proved that these indexes returns exhibit
the “long memory” path feature hence they can be modeled by self-similar pro-
cesses such as fractional Brownian motions (see e.g. Comte and Renault (1998);
Bianchi and Pianese (2008)). Therefore similar to Section 4.2 we may cluster the
increments of the indexes returns with the log˚-transformed dissimilarity mea-
sure yd˚˚. MSCI ACWI is the leading global equity market index and has $3.2
billion in underlying market capitalization6. MSCI ACWI contains 23 devel-
oped markets, 24 emerging markets from 4 regions: Americas, EMEA (Europe,
Middle East and Africa), Paciﬁc and Asia. Table 1 lists all markets included in
this empirical study. We exclude Greece market due to its bankruptcy after the
global ﬁnancial crisis.

We construct both oﬄine and online datasets starting from diﬀerent dates.
For oﬄine dataset we let it start from Jan. 30, 2009 to exclude the ﬁnancial
crisis period in 2007 and 2008. This is because, under global stock market crisis,
the (downside) performance of equity market is contagious and thus blurs the
cluster analysis. The online dataset starts on Jan. 31, 1989, which covers 1997
Asian ﬁnancial crisis, 2003 dot-com bubble and 2007 subprime mortgage crisis.

5Source: U.S. Department of Commerce, Census Bureau, Economic Indicators Division.
6As of June 30, 2017, as reported on September 30, 2017 by eVestment, Morningstar and

Bloomberg.

38

Another key feature is that 14 markets are added to the MSCI ACWI index
(at diﬀerent time) since 1989, including 1 developed market and 13 emerging
markets. Therefore, the case where new time series are observed is handled in
online dataset.

4.4.2 Clustering Results

We compare the clustering outcomes of both oﬄine and online datasets with
separations suggested by region (4 groups) and development level (2 groups).
The factor with the lowest misclassiﬁcation rate is proved to be the correspond-
ing factor that contributes to increase covariance-based dissimilarity measure
the most. In other words, this corresponding factor leads to the clustering of
stock markets with the most signiﬁcant impact.

Table 2 shows that the misclassiﬁcation rates for development levels are sig-
niﬁcantly and consistently lower than that of geographical region, for both algo-
rithms (oﬄine and online algorithms) and datasets (oﬄine and online datasets).
The clustering results seem to infer that the geographical distance is less dom-
inating than the development level of underlying economic entities, when ana-
lyzing diﬀerent groups of equity markets.

The global minimum of the misclassiﬁcation rate occurs when we use online
algorithm on oﬄine dataset. Table 3 presents the detailed clustering outcome
under this circumstance. In each group, the correctly and incorrectly catego-
rized equity markets are listed respectively. For instance, China (Mainland)
market is correctly categorized along with other emerging market. Meanwhile
Austria market, though being developed market in MSCI ACWI, is categorized
to the group where most of the equity markets are emerging markets. The mis-
classiﬁed markets in the emerging group are Austria, Finland, Italy, Norway and
Spain markets. The misclassiﬁed markets in the developed group are Malaysia,
Philippines, Taiwan, Chile and Mexico markets. These empirical results thus
suggest that several capital markets have irregular post-crisis performance which
blurs the barrier between emerging and developed markets.

The contribution of this real-world dataset cluster analysis is two-fold. First,
we explored and determined the principal force that brings structural diﬀerence
in global capital markets, which potentially predicts the “comovement” pattern
of future index performance. Second, we provided new evidence on the impact
of globalization on breaking geographical barriers between economic entities.

5 Conclusion and Future Perspectives

Inspired by Khaleghi et al (2016), we introduce the problem of clustering wide-
sense stationary ergodic processes. A new covariance-based dissimilarity mea-
sure is proposed to obtain asymptotically consistent clustering algorithms for
both oﬄine and online settings. The recommended algorithms are competitive
for at least two reasons:

39

m
o
r
f

s
t
e
k
r
a
m
3
2

e
r
a

e
r
e
h
T

.
)
x
e
d
n
I

d

l
r
o

W
y
r
t
n
u
o
C

l
l

A
(

I

W
C
A

I

C
S
M

e
h
t

n

i

s
t
e
k
r
a
m
y
t
i

u
q
e

r
o
j
a
m

f
o

s
e
i
r
o
g
e
t
a
c

e
h
T

:
1

e
l
b
a
T

,
s
a
c
i
r
e
m
A
s
n
i
a
t
n
o
c

g
n
i
r
e
t
s
u
l
c

l
a
c
i
h
p
a
r
g
o
e
g

e
h
T

.
s
a
e
r
a

r
o

s
e
i
r
t
n
u
o
c

g
n

i
g
r
e
m
e
m
o
r
f

s
t
e
k
r
a
m
4
2

d
n
a

,
s
e
i
t
i
t
n
e

c
i
m
o
n
o
c
e

d
e
p
o
l
e
v
e
d

.
a
i
s
A
d
n
a

c
ﬁ

i
c
a
P

,
)
a
c
i
r
f
A
d
n
a

t
s
a
E
e
l
d
d
i
M

,
e
p
o
r
u
E
(
A
E
M
E

s
t
e
k
r
a
M
g
n
i
g
r
e
m
E

s
t
e
k
r
a
M
d
e
p
o
l
e
v
e
D

a
i
s
A

a
c
i
r
f
A
&
t
s
a
E
e
l
d
d
M
&
e
p
o
r
u
E

i

s
a
c
i
r
e
m
A

c
ﬁ

i
c
a
P

t
s
a
E
e
l
d
d
i
M
&
e
p
o
r
u
E

s
a
c
i
r
e
m
A

)
d
n
a
l
n
i
a
M

(

a
n
i
h
C

c
i
l
b
u
p
e
R
h
c
e
z
C

a
i
d
n
I

a
i
s
e
n
o
d
n
I

a
e
r
o
K

a
i
s
y
a
l
a
M

n
a
t
s
i
k
a
P

s
e
n
i
p
p
i
l
i
h
P

n
a
w
i
a
T

d
n
a
l
i
a
h
T

e
c
e
e
r
G

y
r
a
g
n
u
H

d
n
a
l
o
P

a
i
s
s
u
R

y
e
k
r
u
T

t
p
y
g
E

a
c
i
r
f
A
h
t
u
o
S

r
a
t
a
Q

s
e
t
a
r
i

m
E
b
a
r
A
d
e
t
i
n
U

l
i
z
a
r
B

e
l
i

h
C

a
i
b
m
o
l
o
C

o
c
i
x
e
M

u
r
e
P

a
i
l
a
r
t
s
u
A

g
n
o
K
g
n
o
H

n
a
p
a
J

d
n
a
l
a
e
Z
w
e
N

e
r
o
p
a
g
n
S

i

a
d
a
n
a
C

A
S
U

40

a
i
r
t
s
u
A

m
u
i
g
l
e
B

k
r
a
m
n
e
D

d
n
a
l
n
i
F

e
c
n
a
r
F

y
n
a
m
r
e
G

d
n
a
l
e
r
I

l
e
a
r
s
I

y
l
a
t
I

y
a
w
r
o
N

l
a
g
u
t
r
o
P

n
i
a
p
S

n
e
d
e
w
S

s
d
n
a
l
r
e
h
t
e
N

d
n
a
l
r
e
z
t
i
w
S

m
o
d
g
n
i
K
d
e
t
i
n
U

.
i
w
c
a
/
m
o
c
.
i
c
s
m
.
w
w
w
/
/
:
s
p
t
t
h

.
n
o
i
t
a
c
o
l
l
a

t
e
k
r
a
m

)
x
e
d
n
I

d
l
r
o

W
y
r
t
n
u
o
C

l
l

A
(

I

W
C
A

I
C
S
M

:
e
c
r
u
o
S

Table 2: The misclassiﬁcation rates of clustering algorithms on datasets, com-
paring to clusters suggested by geographical region and development levels.

oﬄine algorithm

online algorithm

region

development level

region

development level

oﬄine dataset
online dataset

63.04%
59.57%

28.26%
44.68%

60.87%
57.45%

23.91%
38.30%

Table 3: The clustering outcome of equity markets using oﬄine dataset (starting
from Jan. 30, 2009) and online algorithm. The algorithm divides the whole
dataset (excluding Greece) into two groups, and in each group the correctly and
correctly separated markets are listed, respectively.

Group 1 (Emerging Markets)

Group 2 (Developed Markets)

Incorrect
Malaysia
Philippines
Taiwan
Thailand
Chile
Mexico

Incorrect
Austria
Finland
Italy
Norway
Spain

Correct
China (Mainland)
India
Indonesia
Korea
Pakistan
Brazil
Colombia
PERU
Czech Republic
Hungary
Poland
Russia
Turkey
Egypt
South Africa
Qatar
United Arab Emirates

Correct
Belgium
Denmark
France
Germany
Ireland
Israel
Netherlands
Portugal
Sweden
Switzerland
United Kingdom
Australia
Hong Kong
Japan
New Zealand
Singapore
Canada
USA

41

1. Our algorithms are applicable to clustering a wide class of stochastic pro-
cesses, including any strict-sense stationary ergodic processes whose co-
variance structures are ﬁnite.

2. Our algorithms are eﬃcient enough in terms of their computational com-
plexity cost. In particular, a so-called log˚-transformation is introduced
to improve the eﬃciency of clustering, for self-similar processes.

The above advantages have been supported through the simulation study on
non-Gaussian discrete-time processes, fractional Brownian motions, non-Gaussian
non strict-sense stationary ergodic ARp1q processes, and a real-world applica-
tion: clustering global equity markets. The implementations in MATLAB of
our clustering algorithms are provided publicly online.

Finally we note that, the clustering framework proposed in our paper focuses
on the cases where the true number of clusters κ is known. The case for which κ
is unknown is still open and left to future research. Another interesting problem
is that, many stochastic processes are not wide-sense stationary but they get a
tight relationship with the wide-sense stationarity. For example, a self-similar
process does not necessarily have wide-sense stationary increments, but their
Lamperti transformations are strict-sense stationary (Lamperti, 1962); locally
asymptotically self-similar processes are generally not self-similar but their tan-
gent processes are self-similar (Boufoussi et al, 2008). Our cluster analysis sheds
light on clustering the above processes. These topics can be left for future re-
search.

References

Bastos J. A., Caiado J. (2014) Clustering ﬁnancial time series with variance

ratio statistics. Quantitative Finance 14(12):2121–2133.

Bianchi S., Pianese A. (2008) Multifractional properties of stock indices decom-
posed by ﬁltering their pointwise H¨older regularity. International Journal of
Theoretical and Applied Finance 11(06):567–595.

Boufoussi B., Dozzi M., Guerbaz R. (2008) Path properties of a class of lo-
cally asymptotically self similar processes. Electronic Journal of Probability
13(29):898–921.

Cambanis S., Hardin C. J., Weron A. (1987) Ergodic properties of stationary
stable processes. Stochastic Processes and their Applications 24(1):1–18.

Cesa-Bianchi N., Lugosi G. (2006) Prediction, Learning, and Games. Cambridge

University Press.

Comte F., Renault E. (1998) Long memory in continuous-time stochastic volatil-

ity models. Mathematical Finance 8(4):291–323.

42

Damian D., Oreˇsiˇc M., Verheij E., et al. (2007) Applications of a new subspace
clustering algorithm (COSA) in medical systems biology. Metabolomics
3(1):69–77.

Embrechts P., Maejima M. (2000) An introduction to the theory of self-
similar stochastic processes. International Journal of Modern Physics B
14(12):1399–1420.

Gray R. M. (1988) Probability, Random Processes, and Ergodic Properties.

Springer.

Hartigan J. A. (1975) Clustering Algorithms. John Wiley & Sons, Inc.

Herdin M., Czink N., Ozcelik H., Bonek E. (2005) Correlation matrix distance,
a meaningful measure for evaluation of non-stationary MIMO channels. In:
IEEE 61st Vehicular Technology Conference, 2005., vol 1, pp. 136–140.

Hirkhorshidi A. S., Aghabozorgi S., Wah T. Y. (2015) A comparison study on
similarity and dissimilarity measures in clustering continuous data. PLoS
ONE 10(12):e0144059.

Ieva F., Paganoni A. M., Tarabelloni N. (2016) Covariance-based clustering
in multivariate and functional data analysis. Journal of Machine Learning
Research 17:1–21.

J¨a¨askinen V., Parkkinen V., Cheng L., Corander J. (2014) Bayesian clustering
of DNA sequences using markov chains and a stochastic partition model.
Statistical Applications in Genetics and Molecular Biology 13(1):105–121.

Jain A. K., Mao J. (1996) A self-organizing network for hyperellipsoidal clus-

tering (HEC). IEEE Transactions on Neural Networks 7:16–29.

Jain A. K., Murty M. N., Flynn P. J. (1999) Data clustering: a review. ACM

Computing Surveys (CSUR) 31(3):264–323.

Juozapaviˇcius A., Rapsevicius V. (2001) Clustering through decision tree con-
struction in geology. Nonlinear Analysis: Modelling and Control 6(2):29–41.

Katsavounidis I., Kuo C. J., Zhang Z. (1994) A new initialization technique for
generalized Lloyd iteration. IEEE Signal Processing Letters 1(10):144–146.

Khaleghi A., Ryabko D., Mari J., Preux P. (2016) Consistent algorithms for
clustering time series. Journal of Machine Learning Research 17(3):1–32.

Kleinberg J. M. (2003) An impossibility theorem for clustering. In: Advances
in Neural Information Processing Systems (NIPS), vol 15, pp. 463–470.

Lamperti J. W. (1962) Semi-stable stochastic processes. Transactions of the

American Mathematical Society 104:62–78.

Magdziarz M., Weron A. (2011) Ergodic properties of anomalous diﬀusion pro-

cesses. Annals of Physics 326:2431–2443.

Mandelbrot B., van Ness J. W. (1968) Fractional Brownian motions, fractional

noises and applications. SIAM Review 10(4):422–437.

Maruyama G. (1970) Inﬁnitely divisible processes. Theory of Probability and

43

its Applications 15(1):1–22.

Pavlidis N. G., Plagianakos V. P., Tasoulis D. K., Vrahatis M. N. (2006) Fi-
nancial forecasting through unsupervised clustering and neural networks.
Operational Research 6(2):103–127.

Peng J., M¨uller H.-G. (2008) Distance-based clustering of sparsely observed
stochastic processes, with applications to online auctions. The Annals of
Applied Statistics 2(3):1056–1077.

Peng Q. (2012) Uniform H¨older exponent of a stationary increments Gaussian
process: estimation starting from average values. Statistics & Probability
Letters 81(8):1326–1335.

Rubinstein M., Joulin A., Kopf J., Liu C. (2013) Unsupervised joint object
discovery and segmentation in internet images. In: The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pp. 1939–1946.

Samorodnitsky G. (2004) Extreme value theory, ergodic theory and the bound-
ary between short memory and long memory for stationary stable processes.
The Annals of Probability 32(2):1438–1468.

Samorodnitsky G., Taqqu M. S. (1994) Stable Non-Gaussian Random Processes:
Stochastic Models with Inﬁnite Variance. Chapman & Hall, New York.

Sen P. K., Singer J. M. (1993) Large Sample Methods in Statistics. Chapman

& Hall, Inc.

Shields P. C. (1996) The Ergodic Theory of Discrete Sample Paths, Graduate

Studies in Mathematics, vol 13. American Mathematical Society.

`Slęzak J. (2017) Asymptotic behaviour of time averages for non-ergodic Gaus-

sian processes. Annals of Physics 383:285–311.

Slonim N., Atwal G. S., Tkaˇcik G., Bialek W. (2005) Information-based clus-

tering. PNAS 102(51):18297–18302.

Wilson D. R., Martinez T. R. (1997) Improved heterogeneous distance functions.

JAIR 6:1–34.

Xu R., Wunsch D. (2005) Survey of clustering algorithms. IEEE Transactions

on Neural Networks 16(3):645–678.

Zhao W., Zou W., Chen J. J. (2014) Topic modeling for cluster analysis of large

biological and medical datasets. BMC Bioinformatics 15:S11.

44

