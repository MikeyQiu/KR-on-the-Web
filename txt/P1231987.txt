Can Evolutionary Sampling Improve Bagged Ensembles?

6
1
0
2
 
t
c
O
 
3
 
 
]

G
L
.
s
c
[
 
 
1
v
5
6
4
0
0
.
0
1
6
1
:
v
i
X
r
a

Harsh Nisar
Bhanu Pratap Singh

NISAR.HARSH@GMAIL.COM
BHANUPRATAP.MNIT@GMAIL.COM

Abstract

constructed on randomly chosen sub-spaces.

Perturb and Combine (P&C) group of meth-
ods generate multiple versions of the predictor
by perturbing the training set or construction
and then combining them into a single predictor
(Breiman, 1996b). The motive is to improve the
accuracy in unstable classiﬁcation and regression
methods. One of the most well known method in
this group is Bagging. Arcing or Adaptive Re-
sampling and Combining methods like AdaBoost
are smarter variants of P&C methods.
In this
extended abstract, we lay the groundwork for a
new family of methods under the P&C umbrella,
known as Evolutionary Sampling (ES). We em-
ploy Evolutionary algorithms to suggest smarter
sampling in both the feature space (sub-spaces)
as well as training samples. We discuss multiple
ﬁtness functions to assess ensembles and empir-
ically compare our performance against random-
ized sampling of training data and feature sub-
spaces.

1. Introduction

Bagging and various variants of it have been widely
popular and studied extensively in the last two decades
(Breiman, 2001; 1996a; 1999a; Ho, 1998). There has been
notable work in understanding the theoretical underpin-
ning of bootstrap aggregating and as to what makes it such
a powerful method (Domingos, 1997; B¨uchlmann & Yu,
In traditional bagging, each training example is
2002).
sampled with replacement and with probability 1
N . Adap-
tive Resampling and Combining (Arcing) techniques which
modify the probability of each training example being sam-
pled based on heuristics have also been developed and
widely used (Freund et al., 1996; Breiman, 1999b).

Random subspace methods also known as attribute bagging
refer to creating ensembles of predictors trained on ran-
domly selected subsets of total features, that is, predictors

Proceedings of the 33 rd International Conference on Machine
Learning, New York, NY, USA, 2016. JMLR: W&CP volume
48. Copyright 2016 by the author(s).

Both methods, sub-sampling and sub-spacing reduce the
variance of the ﬁnal ensemble and hence increase the accu-
racy. Arcing methods are known to reduce the bias of the
model as well.

Error based resampling algorithms which try to set
to zero (Freund et al., 1996), de-
the train-set error
signed bagged ensembles with minimal
intersection
(Papakonstantinou et al., 2014), diversity and uncorrelated
errors (Kuncheva & Whitaker, 2003; Tang et al., 2006),
importance sampling (Breiman, 1999a) etc. are some of
the areas being studied to improve bagged ensembles. Ei-
ther there are multiple answers to the question, or the an-
swer changes with each dataset.

Instead of ﬁguring out precisely as to what sampling and
combination of training sets make a bagged ensemble bet-
ter, we try to ﬁx the deﬁnition of better, and allow the boot-
strapped training sets to evolve themselves in order to align
with the deﬁnition. We generate multiple sampled candi-
date training sets for the ﬁnal ensemble and let them com-
pete, mutate and mate their way to the optimal sampling
and combination. Evolutionary computation has been used
for selection of different predictors to be part of an ensem-
ble (Gagn´e et al., 2007) and also for the selection of the
most suitable machine learning pipeline for a classiﬁcation
problem (Olson et al., 2016).

To our best knowledge, genetic algorithms haven’t been di-
rectly used to evolve bootstrapped samples of the training
data.

Table1. Genetic algorithm’s experiment setting
GA parameter
Selection
Crossover
Population Size
Per-individual mutation rate
Per-individual crossover rate
Generations
Ensemble size in an individual

Value
3-way tournament
2-point crossover
30
0.4
0.6
30
10

2. Algorithm

Evolutionary computation techniques evolve a population
of solution variables (bootstrapped training sets in our case)
to optimize towards a given criteria. The ﬁttest offspring

Can Evolutionary Sampling Improve Bagged Ensembles?

Table2. Results comparing performance of ﬁrst individual (FI) and hall of fame (HOF) on unseen data. Values are averages (standard
deviations) over 100 runs. Statistical tests are p-values of paired t-tests on the test mean squared error (mse) compared between FI and
HOI. Win(%) is the proportion of runs where HOF had a lower mse than FI on unseen data.

Data Set
Servo
Nr. Inputs
#Training
#Test
Ozone
Nr. Inputs
#Training
#Test
Boston
Nr. Inputs
#Training
#Test
Abalone
Nr. Inputs
#Training
#Test

Parameters
HOF - Mean (Std. Dev.)
FI - Mean (Std. Dev.)

HOF - Mean (Std. Dev.)
FI - Mean (Std. Dev.)

HOF - Mean (Std. Dev.)
FI - Mean (Std. Dev.)

HOF - Mean (Std. Dev.)
FI - Mean (Std. Dev.)

4
167 Win (%)
10% p-value

8
330 Win (%)
10% p-value

12
506 Win (%)
10% p-value

8
4177 Win (%)
25% p-value

FEMPO
0.16 (0.11)
0.20 (0.15)
64
0
18.20 (4.77)
18.12 (4.58)
51
0.71
12.49 (6.88)
12.96 (6.76)
63
0.16
4.93 (0.37)
4.92 (0.35)
54
0.42

Sub-sampling
FEMPT
0.2 (0.43)
0.2 (0.38)
43
0.475
18.28 (6.02)
18.15 (5.62)
52
0.59
13.09 (7.09)
13.11 (6.6)
48
0.93
5.025 (0.402)
5.012 (0.409)
40
0.25

FEGT
0.26 (0.23)
0.28 (0.32)
52
0.47
18.79 (5.62)
19.66 (6.55)
57
0.025
12.74 (5.4)
13.95 (6.96)
58
0
4.81(0.26)
4.86(0.22)
72
0

Sub-spacing

FEMPO
0.25(0.29)
0.71(0.63)
94
0
18.48 (5.38)
20.16 (5.59)
67
0
20.78 (11.35)
25.07 (13.41)
75
0
5.09(0.59)
5.49(0.71)
90
0

FEGT
0.59(0.52)
0.72(0.49)
65
0.01
19.11(5.98)
20.27(6.02)
62
0.01
21.02(12.23)
27.8(15.06)
81
0
5.02(0.54)
5.4(0.62)
84
0

across all the generations is considered as the most optimal
solution. In Evolutionary Sampling (ES), we followed a
standard Genetic Algorithm. Initially, a population of mul-
tiple ensembles is generated by randomly sampling from
the training data multiple times. Each ensemble (hence-
forth referred to as an individual) in a generation is evalu-
ated based on a ﬁtness function. Fit individuals are selected
for the next generation. After this, crossover is applied on
a ﬁxed percentage of individuals wherein two individuals
swap their predictors. Post this, a ﬁxed percentage of in-
dividuals unaffected by crossover undergo a random mu-
tation. Randomly selected member datasets from the se-
lected individual have some of their rows/features deleted,
replaced or inserted with equal probability. In feature sub-
spacing the features are subject to perturbation whereas in
sub-sampling rows are perturbed.

We’ve used the Python package DEAP (Fortin et al., 2012)
to implement ES. GA parameters are shown in table 1.
As suggested before, instead of understanding as to what
makes a bagged ensemble better, we try to rely on the def-
inition of better and try to evolve our ensemble into the
same. The ﬁtness function is what guides the sampling and
combination of the different sampled datasets. We propose
three ﬁtness functions and then try to analyse their perfor-
mance.

FEMPO: Fitness Each Model Private Out of Bag. It takes
each predictor part of the candidate ensemble and measures
their performance on the samples that were left out of it’s
training bag (Breiman, 1996c). Final ﬁtness of the ensem-
ble is the mean of each model’s RMSE.

FEMPT: Fitness Each Model Private Test. It is the average
of the performance of member predictors on a private test-
set which is held out for each sampled dataset during its
instantiation.

FEGT: Fitness Ensemble Global Test. During the start of
the algorithm, 20% of the training data is set aside. Each
ensemble’s prediction is based on the average prediction of
it’s member predictors. RMSE is calculated against the set
aside global test.

3. Experiment

We conduct experiments on two variants of sampling : Sub-
sampling and Sub-spacing. Sub-sampling works on sam-
pling training examples, whereas sub-spacing works on
generating multiple feature sets. We conduct our experi-
ments on 4 benchmark datasets [see table 2]. We compare
the mean squared error of the ﬁrst individual (FI) of the
ﬁrst generation with the hall of fame (best individual) after
30 generations. We assume that the ﬁrst individual in the
ﬁrst generation is representative of an ensemble which ran-
domly samples its rows or features like in traditional bag-
ging. We try to analyse whether ES is able to evolve better
ensembles starting from random specimens. We uniformly
use an unpruned Decision Tree Regressor with max depth
arbitrarily set as 5.

4. Results and conclusion

A 50% win-ratio would suggest that the performance of
the ensemble after undergoing ES is better than its random
counterpart only half the times. Mean MSE and standard
deviation of the same is also a good metric to compare
ES with random instantiation. The null hypothesis in the
paired t-test suggests that the average mean squared error
between the two methods is the same.
If the p-value is
smaller than a threshold, then we reject the null hypothesis
of equal averages.

In sub-sampling, FEMPO and FEGT performs equally or

Can Evolutionary Sampling Improve Bagged Ensembles?

better than their random counterparts. Though the win per-
centages are almost half in many cases, it could be that the
algorithm was initialized with an optimal combination and
sampling. FEGT shows the most improvement in Abalone
(72% win-ratio).

In sub-spacing, both FEMPO and FEGT do signiﬁcantly
better than random sub-spacing. GA has deﬁnitely helped
in improving accuracy of the model. One should note,
GA has a narrow exploration space in case of feature sub-
spacing as compared to sub-sampling and features play a
more signiﬁcant role in deciding the model’s behaviour
than a few rows of data.

Results suggest that ES is possibly useful in cases where
maximum accuracy needs to be juiced out and computation
is not an issue. It’s evident that better and more robust ﬁt-
ness functions need to be explored, even multi-objective ﬁt-
ness functions, which better represent generalizability and
error of the ensemble.

It needs to be explored how these methods can be used to
generate different models for smaller segments or patches
of the dataset (Breiman, 1999a). Can the segments, sug-
gested by ES along with ﬁtness functions that take into ac-
count each models ﬁtness (FEPT or FEMPO), be used to
ﬁnd different cohorts in the dataset?

ES guided sub-spacing using linear base estimators can
be useful in high dimensional problems like genomic data
where selecting features is very important while keeping
ﬁnal models interpretable.

It will be interesting to see what happens if the algorithm is
allowed to run for generations till the ﬁtness test error re-
duces approximately to zero. We plan to experiment with
different base estimators for each sampled dataset and also
explore how sub-spacing and sub-sampling can be com-
bined into one algorithm.

the

reproduction, we’ve re-
Going with the theme of
framework for ES on GitHub
basic
leased
(http://github.com/evoml/evoml).
We encourage re-
searchers to contribute to the project and test out different
ﬁtness functions themselves.

References

Breiman, Leo. Bagging predictors. Machine learning, 24

(2):123–140, 1996a.

Breiman, Leo. Bias, variance, and arcing classiﬁers. 1996b.

Breiman, Leo. Out-of-bag estimation. Technical report,

Citeseer, 1996c.

Breiman, Leo. Pasting small votes for classiﬁcation in large

databases and on-line. Machine Learning, 36(1-2):85–
103, 1999a.

Breiman, Leo. Using adaptive bagging to debias regres-
sions. Technical report, Technical Report 547, Statistics
Dept. UCB, 1999b.

Breiman, Leo. Random forests. Machine learning, 45(1):

5–32, 2001.

B¨uchlmann, Peter and Yu, Bin. Analyzing bagging. Annals

of Statistics, pp. 927–961, 2002.

Domingos, Pedro M. Why does bagging work? a bayesian
account and its implications. In KDD, pp. 155–158. Cite-
seer, 1997.

Fortin, F´elix-Antoine, De Rainville, Franc¸ois-Michel,
Gardner, Marc-Andr´e, Parizeau, Marc, and Gagn´e,
Christian. DEAP: Evolutionary algorithms made easy.
Journal of Machine Learning Research, 13:2171–2175,
jul 2012.

Freund, Yoav, Schapire, Robert E, et al. Experiments with
a new boosting algorithm. In ICML, volume 96, pp. 148–
156, 1996.

Gagn´e, Christian, Sebag, Michele, Schoenauer, Marc, and
Tomassini, Marco. Ensemble learning for free with evo-
lutionary algorithms?
In Proceedings of the 9th an-
nual conference on Genetic and evolutionary computa-
tion, pp. 1782–1789. ACM, 2007.

Ho, Tin Kam. The random subspace method for construct-
ing decision forests. Pattern Analysis and Machine In-
telligence, IEEE Transactions on, 20(8):832–844, 1998.

Kuncheva, Ludmila I and Whitaker, Christopher J. Mea-
sures of diversity in classiﬁer ensembles and their rela-
tionship with the ensemble accuracy. Machine learning,
51(2):181–207, 2003.

Olson, Randal S., Urbanowicz, Ryan J., Andrews, Peter C.,
Lavender, Nicole A., Kidd, La Creis, and Moore, Ja-
son H. Automating biomedical data science through
tree-based pipeline optimization.
In Squillero, G and
Burelli, P (eds.), Proceedings of the 18th European Con-
ference on the Applications of Evolutionary and Bio-
inspired Computation, Lecture Notes in Computer Sci-
ence, Berlin, Germany, 2016. Springer-Verlag.

Papakonstantinou, Periklis A, Xu, Jia, and Cao, Zhu. Bag-
ging by design (on the suboptimality of bagging).
In
AAAI, pp. 2041–2047, 2014.

Tang, E Ke, Suganthan, Ponnuthurai N, and Yao, Xin. An
analysis of diversity measures. Machine Learning, 65
(1):247–271, 2006.

