Bayesian Cluster Enumeration Criterion for
Unsupervised Learning

Freweyni K. Teklehaymanot, Student Member, IEEE, Michael Muma, Member, IEEE,
and Abdelhak M. Zoubir, Fellow, IEEE

1

8
1
0
2
 
g
u
A
 
7
2
 
 
]
T
S
.
h
t
a
m

[
 
 
3
v
4
5
9
7
0
.
0
1
7
1
:
v
i
X
r
a

Abstract—We derive a new Bayesian Information Criterion
(BIC) by formulating the problem of estimating the number of
clusters in an observed data set as maximization of the posterior
probability of
the candidate models. Given that some mild
assumptions are satisﬁed, we provide a general BIC expression
for a broad class of data distributions. This serves as a starting
point when deriving the BIC for speciﬁc distributions. Along this
line, we provide a closed-form BIC expression for multivariate
Gaussian distributed variables. We show that incorporating the
data structure of the clustering problem into the derivation of
the BIC results in an expression whose penalty term is different
from that of the original BIC. We propose a two-step cluster enu-
meration algorithm. First, a model-based unsupervised learning
algorithm partitions the data according to a given set of candidate
models. Subsequently, the number of clusters is determined as
the one associated with the model for which the proposed BIC is
maximal. The performance of the proposed two-step algorithm
is tested using synthetic and real data sets.

Index Terms—model selection, Bayesian information criterion,
cluster enumeration, cluster analysis, unsupervised learning,
multivariate Gaussian distribution

I. INTRODUCTION

S TATISTICAL model selection is concerned with choosing

a model that adequately explains the observations from
a family of candidate models. Many methods have been pro-
posed in the literature, see for example [1]–[25] and the review
in [26]. Model selection problems arise in various applications,
such as the estimation of the number of signal components
[15], [18]–[20], [23]–[25], the selection of the number of non-
zero regression parameters in regression analysis [4]–[6], [11],
[12], [14], [21], [22], and the estimation of the number of data
clusters in unsupervised learning problems [27]–[45]. In this
paper, our focus lies on the derivation of a Bayesian model
selection criterion for cluster analysis.
The estimation of the number of clusters, also called cluster
enumeration, has been intensively researched for decades
[27]–[45] and a popular approach is to apply the Bayesian
Information Criterion (BIC) [29], [31]–[33], [37]–[41], [44].
The BIC ﬁnds the large sample limit of the Bayes’ estimator
which leads to the selection of a model that is a posteriori
most probable. It is consistent if the true data generating model
belongs to the family of candidate models under investigation.

F. K. Teklehaymanot and A. M. Zoubir are with the Signal Process-
ing Group and the Graduate School of Computational Engineering, Tech-
nische Universit¨at Darmstadt, Darmstadt, Germany (e-mail: ftekle@spg.tu-
darmstadt.de; zoubir@spg.tu-darmstadt.de).

M. Muma is with the Signal Processing Group, Technische Universit¨at

Darmstadt, Darmstadt, Germany (e-mail: muma@spg.tu-darmstadt.de).

The BIC was originally derived by Schwarz in [8] assum-
ing that (i) the observations are independent and identically
distributed (iid), (ii) they arise from an exponential family
of distributions, and (iii) the candidate models are linear in
parameters. Ignoring these rather restrictive assumptions, the
BIC has been used in a much larger scope of model selection
problems. A justiﬁcation of the widespread applicability of
the BIC was provided in [16] by generalizing Schwarz’s
derivation. In [16], the authors drop the ﬁrst two assumptions
made by Schwarz given that some regularity conditions are
satisﬁed. The BIC is a generic criterion in the sense that
it does not
incorporate information regarding the speciﬁc
model selection problem at hand. As a result, it penalizes two
structurally different models the same way if they have the
same number of unknown parameters.
The works in [15], [46] have shown that model selection
rules that penalize for model complexity have to be examined
carefully before they are applied to speciﬁc model selection
problems. Nevertheless, despite the widespread use of the BIC
for cluster enumeration [29], [31]–[33], [37]–[41], [44], very
little effort has been made to check the appropriateness of
the original BIC formulation [16] for cluster analysis. One
noticeable work towards this direction was made in [38] by
providing a more accurate approximation to the marginal
likelihood for small sample sizes. This derivation was made
speciﬁcally for mixture models assuming that they are well
separated. The resulting expression contains the original BIC
term plus some additional terms that are based on the mixing
probability and the Fisher Information Matrix (FIM) of each
partition. The method proposed in [38] requires the calculation
of the FIM for each cluster in each candidate model, which is
computationally very expensive and impractical in real world
applications with high dimensional data. This greatly limits
the applicability of the cluster enumeration method proposed
in [38]. Other than the above mentioned work, to the best
of our knowledge, no one has thoroughly investigated the
derivation of the BIC for cluster analysis using large sample
approximations.
We derive a new BIC by formulating the problem of estimating
the number of partitions (clusters) in an observed data set
as maximization of the posterior probability of the candidate
models. Under some mild assumptions, we provide a general
expression for the BIC, BICG(·), which is applicable to a
broad class of data distributions. This serves as a starting
point when deriving the BIC for speciﬁc data distributions
in cluster analysis. Along this line, we simplify BICG(·) by
imposing an assumption on the data distribution. A closed-

c(cid:13)2018 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other works. DOI: 10.1109/TSP.2018.2866385, IEEE Transactions on Signal Processing

such as

form expression, BICN(·), is derived assuming that the data
set
is distributed as a multivariate Gaussian. The derived
model selection criterion, BICN(·), is based on large sample
approximations and it does not require the calculation of the
FIM. This renders our criterion computationally cheap and
practical compared to the criterion presented in [38].
the Expectation-
Standard clustering methods,
Maximization (EM) and K-means algorithm, can be used to
cluster data only when the number of clusters is supplied by
the user. To mitigate this shortcoming, we propose a two-
step cluster enumeration algorithm which provides a principled
way of estimating the number of clusters by utilizing existing
clustering algorithms. The proposed two-step algorithm uses
a model-based unsupervised learning algorithm to partition
the observed data into the number of clusters provided by
the candidate model prior to the calculation of BICN(·) for
that particular model. We use the EM algorithm, which is
a model-based unsupervised learning algorithm, because it is
suitable for Gaussian mixture models and this complies with
the Gaussianity assumption made by BICN(·). However, the
model selection criterion that we propose is more general and
can be used as a wrapper around any clustering algorithm, see
[47] for a survey of clustering methods.
The paper is organized as follows. Section II formulates the
problem of estimating the number of clusters given data.
The proposed generic Bayesian cluster enumeration criterion,
BICG(·), is introduced in Section III. Section IV presents the
proposed Bayesian cluster enumeration algorithm for multi-
variate Gaussian data in detail. A brief description of the
existing BIC-based cluster enumeration methods is given in
Section V. Section VI provides a comparison of the penalty
terms of different cluster enumeration criteria. A detailed
performance evaluation of the proposed criterion and compar-
isons to existing BIC-based cluster enumeration criteria using
simulated and real data sets are given in Section VII. Finally,
concluding remarks are drawn and future directions are brieﬂy
discussed in Section VIII. A detailed proof is provided in
Appendix A, whereas Appendix B contains the vector and
matrix differentiation rules that we used in the derivations.
Notation: Lower- and upper-case boldface letters stand for
column vectors and matrices, respectively; Calligraphic letters
denote sets with the exception of L which is used for the
likelihood function; R represents the set of real numbers; Z+
denotes the set of positive integers; Probability density and
mass functions are denoted by f (·) and p(·), respectively;
x ∼ N (µ, Σ) represents a Gaussian distributed random
variable x with mean µ and covariance matrix Σ; ˆθ stands for
the estimator (or estimate) of the parameter θ; log denotes the
natural logarithm; iid stands for independent and identically
distributed; (A.) denotes an assumption, for example (A.1)
stands for the ﬁrst assumption; O(1) represents Landau’s term
which tends to a constant as the data size goes to inﬁnity; Ir
stands for an r × r identity matrix; 0r×r denotes an r × r
all zero matrix; #X represents the cardinality of the set X ;
⊤ stands for vector or matrix transpose; |Y | denotes the
determinant of the matrix Y ; Tr(·) represents the trace of
a matrix; ⊗ denotes the Kronecker product; vec(Y ) refers to

2

the staking of the columns of an arbitrary matrix Y into one
long column vector.

II. PROBLEM FORMULATION
Given a set of r-dimensional vectors X , {x1, . . . , xN }, let
{X1, . . . , XK } be a partition of X into K clusters Xk ⊆ X for
k ∈ K , {1, . . . , K}. The subsets (clusters) Xk, k ∈ K, are
independent, mutually exclusive, and non-empty. Let M ,
{MLmin, . . . , MLmax} be a family of candidate models that
represent a partitioning of X into l = Lmin, . . . , Lmax subsets,
where l ∈ Z+. The parameters of each model Ml ∈ M are
denoted by Θl = [θ1, . . . , θl] which lies in a parameter space
Ωl ⊂ Rq×l. Let f (X |Ml, Θl) denote the probability density
function (pdf) of the observation set X given the candidate
model Ml and its associated parameter matrix Θl. Let p(Ml)
be the discrete prior of the model Ml over the set of candidate
models M and let f (Θl|Ml) denote a prior on the parameter
vectors in Θl given Ml ∈ M.
According to Bayes’ theorem, the joint posterior density of
Ml and Θl given the observed data set X is given by
p(Ml)f (Θl|Ml)f (X |Ml, Θl)
f (X )

f (Ml, Θl|X ) =

(1)

,

where f (X ) is the pdf of X . Our objective is to choose the
candidate model M ˆK ∈ M, where ˆK ∈ {Lmin, . . . , Lmax},
which is most probable a posteriori assuming that
(A.1) the true number of clusters (K) in the observed data set

X satisﬁes the constraint Lmin ≤ K ≤ Lmax.

Mathematically, this corresponds to solving

M ˆK = arg max
M

p(Ml|X ),

(2)

where p(Ml|X ) is the posterior probability of Ml given the
observations X . p(Ml|X ) can be written as

p(Ml|X ) =

f (Ml, Θl|X )dΘl

ZΩl

= f (X )−1p(Ml)

f (Θl|Ml)L(Θl|X )dΘl,

(3)

ZΩl
where L(Θl|X ) , f (X |Ml, Θl) is the likelihood function.
M ˆK can also be determined via

arg max
M

log p(Ml|X )

(4)

instead of Eq. (2) since log is a monotonic function. Hence,
taking the logarithm of Eq. (3) results in

ZΩl

log p(Ml|X ) = log p(Ml)+log

f (Θl|Ml)L(Θl|X )dΘl +ρ,
(5)
where − log f (X ) is replaced by ρ (a constant) since it is
not a function of Ml ∈ M and thus has no effect on the
maximization of log p(Ml|X ) over M. Since the partitions
(clusters) Xm ⊆ X , m = 1, . . . , l, are independent, mutually
exclusive, and non-empty, f (Θl|Ml) and L(Θl|X ) can be
written as

f (Θl|Ml) =

f (θm|Ml)

(6)

l

m=1
Y

log p(Ml|X )= log p(Ml)+

log

f (θm|Ml)L(θm|Xm)dθm
Rq

+ρ.

m=1
X

Z

where ˜θm , θm − ˆθm, m = 1, . . . , l. The ﬁrst derivative of
log L(θm|Xm) evaluated at ˆθm vanishes because of assump-
tion (A.3). With

(8)

L(Θl|X ) =

L(θm|Xm).

(7)

Substituting Eqs. (6) and (7) into Eq. (5) results in

l

m=1
Y

l

Maximizing log p(Ml|X ) over all candidate models Ml ∈ M
involves the computation of the logarithm of a multidimen-
sional integral. Unfortunately, the solution of the multidimen-
sional integral does not possess a closed analytical form for
most practical cases. This problem can be solved using either
numerical integration or approximations that allow a closed-
form solution. In the context of model selection, closed-form
approximations are known to provide more insight into the
problem than numerical integration [15]. Following this line
of argument, we use Laplace’s method of integration [15],
[46], [48] and provide an asymptotic approximation to the
multidimensional integral in Eq. (8).

III. PROPOSED BAYESIAN CLUSTER ENUMERATION
CRITERION

In this section, we derive a general BIC expression for
cluster analysis, which we call BICG(·). Under some mild
is
assumptions, we provide a closed-form expression that
applicable to a broad class of data distributions.
In order to provide a closed-form analytic approximation to
Eq. (8), we begin by approximating the multidimensional inte-
gral using Laplace’s method of integration. Laplace’s method
of integration makes the following assumptions.
(A.2) log L(θm|Xm) with m = 1, . . . , l has ﬁrst- and second-
order derivatives which are continuous over the param-
eter space Ωl.

(A.3) log L(θm|Xm) with m = 1, . . . , l has a global maximum

at ˆθm, where ˆθm is an interior point of Ωl.

(A.4) f (θm|Ml) with m = 1, . . . , l is continuously differen-
tiable and its ﬁrst-order derivatives are bounded on Ωl.
of
the

Hessian matrix

(A.5) The

of

negative
log L(θm|Xm)

1
Nm

∈ Rq×q

(9)

ˆHm , −

1
Nm

d2 log L(θm|Xm)
dθmdθ⊤
m

θm= ˆθm

(cid:12)
(cid:12)
(cid:12)
is positive deﬁnite, where Nm is the cardinality of Xm
(cid:12)
> ǫ for s =
(Nm = #Xm). That is, mins,m λs

ˆHm
(cid:16)
1, . . . , q and m = 1, . . . , l, where λs
eigenvalue of ˆHm and ǫ is a small positive constant.

ˆHm
(cid:17)
(cid:16)

is the sth

(cid:17)

where

The ﬁrst step in Laplace’s method of integration is to write
the Taylor series expansion of f (θm|Ml) and log L(θm|Xm)
around ˆθm, m = 1, . . . , l. We begin by approximating
log L(θm|Xm) by its second-order Taylor series expansion
around ˆθm as follows:

log L(θm|Xm) ≈ log L( ˆθm|Xm)+ ˜θ⊤
m

d log L(θm|Xm)
dθm

θm= ˆθm

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

1
2

˜θ⊤
m

"
= log L( ˆθm|Xm) −

d2 log L(θm|Xm)
dθmdθ⊤
m
Nm
2

θm= ˆθm#
(cid:12)
(cid:12)
(cid:12)
ˆHm ˜θm,
˜θ⊤
(cid:12)
m

˜θm

3

(10)

U ,

Rq

Z

f (θm|Ml) exp (log L(θm|Xm)) dθm,

(11)

substituting Eq.
f (θm|Ml) by its Taylor series expansion yields

into Eq.

(10)

(11) and approximating

+ HOT

U ≈

f ( ˆθm|Ml) + ˜θ⊤
m

df (θm|Ml)
dθm

Rq (cid:18)(cid:20)

Z

× L( ˆθm|Xm) exp

−

θm= ˆθm

(cid:21)

˜θ⊤
m

Nm
2

ˆHm ˜θm

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:19)
and
order
is a Gaussian kernel with mean

,
(cid:19)
terms

higher

dθm

(12)

(cid:18)

(cid:16)

denotes

− Nm
2

ˆHm ˜θm

where
exp

HOT
˜θ⊤
m
ˆθm and covariance matrix
. The second term
in the ﬁrst line of Eq. (12) vanishes because it simpliﬁes to
(cid:17)
κ E
= 0, where κ is a constant (see [48, p. 53]
for more details). Consequently, Eq. (12) reduces to

Nm ˆHm
(cid:16)

θm − ˆθm
h

−1

(cid:17)

i

˜θ⊤
m

ˆHm ˜θm

dθm

(cid:19)
1/2

N −1
m

ˆH −1
m

(cid:12)
(cid:12)
ˆHm ˜θm
˜θ⊤
(cid:12)
m

(cid:12)
(cid:12)
dθm
(cid:12)

U ≈ f ( ˆθm|Ml)L( ˆθm|Xm)

Z
= f ( ˆθm|Ml)L( ˆθm|Xm)

exp

−

Rq

Nm
2

(cid:18)
(2π)q/2

Rq (cid:18)
Z
−
1/2 exp

Nm
2

(cid:18)

1

(cid:12)
(cid:12)
(cid:12)

(2π)q/2

N −1

m ˆH −1
m

= f ( ˆθm|Ml)L( ˆθm|Xm)(2π)q/2

N −1
m

ˆH −1
m

(cid:12)
(cid:12)
(cid:12)

where | · | stands for the determinant, given that Nm → ∞.
Using Eq. (13), we are thus able to provide an asymptotic
approximation to the multidimensional integral in Eq. (8).
Now, substituting Eq. (13) into Eq. (8), we arrive at

(cid:12)
(cid:12)
(cid:12)

(cid:19)

(13)

(cid:19)

,

1/2

(cid:12)
(cid:12)
(cid:12)

log p(Ml|X ) ≈ log p(Ml) +

log

l

m=1
X
l
1
2

f ( ˆθm|Ml)L( ˆθm|Xm)
(cid:17)

(cid:16)

+ ρ,

(14)

+

log 2π −

lq
2

log

m=1
X

ˆJm
(cid:12)
(cid:12)
(cid:12)
d2 log L(θm|Xm)
dθmdθ⊤
m

(cid:12)
(cid:12)
(cid:12)

ˆJm , Nm ˆHm = −

∈ Rq×q

(15)

θm= ˆθm

(cid:12)
(cid:12)
(cid:12)
(cid:12)

is the Fisher Information Matrix (FIM) of data from the mth
partition.
In the derivation of log p(Ml|X ), so far, we have made no
distributional assumption on the data set X except that the
log-likelihood function log L(θm|Xm) and the prior on the
parameter vectors f (θm|Ml), for m = 1, . . . , l, should satisfy
some mild conditions under each model Ml ∈ M. Hence,
Eq. (14) is a general expression of the posterior probability of

the model Ml given X for a general class of data distributions
that satisfy assumptions (A.2)-(A.5). The BIC is concerned
with the computation of the posterior probability of candidate
models and thus Eq. (14) can also be written as

BICG(Ml) , log p(Ml|X )

≈ log p(Ml) + log f ( ˆΘl|Ml) + log L( ˆΘl|X )

+

log 2π −

log

+ ρ.

(16)

lq
2

l

1
2

m=1
X

ˆJm
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

After calculating BICG(Ml) for each candidate model Ml ∈
M, the number of clusters in X is estimated as

ˆKBICG = arg max

BICG(Ml).

l=Lmin,...,Lmax

(17)

However, calculating BICG(Ml) using Eq. (16) is a compu-
tationally expensive task as it requires the estimation of the
FIM, ˆJm, for each cluster m = 1, . . . , l in the candidate model
Ml ∈ M. Our objective is to ﬁnd an asymptotic approximation
, m = 1, . . . , l, in order to simplify the com-
for log
putation of BICG(Ml). We solve this problem by imposing
speciﬁc assumptions on the distribution of the data set X .
In the next section, we provide an asymptotic approximation
, m = 1, . . . , l, assuming that Xm contains iid
for log
multivariate Gaussian data points.

ˆJm
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

ˆJm
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

IV. PROPOSED BAYESIAN CLUSTER ENUMERATION
ALGORITHM FOR MULTIVARIATE GAUSSIAN DATA

We propose a two-step approach to estimate the number of
partitions (clusters) in X and provide an estimate of cluster pa-
rameters, such as cluster centroids and covariance matrices, in
an unsupervised learning framework. The proposed approach
consists of a model-based clustering algorithm, which clusters
the data set X according to each candidate model Ml ∈ M,
and a Bayesian cluster enumeration criterion that selects the
model which is a posteriori most probable.

A. Proposed Bayesian Cluster Enumeration Criterion for Mul-
tivariate Gaussian Data

Let X , {x1, . . . , xN } denote the observed data set which
can be partitioned into K clusters {X1, . . . , XK }. Each cluster
Xk, k ∈ K, contains Nk data vectors that are realizations
of iid Gaussian random variables xk ∼ N (µk, Σk), where
µk ∈ Rr×1 and Σk ∈ Rr×r represent the centroid and the
covariance matrix of the kth cluster, respectively. Further,
let M , {MLmin, . . . , MLmax} denote a set of Gaussian
candidate models and let there be a clustering algorithm that
partitions X into l independent, mutually exclusive, and non-
empty subsets (clusters) Xm, m = 1, . . . , l, by providing
parameter estimates ˆθm = [ ˆµm, ˆΣm]⊤ for each candidate
model Ml ∈ M, where l = Lmin, . . . , Lmax and l ∈ Z+.
Assume that (A.1)-(A.7) are satisﬁed.

Theorem 1. The posterior probability of Ml ∈ M given X
can be asymptotically approximated as

BICN(Ml) , log p(Ml|X )

4

Nm log Nm −

l

Nm
2

m=1
X

log

ˆΣm

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

log Nm,

(18)

≈

−

l

m=1
X
l
q
2

m=1
X

where Nm is the cardinality of the subset Xm and it sat-
l
m=1 log Nm sums the
isﬁes N =
logarithms of the number of data vectors in each cluster
m = 1, . . . , l.

l
m=1 Nm. The term

P

P

Proof. Proving Theorem 1 requires ﬁnding an asymptotic
approximation to log
in Eq. (16) and, based on this ap-
proximation, deriving an expression for BICN(Ml). A detailed
(cid:4)
proof is given in Appendix A.

ˆJm

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

Once the Bayesian Information Criterion, BICN(Ml), is com-
puted for each candidate model Ml ∈ M, the number of
partitions (clusters) in X is estimated as

ˆKBICN = arg max

BICN(Ml).

l=Lmin,...,Lmax

(19)

Remark. The proposed criterion, BICN, and the original BIC
as derived in [8], [16] differ in terms of their penalty terms.
A detailed discussion is provided in Section VI.

The ﬁrst step in calculating BICN(Ml) for each model Ml ∈
M is the partitioning of the data set X into l clusters
Xm, m = 1, . . . , l, and the estimation of the associated cluster
parameters using an unsupervised learning algorithm. Since
the approximations in BICN(Ml) are based on maximizing the
likelihood function of Gaussian distributed random variables,
we use a clustering algorithm that is based on the maximum
likelihood principle. Accordingly, a natural choice is the EM
algorithm for Gaussian mixture models.

B. The Expectation-Maximization (EM) Algorithm for Gaus-
sian Mixture Models

The EM algorithm ﬁnds maximum likelihood solutions for
models with latent variables [49]. In our case,
the latent
variables are the cluster memberships of the data vectors in
X , given that the l-component Gaussian mixture distribution
of a data vector xn can be written as

l

f (xn|Ml, Θl) =

τmg(xn; µm, Σm),

(20)

m=1
X
where g(xn; µm, Σm) represents the r-variate Gaussian pdf
and τm is the mixing coefﬁcient of the mth cluster. The goal of
the EM algorithm is to maximize the log-likelihood function
of the data set X with respect to the parameters of interest as
follows:

N

l

arg max
Ψl

log L(Ψl|X )=arg max
Ψl

log

τmg(xn; µm, Σm),

n=1
X

m=1
X
(21)
l ] and τl = [τ1, . . . , τl]⊤. Maximizing
where Ψl = [τl, Θ⊤
Eq. (21) with respect to the elements of Ψl results in coupled
equations. The EM algorithm solves these coupled equations
using a two-step iterative procedure. The ﬁrst step (E step)

m )(xn − ˆµ(i)

m )⊤

for m = 1, . . . , l do
Calculate ˆυ(i)

nm using Eq. (22)

5

evaluates ˆυ(i)
nm, which is an estimate of the probability that
data vector xn belongs to the mth cluster at the ith iteration,
for n = 1, . . . , N and m = 1, . . . , l. ˆυ(i)
nm is calculated as
follows:

Algorithm 1 Proposed two-step cluster enumeration approach
Inputs: data set X ; set of candidate models M ,
{MLmin, . . . , MLmax}
for l = Lmin, . . . , Lmax do

ˆυ(i)
nm =

m g(xn; ˆµ(i−1)
ˆτ (i−1)
j=1 ˆτ (i−1)

m , ˆΣ(i−1)
m )
, ˆΣ(i−1)
j

g(xn; ˆµ(i−1)

j

j

l

,

)

(22)

Step 1: Model-based clustering
Step 1.1: The EM algorithm
for m = 1, . . . , l do

m

m

and ˆΣ(i−1)
P

where ˆµ(i−1)
represent the centroid and covariance
matrix estimates, respectively, of the mth cluster at the previ-
ous iteration (i − 1). The second step (M step) re-estimates the
cluster parameters using the current values of ˆυnm as follows:

Initialize µm using K-means++ [50], [51]
ˆΣm = 1
Nm
ˆτm = Nm
N

xn∈Xm(xn − ˆµm)(xn − ˆµm)⊤

ˆµ(i)

m =

ˆΣ(i)

m =

ˆτ (i)
m =

nmxn

N

N

n=1 ˆυ(i)
n=1 ˆυ(i)
n=1 ˆυ(i)
N
P

P

nm
nm(xn − ˆµ(i)
n=1 ˆυ(i)
N

nm

P

P

N

n=1 ˆυ(i)
N

nm

P

(23)

(24)

(25)

The E and M steps are performed iteratively until either the
cluster parameter estimates ˆΨl or the log-likelihood estimate
log L( ˆΨl|X ) converges.
A summary of the estimation of the number of clusters in
an observed data set using the proposed two-step approach
is provided in Algorithm 1. Note that
the computational
complexity of BICN(Ml) is only O(1), which can easily be
ignored during the run-time analysis of the proposed approach.
Hence, since the EM algorithm is run for all candidate models
in M, the computational complexity of the proposed two-step
approach is O(ζN r2 (Lmin + . . . + Lmax)), where ζ is a ﬁxed
stopping threshold of the EM algorithm.

V. EXISTING BIC-BASED CLUSTER ENUMERATION
METHODS

As discussed in Section I, existing cluster enumeration
algorithms that are based on the original BIC use the crite-
rion as it is known from parameter estimation tasks without
questioning its validity on cluster analysis. Nevertheless, since
these criteria have been widely used, we brieﬂy review them
to provide a comparison to the proposed criterion BICN, which
is given by Eq. (18).
The original BIC, as derived in [16], evaluated at a candidate
model Ml ∈ M is written as

(26)

BICO(Ml) = 2 log L( ˆΘl|X ) − ql log N,
where L( ˆΘl|X ) denotes the likelihood function and N = #X .
In Eq. (26), 2 log L( ˆΘl|X ) denotes the data-ﬁdelity term,
while ql log N is the penalty term. Under the assumption
that the observed data is Gaussian distributed, the data-ﬁdelity
terms of BICO and the ones of our proposed criterion, BICN,
are exactly the same. The only deference between the two
is the penalty term. Hence, we use a similar procedure as
in Algorithm 1 to implement the original BIC as a wrapper
around the EM algorithm.
Moreover, the original BIC is commonly used as a wrapper
around K-means by assuming that the data points that belong

P

end for
for i = 1, 2, . . . do

E step:
for n = 1, . . . , N do

end for

end for
M step:
for m = 1, . . . , l do
Determine ˆµ(i)

Exit for loop

end if

end for
Step 1.2: Hard clustering
for n = 1, . . . , N do

for m = 1, . . . , l do






end for

end for
for m = 1, . . . , l do
N
n=1 ιnm

Nm =

m , ˆΣ(i)

m , and ˆτ (i)

m via Eqs. (23)-(25)

for

convergence

of

either

ˆΨ(i)
l

or

end for
Check
|X )

log L( ˆΨ(i)

l
if convergence condition is satisﬁed then

ιnm =

1, m = arg max
j=1,...,l

ˆυ(i)
nj

0, otherwise

end for
Step 2: Calculate BICN(Ml) via Eq. (18)

P

end for
Estimate the number of clusters, ˆKBICN, in X via Eq. (19)

to each cluster are iid as Gaussian and all clusters are spherical
with an identical variance, i.e. Σm = Σj = σ2Ir for m 6= j,
where σ2 is the common variance of the clusters in Ml [29],
[32], [33]. Under these assumptions, the original BIC is given
by

BICOS(Ml) = 2 log L( ˆΘl|X ) − α log N,

(27)

where BICOS(Ml) denotes the original BIC of the candidate
model Ml derived under the assumptions stated above and
α = (rl + 1) is the number of estimated parameters in Ml ∈
M. Ignoring the model independent terms, BICOS(Ml) can be

BICOS(Ml) = 2

Nm log Nm − rN log ˆσ2 − α log N, (28)

written as

where

ˆσ2 =

1
rN

l

m=1
X

l

m=1
X

xn∈Xm
X

(xn − ˆµm)⊤ (xn − ˆµm)

(29)

is the maximum likelihood estimator of the common variance.
In our experiments, we implement BICOS as a wrapper around
the K-means++ algorithm [51]. The implementation of the
proposed BIC as a wrapper around the K-means++ algorithm
is given by Eq. (37).

VI. COMPARISON OF THE PENALTY TERMS OF DIFFERENT
BAYESIAN CLUSTER ENUMERATION CRITERIA

Comparing Eqs. (18),

(26), and (27), we notice that they

have a common form [11], [46], that is,
2 log L( ˆΘl|X ) − η,

but with different penalty terms, where

l

m=1
X

BICN :

η = q

log Nm

BICO :
BICOS :

η = ql log N
η = (rl + 1) log N.

(30)

(31)

(32)

(33)

Remark. BICO and BICOS carry information about the struc-
ture of the data only on their data-ﬁdelity term, which is the
ﬁrst term in Eq. (30). On the other hand, as shown in Eq. (18),
both the data-ﬁdelity and penalty terms of our proposed
criterion, BICN, contain information about the structure of the
data.

The penalty terms of BICO and BICOS depend linearly on
l, while the penalty term of our proposed criterion, BICN,
depends on l in a non-linear manner. Comparing the penalty
terms in Eqs. (31)-(33), BICOS has the weakest penalty term.
In the asymptotic regime, the penalty terms of BICN and BICO
coincide. But, in the ﬁnite sample regime, for values of l > 1,
the penalty term of BICO is stronger than the penalty term of
BICN. Note that the penalty term of our proposed criterion,
BICN, depends on the number of data vectors in each cluster,
Nm, m = 1, . . . , l, of each candidate model Ml ∈ M, while
the penalty term of the original BIC depends only on the total
number of data vectors in the data set. Hence, the penalty
term of our proposed criterion might exhibit sensitivities to the
initialization of cluster parameters and the associated number
of data vectors per cluster.

VII. EXPERIMENTAL EVALUATION

In this section, we compare the cluster enumeration per-
formance of our proposed criterion, BICN given by Eq. (18),
with the cluster enumeration methods discussed in Section V,
namely BICO and BICOS given by Eqs. (26) and (28), respec-
tively, using synthetic and real data sets. We ﬁrst describe the
performance measures used for comparing the different cluster

6

enumeration criteria. Then, the numerical experiments per-
formed on synthetic data sets and the results obtained from real
data sets are discussed in detail. For all simulations, we assume
that a family of candidate models M , {MLmin, . . . , MLmax}
is given with Lmin = 1 and Lmax = 2K, where K is the
true number of clusters in the data set X . All simulation
results are an average of 1000 Monte Carlo experiments unless
stated otherwise. The compared cluster enumeration criteria
are based on the same initial cluster parameters in each Monte
Carlo experiment, which allows for a fair comparison.
The MATLAB c(cid:13) code that implements the proposed two-
step algorithm and the Bayesian cluster enumeration methods
discussed in Section V is available at:
https://github.com/FreTekle/Bayesian-Cluster-Enumeration

A. Performance Measures

The empirical probability of detection (pdet), the empirical
probability of underestimation (punder), the empirical proba-
bility of selection, and the Mean Absolute Error (MAE) are
used as performance measures. The empirical probability of
detection is deﬁned as the probability with which the correct
number of clusters is selected and it is calculated as

MC

pdet =

1
MC

1{ ˆKi=K},

(34)

i=1
X
where MC is the total number of Monte Carlo experiments,
ˆKi is the estimated number of clusters in the ith Monte Carlo
experiment, and 1{ ˆKi=K} is an indicator function which is
deﬁned as

1{ ˆKi=K}

,

1,
0,

(

ˆKi = K

if
otherwise

.

(35)

The empirical probability of underestimation (punder) is the
probability that ˆK < K. The empirical probability of selection
is deﬁned as the probability with which the number of clusters
speciﬁed by each candidate model in M is selected. The
last performance measure, which is the Mean Absolute Error
(MAE), is computed as

MAE =

1
MC

MC

i=1 (cid:12)
X
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

K − ˆKi

.

(36)

B. Numerical Experiments

1) Simulation Setup: We consider two synthetic data sets,
namely Data-1 and Data-2, in our simulations. Data-1, shown
in Fig. 1a, contains realizations of the random variables
xk ∼ N (µk, Σk), where k = 1, 2, 3, with cluster centroids
µ1 = [2, 3.5]⊤, µ2 = [6, 2.7]⊤, µ3 = [9, 4]⊤, and covariance
matrices

Σ1 =

0.1
0.2
0.1 0.75

(cid:20)

, Σ2 =

(cid:21)

(cid:20)

0.5
0.25

0.25
0.5

, Σ3 =

(cid:21)

1
0.5

(cid:20)

0.5
1

.

(cid:21)

The ﬁrst cluster is linearly separable from the others, while the
remaining clusters are overlapping. The number of data vectors
per cluster is speciﬁed as N1 = γ × 50, N2 = γ × 100, and
N3 = γ × 200, where γ is a constant.

2

 
e
r
u
t
a
e
F

7

6

5

4

3

2

1

6

4

2

0

2
 
e
r
u
t
a
e
F

-2

-4

-5

0

2

10

12

8

4

6
Feature 1
(a) Data-1 for γ = 1

0
Feature 1
(b) Data-2 for Nk = 100

Fig. 1: Synthetic data sets.

5

The second data set, Data-2, contains realizations of the ran-
dom variables xk ∼ N (µk, Σk), where k = 1, . . . , 10, with
cluster centroids µ1 = [0, 0]⊤, µ2 = [3, −2.5], µ3 = [3, 1]⊤,
µ4 = [−1, −3]⊤, µ5 = [−4, 0]⊤, µ6 = [−1, 1]⊤, µ7 =
[−3, 3]⊤, µ8 = [2.5, 4]⊤, µ9 = [−3.5, −2.5], µ10 = [0, 3]⊤,
and covariance matrices

Σ1 =

0.25 −0.15
0.15
−0.15

, Σ2 =

0.5
0

0
0.15
(cid:21)

, Σi =

0.1
0

0
0.1

,

(cid:20)

(cid:21)

(cid:20)
(cid:21)
where i = 3, . . . , 10. As depicted in Fig. 1b, Data-2 contains
eight identical and spherical clusters and two elliptical clusters.
There exists an overlap between two clusters, while the rest
of the clusters are well separated. All clusters in this data set
have the same number of data vectors.

(cid:20)

2) Simulation Results: Data-1 is particularly challenging
for cluster enumeration criteria because it has not only overlap-
ping but also unbalanced clusters. Cluster unbalance refers to
the fact that different clusters have a different number of data
vectors, which might result in some clusters dominating the
others. The impact of cluster overlap and unbalance on pdet and
MAE is displayed in Table I. This table shows pdet and MAE

7

TABLE I: The empirical probability of detection in %, the
empirical probability of underestimation in %, and the Mean
Absolute Error (MAE) of various Bayesian cluster enumera-
tion criteria as a function of γ for Data-1.

γ

pdet(%)

punder(%)

MAE

1

55.2
43.6
53.9

44.5
56.4
0

3

74.3
69.7
50.5

25.7
30.3
0

6

87.4
85.1
49.4

12.6
14.9
0

12

95.7
94.9
42.4

4.3
5.1
0

48

100
100
31.8

0
0
0

0.449
0.564
0.469

0.257
0.303
0.495

0.126
0.149
0.506

0.043
0.051
0.576

0
0
0.682

BICN
BICO
BICOS

BICN
BICO
BICOS

BICN
BICO
BICOS

TABLE II: The empirical probability of detection in %, the
empirical probability of underestimation in %, and the Mean
Absolute Error (MAE) of various Bayesian cluster enumera-
tion criteria as a function of the number of data vectors per
cluster (Nk) for Data-2.

Nk

pdet(%)

punder(%)

MAE

100

56.1
41
2.7

37.6
58.6
0

200

66
57.1
0.9

30.2
41.7
0

500

81
78
0.1

18.2
21.4
0

1000

85.3
84.9
0

13.5
14.1
0

0.452
0.59
1.613

0.341
0.429
1.659

0.19
0.22
1.745

0.148
0.151
1.8

BICN
BICO
BICOS

BICN
BICO
BICOS

BICN
BICO
BICOS

as a function of γ, where γ is allowed to take values from the
set {1, 3, 6, 12, 48}. The cluster enumeration performance of
BICOS is lower than the other methods because it is designed
for spherical clusters with identical variance, while Data-1
has one elliptical and two spherical clusters with different
covariance matrices. Our proposed criterion performs best in
terms of pdet and MAE for all values of γ. As γ increases,
which corresponds to an increase in the number of data vectors
in the data set, the cluster enumeration performance of BICN
and BICO greatly improves, while the performance of BICOS
deteriorates because of the increase in overestimation. The
total criterion (BIC) and penalty term of different Bayesian
cluster enumeration criteria as a function of the number of
clusters speciﬁed by the candidate models for γ = 1 is
depicted in Fig. 2. The BIC plot in Fig. 2a is the result of
one Monte Carlo run. It shows that BICN and BICO have a
maximum at the true number of clusters (K = 3), while
BICOS overestimates the number of clusters to ˆKBICOS = 4.
As shown in Fig. 2b, our proposed criterion, BICN, has the
second strongest penalty term. Note that, the penalty term of
our proposed criterion shows a curvature at the true number
of clusters, while the penalty terms of BICO and BICOS are
uninformative on their own.
Table II shows pdet and MAE as a function of the number
of data vectors per cluster, Nk, k = 1, . . . , 10, where Nk is
allowed to take values from the set {100, 200, 500, 1000}, for

N

O

BIC

BIC

BIC

OS

N

O

BIC

BIC

BIC

OS

C
B

I

3400

3700

3600

3500

3300

3200

3100

)

(
 

m
r
e
t
 
y
t
l
a
n
e
P

200

150

100

50

0

6
1
Number of clusters specified by the candidate models

5

3

4

2

(a) Total criteria

6
1
Number of clusters specified by the candidate models

3

4

5

2

(b) Penalty terms

Fig. 2: The BIC (a) and penalty term (b) of different Bayesian
cluster enumeration criteria for Data-1 when γ = 1.

Data-2. Data-2 contains both spherical and elliptical clusters
and there is an overlap between two clusters, while the rest of
the clusters are well separated. The proposed criterion, BICN,
consistently outperforms the cluster enumeration methods that
are based on the original BIC for the speciﬁed number of
data vectors per cluster (Nk). BICO tends to underestimate
the number of clusters to ˆKBICO = 9 when Nk is small, and
it merges the two overlapping clusters. Even though majority
of the clusters are spherical, BICOS rarely ﬁnds the correct
number of clusters.

3) Initialization Strategies for Clustering Algorithms: The
overall performance of the two-step approach presented in
Algorithm 1 depends on how well the clustering algorithm in
the ﬁrst step is able to partition the given data set. Clustering
algorithms such as K-means and EM are known to converge
to a local optimum and exhibit sensitivity to initialization
of cluster parameters. The simplest initialization method is
to randomly select cluster centroids from the set of data

8

points. However, unless the random initializations are repeated
sufﬁciently many times, the algorithms tend to converge to a
poor local optimum. K-means++ [51] attempts to solve this
problem by providing a systematic initialization to K-means.
One can also use a few runs of K-means++ to initialize the EM
algorithm. An alternative approach to the initialization prob-
lem is to use random swap [52], [53]. Unlike repeated random
initializations, random swap creates random perturbations to
the solutions of K-means and EM in an attempt to move the
clustering result away from an inferior local optimum.
We compare the performance of the proposed criterion and
the original BIC as wrappers around the above discussed
clustering methods using ﬁve synthetic data sets, which in-
clude Data-1 with γ = 6, Data-2 with Nk = 500, and the
ones summarized in Table III. The number of random swaps
is set to 100 and the results are an average of 100 Monte
Carlo experiments. To allow for a fair comparison, the number
of replicates required by the clustering methods that use K-
means++ initialization is set equal to the number of random
swaps.
The empirical probability of detection (pdet) of the proposed
criterion and the original BIC as wrappers around the different
clustering methods is depicted in Table IV, where RSK-means
is the random swap K-means and REM is the random swap
EM. BICNS is the implementation of the proposed BIC as a
wrapper around the K-means variants and is given by

BICNS =

Nm log Nm −

N r
2

log ˆσ2

−

log Nm,

(37)

where α = r + 1 and ˆσ2 is given by Eq. (29). For the data sets
that are mostly spherical, the K-means variants outperform the
ones that are based on EM in terms of the correct estimation
of the number of clusters, while, as expected, EM is superior
for the elliptical data sets. Among the K-means variants, the
gain obtained from using random swap instead of simple K-
means++ is almost negligible. On the other hand, for the EM
variants, EM signiﬁcantly outperforms RSEM especially for
BICN.

TABLE III: Summary of synthetic data sets in terms of their
number of features (r), number of samples (N ), number of
samples per cluster (Nk), and number of clusters (K).

l

m=1
X
l
α
2

m=1
X

Data sets

S3 [54]
A1 [55]
G2-2-40 [56]

r

2
2
2

N

5000
3000
2048

Nk

333
150
1024

K

15
20
2

C. Real Data Results

Although there is no randomness when repeating the ex-
periments for the real data sets, we still use the empirical
probabilities deﬁned in Section VII-A as performance mea-
sures because the cluster enumeration results vary depending
on the initialization of the EM and K-means++ algorithm.

TABLE IV: Empirical probability of detection in %.

K-means++ [51]

RSK-means [52]

EM [49]

RSEM [53]

BICNS
BICOS

BICNS
BICOS

BICN
BICO

BICN
BICO

49
48

49
48

87
85

22
85

Data-1

Data-2

G2-2-40

S3

100
100

100
100

10
10

11
9

A1

98
98

100
100

98
98

16
28

0
0

0
0

92
89

68
89

100
100

100
100

100
100

90
97

1) Iris Data Set: The Iris data set, also called Fisher’s
Iris data set [57], is a 4-dimensional data set collected from
three species of the Iris ﬂower. It contains three clusters of
50 instances each, where each cluster corresponds to one
species of the Iris ﬂower [58]. One cluster is linearly separable
from the other two, while the remaining ones overlap. We
have normalized the data set by dividing the features by their
corresponding mean.
Fig. 3 shows the empirical probability of selection of different
cluster enumeration criteria as a function of the number of
clusters speciﬁed by the candidate models in M. Our proposed
criterion, BICN, is able to estimate the correct number of
clusters (K = 3) 98.8% of the time, while BICO always
underestimates the number of clusters to ˆKBICO = 2. BICOS
completely breaks down and, in most cases, goes for the
speciﬁed maximum number of clusters. Even though two
out of three clusters are not linearly separable, our proposed
criterion is able to estimate the correct number of clusters with
a very high empirical probability of detection. Fig. 4 shows
the behavior of the BIC curves of the proposed criterion, BICN
given by Eq. (18), and the original BIC implemented as a
wrapper around the EM algorithm, BICO given by Eq. (26),
for one Monte Carlo experiment. From Eq. (30), we know
that the data-ﬁdelity terms of both criteria are the same and
this can be seen in Fig. 4a. But, their penalty terms are quite
different, see Fig. 4b. Due to the difference in the penalty
terms of BICN and BICO, we observe a different BIC curve
in Fig. 4c. The total criterion (BIC) curve of BICN has a
maximum at the true number of clusters (K = 3), while
BICO has a maximum at ˆKBICO = 2. Observe that, again, the
penalty term of our proposed criterion, BICN, has a curvature
at the true number of clusters K = 3. Just as in the simulated
data experiments, the penalty term of our proposed criterion
gives valuable information about the true number of clusters
in the data set while the penalty terms of the other methods
are uninformative on their own.

2) Multi-Object Multi-Camera Network Application: The
multi-object multi-camera network application [44], [45] de-
picted in Fig. 5 contains seven cameras that actively monitor
a common scene of interest from different view points. There
are six cars that enter and leave the scene of interest at
different time frames. The video captured by each camera in
the network is 18 seconds long and 550 frames are captured
by each camera. Our objective is to estimate the total number
of cars observed by the camera network. This multi-object
multi-camera network example is a challenging scenario for

9

BIC

N

BIC

O

%
 
n
i
 
n
o
i
t
c
e
l
e
s
 
f
o
 
y
t
i
l
i
b
a
b
o
r
p
 
l
a
c
i
r
i
p
m
E

100

50

0

100

50

0

100

50

0

1

2

3

4

5

6

1

2

3

4

5

6

BIC

OS

1

2

3

4

5

6

Number of clusters specified by the candidate models

Fig. 3: Empirical probability of selection of our proposed
criterion, BICN, and existing Bayesian cluster enumeration
criteria for the Iris data set.

cluster enumeration in the sense that each camera monitors the
scene from different angles, which can result in differences in
the extracted feature vectors (descriptors) of the same object.
Furthermore, as shown in Fig. 5, the video that is captured by
the cameras has a low resolution.
We consider a centralized network structure where the spa-
tially distributed cameras send feature vectors to a central
fusion center for further processing. Hence, each camera
ci ∈ C , {c1, . . . , c7} ﬁrst extracts the objects of interest, cars
in this case, from the frames in the video using a Gaussian
mixture model-based foreground detector. Then, SURF [59]
and color features are extracted from the cars. A standard
MATLAB c(cid:13) implementation of SURF is used to generate a
64-dimensional feature vector for each detected object. Addi-
tionally, a 10 bin histogram for each of the RGB color channels
is extracted, resulting in a 30-dimensional color feature vector.
In our simulations, we apply Principal Component Analysis
(PCA) to reduce the dimension of the color features to 15.
Each camera ci ∈ C stores its feature vectors in Xci. Finally,
the feature vectors extracted by each camera, Xci , are sent to
the fusion center. At the fusion center, we have the total set
of feature vectors X , {Xc1, . . . , Xc7} ⊂ R79×5213 based on
which the cluster enumeration is performed.
The empirical probability of selection for different Bayesian
cluster enumeration criteria as a function of the number of
clusters speciﬁed by the candidate models in M is displayed
in Fig. 6. Even though there are six cars in the scene of
interest, two cars have similar colors. Our proposed criterion,
BICN, ﬁnds six clusters only 14.7% of the time, while the
other cluster enumeration criteria are unable to ﬁnd the correct
number of clusters (cars). BICN ﬁnds ﬁve clusters majority of
the time. This is very reasonable due to the color similarity
of the two cars, which results in the merging of their clusters.
The original BIC, BICO, also ﬁnds ﬁve clusters majority of the
time. But, it also tends to underestimate the number of clusters
even more by detecting only four clusters. Hence, our proposed

10

BIC

BIC

N

O

BIC

BIC

N

O

m
r
e
t
 

y
t
i
l
e
d
i
f
-
a
t
a
D

1900

1850

1800

1750

1700

1650

1600

)

(
 

m
r
e
t
 
y
t
l
a
n
e
P

450

400

350

300

250

200

150

100

50

3550

3500

3450

I

C
B

3400

3350

3300

3250

3200

1
6
Number of clusters specified by the candidate models

2

3

4

5

(a) Data-ﬁdelity terms

c1

c2

c3

c4

c7

c5

c6

1
6
Number of clusters specified by the candidate models

2

3

4

5

(b) Penalty terms

Fig. 5: A wireless camera network continuously observing a
common scene of interest. The top image depicts a camera
network with 7 spatially distributed cameras that actively
monitor the scene from different viewpoints. The bottom left
and right images show a frame captured by cameras 1 and 7,
respectively, at the same time instant.

BIC

N

BIC

O

1

2

3

4

5

6

7

8

9

10

11

12

1

2

3

4

5

6

7

8

9

10

11

12

BIC

OS

%
 
n
i
 
n
o
i
t
c
e
l
e
s
 
f
o
 
y
t
i
l
i
b
a
b
o
r
p
 
l
a
c
i
r
i
p
m
E

100

50

0

100

50

0

100

50

0

BIC

BIC

N

O

6
1
Number of clusters specified by the candidate models

2

3

4

5

(c) Total criteria

Fig. 4: The data-ﬁdelity term (a) penalty term (b) and the BIC
(c) of the proposed criterion, BICN, and BICO for the Iris data
set.

cluster enumeration criterion outperforms existing BIC-based
methods in terms of MAE as shown in Table V, which
summarizes the performance of different Bayesian cluster
enumeration criteria on the real data sets.

3) Seeds Data Set: The Seeds data set is a 7 dimensional
data set which contains measurements of geometric properties

1

2

3

4

5

6

7

8

9

10

11

12

Number of clusters specified by the candidate models

Fig. 6: Empirical probability of selection of our proposed
criterion, BICN, and existing Bayesian cluster enumeration
criteria for the multi-object multi-camera network application.

of kernels belonging to three different varieties of wheat,
where each variety is represented by 70 instances [58].
As can be seen in Fig. 7 the proposed criterion, BICN, and
BICO are able to estimate the correct number of clusters 100%
of the time while BICOS overestimates the number of clusters
to ˆKBICOS = 6.
In cases where either the maximum found from the BIC curve
is very near to the maximum number of clusters speciﬁed

TABLE V: Comparison of cluster enumeration performance
of different Bayesian criteria for the real data sets. The
performance metrics are the empirical probability of detection
in %, the empirical probability of underestimation in %, and
the Mean Absolute Error (MAE).

pdet(%)

punder(%)

MAE

Iris

98.8
0
0

0
100
0

Cars

14.7
0
0

85
100
0

0.024
1
2.674

0.853
1.012
6

Seeds

100
100
0

0
0
0

0
0
3

BICN
BICO
BICOS

BICN
BICO
BICOS

BICN
BICO
BICOS

by the candidate models or no clear maximum can be found,
different post-processing steps that attempt to ﬁnd a signiﬁcant
curvature in the BIC curve have been proposed in the literature.
One such method is the knee point detection strategy [32],
[33]. For the Seeds data set, applying the knee point detection
method to the BIC curve generated by BICOS allows for the
correct estimation of the number of clusters 100% of the time.
Once the number of clusters in an observed data set
is
estimated, the next step is to analyze the overall classiﬁcation
performance of the proposed two-step approach. An evaluation
of the cluster enumeration and classiﬁcation performance of
the proposed algorithm using radar data of human gait can be
found in [60].

11

closed-form BIC expression. Further, we have presented a two-
step cluster enumeration algorithm. The proposed criterion
contains information about the structure of the data in both
its data-ﬁdelity and penalty terms because it is derived by
taking the cluster analysis problem into account. Simulation
results indicate that the penalty term of the proposed criterion
has a curvature point at the true number of clusters which is
created due to the change in the trend of the curve at that point.
Hence, the penalty term of the proposed criterion can contain
information about the true number of clusters in an observed
data set. In contrast, the penalty terms of the existing BIC-
based cluster enumeration methods are uninformative on their
own. In a forthcoming paper, we will alleviate the Gaussian
assumption and consider robust [61], [62] cluster enumeration
in the presence of outliers.

APPENDIX A
PROOF OF THEOREM 1
To obtain an asymptotic approximation of the FIM ˆJm, we
ﬁrst express the log-likelihood of the data points that belong
to the mth cluster as follows:

log L(θm|Xm) = log

p(xn ∈ Xm)f (xn|θm)

Nm
N

(cid:18)

1
2 |Σm|

r

1
2

(2π)

× exp

−

˜x⊤
n

Σ−1

m ˜xn

Nm
N

(cid:19)(cid:19)
r
2

log

−

log 2π −

log |Σm|

(cid:19)

xn∈Xm
Y

log

=

=

−

xn∈Xm
X

1
2

(cid:18)

xn∈Xm(cid:18)
X
1
2

Tr

 

= Nm log

−

Tr

1
2

Σ−1
m

 

= Nm log

Nm
N
Σ−1
m

1
2

Nm
2

Nm
2

˜x⊤
n

Σ−1

m ˜xn

!

xn∈Xm
X
Nm
N

−

rNm
2

˜xn ˜x⊤
n

!

xn∈Xm
X
−

rNm
2

log 2π −

log |Σm|

−

Tr

∆m

1
2
(cid:1)
where ˜xn , xn − µm and ∆m ,
have assumed that
(A.6) the covariance matrix Σm, m = 1, . . . , l, is positive

n . Here, we

˜xn ˜x⊤

xn∈Xm

(38)

P

(cid:0)

,

Then, we take the ﬁrst- and second-order derivatives of
log L(θm|Xm) with respect
to the elements of θm =
[µm, Σm]⊤. To make the paper self contained, we have
included the vector and matrix differentiation rules used in
Eqs. (39)-(41), and (50) in Appendix B (see [63] for de-
tails). A generic expression of the ﬁrst-order derivative of
log L(θm|Xm) with respect to θm is given by

d log L(θm|Xm)
dθm

= −

Nm
2

Tr

Σ−1
m

(cid:18)

dΣm
dθm (cid:19)

1

2

3

4

5

6

log 2π −

log |Σm|

BIC

N

BIC

O

 

%
n
i
 

 

n
o
i
t
c
e
l
e
s
 
f
o
y
t
i
l
i
b
a
b
o
r
p
 
l
a
c
i
r
i
p
m
E

100

50

0

100

50

0

100

50

0

1

2

3

4

5

6

BIC

OS

1

2

3

4

5

6

Number of clusters specified by the candidate models

Fig. 7: Empirical probability of selection of our proposed
criterion, BICN, and existing Bayesian cluster enumeration
criteria for the Seeds data set.

deﬁnite.

VIII. CONCLUSION

We have derived a general expression of the BIC for cluster
analysis which is applicable to a broad class of data distri-
butions. By imposing the multivariate Gaussian assumption
on the distribution of the observations, we have provided a

where ¯xm , 1
xn is the sample mean of the data
Nm
points that belong to the mth cluster and Em , ∆m−NmΣm.
Differentiating Eq. (39) with respect to θ⊤

xn∈Xm

P

m results in

d2 log L(θm|Xm)
dθmdθ⊤
m

+

Tr

Σ−1
m

Σ−1
m

∆m

dΣm
dθm

1
2

1
2

+ Tr

 

=

Tr

+ Nm

(cid:18)
Σ−1
m

(cid:19)

˜xn

dµ⊤
m
dθm !

xn∈Xm
X

dΣm
dθm

Σ−1

m EmΣ−1

m

(cid:19)
m ( ¯xm − µm),

Σ−1

(cid:18)
dµ⊤
m
dθm

(39)

=

Tr

1
2
1
2
1
2

+

Tr

+

Tr

+ Nm

− Nm

=

Nm
2

− Tr

(cid:18)

(cid:18)

(cid:18)

dΣm
dθm
dΣm
dθm
dΣm
dθm

dΣ−1
m
dθ⊤
m

Σ−1

m Em

EmΣ−1
m

(cid:19)

dΣ−1
m
dθ⊤

m (cid:19)

Σ−1
m

dEm
dθ⊤
m

Σ−1
m

(cid:19)

( ¯xm − µm)

(cid:18)
dµ⊤
m
dθm
dµ⊤
m
dθm

Tr

(cid:18)
dΣm
dθm

dΣ−1
m
dθ⊤
m
dµm
dθ⊤
m

Σ−1
m
dΣm
dθm

Σ−1
m
dΣm
dθ⊤
m

dΣm
dθ⊤
m

Σ−1
m

(cid:19)

Σ−1
m

Σ−1
m

∆mΣ−1
m

−Nm Tr

Σ−1

m ( ¯xm−µm)

dΣm
dθm

− Nm

(cid:18)
dµ⊤
m
dθm

Σ−1
m

dµm
dθ⊤
m

.

(cid:19)
Σ−1
m

dµ⊤
m
dθ⊤
m

(cid:19)
(40)

Next, we exploit the symmetry of the covariance matrix Σm
to come up with a ﬁnal expression for the second-order
derivative of log L(θm|Xm). The unique elements of Σm can
be collected into a vector um ∈ R 1
2 r(r+1)×1 as deﬁned in
[63, pp. 56–57]. Hence, incorporating the symmetry of the
covariance matrix Σm and replacing the parameter vector θm
by ˇθm = [µm, um]⊤ in Eq. (40) results in the following
expression:

d2 log L(θm|Xm)
d ˇθmd ˇθ⊤
m

=

Nm
2

vec

⊤

dΣm
d ˇθm (cid:19)

⊤

Vmvec

dΣm
d ˇθ⊤

m (cid:19)

(cid:18)
dΣm
d ˇθ⊤

(cid:18)

− vec

Wmvec

− Nmvec

− Nm

(cid:18)

Zmvec

⊤

m (cid:19)
dΣm
d ˇθm (cid:19)
dµm
Σ−1
m
d ˇθ⊤
m

,

(cid:18)
dµ⊤
m
d ˇθm

(cid:18)
dΣm
d ˇθm (cid:19)
dµ⊤
m
d ˇθ⊤

(cid:18)

m (cid:19)

(41)

where

2

2

×r

Vm , Σ−1
Wm , Σ−1
Zm , Σ−1

m ∈ Rr

m ⊗ Σ−1
m ⊗ Σ−1
m ( ¯xm − µm) ⊗ Σ−1

∆mΣ−1

m

m ∈ Rr

2

×r

2

2

m ∈ Rr

×r.

(42)

(43)

(44)

12

(47)

(48)

(49)

(50)

(51)

(52)

Eq. (41) can be further simpliﬁed into

d2 log L(θm|Xm)
d ˇθmd ˇθ⊤
m

=

−

Nm
2

(cid:18)
dum
du⊤

(cid:18)

− Nm

− Nm

⊤

dum
dum (cid:19)

⊤

D⊤VmD

dum
du⊤
m

D⊤WmD

dum
dum

D⊤Zmvec

⊤

m (cid:19)
dum
dum (cid:19)
Σ−1
m

(cid:18)
dµ⊤
m
dµm

dµm
dµ⊤
m

,

dµ⊤
m
dµ⊤

(cid:18)

m (cid:19)

(45)

2

× 1

where D ∈ Rr
2 r(r+1) denotes the duplication matrix. For
the symmetric matrix Σm, the duplication matrix transforms
um into vec(Σm) using the relation vec(Σm) = Dum [63,
pp. 56–57].
A compact matrix representation of the second-order derivative
of log L(θm|Xm) is given by

d2 log L(θm|Xm)
d ˇθmd ˇθ⊤
m

=

2

∂

2

∂

log L(θm|Xm)
∂µm∂µ⊤
m
log L(θm|Xm)
∂um∂µ⊤
m

2

∂

2

∂

log L(θm|Xm)
∂µm∂u⊤
m
log L(θm|Xm)
∂um∂u⊤
m

.


(46)

The individual elements of the above matrix can be written as





∂2 log L(θm|Xm)
∂µm∂µ⊤
m
∂2 log L(θm|Xm)
∂µm∂u⊤
m
∂2 log L(θm|Xm)
∂um∂µ⊤
m
∂2 log L(θm|Xm)
∂um∂u⊤
m

= −NmΣ−1
m

= −NmZ ⊤

mD

= −NmD⊤Zm

=

Nm
2

D⊤FmD,

∈ Rr

2

2

×r

.

2

Σ−1

m ⊗

Σ−1
m

m − 2
Nm

where Fm , Σ−1
∆mΣ−1
m
The FIM of the mth cluster can be written as
(cid:16)
log L( ˆθm|Xm)
∂µm∂µ⊤
m
log L( ˆθm|Xm)
∂um∂µ⊤
m

log L( ˆθm|Xm)
∂µm∂u⊤
m
log L( ˆθm|Xm)
∂um∂u⊤
m

− ∂
− ∂

Nm ˆΣ−1
m
NmD⊤ ˆZm − Nm
(cid:20)

− ∂
− ∂
Nm ˆZ ⊤
mD
2 D⊤ ˆFmD

ˆJm =

=



(cid:17)

(cid:21)

.

2

2

2





The maximum likelihood estimators of the mean and covari-
ance matrix of the mth Gaussian cluster are given by

ˆµm =

ˆΣm =

1
Nm

1
Nm

xn = ¯xm

x∈Xm
X

(xn − ˆµm) (xn − ˆµm)⊤ .

(53)

Hence, ˆZm , ˆΣ−1
quently, Eq. (51) can be further simpliﬁed to

xn∈Xm
X
m ( ¯xm − ˆµm) ⊗ ˆΣ−1

m = 0r2×r. Conse-

ˆJm =

Nm ˆΣ−1
0
r× 1
2 r(r+1)
m
2 D⊤ ˆFmD#
0 1
2 r(r+1)×r − Nm

.

"

(54)

The determinant of the FIM, ˆJm, can be written as

=

Nm ˆΣ−1
m

×

−

D⊤ ˆFmD

.

(55)

ˆJm
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

Nm
2

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

As N → ∞, Nm → ∞ given that l << N , it follows that

1
Nm

ˆJm

≈ O(1),

(56)

(cid:12)
(cid:12)
where O(1) denotes Landau’s term which tends to a constant
(cid:12)
(cid:12)
as N → ∞. Using this result, we provide an asymptotic
approximation to Eq. (16), in the case where X is composed
of Gaussian distributed data vectors, as follows:

(cid:12)
(cid:12)
(cid:12)
(cid:12)

log p(Ml|X ) ≈ log p(Ml) + log f ( ˆΘl|Ml) + log L( ˆΘl|X )

+

log 2π −

log

Nm

ˆJm

+ ρ

= log p(Ml) + log f ( ˆΘl|Ml) + log L( ˆΘl|X )

1
Nm

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

log 2π −

log Nm

lq
2

lq
2

1
2

l

m=1
X

l

l

m=1
X

1
2

q
2

m=1
X
ˆJm

1
Nm

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

log

+ ρ,

(57)

where q = 1
(A.7) p(Ml) and f ( ˆΘl|Ml) are independent of the data length

2 r(r + 3). Assume that

N .

Then, ignoring the terms in Eq. (57) that do not grow as N →
∞ results in

BICN(Ml) , log p(Ml|X )

≈ log L( ˆΘl|X ) −

log Nm + ρ.

(58)

l

q
2

m=1
X

Since X is composed of multivariate Gaussian distributed data,
BICN(Ml) can be further simpliﬁed as follows:

BICN(Ml) = log L( ˆΘl|X ) + pl
l

Nm log

Nm
N

−

rNm
2

log 2π

−

Tr

Nm ˆΣ−1
m

ˆΣm

+ pl

(cid:17)(cid:19)
log 2π

rN
2

(cid:16)

rN
2

log

ˆΣm

−

+ pl,

=

−

=

−

m=1(cid:18)
X
Nm
2
l

log

ˆΣm
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

1
2

m=1
X
l

m=1
X

Nm
2

pl , −

q
2

(cid:12)
(cid:12)
(cid:12)
l

(cid:12)
(cid:12)
(cid:12)

m=1
X

where

log Nm + ρ.

Finally, ignoring the model independent terms in Eq. (59)
results in Eq. (18) which concludes the proof.

APPENDIX B
VECTOR AND MATRIX DIFFERENTIATION RULES

13

(61)

(62)

(63)

(64)

(65)

(66)

matrix Σ has no special structure, the following vector and
matrix differentiation rules hold.

Σ−1 dΣ
dΣ
(cid:18)

(cid:19)

d
dΣ log |Σ| = Tr
d
dΣ Tr (Σ) = Tr

dΣ
dΣ
(cid:19)
Σ−1 = −Σ−1 dΣ
dΣ

(cid:18)

Σ−1

µ⊤µ = 2µ⊤

d
dΣ
d
dµ

Given three arbitrary symmetric matrices A, B, and Y with
matching dimensions

Tr (AY BY ) = vec(Y )⊤(A ⊗ B)vec(Y )

= u⊤D⊤(A ⊗ B)Du,

where u contains the unique elements of the symmetric matrix
Y and D denotes the duplication matrix of Y . In Eq. (50)
we used the relation vec

dY
du

= D du
du .

(cid:0)

(cid:1)
ACKNOWLEDGMENT

We thank the anonymous reviewers for their insightful
comments and suggestions. Further, we would like to thank Dr.
Benjamin B´ejar Haro for providing us with the multi-object
multi-camera data set which was created as a benchmark with
in the project HANDiCAMS. HANDiCAMS acknowledges
the ﬁnancial support of the Future and Emerging Technologies
(FET) programme within the Seventh Framework Programme
for Research of the European Commission, under FET-Open
grant number: 323944. The work of F. K. Teklehaymanot
is supported by the ‘Excellence Initiative’ of the German
Federal and State Governments and the Graduate School of
Computational Engineering at Technische Universit¨at Darm-
stadt and by the LOEWE initiative (Hessen, Germany) within
the NICER project. The work of M. Muma is supported by
the ‘Athene Young Investigator Programme’ of Technische
Universit¨at Darmstadt.

(59)

(60)

[1] H. Jeffreys, The Theory of Probability (3 ed.). New York, USA: Oxford

University Press, 1961.

[2] H. Akaike, “Fitting autoregressive models for prediction,” Ann. Inst.

Statist. Math., vol. 21, pp. 243–247, 1969.

[3] ——, “Statistical predictor identiﬁcation,” Ann. Inst. Statist. Math.,

vol. 22, pp. 203–217, 1970.

[4] ——, “Information theory and an extension of the maximum likelihood

principle,” in 2nd Int. Symp. Inf. Theory, 1973, pp. 267–281.

[5] D. M. Allen, “The relationship between variable selection and data
augmentation and a method for prediction,” Technometrics, vol. 16,
no. 1, pp. 125–127, Feb. 1974.

[6] M. Stone, “Cross-validatory choice and assessment of statistical predic-

tion,” J. R. Statist. Soc. B, vol. 36, no. 2, pp. 111–133, 1974.

[7] J. Rissanen, “Modeling by shortest data description,” Automatica,

[8] G. Schwarz, “Estimating the dimension of a model,” Ann. Stat., vol. 6,

vol. 14, pp. 465–471, 1978.

no. 2, pp. 461–464, 1978.

Nm log Nm − N log N −

REFERENCES

Here, we describe the vector and matrix differentiation rules
used in this paper (see [63] for details). Let µ ∈ Rr×1 be the
mean and Σ ∈ Rr×r be the covariance matrix of a multivariate
Gaussian random variable x. Assuming that the covariance

[9] E. J. Hannan and B. G. Quinn, “The determination of the order of an
autoregression,” J. R. Statist. Soc. B, vol. 41, no. 2, pp. 190–195, 1979.
[10] R. Shibata, “Asymptotically efﬁcient selection of the order of the model
for estimating parameters of a linear process,” Ann. Stat., vol. 8, no. 1,
pp. 147–164, 1980.

[11] C. R. Rao and Y. Wu, “A strongly consistent procedure for model
selection in a regression problem,” Biometrika, vol. 76, no. 2, pp. 369–
74, 1989.

[12] L. Breiman, “The little bootstrap and other methods for dimensionality
selection in regression: X-ﬁxed prediction error,” J. Am. Stat. Assoc,
vol. 87, no. 419, pp. 738–754, Sept. 1992.

[13] R. E. Kass and A. E. Raftery, “Bayes factors,” J. Am. Stat. Assoc.,

vol. 90, no. 430, pp. 773–795, June 1995.

[14] J. Shao, “Bootstrap model selection,” J. Am. Stat. Assoc., vol. 91, no.

434, pp. 655–665, June 1996.

[15] P. M. Djuri´c, “Asymptotic MAP criteria for model selection,” IEEE
Trans. Signal Process., vol. 46, no. 10, pp. 2726–2735, Oct. 1998.
[16] J. E. Cavanaugh and A. A. Neath, “Generalizing the derivation of the
Schwarz information criterion,” Commun. Statist.-Theory Meth., vol. 28,
no. 1, pp. 49–66, 1999.

[17] A. M. Zoubir, “Bootstrap methods for model selection,” Int. J. Electron.

Commun., vol. 53, no. 6, pp. 386–392, 1999.

[18] A. M. Zoubir and D. R. Iskander, “Bootstrap modeling of a class of
nonstationary signals,” IEEE Trans. Signal Process., vol. 48, no. 2, pp.
399–408, Feb. 2000.

[19] R. F. Brcich, A. M. Zoubir, and P. Pelin, “Detection of sources using
bootstrap techniques,” IEEE Trans. Signal Process., vol. 50, no. 2, pp.
206–215, Feb. 2002.

[20] M. R. Morelande and A. M. Zoubir, “Model selection of random
amplitude polynomial phase signals,” IEEE Trans. Signal Process.,
vol. 50, no. 3, pp. 578–589, Mar. 2002.

[21] D. J. Spiegelhalter, N. G. Best, B. P. Carlin, and A. van der Linde,
“Bayesian measures of model complexity and ﬁt,” J. R. Statist. Soc. B,
vol. 64, no. 4, pp. 583–639, 2002.

[22] G. Claeskens and N. L. Hjort, “The focused information criterion,” J.

Am. Stat. Assoc., vol. 98, no. 464, pp. 900–916, Dec. 2003.

[23] Z. Lu and A. M. Zoubir, “Generalized Bayesian information criterion for
source enumeration in array processing,” IEEE Trans. Signal Process.,
vol. 61, no. 6, pp. 1470–1480, Mar. 2013.

[24] ——, “Flexible detection criterion for source enumeration in array
processing,” IEEE Trans. Signal Process., vol. 61, no. 6, pp. 1303–1314,
Mar. 2013.

[25] ——, “Source enumeration in array processing using a two-step test,”
IEEE Trans. Signal Process., vol. 63, no. 10, pp. 2718–2727, May 2015.
[26] C. R. Rao and Y. Wu, “On model selection,” IMS Lecture Notes -

Monograph Series, pp. 1–57, 2001.

[27] A. Kalogeratos and A. Likas, “Dip-means: an incremental clustering
method for estimating the number of clusters,” in Proc. Adv. Neural Inf.
Process. Syst. 25, 2012, pp. 2402–2410.

[28] G. Hamerly and E. Charles, “Learning the K in K-Means,” in Proc. 16th
Int. Conf. Neural Inf. Process. Syst. (NIPS), Whistler, Canada, 2003, pp.
281–288.

[29] D. Pelleg and A. Moore, “X-means: extending K-means with efﬁcient
estimation of the number of clusters,” in Proc. 17th Int. Conf. Mach.
Learn. (ICML), 2000, pp. 727–734.

[30] M. Shahbaba and S. Beheshti, “Improving X-means clustering with
MNDL,” in Proc. 11th Int. Conf. Inf. Sci., Signal Process. and Appl.
(ISSPA), Montreal, Canada, 2012, pp. 1298–1302.

[31] T. Ishioka, “An expansion of X-means for automatically determining the
optimal number of clusters,” in Proc. 4th IASTED Int. Conf. Comput.
Intell., Calgary, Canada, 2005, pp. 91–96.

[32] Q. Zhao, V. Hautamaki, and P. Fr¨anti, “Knee point detection in BIC for
detecting the number of clusters,” in Proc. 10th Int. Conf. Adv. Concepts
Intell. Vis. Syst. (ACIVS), Juan-les-Pins, France, 2008, pp. 664–673.
[33] Q. Zhao, M. Xu, and P. Fr¨anti, “Knee point detection on Bayesian
information criterion,” in Proc. 20th IEEE Int. Conf. Tools with Artiﬁcial
Intell., Dayton, USA, 2008, pp. 431–438.

[34] Y. Feng and G. Hamerly, “PG-means: learning the number of clusters
in data,” in Proc. Conf. Adv. Neural Inf. Process. Syst. 19 (NIPS), 2006,
pp. 393–400.

[35] C. Constantinopoulos, M. K. Titsias, and A. Likas, “Bayesian feature
and model selection for Gaussian mixture models,” IEEE Trans. Pattern
Anal. Mach. Intell., vol. 28, no. 6, June 2006.

[36] T. Huang, H. Peng, and K. Zhang, “Model selection for Gaussian

mixture models,” Statistica Sinica, vol. 27, no. 1, pp. 147–169, 2017.

[37] C. Fraley and A. Raftery, “How many clusters? Which clustering
method? Answers via model-based cluster analysis,” Comput. J., vol. 41,
no. 8, 1998.

[38] A. Mehrjou, R. Hosseini, and B. N. Araabi, “Improved Bayesian
information criterion for mixture model selection,” Pattern Recognit.
Lett., vol. 69, pp. 22–27, Jan. 2016.

14

[39] A. Dasgupta and A. E. Raftery, “Detecting features in spatial point
processes with clutter via model-based clustering,” J. Am. Stat. Assoc.,
vol. 93, no. 441, pp. 294–302, Mar. 1998.

[40] J. G. Campbell, C. Fraley, F. Murtagh, and A. E. Raftery, “Linear
ﬂaw detection in woven textiles using model-based clustering,” Pattern
Recognit. Lett., vol. 18, pp. 1539–1548, Aug. 1997.

[41] S. Mukherjee, E. D. Feigelson, G. J. Babu, F. Murtagh, C. Fraley, and
A. Raftery, “Three types of Gamma-ray bursts,” Astrophysical J., vol.
508, pp. 314–327, Nov. 1998.

[42] W. J. Krzanowski and Y. T. Lai, “A criterion for determining the number
of groups in a data set using sum-of-squares clustering,” Biometrics,
vol. 44, no. 1, pp. 23–34, Mar. 1988.

[43] R. Tibshirani, G. Walther, and T. Hastie, “Estimating the number of
clusters in a dataset via the gap statistic,” J. R. Statist. Soc. B, vol. 63,
pp. 411–423, 2001.

[44] F. K. Teklehaymanot, M. Muma, J. Liu, and A. M. Zoubir, “In-network
adaptive cluster enumeration for distributed classiﬁcation/labeling,” in
Proc. 24th Eur. Signal Process. Conf. (EUSIPCO), Budapest, Hungary,
2016, pp. 448–452.

[45] P. Binder, M. Muma, and A. M. Zoubir, “Gravitational clustering: a
simple, robust and adaptive approach for distributed networks,” Signal
Process., vol. 149, pp. 36–48, Aug. 2018.

[46] P. Stoica and Y. Selen, “Model-order selection: a review of information
criterion rules,” IEEE Signal Process. Mag., vol. 21, no. 4, pp. 36–47,
July 2004.

[47] R. Xu and D. Wunsch, “Survey of clustering algorithms,” IEEE Trans.

Neural Netw., vol. 16, no. 3, pp. 645–678, May 2005.

[48] T. Ando, Bayesian Model Selection and Statistical Modeling, ser. Statis-
Florida, USA: Taylor and Francis

tics: Textbooks and Monographs.
Group, LLC, 2010.

[49] C. M. Bishop, Pattern Recognition and Machine Learning. New York,

USA: Springer Science+Business Media, LLC, 2006.

[50] J. Bl¨omer and K. Bujna, “Adaptive seeding for Gaussian mixture
models,” in Proc. 20th Paciﬁc Asia Conf. Adv. Knowl. Discovery and
Data Mining (PAKDD), vol. 9652, Auckland, New Zealand, 2016, pp.
296–308.

[51] D. Arthur and S. Vassilvitskii, “K-means++: the advantages of careful
seeding,” in Proc. 18th Annu. ACM-SIAM Symp. Discrete Algorithms,
New Orleans, USA, 2007, pp. 1027–1035.

[52] P. Fr¨anti, “Efﬁciency of random swap clustering,” J. Big Data, vol. 5,

no. 13, 2018.

[53] Q. Zhao, V. Hautam¨aki, I. K¨arkk¨ainen, and P. Fr¨anti, “Random swap
EM algorithm for Gaussian mixture models,” Pattern Recognit. Lett.,
vol. 33, pp. 2120–2126, 2012.

[54] P. Fr¨anti and O. Virmajoki, “Iterative shrinking method for clustering
problems,” Pattern Recognit., vol. 39, no. 5, pp. 761–765, 2006.
[Online]. Available: http://dx.doi.org/10.1016/j.patcog.2005.09.012
[55] I. K¨arkk¨ainen and P. Fr¨anti, “Dynamic local search algorithm for the
clustering problem,” Department of Computer Science, University of
Joensuu, Joensuu, Finland, Tech. Rep. A-2002-6, 2002.

[56] P. Fr¨anti, R. Mariescu-Istodor, and C. Zhong, “Xnn graph,” Joint Int.
Workshop on Structural, Syntactic, and Statist. Pattern Recognit., vol.
LNCS 10029, pp. 207–217, 2016.

[57] R. A. Fisher, “The use of multiple measurements in taxonomic prob-

lems,” Ann. Eugenics, vol. 7, pp. 179–188, 1936.

[58] M. Lichman, “UCI machine learning repository,” 2013.

[Online].

Available: http://archive.ics.uci.edu/ml

[59] H. Bay, A. Ess, T. Tuytelaars, and L. Van Gool, “Speeded-Up Robust
Features (SURF),” Comput. Vis. Image Underst., vol. 110, pp. 346–359,
June 2008.

[60] F. K. Teklehaymanot, A.-K. Seifert, M. Muma, M. G. Amin, and A. M.
Zoubir, “Bayesian target enumeration and labeling using radar data of
human gait,” in 26th Eur. Signal Process. Conf. (EUSIPCO) (accepted),
2018.

[61] A. M. Zoubir, V. Koivunen, Y. Chakhchoukh, and M. Muma, “Robust
estimation in signal processing,” IEEE Signal Process. Mag., vol. 29,
no. 4, pp. 61–80, July 2012.

[62] A. M. Zoubir, V. Koivunen, E. Ollila, and M. Muma, Robust Statistics

for Signal Processing. Cambridge University Press, 2018.

[63] J. R. Magnus and H. Neudecker, Matrix Differential Calculus with
Applications in Statistics and Econometrics (3 ed.), ser. Wiley Series
in Probability and Statistics. Chichester, England: John Wiley & Sons
Ltd, 2007.

