Mapping Instructions to Actions in 3D Environments
with Visual Goal Prediction

Andrew Bennett
Dipendra Misra
Eyvind Niklasson Max Shatkhin

Valts Blukis
Yoav Artzi

Department of Computer Science and Cornell Tech, Cornell University, New York, NY, 10044
{dkm, awbennett, valts, yoav}@cs.cornell.edu
{een7, ms3448}@cornell.edu

9
1
0
2
 
r
a

M
 
8
1
 
 
]
L
C
.
s
c
[
 
 
2
v
6
8
7
0
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

We propose to decompose instruction exe-
cution to goal prediction and action genera-
tion. We design a model that maps raw vi-
sual observations to goals using LINGUNET,
a language-conditioned image generation net-
work, and then generates the actions required
to complete them. Our model
is trained
from demonstration only without external re-
sources. To evaluate our approach, we intro-
duce two benchmarks for instruction follow-
ing: LANI, a navigation task; and CHAI, where
an agent executes household instructions. Our
evaluation demonstrates the advantages of our
model decomposition, and illustrates the chal-
lenges posed by our new benchmarks.

1

Introduction

Executing instructions in interactive environments
requires mapping natural language and observa-
tions to actions. Recent approaches propose learn-
ing to directly map from inputs to actions, for ex-
ample given language and either structured obser-
vations (Mei et al., 2016; Suhr and Artzi, 2018) or
raw visual observations (Misra et al., 2017; Xiong
et al., 2018). Rather than using a combination
of models, these approaches learn a single model
to solve language, perception, and planning chal-
lenges. This reduces the amount of engineering
required and eliminates the need for hand-crafted
meaning representations. At each step, the agent
maps its current inputs to the next action using a
single learned function that is executed repeatedly
until task completion.

Although executing the same computation at
each step simpliﬁes modeling, it exempliﬁes cer-
tain inefﬁciencies; while the agent needs to de-
cide what action to take at each step, identifying
its goal is only required once every several steps
or even once per execution. The left instruction in
Figure 1 illustrates this. The agent can compute its

After reaching the hydrant
head towards
the blue
fence and pass towards the
right side of the well.

Put the cereal, the sponge,
and the dishwashing soap
into the cupboard above
the sink.

Figure 1: Example instructions from our two tasks:
LANI (left) and CHAI (right). LANI is a landmark nav-
igation task, and CHAI is a corpus of instructions in the
CHALET environment.

goal once given the initial observation, and given
this goal can then generate the actions required.
In this paper, we study a new model that explic-
itly distinguishes between goal selection and ac-
tion generation, and introduce two instruction fol-
lowing benchmark tasks to evaluate it.

Our model decomposes into goal prediction and
action generation. Given a natural language in-
struction and system observations, the model pre-
dicts the goal to complete. Given the goal, the
model generates a sequence of actions.

The key challenge we address is designing the
goal representation. We avoid manually designing
a meaning representation, and predict the goal in
the agent’s observation space. Given the image of
the environment the agent observes, we generate a
probability distribution over the image to highlight
the goal location. We treat this prediction as image
generation, and develop LINGUNET, a language
conditioned variant of the U-NET image-to-image
architecture (Ronneberger et al., 2015). Given the
visual goal prediction, we generate actions using a
recurrent neural network (RNN).

Our model decomposition offers two key advan-
tages. First, we can use different learning methods
as appropriate for the goal prediction and action

generation problems. We ﬁnd supervised learning
more effective for goal prediction, where only a
limited amount of natural language data is avail-
able. For action generation, where exploration is
critical, we use policy gradient in a contextual ban-
dit setting (Misra et al., 2017). Second, the goal
distribution is easily interpretable by overlaying it
on the agent observations. This can be used to in-
crease the safety of physical systems by letting the
user verify the goal before any action is executed.
Despite the decomposition, our approach retains
the advantages of the single-model approach.
It
does not require designing intermediate represen-
tations, and training does not rely on external re-
sources, such as pre-trained parsers or object de-
tectors, instead using demonstrations only.

We introduce two new benchmark tasks with
different levels of complexity of goal prediction
and action generation. LANI is a 3D navigation
environment and corpus, where an agent navigates
between landmarks. The corpus includes 6,000
sequences of natural language instructions, each
containing on average 4.7 instructions. CHAI is
a corpus of 1,596 instruction sequences, each in-
cluding 7.7 instructions on average, for CHALET,
a 3D house environment (Yan et al., 2018).
In-
structions combine navigation and simple manipu-
lation, including moving objects and opening con-
tainers. Both tasks require solving language chal-
lenges, including spatial and temporal reasoning,
as well as complex perception and planning prob-
lems. While LANI provides a task where most in-
structions include a single goal, the CHAI instruc-
tions often require multiple intermediate goals.
For example, the household instruction in Fig-
ure 1 can be decomposed to eight goals: opening
the cupboard, picking each item and moving it to
the cupboard, and closing the cupboard. Achiev-
ing each goal requires multiple actions of differ-
ent types, including moving and acting on objects.
This allows us to experiment with a simple varia-
tion of our model to generate intermediate goals.

We compare our approach to multiple recent
methods. Experiments on the LANI navigation
task indicate that decomposing goal prediction
and action generation signiﬁcantly improves in-
struction execution performance. While we ob-
serve similar trends on the CHAI instructions, re-
sults are overall weaker, illustrating the complex-
ity of the task. We also observe that inherent
ambiguities in instruction following make exact

goal identiﬁcation difﬁcult, as demonstrated by
imperfect human performance. However, the gap
to human-level performance still remains large
across both tasks. Our code and data are available
at github.com/clic-lab/ciff.

2 Technical Overview

Task Let X be the set of all instructions, S the
set of all world states, and A the set of all actions.
An instruction ¯x ∈ X is a sequence (cid:104)x1, . . . , xn(cid:105),
where each xi is a token. The agent executes
instructions by generating a sequence of actions,
and indicates execution completion with the spe-
cial action STOP.

The sets of actions A and states S are domain
speciﬁc. In the navigation domain LANI, the ac-
tions include moving the agent and changing its
orientation. The state information includes the po-
sition and orientation of the agent and the differ-
ent landmarks. The agent actions in the CHALET
house environment include moving and changing
the agent orientation, as well as an object interac-
tion action. The state encodes the position and ori-
entation of the agent and all objects in the house.
For interactive objects, the state also includes their
status, for example if a drawer is open or closed.
In both domains, the actions are discrete. The do-
mains are described in Section 6.
Model The agent does not observe the world
state directly, but instead observes its pose and an
RGB image of the environment from its point of
view. We deﬁne these observations as the agent
context ˜s. An agent model is a function from an
agent context ˜s to an action a ∈ A. We model
goal prediction as predicting a probability distri-
bution over the agent visual observations, repre-
senting the likelihood of locations or objects in the
environment being target positions or objects to be
acted on. Our model is described in Section 4.
Learning We assume access to training data
i=1, where ¯x(i)
with N examples {(¯x(i), s
(i)
(i)
is the
is an instruction, s
g
1
goal state. We decompose learning; training goal
prediction using supervised learning, and action
generation using oracle goals with policy gradient
in a contextual bandit setting. We assume an in-
strumented environment with access to the world
state, which is used to compute rewards during
training only. Learning is described in Section 5.
Evaluation We evaluate task performance on a
(i)
i=1, where ¯x(i) is an in-
g )}M
test set {(¯x(i), s

is a start state, and s

(i)
g )}N

(i)
1 , s

(i)
1 , s

(i)
1

struction, s
is the goal
is a start state, and s
state. We evaluate task completion accuracy and
the distance of the agent’s ﬁnal state to s

(i)
g .

(i)
g

3 Related Work

Mapping instruction to action has been studied
extensively with intermediate symbolic represen-
tations (e.g., Chen and Mooney, 2011; Kim and
Mooney, 2012; Artzi and Zettlemoyer, 2013; Artzi
et al., 2014; Misra et al., 2015, 2016). Recently,
there has been growing interest in direct mapping
from raw visual observations to actions (Misra
et al., 2017; Xiong et al., 2018; Anderson et al.,
2018; Fried et al., 2018). We propose a model that
enjoys the beneﬁts of such direct mapping, but ex-
plicitly decomposes that task to interpretable goal
prediction and action generation. While we focus
on natural language, the problem has also been
studied using synthetic language (Chaplot et al.,
2018; Hermann et al., 2017).

Our model design is related to hierarchical re-
inforcement learning, where sub-policies at differ-
ent levels of the hierarchy are used at different fre-
quencies (Sutton et al., 1998). Oh et al. (2017)
uses a two-level hierarchy for mapping synthetic
language to actions. Unlike our visual goal rep-
resentation, they use an opaque vector representa-
tion. Also, instead of reinforcement learning, our
methods emphasize sample efﬁciency.

Goal prediction is related to referring expres-
sion interpretation (Matuszek et al., 2012a; Krish-
namurthy and Kollar, 2013; Kazemzadeh et al.,
2014; Kong et al., 2014; Yu et al., 2016; Mao et al.,
2016; Kitaev and Klein, 2017). While our model
solves a similar problem for goal prediction, we
focus on detecting visual goals for actions, includ-
ing both navigation and manipulation, as part of
an instruction following model. Using formal goal
representation for instruction following was stud-
ied by MacGlashan et al. (2015). In contrast, our
model generates a probability distribution over im-
ages, and does not require an ontology.

Our data collection is related to existing work.
LANI is inspired by the HCRC Map Task (An-
derson et al., 1991), where a leader directs a fol-
lower to navigate between landmarks on a map.
We use a similar task, but our scalable data collec-
tion process allows for a signiﬁcantly larger cor-
pus. We also provide an interactive navigation
environment, instead of only map diagrams. Un-
like Map Task, our leaders and followers do not
interact in real time. This abstracts away inter-

action challenges, similar to how the SAIL nav-
igation corpus was collected (MacMahon et al.,
2006). CHAI instructions were collected using
scenarios given to workers, similar to the ATIS
collection process (Hemphill et al., 1990; Dahl
et al., 1994). Recently, multiple 3D research envi-
ronments were released. LANI has a signiﬁcantly
larger state space than existing navigation envi-
ronments (Hermann et al., 2017; Chaplot et al.,
2018), and CHALET, the environment used for
CHAI, is larger and has more complex manipu-
lation compared to similar environments (Gordon
et al., 2018; Das et al., 2018). In addition, only
synthetic language data has been released for these
environment. An exception is the Room-to-Room
dataset (Anderson et al., 2018) that makes use of
an environment of connected panoramas of house
settings. Although it provides a realistic vision
challenge, unlike our environments, the state space
is limited to a small number of panoramas and ma-
nipulation is not possible.

4 Model

We model the agent policy as a neural network.
The agent observes the world state st at time t as
an RGB image It. The agent context ˜st, the infor-
mation available to the agent to select the next ac-
tion at, is a tuple (¯x, IP , (cid:104)(I1, p1), . . . , (It, pt)(cid:105)),
where ¯x is the natural
language instructions,
IP is a panoramic view of the environment
from the starting position at time t = 1, and
(cid:104)(I1, p1), . . . , (It, pt)(cid:105) is the sequence of observa-
tions It and poses pt up to time t. The panorama
IP is generated through deterministic exploration
by rotating 360◦ to observe the environment at the
beginning of the execution.1

The model includes two main components: goal
prediction and action generation. The agent uses
the panorama IP to predict the goal location lg. At
each time step t, a projection of the goal location
into the agent’s current view Mt is given as input
to an RNN to generate actions. The probability of
an action at at time t decomposes to:
(cid:16)

P (at | ˜st) =

P (lg | ¯x, IP )

(cid:88)

lg

P (at | lg, (I1, p1), . . . , (It, pt))

,

(cid:17)

where the ﬁrst term puts the complete distribution
mass on a single location (i.e., a delta function).
Figure 2 illustrates the model.

1The panorama is a concatenation of deterministic obser-
vations along the width dimension. For simplicity, we do not
include these deterministic steps in the execution.

Figure 2: An illustration for our architecture (Section 4) for the instruction turn left and go to the red oil drum
with a LINGUNET depth of m = 4. The instruction ¯x is mapped to ¯x with an RNN, and the initial panorama
observation IP to F0 with a CNN. LINGUNET generates H1, a visual representation of the goal. First, a sequence
of convolutions maps the image features F0 to feature maps F1,. . . ,F4. The text representation ¯x is used to
generate the kernels K1,. . . ,K4, which are convolved to generate the text-conditioned feature maps G1,. . . ,G4.
These feature maps are de-convolved to H1,. . . ,H4. The goal probability distribution Pg is computed from H1.
The goal location is the inferred from the max of Pg. Given lg and pt, the pose at step t, the goal mask Mt is
computed and passed into an RNN that outputs the action to execute.

Goal Prediction To predict the goal location,
we generate a probability distribution Pg over
a feature map F0 generated using convolutions
from the initial panorama observation IP . Each
element in the probability distribution Pg corre-
sponds to an area in IP . Given the instruction
¯x and panorama IP , we ﬁrst generate their rep-
resentations. From the panorama IP , we gener-
ate a feature map F0 = [CNN0(IP ); Fp], where
CNN0 is a two-layer convolutional neural net-
work (CNN; LeCun et al., 1998) with rectiﬁed
linear units (ReLU; Nair and Hinton, 2010) and
Fp are positional embeddings.2 The concatena-
tion is along the channel dimension. The instruc-
tion ¯x = (cid:104)x1, · · · xn(cid:105) is mapped to a sequence
of hidden states li = LSTMx(ψx(xi), li−1), i =
1, . . . , n using a learned embedding function ψx
and a long short-term memory (LSTM; Hochre-
iter and Schmidhuber, 1997) RNN LSTMx. The
instruction representation is ¯x = ln.

We generate the probability distribution Pg over
pixels in F0 using LINGUNET. The architecture
of LINGUNET is inspired by the U-NET image
generation method (Ronneberger et al., 2015), ex-
cept that the reconstruction phase is conditioned
on the natural language instruction. LINGUNET
ﬁrst applies m convolutional layers to generate a
sequence of feature maps Fj = CNNj(Fj−1),

2We generate Fp by creating a channel for each determin-
istic observation used to create the panorama, and setting all
the pixels corresponding to that observation location in the
panorama to 1 and all others to 0. The number of observa-
tions depends on the agent’s camera angle.

j = 1 . . . m, where each CNNj is a convolutional
layer with leaky ReLU non-linearities (Maas et al.,
2013) and instance normalization (Ulyanov et al.,
2016). The instruction representation ¯x is split
evenly into m vectors {¯xj}m
j=1, each is used to
create a 1 × 1 kernel Kj = AFFINEj(¯xj), where
each AFFINEj is an afﬁne transformation followed
by normalizing and reshaping. For each Fj, we
apply a 2D 1 × 1 convolution using the text ker-
nel Kj to generate a text-conditioned feature map
Gj = CONVOLVE(Kj, Fj), where CONVOLVE
convolves the kernel over the feature map. We
then perform m deconvolutions to generate a se-
quence of feature maps Hm,. . . ,H1:

Hm = DECONVm(DROPOUT(Gm))
Hj = DECONVj([Hj+1; Gj]) .

DROPOUT is dropout regularization (Srivastava
et al., 2014) and each DECONVj
is a decon-
volution operation followed a leaky ReLU non-
linearity and instance norm.3 Finally, we gener-
ate Pg by applying a softmax to H1 and an ad-
ditional learned scalar bias term bg to represent
events where the goal is out of sight. For example,
when the agent already stands in the goal position
and therefore the panorama does not show it.

We use Pg to predict the goal position in the
environment. We ﬁrst select the goal pixel in F0 as
the pixel corresponding to the highest probability
element in Pg. We then identify the corresponding
3D location lg in the environment using backward
camera projection, which is computed given the

3DECONV1 does deconvolution only.

camera parameters and p1, the agent pose at the
beginning of the execution.
Action Generation Given the predicted goal lg,
we generate actions using an RNN. At each time
step t, given pt, we generate the goal mask Mt,
which has the same shape as the observed image
It. The goal mask Mt has a value of 1 for each
element that corresponds to the goal location lg in
It. We do not distinguish between visible or oc-
cluded locations. All other elements are set to 0.
We also maintain an out-of-sight ﬂag ot that is set
to 1 if (a) lg is not within the agent’s view; or (b)
the max scoring element in Pg corresponds to bg,
the term for events when the goal is not visible in
IP . Otherwise, ot is set to 0. We compute an ac-
tion generation hidden state yt with an RNN:

yt = LSTMA (AFFINEA([FLAT(Mt); ot]), yt−1) ,

where FLAT ﬂattens Mt into a vector, AFFINEA
is a learned afﬁne transformation with ReLU, and
LSTMA is an LSTM RNN. The previous hidden
state yt−1 was computed when generating the pre-
vious action, and the RNN is extended gradually
during execution. Finally, we compute a probabil-
ity distribution over actions:

P (at | lg, (I1, p1), . . . , (It, pt)) =

SOFTMAX(AFFINEp([yt; ψT (t)])) ,

where ψT is a learned embedding lookup table
for the current time (Chaplot et al., 2018) and
AFFINEp is a learned afﬁne transformation.
Model Parameters The model parameters θ in-
clude the parameters of the convolutions CNN0
and the components of LINGUNET: CNNj,
for j = 1, . . . , m.
AFFINEj, and DECONVj
In addition we learn two afﬁne transformations
AFFINEA and AFFINEp, two RNNs LSTMx and
LSTMA, two embedding functions ψx and ψT ,
and the goal distribution bias term bg. In our ex-
periments (Section 7), all parameters are learned
without external resources.

5 Learning

Our modeling decomposition enables us to choose
different learning algorithms for the two parts.
While reinforcement learning is commonly de-
ployed for tasks that beneﬁt from exploration (e.g.,
Peters and Schaal, 2008; Mnih et al., 2013), these
methods require many samples due to their high
sample complexity. However, when learning with
natural language, only a relatively small number
of samples is realistically available. This problem

was addressed in prior work by learning in a con-
textual bandit setting (Misra et al., 2017) or mix-
ing reinforcement and supervised learning (Xiong
et al., 2018). Our decomposition uniquely offers
to tease apart the language understanding prob-
lem and address it with supervised learning, which
generally has lower sample complexity. For action
generation though, where exploration can be au-
tonomous, we use policy gradient in a contextual
bandit setting (Misra et al., 2017).

(i)
1

(i)
1 , s

(i)
g )}N

We assume access to training data with N ex-
i=1, where ¯x(i) is an in-
amples {(¯x(i), s
(i)
struction, s
is the goal
is a start state, and s
g
state. We train the goal prediction component by
minimizing the cross-entropy of the predicted dis-
tribution with the gold-standard goal distribution.
The gold-standard goal distribution is a determin-
istic distribution with probability one at the pixel
corresponding to the goal location if the goal is in
the ﬁeld of view, or probability one at the extra
out-of-sight position otherwise. The gold location
(i)
is the agent’s location in s
g . We update the model
parameters using Adam (Kingma and Ba, 2014).

We train action generation by maximizing the
expected immediate reward the agent observes
while exploring the environment. The objective
for a single example i and time stamp t is:

J =

π(a | ˜st)R(i)(st, a) + λH(π(. | ˜st)) ,

(cid:88)

a∈A

where R(i) : S × A → R is an example-speciﬁc
reward function, H(·) is an entropy regularization
term, and λ is the regularization coefﬁcient. The
reward function R(i) details are described in de-
tails in Appendix B. Roughly speaking, the re-
ward function includes two additive components:
a problem reward and a shaping term (Ng et al.,
1999). The problem reward provides a positive re-
ward for successful task completion, and a nega-
tive reward for incorrect completion or collision.
The shaping term is positive when the agent gets
closer to the goal position, and negative if it is
moving away. The gradient of the objective is:

∇J =

π(a | ˜st)∇ log π(a | ˜st)R(st, a)

(cid:88)

a∈A

+λ∇H(π(. | ˜st) .

We approximate the gradient by sampling an ac-
tion using the policy (Williams, 1992), and use the
(i)
gold goal location computed from s
g . We per-
form several parallel rollouts to compute gradients
and update the parameters using Hogwild! (Recht
et al., 2011) and Adam learning rates.

Dataset Statistic
Number paragraphs
Mean instructions per paragraph
Mean actions per instruction
Mean tokens per instruction
Vocabulary size

LANI
6,000
4.7
24.6
12.1
2,292

CHAI
1,596
7.70
54.5
8.4
1,018

Table 1: Summary statistics of the two corpora.

6 Tasks and Data

6.1 LANI

The goal of LANI is to evaluate how well an agent
can follow navigation instructions. The agent task
is to follow a sequence of instructions that specify
a path in an environment with multiple landmarks.
Figure 1 (left) shows an example instruction.

The environment

is a fenced, square, grass
ﬁeld. Each instance of the environment con-
tains between 6–13 randomly placed landmarks,
sampled from 63 unique landmarks. The agent
can take four types of discrete actions: FORWARD,
TURNRIGHT, TURNLEFT, and STOP. The ﬁeld is
of size 50×50, the distance of the FORWARD ac-
tion is 1.5, and the turn angle is 15◦. The en-
vironment simulator is implemented in Unity3D.
At each time step, the agent performs an action,
observes a ﬁrst person view of the environment
as an RGB image, and receives a scalar reward.
The simulator provides a socket API to control the
agent and the environment.

Agent performance is evaluated using two met-
rics: task completion accuracy, and stop distance
error. A task is completed correctly if the agent
stops within an aerial distance of 5 from the goal.
We collect a corpus of navigation instructions
using crowdsourcing. We randomly generate en-
vironments, and generate one reference path for
each environment. To elicit linguistically interest-
ing instructions, reference paths are generated to
pass near landmarks. We use Amazon Mechanical
Turk, and split the annotation process to two tasks.
First, given an environment and a reference path,
a worker writes an instruction paragraph for fol-
lowing the path. The second task requires another
worker to control the agent to perform the instruc-
tions and simultaneously mark at each point what
part of the instruction was executed. The record-
ing of the second worker creates the ﬁnal data of
segmented instructions and demonstrations. The
generated reference path is displayed in both tasks.
The second worker could also mark the paragraph
as invalid. Both tasks are done from an over-
head view of the environment, but workers are in-
structed to provide instructions for a robot that ob-

[Go around the pillar on the right hand side] [and head
towards the boat, circling around it clockwise.] [When
you are facing the tree, walk towards it, and the pass on
the right hand side,] [and the left hand side of the cone.
Circle around the cone,] [and then walk past the hydrant
on your right,] [and the the tree stump.] [Circle around
the stump and then stop right behind it.]

Figure 3: Segmented instructions in the LANI domain.
The original reference path is marked in red (start) and
blue (end). The agent, using a drone icon, is placed at
the beginning of the path. The follower path is coded in
colors to align to the segmented instruction paragraph.

serves the environment from a ﬁrst person view.
Figure 3 shows a reference path and the written
instruction. This data can be used for evaluating
both executing sequences of instructions and sin-
gle instructions in isolation.

Table 1 shows the corpus statistics.4 Each para-
graph corresponds to a single unique instance of
the environment. The paragraphs are split into
train, test, and development, with a 70% / 15% /
15% split. Finally, we sample 200 single devel-
opment instructions for qualitative analysis of the
language challenge the corpus presents (Table 2).

6.2 CHAI

The CHAI corpus combines both navigation and
simple manipulation in a complex, simulated
household environment. We use the CHALET sim-
ulator (Yan et al., 2018), a 3D house simulator
that provides multiple houses, each with multi-
ple rooms. The environment supports moving be-
tween rooms, picking and placing objects, and
opening and closing cabinets and similar contain-
ers. Objects can be moved between rooms and
in and out of containers. The agent observes the
world in ﬁrst-person view, and can take ﬁve ac-
tions: FORWARD, TURNLEFT, TURNRIGHT, STOP,
and INTERACT. The INTERACT action acts on ob-
jects.
It takes as argument a 2D position in the
agent’s view. Agent performance is evaluated with
two metrics: (a) stop distance, which measures the
distance of the agent’s ﬁnal state to the ﬁnal an-
notated position; and (b) manipulation accuracy,
which compares the set of manipulation actions

4Appendix A provides statistics for related datasets.

Category
Spatial relations
between locations
Conjunctions of two
more locations
Temporal coordination
of sub-goals
Constraints on the
shape of trajectory

Co-reference

Comparatives

Count

LANI

CHAI

123

36

65

94

32

2

52

68

5

0

18

0

Example
LANI: go to the right side of the rock
CHAI: pick up the cup next to the bathtub and place it on . . .
LANI: ﬂy between the mushroom and the yellow cone
CHAI: . . . set it on the table next to the juice and milk.
LANI: at the mushroom turn right and move forward towards the statue
CHAI: go back to the kitchen and put the glass in the sink.

LANI: go past the house by the right side of the apple

LANI: turn around it and move in front of fern plant
CHAI: turn left, towards the kitchen door and move through it.
LANI: . . . the small stone closest to the blue and white fences stop

Table 2: Qualitative analysis of the LANI and CHAI corpora. We sample 200 single development instructions from
each corpora. For each category, we count how many examples of the 200 contained it and show an example.

Scenario
You have several hours before guests begin to arrive for
a dinner party. You are preparing a wide variety of meat
dishes, and need to put them in the sink.
In addition,
you want to remove things in the kitchen, and bathroom
which you don’t want your guests seeing, like the soaps
in the bathroom, and the dish cleaning items. You can
put these in the cupboards. Finally, put the dirty dishes
around the house in the dishwasher and close it.
Written Instructions
[In the kitchen, open the cupboard above the sink.] [Put
the cereal, the sponge, and the dishwashing soap into the
cupboard above the sink.] [Close the cupboard.] [Pick
up the meats and put them into the sink.] [Open the dish-
washer, grab the dirty dishes on the counter, and put the
dishes into the dishwasher.]

Figure 4: Scenario and segmented instruction from the
CHAI corpus.

to a reference set. When measuring distance, to
consider the house plan, we compute the minimal
aerial distance for each room that must be visited.
Yan et al. (2018) provides the full details of the
simulator and evaluation. We use ﬁve different
houses, each with up to six rooms. Each room
contains on average 30 objects. A typical room
is of size 6×6. We set the distance of FORWARD to
0.1, the turn angle to 90◦, and divide the agent’s
view to a 32×32 grid for the INTERACT action.

We collected a corpus of navigation and ma-
nipulation instructions using Amazon Mechanical
Turk. We created 36 common household scenar-
ios to provide a familiar context to the task.5 We
use two crowdsourcing tasks. First, we provide
workers with a scenario and ask them to write in-
structions. The workers are encouraged to explore
the environment and interact with it. We then seg-
ment the instructions to sentences automatically.
In the second task, workers are presented with the
segmented sentences in order and asked to execute
them. After ﬁnishing a sentence, the workers re-

5We observed that asking workers to simply write instruc-
tions without providing a scenario leads to combinations of
repetitive instructions unlikely to occur in reality.

quest the next sentence. The workers do not see
the original scenario. Figure 4 shows a scenario
and the written segmented paragraph. Similar to
LANI, CHAI data can be used for studying com-
plete paragraphs and single instructions.

Table 1 shows the corpus statistics.6 The para-
graphs are split into train, test, and development,
with a 70% / 15% / 15% split. Table 2 shows qual-
itative analysis of a sample of 200 instructions.

7 Experimental Setup

instructions.

Method Adaptations for CHAI We apply two
modiﬁcations to our model to support interme-
diate goal for the CHAI
First,
we train an additional RNN to predict the se-
quence of intermediate goals given the instruc-
There are two types of goals:
tion only.
NAVIGATION,
for action sequences requiring
movement only and ending with the STOP action;
and INTERACTION, for sequence of movement ac-
tions that end with an INTERACT action. For ex-
ample, for the instruction pick up the red book
and go to the kitchen, the sequence of goals will
be (cid:104)INTERACTION, NAVIGATION, NAVIGATION(cid:105).
This indicates the agent must ﬁrst move to the
object to pick it up via interaction, move to the
kitchen door, and ﬁnally move within the kitchen.
The process of executing an instruction starts with
predicting the sequence of goal types. We call our
model (Section 4) separately for each goal type.
The execution concludes when the ﬁnal goal is
completed. For learning, we create a separate ex-
ample for each intermediate goal and train the ad-
ditional RNN separately. The second modiﬁcation
is replacing the backward camera projection for
inferring the goal location with ray casting to iden-

6The number of actions per instruction is given in the
more ﬁne-grained action space used during collection. To
make the required number of actions smaller, we use the more
coarse action space speciﬁed.

Method
STOP
RANDOMWALK
MOSTFREQUENT
MISRA17
CHAPLOT18
Our Approach (OA)
OA w/o RNN
OA w/o Language
OA w/joint
OA w/oracle goals

LANI

CHAI

SD
15.37
14.80
19.31
10.54
9.05
8.65
9.21
10.65
11.54
2.13

TC
8.20
9.66
2.94
22.9
31.0
35.72
31.30
23.02
21.76
94.60

SD
2.99
2.99
3.80
2.99
2.99
2.75
3.75
3.22
2.99
2.19

MA
37.53
28.96
37.53
32.25
37.53
37.53
37.43
37.53
36.90
41.07

Table 3: Performance on the development data.

tify INTERACTION goals, which are often objects
that are not located on the ground.
Baselines We compare our approach against the
following baselines: (a) STOP: Agent stops im-
mediately; (b) RANDOMWALK: Agent samples
actions uniformly until it exhausts the horizon
or stops; (c) MOSTFREQUENT: Agent takes the
most frequent action in the data, FORWARD for
both datasets, until it exhausts the horizon; (d)
the approach of Misra et al. (2017);
MISRA17:
and (e) CHAPLOT18:
the approach of Chaplot
et al. (2018). We also evaluate goal prediction and
compare to the method of Janner et al. (2018) and
a CENTER baseline, which always predict the cen-
ter pixel. Appendix C provides baseline details.
Evaluation Metrics We evaluate using the met-
rics described in Section 6: stop distance (SD) and
task completion (TC) for LANI, and stop distance
(SD) and manipulation accuracy (MA) for CHAI.
To evaluate the goal prediction, we report the real
distance of the predicted goal from the annotated
goal and the percentage of correct predictions. We
consider a goal correct if it is within a distance of
5.0 for LANI and 1.0 for CHAI. We also report
human evaluation for LANI by asking raters if the
generated path follows the instruction on a Likert-
type scale of 1–5. Raters were shown the gener-
ated path, the reference path, and the instruction.
Parameters We use a horizon of 40 for both
domains. During training, we allow additional
5 steps to encourage learning even after errors.
When using intermediate goals in CHAI, the hori-
zon is used for each intermediate goal separately.
All other parameters and detailed in Appendix D.

8 Results

Tables 3 and 4 show development and test re-
sults. Both sets of experiments demonstrate sim-
ilar trends. The low performance of STOP, RAN-
DOMWALK, and MOSTFREQUENT demonstrates

Method
STOP
RANDOMWALK
MOSTFREQUENT
MISRA17
CHAPLOT18
Our Approach

LANI

CHAI

SD
15.18
14.63
19.14
10.23
8.78
8.43

TC
8.29
9.76
3.15
23.2
31.9
36.9

SD
3.59
3.59
4.36
3.59
3.59
3.34

MA
39.77
33.29
39.77
36.84
39.76
39.97

Table 4: Performance on the held-out test dataset.

Method
CENTER
Janner et al. (2018)
Our Approach

LANI

CHAI

Dist
12.0
9.61
8.67

Acc
19.51
30.26
35.83

Dist
3.41
2.81
2.12

Acc
19.0
28.3
40.3

Table 5: Development goal prediction performance.
We measure distance (Dist) and accuracy (Acc).

the challenges of both tasks, and shows the tasks
are robust to simple biases. On LANI, our ap-
proach outperforms CHAPLOT18, improving task
completion (TC) accuracy by 5%, and both meth-
ods outperform MISRA17. On CHAI, CHAP-
LOT18 and MISRA17 both fail to learn, while
our approach shows an improvement on stop dis-
tance (SD). However, all models perform poorly
on CHAI, especially on manipulation (MA).

To isolate navigation performance on CHAI, we
limit our train and test data to instructions that in-
clude navigation actions only. The STOP baseline
on these instructions gives a stop distance (SD) of
3.91, higher than the average for the entire data
as these instructions require more movement. Our
approach gives a stop distance (SD) of 3.24, a 17%
reduction of error, signiﬁcantly better than the 8%
reduction of error over the entire corpus.

We also measure human performance on a sam-
ple of 100 development examples for both tasks.
On LANI, we observe a stop distance error (SD)
of 5.2 and successful task completion (TC) 63%
of the time. On CHAI, the human distance er-
ror (SD) is 1.34 and the manipulation accuracy is
100%. The imperfect performance demonstrates
the inherent ambiguity of the tasks. The gap to
human performance is still large though, demon-
strating that both tasks are largely open problems.
The imperfect human performance raises ques-
tions about automated evaluation.
In general,
we observe that often measuring execution qual-
ity with rigid goals is insufﬁcient. We conduct
a human evaluation with 50 development exam-
ples from LANI rating human performance and
our approach. Figure 5 shows a histogram of the
ratings. The mean rating for human followers is
4.38, while our approach’s is 3.78; we observe
a similar trend to before with this metric. Using

Category
Spatial relations
Location conjunction
Temporal coordination
Trajectory constraints
Co-reference
Comparatives

Present Absent
10.09
9.05
8.24
8.99
8.59
9.25

8.75
10.19
11.38
9.56
12.88
10.22

p-value
.262
.327
.015
.607
.016
.906

Table 6: Mean goal prediction error for LANI instruc-
tions with and without the analysis categories we used
in Table 2. The p-values are from two-sided t-tests
comparing the means in each row.

e
g
a
t
n
e
c
r
e
P

60

40

20

0

Human
Our Approach

1

2

3

4

5

Figure 5: Likert rating histogram for expert human fol-
lower and our approach for LANI.

judgements on our approach, we correlate the hu-
man metric with the SD measure. We observe a
Pearson correlation -0.65 (p=5e-7), indicating that
our automated metric correlates well with human
judgment.7 This initial study suggests that our au-
tomated evaluation is appropriate for this task.

Our ablations (Table 3) demonstrate the impor-
tance of each of the components of the model.
We ablate the action generation RNN (w/o RNN),
completely remove the language input (w/o Lan-
guage), and train the model jointly (w/joint Learn-
ing).8 On CHAI especially, ablations results in
models that display ineffective behavior. Of the
ablations, we observe the largest beneﬁt from
decomposing the learning and using supervised
learning for the language problem.

We also evaluate our approach with access to
oracle goals (Table 3). We observe this im-
proves navigation performance signiﬁcantly on
both tasks. However, the model completely fails
to learn a reasonable manipulation behavior for
CHAI. This illustrates the planning complexity
of this domain. A large part of the improvement
in measured navigation behavior is likely due to
eliminating much of the ambiguity the automated
metric often fails to capture.

Finally, on goal prediction (Table 5), our ap-
proach outperforms the method of Janner et al.
(2018). Figure 6 and Appendix Figure 7 show ex-
ample goal predictions. In Table 6, we break down
LANI goal prediction results for the analysis cate-

7We did not observe this kind of clear anti-correlation
comparing the two results for human performance (Pearson
correlation of 0.09 and p=0.52). The limited variance in hu-
man performance makes correlation harder to test.

8Appendix C provides the details of joint learning.

curve around big rock keeping it to your left .

walk over to the cabinets and open the cabinet doors up

Figure 6: Goal prediction probability maps Pg overlaid
on the corresponding observed panoramas IP . The top
example shows a result on LANI, the bottom on CHAI.
gories we used in Table 2 using the same sample of
the data. Appendix E includes a similar table for
CHAI. We observe that our approach ﬁnds instruc-
tions with temporal coordination or co-reference
challenging. Co-reference is an expected limita-
tion; with single instructions, the model can not
resolve references to previous instructions.
9 Discussion
We propose a model for instruction following with
explicit separation of goal prediction and action
generation. Our representation of goal prediction
is easily interpretable, while not requiring the de-
sign of logical ontologies and symbolic represen-
tations. A potential limitation of our approach is
cascading errors. Action generation relies com-
pletely on the predicted goal and is not exposed
to the language otherwise. This also suggests a
second related limitation:
the model is unlikely
to successfully reason about instructions that in-
clude constraints on the execution itself. While
the model may reach the ﬁnal goal correctly, it is
unlikely to account for the intermediate trajectory
constraints. As we show (Table 2), such instruc-
tions are common in our data. These two limita-
tions may be addressed by allowing action genera-
tion access to the instruction. Achieving this while
retaining an interpretable goal representation that
clearly determines the execution is an important
direction for future work. Another important open
question concerns automated evaluation, which re-
mains especially challenging when instructions do
not only specify goals, but also constraints on how
to achieve them. Our resources provide the plat-
form and data to conduct this research.
Acknowledgments
This research was supported by NSF (CAREER-
1750499), Schmidt Sciences, a Google Faculty
Award, and cloud credits from Microsoft. We
thank John Langford, Claudia Yan, Bharath Har-
iharan, Noah Snavely, the Cornell NLP group, and
the anonymous reviewers for their advice.

References
Anne H Anderson, Miles Bader, Ellen Gurman Bard,
Elizabeth Boyle, Gwyneth Doherty, Simon Garrod,
Stephen Isard, Jacqueline Kowtko, Jan McAllister,
Jim Miller, Catherine Sotillo, Henry S. Thompson,
and Regina Weinert. 1991. The HCRC map task
corpus. Language and Speech, 34.

Peter Anderson, Qi Wu, Damien Teney, Jake Bruce,
Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen
Gould, and Anton van den Hengel. 2018. Vision-
and-language navigation:
Interpreting visually-
grounded navigation instructions in real environ-
ments. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition.

Yoav Artzi, Dipanjan Das, and Slav Petrov. 2014.
Learning compact lexicons for CCG semantic pars-
ing. In Proceedings of the 2014 Conference on Em-
pirical Methods in Natural Language Processing.

Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion of Computational Linguistics, 1.

Yonatan Bisk, Daniel Marcu, and William Wong. 2016.
Towards a dataset for human computer communica-
tion via grounded language acquisition. In Proceed-
ings of the AAAI Workshop on Symbiotic Cognitive
Systems.

Devendra Singh Chaplot, Kanthashree Mysore
Sathyendra, Rama Kumar Pasumarthi, Dheeraj
Rajagopal, and Ruslan Salakhutdinov. 2018. Gated-
attention architectures for task-oriented language
grounding.

David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the Na-
tional Conference on Artiﬁcial Intelligence.

Deborah A Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the ATIS
In Proceedings of the
task: The ATIS-3 corpus.
workshop on Human Language Technology.

Abhishek Das, Samyak Datta, Georgia Gkioxari, Ste-
fan Lee, Devi Parikh, and Dhruv Batra. 2018. Em-
In Proceedings of the
bodied question answering.
IEEE Conference on Computer Vision and Pattern
Recognition.

Daniel Fried, Ronghang Hu, Volkan Cirik, Anna
Rohrbach, Jacob Andreas, Louis-Philippe Morency,
Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein,
and Trevor Darrell. 2018. Speaker-follower mod-
CoRR,
els for vision-and-language navigation.
abs/1806.02724.

Daniel Gordon, Aniruddha Kembhavi, Mohammad
Rastegari, Joseph Redmon, Dieter Fox, and Ali

Farhadi. 2018. Iqa: Visual question answering in in-
teractive environments. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition.

Charles T. Hemphill, John J. Godfrey, and George R.
Doddington. 1990. The ATIS spoken language sys-
In Proceedings of the DARPA
tems pilot corpus.
speech and natural language workshop.

Karl Moritz Hermann, Felix Hill, Simon Green, Fumin
Wang, Ryan Faulkner, Hubert Soyer, David Szepes-
vari, Wojciech Czarnecki, Max Jaderberg, Denis
Teplyashin, Marcus Wainwright, Chris Apps, Demis
Hassabis, and Phil Blunsom. 2017. Grounded lan-
guage learning in a simulated 3D world. CoRR,
abs/1706.06551.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long

short-term memory. Neural computation, 9.

Michael Janner, Karthik Narasimhan, and Regina
Barzilay. 2018.
Representation learning for
grounded spatial reasoning. Transactions of the As-
sociation for Computational Linguistics, 6.

Sahar Kazemzadeh, Vicente Ordonez, Mark Matten,
and Tamara L. Berg. 2014. Referitgame: Referring
to objects in photographs of natural scenes. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.

Joohyun Kim and Raymond Mooney. 2012. Unsuper-
vised PCFG induction for grounded language learn-
ing with highly ambiguous supervision. In Proceed-
ings of the Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. In Proceedings
of the International Conference on Learning Repre-
sentations.

Nikita Kitaev and Dan Klein. 2017. Where is misty?
interpreting spatial descriptors by modeling regions
in space. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.

Chen Kong, Dahua Lin, Mohit Bansal, Raquel Urta-
sun, and Sanja Fidler. 2014. What are you talking
about? text-to-image coreference. In Proceedings of
the IEEE Conference on Computer Vision and Pat-
tern Recognition.

Jayant Krishnamurthy and T. Kollar. 2013.

Jointly
learning to parse and perceive: Connecting natural
language to the physical world. Transactions of the
Association for Computational Linguistics, 1.

Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86.

Andrew L Maas, Awni Y Hannun, and Andrew Y Ng.
2013. Rectiﬁer nonlinearities improve neural net-
work acoustic models. In Proceedings of the inter-
national conference on machine learning.

James MacGlashan, Monica Babes-Vroman, Marie
desJardins, Michael L. Littman, Smaranda Muresan,
S Bertel Squire, Stefanie Tellex, Dilip Arumugam,
and Lei Yang. 2015. Grounding english commands
to reward functions. In Robotics: Science and Sys-
tems.

Matthew MacMahon, Brian Stankiewics, and Ben-
jamin Kuipers. 2006. Walk the talk: Connecting
language, knowledge, action in route instructions.
In Proceedings of the National Conference on Ar-
tiﬁcial Intelligence.

Junhua Mao, Jonathan Huang, Alexander Toshev, Oana
Camburu, Alan Yuille, and Kevin Murphy. 2016.
Generation and Comprehension of Unambiguous
Object Descriptions. In Proceedings of IEEE Con-
ference on Computer Vision and Pattern Recogni-
tion.

Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012a. A joint
model of language and perception for grounded at-
tribute learning. In Proceedings of the International
Conference on Machine Learning.

Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer,
and Dieter Fox. 2012b. Learning to parse natural
language commands to a robot control system.
In
Proceedings of the International Symposium on Ex-
perimental Robotics.

Hongyuan Mei, Mohit Bansal, and R. Matthew Walter.
2016. What to talk about and how? selective gener-
ation using lstms with coarse-to-ﬁne alignment. In
Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies.

Dipendra Misra, John Langford, and Yoav Artzi. 2017.
Mapping instructions and visual observations to ac-
In Proceedings
tions with reinforcement learning.
of the Conference on Empirical Methods in Natural
Language Processing.

Dipendra K. Misra, Jaeyong Sung, Kevin Lee, and
Ashutosh Saxena. 2016. Tell me dave: Context-
sensitive grounding of natural language to manip-
ulation instructions. The International Journal of
Robotics Research, 35.

Kumar Dipendra Misra, Kejia Tao, Percy Liang, and
Ashutosh Saxena. 2015. Environment-driven lexi-
In Pro-
con induction for high-level instructions.
ceedings of the Annual Meeting of the Association
for Computational Linguistics and the International
Joint Conference on Natural Language Processing.

deep reinforcement learning. In Advances in Neural
Information Processing Systems.

Vinod Nair and Geoffrey E Hinton. 2010. Rectiﬁed lin-
ear units improve restricted boltzmann machines. In
Proceedings of the international conference on ma-
chine learning.

Andrew Y. Ng, Daishi Harada, and Stuart J. Russell.
1999. Policy invariance under reward transforma-
tions: Theory and application to reward shaping. In
Proceedings of the International Conference on Ma-
chine Learning.

Junhyuk Oh, Satinder P. Singh, Honglak Lee, and
Pushmeet Kohli. 2017. Zero-shot task generaliza-
tion with multi-task deep reinforcement learning. In
Proceedings of the international conference on ma-
chine learning.

Jan Peters and Stefan Schaal. 2008. Reinforcement
learning of motor skills with policy gradients. Neu-
ral networks, 21.

Benjamin Recht, Christopher Re, Stephen Wright, and
Feng Niu. 2011. Hogwild: A lock-free approach
to parallelizing stochastic gradient descent. In Ad-
vances in Neural Information Processing Systems.

Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
2015. U-net: Convolutional networks for biomed-
In International Confer-
ical image segmentation.
ence on Medical image computing and computer-
assisted intervention.

John Schulman, Philipp Moritz, Sergey Levine,
Michael I. Jordan, and Pieter Abbeel. 2015. High-
dimensional continuous control using generalized
advantage estimation. CoRR, abs/1506.02438.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overﬁtting. The Journal of Machine Learning
Research, 15.

Alane Suhr and Yoav Artzi. 2018. Situated mapping
of sequential instructions to actions with single-step
In Proceedings of the Annual
reward observation.
Meeting of the Association for Computational Lin-
guistics.

Richard S. Sutton, Doina Precup, and Satinder P. Singh.
Intra-option learning about temporally ab-
In Proceedings of the international

1998.
stract actions.
conference on machine learning.

Dmitry Ulyanov, Andrea Vedaldi, and Victor S.
Instance normalization: The
CoRR,

Lempitsky. 2016.
missing ingredient for fast stylization.
abs/1607.08022.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Alex Graves, Ioannis Antonoglou, Daan Wierstra,
and Martin A. Riedmiller. 2013. Playing atari with

Ronald J. Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine Learning, 8.

Wenhan Xiong, Xiaoxiao Guo, Mo Yu, Shiyu Chang,
Bowen Zhou, and William Yang Wang. 2018.
Scheduled policy optimization for natural language
communication with intelligent agents. In Proceed-
ings of the International Joint Conferences on Arti-
ﬁcial Intelligence.

Claudia Yan, Dipendra Kumar Misra, Andrew Ben-
nett, Aaron Walsman, Yonatan Bisk, and Yoav Artzi.
2018. Chalet: Cornell house agent learning environ-
ment. CoRR, abs/1801.07357.

Licheng Yu, Patrick Poirson, Shan Yang, Alexander C.
Berg, and Tamara L. Berg. 2016. Modeling context
in referring expressions. In Proceedings of the Eu-
ropean Conference on Computer Vision.

A Tasks and Data: Comparisons

Table 7 provides summary statistics comparing
LANI and CHAI to existing related resources.

B Reward Function

LANI Following Misra et al. (2017), we use a
shaped reward function that rewards the agent for
moving towards the goal location. The reward for
exampl i is:

R(i)(s, a, s(cid:48)) = R(i)

p + φ(i)(s) − φ(i)(s(cid:48))

(1)

where s(cid:48) is the origin state, a is the action, s is
(i)
is the problem reward, and
the target state, R
p
φ(i)(s)−φ(i) is a shaping term. We use a potential-
based shaping (Ng et al., 1999) that encourages the
agent to both move and turn towards the goal. The
potential function is:
φ(i)(s) =

δTURNDIST(s, s(i)
g )
+(1 − δ)MOVEDIST(s, s(i)

g ) ,

where MOVEDIST is the euclidean distance to the
goal normalized by the agent’s forward movement
distance, and TURNDIST is the angle the agent
needs to turn to face the goal normalized by the
agent’s turn angle. We use δ as a gating term,
which is 0 when the agent is near the goal and
increases monotonically towards 1 the further the
agent is from the goal. This decreases the sensi-
tivity of the potential function to the TURNDIST
(i)
term close to the goal. The problem reward R
p
provides a negative reward of up to -1 on collision
with any object or boundary (based on the angle
and magnitude of collision), a negative reward of
-0.005 on every action to discourage long trajec-
tories, a negative reward of -1 on an unsuccess-
ful stop, when the distance to the goal location is
greater than 5, and a positive reward of +1 on a
successful stop.
CHAI We use a similar potential based reward
function as LANI. Instead of rewarding the agent
to move towards the ﬁnal goal the model is re-
warded for moving towards the next intermedi-
ate goal. We heuristically generate intermediate
goals from the human demonstration by generat-
ing goals for objects to be interacted with, doors
that the agent should enter, and the ﬁnal position
of the agent. The potential function is:

φ(i)(s) =

TURNDIST(s, s(i)
MOVEDIST(s, s(i)

g,j) +
g,j) + INTDIST(s, s(i)

g,j) ,

(i)
g,j

where s
intermediate goal,
TURNDIST rewards the agent for turning to-

the next

is

wards the goal, MOVEDIST rewards the agent for
moving closer to the goal, and INTDIST rewards
the agent for accomplishing the interaction in the
intermediate goal. The goal is updated on being
accomplished. Besides the potential term, we
(i)
use a problem reward R
that gives a reward of
p
1 for stopping near a goal, -1 for colliding with
obstacles, and -0.002 as a verbosity penalty for
each step.

C Baseline Details

MISRA17 We use the model of Misra et al.
(2017). The model uses a convolution neural net-
work for encoding the visual observations, a re-
current neural network with LSTM units to en-
code the instruction, and a feed-forward network
to generate actions using these encodings. The
model is trained using policy gradient in a con-
textual bandit setting. We use the code provided
by the authors.
CHAPLOT18 We use the gated attention archi-
tecture of Chaplot et al. (2018). The model is
trained using policy gradient with generalized ad-
vantage estimation (Schulman et al., 2015). We
use the code provided by the authors.
Our Approach with Joint Training We train
the full model with policy gradient. We maxi-
mize the expected reward objective with entropy
regularization. Given a sampled goal location
lg ∼ p(. | ¯x, IP ) and a sampled action a ∼ p(. |
lg, (I1, p1), . . . , (It, pt)), the update is:
∇J ≈ {∇ log P (lg | ¯x, IP ) +

∇ log P (at | lg, (I1, p1), . . . , (It, pt))} R(st, a)
λ∇H(π(. | ˜st) .

We perform joint training with randomly initial-
ized goal prediction and action generation models.

D Hyperparameters

For LANI experiments, we use 5% of the training
data for tuning the hyperparameters and train on
the remaining. For CHAI, we use the development
set for tuning the hyperparameters. We train our
models for 20 epochs and ﬁnd the optimal stop-
ping epoch using the tuning set. We use 32 dimen-
sional embeddings for words and time. LSTMx
and LSTMA are single layer LSTMs with 256
hidden units. The ﬁrst layer of CNN0 contains
128 8×8 kernels with a stride of 4 and padding 3,
and the second layer contains 64 3×3 kernels with
a stride of 1 and padding 1. The convolution lay-
ers in LINGUNET use 32 5×5 kernels with stride

Dataset

Vocabulary Mean Instruction

Num
Instructions

Num.
Actions

Avg Trajectory
Length

Partially
Observed

Bisk et al.
(2016)
MacMahon
et al. (2006)
Matuszek et al.
(2012b)
Misra et al.
(2015)
LANI
CHAI

16,767

3,237

217

469

28,204
13,729

Size

1,426

563

39

775

2,292
1018

Length

15.27

7.96

6.65

48.7

12.07
10.14

81

3

3

>100

4
1028

15.4

3.12

N/A

21.5

24.6
54.5

No

Yes

No

No

Yes
Yes

Table 7: Comparison of LANI and CHAI to several existing natural language instructions corpora.

Category
Spatial relations
Location conjunction
Temporal coordination
Co-reference

Present Absent

2.56
3.85
1.70
1.98

1.77
1.93
2.14
1.98

p-value
.023
.226
.164
.993

Table 8: Mean goal prediction error for CHAI instruc-
tions with and without the analysis categories we used
in Table 2. The p-values are from two-sided t-tests
comparing the means in each row.

2. All deconvolutions except the ﬁnal one, also use
32 5×5 kernels with stride 2. The dropout proba-
bility in LINGUNET is 0.5. The size of attention
mask is 32×32 + 1. For both LANI and CHAI, we
use a camera angle of 60◦ and create panoramas
using 6 separate RGB images. Each image is of
size 128×128. We use a learning rate of 0.00025
and entropy coefﬁcient λ of 0.05.

E CHAI Error Analysis

Table 8 provides the same kind of error analysis
results here for the CHAI dataset as we produced
for LANI, comparing performance of the model on
samples of sentences with and without the analysis
phenomena that occurred in CHAI.

F Examples of Generated Goal

Prediction

Figure 7 shows example goal predictions from the
development sets. We found the predicted proba-
bility distributions to be reasonable even in many
cases where the agent failed to successfully com-
plete the task. We observed that often the eval-
uation metric is too strict for LANI instructions,
especially in cases of instruction ambiguity.

Success

Success

Failure

go round the ﬂowers

ﬂy between the palm tree and pond

head toward the wishing well and keep it on your right .

move back to the kitchen .

then drop the tropicana onto the coffee table .

walk the cup to the table and set the cup on the table .

Figure 7: Goal prediction probability maps Pg overlaid on the corresponding observed panoramas IP . The top
three examples show results from LANI, the bottom three from CHAI. The white arrow indicates the forward
direction that the agent is facing. The success/failure in the LANI examples indicate if the task was completed
accurately or not following the task completion (TC) metric.

Mapping Instructions to Actions in 3D Environments
with Visual Goal Prediction

Andrew Bennett
Dipendra Misra
Eyvind Niklasson Max Shatkhin

Valts Blukis
Yoav Artzi

Department of Computer Science and Cornell Tech, Cornell University, New York, NY, 10044
{dkm, awbennett, valts, yoav}@cs.cornell.edu
{een7, ms3448}@cornell.edu

9
1
0
2
 
r
a

M
 
8
1
 
 
]
L
C
.
s
c
[
 
 
2
v
6
8
7
0
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

We propose to decompose instruction exe-
cution to goal prediction and action genera-
tion. We design a model that maps raw vi-
sual observations to goals using LINGUNET,
a language-conditioned image generation net-
work, and then generates the actions required
to complete them. Our model
is trained
from demonstration only without external re-
sources. To evaluate our approach, we intro-
duce two benchmarks for instruction follow-
ing: LANI, a navigation task; and CHAI, where
an agent executes household instructions. Our
evaluation demonstrates the advantages of our
model decomposition, and illustrates the chal-
lenges posed by our new benchmarks.

1

Introduction

Executing instructions in interactive environments
requires mapping natural language and observa-
tions to actions. Recent approaches propose learn-
ing to directly map from inputs to actions, for ex-
ample given language and either structured obser-
vations (Mei et al., 2016; Suhr and Artzi, 2018) or
raw visual observations (Misra et al., 2017; Xiong
et al., 2018). Rather than using a combination
of models, these approaches learn a single model
to solve language, perception, and planning chal-
lenges. This reduces the amount of engineering
required and eliminates the need for hand-crafted
meaning representations. At each step, the agent
maps its current inputs to the next action using a
single learned function that is executed repeatedly
until task completion.

Although executing the same computation at
each step simpliﬁes modeling, it exempliﬁes cer-
tain inefﬁciencies; while the agent needs to de-
cide what action to take at each step, identifying
its goal is only required once every several steps
or even once per execution. The left instruction in
Figure 1 illustrates this. The agent can compute its

After reaching the hydrant
head towards
the blue
fence and pass towards the
right side of the well.

Put the cereal, the sponge,
and the dishwashing soap
into the cupboard above
the sink.

Figure 1: Example instructions from our two tasks:
LANI (left) and CHAI (right). LANI is a landmark nav-
igation task, and CHAI is a corpus of instructions in the
CHALET environment.

goal once given the initial observation, and given
this goal can then generate the actions required.
In this paper, we study a new model that explic-
itly distinguishes between goal selection and ac-
tion generation, and introduce two instruction fol-
lowing benchmark tasks to evaluate it.

Our model decomposes into goal prediction and
action generation. Given a natural language in-
struction and system observations, the model pre-
dicts the goal to complete. Given the goal, the
model generates a sequence of actions.

The key challenge we address is designing the
goal representation. We avoid manually designing
a meaning representation, and predict the goal in
the agent’s observation space. Given the image of
the environment the agent observes, we generate a
probability distribution over the image to highlight
the goal location. We treat this prediction as image
generation, and develop LINGUNET, a language
conditioned variant of the U-NET image-to-image
architecture (Ronneberger et al., 2015). Given the
visual goal prediction, we generate actions using a
recurrent neural network (RNN).

Our model decomposition offers two key advan-
tages. First, we can use different learning methods
as appropriate for the goal prediction and action

generation problems. We ﬁnd supervised learning
more effective for goal prediction, where only a
limited amount of natural language data is avail-
able. For action generation, where exploration is
critical, we use policy gradient in a contextual ban-
dit setting (Misra et al., 2017). Second, the goal
distribution is easily interpretable by overlaying it
on the agent observations. This can be used to in-
crease the safety of physical systems by letting the
user verify the goal before any action is executed.
Despite the decomposition, our approach retains
the advantages of the single-model approach.
It
does not require designing intermediate represen-
tations, and training does not rely on external re-
sources, such as pre-trained parsers or object de-
tectors, instead using demonstrations only.

We introduce two new benchmark tasks with
different levels of complexity of goal prediction
and action generation. LANI is a 3D navigation
environment and corpus, where an agent navigates
between landmarks. The corpus includes 6,000
sequences of natural language instructions, each
containing on average 4.7 instructions. CHAI is
a corpus of 1,596 instruction sequences, each in-
cluding 7.7 instructions on average, for CHALET,
a 3D house environment (Yan et al., 2018).
In-
structions combine navigation and simple manipu-
lation, including moving objects and opening con-
tainers. Both tasks require solving language chal-
lenges, including spatial and temporal reasoning,
as well as complex perception and planning prob-
lems. While LANI provides a task where most in-
structions include a single goal, the CHAI instruc-
tions often require multiple intermediate goals.
For example, the household instruction in Fig-
ure 1 can be decomposed to eight goals: opening
the cupboard, picking each item and moving it to
the cupboard, and closing the cupboard. Achiev-
ing each goal requires multiple actions of differ-
ent types, including moving and acting on objects.
This allows us to experiment with a simple varia-
tion of our model to generate intermediate goals.

We compare our approach to multiple recent
methods. Experiments on the LANI navigation
task indicate that decomposing goal prediction
and action generation signiﬁcantly improves in-
struction execution performance. While we ob-
serve similar trends on the CHAI instructions, re-
sults are overall weaker, illustrating the complex-
ity of the task. We also observe that inherent
ambiguities in instruction following make exact

goal identiﬁcation difﬁcult, as demonstrated by
imperfect human performance. However, the gap
to human-level performance still remains large
across both tasks. Our code and data are available
at github.com/clic-lab/ciff.

2 Technical Overview

Task Let X be the set of all instructions, S the
set of all world states, and A the set of all actions.
An instruction ¯x ∈ X is a sequence (cid:104)x1, . . . , xn(cid:105),
where each xi is a token. The agent executes
instructions by generating a sequence of actions,
and indicates execution completion with the spe-
cial action STOP.

The sets of actions A and states S are domain
speciﬁc. In the navigation domain LANI, the ac-
tions include moving the agent and changing its
orientation. The state information includes the po-
sition and orientation of the agent and the differ-
ent landmarks. The agent actions in the CHALET
house environment include moving and changing
the agent orientation, as well as an object interac-
tion action. The state encodes the position and ori-
entation of the agent and all objects in the house.
For interactive objects, the state also includes their
status, for example if a drawer is open or closed.
In both domains, the actions are discrete. The do-
mains are described in Section 6.
Model The agent does not observe the world
state directly, but instead observes its pose and an
RGB image of the environment from its point of
view. We deﬁne these observations as the agent
context ˜s. An agent model is a function from an
agent context ˜s to an action a ∈ A. We model
goal prediction as predicting a probability distri-
bution over the agent visual observations, repre-
senting the likelihood of locations or objects in the
environment being target positions or objects to be
acted on. Our model is described in Section 4.
Learning We assume access to training data
i=1, where ¯x(i)
with N examples {(¯x(i), s
(i)
(i)
is the
is an instruction, s
g
1
goal state. We decompose learning; training goal
prediction using supervised learning, and action
generation using oracle goals with policy gradient
in a contextual bandit setting. We assume an in-
strumented environment with access to the world
state, which is used to compute rewards during
training only. Learning is described in Section 5.
Evaluation We evaluate task performance on a
(i)
i=1, where ¯x(i) is an in-
g )}M
test set {(¯x(i), s

is a start state, and s

(i)
g )}N

(i)
1 , s

(i)
1 , s

(i)
1

struction, s
is the goal
is a start state, and s
state. We evaluate task completion accuracy and
the distance of the agent’s ﬁnal state to s

(i)
g .

(i)
g

3 Related Work

Mapping instruction to action has been studied
extensively with intermediate symbolic represen-
tations (e.g., Chen and Mooney, 2011; Kim and
Mooney, 2012; Artzi and Zettlemoyer, 2013; Artzi
et al., 2014; Misra et al., 2015, 2016). Recently,
there has been growing interest in direct mapping
from raw visual observations to actions (Misra
et al., 2017; Xiong et al., 2018; Anderson et al.,
2018; Fried et al., 2018). We propose a model that
enjoys the beneﬁts of such direct mapping, but ex-
plicitly decomposes that task to interpretable goal
prediction and action generation. While we focus
on natural language, the problem has also been
studied using synthetic language (Chaplot et al.,
2018; Hermann et al., 2017).

Our model design is related to hierarchical re-
inforcement learning, where sub-policies at differ-
ent levels of the hierarchy are used at different fre-
quencies (Sutton et al., 1998). Oh et al. (2017)
uses a two-level hierarchy for mapping synthetic
language to actions. Unlike our visual goal rep-
resentation, they use an opaque vector representa-
tion. Also, instead of reinforcement learning, our
methods emphasize sample efﬁciency.

Goal prediction is related to referring expres-
sion interpretation (Matuszek et al., 2012a; Krish-
namurthy and Kollar, 2013; Kazemzadeh et al.,
2014; Kong et al., 2014; Yu et al., 2016; Mao et al.,
2016; Kitaev and Klein, 2017). While our model
solves a similar problem for goal prediction, we
focus on detecting visual goals for actions, includ-
ing both navigation and manipulation, as part of
an instruction following model. Using formal goal
representation for instruction following was stud-
ied by MacGlashan et al. (2015). In contrast, our
model generates a probability distribution over im-
ages, and does not require an ontology.

Our data collection is related to existing work.
LANI is inspired by the HCRC Map Task (An-
derson et al., 1991), where a leader directs a fol-
lower to navigate between landmarks on a map.
We use a similar task, but our scalable data collec-
tion process allows for a signiﬁcantly larger cor-
pus. We also provide an interactive navigation
environment, instead of only map diagrams. Un-
like Map Task, our leaders and followers do not
interact in real time. This abstracts away inter-

action challenges, similar to how the SAIL nav-
igation corpus was collected (MacMahon et al.,
2006). CHAI instructions were collected using
scenarios given to workers, similar to the ATIS
collection process (Hemphill et al., 1990; Dahl
et al., 1994). Recently, multiple 3D research envi-
ronments were released. LANI has a signiﬁcantly
larger state space than existing navigation envi-
ronments (Hermann et al., 2017; Chaplot et al.,
2018), and CHALET, the environment used for
CHAI, is larger and has more complex manipu-
lation compared to similar environments (Gordon
et al., 2018; Das et al., 2018). In addition, only
synthetic language data has been released for these
environment. An exception is the Room-to-Room
dataset (Anderson et al., 2018) that makes use of
an environment of connected panoramas of house
settings. Although it provides a realistic vision
challenge, unlike our environments, the state space
is limited to a small number of panoramas and ma-
nipulation is not possible.

4 Model

We model the agent policy as a neural network.
The agent observes the world state st at time t as
an RGB image It. The agent context ˜st, the infor-
mation available to the agent to select the next ac-
tion at, is a tuple (¯x, IP , (cid:104)(I1, p1), . . . , (It, pt)(cid:105)),
where ¯x is the natural
language instructions,
IP is a panoramic view of the environment
from the starting position at time t = 1, and
(cid:104)(I1, p1), . . . , (It, pt)(cid:105) is the sequence of observa-
tions It and poses pt up to time t. The panorama
IP is generated through deterministic exploration
by rotating 360◦ to observe the environment at the
beginning of the execution.1

The model includes two main components: goal
prediction and action generation. The agent uses
the panorama IP to predict the goal location lg. At
each time step t, a projection of the goal location
into the agent’s current view Mt is given as input
to an RNN to generate actions. The probability of
an action at at time t decomposes to:
(cid:16)

P (at | ˜st) =

P (lg | ¯x, IP )

(cid:88)

lg

P (at | lg, (I1, p1), . . . , (It, pt))

,

(cid:17)

where the ﬁrst term puts the complete distribution
mass on a single location (i.e., a delta function).
Figure 2 illustrates the model.

1The panorama is a concatenation of deterministic obser-
vations along the width dimension. For simplicity, we do not
include these deterministic steps in the execution.

Figure 2: An illustration for our architecture (Section 4) for the instruction turn left and go to the red oil drum
with a LINGUNET depth of m = 4. The instruction ¯x is mapped to ¯x with an RNN, and the initial panorama
observation IP to F0 with a CNN. LINGUNET generates H1, a visual representation of the goal. First, a sequence
of convolutions maps the image features F0 to feature maps F1,. . . ,F4. The text representation ¯x is used to
generate the kernels K1,. . . ,K4, which are convolved to generate the text-conditioned feature maps G1,. . . ,G4.
These feature maps are de-convolved to H1,. . . ,H4. The goal probability distribution Pg is computed from H1.
The goal location is the inferred from the max of Pg. Given lg and pt, the pose at step t, the goal mask Mt is
computed and passed into an RNN that outputs the action to execute.

Goal Prediction To predict the goal location,
we generate a probability distribution Pg over
a feature map F0 generated using convolutions
from the initial panorama observation IP . Each
element in the probability distribution Pg corre-
sponds to an area in IP . Given the instruction
¯x and panorama IP , we ﬁrst generate their rep-
resentations. From the panorama IP , we gener-
ate a feature map F0 = [CNN0(IP ); Fp], where
CNN0 is a two-layer convolutional neural net-
work (CNN; LeCun et al., 1998) with rectiﬁed
linear units (ReLU; Nair and Hinton, 2010) and
Fp are positional embeddings.2 The concatena-
tion is along the channel dimension. The instruc-
tion ¯x = (cid:104)x1, · · · xn(cid:105) is mapped to a sequence
of hidden states li = LSTMx(ψx(xi), li−1), i =
1, . . . , n using a learned embedding function ψx
and a long short-term memory (LSTM; Hochre-
iter and Schmidhuber, 1997) RNN LSTMx. The
instruction representation is ¯x = ln.

We generate the probability distribution Pg over
pixels in F0 using LINGUNET. The architecture
of LINGUNET is inspired by the U-NET image
generation method (Ronneberger et al., 2015), ex-
cept that the reconstruction phase is conditioned
on the natural language instruction. LINGUNET
ﬁrst applies m convolutional layers to generate a
sequence of feature maps Fj = CNNj(Fj−1),

2We generate Fp by creating a channel for each determin-
istic observation used to create the panorama, and setting all
the pixels corresponding to that observation location in the
panorama to 1 and all others to 0. The number of observa-
tions depends on the agent’s camera angle.

j = 1 . . . m, where each CNNj is a convolutional
layer with leaky ReLU non-linearities (Maas et al.,
2013) and instance normalization (Ulyanov et al.,
2016). The instruction representation ¯x is split
evenly into m vectors {¯xj}m
j=1, each is used to
create a 1 × 1 kernel Kj = AFFINEj(¯xj), where
each AFFINEj is an afﬁne transformation followed
by normalizing and reshaping. For each Fj, we
apply a 2D 1 × 1 convolution using the text ker-
nel Kj to generate a text-conditioned feature map
Gj = CONVOLVE(Kj, Fj), where CONVOLVE
convolves the kernel over the feature map. We
then perform m deconvolutions to generate a se-
quence of feature maps Hm,. . . ,H1:

Hm = DECONVm(DROPOUT(Gm))
Hj = DECONVj([Hj+1; Gj]) .

DROPOUT is dropout regularization (Srivastava
et al., 2014) and each DECONVj
is a decon-
volution operation followed a leaky ReLU non-
linearity and instance norm.3 Finally, we gener-
ate Pg by applying a softmax to H1 and an ad-
ditional learned scalar bias term bg to represent
events where the goal is out of sight. For example,
when the agent already stands in the goal position
and therefore the panorama does not show it.

We use Pg to predict the goal position in the
environment. We ﬁrst select the goal pixel in F0 as
the pixel corresponding to the highest probability
element in Pg. We then identify the corresponding
3D location lg in the environment using backward
camera projection, which is computed given the

3DECONV1 does deconvolution only.

camera parameters and p1, the agent pose at the
beginning of the execution.
Action Generation Given the predicted goal lg,
we generate actions using an RNN. At each time
step t, given pt, we generate the goal mask Mt,
which has the same shape as the observed image
It. The goal mask Mt has a value of 1 for each
element that corresponds to the goal location lg in
It. We do not distinguish between visible or oc-
cluded locations. All other elements are set to 0.
We also maintain an out-of-sight ﬂag ot that is set
to 1 if (a) lg is not within the agent’s view; or (b)
the max scoring element in Pg corresponds to bg,
the term for events when the goal is not visible in
IP . Otherwise, ot is set to 0. We compute an ac-
tion generation hidden state yt with an RNN:

yt = LSTMA (AFFINEA([FLAT(Mt); ot]), yt−1) ,

where FLAT ﬂattens Mt into a vector, AFFINEA
is a learned afﬁne transformation with ReLU, and
LSTMA is an LSTM RNN. The previous hidden
state yt−1 was computed when generating the pre-
vious action, and the RNN is extended gradually
during execution. Finally, we compute a probabil-
ity distribution over actions:

P (at | lg, (I1, p1), . . . , (It, pt)) =

SOFTMAX(AFFINEp([yt; ψT (t)])) ,

where ψT is a learned embedding lookup table
for the current time (Chaplot et al., 2018) and
AFFINEp is a learned afﬁne transformation.
Model Parameters The model parameters θ in-
clude the parameters of the convolutions CNN0
and the components of LINGUNET: CNNj,
for j = 1, . . . , m.
AFFINEj, and DECONVj
In addition we learn two afﬁne transformations
AFFINEA and AFFINEp, two RNNs LSTMx and
LSTMA, two embedding functions ψx and ψT ,
and the goal distribution bias term bg. In our ex-
periments (Section 7), all parameters are learned
without external resources.

5 Learning

Our modeling decomposition enables us to choose
different learning algorithms for the two parts.
While reinforcement learning is commonly de-
ployed for tasks that beneﬁt from exploration (e.g.,
Peters and Schaal, 2008; Mnih et al., 2013), these
methods require many samples due to their high
sample complexity. However, when learning with
natural language, only a relatively small number
of samples is realistically available. This problem

was addressed in prior work by learning in a con-
textual bandit setting (Misra et al., 2017) or mix-
ing reinforcement and supervised learning (Xiong
et al., 2018). Our decomposition uniquely offers
to tease apart the language understanding prob-
lem and address it with supervised learning, which
generally has lower sample complexity. For action
generation though, where exploration can be au-
tonomous, we use policy gradient in a contextual
bandit setting (Misra et al., 2017).

(i)
1

(i)
1 , s

(i)
g )}N

We assume access to training data with N ex-
i=1, where ¯x(i) is an in-
amples {(¯x(i), s
(i)
struction, s
is the goal
is a start state, and s
g
state. We train the goal prediction component by
minimizing the cross-entropy of the predicted dis-
tribution with the gold-standard goal distribution.
The gold-standard goal distribution is a determin-
istic distribution with probability one at the pixel
corresponding to the goal location if the goal is in
the ﬁeld of view, or probability one at the extra
out-of-sight position otherwise. The gold location
(i)
is the agent’s location in s
g . We update the model
parameters using Adam (Kingma and Ba, 2014).

We train action generation by maximizing the
expected immediate reward the agent observes
while exploring the environment. The objective
for a single example i and time stamp t is:

J =

π(a | ˜st)R(i)(st, a) + λH(π(. | ˜st)) ,

(cid:88)

a∈A

where R(i) : S × A → R is an example-speciﬁc
reward function, H(·) is an entropy regularization
term, and λ is the regularization coefﬁcient. The
reward function R(i) details are described in de-
tails in Appendix B. Roughly speaking, the re-
ward function includes two additive components:
a problem reward and a shaping term (Ng et al.,
1999). The problem reward provides a positive re-
ward for successful task completion, and a nega-
tive reward for incorrect completion or collision.
The shaping term is positive when the agent gets
closer to the goal position, and negative if it is
moving away. The gradient of the objective is:

∇J =

π(a | ˜st)∇ log π(a | ˜st)R(st, a)

(cid:88)

a∈A

+λ∇H(π(. | ˜st) .

We approximate the gradient by sampling an ac-
tion using the policy (Williams, 1992), and use the
(i)
gold goal location computed from s
g . We per-
form several parallel rollouts to compute gradients
and update the parameters using Hogwild! (Recht
et al., 2011) and Adam learning rates.

Dataset Statistic
Number paragraphs
Mean instructions per paragraph
Mean actions per instruction
Mean tokens per instruction
Vocabulary size

LANI
6,000
4.7
24.6
12.1
2,292

CHAI
1,596
7.70
54.5
8.4
1,018

Table 1: Summary statistics of the two corpora.

6 Tasks and Data

6.1 LANI

The goal of LANI is to evaluate how well an agent
can follow navigation instructions. The agent task
is to follow a sequence of instructions that specify
a path in an environment with multiple landmarks.
Figure 1 (left) shows an example instruction.

The environment

is a fenced, square, grass
ﬁeld. Each instance of the environment con-
tains between 6–13 randomly placed landmarks,
sampled from 63 unique landmarks. The agent
can take four types of discrete actions: FORWARD,
TURNRIGHT, TURNLEFT, and STOP. The ﬁeld is
of size 50×50, the distance of the FORWARD ac-
tion is 1.5, and the turn angle is 15◦. The en-
vironment simulator is implemented in Unity3D.
At each time step, the agent performs an action,
observes a ﬁrst person view of the environment
as an RGB image, and receives a scalar reward.
The simulator provides a socket API to control the
agent and the environment.

Agent performance is evaluated using two met-
rics: task completion accuracy, and stop distance
error. A task is completed correctly if the agent
stops within an aerial distance of 5 from the goal.
We collect a corpus of navigation instructions
using crowdsourcing. We randomly generate en-
vironments, and generate one reference path for
each environment. To elicit linguistically interest-
ing instructions, reference paths are generated to
pass near landmarks. We use Amazon Mechanical
Turk, and split the annotation process to two tasks.
First, given an environment and a reference path,
a worker writes an instruction paragraph for fol-
lowing the path. The second task requires another
worker to control the agent to perform the instruc-
tions and simultaneously mark at each point what
part of the instruction was executed. The record-
ing of the second worker creates the ﬁnal data of
segmented instructions and demonstrations. The
generated reference path is displayed in both tasks.
The second worker could also mark the paragraph
as invalid. Both tasks are done from an over-
head view of the environment, but workers are in-
structed to provide instructions for a robot that ob-

[Go around the pillar on the right hand side] [and head
towards the boat, circling around it clockwise.] [When
you are facing the tree, walk towards it, and the pass on
the right hand side,] [and the left hand side of the cone.
Circle around the cone,] [and then walk past the hydrant
on your right,] [and the the tree stump.] [Circle around
the stump and then stop right behind it.]

Figure 3: Segmented instructions in the LANI domain.
The original reference path is marked in red (start) and
blue (end). The agent, using a drone icon, is placed at
the beginning of the path. The follower path is coded in
colors to align to the segmented instruction paragraph.

serves the environment from a ﬁrst person view.
Figure 3 shows a reference path and the written
instruction. This data can be used for evaluating
both executing sequences of instructions and sin-
gle instructions in isolation.

Table 1 shows the corpus statistics.4 Each para-
graph corresponds to a single unique instance of
the environment. The paragraphs are split into
train, test, and development, with a 70% / 15% /
15% split. Finally, we sample 200 single devel-
opment instructions for qualitative analysis of the
language challenge the corpus presents (Table 2).

6.2 CHAI

The CHAI corpus combines both navigation and
simple manipulation in a complex, simulated
household environment. We use the CHALET sim-
ulator (Yan et al., 2018), a 3D house simulator
that provides multiple houses, each with multi-
ple rooms. The environment supports moving be-
tween rooms, picking and placing objects, and
opening and closing cabinets and similar contain-
ers. Objects can be moved between rooms and
in and out of containers. The agent observes the
world in ﬁrst-person view, and can take ﬁve ac-
tions: FORWARD, TURNLEFT, TURNRIGHT, STOP,
and INTERACT. The INTERACT action acts on ob-
jects.
It takes as argument a 2D position in the
agent’s view. Agent performance is evaluated with
two metrics: (a) stop distance, which measures the
distance of the agent’s ﬁnal state to the ﬁnal an-
notated position; and (b) manipulation accuracy,
which compares the set of manipulation actions

4Appendix A provides statistics for related datasets.

Category
Spatial relations
between locations
Conjunctions of two
more locations
Temporal coordination
of sub-goals
Constraints on the
shape of trajectory

Co-reference

Comparatives

Count

LANI

CHAI

123

36

65

94

32

2

52

68

5

0

18

0

Example
LANI: go to the right side of the rock
CHAI: pick up the cup next to the bathtub and place it on . . .
LANI: ﬂy between the mushroom and the yellow cone
CHAI: . . . set it on the table next to the juice and milk.
LANI: at the mushroom turn right and move forward towards the statue
CHAI: go back to the kitchen and put the glass in the sink.

LANI: go past the house by the right side of the apple

LANI: turn around it and move in front of fern plant
CHAI: turn left, towards the kitchen door and move through it.
LANI: . . . the small stone closest to the blue and white fences stop

Table 2: Qualitative analysis of the LANI and CHAI corpora. We sample 200 single development instructions from
each corpora. For each category, we count how many examples of the 200 contained it and show an example.

Scenario
You have several hours before guests begin to arrive for
a dinner party. You are preparing a wide variety of meat
dishes, and need to put them in the sink.
In addition,
you want to remove things in the kitchen, and bathroom
which you don’t want your guests seeing, like the soaps
in the bathroom, and the dish cleaning items. You can
put these in the cupboards. Finally, put the dirty dishes
around the house in the dishwasher and close it.
Written Instructions
[In the kitchen, open the cupboard above the sink.] [Put
the cereal, the sponge, and the dishwashing soap into the
cupboard above the sink.] [Close the cupboard.] [Pick
up the meats and put them into the sink.] [Open the dish-
washer, grab the dirty dishes on the counter, and put the
dishes into the dishwasher.]

Figure 4: Scenario and segmented instruction from the
CHAI corpus.

to a reference set. When measuring distance, to
consider the house plan, we compute the minimal
aerial distance for each room that must be visited.
Yan et al. (2018) provides the full details of the
simulator and evaluation. We use ﬁve different
houses, each with up to six rooms. Each room
contains on average 30 objects. A typical room
is of size 6×6. We set the distance of FORWARD to
0.1, the turn angle to 90◦, and divide the agent’s
view to a 32×32 grid for the INTERACT action.

We collected a corpus of navigation and ma-
nipulation instructions using Amazon Mechanical
Turk. We created 36 common household scenar-
ios to provide a familiar context to the task.5 We
use two crowdsourcing tasks. First, we provide
workers with a scenario and ask them to write in-
structions. The workers are encouraged to explore
the environment and interact with it. We then seg-
ment the instructions to sentences automatically.
In the second task, workers are presented with the
segmented sentences in order and asked to execute
them. After ﬁnishing a sentence, the workers re-

5We observed that asking workers to simply write instruc-
tions without providing a scenario leads to combinations of
repetitive instructions unlikely to occur in reality.

quest the next sentence. The workers do not see
the original scenario. Figure 4 shows a scenario
and the written segmented paragraph. Similar to
LANI, CHAI data can be used for studying com-
plete paragraphs and single instructions.

Table 1 shows the corpus statistics.6 The para-
graphs are split into train, test, and development,
with a 70% / 15% / 15% split. Table 2 shows qual-
itative analysis of a sample of 200 instructions.

7 Experimental Setup

instructions.

Method Adaptations for CHAI We apply two
modiﬁcations to our model to support interme-
diate goal for the CHAI
First,
we train an additional RNN to predict the se-
quence of intermediate goals given the instruc-
There are two types of goals:
tion only.
NAVIGATION,
for action sequences requiring
movement only and ending with the STOP action;
and INTERACTION, for sequence of movement ac-
tions that end with an INTERACT action. For ex-
ample, for the instruction pick up the red book
and go to the kitchen, the sequence of goals will
be (cid:104)INTERACTION, NAVIGATION, NAVIGATION(cid:105).
This indicates the agent must ﬁrst move to the
object to pick it up via interaction, move to the
kitchen door, and ﬁnally move within the kitchen.
The process of executing an instruction starts with
predicting the sequence of goal types. We call our
model (Section 4) separately for each goal type.
The execution concludes when the ﬁnal goal is
completed. For learning, we create a separate ex-
ample for each intermediate goal and train the ad-
ditional RNN separately. The second modiﬁcation
is replacing the backward camera projection for
inferring the goal location with ray casting to iden-

6The number of actions per instruction is given in the
more ﬁne-grained action space used during collection. To
make the required number of actions smaller, we use the more
coarse action space speciﬁed.

Method
STOP
RANDOMWALK
MOSTFREQUENT
MISRA17
CHAPLOT18
Our Approach (OA)
OA w/o RNN
OA w/o Language
OA w/joint
OA w/oracle goals

LANI

CHAI

SD
15.37
14.80
19.31
10.54
9.05
8.65
9.21
10.65
11.54
2.13

TC
8.20
9.66
2.94
22.9
31.0
35.72
31.30
23.02
21.76
94.60

SD
2.99
2.99
3.80
2.99
2.99
2.75
3.75
3.22
2.99
2.19

MA
37.53
28.96
37.53
32.25
37.53
37.53
37.43
37.53
36.90
41.07

Table 3: Performance on the development data.

tify INTERACTION goals, which are often objects
that are not located on the ground.
Baselines We compare our approach against the
following baselines: (a) STOP: Agent stops im-
mediately; (b) RANDOMWALK: Agent samples
actions uniformly until it exhausts the horizon
or stops; (c) MOSTFREQUENT: Agent takes the
most frequent action in the data, FORWARD for
both datasets, until it exhausts the horizon; (d)
the approach of Misra et al. (2017);
MISRA17:
and (e) CHAPLOT18:
the approach of Chaplot
et al. (2018). We also evaluate goal prediction and
compare to the method of Janner et al. (2018) and
a CENTER baseline, which always predict the cen-
ter pixel. Appendix C provides baseline details.
Evaluation Metrics We evaluate using the met-
rics described in Section 6: stop distance (SD) and
task completion (TC) for LANI, and stop distance
(SD) and manipulation accuracy (MA) for CHAI.
To evaluate the goal prediction, we report the real
distance of the predicted goal from the annotated
goal and the percentage of correct predictions. We
consider a goal correct if it is within a distance of
5.0 for LANI and 1.0 for CHAI. We also report
human evaluation for LANI by asking raters if the
generated path follows the instruction on a Likert-
type scale of 1–5. Raters were shown the gener-
ated path, the reference path, and the instruction.
Parameters We use a horizon of 40 for both
domains. During training, we allow additional
5 steps to encourage learning even after errors.
When using intermediate goals in CHAI, the hori-
zon is used for each intermediate goal separately.
All other parameters and detailed in Appendix D.

8 Results

Tables 3 and 4 show development and test re-
sults. Both sets of experiments demonstrate sim-
ilar trends. The low performance of STOP, RAN-
DOMWALK, and MOSTFREQUENT demonstrates

Method
STOP
RANDOMWALK
MOSTFREQUENT
MISRA17
CHAPLOT18
Our Approach

LANI

CHAI

SD
15.18
14.63
19.14
10.23
8.78
8.43

TC
8.29
9.76
3.15
23.2
31.9
36.9

SD
3.59
3.59
4.36
3.59
3.59
3.34

MA
39.77
33.29
39.77
36.84
39.76
39.97

Table 4: Performance on the held-out test dataset.

Method
CENTER
Janner et al. (2018)
Our Approach

LANI

CHAI

Dist
12.0
9.61
8.67

Acc
19.51
30.26
35.83

Dist
3.41
2.81
2.12

Acc
19.0
28.3
40.3

Table 5: Development goal prediction performance.
We measure distance (Dist) and accuracy (Acc).

the challenges of both tasks, and shows the tasks
are robust to simple biases. On LANI, our ap-
proach outperforms CHAPLOT18, improving task
completion (TC) accuracy by 5%, and both meth-
ods outperform MISRA17. On CHAI, CHAP-
LOT18 and MISRA17 both fail to learn, while
our approach shows an improvement on stop dis-
tance (SD). However, all models perform poorly
on CHAI, especially on manipulation (MA).

To isolate navigation performance on CHAI, we
limit our train and test data to instructions that in-
clude navigation actions only. The STOP baseline
on these instructions gives a stop distance (SD) of
3.91, higher than the average for the entire data
as these instructions require more movement. Our
approach gives a stop distance (SD) of 3.24, a 17%
reduction of error, signiﬁcantly better than the 8%
reduction of error over the entire corpus.

We also measure human performance on a sam-
ple of 100 development examples for both tasks.
On LANI, we observe a stop distance error (SD)
of 5.2 and successful task completion (TC) 63%
of the time. On CHAI, the human distance er-
ror (SD) is 1.34 and the manipulation accuracy is
100%. The imperfect performance demonstrates
the inherent ambiguity of the tasks. The gap to
human performance is still large though, demon-
strating that both tasks are largely open problems.
The imperfect human performance raises ques-
tions about automated evaluation.
In general,
we observe that often measuring execution qual-
ity with rigid goals is insufﬁcient. We conduct
a human evaluation with 50 development exam-
ples from LANI rating human performance and
our approach. Figure 5 shows a histogram of the
ratings. The mean rating for human followers is
4.38, while our approach’s is 3.78; we observe
a similar trend to before with this metric. Using

Category
Spatial relations
Location conjunction
Temporal coordination
Trajectory constraints
Co-reference
Comparatives

Present Absent
10.09
9.05
8.24
8.99
8.59
9.25

8.75
10.19
11.38
9.56
12.88
10.22

p-value
.262
.327
.015
.607
.016
.906

Table 6: Mean goal prediction error for LANI instruc-
tions with and without the analysis categories we used
in Table 2. The p-values are from two-sided t-tests
comparing the means in each row.

e
g
a
t
n
e
c
r
e
P

60

40

20

0

Human
Our Approach

1

2

3

4

5

Figure 5: Likert rating histogram for expert human fol-
lower and our approach for LANI.

judgements on our approach, we correlate the hu-
man metric with the SD measure. We observe a
Pearson correlation -0.65 (p=5e-7), indicating that
our automated metric correlates well with human
judgment.7 This initial study suggests that our au-
tomated evaluation is appropriate for this task.

Our ablations (Table 3) demonstrate the impor-
tance of each of the components of the model.
We ablate the action generation RNN (w/o RNN),
completely remove the language input (w/o Lan-
guage), and train the model jointly (w/joint Learn-
ing).8 On CHAI especially, ablations results in
models that display ineffective behavior. Of the
ablations, we observe the largest beneﬁt from
decomposing the learning and using supervised
learning for the language problem.

We also evaluate our approach with access to
oracle goals (Table 3). We observe this im-
proves navigation performance signiﬁcantly on
both tasks. However, the model completely fails
to learn a reasonable manipulation behavior for
CHAI. This illustrates the planning complexity
of this domain. A large part of the improvement
in measured navigation behavior is likely due to
eliminating much of the ambiguity the automated
metric often fails to capture.

Finally, on goal prediction (Table 5), our ap-
proach outperforms the method of Janner et al.
(2018). Figure 6 and Appendix Figure 7 show ex-
ample goal predictions. In Table 6, we break down
LANI goal prediction results for the analysis cate-

7We did not observe this kind of clear anti-correlation
comparing the two results for human performance (Pearson
correlation of 0.09 and p=0.52). The limited variance in hu-
man performance makes correlation harder to test.

8Appendix C provides the details of joint learning.

curve around big rock keeping it to your left .

walk over to the cabinets and open the cabinet doors up

Figure 6: Goal prediction probability maps Pg overlaid
on the corresponding observed panoramas IP . The top
example shows a result on LANI, the bottom on CHAI.
gories we used in Table 2 using the same sample of
the data. Appendix E includes a similar table for
CHAI. We observe that our approach ﬁnds instruc-
tions with temporal coordination or co-reference
challenging. Co-reference is an expected limita-
tion; with single instructions, the model can not
resolve references to previous instructions.
9 Discussion
We propose a model for instruction following with
explicit separation of goal prediction and action
generation. Our representation of goal prediction
is easily interpretable, while not requiring the de-
sign of logical ontologies and symbolic represen-
tations. A potential limitation of our approach is
cascading errors. Action generation relies com-
pletely on the predicted goal and is not exposed
to the language otherwise. This also suggests a
second related limitation:
the model is unlikely
to successfully reason about instructions that in-
clude constraints on the execution itself. While
the model may reach the ﬁnal goal correctly, it is
unlikely to account for the intermediate trajectory
constraints. As we show (Table 2), such instruc-
tions are common in our data. These two limita-
tions may be addressed by allowing action genera-
tion access to the instruction. Achieving this while
retaining an interpretable goal representation that
clearly determines the execution is an important
direction for future work. Another important open
question concerns automated evaluation, which re-
mains especially challenging when instructions do
not only specify goals, but also constraints on how
to achieve them. Our resources provide the plat-
form and data to conduct this research.
Acknowledgments
This research was supported by NSF (CAREER-
1750499), Schmidt Sciences, a Google Faculty
Award, and cloud credits from Microsoft. We
thank John Langford, Claudia Yan, Bharath Har-
iharan, Noah Snavely, the Cornell NLP group, and
the anonymous reviewers for their advice.

References
Anne H Anderson, Miles Bader, Ellen Gurman Bard,
Elizabeth Boyle, Gwyneth Doherty, Simon Garrod,
Stephen Isard, Jacqueline Kowtko, Jan McAllister,
Jim Miller, Catherine Sotillo, Henry S. Thompson,
and Regina Weinert. 1991. The HCRC map task
corpus. Language and Speech, 34.

Peter Anderson, Qi Wu, Damien Teney, Jake Bruce,
Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen
Gould, and Anton van den Hengel. 2018. Vision-
and-language navigation:
Interpreting visually-
grounded navigation instructions in real environ-
ments. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition.

Yoav Artzi, Dipanjan Das, and Slav Petrov. 2014.
Learning compact lexicons for CCG semantic pars-
ing. In Proceedings of the 2014 Conference on Em-
pirical Methods in Natural Language Processing.

Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion of Computational Linguistics, 1.

Yonatan Bisk, Daniel Marcu, and William Wong. 2016.
Towards a dataset for human computer communica-
tion via grounded language acquisition. In Proceed-
ings of the AAAI Workshop on Symbiotic Cognitive
Systems.

Devendra Singh Chaplot, Kanthashree Mysore
Sathyendra, Rama Kumar Pasumarthi, Dheeraj
Rajagopal, and Ruslan Salakhutdinov. 2018. Gated-
attention architectures for task-oriented language
grounding.

David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the Na-
tional Conference on Artiﬁcial Intelligence.

Deborah A Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the ATIS
In Proceedings of the
task: The ATIS-3 corpus.
workshop on Human Language Technology.

Abhishek Das, Samyak Datta, Georgia Gkioxari, Ste-
fan Lee, Devi Parikh, and Dhruv Batra. 2018. Em-
In Proceedings of the
bodied question answering.
IEEE Conference on Computer Vision and Pattern
Recognition.

Daniel Fried, Ronghang Hu, Volkan Cirik, Anna
Rohrbach, Jacob Andreas, Louis-Philippe Morency,
Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein,
and Trevor Darrell. 2018. Speaker-follower mod-
CoRR,
els for vision-and-language navigation.
abs/1806.02724.

Daniel Gordon, Aniruddha Kembhavi, Mohammad
Rastegari, Joseph Redmon, Dieter Fox, and Ali

Farhadi. 2018. Iqa: Visual question answering in in-
teractive environments. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition.

Charles T. Hemphill, John J. Godfrey, and George R.
Doddington. 1990. The ATIS spoken language sys-
In Proceedings of the DARPA
tems pilot corpus.
speech and natural language workshop.

Karl Moritz Hermann, Felix Hill, Simon Green, Fumin
Wang, Ryan Faulkner, Hubert Soyer, David Szepes-
vari, Wojciech Czarnecki, Max Jaderberg, Denis
Teplyashin, Marcus Wainwright, Chris Apps, Demis
Hassabis, and Phil Blunsom. 2017. Grounded lan-
guage learning in a simulated 3D world. CoRR,
abs/1706.06551.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long

short-term memory. Neural computation, 9.

Michael Janner, Karthik Narasimhan, and Regina
Barzilay. 2018.
Representation learning for
grounded spatial reasoning. Transactions of the As-
sociation for Computational Linguistics, 6.

Sahar Kazemzadeh, Vicente Ordonez, Mark Matten,
and Tamara L. Berg. 2014. Referitgame: Referring
to objects in photographs of natural scenes. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.

Joohyun Kim and Raymond Mooney. 2012. Unsuper-
vised PCFG induction for grounded language learn-
ing with highly ambiguous supervision. In Proceed-
ings of the Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. In Proceedings
of the International Conference on Learning Repre-
sentations.

Nikita Kitaev and Dan Klein. 2017. Where is misty?
interpreting spatial descriptors by modeling regions
in space. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.

Chen Kong, Dahua Lin, Mohit Bansal, Raquel Urta-
sun, and Sanja Fidler. 2014. What are you talking
about? text-to-image coreference. In Proceedings of
the IEEE Conference on Computer Vision and Pat-
tern Recognition.

Jayant Krishnamurthy and T. Kollar. 2013.

Jointly
learning to parse and perceive: Connecting natural
language to the physical world. Transactions of the
Association for Computational Linguistics, 1.

Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86.

Andrew L Maas, Awni Y Hannun, and Andrew Y Ng.
2013. Rectiﬁer nonlinearities improve neural net-
work acoustic models. In Proceedings of the inter-
national conference on machine learning.

James MacGlashan, Monica Babes-Vroman, Marie
desJardins, Michael L. Littman, Smaranda Muresan,
S Bertel Squire, Stefanie Tellex, Dilip Arumugam,
and Lei Yang. 2015. Grounding english commands
to reward functions. In Robotics: Science and Sys-
tems.

Matthew MacMahon, Brian Stankiewics, and Ben-
jamin Kuipers. 2006. Walk the talk: Connecting
language, knowledge, action in route instructions.
In Proceedings of the National Conference on Ar-
tiﬁcial Intelligence.

Junhua Mao, Jonathan Huang, Alexander Toshev, Oana
Camburu, Alan Yuille, and Kevin Murphy. 2016.
Generation and Comprehension of Unambiguous
Object Descriptions. In Proceedings of IEEE Con-
ference on Computer Vision and Pattern Recogni-
tion.

Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012a. A joint
model of language and perception for grounded at-
tribute learning. In Proceedings of the International
Conference on Machine Learning.

Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer,
and Dieter Fox. 2012b. Learning to parse natural
language commands to a robot control system.
In
Proceedings of the International Symposium on Ex-
perimental Robotics.

Hongyuan Mei, Mohit Bansal, and R. Matthew Walter.
2016. What to talk about and how? selective gener-
ation using lstms with coarse-to-ﬁne alignment. In
Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies.

Dipendra Misra, John Langford, and Yoav Artzi. 2017.
Mapping instructions and visual observations to ac-
In Proceedings
tions with reinforcement learning.
of the Conference on Empirical Methods in Natural
Language Processing.

Dipendra K. Misra, Jaeyong Sung, Kevin Lee, and
Ashutosh Saxena. 2016. Tell me dave: Context-
sensitive grounding of natural language to manip-
ulation instructions. The International Journal of
Robotics Research, 35.

Kumar Dipendra Misra, Kejia Tao, Percy Liang, and
Ashutosh Saxena. 2015. Environment-driven lexi-
In Pro-
con induction for high-level instructions.
ceedings of the Annual Meeting of the Association
for Computational Linguistics and the International
Joint Conference on Natural Language Processing.

deep reinforcement learning. In Advances in Neural
Information Processing Systems.

Vinod Nair and Geoffrey E Hinton. 2010. Rectiﬁed lin-
ear units improve restricted boltzmann machines. In
Proceedings of the international conference on ma-
chine learning.

Andrew Y. Ng, Daishi Harada, and Stuart J. Russell.
1999. Policy invariance under reward transforma-
tions: Theory and application to reward shaping. In
Proceedings of the International Conference on Ma-
chine Learning.

Junhyuk Oh, Satinder P. Singh, Honglak Lee, and
Pushmeet Kohli. 2017. Zero-shot task generaliza-
tion with multi-task deep reinforcement learning. In
Proceedings of the international conference on ma-
chine learning.

Jan Peters and Stefan Schaal. 2008. Reinforcement
learning of motor skills with policy gradients. Neu-
ral networks, 21.

Benjamin Recht, Christopher Re, Stephen Wright, and
Feng Niu. 2011. Hogwild: A lock-free approach
to parallelizing stochastic gradient descent. In Ad-
vances in Neural Information Processing Systems.

Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
2015. U-net: Convolutional networks for biomed-
In International Confer-
ical image segmentation.
ence on Medical image computing and computer-
assisted intervention.

John Schulman, Philipp Moritz, Sergey Levine,
Michael I. Jordan, and Pieter Abbeel. 2015. High-
dimensional continuous control using generalized
advantage estimation. CoRR, abs/1506.02438.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overﬁtting. The Journal of Machine Learning
Research, 15.

Alane Suhr and Yoav Artzi. 2018. Situated mapping
of sequential instructions to actions with single-step
In Proceedings of the Annual
reward observation.
Meeting of the Association for Computational Lin-
guistics.

Richard S. Sutton, Doina Precup, and Satinder P. Singh.
Intra-option learning about temporally ab-
In Proceedings of the international

1998.
stract actions.
conference on machine learning.

Dmitry Ulyanov, Andrea Vedaldi, and Victor S.
Instance normalization: The
CoRR,

Lempitsky. 2016.
missing ingredient for fast stylization.
abs/1607.08022.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Alex Graves, Ioannis Antonoglou, Daan Wierstra,
and Martin A. Riedmiller. 2013. Playing atari with

Ronald J. Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine Learning, 8.

Wenhan Xiong, Xiaoxiao Guo, Mo Yu, Shiyu Chang,
Bowen Zhou, and William Yang Wang. 2018.
Scheduled policy optimization for natural language
communication with intelligent agents. In Proceed-
ings of the International Joint Conferences on Arti-
ﬁcial Intelligence.

Claudia Yan, Dipendra Kumar Misra, Andrew Ben-
nett, Aaron Walsman, Yonatan Bisk, and Yoav Artzi.
2018. Chalet: Cornell house agent learning environ-
ment. CoRR, abs/1801.07357.

Licheng Yu, Patrick Poirson, Shan Yang, Alexander C.
Berg, and Tamara L. Berg. 2016. Modeling context
in referring expressions. In Proceedings of the Eu-
ropean Conference on Computer Vision.

A Tasks and Data: Comparisons

Table 7 provides summary statistics comparing
LANI and CHAI to existing related resources.

B Reward Function

LANI Following Misra et al. (2017), we use a
shaped reward function that rewards the agent for
moving towards the goal location. The reward for
exampl i is:

R(i)(s, a, s(cid:48)) = R(i)

p + φ(i)(s) − φ(i)(s(cid:48))

(1)

where s(cid:48) is the origin state, a is the action, s is
(i)
is the problem reward, and
the target state, R
p
φ(i)(s)−φ(i) is a shaping term. We use a potential-
based shaping (Ng et al., 1999) that encourages the
agent to both move and turn towards the goal. The
potential function is:
φ(i)(s) =

δTURNDIST(s, s(i)
g )
+(1 − δ)MOVEDIST(s, s(i)

g ) ,

where MOVEDIST is the euclidean distance to the
goal normalized by the agent’s forward movement
distance, and TURNDIST is the angle the agent
needs to turn to face the goal normalized by the
agent’s turn angle. We use δ as a gating term,
which is 0 when the agent is near the goal and
increases monotonically towards 1 the further the
agent is from the goal. This decreases the sensi-
tivity of the potential function to the TURNDIST
(i)
term close to the goal. The problem reward R
p
provides a negative reward of up to -1 on collision
with any object or boundary (based on the angle
and magnitude of collision), a negative reward of
-0.005 on every action to discourage long trajec-
tories, a negative reward of -1 on an unsuccess-
ful stop, when the distance to the goal location is
greater than 5, and a positive reward of +1 on a
successful stop.
CHAI We use a similar potential based reward
function as LANI. Instead of rewarding the agent
to move towards the ﬁnal goal the model is re-
warded for moving towards the next intermedi-
ate goal. We heuristically generate intermediate
goals from the human demonstration by generat-
ing goals for objects to be interacted with, doors
that the agent should enter, and the ﬁnal position
of the agent. The potential function is:

φ(i)(s) =

TURNDIST(s, s(i)
MOVEDIST(s, s(i)

g,j) +
g,j) + INTDIST(s, s(i)

g,j) ,

(i)
g,j

where s
intermediate goal,
TURNDIST rewards the agent for turning to-

the next

is

wards the goal, MOVEDIST rewards the agent for
moving closer to the goal, and INTDIST rewards
the agent for accomplishing the interaction in the
intermediate goal. The goal is updated on being
accomplished. Besides the potential term, we
(i)
use a problem reward R
that gives a reward of
p
1 for stopping near a goal, -1 for colliding with
obstacles, and -0.002 as a verbosity penalty for
each step.

C Baseline Details

MISRA17 We use the model of Misra et al.
(2017). The model uses a convolution neural net-
work for encoding the visual observations, a re-
current neural network with LSTM units to en-
code the instruction, and a feed-forward network
to generate actions using these encodings. The
model is trained using policy gradient in a con-
textual bandit setting. We use the code provided
by the authors.
CHAPLOT18 We use the gated attention archi-
tecture of Chaplot et al. (2018). The model is
trained using policy gradient with generalized ad-
vantage estimation (Schulman et al., 2015). We
use the code provided by the authors.
Our Approach with Joint Training We train
the full model with policy gradient. We maxi-
mize the expected reward objective with entropy
regularization. Given a sampled goal location
lg ∼ p(. | ¯x, IP ) and a sampled action a ∼ p(. |
lg, (I1, p1), . . . , (It, pt)), the update is:
∇J ≈ {∇ log P (lg | ¯x, IP ) +

∇ log P (at | lg, (I1, p1), . . . , (It, pt))} R(st, a)
λ∇H(π(. | ˜st) .

We perform joint training with randomly initial-
ized goal prediction and action generation models.

D Hyperparameters

For LANI experiments, we use 5% of the training
data for tuning the hyperparameters and train on
the remaining. For CHAI, we use the development
set for tuning the hyperparameters. We train our
models for 20 epochs and ﬁnd the optimal stop-
ping epoch using the tuning set. We use 32 dimen-
sional embeddings for words and time. LSTMx
and LSTMA are single layer LSTMs with 256
hidden units. The ﬁrst layer of CNN0 contains
128 8×8 kernels with a stride of 4 and padding 3,
and the second layer contains 64 3×3 kernels with
a stride of 1 and padding 1. The convolution lay-
ers in LINGUNET use 32 5×5 kernels with stride

Dataset

Vocabulary Mean Instruction

Num
Instructions

Num.
Actions

Avg Trajectory
Length

Partially
Observed

Bisk et al.
(2016)
MacMahon
et al. (2006)
Matuszek et al.
(2012b)
Misra et al.
(2015)
LANI
CHAI

16,767

3,237

217

469

28,204
13,729

Size

1,426

563

39

775

2,292
1018

Length

15.27

7.96

6.65

48.7

12.07
10.14

81

3

3

>100

4
1028

15.4

3.12

N/A

21.5

24.6
54.5

No

Yes

No

No

Yes
Yes

Table 7: Comparison of LANI and CHAI to several existing natural language instructions corpora.

Category
Spatial relations
Location conjunction
Temporal coordination
Co-reference

Present Absent

2.56
3.85
1.70
1.98

1.77
1.93
2.14
1.98

p-value
.023
.226
.164
.993

Table 8: Mean goal prediction error for CHAI instruc-
tions with and without the analysis categories we used
in Table 2. The p-values are from two-sided t-tests
comparing the means in each row.

2. All deconvolutions except the ﬁnal one, also use
32 5×5 kernels with stride 2. The dropout proba-
bility in LINGUNET is 0.5. The size of attention
mask is 32×32 + 1. For both LANI and CHAI, we
use a camera angle of 60◦ and create panoramas
using 6 separate RGB images. Each image is of
size 128×128. We use a learning rate of 0.00025
and entropy coefﬁcient λ of 0.05.

E CHAI Error Analysis

Table 8 provides the same kind of error analysis
results here for the CHAI dataset as we produced
for LANI, comparing performance of the model on
samples of sentences with and without the analysis
phenomena that occurred in CHAI.

F Examples of Generated Goal

Prediction

Figure 7 shows example goal predictions from the
development sets. We found the predicted proba-
bility distributions to be reasonable even in many
cases where the agent failed to successfully com-
plete the task. We observed that often the eval-
uation metric is too strict for LANI instructions,
especially in cases of instruction ambiguity.

Success

Success

Failure

go round the ﬂowers

ﬂy between the palm tree and pond

head toward the wishing well and keep it on your right .

move back to the kitchen .

then drop the tropicana onto the coffee table .

walk the cup to the table and set the cup on the table .

Figure 7: Goal prediction probability maps Pg overlaid on the corresponding observed panoramas IP . The top
three examples show results from LANI, the bottom three from CHAI. The white arrow indicates the forward
direction that the agent is facing. The success/failure in the LANI examples indicate if the task was completed
accurately or not following the task completion (TC) metric.

Mapping Instructions to Actions in 3D Environments
with Visual Goal Prediction

Andrew Bennett
Dipendra Misra
Eyvind Niklasson Max Shatkhin

Valts Blukis
Yoav Artzi

Department of Computer Science and Cornell Tech, Cornell University, New York, NY, 10044
{dkm, awbennett, valts, yoav}@cs.cornell.edu
{een7, ms3448}@cornell.edu

9
1
0
2
 
r
a

M
 
8
1
 
 
]
L
C
.
s
c
[
 
 
2
v
6
8
7
0
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

We propose to decompose instruction exe-
cution to goal prediction and action genera-
tion. We design a model that maps raw vi-
sual observations to goals using LINGUNET,
a language-conditioned image generation net-
work, and then generates the actions required
to complete them. Our model
is trained
from demonstration only without external re-
sources. To evaluate our approach, we intro-
duce two benchmarks for instruction follow-
ing: LANI, a navigation task; and CHAI, where
an agent executes household instructions. Our
evaluation demonstrates the advantages of our
model decomposition, and illustrates the chal-
lenges posed by our new benchmarks.

1

Introduction

Executing instructions in interactive environments
requires mapping natural language and observa-
tions to actions. Recent approaches propose learn-
ing to directly map from inputs to actions, for ex-
ample given language and either structured obser-
vations (Mei et al., 2016; Suhr and Artzi, 2018) or
raw visual observations (Misra et al., 2017; Xiong
et al., 2018). Rather than using a combination
of models, these approaches learn a single model
to solve language, perception, and planning chal-
lenges. This reduces the amount of engineering
required and eliminates the need for hand-crafted
meaning representations. At each step, the agent
maps its current inputs to the next action using a
single learned function that is executed repeatedly
until task completion.

Although executing the same computation at
each step simpliﬁes modeling, it exempliﬁes cer-
tain inefﬁciencies; while the agent needs to de-
cide what action to take at each step, identifying
its goal is only required once every several steps
or even once per execution. The left instruction in
Figure 1 illustrates this. The agent can compute its

After reaching the hydrant
head towards
the blue
fence and pass towards the
right side of the well.

Put the cereal, the sponge,
and the dishwashing soap
into the cupboard above
the sink.

Figure 1: Example instructions from our two tasks:
LANI (left) and CHAI (right). LANI is a landmark nav-
igation task, and CHAI is a corpus of instructions in the
CHALET environment.

goal once given the initial observation, and given
this goal can then generate the actions required.
In this paper, we study a new model that explic-
itly distinguishes between goal selection and ac-
tion generation, and introduce two instruction fol-
lowing benchmark tasks to evaluate it.

Our model decomposes into goal prediction and
action generation. Given a natural language in-
struction and system observations, the model pre-
dicts the goal to complete. Given the goal, the
model generates a sequence of actions.

The key challenge we address is designing the
goal representation. We avoid manually designing
a meaning representation, and predict the goal in
the agent’s observation space. Given the image of
the environment the agent observes, we generate a
probability distribution over the image to highlight
the goal location. We treat this prediction as image
generation, and develop LINGUNET, a language
conditioned variant of the U-NET image-to-image
architecture (Ronneberger et al., 2015). Given the
visual goal prediction, we generate actions using a
recurrent neural network (RNN).

Our model decomposition offers two key advan-
tages. First, we can use different learning methods
as appropriate for the goal prediction and action

generation problems. We ﬁnd supervised learning
more effective for goal prediction, where only a
limited amount of natural language data is avail-
able. For action generation, where exploration is
critical, we use policy gradient in a contextual ban-
dit setting (Misra et al., 2017). Second, the goal
distribution is easily interpretable by overlaying it
on the agent observations. This can be used to in-
crease the safety of physical systems by letting the
user verify the goal before any action is executed.
Despite the decomposition, our approach retains
the advantages of the single-model approach.
It
does not require designing intermediate represen-
tations, and training does not rely on external re-
sources, such as pre-trained parsers or object de-
tectors, instead using demonstrations only.

We introduce two new benchmark tasks with
different levels of complexity of goal prediction
and action generation. LANI is a 3D navigation
environment and corpus, where an agent navigates
between landmarks. The corpus includes 6,000
sequences of natural language instructions, each
containing on average 4.7 instructions. CHAI is
a corpus of 1,596 instruction sequences, each in-
cluding 7.7 instructions on average, for CHALET,
a 3D house environment (Yan et al., 2018).
In-
structions combine navigation and simple manipu-
lation, including moving objects and opening con-
tainers. Both tasks require solving language chal-
lenges, including spatial and temporal reasoning,
as well as complex perception and planning prob-
lems. While LANI provides a task where most in-
structions include a single goal, the CHAI instruc-
tions often require multiple intermediate goals.
For example, the household instruction in Fig-
ure 1 can be decomposed to eight goals: opening
the cupboard, picking each item and moving it to
the cupboard, and closing the cupboard. Achiev-
ing each goal requires multiple actions of differ-
ent types, including moving and acting on objects.
This allows us to experiment with a simple varia-
tion of our model to generate intermediate goals.

We compare our approach to multiple recent
methods. Experiments on the LANI navigation
task indicate that decomposing goal prediction
and action generation signiﬁcantly improves in-
struction execution performance. While we ob-
serve similar trends on the CHAI instructions, re-
sults are overall weaker, illustrating the complex-
ity of the task. We also observe that inherent
ambiguities in instruction following make exact

goal identiﬁcation difﬁcult, as demonstrated by
imperfect human performance. However, the gap
to human-level performance still remains large
across both tasks. Our code and data are available
at github.com/clic-lab/ciff.

2 Technical Overview

Task Let X be the set of all instructions, S the
set of all world states, and A the set of all actions.
An instruction ¯x ∈ X is a sequence (cid:104)x1, . . . , xn(cid:105),
where each xi is a token. The agent executes
instructions by generating a sequence of actions,
and indicates execution completion with the spe-
cial action STOP.

The sets of actions A and states S are domain
speciﬁc. In the navigation domain LANI, the ac-
tions include moving the agent and changing its
orientation. The state information includes the po-
sition and orientation of the agent and the differ-
ent landmarks. The agent actions in the CHALET
house environment include moving and changing
the agent orientation, as well as an object interac-
tion action. The state encodes the position and ori-
entation of the agent and all objects in the house.
For interactive objects, the state also includes their
status, for example if a drawer is open or closed.
In both domains, the actions are discrete. The do-
mains are described in Section 6.
Model The agent does not observe the world
state directly, but instead observes its pose and an
RGB image of the environment from its point of
view. We deﬁne these observations as the agent
context ˜s. An agent model is a function from an
agent context ˜s to an action a ∈ A. We model
goal prediction as predicting a probability distri-
bution over the agent visual observations, repre-
senting the likelihood of locations or objects in the
environment being target positions or objects to be
acted on. Our model is described in Section 4.
Learning We assume access to training data
i=1, where ¯x(i)
with N examples {(¯x(i), s
(i)
(i)
is the
is an instruction, s
g
1
goal state. We decompose learning; training goal
prediction using supervised learning, and action
generation using oracle goals with policy gradient
in a contextual bandit setting. We assume an in-
strumented environment with access to the world
state, which is used to compute rewards during
training only. Learning is described in Section 5.
Evaluation We evaluate task performance on a
(i)
i=1, where ¯x(i) is an in-
g )}M
test set {(¯x(i), s

is a start state, and s

(i)
g )}N

(i)
1 , s

(i)
1 , s

(i)
1

struction, s
is the goal
is a start state, and s
state. We evaluate task completion accuracy and
the distance of the agent’s ﬁnal state to s

(i)
g .

(i)
g

3 Related Work

Mapping instruction to action has been studied
extensively with intermediate symbolic represen-
tations (e.g., Chen and Mooney, 2011; Kim and
Mooney, 2012; Artzi and Zettlemoyer, 2013; Artzi
et al., 2014; Misra et al., 2015, 2016). Recently,
there has been growing interest in direct mapping
from raw visual observations to actions (Misra
et al., 2017; Xiong et al., 2018; Anderson et al.,
2018; Fried et al., 2018). We propose a model that
enjoys the beneﬁts of such direct mapping, but ex-
plicitly decomposes that task to interpretable goal
prediction and action generation. While we focus
on natural language, the problem has also been
studied using synthetic language (Chaplot et al.,
2018; Hermann et al., 2017).

Our model design is related to hierarchical re-
inforcement learning, where sub-policies at differ-
ent levels of the hierarchy are used at different fre-
quencies (Sutton et al., 1998). Oh et al. (2017)
uses a two-level hierarchy for mapping synthetic
language to actions. Unlike our visual goal rep-
resentation, they use an opaque vector representa-
tion. Also, instead of reinforcement learning, our
methods emphasize sample efﬁciency.

Goal prediction is related to referring expres-
sion interpretation (Matuszek et al., 2012a; Krish-
namurthy and Kollar, 2013; Kazemzadeh et al.,
2014; Kong et al., 2014; Yu et al., 2016; Mao et al.,
2016; Kitaev and Klein, 2017). While our model
solves a similar problem for goal prediction, we
focus on detecting visual goals for actions, includ-
ing both navigation and manipulation, as part of
an instruction following model. Using formal goal
representation for instruction following was stud-
ied by MacGlashan et al. (2015). In contrast, our
model generates a probability distribution over im-
ages, and does not require an ontology.

Our data collection is related to existing work.
LANI is inspired by the HCRC Map Task (An-
derson et al., 1991), where a leader directs a fol-
lower to navigate between landmarks on a map.
We use a similar task, but our scalable data collec-
tion process allows for a signiﬁcantly larger cor-
pus. We also provide an interactive navigation
environment, instead of only map diagrams. Un-
like Map Task, our leaders and followers do not
interact in real time. This abstracts away inter-

action challenges, similar to how the SAIL nav-
igation corpus was collected (MacMahon et al.,
2006). CHAI instructions were collected using
scenarios given to workers, similar to the ATIS
collection process (Hemphill et al., 1990; Dahl
et al., 1994). Recently, multiple 3D research envi-
ronments were released. LANI has a signiﬁcantly
larger state space than existing navigation envi-
ronments (Hermann et al., 2017; Chaplot et al.,
2018), and CHALET, the environment used for
CHAI, is larger and has more complex manipu-
lation compared to similar environments (Gordon
et al., 2018; Das et al., 2018). In addition, only
synthetic language data has been released for these
environment. An exception is the Room-to-Room
dataset (Anderson et al., 2018) that makes use of
an environment of connected panoramas of house
settings. Although it provides a realistic vision
challenge, unlike our environments, the state space
is limited to a small number of panoramas and ma-
nipulation is not possible.

4 Model

We model the agent policy as a neural network.
The agent observes the world state st at time t as
an RGB image It. The agent context ˜st, the infor-
mation available to the agent to select the next ac-
tion at, is a tuple (¯x, IP , (cid:104)(I1, p1), . . . , (It, pt)(cid:105)),
where ¯x is the natural
language instructions,
IP is a panoramic view of the environment
from the starting position at time t = 1, and
(cid:104)(I1, p1), . . . , (It, pt)(cid:105) is the sequence of observa-
tions It and poses pt up to time t. The panorama
IP is generated through deterministic exploration
by rotating 360◦ to observe the environment at the
beginning of the execution.1

The model includes two main components: goal
prediction and action generation. The agent uses
the panorama IP to predict the goal location lg. At
each time step t, a projection of the goal location
into the agent’s current view Mt is given as input
to an RNN to generate actions. The probability of
an action at at time t decomposes to:
(cid:16)

P (at | ˜st) =

P (lg | ¯x, IP )

(cid:88)

lg

P (at | lg, (I1, p1), . . . , (It, pt))

,

(cid:17)

where the ﬁrst term puts the complete distribution
mass on a single location (i.e., a delta function).
Figure 2 illustrates the model.

1The panorama is a concatenation of deterministic obser-
vations along the width dimension. For simplicity, we do not
include these deterministic steps in the execution.

Figure 2: An illustration for our architecture (Section 4) for the instruction turn left and go to the red oil drum
with a LINGUNET depth of m = 4. The instruction ¯x is mapped to ¯x with an RNN, and the initial panorama
observation IP to F0 with a CNN. LINGUNET generates H1, a visual representation of the goal. First, a sequence
of convolutions maps the image features F0 to feature maps F1,. . . ,F4. The text representation ¯x is used to
generate the kernels K1,. . . ,K4, which are convolved to generate the text-conditioned feature maps G1,. . . ,G4.
These feature maps are de-convolved to H1,. . . ,H4. The goal probability distribution Pg is computed from H1.
The goal location is the inferred from the max of Pg. Given lg and pt, the pose at step t, the goal mask Mt is
computed and passed into an RNN that outputs the action to execute.

Goal Prediction To predict the goal location,
we generate a probability distribution Pg over
a feature map F0 generated using convolutions
from the initial panorama observation IP . Each
element in the probability distribution Pg corre-
sponds to an area in IP . Given the instruction
¯x and panorama IP , we ﬁrst generate their rep-
resentations. From the panorama IP , we gener-
ate a feature map F0 = [CNN0(IP ); Fp], where
CNN0 is a two-layer convolutional neural net-
work (CNN; LeCun et al., 1998) with rectiﬁed
linear units (ReLU; Nair and Hinton, 2010) and
Fp are positional embeddings.2 The concatena-
tion is along the channel dimension. The instruc-
tion ¯x = (cid:104)x1, · · · xn(cid:105) is mapped to a sequence
of hidden states li = LSTMx(ψx(xi), li−1), i =
1, . . . , n using a learned embedding function ψx
and a long short-term memory (LSTM; Hochre-
iter and Schmidhuber, 1997) RNN LSTMx. The
instruction representation is ¯x = ln.

We generate the probability distribution Pg over
pixels in F0 using LINGUNET. The architecture
of LINGUNET is inspired by the U-NET image
generation method (Ronneberger et al., 2015), ex-
cept that the reconstruction phase is conditioned
on the natural language instruction. LINGUNET
ﬁrst applies m convolutional layers to generate a
sequence of feature maps Fj = CNNj(Fj−1),

2We generate Fp by creating a channel for each determin-
istic observation used to create the panorama, and setting all
the pixels corresponding to that observation location in the
panorama to 1 and all others to 0. The number of observa-
tions depends on the agent’s camera angle.

j = 1 . . . m, where each CNNj is a convolutional
layer with leaky ReLU non-linearities (Maas et al.,
2013) and instance normalization (Ulyanov et al.,
2016). The instruction representation ¯x is split
evenly into m vectors {¯xj}m
j=1, each is used to
create a 1 × 1 kernel Kj = AFFINEj(¯xj), where
each AFFINEj is an afﬁne transformation followed
by normalizing and reshaping. For each Fj, we
apply a 2D 1 × 1 convolution using the text ker-
nel Kj to generate a text-conditioned feature map
Gj = CONVOLVE(Kj, Fj), where CONVOLVE
convolves the kernel over the feature map. We
then perform m deconvolutions to generate a se-
quence of feature maps Hm,. . . ,H1:

Hm = DECONVm(DROPOUT(Gm))
Hj = DECONVj([Hj+1; Gj]) .

DROPOUT is dropout regularization (Srivastava
et al., 2014) and each DECONVj
is a decon-
volution operation followed a leaky ReLU non-
linearity and instance norm.3 Finally, we gener-
ate Pg by applying a softmax to H1 and an ad-
ditional learned scalar bias term bg to represent
events where the goal is out of sight. For example,
when the agent already stands in the goal position
and therefore the panorama does not show it.

We use Pg to predict the goal position in the
environment. We ﬁrst select the goal pixel in F0 as
the pixel corresponding to the highest probability
element in Pg. We then identify the corresponding
3D location lg in the environment using backward
camera projection, which is computed given the

3DECONV1 does deconvolution only.

camera parameters and p1, the agent pose at the
beginning of the execution.
Action Generation Given the predicted goal lg,
we generate actions using an RNN. At each time
step t, given pt, we generate the goal mask Mt,
which has the same shape as the observed image
It. The goal mask Mt has a value of 1 for each
element that corresponds to the goal location lg in
It. We do not distinguish between visible or oc-
cluded locations. All other elements are set to 0.
We also maintain an out-of-sight ﬂag ot that is set
to 1 if (a) lg is not within the agent’s view; or (b)
the max scoring element in Pg corresponds to bg,
the term for events when the goal is not visible in
IP . Otherwise, ot is set to 0. We compute an ac-
tion generation hidden state yt with an RNN:

yt = LSTMA (AFFINEA([FLAT(Mt); ot]), yt−1) ,

where FLAT ﬂattens Mt into a vector, AFFINEA
is a learned afﬁne transformation with ReLU, and
LSTMA is an LSTM RNN. The previous hidden
state yt−1 was computed when generating the pre-
vious action, and the RNN is extended gradually
during execution. Finally, we compute a probabil-
ity distribution over actions:

P (at | lg, (I1, p1), . . . , (It, pt)) =

SOFTMAX(AFFINEp([yt; ψT (t)])) ,

where ψT is a learned embedding lookup table
for the current time (Chaplot et al., 2018) and
AFFINEp is a learned afﬁne transformation.
Model Parameters The model parameters θ in-
clude the parameters of the convolutions CNN0
and the components of LINGUNET: CNNj,
for j = 1, . . . , m.
AFFINEj, and DECONVj
In addition we learn two afﬁne transformations
AFFINEA and AFFINEp, two RNNs LSTMx and
LSTMA, two embedding functions ψx and ψT ,
and the goal distribution bias term bg. In our ex-
periments (Section 7), all parameters are learned
without external resources.

5 Learning

Our modeling decomposition enables us to choose
different learning algorithms for the two parts.
While reinforcement learning is commonly de-
ployed for tasks that beneﬁt from exploration (e.g.,
Peters and Schaal, 2008; Mnih et al., 2013), these
methods require many samples due to their high
sample complexity. However, when learning with
natural language, only a relatively small number
of samples is realistically available. This problem

was addressed in prior work by learning in a con-
textual bandit setting (Misra et al., 2017) or mix-
ing reinforcement and supervised learning (Xiong
et al., 2018). Our decomposition uniquely offers
to tease apart the language understanding prob-
lem and address it with supervised learning, which
generally has lower sample complexity. For action
generation though, where exploration can be au-
tonomous, we use policy gradient in a contextual
bandit setting (Misra et al., 2017).

(i)
1

(i)
1 , s

(i)
g )}N

We assume access to training data with N ex-
i=1, where ¯x(i) is an in-
amples {(¯x(i), s
(i)
struction, s
is the goal
is a start state, and s
g
state. We train the goal prediction component by
minimizing the cross-entropy of the predicted dis-
tribution with the gold-standard goal distribution.
The gold-standard goal distribution is a determin-
istic distribution with probability one at the pixel
corresponding to the goal location if the goal is in
the ﬁeld of view, or probability one at the extra
out-of-sight position otherwise. The gold location
(i)
is the agent’s location in s
g . We update the model
parameters using Adam (Kingma and Ba, 2014).

We train action generation by maximizing the
expected immediate reward the agent observes
while exploring the environment. The objective
for a single example i and time stamp t is:

J =

π(a | ˜st)R(i)(st, a) + λH(π(. | ˜st)) ,

(cid:88)

a∈A

where R(i) : S × A → R is an example-speciﬁc
reward function, H(·) is an entropy regularization
term, and λ is the regularization coefﬁcient. The
reward function R(i) details are described in de-
tails in Appendix B. Roughly speaking, the re-
ward function includes two additive components:
a problem reward and a shaping term (Ng et al.,
1999). The problem reward provides a positive re-
ward for successful task completion, and a nega-
tive reward for incorrect completion or collision.
The shaping term is positive when the agent gets
closer to the goal position, and negative if it is
moving away. The gradient of the objective is:

∇J =

π(a | ˜st)∇ log π(a | ˜st)R(st, a)

(cid:88)

a∈A

+λ∇H(π(. | ˜st) .

We approximate the gradient by sampling an ac-
tion using the policy (Williams, 1992), and use the
(i)
gold goal location computed from s
g . We per-
form several parallel rollouts to compute gradients
and update the parameters using Hogwild! (Recht
et al., 2011) and Adam learning rates.

Dataset Statistic
Number paragraphs
Mean instructions per paragraph
Mean actions per instruction
Mean tokens per instruction
Vocabulary size

LANI
6,000
4.7
24.6
12.1
2,292

CHAI
1,596
7.70
54.5
8.4
1,018

Table 1: Summary statistics of the two corpora.

6 Tasks and Data

6.1 LANI

The goal of LANI is to evaluate how well an agent
can follow navigation instructions. The agent task
is to follow a sequence of instructions that specify
a path in an environment with multiple landmarks.
Figure 1 (left) shows an example instruction.

The environment

is a fenced, square, grass
ﬁeld. Each instance of the environment con-
tains between 6–13 randomly placed landmarks,
sampled from 63 unique landmarks. The agent
can take four types of discrete actions: FORWARD,
TURNRIGHT, TURNLEFT, and STOP. The ﬁeld is
of size 50×50, the distance of the FORWARD ac-
tion is 1.5, and the turn angle is 15◦. The en-
vironment simulator is implemented in Unity3D.
At each time step, the agent performs an action,
observes a ﬁrst person view of the environment
as an RGB image, and receives a scalar reward.
The simulator provides a socket API to control the
agent and the environment.

Agent performance is evaluated using two met-
rics: task completion accuracy, and stop distance
error. A task is completed correctly if the agent
stops within an aerial distance of 5 from the goal.
We collect a corpus of navigation instructions
using crowdsourcing. We randomly generate en-
vironments, and generate one reference path for
each environment. To elicit linguistically interest-
ing instructions, reference paths are generated to
pass near landmarks. We use Amazon Mechanical
Turk, and split the annotation process to two tasks.
First, given an environment and a reference path,
a worker writes an instruction paragraph for fol-
lowing the path. The second task requires another
worker to control the agent to perform the instruc-
tions and simultaneously mark at each point what
part of the instruction was executed. The record-
ing of the second worker creates the ﬁnal data of
segmented instructions and demonstrations. The
generated reference path is displayed in both tasks.
The second worker could also mark the paragraph
as invalid. Both tasks are done from an over-
head view of the environment, but workers are in-
structed to provide instructions for a robot that ob-

[Go around the pillar on the right hand side] [and head
towards the boat, circling around it clockwise.] [When
you are facing the tree, walk towards it, and the pass on
the right hand side,] [and the left hand side of the cone.
Circle around the cone,] [and then walk past the hydrant
on your right,] [and the the tree stump.] [Circle around
the stump and then stop right behind it.]

Figure 3: Segmented instructions in the LANI domain.
The original reference path is marked in red (start) and
blue (end). The agent, using a drone icon, is placed at
the beginning of the path. The follower path is coded in
colors to align to the segmented instruction paragraph.

serves the environment from a ﬁrst person view.
Figure 3 shows a reference path and the written
instruction. This data can be used for evaluating
both executing sequences of instructions and sin-
gle instructions in isolation.

Table 1 shows the corpus statistics.4 Each para-
graph corresponds to a single unique instance of
the environment. The paragraphs are split into
train, test, and development, with a 70% / 15% /
15% split. Finally, we sample 200 single devel-
opment instructions for qualitative analysis of the
language challenge the corpus presents (Table 2).

6.2 CHAI

The CHAI corpus combines both navigation and
simple manipulation in a complex, simulated
household environment. We use the CHALET sim-
ulator (Yan et al., 2018), a 3D house simulator
that provides multiple houses, each with multi-
ple rooms. The environment supports moving be-
tween rooms, picking and placing objects, and
opening and closing cabinets and similar contain-
ers. Objects can be moved between rooms and
in and out of containers. The agent observes the
world in ﬁrst-person view, and can take ﬁve ac-
tions: FORWARD, TURNLEFT, TURNRIGHT, STOP,
and INTERACT. The INTERACT action acts on ob-
jects.
It takes as argument a 2D position in the
agent’s view. Agent performance is evaluated with
two metrics: (a) stop distance, which measures the
distance of the agent’s ﬁnal state to the ﬁnal an-
notated position; and (b) manipulation accuracy,
which compares the set of manipulation actions

4Appendix A provides statistics for related datasets.

Category
Spatial relations
between locations
Conjunctions of two
more locations
Temporal coordination
of sub-goals
Constraints on the
shape of trajectory

Co-reference

Comparatives

Count

LANI

CHAI

123

36

65

94

32

2

52

68

5

0

18

0

Example
LANI: go to the right side of the rock
CHAI: pick up the cup next to the bathtub and place it on . . .
LANI: ﬂy between the mushroom and the yellow cone
CHAI: . . . set it on the table next to the juice and milk.
LANI: at the mushroom turn right and move forward towards the statue
CHAI: go back to the kitchen and put the glass in the sink.

LANI: go past the house by the right side of the apple

LANI: turn around it and move in front of fern plant
CHAI: turn left, towards the kitchen door and move through it.
LANI: . . . the small stone closest to the blue and white fences stop

Table 2: Qualitative analysis of the LANI and CHAI corpora. We sample 200 single development instructions from
each corpora. For each category, we count how many examples of the 200 contained it and show an example.

Scenario
You have several hours before guests begin to arrive for
a dinner party. You are preparing a wide variety of meat
dishes, and need to put them in the sink.
In addition,
you want to remove things in the kitchen, and bathroom
which you don’t want your guests seeing, like the soaps
in the bathroom, and the dish cleaning items. You can
put these in the cupboards. Finally, put the dirty dishes
around the house in the dishwasher and close it.
Written Instructions
[In the kitchen, open the cupboard above the sink.] [Put
the cereal, the sponge, and the dishwashing soap into the
cupboard above the sink.] [Close the cupboard.] [Pick
up the meats and put them into the sink.] [Open the dish-
washer, grab the dirty dishes on the counter, and put the
dishes into the dishwasher.]

Figure 4: Scenario and segmented instruction from the
CHAI corpus.

to a reference set. When measuring distance, to
consider the house plan, we compute the minimal
aerial distance for each room that must be visited.
Yan et al. (2018) provides the full details of the
simulator and evaluation. We use ﬁve different
houses, each with up to six rooms. Each room
contains on average 30 objects. A typical room
is of size 6×6. We set the distance of FORWARD to
0.1, the turn angle to 90◦, and divide the agent’s
view to a 32×32 grid for the INTERACT action.

We collected a corpus of navigation and ma-
nipulation instructions using Amazon Mechanical
Turk. We created 36 common household scenar-
ios to provide a familiar context to the task.5 We
use two crowdsourcing tasks. First, we provide
workers with a scenario and ask them to write in-
structions. The workers are encouraged to explore
the environment and interact with it. We then seg-
ment the instructions to sentences automatically.
In the second task, workers are presented with the
segmented sentences in order and asked to execute
them. After ﬁnishing a sentence, the workers re-

5We observed that asking workers to simply write instruc-
tions without providing a scenario leads to combinations of
repetitive instructions unlikely to occur in reality.

quest the next sentence. The workers do not see
the original scenario. Figure 4 shows a scenario
and the written segmented paragraph. Similar to
LANI, CHAI data can be used for studying com-
plete paragraphs and single instructions.

Table 1 shows the corpus statistics.6 The para-
graphs are split into train, test, and development,
with a 70% / 15% / 15% split. Table 2 shows qual-
itative analysis of a sample of 200 instructions.

7 Experimental Setup

instructions.

Method Adaptations for CHAI We apply two
modiﬁcations to our model to support interme-
diate goal for the CHAI
First,
we train an additional RNN to predict the se-
quence of intermediate goals given the instruc-
There are two types of goals:
tion only.
NAVIGATION,
for action sequences requiring
movement only and ending with the STOP action;
and INTERACTION, for sequence of movement ac-
tions that end with an INTERACT action. For ex-
ample, for the instruction pick up the red book
and go to the kitchen, the sequence of goals will
be (cid:104)INTERACTION, NAVIGATION, NAVIGATION(cid:105).
This indicates the agent must ﬁrst move to the
object to pick it up via interaction, move to the
kitchen door, and ﬁnally move within the kitchen.
The process of executing an instruction starts with
predicting the sequence of goal types. We call our
model (Section 4) separately for each goal type.
The execution concludes when the ﬁnal goal is
completed. For learning, we create a separate ex-
ample for each intermediate goal and train the ad-
ditional RNN separately. The second modiﬁcation
is replacing the backward camera projection for
inferring the goal location with ray casting to iden-

6The number of actions per instruction is given in the
more ﬁne-grained action space used during collection. To
make the required number of actions smaller, we use the more
coarse action space speciﬁed.

Method
STOP
RANDOMWALK
MOSTFREQUENT
MISRA17
CHAPLOT18
Our Approach (OA)
OA w/o RNN
OA w/o Language
OA w/joint
OA w/oracle goals

LANI

CHAI

SD
15.37
14.80
19.31
10.54
9.05
8.65
9.21
10.65
11.54
2.13

TC
8.20
9.66
2.94
22.9
31.0
35.72
31.30
23.02
21.76
94.60

SD
2.99
2.99
3.80
2.99
2.99
2.75
3.75
3.22
2.99
2.19

MA
37.53
28.96
37.53
32.25
37.53
37.53
37.43
37.53
36.90
41.07

Table 3: Performance on the development data.

tify INTERACTION goals, which are often objects
that are not located on the ground.
Baselines We compare our approach against the
following baselines: (a) STOP: Agent stops im-
mediately; (b) RANDOMWALK: Agent samples
actions uniformly until it exhausts the horizon
or stops; (c) MOSTFREQUENT: Agent takes the
most frequent action in the data, FORWARD for
both datasets, until it exhausts the horizon; (d)
the approach of Misra et al. (2017);
MISRA17:
and (e) CHAPLOT18:
the approach of Chaplot
et al. (2018). We also evaluate goal prediction and
compare to the method of Janner et al. (2018) and
a CENTER baseline, which always predict the cen-
ter pixel. Appendix C provides baseline details.
Evaluation Metrics We evaluate using the met-
rics described in Section 6: stop distance (SD) and
task completion (TC) for LANI, and stop distance
(SD) and manipulation accuracy (MA) for CHAI.
To evaluate the goal prediction, we report the real
distance of the predicted goal from the annotated
goal and the percentage of correct predictions. We
consider a goal correct if it is within a distance of
5.0 for LANI and 1.0 for CHAI. We also report
human evaluation for LANI by asking raters if the
generated path follows the instruction on a Likert-
type scale of 1–5. Raters were shown the gener-
ated path, the reference path, and the instruction.
Parameters We use a horizon of 40 for both
domains. During training, we allow additional
5 steps to encourage learning even after errors.
When using intermediate goals in CHAI, the hori-
zon is used for each intermediate goal separately.
All other parameters and detailed in Appendix D.

8 Results

Tables 3 and 4 show development and test re-
sults. Both sets of experiments demonstrate sim-
ilar trends. The low performance of STOP, RAN-
DOMWALK, and MOSTFREQUENT demonstrates

Method
STOP
RANDOMWALK
MOSTFREQUENT
MISRA17
CHAPLOT18
Our Approach

LANI

CHAI

SD
15.18
14.63
19.14
10.23
8.78
8.43

TC
8.29
9.76
3.15
23.2
31.9
36.9

SD
3.59
3.59
4.36
3.59
3.59
3.34

MA
39.77
33.29
39.77
36.84
39.76
39.97

Table 4: Performance on the held-out test dataset.

Method
CENTER
Janner et al. (2018)
Our Approach

LANI

CHAI

Dist
12.0
9.61
8.67

Acc
19.51
30.26
35.83

Dist
3.41
2.81
2.12

Acc
19.0
28.3
40.3

Table 5: Development goal prediction performance.
We measure distance (Dist) and accuracy (Acc).

the challenges of both tasks, and shows the tasks
are robust to simple biases. On LANI, our ap-
proach outperforms CHAPLOT18, improving task
completion (TC) accuracy by 5%, and both meth-
ods outperform MISRA17. On CHAI, CHAP-
LOT18 and MISRA17 both fail to learn, while
our approach shows an improvement on stop dis-
tance (SD). However, all models perform poorly
on CHAI, especially on manipulation (MA).

To isolate navigation performance on CHAI, we
limit our train and test data to instructions that in-
clude navigation actions only. The STOP baseline
on these instructions gives a stop distance (SD) of
3.91, higher than the average for the entire data
as these instructions require more movement. Our
approach gives a stop distance (SD) of 3.24, a 17%
reduction of error, signiﬁcantly better than the 8%
reduction of error over the entire corpus.

We also measure human performance on a sam-
ple of 100 development examples for both tasks.
On LANI, we observe a stop distance error (SD)
of 5.2 and successful task completion (TC) 63%
of the time. On CHAI, the human distance er-
ror (SD) is 1.34 and the manipulation accuracy is
100%. The imperfect performance demonstrates
the inherent ambiguity of the tasks. The gap to
human performance is still large though, demon-
strating that both tasks are largely open problems.
The imperfect human performance raises ques-
tions about automated evaluation.
In general,
we observe that often measuring execution qual-
ity with rigid goals is insufﬁcient. We conduct
a human evaluation with 50 development exam-
ples from LANI rating human performance and
our approach. Figure 5 shows a histogram of the
ratings. The mean rating for human followers is
4.38, while our approach’s is 3.78; we observe
a similar trend to before with this metric. Using

Category
Spatial relations
Location conjunction
Temporal coordination
Trajectory constraints
Co-reference
Comparatives

Present Absent
10.09
9.05
8.24
8.99
8.59
9.25

8.75
10.19
11.38
9.56
12.88
10.22

p-value
.262
.327
.015
.607
.016
.906

Table 6: Mean goal prediction error for LANI instruc-
tions with and without the analysis categories we used
in Table 2. The p-values are from two-sided t-tests
comparing the means in each row.

e
g
a
t
n
e
c
r
e
P

60

40

20

0

Human
Our Approach

1

2

3

4

5

Figure 5: Likert rating histogram for expert human fol-
lower and our approach for LANI.

judgements on our approach, we correlate the hu-
man metric with the SD measure. We observe a
Pearson correlation -0.65 (p=5e-7), indicating that
our automated metric correlates well with human
judgment.7 This initial study suggests that our au-
tomated evaluation is appropriate for this task.

Our ablations (Table 3) demonstrate the impor-
tance of each of the components of the model.
We ablate the action generation RNN (w/o RNN),
completely remove the language input (w/o Lan-
guage), and train the model jointly (w/joint Learn-
ing).8 On CHAI especially, ablations results in
models that display ineffective behavior. Of the
ablations, we observe the largest beneﬁt from
decomposing the learning and using supervised
learning for the language problem.

We also evaluate our approach with access to
oracle goals (Table 3). We observe this im-
proves navigation performance signiﬁcantly on
both tasks. However, the model completely fails
to learn a reasonable manipulation behavior for
CHAI. This illustrates the planning complexity
of this domain. A large part of the improvement
in measured navigation behavior is likely due to
eliminating much of the ambiguity the automated
metric often fails to capture.

Finally, on goal prediction (Table 5), our ap-
proach outperforms the method of Janner et al.
(2018). Figure 6 and Appendix Figure 7 show ex-
ample goal predictions. In Table 6, we break down
LANI goal prediction results for the analysis cate-

7We did not observe this kind of clear anti-correlation
comparing the two results for human performance (Pearson
correlation of 0.09 and p=0.52). The limited variance in hu-
man performance makes correlation harder to test.

8Appendix C provides the details of joint learning.

curve around big rock keeping it to your left .

walk over to the cabinets and open the cabinet doors up

Figure 6: Goal prediction probability maps Pg overlaid
on the corresponding observed panoramas IP . The top
example shows a result on LANI, the bottom on CHAI.
gories we used in Table 2 using the same sample of
the data. Appendix E includes a similar table for
CHAI. We observe that our approach ﬁnds instruc-
tions with temporal coordination or co-reference
challenging. Co-reference is an expected limita-
tion; with single instructions, the model can not
resolve references to previous instructions.
9 Discussion
We propose a model for instruction following with
explicit separation of goal prediction and action
generation. Our representation of goal prediction
is easily interpretable, while not requiring the de-
sign of logical ontologies and symbolic represen-
tations. A potential limitation of our approach is
cascading errors. Action generation relies com-
pletely on the predicted goal and is not exposed
to the language otherwise. This also suggests a
second related limitation:
the model is unlikely
to successfully reason about instructions that in-
clude constraints on the execution itself. While
the model may reach the ﬁnal goal correctly, it is
unlikely to account for the intermediate trajectory
constraints. As we show (Table 2), such instruc-
tions are common in our data. These two limita-
tions may be addressed by allowing action genera-
tion access to the instruction. Achieving this while
retaining an interpretable goal representation that
clearly determines the execution is an important
direction for future work. Another important open
question concerns automated evaluation, which re-
mains especially challenging when instructions do
not only specify goals, but also constraints on how
to achieve them. Our resources provide the plat-
form and data to conduct this research.
Acknowledgments
This research was supported by NSF (CAREER-
1750499), Schmidt Sciences, a Google Faculty
Award, and cloud credits from Microsoft. We
thank John Langford, Claudia Yan, Bharath Har-
iharan, Noah Snavely, the Cornell NLP group, and
the anonymous reviewers for their advice.

References
Anne H Anderson, Miles Bader, Ellen Gurman Bard,
Elizabeth Boyle, Gwyneth Doherty, Simon Garrod,
Stephen Isard, Jacqueline Kowtko, Jan McAllister,
Jim Miller, Catherine Sotillo, Henry S. Thompson,
and Regina Weinert. 1991. The HCRC map task
corpus. Language and Speech, 34.

Peter Anderson, Qi Wu, Damien Teney, Jake Bruce,
Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen
Gould, and Anton van den Hengel. 2018. Vision-
and-language navigation:
Interpreting visually-
grounded navigation instructions in real environ-
ments. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition.

Yoav Artzi, Dipanjan Das, and Slav Petrov. 2014.
Learning compact lexicons for CCG semantic pars-
ing. In Proceedings of the 2014 Conference on Em-
pirical Methods in Natural Language Processing.

Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion of Computational Linguistics, 1.

Yonatan Bisk, Daniel Marcu, and William Wong. 2016.
Towards a dataset for human computer communica-
tion via grounded language acquisition. In Proceed-
ings of the AAAI Workshop on Symbiotic Cognitive
Systems.

Devendra Singh Chaplot, Kanthashree Mysore
Sathyendra, Rama Kumar Pasumarthi, Dheeraj
Rajagopal, and Ruslan Salakhutdinov. 2018. Gated-
attention architectures for task-oriented language
grounding.

David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the Na-
tional Conference on Artiﬁcial Intelligence.

Deborah A Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the ATIS
In Proceedings of the
task: The ATIS-3 corpus.
workshop on Human Language Technology.

Abhishek Das, Samyak Datta, Georgia Gkioxari, Ste-
fan Lee, Devi Parikh, and Dhruv Batra. 2018. Em-
In Proceedings of the
bodied question answering.
IEEE Conference on Computer Vision and Pattern
Recognition.

Daniel Fried, Ronghang Hu, Volkan Cirik, Anna
Rohrbach, Jacob Andreas, Louis-Philippe Morency,
Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein,
and Trevor Darrell. 2018. Speaker-follower mod-
CoRR,
els for vision-and-language navigation.
abs/1806.02724.

Daniel Gordon, Aniruddha Kembhavi, Mohammad
Rastegari, Joseph Redmon, Dieter Fox, and Ali

Farhadi. 2018. Iqa: Visual question answering in in-
teractive environments. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition.

Charles T. Hemphill, John J. Godfrey, and George R.
Doddington. 1990. The ATIS spoken language sys-
In Proceedings of the DARPA
tems pilot corpus.
speech and natural language workshop.

Karl Moritz Hermann, Felix Hill, Simon Green, Fumin
Wang, Ryan Faulkner, Hubert Soyer, David Szepes-
vari, Wojciech Czarnecki, Max Jaderberg, Denis
Teplyashin, Marcus Wainwright, Chris Apps, Demis
Hassabis, and Phil Blunsom. 2017. Grounded lan-
guage learning in a simulated 3D world. CoRR,
abs/1706.06551.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long

short-term memory. Neural computation, 9.

Michael Janner, Karthik Narasimhan, and Regina
Barzilay. 2018.
Representation learning for
grounded spatial reasoning. Transactions of the As-
sociation for Computational Linguistics, 6.

Sahar Kazemzadeh, Vicente Ordonez, Mark Matten,
and Tamara L. Berg. 2014. Referitgame: Referring
to objects in photographs of natural scenes. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.

Joohyun Kim and Raymond Mooney. 2012. Unsuper-
vised PCFG induction for grounded language learn-
ing with highly ambiguous supervision. In Proceed-
ings of the Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. In Proceedings
of the International Conference on Learning Repre-
sentations.

Nikita Kitaev and Dan Klein. 2017. Where is misty?
interpreting spatial descriptors by modeling regions
in space. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.

Chen Kong, Dahua Lin, Mohit Bansal, Raquel Urta-
sun, and Sanja Fidler. 2014. What are you talking
about? text-to-image coreference. In Proceedings of
the IEEE Conference on Computer Vision and Pat-
tern Recognition.

Jayant Krishnamurthy and T. Kollar. 2013.

Jointly
learning to parse and perceive: Connecting natural
language to the physical world. Transactions of the
Association for Computational Linguistics, 1.

Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86.

Andrew L Maas, Awni Y Hannun, and Andrew Y Ng.
2013. Rectiﬁer nonlinearities improve neural net-
work acoustic models. In Proceedings of the inter-
national conference on machine learning.

James MacGlashan, Monica Babes-Vroman, Marie
desJardins, Michael L. Littman, Smaranda Muresan,
S Bertel Squire, Stefanie Tellex, Dilip Arumugam,
and Lei Yang. 2015. Grounding english commands
to reward functions. In Robotics: Science and Sys-
tems.

Matthew MacMahon, Brian Stankiewics, and Ben-
jamin Kuipers. 2006. Walk the talk: Connecting
language, knowledge, action in route instructions.
In Proceedings of the National Conference on Ar-
tiﬁcial Intelligence.

Junhua Mao, Jonathan Huang, Alexander Toshev, Oana
Camburu, Alan Yuille, and Kevin Murphy. 2016.
Generation and Comprehension of Unambiguous
Object Descriptions. In Proceedings of IEEE Con-
ference on Computer Vision and Pattern Recogni-
tion.

Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012a. A joint
model of language and perception for grounded at-
tribute learning. In Proceedings of the International
Conference on Machine Learning.

Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer,
and Dieter Fox. 2012b. Learning to parse natural
language commands to a robot control system.
In
Proceedings of the International Symposium on Ex-
perimental Robotics.

Hongyuan Mei, Mohit Bansal, and R. Matthew Walter.
2016. What to talk about and how? selective gener-
ation using lstms with coarse-to-ﬁne alignment. In
Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies.

Dipendra Misra, John Langford, and Yoav Artzi. 2017.
Mapping instructions and visual observations to ac-
In Proceedings
tions with reinforcement learning.
of the Conference on Empirical Methods in Natural
Language Processing.

Dipendra K. Misra, Jaeyong Sung, Kevin Lee, and
Ashutosh Saxena. 2016. Tell me dave: Context-
sensitive grounding of natural language to manip-
ulation instructions. The International Journal of
Robotics Research, 35.

Kumar Dipendra Misra, Kejia Tao, Percy Liang, and
Ashutosh Saxena. 2015. Environment-driven lexi-
In Pro-
con induction for high-level instructions.
ceedings of the Annual Meeting of the Association
for Computational Linguistics and the International
Joint Conference on Natural Language Processing.

deep reinforcement learning. In Advances in Neural
Information Processing Systems.

Vinod Nair and Geoffrey E Hinton. 2010. Rectiﬁed lin-
ear units improve restricted boltzmann machines. In
Proceedings of the international conference on ma-
chine learning.

Andrew Y. Ng, Daishi Harada, and Stuart J. Russell.
1999. Policy invariance under reward transforma-
tions: Theory and application to reward shaping. In
Proceedings of the International Conference on Ma-
chine Learning.

Junhyuk Oh, Satinder P. Singh, Honglak Lee, and
Pushmeet Kohli. 2017. Zero-shot task generaliza-
tion with multi-task deep reinforcement learning. In
Proceedings of the international conference on ma-
chine learning.

Jan Peters and Stefan Schaal. 2008. Reinforcement
learning of motor skills with policy gradients. Neu-
ral networks, 21.

Benjamin Recht, Christopher Re, Stephen Wright, and
Feng Niu. 2011. Hogwild: A lock-free approach
to parallelizing stochastic gradient descent. In Ad-
vances in Neural Information Processing Systems.

Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
2015. U-net: Convolutional networks for biomed-
In International Confer-
ical image segmentation.
ence on Medical image computing and computer-
assisted intervention.

John Schulman, Philipp Moritz, Sergey Levine,
Michael I. Jordan, and Pieter Abbeel. 2015. High-
dimensional continuous control using generalized
advantage estimation. CoRR, abs/1506.02438.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overﬁtting. The Journal of Machine Learning
Research, 15.

Alane Suhr and Yoav Artzi. 2018. Situated mapping
of sequential instructions to actions with single-step
In Proceedings of the Annual
reward observation.
Meeting of the Association for Computational Lin-
guistics.

Richard S. Sutton, Doina Precup, and Satinder P. Singh.
Intra-option learning about temporally ab-
In Proceedings of the international

1998.
stract actions.
conference on machine learning.

Dmitry Ulyanov, Andrea Vedaldi, and Victor S.
Instance normalization: The
CoRR,

Lempitsky. 2016.
missing ingredient for fast stylization.
abs/1607.08022.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Alex Graves, Ioannis Antonoglou, Daan Wierstra,
and Martin A. Riedmiller. 2013. Playing atari with

Ronald J. Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine Learning, 8.

Wenhan Xiong, Xiaoxiao Guo, Mo Yu, Shiyu Chang,
Bowen Zhou, and William Yang Wang. 2018.
Scheduled policy optimization for natural language
communication with intelligent agents. In Proceed-
ings of the International Joint Conferences on Arti-
ﬁcial Intelligence.

Claudia Yan, Dipendra Kumar Misra, Andrew Ben-
nett, Aaron Walsman, Yonatan Bisk, and Yoav Artzi.
2018. Chalet: Cornell house agent learning environ-
ment. CoRR, abs/1801.07357.

Licheng Yu, Patrick Poirson, Shan Yang, Alexander C.
Berg, and Tamara L. Berg. 2016. Modeling context
in referring expressions. In Proceedings of the Eu-
ropean Conference on Computer Vision.

A Tasks and Data: Comparisons

Table 7 provides summary statistics comparing
LANI and CHAI to existing related resources.

B Reward Function

LANI Following Misra et al. (2017), we use a
shaped reward function that rewards the agent for
moving towards the goal location. The reward for
exampl i is:

R(i)(s, a, s(cid:48)) = R(i)

p + φ(i)(s) − φ(i)(s(cid:48))

(1)

where s(cid:48) is the origin state, a is the action, s is
(i)
is the problem reward, and
the target state, R
p
φ(i)(s)−φ(i) is a shaping term. We use a potential-
based shaping (Ng et al., 1999) that encourages the
agent to both move and turn towards the goal. The
potential function is:
φ(i)(s) =

δTURNDIST(s, s(i)
g )
+(1 − δ)MOVEDIST(s, s(i)

g ) ,

where MOVEDIST is the euclidean distance to the
goal normalized by the agent’s forward movement
distance, and TURNDIST is the angle the agent
needs to turn to face the goal normalized by the
agent’s turn angle. We use δ as a gating term,
which is 0 when the agent is near the goal and
increases monotonically towards 1 the further the
agent is from the goal. This decreases the sensi-
tivity of the potential function to the TURNDIST
(i)
term close to the goal. The problem reward R
p
provides a negative reward of up to -1 on collision
with any object or boundary (based on the angle
and magnitude of collision), a negative reward of
-0.005 on every action to discourage long trajec-
tories, a negative reward of -1 on an unsuccess-
ful stop, when the distance to the goal location is
greater than 5, and a positive reward of +1 on a
successful stop.
CHAI We use a similar potential based reward
function as LANI. Instead of rewarding the agent
to move towards the ﬁnal goal the model is re-
warded for moving towards the next intermedi-
ate goal. We heuristically generate intermediate
goals from the human demonstration by generat-
ing goals for objects to be interacted with, doors
that the agent should enter, and the ﬁnal position
of the agent. The potential function is:

φ(i)(s) =

TURNDIST(s, s(i)
MOVEDIST(s, s(i)

g,j) +
g,j) + INTDIST(s, s(i)

g,j) ,

(i)
g,j

where s
intermediate goal,
TURNDIST rewards the agent for turning to-

the next

is

wards the goal, MOVEDIST rewards the agent for
moving closer to the goal, and INTDIST rewards
the agent for accomplishing the interaction in the
intermediate goal. The goal is updated on being
accomplished. Besides the potential term, we
(i)
use a problem reward R
that gives a reward of
p
1 for stopping near a goal, -1 for colliding with
obstacles, and -0.002 as a verbosity penalty for
each step.

C Baseline Details

MISRA17 We use the model of Misra et al.
(2017). The model uses a convolution neural net-
work for encoding the visual observations, a re-
current neural network with LSTM units to en-
code the instruction, and a feed-forward network
to generate actions using these encodings. The
model is trained using policy gradient in a con-
textual bandit setting. We use the code provided
by the authors.
CHAPLOT18 We use the gated attention archi-
tecture of Chaplot et al. (2018). The model is
trained using policy gradient with generalized ad-
vantage estimation (Schulman et al., 2015). We
use the code provided by the authors.
Our Approach with Joint Training We train
the full model with policy gradient. We maxi-
mize the expected reward objective with entropy
regularization. Given a sampled goal location
lg ∼ p(. | ¯x, IP ) and a sampled action a ∼ p(. |
lg, (I1, p1), . . . , (It, pt)), the update is:
∇J ≈ {∇ log P (lg | ¯x, IP ) +

∇ log P (at | lg, (I1, p1), . . . , (It, pt))} R(st, a)
λ∇H(π(. | ˜st) .

We perform joint training with randomly initial-
ized goal prediction and action generation models.

D Hyperparameters

For LANI experiments, we use 5% of the training
data for tuning the hyperparameters and train on
the remaining. For CHAI, we use the development
set for tuning the hyperparameters. We train our
models for 20 epochs and ﬁnd the optimal stop-
ping epoch using the tuning set. We use 32 dimen-
sional embeddings for words and time. LSTMx
and LSTMA are single layer LSTMs with 256
hidden units. The ﬁrst layer of CNN0 contains
128 8×8 kernels with a stride of 4 and padding 3,
and the second layer contains 64 3×3 kernels with
a stride of 1 and padding 1. The convolution lay-
ers in LINGUNET use 32 5×5 kernels with stride

Dataset

Vocabulary Mean Instruction

Num
Instructions

Num.
Actions

Avg Trajectory
Length

Partially
Observed

Bisk et al.
(2016)
MacMahon
et al. (2006)
Matuszek et al.
(2012b)
Misra et al.
(2015)
LANI
CHAI

16,767

3,237

217

469

28,204
13,729

Size

1,426

563

39

775

2,292
1018

Length

15.27

7.96

6.65

48.7

12.07
10.14

81

3

3

>100

4
1028

15.4

3.12

N/A

21.5

24.6
54.5

No

Yes

No

No

Yes
Yes

Table 7: Comparison of LANI and CHAI to several existing natural language instructions corpora.

Category
Spatial relations
Location conjunction
Temporal coordination
Co-reference

Present Absent

2.56
3.85
1.70
1.98

1.77
1.93
2.14
1.98

p-value
.023
.226
.164
.993

Table 8: Mean goal prediction error for CHAI instruc-
tions with and without the analysis categories we used
in Table 2. The p-values are from two-sided t-tests
comparing the means in each row.

2. All deconvolutions except the ﬁnal one, also use
32 5×5 kernels with stride 2. The dropout proba-
bility in LINGUNET is 0.5. The size of attention
mask is 32×32 + 1. For both LANI and CHAI, we
use a camera angle of 60◦ and create panoramas
using 6 separate RGB images. Each image is of
size 128×128. We use a learning rate of 0.00025
and entropy coefﬁcient λ of 0.05.

E CHAI Error Analysis

Table 8 provides the same kind of error analysis
results here for the CHAI dataset as we produced
for LANI, comparing performance of the model on
samples of sentences with and without the analysis
phenomena that occurred in CHAI.

F Examples of Generated Goal

Prediction

Figure 7 shows example goal predictions from the
development sets. We found the predicted proba-
bility distributions to be reasonable even in many
cases where the agent failed to successfully com-
plete the task. We observed that often the eval-
uation metric is too strict for LANI instructions,
especially in cases of instruction ambiguity.

Success

Success

Failure

go round the ﬂowers

ﬂy between the palm tree and pond

head toward the wishing well and keep it on your right .

move back to the kitchen .

then drop the tropicana onto the coffee table .

walk the cup to the table and set the cup on the table .

Figure 7: Goal prediction probability maps Pg overlaid on the corresponding observed panoramas IP . The top
three examples show results from LANI, the bottom three from CHAI. The white arrow indicates the forward
direction that the agent is facing. The success/failure in the LANI examples indicate if the task was completed
accurately or not following the task completion (TC) metric.

Mapping Instructions to Actions in 3D Environments
with Visual Goal Prediction

Andrew Bennett
Dipendra Misra
Eyvind Niklasson Max Shatkhin

Valts Blukis
Yoav Artzi

Department of Computer Science and Cornell Tech, Cornell University, New York, NY, 10044
{dkm, awbennett, valts, yoav}@cs.cornell.edu
{een7, ms3448}@cornell.edu

9
1
0
2
 
r
a

M
 
8
1
 
 
]
L
C
.
s
c
[
 
 
2
v
6
8
7
0
0
.
9
0
8
1
:
v
i
X
r
a

Abstract

We propose to decompose instruction exe-
cution to goal prediction and action genera-
tion. We design a model that maps raw vi-
sual observations to goals using LINGUNET,
a language-conditioned image generation net-
work, and then generates the actions required
to complete them. Our model
is trained
from demonstration only without external re-
sources. To evaluate our approach, we intro-
duce two benchmarks for instruction follow-
ing: LANI, a navigation task; and CHAI, where
an agent executes household instructions. Our
evaluation demonstrates the advantages of our
model decomposition, and illustrates the chal-
lenges posed by our new benchmarks.

1

Introduction

Executing instructions in interactive environments
requires mapping natural language and observa-
tions to actions. Recent approaches propose learn-
ing to directly map from inputs to actions, for ex-
ample given language and either structured obser-
vations (Mei et al., 2016; Suhr and Artzi, 2018) or
raw visual observations (Misra et al., 2017; Xiong
et al., 2018). Rather than using a combination
of models, these approaches learn a single model
to solve language, perception, and planning chal-
lenges. This reduces the amount of engineering
required and eliminates the need for hand-crafted
meaning representations. At each step, the agent
maps its current inputs to the next action using a
single learned function that is executed repeatedly
until task completion.

Although executing the same computation at
each step simpliﬁes modeling, it exempliﬁes cer-
tain inefﬁciencies; while the agent needs to de-
cide what action to take at each step, identifying
its goal is only required once every several steps
or even once per execution. The left instruction in
Figure 1 illustrates this. The agent can compute its

After reaching the hydrant
head towards
the blue
fence and pass towards the
right side of the well.

Put the cereal, the sponge,
and the dishwashing soap
into the cupboard above
the sink.

Figure 1: Example instructions from our two tasks:
LANI (left) and CHAI (right). LANI is a landmark nav-
igation task, and CHAI is a corpus of instructions in the
CHALET environment.

goal once given the initial observation, and given
this goal can then generate the actions required.
In this paper, we study a new model that explic-
itly distinguishes between goal selection and ac-
tion generation, and introduce two instruction fol-
lowing benchmark tasks to evaluate it.

Our model decomposes into goal prediction and
action generation. Given a natural language in-
struction and system observations, the model pre-
dicts the goal to complete. Given the goal, the
model generates a sequence of actions.

The key challenge we address is designing the
goal representation. We avoid manually designing
a meaning representation, and predict the goal in
the agent’s observation space. Given the image of
the environment the agent observes, we generate a
probability distribution over the image to highlight
the goal location. We treat this prediction as image
generation, and develop LINGUNET, a language
conditioned variant of the U-NET image-to-image
architecture (Ronneberger et al., 2015). Given the
visual goal prediction, we generate actions using a
recurrent neural network (RNN).

Our model decomposition offers two key advan-
tages. First, we can use different learning methods
as appropriate for the goal prediction and action

generation problems. We ﬁnd supervised learning
more effective for goal prediction, where only a
limited amount of natural language data is avail-
able. For action generation, where exploration is
critical, we use policy gradient in a contextual ban-
dit setting (Misra et al., 2017). Second, the goal
distribution is easily interpretable by overlaying it
on the agent observations. This can be used to in-
crease the safety of physical systems by letting the
user verify the goal before any action is executed.
Despite the decomposition, our approach retains
the advantages of the single-model approach.
It
does not require designing intermediate represen-
tations, and training does not rely on external re-
sources, such as pre-trained parsers or object de-
tectors, instead using demonstrations only.

We introduce two new benchmark tasks with
different levels of complexity of goal prediction
and action generation. LANI is a 3D navigation
environment and corpus, where an agent navigates
between landmarks. The corpus includes 6,000
sequences of natural language instructions, each
containing on average 4.7 instructions. CHAI is
a corpus of 1,596 instruction sequences, each in-
cluding 7.7 instructions on average, for CHALET,
a 3D house environment (Yan et al., 2018).
In-
structions combine navigation and simple manipu-
lation, including moving objects and opening con-
tainers. Both tasks require solving language chal-
lenges, including spatial and temporal reasoning,
as well as complex perception and planning prob-
lems. While LANI provides a task where most in-
structions include a single goal, the CHAI instruc-
tions often require multiple intermediate goals.
For example, the household instruction in Fig-
ure 1 can be decomposed to eight goals: opening
the cupboard, picking each item and moving it to
the cupboard, and closing the cupboard. Achiev-
ing each goal requires multiple actions of differ-
ent types, including moving and acting on objects.
This allows us to experiment with a simple varia-
tion of our model to generate intermediate goals.

We compare our approach to multiple recent
methods. Experiments on the LANI navigation
task indicate that decomposing goal prediction
and action generation signiﬁcantly improves in-
struction execution performance. While we ob-
serve similar trends on the CHAI instructions, re-
sults are overall weaker, illustrating the complex-
ity of the task. We also observe that inherent
ambiguities in instruction following make exact

goal identiﬁcation difﬁcult, as demonstrated by
imperfect human performance. However, the gap
to human-level performance still remains large
across both tasks. Our code and data are available
at github.com/clic-lab/ciff.

2 Technical Overview

Task Let X be the set of all instructions, S the
set of all world states, and A the set of all actions.
An instruction ¯x ∈ X is a sequence (cid:104)x1, . . . , xn(cid:105),
where each xi is a token. The agent executes
instructions by generating a sequence of actions,
and indicates execution completion with the spe-
cial action STOP.

The sets of actions A and states S are domain
speciﬁc. In the navigation domain LANI, the ac-
tions include moving the agent and changing its
orientation. The state information includes the po-
sition and orientation of the agent and the differ-
ent landmarks. The agent actions in the CHALET
house environment include moving and changing
the agent orientation, as well as an object interac-
tion action. The state encodes the position and ori-
entation of the agent and all objects in the house.
For interactive objects, the state also includes their
status, for example if a drawer is open or closed.
In both domains, the actions are discrete. The do-
mains are described in Section 6.
Model The agent does not observe the world
state directly, but instead observes its pose and an
RGB image of the environment from its point of
view. We deﬁne these observations as the agent
context ˜s. An agent model is a function from an
agent context ˜s to an action a ∈ A. We model
goal prediction as predicting a probability distri-
bution over the agent visual observations, repre-
senting the likelihood of locations or objects in the
environment being target positions or objects to be
acted on. Our model is described in Section 4.
Learning We assume access to training data
i=1, where ¯x(i)
with N examples {(¯x(i), s
(i)
(i)
is the
is an instruction, s
g
1
goal state. We decompose learning; training goal
prediction using supervised learning, and action
generation using oracle goals with policy gradient
in a contextual bandit setting. We assume an in-
strumented environment with access to the world
state, which is used to compute rewards during
training only. Learning is described in Section 5.
Evaluation We evaluate task performance on a
(i)
i=1, where ¯x(i) is an in-
g )}M
test set {(¯x(i), s

is a start state, and s

(i)
g )}N

(i)
1 , s

(i)
1 , s

(i)
1

struction, s
is the goal
is a start state, and s
state. We evaluate task completion accuracy and
the distance of the agent’s ﬁnal state to s

(i)
g .

(i)
g

3 Related Work

Mapping instruction to action has been studied
extensively with intermediate symbolic represen-
tations (e.g., Chen and Mooney, 2011; Kim and
Mooney, 2012; Artzi and Zettlemoyer, 2013; Artzi
et al., 2014; Misra et al., 2015, 2016). Recently,
there has been growing interest in direct mapping
from raw visual observations to actions (Misra
et al., 2017; Xiong et al., 2018; Anderson et al.,
2018; Fried et al., 2018). We propose a model that
enjoys the beneﬁts of such direct mapping, but ex-
plicitly decomposes that task to interpretable goal
prediction and action generation. While we focus
on natural language, the problem has also been
studied using synthetic language (Chaplot et al.,
2018; Hermann et al., 2017).

Our model design is related to hierarchical re-
inforcement learning, where sub-policies at differ-
ent levels of the hierarchy are used at different fre-
quencies (Sutton et al., 1998). Oh et al. (2017)
uses a two-level hierarchy for mapping synthetic
language to actions. Unlike our visual goal rep-
resentation, they use an opaque vector representa-
tion. Also, instead of reinforcement learning, our
methods emphasize sample efﬁciency.

Goal prediction is related to referring expres-
sion interpretation (Matuszek et al., 2012a; Krish-
namurthy and Kollar, 2013; Kazemzadeh et al.,
2014; Kong et al., 2014; Yu et al., 2016; Mao et al.,
2016; Kitaev and Klein, 2017). While our model
solves a similar problem for goal prediction, we
focus on detecting visual goals for actions, includ-
ing both navigation and manipulation, as part of
an instruction following model. Using formal goal
representation for instruction following was stud-
ied by MacGlashan et al. (2015). In contrast, our
model generates a probability distribution over im-
ages, and does not require an ontology.

Our data collection is related to existing work.
LANI is inspired by the HCRC Map Task (An-
derson et al., 1991), where a leader directs a fol-
lower to navigate between landmarks on a map.
We use a similar task, but our scalable data collec-
tion process allows for a signiﬁcantly larger cor-
pus. We also provide an interactive navigation
environment, instead of only map diagrams. Un-
like Map Task, our leaders and followers do not
interact in real time. This abstracts away inter-

action challenges, similar to how the SAIL nav-
igation corpus was collected (MacMahon et al.,
2006). CHAI instructions were collected using
scenarios given to workers, similar to the ATIS
collection process (Hemphill et al., 1990; Dahl
et al., 1994). Recently, multiple 3D research envi-
ronments were released. LANI has a signiﬁcantly
larger state space than existing navigation envi-
ronments (Hermann et al., 2017; Chaplot et al.,
2018), and CHALET, the environment used for
CHAI, is larger and has more complex manipu-
lation compared to similar environments (Gordon
et al., 2018; Das et al., 2018). In addition, only
synthetic language data has been released for these
environment. An exception is the Room-to-Room
dataset (Anderson et al., 2018) that makes use of
an environment of connected panoramas of house
settings. Although it provides a realistic vision
challenge, unlike our environments, the state space
is limited to a small number of panoramas and ma-
nipulation is not possible.

4 Model

We model the agent policy as a neural network.
The agent observes the world state st at time t as
an RGB image It. The agent context ˜st, the infor-
mation available to the agent to select the next ac-
tion at, is a tuple (¯x, IP , (cid:104)(I1, p1), . . . , (It, pt)(cid:105)),
where ¯x is the natural
language instructions,
IP is a panoramic view of the environment
from the starting position at time t = 1, and
(cid:104)(I1, p1), . . . , (It, pt)(cid:105) is the sequence of observa-
tions It and poses pt up to time t. The panorama
IP is generated through deterministic exploration
by rotating 360◦ to observe the environment at the
beginning of the execution.1

The model includes two main components: goal
prediction and action generation. The agent uses
the panorama IP to predict the goal location lg. At
each time step t, a projection of the goal location
into the agent’s current view Mt is given as input
to an RNN to generate actions. The probability of
an action at at time t decomposes to:
(cid:16)

P (at | ˜st) =

P (lg | ¯x, IP )

(cid:88)

lg

P (at | lg, (I1, p1), . . . , (It, pt))

,

(cid:17)

where the ﬁrst term puts the complete distribution
mass on a single location (i.e., a delta function).
Figure 2 illustrates the model.

1The panorama is a concatenation of deterministic obser-
vations along the width dimension. For simplicity, we do not
include these deterministic steps in the execution.

Figure 2: An illustration for our architecture (Section 4) for the instruction turn left and go to the red oil drum
with a LINGUNET depth of m = 4. The instruction ¯x is mapped to ¯x with an RNN, and the initial panorama
observation IP to F0 with a CNN. LINGUNET generates H1, a visual representation of the goal. First, a sequence
of convolutions maps the image features F0 to feature maps F1,. . . ,F4. The text representation ¯x is used to
generate the kernels K1,. . . ,K4, which are convolved to generate the text-conditioned feature maps G1,. . . ,G4.
These feature maps are de-convolved to H1,. . . ,H4. The goal probability distribution Pg is computed from H1.
The goal location is the inferred from the max of Pg. Given lg and pt, the pose at step t, the goal mask Mt is
computed and passed into an RNN that outputs the action to execute.

Goal Prediction To predict the goal location,
we generate a probability distribution Pg over
a feature map F0 generated using convolutions
from the initial panorama observation IP . Each
element in the probability distribution Pg corre-
sponds to an area in IP . Given the instruction
¯x and panorama IP , we ﬁrst generate their rep-
resentations. From the panorama IP , we gener-
ate a feature map F0 = [CNN0(IP ); Fp], where
CNN0 is a two-layer convolutional neural net-
work (CNN; LeCun et al., 1998) with rectiﬁed
linear units (ReLU; Nair and Hinton, 2010) and
Fp are positional embeddings.2 The concatena-
tion is along the channel dimension. The instruc-
tion ¯x = (cid:104)x1, · · · xn(cid:105) is mapped to a sequence
of hidden states li = LSTMx(ψx(xi), li−1), i =
1, . . . , n using a learned embedding function ψx
and a long short-term memory (LSTM; Hochre-
iter and Schmidhuber, 1997) RNN LSTMx. The
instruction representation is ¯x = ln.

We generate the probability distribution Pg over
pixels in F0 using LINGUNET. The architecture
of LINGUNET is inspired by the U-NET image
generation method (Ronneberger et al., 2015), ex-
cept that the reconstruction phase is conditioned
on the natural language instruction. LINGUNET
ﬁrst applies m convolutional layers to generate a
sequence of feature maps Fj = CNNj(Fj−1),

2We generate Fp by creating a channel for each determin-
istic observation used to create the panorama, and setting all
the pixels corresponding to that observation location in the
panorama to 1 and all others to 0. The number of observa-
tions depends on the agent’s camera angle.

j = 1 . . . m, where each CNNj is a convolutional
layer with leaky ReLU non-linearities (Maas et al.,
2013) and instance normalization (Ulyanov et al.,
2016). The instruction representation ¯x is split
evenly into m vectors {¯xj}m
j=1, each is used to
create a 1 × 1 kernel Kj = AFFINEj(¯xj), where
each AFFINEj is an afﬁne transformation followed
by normalizing and reshaping. For each Fj, we
apply a 2D 1 × 1 convolution using the text ker-
nel Kj to generate a text-conditioned feature map
Gj = CONVOLVE(Kj, Fj), where CONVOLVE
convolves the kernel over the feature map. We
then perform m deconvolutions to generate a se-
quence of feature maps Hm,. . . ,H1:

Hm = DECONVm(DROPOUT(Gm))
Hj = DECONVj([Hj+1; Gj]) .

DROPOUT is dropout regularization (Srivastava
et al., 2014) and each DECONVj
is a decon-
volution operation followed a leaky ReLU non-
linearity and instance norm.3 Finally, we gener-
ate Pg by applying a softmax to H1 and an ad-
ditional learned scalar bias term bg to represent
events where the goal is out of sight. For example,
when the agent already stands in the goal position
and therefore the panorama does not show it.

We use Pg to predict the goal position in the
environment. We ﬁrst select the goal pixel in F0 as
the pixel corresponding to the highest probability
element in Pg. We then identify the corresponding
3D location lg in the environment using backward
camera projection, which is computed given the

3DECONV1 does deconvolution only.

camera parameters and p1, the agent pose at the
beginning of the execution.
Action Generation Given the predicted goal lg,
we generate actions using an RNN. At each time
step t, given pt, we generate the goal mask Mt,
which has the same shape as the observed image
It. The goal mask Mt has a value of 1 for each
element that corresponds to the goal location lg in
It. We do not distinguish between visible or oc-
cluded locations. All other elements are set to 0.
We also maintain an out-of-sight ﬂag ot that is set
to 1 if (a) lg is not within the agent’s view; or (b)
the max scoring element in Pg corresponds to bg,
the term for events when the goal is not visible in
IP . Otherwise, ot is set to 0. We compute an ac-
tion generation hidden state yt with an RNN:

yt = LSTMA (AFFINEA([FLAT(Mt); ot]), yt−1) ,

where FLAT ﬂattens Mt into a vector, AFFINEA
is a learned afﬁne transformation with ReLU, and
LSTMA is an LSTM RNN. The previous hidden
state yt−1 was computed when generating the pre-
vious action, and the RNN is extended gradually
during execution. Finally, we compute a probabil-
ity distribution over actions:

P (at | lg, (I1, p1), . . . , (It, pt)) =

SOFTMAX(AFFINEp([yt; ψT (t)])) ,

where ψT is a learned embedding lookup table
for the current time (Chaplot et al., 2018) and
AFFINEp is a learned afﬁne transformation.
Model Parameters The model parameters θ in-
clude the parameters of the convolutions CNN0
and the components of LINGUNET: CNNj,
for j = 1, . . . , m.
AFFINEj, and DECONVj
In addition we learn two afﬁne transformations
AFFINEA and AFFINEp, two RNNs LSTMx and
LSTMA, two embedding functions ψx and ψT ,
and the goal distribution bias term bg. In our ex-
periments (Section 7), all parameters are learned
without external resources.

5 Learning

Our modeling decomposition enables us to choose
different learning algorithms for the two parts.
While reinforcement learning is commonly de-
ployed for tasks that beneﬁt from exploration (e.g.,
Peters and Schaal, 2008; Mnih et al., 2013), these
methods require many samples due to their high
sample complexity. However, when learning with
natural language, only a relatively small number
of samples is realistically available. This problem

was addressed in prior work by learning in a con-
textual bandit setting (Misra et al., 2017) or mix-
ing reinforcement and supervised learning (Xiong
et al., 2018). Our decomposition uniquely offers
to tease apart the language understanding prob-
lem and address it with supervised learning, which
generally has lower sample complexity. For action
generation though, where exploration can be au-
tonomous, we use policy gradient in a contextual
bandit setting (Misra et al., 2017).

(i)
1

(i)
1 , s

(i)
g )}N

We assume access to training data with N ex-
i=1, where ¯x(i) is an in-
amples {(¯x(i), s
(i)
struction, s
is the goal
is a start state, and s
g
state. We train the goal prediction component by
minimizing the cross-entropy of the predicted dis-
tribution with the gold-standard goal distribution.
The gold-standard goal distribution is a determin-
istic distribution with probability one at the pixel
corresponding to the goal location if the goal is in
the ﬁeld of view, or probability one at the extra
out-of-sight position otherwise. The gold location
(i)
is the agent’s location in s
g . We update the model
parameters using Adam (Kingma and Ba, 2014).

We train action generation by maximizing the
expected immediate reward the agent observes
while exploring the environment. The objective
for a single example i and time stamp t is:

J =

π(a | ˜st)R(i)(st, a) + λH(π(. | ˜st)) ,

(cid:88)

a∈A

where R(i) : S × A → R is an example-speciﬁc
reward function, H(·) is an entropy regularization
term, and λ is the regularization coefﬁcient. The
reward function R(i) details are described in de-
tails in Appendix B. Roughly speaking, the re-
ward function includes two additive components:
a problem reward and a shaping term (Ng et al.,
1999). The problem reward provides a positive re-
ward for successful task completion, and a nega-
tive reward for incorrect completion or collision.
The shaping term is positive when the agent gets
closer to the goal position, and negative if it is
moving away. The gradient of the objective is:

∇J =

π(a | ˜st)∇ log π(a | ˜st)R(st, a)

(cid:88)

a∈A

+λ∇H(π(. | ˜st) .

We approximate the gradient by sampling an ac-
tion using the policy (Williams, 1992), and use the
(i)
gold goal location computed from s
g . We per-
form several parallel rollouts to compute gradients
and update the parameters using Hogwild! (Recht
et al., 2011) and Adam learning rates.

Dataset Statistic
Number paragraphs
Mean instructions per paragraph
Mean actions per instruction
Mean tokens per instruction
Vocabulary size

LANI
6,000
4.7
24.6
12.1
2,292

CHAI
1,596
7.70
54.5
8.4
1,018

Table 1: Summary statistics of the two corpora.

6 Tasks and Data

6.1 LANI

The goal of LANI is to evaluate how well an agent
can follow navigation instructions. The agent task
is to follow a sequence of instructions that specify
a path in an environment with multiple landmarks.
Figure 1 (left) shows an example instruction.

The environment

is a fenced, square, grass
ﬁeld. Each instance of the environment con-
tains between 6–13 randomly placed landmarks,
sampled from 63 unique landmarks. The agent
can take four types of discrete actions: FORWARD,
TURNRIGHT, TURNLEFT, and STOP. The ﬁeld is
of size 50×50, the distance of the FORWARD ac-
tion is 1.5, and the turn angle is 15◦. The en-
vironment simulator is implemented in Unity3D.
At each time step, the agent performs an action,
observes a ﬁrst person view of the environment
as an RGB image, and receives a scalar reward.
The simulator provides a socket API to control the
agent and the environment.

Agent performance is evaluated using two met-
rics: task completion accuracy, and stop distance
error. A task is completed correctly if the agent
stops within an aerial distance of 5 from the goal.
We collect a corpus of navigation instructions
using crowdsourcing. We randomly generate en-
vironments, and generate one reference path for
each environment. To elicit linguistically interest-
ing instructions, reference paths are generated to
pass near landmarks. We use Amazon Mechanical
Turk, and split the annotation process to two tasks.
First, given an environment and a reference path,
a worker writes an instruction paragraph for fol-
lowing the path. The second task requires another
worker to control the agent to perform the instruc-
tions and simultaneously mark at each point what
part of the instruction was executed. The record-
ing of the second worker creates the ﬁnal data of
segmented instructions and demonstrations. The
generated reference path is displayed in both tasks.
The second worker could also mark the paragraph
as invalid. Both tasks are done from an over-
head view of the environment, but workers are in-
structed to provide instructions for a robot that ob-

[Go around the pillar on the right hand side] [and head
towards the boat, circling around it clockwise.] [When
you are facing the tree, walk towards it, and the pass on
the right hand side,] [and the left hand side of the cone.
Circle around the cone,] [and then walk past the hydrant
on your right,] [and the the tree stump.] [Circle around
the stump and then stop right behind it.]

Figure 3: Segmented instructions in the LANI domain.
The original reference path is marked in red (start) and
blue (end). The agent, using a drone icon, is placed at
the beginning of the path. The follower path is coded in
colors to align to the segmented instruction paragraph.

serves the environment from a ﬁrst person view.
Figure 3 shows a reference path and the written
instruction. This data can be used for evaluating
both executing sequences of instructions and sin-
gle instructions in isolation.

Table 1 shows the corpus statistics.4 Each para-
graph corresponds to a single unique instance of
the environment. The paragraphs are split into
train, test, and development, with a 70% / 15% /
15% split. Finally, we sample 200 single devel-
opment instructions for qualitative analysis of the
language challenge the corpus presents (Table 2).

6.2 CHAI

The CHAI corpus combines both navigation and
simple manipulation in a complex, simulated
household environment. We use the CHALET sim-
ulator (Yan et al., 2018), a 3D house simulator
that provides multiple houses, each with multi-
ple rooms. The environment supports moving be-
tween rooms, picking and placing objects, and
opening and closing cabinets and similar contain-
ers. Objects can be moved between rooms and
in and out of containers. The agent observes the
world in ﬁrst-person view, and can take ﬁve ac-
tions: FORWARD, TURNLEFT, TURNRIGHT, STOP,
and INTERACT. The INTERACT action acts on ob-
jects.
It takes as argument a 2D position in the
agent’s view. Agent performance is evaluated with
two metrics: (a) stop distance, which measures the
distance of the agent’s ﬁnal state to the ﬁnal an-
notated position; and (b) manipulation accuracy,
which compares the set of manipulation actions

4Appendix A provides statistics for related datasets.

Category
Spatial relations
between locations
Conjunctions of two
more locations
Temporal coordination
of sub-goals
Constraints on the
shape of trajectory

Co-reference

Comparatives

Count

LANI

CHAI

123

36

65

94

32

2

52

68

5

0

18

0

Example
LANI: go to the right side of the rock
CHAI: pick up the cup next to the bathtub and place it on . . .
LANI: ﬂy between the mushroom and the yellow cone
CHAI: . . . set it on the table next to the juice and milk.
LANI: at the mushroom turn right and move forward towards the statue
CHAI: go back to the kitchen and put the glass in the sink.

LANI: go past the house by the right side of the apple

LANI: turn around it and move in front of fern plant
CHAI: turn left, towards the kitchen door and move through it.
LANI: . . . the small stone closest to the blue and white fences stop

Table 2: Qualitative analysis of the LANI and CHAI corpora. We sample 200 single development instructions from
each corpora. For each category, we count how many examples of the 200 contained it and show an example.

Scenario
You have several hours before guests begin to arrive for
a dinner party. You are preparing a wide variety of meat
dishes, and need to put them in the sink.
In addition,
you want to remove things in the kitchen, and bathroom
which you don’t want your guests seeing, like the soaps
in the bathroom, and the dish cleaning items. You can
put these in the cupboards. Finally, put the dirty dishes
around the house in the dishwasher and close it.
Written Instructions
[In the kitchen, open the cupboard above the sink.] [Put
the cereal, the sponge, and the dishwashing soap into the
cupboard above the sink.] [Close the cupboard.] [Pick
up the meats and put them into the sink.] [Open the dish-
washer, grab the dirty dishes on the counter, and put the
dishes into the dishwasher.]

Figure 4: Scenario and segmented instruction from the
CHAI corpus.

to a reference set. When measuring distance, to
consider the house plan, we compute the minimal
aerial distance for each room that must be visited.
Yan et al. (2018) provides the full details of the
simulator and evaluation. We use ﬁve different
houses, each with up to six rooms. Each room
contains on average 30 objects. A typical room
is of size 6×6. We set the distance of FORWARD to
0.1, the turn angle to 90◦, and divide the agent’s
view to a 32×32 grid for the INTERACT action.

We collected a corpus of navigation and ma-
nipulation instructions using Amazon Mechanical
Turk. We created 36 common household scenar-
ios to provide a familiar context to the task.5 We
use two crowdsourcing tasks. First, we provide
workers with a scenario and ask them to write in-
structions. The workers are encouraged to explore
the environment and interact with it. We then seg-
ment the instructions to sentences automatically.
In the second task, workers are presented with the
segmented sentences in order and asked to execute
them. After ﬁnishing a sentence, the workers re-

5We observed that asking workers to simply write instruc-
tions without providing a scenario leads to combinations of
repetitive instructions unlikely to occur in reality.

quest the next sentence. The workers do not see
the original scenario. Figure 4 shows a scenario
and the written segmented paragraph. Similar to
LANI, CHAI data can be used for studying com-
plete paragraphs and single instructions.

Table 1 shows the corpus statistics.6 The para-
graphs are split into train, test, and development,
with a 70% / 15% / 15% split. Table 2 shows qual-
itative analysis of a sample of 200 instructions.

7 Experimental Setup

instructions.

Method Adaptations for CHAI We apply two
modiﬁcations to our model to support interme-
diate goal for the CHAI
First,
we train an additional RNN to predict the se-
quence of intermediate goals given the instruc-
There are two types of goals:
tion only.
NAVIGATION,
for action sequences requiring
movement only and ending with the STOP action;
and INTERACTION, for sequence of movement ac-
tions that end with an INTERACT action. For ex-
ample, for the instruction pick up the red book
and go to the kitchen, the sequence of goals will
be (cid:104)INTERACTION, NAVIGATION, NAVIGATION(cid:105).
This indicates the agent must ﬁrst move to the
object to pick it up via interaction, move to the
kitchen door, and ﬁnally move within the kitchen.
The process of executing an instruction starts with
predicting the sequence of goal types. We call our
model (Section 4) separately for each goal type.
The execution concludes when the ﬁnal goal is
completed. For learning, we create a separate ex-
ample for each intermediate goal and train the ad-
ditional RNN separately. The second modiﬁcation
is replacing the backward camera projection for
inferring the goal location with ray casting to iden-

6The number of actions per instruction is given in the
more ﬁne-grained action space used during collection. To
make the required number of actions smaller, we use the more
coarse action space speciﬁed.

Method
STOP
RANDOMWALK
MOSTFREQUENT
MISRA17
CHAPLOT18
Our Approach (OA)
OA w/o RNN
OA w/o Language
OA w/joint
OA w/oracle goals

LANI

CHAI

SD
15.37
14.80
19.31
10.54
9.05
8.65
9.21
10.65
11.54
2.13

TC
8.20
9.66
2.94
22.9
31.0
35.72
31.30
23.02
21.76
94.60

SD
2.99
2.99
3.80
2.99
2.99
2.75
3.75
3.22
2.99
2.19

MA
37.53
28.96
37.53
32.25
37.53
37.53
37.43
37.53
36.90
41.07

Table 3: Performance on the development data.

tify INTERACTION goals, which are often objects
that are not located on the ground.
Baselines We compare our approach against the
following baselines: (a) STOP: Agent stops im-
mediately; (b) RANDOMWALK: Agent samples
actions uniformly until it exhausts the horizon
or stops; (c) MOSTFREQUENT: Agent takes the
most frequent action in the data, FORWARD for
both datasets, until it exhausts the horizon; (d)
the approach of Misra et al. (2017);
MISRA17:
and (e) CHAPLOT18:
the approach of Chaplot
et al. (2018). We also evaluate goal prediction and
compare to the method of Janner et al. (2018) and
a CENTER baseline, which always predict the cen-
ter pixel. Appendix C provides baseline details.
Evaluation Metrics We evaluate using the met-
rics described in Section 6: stop distance (SD) and
task completion (TC) for LANI, and stop distance
(SD) and manipulation accuracy (MA) for CHAI.
To evaluate the goal prediction, we report the real
distance of the predicted goal from the annotated
goal and the percentage of correct predictions. We
consider a goal correct if it is within a distance of
5.0 for LANI and 1.0 for CHAI. We also report
human evaluation for LANI by asking raters if the
generated path follows the instruction on a Likert-
type scale of 1–5. Raters were shown the gener-
ated path, the reference path, and the instruction.
Parameters We use a horizon of 40 for both
domains. During training, we allow additional
5 steps to encourage learning even after errors.
When using intermediate goals in CHAI, the hori-
zon is used for each intermediate goal separately.
All other parameters and detailed in Appendix D.

8 Results

Tables 3 and 4 show development and test re-
sults. Both sets of experiments demonstrate sim-
ilar trends. The low performance of STOP, RAN-
DOMWALK, and MOSTFREQUENT demonstrates

Method
STOP
RANDOMWALK
MOSTFREQUENT
MISRA17
CHAPLOT18
Our Approach

LANI

CHAI

SD
15.18
14.63
19.14
10.23
8.78
8.43

TC
8.29
9.76
3.15
23.2
31.9
36.9

SD
3.59
3.59
4.36
3.59
3.59
3.34

MA
39.77
33.29
39.77
36.84
39.76
39.97

Table 4: Performance on the held-out test dataset.

Method
CENTER
Janner et al. (2018)
Our Approach

LANI

CHAI

Dist
12.0
9.61
8.67

Acc
19.51
30.26
35.83

Dist
3.41
2.81
2.12

Acc
19.0
28.3
40.3

Table 5: Development goal prediction performance.
We measure distance (Dist) and accuracy (Acc).

the challenges of both tasks, and shows the tasks
are robust to simple biases. On LANI, our ap-
proach outperforms CHAPLOT18, improving task
completion (TC) accuracy by 5%, and both meth-
ods outperform MISRA17. On CHAI, CHAP-
LOT18 and MISRA17 both fail to learn, while
our approach shows an improvement on stop dis-
tance (SD). However, all models perform poorly
on CHAI, especially on manipulation (MA).

To isolate navigation performance on CHAI, we
limit our train and test data to instructions that in-
clude navigation actions only. The STOP baseline
on these instructions gives a stop distance (SD) of
3.91, higher than the average for the entire data
as these instructions require more movement. Our
approach gives a stop distance (SD) of 3.24, a 17%
reduction of error, signiﬁcantly better than the 8%
reduction of error over the entire corpus.

We also measure human performance on a sam-
ple of 100 development examples for both tasks.
On LANI, we observe a stop distance error (SD)
of 5.2 and successful task completion (TC) 63%
of the time. On CHAI, the human distance er-
ror (SD) is 1.34 and the manipulation accuracy is
100%. The imperfect performance demonstrates
the inherent ambiguity of the tasks. The gap to
human performance is still large though, demon-
strating that both tasks are largely open problems.
The imperfect human performance raises ques-
tions about automated evaluation.
In general,
we observe that often measuring execution qual-
ity with rigid goals is insufﬁcient. We conduct
a human evaluation with 50 development exam-
ples from LANI rating human performance and
our approach. Figure 5 shows a histogram of the
ratings. The mean rating for human followers is
4.38, while our approach’s is 3.78; we observe
a similar trend to before with this metric. Using

Category
Spatial relations
Location conjunction
Temporal coordination
Trajectory constraints
Co-reference
Comparatives

Present Absent
10.09
9.05
8.24
8.99
8.59
9.25

8.75
10.19
11.38
9.56
12.88
10.22

p-value
.262
.327
.015
.607
.016
.906

Table 6: Mean goal prediction error for LANI instruc-
tions with and without the analysis categories we used
in Table 2. The p-values are from two-sided t-tests
comparing the means in each row.

e
g
a
t
n
e
c
r
e
P

60

40

20

0

Human
Our Approach

1

2

3

4

5

Figure 5: Likert rating histogram for expert human fol-
lower and our approach for LANI.

judgements on our approach, we correlate the hu-
man metric with the SD measure. We observe a
Pearson correlation -0.65 (p=5e-7), indicating that
our automated metric correlates well with human
judgment.7 This initial study suggests that our au-
tomated evaluation is appropriate for this task.

Our ablations (Table 3) demonstrate the impor-
tance of each of the components of the model.
We ablate the action generation RNN (w/o RNN),
completely remove the language input (w/o Lan-
guage), and train the model jointly (w/joint Learn-
ing).8 On CHAI especially, ablations results in
models that display ineffective behavior. Of the
ablations, we observe the largest beneﬁt from
decomposing the learning and using supervised
learning for the language problem.

We also evaluate our approach with access to
oracle goals (Table 3). We observe this im-
proves navigation performance signiﬁcantly on
both tasks. However, the model completely fails
to learn a reasonable manipulation behavior for
CHAI. This illustrates the planning complexity
of this domain. A large part of the improvement
in measured navigation behavior is likely due to
eliminating much of the ambiguity the automated
metric often fails to capture.

Finally, on goal prediction (Table 5), our ap-
proach outperforms the method of Janner et al.
(2018). Figure 6 and Appendix Figure 7 show ex-
ample goal predictions. In Table 6, we break down
LANI goal prediction results for the analysis cate-

7We did not observe this kind of clear anti-correlation
comparing the two results for human performance (Pearson
correlation of 0.09 and p=0.52). The limited variance in hu-
man performance makes correlation harder to test.

8Appendix C provides the details of joint learning.

curve around big rock keeping it to your left .

walk over to the cabinets and open the cabinet doors up

Figure 6: Goal prediction probability maps Pg overlaid
on the corresponding observed panoramas IP . The top
example shows a result on LANI, the bottom on CHAI.
gories we used in Table 2 using the same sample of
the data. Appendix E includes a similar table for
CHAI. We observe that our approach ﬁnds instruc-
tions with temporal coordination or co-reference
challenging. Co-reference is an expected limita-
tion; with single instructions, the model can not
resolve references to previous instructions.
9 Discussion
We propose a model for instruction following with
explicit separation of goal prediction and action
generation. Our representation of goal prediction
is easily interpretable, while not requiring the de-
sign of logical ontologies and symbolic represen-
tations. A potential limitation of our approach is
cascading errors. Action generation relies com-
pletely on the predicted goal and is not exposed
to the language otherwise. This also suggests a
second related limitation:
the model is unlikely
to successfully reason about instructions that in-
clude constraints on the execution itself. While
the model may reach the ﬁnal goal correctly, it is
unlikely to account for the intermediate trajectory
constraints. As we show (Table 2), such instruc-
tions are common in our data. These two limita-
tions may be addressed by allowing action genera-
tion access to the instruction. Achieving this while
retaining an interpretable goal representation that
clearly determines the execution is an important
direction for future work. Another important open
question concerns automated evaluation, which re-
mains especially challenging when instructions do
not only specify goals, but also constraints on how
to achieve them. Our resources provide the plat-
form and data to conduct this research.
Acknowledgments
This research was supported by NSF (CAREER-
1750499), Schmidt Sciences, a Google Faculty
Award, and cloud credits from Microsoft. We
thank John Langford, Claudia Yan, Bharath Har-
iharan, Noah Snavely, the Cornell NLP group, and
the anonymous reviewers for their advice.

References
Anne H Anderson, Miles Bader, Ellen Gurman Bard,
Elizabeth Boyle, Gwyneth Doherty, Simon Garrod,
Stephen Isard, Jacqueline Kowtko, Jan McAllister,
Jim Miller, Catherine Sotillo, Henry S. Thompson,
and Regina Weinert. 1991. The HCRC map task
corpus. Language and Speech, 34.

Peter Anderson, Qi Wu, Damien Teney, Jake Bruce,
Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen
Gould, and Anton van den Hengel. 2018. Vision-
and-language navigation:
Interpreting visually-
grounded navigation instructions in real environ-
ments. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition.

Yoav Artzi, Dipanjan Das, and Slav Petrov. 2014.
Learning compact lexicons for CCG semantic pars-
ing. In Proceedings of the 2014 Conference on Em-
pirical Methods in Natural Language Processing.

Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion of Computational Linguistics, 1.

Yonatan Bisk, Daniel Marcu, and William Wong. 2016.
Towards a dataset for human computer communica-
tion via grounded language acquisition. In Proceed-
ings of the AAAI Workshop on Symbiotic Cognitive
Systems.

Devendra Singh Chaplot, Kanthashree Mysore
Sathyendra, Rama Kumar Pasumarthi, Dheeraj
Rajagopal, and Ruslan Salakhutdinov. 2018. Gated-
attention architectures for task-oriented language
grounding.

David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the Na-
tional Conference on Artiﬁcial Intelligence.

Deborah A Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the ATIS
In Proceedings of the
task: The ATIS-3 corpus.
workshop on Human Language Technology.

Abhishek Das, Samyak Datta, Georgia Gkioxari, Ste-
fan Lee, Devi Parikh, and Dhruv Batra. 2018. Em-
In Proceedings of the
bodied question answering.
IEEE Conference on Computer Vision and Pattern
Recognition.

Daniel Fried, Ronghang Hu, Volkan Cirik, Anna
Rohrbach, Jacob Andreas, Louis-Philippe Morency,
Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein,
and Trevor Darrell. 2018. Speaker-follower mod-
CoRR,
els for vision-and-language navigation.
abs/1806.02724.

Daniel Gordon, Aniruddha Kembhavi, Mohammad
Rastegari, Joseph Redmon, Dieter Fox, and Ali

Farhadi. 2018. Iqa: Visual question answering in in-
teractive environments. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition.

Charles T. Hemphill, John J. Godfrey, and George R.
Doddington. 1990. The ATIS spoken language sys-
In Proceedings of the DARPA
tems pilot corpus.
speech and natural language workshop.

Karl Moritz Hermann, Felix Hill, Simon Green, Fumin
Wang, Ryan Faulkner, Hubert Soyer, David Szepes-
vari, Wojciech Czarnecki, Max Jaderberg, Denis
Teplyashin, Marcus Wainwright, Chris Apps, Demis
Hassabis, and Phil Blunsom. 2017. Grounded lan-
guage learning in a simulated 3D world. CoRR,
abs/1706.06551.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long

short-term memory. Neural computation, 9.

Michael Janner, Karthik Narasimhan, and Regina
Barzilay. 2018.
Representation learning for
grounded spatial reasoning. Transactions of the As-
sociation for Computational Linguistics, 6.

Sahar Kazemzadeh, Vicente Ordonez, Mark Matten,
and Tamara L. Berg. 2014. Referitgame: Referring
to objects in photographs of natural scenes. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.

Joohyun Kim and Raymond Mooney. 2012. Unsuper-
vised PCFG induction for grounded language learn-
ing with highly ambiguous supervision. In Proceed-
ings of the Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. In Proceedings
of the International Conference on Learning Repre-
sentations.

Nikita Kitaev and Dan Klein. 2017. Where is misty?
interpreting spatial descriptors by modeling regions
in space. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.

Chen Kong, Dahua Lin, Mohit Bansal, Raquel Urta-
sun, and Sanja Fidler. 2014. What are you talking
about? text-to-image coreference. In Proceedings of
the IEEE Conference on Computer Vision and Pat-
tern Recognition.

Jayant Krishnamurthy and T. Kollar. 2013.

Jointly
learning to parse and perceive: Connecting natural
language to the physical world. Transactions of the
Association for Computational Linguistics, 1.

Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86.

Andrew L Maas, Awni Y Hannun, and Andrew Y Ng.
2013. Rectiﬁer nonlinearities improve neural net-
work acoustic models. In Proceedings of the inter-
national conference on machine learning.

James MacGlashan, Monica Babes-Vroman, Marie
desJardins, Michael L. Littman, Smaranda Muresan,
S Bertel Squire, Stefanie Tellex, Dilip Arumugam,
and Lei Yang. 2015. Grounding english commands
to reward functions. In Robotics: Science and Sys-
tems.

Matthew MacMahon, Brian Stankiewics, and Ben-
jamin Kuipers. 2006. Walk the talk: Connecting
language, knowledge, action in route instructions.
In Proceedings of the National Conference on Ar-
tiﬁcial Intelligence.

Junhua Mao, Jonathan Huang, Alexander Toshev, Oana
Camburu, Alan Yuille, and Kevin Murphy. 2016.
Generation and Comprehension of Unambiguous
Object Descriptions. In Proceedings of IEEE Con-
ference on Computer Vision and Pattern Recogni-
tion.

Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012a. A joint
model of language and perception for grounded at-
tribute learning. In Proceedings of the International
Conference on Machine Learning.

Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer,
and Dieter Fox. 2012b. Learning to parse natural
language commands to a robot control system.
In
Proceedings of the International Symposium on Ex-
perimental Robotics.

Hongyuan Mei, Mohit Bansal, and R. Matthew Walter.
2016. What to talk about and how? selective gener-
ation using lstms with coarse-to-ﬁne alignment. In
Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies.

Dipendra Misra, John Langford, and Yoav Artzi. 2017.
Mapping instructions and visual observations to ac-
In Proceedings
tions with reinforcement learning.
of the Conference on Empirical Methods in Natural
Language Processing.

Dipendra K. Misra, Jaeyong Sung, Kevin Lee, and
Ashutosh Saxena. 2016. Tell me dave: Context-
sensitive grounding of natural language to manip-
ulation instructions. The International Journal of
Robotics Research, 35.

Kumar Dipendra Misra, Kejia Tao, Percy Liang, and
Ashutosh Saxena. 2015. Environment-driven lexi-
In Pro-
con induction for high-level instructions.
ceedings of the Annual Meeting of the Association
for Computational Linguistics and the International
Joint Conference on Natural Language Processing.

deep reinforcement learning. In Advances in Neural
Information Processing Systems.

Vinod Nair and Geoffrey E Hinton. 2010. Rectiﬁed lin-
ear units improve restricted boltzmann machines. In
Proceedings of the international conference on ma-
chine learning.

Andrew Y. Ng, Daishi Harada, and Stuart J. Russell.
1999. Policy invariance under reward transforma-
tions: Theory and application to reward shaping. In
Proceedings of the International Conference on Ma-
chine Learning.

Junhyuk Oh, Satinder P. Singh, Honglak Lee, and
Pushmeet Kohli. 2017. Zero-shot task generaliza-
tion with multi-task deep reinforcement learning. In
Proceedings of the international conference on ma-
chine learning.

Jan Peters and Stefan Schaal. 2008. Reinforcement
learning of motor skills with policy gradients. Neu-
ral networks, 21.

Benjamin Recht, Christopher Re, Stephen Wright, and
Feng Niu. 2011. Hogwild: A lock-free approach
to parallelizing stochastic gradient descent. In Ad-
vances in Neural Information Processing Systems.

Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
2015. U-net: Convolutional networks for biomed-
In International Confer-
ical image segmentation.
ence on Medical image computing and computer-
assisted intervention.

John Schulman, Philipp Moritz, Sergey Levine,
Michael I. Jordan, and Pieter Abbeel. 2015. High-
dimensional continuous control using generalized
advantage estimation. CoRR, abs/1506.02438.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overﬁtting. The Journal of Machine Learning
Research, 15.

Alane Suhr and Yoav Artzi. 2018. Situated mapping
of sequential instructions to actions with single-step
In Proceedings of the Annual
reward observation.
Meeting of the Association for Computational Lin-
guistics.

Richard S. Sutton, Doina Precup, and Satinder P. Singh.
Intra-option learning about temporally ab-
In Proceedings of the international

1998.
stract actions.
conference on machine learning.

Dmitry Ulyanov, Andrea Vedaldi, and Victor S.
Instance normalization: The
CoRR,

Lempitsky. 2016.
missing ingredient for fast stylization.
abs/1607.08022.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Alex Graves, Ioannis Antonoglou, Daan Wierstra,
and Martin A. Riedmiller. 2013. Playing atari with

Ronald J. Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine Learning, 8.

Wenhan Xiong, Xiaoxiao Guo, Mo Yu, Shiyu Chang,
Bowen Zhou, and William Yang Wang. 2018.
Scheduled policy optimization for natural language
communication with intelligent agents. In Proceed-
ings of the International Joint Conferences on Arti-
ﬁcial Intelligence.

Claudia Yan, Dipendra Kumar Misra, Andrew Ben-
nett, Aaron Walsman, Yonatan Bisk, and Yoav Artzi.
2018. Chalet: Cornell house agent learning environ-
ment. CoRR, abs/1801.07357.

Licheng Yu, Patrick Poirson, Shan Yang, Alexander C.
Berg, and Tamara L. Berg. 2016. Modeling context
in referring expressions. In Proceedings of the Eu-
ropean Conference on Computer Vision.

A Tasks and Data: Comparisons

Table 7 provides summary statistics comparing
LANI and CHAI to existing related resources.

B Reward Function

LANI Following Misra et al. (2017), we use a
shaped reward function that rewards the agent for
moving towards the goal location. The reward for
exampl i is:

R(i)(s, a, s(cid:48)) = R(i)

p + φ(i)(s) − φ(i)(s(cid:48))

(1)

where s(cid:48) is the origin state, a is the action, s is
(i)
is the problem reward, and
the target state, R
p
φ(i)(s)−φ(i) is a shaping term. We use a potential-
based shaping (Ng et al., 1999) that encourages the
agent to both move and turn towards the goal. The
potential function is:
φ(i)(s) =

δTURNDIST(s, s(i)
g )
+(1 − δ)MOVEDIST(s, s(i)

g ) ,

where MOVEDIST is the euclidean distance to the
goal normalized by the agent’s forward movement
distance, and TURNDIST is the angle the agent
needs to turn to face the goal normalized by the
agent’s turn angle. We use δ as a gating term,
which is 0 when the agent is near the goal and
increases monotonically towards 1 the further the
agent is from the goal. This decreases the sensi-
tivity of the potential function to the TURNDIST
(i)
term close to the goal. The problem reward R
p
provides a negative reward of up to -1 on collision
with any object or boundary (based on the angle
and magnitude of collision), a negative reward of
-0.005 on every action to discourage long trajec-
tories, a negative reward of -1 on an unsuccess-
ful stop, when the distance to the goal location is
greater than 5, and a positive reward of +1 on a
successful stop.
CHAI We use a similar potential based reward
function as LANI. Instead of rewarding the agent
to move towards the ﬁnal goal the model is re-
warded for moving towards the next intermedi-
ate goal. We heuristically generate intermediate
goals from the human demonstration by generat-
ing goals for objects to be interacted with, doors
that the agent should enter, and the ﬁnal position
of the agent. The potential function is:

φ(i)(s) =

TURNDIST(s, s(i)
MOVEDIST(s, s(i)

g,j) +
g,j) + INTDIST(s, s(i)

g,j) ,

(i)
g,j

where s
intermediate goal,
TURNDIST rewards the agent for turning to-

the next

is

wards the goal, MOVEDIST rewards the agent for
moving closer to the goal, and INTDIST rewards
the agent for accomplishing the interaction in the
intermediate goal. The goal is updated on being
accomplished. Besides the potential term, we
(i)
use a problem reward R
that gives a reward of
p
1 for stopping near a goal, -1 for colliding with
obstacles, and -0.002 as a verbosity penalty for
each step.

C Baseline Details

MISRA17 We use the model of Misra et al.
(2017). The model uses a convolution neural net-
work for encoding the visual observations, a re-
current neural network with LSTM units to en-
code the instruction, and a feed-forward network
to generate actions using these encodings. The
model is trained using policy gradient in a con-
textual bandit setting. We use the code provided
by the authors.
CHAPLOT18 We use the gated attention archi-
tecture of Chaplot et al. (2018). The model is
trained using policy gradient with generalized ad-
vantage estimation (Schulman et al., 2015). We
use the code provided by the authors.
Our Approach with Joint Training We train
the full model with policy gradient. We maxi-
mize the expected reward objective with entropy
regularization. Given a sampled goal location
lg ∼ p(. | ¯x, IP ) and a sampled action a ∼ p(. |
lg, (I1, p1), . . . , (It, pt)), the update is:
∇J ≈ {∇ log P (lg | ¯x, IP ) +

∇ log P (at | lg, (I1, p1), . . . , (It, pt))} R(st, a)
λ∇H(π(. | ˜st) .

We perform joint training with randomly initial-
ized goal prediction and action generation models.

D Hyperparameters

For LANI experiments, we use 5% of the training
data for tuning the hyperparameters and train on
the remaining. For CHAI, we use the development
set for tuning the hyperparameters. We train our
models for 20 epochs and ﬁnd the optimal stop-
ping epoch using the tuning set. We use 32 dimen-
sional embeddings for words and time. LSTMx
and LSTMA are single layer LSTMs with 256
hidden units. The ﬁrst layer of CNN0 contains
128 8×8 kernels with a stride of 4 and padding 3,
and the second layer contains 64 3×3 kernels with
a stride of 1 and padding 1. The convolution lay-
ers in LINGUNET use 32 5×5 kernels with stride

Dataset

Vocabulary Mean Instruction

Num
Instructions

Num.
Actions

Avg Trajectory
Length

Partially
Observed

Bisk et al.
(2016)
MacMahon
et al. (2006)
Matuszek et al.
(2012b)
Misra et al.
(2015)
LANI
CHAI

16,767

3,237

217

469

28,204
13,729

Size

1,426

563

39

775

2,292
1018

Length

15.27

7.96

6.65

48.7

12.07
10.14

81

3

3

>100

4
1028

15.4

3.12

N/A

21.5

24.6
54.5

No

Yes

No

No

Yes
Yes

Table 7: Comparison of LANI and CHAI to several existing natural language instructions corpora.

Category
Spatial relations
Location conjunction
Temporal coordination
Co-reference

Present Absent

2.56
3.85
1.70
1.98

1.77
1.93
2.14
1.98

p-value
.023
.226
.164
.993

Table 8: Mean goal prediction error for CHAI instruc-
tions with and without the analysis categories we used
in Table 2. The p-values are from two-sided t-tests
comparing the means in each row.

2. All deconvolutions except the ﬁnal one, also use
32 5×5 kernels with stride 2. The dropout proba-
bility in LINGUNET is 0.5. The size of attention
mask is 32×32 + 1. For both LANI and CHAI, we
use a camera angle of 60◦ and create panoramas
using 6 separate RGB images. Each image is of
size 128×128. We use a learning rate of 0.00025
and entropy coefﬁcient λ of 0.05.

E CHAI Error Analysis

Table 8 provides the same kind of error analysis
results here for the CHAI dataset as we produced
for LANI, comparing performance of the model on
samples of sentences with and without the analysis
phenomena that occurred in CHAI.

F Examples of Generated Goal

Prediction

Figure 7 shows example goal predictions from the
development sets. We found the predicted proba-
bility distributions to be reasonable even in many
cases where the agent failed to successfully com-
plete the task. We observed that often the eval-
uation metric is too strict for LANI instructions,
especially in cases of instruction ambiguity.

Success

Success

Failure

go round the ﬂowers

ﬂy between the palm tree and pond

head toward the wishing well and keep it on your right .

move back to the kitchen .

then drop the tropicana onto the coffee table .

walk the cup to the table and set the cup on the table .

Figure 7: Goal prediction probability maps Pg overlaid on the corresponding observed panoramas IP . The top
three examples show results from LANI, the bottom three from CHAI. The white arrow indicates the forward
direction that the agent is facing. The success/failure in the LANI examples indicate if the task was completed
accurately or not following the task completion (TC) metric.

