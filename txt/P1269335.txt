Trajectory-Based Off-Policy Deep Reinforcement Learning

Andreas Doerr 1 2 3 Michael Volpp 1 Marc Toussaint 3 Sebastian Trimpe 2
Christian Daniel 1

9
1
0
2
 
y
a
M
 
4
1
 
 
]

G
L
.
s
c
[
 
 
1
v
0
1
7
5
0
.
5
0
9
1
:
v
i
X
r
a

Abstract

Policy gradient methods are powerful reinforce-
ment learning algorithms and have been demon-
strated to solve many complex tasks. However,
these methods are also data-inefﬁcient, afﬂicted
with high variance gradient estimates, and fre-
quently get stuck in local optima. This work ad-
dresses these weaknesses by combining recent
improvements in the reuse of off-policy data and
exploration in parameter space with determinis-
tic behavioral policies. The resulting objective
is amenable to standard neural network optimiza-
tion strategies like stochastic gradient descent or
stochastic gradient Hamiltonian Monte Carlo. In-
corporation of previous rollouts via importance
sampling greatly improves data-efﬁciency, whilst
stochastic optimization schemes facilitate the es-
cape from local optima. We evaluate the proposed
approach on a series of continuous control bench-
mark tasks. The results show that the proposed
algorithm is able to successfully and reliably learn
solutions using fewer system interactions than
standard policy gradient methods.

1. Introduction

Policy search methods are amongst the few successful Re-
inforcement Learning (RL) (Sutton et al., 2000) methods
which are applicable to high-dimensional or continuous con-
trol problems, such as the ones typically encountered in
robotics (Peters & Schaal, 2008b; Deisenroth et al., 2013).
One particular class of policy search methods directly esti-
mates the gradient of the expected return with respect to the
parameters of a differentiable policy. These Policy Gradient
(PG) algorithms have achieved impressive results on highly
complex tasks (Schulman et al., 2015; 2017). However,

1Bosch Center for Artiﬁcial Intelligence, Renningen, Germany.
2Max Planck Institute for Intelligent Systems, Stuttgart/T¨ubingen,
Germany. 3Machine Learning and Robotics Lab, University of
Stuttgart, Germany. Correspondence to: Andreas Doerr <andreas-
doerr@gmx.net>.

Proceedings of the 36 th International Conference on Machine
Learning, Long Beach, California, PMLR 97, 2019. Copyright
2019 by the author(s).

standard algorithms are vastly data-inefﬁcient and rely on
millions of data points to achieve the aforementioned re-
sults. Typical applications are therefore limited to simulated
problems where policy rollouts can be cheaply obtained.

Algorithms based on stochastic policy gradients, like RE-
INFORCE (Williams, 1992) and G(PO)MDP (Baxter &
Bartlett, 2001), typically estimate the policy gradient based
on a batch of trajectories, which are obtained by executing
the current policy on the system (i.e. based on on-policy
samples). In the next step, all previous experience is dis-
carded and new trajectories are sampled using the updated
policy. This scheme holds true also for more recent meth-
ods, like PPO (Schulman et al., 2017) or POIS (Metelli
et al., 2018), where a surrogate objective is constructed,
which can be optimized till convergence. Typically, Impor-
tance Sampling (IS) techniques are employed to evaluate a
target policy based on rollouts obtained from behavioural
policies (i.e. from off-policy samples). Albeit these off-
policy evaluation schemes, in these algorithms, no data
is shared between iterations. Prominent examples of off-
policy ofﬂine algorithms typically employ actor-critic ar-
chitectures (Silver et al., 2014), where the parametric critic
model, typically a value function, is updated to summarize
all knowledge gathered so far. In contrast, we proposed the
model-free Deep Deterministic Off-Policy Gradient method
(DD-OPG)1, which incorporates previously gathered rollout
data by sampling from a trajectory replay buffer. This effec-
tively enables backtracking to promising solutions, whilst
requiring only minimal assumptions to construct the surro-
gate model.

Next to the inefﬁcient use of available data, stochasticity in
both the policy and the environment causes highly variable
gradient estimates and therefore slow convergence. When
executing the probabilistic policy on the system, noise is
injected into the policy gradient in each time step, leading
to a variance, which linearly increases with the length of
the horizon (Munos, 2006). Additive Gaussian noise is typi-
cally employed as source of exploration. Additionally, PG
methods built around the likelihood ratio trick intrinsically
require probabilistic policies. Only then, policies can be up-
dated to increase the likelihood of actions, which have been

1https://github.com/boschresearch/DD_OPG

Trajectory-Based Off-Policy Deep Reinforcement Learning

advantageous in previous rollouts. Instead of independent
noise, temporally-correlated noise (Osband et al., 2016), or
exploration directly in parameter space can lead to a larger
variety of behaviours (Plappert et al., 2017). Here, the be-
havioural policy is deterministic, thereby effectively reduc-
ing the gradient variance. Methods like DPG (Silver et al.,
2014) and DDPG (Lillicrap et al., 2015) learn a paramet-
ric value function model to translate changes in policy and
therefore actions to changes in expected value. Similarly,
our proposed model-free DD-OPG algorithm constructs a
non-parametric critic based on importance sampling. This
critic, called surrogate model in the following, allows for
updating a deterministic policy without the need for explicit
parametric value models.

To summarize: We propose an importance sampling based
surrogate model of the return distribution, which enables off-
policy, ofﬂine policy optimization. This surrogate facilitates
deterministic policy gradients to reduce gradient variance
and enables incorporation of all available data from a replay
buffer. Exploration in the policy parameter space is achieved
by a prioritized resampling of the surrogates support data,
thus favouring promising regions in policy space. Normal-
ized IS, which we demonstrate to act similarly as a baseline
in standard PG methods, additionally reduces the variance of
the employed estimates. Although no additional, parametric
value function baseline (as utilized in TRPO/PPO for vari-
ance reduction) is required in our method, fast progress and
therefore data-efﬁcient learning is demonstrated on typical
continuous control tasks.

The general problem formulation and policy gradient frame-
work is highlighted in Sec. 2, followed by a short presen-
tation of the standard importance sampling estimators to
incorporate off-policy data in Sec. 3. The surrogate model,
necessary to efﬁciently incorporate deterministic policy data,
as the core of the proposed model-free DD-OPG method is
detailed in Sec. 4. In Sec. 5, the main policy optimization
scheme is presented and experimentally evaluated in Sec. 6.
This work closes with a discussion of connections to related
work in Sec. 7 and concludes with an outlook into future
work and open topics in Sec. 8.

2. Preliminaries

This section depicts the general episodic RL problem in a
discrete-time Markovian environment and summarizes as
core building-block of the proposed DD-OPG method, the
standard return based policy gradient estimators (Williams,
1992). DD-OPG closely follows this algorithmic structure
(cf. Alg. 1), however with extensions to incorporate deter-
ministic, off-policy rollouts as detailed in the following sec-
tions. The RL problem is characterized by a discrete-time
, p, r, γ, p0).
Markov Decision Process (MDP)
An agent is interacting with an environment, whose states

= (

M

A

S

,

∈ S

transitions according to the agent’s actions at

st
∈ A
and the environment’s transition probabilities p(st+1
|
st, at) into a successor state. Starting from a state s0 drawn
from the initial state distribution p(s0), agent tries to maxi-
mize its discounted reward, according to a reward function
R and discount factor γ, accumulated over a
r : S
horizon length H. In policy search, the agent acts according
to a (stochastic) policy πθ = π(at
st; θ), parameterized
by θ. The expected accumulated reward is given by
(cid:90)

→

×

A

|

J(θ) =

p(τ

θ)R(τ )dτ ,

(1)

|

∈ T

where the trajectory τ
is the sequence of state-action
pairs τ = (s0, a0, . . . , sH , aH ), the (discounted) trajectory
return is given by R(τ ) = (cid:80)H−1
t=0 γtr(sτ,t, aτ,t), and due
to the Markov property, the trajectory distribution in (1) is
given by

p(τ

θ) = p(s0)

p(st+1

st, at)π(at

st; θ) .

(2)

|

|

|

H
(cid:89)

t=0

st, at) and the ini-
The dynamics of the system p(st+1
tial state distribution p(s0) are generally unknown to the
learning agent.

|

Model-free policy gradient methods typically directly esti-
mate the expected cost gradient based on the log-derivative
trick. The gradient is given by

(cid:90)

θJ(θ) =

p(τ

θ)

θ log p(τ

θ)R(τ )dτ .

(3)

∇

|

∇

|

θ), the following Monte
Given on-policy samples τi
∼
Carlo (MC) estimators are obtained for the expected return

p(τ

|

ˆJ M C(θ) =

R(τi) ,

(4)

1
N

N
(cid:88)

i=1

and the policy gradient

ˆ

θJ M C(θ) =

∇

1
N

N
(cid:88)

i=1

(cid:34) H
(cid:88)
t=0 ∇

θ log π(at

st; θ)R(τi)

. (5)

(cid:35)

Since the unknown initial state and dynamics distributions
are independent of the policy parameters θ (cf. (2)), the
θ) with respect
trajectory likelihood gradient
to the policy parameters can be computed analytically for a
given, differentiable policy

θ log π(at

θ log p(τ

st; θ).

∇

|

∇

|

|

3. Off-Policy Evaluation

p(τ

The MC estimators require a substantial amount of on-policy
θ∗) to reduce the gradient estimator’s
rollouts τi
variance and typically many more rollouts than used in state-
of-the-art implementations to closely approximate the true
gradient (Ilyas et al., 2018).

∼

|

Trajectory-Based Off-Policy Deep Reinforcement Learning

For off-policy data, Importance Sampling (IS) can be uti-
lized to incorporate trajectories from a behavioural policy
πθ(cid:48) in order to evaluate a new target policy πθ∗ (Zhao et al.,
2013; Espeholt et al., 2018; Munos et al., 2016; Metelli et al.,
2018). In general, a Monte Carlo estimate of an expectation
(cid:82) p(x)f (x)dx (such as (1)) can be obtained by sampling
q(x) and re-weighting the
from a tractable distribution xi
sampled function evaluations f (xi) based on the likelihood-
ratio p(xi)/q(xi). The expected return can be rewritten as

∼

(cid:90)

J(θ) =

p(τ

θ(cid:48))

|

p(τ
p(τ

θ)
θ(cid:48))

|
|

R(τ )dτ ,

(6)

such that the IS weighted Monte Carlo estimator is given by

datasets and at the same time helps to avoid local minima
by stochastically optimizing the objective.

Another technique typically employed for IS is weight nor-
malization (Metelli et al., 2018). The weighted importance
sampling estimator obtains a lower variance estimate at the
cost of adding bias. It has been employed in (Peshkin &
Shelton, 2002) and is both theoretically and empirically
better-behaved (Meuleau et al., 2000; Precup et al., 2000;
Shelton, 2001) compared to the pure IS estimator. The
weighted importance sampling estimator is given by

ˆJ WIS(θ) =

w(τi, θ)R(τi) ,

(11)

1
Z

N
(cid:88)

i=0

ˆJ IS(θ) =

p(τi
p(τi

θ)
θ(cid:48))

|
|

R(τi)

=

w(τi, θ)R(τi) ,

1
N

1
N

N
(cid:88)

i=0

N
(cid:88)

i=0

where N trajectories are sampled from a policy πθ(cid:48) to infer
the expected cost of policy πθ. Although system dynam-
ics and initial state distribution in (2) are unknown, the
likelihood-ratio, i.e. the importance weights, can be com-
puted since the unknown parts cancel out, such that

(7)

(8)

where importance weights w(τi, θ) might be computed ac-
cording to (9) or (10) and a normalizing constant Z =
(cid:80)N
i=0 w(τi, θ) instead of the standard normalization Z =

N , previously used in (8).

From the policy gradient perspective, by normalizing the
importance weights, we obtain a gradient estimator, which
includes a parameter dependent baseline.

Proposition 1 The policy gradient estimator obtained from
the self-normalized importance sampling expected cost esti-
mator ˆJ W IS is given by

w(τ, θ) =

p(τ
p(τ

θ)
θ(cid:48))

|
|

=

(cid:81)H

t=0 π(at
t=0 π(at

(cid:81)H

st; θ)
st; θ(cid:48))

.

|

|

(9)

θ ˆJ WIS(θ) =

∇

1
Z

N
(cid:88)

(cid:20)
w(τi, θ)

i=1

D

=

During learning, trajectories are collected from multiple
N
(τi, θi)
different policies
i=1. To incorporate all
}
{
data, the importance sampling distribution can be replaced
by an empirical mixture distribution q(τ
θ1, . . . , θN ) =
1/N (cid:80)
θi) such that the available trajectories are i.i.d.
draws from the empirical mixture distribution τi
|
θ1, . . . , θN ) (Jie & Abbeel, 2010). The resulting importance
weights are given by

i p(τ

q(τ

∼

|

|

w(τ, θ) =

(cid:81)H

st; θ)

t=0 π(at
(cid:81)H

|

t=0 π(at

st; θj)

.

1
N

(cid:80)
j

|

(10)

Computing the importance weights in (10), however, scales
quadratically with the number of available trajectories due to
the summation over the likelihoods of all trajectories given
all available policies. Scaling this estimator to today’s deep
neural network policies with a large number of required
rollouts is, thus, a major challenge. Instead of computing
the surrogate based on all data, as in (Jie & Abbeel, 2010),
which is only feasible for several hundred rollouts, the pro-
posed DD-OPG method employs a trajectory replay buffer
and a probabilistic selection scheme to recompute a stochas-
tic approximation of the full surrogate model. This idea is
related to prioritized experience replay (Schaul et al., 2015)
but for full trajectories. It enables scaling to much larger

H
(cid:88)

(cid:104)

∇

t=0

θ log π(a(i)
t

s(i)
t

; θ)

R(τi)

ˆJ WIS(θ)

(cid:105)(cid:104)

|

−

(12)

(cid:105) (cid:21)

.

A proof of this proposition is shown in Appendix A. This
estimator is closely related to standard PG estimators with
an added baseline term for variance reduction.

In standard, REINFORCE like, PG methods, two of the
most common variance reduction techniques (Greensmith
et al., 2004) are: i) incorporation of the reward-to-go for
each policy action update instead of the entire Monte Carlo
path return; and ii) subtraction of a state dependent base-
line term, such as to obtain an estimate of the advantage of
the previously taken action. The intuition behind method
i) is to reward actions only for rewards obtained after the
action took effect, but not for those obtained earlier on.
However, to compute the importance weights not for the
full trajectory distribution but for each state-action pair in-
(N 2H 2)
dividually, the computation of a matrix of size
would be required. Therefore, the model-free, importance
sampling based approaches are typically limited to the path
return based estimators. Model-based methods (i.e. a para-
metric models for the value function) are employed in the
cost-to-go estimators. Variance reduction method ii) is au-
tomatically obtained by the normalized estimator as shown

O

Trajectory-Based Off-Policy Deep Reinforcement Learning

in proposition 1, however, in contrast to the bias free value
function control variates, at the cost of adding bias. Addi-
tional, optimal baselines to further decrease the variance of
the gradient estimator have been derived in (Jie & Abbeel,
2010) and could be incorporated into DD-OPG.

4. Deterministic Policy Gradients

The policy gradient estimators in (5) and (12) rely on a
st; θ) in order to obtain a gradient
policy distribution π(at
|
signal on how to update the policy parameters to increase
the likelihood of successful actions. In this situation the,
typically Gaussian, additive policy noise acts in two ways,
causing exploration and serving as the basis for the estima-
tion of the objective function.
Exploration is being driven directly through noise in the
action space, i.e., the policy covariance. While driving ex-
ploration through noisy actions will converge in the limit,
the resulting explorative behaviour exhibits no temporal cor-
relations, which can make it inefﬁcient.
Estimation of the objective function is typically achieved by
reweighting the action distribution according to the policy’s
st; θ) =
likelihood. Standard policies are given as π(at
µθ(st), Σθ), where µθ is represented by some func-
N
|
tion approximator parameterized by θ, e.g. a neural network.
The additive Gaussian noise covariance is typically a diag-
onal matrix, parameterized by θ as well. The proposed
deterministic policy gradient method strives to separate the
exploration and estimation part.

(at

|

Parameter Space Exploration By utilizing deterministic roll-
out policies, the only noise introduced into the gradient es-
timate originates from the stochasticity of the environment
and we have to perform exploration in parameter space in-
stead of action space exploration. However, as stated above,
parameter based exploration may in many cases be more
efﬁcient than exploration in action space, since parameter
based exploration will lead to temporally correlated actions
which can explore the state space faster. Typically, however,
this effect is negated for neural network policies since the
parameter space that has to be explored is prohibitively large.
Thus, to navigate large parameter spaces efﬁciently, some
approximate evaluation of the cost function (1) is needed.

Trajectory based objective estimate Whilst evaluation of
the Monte Carlo based expected cost estimate is possible
also for deterministic policies, the off-policy evaluation is
θ(cid:48))
no longer feasible since the likelihood ratio p(τ
(cf. (9)) becomes zero for two distinct dirac policy action
distributions if µθ(s)

θ)/p(τ
|

= µθ(cid:48)(s).

|

However, we can still compare trajectories under a stochas-
tic evaluation distribution, similar to a kernel function where
the standard deviation of the evaluation function relates to a
kernel lengthscale in action space.

Thus, we introduce the evaluation policy

˜p(at

st; θ) =
|

(at

µθ(st), Σ) ,

(13)

|

N
where Σ = diag(σ1, . . . , σDu ) is a diagonal covariance
matrix as typically employed in deep RL methods with
Gaussian action noise. The deterministic policy is given
by p(at
st; θ) = δ(a = µθ(st)), where δ is the dirac delta.
|
From the general IS expectation in (6) and our evaluation
policy in (13), the surrogate model follows as

ˆJ surr(θ) =

˜w(τi, θ)R(τi) ,

(14)

1
Z

N
(cid:88)

i=1

with surrogate weights

˜w(τi, θ) =

(cid:81)H

t=0 N
(cid:81)H

(cid:80)N

(a(i)
t

µθ(s(i)
|
(a(i)
t

t ), Σ)
µθj (s(i)

t ), Σ)

1
N

,

(15)

|

j=0

t=0 N
where, depending on the choice of normalization con-
stant Z, we obtain the analogue to the standard IS esti-
mator (Z = N ) or the analog to the weighted IS estimator
(Z = (cid:80)N
i=1 ˜w(τi, θ)). Reintroducing the ﬁxed Gaussian
noise as an implicit loss to obtain gradients for the evalua-
tion of deterministic policies is clearly a model assumption
in the proposed method but can be justiﬁed from several
perspectives.

The hyper-parameter Σ allows for control over the amount
of information shared between neighbouring policies. Sim-
ilar to the cap of importance weights in PPO (Schulman
et al., 2017), this parameter allows to control bias and vari-
ance of the surrogate model. Analyzing the introduced bias
and relation to the PPO weight cap is however ongoing re-
search. In the limit of Σ
0, the proposed surrogate (14)
approaches the MC estimator (4). Only in case of two differ-
= θi, but equivalent actions
ent policy parameterizations θj
µθj (s) = µθi (s) for the sampled states s, the surrogate
model would output an average whereas the MC estimator
would not mix up the obtained returns. For Σ = Σθ, the
surrogate model recovers the true IS estimate, given that
all trajectories are generated using the same additive Gaus-
sian noise. Finally, for Σ
inf, the estimate is simply the
average over all available path returns.

→

→

Modelling the expected return distribution by choosing a
lengthscale in action space can furthermore be motivated
from a second perspective. Typical expected return distri-
butions oftentimes comprise sharp transitions between sta-
ble and unstable regions, where policy parameters change
only slightly but reward changes drastically. One global
lengthscale is therefore typically not well suited to directly
model the expected return. This is a standard problem in
Bayesian Optimization for reinforcement learning, where
typical smooth kernel functions (e.g. squared exponential
kernel) with globally ﬁxed lengthscales are unable to model

Trajectory-Based Off-Policy Deep Reinforcement Learning

both stable and unstable regimes at the same time. How-
ever, in the proposed model, a lengthscale in action space
is translated via the sampled state distribution and policy
function µ into implicit assumptions in the actual policy
parameter space. Doing so, instead of operating on arbitrary
euclidean distances in policy parameter space, a more mean-
ingful distance in trajectory and action space is available.
Typically, for a given system, distance of trajectories and
between actions is more graspable, compared to arbitrary
deep neural network policy parameters.

The expected return estimator (14) falls back to zero for
policy evaluation far away from training data. To estimate
the variance of the importance sampling estimator itself, typ-
ically, the Effective Sample Size (ESS) is evaluated. Based
on the variance of the importance weights, it analyses the
effective number of available data points at a speciﬁc policy
evaluation position. In (Metelli et al., 2018), a lower bound
on the expected return has been proposed such that with
probability 1

δ it holds that

Eτ ∼p(τ |θ)[R(τ )]

˜w(τi, θ)R(τi)

−

1
N

≥

N
(cid:88)

i=1

(cid:114)

(1

R

∞
(cid:107)

− (cid:107)

−

δ)d2(p(τ
|
δN

θ)

p(τ
(cid:107)

θ(cid:48)))
|

,

where d2 is the exponentiated 2-R´enyi divergence. Due to
the identity ESS(P
Q), this lower bound
Q) = N/d2(P
can be estimated in a sample-based way by employing the
ESS estimator

||

||

(16)

(17)

ˆESS =

1
i=1 ˜w(τi, θ)2

(cid:80)N

,

such as to obtain the lower bound estimate

Eτ ∼p(τ |θ)[R(τ )]

˜w(τi, θ)R(τi)

(18)

1
N

≥

N
(cid:88)

i=1

(cid:114)

1

δ

−
δ

R

∞
(cid:107)

− (cid:107)

ESS(θ)−1 .

(19)

Refer to theorem 4.1 in (Metelli et al., 2018) for details and
proof regarding the lower bound in (16). The conﬁdence
parameter δ determines, similar to the KL-divergence in
TRPO (Schulman et al., 2015), how far the policy optimiza-
tion can step away from known regions. In DD-OPG, this
uncertainty estimate is employed as penalty

penalty(θ) =

R

∞γ
(cid:107)

−(cid:107)

(cid:113)

ˆESS(θ)−1 ,

(20)

with penalty factor γ as an hyper-parameter to control ex-
ploration, i.e.
risk
awareness, i.e. staying within a trust region.

following the objective estimate vs.

5. Model-Free Off-Policy Optimization

The surrogate model of the return distribution, as derived
in Sec. 4, can now be directly incorporated for policy opti-
mization. In related work, parametric search distributions
(e.g. Gaussian) are employed as policy search distribution
or hyperpolicy (Zhao et al., 2013; Plappert et al., 2017;
Metelli et al., 2018). However, in high-dimensional spaces,
as typically obtained with deep network policy representa-
tions, updating the full search distribution is challenging and
common approaches usually revert to heuristics to control a
simpliﬁed, e.g. diagonal or block-wise search distribution’s
covariance matrix.

Instead, the proposed model-free DD-OPG method fully
optimizes a stochastic version of the surrogate objective to
foster exploration and overcome local minima. At the same
time, the stochastic evaluation mitigates the unfavourable
complexity of computing the full importance sampling es-
timate based on all available data. Due to the empirical
mixture distribution in (10), computing the likelihood of all
observed trajectories under all policies is quadratic in the
number of observed paths. Instead, the proposed method
employs a selection criterion to construct a stochastic sur-
rogate model based on a subset of rollouts in each policy
optimization step. In particular, a predeﬁned number of
Nmax rollout indices is drawn from the softmax distribution
over the discrete set of available trajctory indices
. The
softmax is computed based on the normalized, empirical
returns ˜R and a temperature factor λ.

I

p(

τ1, . . . , τN ) =

I|

exp( ˜R(τI)/λ)
j=1 exp( ˜R(τj)/λ)

.

(cid:80)N

(21)

The temperature λ is used to trade off exploration against
exploitation in the selection of reference trajectories. This
scheme is closely related to prioritized experience replay
(Schaul et al., 2015). A study of the effect of temperature
selection on the learning progress is shown in Sec. 6.3.

The full DD-OPG algorithm is detailed in Alg. 1. The main
objective is to incorporate all available deterministic policy
rollouts, not only the ones from the current iteration, into the
surrogate model by means of the softmax replay selection.
The lower bound expected return can then be fully optimized
using standard optimization techniques. In practice Adam
(Kingma & Ba, 2014) is employed, but other techniques,
e.g. based on the natural policy gradient (Peters & Schaal,
2008a) could be incorporated as well.

6. Experimental Evaluation

The experimental evaluation of the proposed DD-OPG
method is threefold. In Sec. 6.1, the resulting surrogate
return model is visualized, highlighting different modeling
options. A benchmark against state-of-the-art PG methods

Trajectory-Based Off-Policy Deep Reinforcement Learning

(a) log(Σ) = 0 · I

(b) log(Σ) = −1 · I

(c) log(Σ) = −2 · I

Figure 1. Visualization of the surrogate return model. A cross-section along a random direction in parameter space is shown for parameters
close to optimum. The ground truth mean and std (blue) of the return distribution is shown together with the mean and std estimate
(orange) from the weighted importance sampling surrogate model. The lower conﬁdence bound (δ = 0.2, dashed orange line) is shown
together with the model’s input data (grey dots). Notice how more or less information is shared between points where data is available
depending on the chosen lengthscale parameter Σ.

Algorithm 1 Model-free DD-OPG

Input: Initial policy parameters θ0
Empty trajectory replay buffer
repeat

D

0 =

{}

p(τ

θi)
Sample trajectory: τi
∼
|
i+1 =
Update trajectory buffer:
i
D
iid
Memory selection: i1, . . . , iNmax
∼
Surrogate model: ˜J(θ), penalty(θ)
Lower bound optimization:
θi+1 = argmax

penalty(θ)

˜J(θ)

D

θ

−

until converged or maximum iterations

(τi, Ri)
τ1, . . . , τi)

∪
p(

I|

is shown in Sec. 6.2 to highlight fast and data-efﬁcient learn-
ing. Finally, important parts of the proposed algorithms
and their effects on the ﬁnal learning performance are high-
lighted in an ablation study in Sec. 6.3.

6.1. Surrogate Model

As discussed in Sec. 4, the proposed surrogate model can
smoothly interpolate between the Monte Carlo estimate,
the importance sampling estimate, and an average of all
available returns. In Fig. 1, the available surrogate model
predictions are visualized for multiple settings of the model
hyper-parameter Σ. In particular, the estimate for expected
return (solid orange line), return variance (shaded orange
visualizes one standard deviation), and the lower bound of
the expected return (dashed orange line) are visualized for
policy evaluations along a random direction around the op-
timal policy θ∗ for the cartpole environment (experimental
details can be found in Appendix B). Trajectory data, which
is available to the estimator is highlighted by grey dots. The
groundtruth return distribution (mean +/- one std. in blue) is
computed using the standard MC estimator, based on inde-
pendent policy rollouts, which are not part of the surrogate
model.

Stepping from long lengthscales (cf. Fig. 1a) to shorter
lengthscales (cf. Fig. 1c), the surrogate model predictions
become more local. Most visibly in the lowerbound esti-
mate, the ESS drops signiﬁcantly when moving away from
data points and small model lengthscales, resulting in much
higher uncertainty.

6.2. Policy Gradient Benchmark

The proposed DD-OPG method is evaluated in terms of data-
efﬁciency and learning progress in comparison to state-of-
the-art policy gradient methods based on Monte Carlo return
estimates. In contrast, methods such as DDPG (Lillicrap
et al., 2015) employ TD learning for their value function
model and are not part of this evaluation. The benchmark
compares DD-OPG to the standard REINFORCE (Williams,
1992) baseline and both TRPO (Schulman et al., 2015) and
PPO (Schulman et al., 2017). All competitor algorithms em-
ploy, as it is common practice, the reward-to-go formulation
and a linear feature-based baseline for variance reduction.
For all methods, hyper-parameters are selected to achieve
maximal accumulated average return, i.e. fast and stable
policy optimization. Details about the individual methods’
conﬁguration and the employed environments can be found
in Appendix B.

The resulting learning performances are visualized in Fig. 2
for the cartpole, mountaincar and swimmer environment
(left to right) (Duan et al., 2016). For REINFORCE (blue),
TRPO (yellow), PPO (green), and DD-OPG (red), the mean
average return (solid line) and its conﬁdence intervals (one
standard deviation as shaded area) are depicted, as obtained
from 10 independent runs out of 10 random seeds for each
environment and method. To compare the learning speed
and data-efﬁciency between the batch-wise learning com-
petitors and the rollout-based DD-OPG, the results are vi-
sualized as a function of collected environment interactions
(scaled by 105) in Fig. 2.

Trajectory-Based Off-Policy Deep Reinforcement Learning

(a) Cartpole, [-] ×105 steps

(b) Mountaincar, [-] ×105 steps

(c) Swimmer, [-] ×105 steps

Figure 2. Policy gradient methods benchmark. The proposed method DD-OPG (red) is compared to standard REINFORCE (blue), TRPO
(orange) and PPO (green) on three continuous control benchmark problems. Mean and standard deviation of the average return (obtained
from 10 independent random seeds) are plotted as a function of the system interaction steps (scaled by 105). Signiﬁcant faster learning
speed in the beginning is observed for the model-free off-policy method in comparison to the on-policy PG methods.

With DD-OPG, rapid learning progress is achieved already
and the ﬁnal performance of the competitive, state-of-the-art
policy gradient methods is matched. In the hyper-parameter
tuning phase, experiments with TRPO and PPO have been
conducted based on smaller batchsizes, but due to the lack
of data-efﬁcient incorporation of off-policy data, no faster
and stable learning progress could be achieved for these
methods, compared to the one visualized in Fig. 2. Notice
the large variance of the DD-OPG learning progress in the
swimmer environment. Albeit the superior learning perfor-
mance of DD-OPG on the swimmer environment, some of
the runs got stuck in local minima, resulting in the large
variance estimate. This trade-off between exploration and
exploitation is partially achieved by the stochastic memory
selection. A mix of prioritized trajectory replay and current
trajectories is mandatory to prevent greedy exploitation of
previously seen, local minima and to facilitate exploration.
Our experiments show that it is mandatory to incorporate
previously seen rollout data, as it is done in DD-OPG, to
enable rapid progress already in the early stages of training.

6.3. Ablation Study

In the ﬁnal DD-OPG algorithm, multiple aspects come to-
gether: i) the deterministic surrogate model, ii) the memory
selection strategy, and iii) the optimization scheme. In this
ablation study, we separate the individual components to
analyse their effect on the ﬁnal learning performance. Ex-
periments are conducted on the cartpole environment and
results are averaged over three random seeds.

In the ﬁrst experiment, DD-OPG is reconstructed starting
from the REINFORCE baseline. A visualization is shown
in Fig. 3. In REINFORCE (red dotted line), only one policy
gradient step is taken based on the current on-policy data.
This is comparable to DD-OPG with almost no memory
(Nmax = 5) and only one step gradient update (visualized
as blue dotted line). Learning performance is already in-
creased by adding more memory paths (green: Nmax = 20,
yellow: Nmax = 50). More signiﬁcantly, the full opti-

Figure 3. Ablation study of DD-OPG. The full DD-OPG model is
constructed from the REINFORCE baseline by iteratively adding i)
deterministic off-policy data incorporation and ii) full optimization
of the surrogate model. Visualized is the mean learning progress
from 3 random seeds on the cartpole environment. REINFORCE
(dashed red line) is shown with DD-OPG optimizing only for
one gradient step (dotted lines) and fully optimizing the surrogate
model (solid lines). For DD-OPG, three levels of history are shown
(blue: Nmax = 5, green: Nmax = 20, yellow: Nmax = 50).

mization of the surrogate model (solid lines) achieves much
faster learning progress.

In Fig. 4, the effect of the surrogate model’s lengthscale pa-
rameter Σ is evaluated. Four different lengthscales log Σ are
evaluated (red: 1.0, green: 2.0, yellow: 3.0, blue: 4.0). In
this experiment, longer lengthscales clearly improve learn-
ing speed despite the introduced model bias.

The effects of the softmax temperature λ on the proposed
prioritized trajectory replay and the learning progress are
depicted in Fig. 5. Explorative behaviour is favoured for
higher temperatures (red), whereas for low temperatures
(blue), previous trajectories are selected more greedily. In
this example, an intermediate temperature achieves the best
trade-off exploration-exploitation trade-off.

Trajectory-Based Off-Policy Deep Reinforcement Learning

Figure 4. Effect of the surrogate hyper-parameter Σ on the learn-
ing progress. Learning speed increases from short lengthscales
log Σ = 1.0 (red) to log Σ = 2.0 (green), log Σ = 3.0 (yellow),
and log Σ = 4.0 (blue). Visualized are DD-OPG mean learning
curves from three random seeds as a function of the number of
interaction steps with the cartpole environment (scaled by 103).

7. Connections to Related Work

Policy search methods (Peters & Schaal, 2008b; Deisen-
roth et al., 2013) and policy gradient methods (Williams,
1992; Baxter & Bartlett, 2001) are well studied in the RL
community and many connections to DD-OPG exist.

Importance sampling has been employed to either reweight
full trajectory distributions (Shelton, 2001; Jie & Abbeel,
2010; Zhao et al., 2013; Metelli et al., 2018) or to reweight
individual state-action pairs (Munos et al., 2016; Espeholt
et al., 2018). Except for (Jie & Abbeel, 2010), no global
IS estimator is derived, but estimates are only based on the
current iteration’s data. In contrast, DD-OPG introduces
global surrogate model based on all available deterministic
policy rollouts and computes local, stochastic approxima-
tions using prioritized replay. Instead of DD-OPG’s action
space lengthscale, alternative appraoches consider trunca-
tion of the importance weights (Wawrzynski & Pacut, 2007;
Schulman et al., 2017; Espeholt et al., 2018). So far, the con-
nection between both approaches has not yet been subject
of greater analysis.

Concepts for policy updates range from standard gradient
ascent (Williams, 1992), to trust region methods (Schulman
et al., 2015) to lower bounds, which can be fully optimized
till convergence (Schulman et al., 2017; Metelli et al., 2018).
The proposed DD-OPG optimizes a stochastic version based
on the lower bound, derived in (Metelli et al., 2018).

Deterministic policies as means of variance reduction have
been previously discussed for example in (Sehnke et al.,
2008; Plappert et al., 2017). Instead of action noise for
exploration, exploration is achieved by stochasticity in pa-
rameter space The DD-OPG method relies on deterministic

Figure 5. Learning progress for multiple temperature settings for
softmax trajectory selection. From lowest temperature (λ = 0.01,
blue) to highest temperature (λ = 2.0, red). Both too high and too
low temperatures lead to suboptimal behaviour, either by too much
exploration or too greedy behaviour.

policies for variance reduction, but introduces exploration
by means of stochastic gradients from the prioritized replay
model.

8. Discussion

This work presents a new surrogate model of the RL return
distribution inspired by importance sampling. It can incor-
porate off-policy data and deterministic rollouts to reduce
estimator variance. Despite the promising results and the
data-efﬁcient learning progress, several interesting topics
remain for future work.

The proposed surrogate model is motivated by its close
connections to the importance sampling estimator, the inter-
pretability of the model assumption in action space and its
desirable behaviour in the model limits. A detailed analysis
of the resulting model assumptions in policy space, implied
by the model assumptions in action space and an analysis
of the resulting bias remains an open question.

The proposed optimization scheme empirically achieved
good performance in our benchmark experiments, outper-
forming state-of-the-art methods, although no additional
parametric value function baseline (as in TRPO/PPO) is
employed. However, extensions to other strategies for ex-
ploration vs. exploitation, for example acquisition functions
like Expected Improvement or Probability of Improvement
from Bayesian Optimization (Snoek et al., 2012), are to be
explored and directly carry over to the proposed surrogate
return model.

Finally, memory selection is required to scale the non-
parametric model structure to typical deep RL applications.
The proposed prioritized trajectory replay is only one possi-
ble option to address this challenge.

Trajectory-Based Off-Policy Deep Reinforcement Learning

References

Baxter, J. and Bartlett, P. L. Inﬁnite-horizon policy-gradient
estimation. Journal of Artiﬁcial Intelligence Research,
15:319–350, 2001.

Deisenroth, M. P., Neumann, G., Peters, J., et al. A survey
on policy search for robotics. Foundations and Trends R
(cid:13)
in Robotics, 2(1–2):1–142, 2013.

Duan, Y., Chen, X., Houthooft, R., Schulman, J., and
Abbeel, P. Benchmarking deep reinforcement learning
for continuous control. In International Conference on
Machine Learning (ICML), pp. 1329–1338, 2016.

Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih,
V., Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning,
I., et al. Impala: Scalable distributed deep-rl with im-
portance weighted actor-learner architectures. In Inter-
national Conference on Machine Learning (ICML), pp.
1406–1415, 2018.

Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare,
M. Safe and efﬁcient off-policy reinforcement learning.
In Advances in neural information processing systems
(NIPS), pp. 1054–1062, 2016.

Osband, I., Blundell, C., Pritzel, A., and Van Roy, B. Deep
exploration via bootstrapped DQN. In Advances in neural
information processing systems (NIPS), pp. 4026–4034,
2016.

Peshkin, L. and Shelton, C. R. Learning from scarce experi-
ence. In International Conference on Machine Learning
(ICML), pp. 498–505. Morgan Kaufmann Publishers Inc.,
2002.

Peters, J. and Schaal, S. Natural actor-critic. Neurocomput-

ing, 71(7-9):1180–1190, 2008a.

Peters, J. and Schaal, S. Reinforcement learning of motor
skills with policy gradients. Neural networks, 21(4):682–
697, 2008b.

Greensmith, E., Bartlett, P. L., and Baxter, J. Variance reduc-
tion techniques for gradient estimates in reinforcement
learning. Journal of Machine Learning Research, 5(Nov):
1471–1530, 2004.

Plappert, M., Houthooft, R., Dhariwal, P., Sidor, S., Chen,
R. Y., Chen, X., Asfour, T., Abbeel, P., and Andrychow-
icz, M. Parameter space noise for exploration. arXiv
preprint arXiv:1706.01905, 2017.

Ilyas, A., Engstrom, L., Santurkar, S., Tsipras, D., Janoos,
F., Rudolph, L., and Madry, A. Are deep policy gradi-
ent algorithms truly policy gradient algorithms? arXiv
preprint arXiv:1811.02553, 2018.

Precup, D., Sutton, R. S., and Singh, S. P. Eligibility traces
for off-policy policy evaluation. In International Confer-
ence on Machine Learning (ICML), pp. 759–766. Cite-
seer, 2000.

Jie, T. and Abbeel, P. On a connection between importance
sampling and the likelihood ratio policy gradient. In Ad-
vances in Neural Information Processing Systems (NIPS),
pp. 1000–1008, 2010.

Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.

Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,
T., Tassa, Y., Silver, D., and Wierstra, D. Continuous
control with deep reinforcement learning. arXiv preprint
arXiv:1509.02971, 2015.

Metelli, A. M., Papini, M., Faccio, F., and Restelli, M. Pol-
icy optimization via importance sampling. In Advances in
neural information processing systems (NIPS), pp. 5442–
5454, 2018.

Meuleau, N., Peshkin, L., Kaelbling, L. P., and Kim, K.-
E. Off-policy policy search. MIT Articical Intelligence
Laboratory, 2000.

Munos, R. Policy gradient in continuous time. Journal of
Machine Learning Research, 7(May):771–791, 2006.

Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-
tized experience replay. arXiv preprint arXiv:1511.05952,
2015.

Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz,
P. Trust region policy optimization. In International Con-
ference on Machine Learning (ICML), pp. 1889–1897,
2015.

Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
Klimov, O. Proximal policy optimization algorithms.
arXiv preprint arXiv:1707.06347, 2017.

Sehnke, F., Osendorfer, C., R¨uckstieß, T., Graves, A., Pe-
ters, J., and Schmidhuber, J. Policy gradients with
parameter-based exploration for control. In International
Conference on Artiﬁcial Neural Networks, pp. 387–396.
Springer, 2008.

Shelton, C. R. Policy improvement for POMDPs using
normalized importance sampling. In Proceedings of the
Seventeenth conference on Uncertainty in artiﬁcial intelli-
gence (UAI), pp. 496–503. Morgan Kaufmann Publishers
Inc., 2001.

Trajectory-Based Off-Policy Deep Reinforcement Learning

Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and
Riedmiller, M. Deterministic policy gradient algorithms.
In ICML, 2014.

Snoek, J., Larochelle, H., and Adams, R. P. Practical
bayesian optimization of machine learning algorithms.
In Advances in neural information processing systems,
pp. 2951–2959, 2012.

Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour,
Y. Policy gradient methods for reinforcement learning
with function approximation. In Advances in neural infor-
mation processing systems (NIPS), pp. 1057–1063, 2000.

Wawrzynski, P. and Pacut, A. Truncated importance sam-
pling for reinforcement learning with experience replay.
Proc. CSIT Int. Multiconf, pp. 305–315, 2007.

Williams, R. J. Simple statistical gradient-following al-
gorithms for connectionist reinforcement learning. In
Reinforcement Learning, pp. 5–32. Springer, 1992.

Zhao, T., Hachiya, H., Tangkaratt, V., Morimoto, J., and
Sugiyama, M. Efﬁcient sample reuse in policy gradients
with parameter-based exploration. Neural computation,
25(6):1512–1547, 2013.

Trajectory-Based Off-Policy Deep Reinforcement Learning

Table 1. Algorithm hyper-parameters for the benchmark tasks.

Table 2. Information about the benchmark environments.

ENVIRONMENT

INPUTS

STATES HORIZON

CARTPOLE
MOUNTAINCAR
SWIMMER

1
1
2

4
2
13

100
500
1000

input and state dimensions, as well as the task horizons are
listed in Tab. 2.

5000
0.03
5000
0.1
2000
0.2

0.1
0.05
3I
50

ALGORITHM

PARAMETER

RANGE

SELECTED

REINFORCE

TRPO

PPO

BATCH SIZE
STEP SIZE
BATCH SIZE
STEP SIZE
BATCH SIZE
STEP SIZE

[400, 5000]
[0.0001, 0.1]
[400, 5000]
[0.0001, 0.1]
[400, 5000]
[0.0001, 0.2]

ALGORITHM

PARAMETER

SYMBOL

SELECTED

DD-OPG

TEMPERATURE
PENALTY
LENGTHSCALE
PATH BUFFER

λ
γ
log Σ
Nmax

A. Proof of Proposition 1

The weighted importance sampling estimator of the ex-
pected cost is given by

ˆJ WIS(θ) =

1
i=0 w(τi, θ)

(cid:80)M

M
(cid:88)

i=0

w(τi, θ)R(τi) ,

(22)

as derived in Sec. 3. Talking the derivative with respect to
the policy parameters, we obtain the policy gradient formu-
lation from theorem 1 as shown in (28).

B. Experimental Details

In the following section, details about the reference im-
plementations of REINFORCE, TRPO and PPO and their
parameter settings are summarized for the benchmark ex-
periments and the ablation study. Information about the
benchmark environments is given in Sec. B.2

B.1. Algorithm Conﬁgurations

The reference implementations of the benchmark algorithms
REINFORCE, TRPO and PPO are from the Garage RL
framework (Duan et al., 2016). A hyper-parameter grid
search has been conducted for each algorithm and each en-
vironment on separate random seeds. The parameter ranges
and selected hyper-parameters are indicated in Tab. 1. For
the benchmark itself, ten runs have been conducted for each
algorithm and each environment on the random seeds (404,
931, 159, 380, 858, 708, 16, 448, 136, 989).

The conﬁguration of the DD-OPG method is summarized
in Tab. 1.

B.2. Benchmark Environments

The benchmark environments are cartpole, mountaincar and
swimmer from the Garage RL framework. Details about the

Trajectory-Based Off-Policy Deep Reinforcement Learning

θ ˆJ WIS(θ) =

∇



(cid:32) N
(cid:88)

p(τi
θ)
|
(cid:80)
j p(τi

1
N

i=1

Figure 6. Derivation of the weighted IS policy gradient.

(cid:33)−1


N
(cid:88)

i=0

(cid:32)

θj)

|

(cid:33)−1 N
(cid:88)

θ

∇

i=0

p(τi
θ)
|
(cid:80)
j p(τi

1
N

θj)

|

R(τi)+

(cid:33)

R(τi)

p(τi
θ)
|
(cid:80)
j p(τi

θj)

|

(cid:33)

p(τi
θ)
|
(cid:80)
j p(τi

θj)

|
(cid:33)−2 (cid:32) N
(cid:88)

∇

i=1

(cid:33)−1 (cid:32) N
(cid:88)

1
N
(cid:33) (cid:32) N
(cid:88)

i=1

(cid:33)

wi(θ)

θwi(θ)

wi(θ)R(τi)

+

θwi(θ)R(τi)

i=1

∇
(cid:33) (cid:32) N
(cid:88)

(cid:33)

θwi(θ)

wi(θ)R(τi)

+

θwi(θ)R(τi)

(cid:33)

1
Z

(cid:32) N
(cid:88)

∇

i=1

(cid:33)

(cid:80)N

i=1 wi(θ)R(τi)
Z
(cid:33)

θwi(θ)R(τi)

θwi(θ)

−

∇

∇

θwi(θ)R(τi)

θwi(θ) ˆJ WIS(θ)

i=1

N
(cid:88)

i=1

N
(cid:88)

−

∇

i=1

θ



∇

(cid:32) N
(cid:88)

1
N
i=1
(cid:32) N
(cid:88)

i=1

=

−

(cid:32) N
(cid:88)

i=1

wi(θ)

(cid:32) N
(cid:88)

∇

i=1

=

−

=

=

1
Z

1
Z

1
Z

1
Z 2
(cid:32) N
(cid:88)

i=1
(cid:32) N
(cid:88)

∇

i=1

N
(cid:88)

∇

i=1

θ ˆJ WIS(θ) =

∇

(cid:16)

θwi(θ)

R(τi)

(cid:17)

ˆJ WIS(θ)

−

(23)

(24)

(25)

(26)

(27)

(28)

