Kapre: On-GPU Audio Preprocessing Layers for a Quick Implementation of
Deep Neural Network Models with Keras

Keunwoo Choi 1 Deokjin Joo 2 Juho Kim 2

7
1
0
2
 
n
u
J
 
9
1
 
 
]

D
S
.
s
c
[
 
 
1
v
1
8
7
5
0
.
6
0
7
1
:
v
i
X
r
a

Abstract

We introduce Kapre, Keras layers for audio and
music signal preprocessing. Music research us-
ing deep neural networks requires a heavy and
tedious preprocessing stage, for which audio pro-
cessing parameters are often ignored in param-
eter optimisation. To solve this problem, Kapre
implements time-frequency conversions, normal-
isation, and data augmentation as Keras layers.
We report simple benchmark results, showing
real-time on-GPU preprocessing adds a reason-
able amount of computation.

1. Introduction

Deep learning approach has been gaining attention in mu-
sic and audio informatics research, achieving state-of-the-
art performances in many problems including audio event
detection (Aytar et al., 2016) and music tagging (Choi et al.,
2016).

Since building deep neural network models is becoming
easier using frameworks that provide off-the-shelf mod-
ules, e.g., Keras (Chollet, 2015), preprocessing data often
occupies lots of time and effort.
It is more problematic
when dealing with audio data than images or texts due to
its large size and heavy decoding computation. A proce-
dure for audio data preparation generally includes i) decod-
ing, ii) resampling, and iii) conversion to a time-frequency
representation. Stages i/ii should be readily done since oth-
erwise they would be a huge bottleneck. However, Stage iii
can be implemented in many ways with pros and cons.

Running Stage iii in whether real-time or not is a trade-off
between storage and computation time. Pros: It enables
to search the best audio preprocessing conﬁguration e.g.,

1Centre for Digital Music, Queen Mary University of
London, London, UK 2University of
Illinois at Urbana-
Champaign, USA. Correspondence to: Keunwoo Choi <keun-
woo.choi@qmul.ac.uk>.

model = Sequential()
model.add(Melspectrogram(

input_shape=(2, 44100), # 1-sec stereo input
n_dft=512, n_hop=256, n_mels=128, sr=sr,
fmin=0.0, fmax=sr/2, return_decibel=False,
trainable_fb=False, trainable_kernel=False))

model.add(Normalization2D(str_axis=’freq’))
model.add(AdditiveNoise(power=0.2))
# and more layers for model hereafter

Listing 1: A code snippet that computes Mel-spectrogram,
normalises per frequency, and adds Gaussian noise.

time-frequency representations and their parameters. It can
vastly save storage, which usually is needed as much as
decoded audio samples for each conﬁguration. Cons: Ad-
ditional computation may impede training and inference.

One of the main reasons to propose an on-GPU audio pre-
processing is for a quick and easy implementation. Adding
a preprocessing layer can be done by a single line of code.
It can be done and may be faster on CPU with multipro-
cessing, but an optimised implementation is challenging.

With Kapre1, the whole preparation and training procedure
becomes simple: i) Decode (and possibly resample) audio
ﬁles and save them as binary formats, ii) implement a gen-
erator that loads the data, and iii) add a Kapre layer at the
input side of Keras model. In this way, the researcher’s user
experience is improved as well as audio processing param-
eters can be optimised.

A relevant question on the proposed practice is how much
time this approach would take for those merits. We present
the results of a simple benchmark in Section 3.

Librosa (McFee et al., 2017) and Essentia (Bogdanov et al.,
2013) provide audio and music analysis on Python. They
can be combined with input preprocessing utility such as
Pescador 2 and Fuel (Van Merri¨enboer et al., 2015) to im-
plement an effective data pipeline. PyTorch has its own au-
dio ﬁle loader3 but it does not support audio analysis such
as computing spectrograms.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

1https://github.com/keunwoochoi/kapre
2http://pescador.readthedocs.io/
3https://github.com/pytorch/audio

Kapre: Keras Audio Preprocessing Layers

Table 1. A 5-layer convolutional neural network used in the ex-
periment.

Layer index Layer type

Conv2D(64, (20, 3))

2 – 5

Conv2D(64, (3, 3))

Note
ReLU activation
(2, 2) stride
ReLU activation
(2, 2) stride

88 output nodes,
Softmax activation

AveragePooling2D() Global average

Dense

1

6

7

2. Kapre

As mentioned earlier, the main goal of Kapre is to perform
audio preprocessing in Keras layers. Listing 1 is an ex-
ample code that computes Mel-spectrogram, normalises it
per frequency, and adds Gaussian noise. Several important
layers are summarised as below and available as of Kapre
version 0.1.

• Spectrogram uses two 1-dimensional convolutions,
each of which is initialised with real and imaginary part of
discrete Fourier transform kernels respectively, following
the deﬁnition of discrete Fourier transform:

Xk =

xn · [cos(2πkn/N ) − i · sin(2πkn/N )]

(1)

N −1
(cid:88)

n=0

for k ∈ [0, N − 1]. The computation is implemented with
conv2d of Keras backend, which means the Fourier trans-
form kernels can be trained with backpropagation.

• Melspectrogram is an extended layer based on
Spectrogram with a multiplication by mel-scale conver-
sion matrix from linear frequencies which can be trained.

• Normalization2D normalises 2D input data (e.g., time-
frequency representations) per frequency, time, channel,
(single) data, and batch.

• Filterbank provides a general ﬁlterbank layer that can
be initialised with mel/log/linear frequency scales as well
as random.

• AdditiveNoise adds different types of noise for data
augmentation. The noise gain can be randomised for fur-
ther augmentation. Noise is only applied in training phase.

3. Experiments and Conclusions

An experiment is designed to investigate the additional
computation that Kapre introduces during training. We do
not directly measure the computation time of Kapre pre-
processing.
Instead, we measure the time difference be-
tween ‘time-frequency conversion + convnet’ vs. ‘convnet’
because these are realistic scenarios if we consider pre-
computing spectrograms as an alternative. This compari-
son compensates the effects of potential overheads e.g., i/o.

Figure 1. Normalised time consumed to train a convnet with and
without audio preprocessing layer

A 5-layer convolutional neural network summarised in Ta-
ble 1 is used. We use a dummy input/output data that is
generated before training and simulates 30-second mono
signal with 32,000 Hz sampling rate/88D one-hot-vector
respectively. There are 157,336 parameters in this net-
work, which is relatively small compared to many models
recently used in music/audio research. The training is con-
ﬁgured as batch size of 16, 512 batches per epoch, and 2
epochs, iterating 16,384 training samples overall. The ex-
periment is implemented with Keras 2.0.4 (Chollet, 2015),
Theano 0.9.0 (Theano Development Team, 2016), CUDA
8, and cuDNN 7.

For time-frequency conversion, short-time Fourier trans-
form with 512-point DFT, 50% overlap, and decibel scaling
is computed using kapre.time frequency.Spectrogram
layer.

Figure 1 compares the time consumptions normalised by
In the
without-conversion cases and on different GPUs.
experiment, the audio preprocessing layer adds about 20%
training time. Changing batch size did not affect this pro-
portion. Although it is plotted after normalisation for de-
vices, we should focus on the absolute time difference, too,
because that is the time consumed regardless of the size
of deep neural network model. This means on-GPU pre-
processing is more suitable for large-scale work since the
additional time cost, which is constant, can be relatively
minor for the training of bigger networks while beneﬁting
on storage usage with potentially larger-scale dataset.

To conclude, we proposed to adopt on-GPU audio prepro-
cessing for faster and easier prototyping. Kapre layers en-
ables a storage-efﬁcient input preprocessing optimisation.
We presented the additional computation time due to the
preprocessing on GPU which can be relatively small when
In practice, data may readily
large networks are used.
be preprocessed for more efﬁcient model hyperparameter
search once preprocessing parameters are set, until which
the proposed method can be efﬁcient.

Kapre: Keras Audio Preprocessing Layers

References

Aytar, Yusuf, Vondrick, Carl, and Torralba, Antonio.
Soundnet: Learning sound representations from unla-
In Advances in Neural Information Pro-
beled video.
cessing Systems, pp. 892–900, 2016.

Bogdanov, Dmitry, Wack, Nicolas, G´omez, Emilia, Gulati,
Sankalp, Herrera, Perfecto, Mayor, Oscar, Roma, Ger-
ard, Salamon, Justin, Zapata, Jos´e R, Serra, Xavier, et al.
Essentia: An audio analysis library for music informa-
tion retrieval. In ISMIR, pp. 493–498, 2013.

Choi, Keunwoo, Fazekas, George, and Sandler, Mark. Au-
tomatic tagging using deep convolutional neural net-
In The 17th International Society of Music In-
works.
formation Retrieval Conference, New York, USA. Inter-
national Society of Music Information Retrieval, 2016.

Chollet, Franc¸ois. Keras: Deep learning library for theano
and tensorﬂow. https://github.com/fchollet/keras, 2015.

McFee, Brian, McVicar, Matt, Nieto, Oriol, Balke, Stefan,
Thome, Carl, Liang, Dawen, Battenberg, Eric, Moore,
Josh, Bittner, Rachel, Yamamoto, Ryuichi, Ellis, Dan,
Stoter, Fabian-Robert, Repetto, Douglas, Waloschek, Si-
mon, Carr, CJ, Kranzler, Seth, Choi, Keunwoo, Viktorin,
Petr, Santos, Joao Felipe, Holovaty, Adrian, Pimenta,
Waldir, and Lee, Hojin.
librosa 0.5.0, February 2017.
URL https://doi.org/10.5281/zenodo.293021.

Theano Development Team. Theano: A Python frame-
work for fast computation of mathematical expressions.
arXiv e-prints, abs/1605.02688, May 2016. URL http:
//arxiv.org/abs/1605.02688.

Van Merri¨enboer, Bart, Bahdanau, Dzmitry, Dumoulin,
Vincent, Serdyuk, Dmitriy, Warde-Farley, David,
Chorowski, Jan, and Bengio, Yoshua. Blocks and
arXiv preprint
fuel: Frameworks for deep learning.
arXiv:1506.00619, 2015.

Kapre: On-GPU Audio Preprocessing Layers for a Quick Implementation of
Deep Neural Network Models with Keras

Keunwoo Choi 1 Deokjin Joo 2 Juho Kim 2

7
1
0
2
 
n
u
J
 
9
1
 
 
]

D
S
.
s
c
[
 
 
1
v
1
8
7
5
0
.
6
0
7
1
:
v
i
X
r
a

Abstract

We introduce Kapre, Keras layers for audio and
music signal preprocessing. Music research us-
ing deep neural networks requires a heavy and
tedious preprocessing stage, for which audio pro-
cessing parameters are often ignored in param-
eter optimisation. To solve this problem, Kapre
implements time-frequency conversions, normal-
isation, and data augmentation as Keras layers.
We report simple benchmark results, showing
real-time on-GPU preprocessing adds a reason-
able amount of computation.

1. Introduction

Deep learning approach has been gaining attention in mu-
sic and audio informatics research, achieving state-of-the-
art performances in many problems including audio event
detection (Aytar et al., 2016) and music tagging (Choi et al.,
2016).

Since building deep neural network models is becoming
easier using frameworks that provide off-the-shelf mod-
ules, e.g., Keras (Chollet, 2015), preprocessing data often
occupies lots of time and effort.
It is more problematic
when dealing with audio data than images or texts due to
its large size and heavy decoding computation. A proce-
dure for audio data preparation generally includes i) decod-
ing, ii) resampling, and iii) conversion to a time-frequency
representation. Stages i/ii should be readily done since oth-
erwise they would be a huge bottleneck. However, Stage iii
can be implemented in many ways with pros and cons.

Running Stage iii in whether real-time or not is a trade-off
between storage and computation time. Pros: It enables
to search the best audio preprocessing conﬁguration e.g.,

1Centre for Digital Music, Queen Mary University of
London, London, UK 2University of
Illinois at Urbana-
Champaign, USA. Correspondence to: Keunwoo Choi <keun-
woo.choi@qmul.ac.uk>.

model = Sequential()
model.add(Melspectrogram(

input_shape=(2, 44100), # 1-sec stereo input
n_dft=512, n_hop=256, n_mels=128, sr=sr,
fmin=0.0, fmax=sr/2, return_decibel=False,
trainable_fb=False, trainable_kernel=False))

model.add(Normalization2D(str_axis=’freq’))
model.add(AdditiveNoise(power=0.2))
# and more layers for model hereafter

Listing 1: A code snippet that computes Mel-spectrogram,
normalises per frequency, and adds Gaussian noise.

time-frequency representations and their parameters. It can
vastly save storage, which usually is needed as much as
decoded audio samples for each conﬁguration. Cons: Ad-
ditional computation may impede training and inference.

One of the main reasons to propose an on-GPU audio pre-
processing is for a quick and easy implementation. Adding
a preprocessing layer can be done by a single line of code.
It can be done and may be faster on CPU with multipro-
cessing, but an optimised implementation is challenging.

With Kapre1, the whole preparation and training procedure
becomes simple: i) Decode (and possibly resample) audio
ﬁles and save them as binary formats, ii) implement a gen-
erator that loads the data, and iii) add a Kapre layer at the
input side of Keras model. In this way, the researcher’s user
experience is improved as well as audio processing param-
eters can be optimised.

A relevant question on the proposed practice is how much
time this approach would take for those merits. We present
the results of a simple benchmark in Section 3.

Librosa (McFee et al., 2017) and Essentia (Bogdanov et al.,
2013) provide audio and music analysis on Python. They
can be combined with input preprocessing utility such as
Pescador 2 and Fuel (Van Merri¨enboer et al., 2015) to im-
plement an effective data pipeline. PyTorch has its own au-
dio ﬁle loader3 but it does not support audio analysis such
as computing spectrograms.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

1https://github.com/keunwoochoi/kapre
2http://pescador.readthedocs.io/
3https://github.com/pytorch/audio

Kapre: Keras Audio Preprocessing Layers

Table 1. A 5-layer convolutional neural network used in the ex-
periment.

Layer index Layer type

Conv2D(64, (20, 3))

2 – 5

Conv2D(64, (3, 3))

Note
ReLU activation
(2, 2) stride
ReLU activation
(2, 2) stride

88 output nodes,
Softmax activation

AveragePooling2D() Global average

Dense

1

6

7

2. Kapre

As mentioned earlier, the main goal of Kapre is to perform
audio preprocessing in Keras layers. Listing 1 is an ex-
ample code that computes Mel-spectrogram, normalises it
per frequency, and adds Gaussian noise. Several important
layers are summarised as below and available as of Kapre
version 0.1.

• Spectrogram uses two 1-dimensional convolutions,
each of which is initialised with real and imaginary part of
discrete Fourier transform kernels respectively, following
the deﬁnition of discrete Fourier transform:

Xk =

xn · [cos(2πkn/N ) − i · sin(2πkn/N )]

(1)

N −1
(cid:88)

n=0

for k ∈ [0, N − 1]. The computation is implemented with
conv2d of Keras backend, which means the Fourier trans-
form kernels can be trained with backpropagation.

• Melspectrogram is an extended layer based on
Spectrogram with a multiplication by mel-scale conver-
sion matrix from linear frequencies which can be trained.

• Normalization2D normalises 2D input data (e.g., time-
frequency representations) per frequency, time, channel,
(single) data, and batch.

• Filterbank provides a general ﬁlterbank layer that can
be initialised with mel/log/linear frequency scales as well
as random.

• AdditiveNoise adds different types of noise for data
augmentation. The noise gain can be randomised for fur-
ther augmentation. Noise is only applied in training phase.

3. Experiments and Conclusions

An experiment is designed to investigate the additional
computation that Kapre introduces during training. We do
not directly measure the computation time of Kapre pre-
processing.
Instead, we measure the time difference be-
tween ‘time-frequency conversion + convnet’ vs. ‘convnet’
because these are realistic scenarios if we consider pre-
computing spectrograms as an alternative. This compari-
son compensates the effects of potential overheads e.g., i/o.

Figure 1. Normalised time consumed to train a convnet with and
without audio preprocessing layer

A 5-layer convolutional neural network summarised in Ta-
ble 1 is used. We use a dummy input/output data that is
generated before training and simulates 30-second mono
signal with 32,000 Hz sampling rate/88D one-hot-vector
respectively. There are 157,336 parameters in this net-
work, which is relatively small compared to many models
recently used in music/audio research. The training is con-
ﬁgured as batch size of 16, 512 batches per epoch, and 2
epochs, iterating 16,384 training samples overall. The ex-
periment is implemented with Keras 2.0.4 (Chollet, 2015),
Theano 0.9.0 (Theano Development Team, 2016), CUDA
8, and cuDNN 7.

For time-frequency conversion, short-time Fourier trans-
form with 512-point DFT, 50% overlap, and decibel scaling
is computed using kapre.time frequency.Spectrogram
layer.

Figure 1 compares the time consumptions normalised by
In the
without-conversion cases and on different GPUs.
experiment, the audio preprocessing layer adds about 20%
training time. Changing batch size did not affect this pro-
portion. Although it is plotted after normalisation for de-
vices, we should focus on the absolute time difference, too,
because that is the time consumed regardless of the size
of deep neural network model. This means on-GPU pre-
processing is more suitable for large-scale work since the
additional time cost, which is constant, can be relatively
minor for the training of bigger networks while beneﬁting
on storage usage with potentially larger-scale dataset.

To conclude, we proposed to adopt on-GPU audio prepro-
cessing for faster and easier prototyping. Kapre layers en-
ables a storage-efﬁcient input preprocessing optimisation.
We presented the additional computation time due to the
preprocessing on GPU which can be relatively small when
In practice, data may readily
large networks are used.
be preprocessed for more efﬁcient model hyperparameter
search once preprocessing parameters are set, until which
the proposed method can be efﬁcient.

Kapre: Keras Audio Preprocessing Layers

References

Aytar, Yusuf, Vondrick, Carl, and Torralba, Antonio.
Soundnet: Learning sound representations from unla-
In Advances in Neural Information Pro-
beled video.
cessing Systems, pp. 892–900, 2016.

Bogdanov, Dmitry, Wack, Nicolas, G´omez, Emilia, Gulati,
Sankalp, Herrera, Perfecto, Mayor, Oscar, Roma, Ger-
ard, Salamon, Justin, Zapata, Jos´e R, Serra, Xavier, et al.
Essentia: An audio analysis library for music informa-
tion retrieval. In ISMIR, pp. 493–498, 2013.

Choi, Keunwoo, Fazekas, George, and Sandler, Mark. Au-
tomatic tagging using deep convolutional neural net-
In The 17th International Society of Music In-
works.
formation Retrieval Conference, New York, USA. Inter-
national Society of Music Information Retrieval, 2016.

Chollet, Franc¸ois. Keras: Deep learning library for theano
and tensorﬂow. https://github.com/fchollet/keras, 2015.

McFee, Brian, McVicar, Matt, Nieto, Oriol, Balke, Stefan,
Thome, Carl, Liang, Dawen, Battenberg, Eric, Moore,
Josh, Bittner, Rachel, Yamamoto, Ryuichi, Ellis, Dan,
Stoter, Fabian-Robert, Repetto, Douglas, Waloschek, Si-
mon, Carr, CJ, Kranzler, Seth, Choi, Keunwoo, Viktorin,
Petr, Santos, Joao Felipe, Holovaty, Adrian, Pimenta,
Waldir, and Lee, Hojin.
librosa 0.5.0, February 2017.
URL https://doi.org/10.5281/zenodo.293021.

Theano Development Team. Theano: A Python frame-
work for fast computation of mathematical expressions.
arXiv e-prints, abs/1605.02688, May 2016. URL http:
//arxiv.org/abs/1605.02688.

Van Merri¨enboer, Bart, Bahdanau, Dzmitry, Dumoulin,
Vincent, Serdyuk, Dmitriy, Warde-Farley, David,
Chorowski, Jan, and Bengio, Yoshua. Blocks and
arXiv preprint
fuel: Frameworks for deep learning.
arXiv:1506.00619, 2015.

Kapre: On-GPU Audio Preprocessing Layers for a Quick Implementation of
Deep Neural Network Models with Keras

Keunwoo Choi 1 Deokjin Joo 2 Juho Kim 2

7
1
0
2
 
n
u
J
 
9
1
 
 
]

D
S
.
s
c
[
 
 
1
v
1
8
7
5
0
.
6
0
7
1
:
v
i
X
r
a

Abstract

We introduce Kapre, Keras layers for audio and
music signal preprocessing. Music research us-
ing deep neural networks requires a heavy and
tedious preprocessing stage, for which audio pro-
cessing parameters are often ignored in param-
eter optimisation. To solve this problem, Kapre
implements time-frequency conversions, normal-
isation, and data augmentation as Keras layers.
We report simple benchmark results, showing
real-time on-GPU preprocessing adds a reason-
able amount of computation.

1. Introduction

Deep learning approach has been gaining attention in mu-
sic and audio informatics research, achieving state-of-the-
art performances in many problems including audio event
detection (Aytar et al., 2016) and music tagging (Choi et al.,
2016).

Since building deep neural network models is becoming
easier using frameworks that provide off-the-shelf mod-
ules, e.g., Keras (Chollet, 2015), preprocessing data often
occupies lots of time and effort.
It is more problematic
when dealing with audio data than images or texts due to
its large size and heavy decoding computation. A proce-
dure for audio data preparation generally includes i) decod-
ing, ii) resampling, and iii) conversion to a time-frequency
representation. Stages i/ii should be readily done since oth-
erwise they would be a huge bottleneck. However, Stage iii
can be implemented in many ways with pros and cons.

Running Stage iii in whether real-time or not is a trade-off
between storage and computation time. Pros: It enables
to search the best audio preprocessing conﬁguration e.g.,

1Centre for Digital Music, Queen Mary University of
London, London, UK 2University of
Illinois at Urbana-
Champaign, USA. Correspondence to: Keunwoo Choi <keun-
woo.choi@qmul.ac.uk>.

model = Sequential()
model.add(Melspectrogram(

input_shape=(2, 44100), # 1-sec stereo input
n_dft=512, n_hop=256, n_mels=128, sr=sr,
fmin=0.0, fmax=sr/2, return_decibel=False,
trainable_fb=False, trainable_kernel=False))

model.add(Normalization2D(str_axis=’freq’))
model.add(AdditiveNoise(power=0.2))
# and more layers for model hereafter

Listing 1: A code snippet that computes Mel-spectrogram,
normalises per frequency, and adds Gaussian noise.

time-frequency representations and their parameters. It can
vastly save storage, which usually is needed as much as
decoded audio samples for each conﬁguration. Cons: Ad-
ditional computation may impede training and inference.

One of the main reasons to propose an on-GPU audio pre-
processing is for a quick and easy implementation. Adding
a preprocessing layer can be done by a single line of code.
It can be done and may be faster on CPU with multipro-
cessing, but an optimised implementation is challenging.

With Kapre1, the whole preparation and training procedure
becomes simple: i) Decode (and possibly resample) audio
ﬁles and save them as binary formats, ii) implement a gen-
erator that loads the data, and iii) add a Kapre layer at the
input side of Keras model. In this way, the researcher’s user
experience is improved as well as audio processing param-
eters can be optimised.

A relevant question on the proposed practice is how much
time this approach would take for those merits. We present
the results of a simple benchmark in Section 3.

Librosa (McFee et al., 2017) and Essentia (Bogdanov et al.,
2013) provide audio and music analysis on Python. They
can be combined with input preprocessing utility such as
Pescador 2 and Fuel (Van Merri¨enboer et al., 2015) to im-
plement an effective data pipeline. PyTorch has its own au-
dio ﬁle loader3 but it does not support audio analysis such
as computing spectrograms.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

1https://github.com/keunwoochoi/kapre
2http://pescador.readthedocs.io/
3https://github.com/pytorch/audio

Kapre: Keras Audio Preprocessing Layers

Table 1. A 5-layer convolutional neural network used in the ex-
periment.

Layer index Layer type

Conv2D(64, (20, 3))

2 – 5

Conv2D(64, (3, 3))

Note
ReLU activation
(2, 2) stride
ReLU activation
(2, 2) stride

88 output nodes,
Softmax activation

AveragePooling2D() Global average

Dense

1

6

7

2. Kapre

As mentioned earlier, the main goal of Kapre is to perform
audio preprocessing in Keras layers. Listing 1 is an ex-
ample code that computes Mel-spectrogram, normalises it
per frequency, and adds Gaussian noise. Several important
layers are summarised as below and available as of Kapre
version 0.1.

• Spectrogram uses two 1-dimensional convolutions,
each of which is initialised with real and imaginary part of
discrete Fourier transform kernels respectively, following
the deﬁnition of discrete Fourier transform:

Xk =

xn · [cos(2πkn/N ) − i · sin(2πkn/N )]

(1)

N −1
(cid:88)

n=0

for k ∈ [0, N − 1]. The computation is implemented with
conv2d of Keras backend, which means the Fourier trans-
form kernels can be trained with backpropagation.

• Melspectrogram is an extended layer based on
Spectrogram with a multiplication by mel-scale conver-
sion matrix from linear frequencies which can be trained.

• Normalization2D normalises 2D input data (e.g., time-
frequency representations) per frequency, time, channel,
(single) data, and batch.

• Filterbank provides a general ﬁlterbank layer that can
be initialised with mel/log/linear frequency scales as well
as random.

• AdditiveNoise adds different types of noise for data
augmentation. The noise gain can be randomised for fur-
ther augmentation. Noise is only applied in training phase.

3. Experiments and Conclusions

An experiment is designed to investigate the additional
computation that Kapre introduces during training. We do
not directly measure the computation time of Kapre pre-
processing.
Instead, we measure the time difference be-
tween ‘time-frequency conversion + convnet’ vs. ‘convnet’
because these are realistic scenarios if we consider pre-
computing spectrograms as an alternative. This compari-
son compensates the effects of potential overheads e.g., i/o.

Figure 1. Normalised time consumed to train a convnet with and
without audio preprocessing layer

A 5-layer convolutional neural network summarised in Ta-
ble 1 is used. We use a dummy input/output data that is
generated before training and simulates 30-second mono
signal with 32,000 Hz sampling rate/88D one-hot-vector
respectively. There are 157,336 parameters in this net-
work, which is relatively small compared to many models
recently used in music/audio research. The training is con-
ﬁgured as batch size of 16, 512 batches per epoch, and 2
epochs, iterating 16,384 training samples overall. The ex-
periment is implemented with Keras 2.0.4 (Chollet, 2015),
Theano 0.9.0 (Theano Development Team, 2016), CUDA
8, and cuDNN 7.

For time-frequency conversion, short-time Fourier trans-
form with 512-point DFT, 50% overlap, and decibel scaling
is computed using kapre.time frequency.Spectrogram
layer.

Figure 1 compares the time consumptions normalised by
In the
without-conversion cases and on different GPUs.
experiment, the audio preprocessing layer adds about 20%
training time. Changing batch size did not affect this pro-
portion. Although it is plotted after normalisation for de-
vices, we should focus on the absolute time difference, too,
because that is the time consumed regardless of the size
of deep neural network model. This means on-GPU pre-
processing is more suitable for large-scale work since the
additional time cost, which is constant, can be relatively
minor for the training of bigger networks while beneﬁting
on storage usage with potentially larger-scale dataset.

To conclude, we proposed to adopt on-GPU audio prepro-
cessing for faster and easier prototyping. Kapre layers en-
ables a storage-efﬁcient input preprocessing optimisation.
We presented the additional computation time due to the
preprocessing on GPU which can be relatively small when
In practice, data may readily
large networks are used.
be preprocessed for more efﬁcient model hyperparameter
search once preprocessing parameters are set, until which
the proposed method can be efﬁcient.

Kapre: Keras Audio Preprocessing Layers

References

Aytar, Yusuf, Vondrick, Carl, and Torralba, Antonio.
Soundnet: Learning sound representations from unla-
In Advances in Neural Information Pro-
beled video.
cessing Systems, pp. 892–900, 2016.

Bogdanov, Dmitry, Wack, Nicolas, G´omez, Emilia, Gulati,
Sankalp, Herrera, Perfecto, Mayor, Oscar, Roma, Ger-
ard, Salamon, Justin, Zapata, Jos´e R, Serra, Xavier, et al.
Essentia: An audio analysis library for music informa-
tion retrieval. In ISMIR, pp. 493–498, 2013.

Choi, Keunwoo, Fazekas, George, and Sandler, Mark. Au-
tomatic tagging using deep convolutional neural net-
In The 17th International Society of Music In-
works.
formation Retrieval Conference, New York, USA. Inter-
national Society of Music Information Retrieval, 2016.

Chollet, Franc¸ois. Keras: Deep learning library for theano
and tensorﬂow. https://github.com/fchollet/keras, 2015.

McFee, Brian, McVicar, Matt, Nieto, Oriol, Balke, Stefan,
Thome, Carl, Liang, Dawen, Battenberg, Eric, Moore,
Josh, Bittner, Rachel, Yamamoto, Ryuichi, Ellis, Dan,
Stoter, Fabian-Robert, Repetto, Douglas, Waloschek, Si-
mon, Carr, CJ, Kranzler, Seth, Choi, Keunwoo, Viktorin,
Petr, Santos, Joao Felipe, Holovaty, Adrian, Pimenta,
Waldir, and Lee, Hojin.
librosa 0.5.0, February 2017.
URL https://doi.org/10.5281/zenodo.293021.

Theano Development Team. Theano: A Python frame-
work for fast computation of mathematical expressions.
arXiv e-prints, abs/1605.02688, May 2016. URL http:
//arxiv.org/abs/1605.02688.

Van Merri¨enboer, Bart, Bahdanau, Dzmitry, Dumoulin,
Vincent, Serdyuk, Dmitriy, Warde-Farley, David,
Chorowski, Jan, and Bengio, Yoshua. Blocks and
arXiv preprint
fuel: Frameworks for deep learning.
arXiv:1506.00619, 2015.

Kapre: On-GPU Audio Preprocessing Layers for a Quick Implementation of
Deep Neural Network Models with Keras

Keunwoo Choi 1 Deokjin Joo 2 Juho Kim 2

7
1
0
2
 
n
u
J
 
9
1
 
 
]

D
S
.
s
c
[
 
 
1
v
1
8
7
5
0
.
6
0
7
1
:
v
i
X
r
a

Abstract

We introduce Kapre, Keras layers for audio and
music signal preprocessing. Music research us-
ing deep neural networks requires a heavy and
tedious preprocessing stage, for which audio pro-
cessing parameters are often ignored in param-
eter optimisation. To solve this problem, Kapre
implements time-frequency conversions, normal-
isation, and data augmentation as Keras layers.
We report simple benchmark results, showing
real-time on-GPU preprocessing adds a reason-
able amount of computation.

1. Introduction

Deep learning approach has been gaining attention in mu-
sic and audio informatics research, achieving state-of-the-
art performances in many problems including audio event
detection (Aytar et al., 2016) and music tagging (Choi et al.,
2016).

Since building deep neural network models is becoming
easier using frameworks that provide off-the-shelf mod-
ules, e.g., Keras (Chollet, 2015), preprocessing data often
occupies lots of time and effort.
It is more problematic
when dealing with audio data than images or texts due to
its large size and heavy decoding computation. A proce-
dure for audio data preparation generally includes i) decod-
ing, ii) resampling, and iii) conversion to a time-frequency
representation. Stages i/ii should be readily done since oth-
erwise they would be a huge bottleneck. However, Stage iii
can be implemented in many ways with pros and cons.

Running Stage iii in whether real-time or not is a trade-off
between storage and computation time. Pros: It enables
to search the best audio preprocessing conﬁguration e.g.,

1Centre for Digital Music, Queen Mary University of
London, London, UK 2University of
Illinois at Urbana-
Champaign, USA. Correspondence to: Keunwoo Choi <keun-
woo.choi@qmul.ac.uk>.

model = Sequential()
model.add(Melspectrogram(

input_shape=(2, 44100), # 1-sec stereo input
n_dft=512, n_hop=256, n_mels=128, sr=sr,
fmin=0.0, fmax=sr/2, return_decibel=False,
trainable_fb=False, trainable_kernel=False))

model.add(Normalization2D(str_axis=’freq’))
model.add(AdditiveNoise(power=0.2))
# and more layers for model hereafter

Listing 1: A code snippet that computes Mel-spectrogram,
normalises per frequency, and adds Gaussian noise.

time-frequency representations and their parameters. It can
vastly save storage, which usually is needed as much as
decoded audio samples for each conﬁguration. Cons: Ad-
ditional computation may impede training and inference.

One of the main reasons to propose an on-GPU audio pre-
processing is for a quick and easy implementation. Adding
a preprocessing layer can be done by a single line of code.
It can be done and may be faster on CPU with multipro-
cessing, but an optimised implementation is challenging.

With Kapre1, the whole preparation and training procedure
becomes simple: i) Decode (and possibly resample) audio
ﬁles and save them as binary formats, ii) implement a gen-
erator that loads the data, and iii) add a Kapre layer at the
input side of Keras model. In this way, the researcher’s user
experience is improved as well as audio processing param-
eters can be optimised.

A relevant question on the proposed practice is how much
time this approach would take for those merits. We present
the results of a simple benchmark in Section 3.

Librosa (McFee et al., 2017) and Essentia (Bogdanov et al.,
2013) provide audio and music analysis on Python. They
can be combined with input preprocessing utility such as
Pescador 2 and Fuel (Van Merri¨enboer et al., 2015) to im-
plement an effective data pipeline. PyTorch has its own au-
dio ﬁle loader3 but it does not support audio analysis such
as computing spectrograms.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

1https://github.com/keunwoochoi/kapre
2http://pescador.readthedocs.io/
3https://github.com/pytorch/audio

Kapre: Keras Audio Preprocessing Layers

Table 1. A 5-layer convolutional neural network used in the ex-
periment.

Layer index Layer type

Conv2D(64, (20, 3))

2 – 5

Conv2D(64, (3, 3))

Note
ReLU activation
(2, 2) stride
ReLU activation
(2, 2) stride

88 output nodes,
Softmax activation

AveragePooling2D() Global average

Dense

1

6

7

2. Kapre

As mentioned earlier, the main goal of Kapre is to perform
audio preprocessing in Keras layers. Listing 1 is an ex-
ample code that computes Mel-spectrogram, normalises it
per frequency, and adds Gaussian noise. Several important
layers are summarised as below and available as of Kapre
version 0.1.

• Spectrogram uses two 1-dimensional convolutions,
each of which is initialised with real and imaginary part of
discrete Fourier transform kernels respectively, following
the deﬁnition of discrete Fourier transform:

Xk =

xn · [cos(2πkn/N ) − i · sin(2πkn/N )]

(1)

N −1
(cid:88)

n=0

for k ∈ [0, N − 1]. The computation is implemented with
conv2d of Keras backend, which means the Fourier trans-
form kernels can be trained with backpropagation.

• Melspectrogram is an extended layer based on
Spectrogram with a multiplication by mel-scale conver-
sion matrix from linear frequencies which can be trained.

• Normalization2D normalises 2D input data (e.g., time-
frequency representations) per frequency, time, channel,
(single) data, and batch.

• Filterbank provides a general ﬁlterbank layer that can
be initialised with mel/log/linear frequency scales as well
as random.

• AdditiveNoise adds different types of noise for data
augmentation. The noise gain can be randomised for fur-
ther augmentation. Noise is only applied in training phase.

3. Experiments and Conclusions

An experiment is designed to investigate the additional
computation that Kapre introduces during training. We do
not directly measure the computation time of Kapre pre-
processing.
Instead, we measure the time difference be-
tween ‘time-frequency conversion + convnet’ vs. ‘convnet’
because these are realistic scenarios if we consider pre-
computing spectrograms as an alternative. This compari-
son compensates the effects of potential overheads e.g., i/o.

Figure 1. Normalised time consumed to train a convnet with and
without audio preprocessing layer

A 5-layer convolutional neural network summarised in Ta-
ble 1 is used. We use a dummy input/output data that is
generated before training and simulates 30-second mono
signal with 32,000 Hz sampling rate/88D one-hot-vector
respectively. There are 157,336 parameters in this net-
work, which is relatively small compared to many models
recently used in music/audio research. The training is con-
ﬁgured as batch size of 16, 512 batches per epoch, and 2
epochs, iterating 16,384 training samples overall. The ex-
periment is implemented with Keras 2.0.4 (Chollet, 2015),
Theano 0.9.0 (Theano Development Team, 2016), CUDA
8, and cuDNN 7.

For time-frequency conversion, short-time Fourier trans-
form with 512-point DFT, 50% overlap, and decibel scaling
is computed using kapre.time frequency.Spectrogram
layer.

Figure 1 compares the time consumptions normalised by
In the
without-conversion cases and on different GPUs.
experiment, the audio preprocessing layer adds about 20%
training time. Changing batch size did not affect this pro-
portion. Although it is plotted after normalisation for de-
vices, we should focus on the absolute time difference, too,
because that is the time consumed regardless of the size
of deep neural network model. This means on-GPU pre-
processing is more suitable for large-scale work since the
additional time cost, which is constant, can be relatively
minor for the training of bigger networks while beneﬁting
on storage usage with potentially larger-scale dataset.

To conclude, we proposed to adopt on-GPU audio prepro-
cessing for faster and easier prototyping. Kapre layers en-
ables a storage-efﬁcient input preprocessing optimisation.
We presented the additional computation time due to the
preprocessing on GPU which can be relatively small when
In practice, data may readily
large networks are used.
be preprocessed for more efﬁcient model hyperparameter
search once preprocessing parameters are set, until which
the proposed method can be efﬁcient.

Kapre: Keras Audio Preprocessing Layers

References

Aytar, Yusuf, Vondrick, Carl, and Torralba, Antonio.
Soundnet: Learning sound representations from unla-
In Advances in Neural Information Pro-
beled video.
cessing Systems, pp. 892–900, 2016.

Bogdanov, Dmitry, Wack, Nicolas, G´omez, Emilia, Gulati,
Sankalp, Herrera, Perfecto, Mayor, Oscar, Roma, Ger-
ard, Salamon, Justin, Zapata, Jos´e R, Serra, Xavier, et al.
Essentia: An audio analysis library for music informa-
tion retrieval. In ISMIR, pp. 493–498, 2013.

Choi, Keunwoo, Fazekas, George, and Sandler, Mark. Au-
tomatic tagging using deep convolutional neural net-
In The 17th International Society of Music In-
works.
formation Retrieval Conference, New York, USA. Inter-
national Society of Music Information Retrieval, 2016.

Chollet, Franc¸ois. Keras: Deep learning library for theano
and tensorﬂow. https://github.com/fchollet/keras, 2015.

McFee, Brian, McVicar, Matt, Nieto, Oriol, Balke, Stefan,
Thome, Carl, Liang, Dawen, Battenberg, Eric, Moore,
Josh, Bittner, Rachel, Yamamoto, Ryuichi, Ellis, Dan,
Stoter, Fabian-Robert, Repetto, Douglas, Waloschek, Si-
mon, Carr, CJ, Kranzler, Seth, Choi, Keunwoo, Viktorin,
Petr, Santos, Joao Felipe, Holovaty, Adrian, Pimenta,
Waldir, and Lee, Hojin.
librosa 0.5.0, February 2017.
URL https://doi.org/10.5281/zenodo.293021.

Theano Development Team. Theano: A Python frame-
work for fast computation of mathematical expressions.
arXiv e-prints, abs/1605.02688, May 2016. URL http:
//arxiv.org/abs/1605.02688.

Van Merri¨enboer, Bart, Bahdanau, Dzmitry, Dumoulin,
Vincent, Serdyuk, Dmitriy, Warde-Farley, David,
Chorowski, Jan, and Bengio, Yoshua. Blocks and
arXiv preprint
fuel: Frameworks for deep learning.
arXiv:1506.00619, 2015.

