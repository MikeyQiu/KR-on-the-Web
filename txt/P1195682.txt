7
1
0
2
 
v
o
N
 
8
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
0
8
5
0
.
6
0
7
1
:
v
i
X
r
a

SVCCA: Singular Vector Canonical Correlation
Analysis for Deep Learning Dynamics and
Interpretability

Maithra Raghu,1,2 Justin Gilmer,1 Jason Yosinski,3 & Jascha Sohl-Dickstein1
1Google Brain 2Cornell University 3Uber AI Labs
maithrar gmail com, gilmer google com, yosinski uber com, jaschasd google com

Abstract

We propose a new technique, Singular Vector Canonical Correlation Analysis
(SVCCA), a tool for quickly comparing two representations in a way that is both
invariant to afﬁne transform (allowing comparison between different layers and
networks) and fast to compute (allowing more comparisons to be calculated than
with previous methods). We deploy this tool to measure the intrinsic dimension-
ality of layers, showing in some cases needless over-parameterization; to probe
learning dynamics throughout training, ﬁnding that networks converge to ﬁnal
representations from the bottom up; to show where class-speciﬁc information in
networks is formed; and to suggest new training regimes that simultaneously save
computation and overﬁt less.

1

Introduction

As the empirical success of deep neural networks ([7, 9, 18]) become an indisputable fact, the goal
of better understanding these models escalates in importance. Central to this aim is a core issue
of deciphering learned representations. Facets of this key question have been explored empirically,
particularly for image models, in [1, 2, 10, 12, 13, 14, 15, 19, 20]. Most of these approaches are
motivated by interpretability of learned representations. More recently, [11] studied the similarities
of representations learned by multiple networks by ﬁnding permutations of neurons with maximal
correlation.

In this work we introduce a new approach to the study of network representations, based on an
analysis of each neuron’s activation vector – the scalar outputs it emits on input datapoints. With
this interpretation of neurons as vectors (and layers as subspaces, spanned by neurons), we intro-
duce SVCCA, Singular Vector Canonical Correlation Analysis, an amalgamation of Singular Value
Decomposition and Canonical Correlation Analysis (CCA) [5], as a powerful method for analyzing
deep representations. Although CCA has not previously been used to compare deep representations,
it has been used for related tasks such as computing the similarity between modeled and measured
brain activity [16], and training multi-lingual word embeddings in language models [3].

The main contributions resulting from the introduction of SVCCA are the following:

1. We ask: is the dimensionality of a layer’s learned representation the same as the number
of neurons in the layer? Answer: No. We show that trained networks perform equally well
with a number of directions just a fraction of the number of neurons with no additional
training, provided they are carefully chosen with SVCCA (Section 2.1). We explore the
consequences for model compression (Section 4.4).

2. We ask: what do deep representation learning dynamics look like? Answer: Networks
broadly converge bottom up. Using SVCCA, we compare layers across time and ﬁnd they

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

Figure 1: To demonstrate SVCCA, we consider a toy regression task (regression target as in Figure 3). (a)
We train two networks with four fully connected hidden layers starting from different random initializations,
and examine the representation learned by the penultimate (shaded) layer in each network. (b) The neurons
with the highest activations in net 1 (maroon) and in net 2 (green). The x-axis indexes over the dataset: in
our formulation, the representation of a neuron is simply its value over a dataset (Section 2). (c) The SVD
directions — i.e. the directions of maximal variance — for each network. (d) The top SVCCA directions. We
see that each pair of maroon/green lines (starting from the top) are almost visually identical (up to a sign). Thus,
although looking at just neurons (b) seems to indicate that the networks learn very different representations,
looking at the SVCCA subspace (d) shows that the information in the representations are (up to a sign) nearly
identical.

solidify from the bottom up. This suggests a simple, computationally more efﬁcient method
of training networks, Freeze Training, where lower layers are sequentially frozen after a
certain number of timesteps (Sections 4.1, 4.2).

3. We develop a method based on the discrete Fourier transform which greatly speeds up the

application of SVCCA to convolutional neural networks (Section 3).

4. We also explore an interpretability question, of when an architecture becomes sensitive to
different classes. We ﬁnd that SVCCA captures the semantics of different classes, with
similar classes having similar sensitivities, and vice versa. (Section 4.3).

Experimental Details Most of our experiments are performed on CIFAR-10 (augmented with
random translations). The main architectures we use are a convolutional network and a residual
network1. To produce a few ﬁgures, we also use a toy regression task: training a four hidden layer
fully connected network with 1D input and 4D output, to regress on four different simple functions.

2 Measuring Representations in Neural Networks

Our goal in this paper is to analyze and interpret the representations learned by neural networks. The
critical question from which our investigation departs is: how should we deﬁne the representation
of a neuron? Consider that a neuron at a particular layer in a network computes a real-valued
function over the network’s input domain. In other words, if we had a lookup table of all possible
input → output mappings for a neuron, it would be a complete portrayal of that neuron’s functional
form.

However, such inﬁnite tables are not only practically infeasible, but are also problematic to process
into a set of conclusions. Our primary interest is not in the neuron’s response to random data, but
rather in how it represents features of a speciﬁc dataset (e.g. natural images). Therefore, in this
study we take a neuron’s representation to be its set of responses over a ﬁnite set of inputs — those
drawn from some training or validation set.
More concretely, for a given dataset X = {x1, · · · xm} and a neuron i on layer l, zzzl
be the vector of outputs on X, i.e.

i, we deﬁne zzzl

i to

i(x1), · · · , zzzl
1Convnet layers: conv-conv-bn-pool-conv-conv-conv-bn-pool-fc-bn-fc-bn-out. Resnet layers:

i = (zzzl
zzzl

i(xm))

conv-(x10 c/bn/r block)-(x10 c/bn/r block)-(x10 c/bn/r block)-bn-fc-out.

2

Note that this is a different vector from the often-considered vector of the “representation at a layer
of a single input.” Here zzzl
i is a single neuron’s response over the entire dataset, not an entire layer’s
response for a single input. In this view, a neuron’s representation can be thought of as a single
vector in a high-dimensional space. Broadening our view from a single neuron to the collection of
neurons in a layer, the layer can be thought of as the set of neuron vectors contained within that
layer. This set of vectors will span some subspace. To summarize:

Considered over a dataset X with m examples, a neuron is a vector in Rm.
A layer is the subspace of Rm spanned by its neurons’ vectors.

Within this formalism, we introduce Singular Vector Canonical Correlation Analysis (SVCCA) as
a method for analysing representations. SVCCA proceeds as follows:

• Input: SVCCA takes as input two (not necessarily different) sets of neurons (typically

layers of a network) l1 = {zzzl1

1 , ..., zzzl1
m1

} and l2 = {zzzl2

1 , ..., zzzl2
m2

}

1 ⊂ l1, l(cid:48)

• Step 1 First SVCCA performs a singular value decomposition of each subspace to get sub-
subspaces l(cid:48)
2 ⊂ l2 which comprise of the most important directions of the original
subspaces l1, l2. In general we take enough directions to explain 99% of variance in the
subspace. This is especially important in neural network representations, where as we will
show many low variance directions (neurons) are primarily noise.

1, l(cid:48)

• Step 2 Second, compute the Canonical Correlation similarity ([5]) of l(cid:48)

2: linearly trans-
2 to be as aligned as possible and compute correlation coefﬁcients. In particu-
1 , ..., zzz(cid:48)l2
}, l(cid:48)
}, CCA linearly
m(cid:48)
2
2 such as to maximize the correlations

form l(cid:48)
lar, given the output of step 1, l(cid:48)
transforms these subspaces ˜l1 = WX l(cid:48)
corrs = {ρ1, . . . ρmin(m(cid:48)

1 , ..., zzz(cid:48)l1
m(cid:48)
1
1, ˜l2 = WY l(cid:48)

2)} between the transformed subspaces.

2 = {zzz(cid:48)l2

1 = {zzz(cid:48)l1

1, l(cid:48)

1,m(cid:48)

• Output: With these steps, SVCCA outputs pairs of aligned directions, (˜zzzl1

i ) and how
well they correlate, ρi. Step 1 also produces intermediate output in the form of the top
singular values and directions.

i , ˜zzzl2

For a more detailed description of each step, see the Appendix. SVCCA can be used to analyse
any two sets of neurons. In our experiments, we utilize this ﬂexibility to compare representations
across different random initializations, architectures, timesteps during training, and speciﬁc classes
and layers.

Figure 1 shows a simple, intuitive demonstration of SVCCA. We train a small network on a toy
regression task and show each step of SVCCA, along with the resulting very similar representations.
SVCCA is able to ﬁnd hidden similarities in the representations.

2.1 Distributed Representations

An important property of SVCCA is that it is truly a subspace method: both SVD and CCA work
with span(zzz1, . . . , zzzm) instead of being axis aligned to the zzzi directions. SVD ﬁnds singular vectors
i = (cid:80)m
zzz(cid:48)
j=1 sijzzzj, and the subsequent CCA ﬁnds a linear transform W , giving orthogonal canon-
ically correlated directions {˜zzz1, . . . , ˜zzzm} = {(cid:80)m
j=1 w1jzzz(cid:48)
In other words,
SVCCA has no preference for representations that are neuron (axes) aligned.

j, . . . , (cid:80)m

j=1 wmjzzz(cid:48)

j}.

If representations are distributed across many dimensions, then this is a desirable property of a
representation analysis method. Previous studies have reported that representations may be more
complex than either fully distributed or axis-aligned [17, 21, 11] but this question remains open.

We use SVCCA as a tool to probe the nature of representations via two experiments:

(a) We ﬁnd that the subspace directions found by SVCCA are disproportionately important to

the representation learned by a layer, relative to neuron-aligned directions.

(b) We show that at least some of these directions are distributed across many neurons.

Experiments for (a), (b) are shown in Figure 2 as (a), (b) respectively. For both experiments, we ﬁrst
acquire two different representations, l1, l2, for a layer l by training two different random initializa-
tions of a convolutional network on CIFAR-10. We then apply SVCCA to l1 and l2 to get directions

3

(a)

(b)

Figure 2: Demonstration of (a) disproportionate importance of SVCCA directions, and (b) distributed nature
of some of these directions. For both panes, we ﬁrst ﬁnd the top k SVCCA directions by training two conv nets
on CIFAR-10 and comparing corresponding layers. (a) We project the output of the top three layers, pool1, fc1,
fc2, onto this top-k subspace. We see accuracy rises rapidly with increasing k, with even k (cid:28) num neurons
giving reasonable performance, with no retraining. Baselines of random k neuron subspaces and max activation
neurons require larger k to perform as well. (b): after projecting onto top k subspace (like left), dotted lines
then project again onto m neurons, chosen to correspond highly to the top k-SVCCA subspace. Many more
neurons are needed than k for better performance, suggesting distributedness of SVCCA directions.

1 , ..., ˜zzzl1

m} and {˜zzzl2

{˜zzzl1
1 , ..., ˜zzzl2
linear combination of the original neurons, i.e. ˜zzzli

m}, ordered according to importance by SVCCA, with each ˜zzzli
r=1 α(li)

j = (cid:80)m

jr zzzli
r .

j being a

For different values of k < m, we can then restrict layer li’s output to lie in the subspace of
span(˜zzzli
k ), the most useful k-dimensional subspace as found by SVCCA, done by projecting
each neuron into this k dimensional space.

1 , . . . , ˜zzzli

We ﬁnd — somewhat surprisingly — that very few SVCCA directions are required for the network
to perform the task well. As shown in Figure 2(a), for a network trained on CIFAR-10, the ﬁrst
25 dimensions provide nearly the same accuracy as using all 512 dimensions of a fully connected
layer with 512 neurons. The accuracy curve rises rapidly with the ﬁrst few SVCCA directions, and
plateaus quickly afterwards, for k (cid:28) m. This suggests that the useful information contained in m
neurons is well summarized by the subspace formed by the top k SVCCA directions. Two base-
lines for comparison are picking random and maximum activation neuron aligned subspaces and
projecting outputs onto these. Both of these baselines require far more directions (in this case: neu-
rons) before matching the accuracy achieved by the SVCCA directions. These results also suggest
approaches to model compression, which are explored in more detail in Section 4.4.

Figure 2(b) next demonstrates that these useful SVCCA directions are at least somewhat distributed
over neurons rather than axis-aligned. First, the top k SVCCA directions are picked and the rep-
resentation is projected onto this subspace. Next, the representation is further projected onto m
neurons, where the m are chosen as those most important to the SVCCA directions . The resulting
accuracy is plotted for different choices of k (given by x-axis) and different choices of m (different
lines). That, for example, keeping even 100 fc1 neurons (dashed green line) cannot maintain the
accuracy of the ﬁrst 20 SVCCA directions (solid green line at x-axis 20) suggests that those 20
SVCCA directions are distributed across 5 or more neurons each, on average. Figure 3 shows a
further demonstration of the effect on the output of projecting onto top SVCCA directions, here for
the toy regression case.

Why the two step SV + CCA method is needed. Both SVD and CCA have important properties
for analysing network representations and SVCCA consequently beneﬁts greatly from being a two
step method. CCA is invariant to afﬁne transformations, enabling comparisons without natural
alignment (e.g. different architectures, Section 4.4). See Appendix B for proofs and a demonstrative
ﬁgure. While CCA is a powerful method, it also suffers from certain shortcomings, particularly in
determining how many directions were important to the original space X, which is the strength of

4

Figure 3: The effect on the output of a latent representation being projected onto top SVCCA directions in
the toy regression task. Representations of the penultimate layer are projected onto 2, 6, 15, 30 top SVCCA
directions (from second pane). By 30, the output looks very similar to the full 200 neuron output (left).

SVD. See Appendix for an example where naive CCA performs badly. Both the SVD and CCA
steps are critical to the analysis of learning dynamics in Section 4.1.

3 Scaling SVCCA for Convolutional Layers

Applying SVCCA to convolutional layers can be done in two natural ways:

(1) Same layer comparisons: If X, Y are the same layer (at different timesteps or across ran-
dom initializations) receiving the same input we can concatenate along the pixel (height h,
width w) coordinates to form a vector: a conv layer h × w × c maps to c vectors, each
of dimension hwd, where d is the number of datapoints. This is a natural choice because
neurons at different pixel coordinates see different image data patches to each other. When
X, Y are two versions of the same layer, these c different views correspond perfectly.

(2) Different layer comparisons: When X, Y are not the same layer, the image patches seen by
different neurons have no natural correspondence. But we can ﬂatten an h×w ×c conv into
hwc neurons, each of dimension d. This approach is valid for convs in different networks
or at different depths.

3.1 Scaling SVCCA with Discrete Fourier Transforms

Applying SVCCA to convolutions introduces a computational challenge: the number of neurons
(h×w×c) in convolutional layers, especially early ones, is very large, making SVCCA prohibitively
expensive due to the large matrices involved. Luckily the problem of approximate dimensionality
reduction of large matrices is well studied, and efﬁcient algorithms exist, e.g. [4].

For convolutional layers however, we can avoid dimensionality reduction and perform exact
SVCCA, even for large networks. This is achieved by preprocessing each channel with a Discrete
Fourier Transform (which preserves CCA due to invariances, see Appendix), causing all (covari-
ance) matrices to be block-diagonal. This allows all matrix operations to be performed block by
block, and only over the diagonal blocks, vastly reducing computation. We show:

Theorem 1. Suppose we have a translation invariant (image) dataset X and convolutional layers
l1, l2. Letting DF T (li) denote the discrete fourier transform applied to each channel of li, the
covariance cov(DF T (l1), DF T (l2)) is block diagonal, with blocks of size c × c.

We make only two assumptions: 1) all layers below l1, l2 are either conv or pooling layers with
circular boundary conditions (translation equivariance) 2) The dataset X has all translations of the
images Xi. This is necessary in the proof for certain symmetries in neuron activations, but these
symmetries typically exist in natural images even without translation invariance, as shown in Fig-
ure App.2 in the Appendix. Below are key statements, with proofs in Appendix.

Deﬁnition 1. Say a single channel image dataset X of images is translation invariant if for any
(wlog n × n) image Xi ∈ X, with pixel values {zzz11, ...zzznn}, X (a,b)
= {zzzσa(1)σb(1), ...zzzσa(n)σb(n)}
is also in X, for all 0 ≤ a, b ≤ n − 1, where σa(i) = a + i mod n (and similarly for b).

i

For a multiple channel image Xi, an (a, b) translation is an (a, b) height/width shift on every channel
separately. X is then translation invariant as above.

5

To prove Theorem 1, we ﬁrst show another theorem:
Theorem 2. Given a translation invariant dataset X, and a convolutional layer l with channels
{c1, . . . ck} applied to X

(a) the DFT of ci, F cF T has diagonal covariance matrix (with itself).
(b) the DFT of ci, cj, F ciF T , F cjF T have diagonal covariance with each other.
Finally, both of these theorems rely on properties of circulant matrices and their DFTs:
Lemma 1. The covariance matrix of ci applied to translation invariant X is circulant and block
circulant.
Lemma 2. The DFT of a circulant matrix is diagonal.

4 Applications of SVCCA

4.1 Learning Dynamics with SVCCA

We can use SVCCA as a window into learning dynamics by comparing the representation at a
layer at different points during training to its ﬁnal representation. Furthermore, as the SVCCA
computations are relatively cheap to compute compared to methods that require training an auxiliary
network for each comparison [1, 10, 11], we can compare all layers during training at all timesteps
to all layers at the ﬁnal time step, producing a rich view into the learning process.

The outputs of SVCCA are the aligned directions (˜xi, ˜yi), how well they align, ρi, as well as in-
termediate output from the ﬁrst step, of singular values and directions, λ(i)
Y , y(cid:48)(j). We
condense these outputs into a single value, the SVCCA similarity ¯ρ, that encapsulates how well the
representations of two layers are aligned with each other,

X , x(cid:48)(i), λ(j)

¯ρ =

1
min (m1, m2)

(cid:88)

ρi,

i

(1)

where min (m1, m2) is the size of the smaller of the two layers being compared. The SVCCA
similarity ¯ρ is the average correlation across aligned directions, and is a direct multidimensional
analogue of Pearson correlation.

The SVCCA similarity for all pairs of layers, and all time steps, is shown in Figure 4 for a convnet
and a resnet architecture trained on CIFAR10.

4.2 Freeze Training

Observing in Figure 4 that networks broadly converge from the bottom up, we propose a training
method where we successively freeze lower layers during training, only updating higher and higher
layers, saving all computation needed for deriving gradients and updating in lower layers.

We apply this method to convolutional and residual networks trained on CIFAR-10, Figure 5, using
a linear freezing regime: in the convolutional network, each layer is frozen at a fraction (layer num-
ber/total layers) of total training time, while for resnets, each residual block is frozen at a fraction
(block number/total blocks). The vertical grey dotted lines show which steps have another set of lay-
ers frozen. Aside from saving computation, Freeze Training appears to actively help generalization
accuracy, like early stopping but with different layers requiring different stopping points.

4.3

Interpreting Representations: when are classes learned?

We also can use SVCCA to compare how correlated representations in each layer are with the logits
of each class in order to measure how knowledge about the target evolves throughout the network.
In Figure 6 we apply the DFT CCA technique on the Imagenet Resnet [6]. We take ﬁve different
classes and for different layers in the network, compute the DFT CCA similarity between the logit
of that class and the network layer. The results successfully reﬂect semantic aspects of the classes:
the ﬁretruck class sensitivity line is clearly distinct from the two pairs of dog breeds, and network
develops greater sensitivity to ﬁretruck earlier on. The two pairs of dog breeds, purposefully chosen
so that each pair is similar to the other in appearance, have cca similarity lines that are very close to
each other through the network, indicating these classes are similar to each other.

6

Figure 4: Learning dynamics plots for conv (top) and res (bottom) nets trained on CIFAR-10. Each pane is
a matrix of size layers × layers, with each entry showing the SVCCA similarity ¯ρ between the two layers.
Note that learning broadly happens ‘bottom up’ – layers closer to the input seem to solidify into their ﬁnal
representations with the exception of the very top layers. Per layer plots are included in the Appendix. Other
patterns are also visible – batch norm layers maintain nearly perfect similarity to the layer preceding them due
to scaling invariance (with a slight reduction since batch norm changes the SVD directions which capture 99%
of the variance). In the resnet plot, we see a stripe like pattern due to skip connections inducing high similarities
to previous layers.

Figure 5: Freeze Training reduces training cost and improves generalization. We apply Freeze Training to a
convolutional network on CIFAR-10 and a residual network on CIFAR-10. As shown by the grey dotted lines
(which indicate the timestep at which another layer is frozen), both networks have a ‘linear’ freezing regime:
for the convolutional network, we freeze individual layers at evenly spaced timesteps throughout training. For
the residual network, we freeze entire residual blocks at each freeze step. The curves were averaged over ten
runs.

4.4 Other Applications: Cross Model Comparison and compression

SVCCA similarity can also be used to compare the similarity of representations across different
random initializations, and even different architectures. We compare convolutional networks on
CIFAR-10 across random initializations (Appendix) and also a convolutional network to a residual
network in Figure 7, using the DFT method described in 3.

In Figure 3, we saw that projecting onto the subspace of the top few SVCCA directions resulted in
comparable accuracy. This observations motivates an approach to model compression. In particular,
letting the output vector of layer l be xxx(l) ∈ Rn×1, and the weights W (l), we replace the usual
W (l)xxx(l) with (W (l)P T
x )(Pxxxx(l)) where Px is a k × n projection matrix, projecting xxx onto the top
SVCCA directions. This bottleneck reduces both parameter count and inference computational cost

7

Figure 6: We plot the CCA similarity using the Discrete Fourier Transform between the logits of ﬁve classes
and layers in the Imagenet Resnet. The classes are ﬁretruck and two pairs of dog breeds (terriers and husky
like dogs: husky and eskimo dog) that are chosen to be similar to each other. These semantic properties are
captured in CCA similarity, where we see that the line corresponding to ﬁretruck is clearly distinct from the
two pairs of dog breeds, and the two lines in each pair are both very close to each other, reﬂecting the fact that
each pair consists of visually similar looking images. Firetruck also appears to be easier for the network to
learn, with greater sensitivity displayed much sooner.

Figure 7: We plot the CCA similarity using the Discrete Fourier Transform between convolutional layers of a
Resnet and Convnet trained on CIFAR-10. We ﬁnd that the lower layrs of both models are noticeably similar to
each other, and get progressively less similar as we compare higher layers. Note that the highest layers of the
resnet are least similar to the lower layers of the convnet.

for the layer by a factor ∼ k
n . In Figure App.5 in the Appendix, we show that we can consecutively
compress top layers with SVCCA by a signiﬁcant amount (in one case reducing each layer to 0.35
original size) and hardly affect performance.

5 Conclusion

In this paper we present SVCCA, a general method which allows for comparison of the learned dis-
tributed representations between different neural network layers and architectures. Using SVCCA
we obtain novel insights into the learning dynamics and learned representations of common neural
network architectures. These insights motivated a new Freeze Training technique which can reduce
the number of ﬂops required to train networks and potentially even increase generalization perfor-
mance. We observe that CCA similarity can be a helpful tool for interpretability, with sensitivity
to different classes reﬂecting their semantic properties. This technique also motivates a new algo-
rithm for model compression. Finally, the “lower layers learn ﬁrst” behavior was also observed for
recurrent neural networks as shown in Figure App.6 in the Appendix.

8

References

[1] Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classiﬁer

probes. arXiv preprint arXiv:1610.01644, 2016.

[2] David Eigen, Jason Rolfe, Rob Fergus, and Yann LeCun. Understanding deep architectures

using a recursive convolutional network. arXiv preprint arXiv:1312.1847, 2013.

[3] Manaal Faruqui and Chris Dyer. Improving vector space word representations using multilin-

gual correlation. Association for Computational Linguistics, 2014.

[4] Nathan Halko, Martinsson Per-Gunnar, and Joel A. Tropp. Finding structure with random-
ness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM
Rev., 53:217–288, 2011.

[5] D. R. Hardoon, S. Szedmak, and J. Shawe-Taylor. Canonical correlation analysis: An overview

with application to learning methods. Neural Computation, 16:2639–2664, 2004.

[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image

recognition. CoRR, abs/1512.03385, 2015.

[7] Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel-rahman Mohamed, Navdeep
Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neu-
ral networks for acoustic modeling in speech recognition: The shared views of four research
groups. IEEE Signal Processing Magazine, 29(6):82–97, 2012.

[8] Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 1985.
[9] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems, pages
1097–1105, 2012.

[10] Karel Lenc and Andrea Vedaldi. Understanding image representations by measuring their
equivariance and equivalence. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 991–999, 2015.

[11] Y. Li, J. Yosinski, J. Clune, H. Lipson, and J. Hopcroft. Convergent Learning: Do different
In International Conference on Learning

neural networks learn the same representations?
Representations (ICLR), May 2016.

[12] Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. Convergent learning:
Do different neural networks learn the same representations? In Feature Extraction: Modern
Questions and Challenges, pages 196–212, 2015.

[13] Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by in-
verting them. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-
nition, pages 5188–5196, 2015.

[14] Gr´egoire Montavon, Mikio L Braun, and Klaus-Robert M¨uller. Kernel analysis of deep net-

works. Journal of Machine Learning Research, 12(Sep):2563–2581, 2011.

[15] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional
arXiv preprint

networks: Visualising image classiﬁcation models and saliency maps.
arXiv:1312.6034, 2013.

[16] David Sussillo, Mark M Churchland, Matthew T Kaufman, and Krishna V Shenoy. A neural
network that ﬁnds a naturalistic solution for the production of muscle activity. Nature neuro-
science, 18(7):1025–1033, 2015.

[17] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
arXiv preprint

Intriguing properties of neural networks.

Goodfellow, and Rob Fergus.
arXiv:1312.6199, 2013.

[18] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural ma-
chine translation system: Bridging the gap between human and machine translation. arXiv
preprint arXiv:1609.08144, 2016.

[19] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding
neural networks through deep visualization. In Deep Learning Workshop, International Con-
ference on Machine Learning (ICML), 2015.

9

[20] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In

European conference on computer vision, pages 818–833. Springer, 2014.

[21] Bolei Zhou, Aditya Khosla, `Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object de-
tectors emerge in deep scene cnns. In International Conference on Learning Representations
(ICLR), volume abs/1412.6856, 2014.

10

Appendix

A Mathematical details of CCA and SVCCA

Canonical Correlation of X, Y
ﬁnding a, b to maximise:

Finding maximal correlations between X, Y can be expressed as

aT ΣXY b

(cid:112)

(cid:112)

aT ΣXX a

bT ΣY Y b

where ΣXX , ΣXY , ΣY X , ΣY Y are the covariance and cross-covariance terms. By performing the
change of basis ˜x˜x˜x1 = Σ1/2
Y Y b and using Cauchy-Schwarz we recover an eigenvalue
problem:

xx a and ˜y˜y˜y1 = Σ1/2

˜x˜x˜x1 = argmax

(cid:34)

xT Σ−1/2

XX ΣXY Σ−1
||x||

Y Y ΣY X Σ−1/2
XX x

(cid:35)

(*)

SVCCA Given two subspaces X = {xxx1, ..., xxxm1 }, Y = {yyy1, ..., yyym2}, SVCCA ﬁrst performs a
singular value decomposition on X, Y . This results in singular vectors {x(cid:48)x(cid:48)x(cid:48)
m1 } with associ-
ated singular values {λ1, ..., λm1} (for X, and similarly for Y ). Of these m1 singular vectors, we
keep the top m(cid:48)
i=1 |λi|). That is, 99% of
the variation of X is explainable by the top m(cid:48)
1 vectors. This helps remove directions/neurons that
are constant zero, or noise with small magnitude.
Then, we apply Canonical Correlation Analysis (CCA) to the sets {x(cid:48)x(cid:48)x(cid:48)
top singular vectors.

1 is the smallest value that (cid:80)m(cid:48)

i=1 |λi|(≥ 0.99 (cid:80)m1

1 where m(cid:48)

1, ..., y(cid:48)y(cid:48)y(cid:48)

1, ..., x(cid:48)x(cid:48)x(cid:48)

1, ..., x(cid:48)x(cid:48)x(cid:48)

}, {y(cid:48)y(cid:48)y(cid:48)

} of

m(cid:48)
2

m(cid:48)
1

1

CCA is a well established statistical method for understanding the similarity of two different sets
of random variables – given our two sets of vectors {x(cid:48)x(cid:48)x(cid:48)
}, we wish to ﬁnd
linear transformations, WX , WY that maximally correlate the subspaces. This can be reduced to
an eigenvalue problem. Solving this results in linearly transformed subspaces ˜X, ˜Y with directions
˜xxxi, ˜yyyi that are maximally correlated with each other, and orthogonal to ˜xxxj, ˜yyyj, j < i. We let ρi =
corr(˜xxxi, ˜yyyi). In summary, we have:

1, ..., y(cid:48)y(cid:48)y(cid:48)

1, ..., x(cid:48)x(cid:48)x(cid:48)

}, {y(cid:48)y(cid:48)y(cid:48)

m(cid:48)
1

m(cid:48)
2

SVCCA Summary

1. Input: X, Y
2. Perform: SVD(X), SVD(Y). Output: X (cid:48) = U X, Y (cid:48) = V Y
3. Perform CCA(X (cid:48), Y (cid:48)). Output:

{ρ1, . . . ρmin(m1,m2)}

˜X = WX X (cid:48), ˜Y = WY Y (cid:48) and corrs =

B Additional Proofs and Figures from Section 2.1

Proof of Orthonormal and Scaling Invariance of CCA:

We can see this using equation (*) as follows: suppose U, V are orthonormal transforms applied to
the sets X, Y . Then it follows that Σa
XX becomes U Σa
XX U T , for a = {1, −1, 1/2, −1/2}, and
similarly for Y and V . Also note ΣXY becomes U ΣXY V T . Equation (*) then becomes
xT U Σ−1/2

(cid:34)

(cid:35)

˜x1 = argmax

Y Y ΣY X Σ−1/2
XX ΣXY Σ−1
||x||

XX U T x

So if ˜u is a solution to equation (*), then U ˜u is a solution to the equation above, which results in the
same correlation coefﬁcients.

B.0.1 The importance of SVD: how many directions matter?

While CCA is excellent at identifying useful learned directions that correlate, independent of certain
common transforms, it doesn’t capture the full picture entirely. Consider the following setting:

11

Figure App.1: This ﬁgure shows the ability of CCA to deal with orthogonal and scaling transforms.
In the ﬁrst pane, the maroon plot shows one of the highest activation neurons in the penultimate
layer of a network trained on CIFAR-10, with the x-axis being (ordered) image ids and the y-axis
being activation on that image. The green plots show two resulting distorted directions after this and
two of the other top activation neurons are permuted, rotated and scaled. Pane two shows the result
of applying CCA to the distorted directions and the original signal, which succeeds in recovering
the original signal.

suppose we have subspaces A, B, C, with A being 50 dimensions, B being 200 dimensions, 50 of
which are perfectly aligned with A and the other 150 being noise, and C being 200 dimensions, 50
of which are aligned with A (and B) and the other 150 being useful, but different directions.

Then looking at the canonical correlation coefﬁcients of (A, B) and (A, C) will give the same result,
both being 1 for 50 values and 0 for everything else. But these are two very different cases – the
subspace B is indeed well represented by the 50 directions that are aligned with A. But the subspace
C has 150 more useful directions.

This distinction becomes particularly important when aggregating canonical correlation coefﬁcients
as a measure of similarity, as used in analysing network learning dynamics. However, by ﬁrst ap-
plying SVD to determine the number of directions needed to explain 99% of the observed variance,
we can distinguish between pathological cases like the one above.

C Proof of Theorem 1

Here we provide the proofs for Lemma 1, Lemma 2, Theorem 2 and ﬁnally Theorem 1.

A preliminary note before we begin:

When we consider a (wlog) n by n channel c of a convolutional layer, we assume it has shape







zzz0,0
zzz1,0
...
zzzn−1,0

zzz1,2
zzz2,2
...
zzzn−1,1

. . .
. . .
. . .
. . .







zzz0,n−1
zzz1,n−1
...
zzzn−1,n−1

12

(a)

(b)

(c)

(d)

Figure App.2:
This ﬁgure visualizes the covariance matrix of one of the channels of a resnet
trained on Imagenet. Black correspond to large values and white to small values. (a) we compute the
covariance without a translation invariant dataset and without ﬁrst preprocessing the images by DFT.
We see that the covariance matrix is dense. (b) We compute the covariance after applying DFT, but
without augmenting the dataset with translations. Even without enforcing translation invariance, we
see that the covariance in the DFT basis is approximately diagonal. (c) Same as (a), but the dataset
is augmented to be fully translation invariant. The covariance in the pixel basis is still dense. (d)
Same as (c), but with dataset augmented to be translation invariant. The covariance matrix is exactly
diagonal for a translation invariant dataset in a DFT basis.

When computing the covariance matrix however, we vectorize c by stacking the columns under each
other, and call the result vec(c):

vec(c) =

:=














zzz0,0
zzz1,0
...
zzzn−1,0
zzz0,1
...
zzzn−1,n−1








































zzz0
zzz1
...
zzzn−1
zzzn
...
zzzn2−1

vec(AcB) = (BT ⊗ A)vec(c)

One useful identity when switching between these two notations (see e.g. [8]) is

where A, B are matrices and ⊗ is the Kronecker product. A useful observation arising from this is:
Lemma 3. The CCA vectors of DF T (ci), DF T (cj) are the same (up to a rotation by F ) as the
CCA of ci, cj.

Proof: From Section B we know that unitary transforms only rotate the CCA directions. But while
DFT pre and postmultiplies by F, F T – unitary matrices, we cannot directly apply this as the result
is for unitary transforms on vec(ci). But, using the identity above, we see that vec(DF T (ci)) =
vec(F ciF T ) = (F ⊗ F )vec(ci), which is unitary as F is unitary. Applying the same identity to cj,
we can thus conclude that the DFT preserves CCA (up to rotations).

As Theorem 1 preprocesses the neurons with DFT, it is important to note that by the Lemma above,
we do not change the CCA vectors (except by a rotation).

C.1 Proof of Lemma 1

Proof. Translation invariance is preserved We show inductively that any translation invariant input
to a convolutional channel results in a translation invariant output: Suppose the input to channel c,
(n by n) is translation invariant. It is sufﬁcient to show that for inputs Xi, Xj and 0 ≤ a, b, ≤ n − 1,
c(Xi) + (a, b) mod n = c(Xj). But an (a, b) shift in neuron coordinates in c corresponds to a
(height stride · a, width stride · b) shift in the input. And as X is translation invariant, there is some
Xj = Xi + (height stride · a, width stride · b).

cov(c) is circulant:

13

Let X be (by proof above) a translation invariant input to a channel c in some convolution or pooling
layer. The empirical covariance, cov(c) is the n2 by n2 matrix computed by (assuming c is centered)

1
|X|

(cid:88)

Xi∈X

vec(c(Xi)) · vec(c(Xi))T

So, cov(c)ij = 1
j.

|X|zzzT

i zzzj = 1
|X|

(cid:80)

Xl∈X zzzT

i (Xl)zzzj(Xl), i.e. the inner products of the neurons i and

The indexes i and j refer to the neurons in their vectorized order in vec(c). But in the matrix ordering
of neurons in c, i and j correspond to some (a1, b1) and (a2, b2). If we applied a translation (a, b),
to both, we would get new neuron coordinates (a1 + a, b1 + b), (a2 + a, b2 + b) (all coordinates
mod n) which would correspond to i + an + b mod n2 and j + an + b mod n2, by our stacking
of columns and reindexing.

Let τa,b be the translation in inputs corresponding to an (a, b) translation in c, i.e.
τa,b =
(height stride·a, width stride·b). Then clearly zzz(a1,b1)(Xi) = zzz(a1+a,b1+b)(τ(a,b)(Xi), and similarly
for zzz(a2,b2)
It follows that 1

(a1+b,b1+b)zzz(a2+a,b2+b), or, with vec(c) indexing

(a1,b1)zzz(a2,b2) = 1

|X|zzzT

|X|zzzT

1
|X|

1
|X|

zzzT
i zzzj =

zzzT
(i+an+b mod n2)zzz(j+an+b mod n2)

This gives us the circulant structure of cov(c).

cov(c) is block circulant: Let zzz(i) be the ith column of c, and zzz(j) the jth. In vec(c), these correspond
to zzz(i−1)n, . . . zzzin−1 and zzz(j−1)n, . . . zzzjn−1, and the n by n submatrix at those row and column in-
dexes of cov(vec(c)) corresponds to the covariance of column i, j. But then we see that the covari-
ance of columns i+k, j +k, corresponding to the covariance of neurons zzz(i−1)n+k·n, . . . zzzin−1+k·n,
and zzz(j−1)n+k·n, . . . zzzjn−1+k·n, which corresponds to the 2-d shift (1, 0), applied to every neuron.
So by an identical argument to above, we see that for all 0 ≤ k ≤ n − 1

cov(zzz(i), zzz(j)) = cov(zzz(i+k), zzz(j+k))

In particular, cov(vec(c)) is block circulant.

An example cov(vec(c)) with c being 3 by 3 look like below:

(cid:35)

(cid:34)A0 A1 A2
A2 A0 A1
A1 A2 A0

where each Ai is itself a circulant matrix.

C.2 Proof of Lemma 2

Proof. This is a standard result, following from expressing a circulant matrix A in terms of its
diagonal form , i.e. A = V ΣV T with the columns of V being its eigenvectors. Noting that V = F ,
the DFT matrix, and that vectors of powers of ωk = exp( 2πik
n ) are orthogonal
gives the result.

n ), ωj = exp( 2πik

C.3 Proof of Theorem 2

Proof. Starting with (a), we need to show that cov(vec(DF T (ci)), vec(DF T (ci)) is diagonal. But
by the identity above, this becomes:

cov(vec(DF T (ci)), vec(DF T (ci)) = (F ⊗ F )vec(ci)vec(ci)T (F ⊗ F )∗

14

By Lemma 1, we see that

cov(vec(ci)) = vec(ci)vec(ci)T =







A0

A1
An−1 A0
...
A2

...
A1

. . . An−1
. . . An−2
. . .
. . .

...
A0







with each Ai circulant.

And so cov(vec(DF T (ci)), vec(DF T (ci)) becomes







f00F
f10F
...

. . .
. . .
. . .
fn−1,0F fn−1,1F . . .

f01F
f11F
...

f0,n−1F
f1,n−1F
...
fn−1,n−1F













A0

A1
An−1 A0
...
A2

...
A1

. . . An−1
. . . An−2
. . .
. . .

...
A0













f ∗
00F ∗
01F ∗
f ∗
...
0,n−1F ∗
f ∗

f ∗
10F ∗
11F ∗
f ∗
...
1,n−1F ∗
f ∗

. . .
. . .
. . .
. . .

f ∗
n−1,0F ∗
n−1,1F ∗
f ∗
...
n−1,n−1F ∗
f ∗







From this, we see that the sjth entry has the form

n−1
(cid:88)

(cid:32)n−1
(cid:88)

l=0

k=0

fskF Al−k

ljF ∗ =
f ∗

fskf ∗

ljF Al−kF ∗

(cid:33)

(cid:88)

k,l

Letting [F ArF ∗] denote the coefﬁcient of the term F ArF ∗, we see that (addition being mod n)

[F ArF ∗] =

fskf ∗

(k+r)j =

2πisk
n

e

· e

−2πij(k+r)
n

−2πijr
n

= e

2πik(s−j)
n

e

= e

−2πijr
n

· δsj

n−1
(cid:88)

k=0

(cid:88)

k

n−1
(cid:88)

k=0

with the last step following by the fact that the sum of powers of non trivial roots of unity are 0.

In particular, we see that only the diagonal entries (of the n by n matrix of matrices) are non zero.
The diagonal elements are linear combinations of terms of form F ArF ∗, and by Lemma 2 these are
diagonal. So the covariance of the DFT is diagonal as desired.

Part (b) follows almost identically to part (a), but by ﬁrst noting that exactly by the proof of Lemma
1, cov(ci, cj) is also a circulant and block circulant matrix.

C.4 Proof of Theorem 1

Proof. This Theorem now follows easily from the previous. Suppose we have a layer l, with chan-
nels c1, ..., ck. And let vec(DF T (ci)) have directions ˜zzz(i)
n2−1. By the previous theorem, we
k , ˜zzz(j)
know that the covariance of all of these neurons only has non-zero terms cov(˜zzz(i)
k .
So arranging the
0 , . . . ˜zzz(k)
0 , ˜zzz(1)
˜zzz(1)
diagonal of the matrix, proving the theorem.

row and column indexes being
the nonzero terms all live in the n2 k by k blocks down the

covariance matrix to have

full
. . . ˜zzz(k)
n2

0 , · · · ˜zzz(i)

0 , ˜zzz(1)

1

C.5 Computational Gains

As the covariance matrix is block diagonal, our more efﬁcient algorithm for computation is as fol-
lows: take the DFT of every channel (n log n due to FFT) and then compute covariances according
to blocks: partition the kn directions into the n2 k by k matrices that are non-zero, and compute the
covariance, inverses and square roots along these.
A rough computational budget for the covariance is therefore kn log n + n2k2.5, while the naive
computation would be of order (kn2)2.5, a polynomial difference. Furthermore, the DFT method
also makes for easy parallelization as each of the n2 blocks does not interact with any of the others.

15

Figure App.3: Learning dynamics per layer plots for conv (left pane) and res (right pane) nets trained on
CIFAR-10. Each line plots the SVCCA similarity of each layer with its ﬁnal representation, as a function of
training step, for both the conv (left pane) and res (right pane) nets. Note the bottom up convergence of different
layers

D Per Layer Learning Dynamics Plots from Section 4.1

E Additional Figure from Section 4.4

Figure App.4 compares the converged representations of two different initializations of the same
convolutional network on CIFAR-10.

Figure App.4: Comparing the converged representations of two different initializations of the same
convolutional architecture. The results support ﬁndings in [12], where initial and ﬁnal layers are
found to be similar, with middle layers differing in representation similarity.

F Experiment from Section 4.4

G Learning Dynamics for an LSTM

16

Figure App.5: Using SVCCA to perform model compression on the fully connected layers in a CIFAR-
10 convnet. The two gray lines indicate the original train (top) and test (bottom) accuracy. The two sets of
representations for SVCCA are obtained through 1) two different initialization and training of convnets on
CIFAR-10 2) the layer activations and the activations of the logits. The latter provides better results, with the
ﬁnal ﬁve layers: pool1, fc1, bn3, fc2 and bn4 all being compressed to 0.35 of their original size.

Figure App.6: Learning dynamics of the different layers of a stacked LSTM trained on the Penn Tree
Bank language modeling task. We observe a similar pattern to that of convolutional architectures
trained on image data: lower layer converge faster than upper layers.

17

7
1
0
2
 
v
o
N
 
8
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
0
8
5
0
.
6
0
7
1
:
v
i
X
r
a

SVCCA: Singular Vector Canonical Correlation
Analysis for Deep Learning Dynamics and
Interpretability

Maithra Raghu,1,2 Justin Gilmer,1 Jason Yosinski,3 & Jascha Sohl-Dickstein1
1Google Brain 2Cornell University 3Uber AI Labs
maithrar gmail com, gilmer google com, yosinski uber com, jaschasd google com

Abstract

We propose a new technique, Singular Vector Canonical Correlation Analysis
(SVCCA), a tool for quickly comparing two representations in a way that is both
invariant to afﬁne transform (allowing comparison between different layers and
networks) and fast to compute (allowing more comparisons to be calculated than
with previous methods). We deploy this tool to measure the intrinsic dimension-
ality of layers, showing in some cases needless over-parameterization; to probe
learning dynamics throughout training, ﬁnding that networks converge to ﬁnal
representations from the bottom up; to show where class-speciﬁc information in
networks is formed; and to suggest new training regimes that simultaneously save
computation and overﬁt less.

1

Introduction

As the empirical success of deep neural networks ([7, 9, 18]) become an indisputable fact, the goal
of better understanding these models escalates in importance. Central to this aim is a core issue
of deciphering learned representations. Facets of this key question have been explored empirically,
particularly for image models, in [1, 2, 10, 12, 13, 14, 15, 19, 20]. Most of these approaches are
motivated by interpretability of learned representations. More recently, [11] studied the similarities
of representations learned by multiple networks by ﬁnding permutations of neurons with maximal
correlation.

In this work we introduce a new approach to the study of network representations, based on an
analysis of each neuron’s activation vector – the scalar outputs it emits on input datapoints. With
this interpretation of neurons as vectors (and layers as subspaces, spanned by neurons), we intro-
duce SVCCA, Singular Vector Canonical Correlation Analysis, an amalgamation of Singular Value
Decomposition and Canonical Correlation Analysis (CCA) [5], as a powerful method for analyzing
deep representations. Although CCA has not previously been used to compare deep representations,
it has been used for related tasks such as computing the similarity between modeled and measured
brain activity [16], and training multi-lingual word embeddings in language models [3].

The main contributions resulting from the introduction of SVCCA are the following:

1. We ask: is the dimensionality of a layer’s learned representation the same as the number
of neurons in the layer? Answer: No. We show that trained networks perform equally well
with a number of directions just a fraction of the number of neurons with no additional
training, provided they are carefully chosen with SVCCA (Section 2.1). We explore the
consequences for model compression (Section 4.4).

2. We ask: what do deep representation learning dynamics look like? Answer: Networks
broadly converge bottom up. Using SVCCA, we compare layers across time and ﬁnd they

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

Figure 1: To demonstrate SVCCA, we consider a toy regression task (regression target as in Figure 3). (a)
We train two networks with four fully connected hidden layers starting from different random initializations,
and examine the representation learned by the penultimate (shaded) layer in each network. (b) The neurons
with the highest activations in net 1 (maroon) and in net 2 (green). The x-axis indexes over the dataset: in
our formulation, the representation of a neuron is simply its value over a dataset (Section 2). (c) The SVD
directions — i.e. the directions of maximal variance — for each network. (d) The top SVCCA directions. We
see that each pair of maroon/green lines (starting from the top) are almost visually identical (up to a sign). Thus,
although looking at just neurons (b) seems to indicate that the networks learn very different representations,
looking at the SVCCA subspace (d) shows that the information in the representations are (up to a sign) nearly
identical.

solidify from the bottom up. This suggests a simple, computationally more efﬁcient method
of training networks, Freeze Training, where lower layers are sequentially frozen after a
certain number of timesteps (Sections 4.1, 4.2).

3. We develop a method based on the discrete Fourier transform which greatly speeds up the

application of SVCCA to convolutional neural networks (Section 3).

4. We also explore an interpretability question, of when an architecture becomes sensitive to
different classes. We ﬁnd that SVCCA captures the semantics of different classes, with
similar classes having similar sensitivities, and vice versa. (Section 4.3).

Experimental Details Most of our experiments are performed on CIFAR-10 (augmented with
random translations). The main architectures we use are a convolutional network and a residual
network1. To produce a few ﬁgures, we also use a toy regression task: training a four hidden layer
fully connected network with 1D input and 4D output, to regress on four different simple functions.

2 Measuring Representations in Neural Networks

Our goal in this paper is to analyze and interpret the representations learned by neural networks. The
critical question from which our investigation departs is: how should we deﬁne the representation
of a neuron? Consider that a neuron at a particular layer in a network computes a real-valued
function over the network’s input domain. In other words, if we had a lookup table of all possible
input → output mappings for a neuron, it would be a complete portrayal of that neuron’s functional
form.

However, such inﬁnite tables are not only practically infeasible, but are also problematic to process
into a set of conclusions. Our primary interest is not in the neuron’s response to random data, but
rather in how it represents features of a speciﬁc dataset (e.g. natural images). Therefore, in this
study we take a neuron’s representation to be its set of responses over a ﬁnite set of inputs — those
drawn from some training or validation set.
More concretely, for a given dataset X = {x1, · · · xm} and a neuron i on layer l, zzzl
be the vector of outputs on X, i.e.

i, we deﬁne zzzl

i to

i(x1), · · · , zzzl
1Convnet layers: conv-conv-bn-pool-conv-conv-conv-bn-pool-fc-bn-fc-bn-out. Resnet layers:

i = (zzzl
zzzl

i(xm))

conv-(x10 c/bn/r block)-(x10 c/bn/r block)-(x10 c/bn/r block)-bn-fc-out.

2

Note that this is a different vector from the often-considered vector of the “representation at a layer
of a single input.” Here zzzl
i is a single neuron’s response over the entire dataset, not an entire layer’s
response for a single input. In this view, a neuron’s representation can be thought of as a single
vector in a high-dimensional space. Broadening our view from a single neuron to the collection of
neurons in a layer, the layer can be thought of as the set of neuron vectors contained within that
layer. This set of vectors will span some subspace. To summarize:

Considered over a dataset X with m examples, a neuron is a vector in Rm.
A layer is the subspace of Rm spanned by its neurons’ vectors.

Within this formalism, we introduce Singular Vector Canonical Correlation Analysis (SVCCA) as
a method for analysing representations. SVCCA proceeds as follows:

• Input: SVCCA takes as input two (not necessarily different) sets of neurons (typically

layers of a network) l1 = {zzzl1

1 , ..., zzzl1
m1

} and l2 = {zzzl2

1 , ..., zzzl2
m2

}

1 ⊂ l1, l(cid:48)

• Step 1 First SVCCA performs a singular value decomposition of each subspace to get sub-
subspaces l(cid:48)
2 ⊂ l2 which comprise of the most important directions of the original
subspaces l1, l2. In general we take enough directions to explain 99% of variance in the
subspace. This is especially important in neural network representations, where as we will
show many low variance directions (neurons) are primarily noise.

1, l(cid:48)

• Step 2 Second, compute the Canonical Correlation similarity ([5]) of l(cid:48)

2: linearly trans-
2 to be as aligned as possible and compute correlation coefﬁcients. In particu-
1 , ..., zzz(cid:48)l2
}, l(cid:48)
}, CCA linearly
m(cid:48)
2
2 such as to maximize the correlations

form l(cid:48)
lar, given the output of step 1, l(cid:48)
transforms these subspaces ˜l1 = WX l(cid:48)
corrs = {ρ1, . . . ρmin(m(cid:48)

1 , ..., zzz(cid:48)l1
m(cid:48)
1
1, ˜l2 = WY l(cid:48)

2)} between the transformed subspaces.

2 = {zzz(cid:48)l2

1 = {zzz(cid:48)l1

1, l(cid:48)

1,m(cid:48)

• Output: With these steps, SVCCA outputs pairs of aligned directions, (˜zzzl1

i ) and how
well they correlate, ρi. Step 1 also produces intermediate output in the form of the top
singular values and directions.

i , ˜zzzl2

For a more detailed description of each step, see the Appendix. SVCCA can be used to analyse
any two sets of neurons. In our experiments, we utilize this ﬂexibility to compare representations
across different random initializations, architectures, timesteps during training, and speciﬁc classes
and layers.

Figure 1 shows a simple, intuitive demonstration of SVCCA. We train a small network on a toy
regression task and show each step of SVCCA, along with the resulting very similar representations.
SVCCA is able to ﬁnd hidden similarities in the representations.

2.1 Distributed Representations

An important property of SVCCA is that it is truly a subspace method: both SVD and CCA work
with span(zzz1, . . . , zzzm) instead of being axis aligned to the zzzi directions. SVD ﬁnds singular vectors
i = (cid:80)m
zzz(cid:48)
j=1 sijzzzj, and the subsequent CCA ﬁnds a linear transform W , giving orthogonal canon-
ically correlated directions {˜zzz1, . . . , ˜zzzm} = {(cid:80)m
j=1 w1jzzz(cid:48)
In other words,
SVCCA has no preference for representations that are neuron (axes) aligned.

j, . . . , (cid:80)m

j=1 wmjzzz(cid:48)

j}.

If representations are distributed across many dimensions, then this is a desirable property of a
representation analysis method. Previous studies have reported that representations may be more
complex than either fully distributed or axis-aligned [17, 21, 11] but this question remains open.

We use SVCCA as a tool to probe the nature of representations via two experiments:

(a) We ﬁnd that the subspace directions found by SVCCA are disproportionately important to

the representation learned by a layer, relative to neuron-aligned directions.

(b) We show that at least some of these directions are distributed across many neurons.

Experiments for (a), (b) are shown in Figure 2 as (a), (b) respectively. For both experiments, we ﬁrst
acquire two different representations, l1, l2, for a layer l by training two different random initializa-
tions of a convolutional network on CIFAR-10. We then apply SVCCA to l1 and l2 to get directions

3

(a)

(b)

Figure 2: Demonstration of (a) disproportionate importance of SVCCA directions, and (b) distributed nature
of some of these directions. For both panes, we ﬁrst ﬁnd the top k SVCCA directions by training two conv nets
on CIFAR-10 and comparing corresponding layers. (a) We project the output of the top three layers, pool1, fc1,
fc2, onto this top-k subspace. We see accuracy rises rapidly with increasing k, with even k (cid:28) num neurons
giving reasonable performance, with no retraining. Baselines of random k neuron subspaces and max activation
neurons require larger k to perform as well. (b): after projecting onto top k subspace (like left), dotted lines
then project again onto m neurons, chosen to correspond highly to the top k-SVCCA subspace. Many more
neurons are needed than k for better performance, suggesting distributedness of SVCCA directions.

1 , ..., ˜zzzl1

m} and {˜zzzl2

{˜zzzl1
1 , ..., ˜zzzl2
linear combination of the original neurons, i.e. ˜zzzli

m}, ordered according to importance by SVCCA, with each ˜zzzli
r=1 α(li)

j = (cid:80)m

jr zzzli
r .

j being a

For different values of k < m, we can then restrict layer li’s output to lie in the subspace of
span(˜zzzli
k ), the most useful k-dimensional subspace as found by SVCCA, done by projecting
each neuron into this k dimensional space.

1 , . . . , ˜zzzli

We ﬁnd — somewhat surprisingly — that very few SVCCA directions are required for the network
to perform the task well. As shown in Figure 2(a), for a network trained on CIFAR-10, the ﬁrst
25 dimensions provide nearly the same accuracy as using all 512 dimensions of a fully connected
layer with 512 neurons. The accuracy curve rises rapidly with the ﬁrst few SVCCA directions, and
plateaus quickly afterwards, for k (cid:28) m. This suggests that the useful information contained in m
neurons is well summarized by the subspace formed by the top k SVCCA directions. Two base-
lines for comparison are picking random and maximum activation neuron aligned subspaces and
projecting outputs onto these. Both of these baselines require far more directions (in this case: neu-
rons) before matching the accuracy achieved by the SVCCA directions. These results also suggest
approaches to model compression, which are explored in more detail in Section 4.4.

Figure 2(b) next demonstrates that these useful SVCCA directions are at least somewhat distributed
over neurons rather than axis-aligned. First, the top k SVCCA directions are picked and the rep-
resentation is projected onto this subspace. Next, the representation is further projected onto m
neurons, where the m are chosen as those most important to the SVCCA directions . The resulting
accuracy is plotted for different choices of k (given by x-axis) and different choices of m (different
lines). That, for example, keeping even 100 fc1 neurons (dashed green line) cannot maintain the
accuracy of the ﬁrst 20 SVCCA directions (solid green line at x-axis 20) suggests that those 20
SVCCA directions are distributed across 5 or more neurons each, on average. Figure 3 shows a
further demonstration of the effect on the output of projecting onto top SVCCA directions, here for
the toy regression case.

Why the two step SV + CCA method is needed. Both SVD and CCA have important properties
for analysing network representations and SVCCA consequently beneﬁts greatly from being a two
step method. CCA is invariant to afﬁne transformations, enabling comparisons without natural
alignment (e.g. different architectures, Section 4.4). See Appendix B for proofs and a demonstrative
ﬁgure. While CCA is a powerful method, it also suffers from certain shortcomings, particularly in
determining how many directions were important to the original space X, which is the strength of

4

Figure 3: The effect on the output of a latent representation being projected onto top SVCCA directions in
the toy regression task. Representations of the penultimate layer are projected onto 2, 6, 15, 30 top SVCCA
directions (from second pane). By 30, the output looks very similar to the full 200 neuron output (left).

SVD. See Appendix for an example where naive CCA performs badly. Both the SVD and CCA
steps are critical to the analysis of learning dynamics in Section 4.1.

3 Scaling SVCCA for Convolutional Layers

Applying SVCCA to convolutional layers can be done in two natural ways:

(1) Same layer comparisons: If X, Y are the same layer (at different timesteps or across ran-
dom initializations) receiving the same input we can concatenate along the pixel (height h,
width w) coordinates to form a vector: a conv layer h × w × c maps to c vectors, each
of dimension hwd, where d is the number of datapoints. This is a natural choice because
neurons at different pixel coordinates see different image data patches to each other. When
X, Y are two versions of the same layer, these c different views correspond perfectly.

(2) Different layer comparisons: When X, Y are not the same layer, the image patches seen by
different neurons have no natural correspondence. But we can ﬂatten an h×w ×c conv into
hwc neurons, each of dimension d. This approach is valid for convs in different networks
or at different depths.

3.1 Scaling SVCCA with Discrete Fourier Transforms

Applying SVCCA to convolutions introduces a computational challenge: the number of neurons
(h×w×c) in convolutional layers, especially early ones, is very large, making SVCCA prohibitively
expensive due to the large matrices involved. Luckily the problem of approximate dimensionality
reduction of large matrices is well studied, and efﬁcient algorithms exist, e.g. [4].

For convolutional layers however, we can avoid dimensionality reduction and perform exact
SVCCA, even for large networks. This is achieved by preprocessing each channel with a Discrete
Fourier Transform (which preserves CCA due to invariances, see Appendix), causing all (covari-
ance) matrices to be block-diagonal. This allows all matrix operations to be performed block by
block, and only over the diagonal blocks, vastly reducing computation. We show:

Theorem 1. Suppose we have a translation invariant (image) dataset X and convolutional layers
l1, l2. Letting DF T (li) denote the discrete fourier transform applied to each channel of li, the
covariance cov(DF T (l1), DF T (l2)) is block diagonal, with blocks of size c × c.

We make only two assumptions: 1) all layers below l1, l2 are either conv or pooling layers with
circular boundary conditions (translation equivariance) 2) The dataset X has all translations of the
images Xi. This is necessary in the proof for certain symmetries in neuron activations, but these
symmetries typically exist in natural images even without translation invariance, as shown in Fig-
ure App.2 in the Appendix. Below are key statements, with proofs in Appendix.

Deﬁnition 1. Say a single channel image dataset X of images is translation invariant if for any
(wlog n × n) image Xi ∈ X, with pixel values {zzz11, ...zzznn}, X (a,b)
= {zzzσa(1)σb(1), ...zzzσa(n)σb(n)}
is also in X, for all 0 ≤ a, b ≤ n − 1, where σa(i) = a + i mod n (and similarly for b).

i

For a multiple channel image Xi, an (a, b) translation is an (a, b) height/width shift on every channel
separately. X is then translation invariant as above.

5

To prove Theorem 1, we ﬁrst show another theorem:
Theorem 2. Given a translation invariant dataset X, and a convolutional layer l with channels
{c1, . . . ck} applied to X

(a) the DFT of ci, F cF T has diagonal covariance matrix (with itself).
(b) the DFT of ci, cj, F ciF T , F cjF T have diagonal covariance with each other.
Finally, both of these theorems rely on properties of circulant matrices and their DFTs:
Lemma 1. The covariance matrix of ci applied to translation invariant X is circulant and block
circulant.
Lemma 2. The DFT of a circulant matrix is diagonal.

4 Applications of SVCCA

4.1 Learning Dynamics with SVCCA

We can use SVCCA as a window into learning dynamics by comparing the representation at a
layer at different points during training to its ﬁnal representation. Furthermore, as the SVCCA
computations are relatively cheap to compute compared to methods that require training an auxiliary
network for each comparison [1, 10, 11], we can compare all layers during training at all timesteps
to all layers at the ﬁnal time step, producing a rich view into the learning process.

The outputs of SVCCA are the aligned directions (˜xi, ˜yi), how well they align, ρi, as well as in-
termediate output from the ﬁrst step, of singular values and directions, λ(i)
Y , y(cid:48)(j). We
condense these outputs into a single value, the SVCCA similarity ¯ρ, that encapsulates how well the
representations of two layers are aligned with each other,

X , x(cid:48)(i), λ(j)

¯ρ =

1
min (m1, m2)

(cid:88)

ρi,

i

(1)

where min (m1, m2) is the size of the smaller of the two layers being compared. The SVCCA
similarity ¯ρ is the average correlation across aligned directions, and is a direct multidimensional
analogue of Pearson correlation.

The SVCCA similarity for all pairs of layers, and all time steps, is shown in Figure 4 for a convnet
and a resnet architecture trained on CIFAR10.

4.2 Freeze Training

Observing in Figure 4 that networks broadly converge from the bottom up, we propose a training
method where we successively freeze lower layers during training, only updating higher and higher
layers, saving all computation needed for deriving gradients and updating in lower layers.

We apply this method to convolutional and residual networks trained on CIFAR-10, Figure 5, using
a linear freezing regime: in the convolutional network, each layer is frozen at a fraction (layer num-
ber/total layers) of total training time, while for resnets, each residual block is frozen at a fraction
(block number/total blocks). The vertical grey dotted lines show which steps have another set of lay-
ers frozen. Aside from saving computation, Freeze Training appears to actively help generalization
accuracy, like early stopping but with different layers requiring different stopping points.

4.3

Interpreting Representations: when are classes learned?

We also can use SVCCA to compare how correlated representations in each layer are with the logits
of each class in order to measure how knowledge about the target evolves throughout the network.
In Figure 6 we apply the DFT CCA technique on the Imagenet Resnet [6]. We take ﬁve different
classes and for different layers in the network, compute the DFT CCA similarity between the logit
of that class and the network layer. The results successfully reﬂect semantic aspects of the classes:
the ﬁretruck class sensitivity line is clearly distinct from the two pairs of dog breeds, and network
develops greater sensitivity to ﬁretruck earlier on. The two pairs of dog breeds, purposefully chosen
so that each pair is similar to the other in appearance, have cca similarity lines that are very close to
each other through the network, indicating these classes are similar to each other.

6

Figure 4: Learning dynamics plots for conv (top) and res (bottom) nets trained on CIFAR-10. Each pane is
a matrix of size layers × layers, with each entry showing the SVCCA similarity ¯ρ between the two layers.
Note that learning broadly happens ‘bottom up’ – layers closer to the input seem to solidify into their ﬁnal
representations with the exception of the very top layers. Per layer plots are included in the Appendix. Other
patterns are also visible – batch norm layers maintain nearly perfect similarity to the layer preceding them due
to scaling invariance (with a slight reduction since batch norm changes the SVD directions which capture 99%
of the variance). In the resnet plot, we see a stripe like pattern due to skip connections inducing high similarities
to previous layers.

Figure 5: Freeze Training reduces training cost and improves generalization. We apply Freeze Training to a
convolutional network on CIFAR-10 and a residual network on CIFAR-10. As shown by the grey dotted lines
(which indicate the timestep at which another layer is frozen), both networks have a ‘linear’ freezing regime:
for the convolutional network, we freeze individual layers at evenly spaced timesteps throughout training. For
the residual network, we freeze entire residual blocks at each freeze step. The curves were averaged over ten
runs.

4.4 Other Applications: Cross Model Comparison and compression

SVCCA similarity can also be used to compare the similarity of representations across different
random initializations, and even different architectures. We compare convolutional networks on
CIFAR-10 across random initializations (Appendix) and also a convolutional network to a residual
network in Figure 7, using the DFT method described in 3.

In Figure 3, we saw that projecting onto the subspace of the top few SVCCA directions resulted in
comparable accuracy. This observations motivates an approach to model compression. In particular,
letting the output vector of layer l be xxx(l) ∈ Rn×1, and the weights W (l), we replace the usual
W (l)xxx(l) with (W (l)P T
x )(Pxxxx(l)) where Px is a k × n projection matrix, projecting xxx onto the top
SVCCA directions. This bottleneck reduces both parameter count and inference computational cost

7

Figure 6: We plot the CCA similarity using the Discrete Fourier Transform between the logits of ﬁve classes
and layers in the Imagenet Resnet. The classes are ﬁretruck and two pairs of dog breeds (terriers and husky
like dogs: husky and eskimo dog) that are chosen to be similar to each other. These semantic properties are
captured in CCA similarity, where we see that the line corresponding to ﬁretruck is clearly distinct from the
two pairs of dog breeds, and the two lines in each pair are both very close to each other, reﬂecting the fact that
each pair consists of visually similar looking images. Firetruck also appears to be easier for the network to
learn, with greater sensitivity displayed much sooner.

Figure 7: We plot the CCA similarity using the Discrete Fourier Transform between convolutional layers of a
Resnet and Convnet trained on CIFAR-10. We ﬁnd that the lower layrs of both models are noticeably similar to
each other, and get progressively less similar as we compare higher layers. Note that the highest layers of the
resnet are least similar to the lower layers of the convnet.

for the layer by a factor ∼ k
n . In Figure App.5 in the Appendix, we show that we can consecutively
compress top layers with SVCCA by a signiﬁcant amount (in one case reducing each layer to 0.35
original size) and hardly affect performance.

5 Conclusion

In this paper we present SVCCA, a general method which allows for comparison of the learned dis-
tributed representations between different neural network layers and architectures. Using SVCCA
we obtain novel insights into the learning dynamics and learned representations of common neural
network architectures. These insights motivated a new Freeze Training technique which can reduce
the number of ﬂops required to train networks and potentially even increase generalization perfor-
mance. We observe that CCA similarity can be a helpful tool for interpretability, with sensitivity
to different classes reﬂecting their semantic properties. This technique also motivates a new algo-
rithm for model compression. Finally, the “lower layers learn ﬁrst” behavior was also observed for
recurrent neural networks as shown in Figure App.6 in the Appendix.

8

References

[1] Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classiﬁer

probes. arXiv preprint arXiv:1610.01644, 2016.

[2] David Eigen, Jason Rolfe, Rob Fergus, and Yann LeCun. Understanding deep architectures

using a recursive convolutional network. arXiv preprint arXiv:1312.1847, 2013.

[3] Manaal Faruqui and Chris Dyer. Improving vector space word representations using multilin-

gual correlation. Association for Computational Linguistics, 2014.

[4] Nathan Halko, Martinsson Per-Gunnar, and Joel A. Tropp. Finding structure with random-
ness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM
Rev., 53:217–288, 2011.

[5] D. R. Hardoon, S. Szedmak, and J. Shawe-Taylor. Canonical correlation analysis: An overview

with application to learning methods. Neural Computation, 16:2639–2664, 2004.

[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image

recognition. CoRR, abs/1512.03385, 2015.

[7] Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel-rahman Mohamed, Navdeep
Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neu-
ral networks for acoustic modeling in speech recognition: The shared views of four research
groups. IEEE Signal Processing Magazine, 29(6):82–97, 2012.

[8] Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 1985.
[9] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems, pages
1097–1105, 2012.

[10] Karel Lenc and Andrea Vedaldi. Understanding image representations by measuring their
equivariance and equivalence. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 991–999, 2015.

[11] Y. Li, J. Yosinski, J. Clune, H. Lipson, and J. Hopcroft. Convergent Learning: Do different
In International Conference on Learning

neural networks learn the same representations?
Representations (ICLR), May 2016.

[12] Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. Convergent learning:
Do different neural networks learn the same representations? In Feature Extraction: Modern
Questions and Challenges, pages 196–212, 2015.

[13] Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by in-
verting them. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-
nition, pages 5188–5196, 2015.

[14] Gr´egoire Montavon, Mikio L Braun, and Klaus-Robert M¨uller. Kernel analysis of deep net-

works. Journal of Machine Learning Research, 12(Sep):2563–2581, 2011.

[15] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional
arXiv preprint

networks: Visualising image classiﬁcation models and saliency maps.
arXiv:1312.6034, 2013.

[16] David Sussillo, Mark M Churchland, Matthew T Kaufman, and Krishna V Shenoy. A neural
network that ﬁnds a naturalistic solution for the production of muscle activity. Nature neuro-
science, 18(7):1025–1033, 2015.

[17] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
arXiv preprint

Intriguing properties of neural networks.

Goodfellow, and Rob Fergus.
arXiv:1312.6199, 2013.

[18] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural ma-
chine translation system: Bridging the gap between human and machine translation. arXiv
preprint arXiv:1609.08144, 2016.

[19] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding
neural networks through deep visualization. In Deep Learning Workshop, International Con-
ference on Machine Learning (ICML), 2015.

9

[20] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In

European conference on computer vision, pages 818–833. Springer, 2014.

[21] Bolei Zhou, Aditya Khosla, `Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object de-
tectors emerge in deep scene cnns. In International Conference on Learning Representations
(ICLR), volume abs/1412.6856, 2014.

10

Appendix

A Mathematical details of CCA and SVCCA

Canonical Correlation of X, Y
ﬁnding a, b to maximise:

Finding maximal correlations between X, Y can be expressed as

aT ΣXY b

(cid:112)

(cid:112)

aT ΣXX a

bT ΣY Y b

where ΣXX , ΣXY , ΣY X , ΣY Y are the covariance and cross-covariance terms. By performing the
change of basis ˜x˜x˜x1 = Σ1/2
Y Y b and using Cauchy-Schwarz we recover an eigenvalue
problem:

xx a and ˜y˜y˜y1 = Σ1/2

˜x˜x˜x1 = argmax

(cid:34)

xT Σ−1/2

XX ΣXY Σ−1
||x||

Y Y ΣY X Σ−1/2
XX x

(cid:35)

(*)

SVCCA Given two subspaces X = {xxx1, ..., xxxm1 }, Y = {yyy1, ..., yyym2}, SVCCA ﬁrst performs a
singular value decomposition on X, Y . This results in singular vectors {x(cid:48)x(cid:48)x(cid:48)
m1 } with associ-
ated singular values {λ1, ..., λm1} (for X, and similarly for Y ). Of these m1 singular vectors, we
keep the top m(cid:48)
i=1 |λi|). That is, 99% of
the variation of X is explainable by the top m(cid:48)
1 vectors. This helps remove directions/neurons that
are constant zero, or noise with small magnitude.
Then, we apply Canonical Correlation Analysis (CCA) to the sets {x(cid:48)x(cid:48)x(cid:48)
top singular vectors.

1 is the smallest value that (cid:80)m(cid:48)

i=1 |λi|(≥ 0.99 (cid:80)m1

1 where m(cid:48)

1, ..., y(cid:48)y(cid:48)y(cid:48)

1, ..., x(cid:48)x(cid:48)x(cid:48)

1, ..., x(cid:48)x(cid:48)x(cid:48)

}, {y(cid:48)y(cid:48)y(cid:48)

} of

m(cid:48)
2

m(cid:48)
1

1

CCA is a well established statistical method for understanding the similarity of two different sets
of random variables – given our two sets of vectors {x(cid:48)x(cid:48)x(cid:48)
}, we wish to ﬁnd
linear transformations, WX , WY that maximally correlate the subspaces. This can be reduced to
an eigenvalue problem. Solving this results in linearly transformed subspaces ˜X, ˜Y with directions
˜xxxi, ˜yyyi that are maximally correlated with each other, and orthogonal to ˜xxxj, ˜yyyj, j < i. We let ρi =
corr(˜xxxi, ˜yyyi). In summary, we have:

1, ..., y(cid:48)y(cid:48)y(cid:48)

1, ..., x(cid:48)x(cid:48)x(cid:48)

}, {y(cid:48)y(cid:48)y(cid:48)

m(cid:48)
2

m(cid:48)
1

SVCCA Summary

1. Input: X, Y
2. Perform: SVD(X), SVD(Y). Output: X (cid:48) = U X, Y (cid:48) = V Y
3. Perform CCA(X (cid:48), Y (cid:48)). Output:

{ρ1, . . . ρmin(m1,m2)}

˜X = WX X (cid:48), ˜Y = WY Y (cid:48) and corrs =

B Additional Proofs and Figures from Section 2.1

Proof of Orthonormal and Scaling Invariance of CCA:

We can see this using equation (*) as follows: suppose U, V are orthonormal transforms applied to
the sets X, Y . Then it follows that Σa
XX becomes U Σa
XX U T , for a = {1, −1, 1/2, −1/2}, and
similarly for Y and V . Also note ΣXY becomes U ΣXY V T . Equation (*) then becomes
xT U Σ−1/2

(cid:34)

(cid:35)

˜x1 = argmax

Y Y ΣY X Σ−1/2
XX ΣXY Σ−1
||x||

XX U T x

So if ˜u is a solution to equation (*), then U ˜u is a solution to the equation above, which results in the
same correlation coefﬁcients.

B.0.1 The importance of SVD: how many directions matter?

While CCA is excellent at identifying useful learned directions that correlate, independent of certain
common transforms, it doesn’t capture the full picture entirely. Consider the following setting:

11

Figure App.1: This ﬁgure shows the ability of CCA to deal with orthogonal and scaling transforms.
In the ﬁrst pane, the maroon plot shows one of the highest activation neurons in the penultimate
layer of a network trained on CIFAR-10, with the x-axis being (ordered) image ids and the y-axis
being activation on that image. The green plots show two resulting distorted directions after this and
two of the other top activation neurons are permuted, rotated and scaled. Pane two shows the result
of applying CCA to the distorted directions and the original signal, which succeeds in recovering
the original signal.

suppose we have subspaces A, B, C, with A being 50 dimensions, B being 200 dimensions, 50 of
which are perfectly aligned with A and the other 150 being noise, and C being 200 dimensions, 50
of which are aligned with A (and B) and the other 150 being useful, but different directions.

Then looking at the canonical correlation coefﬁcients of (A, B) and (A, C) will give the same result,
both being 1 for 50 values and 0 for everything else. But these are two very different cases – the
subspace B is indeed well represented by the 50 directions that are aligned with A. But the subspace
C has 150 more useful directions.

This distinction becomes particularly important when aggregating canonical correlation coefﬁcients
as a measure of similarity, as used in analysing network learning dynamics. However, by ﬁrst ap-
plying SVD to determine the number of directions needed to explain 99% of the observed variance,
we can distinguish between pathological cases like the one above.

C Proof of Theorem 1

Here we provide the proofs for Lemma 1, Lemma 2, Theorem 2 and ﬁnally Theorem 1.

A preliminary note before we begin:

When we consider a (wlog) n by n channel c of a convolutional layer, we assume it has shape







zzz0,0
zzz1,0
...
zzzn−1,0

zzz1,2
zzz2,2
...
zzzn−1,1

. . .
. . .
. . .
. . .







zzz0,n−1
zzz1,n−1
...
zzzn−1,n−1

12

(a)

(b)

(c)

(d)

Figure App.2:
This ﬁgure visualizes the covariance matrix of one of the channels of a resnet
trained on Imagenet. Black correspond to large values and white to small values. (a) we compute the
covariance without a translation invariant dataset and without ﬁrst preprocessing the images by DFT.
We see that the covariance matrix is dense. (b) We compute the covariance after applying DFT, but
without augmenting the dataset with translations. Even without enforcing translation invariance, we
see that the covariance in the DFT basis is approximately diagonal. (c) Same as (a), but the dataset
is augmented to be fully translation invariant. The covariance in the pixel basis is still dense. (d)
Same as (c), but with dataset augmented to be translation invariant. The covariance matrix is exactly
diagonal for a translation invariant dataset in a DFT basis.

When computing the covariance matrix however, we vectorize c by stacking the columns under each
other, and call the result vec(c):

vec(c) =

:=














zzz0,0
zzz1,0
...
zzzn−1,0
zzz0,1
...
zzzn−1,n−1








































zzz0
zzz1
...
zzzn−1
zzzn
...
zzzn2−1

vec(AcB) = (BT ⊗ A)vec(c)

One useful identity when switching between these two notations (see e.g. [8]) is

where A, B are matrices and ⊗ is the Kronecker product. A useful observation arising from this is:
Lemma 3. The CCA vectors of DF T (ci), DF T (cj) are the same (up to a rotation by F ) as the
CCA of ci, cj.

Proof: From Section B we know that unitary transforms only rotate the CCA directions. But while
DFT pre and postmultiplies by F, F T – unitary matrices, we cannot directly apply this as the result
is for unitary transforms on vec(ci). But, using the identity above, we see that vec(DF T (ci)) =
vec(F ciF T ) = (F ⊗ F )vec(ci), which is unitary as F is unitary. Applying the same identity to cj,
we can thus conclude that the DFT preserves CCA (up to rotations).

As Theorem 1 preprocesses the neurons with DFT, it is important to note that by the Lemma above,
we do not change the CCA vectors (except by a rotation).

C.1 Proof of Lemma 1

Proof. Translation invariance is preserved We show inductively that any translation invariant input
to a convolutional channel results in a translation invariant output: Suppose the input to channel c,
(n by n) is translation invariant. It is sufﬁcient to show that for inputs Xi, Xj and 0 ≤ a, b, ≤ n − 1,
c(Xi) + (a, b) mod n = c(Xj). But an (a, b) shift in neuron coordinates in c corresponds to a
(height stride · a, width stride · b) shift in the input. And as X is translation invariant, there is some
Xj = Xi + (height stride · a, width stride · b).

cov(c) is circulant:

13

Let X be (by proof above) a translation invariant input to a channel c in some convolution or pooling
layer. The empirical covariance, cov(c) is the n2 by n2 matrix computed by (assuming c is centered)

1
|X|

(cid:88)

Xi∈X

vec(c(Xi)) · vec(c(Xi))T

So, cov(c)ij = 1
j.

|X|zzzT

i zzzj = 1
|X|

(cid:80)

Xl∈X zzzT

i (Xl)zzzj(Xl), i.e. the inner products of the neurons i and

The indexes i and j refer to the neurons in their vectorized order in vec(c). But in the matrix ordering
of neurons in c, i and j correspond to some (a1, b1) and (a2, b2). If we applied a translation (a, b),
to both, we would get new neuron coordinates (a1 + a, b1 + b), (a2 + a, b2 + b) (all coordinates
mod n) which would correspond to i + an + b mod n2 and j + an + b mod n2, by our stacking
of columns and reindexing.

Let τa,b be the translation in inputs corresponding to an (a, b) translation in c, i.e.
τa,b =
(height stride·a, width stride·b). Then clearly zzz(a1,b1)(Xi) = zzz(a1+a,b1+b)(τ(a,b)(Xi), and similarly
for zzz(a2,b2)
It follows that 1

(a1+b,b1+b)zzz(a2+a,b2+b), or, with vec(c) indexing

(a1,b1)zzz(a2,b2) = 1

|X|zzzT

|X|zzzT

1
|X|

1
|X|

zzzT
i zzzj =

zzzT
(i+an+b mod n2)zzz(j+an+b mod n2)

This gives us the circulant structure of cov(c).

cov(c) is block circulant: Let zzz(i) be the ith column of c, and zzz(j) the jth. In vec(c), these correspond
to zzz(i−1)n, . . . zzzin−1 and zzz(j−1)n, . . . zzzjn−1, and the n by n submatrix at those row and column in-
dexes of cov(vec(c)) corresponds to the covariance of column i, j. But then we see that the covari-
ance of columns i+k, j +k, corresponding to the covariance of neurons zzz(i−1)n+k·n, . . . zzzin−1+k·n,
and zzz(j−1)n+k·n, . . . zzzjn−1+k·n, which corresponds to the 2-d shift (1, 0), applied to every neuron.
So by an identical argument to above, we see that for all 0 ≤ k ≤ n − 1

cov(zzz(i), zzz(j)) = cov(zzz(i+k), zzz(j+k))

In particular, cov(vec(c)) is block circulant.

An example cov(vec(c)) with c being 3 by 3 look like below:

(cid:35)

(cid:34)A0 A1 A2
A2 A0 A1
A1 A2 A0

where each Ai is itself a circulant matrix.

C.2 Proof of Lemma 2

Proof. This is a standard result, following from expressing a circulant matrix A in terms of its
diagonal form , i.e. A = V ΣV T with the columns of V being its eigenvectors. Noting that V = F ,
the DFT matrix, and that vectors of powers of ωk = exp( 2πik
n ) are orthogonal
gives the result.

n ), ωj = exp( 2πik

C.3 Proof of Theorem 2

Proof. Starting with (a), we need to show that cov(vec(DF T (ci)), vec(DF T (ci)) is diagonal. But
by the identity above, this becomes:

cov(vec(DF T (ci)), vec(DF T (ci)) = (F ⊗ F )vec(ci)vec(ci)T (F ⊗ F )∗

14

By Lemma 1, we see that

cov(vec(ci)) = vec(ci)vec(ci)T =







A0

A1
An−1 A0
...
A2

...
A1

. . . An−1
. . . An−2
. . .
. . .

...
A0







with each Ai circulant.

And so cov(vec(DF T (ci)), vec(DF T (ci)) becomes







f00F
f10F
...

. . .
. . .
. . .
fn−1,0F fn−1,1F . . .

f01F
f11F
...

f0,n−1F
f1,n−1F
...
fn−1,n−1F













A0

A1
An−1 A0
...
A2

...
A1

. . . An−1
. . . An−2
. . .
. . .

...
A0













f ∗
00F ∗
01F ∗
f ∗
...
0,n−1F ∗
f ∗

f ∗
10F ∗
11F ∗
f ∗
...
1,n−1F ∗
f ∗

. . .
. . .
. . .
. . .

f ∗
n−1,0F ∗
n−1,1F ∗
f ∗
...
n−1,n−1F ∗
f ∗







From this, we see that the sjth entry has the form

n−1
(cid:88)

(cid:32)n−1
(cid:88)

l=0

k=0

fskF Al−k

ljF ∗ =
f ∗

fskf ∗

ljF Al−kF ∗

(cid:33)

(cid:88)

k,l

Letting [F ArF ∗] denote the coefﬁcient of the term F ArF ∗, we see that (addition being mod n)

[F ArF ∗] =

fskf ∗

(k+r)j =

2πisk
n

e

· e

−2πij(k+r)
n

−2πijr
n

= e

2πik(s−j)
n

e

= e

−2πijr
n

· δsj

n−1
(cid:88)

k=0

(cid:88)

k

n−1
(cid:88)

k=0

with the last step following by the fact that the sum of powers of non trivial roots of unity are 0.

In particular, we see that only the diagonal entries (of the n by n matrix of matrices) are non zero.
The diagonal elements are linear combinations of terms of form F ArF ∗, and by Lemma 2 these are
diagonal. So the covariance of the DFT is diagonal as desired.

Part (b) follows almost identically to part (a), but by ﬁrst noting that exactly by the proof of Lemma
1, cov(ci, cj) is also a circulant and block circulant matrix.

C.4 Proof of Theorem 1

Proof. This Theorem now follows easily from the previous. Suppose we have a layer l, with chan-
nels c1, ..., ck. And let vec(DF T (ci)) have directions ˜zzz(i)
n2−1. By the previous theorem, we
k , ˜zzz(j)
know that the covariance of all of these neurons only has non-zero terms cov(˜zzz(i)
k .
So arranging the
0 , . . . ˜zzz(k)
0 , ˜zzz(1)
˜zzz(1)
diagonal of the matrix, proving the theorem.

row and column indexes being
the nonzero terms all live in the n2 k by k blocks down the

covariance matrix to have

full
. . . ˜zzz(k)
n2

0 , · · · ˜zzz(i)

0 , ˜zzz(1)

1

C.5 Computational Gains

As the covariance matrix is block diagonal, our more efﬁcient algorithm for computation is as fol-
lows: take the DFT of every channel (n log n due to FFT) and then compute covariances according
to blocks: partition the kn directions into the n2 k by k matrices that are non-zero, and compute the
covariance, inverses and square roots along these.
A rough computational budget for the covariance is therefore kn log n + n2k2.5, while the naive
computation would be of order (kn2)2.5, a polynomial difference. Furthermore, the DFT method
also makes for easy parallelization as each of the n2 blocks does not interact with any of the others.

15

Figure App.3: Learning dynamics per layer plots for conv (left pane) and res (right pane) nets trained on
CIFAR-10. Each line plots the SVCCA similarity of each layer with its ﬁnal representation, as a function of
training step, for both the conv (left pane) and res (right pane) nets. Note the bottom up convergence of different
layers

D Per Layer Learning Dynamics Plots from Section 4.1

E Additional Figure from Section 4.4

Figure App.4 compares the converged representations of two different initializations of the same
convolutional network on CIFAR-10.

Figure App.4: Comparing the converged representations of two different initializations of the same
convolutional architecture. The results support ﬁndings in [12], where initial and ﬁnal layers are
found to be similar, with middle layers differing in representation similarity.

F Experiment from Section 4.4

G Learning Dynamics for an LSTM

16

Figure App.5: Using SVCCA to perform model compression on the fully connected layers in a CIFAR-
10 convnet. The two gray lines indicate the original train (top) and test (bottom) accuracy. The two sets of
representations for SVCCA are obtained through 1) two different initialization and training of convnets on
CIFAR-10 2) the layer activations and the activations of the logits. The latter provides better results, with the
ﬁnal ﬁve layers: pool1, fc1, bn3, fc2 and bn4 all being compressed to 0.35 of their original size.

Figure App.6: Learning dynamics of the different layers of a stacked LSTM trained on the Penn Tree
Bank language modeling task. We observe a similar pattern to that of convolutional architectures
trained on image data: lower layer converge faster than upper layers.

17

7
1
0
2
 
v
o
N
 
8
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
0
8
5
0
.
6
0
7
1
:
v
i
X
r
a

SVCCA: Singular Vector Canonical Correlation
Analysis for Deep Learning Dynamics and
Interpretability

Maithra Raghu,1,2 Justin Gilmer,1 Jason Yosinski,3 & Jascha Sohl-Dickstein1
1Google Brain 2Cornell University 3Uber AI Labs
maithrar gmail com, gilmer google com, yosinski uber com, jaschasd google com

Abstract

We propose a new technique, Singular Vector Canonical Correlation Analysis
(SVCCA), a tool for quickly comparing two representations in a way that is both
invariant to afﬁne transform (allowing comparison between different layers and
networks) and fast to compute (allowing more comparisons to be calculated than
with previous methods). We deploy this tool to measure the intrinsic dimension-
ality of layers, showing in some cases needless over-parameterization; to probe
learning dynamics throughout training, ﬁnding that networks converge to ﬁnal
representations from the bottom up; to show where class-speciﬁc information in
networks is formed; and to suggest new training regimes that simultaneously save
computation and overﬁt less.

1

Introduction

As the empirical success of deep neural networks ([7, 9, 18]) become an indisputable fact, the goal
of better understanding these models escalates in importance. Central to this aim is a core issue
of deciphering learned representations. Facets of this key question have been explored empirically,
particularly for image models, in [1, 2, 10, 12, 13, 14, 15, 19, 20]. Most of these approaches are
motivated by interpretability of learned representations. More recently, [11] studied the similarities
of representations learned by multiple networks by ﬁnding permutations of neurons with maximal
correlation.

In this work we introduce a new approach to the study of network representations, based on an
analysis of each neuron’s activation vector – the scalar outputs it emits on input datapoints. With
this interpretation of neurons as vectors (and layers as subspaces, spanned by neurons), we intro-
duce SVCCA, Singular Vector Canonical Correlation Analysis, an amalgamation of Singular Value
Decomposition and Canonical Correlation Analysis (CCA) [5], as a powerful method for analyzing
deep representations. Although CCA has not previously been used to compare deep representations,
it has been used for related tasks such as computing the similarity between modeled and measured
brain activity [16], and training multi-lingual word embeddings in language models [3].

The main contributions resulting from the introduction of SVCCA are the following:

1. We ask: is the dimensionality of a layer’s learned representation the same as the number
of neurons in the layer? Answer: No. We show that trained networks perform equally well
with a number of directions just a fraction of the number of neurons with no additional
training, provided they are carefully chosen with SVCCA (Section 2.1). We explore the
consequences for model compression (Section 4.4).

2. We ask: what do deep representation learning dynamics look like? Answer: Networks
broadly converge bottom up. Using SVCCA, we compare layers across time and ﬁnd they

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

Figure 1: To demonstrate SVCCA, we consider a toy regression task (regression target as in Figure 3). (a)
We train two networks with four fully connected hidden layers starting from different random initializations,
and examine the representation learned by the penultimate (shaded) layer in each network. (b) The neurons
with the highest activations in net 1 (maroon) and in net 2 (green). The x-axis indexes over the dataset: in
our formulation, the representation of a neuron is simply its value over a dataset (Section 2). (c) The SVD
directions — i.e. the directions of maximal variance — for each network. (d) The top SVCCA directions. We
see that each pair of maroon/green lines (starting from the top) are almost visually identical (up to a sign). Thus,
although looking at just neurons (b) seems to indicate that the networks learn very different representations,
looking at the SVCCA subspace (d) shows that the information in the representations are (up to a sign) nearly
identical.

solidify from the bottom up. This suggests a simple, computationally more efﬁcient method
of training networks, Freeze Training, where lower layers are sequentially frozen after a
certain number of timesteps (Sections 4.1, 4.2).

3. We develop a method based on the discrete Fourier transform which greatly speeds up the

application of SVCCA to convolutional neural networks (Section 3).

4. We also explore an interpretability question, of when an architecture becomes sensitive to
different classes. We ﬁnd that SVCCA captures the semantics of different classes, with
similar classes having similar sensitivities, and vice versa. (Section 4.3).

Experimental Details Most of our experiments are performed on CIFAR-10 (augmented with
random translations). The main architectures we use are a convolutional network and a residual
network1. To produce a few ﬁgures, we also use a toy regression task: training a four hidden layer
fully connected network with 1D input and 4D output, to regress on four different simple functions.

2 Measuring Representations in Neural Networks

Our goal in this paper is to analyze and interpret the representations learned by neural networks. The
critical question from which our investigation departs is: how should we deﬁne the representation
of a neuron? Consider that a neuron at a particular layer in a network computes a real-valued
function over the network’s input domain. In other words, if we had a lookup table of all possible
input → output mappings for a neuron, it would be a complete portrayal of that neuron’s functional
form.

However, such inﬁnite tables are not only practically infeasible, but are also problematic to process
into a set of conclusions. Our primary interest is not in the neuron’s response to random data, but
rather in how it represents features of a speciﬁc dataset (e.g. natural images). Therefore, in this
study we take a neuron’s representation to be its set of responses over a ﬁnite set of inputs — those
drawn from some training or validation set.
More concretely, for a given dataset X = {x1, · · · xm} and a neuron i on layer l, zzzl
be the vector of outputs on X, i.e.

i, we deﬁne zzzl

i to

i(x1), · · · , zzzl
1Convnet layers: conv-conv-bn-pool-conv-conv-conv-bn-pool-fc-bn-fc-bn-out. Resnet layers:

i = (zzzl
zzzl

i(xm))

conv-(x10 c/bn/r block)-(x10 c/bn/r block)-(x10 c/bn/r block)-bn-fc-out.

2

Note that this is a different vector from the often-considered vector of the “representation at a layer
of a single input.” Here zzzl
i is a single neuron’s response over the entire dataset, not an entire layer’s
response for a single input. In this view, a neuron’s representation can be thought of as a single
vector in a high-dimensional space. Broadening our view from a single neuron to the collection of
neurons in a layer, the layer can be thought of as the set of neuron vectors contained within that
layer. This set of vectors will span some subspace. To summarize:

Considered over a dataset X with m examples, a neuron is a vector in Rm.
A layer is the subspace of Rm spanned by its neurons’ vectors.

Within this formalism, we introduce Singular Vector Canonical Correlation Analysis (SVCCA) as
a method for analysing representations. SVCCA proceeds as follows:

• Input: SVCCA takes as input two (not necessarily different) sets of neurons (typically

layers of a network) l1 = {zzzl1

1 , ..., zzzl1
m1

} and l2 = {zzzl2

1 , ..., zzzl2
m2

}

1 ⊂ l1, l(cid:48)

• Step 1 First SVCCA performs a singular value decomposition of each subspace to get sub-
subspaces l(cid:48)
2 ⊂ l2 which comprise of the most important directions of the original
subspaces l1, l2. In general we take enough directions to explain 99% of variance in the
subspace. This is especially important in neural network representations, where as we will
show many low variance directions (neurons) are primarily noise.

1, l(cid:48)

• Step 2 Second, compute the Canonical Correlation similarity ([5]) of l(cid:48)

2: linearly trans-
2 to be as aligned as possible and compute correlation coefﬁcients. In particu-
1 , ..., zzz(cid:48)l2
}, l(cid:48)
}, CCA linearly
m(cid:48)
2
2 such as to maximize the correlations

form l(cid:48)
lar, given the output of step 1, l(cid:48)
transforms these subspaces ˜l1 = WX l(cid:48)
corrs = {ρ1, . . . ρmin(m(cid:48)

1 , ..., zzz(cid:48)l1
m(cid:48)
1
1, ˜l2 = WY l(cid:48)

2)} between the transformed subspaces.

2 = {zzz(cid:48)l2

1 = {zzz(cid:48)l1

1, l(cid:48)

1,m(cid:48)

• Output: With these steps, SVCCA outputs pairs of aligned directions, (˜zzzl1

i ) and how
well they correlate, ρi. Step 1 also produces intermediate output in the form of the top
singular values and directions.

i , ˜zzzl2

For a more detailed description of each step, see the Appendix. SVCCA can be used to analyse
any two sets of neurons. In our experiments, we utilize this ﬂexibility to compare representations
across different random initializations, architectures, timesteps during training, and speciﬁc classes
and layers.

Figure 1 shows a simple, intuitive demonstration of SVCCA. We train a small network on a toy
regression task and show each step of SVCCA, along with the resulting very similar representations.
SVCCA is able to ﬁnd hidden similarities in the representations.

2.1 Distributed Representations

An important property of SVCCA is that it is truly a subspace method: both SVD and CCA work
with span(zzz1, . . . , zzzm) instead of being axis aligned to the zzzi directions. SVD ﬁnds singular vectors
i = (cid:80)m
zzz(cid:48)
j=1 sijzzzj, and the subsequent CCA ﬁnds a linear transform W , giving orthogonal canon-
ically correlated directions {˜zzz1, . . . , ˜zzzm} = {(cid:80)m
j=1 w1jzzz(cid:48)
In other words,
SVCCA has no preference for representations that are neuron (axes) aligned.

j, . . . , (cid:80)m

j=1 wmjzzz(cid:48)

j}.

If representations are distributed across many dimensions, then this is a desirable property of a
representation analysis method. Previous studies have reported that representations may be more
complex than either fully distributed or axis-aligned [17, 21, 11] but this question remains open.

We use SVCCA as a tool to probe the nature of representations via two experiments:

(a) We ﬁnd that the subspace directions found by SVCCA are disproportionately important to

the representation learned by a layer, relative to neuron-aligned directions.

(b) We show that at least some of these directions are distributed across many neurons.

Experiments for (a), (b) are shown in Figure 2 as (a), (b) respectively. For both experiments, we ﬁrst
acquire two different representations, l1, l2, for a layer l by training two different random initializa-
tions of a convolutional network on CIFAR-10. We then apply SVCCA to l1 and l2 to get directions

3

(a)

(b)

Figure 2: Demonstration of (a) disproportionate importance of SVCCA directions, and (b) distributed nature
of some of these directions. For both panes, we ﬁrst ﬁnd the top k SVCCA directions by training two conv nets
on CIFAR-10 and comparing corresponding layers. (a) We project the output of the top three layers, pool1, fc1,
fc2, onto this top-k subspace. We see accuracy rises rapidly with increasing k, with even k (cid:28) num neurons
giving reasonable performance, with no retraining. Baselines of random k neuron subspaces and max activation
neurons require larger k to perform as well. (b): after projecting onto top k subspace (like left), dotted lines
then project again onto m neurons, chosen to correspond highly to the top k-SVCCA subspace. Many more
neurons are needed than k for better performance, suggesting distributedness of SVCCA directions.

1 , ..., ˜zzzl1

m} and {˜zzzl2

{˜zzzl1
1 , ..., ˜zzzl2
linear combination of the original neurons, i.e. ˜zzzli

m}, ordered according to importance by SVCCA, with each ˜zzzli
r=1 α(li)

j = (cid:80)m

jr zzzli
r .

j being a

For different values of k < m, we can then restrict layer li’s output to lie in the subspace of
span(˜zzzli
k ), the most useful k-dimensional subspace as found by SVCCA, done by projecting
each neuron into this k dimensional space.

1 , . . . , ˜zzzli

We ﬁnd — somewhat surprisingly — that very few SVCCA directions are required for the network
to perform the task well. As shown in Figure 2(a), for a network trained on CIFAR-10, the ﬁrst
25 dimensions provide nearly the same accuracy as using all 512 dimensions of a fully connected
layer with 512 neurons. The accuracy curve rises rapidly with the ﬁrst few SVCCA directions, and
plateaus quickly afterwards, for k (cid:28) m. This suggests that the useful information contained in m
neurons is well summarized by the subspace formed by the top k SVCCA directions. Two base-
lines for comparison are picking random and maximum activation neuron aligned subspaces and
projecting outputs onto these. Both of these baselines require far more directions (in this case: neu-
rons) before matching the accuracy achieved by the SVCCA directions. These results also suggest
approaches to model compression, which are explored in more detail in Section 4.4.

Figure 2(b) next demonstrates that these useful SVCCA directions are at least somewhat distributed
over neurons rather than axis-aligned. First, the top k SVCCA directions are picked and the rep-
resentation is projected onto this subspace. Next, the representation is further projected onto m
neurons, where the m are chosen as those most important to the SVCCA directions . The resulting
accuracy is plotted for different choices of k (given by x-axis) and different choices of m (different
lines). That, for example, keeping even 100 fc1 neurons (dashed green line) cannot maintain the
accuracy of the ﬁrst 20 SVCCA directions (solid green line at x-axis 20) suggests that those 20
SVCCA directions are distributed across 5 or more neurons each, on average. Figure 3 shows a
further demonstration of the effect on the output of projecting onto top SVCCA directions, here for
the toy regression case.

Why the two step SV + CCA method is needed. Both SVD and CCA have important properties
for analysing network representations and SVCCA consequently beneﬁts greatly from being a two
step method. CCA is invariant to afﬁne transformations, enabling comparisons without natural
alignment (e.g. different architectures, Section 4.4). See Appendix B for proofs and a demonstrative
ﬁgure. While CCA is a powerful method, it also suffers from certain shortcomings, particularly in
determining how many directions were important to the original space X, which is the strength of

4

Figure 3: The effect on the output of a latent representation being projected onto top SVCCA directions in
the toy regression task. Representations of the penultimate layer are projected onto 2, 6, 15, 30 top SVCCA
directions (from second pane). By 30, the output looks very similar to the full 200 neuron output (left).

SVD. See Appendix for an example where naive CCA performs badly. Both the SVD and CCA
steps are critical to the analysis of learning dynamics in Section 4.1.

3 Scaling SVCCA for Convolutional Layers

Applying SVCCA to convolutional layers can be done in two natural ways:

(1) Same layer comparisons: If X, Y are the same layer (at different timesteps or across ran-
dom initializations) receiving the same input we can concatenate along the pixel (height h,
width w) coordinates to form a vector: a conv layer h × w × c maps to c vectors, each
of dimension hwd, where d is the number of datapoints. This is a natural choice because
neurons at different pixel coordinates see different image data patches to each other. When
X, Y are two versions of the same layer, these c different views correspond perfectly.

(2) Different layer comparisons: When X, Y are not the same layer, the image patches seen by
different neurons have no natural correspondence. But we can ﬂatten an h×w ×c conv into
hwc neurons, each of dimension d. This approach is valid for convs in different networks
or at different depths.

3.1 Scaling SVCCA with Discrete Fourier Transforms

Applying SVCCA to convolutions introduces a computational challenge: the number of neurons
(h×w×c) in convolutional layers, especially early ones, is very large, making SVCCA prohibitively
expensive due to the large matrices involved. Luckily the problem of approximate dimensionality
reduction of large matrices is well studied, and efﬁcient algorithms exist, e.g. [4].

For convolutional layers however, we can avoid dimensionality reduction and perform exact
SVCCA, even for large networks. This is achieved by preprocessing each channel with a Discrete
Fourier Transform (which preserves CCA due to invariances, see Appendix), causing all (covari-
ance) matrices to be block-diagonal. This allows all matrix operations to be performed block by
block, and only over the diagonal blocks, vastly reducing computation. We show:

Theorem 1. Suppose we have a translation invariant (image) dataset X and convolutional layers
l1, l2. Letting DF T (li) denote the discrete fourier transform applied to each channel of li, the
covariance cov(DF T (l1), DF T (l2)) is block diagonal, with blocks of size c × c.

We make only two assumptions: 1) all layers below l1, l2 are either conv or pooling layers with
circular boundary conditions (translation equivariance) 2) The dataset X has all translations of the
images Xi. This is necessary in the proof for certain symmetries in neuron activations, but these
symmetries typically exist in natural images even without translation invariance, as shown in Fig-
ure App.2 in the Appendix. Below are key statements, with proofs in Appendix.

Deﬁnition 1. Say a single channel image dataset X of images is translation invariant if for any
(wlog n × n) image Xi ∈ X, with pixel values {zzz11, ...zzznn}, X (a,b)
= {zzzσa(1)σb(1), ...zzzσa(n)σb(n)}
is also in X, for all 0 ≤ a, b ≤ n − 1, where σa(i) = a + i mod n (and similarly for b).

i

For a multiple channel image Xi, an (a, b) translation is an (a, b) height/width shift on every channel
separately. X is then translation invariant as above.

5

To prove Theorem 1, we ﬁrst show another theorem:
Theorem 2. Given a translation invariant dataset X, and a convolutional layer l with channels
{c1, . . . ck} applied to X

(a) the DFT of ci, F cF T has diagonal covariance matrix (with itself).
(b) the DFT of ci, cj, F ciF T , F cjF T have diagonal covariance with each other.
Finally, both of these theorems rely on properties of circulant matrices and their DFTs:
Lemma 1. The covariance matrix of ci applied to translation invariant X is circulant and block
circulant.
Lemma 2. The DFT of a circulant matrix is diagonal.

4 Applications of SVCCA

4.1 Learning Dynamics with SVCCA

We can use SVCCA as a window into learning dynamics by comparing the representation at a
layer at different points during training to its ﬁnal representation. Furthermore, as the SVCCA
computations are relatively cheap to compute compared to methods that require training an auxiliary
network for each comparison [1, 10, 11], we can compare all layers during training at all timesteps
to all layers at the ﬁnal time step, producing a rich view into the learning process.

The outputs of SVCCA are the aligned directions (˜xi, ˜yi), how well they align, ρi, as well as in-
termediate output from the ﬁrst step, of singular values and directions, λ(i)
Y , y(cid:48)(j). We
condense these outputs into a single value, the SVCCA similarity ¯ρ, that encapsulates how well the
representations of two layers are aligned with each other,

X , x(cid:48)(i), λ(j)

¯ρ =

1
min (m1, m2)

(cid:88)

ρi,

i

(1)

where min (m1, m2) is the size of the smaller of the two layers being compared. The SVCCA
similarity ¯ρ is the average correlation across aligned directions, and is a direct multidimensional
analogue of Pearson correlation.

The SVCCA similarity for all pairs of layers, and all time steps, is shown in Figure 4 for a convnet
and a resnet architecture trained on CIFAR10.

4.2 Freeze Training

Observing in Figure 4 that networks broadly converge from the bottom up, we propose a training
method where we successively freeze lower layers during training, only updating higher and higher
layers, saving all computation needed for deriving gradients and updating in lower layers.

We apply this method to convolutional and residual networks trained on CIFAR-10, Figure 5, using
a linear freezing regime: in the convolutional network, each layer is frozen at a fraction (layer num-
ber/total layers) of total training time, while for resnets, each residual block is frozen at a fraction
(block number/total blocks). The vertical grey dotted lines show which steps have another set of lay-
ers frozen. Aside from saving computation, Freeze Training appears to actively help generalization
accuracy, like early stopping but with different layers requiring different stopping points.

4.3

Interpreting Representations: when are classes learned?

We also can use SVCCA to compare how correlated representations in each layer are with the logits
of each class in order to measure how knowledge about the target evolves throughout the network.
In Figure 6 we apply the DFT CCA technique on the Imagenet Resnet [6]. We take ﬁve different
classes and for different layers in the network, compute the DFT CCA similarity between the logit
of that class and the network layer. The results successfully reﬂect semantic aspects of the classes:
the ﬁretruck class sensitivity line is clearly distinct from the two pairs of dog breeds, and network
develops greater sensitivity to ﬁretruck earlier on. The two pairs of dog breeds, purposefully chosen
so that each pair is similar to the other in appearance, have cca similarity lines that are very close to
each other through the network, indicating these classes are similar to each other.

6

Figure 4: Learning dynamics plots for conv (top) and res (bottom) nets trained on CIFAR-10. Each pane is
a matrix of size layers × layers, with each entry showing the SVCCA similarity ¯ρ between the two layers.
Note that learning broadly happens ‘bottom up’ – layers closer to the input seem to solidify into their ﬁnal
representations with the exception of the very top layers. Per layer plots are included in the Appendix. Other
patterns are also visible – batch norm layers maintain nearly perfect similarity to the layer preceding them due
to scaling invariance (with a slight reduction since batch norm changes the SVD directions which capture 99%
of the variance). In the resnet plot, we see a stripe like pattern due to skip connections inducing high similarities
to previous layers.

Figure 5: Freeze Training reduces training cost and improves generalization. We apply Freeze Training to a
convolutional network on CIFAR-10 and a residual network on CIFAR-10. As shown by the grey dotted lines
(which indicate the timestep at which another layer is frozen), both networks have a ‘linear’ freezing regime:
for the convolutional network, we freeze individual layers at evenly spaced timesteps throughout training. For
the residual network, we freeze entire residual blocks at each freeze step. The curves were averaged over ten
runs.

4.4 Other Applications: Cross Model Comparison and compression

SVCCA similarity can also be used to compare the similarity of representations across different
random initializations, and even different architectures. We compare convolutional networks on
CIFAR-10 across random initializations (Appendix) and also a convolutional network to a residual
network in Figure 7, using the DFT method described in 3.

In Figure 3, we saw that projecting onto the subspace of the top few SVCCA directions resulted in
comparable accuracy. This observations motivates an approach to model compression. In particular,
letting the output vector of layer l be xxx(l) ∈ Rn×1, and the weights W (l), we replace the usual
W (l)xxx(l) with (W (l)P T
x )(Pxxxx(l)) where Px is a k × n projection matrix, projecting xxx onto the top
SVCCA directions. This bottleneck reduces both parameter count and inference computational cost

7

Figure 6: We plot the CCA similarity using the Discrete Fourier Transform between the logits of ﬁve classes
and layers in the Imagenet Resnet. The classes are ﬁretruck and two pairs of dog breeds (terriers and husky
like dogs: husky and eskimo dog) that are chosen to be similar to each other. These semantic properties are
captured in CCA similarity, where we see that the line corresponding to ﬁretruck is clearly distinct from the
two pairs of dog breeds, and the two lines in each pair are both very close to each other, reﬂecting the fact that
each pair consists of visually similar looking images. Firetruck also appears to be easier for the network to
learn, with greater sensitivity displayed much sooner.

Figure 7: We plot the CCA similarity using the Discrete Fourier Transform between convolutional layers of a
Resnet and Convnet trained on CIFAR-10. We ﬁnd that the lower layrs of both models are noticeably similar to
each other, and get progressively less similar as we compare higher layers. Note that the highest layers of the
resnet are least similar to the lower layers of the convnet.

for the layer by a factor ∼ k
n . In Figure App.5 in the Appendix, we show that we can consecutively
compress top layers with SVCCA by a signiﬁcant amount (in one case reducing each layer to 0.35
original size) and hardly affect performance.

5 Conclusion

In this paper we present SVCCA, a general method which allows for comparison of the learned dis-
tributed representations between different neural network layers and architectures. Using SVCCA
we obtain novel insights into the learning dynamics and learned representations of common neural
network architectures. These insights motivated a new Freeze Training technique which can reduce
the number of ﬂops required to train networks and potentially even increase generalization perfor-
mance. We observe that CCA similarity can be a helpful tool for interpretability, with sensitivity
to different classes reﬂecting their semantic properties. This technique also motivates a new algo-
rithm for model compression. Finally, the “lower layers learn ﬁrst” behavior was also observed for
recurrent neural networks as shown in Figure App.6 in the Appendix.

8

References

[1] Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classiﬁer

probes. arXiv preprint arXiv:1610.01644, 2016.

[2] David Eigen, Jason Rolfe, Rob Fergus, and Yann LeCun. Understanding deep architectures

using a recursive convolutional network. arXiv preprint arXiv:1312.1847, 2013.

[3] Manaal Faruqui and Chris Dyer. Improving vector space word representations using multilin-

gual correlation. Association for Computational Linguistics, 2014.

[4] Nathan Halko, Martinsson Per-Gunnar, and Joel A. Tropp. Finding structure with random-
ness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM
Rev., 53:217–288, 2011.

[5] D. R. Hardoon, S. Szedmak, and J. Shawe-Taylor. Canonical correlation analysis: An overview

with application to learning methods. Neural Computation, 16:2639–2664, 2004.

[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image

recognition. CoRR, abs/1512.03385, 2015.

[7] Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel-rahman Mohamed, Navdeep
Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neu-
ral networks for acoustic modeling in speech recognition: The shared views of four research
groups. IEEE Signal Processing Magazine, 29(6):82–97, 2012.

[8] Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 1985.
[9] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems, pages
1097–1105, 2012.

[10] Karel Lenc and Andrea Vedaldi. Understanding image representations by measuring their
equivariance and equivalence. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 991–999, 2015.

[11] Y. Li, J. Yosinski, J. Clune, H. Lipson, and J. Hopcroft. Convergent Learning: Do different
In International Conference on Learning

neural networks learn the same representations?
Representations (ICLR), May 2016.

[12] Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. Convergent learning:
Do different neural networks learn the same representations? In Feature Extraction: Modern
Questions and Challenges, pages 196–212, 2015.

[13] Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by in-
verting them. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-
nition, pages 5188–5196, 2015.

[14] Gr´egoire Montavon, Mikio L Braun, and Klaus-Robert M¨uller. Kernel analysis of deep net-

works. Journal of Machine Learning Research, 12(Sep):2563–2581, 2011.

[15] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional
arXiv preprint

networks: Visualising image classiﬁcation models and saliency maps.
arXiv:1312.6034, 2013.

[16] David Sussillo, Mark M Churchland, Matthew T Kaufman, and Krishna V Shenoy. A neural
network that ﬁnds a naturalistic solution for the production of muscle activity. Nature neuro-
science, 18(7):1025–1033, 2015.

[17] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
arXiv preprint

Intriguing properties of neural networks.

Goodfellow, and Rob Fergus.
arXiv:1312.6199, 2013.

[18] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural ma-
chine translation system: Bridging the gap between human and machine translation. arXiv
preprint arXiv:1609.08144, 2016.

[19] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding
neural networks through deep visualization. In Deep Learning Workshop, International Con-
ference on Machine Learning (ICML), 2015.

9

[20] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In

European conference on computer vision, pages 818–833. Springer, 2014.

[21] Bolei Zhou, Aditya Khosla, `Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object de-
tectors emerge in deep scene cnns. In International Conference on Learning Representations
(ICLR), volume abs/1412.6856, 2014.

10

Appendix

A Mathematical details of CCA and SVCCA

Canonical Correlation of X, Y
ﬁnding a, b to maximise:

Finding maximal correlations between X, Y can be expressed as

aT ΣXY b

(cid:112)

(cid:112)

aT ΣXX a

bT ΣY Y b

where ΣXX , ΣXY , ΣY X , ΣY Y are the covariance and cross-covariance terms. By performing the
change of basis ˜x˜x˜x1 = Σ1/2
Y Y b and using Cauchy-Schwarz we recover an eigenvalue
problem:

xx a and ˜y˜y˜y1 = Σ1/2

˜x˜x˜x1 = argmax

(cid:34)

xT Σ−1/2

XX ΣXY Σ−1
||x||

Y Y ΣY X Σ−1/2
XX x

(cid:35)

(*)

SVCCA Given two subspaces X = {xxx1, ..., xxxm1 }, Y = {yyy1, ..., yyym2}, SVCCA ﬁrst performs a
singular value decomposition on X, Y . This results in singular vectors {x(cid:48)x(cid:48)x(cid:48)
m1 } with associ-
ated singular values {λ1, ..., λm1} (for X, and similarly for Y ). Of these m1 singular vectors, we
keep the top m(cid:48)
i=1 |λi|). That is, 99% of
the variation of X is explainable by the top m(cid:48)
1 vectors. This helps remove directions/neurons that
are constant zero, or noise with small magnitude.
Then, we apply Canonical Correlation Analysis (CCA) to the sets {x(cid:48)x(cid:48)x(cid:48)
top singular vectors.

1 is the smallest value that (cid:80)m(cid:48)

i=1 |λi|(≥ 0.99 (cid:80)m1

1 where m(cid:48)

1, ..., y(cid:48)y(cid:48)y(cid:48)

1, ..., x(cid:48)x(cid:48)x(cid:48)

1, ..., x(cid:48)x(cid:48)x(cid:48)

}, {y(cid:48)y(cid:48)y(cid:48)

} of

m(cid:48)
2

m(cid:48)
1

1

CCA is a well established statistical method for understanding the similarity of two different sets
of random variables – given our two sets of vectors {x(cid:48)x(cid:48)x(cid:48)
}, we wish to ﬁnd
linear transformations, WX , WY that maximally correlate the subspaces. This can be reduced to
an eigenvalue problem. Solving this results in linearly transformed subspaces ˜X, ˜Y with directions
˜xxxi, ˜yyyi that are maximally correlated with each other, and orthogonal to ˜xxxj, ˜yyyj, j < i. We let ρi =
corr(˜xxxi, ˜yyyi). In summary, we have:

1, ..., y(cid:48)y(cid:48)y(cid:48)

1, ..., x(cid:48)x(cid:48)x(cid:48)

}, {y(cid:48)y(cid:48)y(cid:48)

m(cid:48)
1

m(cid:48)
2

SVCCA Summary

1. Input: X, Y
2. Perform: SVD(X), SVD(Y). Output: X (cid:48) = U X, Y (cid:48) = V Y
3. Perform CCA(X (cid:48), Y (cid:48)). Output:

{ρ1, . . . ρmin(m1,m2)}

˜X = WX X (cid:48), ˜Y = WY Y (cid:48) and corrs =

B Additional Proofs and Figures from Section 2.1

Proof of Orthonormal and Scaling Invariance of CCA:

We can see this using equation (*) as follows: suppose U, V are orthonormal transforms applied to
the sets X, Y . Then it follows that Σa
XX becomes U Σa
XX U T , for a = {1, −1, 1/2, −1/2}, and
similarly for Y and V . Also note ΣXY becomes U ΣXY V T . Equation (*) then becomes
xT U Σ−1/2

(cid:34)

(cid:35)

˜x1 = argmax

Y Y ΣY X Σ−1/2
XX ΣXY Σ−1
||x||

XX U T x

So if ˜u is a solution to equation (*), then U ˜u is a solution to the equation above, which results in the
same correlation coefﬁcients.

B.0.1 The importance of SVD: how many directions matter?

While CCA is excellent at identifying useful learned directions that correlate, independent of certain
common transforms, it doesn’t capture the full picture entirely. Consider the following setting:

11

Figure App.1: This ﬁgure shows the ability of CCA to deal with orthogonal and scaling transforms.
In the ﬁrst pane, the maroon plot shows one of the highest activation neurons in the penultimate
layer of a network trained on CIFAR-10, with the x-axis being (ordered) image ids and the y-axis
being activation on that image. The green plots show two resulting distorted directions after this and
two of the other top activation neurons are permuted, rotated and scaled. Pane two shows the result
of applying CCA to the distorted directions and the original signal, which succeeds in recovering
the original signal.

suppose we have subspaces A, B, C, with A being 50 dimensions, B being 200 dimensions, 50 of
which are perfectly aligned with A and the other 150 being noise, and C being 200 dimensions, 50
of which are aligned with A (and B) and the other 150 being useful, but different directions.

Then looking at the canonical correlation coefﬁcients of (A, B) and (A, C) will give the same result,
both being 1 for 50 values and 0 for everything else. But these are two very different cases – the
subspace B is indeed well represented by the 50 directions that are aligned with A. But the subspace
C has 150 more useful directions.

This distinction becomes particularly important when aggregating canonical correlation coefﬁcients
as a measure of similarity, as used in analysing network learning dynamics. However, by ﬁrst ap-
plying SVD to determine the number of directions needed to explain 99% of the observed variance,
we can distinguish between pathological cases like the one above.

C Proof of Theorem 1

Here we provide the proofs for Lemma 1, Lemma 2, Theorem 2 and ﬁnally Theorem 1.

A preliminary note before we begin:

When we consider a (wlog) n by n channel c of a convolutional layer, we assume it has shape







zzz0,0
zzz1,0
...
zzzn−1,0

zzz1,2
zzz2,2
...
zzzn−1,1

. . .
. . .
. . .
. . .







zzz0,n−1
zzz1,n−1
...
zzzn−1,n−1

12

(a)

(b)

(c)

(d)

Figure App.2:
This ﬁgure visualizes the covariance matrix of one of the channels of a resnet
trained on Imagenet. Black correspond to large values and white to small values. (a) we compute the
covariance without a translation invariant dataset and without ﬁrst preprocessing the images by DFT.
We see that the covariance matrix is dense. (b) We compute the covariance after applying DFT, but
without augmenting the dataset with translations. Even without enforcing translation invariance, we
see that the covariance in the DFT basis is approximately diagonal. (c) Same as (a), but the dataset
is augmented to be fully translation invariant. The covariance in the pixel basis is still dense. (d)
Same as (c), but with dataset augmented to be translation invariant. The covariance matrix is exactly
diagonal for a translation invariant dataset in a DFT basis.

When computing the covariance matrix however, we vectorize c by stacking the columns under each
other, and call the result vec(c):

vec(c) =

:=














zzz0,0
zzz1,0
...
zzzn−1,0
zzz0,1
...
zzzn−1,n−1








































zzz0
zzz1
...
zzzn−1
zzzn
...
zzzn2−1

vec(AcB) = (BT ⊗ A)vec(c)

One useful identity when switching between these two notations (see e.g. [8]) is

where A, B are matrices and ⊗ is the Kronecker product. A useful observation arising from this is:
Lemma 3. The CCA vectors of DF T (ci), DF T (cj) are the same (up to a rotation by F ) as the
CCA of ci, cj.

Proof: From Section B we know that unitary transforms only rotate the CCA directions. But while
DFT pre and postmultiplies by F, F T – unitary matrices, we cannot directly apply this as the result
is for unitary transforms on vec(ci). But, using the identity above, we see that vec(DF T (ci)) =
vec(F ciF T ) = (F ⊗ F )vec(ci), which is unitary as F is unitary. Applying the same identity to cj,
we can thus conclude that the DFT preserves CCA (up to rotations).

As Theorem 1 preprocesses the neurons with DFT, it is important to note that by the Lemma above,
we do not change the CCA vectors (except by a rotation).

C.1 Proof of Lemma 1

Proof. Translation invariance is preserved We show inductively that any translation invariant input
to a convolutional channel results in a translation invariant output: Suppose the input to channel c,
(n by n) is translation invariant. It is sufﬁcient to show that for inputs Xi, Xj and 0 ≤ a, b, ≤ n − 1,
c(Xi) + (a, b) mod n = c(Xj). But an (a, b) shift in neuron coordinates in c corresponds to a
(height stride · a, width stride · b) shift in the input. And as X is translation invariant, there is some
Xj = Xi + (height stride · a, width stride · b).

cov(c) is circulant:

13

Let X be (by proof above) a translation invariant input to a channel c in some convolution or pooling
layer. The empirical covariance, cov(c) is the n2 by n2 matrix computed by (assuming c is centered)

1
|X|

(cid:88)

Xi∈X

vec(c(Xi)) · vec(c(Xi))T

So, cov(c)ij = 1
j.

|X|zzzT

i zzzj = 1
|X|

(cid:80)

Xl∈X zzzT

i (Xl)zzzj(Xl), i.e. the inner products of the neurons i and

The indexes i and j refer to the neurons in their vectorized order in vec(c). But in the matrix ordering
of neurons in c, i and j correspond to some (a1, b1) and (a2, b2). If we applied a translation (a, b),
to both, we would get new neuron coordinates (a1 + a, b1 + b), (a2 + a, b2 + b) (all coordinates
mod n) which would correspond to i + an + b mod n2 and j + an + b mod n2, by our stacking
of columns and reindexing.

Let τa,b be the translation in inputs corresponding to an (a, b) translation in c, i.e.
τa,b =
(height stride·a, width stride·b). Then clearly zzz(a1,b1)(Xi) = zzz(a1+a,b1+b)(τ(a,b)(Xi), and similarly
for zzz(a2,b2)
It follows that 1

(a1+b,b1+b)zzz(a2+a,b2+b), or, with vec(c) indexing

(a1,b1)zzz(a2,b2) = 1

|X|zzzT

|X|zzzT

1
|X|

1
|X|

zzzT
i zzzj =

zzzT
(i+an+b mod n2)zzz(j+an+b mod n2)

This gives us the circulant structure of cov(c).

cov(c) is block circulant: Let zzz(i) be the ith column of c, and zzz(j) the jth. In vec(c), these correspond
to zzz(i−1)n, . . . zzzin−1 and zzz(j−1)n, . . . zzzjn−1, and the n by n submatrix at those row and column in-
dexes of cov(vec(c)) corresponds to the covariance of column i, j. But then we see that the covari-
ance of columns i+k, j +k, corresponding to the covariance of neurons zzz(i−1)n+k·n, . . . zzzin−1+k·n,
and zzz(j−1)n+k·n, . . . zzzjn−1+k·n, which corresponds to the 2-d shift (1, 0), applied to every neuron.
So by an identical argument to above, we see that for all 0 ≤ k ≤ n − 1

cov(zzz(i), zzz(j)) = cov(zzz(i+k), zzz(j+k))

In particular, cov(vec(c)) is block circulant.

An example cov(vec(c)) with c being 3 by 3 look like below:

(cid:35)

(cid:34)A0 A1 A2
A2 A0 A1
A1 A2 A0

where each Ai is itself a circulant matrix.

C.2 Proof of Lemma 2

Proof. This is a standard result, following from expressing a circulant matrix A in terms of its
diagonal form , i.e. A = V ΣV T with the columns of V being its eigenvectors. Noting that V = F ,
the DFT matrix, and that vectors of powers of ωk = exp( 2πik
n ) are orthogonal
gives the result.

n ), ωj = exp( 2πik

C.3 Proof of Theorem 2

Proof. Starting with (a), we need to show that cov(vec(DF T (ci)), vec(DF T (ci)) is diagonal. But
by the identity above, this becomes:

cov(vec(DF T (ci)), vec(DF T (ci)) = (F ⊗ F )vec(ci)vec(ci)T (F ⊗ F )∗

14

By Lemma 1, we see that

cov(vec(ci)) = vec(ci)vec(ci)T =







A0

A1
An−1 A0
...
A2

...
A1

. . . An−1
. . . An−2
. . .
. . .

...
A0







with each Ai circulant.

And so cov(vec(DF T (ci)), vec(DF T (ci)) becomes







f00F
f10F
...

. . .
. . .
. . .
fn−1,0F fn−1,1F . . .

f01F
f11F
...

f0,n−1F
f1,n−1F
...
fn−1,n−1F













A0

A1
An−1 A0
...
A2

...
A1

. . . An−1
. . . An−2
. . .
. . .

...
A0













f ∗
00F ∗
01F ∗
f ∗
...
0,n−1F ∗
f ∗

f ∗
10F ∗
11F ∗
f ∗
...
1,n−1F ∗
f ∗

. . .
. . .
. . .
. . .

f ∗
n−1,0F ∗
n−1,1F ∗
f ∗
...
n−1,n−1F ∗
f ∗







From this, we see that the sjth entry has the form

n−1
(cid:88)

(cid:32)n−1
(cid:88)

l=0

k=0

fskF Al−k

ljF ∗ =
f ∗

fskf ∗

ljF Al−kF ∗

(cid:33)

(cid:88)

k,l

Letting [F ArF ∗] denote the coefﬁcient of the term F ArF ∗, we see that (addition being mod n)

[F ArF ∗] =

fskf ∗

(k+r)j =

2πisk
n

e

· e

−2πij(k+r)
n

−2πijr
n

= e

2πik(s−j)
n

e

= e

−2πijr
n

· δsj

n−1
(cid:88)

k=0

(cid:88)

k

n−1
(cid:88)

k=0

with the last step following by the fact that the sum of powers of non trivial roots of unity are 0.

In particular, we see that only the diagonal entries (of the n by n matrix of matrices) are non zero.
The diagonal elements are linear combinations of terms of form F ArF ∗, and by Lemma 2 these are
diagonal. So the covariance of the DFT is diagonal as desired.

Part (b) follows almost identically to part (a), but by ﬁrst noting that exactly by the proof of Lemma
1, cov(ci, cj) is also a circulant and block circulant matrix.

C.4 Proof of Theorem 1

Proof. This Theorem now follows easily from the previous. Suppose we have a layer l, with chan-
nels c1, ..., ck. And let vec(DF T (ci)) have directions ˜zzz(i)
n2−1. By the previous theorem, we
k , ˜zzz(j)
know that the covariance of all of these neurons only has non-zero terms cov(˜zzz(i)
k .
So arranging the
0 , . . . ˜zzz(k)
0 , ˜zzz(1)
˜zzz(1)
diagonal of the matrix, proving the theorem.

row and column indexes being
the nonzero terms all live in the n2 k by k blocks down the

covariance matrix to have

full
. . . ˜zzz(k)
n2

0 , · · · ˜zzz(i)

0 , ˜zzz(1)

1

C.5 Computational Gains

As the covariance matrix is block diagonal, our more efﬁcient algorithm for computation is as fol-
lows: take the DFT of every channel (n log n due to FFT) and then compute covariances according
to blocks: partition the kn directions into the n2 k by k matrices that are non-zero, and compute the
covariance, inverses and square roots along these.
A rough computational budget for the covariance is therefore kn log n + n2k2.5, while the naive
computation would be of order (kn2)2.5, a polynomial difference. Furthermore, the DFT method
also makes for easy parallelization as each of the n2 blocks does not interact with any of the others.

15

Figure App.3: Learning dynamics per layer plots for conv (left pane) and res (right pane) nets trained on
CIFAR-10. Each line plots the SVCCA similarity of each layer with its ﬁnal representation, as a function of
training step, for both the conv (left pane) and res (right pane) nets. Note the bottom up convergence of different
layers

D Per Layer Learning Dynamics Plots from Section 4.1

E Additional Figure from Section 4.4

Figure App.4 compares the converged representations of two different initializations of the same
convolutional network on CIFAR-10.

Figure App.4: Comparing the converged representations of two different initializations of the same
convolutional architecture. The results support ﬁndings in [12], where initial and ﬁnal layers are
found to be similar, with middle layers differing in representation similarity.

F Experiment from Section 4.4

G Learning Dynamics for an LSTM

16

Figure App.5: Using SVCCA to perform model compression on the fully connected layers in a CIFAR-
10 convnet. The two gray lines indicate the original train (top) and test (bottom) accuracy. The two sets of
representations for SVCCA are obtained through 1) two different initialization and training of convnets on
CIFAR-10 2) the layer activations and the activations of the logits. The latter provides better results, with the
ﬁnal ﬁve layers: pool1, fc1, bn3, fc2 and bn4 all being compressed to 0.35 of their original size.

Figure App.6: Learning dynamics of the different layers of a stacked LSTM trained on the Penn Tree
Bank language modeling task. We observe a similar pattern to that of convolutional architectures
trained on image data: lower layer converge faster than upper layers.

17

7
1
0
2
 
v
o
N
 
8
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
0
8
5
0
.
6
0
7
1
:
v
i
X
r
a

SVCCA: Singular Vector Canonical Correlation
Analysis for Deep Learning Dynamics and
Interpretability

Maithra Raghu,1,2 Justin Gilmer,1 Jason Yosinski,3 & Jascha Sohl-Dickstein1
1Google Brain 2Cornell University 3Uber AI Labs
maithrar gmail com, gilmer google com, yosinski uber com, jaschasd google com

Abstract

We propose a new technique, Singular Vector Canonical Correlation Analysis
(SVCCA), a tool for quickly comparing two representations in a way that is both
invariant to afﬁne transform (allowing comparison between different layers and
networks) and fast to compute (allowing more comparisons to be calculated than
with previous methods). We deploy this tool to measure the intrinsic dimension-
ality of layers, showing in some cases needless over-parameterization; to probe
learning dynamics throughout training, ﬁnding that networks converge to ﬁnal
representations from the bottom up; to show where class-speciﬁc information in
networks is formed; and to suggest new training regimes that simultaneously save
computation and overﬁt less.

1

Introduction

As the empirical success of deep neural networks ([7, 9, 18]) become an indisputable fact, the goal
of better understanding these models escalates in importance. Central to this aim is a core issue
of deciphering learned representations. Facets of this key question have been explored empirically,
particularly for image models, in [1, 2, 10, 12, 13, 14, 15, 19, 20]. Most of these approaches are
motivated by interpretability of learned representations. More recently, [11] studied the similarities
of representations learned by multiple networks by ﬁnding permutations of neurons with maximal
correlation.

In this work we introduce a new approach to the study of network representations, based on an
analysis of each neuron’s activation vector – the scalar outputs it emits on input datapoints. With
this interpretation of neurons as vectors (and layers as subspaces, spanned by neurons), we intro-
duce SVCCA, Singular Vector Canonical Correlation Analysis, an amalgamation of Singular Value
Decomposition and Canonical Correlation Analysis (CCA) [5], as a powerful method for analyzing
deep representations. Although CCA has not previously been used to compare deep representations,
it has been used for related tasks such as computing the similarity between modeled and measured
brain activity [16], and training multi-lingual word embeddings in language models [3].

The main contributions resulting from the introduction of SVCCA are the following:

1. We ask: is the dimensionality of a layer’s learned representation the same as the number
of neurons in the layer? Answer: No. We show that trained networks perform equally well
with a number of directions just a fraction of the number of neurons with no additional
training, provided they are carefully chosen with SVCCA (Section 2.1). We explore the
consequences for model compression (Section 4.4).

2. We ask: what do deep representation learning dynamics look like? Answer: Networks
broadly converge bottom up. Using SVCCA, we compare layers across time and ﬁnd they

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

Figure 1: To demonstrate SVCCA, we consider a toy regression task (regression target as in Figure 3). (a)
We train two networks with four fully connected hidden layers starting from different random initializations,
and examine the representation learned by the penultimate (shaded) layer in each network. (b) The neurons
with the highest activations in net 1 (maroon) and in net 2 (green). The x-axis indexes over the dataset: in
our formulation, the representation of a neuron is simply its value over a dataset (Section 2). (c) The SVD
directions — i.e. the directions of maximal variance — for each network. (d) The top SVCCA directions. We
see that each pair of maroon/green lines (starting from the top) are almost visually identical (up to a sign). Thus,
although looking at just neurons (b) seems to indicate that the networks learn very different representations,
looking at the SVCCA subspace (d) shows that the information in the representations are (up to a sign) nearly
identical.

solidify from the bottom up. This suggests a simple, computationally more efﬁcient method
of training networks, Freeze Training, where lower layers are sequentially frozen after a
certain number of timesteps (Sections 4.1, 4.2).

3. We develop a method based on the discrete Fourier transform which greatly speeds up the

application of SVCCA to convolutional neural networks (Section 3).

4. We also explore an interpretability question, of when an architecture becomes sensitive to
different classes. We ﬁnd that SVCCA captures the semantics of different classes, with
similar classes having similar sensitivities, and vice versa. (Section 4.3).

Experimental Details Most of our experiments are performed on CIFAR-10 (augmented with
random translations). The main architectures we use are a convolutional network and a residual
network1. To produce a few ﬁgures, we also use a toy regression task: training a four hidden layer
fully connected network with 1D input and 4D output, to regress on four different simple functions.

2 Measuring Representations in Neural Networks

Our goal in this paper is to analyze and interpret the representations learned by neural networks. The
critical question from which our investigation departs is: how should we deﬁne the representation
of a neuron? Consider that a neuron at a particular layer in a network computes a real-valued
function over the network’s input domain. In other words, if we had a lookup table of all possible
input → output mappings for a neuron, it would be a complete portrayal of that neuron’s functional
form.

However, such inﬁnite tables are not only practically infeasible, but are also problematic to process
into a set of conclusions. Our primary interest is not in the neuron’s response to random data, but
rather in how it represents features of a speciﬁc dataset (e.g. natural images). Therefore, in this
study we take a neuron’s representation to be its set of responses over a ﬁnite set of inputs — those
drawn from some training or validation set.
More concretely, for a given dataset X = {x1, · · · xm} and a neuron i on layer l, zzzl
be the vector of outputs on X, i.e.

i, we deﬁne zzzl

i to

i(x1), · · · , zzzl
1Convnet layers: conv-conv-bn-pool-conv-conv-conv-bn-pool-fc-bn-fc-bn-out. Resnet layers:

i = (zzzl
zzzl

i(xm))

conv-(x10 c/bn/r block)-(x10 c/bn/r block)-(x10 c/bn/r block)-bn-fc-out.

2

Note that this is a different vector from the often-considered vector of the “representation at a layer
of a single input.” Here zzzl
i is a single neuron’s response over the entire dataset, not an entire layer’s
response for a single input. In this view, a neuron’s representation can be thought of as a single
vector in a high-dimensional space. Broadening our view from a single neuron to the collection of
neurons in a layer, the layer can be thought of as the set of neuron vectors contained within that
layer. This set of vectors will span some subspace. To summarize:

Considered over a dataset X with m examples, a neuron is a vector in Rm.
A layer is the subspace of Rm spanned by its neurons’ vectors.

Within this formalism, we introduce Singular Vector Canonical Correlation Analysis (SVCCA) as
a method for analysing representations. SVCCA proceeds as follows:

• Input: SVCCA takes as input two (not necessarily different) sets of neurons (typically

layers of a network) l1 = {zzzl1

1 , ..., zzzl1
m1

} and l2 = {zzzl2

1 , ..., zzzl2
m2

}

1 ⊂ l1, l(cid:48)

• Step 1 First SVCCA performs a singular value decomposition of each subspace to get sub-
subspaces l(cid:48)
2 ⊂ l2 which comprise of the most important directions of the original
subspaces l1, l2. In general we take enough directions to explain 99% of variance in the
subspace. This is especially important in neural network representations, where as we will
show many low variance directions (neurons) are primarily noise.

1, l(cid:48)

• Step 2 Second, compute the Canonical Correlation similarity ([5]) of l(cid:48)

2: linearly trans-
2 to be as aligned as possible and compute correlation coefﬁcients. In particu-
1 , ..., zzz(cid:48)l2
}, l(cid:48)
}, CCA linearly
m(cid:48)
2
2 such as to maximize the correlations

form l(cid:48)
lar, given the output of step 1, l(cid:48)
transforms these subspaces ˜l1 = WX l(cid:48)
corrs = {ρ1, . . . ρmin(m(cid:48)

1 , ..., zzz(cid:48)l1
m(cid:48)
1
1, ˜l2 = WY l(cid:48)

2)} between the transformed subspaces.

2 = {zzz(cid:48)l2

1 = {zzz(cid:48)l1

1, l(cid:48)

1,m(cid:48)

• Output: With these steps, SVCCA outputs pairs of aligned directions, (˜zzzl1

i ) and how
well they correlate, ρi. Step 1 also produces intermediate output in the form of the top
singular values and directions.

i , ˜zzzl2

For a more detailed description of each step, see the Appendix. SVCCA can be used to analyse
any two sets of neurons. In our experiments, we utilize this ﬂexibility to compare representations
across different random initializations, architectures, timesteps during training, and speciﬁc classes
and layers.

Figure 1 shows a simple, intuitive demonstration of SVCCA. We train a small network on a toy
regression task and show each step of SVCCA, along with the resulting very similar representations.
SVCCA is able to ﬁnd hidden similarities in the representations.

2.1 Distributed Representations

An important property of SVCCA is that it is truly a subspace method: both SVD and CCA work
with span(zzz1, . . . , zzzm) instead of being axis aligned to the zzzi directions. SVD ﬁnds singular vectors
i = (cid:80)m
zzz(cid:48)
j=1 sijzzzj, and the subsequent CCA ﬁnds a linear transform W , giving orthogonal canon-
ically correlated directions {˜zzz1, . . . , ˜zzzm} = {(cid:80)m
j=1 w1jzzz(cid:48)
In other words,
SVCCA has no preference for representations that are neuron (axes) aligned.

j, . . . , (cid:80)m

j=1 wmjzzz(cid:48)

j}.

If representations are distributed across many dimensions, then this is a desirable property of a
representation analysis method. Previous studies have reported that representations may be more
complex than either fully distributed or axis-aligned [17, 21, 11] but this question remains open.

We use SVCCA as a tool to probe the nature of representations via two experiments:

(a) We ﬁnd that the subspace directions found by SVCCA are disproportionately important to

the representation learned by a layer, relative to neuron-aligned directions.

(b) We show that at least some of these directions are distributed across many neurons.

Experiments for (a), (b) are shown in Figure 2 as (a), (b) respectively. For both experiments, we ﬁrst
acquire two different representations, l1, l2, for a layer l by training two different random initializa-
tions of a convolutional network on CIFAR-10. We then apply SVCCA to l1 and l2 to get directions

3

(a)

(b)

Figure 2: Demonstration of (a) disproportionate importance of SVCCA directions, and (b) distributed nature
of some of these directions. For both panes, we ﬁrst ﬁnd the top k SVCCA directions by training two conv nets
on CIFAR-10 and comparing corresponding layers. (a) We project the output of the top three layers, pool1, fc1,
fc2, onto this top-k subspace. We see accuracy rises rapidly with increasing k, with even k (cid:28) num neurons
giving reasonable performance, with no retraining. Baselines of random k neuron subspaces and max activation
neurons require larger k to perform as well. (b): after projecting onto top k subspace (like left), dotted lines
then project again onto m neurons, chosen to correspond highly to the top k-SVCCA subspace. Many more
neurons are needed than k for better performance, suggesting distributedness of SVCCA directions.

1 , ..., ˜zzzl1

m} and {˜zzzl2

{˜zzzl1
1 , ..., ˜zzzl2
linear combination of the original neurons, i.e. ˜zzzli

m}, ordered according to importance by SVCCA, with each ˜zzzli
r=1 α(li)

j = (cid:80)m

jr zzzli
r .

j being a

For different values of k < m, we can then restrict layer li’s output to lie in the subspace of
span(˜zzzli
k ), the most useful k-dimensional subspace as found by SVCCA, done by projecting
each neuron into this k dimensional space.

1 , . . . , ˜zzzli

We ﬁnd — somewhat surprisingly — that very few SVCCA directions are required for the network
to perform the task well. As shown in Figure 2(a), for a network trained on CIFAR-10, the ﬁrst
25 dimensions provide nearly the same accuracy as using all 512 dimensions of a fully connected
layer with 512 neurons. The accuracy curve rises rapidly with the ﬁrst few SVCCA directions, and
plateaus quickly afterwards, for k (cid:28) m. This suggests that the useful information contained in m
neurons is well summarized by the subspace formed by the top k SVCCA directions. Two base-
lines for comparison are picking random and maximum activation neuron aligned subspaces and
projecting outputs onto these. Both of these baselines require far more directions (in this case: neu-
rons) before matching the accuracy achieved by the SVCCA directions. These results also suggest
approaches to model compression, which are explored in more detail in Section 4.4.

Figure 2(b) next demonstrates that these useful SVCCA directions are at least somewhat distributed
over neurons rather than axis-aligned. First, the top k SVCCA directions are picked and the rep-
resentation is projected onto this subspace. Next, the representation is further projected onto m
neurons, where the m are chosen as those most important to the SVCCA directions . The resulting
accuracy is plotted for different choices of k (given by x-axis) and different choices of m (different
lines). That, for example, keeping even 100 fc1 neurons (dashed green line) cannot maintain the
accuracy of the ﬁrst 20 SVCCA directions (solid green line at x-axis 20) suggests that those 20
SVCCA directions are distributed across 5 or more neurons each, on average. Figure 3 shows a
further demonstration of the effect on the output of projecting onto top SVCCA directions, here for
the toy regression case.

Why the two step SV + CCA method is needed. Both SVD and CCA have important properties
for analysing network representations and SVCCA consequently beneﬁts greatly from being a two
step method. CCA is invariant to afﬁne transformations, enabling comparisons without natural
alignment (e.g. different architectures, Section 4.4). See Appendix B for proofs and a demonstrative
ﬁgure. While CCA is a powerful method, it also suffers from certain shortcomings, particularly in
determining how many directions were important to the original space X, which is the strength of

4

Figure 3: The effect on the output of a latent representation being projected onto top SVCCA directions in
the toy regression task. Representations of the penultimate layer are projected onto 2, 6, 15, 30 top SVCCA
directions (from second pane). By 30, the output looks very similar to the full 200 neuron output (left).

SVD. See Appendix for an example where naive CCA performs badly. Both the SVD and CCA
steps are critical to the analysis of learning dynamics in Section 4.1.

3 Scaling SVCCA for Convolutional Layers

Applying SVCCA to convolutional layers can be done in two natural ways:

(1) Same layer comparisons: If X, Y are the same layer (at different timesteps or across ran-
dom initializations) receiving the same input we can concatenate along the pixel (height h,
width w) coordinates to form a vector: a conv layer h × w × c maps to c vectors, each
of dimension hwd, where d is the number of datapoints. This is a natural choice because
neurons at different pixel coordinates see different image data patches to each other. When
X, Y are two versions of the same layer, these c different views correspond perfectly.

(2) Different layer comparisons: When X, Y are not the same layer, the image patches seen by
different neurons have no natural correspondence. But we can ﬂatten an h×w ×c conv into
hwc neurons, each of dimension d. This approach is valid for convs in different networks
or at different depths.

3.1 Scaling SVCCA with Discrete Fourier Transforms

Applying SVCCA to convolutions introduces a computational challenge: the number of neurons
(h×w×c) in convolutional layers, especially early ones, is very large, making SVCCA prohibitively
expensive due to the large matrices involved. Luckily the problem of approximate dimensionality
reduction of large matrices is well studied, and efﬁcient algorithms exist, e.g. [4].

For convolutional layers however, we can avoid dimensionality reduction and perform exact
SVCCA, even for large networks. This is achieved by preprocessing each channel with a Discrete
Fourier Transform (which preserves CCA due to invariances, see Appendix), causing all (covari-
ance) matrices to be block-diagonal. This allows all matrix operations to be performed block by
block, and only over the diagonal blocks, vastly reducing computation. We show:

Theorem 1. Suppose we have a translation invariant (image) dataset X and convolutional layers
l1, l2. Letting DF T (li) denote the discrete fourier transform applied to each channel of li, the
covariance cov(DF T (l1), DF T (l2)) is block diagonal, with blocks of size c × c.

We make only two assumptions: 1) all layers below l1, l2 are either conv or pooling layers with
circular boundary conditions (translation equivariance) 2) The dataset X has all translations of the
images Xi. This is necessary in the proof for certain symmetries in neuron activations, but these
symmetries typically exist in natural images even without translation invariance, as shown in Fig-
ure App.2 in the Appendix. Below are key statements, with proofs in Appendix.

Deﬁnition 1. Say a single channel image dataset X of images is translation invariant if for any
(wlog n × n) image Xi ∈ X, with pixel values {zzz11, ...zzznn}, X (a,b)
= {zzzσa(1)σb(1), ...zzzσa(n)σb(n)}
is also in X, for all 0 ≤ a, b ≤ n − 1, where σa(i) = a + i mod n (and similarly for b).

i

For a multiple channel image Xi, an (a, b) translation is an (a, b) height/width shift on every channel
separately. X is then translation invariant as above.

5

To prove Theorem 1, we ﬁrst show another theorem:
Theorem 2. Given a translation invariant dataset X, and a convolutional layer l with channels
{c1, . . . ck} applied to X

(a) the DFT of ci, F cF T has diagonal covariance matrix (with itself).
(b) the DFT of ci, cj, F ciF T , F cjF T have diagonal covariance with each other.
Finally, both of these theorems rely on properties of circulant matrices and their DFTs:
Lemma 1. The covariance matrix of ci applied to translation invariant X is circulant and block
circulant.
Lemma 2. The DFT of a circulant matrix is diagonal.

4 Applications of SVCCA

4.1 Learning Dynamics with SVCCA

We can use SVCCA as a window into learning dynamics by comparing the representation at a
layer at different points during training to its ﬁnal representation. Furthermore, as the SVCCA
computations are relatively cheap to compute compared to methods that require training an auxiliary
network for each comparison [1, 10, 11], we can compare all layers during training at all timesteps
to all layers at the ﬁnal time step, producing a rich view into the learning process.

The outputs of SVCCA are the aligned directions (˜xi, ˜yi), how well they align, ρi, as well as in-
termediate output from the ﬁrst step, of singular values and directions, λ(i)
Y , y(cid:48)(j). We
condense these outputs into a single value, the SVCCA similarity ¯ρ, that encapsulates how well the
representations of two layers are aligned with each other,

X , x(cid:48)(i), λ(j)

¯ρ =

1
min (m1, m2)

(cid:88)

ρi,

i

(1)

where min (m1, m2) is the size of the smaller of the two layers being compared. The SVCCA
similarity ¯ρ is the average correlation across aligned directions, and is a direct multidimensional
analogue of Pearson correlation.

The SVCCA similarity for all pairs of layers, and all time steps, is shown in Figure 4 for a convnet
and a resnet architecture trained on CIFAR10.

4.2 Freeze Training

Observing in Figure 4 that networks broadly converge from the bottom up, we propose a training
method where we successively freeze lower layers during training, only updating higher and higher
layers, saving all computation needed for deriving gradients and updating in lower layers.

We apply this method to convolutional and residual networks trained on CIFAR-10, Figure 5, using
a linear freezing regime: in the convolutional network, each layer is frozen at a fraction (layer num-
ber/total layers) of total training time, while for resnets, each residual block is frozen at a fraction
(block number/total blocks). The vertical grey dotted lines show which steps have another set of lay-
ers frozen. Aside from saving computation, Freeze Training appears to actively help generalization
accuracy, like early stopping but with different layers requiring different stopping points.

4.3

Interpreting Representations: when are classes learned?

We also can use SVCCA to compare how correlated representations in each layer are with the logits
of each class in order to measure how knowledge about the target evolves throughout the network.
In Figure 6 we apply the DFT CCA technique on the Imagenet Resnet [6]. We take ﬁve different
classes and for different layers in the network, compute the DFT CCA similarity between the logit
of that class and the network layer. The results successfully reﬂect semantic aspects of the classes:
the ﬁretruck class sensitivity line is clearly distinct from the two pairs of dog breeds, and network
develops greater sensitivity to ﬁretruck earlier on. The two pairs of dog breeds, purposefully chosen
so that each pair is similar to the other in appearance, have cca similarity lines that are very close to
each other through the network, indicating these classes are similar to each other.

6

Figure 4: Learning dynamics plots for conv (top) and res (bottom) nets trained on CIFAR-10. Each pane is
a matrix of size layers × layers, with each entry showing the SVCCA similarity ¯ρ between the two layers.
Note that learning broadly happens ‘bottom up’ – layers closer to the input seem to solidify into their ﬁnal
representations with the exception of the very top layers. Per layer plots are included in the Appendix. Other
patterns are also visible – batch norm layers maintain nearly perfect similarity to the layer preceding them due
to scaling invariance (with a slight reduction since batch norm changes the SVD directions which capture 99%
of the variance). In the resnet plot, we see a stripe like pattern due to skip connections inducing high similarities
to previous layers.

Figure 5: Freeze Training reduces training cost and improves generalization. We apply Freeze Training to a
convolutional network on CIFAR-10 and a residual network on CIFAR-10. As shown by the grey dotted lines
(which indicate the timestep at which another layer is frozen), both networks have a ‘linear’ freezing regime:
for the convolutional network, we freeze individual layers at evenly spaced timesteps throughout training. For
the residual network, we freeze entire residual blocks at each freeze step. The curves were averaged over ten
runs.

4.4 Other Applications: Cross Model Comparison and compression

SVCCA similarity can also be used to compare the similarity of representations across different
random initializations, and even different architectures. We compare convolutional networks on
CIFAR-10 across random initializations (Appendix) and also a convolutional network to a residual
network in Figure 7, using the DFT method described in 3.

In Figure 3, we saw that projecting onto the subspace of the top few SVCCA directions resulted in
comparable accuracy. This observations motivates an approach to model compression. In particular,
letting the output vector of layer l be xxx(l) ∈ Rn×1, and the weights W (l), we replace the usual
W (l)xxx(l) with (W (l)P T
x )(Pxxxx(l)) where Px is a k × n projection matrix, projecting xxx onto the top
SVCCA directions. This bottleneck reduces both parameter count and inference computational cost

7

Figure 6: We plot the CCA similarity using the Discrete Fourier Transform between the logits of ﬁve classes
and layers in the Imagenet Resnet. The classes are ﬁretruck and two pairs of dog breeds (terriers and husky
like dogs: husky and eskimo dog) that are chosen to be similar to each other. These semantic properties are
captured in CCA similarity, where we see that the line corresponding to ﬁretruck is clearly distinct from the
two pairs of dog breeds, and the two lines in each pair are both very close to each other, reﬂecting the fact that
each pair consists of visually similar looking images. Firetruck also appears to be easier for the network to
learn, with greater sensitivity displayed much sooner.

Figure 7: We plot the CCA similarity using the Discrete Fourier Transform between convolutional layers of a
Resnet and Convnet trained on CIFAR-10. We ﬁnd that the lower layrs of both models are noticeably similar to
each other, and get progressively less similar as we compare higher layers. Note that the highest layers of the
resnet are least similar to the lower layers of the convnet.

for the layer by a factor ∼ k
n . In Figure App.5 in the Appendix, we show that we can consecutively
compress top layers with SVCCA by a signiﬁcant amount (in one case reducing each layer to 0.35
original size) and hardly affect performance.

5 Conclusion

In this paper we present SVCCA, a general method which allows for comparison of the learned dis-
tributed representations between different neural network layers and architectures. Using SVCCA
we obtain novel insights into the learning dynamics and learned representations of common neural
network architectures. These insights motivated a new Freeze Training technique which can reduce
the number of ﬂops required to train networks and potentially even increase generalization perfor-
mance. We observe that CCA similarity can be a helpful tool for interpretability, with sensitivity
to different classes reﬂecting their semantic properties. This technique also motivates a new algo-
rithm for model compression. Finally, the “lower layers learn ﬁrst” behavior was also observed for
recurrent neural networks as shown in Figure App.6 in the Appendix.

8

References

[1] Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classiﬁer

probes. arXiv preprint arXiv:1610.01644, 2016.

[2] David Eigen, Jason Rolfe, Rob Fergus, and Yann LeCun. Understanding deep architectures

using a recursive convolutional network. arXiv preprint arXiv:1312.1847, 2013.

[3] Manaal Faruqui and Chris Dyer. Improving vector space word representations using multilin-

gual correlation. Association for Computational Linguistics, 2014.

[4] Nathan Halko, Martinsson Per-Gunnar, and Joel A. Tropp. Finding structure with random-
ness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM
Rev., 53:217–288, 2011.

[5] D. R. Hardoon, S. Szedmak, and J. Shawe-Taylor. Canonical correlation analysis: An overview

with application to learning methods. Neural Computation, 16:2639–2664, 2004.

[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image

recognition. CoRR, abs/1512.03385, 2015.

[7] Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel-rahman Mohamed, Navdeep
Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neu-
ral networks for acoustic modeling in speech recognition: The shared views of four research
groups. IEEE Signal Processing Magazine, 29(6):82–97, 2012.

[8] Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 1985.
[9] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems, pages
1097–1105, 2012.

[10] Karel Lenc and Andrea Vedaldi. Understanding image representations by measuring their
equivariance and equivalence. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 991–999, 2015.

[11] Y. Li, J. Yosinski, J. Clune, H. Lipson, and J. Hopcroft. Convergent Learning: Do different
In International Conference on Learning

neural networks learn the same representations?
Representations (ICLR), May 2016.

[12] Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. Convergent learning:
Do different neural networks learn the same representations? In Feature Extraction: Modern
Questions and Challenges, pages 196–212, 2015.

[13] Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by in-
verting them. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-
nition, pages 5188–5196, 2015.

[14] Gr´egoire Montavon, Mikio L Braun, and Klaus-Robert M¨uller. Kernel analysis of deep net-

works. Journal of Machine Learning Research, 12(Sep):2563–2581, 2011.

[15] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional
arXiv preprint

networks: Visualising image classiﬁcation models and saliency maps.
arXiv:1312.6034, 2013.

[16] David Sussillo, Mark M Churchland, Matthew T Kaufman, and Krishna V Shenoy. A neural
network that ﬁnds a naturalistic solution for the production of muscle activity. Nature neuro-
science, 18(7):1025–1033, 2015.

[17] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
arXiv preprint

Intriguing properties of neural networks.

Goodfellow, and Rob Fergus.
arXiv:1312.6199, 2013.

[18] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural ma-
chine translation system: Bridging the gap between human and machine translation. arXiv
preprint arXiv:1609.08144, 2016.

[19] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding
neural networks through deep visualization. In Deep Learning Workshop, International Con-
ference on Machine Learning (ICML), 2015.

9

[20] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In

European conference on computer vision, pages 818–833. Springer, 2014.

[21] Bolei Zhou, Aditya Khosla, `Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object de-
tectors emerge in deep scene cnns. In International Conference on Learning Representations
(ICLR), volume abs/1412.6856, 2014.

10

Appendix

A Mathematical details of CCA and SVCCA

Canonical Correlation of X, Y
ﬁnding a, b to maximise:

Finding maximal correlations between X, Y can be expressed as

aT ΣXY b

(cid:112)

(cid:112)

aT ΣXX a

bT ΣY Y b

where ΣXX , ΣXY , ΣY X , ΣY Y are the covariance and cross-covariance terms. By performing the
change of basis ˜x˜x˜x1 = Σ1/2
Y Y b and using Cauchy-Schwarz we recover an eigenvalue
problem:

xx a and ˜y˜y˜y1 = Σ1/2

˜x˜x˜x1 = argmax

(cid:34)

xT Σ−1/2

XX ΣXY Σ−1
||x||

Y Y ΣY X Σ−1/2
XX x

(cid:35)

(*)

SVCCA Given two subspaces X = {xxx1, ..., xxxm1 }, Y = {yyy1, ..., yyym2}, SVCCA ﬁrst performs a
singular value decomposition on X, Y . This results in singular vectors {x(cid:48)x(cid:48)x(cid:48)
m1 } with associ-
ated singular values {λ1, ..., λm1} (for X, and similarly for Y ). Of these m1 singular vectors, we
keep the top m(cid:48)
i=1 |λi|). That is, 99% of
the variation of X is explainable by the top m(cid:48)
1 vectors. This helps remove directions/neurons that
are constant zero, or noise with small magnitude.
Then, we apply Canonical Correlation Analysis (CCA) to the sets {x(cid:48)x(cid:48)x(cid:48)
top singular vectors.

1 is the smallest value that (cid:80)m(cid:48)

i=1 |λi|(≥ 0.99 (cid:80)m1

1 where m(cid:48)

1, ..., y(cid:48)y(cid:48)y(cid:48)

1, ..., x(cid:48)x(cid:48)x(cid:48)

1, ..., x(cid:48)x(cid:48)x(cid:48)

}, {y(cid:48)y(cid:48)y(cid:48)

} of

m(cid:48)
2

m(cid:48)
1

1

CCA is a well established statistical method for understanding the similarity of two different sets
of random variables – given our two sets of vectors {x(cid:48)x(cid:48)x(cid:48)
}, we wish to ﬁnd
linear transformations, WX , WY that maximally correlate the subspaces. This can be reduced to
an eigenvalue problem. Solving this results in linearly transformed subspaces ˜X, ˜Y with directions
˜xxxi, ˜yyyi that are maximally correlated with each other, and orthogonal to ˜xxxj, ˜yyyj, j < i. We let ρi =
corr(˜xxxi, ˜yyyi). In summary, we have:

1, ..., y(cid:48)y(cid:48)y(cid:48)

1, ..., x(cid:48)x(cid:48)x(cid:48)

}, {y(cid:48)y(cid:48)y(cid:48)

m(cid:48)
1

m(cid:48)
2

SVCCA Summary

1. Input: X, Y
2. Perform: SVD(X), SVD(Y). Output: X (cid:48) = U X, Y (cid:48) = V Y
3. Perform CCA(X (cid:48), Y (cid:48)). Output:

{ρ1, . . . ρmin(m1,m2)}

˜X = WX X (cid:48), ˜Y = WY Y (cid:48) and corrs =

B Additional Proofs and Figures from Section 2.1

Proof of Orthonormal and Scaling Invariance of CCA:

We can see this using equation (*) as follows: suppose U, V are orthonormal transforms applied to
the sets X, Y . Then it follows that Σa
XX becomes U Σa
XX U T , for a = {1, −1, 1/2, −1/2}, and
similarly for Y and V . Also note ΣXY becomes U ΣXY V T . Equation (*) then becomes
xT U Σ−1/2

(cid:34)

(cid:35)

˜x1 = argmax

Y Y ΣY X Σ−1/2
XX ΣXY Σ−1
||x||

XX U T x

So if ˜u is a solution to equation (*), then U ˜u is a solution to the equation above, which results in the
same correlation coefﬁcients.

B.0.1 The importance of SVD: how many directions matter?

While CCA is excellent at identifying useful learned directions that correlate, independent of certain
common transforms, it doesn’t capture the full picture entirely. Consider the following setting:

11

Figure App.1: This ﬁgure shows the ability of CCA to deal with orthogonal and scaling transforms.
In the ﬁrst pane, the maroon plot shows one of the highest activation neurons in the penultimate
layer of a network trained on CIFAR-10, with the x-axis being (ordered) image ids and the y-axis
being activation on that image. The green plots show two resulting distorted directions after this and
two of the other top activation neurons are permuted, rotated and scaled. Pane two shows the result
of applying CCA to the distorted directions and the original signal, which succeeds in recovering
the original signal.

suppose we have subspaces A, B, C, with A being 50 dimensions, B being 200 dimensions, 50 of
which are perfectly aligned with A and the other 150 being noise, and C being 200 dimensions, 50
of which are aligned with A (and B) and the other 150 being useful, but different directions.

Then looking at the canonical correlation coefﬁcients of (A, B) and (A, C) will give the same result,
both being 1 for 50 values and 0 for everything else. But these are two very different cases – the
subspace B is indeed well represented by the 50 directions that are aligned with A. But the subspace
C has 150 more useful directions.

This distinction becomes particularly important when aggregating canonical correlation coefﬁcients
as a measure of similarity, as used in analysing network learning dynamics. However, by ﬁrst ap-
plying SVD to determine the number of directions needed to explain 99% of the observed variance,
we can distinguish between pathological cases like the one above.

C Proof of Theorem 1

Here we provide the proofs for Lemma 1, Lemma 2, Theorem 2 and ﬁnally Theorem 1.

A preliminary note before we begin:

When we consider a (wlog) n by n channel c of a convolutional layer, we assume it has shape







zzz0,0
zzz1,0
...
zzzn−1,0

zzz1,2
zzz2,2
...
zzzn−1,1

. . .
. . .
. . .
. . .







zzz0,n−1
zzz1,n−1
...
zzzn−1,n−1

12

(a)

(b)

(c)

(d)

Figure App.2:
This ﬁgure visualizes the covariance matrix of one of the channels of a resnet
trained on Imagenet. Black correspond to large values and white to small values. (a) we compute the
covariance without a translation invariant dataset and without ﬁrst preprocessing the images by DFT.
We see that the covariance matrix is dense. (b) We compute the covariance after applying DFT, but
without augmenting the dataset with translations. Even without enforcing translation invariance, we
see that the covariance in the DFT basis is approximately diagonal. (c) Same as (a), but the dataset
is augmented to be fully translation invariant. The covariance in the pixel basis is still dense. (d)
Same as (c), but with dataset augmented to be translation invariant. The covariance matrix is exactly
diagonal for a translation invariant dataset in a DFT basis.

When computing the covariance matrix however, we vectorize c by stacking the columns under each
other, and call the result vec(c):

vec(c) =

:=














zzz0,0
zzz1,0
...
zzzn−1,0
zzz0,1
...
zzzn−1,n−1








































zzz0
zzz1
...
zzzn−1
zzzn
...
zzzn2−1

vec(AcB) = (BT ⊗ A)vec(c)

One useful identity when switching between these two notations (see e.g. [8]) is

where A, B are matrices and ⊗ is the Kronecker product. A useful observation arising from this is:
Lemma 3. The CCA vectors of DF T (ci), DF T (cj) are the same (up to a rotation by F ) as the
CCA of ci, cj.

Proof: From Section B we know that unitary transforms only rotate the CCA directions. But while
DFT pre and postmultiplies by F, F T – unitary matrices, we cannot directly apply this as the result
is for unitary transforms on vec(ci). But, using the identity above, we see that vec(DF T (ci)) =
vec(F ciF T ) = (F ⊗ F )vec(ci), which is unitary as F is unitary. Applying the same identity to cj,
we can thus conclude that the DFT preserves CCA (up to rotations).

As Theorem 1 preprocesses the neurons with DFT, it is important to note that by the Lemma above,
we do not change the CCA vectors (except by a rotation).

C.1 Proof of Lemma 1

Proof. Translation invariance is preserved We show inductively that any translation invariant input
to a convolutional channel results in a translation invariant output: Suppose the input to channel c,
(n by n) is translation invariant. It is sufﬁcient to show that for inputs Xi, Xj and 0 ≤ a, b, ≤ n − 1,
c(Xi) + (a, b) mod n = c(Xj). But an (a, b) shift in neuron coordinates in c corresponds to a
(height stride · a, width stride · b) shift in the input. And as X is translation invariant, there is some
Xj = Xi + (height stride · a, width stride · b).

cov(c) is circulant:

13

Let X be (by proof above) a translation invariant input to a channel c in some convolution or pooling
layer. The empirical covariance, cov(c) is the n2 by n2 matrix computed by (assuming c is centered)

1
|X|

(cid:88)

Xi∈X

vec(c(Xi)) · vec(c(Xi))T

So, cov(c)ij = 1
j.

|X|zzzT

i zzzj = 1
|X|

(cid:80)

Xl∈X zzzT

i (Xl)zzzj(Xl), i.e. the inner products of the neurons i and

The indexes i and j refer to the neurons in their vectorized order in vec(c). But in the matrix ordering
of neurons in c, i and j correspond to some (a1, b1) and (a2, b2). If we applied a translation (a, b),
to both, we would get new neuron coordinates (a1 + a, b1 + b), (a2 + a, b2 + b) (all coordinates
mod n) which would correspond to i + an + b mod n2 and j + an + b mod n2, by our stacking
of columns and reindexing.

Let τa,b be the translation in inputs corresponding to an (a, b) translation in c, i.e.
τa,b =
(height stride·a, width stride·b). Then clearly zzz(a1,b1)(Xi) = zzz(a1+a,b1+b)(τ(a,b)(Xi), and similarly
for zzz(a2,b2)
It follows that 1

(a1+b,b1+b)zzz(a2+a,b2+b), or, with vec(c) indexing

(a1,b1)zzz(a2,b2) = 1

|X|zzzT

|X|zzzT

1
|X|

1
|X|

zzzT
i zzzj =

zzzT
(i+an+b mod n2)zzz(j+an+b mod n2)

This gives us the circulant structure of cov(c).

cov(c) is block circulant: Let zzz(i) be the ith column of c, and zzz(j) the jth. In vec(c), these correspond
to zzz(i−1)n, . . . zzzin−1 and zzz(j−1)n, . . . zzzjn−1, and the n by n submatrix at those row and column in-
dexes of cov(vec(c)) corresponds to the covariance of column i, j. But then we see that the covari-
ance of columns i+k, j +k, corresponding to the covariance of neurons zzz(i−1)n+k·n, . . . zzzin−1+k·n,
and zzz(j−1)n+k·n, . . . zzzjn−1+k·n, which corresponds to the 2-d shift (1, 0), applied to every neuron.
So by an identical argument to above, we see that for all 0 ≤ k ≤ n − 1

cov(zzz(i), zzz(j)) = cov(zzz(i+k), zzz(j+k))

In particular, cov(vec(c)) is block circulant.

An example cov(vec(c)) with c being 3 by 3 look like below:

(cid:35)

(cid:34)A0 A1 A2
A2 A0 A1
A1 A2 A0

where each Ai is itself a circulant matrix.

C.2 Proof of Lemma 2

Proof. This is a standard result, following from expressing a circulant matrix A in terms of its
diagonal form , i.e. A = V ΣV T with the columns of V being its eigenvectors. Noting that V = F ,
the DFT matrix, and that vectors of powers of ωk = exp( 2πik
n ) are orthogonal
gives the result.

n ), ωj = exp( 2πik

C.3 Proof of Theorem 2

Proof. Starting with (a), we need to show that cov(vec(DF T (ci)), vec(DF T (ci)) is diagonal. But
by the identity above, this becomes:

cov(vec(DF T (ci)), vec(DF T (ci)) = (F ⊗ F )vec(ci)vec(ci)T (F ⊗ F )∗

14

By Lemma 1, we see that

cov(vec(ci)) = vec(ci)vec(ci)T =







A0

A1
An−1 A0
...
A2

...
A1

. . . An−1
. . . An−2
. . .
. . .

...
A0







with each Ai circulant.

And so cov(vec(DF T (ci)), vec(DF T (ci)) becomes







f00F
f10F
...

. . .
. . .
. . .
fn−1,0F fn−1,1F . . .

f01F
f11F
...

f0,n−1F
f1,n−1F
...
fn−1,n−1F













A0

A1
An−1 A0
...
A2

...
A1

. . . An−1
. . . An−2
. . .
. . .

...
A0













f ∗
00F ∗
01F ∗
f ∗
...
0,n−1F ∗
f ∗

f ∗
10F ∗
11F ∗
f ∗
...
1,n−1F ∗
f ∗

. . .
. . .
. . .
. . .

f ∗
n−1,0F ∗
n−1,1F ∗
f ∗
...
n−1,n−1F ∗
f ∗







From this, we see that the sjth entry has the form

n−1
(cid:88)

(cid:32)n−1
(cid:88)

l=0

k=0

fskF Al−k

ljF ∗ =
f ∗

fskf ∗

ljF Al−kF ∗

(cid:33)

(cid:88)

k,l

Letting [F ArF ∗] denote the coefﬁcient of the term F ArF ∗, we see that (addition being mod n)

[F ArF ∗] =

fskf ∗

(k+r)j =

2πisk
n

e

· e

−2πij(k+r)
n

−2πijr
n

= e

2πik(s−j)
n

e

= e

−2πijr
n

· δsj

n−1
(cid:88)

k=0

(cid:88)

k

n−1
(cid:88)

k=0

with the last step following by the fact that the sum of powers of non trivial roots of unity are 0.

In particular, we see that only the diagonal entries (of the n by n matrix of matrices) are non zero.
The diagonal elements are linear combinations of terms of form F ArF ∗, and by Lemma 2 these are
diagonal. So the covariance of the DFT is diagonal as desired.

Part (b) follows almost identically to part (a), but by ﬁrst noting that exactly by the proof of Lemma
1, cov(ci, cj) is also a circulant and block circulant matrix.

C.4 Proof of Theorem 1

Proof. This Theorem now follows easily from the previous. Suppose we have a layer l, with chan-
nels c1, ..., ck. And let vec(DF T (ci)) have directions ˜zzz(i)
n2−1. By the previous theorem, we
k , ˜zzz(j)
know that the covariance of all of these neurons only has non-zero terms cov(˜zzz(i)
k .
So arranging the
0 , . . . ˜zzz(k)
0 , ˜zzz(1)
˜zzz(1)
diagonal of the matrix, proving the theorem.

row and column indexes being
the nonzero terms all live in the n2 k by k blocks down the

covariance matrix to have

full
. . . ˜zzz(k)
n2

0 , · · · ˜zzz(i)

0 , ˜zzz(1)

1

C.5 Computational Gains

As the covariance matrix is block diagonal, our more efﬁcient algorithm for computation is as fol-
lows: take the DFT of every channel (n log n due to FFT) and then compute covariances according
to blocks: partition the kn directions into the n2 k by k matrices that are non-zero, and compute the
covariance, inverses and square roots along these.
A rough computational budget for the covariance is therefore kn log n + n2k2.5, while the naive
computation would be of order (kn2)2.5, a polynomial difference. Furthermore, the DFT method
also makes for easy parallelization as each of the n2 blocks does not interact with any of the others.

15

Figure App.3: Learning dynamics per layer plots for conv (left pane) and res (right pane) nets trained on
CIFAR-10. Each line plots the SVCCA similarity of each layer with its ﬁnal representation, as a function of
training step, for both the conv (left pane) and res (right pane) nets. Note the bottom up convergence of different
layers

D Per Layer Learning Dynamics Plots from Section 4.1

E Additional Figure from Section 4.4

Figure App.4 compares the converged representations of two different initializations of the same
convolutional network on CIFAR-10.

Figure App.4: Comparing the converged representations of two different initializations of the same
convolutional architecture. The results support ﬁndings in [12], where initial and ﬁnal layers are
found to be similar, with middle layers differing in representation similarity.

F Experiment from Section 4.4

G Learning Dynamics for an LSTM

16

Figure App.5: Using SVCCA to perform model compression on the fully connected layers in a CIFAR-
10 convnet. The two gray lines indicate the original train (top) and test (bottom) accuracy. The two sets of
representations for SVCCA are obtained through 1) two different initialization and training of convnets on
CIFAR-10 2) the layer activations and the activations of the logits. The latter provides better results, with the
ﬁnal ﬁve layers: pool1, fc1, bn3, fc2 and bn4 all being compressed to 0.35 of their original size.

Figure App.6: Learning dynamics of the different layers of a stacked LSTM trained on the Penn Tree
Bank language modeling task. We observe a similar pattern to that of convolutional architectures
trained on image data: lower layer converge faster than upper layers.

17

