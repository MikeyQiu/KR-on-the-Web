Estimation of Personalized Effects Associated With Causal Pathways

8
1
0
2
 
p
e
S
 
7
2
 
 
]

G
L
.
s
c
[
 
 
1
v
1
9
7
0
1
.
9
0
8
1
:
v
i
X
r
a

Razieh Nabi
Computer Science Dept.
Johns Hopkins University
Baltimore, MD, 21218

Phyllis Kanki
Immunology and Infectious Diseases Dept.
Harvard T. H. Chan School of Public Health
Boston, MA, 02115

Ilya Shpitser
Computer Science Dept.
Johns Hopkins University
Baltimore, MD, 21218

Abstract

The goal of personalized decision making is to
map a unit’s characteristics to an action tai-
lored to maximize the expected outcome for
that unit. Obtaining high-quality mappings of
this type is the goal of the dynamic regime lit-
erature. In healthcare settings, optimizing poli-
cies with respect to a particular causal pathway
may be of interest as well. For example, we
may wish to maximize the chemical effect of
a drug given data from an observational study
where the chemical effect of the drug on the
outcome is entangled with the indirect effect
mediated by differential adherence.
In such
cases, we may wish to optimize the direct ef-
fect of a drug, while keeping the indirect effect
to that of some reference treatment. [16] shows
how to combine mediation analysis and dy-
namic treatment regime ideas to deﬁnes poli-
cies associated with causal pathways and coun-
terfactual responses to these policies. In this
paper, we derive a variety of methods for learn-
ing high quality policies of this type from data,
in a causal model corresponding to a longitu-
dinal setting of practical importance. We illus-
trate our methods via a dataset of HIV patients
undergoing therapy, gathered in the Nigerian
PEPFAR program.

1

INTRODUCTION

There has been growing interest in making personal-
ized decisions in different domains to account for in-
herent heterogeneity among individuals and optimize
individual-level experiences. For instance, personalized
medicine aims at systematic use of individual patient his-
tory including biological information and biomarkers to

improve patient’s health care. Personalized actions can
be viewed as realizations of decision rules where avail-
able information is mapped to the space of possible de-
cisions.

Making good personalized decisions often involves act-
ing in multiple stages. For instance, multiple successive
medical interventions may be required for long-term care
of patients with chronic diseases. The goal of personal-
ized medicine is to tailor a sequence of decision rules on
treatment, known as dynamic treatment regimes, based
on patient characteristics seen so far, to maximize the
likelihood of a desirable outcome. A number of algo-
rithms have been developed for ﬁnding optimal treatment
regimes from either observational data, or data from ex-
periments tailored for providing information on regime
quality, such as sequential multiple assignment random-
ized trials (SMARTs) [3, 6]. These algorithms use meth-
ods from causal inference, and aim to predict counter-
factual outcomes under policies different from those ac-
tually followed in the data [11, 5, 2].

A natural extension of these methods is ﬁnding treatment
regimes that optimizes a part of the effect of the treat-
ment on the outcome. We illustrate the utility of this
problem with the following example. Patients infected
with human immunodeﬁciency virus (HIV) are typically
put on courses of antiretroviral therapy, as a ﬁrst line of
therapy. Although these sort of medications are effective
in combating the disease, the full beneﬁt is not realized
since patients often do not fully adhere to the medica-
tion regimen. One of the causes of poor adherence to
the therapy is toxicity of the medication. Hence, viral
failure in a patient receiving treatment may be attributed
to either poor drug effectiveness or lack of adherence to
an otherwise effective treatment plan. In observational
studies of HIV patients, treatments are not randomly as-
signed, and patients have differential adherence. Because
of this, ﬁnding a policy that optimizes the overall effect
of the treatment plan on the outcome entangles two very
different causal pathways – the chemical pathway asso-

ciated with the active ingredients in the treatment, and
the pathway associated with adherence.

We may, instead, consider the problem of ﬁnding a set
of policies that optimize only the direct chemical effect
of the drug, in the counterfactual situations where the
indirect effect mediated by adherence can be kept to that
of some reference treatment. Policies of this type may
be more directly relevant in precision medicine contexts
where adherence varies among patients.

[16] deﬁnes counterfactual responses to policies that set
treatments only with respect to a particular causal path-
way, and gives a general identiﬁcation algorithm for
these responses, as a generalization of similar algorithms
for standard dynamic treatment regimes [20], and effects
associated with causal pathways in mediation analysis
[15]. In this paper, we consider algorithms that can be
used to ﬁnd policies that maximize effects along particu-
lar causal pathways from data, in a causal model of most
immediate relevance in longitudinal settings.

The paper is organized as follows. We ﬁx our notation,
and deﬁne counterfactual responses to treatment policies
and policies associated with pathways in Section 2. In
Section 3, we review existing techniques used in learning
policies optimizing the overall effect of actions. In Sec-
tion 4, we ﬁx the causal model corresponding to a typical
longitudinal study, prove identiﬁcation of counterfactual
policy responses under this model, and show how tech-
niques for maximizing treatment policies, described in
Section 4, may be extended to maximize policies associ-
ated with causal pathways. In Section 5, we illustrate the
methods we propose via an application involving data on
treatment of HIV patients in Nigeria. Our conclusions
are in Section 6. The Appendix contains the proofs of
all claims, a basic description of statistical inference in
semi-parametric models, as context for some of our esti-
mation strategies, visualizations of learned policies, and
descriptions of the experiments.

2 NOTATIONS AND PRELIMINARIES

Consider a multi-stage decision problem with K pre-
speciﬁed decision points, indexed by i = 1, . . . , K. Let
Y denote the ﬁnal outcome of interest and Ai denote the
action made at decision point i with the ﬁnite state space
of XAi. The set of all actions is denoted by A. Let W0
denote the available information prior to the ﬁrst deci-
sion, and Wi denote the information collected between
decisions i and i+1, (Y ≡ WK). Given Ai, denote Ai to
be all treatments administered from time 1 to i, similarly
for Wi and W i. We combine the treatment and covariate
history up to treatment decision Ai into a history vector
Hi. The state space of Hi is denoted by XHi.

We are interested in learning policies that map Hi to val-
ues of Ai, for all i, that maximize the expected value
of the outcome Y . Doing this from observed data en-
tails considering counterfactual outcomes Y had Ai been
assigned in a different way from what was actually ob-
served. We brieﬂy review graphical causal models, and
the potential outcome notation from causal inference,
which will be used to deﬁne such counterfactuals.

Causal models are sets of distributions deﬁned by restric-
tions associated with directed acyclic graphs (DAGs).
We will use vertices and variables interchangeably – cap-
ital letters for a vertex or variable (V ), bold capital letter
for a set (V), lowercase letters for values (v), and bold
lowercase letters for sets of values (v). By convention,
each graph is deﬁned on a vertex set V. For a set of val-
ues a of A, and a subset A† ⊆ A, deﬁne aA† to be a
restriction of a to elements in A†. For a DAG G, and any
V ∈ V, we deﬁne the parents of V ∈ V to be the set
paG(V ) ≡ {W ∈ V | W → V }, and the children of
V ∈ V to be the set chG(V ) ≡ {W ∈ V | V → W }.
Causal models of a DAG G consist of distributions de-
ﬁned on counterfactual random variables of the form
V (a) where a are values of paG(V ). These variables rep-
resent outcomes of V had all variables in paG(V ) been
set, possibly contrary to fact, to a. In this paper we as-
sume Pearl’s functional model for a DAG G with vertices
V which is the set containing any joint distribution over
all potential outcome random variables where the sets of
variables (cid:8){V (aV ) | aV ∈ XpaG (V )} | V ∈ V(cid:9) are mu-
tually independent [8]. The atomic counterfactuals in the
above set model the relationship between paG(V ), rep-
resenting direct causes of V , and V itself. From these,
all other counterfactuals may be deﬁned using recursive
substitution. For any A ⊆ V \ {V },

V (a) ≡ V (apaG (V )∩A, {paG(V ) \ A}(a)),

(1)

where {paG(V ) \ A}(a)) is taken to mean the (re-
cursively deﬁned) set of counterfactuals associated with
variables in paG(V ), had A been set to a.
A causal parameter is said to be identiﬁed in a causal
model if it is a function of the observed data distribution
p(V). In all causal models of a DAG G in the literature,
all interventional distributions p({V \ A}(a)) are iden-
tiﬁed by the g-formula:

p({V \ A}(a)) =

(cid:89)

p(V | paG(V ))(cid:12)

(cid:12)A=a

(2)

V ∈V\A

As an example, Y (a) in the DAG in Fig. 1 (a), is deﬁned
to be Y (a, M (a, W ), W ), and its distribution is identi-
ﬁed as (cid:80)
w,m p(Y |a, m, w)p(m|a, w)p(w).
In our se-
quential decision setting, the relevant counterfactual is
Y (aK), i.e.
the response of Y had the treatment his-
tory AK = aK been administered, possibly contrary to
fact. Comparison of Y (aK) and Y (a(cid:48)
K) in expectation,

U

W

M

W0

W1

W2

A

Y

(a)

A1

A2

(b)

Figure 1: (a) A simple causal DAG, with a single treat-
ment A, a single outcome Y , a vector W of baseline
variables, and a single mediator M . (b) A more complex
causal DAG with two treatments A1, A2, an intermediate
outcome W1, and the ﬁnal outcome W2.

E[Y (aK)] − E[Y (a(cid:48)
K)], where aK is the treatment his-
tory of interest, and a(cid:48)
K is the reference treatment history,
gives the average causal effect of aK on the outcome Y .

Counterfactual Response to Policies

A dynamic treatment regime fA = {fA1, . . . , fAK } is a
sequence of decision rules that forms a treatment plan for
patients over time. At the ith decision point, the ith rule
fAi maps the available information Hi prior to the ith
decision point to a treatment ai, i.e. fAi : XHi (cid:55)→ XAi.
Given a treatment regime fA, we deﬁne the counterfac-
tual response Y had A been assigned according to fA, or
Y (fA), as the following generalization of (1)

Y ({fAi (Hi(fA))|Ai ∈ paG (Y ) ∩ A}, {paG (Y ) \ A}(fA)).

(3)

Under a causal model associated with the DAG G, the
distribution p(Y (fA)), is identiﬁed as the following gen-
eralization of the g-formula

(cid:88)

(cid:89)

V\Y,A

V ∈V\A

p(V |{fAi (Hi)|Ai ∈ paG (V ) ∩ A},paG (V ) \ A)

(4)

As an example, Y (a = fA(W )) in Fig. 1 (a) is de-
ﬁned as Y (a = fA(W ), M (a = fA(W ), W ), W ),
and its distribution is identiﬁed as (cid:80)
w,m p(Y |a =
fA(w), m, w)p(m|a = fA(w), w)p(w). We will call
policies corresponding to counterfactuals deﬁned in (3)
overall policies, to distinguish them from policies asso-
ciated with causal pathways to be deﬁned later.

Mediation Analysis

An important goal of causal inference is understanding
the mechanisms by which the treatment A inﬂuences the
outcome Y . A common framework for mechanism anal-
ysis is mediation analysis which seeks to decompose the
effect of A on Y into the direct effect and the indirect
effect mediated by a third variable, or more generally
into components associated with particular causal path-
ways. Consider the graph in Fig. 1 (a): the direct effect
corresponds to the effect along the edge A → Y , and
indirect effect corresponds to the effect along the path
A → M → Y , mediated by M .

Counterfactuals associated with mediation analysis have
been deﬁned using a more general type of intervention in
a graphical causal model, namely the edge intervention
[17], which maps a set of directed edges in G to values
of their source vertices. Edge interventions have a natu-
ral interpretation in cases where a treatment variable has
multiple components that a) inﬂuence the outcome in dif-
ferent ways, b) occur or do not occur together in observed
data, and c) may in principle be intervened on separately.
For instance, smoking leads to poor health outcomes due
to two components: smoke inhalation and exposure to
nicotine. A smoker would be exposed to both of these
components, while a non-smoker to neither. However,
one might imagine exposing someone selectively only to
nicotine but not smoke inhalation (via a nicotine patch),
or only smoke inhalation but not nicotine (via smoking
plant matter not derived from tobacco leaves). These
types of hypothetical experiments correspond precisely
to edge interventions, and have been used to conceptual-
ize direct and indirect effects [12, 7].

We will write the mapping of a set of edges to values
of their source vertices as aα to mean edges in α are
mapped to values in the multiset a (since multiple edges
may share the same source vertex, and be assigned to
different values). For a subset β ⊆ α and an assignment
aα, denote aβ to be a restriction of aα to edges in β. For
simplicity, in the remainder of the paper we assume that
if (AW )→ ∈ α, then for all V ∈ chG(A), (AV )→ ∈ α,
where (XY )→ is a directed edge from X to Y.
We will write counterfactual responses to edge interven-
tions as Y (aα). An edge intervention that sets a set of
edges α to values in the multiset a is deﬁned via the fol-
lowing generalization of recursive substitution (1):

Y (aα) ≡ Y (a{(ZY )→∈α}, {pa ¯α

G(Y )}(aα)),

(5)

where pa ¯α
G(Y ) ≡ {W | (W Y )→ (cid:54)∈ α}. For exam-
ple, in the DAG in Fig. 1 (a), Y (a{(AY )→,(AM )→}) as-
signing (AY )→ to a and (AM )→ to a(cid:48) is deﬁned as
Y (a, M (a(cid:48), W ), W ).

Identiﬁcation of edge interventions in graphical causal
models of a DAG corresponds quite closely with identi-
ﬁcation of regular (node) interventions, as follows. Let
Aα ≡ {A | (AB)→ ∈ α}. Consider an edge interven-
tion given by the mapping aα. Then, under the functional
model of a DAG G, the joint distribution of counterfac-
tual responses p({V \ Aα}(aα)) is identiﬁed via the fol-
lowing generalization of (2) called the edge g-formula:

(cid:89)

V ∈V\Aα

p(V |a{(ZV )→∈α}, pa ¯α

G(V )).

(6)

For example, in Fig 1 (a), p(Y (a{(AY )→,(AM )→})) =
(cid:80)
W,M p(Y |a, M, W )p(M |a(cid:48), W )p(W ), which is ob-
tained by marginalizing W and M from the edge g-
formula.

Counterfactual Responses To Policies Associated
With Pathways

3 LEARNING OPTIMAL OVERALL

POLICIES

We now deﬁne counterfactual responses to policies that
operate only with respect to particular outgoing edges
from A. These counterfactuals generalize those in the
previous two sections.

As an example, we can view Fig. 1 (a) as representing
a cross-sectional study of HIV patients of the kind de-
scribed in [4], where W is a set of baseline character-
istics, A is one of a set of possible antiretroviral treat-
ments, M is adherence to treatment, and Y is a binary
In this type
outcome variable signifying viral failure.
of study, we may wish to ﬁnd fA(W ) that maximizes
the expected outcome Y had A been set according to
fA(W ) for the purposes of the direct effect of A on Y ,
and A were set to some reference level a(cid:48) for the pur-
poses of the effect of A on M . In other words, we may
wish to ﬁnd fA(W ) to maximize the counterfactual mean
E[Y (fA(W ), M (a(cid:48), W ), W )]. This would correspond
to ﬁnding a treatment policy that maximizes the direct
(chemical) effect, if it were possible to keep adherence to
a level M (a(cid:48)) as if a reference (easy to adhere to) treat-
ment a were given.

We now give a general deﬁnition for responses to such
path-speciﬁc policies. Fix a set of directed edges α, and
deﬁne Aα ≡ {A | (AB)→ ∈ α}. As before, we assume
if (AW )→ ∈ α, then for all V ∈ chG(A), (AV )→ ∈ α.
Deﬁne fα ≡ {f (AiW )→
: XHi (cid:55)→ XAi | (AiW )→ ∈ α}
as the set of policies associated with edges in α. Note that
fα may contain multiple policies for a given treatment
variable A.

Ai

Deﬁne Y (fα), the counterfactual response of Y to the set
of path-speciﬁc policies fα, as the following generaliza-
tion of (5) and (3):

Y ({f (AiY )→
Ai

(Hi(fα))|(AiY )→ ∈ α}, {pa ¯α

G(Y )}(fα)) (7)

if ˜f (AM )→
A

(W ), ˜f (AM )→
A

as-
To reformulate our earlier example,
signs A to a constant value a(cid:48), and f{(AY )→,(AM )→} ≡
{f (AY )→
}, then Y (f{(AY )→,(AM )→}) ≡
A
Y (f (AY )→
A
The joint counterfactual distribution for responses to
path-speciﬁc policies, p({V (fα)|V ∈ V \ Aα}), is iden-
tiﬁed under the functional model, and generalizes (6) and
(4) as follows:

(W ), M (a(cid:48), W ), W ).

(cid:89)

p(V |{f (AiV )→
Ai

(Hi)|(AiV )→ ∈ α}, pa ¯α

G(V )) (8)

V ∈V\Aα
For example, p(Y (fA(W ), M (a(cid:48), W ), W )) is identi-
ﬁed as (cid:80)
W,M p(Y |fA(W ), M, W )p(M |a(cid:48), W )p(W ) in
Fig. 1 (a). We prove a general version of this result in the
Appendix. A general identiﬁcation theory for responses
to path-speciﬁc policies can be found in [16].

E[Y (fA)].

The goal of this paper is developing methods for learn-
ing optimal path-speciﬁc policies, in cases where re-
sponses to such policies are identiﬁed. Before discussing
optimal path-speciﬁc policies, we ﬁrst review existing
approaches to ﬁnding optimal overall policies. Opti-
mality may be quantiﬁed in a number of ways. The
set of optimal policies is commonly deﬁned as f ∗
A ≡
arg maxfA
We will discuss existing methods for ﬁnding optimal
policies f ∗
A in the context of a causal model implying
positivity and sequential ignorability [10]. This model
is graphically represented, for two time points, in Fig. 1
(b), where W0 are baseline factors, W1 and W2 are in-
termediate and ﬁnal outcomes, A1, A2 are treatments.
Variables W0, W1, W2 may be confounded by a hidden
common cause U . It is well known that in this model,
E[Y (fA)] is identiﬁed via (4). We now discuss a number
of approaches for computing the optimal set f ∗
A given
this identifying formula.

Approaches Based on Backwards Induction

In sequential decision problems, choosing optimal func-
tions fA may appear to be a difﬁcult search problem over
a large set of function combinations. However, it is pos-
sible to use ideas from dynamic programming to trans-
form the problem of choosing optimal fA into a sequen-
tial problem where only a single optimal function is cho-
sen at a time.

Multiple modeling approaches are possible here. A sim-
ple approach is to model conditional densities of all out-
comes Wi given their past. Assuming all such models
were correctly speciﬁed, given any particular history HK
up to the last decision point AK, the optimal f ∗
is equal
to I(E[WK(AK = 1)|HK] > E[WK(AK = 0)|HK]),
which under our model is equal to I(E[WK|AK =
1, HK] > E[WK|AK = 0, HK]). Assuming optimal
f ∗
, were already chosen,
Ai+1
AK
the optimal f ∗
Ai

is deﬁned inductively as

, denoted by f ∗

, . . . f ∗

Ai+1

AK

(cid:104)

I

E[WK (Ai = 1, f ∗

)|Hi] > E[WK (Ai = 0, f ∗

Ai+1

(cid:105)

)|Hi]

.

Ai+1

Note that under our model, the counterfactual expecta-
tions above are identiﬁed via a modiﬁcation of (4) where
the outer summation is only with respect to variables
Wi+1, . . . , WK−1. For many kinds of statistical mod-
els for densities appearing as terms in (4), evaluating this
sum may be challenging. An alternative strategy that
avoids repeated summations is modeling the above ex-
pectations directly via Q-functions Qi, which are condi-
tional expectations over value functions Vi+1, given the

history. These are deﬁned recursively as follows:

G-Estimation

QK (HK , AK ; γK ) = E[WK | AK , HK ],

VK (HK ) = max
aK

QK (HK , aK ; γK ),

and for i = K − 1, . . . , 1, as

Qi(Hi, Ai; γi) = E[Vi+1(Hi+1) | Ai, Hi],

Vi(Hi) = max

Qi(Hi, ai; γi).

ai

as

f ∗
Ai

(Hi)

derived

from Q-functions

The optimal policy at each stage may be eas-
=
ily
arg maxai Qi(Hi, ai; γi). Q-functions are recursively
deﬁned regression models where outcomes are value
functions, and features are histories up to the current
decision point. Thus, parameters γi, i = 1, . . . , K, of
all Q-functions may be learned recursively by maximum
likelihood methods applied to regression at stage i,
given that the value function at stage i + 1 was already
computed for every row [2, 14].

Value Search

Consider a restricted class of policies F with elements
fA ≡ {fAi(Hi); Ai ∈ A}. It is often of interest to es-
timate the optimal policy within the class F, even if the
class does not contain the true optimal policy. For exam-
ple, F may be the set of clinically interpretable policies.
For a sufﬁciently simple class F, we can directly search
for the optimal f ∗,F
E[Y (fA)]. This is
A ≡ arg maxfA∈F
called policy search or value search.

The expected response to an arbitrary treatment policy
β = E[Y (fA)], for fA ∈ F can be estimated in a number
of ways. For A = {A}, a simple estimator for β that
uses only the treatment assignment model π(H; ψ) for
p(A = 1|H) is the inverse probability weighting (IPW)
estimator:

E

(cid:104)
Y CfA /πfA (H; (cid:98)ψ)

(cid:105)

,

where CfA ≡ I(A = fA(H)), πfA(H; ψ) ≡
π(H; ψ)fA(H) + (1 − π(H; ψ))(1 − fA(H)), the expec-
tation is evaluated empirically, and (cid:98)ψ is ﬁt by maximum
likelihood. This estimator will not in general yield the
optimal policy within F if π is misspeciﬁed. An alterna-
tive estimator for β that provides some protection against
this is the following:

E

(cid:34) CfA Y
πfA (H; ˆψ)

−

CfA − πfA (H; ˆψ)
πfA (H; ˆψ)

E[Y |H, A = fA(H); ˆγ]

.

(cid:35)

(10)

The above estimator is doubly-robust meaning that it is a
consistent estimator if either the propensity score model
π(H; ψ) or the regression model E[Y |H, A; γ] is cor-
rectly speciﬁed.

Value search methods can in some cases be rephrased
as a weighted classiﬁcation problem in machine learning
[22]. In the interest of space, we do not discuss methods
based on this observation further here.

An alternative method for learning policies is to directly
model the counterfactual contrast functions known as op-
timal blip-to-zero functions, or the counterfactual devia-
tions in outcome from a reference treatment value (which
we take to be A = 0), conditional on history, assuming
all future decisions are already optimal. Speciﬁcally, for
each decision point i, we posit a structural nested mean
model (SNMM) γi(Hi, Ai; ψ) for the contrast

(cid:104)

E

Y (¯ai, f ∗

Ai+1

) | Hi

(cid:105)

− E

(cid:104)
Y (¯ai−1, ai = 0, f ∗

(cid:105)

) | Hi

.

Ai+1

Note that if the true γi(Hi, Ai; ψ) were known, the op-
timal treatment policies are those that maximize the blip
function at each stage: f ∗
= arg maxai γi(Hi, Ai; ψi).
Ai
In order to estimate ψ using data, let

U (ψ, ζ(ψ), α) =

{Gi(ψ) − E [Gi(ψ) | Hi; ζ]}

× {di(Hi, Ai) − E [di(Hi, Ai) | Hi; α]} ,

(11)

where di(Hi, Ai) is any function of Hi and Ai, and

Gi(ψ) = Y −γj (Hi, Ai; ψ)+

(cid:2)γk(Hk, a∗

k; ψ) − γk(Hk, ak; ψ)(cid:3) .

K
(cid:88)

i=1

K
(cid:88)

k=i+1

Consistent and asymptotically normal (CAN) estima-
tors of ψ can be obtained using the estimating equa-
tions E[U (ψ, ζ(ψ), α)] = 0, as shown in [11]. The es-
timate obtained in (11) is doubly-robust, meaning that
the estimator (cid:98)ψ is consistent if either E [Gi(ψ) | Hi; ζ]
or pi(Ai = 1 | Hi; α) is correctly speciﬁed.

Methods closely related to G-estimation based on coun-
terfactual regret were developed in [5]. We do not dis-
cuss them here in the interest of space.

(9)

4 LEARNING OPTIMAL

PATH-SPECIFIC POLICIES

We now consider how the methods for optimizing over-
all policies translate to optimizing path-speciﬁc policies.
Some of the generalizations we consider are currently
only known for single-stage decision problems.

Consider the generalization of Fig. 1 (b) to the longitu-
dinal setting with mediators, shown (for two time points)
in Fig. 2 (a). This causal model corresponds to the set-
ting described in detail in [4], representing an observa-
tional longitudinal study of HIV patients. Here, W0 rep-
resents the baseline variables of a patient, A1, A2 repre-
sent treatment assignments, which were chosen based on
observed treatment history according to physician’s best
judgement, W1, W2 are intermediate and ﬁnal outcomes
(such as CD4 count or viral failure), and M1, M2 are
measures of patient adherence to their treatment regimen.
We are interested in ﬁnding policies fA1 (H1), fA2(H2)
that optimize the effect of A1, A2 on W2 that is either
direct or via intermediate outcomes, but not via adher-
ence, and where adherence is kept to that of a reference

1, a(cid:48)

treatment a(cid:48)
2. Speciﬁcally, we are interested in choos-
ing fA1, fA2 to optimize the counterfactual expectation
E[W2(fA1, fA2 )], which expands via (7) to







E

W2







W0, fA1 (H1), M1(a(cid:48)
fA1 (H1), M1(a(cid:48)
1)
2, W1

(cid:0)fA1 (H1), M1(a(cid:48)

(cid:17)

(cid:16)

W1
1, a(cid:48)
a(cid:48)

1),

, fA2 (H2),

1)(cid:1), M1(a(cid:48)
1)

(cid:17)

(cid:16)

M2













.

(12)

the causal model

The K stage version of

in
Fig. 2 (a) is given by a complete DAG on variables
W0, A1, M1, W1, . . . , AK, MK, WK,
listed in topo-
logical order, with a hidden common cause U of
W0, . . . , WK.
Let α be all directed edges out of
A1, . . . , AK. The general version of (12) is the ex-
pectation of WK taken with respect to the distribution
p(WK(fα)), where fα sets all edges (AiMj)→ to a(cid:48)
i, and
all other edges in α to a policy fAi (Hi).

Identiﬁability of p(WK(fα)) is given by the following
corollary of results in [15], which can be viewed as a
generalization of the collapse of the g-formula [10] to
longitudinal mediation settings.

Theorem 1 In the above model, with a positive observed
data distribution p(WK(fα)) is identiﬁed as
(cid:110)

(cid:81)K

(cid:88)

i=1
p(Mi|¯a(cid:48)

HK ,MK

p(Wi|M i−1, W i−1, fAi (Hi))

i, W i−1, M i−1)

p(W0)

(13)

As a consequence (12) is identiﬁed as

(cid:104)

(cid:88)

W 1,M 2

E
W2|fA1 (H1), fA2 (H2), W 1, M 2
p(W1|W0, fA1 (H1), M1) p(M2|W 1, M1, a(cid:48)

p(M1|a(cid:48)
1, a(cid:48)

1, W0)×
2) p(W0).

We now discuss a number of strategies for ﬁnding opti-
mal path-speciﬁc policies identiﬁed by (22).

Parametric Backwards Induction

A simple approach for ﬁnding optimal path-speciﬁc poli-
cies is to combine backwards induction with a maximum
likelihood estimator for the conditional densities in the
identifying functional (22).
Consider Fig. 2 (a), and let ai and a(cid:48)
i denote the active
and reference levels of ai, respectively. Beginning at the
last stage K = 2, and assuming treatment is binary, the
optimal decision, f ∗
, for a given patient with history
A2
(w0, a1, m1, w1) sets A2 to either a2 or a(cid:48)
2 to maximize
the response W2, while keeping M2 at whatever value it
would have attained under a sequence of reference inter-
ventions (a(cid:48)

1, a(cid:48)

2):
E[W2(a2, M2(a(cid:48)

(cid:16)

= I

f ∗
A2

2)) | H2] > E[W2(a(cid:48)

2, M2(a(cid:48)

2)) | H2]

(cid:17)
.

The optimal decision at the ﬁrst stage, f ∗
A1
(cid:40)
I

(H2), M1(a(cid:48)
2, M1(a(cid:48)

1), W1(a1, M1(a(cid:48)
1)))

1), W1(a1, M1(a(cid:48)

(cid:18) a1, f ∗
A2
M2(a(cid:48)

W2

E

(cid:20)

, is given by

1)),

(cid:21)

(cid:19) (cid:12)
(cid:12)
(cid:12)H1

>

(cid:20)

E

W2

(cid:18) a(cid:48)

1, f ∗
A2
M2(a(cid:48)

(H2), M1(a(cid:48)
2, M1(a(cid:48)

1), W1(a(cid:48)

1), W1(a(cid:48)

1, M1(a(cid:48)
1)))

1, M1(a(cid:48)

1)),

(cid:19) (cid:12)
(cid:12)
(cid:12)H1

(cid:21) (cid:41)

.

(cid:111)

(cid:105)

Under the causal model we described, the above coun-
terfactuals can be estimated using a modiﬁcation of (22)
with no summations over, but instead conditioning on
histories H1 or H2. This approach easily generalizes to
any number of decision stages. The difﬁculty here, as
before, is the increasing amount of marginalizations that
must be performed as the number of stages grows.

Path-Speciﬁc Policies Via Q-Learning

We now describe how to generalize Q-learning to path-
speciﬁc policies, using the HIV example in Fig. 2 (a) to
ground the discussion. Recall that in this example, we
wish to set A1 and A2 with respect to edges into W1, W2
to maximize the outcome, while setting A1 and A2 to
reference values a(cid:48)
2 for the purposes of edges into
M1, M2.

1, a(cid:48)

Simply deﬁning Q-functions as expectations over value
functions conditional on history does not work in our set-
ting, since mediators behave in a counterfactually differ-
ent way from either observed variables, or variables we
wish to set according to a policy. Moreover, the sequen-
tial nature of the problem means the nested counterfac-
tuals needed become quite involved to write down.

An alternative is to deﬁne Q-functions not in the ob-
served data distribution, corresponding to Fig. 2 (a), but
in a counterfactual distribution where A1, A2 behave as
observed, except for the purposes of edges into M1, M2,
in which case they are counterfactually set to a(cid:48)
1, a(cid:48)
2.
The graph corresponding to this counterfactual world is
shown in Fig. 2 (b), and can be viewed as a generalization
of a single world intervention graph [9], where treatment
variables are only intervened on for the purposes of cer-
tain outgoing edges. Note that descendant variables of
M1, M2 are marked with a tilde to make clear that these
variables are counterfactual and no longer equal to their
observed counterparts.

The distribution corresponding to this situation is sim-
ply p(V(a(A1M1)→,(A1M2)→,(A2M2)→ )), where a sets
these edges to a(cid:48)
2. For K stages, the distribution is
p(V(aα)), where α are all edges of the form (AiMj)→,
and a sets each such edge to the reference value a(cid:48)
i. We
have the following claim.

1, a(cid:48)

Theorem 2 In K stage version of the model in Fig. 2(a),
p(V(aα)) is identiﬁed as

˜p( ˜W0, ˜A1, ˜M1, ˜W1, . . . , ˜WK , ˜AK , ˜MK ) =

p(W0)

p(Wi|Mi, Ai, Hi)p(Ai|Hi)p(Mi|a(cid:48)

i, Hi \ A)

(14)

K
(cid:89)

i=1

We can now deﬁne Q-functions as value function expec-
tations on this new distribution, and proceed with back-
wards induction as before. The only difference between

the previous formulation is how Q-functions parameters
are ﬁt. In particular, we must compensate for the fact that
˜p above is not the observed data distribution. Deﬁne

U

a(cid:48)
1

U

a(cid:48)
2

M1

M2

˜M1

˜M2

˜QK ( ˜HK , ˜AK ; γK ) = ˜E[ ˜WK | ˜AK , ˜HK ],

W0

W1

W2

W0

˜W1

˜W2

˜VK ( ˜HK ) = max
aK

˜QK ( ˜HK , aK ; γK ),

(15)

and for i = K − 1, . . . , 1, deﬁne

˜Qi(Hi, Ai; γi) = ˜E[ ˜Vi+1( ˜Hi+1) | ˜Ai, ˜Hi],

˜Vi( ˜Hi) = max
ai

˜Qi( ˜Hi, ai; γi).

(16)

In our example in Fig. 2 (a), K = 2, H1 ≡ {W0},
˜H2 ≡ {W0, M1, W1}, and ˜E denotes expectations with
respect to appropriate conditional distributions derived
from ˜p. Q-functions deﬁned in this way can be used to
obtain the optimal path-speciﬁc policy at each stage.

Theorem 3 Given that each ˜Qi, i = 1, . . . , K is speci-
ﬁed correctly, the optimal treatment at stage i given Hi
is equal to: f ∗
Ai
Since parameters γ are not generally known, they must
be estimated from data. This can be done as follows.

(Hi) = arg maxai

˜Qi(Hi, ai; γi).

Theorem 4 Assume
{ ˜Qi( ˜Hi, ˜Ai; γi), p(Mi|Ai, Hi; φ)|∀i}
speciﬁed. Then the estimation equations

models

in

the

set
correctly

are

A1

A2

(a)

A1

˜A2

(b)

Figure 2: (a) A causal model that generalized Fig. 1 (b)
by also considering mediators. (b) A “multiple world in-
tervention graph,” representing the counterfactual situa-
tion where adherence levels are kept to a reference treat-
ment level, but the chemical effect of drugs is operating
normally.

and an estimator which generalizes (10):

(cid:34)

E

(cid:101)C
(cid:101)π(W ; ψ)
(cid:110)

I(A = a(cid:48))

πa(cid:48) (W ; (cid:98)ψ)

f (M |W, A = a(cid:48); (cid:98)φ)
f (M |W, f (W ); (cid:98)φ)

(cid:110)

Y − E[Y |f (W ), M, W ; (cid:98)ζ]

(cid:111)

+

E[Y |f (W ), M, W ; (cid:98)ζ] −

E[Y |f (W ), M, W ; (cid:98)ζ]

(cid:88)

M

p(M |W, A = a(cid:48); (cid:98)ζ)

(cid:111)

+

(cid:88)

(cid:35)
E[Y |f (W ), M, W ; (cid:98)ζ]p(M |W, A = a(cid:48); (cid:98)φ)

,

M

(18)

(cid:34)

E

∂ ˜QK
∂γK
(cid:34)

E

∂ ˜Qi
∂γi

(cid:35)
{WK − ˜QK (AK , HK ; γK )}wK (HK ; (cid:99)φK )

= 0, and

{Vi+1(Hi+1) − ˜Qi(Hi, Ai; γi)}wi(Hi; (cid:99)φi)

= 0,

(cid:35)

where the expectation is evaluated empirically, (cid:101)C ≡
I(A = f (W )), πa(cid:48)(W ; ψ) = p(A = a(cid:48)|W ; ψ),
(cid:101)π(W ; (cid:98)ψ) ≡ (cid:80)
a πa(W ; ψ)I(a = f (W )), and (cid:98)ψ, (cid:98)φ, (cid:98)ζ
are ﬁt by maximum likelihood. We have the following.

are consistent for γK and γi, where

wi(Hi; (cid:99)φi) ≡

p(Mi|Ai = a(cid:48), Hi; (cid:98)φi)
p(Mi|Ai, Hi; (cid:99)φi)

∀i = 1, . . . K.

Path-Speciﬁc Value Search

For simplicity, we restrict attention to a single-stage de-
cision problem, as shown in Fig. 1 (a), where we are in-
terested in picking a policy that maximizes the counter-
factual mean β = E[Y (A = f (W ), M (a(cid:48)))].

Consider a restricted class of path-speciﬁc policies F
with elements f ≡ {f (W ), ˜f } the latter setting A to a
constant value a(cid:48), regardless of W . Give any estimation
strategy for the counterfactual mean under a path-speciﬁc
policy, we can implement a direct search for the optimal
policies within F as before. By analogy with earlier dis-
cussion of value search, we give an IPW estimator for β
which generalizes (9):

(cid:34)

E

Y (cid:101)C
(cid:101)π(W ; (cid:98)ψ)

·

p(M |A = a(cid:48), W ; (cid:98)φ)
p(M |A = f (W ), W ; (cid:98)φ)

(cid:35)

,

(17)

the estimator in (17)
if

Theorem 5 Under regularity assumptions referenced
is consis-
in the Appendix,
the mod-
tent and asymptocally normal
els in the set {π(W ; ψ), p(M |W, A; φ)} are cor-
rectly speciﬁed, and the estimator in (18) is CAN in
the union model, where any two models in the set
{π(W ; ψ), E[Y |A, M, W ; ζ], p(M |W, A; φ)} are cor-
rectly speciﬁed.

(CAN)

This claim, and the estimators above, are extensions of
the results in [18].

Single-Stage G-Estimation For Path-Speciﬁc Policies

Results in [19] generalized optimal blip-to-zero func-
tions to mediation settings, by positing the following
SNMM γ(A, W ; ψ):

E[Y (A, M (A = 0))|W ] − E[Y (A = 0, M (A = 0))|W ].

(19)

Note that
f ∗
A(W ), M (0))] can be directly obtained from γ via

A maximizing E[Y (A =

the policy f ∗

f ∗
A(W ) = arg max

γ(A, W ; ψ).

a

Since ψ is not generally known, it must be estimated
from data. The following is a consistent set of estimating
equations for ψ, under assumptions described in [19]:

(cid:20)(cid:18) I(A = 1)p(M |A = 0, W )

E

p(A = 1|W )p(M |A = 1, W )

{Y − E[Y |A = 1, M, W ]}−

I(A = 0)
p(A = 0|W )

{Y − E[Y |A = 1, M, W ] + γ(1, W ; ψ)}

h(W )

= 0

(20)

(cid:19)

(cid:21)

for h(W ) any |ψ|-dimensional function of W .

Note that unlike SNMMs associated with overall poli-
cies, which can be deﬁned for any number of treatments,
SNMMs associated with path-speciﬁc policies have only
been deﬁned for a single treatment. A longitudinal gen-
eralization of these models is left to future work.

5 EXPERIMENTS

We now illustrate our methods via a dataset on HIV
patients from Nigeria. The data consists of more than
50k treatment-naive HIV-1 infected patients who were
enrolled in the Harvard PEPFAR/AIDS Prevention Ini-
tiative program prior to Oct 2010. The patients were
put on courses of antiretroviral therapy (ART), with ﬁve
standard ﬁrst-line regimen, and were followed every six
months for at least one year after the ART initiation. Pa-
tients stayed on their initial treatment plan unless they
were experiencing toxicity within a ﬁrst-line drug regi-
men and consequently moved to a second-line regimen.
The data has records on demographics and clinical test
results such as CD4 counts, viral loads, and toxicity mea-
sures at 6-month intervals.

In order to combat chronic diseases such as HIV, patients
are required to follow long term use of medications. Full
beneﬁts of the medications are realized when patients
take their medications as prescribed. Unfortunately, fac-
tors such as side effects, caused by drug toxicities, can
be developed alongside the course of treatment and lead
to poor adherence to the therapy. A primary measure of
adherence is provided in the data as average percent ad-
herence that is the total number of days that the patient
has drug supply over the total number of days in the time
period (6-month intervals).

We restricted our attention to the ﬁrst year of follow
up, including two decision points and picked (log) CD4
count at the end of the ﬁrst year as the outcome of inter-
est. CD4 count is one of the biomarkers that quantiﬁes
how well the immune system is functioning and higher
values are more favorable. Drug toxicity and adherence
can mediate the effect of treatment on outcome. Since
reactions to drug toxicity and adherence behavior vary
among patients, we consider the problem of ﬁnding poli-
cies that optimize the direct chemical effect of the drug

where the indirect effect mediated through drug toxicity
and adherence are set to some reference levels. We run
an additional experiment where only the effect through
adherence is set to a reference level [4]. The latter results
are provided in the Appendix for the interest of space.

The causal model can be represented by the DAG in
Fig. 2 (a). Treatment decisions at the baseline and the
ﬁrst follow up are respectively denoted by A1 and A2.
W1 is a dichotomized intermediate outcome that indi-
cates whether the CD4 count is above 450 cells/mm3 at
the the end of the same six month period. W2 is the ﬁ-
nal outcome and denotes the log CD4 count at the end
of the ﬁrst year after ART initiation. W0 includes all the
baseline factors such as sex, age, and test results prior
to any treatment initiation. Toxicity and adherence mea-
sures during the ﬁrst six months and the ﬁrst year after
treatment initiation are collected in M1 and M2, respec-
tively. Drug toxicity indicates any lab toxicities (alanine
transaminase ≥ 120 UI/L, creatinine ≥ 260 mmol/L,
hemoglobin ≤ 8 g/dL), and adherence indicates whether
average percent adherence was no less than 95%.

We ﬁrst illustrate the results on ﬁnding optimal policies
in a two-stage decision problem using G-formula and Q-
learning methods, and then provide the results for ﬁnding
optimal policies for the single (ﬁrst) stage decision prob-
lem using value search and G-estimation. In the single
stage analysis, we consider the log CD4 count at the end
of the sixth month as the outcome of interest. All model-
ing assumptions are described in the Appendix.

Methods For The Two-Stage Decision Problem

Optimal overall polices and path policies are estimated
as described in Sections 3 and 4. Expected outcomes
under optimal policies we learned, along with 95% con-
ﬁdence intervals obtained by bootstrap, are shown in Ta-
ble. 1. In the interest of space, we do not consider correc-
tions necessary to preserve the validity of these intervals
in cases of model discontinuities, but these are straight-
forward [1]. Both optimal polices have higher expected
outcomes than the observed data, using either method.
Path-speciﬁc policies led to higher expected outcomes
compared to overall policies. This could be explained by
the phenomenon of countervailing adherence-mediated
effect, described in [4]. In other words, in some cases
the more chemically effective drugs are also harder to
take. In order to visualize optimal policies we learned
as clinically interpretable ﬂowcharts, we viewed policy-
recommended decisions as class labels, and history as
features, and used a multilabel decision tree classiﬁer, as
implemented in the rpart R package. The results are
shown in Fig. 3 and 4 in the Appendix. Since the clas-
siﬁers are not perfect predictors of the policies, they are

Table 1: Comparison of population log CD4 counts under dif-
ferent policies (under treatment assignments in the observed
data, the value is 5.64 ± 0.01 in the 2-stage and 5.54 ± 0.01
in the 1-stage problem). G-formula and Q-learning are used
with 2-stage decision points. Value search and G-estimation
are used with 1-stage decision point.

G-formula
Q-learning
Value search
G-estimation

Path Policies
6.89 (5.76, 7.10)
7.34 (6.10, 7.55)
5.58 (5.54, 5.62)
5.62 (5.50, 5.67)

Overall Policies
6.79 (5.68, 6.90)
6.89 (5.82, 7.12)
5.56 (5.55, 5.58)
5.79 (5.59, 6.04)

higher than observed log CD4 count, with the path policy
yielding slightly higher outcomes than the overall policy.

Finally, we learned optimal path policies via G-
estimation. We estimated the parameters ψ for the
SNMM using (20). The population log CD4 count
under path policies and overall policies learned by G-
estimation are given in Table. (1), and a decision tree
view of the policies is shown in Fig. 5 in the Appendix.
Under our assumptions, the optimal path policy found by
G-estimation did not do much better than the outcomes
under observed treatments and the overall policies.

to be viewed as interpretable approximations of the true
learned policy. Variables involved in the decisions, such
as age, gender, CD4, virological status, and so on, are
clinically reasonable.

Methods For The Single-Stage Decision Problem

We focus on patients that appear at baseline, and treat the
ﬁve ﬁrst-line treatments as a binary decision by grouping
the ﬁrst three (which we denote as group 1) and the last
two treatments (which we denote as group 2). We are
interested in ﬁnding a treatment group assignment at the
ﬁrst decision point that leads to a higher CD4 count at
the ﬁrst follow up, assuming adherence and toxicity are
set to a reference point for every patient. The model for
this setting is Fig. 1(a), where W is observed history up
to the ﬁrst decision.
We considered policies of the form I{CD4m00 < α},
where CD4m00 denotes the CD4 count right before the
ﬁrst decision point, to decide what treatment group the
patient should be assigned to. The normal range for this
variable is 500 to 1500 cells/mm3. We searched for an
optimal policy in this restricted class by varying α from
100 cells/mm3 to 1000 cells/mm3 by 50 cells/mm3 incre-
ments, and estimated the value for each α using (18). Un-
der the modeling assumptions described in the Appendix,
the optimal threshold between group 1 and group 2 treat-
ments was chosen to be I{CD4m00 < 550 cells/mm3}.
In other words, if the CD4 count is less than 550, it is
better for the patients to receive any of the treatments in
the ﬁrst treatment group. However, if we pick the op-
timal policy based on the overall effect of treatment on
outcome, then value search leads to policies of the form
I{CD4m00 < 250 cells/mm3}. The optimal overall pol-
icy decides to switch to group 2 treatments when HIV
gets more severe, while the optimal path policy decides
on switching when CD4 count is still within a healthy
range but at lower values. This implies that if treatment
adherence could be kept to that of a reference treatment,
initiation of treatments within group 2 could be delayed
to lower CD4 values. As before, both policies lead to a

6 CONCLUSIONS

In this paper, we generalized ideas in mediation analy-
sis and dynamic treatment regimes to consider the prob-
lem of estimation of responses to policies associated
with particular causal pathways, and methods for learn-
ing policies that optimize these responses. Since vali-
dating ﬁndings in causal inference is difﬁcult, and con-
clusions are sensitive to speciﬁc modeling assumptions
made, we developed multiple approaches for learning
optimal policies that rely on orthogonal sets of modeling
strategies. In particular, we considered strategies based
on backwards induction with either plug-in estimation or
Q-learning, value search for restricted classes of policies,
and G-estimation of structural nested mean models (SN-
MMs) generalized to mediation.

We illustrated these methods by ﬁnding policies for HIV-
positive patients that optimize the direct chemical effect
of antiretroviral therapy, where the indirect effect medi-
ated through drug toxicity and adherence are set to a ref-
erence level. The results in the experiment section sug-
gest that policies that aim to optimize the direct effect of
the treatment have better outcome responses than poli-
cies that optimize the overall effect of the treatment, and
optimal policies decide on clinically relevant variables,
such as age, gender, viral load, and CD4 count.

The estimation methods we described are applicable to
sequential decision problems, except for G-estimation
which is currently only limited to single-stage decision
problems. Generalizing the version of G-estimation
to longitudinal mediation problems, and deriving semi-
parametric estimators for the version of Q-learning we
described are areas for future work.

Acknowledgments

This research was supported in part by the NIH grants
R01 AI104459-01A1 and R01 AI127271-01A1. We
thank the anonymous reviewers for their insightful com-
ments that greatly improved this manuscript.

[13] J. M. Robins, S. D. Mark, and W. K. Newey. Esti-
mating exposure effects by modelling the expecta-
tion of exposure conditional on confounders. Bio-
metrics, pages 479–495, 1992.

[14] P. Schulte, A. Tsiatis, E. Laber, and M. David-
ian. Q- and A-learning methods for estimating op-
timal dynamictreatment regimes. Statistical Sci-
ence., 29:640–661, 2014.

[15] I. Shpitser. Counterfactual graphical models for
longitudinal mediation analysis with unobserved
confounding. Cognitive Science (Rumelhart spe-
cial issue), 37:1011–1035, 2013.

[16] I. Shpitser and E. Sherman. Identiﬁcation of per-
sonalized effects associated with causal pathways.
In proceedings of the 34th Conference on Uncer-
tainty in Artiﬁcial Intelligence, 2018.

[17] I. Shpitser and E. T. Tchetgen. Causal inference
with a graphical hierarchy of interventions. Ann.
Stat., 2015.

[18] E. J. T. Tchetgen and I. Shpitser. Semiparamet-
ric theory for causal mediation analysis: efﬁciency
bounds, multiple robustness, and sensitivity analy-
sis. Ann. Stat., 40(3):1816–1845, 2012.

[19] E. J. T. Tchetgen and I. Shpitser. Estimation of a
semiparametric natural direct effect model incorpo-
rating baseline covariates. Biometrika, 101(4):849–
864, 2014.

[20] J. Tian and J. Pearl. On the identiﬁcation of causal
effects. Technical Report R-290-L, Department of
CS, UCLA, 2002.

[21] A. Tsiatis. Semiparametric Theory and Missing

Data. Springer, 1 edition, 2006.

[22] Y. Zhao, D. Zeng, A. J. Rush, and M. R. Kosorok.
Estimating individualized treatment rules using
outcome weighted learning. Journal of the Ameri-
can Statistical Association, 107:1106–1118, 2012.

References

[1] B. Chakraborty, E. B. Laber, and Y. Zhao.

Infer-
ence for optimal dynamic treatment regimes using
an adaptive m-out-of-n bootstrap scheme. Biomet-
rics, 69(3), 2013.

[2] B. Chakraborty and E. Moodie. Statistical Meth-
ods for Dynamic Treatment Regimes: Reinforce-
ment Learning, Causal Inference, and Personalized
Medicine. New York: Springer-Verlag, 2013.

[3] J. W. Lavori and R. Dawson. A design for testing
clinical strategies: Biased adaptive within-subject
randomization. Journal of the Royal Statistical So-
ciety, 163:29–38, 2000.

[4] C. Miles, I. Shpitser, P. Kanki, S. Meloni, and E. T.
Tchetgen. Quantifying an adherence path-speciﬁc
effect of antiretroviral therapy in the nigeria pepfar
program. Journal of the American Statistical Soci-
ety, 2017.

[5] S. Murphy. Optimal dynamic treatment regimes.
Journal of the Royal Statistical Society: Series B,
65:331–366, 2003.

[6] S. A. Murphy. An experimental design for the de-
velopment of adaptive treatment strategies. Statis-
tics in Medicine, 24:1455–1481, 2005.

[7] J. Pearl. Direct and indirect effects.

In Proceed-
ings of the Seventeenth Conference on Uncertainty
in Artiﬁcial Intelligence (UAI-01), pages 411–420.
Morgan Kaufmann, San Francisco, 2001.

[8] J. Pearl. Causality: Models, Reasoning, and Infer-
ence. Cambridge University Press, 2 edition, 2009.

[9] T. S. Richardson and J. M. Robins. Single world
intervention graphs (SWIGs): A uniﬁcation of the
counterfactual and graphical approaches to causal-
ity. Working Paper 128, Center for Statistics
and the Social Sciences, Univ. Washington, Seattle,
WA., 2013.

[10] J. M. Robins. A new approach to causal inference
in mortality studies with sustained exposure peri-
ods – application to control of the healthy worker
survivor effect. Mathematical Modeling, 7:1393–
1512, 1986.

[11] J. M. Robins. Optimal structural nested models for
optimal sequential decisions. In Proceedings of the
second Seattle symposium on biostatistics, pages
189–326, 2004.

[12] J. M. Robins and S. Greenland. Identiﬁability and
exchangeability of direct and indirect effects. Epi-
demiology, 3:143–155, 1992.

Appendix

In Appendix A, we give a brief overview of semi-
parametric statistical inference, which provides context
for some of our subsequent results. Appendix B con-
tains deferred proofs of our results. In Appendix C, we
review the statistical modeling assumptions we made in
our data analysis, provide ﬁgures which use decision tree
classiﬁers to visualize policies we learned, and describe
additional experimental results on policies that optimize
effects not mediated by only adherence.

A: Statistical Inference In Semi-Parametric Models

Let Z1, . . . , Zn, be iid samples from a general class
of probability densities p(Z; θ) parameterized by θT =
(βT , ηT ), where β ∈ Rq denotes the set of target param-
eters, and η denotes a possibly inﬁnite dimensional set
of nuisance parameters. This type of model is termed
semi-parametric, since it has both a parametric and a
non-parametric component. The goal of statistical infer-
ence in semi-parametric models is to ﬁnd “the best” esti-
mator of β in the model, denoted by (cid:98)β. Regular asymp-
totically linear (RAL) estimators are considered in this
setting, which are estimators of the form

√

n( ˆβ − β) =

1
√
n

n
(cid:88)

i=1

φ(Zi) + op(1),

where φ ∈ Rq with mean zero and ﬁnite variance, op(1)
denotes a term that approaches to zero in probability, and
φ(Zi) is the inﬂuence function (IF) of the ith observation
for the parameter vector β. RAL estimators are consis-
tent and asymptotically normal (CAN), with the variance
of the estimator given by its IF:

√

n( ˆβ − β) D−→ N (0, φφT ).

Thus, there is a bijective correspondence between RAL
estimators and IFs.
In fact, IFs provide a geometric
view of the behavior of RAL estimators. Consider a
Hilbert space H of all mean-zero q−dimensional func-
tions, equipped with an inner product, and deﬁne the
inner product of two arbitrary elements of the Hilbert
space, h1 and h2, to be equal to E[hT
1 h2]. Deﬁne a
parametric submodel to be a subset of densities in the
semi-parametric model parameterized by θT
γ = (βT , γT ),
where γT ∈ Rr, such that the subset contains the density
p(Z; θ0) in the semi-parametric model evaluated at the true pa-
rameter values θ0. The nuisance tangent space Λ in the semi-
parametric model is deﬁned to be the mean square closure of el-
ements of the nuisance tangent spaces Λγ = {Bq×rSη(Z; θ)}
of every parametric submodel. The space Λ is important be-
cause it is known all inﬂuence functions lie in the orthogonal
complement Λ⊥ of Λ with respect to H. For this reason, re-
covering Λ⊥ is often the ﬁrst step for constructing RAL esti-
mators in semi-parametric models. Out of all IFs in Λ⊥ there

exists a unique one which lies in the tangent space, and which
yields the most efﬁcient RAL estimator by recovering the semi-
parametric efﬁciency bound, see [21] for details.

B: Proofs

Here we give proofs of all claims in the main body of the paper.

Theorem 1 Fix a causal model given by a complete DAG on
variables W0, A1, M1, W1, . . . , AK , MK , WK , listed in topo-
logical order, with a hidden common cause U of W0, . . . , WK .
Let α be all directed edges out of A1, . . . , AK , and fα which
sets all edges (AiMj)→ to ai, and all other edges in α to a
policy fAi (Hi). In this model, p(WK (fα)) is identiﬁed as

(cid:88)

p(W0)

p(Mi|¯a(cid:48)

K
(cid:89)

i=1

HK ,MK

i, W i−1, M i−1)p(Wi|M i−1, W i−1, fAi (Hi))

(21)

Proof:

The causal model we describe is

simply
Pearl’s functional model corresponding to the K stage ver-
sion of
is well known
that
in this model, given standard positivity assumptions,
p(W0, . . . , WK , M1, . . . , Mk|do(a1, . . . , aK )) is identiﬁed
by the g-formula (2):

the DAG in Fig. 2 (a).

It

p(W0)

p(Mi|¯a(cid:48)

i, W i−1, M i−1)p(Wi|M i−1, W i−1, ¯ai).

Since the recanting district criterion [15] does not hold, we
have that p({W0, . . . , WK , M1, . . . , Mk}(a(cid:48)
α)) is identiﬁed
by

p(W0)

p(Mi|¯a(cid:48)

i, W i−1, M i−1)p(Wi|M i−1, W i−1, ¯ai).

K
(cid:89)

i=1

K
(cid:89)

i=1

where α is all outgoing edges from A, and a(cid:48) sets all edges of
the form (AiMj) to a(cid:48)
i, and all edges of the form (AiWj) to
ai.
Every fAi (Hi) simply chooses ai based on Hi, which is a sub-
set of outcome variables in our distribution. Since the iden-
tiﬁability statement above holds regardless of how a1, . . . , ai
are chosen, this implies p({W0, . . . , WK , M1, . . . , Mk}(fα)),
where fα is given in the Theorem statement is identiﬁed as

(cid:88)

p(W0)

p(Mi|¯a(cid:48)

K
(cid:89)

i=1

HK ,MK

i, W i−1, M i−1)p(Wi|M i−1, W i−1, fAi (Hi))

(22)

(cid:3)

which implies our result by a simple marginalization.

Before proving Theorem 5, we show the following claim.

Theorem 6 Within the model corresponding to Fig. 1 (a), the
unique efﬁcient inﬂuence function U (β) of β = E[Y (A =
f (W ), M (a(cid:48)))] is given by

(cid:101)C
(cid:101)π(W )
I(A = a(cid:48))
πa(cid:48) (W )

(cid:110)

f (M |W, A = a(cid:48))
f (M |W, f (W ))

(cid:110)

Y − E[Y |f (W ), M, W ]

+

(cid:111)

E[Y |f (W ), M, W ] −

E[Y |f (W ), M, W ]

(cid:88)

M

p(M |W, A = a(cid:48))

+

E[Y |f (W ), M, W ]p(M |W, A = a(cid:48)) − β.

(cid:111)

(cid:88)

M

Proof: This proof follows as an extension of similar results
on the inﬂuence function of the mediation functional, found in
[18].

The model in Fig. 1 (a) imposes no restrictions on the observed
data, and so is non-parametric saturated. As a result, the in-
ﬂuence function U (β) for any β is a unique solution to the
following integral equation

∂
∂t

β(Ft)

(cid:12)
(cid:12)
(cid:12)
(cid:12)t=0

= E[S(W, A, M, Y )φ(β)],

where Ft is the distribution function corresponding to a one di-
mensional regular parametric submodel of the non-parametric
model on W, A, M, Y , indexed by a single parameter t, and S
is the score. ∂β(Ft)/∂t is equal to

E[Y | a = fA(w), m, w]p(m | a(cid:48), w)p(w) =

(p(y | a = fA(w), m, w)p(m | a(cid:48), w)p(w)) =

yS(y | a = fA(w), m, w)p(y | a = fA(w), m, w)×

∂
∂t

(cid:88)

w,m

y

∂
∂t

(cid:88)

w,m,y
(cid:88)

+

+

(cid:88)

w,m
(cid:88)

w,m

w,m,y
p(m | a(cid:48), w)p(w)

where S(.) represent appropriate conditional and marginal
scores. By linearity of derivatives, we can solve this equation,
term by term. We have, for the ﬁrst term:

yS(y | a = fA(w), m, w)p(y | a = fA(w), m, w)×

(cid:88)

w,m,y
p(m | a(cid:48), w)p(w)

(cid:88)

=

w,m,y,a(cid:48)(cid:48)

I(a(cid:48)(cid:48) = fA(w))p(m | a(cid:48), w)
p(a(cid:48)(cid:48) = fA(w) | w)p(m | a(cid:48)(cid:48), w)

yS(y | a(cid:48)(cid:48), m, w)p(y | a(cid:48)(cid:48), m, w)p(m | a(cid:48)(cid:48), w)×
p(a(cid:48)(cid:48) = fA(w) | w)p(w)

(cid:88)

=

w,m,y,a(cid:48)(cid:48)

I(a(cid:48)(cid:48) = fA(w))p(m | a(cid:48), w)
p(a(cid:48)(cid:48) | w)p(m | a(cid:48)(cid:48), w)

{y − E[Y | a(cid:48)(cid:48), m, w]}S(y, a(cid:48)(cid:48), m, w)p(y, a(cid:48)(cid:48), m, w)

(cid:104)

= E

I(A = fA(W ))p(M | a(cid:48), W )
p(A = fA(W ) | W )p(M | A, W )

×

(cid:105)
{Y − E[Y | A, M, W ]}S(Y, A, M, W )

.

So

the

ﬁrst

term contribution

to U (β)

is

I(A=fA(W ))p(M |a(cid:48),W )
p(A=fA(W )|W )p(M |a,W ) {Y − E[Y | a, M, W ]}.

For the second term, we have:

(cid:88)

w,m

(cid:88)

=

w,m,a(cid:48)(cid:48)

(cid:88)

=

w,m,a(cid:48)(cid:48)

E[Y | a = fA(w), m, w]S(m | a(cid:48), w)×

p(m | a(cid:48), w)p(w)
I(a(cid:48)(cid:48) = a(cid:48))
p(a(cid:48)(cid:48) | w)

E[Y | a = fA(w), m, w]×

S(m | a(cid:48)(cid:48), w)p(m, a(cid:48)(cid:48), w)
I(a(cid:48)(cid:48) = a(cid:48))
p(a(cid:48)(cid:48) | w)

(cid:110)

E[Y | a = fA(w), m, w]

E[Y | a = fA(w), m, w]p(m | a(cid:48)(cid:48), w)

(cid:111)

×

S(m, a(cid:48)(cid:48), w)p(m, a(cid:48)(cid:48), w)
I(a(cid:48)(cid:48) = a(cid:48))
p(a(cid:48)(cid:48) | w)

(cid:110)

E[Y | a = fA(w), m, w]

(cid:88)

=

w,m,a(cid:48)(cid:48) ,y

E[Y | a = fA(w), m, w]p(m | a(cid:48)(cid:48), w)

(cid:111)

×

(cid:88)

−

m

(cid:88)

−

m

S(y, m, a(cid:48)(cid:48), w)p(y, m, a(cid:48)(cid:48), w)

(cid:104)
= E

S(Y, M, A, W )

I(A = a(cid:48))
p(a(cid:48) | W )

×

(cid:8)E[Y |a = fA(W ), M, W ] − Eq[Y |a, a(cid:48), W ](cid:9) (cid:105)

.

where Eq[Y | a = fA(W ), a(cid:48), W ] = (cid:80)
fA(W ), m, W ]p(m | a(cid:48), W ).

m

E[Y | a =

So the second term contribution to U (β) is

For the third term, we have:

E[Y | a, m, w]p(m | a(cid:48), w)S(w)p(w)

(cid:88)

w,m

=

=

(cid:40)

(cid:88)

(cid:88)

w

m

(cid:40)

(cid:88)

(cid:88)

w

m

(cid:88)

−

w,m

=

(cid:88)

−

w,m

E[Y | a, m, w]p(m | a(cid:48), w)

S(w)p(w)

E[Y | a, m, w]p(m | a(cid:48), w)

E[Y | a, m, w]p(m | a(cid:48), w)p(w)

S(w)p(w)

(cid:88)

(cid:40)

(cid:88)

w,a(cid:48)(cid:48) ,m,y

m

E[Y | a, m, w]p(m | a(cid:48), w)

E[Y | a, m, w]p(m | a(cid:48), w)p(w)

×

(cid:41)











S(y, m, a(cid:48)(cid:48), w)p(y, m, a(cid:48)(cid:48), w)

= E

E[Y | a = fA(W ), m, W ]p(m | a(cid:48), W ) − β

(cid:35)

(cid:34)

(cid:88)

m

So the third term contribution to U (β) is Eq[Y | a =
fA(W ), a(cid:48), W ] − β.

This establishes our result.

(cid:3)

Theorem 2 Fix a causal model given by a complete DAG
on variables V ≡ {W0, A1, M1, W1, . . . , AK , MK , WK },
listed in topological order, with a hidden common cause U of

E[Y | a = fA(w), m, w]S(m | a(cid:48), w)p(m | a(cid:48), w)p(w)

E[Y | a = fA(w), m, w]p(m | a(cid:48), w)S(w)p(w),

I(A = a(cid:48))
p(a(cid:48) | W )

(cid:110)

E[Y | a = fA(W ), M, W ]

−Eq[Y | a = fA(W ), a(cid:48), W ]

(cid:111)

.

W0, . . . , WK . Let α be all directed edges present in the DAG
out of A1, . . . , AK and into M1, . . . MK , and aα which sets all
edges (AiMj)→ to a(cid:48)
i. In this model, p(V(aα)) is identiﬁed as

Proof: We show this inductively on the decision stage. For
stage K, we have

p(V(aα)) ≡ ˜p( ˜W0, ˜A1, ˜M1, ˜W1, . . . , ˜WK , ˜AK , ˜MK ) =

p(W0)

p(Wi|Mi, Ai, Hi)p(Ai|Hi)p(Mi|a(cid:48)

i, Hi \ A)

K
(cid:89)

i=1

Proof: This is a corollary of Theorem 1, where we deﬁne each
fAi (Hi) to be the observed conditional distribution p(Ai|Hi).
(cid:3)

Theorem 3 Given that each ˜Qi, i = 1, . . . , K is speciﬁed
correctly, the optimal treatment at stage i is equal to

f ∗
Ai (Hi) = arg max

˜Qi(Hi, ai; γi).

ai

Proof: This follows by the standard backwards induction argu-
ment giving the relationship between Q-functions and optimal
policies, applied to ˜p and ˜Qi, and the deﬁnition of expected re-
sponse corresponding to path-speciﬁc policies we have chosen.
The optimal policy set f ∗

A is deﬁned as

E[WK (fα)]
arg max
∈f ∗
{f ∗
}
A

Ai

= arg max
∈f ∗
}
A

{f ∗

Ai

(cid:90)

WK

K
(cid:89)

i=1

p(Wi|Ai = f ∗
Ai

(Hi), Hi, Mi)

p(W0)p(Mi|a(cid:48)
(cid:90)

i, Hi)dV

p(W1|a1, H1, M1)p(M1|a(cid:48)

1, H1)dM1, W1

= arg max

a1

. . .
(cid:90)

arg max
aK−1

arg max
aK

p(WK−1|aK−1, HK−1, MK−1)

p(MK−1|a(cid:48)
(cid:90)

K−1, HK−1)dMK−1, WK−1

WK p(WK |aK , HK , MK )

p(MK |a(cid:48)

K , HK )dMK , WK

It’s immediately clear that the last line above yields ˜QK , and
given that line i + 1 yields ˜Qi+1 assuming ai+1, . . . , aK were
chosen optimally, line i yields ˜Qi.
(cid:3)

4

set
Theorem
{ ˜Qi( ˜Hi, ˜Ai; γi), p(Mi|Ai, Hi; φ)|∀i} are correctly speciﬁed.
Then the estimation equations

Assume

models

the

in

(cid:34)

E

∂ ˜QK
∂γK
(cid:34)

E

∂ ˜Qi
∂γi

(cid:35)
{WK − ˜QK (AK , HK ; γK )}wK (HK ; (cid:99)φK )

= 0, and

{Vi+1(Hi+1) − ˜Qi(Hi, Ai; γi)}wi(Hi; (cid:99)φi)

= 0,

(cid:35)

are consistent for γK and γi, where

wi(Hi; (cid:99)φi) ≡

p(Mi|Ai = a(cid:48), Hi; (cid:98)φi)
p(Mi|Ai, Hi; (cid:99)φi)

∀i = 1, . . . K.

(cid:34)

E

∂ ˜QK ; γK )
∂γK

(cid:35)
{WK − ˜QK (AK , HK ; γK )}wK (HK ; (cid:99)φK )

=

(cid:90) ∂ ˜QK
∂γK
p(MK |AK = a(cid:48)

{WK − ˜QK (AK , HK ; γK )}

k, HK )p(HK )dMK dHK
(cid:35)

{˜E[WK |AK , HK ] − ˜QK (AK , HK ; γK )}

,

= E

(cid:34)

∂ ˜QK
∂γK

where ˜E is the expectation taken with respect to the apprio-
priate conditional distribution derived from ˜p. Consistency for
γK then follows by standard results on regression estimators.
Given consistency of the stage i + 1 regression, we have a con-
sistent estimator for ˜Vi+1. This allows us to repeat the consis-
tency argument for ˜Qi, as above.
(cid:3)

(CAN)

The estimator in (17)

is consistent and
Theorem 5
in the set
asymptocally normal
{π(W ; ψ), p(M |W, A; φ)} are correctly speciﬁed, and the es-
timator in (18) is CAN in the union model, where any two mod-
els in the set {π(W ; ψ), E[Y |A, M, W ; ζ], p(M |W, A; φ)}
are correctly speciﬁed.

the models

if

Proof: This proof follows as an extension of consistency results
derived for the triply robust estimator of the counterfactual ex-
pectation β = E[Y (a, M (a(cid:48)))] associated with the natural di-
rect effect in [18].

Assume the models in the set {π(W ; ψ), p(M |W, A; φ)} are
correctly speciﬁed. Let

g(W ; ψ, φ) ≡

p(M |A = a(cid:48), W ; φ)
p(M |A = f (W ), W ; φ)(cid:101)π(W ; ψ)

We have

This is equal to

E

(cid:105)
(cid:104)
Y (cid:101)Cg(W ; ψ, φ)

= E

(cid:104)

(cid:104)

E

(cid:12)
(cid:12)
Y (cid:101)Cg(W ; ψ, φ)
(cid:12)W
(cid:104)
E [Y |W ] (cid:101)Cg(W ; ψ, φ)

(cid:105)(cid:105)

(cid:105)

.

= E

(cid:90)

(cid:90)

(cid:90)

=

=

Y p(Y |M, A, W )p(M |A, W )p(A|W ) (cid:101)Cg(A, W )dA, dM, dY

Y p(Y |M, A, W )p(M |a(cid:48), W )p(A|W )

I(A = f (W ))
(cid:101)π(W )
Y p(Y |M, A = f (W ), W )p(M |a(cid:48), W )dM, dY.

dA, dM, dY

This is precisely β of interest.

The estimator (cid:98)βtriple has the form

(cid:34)

E

(cid:101)C
(cid:101)π(W ; ψ)
(cid:110)

I(A = a(cid:48))
π(cid:48)

a(W ; (cid:98)ψ)

f (M |W, A = a(cid:48); (cid:98)φ)
f (M |W, f (W ); (cid:98)φ)

(cid:110)

Y − E[Y |f (W ), M, W ; (cid:98)ζ]

(cid:111)

+

E[Y |f (W ), M, W ; (cid:98)ζ] −

E[Y |f (W ), M, W ; (cid:98)ζ].

(cid:88)

M

p(M |W, A = a(cid:48); (cid:98)ζ)

(cid:111)

+

(cid:88)

(cid:35)
E[Y |f (W ), M, W ; (cid:98)ζ]p(M |W, A = a(cid:48); (cid:98)φ)

,

M

Assume ˜π was speciﬁed incorrectly. The expectation in the
estimator consists of three terms, where the last term is equal

to true β if models for Y and M are correct. For the ﬁrst term
we have, by iterated expectation,

in (cid:98)βtriple then decompose into

(cid:101)Cg(W ; φ, ψ)

Y − E[Y |f (W ), M, W ; (cid:98)ζ]

(cid:111)(cid:105)

=

(cid:101)Cg(W ; φ, ψ)

E[Y |A, M, W ] − E[Y |f (W ), M, W ; (cid:98)ζ]

(cid:110)

(cid:110)

(cid:110)

(cid:104)

(cid:104)

(cid:104)

E

E

E

(cid:111)(cid:105)

=

(cid:111)(cid:105)

= 0,

(cid:101)Cg(W ; φ, ψ)

E[Y |f (W ), M, W ] − E[Y |f (W ), M, W ; (cid:98)ζ]

+ E

E[Y |f (W ), M, W ; (cid:98)ζ]p(M |W, A = a(cid:48); (cid:98)ζ)

if the Y model is correct. For the second term we have, by
iterated expectation,

(cid:34)(cid:32)

E

1 −

1 − (cid:101)C
1 − (cid:101)π(W ; ψ)

(cid:35)(cid:35)

(cid:33)(cid:12)
(cid:12)
(cid:12)
W
(cid:12)
(cid:12)

− (cid:101)Cg(W ; ψ, φ)

E[Y |f (W ), M, W ; ζ]

(cid:19)

(cid:21)

(cid:105)

(cid:104)

E

(cid:101)Cg(W ; ψ, φ)Y
(cid:20)(cid:18) I(A = a)
πa(W ; ψ)

+ E

(cid:34)

(cid:88)

M

E[Y |f (W ), M, W ; (cid:98)ζ] −

E[Y |f (W ), M, W ; (cid:98)ζ]

The last term is mean zero if (cid:101)π is speciﬁed correctly. The
second term is equal to

(cid:111)(cid:105)

=

E

(cid:110)

π(cid:48)

(cid:34) I(A = a(cid:48))
a(W ; (cid:98)ψ)
p(M |W, A = a(cid:48); (cid:98)ζ)
(cid:34) I(A = a(cid:48))
(cid:34)
a(W ; (cid:98)ψ)

π(cid:48)

(cid:110)

E

E

p(M |W, A = a(cid:48); (cid:98)ζ)
(cid:34) I(A = a(cid:48))
a(W ; (cid:98)ψ)

π(cid:48)

(cid:110)

E

(cid:88)

M

(cid:88)

M

E[Y |f (W ), M, W ; (cid:98)ζ] −

E[Y |f (W ), M, W ; (cid:98)ζ]

(cid:111)(cid:12)
(cid:12)

(cid:12)W, A = a(cid:48)(cid:105)(cid:105)

=

E[E[Y |f (W ), M, W ; (cid:98)ζ]|A = a(cid:48), W ]

E[Y |f (W ), M, W ; (cid:98)ζ]p(M |W, A = a(cid:48); (cid:98)ζ)

= 0

(cid:35)

(cid:111)

(cid:88)

−

M

if the models for Y and M are correct.

(cid:90)

(cid:90)

E[Y |f (W ), M, W ; ζ]p(M |A = a(cid:48), W )p(W )−

E[Y |f (W ), M, W ; ζ]p(M |A = a(cid:48), W )p(W ) = 0

if the A and M models are speciﬁed correctly. The ﬁrst term
is equal to β by the argument above.

Both estimators are special cases of the RAL estimator for β
based on the efﬁcient inﬂuence function. As a result, standard
regularity assumptions [13], and properties of maximum likeli-
hood estimators imply both estimators are CAN.
(cid:3)

Assume the model for M was speciﬁed incorrectly. The ﬁrst
term in the estimator is mean zero by above argument, since
the Y model is still correct.

C: Experiments And Visualizations

C1. Models Used In Data Analysis

The second and last terms decompose into

E[E[Y |f (W ), M, W ; (cid:98)ζ]|A = a(cid:48), W ]

E[Y |f (W ), M, W ; (cid:98)ζ]p(M |W, A = a(cid:48); (cid:98)ζ)

(cid:35)

(cid:111)

(cid:35)

(cid:35)

(cid:35)

E

(cid:110)

(cid:34) I(A = a(cid:48))
π(cid:48)
a(W ; (cid:98)ψ)
(cid:34) I(A = a(cid:48))
a(W ; (cid:98)ψ)

π(cid:48)

− E

(cid:88)

M

(cid:34)

(cid:88)

M

(cid:88)

M

=E

(cid:110)

(cid:34) I(A = a(cid:48))
π(cid:48)
a(W ; (cid:98)ψ)
(cid:34)

E

(cid:34)(cid:32)

1 −

a(W ; (cid:98)ψ)

I(A = a(cid:48))
π(cid:48)
(cid:34) I(A = a(cid:48))
π(cid:48)
a(W ; (cid:98)ψ)
(cid:34)

E

(cid:33)(cid:35)

(cid:88)

M

+ E

(cid:32)

π(cid:48)

a(W )
a(W ; (cid:98)ψ)

1 −

π(cid:48)
(cid:34) I(A = a(cid:48))
a(W ; (cid:98)ψ)

π(cid:48)

=E

+ E

E[Y |f (W ), M, W ; (cid:98)ζ]p(M |W, A = a(cid:48); (cid:98)φ)

E[E[Y |f (W ), M, W ; (cid:98)ζ]|A = a(cid:48), W ]

+ E

E[Y |f (W ), M, W ; (cid:98)ζ]p(M |W, A = a(cid:48); (cid:98)ζ)

(cid:35)(cid:35)

=

(cid:33)(cid:12)
(cid:12)
(cid:12)
W
(cid:12)
(cid:12)

(cid:35)
E[E[Y |f (W ), M, W ; (cid:98)ζ]|A = a(cid:48), W ]

E[Y |f (W ), M, W ; (cid:98)ζ]p(M |W, A = a(cid:48); (cid:98)ζ)

(cid:35)
E[E[Y |f (W ), M, W ; (cid:98)ζ]|A = a(cid:48), W ]

=E

(cid:104)
E[E[Y |f (W ), M, W ; (cid:98)ζ]|A = a(cid:48), W ]

(cid:105)

if the models for Y and A are correct. The remainder is pre-
cisely β.

Assume the model for Y was speciﬁed incorrectly. The terms

We used linear regression with interaction terms between
treatment A2 and history H2 to model the outcome W2:
E[W2|H2, A2, M2; α] = α1(H2, A2, M2) + α2A2H2, and
logistic regression with interaction terms to model all dichoto-
mous variables X with history H and immediate prior treat-
ment A: logit(cid:8)p(X = 1 | H, A; β)(cid:9) = β1H + β2AH for
X ∈ {M1, M2, W1}. We used the same form of linear re-
gression with interaction terms to model Q-functions by ex-
cluding the mediators: Q2(H2, A2; γ2) = γ2
2 A2H2
and Q1(H1, A1; γ1) = γ1
2 A1H1. The parameters in
all models were estimated by maximizing the likelihood.

1 H2 + γ2

1 H1 + γ1

For value search and G-estimation, we used log CD4 count at
the end of sixth month as the outcome of interest, denoted by
W1. We used the same form of models, as described above, for
W1, E[W1|H1, A1, M1; α], and all the mediators, and used
logistic regression with no interaction terms to model the treat-
ment assignment: logit(cid:8)p(A1 = 1 | H1; β)(cid:9) = β0 + β1H1.
We modeled the blip function in (19) as γ(A1, H1; ψ) =
ψ1A1 + ψ2A1H1.

C2: Decision Tree Visualization Of Learned Policies

We derived the optimal policies using G-formula, Q-learning,
G-estimation, and value search techniques. The value search
method considered a simple class of policies based on a thresh-
old, described in the main body of the paper. The optimal poli-
cies obtained from the ﬁrst three methods were more compli-
cated functions of prior history. To aid in interpretability of
these policies, we approximated them by means of decision
tree multi-label classiﬁers which treated history as a set of fea-
tures, and treatment decision as the class label. The resulting
In
decision tree classiﬁers are shown in in Fig. 3, 4, and 5.

Table 2: Population log CD4 counts under different policies
(under treatment assignments in the observed data, the value is
5.64 ± 0.01 in the 2-stage and 5.54 ± 0.01 in the 1-stage prob-
lem). G-formula and Q-learning are used with 2-stage decision
points. Value search and G-estimation are used with 1-stage
decision point.

Path Policies
(not through adherence)
6.78 (5.65, 6.92)
7.00 (4.82, 7.19)
5.56 (5.44, 5.60)
5.56 (5.55, 5.58)

G-formula
Q-learning
Value search
G-estimation

these ﬁgures, the label “path policies” corresponds to policies
that optimize the direct chemical effect of the drug where drug
toxicity and adherence behave as if treatment was set to a refer-
ence level. In the following decision trees, the nodes vl and adh
stand for viral load (log scale) and adherence level, repectively.
m00 and m06 denote the measures at month zero (baseline) and
the end of the ﬁrst six months, and node who denotes the stage
of disease (there is a total of 4 stages, with higher stages de-
noting progressively more severe disease). Since classiﬁers did
not achieve perfect accuracy, these decisions trees should be
viewed as easy to visualize approximations of the true learned
policies.

Note that in Fig. 3, adherence level is relevant to the overall pol-
icy but is omitted in the path speciﬁc one. As mentioned above,
the classiﬁcation accuracies are not perfect and hence visual-
izations are not necessarily a good representation of the true
policy. That said, ﬁnding the path-speciﬁc policy not via M s
corresponds to ﬁnding the overall policy in the world shown in
Fig. 2 (b) in the main body. It is true that in this world, ad-
herence at time one, ˜M1, inﬂuences A2, and as a result it is in
principle possible for adherence at time one to be informative
in an interesting way for the decision at time two. However,
one large source of variability in patient adherence is precisely
due to the treatment we assign, and this source of variability is
removed by construction in the world shown in Fig. 2(b) in the
main body of the paper – the world where everyone adheres as
if on a reference treatment. A low variability variable is cer-
tainly less likely to be relevant for decision making (consider
what would happen in the limit where everyone had perfect ad-
herence had they been on a reference treatment). Hence, we
are certainly not surprised to ﬁnd that a path-speciﬁc (not via
adherence) policy did not include adherence as a relevant vari-
able.

C3. Additional Experiments

To tie the results of this paper to earlier work [4], we ran ad-
ditional experiments to ﬁnd policies that optimize the chem-
ical effect of the drug where only adherence behaves as if
the treatments were set to a reference level. Expected out-
comes under optimal policies we learned, along with 95% con-
ﬁdence intervals obtained by bootstrap, are shown in Table. 2.
The results are consistent with the ones provided in the main
body of the paper. For value search, under the same class of
policies, I{CD4m00 < α}, and the same modeling assump-
tions described above, the optimal path policy is chosen to be
I{CD4m00 < 550 cells/mm3}.

Figure 3: Decision trees for optimal policies obtained via G-formula.

Figure 4: Decision trees for optimal policies obtained via Q-learning.

Figure 5: Decision trees for optimal policies obtained via G-estimation.

