Smooth and Sparse Optimal Transport

8
1
0
2
 
b
e
F
 
0
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
7
2
6
0
.
0
1
7
1
:
v
i
X
r
a

Mathieu Blondel
NTT Communication Science Laboratories

Vivien Seguy*
Kyoto University

Antoine Rolet*
Kyoto University

Abstract

Entropic regularization is quickly emerging
as a new standard in optimal transport (OT).
It enables to cast the OT computation as a
diﬀerentiable and unconstrained convex op-
timization problem, which can be eﬃciently
solved using the Sinkhorn algorithm. How-
ever, entropy keeps the transportation plan
strictly positive and therefore completely
dense, unlike unregularized OT. This lack of
sparsity can be problematic in applications
where the transportation plan itself is of in-
terest. In this paper, we explore regularizing
the primal and dual OT formulations with
a strongly convex term, which corresponds to
relaxing the dual and primal constraints with
smooth approximations. We show how to
incorporate squared 2-norm and group lasso
regularizations within that framework, lead-
ing to sparse and group-sparse transporta-
tion plans. On the theoretical side, we bound
the approximation error introduced by reg-
ularizing the primal and dual formulations.
Our results suggest that, for the regularized
primal, the approximation error can often be
smaller with squared 2-norm than with en-
tropic regularization. We showcase our pro-
posed framework on the task of color transfer.

1

Introduction

Optimal transport (OT) distances (a.k.a. Wasser-
stein or earth mover’s distances) are a powerful com-
putational tool to compare probability distributions
and have recently found widespread use in machine
learning (Cuturi, 2013; Solomon et al., 2014; Kus-
ner et al., 2015; Courty et al., 2016; Arjovsky et al.,

Proceedings of the 21st International Conference on Ar-
tiﬁcial Intelligence and Statistics (AISTATS) 2018, Lan-
zarote, Spain. PMLR: Volume 84. Copyright 2018 by the
author(s). *Work performed during an internship at NTT.

2017). While OT distances exhibit a unique ability
to capture the geometry of the data, their applica-
tion to large-scale problems has been largely ham-
pered by their high computational cost. Indeed, com-
puting OT distances involves a linear program, which
takes super-cubic time in the data size to solve using
state-of-the-art network-ﬂow algorithms. Related to
the Schr¨odinger problem (Schr¨odinger, 1931; L´eonard,
2012), entropy-regularized OT distances have recently
gained popularity due to their desirable properties
(Cuturi, 2013). Their computation involves a compar-
atively easier diﬀerentiable and unconstrained convex
optimization problem, which can be solved using the
Sinkhorn algorithm (Sinkhorn and Knopp, 1967). Un-
like unregularized OT distances, entropy-regularized
OT distances are also diﬀerentiable w.r.t. their inputs,
enabling their use as a loss function in a machine learn-
ing pipeline (Frogner et al., 2015; Rolet et al., 2016).

Despite its considerable merits, however, entropy-
regularized OT has some limitations, such as introduc-
ing blurring in the optimal transportation plan. While
this nuisance can be reduced by using small regulariza-
tion, this requires a carefully engineered implementa-
tion, since the naive Sinkhorn algorithm is numerically
unstable in this regime (Schmitzer, 2016). More im-
portantly, the entropy term keeps the transportation
plan strictly positive and therefore completely dense,
unlike unregularized OT. This lack of sparsity can be
problematic when the optimal transportation plan it-
self is of interest, e.g., in color transfer (Piti´e et al.,
2007), domain adaptation (Courty et al., 2016) and
ecological inference (Muzellec et al., 2017). Sparsity in
these applications is motivated by the principle of par-
simony (simple solutions should be preferred) and by
the enhanced interpretability of transportation plans.

Our contributions. This background motivates us
to study regularization schemes that lead to smooth
optimization problems (i.e., diﬀerentiable everywhere
and with Lipschitz continuous gradient) while retain-
ing the desirable property of sparse transportation
plans. To do so, we make the following contributions.

We regularize the primal with an arbitrary strongly
convex term and derive the corresponding smoothed

Smooth and Sparse Optimal Transport

Unregularized
Sparsity: 94%

Smoothed semi-dual (ent.)
Sparsity: 0%

Smoothed semi-dual (sq. 2-norm)
Sparsity: 90%

Semi-relaxed primal (Eucl.)
Sparsity: 91%

Figure 1: Comparison of transportation plans obtained by diﬀerent formulations on the application of color
transfer. The top and right histograms represent the color distributions a ∈ (cid:52)m and b ∈ (cid:52)n of two images.
For the sake of illustration, the number of colors is reduced to m = n = 32, using k-means clustering. Small
squares indicate non-zero elements in the obtained transportation plan, denoted by T throughout this paper.
The sparsity indicated below each graph is the percentage of zero elements in T . The weight of the elements of
T indicates the extent to which colors from one image must be transferred to colors from the other image. Like
unregularized OT (ﬁrst from left), but unlike entropy-regularized OT (second from left), our squared 2-norm
regularized OT (third from left) is able to produce sparse transportation plans. This is also the case of our
relaxed primal (not shown) and semi-relaxed primal (fourth from left) formulations.

dual and semi-dual. Our derivations abstract away
regularization-speciﬁc terms in an intuitive way (§3).
We show how incorporating squared 2-norm and
group-lasso regularizations within that framework
leads to sparse solutions. This is illustrated in Fig-
ure 1 for squared 2-norm regularization.

Next, we explore the opposite direction:
replacing
one or both of the primal marginal constraints with
approximate smooth constraints. When using the
squared Euclidean distance to approximate the con-
straints, we show that this can be interpreted as adding
squared 2-norm regularization to the dual (§4). As
illustrated in Figure 1, that approach also produces
sparse transportation plans.

For both directions, we bound the approximation er-
ror caused by regularizing the original OT problem.
For the regularized primal, we show that the approxi-
mation error of squared 2-norm regularization can be
smaller than that of entropic regularization (§5). Fi-
nally, we showcase the proposed approaches empiri-
cally on the task of color transfer (§6).

An open-source Python implementation is available at
https://github.com/mblondel/smooth-ot.

Notation. We denote scalars, vectors and matrices
using lower-case, bold lower-case and upper-case let-
ters, e.g., t, t and T , respectively. Given a matrix T ,
we denote its elements by ti,j and its columns by tj.
We denote the set {1, . . . , m} by [m]. We use (cid:107) · (cid:107)p to
denote the p-norm. When p = 2, we simply write (cid:107) · (cid:107).
We denote the (m−1)-dimensional probability simplex

by (cid:52)m := {y ∈ Rm
+ : (cid:107)y(cid:107)1 = 1} and the Euclidean pro-
jection onto it by P(cid:52)m(x) := argminy∈(cid:52)m (cid:107)y − x(cid:107)2.
We denote [x]+ := max(x, 0), performed element-wise.

2 Background

Convex analysis. The convex conjugate of a function
f : Rm → R ∪ {∞} is deﬁned by

f ∗(x) := sup

y(cid:62)x − f (y).

(1)

y∈dom f

If f is strictly convex, then the supremum in (1) is
uniquely achieved. Then,
from Danskin’s theorem
(1966), it is equal to the gradient of f ∗:

∇f ∗(x) = argmax
y∈dom f

y(cid:62)x − f (y).

The dual of a norm (cid:107) · (cid:107) is deﬁned by (cid:107)x(cid:107)∗ :=
sup(cid:107)y(cid:107)≤1 y(cid:62)x. We say that a function is γ-smooth
w.r.t. a norm (cid:107) · (cid:107) if it is diﬀerentiable everywhere and
its gradient is γ-Lipschitz continuous w.r.t. that norm.
Strong convexity plays a crucial role in this paper due
to its well-known duality with smoothness: f is γ-
strongly convex w.r.t. a norm (cid:107) · (cid:107) if and only if f ∗ is
1
γ -smooth w.r.t. (cid:107) · (cid:107)∗ (Kakade et al., 2012).
Optimal transport. We focus throughout this pa-
per on OT between discrete probability distributions
a ∈ (cid:52)m and b ∈ (cid:52)n. Rather than performing a point-
wise comparison of the distributions, OT distances
compute the minimal eﬀort, according to some ground
cost, for moving the probability mass of one distribu-
tion to the other. The modern OT formulation, due to

Mathieu Blondel, Vivien Seguy, Antoine Rolet

Kantorovich [1942], is cast as a linear program (LP):

OT(a, b) := min

(cid:104)T, C(cid:105),

(2)

T ∈U (a,b)

where U(a, b) is the transportation polytope

U(a, b) := (cid:8)T ∈ Rm×n

: T 1n = a, T (cid:62)1m = b(cid:9)

+

+

and C ∈ Rm×n
is a cost matrix. The former can be in-
terpreted as the set of all joint probability distributions
with marginals a and b. Without loss of generality, we
will assume a > 0 and b > 0 throughout this paper (if
ai = 0 or bj = 0 then the ith row or the jth column
of T (cid:63) is zero). When n = m and C is a distance ma-
1
p is a distance on
trix raised to the power p, OT(·, ·)
(cid:52)n, called the Wasserstein distance of order p (Villani,
2003, Theorem 7.3). The dual LP is

OT(a, b) = max

α(cid:62)a + β(cid:62)b,

(3)

α,β∈P(C)

where P(C) := {α ∈ Rm, β ∈ Rn : αi + βj ≤ ci,j}.
Keeping α ﬁxed, an optimal solution w.r.t. β is

βj = min
i∈[m]

ci,j − αi,

∀j ∈ [n],

which is the so-called c-transform. Plugging it back
into the dual, we get the “semi-dual”

OT(a, b) = max
α∈Rm

α(cid:62)a −

bj max
i∈[m]

(αi − ci,j).

(4)

n
(cid:88)

j=1

For a recent and comprehensive survey of computa-
tional OT, see (Peyr´e and Cuturi, 2017).

3 Strong primal ↔ Relaxed dual

We study in this section adding strongly convex regu-
larization to the primal problem (2). We deﬁne

Deﬁnition 1 Strongly convex primal

OTΩ(a, b) := min

(cid:104)T, C(cid:105) +

Ω(tj),

(5)

T ∈U (a,b)

n
(cid:88)

j=1

where we assume that Ω is strongly convex over the
intersection of dom Ω and either Rm

+ or (cid:52)m.

These assumptions are suﬃcient for (5) to be strongly
convex w.r.t. T ∈ U(a, b). On ﬁrst sight, solving (5)
does not seem easier than (2). As we shall now see,
the main beneﬁt occurs when switching to the dual.

3.1 Smooth relaxed dual formulation

To deﬁne a smoothed version of δ, we take the convex
conjugate of Ω, restricted to the non-negative orthant:

δΩ(x) := sup
y≥0

y(cid:62)x − Ω(y).

(6)

If Ω is γ-strongly convex over Rm
+ ∩ dom Ω, then δΩ is
1
γ -smooth and its gradient is ∇δΩ(x) = y(cid:63), where y(cid:63)
is the supremum of (6). We next show that δΩ plays
a crucial role in expressing the dual of (5), which is a
smooth optimization problem in α and β.

Proposition 1 Smooth relaxed dual

n
(cid:88)

j=1

OTΩ(a, b) = max
α∈Rm
β∈Rn

α(cid:62)a+β(cid:62)b−

δΩ(α+βj1m −cj)

(7)
The optimal solution T (cid:63) of (5) can be recovered from
(α(cid:63), β(cid:63)) by t(cid:63)

j 1m − cj) ∀j ∈ [n].

j = ∇δΩ(α(cid:63) + β(cid:63)

For a proof, see Appendix A.1. Intuitively, the hard
dual constraints αi + βj − ci,j ≤ 0 ∀i ∈ [m] ∀j ∈ [n],
which we can write (cid:80)n
j=1 δ(α + βj1m − cj), are now
relaxed with soft ones by substituting δ with δΩ.

3.2 Smoothed semi-dual formulation

We now derive the semi-dual of (5), i.e., the dual (7)
with one of the two variables eliminated. Without loss
of generality, we proceed to eliminate β. To do so, we
use the notion of smoothed max operator. Notice that

max(x) := max
i∈[m]

xi = sup
y∈(cid:52)m

y(cid:62)x ∀x ∈ Rm.

This is indeed true, since the supremum is always
achieved at one of the simplex vertices. To deﬁne a
smoothed max operator (Nesterov, 2005), we take the
conjugate of Ω, this time restricted to the simplex:

maxΩ(x) := sup
y∈(cid:52)m

y(cid:62)x − Ω(y).

(8)

If Ω is γ-strongly convex over (cid:52)m ∩ dom Ω, then
maxΩ is 1
γ -smooth and its gradient is deﬁned by
∇maxΩ(x) = y(cid:63), where y(cid:63) is the supremum of (8).
We next show that maxΩ plays a crucial role in ex-
pressing the conjugate of OTΩ.

Lemma 1 Conjugate of OTΩ w.r.t. its ﬁrst argument

OT∗

Ω(α, b) = sup
a∈(cid:52)m

α(cid:62)a−OTΩ(a, b) =

bjmaxΩj (α−cj),

n
(cid:88)

j=1

(9)

Let the (non-smooth) indicator function of the non-
positive orthant be deﬁned as

where Ωj(y) := 1
bj

Ω(bjy).

(cid:40)

δ(x) :=

if x ≤ 0

0,
∞, o.w.

y(cid:62)x.

= sup
y≥0

A proof is given in Appendix A.2. With the conjugate,
we can now easily express the semi-dual of (5), which
involves a smooth optimization problem in α.

Smooth and Sparse Optimal Transport

Table 1: Closed forms for δΩ (used in smoothed dual), maxΩj (used in smoothed semi-dual) and their gradients.

Ω(y)

m
(cid:88)

γ

i=1

γ

2 (cid:107)y(cid:107)2

Negative entropy

yi log yi

Squared 2-norm

Group-lasso

γ

2 (cid:107)y(cid:107)2 + γµ

(cid:107)yG(cid:107)

(cid:88)

G∈G

∇δΩ(x)

x
γ −1m

e

γ log

maxΩj (x)
m
(cid:88)

xi
γ − γ log bj

e

i=1

∇maxΩj (x)

x
γ

e

xi
γ

(cid:80)m

i=1 e

[xi]2
+

1
γ [x]+

x(cid:62)y(cid:63) − γbj

2 (cid:107)y(cid:63)(cid:107)2

y(cid:63) = P(cid:52)m

(cid:17)

(cid:16) x
γbj

δΩ(x)
m
(cid:88)
e

xi
γ −1

γ

i=1
m
(cid:88)

1
2γ

i=1
(11)

(12)

No closed form available

Proposition 2 Smoothed semi-dual

OTΩ(a, b) = max
α∈Rm

α(cid:62)a − OT∗

Ω(α, b)

(10)

The optimal solution T (cid:63) of (5) can be recovered from
α(cid:63) by t(cid:63)

j = bj∇maxΩj (α(cid:63) − cj) ∀j ∈ [n].

Proof. OTΩ(a, b) is a closed and convex function of a.
Therefore, OTΩ(a, b) = OT∗∗

Ω (a, b). (cid:3)

We can interpret this semi-dual as a variant of (4),
where the max operator has been replaced with its
smoothed counterpart, maxΩj . Note that α(cid:63), as ob-
tained by solving the smoothed dual (7) or semi-dual
(10), is the gradient of OTΩ(a, b) w.r.t. a when α(cid:63)
is unique or a sub-gradient otherwise. This is useful
when learning with OTΩ as a loss, as done with en-
tropic regularization in (Frogner et al., 2015).

Solving the optimization problems. The dual and
semi-dual we derived are unconstrained, diﬀerentiable
and concave optimization problems. They can there-
fore be solved using gradient-based algorithms, as long
as we know how to compute ∇δΩ and ∇maxΩ. In our
experiments, we use L-BFGS (Liu and Nocedal, 1989),
for both the dual and semi-dual formulations.

3.3 Closed-form expressions

We derive in this section closed-form expressions for
δΩ, maxΩ and their gradients for speciﬁc choices of Ω.

Negative entropy. We choose Ω(y) = −γH(y),
where H(y) := − (cid:80)
i yi log yi is the entropy. For that
choice, we get analytical expressions for δΩ, maxΩ and
their gradients (cf. Table 1). Since Ω is γ-strongly con-
vex w.r.t. the 1-norm over (cid:52)m (Shalev-Shwartz, 2007,
Lemma 16), maxΩ is 1
γ -strongly smooth w.r.t. the ∞-
norm. However, since Ω is only strictly convex over
Rm
>0, δΩ is diﬀerentiable but not smooth. The dual
and semi-dual with this Ω were derived in (Cuturi and
Doucet, 2014) and (Genevay et al., 2016), respectively.

Next, we present two choices of Ω that induce sparsity
in transportation plans. The resulting dual and semi-
dual expressions are new, to our knowledge.

Squared 2-norm. We choose Ω(y) = γ
2 (cid:107)y(cid:107)2. We
again obtain closed-form expressions for δΩ, maxΩ and
their gradients (cf. Table 1). Since Ω is γ-strongly
convex w.r.t. the 2-norm over Rm, both δΩ and maxΩ
are 1
γ -strongly smooth w.r.t. the 2-norm. Projecting
a vector onto the simplex, as required to compute
maxΩ and its gradient, can be done exactly in worst-
case O(m log m) time using the algorithm of (Michelot,
1986) and in expected O(m) time using the random-
ized pivot algorithm of (Duchi et al., 2008). Squared
2-norm regularization can output exactly sparse trans-
portation plans (the primal-dual relationship for (7) is
i,j = 1
t(cid:63)
j − ci,j]+) and is numerically stable
without any particular implementation trick.

i + β(cid:63)

γ [α(cid:63)

Group lasso. Courty et al. (2016) recently proposed
to use Ω(y) = γ((cid:80)
i yi log yi + µ (cid:80)
G∈G (cid:107)yG(cid:107)), where
yG denotes the subvector of y restricted to the set
G, and showed that this regularization improves ac-
curacy in the context of domain adaptation. Since Ω
includes a negative entropy term, the same remarks as
for negative entropy apply regarding the diﬀerentiabil-
ity of δΩ and smoothness of maxΩ. Unfortunately, a
closed-form solution is available for neither (6) nor (8).
However, since the log keeps y in the strictly positive
orthant and (cid:107)yG(cid:107) is diﬀerentiable everywhere in that
orthant, we can use any proximal gradient algorithm
to solve these problems to arbitrary precision.

A drawback of this choice of Ω, however, is that group
sparsity is never truly achieved. To address this issue,
we propose to use Ω(y) = γ( 1
G∈G (cid:107)yG(cid:107))
instead. For that choice, δΩ is smooth and is equal to
δΩ(x) = x(cid:62)y(cid:63) − Ω(y(cid:63)),
where y(cid:63) decomposes over groups G ∈ G and equals

2 (cid:107)y(cid:107)2 + µ (cid:80)

(11)

y(cid:63)
G = argmin
yG≥0

1
2

(cid:107)yG − xG/γ(cid:107)2 + µ(cid:107)yG(cid:107) = ∇δΩ(x)G.

(12)
As noted in the context of group-sparse NMF (Kim
et al., 2012), (12) admits a closed-form solution

y(cid:63)
G = argmin

yG

1
2

(cid:13)
(cid:13)yG − x+
G

(cid:13)
2
(cid:13)

+µ(cid:107)yG(cid:107) =

1−

(cid:20)

(cid:21)

µ
(cid:107)x+
G(cid:107)

x+
G,

+

Mathieu Blondel, Vivien Seguy, Antoine Rolet

where we deﬁned x+ := 1
γ [x]+. We have thus ob-
tained an eﬃcient way to compute exact gradients of
δΩ, making it possible to solve the dual using gradient-
based algorithms. In contrast, Courty et al. (2016) use
a generalized conditional gradient algorithm whose it-
erations require expensive calls to Sinkhorn. Finally,
because t(cid:63)
j 1m − cj) ∀j ∈ [n], the ob-
tained transportation plan will be truly group-sparse.

j = ∇δΩ(α(cid:63) + β(cid:63)

4 Relaxed primal ↔ Strong dual

We now explore the opposite way to deﬁne smooth OT
problems while retaining sparse transportation plans:
replace marginal constraints in the primal with ap-
proximate constraints. When relaxing both marginal
constraints, we deﬁne the next formulation:

Deﬁnition 2 Relaxed smooth primal

ROTΦ(a, b) := min
T ≥0

(cid:104)T, C(cid:105)+

Φ(T 1n, a)+

Φ(T (cid:62)1m, b),

1
2

1
2

(13)

where Φ(x, y) is a smooth divergence measure.

We may also relax only one of the marginal constraints:

Deﬁnition 3 Semi-relaxed smooth primal

(cid:93)ROTΦ(a, b) := min

(cid:104)T, C(cid:105) + Φ(T 1n, a)

(14)

T ≥0
T (cid:62)1m=b

where Φ(x, y) is deﬁned as in Deﬁnition 2.

For both (13) and (14), the transportation plans will
be typically sparse. As discussed in more details in §7,
these formulations are similar to (Frogner et al., 2015;
Chizat et al., 2016), with the key diﬀerence that we
do not regularize T with an entropic term. In addi-
tion, for Φ, we propose to use Φ(x, y) = 1
2γ (cid:107)x − y(cid:107)2,
which is 1
γ -smooth, while these works use a general-
ized Kullback-Leibler (KL) divergence, which is not
smooth. Relaxing the marginal constraints is useful
when normalizing input measures to unit mass is not
suitable (Gramfort et al., 2015) or to allow for only
partial displacement of mass. Relaxing only one of
the two constraints is useful in color transfer (Rabin
et al., 2014), where we would like all the probability
mass of the source image to be accounted for but not
necessarily for the reference image.

Dual interpretation. As we show in Appendix A.3,
in the case Φ(x, y) = 1
2γ (cid:107)x − y(cid:107)2, the dual of (13)
can be interpreted as the original dual with additional
squared 2-norm regularization on the dual variables α
and β. For the dual of (14), the additional regulariza-
tion is on α only (on the original dual or equivalently
on the original semi-dual). For that choice of Φ, the

duals of (13) and (14) are strongly convex. The dual
formulations are crucial to derive our bounds in §5.

Solving the optimization problems. While the re-
laxed and semi-relaxed primals (13) and (14) are still
constrained problems, it is much easier to project on
their constraint domain than on U(a, b). For the re-
laxed primal, in our experiments we use L-BFGS-B, a
variant of L-BFGS suitable for box-constrained prob-
lems (Byrd et al., 1995). For the semi-relaxed primal,
we use FISTA (Beck and Teboulle, 2009). Since the
constraint domain of (14) has the structure of a Carte-
sian product b1(cid:52)m ×· · ·× bn(cid:52)m, we can easily project
any T on it by column-wise projection on the (scaled)
simplex. Although not exlored in this paper, the block
Frank-Wolfe algorithm (Lacoste-Julien et al., 2012) is
also a good ﬁt for the semi-relaxed primal.

5 Theoretical bounds

Convergence rates. The dual (7) is not smooth in
α and β when using entropic regularization but it is
when using the squared 2-norm, with constant upper-
bounded by n/γ w.r.t. α and m/γ w.r.t. β. The semi-
dual (10) is smooth for both regularizations, with the
same constant of 1/γ, albeit not in the same norm. The
relaxed and semi-relaxed primals (13) and (14) are
both 1/γ-smooth when using Φ(x, y) = 1
2γ (cid:107)x − y(cid:107)2.
However, none of these problems are strongly con-
vex. From standard convergence analysis of (pro-
jected) gradient descent for smooth but non-strongly
convex problems, the number of iterations to reach
an (cid:15)-accurate solution w.r.t. the smoothed problems is
O(1/γ(cid:15)) or O(1/√

γ(cid:15)) with Nesterov acceleration.

Approximation error. Because the smoothed prob-
lems approach unregularized OT as γ → 0, there is a
trade-oﬀ between convergence rate w.r.t. the smoothed
problem and approximation error w.r.t. unregularized
OT. A question is then which smoothed formulations
and which regularizations have better approximation
error. Our ﬁrst theorem bounds OTΩ −OT in the case
of entropic and squared 2-norm regularization.

Theorem 1 Approximation error of OTΩ

Let a ∈ (cid:52)m and b ∈ (cid:52)n. Then,

γL ≤ OTΩ(a, b) − OT(a, b) ≤ γU,

where we deﬁned L and U as follows.

Ω

L

Neg. entropy

−H(a) − H(b)

U − max{H(a), H(b)}

1
2

m,n
(cid:88)

Squared 2-norm
(cid:18) ai
n

bj
1
m
mn
2 min (cid:8)(cid:107)a(cid:107)2, (cid:107)b(cid:107)2(cid:9)

−

+

i,j=1
1

(cid:19)2

Smooth and Sparse Optimal Transport

Figure 2: Result comparison for diﬀerent formulations on the task of color transfer. For regularized formulations,
we solve the optimization problem with γ ∈ {10−4, 10−2, . . . , 104} and choose the most visually pleasing result.
The sparsity indicated below each image is the percentage of zero elements in the optimal transportation plan.

Proof is given in Appendix A.4. Our result suggests
that, for the same γ, the approximation error can of-
ten be smaller with squared 2-norm than with entropic
regularization.
In particular, this is true whenever
2 min (cid:8)(cid:107)a(cid:107)2, (cid:107)b(cid:107)2(cid:9), which is of-
min{H(a), H(b)} > 1
ten the case in practice since 0 ≤ min{H(a), H(b)} ≤
2 min (cid:8)(cid:107)a(cid:107)2, (cid:107)b(cid:107)2(cid:9) ≤
min{log m, log n} while 0 ≤ 1
1
2 . Our second theorem bounds OT − ROTΦ and
OT− (cid:93)ROTΦ when Φ is the squared Euclidean distance.
Theorem 2 Approximation error of ROTΦ, (cid:93)ROTΦ
Let a ∈ (cid:52)m, b ∈ (cid:52)n, Φ(x, y) = 1
2γ (cid:107)x − y(cid:107)2. Then,

0 ≤ OT(a, b) − ROTΦ(a, b) ≤ γL
0 ≤ OT(a, b) − (cid:93)ROTΦ(a, b) ≤ γ (cid:101)L

(cid:9)

where we deﬁned
∞ min{ν1 + n, ν2 + m}2
L := (cid:107)C(cid:107)2
ν1 := max (cid:8)(2 + n/m) (cid:12)
(cid:12)
(cid:12)
(cid:12)a−1(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)b−1(cid:12)
(cid:12)∞ ,
(cid:12)
(cid:12)
(cid:12)
(cid:12)∞
(cid:12)
(cid:12)
ν2 := max (cid:8)(cid:12)
(cid:12)
(cid:12)∞ , (2 + m/n) (cid:12)
(cid:12)
(cid:12)a−1(cid:12)
(cid:12)
(cid:12)b−1(cid:12)
(cid:9)
(cid:12)
(cid:12)
(cid:12)
(cid:12)∞
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)a−1(cid:12)
2
(cid:101)L := 2 ||C||2
∞ .
(cid:12)
(cid:12)
(cid:12)
Proof is given in Appendix A.5. While the bound for
(cid:93)ROTΦ is better than that of ROTΦ, both are worse
than that of OTΩ, suggesting that the smoothed dual
formulations are the way to go when low approxima-
tion error w.r.t. unregularized OT is important.

∞

6 Experimental results

We showcase our formulations on color transfer, which
is a classical OT application (Piti´e et al., 2007). More
experimental results are presented in Appendix C.

6.1 Application to color transfer

Experimental setup. Given an image of size u × v,
we represent its pixels in RGB color space. We apply
k-means clustering to quantize the image down to m
colors. This produces m color centroids x1, . . . , xm ∈
R3. We can count how many pixels were assigned to
each centroid and normalizing by uv gives us a color
histogram a ∈ (cid:52)m. We repeat the same process with
a second image to obtain y1, . . . , yn ∈ R3 and b ∈ (cid:52)n.
Next, we apply any of the proposed methods with cost
matrix ci,j = d(xi, yj), where d is some discrepancy
measure, to obtain a (possibly relaxed) transportation
plan T ∈ Rm×n
. For each color centroid xi, we apply
a barycentric projection to obtain a new color centroid

+

ˆxi := argmin

ti,j d(x, yj).

n
(cid:88)

j=1

x∈R3

When d(x, y) = (cid:107)x − y(cid:107)2, as used in our experi-
ments, the above admits a closed-form solution: ˆxi =
(cid:80)n
j=1 ti,j yj
. Finally, we use the new color ˆxi for all
(cid:80)n
j=1 ti,j
pixels assigned to xi. The same process can be per-
formed with respect to the yj, in order to transfer
the colors in the other direction. We use two public
domain images “fall foliage” by Bernard Spragg and
“comunion” by Abel Maestro Garcia, and reduce the
number of colors to m = n = 4096. We compare
smoothed dual approaches and (semi-)relaxed primal
approaches. For the semi-relaxed primal, we also com-
pared with Φ(x, y) = 1
γ KL(x||y), where KL(x||y) is
the generalized KL divergence, x(cid:62) log
− x(cid:62)1 +
y(cid:62)1. This choice is diﬀerentiable but not smooth. We
ran the aforementioned solvers for up to 1000 epochs.

(cid:16) x
y

(cid:17)

Mathieu Blondel, Vivien Seguy, Antoine Rolet

Figure 3: Solver comparison for the smoothed dual and semi-dual, with squared 2-norm regularization. With
γ = 10, which was also the best value selected in Figure 2, the maximum is reached in less than 4 minutes.

Results. Our results are presented in Figure 2. All
formulations clearly produced better results than un-
regularized OT. With the exception of the entropy-
smoothed semi-dual formulation, all formulations pro-
duced extremely sparse transportation plans. The
semi-relaxed primal formulation with Φ set to the
squared Euclidean distance was the only one to pro-
duce colors with a darker tone.

6.2 Solver and objective comparison

We compared the smoothed dual and semi-dual when
In addition to
using squared 2-norm regularization.
L-BFGS on both objectives, we also compared with
alternating minimization in the dual. As we show in
Appendix B, exact block minimization w.r.t. α and β
can be carried out by projection onto the simplex.

Results. We ran the comparison using the same data
as in §6.1. Results are indicated in Figure 3. When
the problem is loosely regularized, we made two key
ﬁndings: i) L-BFGS converges much faster in the semi-
dual than in the dual, ii) alternating minimization con-
verges extremely slowly. The reason for i) could be
the better smoothness constant of the semi-dual (cf.
§5). Since alternating minimization and the semi-dual
have roughly the same cost per iteration (cf. Appendix
B), the reason for ii) is not iteration cost but a con-
vergence issue of alternating minimization. When us-
ing larger regularization, L-BFGS appears to converge
slighly faster on the dual than on the semi-dual, which
is likely thanks to its cheap-to-compute gradients.

6.3 Approximation error comparison

We compared empirically the approximation error of
smoothed formulations w.r.t. unregularized OT ac-
transportation plan error,
cording to four criteria:
marginal constraint error, value error and regularized
value error (cf. Figure 4 for a precise deﬁnition). For
the dual approaches, we solved the smoothed semi-
dual objective (10), since, as we discussed in §5, it has
the same smoothness constant of 1/γ for both entropic

and squared 2-norm regularizations, implying similar
convergence rates in theory. In addition, in the case of
entropic regularization, the expressions of maxΩ and
∇maxΩ are trivial to stabilize numerically using stan-
dard log-sum-exp implementation tricks.

Results. We ran the comparison using the same data
as in §6.1. Results are indicated in Figure 4. For
the transportation plan error and the (regularized)
value error, entropic regularization required 100 times
smaller γ to achieve the same error. This conﬁrms, as
suggested by Theorem 1, that squared 2-norm regular-
ization is typically tighter. Unsurprisingly, the semi-
relaxed primal was tighter than the relaxed primal in
all four criteria. A runtime comparison of smoothed
formulations is also important. However, a rigorous
comparison would require carefully engineered imple-
mentations and is therefore left for future work.

7 Related work

Regularized OT. Problems similar to (5) for gen-
eral Ω were considered in (Dessein et al., 2016). Their
work focuses on strictly convex and diﬀerentiable Ω
for which there exists an associated Bregman diver-
gence. Following (Benamou et al., 2015), they show
that (5) can then be reformulated as a Bregman pro-
jection onto the transportation polytope and solved
using Dykstra’s algorithm [1985]. While Dykstra’s al-
gorithm can be interpreted implicitly as a two-block
alternating minimization scheme on the dual problem,
neither the dual nor the semi-dual expressions were
derived. These expressions allow us to make use of
arbitrary solvers, including quasi-Newton ones like L-
BFGS, which as we showed empirically, converge much
faster on loosely regularized problems. Our framework
can also accomodate non-diﬀerentiable regularizations
for which there does not exist an associated Bregman
divergence, such as those that include a group lasso
term. Squared 2-norm regularization was recently con-
sidered in (Li et al., 2016) as well as in (Essid and
Solomon, 2017) but for a reformulation of the Wasser-

Smooth and Sparse Optimal Transport

Figure 4: Approximation error w.r.t. unregularized OT empirically achieved by diﬀerent smoothed formulations
on the task of color transfer. Let T (cid:63) be en optimal solution of the unregularized LP (2) and T (cid:63)
γ be an optimal
solution of one of the smoothed formulations with regularization parameter γ. The transportation plan error is
(cid:107)T (cid:63)
γ , C(cid:105) −
(cid:104)T (cid:63), C(cid:105)|/(cid:104)T (cid:63), C(cid:105). The regularized value error is |v − (cid:104)T (cid:63), C(cid:105)|, where v is one of OTΩ(a, b), ROTΦ(a, b) and
(cid:93)ROTΦ(a, b). For the regularized value error, our empirical ﬁndings conﬁrm what Theorem 1 suggested, namely
that, for the same value of γ, squared 2-norm regularization is quite tighter than entropic regularization.

γ − T (cid:63)(cid:107)/(cid:107)T (cid:63)(cid:107). The marginal constraint error is (cid:107)T (cid:63)

γ )(cid:62)1m − b(cid:107). The value error is |(cid:104)T (cid:63)

γ 1n − a(cid:107) + (cid:107)(T (cid:63)

stein distance of order 1 as a min cost ﬂow problem
along the edges of a graph.

Relaxed OT. There has been a large number of pro-
posals to extend OT to unbalanced positive measures.
Static formulations with approximate marginal con-
straints based on the KL divergence have been pro-
posed in (Frogner et al., 2015; Chizat et al., 2016). The
main diﬀerence with our work is that these formula-
tions include an additional entropic regularization on
T . While this entropic term enables a Sinkhorn-like al-
gorithm, it also prevents from obtaining sparse T and
requires the tuning of an additional hyper-parameter.
Relaxing only one of the two marginal constraints with
an inequality was investigated for color transfer in (Ra-
bin et al., 2014). Benamou (2003) considered an inter-
polation between OT and squared Euclidean distances:

min
x∈(cid:52)m

OT(x, b) +

(cid:107)x − a(cid:107)2.

(15)

1
2γ

While on ﬁrst sight this looks quite diﬀerent, this is in
fact equivalent to our semi-relaxed primal formulation
when Φ(x, y) = 1

2γ (cid:107)x − y(cid:107)2 since (15) is equal to

(cid:104)T, C(cid:105) +

(cid:107)x − a(cid:107)2

1
2γ

(cid:104)T, C(cid:105) +

(cid:107)T 1n − a(cid:107)2 = (cid:93)ROTΦ(a, b).

1
2γ

min
x∈(cid:52)m

min
T ≥0
T 1n=x
T (cid:62)1m=b

= min
T ≥0
T (cid:62)1m=b

However, the bounds in §5 are to our knowledge new.
A similar formulation but with a group-lasso penalty
on T instead of 1
2γ (cid:107)T 1n − a(cid:107)2 was considered in the
context of convex clustering (Carli et al., 2013).

Smoothed LPs. Smoothed linear programs have
been investigated in other contexts. The two closest
works to ours are (Meshi et al., 2015b) and (Meshi
et al., 2015a), in which smoothed LP relaxations based
on the squared 2-norm are proposed for maximum a-
posteriori inference. One innovation we make com-
pared to these works is to abstract away the regular-
ization by introducing the δΩ and maxΩ functions.

8 Conclusion

We proposed in this paper to regularize both the pri-
mal and dual OT formulations with a strongly convex
term, and showed that this corresponds to relaxing the
dual and primal constraints with smooth approxima-
tions. There are several important avenues for future
work. The conjugate expression (9) should be useful
for barycenter computation (Cuturi and Peyr´e, 2016)
or dictionary learning (Rolet et al., 2016) with squared
2-norm instead of entropic regularization. On the the-
oretical side, while we provided convergence guaran-
tees w.r.t. the OT distance value as the regularization
vanishes, which suggested the advantage of squared
2-norm regularization, it would also be important to
study the convergence w.r.t. the transportation plan,
as was done for entropic regularization by Cominetti
and San Mart´ın (1994). Finally, studying optimization
algorithms that can cope with large-scale data is im-
portant. We believe SAGA (Defazio et al., 2014) is a
good candidate since it is stochastic, supports proxim-
ity operators, is adaptive to non-strongly convex prob-
lems and can be parallelized (Leblond et al., 2017).

Mathieu Blondel, Vivien Seguy, Antoine Rolet

Acknowledgements

We thank Arthur Mensch and the anonymous review-
ers for constructive comments.

References

On the transfer of masses (in russian). Doklady

Akademii Nauk, 37(2):227–229, 1942.

Jason Altschuler, Jonathan Wee, and Philippe Rigol-
let. Near-linear time approximation algorithms for
optimal transport via sinkhorn iteration. In Proc.
of NIPS, 2017.

Martin Arjovsky, Soumith Chintala, and L´eon Bot-
tou. Wasserstein generative adversarial networks.
In Proc. of ICML, volume 70, pages 214–223, 2017.

Amir Beck and Marc Teboulle. A fast iterative
shrinkage-thresholding algorithm for linear inverse
problems. SIAM Journal on Imaging Sciences, 2
(1):183–202, 2009.

Jean-David Benamou. Numerical resolution of an un-
balanced mass transport problem. ESAIM: Math-
ematical Modelling and Numerical Analysis, 37(5):
851–868, 2003.

Jean-David Benamou, Guillaume Carlier, Marco Cu-
turi, Luca Nenna, and Gabriel Peyr´e. Iterative breg-
man projections for regularized transportation prob-
lems. SIAM Journal on Scientiﬁc Computing, 37(2):
A1111–A1138, 2015.

Stephen Boyd and Lieven Vandenberghe. Convex Op-

timization. Cambridge University Press, 2004.

Richard H Byrd, Peihuang Lu, Jorge Nocedal, and
Ciyou Zhu. A limited memory algorithm for bound
constrained optimization. SIAM Journal on Scien-
tiﬁc Computing, 16(5):1190–1208, 1995.

Gilberto Calvillo and David Romero. On the closest
point to the origin in transportation polytopes. Dis-
crete Appl. Math., 210:88–102, 2016.

Francesca P Carli, Lipeng Ning, and Tryphon T Geor-
giou. Convex clustering via optimal mass transport.
arXiv preprint arXiv:1307.5459, 2013.

Lenaic Chizat, Gabriel Peyr´e, Bernhard Schmitzer,
Scaling algorithms
and Fran¸cois-Xavier Vialard.
for unbalanced transport problems. arXiv preprint
arXiv:1607.05816, 2016.

Roberto Cominetti and Jaime San Mart´ın. Asymp-
totic analysis of the exponential penalty trajectory
in linear programming. Mathematical Programming,
67(1-3):169–187, 1994.

Nicolas Courty, R´emi Flamary, Devis Tuia, and Alain
Rakotomamonjy. Optimal transport for domain

adaptation. IEEE transactions on pattern analysis
and machine intelligence, 2016.

Thomas M. Cover and Joy A. Thomas. Elements of

Information Theory. Wiley, 2006.

Marco Cuturi. Sinkhorn distances: Lightspeed com-
In Proc. of NIPS,

putation of optimal transport.
pages 2292–2300. 2013.

Marco Cuturi and Arnaud Doucet. Fast computation
of wasserstein barycenters. In International Confer-
ence on Machine Learning, pages 685–693, 2014.

Marco Cuturi and Gabriel Peyr´e. A smoothed dual ap-
proach for variational wasserstein problems. SIAM
Journal on Imaging Sciences, 9(1):320–343, 2016.

John M Danskin. The theory of max-min, with appli-
cations. SIAM Journal on Applied Mathematics, 14
(4):641–664, 1966.

Aaron Defazio, Francis Bach, and Simon Lacoste-
Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite ob-
jectives. In Proc. of NIPS, pages 1646–1654, 2014.

Arnaud Dessein, Nicolas Papadakis, and Jean-Luc
Rouas. Regularized optimal transport and the rot
mover’s distance. arXiv preprint arXiv:1610.06447,
2016.

John Duchi, Shai Shalev-Shwartz, Yoram Singer, and
Tushar Chandra. Eﬃcient projections onto the (cid:96)1-
ball for learning in high dimensions.
In Proc. of
ICML, 2008.

Richard L Dykstra. An iterative procedure for obtain-
ing i-projections onto the intersection of convex sets.
The annals of Probability, pages 975–984, 1985.

Montacer Essid and Justin Solomon. Quadratically-
arXiv

regularized optimal transport on graphs.
preprint arXiv:1704.08200, 2017.

Charlie Frogner, Chiyuan Zhang, Hossein Mobahi,
Mauricio Araya, and Tomaso A Poggio. Learning
with a wasserstein loss.
In Proc. of NIPS, pages
2053–2061, 2015.

Aude Genevay, Marco Cuturi, Gabriel Peyr´e, and
Francis Bach. Stochastic optimization for large-scale
optimal transport. In Proc. of NIPS, pages 3440–
3448. 2016.

Alexandre Gramfort, Gabriel Peyr´e, and Marco Cu-
turi. Fast optimal transport averaging of neuroimag-
ing data. In Proc. of International Conference on
Information Processing in Medical Imaging, pages
261–272, 2015.

Sham M Kakade, Shai Shalev-Shwartz, and Ambuj
Tewari. Regularization techniques for learning with
matrices. Journal of Machine Learning Research,
13:1865–1890, 2012.

Smooth and Sparse Optimal Transport

Jingu Kim, Renato DC Monteiro, and Haesun Park.
Group sparsity in nonnegative matrix factorization.
In Proc. of SIAM International Conference on Data
Mining, pages 851–862, 2012.

Fran¸cois Piti´e, Anil C Kokaram, and Rozenn Dahyot.
Automated colour grading using colour distribution
transfer. Computer Vision and Image Understand-
ing, 107(1):123–137, 2007.

Julien Rabin, Sira Ferradans, and Nicolas Papadakis.
Adaptive color transfer with relaxed optimal trans-
port. In Proc. of International Conference on Image
Processing, pages 4852–4856, 2014.

Antoine Rolet, Marco Cuturi, and Gabriel Peyr´e. Fast
dictionary learning with a smoothed wasserstein
loss. In Proc. of AISTATS, pages 630–638, 2016.

David Romero. Easy transportation-like problems on
k-dimensional arrays. Journal of Optimization The-
ory and Applications, 66:137–147, 1990.

Bernhard Schmitzer. Stabilized sparse scaling algo-
rithms for entropy regularized transport problems.
arXiv preprint arXiv:1610.06519, 2016.

Erwin Schr¨odinger. ¨Uber die umkehrung der naturge-

setze. Phys. Math., 144:144–153, 1931.

Shai Shalev-Shwartz. Online Learning: Theory, Algo-
rithms, and Applications. PhD thesis, The Hebrew
University of Jerusalem, 2007.

Richard Sinkhorn and Paul Knopp.

Concerning
nonnegative matrices and doubly stochastic matri-
ces. Paciﬁc Journal of Mathematics, 21(2):343–348,
1967.

Justin Solomon, Raif Rustamov, Guibas Leonidas, and
Adrian Butscher. Wasserstein propagation for semi-
supervised learning. In Proc. of ICML, pages 306–
314, 2014.

C´edric Villani. Topics in optimal transportation. Num-

ber 58. American Mathematical Soc., 2003.

Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian
Weinberger. From word embeddings to document
distances. In Proc. of ICML, pages 957–966, 2015.

Simon Lacoste-Julien, Martin Jaggi, Mark Schmidt,
and Patrick Pletscher. Block-coordinate Frank-
Wolfe optimization for structural SVMs.
In Proc.
of ICML, 2012.

Rmi Leblond, Fabian Pedregosa, and Simon Lacoste-
Julien. ASAGA: Asynchronous Parallel SAGA. In
Proceedings of the 20th International Conference
on Artiﬁcial Intelligence and Statistics, volume 54,
pages 46–54, 2017.

Christian L´eonard. A survey of the Schr¨odinger prob-
lem and some of its connections with optimal trans-
port. Discrete and Continuous Dynamical Systems,
34:1879–1920, 2012.

Wuchen Li, Stanley Osher, and Wilfrid Gangbo. A
fast algorithm for earth mover’s distance based on
optimal transport and l1 type regularization. arXiv
preprint arXiv:1609.07092, 2016.

Dong C Liu and Jorge Nocedal. On the limited mem-
ory bfgs method for large scale optimization. Math-
ematical programming, 45(1):503–528, 1989.

Ofer Meshi, Amir Globerson, and Tommi S. Jaakkola.
Convergence rate analysis of map coordinate mini-
mization algorithms. In Proc. of NIPS, pages 3014–
3022. 2012.

Ofer Meshi, Mehrdad Mahdavi, and Alexander G.
Schwing. Smooth and strong: MAP inference with
linear convergence. In Proc. of NIPS, 2015a.

Ofer Meshi, Nathan Srebro, and Tamir Hazan. Ef-
ﬁcient training of structured svms via soft con-
straints.
In Proc. of AISTATS, pages 699–707,
2015b.

Christian Michelot. A ﬁnite algorithm for ﬁnding the
projection of a point onto the canonical simplex of
Rn. Journal of Optimization Theory and Applica-
tions, 50(1):195–200, 1986.

Boris Muzellec, Richard Nock, Giorgio Patrini, and
Frank Nielsen. Tsallis regularized optimal transport
and ecological inference. In AAAI, pages 2387–2393,
2017.

Yu Nesterov. Smooth minimization of non-smooth
functions. Mathematical programming, 103(1):127–
152, 2005.

Gabriel Peyr´e and Marco Cuturi. Computational Op-

timal Transport. 2017.

Mathieu Blondel, Vivien Seguy, Antoine Rolet

Appendix

A Proofs

Recall that

A.1 Derivation of the smooth relaxed dual

OTΩ(a, b) = min

T ∈U (a,b)

t(cid:62)
j cj + Ω(tj).

n
(cid:88)

j=1

(16)

We now add Lagrange multipliers for the two equality constraints but keep the constraint T ≥ 0 explicitly:

OTΩ(a, b) = min
T ≥0

max
α∈Rm,β∈Rn

j cj + Ω(tj) + α(cid:62)(T 1n − a) + β(cid:62)(T (cid:62)1m − b).
t(cid:62)

Since (16) is a convex optimization problem with only linear equality and inequality constraints, Slater’s condi-
tions reduce to feasibility (Boyd and Vandenberghe, 2004, §5.2.3) and hence strong duality holds:

OTΩ(a, b) = max

α∈Rm,β∈Rn

j cj + Ω(tj) + α(cid:62)(T 1n − a) + β(cid:62)(T (cid:62)1m − b)
t(cid:62)

n
(cid:88)

j=1

n
(cid:88)

j=1

min
tj ≥0

min
T ≥0

n
(cid:88)

j=1

= max

α∈Rm,β∈Rn

j (cj + α + βj1m) + Ω(tj) − α(cid:62)a − β(cid:62)b
t(cid:62)

= max

α∈Rm,β∈Rn

−

n
(cid:88)

j=1

max
tj ≥0

j (−cj − α − βj1m) − Ω(tj) − α(cid:62)a − β(cid:62)b
t(cid:62)

= max

α∈Rm,β∈Rn

α(cid:62)a + β(cid:62)b −

t(cid:62)
j (α + βj1m − cj) − Ω(tj).

n
(cid:88)

j=1

max
tj ≥0

Finally, plugging the expression of (6) gives the claimed result.

A.2 Derivation of the convex conjugate

The convex conjugate of OTΩ(a, b) w.r.t. the ﬁrst argument is

Following a similar argument as (Cuturi and Peyr´e, 2016, Theorem 2.4), we have

OT∗

Ω(g, b) = sup
a∈(cid:52)m

g(cid:62)a − OTΩ(a, b).

OT∗

Ω(g, b) = max
T ≥0
T (cid:62)1m=b

(cid:104)T, g1(cid:62)

n − C(cid:105) −

Ω(tj).

n
(cid:88)

j=1

Notice that this is an easier optimization problem than (5), since there are equality constraints only in one
direction. Cuturi and Peyr´e (2016) showed that this optimization problem admits a closed form in the case of
entropic regularization. Here, we show how to compute OT∗

Ω for any strongly-convex regularization.

The problem clearly decomposes over columns and we can rewrite it as

OT∗

Ω(g, b) =

t(cid:62)
j (g − cj) − Ω(tj)

max
tj ≥0
t(cid:62)
j 1m=bj

bj max
τ j ∈(cid:52)m

τ (cid:62)

j (g − cj) −

Ω(bjτ j)

1
bj

bjmaxΩj (g − cj),

n
(cid:88)

j=1

n
(cid:88)

j=1
n
(cid:88)

j=1

=

=

Smooth and Sparse Optimal Transport

where we deﬁned Ωj(y) := 1
bj

Ω(bjy) and where maxΩ is deﬁned in (8).

A.3 Expression of the strongly-convex duals

Using a similar derivation as before, we obtain the duals of (13) and (14).

Proposition 3 Duals of (13) and (14)

ROTΦ(a, b) = max

α,β∈P(C)

(cid:93)ROTΦ(a, b) = max

α,β∈P(C)

−

1
2

Φ∗(−2α, a) −

1
2
−Φ∗(−α, a) + β(cid:62)b

Φ∗(−2β, b)

= max
α∈Rm

−Φ∗(−α, a) −

bj max
i∈[m]

(αi − ci,j),

n
(cid:88)

j=1

where Φ∗ is the conjugate of Φ in the ﬁrst argument.

The duals are strongly convex if Φ is smooth.
2γ (cid:107)x − y(cid:107)2, Φ∗(−α, a) = γ
When Φ(x, y) = 1

2 (cid:107)α(cid:107)2 − α(cid:62)a. Plugging that expression in the above, we get

ROTΦ(a, b) = max

α,β∈P(C)

α(cid:62)a + β(cid:62)b − γ (cid:0)(cid:107)α(cid:107)2 + (cid:107)β(cid:107)2(cid:1)

(17)

and

(cid:93)ROTΦ(a, b) = max

α,β∈P(C)

α(cid:62)a + β(cid:62)b −

(cid:107)α(cid:107)2

γ
2

= max
α∈Rm

α(cid:62)a −

bj max
i∈[m]

(αi − ci,j) −

(cid:107)α(cid:107)2.

γ
2

n
(cid:88)

j=1

This corresponds to the original dual and semi-dual with squared 2-norm regularization on the variables.

A.4 Proof of Theorem 1

Before proving the theorem, we introduce the next two lemmas, which bound the regularization value achieved
by any transportation plan.

Lemma 2 Bounding the entropy of a transportation plan
Let H(a) := − (cid:80)
i ai log ai and H(T ) := − (cid:80)
Let a ∈ (cid:52)m, b ∈ (cid:52)n and T ∈ U(a, b). Then,

i,j ti,j log ti,j be the joint entropy.

max{H(a), H(b)} ≤ H(T ) ≤ H(a) + H(b).

Proof. See, for instance, (Cover and Thomas, 2006).

Together with 0 ≤ H(a) ≤ log m and 0 ≤ H(b) ≤ log n, this provides lower and upper bounds for the entropy of
a transportation plan. As noted in (Cuturi, 2013), the upper bound is tight since

max
T ∈U (a,b)

H(T ) = H(ab(cid:62)) = H(a) + H(b).

Lemma 3 Bounding the squared 2-norm of a transportation plan

Let a ∈ (cid:52)m, b ∈ (cid:52)n and T ∈ U(a, b). Then,

m
(cid:88)

n
(cid:88)

i=1

j=1

(cid:18) ai
n

+

−

bj
m

1
mn

(cid:19)2

≤ (cid:107)T (cid:107)2 ≤ min (cid:8)(cid:107)a(cid:107)2, (cid:107)b(cid:107)2(cid:9) .

Mathieu Blondel, Vivien Seguy, Antoine Rolet

Proof. The tightest lower bound is given by min

(cid:107)T (cid:107)2. An exact iterative algorithm was proposed in (Calvillo

and Romero, 2016) to solve this problem. However, since we are interested in an explicit formula, we consider
(cid:107)T (cid:107)2 (i.e., we ignore the non-negativity constraint). It is known (Romero, 1990)
instead the lower bound min
T 1n=a
T (cid:62)1m=b

that the minimum is achieved at ti,j = ai

n + bj

m − 1

mn , hence our lower bound. For the upper bound, we have

T ∈U (a,b)

(cid:107)T (cid:107)2 =

m
(cid:88)

n
(cid:88)

t2
i,j

i=1
m
(cid:88)

j=1
n
(cid:88)

(cid:18)

(cid:19)2

(cid:19)2

(cid:19)

ai

ti,j
ai
(cid:18) ti,j
ai
(cid:18) ti,j
ai

n
(cid:88)

j=1
n
(cid:88)

j=1

=

=

≤

j=1

i=1
m
(cid:88)

i=1
m
(cid:88)

a2
i

a2
i

i=1
= (cid:107)a(cid:107)2.

We can do the same with b ∈ (cid:52)n to obtain (cid:107)T (cid:107)2 ≤ (cid:107)b(cid:107)2, yielding the claimed result. (cid:3)

Together with 0 ≤ (cid:107)a(cid:107)2 ≤ 1 and 0 ≤ (cid:107)b(cid:107)2 ≤ 1, this provides lower and upper bounds for the squared 2-norm of
a transportation plan.

Proof of the theorem. Let T (cid:63) and T (cid:63)

Ω be optimal solutions of (2) and (5), respectively. Then,

OT(a, b) + Ω(T (cid:63)

Ω) = (cid:104)T (cid:63), C(cid:105) + Ω(T (cid:63)

Ω) ≤ (cid:104)T (cid:63)

Ω, C(cid:105) + Ω(T (cid:63)

Ω) = OTΩ(a, b).

Likewise,

Combining the two, we obtain

OTΩ(a, b) = (cid:104)T (cid:63)

Ω, C(cid:105) + Ω(T (cid:63)

Ω) ≤ (cid:104)T (cid:63), C(cid:105) + Ω(T (cid:63)) = OT(a, b) + Ω(T (cid:63)).

OT(a, b) + Ω(T (cid:63)

Ω) ≤ OTΩ(a, b) ≤ OT(a, b) + Ω(T (cid:63)).

Using T (cid:63), T (cid:63)

Ω ∈ U(a, b) together with Lemma 2 and Lemma 3 gives the claimed results.

A.5 Proof of Theorem 2

To prove the theorem, we ﬁrst need the following two lemmas.

Lemma 4 Bounding the 1-norm of α and β for (α, β) ∈ P(C)

Let α, β ∈ P(C) with extra constraints α(cid:62)1m = 0 and α(cid:62)a + β(cid:62)b ≥ 0, where a ∈ (cid:52)m and b ∈ (cid:52)n. Then,

where

0 ≤ (cid:107)α(cid:107)1 + (cid:107)β(cid:107)1 ≤ (cid:107)C(cid:107)∞(ν + n)

(cid:12)
ν = max (cid:8)(2 + n/m) (cid:12)
(cid:12)
(cid:12)a−1(cid:12)
(cid:12)∞ ,
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)b−1(cid:12)
(cid:12)
(cid:12)∞
(cid:12)

(cid:9) .

Proof. The proof technique is inspired by (Meshi et al., 2012, Supplementary material Lemma 1.2).

The 1-norm can be rewritten as

||α||1 + ||β||1 = max

r(cid:62)α + s(cid:62)β.

r∈{−1,1}m
s∈{−1,1}n

Smooth and Sparse Optimal Transport

Our goal is to upper bound the following objective

max
α∈Rm,β∈Rn

r(cid:62)α + s(cid:62)β s.t.

0 ≤ α(cid:62)a + β(cid:62)b,

with a constant that does not depend on r and s. We call the above the dual problem. Its Lagrangian is

L(α, β, µ, ν, T ) = r(cid:62)α + s(cid:62)β + µα(cid:62)1m + ν(α(cid:62)a + β(cid:62)b) +

ti,j (ci,j − αi − βj)

= (r + µ1m + νa − T 1n)(cid:62)α + (s + νb − T (cid:62)1m)(cid:62)β + (cid:104)T, C(cid:105)

with µ ∈ R, ν ≥ 0, T ≥ 0. Maximizing the Lagrangian w.r.t. α and β gives the corresponding primal problem

αi + βj ≤ ci,j,
α(cid:62)1m = 0,

m,n
(cid:88)

i,j=1

min
T ≥0, µ∈R, ν≥0

(cid:104)T, C(cid:105)

s.t. T 1n = νa + r + µ1m,

T (cid:62)1m = νb + s.

m ((cid:80)

By weak duality, any feasible primal point provides an upper bound of the dual problem. We start by choosing
µ = 1
i,j ti,j provides the same values w.r.t. the last two constraints. Next, we choose
(cid:27)

i ri) so that (cid:80)

j sj − (cid:80)

(cid:26)

ν = max

max
i

2 + n/m
ai

, max
j

1
bj

which ensures the non-negativity of νa + r + µ1m and νb + s regardless of r and s.
transportation plan T deﬁned by

It follows that the

T =

1
(νb + s)T 1n

(νa + r + µ1m)(νb + s)(cid:62)

is feasible. We ﬁnally bound the objective, (cid:104)T, C(cid:105) ≤ ||C||∞

(cid:80)

i,j ti,j ≤ ||C||∞ (ν + n). (cid:3)

Lemma 5 Bounding the 1-norm of α for (α, ·) ∈ P(C)
Let α, β ∈ P(C) with extra constraints (cid:80)m

i=1 αi = 0 and α(cid:62)a + β(cid:62)b ≥ 0, where a ∈ (cid:52)m and b ∈ (cid:52)n. Then,

0 ≤ (cid:107)α(cid:107)1 ≤ 2(cid:107)C(cid:107)∞

(cid:12)
(cid:12)a−1(cid:12)
(cid:12)
(cid:12)
(cid:12)∞ .
(cid:12)
(cid:12)

Proof. Similarly as before, our goal is to upper bound

max
α∈Rm,β∈Rn

r(cid:62)α s.t.

0 ≤ α(cid:62)a + β(cid:62)b,

αi + βj ≤ ci,j,
α(cid:62)1m = 0,

with a constant which does not depend on r. The corresponding primal is

min
T ≥0, µ∈R, ν≥0

(cid:104)T, C(cid:105)

s.t. T 1n = νa + r + µ1m,

T (cid:62)1m = νb.

By weak duality, any feasible primal point gives us an upper bound. We start by choosing µ = 1
m
(cid:80)

ij ti,j provides the same values w.r.t. the last two constraints. Next, we choose, ν = max

i ri so that
, which ensures the

(cid:80)

2
ai

i

non-negativity of νa + r + µ1m (νb ≥ 0 is also satisﬁed since ν ≥ 0) which appears in the r.h.s. of the second
constraint, independently of r. It follows that the transportation plan T deﬁned by

T =

1
νb(cid:62)1n

(νa + r + µ1m)(νb)(cid:62) = (νa + r + µ1m)b(cid:62)

Mathieu Blondel, Vivien Seguy, Antoine Rolet

is feasible. We ﬁnally bound the objective

(cid:104)T, C(cid:105) ≤ ||C||∞

ti,j ≤ ν ||C||∞ = 2 ||C||∞

(cid:12)
(cid:12)a−1(cid:12)
(cid:12)
(cid:12)
(cid:12)∞ ,
(cid:12)
(cid:12)

(cid:88)

i,j

which concludes the proof. (cid:3)

Proof of the theorem. We begin by deriving the bound for the relaxed primal. Let (α(cid:63), β(cid:63)) and (α(cid:63)
optimal solutions of (3) and (17), respectively. Since (α(cid:63)

Φ)(cid:62)b ≤ (α(cid:63))(cid:62)a + (β(cid:63))(cid:62)b, we have

Φ)(cid:62)a + (β(cid:63)

Φ, β(cid:63)

Φ) be

ROTΦ(a, b) ≤ OT(a, b) −

((cid:107)αΦ(cid:107)2 + (cid:107)βΦ(cid:107)2).

γ
2

OT(a, b) −

((cid:107)α(cid:63)(cid:107)2 + (cid:107)β(cid:63)(cid:107)2) ≤ ROTΦ(a, b).

γ
2

Likewise,

Combining the two, we get

γ
2

OT(a, b) −

((cid:107)α(cid:63)(cid:107)2 + (cid:107)β(cid:63)(cid:107)2) ≤ ROTΦ(a, b) ≤ OT(a, b) −

((cid:107)αΦ(cid:107)2 + (cid:107)βΦ(cid:107)2).

(18)

γ
2

Hence we need to bound variables α, β ∈ P(C). Since || · ||2 ≤ || · ||1, we can upper bound ||α(cid:63)||1 + ||β(cid:63)||1. In
addition, we can always add the additional constraint that α(cid:62)a + β(cid:62)b ≥ 0(cid:62)a + 0(cid:62)b = 0 since (0, 0) is dual
feasible for (3). Since for any optimal pair α(cid:63), β(cid:63), the pair α(cid:63) − σ1, β(cid:63) + σ1 is also feasible and optimal for
any σ ∈ R, we can also add the constraint α(cid:62)1m = 0. The obtained bound will obviously hold for any optimal
pair α(cid:63), β(cid:63). Hence, we can apply Lemma 4. By the same reasoning but using the constraint β(cid:62)1n = 0 in place
of α(cid:62)1m = 0, we can obtain a similar bound. By combining these two bounds, we obtain our ﬁnal bound:

where

(cid:107)α(cid:107)1 + (cid:107)β(cid:107)1 ≤ (cid:107)C(cid:107)∞ min{ν1 + n, ν2 + m}

(cid:12)
(cid:12)b−1(cid:12)
(cid:12)
(cid:12)∞ , (cid:12)
(cid:12)
(cid:12)a−1(cid:12)
(cid:12)
ν1 = max (cid:8)(2 + n/m) (cid:12)
(cid:12)∞
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)b−1(cid:12)
(cid:12)
(cid:12)∞ , (2 + m/n) (cid:12)
(cid:12)
(cid:12)a−1(cid:12)
(cid:12)
ν2 = max (cid:8)(cid:12)
(cid:12)∞
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:9)
(cid:9) .

Taking the square of this bound and plugging the result in (18) gives the claimed result. Applying the same
reasoning with Lemma 5 gives the claimed result for the semi-relaxed primal.

B Alternating minimization with exact block updates

General case. Let β(α) be an optimal solution of (7) given α ﬁxed, and similarly for α(β). From the ﬁrst-order
optimality conditions,

∇δΩ (α + βj(α)1m − cj)(cid:62) 1m = bj ∀j ∈ [n]

(19)

and similarly for α given β ﬁxed. Solving these equations is non-trivial in general. However, because

∇δΩ (α + βj(α)1m − cj) = bj∇maxΩj (α − cj)

holds ∀α ∈ Rm, j ∈ [n], we can retrieve βj(α) if we know how to compute ∇maxΩ(x) and the inverse map
(∇δΩ)−1(y) exists. That map exists and equals ∇Ω(y) provided that Ω is diﬀerentiable and y > 0.

Entropic regularization. It is easy to verify that (19) is satisﬁed with

β(α) = γ log

(cid:18)

b
α
γ −1m
K (cid:62)e

(cid:19)

where K := e

−C
γ

and similarly for α(β). These updates recover the iterates of the Sinkhorn algorithm (Cuturi, 2013).

Squared 2-norm regularization. Plugging the expression of ∇δΩ in (19), we get that β(α) must satisfy

[α + βj(α)1m − cj](cid:62)

+1m = γbj ∀j ∈ [n].

Smooth and Sparse Optimal Transport

Close inspection shows that it is exactly the same optimality condition as the Euclidean projection onto the
. Let x[1] ≥ · · · ≥ x[m] be the values of x in sorted order.
simplex argmin
y∈(cid:52)m

(cid:107)y − x(cid:107)2 must satisfy, with x = α−cj
γbj

Following (Michelot, 1986; Duchi et al., 2008), if we let

(cid:40)

(cid:33)

(cid:41)

ρ := max

i ∈ [m] : x[i] −

x[r] − 1

> 0

1
i

(cid:32) i

(cid:88)

r=1

then y(cid:63) is exactly achieved at [x + βj (α)
γbj

1m]+, where

βj(α) = −

x[r] − 1

.

γbj
ρ

(cid:32) ρ

(cid:88)

r=1

(cid:33)

The expression for α(β) is completely symmetrical. While a projection onto the simplex is required for each
coordinate, as discussed in §3.3, this can be done in expected linear time. In addition, each coordinate-wise
solution can be computed in parallel.

Alternating minimization. Once we know how to compute β(α) and α(β), there are a number of ways
we can build a proper algorithm to solve the smoothed dual. Perhaps the simplest is to alternate between
β ← β(α) and α ← α(β). For entropic regularization, this two-block coordinate descent (CD) scheme is known
as the Sinkhorn algorithm and was recently popularized in the context of optimal transport by Cuturi (2013).
A disadvantage of this approach, however, is that computational eﬀort is spent updating coordinates that may
already be near-optimal. To address this issue, we can instead adopt a greedy CD scheme as recently proposed
for entropic regularization by Altschuler et al. (2017).

C Additional experiments

We ran the same experiments as Figure 2 and Figure 3 on one more image pair: “Graﬁti” by Jon Ander and
“Rainbow Bridge National Monument Utah”, by Bernard Spragg. Both images are in the public domain. The
results, presented in Figure 5 and Figure 6 below, conﬁrm the empirical ﬁndings described in §6.1 and §6.2. The
images are available at https://github.com/mblondel/smooth-ot/tree/master/data.

Mathieu Blondel, Vivien Seguy, Antoine Rolet

Figure 5: Same experiment as Figure 3 on one more image pair.

Figure 6: Same experiment as Figure 2 on one more image pair.

