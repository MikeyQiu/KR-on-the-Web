On the challenges of learning with inference networks on sparse,
high-dimensional data

7
1
0
2
 
t
c
O
 
7
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
5
8
0
6
0
.
0
1
7
1
:
v
i
X
r
a

Rahul G. Krishnan
MIT

Dawen Liang
Netﬂix

Matthew D. Hoﬀman
Google

Abstract

We study parameter estimation in Nonlin-
ear Factor Analysis (NFA) where the gen-
erative model is parameterized by a deep
neural network. Recent work has focused
on learning such models using inference (or
recognition) networks; we identify a crucial
problem when modeling large, sparse, high-
dimensional datasets – underﬁtting. We study
the extent of underﬁtting, highlighting that
its severity increases with the sparsity of the
data. We propose methods to tackle it via
iterative optimization inspired by stochastic
variational inference (Hoﬀman et al. , 2013)
and improvements in the sparse data represen-
tation used for inference. The proposed tech-
niques drastically improve the ability of these
powerful models to ﬁt sparse data, achieving
state-of-the-art results on a benchmark text-
count dataset and excellent results on the task
of top-N recommendation.

1 Introduction

Factor analysis (FA, Spearman, 1904) is a widely used
latent variable model in the applied sciences. The
generative model assumes data, x, is a linear function
of independent Normally distributed latent variables,
z. The assumption of linearity has been relaxed in
nonlinear factor analysis (NFA) (Gibson, 1960) and
extended across a variety of domains such as economics
(Jones, 2006), signal processing (Jutten & Karhunen,
2003), and machine learning (Valpola & Karhunen,
2002; Lawrence, 2003). NFA assumes the joint distribu-
tion factorizes as p(x, z; θ) = p(z)p(x
z; θ). When the
|
nonlinear conditional distribution is parameterized by
deep neural networks (e.g., Valpola & Karhunen, 2002),
the graphical model is referred to as a deep generative
model. When paired with an inference or recognition
network (Hinton et al. , 1995), a parametric function
x) (local
that approximates posterior distribution p(z

|

variational parameters) from data, such models go by
the name of variational autoencoders (VAEs, Kingma
& Welling, 2014; Rezende et al. , 2014). We study
NFA for the density estimation and analysis of sparse,
high-dimensional categorical data.

Sparse, high-dimensional data is ubiquitous; it arises
naturally in survey and demographic data, bag-of-words
representations of text, mobile app usage logs, rec-
ommender systems, genomics, and electronic health
records. Such data are characterized by few frequently
occurring features and a long-tail of rare features.
When directly learning VAEs on sparse data, a problem
we run into is that the standard learning algorithm
results in underﬁtting and fails to utilize the model’s
full capacity. The inability to fully utilize the capacity
of complex models in the large data regime is cause for
concern since it limits the applicability of this class of
models to problems in the aforementioned domains.

The contributions of this work are as follows. We
identify a problem with standard VAE training when
applied to sparse, high-dimensional data—underﬁtting.
We investigate the underlying causes of this phe-
nomenon, and propose modiﬁcations to the learning
algorithm to address these causes. We combine infer-
ence networks with an iterative optimization scheme
inspired by Stochastic Variational Inference (SVI) (Hoﬀ-
man et al. , 2013). The proposed learning algorithm
dramatically improves the quality of the estimated pa-
rameters. We empirically study various factors that
govern the severity of underﬁtting and how the tech-
niques we propose mitigate it. A practical ramiﬁcation
of our work is that improvements in learning NFA on
recommender system data translate to more accurate
predictions and better recommendations. In contrast,
standard VAE training fails to outperform the simple
shallow linear models that still largely dominate the
collaborative ﬁltering domain (Sedhain et al. , 2016).

2 Background

Generative Model: We consider learning in gener-
ative models of the form shown in Figure 1. We intro-

Figure 1: Nonlinear Factor Analysis:
[Left] The generative model contains a
single latent variable z. The conditional
probability p(x|z; θ) parameterized by
a deep neural network. [Right] The
inference network qφ(z|x) is used for
inference at train and test time.

z

θ

x

z

φ

x

duce the model in the context of performing maximum
likelihood estimation over a corpus of documents. We
observe a set of D word-count1 vectors x1:D, where
xdv denotes the number of times that word index
appears in document d. Given the
v
v xdv, xd
total number of words per document Nd
is generated via the following generative process:

1, . . . , V

∈ {

(cid:80)

≡

}

MLP(zd; θ);

(1)

zd

∼ N

µ(zd)

(0, I); γ(zd)
exp
{
v exp

≡
γ(zd)
}
γ(zd)v
{

(cid:80)

≡

; xd

Mult.(µ(zd), Nd).

∼

}
That is, we draw a Gaussian random vector, pass it
through a multilayer perceptron (MLP) parameterized
by θ, pass the resulting vector through the softmax
(a.k.a. multinomial logistic) function, and sample Nd
times from the resulting distribution over V .2

Variational Learning: For ease of exposition we
drop the subscript on xd when referring to a single data
point. Jensen’s inequality yields the following lower
bound on the log marginal likelihood of the data:

log pθ(x)

Eq(z;ψ)[log pθ(x
(cid:124)

|

≥

z)]

KL( q(z; ψ)

−
(cid:123)(cid:122)
(x;θ,ψ)

L

p(z) ).
(cid:125)

||

(2)

|

q(z; ψ) is a tractable “variational” distribution meant
to approximate the intractable posterior distribution
x); it is controlled by some parameters ψ. For
p(z
example, if q is Gaussian, then we might have ψ =
(z; µ, Σ). We are free to choose ψ
µ, Σ
{
however we want, but ideally we would choose the ψ
that makes the bound in equation 2 as tight as possible,
ψ∗ (cid:44) arg maxψ

, q(z; ψ) =

(x; θ, ψ).

N

}

L

Hoﬀman et al.
(2013) proposed ﬁnding ψ∗ using it-
erative optimization, starting from a random initial-
ization. This is eﬀective, but can be costly. More
recently, Kingma & Welling (2014) and Rezende et al.
(2014) proposed training a feedforward inference net-
work (Hinton et al. , 1995) to ﬁnd good variational

1We use word-count in document for the sake of con-
creteness. Our methodology is generally applicable to other
types of discrete high-dimensional data.

2In keeping with common practice, we neglect the multi-
nomial base measure term N !
x1!···xV ! , which amounts to as-
suming that the words are observed in a particular order.

L

parameters ψ(x) for a given x, where ψ(x) is the out-
put of a neural network with parameters φ that are
trained to maximize
(x; θ, ψ(x)). Often it is much
cheaper to compute ψ(x) than to obtain an optimal ψ∗
using iterative optimization. But there is no guarantee
that ψ(x) produces optimal variational parameters—it
may yield a much looser lower bound than ψ∗ if the
inference network is either not suﬃciently powerful or
its parameters φ are not well tuned.

Throughout the rest of this paper, we will use ψ(x) to
denote an inference network that implicitly depends
on some parameters φ, and ψ∗ to denote a set of vari-
ational parameters obtained by applying an iterative
optimization algorithm to equation 2. Following com-
mon convention, we will sometimes use qφ(z
x) as
shorthand for q(z; ψ(x)).

|

3 Mitigating Underﬁtting

We elucidate our hypothesis on why the learning algo-
rithm for VAEs is susceptible to underﬁtting.

3.1 Tight Lower Bounds on log p(x)

There are two sources of error in variational parameter
estimation with inference networks:

The ﬁrst is the distributional error accrued due to
learning with a tractable-but-approximate family of
x) instead of the true posterior dis-
distributions qφ(z
|
tribution p(z
x). Although diﬃcult to compute in
practice, it is easy to show that this error is exactly
KL(qφ(z
x)). We restrict ourselves to working
|
with normally distributed variational approximations
and do not aim to overcome this source of error.

x)
(cid:107)
|

p(z

|

The second source of error comes from the sub-
optimality of the variational parameters ψ used in
(x; θ, ψ(x)) is a valid
Eq. 2. We are guaranteed that
lower bound on log p(x) for any output of qφ(z
x) but
within the same family of variational distributions,
there exists an optimal choice of variational parameters
realizing the tightest variational bound
µ∗, Σ∗
ψ∗ =
{
for a data point x.

L

}

|

log p(x)
E
(cid:124)

N

≥
(µ∗;Σ∗)[log p(x

(x; θ, ψ(x))

≥ L

(3)

z; θ)]
|

−
(cid:123)(cid:122)
(x;θ,ψ∗)

L

KL(

(µ∗, Σ∗)
(cid:107)

N

p(z))
(cid:125)

Where we deﬁne: ψ∗ :=
E
= arg max
µ,Σ

µ∗, Σ∗
}
{
z; θ)]
(µ,Σ)[log p(x

N

|

KL(

(µ, Σ)
(cid:107)

N

−

p(z)).

The cartoon in Fig. 2 illustrates this double bound.

Standard learning algorithms for VAEs update θ, φ
jointly based on
(x; θ, ψ(x)) (see Alg. 1 for pseu-
docode). That is, they directly use ψ(x) (as output by
qφ(z

x)) to estimate Equation 2.
|

L

L

In contrast, older stochastic variational inference meth-
ods (Hoﬀman et al. , 2013) update θ based on gradients
of
(x; θ, ψ∗) by updating randomly initialized varia-
tional parameters for each example. ψ∗ is obtained by
(x; θ, ψ) with respect to ψ. This max-
maximizing
imization is performed by M gradient ascent steps
yielding ψM

ψ∗. (see Alg. 2).

L

≈

Algorithm 1 Learning with Inference Networks
(Kingma et al. , 2014)

:= [x1, . . . , xD] ,

Inputs:
Model: qφ(z
for k = 1. . . K do

x), pθ(x
|

D

|

z), p(z);

Sample: x
θk+1
φk+1

, ψ(x) = qφ(z
∇θk
L
∇φk
L

θk + ηθ
φk + ηφ

∼ D
←
←

|

(x; θk, ψ(x))
(x; θk, ψ(x))

x), update θ, φ:

end for

Algorithm 2 Learning with Stochastic Variational
Inference: M : number of gradient updates to ψ.

:= [x1, . . . , xD] ,

D

Inputs:
Model: pθ(x
for k = 1. . . K do
1. Sample: x
2. Approx. ψM

z), p(z);
|

∼ D
≈

and initialize: ψ0 = µ0, Σ0
(x; θ; ψ):
ψ∗ = arg maxψ
1:
∂
ψm+1 = ψm + ηψ

−

L

For m = 0, . . . , M

L
θk + ηθ

(x;θk,ψm)
∂ψm
∇θk

L

←

(x; θk, ψM )

3. Update θ: θk+1

end for

3.2 Limitations of Joint Parameter Updates

(1) updates θ, φ jointly. During training, the
Alg.
inference network learns to approximate the posterior,
and the generative model improves itself using local
variational parameters ψ(x) output by qφ(z

x).

|

If the variational parameters ψ(x) output by the in-
ference network are close to the optimal variational
parameters ψ∗ (Eq. 3), then the updates for θ are
based on a relatively tight lower bound on log p(x).
But in practice ψ(x) may not be a good approximation
to ψ∗.

Both the inference network and generative model are
initialized randomly. At the start of learning, ψ(x) is
the output of a randomly initialized neural network,
and will therefore be a poor approximation to the opti-
mal parameters ψ∗. So the gradients used to update θ
will be based on a very loose lower bound on log p(x).
These gradients may push the generative model towards

a poor local minimum— previous work has argued
that deep neural networks (which form the conditional
probability distributions pθ(x
z)) are often sensitive to
|
initialization (Glorot & Bengio, 2010; Larochelle et al.
, 2009). Even later in learning, ψ(x) may yield sub-
optimal gradients for θ if the inference network is not
powerful enough to ﬁnd optimal variational parameters
for all data points.

Learning in the original SVI scheme does not suﬀer
from this problem, since the variational parameters
are optimized within the inner loop of learning before
updating to θ (i.e.
in Alg. (2); ∂θ is eﬀectively de-
(x; θ, ψ∗)). However, this method requires
rived using
potentially an expensive iterative optimization.

L

This motivates blending the two methodologies for
parameter estimation. Rather than rely entirely on
the inference network, we use its output to “warm-
start” an SVI-style optimization that yields higher-
quality estimates of ψ∗, which in turn should yield
more meaningful gradients for θ.

Figure 2: Lower Bounds in Variational Learning:
To estimate θ, we maximize a lower bound on log p(x; θ).
L(x; θ, ψ(x)) denotes the standard training objective used
by VAEs. The tightness of this bound (relative to L(x; θ, ψ∗)
depends on the inference network. The x-axis is θ.

3.3 Optimizing Local Variational Parameters

L

We use the local variational parameters ψ = ψ(x) pre-
dicted by the inference network to initialize an iter-
ative optimizer. As in Alg. 2, we perform gradient
ascent to maximize
(x; θ, ψ) with respect to ψ. The
resulting ψM approximates the optimal variational pa-
rameters: ψM
ψ∗. Since NFA is a continuous latent
variable model, these updates can be achieved via the
re-parameterization gradient (Kingma & Welling, 2014).
(x; θ, ψ∗).
We use ψ∗ to derive gradients for θ under
Finally, the parameters of the inference network (φ) are
updated using stochastic backpropagation and gradient
descent, holding ﬁxed the parameters of the generative
model (θ). Our procedure is detailed in Alg. 3.

≈

L

3.4 Representations for Inference Networks

The inference network must learn to regress to the
optimal variational parameters for any combination of

Algorithm 3 Maximum Likelihood Estimation of θ
with Optimized Local Variational Parameters: Ex-
pectations in L(x, θ, ψ∗) (see Eq. 3) are evaluated with a
single sample from the optimized variational distribution.
M is the number of updates to the variational parame-
ters (M = 0 implies no additional optimization). θ, ψ(x), φ
are updated using stochastic gradient descent with learn-
ing rates ηθ, ηψ, ηφ obtained via ADAM (Kingma & Ba,
2015).
In step 4, we update φ separately from θ. One
could alternatively, update φ using KL(ψ(x)M (cid:107)qφ(z|x)) as
in Salakhutdinov & Larochelle (2010).

:= [x1, . . . , xD] ,

Inputs:
D
Inference Model: qφ(z
Generative Model: pθ(x
for k = 1. . . K do
1. Sample: x
2. Approx. ψM

x),
|

|

z), p(z),

∼ D
≈

and set ψ0 = ψ(x)
ψ∗ = arg maxψ
1:
∂
ψm+1 = ψm + ηψ

(x;θk,ψm)
∂ψm

−

L

L

For m = 0, . . . , M

3. Update θ,
θk+1
←
4. Update φ,
φk+1

←

end for

θk + ηθ

(x; θk, ψM )

∇θk

L

φk + ηφ

(x; θk+1, ψ(x))

∇φk

L

(x; θk; ψ),

features, but in sparse datasets, many words appear
only rarely. To provide more global context about rare
words, we provide to the inference network (but not the
generative network) TF-IDF (Baeza-Yates et al. , 1999)
features instead of raw counts. These give the inference
network a hint that rare words are likely to be highly
informative. TF-IDF is a popular technique in informa-
tion retrieval that re-weights features to increase the
inﬂuence of rarer features while decreasing the inﬂuence
of common features. The transformed feature-count
vector is ˜xdv
. The resulting
vector ˜x is then normalized by its L2 norm.

D
d(cid:48) min

xdv log

xd(cid:48) v,1

≡

(cid:80)

{

}

4 Related Work

Salakhutdinov & Larochelle optimize local mean-ﬁeld
parameters from an inference network in the context
of learning deep Boltzmann machines. Salimans et al.
explore warm starting MCMC with the output of an
inference network. Hjelm et al. explore a similar idea
as ours to derive an importance-sampling-based bound
for learning deep generative models with discrete latent
variables. They ﬁnd that learning with ψ∗ does not
improve results on binarized MNIST. This is consistent
with our experience—we ﬁnd that our secondary opti-
mization procedure helped more when learning models
of sparse data. Miao et al.
learn log-linear models
(multinomial-logistic PCA, Collins et al. , 2001) of
documents using inference networks. We show that
mitigating underﬁtting in deeper models yields better

results on the benchmark RCV1 data. Our use of the
spectra of the Jacobian matrix of log p(x
z) to inspect
|
learned models is inspired by Wang et al. (2016).

Previous work has studied the failure modes of learn-
ing VAEs. They can be broadly categorized into two
classes. The ﬁrst aims to improves the utilization of
latent variables using a richer posterior distribution
(Burda et al. , 2015). However, for sparse data, the
x)
limits of learning with a Normally distributed qφ(z
|
have barely been pushed – our goal is to do so in this
work. Further gains may indeed be obtained with a
richer posterior distribution but the techniques herein
can inform work along this vein. The second class of
methods studies ways to alleviate the underutilization
of latent dimensions due to an overtly expressive choice
of models for p(x
z; θ) such as a Recurrent Neural Net-
|
work (Bowman et al. , 2016; Chen et al. , 2017). This
too, is not the scenario we are in; underﬁtting of VAEs
z; θ) is an MLP.
on sparse data occurs even when p(x
|

Our study here exposes a third failure mode; one in
which learning is challenging not just because of the
objective used in learning but also because of the char-
acteristics of the data.

5 Evaluation

We ﬁrst conﬁrm our hypothesis empirically that un-
derﬁtting is an issue when learning VAEs on high di-
mensional sparse datasets. We quantify the gains (at
training and test time) obtained by the use of TF-IDF
features and the continued optimization of ψ(x) on
two diﬀerent types of high-dimensional sparse data—
text and movie ratings. In Section 5.2, we learn VAEs
on two large scale bag-of-words datasets. We study
(1) where the proposed methods might have the most
impact and (2) present evidence for why the learning
In Section 5.3, we show
algorithm (Alg. 3) works.
that improved inference is crucial to building deep
generative models that can tackle problems in top-N
recommender systems. We conclude with a discussion.

5.1 Setup

Notation:
In all experiments, ψ(x) denotes learning
with Alg. 1 and ψ∗ denotes the results of learning with
Alg. 3. M = 100 (number of updates to the local
variational parameters) on the bag-of-words text data
and M = 50 on the recommender systems task. M
was chosen based on the number of steps it takes for
(x; θ, ψm) (Step 2 in Alg. 3) to converge on training
L
data. 3-ψ∗-norm denotes a model where the MLP
parameterizing γ(z) has three layers: two hidden layers
and one output layer, ψ∗ is used to derive an update of θ
and normalized count features are conditioned on by the

inference network. In all tables, we display evaluation
metrics obtained under both ψ(x) (the output of the
inference network) and ψ∗ (the optimized variational
parameters).
In ﬁgures, we always display metrics
obtained under ψ∗ (even if the model was trained with
ψ(x)) since
(x; θ, ψ∗) always forms a tighter bound to
log p(x). If left unspeciﬁed TF-IDF features are used
as input to the inference network.

L

Training and Evaluation: We update θ using learn-
ing rates given by ADAM (Kingma & Ba, 2015) (using
a batch size of 500), The inference network’s inter-
mediate hidden layer h(x) = MLP(x; φ0) (we use a
two-layer MLP in the inference network for all experi-
ments) are used to parameterize the mean and diagonal
log-variance as: µ(x) = Wµh(x), log Σ(x) = Wlog Σh(x)
where φ =
. Code is available at
github.com/rahulk90/vae_sparse.

Wµ, Wlog Σ, φ0

}

{

5.2 Bag-of-words text data

Datasets and Metrics: We study two large text
(1) RCV1 (Lewis et al. , 2004) dataset
datasets.
(train/valid/test: 789,414/5,000/10,000, V : 10,000).
We follow the preprocessing procedure in Miao et al.
(2016). (2) The Wikipedia corpus used in Huang et al.
(2012) (train/test: 1,104,937/100,000 and V :20,000).
We set all words to lowercase, ignore numbers and re-
strict the dataset to the top 20, 000 frequently occurring
words. We report an upper bound on perplexity (Mnih
log p(xi))
& Gregor, 2014) given by exp(
where log p(xi) is replaced by Eq 2. To study the
utilization of the latent dimension obtained by various
training methods, we compute the Jacobian
(z) ma-
z)). The singular value spectrum
trix (as
|
of the Jacobian directly measures the utilization of
the latent dimensions in the model. We provide more
details on this in the supplementary material.

z log p(x

(cid:80)
i

1
Ni

1
N

∇

−

J

Reducing Underﬁtting:
Is underﬁtting a problem
and does optimizing ψ(x) with the use of TF-IDF
features help? Table 1 conﬁrms both statements.

(1) between
We make the following observations:
“norm” and “tﬁdf” (comparing ﬁrst four rows and sec-
ond four rows), we ﬁnd that the use of TF-IDF features
almost always improves parameter estimation; (2) op-
timizing ψ(x) at test time (comparing column ψ∗ with
ψ(x)) always yields a tighter bound on log p(x), of-
ten by a wide margin. Even after extensive training
the inference network can fail to tightly approximate
(x; θ, ψ∗), suggesting that there may be limitations
L
to the power of generic amortized inference; (3) opti-
mizing ψ(x) during training ameliorates under-ﬁtting
and yields signiﬁcantly better generative models on the
RCV1 dataset. We ﬁnd that the degree of underﬁtting
and subsequently the improvements from training with

ψ∗ are signiﬁcantly more pronounced on the larger and
sparser Wikipedia dataset (Fig. 3a and 3b).

Eﬀect of optimizing ψ(x): How does learning with
ψ∗ aﬀect the rate of convergence the learning algo-
rithm? We plot the upper bound on perplexity versus
epochs on the Wikipedia (Fig. 3a, 3b) datasets. As in
Table 1, the additional optimization does not appear
to help much when the generative model is linear. On
the deeper three-layer model, learning with ψ∗ dramat-
ically improves the model allowing it to fully utilize its
potential for density estimation. Models learned with
ψ∗ quickly converge to a better local minimum early on
(as reﬂected in the perplexity evaluated on the train-
ing data and held-out data). We experimented with
continuing to train 3-ψ(x) beyond 150 epochs, where it
reached a validation perplexity of approximately 1330,
worse than that obtained by 3-ψ∗ at epoch 10 suggest-
ing that longer training is unsuﬃcient to overcome local
minima issues aﬄicting VAEs.

Overpruning of latent dimensions: One cause of
underﬁtting is due to overpruning of the latent dimen-
sions in the model. If the variational distributions for a
subset of the latent dimensions of z are set to the prior,
this eﬀectively reduces the model’s capacity.
If the
KL-divergence in Eq. 2 encourages the approximate
posterior to remain close to the prior early in training,
and if the gradient signals from the likelihood term are
weak or inconsistent, the KL may dominate and prune
out latent dimensions before the model can use them.

In Fig. 3c, we plot the log-spectrum of the Jacobian
matrices for diﬀerent training methods and models. For
the deeper models, optimizing ψ(x) is crucial to utiliz-
ing its capacity, particularly on the sparser Wikipedia
data. Without it, only about ten latent dimensions are

Table 1: Test Perplexity on RCV1:
Left: Base-
lines Legend: LDA (Blei et al. , 2003), Replicated Softmax
(RSM) (Hinton & Salakhutdinov, 2009), Sigmoid Belief Net-
works (SBN) and Deep Autoregressive Networks (DARN)
(Mnih & Gregor, 2014), Neural Variational Document Model
(NVDM) (Miao et al. , 2016). K denotes the latent dimen-
sion in our notation. Right: NFA on text data with
K = 100. We vary the features presented to the inference
network qφ(z|x) during learning between: normalized count
, denoted “norm”) and normalized TF-IDF
vectors (

(cid:80)V

x
i=1 xi

LDA
LDA
RSM
SBN

Model K RCV1
50
200
50
50
fDARN 50
fDARN 200
NVDM 50
NVDM 200

1437
1142
988
784
724
598
563
550

NFA

ψ(x)
1-ψ(x)-norm 501
1-ψ∗-norm
488
3-ψ(x)-norm 396
378
3-ψ∗-norm
480
1-ψ(x)-tﬁdf
482
1-ψ∗-tﬁdf
384
3-ψ(x)-tﬁdf
376
3-ψ∗-tﬁdf

ψ∗
481
454
355
331
456
454
344
331

(a) Wikipedia - Training

(b) Wikipedia - Evaluation

(c) Log-Singular Values

Figure 3: Mechanics of Learning: Best viewed in color. (Left and Middle) For the Wikipedia dataset, we visualize
upper bounds on training and held-out perplexity (evaluated with ψ∗) viewed as a function of epochs. Items in the legend
corresponds to choices of training method. (Right) Sorted log-singular values of ∇z log p(x|z) on Wikipedia (left) on
RCV1 (right) for diﬀerent training methods. The x-axis is latent dimension. The legend is identical to that in Fig. 3a.

used, and the model severely underﬁts the data. Opti-
mizing ψ(x) iteratively likely limits overpruning since
the variational parameters (ψ∗) don’t solely focus on
minimizing the KL-divergence but also on maximizing
the likelihood of the data (the ﬁrst term in Eq. 2).

obtained by training with ψ∗ in Fig. 4.

Learning with ψ∗ helps more as the data dimensionality
increases. Data sparsity, therefore, poses a signiﬁcant
challenge to inference networks. One possible explana-
tion is that many of the tokens in the dataset are rare,
and the inference network therefore needs many sweeps
over the dataset to learn to properly interpret these
rare words; while the inference network is learning to
interpret these rare words the generative model is re-
ceiving essentially random learning signals that drive
it to a poor local optimum.

Designing new strategies that can deal with such data
may be a fruitful direction for future work. This may
require new architectures or algorithms—we found that
simply making the inference network deeper does not
solve the problem.

When should ψ(x) be optimized: When are the
gains obtained from learning with ψ∗ accrued? We
learn three-layer models on Wikipedia under two set-
tings: (a) we train for 10 epochs using ψ∗ and then 10
epochs using ψ(x). and (b) we do the opposite.

Fig. 5 depicts the results of this experiment. We ﬁnd
that: (1) much of the gain from optimizing ψ(x) comes
from the early epochs, (2) somewhat surprisingly using
ψ∗ instead of ψ(x) later on in learning also helps (as
witnessed by the sharp drop in perplexity after epoch
10 and the number of large singular values in Fig. 5c
[Left]). This suggests that even after seeing the data for
several passes, the inference network is unable to ﬁnd
ψ(x) that explain the data well. Finally, (3) for a ﬁxed
computational budget, one is better oﬀ optimizing ψ(x)
sooner than later – the curve that optimizes ψ(x) later
on does not catch up to the one that optimizes ψ(x)
early in learning. This suggests that learning early with
ψ∗, even for a few epochs, may alleviate underﬁtting.

Rare words and loose lower bounds:

Fig. 4

Figure 4: Decrease in Perplexity versus Sparsity:
We plot the relative drop in perplexity obtained by train-
ing with ψ∗ instead of ψ(x) against varying levels of
sparsity in the Wikipedia data. On the y-axis, we plot
P[3−ψ(x)]−P[3−ψ∗ ]
; P denotes the bound on perplexity (eval-
P[3−ψ(x)]
uated with ψ∗) and the subscript denotes the model and
method used during training. Each point on the x-axis is
a restriction of the dataset to the top L most frequently
occurring words (number of features).

Sparse data is challenging: What is the rela-
tionship between data sparsity and how well inference
networks work? We hold ﬁxed the number of training
samples and vary the sparsity of the data. We do so
by restricting the Wikipedia dataset to the top L most
frequently occurring words. We train three layer gener-
ative models on the diﬀerent subsets. On training and
held-out data, we computed the diﬀerence between the
perplexity when the model is trained with (denoted
ψ∗]) and without optimization of ψ(x) (denoted
P[3
ψ(x)]). We plot the relative decrease in perplexity
P[3

−

−

(a) Training Data

(b) Held-out Data

(c) Log-singular Values

Figure 5: Late versus Early optimization of ψ(x): Fig. 5a (5b) denote the train (held-out) perplexity for three-
layered models trained on the Wikipedia data in the following scenarios: ψ∗ is used for training for the ﬁrst ten epochs
following which ψ(x) is used (denoted “ψ∗ then ψ(x)”) and vice versa (denoted “ψ(x) then ψ∗”). Fig. 5c (Left) depicts the
number of singular values of the Jacobian matrix ∇z log p(x|z) with value greater than 1 as a function of training epochs
for each of the two aforementioned methodologies. Fig. 5c (Right) plots the sorted log-singular values of the Jacobian
matrix corresponding to the ﬁnal model under each training strategy.

|

||

x)
|

dimensional data are diﬃcult to map into local vari-
ational parameters that model the term E[log p(x
z)]
|
well (Fig. 4,6); qφ(z
x) therefore focuses on the less
noisy (the KL is evaluated analytically) signal of mini-
p(z)). Doing so prunes out many
mizing KL(qφ(z
latent dimensions early on resulting in underﬁtting
(Fig. 5c [Left]). By using ψ∗, the inadequacies of the
inference network are decoupled from the variational
parameters used to derive gradients to θ. The tighter
(x; θ, ψ∗) achieves a better tradeoﬀ
variational bound
between E[log p(x
p(z)) (evidenced
x)
by the number of large singular values of
z)
when optimizing ψ∗ in Fig. 5c). The gradient updates
with respect to this tighter bound better utilize θ.

L
z)] and KL(qφ(z
|

z log p(x

∇

||

|

|

5.3 Collaborative ﬁltering

Modeling rare features in sparse, high-dimensional data
is necessary to achieve strong results on this task. We
study the top-N recommendation performance of NFA
under strong generalization (Marlin & Zemel, 2009).

Datasets: We study two large user-item rating
datasets: MovieLens-20M (ML-20M) (Harper & Kon-
stan, 2015) and Netﬂix3. Following standard procedure:
we binarize the explicit rating data, keeping ratings of
four or higher and interpreting them as implicit feed-
back (Hu et al. , 2008) and keep users who have posi-
tively rated at least ﬁve movies. We train with users’
binary implicit feedback as xd; the vocabulary is the set
of all movies. The number of training/validation/test
users is 116,677/10,000/10,000 for ML-20M (V : 20,108)
and 383,435/40,000/40,000 for Netﬂix (V : 17,769).

Evaluation and Metrics: We train with the com-
plete feedback history from training users, and evaluate
on held-out validation/test users. We select model ar-
chitecture (MLP with 0, 1, 2 hidden layers) from the

3http://www.netflixprize.com/

Figure 6: Raw KL and Rare Word Counts: We plot
the raw values of KL(ψ(x)(cid:107)ψ∗) versus the number of rare
words. We zoom into the plot and reduce the opacity of
the train points to better see the held-out points. The
Spearman ρ correlation coeﬃcient is computed between the
two across 20, 000 points. We ﬁnd a positive correlation.

suggests that data sparsity presents a problem for infer-
ence networks at an aggregate level. We now ask which
data points beneﬁt from the optimization of ψ(x)? We
sample 20000 training and held-out data points; we
compute KL(ψ(x)
ψ∗) (both are Normal distributions
(cid:107)
and the KL is analytic) and the number of rare words in
each document (where a word is classiﬁed as being rare
if it occurs in less than 5% of training documents). We
visualize them in Fig. 6. We also display the Spearman
ρ correlation between the two values in Fig. 6. There
exists a positive correlation (about 0.88 on the training
data) between the two values suggesting that the gains
in perplexity that we observe empirically in Table 1
and Fig. 3 are due to being able to better model the
likelihood of documents with rare words in them.

Putting it all together: Our analysis describes
a narrative of how underﬁtting occurs in learning
VAEs on sparse data. The rare words in sparse, high-

held-out validation users based on NDCG@100 and
report metrics on the held-out test users. For held-out
users, we randomly select 80% of the feedback as the
input to the inference network and see how the other
20% of the positively rated items are ranked based µ(z).
We report two ranking-based metrics averaged over all
held-out users: Recall@N and truncated normalized
discounted cumulative gain (NDCG@N ) (J¨arvelin &
Kek¨al¨ainen, 2002). For each user, both metrics com-
pare the predicted rank of unobserved items with their
true rank. While Recall@N considers all items ranked
within the ﬁrst N to be equivalent, NDCG@N uses
a monotonically increasing discount to emphasize the
importance of higher ranks versus lower ones.

Deﬁne π as a ranking over all the items where π(v)
indicates the v-th ranked item, I
is the indicator
function, and d(π(v)) returns 1 if user d has positively
rated item π(v). Recall@N for user d is

{·}

Recall@N (d, π) :=

N
(cid:88)

v=1

min(N, (cid:80)V

I

d(π(v)) = 1
}
{
v(cid:48) I
d(π(v(cid:48))) = 1
{

)

}

.

The expression in the denominator evaluates to the min-
imum between N and the number of items consumed
by user d. This normalizes Recall@N to have a max-
imum of 1, which corresponds to ranking all relevant
items in the top N positions. Discounted cumulative

Table 2: Recall and NDCG on Recommender Sys-
tems: “2-ψ∗-tﬁdf” denotes a two-layer (one hidden layer
and one output layer) generative model. Standard errors
are around 0.002 for ML-20M and 0.001 for Netﬂix. Run-
time: WMF takes on the order of minutes [ML-20M &
Netﬂix]; CDAE and NFA (ψ(x)) take 8 hours [ML-20M]
and 32.5 hours [Netﬂix] for 150 epochs; NFA (ψ∗) takes
takes 1.5 days [ML-20M] and 3 days [Netﬂix]; SLIM takes
3-4 days [ML-20M] and 2 weeks [Netﬂix].

Recall@50
ψ∗
0.484
0.508
0.505
0.515

NDCG@100
ψ∗
0.377
0.396
0.396
0.404

ψ(x)
0.371
0.376
0.389
0.395

ML-20M

NFA

ψ(x)
2-ψ(x)-norm 0.475
2-ψ∗-norm
0.483
0.499
2-ψ(x)-tﬁdf
2-ψ∗-tﬁdf
0.509
wmf
slim
cdae

0.498
0.495
0.512

0.386
0.401
0.402

Recall@50
ψ∗
0.393
0.415
0.409
0.424

NDCG@100
ψ∗
0.337
0.358
0.353
0.367

ψ(x)
0.333
0.347
0.348
0.359

Netﬂix

NFA

ψ(x)
2-ψ(x)-norm 0.388
2-ψ∗-norm
0.404
0.404
2-ψ(x)-tﬁdf
2-ψ∗-tﬁdf
0.417
wmf
slim
cdae

0.404
0.427
0.417

0.351
0.378
0.360

gain (DCG@N ) for user d is

DCG@N (d, π) :=

N
(cid:88)

2I
{

v=1

d(π(v))=1

}
log(v + 1)

−

1

.

NDCG@N is the DCG@N normalized by ideal DCGN ,
where all the relevant items are ranked at the top. We
have, NDCG@N

[0, 1]. As baselines, we consider:

∈

Weighted matrix factorization (WMF) (Hu et al.
, 2008): a linear low-rank factor model. We train WMF
with alternating least squares; this generally leads to
better performance than with SGD.
SLIM (Ning & Karypis, 2011): a linear model which
learns a sparse item-to-item similarity matrix by solv-
ing a constrained (cid:96)1-regularized optimization problem.
Collaborative denoising autoencoder (CDAE)
(Wu et al. , 2016): An autoencoder achitecture speciﬁ-
cally designed for top-N recommendation. It augments
a denoising autoencoder (Vincent et al. , 2008) by
adding a per-user latent vector to the input, inspired
by standard linear matrix-factorization approaches.
Among the baselines, CDAE is most akin to NFA.

Table 2 summarizes the results of NFA under diﬀerent
settings. We found that optimizing ψ(x) helps both at
train and test time and that TF-IDF features consis-
tently improve performance. Crucially, the standard
training procedure for VAEs realizes a poorly trained
model that underperforms every baseline. The im-
proved training techniques we recommend generalize
across diﬀerent kinds of sparse data. With them, the
same generative model, outperforms CDAE and WMF
on both datasets, and marginally outperforms SLIM
on ML-20M while achieving nearly state-of-the-art re-
sults on Netﬂix. In terms of runtimes, we found that
learning NFA (with ψ∗) to be approximately two-three
times faster than SLIM. Our results highlight the im-
portance of inference at training time showing NFA,
when properly ﬁt, can outperform the popular linear
factorization approaches.

6 Discussion

Studying the failures of learning with inference net-
works is an important step to designing more robust
neural architectures for inference networks. We show
that avoiding gradients obtained using poor variational
parameters is vital to successfully learning VAEs on
sparse data. An interesting question is why inference
networks have a harder time turning sparse data into
variational parameters compared to images? One hy-
pothesis is that the redundant correlations that exist
among pixels (but occur less frequently in features
found in sparse data) are more easily transformed into
local variational parameters ψ(x) that are, in practice,
often reasonably close to ψ∗ during learning.

Acknowledgements

The authors are grateful to David Sontag, Fredrik
Johansson, Matthew Johnson, and Ardavan Saeedi
for helpful comments and suggestions regarding this
work.

References

Huang, Eric H., Socher, Richard, Manning, Christo-
pher D., & Ng, Andrew Y. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. In: ACL.

J¨arvelin, K., & Kek¨al¨ainen, J. 2002. Cumulated gain-
based evaluation of IR techniques. ACM Transac-
tions on Information Systems (TOIS), 20(4), 422–
446.

Baeza-Yates, Ricardo, Ribeiro-Neto, Berthier, et al. .
1999. Modern information retrieval. Vol. 463. ACM
press New York.

Jones, Christopher S. 2006. A nonlinear factor analysis
of S&P 500 index option returns. The Journal of
Finance.

Blei, David M, Ng, Andrew Y, & Jordan, Michael I.

2003. Latent dirichlet allocation. JMLR.

Bowman, Samuel R, Vilnis, Luke, Vinyals, Oriol, Dai,
Andrew M, Jozefowicz, Rafal, & Bengio, Samy. 2016.
Generating sentences from a continuous space. In:
CoNLL.

Burda, Yuri, Grosse, Roger, & Salakhutdinov, Ruslan.
2015. Importance weighted autoencoders. In: ICLR.

Chen, Xi, Kingma, Diederik P, Salimans, Tim, Duan,
Yan, Dhariwal, Prafulla, Schulman, John, Sutskever,
Ilya, & Abbeel, Pieter. 2017. Variational lossy au-
toencoder. ICLR.

Collins, Michael, Dasgupta, Sanjoy, & Schapire,
Robert E. 2001. A generalization of principal com-
ponent analysis to the exponential family. In: NIPS.

Gibson, WA. 1960. Nonlinear factors in two dimensions.

Psychometrika.

Jutten, Christian, & Karhunen, Juha. 2003. Advances

in nonlinear blind source separation. In: ICA.

Kingma, Diederik, & Ba, Jimmy. 2015. Adam: A

method for stochastic optimization. In: ICLR.

Kingma, Diederik P, & Welling, Max. 2014. Auto-

encoding variational bayes. In: ICLR.

Kingma, Diederik P, Mohamed, Shakir, Rezende,
Semi-
Danilo Jimenez, & Welling, Max. 2014.
supervised learning with deep generative models. In:
NIPS.

Larochelle, Hugo, Bengio, Yoshua, Louradour, J´erˆome,
& Lamblin, Pascal. 2009. Exploring strategies for
training deep neural networks. JMLR.

Lawrence, Neil D. 2003. Gaussian Process Latent Vari-
able Models for Visualisation of High Dimensional
Data. In: NIPS.

Glorot, Xavier, & Bengio, Yoshua. 2010. Understanding
the diﬃculty of training deep feedforward neural
networks. In: AISTATS.

Lewis, David D, Yang, Yiming, Rose, Tony G, & Li,
Fan. 2004. RCV1: A new benchmark collection for
text categorization research. JMLR.

Harper, F Maxwell, & Konstan, Joseph A. 2015.
The MovieLens Datasets: History and Context.
ACM Transactions on Interactive Intelligent Systems
(TiiS).

Hinton, Geoﬀrey E, & Salakhutdinov, Ruslan R. 2009.
Replicated softmax: an undirected topic model. In:
NIPS.

Hinton, Geoﬀrey E, Dayan, Peter, Frey, Brendan J, &
Neal, Radford M. 1995. The” wake-sleep” algorithm
for unsupervised neural networks. Science.

Hjelm, R Devon, Cho, Kyunghyun, Chung, Junyoung,
Salakhutdinov, Russ, Calhoun, Vince, & Jojic, Nebo-
jsa. 2016. Iterative Reﬁnement of Approximate Pos-
In:
terior for Training Directed Belief Networks.
NIPS.

Hoﬀman, Matthew D, Blei, David M, Wang, Chong, &
Paisley, John William. 2013. Stochastic variational
inference. JMLR.

Hu, Y., Koren, Y., & Volinsky, C. 2008. Collaborative
ﬁltering for implicit feedback datasets. In: ICDM.

Marlin, Benjamin M, & Zemel, Richard S. 2009. Col-
laborative prediction and ranking with non-random
missing data. In: RecSys.

Miao, Yishu, Yu, Lei, & Blunsom, Phil. 2016. Neural
Variational Inference for Text Processing. In: ICML.

Mnih, Andriy, & Gregor, Karol. 2014. Neural varia-
tional inference and learning in belief networks. In:
ICML.

Ning, Xia, & Karypis, George. 2011. Slim: Sparse linear
methods for top-n recommender systems. Pages
497–506 of: Data Mining (ICDM), 2011 IEEE 11th
International Conference on. IEEE.

Rezende, Danilo Jimenez, Mohamed, Shakir, & Wier-
stra, Daan. 2014. Stochastic backpropagation and
approximate inference in deep generative models. In:
ICML.

Salakhutdinov, Ruslan, & Larochelle, Hugo. 2010. Ef-
ﬁcient Learning of Deep Boltzmann Machines. In:
AISTATS.

Salimans, Tim, Kingma, Diederik, & Welling, Max.
2015. Markov chain monte carlo and variational
inference: Bridging the gap. Pages 1218–1226 of:
Proceedings of the 32nd International Conference on
Machine Learning (ICML-15).

Sedhain, Suvash, Menon, Aditya Krishna, Sanner,
Scott, & Braziunas, Darius. 2016. On the Eﬀective-
ness of Linear Models for One-Class Collaborative
Filtering. In: AAAI.

Spearman, Charles. 1904. ”General Intelligence,” ob-
jectively determined and measured. The American
Journal of Psychology.

Valpola, Harri, & Karhunen, Juha. 2002. An unsu-
pervised ensemble learning method for nonlinear dy-
namic state-space models. Neural computation.

Vincent, Pascal, Larochelle, Hugo, Bengio, Yoshua,
& Manzagol, Pierre-Antoine. 2008. Extracting and
composing robust features with denoising autoen-
coders. Pages 1096–1103 of: Proceedings of the 25th
international conference on Machine learning.

Wang, Shengjie, Plilipose, Matthai, Richardson,
Matthew, Geras, Krzysztof, Urban, Gregor, & Aslan,
Ozlem. 2016. Analysis of Deep Neural Networks with
the Extended Data Jacobian Matrix. In: ICML.

Wu, Yao, DuBois, Christopher, Zheng, Alice X, &
Ester, Martin. 2016. Collaborative denoising auto-
encoders for top-n recommender systems. Pages 153–
162 of: Proceedings of the Ninth ACM International
Conference on Web Search and Data Mining. ACM.

Supplementary Material

Contents

1. Spectral analysis of the Jacobian matrix

2. Relationship with annealing the KL divergence

3. Inference on documents with rare words

4. Depth of qφ(z

x)
|

5. Learning with ψ∗ on small dataset

7 Spectral Analysis of the Jacobian

Matrix

For any vector valued function f (x) : RK

RV ,
xf (x) is the matrix-valued function representing the
∇
sensitivity of the output to the input. When f (x) is a
deep neural network, Wang et al. (2016) use the spec-
tra of the Jacobian matrix under various inputs x to
quantify the complexity of the learned function. They
ﬁnd that the spectra are correlated with the complexity
of the learned function.

→

We adopt their technique for studying the utilization of
the latent space in deep generative models. In the case
of NFA, we seek to quantify the learned complexity
of the generative model. To do so, we compute the
z). This is a
Jacobian matrix as
|
read-out measure of the sensitivity of the likelihood
with respect to the latent dimension.

z log p(x

(z) =

∇

J

(z) is a matrix valued function that can be evaluated
J
at every point in the latent space. We evaluate it it
at the mode of the (unimodal) prior distribution i.e.
at z = (cid:126)0. The singular values of the resulting matrix
denote how much the log-likelihood changes from the
origin along the singular vectors lying in latent space.
The intensity of these singular values (which we plot)
is a read-out measure of how many intrinsic dimensions
are utilized by the model parameters θ at the mode of
(z)
the prior distribution. Our choice of evaluating
at z = (cid:126)0 is motivated by the fact that much of the
probability mass in latent space under the NFA model
will be placed at the origin. We use the utilization
at the mode as an approximation for the utilization
across the entire latent space. We also plotted the
spectral decomposition obtained under a Monte-Carlo
approximation to the matrix E[
(z)] and found it to
be similar to the decomposition obtained by evaluating
the Jacobian at the mode.

J

J

Another possibility to measure utilization would be
from the KL divergence of the prior and the output of
the inference network (as in Burda et al. (2015)).

8 Learning with ψ∗ on a small dataset

Table 3: Test Perplexity on 20newsgroups: Left:
Baselines Legend: LDA (Blei et al. , 2003), Replicated
Softmax (RSM) (Hinton & Salakhutdinov, 2009), Sigmoid
Belief Networks (SBN) and Deep Autoregressive Networks
(DARN) (Mnih & Gregor, 2014), Neural Variational Doc-
ument Model (NVDM) (Miao et al. , 2016). K denotes
the latent dimension in our notation. Right: NFA on
text data with K = 100. We vary the features presented
to the inference network qφ(z|x) during learning between:
, denoted “norm”) and
normalized count vectors (
(cid:80)V

x
i=1 xi

normalized TF-IDF (denoted “tﬁdf”) features.

LDA
LDA
RSM
SBN

Model K Results
50
200
50
50
fDARN 50
fDARN 200
NVDM 50
NVDM 200

1091
1058
953
909
917
—
836
852

NFA

Perplexity
ψ∗
ψ(x)
903
1-ψ(x)-norm 1018
889
1-ψ∗-norm 1279
3-ψ(x)-norm 986
857
879
3-ψ∗-norm 1292
839
932
1-ψ(x)-tﬁdf
828
953
1-ψ∗-tﬁdf
3-ψ(x)-tﬁdf
842
999
839
1063
3-ψ∗-tﬁdf

In the main paper, we studied the optimization of vari-
ational parameters on the larger RCV1 and Wikipedia
datasets. Here, we study the role of learning with ψ∗
in the small-data regime. Table 3 depicts the results
obtained after training models for 200 passes through
the data. We summarize our ﬁndings: (1) across the
board, TF-IDF features improve learning, and (2) in
the small data regime, deeper non-linear models (3-ψ∗-
tﬁdf) overﬁt quickly and better results are obtained
by the simpler multinomial-logistic PCA model (1-
ψ∗-tﬁdf). Overﬁtting is also evident in Fig. 7 from
comparing curves on the validation set to those on the
training set. Interestingly, in the small dataset setting,
we see that learning with ψ(x) has the potential to
have a regularization eﬀect in that the results obtained
are not much worse than those obtained from learning
with ψ∗.

For completeness, in Fig. 8, we also provide the training
behavior for the RCV1 dataset corresponding to the re-
sults of Table 1. The results here, echo the convergence
behavior on the Wikipedia dataset.

(a) Training Data

(b) Held-out Data

(c) Log-singular Values

Figure 7: 20Newsgroups - Training and Held-out Bounds: Fig. 7a, 7b denotes the train (held-out) perplexity for
diﬀerent models. Fig. 7c depicts the log-singular values of the Jacobian matrix for the trained models.

(a) Training Data

(b) Held-out Data

(c) Log-singular Values

Figure 8: RCV1 - Training and Held-out Bounds: Fig. 8a, 8b denotes the train (held-out) perplexity for diﬀerent
models. Fig. 8c depicts the log-singular values of the Jacobian matrix for the trained models.

9 Comparison with KL-annealing

An empirical observation made in previous work is
that when p(x
z; θ) is complex (parameterized by a
|
recurrent neural network or a neural autoregressive
density estimator (NADE)), the generative model also
must contend with overpruning of the latent dimension.
A proposed ﬁx is the annealing of the KL divergence
term in Equation 2 (e.g., Bowman et al. , 2016) as
one way to overcome local minima. As discussed in
the main paper, this is a diﬀerent failure mode to the
one we present in that our decoder is a vanilla MLP –
nonetheless, we apply KL annealing within our setting.
In particular, we optimized Eqφ(z
−
η KL( qφ(z
p(z) ) where η was annealed from 0 to 1
(linearly – though we also tried exponential annealing)
over the course of several parameter updates. Note
that doing so does not give us a lower bound on the
likelihood of the data anymore. There are few estab-
lished guidelines about the rate of annealing the KL
divergence and in general, we found it tricky to get it
to work reliably. We experimented with diﬀerent rates
of annealing for learning a three-layer generative model
on the Wikipedia data.

x) [log pθ(x
|

x)
|

z))]

||

|

Our ﬁndings (visualized in Fig. 10) are as follows: (1)

on sparse data we found annealing the KL divergence
is very sensitive to the annealing rate – too small an
annealing rate and we were still left with underﬁtting
(as in annealing for 10k), too high an annealing rate
(as in 100k) and this resulted in slow convergence; (2)
learning with ψ∗ always outperformed (in both rate of
convergence and quality of ﬁnal result on train and held-
out data) annealing the KL divergence across various
choices of annealing schedules. Said diﬀerently, on
the Wikipedia dataset, we conjecture there exists a
choice of annealing of the KL divergence for which
the perplexity obtained may match those of learning
with ψ∗ but ﬁnding this schedule requires signiﬁcant
trial and error – Fig. 10 suggests that we did not ﬁnd
it. We found that learning with ψ∗ was more robust,
required less tuning (setting values of M to be larger
than 100 never hurt) and always performed at par or
better than annealing the KL divergence. Furthermore,
we did not ﬁnd annealing the KL to work eﬀectively for
the experiments on the recommender systems task. In
particular, we were unable to ﬁnd an annealing schedule
that reliably produced good results.

10 Depth of qφ(z

x)
|

Can the overall eﬀect of the additional optimization be
learned by the inference network at training time? The
experimental evidence we observe in Fig. 9 suggests
this is diﬃcult.

When learning with ψ(x), increasing the number of
layers in the inference network slightly decreases the
quality of the model learned. This is likely because
the already stochastic gradients of the inference net-
work must propagate along a longer path in a deeper
inference network, slowing down learning of the param-
eters φ which in turn aﬀects ψ(x), thereby reducing
the quality of the gradients used to updated θ.

11

Inference on documents with rare
words

(cid:107)

Here, we present another way to visualize the results
of Fig. 6. We sample 20000 training and held-out data
ψ∗) (both are Normal
points; we compute KL(ψ(x)
distributions and the KL is analytic) and the number of
rare words in each document (where a word is classiﬁed
as being rare if it occurs in less than 5% of training
documents). We scale each value to be between 0
and 1 using:
min(c) where c is the vector of
KL divergences or number of rare words. We sort the
scaled values by the KL divergence and plot them in
Fig. 11. As before, we observe that the documents
that we move the farthest in KL divergence are those
which have many rare words.

ci
−
max(c)

min(c)

−

(a) Training Data

(b) Held-out Data

Figure 9: Varying the Depth of qφ(z|x): Fig. 10a (10b) denotes the train (held-out) perplexity for a three-layer
generative model learned with inference networks of varying depth. The notation q3-ψ∗ denotes that the inference network
contained a two-layer intermediate hidden layer h(x) = MLP(x; φ0) followed by µ(x) = Wµh(x), log Σ(x) = Wlog Σh(x).

(a) Training Data

(b) Held-out Data

(c) Log-singular Values

Figure 10: KL annealing vs learning with ψ∗ Fig. 10a, 10b denotes the train (held-out) perplexity for diﬀerent
training methods. The suﬃx at the end of the model conﬁguration denotes the number of parameter updates that it took
for the KL divergence in Equation 2 to be annealed from 0 to 1. 3-ψ∗-50k denotes that it took 50000 parameter updates
before −L(x; θ, ψ(x)) was used as the loss function. Fig. 7c depicts the log-singular values of the Jacobian matrix for the
trained models.

(a) Sparsity of Wikipedia

(b) Training Data

(c) Held-out Data

Figure 11: Normalized KL and Rare Word Counts: Fig. 11a depicts percentage of times words appear in the
Wikipedia dataset (sorted by frequency). The dotted line in blue denotes the marker for a word that has a 5% occurrence
in documents. In Fig. 11b, 11c, we superimpose (1) the normalized (to be between 0 and 1) values of KL(ψ(x)(cid:107)ψ∗) and
(2) the normalized number of rare words (sorted by value of the KL-divergence) for 20, 000 points (on the x-axis) randomly
sampled from the train and held-out data.

On the challenges of learning with inference networks on sparse,
high-dimensional data

7
1
0
2
 
t
c
O
 
7
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
5
8
0
6
0
.
0
1
7
1
:
v
i
X
r
a

Rahul G. Krishnan
MIT

Dawen Liang
Netﬂix

Matthew D. Hoﬀman
Google

Abstract

We study parameter estimation in Nonlin-
ear Factor Analysis (NFA) where the gen-
erative model is parameterized by a deep
neural network. Recent work has focused
on learning such models using inference (or
recognition) networks; we identify a crucial
problem when modeling large, sparse, high-
dimensional datasets – underﬁtting. We study
the extent of underﬁtting, highlighting that
its severity increases with the sparsity of the
data. We propose methods to tackle it via
iterative optimization inspired by stochastic
variational inference (Hoﬀman et al. , 2013)
and improvements in the sparse data represen-
tation used for inference. The proposed tech-
niques drastically improve the ability of these
powerful models to ﬁt sparse data, achieving
state-of-the-art results on a benchmark text-
count dataset and excellent results on the task
of top-N recommendation.

1 Introduction

Factor analysis (FA, Spearman, 1904) is a widely used
latent variable model in the applied sciences. The
generative model assumes data, x, is a linear function
of independent Normally distributed latent variables,
z. The assumption of linearity has been relaxed in
nonlinear factor analysis (NFA) (Gibson, 1960) and
extended across a variety of domains such as economics
(Jones, 2006), signal processing (Jutten & Karhunen,
2003), and machine learning (Valpola & Karhunen,
2002; Lawrence, 2003). NFA assumes the joint distribu-
tion factorizes as p(x, z; θ) = p(z)p(x
z; θ). When the
|
nonlinear conditional distribution is parameterized by
deep neural networks (e.g., Valpola & Karhunen, 2002),
the graphical model is referred to as a deep generative
model. When paired with an inference or recognition
network (Hinton et al. , 1995), a parametric function
x) (local
that approximates posterior distribution p(z

|

variational parameters) from data, such models go by
the name of variational autoencoders (VAEs, Kingma
& Welling, 2014; Rezende et al. , 2014). We study
NFA for the density estimation and analysis of sparse,
high-dimensional categorical data.

Sparse, high-dimensional data is ubiquitous; it arises
naturally in survey and demographic data, bag-of-words
representations of text, mobile app usage logs, rec-
ommender systems, genomics, and electronic health
records. Such data are characterized by few frequently
occurring features and a long-tail of rare features.
When directly learning VAEs on sparse data, a problem
we run into is that the standard learning algorithm
results in underﬁtting and fails to utilize the model’s
full capacity. The inability to fully utilize the capacity
of complex models in the large data regime is cause for
concern since it limits the applicability of this class of
models to problems in the aforementioned domains.

The contributions of this work are as follows. We
identify a problem with standard VAE training when
applied to sparse, high-dimensional data—underﬁtting.
We investigate the underlying causes of this phe-
nomenon, and propose modiﬁcations to the learning
algorithm to address these causes. We combine infer-
ence networks with an iterative optimization scheme
inspired by Stochastic Variational Inference (SVI) (Hoﬀ-
man et al. , 2013). The proposed learning algorithm
dramatically improves the quality of the estimated pa-
rameters. We empirically study various factors that
govern the severity of underﬁtting and how the tech-
niques we propose mitigate it. A practical ramiﬁcation
of our work is that improvements in learning NFA on
recommender system data translate to more accurate
predictions and better recommendations. In contrast,
standard VAE training fails to outperform the simple
shallow linear models that still largely dominate the
collaborative ﬁltering domain (Sedhain et al. , 2016).

2 Background

Generative Model: We consider learning in gener-
ative models of the form shown in Figure 1. We intro-

Figure 1: Nonlinear Factor Analysis:
[Left] The generative model contains a
single latent variable z. The conditional
probability p(x|z; θ) parameterized by
a deep neural network. [Right] The
inference network qφ(z|x) is used for
inference at train and test time.

z

θ

x

z

φ

x

duce the model in the context of performing maximum
likelihood estimation over a corpus of documents. We
observe a set of D word-count1 vectors x1:D, where
xdv denotes the number of times that word index
appears in document d. Given the
v
v xdv, xd
total number of words per document Nd
is generated via the following generative process:

1, . . . , V

∈ {

(cid:80)

≡

}

MLP(zd; θ);

(1)

zd

∼ N

µ(zd)

(0, I); γ(zd)
exp
{
v exp

≡
γ(zd)
}
γ(zd)v
{

(cid:80)

≡

; xd

Mult.(µ(zd), Nd).

∼

}
That is, we draw a Gaussian random vector, pass it
through a multilayer perceptron (MLP) parameterized
by θ, pass the resulting vector through the softmax
(a.k.a. multinomial logistic) function, and sample Nd
times from the resulting distribution over V .2

Variational Learning: For ease of exposition we
drop the subscript on xd when referring to a single data
point. Jensen’s inequality yields the following lower
bound on the log marginal likelihood of the data:

log pθ(x)

Eq(z;ψ)[log pθ(x
(cid:124)

|

≥

z)]

KL( q(z; ψ)

−
(cid:123)(cid:122)
(x;θ,ψ)

L

p(z) ).
(cid:125)

||

(2)

|

q(z; ψ) is a tractable “variational” distribution meant
to approximate the intractable posterior distribution
x); it is controlled by some parameters ψ. For
p(z
example, if q is Gaussian, then we might have ψ =
(z; µ, Σ). We are free to choose ψ
µ, Σ
{
however we want, but ideally we would choose the ψ
that makes the bound in equation 2 as tight as possible,
ψ∗ (cid:44) arg maxψ

, q(z; ψ) =

(x; θ, ψ).

N

}

L

Hoﬀman et al.
(2013) proposed ﬁnding ψ∗ using it-
erative optimization, starting from a random initial-
ization. This is eﬀective, but can be costly. More
recently, Kingma & Welling (2014) and Rezende et al.
(2014) proposed training a feedforward inference net-
work (Hinton et al. , 1995) to ﬁnd good variational

1We use word-count in document for the sake of con-
creteness. Our methodology is generally applicable to other
types of discrete high-dimensional data.

2In keeping with common practice, we neglect the multi-
nomial base measure term N !
x1!···xV ! , which amounts to as-
suming that the words are observed in a particular order.

L

parameters ψ(x) for a given x, where ψ(x) is the out-
put of a neural network with parameters φ that are
trained to maximize
(x; θ, ψ(x)). Often it is much
cheaper to compute ψ(x) than to obtain an optimal ψ∗
using iterative optimization. But there is no guarantee
that ψ(x) produces optimal variational parameters—it
may yield a much looser lower bound than ψ∗ if the
inference network is either not suﬃciently powerful or
its parameters φ are not well tuned.

Throughout the rest of this paper, we will use ψ(x) to
denote an inference network that implicitly depends
on some parameters φ, and ψ∗ to denote a set of vari-
ational parameters obtained by applying an iterative
optimization algorithm to equation 2. Following com-
mon convention, we will sometimes use qφ(z
x) as
shorthand for q(z; ψ(x)).

|

3 Mitigating Underﬁtting

We elucidate our hypothesis on why the learning algo-
rithm for VAEs is susceptible to underﬁtting.

3.1 Tight Lower Bounds on log p(x)

There are two sources of error in variational parameter
estimation with inference networks:

The ﬁrst is the distributional error accrued due to
learning with a tractable-but-approximate family of
x) instead of the true posterior dis-
distributions qφ(z
|
tribution p(z
x). Although diﬃcult to compute in
practice, it is easy to show that this error is exactly
KL(qφ(z
x)). We restrict ourselves to working
|
with normally distributed variational approximations
and do not aim to overcome this source of error.

x)
(cid:107)
|

p(z

|

The second source of error comes from the sub-
optimality of the variational parameters ψ used in
(x; θ, ψ(x)) is a valid
Eq. 2. We are guaranteed that
lower bound on log p(x) for any output of qφ(z
x) but
within the same family of variational distributions,
there exists an optimal choice of variational parameters
realizing the tightest variational bound
µ∗, Σ∗
ψ∗ =
{
for a data point x.

L

}

|

log p(x)
E
(cid:124)

N

≥
(µ∗;Σ∗)[log p(x

(x; θ, ψ(x))

≥ L

(3)

z; θ)]
|

−
(cid:123)(cid:122)
(x;θ,ψ∗)

L

KL(

(µ∗, Σ∗)
(cid:107)

N

p(z))
(cid:125)

Where we deﬁne: ψ∗ :=
E
= arg max
µ,Σ

µ∗, Σ∗
}
{
z; θ)]
(µ,Σ)[log p(x

N

|

KL(

(µ, Σ)
(cid:107)

N

−

p(z)).

The cartoon in Fig. 2 illustrates this double bound.

Standard learning algorithms for VAEs update θ, φ
jointly based on
(x; θ, ψ(x)) (see Alg. 1 for pseu-
docode). That is, they directly use ψ(x) (as output by
qφ(z

x)) to estimate Equation 2.
|

L

L

In contrast, older stochastic variational inference meth-
ods (Hoﬀman et al. , 2013) update θ based on gradients
of
(x; θ, ψ∗) by updating randomly initialized varia-
tional parameters for each example. ψ∗ is obtained by
(x; θ, ψ) with respect to ψ. This max-
maximizing
imization is performed by M gradient ascent steps
yielding ψM

ψ∗. (see Alg. 2).

L

≈

Algorithm 1 Learning with Inference Networks
(Kingma et al. , 2014)

:= [x1, . . . , xD] ,

Inputs:
Model: qφ(z
for k = 1. . . K do

x), pθ(x
|

D

|

z), p(z);

Sample: x
θk+1
φk+1

, ψ(x) = qφ(z
∇θk
L
∇φk
L

θk + ηθ
φk + ηφ

∼ D
←
←

|

(x; θk, ψ(x))
(x; θk, ψ(x))

x), update θ, φ:

end for

Algorithm 2 Learning with Stochastic Variational
Inference: M : number of gradient updates to ψ.

:= [x1, . . . , xD] ,

D

Inputs:
Model: pθ(x
for k = 1. . . K do
1. Sample: x
2. Approx. ψM

z), p(z);
|

∼ D
≈

and initialize: ψ0 = µ0, Σ0
(x; θ; ψ):
ψ∗ = arg maxψ
1:
∂
ψm+1 = ψm + ηψ

−

L

For m = 0, . . . , M

L
θk + ηθ

(x;θk,ψm)
∂ψm
∇θk

L

←

(x; θk, ψM )

3. Update θ: θk+1

end for

3.2 Limitations of Joint Parameter Updates

(1) updates θ, φ jointly. During training, the
Alg.
inference network learns to approximate the posterior,
and the generative model improves itself using local
variational parameters ψ(x) output by qφ(z

x).

|

If the variational parameters ψ(x) output by the in-
ference network are close to the optimal variational
parameters ψ∗ (Eq. 3), then the updates for θ are
based on a relatively tight lower bound on log p(x).
But in practice ψ(x) may not be a good approximation
to ψ∗.

Both the inference network and generative model are
initialized randomly. At the start of learning, ψ(x) is
the output of a randomly initialized neural network,
and will therefore be a poor approximation to the opti-
mal parameters ψ∗. So the gradients used to update θ
will be based on a very loose lower bound on log p(x).
These gradients may push the generative model towards

a poor local minimum— previous work has argued
that deep neural networks (which form the conditional
probability distributions pθ(x
z)) are often sensitive to
|
initialization (Glorot & Bengio, 2010; Larochelle et al.
, 2009). Even later in learning, ψ(x) may yield sub-
optimal gradients for θ if the inference network is not
powerful enough to ﬁnd optimal variational parameters
for all data points.

Learning in the original SVI scheme does not suﬀer
from this problem, since the variational parameters
are optimized within the inner loop of learning before
updating to θ (i.e.
in Alg. (2); ∂θ is eﬀectively de-
(x; θ, ψ∗)). However, this method requires
rived using
potentially an expensive iterative optimization.

L

This motivates blending the two methodologies for
parameter estimation. Rather than rely entirely on
the inference network, we use its output to “warm-
start” an SVI-style optimization that yields higher-
quality estimates of ψ∗, which in turn should yield
more meaningful gradients for θ.

Figure 2: Lower Bounds in Variational Learning:
To estimate θ, we maximize a lower bound on log p(x; θ).
L(x; θ, ψ(x)) denotes the standard training objective used
by VAEs. The tightness of this bound (relative to L(x; θ, ψ∗)
depends on the inference network. The x-axis is θ.

3.3 Optimizing Local Variational Parameters

L

We use the local variational parameters ψ = ψ(x) pre-
dicted by the inference network to initialize an iter-
ative optimizer. As in Alg. 2, we perform gradient
ascent to maximize
(x; θ, ψ) with respect to ψ. The
resulting ψM approximates the optimal variational pa-
rameters: ψM
ψ∗. Since NFA is a continuous latent
variable model, these updates can be achieved via the
re-parameterization gradient (Kingma & Welling, 2014).
(x; θ, ψ∗).
We use ψ∗ to derive gradients for θ under
Finally, the parameters of the inference network (φ) are
updated using stochastic backpropagation and gradient
descent, holding ﬁxed the parameters of the generative
model (θ). Our procedure is detailed in Alg. 3.

≈

L

3.4 Representations for Inference Networks

The inference network must learn to regress to the
optimal variational parameters for any combination of

Algorithm 3 Maximum Likelihood Estimation of θ
with Optimized Local Variational Parameters: Ex-
pectations in L(x, θ, ψ∗) (see Eq. 3) are evaluated with a
single sample from the optimized variational distribution.
M is the number of updates to the variational parame-
ters (M = 0 implies no additional optimization). θ, ψ(x), φ
are updated using stochastic gradient descent with learn-
ing rates ηθ, ηψ, ηφ obtained via ADAM (Kingma & Ba,
2015).
In step 4, we update φ separately from θ. One
could alternatively, update φ using KL(ψ(x)M (cid:107)qφ(z|x)) as
in Salakhutdinov & Larochelle (2010).

:= [x1, . . . , xD] ,

Inputs:
D
Inference Model: qφ(z
Generative Model: pθ(x
for k = 1. . . K do
1. Sample: x
2. Approx. ψM

x),
|

|

z), p(z),

∼ D
≈

and set ψ0 = ψ(x)
ψ∗ = arg maxψ
1:
∂
ψm+1 = ψm + ηψ

(x;θk,ψm)
∂ψm

−

L

L

For m = 0, . . . , M

3. Update θ,
θk+1
←
4. Update φ,
φk+1

←

end for

θk + ηθ

(x; θk, ψM )

∇θk

L

φk + ηφ

(x; θk+1, ψ(x))

∇φk

L

(x; θk; ψ),

features, but in sparse datasets, many words appear
only rarely. To provide more global context about rare
words, we provide to the inference network (but not the
generative network) TF-IDF (Baeza-Yates et al. , 1999)
features instead of raw counts. These give the inference
network a hint that rare words are likely to be highly
informative. TF-IDF is a popular technique in informa-
tion retrieval that re-weights features to increase the
inﬂuence of rarer features while decreasing the inﬂuence
of common features. The transformed feature-count
vector is ˜xdv
. The resulting
vector ˜x is then normalized by its L2 norm.

D
d(cid:48) min

xdv log

xd(cid:48) v,1

≡

(cid:80)

{

}

4 Related Work

Salakhutdinov & Larochelle optimize local mean-ﬁeld
parameters from an inference network in the context
of learning deep Boltzmann machines. Salimans et al.
explore warm starting MCMC with the output of an
inference network. Hjelm et al. explore a similar idea
as ours to derive an importance-sampling-based bound
for learning deep generative models with discrete latent
variables. They ﬁnd that learning with ψ∗ does not
improve results on binarized MNIST. This is consistent
with our experience—we ﬁnd that our secondary opti-
mization procedure helped more when learning models
of sparse data. Miao et al.
learn log-linear models
(multinomial-logistic PCA, Collins et al. , 2001) of
documents using inference networks. We show that
mitigating underﬁtting in deeper models yields better

results on the benchmark RCV1 data. Our use of the
spectra of the Jacobian matrix of log p(x
z) to inspect
|
learned models is inspired by Wang et al. (2016).

Previous work has studied the failure modes of learn-
ing VAEs. They can be broadly categorized into two
classes. The ﬁrst aims to improves the utilization of
latent variables using a richer posterior distribution
(Burda et al. , 2015). However, for sparse data, the
x)
limits of learning with a Normally distributed qφ(z
|
have barely been pushed – our goal is to do so in this
work. Further gains may indeed be obtained with a
richer posterior distribution but the techniques herein
can inform work along this vein. The second class of
methods studies ways to alleviate the underutilization
of latent dimensions due to an overtly expressive choice
of models for p(x
z; θ) such as a Recurrent Neural Net-
|
work (Bowman et al. , 2016; Chen et al. , 2017). This
too, is not the scenario we are in; underﬁtting of VAEs
z; θ) is an MLP.
on sparse data occurs even when p(x
|

Our study here exposes a third failure mode; one in
which learning is challenging not just because of the
objective used in learning but also because of the char-
acteristics of the data.

5 Evaluation

We ﬁrst conﬁrm our hypothesis empirically that un-
derﬁtting is an issue when learning VAEs on high di-
mensional sparse datasets. We quantify the gains (at
training and test time) obtained by the use of TF-IDF
features and the continued optimization of ψ(x) on
two diﬀerent types of high-dimensional sparse data—
text and movie ratings. In Section 5.2, we learn VAEs
on two large scale bag-of-words datasets. We study
(1) where the proposed methods might have the most
impact and (2) present evidence for why the learning
In Section 5.3, we show
algorithm (Alg. 3) works.
that improved inference is crucial to building deep
generative models that can tackle problems in top-N
recommender systems. We conclude with a discussion.

5.1 Setup

Notation:
In all experiments, ψ(x) denotes learning
with Alg. 1 and ψ∗ denotes the results of learning with
Alg. 3. M = 100 (number of updates to the local
variational parameters) on the bag-of-words text data
and M = 50 on the recommender systems task. M
was chosen based on the number of steps it takes for
(x; θ, ψm) (Step 2 in Alg. 3) to converge on training
L
data. 3-ψ∗-norm denotes a model where the MLP
parameterizing γ(z) has three layers: two hidden layers
and one output layer, ψ∗ is used to derive an update of θ
and normalized count features are conditioned on by the

inference network. In all tables, we display evaluation
metrics obtained under both ψ(x) (the output of the
inference network) and ψ∗ (the optimized variational
parameters).
In ﬁgures, we always display metrics
obtained under ψ∗ (even if the model was trained with
ψ(x)) since
(x; θ, ψ∗) always forms a tighter bound to
log p(x). If left unspeciﬁed TF-IDF features are used
as input to the inference network.

L

Training and Evaluation: We update θ using learn-
ing rates given by ADAM (Kingma & Ba, 2015) (using
a batch size of 500), The inference network’s inter-
mediate hidden layer h(x) = MLP(x; φ0) (we use a
two-layer MLP in the inference network for all experi-
ments) are used to parameterize the mean and diagonal
log-variance as: µ(x) = Wµh(x), log Σ(x) = Wlog Σh(x)
where φ =
. Code is available at
github.com/rahulk90/vae_sparse.

Wµ, Wlog Σ, φ0

}

{

5.2 Bag-of-words text data

Datasets and Metrics: We study two large text
(1) RCV1 (Lewis et al. , 2004) dataset
datasets.
(train/valid/test: 789,414/5,000/10,000, V : 10,000).
We follow the preprocessing procedure in Miao et al.
(2016). (2) The Wikipedia corpus used in Huang et al.
(2012) (train/test: 1,104,937/100,000 and V :20,000).
We set all words to lowercase, ignore numbers and re-
strict the dataset to the top 20, 000 frequently occurring
words. We report an upper bound on perplexity (Mnih
log p(xi))
& Gregor, 2014) given by exp(
where log p(xi) is replaced by Eq 2. To study the
utilization of the latent dimension obtained by various
training methods, we compute the Jacobian
(z) ma-
z)). The singular value spectrum
trix (as
|
of the Jacobian directly measures the utilization of
the latent dimensions in the model. We provide more
details on this in the supplementary material.

z log p(x

(cid:80)
i

1
Ni

1
N

∇

−

J

Reducing Underﬁtting:
Is underﬁtting a problem
and does optimizing ψ(x) with the use of TF-IDF
features help? Table 1 conﬁrms both statements.

(1) between
We make the following observations:
“norm” and “tﬁdf” (comparing ﬁrst four rows and sec-
ond four rows), we ﬁnd that the use of TF-IDF features
almost always improves parameter estimation; (2) op-
timizing ψ(x) at test time (comparing column ψ∗ with
ψ(x)) always yields a tighter bound on log p(x), of-
ten by a wide margin. Even after extensive training
the inference network can fail to tightly approximate
(x; θ, ψ∗), suggesting that there may be limitations
L
to the power of generic amortized inference; (3) opti-
mizing ψ(x) during training ameliorates under-ﬁtting
and yields signiﬁcantly better generative models on the
RCV1 dataset. We ﬁnd that the degree of underﬁtting
and subsequently the improvements from training with

ψ∗ are signiﬁcantly more pronounced on the larger and
sparser Wikipedia dataset (Fig. 3a and 3b).

Eﬀect of optimizing ψ(x): How does learning with
ψ∗ aﬀect the rate of convergence the learning algo-
rithm? We plot the upper bound on perplexity versus
epochs on the Wikipedia (Fig. 3a, 3b) datasets. As in
Table 1, the additional optimization does not appear
to help much when the generative model is linear. On
the deeper three-layer model, learning with ψ∗ dramat-
ically improves the model allowing it to fully utilize its
potential for density estimation. Models learned with
ψ∗ quickly converge to a better local minimum early on
(as reﬂected in the perplexity evaluated on the train-
ing data and held-out data). We experimented with
continuing to train 3-ψ(x) beyond 150 epochs, where it
reached a validation perplexity of approximately 1330,
worse than that obtained by 3-ψ∗ at epoch 10 suggest-
ing that longer training is unsuﬃcient to overcome local
minima issues aﬄicting VAEs.

Overpruning of latent dimensions: One cause of
underﬁtting is due to overpruning of the latent dimen-
sions in the model. If the variational distributions for a
subset of the latent dimensions of z are set to the prior,
this eﬀectively reduces the model’s capacity.
If the
KL-divergence in Eq. 2 encourages the approximate
posterior to remain close to the prior early in training,
and if the gradient signals from the likelihood term are
weak or inconsistent, the KL may dominate and prune
out latent dimensions before the model can use them.

In Fig. 3c, we plot the log-spectrum of the Jacobian
matrices for diﬀerent training methods and models. For
the deeper models, optimizing ψ(x) is crucial to utiliz-
ing its capacity, particularly on the sparser Wikipedia
data. Without it, only about ten latent dimensions are

Table 1: Test Perplexity on RCV1:
Left: Base-
lines Legend: LDA (Blei et al. , 2003), Replicated Softmax
(RSM) (Hinton & Salakhutdinov, 2009), Sigmoid Belief Net-
works (SBN) and Deep Autoregressive Networks (DARN)
(Mnih & Gregor, 2014), Neural Variational Document Model
(NVDM) (Miao et al. , 2016). K denotes the latent dimen-
sion in our notation. Right: NFA on text data with
K = 100. We vary the features presented to the inference
network qφ(z|x) during learning between: normalized count
, denoted “norm”) and normalized TF-IDF
vectors (

(cid:80)V

x
i=1 xi

LDA
LDA
RSM
SBN

Model K RCV1
50
200
50
50
fDARN 50
fDARN 200
NVDM 50
NVDM 200

1437
1142
988
784
724
598
563
550

NFA

ψ(x)
1-ψ(x)-norm 501
1-ψ∗-norm
488
3-ψ(x)-norm 396
378
3-ψ∗-norm
480
1-ψ(x)-tﬁdf
482
1-ψ∗-tﬁdf
384
3-ψ(x)-tﬁdf
376
3-ψ∗-tﬁdf

ψ∗
481
454
355
331
456
454
344
331

(a) Wikipedia - Training

(b) Wikipedia - Evaluation

(c) Log-Singular Values

Figure 3: Mechanics of Learning: Best viewed in color. (Left and Middle) For the Wikipedia dataset, we visualize
upper bounds on training and held-out perplexity (evaluated with ψ∗) viewed as a function of epochs. Items in the legend
corresponds to choices of training method. (Right) Sorted log-singular values of ∇z log p(x|z) on Wikipedia (left) on
RCV1 (right) for diﬀerent training methods. The x-axis is latent dimension. The legend is identical to that in Fig. 3a.

used, and the model severely underﬁts the data. Opti-
mizing ψ(x) iteratively likely limits overpruning since
the variational parameters (ψ∗) don’t solely focus on
minimizing the KL-divergence but also on maximizing
the likelihood of the data (the ﬁrst term in Eq. 2).

obtained by training with ψ∗ in Fig. 4.

Learning with ψ∗ helps more as the data dimensionality
increases. Data sparsity, therefore, poses a signiﬁcant
challenge to inference networks. One possible explana-
tion is that many of the tokens in the dataset are rare,
and the inference network therefore needs many sweeps
over the dataset to learn to properly interpret these
rare words; while the inference network is learning to
interpret these rare words the generative model is re-
ceiving essentially random learning signals that drive
it to a poor local optimum.

Designing new strategies that can deal with such data
may be a fruitful direction for future work. This may
require new architectures or algorithms—we found that
simply making the inference network deeper does not
solve the problem.

When should ψ(x) be optimized: When are the
gains obtained from learning with ψ∗ accrued? We
learn three-layer models on Wikipedia under two set-
tings: (a) we train for 10 epochs using ψ∗ and then 10
epochs using ψ(x). and (b) we do the opposite.

Fig. 5 depicts the results of this experiment. We ﬁnd
that: (1) much of the gain from optimizing ψ(x) comes
from the early epochs, (2) somewhat surprisingly using
ψ∗ instead of ψ(x) later on in learning also helps (as
witnessed by the sharp drop in perplexity after epoch
10 and the number of large singular values in Fig. 5c
[Left]). This suggests that even after seeing the data for
several passes, the inference network is unable to ﬁnd
ψ(x) that explain the data well. Finally, (3) for a ﬁxed
computational budget, one is better oﬀ optimizing ψ(x)
sooner than later – the curve that optimizes ψ(x) later
on does not catch up to the one that optimizes ψ(x)
early in learning. This suggests that learning early with
ψ∗, even for a few epochs, may alleviate underﬁtting.

Rare words and loose lower bounds:

Fig. 4

Figure 4: Decrease in Perplexity versus Sparsity:
We plot the relative drop in perplexity obtained by train-
ing with ψ∗ instead of ψ(x) against varying levels of
sparsity in the Wikipedia data. On the y-axis, we plot
P[3−ψ(x)]−P[3−ψ∗ ]
; P denotes the bound on perplexity (eval-
P[3−ψ(x)]
uated with ψ∗) and the subscript denotes the model and
method used during training. Each point on the x-axis is
a restriction of the dataset to the top L most frequently
occurring words (number of features).

Sparse data is challenging: What is the rela-
tionship between data sparsity and how well inference
networks work? We hold ﬁxed the number of training
samples and vary the sparsity of the data. We do so
by restricting the Wikipedia dataset to the top L most
frequently occurring words. We train three layer gener-
ative models on the diﬀerent subsets. On training and
held-out data, we computed the diﬀerence between the
perplexity when the model is trained with (denoted
ψ∗]) and without optimization of ψ(x) (denoted
P[3
ψ(x)]). We plot the relative decrease in perplexity
P[3

−

−

(a) Training Data

(b) Held-out Data

(c) Log-singular Values

Figure 5: Late versus Early optimization of ψ(x): Fig. 5a (5b) denote the train (held-out) perplexity for three-
layered models trained on the Wikipedia data in the following scenarios: ψ∗ is used for training for the ﬁrst ten epochs
following which ψ(x) is used (denoted “ψ∗ then ψ(x)”) and vice versa (denoted “ψ(x) then ψ∗”). Fig. 5c (Left) depicts the
number of singular values of the Jacobian matrix ∇z log p(x|z) with value greater than 1 as a function of training epochs
for each of the two aforementioned methodologies. Fig. 5c (Right) plots the sorted log-singular values of the Jacobian
matrix corresponding to the ﬁnal model under each training strategy.

|

||

x)
|

dimensional data are diﬃcult to map into local vari-
ational parameters that model the term E[log p(x
z)]
|
well (Fig. 4,6); qφ(z
x) therefore focuses on the less
noisy (the KL is evaluated analytically) signal of mini-
p(z)). Doing so prunes out many
mizing KL(qφ(z
latent dimensions early on resulting in underﬁtting
(Fig. 5c [Left]). By using ψ∗, the inadequacies of the
inference network are decoupled from the variational
parameters used to derive gradients to θ. The tighter
(x; θ, ψ∗) achieves a better tradeoﬀ
variational bound
between E[log p(x
p(z)) (evidenced
x)
by the number of large singular values of
z)
when optimizing ψ∗ in Fig. 5c). The gradient updates
with respect to this tighter bound better utilize θ.

L
z)] and KL(qφ(z
|

z log p(x

∇

||

|

|

5.3 Collaborative ﬁltering

Modeling rare features in sparse, high-dimensional data
is necessary to achieve strong results on this task. We
study the top-N recommendation performance of NFA
under strong generalization (Marlin & Zemel, 2009).

Datasets: We study two large user-item rating
datasets: MovieLens-20M (ML-20M) (Harper & Kon-
stan, 2015) and Netﬂix3. Following standard procedure:
we binarize the explicit rating data, keeping ratings of
four or higher and interpreting them as implicit feed-
back (Hu et al. , 2008) and keep users who have posi-
tively rated at least ﬁve movies. We train with users’
binary implicit feedback as xd; the vocabulary is the set
of all movies. The number of training/validation/test
users is 116,677/10,000/10,000 for ML-20M (V : 20,108)
and 383,435/40,000/40,000 for Netﬂix (V : 17,769).

Evaluation and Metrics: We train with the com-
plete feedback history from training users, and evaluate
on held-out validation/test users. We select model ar-
chitecture (MLP with 0, 1, 2 hidden layers) from the

3http://www.netflixprize.com/

Figure 6: Raw KL and Rare Word Counts: We plot
the raw values of KL(ψ(x)(cid:107)ψ∗) versus the number of rare
words. We zoom into the plot and reduce the opacity of
the train points to better see the held-out points. The
Spearman ρ correlation coeﬃcient is computed between the
two across 20, 000 points. We ﬁnd a positive correlation.

suggests that data sparsity presents a problem for infer-
ence networks at an aggregate level. We now ask which
data points beneﬁt from the optimization of ψ(x)? We
sample 20000 training and held-out data points; we
compute KL(ψ(x)
ψ∗) (both are Normal distributions
(cid:107)
and the KL is analytic) and the number of rare words in
each document (where a word is classiﬁed as being rare
if it occurs in less than 5% of training documents). We
visualize them in Fig. 6. We also display the Spearman
ρ correlation between the two values in Fig. 6. There
exists a positive correlation (about 0.88 on the training
data) between the two values suggesting that the gains
in perplexity that we observe empirically in Table 1
and Fig. 3 are due to being able to better model the
likelihood of documents with rare words in them.

Putting it all together: Our analysis describes
a narrative of how underﬁtting occurs in learning
VAEs on sparse data. The rare words in sparse, high-

held-out validation users based on NDCG@100 and
report metrics on the held-out test users. For held-out
users, we randomly select 80% of the feedback as the
input to the inference network and see how the other
20% of the positively rated items are ranked based µ(z).
We report two ranking-based metrics averaged over all
held-out users: Recall@N and truncated normalized
discounted cumulative gain (NDCG@N ) (J¨arvelin &
Kek¨al¨ainen, 2002). For each user, both metrics com-
pare the predicted rank of unobserved items with their
true rank. While Recall@N considers all items ranked
within the ﬁrst N to be equivalent, NDCG@N uses
a monotonically increasing discount to emphasize the
importance of higher ranks versus lower ones.

Deﬁne π as a ranking over all the items where π(v)
indicates the v-th ranked item, I
is the indicator
function, and d(π(v)) returns 1 if user d has positively
rated item π(v). Recall@N for user d is

{·}

Recall@N (d, π) :=

N
(cid:88)

v=1

min(N, (cid:80)V

I

d(π(v)) = 1
}
{
v(cid:48) I
d(π(v(cid:48))) = 1
{

)

}

.

The expression in the denominator evaluates to the min-
imum between N and the number of items consumed
by user d. This normalizes Recall@N to have a max-
imum of 1, which corresponds to ranking all relevant
items in the top N positions. Discounted cumulative

Table 2: Recall and NDCG on Recommender Sys-
tems: “2-ψ∗-tﬁdf” denotes a two-layer (one hidden layer
and one output layer) generative model. Standard errors
are around 0.002 for ML-20M and 0.001 for Netﬂix. Run-
time: WMF takes on the order of minutes [ML-20M &
Netﬂix]; CDAE and NFA (ψ(x)) take 8 hours [ML-20M]
and 32.5 hours [Netﬂix] for 150 epochs; NFA (ψ∗) takes
takes 1.5 days [ML-20M] and 3 days [Netﬂix]; SLIM takes
3-4 days [ML-20M] and 2 weeks [Netﬂix].

Recall@50
ψ∗
0.484
0.508
0.505
0.515

NDCG@100
ψ∗
0.377
0.396
0.396
0.404

ψ(x)
0.371
0.376
0.389
0.395

ML-20M

NFA

ψ(x)
2-ψ(x)-norm 0.475
2-ψ∗-norm
0.483
0.499
2-ψ(x)-tﬁdf
2-ψ∗-tﬁdf
0.509
wmf
slim
cdae

0.498
0.495
0.512

0.386
0.401
0.402

Recall@50
ψ∗
0.393
0.415
0.409
0.424

NDCG@100
ψ∗
0.337
0.358
0.353
0.367

ψ(x)
0.333
0.347
0.348
0.359

Netﬂix

NFA

ψ(x)
2-ψ(x)-norm 0.388
2-ψ∗-norm
0.404
0.404
2-ψ(x)-tﬁdf
2-ψ∗-tﬁdf
0.417
wmf
slim
cdae

0.404
0.427
0.417

0.351
0.378
0.360

gain (DCG@N ) for user d is

DCG@N (d, π) :=

N
(cid:88)

2I
{

v=1

d(π(v))=1

}
log(v + 1)

−

1

.

NDCG@N is the DCG@N normalized by ideal DCGN ,
where all the relevant items are ranked at the top. We
have, NDCG@N

[0, 1]. As baselines, we consider:

∈

Weighted matrix factorization (WMF) (Hu et al.
, 2008): a linear low-rank factor model. We train WMF
with alternating least squares; this generally leads to
better performance than with SGD.
SLIM (Ning & Karypis, 2011): a linear model which
learns a sparse item-to-item similarity matrix by solv-
ing a constrained (cid:96)1-regularized optimization problem.
Collaborative denoising autoencoder (CDAE)
(Wu et al. , 2016): An autoencoder achitecture speciﬁ-
cally designed for top-N recommendation. It augments
a denoising autoencoder (Vincent et al. , 2008) by
adding a per-user latent vector to the input, inspired
by standard linear matrix-factorization approaches.
Among the baselines, CDAE is most akin to NFA.

Table 2 summarizes the results of NFA under diﬀerent
settings. We found that optimizing ψ(x) helps both at
train and test time and that TF-IDF features consis-
tently improve performance. Crucially, the standard
training procedure for VAEs realizes a poorly trained
model that underperforms every baseline. The im-
proved training techniques we recommend generalize
across diﬀerent kinds of sparse data. With them, the
same generative model, outperforms CDAE and WMF
on both datasets, and marginally outperforms SLIM
on ML-20M while achieving nearly state-of-the-art re-
sults on Netﬂix. In terms of runtimes, we found that
learning NFA (with ψ∗) to be approximately two-three
times faster than SLIM. Our results highlight the im-
portance of inference at training time showing NFA,
when properly ﬁt, can outperform the popular linear
factorization approaches.

6 Discussion

Studying the failures of learning with inference net-
works is an important step to designing more robust
neural architectures for inference networks. We show
that avoiding gradients obtained using poor variational
parameters is vital to successfully learning VAEs on
sparse data. An interesting question is why inference
networks have a harder time turning sparse data into
variational parameters compared to images? One hy-
pothesis is that the redundant correlations that exist
among pixels (but occur less frequently in features
found in sparse data) are more easily transformed into
local variational parameters ψ(x) that are, in practice,
often reasonably close to ψ∗ during learning.

Acknowledgements

The authors are grateful to David Sontag, Fredrik
Johansson, Matthew Johnson, and Ardavan Saeedi
for helpful comments and suggestions regarding this
work.

References

Huang, Eric H., Socher, Richard, Manning, Christo-
pher D., & Ng, Andrew Y. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. In: ACL.

J¨arvelin, K., & Kek¨al¨ainen, J. 2002. Cumulated gain-
based evaluation of IR techniques. ACM Transac-
tions on Information Systems (TOIS), 20(4), 422–
446.

Baeza-Yates, Ricardo, Ribeiro-Neto, Berthier, et al. .
1999. Modern information retrieval. Vol. 463. ACM
press New York.

Jones, Christopher S. 2006. A nonlinear factor analysis
of S&P 500 index option returns. The Journal of
Finance.

Blei, David M, Ng, Andrew Y, & Jordan, Michael I.

2003. Latent dirichlet allocation. JMLR.

Bowman, Samuel R, Vilnis, Luke, Vinyals, Oriol, Dai,
Andrew M, Jozefowicz, Rafal, & Bengio, Samy. 2016.
Generating sentences from a continuous space. In:
CoNLL.

Burda, Yuri, Grosse, Roger, & Salakhutdinov, Ruslan.
2015. Importance weighted autoencoders. In: ICLR.

Chen, Xi, Kingma, Diederik P, Salimans, Tim, Duan,
Yan, Dhariwal, Prafulla, Schulman, John, Sutskever,
Ilya, & Abbeel, Pieter. 2017. Variational lossy au-
toencoder. ICLR.

Collins, Michael, Dasgupta, Sanjoy, & Schapire,
Robert E. 2001. A generalization of principal com-
ponent analysis to the exponential family. In: NIPS.

Gibson, WA. 1960. Nonlinear factors in two dimensions.

Psychometrika.

Jutten, Christian, & Karhunen, Juha. 2003. Advances

in nonlinear blind source separation. In: ICA.

Kingma, Diederik, & Ba, Jimmy. 2015. Adam: A

method for stochastic optimization. In: ICLR.

Kingma, Diederik P, & Welling, Max. 2014. Auto-

encoding variational bayes. In: ICLR.

Kingma, Diederik P, Mohamed, Shakir, Rezende,
Semi-
Danilo Jimenez, & Welling, Max. 2014.
supervised learning with deep generative models. In:
NIPS.

Larochelle, Hugo, Bengio, Yoshua, Louradour, J´erˆome,
& Lamblin, Pascal. 2009. Exploring strategies for
training deep neural networks. JMLR.

Lawrence, Neil D. 2003. Gaussian Process Latent Vari-
able Models for Visualisation of High Dimensional
Data. In: NIPS.

Glorot, Xavier, & Bengio, Yoshua. 2010. Understanding
the diﬃculty of training deep feedforward neural
networks. In: AISTATS.

Lewis, David D, Yang, Yiming, Rose, Tony G, & Li,
Fan. 2004. RCV1: A new benchmark collection for
text categorization research. JMLR.

Harper, F Maxwell, & Konstan, Joseph A. 2015.
The MovieLens Datasets: History and Context.
ACM Transactions on Interactive Intelligent Systems
(TiiS).

Hinton, Geoﬀrey E, & Salakhutdinov, Ruslan R. 2009.
Replicated softmax: an undirected topic model. In:
NIPS.

Hinton, Geoﬀrey E, Dayan, Peter, Frey, Brendan J, &
Neal, Radford M. 1995. The” wake-sleep” algorithm
for unsupervised neural networks. Science.

Hjelm, R Devon, Cho, Kyunghyun, Chung, Junyoung,
Salakhutdinov, Russ, Calhoun, Vince, & Jojic, Nebo-
jsa. 2016. Iterative Reﬁnement of Approximate Pos-
In:
terior for Training Directed Belief Networks.
NIPS.

Hoﬀman, Matthew D, Blei, David M, Wang, Chong, &
Paisley, John William. 2013. Stochastic variational
inference. JMLR.

Hu, Y., Koren, Y., & Volinsky, C. 2008. Collaborative
ﬁltering for implicit feedback datasets. In: ICDM.

Marlin, Benjamin M, & Zemel, Richard S. 2009. Col-
laborative prediction and ranking with non-random
missing data. In: RecSys.

Miao, Yishu, Yu, Lei, & Blunsom, Phil. 2016. Neural
Variational Inference for Text Processing. In: ICML.

Mnih, Andriy, & Gregor, Karol. 2014. Neural varia-
tional inference and learning in belief networks. In:
ICML.

Ning, Xia, & Karypis, George. 2011. Slim: Sparse linear
methods for top-n recommender systems. Pages
497–506 of: Data Mining (ICDM), 2011 IEEE 11th
International Conference on. IEEE.

Rezende, Danilo Jimenez, Mohamed, Shakir, & Wier-
stra, Daan. 2014. Stochastic backpropagation and
approximate inference in deep generative models. In:
ICML.

Salakhutdinov, Ruslan, & Larochelle, Hugo. 2010. Ef-
ﬁcient Learning of Deep Boltzmann Machines. In:
AISTATS.

Salimans, Tim, Kingma, Diederik, & Welling, Max.
2015. Markov chain monte carlo and variational
inference: Bridging the gap. Pages 1218–1226 of:
Proceedings of the 32nd International Conference on
Machine Learning (ICML-15).

Sedhain, Suvash, Menon, Aditya Krishna, Sanner,
Scott, & Braziunas, Darius. 2016. On the Eﬀective-
ness of Linear Models for One-Class Collaborative
Filtering. In: AAAI.

Spearman, Charles. 1904. ”General Intelligence,” ob-
jectively determined and measured. The American
Journal of Psychology.

Valpola, Harri, & Karhunen, Juha. 2002. An unsu-
pervised ensemble learning method for nonlinear dy-
namic state-space models. Neural computation.

Vincent, Pascal, Larochelle, Hugo, Bengio, Yoshua,
& Manzagol, Pierre-Antoine. 2008. Extracting and
composing robust features with denoising autoen-
coders. Pages 1096–1103 of: Proceedings of the 25th
international conference on Machine learning.

Wang, Shengjie, Plilipose, Matthai, Richardson,
Matthew, Geras, Krzysztof, Urban, Gregor, & Aslan,
Ozlem. 2016. Analysis of Deep Neural Networks with
the Extended Data Jacobian Matrix. In: ICML.

Wu, Yao, DuBois, Christopher, Zheng, Alice X, &
Ester, Martin. 2016. Collaborative denoising auto-
encoders for top-n recommender systems. Pages 153–
162 of: Proceedings of the Ninth ACM International
Conference on Web Search and Data Mining. ACM.

Supplementary Material

Contents

1. Spectral analysis of the Jacobian matrix

2. Relationship with annealing the KL divergence

3. Inference on documents with rare words

4. Depth of qφ(z

x)
|

5. Learning with ψ∗ on small dataset

7 Spectral Analysis of the Jacobian

Matrix

For any vector valued function f (x) : RK

RV ,
xf (x) is the matrix-valued function representing the
∇
sensitivity of the output to the input. When f (x) is a
deep neural network, Wang et al. (2016) use the spec-
tra of the Jacobian matrix under various inputs x to
quantify the complexity of the learned function. They
ﬁnd that the spectra are correlated with the complexity
of the learned function.

→

We adopt their technique for studying the utilization of
the latent space in deep generative models. In the case
of NFA, we seek to quantify the learned complexity
of the generative model. To do so, we compute the
z). This is a
Jacobian matrix as
|
read-out measure of the sensitivity of the likelihood
with respect to the latent dimension.

z log p(x

(z) =

∇

J

(z) is a matrix valued function that can be evaluated
J
at every point in the latent space. We evaluate it it
at the mode of the (unimodal) prior distribution i.e.
at z = (cid:126)0. The singular values of the resulting matrix
denote how much the log-likelihood changes from the
origin along the singular vectors lying in latent space.
The intensity of these singular values (which we plot)
is a read-out measure of how many intrinsic dimensions
are utilized by the model parameters θ at the mode of
(z)
the prior distribution. Our choice of evaluating
at z = (cid:126)0 is motivated by the fact that much of the
probability mass in latent space under the NFA model
will be placed at the origin. We use the utilization
at the mode as an approximation for the utilization
across the entire latent space. We also plotted the
spectral decomposition obtained under a Monte-Carlo
approximation to the matrix E[
(z)] and found it to
be similar to the decomposition obtained by evaluating
the Jacobian at the mode.

J

J

Another possibility to measure utilization would be
from the KL divergence of the prior and the output of
the inference network (as in Burda et al. (2015)).

8 Learning with ψ∗ on a small dataset

Table 3: Test Perplexity on 20newsgroups: Left:
Baselines Legend: LDA (Blei et al. , 2003), Replicated
Softmax (RSM) (Hinton & Salakhutdinov, 2009), Sigmoid
Belief Networks (SBN) and Deep Autoregressive Networks
(DARN) (Mnih & Gregor, 2014), Neural Variational Doc-
ument Model (NVDM) (Miao et al. , 2016). K denotes
the latent dimension in our notation. Right: NFA on
text data with K = 100. We vary the features presented
to the inference network qφ(z|x) during learning between:
, denoted “norm”) and
normalized count vectors (
(cid:80)V

x
i=1 xi

normalized TF-IDF (denoted “tﬁdf”) features.

LDA
LDA
RSM
SBN

Model K Results
50
200
50
50
fDARN 50
fDARN 200
NVDM 50
NVDM 200

1091
1058
953
909
917
—
836
852

NFA

Perplexity
ψ∗
ψ(x)
903
1-ψ(x)-norm 1018
889
1-ψ∗-norm 1279
3-ψ(x)-norm 986
857
879
3-ψ∗-norm 1292
839
932
1-ψ(x)-tﬁdf
828
953
1-ψ∗-tﬁdf
3-ψ(x)-tﬁdf
842
999
839
1063
3-ψ∗-tﬁdf

In the main paper, we studied the optimization of vari-
ational parameters on the larger RCV1 and Wikipedia
datasets. Here, we study the role of learning with ψ∗
in the small-data regime. Table 3 depicts the results
obtained after training models for 200 passes through
the data. We summarize our ﬁndings: (1) across the
board, TF-IDF features improve learning, and (2) in
the small data regime, deeper non-linear models (3-ψ∗-
tﬁdf) overﬁt quickly and better results are obtained
by the simpler multinomial-logistic PCA model (1-
ψ∗-tﬁdf). Overﬁtting is also evident in Fig. 7 from
comparing curves on the validation set to those on the
training set. Interestingly, in the small dataset setting,
we see that learning with ψ(x) has the potential to
have a regularization eﬀect in that the results obtained
are not much worse than those obtained from learning
with ψ∗.

For completeness, in Fig. 8, we also provide the training
behavior for the RCV1 dataset corresponding to the re-
sults of Table 1. The results here, echo the convergence
behavior on the Wikipedia dataset.

(a) Training Data

(b) Held-out Data

(c) Log-singular Values

Figure 7: 20Newsgroups - Training and Held-out Bounds: Fig. 7a, 7b denotes the train (held-out) perplexity for
diﬀerent models. Fig. 7c depicts the log-singular values of the Jacobian matrix for the trained models.

(a) Training Data

(b) Held-out Data

(c) Log-singular Values

Figure 8: RCV1 - Training and Held-out Bounds: Fig. 8a, 8b denotes the train (held-out) perplexity for diﬀerent
models. Fig. 8c depicts the log-singular values of the Jacobian matrix for the trained models.

9 Comparison with KL-annealing

An empirical observation made in previous work is
that when p(x
z; θ) is complex (parameterized by a
|
recurrent neural network or a neural autoregressive
density estimator (NADE)), the generative model also
must contend with overpruning of the latent dimension.
A proposed ﬁx is the annealing of the KL divergence
term in Equation 2 (e.g., Bowman et al. , 2016) as
one way to overcome local minima. As discussed in
the main paper, this is a diﬀerent failure mode to the
one we present in that our decoder is a vanilla MLP –
nonetheless, we apply KL annealing within our setting.
In particular, we optimized Eqφ(z
−
η KL( qφ(z
p(z) ) where η was annealed from 0 to 1
(linearly – though we also tried exponential annealing)
over the course of several parameter updates. Note
that doing so does not give us a lower bound on the
likelihood of the data anymore. There are few estab-
lished guidelines about the rate of annealing the KL
divergence and in general, we found it tricky to get it
to work reliably. We experimented with diﬀerent rates
of annealing for learning a three-layer generative model
on the Wikipedia data.

x) [log pθ(x
|

x)
|

z))]

||

|

Our ﬁndings (visualized in Fig. 10) are as follows: (1)

on sparse data we found annealing the KL divergence
is very sensitive to the annealing rate – too small an
annealing rate and we were still left with underﬁtting
(as in annealing for 10k), too high an annealing rate
(as in 100k) and this resulted in slow convergence; (2)
learning with ψ∗ always outperformed (in both rate of
convergence and quality of ﬁnal result on train and held-
out data) annealing the KL divergence across various
choices of annealing schedules. Said diﬀerently, on
the Wikipedia dataset, we conjecture there exists a
choice of annealing of the KL divergence for which
the perplexity obtained may match those of learning
with ψ∗ but ﬁnding this schedule requires signiﬁcant
trial and error – Fig. 10 suggests that we did not ﬁnd
it. We found that learning with ψ∗ was more robust,
required less tuning (setting values of M to be larger
than 100 never hurt) and always performed at par or
better than annealing the KL divergence. Furthermore,
we did not ﬁnd annealing the KL to work eﬀectively for
the experiments on the recommender systems task. In
particular, we were unable to ﬁnd an annealing schedule
that reliably produced good results.

10 Depth of qφ(z

x)
|

Can the overall eﬀect of the additional optimization be
learned by the inference network at training time? The
experimental evidence we observe in Fig. 9 suggests
this is diﬃcult.

When learning with ψ(x), increasing the number of
layers in the inference network slightly decreases the
quality of the model learned. This is likely because
the already stochastic gradients of the inference net-
work must propagate along a longer path in a deeper
inference network, slowing down learning of the param-
eters φ which in turn aﬀects ψ(x), thereby reducing
the quality of the gradients used to updated θ.

11

Inference on documents with rare
words

(cid:107)

Here, we present another way to visualize the results
of Fig. 6. We sample 20000 training and held-out data
ψ∗) (both are Normal
points; we compute KL(ψ(x)
distributions and the KL is analytic) and the number of
rare words in each document (where a word is classiﬁed
as being rare if it occurs in less than 5% of training
documents). We scale each value to be between 0
and 1 using:
min(c) where c is the vector of
KL divergences or number of rare words. We sort the
scaled values by the KL divergence and plot them in
Fig. 11. As before, we observe that the documents
that we move the farthest in KL divergence are those
which have many rare words.

ci
−
max(c)

min(c)

−

(a) Training Data

(b) Held-out Data

Figure 9: Varying the Depth of qφ(z|x): Fig. 10a (10b) denotes the train (held-out) perplexity for a three-layer
generative model learned with inference networks of varying depth. The notation q3-ψ∗ denotes that the inference network
contained a two-layer intermediate hidden layer h(x) = MLP(x; φ0) followed by µ(x) = Wµh(x), log Σ(x) = Wlog Σh(x).

(a) Training Data

(b) Held-out Data

(c) Log-singular Values

Figure 10: KL annealing vs learning with ψ∗ Fig. 10a, 10b denotes the train (held-out) perplexity for diﬀerent
training methods. The suﬃx at the end of the model conﬁguration denotes the number of parameter updates that it took
for the KL divergence in Equation 2 to be annealed from 0 to 1. 3-ψ∗-50k denotes that it took 50000 parameter updates
before −L(x; θ, ψ(x)) was used as the loss function. Fig. 7c depicts the log-singular values of the Jacobian matrix for the
trained models.

(a) Sparsity of Wikipedia

(b) Training Data

(c) Held-out Data

Figure 11: Normalized KL and Rare Word Counts: Fig. 11a depicts percentage of times words appear in the
Wikipedia dataset (sorted by frequency). The dotted line in blue denotes the marker for a word that has a 5% occurrence
in documents. In Fig. 11b, 11c, we superimpose (1) the normalized (to be between 0 and 1) values of KL(ψ(x)(cid:107)ψ∗) and
(2) the normalized number of rare words (sorted by value of the KL-divergence) for 20, 000 points (on the x-axis) randomly
sampled from the train and held-out data.

On the challenges of learning with inference networks on sparse,
high-dimensional data

7
1
0
2
 
t
c
O
 
7
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
5
8
0
6
0
.
0
1
7
1
:
v
i
X
r
a

Rahul G. Krishnan
MIT

Dawen Liang
Netﬂix

Matthew D. Hoﬀman
Google

Abstract

We study parameter estimation in Nonlin-
ear Factor Analysis (NFA) where the gen-
erative model is parameterized by a deep
neural network. Recent work has focused
on learning such models using inference (or
recognition) networks; we identify a crucial
problem when modeling large, sparse, high-
dimensional datasets – underﬁtting. We study
the extent of underﬁtting, highlighting that
its severity increases with the sparsity of the
data. We propose methods to tackle it via
iterative optimization inspired by stochastic
variational inference (Hoﬀman et al. , 2013)
and improvements in the sparse data represen-
tation used for inference. The proposed tech-
niques drastically improve the ability of these
powerful models to ﬁt sparse data, achieving
state-of-the-art results on a benchmark text-
count dataset and excellent results on the task
of top-N recommendation.

1 Introduction

Factor analysis (FA, Spearman, 1904) is a widely used
latent variable model in the applied sciences. The
generative model assumes data, x, is a linear function
of independent Normally distributed latent variables,
z. The assumption of linearity has been relaxed in
nonlinear factor analysis (NFA) (Gibson, 1960) and
extended across a variety of domains such as economics
(Jones, 2006), signal processing (Jutten & Karhunen,
2003), and machine learning (Valpola & Karhunen,
2002; Lawrence, 2003). NFA assumes the joint distribu-
tion factorizes as p(x, z; θ) = p(z)p(x
z; θ). When the
|
nonlinear conditional distribution is parameterized by
deep neural networks (e.g., Valpola & Karhunen, 2002),
the graphical model is referred to as a deep generative
model. When paired with an inference or recognition
network (Hinton et al. , 1995), a parametric function
x) (local
that approximates posterior distribution p(z

|

variational parameters) from data, such models go by
the name of variational autoencoders (VAEs, Kingma
& Welling, 2014; Rezende et al. , 2014). We study
NFA for the density estimation and analysis of sparse,
high-dimensional categorical data.

Sparse, high-dimensional data is ubiquitous; it arises
naturally in survey and demographic data, bag-of-words
representations of text, mobile app usage logs, rec-
ommender systems, genomics, and electronic health
records. Such data are characterized by few frequently
occurring features and a long-tail of rare features.
When directly learning VAEs on sparse data, a problem
we run into is that the standard learning algorithm
results in underﬁtting and fails to utilize the model’s
full capacity. The inability to fully utilize the capacity
of complex models in the large data regime is cause for
concern since it limits the applicability of this class of
models to problems in the aforementioned domains.

The contributions of this work are as follows. We
identify a problem with standard VAE training when
applied to sparse, high-dimensional data—underﬁtting.
We investigate the underlying causes of this phe-
nomenon, and propose modiﬁcations to the learning
algorithm to address these causes. We combine infer-
ence networks with an iterative optimization scheme
inspired by Stochastic Variational Inference (SVI) (Hoﬀ-
man et al. , 2013). The proposed learning algorithm
dramatically improves the quality of the estimated pa-
rameters. We empirically study various factors that
govern the severity of underﬁtting and how the tech-
niques we propose mitigate it. A practical ramiﬁcation
of our work is that improvements in learning NFA on
recommender system data translate to more accurate
predictions and better recommendations. In contrast,
standard VAE training fails to outperform the simple
shallow linear models that still largely dominate the
collaborative ﬁltering domain (Sedhain et al. , 2016).

2 Background

Generative Model: We consider learning in gener-
ative models of the form shown in Figure 1. We intro-

Figure 1: Nonlinear Factor Analysis:
[Left] The generative model contains a
single latent variable z. The conditional
probability p(x|z; θ) parameterized by
a deep neural network. [Right] The
inference network qφ(z|x) is used for
inference at train and test time.

z

θ

x

z

φ

x

duce the model in the context of performing maximum
likelihood estimation over a corpus of documents. We
observe a set of D word-count1 vectors x1:D, where
xdv denotes the number of times that word index
appears in document d. Given the
v
v xdv, xd
total number of words per document Nd
is generated via the following generative process:

1, . . . , V

∈ {

(cid:80)

≡

}

MLP(zd; θ);

(1)

zd

∼ N

µ(zd)

(0, I); γ(zd)
exp
{
v exp

≡
γ(zd)
}
γ(zd)v
{

(cid:80)

≡

; xd

Mult.(µ(zd), Nd).

∼

}
That is, we draw a Gaussian random vector, pass it
through a multilayer perceptron (MLP) parameterized
by θ, pass the resulting vector through the softmax
(a.k.a. multinomial logistic) function, and sample Nd
times from the resulting distribution over V .2

Variational Learning: For ease of exposition we
drop the subscript on xd when referring to a single data
point. Jensen’s inequality yields the following lower
bound on the log marginal likelihood of the data:

log pθ(x)

Eq(z;ψ)[log pθ(x
(cid:124)

|

≥

z)]

KL( q(z; ψ)

−
(cid:123)(cid:122)
(x;θ,ψ)

L

p(z) ).
(cid:125)

||

(2)

|

q(z; ψ) is a tractable “variational” distribution meant
to approximate the intractable posterior distribution
x); it is controlled by some parameters ψ. For
p(z
example, if q is Gaussian, then we might have ψ =
(z; µ, Σ). We are free to choose ψ
µ, Σ
{
however we want, but ideally we would choose the ψ
that makes the bound in equation 2 as tight as possible,
ψ∗ (cid:44) arg maxψ

, q(z; ψ) =

(x; θ, ψ).

N

}

L

Hoﬀman et al.
(2013) proposed ﬁnding ψ∗ using it-
erative optimization, starting from a random initial-
ization. This is eﬀective, but can be costly. More
recently, Kingma & Welling (2014) and Rezende et al.
(2014) proposed training a feedforward inference net-
work (Hinton et al. , 1995) to ﬁnd good variational

1We use word-count in document for the sake of con-
creteness. Our methodology is generally applicable to other
types of discrete high-dimensional data.

2In keeping with common practice, we neglect the multi-
nomial base measure term N !
x1!···xV ! , which amounts to as-
suming that the words are observed in a particular order.

L

parameters ψ(x) for a given x, where ψ(x) is the out-
put of a neural network with parameters φ that are
trained to maximize
(x; θ, ψ(x)). Often it is much
cheaper to compute ψ(x) than to obtain an optimal ψ∗
using iterative optimization. But there is no guarantee
that ψ(x) produces optimal variational parameters—it
may yield a much looser lower bound than ψ∗ if the
inference network is either not suﬃciently powerful or
its parameters φ are not well tuned.

Throughout the rest of this paper, we will use ψ(x) to
denote an inference network that implicitly depends
on some parameters φ, and ψ∗ to denote a set of vari-
ational parameters obtained by applying an iterative
optimization algorithm to equation 2. Following com-
mon convention, we will sometimes use qφ(z
x) as
shorthand for q(z; ψ(x)).

|

3 Mitigating Underﬁtting

We elucidate our hypothesis on why the learning algo-
rithm for VAEs is susceptible to underﬁtting.

3.1 Tight Lower Bounds on log p(x)

There are two sources of error in variational parameter
estimation with inference networks:

The ﬁrst is the distributional error accrued due to
learning with a tractable-but-approximate family of
x) instead of the true posterior dis-
distributions qφ(z
|
tribution p(z
x). Although diﬃcult to compute in
practice, it is easy to show that this error is exactly
KL(qφ(z
x)). We restrict ourselves to working
|
with normally distributed variational approximations
and do not aim to overcome this source of error.

x)
(cid:107)
|

p(z

|

The second source of error comes from the sub-
optimality of the variational parameters ψ used in
(x; θ, ψ(x)) is a valid
Eq. 2. We are guaranteed that
lower bound on log p(x) for any output of qφ(z
x) but
within the same family of variational distributions,
there exists an optimal choice of variational parameters
realizing the tightest variational bound
µ∗, Σ∗
ψ∗ =
{
for a data point x.

L

}

|

log p(x)
E
(cid:124)

N

≥
(µ∗;Σ∗)[log p(x

(x; θ, ψ(x))

≥ L

(3)

z; θ)]
|

−
(cid:123)(cid:122)
(x;θ,ψ∗)

L

KL(

(µ∗, Σ∗)
(cid:107)

N

p(z))
(cid:125)

Where we deﬁne: ψ∗ :=
E
= arg max
µ,Σ

µ∗, Σ∗
}
{
z; θ)]
(µ,Σ)[log p(x

N

|

KL(

(µ, Σ)
(cid:107)

N

−

p(z)).

The cartoon in Fig. 2 illustrates this double bound.

Standard learning algorithms for VAEs update θ, φ
jointly based on
(x; θ, ψ(x)) (see Alg. 1 for pseu-
docode). That is, they directly use ψ(x) (as output by
qφ(z

x)) to estimate Equation 2.
|

L

L

In contrast, older stochastic variational inference meth-
ods (Hoﬀman et al. , 2013) update θ based on gradients
of
(x; θ, ψ∗) by updating randomly initialized varia-
tional parameters for each example. ψ∗ is obtained by
(x; θ, ψ) with respect to ψ. This max-
maximizing
imization is performed by M gradient ascent steps
yielding ψM

ψ∗. (see Alg. 2).

L

≈

Algorithm 1 Learning with Inference Networks
(Kingma et al. , 2014)

:= [x1, . . . , xD] ,

Inputs:
Model: qφ(z
for k = 1. . . K do

x), pθ(x
|

D

|

z), p(z);

Sample: x
θk+1
φk+1

, ψ(x) = qφ(z
∇θk
L
∇φk
L

θk + ηθ
φk + ηφ

∼ D
←
←

|

(x; θk, ψ(x))
(x; θk, ψ(x))

x), update θ, φ:

end for

Algorithm 2 Learning with Stochastic Variational
Inference: M : number of gradient updates to ψ.

:= [x1, . . . , xD] ,

D

Inputs:
Model: pθ(x
for k = 1. . . K do
1. Sample: x
2. Approx. ψM

z), p(z);
|

∼ D
≈

and initialize: ψ0 = µ0, Σ0
(x; θ; ψ):
ψ∗ = arg maxψ
1:
∂
ψm+1 = ψm + ηψ

−

L

For m = 0, . . . , M

L
θk + ηθ

(x;θk,ψm)
∂ψm
∇θk

L

←

(x; θk, ψM )

3. Update θ: θk+1

end for

3.2 Limitations of Joint Parameter Updates

(1) updates θ, φ jointly. During training, the
Alg.
inference network learns to approximate the posterior,
and the generative model improves itself using local
variational parameters ψ(x) output by qφ(z

x).

|

If the variational parameters ψ(x) output by the in-
ference network are close to the optimal variational
parameters ψ∗ (Eq. 3), then the updates for θ are
based on a relatively tight lower bound on log p(x).
But in practice ψ(x) may not be a good approximation
to ψ∗.

Both the inference network and generative model are
initialized randomly. At the start of learning, ψ(x) is
the output of a randomly initialized neural network,
and will therefore be a poor approximation to the opti-
mal parameters ψ∗. So the gradients used to update θ
will be based on a very loose lower bound on log p(x).
These gradients may push the generative model towards

a poor local minimum— previous work has argued
that deep neural networks (which form the conditional
probability distributions pθ(x
z)) are often sensitive to
|
initialization (Glorot & Bengio, 2010; Larochelle et al.
, 2009). Even later in learning, ψ(x) may yield sub-
optimal gradients for θ if the inference network is not
powerful enough to ﬁnd optimal variational parameters
for all data points.

Learning in the original SVI scheme does not suﬀer
from this problem, since the variational parameters
are optimized within the inner loop of learning before
updating to θ (i.e.
in Alg. (2); ∂θ is eﬀectively de-
(x; θ, ψ∗)). However, this method requires
rived using
potentially an expensive iterative optimization.

L

This motivates blending the two methodologies for
parameter estimation. Rather than rely entirely on
the inference network, we use its output to “warm-
start” an SVI-style optimization that yields higher-
quality estimates of ψ∗, which in turn should yield
more meaningful gradients for θ.

Figure 2: Lower Bounds in Variational Learning:
To estimate θ, we maximize a lower bound on log p(x; θ).
L(x; θ, ψ(x)) denotes the standard training objective used
by VAEs. The tightness of this bound (relative to L(x; θ, ψ∗)
depends on the inference network. The x-axis is θ.

3.3 Optimizing Local Variational Parameters

L

We use the local variational parameters ψ = ψ(x) pre-
dicted by the inference network to initialize an iter-
ative optimizer. As in Alg. 2, we perform gradient
ascent to maximize
(x; θ, ψ) with respect to ψ. The
resulting ψM approximates the optimal variational pa-
rameters: ψM
ψ∗. Since NFA is a continuous latent
variable model, these updates can be achieved via the
re-parameterization gradient (Kingma & Welling, 2014).
(x; θ, ψ∗).
We use ψ∗ to derive gradients for θ under
Finally, the parameters of the inference network (φ) are
updated using stochastic backpropagation and gradient
descent, holding ﬁxed the parameters of the generative
model (θ). Our procedure is detailed in Alg. 3.

≈

L

3.4 Representations for Inference Networks

The inference network must learn to regress to the
optimal variational parameters for any combination of

Algorithm 3 Maximum Likelihood Estimation of θ
with Optimized Local Variational Parameters: Ex-
pectations in L(x, θ, ψ∗) (see Eq. 3) are evaluated with a
single sample from the optimized variational distribution.
M is the number of updates to the variational parame-
ters (M = 0 implies no additional optimization). θ, ψ(x), φ
are updated using stochastic gradient descent with learn-
ing rates ηθ, ηψ, ηφ obtained via ADAM (Kingma & Ba,
2015).
In step 4, we update φ separately from θ. One
could alternatively, update φ using KL(ψ(x)M (cid:107)qφ(z|x)) as
in Salakhutdinov & Larochelle (2010).

:= [x1, . . . , xD] ,

Inputs:
D
Inference Model: qφ(z
Generative Model: pθ(x
for k = 1. . . K do
1. Sample: x
2. Approx. ψM

x),
|

|

z), p(z),

∼ D
≈

and set ψ0 = ψ(x)
ψ∗ = arg maxψ
1:
∂
ψm+1 = ψm + ηψ

(x;θk,ψm)
∂ψm

−

L

L

For m = 0, . . . , M

3. Update θ,
θk+1
←
4. Update φ,
φk+1

←

end for

θk + ηθ

(x; θk, ψM )

∇θk

L

φk + ηφ

(x; θk+1, ψ(x))

∇φk

L

(x; θk; ψ),

features, but in sparse datasets, many words appear
only rarely. To provide more global context about rare
words, we provide to the inference network (but not the
generative network) TF-IDF (Baeza-Yates et al. , 1999)
features instead of raw counts. These give the inference
network a hint that rare words are likely to be highly
informative. TF-IDF is a popular technique in informa-
tion retrieval that re-weights features to increase the
inﬂuence of rarer features while decreasing the inﬂuence
of common features. The transformed feature-count
vector is ˜xdv
. The resulting
vector ˜x is then normalized by its L2 norm.

D
d(cid:48) min

xdv log

xd(cid:48) v,1

≡

(cid:80)

{

}

4 Related Work

Salakhutdinov & Larochelle optimize local mean-ﬁeld
parameters from an inference network in the context
of learning deep Boltzmann machines. Salimans et al.
explore warm starting MCMC with the output of an
inference network. Hjelm et al. explore a similar idea
as ours to derive an importance-sampling-based bound
for learning deep generative models with discrete latent
variables. They ﬁnd that learning with ψ∗ does not
improve results on binarized MNIST. This is consistent
with our experience—we ﬁnd that our secondary opti-
mization procedure helped more when learning models
of sparse data. Miao et al.
learn log-linear models
(multinomial-logistic PCA, Collins et al. , 2001) of
documents using inference networks. We show that
mitigating underﬁtting in deeper models yields better

results on the benchmark RCV1 data. Our use of the
spectra of the Jacobian matrix of log p(x
z) to inspect
|
learned models is inspired by Wang et al. (2016).

Previous work has studied the failure modes of learn-
ing VAEs. They can be broadly categorized into two
classes. The ﬁrst aims to improves the utilization of
latent variables using a richer posterior distribution
(Burda et al. , 2015). However, for sparse data, the
x)
limits of learning with a Normally distributed qφ(z
|
have barely been pushed – our goal is to do so in this
work. Further gains may indeed be obtained with a
richer posterior distribution but the techniques herein
can inform work along this vein. The second class of
methods studies ways to alleviate the underutilization
of latent dimensions due to an overtly expressive choice
of models for p(x
z; θ) such as a Recurrent Neural Net-
|
work (Bowman et al. , 2016; Chen et al. , 2017). This
too, is not the scenario we are in; underﬁtting of VAEs
z; θ) is an MLP.
on sparse data occurs even when p(x
|

Our study here exposes a third failure mode; one in
which learning is challenging not just because of the
objective used in learning but also because of the char-
acteristics of the data.

5 Evaluation

We ﬁrst conﬁrm our hypothesis empirically that un-
derﬁtting is an issue when learning VAEs on high di-
mensional sparse datasets. We quantify the gains (at
training and test time) obtained by the use of TF-IDF
features and the continued optimization of ψ(x) on
two diﬀerent types of high-dimensional sparse data—
text and movie ratings. In Section 5.2, we learn VAEs
on two large scale bag-of-words datasets. We study
(1) where the proposed methods might have the most
impact and (2) present evidence for why the learning
In Section 5.3, we show
algorithm (Alg. 3) works.
that improved inference is crucial to building deep
generative models that can tackle problems in top-N
recommender systems. We conclude with a discussion.

5.1 Setup

Notation:
In all experiments, ψ(x) denotes learning
with Alg. 1 and ψ∗ denotes the results of learning with
Alg. 3. M = 100 (number of updates to the local
variational parameters) on the bag-of-words text data
and M = 50 on the recommender systems task. M
was chosen based on the number of steps it takes for
(x; θ, ψm) (Step 2 in Alg. 3) to converge on training
L
data. 3-ψ∗-norm denotes a model where the MLP
parameterizing γ(z) has three layers: two hidden layers
and one output layer, ψ∗ is used to derive an update of θ
and normalized count features are conditioned on by the

inference network. In all tables, we display evaluation
metrics obtained under both ψ(x) (the output of the
inference network) and ψ∗ (the optimized variational
parameters).
In ﬁgures, we always display metrics
obtained under ψ∗ (even if the model was trained with
ψ(x)) since
(x; θ, ψ∗) always forms a tighter bound to
log p(x). If left unspeciﬁed TF-IDF features are used
as input to the inference network.

L

Training and Evaluation: We update θ using learn-
ing rates given by ADAM (Kingma & Ba, 2015) (using
a batch size of 500), The inference network’s inter-
mediate hidden layer h(x) = MLP(x; φ0) (we use a
two-layer MLP in the inference network for all experi-
ments) are used to parameterize the mean and diagonal
log-variance as: µ(x) = Wµh(x), log Σ(x) = Wlog Σh(x)
where φ =
. Code is available at
github.com/rahulk90/vae_sparse.

Wµ, Wlog Σ, φ0

}

{

5.2 Bag-of-words text data

Datasets and Metrics: We study two large text
(1) RCV1 (Lewis et al. , 2004) dataset
datasets.
(train/valid/test: 789,414/5,000/10,000, V : 10,000).
We follow the preprocessing procedure in Miao et al.
(2016). (2) The Wikipedia corpus used in Huang et al.
(2012) (train/test: 1,104,937/100,000 and V :20,000).
We set all words to lowercase, ignore numbers and re-
strict the dataset to the top 20, 000 frequently occurring
words. We report an upper bound on perplexity (Mnih
log p(xi))
& Gregor, 2014) given by exp(
where log p(xi) is replaced by Eq 2. To study the
utilization of the latent dimension obtained by various
training methods, we compute the Jacobian
(z) ma-
z)). The singular value spectrum
trix (as
|
of the Jacobian directly measures the utilization of
the latent dimensions in the model. We provide more
details on this in the supplementary material.

z log p(x

(cid:80)
i

1
Ni

1
N

∇

−

J

Reducing Underﬁtting:
Is underﬁtting a problem
and does optimizing ψ(x) with the use of TF-IDF
features help? Table 1 conﬁrms both statements.

(1) between
We make the following observations:
“norm” and “tﬁdf” (comparing ﬁrst four rows and sec-
ond four rows), we ﬁnd that the use of TF-IDF features
almost always improves parameter estimation; (2) op-
timizing ψ(x) at test time (comparing column ψ∗ with
ψ(x)) always yields a tighter bound on log p(x), of-
ten by a wide margin. Even after extensive training
the inference network can fail to tightly approximate
(x; θ, ψ∗), suggesting that there may be limitations
L
to the power of generic amortized inference; (3) opti-
mizing ψ(x) during training ameliorates under-ﬁtting
and yields signiﬁcantly better generative models on the
RCV1 dataset. We ﬁnd that the degree of underﬁtting
and subsequently the improvements from training with

ψ∗ are signiﬁcantly more pronounced on the larger and
sparser Wikipedia dataset (Fig. 3a and 3b).

Eﬀect of optimizing ψ(x): How does learning with
ψ∗ aﬀect the rate of convergence the learning algo-
rithm? We plot the upper bound on perplexity versus
epochs on the Wikipedia (Fig. 3a, 3b) datasets. As in
Table 1, the additional optimization does not appear
to help much when the generative model is linear. On
the deeper three-layer model, learning with ψ∗ dramat-
ically improves the model allowing it to fully utilize its
potential for density estimation. Models learned with
ψ∗ quickly converge to a better local minimum early on
(as reﬂected in the perplexity evaluated on the train-
ing data and held-out data). We experimented with
continuing to train 3-ψ(x) beyond 150 epochs, where it
reached a validation perplexity of approximately 1330,
worse than that obtained by 3-ψ∗ at epoch 10 suggest-
ing that longer training is unsuﬃcient to overcome local
minima issues aﬄicting VAEs.

Overpruning of latent dimensions: One cause of
underﬁtting is due to overpruning of the latent dimen-
sions in the model. If the variational distributions for a
subset of the latent dimensions of z are set to the prior,
this eﬀectively reduces the model’s capacity.
If the
KL-divergence in Eq. 2 encourages the approximate
posterior to remain close to the prior early in training,
and if the gradient signals from the likelihood term are
weak or inconsistent, the KL may dominate and prune
out latent dimensions before the model can use them.

In Fig. 3c, we plot the log-spectrum of the Jacobian
matrices for diﬀerent training methods and models. For
the deeper models, optimizing ψ(x) is crucial to utiliz-
ing its capacity, particularly on the sparser Wikipedia
data. Without it, only about ten latent dimensions are

Table 1: Test Perplexity on RCV1:
Left: Base-
lines Legend: LDA (Blei et al. , 2003), Replicated Softmax
(RSM) (Hinton & Salakhutdinov, 2009), Sigmoid Belief Net-
works (SBN) and Deep Autoregressive Networks (DARN)
(Mnih & Gregor, 2014), Neural Variational Document Model
(NVDM) (Miao et al. , 2016). K denotes the latent dimen-
sion in our notation. Right: NFA on text data with
K = 100. We vary the features presented to the inference
network qφ(z|x) during learning between: normalized count
, denoted “norm”) and normalized TF-IDF
vectors (

(cid:80)V

x
i=1 xi

LDA
LDA
RSM
SBN

Model K RCV1
50
200
50
50
fDARN 50
fDARN 200
NVDM 50
NVDM 200

1437
1142
988
784
724
598
563
550

NFA

ψ(x)
1-ψ(x)-norm 501
1-ψ∗-norm
488
3-ψ(x)-norm 396
378
3-ψ∗-norm
480
1-ψ(x)-tﬁdf
482
1-ψ∗-tﬁdf
384
3-ψ(x)-tﬁdf
376
3-ψ∗-tﬁdf

ψ∗
481
454
355
331
456
454
344
331

(a) Wikipedia - Training

(b) Wikipedia - Evaluation

(c) Log-Singular Values

Figure 3: Mechanics of Learning: Best viewed in color. (Left and Middle) For the Wikipedia dataset, we visualize
upper bounds on training and held-out perplexity (evaluated with ψ∗) viewed as a function of epochs. Items in the legend
corresponds to choices of training method. (Right) Sorted log-singular values of ∇z log p(x|z) on Wikipedia (left) on
RCV1 (right) for diﬀerent training methods. The x-axis is latent dimension. The legend is identical to that in Fig. 3a.

used, and the model severely underﬁts the data. Opti-
mizing ψ(x) iteratively likely limits overpruning since
the variational parameters (ψ∗) don’t solely focus on
minimizing the KL-divergence but also on maximizing
the likelihood of the data (the ﬁrst term in Eq. 2).

obtained by training with ψ∗ in Fig. 4.

Learning with ψ∗ helps more as the data dimensionality
increases. Data sparsity, therefore, poses a signiﬁcant
challenge to inference networks. One possible explana-
tion is that many of the tokens in the dataset are rare,
and the inference network therefore needs many sweeps
over the dataset to learn to properly interpret these
rare words; while the inference network is learning to
interpret these rare words the generative model is re-
ceiving essentially random learning signals that drive
it to a poor local optimum.

Designing new strategies that can deal with such data
may be a fruitful direction for future work. This may
require new architectures or algorithms—we found that
simply making the inference network deeper does not
solve the problem.

When should ψ(x) be optimized: When are the
gains obtained from learning with ψ∗ accrued? We
learn three-layer models on Wikipedia under two set-
tings: (a) we train for 10 epochs using ψ∗ and then 10
epochs using ψ(x). and (b) we do the opposite.

Fig. 5 depicts the results of this experiment. We ﬁnd
that: (1) much of the gain from optimizing ψ(x) comes
from the early epochs, (2) somewhat surprisingly using
ψ∗ instead of ψ(x) later on in learning also helps (as
witnessed by the sharp drop in perplexity after epoch
10 and the number of large singular values in Fig. 5c
[Left]). This suggests that even after seeing the data for
several passes, the inference network is unable to ﬁnd
ψ(x) that explain the data well. Finally, (3) for a ﬁxed
computational budget, one is better oﬀ optimizing ψ(x)
sooner than later – the curve that optimizes ψ(x) later
on does not catch up to the one that optimizes ψ(x)
early in learning. This suggests that learning early with
ψ∗, even for a few epochs, may alleviate underﬁtting.

Rare words and loose lower bounds:

Fig. 4

Figure 4: Decrease in Perplexity versus Sparsity:
We plot the relative drop in perplexity obtained by train-
ing with ψ∗ instead of ψ(x) against varying levels of
sparsity in the Wikipedia data. On the y-axis, we plot
P[3−ψ(x)]−P[3−ψ∗ ]
; P denotes the bound on perplexity (eval-
P[3−ψ(x)]
uated with ψ∗) and the subscript denotes the model and
method used during training. Each point on the x-axis is
a restriction of the dataset to the top L most frequently
occurring words (number of features).

Sparse data is challenging: What is the rela-
tionship between data sparsity and how well inference
networks work? We hold ﬁxed the number of training
samples and vary the sparsity of the data. We do so
by restricting the Wikipedia dataset to the top L most
frequently occurring words. We train three layer gener-
ative models on the diﬀerent subsets. On training and
held-out data, we computed the diﬀerence between the
perplexity when the model is trained with (denoted
ψ∗]) and without optimization of ψ(x) (denoted
P[3
ψ(x)]). We plot the relative decrease in perplexity
P[3

−

−

(a) Training Data

(b) Held-out Data

(c) Log-singular Values

Figure 5: Late versus Early optimization of ψ(x): Fig. 5a (5b) denote the train (held-out) perplexity for three-
layered models trained on the Wikipedia data in the following scenarios: ψ∗ is used for training for the ﬁrst ten epochs
following which ψ(x) is used (denoted “ψ∗ then ψ(x)”) and vice versa (denoted “ψ(x) then ψ∗”). Fig. 5c (Left) depicts the
number of singular values of the Jacobian matrix ∇z log p(x|z) with value greater than 1 as a function of training epochs
for each of the two aforementioned methodologies. Fig. 5c (Right) plots the sorted log-singular values of the Jacobian
matrix corresponding to the ﬁnal model under each training strategy.

|

||

x)
|

dimensional data are diﬃcult to map into local vari-
ational parameters that model the term E[log p(x
z)]
|
well (Fig. 4,6); qφ(z
x) therefore focuses on the less
noisy (the KL is evaluated analytically) signal of mini-
p(z)). Doing so prunes out many
mizing KL(qφ(z
latent dimensions early on resulting in underﬁtting
(Fig. 5c [Left]). By using ψ∗, the inadequacies of the
inference network are decoupled from the variational
parameters used to derive gradients to θ. The tighter
(x; θ, ψ∗) achieves a better tradeoﬀ
variational bound
between E[log p(x
p(z)) (evidenced
x)
by the number of large singular values of
z)
when optimizing ψ∗ in Fig. 5c). The gradient updates
with respect to this tighter bound better utilize θ.

L
z)] and KL(qφ(z
|

z log p(x

∇

||

|

|

5.3 Collaborative ﬁltering

Modeling rare features in sparse, high-dimensional data
is necessary to achieve strong results on this task. We
study the top-N recommendation performance of NFA
under strong generalization (Marlin & Zemel, 2009).

Datasets: We study two large user-item rating
datasets: MovieLens-20M (ML-20M) (Harper & Kon-
stan, 2015) and Netﬂix3. Following standard procedure:
we binarize the explicit rating data, keeping ratings of
four or higher and interpreting them as implicit feed-
back (Hu et al. , 2008) and keep users who have posi-
tively rated at least ﬁve movies. We train with users’
binary implicit feedback as xd; the vocabulary is the set
of all movies. The number of training/validation/test
users is 116,677/10,000/10,000 for ML-20M (V : 20,108)
and 383,435/40,000/40,000 for Netﬂix (V : 17,769).

Evaluation and Metrics: We train with the com-
plete feedback history from training users, and evaluate
on held-out validation/test users. We select model ar-
chitecture (MLP with 0, 1, 2 hidden layers) from the

3http://www.netflixprize.com/

Figure 6: Raw KL and Rare Word Counts: We plot
the raw values of KL(ψ(x)(cid:107)ψ∗) versus the number of rare
words. We zoom into the plot and reduce the opacity of
the train points to better see the held-out points. The
Spearman ρ correlation coeﬃcient is computed between the
two across 20, 000 points. We ﬁnd a positive correlation.

suggests that data sparsity presents a problem for infer-
ence networks at an aggregate level. We now ask which
data points beneﬁt from the optimization of ψ(x)? We
sample 20000 training and held-out data points; we
compute KL(ψ(x)
ψ∗) (both are Normal distributions
(cid:107)
and the KL is analytic) and the number of rare words in
each document (where a word is classiﬁed as being rare
if it occurs in less than 5% of training documents). We
visualize them in Fig. 6. We also display the Spearman
ρ correlation between the two values in Fig. 6. There
exists a positive correlation (about 0.88 on the training
data) between the two values suggesting that the gains
in perplexity that we observe empirically in Table 1
and Fig. 3 are due to being able to better model the
likelihood of documents with rare words in them.

Putting it all together: Our analysis describes
a narrative of how underﬁtting occurs in learning
VAEs on sparse data. The rare words in sparse, high-

held-out validation users based on NDCG@100 and
report metrics on the held-out test users. For held-out
users, we randomly select 80% of the feedback as the
input to the inference network and see how the other
20% of the positively rated items are ranked based µ(z).
We report two ranking-based metrics averaged over all
held-out users: Recall@N and truncated normalized
discounted cumulative gain (NDCG@N ) (J¨arvelin &
Kek¨al¨ainen, 2002). For each user, both metrics com-
pare the predicted rank of unobserved items with their
true rank. While Recall@N considers all items ranked
within the ﬁrst N to be equivalent, NDCG@N uses
a monotonically increasing discount to emphasize the
importance of higher ranks versus lower ones.

Deﬁne π as a ranking over all the items where π(v)
indicates the v-th ranked item, I
is the indicator
function, and d(π(v)) returns 1 if user d has positively
rated item π(v). Recall@N for user d is

{·}

Recall@N (d, π) :=

N
(cid:88)

v=1

min(N, (cid:80)V

I

d(π(v)) = 1
}
{
v(cid:48) I
d(π(v(cid:48))) = 1
{

)

}

.

The expression in the denominator evaluates to the min-
imum between N and the number of items consumed
by user d. This normalizes Recall@N to have a max-
imum of 1, which corresponds to ranking all relevant
items in the top N positions. Discounted cumulative

Table 2: Recall and NDCG on Recommender Sys-
tems: “2-ψ∗-tﬁdf” denotes a two-layer (one hidden layer
and one output layer) generative model. Standard errors
are around 0.002 for ML-20M and 0.001 for Netﬂix. Run-
time: WMF takes on the order of minutes [ML-20M &
Netﬂix]; CDAE and NFA (ψ(x)) take 8 hours [ML-20M]
and 32.5 hours [Netﬂix] for 150 epochs; NFA (ψ∗) takes
takes 1.5 days [ML-20M] and 3 days [Netﬂix]; SLIM takes
3-4 days [ML-20M] and 2 weeks [Netﬂix].

Recall@50
ψ∗
0.484
0.508
0.505
0.515

NDCG@100
ψ∗
0.377
0.396
0.396
0.404

ψ(x)
0.371
0.376
0.389
0.395

ML-20M

NFA

ψ(x)
2-ψ(x)-norm 0.475
2-ψ∗-norm
0.483
0.499
2-ψ(x)-tﬁdf
2-ψ∗-tﬁdf
0.509
wmf
slim
cdae

0.498
0.495
0.512

0.386
0.401
0.402

Recall@50
ψ∗
0.393
0.415
0.409
0.424

NDCG@100
ψ∗
0.337
0.358
0.353
0.367

ψ(x)
0.333
0.347
0.348
0.359

Netﬂix

NFA

ψ(x)
2-ψ(x)-norm 0.388
2-ψ∗-norm
0.404
0.404
2-ψ(x)-tﬁdf
2-ψ∗-tﬁdf
0.417
wmf
slim
cdae

0.404
0.427
0.417

0.351
0.378
0.360

gain (DCG@N ) for user d is

DCG@N (d, π) :=

N
(cid:88)

2I
{

v=1

d(π(v))=1

}
log(v + 1)

−

1

.

NDCG@N is the DCG@N normalized by ideal DCGN ,
where all the relevant items are ranked at the top. We
have, NDCG@N

[0, 1]. As baselines, we consider:

∈

Weighted matrix factorization (WMF) (Hu et al.
, 2008): a linear low-rank factor model. We train WMF
with alternating least squares; this generally leads to
better performance than with SGD.
SLIM (Ning & Karypis, 2011): a linear model which
learns a sparse item-to-item similarity matrix by solv-
ing a constrained (cid:96)1-regularized optimization problem.
Collaborative denoising autoencoder (CDAE)
(Wu et al. , 2016): An autoencoder achitecture speciﬁ-
cally designed for top-N recommendation. It augments
a denoising autoencoder (Vincent et al. , 2008) by
adding a per-user latent vector to the input, inspired
by standard linear matrix-factorization approaches.
Among the baselines, CDAE is most akin to NFA.

Table 2 summarizes the results of NFA under diﬀerent
settings. We found that optimizing ψ(x) helps both at
train and test time and that TF-IDF features consis-
tently improve performance. Crucially, the standard
training procedure for VAEs realizes a poorly trained
model that underperforms every baseline. The im-
proved training techniques we recommend generalize
across diﬀerent kinds of sparse data. With them, the
same generative model, outperforms CDAE and WMF
on both datasets, and marginally outperforms SLIM
on ML-20M while achieving nearly state-of-the-art re-
sults on Netﬂix. In terms of runtimes, we found that
learning NFA (with ψ∗) to be approximately two-three
times faster than SLIM. Our results highlight the im-
portance of inference at training time showing NFA,
when properly ﬁt, can outperform the popular linear
factorization approaches.

6 Discussion

Studying the failures of learning with inference net-
works is an important step to designing more robust
neural architectures for inference networks. We show
that avoiding gradients obtained using poor variational
parameters is vital to successfully learning VAEs on
sparse data. An interesting question is why inference
networks have a harder time turning sparse data into
variational parameters compared to images? One hy-
pothesis is that the redundant correlations that exist
among pixels (but occur less frequently in features
found in sparse data) are more easily transformed into
local variational parameters ψ(x) that are, in practice,
often reasonably close to ψ∗ during learning.

Acknowledgements

The authors are grateful to David Sontag, Fredrik
Johansson, Matthew Johnson, and Ardavan Saeedi
for helpful comments and suggestions regarding this
work.

References

Huang, Eric H., Socher, Richard, Manning, Christo-
pher D., & Ng, Andrew Y. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. In: ACL.

J¨arvelin, K., & Kek¨al¨ainen, J. 2002. Cumulated gain-
based evaluation of IR techniques. ACM Transac-
tions on Information Systems (TOIS), 20(4), 422–
446.

Baeza-Yates, Ricardo, Ribeiro-Neto, Berthier, et al. .
1999. Modern information retrieval. Vol. 463. ACM
press New York.

Jones, Christopher S. 2006. A nonlinear factor analysis
of S&P 500 index option returns. The Journal of
Finance.

Blei, David M, Ng, Andrew Y, & Jordan, Michael I.

2003. Latent dirichlet allocation. JMLR.

Bowman, Samuel R, Vilnis, Luke, Vinyals, Oriol, Dai,
Andrew M, Jozefowicz, Rafal, & Bengio, Samy. 2016.
Generating sentences from a continuous space. In:
CoNLL.

Burda, Yuri, Grosse, Roger, & Salakhutdinov, Ruslan.
2015. Importance weighted autoencoders. In: ICLR.

Chen, Xi, Kingma, Diederik P, Salimans, Tim, Duan,
Yan, Dhariwal, Prafulla, Schulman, John, Sutskever,
Ilya, & Abbeel, Pieter. 2017. Variational lossy au-
toencoder. ICLR.

Collins, Michael, Dasgupta, Sanjoy, & Schapire,
Robert E. 2001. A generalization of principal com-
ponent analysis to the exponential family. In: NIPS.

Gibson, WA. 1960. Nonlinear factors in two dimensions.

Psychometrika.

Jutten, Christian, & Karhunen, Juha. 2003. Advances

in nonlinear blind source separation. In: ICA.

Kingma, Diederik, & Ba, Jimmy. 2015. Adam: A

method for stochastic optimization. In: ICLR.

Kingma, Diederik P, & Welling, Max. 2014. Auto-

encoding variational bayes. In: ICLR.

Kingma, Diederik P, Mohamed, Shakir, Rezende,
Semi-
Danilo Jimenez, & Welling, Max. 2014.
supervised learning with deep generative models. In:
NIPS.

Larochelle, Hugo, Bengio, Yoshua, Louradour, J´erˆome,
& Lamblin, Pascal. 2009. Exploring strategies for
training deep neural networks. JMLR.

Lawrence, Neil D. 2003. Gaussian Process Latent Vari-
able Models for Visualisation of High Dimensional
Data. In: NIPS.

Glorot, Xavier, & Bengio, Yoshua. 2010. Understanding
the diﬃculty of training deep feedforward neural
networks. In: AISTATS.

Lewis, David D, Yang, Yiming, Rose, Tony G, & Li,
Fan. 2004. RCV1: A new benchmark collection for
text categorization research. JMLR.

Harper, F Maxwell, & Konstan, Joseph A. 2015.
The MovieLens Datasets: History and Context.
ACM Transactions on Interactive Intelligent Systems
(TiiS).

Hinton, Geoﬀrey E, & Salakhutdinov, Ruslan R. 2009.
Replicated softmax: an undirected topic model. In:
NIPS.

Hinton, Geoﬀrey E, Dayan, Peter, Frey, Brendan J, &
Neal, Radford M. 1995. The” wake-sleep” algorithm
for unsupervised neural networks. Science.

Hjelm, R Devon, Cho, Kyunghyun, Chung, Junyoung,
Salakhutdinov, Russ, Calhoun, Vince, & Jojic, Nebo-
jsa. 2016. Iterative Reﬁnement of Approximate Pos-
In:
terior for Training Directed Belief Networks.
NIPS.

Hoﬀman, Matthew D, Blei, David M, Wang, Chong, &
Paisley, John William. 2013. Stochastic variational
inference. JMLR.

Hu, Y., Koren, Y., & Volinsky, C. 2008. Collaborative
ﬁltering for implicit feedback datasets. In: ICDM.

Marlin, Benjamin M, & Zemel, Richard S. 2009. Col-
laborative prediction and ranking with non-random
missing data. In: RecSys.

Miao, Yishu, Yu, Lei, & Blunsom, Phil. 2016. Neural
Variational Inference for Text Processing. In: ICML.

Mnih, Andriy, & Gregor, Karol. 2014. Neural varia-
tional inference and learning in belief networks. In:
ICML.

Ning, Xia, & Karypis, George. 2011. Slim: Sparse linear
methods for top-n recommender systems. Pages
497–506 of: Data Mining (ICDM), 2011 IEEE 11th
International Conference on. IEEE.

Rezende, Danilo Jimenez, Mohamed, Shakir, & Wier-
stra, Daan. 2014. Stochastic backpropagation and
approximate inference in deep generative models. In:
ICML.

Salakhutdinov, Ruslan, & Larochelle, Hugo. 2010. Ef-
ﬁcient Learning of Deep Boltzmann Machines. In:
AISTATS.

Salimans, Tim, Kingma, Diederik, & Welling, Max.
2015. Markov chain monte carlo and variational
inference: Bridging the gap. Pages 1218–1226 of:
Proceedings of the 32nd International Conference on
Machine Learning (ICML-15).

Sedhain, Suvash, Menon, Aditya Krishna, Sanner,
Scott, & Braziunas, Darius. 2016. On the Eﬀective-
ness of Linear Models for One-Class Collaborative
Filtering. In: AAAI.

Spearman, Charles. 1904. ”General Intelligence,” ob-
jectively determined and measured. The American
Journal of Psychology.

Valpola, Harri, & Karhunen, Juha. 2002. An unsu-
pervised ensemble learning method for nonlinear dy-
namic state-space models. Neural computation.

Vincent, Pascal, Larochelle, Hugo, Bengio, Yoshua,
& Manzagol, Pierre-Antoine. 2008. Extracting and
composing robust features with denoising autoen-
coders. Pages 1096–1103 of: Proceedings of the 25th
international conference on Machine learning.

Wang, Shengjie, Plilipose, Matthai, Richardson,
Matthew, Geras, Krzysztof, Urban, Gregor, & Aslan,
Ozlem. 2016. Analysis of Deep Neural Networks with
the Extended Data Jacobian Matrix. In: ICML.

Wu, Yao, DuBois, Christopher, Zheng, Alice X, &
Ester, Martin. 2016. Collaborative denoising auto-
encoders for top-n recommender systems. Pages 153–
162 of: Proceedings of the Ninth ACM International
Conference on Web Search and Data Mining. ACM.

Supplementary Material

Contents

1. Spectral analysis of the Jacobian matrix

2. Relationship with annealing the KL divergence

3. Inference on documents with rare words

4. Depth of qφ(z

x)
|

5. Learning with ψ∗ on small dataset

7 Spectral Analysis of the Jacobian

Matrix

For any vector valued function f (x) : RK

RV ,
xf (x) is the matrix-valued function representing the
∇
sensitivity of the output to the input. When f (x) is a
deep neural network, Wang et al. (2016) use the spec-
tra of the Jacobian matrix under various inputs x to
quantify the complexity of the learned function. They
ﬁnd that the spectra are correlated with the complexity
of the learned function.

→

We adopt their technique for studying the utilization of
the latent space in deep generative models. In the case
of NFA, we seek to quantify the learned complexity
of the generative model. To do so, we compute the
z). This is a
Jacobian matrix as
|
read-out measure of the sensitivity of the likelihood
with respect to the latent dimension.

z log p(x

(z) =

∇

J

(z) is a matrix valued function that can be evaluated
J
at every point in the latent space. We evaluate it it
at the mode of the (unimodal) prior distribution i.e.
at z = (cid:126)0. The singular values of the resulting matrix
denote how much the log-likelihood changes from the
origin along the singular vectors lying in latent space.
The intensity of these singular values (which we plot)
is a read-out measure of how many intrinsic dimensions
are utilized by the model parameters θ at the mode of
(z)
the prior distribution. Our choice of evaluating
at z = (cid:126)0 is motivated by the fact that much of the
probability mass in latent space under the NFA model
will be placed at the origin. We use the utilization
at the mode as an approximation for the utilization
across the entire latent space. We also plotted the
spectral decomposition obtained under a Monte-Carlo
approximation to the matrix E[
(z)] and found it to
be similar to the decomposition obtained by evaluating
the Jacobian at the mode.

J

J

Another possibility to measure utilization would be
from the KL divergence of the prior and the output of
the inference network (as in Burda et al. (2015)).

8 Learning with ψ∗ on a small dataset

Table 3: Test Perplexity on 20newsgroups: Left:
Baselines Legend: LDA (Blei et al. , 2003), Replicated
Softmax (RSM) (Hinton & Salakhutdinov, 2009), Sigmoid
Belief Networks (SBN) and Deep Autoregressive Networks
(DARN) (Mnih & Gregor, 2014), Neural Variational Doc-
ument Model (NVDM) (Miao et al. , 2016). K denotes
the latent dimension in our notation. Right: NFA on
text data with K = 100. We vary the features presented
to the inference network qφ(z|x) during learning between:
, denoted “norm”) and
normalized count vectors (
(cid:80)V

x
i=1 xi

normalized TF-IDF (denoted “tﬁdf”) features.

LDA
LDA
RSM
SBN

Model K Results
50
200
50
50
fDARN 50
fDARN 200
NVDM 50
NVDM 200

1091
1058
953
909
917
—
836
852

NFA

Perplexity
ψ∗
ψ(x)
903
1-ψ(x)-norm 1018
889
1-ψ∗-norm 1279
3-ψ(x)-norm 986
857
879
3-ψ∗-norm 1292
839
932
1-ψ(x)-tﬁdf
828
953
1-ψ∗-tﬁdf
3-ψ(x)-tﬁdf
842
999
839
1063
3-ψ∗-tﬁdf

In the main paper, we studied the optimization of vari-
ational parameters on the larger RCV1 and Wikipedia
datasets. Here, we study the role of learning with ψ∗
in the small-data regime. Table 3 depicts the results
obtained after training models for 200 passes through
the data. We summarize our ﬁndings: (1) across the
board, TF-IDF features improve learning, and (2) in
the small data regime, deeper non-linear models (3-ψ∗-
tﬁdf) overﬁt quickly and better results are obtained
by the simpler multinomial-logistic PCA model (1-
ψ∗-tﬁdf). Overﬁtting is also evident in Fig. 7 from
comparing curves on the validation set to those on the
training set. Interestingly, in the small dataset setting,
we see that learning with ψ(x) has the potential to
have a regularization eﬀect in that the results obtained
are not much worse than those obtained from learning
with ψ∗.

For completeness, in Fig. 8, we also provide the training
behavior for the RCV1 dataset corresponding to the re-
sults of Table 1. The results here, echo the convergence
behavior on the Wikipedia dataset.

(a) Training Data

(b) Held-out Data

(c) Log-singular Values

Figure 7: 20Newsgroups - Training and Held-out Bounds: Fig. 7a, 7b denotes the train (held-out) perplexity for
diﬀerent models. Fig. 7c depicts the log-singular values of the Jacobian matrix for the trained models.

(a) Training Data

(b) Held-out Data

(c) Log-singular Values

Figure 8: RCV1 - Training and Held-out Bounds: Fig. 8a, 8b denotes the train (held-out) perplexity for diﬀerent
models. Fig. 8c depicts the log-singular values of the Jacobian matrix for the trained models.

9 Comparison with KL-annealing

An empirical observation made in previous work is
that when p(x
z; θ) is complex (parameterized by a
|
recurrent neural network or a neural autoregressive
density estimator (NADE)), the generative model also
must contend with overpruning of the latent dimension.
A proposed ﬁx is the annealing of the KL divergence
term in Equation 2 (e.g., Bowman et al. , 2016) as
one way to overcome local minima. As discussed in
the main paper, this is a diﬀerent failure mode to the
one we present in that our decoder is a vanilla MLP –
nonetheless, we apply KL annealing within our setting.
In particular, we optimized Eqφ(z
−
η KL( qφ(z
p(z) ) where η was annealed from 0 to 1
(linearly – though we also tried exponential annealing)
over the course of several parameter updates. Note
that doing so does not give us a lower bound on the
likelihood of the data anymore. There are few estab-
lished guidelines about the rate of annealing the KL
divergence and in general, we found it tricky to get it
to work reliably. We experimented with diﬀerent rates
of annealing for learning a three-layer generative model
on the Wikipedia data.

x) [log pθ(x
|

x)
|

z))]

||

|

Our ﬁndings (visualized in Fig. 10) are as follows: (1)

on sparse data we found annealing the KL divergence
is very sensitive to the annealing rate – too small an
annealing rate and we were still left with underﬁtting
(as in annealing for 10k), too high an annealing rate
(as in 100k) and this resulted in slow convergence; (2)
learning with ψ∗ always outperformed (in both rate of
convergence and quality of ﬁnal result on train and held-
out data) annealing the KL divergence across various
choices of annealing schedules. Said diﬀerently, on
the Wikipedia dataset, we conjecture there exists a
choice of annealing of the KL divergence for which
the perplexity obtained may match those of learning
with ψ∗ but ﬁnding this schedule requires signiﬁcant
trial and error – Fig. 10 suggests that we did not ﬁnd
it. We found that learning with ψ∗ was more robust,
required less tuning (setting values of M to be larger
than 100 never hurt) and always performed at par or
better than annealing the KL divergence. Furthermore,
we did not ﬁnd annealing the KL to work eﬀectively for
the experiments on the recommender systems task. In
particular, we were unable to ﬁnd an annealing schedule
that reliably produced good results.

10 Depth of qφ(z

x)
|

Can the overall eﬀect of the additional optimization be
learned by the inference network at training time? The
experimental evidence we observe in Fig. 9 suggests
this is diﬃcult.

When learning with ψ(x), increasing the number of
layers in the inference network slightly decreases the
quality of the model learned. This is likely because
the already stochastic gradients of the inference net-
work must propagate along a longer path in a deeper
inference network, slowing down learning of the param-
eters φ which in turn aﬀects ψ(x), thereby reducing
the quality of the gradients used to updated θ.

11

Inference on documents with rare
words

(cid:107)

Here, we present another way to visualize the results
of Fig. 6. We sample 20000 training and held-out data
ψ∗) (both are Normal
points; we compute KL(ψ(x)
distributions and the KL is analytic) and the number of
rare words in each document (where a word is classiﬁed
as being rare if it occurs in less than 5% of training
documents). We scale each value to be between 0
and 1 using:
min(c) where c is the vector of
KL divergences or number of rare words. We sort the
scaled values by the KL divergence and plot them in
Fig. 11. As before, we observe that the documents
that we move the farthest in KL divergence are those
which have many rare words.

ci
−
max(c)

min(c)

−

(a) Training Data

(b) Held-out Data

Figure 9: Varying the Depth of qφ(z|x): Fig. 10a (10b) denotes the train (held-out) perplexity for a three-layer
generative model learned with inference networks of varying depth. The notation q3-ψ∗ denotes that the inference network
contained a two-layer intermediate hidden layer h(x) = MLP(x; φ0) followed by µ(x) = Wµh(x), log Σ(x) = Wlog Σh(x).

(a) Training Data

(b) Held-out Data

(c) Log-singular Values

Figure 10: KL annealing vs learning with ψ∗ Fig. 10a, 10b denotes the train (held-out) perplexity for diﬀerent
training methods. The suﬃx at the end of the model conﬁguration denotes the number of parameter updates that it took
for the KL divergence in Equation 2 to be annealed from 0 to 1. 3-ψ∗-50k denotes that it took 50000 parameter updates
before −L(x; θ, ψ(x)) was used as the loss function. Fig. 7c depicts the log-singular values of the Jacobian matrix for the
trained models.

(a) Sparsity of Wikipedia

(b) Training Data

(c) Held-out Data

Figure 11: Normalized KL and Rare Word Counts: Fig. 11a depicts percentage of times words appear in the
Wikipedia dataset (sorted by frequency). The dotted line in blue denotes the marker for a word that has a 5% occurrence
in documents. In Fig. 11b, 11c, we superimpose (1) the normalized (to be between 0 and 1) values of KL(ψ(x)(cid:107)ψ∗) and
(2) the normalized number of rare words (sorted by value of the KL-divergence) for 20, 000 points (on the x-axis) randomly
sampled from the train and held-out data.

