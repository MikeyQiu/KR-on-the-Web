SoftTriple Loss: Deep Metric Learning Without Triplet Sampling

Qi Qian1

Lei Shang2 Baigui Sun2

Juhua Hu3 Hao Li2 Rong Jin1

1 Alibaba Group, Bellevue, WA, 98004, USA
2 Alibaba Group, Hangzhou, China
3 School of Engineering and Technology
University of Washington, Tacoma, WA, 98402, USA
{qi.qian, sl172005, baigui.sbg, lihao.lh, jinrong.jr}@alibaba-inc.com, juhuah@uw.edu

0
2
0
2
 
r
p
A
 
5
1
 
 
]

V
C
.
s
c
[
 
 
2
v
5
3
2
5
0
.
9
0
9
1
:
v
i
X
r
a

Abstract

Distance metric learning (DML) is to learn the embed-
dings where examples from the same class are closer than
examples from different classes. It can be cast as an opti-
mization problem with triplet constraints. Due to the vast
number of triplet constraints, a sampling strategy is essen-
tial for DML. With the tremendous success of deep learning
in classiﬁcations, it has been applied for DML. When learn-
ing embeddings with deep neural networks (DNNs), only a
mini-batch of data is available at each iteration. The set of
triplet constraints has to be sampled within the mini-batch.
Since a mini-batch cannot capture the neighbors in the orig-
inal set well, it makes the learned embeddings sub-optimal.
On the contrary, optimizing SoftMax loss, which is a clas-
siﬁcation loss, with DNN shows a superior performance in
certain DML tasks. It inspires us to investigate the formu-
lation of SoftMax. Our analysis shows that SoftMax loss is
equivalent to a smoothed triplet loss where each class has
a single center. In real-world data, one class can contain
several local clusters rather than a single one, e.g., birds
of different poses. Therefore, we propose the SoftTriple loss
to extend the SoftMax loss with multiple centers for each
class. Compared with conventional deep metric learning
algorithms, optimizing SoftTriple loss can learn the embed-
dings without the sampling phase by mildly increasing the
size of the last fully connected layer. Experiments on the
benchmark ﬁne-grained data sets demonstrate the effective-
ness of the proposed loss function. Code is available at
https://github.com/idstcv/SoftTriple.

1. Introduction

Distance metric learning (DML) has been extensively
studied in the past decades due to its broad range of ap-
plications, e.g., k-nearest neighbor classiﬁcation [29], im-
age retrieval [24] and clustering [31]. With an appropriate

Figure 1. Illustration of the proposed SoftTriple loss. In conven-
tional SoftMax loss, each class has a representative center in the
last fully connected layer. Examples in the same class will be col-
lapsed to the same center. It may be inappropriate for the real-
world data as illustrated. In contrast, SoftTriple loss keeps multi-
ple centers (e.g., 2 centers per class in this example) in the fully
connected layer and each image will be assigned to one of them.
It is more ﬂexible for modeling intra-class variance in real-world
data sets.

distance metric, examples from the same class should be
closer than examples from different classes. Many algo-
rithms have been proposed to learn a good distance met-
ric [15, 16, 21, 29].

In most of conventional DML methods, examples are
represented by hand-crafted features, and DML is to learn a
feature mapping to project examples from the original fea-
ture space to a new space. The distance can be computed as
the Mahalanobis distance [11]

distM (xi, xj) = (xi − xj)(cid:62)M (xi − xj)

where M is the learned distance metric. With this formula-
tion, the main challenge of DML is from the dimensionality
of input space. As a metric, the learned matrix M has to be
positive semi-deﬁnite (PSD) while the cost of keeping the
matrix PSD can be up to O(d3), where d is the dimensional-
ity of original features. The early work directly applies PCA
to shrink the original space [29]. Later, various strategies
are developed to reduce the computational cost [16, 17].

1

Those approaches can obtain the good metric from the
input features, but the hand-crafted features are task in-
dependent and may cause the loss of information, which
limits the performance of DML. With the success of deep
neural networks in classiﬁcation [7], researchers consider
to learn the embeddings directly from deep neural net-
works [15, 21]. Without the explicit feature extraction,
deep metric learning boosts the performance by a large mar-
gin [21]. In deep metric learning, the dimensionality of in-
put features is no longer a challenge since neural networks
can learn low-dimensional features directly from raw mate-
rials, e.g., images, documents, etc. In contrast, generating
appropriate constraints for optimization becomes challeng-
ing for deep metric learning.

It is because most of deep neural networks are trained
with the stochastic gradient descent (SGD) algorithm and
only a mini-batch of examples are available at each itera-
tion. Since embeddings are optimized with the loss deﬁned
on an anchor example and its neighbors (e.g., the active
set of pairwise [31] or triplet [29] constraints), the exam-
ples in a mini-batch may not be able to capture the overall
neighborhood well, especially for relatively large data sets.
Moreover, a mini-batch contains O(m2) pairs and O(m3)
triplets, where m is the size of the mini-batch. An effective
sampling strategy over the mini-batch is essential even for
a small batch (e.g., 32) to learn the embeddings efﬁciently.
Many efforts have been devoted to studying sampling an in-
formative mini-batch [19, 21] and sampling triplets within
a mini-batch [12, 24]. Some work also tried to reduce the
total number of triplets with proxies [14, 18]. The sampling
phase for the mini-batch and constraints not only loses the
information but also makes the optimization complicated.
In this work, we consider to learn embeddings without con-
straints sampling.

Recently, researches have shown that embeddings ob-
tained directly from optimizing SoftMax loss, which is pro-
posed for classiﬁcation, perform well on the simple distance
based tasks [22, 30] and face recognition [2, 9, 10, 27, 28].
It inspires us to investigate the formulation of SoftMax loss.
Our Analysis demonstrates that SoftMax loss is equivalent
to a smoothed triplet loss. By providing a single center for
each class in the last fully connected layer, the triplet con-
straint derived by SoftMax loss can be deﬁned on an orig-
inal example, its corresponding center and a center from
a different class. Therefore, embeddings obtained by op-
timizing SoftMax loss can work well as a distance metric.
However, a class in real-world data can consist of multi-
ple local clusters as illustrated in Fig. 1 and a single center
is insufﬁcient to capture the inherent structure of the data.
Consequently, embeddings learned from SoftMax loss can
fail in the complex scenario [22].

In this work, we propose to improve SoftMax loss by
introducing multiple centers for each class and the novel

loss is denoted as SoftTriple loss. Compared with a single
center, multiple ones can capture the hidden distribution of
the data better due to the fact that they help to reduce the
intra-class variance. This property is also crucial to reserve
the triplet constraints over original examples while training
with multiple centers. Compared with existing deep DML
methods, the number of triplets in SoftTriple is linear in
the number of original examples. Since the centers are en-
coded in the last fully connected layer, SoftTriple loss can
be optimized without sampling triplets. Fig. 1 illustrates the
proposed SoftTriple loss. Apparently, SoftTriple loss has to
determine the number of centers for each class. To allevi-
ate this issue, we develop a strategy that sets a sufﬁciently
large number of centers for each class at the beginning and
then applies L2,1 norm to obtain a compact set of centers.
We demonstrate the proposed loss on the ﬁne-grained vi-
sual categorization tasks, where capturing local clusters is
essential for good performance [17].

The rest of this paper is organized as follows. Section
2 reviews the related work of conventional distance metric
learning and deep metric learning. Section 3 analyzes the
SoftMax loss and proposes the SoftTriple loss accordingly.
Section 4 conducts comparisons on benchmark data sets.
Finally, Section 5 concludes this work and discusses future
directions.

2. Related Work

Distance metric learning Many DML methods have
been developed when input features are provided [29, 31].
The dimensionality of input features is a critical challenge
for those methods due to the PSD projection, and many
strategies have been proposed to alleviate it. The most
straightforward way is to reduce the dimension of input
space by PCA [29]. However, PCA is task independent and
may hurt the performance of learned embeddings. Some
works try to reduce the number of valid parameters with
the low-rank assumption [8]. [16] decreases the computa-
tional cost by reducing the number of PSD projections. [17]
proposes to learn the dual variables in the low-dimensional
space introduced by random projections and then recover
the metric in the original space. After addressing the chal-
lenge from the dimensionality, the hand-crafted features be-
come the bottleneck of performance improvement.

The forms of constraints for metric learning are also de-
veloped in these methods. Early work focuses on optimiz-
ing pairwise constraints, which require the distances be-
tween examples from the same class small while those from
different classes large [31]. Later, [29] develops the triplet
constraints, where given an anchor example, the distance
between the anchor point and a similar example should be
smaller than that between the anchor point and a dissimi-
lar example by a large margin. It is obvious that the num-
ber of pairwise constraints is O(n2) while that of triplet

constraints can be up to O(n3), where n is the number of
original examples. Compared with the pairwise constraints,
triplet constraints optimize the geometry of local cluster and
are more applicable for modeling intra-class variance. In
this work, we will focus on the triplet constraints.

Deep metric learning Deep metric learning aims to learn
the embeddings directly from the raw materials (e.g., im-
ages) by deep neural networks [15, 21]. With the task
dependent embeddings, the performance of metric learn-
ing has a dramatical improvement. However, most of deep
models are trained with SGD that allows only a mini-batch
of data at each iteration. Since the size of mini-batch is
small, the information in it is limited compared to the orig-
inal data. To alleviate this problem, algorithms have to de-
velop an effective sampling strategy to generate the mini-
batch and then sample triplet constraints from it. A straight-
forward way is increasing the size of mini-batch [21]. How-
ever, the large mini-batch will suffer from the GPU mem-
ory limitation and can also increase the challenge of sam-
pling triplets. Later, [19] proposes to generate the mini-
batch from neighbor classes. Besides, there are various
sampling strategies for obtaining constraints [3, 12, 21, 24].
[21] proposes to sample the semi-hard negative examples.
[24] adopts all negative examples within the margin for each
positive pair. [12] develops distance weighted sampling that
samples examples according to the distance from the anchor
[3] selects hard triplets with a dynamic violate
example.
margin from a hierarchical class-level tree. However, all of
these strategies may fail to capture the distribution of the
whole data set. Moreover, they make the optimization in
deep DML complicated.

Learning with proxies Recently, some researchers con-
sider to reduce the total number of triplets to alleviate the
challenge from the large number of triplets. [14] constructs
the triplet loss with one original example and two prox-
ies. Since the number of proxies is signiﬁcantly less than
the number of original examples, proxies can be kept in
the memory that help to avoid the sampling over different
batches. However, it only provides a single proxy for each
class when label information is available, which is similar
to SoftMax. [18] proposes a conventional DML algorithm
to construct the triplet loss only with latent examples, which
assigns multiple centers for each class and further reduces
the number of triplets. In this work, we propose to learn the
embeddings by optimizing the proposed SoftTriple loss to
eliminate the sampling phase and capture the local geome-
try of each class simultaneously.

3. SoftTriple Loss

In this section, we ﬁrst introduce the SoftMax loss and
the triplet loss and then study the relationship between them
to derive the SoftTriple loss.

Denote the embedding of the i-th example as xi and the
corresponding label as yi, then the conditional probability
output by a deep neural network can be estimated via the
SoftMax operator

Pr(Y = yi|xi) =

exp(w(cid:62)
xi)
yi
j exp(w(cid:62)
j xi)

(cid:80)C

where [w1, · · · , wC] ∈ Rd×C is the last fully connected
layer. C denotes the number of classes and d is the dimen-
sion of embeddings. The corresponding SoftMax loss is

(cid:96)SoftMax(xi) = − log

exp(w(cid:62)
xi)
yi
j exp(w(cid:62)
j xi)

(cid:80)

A deep model can be learned by minimizing losses over
examples. This loss has been prevalently applied for classi-
ﬁcation task [7].

Given a triplet (xi, xj, xk), DML aims to learn good em-
beddings such that examples from the same class are closer
than examples from different classes, i.e.,

∀i, j, k,

(cid:107)xi − xk(cid:107)2

2 − (cid:107)xi − xj(cid:107)2

2 ≥ δ

where xi and xj are from the same class and xk is from a
different class. δ is a predeﬁned margin. When each exam-
ple has the unit length (i.e., (cid:107)x(cid:107)2 = 1), the triplet constraint
can be simpliﬁed as

∀i, j, k, x(cid:62)

i xj − x(cid:62)

i xk ≥ δ

(1)

where we ignore the rescaling of δ. The corresponding
triplet loss can be written as

(cid:96)triplet(xi, xj, xk) = [δ + x(cid:62)

i xk − x(cid:62)

i xj]+

(2)

It is obvious from Eqn. 1 that the number of total triplets
can be cubic in the number of examples, which makes sam-
pling inevitable for most of triplet based DML algorithms.
With the unit length for both w and x, the normalized

SoftMax loss can be written as

(cid:96)SoftMaxnorm (xi) = − log

exp(λw(cid:62)
xi)
yi
j exp(λw(cid:62)
j xi)

(cid:80)

(3)

where λ is a scaling factor.

Surprisingly, we ﬁnd that minimizing the normalized
SoftMax loss with the smooth term λ is equivalent to op-
timizing a smoothed triplet loss.

Proposition 1.

(cid:96)SoftMaxnorm(xi) = max
p∈∆

λ

pjx(cid:62)

i (wj − wyi) + H(p)

(cid:88)

j

where p ∈ RC is a distribution over classes and ∆ is the
simplex as ∆ = {p| (cid:80)
j pj = 1, ∀j, pj ≥ 0}. H(p) de-
notes the entropy of the distribution p.

Proof. According to the K.K.T. condition [1], the distribu-
tion p in Eqn. 4 has the closed-form solution

pj =

exp(λx(cid:62)
j exp(λx(cid:62)

(cid:80)

i (wj − wyi))

i (wj − wyi))

Therefore, we have

(cid:96)SoftMaxnorm (xi) = λ

pjx(cid:62)

i (wj − wyi ) + H(p)

(cid:88)

j

= log(

exp(λx(cid:62)

i (wj − wyi ))) = − log

(cid:88)

j

exp(λw(cid:62)
yi xi)
j exp(λw(cid:62)

j xi)

(cid:80)

Remark 3 Applying the similar analysis to the Prox-
exp(w(cid:62)
xi)
yi
j xi) ,
exp(w(cid:62)

yNCA loss [14]: (cid:96)ProxyNCA(xi) = − log
we have

j(cid:54)=yi

(cid:80)

(4)

(cid:96)ProxyNCA(xi) = max
p∈∆

λ

pjx(cid:62)

i (wj − wyi) + H(p)

(cid:88)

j(cid:54)=yi

where p ∈ RC−1. Compared with the SoftMax loss, it
eliminates the benchmark triplet containing only the cor-
responding class center, which makes the loss unbounded.
Our analysis suggests that the loss can be bounded as in
Eqn. 2: (cid:96)hinge
j xi) ]+. Vali-
dating the bounded loss is out of the scope of this work.

ProxyNCA(xi) = [− log

exp(w(cid:62)
xi)
yi
exp(w(cid:62)

j(cid:54)=yi

(cid:80)

Despite optimizing SoftMax loss can learn the meaning-
ful feature embeddings, the drawback is straightforward.
It assumes that there is only a single center for each class
while a real-world class can contain multiple local clusters
due to the large intra-class variance as in Fig. 1. The triplet
constraints generated by conventional SoftMax loss is too
brief to capture the complex geometry of the original data.
Therefore, we introduce multiple centers for each class.

Remark 1 Proposition 1 indicates that the SoftMax loss
optimizes the triplet constraints consisting of an original ex-
ample and two centers, i.e., (xi, wyi, wj). Compared with
triplet constraints in Eqn. 1, the target of SoftMax loss is

3.1. Multiple Centers

Now, we assume that each class has K centers. Then,
the similarity between the example xi and the class c can
be deﬁned as

∀i, j, x(cid:62)

i wyi − x(cid:62)

i wj ≥ 0

Consequently, the embeddings learned by minimizing Soft-
Max loss can be applicable for the distance-based tasks
while it is designed for the classiﬁcation task.

Remark 2 Without the entropy regularizer, the loss be-
comes

max
p∈∆

λ

(cid:88)

j

pjx(cid:62)

i wj − λx(cid:62)

i wyi

which is equivalent to

max
j

{x(cid:62)

i wj} − xiwyi

Explicitly, it punishes the triplet with the most violation
and becomes zero when the nearest neighbor of xi is the
corresponding center wyi. The entropy regularizer reduces
the inﬂuence from outliers and makes the loss more robust.
λ trades between the hardness of triplets and the regular-
izer. Moreover, minimizing the maximal entropy can make
the distribution concentrated and further push the example
away from irrelevant centers, which implies a large margin
property.

Si,c = max

i wk
x(cid:62)
c

k

(5)

Note that other deﬁnitions of similarity can be applicable for
this scenario (e.g., minz∈RK (cid:107)[w1
c ]z − xi(cid:107)2). We
adopt a simple form to illustrate the inﬂuence of multiple
centers.

c , · · · , wK

With the deﬁnition of the similarity, the triplet constraint
requires an example to be closer to its corresponding class
than other classes

∀j, Si,yi − Si,j ≥ 0

As we mentioned above, minimizing the entropy term H(p)
can help to pull the example to the corresponding center. To
break the tie explicitly, we consider to introduce a small
margin as in the conventional triplet loss in Eqn. 1 and de-
ﬁne the constraints as

∀jj(cid:54)=yi, Si,yi − Si,j ≥ δ

By replacing the similarity in Eqn. 4, we can obtain the

HardTriple loss as

(cid:18) (cid:88)

j(cid:54)=yi

(cid:19)

(cid:96)HardTriple(xi) = max
p∈∆

λ

pj(Si,j − (Si,yi − δ))

+ pyi(Si,yi − δ − (Si,yi − δ))

+ H(p)

= − log

exp(λ(Si,yi − δ))

exp(λ(Si,yi − δ)) + (cid:80)

exp(λSi,j)

j(cid:54)=yi

(6)

HardTriple loss improves the SoftMax loss by providing
multiple centers for each class. However, it requires the max
operator to obtain the nearest center in each class while this
operator is not smooth and the assignment can be sensitive
between multiple centers. Inspired by the SoftMax loss, we
can improve the robustness by smoothing the max operator.

Consider the problem

max
k

i wk
x(cid:62)
c

which is equivalent to

Figure 2. Illustration of differences between SoftMax loss and pro-
posed losses. Compared with the SoftMax loss, we ﬁrst increase
the dimension of the FC layer to include multiple centers for each
class (e.g., 2 centers per class in this example). Then, we obtain
the similarity for each class by different operators. Finally, the
distribution over different classes is computed with the similarity
obtained from each class.

Theorem 1. Given two examples xi and xj that are from
the same class and have the same nearest center and xk
is from a different class, if the triple constant containing
centers is satisﬁed

max
q∈∆

(cid:88)

k

qkx(cid:62)

i wk
c

(7)

x(cid:62)
i wyi − x(cid:62)

i wyk ≥ δ

and we assume ∀i, (cid:107)xi − wyi(cid:107)2 ≤ (cid:15), then we have

we add the entropy regularizer to the distribution q as

i xj − x(cid:62)
x(cid:62)

i xk ≥ δ − 2(cid:15)

max
q∈∆

(cid:88)

k

qkx(cid:62)

i wk

c + γH(q)

Proof.

With a similar analysis as in Proposition 1, q has the closed-
form solution as

qk =

exp( 1
γ x(cid:62)
k exp( 1

i wk
c )
γ x(cid:62)
i wk
c )

(cid:80)

i xk = x(cid:62)

i (xj − wyi) + x(cid:62)

i (xj − wyi ) + x(cid:62)
i (wyk − xk) + δ

x(cid:62)
i xj − x(cid:62)
≥ x(cid:62)
≥ δ − (cid:107)xi(cid:107)2(cid:107)xj − wyi(cid:107)2 − (cid:107)xi(cid:107)2(cid:107)wyk − xk(cid:107)2
= δ − (cid:107)xj − wyi(cid:107)2 − (cid:107)wyk − xk(cid:107)2 ≥ δ − 2(cid:15)

i wyi − x(cid:62)

i xk

Taking it back to the Eqn. 7, we deﬁne the relaxed similarity
between the example xi and the class c as

S (cid:48)

i,c =

exp( 1
γ x(cid:62)
k exp( 1

i wk
c )
γ x(cid:62)
i wk
c )

(cid:80)

(cid:88)

k

i wk
x(cid:62)
c

By applying the smoothed similarity, we deﬁne the Soft-

Triple loss as

(cid:96)SoftTriple(xi)

= − log

exp(λ(S (cid:48)

i,yi
− δ)) + (cid:80)

− δ))

exp(λS (cid:48)

i,j)

j(cid:54)=yi

exp(λ(S (cid:48)

i,yi

(8)

Fig. 2 illustrates the differences between the SoftMax

loss and the proposed losses.

Finally, we will show that the strategy of applying cen-
ters to construct triplet constraints can recover the con-
straints on original triplets.

Theorem 1 demonstrates that optimizing the triplets con-
sisting of centers with a margin δ can reserve the large mar-
gin property on the original triplet constraints. It also im-
plies that more centers can be helpful to reduce the intra-
class variance (cid:15).
In the extreme case that the number of
centers is equal to the number of examples, (cid:15) becomes zero.
However, adding more centers will increase the size of the
last fully connected layer and make the optimization slow
and computation expensive. Besides, it may incur the over-
ﬁtting problem.

Therefore, we have to choose an appropriate number of
centers for each class that can have a small approxima-
tion error while keeping a compact set of centers. We will
demonstrate the strategy in the next subsection.

3.2. Adaptive Number of Centers

Finding an appropriate number of centers for data is
a challenging problem that also appears in unsupervised

learning, e.g., clustering. The number of centers K trades
between the efﬁciency and effectiveness. In conventional
DML algorithms, K equals to the number of original ex-
amples. It makes the number of total triplet constraints up
to cubic of the number of original examples. In SoftMax
loss, K = 1 reduces the number of constraints to be linear
in the number of original examples, which is efﬁcient but
can be ineffective. Without the prior knowledge about the
distribution of each class, it is hard to set K precisely.

Different from the strategy of setting the appropriate K
for each class, we propose to set a sufﬁciently large K and
then encourage similar centers to merge with each other. It
can keep the diversity in the generated centers while shrink-
ing the number of unique centers.

For each center wt

j, we can generate a matrix as

M t

j = [w1

j − wt

j, · · · , wK

j − wt

j](cid:62)

j and wt

If ws
same one such that (cid:107)ws
of the s-th row in the matrix M t
the L2 norm of rows in M t
which can be written as the L2,1 norm

j are similar, they can be collapsed to be the
j(cid:107)2 = 0, which is the L2 norm
j . Therefore, we regularize
j to obtain a sparse set of centers,

j − wt

(cid:107)M t

j (cid:107)2,1 =

(cid:107)ws

j − wt

j(cid:107)2

K
(cid:88)

s

By accumulating L2,1 norm over multiple centers, we

can have the regularizer for the j-th class as

R(w1

j , · · · , wK

j ) =

(cid:107)M t

j (cid:107)2,1

K
(cid:88)

t

Since w has the unit length, the regularizer is simpliﬁed as

R(w1

j , · · · , wK

j ) =

2 − 2ws(cid:62)

j wt

j (9)

K
(cid:88)

K
(cid:88)

(cid:113)

t=1

s=t+1

With the regularizer, our ﬁnal objective becomes

min

(cid:96)SoftTriple(xi) +

1
N

(cid:88)

i

τ (cid:80)C

j , · · · , wK
j )

j R(w1
CK(K − 1)

(10)

where N is the number of total examples.

4. Experiments

We conduct experiments on three benchmark ﬁne-
CUB-2011,
grained visual categorization data sets:
Cars196 and SOP. We follow the settings in other works [3,
14] for the fair comparison. Speciﬁcally, we adopt the In-
ception [25] with the batch normalization [5] as the back-
bone architecture. The parameters of the backbone are ini-
tialized with the model trained on the ImageNet ILSVRC

2012 data set [20] and then ﬁne-tuned on the target data
sets. The images are cropped to 224 × 224 as the input of
the network. During training, only random horizontal mir-
roring and random crop are used as the data augmentation.
A single center crop is taken for test. The model is opti-
mized by Adam with the batch size as 32 and the number
of epochs as 50. The initial learning rates for the backbone
and centers are set to be 1e-4 and 1e-2, respectively. Then,
they are divided by 10 at {20, 40} epochs. Considering that
images in CUB-2011 and Cars196 are similar to those in
ImageNet, we freeze BN on these two data sets and keep
BN training on the rest one. Embeddings of examples and
centers have the unit length in the experiments.

We compare the proposed triplet loss to the normalized
SoftMax loss. The SoftMax loss in Eqn. 3 is denoted as
SoftMaxnorm. We refer the objective in Eqn. 10 as Soft-
Triple. We set τ = 0.2 and γ = 0.1 for SoftTriple. Besides,
we set a small margin as δ = 0.01 to break the tie explicitly.
The number of centers is set to K = 10.

We evaluate the performance of the learned embeddings
from different methods on the tasks of retrieval and clus-
tering. For retrieval task, we use the Recall@k metric as
in [24]. The quality of clustering is measured by the Nor-
malized Mutual Information (NMI) [13]. Given the clus-
tering assignment C = {c1, · · · , cn} and the ground-truth
label Ω = {y1, · · · , yn}, NMI is computed as NMI =
H(Ω)+H(C) , where I(·, ·) measures the mutual information
and H(·) denotes the entropy.

2I(Ω;C)

4.1. CUB-2011

First, we compare the methods on a ﬁne-grained birds
data set CUB-2011 [26]. It consists of 200 species of birds
and 11, 788 images. Following the common practice, we
split the data set as that the ﬁrst 100 classes are used for
training and the rest are used for test. We note that different
works report the results with different dimension of embed-
dings while the size of embeddings has a signiﬁcant impact
on the performance. For fair comparison, we report the re-
sults for the dimension of 64, which is adopted by many
existing methods and the results with 512 feature embed-
dings, which reports the state-of-the-art results on most of
data sets.

Table 1 summarizes the results with 64 embeddings.
Note that Npairs∗ applies the multi-scale test while all other
methods take a single crop test. For SemiHard [21], we
report the result recorded in [23]. First, it is surprising
to observe that the performance of SoftMaxnorm surpasses
that of the existing metric learning methods.
It is poten-
tially due to the fact that SoftMax loss optimizes the rela-
tions of examples as a smoothed triplet loss, which is an-
alyzed in Proposition 1. Second, SoftTriple demonstrates
the best performance among all benchmark methods. Com-
pared to ProxyNCA, SoftTriple improves the state-of-the-

Figure 3. Comparison of the number of unique centers in each class on CUB-2011. The initial number of centers is set to 20.

art performance by 10% on R@1. Besides, it is 2% bet-
ter than SoftMaxnorm. It veriﬁes that SoftMax loss cannot
capture the complex geometry of real-world data set with a
single center for each class. When increasing the number
of centers, SoftTriple can depict the inherent structure of
data better. Finally, both of SoftMax and SoftTriple show
the superior performance compared to existing methods. It
demonstrates that meaningful embeddings can be learned
without a sampling phase.

Table 1. Comparison on CUB-2011. The dimension of the embed-
dings for all methods is 64.

Methods
SemiHard [21]
LiftedStruct [24]
Clustering [23]
Npairs∗ [22]
ProxyNCA [14]
SoftMaxnorm
SoftTriple

R@1 R@2
55.0
42.6
56.6
43.6
61.4
48.2
63.3
51.0
61.9
49.2
70.0
57.8
71.9
60.1

R@4 R@8 NMI
55.4
77.2
66.4
56.5
79.6
68.6
59.2
81.9
71.8
60.4
83.2
74.3
59.5
72.4
67.9
65.3
87.9
80.1
66.2
88.5
81.2

Table 2 compares SoftTriple with 512 embeddings to the
methods with large embeddings. HDC [32] applies the di-
mension as 384. Margin [12] takes 128 dimension of em-
beddings and uses ResNet50 [4] as the backbone. HTL [3]
sets the dimension of embeddings to 512 and reports the
state-of-the-art result on the backbone of Inception. With
the large number of embeddings, it is obvious that all meth-
ods outperform existing DML methods with 64 embeddings
in Table 1.
It is as expected since the high dimensional
space can separate examples better, which is consistent with
the observation in other work [24]. Compared with other
methods, the R@1 of SoftTriple improves more than 8%
over HTL that has the same backbone as SoftTriple. It also
increases R@1 by about 2% over Margin, which applies a
stronger backbone than Inception. It shows that SoftTriple
loss is applicable with large embeddings.

To validate the effect of the proposed regularizer, we
compare the number of unique centers for each class in
Fig. 3. We set a larger number of centers as K = 20 to
make the results explicit and then run SoftTriple with and
without the regularizer in Eqn. 9. Fig. 3 illustrates that the

Table 2. Comparison on CUB-2011 with large embeddings. “-”
means the result is not available.

Methods
HDC [32]
Margin [12]
HTL [3]
SoftMaxnorm
SoftTriple

R@1 R@2 R@4 R@8 NMI
77.0
53.6
83.1
63.6
78.7
57.1
84.3
64.2
84.5
65.4

85.6
90.0
86.5
90.2
90.4

65.7
74.4
68.8
75.6
76.4

-
69.0
-
68.3
69.3

one without regularizer will hold a set of similar centers. In
contrast, SoftTriple with the regularizer can shrink the size
of centers signiﬁcantly and make the optimization effective.
Besides, we demonstrate the R@1 of SoftTriple with
varying the number of centers in Fig. 4. Red line denotes
SoftTriple loss equipped with the regularizer while blue
dashed line has no regularizer. We ﬁnd that when increas-
ing the number of centers from 1 to 10, the performance
of SoftTriple is improved signiﬁcantly, which conﬁrms that
with leveraging multiple centers, the learned embeddings
can capture the data distribution better. If adding more cen-
ters, the performance of SoftTriple almost remains the same
and it shows that the regularizer can help to learn the com-
pact set of centers and will not be inﬂuenced by the initial
number of centers. On the contrary, without the regular-
izer, the blue dashed line illustrates that the performance
will degrade due to overﬁtting when the number of centers
are over-parameterized.

Figure 4. Illustration of SoftTriple with different number of centers
and the inﬂuence of the regularizer. With the proposed regularizer
as denoted by the red line, the performance is stable to the initial
number of centers K when it is sufﬁciently large.

Finally, we illustrate the examples of retrieved images
in Fig. 5. The ﬁrst column indicates the query image. The
columns 2-4 show the most similar images retrieved accord-
ing to the embeddings learned by SoftMaxnorm. The last
four columns are the similar images returned by using the
embeddings from SoftTriple. Evidently, embeddings from
SoftMaxnorm can obtain the meaningful neighbors while
the objective is for classiﬁcation. Besides, SoftTriple im-
proves the performance and can eliminate the images from
different classes among the top of retrieved images, which
are highlighted with red bounding boxes in SoftMaxnorm.

methods in the comparison is the same as described in the
experiments on CUB-2011. On this data set, HTL [3] re-
ports the state-of-the-art result while SoftTriple outperforms
it and increases R@1 by 3%.

Table 4. Comparison on Cars196 with large embeddings.
R@1 R@2 R@4 R@8 NMI
89.5
73.7
91.9
79.6
92.7
81.4
94.0
83.2
94.5
84.5

Methods
HDC [32]
Margin [12]
HTL [3]
SoftMaxnorm
SoftTriple

93.8
95.1
95.7
96.6
96.9

83.2
86.5
88.0
89.5
90.7

-
69.1
-
69.7
70.1

4.3. Stanford Online Products

Finally, we evaluate the performance of different meth-
ods on the Stanford Online Products (SOP) data set [24].
It contains 120, 053 product
images downloaded from
eBay.com and includes 22, 634 classes. We adopt the stan-
dard splitting, where 11, 318 classes are used for training
and the rest for test. Note that each class has about 5 im-
ages, so we set K = 2 for this data set and discard the regu-
larizer. We also increase the initial learning rate for centers
from 0.01 to 0.1.

We ﬁrst report the results with 64 embeddings in Table 5.
In this comparison, SoftMaxnorm is 2% better than Prox-
yNCA on R@1. By simply increasing the number of cen-
ters from 1 to 2, we observe that SoftTriple gains another
0.4% on R@1. It conﬁrms that multiple centers can help to
capture the data structure better.

Table 6 states the performance with large embeddings.
We can get a similar conclusion as in Table 5. Both
SoftMaxnorm and SoftTriple outperform the state-of-the-art
methods. SoftTriple improves the state-of-the-art by more
than 3% on R@1. It demonstrates the advantage of learning
embeddings without sampling triplet constraints.

Table 5. Comparison on SOP. The dimension is 64.

Methods
SemiHard [21]
LiftedStruct [24]
Clustering [23]
ProxyNCA [14]
SoftMaxnorm
SoftTriple

R@1 R@10 R@100 NMI
89.5
66.7
88.7
62.5
89.5
67.0
90.6
73.7
91.5
75.9
91.7
76.3

82.4
80.8
83.7
-
88.8
89.1

91.9
91.9
93.2
-
95.2
95.3

Table 6. Comparison on SOP with large embeddings.
R@1 R@10 R@100 NMI
88.1
67.7
-
69.5
90.7
72.7
-
74.8
91.9
78.0
92.0
78.3

Methods
Npairs∗ [22]
HDC [32]
Margin [12]
HTL [3]
SoftMaxnorm
SoftTriple

93.0
92.8
93.8
94.8
96.0
95.9

83.8
84.4
86.2
88.3
90.2
90.3

Figure 5. Examples of retrieved most similar images with the
learned embeddings from SoftMaxnorm and SoftTriple. The im-
ages from the classes that are different from the query image are
highlighted by red bounding boxes.

4.2. Cars196

Then, we conduct the experiments on Cars196 data
set [6], which contains 196 models of cars and 16, 185 im-
ages. We use the ﬁrst 98 classes for training and the rest
for test. Table 3 summaries the performance with 64 em-
beddings. The observation is similar as for CUB-2011.
SoftMaxnorm shows the superior performance and is 3%
better than ProxyNCA on R@1. Additionally, SoftTriple
can further improve the performance by about 2%, which
demonstrates the effectiveness of the proposed loss func-
tion.

Table 3. Comparison on Cars196. The dimension is 64.

Methods
SemiHard [21]
LiftedStruct [24]
Clustering [23]
Npairs∗ [22]
ProxyNCA [14]
SoftMaxnorm
SoftTriple

R@1 R@2
63.8
51.5
65.7
53.0
70.6
58.1
79.7
71.1
82.4
73.2
85.6
76.8
86.6
78.6

R@4 R@8 NMI
53.4
82.4
73.5
56.9
84.3
76.0
59.0
87.8
80.3
64.0
91.6
86.5
64.9
88.7
86.4
66.7
95.2
91.3
67.0
95.4
91.8

In Table 4, we present the comparison with large dimen-
sion of embeddings. The number of embeddings for all

5. Conclusion

Sampling triplets from a mini-batch of data can degrade
the performance of deep metric learning due to its poor cov-
erage over the whole data set. To address the problem, we
propose the novel SoftTriple loss to learn the embeddings
without sampling. By representing each class with mul-
tiple centers, the loss can be optimized with triplets de-
ﬁned with the similarities between the original examples
and classes. Since centers are encoded in the last fully con-
nected layer, we can learn embeddings with the standard
SGD training pipeline for classiﬁcation and eliminate the
sampling phase. The consistent improvement from Soft-
Triple over ﬁne-grained benchmark data sets conﬁrms the
effectiveness of the proposed loss function. Since SoftMax
loss is prevalently applied for classiﬁcation, SoftTriple loss
can also be applicable for that. Evaluating SoftTriple on the
classiﬁcation task can be our future work.

References

[1] Stephen Boyd and Lieven Vandenberghe. Convex optimiza-

tion. Cambridge university press, 2004. 4

[2] Jiankang Deng, Jia Guo, and Stefanos Zafeiriou. Arc-
face: Additive angular margin loss for deep face recognition.
CoRR, abs/1801.07698, 2018. 2

[3] Weifeng Ge, Weilin Huang, Dengke Dong, and Matthew R.
Scott. Deep metric learning with hierarchical triplet loss. In
ECCV, pages 272–288, 2018. 3, 6, 7, 8

[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
pages 770–778, 2016. 7

[5] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. In ICML, pages 448–456, 2015. 6

[6] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
3d object representations for ﬁne-grained categorization. In
4th International IEEE Workshop on 3D Representation and
Recognition (3dRR-13), Sydney, Australia, 2013. 8

[7] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In NIPS, pages 1106–1114, 2012. 2, 3

[8] Daryl Lim, Gert R. G. Lanckriet, and Brian McFee. Robust
structural metric learning. In ICML, pages 615–623, 2013. 2
[9] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha
Raj, and Le Song. Sphereface: Deep hypersphere embedding
for face recognition. In CVPR, pages 6738–6746, 2017. 2

[10] Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang.
Large-margin softmax loss for convolutional neural net-
works. In ICML, pages 507–516, 2016. 2

[11] Prasanta Chandra Mahalanobis. On the generalized distance

in statistics. National Institute of Science of India, 1936. 1

[12] R. Manmatha, Chao-Yuan Wu, Alexander J. Smola, and
Philipp Kr¨ahenb¨uhl. Sampling matters in deep embedding
learning. In ICCV, pages 2859–2867, 2017. 2, 3, 7, 8

[13] Christopher Manning, Prabhakar Raghavan, and Hinrich
Sch¨utze. Introduction to information retrieval. Natural Lan-
guage Engineering, 16(1):100–103, 2010. 6

[14] Yair Movshovitz-Attias, Alexander Toshev, Thomas K. Le-
ung, Sergey Ioffe, and Saurabh Singh. No fuss distance met-
ric learning using proxies. In ICCV, pages 360–368, 2017.
2, 3, 4, 6, 7, 8

[15] Omkar M. Parkhi, Andrea Vedaldi, and Andrew Zisserman.
Deep face recognition. In BMVC, pages 41.1–41.12, 2015.
1, 2, 3

[16] Qi Qian, Rong Jin, Jinfeng Yi, Lijun Zhang, and Shenghuo
Zhu. Efﬁcient distance metric learning by adaptive sampling
and mini-batch stochastic gradient descent (SGD). Machine
Learning, 99(3):353–372, 2015. 1, 2

[17] Qi Qian, Rong Jin, Shenghuo Zhu, and Yuanqing Lin. Fine-
grained visual categorization via multi-stage metric learning.
In CVPR, pages 3716–3724, 2015. 1, 2

[18] Qi Qian, Jiasheng Tang, Hao Li, Shenghuo Zhu, and Rong
Jin. Large-scale distance metric learning with uncertainty. In
CVPR, pages 8542–8550, 2018. 2, 3

[19] Oren Rippel, Manohar Paluri, Piotr Doll´ar, and Lubomir D.
Bourdev. Metric learning with adaptive density discrimina-
tion. ICLR, 2016. 2, 3

[20] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, Alexander C. Berg, and
Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal-
International Journal of Computer Vision (IJCV),
lenge.
115(3):211–252, 2015. 6

[21] Florian Schroff, Dmitry Kalenichenko, and James Philbin.
Facenet: A uniﬁed embedding for face recognition and clus-
tering. In CVPR, pages 815–823, 2015. 1, 2, 3, 6, 7, 8

[22] Kihyuk Sohn.

Improved deep metric learning with multi-
class n-pair loss objective. In NIPS, pages 1849–1857, 2016.
2, 7, 8

[23] Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, and Kevin
Murphy. Deep metric learning via facility location. In CVPR,
pages 2206–2214, 2017. 6, 7, 8

[24] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio
Savarese. Deep metric learning via lifted structured feature
embedding. In CVPR, pages 4004–4012, 2016. 1, 2, 3, 6, 7,
8

[25] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In CVPR, pages 1–9, 2015. 6

[26] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The Caltech-UCSD Birds-200-2011 Dataset. Technical re-
port, 2011. 6

[27] Feng Wang, Jian Cheng, Weiyang Liu, and Haijun Liu. Ad-
ditive margin softmax for face veriﬁcation. IEEE Signal Pro-
cess. Lett., 25(7):926–930, 2018. 2

[28] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong
Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. Cosface:
Large margin cosine loss for deep face recognition. In CVPR,
pages 5265–5274, 2018. 2

[29] Kilian Q. Weinberger and Lawrence K. Saul. Distance met-
ric learning for large margin nearest neighbor classiﬁcation.
Journal of Machine Learning Research, 10:207–244, 2009.
1, 2

[30] Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A
discriminative feature learning approach for deep face recog-
nition. In ECCV, pages 499–515, 2016. 2

[31] Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, and Stuart J.
Russell. Distance metric learning with application to cluster-
ing with side-information. In NIPS, pages 505–512, 2002. 1,
2

[32] Yuhui Yuan, Kuiyuan Yang, and Chao Zhang. Hard-aware
deeply cascaded embedding. In ICCV, pages 814–823, 2017.
7, 8

