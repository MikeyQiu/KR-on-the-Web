On a Topic Model for Sentences

∗

Georgios Balikas,†

Massih-Reza Amini† and Marianne Clausel‡
University of Grenoble Alpes
†Computer Science Laboratory (LIG), ‡Applied Mathematics Laboratory (LJK)
Grenoble, France
{FirstName.LastName}@imag.fr

6
1
0
2
 
n
u
J
 
1
 
 
]
L
C
.
s
c
[
 
 
1
v
3
5
2
0
0
.
6
0
6
1
:
v
i
X
r
a

ABSTRACT
Probabilistic topic models are generative models that de-
scribe the content of documents by discovering the latent
topics underlying them. However, the structure of the tex-
tual input, and for instance the grouping of words in coher-
ent text spans such as sentences, contains much information
which is generally lost with these models. In this paper, we
propose sentenceLDA, an extension of LDA whose goal is to
overcome this limitation by incorporating the structure of
the text in the generative and inference processes. We illus-
trate the advantages of sentenceLDA by comparing it with
LDA using both intrinsic (perplexity) and extrinsic (text
classiﬁcation) evaluation tasks on diﬀerent text collections.

Keywords
Text Mining; Topic Modeling; Unsupervised Learning

1.

INTRODUCTION

Statistical topic models are generative unsupervised mod-
els that describe the content of documents in large textual
collections. Prior research has investigated the application
of topic models such as Latent Dirichlet Allocation (LDA)
[2] in a variety of domains ranging from image analysis to
political science. Most of the work on topic models assumes
exchangeability between words and treats documents in a
bag-of-words fashion. As a result, the words’ grouping in
coherent text segments, such as sentences or phrases, is lost.
However, the inner structure of documents is generally
useful, when identifying topics. For instance, one would
expect that in each sentence, after standard pre-processing
steps such as stop-word removal, only a very limited number
of latent topics would appear. Thus, we argue that coherent
text segments should pose “constraints” on the amount of
topics that appear inside those segments.

In this paper, we propose sentenceLDA (senLDA), whose
purpose is to incorporate part of the text structure in the
∗Also aﬃliated with: Coﬀreo, Clermont Ferrand, France

β

z

φ

w

K

N

α

θ

D

S

Figure 1: The senLDA model. The words w of a
sentence share the same topic z.

topic model. Motivated by the argument that coherent text
spans should be produced by only a handful of topics, we
propose to modify the generative process of LDA. Hence,
we argue that the latent topics of short text spans should be
consistent across the units of those spans. In our approach,
such text spans can vary from paragraphs to sentences and
phrases depending on the task’s purpose. Also, note that
in the extreme case where words are the coherent text seg-
ments, the standard LDA model becomes a special case of
senLDA.

In the remainder of the paper we present the senLDA and
we derive its collapsed Gibbs sampler in Section 2, we illus-
trate its advantages by comparing it with LDA on intrinsic
(in vitro) and extrinsic (ex vivo) evaluation experiments us-
ing collections of Wikipedia and PubMed articles in Section
3, and we conclude in Section 4.

2. THE PROPOSED MODEL

A statistical topic model represents the words in a col-
lection of D documents as mixtures of K “topics”, which
are multinomials over a vocabulary of size V . In the case of
LDA, for each document di a multinomial over topics is sam-
pled from a Dirichlet prior with parameters α. The probabil-
ity p(w|z = k) of a term w, given the topic k, is represented
by φk,t. We refer to the complete K × V matrix of word-
topic probabilities as Φ. The multinomial parameters φk are
again drawn from a Dirichlet prior parametrized by β. Each
observed term w in the collection is drawn from a multino-
mial for the topic represented by a discrete hidden indicator
variable zi. For simplicity in the mathematical development
and notation, we assume symmetric Dirichlet priors but the
extension to the asymmetric case is straightforward. Hence,
the values of α and β are model hyper-parameters.

We extend LDA by adding an extra plate denoting the co-
herent text segments of a document. In the rest, without loss
of generality we use sentences as coherent segments. A ﬁner
level of granularity can be achieved though, by analysing
the structure of sentences and using phrases as such seg-
ments. The graphical representation of the senLDA model
is shown in Figure 1 and the generative process of a docu-
ment collection using senLDA is described in Algorithm 1.
For inference, we use a collapsed Gibbs sampling method [5].
We now derive the Gibbs sampler equations by estimating
the hidden topic variables.

In senLDA the joint distribution can be factored:

p(w, z|α, β) = p(w|z, β)p(z|α)

(1)

because the ﬁrst term is independent of α and the second
from β. After standard manipulations as in the paradigm of
[6] one arrives at:

p((cid:126)z, (cid:126)w|α, β) =

K
(cid:89)

z=1

∆((cid:126)nz + β)
∆(β)

D
(cid:89)

m=1

∆((cid:126)nm + α)
∆(α)

(2)

(cid:81)dim(cid:126)x
Γ((cid:80)dim(cid:126)x

k=1 Γ(xk)
k=1 xk)

is a multidi-

where ∆((cid:126)x) = Beta(x1, . . . , xm) =
mensional extension of the beta function used for notation
convenience, and (cid:126)nm, (cid:126)nz refer to the occurrences of top-
ics with documents and topics with terms respectively. To
calculate the full conditional we take into account the struc-
ture of the document d and the fact that (cid:126)wd = { (cid:126)wd¬s, (cid:126)w¬s},
(cid:126)z = {(cid:126)zd¬s, (cid:126)z¬s}. The subscript s in (cid:126)ws, (cid:126)zs denotes the words
and the topic respectively of sentence s. For the full condi-
tional of topic k we have:

p(zs = k|(cid:126)z¬s, (cid:126)w) =

p( (cid:126)w, (cid:126)z)
p( (cid:126)w, (cid:126)z¬s)

=

p( (cid:126)w, (cid:126)z)
p( (cid:126)w¬s, (cid:126)z¬s)

∝

=

=

p( (cid:126)w|(cid:126)z)
p( (cid:126)w¬s|(cid:126)z¬s)p(ws)
∆((cid:126)nz + β)
∆((cid:126)nz,¬s + β)

p((cid:126)z)
p((cid:126)z¬s)
∆((cid:126)nm + α)
∆((cid:126)nm,¬s + α)
(3)

For the ﬁrst term of equation Eq. (3) we have:

∆((cid:126)nz + β)
∆((cid:126)nz,¬s + β)

=

(cid:81)
w∈s Γ((cid:126)nz +β)
Γ((cid:80)
w∈s((cid:126)nz +β))
(cid:81)
w∈s Γ((cid:126)nz,¬s+β)
Γ((cid:80)
w∈s((cid:126)nz,¬s+β))

=

=

(cid:89)

w∈s

(

Γ((cid:126)nz + β)
Γ((cid:126)nz,¬s + β)

)

Γ((cid:80)
Γ((cid:80)

w∈s(nz,¬s + β))
w∈s(nz + β))

=

A
(cid:125)(cid:124)
k,¬s + β + (n(w)
k,¬s + β) · · · (n(w)

(cid:123)
k,s − 1))

(n(w)

(cid:122)
(cid:89)

w∈s

w∈V (n(w)

k,¬s + β)) · · · ((cid:80)

k,¬s + β + (N (w)

w∈V n(w)
(cid:123)(cid:122)
B

=

((cid:80)
(cid:124)

k,s − 1))
(cid:125)

(4)

Here, for the generation of A and B we used the recursive
property of the Γ function: Γ(x + m) = (x + m − 1)(x + m −
2) · · · (x + 1)xΓ(x); w is a term that can occur many times
in a sentence and n(w)
k,s denotes w’s frequency in sentence s
given that the sentence s belongs to topic k; N (w)
k,s denotes
how many words of sentence s belong to topic t.

The development of the second factor in the ﬁnal step of
Eq. (3) is similar to the LDA calculations with the diﬀerence
that the counts of topics per document are calculated given

Algorithm 1: Text collection generation with senLDA

for document d ∈ [1, . . . , D] do

sample mixture of topics θm ∼ Dirichlet(a)
sample sentence number Sd ∼ P oisson(ξ)
//Sentence plate
for sentence s ∈ [1, Sd] do

sample number of words Ws ∼ P oisson(ξd)
sample topic zd,s ∼ M ultinomial(θm)
//Word plate in each language
for words w ∈ [1, Wd,s] in sentence s do

sample term for w ∼ M ultinomial(φzd,s )

end

end

end

the allocation of sentences to topics and not the allocation
of words to topics. This yields:

p(zs = k|(cid:126)z¬s, (cid:126)w) = (n(k)
w∈s(n(w)

k,¬s + β) · · · (n(w)

m,¬s + α)×

(cid:81)

((cid:80)

w∈V (n(w)

k,¬s + β)) · · · ((cid:80)

×

k,¬s + β + (n(w)
w∈V n(w)

k,s − 1))
k,¬s + β + (N (w)

k,s − 1))

(5)

where n(w)
m,¬s denotes the number of times that topic k has
been observed with a sentence from document d, excluding
the sentence currently sampled. Note that Eq. (5) reduces
to the standard LDA collapsed Gibbs sampling inference
equations if the coherent text spans are reduced to words.

The idea of integrating the sentence limits in the LDA
model has been previously investigated. For instance, in [9]
in the context of summarization the authors combine the
unigram language model with topic models over sentences
so that the latent topics are represented by sentences instead
of terms. In [4] the notion of sentence topics is introduced
and they are sampled from separate topic distributions and
co-exist with the word topics. Also, Boyd et al. [3] propose
an adaptation of topic models to the text structure obtained
by the parsing tree of a document. Our method resembles
these works in that it integrates the notion of sentences to
extend LDA. In our case though, we directly extend LDA
maintaining the association of words to topics, we retain
its simplicity without adding extra hyper-parameters thus
allowing a fast, gibbs sampling inference, and we do not
require any language-dependent tools such as parsers.

3. EMPIRICAL RESULTS

We conduct experiments to verify the applicability and
evaluate the performance of senLDA compared to LDA. The
process is divided into two steps: (i) the training phase,
where the topic models are trained to learn the their pa-
rameters, and (ii) the inference phase that is for new, unseen
documents their topic distributions are estimated. We use
(5).
the Gibbs sampling inference approach given by Eq.
The hyper-parameters α and β are set to 1
K , with K be-
ing the number of topics. Table 1 shows the datasets we
used. They come from the publicly available collections of
Wikipedia [7] and PubMed [8]. The ﬁrst four datasets (Wik-
iTrain* and PubMedTrain*) were used for learning the topic
model parameters; they diﬀer in their respective size. Also,
the vocabulary of the PubMed datasets is signiﬁcantly larger
due to the medical terms that appear. During preprocessing

Documents

|V |

Classes Timing (sec)

WikiTrain1
WikiTrain2
PubMedTrain1
PubMedTrain2
Wiki37
Wiki46
PubMed25
PubMed50

10,000
30,000
10,000
60,000
2,459
3,657
7,248
9,035

46,051
65,820
55,115
150,440
23,559
27,914
40,173
47,199

-
-
-
-
37
46
25
50

182|271
332|434
304|433
1830|2799
-
-
-
-

Table 1: Description of the data used after pre-
processing. “Timing” refers to the 25 ﬁrst training
iterations with the left (resp. right) values corre-
sponding to senLDA (resp. LDA).

Figure 2: The ratio of perplexities of senLDA and
LDA calculated on Wiki37 and PubMed25.

we only applied lower-casing, stop-word removal and lemma-
tization using the WordNet Lemmatizer.1 The rest of the
document collections of Table 1 are used for classiﬁcation
purposes and are discussed later in the section.
Intrinsic evaluation Topic model evaluation has been the
subject of intense research. For intrinsic evaluation we re-
port here perplexity [1], which is probably the dominant
measure for topic models evaluation in the bibliography.
The perplexity of d held out documents given the model
parameters (cid:126)ϑ is deﬁned as the reciprocal geometric mean of
the token likelihoods of those data, given the parameters of
the model:

p(wheldOut) = exp −

(cid:80)d

(cid:80)wi

i=1

j=1 log p(wi,j|(cid:126)ϑ)
j=1 1

(cid:80)wi

i=1

(cid:80)d

(6)

Note that senLDA samples per sentence and thus results in
less ﬂexibility at the word level where perplexity is calcu-
lated. Even though, the comparison between senLDA and
LDA, at word level using perplexity, gives insights in the
relative merits of the the proposed model.

Figure 2 depicts the ratio of the perplexity values be-
tween senLDA and LDA. We set K = 125 after grid search-
ing K ∈ {25, 75, 125, 175} for perplexity with 5-fold cross-
validation on the training data. Values higher (resp. lower)
than one signify that senLDA achieves lower (resp. higher)
perplexity than LDA. The ﬁgure demonstrates that in the
ﬁrst iterations before convergence of both models, senLDA
performs better. What is more, senLDA converges after

1The code and the data are publicly available at https://
github.com/balikasg/topicModelling/

only around 30 iterations, whereas LDA converges after 160
iterations on Wikipedia and 200 iterations on the PubMed
datasets respectively. We deﬁne convergence as the situ-
ation where the model’s perplexity does not any more de-
crease over training iterations. The shaded area in the ﬁgure
highlights the period while senLDA performs better. It is
to be noted, that although competitive, senLDA does not
outperform LDA given unlimited time resources. However,
that was expected since for senLDA the training instances
are sentences, thus the model’s ﬂexibility is restricted when
evaluated against a word-based measure.

An important diﬀerence between the models however, lies
in the way they converge. From Figure 2 it is clear that
senLDA converges faster. We highlight this by providing
exact timings for the ﬁrst 25 iterations of the models (col-
umn “Timing” of Table 1) on a machine using an Intel Xeon
CPU E5-2643 v3 at 3.40GHz. For both models we use our
own Python implementations with the same speed optimisa-
tions. Using “WikiTrain2” and 125 topics, for 25 iterations
the senLDA needs 332 secs, whereas LDA needs 434 sec.,
an improvement of 30%. Furthermore, comparing the con-
vergence, senLDA needs 332 secs (25 iterations) whereas
LDA needs more than 2770 secs (more than 160 iterations)
making senLDA more than 8 times faster. Similarly for the
“PubMedTrain2” dataset which is more complex due to its
larger vocabulary size, senLDA converges around 12 times
(an order of magnitude) faster. Note that senLDA’s fast
convergence is a strong advantage and can be highly ap-
preciated in diﬀerent application scenarios where unlimited
time resources are not available.
Extrinsic evaluation Previous studies have shown that
perplexity does not always agree with human evaluations
of topic models [1] and it is recommended to evaluate topic
models on real tasks. To better support our development for
senLDA applicability we also evaluate it using text classiﬁca-
tion as the evaluation task. For text classiﬁcation, each doc-
ument is represented by its topic distribution, which is the
vectorial input to Support Vector Machines (SVMs). The
classiﬁcation collections are split on train/test (75%/25%)
parts. The SVM regularization hyper-parameter λ is se-
lected from λ ∈ [10−4, . . . , 104] using 5-fold cross-validation
on the training part of the classiﬁcation data. The PubMed
testsets are multilabel, that is each instance is associated
with several classes, 1.4 in average in the sets of Table 1. For
the multilabel problem with the SVMs we used a binary rel-
evance approach. To assess the classiﬁcation performance,
we report the F1 evaluation measure, which is the harmonic
mean of precision and recall.

The classiﬁcation performance on F1 measure for the dif-
ferent classiﬁcation datasets is shown in Figure 3. First note
that in the majority of the classiﬁcation scenarios, senLDA
outperforms LDA. In most cases, the performance diﬀer-
ence increases when the larger train sets (“WikiTrain2” and
“PubMedTrain2”) are used. For instance, in the second line
of ﬁgures with the PubMed classiﬁcation experiments, in-
creasing the topic models’ training data beneﬁts both LDA
and senLDA , but senLDA still performs better. More im-
portantly though and in consistence with the perplexity ex-
periments, the advantage of senLDA remains: the faster
senLDA convergence beneﬁts the classiﬁcation performance.
The senLDA curves are steeper in the ﬁrst training iterations
and stabilize after roughly 30 iterations when the model con-
verges. We believe that assigning the latent topics to coher-

Figure 3: Classiﬁcation performance on PubMed and Wikipedia text collections using F1 measure.

ent groups of words such as sentences results in document
representations of ﬁner level. In this sense, spans larger than
single words can capture and express the document’s content
more eﬃciently for discriminative tasks like classiﬁcation.

To investigate the correlation of topic model representa-
tions learned on diﬀerent levels of text, we report the classi-
ﬁcation performance using as document representations the
concatenation of a document’s topic distributions output by
LDA and senLDA . For instance, the concatenated vecto-
rial representation of a document when K = 125 for each
model is a vector of 250 dimensions. The resulting concate-
nated representations are denoted by “senLDA+” in Figure
3. As it can be seen, “senLDA+” performs better compared
to both LDA and senLDA . Its performance combines the
advantages of both models: during the ﬁrst iterations it is
as steep as the senLDA representations and in the later it-
erations beneﬁts by the LDA convergence to outperform the
simple senLDA representation. Hence, the concatenation of
the two distributions creates a richer representation where
the two models contribute complementary information that
achieves the best classiﬁcation performance. Achieving the
optimal performance using those representations suggests
that the relaxation of the independence assumptions be-
tween the text structural units can be beneﬁcial; this is also
among the contributions of this work.

4. CONCLUSION

We proposed senLDA, an extension of LDA where top-
ics are sampled per coherent text spans. This resulted in
very fast convergence and good classiﬁcation and perplexity
performance. LDA and senLDA diﬀer in that the second as-
sumes a very strong dependence of the latent topics between
the words of sentences, whereas the ﬁrst assumes indepen-
dence between the words of documents in general. In our
future research, our goal is to investigate this dependence
and further adapt the sampling process of topic models to
cope with the rich text structure.

5. ACKNOWLEDGEMENTS

This work is partially supported by the CIFRE N 28/2015.

6. REFERENCES
[1] L. Azzopardi, M. Girolami, and K. van Risjbergen.

Investigating the relationship between language model
perplexity and IR precision-recall measures. In SIGIR,
pages 369–370, 2003.

[2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent

dirichlet allocation. the Journal of machine Learning
research, 3:993–1022, 2003.

[3] J. L. Boyd-Graber and D. M. Blei. Syntactic topic

models. In Advances in neural information processing
systems, pages 185–192, 2009.

[4] R.-C. Chen, R. Swanson, and A. S. Gordon. An
adaptation of topic modeling to sentences. 2010.
[5] T. L. Griﬃths and M. Steyvers. Finding scientiﬁc
topics. Proceedings of the National Academy of
Sciences, 101(suppl 1):5228–5235, 2004.

[6] G. Heinrich. Parameter estimation for text analysis.

Technical report, Technical report, 2005.

[7] I. Partalas, A. Kosmopoulos, N. Baskiotis, T. Artieres,
G. Paliouras, E. Gaussier, I. Androutsopoulos, M.-R.
Amini, and P. Galinari. LSHTC: A benchmark for
large-scale text classiﬁcation. CoRR, abs/1503.08581,
march 2015.

[8] G. Tsatsaronis, G. Balikas, P. Malakasiotis, I. Partalas,

M. Zschunke, M. R. Alvers, D. Weissenborn, et al. An
overview of the BIOASQ large-scale biomedical
semantic indexing and question answering competition.
BMC bioinformatics, 16(1):1, 2015.

[9] D. Wang, S. Zhu, T. Li, and Y. Gong. Multi-document
summarization using sentence-based topic models. In
Proceedings of the ACL-IJCNLP 2009 Conference Short
Papers, pages 297–300. Association for Computational
Linguistics, 2009.

On a Topic Model for Sentences

∗

Georgios Balikas,†

Massih-Reza Amini† and Marianne Clausel‡
University of Grenoble Alpes
†Computer Science Laboratory (LIG), ‡Applied Mathematics Laboratory (LJK)
Grenoble, France
{FirstName.LastName}@imag.fr

6
1
0
2
 
n
u
J
 
1
 
 
]
L
C
.
s
c
[
 
 
1
v
3
5
2
0
0
.
6
0
6
1
:
v
i
X
r
a

ABSTRACT
Probabilistic topic models are generative models that de-
scribe the content of documents by discovering the latent
topics underlying them. However, the structure of the tex-
tual input, and for instance the grouping of words in coher-
ent text spans such as sentences, contains much information
which is generally lost with these models. In this paper, we
propose sentenceLDA, an extension of LDA whose goal is to
overcome this limitation by incorporating the structure of
the text in the generative and inference processes. We illus-
trate the advantages of sentenceLDA by comparing it with
LDA using both intrinsic (perplexity) and extrinsic (text
classiﬁcation) evaluation tasks on diﬀerent text collections.

Keywords
Text Mining; Topic Modeling; Unsupervised Learning

1.

INTRODUCTION

Statistical topic models are generative unsupervised mod-
els that describe the content of documents in large textual
collections. Prior research has investigated the application
of topic models such as Latent Dirichlet Allocation (LDA)
[2] in a variety of domains ranging from image analysis to
political science. Most of the work on topic models assumes
exchangeability between words and treats documents in a
bag-of-words fashion. As a result, the words’ grouping in
coherent text segments, such as sentences or phrases, is lost.
However, the inner structure of documents is generally
useful, when identifying topics. For instance, one would
expect that in each sentence, after standard pre-processing
steps such as stop-word removal, only a very limited number
of latent topics would appear. Thus, we argue that coherent
text segments should pose “constraints” on the amount of
topics that appear inside those segments.

In this paper, we propose sentenceLDA (senLDA), whose
purpose is to incorporate part of the text structure in the
∗Also aﬃliated with: Coﬀreo, Clermont Ferrand, France

β

z

φ

w

K

N

α

θ

D

S

Figure 1: The senLDA model. The words w of a
sentence share the same topic z.

topic model. Motivated by the argument that coherent text
spans should be produced by only a handful of topics, we
propose to modify the generative process of LDA. Hence,
we argue that the latent topics of short text spans should be
consistent across the units of those spans. In our approach,
such text spans can vary from paragraphs to sentences and
phrases depending on the task’s purpose. Also, note that
in the extreme case where words are the coherent text seg-
ments, the standard LDA model becomes a special case of
senLDA.

In the remainder of the paper we present the senLDA and
we derive its collapsed Gibbs sampler in Section 2, we illus-
trate its advantages by comparing it with LDA on intrinsic
(in vitro) and extrinsic (ex vivo) evaluation experiments us-
ing collections of Wikipedia and PubMed articles in Section
3, and we conclude in Section 4.

2. THE PROPOSED MODEL

A statistical topic model represents the words in a col-
lection of D documents as mixtures of K “topics”, which
are multinomials over a vocabulary of size V . In the case of
LDA, for each document di a multinomial over topics is sam-
pled from a Dirichlet prior with parameters α. The probabil-
ity p(w|z = k) of a term w, given the topic k, is represented
by φk,t. We refer to the complete K × V matrix of word-
topic probabilities as Φ. The multinomial parameters φk are
again drawn from a Dirichlet prior parametrized by β. Each
observed term w in the collection is drawn from a multino-
mial for the topic represented by a discrete hidden indicator
variable zi. For simplicity in the mathematical development
and notation, we assume symmetric Dirichlet priors but the
extension to the asymmetric case is straightforward. Hence,
the values of α and β are model hyper-parameters.

We extend LDA by adding an extra plate denoting the co-
herent text segments of a document. In the rest, without loss
of generality we use sentences as coherent segments. A ﬁner
level of granularity can be achieved though, by analysing
the structure of sentences and using phrases as such seg-
ments. The graphical representation of the senLDA model
is shown in Figure 1 and the generative process of a docu-
ment collection using senLDA is described in Algorithm 1.
For inference, we use a collapsed Gibbs sampling method [5].
We now derive the Gibbs sampler equations by estimating
the hidden topic variables.

In senLDA the joint distribution can be factored:

p(w, z|α, β) = p(w|z, β)p(z|α)

(1)

because the ﬁrst term is independent of α and the second
from β. After standard manipulations as in the paradigm of
[6] one arrives at:

p((cid:126)z, (cid:126)w|α, β) =

K
(cid:89)

z=1

∆((cid:126)nz + β)
∆(β)

D
(cid:89)

m=1

∆((cid:126)nm + α)
∆(α)

(2)

(cid:81)dim(cid:126)x
Γ((cid:80)dim(cid:126)x

k=1 Γ(xk)
k=1 xk)

is a multidi-

where ∆((cid:126)x) = Beta(x1, . . . , xm) =
mensional extension of the beta function used for notation
convenience, and (cid:126)nm, (cid:126)nz refer to the occurrences of top-
ics with documents and topics with terms respectively. To
calculate the full conditional we take into account the struc-
ture of the document d and the fact that (cid:126)wd = { (cid:126)wd¬s, (cid:126)w¬s},
(cid:126)z = {(cid:126)zd¬s, (cid:126)z¬s}. The subscript s in (cid:126)ws, (cid:126)zs denotes the words
and the topic respectively of sentence s. For the full condi-
tional of topic k we have:

p(zs = k|(cid:126)z¬s, (cid:126)w) =

p( (cid:126)w, (cid:126)z)
p( (cid:126)w, (cid:126)z¬s)

=

p( (cid:126)w, (cid:126)z)
p( (cid:126)w¬s, (cid:126)z¬s)

∝

=

=

p( (cid:126)w|(cid:126)z)
p( (cid:126)w¬s|(cid:126)z¬s)p(ws)
∆((cid:126)nz + β)
∆((cid:126)nz,¬s + β)

p((cid:126)z)
p((cid:126)z¬s)
∆((cid:126)nm + α)
∆((cid:126)nm,¬s + α)
(3)

For the ﬁrst term of equation Eq. (3) we have:

∆((cid:126)nz + β)
∆((cid:126)nz,¬s + β)

=

(cid:81)
w∈s Γ((cid:126)nz +β)
Γ((cid:80)
w∈s((cid:126)nz +β))
(cid:81)
w∈s Γ((cid:126)nz,¬s+β)
Γ((cid:80)
w∈s((cid:126)nz,¬s+β))

=

=

(cid:89)

w∈s

(

Γ((cid:126)nz + β)
Γ((cid:126)nz,¬s + β)

)

Γ((cid:80)
Γ((cid:80)

w∈s(nz,¬s + β))
w∈s(nz + β))

=

A
(cid:125)(cid:124)
k,¬s + β + (n(w)
k,¬s + β) · · · (n(w)

(cid:123)
k,s − 1))

(n(w)

(cid:122)
(cid:89)

w∈s

w∈V (n(w)

k,¬s + β)) · · · ((cid:80)

k,¬s + β + (N (w)

w∈V n(w)
(cid:123)(cid:122)
B

=

((cid:80)
(cid:124)

k,s − 1))
(cid:125)

(4)

Here, for the generation of A and B we used the recursive
property of the Γ function: Γ(x + m) = (x + m − 1)(x + m −
2) · · · (x + 1)xΓ(x); w is a term that can occur many times
in a sentence and n(w)
k,s denotes w’s frequency in sentence s
given that the sentence s belongs to topic k; N (w)
k,s denotes
how many words of sentence s belong to topic t.

The development of the second factor in the ﬁnal step of
Eq. (3) is similar to the LDA calculations with the diﬀerence
that the counts of topics per document are calculated given

Algorithm 1: Text collection generation with senLDA

for document d ∈ [1, . . . , D] do

sample mixture of topics θm ∼ Dirichlet(a)
sample sentence number Sd ∼ P oisson(ξ)
//Sentence plate
for sentence s ∈ [1, Sd] do

sample number of words Ws ∼ P oisson(ξd)
sample topic zd,s ∼ M ultinomial(θm)
//Word plate in each language
for words w ∈ [1, Wd,s] in sentence s do

sample term for w ∼ M ultinomial(φzd,s )

end

end

end

the allocation of sentences to topics and not the allocation
of words to topics. This yields:

p(zs = k|(cid:126)z¬s, (cid:126)w) = (n(k)
w∈s(n(w)

k,¬s + β) · · · (n(w)

m,¬s + α)×

(cid:81)

((cid:80)

w∈V (n(w)

k,¬s + β)) · · · ((cid:80)

×

k,¬s + β + (n(w)
w∈V n(w)

k,s − 1))
k,¬s + β + (N (w)

k,s − 1))

(5)

where n(w)
m,¬s denotes the number of times that topic k has
been observed with a sentence from document d, excluding
the sentence currently sampled. Note that Eq. (5) reduces
to the standard LDA collapsed Gibbs sampling inference
equations if the coherent text spans are reduced to words.

The idea of integrating the sentence limits in the LDA
model has been previously investigated. For instance, in [9]
in the context of summarization the authors combine the
unigram language model with topic models over sentences
so that the latent topics are represented by sentences instead
of terms. In [4] the notion of sentence topics is introduced
and they are sampled from separate topic distributions and
co-exist with the word topics. Also, Boyd et al. [3] propose
an adaptation of topic models to the text structure obtained
by the parsing tree of a document. Our method resembles
these works in that it integrates the notion of sentences to
extend LDA. In our case though, we directly extend LDA
maintaining the association of words to topics, we retain
its simplicity without adding extra hyper-parameters thus
allowing a fast, gibbs sampling inference, and we do not
require any language-dependent tools such as parsers.

3. EMPIRICAL RESULTS

We conduct experiments to verify the applicability and
evaluate the performance of senLDA compared to LDA. The
process is divided into two steps: (i) the training phase,
where the topic models are trained to learn the their pa-
rameters, and (ii) the inference phase that is for new, unseen
documents their topic distributions are estimated. We use
(5).
the Gibbs sampling inference approach given by Eq.
The hyper-parameters α and β are set to 1
K , with K be-
ing the number of topics. Table 1 shows the datasets we
used. They come from the publicly available collections of
Wikipedia [7] and PubMed [8]. The ﬁrst four datasets (Wik-
iTrain* and PubMedTrain*) were used for learning the topic
model parameters; they diﬀer in their respective size. Also,
the vocabulary of the PubMed datasets is signiﬁcantly larger
due to the medical terms that appear. During preprocessing

Documents

|V |

Classes Timing (sec)

WikiTrain1
WikiTrain2
PubMedTrain1
PubMedTrain2
Wiki37
Wiki46
PubMed25
PubMed50

10,000
30,000
10,000
60,000
2,459
3,657
7,248
9,035

46,051
65,820
55,115
150,440
23,559
27,914
40,173
47,199

-
-
-
-
37
46
25
50

182|271
332|434
304|433
1830|2799
-
-
-
-

Table 1: Description of the data used after pre-
processing. “Timing” refers to the 25 ﬁrst training
iterations with the left (resp. right) values corre-
sponding to senLDA (resp. LDA).

Figure 2: The ratio of perplexities of senLDA and
LDA calculated on Wiki37 and PubMed25.

we only applied lower-casing, stop-word removal and lemma-
tization using the WordNet Lemmatizer.1 The rest of the
document collections of Table 1 are used for classiﬁcation
purposes and are discussed later in the section.
Intrinsic evaluation Topic model evaluation has been the
subject of intense research. For intrinsic evaluation we re-
port here perplexity [1], which is probably the dominant
measure for topic models evaluation in the bibliography.
The perplexity of d held out documents given the model
parameters (cid:126)ϑ is deﬁned as the reciprocal geometric mean of
the token likelihoods of those data, given the parameters of
the model:

p(wheldOut) = exp −

(cid:80)d

(cid:80)wi

i=1

j=1 log p(wi,j|(cid:126)ϑ)
j=1 1

(cid:80)wi

i=1

(cid:80)d

(6)

Note that senLDA samples per sentence and thus results in
less ﬂexibility at the word level where perplexity is calcu-
lated. Even though, the comparison between senLDA and
LDA, at word level using perplexity, gives insights in the
relative merits of the the proposed model.

Figure 2 depicts the ratio of the perplexity values be-
tween senLDA and LDA. We set K = 125 after grid search-
ing K ∈ {25, 75, 125, 175} for perplexity with 5-fold cross-
validation on the training data. Values higher (resp. lower)
than one signify that senLDA achieves lower (resp. higher)
perplexity than LDA. The ﬁgure demonstrates that in the
ﬁrst iterations before convergence of both models, senLDA
performs better. What is more, senLDA converges after

1The code and the data are publicly available at https://
github.com/balikasg/topicModelling/

only around 30 iterations, whereas LDA converges after 160
iterations on Wikipedia and 200 iterations on the PubMed
datasets respectively. We deﬁne convergence as the situ-
ation where the model’s perplexity does not any more de-
crease over training iterations. The shaded area in the ﬁgure
highlights the period while senLDA performs better. It is
to be noted, that although competitive, senLDA does not
outperform LDA given unlimited time resources. However,
that was expected since for senLDA the training instances
are sentences, thus the model’s ﬂexibility is restricted when
evaluated against a word-based measure.

An important diﬀerence between the models however, lies
in the way they converge. From Figure 2 it is clear that
senLDA converges faster. We highlight this by providing
exact timings for the ﬁrst 25 iterations of the models (col-
umn “Timing” of Table 1) on a machine using an Intel Xeon
CPU E5-2643 v3 at 3.40GHz. For both models we use our
own Python implementations with the same speed optimisa-
tions. Using “WikiTrain2” and 125 topics, for 25 iterations
the senLDA needs 332 secs, whereas LDA needs 434 sec.,
an improvement of 30%. Furthermore, comparing the con-
vergence, senLDA needs 332 secs (25 iterations) whereas
LDA needs more than 2770 secs (more than 160 iterations)
making senLDA more than 8 times faster. Similarly for the
“PubMedTrain2” dataset which is more complex due to its
larger vocabulary size, senLDA converges around 12 times
(an order of magnitude) faster. Note that senLDA’s fast
convergence is a strong advantage and can be highly ap-
preciated in diﬀerent application scenarios where unlimited
time resources are not available.
Extrinsic evaluation Previous studies have shown that
perplexity does not always agree with human evaluations
of topic models [1] and it is recommended to evaluate topic
models on real tasks. To better support our development for
senLDA applicability we also evaluate it using text classiﬁca-
tion as the evaluation task. For text classiﬁcation, each doc-
ument is represented by its topic distribution, which is the
vectorial input to Support Vector Machines (SVMs). The
classiﬁcation collections are split on train/test (75%/25%)
parts. The SVM regularization hyper-parameter λ is se-
lected from λ ∈ [10−4, . . . , 104] using 5-fold cross-validation
on the training part of the classiﬁcation data. The PubMed
testsets are multilabel, that is each instance is associated
with several classes, 1.4 in average in the sets of Table 1. For
the multilabel problem with the SVMs we used a binary rel-
evance approach. To assess the classiﬁcation performance,
we report the F1 evaluation measure, which is the harmonic
mean of precision and recall.

The classiﬁcation performance on F1 measure for the dif-
ferent classiﬁcation datasets is shown in Figure 3. First note
that in the majority of the classiﬁcation scenarios, senLDA
outperforms LDA. In most cases, the performance diﬀer-
ence increases when the larger train sets (“WikiTrain2” and
“PubMedTrain2”) are used. For instance, in the second line
of ﬁgures with the PubMed classiﬁcation experiments, in-
creasing the topic models’ training data beneﬁts both LDA
and senLDA , but senLDA still performs better. More im-
portantly though and in consistence with the perplexity ex-
periments, the advantage of senLDA remains: the faster
senLDA convergence beneﬁts the classiﬁcation performance.
The senLDA curves are steeper in the ﬁrst training iterations
and stabilize after roughly 30 iterations when the model con-
verges. We believe that assigning the latent topics to coher-

Figure 3: Classiﬁcation performance on PubMed and Wikipedia text collections using F1 measure.

ent groups of words such as sentences results in document
representations of ﬁner level. In this sense, spans larger than
single words can capture and express the document’s content
more eﬃciently for discriminative tasks like classiﬁcation.

To investigate the correlation of topic model representa-
tions learned on diﬀerent levels of text, we report the classi-
ﬁcation performance using as document representations the
concatenation of a document’s topic distributions output by
LDA and senLDA . For instance, the concatenated vecto-
rial representation of a document when K = 125 for each
model is a vector of 250 dimensions. The resulting concate-
nated representations are denoted by “senLDA+” in Figure
3. As it can be seen, “senLDA+” performs better compared
to both LDA and senLDA . Its performance combines the
advantages of both models: during the ﬁrst iterations it is
as steep as the senLDA representations and in the later it-
erations beneﬁts by the LDA convergence to outperform the
simple senLDA representation. Hence, the concatenation of
the two distributions creates a richer representation where
the two models contribute complementary information that
achieves the best classiﬁcation performance. Achieving the
optimal performance using those representations suggests
that the relaxation of the independence assumptions be-
tween the text structural units can be beneﬁcial; this is also
among the contributions of this work.

4. CONCLUSION

We proposed senLDA, an extension of LDA where top-
ics are sampled per coherent text spans. This resulted in
very fast convergence and good classiﬁcation and perplexity
performance. LDA and senLDA diﬀer in that the second as-
sumes a very strong dependence of the latent topics between
the words of sentences, whereas the ﬁrst assumes indepen-
dence between the words of documents in general. In our
future research, our goal is to investigate this dependence
and further adapt the sampling process of topic models to
cope with the rich text structure.

5. ACKNOWLEDGEMENTS

This work is partially supported by the CIFRE N 28/2015.

6. REFERENCES
[1] L. Azzopardi, M. Girolami, and K. van Risjbergen.

Investigating the relationship between language model
perplexity and IR precision-recall measures. In SIGIR,
pages 369–370, 2003.

[2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent

dirichlet allocation. the Journal of machine Learning
research, 3:993–1022, 2003.

[3] J. L. Boyd-Graber and D. M. Blei. Syntactic topic

models. In Advances in neural information processing
systems, pages 185–192, 2009.

[4] R.-C. Chen, R. Swanson, and A. S. Gordon. An
adaptation of topic modeling to sentences. 2010.
[5] T. L. Griﬃths and M. Steyvers. Finding scientiﬁc
topics. Proceedings of the National Academy of
Sciences, 101(suppl 1):5228–5235, 2004.

[6] G. Heinrich. Parameter estimation for text analysis.

Technical report, Technical report, 2005.

[7] I. Partalas, A. Kosmopoulos, N. Baskiotis, T. Artieres,
G. Paliouras, E. Gaussier, I. Androutsopoulos, M.-R.
Amini, and P. Galinari. LSHTC: A benchmark for
large-scale text classiﬁcation. CoRR, abs/1503.08581,
march 2015.

[8] G. Tsatsaronis, G. Balikas, P. Malakasiotis, I. Partalas,

M. Zschunke, M. R. Alvers, D. Weissenborn, et al. An
overview of the BIOASQ large-scale biomedical
semantic indexing and question answering competition.
BMC bioinformatics, 16(1):1, 2015.

[9] D. Wang, S. Zhu, T. Li, and Y. Gong. Multi-document
summarization using sentence-based topic models. In
Proceedings of the ACL-IJCNLP 2009 Conference Short
Papers, pages 297–300. Association for Computational
Linguistics, 2009.

