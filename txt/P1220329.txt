Variational zero-inﬂated Gaussian processes with sparse kernels

8
1
0
2
 
r
a

M
 
3
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
6
3
0
5
0
.
3
0
8
1
:
v
i
X
r
a

Pashupati Hegde

Markus Heinonen

Samuel Kaski

Helsinki Institute for Information Technology HIIT
Department of Computer Science, Aalto University

Abstract

Zero-inﬂated datasets, which have an ex-
cess of zero outputs, are commonly en-
countered in problems such as climate or
rare event modelling. Conventional ma-
chine learning approaches tend to overesti-
mate the non-zeros leading to poor perfor-
mance. We propose a novel model family
of zero-inﬂated Gaussian processes (ZiGP)
for such zero-inﬂated datasets, produced
by sparse kernels through learning a la-
tent probit Gaussian process that can zero
out kernel rows and columns whenever the
signal is absent. The ZiGPs are particu-
larly useful for making the powerful Gaus-
sian process networks more interpretable.
We introduce sparse GP networks where
variable-order latent modelling is achieved
through sparse mixing signals. We derive
the non-trivial stochastic variational infer-
ence tractably for scalable learning of the
sparse kernels in both models. The novel
output-sparse approach improves both pre-
diction of zero-inﬂated data and inter-
pretability of latent mixing models.

1

INTRODUCTION

Zero-inﬂated quantitative datasets with overabun-
dance of zero output observations are common in
many domains, such as climate and earth sciences
(Enke and Spekat, 1997; Wilby, 1998; Charles et al.,
2004), ecology (del Saz-Salazar and Rausell-K¨oster,
2008; Ancelet et al., 2009), social sciences (Bohn-
ing et al., 1997), and in count processes (Barry and
Welsh, 2002). Traditional regression modelling of
such data tends to underestimate zeros and overes-
timate nonzeros (Andersen et al., 2014).

A conventional way of forming zero-inﬂated mod-
els is to estimate a mixture of a Bernoulli “on-oﬀ”
process and a Poisson count distribution (Johnson
and Kotz, 1969; Lambert, 1992). In hurdle models
a binary “on-oﬀ” process determines whether a hur-
dle is crossed, and the positive responses are gov-
erned by a subsequent process (Cragg, 1971; Mul-
lahy, 1986). The hurdle model is analogous to ﬁrst
performing classiﬁcation and training a continuous
predictor on the positive values only, while the zero-
inﬂated model would regress with all observations.
Both stages can be combined for simultaneous clas-
siﬁcation and regression Abraham and Tan (2010).

Gaussian process models have not been proposed
for zero-inﬂated datasets since their posteriors are
Gaussian, which are ill-ﬁtted for zero predictions.
A suite of Gaussian process models have been pro-
posed for partially related problems, such as mixture
models (Tresp, 2001; Rasmussen and Ghahramani,
2002; L´azaro-Gredilla et al., 2012) and change point
detection (Herlands et al., 2016). Structured spike-
and-slab models place smoothly sparse priors over
the structured inputs (Andersen et al., 2014).

In contrast to other approaches, we propose a
Bayesian model that learns the underlying latent
prediction function, whose covariance is sparsiﬁed
through another Gaussian process switching be-
tween the ‘on’ and ‘oﬀ’ states, resulting in an zero-
inﬂated Gaussian process model. This approach
introduces a tendency of predicting exact zeros
to Gaussian processes, which is directly useful in
datasets with excess zeros.

A Gaussian process network (GPRN) is a latent
signal framework where multi-output data are ex-
plained through a set of latent signals and mixing
weight Gaussian processes (Wilson et al., 2012). The
standard GPRN tends to have dense mixing that
combines all latent signals for all latent outputs. By

applying the zero-predicting Gaussian processes to
latent mixture models, we introduce sparse GPRNs
where latent signals are mixed with sparse instead
of dense mixing weight functions. The sparse model
induces variable-order mixtures of latent signals re-
sulting in simpler and more interpretable models.
We demonstrate both of these properties in our ex-
periments with spatio-temporal and multi-output
datasets.

Main contributions. Our contributions include

1. A novel zero-inﬂated Gaussian process formal-
ism consisting of a latent Gaussian process and
a separate ‘on-oﬀ’ probit-linked Gaussian pro-
cess that can zero out rows and columns of the
model covariance. The novel sparse kernel adds
to GPs the ability to predict zeros.

2. Novel stochastic variational inference (SVI) for
such sparse probit covariances, which in gen-
eral are intractable due to having to compute
expectations of GP covariances with respect to
probit-linked processes. We derive the SVI for
learning both of the underlying processes.

3. A solution to the stochastic variational infer-
ence for conventional Gaussian process net-
works (GPRN) improving the earlier diago-
nalized mean-ﬁeld approximation (Nguyen and
Bonilla, 2013) by taking the covariances fully
into account.

4. A novel sparse GPRN with an on-oﬀ process
in the mixing matrices leading to sparse and
variable-order mixtures of latent signals.

5. A solution to the stochastic variational infer-
ence of sparse GPRN where the SVI is derived
for the network of full probit-linked covariances.

TensorFlow
these methods

implementation
Python
The
of
at
is
github.com/hegdepashupati/zero-inflated-gp
and at github.com/hegdepashupati/gprn-svi.

available

publicly

2 GAUSSIAN PROCESSES

We begin by introducing the basics of conventional
Gaussian processes. Gaussian processes (GP) are a
family of non-parametric, non-linear Bayesian mod-
els (Rasmussen and Williams, 2006). Assume a
dataset of n inputs X = (x1, . . . , xn) with xi ∈ RD
and noisy outputs y = (y1, . . . , yn) ∈ Rn. The ob-
servations y = f (x)+ε are assumed to have additive,

Figure 1: Illustration of a zero-inﬂated GP (a) and
standard GP regression (b). The standard approach
is unable to model sudden loss of signal (at 4 . . . 5)
and signal close to zero (at 0 . . . 1 and 7 . . . 9).

zero mean noise ε ∼ N (0, σ2
prior on the latent function f (x),

y) with a zero-mean GP

f (x) ∼ GP (0, K(x, x(cid:48))) ,

(1)

which deﬁnes a distribution over functions f (x)
whose mean and covariance are

E[f (x)] = 0
cov[f (x), f (x(cid:48))] = K(x, x(cid:48)).

(2)

(3)

Then for any collection of inputs X, the function
values follow a multivariate normal distribution f ∼
N (0, KXX ), where f = (f (x1), . . . , f (xN ))T ∈ Rn,
and where KXX ∈ Rn×n with [KXX ]ij = K(xi, xj).
The key property of Gaussian processes is that they
encode functions that predict similar output values
f (x), f (x(cid:48)) for similar inputs x, x(cid:48), with similarity
determined by the kernel K(x, x(cid:48)). In this paper we
assume the Gaussian ARD kernel

K(x, x(cid:48)) = σ2

f exp

−



1
2

D
(cid:88)

j=1

j)2

(xj − x(cid:48)
(cid:96)2
j



 ,

(4)

with a signal variance σ2
lengthscale (cid:96)1, . . . , (cid:96)D parameters.

f and dimension-speciﬁc

inference of

The
θ =
(σy, σf , (cid:96)1, . . . , (cid:96)D) is performed commonly by max-
imizing the marginal likelihood

the hyperparameters

(cid:90)

p(y|θ) =

p(y|f )p(f |θ)df ,

(5)

which results in a convenient marginal likelihood
called evidence, p(y|θ) = N (y|0, KXX + σ2
yI) for
a Gaussian likelihood.

The Gaussian process deﬁnes a univariate nor-
mal predictive posterior distribution f (x)|y, X ∼
N (µ(x), σ2(x)) for an arbitrary input x with the
prediction mean and variance1

where

µ(x) = KxX (KXX + σ2
yI)−1y,
σ2(x) = Kxx − KxX (KXX + σ2

yI)−1KXx,

(6)

(7)

xX ∈ Rn is the kernel column vector
where KXx = K T
over pairs X ×x, and Kxx = K(x, x) ∈ R is a scalar.
The predictions µ(x) ± σ(x) come with uncertainty
estimates in GP regression.

3 ZERO-INFLATED GAUSSIAN

PROCESSES

Figure 2:
Illustration of the zero-inﬂated GP (a)
and the sparse kernel (b) composed of a smooth
latent function (c,d) ﬁltered by a probit support
function (e,f ), which is induced by the underlying
latent sparsity (g,h).

We introduce zero-inﬂated Gaussian processes that
have – in contrast to standard GP’s – a tendency to
produce exactly zero predictions (See Figure 1). Let
g(x) denote the latent “on-oﬀ” state of a function
f (x). We assume GP priors for both functions with
a joint model

p(y, f , g) = p(y|f )p(f |g)p(g),

(8)

1In the following we omit the implicit conditioning on

data inputs X for clarity.

p(y|f ) = N (y|f , σ2
p(f |g) = N (f |0, Φ(g)Φ(g)T ◦ Kf )

yI)

p(g) = N (g|β1, Kg).

(9)

(10)

(11)

The sparsity values g(x) are squashed between 0 and
1 through a standard Normal cumulative distribu-
tion, or a probit link function, Φ : R → [0, 1]

Φ(g) =

φ(τ )dτ =

1 + erf

,

(12)

(cid:90) g

−∞

(cid:18)

1
2

(cid:19)(cid:19)

(cid:18) g
√
2

2 τ 2

e− 1

where φ(τ ) = 1√
is the standard normal den-
2π
sity function. The structured probit sparsity Φ(g)
models the “on-oﬀ” smoothly due to the latent spar-
sity function g having a GP prior with prior mean
β. The latent function f is modeled throughout but
it is only visible during the “on” states. This mask-
ing eﬀect has similarities to both zero-inﬂated and
hurdle models. The underlying latent function f is
learned from only non-zero data similarly to in hur-
dle models, but the function f is allowed to predict
zeros similarly to zero-inﬂated models.

is the sparse probit-
The key part of our model
sparsiﬁed covariance Φ(g)Φ(g)T ◦ K where the “on-
oﬀ” state Φ(g) has the ability to zero out rows and
columns of the kernel matrix at the “oﬀ” states (See
Figure 2f for the probit pattern Φ(g)Φ(g)T and Fig-
ure 2b for the resulting sparse kernel). As the spar-
sity g(x) converges towards minus inﬁnity, the probit
link Φ(g(x)) approaches zero, which leads the func-
tion distribution approaching N (fi|0, 0), or fi = 0.
Numerical problems are avoided since in practice
Φ(g) > 0, and due to the conditioning noise vari-
ance term σ2

y > 0.

The marginal likelihood of the zero-inﬂated Gaus-
sian process is intractable due to the probit-
sparsiﬁcation of the kernel. We derive a stochastic
variational Bayes approximation, which we show to
be tractable due to the choice of using the probit
link function.

3.1 STOCHASTIC VARIATIONAL

INFERENCE

Inference for standard Gaussian process models is
diﬃcult to scale as complexity grows with O(n3) as
a function of the data size n. Titsias (2009) pro-
posed a variational inference approach for GPs using
m < n inducing variables, with a reduced computa-
tional complexity of O(m3) for m inducing points.
The novelty of this approach lies in the idea that

the locations and values of inducing points can be
treated as variational parameters, and optimized.
Hensman et al. (2013, 2015) introduced more eﬃ-
cient stochastic variational inference (SVI) with fac-
torised likelihoods that has been demonstrated with
up to billion data points (Salimbeni and Deisenroth,
2017). This approach cannot be directly applied to
sparse kernels due to having to compute expectation
of the probit product in the covariance. We derive
the SVI bound tractably for the zero-inﬂated model
and its sparse kernel, which is necessary in order to
apply the eﬃcient parameter estimation techniques
with automatic diﬀerentiation with frameworks such
as TensorFlow (Abadi et al., 2016).

We begin by applying the inducing point augmen-
tations f (zf ) = uf and g(zg) = ug for both
the latent function f (·) and the sparsity function
g(·). We place m inducing points uf 1, . . . uf m
and ug1, . . . ugm for the two functions. The aug-
mented joint distribution is p(y, f , g, uf , ug) =
p(y|f )p(f |g, uf )p(g|ug)p(uf )p(ug), where2

We minimize the Kullback-Leibler divergence be-
tween the true augmented posterior p(y, f , g, uf , ug)
and the variational distribution q(f , g, uf , ug),
which is equivalent to solving the following evi-
dence lower bound (as shown by e.g. Hensman et al.
(2015)):

log p(y) ≥ Eq(f ) log p(y|f ) − KL[q(uf , ug)||p(uf , ug)],
(24)

q(f ) =

p(f |g, uf )q(uf )p(g|ug)q(ug)duf dugdg

where we deﬁne

(cid:90) (cid:90) (cid:90)

(cid:90)

=

q(f |g)q(g)dg,

where the variational approximations are tractably

(cid:90)

(cid:90)

q(g) =

p(g|ug)q(ug)dug

= N (g|µg, Σg)

q(f |g) =

p(f |g, uf )q(uf )duf

p(f |g, uf ) = N (f | diag(Φ(g))Qf uf , Φ(g)Φ(g)T ◦ (cid:101)Kf )
(13)

with

= N (f | diag(Φ(g))µf , Φ(g)Φ(g)T ◦ Σf )

(14)

(15)

(16)

(17)

(18)

(19)

(20)

p(g|ug) = N (g|Qgug, (cid:101)Kg)
p(uf ) = N (uf |0, Kf mm)
p(ug) = N (ug|0, Kgmm)

and where

f mm

Qf = Kf nmK −1
Qg = KgnmK −1
(cid:101)Kf = Kf nn − Kf nmK −1
(cid:101)Kg = Kgnn − KgnmK −1

gmm

f mmKf mn
gmmKgmn.

We denote the kernels for functions f and g by the
corresponding subscripts. The kernel Kf nn is be-
tween all n data points, the kernel Kf nm is between
all n datapoints and m inducing points, and the ker-
nel Kf mm is between all m inducing points (similarly
for g as well).

Next we use the standard variational approach by
introducing approximative variational distributions
for the inducing points,

q(uf ) = N (uf |mf , Sf )
q(ug) = N (ug|mg, Sg)
where Sf , Sg ∈ Rm×m are square positive semi-
deﬁnite matrices. The variational joint posterior is

(22)

(21)

q(f , g, uf , ug) = p(f |g, uf )p(g|ug)q(uf )q(ug). (23)

2We drop the implicit conditioning on z’s for clarity.

µf = Qf mf
µg = Qgmg
Σf = Kf nn + Qf (Sf − Kf mm)QT
f
Σg = Kgnn + Qg(Sg − Kgmm)QT
g .

We additionally assume the likelihood p(y|f ) =
(cid:81)N
i=1 p(yi|fi) factorises. We solve the ﬁnal ELBO
of equations (24) and (25) as (See Supplements for
detailed derivation)

N
(cid:88)

(cid:110)

i=1

−

1
2σ2
y

LZI =

log N (yi|(cid:104)Φ(gi)(cid:105)q(gi)µf i, σ2
y)

(cid:0)Var[Φ(gi)]µ2

f i + (cid:104)Φ(gi)2(cid:105)q(gi)σ2
f i

− KL[q(uf )||p(uf )] − KL[q(ug)||p(ug)],

where µf i is the i’th element of µf and σ2
f i is the
i’th diagonal element of Σf (similarly with g). The
expectations are tractable,

(cid:104)Φ(gi)(cid:105)q(gi) = Φ(λgi),

λgi =

(cid:113)

(cid:104)Φ(gi)2(cid:105)q(gi) = Φ(λgi) − 2T

λgi,

Var[Φ(gi)] = Φ(λgi) − 2T

λgi,

− Φ(λgi)2.

µgi
1 + σ2
gi
(cid:19)

λgi
µgi
λgi
µgi

(cid:19)

(cid:18)

(cid:18)

(25)

(26)

(27)

(28)

(29)

(30)

(31)

(32)

(cid:1) (cid:111)

(33)

(34)

(35)

The Owen’s T function T (a, b) = φ(a) (cid:82) b
φ(aτ )
1+τ 2 dτ
0
(Owen, 1956) has eﬃcient numerical solutions in
practise (Pateﬁeld and Tandy, 2000).

The ELBO is considerably more complex than the
standard stochastic variational bound of a Gaus-
sian process (Hensman et al., 2013), due to the
probit-sparsiﬁed covariance. The bound is likely
only tractable for the choice of probit link func-
tion Φ(g), while other link functions such as the
logit would lead to intractable bounds necessitating
slower numerical integration (Hensman et al., 2015).

We optimize the Lzi with stochastic gradient as-
cent techniques with respect to the inducing lo-
inducing value means mf , mg and
cations zg, zf ,
the sparsity prior mean β,
covariances Sf , Sg,
the noise variance σ2
y, the signal variances σf , σg,
and ﬁnally the dimensions-speciﬁc lengthscales
(cid:96)f 1, . . . , (cid:96)f D; (cid:96)g1, . . . , (cid:96)gD of the Gaussian ARD ker-
nel.

4 GAUSSIAN PROCESS

NETWORK

The Gaussian Process Regression Networks (GPRN)
framework by Wilson et al. (2012) is an eﬃcient
model for multi-target regression problems, where
each individual output is a linear but non-stationary
combination of shared latent functions. Formally, a
vector-valued output function y(x) ∈ RP with P
outputs is modeled using vector-valued latent func-
tions f (x) ∈ RQ with Q latent values and mixing
weights W (x) ∈ RP ×Q as

y(x) = W (x)[f (x) + (cid:15)] + ε,

(36)

where for all q = 1, . . . , Q and p = 1, . . . , P we as-
sume GP priors and additive zero-mean noises,

fq(x) ∼ GP(0, Kf (x, x(cid:48)))
Wqp(x) ∼ GP(0, Kw(x, x(cid:48)))

(cid:15)q ∼ N (0, σ2
f )
εp ∼ N (0, σ2
y).

(37)

(38)

(39)

(40)

The subscripts are used to denote individual compo-
nents of f and W with p and q indicating pth output
dimension and qth latent dimension, respectively.
We assume shared latent and output noise variances
σ2
f , σ2
y without loss of generality. The distributions
of both functions f and W have been inferred either
with variational EM (Wilson et al., 2012) or by vari-
ational mean-ﬁeld approximation with diagonalized
latent and mixing functions (Nguyen and Bonilla,
2013).

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

4.1 STOCHASTIC VARIATIONAL

INFERENCE

Here, we ﬁrst extend the works of Wilson et al.
(2012) and Nguyen and Bonilla (2013) by intro-
ducing the currently missing SVI bounds for the
standard GPRN, and then propose the novel sparse
GPRN model, and solve its SVI bounds as well, in
the following section.

We begin by introducing the inducing variable aug-
mentation technique for latent functions f (x) and
mixing weights W (x) with uf , zf = {ufq , zfq }Q
and uw, zw = {uwqp, zwqp}Q,P

q,p=1:

q=1

p(y, f , W, uf , uw)

(41)
= p(y|f , W )p(f |uf )p(W |uw)p(uf )p(uw)

p(f |uf ) =

N (fq|Qfq ufq , (cid:101)Kfq )

(42)

p(W |uw) =

N (wqp|Qwqpuwqp, (cid:101)Kwqp)

(43)

p(uf ) =

N (ufq |0, Kfq,mm)

(44)

p(uw) =

N (uwqp|0, Kwqp,mm),

(45)

where we have separate kernels K and extrapolation
matrices Q for each component of W (x) and f (x)
that are of the same form as in equations (17–20).
The w is a vectorised form of W . The variational
approximation is then

q(f , W, uf , uw) = p(f |uf )p(W |uw)q(uf )q(uw)

q(ufq ) =

N (ufq |mfq , Sfq )

(46)

(47)

q(uwqp) =

N (uwqp|mwqp, Swqp),

(48)

where uwqp and ufq indicate the inducing points for
the functions Wqp(x) and fq(x), respectively. The
ELBO can be now stated as

log p(y) ≥ Eq(f ,W ) log p(y|f , W )

(49)

− KL[q(uf , uw)||p(uf , uw)],

where the variational distributions decompose as
q(f , W ) = q(f )q(W ) with marginals of the same form

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

as in equations (28–31),

We extend the GPRN with probit sparsity for the
mixing matrix W , resulting in a joint model

q(f ) =

q(f |uf )q(uf )duf = N (f |µf , Σf )

(50)

p(y, f , W, g) = p(y|f , W )p(f )p(W |g)p(g),

(54)

(cid:90)

(cid:90)

q(W ) =

q(W |uw)q(uw)duw = N (w|µw, Σw).

(51)

where all individual components of the latent func-
tion f and mixing matrix W are given GP priors.
We encode the sparsity terms g for all the Q × P
mixing functions Wqp(x) as

Since the noise term ε is assumed to be isotropic
Gaussian, the density p(y|W, f ) factorises across all
target observations and dimensions. The expecta-
tion term in equation (49) then reduces to solving
the following integral for the ith observation and pth
target dimension,

N,P
(cid:88)

(cid:90) (cid:90)

i,p=1

log N (yp,i|wT

p,ifi, σ2

y)q(fi, wp,i)dwp,idfi.

p(Wqp|gqp) = N (wqp|0, Φ(gqp)Φ(gqp)T ◦ Kw).

(55)

To introduce variational inference, the joint model
is augmented with three sets of inducing variables
for f , W and g. After marginalizing out the induc-
ing variables as in equations (25–27), the marginal
likelihood can be written as

(52)

log p(y) ≥ Eq(f ,W,g) log p(y|f , W )

(56)

− KL[q(uf , uw, ug)||p(uf , uw, ug)].

Q,P
(cid:88)

(cid:16)

q,p=1

−

−

1
2σ2
y

Q,P
(cid:88)

q,p

The above integral has a closed form solution result-
ing in the ﬁnal ELBO as (See Supplements)

Lgprn =

N
(cid:88)

(cid:40) P

(cid:88)

i=1

p=1

(cid:16)

log N

yp,i|

µwqp,iµfq,i, σ2
y

(cid:17)

Q
(cid:88)

q=1

wqp,iσ2
µ2

fq,i + µ2

fq,iσ2

wqp,i + σ2

wqp,iσ2

fq,i

(cid:41)
(cid:17)

KL[q(uwqp, ufq )||p(uwqp, ufq )],

(53)

where µfq,i is the i’th element of µfq and σ2
fq,i is
the i’th diagonal element of Σfq (similarly for the
Wqp’s).

5 SPARSE GAUSSIAN PROCESS

NETWORK

In this section we demonstrate how zero-inﬂated
GPs can be used as plug-in components in other
In particular, we propose a sig-
standard models.
niﬁcant modiﬁcation to GPRN by adding sparsity
to the mixing matrix components. This corresponds
to each of the p outputs being a sparse mixture of
the latent Q functions, i.e. they can eﬀectively use
any subset of the Q latent dimensions by having ze-
ros for the rest in the mixing functions. This makes
the mixture more easily interpretable, and induces
a variable number of latent functions to explain the
output of each input x. The latent function f can
also be sparsiﬁed, with a derivation analogous to the
derivation below.

The joint distribution in the variational expecta-
tion factorizes as q(f , W, g) = q(f )q(W |g)q(g). Also,
with a Gaussian noise assumption, the expectation
term factories across all the observations and tar-
get dimensions. The key step reduces to solving the
following integrals:

N,P
(cid:88)

(cid:90) (cid:90) (cid:90)

i,p=1

log N (yp,i|(wp,i ◦ gp,i)T fi, σ2
y)

(57)

· q(fi, wp,i, gp,i)dwp,idfidgp,i.

The above integral has a tractable solution leading
to the ﬁnal sparse GPRN evidence lower bound (See
Supplements)

Lsgprn =

N
(cid:88)

(cid:40) P

(cid:88)

i=1

p=1

(cid:16)

log N

yp,i|

µwqp,iµgqp,iµfq,i, σ2
y

(cid:17)

Q
(cid:88)

q=1

(58)

wqp,iσ2

(cid:17)
fq,i)

−

1
2σ2
y

Q,P
(cid:88)

(cid:16)

q,p=1

(µ2

gqp,i + σ2

gqp,i)

· (µ2

wqp,iσ2

fq,i + µ2

Q,P
(cid:88)

(cid:16)

q,p=1

gqp,iµ2
σ2

fq,iµ2

wqp,i

fq,iσ2
wqp,i + σ2
(cid:41)

(cid:17)

KL[q(ufq , uwqp, ugqp)||p(ufq , uwqp, ugqp)],

−

−

1
2σ2
y

Q,P
(cid:88)

q,p

where µfq,i, µwqp,i are the variational expectation
means for f (·), W (·) as in equations (28, 29), µgqp,i is
the variational expectation mean of g(·) as in equa-
tion (33), and analogously for the variances.

Table 1: Results for the precipitation dataset over
baseline (Zero; majority voting),
four competing
methods and the proposed method ZiGP on test
data. The columns list both quantitative and qual-
itative performance criteria, best performance is
boldfaced.

Model
Zero
GPc
GPr
GPcr
GPcr(cid:54)=0
ZiGP

RMSE MAE
0.104
0.615
-
-
0.159
0.569
0.589
0.102
0.575
0.101
0.121
0.561

F1
0.000
0.367
0.401
0.366
0.358
0.448

Acc.
0.898
0.911
0.750
0.911
0.912
0.861

Prec. Recall
0.000
0.675
0.266
0.679
0.712
0.381

0.000
0.252
0.817
0.251
0.240
0.558

6 EXPERIMENTS

First we demonstrate how the proposed method can
be used for regression problems with zero-inﬂated
targets. We do that both on a simulated dataset
and for real-world climate modeling scenarios on
a Finnish rain precipitation dataset with approx-
imately 90% zeros. Finally, we demonstrate the
GPRN model and how it improves both the inter-
pretability and predictive performance in the JURA
geological dataset.

We use the squared exponential kernel with ARD in
all experiments. All the parameters including in-
ducing locations, values and variances and kernel
parameters were learned through stochastic Adam
optimization (Kingma and Ba, 2014) on the Tensor-
Flow (Abadi et al., 2016) platform.

We compare our approach ZiGP to baseline Zero
voting, to conventional Gaussian process regression
(GPr) and classiﬁcation (GPc) with SVI approxi-
mations from the GPﬂow package (Matthews et al.,
2017). Finally, we also compare to ﬁrst classifying
the non-zeros, and successively applying regression
either to all data points (GPcr), or to only pre-
dicted non-zeros (GPcr(cid:54)=0, hurdle model).

We record the predictive performance by consider-
ing mean squared error and mean absolute error. We
also compare the models’ ability to predict true ze-
ros with F1, accuracy, precision, and recall of the
optimal models.

6.1 SPATIO-TEMPORAL DATASET

Zero-inﬂated cases are commonly found in clima-
tology and ecology domains.
In this experiment
we demonstrate the proposed method by model-
ing precipitation in Finland3. The dataset consists

3Data

be
ilmatieteenlaitos.fi/

can

found

at

http://en.

Figure 3: ZiGP model ﬁt on the precipitation
dataset. Sample of the actual data (a) against the
sparse rain function estimate (b), with the probit
support function (c) showing the rain progress.

of hourly quantitative non-negative observations of
precipitation amount across 105 observatory loca-
tions in Finland for the month of July 2018. The
dataset contains 113015 datapoints with approxi-
mately 90% zero precipitation observations. The
latitude, longi-
data inputs are three-dimensional:
tude and time. Due to the size of the data, this ex-
periment illustrates the scalability of the variational
inference.

We randomly split 80% of the data for training and
the rest 20% for testing purposes. We split across
time only, such that at a single measurement time,
all locations are simultaneously either in the training
set, or in the test set.

We further utilize the underlying spatio-temporal
grid structure of the data to perform inference in an
eﬃcient manner by Kronecker techniques (Saatchi,
2011). All the kernels for latent processes are as-
sumed to factorise as K = Kspace ⊗ Ktime which
allows placing inducing points independently on spa-
tial and temporal grids.

Figure 4: The distribution of errors with the rain
dataset with the ZiGP and the GPr. The zero-
inﬂated GP achieves much higher number of perfect
(zero) predictions.

Figure 3 depicts the components of the zero-inﬂated
GP model on the precipitation dataset. As shown
in panel (c), the latent support function models the
presence or absence of rainfall. It smoothly follows
the change in rain patterns across hourly observa-
tions. The amount of precipitation is modeled by
the other latent process and the combination of these
two results in sparse predictions. Figure 4 shows
that the absolute error distribution is remarkably
better with the ZiGP model due to it identifying
the absence of rain exactly. While both models ﬁt
the high rainfall regions well, for zero and near-zero
regions GPr does not reﬁne its small errors. Table
1 indicates that the ZiGP model achieves the lowest
mean square error, while also achieving the highest
F1 score that takes into account the class imbalance,
which biases the elementary accuracy, precision and
recall quantities towards the majority class.

6.2 MULTI-OUTPUT PREDICTION

In this experiment we model the multi-response Jura
dataset with the sparse Gaussian process regression
network sGPRN model and compare it with stan-
dard GPRN as baseline. Jura contains concentra-
tion measurements of cadmium, nickel and zinc met-
als in the region of Swiss Jura. We follow the ex-
perimental procedure of Wilson et al. (2012) and
Nguyen and Bonilla (2013). The training set con-
sists of n = 259 observations across D = 2 dimen-
sional geo-spatial locations, and the test set consists
of 100 separate locations. For both models we use
Q = 2 latent functions with the stochastic varia-
tional inference techniques proposed in this paper.
Sparse GPRN uses a sparsity inducing kernel in the
mixing weights. The locations of inducing points for

Figure 5: The sparse GPRN model ﬁt on the Jura
dataset with 11 inducing points. The Q = 2 (dense)
latent functions (a) are combined with the 3 × 2
sparse mixing functions (b) into the P = 3 output
predictions (c). The real data are shown in (d).
The white mixing regions are estimated ‘oﬀ’.

the weights W (x) and the support g(x) are shared.
The kernel length-scales are given a gamma prior
with the shape parameter α = 0.3 and rate param-
eter β = 1.0 to induce smoothness. We train both
the models 30 times with random initialization.

Figure 6: The sparse probit support (a) and latent
functions (b) of the weight function W (x) of the
optimized sparse GPRN model. The black regions
of (a) show regional activations, while the white re-
gions show where the latent functions are ‘oﬀ’. The
elementwise product of the support and weight func-
tions is indicated in the Figure 5b).

Table 6.2 shows that our model performs better than
the state-of-the-art SVI-GPRN, both with m = 5
and m = 10 inducing points. Figure 5 visualises the
optimized sparse GPRN model, while Figure 6 in-
dicates the sparsity pattern in the mixing weights.
The weights have considerable smooth ‘on’ regions
(black), and also interesting smooth ‘oﬀ’ regions
(white). The ‘oﬀ’ regions indicate that for certain lo-

Table 2: Results for the Jura dataset for sparse
GPRN and vanilla GPRN models with test data.
Best performance is with boldface. We do not report
RMSE and MAE values GPc, since its a classiﬁca-
tion method.

Model m RMSE MAE RMSE MAE RMSE MAE
GPRN
5
22.14
22.75
sGPRN 5
25.10
10
GPRN
sGPRN 10
23.63

0.732
0.728
0.774
0.749

5.163
5.079
5.656
5.054

0.572
0.567
0.586
0.573

6.807
6.631
7.207
6.524

34.41
35.09
37.87
36.17

cations, only one of the two latent functions is adap-
tively utilised.

7 DISCUSSION

We proposed a novel paradigm of zero-inﬂated Gaus-
sian processes with a novel sparse kernel. The spar-
sity in the kernel is modeled with smooth probit
ﬁltering of the covariance rows and columns. This
model induces zeros in the prediction function out-
puts, which is highly useful for zero-inﬂated datasets
with excess of zero observations. Furthermore, we
showed how the zero-inﬂated GP can be used to
model sparse mixtures of latent signals with the pro-
posed sparse Gaussian process network. The latent
mixture model with sparse mixing coeﬃcients leads
to locally using only a subset of the latent functions,
which improves interpretability and reduces model
complexity. We demonstrated tractable solutions to
stochastic variational inference of the sparse probit
kernel for the zero-inﬂated GP, conventional GPRN,
and sparse GPRN models, which lends to eﬃcient
exploration of the parameter space of the model.

References

M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis,
J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Is-
ard, et al. Tensorﬂow: A system for large-scale
machine learning. In OSDI, volume 16, pages 265–
283, 2016.

Z. Abraham and P-N. Tan. An integrated frame-
work for simultaneous classiﬁcation and regression
In Proceedings of the 2010
of time-series data.
SIAM International Conference on Data Mining,
pages 653–664. SIAM, 2010.

S. Ancelet, M-P. Etienne, H. Benot, and E. Parent.
Modelling spatial zero-inﬂated continuous data
with an exponentially compound Poisson process.
Environmental and Ecological Statistics, 2009.

M. Andersen, O. Winther, and L. Hansen. Bayesian

inference for structured spike and slab priors. In
NIPS, pages 1745–1753, 2014.

S. Barry and A. H. Welsh. Generalized additive
modelling and zero inﬂated count data. Ecolog-
ical Modelling, 157:179–188, 2002.

D. Bohning, E. Dierz, and P. Schlattmann. Zero-
inﬂated count models and their applications in
public health and social science. In J. Rost and
R. Langeheine, editors, Applications of Latent
Trait and Latent Class Models in the Social Sci-
ences. Waxman Publishing Co, 1997.

S. Charles, B. Bates, I. Smith, and J. Hughes. Sta-
tistical downscaling of daily precipitation from ob-
served and modelled atmospheric ﬁelds. Hydrolog-
ical Processes, pages 1373–1394, 2004.

J.G. Cragg.

Some statistical models for limited
dependent variables with application to the de-
mand for durable goods. Econometrica, 39:829–
844, 1971.

S. del Saz-Salazar and P. Rausell-K¨oster. A double-
hurdle model of urban green areas valuation: deal-
ing with zero responses. Landscape and urban
planning, 84(3-4):241–251, 2008.

W. Enke and A. Spekat. Downscaling climate model
outputs into local and regional weather elements
by classiﬁcation and regression. Climate Research,
8:195–207, 1997.

J. Hensman, N. Fusi, and N. Lawrence. Gaus-
sian processes for big data. In Proceedings of the
Twenty-Ninth Conference on Uncertainty in Ar-
tiﬁcial Intelligence, pages 282–290. AUAI Press,
2013.

J. Hensman, A. Matthews, and Z. Ghahramani.
Scalable variational Gaussian process classiﬁca-
tion. In Artiﬁcial Intelligence and Statistics, pages
351–360, 2015.

W. Herlands, A. Wilson, H. Nickisch, S. Flaxman,
D. Neill, W. van Panhuis, and E. Xing. Scalable
Gaussian processes for characterizing multidimen-
sional change surfaces. In AISTATS, volume 51 of
PMLR, pages 1013–1021, 2016.

N. Johnson and S Kotz. Distributions in Statistics:
Discrete Distributions. Houghton MiZin, Boston,
1969.

D. Kingma and J. Ba. Adam: A method for stochas-

tic optimization. arXiv:1412.6980, 2014.

D. Lambert. Zero-inﬂated Poisson regression with
an application to defects in manufacturing. Tech-
nometrics, 34:1–14, 1992.

M. L´azaro-Gredilla, S. Van Vaerenbergh, and
N. Lawrence. Overlapping mixtures of Gaussian
processes for the data association problem. Pat-
tern Recognition, 45(4):1386–1395, 2012.

A. Matthews, M. van der Wilk, T. Nickson, K. Fu-
jii, A. Boukouvalas, P. Leon-Villagra, Z. Ghahra-
mani, and J. Hensman. GPﬂow: A Gaussian pro-
cess library using TensorFlow. Journal of Machine
Learning Research, 18(40):1–6, 2017.

J. Mullahy. Speciﬁcation and testing of some modi-
ﬁed count data models. Journal of Econometrics,
33:341–365, 1986.

T. Nguyen and E. Bonilla. Eﬃcient variational infer-
ence for Gaussian process regression networks. In
Proceedings of the Sixteenth International Confer-
ence on Artiﬁcial Intelligence and Statistics, vol-
ume 31 of Proceedings of Machine Learning Re-
search, pages 472–480. PMLR, 2013.

D.B. Owen. Tables for computing bivariate normal
probabilities. Annals of Mathematical Statistics,
27:1075–1090, 1956.

M. Pateﬁeld and D. Tandy. Fast and accurate calcu-
lation of owens t-function. Journal of Statistical
Software, 5:1–25, 2000.

C. Rasmussen and Z. Ghahramani. Inﬁnite mixtures
of Gaussian process experts. In NIPS, pages 881–
888, 2002.

C.E. Rasmussen and K.I. Williams. Gaussian pro-
cesses for machine learning. MIT Press, 2006.

Y. Saatchi. Scalable Inference for Structured Gaus-
sian Process Models. PhD thesis, University of
Cambridge, 2011.

H. Salimbeni and M. Deisenroth. Doubly stochastic
variational inference for deep Gaussian processes.
In NIPS, volume 30, 2017.

M. Titsias. Variational learning of inducing vari-
In Artiﬁcial

ables in sparse Gaussian processes.
Intelligence and Statistics, pages 567–574, 2009.

V. Tresp. Mixtures of Gaussian processes. In NIPS,

pages 654–660, 2001.

R.L. Wilby. Statistical downscaling of daily precipi-
tation using daily airﬂow and seasonal teleconnec-
tion. Climate Research, 10:163–178, 1998.

A. G. Wilson, D. Knowles, and Z. Ghahramani.
Gaussian process regression networks. In ICML,
2012.

Supplementary material for the “Zero-inﬂated Gaussian processes with sparse
kernels”

Here we show in detail how we arrived at the three new evidence lower bounds for the zero-inﬂated Gaussian
process, for the Gaussian process network, and for the sparse Gaussian process network.

A) The stochastic variational bound of the zero-inﬂated GP

Here, we will derive the evidence lower bound (ELBO) of the zero-inﬂated Gaussian process. We show how
to solve the ELBO of equations (24) and (25), which results in the equation (32).

The augmented true model with inducing points is deﬁned

We deﬁne the variational posterior approximation as

p(y, f , g, uf , ug) = p(y|f )p(f |g, uf )p(g|ug)p(uf )p(ug)

p(y|f ) = N (y|f , σ2

yI)

p(f |g, uf ) = N (f | diag(Φ(g))Kf nmK −1

f mmuf , Φ(g)Φ(g)T ◦ (cid:101)Kf )

p(g|ug) = N (g|KgnmK −1
p(uf ) = N (uf |0, Kf mm)
p(ug) = N (ug|0, Kgmm).

gmmug, (cid:101)Kg)

q(f , g, uf , ug) = p(f |g, uf )p(g|ug)q(uf )q(ug)
p(f |g, uf ) = N (f | diag(Φ(g))Kf nmK −1

f mmuf , Φ(g)Φ(g)T ◦ (cid:101)Kf )

p(g|ug) = N (g|KgnmK −1
q(uf ) = N (uf |mf , Sf )
q(ug) = N (ug|mg, Sg)

gmmug, (cid:101)Kg)

and where Sf , Sg ∈ Rm×m are square positive semi-deﬁnite matrices, and we deﬁne shorthands

(cid:101)Kf = Kf nn − Kf nmK −1
(cid:101)Kg = Kgnn − KgnmK −1

f mmKf mn
gmmKgmn.

In variational inference we minimize the Kullback-Leibler divergence between the variational approximation
q(f , g, uf , ug) and the true augmented joint distribution p(y, f , g, uf , ug):

KL[q(f , g, uf , ug)||p(y, f , g, uf , ug)] =

q(f , g, uf , ug) log

=

q(f , g, uf , ug) log

df dgduf dug

df dgduf dug

p(y, f , g, uf , ug)
q(f , g, uf , ug)
p(y|f )p(f |g, uf )p(g|ug)p(uf )p(ug)
p(f |g, uf )p(g|ug)q(uf )q(ug)

(cid:90)

(cid:90)

(cid:90)

=

=

(cid:90) (cid:90) (cid:90) (cid:90)

(cid:90)

(cid:124)

q(f , g, uf , ug) log

p(y|f )p(uf )p(ug)
q(uf )q(ug)

df dgduf dug

p(f |g, uf )p(g|ug)q(uf )q(ug) log p(y|f )duf dugdgdf

(S.17)

−

q(uf ) log q(uf )duf

−

q(ug) log q(ug)dug

(S.18)

(cid:123)(cid:122)
KL[q(uf )||p(uf )]

(cid:125)

(cid:123)(cid:122)
KL[q(ug)||p(ug)]

(cid:125)

(cid:90)

(cid:124)

(S.1)

(S.2)

(S.3)

(S.4)

(S.5)

(S.6)

(S.7)

(S.8)

(S.9)

(S.10)

(S.11)

(S.12)

(S.13)

(S.14)

(S.15)

(S.16)

Following the derivation of Hensman et al. (2015), this corresponds to maximizing the evidence lower bound
(ELBO) of equation (24):

(cid:90) (cid:90) (cid:90) (cid:90)

log p(y) ≥

log p(y|f )p(f |g, uf )q(uf )p(g|ug)q(ug)duf dugdgdf − KL[q(uf , ug)||p(uf , ug)]

(S.19)

= Eq(f ) log p(y|f ) − KL[q(uf , ug)||p(uf , ug)]

where we deﬁne

q(f ) =

p(f |g, uf )q(uf )p(g|ug)q(ug)duf dugdg

(cid:90) (cid:90) (cid:90)

(cid:90)

=

q(f |g)q(g)dg,

where the variational approximations are tractably

q(g) =

p(g|ug)q(ug)dug

=

N (g|KgnmK −1

gmmug, (cid:101)Kg)N (ug|mg, Sg)dug

= N (g|µg, Σg)

q(f |g) =

p(f |g, uf )q(uf )duf

(cid:90)

(cid:90)

(cid:90)

(cid:90)

=

N (f | diag(Φ(g))Kf nmK −1

f mmuf , Φ(g)Φ(g)T ◦ (cid:101)Kf )N (uf |mf , Sf )duf

= N (f | diag(Φ(g))µf , Φ(g)Φ(g)T ◦ Σf )

with

f mmmf
gmmmg

µf = Kf nmK −1
µg = KgnmK −1
Σf = Kf nn + Kf nmK −1
Σg = Kgnn + KgnmK −1

f mm(Sf − Kf mm)K −1
gmm(Sg − Kgmm)K −1

f mmKf mn
gmmKgmn.

The variational marginalizations q(g) and q(f |) follow from standard Gaussian identities4. Substituting the
variational marginalizations back to the ELBO results in

(cid:90) (cid:90)

log p(y) ≥

log p(y|f )q(f |g)q(g)df dg − KL[q(uf , ug)||p(uf , ug)].

(S.32)

Next, we marginalize the f from the ELBO. We additionally assume the likelihood p(y|f ) = (cid:81)N
factorises, which results in
(cid:90)

(cid:90)

i=1 p(yi|fi)

log p(y|f )q(f |g)df =

log N (y|f , σ2

yI)q(f |g)df

f

log N (yi|fi, σ2

y)q(fi|gi)dfi

log N (yi|Φ(gi)kT

f iK −1

f mmmf i, σ2

y) −

Φ(gi)2(kf ii + kT

f iK −1

f mm(Sf − Kf mm)K −1

f mmkf i)

(cid:111)

(cid:110)

1
2σ2
y

=

log N (yi|Φ(gi)µf i, σ2

y) −

1
2σ2
y

(cid:8)Φ(gi)2σ2

f i

(cid:9) ,

4See for instance Bishop (2006): Pattern recognition and Machine learning, Springer, Section 2.3.

=

=

N
(cid:88)

(cid:90)

i=1

N
(cid:88)

i=1

N
(cid:88)

i=1

(S.20)

(S.21)

(S.22)

(S.23)

(S.24)

(S.25)

(S.26)

(S.27)

(S.28)

(S.29)

(S.30)

(S.31)

(S.33)

(S.34)

(S.35)

(S.36)

where

µf i = [µf ]i = kT
f i = [Σf ]ii = kf ii + kT
σ2

f iK −1

f mmmf i

f iK −1

f mm(Sf − Kf mm)K −1

f mmkf i.

Substituting the above result into the ELBO results in

Eq(f ) log p(y|f ) =

q(g)

q(f |g) log p(y|f )df dg

(cid:90)

f

(cid:90)

g
(cid:90)

g

N
(cid:88)

i=1

N
(cid:88)

i=1

=

=

=

N
(cid:88)

i=1
(cid:90)

gi

log N (yi|Φ(gi)µf i, σ2

y) −

(cid:8)Φ(gi)2σ2

f i

(cid:9) q(g)dg

1
2σ2
y

log N (yi|Φ(gi)µf i, σ2

y) q(gi)dgi −

(cid:8)Φ(gi)2σ2

f i

(cid:9) q(gi)dgi

1
2σ2
y

N
(cid:88)

(cid:90)

i=1

gi

1
2σ2
y

N
(cid:88)

i=1

log N (yi|(cid:104)Φ(gi)(cid:105)q(gi)µf i, σ2

y) −

(cid:8)V ar[Φ(gi)](µf i)2(cid:9) −

(cid:8)(cid:104)Φ(gi)2(cid:105)q(gi)σ2

f i

(cid:9) .

1
2σ2
y

N
(cid:88)

i=1

The expectations (cid:104).(cid:105)q(gi) of CDF transformation of a random variable with univariate Gaussian distribution.
The analytical forms for these integrals can be written:

(cid:104)Φ(gi)(cid:105)q(gi) =

Φ(gi)q(gi)dgi

=

Φ(gi)N (gi|µgi, σ2




gi)dgi

= Φ



(cid:113)

µgi
1 + σ2
gi



V ar[Φ(gi)] =

(Φ(gi) − (cid:104)Φ(gi)(cid:105)q(gi))2q(gi)dgi

(cid:90)

(cid:90)

(cid:90)

(cid:90)







= Φ



(cid:113)

 − 2T



(cid:113)

µgi
1 + σ2
gi

µgi
1 + σ2
gi

,

(cid:113)

1 + 2 σ2
gi





 − Φ



(cid:113)


2



µgi
1 + σ2
gi

(cid:104)Φ(gi)2(cid:105)q(gi) =

Φ(gi)2q(gi)dgi





= Φ



(cid:113)

 − 2T



(cid:113)

µgi
1 + σ2
gi

µgi
1 + σ2
gi

,

(cid:113)

1 + 2 σ2
gi





1

1

where

µgi = [µg]i = kT
gi = [Σg]ii = Kgii + kT
σ2

giK −1

gmmmgi

giK −1

gmm(Sg − Kgmm)K −1

gmmkgi.

Owen’s T function is deﬁned as T (h, a) = φ(h) (cid:82) a
0

φ(hx)
1+x2 dx.

(S.37)

(S.38)

(S.39)

(S.40)

(S.41)

(S.42)

(S.43)

(S.44)

(S.45)

(S.46)

(S.47)

(S.48)

(S.49)

(S.50)

(S.51)

The ﬁnal evidence lower bound with the Kullback-Leibler terms is

p(y) ≥

log N (yi|(cid:104)Φ(gi)(cid:105)q(gi)µf i, σ2

y) −

(cid:0)V ar[Φ(gi)] µ2

f i + (cid:104)Φ(gi)2(cid:105)q(gi) σ2
f i

(cid:27)

(cid:1)

log |Kf mm| −

log |Sf | +

T r

(mf mf

T + Sf )K −1

f mm

(cid:105)

−

log |Kgmm| −

log |Sg| +

T r (cid:2)(mgmg

T + Sg)K −1

gmm

(cid:3) −

1
2
1
2

(cid:27)

m
2
(cid:27)
m
2

1
2σ2
y

(cid:104)

1
2
1
2

N
(cid:88)

(cid:26)

i=1

−

−

(cid:26) 1
2
(cid:26) 1
2

= LZiGP

B) The stochastic variational bound of the Gaussian process network

In GPRN a vector-valued output function y(x) ∈ RP with P outputs is modeled using vector-valued latent
functions f (x) ∈ RQ with Q latent values and mixing weights W (x) ∈ RP ×Q as

where for all q = 1, . . . , Q and p = 1, . . . , P we assume GP priors and additive zero-mean noises,

y(x) = W (x)[f (x) + (cid:15)] + ε,

fq(x) ∼ GP(0, Kf (x, x(cid:48)))
Wqp(x) ∼ GP(0, Kw(x, x(cid:48)))

(cid:15)q ∼ N (0, σ2
f )
εp ∼ N (0, σ2
y).

The subscripts are used to denote individual components of f and W with p and q indicating pth output
dimension and qth latent dimension respectively.

We begin by introducing the inducing variable augmentation for latent functions f (x) and mixing weights
W (x) with uf , zf = {ufq , zfq }Q

q=1 and uw, zw = {uwqp, zwqp}Q,P

q,p=1:

p(y, f , W, uf , uw) = p(y|f , W )p(f |uf )p(W |uw)p(uf )p(uw)

p(f |uf ) =

N (fq|Qfq ufq , (cid:101)Kfq )

p(W |uw) =

N (wqp|Qwqpuwqp, (cid:101)Kwqp)

p(uf ) =

N (ufq |0, Kfq,mm)

p(uw) =

N (uwqp|0, Kwqp,mm),

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

Qf = KfqnmK −1
fqmm
Qg = KwqpnmK −1
(cid:101)Kf = Kfqnn − KfqnmK −1
(cid:101)Kwqp = Kwqpnn − KwqpnmK −1

wqpmm

fqmmKfqmn

wqpmmKwqpmn.

where we have separate kernels K and extrapolation matrices Q for each component of W (x) and f (x) that
are of the form as given below:

(S.52)

(S.53)

(S.54)

(S.55)

(S.56)

(S.57)

(S.58)

(S.59)

(S.60)

(S.61)

(S.62)

(S.63)

(S.64)

(S.65)

(S.66)

(S.67)

(S.68)

(S.69)

Following the variational inference framework, we deﬁne the variational joint distribution as,

q(f , W, uf , uw) = p(f |uf )p(W |uw)q(uf )q(uw)

q(ufq ) =

N (ufq |mfq Sfq )

q(uwqp) =

N (uwqp|mwqp, Swqp),

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

where uwqp and ufq indicate the inducing points for functions Wqp(x) and fq(x), respectively. The ELBO
can be now stated as

log p(y) ≥ Eq(f ,W,uf ,uw) log p(y|f , W ) − KL[q(uf , uw)||p(uf , uw)]

(cid:90) (cid:90) (cid:90) (cid:90)

=

q(f , W, uf , uw) log p(y|f , W )df dW duf duw − KL[q(uf , uw)||p(uf , uw)]

Since the variational joint posterior decomposes as equation (S.70), we begin by marginalizing the inducing
distributions uf and uw,

q(f , W, uf , uw)df dW duf duw =

p(f |uf )q(uf )df duf

p(W |uw)q(uw)dW duw

(S.75)

uf

(cid:90)

(cid:90)

f
(cid:90)

f

(cid:90)

W

=

q(f )df

q(W )dW

(cid:90)

(cid:90)

W

uw

(cid:90) (cid:90) (cid:90) (cid:90)

where

with

q(f ) =

p(f |uf )q(uf )duf

N (fq|µfq , Σfq )

q(W ) =

p(W |uw)q(uw)duw

(cid:90)

Q
(cid:89)

(cid:90)

q=1

Q
(cid:89)

q=1
(cid:90)

Q,P
(cid:89)

(cid:90)

q,p=1

Q,P
(cid:89)

q,p=1

=

=

=

=

N (Wqp|µwqp, Σwqp)

N (fq|KfqnmK −1

fqmmufq , (cid:101)Kfq )N (ufq |mfq , Sfq )dufq

N (Wqp|KwqpnmK −1

wqpmmuwqp, (cid:101)Kwqp)N (uwqp|mwqp, Swqp)duwqp

µfq = KfqnmK −1
fqmmmfq
µwqp = KwqpnmK −1
wqpmmmwqp
Σfq = Kfqnn + KfqnmK −1
Σwqp = Kwqpnn + KwqpnmK −1

fqmm(Sfq − Kfqmm)K −1

fqmmKfqmn

wqpmm(Swqp − Kwqpmm)K −1

wqpmmKwqpmn.

Since the noise term ε is assumed to be isotropic Gaussian, density p(y|W, f ) factorises across all target

(S.70)

(S.71)

(S.72)

(S.73)

(S.74)

(S.76)

(S.77)

(S.78)

(S.79)

(S.80)

(S.81)

(S.82)

(S.83)

(S.84)

(S.85)

(S.86)

(S.87)

(S.88)

(S.89)

(S.90)

(S.91)

(S.92)

(S.93)

(S.94)

(cid:41)
(cid:17)

(S.95)

(S.96)

(S.97)

observations and dimensions. The expectation term in the ELBO then reduces to,

log p(y) ≥ Eq(W )Eq(f ) log p(y|f , W )
(cid:90) (cid:90)

=

N,P
(cid:88)

i,p=1

(cid:90)

The integral with respect to f can be now solved as

log N (yp,i|wT

p,ifi, ε2

p)q(fi, wp,i)dwp,idfi − KL[q(uf , uw)||p(uf , uw)].

log N (yp,i|wT

p,ifi, ε2

p)q(fi)dfi = log N (yp,i|wT

p,iµfi, ε2

p) −

T r(cid:2)wT

p,iΣfiwp,i

(cid:3)

= log N (yp,i|wT

p,iµfi, ε2

p) −

T r(cid:2)Σfiwp,iwT

p,i

(cid:3).

Next we can marginalize W from the above terms,

(cid:90)

log N (yp,i|wT

p,iµfi, ε2

p)q(wp,i)dwp,i = log N (yp,i|µT

wp,i

µfi, ε2

p) −

T r(cid:2)µT
fi

Σwq,iµfi

(cid:3)

1
2ε2
p
1
2ε2
p

1
2ε2
p

1
2ε2
p

Q
(cid:88)

q=1

= log N (yp,i|µT

µfi, ε2

p) −

wp,i

fq,iσ2
µ2

wqp,i

(cid:90)

1
2ε2
p

T r(cid:2)Σfiwp,iwT

p,i

(cid:3)q(wp,i)dwp,i =

T r(cid:2)Σfi(µwp,i µT

wp,i

+ Σwq,i)(cid:3)

1
2ε2
p

1
2ε2
p

=

Q
(cid:88)

(cid:16)

q=1

wqp,iσ2
µ2

fq,i + σ2

wqp,iσ2

fq,i

(cid:17)

.

Finally, adding the above results across all N observations and response dimensions P along with Gaussian
KL divergence terms, we get the ﬁnal lowerbound:

log p(y) ≥

N
(cid:88)

(cid:40) P

(cid:88)

i=1

p=1

(cid:16)

log N

yp,i|

µwqp,iµfq,i, ε2
p

Q
(cid:88)

q=1

(cid:17)

−

1
2ε2
p

Q,P
(cid:88)

(cid:16)

q,p=1

wqp,iσ2
µ2

fq,i + µ2

fq,iσ2

wqp,i + σ2

wqp,iσ2

fq,i

−

KL[q(uwqp, ufq )||p(uwqp, ufq )]

Q,P
(cid:88)

q,p

= Lgprn,

where µfq,i is the i’th element of µfq and σ2

fq,i is the i’th diagonal element of Σfq (similarly for Wqp’s).

C) The stochastic variational bound of the sparse Gaussian process network

Sparse GPRN is a modiﬁcation to standard GPRN where sparsity is added to the mixing matrix components.
This corresponds to the p’th output being a sparse mixture of the latent Q functions, i.e. it can eﬀectively
use any subset of the Q latent dimensions by having zeros in the mixing functions. The joint distribution
for the model can be written as,

p(y, f , W, g) = p(y|f , W )p(f )p(W |g)p(g),

(S.98)

where all individual components of latent function f and mixing matrix W are given GP priors. We encode
the sparsity terms g for all Q × P mixing functions Wqp(x) functions as

p(Wqp|gqp) = N (wqp|0, Φ(gqp)Φ(gqp)T ◦ Kw).

(S.99)

To introduce variational inference, the joint model is augmented with three sets of inducing variables for f ,
W and g. After marginalizing out the inducing variables similar to SVI for standard GPRN, the lower bund
for marginal likelihood can be written as

log p(y) ≥ Eq(f ,W,g) log p(y|f , W ) − KL[q(uf , uw, ug)||p(uf , uw, ug)].

(S.100)

Where the joint distribution in the variational expectation factorizes as q(f , W, g) = q(f )q(W |g)q(g). The
variational posterior after marginalizing inducing variables is written as,

q(f ) =

q(f |uf )q(uf )duf

=

N (fq|µfq , Σfq )

q(W ) =

q(W |uw)q(uw)duw

N (Wqp|µwqp, Σwqp)

q(g) =

q(g|ug)q(ug)dug

N (gqp|µgqp, Σgqp)

(cid:90)

Q
(cid:89)

q=1
(cid:90)

Q,P
(cid:89)

=

q,p=1
(cid:90)

Q,P
(cid:89)

=

q,p=1

with

µfq = KfqnmK −1
fqmmmfq
µwqp = KwqpnmK −1
wqpmmmwqp
µgqp = KgqpnmK −1
gqpmmmgqp
Σfq = Kfqnn + KfqnmK −1
Σwqp = Kwqpnn + KwqpnmK −1
Σgqp = Kgqpnn + KgqpnmK −1

fqmm(Sfq − Kfqmm)K −1

fqmmKfqmn

wqpmm(Swqp − Kwqpmm)K −1
gqpmm(Sgqp − Kgqpmm)K −1

gqpmmKgqpmn.

wqpmmKwqpmn

Similar to standard GPRN, with the isotropic Gaussian, density p(y|W, f ) factorizes across all target obser-
vations and dimensions. The expectation term in the ELBO then reduces to

log p(y) ≥ Eq(W |g)Eq(f )Eq(g) log p(y|f , W )

N,P
(cid:88)

(cid:90) (cid:90) (cid:90)

=

i,p=1

log N (yp,i|(wp,i ◦ gp,i)T fi, ε2

p)q(fi)q(wp,i|q(gp,i)q(gp,i)dwp,idfidgi − KL[q(uf , uw)||p(uf , uw)].

The integral with respect to f can be now solved as

(cid:90)

log N (yp,i|(wp,i ◦ gp,i)T fi, ε2

p)q(fi)dfi = log N (yp,i|(wp,i ◦ gp,i)T µfi, ε2

p) −

T r(cid:2)(wp,i ◦ gp,i)T Σfi(wp,i ◦ gp,i)(cid:3)

= log N (yp,i|(wp,i ◦ gp,i)T µfi, ε2

p) −

T r(cid:2)Σfi(gp,igT

p,i ◦ wp,iwT

p,i)(cid:3).

1
2ε2
p

1
2ε2
p

(S.101)

(S.102)

(S.103)

(S.104)

(S.105)

(S.106)

(S.107)

(S.108)

(S.109)

(S.110)

(S.111)

(S.112)

(S.113)

(S.114)

(S.115)

(S.116)

(cid:90)

1
2ε2
p

(cid:90)

(cid:90)

1
2ε2
p

Next, by integrating individual terms with respect to W we get

(cid:90)

log N (yp,i|(wp,i ◦ gp,i)T µfi, ε2

p)q(wp,i)dwp,i = log N (yp,i|(µwp,i ◦ gp,i)T µfi, ε2

p) −

1
2ε2
p

T r(cid:2)µT
fi

(gp,igT

p,i ◦ Σwq,i)µfi

(cid:3)

T r(cid:2)Σfi(gp,igT

p,i ◦ wp,iwT

p,i)(cid:3) =

T r(cid:2)Σfi(gp,igT

p,i ◦ (µwp,i µT

wp,i

+ Σwq,i))(cid:3)

1
2ε2
p

Finally, integrating all the above terms with respect to g, we get

log N (yp,i|(µwp,i ◦ gp,i)T µfi, ε2

p)q(gp,i)dgp,i = log N (yp,i|(µwp,i ◦ (cid:104)Φ(gp,i)(cid:105))T µfi, ε2
p)

(S.119)

T r(cid:2)µT
fi

(gp,igT

p,i ◦ Σwq,i)µfi

(cid:3)q(gp,i)dgp,i =

T r(cid:2)µT
fi

(cid:0)((cid:104)Φ(gp,i)(cid:105)(cid:104)Φ(gp,i)(cid:105)T + V ar[Φ(gp,i)]) ◦ Σwq,i

(cid:1) µfi

(cid:3)

(cid:90)

1
2ε2
p

T r(cid:2)Σfi(gp,igT

p,i ◦ (µwp,i µT

wp,i

+ Σwq,i ))(cid:3)q(gp,i)dgp,i =

((cid:104)Φ(gp,i)(cid:105)(cid:104)Φ(gp,i)(cid:105)T + V ar[Φ(gp,i)]) ◦ µwp,i µT

wp,i

(cid:17) (cid:3)

−

1
2ε2
p

T r(cid:2)µT
fi

(µwp,i µT

wp,i

◦ V ar[Φ(gp,i)])µfi]

(cid:16)

= log N

yp,i|

µwqp,iµgqp,iµfq,i, ε2
p

(cid:17)

Q
(cid:88)

q=1

−

1
2ε2
p

Q
(cid:88)

(cid:16)

q=1

gqp,iµ2
σ2

fq,iµ2

wqp,i

(cid:17)

=

(µ2

gqp,i + σ2

gqp,i)µ2

fq,iσ2

wqp,i

(cid:17)

1
2ε2
p

1
2ε2
p

1
2ε2
p

Q
(cid:88)

(cid:16)

q=1

(cid:16)

T r(cid:2)Σfi

+

1
2ε2
p

=

1
2ε2
p

Q
(cid:88)

(cid:16)

q=1

T r(cid:2)Σfi

(cid:0)((cid:104)Φ(gp,i)(cid:105)(cid:104)Φ(gp,i)(cid:105)T + V ar[Φ(gp,i)]) ◦ Σwq,i

(cid:1) (cid:3)

(µ2

gqp,i + σ2

gqp,i)(µ2

wqp,iσ2

fq,i + σ2

wqp,iσ2

Adding above results across all the observations N and output dimensions P , we retrieve the ﬁnal evidence
lower bound

p(y) ≥

N
(cid:88)

(cid:40) P

(cid:88)

i=1

p=1

(cid:16)

log N

yp,i|

µwqp,iµgqp,iµfq,i, ε2
p

(cid:17)

Q
(cid:88)

q=1

Q,P
(cid:88)

(cid:16)

−

q,p=1

(µ2

gqp,i + σ2

gqp,i) · (µ2

wqp,iσ2

fq,i + µ2

fq,iσ2

wqp,i + σ2

wqp,iσ2

gqp,iµ2
σ2

fq,iσ2

wqp,i

(cid:17)
fq,i)

−

Q,P
(cid:88)

(cid:16)

q,p=1

−

KL[q(ufq , uwqp, ugqp)||p(ufq , uwqp, ugqp)]

Q,P
(cid:88)

q,p

= Lsgprn.

(S.117)

(S.118)

(S.120)

(S.121)

(S.122)

(S.123)

(S.124)

(S.125)

(S.126)

(cid:17)
fq,i)

.

(S.127)

(S.128)

(cid:41)

(cid:17)

(S.129)

(S.130)

(S.131)

Variational zero-inﬂated Gaussian processes with sparse kernels

8
1
0
2
 
r
a

M
 
3
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
6
3
0
5
0
.
3
0
8
1
:
v
i
X
r
a

Pashupati Hegde

Markus Heinonen

Samuel Kaski

Helsinki Institute for Information Technology HIIT
Department of Computer Science, Aalto University

Abstract

Zero-inﬂated datasets, which have an ex-
cess of zero outputs, are commonly en-
countered in problems such as climate or
rare event modelling. Conventional ma-
chine learning approaches tend to overesti-
mate the non-zeros leading to poor perfor-
mance. We propose a novel model family
of zero-inﬂated Gaussian processes (ZiGP)
for such zero-inﬂated datasets, produced
by sparse kernels through learning a la-
tent probit Gaussian process that can zero
out kernel rows and columns whenever the
signal is absent. The ZiGPs are particu-
larly useful for making the powerful Gaus-
sian process networks more interpretable.
We introduce sparse GP networks where
variable-order latent modelling is achieved
through sparse mixing signals. We derive
the non-trivial stochastic variational infer-
ence tractably for scalable learning of the
sparse kernels in both models. The novel
output-sparse approach improves both pre-
diction of zero-inﬂated data and inter-
pretability of latent mixing models.

1

INTRODUCTION

Zero-inﬂated quantitative datasets with overabun-
dance of zero output observations are common in
many domains, such as climate and earth sciences
(Enke and Spekat, 1997; Wilby, 1998; Charles et al.,
2004), ecology (del Saz-Salazar and Rausell-K¨oster,
2008; Ancelet et al., 2009), social sciences (Bohn-
ing et al., 1997), and in count processes (Barry and
Welsh, 2002). Traditional regression modelling of
such data tends to underestimate zeros and overes-
timate nonzeros (Andersen et al., 2014).

A conventional way of forming zero-inﬂated mod-
els is to estimate a mixture of a Bernoulli “on-oﬀ”
process and a Poisson count distribution (Johnson
and Kotz, 1969; Lambert, 1992). In hurdle models
a binary “on-oﬀ” process determines whether a hur-
dle is crossed, and the positive responses are gov-
erned by a subsequent process (Cragg, 1971; Mul-
lahy, 1986). The hurdle model is analogous to ﬁrst
performing classiﬁcation and training a continuous
predictor on the positive values only, while the zero-
inﬂated model would regress with all observations.
Both stages can be combined for simultaneous clas-
siﬁcation and regression Abraham and Tan (2010).

Gaussian process models have not been proposed
for zero-inﬂated datasets since their posteriors are
Gaussian, which are ill-ﬁtted for zero predictions.
A suite of Gaussian process models have been pro-
posed for partially related problems, such as mixture
models (Tresp, 2001; Rasmussen and Ghahramani,
2002; L´azaro-Gredilla et al., 2012) and change point
detection (Herlands et al., 2016). Structured spike-
and-slab models place smoothly sparse priors over
the structured inputs (Andersen et al., 2014).

In contrast to other approaches, we propose a
Bayesian model that learns the underlying latent
prediction function, whose covariance is sparsiﬁed
through another Gaussian process switching be-
tween the ‘on’ and ‘oﬀ’ states, resulting in an zero-
inﬂated Gaussian process model. This approach
introduces a tendency of predicting exact zeros
to Gaussian processes, which is directly useful in
datasets with excess zeros.

A Gaussian process network (GPRN) is a latent
signal framework where multi-output data are ex-
plained through a set of latent signals and mixing
weight Gaussian processes (Wilson et al., 2012). The
standard GPRN tends to have dense mixing that
combines all latent signals for all latent outputs. By

applying the zero-predicting Gaussian processes to
latent mixture models, we introduce sparse GPRNs
where latent signals are mixed with sparse instead
of dense mixing weight functions. The sparse model
induces variable-order mixtures of latent signals re-
sulting in simpler and more interpretable models.
We demonstrate both of these properties in our ex-
periments with spatio-temporal and multi-output
datasets.

Main contributions. Our contributions include

1. A novel zero-inﬂated Gaussian process formal-
ism consisting of a latent Gaussian process and
a separate ‘on-oﬀ’ probit-linked Gaussian pro-
cess that can zero out rows and columns of the
model covariance. The novel sparse kernel adds
to GPs the ability to predict zeros.

2. Novel stochastic variational inference (SVI) for
such sparse probit covariances, which in gen-
eral are intractable due to having to compute
expectations of GP covariances with respect to
probit-linked processes. We derive the SVI for
learning both of the underlying processes.

3. A solution to the stochastic variational infer-
ence for conventional Gaussian process net-
works (GPRN) improving the earlier diago-
nalized mean-ﬁeld approximation (Nguyen and
Bonilla, 2013) by taking the covariances fully
into account.

4. A novel sparse GPRN with an on-oﬀ process
in the mixing matrices leading to sparse and
variable-order mixtures of latent signals.

5. A solution to the stochastic variational infer-
ence of sparse GPRN where the SVI is derived
for the network of full probit-linked covariances.

TensorFlow
these methods

implementation
Python
The
of
at
is
github.com/hegdepashupati/zero-inflated-gp
and at github.com/hegdepashupati/gprn-svi.

available

publicly

2 GAUSSIAN PROCESSES

We begin by introducing the basics of conventional
Gaussian processes. Gaussian processes (GP) are a
family of non-parametric, non-linear Bayesian mod-
els (Rasmussen and Williams, 2006). Assume a
dataset of n inputs X = (x1, . . . , xn) with xi ∈ RD
and noisy outputs y = (y1, . . . , yn) ∈ Rn. The ob-
servations y = f (x)+ε are assumed to have additive,

Figure 1: Illustration of a zero-inﬂated GP (a) and
standard GP regression (b). The standard approach
is unable to model sudden loss of signal (at 4 . . . 5)
and signal close to zero (at 0 . . . 1 and 7 . . . 9).

zero mean noise ε ∼ N (0, σ2
prior on the latent function f (x),

y) with a zero-mean GP

f (x) ∼ GP (0, K(x, x(cid:48))) ,

(1)

which deﬁnes a distribution over functions f (x)
whose mean and covariance are

E[f (x)] = 0
cov[f (x), f (x(cid:48))] = K(x, x(cid:48)).

(2)

(3)

Then for any collection of inputs X, the function
values follow a multivariate normal distribution f ∼
N (0, KXX ), where f = (f (x1), . . . , f (xN ))T ∈ Rn,
and where KXX ∈ Rn×n with [KXX ]ij = K(xi, xj).
The key property of Gaussian processes is that they
encode functions that predict similar output values
f (x), f (x(cid:48)) for similar inputs x, x(cid:48), with similarity
determined by the kernel K(x, x(cid:48)). In this paper we
assume the Gaussian ARD kernel

K(x, x(cid:48)) = σ2

f exp

−



1
2

D
(cid:88)

j=1

j)2

(xj − x(cid:48)
(cid:96)2
j



 ,

(4)

with a signal variance σ2
lengthscale (cid:96)1, . . . , (cid:96)D parameters.

f and dimension-speciﬁc

inference of

The
θ =
(σy, σf , (cid:96)1, . . . , (cid:96)D) is performed commonly by max-
imizing the marginal likelihood

the hyperparameters

(cid:90)

p(y|θ) =

p(y|f )p(f |θ)df ,

(5)

which results in a convenient marginal likelihood
called evidence, p(y|θ) = N (y|0, KXX + σ2
yI) for
a Gaussian likelihood.

The Gaussian process deﬁnes a univariate nor-
mal predictive posterior distribution f (x)|y, X ∼
N (µ(x), σ2(x)) for an arbitrary input x with the
prediction mean and variance1

where

µ(x) = KxX (KXX + σ2
yI)−1y,
σ2(x) = Kxx − KxX (KXX + σ2

yI)−1KXx,

(6)

(7)

xX ∈ Rn is the kernel column vector
where KXx = K T
over pairs X ×x, and Kxx = K(x, x) ∈ R is a scalar.
The predictions µ(x) ± σ(x) come with uncertainty
estimates in GP regression.

3 ZERO-INFLATED GAUSSIAN

PROCESSES

Figure 2:
Illustration of the zero-inﬂated GP (a)
and the sparse kernel (b) composed of a smooth
latent function (c,d) ﬁltered by a probit support
function (e,f ), which is induced by the underlying
latent sparsity (g,h).

We introduce zero-inﬂated Gaussian processes that
have – in contrast to standard GP’s – a tendency to
produce exactly zero predictions (See Figure 1). Let
g(x) denote the latent “on-oﬀ” state of a function
f (x). We assume GP priors for both functions with
a joint model

p(y, f , g) = p(y|f )p(f |g)p(g),

(8)

1In the following we omit the implicit conditioning on

data inputs X for clarity.

p(y|f ) = N (y|f , σ2
p(f |g) = N (f |0, Φ(g)Φ(g)T ◦ Kf )

yI)

p(g) = N (g|β1, Kg).

(9)

(10)

(11)

The sparsity values g(x) are squashed between 0 and
1 through a standard Normal cumulative distribu-
tion, or a probit link function, Φ : R → [0, 1]

Φ(g) =

φ(τ )dτ =

1 + erf

,

(12)

(cid:90) g

−∞

(cid:18)

1
2

(cid:19)(cid:19)

(cid:18) g
√
2

2 τ 2

e− 1

where φ(τ ) = 1√
is the standard normal den-
2π
sity function. The structured probit sparsity Φ(g)
models the “on-oﬀ” smoothly due to the latent spar-
sity function g having a GP prior with prior mean
β. The latent function f is modeled throughout but
it is only visible during the “on” states. This mask-
ing eﬀect has similarities to both zero-inﬂated and
hurdle models. The underlying latent function f is
learned from only non-zero data similarly to in hur-
dle models, but the function f is allowed to predict
zeros similarly to zero-inﬂated models.

is the sparse probit-
The key part of our model
sparsiﬁed covariance Φ(g)Φ(g)T ◦ K where the “on-
oﬀ” state Φ(g) has the ability to zero out rows and
columns of the kernel matrix at the “oﬀ” states (See
Figure 2f for the probit pattern Φ(g)Φ(g)T and Fig-
ure 2b for the resulting sparse kernel). As the spar-
sity g(x) converges towards minus inﬁnity, the probit
link Φ(g(x)) approaches zero, which leads the func-
tion distribution approaching N (fi|0, 0), or fi = 0.
Numerical problems are avoided since in practice
Φ(g) > 0, and due to the conditioning noise vari-
ance term σ2

y > 0.

The marginal likelihood of the zero-inﬂated Gaus-
sian process is intractable due to the probit-
sparsiﬁcation of the kernel. We derive a stochastic
variational Bayes approximation, which we show to
be tractable due to the choice of using the probit
link function.

3.1 STOCHASTIC VARIATIONAL

INFERENCE

Inference for standard Gaussian process models is
diﬃcult to scale as complexity grows with O(n3) as
a function of the data size n. Titsias (2009) pro-
posed a variational inference approach for GPs using
m < n inducing variables, with a reduced computa-
tional complexity of O(m3) for m inducing points.
The novelty of this approach lies in the idea that

the locations and values of inducing points can be
treated as variational parameters, and optimized.
Hensman et al. (2013, 2015) introduced more eﬃ-
cient stochastic variational inference (SVI) with fac-
torised likelihoods that has been demonstrated with
up to billion data points (Salimbeni and Deisenroth,
2017). This approach cannot be directly applied to
sparse kernels due to having to compute expectation
of the probit product in the covariance. We derive
the SVI bound tractably for the zero-inﬂated model
and its sparse kernel, which is necessary in order to
apply the eﬃcient parameter estimation techniques
with automatic diﬀerentiation with frameworks such
as TensorFlow (Abadi et al., 2016).

We begin by applying the inducing point augmen-
tations f (zf ) = uf and g(zg) = ug for both
the latent function f (·) and the sparsity function
g(·). We place m inducing points uf 1, . . . uf m
and ug1, . . . ugm for the two functions. The aug-
mented joint distribution is p(y, f , g, uf , ug) =
p(y|f )p(f |g, uf )p(g|ug)p(uf )p(ug), where2

We minimize the Kullback-Leibler divergence be-
tween the true augmented posterior p(y, f , g, uf , ug)
and the variational distribution q(f , g, uf , ug),
which is equivalent to solving the following evi-
dence lower bound (as shown by e.g. Hensman et al.
(2015)):

log p(y) ≥ Eq(f ) log p(y|f ) − KL[q(uf , ug)||p(uf , ug)],
(24)

q(f ) =

p(f |g, uf )q(uf )p(g|ug)q(ug)duf dugdg

where we deﬁne

(cid:90) (cid:90) (cid:90)

(cid:90)

=

q(f |g)q(g)dg,

where the variational approximations are tractably

(cid:90)

(cid:90)

q(g) =

p(g|ug)q(ug)dug

= N (g|µg, Σg)

q(f |g) =

p(f |g, uf )q(uf )duf

p(f |g, uf ) = N (f | diag(Φ(g))Qf uf , Φ(g)Φ(g)T ◦ (cid:101)Kf )
(13)

with

= N (f | diag(Φ(g))µf , Φ(g)Φ(g)T ◦ Σf )

(14)

(15)

(16)

(17)

(18)

(19)

(20)

p(g|ug) = N (g|Qgug, (cid:101)Kg)
p(uf ) = N (uf |0, Kf mm)
p(ug) = N (ug|0, Kgmm)

and where

f mm

Qf = Kf nmK −1
Qg = KgnmK −1
(cid:101)Kf = Kf nn − Kf nmK −1
(cid:101)Kg = Kgnn − KgnmK −1

gmm

f mmKf mn
gmmKgmn.

We denote the kernels for functions f and g by the
corresponding subscripts. The kernel Kf nn is be-
tween all n data points, the kernel Kf nm is between
all n datapoints and m inducing points, and the ker-
nel Kf mm is between all m inducing points (similarly
for g as well).

Next we use the standard variational approach by
introducing approximative variational distributions
for the inducing points,

q(uf ) = N (uf |mf , Sf )
q(ug) = N (ug|mg, Sg)
where Sf , Sg ∈ Rm×m are square positive semi-
deﬁnite matrices. The variational joint posterior is

(22)

(21)

q(f , g, uf , ug) = p(f |g, uf )p(g|ug)q(uf )q(ug). (23)

2We drop the implicit conditioning on z’s for clarity.

µf = Qf mf
µg = Qgmg
Σf = Kf nn + Qf (Sf − Kf mm)QT
f
Σg = Kgnn + Qg(Sg − Kgmm)QT
g .

We additionally assume the likelihood p(y|f ) =
(cid:81)N
i=1 p(yi|fi) factorises. We solve the ﬁnal ELBO
of equations (24) and (25) as (See Supplements for
detailed derivation)

N
(cid:88)

(cid:110)

i=1

−

1
2σ2
y

LZI =

log N (yi|(cid:104)Φ(gi)(cid:105)q(gi)µf i, σ2
y)

(cid:0)Var[Φ(gi)]µ2

f i + (cid:104)Φ(gi)2(cid:105)q(gi)σ2
f i

− KL[q(uf )||p(uf )] − KL[q(ug)||p(ug)],

where µf i is the i’th element of µf and σ2
f i is the
i’th diagonal element of Σf (similarly with g). The
expectations are tractable,

(cid:104)Φ(gi)(cid:105)q(gi) = Φ(λgi),

λgi =

(cid:113)

(cid:104)Φ(gi)2(cid:105)q(gi) = Φ(λgi) − 2T

λgi,

Var[Φ(gi)] = Φ(λgi) − 2T

λgi,

− Φ(λgi)2.

µgi
1 + σ2
gi
(cid:19)

λgi
µgi
λgi
µgi

(cid:19)

(cid:18)

(cid:18)

(25)

(26)

(27)

(28)

(29)

(30)

(31)

(32)

(cid:1) (cid:111)

(33)

(34)

(35)

The Owen’s T function T (a, b) = φ(a) (cid:82) b
φ(aτ )
1+τ 2 dτ
0
(Owen, 1956) has eﬃcient numerical solutions in
practise (Pateﬁeld and Tandy, 2000).

The ELBO is considerably more complex than the
standard stochastic variational bound of a Gaus-
sian process (Hensman et al., 2013), due to the
probit-sparsiﬁed covariance. The bound is likely
only tractable for the choice of probit link func-
tion Φ(g), while other link functions such as the
logit would lead to intractable bounds necessitating
slower numerical integration (Hensman et al., 2015).

We optimize the Lzi with stochastic gradient as-
cent techniques with respect to the inducing lo-
inducing value means mf , mg and
cations zg, zf ,
the sparsity prior mean β,
covariances Sf , Sg,
the noise variance σ2
y, the signal variances σf , σg,
and ﬁnally the dimensions-speciﬁc lengthscales
(cid:96)f 1, . . . , (cid:96)f D; (cid:96)g1, . . . , (cid:96)gD of the Gaussian ARD ker-
nel.

4 GAUSSIAN PROCESS

NETWORK

The Gaussian Process Regression Networks (GPRN)
framework by Wilson et al. (2012) is an eﬃcient
model for multi-target regression problems, where
each individual output is a linear but non-stationary
combination of shared latent functions. Formally, a
vector-valued output function y(x) ∈ RP with P
outputs is modeled using vector-valued latent func-
tions f (x) ∈ RQ with Q latent values and mixing
weights W (x) ∈ RP ×Q as

y(x) = W (x)[f (x) + (cid:15)] + ε,

(36)

where for all q = 1, . . . , Q and p = 1, . . . , P we as-
sume GP priors and additive zero-mean noises,

fq(x) ∼ GP(0, Kf (x, x(cid:48)))
Wqp(x) ∼ GP(0, Kw(x, x(cid:48)))

(cid:15)q ∼ N (0, σ2
f )
εp ∼ N (0, σ2
y).

(37)

(38)

(39)

(40)

The subscripts are used to denote individual compo-
nents of f and W with p and q indicating pth output
dimension and qth latent dimension, respectively.
We assume shared latent and output noise variances
σ2
f , σ2
y without loss of generality. The distributions
of both functions f and W have been inferred either
with variational EM (Wilson et al., 2012) or by vari-
ational mean-ﬁeld approximation with diagonalized
latent and mixing functions (Nguyen and Bonilla,
2013).

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

4.1 STOCHASTIC VARIATIONAL

INFERENCE

Here, we ﬁrst extend the works of Wilson et al.
(2012) and Nguyen and Bonilla (2013) by intro-
ducing the currently missing SVI bounds for the
standard GPRN, and then propose the novel sparse
GPRN model, and solve its SVI bounds as well, in
the following section.

We begin by introducing the inducing variable aug-
mentation technique for latent functions f (x) and
mixing weights W (x) with uf , zf = {ufq , zfq }Q
and uw, zw = {uwqp, zwqp}Q,P

q,p=1:

q=1

p(y, f , W, uf , uw)

(41)
= p(y|f , W )p(f |uf )p(W |uw)p(uf )p(uw)

p(f |uf ) =

N (fq|Qfq ufq , (cid:101)Kfq )

(42)

p(W |uw) =

N (wqp|Qwqpuwqp, (cid:101)Kwqp)

(43)

p(uf ) =

N (ufq |0, Kfq,mm)

(44)

p(uw) =

N (uwqp|0, Kwqp,mm),

(45)

where we have separate kernels K and extrapolation
matrices Q for each component of W (x) and f (x)
that are of the same form as in equations (17–20).
The w is a vectorised form of W . The variational
approximation is then

q(f , W, uf , uw) = p(f |uf )p(W |uw)q(uf )q(uw)

q(ufq ) =

N (ufq |mfq , Sfq )

(46)

(47)

q(uwqp) =

N (uwqp|mwqp, Swqp),

(48)

where uwqp and ufq indicate the inducing points for
the functions Wqp(x) and fq(x), respectively. The
ELBO can be now stated as

log p(y) ≥ Eq(f ,W ) log p(y|f , W )

(49)

− KL[q(uf , uw)||p(uf , uw)],

where the variational distributions decompose as
q(f , W ) = q(f )q(W ) with marginals of the same form

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

as in equations (28–31),

We extend the GPRN with probit sparsity for the
mixing matrix W , resulting in a joint model

q(f ) =

q(f |uf )q(uf )duf = N (f |µf , Σf )

(50)

p(y, f , W, g) = p(y|f , W )p(f )p(W |g)p(g),

(54)

(cid:90)

(cid:90)

q(W ) =

q(W |uw)q(uw)duw = N (w|µw, Σw).

(51)

where all individual components of the latent func-
tion f and mixing matrix W are given GP priors.
We encode the sparsity terms g for all the Q × P
mixing functions Wqp(x) as

Since the noise term ε is assumed to be isotropic
Gaussian, the density p(y|W, f ) factorises across all
target observations and dimensions. The expecta-
tion term in equation (49) then reduces to solving
the following integral for the ith observation and pth
target dimension,

N,P
(cid:88)

(cid:90) (cid:90)

i,p=1

log N (yp,i|wT

p,ifi, σ2

y)q(fi, wp,i)dwp,idfi.

p(Wqp|gqp) = N (wqp|0, Φ(gqp)Φ(gqp)T ◦ Kw).

(55)

To introduce variational inference, the joint model
is augmented with three sets of inducing variables
for f , W and g. After marginalizing out the induc-
ing variables as in equations (25–27), the marginal
likelihood can be written as

(52)

log p(y) ≥ Eq(f ,W,g) log p(y|f , W )

(56)

− KL[q(uf , uw, ug)||p(uf , uw, ug)].

Q,P
(cid:88)

(cid:16)

q,p=1

−

−

1
2σ2
y

Q,P
(cid:88)

q,p

The above integral has a closed form solution result-
ing in the ﬁnal ELBO as (See Supplements)

Lgprn =

N
(cid:88)

(cid:40) P

(cid:88)

i=1

p=1

(cid:16)

log N

yp,i|

µwqp,iµfq,i, σ2
y

(cid:17)

Q
(cid:88)

q=1

wqp,iσ2
µ2

fq,i + µ2

fq,iσ2

wqp,i + σ2

wqp,iσ2

fq,i

(cid:41)
(cid:17)

KL[q(uwqp, ufq )||p(uwqp, ufq )],

(53)

where µfq,i is the i’th element of µfq and σ2
fq,i is
the i’th diagonal element of Σfq (similarly for the
Wqp’s).

5 SPARSE GAUSSIAN PROCESS

NETWORK

In this section we demonstrate how zero-inﬂated
GPs can be used as plug-in components in other
In particular, we propose a sig-
standard models.
niﬁcant modiﬁcation to GPRN by adding sparsity
to the mixing matrix components. This corresponds
to each of the p outputs being a sparse mixture of
the latent Q functions, i.e. they can eﬀectively use
any subset of the Q latent dimensions by having ze-
ros for the rest in the mixing functions. This makes
the mixture more easily interpretable, and induces
a variable number of latent functions to explain the
output of each input x. The latent function f can
also be sparsiﬁed, with a derivation analogous to the
derivation below.

The joint distribution in the variational expecta-
tion factorizes as q(f , W, g) = q(f )q(W |g)q(g). Also,
with a Gaussian noise assumption, the expectation
term factories across all the observations and tar-
get dimensions. The key step reduces to solving the
following integrals:

N,P
(cid:88)

(cid:90) (cid:90) (cid:90)

i,p=1

log N (yp,i|(wp,i ◦ gp,i)T fi, σ2
y)

(57)

· q(fi, wp,i, gp,i)dwp,idfidgp,i.

The above integral has a tractable solution leading
to the ﬁnal sparse GPRN evidence lower bound (See
Supplements)

Lsgprn =

N
(cid:88)

(cid:40) P

(cid:88)

i=1

p=1

(cid:16)

log N

yp,i|

µwqp,iµgqp,iµfq,i, σ2
y

(cid:17)

Q
(cid:88)

q=1

(58)

wqp,iσ2

(cid:17)
fq,i)

−

1
2σ2
y

Q,P
(cid:88)

(cid:16)

q,p=1

(µ2

gqp,i + σ2

gqp,i)

· (µ2

wqp,iσ2

fq,i + µ2

Q,P
(cid:88)

(cid:16)

q,p=1

gqp,iµ2
σ2

fq,iµ2

wqp,i

fq,iσ2
wqp,i + σ2
(cid:41)

(cid:17)

KL[q(ufq , uwqp, ugqp)||p(ufq , uwqp, ugqp)],

−

−

1
2σ2
y

Q,P
(cid:88)

q,p

where µfq,i, µwqp,i are the variational expectation
means for f (·), W (·) as in equations (28, 29), µgqp,i is
the variational expectation mean of g(·) as in equa-
tion (33), and analogously for the variances.

Table 1: Results for the precipitation dataset over
baseline (Zero; majority voting),
four competing
methods and the proposed method ZiGP on test
data. The columns list both quantitative and qual-
itative performance criteria, best performance is
boldfaced.

Model
Zero
GPc
GPr
GPcr
GPcr(cid:54)=0
ZiGP

RMSE MAE
0.104
0.615
-
-
0.159
0.569
0.589
0.102
0.575
0.101
0.121
0.561

F1
0.000
0.367
0.401
0.366
0.358
0.448

Acc.
0.898
0.911
0.750
0.911
0.912
0.861

Prec. Recall
0.000
0.675
0.266
0.679
0.712
0.381

0.000
0.252
0.817
0.251
0.240
0.558

6 EXPERIMENTS

First we demonstrate how the proposed method can
be used for regression problems with zero-inﬂated
targets. We do that both on a simulated dataset
and for real-world climate modeling scenarios on
a Finnish rain precipitation dataset with approx-
imately 90% zeros. Finally, we demonstrate the
GPRN model and how it improves both the inter-
pretability and predictive performance in the JURA
geological dataset.

We use the squared exponential kernel with ARD in
all experiments. All the parameters including in-
ducing locations, values and variances and kernel
parameters were learned through stochastic Adam
optimization (Kingma and Ba, 2014) on the Tensor-
Flow (Abadi et al., 2016) platform.

We compare our approach ZiGP to baseline Zero
voting, to conventional Gaussian process regression
(GPr) and classiﬁcation (GPc) with SVI approxi-
mations from the GPﬂow package (Matthews et al.,
2017). Finally, we also compare to ﬁrst classifying
the non-zeros, and successively applying regression
either to all data points (GPcr), or to only pre-
dicted non-zeros (GPcr(cid:54)=0, hurdle model).

We record the predictive performance by consider-
ing mean squared error and mean absolute error. We
also compare the models’ ability to predict true ze-
ros with F1, accuracy, precision, and recall of the
optimal models.

6.1 SPATIO-TEMPORAL DATASET

Zero-inﬂated cases are commonly found in clima-
tology and ecology domains.
In this experiment
we demonstrate the proposed method by model-
ing precipitation in Finland3. The dataset consists

3Data

be
ilmatieteenlaitos.fi/

can

found

at

http://en.

Figure 3: ZiGP model ﬁt on the precipitation
dataset. Sample of the actual data (a) against the
sparse rain function estimate (b), with the probit
support function (c) showing the rain progress.

of hourly quantitative non-negative observations of
precipitation amount across 105 observatory loca-
tions in Finland for the month of July 2018. The
dataset contains 113015 datapoints with approxi-
mately 90% zero precipitation observations. The
latitude, longi-
data inputs are three-dimensional:
tude and time. Due to the size of the data, this ex-
periment illustrates the scalability of the variational
inference.

We randomly split 80% of the data for training and
the rest 20% for testing purposes. We split across
time only, such that at a single measurement time,
all locations are simultaneously either in the training
set, or in the test set.

We further utilize the underlying spatio-temporal
grid structure of the data to perform inference in an
eﬃcient manner by Kronecker techniques (Saatchi,
2011). All the kernels for latent processes are as-
sumed to factorise as K = Kspace ⊗ Ktime which
allows placing inducing points independently on spa-
tial and temporal grids.

Figure 4: The distribution of errors with the rain
dataset with the ZiGP and the GPr. The zero-
inﬂated GP achieves much higher number of perfect
(zero) predictions.

Figure 3 depicts the components of the zero-inﬂated
GP model on the precipitation dataset. As shown
in panel (c), the latent support function models the
presence or absence of rainfall. It smoothly follows
the change in rain patterns across hourly observa-
tions. The amount of precipitation is modeled by
the other latent process and the combination of these
two results in sparse predictions. Figure 4 shows
that the absolute error distribution is remarkably
better with the ZiGP model due to it identifying
the absence of rain exactly. While both models ﬁt
the high rainfall regions well, for zero and near-zero
regions GPr does not reﬁne its small errors. Table
1 indicates that the ZiGP model achieves the lowest
mean square error, while also achieving the highest
F1 score that takes into account the class imbalance,
which biases the elementary accuracy, precision and
recall quantities towards the majority class.

6.2 MULTI-OUTPUT PREDICTION

In this experiment we model the multi-response Jura
dataset with the sparse Gaussian process regression
network sGPRN model and compare it with stan-
dard GPRN as baseline. Jura contains concentra-
tion measurements of cadmium, nickel and zinc met-
als in the region of Swiss Jura. We follow the ex-
perimental procedure of Wilson et al. (2012) and
Nguyen and Bonilla (2013). The training set con-
sists of n = 259 observations across D = 2 dimen-
sional geo-spatial locations, and the test set consists
of 100 separate locations. For both models we use
Q = 2 latent functions with the stochastic varia-
tional inference techniques proposed in this paper.
Sparse GPRN uses a sparsity inducing kernel in the
mixing weights. The locations of inducing points for

Figure 5: The sparse GPRN model ﬁt on the Jura
dataset with 11 inducing points. The Q = 2 (dense)
latent functions (a) are combined with the 3 × 2
sparse mixing functions (b) into the P = 3 output
predictions (c). The real data are shown in (d).
The white mixing regions are estimated ‘oﬀ’.

the weights W (x) and the support g(x) are shared.
The kernel length-scales are given a gamma prior
with the shape parameter α = 0.3 and rate param-
eter β = 1.0 to induce smoothness. We train both
the models 30 times with random initialization.

Figure 6: The sparse probit support (a) and latent
functions (b) of the weight function W (x) of the
optimized sparse GPRN model. The black regions
of (a) show regional activations, while the white re-
gions show where the latent functions are ‘oﬀ’. The
elementwise product of the support and weight func-
tions is indicated in the Figure 5b).

Table 6.2 shows that our model performs better than
the state-of-the-art SVI-GPRN, both with m = 5
and m = 10 inducing points. Figure 5 visualises the
optimized sparse GPRN model, while Figure 6 in-
dicates the sparsity pattern in the mixing weights.
The weights have considerable smooth ‘on’ regions
(black), and also interesting smooth ‘oﬀ’ regions
(white). The ‘oﬀ’ regions indicate that for certain lo-

Table 2: Results for the Jura dataset for sparse
GPRN and vanilla GPRN models with test data.
Best performance is with boldface. We do not report
RMSE and MAE values GPc, since its a classiﬁca-
tion method.

Model m RMSE MAE RMSE MAE RMSE MAE
GPRN
5
22.14
22.75
sGPRN 5
25.10
10
GPRN
sGPRN 10
23.63

0.732
0.728
0.774
0.749

0.572
0.567
0.586
0.573

5.163
5.079
5.656
5.054

6.807
6.631
7.207
6.524

34.41
35.09
37.87
36.17

cations, only one of the two latent functions is adap-
tively utilised.

7 DISCUSSION

We proposed a novel paradigm of zero-inﬂated Gaus-
sian processes with a novel sparse kernel. The spar-
sity in the kernel is modeled with smooth probit
ﬁltering of the covariance rows and columns. This
model induces zeros in the prediction function out-
puts, which is highly useful for zero-inﬂated datasets
with excess of zero observations. Furthermore, we
showed how the zero-inﬂated GP can be used to
model sparse mixtures of latent signals with the pro-
posed sparse Gaussian process network. The latent
mixture model with sparse mixing coeﬃcients leads
to locally using only a subset of the latent functions,
which improves interpretability and reduces model
complexity. We demonstrated tractable solutions to
stochastic variational inference of the sparse probit
kernel for the zero-inﬂated GP, conventional GPRN,
and sparse GPRN models, which lends to eﬃcient
exploration of the parameter space of the model.

References

M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis,
J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Is-
ard, et al. Tensorﬂow: A system for large-scale
machine learning. In OSDI, volume 16, pages 265–
283, 2016.

Z. Abraham and P-N. Tan. An integrated frame-
work for simultaneous classiﬁcation and regression
In Proceedings of the 2010
of time-series data.
SIAM International Conference on Data Mining,
pages 653–664. SIAM, 2010.

S. Ancelet, M-P. Etienne, H. Benot, and E. Parent.
Modelling spatial zero-inﬂated continuous data
with an exponentially compound Poisson process.
Environmental and Ecological Statistics, 2009.

M. Andersen, O. Winther, and L. Hansen. Bayesian

inference for structured spike and slab priors. In
NIPS, pages 1745–1753, 2014.

S. Barry and A. H. Welsh. Generalized additive
modelling and zero inﬂated count data. Ecolog-
ical Modelling, 157:179–188, 2002.

D. Bohning, E. Dierz, and P. Schlattmann. Zero-
inﬂated count models and their applications in
public health and social science. In J. Rost and
R. Langeheine, editors, Applications of Latent
Trait and Latent Class Models in the Social Sci-
ences. Waxman Publishing Co, 1997.

S. Charles, B. Bates, I. Smith, and J. Hughes. Sta-
tistical downscaling of daily precipitation from ob-
served and modelled atmospheric ﬁelds. Hydrolog-
ical Processes, pages 1373–1394, 2004.

J.G. Cragg.

Some statistical models for limited
dependent variables with application to the de-
mand for durable goods. Econometrica, 39:829–
844, 1971.

S. del Saz-Salazar and P. Rausell-K¨oster. A double-
hurdle model of urban green areas valuation: deal-
ing with zero responses. Landscape and urban
planning, 84(3-4):241–251, 2008.

W. Enke and A. Spekat. Downscaling climate model
outputs into local and regional weather elements
by classiﬁcation and regression. Climate Research,
8:195–207, 1997.

J. Hensman, N. Fusi, and N. Lawrence. Gaus-
sian processes for big data. In Proceedings of the
Twenty-Ninth Conference on Uncertainty in Ar-
tiﬁcial Intelligence, pages 282–290. AUAI Press,
2013.

J. Hensman, A. Matthews, and Z. Ghahramani.
Scalable variational Gaussian process classiﬁca-
tion. In Artiﬁcial Intelligence and Statistics, pages
351–360, 2015.

W. Herlands, A. Wilson, H. Nickisch, S. Flaxman,
D. Neill, W. van Panhuis, and E. Xing. Scalable
Gaussian processes for characterizing multidimen-
sional change surfaces. In AISTATS, volume 51 of
PMLR, pages 1013–1021, 2016.

N. Johnson and S Kotz. Distributions in Statistics:
Discrete Distributions. Houghton MiZin, Boston,
1969.

D. Kingma and J. Ba. Adam: A method for stochas-

tic optimization. arXiv:1412.6980, 2014.

D. Lambert. Zero-inﬂated Poisson regression with
an application to defects in manufacturing. Tech-
nometrics, 34:1–14, 1992.

M. L´azaro-Gredilla, S. Van Vaerenbergh, and
N. Lawrence. Overlapping mixtures of Gaussian
processes for the data association problem. Pat-
tern Recognition, 45(4):1386–1395, 2012.

A. Matthews, M. van der Wilk, T. Nickson, K. Fu-
jii, A. Boukouvalas, P. Leon-Villagra, Z. Ghahra-
mani, and J. Hensman. GPﬂow: A Gaussian pro-
cess library using TensorFlow. Journal of Machine
Learning Research, 18(40):1–6, 2017.

J. Mullahy. Speciﬁcation and testing of some modi-
ﬁed count data models. Journal of Econometrics,
33:341–365, 1986.

T. Nguyen and E. Bonilla. Eﬃcient variational infer-
ence for Gaussian process regression networks. In
Proceedings of the Sixteenth International Confer-
ence on Artiﬁcial Intelligence and Statistics, vol-
ume 31 of Proceedings of Machine Learning Re-
search, pages 472–480. PMLR, 2013.

D.B. Owen. Tables for computing bivariate normal
probabilities. Annals of Mathematical Statistics,
27:1075–1090, 1956.

M. Pateﬁeld and D. Tandy. Fast and accurate calcu-
lation of owens t-function. Journal of Statistical
Software, 5:1–25, 2000.

C. Rasmussen and Z. Ghahramani. Inﬁnite mixtures
of Gaussian process experts. In NIPS, pages 881–
888, 2002.

C.E. Rasmussen and K.I. Williams. Gaussian pro-
cesses for machine learning. MIT Press, 2006.

Y. Saatchi. Scalable Inference for Structured Gaus-
sian Process Models. PhD thesis, University of
Cambridge, 2011.

H. Salimbeni and M. Deisenroth. Doubly stochastic
variational inference for deep Gaussian processes.
In NIPS, volume 30, 2017.

M. Titsias. Variational learning of inducing vari-
In Artiﬁcial

ables in sparse Gaussian processes.
Intelligence and Statistics, pages 567–574, 2009.

V. Tresp. Mixtures of Gaussian processes. In NIPS,

pages 654–660, 2001.

R.L. Wilby. Statistical downscaling of daily precipi-
tation using daily airﬂow and seasonal teleconnec-
tion. Climate Research, 10:163–178, 1998.

A. G. Wilson, D. Knowles, and Z. Ghahramani.
Gaussian process regression networks. In ICML,
2012.

Supplementary material for the “Zero-inﬂated Gaussian processes with sparse
kernels”

Here we show in detail how we arrived at the three new evidence lower bounds for the zero-inﬂated Gaussian
process, for the Gaussian process network, and for the sparse Gaussian process network.

A) The stochastic variational bound of the zero-inﬂated GP

Here, we will derive the evidence lower bound (ELBO) of the zero-inﬂated Gaussian process. We show how
to solve the ELBO of equations (24) and (25), which results in the equation (32).

The augmented true model with inducing points is deﬁned

We deﬁne the variational posterior approximation as

p(y, f , g, uf , ug) = p(y|f )p(f |g, uf )p(g|ug)p(uf )p(ug)

p(y|f ) = N (y|f , σ2

yI)

p(f |g, uf ) = N (f | diag(Φ(g))Kf nmK −1

f mmuf , Φ(g)Φ(g)T ◦ (cid:101)Kf )

p(g|ug) = N (g|KgnmK −1
p(uf ) = N (uf |0, Kf mm)
p(ug) = N (ug|0, Kgmm).

gmmug, (cid:101)Kg)

q(f , g, uf , ug) = p(f |g, uf )p(g|ug)q(uf )q(ug)
p(f |g, uf ) = N (f | diag(Φ(g))Kf nmK −1

f mmuf , Φ(g)Φ(g)T ◦ (cid:101)Kf )

p(g|ug) = N (g|KgnmK −1
q(uf ) = N (uf |mf , Sf )
q(ug) = N (ug|mg, Sg)

gmmug, (cid:101)Kg)

and where Sf , Sg ∈ Rm×m are square positive semi-deﬁnite matrices, and we deﬁne shorthands

(cid:101)Kf = Kf nn − Kf nmK −1
(cid:101)Kg = Kgnn − KgnmK −1

f mmKf mn
gmmKgmn.

In variational inference we minimize the Kullback-Leibler divergence between the variational approximation
q(f , g, uf , ug) and the true augmented joint distribution p(y, f , g, uf , ug):

KL[q(f , g, uf , ug)||p(y, f , g, uf , ug)] =

q(f , g, uf , ug) log

=

q(f , g, uf , ug) log

df dgduf dug

df dgduf dug

p(y, f , g, uf , ug)
q(f , g, uf , ug)
p(y|f )p(f |g, uf )p(g|ug)p(uf )p(ug)
p(f |g, uf )p(g|ug)q(uf )q(ug)

(cid:90)

(cid:90)

(cid:90)

=

=

(cid:90) (cid:90) (cid:90) (cid:90)

(cid:90)

(cid:124)

q(f , g, uf , ug) log

p(y|f )p(uf )p(ug)
q(uf )q(ug)

df dgduf dug

p(f |g, uf )p(g|ug)q(uf )q(ug) log p(y|f )duf dugdgdf

(S.17)

−

q(uf ) log q(uf )duf

−

q(ug) log q(ug)dug

(S.18)

(cid:123)(cid:122)
KL[q(uf )||p(uf )]

(cid:125)

(cid:123)(cid:122)
KL[q(ug)||p(ug)]

(cid:125)

(cid:90)

(cid:124)

(S.1)

(S.2)

(S.3)

(S.4)

(S.5)

(S.6)

(S.7)

(S.8)

(S.9)

(S.10)

(S.11)

(S.12)

(S.13)

(S.14)

(S.15)

(S.16)

Following the derivation of Hensman et al. (2015), this corresponds to maximizing the evidence lower bound
(ELBO) of equation (24):

(cid:90) (cid:90) (cid:90) (cid:90)

log p(y) ≥

log p(y|f )p(f |g, uf )q(uf )p(g|ug)q(ug)duf dugdgdf − KL[q(uf , ug)||p(uf , ug)]

(S.19)

= Eq(f ) log p(y|f ) − KL[q(uf , ug)||p(uf , ug)]

where we deﬁne

q(f ) =

p(f |g, uf )q(uf )p(g|ug)q(ug)duf dugdg

(cid:90) (cid:90) (cid:90)

(cid:90)

=

q(f |g)q(g)dg,

where the variational approximations are tractably

q(g) =

p(g|ug)q(ug)dug

=

N (g|KgnmK −1

gmmug, (cid:101)Kg)N (ug|mg, Sg)dug

= N (g|µg, Σg)

q(f |g) =

p(f |g, uf )q(uf )duf

(cid:90)

(cid:90)

(cid:90)

(cid:90)

=

N (f | diag(Φ(g))Kf nmK −1

f mmuf , Φ(g)Φ(g)T ◦ (cid:101)Kf )N (uf |mf , Sf )duf

= N (f | diag(Φ(g))µf , Φ(g)Φ(g)T ◦ Σf )

with

f mmmf
gmmmg

µf = Kf nmK −1
µg = KgnmK −1
Σf = Kf nn + Kf nmK −1
Σg = Kgnn + KgnmK −1

f mm(Sf − Kf mm)K −1
gmm(Sg − Kgmm)K −1

f mmKf mn
gmmKgmn.

The variational marginalizations q(g) and q(f |) follow from standard Gaussian identities4. Substituting the
variational marginalizations back to the ELBO results in

(cid:90) (cid:90)

log p(y) ≥

log p(y|f )q(f |g)q(g)df dg − KL[q(uf , ug)||p(uf , ug)].

(S.32)

Next, we marginalize the f from the ELBO. We additionally assume the likelihood p(y|f ) = (cid:81)N
factorises, which results in
(cid:90)

(cid:90)

i=1 p(yi|fi)

log p(y|f )q(f |g)df =

log N (y|f , σ2

yI)q(f |g)df

f

log N (yi|fi, σ2

y)q(fi|gi)dfi

log N (yi|Φ(gi)kT

f iK −1

f mmmf i, σ2

y) −

Φ(gi)2(kf ii + kT

f iK −1

f mm(Sf − Kf mm)K −1

f mmkf i)

(cid:111)

(cid:110)

1
2σ2
y

=

log N (yi|Φ(gi)µf i, σ2

y) −

1
2σ2
y

(cid:8)Φ(gi)2σ2

f i

(cid:9) ,

4See for instance Bishop (2006): Pattern recognition and Machine learning, Springer, Section 2.3.

=

=

N
(cid:88)

(cid:90)

i=1

N
(cid:88)

i=1

N
(cid:88)

i=1

(S.20)

(S.21)

(S.22)

(S.23)

(S.24)

(S.25)

(S.26)

(S.27)

(S.28)

(S.29)

(S.30)

(S.31)

(S.33)

(S.34)

(S.35)

(S.36)

where

µf i = [µf ]i = kT
f i = [Σf ]ii = kf ii + kT
σ2

f iK −1

f mmmf i

f iK −1

f mm(Sf − Kf mm)K −1

f mmkf i.

Substituting the above result into the ELBO results in

Eq(f ) log p(y|f ) =

q(g)

q(f |g) log p(y|f )df dg

(cid:90)

f

(cid:90)

g
(cid:90)

g

N
(cid:88)

i=1

N
(cid:88)

i=1

=

=

=

N
(cid:88)

i=1
(cid:90)

gi

log N (yi|Φ(gi)µf i, σ2

y) −

(cid:8)Φ(gi)2σ2

f i

(cid:9) q(g)dg

1
2σ2
y

log N (yi|Φ(gi)µf i, σ2

y) q(gi)dgi −

(cid:8)Φ(gi)2σ2

f i

(cid:9) q(gi)dgi

1
2σ2
y

N
(cid:88)

(cid:90)

i=1

gi

1
2σ2
y

N
(cid:88)

i=1

log N (yi|(cid:104)Φ(gi)(cid:105)q(gi)µf i, σ2

y) −

(cid:8)V ar[Φ(gi)](µf i)2(cid:9) −

(cid:8)(cid:104)Φ(gi)2(cid:105)q(gi)σ2

f i

(cid:9) .

1
2σ2
y

N
(cid:88)

i=1

The expectations (cid:104).(cid:105)q(gi) of CDF transformation of a random variable with univariate Gaussian distribution.
The analytical forms for these integrals can be written:

(cid:104)Φ(gi)(cid:105)q(gi) =

Φ(gi)q(gi)dgi

=

Φ(gi)N (gi|µgi, σ2




gi)dgi

= Φ



(cid:113)

µgi
1 + σ2
gi



V ar[Φ(gi)] =

(Φ(gi) − (cid:104)Φ(gi)(cid:105)q(gi))2q(gi)dgi

(cid:90)

(cid:90)

(cid:90)

(cid:90)







= Φ



(cid:113)

 − 2T



(cid:113)

µgi
1 + σ2
gi

µgi
1 + σ2
gi

,

(cid:113)

1 + 2 σ2
gi





 − Φ



(cid:113)


2



µgi
1 + σ2
gi

(cid:104)Φ(gi)2(cid:105)q(gi) =

Φ(gi)2q(gi)dgi





= Φ



(cid:113)

 − 2T



(cid:113)

µgi
1 + σ2
gi

µgi
1 + σ2
gi

,

(cid:113)

1 + 2 σ2
gi





1

1

where

µgi = [µg]i = kT
gi = [Σg]ii = Kgii + kT
σ2

giK −1

gmmmgi

giK −1

gmm(Sg − Kgmm)K −1

gmmkgi.

Owen’s T function is deﬁned as T (h, a) = φ(h) (cid:82) a
0

φ(hx)
1+x2 dx.

(S.37)

(S.38)

(S.39)

(S.40)

(S.41)

(S.42)

(S.43)

(S.44)

(S.45)

(S.46)

(S.47)

(S.48)

(S.49)

(S.50)

(S.51)

The ﬁnal evidence lower bound with the Kullback-Leibler terms is

p(y) ≥

log N (yi|(cid:104)Φ(gi)(cid:105)q(gi)µf i, σ2

y) −

(cid:0)V ar[Φ(gi)] µ2

f i + (cid:104)Φ(gi)2(cid:105)q(gi) σ2
f i

(cid:27)

(cid:1)

log |Kf mm| −

log |Sf | +

T r

(mf mf

T + Sf )K −1

f mm

(cid:105)

−

log |Kgmm| −

log |Sg| +

T r (cid:2)(mgmg

T + Sg)K −1

gmm

(cid:3) −

1
2
1
2

(cid:27)

m
2
(cid:27)
m
2

1
2σ2
y

(cid:104)

1
2
1
2

N
(cid:88)

(cid:26)

i=1

−

−

(cid:26) 1
2
(cid:26) 1
2

= LZiGP

B) The stochastic variational bound of the Gaussian process network

In GPRN a vector-valued output function y(x) ∈ RP with P outputs is modeled using vector-valued latent
functions f (x) ∈ RQ with Q latent values and mixing weights W (x) ∈ RP ×Q as

where for all q = 1, . . . , Q and p = 1, . . . , P we assume GP priors and additive zero-mean noises,

y(x) = W (x)[f (x) + (cid:15)] + ε,

fq(x) ∼ GP(0, Kf (x, x(cid:48)))
Wqp(x) ∼ GP(0, Kw(x, x(cid:48)))

(cid:15)q ∼ N (0, σ2
f )
εp ∼ N (0, σ2
y).

The subscripts are used to denote individual components of f and W with p and q indicating pth output
dimension and qth latent dimension respectively.

We begin by introducing the inducing variable augmentation for latent functions f (x) and mixing weights
W (x) with uf , zf = {ufq , zfq }Q

q=1 and uw, zw = {uwqp, zwqp}Q,P

q,p=1:

p(y, f , W, uf , uw) = p(y|f , W )p(f |uf )p(W |uw)p(uf )p(uw)

p(f |uf ) =

N (fq|Qfq ufq , (cid:101)Kfq )

p(W |uw) =

N (wqp|Qwqpuwqp, (cid:101)Kwqp)

p(uf ) =

N (ufq |0, Kfq,mm)

p(uw) =

N (uwqp|0, Kwqp,mm),

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

Qf = KfqnmK −1
fqmm
Qg = KwqpnmK −1
(cid:101)Kf = Kfqnn − KfqnmK −1
(cid:101)Kwqp = Kwqpnn − KwqpnmK −1

wqpmm

fqmmKfqmn

wqpmmKwqpmn.

where we have separate kernels K and extrapolation matrices Q for each component of W (x) and f (x) that
are of the form as given below:

(S.52)

(S.53)

(S.54)

(S.55)

(S.56)

(S.57)

(S.58)

(S.59)

(S.60)

(S.61)

(S.62)

(S.63)

(S.64)

(S.65)

(S.66)

(S.67)

(S.68)

(S.69)

Following the variational inference framework, we deﬁne the variational joint distribution as,

q(f , W, uf , uw) = p(f |uf )p(W |uw)q(uf )q(uw)

q(ufq ) =

N (ufq |mfq Sfq )

q(uwqp) =

N (uwqp|mwqp, Swqp),

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

where uwqp and ufq indicate the inducing points for functions Wqp(x) and fq(x), respectively. The ELBO
can be now stated as

log p(y) ≥ Eq(f ,W,uf ,uw) log p(y|f , W ) − KL[q(uf , uw)||p(uf , uw)]

(cid:90) (cid:90) (cid:90) (cid:90)

=

q(f , W, uf , uw) log p(y|f , W )df dW duf duw − KL[q(uf , uw)||p(uf , uw)]

Since the variational joint posterior decomposes as equation (S.70), we begin by marginalizing the inducing
distributions uf and uw,

q(f , W, uf , uw)df dW duf duw =

p(f |uf )q(uf )df duf

p(W |uw)q(uw)dW duw

(S.75)

uf

(cid:90)

(cid:90)

f
(cid:90)

f

(cid:90)

W

=

q(f )df

q(W )dW

(cid:90)

(cid:90)

W

uw

(cid:90) (cid:90) (cid:90) (cid:90)

where

with

q(f ) =

p(f |uf )q(uf )duf

N (fq|µfq , Σfq )

q(W ) =

p(W |uw)q(uw)duw

(cid:90)

Q
(cid:89)

(cid:90)

q=1

Q
(cid:89)

q=1
(cid:90)

Q,P
(cid:89)

(cid:90)

q,p=1

Q,P
(cid:89)

q,p=1

=

=

=

=

N (Wqp|µwqp, Σwqp)

N (fq|KfqnmK −1

fqmmufq , (cid:101)Kfq )N (ufq |mfq , Sfq )dufq

N (Wqp|KwqpnmK −1

wqpmmuwqp, (cid:101)Kwqp)N (uwqp|mwqp, Swqp)duwqp

µfq = KfqnmK −1
fqmmmfq
µwqp = KwqpnmK −1
wqpmmmwqp
Σfq = Kfqnn + KfqnmK −1
Σwqp = Kwqpnn + KwqpnmK −1

fqmm(Sfq − Kfqmm)K −1

fqmmKfqmn

wqpmm(Swqp − Kwqpmm)K −1

wqpmmKwqpmn.

Since the noise term ε is assumed to be isotropic Gaussian, density p(y|W, f ) factorises across all target

(S.70)

(S.71)

(S.72)

(S.73)

(S.74)

(S.76)

(S.77)

(S.78)

(S.79)

(S.80)

(S.81)

(S.82)

(S.83)

(S.84)

(S.85)

(S.86)

(S.87)

(S.88)

(S.89)

(S.90)

(S.91)

(S.92)

(S.93)

(S.94)

(cid:41)
(cid:17)

(S.95)

(S.96)

(S.97)

observations and dimensions. The expectation term in the ELBO then reduces to,

log p(y) ≥ Eq(W )Eq(f ) log p(y|f , W )
(cid:90) (cid:90)

=

N,P
(cid:88)

i,p=1

(cid:90)

The integral with respect to f can be now solved as

log N (yp,i|wT

p,ifi, ε2

p)q(fi, wp,i)dwp,idfi − KL[q(uf , uw)||p(uf , uw)].

log N (yp,i|wT

p,ifi, ε2

p)q(fi)dfi = log N (yp,i|wT

p,iµfi, ε2

p) −

T r(cid:2)wT

p,iΣfiwp,i

(cid:3)

= log N (yp,i|wT

p,iµfi, ε2

p) −

T r(cid:2)Σfiwp,iwT

p,i

(cid:3).

Next we can marginalize W from the above terms,

(cid:90)

log N (yp,i|wT

p,iµfi, ε2

p)q(wp,i)dwp,i = log N (yp,i|µT

wp,i

µfi, ε2

p) −

T r(cid:2)µT
fi

Σwq,iµfi

(cid:3)

1
2ε2
p
1
2ε2
p

1
2ε2
p

1
2ε2
p

Q
(cid:88)

q=1

= log N (yp,i|µT

µfi, ε2

p) −

wp,i

fq,iσ2
µ2

wqp,i

(cid:90)

1
2ε2
p

T r(cid:2)Σfiwp,iwT

p,i

(cid:3)q(wp,i)dwp,i =

T r(cid:2)Σfi(µwp,i µT

wp,i

+ Σwq,i)(cid:3)

1
2ε2
p

1
2ε2
p

=

Q
(cid:88)

(cid:16)

q=1

wqp,iσ2
µ2

fq,i + σ2

wqp,iσ2

fq,i

(cid:17)

.

Finally, adding the above results across all N observations and response dimensions P along with Gaussian
KL divergence terms, we get the ﬁnal lowerbound:

log p(y) ≥

N
(cid:88)

(cid:40) P

(cid:88)

i=1

p=1

(cid:16)

log N

yp,i|

µwqp,iµfq,i, ε2
p

Q
(cid:88)

q=1

(cid:17)

−

1
2ε2
p

Q,P
(cid:88)

(cid:16)

q,p=1

wqp,iσ2
µ2

fq,i + µ2

fq,iσ2

wqp,i + σ2

wqp,iσ2

fq,i

−

KL[q(uwqp, ufq )||p(uwqp, ufq )]

Q,P
(cid:88)

q,p

= Lgprn,

where µfq,i is the i’th element of µfq and σ2

fq,i is the i’th diagonal element of Σfq (similarly for Wqp’s).

C) The stochastic variational bound of the sparse Gaussian process network

Sparse GPRN is a modiﬁcation to standard GPRN where sparsity is added to the mixing matrix components.
This corresponds to the p’th output being a sparse mixture of the latent Q functions, i.e. it can eﬀectively
use any subset of the Q latent dimensions by having zeros in the mixing functions. The joint distribution
for the model can be written as,

p(y, f , W, g) = p(y|f , W )p(f )p(W |g)p(g),

(S.98)

where all individual components of latent function f and mixing matrix W are given GP priors. We encode
the sparsity terms g for all Q × P mixing functions Wqp(x) functions as

p(Wqp|gqp) = N (wqp|0, Φ(gqp)Φ(gqp)T ◦ Kw).

(S.99)

To introduce variational inference, the joint model is augmented with three sets of inducing variables for f ,
W and g. After marginalizing out the inducing variables similar to SVI for standard GPRN, the lower bund
for marginal likelihood can be written as

log p(y) ≥ Eq(f ,W,g) log p(y|f , W ) − KL[q(uf , uw, ug)||p(uf , uw, ug)].

(S.100)

Where the joint distribution in the variational expectation factorizes as q(f , W, g) = q(f )q(W |g)q(g). The
variational posterior after marginalizing inducing variables is written as,

q(f ) =

q(f |uf )q(uf )duf

=

N (fq|µfq , Σfq )

q(W ) =

q(W |uw)q(uw)duw

N (Wqp|µwqp, Σwqp)

q(g) =

q(g|ug)q(ug)dug

N (gqp|µgqp, Σgqp)

(cid:90)

Q
(cid:89)

q=1
(cid:90)

Q,P
(cid:89)

=

q,p=1
(cid:90)

Q,P
(cid:89)

=

q,p=1

with

µfq = KfqnmK −1
fqmmmfq
µwqp = KwqpnmK −1
wqpmmmwqp
µgqp = KgqpnmK −1
gqpmmmgqp
Σfq = Kfqnn + KfqnmK −1
Σwqp = Kwqpnn + KwqpnmK −1
Σgqp = Kgqpnn + KgqpnmK −1

fqmm(Sfq − Kfqmm)K −1

fqmmKfqmn

wqpmm(Swqp − Kwqpmm)K −1
gqpmm(Sgqp − Kgqpmm)K −1

gqpmmKgqpmn.

wqpmmKwqpmn

Similar to standard GPRN, with the isotropic Gaussian, density p(y|W, f ) factorizes across all target obser-
vations and dimensions. The expectation term in the ELBO then reduces to

log p(y) ≥ Eq(W |g)Eq(f )Eq(g) log p(y|f , W )

N,P
(cid:88)

(cid:90) (cid:90) (cid:90)

=

i,p=1

log N (yp,i|(wp,i ◦ gp,i)T fi, ε2

p)q(fi)q(wp,i|q(gp,i)q(gp,i)dwp,idfidgi − KL[q(uf , uw)||p(uf , uw)].

The integral with respect to f can be now solved as

(cid:90)

log N (yp,i|(wp,i ◦ gp,i)T fi, ε2

p)q(fi)dfi = log N (yp,i|(wp,i ◦ gp,i)T µfi, ε2

p) −

T r(cid:2)(wp,i ◦ gp,i)T Σfi(wp,i ◦ gp,i)(cid:3)

= log N (yp,i|(wp,i ◦ gp,i)T µfi, ε2

p) −

T r(cid:2)Σfi(gp,igT

p,i ◦ wp,iwT

p,i)(cid:3).

1
2ε2
p

1
2ε2
p

(S.101)

(S.102)

(S.103)

(S.104)

(S.105)

(S.106)

(S.107)

(S.108)

(S.109)

(S.110)

(S.111)

(S.112)

(S.113)

(S.114)

(S.115)

(S.116)

(cid:90)

1
2ε2
p

(cid:90)

(cid:90)

1
2ε2
p

Next, by integrating individual terms with respect to W we get

(cid:90)

log N (yp,i|(wp,i ◦ gp,i)T µfi, ε2

p)q(wp,i)dwp,i = log N (yp,i|(µwp,i ◦ gp,i)T µfi, ε2

p) −

1
2ε2
p

T r(cid:2)µT
fi

(gp,igT

p,i ◦ Σwq,i)µfi

(cid:3)

T r(cid:2)Σfi(gp,igT

p,i ◦ wp,iwT

p,i)(cid:3) =

T r(cid:2)Σfi(gp,igT

p,i ◦ (µwp,i µT

wp,i

+ Σwq,i))(cid:3)

1
2ε2
p

Finally, integrating all the above terms with respect to g, we get

log N (yp,i|(µwp,i ◦ gp,i)T µfi, ε2

p)q(gp,i)dgp,i = log N (yp,i|(µwp,i ◦ (cid:104)Φ(gp,i)(cid:105))T µfi, ε2
p)

(S.119)

T r(cid:2)µT
fi

(gp,igT

p,i ◦ Σwq,i)µfi

(cid:3)q(gp,i)dgp,i =

T r(cid:2)µT
fi

(cid:0)((cid:104)Φ(gp,i)(cid:105)(cid:104)Φ(gp,i)(cid:105)T + V ar[Φ(gp,i)]) ◦ Σwq,i

(cid:1) µfi

(cid:3)

(cid:90)

1
2ε2
p

T r(cid:2)Σfi(gp,igT

p,i ◦ (µwp,i µT

wp,i

+ Σwq,i ))(cid:3)q(gp,i)dgp,i =

((cid:104)Φ(gp,i)(cid:105)(cid:104)Φ(gp,i)(cid:105)T + V ar[Φ(gp,i)]) ◦ µwp,i µT

wp,i

(cid:17) (cid:3)

−

1
2ε2
p

T r(cid:2)µT
fi

(µwp,i µT

wp,i

◦ V ar[Φ(gp,i)])µfi]

(cid:16)

= log N

yp,i|

µwqp,iµgqp,iµfq,i, ε2
p

(cid:17)

Q
(cid:88)

q=1

−

1
2ε2
p

Q
(cid:88)

(cid:16)

q=1

gqp,iµ2
σ2

fq,iµ2

wqp,i

(cid:17)

=

(µ2

gqp,i + σ2

gqp,i)µ2

fq,iσ2

wqp,i

(cid:17)

1
2ε2
p

1
2ε2
p

1
2ε2
p

Q
(cid:88)

(cid:16)

q=1

(cid:16)

T r(cid:2)Σfi

+

1
2ε2
p

=

1
2ε2
p

Q
(cid:88)

(cid:16)

q=1

T r(cid:2)Σfi

(cid:0)((cid:104)Φ(gp,i)(cid:105)(cid:104)Φ(gp,i)(cid:105)T + V ar[Φ(gp,i)]) ◦ Σwq,i

(cid:1) (cid:3)

(µ2

gqp,i + σ2

gqp,i)(µ2

wqp,iσ2

fq,i + σ2

wqp,iσ2

Adding above results across all the observations N and output dimensions P , we retrieve the ﬁnal evidence
lower bound

p(y) ≥

N
(cid:88)

(cid:40) P

(cid:88)

i=1

p=1

(cid:16)

log N

yp,i|

µwqp,iµgqp,iµfq,i, ε2
p

(cid:17)

Q
(cid:88)

q=1

Q,P
(cid:88)

(cid:16)

−

q,p=1

(µ2

gqp,i + σ2

gqp,i) · (µ2

wqp,iσ2

fq,i + µ2

fq,iσ2

wqp,i + σ2

wqp,iσ2

gqp,iµ2
σ2

fq,iσ2

wqp,i

(cid:17)
fq,i)

−

Q,P
(cid:88)

(cid:16)

q,p=1

−

KL[q(ufq , uwqp, ugqp)||p(ufq , uwqp, ugqp)]

Q,P
(cid:88)

q,p

= Lsgprn.

(S.117)

(S.118)

(S.120)

(S.121)

(S.122)

(S.123)

(S.124)

(S.125)

(S.126)

(cid:17)
fq,i)

.

(S.127)

(S.128)

(cid:41)

(cid:17)

(S.129)

(S.130)

(S.131)

Variational zero-inﬂated Gaussian processes with sparse kernels

8
1
0
2
 
r
a

M
 
3
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
6
3
0
5
0
.
3
0
8
1
:
v
i
X
r
a

Pashupati Hegde

Markus Heinonen

Samuel Kaski

Helsinki Institute for Information Technology HIIT
Department of Computer Science, Aalto University

Abstract

Zero-inﬂated datasets, which have an ex-
cess of zero outputs, are commonly en-
countered in problems such as climate or
rare event modelling. Conventional ma-
chine learning approaches tend to overesti-
mate the non-zeros leading to poor perfor-
mance. We propose a novel model family
of zero-inﬂated Gaussian processes (ZiGP)
for such zero-inﬂated datasets, produced
by sparse kernels through learning a la-
tent probit Gaussian process that can zero
out kernel rows and columns whenever the
signal is absent. The ZiGPs are particu-
larly useful for making the powerful Gaus-
sian process networks more interpretable.
We introduce sparse GP networks where
variable-order latent modelling is achieved
through sparse mixing signals. We derive
the non-trivial stochastic variational infer-
ence tractably for scalable learning of the
sparse kernels in both models. The novel
output-sparse approach improves both pre-
diction of zero-inﬂated data and inter-
pretability of latent mixing models.

1

INTRODUCTION

Zero-inﬂated quantitative datasets with overabun-
dance of zero output observations are common in
many domains, such as climate and earth sciences
(Enke and Spekat, 1997; Wilby, 1998; Charles et al.,
2004), ecology (del Saz-Salazar and Rausell-K¨oster,
2008; Ancelet et al., 2009), social sciences (Bohn-
ing et al., 1997), and in count processes (Barry and
Welsh, 2002). Traditional regression modelling of
such data tends to underestimate zeros and overes-
timate nonzeros (Andersen et al., 2014).

A conventional way of forming zero-inﬂated mod-
els is to estimate a mixture of a Bernoulli “on-oﬀ”
process and a Poisson count distribution (Johnson
and Kotz, 1969; Lambert, 1992). In hurdle models
a binary “on-oﬀ” process determines whether a hur-
dle is crossed, and the positive responses are gov-
erned by a subsequent process (Cragg, 1971; Mul-
lahy, 1986). The hurdle model is analogous to ﬁrst
performing classiﬁcation and training a continuous
predictor on the positive values only, while the zero-
inﬂated model would regress with all observations.
Both stages can be combined for simultaneous clas-
siﬁcation and regression Abraham and Tan (2010).

Gaussian process models have not been proposed
for zero-inﬂated datasets since their posteriors are
Gaussian, which are ill-ﬁtted for zero predictions.
A suite of Gaussian process models have been pro-
posed for partially related problems, such as mixture
models (Tresp, 2001; Rasmussen and Ghahramani,
2002; L´azaro-Gredilla et al., 2012) and change point
detection (Herlands et al., 2016). Structured spike-
and-slab models place smoothly sparse priors over
the structured inputs (Andersen et al., 2014).

In contrast to other approaches, we propose a
Bayesian model that learns the underlying latent
prediction function, whose covariance is sparsiﬁed
through another Gaussian process switching be-
tween the ‘on’ and ‘oﬀ’ states, resulting in an zero-
inﬂated Gaussian process model. This approach
introduces a tendency of predicting exact zeros
to Gaussian processes, which is directly useful in
datasets with excess zeros.

A Gaussian process network (GPRN) is a latent
signal framework where multi-output data are ex-
plained through a set of latent signals and mixing
weight Gaussian processes (Wilson et al., 2012). The
standard GPRN tends to have dense mixing that
combines all latent signals for all latent outputs. By

applying the zero-predicting Gaussian processes to
latent mixture models, we introduce sparse GPRNs
where latent signals are mixed with sparse instead
of dense mixing weight functions. The sparse model
induces variable-order mixtures of latent signals re-
sulting in simpler and more interpretable models.
We demonstrate both of these properties in our ex-
periments with spatio-temporal and multi-output
datasets.

Main contributions. Our contributions include

1. A novel zero-inﬂated Gaussian process formal-
ism consisting of a latent Gaussian process and
a separate ‘on-oﬀ’ probit-linked Gaussian pro-
cess that can zero out rows and columns of the
model covariance. The novel sparse kernel adds
to GPs the ability to predict zeros.

2. Novel stochastic variational inference (SVI) for
such sparse probit covariances, which in gen-
eral are intractable due to having to compute
expectations of GP covariances with respect to
probit-linked processes. We derive the SVI for
learning both of the underlying processes.

3. A solution to the stochastic variational infer-
ence for conventional Gaussian process net-
works (GPRN) improving the earlier diago-
nalized mean-ﬁeld approximation (Nguyen and
Bonilla, 2013) by taking the covariances fully
into account.

4. A novel sparse GPRN with an on-oﬀ process
in the mixing matrices leading to sparse and
variable-order mixtures of latent signals.

5. A solution to the stochastic variational infer-
ence of sparse GPRN where the SVI is derived
for the network of full probit-linked covariances.

TensorFlow
these methods

implementation
Python
The
of
at
is
github.com/hegdepashupati/zero-inflated-gp
and at github.com/hegdepashupati/gprn-svi.

available

publicly

2 GAUSSIAN PROCESSES

We begin by introducing the basics of conventional
Gaussian processes. Gaussian processes (GP) are a
family of non-parametric, non-linear Bayesian mod-
els (Rasmussen and Williams, 2006). Assume a
dataset of n inputs X = (x1, . . . , xn) with xi ∈ RD
and noisy outputs y = (y1, . . . , yn) ∈ Rn. The ob-
servations y = f (x)+ε are assumed to have additive,

Figure 1: Illustration of a zero-inﬂated GP (a) and
standard GP regression (b). The standard approach
is unable to model sudden loss of signal (at 4 . . . 5)
and signal close to zero (at 0 . . . 1 and 7 . . . 9).

zero mean noise ε ∼ N (0, σ2
prior on the latent function f (x),

y) with a zero-mean GP

f (x) ∼ GP (0, K(x, x(cid:48))) ,

(1)

which deﬁnes a distribution over functions f (x)
whose mean and covariance are

E[f (x)] = 0
cov[f (x), f (x(cid:48))] = K(x, x(cid:48)).

(2)

(3)

Then for any collection of inputs X, the function
values follow a multivariate normal distribution f ∼
N (0, KXX ), where f = (f (x1), . . . , f (xN ))T ∈ Rn,
and where KXX ∈ Rn×n with [KXX ]ij = K(xi, xj).
The key property of Gaussian processes is that they
encode functions that predict similar output values
f (x), f (x(cid:48)) for similar inputs x, x(cid:48), with similarity
determined by the kernel K(x, x(cid:48)). In this paper we
assume the Gaussian ARD kernel

K(x, x(cid:48)) = σ2

f exp

−



1
2

D
(cid:88)

j=1

j)2

(xj − x(cid:48)
(cid:96)2
j



 ,

(4)

with a signal variance σ2
lengthscale (cid:96)1, . . . , (cid:96)D parameters.

f and dimension-speciﬁc

inference of

The
θ =
(σy, σf , (cid:96)1, . . . , (cid:96)D) is performed commonly by max-
imizing the marginal likelihood

the hyperparameters

(cid:90)

p(y|θ) =

p(y|f )p(f |θ)df ,

(5)

which results in a convenient marginal likelihood
called evidence, p(y|θ) = N (y|0, KXX + σ2
yI) for
a Gaussian likelihood.

The Gaussian process deﬁnes a univariate nor-
mal predictive posterior distribution f (x)|y, X ∼
N (µ(x), σ2(x)) for an arbitrary input x with the
prediction mean and variance1

where

µ(x) = KxX (KXX + σ2
yI)−1y,
σ2(x) = Kxx − KxX (KXX + σ2

yI)−1KXx,

(6)

(7)

xX ∈ Rn is the kernel column vector
where KXx = K T
over pairs X ×x, and Kxx = K(x, x) ∈ R is a scalar.
The predictions µ(x) ± σ(x) come with uncertainty
estimates in GP regression.

3 ZERO-INFLATED GAUSSIAN

PROCESSES

Figure 2:
Illustration of the zero-inﬂated GP (a)
and the sparse kernel (b) composed of a smooth
latent function (c,d) ﬁltered by a probit support
function (e,f ), which is induced by the underlying
latent sparsity (g,h).

We introduce zero-inﬂated Gaussian processes that
have – in contrast to standard GP’s – a tendency to
produce exactly zero predictions (See Figure 1). Let
g(x) denote the latent “on-oﬀ” state of a function
f (x). We assume GP priors for both functions with
a joint model

p(y, f , g) = p(y|f )p(f |g)p(g),

(8)

1In the following we omit the implicit conditioning on

data inputs X for clarity.

p(y|f ) = N (y|f , σ2
p(f |g) = N (f |0, Φ(g)Φ(g)T ◦ Kf )

yI)

p(g) = N (g|β1, Kg).

(9)

(10)

(11)

The sparsity values g(x) are squashed between 0 and
1 through a standard Normal cumulative distribu-
tion, or a probit link function, Φ : R → [0, 1]

Φ(g) =

φ(τ )dτ =

1 + erf

,

(12)

(cid:90) g

−∞

(cid:18)

1
2

(cid:19)(cid:19)

(cid:18) g
√
2

2 τ 2

e− 1

where φ(τ ) = 1√
is the standard normal den-
2π
sity function. The structured probit sparsity Φ(g)
models the “on-oﬀ” smoothly due to the latent spar-
sity function g having a GP prior with prior mean
β. The latent function f is modeled throughout but
it is only visible during the “on” states. This mask-
ing eﬀect has similarities to both zero-inﬂated and
hurdle models. The underlying latent function f is
learned from only non-zero data similarly to in hur-
dle models, but the function f is allowed to predict
zeros similarly to zero-inﬂated models.

is the sparse probit-
The key part of our model
sparsiﬁed covariance Φ(g)Φ(g)T ◦ K where the “on-
oﬀ” state Φ(g) has the ability to zero out rows and
columns of the kernel matrix at the “oﬀ” states (See
Figure 2f for the probit pattern Φ(g)Φ(g)T and Fig-
ure 2b for the resulting sparse kernel). As the spar-
sity g(x) converges towards minus inﬁnity, the probit
link Φ(g(x)) approaches zero, which leads the func-
tion distribution approaching N (fi|0, 0), or fi = 0.
Numerical problems are avoided since in practice
Φ(g) > 0, and due to the conditioning noise vari-
ance term σ2

y > 0.

The marginal likelihood of the zero-inﬂated Gaus-
sian process is intractable due to the probit-
sparsiﬁcation of the kernel. We derive a stochastic
variational Bayes approximation, which we show to
be tractable due to the choice of using the probit
link function.

3.1 STOCHASTIC VARIATIONAL

INFERENCE

Inference for standard Gaussian process models is
diﬃcult to scale as complexity grows with O(n3) as
a function of the data size n. Titsias (2009) pro-
posed a variational inference approach for GPs using
m < n inducing variables, with a reduced computa-
tional complexity of O(m3) for m inducing points.
The novelty of this approach lies in the idea that

the locations and values of inducing points can be
treated as variational parameters, and optimized.
Hensman et al. (2013, 2015) introduced more eﬃ-
cient stochastic variational inference (SVI) with fac-
torised likelihoods that has been demonstrated with
up to billion data points (Salimbeni and Deisenroth,
2017). This approach cannot be directly applied to
sparse kernels due to having to compute expectation
of the probit product in the covariance. We derive
the SVI bound tractably for the zero-inﬂated model
and its sparse kernel, which is necessary in order to
apply the eﬃcient parameter estimation techniques
with automatic diﬀerentiation with frameworks such
as TensorFlow (Abadi et al., 2016).

We begin by applying the inducing point augmen-
tations f (zf ) = uf and g(zg) = ug for both
the latent function f (·) and the sparsity function
g(·). We place m inducing points uf 1, . . . uf m
and ug1, . . . ugm for the two functions. The aug-
mented joint distribution is p(y, f , g, uf , ug) =
p(y|f )p(f |g, uf )p(g|ug)p(uf )p(ug), where2

We minimize the Kullback-Leibler divergence be-
tween the true augmented posterior p(y, f , g, uf , ug)
and the variational distribution q(f , g, uf , ug),
which is equivalent to solving the following evi-
dence lower bound (as shown by e.g. Hensman et al.
(2015)):

log p(y) ≥ Eq(f ) log p(y|f ) − KL[q(uf , ug)||p(uf , ug)],
(24)

q(f ) =

p(f |g, uf )q(uf )p(g|ug)q(ug)duf dugdg

where we deﬁne

(cid:90) (cid:90) (cid:90)

(cid:90)

=

q(f |g)q(g)dg,

where the variational approximations are tractably

(cid:90)

(cid:90)

q(g) =

p(g|ug)q(ug)dug

= N (g|µg, Σg)

q(f |g) =

p(f |g, uf )q(uf )duf

p(f |g, uf ) = N (f | diag(Φ(g))Qf uf , Φ(g)Φ(g)T ◦ (cid:101)Kf )
(13)

with

= N (f | diag(Φ(g))µf , Φ(g)Φ(g)T ◦ Σf )

(14)

(15)

(16)

(17)

(18)

(19)

(20)

p(g|ug) = N (g|Qgug, (cid:101)Kg)
p(uf ) = N (uf |0, Kf mm)
p(ug) = N (ug|0, Kgmm)

and where

f mm

Qf = Kf nmK −1
Qg = KgnmK −1
(cid:101)Kf = Kf nn − Kf nmK −1
(cid:101)Kg = Kgnn − KgnmK −1

gmm

f mmKf mn
gmmKgmn.

We denote the kernels for functions f and g by the
corresponding subscripts. The kernel Kf nn is be-
tween all n data points, the kernel Kf nm is between
all n datapoints and m inducing points, and the ker-
nel Kf mm is between all m inducing points (similarly
for g as well).

Next we use the standard variational approach by
introducing approximative variational distributions
for the inducing points,

q(uf ) = N (uf |mf , Sf )
q(ug) = N (ug|mg, Sg)
where Sf , Sg ∈ Rm×m are square positive semi-
deﬁnite matrices. The variational joint posterior is

(22)

(21)

q(f , g, uf , ug) = p(f |g, uf )p(g|ug)q(uf )q(ug). (23)

2We drop the implicit conditioning on z’s for clarity.

µf = Qf mf
µg = Qgmg
Σf = Kf nn + Qf (Sf − Kf mm)QT
f
Σg = Kgnn + Qg(Sg − Kgmm)QT
g .

We additionally assume the likelihood p(y|f ) =
(cid:81)N
i=1 p(yi|fi) factorises. We solve the ﬁnal ELBO
of equations (24) and (25) as (See Supplements for
detailed derivation)

N
(cid:88)

(cid:110)

i=1

−

1
2σ2
y

LZI =

log N (yi|(cid:104)Φ(gi)(cid:105)q(gi)µf i, σ2
y)

(cid:0)Var[Φ(gi)]µ2

f i + (cid:104)Φ(gi)2(cid:105)q(gi)σ2
f i

− KL[q(uf )||p(uf )] − KL[q(ug)||p(ug)],

where µf i is the i’th element of µf and σ2
f i is the
i’th diagonal element of Σf (similarly with g). The
expectations are tractable,

(cid:104)Φ(gi)(cid:105)q(gi) = Φ(λgi),

λgi =

(cid:113)

(cid:104)Φ(gi)2(cid:105)q(gi) = Φ(λgi) − 2T

λgi,

Var[Φ(gi)] = Φ(λgi) − 2T

λgi,

− Φ(λgi)2.

µgi
1 + σ2
gi
(cid:19)

λgi
µgi
λgi
µgi

(cid:19)

(cid:18)

(cid:18)

(25)

(26)

(27)

(28)

(29)

(30)

(31)

(32)

(cid:1) (cid:111)

(33)

(34)

(35)

The Owen’s T function T (a, b) = φ(a) (cid:82) b
φ(aτ )
1+τ 2 dτ
0
(Owen, 1956) has eﬃcient numerical solutions in
practise (Pateﬁeld and Tandy, 2000).

The ELBO is considerably more complex than the
standard stochastic variational bound of a Gaus-
sian process (Hensman et al., 2013), due to the
probit-sparsiﬁed covariance. The bound is likely
only tractable for the choice of probit link func-
tion Φ(g), while other link functions such as the
logit would lead to intractable bounds necessitating
slower numerical integration (Hensman et al., 2015).

We optimize the Lzi with stochastic gradient as-
cent techniques with respect to the inducing lo-
inducing value means mf , mg and
cations zg, zf ,
the sparsity prior mean β,
covariances Sf , Sg,
the noise variance σ2
y, the signal variances σf , σg,
and ﬁnally the dimensions-speciﬁc lengthscales
(cid:96)f 1, . . . , (cid:96)f D; (cid:96)g1, . . . , (cid:96)gD of the Gaussian ARD ker-
nel.

4 GAUSSIAN PROCESS

NETWORK

The Gaussian Process Regression Networks (GPRN)
framework by Wilson et al. (2012) is an eﬃcient
model for multi-target regression problems, where
each individual output is a linear but non-stationary
combination of shared latent functions. Formally, a
vector-valued output function y(x) ∈ RP with P
outputs is modeled using vector-valued latent func-
tions f (x) ∈ RQ with Q latent values and mixing
weights W (x) ∈ RP ×Q as

y(x) = W (x)[f (x) + (cid:15)] + ε,

(36)

where for all q = 1, . . . , Q and p = 1, . . . , P we as-
sume GP priors and additive zero-mean noises,

fq(x) ∼ GP(0, Kf (x, x(cid:48)))
Wqp(x) ∼ GP(0, Kw(x, x(cid:48)))

(cid:15)q ∼ N (0, σ2
f )
εp ∼ N (0, σ2
y).

(37)

(38)

(39)

(40)

The subscripts are used to denote individual compo-
nents of f and W with p and q indicating pth output
dimension and qth latent dimension, respectively.
We assume shared latent and output noise variances
σ2
f , σ2
y without loss of generality. The distributions
of both functions f and W have been inferred either
with variational EM (Wilson et al., 2012) or by vari-
ational mean-ﬁeld approximation with diagonalized
latent and mixing functions (Nguyen and Bonilla,
2013).

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

4.1 STOCHASTIC VARIATIONAL

INFERENCE

Here, we ﬁrst extend the works of Wilson et al.
(2012) and Nguyen and Bonilla (2013) by intro-
ducing the currently missing SVI bounds for the
standard GPRN, and then propose the novel sparse
GPRN model, and solve its SVI bounds as well, in
the following section.

We begin by introducing the inducing variable aug-
mentation technique for latent functions f (x) and
mixing weights W (x) with uf , zf = {ufq , zfq }Q
and uw, zw = {uwqp, zwqp}Q,P

q,p=1:

q=1

p(y, f , W, uf , uw)

(41)
= p(y|f , W )p(f |uf )p(W |uw)p(uf )p(uw)

p(f |uf ) =

N (fq|Qfq ufq , (cid:101)Kfq )

(42)

p(W |uw) =

N (wqp|Qwqpuwqp, (cid:101)Kwqp)

(43)

p(uf ) =

N (ufq |0, Kfq,mm)

(44)

p(uw) =

N (uwqp|0, Kwqp,mm),

(45)

where we have separate kernels K and extrapolation
matrices Q for each component of W (x) and f (x)
that are of the same form as in equations (17–20).
The w is a vectorised form of W . The variational
approximation is then

q(f , W, uf , uw) = p(f |uf )p(W |uw)q(uf )q(uw)

q(ufq ) =

N (ufq |mfq , Sfq )

(46)

(47)

q(uwqp) =

N (uwqp|mwqp, Swqp),

(48)

where uwqp and ufq indicate the inducing points for
the functions Wqp(x) and fq(x), respectively. The
ELBO can be now stated as

log p(y) ≥ Eq(f ,W ) log p(y|f , W )

(49)

− KL[q(uf , uw)||p(uf , uw)],

where the variational distributions decompose as
q(f , W ) = q(f )q(W ) with marginals of the same form

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

as in equations (28–31),

We extend the GPRN with probit sparsity for the
mixing matrix W , resulting in a joint model

q(f ) =

q(f |uf )q(uf )duf = N (f |µf , Σf )

(50)

p(y, f , W, g) = p(y|f , W )p(f )p(W |g)p(g),

(54)

(cid:90)

(cid:90)

q(W ) =

q(W |uw)q(uw)duw = N (w|µw, Σw).

(51)

where all individual components of the latent func-
tion f and mixing matrix W are given GP priors.
We encode the sparsity terms g for all the Q × P
mixing functions Wqp(x) as

Since the noise term ε is assumed to be isotropic
Gaussian, the density p(y|W, f ) factorises across all
target observations and dimensions. The expecta-
tion term in equation (49) then reduces to solving
the following integral for the ith observation and pth
target dimension,

N,P
(cid:88)

(cid:90) (cid:90)

i,p=1

log N (yp,i|wT

p,ifi, σ2

y)q(fi, wp,i)dwp,idfi.

p(Wqp|gqp) = N (wqp|0, Φ(gqp)Φ(gqp)T ◦ Kw).

(55)

To introduce variational inference, the joint model
is augmented with three sets of inducing variables
for f , W and g. After marginalizing out the induc-
ing variables as in equations (25–27), the marginal
likelihood can be written as

(52)

log p(y) ≥ Eq(f ,W,g) log p(y|f , W )

(56)

− KL[q(uf , uw, ug)||p(uf , uw, ug)].

Q,P
(cid:88)

(cid:16)

q,p=1

−

−

1
2σ2
y

Q,P
(cid:88)

q,p

The above integral has a closed form solution result-
ing in the ﬁnal ELBO as (See Supplements)

Lgprn =

N
(cid:88)

(cid:40) P

(cid:88)

i=1

p=1

(cid:16)

log N

yp,i|

µwqp,iµfq,i, σ2
y

(cid:17)

Q
(cid:88)

q=1

wqp,iσ2
µ2

fq,i + µ2

fq,iσ2

wqp,i + σ2

wqp,iσ2

fq,i

(cid:41)
(cid:17)

KL[q(uwqp, ufq )||p(uwqp, ufq )],

(53)

where µfq,i is the i’th element of µfq and σ2
fq,i is
the i’th diagonal element of Σfq (similarly for the
Wqp’s).

5 SPARSE GAUSSIAN PROCESS

NETWORK

In this section we demonstrate how zero-inﬂated
GPs can be used as plug-in components in other
In particular, we propose a sig-
standard models.
niﬁcant modiﬁcation to GPRN by adding sparsity
to the mixing matrix components. This corresponds
to each of the p outputs being a sparse mixture of
the latent Q functions, i.e. they can eﬀectively use
any subset of the Q latent dimensions by having ze-
ros for the rest in the mixing functions. This makes
the mixture more easily interpretable, and induces
a variable number of latent functions to explain the
output of each input x. The latent function f can
also be sparsiﬁed, with a derivation analogous to the
derivation below.

The joint distribution in the variational expecta-
tion factorizes as q(f , W, g) = q(f )q(W |g)q(g). Also,
with a Gaussian noise assumption, the expectation
term factories across all the observations and tar-
get dimensions. The key step reduces to solving the
following integrals:

N,P
(cid:88)

(cid:90) (cid:90) (cid:90)

i,p=1

log N (yp,i|(wp,i ◦ gp,i)T fi, σ2
y)

(57)

· q(fi, wp,i, gp,i)dwp,idfidgp,i.

The above integral has a tractable solution leading
to the ﬁnal sparse GPRN evidence lower bound (See
Supplements)

Lsgprn =

N
(cid:88)

(cid:40) P

(cid:88)

i=1

p=1

(cid:16)

log N

yp,i|

µwqp,iµgqp,iµfq,i, σ2
y

(cid:17)

Q
(cid:88)

q=1

(58)

wqp,iσ2

(cid:17)
fq,i)

−

1
2σ2
y

Q,P
(cid:88)

(cid:16)

q,p=1

(µ2

gqp,i + σ2

gqp,i)

· (µ2

wqp,iσ2

fq,i + µ2

Q,P
(cid:88)

(cid:16)

q,p=1

gqp,iµ2
σ2

fq,iµ2

wqp,i

fq,iσ2
wqp,i + σ2
(cid:41)

(cid:17)

KL[q(ufq , uwqp, ugqp)||p(ufq , uwqp, ugqp)],

−

−

1
2σ2
y

Q,P
(cid:88)

q,p

where µfq,i, µwqp,i are the variational expectation
means for f (·), W (·) as in equations (28, 29), µgqp,i is
the variational expectation mean of g(·) as in equa-
tion (33), and analogously for the variances.

Table 1: Results for the precipitation dataset over
baseline (Zero; majority voting),
four competing
methods and the proposed method ZiGP on test
data. The columns list both quantitative and qual-
itative performance criteria, best performance is
boldfaced.

Model
Zero
GPc
GPr
GPcr
GPcr(cid:54)=0
ZiGP

RMSE MAE
0.104
0.615
-
-
0.159
0.569
0.589
0.102
0.575
0.101
0.121
0.561

F1
0.000
0.367
0.401
0.366
0.358
0.448

Acc.
0.898
0.911
0.750
0.911
0.912
0.861

Prec. Recall
0.000
0.675
0.266
0.679
0.712
0.381

0.000
0.252
0.817
0.251
0.240
0.558

6 EXPERIMENTS

First we demonstrate how the proposed method can
be used for regression problems with zero-inﬂated
targets. We do that both on a simulated dataset
and for real-world climate modeling scenarios on
a Finnish rain precipitation dataset with approx-
imately 90% zeros. Finally, we demonstrate the
GPRN model and how it improves both the inter-
pretability and predictive performance in the JURA
geological dataset.

We use the squared exponential kernel with ARD in
all experiments. All the parameters including in-
ducing locations, values and variances and kernel
parameters were learned through stochastic Adam
optimization (Kingma and Ba, 2014) on the Tensor-
Flow (Abadi et al., 2016) platform.

We compare our approach ZiGP to baseline Zero
voting, to conventional Gaussian process regression
(GPr) and classiﬁcation (GPc) with SVI approxi-
mations from the GPﬂow package (Matthews et al.,
2017). Finally, we also compare to ﬁrst classifying
the non-zeros, and successively applying regression
either to all data points (GPcr), or to only pre-
dicted non-zeros (GPcr(cid:54)=0, hurdle model).

We record the predictive performance by consider-
ing mean squared error and mean absolute error. We
also compare the models’ ability to predict true ze-
ros with F1, accuracy, precision, and recall of the
optimal models.

6.1 SPATIO-TEMPORAL DATASET

Zero-inﬂated cases are commonly found in clima-
tology and ecology domains.
In this experiment
we demonstrate the proposed method by model-
ing precipitation in Finland3. The dataset consists

3Data

be
ilmatieteenlaitos.fi/

can

found

at

http://en.

Figure 3: ZiGP model ﬁt on the precipitation
dataset. Sample of the actual data (a) against the
sparse rain function estimate (b), with the probit
support function (c) showing the rain progress.

of hourly quantitative non-negative observations of
precipitation amount across 105 observatory loca-
tions in Finland for the month of July 2018. The
dataset contains 113015 datapoints with approxi-
mately 90% zero precipitation observations. The
latitude, longi-
data inputs are three-dimensional:
tude and time. Due to the size of the data, this ex-
periment illustrates the scalability of the variational
inference.

We randomly split 80% of the data for training and
the rest 20% for testing purposes. We split across
time only, such that at a single measurement time,
all locations are simultaneously either in the training
set, or in the test set.

We further utilize the underlying spatio-temporal
grid structure of the data to perform inference in an
eﬃcient manner by Kronecker techniques (Saatchi,
2011). All the kernels for latent processes are as-
sumed to factorise as K = Kspace ⊗ Ktime which
allows placing inducing points independently on spa-
tial and temporal grids.

Figure 4: The distribution of errors with the rain
dataset with the ZiGP and the GPr. The zero-
inﬂated GP achieves much higher number of perfect
(zero) predictions.

Figure 3 depicts the components of the zero-inﬂated
GP model on the precipitation dataset. As shown
in panel (c), the latent support function models the
presence or absence of rainfall. It smoothly follows
the change in rain patterns across hourly observa-
tions. The amount of precipitation is modeled by
the other latent process and the combination of these
two results in sparse predictions. Figure 4 shows
that the absolute error distribution is remarkably
better with the ZiGP model due to it identifying
the absence of rain exactly. While both models ﬁt
the high rainfall regions well, for zero and near-zero
regions GPr does not reﬁne its small errors. Table
1 indicates that the ZiGP model achieves the lowest
mean square error, while also achieving the highest
F1 score that takes into account the class imbalance,
which biases the elementary accuracy, precision and
recall quantities towards the majority class.

6.2 MULTI-OUTPUT PREDICTION

In this experiment we model the multi-response Jura
dataset with the sparse Gaussian process regression
network sGPRN model and compare it with stan-
dard GPRN as baseline. Jura contains concentra-
tion measurements of cadmium, nickel and zinc met-
als in the region of Swiss Jura. We follow the ex-
perimental procedure of Wilson et al. (2012) and
Nguyen and Bonilla (2013). The training set con-
sists of n = 259 observations across D = 2 dimen-
sional geo-spatial locations, and the test set consists
of 100 separate locations. For both models we use
Q = 2 latent functions with the stochastic varia-
tional inference techniques proposed in this paper.
Sparse GPRN uses a sparsity inducing kernel in the
mixing weights. The locations of inducing points for

Figure 5: The sparse GPRN model ﬁt on the Jura
dataset with 11 inducing points. The Q = 2 (dense)
latent functions (a) are combined with the 3 × 2
sparse mixing functions (b) into the P = 3 output
predictions (c). The real data are shown in (d).
The white mixing regions are estimated ‘oﬀ’.

the weights W (x) and the support g(x) are shared.
The kernel length-scales are given a gamma prior
with the shape parameter α = 0.3 and rate param-
eter β = 1.0 to induce smoothness. We train both
the models 30 times with random initialization.

Figure 6: The sparse probit support (a) and latent
functions (b) of the weight function W (x) of the
optimized sparse GPRN model. The black regions
of (a) show regional activations, while the white re-
gions show where the latent functions are ‘oﬀ’. The
elementwise product of the support and weight func-
tions is indicated in the Figure 5b).

Table 6.2 shows that our model performs better than
the state-of-the-art SVI-GPRN, both with m = 5
and m = 10 inducing points. Figure 5 visualises the
optimized sparse GPRN model, while Figure 6 in-
dicates the sparsity pattern in the mixing weights.
The weights have considerable smooth ‘on’ regions
(black), and also interesting smooth ‘oﬀ’ regions
(white). The ‘oﬀ’ regions indicate that for certain lo-

Table 2: Results for the Jura dataset for sparse
GPRN and vanilla GPRN models with test data.
Best performance is with boldface. We do not report
RMSE and MAE values GPc, since its a classiﬁca-
tion method.

Model m RMSE MAE RMSE MAE RMSE MAE
GPRN
5
22.14
22.75
sGPRN 5
25.10
10
GPRN
sGPRN 10
23.63

0.572
0.567
0.586
0.573

0.732
0.728
0.774
0.749

5.163
5.079
5.656
5.054

6.807
6.631
7.207
6.524

34.41
35.09
37.87
36.17

cations, only one of the two latent functions is adap-
tively utilised.

7 DISCUSSION

We proposed a novel paradigm of zero-inﬂated Gaus-
sian processes with a novel sparse kernel. The spar-
sity in the kernel is modeled with smooth probit
ﬁltering of the covariance rows and columns. This
model induces zeros in the prediction function out-
puts, which is highly useful for zero-inﬂated datasets
with excess of zero observations. Furthermore, we
showed how the zero-inﬂated GP can be used to
model sparse mixtures of latent signals with the pro-
posed sparse Gaussian process network. The latent
mixture model with sparse mixing coeﬃcients leads
to locally using only a subset of the latent functions,
which improves interpretability and reduces model
complexity. We demonstrated tractable solutions to
stochastic variational inference of the sparse probit
kernel for the zero-inﬂated GP, conventional GPRN,
and sparse GPRN models, which lends to eﬃcient
exploration of the parameter space of the model.

References

M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis,
J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Is-
ard, et al. Tensorﬂow: A system for large-scale
machine learning. In OSDI, volume 16, pages 265–
283, 2016.

Z. Abraham and P-N. Tan. An integrated frame-
work for simultaneous classiﬁcation and regression
In Proceedings of the 2010
of time-series data.
SIAM International Conference on Data Mining,
pages 653–664. SIAM, 2010.

S. Ancelet, M-P. Etienne, H. Benot, and E. Parent.
Modelling spatial zero-inﬂated continuous data
with an exponentially compound Poisson process.
Environmental and Ecological Statistics, 2009.

M. Andersen, O. Winther, and L. Hansen. Bayesian

inference for structured spike and slab priors. In
NIPS, pages 1745–1753, 2014.

S. Barry and A. H. Welsh. Generalized additive
modelling and zero inﬂated count data. Ecolog-
ical Modelling, 157:179–188, 2002.

D. Bohning, E. Dierz, and P. Schlattmann. Zero-
inﬂated count models and their applications in
public health and social science. In J. Rost and
R. Langeheine, editors, Applications of Latent
Trait and Latent Class Models in the Social Sci-
ences. Waxman Publishing Co, 1997.

S. Charles, B. Bates, I. Smith, and J. Hughes. Sta-
tistical downscaling of daily precipitation from ob-
served and modelled atmospheric ﬁelds. Hydrolog-
ical Processes, pages 1373–1394, 2004.

J.G. Cragg.

Some statistical models for limited
dependent variables with application to the de-
mand for durable goods. Econometrica, 39:829–
844, 1971.

S. del Saz-Salazar and P. Rausell-K¨oster. A double-
hurdle model of urban green areas valuation: deal-
ing with zero responses. Landscape and urban
planning, 84(3-4):241–251, 2008.

W. Enke and A. Spekat. Downscaling climate model
outputs into local and regional weather elements
by classiﬁcation and regression. Climate Research,
8:195–207, 1997.

J. Hensman, N. Fusi, and N. Lawrence. Gaus-
sian processes for big data. In Proceedings of the
Twenty-Ninth Conference on Uncertainty in Ar-
tiﬁcial Intelligence, pages 282–290. AUAI Press,
2013.

J. Hensman, A. Matthews, and Z. Ghahramani.
Scalable variational Gaussian process classiﬁca-
tion. In Artiﬁcial Intelligence and Statistics, pages
351–360, 2015.

W. Herlands, A. Wilson, H. Nickisch, S. Flaxman,
D. Neill, W. van Panhuis, and E. Xing. Scalable
Gaussian processes for characterizing multidimen-
sional change surfaces. In AISTATS, volume 51 of
PMLR, pages 1013–1021, 2016.

N. Johnson and S Kotz. Distributions in Statistics:
Discrete Distributions. Houghton MiZin, Boston,
1969.

D. Kingma and J. Ba. Adam: A method for stochas-

tic optimization. arXiv:1412.6980, 2014.

D. Lambert. Zero-inﬂated Poisson regression with
an application to defects in manufacturing. Tech-
nometrics, 34:1–14, 1992.

M. L´azaro-Gredilla, S. Van Vaerenbergh, and
N. Lawrence. Overlapping mixtures of Gaussian
processes for the data association problem. Pat-
tern Recognition, 45(4):1386–1395, 2012.

A. Matthews, M. van der Wilk, T. Nickson, K. Fu-
jii, A. Boukouvalas, P. Leon-Villagra, Z. Ghahra-
mani, and J. Hensman. GPﬂow: A Gaussian pro-
cess library using TensorFlow. Journal of Machine
Learning Research, 18(40):1–6, 2017.

J. Mullahy. Speciﬁcation and testing of some modi-
ﬁed count data models. Journal of Econometrics,
33:341–365, 1986.

T. Nguyen and E. Bonilla. Eﬃcient variational infer-
ence for Gaussian process regression networks. In
Proceedings of the Sixteenth International Confer-
ence on Artiﬁcial Intelligence and Statistics, vol-
ume 31 of Proceedings of Machine Learning Re-
search, pages 472–480. PMLR, 2013.

D.B. Owen. Tables for computing bivariate normal
probabilities. Annals of Mathematical Statistics,
27:1075–1090, 1956.

M. Pateﬁeld and D. Tandy. Fast and accurate calcu-
lation of owens t-function. Journal of Statistical
Software, 5:1–25, 2000.

C. Rasmussen and Z. Ghahramani. Inﬁnite mixtures
of Gaussian process experts. In NIPS, pages 881–
888, 2002.

C.E. Rasmussen and K.I. Williams. Gaussian pro-
cesses for machine learning. MIT Press, 2006.

Y. Saatchi. Scalable Inference for Structured Gaus-
sian Process Models. PhD thesis, University of
Cambridge, 2011.

H. Salimbeni and M. Deisenroth. Doubly stochastic
variational inference for deep Gaussian processes.
In NIPS, volume 30, 2017.

M. Titsias. Variational learning of inducing vari-
In Artiﬁcial

ables in sparse Gaussian processes.
Intelligence and Statistics, pages 567–574, 2009.

V. Tresp. Mixtures of Gaussian processes. In NIPS,

pages 654–660, 2001.

R.L. Wilby. Statistical downscaling of daily precipi-
tation using daily airﬂow and seasonal teleconnec-
tion. Climate Research, 10:163–178, 1998.

A. G. Wilson, D. Knowles, and Z. Ghahramani.
Gaussian process regression networks. In ICML,
2012.

Supplementary material for the “Zero-inﬂated Gaussian processes with sparse
kernels”

Here we show in detail how we arrived at the three new evidence lower bounds for the zero-inﬂated Gaussian
process, for the Gaussian process network, and for the sparse Gaussian process network.

A) The stochastic variational bound of the zero-inﬂated GP

Here, we will derive the evidence lower bound (ELBO) of the zero-inﬂated Gaussian process. We show how
to solve the ELBO of equations (24) and (25), which results in the equation (32).

The augmented true model with inducing points is deﬁned

We deﬁne the variational posterior approximation as

p(y, f , g, uf , ug) = p(y|f )p(f |g, uf )p(g|ug)p(uf )p(ug)

p(y|f ) = N (y|f , σ2

yI)

p(f |g, uf ) = N (f | diag(Φ(g))Kf nmK −1

f mmuf , Φ(g)Φ(g)T ◦ (cid:101)Kf )

p(g|ug) = N (g|KgnmK −1
p(uf ) = N (uf |0, Kf mm)
p(ug) = N (ug|0, Kgmm).

gmmug, (cid:101)Kg)

q(f , g, uf , ug) = p(f |g, uf )p(g|ug)q(uf )q(ug)
p(f |g, uf ) = N (f | diag(Φ(g))Kf nmK −1

f mmuf , Φ(g)Φ(g)T ◦ (cid:101)Kf )

p(g|ug) = N (g|KgnmK −1
q(uf ) = N (uf |mf , Sf )
q(ug) = N (ug|mg, Sg)

gmmug, (cid:101)Kg)

and where Sf , Sg ∈ Rm×m are square positive semi-deﬁnite matrices, and we deﬁne shorthands

(cid:101)Kf = Kf nn − Kf nmK −1
(cid:101)Kg = Kgnn − KgnmK −1

f mmKf mn
gmmKgmn.

In variational inference we minimize the Kullback-Leibler divergence between the variational approximation
q(f , g, uf , ug) and the true augmented joint distribution p(y, f , g, uf , ug):

KL[q(f , g, uf , ug)||p(y, f , g, uf , ug)] =

q(f , g, uf , ug) log

=

q(f , g, uf , ug) log

df dgduf dug

df dgduf dug

p(y, f , g, uf , ug)
q(f , g, uf , ug)
p(y|f )p(f |g, uf )p(g|ug)p(uf )p(ug)
p(f |g, uf )p(g|ug)q(uf )q(ug)

(cid:90)

(cid:90)

(cid:90)

=

=

(cid:90) (cid:90) (cid:90) (cid:90)

(cid:90)

(cid:124)

q(f , g, uf , ug) log

p(y|f )p(uf )p(ug)
q(uf )q(ug)

df dgduf dug

p(f |g, uf )p(g|ug)q(uf )q(ug) log p(y|f )duf dugdgdf

(S.17)

−

q(uf ) log q(uf )duf

−

q(ug) log q(ug)dug

(S.18)

(cid:123)(cid:122)
KL[q(uf )||p(uf )]

(cid:125)

(cid:123)(cid:122)
KL[q(ug)||p(ug)]

(cid:125)

(cid:90)

(cid:124)

(S.1)

(S.2)

(S.3)

(S.4)

(S.5)

(S.6)

(S.7)

(S.8)

(S.9)

(S.10)

(S.11)

(S.12)

(S.13)

(S.14)

(S.15)

(S.16)

Following the derivation of Hensman et al. (2015), this corresponds to maximizing the evidence lower bound
(ELBO) of equation (24):

(cid:90) (cid:90) (cid:90) (cid:90)

log p(y) ≥

log p(y|f )p(f |g, uf )q(uf )p(g|ug)q(ug)duf dugdgdf − KL[q(uf , ug)||p(uf , ug)]

(S.19)

= Eq(f ) log p(y|f ) − KL[q(uf , ug)||p(uf , ug)]

where we deﬁne

q(f ) =

p(f |g, uf )q(uf )p(g|ug)q(ug)duf dugdg

(cid:90) (cid:90) (cid:90)

(cid:90)

=

q(f |g)q(g)dg,

where the variational approximations are tractably

q(g) =

p(g|ug)q(ug)dug

=

N (g|KgnmK −1

gmmug, (cid:101)Kg)N (ug|mg, Sg)dug

= N (g|µg, Σg)

q(f |g) =

p(f |g, uf )q(uf )duf

(cid:90)

(cid:90)

(cid:90)

(cid:90)

=

N (f | diag(Φ(g))Kf nmK −1

f mmuf , Φ(g)Φ(g)T ◦ (cid:101)Kf )N (uf |mf , Sf )duf

= N (f | diag(Φ(g))µf , Φ(g)Φ(g)T ◦ Σf )

with

f mmmf
gmmmg

µf = Kf nmK −1
µg = KgnmK −1
Σf = Kf nn + Kf nmK −1
Σg = Kgnn + KgnmK −1

f mm(Sf − Kf mm)K −1
gmm(Sg − Kgmm)K −1

f mmKf mn
gmmKgmn.

The variational marginalizations q(g) and q(f |) follow from standard Gaussian identities4. Substituting the
variational marginalizations back to the ELBO results in

(cid:90) (cid:90)

log p(y) ≥

log p(y|f )q(f |g)q(g)df dg − KL[q(uf , ug)||p(uf , ug)].

(S.32)

Next, we marginalize the f from the ELBO. We additionally assume the likelihood p(y|f ) = (cid:81)N
factorises, which results in
(cid:90)

(cid:90)

i=1 p(yi|fi)

log p(y|f )q(f |g)df =

log N (y|f , σ2

yI)q(f |g)df

f

log N (yi|fi, σ2

y)q(fi|gi)dfi

log N (yi|Φ(gi)kT

f iK −1

f mmmf i, σ2

y) −

Φ(gi)2(kf ii + kT

f iK −1

f mm(Sf − Kf mm)K −1

f mmkf i)

(cid:111)

(cid:110)

1
2σ2
y

=

log N (yi|Φ(gi)µf i, σ2

y) −

1
2σ2
y

(cid:8)Φ(gi)2σ2

f i

(cid:9) ,

4See for instance Bishop (2006): Pattern recognition and Machine learning, Springer, Section 2.3.

=

=

N
(cid:88)

(cid:90)

i=1

N
(cid:88)

i=1

N
(cid:88)

i=1

(S.20)

(S.21)

(S.22)

(S.23)

(S.24)

(S.25)

(S.26)

(S.27)

(S.28)

(S.29)

(S.30)

(S.31)

(S.33)

(S.34)

(S.35)

(S.36)

where

µf i = [µf ]i = kT
f i = [Σf ]ii = kf ii + kT
σ2

f iK −1

f mmmf i

f iK −1

f mm(Sf − Kf mm)K −1

f mmkf i.

Substituting the above result into the ELBO results in

Eq(f ) log p(y|f ) =

q(g)

q(f |g) log p(y|f )df dg

(cid:90)

f

(cid:90)

g
(cid:90)

g

N
(cid:88)

i=1

N
(cid:88)

i=1

=

=

=

N
(cid:88)

i=1
(cid:90)

gi

log N (yi|Φ(gi)µf i, σ2

y) −

(cid:8)Φ(gi)2σ2

f i

(cid:9) q(g)dg

1
2σ2
y

log N (yi|Φ(gi)µf i, σ2

y) q(gi)dgi −

(cid:8)Φ(gi)2σ2

f i

(cid:9) q(gi)dgi

1
2σ2
y

N
(cid:88)

(cid:90)

i=1

gi

1
2σ2
y

N
(cid:88)

i=1

log N (yi|(cid:104)Φ(gi)(cid:105)q(gi)µf i, σ2

y) −

(cid:8)V ar[Φ(gi)](µf i)2(cid:9) −

(cid:8)(cid:104)Φ(gi)2(cid:105)q(gi)σ2

f i

(cid:9) .

1
2σ2
y

N
(cid:88)

i=1

The expectations (cid:104).(cid:105)q(gi) of CDF transformation of a random variable with univariate Gaussian distribution.
The analytical forms for these integrals can be written:

(cid:104)Φ(gi)(cid:105)q(gi) =

Φ(gi)q(gi)dgi

=

Φ(gi)N (gi|µgi, σ2




gi)dgi

= Φ



(cid:113)

µgi
1 + σ2
gi



V ar[Φ(gi)] =

(Φ(gi) − (cid:104)Φ(gi)(cid:105)q(gi))2q(gi)dgi

(cid:90)

(cid:90)

(cid:90)

(cid:90)







= Φ



(cid:113)

 − 2T



(cid:113)

µgi
1 + σ2
gi

µgi
1 + σ2
gi

,

(cid:113)

1 + 2 σ2
gi





 − Φ



(cid:113)


2



µgi
1 + σ2
gi

(cid:104)Φ(gi)2(cid:105)q(gi) =

Φ(gi)2q(gi)dgi





= Φ



(cid:113)

 − 2T



(cid:113)

µgi
1 + σ2
gi

µgi
1 + σ2
gi

,

(cid:113)

1 + 2 σ2
gi





1

1

where

µgi = [µg]i = kT
gi = [Σg]ii = Kgii + kT
σ2

giK −1

gmmmgi

giK −1

gmm(Sg − Kgmm)K −1

gmmkgi.

Owen’s T function is deﬁned as T (h, a) = φ(h) (cid:82) a
0

φ(hx)
1+x2 dx.

(S.37)

(S.38)

(S.39)

(S.40)

(S.41)

(S.42)

(S.43)

(S.44)

(S.45)

(S.46)

(S.47)

(S.48)

(S.49)

(S.50)

(S.51)

The ﬁnal evidence lower bound with the Kullback-Leibler terms is

p(y) ≥

log N (yi|(cid:104)Φ(gi)(cid:105)q(gi)µf i, σ2

y) −

(cid:0)V ar[Φ(gi)] µ2

f i + (cid:104)Φ(gi)2(cid:105)q(gi) σ2
f i

(cid:27)

(cid:1)

log |Kf mm| −

log |Sf | +

T r

(mf mf

T + Sf )K −1

f mm

(cid:105)

−

log |Kgmm| −

log |Sg| +

T r (cid:2)(mgmg

T + Sg)K −1

gmm

(cid:3) −

1
2
1
2

(cid:27)

m
2
(cid:27)
m
2

1
2σ2
y

(cid:104)

1
2
1
2

N
(cid:88)

(cid:26)

i=1

−

−

(cid:26) 1
2
(cid:26) 1
2

= LZiGP

B) The stochastic variational bound of the Gaussian process network

In GPRN a vector-valued output function y(x) ∈ RP with P outputs is modeled using vector-valued latent
functions f (x) ∈ RQ with Q latent values and mixing weights W (x) ∈ RP ×Q as

where for all q = 1, . . . , Q and p = 1, . . . , P we assume GP priors and additive zero-mean noises,

y(x) = W (x)[f (x) + (cid:15)] + ε,

fq(x) ∼ GP(0, Kf (x, x(cid:48)))
Wqp(x) ∼ GP(0, Kw(x, x(cid:48)))

(cid:15)q ∼ N (0, σ2
f )
εp ∼ N (0, σ2
y).

The subscripts are used to denote individual components of f and W with p and q indicating pth output
dimension and qth latent dimension respectively.

We begin by introducing the inducing variable augmentation for latent functions f (x) and mixing weights
W (x) with uf , zf = {ufq , zfq }Q

q=1 and uw, zw = {uwqp, zwqp}Q,P

q,p=1:

p(y, f , W, uf , uw) = p(y|f , W )p(f |uf )p(W |uw)p(uf )p(uw)

p(f |uf ) =

N (fq|Qfq ufq , (cid:101)Kfq )

p(W |uw) =

N (wqp|Qwqpuwqp, (cid:101)Kwqp)

p(uf ) =

N (ufq |0, Kfq,mm)

p(uw) =

N (uwqp|0, Kwqp,mm),

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

Qf = KfqnmK −1
fqmm
Qg = KwqpnmK −1
(cid:101)Kf = Kfqnn − KfqnmK −1
(cid:101)Kwqp = Kwqpnn − KwqpnmK −1

wqpmm

fqmmKfqmn

wqpmmKwqpmn.

where we have separate kernels K and extrapolation matrices Q for each component of W (x) and f (x) that
are of the form as given below:

(S.52)

(S.53)

(S.54)

(S.55)

(S.56)

(S.57)

(S.58)

(S.59)

(S.60)

(S.61)

(S.62)

(S.63)

(S.64)

(S.65)

(S.66)

(S.67)

(S.68)

(S.69)

Following the variational inference framework, we deﬁne the variational joint distribution as,

q(f , W, uf , uw) = p(f |uf )p(W |uw)q(uf )q(uw)

q(ufq ) =

N (ufq |mfq Sfq )

q(uwqp) =

N (uwqp|mwqp, Swqp),

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

where uwqp and ufq indicate the inducing points for functions Wqp(x) and fq(x), respectively. The ELBO
can be now stated as

log p(y) ≥ Eq(f ,W,uf ,uw) log p(y|f , W ) − KL[q(uf , uw)||p(uf , uw)]

(cid:90) (cid:90) (cid:90) (cid:90)

=

q(f , W, uf , uw) log p(y|f , W )df dW duf duw − KL[q(uf , uw)||p(uf , uw)]

Since the variational joint posterior decomposes as equation (S.70), we begin by marginalizing the inducing
distributions uf and uw,

q(f , W, uf , uw)df dW duf duw =

p(f |uf )q(uf )df duf

p(W |uw)q(uw)dW duw

(S.75)

uf

(cid:90)

(cid:90)

f
(cid:90)

f

(cid:90)

W

=

q(f )df

q(W )dW

(cid:90)

(cid:90)

W

uw

(cid:90) (cid:90) (cid:90) (cid:90)

where

with

q(f ) =

p(f |uf )q(uf )duf

N (fq|µfq , Σfq )

q(W ) =

p(W |uw)q(uw)duw

(cid:90)

Q
(cid:89)

(cid:90)

q=1

Q
(cid:89)

q=1
(cid:90)

Q,P
(cid:89)

(cid:90)

q,p=1

Q,P
(cid:89)

q,p=1

=

=

=

=

N (Wqp|µwqp, Σwqp)

N (fq|KfqnmK −1

fqmmufq , (cid:101)Kfq )N (ufq |mfq , Sfq )dufq

N (Wqp|KwqpnmK −1

wqpmmuwqp, (cid:101)Kwqp)N (uwqp|mwqp, Swqp)duwqp

µfq = KfqnmK −1
fqmmmfq
µwqp = KwqpnmK −1
wqpmmmwqp
Σfq = Kfqnn + KfqnmK −1
Σwqp = Kwqpnn + KwqpnmK −1

fqmm(Sfq − Kfqmm)K −1

fqmmKfqmn

wqpmm(Swqp − Kwqpmm)K −1

wqpmmKwqpmn.

Since the noise term ε is assumed to be isotropic Gaussian, density p(y|W, f ) factorises across all target

(S.70)

(S.71)

(S.72)

(S.73)

(S.74)

(S.76)

(S.77)

(S.78)

(S.79)

(S.80)

(S.81)

(S.82)

(S.83)

(S.84)

(S.85)

(S.86)

(S.87)

(S.88)

(S.89)

(S.90)

(S.91)

(S.92)

(S.93)

(S.94)

(cid:41)
(cid:17)

(S.95)

(S.96)

(S.97)

observations and dimensions. The expectation term in the ELBO then reduces to,

log p(y) ≥ Eq(W )Eq(f ) log p(y|f , W )
(cid:90) (cid:90)

=

N,P
(cid:88)

i,p=1

(cid:90)

The integral with respect to f can be now solved as

log N (yp,i|wT

p,ifi, ε2

p)q(fi, wp,i)dwp,idfi − KL[q(uf , uw)||p(uf , uw)].

log N (yp,i|wT

p,ifi, ε2

p)q(fi)dfi = log N (yp,i|wT

p,iµfi, ε2

p) −

T r(cid:2)wT

p,iΣfiwp,i

(cid:3)

= log N (yp,i|wT

p,iµfi, ε2

p) −

T r(cid:2)Σfiwp,iwT

p,i

(cid:3).

Next we can marginalize W from the above terms,

(cid:90)

log N (yp,i|wT

p,iµfi, ε2

p)q(wp,i)dwp,i = log N (yp,i|µT

wp,i

µfi, ε2

p) −

T r(cid:2)µT
fi

Σwq,iµfi

(cid:3)

1
2ε2
p
1
2ε2
p

1
2ε2
p

1
2ε2
p

Q
(cid:88)

q=1

= log N (yp,i|µT

µfi, ε2

p) −

wp,i

fq,iσ2
µ2

wqp,i

(cid:90)

1
2ε2
p

T r(cid:2)Σfiwp,iwT

p,i

(cid:3)q(wp,i)dwp,i =

T r(cid:2)Σfi(µwp,i µT

wp,i

+ Σwq,i)(cid:3)

1
2ε2
p

1
2ε2
p

=

Q
(cid:88)

(cid:16)

q=1

wqp,iσ2
µ2

fq,i + σ2

wqp,iσ2

fq,i

(cid:17)

.

Finally, adding the above results across all N observations and response dimensions P along with Gaussian
KL divergence terms, we get the ﬁnal lowerbound:

log p(y) ≥

N
(cid:88)

(cid:40) P

(cid:88)

i=1

p=1

(cid:16)

log N

yp,i|

µwqp,iµfq,i, ε2
p

Q
(cid:88)

q=1

(cid:17)

−

1
2ε2
p

Q,P
(cid:88)

(cid:16)

q,p=1

wqp,iσ2
µ2

fq,i + µ2

fq,iσ2

wqp,i + σ2

wqp,iσ2

fq,i

−

KL[q(uwqp, ufq )||p(uwqp, ufq )]

Q,P
(cid:88)

q,p

= Lgprn,

where µfq,i is the i’th element of µfq and σ2

fq,i is the i’th diagonal element of Σfq (similarly for Wqp’s).

C) The stochastic variational bound of the sparse Gaussian process network

Sparse GPRN is a modiﬁcation to standard GPRN where sparsity is added to the mixing matrix components.
This corresponds to the p’th output being a sparse mixture of the latent Q functions, i.e. it can eﬀectively
use any subset of the Q latent dimensions by having zeros in the mixing functions. The joint distribution
for the model can be written as,

p(y, f , W, g) = p(y|f , W )p(f )p(W |g)p(g),

(S.98)

where all individual components of latent function f and mixing matrix W are given GP priors. We encode
the sparsity terms g for all Q × P mixing functions Wqp(x) functions as

p(Wqp|gqp) = N (wqp|0, Φ(gqp)Φ(gqp)T ◦ Kw).

(S.99)

To introduce variational inference, the joint model is augmented with three sets of inducing variables for f ,
W and g. After marginalizing out the inducing variables similar to SVI for standard GPRN, the lower bund
for marginal likelihood can be written as

log p(y) ≥ Eq(f ,W,g) log p(y|f , W ) − KL[q(uf , uw, ug)||p(uf , uw, ug)].

(S.100)

Where the joint distribution in the variational expectation factorizes as q(f , W, g) = q(f )q(W |g)q(g). The
variational posterior after marginalizing inducing variables is written as,

q(f ) =

q(f |uf )q(uf )duf

=

N (fq|µfq , Σfq )

q(W ) =

q(W |uw)q(uw)duw

N (Wqp|µwqp, Σwqp)

q(g) =

q(g|ug)q(ug)dug

N (gqp|µgqp, Σgqp)

(cid:90)

Q
(cid:89)

q=1
(cid:90)

Q,P
(cid:89)

=

q,p=1
(cid:90)

Q,P
(cid:89)

=

q,p=1

with

µfq = KfqnmK −1
fqmmmfq
µwqp = KwqpnmK −1
wqpmmmwqp
µgqp = KgqpnmK −1
gqpmmmgqp
Σfq = Kfqnn + KfqnmK −1
Σwqp = Kwqpnn + KwqpnmK −1
Σgqp = Kgqpnn + KgqpnmK −1

fqmm(Sfq − Kfqmm)K −1

fqmmKfqmn

wqpmm(Swqp − Kwqpmm)K −1
gqpmm(Sgqp − Kgqpmm)K −1

gqpmmKgqpmn.

wqpmmKwqpmn

Similar to standard GPRN, with the isotropic Gaussian, density p(y|W, f ) factorizes across all target obser-
vations and dimensions. The expectation term in the ELBO then reduces to

log p(y) ≥ Eq(W |g)Eq(f )Eq(g) log p(y|f , W )

N,P
(cid:88)

(cid:90) (cid:90) (cid:90)

=

i,p=1

log N (yp,i|(wp,i ◦ gp,i)T fi, ε2

p)q(fi)q(wp,i|q(gp,i)q(gp,i)dwp,idfidgi − KL[q(uf , uw)||p(uf , uw)].

The integral with respect to f can be now solved as

(cid:90)

log N (yp,i|(wp,i ◦ gp,i)T fi, ε2

p)q(fi)dfi = log N (yp,i|(wp,i ◦ gp,i)T µfi, ε2

p) −

T r(cid:2)(wp,i ◦ gp,i)T Σfi(wp,i ◦ gp,i)(cid:3)

= log N (yp,i|(wp,i ◦ gp,i)T µfi, ε2

p) −

T r(cid:2)Σfi(gp,igT

p,i ◦ wp,iwT

p,i)(cid:3).

1
2ε2
p

1
2ε2
p

(S.101)

(S.102)

(S.103)

(S.104)

(S.105)

(S.106)

(S.107)

(S.108)

(S.109)

(S.110)

(S.111)

(S.112)

(S.113)

(S.114)

(S.115)

(S.116)

(cid:90)

1
2ε2
p

(cid:90)

(cid:90)

1
2ε2
p

Next, by integrating individual terms with respect to W we get

(cid:90)

log N (yp,i|(wp,i ◦ gp,i)T µfi, ε2

p)q(wp,i)dwp,i = log N (yp,i|(µwp,i ◦ gp,i)T µfi, ε2

p) −

1
2ε2
p

T r(cid:2)µT
fi

(gp,igT

p,i ◦ Σwq,i)µfi

(cid:3)

T r(cid:2)Σfi(gp,igT

p,i ◦ wp,iwT

p,i)(cid:3) =

T r(cid:2)Σfi(gp,igT

p,i ◦ (µwp,i µT

wp,i

+ Σwq,i))(cid:3)

1
2ε2
p

Finally, integrating all the above terms with respect to g, we get

log N (yp,i|(µwp,i ◦ gp,i)T µfi, ε2

p)q(gp,i)dgp,i = log N (yp,i|(µwp,i ◦ (cid:104)Φ(gp,i)(cid:105))T µfi, ε2
p)

(S.119)

T r(cid:2)µT
fi

(gp,igT

p,i ◦ Σwq,i)µfi

(cid:3)q(gp,i)dgp,i =

T r(cid:2)µT
fi

(cid:0)((cid:104)Φ(gp,i)(cid:105)(cid:104)Φ(gp,i)(cid:105)T + V ar[Φ(gp,i)]) ◦ Σwq,i

(cid:1) µfi

(cid:3)

(cid:90)

1
2ε2
p

T r(cid:2)Σfi(gp,igT

p,i ◦ (µwp,i µT

wp,i

+ Σwq,i ))(cid:3)q(gp,i)dgp,i =

((cid:104)Φ(gp,i)(cid:105)(cid:104)Φ(gp,i)(cid:105)T + V ar[Φ(gp,i)]) ◦ µwp,i µT

wp,i

(cid:17) (cid:3)

−

1
2ε2
p

T r(cid:2)µT
fi

(µwp,i µT

wp,i

◦ V ar[Φ(gp,i)])µfi]

(cid:16)

= log N

yp,i|

µwqp,iµgqp,iµfq,i, ε2
p

(cid:17)

Q
(cid:88)

q=1

−

1
2ε2
p

Q
(cid:88)

(cid:16)

q=1

gqp,iµ2
σ2

fq,iµ2

wqp,i

(cid:17)

=

(µ2

gqp,i + σ2

gqp,i)µ2

fq,iσ2

wqp,i

(cid:17)

1
2ε2
p

1
2ε2
p

1
2ε2
p

Q
(cid:88)

(cid:16)

q=1

(cid:16)

T r(cid:2)Σfi

+

1
2ε2
p

=

1
2ε2
p

Q
(cid:88)

(cid:16)

q=1

T r(cid:2)Σfi

(cid:0)((cid:104)Φ(gp,i)(cid:105)(cid:104)Φ(gp,i)(cid:105)T + V ar[Φ(gp,i)]) ◦ Σwq,i

(cid:1) (cid:3)

(µ2

gqp,i + σ2

gqp,i)(µ2

wqp,iσ2

fq,i + σ2

wqp,iσ2

Adding above results across all the observations N and output dimensions P , we retrieve the ﬁnal evidence
lower bound

p(y) ≥

N
(cid:88)

(cid:40) P

(cid:88)

i=1

p=1

(cid:16)

log N

yp,i|

µwqp,iµgqp,iµfq,i, ε2
p

(cid:17)

Q
(cid:88)

q=1

Q,P
(cid:88)

(cid:16)

−

q,p=1

(µ2

gqp,i + σ2

gqp,i) · (µ2

wqp,iσ2

fq,i + µ2

fq,iσ2

wqp,i + σ2

wqp,iσ2

gqp,iµ2
σ2

fq,iσ2

wqp,i

(cid:17)
fq,i)

−

Q,P
(cid:88)

(cid:16)

q,p=1

−

KL[q(ufq , uwqp, ugqp)||p(ufq , uwqp, ugqp)]

Q,P
(cid:88)

q,p

= Lsgprn.

(S.117)

(S.118)

(S.120)

(S.121)

(S.122)

(S.123)

(S.124)

(S.125)

(S.126)

(cid:17)
fq,i)

.

(S.127)

(S.128)

(cid:41)

(cid:17)

(S.129)

(S.130)

(S.131)

Variational zero-inﬂated Gaussian processes with sparse kernels

8
1
0
2
 
r
a

M
 
3
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
6
3
0
5
0
.
3
0
8
1
:
v
i
X
r
a

Pashupati Hegde

Markus Heinonen

Samuel Kaski

Helsinki Institute for Information Technology HIIT
Department of Computer Science, Aalto University

Abstract

Zero-inﬂated datasets, which have an ex-
cess of zero outputs, are commonly en-
countered in problems such as climate or
rare event modelling. Conventional ma-
chine learning approaches tend to overesti-
mate the non-zeros leading to poor perfor-
mance. We propose a novel model family
of zero-inﬂated Gaussian processes (ZiGP)
for such zero-inﬂated datasets, produced
by sparse kernels through learning a la-
tent probit Gaussian process that can zero
out kernel rows and columns whenever the
signal is absent. The ZiGPs are particu-
larly useful for making the powerful Gaus-
sian process networks more interpretable.
We introduce sparse GP networks where
variable-order latent modelling is achieved
through sparse mixing signals. We derive
the non-trivial stochastic variational infer-
ence tractably for scalable learning of the
sparse kernels in both models. The novel
output-sparse approach improves both pre-
diction of zero-inﬂated data and inter-
pretability of latent mixing models.

1

INTRODUCTION

Zero-inﬂated quantitative datasets with overabun-
dance of zero output observations are common in
many domains, such as climate and earth sciences
(Enke and Spekat, 1997; Wilby, 1998; Charles et al.,
2004), ecology (del Saz-Salazar and Rausell-K¨oster,
2008; Ancelet et al., 2009), social sciences (Bohn-
ing et al., 1997), and in count processes (Barry and
Welsh, 2002). Traditional regression modelling of
such data tends to underestimate zeros and overes-
timate nonzeros (Andersen et al., 2014).

A conventional way of forming zero-inﬂated mod-
els is to estimate a mixture of a Bernoulli “on-oﬀ”
process and a Poisson count distribution (Johnson
and Kotz, 1969; Lambert, 1992). In hurdle models
a binary “on-oﬀ” process determines whether a hur-
dle is crossed, and the positive responses are gov-
erned by a subsequent process (Cragg, 1971; Mul-
lahy, 1986). The hurdle model is analogous to ﬁrst
performing classiﬁcation and training a continuous
predictor on the positive values only, while the zero-
inﬂated model would regress with all observations.
Both stages can be combined for simultaneous clas-
siﬁcation and regression Abraham and Tan (2010).

Gaussian process models have not been proposed
for zero-inﬂated datasets since their posteriors are
Gaussian, which are ill-ﬁtted for zero predictions.
A suite of Gaussian process models have been pro-
posed for partially related problems, such as mixture
models (Tresp, 2001; Rasmussen and Ghahramani,
2002; L´azaro-Gredilla et al., 2012) and change point
detection (Herlands et al., 2016). Structured spike-
and-slab models place smoothly sparse priors over
the structured inputs (Andersen et al., 2014).

In contrast to other approaches, we propose a
Bayesian model that learns the underlying latent
prediction function, whose covariance is sparsiﬁed
through another Gaussian process switching be-
tween the ‘on’ and ‘oﬀ’ states, resulting in an zero-
inﬂated Gaussian process model. This approach
introduces a tendency of predicting exact zeros
to Gaussian processes, which is directly useful in
datasets with excess zeros.

A Gaussian process network (GPRN) is a latent
signal framework where multi-output data are ex-
plained through a set of latent signals and mixing
weight Gaussian processes (Wilson et al., 2012). The
standard GPRN tends to have dense mixing that
combines all latent signals for all latent outputs. By

applying the zero-predicting Gaussian processes to
latent mixture models, we introduce sparse GPRNs
where latent signals are mixed with sparse instead
of dense mixing weight functions. The sparse model
induces variable-order mixtures of latent signals re-
sulting in simpler and more interpretable models.
We demonstrate both of these properties in our ex-
periments with spatio-temporal and multi-output
datasets.

Main contributions. Our contributions include

1. A novel zero-inﬂated Gaussian process formal-
ism consisting of a latent Gaussian process and
a separate ‘on-oﬀ’ probit-linked Gaussian pro-
cess that can zero out rows and columns of the
model covariance. The novel sparse kernel adds
to GPs the ability to predict zeros.

2. Novel stochastic variational inference (SVI) for
such sparse probit covariances, which in gen-
eral are intractable due to having to compute
expectations of GP covariances with respect to
probit-linked processes. We derive the SVI for
learning both of the underlying processes.

3. A solution to the stochastic variational infer-
ence for conventional Gaussian process net-
works (GPRN) improving the earlier diago-
nalized mean-ﬁeld approximation (Nguyen and
Bonilla, 2013) by taking the covariances fully
into account.

4. A novel sparse GPRN with an on-oﬀ process
in the mixing matrices leading to sparse and
variable-order mixtures of latent signals.

5. A solution to the stochastic variational infer-
ence of sparse GPRN where the SVI is derived
for the network of full probit-linked covariances.

TensorFlow
these methods

implementation
Python
The
of
at
is
github.com/hegdepashupati/zero-inflated-gp
and at github.com/hegdepashupati/gprn-svi.

available

publicly

2 GAUSSIAN PROCESSES

We begin by introducing the basics of conventional
Gaussian processes. Gaussian processes (GP) are a
family of non-parametric, non-linear Bayesian mod-
els (Rasmussen and Williams, 2006). Assume a
dataset of n inputs X = (x1, . . . , xn) with xi ∈ RD
and noisy outputs y = (y1, . . . , yn) ∈ Rn. The ob-
servations y = f (x)+ε are assumed to have additive,

Figure 1: Illustration of a zero-inﬂated GP (a) and
standard GP regression (b). The standard approach
is unable to model sudden loss of signal (at 4 . . . 5)
and signal close to zero (at 0 . . . 1 and 7 . . . 9).

zero mean noise ε ∼ N (0, σ2
prior on the latent function f (x),

y) with a zero-mean GP

f (x) ∼ GP (0, K(x, x(cid:48))) ,

(1)

which deﬁnes a distribution over functions f (x)
whose mean and covariance are

E[f (x)] = 0
cov[f (x), f (x(cid:48))] = K(x, x(cid:48)).

(2)

(3)

Then for any collection of inputs X, the function
values follow a multivariate normal distribution f ∼
N (0, KXX ), where f = (f (x1), . . . , f (xN ))T ∈ Rn,
and where KXX ∈ Rn×n with [KXX ]ij = K(xi, xj).
The key property of Gaussian processes is that they
encode functions that predict similar output values
f (x), f (x(cid:48)) for similar inputs x, x(cid:48), with similarity
determined by the kernel K(x, x(cid:48)). In this paper we
assume the Gaussian ARD kernel

K(x, x(cid:48)) = σ2

f exp

−



1
2

D
(cid:88)

j=1

j)2

(xj − x(cid:48)
(cid:96)2
j



 ,

(4)

with a signal variance σ2
lengthscale (cid:96)1, . . . , (cid:96)D parameters.

f and dimension-speciﬁc

inference of

The
θ =
(σy, σf , (cid:96)1, . . . , (cid:96)D) is performed commonly by max-
imizing the marginal likelihood

the hyperparameters

(cid:90)

p(y|θ) =

p(y|f )p(f |θ)df ,

(5)

which results in a convenient marginal likelihood
called evidence, p(y|θ) = N (y|0, KXX + σ2
yI) for
a Gaussian likelihood.

The Gaussian process deﬁnes a univariate nor-
mal predictive posterior distribution f (x)|y, X ∼
N (µ(x), σ2(x)) for an arbitrary input x with the
prediction mean and variance1

where

µ(x) = KxX (KXX + σ2
yI)−1y,
σ2(x) = Kxx − KxX (KXX + σ2

yI)−1KXx,

(6)

(7)

xX ∈ Rn is the kernel column vector
where KXx = K T
over pairs X ×x, and Kxx = K(x, x) ∈ R is a scalar.
The predictions µ(x) ± σ(x) come with uncertainty
estimates in GP regression.

3 ZERO-INFLATED GAUSSIAN

PROCESSES

Figure 2:
Illustration of the zero-inﬂated GP (a)
and the sparse kernel (b) composed of a smooth
latent function (c,d) ﬁltered by a probit support
function (e,f ), which is induced by the underlying
latent sparsity (g,h).

We introduce zero-inﬂated Gaussian processes that
have – in contrast to standard GP’s – a tendency to
produce exactly zero predictions (See Figure 1). Let
g(x) denote the latent “on-oﬀ” state of a function
f (x). We assume GP priors for both functions with
a joint model

p(y, f , g) = p(y|f )p(f |g)p(g),

(8)

1In the following we omit the implicit conditioning on

data inputs X for clarity.

p(y|f ) = N (y|f , σ2
p(f |g) = N (f |0, Φ(g)Φ(g)T ◦ Kf )

yI)

p(g) = N (g|β1, Kg).

(9)

(10)

(11)

The sparsity values g(x) are squashed between 0 and
1 through a standard Normal cumulative distribu-
tion, or a probit link function, Φ : R → [0, 1]

Φ(g) =

φ(τ )dτ =

1 + erf

,

(12)

(cid:90) g

−∞

(cid:18)

1
2

(cid:19)(cid:19)

(cid:18) g
√
2

2 τ 2

e− 1

where φ(τ ) = 1√
is the standard normal den-
2π
sity function. The structured probit sparsity Φ(g)
models the “on-oﬀ” smoothly due to the latent spar-
sity function g having a GP prior with prior mean
β. The latent function f is modeled throughout but
it is only visible during the “on” states. This mask-
ing eﬀect has similarities to both zero-inﬂated and
hurdle models. The underlying latent function f is
learned from only non-zero data similarly to in hur-
dle models, but the function f is allowed to predict
zeros similarly to zero-inﬂated models.

is the sparse probit-
The key part of our model
sparsiﬁed covariance Φ(g)Φ(g)T ◦ K where the “on-
oﬀ” state Φ(g) has the ability to zero out rows and
columns of the kernel matrix at the “oﬀ” states (See
Figure 2f for the probit pattern Φ(g)Φ(g)T and Fig-
ure 2b for the resulting sparse kernel). As the spar-
sity g(x) converges towards minus inﬁnity, the probit
link Φ(g(x)) approaches zero, which leads the func-
tion distribution approaching N (fi|0, 0), or fi = 0.
Numerical problems are avoided since in practice
Φ(g) > 0, and due to the conditioning noise vari-
ance term σ2

y > 0.

The marginal likelihood of the zero-inﬂated Gaus-
sian process is intractable due to the probit-
sparsiﬁcation of the kernel. We derive a stochastic
variational Bayes approximation, which we show to
be tractable due to the choice of using the probit
link function.

3.1 STOCHASTIC VARIATIONAL

INFERENCE

Inference for standard Gaussian process models is
diﬃcult to scale as complexity grows with O(n3) as
a function of the data size n. Titsias (2009) pro-
posed a variational inference approach for GPs using
m < n inducing variables, with a reduced computa-
tional complexity of O(m3) for m inducing points.
The novelty of this approach lies in the idea that

the locations and values of inducing points can be
treated as variational parameters, and optimized.
Hensman et al. (2013, 2015) introduced more eﬃ-
cient stochastic variational inference (SVI) with fac-
torised likelihoods that has been demonstrated with
up to billion data points (Salimbeni and Deisenroth,
2017). This approach cannot be directly applied to
sparse kernels due to having to compute expectation
of the probit product in the covariance. We derive
the SVI bound tractably for the zero-inﬂated model
and its sparse kernel, which is necessary in order to
apply the eﬃcient parameter estimation techniques
with automatic diﬀerentiation with frameworks such
as TensorFlow (Abadi et al., 2016).

We begin by applying the inducing point augmen-
tations f (zf ) = uf and g(zg) = ug for both
the latent function f (·) and the sparsity function
g(·). We place m inducing points uf 1, . . . uf m
and ug1, . . . ugm for the two functions. The aug-
mented joint distribution is p(y, f , g, uf , ug) =
p(y|f )p(f |g, uf )p(g|ug)p(uf )p(ug), where2

We minimize the Kullback-Leibler divergence be-
tween the true augmented posterior p(y, f , g, uf , ug)
and the variational distribution q(f , g, uf , ug),
which is equivalent to solving the following evi-
dence lower bound (as shown by e.g. Hensman et al.
(2015)):

log p(y) ≥ Eq(f ) log p(y|f ) − KL[q(uf , ug)||p(uf , ug)],
(24)

q(f ) =

p(f |g, uf )q(uf )p(g|ug)q(ug)duf dugdg

where we deﬁne

(cid:90) (cid:90) (cid:90)

(cid:90)

=

q(f |g)q(g)dg,

where the variational approximations are tractably

(cid:90)

(cid:90)

q(g) =

p(g|ug)q(ug)dug

= N (g|µg, Σg)

q(f |g) =

p(f |g, uf )q(uf )duf

p(f |g, uf ) = N (f | diag(Φ(g))Qf uf , Φ(g)Φ(g)T ◦ (cid:101)Kf )
(13)

with

= N (f | diag(Φ(g))µf , Φ(g)Φ(g)T ◦ Σf )

(14)

(15)

(16)

(17)

(18)

(19)

(20)

p(g|ug) = N (g|Qgug, (cid:101)Kg)
p(uf ) = N (uf |0, Kf mm)
p(ug) = N (ug|0, Kgmm)

and where

f mm

Qf = Kf nmK −1
Qg = KgnmK −1
(cid:101)Kf = Kf nn − Kf nmK −1
(cid:101)Kg = Kgnn − KgnmK −1

gmm

f mmKf mn
gmmKgmn.

We denote the kernels for functions f and g by the
corresponding subscripts. The kernel Kf nn is be-
tween all n data points, the kernel Kf nm is between
all n datapoints and m inducing points, and the ker-
nel Kf mm is between all m inducing points (similarly
for g as well).

Next we use the standard variational approach by
introducing approximative variational distributions
for the inducing points,

q(uf ) = N (uf |mf , Sf )
q(ug) = N (ug|mg, Sg)
where Sf , Sg ∈ Rm×m are square positive semi-
deﬁnite matrices. The variational joint posterior is

(22)

(21)

q(f , g, uf , ug) = p(f |g, uf )p(g|ug)q(uf )q(ug). (23)

2We drop the implicit conditioning on z’s for clarity.

µf = Qf mf
µg = Qgmg
Σf = Kf nn + Qf (Sf − Kf mm)QT
f
Σg = Kgnn + Qg(Sg − Kgmm)QT
g .

We additionally assume the likelihood p(y|f ) =
(cid:81)N
i=1 p(yi|fi) factorises. We solve the ﬁnal ELBO
of equations (24) and (25) as (See Supplements for
detailed derivation)

N
(cid:88)

(cid:110)

i=1

−

1
2σ2
y

LZI =

log N (yi|(cid:104)Φ(gi)(cid:105)q(gi)µf i, σ2
y)

(cid:0)Var[Φ(gi)]µ2

f i + (cid:104)Φ(gi)2(cid:105)q(gi)σ2
f i

− KL[q(uf )||p(uf )] − KL[q(ug)||p(ug)],

where µf i is the i’th element of µf and σ2
f i is the
i’th diagonal element of Σf (similarly with g). The
expectations are tractable,

(cid:104)Φ(gi)(cid:105)q(gi) = Φ(λgi),

λgi =

(cid:113)

(cid:104)Φ(gi)2(cid:105)q(gi) = Φ(λgi) − 2T

λgi,

Var[Φ(gi)] = Φ(λgi) − 2T

λgi,

− Φ(λgi)2.

µgi
1 + σ2
gi
(cid:19)

λgi
µgi
λgi
µgi

(cid:19)

(cid:18)

(cid:18)

(25)

(26)

(27)

(28)

(29)

(30)

(31)

(32)

(cid:1) (cid:111)

(33)

(34)

(35)

The Owen’s T function T (a, b) = φ(a) (cid:82) b
φ(aτ )
1+τ 2 dτ
0
(Owen, 1956) has eﬃcient numerical solutions in
practise (Pateﬁeld and Tandy, 2000).

The ELBO is considerably more complex than the
standard stochastic variational bound of a Gaus-
sian process (Hensman et al., 2013), due to the
probit-sparsiﬁed covariance. The bound is likely
only tractable for the choice of probit link func-
tion Φ(g), while other link functions such as the
logit would lead to intractable bounds necessitating
slower numerical integration (Hensman et al., 2015).

We optimize the Lzi with stochastic gradient as-
cent techniques with respect to the inducing lo-
inducing value means mf , mg and
cations zg, zf ,
the sparsity prior mean β,
covariances Sf , Sg,
the noise variance σ2
y, the signal variances σf , σg,
and ﬁnally the dimensions-speciﬁc lengthscales
(cid:96)f 1, . . . , (cid:96)f D; (cid:96)g1, . . . , (cid:96)gD of the Gaussian ARD ker-
nel.

4 GAUSSIAN PROCESS

NETWORK

The Gaussian Process Regression Networks (GPRN)
framework by Wilson et al. (2012) is an eﬃcient
model for multi-target regression problems, where
each individual output is a linear but non-stationary
combination of shared latent functions. Formally, a
vector-valued output function y(x) ∈ RP with P
outputs is modeled using vector-valued latent func-
tions f (x) ∈ RQ with Q latent values and mixing
weights W (x) ∈ RP ×Q as

y(x) = W (x)[f (x) + (cid:15)] + ε,

(36)

where for all q = 1, . . . , Q and p = 1, . . . , P we as-
sume GP priors and additive zero-mean noises,

fq(x) ∼ GP(0, Kf (x, x(cid:48)))
Wqp(x) ∼ GP(0, Kw(x, x(cid:48)))

(cid:15)q ∼ N (0, σ2
f )
εp ∼ N (0, σ2
y).

(37)

(38)

(39)

(40)

The subscripts are used to denote individual compo-
nents of f and W with p and q indicating pth output
dimension and qth latent dimension, respectively.
We assume shared latent and output noise variances
σ2
f , σ2
y without loss of generality. The distributions
of both functions f and W have been inferred either
with variational EM (Wilson et al., 2012) or by vari-
ational mean-ﬁeld approximation with diagonalized
latent and mixing functions (Nguyen and Bonilla,
2013).

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

4.1 STOCHASTIC VARIATIONAL

INFERENCE

Here, we ﬁrst extend the works of Wilson et al.
(2012) and Nguyen and Bonilla (2013) by intro-
ducing the currently missing SVI bounds for the
standard GPRN, and then propose the novel sparse
GPRN model, and solve its SVI bounds as well, in
the following section.

We begin by introducing the inducing variable aug-
mentation technique for latent functions f (x) and
mixing weights W (x) with uf , zf = {ufq , zfq }Q
and uw, zw = {uwqp, zwqp}Q,P

q,p=1:

q=1

p(y, f , W, uf , uw)

(41)
= p(y|f , W )p(f |uf )p(W |uw)p(uf )p(uw)

p(f |uf ) =

N (fq|Qfq ufq , (cid:101)Kfq )

(42)

p(W |uw) =

N (wqp|Qwqpuwqp, (cid:101)Kwqp)

(43)

p(uf ) =

N (ufq |0, Kfq,mm)

(44)

p(uw) =

N (uwqp|0, Kwqp,mm),

(45)

where we have separate kernels K and extrapolation
matrices Q for each component of W (x) and f (x)
that are of the same form as in equations (17–20).
The w is a vectorised form of W . The variational
approximation is then

q(f , W, uf , uw) = p(f |uf )p(W |uw)q(uf )q(uw)

q(ufq ) =

N (ufq |mfq , Sfq )

(46)

(47)

q(uwqp) =

N (uwqp|mwqp, Swqp),

(48)

where uwqp and ufq indicate the inducing points for
the functions Wqp(x) and fq(x), respectively. The
ELBO can be now stated as

log p(y) ≥ Eq(f ,W ) log p(y|f , W )

(49)

− KL[q(uf , uw)||p(uf , uw)],

where the variational distributions decompose as
q(f , W ) = q(f )q(W ) with marginals of the same form

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

as in equations (28–31),

We extend the GPRN with probit sparsity for the
mixing matrix W , resulting in a joint model

q(f ) =

q(f |uf )q(uf )duf = N (f |µf , Σf )

(50)

p(y, f , W, g) = p(y|f , W )p(f )p(W |g)p(g),

(54)

(cid:90)

(cid:90)

q(W ) =

q(W |uw)q(uw)duw = N (w|µw, Σw).

(51)

where all individual components of the latent func-
tion f and mixing matrix W are given GP priors.
We encode the sparsity terms g for all the Q × P
mixing functions Wqp(x) as

Since the noise term ε is assumed to be isotropic
Gaussian, the density p(y|W, f ) factorises across all
target observations and dimensions. The expecta-
tion term in equation (49) then reduces to solving
the following integral for the ith observation and pth
target dimension,

N,P
(cid:88)

(cid:90) (cid:90)

i,p=1

log N (yp,i|wT

p,ifi, σ2

y)q(fi, wp,i)dwp,idfi.

p(Wqp|gqp) = N (wqp|0, Φ(gqp)Φ(gqp)T ◦ Kw).

(55)

To introduce variational inference, the joint model
is augmented with three sets of inducing variables
for f , W and g. After marginalizing out the induc-
ing variables as in equations (25–27), the marginal
likelihood can be written as

(52)

log p(y) ≥ Eq(f ,W,g) log p(y|f , W )

(56)

− KL[q(uf , uw, ug)||p(uf , uw, ug)].

Q,P
(cid:88)

(cid:16)

q,p=1

−

−

1
2σ2
y

Q,P
(cid:88)

q,p

The above integral has a closed form solution result-
ing in the ﬁnal ELBO as (See Supplements)

Lgprn =

N
(cid:88)

(cid:40) P

(cid:88)

i=1

p=1

(cid:16)

log N

yp,i|

µwqp,iµfq,i, σ2
y

(cid:17)

Q
(cid:88)

q=1

wqp,iσ2
µ2

fq,i + µ2

fq,iσ2

wqp,i + σ2

wqp,iσ2

fq,i

(cid:41)
(cid:17)

KL[q(uwqp, ufq )||p(uwqp, ufq )],

(53)

where µfq,i is the i’th element of µfq and σ2
fq,i is
the i’th diagonal element of Σfq (similarly for the
Wqp’s).

5 SPARSE GAUSSIAN PROCESS

NETWORK

In this section we demonstrate how zero-inﬂated
GPs can be used as plug-in components in other
In particular, we propose a sig-
standard models.
niﬁcant modiﬁcation to GPRN by adding sparsity
to the mixing matrix components. This corresponds
to each of the p outputs being a sparse mixture of
the latent Q functions, i.e. they can eﬀectively use
any subset of the Q latent dimensions by having ze-
ros for the rest in the mixing functions. This makes
the mixture more easily interpretable, and induces
a variable number of latent functions to explain the
output of each input x. The latent function f can
also be sparsiﬁed, with a derivation analogous to the
derivation below.

The joint distribution in the variational expecta-
tion factorizes as q(f , W, g) = q(f )q(W |g)q(g). Also,
with a Gaussian noise assumption, the expectation
term factories across all the observations and tar-
get dimensions. The key step reduces to solving the
following integrals:

N,P
(cid:88)

(cid:90) (cid:90) (cid:90)

i,p=1

log N (yp,i|(wp,i ◦ gp,i)T fi, σ2
y)

(57)

· q(fi, wp,i, gp,i)dwp,idfidgp,i.

The above integral has a tractable solution leading
to the ﬁnal sparse GPRN evidence lower bound (See
Supplements)

Lsgprn =

N
(cid:88)

(cid:40) P

(cid:88)

i=1

p=1

(cid:16)

log N

yp,i|

µwqp,iµgqp,iµfq,i, σ2
y

(cid:17)

Q
(cid:88)

q=1

(58)

wqp,iσ2

(cid:17)
fq,i)

−

1
2σ2
y

Q,P
(cid:88)

(cid:16)

q,p=1

(µ2

gqp,i + σ2

gqp,i)

· (µ2

wqp,iσ2

fq,i + µ2

Q,P
(cid:88)

(cid:16)

q,p=1

gqp,iµ2
σ2

fq,iµ2

wqp,i

fq,iσ2
wqp,i + σ2
(cid:41)

(cid:17)

KL[q(ufq , uwqp, ugqp)||p(ufq , uwqp, ugqp)],

−

−

1
2σ2
y

Q,P
(cid:88)

q,p

where µfq,i, µwqp,i are the variational expectation
means for f (·), W (·) as in equations (28, 29), µgqp,i is
the variational expectation mean of g(·) as in equa-
tion (33), and analogously for the variances.

Table 1: Results for the precipitation dataset over
baseline (Zero; majority voting),
four competing
methods and the proposed method ZiGP on test
data. The columns list both quantitative and qual-
itative performance criteria, best performance is
boldfaced.

Model
Zero
GPc
GPr
GPcr
GPcr(cid:54)=0
ZiGP

RMSE MAE
0.104
0.615
-
-
0.159
0.569
0.589
0.102
0.575
0.101
0.121
0.561

F1
0.000
0.367
0.401
0.366
0.358
0.448

Acc.
0.898
0.911
0.750
0.911
0.912
0.861

Prec. Recall
0.000
0.675
0.266
0.679
0.712
0.381

0.000
0.252
0.817
0.251
0.240
0.558

6 EXPERIMENTS

First we demonstrate how the proposed method can
be used for regression problems with zero-inﬂated
targets. We do that both on a simulated dataset
and for real-world climate modeling scenarios on
a Finnish rain precipitation dataset with approx-
imately 90% zeros. Finally, we demonstrate the
GPRN model and how it improves both the inter-
pretability and predictive performance in the JURA
geological dataset.

We use the squared exponential kernel with ARD in
all experiments. All the parameters including in-
ducing locations, values and variances and kernel
parameters were learned through stochastic Adam
optimization (Kingma and Ba, 2014) on the Tensor-
Flow (Abadi et al., 2016) platform.

We compare our approach ZiGP to baseline Zero
voting, to conventional Gaussian process regression
(GPr) and classiﬁcation (GPc) with SVI approxi-
mations from the GPﬂow package (Matthews et al.,
2017). Finally, we also compare to ﬁrst classifying
the non-zeros, and successively applying regression
either to all data points (GPcr), or to only pre-
dicted non-zeros (GPcr(cid:54)=0, hurdle model).

We record the predictive performance by consider-
ing mean squared error and mean absolute error. We
also compare the models’ ability to predict true ze-
ros with F1, accuracy, precision, and recall of the
optimal models.

6.1 SPATIO-TEMPORAL DATASET

Zero-inﬂated cases are commonly found in clima-
tology and ecology domains.
In this experiment
we demonstrate the proposed method by model-
ing precipitation in Finland3. The dataset consists

3Data

be
ilmatieteenlaitos.fi/

can

found

at

http://en.

Figure 3: ZiGP model ﬁt on the precipitation
dataset. Sample of the actual data (a) against the
sparse rain function estimate (b), with the probit
support function (c) showing the rain progress.

of hourly quantitative non-negative observations of
precipitation amount across 105 observatory loca-
tions in Finland for the month of July 2018. The
dataset contains 113015 datapoints with approxi-
mately 90% zero precipitation observations. The
latitude, longi-
data inputs are three-dimensional:
tude and time. Due to the size of the data, this ex-
periment illustrates the scalability of the variational
inference.

We randomly split 80% of the data for training and
the rest 20% for testing purposes. We split across
time only, such that at a single measurement time,
all locations are simultaneously either in the training
set, or in the test set.

We further utilize the underlying spatio-temporal
grid structure of the data to perform inference in an
eﬃcient manner by Kronecker techniques (Saatchi,
2011). All the kernels for latent processes are as-
sumed to factorise as K = Kspace ⊗ Ktime which
allows placing inducing points independently on spa-
tial and temporal grids.

Figure 4: The distribution of errors with the rain
dataset with the ZiGP and the GPr. The zero-
inﬂated GP achieves much higher number of perfect
(zero) predictions.

Figure 3 depicts the components of the zero-inﬂated
GP model on the precipitation dataset. As shown
in panel (c), the latent support function models the
presence or absence of rainfall. It smoothly follows
the change in rain patterns across hourly observa-
tions. The amount of precipitation is modeled by
the other latent process and the combination of these
two results in sparse predictions. Figure 4 shows
that the absolute error distribution is remarkably
better with the ZiGP model due to it identifying
the absence of rain exactly. While both models ﬁt
the high rainfall regions well, for zero and near-zero
regions GPr does not reﬁne its small errors. Table
1 indicates that the ZiGP model achieves the lowest
mean square error, while also achieving the highest
F1 score that takes into account the class imbalance,
which biases the elementary accuracy, precision and
recall quantities towards the majority class.

6.2 MULTI-OUTPUT PREDICTION

In this experiment we model the multi-response Jura
dataset with the sparse Gaussian process regression
network sGPRN model and compare it with stan-
dard GPRN as baseline. Jura contains concentra-
tion measurements of cadmium, nickel and zinc met-
als in the region of Swiss Jura. We follow the ex-
perimental procedure of Wilson et al. (2012) and
Nguyen and Bonilla (2013). The training set con-
sists of n = 259 observations across D = 2 dimen-
sional geo-spatial locations, and the test set consists
of 100 separate locations. For both models we use
Q = 2 latent functions with the stochastic varia-
tional inference techniques proposed in this paper.
Sparse GPRN uses a sparsity inducing kernel in the
mixing weights. The locations of inducing points for

Figure 5: The sparse GPRN model ﬁt on the Jura
dataset with 11 inducing points. The Q = 2 (dense)
latent functions (a) are combined with the 3 × 2
sparse mixing functions (b) into the P = 3 output
predictions (c). The real data are shown in (d).
The white mixing regions are estimated ‘oﬀ’.

the weights W (x) and the support g(x) are shared.
The kernel length-scales are given a gamma prior
with the shape parameter α = 0.3 and rate param-
eter β = 1.0 to induce smoothness. We train both
the models 30 times with random initialization.

Figure 6: The sparse probit support (a) and latent
functions (b) of the weight function W (x) of the
optimized sparse GPRN model. The black regions
of (a) show regional activations, while the white re-
gions show where the latent functions are ‘oﬀ’. The
elementwise product of the support and weight func-
tions is indicated in the Figure 5b).

Table 6.2 shows that our model performs better than
the state-of-the-art SVI-GPRN, both with m = 5
and m = 10 inducing points. Figure 5 visualises the
optimized sparse GPRN model, while Figure 6 in-
dicates the sparsity pattern in the mixing weights.
The weights have considerable smooth ‘on’ regions
(black), and also interesting smooth ‘oﬀ’ regions
(white). The ‘oﬀ’ regions indicate that for certain lo-

Table 2: Results for the Jura dataset for sparse
GPRN and vanilla GPRN models with test data.
Best performance is with boldface. We do not report
RMSE and MAE values GPc, since its a classiﬁca-
tion method.

Model m RMSE MAE RMSE MAE RMSE MAE
GPRN
5
22.14
22.75
sGPRN 5
25.10
10
GPRN
sGPRN 10
23.63

0.572
0.567
0.586
0.573

0.732
0.728
0.774
0.749

6.807
6.631
7.207
6.524

5.163
5.079
5.656
5.054

34.41
35.09
37.87
36.17

cations, only one of the two latent functions is adap-
tively utilised.

7 DISCUSSION

We proposed a novel paradigm of zero-inﬂated Gaus-
sian processes with a novel sparse kernel. The spar-
sity in the kernel is modeled with smooth probit
ﬁltering of the covariance rows and columns. This
model induces zeros in the prediction function out-
puts, which is highly useful for zero-inﬂated datasets
with excess of zero observations. Furthermore, we
showed how the zero-inﬂated GP can be used to
model sparse mixtures of latent signals with the pro-
posed sparse Gaussian process network. The latent
mixture model with sparse mixing coeﬃcients leads
to locally using only a subset of the latent functions,
which improves interpretability and reduces model
complexity. We demonstrated tractable solutions to
stochastic variational inference of the sparse probit
kernel for the zero-inﬂated GP, conventional GPRN,
and sparse GPRN models, which lends to eﬃcient
exploration of the parameter space of the model.

References

M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis,
J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Is-
ard, et al. Tensorﬂow: A system for large-scale
machine learning. In OSDI, volume 16, pages 265–
283, 2016.

Z. Abraham and P-N. Tan. An integrated frame-
work for simultaneous classiﬁcation and regression
In Proceedings of the 2010
of time-series data.
SIAM International Conference on Data Mining,
pages 653–664. SIAM, 2010.

S. Ancelet, M-P. Etienne, H. Benot, and E. Parent.
Modelling spatial zero-inﬂated continuous data
with an exponentially compound Poisson process.
Environmental and Ecological Statistics, 2009.

M. Andersen, O. Winther, and L. Hansen. Bayesian

inference for structured spike and slab priors. In
NIPS, pages 1745–1753, 2014.

S. Barry and A. H. Welsh. Generalized additive
modelling and zero inﬂated count data. Ecolog-
ical Modelling, 157:179–188, 2002.

D. Bohning, E. Dierz, and P. Schlattmann. Zero-
inﬂated count models and their applications in
public health and social science. In J. Rost and
R. Langeheine, editors, Applications of Latent
Trait and Latent Class Models in the Social Sci-
ences. Waxman Publishing Co, 1997.

S. Charles, B. Bates, I. Smith, and J. Hughes. Sta-
tistical downscaling of daily precipitation from ob-
served and modelled atmospheric ﬁelds. Hydrolog-
ical Processes, pages 1373–1394, 2004.

J.G. Cragg.

Some statistical models for limited
dependent variables with application to the de-
mand for durable goods. Econometrica, 39:829–
844, 1971.

S. del Saz-Salazar and P. Rausell-K¨oster. A double-
hurdle model of urban green areas valuation: deal-
ing with zero responses. Landscape and urban
planning, 84(3-4):241–251, 2008.

W. Enke and A. Spekat. Downscaling climate model
outputs into local and regional weather elements
by classiﬁcation and regression. Climate Research,
8:195–207, 1997.

J. Hensman, N. Fusi, and N. Lawrence. Gaus-
sian processes for big data. In Proceedings of the
Twenty-Ninth Conference on Uncertainty in Ar-
tiﬁcial Intelligence, pages 282–290. AUAI Press,
2013.

J. Hensman, A. Matthews, and Z. Ghahramani.
Scalable variational Gaussian process classiﬁca-
tion. In Artiﬁcial Intelligence and Statistics, pages
351–360, 2015.

W. Herlands, A. Wilson, H. Nickisch, S. Flaxman,
D. Neill, W. van Panhuis, and E. Xing. Scalable
Gaussian processes for characterizing multidimen-
sional change surfaces. In AISTATS, volume 51 of
PMLR, pages 1013–1021, 2016.

N. Johnson and S Kotz. Distributions in Statistics:
Discrete Distributions. Houghton MiZin, Boston,
1969.

D. Kingma and J. Ba. Adam: A method for stochas-

tic optimization. arXiv:1412.6980, 2014.

D. Lambert. Zero-inﬂated Poisson regression with
an application to defects in manufacturing. Tech-
nometrics, 34:1–14, 1992.

M. L´azaro-Gredilla, S. Van Vaerenbergh, and
N. Lawrence. Overlapping mixtures of Gaussian
processes for the data association problem. Pat-
tern Recognition, 45(4):1386–1395, 2012.

A. Matthews, M. van der Wilk, T. Nickson, K. Fu-
jii, A. Boukouvalas, P. Leon-Villagra, Z. Ghahra-
mani, and J. Hensman. GPﬂow: A Gaussian pro-
cess library using TensorFlow. Journal of Machine
Learning Research, 18(40):1–6, 2017.

J. Mullahy. Speciﬁcation and testing of some modi-
ﬁed count data models. Journal of Econometrics,
33:341–365, 1986.

T. Nguyen and E. Bonilla. Eﬃcient variational infer-
ence for Gaussian process regression networks. In
Proceedings of the Sixteenth International Confer-
ence on Artiﬁcial Intelligence and Statistics, vol-
ume 31 of Proceedings of Machine Learning Re-
search, pages 472–480. PMLR, 2013.

D.B. Owen. Tables for computing bivariate normal
probabilities. Annals of Mathematical Statistics,
27:1075–1090, 1956.

M. Pateﬁeld and D. Tandy. Fast and accurate calcu-
lation of owens t-function. Journal of Statistical
Software, 5:1–25, 2000.

C. Rasmussen and Z. Ghahramani. Inﬁnite mixtures
of Gaussian process experts. In NIPS, pages 881–
888, 2002.

C.E. Rasmussen and K.I. Williams. Gaussian pro-
cesses for machine learning. MIT Press, 2006.

Y. Saatchi. Scalable Inference for Structured Gaus-
sian Process Models. PhD thesis, University of
Cambridge, 2011.

H. Salimbeni and M. Deisenroth. Doubly stochastic
variational inference for deep Gaussian processes.
In NIPS, volume 30, 2017.

M. Titsias. Variational learning of inducing vari-
In Artiﬁcial

ables in sparse Gaussian processes.
Intelligence and Statistics, pages 567–574, 2009.

V. Tresp. Mixtures of Gaussian processes. In NIPS,

pages 654–660, 2001.

R.L. Wilby. Statistical downscaling of daily precipi-
tation using daily airﬂow and seasonal teleconnec-
tion. Climate Research, 10:163–178, 1998.

A. G. Wilson, D. Knowles, and Z. Ghahramani.
Gaussian process regression networks. In ICML,
2012.

Supplementary material for the “Zero-inﬂated Gaussian processes with sparse
kernels”

Here we show in detail how we arrived at the three new evidence lower bounds for the zero-inﬂated Gaussian
process, for the Gaussian process network, and for the sparse Gaussian process network.

A) The stochastic variational bound of the zero-inﬂated GP

Here, we will derive the evidence lower bound (ELBO) of the zero-inﬂated Gaussian process. We show how
to solve the ELBO of equations (24) and (25), which results in the equation (32).

The augmented true model with inducing points is deﬁned

We deﬁne the variational posterior approximation as

p(y, f , g, uf , ug) = p(y|f )p(f |g, uf )p(g|ug)p(uf )p(ug)

p(y|f ) = N (y|f , σ2

yI)

p(f |g, uf ) = N (f | diag(Φ(g))Kf nmK −1

f mmuf , Φ(g)Φ(g)T ◦ (cid:101)Kf )

p(g|ug) = N (g|KgnmK −1
p(uf ) = N (uf |0, Kf mm)
p(ug) = N (ug|0, Kgmm).

gmmug, (cid:101)Kg)

q(f , g, uf , ug) = p(f |g, uf )p(g|ug)q(uf )q(ug)
p(f |g, uf ) = N (f | diag(Φ(g))Kf nmK −1

f mmuf , Φ(g)Φ(g)T ◦ (cid:101)Kf )

p(g|ug) = N (g|KgnmK −1
q(uf ) = N (uf |mf , Sf )
q(ug) = N (ug|mg, Sg)

gmmug, (cid:101)Kg)

and where Sf , Sg ∈ Rm×m are square positive semi-deﬁnite matrices, and we deﬁne shorthands

(cid:101)Kf = Kf nn − Kf nmK −1
(cid:101)Kg = Kgnn − KgnmK −1

f mmKf mn
gmmKgmn.

In variational inference we minimize the Kullback-Leibler divergence between the variational approximation
q(f , g, uf , ug) and the true augmented joint distribution p(y, f , g, uf , ug):

KL[q(f , g, uf , ug)||p(y, f , g, uf , ug)] =

q(f , g, uf , ug) log

=

q(f , g, uf , ug) log

df dgduf dug

df dgduf dug

p(y, f , g, uf , ug)
q(f , g, uf , ug)
p(y|f )p(f |g, uf )p(g|ug)p(uf )p(ug)
p(f |g, uf )p(g|ug)q(uf )q(ug)

(cid:90)

(cid:90)

(cid:90)

=

=

(cid:90) (cid:90) (cid:90) (cid:90)

(cid:90)

(cid:124)

q(f , g, uf , ug) log

p(y|f )p(uf )p(ug)
q(uf )q(ug)

df dgduf dug

p(f |g, uf )p(g|ug)q(uf )q(ug) log p(y|f )duf dugdgdf

(S.17)

−

q(uf ) log q(uf )duf

−

q(ug) log q(ug)dug

(S.18)

(cid:123)(cid:122)
KL[q(uf )||p(uf )]

(cid:125)

(cid:123)(cid:122)
KL[q(ug)||p(ug)]

(cid:125)

(cid:90)

(cid:124)

(S.1)

(S.2)

(S.3)

(S.4)

(S.5)

(S.6)

(S.7)

(S.8)

(S.9)

(S.10)

(S.11)

(S.12)

(S.13)

(S.14)

(S.15)

(S.16)

Following the derivation of Hensman et al. (2015), this corresponds to maximizing the evidence lower bound
(ELBO) of equation (24):

(cid:90) (cid:90) (cid:90) (cid:90)

log p(y) ≥

log p(y|f )p(f |g, uf )q(uf )p(g|ug)q(ug)duf dugdgdf − KL[q(uf , ug)||p(uf , ug)]

(S.19)

= Eq(f ) log p(y|f ) − KL[q(uf , ug)||p(uf , ug)]

where we deﬁne

q(f ) =

p(f |g, uf )q(uf )p(g|ug)q(ug)duf dugdg

(cid:90) (cid:90) (cid:90)

(cid:90)

=

q(f |g)q(g)dg,

where the variational approximations are tractably

q(g) =

p(g|ug)q(ug)dug

=

N (g|KgnmK −1

gmmug, (cid:101)Kg)N (ug|mg, Sg)dug

= N (g|µg, Σg)

q(f |g) =

p(f |g, uf )q(uf )duf

(cid:90)

(cid:90)

(cid:90)

(cid:90)

=

N (f | diag(Φ(g))Kf nmK −1

f mmuf , Φ(g)Φ(g)T ◦ (cid:101)Kf )N (uf |mf , Sf )duf

= N (f | diag(Φ(g))µf , Φ(g)Φ(g)T ◦ Σf )

with

f mmmf
gmmmg

µf = Kf nmK −1
µg = KgnmK −1
Σf = Kf nn + Kf nmK −1
Σg = Kgnn + KgnmK −1

f mm(Sf − Kf mm)K −1
gmm(Sg − Kgmm)K −1

f mmKf mn
gmmKgmn.

The variational marginalizations q(g) and q(f |) follow from standard Gaussian identities4. Substituting the
variational marginalizations back to the ELBO results in

(cid:90) (cid:90)

log p(y) ≥

log p(y|f )q(f |g)q(g)df dg − KL[q(uf , ug)||p(uf , ug)].

(S.32)

Next, we marginalize the f from the ELBO. We additionally assume the likelihood p(y|f ) = (cid:81)N
factorises, which results in
(cid:90)

(cid:90)

i=1 p(yi|fi)

log p(y|f )q(f |g)df =

log N (y|f , σ2

yI)q(f |g)df

f

log N (yi|fi, σ2

y)q(fi|gi)dfi

log N (yi|Φ(gi)kT

f iK −1

f mmmf i, σ2

y) −

Φ(gi)2(kf ii + kT

f iK −1

f mm(Sf − Kf mm)K −1

f mmkf i)

(cid:111)

(cid:110)

1
2σ2
y

=

log N (yi|Φ(gi)µf i, σ2

y) −

1
2σ2
y

(cid:8)Φ(gi)2σ2

f i

(cid:9) ,

4See for instance Bishop (2006): Pattern recognition and Machine learning, Springer, Section 2.3.

=

=

N
(cid:88)

(cid:90)

i=1

N
(cid:88)

i=1

N
(cid:88)

i=1

(S.20)

(S.21)

(S.22)

(S.23)

(S.24)

(S.25)

(S.26)

(S.27)

(S.28)

(S.29)

(S.30)

(S.31)

(S.33)

(S.34)

(S.35)

(S.36)

where

µf i = [µf ]i = kT
f i = [Σf ]ii = kf ii + kT
σ2

f iK −1

f mmmf i

f iK −1

f mm(Sf − Kf mm)K −1

f mmkf i.

Substituting the above result into the ELBO results in

Eq(f ) log p(y|f ) =

q(g)

q(f |g) log p(y|f )df dg

(cid:90)

f

(cid:90)

g
(cid:90)

g

N
(cid:88)

i=1

N
(cid:88)

i=1

=

=

=

N
(cid:88)

i=1
(cid:90)

gi

log N (yi|Φ(gi)µf i, σ2

y) −

(cid:8)Φ(gi)2σ2

f i

(cid:9) q(g)dg

1
2σ2
y

log N (yi|Φ(gi)µf i, σ2

y) q(gi)dgi −

(cid:8)Φ(gi)2σ2

f i

(cid:9) q(gi)dgi

1
2σ2
y

N
(cid:88)

(cid:90)

i=1

gi

1
2σ2
y

N
(cid:88)

i=1

log N (yi|(cid:104)Φ(gi)(cid:105)q(gi)µf i, σ2

y) −

(cid:8)V ar[Φ(gi)](µf i)2(cid:9) −

(cid:8)(cid:104)Φ(gi)2(cid:105)q(gi)σ2

f i

(cid:9) .

1
2σ2
y

N
(cid:88)

i=1

The expectations (cid:104).(cid:105)q(gi) of CDF transformation of a random variable with univariate Gaussian distribution.
The analytical forms for these integrals can be written:

(cid:104)Φ(gi)(cid:105)q(gi) =

Φ(gi)q(gi)dgi

=

Φ(gi)N (gi|µgi, σ2




gi)dgi

= Φ



(cid:113)

µgi
1 + σ2
gi



V ar[Φ(gi)] =

(Φ(gi) − (cid:104)Φ(gi)(cid:105)q(gi))2q(gi)dgi

(cid:90)

(cid:90)

(cid:90)

(cid:90)







= Φ



(cid:113)

 − 2T



(cid:113)

µgi
1 + σ2
gi

µgi
1 + σ2
gi

,

(cid:113)

1 + 2 σ2
gi





 − Φ



(cid:113)


2



µgi
1 + σ2
gi

(cid:104)Φ(gi)2(cid:105)q(gi) =

Φ(gi)2q(gi)dgi





= Φ



(cid:113)

 − 2T



(cid:113)

µgi
1 + σ2
gi

µgi
1 + σ2
gi

,

(cid:113)

1 + 2 σ2
gi





1

1

where

µgi = [µg]i = kT
gi = [Σg]ii = Kgii + kT
σ2

giK −1

gmmmgi

giK −1

gmm(Sg − Kgmm)K −1

gmmkgi.

Owen’s T function is deﬁned as T (h, a) = φ(h) (cid:82) a
0

φ(hx)
1+x2 dx.

(S.37)

(S.38)

(S.39)

(S.40)

(S.41)

(S.42)

(S.43)

(S.44)

(S.45)

(S.46)

(S.47)

(S.48)

(S.49)

(S.50)

(S.51)

The ﬁnal evidence lower bound with the Kullback-Leibler terms is

p(y) ≥

log N (yi|(cid:104)Φ(gi)(cid:105)q(gi)µf i, σ2

y) −

(cid:0)V ar[Φ(gi)] µ2

f i + (cid:104)Φ(gi)2(cid:105)q(gi) σ2
f i

(cid:27)

(cid:1)

log |Kf mm| −

log |Sf | +

T r

(mf mf

T + Sf )K −1

f mm

(cid:105)

−

log |Kgmm| −

log |Sg| +

T r (cid:2)(mgmg

T + Sg)K −1

gmm

(cid:3) −

1
2
1
2

(cid:27)

m
2
(cid:27)
m
2

1
2σ2
y

(cid:104)

1
2
1
2

N
(cid:88)

(cid:26)

i=1

−

−

(cid:26) 1
2
(cid:26) 1
2

= LZiGP

B) The stochastic variational bound of the Gaussian process network

In GPRN a vector-valued output function y(x) ∈ RP with P outputs is modeled using vector-valued latent
functions f (x) ∈ RQ with Q latent values and mixing weights W (x) ∈ RP ×Q as

where for all q = 1, . . . , Q and p = 1, . . . , P we assume GP priors and additive zero-mean noises,

y(x) = W (x)[f (x) + (cid:15)] + ε,

fq(x) ∼ GP(0, Kf (x, x(cid:48)))
Wqp(x) ∼ GP(0, Kw(x, x(cid:48)))

(cid:15)q ∼ N (0, σ2
f )
εp ∼ N (0, σ2
y).

The subscripts are used to denote individual components of f and W with p and q indicating pth output
dimension and qth latent dimension respectively.

We begin by introducing the inducing variable augmentation for latent functions f (x) and mixing weights
W (x) with uf , zf = {ufq , zfq }Q

q=1 and uw, zw = {uwqp, zwqp}Q,P

q,p=1:

p(y, f , W, uf , uw) = p(y|f , W )p(f |uf )p(W |uw)p(uf )p(uw)

p(f |uf ) =

N (fq|Qfq ufq , (cid:101)Kfq )

p(W |uw) =

N (wqp|Qwqpuwqp, (cid:101)Kwqp)

p(uf ) =

N (ufq |0, Kfq,mm)

p(uw) =

N (uwqp|0, Kwqp,mm),

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

Qf = KfqnmK −1
fqmm
Qg = KwqpnmK −1
(cid:101)Kf = Kfqnn − KfqnmK −1
(cid:101)Kwqp = Kwqpnn − KwqpnmK −1

wqpmm

fqmmKfqmn

wqpmmKwqpmn.

where we have separate kernels K and extrapolation matrices Q for each component of W (x) and f (x) that
are of the form as given below:

(S.52)

(S.53)

(S.54)

(S.55)

(S.56)

(S.57)

(S.58)

(S.59)

(S.60)

(S.61)

(S.62)

(S.63)

(S.64)

(S.65)

(S.66)

(S.67)

(S.68)

(S.69)

Following the variational inference framework, we deﬁne the variational joint distribution as,

q(f , W, uf , uw) = p(f |uf )p(W |uw)q(uf )q(uw)

q(ufq ) =

N (ufq |mfq Sfq )

q(uwqp) =

N (uwqp|mwqp, Swqp),

Q
(cid:89)

q=1

Q,P
(cid:89)

q,p=1

where uwqp and ufq indicate the inducing points for functions Wqp(x) and fq(x), respectively. The ELBO
can be now stated as

log p(y) ≥ Eq(f ,W,uf ,uw) log p(y|f , W ) − KL[q(uf , uw)||p(uf , uw)]

(cid:90) (cid:90) (cid:90) (cid:90)

=

q(f , W, uf , uw) log p(y|f , W )df dW duf duw − KL[q(uf , uw)||p(uf , uw)]

Since the variational joint posterior decomposes as equation (S.70), we begin by marginalizing the inducing
distributions uf and uw,

q(f , W, uf , uw)df dW duf duw =

p(f |uf )q(uf )df duf

p(W |uw)q(uw)dW duw

(S.75)

uf

(cid:90)

(cid:90)

f
(cid:90)

f

(cid:90)

W

=

q(f )df

q(W )dW

(cid:90)

(cid:90)

W

uw

(cid:90) (cid:90) (cid:90) (cid:90)

where

with

q(f ) =

p(f |uf )q(uf )duf

N (fq|µfq , Σfq )

q(W ) =

p(W |uw)q(uw)duw

(cid:90)

Q
(cid:89)

(cid:90)

q=1

Q
(cid:89)

q=1
(cid:90)

Q,P
(cid:89)

(cid:90)

q,p=1

Q,P
(cid:89)

q,p=1

=

=

=

=

N (Wqp|µwqp, Σwqp)

N (fq|KfqnmK −1

fqmmufq , (cid:101)Kfq )N (ufq |mfq , Sfq )dufq

N (Wqp|KwqpnmK −1

wqpmmuwqp, (cid:101)Kwqp)N (uwqp|mwqp, Swqp)duwqp

µfq = KfqnmK −1
fqmmmfq
µwqp = KwqpnmK −1
wqpmmmwqp
Σfq = Kfqnn + KfqnmK −1
Σwqp = Kwqpnn + KwqpnmK −1

fqmm(Sfq − Kfqmm)K −1

fqmmKfqmn

wqpmm(Swqp − Kwqpmm)K −1

wqpmmKwqpmn.

Since the noise term ε is assumed to be isotropic Gaussian, density p(y|W, f ) factorises across all target

(S.70)

(S.71)

(S.72)

(S.73)

(S.74)

(S.76)

(S.77)

(S.78)

(S.79)

(S.80)

(S.81)

(S.82)

(S.83)

(S.84)

(S.85)

(S.86)

(S.87)

(S.88)

(S.89)

(S.90)

(S.91)

(S.92)

(S.93)

(S.94)

(cid:41)
(cid:17)

(S.95)

(S.96)

(S.97)

observations and dimensions. The expectation term in the ELBO then reduces to,

log p(y) ≥ Eq(W )Eq(f ) log p(y|f , W )
(cid:90) (cid:90)

=

N,P
(cid:88)

i,p=1

(cid:90)

The integral with respect to f can be now solved as

log N (yp,i|wT

p,ifi, ε2

p)q(fi, wp,i)dwp,idfi − KL[q(uf , uw)||p(uf , uw)].

log N (yp,i|wT

p,ifi, ε2

p)q(fi)dfi = log N (yp,i|wT

p,iµfi, ε2

p) −

T r(cid:2)wT

p,iΣfiwp,i

(cid:3)

= log N (yp,i|wT

p,iµfi, ε2

p) −

T r(cid:2)Σfiwp,iwT

p,i

(cid:3).

Next we can marginalize W from the above terms,

(cid:90)

log N (yp,i|wT

p,iµfi, ε2

p)q(wp,i)dwp,i = log N (yp,i|µT

wp,i

µfi, ε2

p) −

T r(cid:2)µT
fi

Σwq,iµfi

(cid:3)

1
2ε2
p
1
2ε2
p

1
2ε2
p

1
2ε2
p

Q
(cid:88)

q=1

= log N (yp,i|µT

µfi, ε2

p) −

wp,i

fq,iσ2
µ2

wqp,i

(cid:90)

1
2ε2
p

T r(cid:2)Σfiwp,iwT

p,i

(cid:3)q(wp,i)dwp,i =

T r(cid:2)Σfi(µwp,i µT

wp,i

+ Σwq,i)(cid:3)

1
2ε2
p

1
2ε2
p

=

Q
(cid:88)

(cid:16)

q=1

wqp,iσ2
µ2

fq,i + σ2

wqp,iσ2

fq,i

(cid:17)

.

Finally, adding the above results across all N observations and response dimensions P along with Gaussian
KL divergence terms, we get the ﬁnal lowerbound:

log p(y) ≥

N
(cid:88)

(cid:40) P

(cid:88)

i=1

p=1

(cid:16)

log N

yp,i|

µwqp,iµfq,i, ε2
p

Q
(cid:88)

q=1

(cid:17)

−

1
2ε2
p

Q,P
(cid:88)

(cid:16)

q,p=1

wqp,iσ2
µ2

fq,i + µ2

fq,iσ2

wqp,i + σ2

wqp,iσ2

fq,i

−

KL[q(uwqp, ufq )||p(uwqp, ufq )]

Q,P
(cid:88)

q,p

= Lgprn,

where µfq,i is the i’th element of µfq and σ2

fq,i is the i’th diagonal element of Σfq (similarly for Wqp’s).

C) The stochastic variational bound of the sparse Gaussian process network

Sparse GPRN is a modiﬁcation to standard GPRN where sparsity is added to the mixing matrix components.
This corresponds to the p’th output being a sparse mixture of the latent Q functions, i.e. it can eﬀectively
use any subset of the Q latent dimensions by having zeros in the mixing functions. The joint distribution
for the model can be written as,

p(y, f , W, g) = p(y|f , W )p(f )p(W |g)p(g),

(S.98)

where all individual components of latent function f and mixing matrix W are given GP priors. We encode
the sparsity terms g for all Q × P mixing functions Wqp(x) functions as

p(Wqp|gqp) = N (wqp|0, Φ(gqp)Φ(gqp)T ◦ Kw).

(S.99)

To introduce variational inference, the joint model is augmented with three sets of inducing variables for f ,
W and g. After marginalizing out the inducing variables similar to SVI for standard GPRN, the lower bund
for marginal likelihood can be written as

log p(y) ≥ Eq(f ,W,g) log p(y|f , W ) − KL[q(uf , uw, ug)||p(uf , uw, ug)].

(S.100)

Where the joint distribution in the variational expectation factorizes as q(f , W, g) = q(f )q(W |g)q(g). The
variational posterior after marginalizing inducing variables is written as,

q(f ) =

q(f |uf )q(uf )duf

=

N (fq|µfq , Σfq )

q(W ) =

q(W |uw)q(uw)duw

N (Wqp|µwqp, Σwqp)

q(g) =

q(g|ug)q(ug)dug

N (gqp|µgqp, Σgqp)

(cid:90)

Q
(cid:89)

q=1
(cid:90)

Q,P
(cid:89)

=

q,p=1
(cid:90)

Q,P
(cid:89)

=

q,p=1

with

µfq = KfqnmK −1
fqmmmfq
µwqp = KwqpnmK −1
wqpmmmwqp
µgqp = KgqpnmK −1
gqpmmmgqp
Σfq = Kfqnn + KfqnmK −1
Σwqp = Kwqpnn + KwqpnmK −1
Σgqp = Kgqpnn + KgqpnmK −1

fqmm(Sfq − Kfqmm)K −1

fqmmKfqmn

wqpmm(Swqp − Kwqpmm)K −1
gqpmm(Sgqp − Kgqpmm)K −1

gqpmmKgqpmn.

wqpmmKwqpmn

Similar to standard GPRN, with the isotropic Gaussian, density p(y|W, f ) factorizes across all target obser-
vations and dimensions. The expectation term in the ELBO then reduces to

log p(y) ≥ Eq(W |g)Eq(f )Eq(g) log p(y|f , W )

N,P
(cid:88)

(cid:90) (cid:90) (cid:90)

=

i,p=1

log N (yp,i|(wp,i ◦ gp,i)T fi, ε2

p)q(fi)q(wp,i|q(gp,i)q(gp,i)dwp,idfidgi − KL[q(uf , uw)||p(uf , uw)].

The integral with respect to f can be now solved as

(cid:90)

log N (yp,i|(wp,i ◦ gp,i)T fi, ε2

p)q(fi)dfi = log N (yp,i|(wp,i ◦ gp,i)T µfi, ε2

p) −

T r(cid:2)(wp,i ◦ gp,i)T Σfi(wp,i ◦ gp,i)(cid:3)

= log N (yp,i|(wp,i ◦ gp,i)T µfi, ε2

p) −

T r(cid:2)Σfi(gp,igT

p,i ◦ wp,iwT

p,i)(cid:3).

1
2ε2
p

1
2ε2
p

(S.101)

(S.102)

(S.103)

(S.104)

(S.105)

(S.106)

(S.107)

(S.108)

(S.109)

(S.110)

(S.111)

(S.112)

(S.113)

(S.114)

(S.115)

(S.116)

(cid:90)

1
2ε2
p

(cid:90)

(cid:90)

1
2ε2
p

Next, by integrating individual terms with respect to W we get

(cid:90)

log N (yp,i|(wp,i ◦ gp,i)T µfi, ε2

p)q(wp,i)dwp,i = log N (yp,i|(µwp,i ◦ gp,i)T µfi, ε2

p) −

1
2ε2
p

T r(cid:2)µT
fi

(gp,igT

p,i ◦ Σwq,i)µfi

(cid:3)

T r(cid:2)Σfi(gp,igT

p,i ◦ wp,iwT

p,i)(cid:3) =

T r(cid:2)Σfi(gp,igT

p,i ◦ (µwp,i µT

wp,i

+ Σwq,i))(cid:3)

1
2ε2
p

Finally, integrating all the above terms with respect to g, we get

log N (yp,i|(µwp,i ◦ gp,i)T µfi, ε2

p)q(gp,i)dgp,i = log N (yp,i|(µwp,i ◦ (cid:104)Φ(gp,i)(cid:105))T µfi, ε2
p)

(S.119)

T r(cid:2)µT
fi

(gp,igT

p,i ◦ Σwq,i)µfi

(cid:3)q(gp,i)dgp,i =

T r(cid:2)µT
fi

(cid:0)((cid:104)Φ(gp,i)(cid:105)(cid:104)Φ(gp,i)(cid:105)T + V ar[Φ(gp,i)]) ◦ Σwq,i

(cid:1) µfi

(cid:3)

(cid:90)

1
2ε2
p

T r(cid:2)Σfi(gp,igT

p,i ◦ (µwp,i µT

wp,i

+ Σwq,i ))(cid:3)q(gp,i)dgp,i =

((cid:104)Φ(gp,i)(cid:105)(cid:104)Φ(gp,i)(cid:105)T + V ar[Φ(gp,i)]) ◦ µwp,i µT

wp,i

(cid:17) (cid:3)

−

1
2ε2
p

T r(cid:2)µT
fi

(µwp,i µT

wp,i

◦ V ar[Φ(gp,i)])µfi]

(cid:16)

= log N

yp,i|

µwqp,iµgqp,iµfq,i, ε2
p

(cid:17)

Q
(cid:88)

q=1

−

1
2ε2
p

Q
(cid:88)

(cid:16)

q=1

gqp,iµ2
σ2

fq,iµ2

wqp,i

(cid:17)

=

(µ2

gqp,i + σ2

gqp,i)µ2

fq,iσ2

wqp,i

(cid:17)

1
2ε2
p

1
2ε2
p

1
2ε2
p

Q
(cid:88)

(cid:16)

q=1

(cid:16)

T r(cid:2)Σfi

+

1
2ε2
p

=

1
2ε2
p

Q
(cid:88)

(cid:16)

q=1

T r(cid:2)Σfi

(cid:0)((cid:104)Φ(gp,i)(cid:105)(cid:104)Φ(gp,i)(cid:105)T + V ar[Φ(gp,i)]) ◦ Σwq,i

(cid:1) (cid:3)

(µ2

gqp,i + σ2

gqp,i)(µ2

wqp,iσ2

fq,i + σ2

wqp,iσ2

Adding above results across all the observations N and output dimensions P , we retrieve the ﬁnal evidence
lower bound

p(y) ≥

N
(cid:88)

(cid:40) P

(cid:88)

i=1

p=1

(cid:16)

log N

yp,i|

µwqp,iµgqp,iµfq,i, ε2
p

(cid:17)

Q
(cid:88)

q=1

Q,P
(cid:88)

(cid:16)

−

q,p=1

(µ2

gqp,i + σ2

gqp,i) · (µ2

wqp,iσ2

fq,i + µ2

fq,iσ2

wqp,i + σ2

wqp,iσ2

gqp,iµ2
σ2

fq,iσ2

wqp,i

(cid:17)
fq,i)

−

Q,P
(cid:88)

(cid:16)

q,p=1

−

KL[q(ufq , uwqp, ugqp)||p(ufq , uwqp, ugqp)]

Q,P
(cid:88)

q,p

= Lsgprn.

(S.117)

(S.118)

(S.120)

(S.121)

(S.122)

(S.123)

(S.124)

(S.125)

(S.126)

(cid:17)
fq,i)

.

(S.127)

(S.128)

(cid:41)

(cid:17)

(S.129)

(S.130)

(S.131)

