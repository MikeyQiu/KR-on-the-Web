Distributional Reinforcement Learning with Quantile Regression

Will Dabney
DeepMind

Mark Rowland
University of Cambridge∗

Marc G. Bellemare
Google Brain

R´emi Munos
DeepMind

7
1
0
2
 
t
c
O
 
7
2
 
 
]
I

A
.
s
c
[
 
 
1
v
4
4
0
0
1
.
0
1
7
1
:
v
i
X
r
a

Abstract

In reinforcement learning an agent interacts with the environ-
ment by taking actions and observing the next state and re-
ward. When sampled probabilistically, these state transitions,
rewards, and actions can all induce randomness in the ob-
served long-term return. Traditionally, reinforcement learn-
ing algorithms average over this randomness to estimate the
value function. In this paper, we build on recent work ad-
vocating a distributional approach to reinforcement learning
in which the distribution over returns is modeled explicitly
instead of only estimating the mean. That is, we examine
methods of learning the value distribution instead of the value
function. We give results that close a number of gaps between
the theoretical and algorithmic results given by Bellemare,
Dabney, and Munos (2017). First, we extend existing results
to the approximate distribution setting. Second, we present
a novel distributional reinforcement learning algorithm con-
sistent with our theoretical formulation. Finally, we evaluate
this new algorithm on the Atari 2600 games, observing that
it signiﬁcantly outperforms many of the recent improvements
on DQN, including the related distributional algorithm C51.

Introduction
In reinforcement learning, the value of an action a in state s
describes the expected return, or discounted sum of rewards,
obtained from beginning in that state, choosing action a, and
subsequently following a prescribed policy. Because know-
ing this value for the optimal policy is sufﬁcient to act opti-
mally, it is the object modelled by classic value-based meth-
ods such as SARSA (Rummery and Niranjan 1994) and Q-
Learning (Watkins and Dayan 1992), which use Bellman’s
equation (Bellman 1957) to efﬁciently reason about value.

Recently, Bellemare, Dabney, and Munos (2017) showed
that the distribution of the random returns, whose expecta-
tion constitutes the aforementioned value, can be described
by the distributional analogue of Bellman’s equation, echo-
ing previous results in risk-sensitive reinforcement learning
(Heger 1994; Morimura et al. 2010; Chow et al. 2015). In
this previous work, however, the authors argued for the use-
fulness in modeling this value distribution in and of itself.
Their claim was asserted by exhibiting a distributional rein-
forcement learning algorithm, C51, which achieved state-of-

∗Contributed during an internship at DeepMind.

the-art on the suite of benchmark Atari 2600 games (Belle-
mare et al. 2013).

One of the theoretical contributions of the C51 work was
a proof that the distributional Bellman operator is a contrac-
tion in a maximal form of the Wasserstein metric between
probability distributions. In this context, the Wasserstein
metric is particularly interesting because it does not suffer
from disjoint-support issues (Arjovsky, Chintala, and Bot-
tou 2017) which arise when performing Bellman updates.
Unfortunately, this result does not directly lead to a practical
algorithm: as noted by the authors, and further developed by
Bellemare et al. (2017), the Wasserstein metric, viewed as a
loss, cannot generally be minimized using stochastic gradi-
ent methods.

This negative result left open the question as to whether it
is possible to devise an online distributional reinforcement
learning algorithm which takes advantage of the contraction
result. Instead, the C51 algorithm ﬁrst performs a heuristic
projection step, followed by the minimization of a KL di-
vergence between projected Bellman update and prediction.
The work therefore leaves a theory-practice gap in our un-
derstanding of distributional reinforcement learning, which
makes it difﬁcult to explain the good performance of C51.
Thus, the existence of a distributional algorithm that oper-
ates end-to-end on the Wasserstein metric remains an open
question.

In this paper, we answer this question afﬁrmatively. By
appealing to the theory of quantile regression (Koenker
2005), we show that there exists an algorithm, applicable in
a stochastic approximation setting, which can perform distri-
butional reinforcement learning over the Wasserstein metric.
Our method relies on the following techniques:

•

•

•

We “transpose” the parametrization from C51: whereas
the former uses N ﬁxed locations for its approxima-
tion distribution and adjusts their probabilities, we assign
ﬁxed, uniform probabilities to N adjustable locations;

We show that quantile regression may be used to stochas-
tically adjust the distributions’ locations so as to minimize
the Wasserstein distance to a target distribution.

We formally prove contraction mapping results for our
overall algorithm, and use these results to conclude that
our method performs distributional RL end-to-end under
the Wasserstein metric, as desired.

The main interest of the original distributional algorithm
was its state-of-the-art performance, despite still acting by
maximizing expectations. One might naturally expect that a
direct minimization of the Wasserstein metric, rather than its
heuristic approximation, may yield even better results. We
derive the Q-Learning analogue for our method (QR-DQN),
apply it to the same suite of Atari 2600 games, and ﬁnd that it
achieves even better performance. By using a smoothed ver-
sion of quantile regression, Huber quantile regression, we
gain an impressive 33% median score increment over the al-
ready state-of-the-art C51.

X

X

and

Distributional RL
We model the agent-environment interactions by a Markov
decision process (MDP) (
, R, P, γ) (Puterman 1994),
,
A
the state and action spaces, R the random
with
A
variable reward function, P (x(cid:48)
x, a) the probability of tran-
|
sitioning from state x to state x(cid:48) after taking action a, and
x) maps each
γ
∈
state x
For a ﬁxed policy π, the return, Z π = (cid:80)∞t=0 γtRt, is a
random variable representing the sum of discounted rewards
observed along one trajectory of states while following π.
Standard RL algorithms estimate the expected value of Z π,
the value function,

[0, 1) the discount factor. A policy π(

to a distribution over

.
A

∈ X

·|

V π(x) := E [Z π(x)] = E

γtR(xt, at)

x0 = x

.

(cid:34)

∞(cid:88)

t=0

(cid:35)

|

(1)
Similarly, many RL algorithms estimate the action-value
function,

Qπ(x, a) := E [Z π(x, a)] = E

(cid:35)
γtR(xt, at)

,

(2)

(cid:34)

∞(cid:88)

t=0

−

−

·|

xt

π(

P (

1, at

1), at ∼

xt ∼
·|
The (cid:15)-greedy policy on Qπ chooses actions uniformly
at random with probability (cid:15) and otherwise according to
arg maxa Qπ(x, a).

xt), x0 = x, a0 = a.

In distributional RL the distribution over returns (i.e. the
probability law of Z π), plays the central role and replaces
the value function. We will refer to the value distribution
by its random variable. When we say that the value func-
tion is the mean of the value distribution we are saying
that the value function is the expected value, taken over
all sources of intrinsic randomness (Goldstein, Misra, and
Courtage 1981), of the value distribution. This should high-
light that the value distribution is not designed to capture
the uncertainty in the estimate of the value function (Dear-
den, Friedman, and Russell 1998; Engel, Mannor, and Meir
2005), that is the parametric uncertainty, but rather the ran-
domness in the returns intrinsic to the MDP.

Temporal difference (TD) methods signiﬁcantly speed up
the learning process by incrementally improving an estimate
of Qπ using dynamic programming through the Bellman op-
erator (Bellman 1957),

Figure 1: Projection used by C51 assigns mass inversely
proportional to distance from nearest support. Update mini-
mizes KL between projected target and estimate.

T

·|

·|

∼

∼

x(cid:48)

Similarly, the value distribution can be computed through
dynamic programming using a distributional Bellman oper-
ator (Bellemare, Dabney, and Munos 2017),
πZ(x, a) :D= R(x, a) + γZ(x(cid:48), a(cid:48)),
x(cid:48)),

x, a), a(cid:48)
where Y :D= U denotes equality of probability laws, that is
the random variable Y is distributed according to the same
law as U .

P (

(3)

π(

The C51 algorithm models Z π(x, a) using a discrete dis-
tribution supported on a “comb” of ﬁxed locations z1 ≤
zN uniformly spaced over a predetermined interval.
· · · ≤
The parameters of that distribution are the probabilities qi,
expressed as logits, associated with each location zi. Given
a current value distribution, the C51 algorithm applies a pro-
πZ onto its ﬁnite ele-
jection step Φ to map the target
ment support, followed by a Kullback-Leibler (KL) mini-
mization step (see Figure 1). C51 achieved state-of-the-art
performance on Atari 2600 games, but did so with a clear
disconnect with the theoretical results of Bellemare, Dab-
ney, and Munos (2017). We now review these results before
extending them to the case of approximate distributions.

T

[1,

The Wasserstein Metric
The p-Wasserstein metric Wp, for p
], also known
as the Mallows metric (Bickel and Freedman 1981) or the
Earth Mover’s Distance (EMD) when p = 1 (Levina and
Bickel 2001), is an integral probability metric between dis-
tributions. The p-Wasserstein distance is characterized as the
Lp metric on inverse cumulative distribution functions (in-
verse CDFs) (M¨uller 1997). That is, the p-Wasserstein met-
ric between distributions U and Y is given by,1

∞

∈

(cid:18)(cid:90) 1

Wp(U, Y ) =

F −

1
Y (ω)

F −

1
U (ω)

pdω
|

−

0 |

(cid:19)1/p

(4)

,

1

where for a random variable Y , the inverse CDF F −
Y
is deﬁned by

of Y

1

y

F −

Y (ω) := inf

FY (y)
}
where FY (y) = P r(Y
y) is the CDF of Y . Figure 2 il-
lustrates the 1-Wasserstein distance as the area between two
CDFs.

≤

≤

∈

{

(5)

,

R : ω

πQ(x, a) = E [R(x, a)] + γEP,π [Q(x(cid:48), a(cid:48))] .

1For p = ∞, W∞(Y, U ) = supω∈[0,1] |F −1

Y (ω) − F −1

U (ω)|.

T

Recently, the Wasserstein metric has been the focus of in-
creased research due to its appealing properties of respect-
ing the underlying metric distances between outcomes (Ar-
jovsky, Chintala, and Bottou 2017; Bellemare et al. 2017).
Unlike the Kullback-Leibler divergence, the Wasserstein
metric is a true probability metric and considers both the
probability of and the distance between various outcome
events. These properties make the Wasserstein well-suited to
domains where an underlying similarity in outcome is more
important than exactly matching likelihoods.

Convergence of Distributional Bellman Operator
In the context of distributional RL, let
action-value distributions with ﬁnite moments:

be the space of

Z

=

Z :
E [
|

{

Z

X × A →
p] <
Z(x, a)
1
.
|
}
, we will
Then, for two action-value distributions Z1, Z2 ∈ Z
use the maximal form of the Wasserstein metric introduced
by (Bellemare, Dabney, and Munos 2017),

(x, a), p

∞

≥

P(R)
|
,
∀

¯dp(Z1, Z2) := sup
x,a

Wp(Z1(x, a), Z2(x, a)).

(6)

,

T

T

T

¯dp(

πZ1,

πZ2)

It was shown that ¯dp is a metric over value distributions. Fur-
π is a con-
thermore, the distributional Bellman operator
traction in ¯dp, a result that we now recall.
Lemma 1 (Lemma 3, Bellemare, Dabney, and Munos 2017).
π is a γ-contraction: for any two Z1, Z2 ∈ Z
γ ¯dp(Z1, Z2).
T

≤
Lemma 1 tells us that ¯dp is a useful metric for studying
the behaviour of distributional reinforcement learning algo-
rithms, in particular to show their convergence to the ﬁxed
point Z π. Moreover, the lemma suggests that an effective
way in practice to learn value distributions is to attempt to
minimize the Wasserstein distance between a distribution Z
πZ, analogous to the way that TD-
and its Bellman update
learning attempts to iteratively minimize the L2 distance be-
tween Q and

Q.

T

(cid:80)m

Unfortunately, another result shows that we cannot in gen-
eral minimize the Wasserstein metric (viewed as a loss) us-
ing stochastic gradient descent.
Theorem 1 (Theorem 1, Bellemare et al. 2017). Let ˆYm :=
1
i=1 δYi be the empirical distribution derived from sam-
m
ples Y1, . . . , Ym drawn from a Bernoulli distribution B. Let
Bµ be a Bernoulli distribution parametrized by µ, the prob-
ability of the variable taking the value 1. Then the minimum
of the expected sample loss is in general different from the
minimum of the true Wasserstein loss; that is,
(cid:2)Wp( ˆYm, Bµ)(cid:3)

Wp(B, Bµ).

= arg min

T

arg min
µ

E
Y1:m

µ

This issue becomes salient in a practical context, where
the value distribution must be approximated. Crucially,
the C51 algorithm is not guaranteed to minimize any p-
Wasserstein metric. This gap between theory and practice
in distributional RL is not restricted to C51. Morimura et

Figure 2: 1-Wasserstein minimizing projection onto N = 4
uniformly weighted Diracs. Shaded regions sum to form the
1-Wasserstein error.

al. (2010) parameterize a value distribution with the mean
and scale of a Gaussian or Laplace distribution, and min-
πZ and the
imize the KL divergence between the target
prediction Z. They demonstrate that value distributions
learned in this way are sufﬁcient to perform risk-sensitive Q-
Learning. However, any theoretical guarantees derived from
their method can only be asymptotic; the Bellman operator
is at best a non-expansion in KL divergence.

T

Approximately Minimizing Wasserstein

Recall
that C51 approximates the distribution at each
state by attaching variable (parametrized) probabilities
zN . Our approach
q1, . . . , qN to ﬁxed locations z1 ≤ · · · ≤
is to “transpose” this parametrization by considering ﬁxed
probabilities but variable locations. Speciﬁcally, we take
uniform weights, so that qi = 1/N for each i = 1, . . . , N .

Effectively, our new approximation aims to estimate
quantiles of the target distribution. Accordingly, we will call
it a quantile distribution, and let
ZQ be the space of quan-
tile distributions for ﬁxed N . We will denote the cumulative
probabilities associated with such a distribution (that is, the
discrete values taken on by the CDF) by τ1, . . . , τN , so that
τi = i
N for i = 1, . . . , N . We will also write τ0 = 0 to
simplify notation.
Formally, let θ :

RN be some parametric model.
A quantile distribution Zθ ∈ ZQ maps each state-action
pair (x, a) to a uniform probability distribution supported
on

X ×A →

. That is,
θi(x, a)
}

{

Zθ(x, a) := 1
N

δθi(x,a),

(7)

where δz denotes a Dirac at z

∈
Compared to the original parametrization, the beneﬁts of
a parameterized quantile distribution are threefold. First, (1)
we are not restricted to prespeciﬁed bounds on the support,
or a uniform resolution, potentially leading to signiﬁcantly
more accurate predictions when the range of returns vary

N
(cid:88)

i=1
R.

greatly across states. This also (2) lets us do away with the
unwieldy projection step present in C51, as there are no is-
sues of disjoint supports. Together, these obviate the need
for domain knowledge about the bounds of the return dis-
tribution when applying the algorithm to new tasks. Finally,
(3) this reparametrization allows us to minimize the Wasser-
stein loss, without suffering from biased gradients, speciﬁ-
cally, using quantile regression.

The Quantile Approximation
It is well-known that in reinforcement learning, the use of
function approximation may result in instabilities in the
learning process (Tsitsiklis and Van Roy 1997). Speciﬁcally,
the Bellman update projected onto the approximation space
may no longer be a contraction. In our case, we analyze the
distributional Bellman update, projected onto a parameter-
ized quantile distribution, and prove that the combined op-
erator is a contraction.

Quantile Projection We are interested in quantifying the
ZQ,
projection of an arbitrary value distribution Z
that is

onto

∈ Z

ΠW1 Z := arg min
Zθ∈ZQ

W1(Z, Zθ),

Let Y be a distribution with bounded ﬁrst moment and U
a uniform distribution over N Diracs as in (7), with support
θ1, . . . , θN }
{

. Then

W1(Y, U ) =

N
(cid:88)

(cid:90) τi

τi−1 |

i=1

F −

1
Y (ω)

dω.

θi|

−

Lemma 2. For any τ, τ (cid:48)
distribution function F with inverse F −
minimizing

[0, 1] with τ < τ (cid:48) and cumulative
R

1, the set of θ

∈

∈

is given by

(cid:90) τ (cid:48)

τ

1(ω)

F −
|

θ

dω ,
|

−

(cid:26)

θ

R

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∈

F (θ) =

(cid:18) τ + τ (cid:48)
2

(cid:19)(cid:27)

.

1 is the inverse CDF, then F −

In particular, if F −
τ (cid:48))/2) is always a valid minimizer, and if F −
at (τ + τ (cid:48))/2, then F −

1((τ +
1 is continuous
1((τ + τ (cid:48))/2) is the unique minimizer.
These quantile midpoints will be denoted by ˆτi = τi−1+τi
N . Therefore, by Lemma 2, the values
for 1
≤
that minimize W1(Y, U ) are given by
for
{
Y (ˆτi). Figure 2 shows an example of the quantile
θi = F −
projection ΠW1Z minimizing the 1-Wasserstein distance to
Z.2

i
≤
θ1, θ1, . . . , θN }

1

2

Quantile Regression
The original proof of Theorem 1 only states the existence
of a distribution whose gradients are biased. As a result, we
might hope that our quantile parametrization leads to unbi-
ased gradients. Unfortunately, this is not true.

Proposition 1. Let Zθ be a quantile distribution, and ˆZm
the empirical distribution composed of m samples from Z.
1, there exists a Z such that
Then for all p

≥

arg min E[Wp( ˆZm, Zθ)]

= arg min Wp(Z, Zθ).

However, there is a method, more widely used in eco-
nomics than machine learning, for unbiased stochastic ap-
proximation of the quantile function. Quantile regression,
and conditional quantile regression, are methods for ap-
proximating the quantile functions of distributions and con-
ditional distributions respectively (Koenker 2005). These
methods have been used in a variety of settings where
outcomes have intrinsic randomness (Koenker and Hallock
2001); from food expenditure as a function of household in-
come (Engel 1857), to studying value-at-risk in economic
models (Taylor 1999).

The quantile regression loss, for quantile τ

[0, 1], is
an asymmetric convex loss function that penalizes overesti-
mation errors with weight τ and underestimation errors with
weight 1
τ . For a distribution Z, and a given quantile τ , the
Z (τ ) may be characterized
value of the quantile function F −
as the minimizer of the quantile regression loss

−

∈

1

τ
L

QR(θ) := E ˆZ
∼
ρτ (u) = u(τ

Z[ρτ ( ˆZ
δ

−
u<0

θ)] , where
R.
u
),

(8)

−
More generally, by Lemma 2 we have that the minimizing
values of
for W1(Z, Zθ) are those that mini-
mize the following objective:

θ1, . . . , θN }
{

∈

∀

{

}

N
(cid:88)

i=1

E ˆZ

Z[ρˆτi( ˆZ

∼

θi)]

−

In particular, this loss gives unbiased sample gradients.
by

As a result, we can ﬁnd the minimizing
stochastic gradient descent.

θ1, . . . , θN }
{

→

Quantile Huber Loss The quantile regression loss is not
0+, the gradient of Equation 8 stays
smooth at zero; as u
constant. We hypothesized that this could limit performance
when using non-linear function approximation. To this end,
we also consider a modiﬁed quantile loss, called the quantile
Huber loss.3 This quantile regression loss acts as an asym-
κ, κ] around zero and
metric squared loss in an interval [
reverts to a standard quantile loss outside this interval.

−

The Huber loss is given by (Huber 1964),

Lκ(u) =

(cid:26) 1

2 u2,
u
κ(
|

1
2 κ),

| −

u

if
κ
| ≤
otherwise

|

.

(9)

The quantile Huber loss is then simply the asymmetric vari-
ant of the Huber loss,

ρκ
τ (u) =

}|Lκ(u).
For notational simplicity we will denote ρ0
will revert to the standard quantile regression loss.

τ
|

u<0

−

δ

{

τ = ρτ , that is, it

(10)

3Our quantile Huber loss is related to, but distinct from that of

2We save proofs for the appendix due to space limitations.

Aravkin et al. (2014).

∞

Combining Projection and Bellman Update
We are now in a position to prove our main result, which
states that the combination of the projection implied by
quantile regression with the Bellman operator is a contrac-
tion. The result is in
-Wasserstein metric, i.e. the size of
the largest gap between the two CDFs.
Proposition 2. Let ΠW1 be the quantile projection deﬁned
as above, and when applied to value distributions gives the
projection for each state-value distribution. For any two
for an MDP with countable
value distributions Z1, Z2 ∈ Z
state and action spaces,
πZ2)
(11)
(Z1, Z2).
(ΠW1 T
≤
the combined operator
We therefore conclude that
π has a unique ﬁxed point ˆZ π, and the repeated appli-
ΠW1T
cation of this operator, or its stochastic approximation, con-
¯d
verges to ˆZ π. Because ¯dp ≤
, we conclude that conver-
∞
]. Interestingly, the contraction
gence occurs for all p
[1,
∞
property does not directly hold for p <
; see Lemma 5 in
the appendix.

πZ1, ΠW1 T

¯d
∞

γ ¯d

∞

∈

∞

Distributional RL using Quantile Regression
We can now form a complete algorithmic approach to distri-
butional RL consistent with our theoretical results. That is,
approximating the value distribution with a parameterized
quantile distribution over the set of quantile midpoints, de-
ﬁned by Lemma 2. Then, training the location parameters
using quantile regression (Equation 8).

Quantile Regression Temporal Difference Learning
Recall the standard TD update for evaluating a policy π,

∼

V (x)
a

V (x) + α(r + γV (x(cid:48))
x), r

V (x)),
x, a).

−
P (

←
π(

·|

∼

∼

R(x, a), x(cid:48)
TD allows us to update the estimated value function with
a single unbiased sample following π. Quantile regression
also allows us to improve the estimate of the quantile func-
tion for some target distribution, Y (x), by observing sam-
ples y

Y (x) and minimizing Equation 8.

∼

·|

∈

θi(x)
π(

Furthermore, we have shown that by estimating the quan-
tile function for well-chosen values of τ
(0, 1) we can ob-
tain an approximation with minimal 1-Wasserstein distance
from the original (Lemma 2). Finally, we can combine this
with the distributional Bellman operator to give a target dis-
tribution for quantile regression. This gives us the quantile
regression temporal difference learning (QRTD) algorithm,
summarized simply by the update,
δ
θi(x) + α(ˆτi −
R(x, a), x(cid:48)
∼
∼

r+γz(cid:48)<θi(x))
}
x, a), z(cid:48)

where Zθ is a quantile distribution as in (7), and θi(x) is the
estimated value of F −
Zπ(x)(ˆτi) in state x. It is important to
note that this update is for each value of ˆτi and is deﬁned
for a single sample from the next state value distribution.
In general it is better to draw many samples of z(cid:48)
Z(x(cid:48))
and minimize the expected update. A natural approach in
this case, which we use in practice, is to compute the update
for all pairs of (θi(x), θj(x(cid:48))). Next, we turn to a control
algorithm and the use of non-linear function approximation.

),
Zθ(x(cid:48)),

←
x), r

{
P (

(12)

∼

∼

∼

·|

·|

a

1

Quantile Regression DQN
Q-Learning is an off-policy reinforcement learning algo-
rithm built around directly learning the optimal action-value
function using the Bellman optimality operator (Watkins and
Dayan 1992),

Q(x, a) = E [R(x, a)] + γ E
∼

x(cid:48)

P

T

(cid:104)

max
a(cid:48)

(cid:105)
Q(x(cid:48), a(cid:48))

.

The distributional variant of this is to estimate a state-
action value distribution and apply a distributional Bellman
optimality operator,

T
P (

Z(x, a) = R(x, a) + γZ(x(cid:48), a(cid:48)),
x, a), a(cid:48) = arg maxa(cid:48) E
·|

∼

z

Z(x(cid:48),a(cid:48)) [z] .

x(cid:48)

∼

(13)

Notice in particular that the action used for the next state is
the greedy action with respect to the mean of the next state-
action value distribution.

For a concrete algorithm we will build on the DQN archi-
tecture (Mnih et al. 2015). We focus on the minimal changes
necessary to form a distributional version of DQN. Speciﬁ-
cally, we require three modiﬁcations to DQN. First, we use a
nearly identical neural network architecture as DQN, only
changing the output layer to be of size
N , where
N is a hyper-parameter giving the number of quantile tar-
gets. Second, we replace the Huber loss used by DQN4,
Q(xt, at)) with κ = 1,
Lκ(rt + γ maxa(cid:48) Q(xt+1, a(cid:48))
with a quantile Huber loss (full loss given by Algorithm 1).
Finally, we replace RMSProp (Tieleman and Hinton 2012)
with Adam (Kingma and Ba 2015). We call this new algo-
rithm quantile regression DQN (QR-DQN).

|A| ×

−

Algorithm 1 Quantile Regression Q-Learning

Require: N, κ
input x, a, r, x(cid:48), γ

[0, 1)

∈
j qjθj(x(cid:48), a(cid:48))

# Compute distributional Bellman target
Q(x(cid:48), a(cid:48)) := (cid:80)
a∗
←
θj ←
T
# Compute quantile regression loss (Equation 10)
output (cid:80)N
θj −
i=1

arg maxa(cid:48) Q(x, a(cid:48))
r + γθj(x(cid:48), a∗),

j
∀
θi(x, a))(cid:3)

(cid:2)ρκ
ˆτi

(
T

Ej

Unlike C51, QR-DQN does not require projection onto the
approximating distribution’s support, instead it is able to ex-
pand or contract the values arbitrarily to cover the true range
of return values. As an additional advantage, this means that
QR-DQN does not require the additional hyper-parameter
giving the bounds of the support required by C51. The only
additional hyper-parameter of QR-DQN not shared by DQN is
the number of quantiles N , which controls with what reso-
lution we approximate the value distribution. As we increase
N , QR-DQN goes from DQN to increasingly able to estimate
the upper and lower quantiles of the value distribution. It
becomes increasingly capable of distinguishing low proba-
bility events at either end of the cumulative distribution over
returns.

4 DQN uses gradient clipping of the squared error that makes it

equivalent to a Huber loss with κ = 1.

Figure 3: (a) Two-room windy gridworld, with wind magnitude shown along bottom row. Policy trajectory shown by blue path,
with additional cycles caused by randomness shown by dashed line. (b, c) (Cumulative) Value distribution at start state xS,
estimated by MC, Z π

M C, and by QRTD, Zθ. (d, e) Value function (distribution) approximation errors for TD(0) and QRTD.

Experimental Results

In the introduction we claimed that learning the distribution
over returns had distinct advantages over learning the value
function alone. We have now given theoretically justiﬁed al-
gorithms for performing distributional reinforcement learn-
ing, QRTD for policy evaluation and QR-DQN for control. In
this section we will empirically validate that the proposed
distributional reinforcement learning algorithms: (1) learn
the true distribution over returns, (2) show increased robust-
ness during training, and (3) signiﬁcantly improve sample
complexity and ﬁnal performance over baseline algorithms.

Value Distribution Approximation Error We begin our
experimental results by demonstrating that QRTD actually
learns an approximate value distribution that minimizes the
1-Wasserstein to the ground truth distribution over returns.
Although our theoretical results already establish conver-
gence of the former to the latter, the empirical performance
helps to round out our understanding.

We use a variant of the classic windy gridworld domain
(Sutton and Barto 1998), modiﬁed to have two rooms and
randomness in the transitions. Figure 3(a) shows our ver-
sion of the domain, where we have combined the transition
stochasticity, wind, and the doorway to produce a multi-
modal distribution over returns when anywhere in the ﬁrst
room. Each state transition has probability 0.1 of moving in
a random direction, otherwise the transition is affected by
wind moving the agent northward. The reward function is
zero until reaching the goal state xG, which terminates the
episode and gives a reward of 1.0. The discount factor is
γ = 0.99.

We compute the ground truth value distribution for opti-
mal policy π, learned by policy iteration, at each state by per-
forming 1K Monte-Carlo (MC) rollouts and recording the
observed returns as an empirical distribution, shown in Fig-
ure 3(b). Next, we ran both TD(0) and QRTD while following
π for 10K episodes. Each episode begins in the designated
start state (xS). Both algorithms started with a learning rate
of α = 0.1. For QRTD we used N = 32 and drop α by half
every 2K episodes.

Let Z π

turns from the start state xS, similarly V π

M C(xS) be the MC estimated distribution over re-
M C(xS) its mean.

M C −

In Figure 3 we show the approximation errors at xS for both
algorithms with respect to the number of episodes. In (d)
we evaluated, for both TD(0) and QRTD, the squared error,
V (xS))2, and in (e) we show the 1-Wasserstein
(V π
metric for QRTD, W1(Z π
M C(xS), Z(xS)), where V (xS) and
Z(xS) are the expected returns and value distribution at
state xS estimated by the algorithm. As expected both al-
gorithms converge correctly in mean, and QRTD minimizes
the 1-Wasserstein distance to Z π

M C.

Evaluation on Atari 2600

We now provide experimental results that demonstrate the
practical advantages of minimizing the Wasserstein metric
end-to-end, in contrast to the C51 approach. We use the 57
Atari 2600 games from the Arcade Learning Environment
(ALE) (Bellemare et al. 2013). Both C51 and QR-DQN build
on the standard DQN architecture, and we expect both to
beneﬁt from recent improvements to DQN such as the du-
eling architectures (Wang et al. 2016) and prioritized replay
(Schaul et al. 2016). However, in our evaluations we com-
pare the pure versions of C51 and QR-DQN without these
additions. We present results for both a strict quantile loss,
κ = 0 (QR-DQN-0), and with a Huber quantile loss with
κ = 1 (QR-DQN-1).

We performed hyper-parameter tuning over a set of ﬁve
training games and evaluated on the full set of 57 games
using these best settings (α = 0.00005, (cid:15)ADAM = 0.01/32,
and N = 200).5 As with DQN we use a target network when
computing the distributional Bellman update. We also allow
(cid:15) to decay at the same rate as in DQN, but to a lower value
of 0.01, as is common in recent work (Bellemare, Dabney,
and Munos 2017; Wang et al. 2016; van Hasselt, Guez, and
Silver 2016).

Out

training procedure follows that of Mnih et al.
(2015)’s, and we present results under two evaluation pro-
tocols: best agent performance and online performance. In
both evaluation protocols we consider performance over all
57 Atari 2600 games, and transform raw scores into human-
normalized scores (van Hasselt, Guez, and Silver 2016).

5We swept over α in (10−3, 5 × 10−4, 10−4, 5 × 10−5, 10−5);
(cid:15)ADAM in (0.01/32, 0.005/32, 0.001/32); N (10, 50, 100, 200)

Figure 4: Online evaluation results, in human-normalized scores, over 57 Atari 2600 games for 200 million training samples.
(Left) Testing performance for one seed, showing median over games. (Right) Training performance, averaged over three seeds,
showing percentiles (10, 20, 30, 40, and 50) over games.

DQN
DDQN
DUEL.
PRIOR.
PR. DUEL.
C51
QR-DQN-0
QR-DQN-1

Mean Median >human >DQN
0
228%
43
307%
50
373%
48
434%
44
592%
50
701%
52
881%
54
915%

79%
118%
151%
124%
172%
178%
199%
211%

24
33
37
39
39
40
38
41

Table 1: Mean and median of best scores across 57 Atari
2600 games, measured as percentages of human baseline
(Nair et al. 2015).

Best agent performance To provide comparable results
with existing work we report test evaluation results un-
der the best agent protocol. Every one million training
frames, learning is frozen and the agent is evaluated for
500K frames while recording the average return. Evalua-
tion episodes begin with up to 30 random no-ops (Mnih
et al. 2015), and the agent uses a lower exploration rate
((cid:15) = 0.001). As training progresses we keep track of the
best agent performance achieved thus far.

Table 1 gives the best agent performance, at 200 million
frames trained, for QR-DQN, C51, DQN, Double DQN (van
Hasselt, Guez, and Silver 2016), Prioritized replay (Schaul
et al. 2016), and Dueling architecture (Wang et al. 2016). We
see that QR-DQN outperforms all previous agents in mean
and median human-normalized score.

Online performance
In this evaluation protocol (Fig-
ure 4) we track the average return attained during each test-
ing (left) and training (right) iteration. For the testing perfor-
mance we use a single seed for each algorithm, but show on-
line performance with no form of early stopping. For train-
ing performance, values are averages over three seeds. In-
stead of reporting only median performance, we look at the
distribution of human-normalized scores over the full set of
games. Each bar represents the score distribution at a ﬁxed

percentile (10th, 20th, 30th, 40th, and 50th). The upper per-
centiles show a similar trend but are omitted here for visual
clarity, as their scale dwarfs the more informative lower half.
From this, we can infer a few interesting results. (1) Early
in learning, most algorithms perform worse than random for
at least 10% of games. (2) QRTD gives similar improvements
to sample complexity as prioritized replay, while also im-
proving ﬁnal performance. (3) Even at 200 million frames,
there are 10% of games where all algorithms reach less than
10% of human. This ﬁnal point in particular shows us that
all of our recent advances continue to be severely limited on
a small subset of the Atari 2600 games.

Conclusions
The importance of the distribution over returns in reinforce-
ment learning has been (re)discovered and highlighted many
times by now. In Bellemare, Dabney, and Munos (2017) the
idea was taken a step further, and argued to be a central part
of approximate reinforcement learning. However, the paper
left open the question of whether there exists an algorithm
which could bridge the gap between Wasserstein-metric the-
ory and practical concerns.

In this paper we have closed this gap with both theoreti-
cal contributions and a new algorithm which achieves state-
of-the-art performance in Atari 2600. There remain many
promising directions for future work. Most exciting will be
to expand on the promise of a richer policy class, made pos-
sible by action-value distributions. We have mentioned a few
examples of such policies, often used for risk-sensitive deci-
sion making. However, there are many more possible deci-
sion policies that consider the action-value distributions as a
whole.

Additionally, QR-DQN is likely to beneﬁt from the im-
provements on DQN made in recent years. For instance, due
to the similarity in loss functions and Bellman operators
we might expect that QR-DQN suffers from similar over-
estimation biases to those that Double DQN was designed
to address (van Hasselt, Guez, and Silver 2016). A natu-
ral next step would be to combine QR-DQN with the non-
distributional methods found in Table 1.

Acknowledgements

The authors acknowledge the vital contributions of their col-
leagues at DeepMind. Special thanks to Tom Schaul, Au-
drunas Gruslys, Charles Blundell, and Benigno Uria for their
early suggestions and discussions on the topic of quantile
regression. Additionally, we are grateful for feedback from
David Silver, Yee Whye Teh, Georg Ostrovski, Joseph Mo-
dayil, Matt Hoffman, Hado van Hasselt, Ian Osband, Mo-
hammad Azar, Tom Stepleton, Olivier Pietquin, Bilal Piot;
and a second acknowledgement in particular of Tom Schaul
for his detailed review of an previous draft.

References

Aravkin, A. Y.; Kambadur, A.; Lozano, A. C.; and Luss, R.
2014. Sparse Quantile Huber Regression for Efﬁcient and
Robust Estimation. arXiv.
Arjovsky, M.; Chintala, S.; and Bottou, L. 2017. Wasser-
stein Generative Adversarial Networks. In Proceedings of
the 34th International Conference on Machine Learning
(ICML).
Bellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M.
2013. The Arcade Learning Environment: An Evaluation
Platform for General Agents. Journal of Artiﬁcial Intelli-
gence Research 47:253–279.
Bellemare, M. G.; Danihelka, I.; Dabney, W.; Mohamed, S.;
Lakshminarayanan, B.; Hoyer, S.; and Munos, R. 2017. The
Cramer Distance as a Solution to Biased Wasserstein Gradi-
ents. arXiv.
Bellemare, M. G.; Dabney, W.; and Munos, R. 2017. A
Distributional Perspective on Reinforcement Learning. Pro-
ceedings of the 34th International Conference on Machine
Learning (ICML).
Bellman, R. E. 1957. Dynamic Programming. Princeton,
NJ: Princeton University Press.
Bickel, P. J., and Freedman, D. A. 1981. Some Asymptotic
Theory for the Bootstrap. The Annals of Statistics 1196–
1217.
Chow, Y.; Tamar, A.; Mannor, S.; and Pavone, M. 2015.
Risk-Sensitive and Robust Decision-Making: a CVaR Op-
In Advances in Neural Information
timization Approach.
Processing Systems (NIPS), 1522–1530.
Dearden, R.; Friedman, N.; and Russell, S. 1998. Bayesian
Q-learning. In Proceedings of the National Conference on
Artiﬁcial Intelligence.
Engel, Y.; Mannor, S.; and Meir, R. 2005. Reinforcement
In Proceedings of the
Learning with Gaussian Processes.
International Conference on Machine Learning (ICML).
Die Productions-und Consum-
Engel, E.
tionsverh¨altnisse des K¨onigreichs Sachsen. Zeitschrift des
Statistischen Bureaus des K¨oniglich S¨achsischen Ministeri-
ums des Innern 8:1–54.
Goldstein, S.; Misra, B.; and Courtage, M. 1981. On Intrin-
sic Randomness of Dynamical Systems. Journal of Statisti-
cal Physics 25(1):111–126.

1857.

Heger, M. 1994. Consideration of Risk in Reinforcement
Learning. In Proceedings of the 11th International Confer-
ence on Machine Learning, 105–111.
Huber, P. J. 1964. Robust Estimation of a Location Param-
eter. The Annals of Mathematical Statistics 35(1):73–101.
Kingma, D., and Ba, J. 2015. Adam: A Method for Stochas-
tic Optimization. Proceedings of the International Confer-
ence on Learning Representations.
Koenker, R., and Hallock, K. 2001. Quantile Regression: An
Introduction. Journal of Economic Perspectives 15(4):43–
56.
Koenker, R. 2005. Quantile Regression. Cambridge Univer-
sity Press.
Levina, E., and Bickel, P. 2001. The Earth Mover’s Distance
is the Mallows Distance: Some Insights from Statistics. In
The 8th IEEE International Conference on Computer Vision
(ICCV). IEEE.
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Ve-
ness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fid-
jeland, A. K.; Ostrovski, G.; et al. 2015. Human-level
Control through Deep Reinforcement Learning. Nature
518(7540):529–533.
Morimura, T.; Hachiya, H.; Sugiyama, M.; Tanaka, T.; and
Kashima, H. 2010. Parametric Return Density Estimation
for Reinforcement Learning. In Proceedings of the Confer-
ence on Uncertainty in Artiﬁcial Intelligence (UAI).
M¨uller, A. 1997. Integral Probability Metrics and their Gen-
erating Classes of Functions. Advances in Applied Proba-
bility 29(2):429–443.
Nair, A.; Srinivasan, P.; Blackwell, S.; Alcicek, C.; Fearon,
R.; De Maria, A.; Panneershelvam, V.; Suleyman, M.; Beat-
tie, C.; and Petersen, S. e. a. 2015. Massively Parallel Meth-
ods for Deep Reinforcement Learning. In ICML Workshop
on Deep Learning.
Puterman, M. L. 1994. Markov Decision Processes: Dis-
crete stochastic dynamic programming. John Wiley & Sons,
Inc.
Rummery, G. A., and Niranjan, M.
1994. On-line Q-
learning using Connectionist Systems. Technical report,
Cambridge University Engineering Department.
Schaul, T.; Quan, J.; Antonoglou, I.; and Silver, D. 2016.
Prioritized Experience Replay. In Proceedings of the Inter-
national Conference on Learning Representations (ICLR).
Sutton, R. S., and Barto, A. G. 1998. Reinforcement Learn-
ing: An Introduction. MIT Press.
Taylor, J. W. 1999. A Quantile Regression Approach to Esti-
mating the Distribution of Multiperiod Returns. The Journal
of Derivatives 7(1):64–78.
Tieleman, T., and Hinton, G. 2012. Lecture 6.5: Rmsprop.
COURSERA: Neural Networks for Machine Learning 4(2).
Tsitsiklis, J. N., and Van Roy, B. 1997. An Analysis of
Temporal-Difference Learning with Function Approxima-
tion. IEEE Transactions on Automatic Control 42(5):674–
690.

van Hasselt, H.; Guez, A.; and Silver, D. 2016. Deep Rein-
forcement Learning with Double Q-Learning. In Proceed-
ings of the AAAI Conference on Artiﬁcial Intelligence.
Wang, Z.; Schaul, T.; Hessel, M.; Hasselt, H. v.; Lanctot, M.;
and de Freitas, N. 2016. Dueling Network Architectures
In Proceedings of the
for Deep Reinforcement Learning.
International Conference on Machine Learning (ICML).
Watkins, C. J., and Dayan, P. 1992. Q-learning. Machine
Learning 8(3):279–292.

Appendix

Proofs
Lemma 2. For any τ, τ (cid:48)
distribution function F with inverse F −
minimizing

∈

[0, 1] with τ < τ (cid:48) and cumulative
R

1, the set of θ

∈

is given by

(cid:90) τ (cid:48)

τ

1(ω)

F −
|

θ

dω ,
|

−

(cid:26)

θ

R

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∈

F (θ) =

(cid:18) τ + τ (cid:48)
2

(cid:19)(cid:27)

.

In particular, if F −
τ (cid:48))/2) is always a valid minimizer, and if F −
at (τ + τ (cid:48))/2, then F −

1((τ +
1 is continuous
1((τ + τ (cid:48))/2) is the unique minimizer.

1 is the inverse CDF, then F −

Proof. For any ω
∈
is convex, and has subgradient given by

[0, 1], the function θ

(cid:55)→ |

F −

1(ω)

θ

|

−




θ

1
1, 1]
[
−
1
−
(cid:82) τ (cid:48)
so the function θ
τ
|
has subgradient given by

(cid:55)→

(cid:55)→



if θ < F −
if θ = F −
if θ > F −

1(ω)
1(ω)
1(ω) ,

F −

1(ω)

θ

dω is also convex, and
|

−

(cid:90) F (θ)

θ

(cid:55)→

τ

−

1dω +

1dω .

(cid:90) τ (cid:48)

F (θ)

Setting this subgradient equal to 0 yields

(τ + τ (cid:48))

2F (θ) = 0 ,

−

(14)

F −

1 is the identity map on [0, 1], it is clear
and since F
◦
1((τ + τ (cid:48))/2) satisﬁes Equation 14. Note that in
that θ = F −
fact any θ such that F (θ) = (τ + τ (cid:48))/2 yields a subgradient
1 is not
of 0, which leads to a multitude of minimizers if F −
continuous at (τ + τ (cid:48))/2.

Proposition 1. Let Zθ be a quantile distribution, and ˆZm
the empirical distribution composed of m samples from Z.
1, there exists a Z such that
Then for all p

≥

arg min E[Wp( ˆZm, Zθ)]

= arg min Wp(Z, Zθ).

Proof. Write Zθ = (cid:80)N
θN . We
take Z to be of the same form as Zθ. Speciﬁcally, consider
Z given by

N δθi, with θ1 ≤ · · · ≤

i=1

1

supported on the set
, and take m = N . Then
}
clearly the unique minimizing Zθ for Wp(Z, Zθ) is given by
taking Zθ = Z. However, consider the gradient with respect
to θ1 for the objective

{

1
N

δi ,

N
(cid:88)

Z =

i=1
1, . . . , N

E[Wp( ˆZN , Zθ)] .

We have

E[Wp( ˆZN , Zθ)]

|θ1=1 = E[

∇θ1 Wp( ˆZN , Zθ)

|θ1=1] .

∇θ1

In the event that the sample distribution ˆZN has an atom at
1, then the optimal transport plan pairs the atom of Zθ at
θ1 = 1 with this atom of ˆZN , and gradient with respect to
θ1 of Wp( ˆZN , Zθ) is 0. If the sample distribution ˆZN does
not contain an atom at 1, then the left-most atom of ˆZN is
greater than 1 (since Z is supported on
. In this
}
case, the gradient on θ1 is negative. Since this happens with
non-zero probability, we conclude that

1, . . . , N

{

E[Wp( ˆZN , Zθ)]

|θ1=1 < 0 ,

∇θ1

and therefore Zθ = Z cannot be the minimizer of
E[Wp( ˆZN , Zθ)].

Proposition 2. Let ΠW1 be the quantile projection deﬁned
as above, and when applied to value distributions gives the
projection for each state-value distribution. For any two
for an MDP with countable
value distributions Z1, Z2 ∈ Z
state and action spaces,

¯d
∞

(ΠW1 T

πZ1, ΠW1 T

πZ2)

γ ¯d

∞

≤

(Z1, Z2).

(11)

T

∞

≡

k=1

k=1

(cid:80)N

∈ X × A

Proof. We assume that instantaneous rewards given a state-
action pair are deterministic; the general case is a straight-
π is a
forward generalization. Further, since the operator
, it is sufﬁcient to prove the claim in the
γ-contraction in d
case γ = 1. In addition, since Wasserstein distances are in-
variant under translation of the support of distributions, it is
sufﬁcient to deal with the case where r(x, a)
0 for all
. The proof then proceeds by ﬁrst reducing
(x, a)
to the case where every value distribution consists only of
single Diracs, and then dealing with this reduced case using
Lemma 3.

We write Z(x, a) = (cid:80)N

1
N δθk(x,a) and Y (x, a) =
Rn.
1
N δψk(x,a), for some functions θ, ψ :
Let (x, a) be a state-action pair, and let ((xi, ai))i
I be all
the state-action pairs that are accessible from (x(cid:48), a(cid:48)) in a
single transition, where I is a (ﬁnite or countable) index-
ing set. Write pi for the probability of transitioning from
(x(cid:48), a(cid:48)) to (xi, ai), for each i
I. We now construct a new
∈
MDP and new value distributions for this MDP in which
all distributions are given by single Diracs, with a view
to applying Lemma 3. The new MDP is of the following
form. We take the state-action pair (x(cid:48), a(cid:48)), and deﬁne new
states, actions, transitions, and a policy (cid:101)π, so that the state-
action pairs accessible from (x(cid:48), a(cid:48)) in this new MDP are
given by (((cid:101)xj
I )N
j=1, and the probability of reaching
the state-action pair ((cid:101)xj
i , (cid:101)aj
i ) is pi/n. Further, we deﬁne new
value distributions (cid:101)Z, (cid:101)Y as follows. For each i
I and
j = 1, . . . , N , we set:

X × A →

i , (cid:101)aj
i )i

∈

∈

∈

i , (cid:101)aj
(cid:101)Z((cid:101)xj
i ) = δθj (xi,ai)
i , (cid:101)aj
(cid:101)Y ((cid:101)xj
i ) = δψj (xi,ai) .
The construction is illustrated in Figure 5.

Since, by Lemma 4, the d

distance between the 1-
Wasserstein projections of two real-valued distributions is
the max over the difference of a certain set of quantiles, we

∞

Figure 5: Initial MDP and value distribution Z (top), and
transformed MDP and value distribution (cid:101)Z (bottom).

may appeal to Lemma 3 to obtain the following:

(cid:101)π (cid:101)Z)(x(cid:48), a(cid:48)), ΠW1(

(cid:101)π (cid:101)Y )(x(cid:48), a(cid:48)))

d

∞

(ΠW1 (
T
θj(xi, ai)
sup
|
I
i=1

≤

−

T
ψj(xi, ai)

|

∈

j=1,...,N
d

= sup
I
i=1

∞

∈

(Z(xi, ai), Y (xi, ai))

(15)

spectively, (
(
T

T

Now note that by construction, (

(cid:101)π (cid:101)Z)(x(cid:48), a(cid:48)) (re-
(cid:101)π (cid:101)Y )(x(cid:48), a(cid:48))) has the same distribution as

T
πY )(x(cid:48), a(cid:48))), and so

πZ)(x(cid:48), a(cid:48)) (respectively, (
T
(cid:101)π (cid:101)Y )(x(cid:48), a(cid:48)))
(cid:101)π (cid:101)Z)(x(cid:48), a(cid:48)), ΠW1 (
T
T
πY )(x(cid:48), a(cid:48))) .
πZ)(x(cid:48), a(cid:48)), ΠW1(
T

(ΠW1(
∞
(ΠW1(

= d

∞

T

d

Therefore, substituting this into the Inequality 15, we obtain

(ΠW1 (
T
d

πZ)(x(cid:48), a(cid:48)), ΠW1(
T
(Z(xi, ai), Y (xi, ai)) .

d
∞
sup
I
i

∈

≤

∞

πY )(x(cid:48), a(cid:48)))

Taking suprema over the initial state (x(cid:48), a(cid:48)) then yields the
result.

Supporting results
Lemma 3. Consider an MDP with countable state and ac-
tion spaces. Let Z, Y be value distributions such that each
state-action distribution Z(x, a), Y (x, a) is given by a sin-
gle Dirac. Consider the particular case where rewards are
[0, 1]. Denote by Πτ
identically 0 and γ = 1, and let τ

∈

d

d

∞

≤

πY )

(Z, Y )

(Πτ T

the projection operator that maps a probability distribution
onto a Dirac delta located at its τ th quantile. Then
πZ, Πτ T

X × A →
∈

Proof. Let Z(x, a) = δθ(x,a) and Y (x, a) = δψ(x,a) for
each state-action pair (x, a)
, for some functions
R. Let (x(cid:48), a(cid:48)) be a state-action pair, and let
ψ, θ :
I be all the state-action pairs that are accessible
((xi, ai))i
from (x(cid:48), a(cid:48)) in a single transition, with I a (ﬁnite or count-
ably inﬁnite) indexing set. To lighten notation, we write θi
for θ(xi, ai) and ψi for ψ(xi, ai). Further, let the probability
I.
of transitioning from (x(cid:48), a(cid:48)) to (xi, ai) be pi, for all i

∈ X × A

∞

Then we have

πZ)(x(cid:48), a(cid:48)) =

(cid:88)

piδθi

πY )(x(cid:48), a(cid:48)) =

piδψi .

(

T

(
T

I

i
∈
(cid:88)

i

I

∈

∈

(16)

(17)

d

T

T

∞

∈

θu −

[0, 1] arbitrary. Let u

∈
πZ)(x(cid:48), a(cid:48)), and let v

Now consider the τ th quantile of each of these distributions,
I be such that θu is equal to
for τ
I such that ψv is
this quantile of (
equal to this quantile of (
(Πτ T
We now show that
θu −

∈
πY )(x(cid:48), a(cid:48)). Now note that
ψv|

πZ(x(cid:48), a(cid:48)), Πτ T

ψv|
∈
is impossible, from which it will follow that
πZ(x(cid:48), a(cid:48)), Πτ T

πY (x(cid:48), a(cid:48))) =

πY (x(cid:48), a(cid:48)))

and the result then follows by taking maxima over state-
action pairs (x(cid:48), a(cid:48)). To demonstrate the impossibility of
(18), without loss of generality we take θu ≤
set I. Deﬁne:

We now introduce the following partitions of the indexing

ψi| ∀

(Πτ T

(Z, Y ) ,

θi −

(18)

ψv.

≤

>

∞

∞

d

d

I

i

|

|

|

I
∈
I
∈
I
∈
I
∈
and observe that we clearly have the following disjoint
unions:

θi ≤
θu}
|
θi > θu}
|
ψi < ψv}
|
ψv}
ψi ≥
|

I
θu =
≤
I>θu =
I<ψv =
ψv =
I
≥

i
{
i
{
i
{
i
{

,
,
,
,

If (18) is to hold, then we must have I
.
I
θu ∩
∅
≤
≥
I<ψv . But if this is the
Therefore, we must have I
case, then since θu is the τ th quantile of (
πZ)(x(cid:48), a(cid:48)), we
T
must have

θu ⊆
≤

ψv =

and so consequently

I = I
θu ∪
I = I<ψv ∪

≤

I>θu ,
ψv .
I

≥

(cid:88)

I≤θu

i

∈

(cid:88)

I<ψv

i
∈

pi ≥

τ ,

pi ≥

τ ,

the τ th quantile of
πY )(x(cid:48), a(cid:48)) is less than ψv, a contradiction. Therefore

from which we conclude that
(
T
(18) cannot hold, completing the proof.

Lemma 4. For any two probability distributions ν1, ν2 over
the real numbers, and the Wasserstein projection operator
ΠW1 that projects distributions onto support of size n, we
have that
d

(ΠW1ν1, ΠW1ν2)

∞

= max

i=1,...,n

1

F −
ν1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18) 2i

(cid:19)

1

−
2n

1

F −
ν2

−

(cid:18) 2i

1

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

−
2n

Proof. By the discussion surrounding Lemma 2, we have
that ΠW1νk = (cid:80)n
2n ) for k = 1, 2. Therefore,
νk ( 2i−1
the optimal coupling between ΠW1ν1 and ΠW1ν2 must be
ν2 ( 2i
ν1 ( 2i
1
1
1
2n ) for each i = 1, . . . , n.
given by F −
F −
2n )
−
−
This immediately leads to the expression of the lemma.

1
n δF −1

(cid:55)→

i=1

1

Further theoretical results
Lemma 5. The projected Bellman operator ΠW1T
).
general not a non-expansion in dp, for p

[1,

π is in

∈

∞

Proof. Consider the case where the number of Dirac deltas
in each distribution, N , is equal to 2, and let γ = 1. We con-
sider an MDP with a single initial state, x, and two terminal
states, x1 and x2. We take the action space of the MDP to
be trivial, and therefore omit it in the notation that follows.
Let the MDP have a 2/3 probability of transitioning from
x to x1, and 1/3 probability of transitioning from x to x2.
We take all rewards in the MDP to be identically 0. Further,
consider two value distributions, Z and Y , given by:
1
2
1
2

1
1
2
2
1
1
2
2
Z(x) = δ0 , Y (x) = δ0 .

δ2 , Y (x1) =

δ5 , Y (x2) =

Z(x2) =

Z(x1) =

1
2
1
2

δ0 +

δ1 +

δ4 +

δ3 +

δ2 ,

δ5 ,

Then note that we have

dp(Z(x1), Y (x1)) =

dp(Z(x2), Y (x2)) =

0

−

=

(cid:19)1/p

|
(cid:19)1/p

(cid:18) 1
1
1
21/p ,
2 |
(cid:18) 1
1
4
21/p ,
2 |
dp(Z(x), Y (x)) = 0 ,

=

−

3

|

and so

dp(Z, Y ) =

1
21/p .

We now consider the projected backup for these two value
distributions at the state x. We ﬁrst compute the full backup:

πZ)(x) =

δ0 +

δ2 +

δ3 +

δ5 ,

1
3
1
3

1
3
1
3

1
6
1
6

1
6
1
6

πY )(x) =

δ1 +

δ2 +

δ4 +

δ5 .

(
T

(

T

Appealing to Lemma 2, we note that when projected these
distributions onto two equally-weighted Diracs, the loca-
tions of these Diracs correspond to the 25% and 75% quan-
tiles of the original distributions. We therefore have

(ΠW1 T
(ΠW1T

πZ)(x) =

δ0 +

δ3 ,

πY )(x) =

δ1 +

δ4 ,

1
2
1
2

1
2
1
2

and we therefore obtain

d1(ΠW1T

πZ, ΠW1 T

πY ) =

completing the proof.

(cid:19)1/p

(cid:18) 1
2

=1 >

p)

1

(
|

3
|

0

p +
|

4
|

−

−
1
21/p = d1(Z, Y ) ,

Notation
Human-normalized scores are given by (van Hasselt, Guez,
and Silver 2016),

score =

agent
human

random
random

,

−
−

where agent, human and random represent the per-game
raw scores for the agent, human baseline, and random agent
baseline.

Table 2: Notation used in the paper

Symbol

Description of usage

|

∈

X

x, a)

[0, 1)

, R, P , γ)

Reinforcement Learning
MDP (
,
A
State space of MDP
Action space of MDP
Reward function, random variable reward
Transition probabilities, P (x(cid:48)
Discount factor, γ
States
Actions
Rewards
Policy
(dist.) Bellman operator
(dist.) Bellman optimality operator
Value function, state-value function
Action-value function
Step-size parameter, learning rate
Exploration rate, (cid:15)-greedy
Adam parameter
Huber-loss parameter
Huber-loss with parameter κ

M
X
A
R, Rt
P
γ
x, xt ∈ X
a, a∗, b
∈ A
R
r, rt ∈
π

π

T
T
V π, V
Qπ, Q
α
(cid:15)
(cid:15)ADAM
κ
Lκ

π

Z

M C

Z π, Z
Z π

Distributional Reinforcement Learning
Random return, value distribution
Monte-Carlo value distribution under policy π
Space of value distributions
Fixed point of convergence for ΠW1T
Instantiated return sample
Metric order
p-Wasserstein metric
Metric order p
maximal form of Wasserstein
Projection used by C51
1-Wasserstein projection
Quantile regression loss
Huber quantile loss
Probabilities, parameterized probabilities

Z
ˆZ π
z
∼
p
Wp
Lp
¯dp
Φ
ΠW1
ρτ
ρκ
τ
q1, . . . , qN
τ0, τ1, . . . , τN Cumulative probabilities with τ0 := 0
ˆτ1, . . . , ˆτN
ω
δz
θ
B
Bµ
ZQ
Zθ
Y
Y1, . . . , Ym
ˆYm

Midpoint quantile targets
Sample from unit interval
R
Dirac function at z
∈
Parameterized function
Bernoulli distribution
Parameterized Bernoulli distribution
Space of quantile (value) distributions
Parameterized quantile (value) distribution
Random variable over R
Random variable samples
Empirical distribution from m-Diracs

Figure 6: Online training curves for DQN, C51, and QR-DQN on 57 Atari 2600 games. Curves are averages over three seeds,
smoothed over a sliding window of 5 iterations, and error bands give standard deviations.

GAMES
Alien
Amidar
Assault
Asterix
Asteroids
Atlantis
Bank Heist
Battle Zone
Beam Rider
Berzerk
Bowling
Boxing
Breakout
Centipede
Chopper Command
Crazy Climber
Defender
Demon Attack
Double Dunk
Enduro
Fishing Derby
Freeway
Frostbite
Gopher
Gravitar
H.E.R.O.
Ice Hockey
James Bond
Kangaroo
Krull
Kung-Fu Master
Montezuma’s Revenge
Ms. Pac-Man
Name This Game
Phoenix
Pitfall!
Pong
Private Eye
Q*Bert
River Raid
Road Runner
Robotank
Seaquest
Skiing
Solaris
Space Invaders
Star Gunner
Surround
Tennis
Time Pilot
Tutankham
Up and Down
Venture
Video Pinball
Wizard Of Wor
Yars’ Revenge
Zaxxon

RANDOM
227.8
5.8
222.4
210.0
719.1
12,850.0
14.2
2,360.0
363.9
123.7
23.1
0.1
1.7
2,090.9
811.0
10,780.5
2,874.5
152.1
-18.6
0.0
-91.7
0.0
65.2
257.6
173.0
1,027.0
-11.2
29.0
52.0
1,598.0
258.5
0.0
307.3
2,292.3
761.4
-229.4
-20.7
24.9
163.9
1,338.5
11.5
2.2
68.4
-17,098.1
1,236.3
148.0
664.0
-10.0
-23.8
3,568.0
11.4
533.4
0.0
16,256.9
563.5
3,092.9
32.5

HUMAN
7,127.7
1,719.5
742.0
8,503.3
47,388.7
29,028.1
753.1
37,187.5
16,926.5
2,630.4
160.7
12.1
30.5
12,017.0
7,387.8
35,829.4
18,688.9
1,971.0
-16.4
860.5
-38.7
29.6
4,334.7
2,412.5
3,351.4
30,826.4
0.9
302.8
3,035.0
2,665.5
22,736.3
4,753.3
6,951.6
8,049.0
7,242.6
6,463.7
14.6
69,571.3
13,455.0
17,118.0
7,845.0
11.9
42,054.7
-4,336.9
12,326.7
1,668.7
10,250.0
6.5
-8.3
5,229.2
167.6
11,693.2
1,187.5
17,667.9
4,756.5
54,576.9
9,173.3

DQN
1,620.0
978.0
4,280.4
4,359.0
1,364.5
279,987.0
455.0
29,900.0
8,627.5
585.6
50.4
88.0
385.5
4,657.7
6,126.0
110,763.0
23,633.0
12,149.4
-6.6
729.0
-4.9
30.8
797.4
8,777.4
473.0
20,437.8
-1.9
768.5
7,259.0
8,422.3
26,059.0
0.0
3,085.6
8,207.8
8,485.2
-286.1
19.5
146.7
13,117.3
7,377.6
39,544.0
63.9
5,860.6
-13,062.3
3,482.8
1,692.3
54,282.0
-5.6
12.2
4,870.0
68.1
9,989.9
163.0
196,760.4
2,704.0
18,098.9
5,363.0

PRIOR. DUEL.
3,941.0
2,296.8
11,477.0
375,080.0
1,192.7
395,762.0
1,503.1
35,520.0
30,276.5
3,409.0
46.7
98.9
366.0
7,687.5
13,185.0
162,224.0
41,324.5
72,878.6
-12.5
2,306.4
41.3
33.0
7,413.0
104,368.2
238.0
21,036.5
-0.4
812.0
1,792.0
10,374.4
48,375.0
0.0
3,327.3
15,572.5
70,324.3
0.0
20.9
206.0
18,760.3
20,607.6
62,151.0
27.5
931.6
-19,949.9
133.4
15,311.5
125,117.0
1.2
0.0
7,553.0
245.9
33,879.1
48.0
479,197.0
12,352.0
69,618.1
13,886.0

C51
3,166
1,735
7,203
406,211
1,516
841,075
976
28,742
14,074
1,645
81.8
97.8
748
9,646
15,600
179,877
47,092
130,955
2.5
3,454
8.9
33.9
3,965
33,641
440
38,874
-3.5
1,909
12,853
9,735
48,192
0.0
3,415
12,542
17,490
0.0
20.9
15,095
23,784
17,322
55,839
52.3
266,434
-13,901
8,342
5,747
49,095
6.8
23.1
8,329
280
15,612
1,520
949,604
9,300
35,050
10,513

QR-DQN-0
9,983
2,726
19,961
454,461
2,335
1,046,625
1,245
35,580
24,919
34,798
85.3
99.8
766
9,163
7,138
181,233
42,120
117,577
12.3
2,357
37.4
34.0
4,839
118,050
546
21,785
-3.6
1,028
14,780
11,139
71,514
75.0
5,822
17,557
65,767
0.0
21.0
146
26,646
9,336
67,780
61.1
2,680
-9,163
2,522
21,039
70,055
9.7
23.7
9,344
312
53,585
0.0
701,779
26,844
32,605
7,200

QR-DQN-1
4,871
1,641
22,012
261,025
4,226
971,850
1,249
39,268
34,821
3,117
77.2
99.9
742
12,447
14,667
161,196
47,887
121,551
21.9
2,355
39.0
34.0
4,384
113,585
995
21,395
-1.7
4,703
15,356
11,447
76,642
0.0
5,821
21,890
16,585
0.0
21.0
350
572,510
17,571
64,262
59.4
8,268
-9,324
6,740
20,972
77,495
8.2
23.6
10,345
297
71,260
43.9
705,662
25,061
26,447
13,112

Figure 7: Raw scores across all games, starting with 30 no-op actions. Reference values from Wang et al. (2016) and Bellemare,
Dabney, and Munos (2017).

