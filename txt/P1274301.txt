9
1
0
2
 
c
e
D
 
2
 
 
]
L
C
.
s
c
[
 
 
1
v
4
4
5
0
0
.
2
1
9
1
:
v
i
X
r
a

Multi-Scale Self-Attention for Text Classiﬁcation

Qipeng Guo‡∗, Xipeng Qiu‡ †, Pengfei Liu‡, Xiangyang Xue‡, Zheng Zhang§
‡ Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
‡ School of Computer Science, Fudan University
§ AWS Shanghai AI Lab
§ New York University Shanghai

qpguo16, xpqiu, pﬂiu14, xyxue

@fudan.edu.cn, zz@nyu.edu

{

}

Abstract

In this paper, we introduce the prior knowledge, multi-scale
structure, into self-attention modules. We propose a Multi-
Scale Transformer which uses multi-scale multi-head self-
attention to capture features from different scales. Based
on the linguistic perspective and the analysis of pre-trained
Transformer (BERT) on a huge corpus, we further design
a strategy to control the scale distribution for each layer.
Results of three different kinds of tasks (21 datasets) show
our Multi-Scale Transformer outperforms the standard Trans-
former consistently and signiﬁcantly on small and moderate
size datasets.

Introduction
Self-Attention mechanism is widely used in text classiﬁca-
tion tasks, and models based on self-attention mechanism
like Transformer (Vaswani et al. 2017), BERT (Devlin et al.
2018) achieves many exciting results on natural language
processing (NLP) tasks, such as machine translation, lan-
guage modeling (Dai et al. 2019), and text classiﬁcation.
Recently, Radford et al. (2018) points out the weakness of
self-attention modules, especially the poor performance on
small and moderate size datasets. Although the pre-training
on huge corpus could help this, we believe the fundamental
reason is that self-attention module lacks suitable inductive
bias, so the learning process heavily depends on the train-
ing data. It is hard to learn a model with good generalization
ability from scratch on a limited training set without a good
inductive bias.

Multi-Scale structures are widely used in computer vi-
sion (CV), NLP, and signal processing domains. It can
help the model to capture patterns at different scales and
extract robust features. Speciﬁc to NLP domain, a com-
mon way to implement multi-scale is the hierarchical struc-
ture, such as convolutional neural networks (CNN) (Kalch-
brenner, Grefenstette, and Blunsom 2014), multi-scale re-
current neural networks (RNN) (Chung, Ahn, and Bengio
2016) tree-structured neural networks (Socher et al. 2013;
Tai, Socher, and Manning 2015) and hierarchical attention

(Yang et al. 2016). The principle behind these models is
the characteristic of language: the high-level feature is the
composition of low-level terms. With hierarchical structures,
these models capture the local compositions at lower layers
and non-local composition at high layers. This division of
labor makes the model less data-hungry.

However, for self-attention modules, there is no restric-
tion of composition bias. The dependencies between words
are purely data-driven without any prior, leading to easily
overﬁt on small or moderate size datasets.

In this paper, we propose a multi-scale multi-head self-
attention (MSMSA), in which each attention head has a vari-
able scale. The scale of a head restricts the working area
of self-attention. Intuitively, a large scale makes the fea-
ture involving more contextual information and being more
smooth. A small scale insists on the local bias and encour-
ages the features to be outstanding and sharp. Based on
MSMSA, we further propose multi-scale Transformer, con-
sisting of multiple MSMSA layers. Different from the multi-
scale in hierarchical structures, each layer of multi-scale
Transformer consists of several attention heads with multi-
ple scales, which brings an ability to capture the multi-scale
features in a single layer.

Contributions of this paper are:

•

•

•

We introduce the multi-scale structure into self-attention
framework, the proposed model Multi-Scale Transformer
can extract features from different scales.

Inspired by the hierarchical structure of language, we fur-
ther develop a simple strategy to control the scale distribu-
tion for different layers. Based on the empirical result on
real tasks and the analysis from BERT, we suggest using
more small-scale attention heads in shallow layers and a
balanced choice for different scales in deep layers.

The building block of Multi-Scale Transformer, multi-
scale multi-head self-attention provides a ﬂexible way to
introduce scale bias (local or global), and it is a replace-
ment of the multi-head self-attention and position-wise
feed-forward networks.

∗Work done during internship at AWS Shanghai AI Lab.
†Corresponding author: Xipeng Qiu (xpqiu@fudan.edu.cn).

Results on three tasks (21 datasets) show our Multi-Scale
Transformer outperforms the standard Transformer con-

•

sistently and signiﬁcantly on small and moderate size
datasets.

Background
Self-attention and its extend architecture, Transformer,
achieved many good results on NLP tasks. Instead of uti-
lizing CNN or RNN unit to model the interaction between
different words, Transformer achieves pair-wised interaction
by attention mechanism.

∈

Mulit-Head Self-attention The main component of the
Transformer is the multi-head dot-product attention, which
could be formalized as follows. Given a sequence of vectors
RN ×D, where N is the length of the sequence and the
H
D is the dimension of the vector. When doing multi-head
self-attention, the module projects the H into three matri-
ces: the query Q, the key K and the value V. These three
matrices would be further decomposed into N ′ sub-spaces
which corresponds to the N ′ heads and each head has D′
units.

MSA(H) = [head1,

, headN ′]WO,

headi = softmax(

· · ·
QiKT
i
√D′
Q = HWQ, K = HWK, V = HWV ,

)Vi,

(1)

(2)

(3)

where MSA(
·
and WQ, WK, WV , WO are learnable parameters.

) represents the Multi-head Self-Attention,

Transformer Each layer in the Transformer consists
of a multi-head self-attention and a FFN layer
(also
called Position-wise Feed-Forward Networks in Vaswani et
al. (2017)). The hidden state of l + 1 layer, Hl+1 could be
calculated as followings.

Zl =norm(Hl + MSA(Hl)),

Hl+1 =norm(Zl + FFN(Zl)),

(4)
(5)

) means the layer normalization (Ba, Kiros,
where norm(
·
and Hinton 2016). In addition, the Transformer augments the
input features by adding a positional embedding since the
self-attention could not capture the positional information
by itself.

Despite its effectiveness on machine translation and lan-
guage modeling, Transformer usually fails on the task with
moderate size datasets due to its shortage of inductive bias.

Model
In the multi-head self-attention, each head captures the pair-
wise interactions between words in different feature space.
Each head has the same scale of sentence length.

Scale-Aware Self-Attention To introduce the concept of
multi-scale into the self-attention mechanism, we use a
simple way to enhance the regular self-attention, named
Scale-Aware Self-Attention (SASA) which is equal to the
restricted self-attention which is proposed in Vaswani et
al. (2017) but using a dynamic size of the window.

hl+1
i

head1

head2

head3

hl

i−2

hl

i−1

hl
i

hl

i+1

hl

i+2

Figure 1: A diagram of Multi-Scale Multi-Head Self-
Attention, we can see three heads which correspond to three
different scales in the ﬁgure. The blue, green, red box illus-
trate the scale of ω = 1, ω = 3, ω = 5, respectively.

Given a sequence of vectors H

RN ×D with length N .
SASA has a parameter ω which is either a constant num-
ber or a ratio according to the sequence length to control its
working scope. An attention head can be computed as

∈

head(H, ω)i,j = SM(

QijCij(K, ω)T
√D
Cij(x, ω) = [xi,j−ω, ..., xi,j+ω],
Q = HWQ, K = HWK, V = HWV ,

)Cij (V, ω),

(6)

(7)

(8)

where i indicates the i-th head and j means j-th position. The
SM represents “Softmax” function, C is a function to extract
context for a given position. WQ, WK, WV are learnable
parameters. The scale of a head could be a variable like N
2
or a ﬁxed number like 3.

Multi-Scale Multi-Head Self-Attention With SASA, we
can implement a Multi-Scale Multi-Head Self-Attention
(MSMSA) with multiple scale-aware heads. Each head
works on different scales. For N ′ heads self-attention,
MSMSA with scales Ω = [ω1,

, ωN ′] is

· · ·

MSMSA(H, Ω) = [head1(H, ω1);

;
· · ·
headN ′ (H, ωN ′)]WO,

(9)

where WO is a parameter matrix.

Compared to the vanilla multi-head self-attention, vari-
able Ω controls the attended area and makes different heads
have different duties.

Multi-Scale Transformer With the MSMSA, we could
construct the multi-scale Transformer (MS-Transformer. In
short, MS-Trans). Besides using MSMSA to replace the
vanilla multi-head self-attention, we also remove the FFN
(see Eq. (5)) because it could be view as a self-attention
with scale ω = 1 plus a non-linear activation function. Since
MSMSA introduces the locality to the model, the MSMSA
with a small scale can be an alternative to the positional
embedding. Therefore, the multi-scale Transformer could be
formalized as followings.

Hl+1 = norm(Hl + ReLU(MSMSA(Hl), Ωl))

(10)

where l is the layer index.

In this work, we limit the choice of scale size among
or variables depend on the se-
. And we force the scale size to

1, 3,
constant numbers
{
N
16 , N
8 ,
quence length
be an odd number.

· · · }
· · · }

{

Compared to the hierarchical multi-scale models, multi-
scale Transformer allows the attention heads in one layer
have various scales of vision, it is a “soft” version of viewing
different layers as different scales.

Classiﬁcation node We also ﬁnd adding a special node at
the beginning of each sequence and connecting it directly
to the ﬁnal sentence representation can improve the perfor-
mance. This technique was introduced in BERT (Devlin et
al. 2018), refer as “[CLS]”. And it is also similar to the “re-
lay node” in Guo et al. (2019). Different with them, we com-
bine the “[CLS]” node and the feature from applying max-
pooling over all the nodes in the ﬁnal layer to represent the
sentence. There is no difference between the “[CLS]” node
and other tokens in the input sequence except it can directly
contribute to the ﬁnal representation.

Looking for Effective Attention Scales
Multi-scale Transformer is a ﬂexible module in which each
layer can have multi-scale attention heads. Therefore, an im-
portant factor is how to design the scale distribution for each
layer.

Hierarchical Multi-Scale or Flexible Multi-Scale
A simple way to implement multi-scale feature extraction is
following the hierarchical way, which stacks several layers
with small-scale heads. The lower layers capture the local
features and the higher capture the non-local features.

,

a

To verify the ability of hierarchical multi-scale, we design
a very simple simulation task named mirrored summation.
Rd and drawn
Given a sequence A =
a1, ..., aN
}
{
K
from U(0, 1). The target is
aN −i+1, where K is
i=1 ai
⊙
a ﬁxed integer less than the sequence length N and
means
P
the Hadamard production. The minimum dependency path
length is N
K, we use this task to test the ability of models
for capturing long-range dependencies. Both train and test
set are random generated and they have 200k samples each.
We can assume the size of training set is enough to train
these models thoroughly.

⊙

−

∈

We use three different settings of MS-Trans.

1. MS-Trans-Hier-S: MS-Transformer with two layers, and

each layer has 10 heads with a small scale ω = 3.

2. MS-Trans-deepHier-S: MS-Transformer with six layers,
and each layer has 10 heads with a small scale ω = 3.

4 , N

8 , N

16 , N

3. MS-Trans-Flex: MS-Transformer with two layers, and
each layer has 10 heads with ﬂexible multi-scales ω =
3, N
2 . Each scale has two heads.
As shown in Fig-2, Transformer achieves the best re-
sult, and MS-Trans-Flex follows with it. Although Trans-
former has the highest potential to capture long-range de-
pendencies, it requires large training samples. Meanwhile,

BiLSTM
MS-Trans-deepHier-S
MS-Trans-Hier-S
MS-Trans-Flex
Trans

0.1

0.09

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

0

E
S
M

t
s
e
T

0

10 20 30 40 50 60 70 80 90 100

K

Figure 2: Mirrored Summation Task. The curve of MSE ver-
sus valid number K with the sequence length n = 100 and
the dimension of input vectors d = 50.

our model balance the data-hungry problem and the ability
for capturing long-range dependencies. Based on the com-
parison of MS-Trans-Hier-S and MS-Trans-deepHier-S, we
can ﬁnd the improvement of additional layers is relatively
small. According to the synthetic experiment and the per-
formance on real tasks (see. Sec-), we think the large-scale
heads are necessary for the lower layers and stacked small-
scale heads are hard to capture long-range dependencies. In
this case, a good prior should contain both small-scale and
large-scale.

Scale Distributions of Different Layers
Since multi-scale is necessary for each layer, the second
question is how to design the proportion of different
scales for each layer? We think each layer may have its
preference for the scale distribution. From the linguistic per-
spective, an intuitive assumption may be: the higher layer
has a higher probability for the large-scale heads and the
shallow layer has a higher probability for the small-scale
heads.

To look for the empirical evidence, we probe several
typical cases and analysis the corresponding behavior in a
data-driven model, BERT (Bidirectional Encoder Represen-
tations from Transformers) (Devlin et al. 2018).

Analogy Analysis from BERT BERT is a pre-trained
Transformer on large scale data, which has shown its power
on many NLP tasks and it has good generalization ability.
Since BERT is based on Transformer, it is not guided by
prior knowledge, their knowledge is learned from data. We
probe the behavior of BERT to see whether it ﬁts the linguis-
tic assumption. There are two aspects we want to study, the
ﬁrst is the working scales of attention heads of each layer in
BERT. The second is the difference between the scale distri-
butions of different layers, especially the preference of local
and global relations. To probe these behaviors, we ﬁrst run
the BERT over many natural sentences and pick the highest

40

20

e
g
a
t
n
e
c
r
e
P

head-1 at layer-1
head-2 at layer-1
head-3 at layer-1

20

10

e
g
a
t
n
e
c
r
e
P

layer-1
layer-6
layer-12

0

0

10

15

0

0

10

15

Relative Distance of Attention Head

Relative Distance of Attention Head

5

(b)

5

(a)

Figure 3: The visualization of BERT. (a) The attention distance distributions of three heads in the ﬁrst layer. The red head only
cares about the local pattern and the blue head equally looks at different distances. (b) The attention distance distribution of
different layers. The shallow layer prefers the small scale size and tends to large scale size slowly when the layer gets deeper.
Even in the ﬁnal layer, local patterns still occupy a large percentage. We truncate the distance at N
2 for better visualization. The
full ﬁgure can be found in the Appendix.

activation of the attention map as the attention edge. Then
we record the relative distances of these attention edges.
In this work, we obtain the data from running BERT on
CoNLL03 dataset (see Tab-1).

We ﬁrst draw the Fig-3a for observing the behavior of
heads in the same layer. We pick three heads, and their dif-
ference is signiﬁcant. As shown in the ﬁgure, “head-2” focus
on a certain distance with a small scale, and “head-1” cover
all the distances. There is a clear division of labor of these
heads, and one layer can have both local and global vision
via combining features from different heads.

The second ﬁgure Fig-3b shows the trend of distance pref-
erence when the depth of layer increased. We can ﬁnd the
model move from local vision to global vision slowly and
shallow layers have a strong interest in local patterns.

The visualization of BERT ﬁts the design of Multi-Scale
Transformer, the ﬁrst observation corresponds to the design
of multi-scale multi-head self-attention which ask different
heads focus on different scales, and the second observation
provides a good reference of the trend of scale distribution
across layers. Using such knowledge can largely reduce the
requirement of training data for Transformer-like models.

Control Factor of Scale Distributions for Different
Layers

From the intuitive linguistic perspective and empirical evi-
dence, we design a control factor of scale distributions for
different layers of multi-scale Transformer.

Let L denote the number of layers in multi-scale Trans-
former,
denote the number of candidate scale sizes, and
nl
k denote the number of heads for l-th layer and k-th scale
size.

Ω

|

|

The head number nl

k is computed by

·

|

,

}

1

Ω

· · ·

∈ {

(11)

zl
k =

0
k+1 + α
zl
(cid:26)
l
nl = softmax(zl)

l = L or k =
|
|
k
0,
Ω
| −
N ′
(12)
In the above equations, we introduce a hyper-parameter α
to control the change of preference of scale sizes for each
layer. For example, α = 0 means all the layers use the
same strategy of scale size, α > 0 means the preference
of smaller scale increased with the decline of layer depth,
and α < 0 indicates the preference of larger scale increased
with the decline of layer depth. As the conclusion of analyz-
ing BERT, we believe the deep layer has a balanced vision
over both local and global patterns, so the top layer should
be set to looking all the scale size uniformly. More speciﬁc,
when α = 0.5 and N ′ = 10, three layers have nl=1 =
4, 2, 2, 1, 1
5, 2, 2, 1, 0
,
{
}
it represents the ﬁrst layer has 5 head with scale size of 1, 2
head with scale size of 3, 2 head with scale size of N
16 and 1
head with scale size of N
8 .

2, 2, 2, 2, 2

, nl=3 =

, nl=2 =

}

{

}

{

Experiments
We evaluate our model on 17 text classiﬁcation datasets, 3
sequence labeling datasets and 1 natural language inference
dataset. All the statistics can be found in Tab-1. Besides,
we use GloVe (Pennington, Socher, and Manning 2014) to
initialize the word embedding and JMT (Hashimoto et al.
2017) for character-level features. The optimizer is Adam
(Kingma and Ba 2014) and the learning rate and dropout ra-
tio are listed in the Appendix.

To focus on the comparison between different model de-
signs, we don’t list results of BERT-like models because the
data augmentation and pre-training is an orthogonal direc-
tion.

Table 1: An overall of datasets and its hyper-parameters, “H DIM, α, head DIM” indicates the dimension of hidden states,
the hyper-parameter for controlling the scale distribution, the dimension of each head, respectively. The candidate scales are
1, 3, N
4 for SST,MTL-16,SNLI datasets. And we use 1, 3, 5, 7, 9 for sequence labeling tasks. MTL-16† consists of 16
datasets, each of them has 1400/200/400 samples in train/dev/test.

16 , N

8 , N

Dataset

Train Dev.

Test

H DIM α

head DIM

SST (Socher et al. 2013)

8k

1k

2k

300

0.5

30

MTL-16 †
(Liu, Qiu,
and Huang
2017)

Apparel Baby Books Camera
DVD Electronics Health IMDB
Kitchen Magazines MR Music
Software Sports Toys Video

PTB POS (Marcus, Santorini, and Marcinkiewicz 1993)

CoNLL03 (Sang and Meulder 2003)

CoNLL2012 NER (Pradhan et al. 2012)

SNLI (Bowman et al. 2015)

1400

200

400

8k∼28k

300

0.5

38k

15k

94k

550k

5k

3k

14k

10k

5k

3k

8k

10k

300

300

300

600

1.0

1.0

1.0

0.5

30

30

30

30

64

|V |

20k

44k

25k

63k

34k

87.25
85.50
85.25
89.00
86.25
86.50
87.50
84.25
85.50
91.50
79.25
82.00
88.50
85.75
87.50
90.00

86.34

Dataset

Apparel
Baby
Books
Camera
DVD
Electronics
Health
IMDB
Kitchen
Magazines
MR
Music
Software
Sports
Toys
Video

Average

Table 3: Test Accuracy over MTL-16 datasets. “SLSTM”
refer to the sentence-state LSTM (Zhang, Liu, and Song
2018).

Acc (%)

MS-Trans Transformer BiLSTM SLSTM

82.25
84.50
81.50
86.00
77.75
81.50
83.50
82.50
83.00
89.50
77.25
79.00
85.25
84.75
82.00
84.25

82.78

86.05
84.51
82.12
87.05
83.71
82.51
85.52
86.02
82.22
92.52
75.73
78.74
86.73
84.04
85.72
84.73

84.01

85.75
86.25
83.44
90.02
85.52
83.25
86.50
87.15
84.54
93.75
76.20
82.04
87.75
85.75
85.25
86.75

85.38

and Multi-Scale Transformer can achieve 1.8 times acceler-
ation on very short sentences (average 22 tokens).

Sequence Labelling
Besides tasks which use the model as a sentence encoder, we
are also interested in the effectiveness of our model on se-
quence labeling tasks. We choose the Part-of-Speech (POS)
tagging and the Named Entity Recognition (NER) task to
verify our model. We use three datasets as our benchmark:
Penn Treebank (PTB) POS tagging dataset (Marcus, San-
torini, and Marcinkiewicz 1993), CoNLL2003 NER dataset
(Sang and Meulder 2003), CoNLL2012 NER dataset (Prad-
han et al. 2012).

Results in Tab-4 shows Multi-Scale Transformer beats

Table 2: Test Accuracy on SST dataset.

Model

BiLSTM (Li et al. 2015)
Tree-LSTM (Tai, Socher, and Manning 2015)
CNN-Tensor (Lei, Barzilay, and Jaakkola 2015)
Emb + self-att (Shen et al. 2018a)
BiLSTM + self-att (Yoon, Lee, and Lee 2018)
CNN + self-att (Yoon, Lee, and Lee 2018)
Dynamic self-att (Yoon, Lee, and Lee 2018)
DiSAN (Shen et al. 2018a)

Transformer
Multi-Scale Transformer

Acc

49.8
51.0
51.2
48.9
50.4
50.6
50.6
51.7

50.4
51.9

Text Classiﬁcation

Text Classiﬁcation experiments are conducted on Stanford
Sentiment Treebank(SST) dataset (Socher et al. 2013) and
MTL-16 (Liu, Qiu, and Huang 2017) consists of 16 small
datasets in different domains. Besides the base model we in-
troduced before, we use a two-layer MLP(Multi-Layer Per-
ceptron) with softmax function as the classiﬁer. It receives
the feature from applying max-pooling over the top layer
plus the classiﬁcation node.

Tab-2 and 3 give the results on SST and MTL-16. Multi-
Scale Transformer achieves 1.5 and 3.56 points against
Transformer on these two datasets, respectively. Meanwhile,
Multi-Scale Transformer also beats many existing models
including CNNs and RNNs.

Since the sentence average length of MTL-16 dataset is
relatively large, we also report the efﬁciency result in Fig-4.
We implement the MS-Trans with Pytorch1 and DGL(Wang
et al. 2019). Multi-Scale Transformer achieves 6.5 times ac-
celeration against Transformer on MTL-16 dataset on aver-
age (average sentence length equals 109 tokens). The maxi-
mum of acceleration reaches 10 times (average 201 tokens)

1https://pytorch.org

Table 4: Results on sequence labeling tasks. We list “Advanced Techniques” except pre-trained embeddings (GloVe, Word2Vec,
JMT) in columns. The “Char” indicates character-level features, it also includes the Capitalization Features, Lexicon Features,
etc. The “CRF” means an additional Conditional Random Field layer.

Model

Ling et al. (2015)
Huang, Xu, and Yu (2015)
Chiu and Nichols (2016a)
Ma and Hovy (2016)
Chiu and Nichols (2016b)
Zhang, Liu, and Song (2018)
Akhundov, Trautmann, and Groh (2018)

Transformer
Transformer + Char
Multi-Scale Transformer
Multi-Scale Transformer + Char
Multi-Scale Transformer + Char + CRF

NER

CoNLL2003
F1

CoNLL2012
F1

Adv Tech

char
X
X
X
X
X
X
X

X

X
X

CRF
X
X
X
X
X
X
X

X

POS
PTB
Acc

97.78
97.55
-
97.55
-
97.55
97.43

96.31
97.04
97.24
97.54
97.66

-
90.10
90.69
91.06
91.62
91.57
91.11

86.48
88.26
89.38
91.33
91.59

-
-
86.35
-
86.28
-
87.84

83.57
85.14
85.26
86.77
87.80

MS-Trans
Trans
BiLSTM

Table 5: Test Accuracy on SNLI dataset for sentence vector-
based models.

d
n
o
c
e
s
i
l
l
i

M

200

150

100

50

0

IMDB

AVG

MR
Dataset

Figure 4: Test time per batch (batch size is 128) on the
dataset which has the longest length (IMDB), the dataset
which has the shortest length (MR), and the average over
16 datasets in MTL-16.

the vanilla Transformer on these three sequence labeling
datasets, which consists of other results reported above. It
shows Multi-Scale Transformer can extract useful features
for each position as well.

Natural Language Inference
Natural Language Inference (NLI) is a classiﬁcation which
ask the model to judge the relationship of two sentences
from three candidates, “entailment”, “contradiction”, and
“neutral”. We use a widely-used benchmark Stanford Natu-
ral Language Inference (SNLI) (Bowman et al. 2015) dataset
to probe the ability of our model for encoding sentence, we
compare our model with sentence vector-based models. Dif-
ferent with the classiﬁer in text classiﬁcation task, we follow
the previous work (Bowman et al. 2016) to use a two-layer
r2)
MLP classiﬁer which takes concat(r1, r2,
as inputs, where r1, r2 are representations of two sentences
and equals the feature used in text classiﬁcation task.

, r1

r1

r2

−

−

k

k

Model

BiLSTM (Liu et al. 2016)
BiLSTM + self-att (Liu et al. 2016)
4096D BiLSTM-max (Conneau et al. 2017)
300D DiSAN (Shen et al. 2018a)
Residual encoders (Nie and Bansal 2017)
Gumbel TreeLSTM (Choi, Yoo, and Lee 2018)
Reinforced self-att (Shen et al. 2018b)
2400D Multiple DSA (Yoon, Lee, and Lee 2018)

Transformer
Multi-Scale Transformer

Acc

83.3
84.2
84.5
85.6
86.0
86.0
86.3
87.4

82.2
85.9

As shown in Tab-5, Multi-Scale Transformer outperforms
Transformer and most classical models, and the result is
comparable with the state-of-the-art. The reported number
of Transformer is obtained with heuristic hyper-parameter
selection, we use a three-layer Transformer with heavy
dropout and weight decay. And there is still a large mar-
gin compared to Multi-Scale Transformer. This compari-
son also indicates the moderate size training data (SNLI has
550k training samples) cannot replace the usefulness of prior
knowledge.

Analysis
Inﬂuence of Scale Distributions As we introduced in Eq.
(11), we control the scale distributions over layers by a
hyper-parameter α. In this section, we give a comparison of
using different α, where the positive value means local bias
increased with the decline of layer depth and the negative
value means global bias increased with the decline of layer
depth.

As shown in the upper part of Tab-6, local bias in shallow
layers is a key factor to achieve good performance, and an
appropriate positive α achieves the best result. In contrast,

Table 6: Analysis of different scale distributions on SNLI
test set. The upper part shows the inﬂuence of hyper-
parameter α which change the distribution of scales across
layers. The ﬁve candidates of scale size are 1, 3, N
8 , N
4 ,
respectively. The lower part lists the performance of single-
scale models which use a ﬁxed scale for the whole model.

16 , N

multi-scale

N’

L Acc

α

1.0
0.5
0.0
-0.5
-1.0

5
5
5
5
5

3
N/16
N/8
N/4

3
3
3
3
3

3
3
3
3

85.5
85.9
84.9
84.7
84.3

84.3
83.9
81.7
80.7

A
B
C
D
E

F
G
H
I

single-scale

ω

L Acc

all the negative values harm the performance, that means too
much global bias in shallow layers may lead the model to a
wrong direction. The observation of this experiment ﬁts our
intuition, the high-level feature is the composition of low-
level terms.

Multi-Scale vs. Single-Scale As we claimed before,
Multi-Scale Transformer can capture knowledge at differ-
ent scales at each layer. Therefore, a simple question needs
to be evaluated is whether the multi-scale model outper-
forms the single-scale model or not. To answer this question,
we compare Multi-Scale Transformer with several single-
scale models. Model F,G,H,I have the same number of layers
and attention heads with Multi-scale Transformer, but their
scales are ﬁxed.

Result in the lower part of Tab-6 reveal the value of
Multi-Scale Transformer, it achieves 1.6 points improve-
ment against the best single-scale model. And this result
also supports that local bias is an important inductive bias
for NLP task.

Related Works
Typical multi-scale models The multi-scale structure has
been used in many NLP models, it could be implemented in
many different ways. Such as stacked layers (Kalchbrenner,
Grefenstette, and Blunsom 2014; Kim 2014), tree-structure
(Socher et al. 2013; Tai, Socher, and Manning 2015; Zhu,
Sobhani, and Guo 2015), hierarchical timescale (El Hihi and
Bengio 1995; Chung, Ahn, and Bengio 2016), layer-wise
gating (Chung et al. 2015). Since these models are built on
modules like RNNs and CNNs, which embodies the intrin-
sic local bias by design, the common spirit of introducing
multi-scale is to enable long-range communications. In con-
trast, Transformer allows long-range communications, so we
want the multi-scale brings local bias.

Transformers with additional inductive bias This work
is not the ﬁrst attempt of introducing inductive bias into

Transformer.

Shaw, Uszkoreit, and Vaswani (2018) suggest Trans-
former should care about the relative distance between to-
kens rather than the absolute position in the sequence. The
information of relative distance could be obtained by look-
ing multi-scale of the same position, so our model could be
aware of the relative distance if using enough scales.

Li et al. (2018) propose a regularization of enhancing the
diversity of attention heads. Our multi-scale multi-head self-
attention can make a good division of labor of heads via re-
stricting them in different scales.

Yang et al. (2018a) and Yang et al. (2018b) also introduce

the local bias into Transformer.

Different from the above models, we focus on importing
the notion of multi-scale to self-attention. Meanwhile, their
models use the single-scale structure. Our experimental re-
sults have shown the effectiveness of the multi-scale mech-
anism.

Conclusion
In this work, we present Multi-Scale Self-Attention and
Multi-Scale Transformer which combines the prior knowl-
edge of multi-scale and the self-attention mechanism. As
a result, it has the ability to extract rich and robust fea-
tures from different scales. We compare our model with the
vanilla Transformer on three real tasks (21 datasets). The
result suggests our proposal outperforms the vanilla Trans-
former consistently and achieves comparable results with
state-of-the-art models.

Acknowledgments
This work was supported by the National Key Research and
Development Program of China (No. 2018YFC0831103),
National Natural Science Foundation of China (No.
61672162), Shanghai Municipal Science and Technology
Major Project (No. 2018SHZDZX01) and ZJLab.

References
Akhundov, A.; Trautmann, D.; and Groh, G. 2018. Sequence la-
beling: A practical approach. CoRR abs/1808.03926.
Ba, L. J.; Kiros, R.; and Hinton, G. E. 2016. Layer normalization.
CoRR abs/1607.06450.
Bowman, S. R.; Angeli, G.; Potts, C.; and Manning, C. D. 2015. A
large annotated corpus for learning natural language inference. In
EMNLP, 632–642. The Association for Computational Linguistics.
Bowman, S. R.; Gauthier, J.; Rastogi, A.; Gupta, R.; Manning,
C. D.; and Potts, C. 2016. A fast uniﬁed model for parsing and
In ACL (1). The Association for Com-
sentence understanding.
puter Linguistics.
Chiu, J., and Nichols, E. 2016a. Sequential labeling with bidirec-
tional lstm-cnns. In Proc. International Conf. of Japanese Associ-
ation for NLP, 937–940.
Chiu, J. P. C., and Nichols, E. 2016b. Named entity recognition
with bidirectional lstm-cnns. TACL 4:357–370.
Choi, J.; Yoo, K. M.; and Lee, S. 2018. Learning to compose
task-speciﬁc tree structures. In AAAI, 5094–5101. AAAI Press.
Chung, J.; Ahn, S.; and Bengio, Y. 2016. Hierarchical multiscale
recurrent neural networks. arXiv preprint arXiv:1609.01704.

Chung, J.; G¨ulc¸ehre, C¸ .; Cho, K.; and Bengio, Y. 2015. Gated
feedback recurrent neural networks. In ICML, volume 37 of JMLR
Workshop and Conference Proceedings, 2067–2075. JMLR.org.
Conneau, A.; Kiela, D.; Schwenk, H.; Barrault, L.; and Bordes, A.
2017. Supervised learning of universal sentence representations
from natural language inference data. In EMNLP, 670–680. Asso-
ciation for Computational Linguistics.
Dai, Z.; Yang, Z.; Yang, Y.; Carbonell, J. G.; Le, Q. V.; and
Salakhutdinov, R. 2019. Transformer-xl: Attentive language mod-
els beyond a ﬁxed-length context. CoRR abs/1901.02860.
Devlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2018. BERT:
pre-training of deep bidirectional transformers for language under-
standing. CoRR abs/1810.04805.
El Hihi, S., and Bengio, Y. 1995. Hierarchical recurrent neural
networks for long-term dependencies. In NIPS, 493–499. Citeseer.
Guo, Q.; Qiu, X.; Liu, P.; Shao, Y.; Xue, X.; and Zhang, Z. 2019.
Star-transformer. In NAACL-HLT (1), 1315–1325. Association for
Computational Linguistics.
Hashimoto, K.; Xiong, C.; Tsuruoka, Y.; and Socher, R. 2017.
A joint many-task model: Growing a neural network for multiple
NLP tasks. In EMNLP, 1923–1933. Association for Computational
Linguistics.
Huang, Z.; Xu, W.; and Yu, K. 2015. Bidirectional LSTM-CRF
models for sequence tagging. CoRR abs/1508.01991.
Kalchbrenner, N.; Grefenstette, E.; and Blunsom, P. 2014. A con-
volutional neural network for modelling sentences. In Proceedings
of ACL.
Kim, Y. 2014. Convolutional neural networks for sentence classiﬁ-
cation. In Proceedings of the 2014 Conference on EMNLP, 1746–
1751.
Kingma, D. P., and Ba, J. 2014. Adam: A method for stochastic
optimization. CoRR abs/1412.6980.
Lei, T.; Barzilay, R.; and Jaakkola, T. S. 2015. Molding cnns for
text: non-linear, non-consecutive convolutions. In EMNLP, 1565–
1575. The Association for Computational Linguistics.
Li, J.; Luong, T.; Jurafsky, D.; and Hovy, E. H. 2015. When are
tree structures necessary for deep learning of representations? In
EMNLP, 2304–2314. The Association for Computational Linguis-
tics.
Li, J.; Tu, Z.; Yang, B.; Lyu, M. R.; and Zhang, T. 2018. Multi-
head attention with disagreement regularization. In EMNLP, 2897–
2903. Association for Computational Linguistics.
Ling, W.; Dyer, C.; Black, A. W.; Trancoso, I.; Fermandez, R.;
Amir, S.; Marujo, L.; and Lu´ıs, T. 2015. Finding function in
form: Compositional character models for open vocabulary word
representation. In EMNLP, 1520–1530. The Association for Com-
putational Linguistics.
Liu, Y.; Sun, C.; Lin, L.; and Wang, X. 2016. Learning natu-
ral language inference using bidirectional LSTM model and inner-
attention. CoRR abs/1605.09090.
Liu, P.; Qiu, X.; and Huang, X. 2017. Adversarial multi-task learn-
ing for text classiﬁcation. In ACL (1), 1–10. Association for Com-
putational Linguistics.
Ma, X., and Hovy, E. H. 2016. End-to-end sequence labeling via
bi-directional lstm-cnns-crf. In ACL (1). The Association for Com-
puter Linguistics.
Marcus, M. P.; Santorini, B.; and Marcinkiewicz, M. A. 1993.
Building a large annotated corpus of english: The penn treebank.
Computational Linguistics 19(2):313–330.

Nie, Y., and Bansal, M. 2017. Shortcut-stacked sentence encoders
for multi-domain inference. In RepEval@EMNLP, 41–45. Associ-
ation for Computational Linguistics.
Pennington, J.; Socher, R.; and Manning, C. 2014. Glove: Global
In Proceedings of the 2014
vectors for word representation.
conference on empirical methods in natural language processing
(EMNLP), 1532–1543.
Pradhan, S.; Moschitti, A.; Xue, N.; Uryupina, O.; and Zhang, Y.
2012. Conll-2012 shared task: Modeling multilingual unrestricted
coreference in ontonotes. In EMNLP-CoNLL Shared Task, 1–40.
ACL.
Radford, A.; Narasimhan, K.; Salimans, T.; and Sutskever, I. 2018.
Improving language understanding by generative pre-training.
Sang, E. F. T. K., and Meulder, F. D. 2003.
Introduction to
the conll-2003 shared task: Language-independent named entity
recognition. In CoNLL, 142–147. ACL.
Shaw, P.; Uszkoreit, J.; and Vaswani, A. 2018. Self-attention with
relative position representations. In NAACL-HLT (2), 464–468. As-
sociation for Computational Linguistics.
Shen, T.; Zhou, T.; Long, G.; Jiang, J.; Pan, S.; and Zhang, C.
2018a. Disan: Directional self-attention network for rnn/cnn-free
language understanding. In AAAI, 5446–5455. AAAI Press.
Shen, T.; Zhou, T.; Long, G.; Jiang, J.; Wang, S.; and Zhang, C.
2018b. Reinforced self-attention network: a hybrid of hard and soft
attention for sequence modeling. In IJCAI, 4345–4352. ijcai.org.
Socher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning, C. D.; Ng,
A. Y.; and Potts, C. 2013. Recursive deep models for semantic
In EMNLP, 1631–
compositionality over a sentiment treebank.
1642. ACL.
Tai, K. S.; Socher, R.; and Manning, C. D. 2015.
Improved se-
mantic representations from tree-structured long short-term mem-
ory networks. In ACL (1), 1556–1566. The Association for Com-
puter Linguistics.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;
Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention is
all you need. In NIPS, 6000–6010.
Wang, M.; Yu, L.; Zheng, D.; Gan, Q.; Gai, Y.; Ye, Z.; Li, M.;
Zhou, J.; Huang, Q.; Ma, C.; Huang, Z.; Guo, Q.; Zhang, H.; Lin,
H.; Zhao, J.; Li, J.; Smola, A. J.; and Zhang, Z. 2019. Deep graph
library: Towards efﬁcient and scalable deep learning on graphs.
CoRR abs/1909.01315.
Yang, Z.; Yang, D.; Dyer, C.; He, X.; Smola, A.; and Hovy, E.
2016. Hierarchical attention networks for document classiﬁcation.
In Proceedings of the 2016 Conference of NAACL, 1480–1489.
Yang, B.; Tu, Z.; Wong, D. F.; Meng, F.; Chao, L. S.; and Zhang, T.
2018a. Modeling localness for self-attention networks. In EMNLP,
4449–4458. Association for Computational Linguistics.
Yang, B.; Wang, L.; Wong, D. F.; Chao, L. S.; and Tu, Z. 2018b.
Convolutional self-attention network. CoRR abs/1810.13320.
Yoon, D.; Lee, D.; and Lee, S. 2018. Dynamic self-attention : Com-
puting attention over words dynamically for sentence embedding.
CoRR abs/1808.07383.
Zhang, Y.; Liu, Q.; and Song, L. 2018. Sentence-state LSTM for
text representation. In ACL (1), 317–327. Association for Compu-
tational Linguistics.
Zhu, X.-D.; Sobhani, P.; and Guo, H. 2015. Long short-term mem-
ory over recursive structures. In ICML, 1604–1612.

