Stein Points

Wilson Ye Chen 1 Lester Mackey 2 Jackson Gorham 3 Franc¸ois-Xavier Briol 4 5 6 Chris. J. Oates 7 6

8
1
0
2
 
n
u
J
 
9
1
 
 
]

O
C

.
t
a
t
s
[
 
 
4
v
1
6
1
0
1
.
3
0
8
1
:
v
i
X
r
a

Abstract

An important task in computational statistics and
machine learning is to approximate a posterior
distribution p(x) with an empirical measure sup-
ported on a set of representative points {xi}n
i=1.
This paper focuses on methods where the selec-
tion of points is essentially deterministic, with an
emphasis on achieving accurate approximation
when n is small. To this end, we present Stein
Points. The idea is to exploit either a greedy or
a conditional gradient method to iteratively min-
imise a kernel Stein discrepancy between the em-
pirical measure and p(x). Our empirical results
demonstrate that Stein Points enable accurate ap-
proximation of the posterior at modest compu-
tational cost. In addition, theoretical results are
provided to establish convergence of the method.

1. Introduction

This paper is motivated by approximation of a Borel distribu-
tion P , deﬁned on a topological space X, with deterministic
point sets or sequences {xi}n
i=1

⊂ X for n ∈ N, such that

1
n

∑n

i=1 h(xi) → ∫ h dP

(1)

as n → ∞ for all functions h ∶ X → R in a speciﬁed set H.
Throughout it will be assumed that P admits a density p,
with respect to a reference measure, available in a form that
is un-normalised (i.e., we know q(x) in closed form where
p(x) = q(x)/C for some C > 0). Such problems occur
in Bayesian statistics where P represents a posterior distri-
bution, and the integral represents a posterior expectation
of interest. Markov chain Monte Carlo (MCMC) methods
are extensively used for this task but suffer (in terms of

1School of Mathematical and Physical Sciences, University of
Technology Sydney, Australia 2Microsoft Research New England,
USA 3Opendoor Labs, Inc., USA 4Department of Statistics, Uni-
versity of Warwick, UK 5Department of Mathematics, Imperial
College London, UK 6Alan Turing Institute, UK 7School of Math-
ematics, Statistics and Physics, Newcastle University, UK. Cor-
respondence to: Wilson Ye Chen <ye.chen@uts.edu.au>, Lester
Mackey <lmackey@microsoft.com>.

accuracy) from ‘clustering’ of the points {xi}n
i=1 when n is
small. This observation motivates us to instead consider a
range of goal-oriented discrete approximation methods that
are designed with un-normalised densities in mind.

The problem of discrete approximation of a distribution,
given its normalised density, has been considered in detail
and relevant methods include quasi-Monte Carlo (QMC)
(Dick & Pillichshammer, 2010), kernel herding (Chen et al.,
2010; Lacoste-Julien et al., 2015), support points (Mak &
Joseph, 2016; 2017), transport maps (Marzouk et al., 2016),
and minimum energy methods (Johnson et al., 1990). On
the other hand, the question of how to proceed with un-
normalised densities has been primarily answered with in-
creasingly sophisticated MCMC.

At the same time, recent work had led to theoretically-
justiﬁed measures of sample quality in the case of an un-
normalised target. In (Gorham & Mackey, 2015; Mackey
& Gorham, 2016) it was shown that Stein’s method can be
used to construct discrepancy measures that control weak
convergence of an empirical measure to a target. This was
later extended in (Gorham & Mackey, 2017) to encompass
a family of discrepancy measures indexed by a reproducing
kernel. In the latter case, the discrepancy measure can be
recognised as a maximum mean discrepancy (Smola et al.,
2007). As such, one can consider discrete approximation as
an optimisation problem in a Hilbert space and attempt to
optimise this objective with either a greedy or a conditional
gradient method. The resulting method – Stein Points – and
its variants are proposed and studied in this work.

Our Contribution This paper makes the following con-
tributions:

• Two algorithms are proposed for minimisation of the
kernel Stein discrepancy (KSD; Chwialkowski et al.,
2016; Liu et al., 2016; Gorham & Mackey, 2017); a
greedy algorithm and a conditional gradient method.
In each case, a convergence result of the form in Eqn.
1 is established.

• Novel kernels are proposed for the KSD, and we prove
that, with these kernels, the KSD controls weak conver-
gence of the empirical measure to the target. In other
words, the test functions h for which our results hold
constitute a rich set H.

Stein Points

Outline The paper proceeds as follows. In Section 2 we
provide background, and in Section 3 we present the ap-
proximation methods that will be studied. Section 4 applies
these methods to both simulated and real approximation
problems and provides a extensive empirical comparison.
All technical material is contained in Section 5, where we
derive novel theoretical results for the methods we proposed.
Finally we summarise our ﬁndings in Section 6.

2. Background

Throughout this section it will be assumed that X is a metric
space, and we let P(X) denote the collection of Borel
distributions on X. In this context, weak convergence of the
empirical measure to P corresponds to taking the set H in
Eqn. 1 to be the set HCB of functions which are continuous
and bounded. In this work we also consider sets H that
correspond to stronger modes of convergence in P(X).

First, in 2.1, we recall how discrepancy measures are con-
structed. Then we recall the use of Stein’s method in this
context in 2.2. Formulae for KSD are presented in 2.3.

2.1. Discrepancy Measures

A discrepancy is a quantiﬁcation of how well the points
{xi}n
i=1 cover the domain X with respect to the distribution
P . This framework will be developed below in reproducing
kernel Hilbert spaces (RKHS; Hickernell, 1998), but the
general theory of discrepancy can be found in (Dick &
Pillichshammer, 2010). Note that we focus on unweighted
point sets for ease of presentation, but our discussions and
results generalise straightforwardly to point sets that are
weighted.
Let k ∶ X × X → R be the reproducing kernel of a RKHS K
of functions X → R. That is, K is a Hilbert space of func-
tions with inner product ⟨⋅, ⋅⟩K and induced norm ∥ ⋅ ∥K such
that, for all x ∈ X, k(x, ⋅) ∈ K and f (x) = ⟨f, k(x, ⋅)⟩
K
whenever f ∈ K. The Cauchy-Schwarz inequality in K
gives that

∣ 1
n

∑n

i=1 f (xi) − ∫ f dP ∣ ≤ ∥f ∥K DK,P ({xi}n
i=1

)

where the ﬁnal term

DK,P ({xi}n
i=1

) ∶= ∥ 1
n

∑n

i=1 k(xi, ⋅) − ∫ k(x, ⋅)dP (x)∥

K

is the canonical discrepancy measure for the RKHS. The
Bochner integral kP ∶= ∫ k(x, ⋅)dP (x) ∈ K is known as the
mean embedding of P into K (Smola et al., 2007). Thus, if
H = B(K) ∶= {f ∈ K ∶ ∥f ∥K ≤ 1} is the unit ball in K, then
DK,P ({xi}n
) → 0 implies the convergence result in Eqn.
i=1
1.

the fact that, when both kP and kP,P ∶= ∫ kP dP are explicit,
the canonical discrepancy measure is also explicit:

) =

DK,P ({xi}n
i=1
kP,P − 2
n

√

∑n

i=1 kP (xi) + 1

n2 ∑n

i,j=1 k(xi, xj)

(2)

Table 1 in (Briol et al., 2015) collates pairs (k, P ) for which
kP and kP,P are explicit.

If P is a posterior distribution, so that p has unknown nor-
malisation constant, it is unclear how the terms kP and kP,P
can be computed in closed form, and so similarly for the
discrepancy DK,P . This has so far prevented QMC and
related methods such as kernel herding (Chen et al., 2010)
from being used to compute posterior integrals. A solution
to this problem can be found in Stein’s method, presented
next.

2.2. Kernel Stein Discrepancy

The method of Stein (1972) was introduced as an analytical
tool for establishing convergence in distribution of random
variables, but its potential for generating and analyzing com-
putable discrepancies was developed in (Gorham & Mackey,
2015). In what follows, we recall the kernelised version
of the Stein discrepancy, ﬁrst presented for an optimally-
weighted point set in 2.3.3 of (Oates et al., 2017b) and
later generalised to an arbitrarily-weighted point set in
(Chwialkowski et al., 2016; Liu et al., 2016; Gorham &
Mackey, 2017).

Suppose that X carries the structure of a smooth manifold,
and consider a linear differential operator TP on X, together
with a set F of sufﬁciently differentiable functions, with the
following property:

∫ TP [f ] dP = 0 ∀f ∈ F.

(3)

j=1

∥fj∥2
K

Then TP is called a Stein operator and F a Stein set. In
the kernelised version of Stein’s method, the set F is ei-
ther an RKHS K with reproducing kernel k ∶ X × X → R,
or the product Kd, which contains vector-valued functions
f = (f1, . . . , fd) with fj ∈ K and is equipped with a norm1
∥f ∥Kd = (∑d
)1/2. For the case F = K, the im-
age of K under a Stein operator TP is denoted K0 = TP K.
The notation can be justiﬁed since, under appropriate regu-
larity assumptions, the set TP K admits structure from the
reproducing kernel k0(x, x′) = TP TP k(x, x′) (Oates et al.,
2017b). Here TP is the adjoint of the operator TP and acts
on the second argument x′ of the kernel. If instead F = Kd,
then we suppose that TP f = ∑d
TP,jfj so that the set
K0 = TP Kd admits structure from the reproducing kernel
k0(x, x′) = ∑d
TP,jTP,jk(x, x′). In either case, we will

j=1

j=1

The RKHS framework is now standard for QMC analysis
(Dick & Pillichshammer, 2010). Its popularity derives from

1For what follows, any vector norm can be used to combine the

component norms ∥fj∥K (Gorham & Mackey, 2017, Prop. 3).

Stein Points

which at each iteration attempts to minimise the KSD, whilst
the second is a conditional gradient algorithm, known as
herding, which also targets the KSD. In 3.1 and 3.2 the
two algorithms are described, whilst in 3.3 some alternative
approaches are brieﬂy discussed.

√

3.1. Greedy Algorithm

call the reproducing kernel k0 of K0 a Stein reproducing
kernel.

Stein reproducing kernels possess the useful property that
k0,P = ∫ k0(x, ⋅)dP = 0 and k0,P,P = ∫ k0,P dP = 0, so in
particular both are explicit. Thus, if k0 is a Stein reproduc-
ing kernel, then Eqn. 2 can be simpliﬁed:

DK0,P ({xi}n
i=1

) =

1

n2 ∑n

i,j=1 k0(xi, xj).

(4)

We call this quantity a kernel Stein discrepancy (KSD). Next,
we exhibit some differential operators for which Eqn. 3 is
satisﬁed and Eqn. 4 can be computed.

2.3. Stein Operators and Their Reproducing Kernels

The divergence theorem can be used to construct Stein op-
erators on a manifold. For P supported on X = Rd, (Oates
et al., 2017b; Gorham & Mackey, 2015; Chwialkowski et al.,
2016; Liu et al., 2016; Gorham & Mackey, 2017) considered
the Langevin Stein operator

TP f

∶= ∇⋅(pf )

p

where ∇⋅ is the usual divergence operator and f ∈ Kd. Thus,
for the Langevin Stein operator, we obtain a Stein reproduc-
ing kernel

k0(x, x′) = ∇x ⋅ ∇x′ k(x, x′)

(5)

(6)

+∇xk(x, x′) ⋅ ∇x′ log p(x′)
+∇x′k(x, x′) ⋅ ∇x log p(x)
+k(x, x′)∇x log p(x) ⋅ ∇x′ log p(x′).

To evaluate this kernel, the normalisation constant for p is
not required. Other Stein operators for the Euclidean case
were developed in (Gorham et al., 2016). For P supported
on a closed Riemannian manifold X, (Oates et al., 2017a;
Liu & Zhu, 2017) proposed the second order Stein operator
TP f ∶= 1
∇ ⋅ (p∇f ) where ∇ and ∇⋅ are, respectively, the
p
gradient and divergence operators on the manifold and f ∈
K. Other Stein operators for the general case are proposed
in the supplement of (Oates et al., 2017a).

The theoretical results in (Gorham & Mackey, 2017) estab-
lished that certain combinations of Stein operator TP and
base kernel k ensure that KSD controls weak convergence;
that is, DK0,P ({xi}n
) → 0 implies that Eqn. 1 holds with
i=1
H = HCB. This important result motivates our next con-
tribution, where numerical optimisation methods are used
to select points {xi}n
i=1 to approximately minimise KSD.
Theoretical analysis of the proposed methods is reserved for
Section 5.

3. Methods

In this paper, two algorithms to select points {xi}n
i=1 are
studied in detail. The ﬁrst of these is a greedy algorithm,

The simplest algorithm that we consider follows a greedy
strategy, whereby the ﬁrst point x1 is taken to be a global
maximum of p (an operation which does not require the nor-
malisation constant) and each subsequent point xn is taken
to be a global maximum of DK0,P ({xi}n
), with the KSD
i=1
being viewed as a function of xn holding {xi}n−1
i=1 ﬁxed.
Equivalently, at iteration n > 1 of the greedy algorithm, we
select

xn

∈ arg minx∈X

k0(x,x)
2

+ ∑n−1

i=1 k0(xi, x).

(7)

Note that each iteration of the algorithm requires the solution
of a global optimisation problem over X; in practice we
employed a numerical optimisation method, and this choice
is discussed in detail in connection with the empirical results
in Section 4 and the theoretical results in Section 5.

If a user has a budget of at most n points, the greedy algo-
rithm can be run for n iterations and thereafter improved
using (block) coordinate descent on the KSD objective to
update an existing point xi instead of introducing a new
point. The cost of each update is equal to the cost of adding
the n-th greedy Stein Point. This budget-constrained variant
of the method will be called Stein Greedy-n in the sequel
(see Section B.1.3 for more details).

3.2. Herding Algorithm

The deﬁnition of discrepancy in Section 2.1 suggests that
selection of {xi}n
i=1 can be elegantly formulated as a single
global optimisation problem over K0. Let M (K0) be the
marginal polytope of K0; i.e.
the convex hull of the set
{k0(x, ⋅)}x∈X (Wainwright & Jordan, 2008). The mean
embedding Q ↦ kQ, as a map P(X) → M (K), is injective
whenever the kernel k is universal and X is compact (Smola
et al., 2007), so that in this case kQ fully characterises Q.
Results in a similar direction for Stein reproducing kernels
were established in Chwialkowski et al. (2016, Theorem 2.1)
and Liu et al. (2016, Proposition 3.3). Thus, as P is mapped
to 0 under the embedding, we are motivated to consider
non-trivial solutions to

arg minf ∈M (K0) J(f ),

J(f ) ∶= 1
2

∥f ∥2
K0

.

(8)

As might be expected, the objective function is closely re-
∑n
i=1 k0(xi, ⋅) we have J(f ) =
lated to KSD; for f (⋅) = 1
n
2 DK0,P ({xi}n
)2. An iterative algorithm, called kernel
1
herding, was proposed in (Chen et al., 2010) to solve prob-
lems in the form of Eqn. 8. This was later shown to be

i=1

Stein Points

equivalent to a conditional gradient algorithm, the Frank-
Wolfe algorithm, in (Bach et al., 2012). The canonical Frank-
Wolfe algorithm, which results in an unweighted point set
(as opposed to a more general weighted point set; Bach
et al., 2012), is presented next.

of our approach would be to optimise KSD for n ﬁxed. This
approach was considered for other discrepancy measures in
(Oettershagen, 2017), where the Newton method was used.
We instead employ our budget-constrained algorithms Stein
Greedy-n and Stein Herding-n for this use case.

The ﬁrst point x1 is again taken to be a global maximum of
p; this corresponds to an element f1 = k0(x1, ⋅) ∈ M (K0).
Then, at iteration n > 1, the convex combination fn =
n fn−1 + 1
∈ M (K0) is constructed where the element
n−1
f ∗
n encodes a direction of steepest descent:

n f ∗

n

fn

∈ arg minf ∈M (K0)

⟨f, DJ(fn−1)⟩

,

K0

where DJ(f ) is the representer of the Fr´echet derivative of
J at f . Given that minimisation of a linear objective over
a convex set can be restricted to the boundary of that set,
= k(xn, ⋅) for some xn ∈ X. Thus, at
it follows that f ∗
n
iteration n > 1 of the proposed algorithm, we select

xn

∈ arg minx∈X ∑n−1

i=1 k0(xi, x)

(9)

∑n

to obtain fn(⋅) = 1
i=1 k0(xi, ⋅), the embedding of the
n
empirical distribution of {xi}n
i=1. As in the standard kernel
herding algorithm of (Chen et al., 2010), each iteration
in practice requires the solution of a global optimisation
problem over X.

Compared to Eqn. 7, the greedy algorithm is seen to be a
2 k0(x, x).
regularised version of herding with regulariser 1
The two algorithms coincide if k0(x, x) is independent of
x; however, this is typically not true for a Stein reproducing
kernel. The computational cost of either method is O(n2);
thus we anticipate applications in which evaluation of p(x)
(and its gradient) constitute the principal computational
bottleneck. The performance of both algorithms is studied
empirically in Section 4 and theoretically in Section 5. In
a similar manner to Stein Greedy-n, a budget-constrained
variant of the above method can be considered, which we
call Stein Herding-n in the sequel.

3.3. Other Algorithms

The output of either of our algorithms will be called Stein
Points. These are extensible point sequence Sn = (xi)n
i=1,
meaning that Sn can be incrementally extended Sn =
(Sn−1, xn) as required. Another recently proposed exten-
sible method is the (sequential) minimum energy design
(MED) of (Joseph et al., 2015; 2017), here used as a bench-
mark.

4. Results

In this section, the proposed greedy and herding algorithms
are empirically assessed and compared. In 4.2 a Gaussian
mixture problem is studied in detail, whilst in 4.3 and 4.4,
respectively, the methods are applied to approximate the
parameter posterior in a non-parametric regression model
and an IGARCH model. First, in 4.1 we provide details on
the experimental protocol.

4.1. Experimental Protocol

Here we describe the parameters and settings that were
varied in the experiments that are presented.

Stein Operator To limit scope, we focus on the case X =
Rd and always take TP to be the Langevin Stein operator in
Eqn. 5.

Choice of Kernel For the kernel k in Eqn. 6 we con-
sidered one standard choice – the inverse multi-quadratic
(IMQ) kernel – together with two novel alternatives:

(k1) (IMQ) k1(x, x′) = (α + ∥x − x′∥2
2

)β

(k2) (inverse log) k2(x, x′) = (α + log(1 + ∥x − x′∥2
2

))−1

(k3) (IMQ score)

k3(x, x′) = (α + ∥∇ log p(x) − ∇ log p(x′)∥2
2

)β

.

In all cases α > 0 and β ∈ (−1, 0). To limit scope, in
what follows we considered a ﬁnite number of judiciously
selected conﬁgurations for α, β, though in principle these
could be optimised as in (Jitkrittum et al., 2017). The best
set of parameter values was selected for each algorithm and
each target distribution, where the possible values were α ∈
{0.1η, 0.5η, η, 2η, 4η, 8η} and β ∈ {0.1, 0.3, 0.5, 0.7, 0.9},
with η > 0 problem-dependent (see the Supplement). The
IMQ kernel, together with the Langevin Stein operator, was
proven in Gorham & Mackey (2017, Theorem 8) to provide
a KSD that controls weak convergence. Similar results for
novel kernels k2 and k3 are established in Section 5.

For some problems the number of points n will be ﬁxed
in advance and the aim will instead be to select a single
optimal point set {xi}n
i=1. This alternative problem demands
different methodologies, and a promising method in this
direction is Stein variational gradient descent (SVGD-n;
Liu & Wang, 2016; Liu, 2017). A natural point set analogue

Numerical Optimisation Method Any optimisation pro-
cedure could be used to (approximately) solve the global
optimisation problem embedded in each iteration of the pro-
posed algorithms. In our experiments, we considered the
following numerical methods, for which full details appear
in the Section B.2.

Stein Points

1. Nelder-Mead (NM): At iteration n, parallel runs of
Nelder-Mead were employed, initialised at draws from
a Gaussian mixture proposal centred on the current
point set Π =
N (xi, λI) with problem-
speciﬁc λ > 0.

∑n−1
i=1

1
n−1

2. Monte Carlo (MC): The optimisation problem at itera-
tion n was solved over a sample of points drawn from
the same Gaussian mixture proposal Π.

3. Grid search (GS): Through brute force, the optimisa-
tion problem at iteration n was solved over a regular
2 ) points; if
grid of width 1√
required, the domain was ﬁrst truncated with a large
bounding box.

n . This required O(n− d

Performance Assessment To obtain a reasonably objec-
tive assessment, we focused on the 1-Wasserstein distance
between the empirical measure and P :

WP ({xi}n
i=1

) = suph∈HLip

∣ 1
n

∑n

i=1 h(xi) − ∫ hdP ∣ ,

i=1 δyi for yi

2 log N rate for d = 2 and N − 1

where HLip is the set of all function h ∶ X → R with Lip-
schitz constant Lip(h) ≤ 1. By replacing P with the em-
iid∼ P , the expected
∑N
pirical measure PN = 1
N
)
) in lieu of WP ({xi}n
({xi}n
error from using WPN
i=1
i=1
converges at a N − 1
d rate
for d > 2 (Fournier & Guillin, 2015). By employing L1-
) can be com-
spanners, the approximation WPN
puted in O((n + N )2 log2d−1(n + N )) time (Gudmundsson
et al., 2007). For all reported results, the {yi}N
i=1 were ob-
tained by brute-force Monte Carlo methods applied to P ,
with N sufﬁciently large that approximation error can be
neglected.

({xi}n
i=1

The computational cost associated to any given method was
quantiﬁed as the total number neval of times either the log-
density log p or its gradient ∇ log p were evaluated. This
can be justiﬁed since in most applications the ‘parameter to
data’ map dominates the computational cost associated with
the likelihood.

Benchmarks Two existing methods were used as a bench-
mark:

1. The MED method of (Joseph et al., 2015; 2017) relies
on numerical optimisation methods to minimise an
), adapted to P . This
energy measure Eδ,P ({xi}n
i=1
measure has one tuning parameter δ ∈ [1, ∞). See
Section B.1.1 of the Supplement for full detail.

2. The SVGD method of (Liu & Wang, 2016; Liu, 2017)
performs a version of gradient descent on the Kullback-
Leibler divergence, described in Section B.1.2 of the
Supplement.

To avoid confounding of the empirical results by incompa-
rable algorithm parameters, (1) the collection of numerical
optimisation methods used for KSD were also used for
MED, and (2) the same collection of kernels k1, . . . , k3 was
considered for SVGD as was used for KSD. Note that, apart
from standard Monte Carlo, none of the methods considered
in these experiments are re-parametrisation invariant.

4.2. Gaussian Mixture Test

For our ﬁrst test, we considered a Gaussian mixture model

P =

1
2

N (µ1, Σ1) + 1
2

N (µ2, Σ2)

deﬁned on X = R2. Full settings for each of the methods
considered are detailed in Section C.1 in the Supplement.
Typical point sets are displayed over the contours of P for
µ1 = (−1.5, 0), µ2 = (1.5, 0), Σ1 = Σ2 = I in Figure 1. Ad-
ditionally, point sets for the n point budget-constrained al-
gorithms Stein Greedy-n and Stein Herding-n are presented
in Figure 6 in the Supplement. For each of the methods
shown in Figures 1 and 6, tuning parameters were varied
and the overall performance was captured in Figure 2. It was
observed that for (a-c) the choice of numerical optimisation
method was the most inﬂuential tuning parameter, with the
simpler Monte Carlo-based method being most successful.
The kernels k1, k2 were seen to perform well, but in (a,b,d)
the kernel k3 was sometimes seen to fail.

A subjectively-selected exemplar was extracted for each
method, and these ‘best’ results for each method are overlaid
in Figure 3. The total number of points was limited to
n = 100. In terms of our proposed methods, two qualitative
regimes were observed: (i) For low computational budget
log neval ≤ 7, the standard Monte Carlo method performed
best. (ii) For a larger computational budget 7 < log neval,
greedy Stein points were not out-performed.

Note that KSD and SVGD are based on the log target and
its gradient, whilst for MED the target p(x) itself is re-
quired. As a result, numerical instabilities were sometimes
encountered with MED.

Next, we turned our attention to two important posterior
approximation problems that occur in the real world.

4.3. Gaussian Process Regression Model

The Gaussian process (GP) model is a popular choice
for uncertainty quantiﬁcation in the non-parametric regres-
sion context (Rasmussen & Williams, 2006). The data
D = {(xi, yi)}n
i=1 that we considered are from a light de-
tection and ranging (LIDAR) experiment (Ruppert et al.,
2003). They consist of 221 realisations of an independent
scalar variable xi (distances travelled before the light is
reﬂected back to its source) and a dependent scalar vari-
able yi (log-ratios of received light from two laser sources);

Stein Points

(a) Stein Points (Greedy)

(b) Stein Points (Herding)

(c) MED

(d) SVGD

Figure 2: Results for the Gaussian mixture test.
[Here
n = 100. x-axis: log of the number neval of model evalu-
ations that were used. y-axis: log of the Wasserstein dis-
) obtained. Kernel parameters α, β were
tance WP ({xi}n
i=1
optimised according to WP in all cases, with sensitivities
reported in Fig. 7 of the Supplement.]

Figure 1: Typical point sets obtained in the Gaussian mix-
ture test. [Here the left border of each sub-plot is aligned to
the exact value of log neval spent to obtain each point set.]

i.i.d.∼ N (0, σ2)
these were modelled as yi = g(xi) + (cid:15)i, for (cid:15)i
and a known value of σ. The unknown regression function
g is modelled as a centred GP with covariance function
cov(x, x′) = θ1 exp(−θ2(x − x′)2). The hyper-parameters
θ1, θ2 > 0 determine the suitability of the GP model, but
appropriate values will be unknown in general. In this ex-
periment we re-parametrised φi = log θi and placed a stan-
dard multivariate Cauchy prior on φ = (φ1, φ2), deﬁned
on X = R2. The task is thus to approximate the condi-
tional distribution p(φ∣D). This problem is motivated by
the computation of posterior predictive marginal distribu-
tions p(y∗∣x∗, D) for a new input x∗, which is deﬁned as
the integral ∫ p(y∗∣x∗, φ, D)p(φ∣D)dφ. Note that the den-
sity p(φ∣D) can be differentiated, and an explicit formula is
provided in Rasmussen & Williams (2006, Eqn. 5.9).

For each class of method, ‘best’ tuning parameters were se-
lected and these are presented on the same plot in Figure 4a.
In addition, typical point sets provided by each method are
presented in Figures 8 and 9 in the Supplement. MED was
not included because the method exhibited severe numerical
instability on this task, as earlier discussed. Results indi-
cated three qualitative regimes where, respectively, Monte
Carlo, greedy Stein points and SVGD provided the best

Figure 3: Combined results for the Gaussian mixture test.
[Here n = 100. x-axis: log of the number neval of model
evaluations that were used. y-axis: log of the the Wasser-
stein distance WP ({xi}n
) obtained. Tuning parameters
i=1
were selected to minimise WP , as described in the main text.
The dashed line indicates the point at which n Stein Points
have been generated; block coordinate descent is performed
thereafter to satisfy the n point budget constraint.]

performance for ﬁxed cost.

4.4. IGARCH Model

The integrated generalised autoregressive conditional het-
eroskedasticity (IGARCH) model is widely-used to describe

Stein Points

ﬁnancial time series (yt) with time-varying volatility (σt)
(Taylor, 2011). The model is as follows:

yt = σt(cid:15)t,
σ2
t

= θ1 + θ2y2

i.i.d.∼ N (0, 1)

(cid:15)t

+ (1 − θ2)σ2

t−1

t−1
with parameters θ = (θ1, θ2), θ1 > 0 and 0 < θ2 < 1. The
data y = (yt) that we considered were 2,000 daily percent-
age returns of the S&P 500 stock index (from December 6,
2005 to November 14, 2013), and an improper uniform prior
was placed on θ. Thus the task was to approximate the pos-
terior p(θ∣y). Note that, whilst the domain X = R+ × (0, 1)
is bounded, for these data the posterior density is negligi-
ble on the boundary ∂X. This ensures that Eqn. 3 holds
essentially to machine precision; see also the discussion in
Oates et al. (2018, Section 3.2). For the IGARCH model,
gradients ∇ log p(θ∣y) can be obtained as the solution of a
recursive system of equations for ∂σt/∂θ2.

As before, the ‘best’ performing of each class of method was
selected and these are presented on the same plot in Figure
4b. In addition, typical point sets provided by each method
are presented in Figures 12 and 13 in the Supplement. (Nu-
merical instability again prevented results for MED from
being obtained.) Results were consistent with the Gaussian
mixture experiment, favouring either Monte Carlo or greedy
Stein points depending on the computational budget.

5. Theoretical Results

(1) discrepancy control,

In this section we establish two important forms of
i.e.,
theoretical guarantees:
DK0,P ({xi}n
) → 0 as n → ∞ for our extensible Stein
i=1
Point sequences and (2) distributional convergence control,
i.e., for our kernel choices and appropriate choices of target,
) → 0 implies that the empirical distribution
DK0,P ({xi}n
i=1
1
n

i=1 δxi converges in distribution to P .

∑n

5.1. Discrepancy Control

Earlier work has shown that, when a kernel is uniformly
bounded (i.e., supx∈X k0(x, x) ≤ R2), the greedy and ker-
nel herding algorithms decrease the associated discrepancy
DK0,P at an O(n− 1
2 ) rate (Lacoste-Julien et al., 2015; Jones,
1992). We extend these results to cover all growing, P -sub-
exponential kernels.
Deﬁnition 1 (P -sub-exponential reproducing kernel). We
say a reproducing kernel k0 is P -sub-exponential if

PZ∼P [k0(Z, Z) ≥ t] ≤ c1e−c2t

for some constants c1, c2 > 0 and all t ≥ 0.

Notably, any uniformly bounded reproducing kernel is
P -sub-exponential, and, when P is a sub-Gaussian dis-
tribution, any kernel with at most quadratic growth (i.e.,

k0(x, x) = O(∥x∥2
)) is also P -sub-exponential. Our ﬁrst
2
result, proved in Section A.1.1, shows that if we truncate
the search domain suitably in each step, Stein Herding de-
creases the discrepancy at an O(
log(n)/n) rate. This
result holds even if each point xi is selected suboptimally
with error δ/2. This extra degree of freedom allows a user
to conduct a grid search or a search over appropriately gen-
erated random points on each step (see, e.g., Lacoste-Julien
et al., 2015) and still obtain a rate of convergence.

√

Theorem 1 (Stein Herding Convergence). Suppose k0 with
k0,P = 0 is a P -sub-exponential reproducing kernel. Then
there exist constants c1, c2 > 0 depending only on k0 and P
such that any point sequence {xi}n

i=1 satisfying

∑j−1

i=1 k0(xi, xj) ≤ δ

2

+

min
x∈X∶k0(x,x)≤R2
j

∑j−1

i=1 k0(xi, x)

with k0(xj, xj) ≤ R2
j
1 ≤ j ≤ n also satisﬁes

∈ [2 log(j)/c2, 2 log(n)/c2] for each

DK0,P ({xi}n
i=1

) ≤ eπ/2

2 log(n)
c2n

+ c1
n

+ δ
n .

√

√

Our next result, proved in Section A.1.2, shows that Stein
Greedy decreases the discrepancy at an O(
log(n)/n) rate
whether we choose to truncate (Rj < ∞) or not (Rj = ∞).
This highlights an advantage of the Stein Greedy algorithm
the extra k0(x, x)/2 term acts as a
over Stein Herding:
regularizer ensuring that no truncation is necessary. The
result also accommodates points xi selected suboptimally
with error δ/2.
Theorem 2 (Stein Greedy Convergence). Suppose k0 with
k0,P = 0 is a P -sub-exponential reproducing kernel. Then
there exist constants c1, c2 > 0 depending only on k0 and P
such that any point sequence {xi}n

i=1 satisfying

k0(xj ,xj )
2

+ ∑j−1

i=1 k0(xi, xj)
+

min
x∈X∶k0(x,x)≤R2
j

≤ δ
2

k0(x,x)
2

+ ∑j−1

i=1 k0(xi, x)

2 log(j)/c2 ≤ Rj ≤ ∞ for each 1 ≤ j ≤ n also

√

with
satisﬁes

DK0,P ({xi}n
i=1

) ≤ eπ/2

2 log(n)
c2n

+ c1
n

+ δ
n .

√

5.2. Distributional Convergence Control

To present our ﬁnal results, we overload notation to deﬁne
the KSD associated with any probability measure µ:
√

DK0,P (µ) =

E(Z,Z′)∼µ×µ [k0(Z, Z ′)].

Our original DK0,P deﬁnition (Eq. 4) for a point set {xi}n
i=1
is recovered when µ is the empirical measure 1
i=1 δxi.
n

∑n

Stein Points

(a) Gaussian Process Test

(b) IGARCH Test

Figure 4: Combined results for the (a) Gaussian process test and (b) IGARCH test. [Here n = 100. x-axis: log of the
number neval of model evaluations that were used. y-axis: log of the Wasserstein distance WP ({xi}n
) obtained. Tuning
i=1
parameters were selected to minimise WP , as described in the main text. The dashed line indicates the point at which n
Stein Points have been generated; block coordinate descent is performed thereafter to satisfy the n point budget constraint.]

We also write µm ⇒ P to indicate that a sequence of proba-
bility measures (µm)∞

m=1 converges in distribution to P .

Gorham & Mackey (2017, Thm. 8) showed that KSDs
with IMQ base kernel (k1) and Langevin Stein opera-
tor TP control distributional convergence whenever P be-
longs to the set P of distantly dissipative distributions
(i.e., ⟨∇ log p(x) − ∇ log p(y), x − y⟩ ≤ −κ ∥x − y∥2
+C for
2
some C ≥ 0, κ > 0) with Lipschitz ∇ log p. Surprisingly,
Gaussian, Mat´ern, and other kernels with light tails do not
satisfy this property (Gorham & Mackey, 2017, Thm. 6).

Our next theorem establishes distributional convergence
control for our newly introduced log inverse kernel (k2).
Theorem 3 (Log Inverse KSD Controls Convergence).
Suppose P ∈ P. Consider a Stein reproducing kernel
k0 = TP TP k2 with Langevin operator TP and base ker-
nel k2(x, x′) = (α + log(1 + ∥x − x′∥2
))β for α > 0 and
2
β < 0. If DK0,P (µm) → 0, then µm ⇒ P .

Our ﬁnal theorem, proved in Section A.3, guarantees distri-
butional convergence control for the new IMQ score kernel
(k3) under the additional assumption that log p is strictly
concave.

Theorem 4 (IMQ Score KSD Controls Convergence). Sup-
pose P ∈ P has strictly concave log density. Con-
sider a Stein reproducing kernel k0 = TP TP k3 with
Langevin operator TP and base kernel k3(x, x′) = (c2 +
∥∇ log p(x) − ∇ log p(x′)∥2
)β for c > 0 and β ∈ (−1, 0). If
2
DK0,P (µm) → 0, then µm ⇒ P .

6. Conclusion

This paper proposed and studied Stein Points, extensible
point sequences rooted in minimisation of a KSD, build-

ing on the recent theoretical work of (Gorham & Mackey,
2017). Although we focused on KSD to limit scope, our
methods could in fact be applied to any computable Stein
discrepancy, even those not based on reproducing kernels
(see, e.g., Gorham & Mackey, 2015; Gorham et al., 2016).
Stein Points provide an interesting counterpoint to other re-
cent work focussing on point sequences (Joseph et al., 2015;
2017) and point sets (Liu & Wang, 2016; Liu, 2017). More-
over, when X is a ﬁnite set {yi}N
i=1 (e.g., an inexpensive
initial point set generated by MCMC), Stein Points provide
a compact and convergent approximation to the optimally
weighted probability measure ∑N
i=1 wiδyi with minimum
KSD (see Section B.3 for more details).

Theoretical results were provided which guarantee the
asymptotic correctness of our methods. However, we were
√
only able to establish an O(
log(n)/n) rate, which leaves
a theoretical gap between the faster convergence that was
sometimes empirically observed. Relatedly, the O(n2) com-
putational cost could be reduced to O(n) by using ﬁnite-
dimensional kernels (see, e.g., Jitkrittum et al., 2017), but
the associated distributional convergence control results
must ﬁrst be developed.

Our experiments were relatively comprehensive, but we did
not consider other Stein operators, nor higher-dimensional
or non-Euclidean manifolds X. Related methods not consid-
ered in this work include those based on optimal transport
(Marzouk et al., 2016) and self-avoiding particle-based sam-
plers (Robert & Mengersen, 2003). The comparison against
these methods is left for future work.

Stein Points

Acknowledgements

WYC was supported by the ARC Centre of Excellence
for Mathematical and Statistical Frontiers. FXB was sup-
ported by EPSRC [EP/L016710/1, EP/R018413/1]. CJO
was supported by the Lloyd’s Register Foundation pro-
gramme on data-centric engineering at the Alan Turing
Institute, UK. This material was based upon work partially
supported by the National Science Foundation under Grant
DMS-1127914 to the Statistical and Applied Mathematical
Sciences Institute. Any opinions, ﬁndings, and conclusions
or recommendations expressed in this material are those of
the author(s) and do not necessarily reﬂect the views of the
National Science Foundation.

References

Bach, F., Lacoste-Julien, S., and Obozinski, G. On the
equivalence between herding and conditional gradient
In Proceedings of the 29th International
algorithms.
Conference on Machine Learning, pp. 1355–1362, 2012.

Baker, J.

Integration of radial functions. Mathematics

Magazine, 72(5):392–395, 1999.

Briol, F-X, Oates, CJ, Girolami, M, Osborne, MA, and Se-
jdinovic, D. Probabilistic integration: A role in statistical
computation? arXiv:1512.00933, 2015.

Chen, Y., Welling, M., and Smola, A. Super-samples from
kernel herding. In Proceedings of the 26th Conference on
Uncertainty in Artiﬁcial Intelligence, 2010.

Chwialkowski, K., Strathmann, H., and Gretton, A. A kernel
test of goodness of ﬁt. In Proceedings of the 33rd Inter-
national Conference on Machine Learning, volume 48,
pp. 2606–2615, 2016.

Dick, J. and Pillichshammer, F. Digital Nets and Sequences.
Discrepancy Theory and Quasi-Monte Carlo Integration.
Cambridge University Press, 2010.

Fournier, N. and Guillin, A. On the rate of convergence in
Wasserstein distance of the empirical measure. Probabil-
ity Theory Related Fields, 162(3-4):707–738, 2015.

Gorham, J. and Mackey, L. Measuring sample quality with
Stein’s method. In Advances in Neural Information Pro-
cessing Systems, pp. 226–234, 2015.

Gorham, J. and Mackey, L. Measuring sample quality with
kernels. In Proceedings of the 34th International Confer-
ence on Machine Learning, pp. 1292–1301, 2017.

Gorham, J., Duncan, A.B., Vollmer, S.J., and Mackey,
Measuring sample quality with diffusions.

L.
arXiv:1611.06972, 2016.

Gudmundsson, J., Klein, O., Knauer, C., and Smid, M.
Small Manhattan networks and algorithmic applications
In Proceedings of the
for the earth movers distance.
23rd European Workshop on Computational Geometry,
pp. 174–177, 2007.

Hickernell, F. A generalized discrepancy and quadrature
error bound. Mathematics of Computation, 67(221):299–
322, 1998.

Jitkrittum, W., Xu, W., Szabo, Z., Fukumizu, K., and Gret-
ton, A. A linear-time kernel goodness-of-ﬁt test.
In
Advances in Neural Information Processing Systems, pp.
261–270, 2017.

Johnson, M.E., Moore, L.M., and Ylvisaker, D. Minimax
and maximin distance designs. Journal of Statistical
Planning and Inference, 26(2):131–148, 1990.

Jones, L.K. A simple lemma on greedy approximation
in Hilbert space and convergence rates for projection
pursuit regression and neural network training. Annals of
Statistics, 20(1):608–613, 1992.

Joseph, V.R., Dasgupta, T., Tuo, R., and Wu, C.F.J. Se-
quential exploration of complex surfaces using minimum
energy designs. Technometrics, 57(1):64–74, 2015.

Joseph, V.R., Wang, D., Gu, L., Lv, S., and Tuo, R. Deter-
ministic sampling of expensive posteriors using minimum
energy designs. arXiv:1712.08929, 2017.

Joshi, K.D. Introduction to General Topology. New Age

International, 1983.

Lacoste-Julien, S., Lindsten, F., and Bach, F. Sequential
kernel herding : Frank-Wolfe optimization for particle
ﬁltering. In Proceedings of the 18th International Confer-
ence on Artiﬁcial Intelligence and Statistics, pp. 544–552,
2015.

Liu, C. and Zhu, J. Riemannian Stein variational gradient
descent for Bayesian inference. arXiv:1711.11216, 2017.

Liu, Q. Stein variational gradient descent as gradient ﬂow.
In Advances in Neural Information Processing Systems,
pp. 3118–3126, 2017.

Liu, Q. and Wang, D. Stein variational gradient descent:
A general purpose Bayesian inference algorithm. In Ad-
vances In Neural Information Processing Systems, pp.
2370–2378, 2016.

Liu, Q., Lee, J., and Jordan, M. A kernelized Stein dis-
crepancy for goodness-of-ﬁt tests. In Proceedings of the
33rd International Conference on Machine Learning, pp.
276–284, 2016.

Mackey, L. and Gorham, J. Multivariate Stein factors for
a class of strongly log-concave distributions. Electronic
Communications in Probability, 21(56), 2016.

Spivak, M. Calculus on Manifolds: A Modern Approach
to Classical Theorems of Advanced Calculus. Westview
Press, 1965.

Stein Points

Stein, C. A bound for the error in the normal approxima-
tion to the distribution of a sum of dependent random
variables. In Proceedings of the Sixth Berkeley Sympo-
sium on Mathematical Statistics and Probability, Volume
2: Probability Theory. The Regents of the University of
California, 1972.

Steinwart, I. and Christmann, A. Support Vector Machines.

Springer Science & Business Media, 2008.

Taylor, S.J. Asset Price Dynamics, Volatility, and Prediction.

Princeton University Press, 2011.

Villani, C. Optimal Transport, Old and New. Number
338 in Fundamental Principles of Mathematical Sciences.
Springer-Verlag, 2009.

Wainwright, M. High-Dimensional Statistics: A Non-
Asymptotic Viewpoint. 2017. URL https://www.
stat.berkeley.edu/˜mjwain/stat210b/
Chap2_TailBounds_Jan22_2015.pdf.

Wainwright, M. J. and Jordan, M. I. Graphical models, ex-
ponential families, and variational inference. Foundations
and Trends in Machine Learning, 1(1–2):1–305, 2008.

Wendland, H. Scattered Data Approximation. Cambridge

University Press, 2004.

Mak, S and Joseph, V R. Support points. arXiv:1609.01811,

2016.

Mak, S and Joseph, V R. Projected support points, with ap-
plication to optimal MCMC reduction. arXiv:1708.06897,
2017.

Marzouk, Y., Moselhy, T., Parno, M., and Spantini, A. Hand-
book of Uncertainty Quantiﬁcation, chapter Sampling via
Measure Transport: An Introduction. Springer, 2016.

Nelder, J.A. and Mead, R. A simplex method for func-
tion minimization. The Computer Journal, 7(4):308–313,
1965.

Oates, C.J., Barp, A., and Girolami, M. Posterior integration
on a Riemannian manifold. arXiv:1712.01793, 2017a.

Oates, C.J., Girolami, M., and Chopin, N. Control func-
tionals for Monte Carlo integration. Journal of the Royal
Statistical Society: Series B, 79(3):695–718, 2017b.

Oates, C.J., Cockayne, J., Briol, F-X., and Girolami, M.
Convergence rates for a class of estimators based on
Stein’s identity. Bernoulli, 2018. To appear.

Oettershagen, J. Construction of Optimal Cubature Algo-
rithms with Applications to Econometrics and Uncer-
tainty Quantiﬁcation. PhD thesis, University of Bonn,
2017.

Rasmussen, C. and Williams, C. Gaussian Processes for

Machine Learning. MIT Press, 2006.

Robert, C.P. and Mengersen, K.L.

IID sampling with
self-avoiding particle ﬁlters: The pinball sampler.
In
Bayesian Statistics, volume 7, chapter IID sampling with
self-avoiding particle ﬁlters: The pinball sampler. Oxford
University Press, 2003. Eds. Bernardo, J., Bayarri, M.,
Berger, J., Dawid, A., Heckerman, D., Smith, A., West,
M.

Ruppert, D., Wand, M.P., and Carroll, R.J. Semiparametric
Regression. Number 12. Cambridge Series in Statistical
and Probabilistic Mathematics, 2003.

Sejdinovic, D., Sriperumbudur, B., Gretton, A., and Fuku-
mizu, K. Equivalence of distance-based and RKHS-based
statistics in hypothesis testing. Annals of Statistics, 41(5):
2263–2291, 2013.

Smola, A., Gretton, A., Song, L., and Sch¨olkopf, B. A
Hilbert space embedding for distributions. In Proceed-
ings of the 18th International Conference on Algorithmic
Learning Theory, pp. 13–31, 2007.

Stein Points

Supplement
This electronic supplement is organised as follows: In Section A proofs for the theoretical results in the main text are
provided. In Section B we provide details for the two existing methods (MED, SVGD) that formed our experimental
benchmark. Then, in Section C, we provide additional numerical results that elaborate on those reported in the main text.

Code Code to reproduce these experiments is available from:

github.com/wilson-ye-chen/stein_points

A. Proof of Theoretical Results in the Main Text

A.1. Proofs of Theorems 1 and 2: Stein Herding and Stein Greedy Convergence

We will show that both Theorem 1 and Theorem 2 follow from the following uniﬁed Stein Point convergence result, proved
in Section A.1.3.
Theorem 5 (Stein Point Convergence). Suppose k0 with k0,P = 0 is a P -sub-exponential reproducing kernel. Then there
exist constants c1, c2 > 0 depending only on k and P such that any point sequence {xi}n

i=1 satisfying

k0(xj, xj)
2

+

j−1
∑
i=1

k0(xi, xj) ≤ δ
2

+

S2
j
2

+

min
x∈X∶k0(x,x)≤S2
j

j−1
∑
i=1

k0(xi, x)

√

√

with Sj ∈ [

2 log(j)/c2,

2 log(n)/c2] for each 1 ≤ j ≤ n and δ ≥ 0 also satisﬁes

DK0,P ({xi}n
i=1

) ≤ eπ/2

2 log(n)
c2n

+ c1
n

+ δ
n .

√

A.1.1. PROOF OF THEOREM 1: STEIN HERDING CONVERGENCE

Instantiate the constants c1, c2 > 0 from Theorem 5, and consider any point sequence {xi}n

i=1 satisfying

j−1
∑
i=1

k0(xi, xj) ≤ δ
2

+

min
x∈X∶k0(x,x)≤R2
j

∑j−1

i=1 k0(xi, x)

with k0(xj, xj) ≤ R2
j

∈ [2 log(j)/c2, 2 log(n)/c2]. We immediately have

k0(xj, xj)
2

+

j−1
∑
i=1

k0(xi, xj) ≤ δ
2

+

R2
j
2

+

min
x∈X∶k0(x,x)≤R2
j

j−1
∑
i=1

k0(xi, x)

so the desired conclusion follows from Theorem 5.

A.1.2. PROOF OF THEOREM 2: STEIN GREEDY CONVERGENCE

Instantiate the constants c1, c2 > 0 from Theorem 5, and consider any point sequence {xi}n

i=1 satisfying

k0(xj, xj)
2

+

j−1
∑
i=1

k0(xi, xj) ≤ δ
2

+

min
x∈X∶k0(x,x)≤R2
j

k0(x, x)
2

+

j−1
∑
i=1

k0(xi, x)

√

with Sj =

2 log(j)/c2 ≤ Rj ≤ ∞ for each 1 ≤ j ≤ n. We immediately have

k0(xj, xj)
2

+

j−1
∑
i=1

k0(xi, xj) ≤ δ
2

+

min
x∈X∶k0(x,x)≤S2
j

k0(x, x)
2

+

j−1
∑
i=1

k0(xi, x) ≤ δ
2

+

S2
j
2

+

min
x∈X∶k0(x,x)≤S2
j

j−1
∑
i=1

k0(xi, x),

so the desired conclusion follows from Theorem 5.

A.1.3. PROOF OF THEOREM 5: STEIN POINT CONVERGENCE

Stein Points

Our high-level strategy is to show that, when k0 is P -sub-exponential, optimizing over a suitably truncated search space on
each step is sufﬁcient to optimize the discrepancy globally. To obtain an explicit rate of convergence, we adapt the greedy
approximation error analysis of Jones (1992), which applies to uniformly bounded kernels. We begin by ﬁxing any sequence
of truncation levels (Sj)∞
}, and letting
Mj denote the convex hull of {k0(x, ⋅)}x∈Bj . Next we identify a truncation-optimal hj ∈ arg minf ∈Mj J(f ). Now, ﬁx any
point sequence {xi}n

j=1 with each Sj ∈ [0, ∞), deﬁning the truncation sets Bj = {x ∈ X ∶ k0(x, x) ≤ S2
j

i=1 satisfying

k0(xj, xj)
2

+

j−1
∑
i=1

k0(xi, xj) ≤ δ
2

+

S2
j
2

+

min
x∈X∶k0(x,x)≤S2
j

j−1
∑
i=1

k0(xi, x)

for some approximation level δ ≥ 0 and each 1 ≤ j ≤ n. In the remainder, we will recursively bound the discrepancy of this
point sequence in terms of each Sj and ∥hj∥
in terms of Sj using the P -sub-exponential tails of k0,
and show that an appropriate setting of each Sj delivers the advertised claim.

, bound each ∥hj∥

K0

K0

Bounding discrepancy For each j, let fj = 1
j
and the arithmetic-geometric mean inequality, we have the estimates

i=1 k0(xi, ⋅) and (cid:15)j = DK0,P ({xi}j

∑j

i=1

) = ∥fj∥K0 . By Cauchy-Schwarz

n2(cid:15)2
n

− δ = k0(xn, xn) + (n − 1)2(cid:15)2
+ (n − 1)2(cid:15)2

≤ S2
n

n−1

n−1

+ 2(n − 1)fn−1(xn) − δ
fn−1(x)

= S2
n

+ (n − 1)2(cid:15)2

n−1

≤ S2
n

+ (n − 1)2(cid:15)2

n−1

≤ S2
n

+ (n − 1)2(cid:15)2

n−1

+ 2(n − 1) min
x∈Bn
+ 2(n − 1) inf
f ∈Mn

⟨f, fn−1⟩K0

+ 2(n − 1)⟨hn, fn−1⟩K0
(n − 1)2
n2

+ n2 ∥hn∥2
K0

+

(cid:15)2
n−1.

Unrolling the recursion, we obtain

n2(cid:15)2
n

≤

(S2

n−i

+ δ + ∥hn−i∥2
K0

(n − i)2)

(1 + 1/(n − j + 1)2).

i
∏
j=1

n−1
∑
i=0

Moreover, the products in this expression are uniformly bounded in i as

log(

(1 + 1/(n − j + 1)2)) =

log((1 + 1/(n − j + 1)2)) ≤ ∫

log(1 + 1/x2) dx = π.

i
∏
j=1

i
∑
j=1

∞

0

Therefore,

n2(cid:15)2
n

≤ eπ

S2
i

+ δ + i2 ∥hi∥2
K0

.

n
∑
i=1

Bounding ∥hi∥

K0

To bound each ∥hi∥

K0

, we consider the truncated mean embeddings

Since kP = 0, we have ∥k+
i

∥

= ∥k−
i

∥

. Moreover, since k−
i

∈ Mi, we deduce that

k−
i

∶= ∫ k0(x, ⋅)I [k0(x, x) ≤ S2
i

] dP (x)

and

k+
i

∶= ∫ k0(x, ⋅)I [k0(x, x) > S2
i

] dP (x) = kP − k−
i .

K0
∥hi∥2
K0

≤ ∥k−
i

= ∥k+
i
= ∬ k0(x, y)I [k0(x, x) > S2
i

∥2
K0

K0
∥2
K0

√

≤ (∫

k0(x, x)I [k0(x, x) > S2
i

] dP (x))

2

≤ ∫ k0(x, x)I [k0(x, x) > S2
i

] dP (x)

] dP (x)I [k0(y, y) > S2
i

] dP (y)

Stein Points

where the ﬁnal two inequalities follow by Cauchy-Schwarz and Jensen’s inequality.
Let Y = k0(Z, Z) for Z ∼ P . We will bound the tail expectation in the ﬁnal display by considering the biased random
variable Y ∗ = k0(Z ∗, Z ∗) for Z ∗ with density ρ(z∗) = k0(z∗,z∗)p(z∗)
. By (Wainwright, 2017, Thm. 2.2), since Y is
sub-exponential, there exists c0 > 0 such that E[eλY ] < ∞ for all ∣λ∣ ≤ c0. For any λ ≠ 0 with ∣λ∣ ≤ c0/2, we have, by the
relation x ≤ ex,

E[Y ]

E[eλY ∗ ] = E[eλk0(Z∗,Z∗)] =

E[k0(Z, Z)eλk0(Z,Z)]
E[Y ]

=

E[λY eλY ]
λE[Y ]

≤

E[e2λY ]
λE[Y ]

< ∞.

Hence, by (Wainwright, 2017, Thm. 2.2), Y ∗ is also sub-exponential and satisﬁes, for some ˜c1, c2 > 0, P (Y ∗ ≥ t) ≤ ˜c1e−c2t
for all t > 0.

Applying this ﬁnding to the bounding of hi, we obtain

∥hi∥2
K0

≤ ∫ k0(x, x)I [k0(x, x) > S2
i

] dP (x) = E[Y ] ∫ I [k0(x, x) > S2
i

] ρ(x) dx = E[Y ]P (Y ∗ ≥ S2
i

) ≤ c1e−c2S2

i

where c1 = ˜c1E[Y ]. Hence

DK0,P ({xi}n
i=1

) ≤ eπ/2

S2
i

+ δ + i2c1e−c2S2
i .

¿
`
`(cid:192) 1
n2

n
∑
i=1

Setting each Si By choosing Si ∈ [

2 log(n)/c2] for each i we obtain

√

√

2 log(i)/c2,
¿
`
`(cid:192) 1
n2

DK0,P ({xi}n
i=1

) ≤ eπ/2

n
∑
i=1

2 log(n)
c2

+ δ + c1 ≤ eπ/2

2 log(n)
c2n

+ δ
n

+ c1
n

.

√

A.2. Proof of Theorem 3: Log Inverse KSD Controls Convergence

Fix any α > 0 and β < 0. Our proof will leverage (Gorham & Mackey, 2017, Thm. 7). This requires demonstrating two
separate properties for the log inverse kernel: ﬁrst, the log inverse function Φ(z) ≜ (α + log(1 + ∥z∥2
))β has a nonvanishing
2
generalized Fourier transform, and second, whenever DK0,P (µm) → 0, the measures µm are uniformly tight. We will
repeatedly use the notation γ(r) ≜ (α + log(1 + r))β and φ(r) ≜ γ(r2) throughout the proof. Moveover, we will use ˆf to
denote the (generalized) Fourier transform of a function f , and Vd will represent the volume of the unit Euclidean ball in d
dimensions. Finally, we write f (m) for the m-th derivative of any sufﬁciently differentiable function f ∶ R → R.

To demonstrate the ﬁrst property, we begin with the following lemma.
Lemma 6 (Log Inverse Function Is Completely Monotone). Fix any α > 0 and β < 0. The function γ(r) ≜ (α + log(1 + r))β
is completely monotone, i.e., γ ∈ C∞ and (−1)mγ(m)(r) ≥ 0 for all m ∈ N0 and all r ≥ 0, and hence the function
k2 ∶ Rd × Rd → R given by k2(x, x′) ≜ γ(∥x − x′∥2
2

) is a kernel function for all dimensions d ∈ N.

Proof. By (Wendland, 2004, Theorem 7.13) we know that Φ is positive semideﬁnite for all dimensions d ∈ N if and only if
γ is completely monotone. Thus it remains to show that γ is completely monotone.
Since α > 0, γ(r) > 0 for all r ≥ 0. To verify (−1)mγ(m)(r) ≥ 0 for all m ≥ 1, we will proceed by induction. Let us
suppose that for some m ≥ 1,

γ(m)(r) = (−1)m

cl,m(α + log(1 + r))β−l(1 + r)−m

(10)

where each cl,m ∈ R is positive. Taking another derivative yields

γ(m+1)(r) = (−1)m+1

cl,m+1(α + log(1 + r))β−l(1 + r)−m−1,

m
∑
l=1

m+1
∑
l=1

Stein Points

where c1,m+1 ≜ m c1,m, cl,m+1 ≜ m cl,m + (l − β − 1) cl−1,m for l > 1 and cl,m ≜ 0 for all l > m, completing the induction
step.
As for the base case, notice γ′(r) = β(α + log(1 + r))β−1(1 + r)−1, which establishes the identity for l = 1 by setting
c1,1 ≜ −β. The conclusion of this proof by induction implies (−1)mγ(m)(r) ≥ 0 for all m and all r ≥ 0. By (10), γ ∈ C∞,
establishing the lemma.

Knowing that γ is a completely monotone function, we can now demonstrate ˆΦ has a nonvanishing generalized Fourier
transform.
Lemma 7 (Log Inverse Function Has Nonvanishing GFT). Consider the function Φ ∶ Rd → R given by Φ(z) = (α + log(1 +
∥z∥2
))β for some α > 0 and β < 0. Its generalized Fourier transform ˆΦ(w) is radial, nonvanishing, and continuous for
2
w ≠ 0. Moreover, ˆΦ(w) → 0 as ∥w∥

→ ∞.

2

Proof. We will ﬁrst use induction to prove an intermediate result that states for any m ∈ N0,

∆mΦ(z) = ∑

τu,v ∥z∥2v

2 γ(u)(∥z∥2

2

)

(u,v)∈Sm

(11)

where τu,v > 0 are positive reals, Sm = {(u, v) ∈ N2
0
Note for the base case m = 0, the claim above for ∆0Φ = Φ clearly holds. Now suppose it holds from some m ∈ N0. If
A ∶ Rd → R is a function that can decomposed as A(z) ≜ f (∥z∥2
2

∣ v ≤ u − m, u ≤ 2m} and γ(r) ≜ (α + log(1 + r))β.

) where f, g ∈ C∞([0, ∞)), then we have

) g(∥z∥2
2

∆A(z) = [2dg′(∥z∥2
2

) + 4 ∥z∥2

2 g′′(∥z∥2

2

)] f (∥z∥2
2

) + [2dg(∥z∥2
2

) + 4 ∥z∥2

2 g′(∥z∥2

2

)] f ′(∥z∥2
2

) + 4 ∥z∥2

2 g(∥z∥2

2

)f ′′(∥z∥2
).
2
(12)

Consider each term in the decomposition of ∆mΦ(z) from the induction hypothesis. If we let g(r) = rv and f (r) = φ(u)(r),
2 φ(u′)(∥z∥2
) where the values for (u′, v′) are (u, v − 1), (u, v −
we see that each term from (12) is of the form τ ′
1), (u + 1, v), (u + 1, v), (u + 2, v + 1) respectively. Notice that when v = 0 or v = 1, the ﬁrst or second derivative of g will
be zero and these terms may disappear altogether. Thus all these tuples will lie in Sm+1 for any (u, v) ∈ Sm, and so we must
have ∆m+1Φ(z) satisﬁes the induction hypothesis as well, completing the proof by induction.
Now we can prove the lemma. Suppose 2m ≥ d. Then by the triangle inequality and a radial substitution (Baker, 1999),

∥z∥2v′

u,v

2

∫

Rd

∣∆mΦ(z)∣ dz ≤ ∑

∫

Rd

(u,v)∈Sm

τu,v ∥z∥2v
2

∣φ(u)(∥z∥2
2

)∣ dz = d Vd ∑

τu,vr2v+d−1∣φ(u)(r2)∣ dr.

∞

∫

0

(u,v)∈Sm

Because ∣φ(u)(r)∣ = O(r−u logβ−1(r)) as r → ∞ for u ∈ N by (10), we see that each integrand above is
O(r2(v−u)+d−1 logβ−1(r)). But since v ≤ u − m, this will imply that each integrand is O(r−2m+d−1 logβ−1(r)), which is
integrable for large r yielding ∆mΦ ∈ L1(Rd).

By (Steinwart & Christmann, 2008, Lemma 4.34) and the fact that positive deﬁniteness is preserved by summation, we
have ∆mΦ is a positive deﬁnite function. This along with the fact that ∆mΦ ∈ L1(Rd) allows us to invoke (Wendland,
2004, Theorem 6.11) and (Wendland, 2004, Theorem 6.18) to obtain ̂∆mΦ is continuous, radial and nonvanishing.
Moreover, ∆mΦ belonging to L1(Rd) implies its Fourier transform belongs to L∞(Rd). The lemma follows by noticing
̂∆mΦ(w) for all w ≠ 0.
̂∆mΦ(w) = ∥w∥2m

ˆΦ(w), i.e., ˆΦ(w) = ∥w∥−2m

2

2

We now need to demonstrate the second property to complete the proof of Theorem 3, but in order to do so, we ﬁrst will
establish the lemma below. By Lemma 7, we know ˆΦ is radial and thus can write ˆΦ(w) = φ∧(∥w∥
) for some continuous
2
function φ∧ ∶ (0, ∞) → (0, ∞). Our ﬁrst priority will be to lower bound φ∧ near the origin.
Lemma 8 (Log Inverse GFT Lower Bound). If Φ is the log inverse function on Rd from Lemma 7, then lim inf r→0+ rd(α +
log(1 + 1/r2))−β+1φ∧(r) > 0 where ˆΦ(w) = φ∧(∥w∥
) for all w ≠ 0.
2

Proof. First we will show that φ∧ is strictly decreasing. Since r ↦ (α + log(1 + r))β was shown to be completely monotone
in Lemma 6, by (Wendland, 2004, Theorem 7.14) we must have Φ(z) = ∫ ∞
2 ∂v(t) for some ﬁnite, non-negative

0 e−t∥z∥2

Stein Points

Borel measure v on [0, ∞) that is not concentrated at zero. Let (ϕm)∞
2004, Deﬁnition 5.17) deﬁned on Rd. Then, for each m, both ˆϕm and Φ ˆϕm are also Schwartz functions, and thus

m=1 be a sequence of Schwartz functions (Wendland,

∞

∫

Rd

[∫

0

∣e−t∥x∥2

2 ˆϕm(x)∣∂v(t)] dx = ∫

e−t∥x∥2

2∣ ˆϕm(x)∣∂v(t)] dx = ∫

Φ(x)∣ ˆϕm(x)∣ dx < ∞,

Rd

∞

[∫

0

Rd

as all Schwartz functions are integrable. This allows us to use Fubini’s theorem in conjunction with Plancherel’s Theorem to
argue

∫

Rd

ˆΦ(w)ϕm(w) dw = ∫

Φ(x) ˆϕm(x) dx = ∫

[∫

e−t∥x∥2

2∂v(t)] ˆϕm(x) dx

Rd

∞

0
∞

Rd

∫

Rd

∞

∞

0

0

0

∫

∫

Rd

Rd

= ∫

= ∫

= ∫

e−t∥x∥2

2 ˆϕm(x)∂v(t) dx

e−t∥x∥2

2 ˆϕm(x) dx∂v(t)

(2t)d/2e− 1

4t ∥w∥2

2ϕm(w) dw∂v+(t) + v0ϕm(0),

where we have used the decomposition v ≜ v+ + v0δ0 for v0 ≥ 0 and v+ non-zero and absolutely continuous with respect
to Lebesgue measure on [0, ∞). Let B ∶ Rd → R be a bump function, e.g., B(x) ≜ Z −1 exp{−1/(1 − ∥x∥2
< 1]
2
where Z is the normalization constant chosen such that ∫Rd B(x) dx = 1. Then let us deﬁne ϕm ∶ Rd → R via the mapping
ϕm(w) ≜ mdB(m(w − w0e1)) − mdB(m(w − w1e1)), where 0 < w0 < w1 and e1 ∈ Rd is the ﬁrst standard basis vector.
ˆΦ(w)ϕm(w) dw → ˆΦ(w0e1) − ˆΦ(w1e1) = φ∧(w0) − φ∧(w1) since ˆΦ is a continuous in neighborhoods of w0e1
Then ∫Rd
and w1e1 (Wendland, 2004, Theorem 5.22).
Because v+ cannot be the zero measure, there must be some ﬁnite interval [a0, b0] ⊂ (0, ∞) such that v+([a0, b0]) > 0. For
each t > 0 and m > max( 1
w0

)}I [∥x∥
2

), we have

2
w1−w0

,

Am(t) ≜ ∫

(2t)d/2e− 1

4t ∥w∥2

2ϕm(w) dw = ∫

(2t)d/2(e− 1

4t ∥w−w0e1∥2

2 − e− 1

4t ∥w−w1e1∥2

2 )mdB(mw) dw > 0,

Rd

Rd

since ∥w − w0e1∥
Am(t) → (2t)d/2(e− 1

< ∥w − w1e1∥
0 − e− 1

4t w2

2

2 when ∥w∥
1 ) as m → ∞ for any t > 0. Moreover, for all t ∈ [a0, b0] and m ≥ 1, we have

, w0). Using (Wendland, 2004, Theorem 5.22) again, we have

< min( w0−w1

2

2

4t w2

∣Am(t)∣ ≤ ∫

(2t)d/2e− 1

4t ∥w−w0e1∥2

Rd

2mdB(mw) dw ≤ (2b0)d/2 sup
∥w∥2<1

e− 1

4b0

∥w−w0e1∥2

2 < ∞.

(13)

Hence, the dominated convergence theorem allows us to exchange the limit over m and integral over t below to conclude

φ∧(w0) − φ∧(w1) = lim
m→∞

∫

0

Am(t)∂v+(t) + v0ϕm(0) ≥ lim
m→∞

∫

∞

b0

a0

Am(t)∂v+(t) = ∫

Am(t)∂v+(t)

b0

a0

lim
m→∞

b0

= ∫

a0

(2t)d/2(e− 1

4t w2

0 − e− 1

4t w2

1 )∂v+(t) ≥ v+([a0, b0]) min

{(2t)d/2(e− 1

4t w2

0 − e− 1

4t w2

1 )} > 0,

t∈[a0,b0]

showing φ∧ is strictly decreasing as claimed.
Suppose ψ ∶ [0, ∞) → R is a C∞ function with support [a, b] for 0 < a < b such that ψ(r) > 0 for all r ∈ (a, b) and
∫ ∞
0 ψ(r) dr = 1. Then because φ∧ is strictly decreasing, by the mean value theorem we have

for all λ > 0. If we assign Ψ(w) ≜ ψ(∥w∥
above becomes

2

φ∧(b/λ) ≤ ∫

λφ∧(r)ψ(λr) dr ≤ φ∧(a/λ)

∞

0

(14)

) to be the radial continuation of ψ, by (Baker, 1999) the quantity sandwiched

λφ∧(r)ψ(λr) dr = ∫

∞

0

φ∧(s/λ)ψ(s) ds = 1
dVd

∫

Rd

ˆΦ(w/λ) Ψ(w)
∥w∥d−1
2

dw.

Next suppose that ξ ∶ [0, ∞) → R is a Schwartz function satisfying ξ(k)(0) = 0 for all integral k ≥ 0, and let Ξ ∶ Rd → R
) be the radial continuation of ξ. Then by Plancherel’s Theorem, scaling the input of a Fourier
given by Ξ(x) ≜ ξ(∥x∥

∞

∫
0

2

Stein Points

transform as in (Wendland, 2004, Theorem 5.16), and the change to spherical coordinates in (Baker, 1999), for any λ > 0,
we have

∫

Rd

ˆΦ(w/λ)Ξ(w) dw = ∫

Φ(w)ˆΞ(w/λ) dw = d Vd ∫

Rd

∞

0

rd−1φ(r)ξ∧(r/λ) dr = d Vd λd ∫
0

∞

sd−1φ(λs)ξ∧(s) ds,

(15)

where s = r/λ and ξ∧ is the radial function associated with ˆΞ, i.e., ˆΞ(w) = ξ∧(∥w∥
Let us deﬁne ω ∶ [0, ∞) → R by the mapping ω(t) ≜ (α + t)β. Then by the mean value theorem and the fact that ω′ is
increasing, we have for all s > 1

) for all w.

2

−ω′(log(1 + λ2s2)) ≤ − ω(log(1 + λ2s2)) − ω(log(1 + λ2))

log(1 + λ2s2) − log(1 + λ2)

≤ −ω′(log(1 + λ2)).

By rearranging terms, this implies for all λ > 0

(−β) ( α + log(1 + λ2s2)
α + log(1 + λ2)

)

log ( 1 + λ2s2
1 + λ2

) ≤ − ω(log(1 + λ2s2)) − ω(log(1 + λ2))
ω(log(1 + λ2))(α + log(1 + λ2))−1

≤ (−β) log ( 1 + λ2s2
1 + λ2

) .

β−1

Since log( 1+λ2s2
) → 2 log s as λ → ∞, and the sandwiched term above is −(α + log(1 + λ2))(φ(λs)/φ(λ) − 1), we have
1+λ2
(α + log(1 + λ2))(φ(λs)/φ(λ) − 1) → 2β log s as λ → ∞ for all s > 1. The case for s ∈ (0, 1] is analogous and yields the
same asymptotic limit.

With this new asymptotic expansion in hand, we will revisit (15). We have

λ−dφ(λ)−1(α + log(1 + λ2)) ∫

ˆΦ(w/λ)Ξ(w) dw = d Vdφ(λ)−1(α + log(1 + λ2)) ∫

φ(λs)sd−1ξ∧(s) ds

Rd

= d Vd (α + log(1 + λ2)) ∫

sd−1ξ∧(s) ds

∞

∞

0
φ(λs)
φ(λ)
(α + log(1 + λ2)) [ φ(λs)
φ(λ)

0

= d Vd ∫

∞

0

− 1] sd−1ξ∧(s) ds.

Notice that ﬁnal integrand converges to 2βsd−1(log s)ξ∧(s) pointwise for all s ≥ 0 as λ → ∞. Since ξ∧ is a Schwartz
function on [0, ∞), we can utilize the fact that s ↦ log s is integrable near the origin to reason that sd−1(log s)ξ∧(s) is a
Schwartz function as well, and thus integrable. Hence by the dominated convergence theorem, we have the integral above
converges to 2 β d Vd ∫ ∞
Now suppose we choose Ξ(x) ≜ ∥x∥1−d

0 sd−1(log s)ξ∧(s) ds as λ → ∞.

2 Ψ(x). By (14) we have

λ−dφ(λ)−1(α + log(1 + λ2))φ∧(b/λ) ≤ 2β ∫

lim
λ→∞

∞

0

sd−1(log s)ξ∧(s) ds ≤ lim
λ→∞

λ−dφ(λ)−1(α + log(1 + λ2))φ∧(a/λ).

By Lemma 7, we know φ∧(r) > 0 for all r > 0, and thus the left-hand side above must be non-negative. Hence if we can
show for some choice of ψ that the sandwiched term is non-zero, then the proof of the lemma will follow from choosing
r = a/λ.
Let us deﬁne L(x) = log ∥x∥
the radial functions associated with L and ˆL. Notice that again by Plancherel’s Theorem

2 with generalized Fourier transform ˆL. As usual, let l ∶ [0, ∞) → R and l∧ ∶ [0, ∞) → R be

∞

∫

0

sd−1(log s)ξ∧(s) ds = 1
dVd

∫

Rd

ˆΞ(w)L(w) dw = 1
dVd

∫

Rd

Ξ(x) ˆL(x) dx = 1
dVd
∞

= ∫

0

ˆL(x) dx

∫

Rd

Ψ(x)
∥x∥d−1
2
ψ(r)l∧(r) dr.

(16)

Since we are free to choose ψ to be any Schwartz function with support [a, b], if we could not ﬁnd a function ψ such that the
quantity in (16) is non-zero, this would imply the support of l∧ is a subset of {0}. But this would mean l∧ is some multiple
of a point mass at zero, which would imply l is a constant function, a contradiction. Thus we must be able to ﬁnd some ψ
such that the integral above is non-zero, completing the lemma.

Stein Points

). Our strategy for showing the KSD controls tightness will mimic (Gorham & Mackey, 2017,
)α0−1 belongs to the

Fix any a0 > 0 and α0 ∈ (0, 1
2
Lem. 16): we will show that a bandlimited approximation of the function gj(x) = 2α0xj(a2
0
inverse log RKHS and thus enforces tightness.
First note that in the proof of (Gorham & Mackey, 2017, Lem. 16), it was shown h = TP g was a coercive, Lipschitz, and
bounded-below function for P ∈ P. Moreover, in the proof of (Gorham & Mackey, 2017, Lem. 12), a random vector Y with
density ρ(y) is constructed such that the support of ˆρ belongs to [−4, 4]d and also ∥Y ∥
2 is integrable. Consider the new
function g○(x) ≜ E [g(x + Y )] for all x ∈ Rd. By the convolution theorem, ˆg○
j is bandlimited for all j. In
j
the proof of (Gorham & Mackey, 2017, Lem. 16), ˆgj was shown to grow asymptotically at the rate (iwj) ∥w∥−d−2α0
as
∥w∥
2

= ˆgj ˆρ and so g○

→ 0. Thus

+ ∥x∥2
2

2

d
∑
j=1

∫

Rd

(w)

ˆg○
j

(w)ˆg○
j
ˆΦ(w)

dw =

d
∑
j=1

∫

[−4,4]d

ˆgj(w)ˆgj(w)ˆρ(w)2
ˆΦ(w)

dw ≤ κ0 ∫

∥w∥−2d−4α0+2
2
ˆΦ(w)

dw

[−4,4]d
√

4

d

≤ κ1 ∫
0

r−4α0+1 log−β+1(1 + r−2) dr,

for some constants κ0, κ1 > 0 where we used Lemma 8 in the ﬁnal inequality. This integral is ﬁnite for all α0 ∈ (0, 1
2
any β < 0, which implies g○ is in the log inverse RKHS by (Wendland, 2004, Theorem 10.21).

) and

Finally, notice that via the argument proving (Gorham & Mackey, 2017, Lemma 12),

∣TP g○(x) − h(x)∣ ≤ 3d log 2

sup
x∈Rd

( sup
x∈Rd

∥∇h(x)∥

2

+ sup
x∈Rd

π

∥∇2 log p(x)∥

⋅ sup
x∈Rd

op

∥g(x)∥

) < ∞.

2

Since h is bounded below and coercive, these properties are inherited by TP g○. This allows us to apply (Gorham & Mackey,
2017, Lemma 17) to argue DK0,P (µm) → 0 implies the measures µm are uniformly tight. Combining this with Lemma 7
allows us to utilize (Gorham & Mackey, 2017, Theorem 7) for the log inverse kernel, thereby concluding the proof.

A.3. Proof of Theorem 4: IMQ Score KSD Convergence Control

For b = ∇ log p, introduce the alias kb = k3, let Kb denote the RKHS of kb, and let Cc represent the set of continuous
compactly supported functions on X. Since P ∈ P, the proof of Thm. 13 in (Gorham & Mackey, 2017) shows that if, for
each h ∈ C 1 ∩ Cc and (cid:15) > 0, there exists h(cid:15) ∈ Kb such that supx∈X
∣(TP h)(x) − (TP h(cid:15))(x)∣ ≤ (cid:15), then µm ⇒ P whenever
DK0,P (µm) → 0 and (µm)∞
m=1 is uniformly tight. Hence, to establish our result, it sufﬁces to show (1) that, for each
h ∈ C 1 ∩ Cc and (cid:15) > 0, there exists h(cid:15) ∈ Kb such that supx∈X max(∥∇(h − h(cid:15))(x)∥
) ≤ (cid:15) and (2) that
DK0,P (µm) → 0 implies (µm)∞

2 , ∥b(x)(h − h(cid:15))(x)∥

m=1 is uniformly tight.

2

A.3.1. APPROXIMATING C 1 ∩ Cc WITH Kb
Fix any f ∈ C 1 ∩ Cc and (cid:15) > 0, and let K denote the RKHS of k(x, y) = (c2 + ∥x − y∥2
)β. Since p is strictly log-concave, b
2
is invertible with det(∇b(x)) never zero. Since P ∈ P, b is Lipschitz. By the following theorem, proved in Section A.4, it
therefore sufﬁces to show that there exists f(cid:15) ∈ K such that supx∈X max(∥∇(f − f(cid:15))(x)∥
Theorem 9 (Composition Kernel Approximation). For b ∶ X → X invertible and k a reproducing kernel on X with induced
RKHS K, deﬁne the composition kernel kb(x, y) = k(b(x), b(y)) with induced RKHS Kb. Suppose that, for each f ∈ C 1 ∩Cc
and (cid:15) > 0, there exists f(cid:15) ∈ K such that

2 , ∥x(f − f(cid:15))(x)∥

) ≤ (cid:15).

2

If b is Lipschitz and det(∇b(x)) is never zero, then, for each h ∈ C 1 ∩ Cc and (cid:15) > 0, there exists h(cid:15) ∈ Kb such that

max(∥∇(f − f(cid:15))(x)∥

2 , ∥x(f − f(cid:15))(x)∥

2

) ≤ (cid:15).

max(∥∇(h − h(cid:15))(x)∥

2 , ∥b(x)(h − h(cid:15))(x)∥

2

) ≤ (cid:15).

sup
x∈X

sup
x∈X

Since the identity map x ↦ x is Lipschitz and f ∈ L2 because it is continuous and compactly supported, (Gorham & Mackey,
2017, Lem. 12) provides an explicit construction of f(cid:15) ∈ K satisfying our desired property whenever k(x, y) = Φ(x − y) for
Φ ∈ C 2 with non-vanishing Fourier transform. Our choice of IMQ k satisﬁes these properties by (Wendland, 2004, Thm.
8.15).

A.3.2. CONTROLLING TIGHTNESS

Since P is distantly dissipative,

Stein Points

− ∥b(x)∥

∥x∥

2

2

≤ ⟨b(x), x⟩ ≤ −κ ∥x∥2
2

+ C + ⟨b(0), x⟩ ≤ −κ ∥x∥2
2

+ C + ∥b(0)∥

∥x∥

2

2

by Cauchy-Schwarz. Hence, b is norm-coercive, i.e., ∥b(x)∥
result follows from the following lemma which guarantees tightness control on b under weaker conditions.
Lemma 10 (Coercive Score Kernel KSDs Control Tightness). If b ∶ X → X is norm coercive and differentiable, and
∇jbj(x) = o(∥b(x)∥2
2

→ ∞, then lim supm DK0,P (µm) < ∞ implies (µm)∞

→ ∞. Since ∇b is bounded, our desired

→ ∞ whenever ∥x∥

m=1 is tight.

) as ∥x∥

2

2

2

Proof. Fix any a > c/2 and α ∈ (0, 1
2
gj(x) = 2αxj(a2 + ∥x∥2
2
Lemma 12. By our assumptions on ∇b, we have

(β + 1)). The proof of (Gorham & Mackey, 2017, Lem. 16) showed that the function
)α−1 ∈ K for each j ∈ {1, . . . , d}. Hence gb,j(x) ≜ gj(b(x)) ∈ Kb for each j ∈ {1, . . . , d} by

(TP gb)(x) = 2α(∥b(x)∥2
2

(a2 + ∥b(x)∥2
2

∇jbj(x)(a2 + ∥b(x)∥2
2

)α−1 + bj(x)22(α − 1)(a2 + ∥b(x)∥2
2

)α−2∇jbj(x))

= 2α ∥b(x)∥2
2

(a2 + ∥b(x)∥2
2

)α−1 +

d
∑
j=1
)α−1 + o(∥b(x)∥2α
2

),

so TP gb is coercive, and the proof of (Gorham & Mackey, 2017, Lem. 17) therefore gives the result (µm)∞
tight whenever lim supm DK0,P (µm) ﬁnite.

m=1 is uniformly

A.4. Proof of Theorem 9: Composition Kernel Approximation

Let c = b−1 represent the inverse of b, and for any function f on X, let fc(y) = f (c(y)) denote the composition of f and c
so that fc(b(x)) = f (x). The following lemma shows that fc inherits many of the properties of f under suitable restrictions
on b.
Lemma 11 (Composition Properties). For any function f on X and invertible function b on X, deﬁne fc(y) = f (c(y)) for
c = b−1. The following properties hold.

1. If f has compact support and b is continuous, then fc has compact support.

2. If f ∈ C 1, b ∈ C 1, and det(∇b(x)) is never zero, then fc ∈ C 1.

Proof. We prove each claim in turn.

1. If f is compactly supported and b is continuous, then supp(fc) = b(supp(f )) is also compact, since continuous

functions are compact-preserving (Joshi, 1983, Prop. 1.8).

2. If f ∈ C 1, b ∈ C 1, and det(∇b(x)) is never zero, then c is continuous by the inverse function theorem (Spivak, 1965,
Thm. 2-11), x ↦ (∇b(x))−1 is continuous, and hence ∇fc(y) = (∇c(y))(∇f )(c(y)) = ((∇b)(c(y)))−1(∇f )(c(y))
is continuous.

Our next lemma exposes an important relationship between the RKHSes K and Kb.
Lemma 12. Suppose f is in the RKHS K of a reproducing kernel k on X and b ∶ X → X is invertible. Then fb is in the
RKHS Kb of kb for fb(x) = f (b(x)) and kb(x, y) = k(b(x), b(y)).

Proof. Since f ∈ K,
limm→∞ ∥fm − f ∥

K

there exist fm = ∑Jm

j=1 am,jk(xm,j, ⋅) for m ∈ N, am,j ∈ R, and xm,j ∈ X such that

= 0 and limm→∞ fm(x) = f (x) for all x ∈ X. Now let c = b−1, and deﬁne

fm,b(x) = fm(b(x)) =

am,jk(xm,j, b(x)) =

am,jkb(c(xm,j), x).

Jm
∑
j=1

Jm
∑
j=1

Stein Points

Since Kb = {∑J
⟨fm,b, fm′,b⟩2
Kb
for all m, m′, the sequence (fm,b)∞
complete, fb ∈ Kb.

j=1 ajkb(yj, ⋅) ∶ J ∈ N, aj ∈ R, yj ∈ X}, each fm,b ∈ Kb. Since (fm)∞
= ∑Jm

j′=1 am′,j′kb(c(xm,j), c(xm′,j′ )) = ⟨fm, fm′ ⟩K so that ∥fm,b − fm′,b∥

j=1 am,j ∑Jm′

m=1 is a Cauchy sequence, and

= ∥fm − fm′ ∥
K
to its pointwise limit fb. Since an RKHS is

Kb

m=1 is also Cauchy and converges in ∥⋅∥

Kb

With our lemmata in hand, we now prove the advertised claim. Suppose b is Lipschitz, det(∇b(x)) is never zero, and
for each f ∈ C 1 ∩ Cc and (cid:15) > 0 there exists f(cid:15) ∈ K such that supx∈X max(∥∇(f − f(cid:15))(x)∥
) ≤ (cid:15).
Select any h ∈ C 1 ∩ Cc and any (cid:15) > 0. By Lemma 11, hc ∈ C 1 ∩ Cc, and hence there exists hc,(cid:15) ∈ K such that
) ≤ (cid:15)/ max(1, M1(b)). Now deﬁne h(cid:15)(x) = hc,(cid:15)(b(x)) so that
supy∈X max(∥∇(hc − hc,(cid:15))(y)∥
h(cid:15) ∈ Kb by Lemma 12. We have supx∈X
∥∇h(cid:15)(x) − ∇h(x)∥

2
∥(∇b(x))((∇hc,(cid:15))(b(x)) − (∇hc)(b(x)))∥

≤ M1(b)(cid:15)/ max(1, M1(b)) ≤ (cid:15).

2 , ∥y(hc − hc,(cid:15))(y)∥

2 , ∥x(f − f(cid:15))(x)∥

∥b(x)(h(cid:15) − h)(x)∥

∥y(hc,(cid:15) − hc)(y)∥

≤ supy∈X

≤ (cid:15), and

2

2

2

2

sup
x∈X

2

= sup
x∈X

B. Implementational Detail

B.1. Benchmark Methods

In this section we brieﬂy describe the MED and SVGD methods used as our empirical benchmark, as well as the (block)
coordinate descent method that was used in conjunction with Stein Points.

B.1.1. MINIMUM ENERGY DESIGNS

The ﬁrst class of method that we consider is due to (Joseph et al., 2015). That work restricted attention to X = [0, 1]d and
constructed an energy functional:

Eδ,P ({xi}n
i=1

)

⎡
⎢
⎢
⎢
⎣

∶= ∑
i≠j

p(xi)− 1

2d p(xj)− 1

2d

∥xi − xj∥2

⎤
δ
⎥
⎥
⎥
⎦

for some tuning parameter δ ∈ [1, ∞) to be speciﬁed. In (Joseph et al., 2017) the rule-of-thumb δ = 4d was recommended. A
heuristic argument in (Joseph et al., 2015) suggests that the points {xi}n
) form an empirical
approximation that converges weakly to P . The argument was recently made rigorous in (Joseph et al., 2017).
Minimisation of Eδ,P does not require knowledge of how p is normalised. However, the actual minimisation of Eδ,P can be
difﬁcult. In (Joseph et al., 2015) an extensible (greedy) method was considered, wherein the ﬁrst point is selected as

i=1 that minimise Eδ,P ({xi}n
i=1

and subsequent points are selected as

x1 ∈ arg max

p(x)

x∈X

xn

∈ arg min

p(x)− δ

2d

x∈X

n−1
∑
i=1

2d

p(xi)− δ
∥xi − x∥δ
2

.

However, alternative approaches could easily be envisioned. For instance, if n were ﬁxed then one could consider e.g.
applying the Newton method for optimisation over the points {xi}n

i=1.

Remark: There is a connection between certain minimum energy methods and discrepancy measures in RKHS; see
(Sejdinovic et al., 2013).
Remark: Several potential modiﬁcations to Eδ,P were suggested in (Joseph et al., 2017), but that report appeared after this
work was completed. These could be explored in future work.
Remark: The MED objective function is typically numerically unstable due to the fact that the values of the density p(⋅) can
be very small. In contrast, our proposed methods operate on log p(⋅) and its gradient, which is more numerically robust.

B.1.2. STEIN VARIATIONAL GRADIENT DESCENT

The second method that we considered was due to (Liu & Wang, 2016; Liu, 2017) and recently generalised in (Liu &
Zhu, 2017). The idea starts by formulating a continuous version of gradient descent on P(X) with the Kullback-Leibler

divergence KL(⋅∣∣P ) as a target. To this end, restrict attention to X = Rd and consider the dynamics

Stein Points

Sf (x) = x + (cid:15)f (x)

parametrised by a function f ∈ Kd. For inﬁnitesimal values of (cid:15) we can lift Sf to a pushforward map on P(X); i.e.
Q ↦ Sf Q. It was then shown in (Liu & Wang, 2016) that

− d
d(cid:15)

KL(Sf Q∣∣P )∣

= ∫ TP f dQ

(cid:15)=0

(17)

where TP is the Langevin Stein operator in Eqn. 5. Recall that this operator can be decomposed as TP f = ∑d
TP,jfj with
TP,j = ∇j + ∇j log p, where ∇j denotes differentiation with respect to the jth coordinate in X. Then the direction of fastest
descent

j=1

has a closed-form, with jth coordinate

f ∗(⋅)

∶= arg max
f ∈B(Kd)

− d
d(cid:15)

KL(Sf Q∣∣P )∣

(cid:15)=0

f ∗
j

(⋅; Q) = ∫ TP,jk(x, ⋅) dQ(x).

The algorithm proposed in (Liu & Wang, 2016) discretises this dynamics in both space X, through the use of n points,
and in time, through the use of a positive step size (cid:15) > 0, leading to a sequence of empirical measures based on point sets
{xm
i

i=1 of the points, at iteration m ≥ 1 of the algorithm we update
}n

i=1 for m ∈ N. Thus, given an initialisation {x0
}n
i

in parallel, where

xm
i

= xm−1
i

+ (cid:15)f ∗(xm−1

i

; Qm
n

)

Qm
n

=

1
n

n
∑
i=1

δxm−1

i

is the empirical measure, at a computational cost of O(n). The output is the empirical measure Qm
n .

Remark: The step size (cid:15) is a tuning parameter of the method.

Remark: At present there are not theoretical guarantees for this method. Initial steps toward this goal are presented in (Liu,
2017).

B.1.3. BLOCK COORDINATE DESCENT

The Stein Point methods developed in the main text can be adapted to return a ﬁxed number n of points for a given ﬁnite
computational budget by ﬁrst iteratively generating a size n point set, as described in the main text, and then performing
(block) coordinate descent on this point set. The (block) coordinate descent procedure is now described:
Fix an initial conﬁguration {x0
i

i=1. Then at iteration m ≥ 1 of the algorithm, perform the following sequence of operations:
}n

∀i
for i = 1, . . . , n

xm
i
xm
i

← xm−1
i
← arg min
x∈X

then:
DK0,P ({xm
j

}j≠i ∪ {x})

The output is the point set {xm
i

}n
i=1.

Remark: The block coordinate descent method can equally be applied to MED; this was not considered in our empirical
work.

Remark: Any numerical optimisation method can be used to solve the global optimisation problem in the inner loop. In
this work we considered the same three candidates in the main text; Monte Carlo, Nelder-Mead and grid search. These are
described next.

Stein Points

B.2. Numerical Optimisation Methods

Computation of the nth term in the proposed Stein Point sequences, given the previous n − 1 terms, requires that a global
optimisation is performed over xn ∈ X. The same is true for both MED and KSD in the coordinate descent context. For all
experiments reported in the main text, three different numerical methods were considered for this task, denoted NM, MC, GS
in the main text. In this section we provide full details for how these methods were implemented.

B.2.1. NELDER-MEAD

The Nelder-Mead (NM) method (Nelder & Mead, 1965) proceeds as in Algorithm 1. The function NM takes the following
inputs: f is the objective function; t is the iteration count; ninit is the number of initial points to be drawn from a
proposal distribution; ndelay is the number of iterations after which the proposal distribution becomes adaptive; µ0
and Σ0 are the mean vector and the covariance matrix of the initial proposal distribution; {xcurr
is the set of
existing points; λ is the variance of each mixture component of the adaptive proposal distribution; l and u are the
lower- and upper-bounds of the search space. The non-adaptive initial proposal distribution is a truncated multivariate
Gaussian N (µ0, Σ0) whose support is bounded by the hypercube [l, u]. The adaptive proposal distribution is a truncated
, λI) with λ > 0 and support [l, u]. The expression
Gaussian mixture Π({xcurr
j
NelderMeadx [f (x), xinit
, l, u] denotes the standard Nelder-Mead procedure for objective function f , initial point xinit
,
and bound constraint x ∈ [l, u]. We use the symbol ¢ to denote the assignment of a realised independent draw. The operator
truncu
l

[⋅] bounds the support of a distribution by the hypercube [l, u].

}ncurr
j=1 , λ) ∶=

∑ncurr−1
j=1

N (xcurr
j

1
ncurr−1

}ncurr
j=1

j

i

i

}ncurr
j=1 , λ, l, u

j

xinit
i

for i ← 1 ∶ ninit do
if t ≤ ndelay then
¢ truncu
l

Algorithm 1 Nelder-Mead
input f , t, ninit, ndelay, µ0, Σ0, {xcurr
output x∗
1: function NM
2:
3:
4:
5:
6:
7:
8:
9:
10:

xinit
i
end if
xlocal
i
end for
i∗ ← arg mini∈{1...ninit} f (xlocal
x∗ ← xlocal
11:
12: end function

¢ truncu
l

[Π({xcurr

else

i∗

[N (µ0, Σ0)]

)

j

i

j=1 , λ)]
}ncurr

← NelderMeadx [f (x), xinit

, l, u]

i

B.2.2. MONTE CARLO

The Monte Carlo (MC) optimisation method proceeds as in Algorithm 2. The function MC takes the following inputs:
f is the objective function; t is the iteration count; ntest is the number of test points to be drawn from a proposal
distribution; ndelay is the number of iterations after which the proposal distribution becomes adaptive; µ0 and Σ0 are
the mean vector and the covariance matrix of the initial proposal distribution; {xcurr
is the set of existing points;
λ is the variance of each mixture component of the adaptive proposal distribution; l and u are the lower- and upper-
bounds of the search space. The non-adaptive initial proposal distribution is a truncated multivariate Gaussian N (µ0, Σ0)
whose support is bounded by the hypercube [l, u]. The adaptive proposal distribution is a truncated Gaussian mixture
Π({xcurr
j

, λI) with λ > 0 and support [l, u].

}ncurr
j=1 , λ) ∶=

∑ncurr−1
j=1

N (xcurr
j

}ncurr
j=1

j

1
ncurr−1

Stein Points

j

if t ≤ ndelay then

Algorithm 2 Monte Carlo
input f , t, ntest, ndelay, µ0, Σ0, {xcurr
output x∗
1: function MC
2:
3:
4:
{xtest
5:
i
end if
6:
i∗ ← arg mini∈{1...ntest} f (xtest
7:
x∗ ← xtest
8:
9: end function

¢ truncu
l

¢ truncu
l

{xtest
i

}ntest
i=1

}ntest
i=1

else

i∗

)

j

i

}ncurr
j=1 , λ, l, u

[N (µ0, Σ0)]

[Π({xcurr

j=1 , λ)]
}ncurr

B.2.3. GRID SEARCH

Algorithm 3 Grid Search
input f , t, l, u, n0
output x∗
1: function GS
2:
3:
4: Xgrid ← {l, l + δgrid, . . . , u}d
x∗ ← arg minx∈Xgrid
5:
6: end function

t)
ngrid ← n0 + Round(
δgrid ← (u − l)/(ngrid − 1)

f (x)

√

The grid search (GS) optimisation method proceeds as in Algorithm 3. The function GS takes the following inputs: f is the
objective function; t is the iteration count; l and u are the lower- and upper-bounds of the grid; n0 is the initial grid size.

B.3. Remark on Application to a Reference Point Set

It is interesting to comment on the behaviour of our proposed methods in the case where X is a ﬁnite set or the global
optimisation over X is replaced by a discrete optimisation over a pre-determined ﬁxed set Y = {yi}N
⊆ X. In this case it
i=1
can be shown that:

• The algorithm after n iterations will have selected n points {yπ(i)}n

i=1 with replacement from Y . (Here π(i) indexes

the point that was selected at iteration i of the algorithm.)

• The empirical measure 1
n

∑n

i=1 δyπ(i) can be expressed as ∑N

i=1 wiyi for some weights wi.

• The weights wi converge to

(∗) =

arg min
w≥0
w1+⋅⋅⋅+wN =1

wiwjk0(yi, yj).

¿
`
`(cid:192) 1
N 2

N
∑
i,j=1

√

• At iteration n, it holds that DK0,P ({yπ(i)}n
i=1

) = (∗) + O(

log(n)/n).

Thus in this scenario the algorithms that we have proposed act to ensure that these points are optimally weighted in the
sense just described.

C. Experimental Protocol and Additional Numerical Results

This section contains additional numerical results that elaborate on the three experiments reported in the main text.

C.1. Gaussian Mixture Test

Stein Points

Recall from the main text that the kernels k1, k2 and k3 contain either one or two hyper-parameters that must be selected.
For each of the methods (a)-(f) reported in Figure 2 in the main text we optimised these parameters over a discrete set, with
respect to an objective function of WP based on a point set of size n = 100 and the Nelder-Mead optimisation method. The
set of possible values for α was {0.1η, 0.5η, η, 2η, 4η, 8η}, where η is a problem dependent “base scale” and chosen to be 1
for the Gaussian mixture test. The set of possible values for β was {−0.1, −0.3, −0.5, −0.7, −0.9}. The sensitivity of the
reported results to the variation in hyper-parameters is shown, for the Gaussian mixture test, in Figure 5. Point sets obtained
under representatives of each method class are shown in Figure 6.
For all the global optimisation methods we imposed a bounding box (−5, 5) × (−5, 5); for the Nelder-Mead method, we
set ninit = 3, ndelay = 20, µ0 = (0, 0), Σ0 = 25I, and λ = 1; for the Monte Carlo method, we set ntest = 20, ndelay = 20,
µ0 = (0, 0), Σ0 = 25I, and λ = 1; for the grid search, we set n0 = 100.
For MED the tuning parameter δ was considered for δ = 4, δ = 8 or δ = 16, with δ = 4d = 8 being the recommendation in
(Joseph et al., 2017).

For SVGD we set the initial point-set to be an equally spaced rectangular grid over the bounding box. Following (Liu &
Wang, 2016), the step-size (cid:15) for SVGD was determined by AdaGrad with a master step-size of 0.1 and a momentum factor
of 0.9.

(a) Stein Points (Greedy)

(b) Stein Points (Herding)

(c) SVGD

Figure 5: Kernel parameter selection results for the Gaussian mixture test. Parameters α, β in the kernels k1, k2, k3 were
optimised over a discrete set with respect to the Wasserstein distance WP for a point set of size n = 100. The values log WP
(y-axis) are shown for all different conﬁgurations of parameters (x-axis) considered. Optimal parameter conﬁgurations are
circled and detailed in the legend.

C.2. Gaussian Process Test

For the Gaussian process test, the base scale η is also set to 1. The sensitivity of results to the selection of kernel parameters
was reported in Figure 7. Point sets obtained under representatives of each method class are shown in Figures 8 and 9.
Detailed results for each method considered are contained in Figure 10.
For all the global optimisation methods we imposed a bounding box of (−5, 5) × (−13, −7); for the Nelder-Mead method, we
set ninit = 3, ndelay = 20, µ0 = (0, −10), Σ0 = 25I, and λ = 1; for the Monte Carlo method, we set ntest = 20, ndelay = 20,
µ0 = (0, −10), Σ0 = 25I, and λ = 1; for the grid search, we set n0 = 100.

For SVGD we set the initial point-set to be an equally spaced rectangular grid over the bounding box. Following (Liu &
Wang, 2016), the step-size (cid:15) for SVGD was determined by AdaGrad with a master step-size of 0.1 and a momentum factor
of 0.9.

C.3. IGARCH Test

For the IGARCH test, we choose the base scale η to be 1e-5. The sensitivity of results to the selection of kernel parameters
was reported in Figure 11. Point sets obtained under representatives of each method class are shown in Figures 12 and 13.
Detailed results for each method considered are contained in Figure 14.
For all the global optimisation methods we impose a bounding box of (0.002, 0.04) × (0.05, 0.2); for the Nelder-Mead

Stein Points

method, we set ninit = 3, ndelay = 20, µ0 = (0.021, 0.125), Σ0 = diag[(1e-4, 1e-3)], and λ = 1e-5; for the Monte Carlo
method, we set ntest = 20, ndelay = 20, µ0 = (0.021, 0.125), Σ0 = diag[(1e-4, 1e-3)], and λ = 1e-5; for the grid search, we
set n0 = 100.

For SVGD we set the initial point-set to be an equally spaced rectangular grid over the bounding box. Following (Liu &
Wang, 2016), the step-size (cid:15) for SVGD was determined by AdaGrad with a master step-size of 1e-3 and a momentum factor
of 0.9.

Stein Points

Figure 6: Typical point sets obtained in the Gaussian mixture test, where the budget-constrained methods Stein Greedy-100
(Stn Grdy-100) and Stein Herding-100 (Stn Hrd-100) are considered. [Here each row corresponds to an algorithm, and each
column corresponds to a chosen level of computational cost. The left border of each sub-plot is aligned to the exact value of
log neval spent to obtain each point-set.]

Stein Points

(a) Stein Points (Greedy)

(b) Stein Points (Herding)

(c) SVGD

Figure 7: Kernel parameter selection results for the Gaussian process test. Parameters α, β in the kernels k1, k2, k3 were
optimised over a discrete set with respect to the Wasserstein distance WP for a point set of size n = 100. The values log WP
(y-axis) are shown for all different conﬁgurations of parameters (x-axis) considered. Optimal parameter conﬁgurations are
circled and detailed in the legend.

Figure 8: Typical point sets obtained in the Gaussian process test. [Here each row corresponds to an algorithm, and each
column corresponds to a chosen level of computational cost. The left border of each sub-plot is aligned to the exact value of
log neval spent to obtain each point-set. MCMC represents a random-walk Metropolis algorithm with a proposal distribution
optimised according to acceptance rate.]

Stein Points

Figure 9: Typical point sets obtained in the Gaussian process test, where the budget-constrained methods Stein Greedy-100
(Stn Grdy-100) and Stein Herding-100 (Stn Hrd-100) are considered. [Here each row corresponds to an algorithm, and each
column corresponds to a chosen level of computational cost. The left border of each sub-plot is aligned to the exact value of
log neval spent to obtain each point-set. MCMC represents a random-walk Metropolis algorithm with a proposal distribution
optimised according to acceptance rate.]

Stein Points

(a) Monte Carlo

(b) Stein Points (Greedy)

(c) Stein Points (Herding)

(d) SVGD

Figure 10: Results for the Gaussian process test. [Here n = 100. x-axis: log of the number neval of model evaluations
that were used. y-axis: log of the Wasserstein distance WP ({xi}n
) obtained. Kernel parameters α, β were optimised
i=1
according to WP . In sub-ﬁgure 10a, MCMC represents a random-walk Metropolis algorithm with a proposal distribution
optimised according to acceptance rate. MCMC-Thin represents a thinned chain by taking every 100th observation.]

(a) Stein Points (Greedy)

(b) Stein Points (Herding)

(c) SVGD

Figure 11: Kernel parameter selection results for the IGARCH test. Parameters α, β in the kernels k1, k2, k3 were optimised
over a discrete set with respect to the Wasserstein distance WP for a point set of size n = 100. The values log WP (y-axis)
are shown for all different conﬁgurations of parameters (x-axis) considered. Optimal parameter conﬁgurations are circled
and detailed in the legend.

Stein Points

Figure 12: Typical point sets obtained in the IGARCH test. [Here each row corresponds to an algorithm, and each column
corresponds to a chosen level of computational cost. The left border of each sub-plot is aligned to the exact value of
log neval spent to obtain each point-set. MCMC represents a random-walk Metropolis algorithm with a proposal distribution
optimised according to acceptance rate.]

Stein Points

Figure 13: Typical point sets obtained in the IGARCH test, where the budget-constrained methods Stein Greedy-100 (Stn
Grdy-100) and Stein Herding-100 (Stn Hrd-100) are considered. [Here each row corresponds to an algorithm, and each
column corresponds to a chosen level of computational cost. The left border of each sub-plot is aligned to the exact value of
log neval spent to obtain each point-set. MCMC represents a random-walk Metropolis algorithm with a proposal distribution
optimised according to acceptance rate.]

Stein Points

(a) Monte Carlo

(b) Stein Points (Greedy)

(c) Stein Points (Herding)

(d) SVGD

Figure 14: Results for the IGARCH test. [Here n = 100. x-axis: log of the number neval of model evaluations that were used.
) obtained. Kernel parameters α, β were optimised according to WP . In
y-axis: log of the Wasserstein distance WP ({xi}n
i=1
sub-ﬁgure 14a, MCMC represents a random-walk Metropolis algorithm with a proposal distribution optimised according to
acceptance rate. MCMC-Thin represents a thinned chain by taking every 100th observation.]

Stein Points

Wilson Ye Chen 1 Lester Mackey 2 Jackson Gorham 3 Franc¸ois-Xavier Briol 4 5 6 Chris. J. Oates 7 6

8
1
0
2
 
n
u
J
 
9
1
 
 
]

O
C

.
t
a
t
s
[
 
 
4
v
1
6
1
0
1
.
3
0
8
1
:
v
i
X
r
a

Abstract

An important task in computational statistics and
machine learning is to approximate a posterior
distribution p(x) with an empirical measure sup-
ported on a set of representative points {xi}n
i=1.
This paper focuses on methods where the selec-
tion of points is essentially deterministic, with an
emphasis on achieving accurate approximation
when n is small. To this end, we present Stein
Points. The idea is to exploit either a greedy or
a conditional gradient method to iteratively min-
imise a kernel Stein discrepancy between the em-
pirical measure and p(x). Our empirical results
demonstrate that Stein Points enable accurate ap-
proximation of the posterior at modest compu-
tational cost. In addition, theoretical results are
provided to establish convergence of the method.

1. Introduction

This paper is motivated by approximation of a Borel distribu-
tion P , deﬁned on a topological space X, with deterministic
point sets or sequences {xi}n
i=1

⊂ X for n ∈ N, such that

1
n

∑n

i=1 h(xi) → ∫ h dP

(1)

as n → ∞ for all functions h ∶ X → R in a speciﬁed set H.
Throughout it will be assumed that P admits a density p,
with respect to a reference measure, available in a form that
is un-normalised (i.e., we know q(x) in closed form where
p(x) = q(x)/C for some C > 0). Such problems occur
in Bayesian statistics where P represents a posterior distri-
bution, and the integral represents a posterior expectation
of interest. Markov chain Monte Carlo (MCMC) methods
are extensively used for this task but suffer (in terms of

1School of Mathematical and Physical Sciences, University of
Technology Sydney, Australia 2Microsoft Research New England,
USA 3Opendoor Labs, Inc., USA 4Department of Statistics, Uni-
versity of Warwick, UK 5Department of Mathematics, Imperial
College London, UK 6Alan Turing Institute, UK 7School of Math-
ematics, Statistics and Physics, Newcastle University, UK. Cor-
respondence to: Wilson Ye Chen <ye.chen@uts.edu.au>, Lester
Mackey <lmackey@microsoft.com>.

accuracy) from ‘clustering’ of the points {xi}n
i=1 when n is
small. This observation motivates us to instead consider a
range of goal-oriented discrete approximation methods that
are designed with un-normalised densities in mind.

The problem of discrete approximation of a distribution,
given its normalised density, has been considered in detail
and relevant methods include quasi-Monte Carlo (QMC)
(Dick & Pillichshammer, 2010), kernel herding (Chen et al.,
2010; Lacoste-Julien et al., 2015), support points (Mak &
Joseph, 2016; 2017), transport maps (Marzouk et al., 2016),
and minimum energy methods (Johnson et al., 1990). On
the other hand, the question of how to proceed with un-
normalised densities has been primarily answered with in-
creasingly sophisticated MCMC.

At the same time, recent work had led to theoretically-
justiﬁed measures of sample quality in the case of an un-
normalised target. In (Gorham & Mackey, 2015; Mackey
& Gorham, 2016) it was shown that Stein’s method can be
used to construct discrepancy measures that control weak
convergence of an empirical measure to a target. This was
later extended in (Gorham & Mackey, 2017) to encompass
a family of discrepancy measures indexed by a reproducing
kernel. In the latter case, the discrepancy measure can be
recognised as a maximum mean discrepancy (Smola et al.,
2007). As such, one can consider discrete approximation as
an optimisation problem in a Hilbert space and attempt to
optimise this objective with either a greedy or a conditional
gradient method. The resulting method – Stein Points – and
its variants are proposed and studied in this work.

Our Contribution This paper makes the following con-
tributions:

• Two algorithms are proposed for minimisation of the
kernel Stein discrepancy (KSD; Chwialkowski et al.,
2016; Liu et al., 2016; Gorham & Mackey, 2017); a
greedy algorithm and a conditional gradient method.
In each case, a convergence result of the form in Eqn.
1 is established.

• Novel kernels are proposed for the KSD, and we prove
that, with these kernels, the KSD controls weak conver-
gence of the empirical measure to the target. In other
words, the test functions h for which our results hold
constitute a rich set H.

Stein Points

Outline The paper proceeds as follows. In Section 2 we
provide background, and in Section 3 we present the ap-
proximation methods that will be studied. Section 4 applies
these methods to both simulated and real approximation
problems and provides a extensive empirical comparison.
All technical material is contained in Section 5, where we
derive novel theoretical results for the methods we proposed.
Finally we summarise our ﬁndings in Section 6.

2. Background

Throughout this section it will be assumed that X is a metric
space, and we let P(X) denote the collection of Borel
distributions on X. In this context, weak convergence of the
empirical measure to P corresponds to taking the set H in
Eqn. 1 to be the set HCB of functions which are continuous
and bounded. In this work we also consider sets H that
correspond to stronger modes of convergence in P(X).

First, in 2.1, we recall how discrepancy measures are con-
structed. Then we recall the use of Stein’s method in this
context in 2.2. Formulae for KSD are presented in 2.3.

2.1. Discrepancy Measures

A discrepancy is a quantiﬁcation of how well the points
{xi}n
i=1 cover the domain X with respect to the distribution
P . This framework will be developed below in reproducing
kernel Hilbert spaces (RKHS; Hickernell, 1998), but the
general theory of discrepancy can be found in (Dick &
Pillichshammer, 2010). Note that we focus on unweighted
point sets for ease of presentation, but our discussions and
results generalise straightforwardly to point sets that are
weighted.
Let k ∶ X × X → R be the reproducing kernel of a RKHS K
of functions X → R. That is, K is a Hilbert space of func-
tions with inner product ⟨⋅, ⋅⟩K and induced norm ∥ ⋅ ∥K such
that, for all x ∈ X, k(x, ⋅) ∈ K and f (x) = ⟨f, k(x, ⋅)⟩
K
whenever f ∈ K. The Cauchy-Schwarz inequality in K
gives that

∣ 1
n

∑n

i=1 f (xi) − ∫ f dP ∣ ≤ ∥f ∥K DK,P ({xi}n
i=1

)

where the ﬁnal term

DK,P ({xi}n
i=1

) ∶= ∥ 1
n

∑n

i=1 k(xi, ⋅) − ∫ k(x, ⋅)dP (x)∥

K

is the canonical discrepancy measure for the RKHS. The
Bochner integral kP ∶= ∫ k(x, ⋅)dP (x) ∈ K is known as the
mean embedding of P into K (Smola et al., 2007). Thus, if
H = B(K) ∶= {f ∈ K ∶ ∥f ∥K ≤ 1} is the unit ball in K, then
DK,P ({xi}n
) → 0 implies the convergence result in Eqn.
i=1
1.

the fact that, when both kP and kP,P ∶= ∫ kP dP are explicit,
the canonical discrepancy measure is also explicit:

) =

DK,P ({xi}n
i=1
kP,P − 2
n

√

∑n

i=1 kP (xi) + 1

n2 ∑n

i,j=1 k(xi, xj)

(2)

Table 1 in (Briol et al., 2015) collates pairs (k, P ) for which
kP and kP,P are explicit.

If P is a posterior distribution, so that p has unknown nor-
malisation constant, it is unclear how the terms kP and kP,P
can be computed in closed form, and so similarly for the
discrepancy DK,P . This has so far prevented QMC and
related methods such as kernel herding (Chen et al., 2010)
from being used to compute posterior integrals. A solution
to this problem can be found in Stein’s method, presented
next.

2.2. Kernel Stein Discrepancy

The method of Stein (1972) was introduced as an analytical
tool for establishing convergence in distribution of random
variables, but its potential for generating and analyzing com-
putable discrepancies was developed in (Gorham & Mackey,
2015). In what follows, we recall the kernelised version
of the Stein discrepancy, ﬁrst presented for an optimally-
weighted point set in 2.3.3 of (Oates et al., 2017b) and
later generalised to an arbitrarily-weighted point set in
(Chwialkowski et al., 2016; Liu et al., 2016; Gorham &
Mackey, 2017).

Suppose that X carries the structure of a smooth manifold,
and consider a linear differential operator TP on X, together
with a set F of sufﬁciently differentiable functions, with the
following property:

∫ TP [f ] dP = 0 ∀f ∈ F.

(3)

j=1

∥fj∥2
K

Then TP is called a Stein operator and F a Stein set. In
the kernelised version of Stein’s method, the set F is ei-
ther an RKHS K with reproducing kernel k ∶ X × X → R,
or the product Kd, which contains vector-valued functions
f = (f1, . . . , fd) with fj ∈ K and is equipped with a norm1
∥f ∥Kd = (∑d
)1/2. For the case F = K, the im-
age of K under a Stein operator TP is denoted K0 = TP K.
The notation can be justiﬁed since, under appropriate regu-
larity assumptions, the set TP K admits structure from the
reproducing kernel k0(x, x′) = TP TP k(x, x′) (Oates et al.,
2017b). Here TP is the adjoint of the operator TP and acts
on the second argument x′ of the kernel. If instead F = Kd,
then we suppose that TP f = ∑d
TP,jfj so that the set
K0 = TP Kd admits structure from the reproducing kernel
k0(x, x′) = ∑d
TP,jTP,jk(x, x′). In either case, we will

j=1

j=1

The RKHS framework is now standard for QMC analysis
(Dick & Pillichshammer, 2010). Its popularity derives from

1For what follows, any vector norm can be used to combine the

component norms ∥fj∥K (Gorham & Mackey, 2017, Prop. 3).

Stein Points

which at each iteration attempts to minimise the KSD, whilst
the second is a conditional gradient algorithm, known as
herding, which also targets the KSD. In 3.1 and 3.2 the
two algorithms are described, whilst in 3.3 some alternative
approaches are brieﬂy discussed.

√

3.1. Greedy Algorithm

call the reproducing kernel k0 of K0 a Stein reproducing
kernel.

Stein reproducing kernels possess the useful property that
k0,P = ∫ k0(x, ⋅)dP = 0 and k0,P,P = ∫ k0,P dP = 0, so in
particular both are explicit. Thus, if k0 is a Stein reproduc-
ing kernel, then Eqn. 2 can be simpliﬁed:

DK0,P ({xi}n
i=1

) =

1

n2 ∑n

i,j=1 k0(xi, xj).

(4)

We call this quantity a kernel Stein discrepancy (KSD). Next,
we exhibit some differential operators for which Eqn. 3 is
satisﬁed and Eqn. 4 can be computed.

2.3. Stein Operators and Their Reproducing Kernels

The divergence theorem can be used to construct Stein op-
erators on a manifold. For P supported on X = Rd, (Oates
et al., 2017b; Gorham & Mackey, 2015; Chwialkowski et al.,
2016; Liu et al., 2016; Gorham & Mackey, 2017) considered
the Langevin Stein operator

TP f

∶= ∇⋅(pf )

p

where ∇⋅ is the usual divergence operator and f ∈ Kd. Thus,
for the Langevin Stein operator, we obtain a Stein reproduc-
ing kernel

k0(x, x′) = ∇x ⋅ ∇x′ k(x, x′)

(5)

(6)

+∇xk(x, x′) ⋅ ∇x′ log p(x′)
+∇x′k(x, x′) ⋅ ∇x log p(x)
+k(x, x′)∇x log p(x) ⋅ ∇x′ log p(x′).

To evaluate this kernel, the normalisation constant for p is
not required. Other Stein operators for the Euclidean case
were developed in (Gorham et al., 2016). For P supported
on a closed Riemannian manifold X, (Oates et al., 2017a;
Liu & Zhu, 2017) proposed the second order Stein operator
TP f ∶= 1
∇ ⋅ (p∇f ) where ∇ and ∇⋅ are, respectively, the
p
gradient and divergence operators on the manifold and f ∈
K. Other Stein operators for the general case are proposed
in the supplement of (Oates et al., 2017a).

The theoretical results in (Gorham & Mackey, 2017) estab-
lished that certain combinations of Stein operator TP and
base kernel k ensure that KSD controls weak convergence;
that is, DK0,P ({xi}n
) → 0 implies that Eqn. 1 holds with
i=1
H = HCB. This important result motivates our next con-
tribution, where numerical optimisation methods are used
to select points {xi}n
i=1 to approximately minimise KSD.
Theoretical analysis of the proposed methods is reserved for
Section 5.

3. Methods

In this paper, two algorithms to select points {xi}n
i=1 are
studied in detail. The ﬁrst of these is a greedy algorithm,

The simplest algorithm that we consider follows a greedy
strategy, whereby the ﬁrst point x1 is taken to be a global
maximum of p (an operation which does not require the nor-
malisation constant) and each subsequent point xn is taken
to be a global maximum of DK0,P ({xi}n
), with the KSD
i=1
being viewed as a function of xn holding {xi}n−1
i=1 ﬁxed.
Equivalently, at iteration n > 1 of the greedy algorithm, we
select

xn

∈ arg minx∈X

k0(x,x)
2

+ ∑n−1

i=1 k0(xi, x).

(7)

Note that each iteration of the algorithm requires the solution
of a global optimisation problem over X; in practice we
employed a numerical optimisation method, and this choice
is discussed in detail in connection with the empirical results
in Section 4 and the theoretical results in Section 5.

If a user has a budget of at most n points, the greedy algo-
rithm can be run for n iterations and thereafter improved
using (block) coordinate descent on the KSD objective to
update an existing point xi instead of introducing a new
point. The cost of each update is equal to the cost of adding
the n-th greedy Stein Point. This budget-constrained variant
of the method will be called Stein Greedy-n in the sequel
(see Section B.1.3 for more details).

3.2. Herding Algorithm

The deﬁnition of discrepancy in Section 2.1 suggests that
selection of {xi}n
i=1 can be elegantly formulated as a single
global optimisation problem over K0. Let M (K0) be the
marginal polytope of K0; i.e.
the convex hull of the set
{k0(x, ⋅)}x∈X (Wainwright & Jordan, 2008). The mean
embedding Q ↦ kQ, as a map P(X) → M (K), is injective
whenever the kernel k is universal and X is compact (Smola
et al., 2007), so that in this case kQ fully characterises Q.
Results in a similar direction for Stein reproducing kernels
were established in Chwialkowski et al. (2016, Theorem 2.1)
and Liu et al. (2016, Proposition 3.3). Thus, as P is mapped
to 0 under the embedding, we are motivated to consider
non-trivial solutions to

arg minf ∈M (K0) J(f ),

J(f ) ∶= 1
2

∥f ∥2
K0

.

(8)

As might be expected, the objective function is closely re-
∑n
i=1 k0(xi, ⋅) we have J(f ) =
lated to KSD; for f (⋅) = 1
n
2 DK0,P ({xi}n
)2. An iterative algorithm, called kernel
1
herding, was proposed in (Chen et al., 2010) to solve prob-
lems in the form of Eqn. 8. This was later shown to be

i=1

Stein Points

equivalent to a conditional gradient algorithm, the Frank-
Wolfe algorithm, in (Bach et al., 2012). The canonical Frank-
Wolfe algorithm, which results in an unweighted point set
(as opposed to a more general weighted point set; Bach
et al., 2012), is presented next.

of our approach would be to optimise KSD for n ﬁxed. This
approach was considered for other discrepancy measures in
(Oettershagen, 2017), where the Newton method was used.
We instead employ our budget-constrained algorithms Stein
Greedy-n and Stein Herding-n for this use case.

The ﬁrst point x1 is again taken to be a global maximum of
p; this corresponds to an element f1 = k0(x1, ⋅) ∈ M (K0).
Then, at iteration n > 1, the convex combination fn =
n fn−1 + 1
∈ M (K0) is constructed where the element
n−1
f ∗
n encodes a direction of steepest descent:

n f ∗

n

fn

∈ arg minf ∈M (K0)

⟨f, DJ(fn−1)⟩

,

K0

where DJ(f ) is the representer of the Fr´echet derivative of
J at f . Given that minimisation of a linear objective over
a convex set can be restricted to the boundary of that set,
= k(xn, ⋅) for some xn ∈ X. Thus, at
it follows that f ∗
n
iteration n > 1 of the proposed algorithm, we select

xn

∈ arg minx∈X ∑n−1

i=1 k0(xi, x)

(9)

∑n

to obtain fn(⋅) = 1
i=1 k0(xi, ⋅), the embedding of the
n
empirical distribution of {xi}n
i=1. As in the standard kernel
herding algorithm of (Chen et al., 2010), each iteration
in practice requires the solution of a global optimisation
problem over X.

Compared to Eqn. 7, the greedy algorithm is seen to be a
2 k0(x, x).
regularised version of herding with regulariser 1
The two algorithms coincide if k0(x, x) is independent of
x; however, this is typically not true for a Stein reproducing
kernel. The computational cost of either method is O(n2);
thus we anticipate applications in which evaluation of p(x)
(and its gradient) constitute the principal computational
bottleneck. The performance of both algorithms is studied
empirically in Section 4 and theoretically in Section 5. In
a similar manner to Stein Greedy-n, a budget-constrained
variant of the above method can be considered, which we
call Stein Herding-n in the sequel.

3.3. Other Algorithms

The output of either of our algorithms will be called Stein
Points. These are extensible point sequence Sn = (xi)n
i=1,
meaning that Sn can be incrementally extended Sn =
(Sn−1, xn) as required. Another recently proposed exten-
sible method is the (sequential) minimum energy design
(MED) of (Joseph et al., 2015; 2017), here used as a bench-
mark.

4. Results

In this section, the proposed greedy and herding algorithms
are empirically assessed and compared. In 4.2 a Gaussian
mixture problem is studied in detail, whilst in 4.3 and 4.4,
respectively, the methods are applied to approximate the
parameter posterior in a non-parametric regression model
and an IGARCH model. First, in 4.1 we provide details on
the experimental protocol.

4.1. Experimental Protocol

Here we describe the parameters and settings that were
varied in the experiments that are presented.

Stein Operator To limit scope, we focus on the case X =
Rd and always take TP to be the Langevin Stein operator in
Eqn. 5.

Choice of Kernel For the kernel k in Eqn. 6 we con-
sidered one standard choice – the inverse multi-quadratic
(IMQ) kernel – together with two novel alternatives:

(k1) (IMQ) k1(x, x′) = (α + ∥x − x′∥2
2

)β

(k2) (inverse log) k2(x, x′) = (α + log(1 + ∥x − x′∥2
2

))−1

(k3) (IMQ score)

k3(x, x′) = (α + ∥∇ log p(x) − ∇ log p(x′)∥2
2

)β

.

In all cases α > 0 and β ∈ (−1, 0). To limit scope, in
what follows we considered a ﬁnite number of judiciously
selected conﬁgurations for α, β, though in principle these
could be optimised as in (Jitkrittum et al., 2017). The best
set of parameter values was selected for each algorithm and
each target distribution, where the possible values were α ∈
{0.1η, 0.5η, η, 2η, 4η, 8η} and β ∈ {0.1, 0.3, 0.5, 0.7, 0.9},
with η > 0 problem-dependent (see the Supplement). The
IMQ kernel, together with the Langevin Stein operator, was
proven in Gorham & Mackey (2017, Theorem 8) to provide
a KSD that controls weak convergence. Similar results for
novel kernels k2 and k3 are established in Section 5.

For some problems the number of points n will be ﬁxed
in advance and the aim will instead be to select a single
optimal point set {xi}n
i=1. This alternative problem demands
different methodologies, and a promising method in this
direction is Stein variational gradient descent (SVGD-n;
Liu & Wang, 2016; Liu, 2017). A natural point set analogue

Numerical Optimisation Method Any optimisation pro-
cedure could be used to (approximately) solve the global
optimisation problem embedded in each iteration of the pro-
posed algorithms. In our experiments, we considered the
following numerical methods, for which full details appear
in the Section B.2.

Stein Points

1. Nelder-Mead (NM): At iteration n, parallel runs of
Nelder-Mead were employed, initialised at draws from
a Gaussian mixture proposal centred on the current
point set Π =
N (xi, λI) with problem-
speciﬁc λ > 0.

∑n−1
i=1

1
n−1

2. Monte Carlo (MC): The optimisation problem at itera-
tion n was solved over a sample of points drawn from
the same Gaussian mixture proposal Π.

3. Grid search (GS): Through brute force, the optimisa-
tion problem at iteration n was solved over a regular
2 ) points; if
grid of width 1√
required, the domain was ﬁrst truncated with a large
bounding box.

n . This required O(n− d

Performance Assessment To obtain a reasonably objec-
tive assessment, we focused on the 1-Wasserstein distance
between the empirical measure and P :

WP ({xi}n
i=1

) = suph∈HLip

∣ 1
n

∑n

i=1 h(xi) − ∫ hdP ∣ ,

i=1 δyi for yi

2 log N rate for d = 2 and N − 1

where HLip is the set of all function h ∶ X → R with Lip-
schitz constant Lip(h) ≤ 1. By replacing P with the em-
iid∼ P , the expected
∑N
pirical measure PN = 1
N
)
) in lieu of WP ({xi}n
({xi}n
error from using WPN
i=1
i=1
converges at a N − 1
d rate
for d > 2 (Fournier & Guillin, 2015). By employing L1-
) can be com-
spanners, the approximation WPN
puted in O((n + N )2 log2d−1(n + N )) time (Gudmundsson
et al., 2007). For all reported results, the {yi}N
i=1 were ob-
tained by brute-force Monte Carlo methods applied to P ,
with N sufﬁciently large that approximation error can be
neglected.

({xi}n
i=1

The computational cost associated to any given method was
quantiﬁed as the total number neval of times either the log-
density log p or its gradient ∇ log p were evaluated. This
can be justiﬁed since in most applications the ‘parameter to
data’ map dominates the computational cost associated with
the likelihood.

Benchmarks Two existing methods were used as a bench-
mark:

1. The MED method of (Joseph et al., 2015; 2017) relies
on numerical optimisation methods to minimise an
), adapted to P . This
energy measure Eδ,P ({xi}n
i=1
measure has one tuning parameter δ ∈ [1, ∞). See
Section B.1.1 of the Supplement for full detail.

2. The SVGD method of (Liu & Wang, 2016; Liu, 2017)
performs a version of gradient descent on the Kullback-
Leibler divergence, described in Section B.1.2 of the
Supplement.

To avoid confounding of the empirical results by incompa-
rable algorithm parameters, (1) the collection of numerical
optimisation methods used for KSD were also used for
MED, and (2) the same collection of kernels k1, . . . , k3 was
considered for SVGD as was used for KSD. Note that, apart
from standard Monte Carlo, none of the methods considered
in these experiments are re-parametrisation invariant.

4.2. Gaussian Mixture Test

For our ﬁrst test, we considered a Gaussian mixture model

P =

1
2

N (µ1, Σ1) + 1
2

N (µ2, Σ2)

deﬁned on X = R2. Full settings for each of the methods
considered are detailed in Section C.1 in the Supplement.
Typical point sets are displayed over the contours of P for
µ1 = (−1.5, 0), µ2 = (1.5, 0), Σ1 = Σ2 = I in Figure 1. Ad-
ditionally, point sets for the n point budget-constrained al-
gorithms Stein Greedy-n and Stein Herding-n are presented
in Figure 6 in the Supplement. For each of the methods
shown in Figures 1 and 6, tuning parameters were varied
and the overall performance was captured in Figure 2. It was
observed that for (a-c) the choice of numerical optimisation
method was the most inﬂuential tuning parameter, with the
simpler Monte Carlo-based method being most successful.
The kernels k1, k2 were seen to perform well, but in (a,b,d)
the kernel k3 was sometimes seen to fail.

A subjectively-selected exemplar was extracted for each
method, and these ‘best’ results for each method are overlaid
in Figure 3. The total number of points was limited to
n = 100. In terms of our proposed methods, two qualitative
regimes were observed: (i) For low computational budget
log neval ≤ 7, the standard Monte Carlo method performed
best. (ii) For a larger computational budget 7 < log neval,
greedy Stein points were not out-performed.

Note that KSD and SVGD are based on the log target and
its gradient, whilst for MED the target p(x) itself is re-
quired. As a result, numerical instabilities were sometimes
encountered with MED.

Next, we turned our attention to two important posterior
approximation problems that occur in the real world.

4.3. Gaussian Process Regression Model

The Gaussian process (GP) model is a popular choice
for uncertainty quantiﬁcation in the non-parametric regres-
sion context (Rasmussen & Williams, 2006). The data
D = {(xi, yi)}n
i=1 that we considered are from a light de-
tection and ranging (LIDAR) experiment (Ruppert et al.,
2003). They consist of 221 realisations of an independent
scalar variable xi (distances travelled before the light is
reﬂected back to its source) and a dependent scalar vari-
able yi (log-ratios of received light from two laser sources);

Stein Points

(a) Stein Points (Greedy)

(b) Stein Points (Herding)

(c) MED

(d) SVGD

Figure 2: Results for the Gaussian mixture test.
[Here
n = 100. x-axis: log of the number neval of model evalu-
ations that were used. y-axis: log of the Wasserstein dis-
) obtained. Kernel parameters α, β were
tance WP ({xi}n
i=1
optimised according to WP in all cases, with sensitivities
reported in Fig. 7 of the Supplement.]

Figure 1: Typical point sets obtained in the Gaussian mix-
ture test. [Here the left border of each sub-plot is aligned to
the exact value of log neval spent to obtain each point set.]

i.i.d.∼ N (0, σ2)
these were modelled as yi = g(xi) + (cid:15)i, for (cid:15)i
and a known value of σ. The unknown regression function
g is modelled as a centred GP with covariance function
cov(x, x′) = θ1 exp(−θ2(x − x′)2). The hyper-parameters
θ1, θ2 > 0 determine the suitability of the GP model, but
appropriate values will be unknown in general. In this ex-
periment we re-parametrised φi = log θi and placed a stan-
dard multivariate Cauchy prior on φ = (φ1, φ2), deﬁned
on X = R2. The task is thus to approximate the condi-
tional distribution p(φ∣D). This problem is motivated by
the computation of posterior predictive marginal distribu-
tions p(y∗∣x∗, D) for a new input x∗, which is deﬁned as
the integral ∫ p(y∗∣x∗, φ, D)p(φ∣D)dφ. Note that the den-
sity p(φ∣D) can be differentiated, and an explicit formula is
provided in Rasmussen & Williams (2006, Eqn. 5.9).

For each class of method, ‘best’ tuning parameters were se-
lected and these are presented on the same plot in Figure 4a.
In addition, typical point sets provided by each method are
presented in Figures 8 and 9 in the Supplement. MED was
not included because the method exhibited severe numerical
instability on this task, as earlier discussed. Results indi-
cated three qualitative regimes where, respectively, Monte
Carlo, greedy Stein points and SVGD provided the best

Figure 3: Combined results for the Gaussian mixture test.
[Here n = 100. x-axis: log of the number neval of model
evaluations that were used. y-axis: log of the the Wasser-
stein distance WP ({xi}n
) obtained. Tuning parameters
i=1
were selected to minimise WP , as described in the main text.
The dashed line indicates the point at which n Stein Points
have been generated; block coordinate descent is performed
thereafter to satisfy the n point budget constraint.]

performance for ﬁxed cost.

4.4. IGARCH Model

The integrated generalised autoregressive conditional het-
eroskedasticity (IGARCH) model is widely-used to describe

Stein Points

ﬁnancial time series (yt) with time-varying volatility (σt)
(Taylor, 2011). The model is as follows:

yt = σt(cid:15)t,
σ2
t

= θ1 + θ2y2

i.i.d.∼ N (0, 1)

(cid:15)t

+ (1 − θ2)σ2

t−1

t−1
with parameters θ = (θ1, θ2), θ1 > 0 and 0 < θ2 < 1. The
data y = (yt) that we considered were 2,000 daily percent-
age returns of the S&P 500 stock index (from December 6,
2005 to November 14, 2013), and an improper uniform prior
was placed on θ. Thus the task was to approximate the pos-
terior p(θ∣y). Note that, whilst the domain X = R+ × (0, 1)
is bounded, for these data the posterior density is negligi-
ble on the boundary ∂X. This ensures that Eqn. 3 holds
essentially to machine precision; see also the discussion in
Oates et al. (2018, Section 3.2). For the IGARCH model,
gradients ∇ log p(θ∣y) can be obtained as the solution of a
recursive system of equations for ∂σt/∂θ2.

As before, the ‘best’ performing of each class of method was
selected and these are presented on the same plot in Figure
4b. In addition, typical point sets provided by each method
are presented in Figures 12 and 13 in the Supplement. (Nu-
merical instability again prevented results for MED from
being obtained.) Results were consistent with the Gaussian
mixture experiment, favouring either Monte Carlo or greedy
Stein points depending on the computational budget.

5. Theoretical Results

(1) discrepancy control,

In this section we establish two important forms of
i.e.,
theoretical guarantees:
DK0,P ({xi}n
) → 0 as n → ∞ for our extensible Stein
i=1
Point sequences and (2) distributional convergence control,
i.e., for our kernel choices and appropriate choices of target,
) → 0 implies that the empirical distribution
DK0,P ({xi}n
i=1
1
n

i=1 δxi converges in distribution to P .

∑n

5.1. Discrepancy Control

Earlier work has shown that, when a kernel is uniformly
bounded (i.e., supx∈X k0(x, x) ≤ R2), the greedy and ker-
nel herding algorithms decrease the associated discrepancy
DK0,P at an O(n− 1
2 ) rate (Lacoste-Julien et al., 2015; Jones,
1992). We extend these results to cover all growing, P -sub-
exponential kernels.
Deﬁnition 1 (P -sub-exponential reproducing kernel). We
say a reproducing kernel k0 is P -sub-exponential if

PZ∼P [k0(Z, Z) ≥ t] ≤ c1e−c2t

for some constants c1, c2 > 0 and all t ≥ 0.

Notably, any uniformly bounded reproducing kernel is
P -sub-exponential, and, when P is a sub-Gaussian dis-
tribution, any kernel with at most quadratic growth (i.e.,

k0(x, x) = O(∥x∥2
)) is also P -sub-exponential. Our ﬁrst
2
result, proved in Section A.1.1, shows that if we truncate
the search domain suitably in each step, Stein Herding de-
creases the discrepancy at an O(
log(n)/n) rate. This
result holds even if each point xi is selected suboptimally
with error δ/2. This extra degree of freedom allows a user
to conduct a grid search or a search over appropriately gen-
erated random points on each step (see, e.g., Lacoste-Julien
et al., 2015) and still obtain a rate of convergence.

√

Theorem 1 (Stein Herding Convergence). Suppose k0 with
k0,P = 0 is a P -sub-exponential reproducing kernel. Then
there exist constants c1, c2 > 0 depending only on k0 and P
such that any point sequence {xi}n

i=1 satisfying

∑j−1

i=1 k0(xi, xj) ≤ δ

2

+

min
x∈X∶k0(x,x)≤R2
j

∑j−1

i=1 k0(xi, x)

with k0(xj, xj) ≤ R2
j
1 ≤ j ≤ n also satisﬁes

∈ [2 log(j)/c2, 2 log(n)/c2] for each

DK0,P ({xi}n
i=1

) ≤ eπ/2

2 log(n)
c2n

+ c1
n

+ δ
n .

√

√

Our next result, proved in Section A.1.2, shows that Stein
Greedy decreases the discrepancy at an O(
log(n)/n) rate
whether we choose to truncate (Rj < ∞) or not (Rj = ∞).
This highlights an advantage of the Stein Greedy algorithm
the extra k0(x, x)/2 term acts as a
over Stein Herding:
regularizer ensuring that no truncation is necessary. The
result also accommodates points xi selected suboptimally
with error δ/2.
Theorem 2 (Stein Greedy Convergence). Suppose k0 with
k0,P = 0 is a P -sub-exponential reproducing kernel. Then
there exist constants c1, c2 > 0 depending only on k0 and P
such that any point sequence {xi}n

i=1 satisfying

k0(xj ,xj )
2

+ ∑j−1

i=1 k0(xi, xj)
+

min
x∈X∶k0(x,x)≤R2
j

≤ δ
2

k0(x,x)
2

+ ∑j−1

i=1 k0(xi, x)

2 log(j)/c2 ≤ Rj ≤ ∞ for each 1 ≤ j ≤ n also

√

with
satisﬁes

DK0,P ({xi}n
i=1

) ≤ eπ/2

2 log(n)
c2n

+ c1
n

+ δ
n .

√

5.2. Distributional Convergence Control

To present our ﬁnal results, we overload notation to deﬁne
the KSD associated with any probability measure µ:
√

DK0,P (µ) =

E(Z,Z′)∼µ×µ [k0(Z, Z ′)].

Our original DK0,P deﬁnition (Eq. 4) for a point set {xi}n
i=1
is recovered when µ is the empirical measure 1
i=1 δxi.
n

∑n

Stein Points

(a) Gaussian Process Test

(b) IGARCH Test

Figure 4: Combined results for the (a) Gaussian process test and (b) IGARCH test. [Here n = 100. x-axis: log of the
number neval of model evaluations that were used. y-axis: log of the Wasserstein distance WP ({xi}n
) obtained. Tuning
i=1
parameters were selected to minimise WP , as described in the main text. The dashed line indicates the point at which n
Stein Points have been generated; block coordinate descent is performed thereafter to satisfy the n point budget constraint.]

We also write µm ⇒ P to indicate that a sequence of proba-
bility measures (µm)∞

m=1 converges in distribution to P .

Gorham & Mackey (2017, Thm. 8) showed that KSDs
with IMQ base kernel (k1) and Langevin Stein opera-
tor TP control distributional convergence whenever P be-
longs to the set P of distantly dissipative distributions
(i.e., ⟨∇ log p(x) − ∇ log p(y), x − y⟩ ≤ −κ ∥x − y∥2
+C for
2
some C ≥ 0, κ > 0) with Lipschitz ∇ log p. Surprisingly,
Gaussian, Mat´ern, and other kernels with light tails do not
satisfy this property (Gorham & Mackey, 2017, Thm. 6).

Our next theorem establishes distributional convergence
control for our newly introduced log inverse kernel (k2).
Theorem 3 (Log Inverse KSD Controls Convergence).
Suppose P ∈ P. Consider a Stein reproducing kernel
k0 = TP TP k2 with Langevin operator TP and base ker-
nel k2(x, x′) = (α + log(1 + ∥x − x′∥2
))β for α > 0 and
2
β < 0. If DK0,P (µm) → 0, then µm ⇒ P .

Our ﬁnal theorem, proved in Section A.3, guarantees distri-
butional convergence control for the new IMQ score kernel
(k3) under the additional assumption that log p is strictly
concave.

Theorem 4 (IMQ Score KSD Controls Convergence). Sup-
pose P ∈ P has strictly concave log density. Con-
sider a Stein reproducing kernel k0 = TP TP k3 with
Langevin operator TP and base kernel k3(x, x′) = (c2 +
∥∇ log p(x) − ∇ log p(x′)∥2
)β for c > 0 and β ∈ (−1, 0). If
2
DK0,P (µm) → 0, then µm ⇒ P .

6. Conclusion

This paper proposed and studied Stein Points, extensible
point sequences rooted in minimisation of a KSD, build-

ing on the recent theoretical work of (Gorham & Mackey,
2017). Although we focused on KSD to limit scope, our
methods could in fact be applied to any computable Stein
discrepancy, even those not based on reproducing kernels
(see, e.g., Gorham & Mackey, 2015; Gorham et al., 2016).
Stein Points provide an interesting counterpoint to other re-
cent work focussing on point sequences (Joseph et al., 2015;
2017) and point sets (Liu & Wang, 2016; Liu, 2017). More-
over, when X is a ﬁnite set {yi}N
i=1 (e.g., an inexpensive
initial point set generated by MCMC), Stein Points provide
a compact and convergent approximation to the optimally
weighted probability measure ∑N
i=1 wiδyi with minimum
KSD (see Section B.3 for more details).

Theoretical results were provided which guarantee the
asymptotic correctness of our methods. However, we were
√
only able to establish an O(
log(n)/n) rate, which leaves
a theoretical gap between the faster convergence that was
sometimes empirically observed. Relatedly, the O(n2) com-
putational cost could be reduced to O(n) by using ﬁnite-
dimensional kernels (see, e.g., Jitkrittum et al., 2017), but
the associated distributional convergence control results
must ﬁrst be developed.

Our experiments were relatively comprehensive, but we did
not consider other Stein operators, nor higher-dimensional
or non-Euclidean manifolds X. Related methods not consid-
ered in this work include those based on optimal transport
(Marzouk et al., 2016) and self-avoiding particle-based sam-
plers (Robert & Mengersen, 2003). The comparison against
these methods is left for future work.

Stein Points

Acknowledgements

WYC was supported by the ARC Centre of Excellence
for Mathematical and Statistical Frontiers. FXB was sup-
ported by EPSRC [EP/L016710/1, EP/R018413/1]. CJO
was supported by the Lloyd’s Register Foundation pro-
gramme on data-centric engineering at the Alan Turing
Institute, UK. This material was based upon work partially
supported by the National Science Foundation under Grant
DMS-1127914 to the Statistical and Applied Mathematical
Sciences Institute. Any opinions, ﬁndings, and conclusions
or recommendations expressed in this material are those of
the author(s) and do not necessarily reﬂect the views of the
National Science Foundation.

References

Bach, F., Lacoste-Julien, S., and Obozinski, G. On the
equivalence between herding and conditional gradient
In Proceedings of the 29th International
algorithms.
Conference on Machine Learning, pp. 1355–1362, 2012.

Baker, J.

Integration of radial functions. Mathematics

Magazine, 72(5):392–395, 1999.

Briol, F-X, Oates, CJ, Girolami, M, Osborne, MA, and Se-
jdinovic, D. Probabilistic integration: A role in statistical
computation? arXiv:1512.00933, 2015.

Chen, Y., Welling, M., and Smola, A. Super-samples from
kernel herding. In Proceedings of the 26th Conference on
Uncertainty in Artiﬁcial Intelligence, 2010.

Chwialkowski, K., Strathmann, H., and Gretton, A. A kernel
test of goodness of ﬁt. In Proceedings of the 33rd Inter-
national Conference on Machine Learning, volume 48,
pp. 2606–2615, 2016.

Dick, J. and Pillichshammer, F. Digital Nets and Sequences.
Discrepancy Theory and Quasi-Monte Carlo Integration.
Cambridge University Press, 2010.

Fournier, N. and Guillin, A. On the rate of convergence in
Wasserstein distance of the empirical measure. Probabil-
ity Theory Related Fields, 162(3-4):707–738, 2015.

Gorham, J. and Mackey, L. Measuring sample quality with
Stein’s method. In Advances in Neural Information Pro-
cessing Systems, pp. 226–234, 2015.

Gorham, J. and Mackey, L. Measuring sample quality with
kernels. In Proceedings of the 34th International Confer-
ence on Machine Learning, pp. 1292–1301, 2017.

Gorham, J., Duncan, A.B., Vollmer, S.J., and Mackey,
Measuring sample quality with diffusions.

L.
arXiv:1611.06972, 2016.

Gudmundsson, J., Klein, O., Knauer, C., and Smid, M.
Small Manhattan networks and algorithmic applications
In Proceedings of the
for the earth movers distance.
23rd European Workshop on Computational Geometry,
pp. 174–177, 2007.

Hickernell, F. A generalized discrepancy and quadrature
error bound. Mathematics of Computation, 67(221):299–
322, 1998.

Jitkrittum, W., Xu, W., Szabo, Z., Fukumizu, K., and Gret-
ton, A. A linear-time kernel goodness-of-ﬁt test.
In
Advances in Neural Information Processing Systems, pp.
261–270, 2017.

Johnson, M.E., Moore, L.M., and Ylvisaker, D. Minimax
and maximin distance designs. Journal of Statistical
Planning and Inference, 26(2):131–148, 1990.

Jones, L.K. A simple lemma on greedy approximation
in Hilbert space and convergence rates for projection
pursuit regression and neural network training. Annals of
Statistics, 20(1):608–613, 1992.

Joseph, V.R., Dasgupta, T., Tuo, R., and Wu, C.F.J. Se-
quential exploration of complex surfaces using minimum
energy designs. Technometrics, 57(1):64–74, 2015.

Joseph, V.R., Wang, D., Gu, L., Lv, S., and Tuo, R. Deter-
ministic sampling of expensive posteriors using minimum
energy designs. arXiv:1712.08929, 2017.

Joshi, K.D. Introduction to General Topology. New Age

International, 1983.

Lacoste-Julien, S., Lindsten, F., and Bach, F. Sequential
kernel herding : Frank-Wolfe optimization for particle
ﬁltering. In Proceedings of the 18th International Confer-
ence on Artiﬁcial Intelligence and Statistics, pp. 544–552,
2015.

Liu, C. and Zhu, J. Riemannian Stein variational gradient
descent for Bayesian inference. arXiv:1711.11216, 2017.

Liu, Q. Stein variational gradient descent as gradient ﬂow.
In Advances in Neural Information Processing Systems,
pp. 3118–3126, 2017.

Liu, Q. and Wang, D. Stein variational gradient descent:
A general purpose Bayesian inference algorithm. In Ad-
vances In Neural Information Processing Systems, pp.
2370–2378, 2016.

Liu, Q., Lee, J., and Jordan, M. A kernelized Stein dis-
crepancy for goodness-of-ﬁt tests. In Proceedings of the
33rd International Conference on Machine Learning, pp.
276–284, 2016.

Mackey, L. and Gorham, J. Multivariate Stein factors for
a class of strongly log-concave distributions. Electronic
Communications in Probability, 21(56), 2016.

Spivak, M. Calculus on Manifolds: A Modern Approach
to Classical Theorems of Advanced Calculus. Westview
Press, 1965.

Stein Points

Stein, C. A bound for the error in the normal approxima-
tion to the distribution of a sum of dependent random
variables. In Proceedings of the Sixth Berkeley Sympo-
sium on Mathematical Statistics and Probability, Volume
2: Probability Theory. The Regents of the University of
California, 1972.

Steinwart, I. and Christmann, A. Support Vector Machines.

Springer Science & Business Media, 2008.

Taylor, S.J. Asset Price Dynamics, Volatility, and Prediction.

Princeton University Press, 2011.

Villani, C. Optimal Transport, Old and New. Number
338 in Fundamental Principles of Mathematical Sciences.
Springer-Verlag, 2009.

Wainwright, M. High-Dimensional Statistics: A Non-
Asymptotic Viewpoint. 2017. URL https://www.
stat.berkeley.edu/˜mjwain/stat210b/
Chap2_TailBounds_Jan22_2015.pdf.

Wainwright, M. J. and Jordan, M. I. Graphical models, ex-
ponential families, and variational inference. Foundations
and Trends in Machine Learning, 1(1–2):1–305, 2008.

Wendland, H. Scattered Data Approximation. Cambridge

University Press, 2004.

Mak, S and Joseph, V R. Support points. arXiv:1609.01811,

2016.

Mak, S and Joseph, V R. Projected support points, with ap-
plication to optimal MCMC reduction. arXiv:1708.06897,
2017.

Marzouk, Y., Moselhy, T., Parno, M., and Spantini, A. Hand-
book of Uncertainty Quantiﬁcation, chapter Sampling via
Measure Transport: An Introduction. Springer, 2016.

Nelder, J.A. and Mead, R. A simplex method for func-
tion minimization. The Computer Journal, 7(4):308–313,
1965.

Oates, C.J., Barp, A., and Girolami, M. Posterior integration
on a Riemannian manifold. arXiv:1712.01793, 2017a.

Oates, C.J., Girolami, M., and Chopin, N. Control func-
tionals for Monte Carlo integration. Journal of the Royal
Statistical Society: Series B, 79(3):695–718, 2017b.

Oates, C.J., Cockayne, J., Briol, F-X., and Girolami, M.
Convergence rates for a class of estimators based on
Stein’s identity. Bernoulli, 2018. To appear.

Oettershagen, J. Construction of Optimal Cubature Algo-
rithms with Applications to Econometrics and Uncer-
tainty Quantiﬁcation. PhD thesis, University of Bonn,
2017.

Rasmussen, C. and Williams, C. Gaussian Processes for

Machine Learning. MIT Press, 2006.

Robert, C.P. and Mengersen, K.L.

IID sampling with
self-avoiding particle ﬁlters: The pinball sampler.
In
Bayesian Statistics, volume 7, chapter IID sampling with
self-avoiding particle ﬁlters: The pinball sampler. Oxford
University Press, 2003. Eds. Bernardo, J., Bayarri, M.,
Berger, J., Dawid, A., Heckerman, D., Smith, A., West,
M.

Ruppert, D., Wand, M.P., and Carroll, R.J. Semiparametric
Regression. Number 12. Cambridge Series in Statistical
and Probabilistic Mathematics, 2003.

Sejdinovic, D., Sriperumbudur, B., Gretton, A., and Fuku-
mizu, K. Equivalence of distance-based and RKHS-based
statistics in hypothesis testing. Annals of Statistics, 41(5):
2263–2291, 2013.

Smola, A., Gretton, A., Song, L., and Sch¨olkopf, B. A
Hilbert space embedding for distributions. In Proceed-
ings of the 18th International Conference on Algorithmic
Learning Theory, pp. 13–31, 2007.

Stein Points

Supplement
This electronic supplement is organised as follows: In Section A proofs for the theoretical results in the main text are
provided. In Section B we provide details for the two existing methods (MED, SVGD) that formed our experimental
benchmark. Then, in Section C, we provide additional numerical results that elaborate on those reported in the main text.

Code Code to reproduce these experiments is available from:

github.com/wilson-ye-chen/stein_points

A. Proof of Theoretical Results in the Main Text

A.1. Proofs of Theorems 1 and 2: Stein Herding and Stein Greedy Convergence

We will show that both Theorem 1 and Theorem 2 follow from the following uniﬁed Stein Point convergence result, proved
in Section A.1.3.
Theorem 5 (Stein Point Convergence). Suppose k0 with k0,P = 0 is a P -sub-exponential reproducing kernel. Then there
exist constants c1, c2 > 0 depending only on k and P such that any point sequence {xi}n

i=1 satisfying

k0(xj, xj)
2

+

j−1
∑
i=1

k0(xi, xj) ≤ δ
2

+

S2
j
2

+

min
x∈X∶k0(x,x)≤S2
j

j−1
∑
i=1

k0(xi, x)

√

√

with Sj ∈ [

2 log(j)/c2,

2 log(n)/c2] for each 1 ≤ j ≤ n and δ ≥ 0 also satisﬁes

DK0,P ({xi}n
i=1

) ≤ eπ/2

2 log(n)
c2n

+ c1
n

+ δ
n .

√

A.1.1. PROOF OF THEOREM 1: STEIN HERDING CONVERGENCE

Instantiate the constants c1, c2 > 0 from Theorem 5, and consider any point sequence {xi}n

i=1 satisfying

j−1
∑
i=1

k0(xi, xj) ≤ δ
2

+

min
x∈X∶k0(x,x)≤R2
j

∑j−1

i=1 k0(xi, x)

with k0(xj, xj) ≤ R2
j

∈ [2 log(j)/c2, 2 log(n)/c2]. We immediately have

k0(xj, xj)
2

+

j−1
∑
i=1

k0(xi, xj) ≤ δ
2

+

R2
j
2

+

min
x∈X∶k0(x,x)≤R2
j

j−1
∑
i=1

k0(xi, x)

so the desired conclusion follows from Theorem 5.

A.1.2. PROOF OF THEOREM 2: STEIN GREEDY CONVERGENCE

Instantiate the constants c1, c2 > 0 from Theorem 5, and consider any point sequence {xi}n

i=1 satisfying

k0(xj, xj)
2

+

j−1
∑
i=1

k0(xi, xj) ≤ δ
2

+

min
x∈X∶k0(x,x)≤R2
j

k0(x, x)
2

+

j−1
∑
i=1

k0(xi, x)

√

with Sj =

2 log(j)/c2 ≤ Rj ≤ ∞ for each 1 ≤ j ≤ n. We immediately have

k0(xj, xj)
2

+

j−1
∑
i=1

k0(xi, xj) ≤ δ
2

+

min
x∈X∶k0(x,x)≤S2
j

k0(x, x)
2

+

j−1
∑
i=1

k0(xi, x) ≤ δ
2

+

S2
j
2

+

min
x∈X∶k0(x,x)≤S2
j

j−1
∑
i=1

k0(xi, x),

so the desired conclusion follows from Theorem 5.

A.1.3. PROOF OF THEOREM 5: STEIN POINT CONVERGENCE

Stein Points

Our high-level strategy is to show that, when k0 is P -sub-exponential, optimizing over a suitably truncated search space on
each step is sufﬁcient to optimize the discrepancy globally. To obtain an explicit rate of convergence, we adapt the greedy
approximation error analysis of Jones (1992), which applies to uniformly bounded kernels. We begin by ﬁxing any sequence
of truncation levels (Sj)∞
}, and letting
Mj denote the convex hull of {k0(x, ⋅)}x∈Bj . Next we identify a truncation-optimal hj ∈ arg minf ∈Mj J(f ). Now, ﬁx any
point sequence {xi}n

j=1 with each Sj ∈ [0, ∞), deﬁning the truncation sets Bj = {x ∈ X ∶ k0(x, x) ≤ S2
j

i=1 satisfying

k0(xj, xj)
2

+

j−1
∑
i=1

k0(xi, xj) ≤ δ
2

+

S2
j
2

+

min
x∈X∶k0(x,x)≤S2
j

j−1
∑
i=1

k0(xi, x)

for some approximation level δ ≥ 0 and each 1 ≤ j ≤ n. In the remainder, we will recursively bound the discrepancy of this
point sequence in terms of each Sj and ∥hj∥
in terms of Sj using the P -sub-exponential tails of k0,
and show that an appropriate setting of each Sj delivers the advertised claim.

, bound each ∥hj∥

K0

K0

Bounding discrepancy For each j, let fj = 1
j
and the arithmetic-geometric mean inequality, we have the estimates

i=1 k0(xi, ⋅) and (cid:15)j = DK0,P ({xi}j

∑j

i=1

) = ∥fj∥K0 . By Cauchy-Schwarz

n2(cid:15)2
n

− δ = k0(xn, xn) + (n − 1)2(cid:15)2
+ (n − 1)2(cid:15)2

≤ S2
n

n−1

n−1

+ 2(n − 1)fn−1(xn) − δ
fn−1(x)

= S2
n

+ (n − 1)2(cid:15)2

n−1

≤ S2
n

+ (n − 1)2(cid:15)2

n−1

≤ S2
n

+ (n − 1)2(cid:15)2

n−1

+ 2(n − 1) min
x∈Bn
+ 2(n − 1) inf
f ∈Mn

⟨f, fn−1⟩K0

+ 2(n − 1)⟨hn, fn−1⟩K0
(n − 1)2
n2

+ n2 ∥hn∥2
K0

+

(cid:15)2
n−1.

Unrolling the recursion, we obtain

n2(cid:15)2
n

≤

(S2

n−i

+ δ + ∥hn−i∥2
K0

(n − i)2)

(1 + 1/(n − j + 1)2).

i
∏
j=1

n−1
∑
i=0

Moreover, the products in this expression are uniformly bounded in i as

log(

(1 + 1/(n − j + 1)2)) =

log((1 + 1/(n − j + 1)2)) ≤ ∫

log(1 + 1/x2) dx = π.

i
∏
j=1

i
∑
j=1

∞

0

Therefore,

n2(cid:15)2
n

≤ eπ

S2
i

+ δ + i2 ∥hi∥2
K0

.

n
∑
i=1

Bounding ∥hi∥

K0

To bound each ∥hi∥

K0

, we consider the truncated mean embeddings

Since kP = 0, we have ∥k+
i

∥

= ∥k−
i

∥

. Moreover, since k−
i

∈ Mi, we deduce that

k−
i

∶= ∫ k0(x, ⋅)I [k0(x, x) ≤ S2
i

] dP (x)

and

k+
i

∶= ∫ k0(x, ⋅)I [k0(x, x) > S2
i

] dP (x) = kP − k−
i .

K0
∥hi∥2
K0

≤ ∥k−
i

= ∥k+
i
= ∬ k0(x, y)I [k0(x, x) > S2
i

∥2
K0

K0
∥2
K0

√

≤ (∫

k0(x, x)I [k0(x, x) > S2
i

] dP (x))

2

≤ ∫ k0(x, x)I [k0(x, x) > S2
i

] dP (x)

] dP (x)I [k0(y, y) > S2
i

] dP (y)

Stein Points

where the ﬁnal two inequalities follow by Cauchy-Schwarz and Jensen’s inequality.
Let Y = k0(Z, Z) for Z ∼ P . We will bound the tail expectation in the ﬁnal display by considering the biased random
variable Y ∗ = k0(Z ∗, Z ∗) for Z ∗ with density ρ(z∗) = k0(z∗,z∗)p(z∗)
. By (Wainwright, 2017, Thm. 2.2), since Y is
sub-exponential, there exists c0 > 0 such that E[eλY ] < ∞ for all ∣λ∣ ≤ c0. For any λ ≠ 0 with ∣λ∣ ≤ c0/2, we have, by the
relation x ≤ ex,

E[Y ]

E[eλY ∗ ] = E[eλk0(Z∗,Z∗)] =

E[k0(Z, Z)eλk0(Z,Z)]
E[Y ]

=

E[λY eλY ]
λE[Y ]

≤

E[e2λY ]
λE[Y ]

< ∞.

Hence, by (Wainwright, 2017, Thm. 2.2), Y ∗ is also sub-exponential and satisﬁes, for some ˜c1, c2 > 0, P (Y ∗ ≥ t) ≤ ˜c1e−c2t
for all t > 0.

Applying this ﬁnding to the bounding of hi, we obtain

∥hi∥2
K0

≤ ∫ k0(x, x)I [k0(x, x) > S2
i

] dP (x) = E[Y ] ∫ I [k0(x, x) > S2
i

] ρ(x) dx = E[Y ]P (Y ∗ ≥ S2
i

) ≤ c1e−c2S2

i

where c1 = ˜c1E[Y ]. Hence

DK0,P ({xi}n
i=1

) ≤ eπ/2

S2
i

+ δ + i2c1e−c2S2
i .

¿
`
`(cid:192) 1
n2

n
∑
i=1

Setting each Si By choosing Si ∈ [

2 log(n)/c2] for each i we obtain

√

√

2 log(i)/c2,
¿
`
`(cid:192) 1
n2

DK0,P ({xi}n
i=1

) ≤ eπ/2

n
∑
i=1

2 log(n)
c2

+ δ + c1 ≤ eπ/2

2 log(n)
c2n

+ δ
n

+ c1
n

.

√

A.2. Proof of Theorem 3: Log Inverse KSD Controls Convergence

Fix any α > 0 and β < 0. Our proof will leverage (Gorham & Mackey, 2017, Thm. 7). This requires demonstrating two
separate properties for the log inverse kernel: ﬁrst, the log inverse function Φ(z) ≜ (α + log(1 + ∥z∥2
))β has a nonvanishing
2
generalized Fourier transform, and second, whenever DK0,P (µm) → 0, the measures µm are uniformly tight. We will
repeatedly use the notation γ(r) ≜ (α + log(1 + r))β and φ(r) ≜ γ(r2) throughout the proof. Moveover, we will use ˆf to
denote the (generalized) Fourier transform of a function f , and Vd will represent the volume of the unit Euclidean ball in d
dimensions. Finally, we write f (m) for the m-th derivative of any sufﬁciently differentiable function f ∶ R → R.

To demonstrate the ﬁrst property, we begin with the following lemma.
Lemma 6 (Log Inverse Function Is Completely Monotone). Fix any α > 0 and β < 0. The function γ(r) ≜ (α + log(1 + r))β
is completely monotone, i.e., γ ∈ C∞ and (−1)mγ(m)(r) ≥ 0 for all m ∈ N0 and all r ≥ 0, and hence the function
k2 ∶ Rd × Rd → R given by k2(x, x′) ≜ γ(∥x − x′∥2
2

) is a kernel function for all dimensions d ∈ N.

Proof. By (Wendland, 2004, Theorem 7.13) we know that Φ is positive semideﬁnite for all dimensions d ∈ N if and only if
γ is completely monotone. Thus it remains to show that γ is completely monotone.
Since α > 0, γ(r) > 0 for all r ≥ 0. To verify (−1)mγ(m)(r) ≥ 0 for all m ≥ 1, we will proceed by induction. Let us
suppose that for some m ≥ 1,

γ(m)(r) = (−1)m

cl,m(α + log(1 + r))β−l(1 + r)−m

(10)

where each cl,m ∈ R is positive. Taking another derivative yields

γ(m+1)(r) = (−1)m+1

cl,m+1(α + log(1 + r))β−l(1 + r)−m−1,

m
∑
l=1

m+1
∑
l=1

Stein Points

where c1,m+1 ≜ m c1,m, cl,m+1 ≜ m cl,m + (l − β − 1) cl−1,m for l > 1 and cl,m ≜ 0 for all l > m, completing the induction
step.
As for the base case, notice γ′(r) = β(α + log(1 + r))β−1(1 + r)−1, which establishes the identity for l = 1 by setting
c1,1 ≜ −β. The conclusion of this proof by induction implies (−1)mγ(m)(r) ≥ 0 for all m and all r ≥ 0. By (10), γ ∈ C∞,
establishing the lemma.

Knowing that γ is a completely monotone function, we can now demonstrate ˆΦ has a nonvanishing generalized Fourier
transform.
Lemma 7 (Log Inverse Function Has Nonvanishing GFT). Consider the function Φ ∶ Rd → R given by Φ(z) = (α + log(1 +
∥z∥2
))β for some α > 0 and β < 0. Its generalized Fourier transform ˆΦ(w) is radial, nonvanishing, and continuous for
2
w ≠ 0. Moreover, ˆΦ(w) → 0 as ∥w∥

→ ∞.

2

Proof. We will ﬁrst use induction to prove an intermediate result that states for any m ∈ N0,

∆mΦ(z) = ∑

τu,v ∥z∥2v

2 γ(u)(∥z∥2

2

)

(u,v)∈Sm

(11)

where τu,v > 0 are positive reals, Sm = {(u, v) ∈ N2
0
Note for the base case m = 0, the claim above for ∆0Φ = Φ clearly holds. Now suppose it holds from some m ∈ N0. If
A ∶ Rd → R is a function that can decomposed as A(z) ≜ f (∥z∥2
2

∣ v ≤ u − m, u ≤ 2m} and γ(r) ≜ (α + log(1 + r))β.

) where f, g ∈ C∞([0, ∞)), then we have

) g(∥z∥2
2

∆A(z) = [2dg′(∥z∥2
2

) + 4 ∥z∥2

2 g′′(∥z∥2

2

)] f (∥z∥2
2

) + [2dg(∥z∥2
2

) + 4 ∥z∥2

2 g′(∥z∥2

2

)] f ′(∥z∥2
2

) + 4 ∥z∥2

2 g(∥z∥2

2

)f ′′(∥z∥2
).
2
(12)

Consider each term in the decomposition of ∆mΦ(z) from the induction hypothesis. If we let g(r) = rv and f (r) = φ(u)(r),
2 φ(u′)(∥z∥2
) where the values for (u′, v′) are (u, v − 1), (u, v −
we see that each term from (12) is of the form τ ′
1), (u + 1, v), (u + 1, v), (u + 2, v + 1) respectively. Notice that when v = 0 or v = 1, the ﬁrst or second derivative of g will
be zero and these terms may disappear altogether. Thus all these tuples will lie in Sm+1 for any (u, v) ∈ Sm, and so we must
have ∆m+1Φ(z) satisﬁes the induction hypothesis as well, completing the proof by induction.
Now we can prove the lemma. Suppose 2m ≥ d. Then by the triangle inequality and a radial substitution (Baker, 1999),

∥z∥2v′

u,v

2

∫

Rd

∣∆mΦ(z)∣ dz ≤ ∑

∫

Rd

(u,v)∈Sm

τu,v ∥z∥2v
2

∣φ(u)(∥z∥2
2

)∣ dz = d Vd ∑

τu,vr2v+d−1∣φ(u)(r2)∣ dr.

∞

∫

0

(u,v)∈Sm

Because ∣φ(u)(r)∣ = O(r−u logβ−1(r)) as r → ∞ for u ∈ N by (10), we see that each integrand above is
O(r2(v−u)+d−1 logβ−1(r)). But since v ≤ u − m, this will imply that each integrand is O(r−2m+d−1 logβ−1(r)), which is
integrable for large r yielding ∆mΦ ∈ L1(Rd).

By (Steinwart & Christmann, 2008, Lemma 4.34) and the fact that positive deﬁniteness is preserved by summation, we
have ∆mΦ is a positive deﬁnite function. This along with the fact that ∆mΦ ∈ L1(Rd) allows us to invoke (Wendland,
2004, Theorem 6.11) and (Wendland, 2004, Theorem 6.18) to obtain ̂∆mΦ is continuous, radial and nonvanishing.
Moreover, ∆mΦ belonging to L1(Rd) implies its Fourier transform belongs to L∞(Rd). The lemma follows by noticing
̂∆mΦ(w) for all w ≠ 0.
̂∆mΦ(w) = ∥w∥2m

ˆΦ(w), i.e., ˆΦ(w) = ∥w∥−2m

2

2

We now need to demonstrate the second property to complete the proof of Theorem 3, but in order to do so, we ﬁrst will
establish the lemma below. By Lemma 7, we know ˆΦ is radial and thus can write ˆΦ(w) = φ∧(∥w∥
) for some continuous
2
function φ∧ ∶ (0, ∞) → (0, ∞). Our ﬁrst priority will be to lower bound φ∧ near the origin.
Lemma 8 (Log Inverse GFT Lower Bound). If Φ is the log inverse function on Rd from Lemma 7, then lim inf r→0+ rd(α +
log(1 + 1/r2))−β+1φ∧(r) > 0 where ˆΦ(w) = φ∧(∥w∥
) for all w ≠ 0.
2

Proof. First we will show that φ∧ is strictly decreasing. Since r ↦ (α + log(1 + r))β was shown to be completely monotone
in Lemma 6, by (Wendland, 2004, Theorem 7.14) we must have Φ(z) = ∫ ∞
2 ∂v(t) for some ﬁnite, non-negative

0 e−t∥z∥2

Stein Points

Borel measure v on [0, ∞) that is not concentrated at zero. Let (ϕm)∞
2004, Deﬁnition 5.17) deﬁned on Rd. Then, for each m, both ˆϕm and Φ ˆϕm are also Schwartz functions, and thus

m=1 be a sequence of Schwartz functions (Wendland,

∞

∫

Rd

[∫

0

∣e−t∥x∥2

2 ˆϕm(x)∣∂v(t)] dx = ∫

e−t∥x∥2

2∣ ˆϕm(x)∣∂v(t)] dx = ∫

Φ(x)∣ ˆϕm(x)∣ dx < ∞,

Rd

∞

[∫

0

Rd

as all Schwartz functions are integrable. This allows us to use Fubini’s theorem in conjunction with Plancherel’s Theorem to
argue

∫

Rd

ˆΦ(w)ϕm(w) dw = ∫

Φ(x) ˆϕm(x) dx = ∫

[∫

e−t∥x∥2

2∂v(t)] ˆϕm(x) dx

Rd

∞

0
∞

Rd

∫

Rd

∞

∞

0

0

0

∫

∫

Rd

Rd

= ∫

= ∫

= ∫

e−t∥x∥2

2 ˆϕm(x)∂v(t) dx

e−t∥x∥2

2 ˆϕm(x) dx∂v(t)

(2t)d/2e− 1

4t ∥w∥2

2ϕm(w) dw∂v+(t) + v0ϕm(0),

where we have used the decomposition v ≜ v+ + v0δ0 for v0 ≥ 0 and v+ non-zero and absolutely continuous with respect
to Lebesgue measure on [0, ∞). Let B ∶ Rd → R be a bump function, e.g., B(x) ≜ Z −1 exp{−1/(1 − ∥x∥2
< 1]
2
where Z is the normalization constant chosen such that ∫Rd B(x) dx = 1. Then let us deﬁne ϕm ∶ Rd → R via the mapping
ϕm(w) ≜ mdB(m(w − w0e1)) − mdB(m(w − w1e1)), where 0 < w0 < w1 and e1 ∈ Rd is the ﬁrst standard basis vector.
ˆΦ(w)ϕm(w) dw → ˆΦ(w0e1) − ˆΦ(w1e1) = φ∧(w0) − φ∧(w1) since ˆΦ is a continuous in neighborhoods of w0e1
Then ∫Rd
and w1e1 (Wendland, 2004, Theorem 5.22).
Because v+ cannot be the zero measure, there must be some ﬁnite interval [a0, b0] ⊂ (0, ∞) such that v+([a0, b0]) > 0. For
each t > 0 and m > max( 1
w0

)}I [∥x∥
2

), we have

2
w1−w0

,

Am(t) ≜ ∫

(2t)d/2e− 1

4t ∥w∥2

2ϕm(w) dw = ∫

(2t)d/2(e− 1

4t ∥w−w0e1∥2

2 − e− 1

4t ∥w−w1e1∥2

2 )mdB(mw) dw > 0,

Rd

Rd

since ∥w − w0e1∥
Am(t) → (2t)d/2(e− 1

< ∥w − w1e1∥
0 − e− 1

4t w2

2

2 when ∥w∥
1 ) as m → ∞ for any t > 0. Moreover, for all t ∈ [a0, b0] and m ≥ 1, we have

, w0). Using (Wendland, 2004, Theorem 5.22) again, we have

< min( w0−w1

2

2

4t w2

∣Am(t)∣ ≤ ∫

(2t)d/2e− 1

4t ∥w−w0e1∥2

Rd

2mdB(mw) dw ≤ (2b0)d/2 sup
∥w∥2<1

e− 1

4b0

∥w−w0e1∥2

2 < ∞.

(13)

Hence, the dominated convergence theorem allows us to exchange the limit over m and integral over t below to conclude

φ∧(w0) − φ∧(w1) = lim
m→∞

∫

0

Am(t)∂v+(t) + v0ϕm(0) ≥ lim
m→∞

∫

∞

b0

a0

Am(t)∂v+(t) = ∫

Am(t)∂v+(t)

b0

a0

lim
m→∞

b0

= ∫

a0

(2t)d/2(e− 1

4t w2

0 − e− 1

4t w2

1 )∂v+(t) ≥ v+([a0, b0]) min

{(2t)d/2(e− 1

4t w2

0 − e− 1

4t w2

1 )} > 0,

t∈[a0,b0]

showing φ∧ is strictly decreasing as claimed.
Suppose ψ ∶ [0, ∞) → R is a C∞ function with support [a, b] for 0 < a < b such that ψ(r) > 0 for all r ∈ (a, b) and
∫ ∞
0 ψ(r) dr = 1. Then because φ∧ is strictly decreasing, by the mean value theorem we have

for all λ > 0. If we assign Ψ(w) ≜ ψ(∥w∥
above becomes

2

φ∧(b/λ) ≤ ∫

λφ∧(r)ψ(λr) dr ≤ φ∧(a/λ)

∞

0

(14)

) to be the radial continuation of ψ, by (Baker, 1999) the quantity sandwiched

λφ∧(r)ψ(λr) dr = ∫

∞

0

φ∧(s/λ)ψ(s) ds = 1
dVd

∫

Rd

ˆΦ(w/λ) Ψ(w)
∥w∥d−1
2

dw.

Next suppose that ξ ∶ [0, ∞) → R is a Schwartz function satisfying ξ(k)(0) = 0 for all integral k ≥ 0, and let Ξ ∶ Rd → R
) be the radial continuation of ξ. Then by Plancherel’s Theorem, scaling the input of a Fourier
given by Ξ(x) ≜ ξ(∥x∥

∞

∫
0

2

Stein Points

transform as in (Wendland, 2004, Theorem 5.16), and the change to spherical coordinates in (Baker, 1999), for any λ > 0,
we have

∫

Rd

ˆΦ(w/λ)Ξ(w) dw = ∫

Φ(w)ˆΞ(w/λ) dw = d Vd ∫

Rd

∞

0

rd−1φ(r)ξ∧(r/λ) dr = d Vd λd ∫
0

∞

sd−1φ(λs)ξ∧(s) ds,

(15)

where s = r/λ and ξ∧ is the radial function associated with ˆΞ, i.e., ˆΞ(w) = ξ∧(∥w∥
Let us deﬁne ω ∶ [0, ∞) → R by the mapping ω(t) ≜ (α + t)β. Then by the mean value theorem and the fact that ω′ is
increasing, we have for all s > 1

) for all w.

2

−ω′(log(1 + λ2s2)) ≤ − ω(log(1 + λ2s2)) − ω(log(1 + λ2))

log(1 + λ2s2) − log(1 + λ2)

≤ −ω′(log(1 + λ2)).

By rearranging terms, this implies for all λ > 0

(−β) ( α + log(1 + λ2s2)
α + log(1 + λ2)

)

log ( 1 + λ2s2
1 + λ2

) ≤ − ω(log(1 + λ2s2)) − ω(log(1 + λ2))
ω(log(1 + λ2))(α + log(1 + λ2))−1

≤ (−β) log ( 1 + λ2s2
1 + λ2

) .

β−1

Since log( 1+λ2s2
) → 2 log s as λ → ∞, and the sandwiched term above is −(α + log(1 + λ2))(φ(λs)/φ(λ) − 1), we have
1+λ2
(α + log(1 + λ2))(φ(λs)/φ(λ) − 1) → 2β log s as λ → ∞ for all s > 1. The case for s ∈ (0, 1] is analogous and yields the
same asymptotic limit.

With this new asymptotic expansion in hand, we will revisit (15). We have

λ−dφ(λ)−1(α + log(1 + λ2)) ∫

ˆΦ(w/λ)Ξ(w) dw = d Vdφ(λ)−1(α + log(1 + λ2)) ∫

φ(λs)sd−1ξ∧(s) ds

Rd

= d Vd (α + log(1 + λ2)) ∫

sd−1ξ∧(s) ds

∞

∞

0
φ(λs)
φ(λ)
(α + log(1 + λ2)) [ φ(λs)
φ(λ)

0

= d Vd ∫

∞

0

− 1] sd−1ξ∧(s) ds.

Notice that ﬁnal integrand converges to 2βsd−1(log s)ξ∧(s) pointwise for all s ≥ 0 as λ → ∞. Since ξ∧ is a Schwartz
function on [0, ∞), we can utilize the fact that s ↦ log s is integrable near the origin to reason that sd−1(log s)ξ∧(s) is a
Schwartz function as well, and thus integrable. Hence by the dominated convergence theorem, we have the integral above
converges to 2 β d Vd ∫ ∞
Now suppose we choose Ξ(x) ≜ ∥x∥1−d

0 sd−1(log s)ξ∧(s) ds as λ → ∞.

2 Ψ(x). By (14) we have

λ−dφ(λ)−1(α + log(1 + λ2))φ∧(b/λ) ≤ 2β ∫

lim
λ→∞

∞

0

sd−1(log s)ξ∧(s) ds ≤ lim
λ→∞

λ−dφ(λ)−1(α + log(1 + λ2))φ∧(a/λ).

By Lemma 7, we know φ∧(r) > 0 for all r > 0, and thus the left-hand side above must be non-negative. Hence if we can
show for some choice of ψ that the sandwiched term is non-zero, then the proof of the lemma will follow from choosing
r = a/λ.
Let us deﬁne L(x) = log ∥x∥
the radial functions associated with L and ˆL. Notice that again by Plancherel’s Theorem

2 with generalized Fourier transform ˆL. As usual, let l ∶ [0, ∞) → R and l∧ ∶ [0, ∞) → R be

∞

∫

0

sd−1(log s)ξ∧(s) ds = 1
dVd

∫

Rd

ˆΞ(w)L(w) dw = 1
dVd

∫

Rd

Ξ(x) ˆL(x) dx = 1
dVd
∞

= ∫

0

ˆL(x) dx

∫

Rd

Ψ(x)
∥x∥d−1
2
ψ(r)l∧(r) dr.

(16)

Since we are free to choose ψ to be any Schwartz function with support [a, b], if we could not ﬁnd a function ψ such that the
quantity in (16) is non-zero, this would imply the support of l∧ is a subset of {0}. But this would mean l∧ is some multiple
of a point mass at zero, which would imply l is a constant function, a contradiction. Thus we must be able to ﬁnd some ψ
such that the integral above is non-zero, completing the lemma.

Stein Points

). Our strategy for showing the KSD controls tightness will mimic (Gorham & Mackey, 2017,
)α0−1 belongs to the

Fix any a0 > 0 and α0 ∈ (0, 1
2
Lem. 16): we will show that a bandlimited approximation of the function gj(x) = 2α0xj(a2
0
inverse log RKHS and thus enforces tightness.
First note that in the proof of (Gorham & Mackey, 2017, Lem. 16), it was shown h = TP g was a coercive, Lipschitz, and
bounded-below function for P ∈ P. Moreover, in the proof of (Gorham & Mackey, 2017, Lem. 12), a random vector Y with
density ρ(y) is constructed such that the support of ˆρ belongs to [−4, 4]d and also ∥Y ∥
2 is integrable. Consider the new
function g○(x) ≜ E [g(x + Y )] for all x ∈ Rd. By the convolution theorem, ˆg○
j is bandlimited for all j. In
j
the proof of (Gorham & Mackey, 2017, Lem. 16), ˆgj was shown to grow asymptotically at the rate (iwj) ∥w∥−d−2α0
as
∥w∥
2

= ˆgj ˆρ and so g○

→ 0. Thus

+ ∥x∥2
2

2

d
∑
j=1

∫

Rd

(w)

ˆg○
j

(w)ˆg○
j
ˆΦ(w)

dw =

d
∑
j=1

∫

[−4,4]d

ˆgj(w)ˆgj(w)ˆρ(w)2
ˆΦ(w)

dw ≤ κ0 ∫

∥w∥−2d−4α0+2
2
ˆΦ(w)

dw

[−4,4]d
√

4

d

≤ κ1 ∫
0

r−4α0+1 log−β+1(1 + r−2) dr,

for some constants κ0, κ1 > 0 where we used Lemma 8 in the ﬁnal inequality. This integral is ﬁnite for all α0 ∈ (0, 1
2
any β < 0, which implies g○ is in the log inverse RKHS by (Wendland, 2004, Theorem 10.21).

) and

Finally, notice that via the argument proving (Gorham & Mackey, 2017, Lemma 12),

∣TP g○(x) − h(x)∣ ≤ 3d log 2

sup
x∈Rd

( sup
x∈Rd

∥∇h(x)∥

2

+ sup
x∈Rd

π

∥∇2 log p(x)∥

⋅ sup
x∈Rd

op

∥g(x)∥

) < ∞.

2

Since h is bounded below and coercive, these properties are inherited by TP g○. This allows us to apply (Gorham & Mackey,
2017, Lemma 17) to argue DK0,P (µm) → 0 implies the measures µm are uniformly tight. Combining this with Lemma 7
allows us to utilize (Gorham & Mackey, 2017, Theorem 7) for the log inverse kernel, thereby concluding the proof.

A.3. Proof of Theorem 4: IMQ Score KSD Convergence Control

For b = ∇ log p, introduce the alias kb = k3, let Kb denote the RKHS of kb, and let Cc represent the set of continuous
compactly supported functions on X. Since P ∈ P, the proof of Thm. 13 in (Gorham & Mackey, 2017) shows that if, for
each h ∈ C 1 ∩ Cc and (cid:15) > 0, there exists h(cid:15) ∈ Kb such that supx∈X
∣(TP h)(x) − (TP h(cid:15))(x)∣ ≤ (cid:15), then µm ⇒ P whenever
DK0,P (µm) → 0 and (µm)∞
m=1 is uniformly tight. Hence, to establish our result, it sufﬁces to show (1) that, for each
h ∈ C 1 ∩ Cc and (cid:15) > 0, there exists h(cid:15) ∈ Kb such that supx∈X max(∥∇(h − h(cid:15))(x)∥
) ≤ (cid:15) and (2) that
DK0,P (µm) → 0 implies (µm)∞

2 , ∥b(x)(h − h(cid:15))(x)∥

m=1 is uniformly tight.

2

A.3.1. APPROXIMATING C 1 ∩ Cc WITH Kb
Fix any f ∈ C 1 ∩ Cc and (cid:15) > 0, and let K denote the RKHS of k(x, y) = (c2 + ∥x − y∥2
)β. Since p is strictly log-concave, b
2
is invertible with det(∇b(x)) never zero. Since P ∈ P, b is Lipschitz. By the following theorem, proved in Section A.4, it
therefore sufﬁces to show that there exists f(cid:15) ∈ K such that supx∈X max(∥∇(f − f(cid:15))(x)∥
Theorem 9 (Composition Kernel Approximation). For b ∶ X → X invertible and k a reproducing kernel on X with induced
RKHS K, deﬁne the composition kernel kb(x, y) = k(b(x), b(y)) with induced RKHS Kb. Suppose that, for each f ∈ C 1 ∩Cc
and (cid:15) > 0, there exists f(cid:15) ∈ K such that

2 , ∥x(f − f(cid:15))(x)∥

) ≤ (cid:15).

2

If b is Lipschitz and det(∇b(x)) is never zero, then, for each h ∈ C 1 ∩ Cc and (cid:15) > 0, there exists h(cid:15) ∈ Kb such that

max(∥∇(f − f(cid:15))(x)∥

2 , ∥x(f − f(cid:15))(x)∥

2

) ≤ (cid:15).

max(∥∇(h − h(cid:15))(x)∥

2 , ∥b(x)(h − h(cid:15))(x)∥

2

) ≤ (cid:15).

sup
x∈X

sup
x∈X

Since the identity map x ↦ x is Lipschitz and f ∈ L2 because it is continuous and compactly supported, (Gorham & Mackey,
2017, Lem. 12) provides an explicit construction of f(cid:15) ∈ K satisfying our desired property whenever k(x, y) = Φ(x − y) for
Φ ∈ C 2 with non-vanishing Fourier transform. Our choice of IMQ k satisﬁes these properties by (Wendland, 2004, Thm.
8.15).

A.3.2. CONTROLLING TIGHTNESS

Since P is distantly dissipative,

Stein Points

− ∥b(x)∥

∥x∥

2

2

≤ ⟨b(x), x⟩ ≤ −κ ∥x∥2
2

+ C + ⟨b(0), x⟩ ≤ −κ ∥x∥2
2

+ C + ∥b(0)∥

∥x∥

2

2

by Cauchy-Schwarz. Hence, b is norm-coercive, i.e., ∥b(x)∥
result follows from the following lemma which guarantees tightness control on b under weaker conditions.
Lemma 10 (Coercive Score Kernel KSDs Control Tightness). If b ∶ X → X is norm coercive and differentiable, and
∇jbj(x) = o(∥b(x)∥2
2

→ ∞, then lim supm DK0,P (µm) < ∞ implies (µm)∞

→ ∞. Since ∇b is bounded, our desired

→ ∞ whenever ∥x∥

m=1 is tight.

) as ∥x∥

2

2

2

Proof. Fix any a > c/2 and α ∈ (0, 1
2
gj(x) = 2αxj(a2 + ∥x∥2
2
Lemma 12. By our assumptions on ∇b, we have

(β + 1)). The proof of (Gorham & Mackey, 2017, Lem. 16) showed that the function
)α−1 ∈ K for each j ∈ {1, . . . , d}. Hence gb,j(x) ≜ gj(b(x)) ∈ Kb for each j ∈ {1, . . . , d} by

(TP gb)(x) = 2α(∥b(x)∥2
2

(a2 + ∥b(x)∥2
2

∇jbj(x)(a2 + ∥b(x)∥2
2

)α−1 + bj(x)22(α − 1)(a2 + ∥b(x)∥2
2

)α−2∇jbj(x))

= 2α ∥b(x)∥2
2

(a2 + ∥b(x)∥2
2

)α−1 +

d
∑
j=1
)α−1 + o(∥b(x)∥2α
2

),

so TP gb is coercive, and the proof of (Gorham & Mackey, 2017, Lem. 17) therefore gives the result (µm)∞
tight whenever lim supm DK0,P (µm) ﬁnite.

m=1 is uniformly

A.4. Proof of Theorem 9: Composition Kernel Approximation

Let c = b−1 represent the inverse of b, and for any function f on X, let fc(y) = f (c(y)) denote the composition of f and c
so that fc(b(x)) = f (x). The following lemma shows that fc inherits many of the properties of f under suitable restrictions
on b.
Lemma 11 (Composition Properties). For any function f on X and invertible function b on X, deﬁne fc(y) = f (c(y)) for
c = b−1. The following properties hold.

1. If f has compact support and b is continuous, then fc has compact support.

2. If f ∈ C 1, b ∈ C 1, and det(∇b(x)) is never zero, then fc ∈ C 1.

Proof. We prove each claim in turn.

1. If f is compactly supported and b is continuous, then supp(fc) = b(supp(f )) is also compact, since continuous

functions are compact-preserving (Joshi, 1983, Prop. 1.8).

2. If f ∈ C 1, b ∈ C 1, and det(∇b(x)) is never zero, then c is continuous by the inverse function theorem (Spivak, 1965,
Thm. 2-11), x ↦ (∇b(x))−1 is continuous, and hence ∇fc(y) = (∇c(y))(∇f )(c(y)) = ((∇b)(c(y)))−1(∇f )(c(y))
is continuous.

Our next lemma exposes an important relationship between the RKHSes K and Kb.
Lemma 12. Suppose f is in the RKHS K of a reproducing kernel k on X and b ∶ X → X is invertible. Then fb is in the
RKHS Kb of kb for fb(x) = f (b(x)) and kb(x, y) = k(b(x), b(y)).

Proof. Since f ∈ K,
limm→∞ ∥fm − f ∥

K

there exist fm = ∑Jm

j=1 am,jk(xm,j, ⋅) for m ∈ N, am,j ∈ R, and xm,j ∈ X such that

= 0 and limm→∞ fm(x) = f (x) for all x ∈ X. Now let c = b−1, and deﬁne

fm,b(x) = fm(b(x)) =

am,jk(xm,j, b(x)) =

am,jkb(c(xm,j), x).

Jm
∑
j=1

Jm
∑
j=1

Stein Points

Since Kb = {∑J
⟨fm,b, fm′,b⟩2
Kb
for all m, m′, the sequence (fm,b)∞
complete, fb ∈ Kb.

j=1 am,j ∑Jm′

j=1 ajkb(yj, ⋅) ∶ J ∈ N, aj ∈ R, yj ∈ X}, each fm,b ∈ Kb. Since (fm)∞
= ∑Jm

j′=1 am′,j′kb(c(xm,j), c(xm′,j′ )) = ⟨fm, fm′ ⟩K so that ∥fm,b − fm′,b∥

m=1 is a Cauchy sequence, and

= ∥fm − fm′ ∥
K
to its pointwise limit fb. Since an RKHS is

Kb

m=1 is also Cauchy and converges in ∥⋅∥

Kb

With our lemmata in hand, we now prove the advertised claim. Suppose b is Lipschitz, det(∇b(x)) is never zero, and
for each f ∈ C 1 ∩ Cc and (cid:15) > 0 there exists f(cid:15) ∈ K such that supx∈X max(∥∇(f − f(cid:15))(x)∥
) ≤ (cid:15).
Select any h ∈ C 1 ∩ Cc and any (cid:15) > 0. By Lemma 11, hc ∈ C 1 ∩ Cc, and hence there exists hc,(cid:15) ∈ K such that
) ≤ (cid:15)/ max(1, M1(b)). Now deﬁne h(cid:15)(x) = hc,(cid:15)(b(x)) so that
supy∈X max(∥∇(hc − hc,(cid:15))(y)∥
h(cid:15) ∈ Kb by Lemma 12. We have supx∈X
∥∇h(cid:15)(x) − ∇h(x)∥

2
∥(∇b(x))((∇hc,(cid:15))(b(x)) − (∇hc)(b(x)))∥

≤ M1(b)(cid:15)/ max(1, M1(b)) ≤ (cid:15).

2 , ∥y(hc − hc,(cid:15))(y)∥

2 , ∥x(f − f(cid:15))(x)∥

∥b(x)(h(cid:15) − h)(x)∥

∥y(hc,(cid:15) − hc)(y)∥

≤ supy∈X

≤ (cid:15), and

2

2

2

2

sup
x∈X

2

= sup
x∈X

B. Implementational Detail

B.1. Benchmark Methods

In this section we brieﬂy describe the MED and SVGD methods used as our empirical benchmark, as well as the (block)
coordinate descent method that was used in conjunction with Stein Points.

B.1.1. MINIMUM ENERGY DESIGNS

The ﬁrst class of method that we consider is due to (Joseph et al., 2015). That work restricted attention to X = [0, 1]d and
constructed an energy functional:

Eδ,P ({xi}n
i=1

)

⎡
⎢
⎢
⎢
⎣

∶= ∑
i≠j

p(xi)− 1

2d p(xj)− 1

2d

∥xi − xj∥2

⎤
δ
⎥
⎥
⎥
⎦

for some tuning parameter δ ∈ [1, ∞) to be speciﬁed. In (Joseph et al., 2017) the rule-of-thumb δ = 4d was recommended. A
heuristic argument in (Joseph et al., 2015) suggests that the points {xi}n
) form an empirical
approximation that converges weakly to P . The argument was recently made rigorous in (Joseph et al., 2017).
Minimisation of Eδ,P does not require knowledge of how p is normalised. However, the actual minimisation of Eδ,P can be
difﬁcult. In (Joseph et al., 2015) an extensible (greedy) method was considered, wherein the ﬁrst point is selected as

i=1 that minimise Eδ,P ({xi}n
i=1

and subsequent points are selected as

x1 ∈ arg max

p(x)

x∈X

xn

∈ arg min

p(x)− δ

2d

x∈X

n−1
∑
i=1

2d

p(xi)− δ
∥xi − x∥δ
2

.

However, alternative approaches could easily be envisioned. For instance, if n were ﬁxed then one could consider e.g.
applying the Newton method for optimisation over the points {xi}n

i=1.

Remark: There is a connection between certain minimum energy methods and discrepancy measures in RKHS; see
(Sejdinovic et al., 2013).
Remark: Several potential modiﬁcations to Eδ,P were suggested in (Joseph et al., 2017), but that report appeared after this
work was completed. These could be explored in future work.
Remark: The MED objective function is typically numerically unstable due to the fact that the values of the density p(⋅) can
be very small. In contrast, our proposed methods operate on log p(⋅) and its gradient, which is more numerically robust.

B.1.2. STEIN VARIATIONAL GRADIENT DESCENT

The second method that we considered was due to (Liu & Wang, 2016; Liu, 2017) and recently generalised in (Liu &
Zhu, 2017). The idea starts by formulating a continuous version of gradient descent on P(X) with the Kullback-Leibler

divergence KL(⋅∣∣P ) as a target. To this end, restrict attention to X = Rd and consider the dynamics

Stein Points

Sf (x) = x + (cid:15)f (x)

parametrised by a function f ∈ Kd. For inﬁnitesimal values of (cid:15) we can lift Sf to a pushforward map on P(X); i.e.
Q ↦ Sf Q. It was then shown in (Liu & Wang, 2016) that

− d
d(cid:15)

KL(Sf Q∣∣P )∣

= ∫ TP f dQ

(cid:15)=0

(17)

where TP is the Langevin Stein operator in Eqn. 5. Recall that this operator can be decomposed as TP f = ∑d
TP,jfj with
TP,j = ∇j + ∇j log p, where ∇j denotes differentiation with respect to the jth coordinate in X. Then the direction of fastest
descent

j=1

has a closed-form, with jth coordinate

f ∗(⋅)

∶= arg max
f ∈B(Kd)

− d
d(cid:15)

KL(Sf Q∣∣P )∣

(cid:15)=0

f ∗
j

(⋅; Q) = ∫ TP,jk(x, ⋅) dQ(x).

The algorithm proposed in (Liu & Wang, 2016) discretises this dynamics in both space X, through the use of n points,
and in time, through the use of a positive step size (cid:15) > 0, leading to a sequence of empirical measures based on point sets
{xm
i

i=1 of the points, at iteration m ≥ 1 of the algorithm we update
}n

i=1 for m ∈ N. Thus, given an initialisation {x0
}n
i

in parallel, where

xm
i

= xm−1
i

+ (cid:15)f ∗(xm−1

i

; Qm
n

)

Qm
n

=

1
n

n
∑
i=1

δxm−1

i

is the empirical measure, at a computational cost of O(n). The output is the empirical measure Qm
n .

Remark: The step size (cid:15) is a tuning parameter of the method.

Remark: At present there are not theoretical guarantees for this method. Initial steps toward this goal are presented in (Liu,
2017).

B.1.3. BLOCK COORDINATE DESCENT

The Stein Point methods developed in the main text can be adapted to return a ﬁxed number n of points for a given ﬁnite
computational budget by ﬁrst iteratively generating a size n point set, as described in the main text, and then performing
(block) coordinate descent on this point set. The (block) coordinate descent procedure is now described:
Fix an initial conﬁguration {x0
i

i=1. Then at iteration m ≥ 1 of the algorithm, perform the following sequence of operations:
}n

∀i
for i = 1, . . . , n

xm
i
xm
i

← xm−1
i
← arg min
x∈X

then:
DK0,P ({xm
j

}j≠i ∪ {x})

The output is the point set {xm
i

}n
i=1.

Remark: The block coordinate descent method can equally be applied to MED; this was not considered in our empirical
work.

Remark: Any numerical optimisation method can be used to solve the global optimisation problem in the inner loop. In
this work we considered the same three candidates in the main text; Monte Carlo, Nelder-Mead and grid search. These are
described next.

Stein Points

B.2. Numerical Optimisation Methods

Computation of the nth term in the proposed Stein Point sequences, given the previous n − 1 terms, requires that a global
optimisation is performed over xn ∈ X. The same is true for both MED and KSD in the coordinate descent context. For all
experiments reported in the main text, three different numerical methods were considered for this task, denoted NM, MC, GS
in the main text. In this section we provide full details for how these methods were implemented.

B.2.1. NELDER-MEAD

The Nelder-Mead (NM) method (Nelder & Mead, 1965) proceeds as in Algorithm 1. The function NM takes the following
inputs: f is the objective function; t is the iteration count; ninit is the number of initial points to be drawn from a
proposal distribution; ndelay is the number of iterations after which the proposal distribution becomes adaptive; µ0
and Σ0 are the mean vector and the covariance matrix of the initial proposal distribution; {xcurr
is the set of
existing points; λ is the variance of each mixture component of the adaptive proposal distribution; l and u are the
lower- and upper-bounds of the search space. The non-adaptive initial proposal distribution is a truncated multivariate
Gaussian N (µ0, Σ0) whose support is bounded by the hypercube [l, u]. The adaptive proposal distribution is a truncated
, λI) with λ > 0 and support [l, u]. The expression
Gaussian mixture Π({xcurr
j
NelderMeadx [f (x), xinit
, l, u] denotes the standard Nelder-Mead procedure for objective function f , initial point xinit
,
and bound constraint x ∈ [l, u]. We use the symbol ¢ to denote the assignment of a realised independent draw. The operator
truncu
l

[⋅] bounds the support of a distribution by the hypercube [l, u].

}ncurr
j=1 , λ) ∶=

∑ncurr−1
j=1

N (xcurr
j

1
ncurr−1

}ncurr
j=1

j

i

i

}ncurr
j=1 , λ, l, u

j

xinit
i

for i ← 1 ∶ ninit do
if t ≤ ndelay then
¢ truncu
l

Algorithm 1 Nelder-Mead
input f , t, ninit, ndelay, µ0, Σ0, {xcurr
output x∗
1: function NM
2:
3:
4:
5:
6:
7:
8:
9:
10:

xinit
i
end if
xlocal
i
end for
i∗ ← arg mini∈{1...ninit} f (xlocal
x∗ ← xlocal
11:
12: end function

¢ truncu
l

[Π({xcurr

else

i∗

[N (µ0, Σ0)]

)

j

i

j=1 , λ)]
}ncurr

← NelderMeadx [f (x), xinit

, l, u]

i

B.2.2. MONTE CARLO

The Monte Carlo (MC) optimisation method proceeds as in Algorithm 2. The function MC takes the following inputs:
f is the objective function; t is the iteration count; ntest is the number of test points to be drawn from a proposal
distribution; ndelay is the number of iterations after which the proposal distribution becomes adaptive; µ0 and Σ0 are
the mean vector and the covariance matrix of the initial proposal distribution; {xcurr
is the set of existing points;
λ is the variance of each mixture component of the adaptive proposal distribution; l and u are the lower- and upper-
bounds of the search space. The non-adaptive initial proposal distribution is a truncated multivariate Gaussian N (µ0, Σ0)
whose support is bounded by the hypercube [l, u]. The adaptive proposal distribution is a truncated Gaussian mixture
Π({xcurr
j

, λI) with λ > 0 and support [l, u].

}ncurr
j=1 , λ) ∶=

∑ncurr−1
j=1

N (xcurr
j

}ncurr
j=1

j

1
ncurr−1

Stein Points

j

if t ≤ ndelay then

Algorithm 2 Monte Carlo
input f , t, ntest, ndelay, µ0, Σ0, {xcurr
output x∗
1: function MC
2:
3:
4:
{xtest
5:
i
end if
6:
i∗ ← arg mini∈{1...ntest} f (xtest
7:
x∗ ← xtest
8:
9: end function

¢ truncu
l

¢ truncu
l

{xtest
i

}ntest
i=1

}ntest
i=1

else

i∗

)

j

i

}ncurr
j=1 , λ, l, u

[N (µ0, Σ0)]

[Π({xcurr

j=1 , λ)]
}ncurr

B.2.3. GRID SEARCH

Algorithm 3 Grid Search
input f , t, l, u, n0
output x∗
1: function GS
2:
3:
4: Xgrid ← {l, l + δgrid, . . . , u}d
x∗ ← arg minx∈Xgrid
5:
6: end function

t)
ngrid ← n0 + Round(
δgrid ← (u − l)/(ngrid − 1)

f (x)

√

The grid search (GS) optimisation method proceeds as in Algorithm 3. The function GS takes the following inputs: f is the
objective function; t is the iteration count; l and u are the lower- and upper-bounds of the grid; n0 is the initial grid size.

B.3. Remark on Application to a Reference Point Set

It is interesting to comment on the behaviour of our proposed methods in the case where X is a ﬁnite set or the global
optimisation over X is replaced by a discrete optimisation over a pre-determined ﬁxed set Y = {yi}N
⊆ X. In this case it
i=1
can be shown that:

• The algorithm after n iterations will have selected n points {yπ(i)}n

i=1 with replacement from Y . (Here π(i) indexes

the point that was selected at iteration i of the algorithm.)

• The empirical measure 1
n

∑n

i=1 δyπ(i) can be expressed as ∑N

i=1 wiyi for some weights wi.

• The weights wi converge to

(∗) =

arg min
w≥0
w1+⋅⋅⋅+wN =1

wiwjk0(yi, yj).

¿
`
`(cid:192) 1
N 2

N
∑
i,j=1

√

• At iteration n, it holds that DK0,P ({yπ(i)}n
i=1

) = (∗) + O(

log(n)/n).

Thus in this scenario the algorithms that we have proposed act to ensure that these points are optimally weighted in the
sense just described.

C. Experimental Protocol and Additional Numerical Results

This section contains additional numerical results that elaborate on the three experiments reported in the main text.

C.1. Gaussian Mixture Test

Stein Points

Recall from the main text that the kernels k1, k2 and k3 contain either one or two hyper-parameters that must be selected.
For each of the methods (a)-(f) reported in Figure 2 in the main text we optimised these parameters over a discrete set, with
respect to an objective function of WP based on a point set of size n = 100 and the Nelder-Mead optimisation method. The
set of possible values for α was {0.1η, 0.5η, η, 2η, 4η, 8η}, where η is a problem dependent “base scale” and chosen to be 1
for the Gaussian mixture test. The set of possible values for β was {−0.1, −0.3, −0.5, −0.7, −0.9}. The sensitivity of the
reported results to the variation in hyper-parameters is shown, for the Gaussian mixture test, in Figure 5. Point sets obtained
under representatives of each method class are shown in Figure 6.
For all the global optimisation methods we imposed a bounding box (−5, 5) × (−5, 5); for the Nelder-Mead method, we
set ninit = 3, ndelay = 20, µ0 = (0, 0), Σ0 = 25I, and λ = 1; for the Monte Carlo method, we set ntest = 20, ndelay = 20,
µ0 = (0, 0), Σ0 = 25I, and λ = 1; for the grid search, we set n0 = 100.
For MED the tuning parameter δ was considered for δ = 4, δ = 8 or δ = 16, with δ = 4d = 8 being the recommendation in
(Joseph et al., 2017).

For SVGD we set the initial point-set to be an equally spaced rectangular grid over the bounding box. Following (Liu &
Wang, 2016), the step-size (cid:15) for SVGD was determined by AdaGrad with a master step-size of 0.1 and a momentum factor
of 0.9.

(a) Stein Points (Greedy)

(b) Stein Points (Herding)

(c) SVGD

Figure 5: Kernel parameter selection results for the Gaussian mixture test. Parameters α, β in the kernels k1, k2, k3 were
optimised over a discrete set with respect to the Wasserstein distance WP for a point set of size n = 100. The values log WP
(y-axis) are shown for all different conﬁgurations of parameters (x-axis) considered. Optimal parameter conﬁgurations are
circled and detailed in the legend.

C.2. Gaussian Process Test

For the Gaussian process test, the base scale η is also set to 1. The sensitivity of results to the selection of kernel parameters
was reported in Figure 7. Point sets obtained under representatives of each method class are shown in Figures 8 and 9.
Detailed results for each method considered are contained in Figure 10.
For all the global optimisation methods we imposed a bounding box of (−5, 5) × (−13, −7); for the Nelder-Mead method, we
set ninit = 3, ndelay = 20, µ0 = (0, −10), Σ0 = 25I, and λ = 1; for the Monte Carlo method, we set ntest = 20, ndelay = 20,
µ0 = (0, −10), Σ0 = 25I, and λ = 1; for the grid search, we set n0 = 100.

For SVGD we set the initial point-set to be an equally spaced rectangular grid over the bounding box. Following (Liu &
Wang, 2016), the step-size (cid:15) for SVGD was determined by AdaGrad with a master step-size of 0.1 and a momentum factor
of 0.9.

C.3. IGARCH Test

For the IGARCH test, we choose the base scale η to be 1e-5. The sensitivity of results to the selection of kernel parameters
was reported in Figure 11. Point sets obtained under representatives of each method class are shown in Figures 12 and 13.
Detailed results for each method considered are contained in Figure 14.
For all the global optimisation methods we impose a bounding box of (0.002, 0.04) × (0.05, 0.2); for the Nelder-Mead

Stein Points

method, we set ninit = 3, ndelay = 20, µ0 = (0.021, 0.125), Σ0 = diag[(1e-4, 1e-3)], and λ = 1e-5; for the Monte Carlo
method, we set ntest = 20, ndelay = 20, µ0 = (0.021, 0.125), Σ0 = diag[(1e-4, 1e-3)], and λ = 1e-5; for the grid search, we
set n0 = 100.

For SVGD we set the initial point-set to be an equally spaced rectangular grid over the bounding box. Following (Liu &
Wang, 2016), the step-size (cid:15) for SVGD was determined by AdaGrad with a master step-size of 1e-3 and a momentum factor
of 0.9.

Stein Points

Figure 6: Typical point sets obtained in the Gaussian mixture test, where the budget-constrained methods Stein Greedy-100
(Stn Grdy-100) and Stein Herding-100 (Stn Hrd-100) are considered. [Here each row corresponds to an algorithm, and each
column corresponds to a chosen level of computational cost. The left border of each sub-plot is aligned to the exact value of
log neval spent to obtain each point-set.]

Stein Points

(a) Stein Points (Greedy)

(b) Stein Points (Herding)

(c) SVGD

Figure 7: Kernel parameter selection results for the Gaussian process test. Parameters α, β in the kernels k1, k2, k3 were
optimised over a discrete set with respect to the Wasserstein distance WP for a point set of size n = 100. The values log WP
(y-axis) are shown for all different conﬁgurations of parameters (x-axis) considered. Optimal parameter conﬁgurations are
circled and detailed in the legend.

Figure 8: Typical point sets obtained in the Gaussian process test. [Here each row corresponds to an algorithm, and each
column corresponds to a chosen level of computational cost. The left border of each sub-plot is aligned to the exact value of
log neval spent to obtain each point-set. MCMC represents a random-walk Metropolis algorithm with a proposal distribution
optimised according to acceptance rate.]

Stein Points

Figure 9: Typical point sets obtained in the Gaussian process test, where the budget-constrained methods Stein Greedy-100
(Stn Grdy-100) and Stein Herding-100 (Stn Hrd-100) are considered. [Here each row corresponds to an algorithm, and each
column corresponds to a chosen level of computational cost. The left border of each sub-plot is aligned to the exact value of
log neval spent to obtain each point-set. MCMC represents a random-walk Metropolis algorithm with a proposal distribution
optimised according to acceptance rate.]

Stein Points

(a) Monte Carlo

(b) Stein Points (Greedy)

(c) Stein Points (Herding)

(d) SVGD

Figure 10: Results for the Gaussian process test. [Here n = 100. x-axis: log of the number neval of model evaluations
that were used. y-axis: log of the Wasserstein distance WP ({xi}n
) obtained. Kernel parameters α, β were optimised
i=1
according to WP . In sub-ﬁgure 10a, MCMC represents a random-walk Metropolis algorithm with a proposal distribution
optimised according to acceptance rate. MCMC-Thin represents a thinned chain by taking every 100th observation.]

(a) Stein Points (Greedy)

(b) Stein Points (Herding)

(c) SVGD

Figure 11: Kernel parameter selection results for the IGARCH test. Parameters α, β in the kernels k1, k2, k3 were optimised
over a discrete set with respect to the Wasserstein distance WP for a point set of size n = 100. The values log WP (y-axis)
are shown for all different conﬁgurations of parameters (x-axis) considered. Optimal parameter conﬁgurations are circled
and detailed in the legend.

Stein Points

Figure 12: Typical point sets obtained in the IGARCH test. [Here each row corresponds to an algorithm, and each column
corresponds to a chosen level of computational cost. The left border of each sub-plot is aligned to the exact value of
log neval spent to obtain each point-set. MCMC represents a random-walk Metropolis algorithm with a proposal distribution
optimised according to acceptance rate.]

Stein Points

Figure 13: Typical point sets obtained in the IGARCH test, where the budget-constrained methods Stein Greedy-100 (Stn
Grdy-100) and Stein Herding-100 (Stn Hrd-100) are considered. [Here each row corresponds to an algorithm, and each
column corresponds to a chosen level of computational cost. The left border of each sub-plot is aligned to the exact value of
log neval spent to obtain each point-set. MCMC represents a random-walk Metropolis algorithm with a proposal distribution
optimised according to acceptance rate.]

Stein Points

(a) Monte Carlo

(b) Stein Points (Greedy)

(c) Stein Points (Herding)

(d) SVGD

Figure 14: Results for the IGARCH test. [Here n = 100. x-axis: log of the number neval of model evaluations that were used.
) obtained. Kernel parameters α, β were optimised according to WP . In
y-axis: log of the Wasserstein distance WP ({xi}n
i=1
sub-ﬁgure 14a, MCMC represents a random-walk Metropolis algorithm with a proposal distribution optimised according to
acceptance rate. MCMC-Thin represents a thinned chain by taking every 100th observation.]

