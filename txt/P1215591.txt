8
1
0
2
 
b
e
F
 
9
 
 
]

C
O
.
h
t
a
m

[
 
 
2
v
5
3
6
6
0
.
0
1
7
1
:
v
i
X
r
a

a sinkhorn–newton method for entropic
optimal transport

Christoph Brauer∗

Christian Clason†

Dirk Lorenz∗

Benedikt Wirth‡

February 2, 2018

Abstract We consider the entropic regularization of discretized optimal transport and
propose to solve its optimality conditions via a logarithmic Newton iteration. We show a
quadratic convergence rate and validate numerically that the method compares favorably
with the more commonly used Sinkhorn–Knopp algorithm for small regularization strength.
We further investigate numerically the robustness of the proposed method with respect to
parameters such as the mesh size of the discretization.

1 introduction

The mathematical problem of optimal mass transport has a long history dating back to its
introduction in Monge [10], with key contributions by Kantorovivc [6] and Kantorovivc &
Rubinvsteuin [7]. It has recently received increased interest due to numerous applications in
machine learning; see, e.g., the recent overview of Kolouri, Park, Thorpe, Slepcev & Rohde
[9] and the references therein. In a nutshell, the (discrete) problem of optimal transport in its
Kantorovich form is to compute for given mass distributions a and b with equal mass a transport
plan, i.e., an assignment of how much mass of a at some point should be moved to another point
to match the mass in b. This should be done in a way such that some transport cost (usually
proportional to the amount of mass and dependent on the distance) is minimized. This leads
to a linear optimization problem which has been well studied, but its application in machine
learning has been problematic due to large memory requirement and long run time. Recently,
Cuturi [2] proposed a method that overcomes the memory requirement by so-called entropic
regularization that has found broad applications; see, e.g., Carlier, Duval, Peyré & Schmitzer
[1], Cuturi & Doucet [3], and Frogner, Zhang, Mobahi, Araya & Poggio [5]. The resulting
iteration resembles the so-called Sinkhorn–Knopp method from Sinkhorn & Knopp [11] for
matrix balancing and allows for a simple and efficient implementation.

∗Institute of Analysis and Algebra, TU Braunschweig, 38092 Braunschweig, Germany (ch.brauer@tu-

braunschweig.de, d.lorenz@tu-braunschweig.de)

†Faculty of Mathematics, University Duisburg-Essen, 45117 Essen, Germany (christian.clason@uni-due.de)
‡Institute for Numerical and Applied Mathematics, University of Münster, Einsteinstraße 62, 48149 Münster, Germany

(benedikt.wirth@uni-muenster.de)

1

1.1 our contribution

In this work, we show that the Sinkhorn–Knopp method can be viewed as an approximate
Newton method and derive a full Newton method for entropically regularized optimal transport
problems that is demonstrated to perform significantly better for small entropic regularization
parameters. Here, compared to Cuturi [2], the key idea is to apply a logarithmic transform to
the variables.

This paper is organized as follows. In Section 2, we state the Kantorovich formulation of
optimal transport together with its dual which serves as the basis of the derived algorithm.
Afterwards, we establish local quadratic convergence and discuss the relation of the proposed
Newton method to the Sinkhorn–Knopp iteration. The performance and parameter dependence
of the proposed method are illustrated with numerical examples in Section 3. Section 4 contains
the proof of the key estimate for quadratic convergence, and Section 5 concludes the paper.

1.2 notation

In the following, (cid:49)n represents the n-dimensional vector with all ones and (cid:49)n,m refers to the
n a = 1} denotes the probability
n × m matrix with all ones. Moreover, Σn (cid:66) {a ∈ (cid:82)n
simplex in (cid:82)n
+ whose elements are called probability vectors, or equivalently, histograms. For
two histograms a ∈ Σn and b ∈ Σm,

+ : (cid:49)(cid:62)

U (a, b) (cid:66) {P ∈ (cid:82)n×m

+

: P(cid:49)m = a, P (cid:62)(cid:49)n = b}

(1.1)

is the set of admissible coupling matrices. In the context of optimal transport, the elements
of U (a, b) are also referred to as transport plans. Histograms a and b can be viewed as mass
distributions, and an entry Pi j of a transport plan P ∈ U (a, b) can be interpreted as the amount
of mass moved from ai to bj .

We refer to the Frobenius inner product of two matrices P, P (cid:48) ∈ (cid:82)n×m as (cid:104)P, P (cid:48)(cid:105) (cid:66) (cid:205)

i j Pi jP (cid:48)
i j .
At the same time, (cid:104)a, a(cid:48)(cid:105) (cid:66) (cid:205)
i denotes the standard dot product of two vectors a, a(cid:48) ∈ (cid:82)n.
Finally, Diag(a) ∈ (cid:82)n×n is defined as the diagonal matrix with Diag(a)ii (cid:66) ai and Diag(a)i j (cid:66) 0
for i (cid:44) j, and a (cid:12) a(cid:48) (cid:66) Diag(a)a(cid:48) is the Hadamard product (i.e., the component-wise product) of
a and a(cid:48).

i aia(cid:48)

2 sinkhorn–newton method

In this section we derive our Sinkhorn–Newton method. We start by introducing the problem of
entropically regularized optimal transport in Section 2.1. Afterwards, in Section 2.2, we present
our approach, which is essentially applying Newton’s method to the optimality system associated
with the transport problem and its dual, before we discuss its local quadratic convergence in
Section 2.3. In Section 2.4, we finally establish a connection between our Newton iteration and
the Sinkhorn–Knopp type iteration introduced by Cuturi [2].

2

2.1 problem setting

Let a ∈ Σn and b ∈ Σm be given histograms together with a non-negative cost matrix C ∈ (cid:82)n×m.
The entropically regularized Kantorovich problem of optimal mass transport between a and b is

inf
P ∈U (a,b)

(cid:104)C, P(cid:105) + ε (cid:104)P, log P − (cid:49)n,m(cid:105),

where the logarithm is applied componentwise to P and ε > 0 is the regularization strength.
The variables Pi j indicate how much of ai ends up in bj , while Ci j is the corresponding transport
cost per unit mass. Abbreviating K (cid:66) exp(−C/ε), standard convex duality theory leads us to
the dual problem

sup
f ∈(cid:82)n,д ∈(cid:82)m

−(cid:104)a, f (cid:105) − (cid:104)b, д(cid:105) − ε (cid:104)e−f /ε , Ke−д/ε (cid:105),

where f and д are the dual variables and the exponential function is applied componentwise.
The problems (Pε ) and (Dε ) are linked via the optimality conditions

P = Diag(e−f /ε )K Diag(e−д/ε )
a = Diag(e−f /ε )Ke−д/ε
b = Diag(e−д/ε )K (cid:62)e−f /ε .

The first condition (2.1a) connects the optimal transport plan with the dual variables. The
conditions (2.1b) and (2.1c) simply reflect the feasibility of P for (Pε ), i.e., for the mass conservation
constraints in (1.1).

2.2 algorithm

Finding dual vectors f and д that satisfy (2.1b) and (2.1c) is equivalent to finding a root of the
function

i.e., to solving F (f , д) = 0. A Newton iteration for this equation is given by

The Jacobian matrix of F is

where we used (2.1a) to simplify the notation. Performing the Newton step (2.3) requires finding
a solution of the linear equation system

F (f , д) (cid:66)

(cid:18)a − Diag(e−f /ε )Ke−д/ε
b − Diag(e−д/ε )K (cid:62)e−f /ε

(cid:19)

,

(cid:19)

(cid:18)f k +1
дk+1

=

(cid:19)

(cid:18)f k
дk

− JF (f k , дk )−1F (f k , дk ).

JF (f , д) = 1
ε

(cid:20)Diag(P(cid:49)m)
P (cid:62)

P
Diag(P (cid:62)(cid:49)n)

(cid:21)

,

JF (f k , дk )

= −F (f k , дk ).

(cid:19)

(cid:18)δ f
δд

3

(Pε )

(Dε )

(2.1a)

(2.1b)

(2.1c)

(2.2)

(2.3)

(2.4)

(2.5)

Algorithm 1 Sinkhorn-Newton method in primal variable

1: Input: a ∈ Σn, b ∈ Σm, C ∈ (cid:82)n×m
2: Initialize: P 0 = exp(−C/ε), set k = 0
3: repeat
4:

Compute approximate histograms

ak = P k (cid:49)m,

bk = (P k )(cid:62)(cid:49)n .

5:

Compute updates δ f and δд by solving

(cid:20)Diag(ak )
(P k )(cid:62)

1
ε

P k
Diag(bk )

(cid:21)

(cid:21) (cid:20)δ f
δд

=

(cid:21)

(cid:20)ak − a
bk − b

.

6:

Update P by

P k+1 = Diag(e−δ f /ε )P k Diag(e−δ д/ε ).

k ← k + 1

7:
8: until some stopping criteria fulfilled

The new iterates are then given by

f k+1 = f k + δ f
дk+1 = дk + δд.

If one is only interested in the optimal transport plan, then it is actually not necessary to keep
track of the dual iterates f k and дk after initialization (in our subsequent experiments, we use
f 0 = д0 = 0 and hence, P 0 = K). This is true because (2.5) can be expressed entirely in terms of

P k (cid:66) Diag(e−f k /ε )K Diag(e−дk /ε ),

and thus, using (2.6) and (2.7), we obtain the multiplicative update rule

P k+1 = Diag(e−[f k +δ f ]/ε )K Diag(e−[дk +δ д]/ε )
= Diag(e−δ f /ε )P k Diag(e−δ д/ε ).

In this way, we obtain an algorithm which only operates with primal variables, see Algorithm 1.
In applications where the storage demand for the plans P k is too high and one is only interested
in the optimal value, there is another form which does not form the plans P k , but only the dual
variables f k and дk and which can basically operate matrix-free. We sketch it as Algorithm 2
below.

(2.6a)

(2.6b)

(2.7)

(2.8)

4

Algorithm 2 Sinkhorn-Newton method in dual variables

1: Input: a ∈ Σn, b ∈ Σm, function handle for application of K and K (cid:62)
2: Initialize: a0 ∈ (cid:82)n, b0 ∈ (cid:82)m, set k = 0
3: repeat
4:

Compute approximate histograms

ak = e−f k /ε (cid:12) Ke−дk /ε ,

bk = e−дk /ε (cid:12) K (cid:62)e−f k /ε .

5:

Compute updates δ f and δд by solving

(cid:21)

(cid:20)δ f
δд

=

(cid:21)

(cid:20)ak − a
bk − b

M

where the application of M is given by

(cid:34)

M

(cid:21)

(cid:20)δ f
δд

= 1
ε

ak (cid:12) δ f + e−f k /ε (cid:12) K(e−дk /ε (cid:12) δд)
bk (cid:12) δд + e−дk /ε (cid:12) K (cid:62)(e−f k /ε (cid:12) δ f )

(cid:35)

.

6:

Update f and д by

f k+1 = f k + δ f ,

дk+1 = дk + δд.

k ← k + 1

7:
8: until some stopping criteria fulfilled

2.3 convergence and numerical aspects

In the following, we first argue that (2.5) is solvable. Then we show that the sequence of Newton
iterates converges locally at a quadratic rate as long as the optimal transport plan satisfies
P ≥ c · (cid:49)n,m for some constant c > 0.
Lemma 2.1. For f ∈ (cid:82)n and д ∈ (cid:82)m, the Jacobian matrix JF (f , д) is symmetric positive semi-
definite, and its kernel is given by

ker [JF (f , д)] = span

(cid:26)(cid:18) (cid:49)n
−(cid:49)m

(cid:19)(cid:27)

.

Proof. The matrix is obviously symmetric. For arbitrary φ ∈ (cid:82)n and γ ∈ (cid:82)m, we obtain from
(2.4) that

(cid:0)φ(cid:62) γ (cid:62)(cid:1) JF (f , д)

Pi j (φi + γj )2 ≥ 0,

(cid:19)

(cid:18)φ
γ

= 1
ε

(cid:213)

i j

which holds with equality if and only if we have φi + γj = 0 for all i, j.

Hence, the system (2.5) can be solved by a conjugate gradient (CG) method. To see that,
recall that the CG method iterates on the orthogonal complement of the kernel as long as

(2.9)

(2.10)

(cid:3)

5

the initial iterate (δ f 0, δд0) is chosen from this subspace, in this case with (cid:49)(cid:62)
mδд0.
Furthermore, the Newton matrix can be applied matrix-free in an efficient manner as soon as the
multiplication with K = exp(−C/ε) and its transpose can be done efficiently, see Algorithm 2.
This is the case, for example if Ci j only depends on i − j and thus, multiplication with K amounts
to a convolution. A cheap diagonal preconditioner is provided by the matrix

n δ f 0 = (cid:49)(cid:62)

(cid:20)Diag(P k (cid:49)n)
0

1
ε

0
Diag([P k ](cid:62)(cid:49)m)

(cid:21)

.

According to Deuflhard [4, Thm. 2.3], we expect local quadratic convergence as long as

(cid:107) JF (y k )−1[JF (y k ) − JF (η)](y k − η)(cid:107) ≤ ω (cid:107)y k − η(cid:107)2

holds for all η ∈ (cid:82)n × (cid:82)m and k ∈ (cid:78), with an arbitrary norm and some constant ω > 0 in a
neighborhood of the solution. Here, we abbreviated y k (cid:66) (f k , дk ).

Theorem 2.2. For any k ∈ (cid:78) with P k

i j > 0, (2.12) holds in the (cid:96)∞-norm for

(cid:32)

ω ≤ (e

1
ε − 1)

1 + 2e

1
ε

max (cid:8)(cid:107)P k (cid:49)m (cid:107)∞, (cid:107)[P k ](cid:62)(cid:49)n (cid:107)∞
mini j P k
i j

(cid:9)

(cid:33)

when (cid:107)y k − η(cid:107)∞ ≤ 1.

We postpone the proof of Theorem 2.2 to Section 4.

Remark 2.3. In fact, one can show that necessarily ω ≥ e
then one can explicitly compute

1

ε −1. Indeed, if y k −η = (φ, 0) ∈ (cid:82)n ×(cid:82)n,

JF (y k )−1[JF (y k ) − JF (η)](y k − η) = ((eφ/ε − 1)φ, 0),

where the exponential and the multiplication are pointwise (the calculation is detailed in the
proof of Theorem 2.2).

Hence, if (f 0, д0) is chosen sufficiently close to a solution of F (f , д) = 0, then the contraction
property of Newton’s method shows that the sequence of Newton iterates (f k , дk ), and hence
P k , remain bounded. If the optimal plan satisfies P ∗ ≥ c · (cid:49)n,m for some c > 0, we can therefore
expect local quadractic convergence of Newton’s method.

2.4 relation to sinkhorn–knopp

Substituting u (cid:66) e−f /ε and v (cid:66) e−д/ε in (2.1) shows that the optimality system can be written
equivalently as

(2.11)

(2.12)

(2.13)

(2.14a)

(2.14b)

(2.14c)

P = Diag(u)K Diag(v)
a = Diag(u)Kv
b = Diag(v)K (cid:62)u.

6

In order to find a solution of (2.14b)–(2.14c), one can apply the Sinkhorn–Knopp algorithm [11]
as recently proposed in Cuturi [2]. This amounts to alternating updates in the form of

In (2.15a), uk +1 is updated such that uk+1 and vk solve (2.14b), and in the subsequent (2.15b), vk+1
is updated such that uk +1 and vk +1 form a solution of (2.14c).

If we proceed analogously to Section 2.2 and derive a Newton iteration to find a root of the

function

then the associated Jacobian matrix is

Neglecting the off-diagonal blocks in (2.17) and using the approximation

uk+1 (cid:66) Diag(Kvk )−1a
vk+1 (cid:66) Diag(K (cid:62)uk+1)−1b.

G(u, v) (cid:66)

(cid:18)Diag(u)Kv − a
Diag(v)K (cid:62)u − b

(cid:19)

,

JG (u, v) =

(cid:18)Diag(Kv)
Diag(v)K (cid:62) Diag(K (cid:62)u)

Diag(u)K

(cid:19)

.

ˆJG (u, v) =

(cid:18)Diag(Kv) 0
0

Diag(K (cid:62)u)

(cid:19)

(cid:19)

(cid:18)uk +1
vk+1

=

(cid:19)

(cid:18)uk
vk

− ˆJG (uk , vk )−1G(uk , vk )

uk+1 (cid:66) Diag(Kvk )−1a
vk+1 (cid:66) Diag(K (cid:62)uk )−1b.

(2.15a)

(2.15b)

(2.16)

(2.17)

(2.18)

(2.19)

(2.20a)

(2.20b)

to perform the Newton iteration

leads us to the parallel updates

Hence, we see that a Sinkhorn–Knopp step (2.15) simply approximates one Newton step (2.20)
by neglecting the off-diagonal blocks and replacing uk by uk +1 in (2.20b). In our experience,
neither the Newton iteration for G(u, v) = 0 (which seems to work for the less general problem
of matrix balancing; see Knight & Ruiz [8]) nor the version of Sinkhorn–Knopp in which vk+1
is updated using uk instead of uk +1 converge.

3 numerical examples

We illustrate the performance of the Sinkhorn–Newton method and its behavior by several
examples. We note that a numerical comparison is not straightforward as there a several
possibilities to tune the method, depending on the structure at hand and on the specific goals.
As illustrated in Section 2.2, one could take advantage of fast applications of the matrix K or
use less memory if one does not want to store P during the iteration. Here we focus on the

7

105

100

10−5

10−10

10−15

0

S-viol.
S–cost
S–plan
N–viol.
N–cost
N–plan

105

100

10−5

10−10

10−15

0

S-viol.
S–cost
S–plan
N–viol.
N–cost
N–plan

1,000

2,000

3,000

0.5

1

1.5

(a) Errors over iterations (CG for Newton).

(b) Errors over run time in seconds.

Figure 1: Performance of Sinkhorn (S) and Newton (N) iterations measured by constraint viola-
tion (viol.), distance to optimal transport cost (cost) and distance to optimal transport
plan (plan).

comparison with the usual (linearly convergent) Sinkhorn iteration. Thus, we do not aim for
greatest overall speed but for a fair comparison between the Sinkhorn-Newton method and
the Sinkhorn iteration. To that end, we observe that one application of the Newton matrix (2.4)
amounts to one multiplication with P and P (cid:62) each, two coordinate-wise products and sums of
vectors. For one Sinkhorn iteration we need one multiplication with K and K (cid:62) and two additional
coordinate-wise operations. Although Algorithm 2 looks a little closer to the Sinkhorn iteration,
we still compare Algorithm 1, as we did not exploit any of the special structure in K or P.

All timings are reported using MATLAB (R2017b) implementations of the methods on an
Intel Xeon E3-1270v3 (four cores at 3.5 GHz) with 16 GB RAM. The code used to generate the
results below can be downloaded from https://github.com/dirloren/sinkhornnewton.

In all our experiments, we address the case m = n and the considered histograms are defined
i=1 ⊂ [0, 1]d with d = 2 (in Sections 3.1 and 3.2) and d = 1 (in Section 3.3),

on equidistant grids {xi }n
respectively. Throughout, the cost is chosen as quadratic, i.e., Ci j (cid:66) (cid:107)xi − xj (cid:107)2
2.

Our Sinkhorn–Newton method is implemented according to Algorithm 1 and using a pre-
conditioned CG method. The iteration is terminated as soon as the maximal violation of the
constraints (cid:107)ak − a(cid:107)∞ and (cid:107)bk − b (cid:107)∞ drops below some threshold. If applicable, the same
termination criterion is chosen for the Sinkhorn method, which is initialized with u0 = v0 = (cid:49)n.

3.1 comparison with sinkhorn–knopp

We first address the comparison of Sinkhorn–Newton with the classical Sinkhorn iteration.
For this purpose, we discretize the unit square [0, 1]2 using a 20 × 20 equidistant grid {xi =

8

(xi1, xi2)}400

i=1 ⊂ [0, 1]2 and take

which are then normalized to unit mass by setting

˜ai (cid:66) e−36([xi 1− 1
˜bj (cid:66) e−9([x j1− 2

3 ]2−[xi 2− 1

3 ]2−[x j 2− 2

3 ]2) + 10−1,
3 ]2) + 10−1,

a (cid:66) ˜a
(cid:205)
i ˜ai

and b (cid:66)

˜b
(cid:205)

j

.

˜bj

(3.1a)

(3.1b)

(3.2)

The entropic regularization parameter is set to ε (cid:66) 10−3 and in case of Sinkhorn-Newton, the
CG method is implemented with a tolerance of 10−13 and a maximum number of 34 iterations.
Moreover, the threshold for the termination criterion is chosen as 10−13.

Figure 1 shows the convergence history of the constraint violation for both iterations together
with the error in the unregularized transport cost |(cid:104)C, P k − P ∗(cid:105)|, where P ∗ denotes the final
transport plan, and the error in the transport plan (cid:107)P k − P ∗(cid:107)1. In Figure 1a, we compare the
error as a function of the iterations, where we take the total number of CG iterations for
Sinkhorn–Newton to allow for a fair comparison (since both a Sinkhorn and a CG step have
comparable costs, dominated by the two dense matrix–vector products Kv, K (cid:62)u and P (cid:62)δ f , Pδд,
respectively). It can be seen clearly that with respect to all error measures, Sinkhorn converges
linearly while Sinkhorn–Newton converges roughly quadratically, as expected, with Sinkhorn–
Newton significantly outperforming classical Sinkhorn for this choice of parameters. The same
behavior holds if the error is measured as a function of runtime; see Figure 1b.

3.2 dependence on the regularization strength

The second example addresses the dependence of the Sinkhorn–Newton method on the problem
parameters. In particular, we consider the dependence on ε and on the minimal value of a and b
(via the corresponding transport plans P), since these enter into the convergence rate estimate
(2.13). Here, we take an example which is also used in Cuturi [2]: computing the transport
distances between different images from the MNIST database, which contains 28 × 28 images
of handwritten digits. We consider these as discrete distributions of dimension 282 = 784 on
[0, 1]2, to which we add a small offset γ before normalizing to unit mass as before. Here, the
tolerance for both the Newton and the CG iteration is set to 10−12, and the maximum number of
CG iterations is fixed at 66. The entropic regularization parameter ε is chosen as multiples of
the median of the cost (which is q50 = 0.2821 in this case).

Figure 2 shows the convergence history for different offsets γ ∈ {0.5, 0.1, 0.01} and ε ∈
q50 · {1, 0.1, 0.01, 0.005}, where we again report the constraint violation both as a function of CG
iterations and of the run time in seconds. Comparing Figures 2a to 2f, we see that as ε decreases,
an increasing number of CG iterations is required to achieve the prescribed tolerance. However,
the convergence seems to be robust in ε at least for larger values of ε and only moderately
deteriorate for ε ≤ 0.01q50.

9

105

100

10−5

10−10

105

100

10−5

10−10

105

100

10−5

10−10

10−15

0

200

400

600

800

1,000

1,200

0.1

0.2

0.3

0.4

0.5

10−15

0

(a) γ = 0.5 (CG iterations)

(b) γ = 0.5 (run time in seconds)

10−15

0

200

400

600

800

1,000

1,200

0.1

0.2

0.3

0.4

0.5

10−15

0

(c) γ = 0.1 (CG iterations)

(d) γ = 0.1 (run time in seconds)

ε = q50
ε = 0.1 · q50
ε = 0.01 · q50
ε = 0.005 · q50

ε = q50
ε = 0.1 · q50
ε = 0.01 · q50
ε = 0.005 · q50

ε = q50
ε = 0.1 · q50
ε = 0.01 · q50
ε = 0.005 · q50

ε = q50
ε = 0.1 · q50
ε = 0.01 · q50
ε = 0.005 · q50

ε = q50
ε = 0.1 · q50
ε = 0.01 · q50
ε = 0.005 · q50

ε = q50
ε = 0.1 · q50
ε = 0.01 · q50
ε = 0.005 · q50

105

100

10−5

10−10

105

100

10−5

10−10

105

100

10−5

10−10

10

10−15

0

200

400

600

800

1,000

1,200

0.1

0.2

0.3

0.4

0.5

10−15

0

(e) γ = 0.01 (CG iterations)

(f) γ = 0.01 (run time in seconds)

Figure 2: Constraint violation for different offsets γ and different regularization parameters ε.

105

100

10−5

10−10

10−15

0

105

100

10−5

10−10

10−15

0

n = 1000
n = 2000
n = 4000
n = 8000

n = 1000
n = 2000
n = 4000
n = 8000

200

400

600

800

1,000

1,200

20

40

60

(a) Errors over CG iterations.

(b) Errors over run time in seconds.

Figure 3: Constraint violation for different mesh sizes n

3.3 dependence on the problem dimension

We finally address the dependence on the dimension of the problem. For this purpose, we
discretize the unit interval [0, 1] using n equidistant points xi ∈ [0, 1] and take

˜ai = e−100(xi −0.2)2 + e−20|xi −0.4 | + 10−2,
˜bj = e−100(x j −0.6)2 + 10−2,

(3.3a)

(3.3b)

which are again normalized to unit mass to obtain a and b. The regularization parameter is fixed
at ε = 10−3. Moreover, the inner and outer tolerances are here set to 10−10, and the maximum
number of CG iterations is coupled to the mesh size via (cid:100)n/12(cid:101).

Figure 3 shows the convergence behavior of Sinkhorn–Newton for n ∈ {1000, 2000, 4000, 8000}.

As can be seen from Figure 3a, the behavior is nearly independent of n; in particular, the number
of CG iterations required to reach the prescribed tolerance stays almost the same. (This is also
true for the Newton method itself with 21, 22, 23 and 23 iterations.) Since each CG iteration
involves two dense matrix–vector products with complexity O(n2), the total run time scales
quadratically; see Figure 3b.

4 proof of theorem 2.2

For the sake of presentation, we restrict ourselves to the case m = n here. However, in the end,
we suggest how the proof can be generalized to the case m (cid:44) n.

To estimate (2.12) and in particular JF (y k ) − JF (η) for y k = (f k , дk ) and η = (α, β) ∈ (cid:82)n × (cid:82)n

we observe that

JF (y k ) − JF (η) = 1
ε

(cid:34)

e

−ci j −f k
i
ε

−дk
j

−ci j −αi −βj
ε

− e

(cid:35)

(cid:34)

=

P k
i j (1 − e

f k
i

−αi +дk
j
ε

−βj

i j

(cid:35)

)

.

i j

11

To keep the notation concise, we abbreviate ψ = (φ, γ ) (cid:66) y k − η and also write y and P for y k
and P k , respectively. Then, we compute

JF (y)−1[JF (y) − JF (η)](y − η) = JF (y)−1

= JF (y)−1

(cid:16)(cid:205)
(cid:16)(cid:205)

(cid:205)
j

(cid:205)
i

(cid:19)

(cid:18)
















(cid:18)k
l

(cid:18)

j Pi j (e(φi +γj )/ε − 1)(φi + γj )/ε
i Pi j (e(φi +γj )/ε − 1)(φi + γj )/ε
(cid:19)

(cid:17)

(cid:17)








i

j

Pi j

Pi j

∞
(cid:205)
k=2
∞
(cid:205)
k =2

1
(k −1)!ε k

1
(k −1)!ε k

k
(cid:205)
l =0
k
(cid:205)
l =0

(cid:0)k
l

(cid:1)φl

iγ k−l

j

(cid:0)k
l

(cid:1)φl

iγ k−l

j

(cid:16)(cid:205)
(cid:16)(cid:205)

j Pi jφl
i Pi jφl

i

(cid:19)










iγ k −l
iγ k −l

j

j

j








(cid:17)

(cid:17)








i

j

=

∞
(cid:213)

k
(cid:213)

k=2

l =0

1
(k − 1)!εk

JF (y)−1

where all exponents are applied componentwise. Now we first treat only the summands for
l = 0 and l = k. For those terms (2.4) immediately implies

∞
(cid:213)

(cid:213)

k=2

l =0,k

(cid:19)

(cid:18)k
l

1
(k − 1)!εk

JF (y)−1

j Pi j (φk
i
i Pi j (φk
i

(cid:17)

+ γ k
j )
(cid:17)
+ γ k
j )

(cid:16)(cid:205)
(cid:16)(cid:205)








∞
(cid:213)

(cid:213)

=

k=2

l =0,k








i

j

1
(k − 1)!εk−1

(cid:21)

(cid:20)φk
γ k

=

(cid:21)

(cid:20)(eφ/ε − 1)φ
(eγ /ε − 1)γ

,

which has supremum norm bounded by (e
summands (i.e. 1 ≤ l ≤ k − 1), we write

1

ε − 1)(cid:107)(φ, γ )(cid:107)2

∞ for all (cid:107)(φ, γ )(cid:107)∞ ≤ 1. For all other

(cid:21)

(cid:20)α
β

(cid:66)

(cid:1)

(cid:0)k
l
(k − 1)!εk

JF (y)−1

(cid:16)(cid:205)
(cid:16)(cid:205)








j

j Pi jφl
i Pi jφl

iγ k−l
iγ k−l

j

(cid:17)

(cid:17)








i

j

.

Using (2.4) again, it follows that

(cid:20)Diag(P(cid:49)n)
P (cid:62)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

(cid:124)

P
Diag(P (cid:62)(cid:49)n)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:125)

(cid:123)(cid:122)
(cid:67)A

(cid:21)

(cid:21)

(cid:20)α
β

=

(cid:1)

(cid:0)k
l
(k − 1)!εk −1

(cid:20)Diag(Pγ k−l )φl
Diag(P (cid:62)φl )γ k−l

(cid:21)

,

and we aim to estimate (cid:107)α (cid:107)∞ and (cid:107)β (cid:107)∞ by (cid:107)φ (cid:107)∞ and (cid:107)γ (cid:107)∞. By Lemma 2.1, the matrix A has a
one-dimensional kernel spanned by q (cid:66) ((cid:49)(cid:62)
n )(cid:62), and a solution (α, β) in the orthogonal
n , −(cid:49)(cid:62)
complement is also a solution to

(cid:21)

(cid:20)α
β

=

(cid:1)

(cid:0)k
l
(k − 1)!εk−1

(cid:20)Diag(Pγ k −l )φl
Diag(P (cid:62)φl )γ k −l

(cid:21)

(A + ∆qq(cid:62))
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:123)(cid:122)
(cid:125)
(cid:124)
B

for any ∆ > 0. From Varah [12], we know that the (cid:96)∞-norm of the inverse matrix of B is
estimated by

(cid:107)B−1(cid:107)∞ ≤ (cid:2)mini

(cid:0)|Bii | − (cid:205)

j(cid:44)i |Bi j |(cid:1) (cid:3) −1 .

(4.1)

12

In this case, we calculate for i = 1, . . . , n that

|Bii | −

(cid:213)

|Bi j | = (cid:213)

1≤j ≤2n
j(cid:44)i

1≤j ≤n

Pi j + ∆ − (n − 1)∆ −

|Pi j − ∆|.

(cid:213)

1≤j ≤n

For any ∆ ≤ minj Pi j , this leads to

Similarly, we get that

|Bn+i,n+i | −

|Bn+i, j | ≤ 2∆.

Choosing ∆ (cid:66) mini j Pi j , we thus obtain that

|Bii | −

|Bi j | ≤ 2∆.

(cid:213)

1≤j ≤2n
j(cid:44)i

(cid:213)

1≤j ≤2n
j(cid:44)n+i

(cid:107)B−1(cid:107)∞ ≤

(cid:20)

2 min
i j

Pi j

(cid:21) −1

.

Using that

(cid:107) Diag(P (cid:62)φl )γ k−l (cid:107) ≤ (cid:107)P (cid:62)(cid:49)n (cid:107)∞(cid:107)φ (cid:107)l

∞(cid:107)γ (cid:107)k −l
∞ ,

and similarly for Diag(Pγ k−l )φl , finally gives

(cid:107)(α, β)(cid:107)∞ ≤

M (cid:107)φ (cid:107)l

∞(cid:107)γ (cid:107)k−l
∞

(cid:1)

(cid:0)k
l
(k − 1)!εk −1

M = max{(cid:107)P(cid:49)n (cid:107)∞, (cid:107)P (cid:62)(cid:49)n (cid:107)∞}

.

2 mini j Pi j

with

Hence we obtain

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∞
(cid:213)

k −1
(cid:213)

k=2

l =1

(cid:19)

(cid:18)k
l

1
(k − 1)!εk

JF (y)−1

(cid:16)(cid:205)
(cid:16)(cid:205)








j

j Pi jφl
i Pi jφl

iγ k −l
iγ k −l

j

k−1
(cid:213)

l =1

(cid:19)

(cid:18)k
l

1
(k − 1)!εk−1

(cid:107)φ (cid:107)l

∞(cid:107)γ (cid:107)k−l
∞

= M

e( (cid:107)φ (cid:107)∞+ (cid:107)γ (cid:107)∞)/ε − 1

((cid:107)φ (cid:107)∞ + (cid:107)γ (cid:107)∞)

(cid:16)

(cid:17)

−

e (cid:107)φ (cid:107)∞/ε − 1

(cid:107)φ (cid:107)∞ −

e (cid:107)γ (cid:107)∞/ε − 1

(cid:107)γ (cid:107)∞

(cid:17)

(cid:21)

(cid:17)

(cid:16)

≤ 2e

1

1
ε M(e

ε − 1)(cid:107)(φ, γ )(cid:107)2
∞

(cid:17)

(cid:17)








i

j

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
∞
(cid:213)

≤ M

k =2
(cid:20) (cid:16)

13

for all (cid:107)(φ, γ )(cid:107)∞ ≤ 1. In summary we obtain

(cid:107) JF (y)−1[JF (y) − JF (η)](y − η)(cid:107)∞ ≤ (1 + 2e

1
ε M)(e

1

ε − 1)(cid:107)y − η(cid:107)2
∞,

To generalize this to the case m (cid:44) n, one can take q = (∆1(cid:49)n, −∆2(cid:49)m) for ∆1 (cid:44) −∆2 and
∆1∆2 = ∆ and choose ∆1 to equilibrate the lower bounds for the first m and the last n rows of B.

We have proposed a Newton iteration to solve the entropically regularized discrete optimal
transport problem. Different from related Newton type approaches for matrix balancing, our
method iterates on the logarithm of the scalings, which seems to be necessary for robust
convergence in the optimal transport setting. Numerical examples show that our algorithm is
a robust and efficient alternative to the more commonly used Sinkhorn–Knopp algorithm, at
least for small regularization strength.

as desired

5 conclusion

references

[1] Carlier, Duval, Peyré & Schmitzer, Convergence of entropic schemes for optimal transport
and gradient flows, SIAM Journal on Mathematical Analysis 49 (2017), 1385–1418, doi: 10.
1137/15M1050264.

[2] Cuturi, Sinkhorn distances: Lightspeed computation of optimal transport, in: Advances in
Neural Information Processing Systems (NIPS) 26, 2013, 2292–2300, url: http://papers.nips.
cc/paper/4927-sinkhorn-distances-lightspeed-computation-of-optimal-transport.
[3] Cuturi & Doucet, Fast computation of Wasserstein barycenters, in: Proceedings of the 31st
International Conference on Machine Learning, 2014, 685–693, url: http://proceedings.mlr.
press/v32/cuturi14.pdf.

[4] Deuflhard, Newton Methods for Nonlinear Problems, Affine Invariance and Adaptive Algo-

rithms, Springer, 2011, doi: 10.1007/978-3-642-23899-4.

[5] Frogner, Zhang, Mobahi, Araya & Poggio, Learning with a Wasserstein loss, in: Advances
in Neural Information Processing Systems (NIPS) 28, 2015, 2053–2061, url: https://papers.
nips.cc/paper/5679-learning-with-a-wasserstein-loss.

[6] Kantorovivc, On the translocation of masses, C. R. (Doklady) Acad. Sci. URSS (N.S.) 37

(1942), 199–201.

[7] Kantorovivc & Rubinvsteuin, On a functional space and certain extremum problems,

Doklady Akademii Nauk SSSR 115 (1957), 1058–1061.

[8] Knight & Ruiz, A fast algorithm for matrix balancing, IMA J. Numer. Anal. 33 (2013), 1029–

1047, doi: 10.1093/imanum/drs019.

14

[9] Kolouri, Park, Thorpe, Slepcev & Rohde, Optimal mass transport: signal processing
and machine-learning applications, IEEE Signal Processing Magazine 34 (2017), 43–59, doi:
10.1109/MSP.2017.2695801.

[10] Monge, Mémoire sur la théorie des déblais et des remblais, Histoire de l’Académie Royale

des Sciences de Paris (1781).

[11] Sinkhorn & Knopp, Concerning nonnegative matrices and doubly stochastic matrices,

Pacific Journal of Mathematics 21 (1967), 343–348, doi: 10.2140/pjm.1967.21.343.

[12] Varah, A lower bound for the smallest singular value of a matrix, Linear Algebra and Appl.

11 (1975), 3–5, doi: 10.1016/0024-3795(75)90112-3.

15

8
1
0
2
 
b
e
F
 
9
 
 
]

C
O
.
h
t
a
m

[
 
 
2
v
5
3
6
6
0
.
0
1
7
1
:
v
i
X
r
a

a sinkhorn–newton method for entropic
optimal transport

Christoph Brauer∗

Christian Clason†

Dirk Lorenz∗

Benedikt Wirth‡

February 2, 2018

Abstract We consider the entropic regularization of discretized optimal transport and
propose to solve its optimality conditions via a logarithmic Newton iteration. We show a
quadratic convergence rate and validate numerically that the method compares favorably
with the more commonly used Sinkhorn–Knopp algorithm for small regularization strength.
We further investigate numerically the robustness of the proposed method with respect to
parameters such as the mesh size of the discretization.

1 introduction

The mathematical problem of optimal mass transport has a long history dating back to its
introduction in Monge [10], with key contributions by Kantorovivc [6] and Kantorovivc &
Rubinvsteuin [7]. It has recently received increased interest due to numerous applications in
machine learning; see, e.g., the recent overview of Kolouri, Park, Thorpe, Slepcev & Rohde
[9] and the references therein. In a nutshell, the (discrete) problem of optimal transport in its
Kantorovich form is to compute for given mass distributions a and b with equal mass a transport
plan, i.e., an assignment of how much mass of a at some point should be moved to another point
to match the mass in b. This should be done in a way such that some transport cost (usually
proportional to the amount of mass and dependent on the distance) is minimized. This leads
to a linear optimization problem which has been well studied, but its application in machine
learning has been problematic due to large memory requirement and long run time. Recently,
Cuturi [2] proposed a method that overcomes the memory requirement by so-called entropic
regularization that has found broad applications; see, e.g., Carlier, Duval, Peyré & Schmitzer
[1], Cuturi & Doucet [3], and Frogner, Zhang, Mobahi, Araya & Poggio [5]. The resulting
iteration resembles the so-called Sinkhorn–Knopp method from Sinkhorn & Knopp [11] for
matrix balancing and allows for a simple and efficient implementation.

∗Institute of Analysis and Algebra, TU Braunschweig, 38092 Braunschweig, Germany (ch.brauer@tu-

braunschweig.de, d.lorenz@tu-braunschweig.de)

†Faculty of Mathematics, University Duisburg-Essen, 45117 Essen, Germany (christian.clason@uni-due.de)
‡Institute for Numerical and Applied Mathematics, University of Münster, Einsteinstraße 62, 48149 Münster, Germany

(benedikt.wirth@uni-muenster.de)

1

1.1 our contribution

In this work, we show that the Sinkhorn–Knopp method can be viewed as an approximate
Newton method and derive a full Newton method for entropically regularized optimal transport
problems that is demonstrated to perform significantly better for small entropic regularization
parameters. Here, compared to Cuturi [2], the key idea is to apply a logarithmic transform to
the variables.

This paper is organized as follows. In Section 2, we state the Kantorovich formulation of
optimal transport together with its dual which serves as the basis of the derived algorithm.
Afterwards, we establish local quadratic convergence and discuss the relation of the proposed
Newton method to the Sinkhorn–Knopp iteration. The performance and parameter dependence
of the proposed method are illustrated with numerical examples in Section 3. Section 4 contains
the proof of the key estimate for quadratic convergence, and Section 5 concludes the paper.

1.2 notation

In the following, (cid:49)n represents the n-dimensional vector with all ones and (cid:49)n,m refers to the
n a = 1} denotes the probability
n × m matrix with all ones. Moreover, Σn (cid:66) {a ∈ (cid:82)n
simplex in (cid:82)n
+ whose elements are called probability vectors, or equivalently, histograms. For
two histograms a ∈ Σn and b ∈ Σm,

+ : (cid:49)(cid:62)

U (a, b) (cid:66) {P ∈ (cid:82)n×m

+

: P(cid:49)m = a, P (cid:62)(cid:49)n = b}

(1.1)

is the set of admissible coupling matrices. In the context of optimal transport, the elements
of U (a, b) are also referred to as transport plans. Histograms a and b can be viewed as mass
distributions, and an entry Pi j of a transport plan P ∈ U (a, b) can be interpreted as the amount
of mass moved from ai to bj .

We refer to the Frobenius inner product of two matrices P, P (cid:48) ∈ (cid:82)n×m as (cid:104)P, P (cid:48)(cid:105) (cid:66) (cid:205)

i j Pi jP (cid:48)
i j .
At the same time, (cid:104)a, a(cid:48)(cid:105) (cid:66) (cid:205)
i denotes the standard dot product of two vectors a, a(cid:48) ∈ (cid:82)n.
Finally, Diag(a) ∈ (cid:82)n×n is defined as the diagonal matrix with Diag(a)ii (cid:66) ai and Diag(a)i j (cid:66) 0
for i (cid:44) j, and a (cid:12) a(cid:48) (cid:66) Diag(a)a(cid:48) is the Hadamard product (i.e., the component-wise product) of
a and a(cid:48).

i aia(cid:48)

2 sinkhorn–newton method

In this section we derive our Sinkhorn–Newton method. We start by introducing the problem of
entropically regularized optimal transport in Section 2.1. Afterwards, in Section 2.2, we present
our approach, which is essentially applying Newton’s method to the optimality system associated
with the transport problem and its dual, before we discuss its local quadratic convergence in
Section 2.3. In Section 2.4, we finally establish a connection between our Newton iteration and
the Sinkhorn–Knopp type iteration introduced by Cuturi [2].

2

2.1 problem setting

Let a ∈ Σn and b ∈ Σm be given histograms together with a non-negative cost matrix C ∈ (cid:82)n×m.
The entropically regularized Kantorovich problem of optimal mass transport between a and b is

inf
P ∈U (a,b)

(cid:104)C, P(cid:105) + ε (cid:104)P, log P − (cid:49)n,m(cid:105),

where the logarithm is applied componentwise to P and ε > 0 is the regularization strength.
The variables Pi j indicate how much of ai ends up in bj , while Ci j is the corresponding transport
cost per unit mass. Abbreviating K (cid:66) exp(−C/ε), standard convex duality theory leads us to
the dual problem

sup
f ∈(cid:82)n,д ∈(cid:82)m

−(cid:104)a, f (cid:105) − (cid:104)b, д(cid:105) − ε (cid:104)e−f /ε , Ke−д/ε (cid:105),

where f and д are the dual variables and the exponential function is applied componentwise.
The problems (Pε ) and (Dε ) are linked via the optimality conditions

P = Diag(e−f /ε )K Diag(e−д/ε )
a = Diag(e−f /ε )Ke−д/ε
b = Diag(e−д/ε )K (cid:62)e−f /ε .

The first condition (2.1a) connects the optimal transport plan with the dual variables. The
conditions (2.1b) and (2.1c) simply reflect the feasibility of P for (Pε ), i.e., for the mass conservation
constraints in (1.1).

2.2 algorithm

Finding dual vectors f and д that satisfy (2.1b) and (2.1c) is equivalent to finding a root of the
function

i.e., to solving F (f , д) = 0. A Newton iteration for this equation is given by

The Jacobian matrix of F is

where we used (2.1a) to simplify the notation. Performing the Newton step (2.3) requires finding
a solution of the linear equation system

F (f , д) (cid:66)

(cid:18)a − Diag(e−f /ε )Ke−д/ε
b − Diag(e−д/ε )K (cid:62)e−f /ε

(cid:19)

,

(cid:19)

(cid:18)f k +1
дk+1

=

(cid:19)

(cid:18)f k
дk

− JF (f k , дk )−1F (f k , дk ).

JF (f , д) = 1
ε

(cid:20)Diag(P(cid:49)m)
P (cid:62)

P
Diag(P (cid:62)(cid:49)n)

(cid:21)

,

JF (f k , дk )

= −F (f k , дk ).

(cid:19)

(cid:18)δ f
δд

3

(Pε )

(Dε )

(2.1a)

(2.1b)

(2.1c)

(2.2)

(2.3)

(2.4)

(2.5)

Algorithm 1 Sinkhorn-Newton method in primal variable

1: Input: a ∈ Σn, b ∈ Σm, C ∈ (cid:82)n×m
2: Initialize: P 0 = exp(−C/ε), set k = 0
3: repeat
4:

Compute approximate histograms

ak = P k (cid:49)m,

bk = (P k )(cid:62)(cid:49)n .

5:

Compute updates δ f and δд by solving

(cid:20)Diag(ak )
(P k )(cid:62)

1
ε

P k
Diag(bk )

(cid:21)

(cid:21) (cid:20)δ f
δд

=

(cid:21)

(cid:20)ak − a
bk − b

.

6:

Update P by

P k+1 = Diag(e−δ f /ε )P k Diag(e−δ д/ε ).

k ← k + 1

7:
8: until some stopping criteria fulfilled

The new iterates are then given by

f k+1 = f k + δ f
дk+1 = дk + δд.

If one is only interested in the optimal transport plan, then it is actually not necessary to keep
track of the dual iterates f k and дk after initialization (in our subsequent experiments, we use
f 0 = д0 = 0 and hence, P 0 = K). This is true because (2.5) can be expressed entirely in terms of

P k (cid:66) Diag(e−f k /ε )K Diag(e−дk /ε ),

and thus, using (2.6) and (2.7), we obtain the multiplicative update rule

P k+1 = Diag(e−[f k +δ f ]/ε )K Diag(e−[дk +δ д]/ε )
= Diag(e−δ f /ε )P k Diag(e−δ д/ε ).

In this way, we obtain an algorithm which only operates with primal variables, see Algorithm 1.
In applications where the storage demand for the plans P k is too high and one is only interested
in the optimal value, there is another form which does not form the plans P k , but only the dual
variables f k and дk and which can basically operate matrix-free. We sketch it as Algorithm 2
below.

(2.6a)

(2.6b)

(2.7)

(2.8)

4

Algorithm 2 Sinkhorn-Newton method in dual variables

1: Input: a ∈ Σn, b ∈ Σm, function handle for application of K and K (cid:62)
2: Initialize: a0 ∈ (cid:82)n, b0 ∈ (cid:82)m, set k = 0
3: repeat
4:

Compute approximate histograms

ak = e−f k /ε (cid:12) Ke−дk /ε ,

bk = e−дk /ε (cid:12) K (cid:62)e−f k /ε .

5:

Compute updates δ f and δд by solving

(cid:21)

(cid:20)δ f
δд

=

(cid:21)

(cid:20)ak − a
bk − b

M

where the application of M is given by

(cid:34)

M

(cid:21)

(cid:20)δ f
δд

= 1
ε

ak (cid:12) δ f + e−f k /ε (cid:12) K(e−дk /ε (cid:12) δд)
bk (cid:12) δд + e−дk /ε (cid:12) K (cid:62)(e−f k /ε (cid:12) δ f )

(cid:35)

.

6:

Update f and д by

f k+1 = f k + δ f ,

дk+1 = дk + δд.

k ← k + 1

7:
8: until some stopping criteria fulfilled

2.3 convergence and numerical aspects

In the following, we first argue that (2.5) is solvable. Then we show that the sequence of Newton
iterates converges locally at a quadratic rate as long as the optimal transport plan satisfies
P ≥ c · (cid:49)n,m for some constant c > 0.
Lemma 2.1. For f ∈ (cid:82)n and д ∈ (cid:82)m, the Jacobian matrix JF (f , д) is symmetric positive semi-
definite, and its kernel is given by

ker [JF (f , д)] = span

(cid:26)(cid:18) (cid:49)n
−(cid:49)m

(cid:19)(cid:27)

.

Proof. The matrix is obviously symmetric. For arbitrary φ ∈ (cid:82)n and γ ∈ (cid:82)m, we obtain from
(2.4) that

(cid:0)φ(cid:62) γ (cid:62)(cid:1) JF (f , д)

Pi j (φi + γj )2 ≥ 0,

(cid:19)

(cid:18)φ
γ

= 1
ε

(cid:213)

i j

which holds with equality if and only if we have φi + γj = 0 for all i, j.

Hence, the system (2.5) can be solved by a conjugate gradient (CG) method. To see that,
recall that the CG method iterates on the orthogonal complement of the kernel as long as

(2.9)

(2.10)

(cid:3)

5

the initial iterate (δ f 0, δд0) is chosen from this subspace, in this case with (cid:49)(cid:62)
mδд0.
Furthermore, the Newton matrix can be applied matrix-free in an efficient manner as soon as the
multiplication with K = exp(−C/ε) and its transpose can be done efficiently, see Algorithm 2.
This is the case, for example if Ci j only depends on i − j and thus, multiplication with K amounts
to a convolution. A cheap diagonal preconditioner is provided by the matrix

n δ f 0 = (cid:49)(cid:62)

(cid:20)Diag(P k (cid:49)n)
0

1
ε

0
Diag([P k ](cid:62)(cid:49)m)

(cid:21)

.

According to Deuflhard [4, Thm. 2.3], we expect local quadratic convergence as long as

(cid:107) JF (y k )−1[JF (y k ) − JF (η)](y k − η)(cid:107) ≤ ω (cid:107)y k − η(cid:107)2

holds for all η ∈ (cid:82)n × (cid:82)m and k ∈ (cid:78), with an arbitrary norm and some constant ω > 0 in a
neighborhood of the solution. Here, we abbreviated y k (cid:66) (f k , дk ).

Theorem 2.2. For any k ∈ (cid:78) with P k

i j > 0, (2.12) holds in the (cid:96)∞-norm for

(cid:32)

ω ≤ (e

1
ε − 1)

1 + 2e

1
ε

max (cid:8)(cid:107)P k (cid:49)m (cid:107)∞, (cid:107)[P k ](cid:62)(cid:49)n (cid:107)∞
mini j P k
i j

(cid:9)

(cid:33)

when (cid:107)y k − η(cid:107)∞ ≤ 1.

We postpone the proof of Theorem 2.2 to Section 4.

Remark 2.3. In fact, one can show that necessarily ω ≥ e
then one can explicitly compute

1

ε −1. Indeed, if y k −η = (φ, 0) ∈ (cid:82)n ×(cid:82)n,

JF (y k )−1[JF (y k ) − JF (η)](y k − η) = ((eφ/ε − 1)φ, 0),

where the exponential and the multiplication are pointwise (the calculation is detailed in the
proof of Theorem 2.2).

Hence, if (f 0, д0) is chosen sufficiently close to a solution of F (f , д) = 0, then the contraction
property of Newton’s method shows that the sequence of Newton iterates (f k , дk ), and hence
P k , remain bounded. If the optimal plan satisfies P ∗ ≥ c · (cid:49)n,m for some c > 0, we can therefore
expect local quadractic convergence of Newton’s method.

2.4 relation to sinkhorn–knopp

Substituting u (cid:66) e−f /ε and v (cid:66) e−д/ε in (2.1) shows that the optimality system can be written
equivalently as

(2.11)

(2.12)

(2.13)

(2.14a)

(2.14b)

(2.14c)

P = Diag(u)K Diag(v)
a = Diag(u)Kv
b = Diag(v)K (cid:62)u.

6

In order to find a solution of (2.14b)–(2.14c), one can apply the Sinkhorn–Knopp algorithm [11]
as recently proposed in Cuturi [2]. This amounts to alternating updates in the form of

In (2.15a), uk +1 is updated such that uk+1 and vk solve (2.14b), and in the subsequent (2.15b), vk+1
is updated such that uk +1 and vk +1 form a solution of (2.14c).

If we proceed analogously to Section 2.2 and derive a Newton iteration to find a root of the

function

then the associated Jacobian matrix is

Neglecting the off-diagonal blocks in (2.17) and using the approximation

uk+1 (cid:66) Diag(Kvk )−1a
vk+1 (cid:66) Diag(K (cid:62)uk+1)−1b.

G(u, v) (cid:66)

(cid:18)Diag(u)Kv − a
Diag(v)K (cid:62)u − b

(cid:19)

,

JG (u, v) =

(cid:18)Diag(Kv)
Diag(v)K (cid:62) Diag(K (cid:62)u)

Diag(u)K

(cid:19)

.

ˆJG (u, v) =

(cid:18)Diag(Kv) 0
0

Diag(K (cid:62)u)

(cid:19)

(cid:19)

(cid:18)uk +1
vk+1

=

(cid:19)

(cid:18)uk
vk

− ˆJG (uk , vk )−1G(uk , vk )

uk+1 (cid:66) Diag(Kvk )−1a
vk+1 (cid:66) Diag(K (cid:62)uk )−1b.

(2.15a)

(2.15b)

(2.16)

(2.17)

(2.18)

(2.19)

(2.20a)

(2.20b)

to perform the Newton iteration

leads us to the parallel updates

Hence, we see that a Sinkhorn–Knopp step (2.15) simply approximates one Newton step (2.20)
by neglecting the off-diagonal blocks and replacing uk by uk +1 in (2.20b). In our experience,
neither the Newton iteration for G(u, v) = 0 (which seems to work for the less general problem
of matrix balancing; see Knight & Ruiz [8]) nor the version of Sinkhorn–Knopp in which vk+1
is updated using uk instead of uk +1 converge.

3 numerical examples

We illustrate the performance of the Sinkhorn–Newton method and its behavior by several
examples. We note that a numerical comparison is not straightforward as there a several
possibilities to tune the method, depending on the structure at hand and on the specific goals.
As illustrated in Section 2.2, one could take advantage of fast applications of the matrix K or
use less memory if one does not want to store P during the iteration. Here we focus on the

7

105

100

10−5

10−10

10−15

0

S-viol.
S–cost
S–plan
N–viol.
N–cost
N–plan

105

100

10−5

10−10

10−15

0

S-viol.
S–cost
S–plan
N–viol.
N–cost
N–plan

1,000

2,000

3,000

0.5

1

1.5

(a) Errors over iterations (CG for Newton).

(b) Errors over run time in seconds.

Figure 1: Performance of Sinkhorn (S) and Newton (N) iterations measured by constraint viola-
tion (viol.), distance to optimal transport cost (cost) and distance to optimal transport
plan (plan).

comparison with the usual (linearly convergent) Sinkhorn iteration. Thus, we do not aim for
greatest overall speed but for a fair comparison between the Sinkhorn-Newton method and
the Sinkhorn iteration. To that end, we observe that one application of the Newton matrix (2.4)
amounts to one multiplication with P and P (cid:62) each, two coordinate-wise products and sums of
vectors. For one Sinkhorn iteration we need one multiplication with K and K (cid:62) and two additional
coordinate-wise operations. Although Algorithm 2 looks a little closer to the Sinkhorn iteration,
we still compare Algorithm 1, as we did not exploit any of the special structure in K or P.

All timings are reported using MATLAB (R2017b) implementations of the methods on an
Intel Xeon E3-1270v3 (four cores at 3.5 GHz) with 16 GB RAM. The code used to generate the
results below can be downloaded from https://github.com/dirloren/sinkhornnewton.

In all our experiments, we address the case m = n and the considered histograms are defined
i=1 ⊂ [0, 1]d with d = 2 (in Sections 3.1 and 3.2) and d = 1 (in Section 3.3),

on equidistant grids {xi }n
respectively. Throughout, the cost is chosen as quadratic, i.e., Ci j (cid:66) (cid:107)xi − xj (cid:107)2
2.

Our Sinkhorn–Newton method is implemented according to Algorithm 1 and using a pre-
conditioned CG method. The iteration is terminated as soon as the maximal violation of the
constraints (cid:107)ak − a(cid:107)∞ and (cid:107)bk − b (cid:107)∞ drops below some threshold. If applicable, the same
termination criterion is chosen for the Sinkhorn method, which is initialized with u0 = v0 = (cid:49)n.

3.1 comparison with sinkhorn–knopp

We first address the comparison of Sinkhorn–Newton with the classical Sinkhorn iteration.
For this purpose, we discretize the unit square [0, 1]2 using a 20 × 20 equidistant grid {xi =

8

(xi1, xi2)}400

i=1 ⊂ [0, 1]2 and take

which are then normalized to unit mass by setting

˜ai (cid:66) e−36([xi 1− 1
˜bj (cid:66) e−9([x j1− 2

3 ]2−[xi 2− 1

3 ]2−[x j 2− 2

3 ]2) + 10−1,
3 ]2) + 10−1,

a (cid:66) ˜a
(cid:205)
i ˜ai

and b (cid:66)

˜b
(cid:205)

j

.

˜bj

(3.1a)

(3.1b)

(3.2)

The entropic regularization parameter is set to ε (cid:66) 10−3 and in case of Sinkhorn-Newton, the
CG method is implemented with a tolerance of 10−13 and a maximum number of 34 iterations.
Moreover, the threshold for the termination criterion is chosen as 10−13.

Figure 1 shows the convergence history of the constraint violation for both iterations together
with the error in the unregularized transport cost |(cid:104)C, P k − P ∗(cid:105)|, where P ∗ denotes the final
transport plan, and the error in the transport plan (cid:107)P k − P ∗(cid:107)1. In Figure 1a, we compare the
error as a function of the iterations, where we take the total number of CG iterations for
Sinkhorn–Newton to allow for a fair comparison (since both a Sinkhorn and a CG step have
comparable costs, dominated by the two dense matrix–vector products Kv, K (cid:62)u and P (cid:62)δ f , Pδд,
respectively). It can be seen clearly that with respect to all error measures, Sinkhorn converges
linearly while Sinkhorn–Newton converges roughly quadratically, as expected, with Sinkhorn–
Newton significantly outperforming classical Sinkhorn for this choice of parameters. The same
behavior holds if the error is measured as a function of runtime; see Figure 1b.

3.2 dependence on the regularization strength

The second example addresses the dependence of the Sinkhorn–Newton method on the problem
parameters. In particular, we consider the dependence on ε and on the minimal value of a and b
(via the corresponding transport plans P), since these enter into the convergence rate estimate
(2.13). Here, we take an example which is also used in Cuturi [2]: computing the transport
distances between different images from the MNIST database, which contains 28 × 28 images
of handwritten digits. We consider these as discrete distributions of dimension 282 = 784 on
[0, 1]2, to which we add a small offset γ before normalizing to unit mass as before. Here, the
tolerance for both the Newton and the CG iteration is set to 10−12, and the maximum number of
CG iterations is fixed at 66. The entropic regularization parameter ε is chosen as multiples of
the median of the cost (which is q50 = 0.2821 in this case).

Figure 2 shows the convergence history for different offsets γ ∈ {0.5, 0.1, 0.01} and ε ∈
q50 · {1, 0.1, 0.01, 0.005}, where we again report the constraint violation both as a function of CG
iterations and of the run time in seconds. Comparing Figures 2a to 2f, we see that as ε decreases,
an increasing number of CG iterations is required to achieve the prescribed tolerance. However,
the convergence seems to be robust in ε at least for larger values of ε and only moderately
deteriorate for ε ≤ 0.01q50.

9

105

100

10−5

10−10

105

100

10−5

10−10

105

100

10−5

10−10

10−15

0

200

400

600

800

1,000

1,200

0.1

0.2

0.3

0.4

0.5

10−15

0

(a) γ = 0.5 (CG iterations)

(b) γ = 0.5 (run time in seconds)

10−15

0

200

400

600

800

1,000

1,200

0.1

0.2

0.3

0.4

0.5

10−15

0

(c) γ = 0.1 (CG iterations)

(d) γ = 0.1 (run time in seconds)

ε = q50
ε = 0.1 · q50
ε = 0.01 · q50
ε = 0.005 · q50

ε = q50
ε = 0.1 · q50
ε = 0.01 · q50
ε = 0.005 · q50

ε = q50
ε = 0.1 · q50
ε = 0.01 · q50
ε = 0.005 · q50

ε = q50
ε = 0.1 · q50
ε = 0.01 · q50
ε = 0.005 · q50

ε = q50
ε = 0.1 · q50
ε = 0.01 · q50
ε = 0.005 · q50

ε = q50
ε = 0.1 · q50
ε = 0.01 · q50
ε = 0.005 · q50

105

100

10−5

10−10

105

100

10−5

10−10

105

100

10−5

10−10

10

10−15

0

200

400

600

800

1,000

1,200

0.1

0.2

0.3

0.4

0.5

10−15

0

(e) γ = 0.01 (CG iterations)

(f) γ = 0.01 (run time in seconds)

Figure 2: Constraint violation for different offsets γ and different regularization parameters ε.

105

100

10−5

10−10

10−15

0

105

100

10−5

10−10

10−15

0

n = 1000
n = 2000
n = 4000
n = 8000

n = 1000
n = 2000
n = 4000
n = 8000

200

400

600

800

1,000

1,200

20

40

60

(a) Errors over CG iterations.

(b) Errors over run time in seconds.

Figure 3: Constraint violation for different mesh sizes n

3.3 dependence on the problem dimension

We finally address the dependence on the dimension of the problem. For this purpose, we
discretize the unit interval [0, 1] using n equidistant points xi ∈ [0, 1] and take

˜ai = e−100(xi −0.2)2 + e−20|xi −0.4 | + 10−2,
˜bj = e−100(x j −0.6)2 + 10−2,

(3.3a)

(3.3b)

which are again normalized to unit mass to obtain a and b. The regularization parameter is fixed
at ε = 10−3. Moreover, the inner and outer tolerances are here set to 10−10, and the maximum
number of CG iterations is coupled to the mesh size via (cid:100)n/12(cid:101).

Figure 3 shows the convergence behavior of Sinkhorn–Newton for n ∈ {1000, 2000, 4000, 8000}.

As can be seen from Figure 3a, the behavior is nearly independent of n; in particular, the number
of CG iterations required to reach the prescribed tolerance stays almost the same. (This is also
true for the Newton method itself with 21, 22, 23 and 23 iterations.) Since each CG iteration
involves two dense matrix–vector products with complexity O(n2), the total run time scales
quadratically; see Figure 3b.

4 proof of theorem 2.2

For the sake of presentation, we restrict ourselves to the case m = n here. However, in the end,
we suggest how the proof can be generalized to the case m (cid:44) n.

To estimate (2.12) and in particular JF (y k ) − JF (η) for y k = (f k , дk ) and η = (α, β) ∈ (cid:82)n × (cid:82)n

we observe that

JF (y k ) − JF (η) = 1
ε

(cid:34)

e

−ci j −f k
i
ε

−дk
j

−ci j −αi −βj
ε

− e

(cid:35)

(cid:34)

=

P k
i j (1 − e

f k
i

−αi +дk
j
ε

−βj

i j

(cid:35)

)

.

i j

11

To keep the notation concise, we abbreviate ψ = (φ, γ ) (cid:66) y k − η and also write y and P for y k
and P k , respectively. Then, we compute

JF (y)−1[JF (y) − JF (η)](y − η) = JF (y)−1

= JF (y)−1

(cid:16)(cid:205)
(cid:16)(cid:205)

(cid:205)
j

(cid:205)
i

(cid:19)

(cid:18)
















(cid:18)k
l

(cid:18)

j Pi j (e(φi +γj )/ε − 1)(φi + γj )/ε
i Pi j (e(φi +γj )/ε − 1)(φi + γj )/ε
(cid:19)

(cid:17)

(cid:17)








i

j

Pi j

Pi j

∞
(cid:205)
k=2
∞
(cid:205)
k =2

1
(k −1)!ε k

1
(k −1)!ε k

k
(cid:205)
l =0
k
(cid:205)
l =0

(cid:0)k
l

(cid:1)φl

iγ k−l

j

(cid:0)k
l

(cid:1)φl

iγ k−l

j

(cid:16)(cid:205)
(cid:16)(cid:205)

j Pi jφl
i Pi jφl

i

(cid:19)










iγ k −l
iγ k −l

j

j

j








(cid:17)

(cid:17)








i

j

=

∞
(cid:213)

k
(cid:213)

k=2

l =0

1
(k − 1)!εk

JF (y)−1

where all exponents are applied componentwise. Now we first treat only the summands for
l = 0 and l = k. For those terms (2.4) immediately implies

∞
(cid:213)

(cid:213)

k=2

l =0,k

(cid:19)

(cid:18)k
l

1
(k − 1)!εk

JF (y)−1

j Pi j (φk
i
i Pi j (φk
i

(cid:17)

+ γ k
j )
(cid:17)
+ γ k
j )

(cid:16)(cid:205)
(cid:16)(cid:205)








∞
(cid:213)

(cid:213)

=

k=2

l =0,k








i

j

1
(k − 1)!εk−1

(cid:21)

(cid:20)φk
γ k

=

(cid:21)

(cid:20)(eφ/ε − 1)φ
(eγ /ε − 1)γ

,

which has supremum norm bounded by (e
summands (i.e. 1 ≤ l ≤ k − 1), we write

1

ε − 1)(cid:107)(φ, γ )(cid:107)2

∞ for all (cid:107)(φ, γ )(cid:107)∞ ≤ 1. For all other

(cid:21)

(cid:20)α
β

(cid:66)

(cid:1)

(cid:0)k
l
(k − 1)!εk

JF (y)−1

(cid:16)(cid:205)
(cid:16)(cid:205)








j

j Pi jφl
i Pi jφl

iγ k−l
iγ k−l

j

(cid:17)

(cid:17)








i

j

.

Using (2.4) again, it follows that

(cid:20)Diag(P(cid:49)n)
P (cid:62)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

(cid:124)

P
Diag(P (cid:62)(cid:49)n)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:125)

(cid:123)(cid:122)
(cid:67)A

(cid:21)

(cid:21)

(cid:20)α
β

=

(cid:1)

(cid:0)k
l
(k − 1)!εk −1

(cid:20)Diag(Pγ k−l )φl
Diag(P (cid:62)φl )γ k−l

(cid:21)

,

and we aim to estimate (cid:107)α (cid:107)∞ and (cid:107)β (cid:107)∞ by (cid:107)φ (cid:107)∞ and (cid:107)γ (cid:107)∞. By Lemma 2.1, the matrix A has a
one-dimensional kernel spanned by q (cid:66) ((cid:49)(cid:62)
n )(cid:62), and a solution (α, β) in the orthogonal
n , −(cid:49)(cid:62)
complement is also a solution to

(cid:21)

(cid:20)α
β

=

(cid:1)

(cid:0)k
l
(k − 1)!εk−1

(cid:20)Diag(Pγ k −l )φl
Diag(P (cid:62)φl )γ k −l

(cid:21)

(A + ∆qq(cid:62))
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:123)(cid:122)
(cid:125)
(cid:124)
B

for any ∆ > 0. From Varah [12], we know that the (cid:96)∞-norm of the inverse matrix of B is
estimated by

(cid:107)B−1(cid:107)∞ ≤ (cid:2)mini

(cid:0)|Bii | − (cid:205)

j(cid:44)i |Bi j |(cid:1) (cid:3) −1 .

(4.1)

12

In this case, we calculate for i = 1, . . . , n that

|Bii | −

(cid:213)

|Bi j | = (cid:213)

1≤j ≤2n
j(cid:44)i

1≤j ≤n

Pi j + ∆ − (n − 1)∆ −

|Pi j − ∆|.

(cid:213)

1≤j ≤n

For any ∆ ≤ minj Pi j , this leads to

Similarly, we get that

|Bn+i,n+i | −

|Bn+i, j | ≤ 2∆.

Choosing ∆ (cid:66) mini j Pi j , we thus obtain that

|Bii | −

|Bi j | ≤ 2∆.

(cid:213)

1≤j ≤2n
j(cid:44)i

(cid:213)

1≤j ≤2n
j(cid:44)n+i

(cid:107)B−1(cid:107)∞ ≤

(cid:20)

2 min
i j

Pi j

(cid:21) −1

.

Using that

(cid:107) Diag(P (cid:62)φl )γ k−l (cid:107) ≤ (cid:107)P (cid:62)(cid:49)n (cid:107)∞(cid:107)φ (cid:107)l

∞(cid:107)γ (cid:107)k −l
∞ ,

and similarly for Diag(Pγ k−l )φl , finally gives

(cid:107)(α, β)(cid:107)∞ ≤

M (cid:107)φ (cid:107)l

∞(cid:107)γ (cid:107)k−l
∞

(cid:1)

(cid:0)k
l
(k − 1)!εk −1

M = max{(cid:107)P(cid:49)n (cid:107)∞, (cid:107)P (cid:62)(cid:49)n (cid:107)∞}

.

2 mini j Pi j

with

Hence we obtain

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∞
(cid:213)

k −1
(cid:213)

k=2

l =1

(cid:19)

(cid:18)k
l

1
(k − 1)!εk

JF (y)−1

(cid:16)(cid:205)
(cid:16)(cid:205)








j

j Pi jφl
i Pi jφl

iγ k −l
iγ k −l

j

k−1
(cid:213)

l =1

(cid:19)

(cid:18)k
l

1
(k − 1)!εk−1

(cid:107)φ (cid:107)l

∞(cid:107)γ (cid:107)k−l
∞

= M

e( (cid:107)φ (cid:107)∞+ (cid:107)γ (cid:107)∞)/ε − 1

((cid:107)φ (cid:107)∞ + (cid:107)γ (cid:107)∞)

(cid:16)

(cid:17)

−

e (cid:107)φ (cid:107)∞/ε − 1

(cid:107)φ (cid:107)∞ −

e (cid:107)γ (cid:107)∞/ε − 1

(cid:107)γ (cid:107)∞

(cid:17)

(cid:21)

(cid:17)

(cid:16)

≤ 2e

1

1
ε M(e

ε − 1)(cid:107)(φ, γ )(cid:107)2
∞

(cid:17)

(cid:17)








i

j

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
∞
(cid:213)

≤ M

k =2
(cid:20) (cid:16)

13

for all (cid:107)(φ, γ )(cid:107)∞ ≤ 1. In summary we obtain

(cid:107) JF (y)−1[JF (y) − JF (η)](y − η)(cid:107)∞ ≤ (1 + 2e

1
ε M)(e

1

ε − 1)(cid:107)y − η(cid:107)2
∞,

To generalize this to the case m (cid:44) n, one can take q = (∆1(cid:49)n, −∆2(cid:49)m) for ∆1 (cid:44) −∆2 and
∆1∆2 = ∆ and choose ∆1 to equilibrate the lower bounds for the first m and the last n rows of B.

We have proposed a Newton iteration to solve the entropically regularized discrete optimal
transport problem. Different from related Newton type approaches for matrix balancing, our
method iterates on the logarithm of the scalings, which seems to be necessary for robust
convergence in the optimal transport setting. Numerical examples show that our algorithm is
a robust and efficient alternative to the more commonly used Sinkhorn–Knopp algorithm, at
least for small regularization strength.

as desired

5 conclusion

references

[1] Carlier, Duval, Peyré & Schmitzer, Convergence of entropic schemes for optimal transport
and gradient flows, SIAM Journal on Mathematical Analysis 49 (2017), 1385–1418, doi: 10.
1137/15M1050264.

[2] Cuturi, Sinkhorn distances: Lightspeed computation of optimal transport, in: Advances in
Neural Information Processing Systems (NIPS) 26, 2013, 2292–2300, url: http://papers.nips.
cc/paper/4927-sinkhorn-distances-lightspeed-computation-of-optimal-transport.
[3] Cuturi & Doucet, Fast computation of Wasserstein barycenters, in: Proceedings of the 31st
International Conference on Machine Learning, 2014, 685–693, url: http://proceedings.mlr.
press/v32/cuturi14.pdf.

[4] Deuflhard, Newton Methods for Nonlinear Problems, Affine Invariance and Adaptive Algo-

rithms, Springer, 2011, doi: 10.1007/978-3-642-23899-4.

[5] Frogner, Zhang, Mobahi, Araya & Poggio, Learning with a Wasserstein loss, in: Advances
in Neural Information Processing Systems (NIPS) 28, 2015, 2053–2061, url: https://papers.
nips.cc/paper/5679-learning-with-a-wasserstein-loss.

[6] Kantorovivc, On the translocation of masses, C. R. (Doklady) Acad. Sci. URSS (N.S.) 37

(1942), 199–201.

[7] Kantorovivc & Rubinvsteuin, On a functional space and certain extremum problems,

Doklady Akademii Nauk SSSR 115 (1957), 1058–1061.

[8] Knight & Ruiz, A fast algorithm for matrix balancing, IMA J. Numer. Anal. 33 (2013), 1029–

1047, doi: 10.1093/imanum/drs019.

14

[9] Kolouri, Park, Thorpe, Slepcev & Rohde, Optimal mass transport: signal processing
and machine-learning applications, IEEE Signal Processing Magazine 34 (2017), 43–59, doi:
10.1109/MSP.2017.2695801.

[10] Monge, Mémoire sur la théorie des déblais et des remblais, Histoire de l’Académie Royale

des Sciences de Paris (1781).

[11] Sinkhorn & Knopp, Concerning nonnegative matrices and doubly stochastic matrices,

Pacific Journal of Mathematics 21 (1967), 343–348, doi: 10.2140/pjm.1967.21.343.

[12] Varah, A lower bound for the smallest singular value of a matrix, Linear Algebra and Appl.

11 (1975), 3–5, doi: 10.1016/0024-3795(75)90112-3.

15

