AMR Parsing as Graph Prediction with Latent Alignment

Chunchuan Lyu1

Ivan Titov1,2

1ILCC, School of Informatics, University of Edinburgh
2ILLC, University of Amsterdam

8
1
0
2
 
y
a
M
 
4
1
 
 
]
L
C
.
s
c
[
 
 
1
v
6
8
2
5
0
.
5
0
8
1
:
v
i
X
r
a

Abstract

Abstract meaning representations (AMRs)
are broad-coverage sentence-level seman-
tic representations. AMRs represent sen-
tences as rooted labeled directed acyclic
graphs. AMR parsing is challenging partly
due to the lack of annotated alignments be-
tween nodes in the graphs and words in
the corresponding sentences. We intro-
duce a neural parser which treats align-
ments as latent variables within a joint
probabilistic model of concepts, relations
and alignments. As exact inference re-
quires marginalizing over alignments and
is infeasible, we use the variational auto-
encoding framework and a continuous re-
laxation of the discrete alignments. We
show that joint modeling is preferable to
using a pipeline of align and parse. The
parser achieves the best reported results
on the standard benchmark (74.4% on
LDC2016E25).

1

Introduction

Abstract meaning representations (AMRs) (Ba-
narescu et al., 2013) are broad-coverage sentence-
level semantic representations. AMR encodes,
among others, information about semantic rela-
tions, named entities, co-reference, negation and
modality. The semantic representations can be re-
garded as rooted labeled directed acyclic graphs
(see Figure 1). As AMR abstracts away from de-
tails of surface realization, it is potentially beneﬁ-
cial in many semantic related NLP tasks, including
text summarization (Liu et al., 2015; Dohare and
Karnick, 2017), machine translation (Jones et al.,
2012) and question answering (Mitra and Baral,
2016).

Figure 1: An example of AMR, the dashed lines
denote latent alignments, obligate-01 is the root.
Numbers indicate depth-ﬁrst traversal order.

AMR parsing has recently received a lot of at-
tention (e.g., (Flanigan et al., 2014; Artzi et al.,
2015; Konstas et al., 2017)). One distinctive
aspect of AMR annotation is the lack of ex-
plicit alignments between nodes in the graph (con-
cepts) and words in the sentences. Though this
arguably simpliﬁed the annotation process (Ba-
narescu et al., 2013), it is not straightforward to
produce an effective parser without relying on an
alignment. Most AMR parsers (Damonte et al.,
2017; Flanigan et al., 2016; Werling et al., 2015;
Wang and Xue, 2017; Foland and Martin, 2017)
use a pipeline where the aligner training stage pre-
cedes training a parser. The aligners are not di-
rectly informed by the AMR parsing objective and
may produce alignments suboptimal for this task.

In this work, we demonstrate that the align-
ments can be treated as latent variables in a joint
probabilistic model and induced in such a way as
to be beneﬁcial for AMR parsing. Intuitively, in
our probabilistic model, every node in a graph
is assumed to be aligned to a word in a sen-
tence: each concept is predicted based on the cor-
responding RNN state. Similarly, graph edges
(i.e. relations) are predicted based on representa-
tions of concepts and aligned words (see Figure 2).
As alignments are latent, exact inference requires
marginalizing over latent alignments, which is in-

feasible.
Instead we use variational inference,
speciﬁcally the variational autoencoding frame-
work of Kingma and Welling (2014). Using dis-
crete latent variables in deep learning has proven
to be challenging (Mnih and Gregor, 2014; Born-
schein and Bengio, 2015). We use a continu-
ous relaxation of the alignment problem, rely-
ing on the recently introduced Gumbel-Sinkhorn
construction (Mena et al., 2018). This yields a
computationally-efﬁcient approximate method for
estimating our joint probabilistic model of con-
cepts, relations and alignments.

We assume injective alignments from concepts
to words: every node in the graph is aligned to
a single word in the sentence and every word is
aligned to at most one node in the graph. This is
necessary for two reasons. First, it lets us treat
concept identiﬁcation as sequence tagging at test
time. For every word we would simply predict the
corresponding concept or predict NULL to signify
that no concept should be generated at this posi-
tion. Secondly, Gumbel-Sinkhorn can only work
under this assumption. This constraint, though of-
ten appropriate, is problematic for certain AMR
constructions (e.g., named entities).
In order to
deal with these cases, we re-categorized AMR
concepts. Similar recategorization strategies have
been used in previous work (Foland and Martin,
2017; Peng et al., 2017).

The resulting parser achieves 74.4% Smatch
set when using
score on the standard test
LDC2016E25 training set,1 an improvement of
3.4% over the previous best result (van Noord and
Bos, 2017). We also demonstrate that inducing
alignments within the joint model is indeed ben-
eﬁcial. When, instead of inducing alignments, we
follow the standard approach and produce them
on preprocessing, the performance drops by 0.9%
Smatch. Our main contributions can be summa-
rized as follows:

• we introduce a joint probabilistic model for
alignment, concept and relation identiﬁca-
tion;

• we demonstrate that a continuous relaxation
can be used to effectively estimate the model;

• the model achieves the best reported results.2

1The standard deviation across multiple training runs was

0.16%.

2The code can be accessed from https://github.

com/ChunchuanLv/AMR_AS_GRAPH_PREDICTION

2 Probabilistic Model

In this section we describe our probabilistic model
and the estimation technique. In section 3, we de-
scribe preprocessing and post-processing (includ-
ing concept re-categorization, sense disambigua-
tion, wikiﬁcation and root selection).

2.1 Notation and setting

We will use the following notation throughout the
paper. We refer to words in the sentences as w =
(w1, . . . , wn), where n is sentence length, wk ∈ V
for k ∈ {1 . . . , n}. The concepts (i.e.
labeled
nodes) are c = (c1, . . . , cm), where m is the num-
ber of concepts and ci ∈ C for i ∈ {1 . . . , m}. For
example, in Figure 1, c = (obligate, go, boy, -).3
Note that senses are predicted at post-processing,
as discussed in Section 3.2 (i.e. go is labeled as
go-02).

A relation between ‘predicate concept’ i and
‘argument concept’ j is denoted by rij ∈ R; it
is set to NULL if j is not an argument of i. In our
example, r2,3 = ARG0 and r1,3 = NULL. We will
use R to denote all relations in the graph.

To represent alignments, we will use a =
{a1, . . . , am}, where ai ∈ {1, . . . , n} returns the
index of a word aligned to concept i. In our exam-
ple, a1 = 3.

All

three model components

rely on bi-
directional LSTM encoders (Schuster and Paliwal,
1997). We denote states of BiLSTM (i.e. con-
catenation of forward and backward LSTM states)
as hk ∈ Rd (k ∈ {1, . . . , n}). The sentence
encoder takes pre-trained ﬁxed word embeddings,
randomly initialized lemma embeddings, part-of-
speech and named-entity tag embeddings.

2.2 Method overview

We believe that using discrete alignments, rather
than attention-based models
(Bahdanau et al.,
2015) is crucial for AMR parsing. AMR banks
are a lot smaller than parallel corpora used in ma-
chine translation (MT) and hence it is important
to inject a useful inductive bias. We constrain our
alignments from concepts to words to be injective.
First, it encodes the observation that concepts are
mostly triggered by single words (especially, after
re-categorization, Section 3.1). Second, it implies

3The probabilistic model is invariant to the ordering of
concepts, though the order affects the inference algorithm
(see Section 2.5). We use depth-ﬁrst traversal of the graph
to generate the ordering.

AMR concepts are assumed to be generated condi-
tional independently relying on the BiLSTM states
and surface forms of the aligned words. Similarly,
relations are predicted based only on AMR con-
cept embeddings and LSTM states corresponding
to words aligned to the involved concepts. Their
combined representations are fed into a bi-afﬁne
classiﬁer (Dozat and Manning, 2017) (see Fig-
ure 2).

The expression involves intractable marginal-
ization over all valid alignments.
As stan-
dard in variational autoencoders, VAEs (Kingma
and Welling, 2014), we lower-bound the log-
likelihood as

log Pθ,φ(c, R|w)
≥ EQ[log Pθ(c|a, w)Pφ(R|a, w, c)]
− DKL(Qψ(a|c, R, w)||P (a)),

(1)

where Qψ(a|c, R, w) is the variational posterior
(aka the inference network), EQ[. . .] refers to the
expectation under Qψ(a|c, R, w) and DKL is the
Kullback-Liebler divergence. In VAEs, the lower
bound is maximized both with respect to model
parameters (θ and φ in our case) and the parame-
ters of the inference network (ψ). Unfortunately,
gradient-based optimization with discrete latent
variables is challenging. We use a continuous re-
laxation of our optimization problem, where real-
valued vectors ˆai ∈ Rn (for every concept i) ap-
proximate discrete alignment variables ai. This
relaxation results in low-variance estimates of the
gradient using the parameterization trick (Kingma
and Welling, 2014), and ensures fast and stable
training. We will describe the model components
and the relaxed inference procedure in detail in
sections 2.6 and 2.7.

Though the estimation procedure requires the
use of the relaxation, the learned parser is straight-
forward to use. Given our assumptions about the
alignments, we can independently choose for each
word wk (k = 1, . . . , m) the most probably con-
cept according to Pθ(c|hk).
If the highest scor-
ing option is NULL, no concept is introduced.
The relations could then be predicted relying on
Pφ(R|a, w, c). This would have led to generating
inconsistent AMR graphs, so instead we search for
the highest scoring valid graph (see Section 3.2).
Note that the alignment model Qψ is not used at
test time and only necessary to train accurate con-
cept and relation identiﬁcation models.

Figure 2: Relation identiﬁcation: predicting a re-
lation between boy and go-02 relying on the two
concepts and corresponding RNN states.

that each word corresponds to at most one con-
cept (if any). This encourages competition: align-
ments are mutually-repulsive. In our example, ob-
ligate is not lexically similar to the word must and
may be hard to align. However, given that other
concepts are easy to predict, alignment candidates
other than must and the will be immediately ruled
out. We believe that these are the key reasons for
why attention-based neural models do not achieve
competitive results on AMR (Konstas et al., 2017)
and why state-of-the-art models rely on aligners.
to
Our goal is to combine best of two worlds:
use alignments (as in state-of-the-art AMR meth-
ods) and to induce them while optimizing for the
end goal (similarly to the attention component of
encoder-decoder models).

Our model consists of three parts:

(1) the
concept identiﬁcation model Pθ(c|a, w); (2) the
relation identiﬁcation model Pφ(R|a, w, c) and
(3) the alignment model Qψ(a|c, R, w).4 For-
mally,
(1) and (2) together with the uniform
prior over alignments P (a) form the generative
In contrast, the align-
model of AMR graphs.
ment model Qψ(a|c, R, w), as will be explained
below, is approximating the intractable posterior
Pθ,φ(a|c, R, w) within that probabilistic model.

In other words, we assume the following model

for generating the AMR graph:

(cid:88)

a

Pθ,φ(c, R|w) =

P (a)Pθ(c|a, w)Pφ(R|a, w, c)

(cid:88)

=

P (a)

a

m
(cid:89)

i=1

m
(cid:89)

i,j=1

P (ci|hai)

P (rij|hai,ci,haj ,cj)

4θ, φ and ψ denote all parameters of the models.

2.3 Concept identiﬁcation model

The concept identiﬁcation model chooses a con-
cept c (i.e. a labeled node) conditioned on the
aligned word k or decides that no concept should
be introduced (i.e. returns NULL). Though it can
be modeled with a softmax classiﬁer, it would
not be effective in handling rare or unseen words.
First, we split the decision into estimating the
probability of concept category τ (c) ∈ T (e.g.
‘number’, ’frame’) and estimating the probabil-
ity of the speciﬁc concept within the chosen cat-
egory. Second, based on a lemmatizer and train-
ing data5 we prepare one candidate concept ek for
each word k in vocabulary (e.g., it would propose
want if the word is wants). Similar to Luong et al.
(2015), our model can then either copy the candi-
date ek or rely on the softmax over potential con-
cepts of category τ . Formally, the concept predic-
tion model is deﬁned as

Pθ(c|hk, wk) = P (τ (c)|hk, wk)×

[[ek = c]] × exp(vT

copyhk) + exp(vT

c hk)

,

Z(hk, θ)

where the ﬁrst multiplicative term is a soft-
max classiﬁer over categories (including NULL);
vcopy, vc ∈ Rd (for c ∈ C) are model parameters;
[[. . .]] denotes the indicator function and equals 1
if its argument is true and 0, otherwise; Z(h, θ) is
the partition function ensuring that the scores sum
to 1.

2.4 Relation identiﬁcation model

We use the following arc-factored relation identi-
ﬁcation model:

Pφ(R|a, w, c) =

P (rij|hai,ci,haj ,cj) (2)

m
(cid:89)

i,j=1

Each term is modeled in exactly the same way:

1. for both endpoints, embedding of the concept
c is concatenated with the RNN state h;

2. they are linearly projected to a lower dimen-
sion separately through Mh(hai ◦ ci) ∈ Rdf
and Md(haj ◦ cj) ∈ Rdf , where ◦ denotes
concatenation;

3. a log-linear model with bilinear scores
Mh(hai ◦ ci)T CrMd(haj ◦ cj), Cr ∈ Rdf ×df
is used to compute the probabilities.

5See supplementary materials.

In the above discussion, we assumed that BiL-
STM encodes a sentence once and the BiLSTM
states are then used to predict concepts and rela-
tions. In semantic role labeling, the task closely
related to the relation identiﬁcation stage of AMR
parsing, a slight modiﬁcation of this approach
was shown more effective (Zhou and Xu, 2015;
Marcheggiani et al., 2017). In that previous work,
the sentence was encoded by a BiLSTM once per
each predicate (i.e. verb) and the encoding was
in turn used to identify arguments of that predi-
cate. The only difference across the re-encoding
passes was a binary ﬂag used as input to the BiL-
STM encoder at each word position. The ﬂag
was set to 1 for the word corresponding to the
In that
predicate and to 0 for all other words.
way, BiLSTM was encoding the sentence specif-
ically for predicting arguments of a given predi-
cate. Inspired by this approach, when predicting
label rij for j ∈ {1, . . . m}, we input binary ﬂags
p1, . . . pn to the BiLSTM encoder which are set
to 1 for the word indexed by ai (pai = 1) and to
0 for other words (pj = 0, for j (cid:54)= ai). This also
means that BiLSTM encoders for predicting rela-
tions and concepts end up being distinct. We use
this multi-pass approach in our experiments.6

2.5 Alignment model

Recall that the alignment model is only used at
training, and hence it can rely both on input (states
h1, . . . , hn) and on the list of concepts c1, . . . , cm.
Formally, we add (m−n) NULL concepts to the
list.7 Aligning a word to any NULL, would corre-
spond to saying that the word is not aligned to any
‘real’ concept. Note that each one-to-one align-
ment (i.e. permutation) between n such concepts
and n words implies a valid injective alignment
of n words to m ‘real’ concepts. This reduction
to permutations will come handy when we turn to
the Gumbel-Sinkhorn relaxation in the next sec-
tion. Given this reduction, from now on, we will
assume that m = n.

As with sentences, we use a BiLSTM model
to encode concepts c, where gi ∈ Rdg , i ∈
{1, . . . , n}. We use a globally-normalized align-

6Using the vanilla one-pass model from equation (2) re-

sults in 1.4% drop in Smatch score.

7After re-categorization (Section 3.1), m ≥ n holds for
most cases. For exceptions, we append NULL to the sentence.

ment model:

Qψ(a|c, R, w) =

exp((cid:80)n

i=1 ϕ(gi, hai))
Zψ(c, w)

,

where Zψ(c, w) is the intractable partition func-
tion and the terms ϕ(gi, hai) score each alignment
link according to a bilinear form

ϕ(gi, hai) = gT

i Bhai,

(3)

where B ∈ Rdg×d is a parameter matrix.

2.6 Estimating model with Gumbel-Sinkhorn

Recall that our learning objective (1) involves ex-
pectation under the alignment model. The parti-
tion function of the alignment model Zψ(c, w) is
intractable, and it is tricky even to draw samples
from the distribution. Luckily, the recently pro-
posed relaxation (Mena et al., 2018) lets us cir-
cumvent this issue. First, note that exact samples
from a categorical distribution can be obtained us-
ing the perturb-and-max technique (Papandreou
and Yuille, 2011). For our alignment model, it
would correspond to adding independent noise to
the score for every possible alignment and choos-
ing the highest scoring one:

a(cid:63) = argmax

a∈P

n
(cid:88)

i=1

ϕ(gi, hai) + (cid:15)a,

(4)

where P is the set of all permutations of n
elements, (cid:15)a is a noise drawn independently
for each a from the ﬁxed Gumbel distribution
(G(0, 1)). Unfortunately, this is also intractable,
as there are n! permutations. Instead, in perturb-
and-max an approximate schema is used where
noise is assumed factorizable.
In other words,
ﬁrst noisy scores are computed as ˆϕ(gi, hai) =
ϕ(gi, hai) + (cid:15)i,ai, where (cid:15)i,ai ∼ G(0, 1) and
an approximate sample is obtained by a(cid:63) =
argmaxa

i=1 ˆϕ(gi, hai),

(cid:80)n

Such sampling procedure is still intractable in
our case and also non-differentiable. The main
contribution of Mena et al. (2018) is approximat-
ing this argmax with a simple differentiable com-
putation ˆa = St(Φ, Σ) which yields an approxi-
mate (i.e. relaxed) permutation. We use Φ and Σ
to denote the n × n matrices of alignment scores
ϕ(gi, hk) and noise variables (cid:15)ik, respectively. In-
stead of returning index ai for every concept i,
it would return a (peaky) distribution over words
ˆai. The peakiness is controlled by the temperature

parameter t of Gumbel-Sinkhorn which balances
smoothness (‘differentiability’) vs. bias of the es-
timator. For further details and the derivation, we
refer the reader to the original paper (Mena et al.,
2018).

Note that Φ is a function of the alignment model
Qψ, so we will write Φψ in what follows. The
variational bound (1) can now be approximated as

EΣ∼G(0,1)[log Pθ(c|St(Φψ, Σ), w)
+ log Pφ(R|St(Φψ, Σ), w, c)]

− DKL(

Φψ + Σ
t

||

Σ
t0

)

(5)

Following Mena et al. (2018), the original KL
term from equation (1) is approximated by the KL
term between two n × n matrices of i.i.d. Gumbel
distributions with different temperature and mean.
The parameter t0 is the ‘prior temperature’.

Using the Gumbel-Sinkhorn construction un-
fortunately does not guarantee that (cid:80)
i ˆaij = 1. To
encourage this equality to hold, and equivalently
to discourage overlapping alignments, we add an-
other regularizer to the objective (5):

Ω(ˆa, λ) = λ

max(

(ˆaij) − 1, 0).

(6)

(cid:88)

j

(cid:88)

i

Our ﬁnal objective is fully differentiable with
respect to all parameters (i.e. θ, φ and ψ) and has
low variance as sampling is performed from the
ﬁxed non-parameterized distribution, as in stan-
dard VAEs.

2.7 Relaxing concept and relation

identiﬁcation

One remaining question is how to use the soft
input ˆa = St(Φψ, Σ) in the concept and re-
In
lation identiﬁcation models in equation (5).
other words, we need to deﬁne how we compute
Pθ(c|St(Φψ, Σ), w) and Pφ(R|St(Φψ, Σ), w, c).
The standard technique would be to pass to the
models expectations under the relaxed variables
(cid:80)n
k=1 ˆaikhk, instead of the vectors hai (Maddison
et al., 2017; Jang et al., 2017). This is what we do
for the relation identiﬁcation model. We use this
approach also to relax the one-hot encoding of the
predicate position (p, see Section 2.4).

the

However,

prediction model
concept
log Pθ(c|St(Φψ, Σ), w) relies on the pointing
mechanism, i.e. directly exploits the words w
rather than relies only on biLSTM states hk. So

Such ‘primary’ concepts get encoded in the cat-
egory of the concept (the set of categories is τ , see
also section 2.3). In Figure 3, the re-categorized
concept thing(opinion) is produced from thing and
opine-01. We use concept as the dummy cate-
gory type. There are 8 templates in our system
which extract re-categorizations for ﬁxed phrases
thing(opinion)), and a deterministic system
(e.g.
for grouping lexically ﬂexible, but structurally sta-
ble sub-graphs (e.g., named entities, have-rel-role-
91 and have-org-role-91 concepts).

Figure 3: An example of re-categorized AMR.
AMR graph at the top, re-categorized concepts in
the middle, and the sentence is at the bottom.

instead we treat ˆai as a prior in a hierarchical
model:

Details of the re-categorization procedure and

other pre-processing are provided in appendix.

logPθ(ci|ˆai, w)

n
(cid:88)

k=1

≈ log

ˆaikPθ(ci|ai = k, w)

(7)

3.2 Post-processing

As we will show in our experiments, a softer ver-
sion of the loss is even more effective:

logPθ(ci|ˆai, w)

n
(cid:88)

k=1

≈ log

(ˆaikPθ(ci|ai = k, w))α,

(8)

where we set the parameter α = 0.5. We believe
that using this loss encourages the model to more
actively explore the alignment space. Geometri-
cally, the loss surface shaped as a ball in the 0.5-
norm space would push the model away from the
corners, thus encouraging exploration.

3 Pre- and post-pocessing

3.1 Re-Categorization

AMR parsers often rely on a pre-processing stage,
where speciﬁc subgraphs of AMR are grouped to-
gether and assigned to a single node with a new
compound category (e.g., Werling et al. (2015);
Foland and Martin (2017); Peng et al. (2017)); this
transformation is reversed at the post-processing
stage. Our approach is very similar to the Factored
Concept Label system of Wang and Xue (2017),
with one important difference that we unpack our
concepts before the relation identiﬁcation stage, so
the relations are predicted between original con-
cepts (all nodes in each group share the same
alignment distributions to the RNN states). Intu-
itively, the goal is to ensure that concepts rarely
lexically triggered (e.g., thing in Figure 3) get
grouped together with lexically triggered nodes.

we

handle

post-processing,

For
sense-
disambiguation, wikiﬁcation and ensure le-
gitimacy of the produced AMR graph. For sense
disambiguation we pick the most frequent sense
for that particular concept (‘-01’, if unseen). For
wikiﬁcation we again look-up in the training set
and default to ”-”. There is certainly room for
improvement
in both stages. Our probability
model predicts edges conditional independently
and thus cannot guarantee the connectivity of
AMR graph, also there are additional constraints
which are useful to impose. We enforce three
constraints: (1) speciﬁc concepts can have only
one neighbor (e.g., ‘number’ and ‘string’; see
appendix for details); (2) each predicate concept
can have at most one argument for each relation
r ∈ R;
(3) the graph should be connected.
Constraint (1) is addressed by keeping only the
highest scoring neighbor. In order to satisfy the
last two constraints we use a simple greedy proce-
dure. First, for each edge, we pick-up the highest
scoring relation and edge (possibly NULL). If
the constraint (2) is violated, we simply keep the
highest scoring edge among the duplicates and
drop the rest. If the graph is not connected (i.e.
constraint (3) is violated), we greedily choose
edges linking the connected components until the
graph gets connected (MSCG in Flanigan et al.
(2014)).

Finally, we need to select a root node. Simi-
larly to relation identiﬁcation, for each candidate
concept ci, we concatenate its embedding with
the corresponding LSTM state (hai) and use these
scores in a softmax classiﬁer over all the concepts.

Data Smatch
Model
R1
JAMR (Flanigan et al., 2016)
R1
AMREager (Damonte et al., 2017)
CAMR (Wang et al., 2016)
R1
SEQ2SEQ + 20M (Konstas et al., 2017) R1
R1
Mul-BiLSTM (Foland and Martin, 2017)
Ours
R1
Neural-Pointer (Buys and Blunsom, 2017) R2
R2
ChSeq (van Noord and Bos, 2017)
ChSeq + 100K (van Noord and Bos, 2017) R2
R2
Ours

67.0
64.0
66.5
62.1
70.7
73.7
61.9
64.0
71.0
74.4 ± 0.16

Table 1: Smatch scores on the test set. R2 is
LDC2016E25 dataset, and R1 is LDC2015E86
dataset. Statistics on R2 are over 8 runs.

4 Experiments and Discussion

4.1 Data and setting

on

focus

primarily

the most

recent
We
LDC2016E25 (R2) dataset, which consists
of 36521, 1368 and 1371 sentences in training,
development and testing sets, respectively. The
earlier LDC2015E86 (R1) dataset has been
used by much of the previous work. It contains
16833 training sentences, and same sentences for
development and testing as R2.8

We used the development set to perform model
selection and hyperparameter tuning. The hyper-
parameters, as well as information about embed-
dings and pre-processing, are presented in the sup-
plementary materials.

We used Adam (Kingma and Ba, 2014) to opti-
mize the loss (5) and to train the root classiﬁer.
Our best model is trained fully jointly, and we
do early stopping on the development set scores.
Training takes approximately 6 hours on a single
GeForce GTX 1080 Ti with Intel Xeon CPU E5-
2620 v4.

4.2 Experiments and discussion

We start by comparing our parser to previous work
(see Table 1). Our model substantially outper-
forms all the previous models on both datasets.
Speciﬁcally, it achieves 74.4% Smatch score on
LDC2016E25 (R2), which is an improvement of
3.4% over character seq2seq model relying on
silver data (van Noord and Bos, 2017).
For
LDC2015E86 (R1), we obtain 73.7% Smatch
score, which is an improvement of 3.0% over

8Annotation in R2 has also been slightly revised.

Models

Dataset
Smatch
Unlabeled
No WSD
Reentrancy
Concepts
NER
Wiki
Negations
SRL

J’
16

A’ C’
Ch’ Ours
17
16
17
R1 R1 R1 R2
71
64
74
69
65
72
52
41
82
83
79
83
64
65
62
48
66
56

R2
74.4±0.16
77.1±0.10
75.5±0.12
52.3±0.43
85.9±0.11
86.0±0.46
75.7±0.30
58.4±1.32
69.8±0.24

63
69
64
41
80
75
0
18
60

67
69
68
42
83
79
75
45
60

Table 2:
F1 scores on individual phenom-
ena. A’17 is AMREager, C’16 is CAMR, J’16 is
JAMR, Ch’17 is ChSeq+100K. Ours are marked
with standard deviation.

Metric

Smatch
Unlabeled
No WSD
Reentrancy
Concepts
NER
Wiki
Negations
SRL

Pre-
Align
72.8
75.3
73.8
50.2
85.4
85.3
66.8
56.0
68.8

R1

73.7
76.3
74.7
50.6
85.5
84.8
75.6
57.2
68.9

R2

Pre-
Align mean
74.4
73.5
77.1
76.1
75.5
74.6
52.6
52.3
85.9
85.5
86.0
85.3
75.7
67.8
58.4
56.6
70.2
69.8

Table 3:
F1 scores of on subtasks. Scores on
ablations are averaged over 2 runs. The left side
results are from LDC2015E86 and right results are
from LDC2016E25.

the previous best model, multi-BiLSTM parser
of Foland and Martin (2017).

In order to disentangle individual phenomena,
we use the AMR-evaluation tools (Damonte et al.,
2017) and compare to systems which reported
these scores (Table 2). We obtain the highest
scores on most subtasks. The exception is nega-
tion detection. However, this is not too surpris-
ing as many negations are encoded with morphol-
ogy, and character models, unlike our word-level
model, are able to capture predictive morphologi-
cal features (e.g., detect preﬁxes such as “un-” or
“im-”).

Now, we turn to ablation tests (see Table 3).
First, we would like to see if our latent align-
ment framework is beneﬁcial. In order to test this,
we create a baseline version of our system (‘pre-
align’) which relies on the JAMR aligner (Flani-

Ablation
No Sinkhorn
No Sinkhorn reg
No soft loss
Full model

Concepts SRL Smatch
69.3
85.7
69.5
85.6
69.1
85.2
69.8
85.9

73.8
74.2
73.7
74.4

Table 5: Ablation studies: alignment modeling
and relaxation (all on R2). Scores on ablations are
averaged over 2 runs.

as to beneﬁt the relation identiﬁcation task. For
this ablation we break the full joint training into
two stages. We start by jointly training the align-
ment model and the concept identiﬁcation model.
When these are trained, we optimizing the relation
model but keep the concept identiﬁcation model
and alignment models ﬁxed (‘2 stages’ in see Ta-
ble 4). When compared to our joint model (‘full
model’), we observe a substantial drop in Smatch
score (-0.8%). In another version (‘2 stages, tune
align’) we also use two stages but we ﬁne-tune
the alignment model on the second stage. This
approach appears slightly more accurate but still
In both cases, the
-0.5% below the full model.
drop is more substantial for relations (‘SRL’). In
order to see why relations are potentially useful
in learning alignments, consider Figure 4. The
example contains duplicate concepts long. The
concept prediction model factorizes over concepts
and does not care which way these duplicates are
aligned: correctly (green edges) or not (red edges).
Formally, the true posterior under the concept-
only model in ‘2 stages’ assigns exactly the same
probability to both conﬁgurations, and the align-
ment model Qψ will be forced to mimic it (even
though it relies on an LSTM model of the graph).
The spurious ambiguity will have a detrimental ef-
fect on the relation identiﬁcation stage.

First,

It is interesting to see the contribution of other
modeling decisions we made when modeling and
relaxing alignments.
instead of using
Gumbel-Sinkhorn, which encourages mutually-
repulsive alignments, we now use a factorized
alignment model. Note that this model (‘No
Sinkhorn’ in Table 5) still relies on (relaxed) dis-
crete alignments (using Gumbel softmax) but does
not constrain the alignments to be injective. A
substantial drop in performance indicates that the
prior knowledge about the nature of alignments
appears beneﬁcial. Second, we remove the addi-
tional regularizer for Gumbel-Sinkhorn approxi-
mation (equation (6)). The performance drop in

Figure 4: When modeling concepts alone, the pos-
terior probability of the correct (green) and wrong
(red) alignment links will be the same.

Ablation
2 stages
2 stages, tune align
Full model

Concepts SRL Smatch
68.9
85.6
69.2
85.6
69.8
85.9

73.6
73.9
74.4

Table 4: Ablation studies: effect of joint model-
ing (all on R2). Scores on ablations are averaged
over 2 runs. The ﬁrst two models load the same
concept and alignment model before the second
stage.

gan et al., 2014), rather than induces alignments as
latent variables. Recall that in our model we used
training data and a lemmatizer to produce candi-
dates for the concept prediction model (see Sec-
tion 2.3, the copy function).
In order to have a
fair comparison, if a concept is not aligned after
JAMR, we try to use our copy function to align it.
If an alignment is not found, we make the align-
ment uniform across the unaligned words. In pre-
liminary experiments, we considered alternatives
versions (e.g., dropping concepts unaligned by
JAMR or dropping concepts unaligned after both
JAMR and the matching heuristic), but the chosen
strategy was the most effective. These scores of
pre-align are superior to the results from Foland
and Martin (2017) which also relies on JAMR
alignments and uses BiLSTM encoders. There
are many potential reasons for this difference in
performance. For example, their relation identi-
ﬁcation model is different (e.g., single pass, no
bi-afﬁne modeling), they used much smaller net-
works than us, they use plain JAMR rather than a
combination of JAMR and our copy function, they
use a different recategorization system. These re-
sults conﬁrm that we started with a strong basic
model, and that our variational alignment frame-
work provided further gains in performance.

Now we would like to conﬁrm that joint train-
ing of alignments with both concepts and relations
is beneﬁcial. In other words, we would like to see
if alignments need to be induced in such a way

Smatch score (‘No Sinkhorn reg’) is only moder-
ate. Finally, we show that using the simple hier-
archical relaxation (equation (7)) rather than our
softer version of the loss (equation (8)) results in
a substantial drop in performance (‘No soft loss’,
-0.7% Smatch). We hypothesize that the softer
relaxation favors exploration of alignments and
helps to discover better conﬁgurations.

5 Additional Related Work

Alignment performance has been previously iden-
tiﬁed as a potential bottleneck affecting AMR
parsing (Damonte et al., 2017; Foland and Mar-
tin, 2017). Some recent work has focused on
building aligners speciﬁcally for training their
parsers (Werling et al., 2015; Wang and Xue,
2017). However, those aligners are trained in-
dependently of concept and relation identiﬁcation
and only used at pre-processing.

Treating alignment as discrete variables has
been successful in some sequence transduction
tasks with neural models (Yu et al., 2017, 2016).
Our work is similar in that we also train dis-
crete alignments jointly but the tasks, the inference
framework and the decoders are very different.

The discrete alignment modeling framework
has been developed in the context of traditional
non-neural) statistical machine transla-
(i.e.
tion (Brown et al., 1993). Such translation mod-
els have also been successfully applied to semantic
parsing tasks (e.g., (Andreas et al., 2013)), where
they rivaled specialized semantic parsers from that
period. However, they are considerably less accu-
rate than current state-of-the-art parsers applied to
the same datasets (e.g., (Dong and Lapata, 2016)).

For AMR parsing, another way to avoid us-
ing pre-trained aligners is to use seq2seq models
(Konstas et al., 2017; van Noord and Bos, 2017).
In particular, van Noord and Bos (2017) used char-
acter level seq2seq model and achieved the previ-
ous state-of-the-art result. However, their model is
very data demanding as they needed to train it on
additional 100K sentences parsed by other parsers.
This may be due to two reasons. First, seq2seq
models are often not as strong on smaller datasets.
Second, recurrent decoders may struggle with pre-
dicting the linearized AMRs, as many statistical
dependencies are highly non-local.

6 Conclusions

We introduced a neural AMR parser trained by
jointly modeling alignments, concepts and rela-
tions. We make such joint modeling computa-
tionally feasible by using the variational auto-
encoding framework and continuous relaxations.
The parser achieves state-of-the-art results and ab-
lation tests show that joint modeling is indeed ben-
eﬁcial.

We believe that the proposed approach may be
extended to other parsing tasks where alignments
are latent (e.g., parsing to logical form (Liang,
2016)). Another promising direction is integrating
character seq2seq to substitute the copy function.
This should also improve the handling of nega-
tion and rare words. Though our parsing model
does not use any linearization of the graph, we re-
lied on LSTMs and somewhat arbitrary lineariza-
tion (depth-ﬁrst traversal) to encode the AMR
graph in our alignment model. A better alter-
native would be to use graph convolutional net-
works (Marcheggiani and Titov, 2017; Kipf and
Welling, 2017): neighborhoods in the graph are
likely to be more informative for predicting align-
ments than the neighborhoods in the graph traver-
sal.

Acknowledgments

We thank Marco Damonte, Shay Cohen, Diego
Marcheggiani and Wilker Aziz for helpful discus-
sions as well as anonymous reviewers for their
suggestions. The project was supported by the
European Research Council (ERC StG BroadSem
678254) and the Dutch National Science Founda-
tion (NWO VIDI 639.022.518).

References

Jacob Andreas, Andreas Vlachos, and Stephen Clark.
2013. Semantic parsing as machine translation. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), volume 2, pages 47–52.

Yoav Artzi, Kenton Lee, and Luke Zettlemoyer. 2015.
Broad-coverage CCG semantic parsing with AMR.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1699–1710.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
International Con-
learning to align and translate.
ference on Learning Representations.

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Grifﬁtt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract Meaning Representation
for Sembanking.

J¨org Bornschein

Reweighted wake-sleep.
ence on Learning Representations.

and Yoshua Bengio.

2015.
International Confer-

Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Comput. Linguist., 19(2):263–
311.

Jan Buys and Phil Blunsom. 2017. Oxford at semeval-
2017 task 9: Neural amr parsing with pointer-
In Proceedings of the 11th
augmented attention.
International Workshop on Semantic Evaluation
(SemEval-2017), pages 914–919. Association for
Computational Linguistics.

Marco Damonte, Shay B Cohen, and Giorgio Satta.
2017. An Incremental Parser for Abstract Meaning
Representation. In Proceedings of the 15th Confer-
ence of the European Chapter of the Association for
Computational Linguistics: Volume 1, Long Papers,
volume 1, pages 536–546.

Shibhansh Dohare and Harish Karnick. 2017. Text
Summarization using Abstract Meaning Represen-
tation. arXiv preprint arXiv:1706.01678.

Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 33–43.

Timothy Dozat and Christopher D. Manning. 2017.
Deep Biafﬁne Attention for Neural Dependency
Parsing. International Conference on Learning Rep-
resentations.

Jeffrey Flanigan, Chris Dyer, Noah A. Smith, and
Jaime Carbonell. 2016. CMU at SemEval-2016
Task 8: Graph-based AMR Parsing with Inﬁnite
In Proceedings of the 10th Interna-
Ramp Loss.
tional Workshop on Semantic Evaluation (SemEval-
2016), pages 1202–1206. Association for Computa-
tional Linguistics.

Jeffrey Flanigan, Sam Thomson, Jaime Carbonell,
Chris Dyer, and Noah A. Smith. 2014. A Discrim-
inative Graph-Based Parser for the Abstract Mean-
ing Representation. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1426–
1436, Baltimore, Maryland. Association for Com-
putational Linguistics.

William Foland and James H. Martin. 2017. Abstract
Meaning Representation Parsing using LSTM Re-
In Proceedings of the
current Neural Networks.

55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
463–472, Vancouver, Canada. Association for Com-
putational Linguistics.

Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categor-
ical reparameterization with gumbel-softmax. Inter-
national Conference on Learning Representations.

Bevan K. Jones,

Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
Semantics-Based Machine Translation with Hyper-
edge Replacement Grammars. In COLING.

Diederik P. Kingma and Jimmy Ba. 2014. Adam: A
Method for Stochastic Optimization. International
Conference on Learning Representations.

Diederik P Kingma and Max Welling. 2014. Auto-
International Confer-

encoding variational bayes.
ence on Learning Representations.

Thomas N Kipf and Max Welling. 2017.

Semi-
supervised classiﬁcation with graph convolutional
International Conference on Learning
networks.
Representations.

Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin
Choi, and Luke Zettlemoyer. 2017. Neural AMR:
Sequence-to-Sequence Models for Parsing and Gen-
In Proceedings of the 55th Annual Meet-
eration.
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 146–157, Van-
couver, Canada. Association for Computational Lin-
guistics.

Percy Liang. 2016.

Learning executable semantic
parsers for natural language understanding. Com-
munications of the ACM, 59(9):68–76.

Fei Liu, Jeffrey Flanigan, Sam Thomson, Norman M.
Sadeh, and Noah A. Smith. 2015. Toward Ab-
stractive Summarization Using Semantic Represen-
tations. In HLT-NAACL.

Edward Loper and Steven Bird. 2002. NLTK: The Nat-
ural Language Toolkit. In Proceedings of the ACL-
02 Workshop on Effective Tools and Methodolo-
gies for Teaching Natural Language Processing and
Computational Linguistics - Volume 1, ETMTNLP
’02, pages 63–70, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.

Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals,
and Wojciech Zaremba. 2015. Addressing the rare
word problem in neural machine translation. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pages 11–19,
Beijing, China. Association for Computational Lin-
guistics.

Chris J Maddison, Andriy Mnih, and Yee Whye Teh.
2017. The concrete distribution: A continuous re-
laxation of discrete random variables. International
Conference on Learning Representations.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global Vectors for
Word Representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Nima Pourdamghani, Yang Gao, Ulf Hermjakob, and
Kevin Knight. 2014. Aligning english strings with
abstract meaning representation graphs. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
425–429.

M. Schuster and K.K. Paliwal. 1997. Bidirectional
Trans. Sig. Proc.,

Recurrent Neural Networks.
45(11):2673–2681.

Chuan Wang, Sameer Pradhan, Xiaoman Pan, Heng
Ji, and Nianwen Xue. 2016. CAMR at SemEval-
2016 Task 8: An Extended Transition-based AMR
In Proceedings of the 10th International
Parser.
Workshop on Semantic Evaluation (SemEval-2016),
pages 1173–1178, San Diego, California. Associa-
tion for Computational Linguistics.

Chuan Wang and Nianwen Xue. 2017. Getting the
In Proceedings of the
Most out of AMR Parsing.
2017 Conference on Empirical Methods in Natural
Language Processing, pages 1257–1268.

Keenon Werling, Gabor Angeli, and Christopher D.
Manning. 2015. Robust Subgraph Generation Im-
proves Abstract Meaning Representation Parsing. In
ACL.

Lei Yu, Phil Blunsom, Chris Dyer, Edward Grefen-
stette, and Tomas Kocisky. 2017. The Neural Noisy
Channel. In International Conference on Learning
Representations.

Lei Yu, Jan Buys, and Phil Blunsom. 2016. Online
Segment to Segment Neural Transduction. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1307–
1316. Association for Computational Linguistics.

Jie Zhou and Wei Xu. 2015. End-to-end learning of
semantic role labeling using recurrent neural net-
In Proceedings of the 53rd Annual Meet-
works.
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
volume 1, pages 1127–1137.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP Natural Lan-
guage Processing Toolkit. In Association for Com-
putational Linguistics (ACL) System Demonstra-
tions, pages 55–60.

Diego Marcheggiani, Anton Frolov, and Ivan Titov.
2017. A Simple and Accurate Syntax-Agnostic
Neural Model for Dependency-based Semantic Role
Labeling. In Proceedings of the 21st Conference on
Computational Natural Language Learning (CoNLL
2017), pages 411–420, Vancouver, Canada. Associ-
ation for Computational Linguistics.

Diego Marcheggiani and Ivan Titov. 2017. Encoding
Sentences with Graph Convolutional Networks for
Semantic Role Labeling. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1507–1516, Copenhagen,
Denmark. Association for Computational Linguis-
tics.

Gonzalo Mena, David Belanger, Scott Linderman, and
Jasper Snoek. 2018. Learning Latent Permutations
International
with Gumbel-Sinkhorn Networks.
Conference on Learning Representations. Accepted
as poster.

Arindam Mitra and Chitta Baral. 2016. Addressing a
question answering challenge by combining statisti-
cal methods with inductive rule learning and reason-
ing. In 30th AAAI Conference on Artiﬁcial Intelli-
gence, AAAI 2016. AAAI press.

Andriy Mnih and Karol Gregor. 2014. Neural varia-
tional inference and learning in belief networks. In
Proceedings of the International Conference on Ma-
chine Learning.

Rik van Noord and Johan Bos. 2017. Neural Se-
mantic Parsing by Character-based Translation: Ex-
periments with Abstract Meaning Representations.
Computational Linguistics in the Netherlands Jour-
nal, 7:93–108.

George Papandreou and Alan L Yuille. 2011. Perturb-
and-map random ﬁelds: Using discrete optimization
to learn and sample from energy models. In Com-
puter Vision (ICCV), 2011 IEEE International Con-
ference on, pages 193–200. IEEE.

Adam Paszke, Sam Gross, Soumith Chintala, Gre-
gory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in PyTorch.

Xiaochang Peng, Chuan Wang, Daniel Gildea, and Ni-
anwen Xue. 2017. Addressing the Data Sparsity Is-
sue in Neural AMR Parsing. In Proceedings of the
15th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Volume 1,
Long Papers, pages 366–375. Association for Com-
putational Linguistics.

Supplementary Material

7 Matching algorithm for copying concepts

Only frequent concepts c (frequency at least 10 for R2 and 5 for R1) can be generated without the
copying mechanism (i.e. have their own vector vc associated with them). Both frequent and infrequent
ones are processed with coping, using candidates produced by the algorithm below and the matching rule
in Table 6.

l=1

: {wl, cl}L
Input
Output: D copy dictionary
Counter ← ∅
for l = 1 to L do
for all pairs cl
if match(cl

i and wl
i, wl

j do
j) then

Increment Counter[wl

j][cl
i]

end

end

end
D ← default Stanford lemmatizer
for w ← Counter do

D[w] ← argmaxc Counter[w][c]

end
return D

Algorithm 1: Copy function construction

Rules
Verbalization Match
PropBank Match
Sufﬁx Removal Match word with sufﬁx (“-ed”, “-ly”,“-ing”) removed is identical to concept lemma
Edit-distance Match

Matching Criteria
exact match frame in ”verbalization-list-v1.06.txt”
exact match frame in PropBank frame ﬁles

edit distance smaller than 50% of the length

Table 6: Matching rules for Algorithm 1

8 Re-categorization details

Re-categorization is handled with rules listed in Table 2. They are triggered if a given primary concept
(‘primary’) appears adjacent to edges labeled with relations given in column ‘rel’. The assigned category
is shown in column ‘re-categorized’. The rules yield 32 categories when applied to the training set.

There are also rules of another type shown in Table 3 below. The templates and examples are in column
‘original’, the resulting concepts are in column ‘re-categorized’. These rules yield 109 additional types
when applied to the training set.

primary
person
thing
most
-quantity
date-entity
monetary-quantity
temporal-quantity

rel
ARG0-of/ARG1-of
ARG0-of/ARG1-of/ARG2-of
degree-of
unit
weekday/dayperiod/season
unit/ARG2-of/ARG1-of/quant monetary-quantity([second])
temporal-quantity([second])
unit/ARG3-of

re-categorized
person([second])
thing([second])
most([second])
primary([second])
date-entity([second])

Table 7: Templates for re-categorization.

re-categorized

(B-Ner type(n1),...,Ner type(nx))

B-Ner city(New),Ner city(York)

original
( c /

t y p e

: name ( n / name

: op1 ‘ n1 ’
. . .
: opx ‘ nx ’ )

( c /

c i t y

: name ( n / name

: op1 ‘New’
: op2 ‘ York ’ )

( p /

t y p e

( p / p e r s o n

( o1 / x−e n t i t y

: x c o n s t a n t )
( o1 / o r d i n a l −e n t i t y

: v a l u e 1 )

: ARG0−o f

( h / have−x−r o l e −91

have-x-role type(role)

: ARG2 ( p /

r o l e )

: ARG0−o f

( h / have−org−r o l e −91

have-org-role person(premier)

: ARG2 ( p / p r e m i e r )

x-entity(constant)

ordinal-entity(1)

Table 8: Extra rules for re-categorization.

9 Additional pre-processing

Besides constructing re-categorized AMR concepts, we perform additional preprocessing. We start with
tokenized dataset of Pourdamghani et al. (2014). We take all dashed AMR concepts (e.g, make-up and
more-than) and concatenate the corresponding spans (based on statistics from training set and PropBank
frame ﬁles). We also combine spans of words corresponding to a single number. For relation identiﬁca-
tion, we normalize relations to one canonical direction (e.g. arg0, time-of). For named entity recognition,
and lemmatization, we use Stanford CoreNLP toolkit (Manning et al., 2014). For pre-trained embedding,
we used Glove (300 dimensional embeddings) (Pennington et al., 2014).

10 Model parameters and optimization details

We selected hyper-parameters based on the best performance on the development set. For all the ab-
lation tests, the hyper parameters are ﬁxed. We used 2 different BiLSTM encoders of the same hyper-
parameters to encode sentence for concept identiﬁcation and alignment prediction, another BiLSTM
to encode AMR concept sequence for alignment, and ﬁnally 2 different BiLSTM of the same hyper-
parameters to encode sentence for relation identiﬁcation and root identiﬁcation. There are 5 BiLSTM
encoders in total. Hyper parameters for the model are summarized in Table 9, and optimization parame-
ters are summarized in Table 10.

Model components
Glove Embeddings
Lemma Embeddings
POS Embeddings
NER Embeddings
Category Embeddings
Concept/Alignment
Sentence BiLSTM
AMR Categories T
AMR Lemmas C
AMR NER types
Alignment
AMR BiLSTM
B bilinear align
Relation map dimensionality dg
Relation/Root
Sentence BiLSTM
df relation vector
vc, vcopy lemma vector
vroot root vector
Sinkhorn temperature
Sinkhorn prior temperature
Sinkhorn steps l for full joint training
Sinkhorn steps l for two stages training
λ
Dropout

Hyper-parameters
300
200
32
16
32
1 layer 548 input
256 hidden (each direction)
32
506
109
1 layer 232 input
100 hidden (each direction)
200 × 512
200
2 layers 549 input (predicate position)
256 hidden (each direction)
200
512
200
1
5
10
5
10
.2

Table 9: Model hyper-parameters

Optimizer Parameters
Batch size for single stage
Maximum Epochs
Batch size for ﬁrst stage
Batch size for second stage
Maximum Epochs for both stages
Learning Rate
Adam betas
Adam eps
Weight decay

Values
64
30
512
64
30
1e-4
(0.9, 0.999)
1e-8
1e-5

Table 10: Optimization parameters for full joint training and two stages training.

