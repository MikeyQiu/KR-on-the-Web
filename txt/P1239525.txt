7
1
0
2
 
b
e
F
 
1
 
 
]

V
C
.
s
c
[
 
 
2
v
0
8
4
7
0
.
2
0
6
1
:
v
i
X
r
a

Improving patch-based scene text script identiﬁcation
with ensembles of conjoined networks

Lluis Gomez, Anguelos Nicolaou, Dimosthenis Karatzas

Computer Vision Center, Universitat Autonoma de Barcelona. Ediﬁci O, Campus UAB,
08193 Bellaterra (Cerdanyola) Barcelona, Spain. E-mail: lgomez,dimos@cvc.uab.cat

Abstract

This paper focuses on the problem of script identiﬁcation in scene text im-

ages. Facing this problem with state of the art CNN classiﬁers is not straight-

forward, as they fail to address a key characteristic of scene text instances:

their extremely variable aspect ratio. Instead of resizing input images to a

ﬁxed aspect ratio as in the typical use of holistic CNN classiﬁers, we propose

here a patch-based classiﬁcation framework in order to preserve discrimina-

tive parts of the image that are characteristic of its class.

We describe a novel method based on the use of ensembles of conjoined

networks to jointly learn discriminative stroke-parts representations and their

relative importance in a patch-based classiﬁcation scheme. Our experiments

with this learning procedure demonstrate state-of-the-art results in two pub-

lic script identiﬁcation datasets.

In addition, we propose a new public benchmark dataset for the evalu-

ation of multi-lingual scene text end-to-end reading systems. Experiments

done in this dataset demonstrate the key role of script identiﬁcation in a

complete end-to-end system that combines our script identiﬁcation method

with a previously published text detector and an oﬀ-the-shelf OCR engine.

Preprint submitted to Pattern Recognition

February 2, 2017

Keywords:

script identiﬁcation, scene text understanding, multi-language

OCR, convolutional neural networks, ensemble of conjoined networks

1. Introduction

Script and language identiﬁcation are important steps in modern OCR

systems designed for multi-language environments. Since text recognition al-

gorithms are language-dependent, detecting the script and language at hand

allows selecting the correct language model to employ [1]. While script iden-

tiﬁcation has been widely studied in document analysis [2, 3], it remains

an almost unexplored problem for scene text. In contrast to document im-

ages, scene text presents a set of speciﬁc challenges, stemming from the high

variability in terms of perspective distortion, physical appearance, variable

illumination and typeface design. At the same time, scene text comprises

typically a few words, contrary to longer text passages available in document

images.

Current end-to-end systems for scene text reading [4, 5, 6] assume single

script and language inputs given beforehand, i.e. provided by the user, or

inferred from available meta-data. The unconstrained text understanding

problem for large collections of images from unknown sources has not been

considered up to very recently [7, 8, 9, 10, 11]. While there exists some previ-

ous research in script identiﬁcation of text over complex backgrounds [12, 13],

such methods have been so far limited to video overlaid-text, which presents

in general diﬀerent challenges than scene text.

This paper addresses the problem of script identiﬁcation in natural scene

images, paving the road towards true multi-lingual end-to-end scene text

2

Figure 1: Collections of images from unknown sources may contain textual

information in diﬀerent scripts.

understanding. Multi-script text exhibits high intra-class variability (words

written in the same script vary a lot) and high inter-class similarity (certain

scripts resemble each other). Examining text samples from diﬀerent scripts,

it is clear that some stroke-parts are quite discriminative, whereas others

can be trivially ignored as they occur in multiple scripts. The ability to

distinguish these relevant stroke-parts can be leveraged for recognising the

corresponding script. Figure 2 shows an example of this idea.

Figure 2: (best viewed in color) Certain stroke-parts (in green) are discrimi-

native for the identiﬁcation of a particular script (left), while others (in red)

can be trivially ignored because are frequent in other classes (right).

The use of state of the art CNN classiﬁers for script identiﬁcation is not

straightforward, as they fail to address a key characteristic of scene text

instances: their extremely variable aspect ratio. As can be seen in Figure 3,

scene text images may span from single characters to long text sentences, and

thus resizing images to a ﬁxed aspect ratio, as in the typical use of holistic

CNN classiﬁers, will deteriorate discriminative parts of the image that are

characteristic of its class. The key intuition behind the proposed method is

3

that in order to retain the discriminative power of stroke parts we must rely

in powerful local feature representations and use them within a patch-based

classiﬁer. In other words, while holistic CNNs have superseded patch-based

methods for image classiﬁcation, we claim that patch-based classiﬁers can

still be essential in tasks where image shrinkage is not feasible.

Figure 3: Scene text images with the larger/smaller aspect ratio available in

three diﬀerent datasets: MLe2e(left), SIW-13(center), and CVSI(right).

In previously published work [10] we have presented a method combining

convolutional features, extracted by sliding a window with a single layer

Convolutional Neural Network (CNN) [14], and the Naive-Bayes Nearest

Neighbour (NBNN) classiﬁer [15] with promising results. In this paper we

demonstrate far superior performance by extending our previous work in two

diﬀerent ways: First, we use deep CNN architectures in order to learn more

discriminative representations for the individual image patches; Second, we

propose a novel learning methodology to jointly learn the patch representa-

tions and their importance (contribution) in a global image to class proba-

bilistic measure. For this, we train our CNN using an Ensemble of Conjoined

Networks and a loss function that takes into account the global classiﬁcation

error for a group of N patches instead of looking only into a single image

patch. Thus, at training time our network is presented with a group of N

patches sharing the same class label and produces a single probability distri-

bution over the classes for all them. This way we model the goal for which

the network is trained, not only to learn good local patch representations,

4

but also to learn their relative importance in the global image classiﬁcation

task.

Experiments performed over two public datasets for scene text classiﬁca-

tion demonstrate state-of-the-art results. In particular we are able to reduce

classiﬁcation error by 5 percentage points in the SIW-13 dataset. We also

introduce a new benchmark dataset, namely the MLe2e dataset, for the eval-

uation of scene text end-to-end reading systems and all intermediate stages

such as text detection, script identiﬁcation and text recognition. The dataset

contains a total of 711 scene images, and 1821 text line instances, covering

four diﬀerent scripts (Latin, Chinese, Kannada, and Hangul) and a large

variability of scene text samples.

2. Related Work

Script identiﬁcation is a well studied problem in document image analy-

sis. Gosh et al. [2] has published a compehensive review of methods dealing

with this problem. They identify two broad categories of methods: structure-

based and visual appearance-based techniques. In the ﬁrst category, Spitz

and Ozaki [16, 17] propose the use of the vertical distribution of upward

concavities in connected components and their optical density for page-wise

script identiﬁcation. Lee et al. [18], and Waked et al. [19] among others build

on top of Spitz seminal work by incorporating additional connected compo-

nent based features. Similarly, Chaudhuri et al. [20] use the projection pro-

ﬁle, statistical and topological features, and stroke features for classiﬁcation

of text lines in printed documents. Hochberg et al. [21] propose the use of

cluster-based templates to identify unique characteristic shapes. A method

5

that is similar in spirit with the one presented in this paper, while requiring

textual symbols to be precisely segmented to generate the templates.

Regarding segmentation-free methods based on visual appearance of scripts,

i.e. not directly analyzing the character patterns in the document, Wood et

al. [22] experimented with the use of vertical and horizontal projection pro-

ﬁles of full-page document images. More recent methods in this category have

used texture features from Gabor ﬁlters analysis [23, 24, 25] or Local Binary

Patterns [26]. Neural networks have been also used for segmentation-free

script identiﬁcation et al. [27, 28] without the use of hand-crafted features.

All the methods discussed above are designed speciﬁcally with printed

document images in mind. Structure-based methods require text connected

components to be precisely segmented from the image, while visual appearance-

based techniques are known to work better in bilevel text. Moreover, some of

these methods require large blocks of text in order to obtain suﬃcient infor-

mation and thus are not well suited for scene text which typically comprises

a few words.

Contrary to the case of printed document images, research in script iden-

tiﬁcation on non traditional paper layouts is more scarce, and has been

mainly dedicated to handwritten text [29, 30, 31, 32, 33], and video overlaid-

text [12, 34, 35, 36, 13] until very recently. Gllavatta et al. [12], in the ﬁrst

work dealing with video text script identiﬁcation, proposed a method using

the wavelet transform to detect edges in overlaid-text images. Then, they

extract a set of low-level edge features, and make use of a K-NN classiﬁer.

Sharma et al. [34] have explored the use of traditional document analysis

techniques for video overlaid-text script identiﬁcation at word level. They

6

analyze three sets of features: Zernike moments, Gabor ﬁlters, and a set

of hand-crafted gradient features previously used for handwritten character

recognition. They propose a number of pre-processing algorithms to over-

come the inherent challenges of video overlaid-text.

In their experiments

the combination of super resolution, gradient features, and a SVM classiﬁer

perform signiﬁcantly better that the other combinations.

Phan et al. [35] propose a method for combined detection of video text

overlay and script identiﬁcation. They propose the extraction of upper and

lower extreme points for each connected component of Canny edges of text

lines and analyse their smoothness and cursiveness.

Shivakumara et al. [36, 13] rely on skeletonization of the dominant gra-

dients They analyze the angular curvatures [36] of skeleton components, and

the spatial/structural [13] distribution of their end, joint, and intersection

points to extract a set of hand-crafted features. For classiﬁcation they build

a set of feature templates from train data, and use the Nearest Neighbor rule

for classifying scripts at word [36] or text block [13] level.

As said before, all these methods have been designed (and evaluated)

speciﬁcally for video overlaid-text, which presents in general diﬀerent chal-

lenges than scene text. Concretely, they mainly rely in accurate edge detec-

tion of text components and this is not always feasible in scene text.

More recently, Sharma et al. [37] explored the use of Bag-of-Visual Words

based techniques for word-wise script identiﬁcation in video-overlaid text.

They use Bag-Of-Features (BoF) and Spatial Pyramid Matching (SPM) with

patch based SIFT descriptors and found that the SPM pipeline outperforms

traditional script identiﬁcation techniques involving gradient based features

7

(e.g. HoG) and texture based features (e.g. LBP).

In 2015, the ICDAR Competition on Video Script Identiﬁcation (CVSI-

2015) [38] challenged the document analysis community with a new compet-

itive benchmark dataset. With images extracted from diﬀerent video sources

(news, sports etc.) covering mainly overlaid-text, but also a few instances of

scene text. The top performing methods in the competition where all based

in Convolutional Neural Networks, showing a clear diﬀerence in overall ac-

curacy over pipelines using hand-crafted features (e.g. LBP and/or HoG).

The ﬁrst dataset for script identiﬁcation in real scene text images was pro-

vided by Shi et al.in [7], where the authors propose the Multi-stage Spatially-

sensitive Pooling Network (MSPN) method. The MSPN network overcomes

the limitation of having a ﬁxed size input in traditional Convolutional Neural

Networks by pooling along each row of the intermediate layers’ outputs by

taking the maximum (or average) value in each row. Their method is ex-

tended in [8] by combining deep features and mid-level representations into a

globally trainable deep model. They extract local deep features at every layer

of the MSPN and describe images with a codebook-based encoding method

that can be used to ﬁne-tune the CNN weights.

Nicolaou et al. [9] has presented a method based on texture features

producing state of the art results in script identiﬁcation for both scene or

overlaid text images. They rely in hand-crafted texture features, a variant

of LBP, and a deep Multi Layer Perceptron to learn a metric space in which

they perform K-NN classiﬁcation.

In our previous work [10] we have proposed a patch-based method for

script identiﬁcation in scene text images. We used Convolutional features,

8

extracted from small image patches, and the Naive-Bayes Nearest Neighbour

classiﬁer (NBNN). We also presented a simple weighting strategy in order to

discover the most discriminative parts (or templates patches) per class in a

ﬁne-grained classiﬁcation approach.

In this paper we build upon our previous work [10] by extending it in two

ways: On one side, we make use of a much deeper Convolutional Neural Net-

work model. On the other hand, we replace the weighted NBNN classiﬁer by

a patch-based classiﬁcation rule that can be integrated in the CNN training

process by using an Ensemble of Conjoined Networks. This way, our CNN

model is able to learn at the same time expressive representations for image

patches and their relative contribution to the patch-based classiﬁcation rule.

From all reviewed methods the one proposed here is the only one based

in a patch-based classiﬁcation framework. Our intuition is that in cases

where holistic CNN models are not directly applicable, as in the case of text

images (because of their highly variable aspect ratios), the contribution of

rich parts descriptors without any deterioration (either by image distortion

or by descriptor quantization) is essential for correct image classiﬁcation.

In this sense our method is related with some CNN extensions that have

been proposed for video classiﬁcation. Unlike still images which can be

cropped and rescaled to a ﬁxed size, video sequences have a variable temporal

dimension and cannot be directly processed with a ﬁxed-size architecture. In

this context, 3D Convolutional Neural Networks [39, 40] have been proposed

to leverage the motion information encoded in multiple contiguous frames.

Basically the idea is to feed the CNN with a stack of a ﬁxed number of

consecutive frames and perform convolutions in both time and space dimen-

9

sions. Still these methods require a ﬁxed size input and thus they must be

applied several times through the whole sequence to obtain a chain of out-

puts that are then averaged [40] or fed into an Recurrent Neural Network [39]

to provide a ﬁnal decision. Karpathy et al. [41] also treat videos as bags of

short ﬁxed-length clips, but they investigate the use of diﬀerent temporal

connectivity patterns (early fusion, late fusion and slow fusion). To produce

predictions for an entire video they randomly sample 20 clips and take the

average of the network class predictions. While we share with these methods

the high-level goal of learning CNN weights from groups of stacked patches

(or frames) there are two key diﬀerences in the way we build our framework:

(1) the groups of patches that are fed into the network at training time are

randomly sampled and do not follow any particular order; and (2) at test

time we decouple the network to densely evaluate single patches and aver-

age their outputs. In other words, while in stacked-frame CNNs for video

recognition having an ordered sequence of input patches is crucial to learn

spatio-temporal features, our design aims to learn which are the most dis-

criminative patches in the input stack, independently of their relative spatial

arrangement.

In the experimental section we compare our method with some of the al-

gorithms reviewed in this section and demonstrate its superiority. Concretely

our approach improves the state-of-the-art in the SIW-13 [8] dataset for scene

text script classiﬁcation by a large margin of 5 percentage points, while per-

forms competitively in the CVSI-2015 [38] video overlaid-text dataset.

10

3. Patch-based classiﬁcation with Ensembles of Conjoined Net-

works

In our patch-based classiﬁcation method an input image is represented as

a collection of local descriptors, from patches extracted following a certain

sampling strategy. Those local features are then fed into a global classiﬁer

rule, that makes a decision for the input image.

3.1. Convolutional Neural Network for image-patch classiﬁcation

Given an input scene text image (i.e. a pre-segmented word or text line)

we ﬁrst resize it to a ﬁxed height of 40 pixels, but retaining its original

aspect ratio. Since scene text can appear in any possible combination of

foreground and background colors, we pre-process the image by converting it

into grayscale and centering pixel values. Then, we densely extract patches

at two diﬀerent scales, 32×32 and 40×40, by sliding a window with a step of

8 pixels. The particular values of these two window scales and step size was

found by cross-validation optimization as explained in section 4.2, and its

choice can be justiﬁed as follows: the 40 × 40 patch, covering the full height

of the resized image, is a natural choice in our system because it provides

the largest squared region we can crop; the 32 × 32 patches are conceived

for better scale invariance of the CNN model, similarly as the random crops

typically used for data augmentation in CNN-based image classiﬁcation [42].

Figure 4 shows the patches extracted from a given example image. This way

we build a large dataset of image patches that take the same label as the

image they were extracted from. With this dataset of patches we train a

CNN classiﬁer for the task of individual image patch classiﬁcation.

11

We use a Deep Convolutional Neural Network to build the expressive

image patch representations needed in our method. For the design of our

network we start from the CNN architecture proposed in [7] as it is known

to work well for script identiﬁcation. We then iteratively do an exhaustive

search to optimize by cross-validation the following CNN hyper-parameters:

number of convolutional and fully connected layers, number of ﬁlters per

layer, kernel sizes, and feature map normalisation schemes. The CNN archi-

tecture providing better performance in our experiments is shown in Figure 5.

Our CNN consists in three convolutional+pooling stages followed by an ex-

tra convolution and three fully connected layers. Details about the speciﬁc

conﬁguration and parameters are given in section 4.2.

At testing time, given a query scene text image the trained CNN model

is applied to image patches following the same sampling strategy described

before. Then, the individual CNN responses for each image patch can be fed

into the global classiﬁcation rule in order to make a single labeling decision

for the query image.

3.2. Training with an Ensemble of Conjoined Networks

Since the output of the CNN for an individual image patch is a probability

distribution over class labels, a simple global decision rule would be just to

average the responses of the CNN for all patches in a given query image:

y(I) =

CN N (xi)

1
nI

nI(cid:88)

i=1

(1)

where an image I takes the label with more probability in the averaged

softmax responses (y(I)) of their nI individual patches {x1, ..., xnI } outputs

12

on the CNN.

The problem with this global classiﬁcation rule is that the CNN weights

have been trained to solve a problem (individual patch classiﬁcation) that is

diﬀerent from the ﬁnal goal (i.e. classifying the whole query image). Besides,

it is based in a simplistic voting strategy for which all patches are assumed

to weight equally, i.e. no patches are more or less discriminative than others.

To overcome this we propose the use of an Ensemble of Conjoined Nets in

order to train the CNN for a task that resembles more the ﬁnal classiﬁcation

goal.

An Ensemble of Conjoined Nets (ECN), depicted in Figure 6, consists in

a set of identical networks that are joined at their outputs in order to provide

a unique classiﬁcation response. At training time the ECN is presented with

a set of N image patches extracted from the same image, thus sharing the

same label, and produces a single output for all them. Thus, to train an

ECN we must build a new training dataset where each sample consists in a

set of N patches with the same label (extracted from the same image).

ECNs take inspiration from Siamese Networks [43] but, instead of trying

to learn a metric space with a distance-based loss function, the individual

networks in the ECN are joined at their last fully connected layer (fc7 in

our case), which has the same number of neurons as the number of classes,

with a simple element-wise sum operation and thus we can use the standard

cross-entropy classiﬁcation loss. This way, the cross-entropy classiﬁcation

loss function of the ECN can be written in terms of the N individual patch

responses as follows:

13

E =

log(ˆpm,lm),

−1
M

M
(cid:88)

m=1

(cid:34) K
(cid:88)

k(cid:48)=1

N
(cid:88)

n=1

N
(cid:88)

n=1

ˆpm,k = exp(

xmnk)/

exp(

xmnk(cid:48))

(cid:35)

(2)

where M is the number of input samples in a mini-batch, ˆpm is the prob-

ability distribution over classes provided by the softmax function, lm is the

label of the m’th sample, N is the number of conjoined networks in the en-

semble, K is the number of classes, and xmnk ∈ [−∞, +∞] indicates the

response (score) of the k’th neuron in the n’th network for the m’th sample.

As can be appreciated in equation 2, in an ECN network a single input

patch contributes to the backpropagation error in terms of a global goal func-

tion for which it is not the only patch responsible. For example, even when

a single patch is correctly scored in the last fully connected layer it may be

penalized, and induced to produce a larger activation, if the other patches in

its same sample contribute to a wrong classiﬁcation at the ensemble output.

At test time, the CNN model trained in this way is applied to all image

patches in the query image and the global classiﬁcation rule is deﬁned as:

y(I) =

CN Nf c7(xi)

(3)

where an image I takes the label with the highest score in the sum (y(I))

of the fc7 layer responses of the nI individual patches {x1, ..., xnI }. This is

the same as in Equation 1 but using the fc7 layer responses instead of the

output softmax responses of the CNN.

Notice that still the task for which the ECN network has been trained is

nI(cid:88)

i=1

14

not exactly the same deﬁned by this global classiﬁcation rule, as the number

of patches nI is variable for each image and usually diﬀerent than the number

of conjoined networks N . However, it certainly resembles more the true

ﬁnal classiﬁcation goal. The number of conjoined networks N is an hyper-

parameter of the method that is largely dependent on the task to be solved

and is discussed in the experimental section.

4. Experiments

All reported experiments were conducted over three datasets, namely the

Video Script Identiﬁcation Competition (CVSI-2015) dataset1, the SIW-13

dataset2, and the MLe2e dataset3.

The CVSI-2015 [38] dataset comprises pre-segmented words in ten scripts:

English, Hindi, Bengali, Oriya, Gujrathi, Punjabi, Kannada, Tamil, Telegu,

and Arabic. The dataset contains about 1000 words for each script and is

divided into three parts: a training set ( 60% of the total images), a validation

set (10%), and a test set (30%). Text is extracted from various video sources

(news, sports etc.) and, while it contains a few instances of scene text, it

covers mainly overlay video text.

The SIW-13 datset [8] comprises 16291 pre-segmented text lines in thir-

teen scripts: Arabic, Cambodian, Chinese, English, Greek, Hebrew, Japanese,

Kannada, Korean, Mongolian, Russian, Thai, and Tibetan. The test set con-

tains 500 text lines for each script, 6500 in total, and all the other images

1http://www.ict.griffith.edu.au/cvsi2015/
2http://mc.eistar.net/~xbai/mspnProjectPage/
3http://github.com/lluisgomez/script_identification/

15

are provided for training. In this case, text was extracted from natural scene

images from Google Street View.

4.1. The MLe2e dataset

This paper introduces the ﬁrst dataset available up to date for the evalu-

ation of multi-lingual scene text end-to-end reading systems and all interme-

diate stages: text detection, script identiﬁcation, and text recognition. The

Multi-Language end-to-end (MLe2e) dataset has been harvested from vari-

ous existing scene text datasets for which the images and ground-truth have

been revised in order to make them homogeneous. The original images come

from the following datasets: Multilanguage(ML) [44] and MSRA-TD500 [45]

contribute Latin and Chinese text samples, Chars74K [46] and MSRRC [47]

contribute Latin and Kannada samples, and KAIST [48] contributes Latin

and Hangul samples.

In order to provide a homogeneous dataset, all images have been resized

proportionally to ﬁt in 640 × 480 pixels, which is the default image size

of the KAIST dataset. Moreover, the groundtruth has been revised to en-

sure a common text line annotation level [49]. During this process human

annotators were asked to review all resized images, adding the script class la-

bels and text transcriptions to the groundtruth, and checking for annotation

consistency: discarding images with unknown scripts or where all text is un-

readable (this may happen because images were resized); joining individual

word annotations into text line level annotations; discarding images where

correct text line segmentation is not clear or cannot be established, and im-

ages where a bounding box annotation contains more than one script (this

happens very rarely e.g.

in trademarks or logos) or where more than half

16

of the bounding box is background (this may happen with heavily slanted

or curved tex). Arabic numerals (0, .., 9), widely used in combination with

many (if not all) scripts, are labeled as follows. A text line containing text

and Arabic numerals is labeled as the script of the text it contains, while a

text line containing only Arabic numerals is labeled as Latin.

The MLe2e dataset contains a total of 711 scene images covering four

diﬀerent scripts (Latin, Chinese, Kannada, and Hangul) and a large vari-

ability of scene text samples. The dataset is split into a train and a test

set with 450 and 261 images respectively. The split was done randomly, but

in a way that the test set contains a balanced number of instances of each

class (aprox. 160 text lines samples of each script), leaving the rest of the

images for the train set (which is not balanced by default). The dataset is

suitable for evaluating various typical stages of end-to-end pipelines, such as

multi-script text detection, joint detection and script identiﬁcation, end-to-

end multi-lingual recognition, and script identiﬁcation in pre-segmented text

lines. For the latter, the dataset also provides the cropped images with the

text lines corresponding to each data split: 1178 and 643 images in the train

and test set respectively.

While being a dataset that has been harvested from a mix of existing

datasets it is important to notice that building it has supposed an important

annotation eﬀort: since some of the original datasets did not provide text

transcriptions, and/or where annotated at diﬀerent granularity levels. More-

over, despite the fact that the number of languages in the dataset is rather

limited (four scripts) it is the ﬁrst public dataset that covers the evaluation

of all stages of multi-lingual end-to-end systems for scene text understanding

17

in natural scenes. We think this is an important contribution of this paper

and hope the dataset will be useful to other researchers in the community.

4.2. Implementation details

In this section we detail the architectures of the network models used in

this paper, as well as the diﬀerent hyper-parameter setups that can be used

to reproduce the results provided in following sections. In all our experiments

we have used the open source Caﬀe [50] framework for deep learning running

on commodity GPUs. Source code and compatible Caﬀe models are made

publicly available4.

We have performed exhaustive experiments by varying many of the pro-

posed methods parameters, training multiple models, and choosing the one

with best cross-validation performance on the SIW-13 training set. The fol-

lowing parameters were tuned in this procedure: the size and step of the

sliding window, the base learning rate, the number of convolutional and fully

connected layers, the number of nodes in all layers, the convolutional kernel

sizes, and the feature map normalisation schemes

This way, the best basic CNN model found for individual image patch

classiﬁcation is described in section 3.1 and Figure 5, and has the following

per layer conﬁguration:

• Input layer: single channel 32 × 32 image patch.

• conv1 layer: 96 ﬁlters with size 5 × 5. Stride=1, pad=0. Output size:

96 × 28 × 28.

• pool1 layer: kernel size=3, stride=2, pad=1. Otput size: 96 × 15 × 15.

4http://github.com/lluisgomez/script_identification/

18

• conv2 layer: 256 ﬁlters with size 3 × 3. Stride=1, pad=0. Output size:

• pool2 layer: kernel size=3, stride=2, pad=1. Otput size: 256 × 7 × 7.

• conv3 layer: 384 ﬁlters with size 3 × 3. Stride=1, pad=0. Output size:

256 × 13 × 13.

384 × 5 × 5.

• pool3 layer: kernel size=3, stride=2, pad=1. Otput size: 384 × 3 × 3.

• conv4 layer: 512 ﬁlters with size 1 × 1. Stride=1, pad=0. Output size:

512 × 3 × 3.

• fc5 layer: 4096 neurons.

• fc6 layer: 1024 neurons.

• fc7 layer: N neurons, where N is the number of classes.

• SoftMax layer: Output a probability distribution over the N class la-

bels.

The total number of parameters of the network is ≈ 24M for the N = 13

case in the SIW-13 dataset. All convolution and fully connected layers use

Rectiﬁed Linear Units (ReLU). In conv1 and conv2 layers we perform normal-

ization over input regions using Local Response Normalization (LRN) [51].

At training time, we use dropout [52] (with a 0.5 ratio) in fc5 and fc6 layers.

To train the basic network model we use Stochastic Gradient Descent

(SGD) with momentum and L2 regularization. We use mini-batches of 64

images. The base learning rate is set to 0.01 and is decreased by a factor of

×10 every 100k iterations. The momentum weight parameter is set to 0.9,

and the weight decay regularization parameter to 5 × 10−4.

When training for individual patch classiﬁcation, we build a dataset of

small patches extracted by dense sampling the original training set images,

19

as explained in section 3.1. Notice that this produces a large set of patch

samples, e.g. in the SIW-13 dataset the number of training samples is close to

half million. With these numbers the network converges after 250k iterations.

In the case of the Ensemble of Conjoined Networks the basic network

detailed above is replicated N times, and all replicas are tied at their fc7

outputs with an element-wise sum layer which is connected to a single output

SoftMax layer. All networks in the ECN share the same parameters values.

Training the ECN requires a dataset where each input sample is com-

posed by N image patches. We generate this dataset as follows: given an

input image we extract patches the same way as for the simple network, then

we generate random N −combinations of the image patches, allowing repeti-

tions if the number of patches is < N . Notice that this way the number of

samples can be increased up to very large-scale numbers because the number
(cid:1) when the number of patches in
of possible diﬀerent N −combinations is (cid:0)M

N

a given image M is larger that the number of conjoined nets N , which is

the usual case. This is an important aspect of ECNs, as the training dataset

generation process becomes a data augmentation technique in itself. We can

see this data augmentation process as generating new small text instances

that are composed from randomly chosen parts of their original generators.

However, it is obviously non-practical to use all possible combinations for

training; thus, in order to get a manageable number of samples, we have used

the simple rule of generating 2 × M samples per input, which for example in

the SIW-13 dataset would produce around one million samples.

In terms of computational training complexity, the ECN has an impor-

tant drawback compared to the simple network model: the number of com-

20

putations is multiplied by N in each forward pass, similarly the amount of

memory needed is linearly increased by N . To overcome this limitation, we

use a ﬁne-tuning approach to train ECNs. First, we train the simple network

model, and then we do ﬁne-tuning on the ECN parameters starting from the

values learned using the simple net. When ﬁne-tuning, we have found that

starting from a fully converged network in the single-patch classiﬁcation task

we reach a local minimum of the global task, thus providing zero loss in most

(if not all) the iterations and not allowing the network to learn anything new.

In order to avoid this local minima situation we start the ﬁne-tuning from

a non-converged network (more or less at about 90/95% of the attainable

individual patch classiﬁcation accuracy).

Using ﬁne-tuning with a base learning rate of 0.001 (decreasing ×10 every

10k iterations) the ECN converges much faster, in the order of 35k iterations.

All other learning parameters are set the same as in the simple network

training setup.

The number of nets N in the ensemble can be seen as an extra hyper-

parameter in the ECN learning algorithm. Intuitively a dataset with larger

text sequences would beneﬁt from larger N values, while on the contrary

in the extreme case of classifying small squared images (i.e. each image is

represented by a single patch) any value of N > 1 does not make sense. Since

our datasets contain text instances with variable length a possible procedure

to select the optimal value of N is by using a validation set. We have done

experiments in the SIW-13 dataset by dividing the provided train set and

keeping 10% for validation. Classiﬁcation accuracy on the validation set for

various N values are shown in Figure 7. As can be appreciated the positive

21

impact of training with an ensemble of networks is evident for small values

of N , and mostly saturated for values N > 9. In the following we use a value

of N = 10 for all the remaining experiments.

4.3. Script identiﬁcation in pre-segmented text lines

In this section we study the performance of the proposed method for

script identiﬁcation in pre-segmented text lines. Table 1 shows the overall

performance comparison of our method with the state-of-the-art in CVSI-

2015, SIW-13, and MLe2e datasets. Figure 8 shows the confusion matrices

for our method in all three datasets with detailed per class classiﬁcation

results.

In Table 1 we also provide comparison with three well known image

recognition pipelines using Scale Invariant Features [53] (SIFT) in three dif-

ferent encodings: Fisher Vectors, Vector of Locally Aggregated Descriptors

(VLAD), and Bag of Words (BoW); and a linear SVM classiﬁer. In all base-

lines we extract SIFT features at four diﬀerent scales in sliding window with

a step of 8 pixels. For the Fisher vectors we use a 256 visual words GMM, for

VLAD a 256 vector quantized visual words, and for BoW 2,048 vector quan-

tized visual words histograms. The step size and number of visual words were

set to similar values to our method when possible in order to oﬀer a fair eval-

uation. These three pipelines have been implemented with the VLFeat [54]

and liblinear [55] open source libraries. The entry “Sequence-based CNN”

in Table 1 corresponds to the results obtained with the early fusion design

proposed in [41] with a stack of 5 consecutive patches.

As shown in Table 1 the proposed method outperforms state of the art

and all baseline methods in the SIW-13 and MLe2e scene text datasets, while

22

Method

SIW-13 MLe2e CVSI

This work - Ensemble of Conjoined Nets

94.8

94.4

This work - Simple CNN (Avg.)

This work - Simple CNN (fc5+SVM)

Shi et al. [8]

HUST [7, 38]

Google [38]

Nicolaou et al. [9]

Gomez et al. [10]

CVC-2 [10, 38]

SRS-LBP + KNN [56]

C-DAC [38]

CUK [38]

97.2

96.7

96.9

94.3

96.69

98.91

98.18

93.1

93.6

-

-

-

-

-

-

91.12

97.91

88.16

96.0

82.71

94.20

84.66

74.06

88.63

94.11

90.19

93.92

86.45

84.38

89.80

93.62

92.8

93.4

89.4

88.0

83.7

76.9

-

-

-

-

-

90.7

89.2

83.4

88.9

Baseline SIFT + Fisher Vectors + SVM

Baseline SIFT + VLAD + SVM

Baseline SIFT + Bag of Words + SVM

Baseline Sequence-based CNN [41] (Early fusion)

Table 1: Overall classiﬁcation performance comparison with state-of-the-art

in three diﬀerent datasets: SIW-13 [8], MLe2e, and CVSI [38].

23

performing competitively in the case of CVSI video overlay text dataset. In

the SIW-13 dataset the proposed method signiﬁcantly outperforms the best

performing method known up to date by more than 4 percentual points.

The entry “Simple CNN (fc5+SVM)” (third row) in Table 1 corresponds

to the results obtained with a linear SVM classiﬁer by using features ex-

tracted from the “Simple CNN” network. For this experiment we represent

each image in the dataset with a ﬁxed length vector with the averaged out-

puts of all its patches in the fc5 layer of the network. Then we train a linear

SVM classiﬁer using cross-validation on the training set, and show the classi-

ﬁcation performance on the test set. Similar results (or slightly worse) have

been found for features extracted from other layers (fc7, fc6, conv4) and us-

ing other linear classiﬁers (e.g.

logistic regression). When compared with

the “Simple CNN” approach we appreciate that classiﬁcation performance

is better for this combination (fc5+SVM). This conﬁrms the intuition that

classiﬁcation performance can be improved by optimizing the combination

of the results for the individual patches. However, the performance of the

CNN trained with the ensemble of conjoined networks is still better. As men-

tioned earlier, the additional beneﬁt of our approach here is in the end-to-end

learning of both the visual features and the optimal combination scheme for

classiﬁcation.

The contribution of training with ensembles of conjoined nets is consistent

in all three evaluated datasets but more notable on SIW-13, as appreciated by

comparing the ﬁrst two rows of Table 1 which correspond to the nets trained

with the ensemble (ﬁrst row) and the simple model (second row). This com-

parison can be further strengthened by testing if the provided improvement is

24

statistically signiﬁcant. For this we use the within-subjects chi-squared test

(McNemar’s test) [57] to compare the predictive accuracy of the two models.

The obtained p-values on the SIW-13, MLe2e, and CVSI datasets are respec-

tively 1.4 × 10−16, 0.057, and 0.0026. In the case of the SIW-13 dataset the

p-value is way smaller than the assumed signiﬁcance threshold (α = 0.05),

thus we can reject the null-hypothesis that both models perform equally well

on this dataset and certify a statistically signiﬁcant improvement. On the

other hand we appreciate a marginal improvement on the other two datasets.

Our interpretation of the results on CVSI and MLe2e datasets in compar-

ison with the ones obtained on SIW-13 relates to its distinct nature. On one

hand the MLe2e dataset covers only four diﬀerent scripts (Latin, Chinese,

Kannada, and Hangul) for which the inter-class similarity does not represent

a real problem. On the other hand, the CVSI overlaid-text variability and

clutter is rather limited compared with that found in the scene text of MLe2e

and SIW-13. As can be appreciated in Figure 9 overlaid-text is usually bi-

level without much clutter. Figure 10 shows another important characteristic

of CVSI dataset: since cropped words in the dataset belong to very long sen-

tences of overlay text in videos, e.g. from rotating headlines, it is common to

ﬁnd a few dozens of samples sharing exactly the same font and background

both in the train and test sets. This particularity makes the ECN network

not really helpful in the case of CVSI, as the data augmentation by image

patches recombinations is somehow already implicit on the dataset.

Furthermore, the CVSI-2015 competition winner (Google) makes use of a

deep convolutional network but applies a binarization pre-processing to the

input images. In our opinion this binarization may not be a realistic pre-

25

processing in general for scene text images. As an example of this argument

one can easily see in Figure 9 that binarization of scene text instances is

not trivial as in overlay text. Similar justiﬁcation applies to other methods

performing better than ours in CVSI. In particular the LBP features used

in [9], as well as the patch-level whitening used in our previous work [10], may

potentially take advantage of the simpler, bi-level, nature of text instances

in CVSI dataset. It is important to notice here that these two algorithms,

corresponding to our previous works in script identiﬁcation, have close num-

bers to the Google ones in CVSI-2015 (see Table 1) but perform quite bad

in SIW-13.

As a conclusion of the experiments performed in this section we can say

that the improvement of training a patch-based CNN classiﬁer with an en-

semble of conjoined nets is especially appreciable in cases where we have a

large number of classes, with large inter-class similarity, and cluttered scene

images, as is the case of the challenging SIW-13 dataset. This demonstrates

our initial claim that a powerful script identiﬁcation method for scene text

images must be based in learning good local patch representations, and also

their relative importance in the global image classiﬁcation task. Figure 11

shows some examples of challenging text images that are correctly classiﬁed

by our method but not with the Simple CNN approach. Figure 12 shows a

set of missclassiﬁed images.

Finally, in Figure 13 we show the classiﬁcation accuracy of the CNN

trained with ensembles of conjoined nets as a function of the image width

on SIW-13 test images. We appreciate that the method is robust even for

small text images which contain a limited number of unique patches. Com-

26

putation time for our method is also dependent on the input image length

and ranges from 4ms.

in for the smaller images up to 23ms. for the larger

ones. The average computation time on the SIW-13 test set is of 13ms using

a commodity GPU. At test time computation is made eﬃcient by stacking

all patches of the input image in a single mini-batch.

4.4. Joint text detection and script identiﬁcation in scene images

In this experiment we evaluate the performance of a complete pipeline for

detection and script identiﬁcation in its joint ability to detect text lines in

natural scene images and properly recognizing their scripts. The key interest

of this experiment is to study the performance of the proposed script identi-

ﬁcation algorithm when realistic, non-perfect, text localisation is available.

Most text detection pipelines are trained explicitly for a speciﬁc script

(typically English) and generalise pretty badly to the multi-script scenario.

We have chosen to use here our previously published script-agnostic method [58],

which is designed for multi-script text detection and generalises well to any

script. The method detects character candidates using the Maximally Sta-

ble Extremal Regions (MSER) [59] algorithm, and builds diﬀerent hierar-

chies where the initial regions are grouped by agglomerative clustering, using

complementary similarity measures. In such hierarchies each node deﬁnes a

possible text hypothesis. Then, an eﬃcient classiﬁer, using incrementally

computable descriptors, is used to walk each hierarchy and select the nodes

with larger text-likelihood.

In this paper script identiﬁcation is performed at the text line level, be-

cause segmentation into words is largely script-dependent, and not meaning-

ful in Chinese/Korean scripts. Notice however that in some cases, by the

27

intrinsic nature of scene text, a text line provided by the text detection mod-

ule may correspond to a single word, so we must deal with a large variability

in the length of provided text lines. The experiments are performed over the

new MLe2e dataset.

For evaluation of the joint text detection and script identiﬁcation task in

the MLe2e dataset we propose the use of a simple two-stage evaluation frame-

work. First, localisation is assessed based on the Intersection-over-Union

(IoU) metric between detected and ground-truth regions, as commonly used

in object detection tasks [60] and the recent ICDAR 2015 Robust Read-

ing Competition5 [61]. Second, the predicted script is veriﬁed against the

ground-truth. A detected bounding box is thus considered correct if it has a

IoU > 0.5 with a bounding box in the ground-truth and the predicted script

is correct.

The localisation-only performance, corresponding to the ﬁrst stage of the

evaluation, yields an F-score of 0.63 (Precision of 0.57 and Recall of 0.69).

This deﬁnes the upper-bound for the joint task. The two stage evaluation,

including script identiﬁcation, of the proposed method compared with our

previous work is shown in Table 2.

Intuitively the proposed method for script identiﬁcation is eﬀective even

when the text region is badly localised, as long as part of the text area is

within the localised region. To support this argument we have performed an

additional experiment where our algorithm is applied to cropped regions from

pre-segmented text images. For this, we take the SIW-13 original images and

5http://rrc.cvc.uab.es

28

Method

Correct Wrong Missing Precision Recall F-score

This work - ECN

Gomez et al. [10]

395

364

376

407

245

278

0.51

0.47

0.62

0.56

0.57

0.52

Table 2: Text detection and script identiﬁcation performance in the MLe2e

dataset.

calculate the performance of our method when applied to cropped regions of

variable length, up to the minimum size possible (40 × 40 pixels). As can

be appreciated in Figure 14 the experiment demonstrates that the proposed

method is eﬀective even when small parts of the text lines are provided.

Such a behaviour is to be expected, due to the way our method treats local

information to decide on a script class. In the case of the pipeline for joint

detection and script identiﬁcation, this extends to regions that did not pass

the 0.5 IoU threshold, but had their script correctly identiﬁed. This opens

the possibility to make use of script identiﬁcation to inform and / or improve

the text localisation process. The information of the identiﬁed script can be

used to reﬁne the detections.

4.5. End-to-end multi-lingual recognition in scene images

In this section we evaluate the performance of a complete pipeline for

end-to-end multi-lingual recognition in scene images. For this, we combine

the pipeline used in the previous section with a well known oﬀ-the-shelf OCR

engine: the open source project Tesseract6 [62]. Similar pipelines [63, 64, 65]

6http://code.google.com/p/tesseract-ocr/

29

using oﬀ-the-shelf OCR engines have demonstrated state-of-the-art end-to-

end performance in English-only datasets up to very recently, provided that

the text detection module is able to produce good pixel-level segmentation

of text.

The setup of the OCR engine in our pipeline is minimal: given a text

detection hypothesis from the detection module we set the recognition lan-

guage to the one provided by the script identiﬁcation module, and we set the

OCR to interpret the input as a single text line. Apart from that we use the

default Tesseract parameters.

The recognition output is ﬁltered with a simple post-processing junk ﬁlter

in order to eliminate garbage recognitions, i.e. sequences of identical charac-

ters like ”IIii” that may appear as the result of trying to recognize repetitive

patterns in the scene. Concretely, we discard the words in which more than

half of their characters are recognized as one of ”i”, ”l”, ”I”, or other special

characters like: punctuation marks, quotes, exclamation, etc. We also reject

those detections for which the recognition conﬁdence provided by the OCR

engine is under a certain threshold.

The evaluation protocol is similar to the one used in other end-to-end

scene text recognition datasets [66, 61]. Ideally, in end-to-end word recogni-

tion, a given output word is considered correct if it overlaps more than 0.5

with a ground-truth word and all its characters are recognized correctly (case

sensitive). However, since in the case of the MLe2e dataset we are evaluating

text lines instead of single words, we relax a bit this correctness criteria by

allowing the OCR output to to make 1

8 character level errors. This relax-
ation is motivated by the fact that for a given test sentence with more than

30

Script identiﬁcation Correct Wrong Missing Precision Recall F-score

This work - ECN

Gomez et al. [10]

Tesseract

96

82

50

212

211

93

503

517

549

0.31

0.28

0.35

0.16

0.21

0.14

0.08

0.18

0.13

Table 3: End-to-end multi-lingual recognition performance in the MLe2e

dataset.

1
8.

test set.

8 characters (e.g. with two words) having only one character mistake may

still produce a partial understanding of the text (e.g. one of the words is

correct), and thus must not be penalized the same way as if all characters

are wrongly recognized. This way, a given output text line is considered cor-

rect if overlaps more than 0.5 with a ground-truth text line and their edit

distance divided by the number of characters of the largest is smaller than

Table 3 shows a comparison of the proposed end-to-end pipeline by using

diﬀerent script identiﬁcation modules: the method presented in this paper,

our previously published work, and Tesseract’s built-in alternative. Figure 15

shows the output of our full end-to-end pipeline for some images in the MLe2e

Tesseract method in Table 3 refers to the use of Tesseract’s own script es-

timation algorithm [1]. We have found that Tesseract’s algorithm is designed

to work with large corpses of text (e.g. full page documents) and does not

work well for the case of single text lines.

Results in Table 3 demonstrate the direct correlation between having

31

better script identiﬁcation rates and better end-to-end recognition results.

The ﬁnal multi-lingual recognition f-score obtained (0.21) is far from the

state-of-the art in end-to-end recognition systems designed for English-only

environments [4, 5, 6]. As a fair comparison, a very similar pipeline using

the Tesseract OCR engine [64] achieves an f-score of 0.40 in the ICDAR

English-only dataset. The lower performance obtained in MLe2e dataset

stems from a number of challenges that are speciﬁc to its multi-lingual nature.

For example, in some scripts (e.g. Chinese and Kannada) glyphs are many

times non single-body regions, composed by (or complemented with) small

strokes that in many cases are lost in the text segmentation stage. In such

cases having a bad pixel-level segmentation of text would make it practically

impossible for the OCR engine to produce a correct recognition.

Our pipeline results represent the ﬁrst reference result for multi-lingual

scene text recognition and a ﬁrst benchmark from which better systems can

be built, e.g. replacing the oﬀ-the-shelf OCR engine by other recognition

modules better suited for scene text imagery.

4.6. Cross-domain performance and confusion in single-language datasets

In this experiment we evaluate the cross-domain performance of learned

CNN weights from one dataset to the other. For example, we evaluate on

the MLe2e and CVSI test sets using the network trained with the SIw-13

train set, by measuring classiﬁcation accuracy only for their common script

classes: Arabic, English, and Kannada in CVSI; Chinese, English, Kannada,

and Korean in MLe2e. Finally, we evaluate the misclassiﬁcation error of our

method (trained in diﬀerent datasets) over two single-script datasets. For

this experiment we use the ICDAR2013 [67] and ALIF [68] datasets, which

32

provide cropped word images of English scene text and Arabic video overlaid

text respectively. Table 4 shows the results of these experiments.

Method

SIW-13 MLe2e CVSI

ICDAR ALIF

ECN CNN (SIW-13)

94.8

86.8

90.6

74.7

100

ECN CNN (MLe2e)

90.8

94.4

98.3

95.3

-

ECN CNN (CVSI)

42.3

43.5

97.2

65.2

91.8

Table 4: Cross-domain performance of our method measured by train-

ing/testing in diﬀerent datasets.

Notice that results in Table 4 are not directly comparable among rows

because each classiﬁer has been trained with a diﬀerent number of classes,

thus having diﬀerent rates for a random choice classiﬁcation. However, the

experiment serves as a validation of how good a given classiﬁer is in perform-

ing with data that is distinct in nature to the one used for training. In this

sense, the obtained results show a clear weakness when the model is trained

on the video overlaid text of CVSI and subsequently applied to scene text

images (SIW-13, MLe2e, and ICDAR). On the contrary, models trained on

scene text datasets are quite stable in other scene text data, as well as in

video overlaid text (CVSI and ALIF).

In fact, this is an expected result, because the domain of video overlay

text can be seen as a sub-domain of the scene text domain. Since the scene

text datasets are richer in text variability, e.g. in terms of perspective distor-

tion, physical appearance, variable illumination, and typeface designs, script

33

identiﬁcation on these datasets is a more diﬃcult problem, and their data

is more indicated if one wants to learn eﬀective cross-domain models. This

demonstrates that our method is able to learn discriminative stroke-part rep-

resentations that are not dataset-speciﬁc, and provides evidence to the claims

made in section 4.3 when interpreting the obtained results in CVSI dataset

comparing with other methods that may be more engineered to the speciﬁc

CVSI data but not generalizing well in scene text datasets.

5. Conclusion

A novel method for script identiﬁcation in natural scene images was pre-

sented. The method is based on the use of ensembles of conjoined convo-

lutional networks to jointly learn discriminative stroke-part representations

and their relative importance in a patch-based classiﬁcation scheme. Exper-

iments performed in three diﬀerent datasets exhibit state of the art accuracy

rates in comparison to a number of state-of-the-art methods, including the

participants in the CVSI-2015 competition and three standard image classi-

ﬁcation pipelines.

In addition, a new public benchmark dataset for the evaluation of all

stages of multi-lingual end-to-end scene text reading systems was introduced.

Our work demonstrates the viability of script identiﬁcation in natural

scene images, paving the road towards true multi-lingual end-to-end scene

text understanding.

34

This project was supported by the Spanish project TIN2014-52072-P,

the fellowship RYC-2009-05031, and the Catalan government scholarship

Acknowledgment

2014FI B1-0017.

References

[1] R. Unnikrishnan, R. Smith, Combined script and page orientation esti-

mation using the tesseract ocr engine, in: Proceedings of the Interna-

tional Workshop on Multilingual OCR, ACM, 2009, p. 6.

[2] D. Ghosh, T. Dube, A. P. Shivaprasad, Script recognitiona review, Pat-

tern Analysis and Machine Intelligence, IEEE Transactions on 32 (12)

(2010) 2142–2161.

[3] U. Pal, B. Chaudhuri, Indian script character recognition: a survey,

pattern Recognition 37 (9) (2004) 1887–1899.

[4] A. Bissacco, M. Cummins, Y. Netzer, H. Neven, Photoocr: Reading text

in uncontrolled conditions, in: Proceedings of the IEEE International

Conference on Computer Vision, 2013, pp. 785–792.

[5] M. Jaderberg, A. Vedaldi, A. Zisserman, Deep features for text spotting,

in: Computer Vision–ECCV 2014, Springer, 2014, pp. 512–528.

[6] L. Neumann, J. Matas, Real-time lexicon-free scene text localization

and recognition, IEEE Transactions on Pattern Analysis and Machine

Intelligence PP (99) (2015) 1–1.

35

[7] B. Shi, C. Yao, C. Zhang, X. Guo, F. Huang, X. Bai, Automatic script

identiﬁcation in the wild, in: Document Analysis and Recognition (IC-

DAR), 2015 13th International Conference on, IEEE, 2015, pp. 531–535.

[8] B. Shi, X. Bai, C. Yao, Script identiﬁcation in the wild via discriminative

convolutional neural network, Pattern Recognition 52 (2016) 448–458.

[9] A. Nicolaou, A. D. Bagdanov, L. Gomez-Bigorda, D. Karatzas, Visual

script and language recognition, in: DAS, 2016.

[10] L. Gomez-Bigorda, D. Karatzas, A ﬁne-grained approach to scene text

script identiﬁcation, in: DAS, 2016.

[11] S. Tian, U. Bhattacharya, S. Lu, B. Su, Q. Wang, X. Wei, Y. Lu, C. L.

Tan, Multilingual scene character recognition with co-occurrence of his-

togram of oriented gradients, Pattern Recognition 51 (2016) 125 – 134.

[12] J. Gllavata, B. Freisleben, Script recognition in images with complex

backgrounds, in: Signal Processing and Information Technology, 2005.

Proceedings of the Fifth IEEE International Symposium on, IEEE, 2005,

pp. 589–594.

[13] P. Shivakumara, Z. Yuan, D. Zhao, T. Lu, C. L. Tan, New gradient-

spatial-structural features for video script identiﬁcation, Computer Vi-

sion and Image Understanding 130 (2015) 35–53.

[14] A. Coates, A. Y. Ng, H. Lee, An analysis of single-layer networks in

unsupervised feature learning, in: International conference on artiﬁcial

intelligence and statistics, 2011, pp. 215–223.

36

[15] O. Boiman, E. Shechtman, M. Irani, In defense of nearest-neighbor

based image classiﬁcation, in: Computer Vision and Pattern Recog-

nition, 2008. CVPR 2008. IEEE Conference on, IEEE, 2008, pp. 1–8.

[16] A. L. Spitz, M. Ozaki, Palace: A multilingual document recognition

system, in: Document Analysis Systems, Vol. 1, Singapore: World Sci-

entiﬁc, 1995, pp. 16–37.

[17] A. L. Spitz, Determination of the script and language content of docu-

ment images, Pattern Analysis and Machine Intelligence, IEEE Trans-

actions on 19 (3) (1997) 235–245.

[18] D. Lee, C. R. Nohl, H. S. Baird, Language identiﬁcation in complex, un-

oriented, and degraded document images, Series in Machine Perception

And Artiﬁcial Intelligence 29 (1998) 17–39.

[19] B. Waked, S. Bergler, C. Suen, S. Khoury, Skew detection, page segmen-

tation, and script classiﬁcation of printed document images, in: Systems,

Man, and Cybernetics, 1998. 1998 IEEE International Conference on,

Vol. 5, IEEE, 1998, pp. 4470–4475.

[20] S. Chaudhury, R. Sheth, Trainable script identiﬁcation strategies for

indian languages, in: Document Analysis and Recognition, 1999. IC-

DAR’99. Proceedings of the Fifth International Conference on, IEEE,

1999, pp. 657–660.

[21] J. Hochberg, L. Kerns, P. Kelly, T. Thomas, Automatic script identiﬁca-

tion from images using cluster-based templates, in: Document Analysis

37

and Recognition, 1995., Proceedings of the Third International Confer-

ence on, Vol. 1, IEEE, 1995, pp. 378–381.

[22] S. L. Wood, X. Yao, K. Krishnamurthi, L. Dang, Language identiﬁcation

for printed text independent of segmentation, in:

Image Processing,

1995. Proceedings., International Conference on, Vol. 3, IEEE, 1995,

pp. 428–431.

[23] T. Tan, Rotation invariant texture features and their use in automatic

script identiﬁcation, Pattern Analysis and Machine Intelligence, IEEE

Transactions on 20 (7) (1998) 751–756.

[24] W. Chan, G. Coghill, Text analysis using local energy, Pattern Recog-

nition 34 (12) (2001) 2523 – 2532.

[25] W. Pan, C. Y. Suen, T. D. Bui, Script identiﬁcation using steerable ga-

bor ﬁlters, in: Document Analysis and Recognition, 2005. Proceedings.

Eighth International Conference on, IEEE, 2005, pp. 883–887.

[26] M. A. Ferrer, A. Morales, U. Pal, Lbp based line-wise script identiﬁ-

cation, in: Document Analysis and Recognition (ICDAR), 2013 12th

International Conference on, IEEE, 2013, pp. 369–373.

[27] A. K. Jain, Y. Zhong, Page segmentation using texture analysis, Pattern

Recognition 29 (5) (1996) 743 – 770.

[28] Z. Chi, Q. Wang, W.-C. Siu, Hierarchical content classiﬁcation and

script determination for automatic document image processing, Pattern

Recognition 36 (11) (2003) 2483–2500.

38

[29] A. Hennig, N. Sherkat, Exploiting zoning based on approximating

splines in cursive script recognition, Pattern Recognition 35 (2) (2002)

445 – 454.

3393.

[30] J. Schenk, J. Lenz, G. Rigoll, Novel script line identiﬁcation method

for script normalization and feature extraction in on-line handwritten

whiteboard note recognition, Pattern Recognition 42 (12) (2009) 3383 –

[31] G. Zhu, X. Yu, Y. Li, D. Doermann, Language identiﬁcation for hand-

written document images using a shape codebook, Pattern Recognition

42 (12) (2009) 3184 – 3191.

[32] S. Basu, N. Das, R. Sarkar, M. Kundu, M. Nasipuri, D. K. Basu, A novel

framework for automatic sorting of postal documents with multi-script

address blocks, Pattern Recognition 43 (10) (2010) 3507 – 3521.

[33] G. Zhong, M. Cheriet, Tensor representation learning based image patch

analysis for text identiﬁcation and recognition, Pattern Recognition

48 (4) (2015) 1211 – 1224.

[34] N. Sharma, S. Chanda, U. Pal, M. Blumenstein, Word-wise script iden-

tiﬁcation from video frames, in: Document Analysis and Recognition

(ICDAR), 2013 12th International Conference on, IEEE, 2013, pp. 867–

871.

[35] T. Q. Phan, P. Shivakumara, Z. Ding, S. Lu, C. L. Tan, Video script

identiﬁcation based on text lines, in: Document Analysis and Recog-

39

nition (ICDAR), 2011 International Conference on, IEEE, 2011, pp.

1240–1244.

[36] P. Shivakumara, N. Sharma, U. Pal, M. Blumenstein, C. L. Tan,

Gradient-angular-features for word-wise video script identiﬁcation, in:

2014 22nd International Conference on Pattern Recognition (ICPR),

IEEE, 2014, pp. 3098–3103.

[37] N. Sharma, R. Mandal, R. Sharma, U. Pal, M. Blumenstein, Bag-of-

visual words for word-wise video script identiﬁcation: A study, in: Neu-

ral Networks (IJCNN), 2015 International Joint Conference on, IEEE,

2015, pp. 1–7.

[38] N. Sharma, R. Mandal, R. Sharma, U. Pal, M. Blumenstein, Icdar2015

competition on video script identiﬁcation (cvsi 2015), in: Document

Analysis and Recognition (ICDAR), 2015 13th International Conference

on, IEEE, 2015, pp. 1196–1200.

[39] M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, A. Baskurt, Sequential

deep learning for human action recognition, in: International Workshop

on Human Behavior Understanding, Springer, 2011, pp. 29–39.

[40] S. Ji, W. Xu, M. Yang, K. Yu, 3d convolutional neural networks for

human action recognition, IEEE transactions on pattern analysis and

machine intelligence 35 (1) (2013) 221–231.

[41] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, L. Fei-

Fei, Large-scale video classiﬁcation with convolutional neural networks,

40

in: Proceedings of the IEEE conference on Computer Vision and Pattern

Recognition, 2014, pp. 1725–1732.

[42] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classiﬁcation with

deep convolutional neural networks, in: Advances in neural information

processing systems, 2012, pp. 1097–1105.

[43] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun, C. Moore,

E. S¨ackinger, R. Shah, Signature veriﬁcation using a siamese time de-

lay neural network, International Journal of Pattern Recognition and

Artiﬁcial Intelligence 7 (04) (1993) 669–688.

[44] Y.-F. Pan, X. Hou, C.-L. Liu, Text localization in natural scene images

based on conditional random ﬁeld, in: Document Analysis and Recog-

nition, 2009. ICDAR’09. 10th International Conference on, IEEE, 2009,

pp. 6–10.

[45] C. Yao, X. Bai, W. Liu, Y. Ma, Z. Tu, Detecting texts of arbitrary orien-

tations in natural images, in: Computer Vision and Pattern Recognition

(CVPR), 2012 IEEE Conference on, IEEE, 2012, pp. 1083–1090.

[46] T. E. de Campos, B. R. Babu, M. Varma, Character recognition in

natural images., in: VISAPP (2), 2009, pp. 273–280.

[47] D. Kumar, M. Prasad, A. Ramakrishnan, Multi-script robust reading

competition in icdar 2013,

in: Proceedings of the 4th International

Workshop on Multilingual OCR, ACM, 2013, p. 14.

[48] S. Lee, M. S. Cho, K. Jung, J. H. Kim, Scene text extraction with edge

41

constraint and text collinearity, in: Pattern Recognition (ICPR), 2010

20th International Conference on, IEEE, 2010, pp. 3983–3986.

[49] D. Karatzas, S. Robles, L. Gomez, An on-line platform for ground

truthing and performance evaluation of text extraction systems, in: Doc-

ument Analysis Systems (DAS), 2014 11th IAPR International Work-

shop on, IEEE, 2014, pp. 242–246.

[50] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,

S. Guadarrama, T. Darrell, Caﬀe: Convolutional architecture for fast

feature embedding, in: Proceedings of the ACM International Confer-

ence on Multimedia, ACM, 2014, pp. 675–678.

[51] K. Jarrett, K. Kavukcuoglu, M. Ranzato, Y. LeCun, What is the best

multi-stage architecture for object recognition?, in: Computer Vision,

2009 IEEE 12th International Conference on, IEEE, 2009, pp. 2146–

2153.

[52] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,

Dropout: A simple way to prevent neural networks from overﬁtting, The

Journal of Machine Learning Research 15 (1) (2014) 1929–1958.

[53] D. G. Lowe, Object recognition from local scale-invariant features, in:

Computer vision, 1999. The proceedings of the seventh IEEE interna-

tional conference on, Vol. 2, Ieee, 1999, pp. 1150–1157.

[54] A. Vedaldi, B. Fulkerson, VLFeat: An open and portable library of

computer vision algorithms, http://www.vlfeat.org/ (2008).

42

[55] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, C.-J. Lin, LIBLIN-

EAR: A library for large linear classiﬁcation, The Journal of Machine

Learning Research 9 (2008) 1871–1874.

[56] A. Nicolaou, A. D. Bagdanov, M. Liwicki, D. Karatzas, Sparse radial

sampling lbp for writer identiﬁcation, in: Document Analysis and Recog-

nition (ICDAR), 2015 13th International Conference on, IEEE, 2015, pp.

716–720.

[57] Q. McNemar, Note on the sampling error of the diﬀerence between corre-

lated proportions or percentages, Psychometrika 12 (2) (1947) 153–157.

[58] L. Gomez, D. Karatzas, A fast hierarchical method for multi-

script and arbitrary oriented scene text extraction, arXiv preprint

arXiv:1407.7504.

[59] J. Matas, O. Chum, M. Urban, T. Pajdla, Robust wide-baseline stereo

from maximally stable extremal regions, Image and vision computing

22 (10) (2004) 761–767.

[60] M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams, J. Winn,

A. Zisserman, The pascal visual object classes challenge: A retrospec-

tive, International Journal of Computer Vision 111 (1) (2015) 98–136.

[61] D. Karatzas, L. Gomez-Bigorda, A. Nicolaou, S. Ghosh, A. Bagdanov,

M. Iwamura, J. Matas, L. Neumann, V. R. Chandrasekhar, S. Lu, et al.,

Icdar 2015 competition on robust reading, in: Document Analysis and

Recognition (ICDAR), 2015 13th International Conference on, IEEE,

2015, pp. 1156–1160.

43

[62] R. Smith, An overview of the tesseract ocr engine, in: icdar, IEEE, 2007,

pp. 629–633.

[63] S. Milyaev, O. Barinova, T. Novikova, P. Kohli, V. Lempitsky, Image

binarization for end-to-end text understanding in natural images, in:

Document Analysis and Recognition (ICDAR), 2013 12th International

Conference on, IEEE, 2013, pp. 128–132.

[64] L. G´omez, D. Karatzas, Scene text recognition: No country for old men?,

in: Computer Vision-ACCV 2014 Workshops, Springer, 2014, pp. 157–

168.

[65] S. Milyaev, O. Barinova, T. Novikova, P. Kohli, V. Lempitsky, Fast and

accurate scene text understanding with image binarization and oﬀ-the-

shelf ocr, International Journal on Document Analysis and Recognition

(IJDAR) 18 (2) (2015) 169–182.

[66] K. Wang, B. Babenko, S. Belongie, End-to-end scene text recognition,

in: Computer Vision (ICCV), 2011 IEEE International Conference on,

IEEE, 2011, pp. 1457–1464.

[67] D. Karatzas, F. Shafait, S. Uchida, M. Iwamura, L. Gomez i Bigorda,

S. Robles Mestre, J. Mas, D. Fernandez Mota, J. Almazan Almazan, L.-

P. de las Heras, Icdar 2013 robust reading competition, in: Document

Analysis and Recognition (ICDAR), 2013 12th International Conference

on, IEEE, 2013, pp. 1484–1493.

[68] S. Yousﬁ, S.-A. Berrani, C. Garcia, Alif: A dataset for arabic embedded

text recognition in tv broadcast, in: Document Analysis and Recognition

44

(ICDAR), 2015 13th International Conference on, IEEE, 2015, pp. 1221–

1225.

45

(a)

(b)

Figure 4: The original scene text images (a) are converted to greyscale and

resized to a ﬁxed height (b) in order to extract small local patches with a

dense sampling strategy (c).

conv1 - pool1conv2 - pool2 conv3 - pool3

conv4

fc5 fc6 fc7

Figure 5: Network architecture of the CNN trained to classify individual

image patches. The network has three convolutional+pooling stages followed

by an extra convolution and three fully connected layers.

(c)

46

x1

x2

x3

Elementwise (cid:80)

xN

Figure 6: An Ensemble of Conjoined Nets consist in a set of identical net-

works that are joined at their outputs in order to provide a unique classiﬁ-

cation response.

Figure 7: Validation accuracy for various number of networks N in the en-

semble of conjoined networks model.

47

Figure 8: Confusion matrices with per class classiﬁcation accuracy of our

method in SIW-13, MLe2e, and CVSI datasets.

Figure 9: Overlaid-text samples (top row) variability and clutter is rather

limited compared with that found in the scene text images (bottom row).

Figure 10: Cropped words in the CVSI dataset belong to very long sentences

of overlay text in videos. It is common to ﬁnd several samples sharing exactly

the same font and background both in the train (top row) and test (bottom

row) sets.

Figure 11: Examples of challenging text images that are correctly classiﬁed

by our ECN method but not with the Simple CNN approach.

48

Figure 12: A selection of misclassiﬁed samples by our method: low contrast

images, rare font types, degraded text, letters mixed with numerals, etc.

Figure 13: Classiﬁcation accuracy of the CNN trained with ensembles of

conjoined nets (top) and number of images (bottom) as a function of the

image width on SIW-13 test images.

Figure 14: Classiﬁcation error of our method when applied to variable length

cropped regions of SIW-13 images, up to the minimum size possible (40 × 40

pixels).

49

Figure 15: End-to-end recognition of text from images containing textual

information in diﬀerent scripts/languages.

50

7
1
0
2
 
b
e
F
 
1
 
 
]

V
C
.
s
c
[
 
 
2
v
0
8
4
7
0
.
2
0
6
1
:
v
i
X
r
a

Improving patch-based scene text script identiﬁcation
with ensembles of conjoined networks

Lluis Gomez, Anguelos Nicolaou, Dimosthenis Karatzas

Computer Vision Center, Universitat Autonoma de Barcelona. Ediﬁci O, Campus UAB,
08193 Bellaterra (Cerdanyola) Barcelona, Spain. E-mail: lgomez,dimos@cvc.uab.cat

Abstract

This paper focuses on the problem of script identiﬁcation in scene text im-

ages. Facing this problem with state of the art CNN classiﬁers is not straight-

forward, as they fail to address a key characteristic of scene text instances:

their extremely variable aspect ratio. Instead of resizing input images to a

ﬁxed aspect ratio as in the typical use of holistic CNN classiﬁers, we propose

here a patch-based classiﬁcation framework in order to preserve discrimina-

tive parts of the image that are characteristic of its class.

We describe a novel method based on the use of ensembles of conjoined

networks to jointly learn discriminative stroke-parts representations and their

relative importance in a patch-based classiﬁcation scheme. Our experiments

with this learning procedure demonstrate state-of-the-art results in two pub-

lic script identiﬁcation datasets.

In addition, we propose a new public benchmark dataset for the evalu-

ation of multi-lingual scene text end-to-end reading systems. Experiments

done in this dataset demonstrate the key role of script identiﬁcation in a

complete end-to-end system that combines our script identiﬁcation method

with a previously published text detector and an oﬀ-the-shelf OCR engine.

Preprint submitted to Pattern Recognition

February 2, 2017

Keywords:

script identiﬁcation, scene text understanding, multi-language

OCR, convolutional neural networks, ensemble of conjoined networks

1. Introduction

Script and language identiﬁcation are important steps in modern OCR

systems designed for multi-language environments. Since text recognition al-

gorithms are language-dependent, detecting the script and language at hand

allows selecting the correct language model to employ [1]. While script iden-

tiﬁcation has been widely studied in document analysis [2, 3], it remains

an almost unexplored problem for scene text. In contrast to document im-

ages, scene text presents a set of speciﬁc challenges, stemming from the high

variability in terms of perspective distortion, physical appearance, variable

illumination and typeface design. At the same time, scene text comprises

typically a few words, contrary to longer text passages available in document

images.

Current end-to-end systems for scene text reading [4, 5, 6] assume single

script and language inputs given beforehand, i.e. provided by the user, or

inferred from available meta-data. The unconstrained text understanding

problem for large collections of images from unknown sources has not been

considered up to very recently [7, 8, 9, 10, 11]. While there exists some previ-

ous research in script identiﬁcation of text over complex backgrounds [12, 13],

such methods have been so far limited to video overlaid-text, which presents

in general diﬀerent challenges than scene text.

This paper addresses the problem of script identiﬁcation in natural scene

images, paving the road towards true multi-lingual end-to-end scene text

2

Figure 1: Collections of images from unknown sources may contain textual

information in diﬀerent scripts.

understanding. Multi-script text exhibits high intra-class variability (words

written in the same script vary a lot) and high inter-class similarity (certain

scripts resemble each other). Examining text samples from diﬀerent scripts,

it is clear that some stroke-parts are quite discriminative, whereas others

can be trivially ignored as they occur in multiple scripts. The ability to

distinguish these relevant stroke-parts can be leveraged for recognising the

corresponding script. Figure 2 shows an example of this idea.

Figure 2: (best viewed in color) Certain stroke-parts (in green) are discrimi-

native for the identiﬁcation of a particular script (left), while others (in red)

can be trivially ignored because are frequent in other classes (right).

The use of state of the art CNN classiﬁers for script identiﬁcation is not

straightforward, as they fail to address a key characteristic of scene text

instances: their extremely variable aspect ratio. As can be seen in Figure 3,

scene text images may span from single characters to long text sentences, and

thus resizing images to a ﬁxed aspect ratio, as in the typical use of holistic

CNN classiﬁers, will deteriorate discriminative parts of the image that are

characteristic of its class. The key intuition behind the proposed method is

3

that in order to retain the discriminative power of stroke parts we must rely

in powerful local feature representations and use them within a patch-based

classiﬁer. In other words, while holistic CNNs have superseded patch-based

methods for image classiﬁcation, we claim that patch-based classiﬁers can

still be essential in tasks where image shrinkage is not feasible.

Figure 3: Scene text images with the larger/smaller aspect ratio available in

three diﬀerent datasets: MLe2e(left), SIW-13(center), and CVSI(right).

In previously published work [10] we have presented a method combining

convolutional features, extracted by sliding a window with a single layer

Convolutional Neural Network (CNN) [14], and the Naive-Bayes Nearest

Neighbour (NBNN) classiﬁer [15] with promising results. In this paper we

demonstrate far superior performance by extending our previous work in two

diﬀerent ways: First, we use deep CNN architectures in order to learn more

discriminative representations for the individual image patches; Second, we

propose a novel learning methodology to jointly learn the patch representa-

tions and their importance (contribution) in a global image to class proba-

bilistic measure. For this, we train our CNN using an Ensemble of Conjoined

Networks and a loss function that takes into account the global classiﬁcation

error for a group of N patches instead of looking only into a single image

patch. Thus, at training time our network is presented with a group of N

patches sharing the same class label and produces a single probability distri-

bution over the classes for all them. This way we model the goal for which

the network is trained, not only to learn good local patch representations,

4

but also to learn their relative importance in the global image classiﬁcation

task.

Experiments performed over two public datasets for scene text classiﬁca-

tion demonstrate state-of-the-art results. In particular we are able to reduce

classiﬁcation error by 5 percentage points in the SIW-13 dataset. We also

introduce a new benchmark dataset, namely the MLe2e dataset, for the eval-

uation of scene text end-to-end reading systems and all intermediate stages

such as text detection, script identiﬁcation and text recognition. The dataset

contains a total of 711 scene images, and 1821 text line instances, covering

four diﬀerent scripts (Latin, Chinese, Kannada, and Hangul) and a large

variability of scene text samples.

2. Related Work

Script identiﬁcation is a well studied problem in document image analy-

sis. Gosh et al. [2] has published a compehensive review of methods dealing

with this problem. They identify two broad categories of methods: structure-

based and visual appearance-based techniques. In the ﬁrst category, Spitz

and Ozaki [16, 17] propose the use of the vertical distribution of upward

concavities in connected components and their optical density for page-wise

script identiﬁcation. Lee et al. [18], and Waked et al. [19] among others build

on top of Spitz seminal work by incorporating additional connected compo-

nent based features. Similarly, Chaudhuri et al. [20] use the projection pro-

ﬁle, statistical and topological features, and stroke features for classiﬁcation

of text lines in printed documents. Hochberg et al. [21] propose the use of

cluster-based templates to identify unique characteristic shapes. A method

5

that is similar in spirit with the one presented in this paper, while requiring

textual symbols to be precisely segmented to generate the templates.

Regarding segmentation-free methods based on visual appearance of scripts,

i.e. not directly analyzing the character patterns in the document, Wood et

al. [22] experimented with the use of vertical and horizontal projection pro-

ﬁles of full-page document images. More recent methods in this category have

used texture features from Gabor ﬁlters analysis [23, 24, 25] or Local Binary

Patterns [26]. Neural networks have been also used for segmentation-free

script identiﬁcation et al. [27, 28] without the use of hand-crafted features.

All the methods discussed above are designed speciﬁcally with printed

document images in mind. Structure-based methods require text connected

components to be precisely segmented from the image, while visual appearance-

based techniques are known to work better in bilevel text. Moreover, some of

these methods require large blocks of text in order to obtain suﬃcient infor-

mation and thus are not well suited for scene text which typically comprises

a few words.

Contrary to the case of printed document images, research in script iden-

tiﬁcation on non traditional paper layouts is more scarce, and has been

mainly dedicated to handwritten text [29, 30, 31, 32, 33], and video overlaid-

text [12, 34, 35, 36, 13] until very recently. Gllavatta et al. [12], in the ﬁrst

work dealing with video text script identiﬁcation, proposed a method using

the wavelet transform to detect edges in overlaid-text images. Then, they

extract a set of low-level edge features, and make use of a K-NN classiﬁer.

Sharma et al. [34] have explored the use of traditional document analysis

techniques for video overlaid-text script identiﬁcation at word level. They

6

analyze three sets of features: Zernike moments, Gabor ﬁlters, and a set

of hand-crafted gradient features previously used for handwritten character

recognition. They propose a number of pre-processing algorithms to over-

come the inherent challenges of video overlaid-text.

In their experiments

the combination of super resolution, gradient features, and a SVM classiﬁer

perform signiﬁcantly better that the other combinations.

Phan et al. [35] propose a method for combined detection of video text

overlay and script identiﬁcation. They propose the extraction of upper and

lower extreme points for each connected component of Canny edges of text

lines and analyse their smoothness and cursiveness.

Shivakumara et al. [36, 13] rely on skeletonization of the dominant gra-

dients They analyze the angular curvatures [36] of skeleton components, and

the spatial/structural [13] distribution of their end, joint, and intersection

points to extract a set of hand-crafted features. For classiﬁcation they build

a set of feature templates from train data, and use the Nearest Neighbor rule

for classifying scripts at word [36] or text block [13] level.

As said before, all these methods have been designed (and evaluated)

speciﬁcally for video overlaid-text, which presents in general diﬀerent chal-

lenges than scene text. Concretely, they mainly rely in accurate edge detec-

tion of text components and this is not always feasible in scene text.

More recently, Sharma et al. [37] explored the use of Bag-of-Visual Words

based techniques for word-wise script identiﬁcation in video-overlaid text.

They use Bag-Of-Features (BoF) and Spatial Pyramid Matching (SPM) with

patch based SIFT descriptors and found that the SPM pipeline outperforms

traditional script identiﬁcation techniques involving gradient based features

7

(e.g. HoG) and texture based features (e.g. LBP).

In 2015, the ICDAR Competition on Video Script Identiﬁcation (CVSI-

2015) [38] challenged the document analysis community with a new compet-

itive benchmark dataset. With images extracted from diﬀerent video sources

(news, sports etc.) covering mainly overlaid-text, but also a few instances of

scene text. The top performing methods in the competition where all based

in Convolutional Neural Networks, showing a clear diﬀerence in overall ac-

curacy over pipelines using hand-crafted features (e.g. LBP and/or HoG).

The ﬁrst dataset for script identiﬁcation in real scene text images was pro-

vided by Shi et al.in [7], where the authors propose the Multi-stage Spatially-

sensitive Pooling Network (MSPN) method. The MSPN network overcomes

the limitation of having a ﬁxed size input in traditional Convolutional Neural

Networks by pooling along each row of the intermediate layers’ outputs by

taking the maximum (or average) value in each row. Their method is ex-

tended in [8] by combining deep features and mid-level representations into a

globally trainable deep model. They extract local deep features at every layer

of the MSPN and describe images with a codebook-based encoding method

that can be used to ﬁne-tune the CNN weights.

Nicolaou et al. [9] has presented a method based on texture features

producing state of the art results in script identiﬁcation for both scene or

overlaid text images. They rely in hand-crafted texture features, a variant

of LBP, and a deep Multi Layer Perceptron to learn a metric space in which

they perform K-NN classiﬁcation.

In our previous work [10] we have proposed a patch-based method for

script identiﬁcation in scene text images. We used Convolutional features,

8

extracted from small image patches, and the Naive-Bayes Nearest Neighbour

classiﬁer (NBNN). We also presented a simple weighting strategy in order to

discover the most discriminative parts (or templates patches) per class in a

ﬁne-grained classiﬁcation approach.

In this paper we build upon our previous work [10] by extending it in two

ways: On one side, we make use of a much deeper Convolutional Neural Net-

work model. On the other hand, we replace the weighted NBNN classiﬁer by

a patch-based classiﬁcation rule that can be integrated in the CNN training

process by using an Ensemble of Conjoined Networks. This way, our CNN

model is able to learn at the same time expressive representations for image

patches and their relative contribution to the patch-based classiﬁcation rule.

From all reviewed methods the one proposed here is the only one based

in a patch-based classiﬁcation framework. Our intuition is that in cases

where holistic CNN models are not directly applicable, as in the case of text

images (because of their highly variable aspect ratios), the contribution of

rich parts descriptors without any deterioration (either by image distortion

or by descriptor quantization) is essential for correct image classiﬁcation.

In this sense our method is related with some CNN extensions that have

been proposed for video classiﬁcation. Unlike still images which can be

cropped and rescaled to a ﬁxed size, video sequences have a variable temporal

dimension and cannot be directly processed with a ﬁxed-size architecture. In

this context, 3D Convolutional Neural Networks [39, 40] have been proposed

to leverage the motion information encoded in multiple contiguous frames.

Basically the idea is to feed the CNN with a stack of a ﬁxed number of

consecutive frames and perform convolutions in both time and space dimen-

9

sions. Still these methods require a ﬁxed size input and thus they must be

applied several times through the whole sequence to obtain a chain of out-

puts that are then averaged [40] or fed into an Recurrent Neural Network [39]

to provide a ﬁnal decision. Karpathy et al. [41] also treat videos as bags of

short ﬁxed-length clips, but they investigate the use of diﬀerent temporal

connectivity patterns (early fusion, late fusion and slow fusion). To produce

predictions for an entire video they randomly sample 20 clips and take the

average of the network class predictions. While we share with these methods

the high-level goal of learning CNN weights from groups of stacked patches

(or frames) there are two key diﬀerences in the way we build our framework:

(1) the groups of patches that are fed into the network at training time are

randomly sampled and do not follow any particular order; and (2) at test

time we decouple the network to densely evaluate single patches and aver-

age their outputs. In other words, while in stacked-frame CNNs for video

recognition having an ordered sequence of input patches is crucial to learn

spatio-temporal features, our design aims to learn which are the most dis-

criminative patches in the input stack, independently of their relative spatial

arrangement.

In the experimental section we compare our method with some of the al-

gorithms reviewed in this section and demonstrate its superiority. Concretely

our approach improves the state-of-the-art in the SIW-13 [8] dataset for scene

text script classiﬁcation by a large margin of 5 percentage points, while per-

forms competitively in the CVSI-2015 [38] video overlaid-text dataset.

10

3. Patch-based classiﬁcation with Ensembles of Conjoined Net-

works

In our patch-based classiﬁcation method an input image is represented as

a collection of local descriptors, from patches extracted following a certain

sampling strategy. Those local features are then fed into a global classiﬁer

rule, that makes a decision for the input image.

3.1. Convolutional Neural Network for image-patch classiﬁcation

Given an input scene text image (i.e. a pre-segmented word or text line)

we ﬁrst resize it to a ﬁxed height of 40 pixels, but retaining its original

aspect ratio. Since scene text can appear in any possible combination of

foreground and background colors, we pre-process the image by converting it

into grayscale and centering pixel values. Then, we densely extract patches

at two diﬀerent scales, 32×32 and 40×40, by sliding a window with a step of

8 pixels. The particular values of these two window scales and step size was

found by cross-validation optimization as explained in section 4.2, and its

choice can be justiﬁed as follows: the 40 × 40 patch, covering the full height

of the resized image, is a natural choice in our system because it provides

the largest squared region we can crop; the 32 × 32 patches are conceived

for better scale invariance of the CNN model, similarly as the random crops

typically used for data augmentation in CNN-based image classiﬁcation [42].

Figure 4 shows the patches extracted from a given example image. This way

we build a large dataset of image patches that take the same label as the

image they were extracted from. With this dataset of patches we train a

CNN classiﬁer for the task of individual image patch classiﬁcation.

11

We use a Deep Convolutional Neural Network to build the expressive

image patch representations needed in our method. For the design of our

network we start from the CNN architecture proposed in [7] as it is known

to work well for script identiﬁcation. We then iteratively do an exhaustive

search to optimize by cross-validation the following CNN hyper-parameters:

number of convolutional and fully connected layers, number of ﬁlters per

layer, kernel sizes, and feature map normalisation schemes. The CNN archi-

tecture providing better performance in our experiments is shown in Figure 5.

Our CNN consists in three convolutional+pooling stages followed by an ex-

tra convolution and three fully connected layers. Details about the speciﬁc

conﬁguration and parameters are given in section 4.2.

At testing time, given a query scene text image the trained CNN model

is applied to image patches following the same sampling strategy described

before. Then, the individual CNN responses for each image patch can be fed

into the global classiﬁcation rule in order to make a single labeling decision

for the query image.

3.2. Training with an Ensemble of Conjoined Networks

Since the output of the CNN for an individual image patch is a probability

distribution over class labels, a simple global decision rule would be just to

average the responses of the CNN for all patches in a given query image:

y(I) =

CN N (xi)

1
nI

nI(cid:88)

i=1

(1)

where an image I takes the label with more probability in the averaged

softmax responses (y(I)) of their nI individual patches {x1, ..., xnI } outputs

12

on the CNN.

The problem with this global classiﬁcation rule is that the CNN weights

have been trained to solve a problem (individual patch classiﬁcation) that is

diﬀerent from the ﬁnal goal (i.e. classifying the whole query image). Besides,

it is based in a simplistic voting strategy for which all patches are assumed

to weight equally, i.e. no patches are more or less discriminative than others.

To overcome this we propose the use of an Ensemble of Conjoined Nets in

order to train the CNN for a task that resembles more the ﬁnal classiﬁcation

goal.

An Ensemble of Conjoined Nets (ECN), depicted in Figure 6, consists in

a set of identical networks that are joined at their outputs in order to provide

a unique classiﬁcation response. At training time the ECN is presented with

a set of N image patches extracted from the same image, thus sharing the

same label, and produces a single output for all them. Thus, to train an

ECN we must build a new training dataset where each sample consists in a

set of N patches with the same label (extracted from the same image).

ECNs take inspiration from Siamese Networks [43] but, instead of trying

to learn a metric space with a distance-based loss function, the individual

networks in the ECN are joined at their last fully connected layer (fc7 in

our case), which has the same number of neurons as the number of classes,

with a simple element-wise sum operation and thus we can use the standard

cross-entropy classiﬁcation loss. This way, the cross-entropy classiﬁcation

loss function of the ECN can be written in terms of the N individual patch

responses as follows:

13

E =

log(ˆpm,lm),

−1
M

M
(cid:88)

m=1

(cid:34) K
(cid:88)

k(cid:48)=1

N
(cid:88)

n=1

N
(cid:88)

n=1

ˆpm,k = exp(

xmnk)/

exp(

xmnk(cid:48))

(cid:35)

(2)

where M is the number of input samples in a mini-batch, ˆpm is the prob-

ability distribution over classes provided by the softmax function, lm is the

label of the m’th sample, N is the number of conjoined networks in the en-

semble, K is the number of classes, and xmnk ∈ [−∞, +∞] indicates the

response (score) of the k’th neuron in the n’th network for the m’th sample.

As can be appreciated in equation 2, in an ECN network a single input

patch contributes to the backpropagation error in terms of a global goal func-

tion for which it is not the only patch responsible. For example, even when

a single patch is correctly scored in the last fully connected layer it may be

penalized, and induced to produce a larger activation, if the other patches in

its same sample contribute to a wrong classiﬁcation at the ensemble output.

At test time, the CNN model trained in this way is applied to all image

patches in the query image and the global classiﬁcation rule is deﬁned as:

y(I) =

CN Nf c7(xi)

(3)

where an image I takes the label with the highest score in the sum (y(I))

of the fc7 layer responses of the nI individual patches {x1, ..., xnI }. This is

the same as in Equation 1 but using the fc7 layer responses instead of the

output softmax responses of the CNN.

Notice that still the task for which the ECN network has been trained is

nI(cid:88)

i=1

14

not exactly the same deﬁned by this global classiﬁcation rule, as the number

of patches nI is variable for each image and usually diﬀerent than the number

of conjoined networks N . However, it certainly resembles more the true

ﬁnal classiﬁcation goal. The number of conjoined networks N is an hyper-

parameter of the method that is largely dependent on the task to be solved

and is discussed in the experimental section.

4. Experiments

All reported experiments were conducted over three datasets, namely the

Video Script Identiﬁcation Competition (CVSI-2015) dataset1, the SIW-13

dataset2, and the MLe2e dataset3.

The CVSI-2015 [38] dataset comprises pre-segmented words in ten scripts:

English, Hindi, Bengali, Oriya, Gujrathi, Punjabi, Kannada, Tamil, Telegu,

and Arabic. The dataset contains about 1000 words for each script and is

divided into three parts: a training set ( 60% of the total images), a validation

set (10%), and a test set (30%). Text is extracted from various video sources

(news, sports etc.) and, while it contains a few instances of scene text, it

covers mainly overlay video text.

The SIW-13 datset [8] comprises 16291 pre-segmented text lines in thir-

teen scripts: Arabic, Cambodian, Chinese, English, Greek, Hebrew, Japanese,

Kannada, Korean, Mongolian, Russian, Thai, and Tibetan. The test set con-

tains 500 text lines for each script, 6500 in total, and all the other images

1http://www.ict.griffith.edu.au/cvsi2015/
2http://mc.eistar.net/~xbai/mspnProjectPage/
3http://github.com/lluisgomez/script_identification/

15

are provided for training. In this case, text was extracted from natural scene

images from Google Street View.

4.1. The MLe2e dataset

This paper introduces the ﬁrst dataset available up to date for the evalu-

ation of multi-lingual scene text end-to-end reading systems and all interme-

diate stages: text detection, script identiﬁcation, and text recognition. The

Multi-Language end-to-end (MLe2e) dataset has been harvested from vari-

ous existing scene text datasets for which the images and ground-truth have

been revised in order to make them homogeneous. The original images come

from the following datasets: Multilanguage(ML) [44] and MSRA-TD500 [45]

contribute Latin and Chinese text samples, Chars74K [46] and MSRRC [47]

contribute Latin and Kannada samples, and KAIST [48] contributes Latin

and Hangul samples.

In order to provide a homogeneous dataset, all images have been resized

proportionally to ﬁt in 640 × 480 pixels, which is the default image size

of the KAIST dataset. Moreover, the groundtruth has been revised to en-

sure a common text line annotation level [49]. During this process human

annotators were asked to review all resized images, adding the script class la-

bels and text transcriptions to the groundtruth, and checking for annotation

consistency: discarding images with unknown scripts or where all text is un-

readable (this may happen because images were resized); joining individual

word annotations into text line level annotations; discarding images where

correct text line segmentation is not clear or cannot be established, and im-

ages where a bounding box annotation contains more than one script (this

happens very rarely e.g.

in trademarks or logos) or where more than half

16

of the bounding box is background (this may happen with heavily slanted

or curved tex). Arabic numerals (0, .., 9), widely used in combination with

many (if not all) scripts, are labeled as follows. A text line containing text

and Arabic numerals is labeled as the script of the text it contains, while a

text line containing only Arabic numerals is labeled as Latin.

The MLe2e dataset contains a total of 711 scene images covering four

diﬀerent scripts (Latin, Chinese, Kannada, and Hangul) and a large vari-

ability of scene text samples. The dataset is split into a train and a test

set with 450 and 261 images respectively. The split was done randomly, but

in a way that the test set contains a balanced number of instances of each

class (aprox. 160 text lines samples of each script), leaving the rest of the

images for the train set (which is not balanced by default). The dataset is

suitable for evaluating various typical stages of end-to-end pipelines, such as

multi-script text detection, joint detection and script identiﬁcation, end-to-

end multi-lingual recognition, and script identiﬁcation in pre-segmented text

lines. For the latter, the dataset also provides the cropped images with the

text lines corresponding to each data split: 1178 and 643 images in the train

and test set respectively.

While being a dataset that has been harvested from a mix of existing

datasets it is important to notice that building it has supposed an important

annotation eﬀort: since some of the original datasets did not provide text

transcriptions, and/or where annotated at diﬀerent granularity levels. More-

over, despite the fact that the number of languages in the dataset is rather

limited (four scripts) it is the ﬁrst public dataset that covers the evaluation

of all stages of multi-lingual end-to-end systems for scene text understanding

17

in natural scenes. We think this is an important contribution of this paper

and hope the dataset will be useful to other researchers in the community.

4.2. Implementation details

In this section we detail the architectures of the network models used in

this paper, as well as the diﬀerent hyper-parameter setups that can be used

to reproduce the results provided in following sections. In all our experiments

we have used the open source Caﬀe [50] framework for deep learning running

on commodity GPUs. Source code and compatible Caﬀe models are made

publicly available4.

We have performed exhaustive experiments by varying many of the pro-

posed methods parameters, training multiple models, and choosing the one

with best cross-validation performance on the SIW-13 training set. The fol-

lowing parameters were tuned in this procedure: the size and step of the

sliding window, the base learning rate, the number of convolutional and fully

connected layers, the number of nodes in all layers, the convolutional kernel

sizes, and the feature map normalisation schemes

This way, the best basic CNN model found for individual image patch

classiﬁcation is described in section 3.1 and Figure 5, and has the following

per layer conﬁguration:

• Input layer: single channel 32 × 32 image patch.

• conv1 layer: 96 ﬁlters with size 5 × 5. Stride=1, pad=0. Output size:

96 × 28 × 28.

• pool1 layer: kernel size=3, stride=2, pad=1. Otput size: 96 × 15 × 15.

4http://github.com/lluisgomez/script_identification/

18

• conv2 layer: 256 ﬁlters with size 3 × 3. Stride=1, pad=0. Output size:

• pool2 layer: kernel size=3, stride=2, pad=1. Otput size: 256 × 7 × 7.

• conv3 layer: 384 ﬁlters with size 3 × 3. Stride=1, pad=0. Output size:

256 × 13 × 13.

384 × 5 × 5.

• pool3 layer: kernel size=3, stride=2, pad=1. Otput size: 384 × 3 × 3.

• conv4 layer: 512 ﬁlters with size 1 × 1. Stride=1, pad=0. Output size:

512 × 3 × 3.

• fc5 layer: 4096 neurons.

• fc6 layer: 1024 neurons.

• fc7 layer: N neurons, where N is the number of classes.

• SoftMax layer: Output a probability distribution over the N class la-

bels.

The total number of parameters of the network is ≈ 24M for the N = 13

case in the SIW-13 dataset. All convolution and fully connected layers use

Rectiﬁed Linear Units (ReLU). In conv1 and conv2 layers we perform normal-

ization over input regions using Local Response Normalization (LRN) [51].

At training time, we use dropout [52] (with a 0.5 ratio) in fc5 and fc6 layers.

To train the basic network model we use Stochastic Gradient Descent

(SGD) with momentum and L2 regularization. We use mini-batches of 64

images. The base learning rate is set to 0.01 and is decreased by a factor of

×10 every 100k iterations. The momentum weight parameter is set to 0.9,

and the weight decay regularization parameter to 5 × 10−4.

When training for individual patch classiﬁcation, we build a dataset of

small patches extracted by dense sampling the original training set images,

19

as explained in section 3.1. Notice that this produces a large set of patch

samples, e.g. in the SIW-13 dataset the number of training samples is close to

half million. With these numbers the network converges after 250k iterations.

In the case of the Ensemble of Conjoined Networks the basic network

detailed above is replicated N times, and all replicas are tied at their fc7

outputs with an element-wise sum layer which is connected to a single output

SoftMax layer. All networks in the ECN share the same parameters values.

Training the ECN requires a dataset where each input sample is com-

posed by N image patches. We generate this dataset as follows: given an

input image we extract patches the same way as for the simple network, then

we generate random N −combinations of the image patches, allowing repeti-

tions if the number of patches is < N . Notice that this way the number of

samples can be increased up to very large-scale numbers because the number
(cid:1) when the number of patches in
of possible diﬀerent N −combinations is (cid:0)M

N

a given image M is larger that the number of conjoined nets N , which is

the usual case. This is an important aspect of ECNs, as the training dataset

generation process becomes a data augmentation technique in itself. We can

see this data augmentation process as generating new small text instances

that are composed from randomly chosen parts of their original generators.

However, it is obviously non-practical to use all possible combinations for

training; thus, in order to get a manageable number of samples, we have used

the simple rule of generating 2 × M samples per input, which for example in

the SIW-13 dataset would produce around one million samples.

In terms of computational training complexity, the ECN has an impor-

tant drawback compared to the simple network model: the number of com-

20

putations is multiplied by N in each forward pass, similarly the amount of

memory needed is linearly increased by N . To overcome this limitation, we

use a ﬁne-tuning approach to train ECNs. First, we train the simple network

model, and then we do ﬁne-tuning on the ECN parameters starting from the

values learned using the simple net. When ﬁne-tuning, we have found that

starting from a fully converged network in the single-patch classiﬁcation task

we reach a local minimum of the global task, thus providing zero loss in most

(if not all) the iterations and not allowing the network to learn anything new.

In order to avoid this local minima situation we start the ﬁne-tuning from

a non-converged network (more or less at about 90/95% of the attainable

individual patch classiﬁcation accuracy).

Using ﬁne-tuning with a base learning rate of 0.001 (decreasing ×10 every

10k iterations) the ECN converges much faster, in the order of 35k iterations.

All other learning parameters are set the same as in the simple network

training setup.

The number of nets N in the ensemble can be seen as an extra hyper-

parameter in the ECN learning algorithm. Intuitively a dataset with larger

text sequences would beneﬁt from larger N values, while on the contrary

in the extreme case of classifying small squared images (i.e. each image is

represented by a single patch) any value of N > 1 does not make sense. Since

our datasets contain text instances with variable length a possible procedure

to select the optimal value of N is by using a validation set. We have done

experiments in the SIW-13 dataset by dividing the provided train set and

keeping 10% for validation. Classiﬁcation accuracy on the validation set for

various N values are shown in Figure 7. As can be appreciated the positive

21

impact of training with an ensemble of networks is evident for small values

of N , and mostly saturated for values N > 9. In the following we use a value

of N = 10 for all the remaining experiments.

4.3. Script identiﬁcation in pre-segmented text lines

In this section we study the performance of the proposed method for

script identiﬁcation in pre-segmented text lines. Table 1 shows the overall

performance comparison of our method with the state-of-the-art in CVSI-

2015, SIW-13, and MLe2e datasets. Figure 8 shows the confusion matrices

for our method in all three datasets with detailed per class classiﬁcation

results.

In Table 1 we also provide comparison with three well known image

recognition pipelines using Scale Invariant Features [53] (SIFT) in three dif-

ferent encodings: Fisher Vectors, Vector of Locally Aggregated Descriptors

(VLAD), and Bag of Words (BoW); and a linear SVM classiﬁer. In all base-

lines we extract SIFT features at four diﬀerent scales in sliding window with

a step of 8 pixels. For the Fisher vectors we use a 256 visual words GMM, for

VLAD a 256 vector quantized visual words, and for BoW 2,048 vector quan-

tized visual words histograms. The step size and number of visual words were

set to similar values to our method when possible in order to oﬀer a fair eval-

uation. These three pipelines have been implemented with the VLFeat [54]

and liblinear [55] open source libraries. The entry “Sequence-based CNN”

in Table 1 corresponds to the results obtained with the early fusion design

proposed in [41] with a stack of 5 consecutive patches.

As shown in Table 1 the proposed method outperforms state of the art

and all baseline methods in the SIW-13 and MLe2e scene text datasets, while

22

Method

SIW-13 MLe2e CVSI

This work - Ensemble of Conjoined Nets

94.8

94.4

This work - Simple CNN (Avg.)

This work - Simple CNN (fc5+SVM)

Shi et al. [8]

HUST [7, 38]

Google [38]

Nicolaou et al. [9]

Gomez et al. [10]

CVC-2 [10, 38]

SRS-LBP + KNN [56]

C-DAC [38]

CUK [38]

97.2

96.7

96.9

94.3

96.69

98.91

98.18

93.1

93.6

-

-

-

-

-

-

91.12

97.91

88.16

96.0

82.71

94.20

84.66

74.06

88.63

94.11

90.19

93.92

86.45

84.38

89.80

93.62

92.8

93.4

89.4

88.0

83.7

76.9

-

-

-

-

-

90.7

89.2

83.4

88.9

Baseline SIFT + Fisher Vectors + SVM

Baseline SIFT + VLAD + SVM

Baseline SIFT + Bag of Words + SVM

Baseline Sequence-based CNN [41] (Early fusion)

Table 1: Overall classiﬁcation performance comparison with state-of-the-art

in three diﬀerent datasets: SIW-13 [8], MLe2e, and CVSI [38].

23

performing competitively in the case of CVSI video overlay text dataset. In

the SIW-13 dataset the proposed method signiﬁcantly outperforms the best

performing method known up to date by more than 4 percentual points.

The entry “Simple CNN (fc5+SVM)” (third row) in Table 1 corresponds

to the results obtained with a linear SVM classiﬁer by using features ex-

tracted from the “Simple CNN” network. For this experiment we represent

each image in the dataset with a ﬁxed length vector with the averaged out-

puts of all its patches in the fc5 layer of the network. Then we train a linear

SVM classiﬁer using cross-validation on the training set, and show the classi-

ﬁcation performance on the test set. Similar results (or slightly worse) have

been found for features extracted from other layers (fc7, fc6, conv4) and us-

ing other linear classiﬁers (e.g.

logistic regression). When compared with

the “Simple CNN” approach we appreciate that classiﬁcation performance

is better for this combination (fc5+SVM). This conﬁrms the intuition that

classiﬁcation performance can be improved by optimizing the combination

of the results for the individual patches. However, the performance of the

CNN trained with the ensemble of conjoined networks is still better. As men-

tioned earlier, the additional beneﬁt of our approach here is in the end-to-end

learning of both the visual features and the optimal combination scheme for

classiﬁcation.

The contribution of training with ensembles of conjoined nets is consistent

in all three evaluated datasets but more notable on SIW-13, as appreciated by

comparing the ﬁrst two rows of Table 1 which correspond to the nets trained

with the ensemble (ﬁrst row) and the simple model (second row). This com-

parison can be further strengthened by testing if the provided improvement is

24

statistically signiﬁcant. For this we use the within-subjects chi-squared test

(McNemar’s test) [57] to compare the predictive accuracy of the two models.

The obtained p-values on the SIW-13, MLe2e, and CVSI datasets are respec-

tively 1.4 × 10−16, 0.057, and 0.0026. In the case of the SIW-13 dataset the

p-value is way smaller than the assumed signiﬁcance threshold (α = 0.05),

thus we can reject the null-hypothesis that both models perform equally well

on this dataset and certify a statistically signiﬁcant improvement. On the

other hand we appreciate a marginal improvement on the other two datasets.

Our interpretation of the results on CVSI and MLe2e datasets in compar-

ison with the ones obtained on SIW-13 relates to its distinct nature. On one

hand the MLe2e dataset covers only four diﬀerent scripts (Latin, Chinese,

Kannada, and Hangul) for which the inter-class similarity does not represent

a real problem. On the other hand, the CVSI overlaid-text variability and

clutter is rather limited compared with that found in the scene text of MLe2e

and SIW-13. As can be appreciated in Figure 9 overlaid-text is usually bi-

level without much clutter. Figure 10 shows another important characteristic

of CVSI dataset: since cropped words in the dataset belong to very long sen-

tences of overlay text in videos, e.g. from rotating headlines, it is common to

ﬁnd a few dozens of samples sharing exactly the same font and background

both in the train and test sets. This particularity makes the ECN network

not really helpful in the case of CVSI, as the data augmentation by image

patches recombinations is somehow already implicit on the dataset.

Furthermore, the CVSI-2015 competition winner (Google) makes use of a

deep convolutional network but applies a binarization pre-processing to the

input images. In our opinion this binarization may not be a realistic pre-

25

processing in general for scene text images. As an example of this argument

one can easily see in Figure 9 that binarization of scene text instances is

not trivial as in overlay text. Similar justiﬁcation applies to other methods

performing better than ours in CVSI. In particular the LBP features used

in [9], as well as the patch-level whitening used in our previous work [10], may

potentially take advantage of the simpler, bi-level, nature of text instances

in CVSI dataset. It is important to notice here that these two algorithms,

corresponding to our previous works in script identiﬁcation, have close num-

bers to the Google ones in CVSI-2015 (see Table 1) but perform quite bad

in SIW-13.

As a conclusion of the experiments performed in this section we can say

that the improvement of training a patch-based CNN classiﬁer with an en-

semble of conjoined nets is especially appreciable in cases where we have a

large number of classes, with large inter-class similarity, and cluttered scene

images, as is the case of the challenging SIW-13 dataset. This demonstrates

our initial claim that a powerful script identiﬁcation method for scene text

images must be based in learning good local patch representations, and also

their relative importance in the global image classiﬁcation task. Figure 11

shows some examples of challenging text images that are correctly classiﬁed

by our method but not with the Simple CNN approach. Figure 12 shows a

set of missclassiﬁed images.

Finally, in Figure 13 we show the classiﬁcation accuracy of the CNN

trained with ensembles of conjoined nets as a function of the image width

on SIW-13 test images. We appreciate that the method is robust even for

small text images which contain a limited number of unique patches. Com-

26

putation time for our method is also dependent on the input image length

and ranges from 4ms.

in for the smaller images up to 23ms. for the larger

ones. The average computation time on the SIW-13 test set is of 13ms using

a commodity GPU. At test time computation is made eﬃcient by stacking

all patches of the input image in a single mini-batch.

4.4. Joint text detection and script identiﬁcation in scene images

In this experiment we evaluate the performance of a complete pipeline for

detection and script identiﬁcation in its joint ability to detect text lines in

natural scene images and properly recognizing their scripts. The key interest

of this experiment is to study the performance of the proposed script identi-

ﬁcation algorithm when realistic, non-perfect, text localisation is available.

Most text detection pipelines are trained explicitly for a speciﬁc script

(typically English) and generalise pretty badly to the multi-script scenario.

We have chosen to use here our previously published script-agnostic method [58],

which is designed for multi-script text detection and generalises well to any

script. The method detects character candidates using the Maximally Sta-

ble Extremal Regions (MSER) [59] algorithm, and builds diﬀerent hierar-

chies where the initial regions are grouped by agglomerative clustering, using

complementary similarity measures. In such hierarchies each node deﬁnes a

possible text hypothesis. Then, an eﬃcient classiﬁer, using incrementally

computable descriptors, is used to walk each hierarchy and select the nodes

with larger text-likelihood.

In this paper script identiﬁcation is performed at the text line level, be-

cause segmentation into words is largely script-dependent, and not meaning-

ful in Chinese/Korean scripts. Notice however that in some cases, by the

27

intrinsic nature of scene text, a text line provided by the text detection mod-

ule may correspond to a single word, so we must deal with a large variability

in the length of provided text lines. The experiments are performed over the

new MLe2e dataset.

For evaluation of the joint text detection and script identiﬁcation task in

the MLe2e dataset we propose the use of a simple two-stage evaluation frame-

work. First, localisation is assessed based on the Intersection-over-Union

(IoU) metric between detected and ground-truth regions, as commonly used

in object detection tasks [60] and the recent ICDAR 2015 Robust Read-

ing Competition5 [61]. Second, the predicted script is veriﬁed against the

ground-truth. A detected bounding box is thus considered correct if it has a

IoU > 0.5 with a bounding box in the ground-truth and the predicted script

is correct.

The localisation-only performance, corresponding to the ﬁrst stage of the

evaluation, yields an F-score of 0.63 (Precision of 0.57 and Recall of 0.69).

This deﬁnes the upper-bound for the joint task. The two stage evaluation,

including script identiﬁcation, of the proposed method compared with our

previous work is shown in Table 2.

Intuitively the proposed method for script identiﬁcation is eﬀective even

when the text region is badly localised, as long as part of the text area is

within the localised region. To support this argument we have performed an

additional experiment where our algorithm is applied to cropped regions from

pre-segmented text images. For this, we take the SIW-13 original images and

5http://rrc.cvc.uab.es

28

Method

Correct Wrong Missing Precision Recall F-score

This work - ECN

Gomez et al. [10]

395

364

376

407

245

278

0.51

0.47

0.62

0.56

0.57

0.52

Table 2: Text detection and script identiﬁcation performance in the MLe2e

dataset.

calculate the performance of our method when applied to cropped regions of

variable length, up to the minimum size possible (40 × 40 pixels). As can

be appreciated in Figure 14 the experiment demonstrates that the proposed

method is eﬀective even when small parts of the text lines are provided.

Such a behaviour is to be expected, due to the way our method treats local

information to decide on a script class. In the case of the pipeline for joint

detection and script identiﬁcation, this extends to regions that did not pass

the 0.5 IoU threshold, but had their script correctly identiﬁed. This opens

the possibility to make use of script identiﬁcation to inform and / or improve

the text localisation process. The information of the identiﬁed script can be

used to reﬁne the detections.

4.5. End-to-end multi-lingual recognition in scene images

In this section we evaluate the performance of a complete pipeline for

end-to-end multi-lingual recognition in scene images. For this, we combine

the pipeline used in the previous section with a well known oﬀ-the-shelf OCR

engine: the open source project Tesseract6 [62]. Similar pipelines [63, 64, 65]

6http://code.google.com/p/tesseract-ocr/

29

using oﬀ-the-shelf OCR engines have demonstrated state-of-the-art end-to-

end performance in English-only datasets up to very recently, provided that

the text detection module is able to produce good pixel-level segmentation

of text.

The setup of the OCR engine in our pipeline is minimal: given a text

detection hypothesis from the detection module we set the recognition lan-

guage to the one provided by the script identiﬁcation module, and we set the

OCR to interpret the input as a single text line. Apart from that we use the

default Tesseract parameters.

The recognition output is ﬁltered with a simple post-processing junk ﬁlter

in order to eliminate garbage recognitions, i.e. sequences of identical charac-

ters like ”IIii” that may appear as the result of trying to recognize repetitive

patterns in the scene. Concretely, we discard the words in which more than

half of their characters are recognized as one of ”i”, ”l”, ”I”, or other special

characters like: punctuation marks, quotes, exclamation, etc. We also reject

those detections for which the recognition conﬁdence provided by the OCR

engine is under a certain threshold.

The evaluation protocol is similar to the one used in other end-to-end

scene text recognition datasets [66, 61]. Ideally, in end-to-end word recogni-

tion, a given output word is considered correct if it overlaps more than 0.5

with a ground-truth word and all its characters are recognized correctly (case

sensitive). However, since in the case of the MLe2e dataset we are evaluating

text lines instead of single words, we relax a bit this correctness criteria by

allowing the OCR output to to make 1

8 character level errors. This relax-
ation is motivated by the fact that for a given test sentence with more than

30

Script identiﬁcation Correct Wrong Missing Precision Recall F-score

This work - ECN

Gomez et al. [10]

Tesseract

96

82

50

212

211

93

503

517

549

0.31

0.28

0.35

0.16

0.21

0.14

0.08

0.18

0.13

Table 3: End-to-end multi-lingual recognition performance in the MLe2e

dataset.

1
8.

test set.

8 characters (e.g. with two words) having only one character mistake may

still produce a partial understanding of the text (e.g. one of the words is

correct), and thus must not be penalized the same way as if all characters

are wrongly recognized. This way, a given output text line is considered cor-

rect if overlaps more than 0.5 with a ground-truth text line and their edit

distance divided by the number of characters of the largest is smaller than

Table 3 shows a comparison of the proposed end-to-end pipeline by using

diﬀerent script identiﬁcation modules: the method presented in this paper,

our previously published work, and Tesseract’s built-in alternative. Figure 15

shows the output of our full end-to-end pipeline for some images in the MLe2e

Tesseract method in Table 3 refers to the use of Tesseract’s own script es-

timation algorithm [1]. We have found that Tesseract’s algorithm is designed

to work with large corpses of text (e.g. full page documents) and does not

work well for the case of single text lines.

Results in Table 3 demonstrate the direct correlation between having

31

better script identiﬁcation rates and better end-to-end recognition results.

The ﬁnal multi-lingual recognition f-score obtained (0.21) is far from the

state-of-the art in end-to-end recognition systems designed for English-only

environments [4, 5, 6]. As a fair comparison, a very similar pipeline using

the Tesseract OCR engine [64] achieves an f-score of 0.40 in the ICDAR

English-only dataset. The lower performance obtained in MLe2e dataset

stems from a number of challenges that are speciﬁc to its multi-lingual nature.

For example, in some scripts (e.g. Chinese and Kannada) glyphs are many

times non single-body regions, composed by (or complemented with) small

strokes that in many cases are lost in the text segmentation stage. In such

cases having a bad pixel-level segmentation of text would make it practically

impossible for the OCR engine to produce a correct recognition.

Our pipeline results represent the ﬁrst reference result for multi-lingual

scene text recognition and a ﬁrst benchmark from which better systems can

be built, e.g. replacing the oﬀ-the-shelf OCR engine by other recognition

modules better suited for scene text imagery.

4.6. Cross-domain performance and confusion in single-language datasets

In this experiment we evaluate the cross-domain performance of learned

CNN weights from one dataset to the other. For example, we evaluate on

the MLe2e and CVSI test sets using the network trained with the SIw-13

train set, by measuring classiﬁcation accuracy only for their common script

classes: Arabic, English, and Kannada in CVSI; Chinese, English, Kannada,

and Korean in MLe2e. Finally, we evaluate the misclassiﬁcation error of our

method (trained in diﬀerent datasets) over two single-script datasets. For

this experiment we use the ICDAR2013 [67] and ALIF [68] datasets, which

32

provide cropped word images of English scene text and Arabic video overlaid

text respectively. Table 4 shows the results of these experiments.

Method

SIW-13 MLe2e CVSI

ICDAR ALIF

ECN CNN (SIW-13)

94.8

86.8

90.6

74.7

100

ECN CNN (MLe2e)

90.8

94.4

98.3

95.3

-

ECN CNN (CVSI)

42.3

43.5

97.2

65.2

91.8

Table 4: Cross-domain performance of our method measured by train-

ing/testing in diﬀerent datasets.

Notice that results in Table 4 are not directly comparable among rows

because each classiﬁer has been trained with a diﬀerent number of classes,

thus having diﬀerent rates for a random choice classiﬁcation. However, the

experiment serves as a validation of how good a given classiﬁer is in perform-

ing with data that is distinct in nature to the one used for training. In this

sense, the obtained results show a clear weakness when the model is trained

on the video overlaid text of CVSI and subsequently applied to scene text

images (SIW-13, MLe2e, and ICDAR). On the contrary, models trained on

scene text datasets are quite stable in other scene text data, as well as in

video overlaid text (CVSI and ALIF).

In fact, this is an expected result, because the domain of video overlay

text can be seen as a sub-domain of the scene text domain. Since the scene

text datasets are richer in text variability, e.g. in terms of perspective distor-

tion, physical appearance, variable illumination, and typeface designs, script

33

identiﬁcation on these datasets is a more diﬃcult problem, and their data

is more indicated if one wants to learn eﬀective cross-domain models. This

demonstrates that our method is able to learn discriminative stroke-part rep-

resentations that are not dataset-speciﬁc, and provides evidence to the claims

made in section 4.3 when interpreting the obtained results in CVSI dataset

comparing with other methods that may be more engineered to the speciﬁc

CVSI data but not generalizing well in scene text datasets.

5. Conclusion

A novel method for script identiﬁcation in natural scene images was pre-

sented. The method is based on the use of ensembles of conjoined convo-

lutional networks to jointly learn discriminative stroke-part representations

and their relative importance in a patch-based classiﬁcation scheme. Exper-

iments performed in three diﬀerent datasets exhibit state of the art accuracy

rates in comparison to a number of state-of-the-art methods, including the

participants in the CVSI-2015 competition and three standard image classi-

ﬁcation pipelines.

In addition, a new public benchmark dataset for the evaluation of all

stages of multi-lingual end-to-end scene text reading systems was introduced.

Our work demonstrates the viability of script identiﬁcation in natural

scene images, paving the road towards true multi-lingual end-to-end scene

text understanding.

34

This project was supported by the Spanish project TIN2014-52072-P,

the fellowship RYC-2009-05031, and the Catalan government scholarship

Acknowledgment

2014FI B1-0017.

References

[1] R. Unnikrishnan, R. Smith, Combined script and page orientation esti-

mation using the tesseract ocr engine, in: Proceedings of the Interna-

tional Workshop on Multilingual OCR, ACM, 2009, p. 6.

[2] D. Ghosh, T. Dube, A. P. Shivaprasad, Script recognitiona review, Pat-

tern Analysis and Machine Intelligence, IEEE Transactions on 32 (12)

(2010) 2142–2161.

[3] U. Pal, B. Chaudhuri, Indian script character recognition: a survey,

pattern Recognition 37 (9) (2004) 1887–1899.

[4] A. Bissacco, M. Cummins, Y. Netzer, H. Neven, Photoocr: Reading text

in uncontrolled conditions, in: Proceedings of the IEEE International

Conference on Computer Vision, 2013, pp. 785–792.

[5] M. Jaderberg, A. Vedaldi, A. Zisserman, Deep features for text spotting,

in: Computer Vision–ECCV 2014, Springer, 2014, pp. 512–528.

[6] L. Neumann, J. Matas, Real-time lexicon-free scene text localization

and recognition, IEEE Transactions on Pattern Analysis and Machine

Intelligence PP (99) (2015) 1–1.

35

[7] B. Shi, C. Yao, C. Zhang, X. Guo, F. Huang, X. Bai, Automatic script

identiﬁcation in the wild, in: Document Analysis and Recognition (IC-

DAR), 2015 13th International Conference on, IEEE, 2015, pp. 531–535.

[8] B. Shi, X. Bai, C. Yao, Script identiﬁcation in the wild via discriminative

convolutional neural network, Pattern Recognition 52 (2016) 448–458.

[9] A. Nicolaou, A. D. Bagdanov, L. Gomez-Bigorda, D. Karatzas, Visual

script and language recognition, in: DAS, 2016.

[10] L. Gomez-Bigorda, D. Karatzas, A ﬁne-grained approach to scene text

script identiﬁcation, in: DAS, 2016.

[11] S. Tian, U. Bhattacharya, S. Lu, B. Su, Q. Wang, X. Wei, Y. Lu, C. L.

Tan, Multilingual scene character recognition with co-occurrence of his-

togram of oriented gradients, Pattern Recognition 51 (2016) 125 – 134.

[12] J. Gllavata, B. Freisleben, Script recognition in images with complex

backgrounds, in: Signal Processing and Information Technology, 2005.

Proceedings of the Fifth IEEE International Symposium on, IEEE, 2005,

pp. 589–594.

[13] P. Shivakumara, Z. Yuan, D. Zhao, T. Lu, C. L. Tan, New gradient-

spatial-structural features for video script identiﬁcation, Computer Vi-

sion and Image Understanding 130 (2015) 35–53.

[14] A. Coates, A. Y. Ng, H. Lee, An analysis of single-layer networks in

unsupervised feature learning, in: International conference on artiﬁcial

intelligence and statistics, 2011, pp. 215–223.

36

[15] O. Boiman, E. Shechtman, M. Irani, In defense of nearest-neighbor

based image classiﬁcation, in: Computer Vision and Pattern Recog-

nition, 2008. CVPR 2008. IEEE Conference on, IEEE, 2008, pp. 1–8.

[16] A. L. Spitz, M. Ozaki, Palace: A multilingual document recognition

system, in: Document Analysis Systems, Vol. 1, Singapore: World Sci-

entiﬁc, 1995, pp. 16–37.

[17] A. L. Spitz, Determination of the script and language content of docu-

ment images, Pattern Analysis and Machine Intelligence, IEEE Trans-

actions on 19 (3) (1997) 235–245.

[18] D. Lee, C. R. Nohl, H. S. Baird, Language identiﬁcation in complex, un-

oriented, and degraded document images, Series in Machine Perception

And Artiﬁcial Intelligence 29 (1998) 17–39.

[19] B. Waked, S. Bergler, C. Suen, S. Khoury, Skew detection, page segmen-

tation, and script classiﬁcation of printed document images, in: Systems,

Man, and Cybernetics, 1998. 1998 IEEE International Conference on,

Vol. 5, IEEE, 1998, pp. 4470–4475.

[20] S. Chaudhury, R. Sheth, Trainable script identiﬁcation strategies for

indian languages, in: Document Analysis and Recognition, 1999. IC-

DAR’99. Proceedings of the Fifth International Conference on, IEEE,

1999, pp. 657–660.

[21] J. Hochberg, L. Kerns, P. Kelly, T. Thomas, Automatic script identiﬁca-

tion from images using cluster-based templates, in: Document Analysis

37

and Recognition, 1995., Proceedings of the Third International Confer-

ence on, Vol. 1, IEEE, 1995, pp. 378–381.

[22] S. L. Wood, X. Yao, K. Krishnamurthi, L. Dang, Language identiﬁcation

for printed text independent of segmentation, in:

Image Processing,

1995. Proceedings., International Conference on, Vol. 3, IEEE, 1995,

pp. 428–431.

[23] T. Tan, Rotation invariant texture features and their use in automatic

script identiﬁcation, Pattern Analysis and Machine Intelligence, IEEE

Transactions on 20 (7) (1998) 751–756.

[24] W. Chan, G. Coghill, Text analysis using local energy, Pattern Recog-

nition 34 (12) (2001) 2523 – 2532.

[25] W. Pan, C. Y. Suen, T. D. Bui, Script identiﬁcation using steerable ga-

bor ﬁlters, in: Document Analysis and Recognition, 2005. Proceedings.

Eighth International Conference on, IEEE, 2005, pp. 883–887.

[26] M. A. Ferrer, A. Morales, U. Pal, Lbp based line-wise script identiﬁ-

cation, in: Document Analysis and Recognition (ICDAR), 2013 12th

International Conference on, IEEE, 2013, pp. 369–373.

[27] A. K. Jain, Y. Zhong, Page segmentation using texture analysis, Pattern

Recognition 29 (5) (1996) 743 – 770.

[28] Z. Chi, Q. Wang, W.-C. Siu, Hierarchical content classiﬁcation and

script determination for automatic document image processing, Pattern

Recognition 36 (11) (2003) 2483–2500.

38

[29] A. Hennig, N. Sherkat, Exploiting zoning based on approximating

splines in cursive script recognition, Pattern Recognition 35 (2) (2002)

445 – 454.

3393.

[30] J. Schenk, J. Lenz, G. Rigoll, Novel script line identiﬁcation method

for script normalization and feature extraction in on-line handwritten

whiteboard note recognition, Pattern Recognition 42 (12) (2009) 3383 –

[31] G. Zhu, X. Yu, Y. Li, D. Doermann, Language identiﬁcation for hand-

written document images using a shape codebook, Pattern Recognition

42 (12) (2009) 3184 – 3191.

[32] S. Basu, N. Das, R. Sarkar, M. Kundu, M. Nasipuri, D. K. Basu, A novel

framework for automatic sorting of postal documents with multi-script

address blocks, Pattern Recognition 43 (10) (2010) 3507 – 3521.

[33] G. Zhong, M. Cheriet, Tensor representation learning based image patch

analysis for text identiﬁcation and recognition, Pattern Recognition

48 (4) (2015) 1211 – 1224.

[34] N. Sharma, S. Chanda, U. Pal, M. Blumenstein, Word-wise script iden-

tiﬁcation from video frames, in: Document Analysis and Recognition

(ICDAR), 2013 12th International Conference on, IEEE, 2013, pp. 867–

871.

[35] T. Q. Phan, P. Shivakumara, Z. Ding, S. Lu, C. L. Tan, Video script

identiﬁcation based on text lines, in: Document Analysis and Recog-

39

nition (ICDAR), 2011 International Conference on, IEEE, 2011, pp.

1240–1244.

[36] P. Shivakumara, N. Sharma, U. Pal, M. Blumenstein, C. L. Tan,

Gradient-angular-features for word-wise video script identiﬁcation, in:

2014 22nd International Conference on Pattern Recognition (ICPR),

IEEE, 2014, pp. 3098–3103.

[37] N. Sharma, R. Mandal, R. Sharma, U. Pal, M. Blumenstein, Bag-of-

visual words for word-wise video script identiﬁcation: A study, in: Neu-

ral Networks (IJCNN), 2015 International Joint Conference on, IEEE,

2015, pp. 1–7.

[38] N. Sharma, R. Mandal, R. Sharma, U. Pal, M. Blumenstein, Icdar2015

competition on video script identiﬁcation (cvsi 2015), in: Document

Analysis and Recognition (ICDAR), 2015 13th International Conference

on, IEEE, 2015, pp. 1196–1200.

[39] M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, A. Baskurt, Sequential

deep learning for human action recognition, in: International Workshop

on Human Behavior Understanding, Springer, 2011, pp. 29–39.

[40] S. Ji, W. Xu, M. Yang, K. Yu, 3d convolutional neural networks for

human action recognition, IEEE transactions on pattern analysis and

machine intelligence 35 (1) (2013) 221–231.

[41] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, L. Fei-

Fei, Large-scale video classiﬁcation with convolutional neural networks,

40

in: Proceedings of the IEEE conference on Computer Vision and Pattern

Recognition, 2014, pp. 1725–1732.

[42] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classiﬁcation with

deep convolutional neural networks, in: Advances in neural information

processing systems, 2012, pp. 1097–1105.

[43] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun, C. Moore,

E. S¨ackinger, R. Shah, Signature veriﬁcation using a siamese time de-

lay neural network, International Journal of Pattern Recognition and

Artiﬁcial Intelligence 7 (04) (1993) 669–688.

[44] Y.-F. Pan, X. Hou, C.-L. Liu, Text localization in natural scene images

based on conditional random ﬁeld, in: Document Analysis and Recog-

nition, 2009. ICDAR’09. 10th International Conference on, IEEE, 2009,

pp. 6–10.

[45] C. Yao, X. Bai, W. Liu, Y. Ma, Z. Tu, Detecting texts of arbitrary orien-

tations in natural images, in: Computer Vision and Pattern Recognition

(CVPR), 2012 IEEE Conference on, IEEE, 2012, pp. 1083–1090.

[46] T. E. de Campos, B. R. Babu, M. Varma, Character recognition in

natural images., in: VISAPP (2), 2009, pp. 273–280.

[47] D. Kumar, M. Prasad, A. Ramakrishnan, Multi-script robust reading

competition in icdar 2013,

in: Proceedings of the 4th International

Workshop on Multilingual OCR, ACM, 2013, p. 14.

[48] S. Lee, M. S. Cho, K. Jung, J. H. Kim, Scene text extraction with edge

41

constraint and text collinearity, in: Pattern Recognition (ICPR), 2010

20th International Conference on, IEEE, 2010, pp. 3983–3986.

[49] D. Karatzas, S. Robles, L. Gomez, An on-line platform for ground

truthing and performance evaluation of text extraction systems, in: Doc-

ument Analysis Systems (DAS), 2014 11th IAPR International Work-

shop on, IEEE, 2014, pp. 242–246.

[50] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,

S. Guadarrama, T. Darrell, Caﬀe: Convolutional architecture for fast

feature embedding, in: Proceedings of the ACM International Confer-

ence on Multimedia, ACM, 2014, pp. 675–678.

[51] K. Jarrett, K. Kavukcuoglu, M. Ranzato, Y. LeCun, What is the best

multi-stage architecture for object recognition?, in: Computer Vision,

2009 IEEE 12th International Conference on, IEEE, 2009, pp. 2146–

2153.

[52] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,

Dropout: A simple way to prevent neural networks from overﬁtting, The

Journal of Machine Learning Research 15 (1) (2014) 1929–1958.

[53] D. G. Lowe, Object recognition from local scale-invariant features, in:

Computer vision, 1999. The proceedings of the seventh IEEE interna-

tional conference on, Vol. 2, Ieee, 1999, pp. 1150–1157.

[54] A. Vedaldi, B. Fulkerson, VLFeat: An open and portable library of

computer vision algorithms, http://www.vlfeat.org/ (2008).

42

[55] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, C.-J. Lin, LIBLIN-

EAR: A library for large linear classiﬁcation, The Journal of Machine

Learning Research 9 (2008) 1871–1874.

[56] A. Nicolaou, A. D. Bagdanov, M. Liwicki, D. Karatzas, Sparse radial

sampling lbp for writer identiﬁcation, in: Document Analysis and Recog-

nition (ICDAR), 2015 13th International Conference on, IEEE, 2015, pp.

716–720.

[57] Q. McNemar, Note on the sampling error of the diﬀerence between corre-

lated proportions or percentages, Psychometrika 12 (2) (1947) 153–157.

[58] L. Gomez, D. Karatzas, A fast hierarchical method for multi-

script and arbitrary oriented scene text extraction, arXiv preprint

arXiv:1407.7504.

[59] J. Matas, O. Chum, M. Urban, T. Pajdla, Robust wide-baseline stereo

from maximally stable extremal regions, Image and vision computing

22 (10) (2004) 761–767.

[60] M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams, J. Winn,

A. Zisserman, The pascal visual object classes challenge: A retrospec-

tive, International Journal of Computer Vision 111 (1) (2015) 98–136.

[61] D. Karatzas, L. Gomez-Bigorda, A. Nicolaou, S. Ghosh, A. Bagdanov,

M. Iwamura, J. Matas, L. Neumann, V. R. Chandrasekhar, S. Lu, et al.,

Icdar 2015 competition on robust reading, in: Document Analysis and

Recognition (ICDAR), 2015 13th International Conference on, IEEE,

2015, pp. 1156–1160.

43

[62] R. Smith, An overview of the tesseract ocr engine, in: icdar, IEEE, 2007,

pp. 629–633.

[63] S. Milyaev, O. Barinova, T. Novikova, P. Kohli, V. Lempitsky, Image

binarization for end-to-end text understanding in natural images, in:

Document Analysis and Recognition (ICDAR), 2013 12th International

Conference on, IEEE, 2013, pp. 128–132.

[64] L. G´omez, D. Karatzas, Scene text recognition: No country for old men?,

in: Computer Vision-ACCV 2014 Workshops, Springer, 2014, pp. 157–

168.

[65] S. Milyaev, O. Barinova, T. Novikova, P. Kohli, V. Lempitsky, Fast and

accurate scene text understanding with image binarization and oﬀ-the-

shelf ocr, International Journal on Document Analysis and Recognition

(IJDAR) 18 (2) (2015) 169–182.

[66] K. Wang, B. Babenko, S. Belongie, End-to-end scene text recognition,

in: Computer Vision (ICCV), 2011 IEEE International Conference on,

IEEE, 2011, pp. 1457–1464.

[67] D. Karatzas, F. Shafait, S. Uchida, M. Iwamura, L. Gomez i Bigorda,

S. Robles Mestre, J. Mas, D. Fernandez Mota, J. Almazan Almazan, L.-

P. de las Heras, Icdar 2013 robust reading competition, in: Document

Analysis and Recognition (ICDAR), 2013 12th International Conference

on, IEEE, 2013, pp. 1484–1493.

[68] S. Yousﬁ, S.-A. Berrani, C. Garcia, Alif: A dataset for arabic embedded

text recognition in tv broadcast, in: Document Analysis and Recognition

44

(ICDAR), 2015 13th International Conference on, IEEE, 2015, pp. 1221–

1225.

45

(a)

(b)

Figure 4: The original scene text images (a) are converted to greyscale and

resized to a ﬁxed height (b) in order to extract small local patches with a

dense sampling strategy (c).

conv1 - pool1conv2 - pool2 conv3 - pool3

conv4

fc5 fc6 fc7

Figure 5: Network architecture of the CNN trained to classify individual

image patches. The network has three convolutional+pooling stages followed

by an extra convolution and three fully connected layers.

(c)

46

x1

x2

x3

Elementwise (cid:80)

xN

Figure 6: An Ensemble of Conjoined Nets consist in a set of identical net-

works that are joined at their outputs in order to provide a unique classiﬁ-

cation response.

Figure 7: Validation accuracy for various number of networks N in the en-

semble of conjoined networks model.

47

Figure 8: Confusion matrices with per class classiﬁcation accuracy of our

method in SIW-13, MLe2e, and CVSI datasets.

Figure 9: Overlaid-text samples (top row) variability and clutter is rather

limited compared with that found in the scene text images (bottom row).

Figure 10: Cropped words in the CVSI dataset belong to very long sentences

of overlay text in videos. It is common to ﬁnd several samples sharing exactly

the same font and background both in the train (top row) and test (bottom

row) sets.

Figure 11: Examples of challenging text images that are correctly classiﬁed

by our ECN method but not with the Simple CNN approach.

48

Figure 12: A selection of misclassiﬁed samples by our method: low contrast

images, rare font types, degraded text, letters mixed with numerals, etc.

Figure 13: Classiﬁcation accuracy of the CNN trained with ensembles of

conjoined nets (top) and number of images (bottom) as a function of the

image width on SIW-13 test images.

Figure 14: Classiﬁcation error of our method when applied to variable length

cropped regions of SIW-13 images, up to the minimum size possible (40 × 40

pixels).

49

Figure 15: End-to-end recognition of text from images containing textual

information in diﬀerent scripts/languages.

50

7
1
0
2
 
b
e
F
 
1
 
 
]

V
C
.
s
c
[
 
 
2
v
0
8
4
7
0
.
2
0
6
1
:
v
i
X
r
a

Improving patch-based scene text script identiﬁcation
with ensembles of conjoined networks

Lluis Gomez, Anguelos Nicolaou, Dimosthenis Karatzas

Computer Vision Center, Universitat Autonoma de Barcelona. Ediﬁci O, Campus UAB,
08193 Bellaterra (Cerdanyola) Barcelona, Spain. E-mail: lgomez,dimos@cvc.uab.cat

Abstract

This paper focuses on the problem of script identiﬁcation in scene text im-

ages. Facing this problem with state of the art CNN classiﬁers is not straight-

forward, as they fail to address a key characteristic of scene text instances:

their extremely variable aspect ratio. Instead of resizing input images to a

ﬁxed aspect ratio as in the typical use of holistic CNN classiﬁers, we propose

here a patch-based classiﬁcation framework in order to preserve discrimina-

tive parts of the image that are characteristic of its class.

We describe a novel method based on the use of ensembles of conjoined

networks to jointly learn discriminative stroke-parts representations and their

relative importance in a patch-based classiﬁcation scheme. Our experiments

with this learning procedure demonstrate state-of-the-art results in two pub-

lic script identiﬁcation datasets.

In addition, we propose a new public benchmark dataset for the evalu-

ation of multi-lingual scene text end-to-end reading systems. Experiments

done in this dataset demonstrate the key role of script identiﬁcation in a

complete end-to-end system that combines our script identiﬁcation method

with a previously published text detector and an oﬀ-the-shelf OCR engine.

Preprint submitted to Pattern Recognition

February 2, 2017

Keywords:

script identiﬁcation, scene text understanding, multi-language

OCR, convolutional neural networks, ensemble of conjoined networks

1. Introduction

Script and language identiﬁcation are important steps in modern OCR

systems designed for multi-language environments. Since text recognition al-

gorithms are language-dependent, detecting the script and language at hand

allows selecting the correct language model to employ [1]. While script iden-

tiﬁcation has been widely studied in document analysis [2, 3], it remains

an almost unexplored problem for scene text. In contrast to document im-

ages, scene text presents a set of speciﬁc challenges, stemming from the high

variability in terms of perspective distortion, physical appearance, variable

illumination and typeface design. At the same time, scene text comprises

typically a few words, contrary to longer text passages available in document

images.

Current end-to-end systems for scene text reading [4, 5, 6] assume single

script and language inputs given beforehand, i.e. provided by the user, or

inferred from available meta-data. The unconstrained text understanding

problem for large collections of images from unknown sources has not been

considered up to very recently [7, 8, 9, 10, 11]. While there exists some previ-

ous research in script identiﬁcation of text over complex backgrounds [12, 13],

such methods have been so far limited to video overlaid-text, which presents

in general diﬀerent challenges than scene text.

This paper addresses the problem of script identiﬁcation in natural scene

images, paving the road towards true multi-lingual end-to-end scene text

2

Figure 1: Collections of images from unknown sources may contain textual

information in diﬀerent scripts.

understanding. Multi-script text exhibits high intra-class variability (words

written in the same script vary a lot) and high inter-class similarity (certain

scripts resemble each other). Examining text samples from diﬀerent scripts,

it is clear that some stroke-parts are quite discriminative, whereas others

can be trivially ignored as they occur in multiple scripts. The ability to

distinguish these relevant stroke-parts can be leveraged for recognising the

corresponding script. Figure 2 shows an example of this idea.

Figure 2: (best viewed in color) Certain stroke-parts (in green) are discrimi-

native for the identiﬁcation of a particular script (left), while others (in red)

can be trivially ignored because are frequent in other classes (right).

The use of state of the art CNN classiﬁers for script identiﬁcation is not

straightforward, as they fail to address a key characteristic of scene text

instances: their extremely variable aspect ratio. As can be seen in Figure 3,

scene text images may span from single characters to long text sentences, and

thus resizing images to a ﬁxed aspect ratio, as in the typical use of holistic

CNN classiﬁers, will deteriorate discriminative parts of the image that are

characteristic of its class. The key intuition behind the proposed method is

3

that in order to retain the discriminative power of stroke parts we must rely

in powerful local feature representations and use them within a patch-based

classiﬁer. In other words, while holistic CNNs have superseded patch-based

methods for image classiﬁcation, we claim that patch-based classiﬁers can

still be essential in tasks where image shrinkage is not feasible.

Figure 3: Scene text images with the larger/smaller aspect ratio available in

three diﬀerent datasets: MLe2e(left), SIW-13(center), and CVSI(right).

In previously published work [10] we have presented a method combining

convolutional features, extracted by sliding a window with a single layer

Convolutional Neural Network (CNN) [14], and the Naive-Bayes Nearest

Neighbour (NBNN) classiﬁer [15] with promising results. In this paper we

demonstrate far superior performance by extending our previous work in two

diﬀerent ways: First, we use deep CNN architectures in order to learn more

discriminative representations for the individual image patches; Second, we

propose a novel learning methodology to jointly learn the patch representa-

tions and their importance (contribution) in a global image to class proba-

bilistic measure. For this, we train our CNN using an Ensemble of Conjoined

Networks and a loss function that takes into account the global classiﬁcation

error for a group of N patches instead of looking only into a single image

patch. Thus, at training time our network is presented with a group of N

patches sharing the same class label and produces a single probability distri-

bution over the classes for all them. This way we model the goal for which

the network is trained, not only to learn good local patch representations,

4

but also to learn their relative importance in the global image classiﬁcation

task.

Experiments performed over two public datasets for scene text classiﬁca-

tion demonstrate state-of-the-art results. In particular we are able to reduce

classiﬁcation error by 5 percentage points in the SIW-13 dataset. We also

introduce a new benchmark dataset, namely the MLe2e dataset, for the eval-

uation of scene text end-to-end reading systems and all intermediate stages

such as text detection, script identiﬁcation and text recognition. The dataset

contains a total of 711 scene images, and 1821 text line instances, covering

four diﬀerent scripts (Latin, Chinese, Kannada, and Hangul) and a large

variability of scene text samples.

2. Related Work

Script identiﬁcation is a well studied problem in document image analy-

sis. Gosh et al. [2] has published a compehensive review of methods dealing

with this problem. They identify two broad categories of methods: structure-

based and visual appearance-based techniques. In the ﬁrst category, Spitz

and Ozaki [16, 17] propose the use of the vertical distribution of upward

concavities in connected components and their optical density for page-wise

script identiﬁcation. Lee et al. [18], and Waked et al. [19] among others build

on top of Spitz seminal work by incorporating additional connected compo-

nent based features. Similarly, Chaudhuri et al. [20] use the projection pro-

ﬁle, statistical and topological features, and stroke features for classiﬁcation

of text lines in printed documents. Hochberg et al. [21] propose the use of

cluster-based templates to identify unique characteristic shapes. A method

5

that is similar in spirit with the one presented in this paper, while requiring

textual symbols to be precisely segmented to generate the templates.

Regarding segmentation-free methods based on visual appearance of scripts,

i.e. not directly analyzing the character patterns in the document, Wood et

al. [22] experimented with the use of vertical and horizontal projection pro-

ﬁles of full-page document images. More recent methods in this category have

used texture features from Gabor ﬁlters analysis [23, 24, 25] or Local Binary

Patterns [26]. Neural networks have been also used for segmentation-free

script identiﬁcation et al. [27, 28] without the use of hand-crafted features.

All the methods discussed above are designed speciﬁcally with printed

document images in mind. Structure-based methods require text connected

components to be precisely segmented from the image, while visual appearance-

based techniques are known to work better in bilevel text. Moreover, some of

these methods require large blocks of text in order to obtain suﬃcient infor-

mation and thus are not well suited for scene text which typically comprises

a few words.

Contrary to the case of printed document images, research in script iden-

tiﬁcation on non traditional paper layouts is more scarce, and has been

mainly dedicated to handwritten text [29, 30, 31, 32, 33], and video overlaid-

text [12, 34, 35, 36, 13] until very recently. Gllavatta et al. [12], in the ﬁrst

work dealing with video text script identiﬁcation, proposed a method using

the wavelet transform to detect edges in overlaid-text images. Then, they

extract a set of low-level edge features, and make use of a K-NN classiﬁer.

Sharma et al. [34] have explored the use of traditional document analysis

techniques for video overlaid-text script identiﬁcation at word level. They

6

analyze three sets of features: Zernike moments, Gabor ﬁlters, and a set

of hand-crafted gradient features previously used for handwritten character

recognition. They propose a number of pre-processing algorithms to over-

come the inherent challenges of video overlaid-text.

In their experiments

the combination of super resolution, gradient features, and a SVM classiﬁer

perform signiﬁcantly better that the other combinations.

Phan et al. [35] propose a method for combined detection of video text

overlay and script identiﬁcation. They propose the extraction of upper and

lower extreme points for each connected component of Canny edges of text

lines and analyse their smoothness and cursiveness.

Shivakumara et al. [36, 13] rely on skeletonization of the dominant gra-

dients They analyze the angular curvatures [36] of skeleton components, and

the spatial/structural [13] distribution of their end, joint, and intersection

points to extract a set of hand-crafted features. For classiﬁcation they build

a set of feature templates from train data, and use the Nearest Neighbor rule

for classifying scripts at word [36] or text block [13] level.

As said before, all these methods have been designed (and evaluated)

speciﬁcally for video overlaid-text, which presents in general diﬀerent chal-

lenges than scene text. Concretely, they mainly rely in accurate edge detec-

tion of text components and this is not always feasible in scene text.

More recently, Sharma et al. [37] explored the use of Bag-of-Visual Words

based techniques for word-wise script identiﬁcation in video-overlaid text.

They use Bag-Of-Features (BoF) and Spatial Pyramid Matching (SPM) with

patch based SIFT descriptors and found that the SPM pipeline outperforms

traditional script identiﬁcation techniques involving gradient based features

7

(e.g. HoG) and texture based features (e.g. LBP).

In 2015, the ICDAR Competition on Video Script Identiﬁcation (CVSI-

2015) [38] challenged the document analysis community with a new compet-

itive benchmark dataset. With images extracted from diﬀerent video sources

(news, sports etc.) covering mainly overlaid-text, but also a few instances of

scene text. The top performing methods in the competition where all based

in Convolutional Neural Networks, showing a clear diﬀerence in overall ac-

curacy over pipelines using hand-crafted features (e.g. LBP and/or HoG).

The ﬁrst dataset for script identiﬁcation in real scene text images was pro-

vided by Shi et al.in [7], where the authors propose the Multi-stage Spatially-

sensitive Pooling Network (MSPN) method. The MSPN network overcomes

the limitation of having a ﬁxed size input in traditional Convolutional Neural

Networks by pooling along each row of the intermediate layers’ outputs by

taking the maximum (or average) value in each row. Their method is ex-

tended in [8] by combining deep features and mid-level representations into a

globally trainable deep model. They extract local deep features at every layer

of the MSPN and describe images with a codebook-based encoding method

that can be used to ﬁne-tune the CNN weights.

Nicolaou et al. [9] has presented a method based on texture features

producing state of the art results in script identiﬁcation for both scene or

overlaid text images. They rely in hand-crafted texture features, a variant

of LBP, and a deep Multi Layer Perceptron to learn a metric space in which

they perform K-NN classiﬁcation.

In our previous work [10] we have proposed a patch-based method for

script identiﬁcation in scene text images. We used Convolutional features,

8

extracted from small image patches, and the Naive-Bayes Nearest Neighbour

classiﬁer (NBNN). We also presented a simple weighting strategy in order to

discover the most discriminative parts (or templates patches) per class in a

ﬁne-grained classiﬁcation approach.

In this paper we build upon our previous work [10] by extending it in two

ways: On one side, we make use of a much deeper Convolutional Neural Net-

work model. On the other hand, we replace the weighted NBNN classiﬁer by

a patch-based classiﬁcation rule that can be integrated in the CNN training

process by using an Ensemble of Conjoined Networks. This way, our CNN

model is able to learn at the same time expressive representations for image

patches and their relative contribution to the patch-based classiﬁcation rule.

From all reviewed methods the one proposed here is the only one based

in a patch-based classiﬁcation framework. Our intuition is that in cases

where holistic CNN models are not directly applicable, as in the case of text

images (because of their highly variable aspect ratios), the contribution of

rich parts descriptors without any deterioration (either by image distortion

or by descriptor quantization) is essential for correct image classiﬁcation.

In this sense our method is related with some CNN extensions that have

been proposed for video classiﬁcation. Unlike still images which can be

cropped and rescaled to a ﬁxed size, video sequences have a variable temporal

dimension and cannot be directly processed with a ﬁxed-size architecture. In

this context, 3D Convolutional Neural Networks [39, 40] have been proposed

to leverage the motion information encoded in multiple contiguous frames.

Basically the idea is to feed the CNN with a stack of a ﬁxed number of

consecutive frames and perform convolutions in both time and space dimen-

9

sions. Still these methods require a ﬁxed size input and thus they must be

applied several times through the whole sequence to obtain a chain of out-

puts that are then averaged [40] or fed into an Recurrent Neural Network [39]

to provide a ﬁnal decision. Karpathy et al. [41] also treat videos as bags of

short ﬁxed-length clips, but they investigate the use of diﬀerent temporal

connectivity patterns (early fusion, late fusion and slow fusion). To produce

predictions for an entire video they randomly sample 20 clips and take the

average of the network class predictions. While we share with these methods

the high-level goal of learning CNN weights from groups of stacked patches

(or frames) there are two key diﬀerences in the way we build our framework:

(1) the groups of patches that are fed into the network at training time are

randomly sampled and do not follow any particular order; and (2) at test

time we decouple the network to densely evaluate single patches and aver-

age their outputs. In other words, while in stacked-frame CNNs for video

recognition having an ordered sequence of input patches is crucial to learn

spatio-temporal features, our design aims to learn which are the most dis-

criminative patches in the input stack, independently of their relative spatial

arrangement.

In the experimental section we compare our method with some of the al-

gorithms reviewed in this section and demonstrate its superiority. Concretely

our approach improves the state-of-the-art in the SIW-13 [8] dataset for scene

text script classiﬁcation by a large margin of 5 percentage points, while per-

forms competitively in the CVSI-2015 [38] video overlaid-text dataset.

10

3. Patch-based classiﬁcation with Ensembles of Conjoined Net-

works

In our patch-based classiﬁcation method an input image is represented as

a collection of local descriptors, from patches extracted following a certain

sampling strategy. Those local features are then fed into a global classiﬁer

rule, that makes a decision for the input image.

3.1. Convolutional Neural Network for image-patch classiﬁcation

Given an input scene text image (i.e. a pre-segmented word or text line)

we ﬁrst resize it to a ﬁxed height of 40 pixels, but retaining its original

aspect ratio. Since scene text can appear in any possible combination of

foreground and background colors, we pre-process the image by converting it

into grayscale and centering pixel values. Then, we densely extract patches

at two diﬀerent scales, 32×32 and 40×40, by sliding a window with a step of

8 pixels. The particular values of these two window scales and step size was

found by cross-validation optimization as explained in section 4.2, and its

choice can be justiﬁed as follows: the 40 × 40 patch, covering the full height

of the resized image, is a natural choice in our system because it provides

the largest squared region we can crop; the 32 × 32 patches are conceived

for better scale invariance of the CNN model, similarly as the random crops

typically used for data augmentation in CNN-based image classiﬁcation [42].

Figure 4 shows the patches extracted from a given example image. This way

we build a large dataset of image patches that take the same label as the

image they were extracted from. With this dataset of patches we train a

CNN classiﬁer for the task of individual image patch classiﬁcation.

11

We use a Deep Convolutional Neural Network to build the expressive

image patch representations needed in our method. For the design of our

network we start from the CNN architecture proposed in [7] as it is known

to work well for script identiﬁcation. We then iteratively do an exhaustive

search to optimize by cross-validation the following CNN hyper-parameters:

number of convolutional and fully connected layers, number of ﬁlters per

layer, kernel sizes, and feature map normalisation schemes. The CNN archi-

tecture providing better performance in our experiments is shown in Figure 5.

Our CNN consists in three convolutional+pooling stages followed by an ex-

tra convolution and three fully connected layers. Details about the speciﬁc

conﬁguration and parameters are given in section 4.2.

At testing time, given a query scene text image the trained CNN model

is applied to image patches following the same sampling strategy described

before. Then, the individual CNN responses for each image patch can be fed

into the global classiﬁcation rule in order to make a single labeling decision

for the query image.

3.2. Training with an Ensemble of Conjoined Networks

Since the output of the CNN for an individual image patch is a probability

distribution over class labels, a simple global decision rule would be just to

average the responses of the CNN for all patches in a given query image:

y(I) =

CN N (xi)

1
nI

nI(cid:88)

i=1

(1)

where an image I takes the label with more probability in the averaged

softmax responses (y(I)) of their nI individual patches {x1, ..., xnI } outputs

12

on the CNN.

The problem with this global classiﬁcation rule is that the CNN weights

have been trained to solve a problem (individual patch classiﬁcation) that is

diﬀerent from the ﬁnal goal (i.e. classifying the whole query image). Besides,

it is based in a simplistic voting strategy for which all patches are assumed

to weight equally, i.e. no patches are more or less discriminative than others.

To overcome this we propose the use of an Ensemble of Conjoined Nets in

order to train the CNN for a task that resembles more the ﬁnal classiﬁcation

goal.

An Ensemble of Conjoined Nets (ECN), depicted in Figure 6, consists in

a set of identical networks that are joined at their outputs in order to provide

a unique classiﬁcation response. At training time the ECN is presented with

a set of N image patches extracted from the same image, thus sharing the

same label, and produces a single output for all them. Thus, to train an

ECN we must build a new training dataset where each sample consists in a

set of N patches with the same label (extracted from the same image).

ECNs take inspiration from Siamese Networks [43] but, instead of trying

to learn a metric space with a distance-based loss function, the individual

networks in the ECN are joined at their last fully connected layer (fc7 in

our case), which has the same number of neurons as the number of classes,

with a simple element-wise sum operation and thus we can use the standard

cross-entropy classiﬁcation loss. This way, the cross-entropy classiﬁcation

loss function of the ECN can be written in terms of the N individual patch

responses as follows:

13

E =

log(ˆpm,lm),

−1
M

M
(cid:88)

m=1

(cid:34) K
(cid:88)

k(cid:48)=1

N
(cid:88)

n=1

N
(cid:88)

n=1

ˆpm,k = exp(

xmnk)/

exp(

xmnk(cid:48))

(cid:35)

(2)

where M is the number of input samples in a mini-batch, ˆpm is the prob-

ability distribution over classes provided by the softmax function, lm is the

label of the m’th sample, N is the number of conjoined networks in the en-

semble, K is the number of classes, and xmnk ∈ [−∞, +∞] indicates the

response (score) of the k’th neuron in the n’th network for the m’th sample.

As can be appreciated in equation 2, in an ECN network a single input

patch contributes to the backpropagation error in terms of a global goal func-

tion for which it is not the only patch responsible. For example, even when

a single patch is correctly scored in the last fully connected layer it may be

penalized, and induced to produce a larger activation, if the other patches in

its same sample contribute to a wrong classiﬁcation at the ensemble output.

At test time, the CNN model trained in this way is applied to all image

patches in the query image and the global classiﬁcation rule is deﬁned as:

y(I) =

CN Nf c7(xi)

(3)

where an image I takes the label with the highest score in the sum (y(I))

of the fc7 layer responses of the nI individual patches {x1, ..., xnI }. This is

the same as in Equation 1 but using the fc7 layer responses instead of the

output softmax responses of the CNN.

Notice that still the task for which the ECN network has been trained is

nI(cid:88)

i=1

14

not exactly the same deﬁned by this global classiﬁcation rule, as the number

of patches nI is variable for each image and usually diﬀerent than the number

of conjoined networks N . However, it certainly resembles more the true

ﬁnal classiﬁcation goal. The number of conjoined networks N is an hyper-

parameter of the method that is largely dependent on the task to be solved

and is discussed in the experimental section.

4. Experiments

All reported experiments were conducted over three datasets, namely the

Video Script Identiﬁcation Competition (CVSI-2015) dataset1, the SIW-13

dataset2, and the MLe2e dataset3.

The CVSI-2015 [38] dataset comprises pre-segmented words in ten scripts:

English, Hindi, Bengali, Oriya, Gujrathi, Punjabi, Kannada, Tamil, Telegu,

and Arabic. The dataset contains about 1000 words for each script and is

divided into three parts: a training set ( 60% of the total images), a validation

set (10%), and a test set (30%). Text is extracted from various video sources

(news, sports etc.) and, while it contains a few instances of scene text, it

covers mainly overlay video text.

The SIW-13 datset [8] comprises 16291 pre-segmented text lines in thir-

teen scripts: Arabic, Cambodian, Chinese, English, Greek, Hebrew, Japanese,

Kannada, Korean, Mongolian, Russian, Thai, and Tibetan. The test set con-

tains 500 text lines for each script, 6500 in total, and all the other images

1http://www.ict.griffith.edu.au/cvsi2015/
2http://mc.eistar.net/~xbai/mspnProjectPage/
3http://github.com/lluisgomez/script_identification/

15

are provided for training. In this case, text was extracted from natural scene

images from Google Street View.

4.1. The MLe2e dataset

This paper introduces the ﬁrst dataset available up to date for the evalu-

ation of multi-lingual scene text end-to-end reading systems and all interme-

diate stages: text detection, script identiﬁcation, and text recognition. The

Multi-Language end-to-end (MLe2e) dataset has been harvested from vari-

ous existing scene text datasets for which the images and ground-truth have

been revised in order to make them homogeneous. The original images come

from the following datasets: Multilanguage(ML) [44] and MSRA-TD500 [45]

contribute Latin and Chinese text samples, Chars74K [46] and MSRRC [47]

contribute Latin and Kannada samples, and KAIST [48] contributes Latin

and Hangul samples.

In order to provide a homogeneous dataset, all images have been resized

proportionally to ﬁt in 640 × 480 pixels, which is the default image size

of the KAIST dataset. Moreover, the groundtruth has been revised to en-

sure a common text line annotation level [49]. During this process human

annotators were asked to review all resized images, adding the script class la-

bels and text transcriptions to the groundtruth, and checking for annotation

consistency: discarding images with unknown scripts or where all text is un-

readable (this may happen because images were resized); joining individual

word annotations into text line level annotations; discarding images where

correct text line segmentation is not clear or cannot be established, and im-

ages where a bounding box annotation contains more than one script (this

happens very rarely e.g.

in trademarks or logos) or where more than half

16

of the bounding box is background (this may happen with heavily slanted

or curved tex). Arabic numerals (0, .., 9), widely used in combination with

many (if not all) scripts, are labeled as follows. A text line containing text

and Arabic numerals is labeled as the script of the text it contains, while a

text line containing only Arabic numerals is labeled as Latin.

The MLe2e dataset contains a total of 711 scene images covering four

diﬀerent scripts (Latin, Chinese, Kannada, and Hangul) and a large vari-

ability of scene text samples. The dataset is split into a train and a test

set with 450 and 261 images respectively. The split was done randomly, but

in a way that the test set contains a balanced number of instances of each

class (aprox. 160 text lines samples of each script), leaving the rest of the

images for the train set (which is not balanced by default). The dataset is

suitable for evaluating various typical stages of end-to-end pipelines, such as

multi-script text detection, joint detection and script identiﬁcation, end-to-

end multi-lingual recognition, and script identiﬁcation in pre-segmented text

lines. For the latter, the dataset also provides the cropped images with the

text lines corresponding to each data split: 1178 and 643 images in the train

and test set respectively.

While being a dataset that has been harvested from a mix of existing

datasets it is important to notice that building it has supposed an important

annotation eﬀort: since some of the original datasets did not provide text

transcriptions, and/or where annotated at diﬀerent granularity levels. More-

over, despite the fact that the number of languages in the dataset is rather

limited (four scripts) it is the ﬁrst public dataset that covers the evaluation

of all stages of multi-lingual end-to-end systems for scene text understanding

17

in natural scenes. We think this is an important contribution of this paper

and hope the dataset will be useful to other researchers in the community.

4.2. Implementation details

In this section we detail the architectures of the network models used in

this paper, as well as the diﬀerent hyper-parameter setups that can be used

to reproduce the results provided in following sections. In all our experiments

we have used the open source Caﬀe [50] framework for deep learning running

on commodity GPUs. Source code and compatible Caﬀe models are made

publicly available4.

We have performed exhaustive experiments by varying many of the pro-

posed methods parameters, training multiple models, and choosing the one

with best cross-validation performance on the SIW-13 training set. The fol-

lowing parameters were tuned in this procedure: the size and step of the

sliding window, the base learning rate, the number of convolutional and fully

connected layers, the number of nodes in all layers, the convolutional kernel

sizes, and the feature map normalisation schemes

This way, the best basic CNN model found for individual image patch

classiﬁcation is described in section 3.1 and Figure 5, and has the following

per layer conﬁguration:

• Input layer: single channel 32 × 32 image patch.

• conv1 layer: 96 ﬁlters with size 5 × 5. Stride=1, pad=0. Output size:

96 × 28 × 28.

• pool1 layer: kernel size=3, stride=2, pad=1. Otput size: 96 × 15 × 15.

4http://github.com/lluisgomez/script_identification/

18

• conv2 layer: 256 ﬁlters with size 3 × 3. Stride=1, pad=0. Output size:

• pool2 layer: kernel size=3, stride=2, pad=1. Otput size: 256 × 7 × 7.

• conv3 layer: 384 ﬁlters with size 3 × 3. Stride=1, pad=0. Output size:

256 × 13 × 13.

384 × 5 × 5.

• pool3 layer: kernel size=3, stride=2, pad=1. Otput size: 384 × 3 × 3.

• conv4 layer: 512 ﬁlters with size 1 × 1. Stride=1, pad=0. Output size:

512 × 3 × 3.

• fc5 layer: 4096 neurons.

• fc6 layer: 1024 neurons.

• fc7 layer: N neurons, where N is the number of classes.

• SoftMax layer: Output a probability distribution over the N class la-

bels.

The total number of parameters of the network is ≈ 24M for the N = 13

case in the SIW-13 dataset. All convolution and fully connected layers use

Rectiﬁed Linear Units (ReLU). In conv1 and conv2 layers we perform normal-

ization over input regions using Local Response Normalization (LRN) [51].

At training time, we use dropout [52] (with a 0.5 ratio) in fc5 and fc6 layers.

To train the basic network model we use Stochastic Gradient Descent

(SGD) with momentum and L2 regularization. We use mini-batches of 64

images. The base learning rate is set to 0.01 and is decreased by a factor of

×10 every 100k iterations. The momentum weight parameter is set to 0.9,

and the weight decay regularization parameter to 5 × 10−4.

When training for individual patch classiﬁcation, we build a dataset of

small patches extracted by dense sampling the original training set images,

19

as explained in section 3.1. Notice that this produces a large set of patch

samples, e.g. in the SIW-13 dataset the number of training samples is close to

half million. With these numbers the network converges after 250k iterations.

In the case of the Ensemble of Conjoined Networks the basic network

detailed above is replicated N times, and all replicas are tied at their fc7

outputs with an element-wise sum layer which is connected to a single output

SoftMax layer. All networks in the ECN share the same parameters values.

Training the ECN requires a dataset where each input sample is com-

posed by N image patches. We generate this dataset as follows: given an

input image we extract patches the same way as for the simple network, then

we generate random N −combinations of the image patches, allowing repeti-

tions if the number of patches is < N . Notice that this way the number of

samples can be increased up to very large-scale numbers because the number
(cid:1) when the number of patches in
of possible diﬀerent N −combinations is (cid:0)M

N

a given image M is larger that the number of conjoined nets N , which is

the usual case. This is an important aspect of ECNs, as the training dataset

generation process becomes a data augmentation technique in itself. We can

see this data augmentation process as generating new small text instances

that are composed from randomly chosen parts of their original generators.

However, it is obviously non-practical to use all possible combinations for

training; thus, in order to get a manageable number of samples, we have used

the simple rule of generating 2 × M samples per input, which for example in

the SIW-13 dataset would produce around one million samples.

In terms of computational training complexity, the ECN has an impor-

tant drawback compared to the simple network model: the number of com-

20

putations is multiplied by N in each forward pass, similarly the amount of

memory needed is linearly increased by N . To overcome this limitation, we

use a ﬁne-tuning approach to train ECNs. First, we train the simple network

model, and then we do ﬁne-tuning on the ECN parameters starting from the

values learned using the simple net. When ﬁne-tuning, we have found that

starting from a fully converged network in the single-patch classiﬁcation task

we reach a local minimum of the global task, thus providing zero loss in most

(if not all) the iterations and not allowing the network to learn anything new.

In order to avoid this local minima situation we start the ﬁne-tuning from

a non-converged network (more or less at about 90/95% of the attainable

individual patch classiﬁcation accuracy).

Using ﬁne-tuning with a base learning rate of 0.001 (decreasing ×10 every

10k iterations) the ECN converges much faster, in the order of 35k iterations.

All other learning parameters are set the same as in the simple network

training setup.

The number of nets N in the ensemble can be seen as an extra hyper-

parameter in the ECN learning algorithm. Intuitively a dataset with larger

text sequences would beneﬁt from larger N values, while on the contrary

in the extreme case of classifying small squared images (i.e. each image is

represented by a single patch) any value of N > 1 does not make sense. Since

our datasets contain text instances with variable length a possible procedure

to select the optimal value of N is by using a validation set. We have done

experiments in the SIW-13 dataset by dividing the provided train set and

keeping 10% for validation. Classiﬁcation accuracy on the validation set for

various N values are shown in Figure 7. As can be appreciated the positive

21

impact of training with an ensemble of networks is evident for small values

of N , and mostly saturated for values N > 9. In the following we use a value

of N = 10 for all the remaining experiments.

4.3. Script identiﬁcation in pre-segmented text lines

In this section we study the performance of the proposed method for

script identiﬁcation in pre-segmented text lines. Table 1 shows the overall

performance comparison of our method with the state-of-the-art in CVSI-

2015, SIW-13, and MLe2e datasets. Figure 8 shows the confusion matrices

for our method in all three datasets with detailed per class classiﬁcation

results.

In Table 1 we also provide comparison with three well known image

recognition pipelines using Scale Invariant Features [53] (SIFT) in three dif-

ferent encodings: Fisher Vectors, Vector of Locally Aggregated Descriptors

(VLAD), and Bag of Words (BoW); and a linear SVM classiﬁer. In all base-

lines we extract SIFT features at four diﬀerent scales in sliding window with

a step of 8 pixels. For the Fisher vectors we use a 256 visual words GMM, for

VLAD a 256 vector quantized visual words, and for BoW 2,048 vector quan-

tized visual words histograms. The step size and number of visual words were

set to similar values to our method when possible in order to oﬀer a fair eval-

uation. These three pipelines have been implemented with the VLFeat [54]

and liblinear [55] open source libraries. The entry “Sequence-based CNN”

in Table 1 corresponds to the results obtained with the early fusion design

proposed in [41] with a stack of 5 consecutive patches.

As shown in Table 1 the proposed method outperforms state of the art

and all baseline methods in the SIW-13 and MLe2e scene text datasets, while

22

Method

SIW-13 MLe2e CVSI

This work - Ensemble of Conjoined Nets

94.8

94.4

This work - Simple CNN (Avg.)

This work - Simple CNN (fc5+SVM)

Shi et al. [8]

HUST [7, 38]

Google [38]

Nicolaou et al. [9]

Gomez et al. [10]

CVC-2 [10, 38]

SRS-LBP + KNN [56]

C-DAC [38]

CUK [38]

97.2

96.7

96.9

94.3

96.69

98.91

98.18

93.1

93.6

-

-

-

-

-

-

91.12

97.91

88.16

96.0

82.71

94.20

84.66

74.06

88.63

94.11

90.19

93.92

86.45

84.38

89.80

93.62

92.8

93.4

89.4

88.0

83.7

76.9

-

-

-

-

-

90.7

89.2

83.4

88.9

Baseline SIFT + Fisher Vectors + SVM

Baseline SIFT + VLAD + SVM

Baseline SIFT + Bag of Words + SVM

Baseline Sequence-based CNN [41] (Early fusion)

Table 1: Overall classiﬁcation performance comparison with state-of-the-art

in three diﬀerent datasets: SIW-13 [8], MLe2e, and CVSI [38].

23

performing competitively in the case of CVSI video overlay text dataset. In

the SIW-13 dataset the proposed method signiﬁcantly outperforms the best

performing method known up to date by more than 4 percentual points.

The entry “Simple CNN (fc5+SVM)” (third row) in Table 1 corresponds

to the results obtained with a linear SVM classiﬁer by using features ex-

tracted from the “Simple CNN” network. For this experiment we represent

each image in the dataset with a ﬁxed length vector with the averaged out-

puts of all its patches in the fc5 layer of the network. Then we train a linear

SVM classiﬁer using cross-validation on the training set, and show the classi-

ﬁcation performance on the test set. Similar results (or slightly worse) have

been found for features extracted from other layers (fc7, fc6, conv4) and us-

ing other linear classiﬁers (e.g.

logistic regression). When compared with

the “Simple CNN” approach we appreciate that classiﬁcation performance

is better for this combination (fc5+SVM). This conﬁrms the intuition that

classiﬁcation performance can be improved by optimizing the combination

of the results for the individual patches. However, the performance of the

CNN trained with the ensemble of conjoined networks is still better. As men-

tioned earlier, the additional beneﬁt of our approach here is in the end-to-end

learning of both the visual features and the optimal combination scheme for

classiﬁcation.

The contribution of training with ensembles of conjoined nets is consistent

in all three evaluated datasets but more notable on SIW-13, as appreciated by

comparing the ﬁrst two rows of Table 1 which correspond to the nets trained

with the ensemble (ﬁrst row) and the simple model (second row). This com-

parison can be further strengthened by testing if the provided improvement is

24

statistically signiﬁcant. For this we use the within-subjects chi-squared test

(McNemar’s test) [57] to compare the predictive accuracy of the two models.

The obtained p-values on the SIW-13, MLe2e, and CVSI datasets are respec-

tively 1.4 × 10−16, 0.057, and 0.0026. In the case of the SIW-13 dataset the

p-value is way smaller than the assumed signiﬁcance threshold (α = 0.05),

thus we can reject the null-hypothesis that both models perform equally well

on this dataset and certify a statistically signiﬁcant improvement. On the

other hand we appreciate a marginal improvement on the other two datasets.

Our interpretation of the results on CVSI and MLe2e datasets in compar-

ison with the ones obtained on SIW-13 relates to its distinct nature. On one

hand the MLe2e dataset covers only four diﬀerent scripts (Latin, Chinese,

Kannada, and Hangul) for which the inter-class similarity does not represent

a real problem. On the other hand, the CVSI overlaid-text variability and

clutter is rather limited compared with that found in the scene text of MLe2e

and SIW-13. As can be appreciated in Figure 9 overlaid-text is usually bi-

level without much clutter. Figure 10 shows another important characteristic

of CVSI dataset: since cropped words in the dataset belong to very long sen-

tences of overlay text in videos, e.g. from rotating headlines, it is common to

ﬁnd a few dozens of samples sharing exactly the same font and background

both in the train and test sets. This particularity makes the ECN network

not really helpful in the case of CVSI, as the data augmentation by image

patches recombinations is somehow already implicit on the dataset.

Furthermore, the CVSI-2015 competition winner (Google) makes use of a

deep convolutional network but applies a binarization pre-processing to the

input images. In our opinion this binarization may not be a realistic pre-

25

processing in general for scene text images. As an example of this argument

one can easily see in Figure 9 that binarization of scene text instances is

not trivial as in overlay text. Similar justiﬁcation applies to other methods

performing better than ours in CVSI. In particular the LBP features used

in [9], as well as the patch-level whitening used in our previous work [10], may

potentially take advantage of the simpler, bi-level, nature of text instances

in CVSI dataset. It is important to notice here that these two algorithms,

corresponding to our previous works in script identiﬁcation, have close num-

bers to the Google ones in CVSI-2015 (see Table 1) but perform quite bad

in SIW-13.

As a conclusion of the experiments performed in this section we can say

that the improvement of training a patch-based CNN classiﬁer with an en-

semble of conjoined nets is especially appreciable in cases where we have a

large number of classes, with large inter-class similarity, and cluttered scene

images, as is the case of the challenging SIW-13 dataset. This demonstrates

our initial claim that a powerful script identiﬁcation method for scene text

images must be based in learning good local patch representations, and also

their relative importance in the global image classiﬁcation task. Figure 11

shows some examples of challenging text images that are correctly classiﬁed

by our method but not with the Simple CNN approach. Figure 12 shows a

set of missclassiﬁed images.

Finally, in Figure 13 we show the classiﬁcation accuracy of the CNN

trained with ensembles of conjoined nets as a function of the image width

on SIW-13 test images. We appreciate that the method is robust even for

small text images which contain a limited number of unique patches. Com-

26

putation time for our method is also dependent on the input image length

and ranges from 4ms.

in for the smaller images up to 23ms. for the larger

ones. The average computation time on the SIW-13 test set is of 13ms using

a commodity GPU. At test time computation is made eﬃcient by stacking

all patches of the input image in a single mini-batch.

4.4. Joint text detection and script identiﬁcation in scene images

In this experiment we evaluate the performance of a complete pipeline for

detection and script identiﬁcation in its joint ability to detect text lines in

natural scene images and properly recognizing their scripts. The key interest

of this experiment is to study the performance of the proposed script identi-

ﬁcation algorithm when realistic, non-perfect, text localisation is available.

Most text detection pipelines are trained explicitly for a speciﬁc script

(typically English) and generalise pretty badly to the multi-script scenario.

We have chosen to use here our previously published script-agnostic method [58],

which is designed for multi-script text detection and generalises well to any

script. The method detects character candidates using the Maximally Sta-

ble Extremal Regions (MSER) [59] algorithm, and builds diﬀerent hierar-

chies where the initial regions are grouped by agglomerative clustering, using

complementary similarity measures. In such hierarchies each node deﬁnes a

possible text hypothesis. Then, an eﬃcient classiﬁer, using incrementally

computable descriptors, is used to walk each hierarchy and select the nodes

with larger text-likelihood.

In this paper script identiﬁcation is performed at the text line level, be-

cause segmentation into words is largely script-dependent, and not meaning-

ful in Chinese/Korean scripts. Notice however that in some cases, by the

27

intrinsic nature of scene text, a text line provided by the text detection mod-

ule may correspond to a single word, so we must deal with a large variability

in the length of provided text lines. The experiments are performed over the

new MLe2e dataset.

For evaluation of the joint text detection and script identiﬁcation task in

the MLe2e dataset we propose the use of a simple two-stage evaluation frame-

work. First, localisation is assessed based on the Intersection-over-Union

(IoU) metric between detected and ground-truth regions, as commonly used

in object detection tasks [60] and the recent ICDAR 2015 Robust Read-

ing Competition5 [61]. Second, the predicted script is veriﬁed against the

ground-truth. A detected bounding box is thus considered correct if it has a

IoU > 0.5 with a bounding box in the ground-truth and the predicted script

is correct.

The localisation-only performance, corresponding to the ﬁrst stage of the

evaluation, yields an F-score of 0.63 (Precision of 0.57 and Recall of 0.69).

This deﬁnes the upper-bound for the joint task. The two stage evaluation,

including script identiﬁcation, of the proposed method compared with our

previous work is shown in Table 2.

Intuitively the proposed method for script identiﬁcation is eﬀective even

when the text region is badly localised, as long as part of the text area is

within the localised region. To support this argument we have performed an

additional experiment where our algorithm is applied to cropped regions from

pre-segmented text images. For this, we take the SIW-13 original images and

5http://rrc.cvc.uab.es

28

Method

Correct Wrong Missing Precision Recall F-score

This work - ECN

Gomez et al. [10]

395

364

376

407

245

278

0.51

0.47

0.62

0.56

0.57

0.52

Table 2: Text detection and script identiﬁcation performance in the MLe2e

dataset.

calculate the performance of our method when applied to cropped regions of

variable length, up to the minimum size possible (40 × 40 pixels). As can

be appreciated in Figure 14 the experiment demonstrates that the proposed

method is eﬀective even when small parts of the text lines are provided.

Such a behaviour is to be expected, due to the way our method treats local

information to decide on a script class. In the case of the pipeline for joint

detection and script identiﬁcation, this extends to regions that did not pass

the 0.5 IoU threshold, but had their script correctly identiﬁed. This opens

the possibility to make use of script identiﬁcation to inform and / or improve

the text localisation process. The information of the identiﬁed script can be

used to reﬁne the detections.

4.5. End-to-end multi-lingual recognition in scene images

In this section we evaluate the performance of a complete pipeline for

end-to-end multi-lingual recognition in scene images. For this, we combine

the pipeline used in the previous section with a well known oﬀ-the-shelf OCR

engine: the open source project Tesseract6 [62]. Similar pipelines [63, 64, 65]

6http://code.google.com/p/tesseract-ocr/

29

using oﬀ-the-shelf OCR engines have demonstrated state-of-the-art end-to-

end performance in English-only datasets up to very recently, provided that

the text detection module is able to produce good pixel-level segmentation

of text.

The setup of the OCR engine in our pipeline is minimal: given a text

detection hypothesis from the detection module we set the recognition lan-

guage to the one provided by the script identiﬁcation module, and we set the

OCR to interpret the input as a single text line. Apart from that we use the

default Tesseract parameters.

The recognition output is ﬁltered with a simple post-processing junk ﬁlter

in order to eliminate garbage recognitions, i.e. sequences of identical charac-

ters like ”IIii” that may appear as the result of trying to recognize repetitive

patterns in the scene. Concretely, we discard the words in which more than

half of their characters are recognized as one of ”i”, ”l”, ”I”, or other special

characters like: punctuation marks, quotes, exclamation, etc. We also reject

those detections for which the recognition conﬁdence provided by the OCR

engine is under a certain threshold.

The evaluation protocol is similar to the one used in other end-to-end

scene text recognition datasets [66, 61]. Ideally, in end-to-end word recogni-

tion, a given output word is considered correct if it overlaps more than 0.5

with a ground-truth word and all its characters are recognized correctly (case

sensitive). However, since in the case of the MLe2e dataset we are evaluating

text lines instead of single words, we relax a bit this correctness criteria by

allowing the OCR output to to make 1

8 character level errors. This relax-
ation is motivated by the fact that for a given test sentence with more than

30

Script identiﬁcation Correct Wrong Missing Precision Recall F-score

This work - ECN

Gomez et al. [10]

Tesseract

96

82

50

212

211

93

503

517

549

0.31

0.28

0.35

0.16

0.21

0.14

0.08

0.18

0.13

Table 3: End-to-end multi-lingual recognition performance in the MLe2e

dataset.

1
8.

test set.

8 characters (e.g. with two words) having only one character mistake may

still produce a partial understanding of the text (e.g. one of the words is

correct), and thus must not be penalized the same way as if all characters

are wrongly recognized. This way, a given output text line is considered cor-

rect if overlaps more than 0.5 with a ground-truth text line and their edit

distance divided by the number of characters of the largest is smaller than

Table 3 shows a comparison of the proposed end-to-end pipeline by using

diﬀerent script identiﬁcation modules: the method presented in this paper,

our previously published work, and Tesseract’s built-in alternative. Figure 15

shows the output of our full end-to-end pipeline for some images in the MLe2e

Tesseract method in Table 3 refers to the use of Tesseract’s own script es-

timation algorithm [1]. We have found that Tesseract’s algorithm is designed

to work with large corpses of text (e.g. full page documents) and does not

work well for the case of single text lines.

Results in Table 3 demonstrate the direct correlation between having

31

better script identiﬁcation rates and better end-to-end recognition results.

The ﬁnal multi-lingual recognition f-score obtained (0.21) is far from the

state-of-the art in end-to-end recognition systems designed for English-only

environments [4, 5, 6]. As a fair comparison, a very similar pipeline using

the Tesseract OCR engine [64] achieves an f-score of 0.40 in the ICDAR

English-only dataset. The lower performance obtained in MLe2e dataset

stems from a number of challenges that are speciﬁc to its multi-lingual nature.

For example, in some scripts (e.g. Chinese and Kannada) glyphs are many

times non single-body regions, composed by (or complemented with) small

strokes that in many cases are lost in the text segmentation stage. In such

cases having a bad pixel-level segmentation of text would make it practically

impossible for the OCR engine to produce a correct recognition.

Our pipeline results represent the ﬁrst reference result for multi-lingual

scene text recognition and a ﬁrst benchmark from which better systems can

be built, e.g. replacing the oﬀ-the-shelf OCR engine by other recognition

modules better suited for scene text imagery.

4.6. Cross-domain performance and confusion in single-language datasets

In this experiment we evaluate the cross-domain performance of learned

CNN weights from one dataset to the other. For example, we evaluate on

the MLe2e and CVSI test sets using the network trained with the SIw-13

train set, by measuring classiﬁcation accuracy only for their common script

classes: Arabic, English, and Kannada in CVSI; Chinese, English, Kannada,

and Korean in MLe2e. Finally, we evaluate the misclassiﬁcation error of our

method (trained in diﬀerent datasets) over two single-script datasets. For

this experiment we use the ICDAR2013 [67] and ALIF [68] datasets, which

32

provide cropped word images of English scene text and Arabic video overlaid

text respectively. Table 4 shows the results of these experiments.

Method

SIW-13 MLe2e CVSI

ICDAR ALIF

ECN CNN (SIW-13)

94.8

86.8

90.6

74.7

100

ECN CNN (MLe2e)

90.8

94.4

98.3

95.3

-

ECN CNN (CVSI)

42.3

43.5

97.2

65.2

91.8

Table 4: Cross-domain performance of our method measured by train-

ing/testing in diﬀerent datasets.

Notice that results in Table 4 are not directly comparable among rows

because each classiﬁer has been trained with a diﬀerent number of classes,

thus having diﬀerent rates for a random choice classiﬁcation. However, the

experiment serves as a validation of how good a given classiﬁer is in perform-

ing with data that is distinct in nature to the one used for training. In this

sense, the obtained results show a clear weakness when the model is trained

on the video overlaid text of CVSI and subsequently applied to scene text

images (SIW-13, MLe2e, and ICDAR). On the contrary, models trained on

scene text datasets are quite stable in other scene text data, as well as in

video overlaid text (CVSI and ALIF).

In fact, this is an expected result, because the domain of video overlay

text can be seen as a sub-domain of the scene text domain. Since the scene

text datasets are richer in text variability, e.g. in terms of perspective distor-

tion, physical appearance, variable illumination, and typeface designs, script

33

identiﬁcation on these datasets is a more diﬃcult problem, and their data

is more indicated if one wants to learn eﬀective cross-domain models. This

demonstrates that our method is able to learn discriminative stroke-part rep-

resentations that are not dataset-speciﬁc, and provides evidence to the claims

made in section 4.3 when interpreting the obtained results in CVSI dataset

comparing with other methods that may be more engineered to the speciﬁc

CVSI data but not generalizing well in scene text datasets.

5. Conclusion

A novel method for script identiﬁcation in natural scene images was pre-

sented. The method is based on the use of ensembles of conjoined convo-

lutional networks to jointly learn discriminative stroke-part representations

and their relative importance in a patch-based classiﬁcation scheme. Exper-

iments performed in three diﬀerent datasets exhibit state of the art accuracy

rates in comparison to a number of state-of-the-art methods, including the

participants in the CVSI-2015 competition and three standard image classi-

ﬁcation pipelines.

In addition, a new public benchmark dataset for the evaluation of all

stages of multi-lingual end-to-end scene text reading systems was introduced.

Our work demonstrates the viability of script identiﬁcation in natural

scene images, paving the road towards true multi-lingual end-to-end scene

text understanding.

34

This project was supported by the Spanish project TIN2014-52072-P,

the fellowship RYC-2009-05031, and the Catalan government scholarship

Acknowledgment

2014FI B1-0017.

References

[1] R. Unnikrishnan, R. Smith, Combined script and page orientation esti-

mation using the tesseract ocr engine, in: Proceedings of the Interna-

tional Workshop on Multilingual OCR, ACM, 2009, p. 6.

[2] D. Ghosh, T. Dube, A. P. Shivaprasad, Script recognitiona review, Pat-

tern Analysis and Machine Intelligence, IEEE Transactions on 32 (12)

(2010) 2142–2161.

[3] U. Pal, B. Chaudhuri, Indian script character recognition: a survey,

pattern Recognition 37 (9) (2004) 1887–1899.

[4] A. Bissacco, M. Cummins, Y. Netzer, H. Neven, Photoocr: Reading text

in uncontrolled conditions, in: Proceedings of the IEEE International

Conference on Computer Vision, 2013, pp. 785–792.

[5] M. Jaderberg, A. Vedaldi, A. Zisserman, Deep features for text spotting,

in: Computer Vision–ECCV 2014, Springer, 2014, pp. 512–528.

[6] L. Neumann, J. Matas, Real-time lexicon-free scene text localization

and recognition, IEEE Transactions on Pattern Analysis and Machine

Intelligence PP (99) (2015) 1–1.

35

[7] B. Shi, C. Yao, C. Zhang, X. Guo, F. Huang, X. Bai, Automatic script

identiﬁcation in the wild, in: Document Analysis and Recognition (IC-

DAR), 2015 13th International Conference on, IEEE, 2015, pp. 531–535.

[8] B. Shi, X. Bai, C. Yao, Script identiﬁcation in the wild via discriminative

convolutional neural network, Pattern Recognition 52 (2016) 448–458.

[9] A. Nicolaou, A. D. Bagdanov, L. Gomez-Bigorda, D. Karatzas, Visual

script and language recognition, in: DAS, 2016.

[10] L. Gomez-Bigorda, D. Karatzas, A ﬁne-grained approach to scene text

script identiﬁcation, in: DAS, 2016.

[11] S. Tian, U. Bhattacharya, S. Lu, B. Su, Q. Wang, X. Wei, Y. Lu, C. L.

Tan, Multilingual scene character recognition with co-occurrence of his-

togram of oriented gradients, Pattern Recognition 51 (2016) 125 – 134.

[12] J. Gllavata, B. Freisleben, Script recognition in images with complex

backgrounds, in: Signal Processing and Information Technology, 2005.

Proceedings of the Fifth IEEE International Symposium on, IEEE, 2005,

pp. 589–594.

[13] P. Shivakumara, Z. Yuan, D. Zhao, T. Lu, C. L. Tan, New gradient-

spatial-structural features for video script identiﬁcation, Computer Vi-

sion and Image Understanding 130 (2015) 35–53.

[14] A. Coates, A. Y. Ng, H. Lee, An analysis of single-layer networks in

unsupervised feature learning, in: International conference on artiﬁcial

intelligence and statistics, 2011, pp. 215–223.

36

[15] O. Boiman, E. Shechtman, M. Irani, In defense of nearest-neighbor

based image classiﬁcation, in: Computer Vision and Pattern Recog-

nition, 2008. CVPR 2008. IEEE Conference on, IEEE, 2008, pp. 1–8.

[16] A. L. Spitz, M. Ozaki, Palace: A multilingual document recognition

system, in: Document Analysis Systems, Vol. 1, Singapore: World Sci-

entiﬁc, 1995, pp. 16–37.

[17] A. L. Spitz, Determination of the script and language content of docu-

ment images, Pattern Analysis and Machine Intelligence, IEEE Trans-

actions on 19 (3) (1997) 235–245.

[18] D. Lee, C. R. Nohl, H. S. Baird, Language identiﬁcation in complex, un-

oriented, and degraded document images, Series in Machine Perception

And Artiﬁcial Intelligence 29 (1998) 17–39.

[19] B. Waked, S. Bergler, C. Suen, S. Khoury, Skew detection, page segmen-

tation, and script classiﬁcation of printed document images, in: Systems,

Man, and Cybernetics, 1998. 1998 IEEE International Conference on,

Vol. 5, IEEE, 1998, pp. 4470–4475.

[20] S. Chaudhury, R. Sheth, Trainable script identiﬁcation strategies for

indian languages, in: Document Analysis and Recognition, 1999. IC-

DAR’99. Proceedings of the Fifth International Conference on, IEEE,

1999, pp. 657–660.

[21] J. Hochberg, L. Kerns, P. Kelly, T. Thomas, Automatic script identiﬁca-

tion from images using cluster-based templates, in: Document Analysis

37

and Recognition, 1995., Proceedings of the Third International Confer-

ence on, Vol. 1, IEEE, 1995, pp. 378–381.

[22] S. L. Wood, X. Yao, K. Krishnamurthi, L. Dang, Language identiﬁcation

for printed text independent of segmentation, in:

Image Processing,

1995. Proceedings., International Conference on, Vol. 3, IEEE, 1995,

pp. 428–431.

[23] T. Tan, Rotation invariant texture features and their use in automatic

script identiﬁcation, Pattern Analysis and Machine Intelligence, IEEE

Transactions on 20 (7) (1998) 751–756.

[24] W. Chan, G. Coghill, Text analysis using local energy, Pattern Recog-

nition 34 (12) (2001) 2523 – 2532.

[25] W. Pan, C. Y. Suen, T. D. Bui, Script identiﬁcation using steerable ga-

bor ﬁlters, in: Document Analysis and Recognition, 2005. Proceedings.

Eighth International Conference on, IEEE, 2005, pp. 883–887.

[26] M. A. Ferrer, A. Morales, U. Pal, Lbp based line-wise script identiﬁ-

cation, in: Document Analysis and Recognition (ICDAR), 2013 12th

International Conference on, IEEE, 2013, pp. 369–373.

[27] A. K. Jain, Y. Zhong, Page segmentation using texture analysis, Pattern

Recognition 29 (5) (1996) 743 – 770.

[28] Z. Chi, Q. Wang, W.-C. Siu, Hierarchical content classiﬁcation and

script determination for automatic document image processing, Pattern

Recognition 36 (11) (2003) 2483–2500.

38

[29] A. Hennig, N. Sherkat, Exploiting zoning based on approximating

splines in cursive script recognition, Pattern Recognition 35 (2) (2002)

445 – 454.

3393.

[30] J. Schenk, J. Lenz, G. Rigoll, Novel script line identiﬁcation method

for script normalization and feature extraction in on-line handwritten

whiteboard note recognition, Pattern Recognition 42 (12) (2009) 3383 –

[31] G. Zhu, X. Yu, Y. Li, D. Doermann, Language identiﬁcation for hand-

written document images using a shape codebook, Pattern Recognition

42 (12) (2009) 3184 – 3191.

[32] S. Basu, N. Das, R. Sarkar, M. Kundu, M. Nasipuri, D. K. Basu, A novel

framework for automatic sorting of postal documents with multi-script

address blocks, Pattern Recognition 43 (10) (2010) 3507 – 3521.

[33] G. Zhong, M. Cheriet, Tensor representation learning based image patch

analysis for text identiﬁcation and recognition, Pattern Recognition

48 (4) (2015) 1211 – 1224.

[34] N. Sharma, S. Chanda, U. Pal, M. Blumenstein, Word-wise script iden-

tiﬁcation from video frames, in: Document Analysis and Recognition

(ICDAR), 2013 12th International Conference on, IEEE, 2013, pp. 867–

871.

[35] T. Q. Phan, P. Shivakumara, Z. Ding, S. Lu, C. L. Tan, Video script

identiﬁcation based on text lines, in: Document Analysis and Recog-

39

nition (ICDAR), 2011 International Conference on, IEEE, 2011, pp.

1240–1244.

[36] P. Shivakumara, N. Sharma, U. Pal, M. Blumenstein, C. L. Tan,

Gradient-angular-features for word-wise video script identiﬁcation, in:

2014 22nd International Conference on Pattern Recognition (ICPR),

IEEE, 2014, pp. 3098–3103.

[37] N. Sharma, R. Mandal, R. Sharma, U. Pal, M. Blumenstein, Bag-of-

visual words for word-wise video script identiﬁcation: A study, in: Neu-

ral Networks (IJCNN), 2015 International Joint Conference on, IEEE,

2015, pp. 1–7.

[38] N. Sharma, R. Mandal, R. Sharma, U. Pal, M. Blumenstein, Icdar2015

competition on video script identiﬁcation (cvsi 2015), in: Document

Analysis and Recognition (ICDAR), 2015 13th International Conference

on, IEEE, 2015, pp. 1196–1200.

[39] M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, A. Baskurt, Sequential

deep learning for human action recognition, in: International Workshop

on Human Behavior Understanding, Springer, 2011, pp. 29–39.

[40] S. Ji, W. Xu, M. Yang, K. Yu, 3d convolutional neural networks for

human action recognition, IEEE transactions on pattern analysis and

machine intelligence 35 (1) (2013) 221–231.

[41] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, L. Fei-

Fei, Large-scale video classiﬁcation with convolutional neural networks,

40

in: Proceedings of the IEEE conference on Computer Vision and Pattern

Recognition, 2014, pp. 1725–1732.

[42] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classiﬁcation with

deep convolutional neural networks, in: Advances in neural information

processing systems, 2012, pp. 1097–1105.

[43] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun, C. Moore,

E. S¨ackinger, R. Shah, Signature veriﬁcation using a siamese time de-

lay neural network, International Journal of Pattern Recognition and

Artiﬁcial Intelligence 7 (04) (1993) 669–688.

[44] Y.-F. Pan, X. Hou, C.-L. Liu, Text localization in natural scene images

based on conditional random ﬁeld, in: Document Analysis and Recog-

nition, 2009. ICDAR’09. 10th International Conference on, IEEE, 2009,

pp. 6–10.

[45] C. Yao, X. Bai, W. Liu, Y. Ma, Z. Tu, Detecting texts of arbitrary orien-

tations in natural images, in: Computer Vision and Pattern Recognition

(CVPR), 2012 IEEE Conference on, IEEE, 2012, pp. 1083–1090.

[46] T. E. de Campos, B. R. Babu, M. Varma, Character recognition in

natural images., in: VISAPP (2), 2009, pp. 273–280.

[47] D. Kumar, M. Prasad, A. Ramakrishnan, Multi-script robust reading

competition in icdar 2013,

in: Proceedings of the 4th International

Workshop on Multilingual OCR, ACM, 2013, p. 14.

[48] S. Lee, M. S. Cho, K. Jung, J. H. Kim, Scene text extraction with edge

41

constraint and text collinearity, in: Pattern Recognition (ICPR), 2010

20th International Conference on, IEEE, 2010, pp. 3983–3986.

[49] D. Karatzas, S. Robles, L. Gomez, An on-line platform for ground

truthing and performance evaluation of text extraction systems, in: Doc-

ument Analysis Systems (DAS), 2014 11th IAPR International Work-

shop on, IEEE, 2014, pp. 242–246.

[50] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,

S. Guadarrama, T. Darrell, Caﬀe: Convolutional architecture for fast

feature embedding, in: Proceedings of the ACM International Confer-

ence on Multimedia, ACM, 2014, pp. 675–678.

[51] K. Jarrett, K. Kavukcuoglu, M. Ranzato, Y. LeCun, What is the best

multi-stage architecture for object recognition?, in: Computer Vision,

2009 IEEE 12th International Conference on, IEEE, 2009, pp. 2146–

2153.

[52] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,

Dropout: A simple way to prevent neural networks from overﬁtting, The

Journal of Machine Learning Research 15 (1) (2014) 1929–1958.

[53] D. G. Lowe, Object recognition from local scale-invariant features, in:

Computer vision, 1999. The proceedings of the seventh IEEE interna-

tional conference on, Vol. 2, Ieee, 1999, pp. 1150–1157.

[54] A. Vedaldi, B. Fulkerson, VLFeat: An open and portable library of

computer vision algorithms, http://www.vlfeat.org/ (2008).

42

[55] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, C.-J. Lin, LIBLIN-

EAR: A library for large linear classiﬁcation, The Journal of Machine

Learning Research 9 (2008) 1871–1874.

[56] A. Nicolaou, A. D. Bagdanov, M. Liwicki, D. Karatzas, Sparse radial

sampling lbp for writer identiﬁcation, in: Document Analysis and Recog-

nition (ICDAR), 2015 13th International Conference on, IEEE, 2015, pp.

716–720.

[57] Q. McNemar, Note on the sampling error of the diﬀerence between corre-

lated proportions or percentages, Psychometrika 12 (2) (1947) 153–157.

[58] L. Gomez, D. Karatzas, A fast hierarchical method for multi-

script and arbitrary oriented scene text extraction, arXiv preprint

arXiv:1407.7504.

[59] J. Matas, O. Chum, M. Urban, T. Pajdla, Robust wide-baseline stereo

from maximally stable extremal regions, Image and vision computing

22 (10) (2004) 761–767.

[60] M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams, J. Winn,

A. Zisserman, The pascal visual object classes challenge: A retrospec-

tive, International Journal of Computer Vision 111 (1) (2015) 98–136.

[61] D. Karatzas, L. Gomez-Bigorda, A. Nicolaou, S. Ghosh, A. Bagdanov,

M. Iwamura, J. Matas, L. Neumann, V. R. Chandrasekhar, S. Lu, et al.,

Icdar 2015 competition on robust reading, in: Document Analysis and

Recognition (ICDAR), 2015 13th International Conference on, IEEE,

2015, pp. 1156–1160.

43

[62] R. Smith, An overview of the tesseract ocr engine, in: icdar, IEEE, 2007,

pp. 629–633.

[63] S. Milyaev, O. Barinova, T. Novikova, P. Kohli, V. Lempitsky, Image

binarization for end-to-end text understanding in natural images, in:

Document Analysis and Recognition (ICDAR), 2013 12th International

Conference on, IEEE, 2013, pp. 128–132.

[64] L. G´omez, D. Karatzas, Scene text recognition: No country for old men?,

in: Computer Vision-ACCV 2014 Workshops, Springer, 2014, pp. 157–

168.

[65] S. Milyaev, O. Barinova, T. Novikova, P. Kohli, V. Lempitsky, Fast and

accurate scene text understanding with image binarization and oﬀ-the-

shelf ocr, International Journal on Document Analysis and Recognition

(IJDAR) 18 (2) (2015) 169–182.

[66] K. Wang, B. Babenko, S. Belongie, End-to-end scene text recognition,

in: Computer Vision (ICCV), 2011 IEEE International Conference on,

IEEE, 2011, pp. 1457–1464.

[67] D. Karatzas, F. Shafait, S. Uchida, M. Iwamura, L. Gomez i Bigorda,

S. Robles Mestre, J. Mas, D. Fernandez Mota, J. Almazan Almazan, L.-

P. de las Heras, Icdar 2013 robust reading competition, in: Document

Analysis and Recognition (ICDAR), 2013 12th International Conference

on, IEEE, 2013, pp. 1484–1493.

[68] S. Yousﬁ, S.-A. Berrani, C. Garcia, Alif: A dataset for arabic embedded

text recognition in tv broadcast, in: Document Analysis and Recognition

44

(ICDAR), 2015 13th International Conference on, IEEE, 2015, pp. 1221–

1225.

45

(a)

(b)

Figure 4: The original scene text images (a) are converted to greyscale and

resized to a ﬁxed height (b) in order to extract small local patches with a

dense sampling strategy (c).

conv1 - pool1conv2 - pool2 conv3 - pool3

conv4

fc5 fc6 fc7

Figure 5: Network architecture of the CNN trained to classify individual

image patches. The network has three convolutional+pooling stages followed

by an extra convolution and three fully connected layers.

(c)

46

x1

x2

x3

Elementwise (cid:80)

xN

Figure 6: An Ensemble of Conjoined Nets consist in a set of identical net-

works that are joined at their outputs in order to provide a unique classiﬁ-

cation response.

Figure 7: Validation accuracy for various number of networks N in the en-

semble of conjoined networks model.

47

Figure 8: Confusion matrices with per class classiﬁcation accuracy of our

method in SIW-13, MLe2e, and CVSI datasets.

Figure 9: Overlaid-text samples (top row) variability and clutter is rather

limited compared with that found in the scene text images (bottom row).

Figure 10: Cropped words in the CVSI dataset belong to very long sentences

of overlay text in videos. It is common to ﬁnd several samples sharing exactly

the same font and background both in the train (top row) and test (bottom

row) sets.

Figure 11: Examples of challenging text images that are correctly classiﬁed

by our ECN method but not with the Simple CNN approach.

48

Figure 12: A selection of misclassiﬁed samples by our method: low contrast

images, rare font types, degraded text, letters mixed with numerals, etc.

Figure 13: Classiﬁcation accuracy of the CNN trained with ensembles of

conjoined nets (top) and number of images (bottom) as a function of the

image width on SIW-13 test images.

Figure 14: Classiﬁcation error of our method when applied to variable length

cropped regions of SIW-13 images, up to the minimum size possible (40 × 40

pixels).

49

Figure 15: End-to-end recognition of text from images containing textual

information in diﬀerent scripts/languages.

50

7
1
0
2
 
b
e
F
 
1
 
 
]

V
C
.
s
c
[
 
 
2
v
0
8
4
7
0
.
2
0
6
1
:
v
i
X
r
a

Improving patch-based scene text script identiﬁcation
with ensembles of conjoined networks

Lluis Gomez, Anguelos Nicolaou, Dimosthenis Karatzas

Computer Vision Center, Universitat Autonoma de Barcelona. Ediﬁci O, Campus UAB,
08193 Bellaterra (Cerdanyola) Barcelona, Spain. E-mail: lgomez,dimos@cvc.uab.cat

Abstract

This paper focuses on the problem of script identiﬁcation in scene text im-

ages. Facing this problem with state of the art CNN classiﬁers is not straight-

forward, as they fail to address a key characteristic of scene text instances:

their extremely variable aspect ratio. Instead of resizing input images to a

ﬁxed aspect ratio as in the typical use of holistic CNN classiﬁers, we propose

here a patch-based classiﬁcation framework in order to preserve discrimina-

tive parts of the image that are characteristic of its class.

We describe a novel method based on the use of ensembles of conjoined

networks to jointly learn discriminative stroke-parts representations and their

relative importance in a patch-based classiﬁcation scheme. Our experiments

with this learning procedure demonstrate state-of-the-art results in two pub-

lic script identiﬁcation datasets.

In addition, we propose a new public benchmark dataset for the evalu-

ation of multi-lingual scene text end-to-end reading systems. Experiments

done in this dataset demonstrate the key role of script identiﬁcation in a

complete end-to-end system that combines our script identiﬁcation method

with a previously published text detector and an oﬀ-the-shelf OCR engine.

Preprint submitted to Pattern Recognition

February 2, 2017

Keywords:

script identiﬁcation, scene text understanding, multi-language

OCR, convolutional neural networks, ensemble of conjoined networks

1. Introduction

Script and language identiﬁcation are important steps in modern OCR

systems designed for multi-language environments. Since text recognition al-

gorithms are language-dependent, detecting the script and language at hand

allows selecting the correct language model to employ [1]. While script iden-

tiﬁcation has been widely studied in document analysis [2, 3], it remains

an almost unexplored problem for scene text. In contrast to document im-

ages, scene text presents a set of speciﬁc challenges, stemming from the high

variability in terms of perspective distortion, physical appearance, variable

illumination and typeface design. At the same time, scene text comprises

typically a few words, contrary to longer text passages available in document

images.

Current end-to-end systems for scene text reading [4, 5, 6] assume single

script and language inputs given beforehand, i.e. provided by the user, or

inferred from available meta-data. The unconstrained text understanding

problem for large collections of images from unknown sources has not been

considered up to very recently [7, 8, 9, 10, 11]. While there exists some previ-

ous research in script identiﬁcation of text over complex backgrounds [12, 13],

such methods have been so far limited to video overlaid-text, which presents

in general diﬀerent challenges than scene text.

This paper addresses the problem of script identiﬁcation in natural scene

images, paving the road towards true multi-lingual end-to-end scene text

2

Figure 1: Collections of images from unknown sources may contain textual

information in diﬀerent scripts.

understanding. Multi-script text exhibits high intra-class variability (words

written in the same script vary a lot) and high inter-class similarity (certain

scripts resemble each other). Examining text samples from diﬀerent scripts,

it is clear that some stroke-parts are quite discriminative, whereas others

can be trivially ignored as they occur in multiple scripts. The ability to

distinguish these relevant stroke-parts can be leveraged for recognising the

corresponding script. Figure 2 shows an example of this idea.

Figure 2: (best viewed in color) Certain stroke-parts (in green) are discrimi-

native for the identiﬁcation of a particular script (left), while others (in red)

can be trivially ignored because are frequent in other classes (right).

The use of state of the art CNN classiﬁers for script identiﬁcation is not

straightforward, as they fail to address a key characteristic of scene text

instances: their extremely variable aspect ratio. As can be seen in Figure 3,

scene text images may span from single characters to long text sentences, and

thus resizing images to a ﬁxed aspect ratio, as in the typical use of holistic

CNN classiﬁers, will deteriorate discriminative parts of the image that are

characteristic of its class. The key intuition behind the proposed method is

3

that in order to retain the discriminative power of stroke parts we must rely

in powerful local feature representations and use them within a patch-based

classiﬁer. In other words, while holistic CNNs have superseded patch-based

methods for image classiﬁcation, we claim that patch-based classiﬁers can

still be essential in tasks where image shrinkage is not feasible.

Figure 3: Scene text images with the larger/smaller aspect ratio available in

three diﬀerent datasets: MLe2e(left), SIW-13(center), and CVSI(right).

In previously published work [10] we have presented a method combining

convolutional features, extracted by sliding a window with a single layer

Convolutional Neural Network (CNN) [14], and the Naive-Bayes Nearest

Neighbour (NBNN) classiﬁer [15] with promising results. In this paper we

demonstrate far superior performance by extending our previous work in two

diﬀerent ways: First, we use deep CNN architectures in order to learn more

discriminative representations for the individual image patches; Second, we

propose a novel learning methodology to jointly learn the patch representa-

tions and their importance (contribution) in a global image to class proba-

bilistic measure. For this, we train our CNN using an Ensemble of Conjoined

Networks and a loss function that takes into account the global classiﬁcation

error for a group of N patches instead of looking only into a single image

patch. Thus, at training time our network is presented with a group of N

patches sharing the same class label and produces a single probability distri-

bution over the classes for all them. This way we model the goal for which

the network is trained, not only to learn good local patch representations,

4

but also to learn their relative importance in the global image classiﬁcation

task.

Experiments performed over two public datasets for scene text classiﬁca-

tion demonstrate state-of-the-art results. In particular we are able to reduce

classiﬁcation error by 5 percentage points in the SIW-13 dataset. We also

introduce a new benchmark dataset, namely the MLe2e dataset, for the eval-

uation of scene text end-to-end reading systems and all intermediate stages

such as text detection, script identiﬁcation and text recognition. The dataset

contains a total of 711 scene images, and 1821 text line instances, covering

four diﬀerent scripts (Latin, Chinese, Kannada, and Hangul) and a large

variability of scene text samples.

2. Related Work

Script identiﬁcation is a well studied problem in document image analy-

sis. Gosh et al. [2] has published a compehensive review of methods dealing

with this problem. They identify two broad categories of methods: structure-

based and visual appearance-based techniques. In the ﬁrst category, Spitz

and Ozaki [16, 17] propose the use of the vertical distribution of upward

concavities in connected components and their optical density for page-wise

script identiﬁcation. Lee et al. [18], and Waked et al. [19] among others build

on top of Spitz seminal work by incorporating additional connected compo-

nent based features. Similarly, Chaudhuri et al. [20] use the projection pro-

ﬁle, statistical and topological features, and stroke features for classiﬁcation

of text lines in printed documents. Hochberg et al. [21] propose the use of

cluster-based templates to identify unique characteristic shapes. A method

5

that is similar in spirit with the one presented in this paper, while requiring

textual symbols to be precisely segmented to generate the templates.

Regarding segmentation-free methods based on visual appearance of scripts,

i.e. not directly analyzing the character patterns in the document, Wood et

al. [22] experimented with the use of vertical and horizontal projection pro-

ﬁles of full-page document images. More recent methods in this category have

used texture features from Gabor ﬁlters analysis [23, 24, 25] or Local Binary

Patterns [26]. Neural networks have been also used for segmentation-free

script identiﬁcation et al. [27, 28] without the use of hand-crafted features.

All the methods discussed above are designed speciﬁcally with printed

document images in mind. Structure-based methods require text connected

components to be precisely segmented from the image, while visual appearance-

based techniques are known to work better in bilevel text. Moreover, some of

these methods require large blocks of text in order to obtain suﬃcient infor-

mation and thus are not well suited for scene text which typically comprises

a few words.

Contrary to the case of printed document images, research in script iden-

tiﬁcation on non traditional paper layouts is more scarce, and has been

mainly dedicated to handwritten text [29, 30, 31, 32, 33], and video overlaid-

text [12, 34, 35, 36, 13] until very recently. Gllavatta et al. [12], in the ﬁrst

work dealing with video text script identiﬁcation, proposed a method using

the wavelet transform to detect edges in overlaid-text images. Then, they

extract a set of low-level edge features, and make use of a K-NN classiﬁer.

Sharma et al. [34] have explored the use of traditional document analysis

techniques for video overlaid-text script identiﬁcation at word level. They

6

analyze three sets of features: Zernike moments, Gabor ﬁlters, and a set

of hand-crafted gradient features previously used for handwritten character

recognition. They propose a number of pre-processing algorithms to over-

come the inherent challenges of video overlaid-text.

In their experiments

the combination of super resolution, gradient features, and a SVM classiﬁer

perform signiﬁcantly better that the other combinations.

Phan et al. [35] propose a method for combined detection of video text

overlay and script identiﬁcation. They propose the extraction of upper and

lower extreme points for each connected component of Canny edges of text

lines and analyse their smoothness and cursiveness.

Shivakumara et al. [36, 13] rely on skeletonization of the dominant gra-

dients They analyze the angular curvatures [36] of skeleton components, and

the spatial/structural [13] distribution of their end, joint, and intersection

points to extract a set of hand-crafted features. For classiﬁcation they build

a set of feature templates from train data, and use the Nearest Neighbor rule

for classifying scripts at word [36] or text block [13] level.

As said before, all these methods have been designed (and evaluated)

speciﬁcally for video overlaid-text, which presents in general diﬀerent chal-

lenges than scene text. Concretely, they mainly rely in accurate edge detec-

tion of text components and this is not always feasible in scene text.

More recently, Sharma et al. [37] explored the use of Bag-of-Visual Words

based techniques for word-wise script identiﬁcation in video-overlaid text.

They use Bag-Of-Features (BoF) and Spatial Pyramid Matching (SPM) with

patch based SIFT descriptors and found that the SPM pipeline outperforms

traditional script identiﬁcation techniques involving gradient based features

7

(e.g. HoG) and texture based features (e.g. LBP).

In 2015, the ICDAR Competition on Video Script Identiﬁcation (CVSI-

2015) [38] challenged the document analysis community with a new compet-

itive benchmark dataset. With images extracted from diﬀerent video sources

(news, sports etc.) covering mainly overlaid-text, but also a few instances of

scene text. The top performing methods in the competition where all based

in Convolutional Neural Networks, showing a clear diﬀerence in overall ac-

curacy over pipelines using hand-crafted features (e.g. LBP and/or HoG).

The ﬁrst dataset for script identiﬁcation in real scene text images was pro-

vided by Shi et al.in [7], where the authors propose the Multi-stage Spatially-

sensitive Pooling Network (MSPN) method. The MSPN network overcomes

the limitation of having a ﬁxed size input in traditional Convolutional Neural

Networks by pooling along each row of the intermediate layers’ outputs by

taking the maximum (or average) value in each row. Their method is ex-

tended in [8] by combining deep features and mid-level representations into a

globally trainable deep model. They extract local deep features at every layer

of the MSPN and describe images with a codebook-based encoding method

that can be used to ﬁne-tune the CNN weights.

Nicolaou et al. [9] has presented a method based on texture features

producing state of the art results in script identiﬁcation for both scene or

overlaid text images. They rely in hand-crafted texture features, a variant

of LBP, and a deep Multi Layer Perceptron to learn a metric space in which

they perform K-NN classiﬁcation.

In our previous work [10] we have proposed a patch-based method for

script identiﬁcation in scene text images. We used Convolutional features,

8

extracted from small image patches, and the Naive-Bayes Nearest Neighbour

classiﬁer (NBNN). We also presented a simple weighting strategy in order to

discover the most discriminative parts (or templates patches) per class in a

ﬁne-grained classiﬁcation approach.

In this paper we build upon our previous work [10] by extending it in two

ways: On one side, we make use of a much deeper Convolutional Neural Net-

work model. On the other hand, we replace the weighted NBNN classiﬁer by

a patch-based classiﬁcation rule that can be integrated in the CNN training

process by using an Ensemble of Conjoined Networks. This way, our CNN

model is able to learn at the same time expressive representations for image

patches and their relative contribution to the patch-based classiﬁcation rule.

From all reviewed methods the one proposed here is the only one based

in a patch-based classiﬁcation framework. Our intuition is that in cases

where holistic CNN models are not directly applicable, as in the case of text

images (because of their highly variable aspect ratios), the contribution of

rich parts descriptors without any deterioration (either by image distortion

or by descriptor quantization) is essential for correct image classiﬁcation.

In this sense our method is related with some CNN extensions that have

been proposed for video classiﬁcation. Unlike still images which can be

cropped and rescaled to a ﬁxed size, video sequences have a variable temporal

dimension and cannot be directly processed with a ﬁxed-size architecture. In

this context, 3D Convolutional Neural Networks [39, 40] have been proposed

to leverage the motion information encoded in multiple contiguous frames.

Basically the idea is to feed the CNN with a stack of a ﬁxed number of

consecutive frames and perform convolutions in both time and space dimen-

9

sions. Still these methods require a ﬁxed size input and thus they must be

applied several times through the whole sequence to obtain a chain of out-

puts that are then averaged [40] or fed into an Recurrent Neural Network [39]

to provide a ﬁnal decision. Karpathy et al. [41] also treat videos as bags of

short ﬁxed-length clips, but they investigate the use of diﬀerent temporal

connectivity patterns (early fusion, late fusion and slow fusion). To produce

predictions for an entire video they randomly sample 20 clips and take the

average of the network class predictions. While we share with these methods

the high-level goal of learning CNN weights from groups of stacked patches

(or frames) there are two key diﬀerences in the way we build our framework:

(1) the groups of patches that are fed into the network at training time are

randomly sampled and do not follow any particular order; and (2) at test

time we decouple the network to densely evaluate single patches and aver-

age their outputs. In other words, while in stacked-frame CNNs for video

recognition having an ordered sequence of input patches is crucial to learn

spatio-temporal features, our design aims to learn which are the most dis-

criminative patches in the input stack, independently of their relative spatial

arrangement.

In the experimental section we compare our method with some of the al-

gorithms reviewed in this section and demonstrate its superiority. Concretely

our approach improves the state-of-the-art in the SIW-13 [8] dataset for scene

text script classiﬁcation by a large margin of 5 percentage points, while per-

forms competitively in the CVSI-2015 [38] video overlaid-text dataset.

10

3. Patch-based classiﬁcation with Ensembles of Conjoined Net-

works

In our patch-based classiﬁcation method an input image is represented as

a collection of local descriptors, from patches extracted following a certain

sampling strategy. Those local features are then fed into a global classiﬁer

rule, that makes a decision for the input image.

3.1. Convolutional Neural Network for image-patch classiﬁcation

Given an input scene text image (i.e. a pre-segmented word or text line)

we ﬁrst resize it to a ﬁxed height of 40 pixels, but retaining its original

aspect ratio. Since scene text can appear in any possible combination of

foreground and background colors, we pre-process the image by converting it

into grayscale and centering pixel values. Then, we densely extract patches

at two diﬀerent scales, 32×32 and 40×40, by sliding a window with a step of

8 pixels. The particular values of these two window scales and step size was

found by cross-validation optimization as explained in section 4.2, and its

choice can be justiﬁed as follows: the 40 × 40 patch, covering the full height

of the resized image, is a natural choice in our system because it provides

the largest squared region we can crop; the 32 × 32 patches are conceived

for better scale invariance of the CNN model, similarly as the random crops

typically used for data augmentation in CNN-based image classiﬁcation [42].

Figure 4 shows the patches extracted from a given example image. This way

we build a large dataset of image patches that take the same label as the

image they were extracted from. With this dataset of patches we train a

CNN classiﬁer for the task of individual image patch classiﬁcation.

11

We use a Deep Convolutional Neural Network to build the expressive

image patch representations needed in our method. For the design of our

network we start from the CNN architecture proposed in [7] as it is known

to work well for script identiﬁcation. We then iteratively do an exhaustive

search to optimize by cross-validation the following CNN hyper-parameters:

number of convolutional and fully connected layers, number of ﬁlters per

layer, kernel sizes, and feature map normalisation schemes. The CNN archi-

tecture providing better performance in our experiments is shown in Figure 5.

Our CNN consists in three convolutional+pooling stages followed by an ex-

tra convolution and three fully connected layers. Details about the speciﬁc

conﬁguration and parameters are given in section 4.2.

At testing time, given a query scene text image the trained CNN model

is applied to image patches following the same sampling strategy described

before. Then, the individual CNN responses for each image patch can be fed

into the global classiﬁcation rule in order to make a single labeling decision

for the query image.

3.2. Training with an Ensemble of Conjoined Networks

Since the output of the CNN for an individual image patch is a probability

distribution over class labels, a simple global decision rule would be just to

average the responses of the CNN for all patches in a given query image:

y(I) =

CN N (xi)

1
nI

nI(cid:88)

i=1

(1)

where an image I takes the label with more probability in the averaged

softmax responses (y(I)) of their nI individual patches {x1, ..., xnI } outputs

12

on the CNN.

The problem with this global classiﬁcation rule is that the CNN weights

have been trained to solve a problem (individual patch classiﬁcation) that is

diﬀerent from the ﬁnal goal (i.e. classifying the whole query image). Besides,

it is based in a simplistic voting strategy for which all patches are assumed

to weight equally, i.e. no patches are more or less discriminative than others.

To overcome this we propose the use of an Ensemble of Conjoined Nets in

order to train the CNN for a task that resembles more the ﬁnal classiﬁcation

goal.

An Ensemble of Conjoined Nets (ECN), depicted in Figure 6, consists in

a set of identical networks that are joined at their outputs in order to provide

a unique classiﬁcation response. At training time the ECN is presented with

a set of N image patches extracted from the same image, thus sharing the

same label, and produces a single output for all them. Thus, to train an

ECN we must build a new training dataset where each sample consists in a

set of N patches with the same label (extracted from the same image).

ECNs take inspiration from Siamese Networks [43] but, instead of trying

to learn a metric space with a distance-based loss function, the individual

networks in the ECN are joined at their last fully connected layer (fc7 in

our case), which has the same number of neurons as the number of classes,

with a simple element-wise sum operation and thus we can use the standard

cross-entropy classiﬁcation loss. This way, the cross-entropy classiﬁcation

loss function of the ECN can be written in terms of the N individual patch

responses as follows:

13

E =

log(ˆpm,lm),

−1
M

M
(cid:88)

m=1

(cid:34) K
(cid:88)

k(cid:48)=1

N
(cid:88)

n=1

N
(cid:88)

n=1

ˆpm,k = exp(

xmnk)/

exp(

xmnk(cid:48))

(cid:35)

(2)

where M is the number of input samples in a mini-batch, ˆpm is the prob-

ability distribution over classes provided by the softmax function, lm is the

label of the m’th sample, N is the number of conjoined networks in the en-

semble, K is the number of classes, and xmnk ∈ [−∞, +∞] indicates the

response (score) of the k’th neuron in the n’th network for the m’th sample.

As can be appreciated in equation 2, in an ECN network a single input

patch contributes to the backpropagation error in terms of a global goal func-

tion for which it is not the only patch responsible. For example, even when

a single patch is correctly scored in the last fully connected layer it may be

penalized, and induced to produce a larger activation, if the other patches in

its same sample contribute to a wrong classiﬁcation at the ensemble output.

At test time, the CNN model trained in this way is applied to all image

patches in the query image and the global classiﬁcation rule is deﬁned as:

y(I) =

CN Nf c7(xi)

(3)

where an image I takes the label with the highest score in the sum (y(I))

of the fc7 layer responses of the nI individual patches {x1, ..., xnI }. This is

the same as in Equation 1 but using the fc7 layer responses instead of the

output softmax responses of the CNN.

Notice that still the task for which the ECN network has been trained is

nI(cid:88)

i=1

14

not exactly the same deﬁned by this global classiﬁcation rule, as the number

of patches nI is variable for each image and usually diﬀerent than the number

of conjoined networks N . However, it certainly resembles more the true

ﬁnal classiﬁcation goal. The number of conjoined networks N is an hyper-

parameter of the method that is largely dependent on the task to be solved

and is discussed in the experimental section.

4. Experiments

All reported experiments were conducted over three datasets, namely the

Video Script Identiﬁcation Competition (CVSI-2015) dataset1, the SIW-13

dataset2, and the MLe2e dataset3.

The CVSI-2015 [38] dataset comprises pre-segmented words in ten scripts:

English, Hindi, Bengali, Oriya, Gujrathi, Punjabi, Kannada, Tamil, Telegu,

and Arabic. The dataset contains about 1000 words for each script and is

divided into three parts: a training set ( 60% of the total images), a validation

set (10%), and a test set (30%). Text is extracted from various video sources

(news, sports etc.) and, while it contains a few instances of scene text, it

covers mainly overlay video text.

The SIW-13 datset [8] comprises 16291 pre-segmented text lines in thir-

teen scripts: Arabic, Cambodian, Chinese, English, Greek, Hebrew, Japanese,

Kannada, Korean, Mongolian, Russian, Thai, and Tibetan. The test set con-

tains 500 text lines for each script, 6500 in total, and all the other images

1http://www.ict.griffith.edu.au/cvsi2015/
2http://mc.eistar.net/~xbai/mspnProjectPage/
3http://github.com/lluisgomez/script_identification/

15

are provided for training. In this case, text was extracted from natural scene

images from Google Street View.

4.1. The MLe2e dataset

This paper introduces the ﬁrst dataset available up to date for the evalu-

ation of multi-lingual scene text end-to-end reading systems and all interme-

diate stages: text detection, script identiﬁcation, and text recognition. The

Multi-Language end-to-end (MLe2e) dataset has been harvested from vari-

ous existing scene text datasets for which the images and ground-truth have

been revised in order to make them homogeneous. The original images come

from the following datasets: Multilanguage(ML) [44] and MSRA-TD500 [45]

contribute Latin and Chinese text samples, Chars74K [46] and MSRRC [47]

contribute Latin and Kannada samples, and KAIST [48] contributes Latin

and Hangul samples.

In order to provide a homogeneous dataset, all images have been resized

proportionally to ﬁt in 640 × 480 pixels, which is the default image size

of the KAIST dataset. Moreover, the groundtruth has been revised to en-

sure a common text line annotation level [49]. During this process human

annotators were asked to review all resized images, adding the script class la-

bels and text transcriptions to the groundtruth, and checking for annotation

consistency: discarding images with unknown scripts or where all text is un-

readable (this may happen because images were resized); joining individual

word annotations into text line level annotations; discarding images where

correct text line segmentation is not clear or cannot be established, and im-

ages where a bounding box annotation contains more than one script (this

happens very rarely e.g.

in trademarks or logos) or where more than half

16

of the bounding box is background (this may happen with heavily slanted

or curved tex). Arabic numerals (0, .., 9), widely used in combination with

many (if not all) scripts, are labeled as follows. A text line containing text

and Arabic numerals is labeled as the script of the text it contains, while a

text line containing only Arabic numerals is labeled as Latin.

The MLe2e dataset contains a total of 711 scene images covering four

diﬀerent scripts (Latin, Chinese, Kannada, and Hangul) and a large vari-

ability of scene text samples. The dataset is split into a train and a test

set with 450 and 261 images respectively. The split was done randomly, but

in a way that the test set contains a balanced number of instances of each

class (aprox. 160 text lines samples of each script), leaving the rest of the

images for the train set (which is not balanced by default). The dataset is

suitable for evaluating various typical stages of end-to-end pipelines, such as

multi-script text detection, joint detection and script identiﬁcation, end-to-

end multi-lingual recognition, and script identiﬁcation in pre-segmented text

lines. For the latter, the dataset also provides the cropped images with the

text lines corresponding to each data split: 1178 and 643 images in the train

and test set respectively.

While being a dataset that has been harvested from a mix of existing

datasets it is important to notice that building it has supposed an important

annotation eﬀort: since some of the original datasets did not provide text

transcriptions, and/or where annotated at diﬀerent granularity levels. More-

over, despite the fact that the number of languages in the dataset is rather

limited (four scripts) it is the ﬁrst public dataset that covers the evaluation

of all stages of multi-lingual end-to-end systems for scene text understanding

17

in natural scenes. We think this is an important contribution of this paper

and hope the dataset will be useful to other researchers in the community.

4.2. Implementation details

In this section we detail the architectures of the network models used in

this paper, as well as the diﬀerent hyper-parameter setups that can be used

to reproduce the results provided in following sections. In all our experiments

we have used the open source Caﬀe [50] framework for deep learning running

on commodity GPUs. Source code and compatible Caﬀe models are made

publicly available4.

We have performed exhaustive experiments by varying many of the pro-

posed methods parameters, training multiple models, and choosing the one

with best cross-validation performance on the SIW-13 training set. The fol-

lowing parameters were tuned in this procedure: the size and step of the

sliding window, the base learning rate, the number of convolutional and fully

connected layers, the number of nodes in all layers, the convolutional kernel

sizes, and the feature map normalisation schemes

This way, the best basic CNN model found for individual image patch

classiﬁcation is described in section 3.1 and Figure 5, and has the following

per layer conﬁguration:

• Input layer: single channel 32 × 32 image patch.

• conv1 layer: 96 ﬁlters with size 5 × 5. Stride=1, pad=0. Output size:

96 × 28 × 28.

• pool1 layer: kernel size=3, stride=2, pad=1. Otput size: 96 × 15 × 15.

4http://github.com/lluisgomez/script_identification/

18

• conv2 layer: 256 ﬁlters with size 3 × 3. Stride=1, pad=0. Output size:

• pool2 layer: kernel size=3, stride=2, pad=1. Otput size: 256 × 7 × 7.

• conv3 layer: 384 ﬁlters with size 3 × 3. Stride=1, pad=0. Output size:

256 × 13 × 13.

384 × 5 × 5.

• pool3 layer: kernel size=3, stride=2, pad=1. Otput size: 384 × 3 × 3.

• conv4 layer: 512 ﬁlters with size 1 × 1. Stride=1, pad=0. Output size:

512 × 3 × 3.

• fc5 layer: 4096 neurons.

• fc6 layer: 1024 neurons.

• fc7 layer: N neurons, where N is the number of classes.

• SoftMax layer: Output a probability distribution over the N class la-

bels.

The total number of parameters of the network is ≈ 24M for the N = 13

case in the SIW-13 dataset. All convolution and fully connected layers use

Rectiﬁed Linear Units (ReLU). In conv1 and conv2 layers we perform normal-

ization over input regions using Local Response Normalization (LRN) [51].

At training time, we use dropout [52] (with a 0.5 ratio) in fc5 and fc6 layers.

To train the basic network model we use Stochastic Gradient Descent

(SGD) with momentum and L2 regularization. We use mini-batches of 64

images. The base learning rate is set to 0.01 and is decreased by a factor of

×10 every 100k iterations. The momentum weight parameter is set to 0.9,

and the weight decay regularization parameter to 5 × 10−4.

When training for individual patch classiﬁcation, we build a dataset of

small patches extracted by dense sampling the original training set images,

19

as explained in section 3.1. Notice that this produces a large set of patch

samples, e.g. in the SIW-13 dataset the number of training samples is close to

half million. With these numbers the network converges after 250k iterations.

In the case of the Ensemble of Conjoined Networks the basic network

detailed above is replicated N times, and all replicas are tied at their fc7

outputs with an element-wise sum layer which is connected to a single output

SoftMax layer. All networks in the ECN share the same parameters values.

Training the ECN requires a dataset where each input sample is com-

posed by N image patches. We generate this dataset as follows: given an

input image we extract patches the same way as for the simple network, then

we generate random N −combinations of the image patches, allowing repeti-

tions if the number of patches is < N . Notice that this way the number of

samples can be increased up to very large-scale numbers because the number
(cid:1) when the number of patches in
of possible diﬀerent N −combinations is (cid:0)M

N

a given image M is larger that the number of conjoined nets N , which is

the usual case. This is an important aspect of ECNs, as the training dataset

generation process becomes a data augmentation technique in itself. We can

see this data augmentation process as generating new small text instances

that are composed from randomly chosen parts of their original generators.

However, it is obviously non-practical to use all possible combinations for

training; thus, in order to get a manageable number of samples, we have used

the simple rule of generating 2 × M samples per input, which for example in

the SIW-13 dataset would produce around one million samples.

In terms of computational training complexity, the ECN has an impor-

tant drawback compared to the simple network model: the number of com-

20

putations is multiplied by N in each forward pass, similarly the amount of

memory needed is linearly increased by N . To overcome this limitation, we

use a ﬁne-tuning approach to train ECNs. First, we train the simple network

model, and then we do ﬁne-tuning on the ECN parameters starting from the

values learned using the simple net. When ﬁne-tuning, we have found that

starting from a fully converged network in the single-patch classiﬁcation task

we reach a local minimum of the global task, thus providing zero loss in most

(if not all) the iterations and not allowing the network to learn anything new.

In order to avoid this local minima situation we start the ﬁne-tuning from

a non-converged network (more or less at about 90/95% of the attainable

individual patch classiﬁcation accuracy).

Using ﬁne-tuning with a base learning rate of 0.001 (decreasing ×10 every

10k iterations) the ECN converges much faster, in the order of 35k iterations.

All other learning parameters are set the same as in the simple network

training setup.

The number of nets N in the ensemble can be seen as an extra hyper-

parameter in the ECN learning algorithm. Intuitively a dataset with larger

text sequences would beneﬁt from larger N values, while on the contrary

in the extreme case of classifying small squared images (i.e. each image is

represented by a single patch) any value of N > 1 does not make sense. Since

our datasets contain text instances with variable length a possible procedure

to select the optimal value of N is by using a validation set. We have done

experiments in the SIW-13 dataset by dividing the provided train set and

keeping 10% for validation. Classiﬁcation accuracy on the validation set for

various N values are shown in Figure 7. As can be appreciated the positive

21

impact of training with an ensemble of networks is evident for small values

of N , and mostly saturated for values N > 9. In the following we use a value

of N = 10 for all the remaining experiments.

4.3. Script identiﬁcation in pre-segmented text lines

In this section we study the performance of the proposed method for

script identiﬁcation in pre-segmented text lines. Table 1 shows the overall

performance comparison of our method with the state-of-the-art in CVSI-

2015, SIW-13, and MLe2e datasets. Figure 8 shows the confusion matrices

for our method in all three datasets with detailed per class classiﬁcation

results.

In Table 1 we also provide comparison with three well known image

recognition pipelines using Scale Invariant Features [53] (SIFT) in three dif-

ferent encodings: Fisher Vectors, Vector of Locally Aggregated Descriptors

(VLAD), and Bag of Words (BoW); and a linear SVM classiﬁer. In all base-

lines we extract SIFT features at four diﬀerent scales in sliding window with

a step of 8 pixels. For the Fisher vectors we use a 256 visual words GMM, for

VLAD a 256 vector quantized visual words, and for BoW 2,048 vector quan-

tized visual words histograms. The step size and number of visual words were

set to similar values to our method when possible in order to oﬀer a fair eval-

uation. These three pipelines have been implemented with the VLFeat [54]

and liblinear [55] open source libraries. The entry “Sequence-based CNN”

in Table 1 corresponds to the results obtained with the early fusion design

proposed in [41] with a stack of 5 consecutive patches.

As shown in Table 1 the proposed method outperforms state of the art

and all baseline methods in the SIW-13 and MLe2e scene text datasets, while

22

Method

SIW-13 MLe2e CVSI

This work - Ensemble of Conjoined Nets

94.8

94.4

This work - Simple CNN (Avg.)

This work - Simple CNN (fc5+SVM)

Shi et al. [8]

HUST [7, 38]

Google [38]

Nicolaou et al. [9]

Gomez et al. [10]

CVC-2 [10, 38]

SRS-LBP + KNN [56]

C-DAC [38]

CUK [38]

97.2

96.7

96.9

94.3

96.69

98.91

98.18

93.1

93.6

-

-

-

-

-

-

91.12

97.91

88.16

96.0

82.71

94.20

84.66

74.06

88.63

94.11

90.19

93.92

86.45

84.38

89.80

93.62

92.8

93.4

89.4

88.0

83.7

76.9

-

-

-

-

-

90.7

89.2

83.4

88.9

Baseline SIFT + Fisher Vectors + SVM

Baseline SIFT + VLAD + SVM

Baseline SIFT + Bag of Words + SVM

Baseline Sequence-based CNN [41] (Early fusion)

Table 1: Overall classiﬁcation performance comparison with state-of-the-art

in three diﬀerent datasets: SIW-13 [8], MLe2e, and CVSI [38].

23

performing competitively in the case of CVSI video overlay text dataset. In

the SIW-13 dataset the proposed method signiﬁcantly outperforms the best

performing method known up to date by more than 4 percentual points.

The entry “Simple CNN (fc5+SVM)” (third row) in Table 1 corresponds

to the results obtained with a linear SVM classiﬁer by using features ex-

tracted from the “Simple CNN” network. For this experiment we represent

each image in the dataset with a ﬁxed length vector with the averaged out-

puts of all its patches in the fc5 layer of the network. Then we train a linear

SVM classiﬁer using cross-validation on the training set, and show the classi-

ﬁcation performance on the test set. Similar results (or slightly worse) have

been found for features extracted from other layers (fc7, fc6, conv4) and us-

ing other linear classiﬁers (e.g.

logistic regression). When compared with

the “Simple CNN” approach we appreciate that classiﬁcation performance

is better for this combination (fc5+SVM). This conﬁrms the intuition that

classiﬁcation performance can be improved by optimizing the combination

of the results for the individual patches. However, the performance of the

CNN trained with the ensemble of conjoined networks is still better. As men-

tioned earlier, the additional beneﬁt of our approach here is in the end-to-end

learning of both the visual features and the optimal combination scheme for

classiﬁcation.

The contribution of training with ensembles of conjoined nets is consistent

in all three evaluated datasets but more notable on SIW-13, as appreciated by

comparing the ﬁrst two rows of Table 1 which correspond to the nets trained

with the ensemble (ﬁrst row) and the simple model (second row). This com-

parison can be further strengthened by testing if the provided improvement is

24

statistically signiﬁcant. For this we use the within-subjects chi-squared test

(McNemar’s test) [57] to compare the predictive accuracy of the two models.

The obtained p-values on the SIW-13, MLe2e, and CVSI datasets are respec-

tively 1.4 × 10−16, 0.057, and 0.0026. In the case of the SIW-13 dataset the

p-value is way smaller than the assumed signiﬁcance threshold (α = 0.05),

thus we can reject the null-hypothesis that both models perform equally well

on this dataset and certify a statistically signiﬁcant improvement. On the

other hand we appreciate a marginal improvement on the other two datasets.

Our interpretation of the results on CVSI and MLe2e datasets in compar-

ison with the ones obtained on SIW-13 relates to its distinct nature. On one

hand the MLe2e dataset covers only four diﬀerent scripts (Latin, Chinese,

Kannada, and Hangul) for which the inter-class similarity does not represent

a real problem. On the other hand, the CVSI overlaid-text variability and

clutter is rather limited compared with that found in the scene text of MLe2e

and SIW-13. As can be appreciated in Figure 9 overlaid-text is usually bi-

level without much clutter. Figure 10 shows another important characteristic

of CVSI dataset: since cropped words in the dataset belong to very long sen-

tences of overlay text in videos, e.g. from rotating headlines, it is common to

ﬁnd a few dozens of samples sharing exactly the same font and background

both in the train and test sets. This particularity makes the ECN network

not really helpful in the case of CVSI, as the data augmentation by image

patches recombinations is somehow already implicit on the dataset.

Furthermore, the CVSI-2015 competition winner (Google) makes use of a

deep convolutional network but applies a binarization pre-processing to the

input images. In our opinion this binarization may not be a realistic pre-

25

processing in general for scene text images. As an example of this argument

one can easily see in Figure 9 that binarization of scene text instances is

not trivial as in overlay text. Similar justiﬁcation applies to other methods

performing better than ours in CVSI. In particular the LBP features used

in [9], as well as the patch-level whitening used in our previous work [10], may

potentially take advantage of the simpler, bi-level, nature of text instances

in CVSI dataset. It is important to notice here that these two algorithms,

corresponding to our previous works in script identiﬁcation, have close num-

bers to the Google ones in CVSI-2015 (see Table 1) but perform quite bad

in SIW-13.

As a conclusion of the experiments performed in this section we can say

that the improvement of training a patch-based CNN classiﬁer with an en-

semble of conjoined nets is especially appreciable in cases where we have a

large number of classes, with large inter-class similarity, and cluttered scene

images, as is the case of the challenging SIW-13 dataset. This demonstrates

our initial claim that a powerful script identiﬁcation method for scene text

images must be based in learning good local patch representations, and also

their relative importance in the global image classiﬁcation task. Figure 11

shows some examples of challenging text images that are correctly classiﬁed

by our method but not with the Simple CNN approach. Figure 12 shows a

set of missclassiﬁed images.

Finally, in Figure 13 we show the classiﬁcation accuracy of the CNN

trained with ensembles of conjoined nets as a function of the image width

on SIW-13 test images. We appreciate that the method is robust even for

small text images which contain a limited number of unique patches. Com-

26

putation time for our method is also dependent on the input image length

and ranges from 4ms.

in for the smaller images up to 23ms. for the larger

ones. The average computation time on the SIW-13 test set is of 13ms using

a commodity GPU. At test time computation is made eﬃcient by stacking

all patches of the input image in a single mini-batch.

4.4. Joint text detection and script identiﬁcation in scene images

In this experiment we evaluate the performance of a complete pipeline for

detection and script identiﬁcation in its joint ability to detect text lines in

natural scene images and properly recognizing their scripts. The key interest

of this experiment is to study the performance of the proposed script identi-

ﬁcation algorithm when realistic, non-perfect, text localisation is available.

Most text detection pipelines are trained explicitly for a speciﬁc script

(typically English) and generalise pretty badly to the multi-script scenario.

We have chosen to use here our previously published script-agnostic method [58],

which is designed for multi-script text detection and generalises well to any

script. The method detects character candidates using the Maximally Sta-

ble Extremal Regions (MSER) [59] algorithm, and builds diﬀerent hierar-

chies where the initial regions are grouped by agglomerative clustering, using

complementary similarity measures. In such hierarchies each node deﬁnes a

possible text hypothesis. Then, an eﬃcient classiﬁer, using incrementally

computable descriptors, is used to walk each hierarchy and select the nodes

with larger text-likelihood.

In this paper script identiﬁcation is performed at the text line level, be-

cause segmentation into words is largely script-dependent, and not meaning-

ful in Chinese/Korean scripts. Notice however that in some cases, by the

27

intrinsic nature of scene text, a text line provided by the text detection mod-

ule may correspond to a single word, so we must deal with a large variability

in the length of provided text lines. The experiments are performed over the

new MLe2e dataset.

For evaluation of the joint text detection and script identiﬁcation task in

the MLe2e dataset we propose the use of a simple two-stage evaluation frame-

work. First, localisation is assessed based on the Intersection-over-Union

(IoU) metric between detected and ground-truth regions, as commonly used

in object detection tasks [60] and the recent ICDAR 2015 Robust Read-

ing Competition5 [61]. Second, the predicted script is veriﬁed against the

ground-truth. A detected bounding box is thus considered correct if it has a

IoU > 0.5 with a bounding box in the ground-truth and the predicted script

is correct.

The localisation-only performance, corresponding to the ﬁrst stage of the

evaluation, yields an F-score of 0.63 (Precision of 0.57 and Recall of 0.69).

This deﬁnes the upper-bound for the joint task. The two stage evaluation,

including script identiﬁcation, of the proposed method compared with our

previous work is shown in Table 2.

Intuitively the proposed method for script identiﬁcation is eﬀective even

when the text region is badly localised, as long as part of the text area is

within the localised region. To support this argument we have performed an

additional experiment where our algorithm is applied to cropped regions from

pre-segmented text images. For this, we take the SIW-13 original images and

5http://rrc.cvc.uab.es

28

Method

Correct Wrong Missing Precision Recall F-score

This work - ECN

Gomez et al. [10]

395

364

376

407

245

278

0.51

0.47

0.62

0.56

0.57

0.52

Table 2: Text detection and script identiﬁcation performance in the MLe2e

dataset.

calculate the performance of our method when applied to cropped regions of

variable length, up to the minimum size possible (40 × 40 pixels). As can

be appreciated in Figure 14 the experiment demonstrates that the proposed

method is eﬀective even when small parts of the text lines are provided.

Such a behaviour is to be expected, due to the way our method treats local

information to decide on a script class. In the case of the pipeline for joint

detection and script identiﬁcation, this extends to regions that did not pass

the 0.5 IoU threshold, but had their script correctly identiﬁed. This opens

the possibility to make use of script identiﬁcation to inform and / or improve

the text localisation process. The information of the identiﬁed script can be

used to reﬁne the detections.

4.5. End-to-end multi-lingual recognition in scene images

In this section we evaluate the performance of a complete pipeline for

end-to-end multi-lingual recognition in scene images. For this, we combine

the pipeline used in the previous section with a well known oﬀ-the-shelf OCR

engine: the open source project Tesseract6 [62]. Similar pipelines [63, 64, 65]

6http://code.google.com/p/tesseract-ocr/

29

using oﬀ-the-shelf OCR engines have demonstrated state-of-the-art end-to-

end performance in English-only datasets up to very recently, provided that

the text detection module is able to produce good pixel-level segmentation

of text.

The setup of the OCR engine in our pipeline is minimal: given a text

detection hypothesis from the detection module we set the recognition lan-

guage to the one provided by the script identiﬁcation module, and we set the

OCR to interpret the input as a single text line. Apart from that we use the

default Tesseract parameters.

The recognition output is ﬁltered with a simple post-processing junk ﬁlter

in order to eliminate garbage recognitions, i.e. sequences of identical charac-

ters like ”IIii” that may appear as the result of trying to recognize repetitive

patterns in the scene. Concretely, we discard the words in which more than

half of their characters are recognized as one of ”i”, ”l”, ”I”, or other special

characters like: punctuation marks, quotes, exclamation, etc. We also reject

those detections for which the recognition conﬁdence provided by the OCR

engine is under a certain threshold.

The evaluation protocol is similar to the one used in other end-to-end

scene text recognition datasets [66, 61]. Ideally, in end-to-end word recogni-

tion, a given output word is considered correct if it overlaps more than 0.5

with a ground-truth word and all its characters are recognized correctly (case

sensitive). However, since in the case of the MLe2e dataset we are evaluating

text lines instead of single words, we relax a bit this correctness criteria by

allowing the OCR output to to make 1

8 character level errors. This relax-
ation is motivated by the fact that for a given test sentence with more than

30

Script identiﬁcation Correct Wrong Missing Precision Recall F-score

This work - ECN

Gomez et al. [10]

Tesseract

96

82

50

212

211

93

503

517

549

0.31

0.28

0.35

0.16

0.21

0.14

0.08

0.18

0.13

Table 3: End-to-end multi-lingual recognition performance in the MLe2e

dataset.

1
8.

test set.

8 characters (e.g. with two words) having only one character mistake may

still produce a partial understanding of the text (e.g. one of the words is

correct), and thus must not be penalized the same way as if all characters

are wrongly recognized. This way, a given output text line is considered cor-

rect if overlaps more than 0.5 with a ground-truth text line and their edit

distance divided by the number of characters of the largest is smaller than

Table 3 shows a comparison of the proposed end-to-end pipeline by using

diﬀerent script identiﬁcation modules: the method presented in this paper,

our previously published work, and Tesseract’s built-in alternative. Figure 15

shows the output of our full end-to-end pipeline for some images in the MLe2e

Tesseract method in Table 3 refers to the use of Tesseract’s own script es-

timation algorithm [1]. We have found that Tesseract’s algorithm is designed

to work with large corpses of text (e.g. full page documents) and does not

work well for the case of single text lines.

Results in Table 3 demonstrate the direct correlation between having

31

better script identiﬁcation rates and better end-to-end recognition results.

The ﬁnal multi-lingual recognition f-score obtained (0.21) is far from the

state-of-the art in end-to-end recognition systems designed for English-only

environments [4, 5, 6]. As a fair comparison, a very similar pipeline using

the Tesseract OCR engine [64] achieves an f-score of 0.40 in the ICDAR

English-only dataset. The lower performance obtained in MLe2e dataset

stems from a number of challenges that are speciﬁc to its multi-lingual nature.

For example, in some scripts (e.g. Chinese and Kannada) glyphs are many

times non single-body regions, composed by (or complemented with) small

strokes that in many cases are lost in the text segmentation stage. In such

cases having a bad pixel-level segmentation of text would make it practically

impossible for the OCR engine to produce a correct recognition.

Our pipeline results represent the ﬁrst reference result for multi-lingual

scene text recognition and a ﬁrst benchmark from which better systems can

be built, e.g. replacing the oﬀ-the-shelf OCR engine by other recognition

modules better suited for scene text imagery.

4.6. Cross-domain performance and confusion in single-language datasets

In this experiment we evaluate the cross-domain performance of learned

CNN weights from one dataset to the other. For example, we evaluate on

the MLe2e and CVSI test sets using the network trained with the SIw-13

train set, by measuring classiﬁcation accuracy only for their common script

classes: Arabic, English, and Kannada in CVSI; Chinese, English, Kannada,

and Korean in MLe2e. Finally, we evaluate the misclassiﬁcation error of our

method (trained in diﬀerent datasets) over two single-script datasets. For

this experiment we use the ICDAR2013 [67] and ALIF [68] datasets, which

32

provide cropped word images of English scene text and Arabic video overlaid

text respectively. Table 4 shows the results of these experiments.

Method

SIW-13 MLe2e CVSI

ICDAR ALIF

ECN CNN (SIW-13)

94.8

86.8

90.6

74.7

100

ECN CNN (MLe2e)

90.8

94.4

98.3

95.3

-

ECN CNN (CVSI)

42.3

43.5

97.2

65.2

91.8

Table 4: Cross-domain performance of our method measured by train-

ing/testing in diﬀerent datasets.

Notice that results in Table 4 are not directly comparable among rows

because each classiﬁer has been trained with a diﬀerent number of classes,

thus having diﬀerent rates for a random choice classiﬁcation. However, the

experiment serves as a validation of how good a given classiﬁer is in perform-

ing with data that is distinct in nature to the one used for training. In this

sense, the obtained results show a clear weakness when the model is trained

on the video overlaid text of CVSI and subsequently applied to scene text

images (SIW-13, MLe2e, and ICDAR). On the contrary, models trained on

scene text datasets are quite stable in other scene text data, as well as in

video overlaid text (CVSI and ALIF).

In fact, this is an expected result, because the domain of video overlay

text can be seen as a sub-domain of the scene text domain. Since the scene

text datasets are richer in text variability, e.g. in terms of perspective distor-

tion, physical appearance, variable illumination, and typeface designs, script

33

identiﬁcation on these datasets is a more diﬃcult problem, and their data

is more indicated if one wants to learn eﬀective cross-domain models. This

demonstrates that our method is able to learn discriminative stroke-part rep-

resentations that are not dataset-speciﬁc, and provides evidence to the claims

made in section 4.3 when interpreting the obtained results in CVSI dataset

comparing with other methods that may be more engineered to the speciﬁc

CVSI data but not generalizing well in scene text datasets.

5. Conclusion

A novel method for script identiﬁcation in natural scene images was pre-

sented. The method is based on the use of ensembles of conjoined convo-

lutional networks to jointly learn discriminative stroke-part representations

and their relative importance in a patch-based classiﬁcation scheme. Exper-

iments performed in three diﬀerent datasets exhibit state of the art accuracy

rates in comparison to a number of state-of-the-art methods, including the

participants in the CVSI-2015 competition and three standard image classi-

ﬁcation pipelines.

In addition, a new public benchmark dataset for the evaluation of all

stages of multi-lingual end-to-end scene text reading systems was introduced.

Our work demonstrates the viability of script identiﬁcation in natural

scene images, paving the road towards true multi-lingual end-to-end scene

text understanding.

34

This project was supported by the Spanish project TIN2014-52072-P,

the fellowship RYC-2009-05031, and the Catalan government scholarship

Acknowledgment

2014FI B1-0017.

References

[1] R. Unnikrishnan, R. Smith, Combined script and page orientation esti-

mation using the tesseract ocr engine, in: Proceedings of the Interna-

tional Workshop on Multilingual OCR, ACM, 2009, p. 6.

[2] D. Ghosh, T. Dube, A. P. Shivaprasad, Script recognitiona review, Pat-

tern Analysis and Machine Intelligence, IEEE Transactions on 32 (12)

(2010) 2142–2161.

[3] U. Pal, B. Chaudhuri, Indian script character recognition: a survey,

pattern Recognition 37 (9) (2004) 1887–1899.

[4] A. Bissacco, M. Cummins, Y. Netzer, H. Neven, Photoocr: Reading text

in uncontrolled conditions, in: Proceedings of the IEEE International

Conference on Computer Vision, 2013, pp. 785–792.

[5] M. Jaderberg, A. Vedaldi, A. Zisserman, Deep features for text spotting,

in: Computer Vision–ECCV 2014, Springer, 2014, pp. 512–528.

[6] L. Neumann, J. Matas, Real-time lexicon-free scene text localization

and recognition, IEEE Transactions on Pattern Analysis and Machine

Intelligence PP (99) (2015) 1–1.

35

[7] B. Shi, C. Yao, C. Zhang, X. Guo, F. Huang, X. Bai, Automatic script

identiﬁcation in the wild, in: Document Analysis and Recognition (IC-

DAR), 2015 13th International Conference on, IEEE, 2015, pp. 531–535.

[8] B. Shi, X. Bai, C. Yao, Script identiﬁcation in the wild via discriminative

convolutional neural network, Pattern Recognition 52 (2016) 448–458.

[9] A. Nicolaou, A. D. Bagdanov, L. Gomez-Bigorda, D. Karatzas, Visual

script and language recognition, in: DAS, 2016.

[10] L. Gomez-Bigorda, D. Karatzas, A ﬁne-grained approach to scene text

script identiﬁcation, in: DAS, 2016.

[11] S. Tian, U. Bhattacharya, S. Lu, B. Su, Q. Wang, X. Wei, Y. Lu, C. L.

Tan, Multilingual scene character recognition with co-occurrence of his-

togram of oriented gradients, Pattern Recognition 51 (2016) 125 – 134.

[12] J. Gllavata, B. Freisleben, Script recognition in images with complex

backgrounds, in: Signal Processing and Information Technology, 2005.

Proceedings of the Fifth IEEE International Symposium on, IEEE, 2005,

pp. 589–594.

[13] P. Shivakumara, Z. Yuan, D. Zhao, T. Lu, C. L. Tan, New gradient-

spatial-structural features for video script identiﬁcation, Computer Vi-

sion and Image Understanding 130 (2015) 35–53.

[14] A. Coates, A. Y. Ng, H. Lee, An analysis of single-layer networks in

unsupervised feature learning, in: International conference on artiﬁcial

intelligence and statistics, 2011, pp. 215–223.

36

[15] O. Boiman, E. Shechtman, M. Irani, In defense of nearest-neighbor

based image classiﬁcation, in: Computer Vision and Pattern Recog-

nition, 2008. CVPR 2008. IEEE Conference on, IEEE, 2008, pp. 1–8.

[16] A. L. Spitz, M. Ozaki, Palace: A multilingual document recognition

system, in: Document Analysis Systems, Vol. 1, Singapore: World Sci-

entiﬁc, 1995, pp. 16–37.

[17] A. L. Spitz, Determination of the script and language content of docu-

ment images, Pattern Analysis and Machine Intelligence, IEEE Trans-

actions on 19 (3) (1997) 235–245.

[18] D. Lee, C. R. Nohl, H. S. Baird, Language identiﬁcation in complex, un-

oriented, and degraded document images, Series in Machine Perception

And Artiﬁcial Intelligence 29 (1998) 17–39.

[19] B. Waked, S. Bergler, C. Suen, S. Khoury, Skew detection, page segmen-

tation, and script classiﬁcation of printed document images, in: Systems,

Man, and Cybernetics, 1998. 1998 IEEE International Conference on,

Vol. 5, IEEE, 1998, pp. 4470–4475.

[20] S. Chaudhury, R. Sheth, Trainable script identiﬁcation strategies for

indian languages, in: Document Analysis and Recognition, 1999. IC-

DAR’99. Proceedings of the Fifth International Conference on, IEEE,

1999, pp. 657–660.

[21] J. Hochberg, L. Kerns, P. Kelly, T. Thomas, Automatic script identiﬁca-

tion from images using cluster-based templates, in: Document Analysis

37

and Recognition, 1995., Proceedings of the Third International Confer-

ence on, Vol. 1, IEEE, 1995, pp. 378–381.

[22] S. L. Wood, X. Yao, K. Krishnamurthi, L. Dang, Language identiﬁcation

for printed text independent of segmentation, in:

Image Processing,

1995. Proceedings., International Conference on, Vol. 3, IEEE, 1995,

pp. 428–431.

[23] T. Tan, Rotation invariant texture features and their use in automatic

script identiﬁcation, Pattern Analysis and Machine Intelligence, IEEE

Transactions on 20 (7) (1998) 751–756.

[24] W. Chan, G. Coghill, Text analysis using local energy, Pattern Recog-

nition 34 (12) (2001) 2523 – 2532.

[25] W. Pan, C. Y. Suen, T. D. Bui, Script identiﬁcation using steerable ga-

bor ﬁlters, in: Document Analysis and Recognition, 2005. Proceedings.

Eighth International Conference on, IEEE, 2005, pp. 883–887.

[26] M. A. Ferrer, A. Morales, U. Pal, Lbp based line-wise script identiﬁ-

cation, in: Document Analysis and Recognition (ICDAR), 2013 12th

International Conference on, IEEE, 2013, pp. 369–373.

[27] A. K. Jain, Y. Zhong, Page segmentation using texture analysis, Pattern

Recognition 29 (5) (1996) 743 – 770.

[28] Z. Chi, Q. Wang, W.-C. Siu, Hierarchical content classiﬁcation and

script determination for automatic document image processing, Pattern

Recognition 36 (11) (2003) 2483–2500.

38

[29] A. Hennig, N. Sherkat, Exploiting zoning based on approximating

splines in cursive script recognition, Pattern Recognition 35 (2) (2002)

445 – 454.

3393.

[30] J. Schenk, J. Lenz, G. Rigoll, Novel script line identiﬁcation method

for script normalization and feature extraction in on-line handwritten

whiteboard note recognition, Pattern Recognition 42 (12) (2009) 3383 –

[31] G. Zhu, X. Yu, Y. Li, D. Doermann, Language identiﬁcation for hand-

written document images using a shape codebook, Pattern Recognition

42 (12) (2009) 3184 – 3191.

[32] S. Basu, N. Das, R. Sarkar, M. Kundu, M. Nasipuri, D. K. Basu, A novel

framework for automatic sorting of postal documents with multi-script

address blocks, Pattern Recognition 43 (10) (2010) 3507 – 3521.

[33] G. Zhong, M. Cheriet, Tensor representation learning based image patch

analysis for text identiﬁcation and recognition, Pattern Recognition

48 (4) (2015) 1211 – 1224.

[34] N. Sharma, S. Chanda, U. Pal, M. Blumenstein, Word-wise script iden-

tiﬁcation from video frames, in: Document Analysis and Recognition

(ICDAR), 2013 12th International Conference on, IEEE, 2013, pp. 867–

871.

[35] T. Q. Phan, P. Shivakumara, Z. Ding, S. Lu, C. L. Tan, Video script

identiﬁcation based on text lines, in: Document Analysis and Recog-

39

nition (ICDAR), 2011 International Conference on, IEEE, 2011, pp.

1240–1244.

[36] P. Shivakumara, N. Sharma, U. Pal, M. Blumenstein, C. L. Tan,

Gradient-angular-features for word-wise video script identiﬁcation, in:

2014 22nd International Conference on Pattern Recognition (ICPR),

IEEE, 2014, pp. 3098–3103.

[37] N. Sharma, R. Mandal, R. Sharma, U. Pal, M. Blumenstein, Bag-of-

visual words for word-wise video script identiﬁcation: A study, in: Neu-

ral Networks (IJCNN), 2015 International Joint Conference on, IEEE,

2015, pp. 1–7.

[38] N. Sharma, R. Mandal, R. Sharma, U. Pal, M. Blumenstein, Icdar2015

competition on video script identiﬁcation (cvsi 2015), in: Document

Analysis and Recognition (ICDAR), 2015 13th International Conference

on, IEEE, 2015, pp. 1196–1200.

[39] M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, A. Baskurt, Sequential

deep learning for human action recognition, in: International Workshop

on Human Behavior Understanding, Springer, 2011, pp. 29–39.

[40] S. Ji, W. Xu, M. Yang, K. Yu, 3d convolutional neural networks for

human action recognition, IEEE transactions on pattern analysis and

machine intelligence 35 (1) (2013) 221–231.

[41] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, L. Fei-

Fei, Large-scale video classiﬁcation with convolutional neural networks,

40

in: Proceedings of the IEEE conference on Computer Vision and Pattern

Recognition, 2014, pp. 1725–1732.

[42] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classiﬁcation with

deep convolutional neural networks, in: Advances in neural information

processing systems, 2012, pp. 1097–1105.

[43] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun, C. Moore,

E. S¨ackinger, R. Shah, Signature veriﬁcation using a siamese time de-

lay neural network, International Journal of Pattern Recognition and

Artiﬁcial Intelligence 7 (04) (1993) 669–688.

[44] Y.-F. Pan, X. Hou, C.-L. Liu, Text localization in natural scene images

based on conditional random ﬁeld, in: Document Analysis and Recog-

nition, 2009. ICDAR’09. 10th International Conference on, IEEE, 2009,

pp. 6–10.

[45] C. Yao, X. Bai, W. Liu, Y. Ma, Z. Tu, Detecting texts of arbitrary orien-

tations in natural images, in: Computer Vision and Pattern Recognition

(CVPR), 2012 IEEE Conference on, IEEE, 2012, pp. 1083–1090.

[46] T. E. de Campos, B. R. Babu, M. Varma, Character recognition in

natural images., in: VISAPP (2), 2009, pp. 273–280.

[47] D. Kumar, M. Prasad, A. Ramakrishnan, Multi-script robust reading

competition in icdar 2013,

in: Proceedings of the 4th International

Workshop on Multilingual OCR, ACM, 2013, p. 14.

[48] S. Lee, M. S. Cho, K. Jung, J. H. Kim, Scene text extraction with edge

41

constraint and text collinearity, in: Pattern Recognition (ICPR), 2010

20th International Conference on, IEEE, 2010, pp. 3983–3986.

[49] D. Karatzas, S. Robles, L. Gomez, An on-line platform for ground

truthing and performance evaluation of text extraction systems, in: Doc-

ument Analysis Systems (DAS), 2014 11th IAPR International Work-

shop on, IEEE, 2014, pp. 242–246.

[50] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,

S. Guadarrama, T. Darrell, Caﬀe: Convolutional architecture for fast

feature embedding, in: Proceedings of the ACM International Confer-

ence on Multimedia, ACM, 2014, pp. 675–678.

[51] K. Jarrett, K. Kavukcuoglu, M. Ranzato, Y. LeCun, What is the best

multi-stage architecture for object recognition?, in: Computer Vision,

2009 IEEE 12th International Conference on, IEEE, 2009, pp. 2146–

2153.

[52] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,

Dropout: A simple way to prevent neural networks from overﬁtting, The

Journal of Machine Learning Research 15 (1) (2014) 1929–1958.

[53] D. G. Lowe, Object recognition from local scale-invariant features, in:

Computer vision, 1999. The proceedings of the seventh IEEE interna-

tional conference on, Vol. 2, Ieee, 1999, pp. 1150–1157.

[54] A. Vedaldi, B. Fulkerson, VLFeat: An open and portable library of

computer vision algorithms, http://www.vlfeat.org/ (2008).

42

[55] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, C.-J. Lin, LIBLIN-

EAR: A library for large linear classiﬁcation, The Journal of Machine

Learning Research 9 (2008) 1871–1874.

[56] A. Nicolaou, A. D. Bagdanov, M. Liwicki, D. Karatzas, Sparse radial

sampling lbp for writer identiﬁcation, in: Document Analysis and Recog-

nition (ICDAR), 2015 13th International Conference on, IEEE, 2015, pp.

716–720.

[57] Q. McNemar, Note on the sampling error of the diﬀerence between corre-

lated proportions or percentages, Psychometrika 12 (2) (1947) 153–157.

[58] L. Gomez, D. Karatzas, A fast hierarchical method for multi-

script and arbitrary oriented scene text extraction, arXiv preprint

arXiv:1407.7504.

[59] J. Matas, O. Chum, M. Urban, T. Pajdla, Robust wide-baseline stereo

from maximally stable extremal regions, Image and vision computing

22 (10) (2004) 761–767.

[60] M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams, J. Winn,

A. Zisserman, The pascal visual object classes challenge: A retrospec-

tive, International Journal of Computer Vision 111 (1) (2015) 98–136.

[61] D. Karatzas, L. Gomez-Bigorda, A. Nicolaou, S. Ghosh, A. Bagdanov,

M. Iwamura, J. Matas, L. Neumann, V. R. Chandrasekhar, S. Lu, et al.,

Icdar 2015 competition on robust reading, in: Document Analysis and

Recognition (ICDAR), 2015 13th International Conference on, IEEE,

2015, pp. 1156–1160.

43

[62] R. Smith, An overview of the tesseract ocr engine, in: icdar, IEEE, 2007,

pp. 629–633.

[63] S. Milyaev, O. Barinova, T. Novikova, P. Kohli, V. Lempitsky, Image

binarization for end-to-end text understanding in natural images, in:

Document Analysis and Recognition (ICDAR), 2013 12th International

Conference on, IEEE, 2013, pp. 128–132.

[64] L. G´omez, D. Karatzas, Scene text recognition: No country for old men?,

in: Computer Vision-ACCV 2014 Workshops, Springer, 2014, pp. 157–

168.

[65] S. Milyaev, O. Barinova, T. Novikova, P. Kohli, V. Lempitsky, Fast and

accurate scene text understanding with image binarization and oﬀ-the-

shelf ocr, International Journal on Document Analysis and Recognition

(IJDAR) 18 (2) (2015) 169–182.

[66] K. Wang, B. Babenko, S. Belongie, End-to-end scene text recognition,

in: Computer Vision (ICCV), 2011 IEEE International Conference on,

IEEE, 2011, pp. 1457–1464.

[67] D. Karatzas, F. Shafait, S. Uchida, M. Iwamura, L. Gomez i Bigorda,

S. Robles Mestre, J. Mas, D. Fernandez Mota, J. Almazan Almazan, L.-

P. de las Heras, Icdar 2013 robust reading competition, in: Document

Analysis and Recognition (ICDAR), 2013 12th International Conference

on, IEEE, 2013, pp. 1484–1493.

[68] S. Yousﬁ, S.-A. Berrani, C. Garcia, Alif: A dataset for arabic embedded

text recognition in tv broadcast, in: Document Analysis and Recognition

44

(ICDAR), 2015 13th International Conference on, IEEE, 2015, pp. 1221–

1225.

45

(a)

(b)

Figure 4: The original scene text images (a) are converted to greyscale and

resized to a ﬁxed height (b) in order to extract small local patches with a

dense sampling strategy (c).

conv1 - pool1conv2 - pool2 conv3 - pool3

conv4

fc5 fc6 fc7

Figure 5: Network architecture of the CNN trained to classify individual

image patches. The network has three convolutional+pooling stages followed

by an extra convolution and three fully connected layers.

(c)

46

x1

x2

x3

Elementwise (cid:80)

xN

Figure 6: An Ensemble of Conjoined Nets consist in a set of identical net-

works that are joined at their outputs in order to provide a unique classiﬁ-

cation response.

Figure 7: Validation accuracy for various number of networks N in the en-

semble of conjoined networks model.

47

Figure 8: Confusion matrices with per class classiﬁcation accuracy of our

method in SIW-13, MLe2e, and CVSI datasets.

Figure 9: Overlaid-text samples (top row) variability and clutter is rather

limited compared with that found in the scene text images (bottom row).

Figure 10: Cropped words in the CVSI dataset belong to very long sentences

of overlay text in videos. It is common to ﬁnd several samples sharing exactly

the same font and background both in the train (top row) and test (bottom

row) sets.

Figure 11: Examples of challenging text images that are correctly classiﬁed

by our ECN method but not with the Simple CNN approach.

48

Figure 12: A selection of misclassiﬁed samples by our method: low contrast

images, rare font types, degraded text, letters mixed with numerals, etc.

Figure 13: Classiﬁcation accuracy of the CNN trained with ensembles of

conjoined nets (top) and number of images (bottom) as a function of the

image width on SIW-13 test images.

Figure 14: Classiﬁcation error of our method when applied to variable length

cropped regions of SIW-13 images, up to the minimum size possible (40 × 40

pixels).

49

Figure 15: End-to-end recognition of text from images containing textual

information in diﬀerent scripts/languages.

50

