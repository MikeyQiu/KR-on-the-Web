UFSAC: Uniﬁcation of Sense Annotated Corpora and Tools

Lo¨ıc Vial, Benjamin Lecouteux, Didier Schwab
LIG – GETALP – Univ. Grenoble Alpes – France
{loic.vial, benjamin.lecouteux, didier.schwab}@imag.fr

Abstract
In Word Sense Disambiguation, sense annotated corpora are often essential for evaluating a system and also valuable in order to reach a
good efﬁciency. Always created for a speciﬁc purpose, there are today a dozen of sense annotated English corpora, in various formats
and using different versions of WordNet. The main hypothesis of this work is that it should be possible to build a disambiguation system
by using any of these corpora during the training phase or during the testing phase regardless of their original purpose. In this article, we
present UFSAC: a format of corpus that can be used for either training or testing a disambiguation system, and the process we followed
for constructing this format. We give to the community the whole set of sense annotated English corpora that we know, in this uniﬁed
format, when the copyright allows it, with sense keys converted to the last version of WordNet. We also provide the source code for
building these corpora from their original data, and a complete Java API for manipulating corpora in this format. The whole resource is
available at the following URL: https://github.com/getalp/UFSAC.

Keywords: Word Sense Disambiguation, sense annotated corpora, uniﬁed resource, tools

1.

Introduction

Whether they are used for the evaluation or for the learn-
ing process of a Word Sense Disambiguation (WSD) sys-
tem, the importance of sense annotated corpora in Natural
Language Processing (NLP) is considerable. On one hand,
the evaluation in vivo, i.e.
the evaluation of a WSD sys-
tem as part of a larger task, has never been really exploited.
On the other hand, the evaluation in vitro, which uses di-
rectly sense annotated corpora by comparing the output of
a system to manual annotations, is predominant. Moreover,
WSD systems exploiting examples from sense annotated
corpora are generally far better than those which do not
(Navigli et al., 2007; Moro and Navigli, 2015).
At the time of its creation, WordNet (Miller, 1995) was un-
doubtedly the only lexical database freely available for En-
glish. Since the beginning of the 2000s, it has become the
de facto standard for WSD in this language. Indeed, most of
sense annotated corpora are either directly annotated with
WordNet sense keys or they are annotated with a sense in-
ventory linked to the senses of WordNet, such as BabelNet
(Navigli and Ponzetto, 2010).
However, it is not trivial to use these corpora, because most
of them differ in their format and on the version of Word-
Net they use. As a consequence, very few works in the lit-
erature of WSD are trained or evaluated on more than two
annotated corpora.
Also, WSD systems are systematically evaluated on cor-
pora that have been initially created for the purpose of eval-
uation, and never on corpora that have been created for an-
other purpose, such as training or for sense distribution es-
timation, whereas there is no scientiﬁc reason for that.
This paper presents a work of uniﬁcation of all existing
English corpora annotated with any version of WordNet to
our knowledge, in a unique format, easy to understand, and
easy to work with in practice. We put on the same level the
corpora originally created for the evaluation and those for
the learning, so to facilitate the creation of robust WSD sys-
tems which could for example be evaluated in a way where
all corpora except one are used for the learning, and the

remaining one is used for the evaluation, then switch the
corpora and do this for every existing corpus.
The language resource that we provide contains all En-
glish sense annotated corpora in UFSAC (Uniﬁed Format
for Sense Annotated Corpora), the format that we pro-
pose, with sense annotations converted to the last version
of WordNet (3.0), along with Java code to easily read, write
and modify any corpus in this format, and scripts for con-
verting a corpus from its original format to UFSAC.
the demonstration of
Our work is the continuity of
(Vial et al., 2017), and it differs from the recent work of
(Raganato et al., 2017) in several points. Their work is fo-
cused on the evaluation of WSD systems, whereas we pro-
vide a complete API for manipulating corpora in a new uni-
ﬁed format (UFSAC), and conversion scripts allowing the
full reconstruction of the corpora from the original data. We
also propose ﬁve additional corpora in our resource among
the most difﬁcult to parse.
In our resource, we provide a script for converting a cor-
pus from our format to theirs, so existing WSD systems
that rely on their format can be trained or evaluated on any
of the corpus that we produced. We also provide a script
for converting their format to ours in order to facilitate any
collaborative work in the community.

2. Sense Annotated Corpora: rare and

costly resources

Generally speaking, a corpus is a collection of documents
which can be used as samples of text for a particular lan-
guage (Habert et al., 1998). A corpus may contain several
millions of words, which can be lemmatized and annotated
with information concerning their part of speech for exam-
ple. Among these corpora, we can ﬁnd the British National
Corpus (Burnard, 1998) (100 million words) and the Amer-
ican National Corpus (Ide and Macleod, 2001) (20 million
words). The texts come from several sources such as news-
papers, books, encyclopedias or from the Web.
A sense annotated corpus is a corpus in which some or
all words are annotated with an identiﬁer of sense from

1027

a speciﬁc lexical database. For example, all words in
the corpus of the 7th task of the SemEval 2007 seman-
tic evaluation campaign (Navigli et al., 2007) are anno-
tated with sense identiﬁers from WordNet 2.1, whereas
in the English corpus of the 13th task of SemEval 2015
(Moro and Navigli, 2015), all words are annotated with
sense identiﬁers from WordNet 3.0, BabelNet 2.5 and
Wikipedia pages.
There are at least three reasons to create a sense annotated
corpus:

It

• Estimate the distribution of senses in the lan-
guage.
the SemCor
is for this purpose that
(Miller et al., 1993) was annotated. Consequently, the
senses in WordNet are, since version 1.7, sorted by
this distribution of senses estimated on the SemCor.

• Build

a Word Sense Disambiguation

system
from examples contained in the
the OMSTI
instance,
this
for

which learns
annotated corpus.
(Taghipour and Ng, 2015) was
purpose.

created

For

• Evaluate a WSD system by comparing its output to
the annotations in the corpus, as it is the case for in-
stance with corpora created as part of the evaluation
campaigns SensEval-SemEval.

After their distribution, there is no scientiﬁc reason not
to use indistinctly these corpora either for building a
WSD system, for estimating the distribution of senses or
Indeed, the SemCor is
for evaluating a WSD system.
used since a long time for the learning of WSD systems
(Chan et al., 2007; Navigli et al., 2007) or more recently
for the evaluation of different methods (Yuan et al., 2016).
This last usage is still very rare, since it is one of the
ﬁrst experiment that we found in the literature, along with
(M`arquez et al., 2002).
However, the format of the resources differs greatly de-
pending on their original purpose. For the SemCor, a single
ﬁle groups all the information, whereas in the case of the
evaluation corpora, there are two ﬁles: one that contains the
unannotated corpus, and the other that contains the sense
annotations. In some corpora, like in the DSO and the OM-
STI, there is one ﬁle for every lemma in the dictionary, and
each ﬁle contains thousands of example sentences, where
this lemma is the only word that is sense annotated.
Few data are manually sense annotated. The Global Word-
Net Association made a list of 26 corpora annotated with
WordNet 1. These corpora concern 17 languages, but only
three of them reach 100,000 annotations. English, with
more than 2 million words sense annotated ranks ﬁrst, be-
fore Dutch with nearly 300,000 annotations and Bulgarian
with 100,000 annotations. Thus, it is unsurprising that most
of researches in WSD focus on English.

3. A single format for sense annotated

corpora

The main purpose of this work is to help the construction
and the evaluation of WSD systems, by giving to the com-

munity the set of all existing English sense annotated cor-
pora to our knowledge, in the same format, using the same
sense inventory, and tools to easily parse them, manipulate
them, and convert corpora from their original format to our
one.
Indeed, a large quantity of sense annotated data is vi-
tal for the construction of WSD systems.
In evaluation
campaigns, this often makes the difference. For exam-
ple,
looking at the data from the SemEval 2007 cam-
paign (Navigli et al., 2007), which most of the recent sys-
tems were evaluated on, we observe that systems that
did not use sense annotated data obtain a precision score
up to 78 − 79%2 (Schwab et al., 2013) (Chen et al., 2014)
whereas those which use a lot of annotated data reach
a score up to 82% (Chan et al., 2007) (Navigli, 2012)
(Vial et al., 2016) and even 84% (Yuan et al., 2016).
Therefore, having all existing corpora in a unique format
and using the same sense inventory offers several advan-
tages: it allows to easily expand the quantity of data avail-
able for improving WSD systems, it allows to better esti-
mate the distribution of senses in English, and ﬁnally, this
format can help creating more robust WSD systems. In-
deed, we still ﬁnd a lot of works that focus on a single evalu-
ation task (Vial et al., 2016; Chen et al., 2014), and in these
cases, the analysis of the results concerning the robustness
of the methods is limited. The uniﬁcation of the format of
sense annotated corpora could improve the evaluation pro-
cess by facilitating a cross validation process for instance,
where the system is evaluated sequentially on every corpus,
with all others used for the training.

4. Provided resource
Our work consists in gathering all English corpora sense
annotated with WordNet, and convert all of them to a
uniﬁed format that is able to contain all the informations
present in the original format. We created format conver-
sion scripts for this purpose, as well as scripts for clean-
ing the corpora, and converting the sense annotation to the
last version of WordNet (3.0). The resulting corpora are
parts of the resource when the copyright allows it, along
with the format conversion scripts, the cleaning scripts,
and the sense conversion scripts. For the corpora that
we cannot distribute because of the licence, anyone that
possess them can still run our scripts to turn the origi-
nal resource into our format. Finally, an API is provided
for parsing, creating and manipulating corpora in our for-
mat. The resource is accessible at the following URL:
https://github.com/getalp/UFSAC.

4.1. Sense annotated corpora
Our resource contains the following corpora:

• The SemCor (Miller et al., 1993), a subset of the
Brown Corpus (Francis and Kuˇcera, 1964). Original
annotations are done with WordNet 1.6.

• The DSO (Defence Science Organisation) (Ng and
Lee, 1997), a non-free corpus, that is focused on 121

1http://globalwordnet.org/wordnet-annotated-corpora/

the human annotators in 78 to 79% of cases

2This means that the system has chosen the same sense than

1028

nouns and 70 verbs among the most frequently used
and the most ambiguous words in English and have
been annotated in various contexts with WordNet 1.5.

• The WordNet Gloss Tag 3, a corpus which consists of
all deﬁnitions of WordNet (Miller, 1995) with every
words sense annotated since version 3.0.

• The OMSTI (One Million Sense-Tagged Instances)
(Taghipour and Ng, 2015), a corpus of approximately
one million words sense annotated with WordNet 3.0.

• The MASC (Manually Annotated Sub-Corpus) (Ide et
al., 2008), we used the version given in the article of
(Yuan et al., 2016), annotated with the NOAD (New
Oxford American Dictionary), but with corresponding
WordNet 3.0 sense keys.

• The Ontonotes 5.0 (Hovy et al., 2006), annotated with

WordNet 3.0.

• The corpora of

the WSD evaluation campaigns
SemEval-SensEval: SensEval 2 (using WordNet 1.7),
SensEval 3 (WN 1.7.1), SemEval 2007 (WN 2.1), Se-
mEval 2013 (WN 3.0) and SemEval 2015 (WN 3.0).

Table 1 summarizes statistics concerning these corpora.
After the conversion of all these corpora into our format,
we executed four post-processing steps: sense annotation
conversion, identical sentences merging, lemma and POS
tagging, and ﬁnally a cleaning step.

4.1.1. Sense Annotation Conversion
Sense annotations have been converted, when necessary,
from their original WordNet sense key to the last ver-
sion of WordNet (3.0) thanks to conversion tables from
(Daud´e et al., 2000).
However, because some senses have been dropped from the
old versions of WordNet, some sense annotations have not
been converted. In any case, the original sense annotations
are always kept alongside the converted sense annotation.
When a sense is mapped to two or more senses with equal
probability, all resulting senses are added to the word anno-
tations, separated by a semicolon.

Identical sentences merging

4.1.2.
This step is only applied on the DSO and the OMSTI: be-
cause these corpora are constructed such that they contain
lists of sentences with only one word that is sense anno-
tated, surrounded by words not annotated, some sentences
are present in different places across the corpus, but with
different words that are sense annotated.
The merging phase identiﬁes identical sentences with an-
notations on different words, and creates a single sentence
containing all annotations. Thus, this steps adds a cru-
cial information for some WSD systems. For instance,
a similarity-based WSD system can now “learn” that two
word senses are often located in the same sentence.

3http://wordnet.princeton.edu/glosstag.shtml

4.1.3. Lemma and POS tagging
For the corpora that do not already contain these informa-
tions, we added the lemma for every word, when existing,
using the WordNet’s morphy tool, and the part-of-speech
tag from the Penn Treebank tag set using Stanford’s Log-
linear POS tagger (Toutanova et al., 2003).

4.1.4. Cleaning
Finally, this last step consists of trimming words, removing
invisible characters and removing inconsistent annotations,
for instance when the part of speech annotation differs from
the part of speech of the sense annotation.

4.2. UFSAC File format

Our approach for the uniﬁcation of the different annotated
corpora begins with a ﬁle format that is descriptive, easily
understandable and readable by a human, and at the same
time, efﬁcient for a program to parse and create. Finally, it
should be able to contain all the information contained in
the original resources. These informations are represented
with the following concepts:
– A Lexical Entity (LE) is an entity that contains a set of
annotations.
– A Corpus is a LE which contains a set of documents.
– A Document is a LE hich contains a set of paragraphs.
– A Paragraph is a LE which contains a set of sentences.
– A Sentence is a LE which contains a set of words.
– A Word is a LE which has a special mandatory annotation
“surface form”, which is the value of the word.

In order to represent these concepts, UFSAC is based on a
simple XML syntax with some conventions: lexical enti-
ties are represented by XML nodes (corpus, document,
paragraph, sentence and word), and annotations are
node attributes.

The annotations also follow a certain convention, we used
the following to annotate words:
– The identiﬁer (id) of a lexical entity, particularly use-
ful for corpora originally created for the evaluation (e.g.
“d001.s002.t003”).
– The surface form (surface form) of a word.
– The lemma (lemma) of a word.
– The part of speech (pos) of a word.
– The sense of a word,
in a speciﬁc lexical database,
for example WordNet 3.0 (wn30 key), WordNet 1.7.1
(wn171 key)... If multiple senses are speciﬁed (it is the
case in the coarse-grained task of SemEval 2007 for in-
stance), they are separated with a semicolon (;).
The information of the sense is the one which is the most
useful in our case, and it is speciﬁc to each lexical database,
instead of having a unique “sense” annotation as we can
ﬁnd in most other formats. That way we allow multiples
sense annotations from different lexical databases at the
same time. For example, the DSO is originally annotated
with senses from WordNet 1.5, and the conversion to Word-
Net 3.0 is sometimes impossible for some senses which
were deleted between the two versions. This convention
allows us to keep the original annotations, yet to have the
annotations from the last version of WordNet, or any other
lexical database (for instance BabelNet) at the same time.

1029

Corpus

Sentences

Words

Annotated parts of speech

Total

Annotated Nouns

Verbs

Adj.

Adv.

SemCor
DSO
WordNet GlossTag
MASC
OMSTI
Ontonotes
SemEval 2007 task 07
SemEval 2007 task 17
SemEval 2013 task 12
SemEval 2015 task 13
Senseval 2
Senseval 3 task 1

37176
178119
117659
34217
820557
21938
245
120
306
138
238
300

778587
5317184
1634691
596333
35843024
435340
5637
3395
8142
2638
5589
5511

229517
176915
496776
114950
920794
52263
2261
455
1644
1053
2301
1957

87581
105925
232319
49263
476944
9220
1108
159
1644
554
1061
886

89037
70990
62211
40325
253644
43042
591
296
0
251
541
723

33751
0
84233
25016
190206
0
356
0
0
166
422
336

19148
0
19445
0
0
0
206
0
0
82
277
12

Table 1: Statistics related to our set of annotated corpora, after the conversion and cleaning phase.

The following is an example of the resulting UFSAC XML:

ing/modifying them.

<corpus id="short_example">

<document id="d001" >

<paragraph>
<sentence>

<word surface_form="A" pos="DT" />
<word surface_form="precise"

wn30_key="precise%3:00:00::" />

<word surface_form="example"

pos="NN" lemma="example" />

<word surface_form="." />

</sentence>
</paragraph>

</document>

</corpus>

Our format thus allows to integrate the whole corpus in a
single ﬁle, and it is easily readable, especially comparing
to most original formats (c.f. the end of section 2.).

4.3. API and tools
An easy-to-use Java API is also provided to read, write
and modify efﬁciently corpora in our format. It allows two
styles of programming: you can either load a full corpus in
memory, perform all your calculations and save it entirely
in a ﬁle; or you can sequentially scan, edit or print a corpus
from a ﬁle, in a streaming manner. The latter is particularly
useful when working with huge ﬁles which do not ﬁt into
memory. Finally, we offer a set of scripts that perform the
conversion of a corpus from its original format to our one,
and some pre-processing and analyses scripts.

4.3.1. Core API
The core API is a package containing the base classes
for manipulating corpora. For simplicity, the class names
match exactly what is described in section 4.2..

The class Annotation describes an annotation on a lex-
ical entity. Concretely, it is a pair of Strings (name/value)
and a pointer to the annotated lexical entity.

The class LexicalEntity describes something that has
zero or more annotations, with public methods for access-

The class Word inherits from LexicalEntity, has a special
mandatory annotation surface form, which is the value
of the word, and a parent sentence.

The class Sentence inherits from LexicalEntity, contains
a list of words and a parent paragraph.

The class Paragraph inherits from LexicalEntity, con-
tains a list of sentences and a parent document.

The class Document inherits from LexicalEntity, contains
a list of paragraphs and a parent corpus.

Finally, the class Corpus inherits from LexicalEntity and
contains a list of documents.

few classes,

functions
These
Corpus.saveToXML and Corpus.loadFromXML
allow to create, save, load and modify any corpus easily.

coupled with

two

4.3.2. Streaming API
For some corpora particularly huge, like the OMSTI, we
also provide a sub-package streaming, which allows to
read, write or modify a corpus sequentially, without be-
ing fully loaded into memory. This is similar to the Java
SAX library (Simple API for XML), events are ﬁred when
reading a word, sentence, paragraph, etc., and the user can
choose to respond to this event or not.
In practice, we provide a set of classes which cover most
use cases.

class
respond

StreamingCorpusReader
to

allows
The
events readBeginCorpus,
to
the
readBeginDocument, readWord, etc..
This can
be useful for printing every word that is sense annotated
for example.

The class StreamingCorpusModifier allows to
modify a corpus in-place. This is specially useful for pre-
processing, for instance convert every word to lowercase.

class
creating

The
for
writeBeginSentence, writeWord and so on.

StreamingCorpusWriter
new corpus, with

used
its methods

is

a

1030

4.3.3. Scripts
Finally, we provide a set of examples and useful scripts
which use our format and our API. The scripts are Java
classes with a main method and are not part of any pack-
age.

The script ConvertOriginalCorpora allows to con-
vert all corpora listed in subsection 4.1. from their original
format to the UFSAC format. This is specially valuable for
non-free corpus like the DSO, that we cannot share directly
in our format, but that one can still buy in their original for-
mat, and then convert to our format. This script includes all
post-processing steps described in subsection 4.1..

scripts

ConvertFromRaganato

and
The
ConvertToRaganato allow to convert a corpus
from the format described by (Raganato et al., 2017) to
UFSAC, and vice-versa.

The script ComputeMostFrequentSenses will calcu-
late, for every lemma in WordNet, the most frequent sense
(MFS), based on all usages in the given UFSAC corpora.
This is helpful since in most evaluation campaigns, the
the score obtained when the MFS is
MFS baseline (i.e.
assigned to every word) is important, and it is generally
implicitly the sense distribution computed on the SemCor
only.

The scripts AddCorpusLemma and AddCorpusPOS use
respectively WordNet’s morphy and Stanford’s POS tag-
ger to annotate a corpus with the lemma and POS of every
word.

The script EvaluateWSD compare the sense annotations
produced by a WSD system to the gold standard annotation,
and compute the usual Precision, Recall, Coverage and F1
metrics for every given corpus.

The script GenerateCorpusStatistics is the one
that was used to produce the table 1.

5. Experiments
In this section, we show an example of using all UFSAC
corpora for the extension of a knowledge-based WSD sys-
tem based on the Lesk measure. This experiment shows
how this ressource can be used to easily improve an exist-
ing WSD system.

5.1. The Lesk and Extended Lesk Similarity

Measures

(Lesk, 1986) proposed a simple algorithm for lexical dis-
ambiguation that evaluates the similarity between two
senses (s1, s2) as the number of words in common in the
deﬁnitions of the senses from a dictionary (D(s1),D(s2)).
The Lesk measure compute an exact lexical match of the
surface forms of the words in the deﬁnitions. If important
words are missing or different synonyms of the same words
are used in the deﬁnition of related senses, the overlap mea-
sure will not capture the proximity of their meanings appro-
priately. As deﬁnitions (especially in Princeton Wordnet)
are very concise, it is difﬁcult to obtain ﬁne grained dis-
tinctions between senses.
In consequence, several variants of the Lesk measure tried
to alleviate this problem, for instance (Baldwin et al., 2010)

and (Miller et al., 2012), but the most common expan-
sion technique is the so-called “extended/adapted Lesk”
(Banerjee and Pedersen, 2002). The sense overlap is here
expanded with the overlap of all deﬁnitions from all pairs
of related senses in a lexico-semantic resource with a rich
structure, such as WordNet.
In our experiment, we will create another expansion of the
Lesk measure, based on UFSAC sense annotated corpora.

5.2. Expansion of Deﬁnitions Through UFSAC

Sense-Annotated Corpora

Our method consists in expanding deﬁnitions with all
neighbours of a target sense, taken from sense-annotated
corpora. We consider that a neighbour is a word found in
the same sentence as the target sense. More precisely, we
proceed as following:

1. We parse every UFSAC corpus, sentence by sentence.

2. For every word which is sense-annotated in a sentence,
we add to the deﬁnition of this sense in the dictionary
every other word present in the sentence.

That is, for every sentence S = w0, w1, . . . , wn, and for
every word wk inside S, we add to the deﬁnition of the
tagged sense of wk, i.e. D(s(wk)), every other words of
the sentence, i.e. wi∀i ∈ [0, n]i 6= k.
As a consequence, every sense’s deﬁnition in the dictionary
will be extended with words that are related to this sense,
in the same manner than (Banerjee and Pedersen, 2002)’s
extended Lesk, but with words taken from sense annotated
corpora.

5.3. Similarity-Based Word Sense

Disambiguation

Now for evaluating this new expansion to the Lesk mea-
sure, we must use a similarity-based WSD algorithm
that belongs to the broader category of knowledge-based
approaches (using dictionaries,
lexical base, encyclope-
dias. . . ). In such systems, the disambiguation process con-
sists of two layers: a local algorithm and a global algorithm.
The local algorithm computes the proximity of two word
senses, namely a semantic relatedness measure. The lo-
cal similarity measurement is then used to ﬁnd an optimal
global sense assignment for all the content words of the text
by the global algorithm. The local algorithm is here the
Lesk measure augmented with the sense annotated corpora.
We also ﬁltered out stopwords according to the “long” list
given in https://www.ranks.nl/stopwords.
As for global algorithms,
they are often probabilistic
combinatorial optimization algorithms, as WSD is funda-
mentally a discrete combinatorial optimization problem.
Many such algorithms have been adapted to WSD, in-
cluding genetic algorithms (Gelbukh et al., 2003), simu-
lated annealing (Cowie et al., 1992), ant colony algorithms
(Schwab et al., 2012) or more recently bee hive algorithms
(Abualhaija and Zimmermann, 2016).
The different global algorithms mainly differ in the con-
vergence speed to a close-to-optimal solution, however the
bottleneck to the accuracy of the algorithm is the local al-
gorithm (similarity measure) used, as it encodes the knowl-

1031

System

Lesk + UFSAC corpora

Lesk
Extended Lesk (Banerjee and Pedersen, 2003)
Most Frequent Sense Baseline

79.83%

68.70%
78.01%
78.90%

66.43%

50.65%
61.42%
67.10%

SemEval 2007 Task 07

SemEval 2015 Task 13

Table 2: F1 scores of our similarity-based system augmented with words taken from all UFSAC corpora (except the eval-
uation corpora) on SemEval 2007 coarse-grained all-words task and SemEval 2015 ﬁne-grained all-words task, compared
to the Lesk, Extended Lesk and MFS baselines.

edge from the resource that allows to discriminate between
the senses.
In this experiments, we use an adaptation to WSD of the
Cuckoo Search Algorithm, the state of the art in combina-
torial search algorithms (Yang and Deb, 2009). The algo-
rithm relies on the L´evy ﬂight distribution for an effective
(and more meaningful) sampling of the search space.
The Cuckoo Search Algorithm is probabilistic and its re-
sult differs slightly from an execution to another (by an
order of magnitude of less than 1%). So for each experi-
ment, 30 executions are performed. Then, using a Shapiro-
Wilk test (Shapiro and Wilk, 1965), we determined that
none of the result distribution follow a normal distribution.
Thus, we used a non-parametric Wilcoxon/Mann-Whitney-
U (Wilcoxon, 1945) (Mann and Whitney, 1947) test in or-
der to check the pairwise signiﬁcance (p < 0.01) of all
pairs of result distributions.

5.4. Results
We evaluate the performance of our expansion of deﬁni-
tions using all UFSAC corpora listed in subsection 4.1. ex-
cept the ones we evaluated our system on: SemEval 2007
task 7 and SemEval 2015 task 13. We compare our simi-
larity measure to the original Lesk and the Extended Lesk
(Banerjee and Pedersen, 2003) measures. The results are
presented in Table 2.
As we can see, our expansion of the deﬁnitions with words
taken from the UFSAC corpora improves considerably the
original Lesk measure, even more than the Extended Lesk
measure. Therefore, this experiment demonstrates how
much the addition of the UFSAC resource can improve a
similarity-based WSD sytem. Of course, every other kind
of WSD system can be improved, in particular supervised
systems which rely solely on sense-annotated corpora and
machine learning techniques (SVM, neural networks, etc.).

6. Conclusion
In this paper we advocate for a more uniform way of dis-
tributing sense annotated corpora, through a unique and un-
complicated ﬁle format. This uniﬁcation can facilitate both
the creation and the evaluation of Word Sense Disambigua-
tion systems. Indeed, sense annotated corpora are histor-
ically separated between those created for the purpose of
training, and those created for the purpose of evaluation.
In addition, the formats of these corpora are often very
different from each other: different ﬁle hierarchy, differ-
ent syntax, and different sense inventory are used. Conse-
quently, most WSD systems are trained and evaluated on

few corpora comparing to the amount of existing corpora.
Moreover, they are systematically evaluated only on cor-
pora originally created for the purpose of evaluation, and
trained only on corpora originally created for the purpose
of training, whereas they could beneﬁt from considering all
of them in both tasks.
The uniﬁcation of all sense annotated corpora hence allows
to quickly expand a system which is trained on some re-
sources to new data without the effort of writing another
parser. Also, a system can now easily include to its training
phase some corpora that were originally created for evalu-
ation, and/or evaluate its performance on parts of corpora
originally created for training. This easily allows a much
better coverage and a more ﬁne-grained analysis of a WSD
system performance.
In our language resource, we gathered all existing English
sense annotated corpora that we know, and we converted
them in a simple and consistent XML ﬁle format that we
named UFSAC. We also converted their sense annotations
to the last version of WordNet (3.0). The corpora are only
available when the licence authorizes it, but we also pro-
vide scripts that can easily convert a corpus from its origi-
nal format to the one we propose. Thus, anyone who pos-
sess the corpora that we cannot distribute can still bene-
ﬁt from this work.
In addition, we provide a complete
Java API for reading, writing and modifying corpora in
our uniﬁed format, along with example codes and tools
for many applications such as lemmatization, POS-tagging,
sense distribution estimation, etc. Finally, a demonstra-
tion of a simple use of all UFSAC corpora for extend-
ing a similarity-based WSD system is shown in section 5..
In the future, we plan to add to our resource other cor-
pora such as the corpora created for the lexical sample
tasks of SensEval/SemEval, and sense annotated corpora
in other languages. We also plan to improve the UFSAC
format by adding a better support for multiword expres-
sions. The resource will be continuously updated at this
url: https://github.com/getalp/UFSAC.

7. Bibliographical References

Abualhaija, S. and Zimmermann, K.-H. (2016). D-bees:
A novel method inspired by bee colony optimization for
solving word sense disambiguation. Swarm and Evolu-
tionary Computation, pages –.

Baldwin, T., Kim, S., Bond, F., Fujita, S., Martinez, D., and
Tanaka, T. (2010). A reexamination of mrd-based word
sense disambiguation. 9(1):4:1–4:21, March.

1032

Banerjee, S. and Pedersen, T. (2002). An adapted lesk al-
gorithm for word sense disambiguation using wordnet.
In CICLing 2002, Mexico City, February.

Banerjee, S. and Pedersen, T. (2003). Extended gloss over-
laps as a measure of semantic relatedness. In In Proceed-
ings of the Eighteenth International Joint Conference on
Artiﬁcial Intelligence, pages 805–810.

Burnard, L. (1998). The British National Corpus.
Chan, Y. S., Ng, H. T., and Zhong, Z. (2007). Nus-pt: Ex-
ploiting parallel texts for word sense disambiguation in
the english all-words tasks. In Proceedings of the 4th
International Workshop on Semantic Evaluations, Se-
mEval ’07, pages 253–256, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.

Chen, X., Liu, Z., and Sun, M. (2014). A uniﬁed model for
word sense representation and disambiguation. In Pro-
ceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages 1025–
1035, Doha, Qatar, October. Association for Computa-
tional Linguistics.

Cowie, J., Guthrie, J., and Guthrie, L.

(1992). Lexical
disambiguation using simulated annealing. In COLING
1992, volume 1, pages 359–365, Nantes, France, aoˆut.
Daud´e, J., Padr´o, L., and Rigau, G. (2000). Mapping word-
nets using structural information. In Proceedings of the
38th Annual Meeting on Association for Computational
Linguistics, ACL ’00, pages 504–511, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Francis, W. N. and Kuˇcera, H. (1964). A standard corpus
of present-day edited american english, for use with dig-
ital computers (brown). Technical report, Brown Univer-
sity, Providence, Rhode Island.

Gelbukh, A., Sidorov, G., and Han, S. Y. (2003). Evolu-
tionary approach to natural language wsd through global
coherence optimization. WSEAS Transactions on Com-
munications, 2(1):11–19.

Habert, B., Fabre, C., and Issac, F. (1998). DE L’ECRIT
AU NUMERIQUE. Constituer, normaliser et exploiter
les corpus ´electroniques. Number ISBN : 2-225-82953-
5. ELSEVIER MASSON.

Ide, N. and Macleod, C. (2001). The american national
corpus: A standardized resource of american english. In
Proceedings of Corpus Linguistics 2001, volume 3.

Lesk, M. (1986). Automatic sense disambiguation using
mrd: how to tell a pine cone from an ice cream cone. In
Proceedings of SIGDOC ’86, pages 24–26, New York,
NY, USA. ACM.

Mann, H. B. and Whitney, D. R. (1947). On a Test of
Whether one of Two Random Variables is Stochasti-
cally Larger than the Other. The Annals of Mathematical
Statistics, 18(1):50–60.

Miller, G. A., Leacock, C., Tengi, R., and Bunker, R. T.
(1993). A semantic concordance. In Proceedings of the
workshop on Human Language Technology, HLT ’93,
pages 303–308, Stroudsburg, PA, USA. Association for
Computational Linguistics.

ceedings of COLING 2012, pages 1781–1796, Mumbai,
India, December. The COLING 2012 Organizing Com-
mittee.

Miller, G. A. (1995). Wordnet: A lexical database. ACM,

Vol. 38(No. 11):p. 1–41.

Moro, A. and Navigli, R. (2015). Semeval-2015 task 13:
Multilingual all-words sense disambiguation and entity
linking. In Proceedings of the 9th International Work-
shop on Semantic Evaluation (SemEval 2015), pages
288–297, Denver, Colorado, June. Association for Com-
putational Linguistics.

M`arquez, L., Raya, J., Carroll, J., McCarthy, D., Agirre, E.,
Mart´ınez, D., Strapparava, C., and Gliozzo, A. (2002).
Experiment a : Several all-words wsd systems for en-
glish. Technical report, Meaning, Developing multilin-
gual Web-scale Language Technologies.

Navigli, R. and Ponzetto, S. P. (2010). Babelnet: Build-
ing a very large multilingual semantic network. In Pro-
ceedings of the 48th annual meeting of the association
for computational linguistics, pages 216–225. Associa-
tion for Computational Linguistics.

Navigli, R., Litkowski, K. C., and Hargraves, O. (2007).
Semeval-2007 task 07: Coarse-grained english all-words
task. In SemEval-2007, pages 30–35, Prague, Czech Re-
public, June.

Navigli, R. (2012). A quick tour of word sense disam-
biguation, induction and related approaches. In Proceed-
ings of the 38th Conference on Current Trends in The-
ory and Practice of Computer Science (SOFSEM), pages
115–129.

Raganato, A., Camacho-Collados, J., and Navigli, R.
(2017). Word sense disambiguation: A uniﬁed evalua-
tion framework and empirical comparison. In Proceed-
ings of the 15th Conference of the European Chapter of
the Association for Computational Linguistics: Volume
1, Long Papers, pages 99–110, Valencia, Spain, April.
Association for Computational Linguistics.

Schwab, D., Goulian, J., Tchechmedjiev, A., and Blan-
chon, H. (2012). Ant Colony Algorithm for the Unsu-
pervised Word Sense Disambiguation of Texts: Compar-
ison and Evaluation. In Proceedings of the 25th Interna-
tional Conference on Computational Linguistics (COL-
ING 2012), Mumbai (India), dec.

Schwab, D., Goulian, J., and Tchechmedjiev, A. (2013).
D´esambigu¨ısation lexicale de textes : efﬁcacit´e qualita-
tive et temporelle d’un algorithme `a colonies de fourmis.
TAL, 54(1):99–138.

Shapiro, S. S. and Wilk, M. B. (1965). An analysis of vari-
ance test for normality (complete samples). Biometrika,
3(52).

Taghipour, K. and Ng, H. T. (2015). One million sense-
tagged instances for word sense disambiguation and in-
duction. In Proceedings of the Nineteenth Conference on
Computational Natural Language Learning, pages 338–
344, Beijing, China, July. Association for Computational
Linguistics.

Miller, T., Biemann, C., Zesch, T., and Gurevych, I. (2012).
Using distributional similarity for lexical expansion in
knowledge-based word sense disambiguation. In Pro-

Toutanova, K., Klein, D., Manning, C. D., and Singer, Y.
(2003). Feature-rich part-of-speech tagging with a cyclic
dependency network. In Proceedings of the 2003 Con-

1033

ference of the North American Chapter of the Associa-
tion for Computational Linguistics on Human Language
Technology - Volume 1, NAACL ’03, pages 173–180,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

Vial, L., Tchechmedjiev, A., and Schwab, D. (2016). Ex-
tension lexicale de d´eﬁnitions grˆace `a des corpus annot´es
en sens. In Traitement Automatique des Langues Na-
turelles (TALN).

Vial, L., Lecouteux, B., and Schwab, D.

(2017). Uni-
formisation de corpus anglais annot´es en sens. In 24`eme
Conf´erence sur le Traitement Automatique des Langues
Naturelles, Orl´eans, France, June.

Wilcoxon, F. (1945). Individual Comparisons by Ranking
Methods. Biometrics Bulletin, 1(6):80–83, December.
Yang, X.-S. and Deb, S. (2009). Cuckoo search via l´evy
ﬂights. Proc. of World Congress on Nature and Biologi-
cally Inspired Computing, pages 210–214.

Yuan, D., Richardson, J., Doherty, R., Evans, C., and Al-
tendorf, E. (2016). Semi-supervised word sense disam-
biguation with neural models. In COLING 2016.

8. Language Resource References
Hovy et al. (2006). OntoNotes: The 90% Solution.
Ide et al. (2008). MASC: the Manually Annotated Sub-

Corpus of American English.

Miller et al. (1993). A Semantic Concordance.
Miller. (1995). Wordnet: A Lexical Database.
Ng and Lee. (1997). DSO Corpus of Sense-Tagged En-

glish.

Taghipour and Ng. (2015). One Million Sense-Tagged In-
stances for Word Sense Disambiguation and Induction.

1034

UFSAC: Uniﬁcation of Sense Annotated Corpora and Tools

Lo¨ıc Vial, Benjamin Lecouteux, Didier Schwab
LIG – GETALP – Univ. Grenoble Alpes – France
{loic.vial, benjamin.lecouteux, didier.schwab}@imag.fr

Abstract
In Word Sense Disambiguation, sense annotated corpora are often essential for evaluating a system and also valuable in order to reach a
good efﬁciency. Always created for a speciﬁc purpose, there are today a dozen of sense annotated English corpora, in various formats
and using different versions of WordNet. The main hypothesis of this work is that it should be possible to build a disambiguation system
by using any of these corpora during the training phase or during the testing phase regardless of their original purpose. In this article, we
present UFSAC: a format of corpus that can be used for either training or testing a disambiguation system, and the process we followed
for constructing this format. We give to the community the whole set of sense annotated English corpora that we know, in this uniﬁed
format, when the copyright allows it, with sense keys converted to the last version of WordNet. We also provide the source code for
building these corpora from their original data, and a complete Java API for manipulating corpora in this format. The whole resource is
available at the following URL: https://github.com/getalp/UFSAC.

Keywords: Word Sense Disambiguation, sense annotated corpora, uniﬁed resource, tools

1.

Introduction

Whether they are used for the evaluation or for the learn-
ing process of a Word Sense Disambiguation (WSD) sys-
tem, the importance of sense annotated corpora in Natural
Language Processing (NLP) is considerable. On one hand,
the evaluation in vivo, i.e.
the evaluation of a WSD sys-
tem as part of a larger task, has never been really exploited.
On the other hand, the evaluation in vitro, which uses di-
rectly sense annotated corpora by comparing the output of
a system to manual annotations, is predominant. Moreover,
WSD systems exploiting examples from sense annotated
corpora are generally far better than those which do not
(Navigli et al., 2007; Moro and Navigli, 2015).
At the time of its creation, WordNet (Miller, 1995) was un-
doubtedly the only lexical database freely available for En-
glish. Since the beginning of the 2000s, it has become the
de facto standard for WSD in this language. Indeed, most of
sense annotated corpora are either directly annotated with
WordNet sense keys or they are annotated with a sense in-
ventory linked to the senses of WordNet, such as BabelNet
(Navigli and Ponzetto, 2010).
However, it is not trivial to use these corpora, because most
of them differ in their format and on the version of Word-
Net they use. As a consequence, very few works in the lit-
erature of WSD are trained or evaluated on more than two
annotated corpora.
Also, WSD systems are systematically evaluated on cor-
pora that have been initially created for the purpose of eval-
uation, and never on corpora that have been created for an-
other purpose, such as training or for sense distribution es-
timation, whereas there is no scientiﬁc reason for that.
This paper presents a work of uniﬁcation of all existing
English corpora annotated with any version of WordNet to
our knowledge, in a unique format, easy to understand, and
easy to work with in practice. We put on the same level the
corpora originally created for the evaluation and those for
the learning, so to facilitate the creation of robust WSD sys-
tems which could for example be evaluated in a way where
all corpora except one are used for the learning, and the

remaining one is used for the evaluation, then switch the
corpora and do this for every existing corpus.
The language resource that we provide contains all En-
glish sense annotated corpora in UFSAC (Uniﬁed Format
for Sense Annotated Corpora), the format that we pro-
pose, with sense annotations converted to the last version
of WordNet (3.0), along with Java code to easily read, write
and modify any corpus in this format, and scripts for con-
verting a corpus from its original format to UFSAC.
the demonstration of
Our work is the continuity of
(Vial et al., 2017), and it differs from the recent work of
(Raganato et al., 2017) in several points. Their work is fo-
cused on the evaluation of WSD systems, whereas we pro-
vide a complete API for manipulating corpora in a new uni-
ﬁed format (UFSAC), and conversion scripts allowing the
full reconstruction of the corpora from the original data. We
also propose ﬁve additional corpora in our resource among
the most difﬁcult to parse.
In our resource, we provide a script for converting a cor-
pus from our format to theirs, so existing WSD systems
that rely on their format can be trained or evaluated on any
of the corpus that we produced. We also provide a script
for converting their format to ours in order to facilitate any
collaborative work in the community.

2. Sense Annotated Corpora: rare and

costly resources

Generally speaking, a corpus is a collection of documents
which can be used as samples of text for a particular lan-
guage (Habert et al., 1998). A corpus may contain several
millions of words, which can be lemmatized and annotated
with information concerning their part of speech for exam-
ple. Among these corpora, we can ﬁnd the British National
Corpus (Burnard, 1998) (100 million words) and the Amer-
ican National Corpus (Ide and Macleod, 2001) (20 million
words). The texts come from several sources such as news-
papers, books, encyclopedias or from the Web.
A sense annotated corpus is a corpus in which some or
all words are annotated with an identiﬁer of sense from

1027

a speciﬁc lexical database. For example, all words in
the corpus of the 7th task of the SemEval 2007 seman-
tic evaluation campaign (Navigli et al., 2007) are anno-
tated with sense identiﬁers from WordNet 2.1, whereas
in the English corpus of the 13th task of SemEval 2015
(Moro and Navigli, 2015), all words are annotated with
sense identiﬁers from WordNet 3.0, BabelNet 2.5 and
Wikipedia pages.
There are at least three reasons to create a sense annotated
corpus:

It

• Estimate the distribution of senses in the lan-
guage.
the SemCor
is for this purpose that
(Miller et al., 1993) was annotated. Consequently, the
senses in WordNet are, since version 1.7, sorted by
this distribution of senses estimated on the SemCor.

• Build

a Word Sense Disambiguation

system
from examples contained in the
the OMSTI
instance,
this
for

which learns
annotated corpus.
(Taghipour and Ng, 2015) was
purpose.

created

For

• Evaluate a WSD system by comparing its output to
the annotations in the corpus, as it is the case for in-
stance with corpora created as part of the evaluation
campaigns SensEval-SemEval.

After their distribution, there is no scientiﬁc reason not
to use indistinctly these corpora either for building a
WSD system, for estimating the distribution of senses or
Indeed, the SemCor is
for evaluating a WSD system.
used since a long time for the learning of WSD systems
(Chan et al., 2007; Navigli et al., 2007) or more recently
for the evaluation of different methods (Yuan et al., 2016).
This last usage is still very rare, since it is one of the
ﬁrst experiment that we found in the literature, along with
(M`arquez et al., 2002).
However, the format of the resources differs greatly de-
pending on their original purpose. For the SemCor, a single
ﬁle groups all the information, whereas in the case of the
evaluation corpora, there are two ﬁles: one that contains the
unannotated corpus, and the other that contains the sense
annotations. In some corpora, like in the DSO and the OM-
STI, there is one ﬁle for every lemma in the dictionary, and
each ﬁle contains thousands of example sentences, where
this lemma is the only word that is sense annotated.
Few data are manually sense annotated. The Global Word-
Net Association made a list of 26 corpora annotated with
WordNet 1. These corpora concern 17 languages, but only
three of them reach 100,000 annotations. English, with
more than 2 million words sense annotated ranks ﬁrst, be-
fore Dutch with nearly 300,000 annotations and Bulgarian
with 100,000 annotations. Thus, it is unsurprising that most
of researches in WSD focus on English.

3. A single format for sense annotated

corpora

The main purpose of this work is to help the construction
and the evaluation of WSD systems, by giving to the com-

munity the set of all existing English sense annotated cor-
pora to our knowledge, in the same format, using the same
sense inventory, and tools to easily parse them, manipulate
them, and convert corpora from their original format to our
one.
Indeed, a large quantity of sense annotated data is vi-
tal for the construction of WSD systems.
In evaluation
campaigns, this often makes the difference. For exam-
ple,
looking at the data from the SemEval 2007 cam-
paign (Navigli et al., 2007), which most of the recent sys-
tems were evaluated on, we observe that systems that
did not use sense annotated data obtain a precision score
up to 78 − 79%2 (Schwab et al., 2013) (Chen et al., 2014)
whereas those which use a lot of annotated data reach
a score up to 82% (Chan et al., 2007) (Navigli, 2012)
(Vial et al., 2016) and even 84% (Yuan et al., 2016).
Therefore, having all existing corpora in a unique format
and using the same sense inventory offers several advan-
tages: it allows to easily expand the quantity of data avail-
able for improving WSD systems, it allows to better esti-
mate the distribution of senses in English, and ﬁnally, this
format can help creating more robust WSD systems. In-
deed, we still ﬁnd a lot of works that focus on a single evalu-
ation task (Vial et al., 2016; Chen et al., 2014), and in these
cases, the analysis of the results concerning the robustness
of the methods is limited. The uniﬁcation of the format of
sense annotated corpora could improve the evaluation pro-
cess by facilitating a cross validation process for instance,
where the system is evaluated sequentially on every corpus,
with all others used for the training.

4. Provided resource
Our work consists in gathering all English corpora sense
annotated with WordNet, and convert all of them to a
uniﬁed format that is able to contain all the informations
present in the original format. We created format conver-
sion scripts for this purpose, as well as scripts for clean-
ing the corpora, and converting the sense annotation to the
last version of WordNet (3.0). The resulting corpora are
parts of the resource when the copyright allows it, along
with the format conversion scripts, the cleaning scripts,
and the sense conversion scripts. For the corpora that
we cannot distribute because of the licence, anyone that
possess them can still run our scripts to turn the origi-
nal resource into our format. Finally, an API is provided
for parsing, creating and manipulating corpora in our for-
mat. The resource is accessible at the following URL:
https://github.com/getalp/UFSAC.

4.1. Sense annotated corpora
Our resource contains the following corpora:

• The SemCor (Miller et al., 1993), a subset of the
Brown Corpus (Francis and Kuˇcera, 1964). Original
annotations are done with WordNet 1.6.

• The DSO (Defence Science Organisation) (Ng and
Lee, 1997), a non-free corpus, that is focused on 121

1http://globalwordnet.org/wordnet-annotated-corpora/

the human annotators in 78 to 79% of cases

2This means that the system has chosen the same sense than

1028

nouns and 70 verbs among the most frequently used
and the most ambiguous words in English and have
been annotated in various contexts with WordNet 1.5.

• The WordNet Gloss Tag 3, a corpus which consists of
all deﬁnitions of WordNet (Miller, 1995) with every
words sense annotated since version 3.0.

• The OMSTI (One Million Sense-Tagged Instances)
(Taghipour and Ng, 2015), a corpus of approximately
one million words sense annotated with WordNet 3.0.

• The MASC (Manually Annotated Sub-Corpus) (Ide et
al., 2008), we used the version given in the article of
(Yuan et al., 2016), annotated with the NOAD (New
Oxford American Dictionary), but with corresponding
WordNet 3.0 sense keys.

• The Ontonotes 5.0 (Hovy et al., 2006), annotated with

WordNet 3.0.

• The corpora of

the WSD evaluation campaigns
SemEval-SensEval: SensEval 2 (using WordNet 1.7),
SensEval 3 (WN 1.7.1), SemEval 2007 (WN 2.1), Se-
mEval 2013 (WN 3.0) and SemEval 2015 (WN 3.0).

Table 1 summarizes statistics concerning these corpora.
After the conversion of all these corpora into our format,
we executed four post-processing steps: sense annotation
conversion, identical sentences merging, lemma and POS
tagging, and ﬁnally a cleaning step.

4.1.1. Sense Annotation Conversion
Sense annotations have been converted, when necessary,
from their original WordNet sense key to the last ver-
sion of WordNet (3.0) thanks to conversion tables from
(Daud´e et al., 2000).
However, because some senses have been dropped from the
old versions of WordNet, some sense annotations have not
been converted. In any case, the original sense annotations
are always kept alongside the converted sense annotation.
When a sense is mapped to two or more senses with equal
probability, all resulting senses are added to the word anno-
tations, separated by a semicolon.

Identical sentences merging

4.1.2.
This step is only applied on the DSO and the OMSTI: be-
cause these corpora are constructed such that they contain
lists of sentences with only one word that is sense anno-
tated, surrounded by words not annotated, some sentences
are present in different places across the corpus, but with
different words that are sense annotated.
The merging phase identiﬁes identical sentences with an-
notations on different words, and creates a single sentence
containing all annotations. Thus, this steps adds a cru-
cial information for some WSD systems. For instance,
a similarity-based WSD system can now “learn” that two
word senses are often located in the same sentence.

3http://wordnet.princeton.edu/glosstag.shtml

4.1.3. Lemma and POS tagging
For the corpora that do not already contain these informa-
tions, we added the lemma for every word, when existing,
using the WordNet’s morphy tool, and the part-of-speech
tag from the Penn Treebank tag set using Stanford’s Log-
linear POS tagger (Toutanova et al., 2003).

4.1.4. Cleaning
Finally, this last step consists of trimming words, removing
invisible characters and removing inconsistent annotations,
for instance when the part of speech annotation differs from
the part of speech of the sense annotation.

4.2. UFSAC File format

Our approach for the uniﬁcation of the different annotated
corpora begins with a ﬁle format that is descriptive, easily
understandable and readable by a human, and at the same
time, efﬁcient for a program to parse and create. Finally, it
should be able to contain all the information contained in
the original resources. These informations are represented
with the following concepts:
– A Lexical Entity (LE) is an entity that contains a set of
annotations.
– A Corpus is a LE which contains a set of documents.
– A Document is a LE hich contains a set of paragraphs.
– A Paragraph is a LE which contains a set of sentences.
– A Sentence is a LE which contains a set of words.
– A Word is a LE which has a special mandatory annotation
“surface form”, which is the value of the word.

In order to represent these concepts, UFSAC is based on a
simple XML syntax with some conventions: lexical enti-
ties are represented by XML nodes (corpus, document,
paragraph, sentence and word), and annotations are
node attributes.

The annotations also follow a certain convention, we used
the following to annotate words:
– The identiﬁer (id) of a lexical entity, particularly use-
ful for corpora originally created for the evaluation (e.g.
“d001.s002.t003”).
– The surface form (surface form) of a word.
– The lemma (lemma) of a word.
– The part of speech (pos) of a word.
– The sense of a word,
in a speciﬁc lexical database,
for example WordNet 3.0 (wn30 key), WordNet 1.7.1
(wn171 key)... If multiple senses are speciﬁed (it is the
case in the coarse-grained task of SemEval 2007 for in-
stance), they are separated with a semicolon (;).
The information of the sense is the one which is the most
useful in our case, and it is speciﬁc to each lexical database,
instead of having a unique “sense” annotation as we can
ﬁnd in most other formats. That way we allow multiples
sense annotations from different lexical databases at the
same time. For example, the DSO is originally annotated
with senses from WordNet 1.5, and the conversion to Word-
Net 3.0 is sometimes impossible for some senses which
were deleted between the two versions. This convention
allows us to keep the original annotations, yet to have the
annotations from the last version of WordNet, or any other
lexical database (for instance BabelNet) at the same time.

1029

Corpus

Sentences

Words

Annotated parts of speech

Total

Annotated Nouns

Verbs

Adj.

Adv.

SemCor
DSO
WordNet GlossTag
MASC
OMSTI
Ontonotes
SemEval 2007 task 07
SemEval 2007 task 17
SemEval 2013 task 12
SemEval 2015 task 13
Senseval 2
Senseval 3 task 1

37176
178119
117659
34217
820557
21938
245
120
306
138
238
300

778587
5317184
1634691
596333
35843024
435340
5637
3395
8142
2638
5589
5511

229517
176915
496776
114950
920794
52263
2261
455
1644
1053
2301
1957

87581
105925
232319
49263
476944
9220
1108
159
1644
554
1061
886

89037
70990
62211
40325
253644
43042
591
296
0
251
541
723

33751
0
84233
25016
190206
0
356
0
0
166
422
336

19148
0
19445
0
0
0
206
0
0
82
277
12

Table 1: Statistics related to our set of annotated corpora, after the conversion and cleaning phase.

The following is an example of the resulting UFSAC XML:

ing/modifying them.

<corpus id="short_example">

<document id="d001" >

<paragraph>
<sentence>

<word surface_form="A" pos="DT" />
<word surface_form="precise"

wn30_key="precise%3:00:00::" />

<word surface_form="example"

pos="NN" lemma="example" />

<word surface_form="." />

</sentence>
</paragraph>

</document>

</corpus>

Our format thus allows to integrate the whole corpus in a
single ﬁle, and it is easily readable, especially comparing
to most original formats (c.f. the end of section 2.).

4.3. API and tools
An easy-to-use Java API is also provided to read, write
and modify efﬁciently corpora in our format. It allows two
styles of programming: you can either load a full corpus in
memory, perform all your calculations and save it entirely
in a ﬁle; or you can sequentially scan, edit or print a corpus
from a ﬁle, in a streaming manner. The latter is particularly
useful when working with huge ﬁles which do not ﬁt into
memory. Finally, we offer a set of scripts that perform the
conversion of a corpus from its original format to our one,
and some pre-processing and analyses scripts.

4.3.1. Core API
The core API is a package containing the base classes
for manipulating corpora. For simplicity, the class names
match exactly what is described in section 4.2..

The class Annotation describes an annotation on a lex-
ical entity. Concretely, it is a pair of Strings (name/value)
and a pointer to the annotated lexical entity.

The class LexicalEntity describes something that has
zero or more annotations, with public methods for access-

The class Word inherits from LexicalEntity, has a special
mandatory annotation surface form, which is the value
of the word, and a parent sentence.

The class Sentence inherits from LexicalEntity, contains
a list of words and a parent paragraph.

The class Paragraph inherits from LexicalEntity, con-
tains a list of sentences and a parent document.

The class Document inherits from LexicalEntity, contains
a list of paragraphs and a parent corpus.

Finally, the class Corpus inherits from LexicalEntity and
contains a list of documents.

few classes,

functions
These
Corpus.saveToXML and Corpus.loadFromXML
allow to create, save, load and modify any corpus easily.

coupled with

two

4.3.2. Streaming API
For some corpora particularly huge, like the OMSTI, we
also provide a sub-package streaming, which allows to
read, write or modify a corpus sequentially, without be-
ing fully loaded into memory. This is similar to the Java
SAX library (Simple API for XML), events are ﬁred when
reading a word, sentence, paragraph, etc., and the user can
choose to respond to this event or not.
In practice, we provide a set of classes which cover most
use cases.

class
respond

StreamingCorpusReader
to

allows
The
events readBeginCorpus,
to
the
readBeginDocument, readWord, etc..
This can
be useful for printing every word that is sense annotated
for example.

The class StreamingCorpusModifier allows to
modify a corpus in-place. This is specially useful for pre-
processing, for instance convert every word to lowercase.

class
creating

The
for
writeBeginSentence, writeWord and so on.

StreamingCorpusWriter
new corpus, with

used
its methods

is

a

1030

4.3.3. Scripts
Finally, we provide a set of examples and useful scripts
which use our format and our API. The scripts are Java
classes with a main method and are not part of any pack-
age.

The script ConvertOriginalCorpora allows to con-
vert all corpora listed in subsection 4.1. from their original
format to the UFSAC format. This is specially valuable for
non-free corpus like the DSO, that we cannot share directly
in our format, but that one can still buy in their original for-
mat, and then convert to our format. This script includes all
post-processing steps described in subsection 4.1..

scripts

ConvertFromRaganato

and
The
ConvertToRaganato allow to convert a corpus
from the format described by (Raganato et al., 2017) to
UFSAC, and vice-versa.

The script ComputeMostFrequentSenses will calcu-
late, for every lemma in WordNet, the most frequent sense
(MFS), based on all usages in the given UFSAC corpora.
This is helpful since in most evaluation campaigns, the
the score obtained when the MFS is
MFS baseline (i.e.
assigned to every word) is important, and it is generally
implicitly the sense distribution computed on the SemCor
only.

The scripts AddCorpusLemma and AddCorpusPOS use
respectively WordNet’s morphy and Stanford’s POS tag-
ger to annotate a corpus with the lemma and POS of every
word.

The script EvaluateWSD compare the sense annotations
produced by a WSD system to the gold standard annotation,
and compute the usual Precision, Recall, Coverage and F1
metrics for every given corpus.

The script GenerateCorpusStatistics is the one
that was used to produce the table 1.

5. Experiments
In this section, we show an example of using all UFSAC
corpora for the extension of a knowledge-based WSD sys-
tem based on the Lesk measure. This experiment shows
how this ressource can be used to easily improve an exist-
ing WSD system.

5.1. The Lesk and Extended Lesk Similarity

Measures

(Lesk, 1986) proposed a simple algorithm for lexical dis-
ambiguation that evaluates the similarity between two
senses (s1, s2) as the number of words in common in the
deﬁnitions of the senses from a dictionary (D(s1),D(s2)).
The Lesk measure compute an exact lexical match of the
surface forms of the words in the deﬁnitions. If important
words are missing or different synonyms of the same words
are used in the deﬁnition of related senses, the overlap mea-
sure will not capture the proximity of their meanings appro-
priately. As deﬁnitions (especially in Princeton Wordnet)
are very concise, it is difﬁcult to obtain ﬁne grained dis-
tinctions between senses.
In consequence, several variants of the Lesk measure tried
to alleviate this problem, for instance (Baldwin et al., 2010)

and (Miller et al., 2012), but the most common expan-
sion technique is the so-called “extended/adapted Lesk”
(Banerjee and Pedersen, 2002). The sense overlap is here
expanded with the overlap of all deﬁnitions from all pairs
of related senses in a lexico-semantic resource with a rich
structure, such as WordNet.
In our experiment, we will create another expansion of the
Lesk measure, based on UFSAC sense annotated corpora.

5.2. Expansion of Deﬁnitions Through UFSAC

Sense-Annotated Corpora

Our method consists in expanding deﬁnitions with all
neighbours of a target sense, taken from sense-annotated
corpora. We consider that a neighbour is a word found in
the same sentence as the target sense. More precisely, we
proceed as following:

1. We parse every UFSAC corpus, sentence by sentence.

2. For every word which is sense-annotated in a sentence,
we add to the deﬁnition of this sense in the dictionary
every other word present in the sentence.

That is, for every sentence S = w0, w1, . . . , wn, and for
every word wk inside S, we add to the deﬁnition of the
tagged sense of wk, i.e. D(s(wk)), every other words of
the sentence, i.e. wi∀i ∈ [0, n]i 6= k.
As a consequence, every sense’s deﬁnition in the dictionary
will be extended with words that are related to this sense,
in the same manner than (Banerjee and Pedersen, 2002)’s
extended Lesk, but with words taken from sense annotated
corpora.

5.3. Similarity-Based Word Sense

Disambiguation

Now for evaluating this new expansion to the Lesk mea-
sure, we must use a similarity-based WSD algorithm
that belongs to the broader category of knowledge-based
approaches (using dictionaries,
lexical base, encyclope-
dias. . . ). In such systems, the disambiguation process con-
sists of two layers: a local algorithm and a global algorithm.
The local algorithm computes the proximity of two word
senses, namely a semantic relatedness measure. The lo-
cal similarity measurement is then used to ﬁnd an optimal
global sense assignment for all the content words of the text
by the global algorithm. The local algorithm is here the
Lesk measure augmented with the sense annotated corpora.
We also ﬁltered out stopwords according to the “long” list
given in https://www.ranks.nl/stopwords.
As for global algorithms,
they are often probabilistic
combinatorial optimization algorithms, as WSD is funda-
mentally a discrete combinatorial optimization problem.
Many such algorithms have been adapted to WSD, in-
cluding genetic algorithms (Gelbukh et al., 2003), simu-
lated annealing (Cowie et al., 1992), ant colony algorithms
(Schwab et al., 2012) or more recently bee hive algorithms
(Abualhaija and Zimmermann, 2016).
The different global algorithms mainly differ in the con-
vergence speed to a close-to-optimal solution, however the
bottleneck to the accuracy of the algorithm is the local al-
gorithm (similarity measure) used, as it encodes the knowl-

1031

System

Lesk + UFSAC corpora

Lesk
Extended Lesk (Banerjee and Pedersen, 2003)
Most Frequent Sense Baseline

79.83%

68.70%
78.01%
78.90%

66.43%

50.65%
61.42%
67.10%

SemEval 2007 Task 07

SemEval 2015 Task 13

Table 2: F1 scores of our similarity-based system augmented with words taken from all UFSAC corpora (except the eval-
uation corpora) on SemEval 2007 coarse-grained all-words task and SemEval 2015 ﬁne-grained all-words task, compared
to the Lesk, Extended Lesk and MFS baselines.

edge from the resource that allows to discriminate between
the senses.
In this experiments, we use an adaptation to WSD of the
Cuckoo Search Algorithm, the state of the art in combina-
torial search algorithms (Yang and Deb, 2009). The algo-
rithm relies on the L´evy ﬂight distribution for an effective
(and more meaningful) sampling of the search space.
The Cuckoo Search Algorithm is probabilistic and its re-
sult differs slightly from an execution to another (by an
order of magnitude of less than 1%). So for each experi-
ment, 30 executions are performed. Then, using a Shapiro-
Wilk test (Shapiro and Wilk, 1965), we determined that
none of the result distribution follow a normal distribution.
Thus, we used a non-parametric Wilcoxon/Mann-Whitney-
U (Wilcoxon, 1945) (Mann and Whitney, 1947) test in or-
der to check the pairwise signiﬁcance (p < 0.01) of all
pairs of result distributions.

5.4. Results
We evaluate the performance of our expansion of deﬁni-
tions using all UFSAC corpora listed in subsection 4.1. ex-
cept the ones we evaluated our system on: SemEval 2007
task 7 and SemEval 2015 task 13. We compare our simi-
larity measure to the original Lesk and the Extended Lesk
(Banerjee and Pedersen, 2003) measures. The results are
presented in Table 2.
As we can see, our expansion of the deﬁnitions with words
taken from the UFSAC corpora improves considerably the
original Lesk measure, even more than the Extended Lesk
measure. Therefore, this experiment demonstrates how
much the addition of the UFSAC resource can improve a
similarity-based WSD sytem. Of course, every other kind
of WSD system can be improved, in particular supervised
systems which rely solely on sense-annotated corpora and
machine learning techniques (SVM, neural networks, etc.).

6. Conclusion
In this paper we advocate for a more uniform way of dis-
tributing sense annotated corpora, through a unique and un-
complicated ﬁle format. This uniﬁcation can facilitate both
the creation and the evaluation of Word Sense Disambigua-
tion systems. Indeed, sense annotated corpora are histor-
ically separated between those created for the purpose of
training, and those created for the purpose of evaluation.
In addition, the formats of these corpora are often very
different from each other: different ﬁle hierarchy, differ-
ent syntax, and different sense inventory are used. Conse-
quently, most WSD systems are trained and evaluated on

few corpora comparing to the amount of existing corpora.
Moreover, they are systematically evaluated only on cor-
pora originally created for the purpose of evaluation, and
trained only on corpora originally created for the purpose
of training, whereas they could beneﬁt from considering all
of them in both tasks.
The uniﬁcation of all sense annotated corpora hence allows
to quickly expand a system which is trained on some re-
sources to new data without the effort of writing another
parser. Also, a system can now easily include to its training
phase some corpora that were originally created for evalu-
ation, and/or evaluate its performance on parts of corpora
originally created for training. This easily allows a much
better coverage and a more ﬁne-grained analysis of a WSD
system performance.
In our language resource, we gathered all existing English
sense annotated corpora that we know, and we converted
them in a simple and consistent XML ﬁle format that we
named UFSAC. We also converted their sense annotations
to the last version of WordNet (3.0). The corpora are only
available when the licence authorizes it, but we also pro-
vide scripts that can easily convert a corpus from its origi-
nal format to the one we propose. Thus, anyone who pos-
sess the corpora that we cannot distribute can still bene-
ﬁt from this work.
In addition, we provide a complete
Java API for reading, writing and modifying corpora in
our uniﬁed format, along with example codes and tools
for many applications such as lemmatization, POS-tagging,
sense distribution estimation, etc. Finally, a demonstra-
tion of a simple use of all UFSAC corpora for extend-
ing a similarity-based WSD system is shown in section 5..
In the future, we plan to add to our resource other cor-
pora such as the corpora created for the lexical sample
tasks of SensEval/SemEval, and sense annotated corpora
in other languages. We also plan to improve the UFSAC
format by adding a better support for multiword expres-
sions. The resource will be continuously updated at this
url: https://github.com/getalp/UFSAC.

7. Bibliographical References

Abualhaija, S. and Zimmermann, K.-H. (2016). D-bees:
A novel method inspired by bee colony optimization for
solving word sense disambiguation. Swarm and Evolu-
tionary Computation, pages –.

Baldwin, T., Kim, S., Bond, F., Fujita, S., Martinez, D., and
Tanaka, T. (2010). A reexamination of mrd-based word
sense disambiguation. 9(1):4:1–4:21, March.

1032

Banerjee, S. and Pedersen, T. (2002). An adapted lesk al-
gorithm for word sense disambiguation using wordnet.
In CICLing 2002, Mexico City, February.

Banerjee, S. and Pedersen, T. (2003). Extended gloss over-
laps as a measure of semantic relatedness. In In Proceed-
ings of the Eighteenth International Joint Conference on
Artiﬁcial Intelligence, pages 805–810.

Burnard, L. (1998). The British National Corpus.
Chan, Y. S., Ng, H. T., and Zhong, Z. (2007). Nus-pt: Ex-
ploiting parallel texts for word sense disambiguation in
the english all-words tasks. In Proceedings of the 4th
International Workshop on Semantic Evaluations, Se-
mEval ’07, pages 253–256, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.

Chen, X., Liu, Z., and Sun, M. (2014). A uniﬁed model for
word sense representation and disambiguation. In Pro-
ceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages 1025–
1035, Doha, Qatar, October. Association for Computa-
tional Linguistics.

Cowie, J., Guthrie, J., and Guthrie, L.

(1992). Lexical
disambiguation using simulated annealing. In COLING
1992, volume 1, pages 359–365, Nantes, France, aoˆut.
Daud´e, J., Padr´o, L., and Rigau, G. (2000). Mapping word-
nets using structural information. In Proceedings of the
38th Annual Meeting on Association for Computational
Linguistics, ACL ’00, pages 504–511, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Francis, W. N. and Kuˇcera, H. (1964). A standard corpus
of present-day edited american english, for use with dig-
ital computers (brown). Technical report, Brown Univer-
sity, Providence, Rhode Island.

Gelbukh, A., Sidorov, G., and Han, S. Y. (2003). Evolu-
tionary approach to natural language wsd through global
coherence optimization. WSEAS Transactions on Com-
munications, 2(1):11–19.

Habert, B., Fabre, C., and Issac, F. (1998). DE L’ECRIT
AU NUMERIQUE. Constituer, normaliser et exploiter
les corpus ´electroniques. Number ISBN : 2-225-82953-
5. ELSEVIER MASSON.

Ide, N. and Macleod, C. (2001). The american national
corpus: A standardized resource of american english. In
Proceedings of Corpus Linguistics 2001, volume 3.

Lesk, M. (1986). Automatic sense disambiguation using
mrd: how to tell a pine cone from an ice cream cone. In
Proceedings of SIGDOC ’86, pages 24–26, New York,
NY, USA. ACM.

Mann, H. B. and Whitney, D. R. (1947). On a Test of
Whether one of Two Random Variables is Stochasti-
cally Larger than the Other. The Annals of Mathematical
Statistics, 18(1):50–60.

Miller, G. A., Leacock, C., Tengi, R., and Bunker, R. T.
(1993). A semantic concordance. In Proceedings of the
workshop on Human Language Technology, HLT ’93,
pages 303–308, Stroudsburg, PA, USA. Association for
Computational Linguistics.

ceedings of COLING 2012, pages 1781–1796, Mumbai,
India, December. The COLING 2012 Organizing Com-
mittee.

Miller, G. A. (1995). Wordnet: A lexical database. ACM,

Vol. 38(No. 11):p. 1–41.

Moro, A. and Navigli, R. (2015). Semeval-2015 task 13:
Multilingual all-words sense disambiguation and entity
linking. In Proceedings of the 9th International Work-
shop on Semantic Evaluation (SemEval 2015), pages
288–297, Denver, Colorado, June. Association for Com-
putational Linguistics.

M`arquez, L., Raya, J., Carroll, J., McCarthy, D., Agirre, E.,
Mart´ınez, D., Strapparava, C., and Gliozzo, A. (2002).
Experiment a : Several all-words wsd systems for en-
glish. Technical report, Meaning, Developing multilin-
gual Web-scale Language Technologies.

Navigli, R. and Ponzetto, S. P. (2010). Babelnet: Build-
ing a very large multilingual semantic network. In Pro-
ceedings of the 48th annual meeting of the association
for computational linguistics, pages 216–225. Associa-
tion for Computational Linguistics.

Navigli, R., Litkowski, K. C., and Hargraves, O. (2007).
Semeval-2007 task 07: Coarse-grained english all-words
task. In SemEval-2007, pages 30–35, Prague, Czech Re-
public, June.

Navigli, R. (2012). A quick tour of word sense disam-
biguation, induction and related approaches. In Proceed-
ings of the 38th Conference on Current Trends in The-
ory and Practice of Computer Science (SOFSEM), pages
115–129.

Raganato, A., Camacho-Collados, J., and Navigli, R.
(2017). Word sense disambiguation: A uniﬁed evalua-
tion framework and empirical comparison. In Proceed-
ings of the 15th Conference of the European Chapter of
the Association for Computational Linguistics: Volume
1, Long Papers, pages 99–110, Valencia, Spain, April.
Association for Computational Linguistics.

Schwab, D., Goulian, J., Tchechmedjiev, A., and Blan-
chon, H. (2012). Ant Colony Algorithm for the Unsu-
pervised Word Sense Disambiguation of Texts: Compar-
ison and Evaluation. In Proceedings of the 25th Interna-
tional Conference on Computational Linguistics (COL-
ING 2012), Mumbai (India), dec.

Schwab, D., Goulian, J., and Tchechmedjiev, A. (2013).
D´esambigu¨ısation lexicale de textes : efﬁcacit´e qualita-
tive et temporelle d’un algorithme `a colonies de fourmis.
TAL, 54(1):99–138.

Shapiro, S. S. and Wilk, M. B. (1965). An analysis of vari-
ance test for normality (complete samples). Biometrika,
3(52).

Taghipour, K. and Ng, H. T. (2015). One million sense-
tagged instances for word sense disambiguation and in-
duction. In Proceedings of the Nineteenth Conference on
Computational Natural Language Learning, pages 338–
344, Beijing, China, July. Association for Computational
Linguistics.

Miller, T., Biemann, C., Zesch, T., and Gurevych, I. (2012).
Using distributional similarity for lexical expansion in
knowledge-based word sense disambiguation. In Pro-

Toutanova, K., Klein, D., Manning, C. D., and Singer, Y.
(2003). Feature-rich part-of-speech tagging with a cyclic
dependency network. In Proceedings of the 2003 Con-

1033

ference of the North American Chapter of the Associa-
tion for Computational Linguistics on Human Language
Technology - Volume 1, NAACL ’03, pages 173–180,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

Vial, L., Tchechmedjiev, A., and Schwab, D. (2016). Ex-
tension lexicale de d´eﬁnitions grˆace `a des corpus annot´es
en sens. In Traitement Automatique des Langues Na-
turelles (TALN).

Vial, L., Lecouteux, B., and Schwab, D.

(2017). Uni-
formisation de corpus anglais annot´es en sens. In 24`eme
Conf´erence sur le Traitement Automatique des Langues
Naturelles, Orl´eans, France, June.

Wilcoxon, F. (1945). Individual Comparisons by Ranking
Methods. Biometrics Bulletin, 1(6):80–83, December.
Yang, X.-S. and Deb, S. (2009). Cuckoo search via l´evy
ﬂights. Proc. of World Congress on Nature and Biologi-
cally Inspired Computing, pages 210–214.

Yuan, D., Richardson, J., Doherty, R., Evans, C., and Al-
tendorf, E. (2016). Semi-supervised word sense disam-
biguation with neural models. In COLING 2016.

8. Language Resource References
Hovy et al. (2006). OntoNotes: The 90% Solution.
Ide et al. (2008). MASC: the Manually Annotated Sub-

Corpus of American English.

Miller et al. (1993). A Semantic Concordance.
Miller. (1995). Wordnet: A Lexical Database.
Ng and Lee. (1997). DSO Corpus of Sense-Tagged En-

glish.

Taghipour and Ng. (2015). One Million Sense-Tagged In-
stances for Word Sense Disambiguation and Induction.

1034

UFSAC: Uniﬁcation of Sense Annotated Corpora and Tools

Lo¨ıc Vial, Benjamin Lecouteux, Didier Schwab
LIG – GETALP – Univ. Grenoble Alpes – France
{loic.vial, benjamin.lecouteux, didier.schwab}@imag.fr

Abstract
In Word Sense Disambiguation, sense annotated corpora are often essential for evaluating a system and also valuable in order to reach a
good efﬁciency. Always created for a speciﬁc purpose, there are today a dozen of sense annotated English corpora, in various formats
and using different versions of WordNet. The main hypothesis of this work is that it should be possible to build a disambiguation system
by using any of these corpora during the training phase or during the testing phase regardless of their original purpose. In this article, we
present UFSAC: a format of corpus that can be used for either training or testing a disambiguation system, and the process we followed
for constructing this format. We give to the community the whole set of sense annotated English corpora that we know, in this uniﬁed
format, when the copyright allows it, with sense keys converted to the last version of WordNet. We also provide the source code for
building these corpora from their original data, and a complete Java API for manipulating corpora in this format. The whole resource is
available at the following URL: https://github.com/getalp/UFSAC.

Keywords: Word Sense Disambiguation, sense annotated corpora, uniﬁed resource, tools

1.

Introduction

Whether they are used for the evaluation or for the learn-
ing process of a Word Sense Disambiguation (WSD) sys-
tem, the importance of sense annotated corpora in Natural
Language Processing (NLP) is considerable. On one hand,
the evaluation in vivo, i.e.
the evaluation of a WSD sys-
tem as part of a larger task, has never been really exploited.
On the other hand, the evaluation in vitro, which uses di-
rectly sense annotated corpora by comparing the output of
a system to manual annotations, is predominant. Moreover,
WSD systems exploiting examples from sense annotated
corpora are generally far better than those which do not
(Navigli et al., 2007; Moro and Navigli, 2015).
At the time of its creation, WordNet (Miller, 1995) was un-
doubtedly the only lexical database freely available for En-
glish. Since the beginning of the 2000s, it has become the
de facto standard for WSD in this language. Indeed, most of
sense annotated corpora are either directly annotated with
WordNet sense keys or they are annotated with a sense in-
ventory linked to the senses of WordNet, such as BabelNet
(Navigli and Ponzetto, 2010).
However, it is not trivial to use these corpora, because most
of them differ in their format and on the version of Word-
Net they use. As a consequence, very few works in the lit-
erature of WSD are trained or evaluated on more than two
annotated corpora.
Also, WSD systems are systematically evaluated on cor-
pora that have been initially created for the purpose of eval-
uation, and never on corpora that have been created for an-
other purpose, such as training or for sense distribution es-
timation, whereas there is no scientiﬁc reason for that.
This paper presents a work of uniﬁcation of all existing
English corpora annotated with any version of WordNet to
our knowledge, in a unique format, easy to understand, and
easy to work with in practice. We put on the same level the
corpora originally created for the evaluation and those for
the learning, so to facilitate the creation of robust WSD sys-
tems which could for example be evaluated in a way where
all corpora except one are used for the learning, and the

remaining one is used for the evaluation, then switch the
corpora and do this for every existing corpus.
The language resource that we provide contains all En-
glish sense annotated corpora in UFSAC (Uniﬁed Format
for Sense Annotated Corpora), the format that we pro-
pose, with sense annotations converted to the last version
of WordNet (3.0), along with Java code to easily read, write
and modify any corpus in this format, and scripts for con-
verting a corpus from its original format to UFSAC.
the demonstration of
Our work is the continuity of
(Vial et al., 2017), and it differs from the recent work of
(Raganato et al., 2017) in several points. Their work is fo-
cused on the evaluation of WSD systems, whereas we pro-
vide a complete API for manipulating corpora in a new uni-
ﬁed format (UFSAC), and conversion scripts allowing the
full reconstruction of the corpora from the original data. We
also propose ﬁve additional corpora in our resource among
the most difﬁcult to parse.
In our resource, we provide a script for converting a cor-
pus from our format to theirs, so existing WSD systems
that rely on their format can be trained or evaluated on any
of the corpus that we produced. We also provide a script
for converting their format to ours in order to facilitate any
collaborative work in the community.

2. Sense Annotated Corpora: rare and

costly resources

Generally speaking, a corpus is a collection of documents
which can be used as samples of text for a particular lan-
guage (Habert et al., 1998). A corpus may contain several
millions of words, which can be lemmatized and annotated
with information concerning their part of speech for exam-
ple. Among these corpora, we can ﬁnd the British National
Corpus (Burnard, 1998) (100 million words) and the Amer-
ican National Corpus (Ide and Macleod, 2001) (20 million
words). The texts come from several sources such as news-
papers, books, encyclopedias or from the Web.
A sense annotated corpus is a corpus in which some or
all words are annotated with an identiﬁer of sense from

1027

a speciﬁc lexical database. For example, all words in
the corpus of the 7th task of the SemEval 2007 seman-
tic evaluation campaign (Navigli et al., 2007) are anno-
tated with sense identiﬁers from WordNet 2.1, whereas
in the English corpus of the 13th task of SemEval 2015
(Moro and Navigli, 2015), all words are annotated with
sense identiﬁers from WordNet 3.0, BabelNet 2.5 and
Wikipedia pages.
There are at least three reasons to create a sense annotated
corpus:

It

• Estimate the distribution of senses in the lan-
guage.
the SemCor
is for this purpose that
(Miller et al., 1993) was annotated. Consequently, the
senses in WordNet are, since version 1.7, sorted by
this distribution of senses estimated on the SemCor.

• Build

a Word Sense Disambiguation

system
from examples contained in the
the OMSTI
instance,
this
for

which learns
annotated corpus.
(Taghipour and Ng, 2015) was
purpose.

created

For

• Evaluate a WSD system by comparing its output to
the annotations in the corpus, as it is the case for in-
stance with corpora created as part of the evaluation
campaigns SensEval-SemEval.

After their distribution, there is no scientiﬁc reason not
to use indistinctly these corpora either for building a
WSD system, for estimating the distribution of senses or
Indeed, the SemCor is
for evaluating a WSD system.
used since a long time for the learning of WSD systems
(Chan et al., 2007; Navigli et al., 2007) or more recently
for the evaluation of different methods (Yuan et al., 2016).
This last usage is still very rare, since it is one of the
ﬁrst experiment that we found in the literature, along with
(M`arquez et al., 2002).
However, the format of the resources differs greatly de-
pending on their original purpose. For the SemCor, a single
ﬁle groups all the information, whereas in the case of the
evaluation corpora, there are two ﬁles: one that contains the
unannotated corpus, and the other that contains the sense
annotations. In some corpora, like in the DSO and the OM-
STI, there is one ﬁle for every lemma in the dictionary, and
each ﬁle contains thousands of example sentences, where
this lemma is the only word that is sense annotated.
Few data are manually sense annotated. The Global Word-
Net Association made a list of 26 corpora annotated with
WordNet 1. These corpora concern 17 languages, but only
three of them reach 100,000 annotations. English, with
more than 2 million words sense annotated ranks ﬁrst, be-
fore Dutch with nearly 300,000 annotations and Bulgarian
with 100,000 annotations. Thus, it is unsurprising that most
of researches in WSD focus on English.

3. A single format for sense annotated

corpora

The main purpose of this work is to help the construction
and the evaluation of WSD systems, by giving to the com-

munity the set of all existing English sense annotated cor-
pora to our knowledge, in the same format, using the same
sense inventory, and tools to easily parse them, manipulate
them, and convert corpora from their original format to our
one.
Indeed, a large quantity of sense annotated data is vi-
tal for the construction of WSD systems.
In evaluation
campaigns, this often makes the difference. For exam-
ple,
looking at the data from the SemEval 2007 cam-
paign (Navigli et al., 2007), which most of the recent sys-
tems were evaluated on, we observe that systems that
did not use sense annotated data obtain a precision score
up to 78 − 79%2 (Schwab et al., 2013) (Chen et al., 2014)
whereas those which use a lot of annotated data reach
a score up to 82% (Chan et al., 2007) (Navigli, 2012)
(Vial et al., 2016) and even 84% (Yuan et al., 2016).
Therefore, having all existing corpora in a unique format
and using the same sense inventory offers several advan-
tages: it allows to easily expand the quantity of data avail-
able for improving WSD systems, it allows to better esti-
mate the distribution of senses in English, and ﬁnally, this
format can help creating more robust WSD systems. In-
deed, we still ﬁnd a lot of works that focus on a single evalu-
ation task (Vial et al., 2016; Chen et al., 2014), and in these
cases, the analysis of the results concerning the robustness
of the methods is limited. The uniﬁcation of the format of
sense annotated corpora could improve the evaluation pro-
cess by facilitating a cross validation process for instance,
where the system is evaluated sequentially on every corpus,
with all others used for the training.

4. Provided resource
Our work consists in gathering all English corpora sense
annotated with WordNet, and convert all of them to a
uniﬁed format that is able to contain all the informations
present in the original format. We created format conver-
sion scripts for this purpose, as well as scripts for clean-
ing the corpora, and converting the sense annotation to the
last version of WordNet (3.0). The resulting corpora are
parts of the resource when the copyright allows it, along
with the format conversion scripts, the cleaning scripts,
and the sense conversion scripts. For the corpora that
we cannot distribute because of the licence, anyone that
possess them can still run our scripts to turn the origi-
nal resource into our format. Finally, an API is provided
for parsing, creating and manipulating corpora in our for-
mat. The resource is accessible at the following URL:
https://github.com/getalp/UFSAC.

4.1. Sense annotated corpora
Our resource contains the following corpora:

• The SemCor (Miller et al., 1993), a subset of the
Brown Corpus (Francis and Kuˇcera, 1964). Original
annotations are done with WordNet 1.6.

• The DSO (Defence Science Organisation) (Ng and
Lee, 1997), a non-free corpus, that is focused on 121

1http://globalwordnet.org/wordnet-annotated-corpora/

the human annotators in 78 to 79% of cases

2This means that the system has chosen the same sense than

1028

nouns and 70 verbs among the most frequently used
and the most ambiguous words in English and have
been annotated in various contexts with WordNet 1.5.

• The WordNet Gloss Tag 3, a corpus which consists of
all deﬁnitions of WordNet (Miller, 1995) with every
words sense annotated since version 3.0.

• The OMSTI (One Million Sense-Tagged Instances)
(Taghipour and Ng, 2015), a corpus of approximately
one million words sense annotated with WordNet 3.0.

• The MASC (Manually Annotated Sub-Corpus) (Ide et
al., 2008), we used the version given in the article of
(Yuan et al., 2016), annotated with the NOAD (New
Oxford American Dictionary), but with corresponding
WordNet 3.0 sense keys.

• The Ontonotes 5.0 (Hovy et al., 2006), annotated with

WordNet 3.0.

• The corpora of

the WSD evaluation campaigns
SemEval-SensEval: SensEval 2 (using WordNet 1.7),
SensEval 3 (WN 1.7.1), SemEval 2007 (WN 2.1), Se-
mEval 2013 (WN 3.0) and SemEval 2015 (WN 3.0).

Table 1 summarizes statistics concerning these corpora.
After the conversion of all these corpora into our format,
we executed four post-processing steps: sense annotation
conversion, identical sentences merging, lemma and POS
tagging, and ﬁnally a cleaning step.

4.1.1. Sense Annotation Conversion
Sense annotations have been converted, when necessary,
from their original WordNet sense key to the last ver-
sion of WordNet (3.0) thanks to conversion tables from
(Daud´e et al., 2000).
However, because some senses have been dropped from the
old versions of WordNet, some sense annotations have not
been converted. In any case, the original sense annotations
are always kept alongside the converted sense annotation.
When a sense is mapped to two or more senses with equal
probability, all resulting senses are added to the word anno-
tations, separated by a semicolon.

Identical sentences merging

4.1.2.
This step is only applied on the DSO and the OMSTI: be-
cause these corpora are constructed such that they contain
lists of sentences with only one word that is sense anno-
tated, surrounded by words not annotated, some sentences
are present in different places across the corpus, but with
different words that are sense annotated.
The merging phase identiﬁes identical sentences with an-
notations on different words, and creates a single sentence
containing all annotations. Thus, this steps adds a cru-
cial information for some WSD systems. For instance,
a similarity-based WSD system can now “learn” that two
word senses are often located in the same sentence.

3http://wordnet.princeton.edu/glosstag.shtml

4.1.3. Lemma and POS tagging
For the corpora that do not already contain these informa-
tions, we added the lemma for every word, when existing,
using the WordNet’s morphy tool, and the part-of-speech
tag from the Penn Treebank tag set using Stanford’s Log-
linear POS tagger (Toutanova et al., 2003).

4.1.4. Cleaning
Finally, this last step consists of trimming words, removing
invisible characters and removing inconsistent annotations,
for instance when the part of speech annotation differs from
the part of speech of the sense annotation.

4.2. UFSAC File format

Our approach for the uniﬁcation of the different annotated
corpora begins with a ﬁle format that is descriptive, easily
understandable and readable by a human, and at the same
time, efﬁcient for a program to parse and create. Finally, it
should be able to contain all the information contained in
the original resources. These informations are represented
with the following concepts:
– A Lexical Entity (LE) is an entity that contains a set of
annotations.
– A Corpus is a LE which contains a set of documents.
– A Document is a LE hich contains a set of paragraphs.
– A Paragraph is a LE which contains a set of sentences.
– A Sentence is a LE which contains a set of words.
– A Word is a LE which has a special mandatory annotation
“surface form”, which is the value of the word.

In order to represent these concepts, UFSAC is based on a
simple XML syntax with some conventions: lexical enti-
ties are represented by XML nodes (corpus, document,
paragraph, sentence and word), and annotations are
node attributes.

The annotations also follow a certain convention, we used
the following to annotate words:
– The identiﬁer (id) of a lexical entity, particularly use-
ful for corpora originally created for the evaluation (e.g.
“d001.s002.t003”).
– The surface form (surface form) of a word.
– The lemma (lemma) of a word.
– The part of speech (pos) of a word.
– The sense of a word,
in a speciﬁc lexical database,
for example WordNet 3.0 (wn30 key), WordNet 1.7.1
(wn171 key)... If multiple senses are speciﬁed (it is the
case in the coarse-grained task of SemEval 2007 for in-
stance), they are separated with a semicolon (;).
The information of the sense is the one which is the most
useful in our case, and it is speciﬁc to each lexical database,
instead of having a unique “sense” annotation as we can
ﬁnd in most other formats. That way we allow multiples
sense annotations from different lexical databases at the
same time. For example, the DSO is originally annotated
with senses from WordNet 1.5, and the conversion to Word-
Net 3.0 is sometimes impossible for some senses which
were deleted between the two versions. This convention
allows us to keep the original annotations, yet to have the
annotations from the last version of WordNet, or any other
lexical database (for instance BabelNet) at the same time.

1029

Corpus

Sentences

Words

Annotated parts of speech

Total

Annotated Nouns

Verbs

Adj.

Adv.

SemCor
DSO
WordNet GlossTag
MASC
OMSTI
Ontonotes
SemEval 2007 task 07
SemEval 2007 task 17
SemEval 2013 task 12
SemEval 2015 task 13
Senseval 2
Senseval 3 task 1

37176
178119
117659
34217
820557
21938
245
120
306
138
238
300

778587
5317184
1634691
596333
35843024
435340
5637
3395
8142
2638
5589
5511

229517
176915
496776
114950
920794
52263
2261
455
1644
1053
2301
1957

87581
105925
232319
49263
476944
9220
1108
159
1644
554
1061
886

89037
70990
62211
40325
253644
43042
591
296
0
251
541
723

33751
0
84233
25016
190206
0
356
0
0
166
422
336

19148
0
19445
0
0
0
206
0
0
82
277
12

Table 1: Statistics related to our set of annotated corpora, after the conversion and cleaning phase.

The following is an example of the resulting UFSAC XML:

ing/modifying them.

<corpus id="short_example">

<document id="d001" >

<paragraph>
<sentence>

<word surface_form="A" pos="DT" />
<word surface_form="precise"

wn30_key="precise%3:00:00::" />

<word surface_form="example"

pos="NN" lemma="example" />

<word surface_form="." />

</sentence>
</paragraph>

</document>

</corpus>

Our format thus allows to integrate the whole corpus in a
single ﬁle, and it is easily readable, especially comparing
to most original formats (c.f. the end of section 2.).

4.3. API and tools
An easy-to-use Java API is also provided to read, write
and modify efﬁciently corpora in our format. It allows two
styles of programming: you can either load a full corpus in
memory, perform all your calculations and save it entirely
in a ﬁle; or you can sequentially scan, edit or print a corpus
from a ﬁle, in a streaming manner. The latter is particularly
useful when working with huge ﬁles which do not ﬁt into
memory. Finally, we offer a set of scripts that perform the
conversion of a corpus from its original format to our one,
and some pre-processing and analyses scripts.

4.3.1. Core API
The core API is a package containing the base classes
for manipulating corpora. For simplicity, the class names
match exactly what is described in section 4.2..

The class Annotation describes an annotation on a lex-
ical entity. Concretely, it is a pair of Strings (name/value)
and a pointer to the annotated lexical entity.

The class LexicalEntity describes something that has
zero or more annotations, with public methods for access-

The class Word inherits from LexicalEntity, has a special
mandatory annotation surface form, which is the value
of the word, and a parent sentence.

The class Sentence inherits from LexicalEntity, contains
a list of words and a parent paragraph.

The class Paragraph inherits from LexicalEntity, con-
tains a list of sentences and a parent document.

The class Document inherits from LexicalEntity, contains
a list of paragraphs and a parent corpus.

Finally, the class Corpus inherits from LexicalEntity and
contains a list of documents.

few classes,

functions
These
Corpus.saveToXML and Corpus.loadFromXML
allow to create, save, load and modify any corpus easily.

coupled with

two

4.3.2. Streaming API
For some corpora particularly huge, like the OMSTI, we
also provide a sub-package streaming, which allows to
read, write or modify a corpus sequentially, without be-
ing fully loaded into memory. This is similar to the Java
SAX library (Simple API for XML), events are ﬁred when
reading a word, sentence, paragraph, etc., and the user can
choose to respond to this event or not.
In practice, we provide a set of classes which cover most
use cases.

class
respond

StreamingCorpusReader
to

allows
The
events readBeginCorpus,
to
the
readBeginDocument, readWord, etc..
This can
be useful for printing every word that is sense annotated
for example.

The class StreamingCorpusModifier allows to
modify a corpus in-place. This is specially useful for pre-
processing, for instance convert every word to lowercase.

class
creating

The
for
writeBeginSentence, writeWord and so on.

StreamingCorpusWriter
new corpus, with

used
its methods

is

a

1030

4.3.3. Scripts
Finally, we provide a set of examples and useful scripts
which use our format and our API. The scripts are Java
classes with a main method and are not part of any pack-
age.

The script ConvertOriginalCorpora allows to con-
vert all corpora listed in subsection 4.1. from their original
format to the UFSAC format. This is specially valuable for
non-free corpus like the DSO, that we cannot share directly
in our format, but that one can still buy in their original for-
mat, and then convert to our format. This script includes all
post-processing steps described in subsection 4.1..

scripts

ConvertFromRaganato

and
The
ConvertToRaganato allow to convert a corpus
from the format described by (Raganato et al., 2017) to
UFSAC, and vice-versa.

The script ComputeMostFrequentSenses will calcu-
late, for every lemma in WordNet, the most frequent sense
(MFS), based on all usages in the given UFSAC corpora.
This is helpful since in most evaluation campaigns, the
the score obtained when the MFS is
MFS baseline (i.e.
assigned to every word) is important, and it is generally
implicitly the sense distribution computed on the SemCor
only.

The scripts AddCorpusLemma and AddCorpusPOS use
respectively WordNet’s morphy and Stanford’s POS tag-
ger to annotate a corpus with the lemma and POS of every
word.

The script EvaluateWSD compare the sense annotations
produced by a WSD system to the gold standard annotation,
and compute the usual Precision, Recall, Coverage and F1
metrics for every given corpus.

The script GenerateCorpusStatistics is the one
that was used to produce the table 1.

5. Experiments
In this section, we show an example of using all UFSAC
corpora for the extension of a knowledge-based WSD sys-
tem based on the Lesk measure. This experiment shows
how this ressource can be used to easily improve an exist-
ing WSD system.

5.1. The Lesk and Extended Lesk Similarity

Measures

(Lesk, 1986) proposed a simple algorithm for lexical dis-
ambiguation that evaluates the similarity between two
senses (s1, s2) as the number of words in common in the
deﬁnitions of the senses from a dictionary (D(s1),D(s2)).
The Lesk measure compute an exact lexical match of the
surface forms of the words in the deﬁnitions. If important
words are missing or different synonyms of the same words
are used in the deﬁnition of related senses, the overlap mea-
sure will not capture the proximity of their meanings appro-
priately. As deﬁnitions (especially in Princeton Wordnet)
are very concise, it is difﬁcult to obtain ﬁne grained dis-
tinctions between senses.
In consequence, several variants of the Lesk measure tried
to alleviate this problem, for instance (Baldwin et al., 2010)

and (Miller et al., 2012), but the most common expan-
sion technique is the so-called “extended/adapted Lesk”
(Banerjee and Pedersen, 2002). The sense overlap is here
expanded with the overlap of all deﬁnitions from all pairs
of related senses in a lexico-semantic resource with a rich
structure, such as WordNet.
In our experiment, we will create another expansion of the
Lesk measure, based on UFSAC sense annotated corpora.

5.2. Expansion of Deﬁnitions Through UFSAC

Sense-Annotated Corpora

Our method consists in expanding deﬁnitions with all
neighbours of a target sense, taken from sense-annotated
corpora. We consider that a neighbour is a word found in
the same sentence as the target sense. More precisely, we
proceed as following:

1. We parse every UFSAC corpus, sentence by sentence.

2. For every word which is sense-annotated in a sentence,
we add to the deﬁnition of this sense in the dictionary
every other word present in the sentence.

That is, for every sentence S = w0, w1, . . . , wn, and for
every word wk inside S, we add to the deﬁnition of the
tagged sense of wk, i.e. D(s(wk)), every other words of
the sentence, i.e. wi∀i ∈ [0, n]i 6= k.
As a consequence, every sense’s deﬁnition in the dictionary
will be extended with words that are related to this sense,
in the same manner than (Banerjee and Pedersen, 2002)’s
extended Lesk, but with words taken from sense annotated
corpora.

5.3. Similarity-Based Word Sense

Disambiguation

Now for evaluating this new expansion to the Lesk mea-
sure, we must use a similarity-based WSD algorithm
that belongs to the broader category of knowledge-based
approaches (using dictionaries,
lexical base, encyclope-
dias. . . ). In such systems, the disambiguation process con-
sists of two layers: a local algorithm and a global algorithm.
The local algorithm computes the proximity of two word
senses, namely a semantic relatedness measure. The lo-
cal similarity measurement is then used to ﬁnd an optimal
global sense assignment for all the content words of the text
by the global algorithm. The local algorithm is here the
Lesk measure augmented with the sense annotated corpora.
We also ﬁltered out stopwords according to the “long” list
given in https://www.ranks.nl/stopwords.
As for global algorithms,
they are often probabilistic
combinatorial optimization algorithms, as WSD is funda-
mentally a discrete combinatorial optimization problem.
Many such algorithms have been adapted to WSD, in-
cluding genetic algorithms (Gelbukh et al., 2003), simu-
lated annealing (Cowie et al., 1992), ant colony algorithms
(Schwab et al., 2012) or more recently bee hive algorithms
(Abualhaija and Zimmermann, 2016).
The different global algorithms mainly differ in the con-
vergence speed to a close-to-optimal solution, however the
bottleneck to the accuracy of the algorithm is the local al-
gorithm (similarity measure) used, as it encodes the knowl-

1031

System

Lesk + UFSAC corpora

Lesk
Extended Lesk (Banerjee and Pedersen, 2003)
Most Frequent Sense Baseline

79.83%

68.70%
78.01%
78.90%

66.43%

50.65%
61.42%
67.10%

SemEval 2007 Task 07

SemEval 2015 Task 13

Table 2: F1 scores of our similarity-based system augmented with words taken from all UFSAC corpora (except the eval-
uation corpora) on SemEval 2007 coarse-grained all-words task and SemEval 2015 ﬁne-grained all-words task, compared
to the Lesk, Extended Lesk and MFS baselines.

edge from the resource that allows to discriminate between
the senses.
In this experiments, we use an adaptation to WSD of the
Cuckoo Search Algorithm, the state of the art in combina-
torial search algorithms (Yang and Deb, 2009). The algo-
rithm relies on the L´evy ﬂight distribution for an effective
(and more meaningful) sampling of the search space.
The Cuckoo Search Algorithm is probabilistic and its re-
sult differs slightly from an execution to another (by an
order of magnitude of less than 1%). So for each experi-
ment, 30 executions are performed. Then, using a Shapiro-
Wilk test (Shapiro and Wilk, 1965), we determined that
none of the result distribution follow a normal distribution.
Thus, we used a non-parametric Wilcoxon/Mann-Whitney-
U (Wilcoxon, 1945) (Mann and Whitney, 1947) test in or-
der to check the pairwise signiﬁcance (p < 0.01) of all
pairs of result distributions.

5.4. Results
We evaluate the performance of our expansion of deﬁni-
tions using all UFSAC corpora listed in subsection 4.1. ex-
cept the ones we evaluated our system on: SemEval 2007
task 7 and SemEval 2015 task 13. We compare our simi-
larity measure to the original Lesk and the Extended Lesk
(Banerjee and Pedersen, 2003) measures. The results are
presented in Table 2.
As we can see, our expansion of the deﬁnitions with words
taken from the UFSAC corpora improves considerably the
original Lesk measure, even more than the Extended Lesk
measure. Therefore, this experiment demonstrates how
much the addition of the UFSAC resource can improve a
similarity-based WSD sytem. Of course, every other kind
of WSD system can be improved, in particular supervised
systems which rely solely on sense-annotated corpora and
machine learning techniques (SVM, neural networks, etc.).

6. Conclusion
In this paper we advocate for a more uniform way of dis-
tributing sense annotated corpora, through a unique and un-
complicated ﬁle format. This uniﬁcation can facilitate both
the creation and the evaluation of Word Sense Disambigua-
tion systems. Indeed, sense annotated corpora are histor-
ically separated between those created for the purpose of
training, and those created for the purpose of evaluation.
In addition, the formats of these corpora are often very
different from each other: different ﬁle hierarchy, differ-
ent syntax, and different sense inventory are used. Conse-
quently, most WSD systems are trained and evaluated on

few corpora comparing to the amount of existing corpora.
Moreover, they are systematically evaluated only on cor-
pora originally created for the purpose of evaluation, and
trained only on corpora originally created for the purpose
of training, whereas they could beneﬁt from considering all
of them in both tasks.
The uniﬁcation of all sense annotated corpora hence allows
to quickly expand a system which is trained on some re-
sources to new data without the effort of writing another
parser. Also, a system can now easily include to its training
phase some corpora that were originally created for evalu-
ation, and/or evaluate its performance on parts of corpora
originally created for training. This easily allows a much
better coverage and a more ﬁne-grained analysis of a WSD
system performance.
In our language resource, we gathered all existing English
sense annotated corpora that we know, and we converted
them in a simple and consistent XML ﬁle format that we
named UFSAC. We also converted their sense annotations
to the last version of WordNet (3.0). The corpora are only
available when the licence authorizes it, but we also pro-
vide scripts that can easily convert a corpus from its origi-
nal format to the one we propose. Thus, anyone who pos-
sess the corpora that we cannot distribute can still bene-
ﬁt from this work.
In addition, we provide a complete
Java API for reading, writing and modifying corpora in
our uniﬁed format, along with example codes and tools
for many applications such as lemmatization, POS-tagging,
sense distribution estimation, etc. Finally, a demonstra-
tion of a simple use of all UFSAC corpora for extend-
ing a similarity-based WSD system is shown in section 5..
In the future, we plan to add to our resource other cor-
pora such as the corpora created for the lexical sample
tasks of SensEval/SemEval, and sense annotated corpora
in other languages. We also plan to improve the UFSAC
format by adding a better support for multiword expres-
sions. The resource will be continuously updated at this
url: https://github.com/getalp/UFSAC.

7. Bibliographical References

Abualhaija, S. and Zimmermann, K.-H. (2016). D-bees:
A novel method inspired by bee colony optimization for
solving word sense disambiguation. Swarm and Evolu-
tionary Computation, pages –.

Baldwin, T., Kim, S., Bond, F., Fujita, S., Martinez, D., and
Tanaka, T. (2010). A reexamination of mrd-based word
sense disambiguation. 9(1):4:1–4:21, March.

1032

Banerjee, S. and Pedersen, T. (2002). An adapted lesk al-
gorithm for word sense disambiguation using wordnet.
In CICLing 2002, Mexico City, February.

Banerjee, S. and Pedersen, T. (2003). Extended gloss over-
laps as a measure of semantic relatedness. In In Proceed-
ings of the Eighteenth International Joint Conference on
Artiﬁcial Intelligence, pages 805–810.

Burnard, L. (1998). The British National Corpus.
Chan, Y. S., Ng, H. T., and Zhong, Z. (2007). Nus-pt: Ex-
ploiting parallel texts for word sense disambiguation in
the english all-words tasks. In Proceedings of the 4th
International Workshop on Semantic Evaluations, Se-
mEval ’07, pages 253–256, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.

Chen, X., Liu, Z., and Sun, M. (2014). A uniﬁed model for
word sense representation and disambiguation. In Pro-
ceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages 1025–
1035, Doha, Qatar, October. Association for Computa-
tional Linguistics.

Cowie, J., Guthrie, J., and Guthrie, L.

(1992). Lexical
disambiguation using simulated annealing. In COLING
1992, volume 1, pages 359–365, Nantes, France, aoˆut.
Daud´e, J., Padr´o, L., and Rigau, G. (2000). Mapping word-
nets using structural information. In Proceedings of the
38th Annual Meeting on Association for Computational
Linguistics, ACL ’00, pages 504–511, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Francis, W. N. and Kuˇcera, H. (1964). A standard corpus
of present-day edited american english, for use with dig-
ital computers (brown). Technical report, Brown Univer-
sity, Providence, Rhode Island.

Gelbukh, A., Sidorov, G., and Han, S. Y. (2003). Evolu-
tionary approach to natural language wsd through global
coherence optimization. WSEAS Transactions on Com-
munications, 2(1):11–19.

Habert, B., Fabre, C., and Issac, F. (1998). DE L’ECRIT
AU NUMERIQUE. Constituer, normaliser et exploiter
les corpus ´electroniques. Number ISBN : 2-225-82953-
5. ELSEVIER MASSON.

Ide, N. and Macleod, C. (2001). The american national
corpus: A standardized resource of american english. In
Proceedings of Corpus Linguistics 2001, volume 3.

Lesk, M. (1986). Automatic sense disambiguation using
mrd: how to tell a pine cone from an ice cream cone. In
Proceedings of SIGDOC ’86, pages 24–26, New York,
NY, USA. ACM.

Mann, H. B. and Whitney, D. R. (1947). On a Test of
Whether one of Two Random Variables is Stochasti-
cally Larger than the Other. The Annals of Mathematical
Statistics, 18(1):50–60.

Miller, G. A., Leacock, C., Tengi, R., and Bunker, R. T.
(1993). A semantic concordance. In Proceedings of the
workshop on Human Language Technology, HLT ’93,
pages 303–308, Stroudsburg, PA, USA. Association for
Computational Linguistics.

ceedings of COLING 2012, pages 1781–1796, Mumbai,
India, December. The COLING 2012 Organizing Com-
mittee.

Miller, G. A. (1995). Wordnet: A lexical database. ACM,

Vol. 38(No. 11):p. 1–41.

Moro, A. and Navigli, R. (2015). Semeval-2015 task 13:
Multilingual all-words sense disambiguation and entity
linking. In Proceedings of the 9th International Work-
shop on Semantic Evaluation (SemEval 2015), pages
288–297, Denver, Colorado, June. Association for Com-
putational Linguistics.

M`arquez, L., Raya, J., Carroll, J., McCarthy, D., Agirre, E.,
Mart´ınez, D., Strapparava, C., and Gliozzo, A. (2002).
Experiment a : Several all-words wsd systems for en-
glish. Technical report, Meaning, Developing multilin-
gual Web-scale Language Technologies.

Navigli, R. and Ponzetto, S. P. (2010). Babelnet: Build-
ing a very large multilingual semantic network. In Pro-
ceedings of the 48th annual meeting of the association
for computational linguistics, pages 216–225. Associa-
tion for Computational Linguistics.

Navigli, R., Litkowski, K. C., and Hargraves, O. (2007).
Semeval-2007 task 07: Coarse-grained english all-words
task. In SemEval-2007, pages 30–35, Prague, Czech Re-
public, June.

Navigli, R. (2012). A quick tour of word sense disam-
biguation, induction and related approaches. In Proceed-
ings of the 38th Conference on Current Trends in The-
ory and Practice of Computer Science (SOFSEM), pages
115–129.

Raganato, A., Camacho-Collados, J., and Navigli, R.
(2017). Word sense disambiguation: A uniﬁed evalua-
tion framework and empirical comparison. In Proceed-
ings of the 15th Conference of the European Chapter of
the Association for Computational Linguistics: Volume
1, Long Papers, pages 99–110, Valencia, Spain, April.
Association for Computational Linguistics.

Schwab, D., Goulian, J., Tchechmedjiev, A., and Blan-
chon, H. (2012). Ant Colony Algorithm for the Unsu-
pervised Word Sense Disambiguation of Texts: Compar-
ison and Evaluation. In Proceedings of the 25th Interna-
tional Conference on Computational Linguistics (COL-
ING 2012), Mumbai (India), dec.

Schwab, D., Goulian, J., and Tchechmedjiev, A. (2013).
D´esambigu¨ısation lexicale de textes : efﬁcacit´e qualita-
tive et temporelle d’un algorithme `a colonies de fourmis.
TAL, 54(1):99–138.

Shapiro, S. S. and Wilk, M. B. (1965). An analysis of vari-
ance test for normality (complete samples). Biometrika,
3(52).

Taghipour, K. and Ng, H. T. (2015). One million sense-
tagged instances for word sense disambiguation and in-
duction. In Proceedings of the Nineteenth Conference on
Computational Natural Language Learning, pages 338–
344, Beijing, China, July. Association for Computational
Linguistics.

Miller, T., Biemann, C., Zesch, T., and Gurevych, I. (2012).
Using distributional similarity for lexical expansion in
knowledge-based word sense disambiguation. In Pro-

Toutanova, K., Klein, D., Manning, C. D., and Singer, Y.
(2003). Feature-rich part-of-speech tagging with a cyclic
dependency network. In Proceedings of the 2003 Con-

1033

ference of the North American Chapter of the Associa-
tion for Computational Linguistics on Human Language
Technology - Volume 1, NAACL ’03, pages 173–180,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

Vial, L., Tchechmedjiev, A., and Schwab, D. (2016). Ex-
tension lexicale de d´eﬁnitions grˆace `a des corpus annot´es
en sens. In Traitement Automatique des Langues Na-
turelles (TALN).

Vial, L., Lecouteux, B., and Schwab, D.

(2017). Uni-
formisation de corpus anglais annot´es en sens. In 24`eme
Conf´erence sur le Traitement Automatique des Langues
Naturelles, Orl´eans, France, June.

Wilcoxon, F. (1945). Individual Comparisons by Ranking
Methods. Biometrics Bulletin, 1(6):80–83, December.
Yang, X.-S. and Deb, S. (2009). Cuckoo search via l´evy
ﬂights. Proc. of World Congress on Nature and Biologi-
cally Inspired Computing, pages 210–214.

Yuan, D., Richardson, J., Doherty, R., Evans, C., and Al-
tendorf, E. (2016). Semi-supervised word sense disam-
biguation with neural models. In COLING 2016.

8. Language Resource References
Hovy et al. (2006). OntoNotes: The 90% Solution.
Ide et al. (2008). MASC: the Manually Annotated Sub-

Corpus of American English.

Miller et al. (1993). A Semantic Concordance.
Miller. (1995). Wordnet: A Lexical Database.
Ng and Lee. (1997). DSO Corpus of Sense-Tagged En-

glish.

Taghipour and Ng. (2015). One Million Sense-Tagged In-
stances for Word Sense Disambiguation and Induction.

1034

UFSAC: Uniﬁcation of Sense Annotated Corpora and Tools

Lo¨ıc Vial, Benjamin Lecouteux, Didier Schwab
LIG – GETALP – Univ. Grenoble Alpes – France
{loic.vial, benjamin.lecouteux, didier.schwab}@imag.fr

Abstract
In Word Sense Disambiguation, sense annotated corpora are often essential for evaluating a system and also valuable in order to reach a
good efﬁciency. Always created for a speciﬁc purpose, there are today a dozen of sense annotated English corpora, in various formats
and using different versions of WordNet. The main hypothesis of this work is that it should be possible to build a disambiguation system
by using any of these corpora during the training phase or during the testing phase regardless of their original purpose. In this article, we
present UFSAC: a format of corpus that can be used for either training or testing a disambiguation system, and the process we followed
for constructing this format. We give to the community the whole set of sense annotated English corpora that we know, in this uniﬁed
format, when the copyright allows it, with sense keys converted to the last version of WordNet. We also provide the source code for
building these corpora from their original data, and a complete Java API for manipulating corpora in this format. The whole resource is
available at the following URL: https://github.com/getalp/UFSAC.

Keywords: Word Sense Disambiguation, sense annotated corpora, uniﬁed resource, tools

1.

Introduction

Whether they are used for the evaluation or for the learn-
ing process of a Word Sense Disambiguation (WSD) sys-
tem, the importance of sense annotated corpora in Natural
Language Processing (NLP) is considerable. On one hand,
the evaluation in vivo, i.e.
the evaluation of a WSD sys-
tem as part of a larger task, has never been really exploited.
On the other hand, the evaluation in vitro, which uses di-
rectly sense annotated corpora by comparing the output of
a system to manual annotations, is predominant. Moreover,
WSD systems exploiting examples from sense annotated
corpora are generally far better than those which do not
(Navigli et al., 2007; Moro and Navigli, 2015).
At the time of its creation, WordNet (Miller, 1995) was un-
doubtedly the only lexical database freely available for En-
glish. Since the beginning of the 2000s, it has become the
de facto standard for WSD in this language. Indeed, most of
sense annotated corpora are either directly annotated with
WordNet sense keys or they are annotated with a sense in-
ventory linked to the senses of WordNet, such as BabelNet
(Navigli and Ponzetto, 2010).
However, it is not trivial to use these corpora, because most
of them differ in their format and on the version of Word-
Net they use. As a consequence, very few works in the lit-
erature of WSD are trained or evaluated on more than two
annotated corpora.
Also, WSD systems are systematically evaluated on cor-
pora that have been initially created for the purpose of eval-
uation, and never on corpora that have been created for an-
other purpose, such as training or for sense distribution es-
timation, whereas there is no scientiﬁc reason for that.
This paper presents a work of uniﬁcation of all existing
English corpora annotated with any version of WordNet to
our knowledge, in a unique format, easy to understand, and
easy to work with in practice. We put on the same level the
corpora originally created for the evaluation and those for
the learning, so to facilitate the creation of robust WSD sys-
tems which could for example be evaluated in a way where
all corpora except one are used for the learning, and the

remaining one is used for the evaluation, then switch the
corpora and do this for every existing corpus.
The language resource that we provide contains all En-
glish sense annotated corpora in UFSAC (Uniﬁed Format
for Sense Annotated Corpora), the format that we pro-
pose, with sense annotations converted to the last version
of WordNet (3.0), along with Java code to easily read, write
and modify any corpus in this format, and scripts for con-
verting a corpus from its original format to UFSAC.
the demonstration of
Our work is the continuity of
(Vial et al., 2017), and it differs from the recent work of
(Raganato et al., 2017) in several points. Their work is fo-
cused on the evaluation of WSD systems, whereas we pro-
vide a complete API for manipulating corpora in a new uni-
ﬁed format (UFSAC), and conversion scripts allowing the
full reconstruction of the corpora from the original data. We
also propose ﬁve additional corpora in our resource among
the most difﬁcult to parse.
In our resource, we provide a script for converting a cor-
pus from our format to theirs, so existing WSD systems
that rely on their format can be trained or evaluated on any
of the corpus that we produced. We also provide a script
for converting their format to ours in order to facilitate any
collaborative work in the community.

2. Sense Annotated Corpora: rare and

costly resources

Generally speaking, a corpus is a collection of documents
which can be used as samples of text for a particular lan-
guage (Habert et al., 1998). A corpus may contain several
millions of words, which can be lemmatized and annotated
with information concerning their part of speech for exam-
ple. Among these corpora, we can ﬁnd the British National
Corpus (Burnard, 1998) (100 million words) and the Amer-
ican National Corpus (Ide and Macleod, 2001) (20 million
words). The texts come from several sources such as news-
papers, books, encyclopedias or from the Web.
A sense annotated corpus is a corpus in which some or
all words are annotated with an identiﬁer of sense from

1027

a speciﬁc lexical database. For example, all words in
the corpus of the 7th task of the SemEval 2007 seman-
tic evaluation campaign (Navigli et al., 2007) are anno-
tated with sense identiﬁers from WordNet 2.1, whereas
in the English corpus of the 13th task of SemEval 2015
(Moro and Navigli, 2015), all words are annotated with
sense identiﬁers from WordNet 3.0, BabelNet 2.5 and
Wikipedia pages.
There are at least three reasons to create a sense annotated
corpus:

It

• Estimate the distribution of senses in the lan-
guage.
the SemCor
is for this purpose that
(Miller et al., 1993) was annotated. Consequently, the
senses in WordNet are, since version 1.7, sorted by
this distribution of senses estimated on the SemCor.

• Build

a Word Sense Disambiguation

system
from examples contained in the
the OMSTI
instance,
this
for

which learns
annotated corpus.
(Taghipour and Ng, 2015) was
purpose.

created

For

• Evaluate a WSD system by comparing its output to
the annotations in the corpus, as it is the case for in-
stance with corpora created as part of the evaluation
campaigns SensEval-SemEval.

After their distribution, there is no scientiﬁc reason not
to use indistinctly these corpora either for building a
WSD system, for estimating the distribution of senses or
Indeed, the SemCor is
for evaluating a WSD system.
used since a long time for the learning of WSD systems
(Chan et al., 2007; Navigli et al., 2007) or more recently
for the evaluation of different methods (Yuan et al., 2016).
This last usage is still very rare, since it is one of the
ﬁrst experiment that we found in the literature, along with
(M`arquez et al., 2002).
However, the format of the resources differs greatly de-
pending on their original purpose. For the SemCor, a single
ﬁle groups all the information, whereas in the case of the
evaluation corpora, there are two ﬁles: one that contains the
unannotated corpus, and the other that contains the sense
annotations. In some corpora, like in the DSO and the OM-
STI, there is one ﬁle for every lemma in the dictionary, and
each ﬁle contains thousands of example sentences, where
this lemma is the only word that is sense annotated.
Few data are manually sense annotated. The Global Word-
Net Association made a list of 26 corpora annotated with
WordNet 1. These corpora concern 17 languages, but only
three of them reach 100,000 annotations. English, with
more than 2 million words sense annotated ranks ﬁrst, be-
fore Dutch with nearly 300,000 annotations and Bulgarian
with 100,000 annotations. Thus, it is unsurprising that most
of researches in WSD focus on English.

3. A single format for sense annotated

corpora

The main purpose of this work is to help the construction
and the evaluation of WSD systems, by giving to the com-

munity the set of all existing English sense annotated cor-
pora to our knowledge, in the same format, using the same
sense inventory, and tools to easily parse them, manipulate
them, and convert corpora from their original format to our
one.
Indeed, a large quantity of sense annotated data is vi-
tal for the construction of WSD systems.
In evaluation
campaigns, this often makes the difference. For exam-
ple,
looking at the data from the SemEval 2007 cam-
paign (Navigli et al., 2007), which most of the recent sys-
tems were evaluated on, we observe that systems that
did not use sense annotated data obtain a precision score
up to 78 − 79%2 (Schwab et al., 2013) (Chen et al., 2014)
whereas those which use a lot of annotated data reach
a score up to 82% (Chan et al., 2007) (Navigli, 2012)
(Vial et al., 2016) and even 84% (Yuan et al., 2016).
Therefore, having all existing corpora in a unique format
and using the same sense inventory offers several advan-
tages: it allows to easily expand the quantity of data avail-
able for improving WSD systems, it allows to better esti-
mate the distribution of senses in English, and ﬁnally, this
format can help creating more robust WSD systems. In-
deed, we still ﬁnd a lot of works that focus on a single evalu-
ation task (Vial et al., 2016; Chen et al., 2014), and in these
cases, the analysis of the results concerning the robustness
of the methods is limited. The uniﬁcation of the format of
sense annotated corpora could improve the evaluation pro-
cess by facilitating a cross validation process for instance,
where the system is evaluated sequentially on every corpus,
with all others used for the training.

4. Provided resource
Our work consists in gathering all English corpora sense
annotated with WordNet, and convert all of them to a
uniﬁed format that is able to contain all the informations
present in the original format. We created format conver-
sion scripts for this purpose, as well as scripts for clean-
ing the corpora, and converting the sense annotation to the
last version of WordNet (3.0). The resulting corpora are
parts of the resource when the copyright allows it, along
with the format conversion scripts, the cleaning scripts,
and the sense conversion scripts. For the corpora that
we cannot distribute because of the licence, anyone that
possess them can still run our scripts to turn the origi-
nal resource into our format. Finally, an API is provided
for parsing, creating and manipulating corpora in our for-
mat. The resource is accessible at the following URL:
https://github.com/getalp/UFSAC.

4.1. Sense annotated corpora
Our resource contains the following corpora:

• The SemCor (Miller et al., 1993), a subset of the
Brown Corpus (Francis and Kuˇcera, 1964). Original
annotations are done with WordNet 1.6.

• The DSO (Defence Science Organisation) (Ng and
Lee, 1997), a non-free corpus, that is focused on 121

1http://globalwordnet.org/wordnet-annotated-corpora/

the human annotators in 78 to 79% of cases

2This means that the system has chosen the same sense than

1028

nouns and 70 verbs among the most frequently used
and the most ambiguous words in English and have
been annotated in various contexts with WordNet 1.5.

• The WordNet Gloss Tag 3, a corpus which consists of
all deﬁnitions of WordNet (Miller, 1995) with every
words sense annotated since version 3.0.

• The OMSTI (One Million Sense-Tagged Instances)
(Taghipour and Ng, 2015), a corpus of approximately
one million words sense annotated with WordNet 3.0.

• The MASC (Manually Annotated Sub-Corpus) (Ide et
al., 2008), we used the version given in the article of
(Yuan et al., 2016), annotated with the NOAD (New
Oxford American Dictionary), but with corresponding
WordNet 3.0 sense keys.

• The Ontonotes 5.0 (Hovy et al., 2006), annotated with

WordNet 3.0.

• The corpora of

the WSD evaluation campaigns
SemEval-SensEval: SensEval 2 (using WordNet 1.7),
SensEval 3 (WN 1.7.1), SemEval 2007 (WN 2.1), Se-
mEval 2013 (WN 3.0) and SemEval 2015 (WN 3.0).

Table 1 summarizes statistics concerning these corpora.
After the conversion of all these corpora into our format,
we executed four post-processing steps: sense annotation
conversion, identical sentences merging, lemma and POS
tagging, and ﬁnally a cleaning step.

4.1.1. Sense Annotation Conversion
Sense annotations have been converted, when necessary,
from their original WordNet sense key to the last ver-
sion of WordNet (3.0) thanks to conversion tables from
(Daud´e et al., 2000).
However, because some senses have been dropped from the
old versions of WordNet, some sense annotations have not
been converted. In any case, the original sense annotations
are always kept alongside the converted sense annotation.
When a sense is mapped to two or more senses with equal
probability, all resulting senses are added to the word anno-
tations, separated by a semicolon.

Identical sentences merging

4.1.2.
This step is only applied on the DSO and the OMSTI: be-
cause these corpora are constructed such that they contain
lists of sentences with only one word that is sense anno-
tated, surrounded by words not annotated, some sentences
are present in different places across the corpus, but with
different words that are sense annotated.
The merging phase identiﬁes identical sentences with an-
notations on different words, and creates a single sentence
containing all annotations. Thus, this steps adds a cru-
cial information for some WSD systems. For instance,
a similarity-based WSD system can now “learn” that two
word senses are often located in the same sentence.

3http://wordnet.princeton.edu/glosstag.shtml

4.1.3. Lemma and POS tagging
For the corpora that do not already contain these informa-
tions, we added the lemma for every word, when existing,
using the WordNet’s morphy tool, and the part-of-speech
tag from the Penn Treebank tag set using Stanford’s Log-
linear POS tagger (Toutanova et al., 2003).

4.1.4. Cleaning
Finally, this last step consists of trimming words, removing
invisible characters and removing inconsistent annotations,
for instance when the part of speech annotation differs from
the part of speech of the sense annotation.

4.2. UFSAC File format

Our approach for the uniﬁcation of the different annotated
corpora begins with a ﬁle format that is descriptive, easily
understandable and readable by a human, and at the same
time, efﬁcient for a program to parse and create. Finally, it
should be able to contain all the information contained in
the original resources. These informations are represented
with the following concepts:
– A Lexical Entity (LE) is an entity that contains a set of
annotations.
– A Corpus is a LE which contains a set of documents.
– A Document is a LE hich contains a set of paragraphs.
– A Paragraph is a LE which contains a set of sentences.
– A Sentence is a LE which contains a set of words.
– A Word is a LE which has a special mandatory annotation
“surface form”, which is the value of the word.

In order to represent these concepts, UFSAC is based on a
simple XML syntax with some conventions: lexical enti-
ties are represented by XML nodes (corpus, document,
paragraph, sentence and word), and annotations are
node attributes.

The annotations also follow a certain convention, we used
the following to annotate words:
– The identiﬁer (id) of a lexical entity, particularly use-
ful for corpora originally created for the evaluation (e.g.
“d001.s002.t003”).
– The surface form (surface form) of a word.
– The lemma (lemma) of a word.
– The part of speech (pos) of a word.
– The sense of a word,
in a speciﬁc lexical database,
for example WordNet 3.0 (wn30 key), WordNet 1.7.1
(wn171 key)... If multiple senses are speciﬁed (it is the
case in the coarse-grained task of SemEval 2007 for in-
stance), they are separated with a semicolon (;).
The information of the sense is the one which is the most
useful in our case, and it is speciﬁc to each lexical database,
instead of having a unique “sense” annotation as we can
ﬁnd in most other formats. That way we allow multiples
sense annotations from different lexical databases at the
same time. For example, the DSO is originally annotated
with senses from WordNet 1.5, and the conversion to Word-
Net 3.0 is sometimes impossible for some senses which
were deleted between the two versions. This convention
allows us to keep the original annotations, yet to have the
annotations from the last version of WordNet, or any other
lexical database (for instance BabelNet) at the same time.

1029

Corpus

Sentences

Words

Annotated parts of speech

Total

Annotated Nouns

Verbs

Adj.

Adv.

SemCor
DSO
WordNet GlossTag
MASC
OMSTI
Ontonotes
SemEval 2007 task 07
SemEval 2007 task 17
SemEval 2013 task 12
SemEval 2015 task 13
Senseval 2
Senseval 3 task 1

37176
178119
117659
34217
820557
21938
245
120
306
138
238
300

778587
5317184
1634691
596333
35843024
435340
5637
3395
8142
2638
5589
5511

229517
176915
496776
114950
920794
52263
2261
455
1644
1053
2301
1957

87581
105925
232319
49263
476944
9220
1108
159
1644
554
1061
886

89037
70990
62211
40325
253644
43042
591
296
0
251
541
723

33751
0
84233
25016
190206
0
356
0
0
166
422
336

19148
0
19445
0
0
0
206
0
0
82
277
12

Table 1: Statistics related to our set of annotated corpora, after the conversion and cleaning phase.

The following is an example of the resulting UFSAC XML:

ing/modifying them.

<corpus id="short_example">

<document id="d001" >

<paragraph>
<sentence>

<word surface_form="A" pos="DT" />
<word surface_form="precise"

wn30_key="precise%3:00:00::" />

<word surface_form="example"

pos="NN" lemma="example" />

<word surface_form="." />

</sentence>
</paragraph>

</document>

</corpus>

Our format thus allows to integrate the whole corpus in a
single ﬁle, and it is easily readable, especially comparing
to most original formats (c.f. the end of section 2.).

4.3. API and tools
An easy-to-use Java API is also provided to read, write
and modify efﬁciently corpora in our format. It allows two
styles of programming: you can either load a full corpus in
memory, perform all your calculations and save it entirely
in a ﬁle; or you can sequentially scan, edit or print a corpus
from a ﬁle, in a streaming manner. The latter is particularly
useful when working with huge ﬁles which do not ﬁt into
memory. Finally, we offer a set of scripts that perform the
conversion of a corpus from its original format to our one,
and some pre-processing and analyses scripts.

4.3.1. Core API
The core API is a package containing the base classes
for manipulating corpora. For simplicity, the class names
match exactly what is described in section 4.2..

The class Annotation describes an annotation on a lex-
ical entity. Concretely, it is a pair of Strings (name/value)
and a pointer to the annotated lexical entity.

The class LexicalEntity describes something that has
zero or more annotations, with public methods for access-

The class Word inherits from LexicalEntity, has a special
mandatory annotation surface form, which is the value
of the word, and a parent sentence.

The class Sentence inherits from LexicalEntity, contains
a list of words and a parent paragraph.

The class Paragraph inherits from LexicalEntity, con-
tains a list of sentences and a parent document.

The class Document inherits from LexicalEntity, contains
a list of paragraphs and a parent corpus.

Finally, the class Corpus inherits from LexicalEntity and
contains a list of documents.

few classes,

functions
These
Corpus.saveToXML and Corpus.loadFromXML
allow to create, save, load and modify any corpus easily.

coupled with

two

4.3.2. Streaming API
For some corpora particularly huge, like the OMSTI, we
also provide a sub-package streaming, which allows to
read, write or modify a corpus sequentially, without be-
ing fully loaded into memory. This is similar to the Java
SAX library (Simple API for XML), events are ﬁred when
reading a word, sentence, paragraph, etc., and the user can
choose to respond to this event or not.
In practice, we provide a set of classes which cover most
use cases.

class
respond

StreamingCorpusReader
to

allows
The
events readBeginCorpus,
to
the
readBeginDocument, readWord, etc..
This can
be useful for printing every word that is sense annotated
for example.

The class StreamingCorpusModifier allows to
modify a corpus in-place. This is specially useful for pre-
processing, for instance convert every word to lowercase.

class
creating

The
for
writeBeginSentence, writeWord and so on.

StreamingCorpusWriter
new corpus, with

used
its methods

is

a

1030

4.3.3. Scripts
Finally, we provide a set of examples and useful scripts
which use our format and our API. The scripts are Java
classes with a main method and are not part of any pack-
age.

The script ConvertOriginalCorpora allows to con-
vert all corpora listed in subsection 4.1. from their original
format to the UFSAC format. This is specially valuable for
non-free corpus like the DSO, that we cannot share directly
in our format, but that one can still buy in their original for-
mat, and then convert to our format. This script includes all
post-processing steps described in subsection 4.1..

scripts

ConvertFromRaganato

and
The
ConvertToRaganato allow to convert a corpus
from the format described by (Raganato et al., 2017) to
UFSAC, and vice-versa.

The script ComputeMostFrequentSenses will calcu-
late, for every lemma in WordNet, the most frequent sense
(MFS), based on all usages in the given UFSAC corpora.
This is helpful since in most evaluation campaigns, the
the score obtained when the MFS is
MFS baseline (i.e.
assigned to every word) is important, and it is generally
implicitly the sense distribution computed on the SemCor
only.

The scripts AddCorpusLemma and AddCorpusPOS use
respectively WordNet’s morphy and Stanford’s POS tag-
ger to annotate a corpus with the lemma and POS of every
word.

The script EvaluateWSD compare the sense annotations
produced by a WSD system to the gold standard annotation,
and compute the usual Precision, Recall, Coverage and F1
metrics for every given corpus.

The script GenerateCorpusStatistics is the one
that was used to produce the table 1.

5. Experiments
In this section, we show an example of using all UFSAC
corpora for the extension of a knowledge-based WSD sys-
tem based on the Lesk measure. This experiment shows
how this ressource can be used to easily improve an exist-
ing WSD system.

5.1. The Lesk and Extended Lesk Similarity

Measures

(Lesk, 1986) proposed a simple algorithm for lexical dis-
ambiguation that evaluates the similarity between two
senses (s1, s2) as the number of words in common in the
deﬁnitions of the senses from a dictionary (D(s1),D(s2)).
The Lesk measure compute an exact lexical match of the
surface forms of the words in the deﬁnitions. If important
words are missing or different synonyms of the same words
are used in the deﬁnition of related senses, the overlap mea-
sure will not capture the proximity of their meanings appro-
priately. As deﬁnitions (especially in Princeton Wordnet)
are very concise, it is difﬁcult to obtain ﬁne grained dis-
tinctions between senses.
In consequence, several variants of the Lesk measure tried
to alleviate this problem, for instance (Baldwin et al., 2010)

and (Miller et al., 2012), but the most common expan-
sion technique is the so-called “extended/adapted Lesk”
(Banerjee and Pedersen, 2002). The sense overlap is here
expanded with the overlap of all deﬁnitions from all pairs
of related senses in a lexico-semantic resource with a rich
structure, such as WordNet.
In our experiment, we will create another expansion of the
Lesk measure, based on UFSAC sense annotated corpora.

5.2. Expansion of Deﬁnitions Through UFSAC

Sense-Annotated Corpora

Our method consists in expanding deﬁnitions with all
neighbours of a target sense, taken from sense-annotated
corpora. We consider that a neighbour is a word found in
the same sentence as the target sense. More precisely, we
proceed as following:

1. We parse every UFSAC corpus, sentence by sentence.

2. For every word which is sense-annotated in a sentence,
we add to the deﬁnition of this sense in the dictionary
every other word present in the sentence.

That is, for every sentence S = w0, w1, . . . , wn, and for
every word wk inside S, we add to the deﬁnition of the
tagged sense of wk, i.e. D(s(wk)), every other words of
the sentence, i.e. wi∀i ∈ [0, n]i 6= k.
As a consequence, every sense’s deﬁnition in the dictionary
will be extended with words that are related to this sense,
in the same manner than (Banerjee and Pedersen, 2002)’s
extended Lesk, but with words taken from sense annotated
corpora.

5.3. Similarity-Based Word Sense

Disambiguation

Now for evaluating this new expansion to the Lesk mea-
sure, we must use a similarity-based WSD algorithm
that belongs to the broader category of knowledge-based
approaches (using dictionaries,
lexical base, encyclope-
dias. . . ). In such systems, the disambiguation process con-
sists of two layers: a local algorithm and a global algorithm.
The local algorithm computes the proximity of two word
senses, namely a semantic relatedness measure. The lo-
cal similarity measurement is then used to ﬁnd an optimal
global sense assignment for all the content words of the text
by the global algorithm. The local algorithm is here the
Lesk measure augmented with the sense annotated corpora.
We also ﬁltered out stopwords according to the “long” list
given in https://www.ranks.nl/stopwords.
As for global algorithms,
they are often probabilistic
combinatorial optimization algorithms, as WSD is funda-
mentally a discrete combinatorial optimization problem.
Many such algorithms have been adapted to WSD, in-
cluding genetic algorithms (Gelbukh et al., 2003), simu-
lated annealing (Cowie et al., 1992), ant colony algorithms
(Schwab et al., 2012) or more recently bee hive algorithms
(Abualhaija and Zimmermann, 2016).
The different global algorithms mainly differ in the con-
vergence speed to a close-to-optimal solution, however the
bottleneck to the accuracy of the algorithm is the local al-
gorithm (similarity measure) used, as it encodes the knowl-

1031

System

Lesk + UFSAC corpora

Lesk
Extended Lesk (Banerjee and Pedersen, 2003)
Most Frequent Sense Baseline

79.83%

68.70%
78.01%
78.90%

66.43%

50.65%
61.42%
67.10%

SemEval 2007 Task 07

SemEval 2015 Task 13

Table 2: F1 scores of our similarity-based system augmented with words taken from all UFSAC corpora (except the eval-
uation corpora) on SemEval 2007 coarse-grained all-words task and SemEval 2015 ﬁne-grained all-words task, compared
to the Lesk, Extended Lesk and MFS baselines.

edge from the resource that allows to discriminate between
the senses.
In this experiments, we use an adaptation to WSD of the
Cuckoo Search Algorithm, the state of the art in combina-
torial search algorithms (Yang and Deb, 2009). The algo-
rithm relies on the L´evy ﬂight distribution for an effective
(and more meaningful) sampling of the search space.
The Cuckoo Search Algorithm is probabilistic and its re-
sult differs slightly from an execution to another (by an
order of magnitude of less than 1%). So for each experi-
ment, 30 executions are performed. Then, using a Shapiro-
Wilk test (Shapiro and Wilk, 1965), we determined that
none of the result distribution follow a normal distribution.
Thus, we used a non-parametric Wilcoxon/Mann-Whitney-
U (Wilcoxon, 1945) (Mann and Whitney, 1947) test in or-
der to check the pairwise signiﬁcance (p < 0.01) of all
pairs of result distributions.

5.4. Results
We evaluate the performance of our expansion of deﬁni-
tions using all UFSAC corpora listed in subsection 4.1. ex-
cept the ones we evaluated our system on: SemEval 2007
task 7 and SemEval 2015 task 13. We compare our simi-
larity measure to the original Lesk and the Extended Lesk
(Banerjee and Pedersen, 2003) measures. The results are
presented in Table 2.
As we can see, our expansion of the deﬁnitions with words
taken from the UFSAC corpora improves considerably the
original Lesk measure, even more than the Extended Lesk
measure. Therefore, this experiment demonstrates how
much the addition of the UFSAC resource can improve a
similarity-based WSD sytem. Of course, every other kind
of WSD system can be improved, in particular supervised
systems which rely solely on sense-annotated corpora and
machine learning techniques (SVM, neural networks, etc.).

6. Conclusion
In this paper we advocate for a more uniform way of dis-
tributing sense annotated corpora, through a unique and un-
complicated ﬁle format. This uniﬁcation can facilitate both
the creation and the evaluation of Word Sense Disambigua-
tion systems. Indeed, sense annotated corpora are histor-
ically separated between those created for the purpose of
training, and those created for the purpose of evaluation.
In addition, the formats of these corpora are often very
different from each other: different ﬁle hierarchy, differ-
ent syntax, and different sense inventory are used. Conse-
quently, most WSD systems are trained and evaluated on

few corpora comparing to the amount of existing corpora.
Moreover, they are systematically evaluated only on cor-
pora originally created for the purpose of evaluation, and
trained only on corpora originally created for the purpose
of training, whereas they could beneﬁt from considering all
of them in both tasks.
The uniﬁcation of all sense annotated corpora hence allows
to quickly expand a system which is trained on some re-
sources to new data without the effort of writing another
parser. Also, a system can now easily include to its training
phase some corpora that were originally created for evalu-
ation, and/or evaluate its performance on parts of corpora
originally created for training. This easily allows a much
better coverage and a more ﬁne-grained analysis of a WSD
system performance.
In our language resource, we gathered all existing English
sense annotated corpora that we know, and we converted
them in a simple and consistent XML ﬁle format that we
named UFSAC. We also converted their sense annotations
to the last version of WordNet (3.0). The corpora are only
available when the licence authorizes it, but we also pro-
vide scripts that can easily convert a corpus from its origi-
nal format to the one we propose. Thus, anyone who pos-
sess the corpora that we cannot distribute can still bene-
ﬁt from this work.
In addition, we provide a complete
Java API for reading, writing and modifying corpora in
our uniﬁed format, along with example codes and tools
for many applications such as lemmatization, POS-tagging,
sense distribution estimation, etc. Finally, a demonstra-
tion of a simple use of all UFSAC corpora for extend-
ing a similarity-based WSD system is shown in section 5..
In the future, we plan to add to our resource other cor-
pora such as the corpora created for the lexical sample
tasks of SensEval/SemEval, and sense annotated corpora
in other languages. We also plan to improve the UFSAC
format by adding a better support for multiword expres-
sions. The resource will be continuously updated at this
url: https://github.com/getalp/UFSAC.

7. Bibliographical References

Abualhaija, S. and Zimmermann, K.-H. (2016). D-bees:
A novel method inspired by bee colony optimization for
solving word sense disambiguation. Swarm and Evolu-
tionary Computation, pages –.

Baldwin, T., Kim, S., Bond, F., Fujita, S., Martinez, D., and
Tanaka, T. (2010). A reexamination of mrd-based word
sense disambiguation. 9(1):4:1–4:21, March.

1032

Banerjee, S. and Pedersen, T. (2002). An adapted lesk al-
gorithm for word sense disambiguation using wordnet.
In CICLing 2002, Mexico City, February.

Banerjee, S. and Pedersen, T. (2003). Extended gloss over-
laps as a measure of semantic relatedness. In In Proceed-
ings of the Eighteenth International Joint Conference on
Artiﬁcial Intelligence, pages 805–810.

Burnard, L. (1998). The British National Corpus.
Chan, Y. S., Ng, H. T., and Zhong, Z. (2007). Nus-pt: Ex-
ploiting parallel texts for word sense disambiguation in
the english all-words tasks. In Proceedings of the 4th
International Workshop on Semantic Evaluations, Se-
mEval ’07, pages 253–256, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.

Chen, X., Liu, Z., and Sun, M. (2014). A uniﬁed model for
word sense representation and disambiguation. In Pro-
ceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages 1025–
1035, Doha, Qatar, October. Association for Computa-
tional Linguistics.

Cowie, J., Guthrie, J., and Guthrie, L.

(1992). Lexical
disambiguation using simulated annealing. In COLING
1992, volume 1, pages 359–365, Nantes, France, aoˆut.
Daud´e, J., Padr´o, L., and Rigau, G. (2000). Mapping word-
nets using structural information. In Proceedings of the
38th Annual Meeting on Association for Computational
Linguistics, ACL ’00, pages 504–511, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Francis, W. N. and Kuˇcera, H. (1964). A standard corpus
of present-day edited american english, for use with dig-
ital computers (brown). Technical report, Brown Univer-
sity, Providence, Rhode Island.

Gelbukh, A., Sidorov, G., and Han, S. Y. (2003). Evolu-
tionary approach to natural language wsd through global
coherence optimization. WSEAS Transactions on Com-
munications, 2(1):11–19.

Habert, B., Fabre, C., and Issac, F. (1998). DE L’ECRIT
AU NUMERIQUE. Constituer, normaliser et exploiter
les corpus ´electroniques. Number ISBN : 2-225-82953-
5. ELSEVIER MASSON.

Ide, N. and Macleod, C. (2001). The american national
corpus: A standardized resource of american english. In
Proceedings of Corpus Linguistics 2001, volume 3.

Lesk, M. (1986). Automatic sense disambiguation using
mrd: how to tell a pine cone from an ice cream cone. In
Proceedings of SIGDOC ’86, pages 24–26, New York,
NY, USA. ACM.

Mann, H. B. and Whitney, D. R. (1947). On a Test of
Whether one of Two Random Variables is Stochasti-
cally Larger than the Other. The Annals of Mathematical
Statistics, 18(1):50–60.

Miller, G. A., Leacock, C., Tengi, R., and Bunker, R. T.
(1993). A semantic concordance. In Proceedings of the
workshop on Human Language Technology, HLT ’93,
pages 303–308, Stroudsburg, PA, USA. Association for
Computational Linguistics.

ceedings of COLING 2012, pages 1781–1796, Mumbai,
India, December. The COLING 2012 Organizing Com-
mittee.

Miller, G. A. (1995). Wordnet: A lexical database. ACM,

Vol. 38(No. 11):p. 1–41.

Moro, A. and Navigli, R. (2015). Semeval-2015 task 13:
Multilingual all-words sense disambiguation and entity
linking. In Proceedings of the 9th International Work-
shop on Semantic Evaluation (SemEval 2015), pages
288–297, Denver, Colorado, June. Association for Com-
putational Linguistics.

M`arquez, L., Raya, J., Carroll, J., McCarthy, D., Agirre, E.,
Mart´ınez, D., Strapparava, C., and Gliozzo, A. (2002).
Experiment a : Several all-words wsd systems for en-
glish. Technical report, Meaning, Developing multilin-
gual Web-scale Language Technologies.

Navigli, R. and Ponzetto, S. P. (2010). Babelnet: Build-
ing a very large multilingual semantic network. In Pro-
ceedings of the 48th annual meeting of the association
for computational linguistics, pages 216–225. Associa-
tion for Computational Linguistics.

Navigli, R., Litkowski, K. C., and Hargraves, O. (2007).
Semeval-2007 task 07: Coarse-grained english all-words
task. In SemEval-2007, pages 30–35, Prague, Czech Re-
public, June.

Navigli, R. (2012). A quick tour of word sense disam-
biguation, induction and related approaches. In Proceed-
ings of the 38th Conference on Current Trends in The-
ory and Practice of Computer Science (SOFSEM), pages
115–129.

Raganato, A., Camacho-Collados, J., and Navigli, R.
(2017). Word sense disambiguation: A uniﬁed evalua-
tion framework and empirical comparison. In Proceed-
ings of the 15th Conference of the European Chapter of
the Association for Computational Linguistics: Volume
1, Long Papers, pages 99–110, Valencia, Spain, April.
Association for Computational Linguistics.

Schwab, D., Goulian, J., Tchechmedjiev, A., and Blan-
chon, H. (2012). Ant Colony Algorithm for the Unsu-
pervised Word Sense Disambiguation of Texts: Compar-
ison and Evaluation. In Proceedings of the 25th Interna-
tional Conference on Computational Linguistics (COL-
ING 2012), Mumbai (India), dec.

Schwab, D., Goulian, J., and Tchechmedjiev, A. (2013).
D´esambigu¨ısation lexicale de textes : efﬁcacit´e qualita-
tive et temporelle d’un algorithme `a colonies de fourmis.
TAL, 54(1):99–138.

Shapiro, S. S. and Wilk, M. B. (1965). An analysis of vari-
ance test for normality (complete samples). Biometrika,
3(52).

Taghipour, K. and Ng, H. T. (2015). One million sense-
tagged instances for word sense disambiguation and in-
duction. In Proceedings of the Nineteenth Conference on
Computational Natural Language Learning, pages 338–
344, Beijing, China, July. Association for Computational
Linguistics.

Miller, T., Biemann, C., Zesch, T., and Gurevych, I. (2012).
Using distributional similarity for lexical expansion in
knowledge-based word sense disambiguation. In Pro-

Toutanova, K., Klein, D., Manning, C. D., and Singer, Y.
(2003). Feature-rich part-of-speech tagging with a cyclic
dependency network. In Proceedings of the 2003 Con-

1033

ference of the North American Chapter of the Associa-
tion for Computational Linguistics on Human Language
Technology - Volume 1, NAACL ’03, pages 173–180,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

Vial, L., Tchechmedjiev, A., and Schwab, D. (2016). Ex-
tension lexicale de d´eﬁnitions grˆace `a des corpus annot´es
en sens. In Traitement Automatique des Langues Na-
turelles (TALN).

Vial, L., Lecouteux, B., and Schwab, D.

(2017). Uni-
formisation de corpus anglais annot´es en sens. In 24`eme
Conf´erence sur le Traitement Automatique des Langues
Naturelles, Orl´eans, France, June.

Wilcoxon, F. (1945). Individual Comparisons by Ranking
Methods. Biometrics Bulletin, 1(6):80–83, December.
Yang, X.-S. and Deb, S. (2009). Cuckoo search via l´evy
ﬂights. Proc. of World Congress on Nature and Biologi-
cally Inspired Computing, pages 210–214.

Yuan, D., Richardson, J., Doherty, R., Evans, C., and Al-
tendorf, E. (2016). Semi-supervised word sense disam-
biguation with neural models. In COLING 2016.

8. Language Resource References
Hovy et al. (2006). OntoNotes: The 90% Solution.
Ide et al. (2008). MASC: the Manually Annotated Sub-

Corpus of American English.

Miller et al. (1993). A Semantic Concordance.
Miller. (1995). Wordnet: A Lexical Database.
Ng and Lee. (1997). DSO Corpus of Sense-Tagged En-

glish.

Taghipour and Ng. (2015). One Million Sense-Tagged In-
stances for Word Sense Disambiguation and Induction.

1034

