SoilingNet: Soiling Detection on Automotive Surround-View Cameras

Michal Uˇriˇc´aˇr1, Pavel Kˇr´ıˇzek1, Ganesh Sistu2 and Senthil Yogamani2
1Valeo R&D Prague, Czech Republic 2Valeo Visions Systems, Ireland
{michal.uricar, pavel.krizek, ganesh.sistu, senthil.yogamani}@valeo.com

9
1
0
2
 
l
u
J
 
7
1
 
 
]

V
C
.
s
c
[
 
 
2
v
2
9
4
1
0
.
5
0
9
1
:
v
i
X
r
a

Abstract— Cameras are an essential part of sensor suite in
autonomous driving. Surround-view cameras are directly ex-
posed to external environment and are vulnerable to get soiled.
Cameras have a much higher degradation in performance due
to soiling compared to other sensors. Thus it is critical to
accurately detect soiling on the cameras, particularly for higher
levels of autonomous driving. We created a new dataset having
multiple types of soiling namely opaque and transparent. It will
be released publicly as part of our WoodScape dataset [15] to
encourage further research. We demonstrate high accuracy us-
ing a Convolutional Neural Network (CNN) based architecture.
We also show that it can be combined with the existing object
detection task in a multi-task learning framework. Finally,
we make use of Generative Adversarial Networks (GANs) to
generate more images for data augmentation and show that it
works successfully similar to the style transfer.

I. INTRODUCTION

Autonomous driving systems are becoming mature by
using a variety of different sensors. Cameras continue to be
one of the key sensors as the road infrastructure is designed
for human visual sensors. The ﬁrst generation systems
primarily used a single camera and more recently more
cameras are used to get full coverage around the vehicle to
handle more complex driving scenarios [5]. Horgan et al.
[6] provides an overview of various visual perception tasks
prior to deep learning era and Sistu et al. [12] provide an
overview from a deep learning perspective.

of

fog,

vision

Quality

computer

algorithms

including rain,

snow, etc. This

degrade
signiﬁcantly in poor environmental conditions due to
is
bad weather
particularly worsened when the camera lens is exposed to
rain or it becomes frozen in winter. In addition, external
cameras are exposed to mud and dust as well. Thus it is
important to detect when camera lens is soiled so that the
system is aware the vision algorithms will degrade severely.
In previous generation systems with lesser automation like
Level 2 [8], soiling detection was done to let the human
driver know that vision algorithms are less reliable. For
higher levels of automation like Level 4, the self driving
system needs to automatically detect soiling and also correct
for it, e.g. by using a cleaning system.

Soiling of camera lens occurs in mobile phones as well
and modern mobile phones detect them and ask the user to
clean it manually. However, there is very little literature and
datasets available for soiling detection. The closest problem
to the soiling detection addressed in literature is in recent

work of Porav et al. [10], where they created a setup to
capture water dripping on camera lens and have a parallel
setup without water drops. However, in our experience for
soiling by water, rain drops splashes hard on the lens as
illustrated in Fig. 1 (c) and slow dripping does not capture
it. A related problem is the design of robust algorithms to
implicitly handle these scenarios, for example Sakaridis et
al. [11] developed a robust semantic segmentation algorithm
to handle foggy scenes. An alternate approach is to image
restoration to improve the quality of the image, for example
Dehazing approaches were proposed in [3], [1] and [9].
More recently,
the computer vision conference CVPR
2019 has organized a competition called UG2 [17] to
evaluate vision algorithms in poor visibility conditions for
autonomous driving.

The main contributions of this paper are as follows:

1) Introduction and formal deﬁnition of soiling detection

task in automotive scenarios.

2) Implementation of a high accuracy soiling detection

algorithm and experimental results.

3) Incorporation of Generative Adversarial Networks
(GAN) [4] based data augmentation to improve results.
4) Release of the ﬁrst public soiling dataset brieﬂy intro-

duced in our WoodScape dataset paper [15].

The paper is organized as follows. Section II deﬁnes
soiling detection task formally, discusses an automated clean-
ing system and design of a soiling dataset. Section III
discusses the experimental setup, proposed Convolutional
Neural Network (CNN) architecture and GAN based data
augmentation. Section IV presents the experimental results
and analysis. Finally, Section V summarizes and concludes
the paper.

II. SOILING DETECTION TASK

A. Motivation

As discussed in the introduction, soiled lens degrades
to notify the system
vision algorithm and it
is critical
and/or the driver about potential degradation impact. More
speciﬁcaly, soiling detection output is used for three aspects
of autonomous driving system discussed below.

Cleaning System: Surround view cameras are usually
directly exposed to the adverse environmental conditions.
Therefore, one cannot avoid a situation when e.g. a splash
of mud or other kind of dirt hits the camera. Another, even

Fig. 1: From left to right: a) soiled camera lens mounted to the car body; b) the image quality of the soiled camera from
the previous image; c) an example of image soiled by a heavy rain.

more common example would be a heavy rain when the
water drops frequently hit the camera lens surface. As the
functionality of visual perception degrades signiﬁcantly,
detection of soiled cameras is necessary for achieving
higher levels of automated driving [8]. Fig. 2 illustrates our
cleaning system which clears ice by spraying warm water.
Some systems additionally have an air blower to clean up
remaining water. As the cleaning system is water based,
its water tank needs to be reﬁlled periodically. Thus, it is
important to have an algorithm with low false positives to
reduce the reﬁlling.

Algorithm Degradation: Soiling detection can be used
for estimation of algorithm degradation to disable partially
or reduce conﬁdence of the output. The simple degradation
mechanism is to switch off all the algorithms if there is any
soiling on the lens. However in practice, it is important to
support partial degradation of the algorithms. For example,
the soiling detection algorithm may output severity of soiling
which can be used to reduce the conﬁdence of the visual
perception algorithms so that the outputs are still useful. It
is also useful to localize the soiling instead of image level
soiling detection because certain parts of the image may still
be clean and perception algorithms may work ﬁne. Many
classical computer vision algorithms are local operators and
thus can be disabled selectively for certain soiled regions.
However, CNNs are global operators and their behavior to
partial soiling is not studied in literature.

Fig. 2: Illustration of our camera cleaning system

Image Restoration: Image restoration is employed in

autonomous driving to improve the quality for conditions
such as motion blurring, low light and other environmental
degradation such as rain, haze, etc. Some of the restoration
techniques are de-raining [10], de-fogging [11] and de-
hazing [9]. Speciﬁcally in case of soiled lens, the restoration
techniques can be specialized for the particular soiling type
and thus soiling detection can be useful. For example, when
transparent soiling is detected, specialized restoration tech-
nique for inpainting these regions can be used by leveraging
partially visibility.

B. Formal deﬁnition

To our best knowledge, the soiling detection task was
so far only brieﬂy described in our previous paper [14].
However, in that paper, we only outlined the problem and
described the possibilities of applying GAN [4] for an
advanced data augmentation related to the soiling imagery.
In this paper, we would like to deﬁne the problem of soiling
detection/classiﬁcation and to present a solution and report
the empirical results.

We deﬁne the camera soiling detection task as a multi-
label classiﬁcation problem. Each image can be described
by a binary indicator array, where zeros, and ones cor-
respond to the absence or presence of a speciﬁc soiling
class, respectively. The soiling classes we deﬁne are C =
{opaque, transparent}. And the deﬁnition of each class is
as follows: as an “opaque” class we label such regions in the
image, where it is impossible to say what would be seen if
there was no soiling, i.e. the regions which are coloured by
nontransparent colors preventing to see what is behind; as
a “transparent” class we label regions which are apparently
blurred or deformed from the expected appearance, however
it is possible to distinguish colors from the original scenery,
i.e. it is possible to see “behind”. Note, that such deﬁnition
makes some seemingly unnatural structures possible, such
as an “opaque” water soiling. However, from the data which
we have recorded for this purpose, we can say that such
structures are quite frequently observed, which justiﬁes our
deﬁnition.

Given a single image I ∈ I, we are interested in a
classiﬁer g : I → C2, where C denotes the set of class
labels: C = {opaque, transparent}, where the labels are
supposed to be binary, specifying if the given type of soiling

A. Dataset Creation

Altogether, we have collected 76, 448 number of images,
our sampling strategy was simply to extract each 10-th frame
out of short video recordings. This is done mainly in order
to decrease the self similarity of images in the dataset. The
annotations were created manually, by clicking points of a
coarse polygonal segmentation of the soiling contained in
the image with an additional information about the qualities
of the soiling (if it is transparent or opaque). Note, that such
type of annotations allows us to use the same representation
for both whole-frame based and tile-based detection exper-
iments, since the polygonal annotation can be converted to
the requested annotation vector format straightforwardly, i.e.
the polygonal annotation (where each plygon is marked to be
either fully transparent or fully opaque) is converted to the
tile-based label1, where the presence of each soiling class is
calculated based on the percentage cover of the tile overlap
with the corresponding polygon.

For the need of training, model selection and ﬁnal ex-
perimental evaluation of the trained classiﬁers, we split the
dataset to non-overlapping training, validation, and test sets.
We used the golden standard of 60/20/20 ratio for obtaining
these splits and also we used stratiﬁed sampling approach,
to retain the underlying distributions of the classes among
the splits. This process was done independently for the
whole-frame based approach and for the tile-based approach,
respectively.

This leads to the following statistics. We have collected
45, 868 and 15, 291 number of images for training and eval-
uation purpose, respectively. The training dataset annotation
statistics are as follows: 22, 015 frames are clean (i.e. their
annotation is a vector [0, 0]), 11, 704 are containing opaque
soiling only (annotation is [1, 0]), 4, 623 contain transparent
soiling only ([0, 1]), and, ﬁnally, 7, 526 are containing both
kind of soiling classes ([1, 1]). The testing dataset annotation
statistics are as follows: 7, 339 frames are clean, 3, 902 are
containing opaque soiling only, 1, 541 contain transparent
soiling only, and ﬁnally, 2, 509 contain both kind of soiling
classes. We plan to provide a subset of 5, 000 images to the
community, to encourage the further research on this topic.

B. GAN based data-augmentation

Getting relevant data for the soiling classiﬁcation task is
a very tedious task. First problem are the suitable conditions
for increasing the probability that the soiling event might
even occur. This makes the problem hard, but still solvable,
one just needs to ﬁnd a way how to manually “soil” the
camera lens, while retaining the characteristic appearance
of the realistic scenes. Another problem is the annotation
of such imagery, which is extremely expensive and time
consuming.

Due to the above mentioned reasons, we used GAN to
alleviate the lack of relevant data. The idea is simply that
we would like to use GAN machinery for creating soiled

1Recall, that the deﬁnition of tiles size might boil down to a singular tile

covering the whole image.

Fig. 3: Soiling annotation using polygons (top) and tile level
ground truth generation derived from polygons (bottom)

(i.e. opaque or transparent) is present (in such case the value
is 1) or not (the value is 0). Then clearly a vector c1 = [0, 0]
denotes a clean image, while c2 = [1, 1] denotes an image
with both type of soiling categories present. The sought
classiﬁer g should be optimal with respect to minimization
of the error, which is measured by the number of mis-
classiﬁcations of the soiling categories. We measure the mis-
classiﬁcations by Hamming distance of the binary indicator
arrays of the ground truth manually annotated labels and the
predictions returned by the classiﬁer:

εcat(cGT, c) = ham(cGT, c)

(1)

where c denotes the predicted binary encoded category, cGT
is the corresponding ground truth label. Finally, ham(·, ·)
denotes the Hamming distance. The overall error is measured
as the average value of εcat(cGT, c) over the whole testing
set.

The above deﬁnition of image level classiﬁcation can be
generalized to tile-level for providing higher level of spatial
resolution of detection as illustrated in Fig. 3 (bottom image).
When the tile size is 1x1, it specializes to pixel-level labelling
semantic segmentation task. When the tile size is equal to
image height by image width, it becomes image classiﬁcation
task. In a classical feature extraction plus classiﬁer setting,
each tile can be independently processed but with deep
learning models global context is leveraged for output of
each tile

III. DATASET DESIGN AND PROPOSED
ARCHITECTURE

This section summarizes the main contributions of the
paper in the following three sub-sections. Firstly we created
a dataset which will be partially shared publicly as it is a
newly deﬁned task in autonomous driving. We demonstrate
successful application of GAN based data augmentation
technique. Finally, we discuss supervised CNN models for
soiling detection and demonstrate good accuracy.

Fig. 4: Illustration of soiling integrated into multi-task object detection and segmentation network.

images from the clean images and use this concept for data
augmentation. Additionally, usage of GAN has the potential
of semi-supervised and unsupervised learning, which we
want to pursue in the future work.

Because of the problem of expensive annotations, we ﬁrst
tried out the CycleGAN [16] approach. Since CycleGAN
requires just a simple categorization of data into 2 cate-
gories. The images can be totally unaligned (thanks to the
cycle-consistency loss involved in the optimization criterion),
which is yet another beneﬁt as one can pre-process his data
very easily. In our case these categories are “clean”, and
“soiled” images, respectively. Therefore, the CycleGAN’s
output of our experiments are 2 generators, one trying to
introduce the soiling into the image and the other trying to
remove it and making the image “clean”.

The main problem of CycleGAN experiment is the inabil-
ity to produce variable output. Therefore, we have pracitcally
no control over the generated image. This lead us to con-
ducting another experiment with the MUNIT [7] approach.
The Multi-modal UNsupervised Image to image Translation
(MUNIT) separates the content in the image from its style
and via this we have the possibility to control the image
appearance by changing the style provided at the image
generation step.

C. Proposed CNN architecture

Our goal is to design an efﬁcient soiling detection ar-
chitecture which can be deployed on a low power embed-
ded platform having computational processing power of 1
Tera OPS. Soiling detection is an additional module in the
autonomous driving platform which already contains CNN
based object detection and segmentation and other classical
computer vision modules like depth estimation. Thus we
propose to leverage existing CNN features in the system by
sharing the encoder for a soiling decoder in a multi-task

network illustrated in Fig. 4. We show that this will provide
higher efﬁciency in the overall system relative to having
a separate network for soiling. The details of the baseline
multi-task network are shared in our previous paper [13]. It
comprises of a simpliﬁed ResNet10 like encoder and two
decoders namely simpliﬁed YOLO V2 for object detection
and simpliﬁed FCN8 for segmentation.

In this work, we propose to add the new soiling decoder
as a third task. The output of the soiling decoder is tiled
soiling class output. The number of tiles is set at training
time. We made use of two conﬁgurations, one in which the
tile size was 64 × 64 and in the other case the output was
at image resolution. The three losses were scalarized into
one loss using a weighted average and the weights were
optimized by hyper-parameter tuning using grid search. As
an ablation study to evaluate soiling detector on its own
without multi-task network, we removed the other decoders
and evaluated the network for soiling task. Soiling decoder
has two convolution layers and a ﬁnal grid level softsign
layer which produces prediction of soiling type for each grid.
In case of image level soiling experiments, decoder has the
same convolution layers but with a higher stride to reduce
the spatial dimensions less gradually. We use binary cross
entropy per class as the loss function. The implementation
was done using Keras [2]. ADAM optimizer was used as it
provides faster convergence with a learning rate of 0.0005.
The input image resolution is 1280x800.

IV. EXPERIMENTAL RESULTS

In this section, we discuss our experimental

results
summarized in three tables. Table III summarizes three sets
of experiments done on image level classiﬁcation. Table
I summarizes tile level experiments on the four cameras
individually. Table II lists the difference in accuracy by
using various encoder training strategies. We discuss the

Camera
Front
Rear
Right
Left

Precision
59.41%
64.17%
71.03%
69.99%

Recall
99.16%
99.94%
98.89%
99.72%

TABLE I: Tile-level soiling classiﬁcation accuracy

Encoder Training Strategy
Single-task - ImageNet pre-training
Single-task - No pre-training
Multi-task training
Mutlti-task with GAN augmentation

TPR
52%
56%
55%
58%

FPR
16%
14%
14%
14%

TABLE II: Evaluation of different
image-level soiling classiﬁcation (metrics rounded off)

training strategies for

results in more detail below.

Image Level Classiﬁcation: The number of annotation
samples is much higher at
image level and we tabulate
more detailed results for these experiments. The perspective
of right and left cameras are very different from front and
rear cameras. The latter is closer to the typical front camera
perspective of public datasets. Thus we perform ablation
studies with and without right/left cameras summarized in
Table III. Both normalized and un-normalized confusion
matrices are shared. There are three sets of experiments -
(1) training and testing only on front and rear cameras, (2)
training on front/rear cameras and testing on all cameras
and (3) training on all cameras and testing on all cameras.
In experiment (1), high detection rate was obtained for
each class. Experiment (2) shows degradation in accuracy
indicating that the network doesn’t generalize from front/rear
to left/right network. Experiment (3) shows that when the
same network is trained on all images, it performs well and
slightly better than (1) as there is better regularization.

Tile Level Classiﬁcation: We perform tile-level classi-
ﬁcation using the conﬁguration of 64 × 64. Precision and
recall values are summarized in Table I. Unlike high accuracy
values of image level task, precision values obtained here
show that there is a lot of room for further improvement
in accuracy. Front and rear have a higher probability of
getting soiling especially with difﬁcult scenarios and thus
it has much higher samples.

Encoder Training Strategies: We evaluate different
encoder training strategies which are summarized in Table II.
As discussed before, the goal is ﬁnally to design an efﬁcient
network which can be deployed on a commercial platform
and we propose to make use of multi-task learning. As an
ablation study, we compare it with single-task soiling only
learning and ImageNet only pre-trained network. ImageNet
pre-trained encoder is worse than soiling single-task encoder
by 4% in True Positive Rate (TPR) and 2% in False Positive
Rate (FPR). Multi-task encoder achieves close to single-task
network encoder with a 1% reduction in TPR, however this
comes at a much higher system level efﬁciency. Multi-task

encoder is further improved by 3% by making using of
additional GAN generated images.

GAN based data augmentation: Just after a few epochs
the CycleGAN experiment started to show promising results,
it was visible that
the generators are able to recognize
correctly which parts of the image are affected by soiling.
After several tens of epochs, we already had generator which
was capable of introducing soiling into the image. The “de-
soiling” generator needed much more time to learn. However,
close to the ﬁnal number of epochs, which we set to 200, it
was also doing quite good job. Note, that the “de-soiling”
task is much harder, as the network has to learn to in-
paint some characteristic structures which are ﬁtting into
the original image seamlessly. In Fig. 5 we depict several
examples of the CycleGAN experiments results.

Unfortunately, we were not able to converge with MUNIT
experiments to something reasonable as it requires more
parameter tuning and probably also higher quality data. The
generated images were containing still too much artifacts
and in some cases failing completely to generate reasonable
images. We want to pursue with MUNIT experiments in our
future work as we believe that it is a promising approach to
be explored further.

Practical Challenges: Soiling detection poses many chal-
lenges in practice. It might appear to be a standard seg-
mentation problem however there is no spatial or geometric
structure present in soiling pattern unlike other objects. Even
the texture can vary drastically across different occurrence of
soiling. Thus it makes it difﬁcult for learning discriminative
features. In particular, transparent soiling is challenging to
detect as it subtly changes the appearance of the scene.
Motion blurred areas which commonly occur in high speed
scenes often becomes classiﬁed as transparent soiling. The
class annotation also has to be appropriately labelled accord-
ing to severity of the soiling, this can be highly subjective.

V. CONCLUSIONS

In this paper, we discussed the importance of soiling
detection task and why its necessary for achieving higher
levels of autonomous driving. We created a multi-camera
surround view soiling dataset and a portion of this dataset
will be made public to encourage further research in this
area. We demonstrated that a relatively small-sized CNN
achieves accurate results. We also demonstrated that GAN
based augmentation using CycleGAN and MUNIT works
well for soiling generation. We evaluated various training
methodologies and demonstrated that soiling can be merged
into an object detection and segmentation multi-task network
for high efﬁciency.

REFERENCES

[1] D. Berman, S. Avidan, et al. Non-local

In
Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 1674–1682, 2016.

image dehazing.

[2] F. Chollet et al. Keras, 2015.
[3] R. Fattal. Single image dehazing. ACM Trans. Graph., 27(3):72:1–

72:9, 2008.

Normalized Confusion Matrix

Raw Confusion Matrix

Training and Testing on front/rear cameras

Clean
1
0.06
0.02
0

Transparent
0
0.92
0.02
0

Opaque
0
0
0.94
0.04

Both
0
0.02
0.03
0.94

Clean
14600
225
135
0

Transparent
0
3730
134
9

Clean
0.7
0.14
0.02
0.01

Training on front/rear cameras and Testing on all cameras
Transparent
1
4192
209
14
Training on all cameras and Testing on all cameras

Transparent
0
0.52
0.01
0

Opaque
0.28
0.29
0.92
0.12

Clean
20563
1145
255
73

Both
0.02
0.06
0.06
0.87

Clean
1
0
0
0

Transparent
0
0.99
0
0

Opaque
0
0
1
0

Both
0
0
0
1

Clean
29333
25
0
0

Transparent
11
8073
0
13

Opaque
17
13
7913
191

Opaque
8280
2337
15401
1206

Opaque
8
1
16816
14

Both
0
91
240
4818

Both
510
463
952
8441

Both
2
38
1
9707

–
Clean
Transparent
Opaque
Both

–
Clean
Transparent
Opaque
Both

–
Clean
Transparent
Opaque
Both

TABLE III: Summary of results of Image-level soiling classiﬁcation

Fig. 5: The results obtained from the CycleGAN proof of concept for the problem of soiling and adverse weather classiﬁcation.
The legend per each quadruple of images: the top left image shows the original clean image; the top right image is the
generated ”soiled“ version of this clean image. The bottom left image is the original soiled image; the bottom right image
is the ”clean“ version of this soiled image.

[4] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In
Advances in neural information processing systems, pages 2672–2680,
2014.

[5] M. Heimberger, J. Horgan, C. Hughes, J. McDonald, and S. Yogamani.
Computer vision in automated parking systems: Design, implementa-
tion and challenges. Image and Vision Computing, 68:88–101, 2017.
[6] J. Horgan, C. Hughes, J. McDonald, and S. Yogamani. Vision-based
driver assistance systems: Survey, taxonomy and advances. In 2015
IEEE 18th International Conference on Intelligent Transportation
Systems, pages 2032–2039. IEEE, 2015.

[7] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. Multimodal unsu-
pervised image-to-image translation. In Proceedings of the European
Conference on Computer Vision (ECCV), pages 172–189, 2018.
[8] S. A. E. International. Taxonomy and deﬁnitions for terms related to
on-road motor vehicle automated driving systems J3016, 2018.
[9] S. Ki, H. Sim, J.-S. Choi, S. Kim, and M. Kim. Fully end-to-end
learning based conditional boundary equilibrium gan with receptive
ﬁeld sizes enlarged for single ultra-high resolution image dehazing. In
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition Workshops, pages 817–824, 2018.

[10] H. Porav, T. Bruls, and P. Newman.

I can see clearly now: Image

restoration via de-raining. arXiv preprint arXiv:1901.00893, 2019.

[11] C. Sakaridis, D. Dai, and L. Van Gool.

Semantic foggy scene
understanding with synthetic data. International Journal of Computer
Vision, pages 1–20, 2018.

[12] G. Sistu,

I. Leang, S. Chennupati, S. Milz, S. Yogamani, and
S. Rawashdeh. NeurAll: Towards a uniﬁed model for visual perception
in automated driving. arXiv preprint arXiv:1902.03589, 2019.
[13] G. Sistu, I. Leang, and S. Yogamani. Real-time joint object detection
arXiv
and semantic segmentation network for automated driving.
preprint arXiv:1901.03912, 2019.

[14] M. Uˇriˇc´aˇr, P. Kˇr´ıˇzek, D. Hurych, I. Sobh, S. Yogamani, and P. Denny.
techniques for autonomous

Yes, We GAN: Applying adversarial
driving. CoRR, abs/1902.03442, 2019.

[15] S. Yogamani, C. Hughes, J. Horgan, G. Sistu, P. Varley, D. O’Dea,
M. Uric´ar, et al. Woodscape: A multi-task, multi-camera ﬁsheye
dataset for autonomous driving. arXiv preprint arXiv:1905.01489,
2019.

[16] J. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired Image-to-Image
In IEEE
Translation Using Cycle-Consistent Adversarial Networks.
International Conference on Computer Vision, ICCV 2017, Venice,
Italy, October 22-29, 2017, pages 2242–2251, 2017.

http://www.ug2challenge.

[17] CVPR 2019 UG2 Challenge.
org/, 2019 (accessed April 12, 2019).

