Uniﬁed Spectral Clustering with Optimal Graph

Zhao Kang1, Chong Peng2, Qiang Cheng3 and Zenglin Xu1∗
1School of Computer Science and Engineering, University of Electronic Science and Technology of China
2Department of Computer Science, Southern Illinois University, Carbondale, USA
3Institute of Biomedical Informatics and Department of Computer Science, University of Kentucky USA
zkang@uestc.edu.cn, pchong@siu.edu, qiang.cheng@uky.edu, zlxu@uestc.edu.cn

7
1
0
2
 
v
o
N
 
2
1
 
 
]

G
L
.
s
c
[
 
 
1
v
8
5
2
4
0
.
1
1
7
1
:
v
i
X
r
a

Abstract

Spectral clustering has found extensive use in many areas.
Most traditional spectral clustering algorithms work in three
separate steps: similarity graph construction; continuous la-
bels learning; discretizing the learned labels by k-means clus-
tering. Such common practice has two potential ﬂaws, which
may lead to severe information loss and performance degra-
dation. First, predeﬁned similarity graph might not be optimal
for subsequent clustering. It is well-accepted that similarity
graph highly affects the clustering results. To this end, we
propose to automatically learn similarity information from
data and simultaneously consider the constraint that the sim-
ilarity matrix has exact c connected components if there are
c clusters. Second, the discrete solution may deviate from the
spectral solution since k-means method is well-known as sen-
sitive to the initialization of cluster centers. In this work, we
transform the candidate solution into a new one that better
approximates the discrete one. Finally, those three subtasks
are integrated into a uniﬁed framework, with each subtask it-
eratively boosted by using the results of the others towards
an overall optimal solution. It is known that the performance
of a kernel method is largely determined by the choice of ker-
nels. To tackle this practical problem of how to select the most
suitable kernel for a particular data set, we further extend our
model to incorporate multiple kernel learning ability. Exten-
sive experiments demonstrate the superiority of our proposed
method as compared to existing clustering approaches.

Introduction
Clustering is a fundamental technique in machine learning,
pattern recognition, and data mining (Huang et al. 2017). In
past decades, a variety of clustering algorithms have been
developed, such as k-means clustering and spectral cluster-
ing.

With the beneﬁts of simplicity and effectiveness, k-means
clustering algorithm is often adopted in various real-world
problems. To deal with the nonlinear structure of many prac-
tical data sets, kernel k-means (KKM) algorithm has been
developed (Sch¨olkopf et al. 1998), where data points are
mapped through a nonlinear transformation into a higher di-
mensional feature space in which the data points are linearly
separable. KKM usually achieves better performance than
the standard k-means. To cope with noise and outliers, ro-
bust kernel k-means (RKKM) (Du et al. 2015) algorithm

∗Corresponding author.

has been proposed. In this approach, the squared (cid:96)2 norm
of error construction term is replaced by (cid:96)2,1 norm. RKKM
demonstrates superior performance on a number of bench-
mark data sets. The performance of such model-based meth-
ods heavily depends on whether the data ﬁt the model. Un-
fortunately, in most cases, we do not know the distribution
of data in advance. To some extent, this problem is alleviated
by multiple kernel learning. Moreover, there is no theoretical
result on how to choose the similarity graph (Von Luxburg
2007).

Spectral clustering is another widely used clustering
method (Kumar et al. 2011). It enjoys the advantage of ex-
ploring the intrinsic data structures by exploiting the differ-
ent similarity graphs of data points (Yang et al. 2015). There
are three kinds of similarity graph constructing strategies:
k-nearest-neighborhood (knn); (cid:15)-nearest-neighborhood; The
fully connected graph. Here, some open issues arise (Huang
et al. 2015): 1) how to choose a proper neighbor number k or
radius (cid:15); 2) how to select an appropriate similarity metric to
measure the similarity among data points; 3) how to counter-
act the adverse effect of noise and outliers; 4) how to tackle
data with structures at different scales of size and density.
Unfortunately, all of these issues heavily inﬂuence the clus-
tering results (Zelnik-Manor and Perona 2004). Nowadays,
many data are often high dimensional, heterogeneous, and
without prior knowledge, and it is therefore a fundamental
challenge to deﬁne a pairwise similarity graph for effective
spectral clustering.

Recently, (Zhu et al. 2014) construct robust afﬁnity
graphs for spectral clustering by identifying discrimina-
tive features. It adopts a random forest approach based on
the motivation that tree leaf nodes contain discriminative
data partitions, which can be exploited to capture subtle
and weak data afﬁnity. This approach shows better per-
formance than other state-of-the-art methods including the
Euclidean-distance-based knn (Wang et al. 2008), dominant
neighbourhoods (Pavan and Pelillo 2007), consensus of knn
(Premachandran and Kakarala 2013), and non-metric based
unsupervised manifold forests (Pei et al. 2013).

The second step of spectral clustering is to use the spec-
trum of the similarity graph to reveal the cluster structure
of the data. Due to the discrete constraint on the cluster la-
bels, this problem is NP-hard. To obtain a feasible approxi-
mation solution, spectral clustering solves a relaxed version

of this problem, i.e., the discrete constraint is relaxed to al-
low continuous values. It ﬁrst performs eigenvalue decom-
position on the Laplacian matrix to generate an approximate
indicator matrix with continuous values. Then, k-means is
often implemented to produce ﬁnal clustering labels (Huang
et al. 2013). Although this approach has been widely used
in practice, it may exhibit poor performance since the k-
means method is well-known as sensitive to the initialization
of cluster centers (Ng et al. 2002).

To address the aforementioned problems, in this paper, we
propose a uniﬁed spectral clustering framework. It jointly
learns the similarity graph from the data and the discrete
clustering labels by solving an optimization problem, in
which the continuous clustering labels just serve as interme-
diate products. To the best of our knowledge, this is the ﬁrst
work that combine the three steps into a single optimization
problem. As we show later, it is not trivial to unify them.
The contributions of our work are as follows:
1. Rather than using predeﬁned similarity metrics, the simi-
larity graph is adaptively learned from the data in a kernel
space. By combining similarity learning with subsequentl
clustering into a uniﬁed framework, we can ensure the op-
timality of the learned similarity graph.

2. Unlike existing spectral clustering methods that work in
three separate steps, we simultaneously learn similarity
graph, continuous labels, and discrete cluster labels. By
leveraging the inherent interactions between these three
subtasks, they can be boosted by each other.

3. Based on our single kernel model, we further extend it to
have the ability to learn the optimal combination of mul-
tiple kernels.
Notations. Given a data set [x1, x2, · · · , xn], we denote
X ∈ Rm×n with m features and n samples. Then the i-
th sample and (i, j)-th element of matrix X are denoted by
xi ∈ Rm×1 and xij, respectively. The (cid:96)2-norm of a vector x
is deﬁned as (cid:107)x(cid:107)2 = x(cid:62) · x, where (cid:62) means transpose. The
squared Frobenius norm is denoted by (cid:107)X(cid:107)2
ij x2
ij.
The (cid:96)1-norm of matrix X is deﬁned as the absolute summa-
tion of its entries, i.e., (cid:107)X(cid:107)1 = (cid:80)
j |xij|. I denotes the
identity matrix. Tr(·) is the trace operator. Z ≥ 0 means all
the elements of Z are nonnegative.

F = (cid:80)

(cid:80)

i

Preliminary Knowledge

Sparse Representation
Recently, sparse representation, which assumes that each
data point can be reconstructed as a linear combination of
the other data points, has shown its power in many tasks
(Cheng et al. 2010; Peng et al. 2016). It often solves the
following problem:
(cid:107)X − XZ(cid:107)2

F + α(cid:107)Z(cid:107)1, s.t. Z ≥ 0, diag(Z) = 0,
(1)
where α > 0 is a balancing parameter. Eq. (1) simultane-
ously determines both the neighboring samples of a data
point and the corresponding weights by the sparse recon-
struction from the remaining samples. In principle, more
similar points should receive bigger weights and the weights

min
Z

should be smaller for less similar points. Thus Z is also
called similarity graph matrix (Kang et al. 2015). In addi-
tion, sparse representation enjoys some nice properties, e.g.,
the robustness to noise and datum-adaptive ability (Huang et
al. 2015). On the other hand, model (1) has a drawback, i.e.,
it does not consider nonlinear data sets where data points
reside in a union of manifolds (Kang et al. 2017a).

Spectral Clustering
Spectral clustering requires Laplacian matrix L ∈ Rn×n as
an input, which is computed as L = D − Z(cid:62)+Z
, where
D ∈ Rn×n is a diagonal matrix with the i-th diagonal ele-
ment (cid:80)
. In traditional spectral clustering methods,
j
similarity graph Z ∈ Rn×n is often constructed in one of
the three ways aforementioned. Supposing there are c clus-
ters in the data X, spectral clustering solves the following
problem:

zij +zij
2

2

T r(F (cid:62)LF ),

s.t. F ∈ Idx,

(2)

min
F

where F = [f1, f2, · · · , fn](cid:62) ∈ Rn×c is the cluster indica-
tor matrix and F ∈ Idx represents the clustering label vec-
tor of each point fi ∈ {0, 1}c×1 contains one and only one
element “1” to indicate the group membership of xi. Due
to the discrete constraint on F , problem (2) is NP-hard. In
practice, F is relaxed to allow continuous values and solve

T r(P (cid:62)LP ),

s.t. P (cid:62)P = I,

(3)

min
P

where P ∈ Rn×c is the relaxed continuous clustering la-
bel matrix, and the orthogonal constraint is adopted to avoid
trivial solutions. The optimal solution is obtained from the c
eigenvectors of L corresponding to the c smallest eigenval-
ues. After obtaining F , traditional clustering method, e.g.,
k-means, is implemented to obtain discrete cluster labels
(Huang et al. 2013).

Although this three-steps approach provides a feasible so-
lution, it comes with two potential risks. First, since the
similarity graph computation is independent of the subse-
quent steps, it may be far from optimal. As we discussed
before, the clustering performance is largely determined by
the similarity graph. Thus, ﬁnal results may be degraded.
Second, the ﬁnal solution may unpredictably deviate from
the ground-truth discrete labels (Yang et al. 2016). To ad-
dress these problems, we propose a uniﬁed spectral cluster-
ing model.

Spectral Clustering with Single Kernel

Model
One drawback of Eq. (1) is that it assumes that all the points
lie in a union of independent or disjoint subspaces and are
noiseless. In the presence of dependent subspaces, nonlinear
manifolds and/or data errors, it may select points from dif-
ferent structures to represent a data point and makes the rep-
resentation less informative (Elhamifar and Vidal 2009). It is
recognized that nonlinear data may represent linearity when
mapped to an implicit, higher-dimensional space via a ker-
nel function. To fully exploit data information, we formulate
Eq. (1) in a general manner with a kernelization framework.

Let φ : RD → H be a kernel mapping the data samples
from the input space to a reproducing kernel Hilbert space
R. Then X is transformed to φ(X) = [φ(x1), · · · , φ(xn)].
The kernel similarity between data samples xi and xj
is deﬁned through a predeﬁned kernel as Kxi,xj =<
φ(xi), φ(xj) >. By applying this kernel trick, we do not
need to know the transformation φ. In the new space, Eq. (1)
becomes (Zhang et al. 2010)

min
Z

(cid:107)φ(X) − φ(X)Z(cid:107)2

F + α(cid:107)Z(cid:107)1,

⇐⇒ min

T r(φ(X)T φ(X) − φ(X)T φ(X)Z

Z
− Z T φ(X)T φ(X) + Z T φ(X)T φ(X)Z) + α(cid:107)Z(cid:107)1,

⇐⇒ min

T r(K − 2KZ + Z (cid:62)KZ) + α(cid:107)Z(cid:107)1,

Z

s.t. Z ≥ 0,

diag(Z) = 0,

This model recovers the linear relations among the data in
the new space, and thus the nonlinear relations in the origi-
nal representation. Eq. (4) is more general than Eq. (1) and
is supposed to learn arbitrarily shaped data structure. More-
over, Eq. (4) goes back to Eq. (1) when a linear kernel is
applied.

To fulﬁll the clustering task, we propose our spectral clus-

min
Z,F,P,Q

tering with single kernel (SCSK) model as following:
T r(K − 2KZ + Z (cid:62)KZ) + α(cid:107)Z(cid:107)1
(cid:124)
(cid:123)(cid:122)
(cid:125)
similarity learning
+ γ(cid:107)F − P Q(cid:107)2
F
(cid:124)
(cid:125)
(cid:123)(cid:122)
discrete label learning

+ βT r(P (cid:62)LP )
(cid:125)

(cid:123)(cid:122)
continuous label learning

(cid:124)

,

(5)

s.t. Z ≥ 0,

diag(Z) = 0,

P (cid:62)P = I, Q(cid:62)Q = I, F ∈ Idx,

where α, β, and γ are penalty parameters, and Q is a rota-
tion matrix. Due to the spectral solution invariance property
(Yu and Shi 2003), for any solution P , P Q is another so-
lution. The purpose of the last term is to ﬁnd a proper or-
thonormal Q such that the resulting P Q is close to the real
discrete clustering labels. In Eq. (5), the similarity graph and
the ﬁnal discrete clustering labels are automatically learned
from the data. Ideally, whenever data points i and j belong
to different clusters, we must have zij = 0 and it is also true
vice versa. That is to say, we have zij (cid:54)= 0 if and only if
data points i and j are in the same cluster, or, equivalently
fi = fj. Therefore, our uniﬁed framework Eq. (5) can ex-
ploit the correlation between the similarity matrix and the
labels. Because of the feedback of inferred labels to induce
the similarity matrix and vice versa, we say that our cluster-
ing framework has a self-taught property.

In fact, Eq. (5) is not a simple uniﬁcation of the pipeline
of steps. It learns a similarity graph with optimal structure
for clustering. Ideally, Z should have exactly c connected
components if there are c clusters in the data set (Kang et
al. 2017b). This is to say that the Laplacian matrix L has
c zero eigenvalues (Mohar et al. 1991), i.e., the summation
of the smallest c eigenvalues is zero. To ensure the optimal-
ity of the similarity graph, we can minimize (cid:80)c
i=1 σi(L).

i=1 σi(L) =
T r(P (cid:62)LP ). Therefore, the spectral clustering term,

According to Ky Fan’s theorem (Fan 1949), (cid:80)c
min
P (cid:62)P =I
i.e., the second term in Eq. (5), will ensure learned Z is op-
timal for clustering.

Optimization
To efﬁciently and effectively solve Eq. (5), we design an al-
ternated iterative method.
Computation of Z: With F , P , Q ﬁxed, the problem is re-
duced to

T r(K − 2KZ + Z (cid:62)KZ) + α(cid:107)Z(cid:107)1 + βT r(P (cid:62)LP ),

min
Z

s.t. Z ≥ 0,

diag(Z) = 0.

(4)

We introduce an auxiliary variable S to make above objec-
tive function separable and solve the following equivalent
problem:

T r(K − 2KZ + Z (cid:62)KZ) + α(cid:107)S(cid:107)1 + βT r(P (cid:62)LP ),

min
Z

s.t. Z ≥ 0,

diag(Z) = 0, S = Z.

(6)

(7)

This can be solved by using the augmented Lagrange mul-
tiplier (ALM) type of method. We turn to minimizing the
following augmented Lagrangian function:

L(S, Z, Y ) =T r(K − 2KZ + Z (cid:62)KZ) + α(cid:107)S(cid:107)1

+ βT r(P (cid:62)LP ) +

(cid:107)S − Z +

µ
2

(8)

Y
µ

(cid:107)2
F ,

where µ > 0 is the penalty parameter and Y is the Lagrange
multiplier. This problem can be minimized with respect to
S, Z, and Y alternatively, by ﬁxing the other variables.

For S, by letting H = Z − Y

µ , it can be updated element-

wisely as below

Sij = max(|Hij| − α/µ, 0) · sign(Hij).

(9)

For Z, by letting E = S + Y
wisely as:

µ , it can be updated column-

Z T
i (

min
Zi

µ
2

β
2

I + K)Zi + (

i − µET
dT

i − 2Ki,:)Zi,

(10)

where di ∈ Rn×1 is a vector with the j-th element dij being
dij = (cid:107)Pi,: − Pj,:(cid:107)2. It is easy to obtain Zi by setting the
derivative of Eq. (10) w.r.t. Zi to be zero.
Computation of P: With F , Z, and Q ﬁxed, it is equivalent
to solving

min
P

βT r(P (cid:62)LP ) + γ(cid:107)F − P Q(cid:107)2

F s.t. P (cid:62)P = I.

(11)

The above problem with orthogonal constraint can be efﬁ-
ciently solved by the algorithm proposed by Wen and Yin
(Wen and Yin 2013).
Computation of Q: With F , Z, and P ﬁxed, we have

min
Q

(cid:107)F − P Q(cid:107)2
F

s.t. Q(cid:62)Q = I.

(12)

Algorithm 1 The algorithm of SCSK
Input: Kernel matrix K, parameters α > 0, β > 0, γ > 0, µ > 0.
Initialize: Random matrices Z, P , and Q. Y = 0 and F = 0.
REPEAT
1: Update S according to Eq. (9).
2: S = S − diag(diag(S)) and S = max(S, 0).
3: Update Z according to Eq. (10).
4: Y = Y + µ(S − Z).
5: Update P by solving the problem of Eq. (11).
6: Update Q according to Eq. (13).
7: Update F according to Eq. (16).
UNTIL stopping criterion is met.

It is the orthogonal Procrustes problem (Sch¨onemann 1966),
which admits a closed-form solution. The solution is

Q = U V (cid:62),

(13)

where U and V are left and right parts of the SVD decom-
position of F (cid:62)P .
Computation of F: With Z, P and Q ﬁxed, the problem
becomes

min
F

(cid:107)F − P Q(cid:107)2
F ,

s.t. F ∈ Idx.

(14)

Note that T r(F (cid:62)F ) = n, the above subproblem can be
rewritten as below:

T r(F (cid:62)P Q)

s.t. F ∈ Idx.

(15)

max
F

The optimal solution can be easily obtained as follows:

(cid:40)1,

j = argmax

(P Q)ik

Fij =

k
0, otherwise

(16)

The updates of Z, P , F , and Q are coupled with each
other, so we could reach an overall optimal solution. The
details of our SCSK optimization are summarized in Algo-
rithm 1.

Complexity Analysis
With our optimization strategy, the updating of S requires
O(n2) complexity. The quadratic program can be solved in
polynomial time. The solution of Q involves SVD and its
complexity is O(nc2 + c3). To update P , we need O(nc2 +
c3). The complexity for F is O(nc2). Note that the number
of clusters c is often a small number. Therefore, the main
computation load is from solving Z, which involves matrix
inversion. Fortunately, Z is solved in parallel.

Spectral Clustering with Multiple Kernels

Model
Although the model in Eq. (5) can automatically learn the
similarity graph matrix and discrete cluster labels, its per-
formance will strongly depend on the choice of kernels. It
is often impractical to exhaustively search for the most suit-
able kernel. Moreover, real world data sets are often gener-
ated from different sources along with heterogeneous fea-
tures. Single kernel method may not be able to fully utilize

such information. Multiple kernel learning has the ability to
integrate complementary information and identify a suitable
kernel for a given task. Here we present a way to learn an
appropriate consensus kernel from a convex combination of
a number of predeﬁned kernel functions.

functions {K i}r

Suppose there are a total number of r different ker-
i=1. An augmented Hilbert space
nel
can be constructed by using the mapping of ˜φ(x) =
√
√
wrφr(x)](cid:62) with different
[
wi(wi ≥ 0). Then the combined kernel Kw can
weights
be represented as (Zeng and Cheung 2011)

w2φ2(x), ...,

w1φ1(x),

√

√

Kw(x, y) =< φw(x), φw(y) >=

wiK i(x, y).

(17)

r
(cid:88)

i=1

Note that the convex combination of the positive semi-
deﬁnite kernel matrices {K i}r
i=1 is still a positive semi-
deﬁnite kernel matrix. Thus the combined kernel still satis-
ﬁes Mercer’s condition. Then our proposed method of spec-
tral clustering with multiple kernels (SCMK) can be formu-
lated as
min
Z,F,P,Q,w

T r(Kw − 2KwZ + Z (cid:62)KwZ) + α(cid:107)Z(cid:107)1+

βT r(P (cid:62)LP ) + γ(cid:107)F − P Q(cid:107)2
F ,
diag(Z) = 0,
s.t. Z ≥ 0,

P (cid:62)P = I, Q(cid:62)Q = I, F ∈ Idx,

Kw =

wiK i,

wi = 1, wi ≥ 0.

r
(cid:88)

i=1

r
(cid:88)

√

i=1

(18)

Now above model will learn the similarity graph, discrete
clustering labels, and kernel weights by itself. By iteratively
updating Z, F , and w, each of them will be iteratively re-
ﬁned according to the results of the others.

Optimization
In this part, we show an efﬁcient and effective algorithm to
iteratively and alternatively solve Eq. (18).

w is ﬁxed: Update other variables when w is ﬁxed: We
can directly calculate Kw, and the optimization problem is
exactly Eq. (5). Thus we just need to use Algorithm 1 with
Kw as the input kernel matrix.

Update w: Optimize with respect to w when other vari-
ables are ﬁxed: Solving Eq. (18) with respect to w can be
rewritten as (Cai et al. 2013)

r
(cid:88)

wihi

i=1
r
(cid:88)

√

min
w

s.t.

i=1

wi = 1, wi ≥ 0,

(19)

where

hi = T r(K i − 2K iZ + Z (cid:62)K iZ).

(20)

The Lagrange function of Eq. (19) is

J (w) = w(cid:62)h + γ(1 −

(21)

r
(cid:88)

√

i=1

wi).

i=1, parameters α > 0,

Algorithm 2 The algorithm of SCMK
Input: A set of kernel matrix {K i}r
β > 0, γ > 0, µ > 0.
Initialize: Random matrices Z, P , and Q. Y = 0 and F =
0. wi = 1/r.
REPEAT
1: Calculate Kw by Eq. (17).
2: Do steps 1-7 in Algorithm 1.
3: Calculate h by Eq. (20).
4: Calculate w by Eq. (22).
UNTIL stopping criterion is met.

By utilizing the Karush-Kuhn-Tucker (KKT) condition with
∂J (w)
∂wi

wi = 1, we obtain the

= 0 and the constraint

√

r
(cid:80)
i=1

solution of w as follows:



wi =

hi



−2



.

r
(cid:88)

j=1

1
hj

(22)

We can see that w is closely related to Z. Therefore, we
could obtain both optimal similarity matrix Z and kernel
weight w. We summarize the optimization process of Eq.
(18) in Algorithm 2.

Experiments

Table 1: Description of the data sets

YALE
JAFFE
ORL
AR
COIL20
BA
TR11
TR41
TR45
TDT2

# instances
165
213
400
840
1440
1404
414
878
690
9394

# features
1024
676
1024
768
1024
320
6429
7454
8261
36771

# classes
15
10
40
120
20
36
9
10
10
30

Data Sets
There are altogether ten real benchmark data sets used in our
experiments. Table 1 summarizes the statistics of these data
sets. Among them, the ﬁrst six are image data, and the other
four are text corpora12.

The six image data sets consist of four famous face
databases (ORL3, YALE4, AR5 and JAFFE6), a toy im-

1http://www-users.cs.umn.edu/ han/data/tmdata.tar.gz
2http://www.cad.zju.edu.cn/home/dengcai/Data/TextData.html
3http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html
4http://vision.ucsd.edu/content/yale-face-database
5http://www2.ece.ohio-state.edu/ aleix/ARdatabase.html
6http://www.kasrl.org/jaffe.html

20.php

age database COIL207, and a binary alpha digits data set
BA8. Speciﬁcally, COIL20 contains images of 20 objects.
For each object, the images were taken ﬁve degrees apart
as the object is rotating on a turntable. There are 72 im-
ages for each object. Each image is represented by a 1,024-
dimensional vector. BA consists of digits of “0” through “9”
and letters of capital “A” through “Z”. There are 39 exam-
ples for each class. YALE, ORL, AR, and JAFEE contain
images of individuals. Each image has different facial ex-
pressions or conﬁgurations due to times, illumination condi-
tions, and glasses/no glasses.

Kernel Design
To assess the effectiveness of multiple kernel learning, we
adopted 12 kernels. They include: seven Gaussian kernels
of the form K(x, y) = exp(−(cid:107)x − y(cid:107)2
max)), where
dmax is the maximal distance between samples and t varies
over the set {0.01, 0.0, 0.1, 1, 10, 50, 100}; a linear kernel
K(x, y) = x(cid:62)y; four polynomial kernels K(x, y) = (a +
x(cid:62)y)b with a ∈ {0, 1} and b ∈ {2, 4}. Furthermore, all
kernels are rescaled to [0, 1] by dividing each element by the
largest pair-wise squared distance.

2/(td2

Comparison Algorithms
For single kernel methods, we run downloaded kernel k-
means (KKM) (Sch¨olkopf et al. 1998), spectral clustering
(SC) (Ng et al. 2002), robust kernel k-means (RKKM) (Du
et al. 2015), and SCSK on each kernel separately. To demon-
strate the advantage of our uniﬁed framework, we also im-
plement three separate steps method (TSEP), i.e., learn the
similarity matrix by (4), spectral clustering, k-means (repeat
20 times). And we report both the best and the average re-
sults over all these kernels.

In addition, we also implement the recent simplex sparse
representation (SSR) (Huang et al. 2015) method and robust
afﬁnity graph construction methods by using random for-
est approach: ClustRF-u and ClustRF-a (Zhu et al. 2014).
ClustRF-u assumes all tree nodes are uniformly important,
while ClustRF-a assigns an adaptive weight to each node.
Note that these three methods can only process data in the
original feature space. Moreover, ClusteRF has a high de-
mand for memory and cannot process high dimensional data
directly. Thus we follow the authors’ strategy and perform
PCA on TR11, TR41, and TR45 to reduce the dimension.
We use different numbers of dominant components and re-
port the best clustering results. Nevertheless, we still cannot
handle TDT2 data set with them.

For multiple kernel methods, we implement our proposed
method and directly use the downloaded programs for the
methods in comparison on a combination of these 12 ker-
nels:

MKKM9. The MKKM (Huang et al. 2012b) extends k-
means in a multiple kernel setting. However, it imposes a
different constraint on the kernel weight distribution.

7http://www.cs.columbia.edu/CAVE/software/softlib/coil-

8http://www.cs.nyu.edu/ roweis/data.html
9http://imp.iis.sinica.edu.tw/IVCLab/research/Sean/mkfc/code

KKMKKM-m SC SC-mRKKMRKKM-mClustRF-uClustRF-a SSR TSEPTSEP-mSCSKSCSK-m MKKMAASCRMKKMSCMK

(a) Accuracy(%)

KKMKKM-m SC SC-mRKKMRKKM-mClustRF-uClustRF-a SSR TSEPTSEP-mSCSKSCSK-m MKKMAASCRMKKMSCMK

Data
YALE 47.12 38.97 49.4240.52 48.09
JAFFE 74.39 67.09 74.8854.03 75.61
ORL

53.53 45.93 57.9646.65 54.96

AR

33.02 30.89 28.8322.22 33.43
COIL2059.49 50.74 67.7043.65 61.64
BA

41.20 33.66 31.0726.25 42.17

TR11

TR41

51.91 44.65 50.9843.32 53.03

55.64 46.34 63.5244.80 56.76

TR45

58.79 45.58 57.3945.96 58.13
TDT2 47.05 35.58 52.6345.26 48.35

Data
YALE 51.34 42.07 52.9244.79 52.29
JAFFE 80.13 71.48 82.0859.35 83.47
ORL

73.43 63.36 75.1666.74 74.23

AR

65.21 60.64 58.3756.05 65.44
COIL2074.05 63.57 80.9854.34 74.63
BA

57.25 46.49 50.7640.09 57.82

TR11

TR41

48.88 33.22 43.1131.39 49.69

59.88 40.37 61.3336.60 60.77

TR45

57.87 38.69 48.0333.22 57.86
TDT2 55.28 38.47 52.2327.16 54.46

Data
YALE 49.15 41.12 51.6143.06 49.79
JAFFE 77.32 70.13 76.8356.56 79.58
ORL

58.03 50.42 61.4551.20 59.60

AR

35.52 33.64 33.2425.99 35.87
COIL2064.61 55.30 69.9246.83 66.35
BA

44.20 36.06 34.5029.07 45.28

TR11

TR41

67.57 56.32 58.7950.23 67.93

74.46 60.00 73.6856.45 74.99

TR45

68.49 53.64 61.2550.02 68.18
TDT2 52.79 49.26 50.3942.81 62.13

39.71

67.98

46.88

31.20

51.89

34.35

45.04

46.80

45.69

36.67

42.87

74.01

63.91

60.81

63.70

46.91

33.48

40.86

38.96

42.19

41.74

71.82

51.46

33.88

56.34

36.86

56.40

60.21

53.75

52.60

57.58

97.65

60.75

24.17

74.44

39.89

29.24

53.19

42.17

-

58.76

97.00

78.69

57.09

83.91

54.66

18.97

52.63

38.12

-

63.64

97.65

67.25

40.71

80.83

41.95

35.75

55.58

45.51

-

57.58 54.5562.58 44.60 63.05 52.88

45.70 40.64

52.18

63.25

98.59 87.3298.30 73.88 99.53 90.06

74.55 30.35

87.07

99.69

62.75 69.0070.15 41.45 74.05 53.56

47.51 27.20

55.60

74.52

35.59 65.0065.03 46.41 78.90 68.21

28.61 33.23

34.37

79.29

72.99 76.3277.68 61.03 81.48 62.59

54.82 34.87

66.65

82.21

44.01 23.9745.92 30.75 46.02 31.50

40.52 27.07

43.42

45.57

34.54 41.0671.05 42.08 74.22 55.09

50.13 47.15

57.71

74.26

60.93 63.7869.45 50.17 70.17 53.05

56.10 45.90

62.65

70.25

48.41 71.4576.54 51.07 77.74 59.53

58.46 52.64

64.00

77.47

-

20.8654.78 46.35 56.04 45.02

34.36 19.82

37.57

56.29

(b) NMI(%)

60.25 57.2660.13 46.10 60.58 52.72

50.06 46.83

55.58

61.04

98.16 92.9398.61 71.95 99.18 88.86

79.79 27.22

89.37

99.20

79.87 84.2383.28 50.76 84.78 70.93

68.86 43.77

74.83

85.21

66.64 84.1684.69 64.63 89.61 80.34

59.17 65.06

65.49

89.93

82.26 86.8984.16 71.36 87.03 72.41

70.64 41.87

77.34

86.72

58.17 30.2959.47 32.45 60.34 42.91

56.88 42.34

58.47

60.55

24.77 27.6062.71 29.88 64.60 44.48

44.56 39.39

56.08

64.89

56.78 59.5664.07 39.58 64.92 47.97

57.75 43.05

63.47

64.89

43.70 67.8270.03 40.17 70.75 50.47

56.17 41.94

62.73

70.79

-

02.4457.74 45.38 59.25 48.73

41.36 02.14

47.13

58.66

(c) Purity(%)

63.03 58.1864.77 55.38 65.87 56.19

47.52 42.33

53.64

67.39

98.59 96.2499.06 77.08 99.23 91.24

76.83 33.08

88.90

99.51

66.00 76.5076.00 52.39 77.02 57.96

52.85 31.56

60.23

78.31

46.79 69.5272.44 57.25 83.08 70.69

30.46 34.98

36.78

83.20

77.71 89.0384.03 74.89 84.24 75.58

58.95 39.14

69.95

83.78

49.14 40.8555.03 43.07 55.49 40.45

43.47 30.29

46.27

55.72

49.76 85.0285.95 63.15 86.25 63.36

65.48 54.67

72.93

85.84

65.60 75.4077.02 56.33 78.53 57.19

72.83 62.05

77.57

78.49

57.83 83.6277.28 60.52 79.70 61.06

69.14 57.49

75.20

79.78

-

46.7967.75 56.07 70.69 64.53

54.89 21.73

60.02

72.84

KKMKKM-m SC SC-mRKKMRKKM-mClustRF-uClustRF-a SSR TSEPTSEP-mSCSKSCSK-m MKKMAASCRMKKMSCMK

Table 2: Clustering results obtained on benchmark data sets. ’-m’ denotes the average performance on the 12 kernels. Both the
best results for single kernel and multiple kernel methods are highlighted in boldface.

AASC10. The AASC (Huang et al. 2012a) is an extension
of spectral clustering to the situation when multiple afﬁnities
exist. It is different from our approach since our method tries

to learn an optimal similarity graph.

RMKKM11. The RMKKM (Du et al. 2015) extends k-
means to deal with noise and outliers in a multiple kernel

10http://imp.iis.sinica.edu.tw/IVCLab/research/Sean/aasc/code

11https://github.com/csliangdu/RMKKM

(a) γ = 10−5

(b) γ = 10−4

(c) γ = 10−3

Figure 1: Parameter inﬂuence on accuracy for YALE data set.

setting.

SCMK. Our proposed method of spectral clustering with
multiple kernels. For the purpose of reproducibility, the code
is publicly available12.

For our method, we only need to run once. For those
methods that involve K-means, we follow the strategy sug-
gested in (Yang et al. 2010); i.e., we repeat clustering 20
times and present the results with the best objective values.
We set the number of clusters to the true number of classes
for all clustering algorithms.

Results
We present the clustering results of different methods on
those benchmark data sets in Table 2. In terms of accuracy,
NMI and Purity, our proposed methods obtain superior re-
sults. The big difference between the best and average re-
sults conﬁrms that the choice of kernels has a huge inﬂuence
on the performance of single kernel methods. This motivates
our extended model for multiple kernel learning. Besides,
our extended model for multiple kernel clustering usually
improves the results over our model for single kernel clus-
tering.

Although the best results of the three separate steps ap-
proach are sometimes close to our proposed uniﬁed method,
their average values are often lower than our method. We no-
tice that random forest based afﬁnity graph method achieves
good performance on image data sets. This observation can
be explained by the fact that ClustRF is suitable to handle
ambiguous and unreliable features caused by variation in il-
lumination, face expression or pose on those data sets. On
the other hand, it is not effective for text data sets. In most
cases, ClustRF-a behaves better than ClustRF-u. This jus-
tiﬁes the importance of considering neighbourhood-scale-
adaptive weighting on the nodes.

Parameter Sensitivity
There are three parameters in our model: α, β, and γ. We use
YALE data set as an example to demonstrate the sensitivity
of our model SCMK to parameters. As shown in Figure 1,

12https://github.com/sckangz/AAAI18

our model is quite insensitive to α and β, and γ over wide
ranges of values. In terms of NMI and Purity, we have simi-
lar observations.

Conclusion
In this work, we address two problems existing in most clas-
sical spectral clustering algorithms, i.e., constructing simi-
larity graph and relaxing discrete constraints to continuous
one. To alleviate performance degradation, we propose a
uniﬁed spectral clustering framework which automatically
learns the similarity graph and discrete labels from the data.
To cope with complex data, we develop our method in kernel
space. A multiple kernel approach is proposed to solve ker-
nel dependent issue. Extensive experiments on nine real data
sets demonstrated the promising performance of our meth-
ods as compared to existing clustering approaches.

Acknowledgments
This paper was in part supported by Grants from the Natural
Science Foundation of China (No. 61572111), the National
High Technology Research and Development Program of
China (863 Program) (No. 2015AA015408), a 985 Project
of UESTC (No.A1098531023601041) and a Fundamental
Research Fund for the Central Universities of China (No.
A03017023701012).

References
[Cai et al. 2013] Xiao Cai, Feiping Nie, Weidong Cai, and
Heng Huang. Heterogeneous image features integration via
multi-modal semi-supervised learning model. In Proceed-
ings of the IEEE International Conference on Computer Vi-
sion, pages 1737–1744, 2013.
[Cheng et al. 2010] Bin Cheng, Jianchao Yang, Shuicheng
Yan, Yun Fu, and Thomas S Huang. Learning with-graph
for image analysis. IEEE transactions on image processing,
19(4):858–866, 2010.
[Du et al. 2015] Liang Du, Peng Zhou, Lei Shi, Hanmo
Wang, Mingyu Fan, Wenjian Wang, and Yi-Dong Shen. Ro-
bust multiple kernel k-means using 2; 1-norm. In Proceed-

ings of the 24th International Conference on Artiﬁcial Intel-
ligence, pages 3476–3482. AAAI Press, 2015.
[Elhamifar and Vidal 2009] Ehsan Elhamifar and Ren´e Vi-
In Computer Vision and
dal. Sparse subspace clustering.
Pattern Recognition, 2009. CVPR 2009. IEEE Conference
on, pages 2790–2797. IEEE, 2009.
[Fan 1949] Ky Fan. On a theorem of weyl concerning eigen-
values of linear transformations i. Proceedings of the Na-
tional Academy of Sciences of the United States of America,
35(11):652, 1949.
[Huang et al. 2012a] Hsin-Chien Huang, Yung-Yu Chuang,
and Chu-Song Chen. Afﬁnity aggregation for spectral clus-
tering. In Computer Vision and Pattern Recognition (CVPR),
2012 IEEE Conference on, pages 773–780. IEEE, 2012.
[Huang et al. 2012b] Hsin-Chien Huang, Yung-Yu Chuang,
and Chu-Song Chen. Multiple kernel fuzzy clustering. IEEE
Transactions on Fuzzy Systems, 20(1):120–134, 2012.
[Huang et al. 2013] Jin Huang, Feiping Nie, and Heng
Huang. Spectral rotation versus k-means in spectral clus-
tering. In AAAI, 2013.
[Huang et al. 2015] Jin Huang, Feiping Nie, and Heng
Huang. A new simplex sparse learning model to measure
In Proceedings of the 24th
data similarity for clustering.
International Conference on Artiﬁcial Intelligence, pages
3569–3575. AAAI Press, 2015.
[Huang et al. 2017] Shudong Huang, Hongjun Wang, Tao Li,
Tianrui Li, and Zenglin Xu. Robust graph regularized non-
negative matrix factorization for clustering. Data Mining
and Knowledge Discovery, pages 1–21, 2017.
[Kang et al. 2015] Zhao Kang, Chong Peng, and Qiang
Cheng. Robust subspace clustering via smoothed rank ap-
proximation. IEEE Signal Processing Letters, 22(11):2088–
2092, 2015.
[Kang et al. 2017a] Zhao Kang, Chong Peng, and Qiang
Cheng. Kernel-driven similarity learning. Neurocomputing,
2017.
[Kang et al. 2017b] Zhao Kang, Chong Peng, and Qiang
Cheng. Twin learning for similarity and clustering: A uniﬁed
kernel approach. In AAAI, pages 2080–2086, 2017.
[Kumar et al. 2011] Abhishek Kumar, Piyush Rai, and Hal
Daume. Co-regularized multi-view spectral clustering. In
Advances in neural information processing systems, pages
1413–1421, 2011.
[Mohar et al. 1991] Bojan Mohar, Y Alavi, G Chartrand, and
OR Oellermann. The laplacian spectrum of graphs. Graph
theory, combinatorics, and applications, 2(871-898):12,
1991.
[Ng et al. 2002] Andrew Y Ng, Michael I Jordan, Yair Weiss,
et al. On spectral clustering: Analysis and an algorithm.
Advances in neural information processing systems, 2:849–
856, 2002.
[Pavan and Pelillo 2007] Massimiliano Pavan and Marcello
IEEE
Pelillo. Dominant sets and pairwise clustering.
transactions on pattern analysis and machine intelligence,
29(1):167–172, 2007.

[Pei et al. 2013] Yuru Pei, Tae-Kyun Kim, and Hongbin Zha.
Unsupervised random forest manifold alignment for lipread-
ing. In Proceedings of the IEEE International Conference on
Computer Vision, pages 129–136, 2013.
[Peng et al. 2016] Chong Peng, Zhao Kang, Ming Yang, and
Qiang Cheng. Feature selection embedded subspace clus-
tering. IEEE Signal Processing Letters, 23(7):1018–1022,
2016.
[Premachandran and Kakarala 2013] Vittal Premachandran
and Ramakrishna Kakarala. Consensus of k-nns for robust
neighborhood selection on graph-based manifolds. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1594–1601, 2013.
[Sch¨olkopf et al. 1998] Bernhard
Sch¨olkopf, Alexander
Smola, and Klaus-Robert M¨uller.
Nonlinear compo-
nent analysis as a kernel eigenvalue problem. Neural
computation, 10(5):1299–1319, 1998.
[Sch¨onemann 1966] Peter H Sch¨onemann. A generalized so-
lution of the orthogonal procrustes problem. Psychometrika,
31(1):1–10, 1966.
[Von Luxburg 2007] Ulrike Von Luxburg. A tutorial on spec-
tral clustering. Statistics and computing, 17(4):395–416,
2007.
[Wang et al. 2008] Jun Wang, Shih-Fu Chang, Xiaobo Zhou,
and Stephen TC Wong. Active microscopic cellular image
annotation by superposable graph transduction with imbal-
anced labels. In Computer Vision and Pattern Recognition,
2008. CVPR 2008. IEEE Conference on, pages 1–8. IEEE,
2008.
[Wen and Yin 2013] Zaiwen Wen and Wotao Yin. A feasi-
ble method for optimization with orthogonality constraints.
Mathematical Programming, 142(1-2):397–434, 2013.
[Yang et al. 2010] Yi Yang, Dong Xu, Feiping Nie,
Shuicheng Yan, and Yueting Zhuang.
Image cluster-
ing using local discriminant models and global integration.
IEEE Transactions on Image Processing, 19(10):2761–
2773, 2010.
[Yang et al. 2015] Yang Yang, Zhigang Ma, Yi Yang, Feip-
ing Nie, and Heng Tao Shen. Multitask spectral clustering
by exploring intertask correlation. IEEE transactions on cy-
bernetics, 45(5):1083–1094, 2015.
[Yang et al. 2016] Yang Yang, Fumin Shen, Zi Huang, and
Heng Tao Shen. A uniﬁed framework for discrete spectral
clustering. In IJCAI, pages 2273–2279, 2016.
[Yu and Shi 2003] Stella X Yu and Jianbo Shi. Multiclass
spectral clustering. In Computer Vision, 2003. Proceedings.
Ninth IEEE International Conference on, pages 313–319.
IEEE, 2003.
[Zelnik-Manor and Perona 2004] Lihi Zelnik-Manor
Pietro Perona. Self-tuning spectral clustering.
volume 17, page 16, 2004.
[Zeng and Cheung 2011] Hong Zeng and Yiu-ming Cheung.
Feature selection and kernel learning for local learning-
based clustering. IEEE transactions on pattern analysis and
machine intelligence, 33(8):1532–1547, 2011.

and
In NIPS,

[Zhang et al. 2010] Changshui Zhang, Feiping Nie, and
Shiming Xiang. A general kernelization framework for
learning algorithms based on kernel pca. Neurocomputing,
73(4):959–967, 2010.
[Zhu et al. 2014] Xiatian Zhu, Chen Change Loy, and Shao-
gang Gong. Constructing robust afﬁnity graphs for spectral
clustering. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 1450–1457,
2014.

Uniﬁed Spectral Clustering with Optimal Graph

Zhao Kang1, Chong Peng2, Qiang Cheng3 and Zenglin Xu1∗
1School of Computer Science and Engineering, University of Electronic Science and Technology of China
2Department of Computer Science, Southern Illinois University, Carbondale, USA
3Institute of Biomedical Informatics and Department of Computer Science, University of Kentucky USA
zkang@uestc.edu.cn, pchong@siu.edu, qiang.cheng@uky.edu, zlxu@uestc.edu.cn

7
1
0
2
 
v
o
N
 
2
1
 
 
]

G
L
.
s
c
[
 
 
1
v
8
5
2
4
0
.
1
1
7
1
:
v
i
X
r
a

Abstract

Spectral clustering has found extensive use in many areas.
Most traditional spectral clustering algorithms work in three
separate steps: similarity graph construction; continuous la-
bels learning; discretizing the learned labels by k-means clus-
tering. Such common practice has two potential ﬂaws, which
may lead to severe information loss and performance degra-
dation. First, predeﬁned similarity graph might not be optimal
for subsequent clustering. It is well-accepted that similarity
graph highly affects the clustering results. To this end, we
propose to automatically learn similarity information from
data and simultaneously consider the constraint that the sim-
ilarity matrix has exact c connected components if there are
c clusters. Second, the discrete solution may deviate from the
spectral solution since k-means method is well-known as sen-
sitive to the initialization of cluster centers. In this work, we
transform the candidate solution into a new one that better
approximates the discrete one. Finally, those three subtasks
are integrated into a uniﬁed framework, with each subtask it-
eratively boosted by using the results of the others towards
an overall optimal solution. It is known that the performance
of a kernel method is largely determined by the choice of ker-
nels. To tackle this practical problem of how to select the most
suitable kernel for a particular data set, we further extend our
model to incorporate multiple kernel learning ability. Exten-
sive experiments demonstrate the superiority of our proposed
method as compared to existing clustering approaches.

Introduction
Clustering is a fundamental technique in machine learning,
pattern recognition, and data mining (Huang et al. 2017). In
past decades, a variety of clustering algorithms have been
developed, such as k-means clustering and spectral cluster-
ing.

With the beneﬁts of simplicity and effectiveness, k-means
clustering algorithm is often adopted in various real-world
problems. To deal with the nonlinear structure of many prac-
tical data sets, kernel k-means (KKM) algorithm has been
developed (Sch¨olkopf et al. 1998), where data points are
mapped through a nonlinear transformation into a higher di-
mensional feature space in which the data points are linearly
separable. KKM usually achieves better performance than
the standard k-means. To cope with noise and outliers, ro-
bust kernel k-means (RKKM) (Du et al. 2015) algorithm

∗Corresponding author.

has been proposed. In this approach, the squared (cid:96)2 norm
of error construction term is replaced by (cid:96)2,1 norm. RKKM
demonstrates superior performance on a number of bench-
mark data sets. The performance of such model-based meth-
ods heavily depends on whether the data ﬁt the model. Un-
fortunately, in most cases, we do not know the distribution
of data in advance. To some extent, this problem is alleviated
by multiple kernel learning. Moreover, there is no theoretical
result on how to choose the similarity graph (Von Luxburg
2007).

Spectral clustering is another widely used clustering
method (Kumar et al. 2011). It enjoys the advantage of ex-
ploring the intrinsic data structures by exploiting the differ-
ent similarity graphs of data points (Yang et al. 2015). There
are three kinds of similarity graph constructing strategies:
k-nearest-neighborhood (knn); (cid:15)-nearest-neighborhood; The
fully connected graph. Here, some open issues arise (Huang
et al. 2015): 1) how to choose a proper neighbor number k or
radius (cid:15); 2) how to select an appropriate similarity metric to
measure the similarity among data points; 3) how to counter-
act the adverse effect of noise and outliers; 4) how to tackle
data with structures at different scales of size and density.
Unfortunately, all of these issues heavily inﬂuence the clus-
tering results (Zelnik-Manor and Perona 2004). Nowadays,
many data are often high dimensional, heterogeneous, and
without prior knowledge, and it is therefore a fundamental
challenge to deﬁne a pairwise similarity graph for effective
spectral clustering.

Recently, (Zhu et al. 2014) construct robust afﬁnity
graphs for spectral clustering by identifying discrimina-
tive features. It adopts a random forest approach based on
the motivation that tree leaf nodes contain discriminative
data partitions, which can be exploited to capture subtle
and weak data afﬁnity. This approach shows better per-
formance than other state-of-the-art methods including the
Euclidean-distance-based knn (Wang et al. 2008), dominant
neighbourhoods (Pavan and Pelillo 2007), consensus of knn
(Premachandran and Kakarala 2013), and non-metric based
unsupervised manifold forests (Pei et al. 2013).

The second step of spectral clustering is to use the spec-
trum of the similarity graph to reveal the cluster structure
of the data. Due to the discrete constraint on the cluster la-
bels, this problem is NP-hard. To obtain a feasible approxi-
mation solution, spectral clustering solves a relaxed version

of this problem, i.e., the discrete constraint is relaxed to al-
low continuous values. It ﬁrst performs eigenvalue decom-
position on the Laplacian matrix to generate an approximate
indicator matrix with continuous values. Then, k-means is
often implemented to produce ﬁnal clustering labels (Huang
et al. 2013). Although this approach has been widely used
in practice, it may exhibit poor performance since the k-
means method is well-known as sensitive to the initialization
of cluster centers (Ng et al. 2002).

To address the aforementioned problems, in this paper, we
propose a uniﬁed spectral clustering framework. It jointly
learns the similarity graph from the data and the discrete
clustering labels by solving an optimization problem, in
which the continuous clustering labels just serve as interme-
diate products. To the best of our knowledge, this is the ﬁrst
work that combine the three steps into a single optimization
problem. As we show later, it is not trivial to unify them.
The contributions of our work are as follows:
1. Rather than using predeﬁned similarity metrics, the simi-
larity graph is adaptively learned from the data in a kernel
space. By combining similarity learning with subsequentl
clustering into a uniﬁed framework, we can ensure the op-
timality of the learned similarity graph.

2. Unlike existing spectral clustering methods that work in
three separate steps, we simultaneously learn similarity
graph, continuous labels, and discrete cluster labels. By
leveraging the inherent interactions between these three
subtasks, they can be boosted by each other.

3. Based on our single kernel model, we further extend it to
have the ability to learn the optimal combination of mul-
tiple kernels.
Notations. Given a data set [x1, x2, · · · , xn], we denote
X ∈ Rm×n with m features and n samples. Then the i-
th sample and (i, j)-th element of matrix X are denoted by
xi ∈ Rm×1 and xij, respectively. The (cid:96)2-norm of a vector x
is deﬁned as (cid:107)x(cid:107)2 = x(cid:62) · x, where (cid:62) means transpose. The
squared Frobenius norm is denoted by (cid:107)X(cid:107)2
ij x2
ij.
The (cid:96)1-norm of matrix X is deﬁned as the absolute summa-
tion of its entries, i.e., (cid:107)X(cid:107)1 = (cid:80)
j |xij|. I denotes the
identity matrix. Tr(·) is the trace operator. Z ≥ 0 means all
the elements of Z are nonnegative.

F = (cid:80)

(cid:80)

i

Preliminary Knowledge

Sparse Representation
Recently, sparse representation, which assumes that each
data point can be reconstructed as a linear combination of
the other data points, has shown its power in many tasks
(Cheng et al. 2010; Peng et al. 2016). It often solves the
following problem:
(cid:107)X − XZ(cid:107)2

F + α(cid:107)Z(cid:107)1, s.t. Z ≥ 0, diag(Z) = 0,
(1)
where α > 0 is a balancing parameter. Eq. (1) simultane-
ously determines both the neighboring samples of a data
point and the corresponding weights by the sparse recon-
struction from the remaining samples. In principle, more
similar points should receive bigger weights and the weights

min
Z

should be smaller for less similar points. Thus Z is also
called similarity graph matrix (Kang et al. 2015). In addi-
tion, sparse representation enjoys some nice properties, e.g.,
the robustness to noise and datum-adaptive ability (Huang et
al. 2015). On the other hand, model (1) has a drawback, i.e.,
it does not consider nonlinear data sets where data points
reside in a union of manifolds (Kang et al. 2017a).

Spectral Clustering
Spectral clustering requires Laplacian matrix L ∈ Rn×n as
an input, which is computed as L = D − Z(cid:62)+Z
, where
D ∈ Rn×n is a diagonal matrix with the i-th diagonal ele-
ment (cid:80)
. In traditional spectral clustering methods,
j
similarity graph Z ∈ Rn×n is often constructed in one of
the three ways aforementioned. Supposing there are c clus-
ters in the data X, spectral clustering solves the following
problem:

zij +zij
2

2

T r(F (cid:62)LF ),

s.t. F ∈ Idx,

(2)

min
F

where F = [f1, f2, · · · , fn](cid:62) ∈ Rn×c is the cluster indica-
tor matrix and F ∈ Idx represents the clustering label vec-
tor of each point fi ∈ {0, 1}c×1 contains one and only one
element “1” to indicate the group membership of xi. Due
to the discrete constraint on F , problem (2) is NP-hard. In
practice, F is relaxed to allow continuous values and solve

T r(P (cid:62)LP ),

s.t. P (cid:62)P = I,

(3)

min
P

where P ∈ Rn×c is the relaxed continuous clustering la-
bel matrix, and the orthogonal constraint is adopted to avoid
trivial solutions. The optimal solution is obtained from the c
eigenvectors of L corresponding to the c smallest eigenval-
ues. After obtaining F , traditional clustering method, e.g.,
k-means, is implemented to obtain discrete cluster labels
(Huang et al. 2013).

Although this three-steps approach provides a feasible so-
lution, it comes with two potential risks. First, since the
similarity graph computation is independent of the subse-
quent steps, it may be far from optimal. As we discussed
before, the clustering performance is largely determined by
the similarity graph. Thus, ﬁnal results may be degraded.
Second, the ﬁnal solution may unpredictably deviate from
the ground-truth discrete labels (Yang et al. 2016). To ad-
dress these problems, we propose a uniﬁed spectral cluster-
ing model.

Spectral Clustering with Single Kernel

Model
One drawback of Eq. (1) is that it assumes that all the points
lie in a union of independent or disjoint subspaces and are
noiseless. In the presence of dependent subspaces, nonlinear
manifolds and/or data errors, it may select points from dif-
ferent structures to represent a data point and makes the rep-
resentation less informative (Elhamifar and Vidal 2009). It is
recognized that nonlinear data may represent linearity when
mapped to an implicit, higher-dimensional space via a ker-
nel function. To fully exploit data information, we formulate
Eq. (1) in a general manner with a kernelization framework.

Let φ : RD → H be a kernel mapping the data samples
from the input space to a reproducing kernel Hilbert space
R. Then X is transformed to φ(X) = [φ(x1), · · · , φ(xn)].
The kernel similarity between data samples xi and xj
is deﬁned through a predeﬁned kernel as Kxi,xj =<
φ(xi), φ(xj) >. By applying this kernel trick, we do not
need to know the transformation φ. In the new space, Eq. (1)
becomes (Zhang et al. 2010)

min
Z

(cid:107)φ(X) − φ(X)Z(cid:107)2

F + α(cid:107)Z(cid:107)1,

⇐⇒ min

T r(φ(X)T φ(X) − φ(X)T φ(X)Z

Z
− Z T φ(X)T φ(X) + Z T φ(X)T φ(X)Z) + α(cid:107)Z(cid:107)1,

⇐⇒ min

T r(K − 2KZ + Z (cid:62)KZ) + α(cid:107)Z(cid:107)1,

Z

s.t. Z ≥ 0,

diag(Z) = 0,

This model recovers the linear relations among the data in
the new space, and thus the nonlinear relations in the origi-
nal representation. Eq. (4) is more general than Eq. (1) and
is supposed to learn arbitrarily shaped data structure. More-
over, Eq. (4) goes back to Eq. (1) when a linear kernel is
applied.

To fulﬁll the clustering task, we propose our spectral clus-

min
Z,F,P,Q

tering with single kernel (SCSK) model as following:
T r(K − 2KZ + Z (cid:62)KZ) + α(cid:107)Z(cid:107)1
(cid:124)
(cid:123)(cid:122)
(cid:125)
similarity learning
+ γ(cid:107)F − P Q(cid:107)2
F
(cid:124)
(cid:125)
(cid:123)(cid:122)
discrete label learning

+ βT r(P (cid:62)LP )
(cid:125)

(cid:123)(cid:122)
continuous label learning

(cid:124)

,

(5)

s.t. Z ≥ 0,

diag(Z) = 0,

P (cid:62)P = I, Q(cid:62)Q = I, F ∈ Idx,

where α, β, and γ are penalty parameters, and Q is a rota-
tion matrix. Due to the spectral solution invariance property
(Yu and Shi 2003), for any solution P , P Q is another so-
lution. The purpose of the last term is to ﬁnd a proper or-
thonormal Q such that the resulting P Q is close to the real
discrete clustering labels. In Eq. (5), the similarity graph and
the ﬁnal discrete clustering labels are automatically learned
from the data. Ideally, whenever data points i and j belong
to different clusters, we must have zij = 0 and it is also true
vice versa. That is to say, we have zij (cid:54)= 0 if and only if
data points i and j are in the same cluster, or, equivalently
fi = fj. Therefore, our uniﬁed framework Eq. (5) can ex-
ploit the correlation between the similarity matrix and the
labels. Because of the feedback of inferred labels to induce
the similarity matrix and vice versa, we say that our cluster-
ing framework has a self-taught property.

In fact, Eq. (5) is not a simple uniﬁcation of the pipeline
of steps. It learns a similarity graph with optimal structure
for clustering. Ideally, Z should have exactly c connected
components if there are c clusters in the data set (Kang et
al. 2017b). This is to say that the Laplacian matrix L has
c zero eigenvalues (Mohar et al. 1991), i.e., the summation
of the smallest c eigenvalues is zero. To ensure the optimal-
ity of the similarity graph, we can minimize (cid:80)c
i=1 σi(L).

i=1 σi(L) =
T r(P (cid:62)LP ). Therefore, the spectral clustering term,

According to Ky Fan’s theorem (Fan 1949), (cid:80)c
min
P (cid:62)P =I
i.e., the second term in Eq. (5), will ensure learned Z is op-
timal for clustering.

Optimization
To efﬁciently and effectively solve Eq. (5), we design an al-
ternated iterative method.
Computation of Z: With F , P , Q ﬁxed, the problem is re-
duced to

T r(K − 2KZ + Z (cid:62)KZ) + α(cid:107)Z(cid:107)1 + βT r(P (cid:62)LP ),

min
Z

s.t. Z ≥ 0,

diag(Z) = 0.

(4)

We introduce an auxiliary variable S to make above objec-
tive function separable and solve the following equivalent
problem:

T r(K − 2KZ + Z (cid:62)KZ) + α(cid:107)S(cid:107)1 + βT r(P (cid:62)LP ),

min
Z

s.t. Z ≥ 0,

diag(Z) = 0, S = Z.

(6)

(7)

This can be solved by using the augmented Lagrange mul-
tiplier (ALM) type of method. We turn to minimizing the
following augmented Lagrangian function:

L(S, Z, Y ) =T r(K − 2KZ + Z (cid:62)KZ) + α(cid:107)S(cid:107)1

+ βT r(P (cid:62)LP ) +

(cid:107)S − Z +

µ
2

(8)

Y
µ

(cid:107)2
F ,

where µ > 0 is the penalty parameter and Y is the Lagrange
multiplier. This problem can be minimized with respect to
S, Z, and Y alternatively, by ﬁxing the other variables.

For S, by letting H = Z − Y

µ , it can be updated element-

wisely as below

Sij = max(|Hij| − α/µ, 0) · sign(Hij).

(9)

For Z, by letting E = S + Y
wisely as:

µ , it can be updated column-

Z T
i (

min
Zi

µ
2

β
2

I + K)Zi + (

i − µET
dT

i − 2Ki,:)Zi,

(10)

where di ∈ Rn×1 is a vector with the j-th element dij being
dij = (cid:107)Pi,: − Pj,:(cid:107)2. It is easy to obtain Zi by setting the
derivative of Eq. (10) w.r.t. Zi to be zero.
Computation of P: With F , Z, and Q ﬁxed, it is equivalent
to solving

min
P

βT r(P (cid:62)LP ) + γ(cid:107)F − P Q(cid:107)2

F s.t. P (cid:62)P = I.

(11)

The above problem with orthogonal constraint can be efﬁ-
ciently solved by the algorithm proposed by Wen and Yin
(Wen and Yin 2013).
Computation of Q: With F , Z, and P ﬁxed, we have

min
Q

(cid:107)F − P Q(cid:107)2
F

s.t. Q(cid:62)Q = I.

(12)

Algorithm 1 The algorithm of SCSK
Input: Kernel matrix K, parameters α > 0, β > 0, γ > 0, µ > 0.
Initialize: Random matrices Z, P , and Q. Y = 0 and F = 0.
REPEAT
1: Update S according to Eq. (9).
2: S = S − diag(diag(S)) and S = max(S, 0).
3: Update Z according to Eq. (10).
4: Y = Y + µ(S − Z).
5: Update P by solving the problem of Eq. (11).
6: Update Q according to Eq. (13).
7: Update F according to Eq. (16).
UNTIL stopping criterion is met.

It is the orthogonal Procrustes problem (Sch¨onemann 1966),
which admits a closed-form solution. The solution is

Q = U V (cid:62),

(13)

where U and V are left and right parts of the SVD decom-
position of F (cid:62)P .
Computation of F: With Z, P and Q ﬁxed, the problem
becomes

min
F

(cid:107)F − P Q(cid:107)2
F ,

s.t. F ∈ Idx.

(14)

Note that T r(F (cid:62)F ) = n, the above subproblem can be
rewritten as below:

T r(F (cid:62)P Q)

s.t. F ∈ Idx.

(15)

max
F

The optimal solution can be easily obtained as follows:

(cid:40)1,

j = argmax

(P Q)ik

Fij =

k
0, otherwise

(16)

The updates of Z, P , F , and Q are coupled with each
other, so we could reach an overall optimal solution. The
details of our SCSK optimization are summarized in Algo-
rithm 1.

Complexity Analysis
With our optimization strategy, the updating of S requires
O(n2) complexity. The quadratic program can be solved in
polynomial time. The solution of Q involves SVD and its
complexity is O(nc2 + c3). To update P , we need O(nc2 +
c3). The complexity for F is O(nc2). Note that the number
of clusters c is often a small number. Therefore, the main
computation load is from solving Z, which involves matrix
inversion. Fortunately, Z is solved in parallel.

Spectral Clustering with Multiple Kernels

Model
Although the model in Eq. (5) can automatically learn the
similarity graph matrix and discrete cluster labels, its per-
formance will strongly depend on the choice of kernels. It
is often impractical to exhaustively search for the most suit-
able kernel. Moreover, real world data sets are often gener-
ated from different sources along with heterogeneous fea-
tures. Single kernel method may not be able to fully utilize

such information. Multiple kernel learning has the ability to
integrate complementary information and identify a suitable
kernel for a given task. Here we present a way to learn an
appropriate consensus kernel from a convex combination of
a number of predeﬁned kernel functions.

functions {K i}r

Suppose there are a total number of r different ker-
i=1. An augmented Hilbert space
nel
can be constructed by using the mapping of ˜φ(x) =
√
√
wrφr(x)](cid:62) with different
[
wi(wi ≥ 0). Then the combined kernel Kw can
weights
be represented as (Zeng and Cheung 2011)

w2φ2(x), ...,

w1φ1(x),

√

√

Kw(x, y) =< φw(x), φw(y) >=

wiK i(x, y).

(17)

r
(cid:88)

i=1

Note that the convex combination of the positive semi-
deﬁnite kernel matrices {K i}r
i=1 is still a positive semi-
deﬁnite kernel matrix. Thus the combined kernel still satis-
ﬁes Mercer’s condition. Then our proposed method of spec-
tral clustering with multiple kernels (SCMK) can be formu-
lated as
min
Z,F,P,Q,w

T r(Kw − 2KwZ + Z (cid:62)KwZ) + α(cid:107)Z(cid:107)1+

βT r(P (cid:62)LP ) + γ(cid:107)F − P Q(cid:107)2
F ,
diag(Z) = 0,
s.t. Z ≥ 0,

P (cid:62)P = I, Q(cid:62)Q = I, F ∈ Idx,

Kw =

wiK i,

wi = 1, wi ≥ 0.

r
(cid:88)

i=1

r
(cid:88)

√

i=1

(18)

Now above model will learn the similarity graph, discrete
clustering labels, and kernel weights by itself. By iteratively
updating Z, F , and w, each of them will be iteratively re-
ﬁned according to the results of the others.

Optimization
In this part, we show an efﬁcient and effective algorithm to
iteratively and alternatively solve Eq. (18).

w is ﬁxed: Update other variables when w is ﬁxed: We
can directly calculate Kw, and the optimization problem is
exactly Eq. (5). Thus we just need to use Algorithm 1 with
Kw as the input kernel matrix.

Update w: Optimize with respect to w when other vari-
ables are ﬁxed: Solving Eq. (18) with respect to w can be
rewritten as (Cai et al. 2013)

r
(cid:88)

wihi

i=1
r
(cid:88)

√

min
w

s.t.

i=1

wi = 1, wi ≥ 0,

(19)

where

hi = T r(K i − 2K iZ + Z (cid:62)K iZ).

(20)

The Lagrange function of Eq. (19) is

J (w) = w(cid:62)h + γ(1 −

(21)

r
(cid:88)

√

i=1

wi).

i=1, parameters α > 0,

Algorithm 2 The algorithm of SCMK
Input: A set of kernel matrix {K i}r
β > 0, γ > 0, µ > 0.
Initialize: Random matrices Z, P , and Q. Y = 0 and F =
0. wi = 1/r.
REPEAT
1: Calculate Kw by Eq. (17).
2: Do steps 1-7 in Algorithm 1.
3: Calculate h by Eq. (20).
4: Calculate w by Eq. (22).
UNTIL stopping criterion is met.

By utilizing the Karush-Kuhn-Tucker (KKT) condition with
∂J (w)
∂wi

wi = 1, we obtain the

= 0 and the constraint

√

r
(cid:80)
i=1

solution of w as follows:



wi =

hi



−2



.

r
(cid:88)

j=1

1
hj

(22)

We can see that w is closely related to Z. Therefore, we
could obtain both optimal similarity matrix Z and kernel
weight w. We summarize the optimization process of Eq.
(18) in Algorithm 2.

Experiments

Table 1: Description of the data sets

YALE
JAFFE
ORL
AR
COIL20
BA
TR11
TR41
TR45
TDT2

# instances
165
213
400
840
1440
1404
414
878
690
9394

# features
1024
676
1024
768
1024
320
6429
7454
8261
36771

# classes
15
10
40
120
20
36
9
10
10
30

Data Sets
There are altogether ten real benchmark data sets used in our
experiments. Table 1 summarizes the statistics of these data
sets. Among them, the ﬁrst six are image data, and the other
four are text corpora12.

The six image data sets consist of four famous face
databases (ORL3, YALE4, AR5 and JAFFE6), a toy im-

1http://www-users.cs.umn.edu/ han/data/tmdata.tar.gz
2http://www.cad.zju.edu.cn/home/dengcai/Data/TextData.html
3http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html
4http://vision.ucsd.edu/content/yale-face-database
5http://www2.ece.ohio-state.edu/ aleix/ARdatabase.html
6http://www.kasrl.org/jaffe.html

20.php

age database COIL207, and a binary alpha digits data set
BA8. Speciﬁcally, COIL20 contains images of 20 objects.
For each object, the images were taken ﬁve degrees apart
as the object is rotating on a turntable. There are 72 im-
ages for each object. Each image is represented by a 1,024-
dimensional vector. BA consists of digits of “0” through “9”
and letters of capital “A” through “Z”. There are 39 exam-
ples for each class. YALE, ORL, AR, and JAFEE contain
images of individuals. Each image has different facial ex-
pressions or conﬁgurations due to times, illumination condi-
tions, and glasses/no glasses.

Kernel Design
To assess the effectiveness of multiple kernel learning, we
adopted 12 kernels. They include: seven Gaussian kernels
of the form K(x, y) = exp(−(cid:107)x − y(cid:107)2
max)), where
dmax is the maximal distance between samples and t varies
over the set {0.01, 0.0, 0.1, 1, 10, 50, 100}; a linear kernel
K(x, y) = x(cid:62)y; four polynomial kernels K(x, y) = (a +
x(cid:62)y)b with a ∈ {0, 1} and b ∈ {2, 4}. Furthermore, all
kernels are rescaled to [0, 1] by dividing each element by the
largest pair-wise squared distance.

2/(td2

Comparison Algorithms
For single kernel methods, we run downloaded kernel k-
means (KKM) (Sch¨olkopf et al. 1998), spectral clustering
(SC) (Ng et al. 2002), robust kernel k-means (RKKM) (Du
et al. 2015), and SCSK on each kernel separately. To demon-
strate the advantage of our uniﬁed framework, we also im-
plement three separate steps method (TSEP), i.e., learn the
similarity matrix by (4), spectral clustering, k-means (repeat
20 times). And we report both the best and the average re-
sults over all these kernels.

In addition, we also implement the recent simplex sparse
representation (SSR) (Huang et al. 2015) method and robust
afﬁnity graph construction methods by using random for-
est approach: ClustRF-u and ClustRF-a (Zhu et al. 2014).
ClustRF-u assumes all tree nodes are uniformly important,
while ClustRF-a assigns an adaptive weight to each node.
Note that these three methods can only process data in the
original feature space. Moreover, ClusteRF has a high de-
mand for memory and cannot process high dimensional data
directly. Thus we follow the authors’ strategy and perform
PCA on TR11, TR41, and TR45 to reduce the dimension.
We use different numbers of dominant components and re-
port the best clustering results. Nevertheless, we still cannot
handle TDT2 data set with them.

For multiple kernel methods, we implement our proposed
method and directly use the downloaded programs for the
methods in comparison on a combination of these 12 ker-
nels:

MKKM9. The MKKM (Huang et al. 2012b) extends k-
means in a multiple kernel setting. However, it imposes a
different constraint on the kernel weight distribution.

7http://www.cs.columbia.edu/CAVE/software/softlib/coil-

8http://www.cs.nyu.edu/ roweis/data.html
9http://imp.iis.sinica.edu.tw/IVCLab/research/Sean/mkfc/code

KKMKKM-m SC SC-mRKKMRKKM-mClustRF-uClustRF-a SSR TSEPTSEP-mSCSKSCSK-m MKKMAASCRMKKMSCMK

(a) Accuracy(%)

KKMKKM-m SC SC-mRKKMRKKM-mClustRF-uClustRF-a SSR TSEPTSEP-mSCSKSCSK-m MKKMAASCRMKKMSCMK

Data
YALE 47.12 38.97 49.4240.52 48.09
JAFFE 74.39 67.09 74.8854.03 75.61
ORL

53.53 45.93 57.9646.65 54.96

AR

33.02 30.89 28.8322.22 33.43
COIL2059.49 50.74 67.7043.65 61.64
BA

41.20 33.66 31.0726.25 42.17

TR11

TR41

51.91 44.65 50.9843.32 53.03

55.64 46.34 63.5244.80 56.76

TR45

58.79 45.58 57.3945.96 58.13
TDT2 47.05 35.58 52.6345.26 48.35

Data
YALE 51.34 42.07 52.9244.79 52.29
JAFFE 80.13 71.48 82.0859.35 83.47
ORL

73.43 63.36 75.1666.74 74.23

AR

65.21 60.64 58.3756.05 65.44
COIL2074.05 63.57 80.9854.34 74.63
BA

57.25 46.49 50.7640.09 57.82

TR11

TR41

48.88 33.22 43.1131.39 49.69

59.88 40.37 61.3336.60 60.77

TR45

57.87 38.69 48.0333.22 57.86
TDT2 55.28 38.47 52.2327.16 54.46

Data
YALE 49.15 41.12 51.6143.06 49.79
JAFFE 77.32 70.13 76.8356.56 79.58
ORL

58.03 50.42 61.4551.20 59.60

AR

35.52 33.64 33.2425.99 35.87
COIL2064.61 55.30 69.9246.83 66.35
BA

44.20 36.06 34.5029.07 45.28

TR11

TR41

67.57 56.32 58.7950.23 67.93

74.46 60.00 73.6856.45 74.99

TR45

68.49 53.64 61.2550.02 68.18
TDT2 52.79 49.26 50.3942.81 62.13

39.71

67.98

46.88

31.20

51.89

34.35

45.04

46.80

45.69

36.67

42.87

74.01

63.91

60.81

63.70

46.91

33.48

40.86

38.96

42.19

41.74

71.82

51.46

33.88

56.34

36.86

56.40

60.21

53.75

52.60

57.58

97.65

60.75

24.17

74.44

39.89

29.24

53.19

42.17

-

58.76

97.00

78.69

57.09

83.91

54.66

18.97

52.63

38.12

-

63.64

97.65

67.25

40.71

80.83

41.95

35.75

55.58

45.51

-

57.58 54.5562.58 44.60 63.05 52.88

45.70 40.64

52.18

63.25

98.59 87.3298.30 73.88 99.53 90.06

74.55 30.35

87.07

99.69

62.75 69.0070.15 41.45 74.05 53.56

47.51 27.20

55.60

74.52

35.59 65.0065.03 46.41 78.90 68.21

28.61 33.23

34.37

79.29

72.99 76.3277.68 61.03 81.48 62.59

54.82 34.87

66.65

82.21

44.01 23.9745.92 30.75 46.02 31.50

40.52 27.07

43.42

45.57

34.54 41.0671.05 42.08 74.22 55.09

50.13 47.15

57.71

74.26

60.93 63.7869.45 50.17 70.17 53.05

56.10 45.90

62.65

70.25

48.41 71.4576.54 51.07 77.74 59.53

58.46 52.64

64.00

77.47

-

20.8654.78 46.35 56.04 45.02

34.36 19.82

37.57

56.29

(b) NMI(%)

60.25 57.2660.13 46.10 60.58 52.72

50.06 46.83

55.58

61.04

98.16 92.9398.61 71.95 99.18 88.86

79.79 27.22

89.37

99.20

79.87 84.2383.28 50.76 84.78 70.93

68.86 43.77

74.83

85.21

66.64 84.1684.69 64.63 89.61 80.34

59.17 65.06

65.49

89.93

82.26 86.8984.16 71.36 87.03 72.41

70.64 41.87

77.34

86.72

58.17 30.2959.47 32.45 60.34 42.91

56.88 42.34

58.47

60.55

24.77 27.6062.71 29.88 64.60 44.48

44.56 39.39

56.08

64.89

56.78 59.5664.07 39.58 64.92 47.97

57.75 43.05

63.47

64.89

43.70 67.8270.03 40.17 70.75 50.47

56.17 41.94

62.73

70.79

-

02.4457.74 45.38 59.25 48.73

41.36 02.14

47.13

58.66

(c) Purity(%)

63.03 58.1864.77 55.38 65.87 56.19

47.52 42.33

53.64

67.39

98.59 96.2499.06 77.08 99.23 91.24

76.83 33.08

88.90

99.51

66.00 76.5076.00 52.39 77.02 57.96

52.85 31.56

60.23

78.31

46.79 69.5272.44 57.25 83.08 70.69

30.46 34.98

36.78

83.20

77.71 89.0384.03 74.89 84.24 75.58

58.95 39.14

69.95

83.78

49.14 40.8555.03 43.07 55.49 40.45

43.47 30.29

46.27

55.72

49.76 85.0285.95 63.15 86.25 63.36

65.48 54.67

72.93

85.84

65.60 75.4077.02 56.33 78.53 57.19

72.83 62.05

77.57

78.49

57.83 83.6277.28 60.52 79.70 61.06

69.14 57.49

75.20

79.78

-

46.7967.75 56.07 70.69 64.53

54.89 21.73

60.02

72.84

KKMKKM-m SC SC-mRKKMRKKM-mClustRF-uClustRF-a SSR TSEPTSEP-mSCSKSCSK-m MKKMAASCRMKKMSCMK

Table 2: Clustering results obtained on benchmark data sets. ’-m’ denotes the average performance on the 12 kernels. Both the
best results for single kernel and multiple kernel methods are highlighted in boldface.

AASC10. The AASC (Huang et al. 2012a) is an extension
of spectral clustering to the situation when multiple afﬁnities
exist. It is different from our approach since our method tries

to learn an optimal similarity graph.

RMKKM11. The RMKKM (Du et al. 2015) extends k-
means to deal with noise and outliers in a multiple kernel

10http://imp.iis.sinica.edu.tw/IVCLab/research/Sean/aasc/code

11https://github.com/csliangdu/RMKKM

(a) γ = 10−5

(b) γ = 10−4

(c) γ = 10−3

Figure 1: Parameter inﬂuence on accuracy for YALE data set.

setting.

SCMK. Our proposed method of spectral clustering with
multiple kernels. For the purpose of reproducibility, the code
is publicly available12.

For our method, we only need to run once. For those
methods that involve K-means, we follow the strategy sug-
gested in (Yang et al. 2010); i.e., we repeat clustering 20
times and present the results with the best objective values.
We set the number of clusters to the true number of classes
for all clustering algorithms.

Results
We present the clustering results of different methods on
those benchmark data sets in Table 2. In terms of accuracy,
NMI and Purity, our proposed methods obtain superior re-
sults. The big difference between the best and average re-
sults conﬁrms that the choice of kernels has a huge inﬂuence
on the performance of single kernel methods. This motivates
our extended model for multiple kernel learning. Besides,
our extended model for multiple kernel clustering usually
improves the results over our model for single kernel clus-
tering.

Although the best results of the three separate steps ap-
proach are sometimes close to our proposed uniﬁed method,
their average values are often lower than our method. We no-
tice that random forest based afﬁnity graph method achieves
good performance on image data sets. This observation can
be explained by the fact that ClustRF is suitable to handle
ambiguous and unreliable features caused by variation in il-
lumination, face expression or pose on those data sets. On
the other hand, it is not effective for text data sets. In most
cases, ClustRF-a behaves better than ClustRF-u. This jus-
tiﬁes the importance of considering neighbourhood-scale-
adaptive weighting on the nodes.

Parameter Sensitivity
There are three parameters in our model: α, β, and γ. We use
YALE data set as an example to demonstrate the sensitivity
of our model SCMK to parameters. As shown in Figure 1,

12https://github.com/sckangz/AAAI18

our model is quite insensitive to α and β, and γ over wide
ranges of values. In terms of NMI and Purity, we have simi-
lar observations.

Conclusion
In this work, we address two problems existing in most clas-
sical spectral clustering algorithms, i.e., constructing simi-
larity graph and relaxing discrete constraints to continuous
one. To alleviate performance degradation, we propose a
uniﬁed spectral clustering framework which automatically
learns the similarity graph and discrete labels from the data.
To cope with complex data, we develop our method in kernel
space. A multiple kernel approach is proposed to solve ker-
nel dependent issue. Extensive experiments on nine real data
sets demonstrated the promising performance of our meth-
ods as compared to existing clustering approaches.

Acknowledgments
This paper was in part supported by Grants from the Natural
Science Foundation of China (No. 61572111), the National
High Technology Research and Development Program of
China (863 Program) (No. 2015AA015408), a 985 Project
of UESTC (No.A1098531023601041) and a Fundamental
Research Fund for the Central Universities of China (No.
A03017023701012).

References
[Cai et al. 2013] Xiao Cai, Feiping Nie, Weidong Cai, and
Heng Huang. Heterogeneous image features integration via
multi-modal semi-supervised learning model. In Proceed-
ings of the IEEE International Conference on Computer Vi-
sion, pages 1737–1744, 2013.
[Cheng et al. 2010] Bin Cheng, Jianchao Yang, Shuicheng
Yan, Yun Fu, and Thomas S Huang. Learning with-graph
for image analysis. IEEE transactions on image processing,
19(4):858–866, 2010.
[Du et al. 2015] Liang Du, Peng Zhou, Lei Shi, Hanmo
Wang, Mingyu Fan, Wenjian Wang, and Yi-Dong Shen. Ro-
bust multiple kernel k-means using 2; 1-norm. In Proceed-

ings of the 24th International Conference on Artiﬁcial Intel-
ligence, pages 3476–3482. AAAI Press, 2015.
[Elhamifar and Vidal 2009] Ehsan Elhamifar and Ren´e Vi-
In Computer Vision and
dal. Sparse subspace clustering.
Pattern Recognition, 2009. CVPR 2009. IEEE Conference
on, pages 2790–2797. IEEE, 2009.
[Fan 1949] Ky Fan. On a theorem of weyl concerning eigen-
values of linear transformations i. Proceedings of the Na-
tional Academy of Sciences of the United States of America,
35(11):652, 1949.
[Huang et al. 2012a] Hsin-Chien Huang, Yung-Yu Chuang,
and Chu-Song Chen. Afﬁnity aggregation for spectral clus-
tering. In Computer Vision and Pattern Recognition (CVPR),
2012 IEEE Conference on, pages 773–780. IEEE, 2012.
[Huang et al. 2012b] Hsin-Chien Huang, Yung-Yu Chuang,
and Chu-Song Chen. Multiple kernel fuzzy clustering. IEEE
Transactions on Fuzzy Systems, 20(1):120–134, 2012.
[Huang et al. 2013] Jin Huang, Feiping Nie, and Heng
Huang. Spectral rotation versus k-means in spectral clus-
tering. In AAAI, 2013.
[Huang et al. 2015] Jin Huang, Feiping Nie, and Heng
Huang. A new simplex sparse learning model to measure
In Proceedings of the 24th
data similarity for clustering.
International Conference on Artiﬁcial Intelligence, pages
3569–3575. AAAI Press, 2015.
[Huang et al. 2017] Shudong Huang, Hongjun Wang, Tao Li,
Tianrui Li, and Zenglin Xu. Robust graph regularized non-
negative matrix factorization for clustering. Data Mining
and Knowledge Discovery, pages 1–21, 2017.
[Kang et al. 2015] Zhao Kang, Chong Peng, and Qiang
Cheng. Robust subspace clustering via smoothed rank ap-
proximation. IEEE Signal Processing Letters, 22(11):2088–
2092, 2015.
[Kang et al. 2017a] Zhao Kang, Chong Peng, and Qiang
Cheng. Kernel-driven similarity learning. Neurocomputing,
2017.
[Kang et al. 2017b] Zhao Kang, Chong Peng, and Qiang
Cheng. Twin learning for similarity and clustering: A uniﬁed
kernel approach. In AAAI, pages 2080–2086, 2017.
[Kumar et al. 2011] Abhishek Kumar, Piyush Rai, and Hal
Daume. Co-regularized multi-view spectral clustering. In
Advances in neural information processing systems, pages
1413–1421, 2011.
[Mohar et al. 1991] Bojan Mohar, Y Alavi, G Chartrand, and
OR Oellermann. The laplacian spectrum of graphs. Graph
theory, combinatorics, and applications, 2(871-898):12,
1991.
[Ng et al. 2002] Andrew Y Ng, Michael I Jordan, Yair Weiss,
et al. On spectral clustering: Analysis and an algorithm.
Advances in neural information processing systems, 2:849–
856, 2002.
[Pavan and Pelillo 2007] Massimiliano Pavan and Marcello
IEEE
Pelillo. Dominant sets and pairwise clustering.
transactions on pattern analysis and machine intelligence,
29(1):167–172, 2007.

[Pei et al. 2013] Yuru Pei, Tae-Kyun Kim, and Hongbin Zha.
Unsupervised random forest manifold alignment for lipread-
ing. In Proceedings of the IEEE International Conference on
Computer Vision, pages 129–136, 2013.
[Peng et al. 2016] Chong Peng, Zhao Kang, Ming Yang, and
Qiang Cheng. Feature selection embedded subspace clus-
tering. IEEE Signal Processing Letters, 23(7):1018–1022,
2016.
[Premachandran and Kakarala 2013] Vittal Premachandran
and Ramakrishna Kakarala. Consensus of k-nns for robust
neighborhood selection on graph-based manifolds. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1594–1601, 2013.
[Sch¨olkopf et al. 1998] Bernhard
Sch¨olkopf, Alexander
Smola, and Klaus-Robert M¨uller.
Nonlinear compo-
nent analysis as a kernel eigenvalue problem. Neural
computation, 10(5):1299–1319, 1998.
[Sch¨onemann 1966] Peter H Sch¨onemann. A generalized so-
lution of the orthogonal procrustes problem. Psychometrika,
31(1):1–10, 1966.
[Von Luxburg 2007] Ulrike Von Luxburg. A tutorial on spec-
tral clustering. Statistics and computing, 17(4):395–416,
2007.
[Wang et al. 2008] Jun Wang, Shih-Fu Chang, Xiaobo Zhou,
and Stephen TC Wong. Active microscopic cellular image
annotation by superposable graph transduction with imbal-
anced labels. In Computer Vision and Pattern Recognition,
2008. CVPR 2008. IEEE Conference on, pages 1–8. IEEE,
2008.
[Wen and Yin 2013] Zaiwen Wen and Wotao Yin. A feasi-
ble method for optimization with orthogonality constraints.
Mathematical Programming, 142(1-2):397–434, 2013.
[Yang et al. 2010] Yi Yang, Dong Xu, Feiping Nie,
Shuicheng Yan, and Yueting Zhuang.
Image cluster-
ing using local discriminant models and global integration.
IEEE Transactions on Image Processing, 19(10):2761–
2773, 2010.
[Yang et al. 2015] Yang Yang, Zhigang Ma, Yi Yang, Feip-
ing Nie, and Heng Tao Shen. Multitask spectral clustering
by exploring intertask correlation. IEEE transactions on cy-
bernetics, 45(5):1083–1094, 2015.
[Yang et al. 2016] Yang Yang, Fumin Shen, Zi Huang, and
Heng Tao Shen. A uniﬁed framework for discrete spectral
clustering. In IJCAI, pages 2273–2279, 2016.
[Yu and Shi 2003] Stella X Yu and Jianbo Shi. Multiclass
spectral clustering. In Computer Vision, 2003. Proceedings.
Ninth IEEE International Conference on, pages 313–319.
IEEE, 2003.
[Zelnik-Manor and Perona 2004] Lihi Zelnik-Manor
Pietro Perona. Self-tuning spectral clustering.
volume 17, page 16, 2004.
[Zeng and Cheung 2011] Hong Zeng and Yiu-ming Cheung.
Feature selection and kernel learning for local learning-
based clustering. IEEE transactions on pattern analysis and
machine intelligence, 33(8):1532–1547, 2011.

and
In NIPS,

[Zhang et al. 2010] Changshui Zhang, Feiping Nie, and
Shiming Xiang. A general kernelization framework for
learning algorithms based on kernel pca. Neurocomputing,
73(4):959–967, 2010.
[Zhu et al. 2014] Xiatian Zhu, Chen Change Loy, and Shao-
gang Gong. Constructing robust afﬁnity graphs for spectral
clustering. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 1450–1457,
2014.

Uniﬁed Spectral Clustering with Optimal Graph

Zhao Kang1, Chong Peng2, Qiang Cheng3 and Zenglin Xu1∗
1School of Computer Science and Engineering, University of Electronic Science and Technology of China
2Department of Computer Science, Southern Illinois University, Carbondale, USA
3Institute of Biomedical Informatics and Department of Computer Science, University of Kentucky USA
zkang@uestc.edu.cn, pchong@siu.edu, qiang.cheng@uky.edu, zlxu@uestc.edu.cn

7
1
0
2
 
v
o
N
 
2
1
 
 
]

G
L
.
s
c
[
 
 
1
v
8
5
2
4
0
.
1
1
7
1
:
v
i
X
r
a

Abstract

Spectral clustering has found extensive use in many areas.
Most traditional spectral clustering algorithms work in three
separate steps: similarity graph construction; continuous la-
bels learning; discretizing the learned labels by k-means clus-
tering. Such common practice has two potential ﬂaws, which
may lead to severe information loss and performance degra-
dation. First, predeﬁned similarity graph might not be optimal
for subsequent clustering. It is well-accepted that similarity
graph highly affects the clustering results. To this end, we
propose to automatically learn similarity information from
data and simultaneously consider the constraint that the sim-
ilarity matrix has exact c connected components if there are
c clusters. Second, the discrete solution may deviate from the
spectral solution since k-means method is well-known as sen-
sitive to the initialization of cluster centers. In this work, we
transform the candidate solution into a new one that better
approximates the discrete one. Finally, those three subtasks
are integrated into a uniﬁed framework, with each subtask it-
eratively boosted by using the results of the others towards
an overall optimal solution. It is known that the performance
of a kernel method is largely determined by the choice of ker-
nels. To tackle this practical problem of how to select the most
suitable kernel for a particular data set, we further extend our
model to incorporate multiple kernel learning ability. Exten-
sive experiments demonstrate the superiority of our proposed
method as compared to existing clustering approaches.

Introduction
Clustering is a fundamental technique in machine learning,
pattern recognition, and data mining (Huang et al. 2017). In
past decades, a variety of clustering algorithms have been
developed, such as k-means clustering and spectral cluster-
ing.

With the beneﬁts of simplicity and effectiveness, k-means
clustering algorithm is often adopted in various real-world
problems. To deal with the nonlinear structure of many prac-
tical data sets, kernel k-means (KKM) algorithm has been
developed (Sch¨olkopf et al. 1998), where data points are
mapped through a nonlinear transformation into a higher di-
mensional feature space in which the data points are linearly
separable. KKM usually achieves better performance than
the standard k-means. To cope with noise and outliers, ro-
bust kernel k-means (RKKM) (Du et al. 2015) algorithm

∗Corresponding author.

has been proposed. In this approach, the squared (cid:96)2 norm
of error construction term is replaced by (cid:96)2,1 norm. RKKM
demonstrates superior performance on a number of bench-
mark data sets. The performance of such model-based meth-
ods heavily depends on whether the data ﬁt the model. Un-
fortunately, in most cases, we do not know the distribution
of data in advance. To some extent, this problem is alleviated
by multiple kernel learning. Moreover, there is no theoretical
result on how to choose the similarity graph (Von Luxburg
2007).

Spectral clustering is another widely used clustering
method (Kumar et al. 2011). It enjoys the advantage of ex-
ploring the intrinsic data structures by exploiting the differ-
ent similarity graphs of data points (Yang et al. 2015). There
are three kinds of similarity graph constructing strategies:
k-nearest-neighborhood (knn); (cid:15)-nearest-neighborhood; The
fully connected graph. Here, some open issues arise (Huang
et al. 2015): 1) how to choose a proper neighbor number k or
radius (cid:15); 2) how to select an appropriate similarity metric to
measure the similarity among data points; 3) how to counter-
act the adverse effect of noise and outliers; 4) how to tackle
data with structures at different scales of size and density.
Unfortunately, all of these issues heavily inﬂuence the clus-
tering results (Zelnik-Manor and Perona 2004). Nowadays,
many data are often high dimensional, heterogeneous, and
without prior knowledge, and it is therefore a fundamental
challenge to deﬁne a pairwise similarity graph for effective
spectral clustering.

Recently, (Zhu et al. 2014) construct robust afﬁnity
graphs for spectral clustering by identifying discrimina-
tive features. It adopts a random forest approach based on
the motivation that tree leaf nodes contain discriminative
data partitions, which can be exploited to capture subtle
and weak data afﬁnity. This approach shows better per-
formance than other state-of-the-art methods including the
Euclidean-distance-based knn (Wang et al. 2008), dominant
neighbourhoods (Pavan and Pelillo 2007), consensus of knn
(Premachandran and Kakarala 2013), and non-metric based
unsupervised manifold forests (Pei et al. 2013).

The second step of spectral clustering is to use the spec-
trum of the similarity graph to reveal the cluster structure
of the data. Due to the discrete constraint on the cluster la-
bels, this problem is NP-hard. To obtain a feasible approxi-
mation solution, spectral clustering solves a relaxed version

of this problem, i.e., the discrete constraint is relaxed to al-
low continuous values. It ﬁrst performs eigenvalue decom-
position on the Laplacian matrix to generate an approximate
indicator matrix with continuous values. Then, k-means is
often implemented to produce ﬁnal clustering labels (Huang
et al. 2013). Although this approach has been widely used
in practice, it may exhibit poor performance since the k-
means method is well-known as sensitive to the initialization
of cluster centers (Ng et al. 2002).

To address the aforementioned problems, in this paper, we
propose a uniﬁed spectral clustering framework. It jointly
learns the similarity graph from the data and the discrete
clustering labels by solving an optimization problem, in
which the continuous clustering labels just serve as interme-
diate products. To the best of our knowledge, this is the ﬁrst
work that combine the three steps into a single optimization
problem. As we show later, it is not trivial to unify them.
The contributions of our work are as follows:
1. Rather than using predeﬁned similarity metrics, the simi-
larity graph is adaptively learned from the data in a kernel
space. By combining similarity learning with subsequentl
clustering into a uniﬁed framework, we can ensure the op-
timality of the learned similarity graph.

2. Unlike existing spectral clustering methods that work in
three separate steps, we simultaneously learn similarity
graph, continuous labels, and discrete cluster labels. By
leveraging the inherent interactions between these three
subtasks, they can be boosted by each other.

3. Based on our single kernel model, we further extend it to
have the ability to learn the optimal combination of mul-
tiple kernels.
Notations. Given a data set [x1, x2, · · · , xn], we denote
X ∈ Rm×n with m features and n samples. Then the i-
th sample and (i, j)-th element of matrix X are denoted by
xi ∈ Rm×1 and xij, respectively. The (cid:96)2-norm of a vector x
is deﬁned as (cid:107)x(cid:107)2 = x(cid:62) · x, where (cid:62) means transpose. The
squared Frobenius norm is denoted by (cid:107)X(cid:107)2
ij x2
ij.
The (cid:96)1-norm of matrix X is deﬁned as the absolute summa-
tion of its entries, i.e., (cid:107)X(cid:107)1 = (cid:80)
j |xij|. I denotes the
identity matrix. Tr(·) is the trace operator. Z ≥ 0 means all
the elements of Z are nonnegative.

F = (cid:80)

(cid:80)

i

Preliminary Knowledge

Sparse Representation
Recently, sparse representation, which assumes that each
data point can be reconstructed as a linear combination of
the other data points, has shown its power in many tasks
(Cheng et al. 2010; Peng et al. 2016). It often solves the
following problem:
(cid:107)X − XZ(cid:107)2

F + α(cid:107)Z(cid:107)1, s.t. Z ≥ 0, diag(Z) = 0,
(1)
where α > 0 is a balancing parameter. Eq. (1) simultane-
ously determines both the neighboring samples of a data
point and the corresponding weights by the sparse recon-
struction from the remaining samples. In principle, more
similar points should receive bigger weights and the weights

min
Z

should be smaller for less similar points. Thus Z is also
called similarity graph matrix (Kang et al. 2015). In addi-
tion, sparse representation enjoys some nice properties, e.g.,
the robustness to noise and datum-adaptive ability (Huang et
al. 2015). On the other hand, model (1) has a drawback, i.e.,
it does not consider nonlinear data sets where data points
reside in a union of manifolds (Kang et al. 2017a).

Spectral Clustering
Spectral clustering requires Laplacian matrix L ∈ Rn×n as
an input, which is computed as L = D − Z(cid:62)+Z
, where
D ∈ Rn×n is a diagonal matrix with the i-th diagonal ele-
ment (cid:80)
. In traditional spectral clustering methods,
j
similarity graph Z ∈ Rn×n is often constructed in one of
the three ways aforementioned. Supposing there are c clus-
ters in the data X, spectral clustering solves the following
problem:

zij +zij
2

2

T r(F (cid:62)LF ),

s.t. F ∈ Idx,

(2)

min
F

where F = [f1, f2, · · · , fn](cid:62) ∈ Rn×c is the cluster indica-
tor matrix and F ∈ Idx represents the clustering label vec-
tor of each point fi ∈ {0, 1}c×1 contains one and only one
element “1” to indicate the group membership of xi. Due
to the discrete constraint on F , problem (2) is NP-hard. In
practice, F is relaxed to allow continuous values and solve

T r(P (cid:62)LP ),

s.t. P (cid:62)P = I,

(3)

min
P

where P ∈ Rn×c is the relaxed continuous clustering la-
bel matrix, and the orthogonal constraint is adopted to avoid
trivial solutions. The optimal solution is obtained from the c
eigenvectors of L corresponding to the c smallest eigenval-
ues. After obtaining F , traditional clustering method, e.g.,
k-means, is implemented to obtain discrete cluster labels
(Huang et al. 2013).

Although this three-steps approach provides a feasible so-
lution, it comes with two potential risks. First, since the
similarity graph computation is independent of the subse-
quent steps, it may be far from optimal. As we discussed
before, the clustering performance is largely determined by
the similarity graph. Thus, ﬁnal results may be degraded.
Second, the ﬁnal solution may unpredictably deviate from
the ground-truth discrete labels (Yang et al. 2016). To ad-
dress these problems, we propose a uniﬁed spectral cluster-
ing model.

Spectral Clustering with Single Kernel

Model
One drawback of Eq. (1) is that it assumes that all the points
lie in a union of independent or disjoint subspaces and are
noiseless. In the presence of dependent subspaces, nonlinear
manifolds and/or data errors, it may select points from dif-
ferent structures to represent a data point and makes the rep-
resentation less informative (Elhamifar and Vidal 2009). It is
recognized that nonlinear data may represent linearity when
mapped to an implicit, higher-dimensional space via a ker-
nel function. To fully exploit data information, we formulate
Eq. (1) in a general manner with a kernelization framework.

Let φ : RD → H be a kernel mapping the data samples
from the input space to a reproducing kernel Hilbert space
R. Then X is transformed to φ(X) = [φ(x1), · · · , φ(xn)].
The kernel similarity between data samples xi and xj
is deﬁned through a predeﬁned kernel as Kxi,xj =<
φ(xi), φ(xj) >. By applying this kernel trick, we do not
need to know the transformation φ. In the new space, Eq. (1)
becomes (Zhang et al. 2010)

min
Z

(cid:107)φ(X) − φ(X)Z(cid:107)2

F + α(cid:107)Z(cid:107)1,

⇐⇒ min

T r(φ(X)T φ(X) − φ(X)T φ(X)Z

Z
− Z T φ(X)T φ(X) + Z T φ(X)T φ(X)Z) + α(cid:107)Z(cid:107)1,

⇐⇒ min

T r(K − 2KZ + Z (cid:62)KZ) + α(cid:107)Z(cid:107)1,

Z

s.t. Z ≥ 0,

diag(Z) = 0,

This model recovers the linear relations among the data in
the new space, and thus the nonlinear relations in the origi-
nal representation. Eq. (4) is more general than Eq. (1) and
is supposed to learn arbitrarily shaped data structure. More-
over, Eq. (4) goes back to Eq. (1) when a linear kernel is
applied.

To fulﬁll the clustering task, we propose our spectral clus-

min
Z,F,P,Q

tering with single kernel (SCSK) model as following:
T r(K − 2KZ + Z (cid:62)KZ) + α(cid:107)Z(cid:107)1
(cid:124)
(cid:123)(cid:122)
(cid:125)
similarity learning
+ γ(cid:107)F − P Q(cid:107)2
F
(cid:124)
(cid:125)
(cid:123)(cid:122)
discrete label learning

+ βT r(P (cid:62)LP )
(cid:125)

(cid:123)(cid:122)
continuous label learning

(cid:124)

,

(5)

s.t. Z ≥ 0,

diag(Z) = 0,

P (cid:62)P = I, Q(cid:62)Q = I, F ∈ Idx,

where α, β, and γ are penalty parameters, and Q is a rota-
tion matrix. Due to the spectral solution invariance property
(Yu and Shi 2003), for any solution P , P Q is another so-
lution. The purpose of the last term is to ﬁnd a proper or-
thonormal Q such that the resulting P Q is close to the real
discrete clustering labels. In Eq. (5), the similarity graph and
the ﬁnal discrete clustering labels are automatically learned
from the data. Ideally, whenever data points i and j belong
to different clusters, we must have zij = 0 and it is also true
vice versa. That is to say, we have zij (cid:54)= 0 if and only if
data points i and j are in the same cluster, or, equivalently
fi = fj. Therefore, our uniﬁed framework Eq. (5) can ex-
ploit the correlation between the similarity matrix and the
labels. Because of the feedback of inferred labels to induce
the similarity matrix and vice versa, we say that our cluster-
ing framework has a self-taught property.

In fact, Eq. (5) is not a simple uniﬁcation of the pipeline
of steps. It learns a similarity graph with optimal structure
for clustering. Ideally, Z should have exactly c connected
components if there are c clusters in the data set (Kang et
al. 2017b). This is to say that the Laplacian matrix L has
c zero eigenvalues (Mohar et al. 1991), i.e., the summation
of the smallest c eigenvalues is zero. To ensure the optimal-
ity of the similarity graph, we can minimize (cid:80)c
i=1 σi(L).

i=1 σi(L) =
T r(P (cid:62)LP ). Therefore, the spectral clustering term,

According to Ky Fan’s theorem (Fan 1949), (cid:80)c
min
P (cid:62)P =I
i.e., the second term in Eq. (5), will ensure learned Z is op-
timal for clustering.

Optimization
To efﬁciently and effectively solve Eq. (5), we design an al-
ternated iterative method.
Computation of Z: With F , P , Q ﬁxed, the problem is re-
duced to

T r(K − 2KZ + Z (cid:62)KZ) + α(cid:107)Z(cid:107)1 + βT r(P (cid:62)LP ),

min
Z

s.t. Z ≥ 0,

diag(Z) = 0.

(4)

We introduce an auxiliary variable S to make above objec-
tive function separable and solve the following equivalent
problem:

T r(K − 2KZ + Z (cid:62)KZ) + α(cid:107)S(cid:107)1 + βT r(P (cid:62)LP ),

min
Z

s.t. Z ≥ 0,

diag(Z) = 0, S = Z.

(6)

(7)

This can be solved by using the augmented Lagrange mul-
tiplier (ALM) type of method. We turn to minimizing the
following augmented Lagrangian function:

L(S, Z, Y ) =T r(K − 2KZ + Z (cid:62)KZ) + α(cid:107)S(cid:107)1

+ βT r(P (cid:62)LP ) +

(cid:107)S − Z +

µ
2

(8)

Y
µ

(cid:107)2
F ,

where µ > 0 is the penalty parameter and Y is the Lagrange
multiplier. This problem can be minimized with respect to
S, Z, and Y alternatively, by ﬁxing the other variables.

For S, by letting H = Z − Y

µ , it can be updated element-

wisely as below

Sij = max(|Hij| − α/µ, 0) · sign(Hij).

(9)

For Z, by letting E = S + Y
wisely as:

µ , it can be updated column-

Z T
i (

min
Zi

µ
2

β
2

I + K)Zi + (

i − µET
dT

i − 2Ki,:)Zi,

(10)

where di ∈ Rn×1 is a vector with the j-th element dij being
dij = (cid:107)Pi,: − Pj,:(cid:107)2. It is easy to obtain Zi by setting the
derivative of Eq. (10) w.r.t. Zi to be zero.
Computation of P: With F , Z, and Q ﬁxed, it is equivalent
to solving

min
P

βT r(P (cid:62)LP ) + γ(cid:107)F − P Q(cid:107)2

F s.t. P (cid:62)P = I.

(11)

The above problem with orthogonal constraint can be efﬁ-
ciently solved by the algorithm proposed by Wen and Yin
(Wen and Yin 2013).
Computation of Q: With F , Z, and P ﬁxed, we have

min
Q

(cid:107)F − P Q(cid:107)2
F

s.t. Q(cid:62)Q = I.

(12)

Algorithm 1 The algorithm of SCSK
Input: Kernel matrix K, parameters α > 0, β > 0, γ > 0, µ > 0.
Initialize: Random matrices Z, P , and Q. Y = 0 and F = 0.
REPEAT
1: Update S according to Eq. (9).
2: S = S − diag(diag(S)) and S = max(S, 0).
3: Update Z according to Eq. (10).
4: Y = Y + µ(S − Z).
5: Update P by solving the problem of Eq. (11).
6: Update Q according to Eq. (13).
7: Update F according to Eq. (16).
UNTIL stopping criterion is met.

It is the orthogonal Procrustes problem (Sch¨onemann 1966),
which admits a closed-form solution. The solution is

Q = U V (cid:62),

(13)

where U and V are left and right parts of the SVD decom-
position of F (cid:62)P .
Computation of F: With Z, P and Q ﬁxed, the problem
becomes

min
F

(cid:107)F − P Q(cid:107)2
F ,

s.t. F ∈ Idx.

(14)

Note that T r(F (cid:62)F ) = n, the above subproblem can be
rewritten as below:

T r(F (cid:62)P Q)

s.t. F ∈ Idx.

(15)

max
F

The optimal solution can be easily obtained as follows:

(cid:40)1,

j = argmax

(P Q)ik

Fij =

k
0, otherwise

(16)

The updates of Z, P , F , and Q are coupled with each
other, so we could reach an overall optimal solution. The
details of our SCSK optimization are summarized in Algo-
rithm 1.

Complexity Analysis
With our optimization strategy, the updating of S requires
O(n2) complexity. The quadratic program can be solved in
polynomial time. The solution of Q involves SVD and its
complexity is O(nc2 + c3). To update P , we need O(nc2 +
c3). The complexity for F is O(nc2). Note that the number
of clusters c is often a small number. Therefore, the main
computation load is from solving Z, which involves matrix
inversion. Fortunately, Z is solved in parallel.

Spectral Clustering with Multiple Kernels

Model
Although the model in Eq. (5) can automatically learn the
similarity graph matrix and discrete cluster labels, its per-
formance will strongly depend on the choice of kernels. It
is often impractical to exhaustively search for the most suit-
able kernel. Moreover, real world data sets are often gener-
ated from different sources along with heterogeneous fea-
tures. Single kernel method may not be able to fully utilize

such information. Multiple kernel learning has the ability to
integrate complementary information and identify a suitable
kernel for a given task. Here we present a way to learn an
appropriate consensus kernel from a convex combination of
a number of predeﬁned kernel functions.

functions {K i}r

Suppose there are a total number of r different ker-
i=1. An augmented Hilbert space
nel
can be constructed by using the mapping of ˜φ(x) =
√
√
wrφr(x)](cid:62) with different
[
wi(wi ≥ 0). Then the combined kernel Kw can
weights
be represented as (Zeng and Cheung 2011)

w2φ2(x), ...,

w1φ1(x),

√

√

Kw(x, y) =< φw(x), φw(y) >=

wiK i(x, y).

(17)

r
(cid:88)

i=1

Note that the convex combination of the positive semi-
deﬁnite kernel matrices {K i}r
i=1 is still a positive semi-
deﬁnite kernel matrix. Thus the combined kernel still satis-
ﬁes Mercer’s condition. Then our proposed method of spec-
tral clustering with multiple kernels (SCMK) can be formu-
lated as
min
Z,F,P,Q,w

T r(Kw − 2KwZ + Z (cid:62)KwZ) + α(cid:107)Z(cid:107)1+

βT r(P (cid:62)LP ) + γ(cid:107)F − P Q(cid:107)2
F ,
diag(Z) = 0,
s.t. Z ≥ 0,

P (cid:62)P = I, Q(cid:62)Q = I, F ∈ Idx,

Kw =

wiK i,

wi = 1, wi ≥ 0.

r
(cid:88)

i=1

r
(cid:88)

√

i=1

(18)

Now above model will learn the similarity graph, discrete
clustering labels, and kernel weights by itself. By iteratively
updating Z, F , and w, each of them will be iteratively re-
ﬁned according to the results of the others.

Optimization
In this part, we show an efﬁcient and effective algorithm to
iteratively and alternatively solve Eq. (18).

w is ﬁxed: Update other variables when w is ﬁxed: We
can directly calculate Kw, and the optimization problem is
exactly Eq. (5). Thus we just need to use Algorithm 1 with
Kw as the input kernel matrix.

Update w: Optimize with respect to w when other vari-
ables are ﬁxed: Solving Eq. (18) with respect to w can be
rewritten as (Cai et al. 2013)

r
(cid:88)

wihi

i=1
r
(cid:88)

√

min
w

s.t.

i=1

wi = 1, wi ≥ 0,

(19)

where

hi = T r(K i − 2K iZ + Z (cid:62)K iZ).

(20)

The Lagrange function of Eq. (19) is

J (w) = w(cid:62)h + γ(1 −

(21)

r
(cid:88)

√

i=1

wi).

i=1, parameters α > 0,

Algorithm 2 The algorithm of SCMK
Input: A set of kernel matrix {K i}r
β > 0, γ > 0, µ > 0.
Initialize: Random matrices Z, P , and Q. Y = 0 and F =
0. wi = 1/r.
REPEAT
1: Calculate Kw by Eq. (17).
2: Do steps 1-7 in Algorithm 1.
3: Calculate h by Eq. (20).
4: Calculate w by Eq. (22).
UNTIL stopping criterion is met.

By utilizing the Karush-Kuhn-Tucker (KKT) condition with
∂J (w)
∂wi

wi = 1, we obtain the

= 0 and the constraint

√

r
(cid:80)
i=1

solution of w as follows:



wi =

hi



−2



.

r
(cid:88)

j=1

1
hj

(22)

We can see that w is closely related to Z. Therefore, we
could obtain both optimal similarity matrix Z and kernel
weight w. We summarize the optimization process of Eq.
(18) in Algorithm 2.

Experiments

Table 1: Description of the data sets

YALE
JAFFE
ORL
AR
COIL20
BA
TR11
TR41
TR45
TDT2

# instances
165
213
400
840
1440
1404
414
878
690
9394

# features
1024
676
1024
768
1024
320
6429
7454
8261
36771

# classes
15
10
40
120
20
36
9
10
10
30

Data Sets
There are altogether ten real benchmark data sets used in our
experiments. Table 1 summarizes the statistics of these data
sets. Among them, the ﬁrst six are image data, and the other
four are text corpora12.

The six image data sets consist of four famous face
databases (ORL3, YALE4, AR5 and JAFFE6), a toy im-

1http://www-users.cs.umn.edu/ han/data/tmdata.tar.gz
2http://www.cad.zju.edu.cn/home/dengcai/Data/TextData.html
3http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html
4http://vision.ucsd.edu/content/yale-face-database
5http://www2.ece.ohio-state.edu/ aleix/ARdatabase.html
6http://www.kasrl.org/jaffe.html

20.php

age database COIL207, and a binary alpha digits data set
BA8. Speciﬁcally, COIL20 contains images of 20 objects.
For each object, the images were taken ﬁve degrees apart
as the object is rotating on a turntable. There are 72 im-
ages for each object. Each image is represented by a 1,024-
dimensional vector. BA consists of digits of “0” through “9”
and letters of capital “A” through “Z”. There are 39 exam-
ples for each class. YALE, ORL, AR, and JAFEE contain
images of individuals. Each image has different facial ex-
pressions or conﬁgurations due to times, illumination condi-
tions, and glasses/no glasses.

Kernel Design
To assess the effectiveness of multiple kernel learning, we
adopted 12 kernels. They include: seven Gaussian kernels
of the form K(x, y) = exp(−(cid:107)x − y(cid:107)2
max)), where
dmax is the maximal distance between samples and t varies
over the set {0.01, 0.0, 0.1, 1, 10, 50, 100}; a linear kernel
K(x, y) = x(cid:62)y; four polynomial kernels K(x, y) = (a +
x(cid:62)y)b with a ∈ {0, 1} and b ∈ {2, 4}. Furthermore, all
kernels are rescaled to [0, 1] by dividing each element by the
largest pair-wise squared distance.

2/(td2

Comparison Algorithms
For single kernel methods, we run downloaded kernel k-
means (KKM) (Sch¨olkopf et al. 1998), spectral clustering
(SC) (Ng et al. 2002), robust kernel k-means (RKKM) (Du
et al. 2015), and SCSK on each kernel separately. To demon-
strate the advantage of our uniﬁed framework, we also im-
plement three separate steps method (TSEP), i.e., learn the
similarity matrix by (4), spectral clustering, k-means (repeat
20 times). And we report both the best and the average re-
sults over all these kernels.

In addition, we also implement the recent simplex sparse
representation (SSR) (Huang et al. 2015) method and robust
afﬁnity graph construction methods by using random for-
est approach: ClustRF-u and ClustRF-a (Zhu et al. 2014).
ClustRF-u assumes all tree nodes are uniformly important,
while ClustRF-a assigns an adaptive weight to each node.
Note that these three methods can only process data in the
original feature space. Moreover, ClusteRF has a high de-
mand for memory and cannot process high dimensional data
directly. Thus we follow the authors’ strategy and perform
PCA on TR11, TR41, and TR45 to reduce the dimension.
We use different numbers of dominant components and re-
port the best clustering results. Nevertheless, we still cannot
handle TDT2 data set with them.

For multiple kernel methods, we implement our proposed
method and directly use the downloaded programs for the
methods in comparison on a combination of these 12 ker-
nels:

MKKM9. The MKKM (Huang et al. 2012b) extends k-
means in a multiple kernel setting. However, it imposes a
different constraint on the kernel weight distribution.

7http://www.cs.columbia.edu/CAVE/software/softlib/coil-

8http://www.cs.nyu.edu/ roweis/data.html
9http://imp.iis.sinica.edu.tw/IVCLab/research/Sean/mkfc/code

KKMKKM-m SC SC-mRKKMRKKM-mClustRF-uClustRF-a SSR TSEPTSEP-mSCSKSCSK-m MKKMAASCRMKKMSCMK

(a) Accuracy(%)

KKMKKM-m SC SC-mRKKMRKKM-mClustRF-uClustRF-a SSR TSEPTSEP-mSCSKSCSK-m MKKMAASCRMKKMSCMK

Data
YALE 47.12 38.97 49.4240.52 48.09
JAFFE 74.39 67.09 74.8854.03 75.61
ORL

53.53 45.93 57.9646.65 54.96

AR

33.02 30.89 28.8322.22 33.43
COIL2059.49 50.74 67.7043.65 61.64
BA

41.20 33.66 31.0726.25 42.17

TR11

TR41

51.91 44.65 50.9843.32 53.03

55.64 46.34 63.5244.80 56.76

TR45

58.79 45.58 57.3945.96 58.13
TDT2 47.05 35.58 52.6345.26 48.35

Data
YALE 51.34 42.07 52.9244.79 52.29
JAFFE 80.13 71.48 82.0859.35 83.47
ORL

73.43 63.36 75.1666.74 74.23

AR

65.21 60.64 58.3756.05 65.44
COIL2074.05 63.57 80.9854.34 74.63
BA

57.25 46.49 50.7640.09 57.82

TR11

TR41

48.88 33.22 43.1131.39 49.69

59.88 40.37 61.3336.60 60.77

TR45

57.87 38.69 48.0333.22 57.86
TDT2 55.28 38.47 52.2327.16 54.46

Data
YALE 49.15 41.12 51.6143.06 49.79
JAFFE 77.32 70.13 76.8356.56 79.58
ORL

58.03 50.42 61.4551.20 59.60

AR

35.52 33.64 33.2425.99 35.87
COIL2064.61 55.30 69.9246.83 66.35
BA

44.20 36.06 34.5029.07 45.28

TR11

TR41

67.57 56.32 58.7950.23 67.93

74.46 60.00 73.6856.45 74.99

TR45

68.49 53.64 61.2550.02 68.18
TDT2 52.79 49.26 50.3942.81 62.13

39.71

67.98

46.88

31.20

51.89

34.35

45.04

46.80

45.69

36.67

42.87

74.01

63.91

60.81

63.70

46.91

33.48

40.86

38.96

42.19

41.74

71.82

51.46

33.88

56.34

36.86

56.40

60.21

53.75

52.60

57.58

97.65

60.75

24.17

74.44

39.89

29.24

53.19

42.17

-

58.76

97.00

78.69

57.09

83.91

54.66

18.97

52.63

38.12

-

63.64

97.65

67.25

40.71

80.83

41.95

35.75

55.58

45.51

-

57.58 54.5562.58 44.60 63.05 52.88

45.70 40.64

52.18

63.25

98.59 87.3298.30 73.88 99.53 90.06

74.55 30.35

87.07

99.69

62.75 69.0070.15 41.45 74.05 53.56

47.51 27.20

55.60

74.52

35.59 65.0065.03 46.41 78.90 68.21

28.61 33.23

34.37

79.29

72.99 76.3277.68 61.03 81.48 62.59

54.82 34.87

66.65

82.21

44.01 23.9745.92 30.75 46.02 31.50

40.52 27.07

43.42

45.57

34.54 41.0671.05 42.08 74.22 55.09

50.13 47.15

57.71

74.26

60.93 63.7869.45 50.17 70.17 53.05

56.10 45.90

62.65

70.25

48.41 71.4576.54 51.07 77.74 59.53

58.46 52.64

64.00

77.47

-

20.8654.78 46.35 56.04 45.02

34.36 19.82

37.57

56.29

(b) NMI(%)

60.25 57.2660.13 46.10 60.58 52.72

50.06 46.83

55.58

61.04

98.16 92.9398.61 71.95 99.18 88.86

79.79 27.22

89.37

99.20

79.87 84.2383.28 50.76 84.78 70.93

68.86 43.77

74.83

85.21

66.64 84.1684.69 64.63 89.61 80.34

59.17 65.06

65.49

89.93

82.26 86.8984.16 71.36 87.03 72.41

70.64 41.87

77.34

86.72

58.17 30.2959.47 32.45 60.34 42.91

56.88 42.34

58.47

60.55

24.77 27.6062.71 29.88 64.60 44.48

44.56 39.39

56.08

64.89

56.78 59.5664.07 39.58 64.92 47.97

57.75 43.05

63.47

64.89

43.70 67.8270.03 40.17 70.75 50.47

56.17 41.94

62.73

70.79

-

02.4457.74 45.38 59.25 48.73

41.36 02.14

47.13

58.66

(c) Purity(%)

63.03 58.1864.77 55.38 65.87 56.19

47.52 42.33

53.64

67.39

98.59 96.2499.06 77.08 99.23 91.24

76.83 33.08

88.90

99.51

66.00 76.5076.00 52.39 77.02 57.96

52.85 31.56

60.23

78.31

46.79 69.5272.44 57.25 83.08 70.69

30.46 34.98

36.78

83.20

77.71 89.0384.03 74.89 84.24 75.58

58.95 39.14

69.95

83.78

49.14 40.8555.03 43.07 55.49 40.45

43.47 30.29

46.27

55.72

49.76 85.0285.95 63.15 86.25 63.36

65.48 54.67

72.93

85.84

65.60 75.4077.02 56.33 78.53 57.19

72.83 62.05

77.57

78.49

57.83 83.6277.28 60.52 79.70 61.06

69.14 57.49

75.20

79.78

-

46.7967.75 56.07 70.69 64.53

54.89 21.73

60.02

72.84

KKMKKM-m SC SC-mRKKMRKKM-mClustRF-uClustRF-a SSR TSEPTSEP-mSCSKSCSK-m MKKMAASCRMKKMSCMK

Table 2: Clustering results obtained on benchmark data sets. ’-m’ denotes the average performance on the 12 kernels. Both the
best results for single kernel and multiple kernel methods are highlighted in boldface.

AASC10. The AASC (Huang et al. 2012a) is an extension
of spectral clustering to the situation when multiple afﬁnities
exist. It is different from our approach since our method tries

to learn an optimal similarity graph.

RMKKM11. The RMKKM (Du et al. 2015) extends k-
means to deal with noise and outliers in a multiple kernel

10http://imp.iis.sinica.edu.tw/IVCLab/research/Sean/aasc/code

11https://github.com/csliangdu/RMKKM

(a) γ = 10−5

(b) γ = 10−4

(c) γ = 10−3

Figure 1: Parameter inﬂuence on accuracy for YALE data set.

setting.

SCMK. Our proposed method of spectral clustering with
multiple kernels. For the purpose of reproducibility, the code
is publicly available12.

For our method, we only need to run once. For those
methods that involve K-means, we follow the strategy sug-
gested in (Yang et al. 2010); i.e., we repeat clustering 20
times and present the results with the best objective values.
We set the number of clusters to the true number of classes
for all clustering algorithms.

Results
We present the clustering results of different methods on
those benchmark data sets in Table 2. In terms of accuracy,
NMI and Purity, our proposed methods obtain superior re-
sults. The big difference between the best and average re-
sults conﬁrms that the choice of kernels has a huge inﬂuence
on the performance of single kernel methods. This motivates
our extended model for multiple kernel learning. Besides,
our extended model for multiple kernel clustering usually
improves the results over our model for single kernel clus-
tering.

Although the best results of the three separate steps ap-
proach are sometimes close to our proposed uniﬁed method,
their average values are often lower than our method. We no-
tice that random forest based afﬁnity graph method achieves
good performance on image data sets. This observation can
be explained by the fact that ClustRF is suitable to handle
ambiguous and unreliable features caused by variation in il-
lumination, face expression or pose on those data sets. On
the other hand, it is not effective for text data sets. In most
cases, ClustRF-a behaves better than ClustRF-u. This jus-
tiﬁes the importance of considering neighbourhood-scale-
adaptive weighting on the nodes.

Parameter Sensitivity
There are three parameters in our model: α, β, and γ. We use
YALE data set as an example to demonstrate the sensitivity
of our model SCMK to parameters. As shown in Figure 1,

12https://github.com/sckangz/AAAI18

our model is quite insensitive to α and β, and γ over wide
ranges of values. In terms of NMI and Purity, we have simi-
lar observations.

Conclusion
In this work, we address two problems existing in most clas-
sical spectral clustering algorithms, i.e., constructing simi-
larity graph and relaxing discrete constraints to continuous
one. To alleviate performance degradation, we propose a
uniﬁed spectral clustering framework which automatically
learns the similarity graph and discrete labels from the data.
To cope with complex data, we develop our method in kernel
space. A multiple kernel approach is proposed to solve ker-
nel dependent issue. Extensive experiments on nine real data
sets demonstrated the promising performance of our meth-
ods as compared to existing clustering approaches.

Acknowledgments
This paper was in part supported by Grants from the Natural
Science Foundation of China (No. 61572111), the National
High Technology Research and Development Program of
China (863 Program) (No. 2015AA015408), a 985 Project
of UESTC (No.A1098531023601041) and a Fundamental
Research Fund for the Central Universities of China (No.
A03017023701012).

References
[Cai et al. 2013] Xiao Cai, Feiping Nie, Weidong Cai, and
Heng Huang. Heterogeneous image features integration via
multi-modal semi-supervised learning model. In Proceed-
ings of the IEEE International Conference on Computer Vi-
sion, pages 1737–1744, 2013.
[Cheng et al. 2010] Bin Cheng, Jianchao Yang, Shuicheng
Yan, Yun Fu, and Thomas S Huang. Learning with-graph
for image analysis. IEEE transactions on image processing,
19(4):858–866, 2010.
[Du et al. 2015] Liang Du, Peng Zhou, Lei Shi, Hanmo
Wang, Mingyu Fan, Wenjian Wang, and Yi-Dong Shen. Ro-
bust multiple kernel k-means using 2; 1-norm. In Proceed-

ings of the 24th International Conference on Artiﬁcial Intel-
ligence, pages 3476–3482. AAAI Press, 2015.
[Elhamifar and Vidal 2009] Ehsan Elhamifar and Ren´e Vi-
In Computer Vision and
dal. Sparse subspace clustering.
Pattern Recognition, 2009. CVPR 2009. IEEE Conference
on, pages 2790–2797. IEEE, 2009.
[Fan 1949] Ky Fan. On a theorem of weyl concerning eigen-
values of linear transformations i. Proceedings of the Na-
tional Academy of Sciences of the United States of America,
35(11):652, 1949.
[Huang et al. 2012a] Hsin-Chien Huang, Yung-Yu Chuang,
and Chu-Song Chen. Afﬁnity aggregation for spectral clus-
tering. In Computer Vision and Pattern Recognition (CVPR),
2012 IEEE Conference on, pages 773–780. IEEE, 2012.
[Huang et al. 2012b] Hsin-Chien Huang, Yung-Yu Chuang,
and Chu-Song Chen. Multiple kernel fuzzy clustering. IEEE
Transactions on Fuzzy Systems, 20(1):120–134, 2012.
[Huang et al. 2013] Jin Huang, Feiping Nie, and Heng
Huang. Spectral rotation versus k-means in spectral clus-
tering. In AAAI, 2013.
[Huang et al. 2015] Jin Huang, Feiping Nie, and Heng
Huang. A new simplex sparse learning model to measure
In Proceedings of the 24th
data similarity for clustering.
International Conference on Artiﬁcial Intelligence, pages
3569–3575. AAAI Press, 2015.
[Huang et al. 2017] Shudong Huang, Hongjun Wang, Tao Li,
Tianrui Li, and Zenglin Xu. Robust graph regularized non-
negative matrix factorization for clustering. Data Mining
and Knowledge Discovery, pages 1–21, 2017.
[Kang et al. 2015] Zhao Kang, Chong Peng, and Qiang
Cheng. Robust subspace clustering via smoothed rank ap-
proximation. IEEE Signal Processing Letters, 22(11):2088–
2092, 2015.
[Kang et al. 2017a] Zhao Kang, Chong Peng, and Qiang
Cheng. Kernel-driven similarity learning. Neurocomputing,
2017.
[Kang et al. 2017b] Zhao Kang, Chong Peng, and Qiang
Cheng. Twin learning for similarity and clustering: A uniﬁed
kernel approach. In AAAI, pages 2080–2086, 2017.
[Kumar et al. 2011] Abhishek Kumar, Piyush Rai, and Hal
Daume. Co-regularized multi-view spectral clustering. In
Advances in neural information processing systems, pages
1413–1421, 2011.
[Mohar et al. 1991] Bojan Mohar, Y Alavi, G Chartrand, and
OR Oellermann. The laplacian spectrum of graphs. Graph
theory, combinatorics, and applications, 2(871-898):12,
1991.
[Ng et al. 2002] Andrew Y Ng, Michael I Jordan, Yair Weiss,
et al. On spectral clustering: Analysis and an algorithm.
Advances in neural information processing systems, 2:849–
856, 2002.
[Pavan and Pelillo 2007] Massimiliano Pavan and Marcello
IEEE
Pelillo. Dominant sets and pairwise clustering.
transactions on pattern analysis and machine intelligence,
29(1):167–172, 2007.

[Pei et al. 2013] Yuru Pei, Tae-Kyun Kim, and Hongbin Zha.
Unsupervised random forest manifold alignment for lipread-
ing. In Proceedings of the IEEE International Conference on
Computer Vision, pages 129–136, 2013.
[Peng et al. 2016] Chong Peng, Zhao Kang, Ming Yang, and
Qiang Cheng. Feature selection embedded subspace clus-
tering. IEEE Signal Processing Letters, 23(7):1018–1022,
2016.
[Premachandran and Kakarala 2013] Vittal Premachandran
and Ramakrishna Kakarala. Consensus of k-nns for robust
neighborhood selection on graph-based manifolds. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1594–1601, 2013.
[Sch¨olkopf et al. 1998] Bernhard
Sch¨olkopf, Alexander
Smola, and Klaus-Robert M¨uller.
Nonlinear compo-
nent analysis as a kernel eigenvalue problem. Neural
computation, 10(5):1299–1319, 1998.
[Sch¨onemann 1966] Peter H Sch¨onemann. A generalized so-
lution of the orthogonal procrustes problem. Psychometrika,
31(1):1–10, 1966.
[Von Luxburg 2007] Ulrike Von Luxburg. A tutorial on spec-
tral clustering. Statistics and computing, 17(4):395–416,
2007.
[Wang et al. 2008] Jun Wang, Shih-Fu Chang, Xiaobo Zhou,
and Stephen TC Wong. Active microscopic cellular image
annotation by superposable graph transduction with imbal-
anced labels. In Computer Vision and Pattern Recognition,
2008. CVPR 2008. IEEE Conference on, pages 1–8. IEEE,
2008.
[Wen and Yin 2013] Zaiwen Wen and Wotao Yin. A feasi-
ble method for optimization with orthogonality constraints.
Mathematical Programming, 142(1-2):397–434, 2013.
[Yang et al. 2010] Yi Yang, Dong Xu, Feiping Nie,
Shuicheng Yan, and Yueting Zhuang.
Image cluster-
ing using local discriminant models and global integration.
IEEE Transactions on Image Processing, 19(10):2761–
2773, 2010.
[Yang et al. 2015] Yang Yang, Zhigang Ma, Yi Yang, Feip-
ing Nie, and Heng Tao Shen. Multitask spectral clustering
by exploring intertask correlation. IEEE transactions on cy-
bernetics, 45(5):1083–1094, 2015.
[Yang et al. 2016] Yang Yang, Fumin Shen, Zi Huang, and
Heng Tao Shen. A uniﬁed framework for discrete spectral
clustering. In IJCAI, pages 2273–2279, 2016.
[Yu and Shi 2003] Stella X Yu and Jianbo Shi. Multiclass
spectral clustering. In Computer Vision, 2003. Proceedings.
Ninth IEEE International Conference on, pages 313–319.
IEEE, 2003.
[Zelnik-Manor and Perona 2004] Lihi Zelnik-Manor
Pietro Perona. Self-tuning spectral clustering.
volume 17, page 16, 2004.
[Zeng and Cheung 2011] Hong Zeng and Yiu-ming Cheung.
Feature selection and kernel learning for local learning-
based clustering. IEEE transactions on pattern analysis and
machine intelligence, 33(8):1532–1547, 2011.

and
In NIPS,

[Zhang et al. 2010] Changshui Zhang, Feiping Nie, and
Shiming Xiang. A general kernelization framework for
learning algorithms based on kernel pca. Neurocomputing,
73(4):959–967, 2010.
[Zhu et al. 2014] Xiatian Zhu, Chen Change Loy, and Shao-
gang Gong. Constructing robust afﬁnity graphs for spectral
clustering. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 1450–1457,
2014.

Uniﬁed Spectral Clustering with Optimal Graph

Zhao Kang1, Chong Peng2, Qiang Cheng3 and Zenglin Xu1∗
1School of Computer Science and Engineering, University of Electronic Science and Technology of China
2Department of Computer Science, Southern Illinois University, Carbondale, USA
3Institute of Biomedical Informatics and Department of Computer Science, University of Kentucky USA
zkang@uestc.edu.cn, pchong@siu.edu, qiang.cheng@uky.edu, zlxu@uestc.edu.cn

7
1
0
2
 
v
o
N
 
2
1
 
 
]

G
L
.
s
c
[
 
 
1
v
8
5
2
4
0
.
1
1
7
1
:
v
i
X
r
a

Abstract

Spectral clustering has found extensive use in many areas.
Most traditional spectral clustering algorithms work in three
separate steps: similarity graph construction; continuous la-
bels learning; discretizing the learned labels by k-means clus-
tering. Such common practice has two potential ﬂaws, which
may lead to severe information loss and performance degra-
dation. First, predeﬁned similarity graph might not be optimal
for subsequent clustering. It is well-accepted that similarity
graph highly affects the clustering results. To this end, we
propose to automatically learn similarity information from
data and simultaneously consider the constraint that the sim-
ilarity matrix has exact c connected components if there are
c clusters. Second, the discrete solution may deviate from the
spectral solution since k-means method is well-known as sen-
sitive to the initialization of cluster centers. In this work, we
transform the candidate solution into a new one that better
approximates the discrete one. Finally, those three subtasks
are integrated into a uniﬁed framework, with each subtask it-
eratively boosted by using the results of the others towards
an overall optimal solution. It is known that the performance
of a kernel method is largely determined by the choice of ker-
nels. To tackle this practical problem of how to select the most
suitable kernel for a particular data set, we further extend our
model to incorporate multiple kernel learning ability. Exten-
sive experiments demonstrate the superiority of our proposed
method as compared to existing clustering approaches.

Introduction
Clustering is a fundamental technique in machine learning,
pattern recognition, and data mining (Huang et al. 2017). In
past decades, a variety of clustering algorithms have been
developed, such as k-means clustering and spectral cluster-
ing.

With the beneﬁts of simplicity and effectiveness, k-means
clustering algorithm is often adopted in various real-world
problems. To deal with the nonlinear structure of many prac-
tical data sets, kernel k-means (KKM) algorithm has been
developed (Sch¨olkopf et al. 1998), where data points are
mapped through a nonlinear transformation into a higher di-
mensional feature space in which the data points are linearly
separable. KKM usually achieves better performance than
the standard k-means. To cope with noise and outliers, ro-
bust kernel k-means (RKKM) (Du et al. 2015) algorithm

∗Corresponding author.

has been proposed. In this approach, the squared (cid:96)2 norm
of error construction term is replaced by (cid:96)2,1 norm. RKKM
demonstrates superior performance on a number of bench-
mark data sets. The performance of such model-based meth-
ods heavily depends on whether the data ﬁt the model. Un-
fortunately, in most cases, we do not know the distribution
of data in advance. To some extent, this problem is alleviated
by multiple kernel learning. Moreover, there is no theoretical
result on how to choose the similarity graph (Von Luxburg
2007).

Spectral clustering is another widely used clustering
method (Kumar et al. 2011). It enjoys the advantage of ex-
ploring the intrinsic data structures by exploiting the differ-
ent similarity graphs of data points (Yang et al. 2015). There
are three kinds of similarity graph constructing strategies:
k-nearest-neighborhood (knn); (cid:15)-nearest-neighborhood; The
fully connected graph. Here, some open issues arise (Huang
et al. 2015): 1) how to choose a proper neighbor number k or
radius (cid:15); 2) how to select an appropriate similarity metric to
measure the similarity among data points; 3) how to counter-
act the adverse effect of noise and outliers; 4) how to tackle
data with structures at different scales of size and density.
Unfortunately, all of these issues heavily inﬂuence the clus-
tering results (Zelnik-Manor and Perona 2004). Nowadays,
many data are often high dimensional, heterogeneous, and
without prior knowledge, and it is therefore a fundamental
challenge to deﬁne a pairwise similarity graph for effective
spectral clustering.

Recently, (Zhu et al. 2014) construct robust afﬁnity
graphs for spectral clustering by identifying discrimina-
tive features. It adopts a random forest approach based on
the motivation that tree leaf nodes contain discriminative
data partitions, which can be exploited to capture subtle
and weak data afﬁnity. This approach shows better per-
formance than other state-of-the-art methods including the
Euclidean-distance-based knn (Wang et al. 2008), dominant
neighbourhoods (Pavan and Pelillo 2007), consensus of knn
(Premachandran and Kakarala 2013), and non-metric based
unsupervised manifold forests (Pei et al. 2013).

The second step of spectral clustering is to use the spec-
trum of the similarity graph to reveal the cluster structure
of the data. Due to the discrete constraint on the cluster la-
bels, this problem is NP-hard. To obtain a feasible approxi-
mation solution, spectral clustering solves a relaxed version

of this problem, i.e., the discrete constraint is relaxed to al-
low continuous values. It ﬁrst performs eigenvalue decom-
position on the Laplacian matrix to generate an approximate
indicator matrix with continuous values. Then, k-means is
often implemented to produce ﬁnal clustering labels (Huang
et al. 2013). Although this approach has been widely used
in practice, it may exhibit poor performance since the k-
means method is well-known as sensitive to the initialization
of cluster centers (Ng et al. 2002).

To address the aforementioned problems, in this paper, we
propose a uniﬁed spectral clustering framework. It jointly
learns the similarity graph from the data and the discrete
clustering labels by solving an optimization problem, in
which the continuous clustering labels just serve as interme-
diate products. To the best of our knowledge, this is the ﬁrst
work that combine the three steps into a single optimization
problem. As we show later, it is not trivial to unify them.
The contributions of our work are as follows:
1. Rather than using predeﬁned similarity metrics, the simi-
larity graph is adaptively learned from the data in a kernel
space. By combining similarity learning with subsequentl
clustering into a uniﬁed framework, we can ensure the op-
timality of the learned similarity graph.

2. Unlike existing spectral clustering methods that work in
three separate steps, we simultaneously learn similarity
graph, continuous labels, and discrete cluster labels. By
leveraging the inherent interactions between these three
subtasks, they can be boosted by each other.

3. Based on our single kernel model, we further extend it to
have the ability to learn the optimal combination of mul-
tiple kernels.
Notations. Given a data set [x1, x2, · · · , xn], we denote
X ∈ Rm×n with m features and n samples. Then the i-
th sample and (i, j)-th element of matrix X are denoted by
xi ∈ Rm×1 and xij, respectively. The (cid:96)2-norm of a vector x
is deﬁned as (cid:107)x(cid:107)2 = x(cid:62) · x, where (cid:62) means transpose. The
squared Frobenius norm is denoted by (cid:107)X(cid:107)2
ij x2
ij.
The (cid:96)1-norm of matrix X is deﬁned as the absolute summa-
tion of its entries, i.e., (cid:107)X(cid:107)1 = (cid:80)
j |xij|. I denotes the
identity matrix. Tr(·) is the trace operator. Z ≥ 0 means all
the elements of Z are nonnegative.

F = (cid:80)

(cid:80)

i

Preliminary Knowledge

Sparse Representation
Recently, sparse representation, which assumes that each
data point can be reconstructed as a linear combination of
the other data points, has shown its power in many tasks
(Cheng et al. 2010; Peng et al. 2016). It often solves the
following problem:
(cid:107)X − XZ(cid:107)2

F + α(cid:107)Z(cid:107)1, s.t. Z ≥ 0, diag(Z) = 0,
(1)
where α > 0 is a balancing parameter. Eq. (1) simultane-
ously determines both the neighboring samples of a data
point and the corresponding weights by the sparse recon-
struction from the remaining samples. In principle, more
similar points should receive bigger weights and the weights

min
Z

should be smaller for less similar points. Thus Z is also
called similarity graph matrix (Kang et al. 2015). In addi-
tion, sparse representation enjoys some nice properties, e.g.,
the robustness to noise and datum-adaptive ability (Huang et
al. 2015). On the other hand, model (1) has a drawback, i.e.,
it does not consider nonlinear data sets where data points
reside in a union of manifolds (Kang et al. 2017a).

Spectral Clustering
Spectral clustering requires Laplacian matrix L ∈ Rn×n as
an input, which is computed as L = D − Z(cid:62)+Z
, where
D ∈ Rn×n is a diagonal matrix with the i-th diagonal ele-
ment (cid:80)
. In traditional spectral clustering methods,
j
similarity graph Z ∈ Rn×n is often constructed in one of
the three ways aforementioned. Supposing there are c clus-
ters in the data X, spectral clustering solves the following
problem:

zij +zij
2

2

T r(F (cid:62)LF ),

s.t. F ∈ Idx,

(2)

min
F

where F = [f1, f2, · · · , fn](cid:62) ∈ Rn×c is the cluster indica-
tor matrix and F ∈ Idx represents the clustering label vec-
tor of each point fi ∈ {0, 1}c×1 contains one and only one
element “1” to indicate the group membership of xi. Due
to the discrete constraint on F , problem (2) is NP-hard. In
practice, F is relaxed to allow continuous values and solve

T r(P (cid:62)LP ),

s.t. P (cid:62)P = I,

(3)

min
P

where P ∈ Rn×c is the relaxed continuous clustering la-
bel matrix, and the orthogonal constraint is adopted to avoid
trivial solutions. The optimal solution is obtained from the c
eigenvectors of L corresponding to the c smallest eigenval-
ues. After obtaining F , traditional clustering method, e.g.,
k-means, is implemented to obtain discrete cluster labels
(Huang et al. 2013).

Although this three-steps approach provides a feasible so-
lution, it comes with two potential risks. First, since the
similarity graph computation is independent of the subse-
quent steps, it may be far from optimal. As we discussed
before, the clustering performance is largely determined by
the similarity graph. Thus, ﬁnal results may be degraded.
Second, the ﬁnal solution may unpredictably deviate from
the ground-truth discrete labels (Yang et al. 2016). To ad-
dress these problems, we propose a uniﬁed spectral cluster-
ing model.

Spectral Clustering with Single Kernel

Model
One drawback of Eq. (1) is that it assumes that all the points
lie in a union of independent or disjoint subspaces and are
noiseless. In the presence of dependent subspaces, nonlinear
manifolds and/or data errors, it may select points from dif-
ferent structures to represent a data point and makes the rep-
resentation less informative (Elhamifar and Vidal 2009). It is
recognized that nonlinear data may represent linearity when
mapped to an implicit, higher-dimensional space via a ker-
nel function. To fully exploit data information, we formulate
Eq. (1) in a general manner with a kernelization framework.

Let φ : RD → H be a kernel mapping the data samples
from the input space to a reproducing kernel Hilbert space
R. Then X is transformed to φ(X) = [φ(x1), · · · , φ(xn)].
The kernel similarity between data samples xi and xj
is deﬁned through a predeﬁned kernel as Kxi,xj =<
φ(xi), φ(xj) >. By applying this kernel trick, we do not
need to know the transformation φ. In the new space, Eq. (1)
becomes (Zhang et al. 2010)

min
Z

(cid:107)φ(X) − φ(X)Z(cid:107)2

F + α(cid:107)Z(cid:107)1,

⇐⇒ min

T r(φ(X)T φ(X) − φ(X)T φ(X)Z

Z
− Z T φ(X)T φ(X) + Z T φ(X)T φ(X)Z) + α(cid:107)Z(cid:107)1,

⇐⇒ min

T r(K − 2KZ + Z (cid:62)KZ) + α(cid:107)Z(cid:107)1,

Z

s.t. Z ≥ 0,

diag(Z) = 0,

This model recovers the linear relations among the data in
the new space, and thus the nonlinear relations in the origi-
nal representation. Eq. (4) is more general than Eq. (1) and
is supposed to learn arbitrarily shaped data structure. More-
over, Eq. (4) goes back to Eq. (1) when a linear kernel is
applied.

To fulﬁll the clustering task, we propose our spectral clus-

min
Z,F,P,Q

tering with single kernel (SCSK) model as following:
T r(K − 2KZ + Z (cid:62)KZ) + α(cid:107)Z(cid:107)1
(cid:124)
(cid:123)(cid:122)
(cid:125)
similarity learning
+ γ(cid:107)F − P Q(cid:107)2
F
(cid:124)
(cid:125)
(cid:123)(cid:122)
discrete label learning

+ βT r(P (cid:62)LP )
(cid:125)

(cid:123)(cid:122)
continuous label learning

(cid:124)

,

(5)

s.t. Z ≥ 0,

diag(Z) = 0,

P (cid:62)P = I, Q(cid:62)Q = I, F ∈ Idx,

where α, β, and γ are penalty parameters, and Q is a rota-
tion matrix. Due to the spectral solution invariance property
(Yu and Shi 2003), for any solution P , P Q is another so-
lution. The purpose of the last term is to ﬁnd a proper or-
thonormal Q such that the resulting P Q is close to the real
discrete clustering labels. In Eq. (5), the similarity graph and
the ﬁnal discrete clustering labels are automatically learned
from the data. Ideally, whenever data points i and j belong
to different clusters, we must have zij = 0 and it is also true
vice versa. That is to say, we have zij (cid:54)= 0 if and only if
data points i and j are in the same cluster, or, equivalently
fi = fj. Therefore, our uniﬁed framework Eq. (5) can ex-
ploit the correlation between the similarity matrix and the
labels. Because of the feedback of inferred labels to induce
the similarity matrix and vice versa, we say that our cluster-
ing framework has a self-taught property.

In fact, Eq. (5) is not a simple uniﬁcation of the pipeline
of steps. It learns a similarity graph with optimal structure
for clustering. Ideally, Z should have exactly c connected
components if there are c clusters in the data set (Kang et
al. 2017b). This is to say that the Laplacian matrix L has
c zero eigenvalues (Mohar et al. 1991), i.e., the summation
of the smallest c eigenvalues is zero. To ensure the optimal-
ity of the similarity graph, we can minimize (cid:80)c
i=1 σi(L).

i=1 σi(L) =
T r(P (cid:62)LP ). Therefore, the spectral clustering term,

According to Ky Fan’s theorem (Fan 1949), (cid:80)c
min
P (cid:62)P =I
i.e., the second term in Eq. (5), will ensure learned Z is op-
timal for clustering.

Optimization
To efﬁciently and effectively solve Eq. (5), we design an al-
ternated iterative method.
Computation of Z: With F , P , Q ﬁxed, the problem is re-
duced to

T r(K − 2KZ + Z (cid:62)KZ) + α(cid:107)Z(cid:107)1 + βT r(P (cid:62)LP ),

min
Z

s.t. Z ≥ 0,

diag(Z) = 0.

(4)

We introduce an auxiliary variable S to make above objec-
tive function separable and solve the following equivalent
problem:

T r(K − 2KZ + Z (cid:62)KZ) + α(cid:107)S(cid:107)1 + βT r(P (cid:62)LP ),

min
Z

s.t. Z ≥ 0,

diag(Z) = 0, S = Z.

(6)

(7)

This can be solved by using the augmented Lagrange mul-
tiplier (ALM) type of method. We turn to minimizing the
following augmented Lagrangian function:

L(S, Z, Y ) =T r(K − 2KZ + Z (cid:62)KZ) + α(cid:107)S(cid:107)1

+ βT r(P (cid:62)LP ) +

(cid:107)S − Z +

µ
2

(8)

Y
µ

(cid:107)2
F ,

where µ > 0 is the penalty parameter and Y is the Lagrange
multiplier. This problem can be minimized with respect to
S, Z, and Y alternatively, by ﬁxing the other variables.

For S, by letting H = Z − Y

µ , it can be updated element-

wisely as below

Sij = max(|Hij| − α/µ, 0) · sign(Hij).

(9)

For Z, by letting E = S + Y
wisely as:

µ , it can be updated column-

Z T
i (

min
Zi

µ
2

β
2

I + K)Zi + (

i − µET
dT

i − 2Ki,:)Zi,

(10)

where di ∈ Rn×1 is a vector with the j-th element dij being
dij = (cid:107)Pi,: − Pj,:(cid:107)2. It is easy to obtain Zi by setting the
derivative of Eq. (10) w.r.t. Zi to be zero.
Computation of P: With F , Z, and Q ﬁxed, it is equivalent
to solving

min
P

βT r(P (cid:62)LP ) + γ(cid:107)F − P Q(cid:107)2

F s.t. P (cid:62)P = I.

(11)

The above problem with orthogonal constraint can be efﬁ-
ciently solved by the algorithm proposed by Wen and Yin
(Wen and Yin 2013).
Computation of Q: With F , Z, and P ﬁxed, we have

min
Q

(cid:107)F − P Q(cid:107)2
F

s.t. Q(cid:62)Q = I.

(12)

Algorithm 1 The algorithm of SCSK
Input: Kernel matrix K, parameters α > 0, β > 0, γ > 0, µ > 0.
Initialize: Random matrices Z, P , and Q. Y = 0 and F = 0.
REPEAT
1: Update S according to Eq. (9).
2: S = S − diag(diag(S)) and S = max(S, 0).
3: Update Z according to Eq. (10).
4: Y = Y + µ(S − Z).
5: Update P by solving the problem of Eq. (11).
6: Update Q according to Eq. (13).
7: Update F according to Eq. (16).
UNTIL stopping criterion is met.

It is the orthogonal Procrustes problem (Sch¨onemann 1966),
which admits a closed-form solution. The solution is

Q = U V (cid:62),

(13)

where U and V are left and right parts of the SVD decom-
position of F (cid:62)P .
Computation of F: With Z, P and Q ﬁxed, the problem
becomes

min
F

(cid:107)F − P Q(cid:107)2
F ,

s.t. F ∈ Idx.

(14)

Note that T r(F (cid:62)F ) = n, the above subproblem can be
rewritten as below:

T r(F (cid:62)P Q)

s.t. F ∈ Idx.

(15)

max
F

The optimal solution can be easily obtained as follows:

(cid:40)1,

j = argmax

(P Q)ik

Fij =

k
0, otherwise

(16)

The updates of Z, P , F , and Q are coupled with each
other, so we could reach an overall optimal solution. The
details of our SCSK optimization are summarized in Algo-
rithm 1.

Complexity Analysis
With our optimization strategy, the updating of S requires
O(n2) complexity. The quadratic program can be solved in
polynomial time. The solution of Q involves SVD and its
complexity is O(nc2 + c3). To update P , we need O(nc2 +
c3). The complexity for F is O(nc2). Note that the number
of clusters c is often a small number. Therefore, the main
computation load is from solving Z, which involves matrix
inversion. Fortunately, Z is solved in parallel.

Spectral Clustering with Multiple Kernels

Model
Although the model in Eq. (5) can automatically learn the
similarity graph matrix and discrete cluster labels, its per-
formance will strongly depend on the choice of kernels. It
is often impractical to exhaustively search for the most suit-
able kernel. Moreover, real world data sets are often gener-
ated from different sources along with heterogeneous fea-
tures. Single kernel method may not be able to fully utilize

such information. Multiple kernel learning has the ability to
integrate complementary information and identify a suitable
kernel for a given task. Here we present a way to learn an
appropriate consensus kernel from a convex combination of
a number of predeﬁned kernel functions.

functions {K i}r

Suppose there are a total number of r different ker-
i=1. An augmented Hilbert space
nel
can be constructed by using the mapping of ˜φ(x) =
√
√
wrφr(x)](cid:62) with different
[
wi(wi ≥ 0). Then the combined kernel Kw can
weights
be represented as (Zeng and Cheung 2011)

w2φ2(x), ...,

w1φ1(x),

√

√

Kw(x, y) =< φw(x), φw(y) >=

wiK i(x, y).

(17)

r
(cid:88)

i=1

Note that the convex combination of the positive semi-
deﬁnite kernel matrices {K i}r
i=1 is still a positive semi-
deﬁnite kernel matrix. Thus the combined kernel still satis-
ﬁes Mercer’s condition. Then our proposed method of spec-
tral clustering with multiple kernels (SCMK) can be formu-
lated as
min
Z,F,P,Q,w

T r(Kw − 2KwZ + Z (cid:62)KwZ) + α(cid:107)Z(cid:107)1+

βT r(P (cid:62)LP ) + γ(cid:107)F − P Q(cid:107)2
F ,
diag(Z) = 0,
s.t. Z ≥ 0,

P (cid:62)P = I, Q(cid:62)Q = I, F ∈ Idx,

Kw =

wiK i,

wi = 1, wi ≥ 0.

r
(cid:88)

i=1

r
(cid:88)

√

i=1

(18)

Now above model will learn the similarity graph, discrete
clustering labels, and kernel weights by itself. By iteratively
updating Z, F , and w, each of them will be iteratively re-
ﬁned according to the results of the others.

Optimization
In this part, we show an efﬁcient and effective algorithm to
iteratively and alternatively solve Eq. (18).

w is ﬁxed: Update other variables when w is ﬁxed: We
can directly calculate Kw, and the optimization problem is
exactly Eq. (5). Thus we just need to use Algorithm 1 with
Kw as the input kernel matrix.

Update w: Optimize with respect to w when other vari-
ables are ﬁxed: Solving Eq. (18) with respect to w can be
rewritten as (Cai et al. 2013)

r
(cid:88)

wihi

i=1
r
(cid:88)

√

min
w

s.t.

i=1

wi = 1, wi ≥ 0,

(19)

where

hi = T r(K i − 2K iZ + Z (cid:62)K iZ).

(20)

The Lagrange function of Eq. (19) is

J (w) = w(cid:62)h + γ(1 −

(21)

r
(cid:88)

√

i=1

wi).

i=1, parameters α > 0,

Algorithm 2 The algorithm of SCMK
Input: A set of kernel matrix {K i}r
β > 0, γ > 0, µ > 0.
Initialize: Random matrices Z, P , and Q. Y = 0 and F =
0. wi = 1/r.
REPEAT
1: Calculate Kw by Eq. (17).
2: Do steps 1-7 in Algorithm 1.
3: Calculate h by Eq. (20).
4: Calculate w by Eq. (22).
UNTIL stopping criterion is met.

By utilizing the Karush-Kuhn-Tucker (KKT) condition with
∂J (w)
∂wi

wi = 1, we obtain the

= 0 and the constraint

√

r
(cid:80)
i=1

solution of w as follows:



wi =

hi



−2



.

r
(cid:88)

j=1

1
hj

(22)

We can see that w is closely related to Z. Therefore, we
could obtain both optimal similarity matrix Z and kernel
weight w. We summarize the optimization process of Eq.
(18) in Algorithm 2.

Experiments

Table 1: Description of the data sets

YALE
JAFFE
ORL
AR
COIL20
BA
TR11
TR41
TR45
TDT2

# instances
165
213
400
840
1440
1404
414
878
690
9394

# features
1024
676
1024
768
1024
320
6429
7454
8261
36771

# classes
15
10
40
120
20
36
9
10
10
30

Data Sets
There are altogether ten real benchmark data sets used in our
experiments. Table 1 summarizes the statistics of these data
sets. Among them, the ﬁrst six are image data, and the other
four are text corpora12.

The six image data sets consist of four famous face
databases (ORL3, YALE4, AR5 and JAFFE6), a toy im-

1http://www-users.cs.umn.edu/ han/data/tmdata.tar.gz
2http://www.cad.zju.edu.cn/home/dengcai/Data/TextData.html
3http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html
4http://vision.ucsd.edu/content/yale-face-database
5http://www2.ece.ohio-state.edu/ aleix/ARdatabase.html
6http://www.kasrl.org/jaffe.html

20.php

age database COIL207, and a binary alpha digits data set
BA8. Speciﬁcally, COIL20 contains images of 20 objects.
For each object, the images were taken ﬁve degrees apart
as the object is rotating on a turntable. There are 72 im-
ages for each object. Each image is represented by a 1,024-
dimensional vector. BA consists of digits of “0” through “9”
and letters of capital “A” through “Z”. There are 39 exam-
ples for each class. YALE, ORL, AR, and JAFEE contain
images of individuals. Each image has different facial ex-
pressions or conﬁgurations due to times, illumination condi-
tions, and glasses/no glasses.

Kernel Design
To assess the effectiveness of multiple kernel learning, we
adopted 12 kernels. They include: seven Gaussian kernels
of the form K(x, y) = exp(−(cid:107)x − y(cid:107)2
max)), where
dmax is the maximal distance between samples and t varies
over the set {0.01, 0.0, 0.1, 1, 10, 50, 100}; a linear kernel
K(x, y) = x(cid:62)y; four polynomial kernels K(x, y) = (a +
x(cid:62)y)b with a ∈ {0, 1} and b ∈ {2, 4}. Furthermore, all
kernels are rescaled to [0, 1] by dividing each element by the
largest pair-wise squared distance.

2/(td2

Comparison Algorithms
For single kernel methods, we run downloaded kernel k-
means (KKM) (Sch¨olkopf et al. 1998), spectral clustering
(SC) (Ng et al. 2002), robust kernel k-means (RKKM) (Du
et al. 2015), and SCSK on each kernel separately. To demon-
strate the advantage of our uniﬁed framework, we also im-
plement three separate steps method (TSEP), i.e., learn the
similarity matrix by (4), spectral clustering, k-means (repeat
20 times). And we report both the best and the average re-
sults over all these kernels.

In addition, we also implement the recent simplex sparse
representation (SSR) (Huang et al. 2015) method and robust
afﬁnity graph construction methods by using random for-
est approach: ClustRF-u and ClustRF-a (Zhu et al. 2014).
ClustRF-u assumes all tree nodes are uniformly important,
while ClustRF-a assigns an adaptive weight to each node.
Note that these three methods can only process data in the
original feature space. Moreover, ClusteRF has a high de-
mand for memory and cannot process high dimensional data
directly. Thus we follow the authors’ strategy and perform
PCA on TR11, TR41, and TR45 to reduce the dimension.
We use different numbers of dominant components and re-
port the best clustering results. Nevertheless, we still cannot
handle TDT2 data set with them.

For multiple kernel methods, we implement our proposed
method and directly use the downloaded programs for the
methods in comparison on a combination of these 12 ker-
nels:

MKKM9. The MKKM (Huang et al. 2012b) extends k-
means in a multiple kernel setting. However, it imposes a
different constraint on the kernel weight distribution.

7http://www.cs.columbia.edu/CAVE/software/softlib/coil-

8http://www.cs.nyu.edu/ roweis/data.html
9http://imp.iis.sinica.edu.tw/IVCLab/research/Sean/mkfc/code

KKMKKM-m SC SC-mRKKMRKKM-mClustRF-uClustRF-a SSR TSEPTSEP-mSCSKSCSK-m MKKMAASCRMKKMSCMK

(a) Accuracy(%)

KKMKKM-m SC SC-mRKKMRKKM-mClustRF-uClustRF-a SSR TSEPTSEP-mSCSKSCSK-m MKKMAASCRMKKMSCMK

Data
YALE 47.12 38.97 49.4240.52 48.09
JAFFE 74.39 67.09 74.8854.03 75.61
ORL

53.53 45.93 57.9646.65 54.96

AR

33.02 30.89 28.8322.22 33.43
COIL2059.49 50.74 67.7043.65 61.64
BA

41.20 33.66 31.0726.25 42.17

TR11

TR41

51.91 44.65 50.9843.32 53.03

55.64 46.34 63.5244.80 56.76

TR45

58.79 45.58 57.3945.96 58.13
TDT2 47.05 35.58 52.6345.26 48.35

Data
YALE 51.34 42.07 52.9244.79 52.29
JAFFE 80.13 71.48 82.0859.35 83.47
ORL

73.43 63.36 75.1666.74 74.23

AR

65.21 60.64 58.3756.05 65.44
COIL2074.05 63.57 80.9854.34 74.63
BA

57.25 46.49 50.7640.09 57.82

TR11

TR41

48.88 33.22 43.1131.39 49.69

59.88 40.37 61.3336.60 60.77

TR45

57.87 38.69 48.0333.22 57.86
TDT2 55.28 38.47 52.2327.16 54.46

Data
YALE 49.15 41.12 51.6143.06 49.79
JAFFE 77.32 70.13 76.8356.56 79.58
ORL

58.03 50.42 61.4551.20 59.60

AR

35.52 33.64 33.2425.99 35.87
COIL2064.61 55.30 69.9246.83 66.35
BA

44.20 36.06 34.5029.07 45.28

TR11

TR41

67.57 56.32 58.7950.23 67.93

74.46 60.00 73.6856.45 74.99

TR45

68.49 53.64 61.2550.02 68.18
TDT2 52.79 49.26 50.3942.81 62.13

39.71

67.98

46.88

31.20

51.89

34.35

45.04

46.80

45.69

36.67

42.87

74.01

63.91

60.81

63.70

46.91

33.48

40.86

38.96

42.19

41.74

71.82

51.46

33.88

56.34

36.86

56.40

60.21

53.75

52.60

57.58

97.65

60.75

24.17

74.44

39.89

29.24

53.19

42.17

-

58.76

97.00

78.69

57.09

83.91

54.66

18.97

52.63

38.12

-

63.64

97.65

67.25

40.71

80.83

41.95

35.75

55.58

45.51

-

57.58 54.5562.58 44.60 63.05 52.88

45.70 40.64

52.18

63.25

98.59 87.3298.30 73.88 99.53 90.06

74.55 30.35

87.07

99.69

62.75 69.0070.15 41.45 74.05 53.56

47.51 27.20

55.60

74.52

35.59 65.0065.03 46.41 78.90 68.21

28.61 33.23

34.37

79.29

72.99 76.3277.68 61.03 81.48 62.59

54.82 34.87

66.65

82.21

44.01 23.9745.92 30.75 46.02 31.50

40.52 27.07

43.42

45.57

34.54 41.0671.05 42.08 74.22 55.09

50.13 47.15

57.71

74.26

60.93 63.7869.45 50.17 70.17 53.05

56.10 45.90

62.65

70.25

48.41 71.4576.54 51.07 77.74 59.53

58.46 52.64

64.00

77.47

-

20.8654.78 46.35 56.04 45.02

34.36 19.82

37.57

56.29

(b) NMI(%)

60.25 57.2660.13 46.10 60.58 52.72

50.06 46.83

55.58

61.04

98.16 92.9398.61 71.95 99.18 88.86

79.79 27.22

89.37

99.20

79.87 84.2383.28 50.76 84.78 70.93

68.86 43.77

74.83

85.21

66.64 84.1684.69 64.63 89.61 80.34

59.17 65.06

65.49

89.93

82.26 86.8984.16 71.36 87.03 72.41

70.64 41.87

77.34

86.72

58.17 30.2959.47 32.45 60.34 42.91

56.88 42.34

58.47

60.55

24.77 27.6062.71 29.88 64.60 44.48

44.56 39.39

56.08

64.89

56.78 59.5664.07 39.58 64.92 47.97

57.75 43.05

63.47

64.89

43.70 67.8270.03 40.17 70.75 50.47

56.17 41.94

62.73

70.79

-

02.4457.74 45.38 59.25 48.73

41.36 02.14

47.13

58.66

(c) Purity(%)

63.03 58.1864.77 55.38 65.87 56.19

47.52 42.33

53.64

67.39

98.59 96.2499.06 77.08 99.23 91.24

76.83 33.08

88.90

99.51

66.00 76.5076.00 52.39 77.02 57.96

52.85 31.56

60.23

78.31

46.79 69.5272.44 57.25 83.08 70.69

30.46 34.98

36.78

83.20

77.71 89.0384.03 74.89 84.24 75.58

58.95 39.14

69.95

83.78

49.14 40.8555.03 43.07 55.49 40.45

43.47 30.29

46.27

55.72

49.76 85.0285.95 63.15 86.25 63.36

65.48 54.67

72.93

85.84

65.60 75.4077.02 56.33 78.53 57.19

72.83 62.05

77.57

78.49

57.83 83.6277.28 60.52 79.70 61.06

69.14 57.49

75.20

79.78

-

46.7967.75 56.07 70.69 64.53

54.89 21.73

60.02

72.84

KKMKKM-m SC SC-mRKKMRKKM-mClustRF-uClustRF-a SSR TSEPTSEP-mSCSKSCSK-m MKKMAASCRMKKMSCMK

Table 2: Clustering results obtained on benchmark data sets. ’-m’ denotes the average performance on the 12 kernels. Both the
best results for single kernel and multiple kernel methods are highlighted in boldface.

AASC10. The AASC (Huang et al. 2012a) is an extension
of spectral clustering to the situation when multiple afﬁnities
exist. It is different from our approach since our method tries

to learn an optimal similarity graph.

RMKKM11. The RMKKM (Du et al. 2015) extends k-
means to deal with noise and outliers in a multiple kernel

10http://imp.iis.sinica.edu.tw/IVCLab/research/Sean/aasc/code

11https://github.com/csliangdu/RMKKM

(a) γ = 10−5

(b) γ = 10−4

(c) γ = 10−3

Figure 1: Parameter inﬂuence on accuracy for YALE data set.

setting.

SCMK. Our proposed method of spectral clustering with
multiple kernels. For the purpose of reproducibility, the code
is publicly available12.

For our method, we only need to run once. For those
methods that involve K-means, we follow the strategy sug-
gested in (Yang et al. 2010); i.e., we repeat clustering 20
times and present the results with the best objective values.
We set the number of clusters to the true number of classes
for all clustering algorithms.

Results
We present the clustering results of different methods on
those benchmark data sets in Table 2. In terms of accuracy,
NMI and Purity, our proposed methods obtain superior re-
sults. The big difference between the best and average re-
sults conﬁrms that the choice of kernels has a huge inﬂuence
on the performance of single kernel methods. This motivates
our extended model for multiple kernel learning. Besides,
our extended model for multiple kernel clustering usually
improves the results over our model for single kernel clus-
tering.

Although the best results of the three separate steps ap-
proach are sometimes close to our proposed uniﬁed method,
their average values are often lower than our method. We no-
tice that random forest based afﬁnity graph method achieves
good performance on image data sets. This observation can
be explained by the fact that ClustRF is suitable to handle
ambiguous and unreliable features caused by variation in il-
lumination, face expression or pose on those data sets. On
the other hand, it is not effective for text data sets. In most
cases, ClustRF-a behaves better than ClustRF-u. This jus-
tiﬁes the importance of considering neighbourhood-scale-
adaptive weighting on the nodes.

Parameter Sensitivity
There are three parameters in our model: α, β, and γ. We use
YALE data set as an example to demonstrate the sensitivity
of our model SCMK to parameters. As shown in Figure 1,

12https://github.com/sckangz/AAAI18

our model is quite insensitive to α and β, and γ over wide
ranges of values. In terms of NMI and Purity, we have simi-
lar observations.

Conclusion
In this work, we address two problems existing in most clas-
sical spectral clustering algorithms, i.e., constructing simi-
larity graph and relaxing discrete constraints to continuous
one. To alleviate performance degradation, we propose a
uniﬁed spectral clustering framework which automatically
learns the similarity graph and discrete labels from the data.
To cope with complex data, we develop our method in kernel
space. A multiple kernel approach is proposed to solve ker-
nel dependent issue. Extensive experiments on nine real data
sets demonstrated the promising performance of our meth-
ods as compared to existing clustering approaches.

Acknowledgments
This paper was in part supported by Grants from the Natural
Science Foundation of China (No. 61572111), the National
High Technology Research and Development Program of
China (863 Program) (No. 2015AA015408), a 985 Project
of UESTC (No.A1098531023601041) and a Fundamental
Research Fund for the Central Universities of China (No.
A03017023701012).

References
[Cai et al. 2013] Xiao Cai, Feiping Nie, Weidong Cai, and
Heng Huang. Heterogeneous image features integration via
multi-modal semi-supervised learning model. In Proceed-
ings of the IEEE International Conference on Computer Vi-
sion, pages 1737–1744, 2013.
[Cheng et al. 2010] Bin Cheng, Jianchao Yang, Shuicheng
Yan, Yun Fu, and Thomas S Huang. Learning with-graph
for image analysis. IEEE transactions on image processing,
19(4):858–866, 2010.
[Du et al. 2015] Liang Du, Peng Zhou, Lei Shi, Hanmo
Wang, Mingyu Fan, Wenjian Wang, and Yi-Dong Shen. Ro-
bust multiple kernel k-means using 2; 1-norm. In Proceed-

ings of the 24th International Conference on Artiﬁcial Intel-
ligence, pages 3476–3482. AAAI Press, 2015.
[Elhamifar and Vidal 2009] Ehsan Elhamifar and Ren´e Vi-
In Computer Vision and
dal. Sparse subspace clustering.
Pattern Recognition, 2009. CVPR 2009. IEEE Conference
on, pages 2790–2797. IEEE, 2009.
[Fan 1949] Ky Fan. On a theorem of weyl concerning eigen-
values of linear transformations i. Proceedings of the Na-
tional Academy of Sciences of the United States of America,
35(11):652, 1949.
[Huang et al. 2012a] Hsin-Chien Huang, Yung-Yu Chuang,
and Chu-Song Chen. Afﬁnity aggregation for spectral clus-
tering. In Computer Vision and Pattern Recognition (CVPR),
2012 IEEE Conference on, pages 773–780. IEEE, 2012.
[Huang et al. 2012b] Hsin-Chien Huang, Yung-Yu Chuang,
and Chu-Song Chen. Multiple kernel fuzzy clustering. IEEE
Transactions on Fuzzy Systems, 20(1):120–134, 2012.
[Huang et al. 2013] Jin Huang, Feiping Nie, and Heng
Huang. Spectral rotation versus k-means in spectral clus-
tering. In AAAI, 2013.
[Huang et al. 2015] Jin Huang, Feiping Nie, and Heng
Huang. A new simplex sparse learning model to measure
In Proceedings of the 24th
data similarity for clustering.
International Conference on Artiﬁcial Intelligence, pages
3569–3575. AAAI Press, 2015.
[Huang et al. 2017] Shudong Huang, Hongjun Wang, Tao Li,
Tianrui Li, and Zenglin Xu. Robust graph regularized non-
negative matrix factorization for clustering. Data Mining
and Knowledge Discovery, pages 1–21, 2017.
[Kang et al. 2015] Zhao Kang, Chong Peng, and Qiang
Cheng. Robust subspace clustering via smoothed rank ap-
proximation. IEEE Signal Processing Letters, 22(11):2088–
2092, 2015.
[Kang et al. 2017a] Zhao Kang, Chong Peng, and Qiang
Cheng. Kernel-driven similarity learning. Neurocomputing,
2017.
[Kang et al. 2017b] Zhao Kang, Chong Peng, and Qiang
Cheng. Twin learning for similarity and clustering: A uniﬁed
kernel approach. In AAAI, pages 2080–2086, 2017.
[Kumar et al. 2011] Abhishek Kumar, Piyush Rai, and Hal
Daume. Co-regularized multi-view spectral clustering. In
Advances in neural information processing systems, pages
1413–1421, 2011.
[Mohar et al. 1991] Bojan Mohar, Y Alavi, G Chartrand, and
OR Oellermann. The laplacian spectrum of graphs. Graph
theory, combinatorics, and applications, 2(871-898):12,
1991.
[Ng et al. 2002] Andrew Y Ng, Michael I Jordan, Yair Weiss,
et al. On spectral clustering: Analysis and an algorithm.
Advances in neural information processing systems, 2:849–
856, 2002.
[Pavan and Pelillo 2007] Massimiliano Pavan and Marcello
IEEE
Pelillo. Dominant sets and pairwise clustering.
transactions on pattern analysis and machine intelligence,
29(1):167–172, 2007.

[Pei et al. 2013] Yuru Pei, Tae-Kyun Kim, and Hongbin Zha.
Unsupervised random forest manifold alignment for lipread-
ing. In Proceedings of the IEEE International Conference on
Computer Vision, pages 129–136, 2013.
[Peng et al. 2016] Chong Peng, Zhao Kang, Ming Yang, and
Qiang Cheng. Feature selection embedded subspace clus-
tering. IEEE Signal Processing Letters, 23(7):1018–1022,
2016.
[Premachandran and Kakarala 2013] Vittal Premachandran
and Ramakrishna Kakarala. Consensus of k-nns for robust
neighborhood selection on graph-based manifolds. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1594–1601, 2013.
[Sch¨olkopf et al. 1998] Bernhard
Sch¨olkopf, Alexander
Smola, and Klaus-Robert M¨uller.
Nonlinear compo-
nent analysis as a kernel eigenvalue problem. Neural
computation, 10(5):1299–1319, 1998.
[Sch¨onemann 1966] Peter H Sch¨onemann. A generalized so-
lution of the orthogonal procrustes problem. Psychometrika,
31(1):1–10, 1966.
[Von Luxburg 2007] Ulrike Von Luxburg. A tutorial on spec-
tral clustering. Statistics and computing, 17(4):395–416,
2007.
[Wang et al. 2008] Jun Wang, Shih-Fu Chang, Xiaobo Zhou,
and Stephen TC Wong. Active microscopic cellular image
annotation by superposable graph transduction with imbal-
anced labels. In Computer Vision and Pattern Recognition,
2008. CVPR 2008. IEEE Conference on, pages 1–8. IEEE,
2008.
[Wen and Yin 2013] Zaiwen Wen and Wotao Yin. A feasi-
ble method for optimization with orthogonality constraints.
Mathematical Programming, 142(1-2):397–434, 2013.
[Yang et al. 2010] Yi Yang, Dong Xu, Feiping Nie,
Shuicheng Yan, and Yueting Zhuang.
Image cluster-
ing using local discriminant models and global integration.
IEEE Transactions on Image Processing, 19(10):2761–
2773, 2010.
[Yang et al. 2015] Yang Yang, Zhigang Ma, Yi Yang, Feip-
ing Nie, and Heng Tao Shen. Multitask spectral clustering
by exploring intertask correlation. IEEE transactions on cy-
bernetics, 45(5):1083–1094, 2015.
[Yang et al. 2016] Yang Yang, Fumin Shen, Zi Huang, and
Heng Tao Shen. A uniﬁed framework for discrete spectral
clustering. In IJCAI, pages 2273–2279, 2016.
[Yu and Shi 2003] Stella X Yu and Jianbo Shi. Multiclass
spectral clustering. In Computer Vision, 2003. Proceedings.
Ninth IEEE International Conference on, pages 313–319.
IEEE, 2003.
[Zelnik-Manor and Perona 2004] Lihi Zelnik-Manor
Pietro Perona. Self-tuning spectral clustering.
volume 17, page 16, 2004.
[Zeng and Cheung 2011] Hong Zeng and Yiu-ming Cheung.
Feature selection and kernel learning for local learning-
based clustering. IEEE transactions on pattern analysis and
machine intelligence, 33(8):1532–1547, 2011.

and
In NIPS,

[Zhang et al. 2010] Changshui Zhang, Feiping Nie, and
Shiming Xiang. A general kernelization framework for
learning algorithms based on kernel pca. Neurocomputing,
73(4):959–967, 2010.
[Zhu et al. 2014] Xiatian Zhu, Chen Change Loy, and Shao-
gang Gong. Constructing robust afﬁnity graphs for spectral
clustering. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 1450–1457,
2014.

