8
1
0
2
 
r
p
A
 
1
1
 
 
]

G
L
.
s
c
[
 
 
1
v
2
1
0
4
0
.
4
0
8
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2018

DORA THE EXPLORER: DIRECTED OUTREACHING
REINFORCEMENT ACTION-SELECTION

Leshem Choshen∗
School of Computer Science and Engineering
and Department of Cognitive Sciences
The Hebrew University of Jerusalem
leshem.choshen@mail.huji.ac.il

Lior Fox∗
The Edmond and Lily Safra Center for Brain Sciences
The Hebrew University of Jerusalem
lior.fox@mail.huji.ac.il

Yonatan Loewenstein
The Edmond and Lily Safra Center for Brain Sciences,
Departments of Neurobiology and Cognitive Sciences
and the Federmann Center for the Study of Rationality
The Hebrew University of Jerusalem
yonatan@huji.ac.il

ABSTRACT

Exploration is a fundamental aspect of Reinforcement Learning, typically imple-
mented using stochastic action-selection. Exploration, however, can be more efﬁ-
cient if directed toward gaining new world knowledge. Visit-counters have been
proven useful both in practice and in theory for directed exploration. However, a
major limitation of counters is their locality. While there are a few model-based
solutions to this shortcoming, a model-free approach is still missing. We propose
E-values, a generalization of counters that can be used to evaluate the propagat-
ing exploratory value over state-action trajectories. We compare our approach to
commonly used RL techniques, and show that using E-values improves learning
and performance over traditional counters. We also show how our method can be
implemented with function approximation to efﬁciently learn continuous MDPs.
We demonstrate this by showing that our approach surpasses state of the art per-
formance in the Freeway Atari 2600 game.

1

INTRODUCTION

”If there’s a place you gotta go - I’m the one you need to know.“

(Map, Dora The Explorer)

, P, R, γ) where

We consider Reinforcement Learning in a Markov Decision Process (MDP). An MDP is a ﬁve-
is a set of actions. The dynamics of
tuple M = (
,
S
s, a) which denotes the transition probability from state s to state s(cid:48)
the process is given by P (s(cid:48)
|
s, a) from which the reward for
following action a. Each such transition also has a distribution R (r
|
such transitions is sampled. Given a policy π :
, a function – possibly stochastic – deciding
R satisﬁes:
which actions to take in each of the states, the state-action value function Qπ :

is a set of states and

S → A

A

A

S

Qπ (s, a) =

[r + γQπ (s(cid:48), π (s(cid:48)))]

E
P (

r,s(cid:48)

R

×

∼

s,a)
·|
where γ is the discount factor. The agent’s goal is to ﬁnd an optimal policy π∗ that maximizes
Qπ (s, π (s)). For brevity, Qπ∗ (cid:44) Q∗. There are two main approaches for learning π∗. The ﬁrst
is a model-based approach, where the agent learns an internal model of the MDP (namely P and
R). Given a model, the optimal policy could be found using dynamic programming methods such
as Value Iteration (Sutton & Barto, 1998). The alternative is a model-free approach, where the agent
learns only the value function of states or state-action pairs, without learning a model (Kaelbling
et al., 1996)1.

S × A →

∗These authors contributed equally to this work
1Supplementary code for this paper can be found at https://github.com/borgr/DORA/

1

Published as a conference paper at ICLR 2018

The ideas put forward in this paper are relevant to any model-free learning of MDPs. For con-
creteness, we focus on a particular example, Q-Learning (Watkins & Dayan, 1992; Sutton & Barto,
1998). Q-Learning is a common method for learning Q∗, where the agent iteratively updates its
values of Q (s, a) by performing actions and observing their outcomes. At each step the agent takes
action at then it is transferred from st to st+1 and observe reward r. Then it applies the update rule
regulated by a learning rate α:

Q (st, at)

(1

α) Q (st, at) + α

r + γ max

Q (st+1, a)

.

←

−

(cid:16)

a

(cid:17)

1.1 EXPLORATION AND EXPLOITATION

Balancing between Exploration and Exploitation is a major challenge in Reinforcement Learning.
Seemingly, the agent may want to choose the alternative associated with the highest expected reward,
a behavior known as exploitation. However, in that case it may fail to learn that there are better
options. Therefore exploration, namely the taking of new actions and the visit of new states, may
also be beneﬁcial. It is important to note that exploitation is also inherently relevant for learning, as
we want the agent to have better estimations of the values of valuable state-actions and we care less
about the exact values of actions that the agent already knows to be clearly inferior.

Formally, to guarantee convergence to Q∗, the Q-Learning algorithm must visit each state-action pair
inﬁnitely many times. A naive random walk exploration is sufﬁcient for converging asymptotically.
However, such random exploration has two major limitations when the learning process is ﬁnite.
First, the agent would not utilize its current knowledge about the world to guide its exploration. For
example, an action with a known disastrous outcome will be explored over and over again. Second,
the agent would not be biased in favor of exploring unvisited trajectories more than the visited ones
– hence ”wasting” exploration resources on actions and trajectories which are already well known
to it.

A widely used method for dealing with the ﬁrst problem is the (cid:15)-greedy schema (Sutton & Barto,
1998), in which with probability 1
(cid:15) the agent greedily chooses the best action (according to
current estimation), and with probability (cid:15) it chooses a random action. Another popular alternative,
emphasizing the preference to learn about actions associated with higher rewards, is to draw actions
from a Boltzmann Distribution (Softmax) over the learned Q values, regulated by a Temperature
parameter. While such approaches lead to more informed exploration that is based on learning
experience, they still fail to address the second issue, namely they are not directed (Thrun, 1992)
towards gaining more knowledge, not biasing actions in the direction of unexplored trajectories.

−

Another important approach in the study of efﬁcient exploration is based on Sample Complexity
of Exploration as deﬁned in the PAC-MDP literature (Kakade et al., 2003). Relevant to our work
is Delayed Q Learning (Strehl et al., 2006), a model-free algorithm that has theoretical PAC-MDP
guarantees. However, to ensure these theoretical guarantees this algorithm uses a conservative ex-
ploration which might be impractical (see also (Kolter & Ng, 2009) and Appendix B).

1.2 CURRENT DIRECTED EXPLORATION AND ITS LIMITATIONS

In order to achieve directed exploration, the estimation of an exploration value of the different state-
actions (often termed exploration bonus) is needed. The most commonly used exploration bonus is
based on counting (Thrun, 1992) – for each pair (s, a), store a counter C (s, a) that indicates how
many times the agent performed action a at state s so far. Counter-based methods are widely used
both in practice and in theory (Kolter & Ng, 2009; Strehl & Littman, 2008; Guez et al., 2012; Buso-
niu et al., 2008). Other options for evaluating exploration include recency and value difference (or
error) measures (Thrun, 1992; Tokic & Palm, 2011). While all of these exploration measures can be
used for directed exploration, their major limitation in a model-free settings is that the exploratory
value of a state-action pair is evaluated with respect only to its immediate outcome, one step ahead.
It seems desirable to determine the exploratory value of an action not only by how much new im-
mediate knowledge the agent gains from it, but also by how much more new knowledge could be
gained from a trajectory starting with it. The goal of this work is to develop a measure for such
exploratory values of state-action pairs, in a model-free settings.

2

Published as a conference paper at ICLR 2018

2 LEARNING EXPLORATION VALUES

2.1 PROPAGATING EXPLORATION VALUES

The challenge discussed in 1.2 is in fact similar to that of learning the value functions. The value of a
state-action represents not only the immediate reward, but also the temporally discounted sum of ex-
pected rewards over a trajectory starting from this state and action. Similarly, the ”exploration-value”
of a state-action should represent not only the immediate knowledge gained but also the expected
future gained knowledge. This suggests that a similar approach to that used for value-learning might
be appropriate for learning the exploration values as well, using exploration bonus as the immediate
reward. However, because it is reasonable to require exploration bonus to decrease over repetitions
of the same trajectories, a naive implementation would violate the Markovian property.

This challenge has been addressed in a model-based setting: The idea is to use at every step the
current estimate of the parameters of the MDP in order to compute, using dynamic programming, the
future exploration bonus (Little & Sommer, 2014). However, this solution cannot be implemented
in a model-free setting. Therefore, a satisfying approach for propagating directed exploration in
model-free reinforcement learning is still missing. In this section, we propose such an approach.

2.2 E-VALUES

We propose a novel approach for directed exploration, based on two parallel MDPs. One MDP is the
original MDP, which is used to estimate the value function. The second MDP is identical except for
one important difference. We posit that there are no rewards associated with any of the state-actions.
Thus, the true value of all state-action pairs is 0. We will use an RL algorithm to ”learn” the ”action-
values” in this new MDP which we denote as E-values. We will show that these E-values represent
the missing knowledge and thus can be used for propagating directed exploration. This will be
done by initializing E-values to 1. These positive initial conditions will subsequently result in an
optimistic bias that will lead to directed exploration, by giving high estimations only to state-action
pairs from which an optimistic outcome has not yet been excluded by the agent’s experience.

→

,

S

A

, P, R, γ) we construct a new MDP M (cid:48) = (

, P, 0, γE)
Formally, given an MDP M = (
with 0 denoting the identically zero function, and 0
γE < 1 is a discount parameter. The agent
now learns both Q and E values concurrently, while initially E (s, a) = 1 for all s, a. Clearly,
E∗ = 0. However intuitively, the value of E (s, a) at a given timestep during training stands
for the knowledge, or uncertainty, that the agent has regarding this state-action pair. Eventually,
after enough exploration, there is no additional knowledge left to discover which corresponds to
E (s, a)

E∗ (s, a) = 0.

A

≤

S

,

For learning E, we use the SARSA algorithm (Rummery & Niranjan, 1994; Sutton & Barto, 1998)
which differs from Watkin’s Q-Learning by being on-policy, following the update rule:

Where αE is the learning rate. For simplicity, we will assume throughout the paper that αE = α.

E (st, at)

(1

αE) E (st, at) + αE (r + γEE (st+1, at+1))

←

−

this learning rule updates the E-values based on E (st+1, at+1) rather

Note that
than
maxa E (st+1, a), thus not considering potentially highly informative actions which are never se-
lected. This is important for guaranteeing that exploration values will decrease when repeating the
same trajectory (as we will show below). Maintaining these additional updates doesn’t affect the
asymptotic space/time complexity of the learning algorithm, since it is simply performing the same
updates of a standard Q-Learning process twice.

2.3 E-VALUES AS GENERALIZED COUNTERS

The logarithm of E-Values can be thought of as a generalization of visit counters, with propagation
of the values along state-action pairs. To see this, let us examine the case of γE = 0 in which there
is no propagation from future states. In this case, the update rule is given by:

E (s, a)

(1

←

−

α) E (s, a) + α (0 + γEE (s(cid:48), a(cid:48))) = (1

α) E (s, a)

So after being visited n times, the value of the state-action pair is (1
rate. By taking a logarithm transformation, we can see that log1
a terminal state with one action, log1

−
α (E) = n for any value of γE.

−

−

−
α)n, where α is the learning
α (E) = n. In addition, when s is

3

Published as a conference paper at ICLR 2018

Figure 1: Left: Tree MDP, with k leaves. Tree: log1
α E (s, start) as function of visit cycles, for
different trees of k leaves (color coded). For each k, a cycle consists of visiting all leaves, hence k
visits of the start action. log1
α E behaves as a generalized counter, where each cycle contributes
approximately one generalized visit.

−

−

When γE > 0 and for non-terminal states, E will decrease more slowly and therefore log1
α E
will increase more slowly than a counter. The exact rate will depend on the MDP, the policy and
the speciﬁc value of γE. Crucially, for state-actions which lead to many potential states, each visit
contributes less to the generalized counter, because more visits are required to exhaust the potential
outcomes of the action. To gain more insight, consider the MDP depicted in Figure 1 left, a tree
with the root as initial state and the leaves as terminal states. If actions are chosen sequentially, one
leaf after the other, we expect that each complete round of choices (which will result with k actual
visits of the (s, start) pair) will be roughly equivalent to one generalized counter. Simulation of this
and other simple MDPs show that E-values behave in accordance with such intuitions (see Figure 1
right).

−

An important property of E-values is that they decrease over repetitions. Formally, by completing
a trajectory of the form s0, a0, . . . , sn, an, s0, a0 in the MDP, the maximal value of E (si, ai) will
decrease. To see this, assume that E (si, ai) was maximal, and consider its value after the update:

E (si, ai)

(1

α) E (si, ai) + αγEE (si+1, ai+1)

−
←
Because γE < 1 and E (si+1, ai+1)
E (si, ai), we get that after the update, the value of E (si, ai)
decreased. For any non-maximal (sj, aj), its value after the update is a convex combination of its
previous value and γEE (sk, ak) which is not larger than its composing terms, which in turn are
smaller than the maximal E-value.

≤

3 APPLYING E-VALUES

The logarithm of E-values can be considered as a generalization of counters. As such, algorithms
that utilize counters can be generalized to incorporate E-values. Here we consider two such gener-
alizations.

3.1 E-VALUES AS REWARD EXPLORATION BONUS

In model-based RL, counters have been used to create an augmented reward function. Motivated
by this result, augmenting the reward with a counter-based exploration bonus has also been used in
model-free RL (Storck et al., 1995; Bellemare et al., 2016). E-Values can naturally generalize this
approach, by replacing the standard counter with its corresponding generalized counter (log1
α E).
To demonstrate the advantage of using E-values over standard counters, we tested an (cid:15)-greedy agent
log1−α E added to the observed reward on the bridge MDP (Figure
with an exploration bonus of
2). To measure the learning progress and its convergence, we calculated the mean square error

−

1

4

Published as a conference paper at ICLR 2018

Figure 2: Bridge
MDP

Figure 3: MSE between Q and Q∗ on optimal policy per episode.
Convergence of (cid:15)-greedy on the short bridge environment (k = 5)
with and without exploration bonuses added to the reward. Note the
logarithmic scale of the abscissa.

E

(cid:104)
(Q (s, a)

Q∗ (s, a))2(cid:105)

−

π∗)
P (s,a
|

, where the average is over the probability of state-action pairs
when following the optimal policy π∗. We varied the value of γE from 0 – resulting effectively in
standard counters – to γE = 0.9. Our results (Figure 3) show that adding the exploration bonus to
the reward leads to faster learning. Moreover, the larger the value of γE in this example the faster
the learning, demonstrating that generalized counters signiﬁcantly outperforming standard counters.

3.2 E-VALUES AND ACTION-SELECTION RULES

Another way in which counters can be used to assist exploration is by adding them to the estimated
Q-values. In this framework, action-selection is a function not only of the Q-values but also of
the counters. Several such action-selection rules have been proposed (Thrun, 1992; Meuleau &
Bourgine, 1999; Kolter & Ng, 2009). These usually take the form of a deterministic policy that
maximizes some combination of the estimated Q-value with a counter-based exploration bonus. It
is easy to generalize such rules using E-values – simply replace the counters C by the generalized
counters log1

α (E).

−

3.2.1 DETERMINIZATION OF STOCHASTIC DECISION RULES

Here, we consider a special family of action-selection rules that are derived as deterministic equiv-
alents of standard stochastic rules. Stochastic action-selection rules are commonly used in RL. In
their simple form they include rules such as the (cid:15)-greedy or Softmax exploration described above.
In this framework, exploratory behavior is achieved by stochastic action selection, independent of
past choices. At ﬁrst glance, it might be unclear how E-values can contribute or improve such rules.
We now turn to show that, by using counters, for every stochastic rule there exist equivalent deter-
ministic rules. Once turned to deterministic counter-based rules, it is again possible improve them
using E-values.

The stochastic action-selection rules determine the frequency of choosing the different actions in the
limit of a large number of repetitions, while abstracting away the speciﬁc order of choices. This fact
is a key to understanding the relation between deterministic and stochastic rules. An equivalence
of two such rules can only be an in-the-limit equivalence, and can be seen as choosing a speciﬁc
realization of sample from the distribution. Therefore, in order to derive a deterministic equivalent
of a given stochastic rule, we only have to make sure that the frequencies of actions selected under
both rules are equal in the limit of inﬁnitely many steps. As the probability for each action is likely
to depend on the current Q-values, we have to consider ﬁxed Q-values to deﬁne this equivalence.

5

Published as a conference paper at ICLR 2018

We prove that given a stochastic action-selection rule f (a
s), every deterministic policy that does
|
not choose an action that was visited too many times until now (with respect to the expected number
according to the probability distribution) is a determinization of f . Formally, lets assume that given
a certain Q function and state s we wish a certain ratio between different choices of actions a
A to
s). For brevity we assume s and Q are constants
hold. We denote the frequency of this ratio fQ (a
|
s) = f (a). We also assume a counter C (s, a) is kept denoting the number of
and denote fQ (a
choices of a in s. For brevity we denote C (s, a) = C (a) and (cid:80)
|
a C (s, a) = C. When we look at
the counters after T steps we use subscript CT (a). Following this notation, note that CT = T .
Theorem 3.1. For any sub-linear function b (t) and for any deterministic policy which chooses at
step T an action a such that CT (a)

b (t) it holds that

f (a)

∈

a

T −

≤
CT (a)
T

lim
T
→∞

= f (a)

∀

∈ A

Proof. For a full proof of the theorem see Appendix A in the supplementary materials

The result above is not a vacuous truth – we now provide two possible determinization rules that
C(a)
achieves it. One rule is straightforward from the theorem, using b = 0, choosing arg mina
C −
f (a). Another rule follows the probability ratio between the stochastic policy and the empirical dis-
f (a)
tribution: arg maxa
C(a) . We denote this determinization LLL, because when generalized counters
are used instead of counters it becomes arg maxa logf (s, a)
αE (s, a).
Now we can replace the visit counters C (s, a) with the generalized counters log1
α (E (s, a)) to
create Directed Outreaching Reinforcement Action-Selection – DORA the explorer. By this, we can
transform any stochastic or counter-based action-selection rule into a deterministic rule in which
exploration propagates over the states and the expected trajectories to follow.

loglog1

−

−

−

Input: Stochastic action-selection rule f , learning rate α, Exploration discount factor γE
initialize Q (s, a) = 0, E (s, a) = 1;
foreach episode do

init s;
while not terminated do

Choose a = arg maxx log fQ (x
s)
|
Observe transitions (s, a, r, s(cid:48), a(cid:48));
Q (s, a)
E (s, a)

−
α) Q (s, a) + α (r + γ maxx Q (s(cid:48), x));
α) E (s, a) + αγEE (s(cid:48), a(cid:48));

log log1

α E (s, x);

(1
(1

−

←
←

−
−

end

end

Algorithm 1: DORA algorithm using LLL determinization for stochastic policy f

3.3 RESULTS – FINITE MDPS

To test this algorithm, the ﬁrst set of experiments were done on Bridge environments of various
lengths k (Figure 2). We considered the following agents: (cid:15)-greedy, Softmax and their respective
LLL determinizations (as described in 3.2.1) using both counters and E-values. In addition, we
compared a more standard counter-based agent in the form of a UCB-like algorithm (Auer et al.,
C . We tested two variants
2002) following an action-selection rule with exploration bonus of
of this algorithm, using ordinary visit counters and E-values. Each agent’s hyperparameters ((cid:15) and
temperature) were ﬁtted separately to optimize learning. For stochastic agents, we averaged the
results over 50 trials for each execution. Unless stated otherwise, γE = 0.9.

(cid:113) log t

We also used a normalized version of the bridge environment, where all rewards are between 0 and
1, to compare DORA with the Delayed Q-Learning algorithm (Strehl et al., 2006).

Our results (Figure 4) demonstrate that E-value based agents outperform both their counter-based
and their stochastic equivalents on the bridge problem. As shown in Figure 4, Stochastic and
counter-based (cid:15)-greedy agents, as well as the standard UCB fail to converge. E-value agents are
the ﬁrst to reach low error values, indicating that they learn faster. Similar results were achieved

6

Published as a conference paper at ICLR 2018

Figure 4: MSE between Q and Q∗ on optimal policy per episode. Convergence measure of all
agents, long bridge environment (k = 15). E-values agents are the ﬁrst to converge, suggesting
their superior learning abilities.

on other gridworld environments, such as the Cliff problem (Sutton & Barto, 1998) (not shown).
We also achieved competitive results with respect to Delayed Q Learning (see supplementary B and
Figure 7 there).

The success of E-values based learning relative to counter based learning implies that the use of
E-values lead to more efﬁcient exploration. If this is indeed the case, we expect E-values to better
represent the agent’s missing knowledge than visit counters during learning. To test this hypothesis
we studied the behavior of an E-value LLL Softmax on a shorter bridge environment (k = 5). For
a given state-action pair, a measure of the missing knowledge is the normalized distance between its
(cid:12)
(cid:12)
(cid:12) for
estimated value (Q) and its optimal-policy value (Q∗). We recorded C, log1
each s, a at the end of each episode. Generally, this measure of missing knowledge is expected to
be a monotonously-decreasing function of the number of visits (C). This is indeed true, as depicted
in Figure 5 (left). However, considering all state-action pairs, visit counters do not capture well the
amount of missing knowledge, as the convergence level depends not only on the counter but also on
the identity of the state-action it counts. By contrast, considering the convergence level as a function
of the generalized counter (Figure 5, right) reveals a strikingly different pattern.
Independently
of the state-action identity, the convergence level is a unique function of the generalized counter.
These results demonstrate that generalized counters are a useful measure of the amount of missing
knowledge.

α (E) and

−
Q∗

Q∗

(cid:12)
(cid:12)
(cid:12)

Q

−

4 E-VALUES WITH FUNCTION APPROXIMATION

So far we discussed E-values in the tabular case, relying on ﬁnite (and small) state and action spaces.
However, a main motivation for using model-free approach is that it can be successfully applied in
large MDPs where tabular methods are intractable. In this case (in particular for continuous MDPs),
achieving directed exploration is a non-trivial task. Because revisiting a state or a state-action pair
is unlikely, and because it is intractable to store individual values for all state-action pairs, counter-
based methods cannot be directly applied. In fact, most implementations in these cases adopt simple
exploration strategies such as (cid:15)-greedy or softmax (Bellemare et al., 2016).

There are standard model-free techniques to estimate value function in function-approximation sce-
narios. Because learning E-values is simply learning another value-function, the same techniques
can be applied for learning E-values in these scenarios. In this case, the concept of visit-count –
or a generalized visit-count – will depend on the representation of states used by the approximating
function.

To test whether E-values can serve as generalized visit-counters in the function-approximation case,
we used a linear approximation architecture on the MountainCar problem (Moore, 1990) (Appendix

7

Published as a conference paper at ICLR 2018

C). To dissociate Q and E-values, actions were chosen by an (cid:15)-greedy agent independently of E-
values. As shown in Appendix C, E-values are an effective way for counting both visits and gen-
eralized visits in continuous MDPs. For completeness, we also compared the performance of LLL
agents to stochastic agents on a sparse-reward MountainCar problem, and found that LLL agents
learns substantially faster than the stochastic agents (Appendix D).

4.1 RESULTS – FUNCTION APPROXIMATION

To show our approach scales to complex problems, we used the Freeway Atari 2600 game, which is
known as a hard exploration problem (Bellemare et al., 2016). We trained a neural network with two
streams to predict the Q and E-values. First, we trained the network using standard DQN technique
(Mnih et al., 2015), which ignores the E-values. Second, we trained the network while adding an
exploration bonus of
to the reward (In all reported simulations, β = 0.05). In both cases,
action-selection was performed by an (cid:15)-greedy rule, as in Bellemare et al. (2016).

β
log E

√

−

Note that the exploration bonus requires 0 < E < 1. To satisfy this requirement, we applied a
logistic activation fucntion on the output of the last layer of the E-value stream, and initialized the
weights of this layer to 0. As a result, the E-values were initialized at 0.5 and satisﬁed 0 < E < 1
throughout the training. In comparison, no non-linearity was applied in the last layer of the Q-value
stream and the weights were randmoly initialized.

We compared our approach to a DQN baseline, as well as to the density model counters suggested
by (Bellemare et al., 2016). The baseline used here does not utilize additional enhancements (such
as Double DQN and Monte-Carlo return) which were used in (Bellemare et al., 2016). Our results,
depicted in Figure 6, demonstrate that the use of E-values outperform both DQN and density model
counters baselines. In addition, our approach results in better performance than in (Bellemare et al.,
106 steps, instead of
2016) (with the mentioned enhancements), converging in approximately 2
10

106 steps2.

·

·

5 RELATED WORK

The idea of using reinforcement-learning techniques to estimate exploration can be traced back to
Storck et al. (1995) and Meuleau & Bourgine (1999) who also analyzed propagation of uncertain-
ties and exploration values. These works followed a model-based approach, and did not fully deal
with the problem of non-Markovity arising from using exploration bonus as the immediate reward.
A related approach was used by Little & Sommer (2014), where exploration was investigated by
information-theoretic measures. Such interpretation of exploration can also be found in other works
(Schmidhuber (1991); Sun et al. (2011); Houthooft et al. (2016)).

Efﬁcient exploration in model-free RL was also analyzed in PAC-MDP framework, most notably
the Delayed Q Learning algorithm by Strehl et al. (2006). For further discussion and comparison of
our approach with Delayed Q Learning, see 1.1 and Appendix B.

In terms of generalizing Counter-based methods, there has been some works on using counter-like
notions for exploration in continuous MDPs (Nouri & Littman, 2009). A more direct attempt was
recently proposed by Bellemare et al. (2016). This generalization provides a way to implement visit
counters in large, continuous state and action spaces by using density models. Our generalization is
different, as it aims ﬁrst on generalizing the notion of visit counts themselves, from actual counters to
”propagating counters”. In addition, our approach does not depend on any estimated model – which
might be an advantage in domains for which good density models are not available. Nevertheless,
we believe that an interesting future work will be comparing between the approach suggested by
Bellemare et al. (2016) and our approach, in particular for the case of γE = 0.

2We used an existing implementation for DQN and density-model

at
https://github.com/brendanator/atari-rl. Training with density-model counters was an order of magnitude
slower than training with two-streamed network for E-values

available

counters

8

Published as a conference paper at ICLR 2018

Figure 5: Convergence of Q to Q∗ for individual state-action pairs (each denoted by a different
color), with respect to counters (left) and generalized counters (right). Results obtained from E-
Value LLL Softmax on the short bridge environment (k = 5). Triangle markers indicate pairs with
”east” actions, which constitute the optimal policy of crossing the bridge. Circle markers indicate
state-action pairs that are not part of the optimal policy. Generalized counters are a useful measure
of the amount of missing knowledge.

Figure 6: Results on Freeway game. All agents used (cid:15)-greedy action-selection rule without explo-
ration bonus (DQN, blue), with a bonus term based on density model counters (Density, orange)
added to the reward, or with bonus term based on E-values (black).

6 ACKNOWLEDGMENTS

We thank Nadav Cohen, Leo Joskowicz, Ron Meir, Michal Moshkovitz, and Jeff Rosenschein for
discussions. This work was supported by the Israel Science Foundation (Grant No. 757/16) and the
Gatsby Charitable Foundation.

9

Published as a conference paper at ICLR 2018

REFERENCES

Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit

problem. Machine learning, 47(2-3):235–256, 2002.

Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation. In D. D. Lee, M. Sugiyama, U. V.
Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems
29, pp. 1471–1479. Curran Associates, Inc., 2016.

Lucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent rein-
forcement learning. IEEE Transactions on Systems, Man, And Cybernetics-Part C: Applications
and Reviews, 38 (2), 2008, 2008.

Arthur Guez, David Silver, and Peter Dayan. Efﬁcient bayes-adaptive reinforcement learning using
sample-based search. In Advances in Neural Information Processing Systems, pp. 1025–1033,
2012.

Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime:
Variational information maximizing exploration. In Advances in Neural Information Processing
Systems, pp. 1109–1117, 2016.

Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A

survey. Journal of artiﬁcial intelligence research, 4:237–285, 1996.

Sham Machandranath Kakade et al. On the sample complexity of reinforcement learning. PhD

thesis, 2003.

J Zico Kolter and Andrew Y Ng. Near-bayesian exploration in polynomial time. In Proceedings of

the 26th Annual International Conference on Machine Learning, pp. 513–520. ACM, 2009.

Daniel Y Little and Friedrich T Sommer. Learning and exploration in action-perception loops.

Closing the Loop Around Neural Systems, pp. 295, 2014.

Nicolas Meuleau and Paul Bourgine. Exploration of multi-state environments: Local measures and

back-propagation of uncertainty. Machine Learning, 35(2):117–154, 1999.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.

Andrew William Moore. Efﬁcient memory-based learning for robot control. 1990.

Ali Nouri and Michael L Littman. Multi-resolution exploration in continuous spaces. In Advances

in neural information processing systems, pp. 1209–1216, 2009.

Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems. Uni-

versity of Cambridge, Department of Engineering, 1994.

J¨urgen Schmidhuber. Curious model-building control systems.

In Neural Networks, 1991. 1991

IEEE International Joint Conference on, pp. 1458–1463. IEEE, 1991.

Jan Storck, Sepp Hochreiter, and J¨urgen Schmidhuber. Reinforcement driven information acquisi-
tion in non-deterministic environments. In Proceedings of the international conference on artiﬁ-
cial neural networks, Paris, volume 2, pp. 159–164. Citeseer, 1995.

Alexander L Strehl and Michael L Littman. An analysis of model-based interval estimation for
markov decision processes. Journal of Computer and System Sciences, 74(8):1309–1331, 2008.

Alexander L Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L Littman. Pac model-
In Proceedings of the 23rd international conference on Machine

free reinforcement learning.
learning, pp. 881–888. ACM, 2006.

10

Published as a conference paper at ICLR 2018

Yi Sun, Faustino Gomez, and J¨urgen Schmidhuber. Planning to be surprised: Optimal bayesian
exploration in dynamic environments. In International Conference on Artiﬁcial General Intelli-
gence, pp. 41–51. Springer, 2011.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 1998.

Sebastian B. Thrun. Efﬁcient exploration in reinforcement learning, 1992.

Michel Tokic and G¨unther Palm. Value-difference based exploration: adaptive control between
In KI 2011: Advances in Artiﬁcial Intelligence, pp. 335–346.

epsilon-greedy and softmax.
Springer, 2011.

Thomas J Walsh, Istv´an Szita, Carlos Diuk, and Michael L Littman.

Exploring compact
reinforcement-learning representations with linear regression. In Proceedings of the Twenty-Fifth
Conference on Uncertainty in Artiﬁcial Intelligence, pp. 591–598. AUAI Press, 2009.

Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992.

11

Published as a conference paper at ICLR 2018

A PROOF OF THE DETERMINIZATION THEOREM

The proof for the determinization mentioned in the paper is achieved based on the following lem-
mata.
Lemma A.1. The absolute sum of positive and negative differences between the empiric distribution
(deterministic frequency) and goal distribution (non-deterministic frequency) is equal.
C (a)
C

C (a)
C

f (a)

f (a)

(cid:88)

(cid:88)

=

−

−

−

a:f (a)

C(a)
C

≥

a:f (a)< C(a)

C

Proof. Straightforward from the observation that

(cid:88)

a

f (a) =

(cid:88)

a

C (a)
C

= 1

Lemma A.2. For any t

(cid:26) Ct (a)
t

max
a

(cid:27)

− f (a)

≤

1 + b (t)
t

Proof. The proof of A.2 is done by induction. For t = 1

∀a ∈ A :

Ct (a)
t

− f (a) = max

a

(cid:26) Ct (a)
t

(cid:27)

− f (a)

Hence we look at a

A.

∈

Ct (a)
t

− f (a) ≤

Ct (a)
t

≤

1 + b (1)
1

CT +1 (a)
T + 1

− f (a) =

CT (a) + 1
T + 1

− f (a)

assume the claim is true for t = T then for t = T + 1 There exists a such that CT (a) /T
b (t) which the algorithm chooses for this a. For it

−

f (a)

≤

It also holds that

a(cid:48)

∀

∈

A s.t. a(cid:48)

= a
CT +1 (a)
T + 1

− f (a) =

1
T + 1

− f (a) +

CT (a)
T + 1
CT (a) − (T + 1) f (a)
T + 1

+

1
T + 1

=

=

≤

1 + b (t)
T + 1

− f (a)

CT (a)
T + 1
CT (a) − (T + 1) f (a)
T + 1

CT (a) − T f (a)
T + 1

1 + b (t)
T + 1

=

<

≤

CT (a)
T

lim
T
→∞

= f (a)

12

Proof of 3.1. It holds from A.2 together with A.1 that in the step t in the worst case all but one of
the actions have Ct(a)
. So by the bound
t −
on sum of positives and negatives we get:

t and the last action has f (a)

f (a) = 1

t =

Ct(a)

A
|

|−
t

−

−

1

Published as a conference paper at ICLR 2018

Figure 7: Normalized MSE between Q and Q∗ on optimal policy per episode. Convergence of
E-value LLL and Delayed Q-Learning on, normalized bridge environment (k = 15). MSE was
noramlized for each agent to enable comparison.

B COMPARISON WITH DELAYED Q-LEARNING

Because Delayed Q learning initializes its values optimistically, which result in a high MSE, we
normalized the MSE of the two agents (separately) to enable comparison. Notably, to achieve this
performance by the Delayed Q Learning, we had to manually choose a low value for m (in Figure 7,
m = 10), the hyperparameter regulating the number of visits required before any update. This is an
order of magnitude smaller than the theoretical value required for even moderate PAC-requirements
in the usual notion of (cid:15), δ, such m also implies learning in orders of magnitudes slower. In fact,
for this limit of m
1 the algorithm is effectively quite similar to a ”Vanilla” Q-Learning with
an optimistic initialization, which is possible due to the assumption made by the algorithm that all
rewards are between 0 and 1. In fact, several exploration schemes relying on optimism in the face
of uncertainty were proposed (Walsh et al., 2009). However, because our approach separate reward
values and exploratory values, we are able to use optimism for the latter without assuming any prior
knowledge about the ﬁrst – while still achieving competitive results to an optimistic initialization
based on prior knowledge.

→

C EVALUATING E-VALUES DYNAMICS IN FUNCTION-APPROXIMATION

To gain insight into the relation between E-values and number of visits, we used the linear-
approximation architecture on the MountainCar problem. Note that when using E-values, they are
generally correlated with visit counts both because visits result in update of the E-values through
learning and because E-values affect visits through the exploration bonus (or action-selection rule).
To dissociate the two, Q-values and E-values were learned in parallel in these simulation, but action-
selection was independent of the E-values. Rather, actions were chosen by an (cid:15)-greedy agent. To
estimate visit-counts, we recorded the entire set of visited states, and computed the empirical visits
histogram by binning the two-dimensional state-space. For each state, its visit counter estimator
˜C (s) is the value of the matching bin in the histogram for this state. In addition, we recorded the
learned model (weights vector for E-values) and computed the E-values map by sampling a state
for each bin, and calculating its E-values using the model. For simplicity, we consider here the
resolution of states alone, summing over all 3 actions for each state. That is, we compare ˜C (s)
to (cid:80)
α E (s, a) = CE (s). Figure 8 depicts the empirical visits histogram (left) and the es-
timated E-values for the case of γE = 0 after the complete training. The results of the analysis
show that, roughly speaking, those regions in the state space that were more often visited, were also
associated with a higher CE (s).

a log1

−

13

Published as a conference paper at ICLR 2018

Figure 8: Empirical visits histogram (left) and learned CE (right) after training, γE = 0.

To better understand these results, we considered smaller time-windows in the learning process.
Speciﬁcally, Figure 9 depicts the empirical visit histogram (left), and the corresponding CE (s)
(right) in the ﬁrst 10 episodes, in which visits were more centrally distributed. Figure 10 depicts
the change in the empirical visit histogram (left), and change in the corresponding CE (s) (right) in
the last 10 episodes of the training, in which visits were distributed along a spiral (forming an near-
optimal behavior). These results demonstrate high similarity between visit-counts and the E-value
representation of them, indicating that E-values are good proxies of visit counters.

Figure 9: Empirical visits histogram (left) and learned CE (right) in the ﬁrst 10 training episodes,
γE = 0.

14

Published as a conference paper at ICLR 2018

Figure 10: Difference in empirical visits histogram (left) and learned CE (right) in the last 10 train-
ing episodes, γE = 0.

The results depicted in Figures 9 and 10 were achieved with γE = 0. For γE > 0, we expect the
generalized counters (represented by E-values) to account not for standard visits but for ”generalized
visits”, weighting the trajectories starting in each state. We repeated the analysis of Figure 10 for
the case of γE = 0.99. Results, depicted in Figure 11, shows that indeed for terminal or near-
terminal states (where position> 0.5) generalized visits, measured by difference in their generalized
counters, are higher – comparing to far-from terminal states – than the empirical visits of these states
(comparing to far-from terminal states).

Figure 11: Difference in empirical visits histogram (left) and learned CE (right) in the last 10 train-
ing episodes, γE = 0.99. Note that the results are based on a different simulation than those in
Figure 10.

To quantify the relation between visits and E-values, we densely sampled the (achievable) state-
space to generate many examples of states. For each sampled state, we computed the correlation
coefﬁcient between CE (s) and ˜C (s) throughout the learning process (snapshots taken each 10
episodes). The values ˜C (s) were estimated by the empirical visits histogram (value of the bin
corresponding to the sampled state) calculated based on visits history up to each snapshot. Figure
12, depicting the histogram of correlation coefﬁcients between the two measures, demonstrating
strong positive correlations between empirical visit-counters and generalized counters represented
by E-values. These results indicate that E-values are an effective way for counting effective visits
in continuous MDPs. Note that the number of model parameters used to estimate E (s, a) in this
case is much smaller than the size of the table we would have to use in order to track state-action
counters in such binning resolution.

15

Published as a conference paper at ICLR 2018

Figure 12: Histogram of correlation coefﬁcients between empirical visit counters and CE throughout
training, per state (γE = 0).

D RESULTS ON CONTINUOUS MDPS – MOUNTAINCAR

To test the performance of E-values based agents, simulations were performed using the Mountain-
Car environment. The version of the problem considered here is with sparse and delayed reward,
meaning that there is a constant reward of 0 unless reaching a goal state which provides a reward
of magnitude 1. Episode length was limited to 1000 steps. We used linear approximation with tile-
coding features (Sutton & Barto, 1998), learning the weights vectors for Q and E in parallel. To
guarantee that E-values are uniformly initialized and are kept between 0 and 1 throughout learning,
we initialized the weights vector for E-values to 0 and added a logistic non-linearity to the results
of the standard linear approximation. In contrast, the Q-values weights vector was initialized at
random, and there was no non-linearity. We compared the performance of several agents. The ﬁrst
two used only Q-values, with a softmax or an (cid:15)-greedy action-selection rules. The other two agents
are the DORA variants using both Q and E values, following the LLL determinization for softmax
either with γE = 0 or with γE = 0.99. Parameters for each agent (temperature and (cid:15)) were ﬁt-
ted separately to maximize performance. The results depicted in Figure 13 demonstrate that using
E-values with γE > 0 lead to better performance in the MountainCar problem

In addition we tested our approach using (relatively simple) neural networks. We trained two neural
networks in parallel (unlike the two-streams single network used for Atari simulations), for pre-
dicting Q and E values.
In this architecture, the same technique of 0 initializing and a logistic
non-linearity was applied to the last linear of the E-network. Similarly to the linear approximation
approach, E-values based agents outperform their (cid:15)-greedy and softmax counterparts (not shown).

16

Published as a conference paper at ICLR 2018

Figure 13: Probability of reaching goal on MountainCar (computed by averaging over 50 simula-
tions of each agent), as a function of training episodes. While Softmax exploration fails to solve the
problem within 1000 episodes, LLL E-values agents with generalized counters (γE > 0) quickly
reach high success rates.

17

