7
1
0
2
 
y
a
M
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
0
8
8
7
0
.
5
0
7
1
:
v
i
X
r
a

Reducing Reparameterization Gradient Variance

Andrew C. Miller∗, Nicholas J. Foti† , Alexander D’Amour§ , and Ryan P. Adams‡

Abstract: Optimization with noisy gradients has become ubiquitous in statistics and
machine learning. Reparameterization gradients, or gradient estimates computed via the
“reparameterization trick,” represent a class of noisy gradients often used in Monte Carlo
variational inference (MCVI). However, when these gradient estimators are too noisy, the
optimization procedure can be slow or fail to converge. One way to reduce noise is to use
more samples for the gradient estimate, but this can be computationally expensive. Instead,
we view the noisy gradient as a random variable, and form an inexpensive approximation of
the generating procedure for the gradient sample. This approximation has high correlation
with the noisy gradient by construction, making it a useful control variate for variance
reduction. We demonstrate our approach on non-conjugate multi-level hierarchical mod-
els and a Bayesian neural net where we observed gradient variance reductions of multiple
orders of magnitude (20-2,000×).

1. Introduction

Representing massive datasets with ﬂexible probabilistic models has been central to the success
of many statistics and machine learning applications, but the computational burden of ﬁtting
these models is a major hurdle. For optimization-based ﬁtting methods, a central approach to this
problem has been replacing expensive evaluations of the gradient of the objective function with
cheap, unbiased, stochastic estimates of the gradient. For example, stochastic gradient descent
using small mini-batches of (conditionally) i.i.d. data to estimate the gradient at each iteration is
a popular approach with massive data sets. Alternatively, some learning methods sample directly
from a generative model or approximating distribution to estimate the gradients of interest, for
example, in learning algorithms for implicit models Mohamed and Lakshminarayanan [2016],
Tran et al. [2017] and generative adversarial networks Arjovsky et al. [2017], Goodfellow et al.
[2014].

Approximate Bayesian inference using variational techniques (variational inference, or VI) has
also motivated the development of new stochastic gradient estimators, as the variational approach
reframes the integration problem of inference as an optimization problem Blei et al. [2017]. VI ap-
proaches seek out the distribution from a well-understood variational family of distributions that
best approximates an intractable posterior distribution. The VI objective function itself is often
intractable, but recent work has shown that it can be optimized with stochastic gradient methods
that use Monte Carlo estimates of the gradient Kingma and Welling [2014], Ranganath et al.
[2014], Rezende et al. [2014], We call this approach Monte Carlo variational inference (MCVI). In
MCVI, generating samples from an approximate posterior distribution is the source of gradient
stochasticity. Alternatively, stochastic variational inference (SVI) Hoﬀman et al. [2013] and other
stochastic optimization procedures induce stochasticity through data subsampling; MCVI can be
augmented with data subsampling to accelerate computation for large data sets.

∗Harvard University, e-mail: acm@seas.harvard.edu, url: http://andymiller.github.io
†University of Washington, e-mail: nfoti@uw.edu
‡Harvard University, Google Brain e-mail: rpa@seas.harvard.edu
§UC Berkeley e-mail: alexdamour@berkeley.edu

1

A.C. Miller et al./Reducing Reparameterization Gradient Variance

2

The two commonly used MCVI gradient estimators are the score function gradient Ranganath
et al. [2014] and the reparameterization gradient Kingma and Welling [2014], Rezende et al.
[2014]. Broadly speaking, score function estimates can be applied to both discrete and contin-
uous variables, but often have high variance and thus are frequently used in conjunction with
variance reduction techniques. On the other hand, the reparameterization gradient often has
lower variance, but is restricted to continuous random variables. See Ruiz et al. [2016] for a uni-
fying perspective on these two estimators. Like other stochastic gradient methods, the success of
MCVI depends on controlling the variance of the stochastic gradient estimator.

√

In this work, we present a novel approach to controlling the variance of the reparameterization
gradient estimator in MCVI. Existing MCVI methods control this variance na¨ıvely by averaging
several gradient estimates, which becomes expensive for large data sets and complex models,
N ). Our approach exploits the fact that, in MCVI,
with error that only diminishes as O(1/
the randomness in the gradient estimator is completely determined by a known Monte Carlo
generating process; this allows us to leverage knowledge about this generating process to de-noise
the gradient estimator. In particular, we construct a cheaply computed control variate based on
an analytical linear approximation to the gradient estimator. Taking a linear combination of a
na¨ıve gradient estimate with this control variate yields a new estimator for the gradient that
remains unbiased but has lower variance. We apply the idea to Gaussian approximating families
and measure the reduction in gradient variance under various conditions. We observe a 20-2,000
× reduction in variance of the gradient norm in some conditions, and much faster convergence
and more stable behavior of optimization traces.

2. Background

Variational Inference Given a model, p(z, D) = p(D|z)p(z), of data D and parameters/latent
variables z, the goal of VI is to approximate the posterior distribution p(z|D). VI approximates
this intractable posterior distribution with one from a simpler family, Q = {q(z; λ), λ ∈ Λ},
parameterized by variational parameters λ. VI procedures seek out the member of that fam-
ily, q(·; λ) ∈ Q, that minimizes some divergence between the approximation q and the true pos-
terior p(z|D).

Variational inference can be framed as an optimization problem, usually in terms of Kullback-
Leibler (KL) divergence, of the following form

λ∗ = arg min

KL(q(z; λ) || p(z|D)) = arg min

Ez∼qλ [ln q(z; λ) − ln p(z|D)] .

λ∈Λ

λ∈Λ

The task is to ﬁnd a setting of λ that makes q(z; λ) close to the posterior p(z|D) in KL diver-
gence.1 Directly computing the KL divergence requires evaluating the posterior itself; therefore,
VI procedures use the evidence lower bound (ELBO) as the optimization objective

L(λ) = Ez∼qλ [ln p(z, D) − ln q(z; λ)]

(1)

which, when maximized, minimizes the KL divergence between q(z; λ) and p(z|D).

To maximize the ELBO with gradient methods, we need to compute the gradient of the ELBO, gλ.
The gradient inherits the ELBO’s form as an expectation, which is in general an intractable quan-
tity to compute. In this work, we focus on reparameterization gradient estimators (RGEs) com-
puted using the reparameterization trick. The reparameterization trick exploits the structure of

1We use q(z; λ) and qλ(z) interchangeably.

A.C. Miller et al./Reducing Reparameterization Gradient Variance

3

(a) step size = .05

(b) step size = .1

Fig 1: Optimization traces for MCVI applied to a Bayesian neural network with various hyper-
parameter settings. Each trace is running adam Kingma and Ba [2015]. The three lines in each
plot correspond to three diﬀerent numbers of samples, L, used to estimate the gradient at each
step. (Left) small stepsize; (Right) stepsize 10 times larger. Large step sizes allow for quicker
progress, however noisier (i.e., small L) gradients combined with large step sizes result in chaotic
optimization dynamics. The converging traces reach diﬀerent ELBOs due to the illustrative con-
stant learning rates; in practice, one decreases the step size over time in keeping with Robbins
and Monro [1951].

the variational data generating procedure — the mechanism by which z is simulated from qλ(z).
To compute the RGE, we ﬁrst express the sampling procedure from qλ(z) as a diﬀerentiable
map

(cid:15) ∼ q0((cid:15))
z = T ((cid:15); λ)

independent of λ

diﬀerentiable map

where the initial distribution q0 and T are jointly deﬁned such that z ∼ q(z; λ) has the de-
sired distribution. As a simple concrete example, if we set q(z; λ) to be a diagonal Gaussian
N (mλ, diag(s2
+ . The sampling procedure could
then be deﬁned as

λ)) where λ = [mλ, sλ], mλ ∈ RD, and sλ ∈ RD

(cid:15) ∼ N (0, ID) ,

z = T ((cid:15); λ) = mλ + sλ (cid:12) (cid:15)

where s (cid:12) (cid:15) denotes an element-wise product. We will also use x/y and x2 to denote pointwise
division and squaring, respectively. Given this map, the reparameterization gradient estimator
is simply the gradient of a Monte Carlo ELBO estimate with respect to λ. For a single sample,
this is

(cid:15) ∼ q0((cid:15)) ,

ˆgλ (cid:44) ∇λ [ln p(T ((cid:15); λ), D) − ln q(T ((cid:15); λ); λ)]

and the L-sample approximation can be computed by the sample average

(2)

(3)

(4)

(5)

ˆg(L)
λ =

ˆgλ((cid:15)(cid:96)).

1
L

L
(cid:88)

(cid:96)=1

Crucially, the reparameterization gradient is unbiased, E[ˆgλ] = ∇λL(λ), guaranteeing the con-
vergence of stochastic gradient optimization procedures that use it Robbins and Monro [1951].

A.C. Miller et al./Reducing Reparameterization Gradient Variance

4

Gradient Variance and Convergence The eﬃciency of Monte Carlo variational inference
hinges on the magnitude of gradient noise and the step size chosen for the optimization procedure.
When the gradient noise is large, smaller gradient steps must be taken to avoid unstable dynamics
of the iterates. However, a smaller step size increases the number of iterations that must be
performed to reach convergence.

We illustrate this trade-oﬀ in Figure 1, which shows realizations of an optimization proce-
dure applied to a Bayesian neural network using reparameterization gradients. The posterior
is over D = 653 parameters that we approximate with a diagonal Gaussian (see Appendix C.2).
We compare the progress of the Adam algorithm using various numbers of samples Kingma and
Ba [2015], ﬁxing the learning rate. The noise present in the single-sample estimator causes ex-
tremely slow convergence, whereas the lower noise 50-sample estimator quickly converges, albeit
at 50 times the cost.

The upshot is that with low noise gradients we are able to safely take larger steps, enabling
faster convergence to a local optimum. The natural question is, how can we reduce the variance
of gradient estimates without introducing too much extra computation? Our approach is to use
information about the variational model, q(·; λ) and carefully construct a control variate to the
gradient.

Control Variates Control variates are random quantities that are used to reduce the variance
of a statistical estimator without introducing any bias by injecting information into the estimator.
Given an unbiased estimator ˆg such that E[ˆg] = g (the quantity of interest), our goal is to
construct another unbiased estimator with lower variance. We can do this by deﬁning a control
variate ˜g with known expectation ˜m. We can write our new estimator as

g(cv) = ˆg − C(˜g − ˜m) .

(6)

where C ∈ RD×D for D-dimensional ˆg. Clearly the new estimator has the same expectation
as the original estimator, but a diﬀerent variance. We can attain optimal variance reduction
by appropriately setting C. Intuitively, the optimal C is very similar to a regression coeﬃcient
— it is related to the covariance between the control variate and the original estimator. See
Appendix A for further details on optimally setting C.

3. Method: Modeling Reparameterization Gradients

In this section we develop our main contribution, a new gradient estimator that can dramatically
reduce reparameterization gradient variance. In MCVI, the reparameterization gradient estimator
(RGE) is a Monte Carlo estimator of the true gradient — the estimator itself is a random variable.
This random variable is generated using the “reparameterization trick” — we ﬁrst generate
some randomness (cid:15) and then compute the gradient of the ELBO with respect to λ holding (cid:15)
ﬁxed. This results in a complex distribution from which we can generate samples, but in general
cannot characterize due to the complexity of the term arising from the gradient of the true
posterior.

However, we do have a lot of information about the sampling procedure — we know the vari-
ational distribution ln q(z; λ), the transformation T , and we can evaluate the model joint den-
sity ln p(z, D) pointwise. Furthermore, with automatic diﬀerentiation, it is often straightforward
to obtain gradients and Hessian-vector products of our model ln p(z, D). We propose a scheme
that uses the structure of qλ and curvature of ln p(z, D) to construct a tractable approximation of

A.C. Miller et al./Reducing Reparameterization Gradient Variance

5

the distribution of the RGE.2 This approximation has a known mean and is correlated with the
RGE distribution, allowing us to use it as a control variate to reduce the RGE variance.

Given a variational family parameterized by λ, we can decompose the ELBO gradient into a few
terms that reveal its “data generating procedure”

(cid:15) ∼ q0 ,

z = T ((cid:15); λ)

ˆgλ (cid:44) ˆg(z; λ) =

∂ ln p(z, D)
∂z
(cid:123)(cid:122)
data term

(cid:125)

(cid:124)

∂z
∂λ

−

∂ ln qλ(z)
∂z
(cid:123)(cid:122)
(cid:125)
(cid:124)
pathwise score

∂z
∂λ

−

∂ ln qλ(z)
∂λ
(cid:123)(cid:122)
parameter score

(cid:124)

(cid:125)

.

(7)

(8)

Certain terms in Eq. (8) have tractable distributions. The Jacobian of T (·; λ), given by ∂z/∂λ,
is deﬁned by our choice of q(z; λ). For some transformations T we can exactly compute the
distribution of the Jacobian given the distribution of (cid:15). The pathwise and parameter score terms
are gradients of our approximate distribution with respect to λ (via z or directly). If our approx-
imation is tractable (e.g., a multivariate Gaussian), we can exactly characterize the distribution
for these components.3

However, the data term in Eq. (8) involves a potentially complicated function of the latent
variable z (and therefore a complicated function of (cid:15)), resulting in a diﬃcult-to-characterize
distribution. Our goal is to construct an approximation to the distribution of ∂ ln p(z, D)/∂z and
its interaction with ∂z/∂λ given a ﬁxed distribution over (cid:15). If the approximation yields random
variables that are highly correlated with ˆgλ, then we can use it to reduce the variance of that
RGE sample.

Linearizing the data term To simplify notation, we write the data term of the gradient
as

f (z(cid:48)) (cid:44) ∂ ln p(z, D)

∂z

(cid:12)
(cid:12)
(cid:12)z=z(cid:48)

,

where f : RD (cid:55)→ RD since z ∈ RD. We then linearize f about some value z0
˜f (z) = f (z0) + [Jzf (z)] (z − z0) = f (z0) + H(z0)(z − z0)

where H(z0) is the Hessian of the model, ln p(z, D), with respect to z evaluated at z0,

H(z0) = Jz

(cid:20) ∂ ln p(z, D)
∂z

(cid:21)

.

z=z0

Note that even though this uses second-order information about the model, it is a ﬁrst-order
approximation of the gradient. We also view this as a transformation of the random (cid:15) for a ﬁxed
λ

˜fλ((cid:15)) = f (z0) + H(z0)(T ((cid:15), λ) − z0) ,

which is linear in z = T ((cid:15), λ). For some forms of T we can analytically derive the distribution
of the random variable ˜fλ((cid:15)). In Eq. (8), the data term interacts with the Jacobian of T , given
by

Jλ(cid:48)((cid:15)) (cid:44) ∂z
∂λ

=

∂T ((cid:15), λ)
∂λ

(cid:12)
(cid:12)
(cid:12)λ=λ(cid:48)

.

2We require the model ln p(z, D) to be twice diﬀerentiable.
3In fact, we know that the expectation of the parameter score term is zero, and removing that term altogether

can sometimes be a source of variance reduction that we do not explore here Roeder et al. [2017].

(9)

(10)

(11)

(12)

(13)

A.C. Miller et al./Reducing Reparameterization Gradient Variance

6

which importantly is a function of the same (cid:15) as in Eq. (12). We form our approximation of the
ﬁrst term in Eq. (8) by multiplying Eqs. (12) and (13):

˜g(data)
λ

((cid:15)) (cid:44) ˜fλ((cid:15))Jλ((cid:15)) .

(14)

The tractability of this approximation hinges on how Eq. (14) depends on (cid:15). When q(z; λ) is
multivariate normal, we show that this approximation has a computable mean and can be used
to reduce variance in MCVI settings. In the following sections we describe and empirically test
this variance reduction technique applied to diagonal Gaussian posterior approximations.

3.1. Gaussian Variational Families

Perhaps the most common choice of approximating distribution for MCVI is a diagonal Gaussian,
parameterized by a mean mλ ∈ RD and scales sλ ∈ RD

+ . 4 The log pdf is

ln q(z; mλ, s2

λ) = −

(z − mλ)(cid:124) (cid:2)diag(s2

λ)(cid:3)−1

(z − mλ) −

ln s2

λ,d −

ln(2π) .

1
2

(cid:88)

d

D
2

1
2

To generate a random variate z from this distribution, we use the sampling procedure in Eq. (4).
We denote the Monte Carlo RGE as ˆgλ (cid:44) [ˆgmλ , ˆgsλ ]. From this variational distribution, it is
straightforward to derive the distributions of the pathwise score, param score, and Jacobian
terms in Eq. (8).

The Jacobian term of the sampling procedure has two straightforward components

The pathwise score term is the partial derivative of the approximate log density with respect to z,
ignoring variation due to the variational distribution parameters and noting that z = mλ + sλ (cid:12) (cid:15):

The parameter score term is the partial derivative of the approximation log density with respect
to variational parameters λ, ignoring variation due to z. The mλ and sλ components are given
by

∂z
∂mλ

= ID ,

= diag((cid:15)) .

∂z
∂sλ

= −diag(s2

λ)−1(z − mλ) = −(cid:15)/sλ .

= (z − mλ)/s2

λ = (cid:15)/sλ

= −1/sλ − (z − mλ)2/s2

λ =

(cid:15)2 − 1
sλ

.

∂ ln q
∂z

∂ ln q
∂mλ
∂ ln q
∂sλ

The data term, f (z), multiplied by the Jacobian of T is all that remains to be approximated
in Eq. (8). We linearize f around z0 = mλ where the approximation is expected to be accu-
rate

˜fλ((cid:15)) = f (mλ) + H(mλ) ((mλ + sλ (cid:12) (cid:15)) − mλ)
λ)H(mλ)(cid:124)(cid:1) .

∼ N (cid:0)f (mλ), H(mλ)diag(s2

4For diagonal Gaussian q, we deﬁne λ = [mλ, sλ].

(15)

(16)

(17)

(18)

(19)

(20)

A.C. Miller et al./Reducing Reparameterization Gradient Variance

7

(cid:15)

ˆgλ

˜gλ

L

Fig 2: Relationship be-
tween the base randomness
(cid:15), RGE ˆg, and approxima-
tion ˜g. Arrows indicate de-
terministic functions. Shar-
ing (cid:15) correlates the random
variables. We know the dis-
tribution of ˜g, which allows
us to use it as a control
variate for ˆg.

Algorithm 1 Gradient descent with RV-RGE with a diagonal
Gaussian variational family
1: procedure RV-RGE-Optimize(λ1, ln p(z, D), (cid:32)L)
2:
3:
4:
5:
6:
7:

(cid:15)((cid:96)) ∼ N (0, ID) for (cid:96) = 1, . . . , L
ˆg((cid:96))
← ∇λ ln p(z((cid:15)((cid:96)), λt), D)
λt
˜g((cid:96))

(cid:46) Base randomness q0
(cid:46) Reparameterization gradients

f (z) ← ∇z ln p(z, D)
H(za, zb) ← (cid:2)∇2
for t = 1, . . . , T do

z ln p(za, D)(cid:3) zb

(cid:46) Mean approx

(cid:16)

mλt

← f (mλt ) + H(mλt , sλt (cid:12) (cid:15)((cid:96)))
(cid:17)
˜g((cid:96))
f (mλt ) + H(mλt , sλt (cid:12) (cid:15)((cid:96)))
←
sλt
E[˜gmλt
E[˜gsλt
ˆg(RV )
(cid:96) ˆg(cid:96)
λt
λt
λt+1 ← grad-update(λt, ˆg(RV )

− E[˜gλt ])
)

− (˜g(cid:96)
λt

(cid:80)

] ← f (mλt )
(cid:46) Mean approx expectation
] ← diag(H(mλt )) (cid:12) sλt + 1/sλt (cid:46) Scale approx expectation
= 1
(cid:46) Subtract control variate
L

(cid:46) Gradient step (sgd, adam, etc.)

(cid:12) (cid:15) + 1
sλt

(cid:46) Scale approx

λt

8:
9:
10:

11:

12:
13:

return λT

Putting It Together: Full RGE Approximation We write the complete approximation
of the RGE in Eq. (8) by combining Eqs. (15), (16), (17), (18), and (20) which results in two
components that are concatenated, ˜gλ = [˜gmλ , ˜gsλ ]. Each component is deﬁned as

˜gmλ = ˜fλ((cid:15)) + (cid:15)/sλ − (cid:15)/sλ

= f (mλ) + H(mλ)(sλ (cid:12) (cid:15))

˜gsλ = ˜fλ((cid:15)) (cid:12) (cid:15) + ((cid:15)/sλ) (cid:12) (cid:15) −

= (f (mλ) + H(mλ)(sλ (cid:12) (cid:15))) (cid:12) (cid:15) +

(cid:15)2 − 1
sλ

(21)

(22)

1
sλ

.

To summarize, we have constructed an approximation, ˜gλ, of the reparameterization gradient,
ˆgλ, as a function of (cid:15). Because both ˜gλ and ˆgλ are functions of the same random variable (cid:15),
and because we have mimicked the random process that generates true gradient samples, the
two gradient estimators will be correlated. This approximation yields two tractable distributions
— a Gaussian for the mean parameter gradient, gmλ , and a location shifted, scaled non-central
χ2 for the scale parameter gradient gsλ . Importantly, we can compute the mean of each compo-
nent

E[˜gmλ ] = f (mλ) ,

E[˜gsλ] = diag(H(mλ)) (cid:12) sλ + 1/sλ .

(23)

We use ˜gλ (along with its expectation) as a control variate to reduce the variance of the RGE
ˆgλ.

3.2. Reduced Variance Reparameterization Gradient Estimators

Now that we have constructed a tractable gradient approximation, ˜gλ, with high correlation to
the original reparameterization gradient estimator, ˆgλ, we can use it as a control variate as in
Eq. (6)

ˆg(RV )
λ

= ˆgλ − C(˜gλ − E[˜gλ]).

(24)

The optimal value for C is the covariance between ˜gλ and ˆgλ (see Appendix A). We can try
to estimate the value of C (or a diagonal approximation to C) on the ﬂy, or we can simply

A.C. Miller et al./Reducing Reparameterization Gradient Variance

8

ﬁx this value. In our case, because we are using an accurate linear approximation to the trans-
formation of a spherical Gaussian, the optimal value of C will be close to the identity (see
Appendix A.1).

High Dimensional Models For models with high dimensional posteriors, direct manipulation
of the Hessian is computationally intractable. However, our approximations in Eqs. (21) and (22)
only require a Hessian-vector product, which can be computed nearly as eﬃciently as the gra-
dient Pearlmutter [1994]. We note that the mean of the control variate ˜gsλ (Eq. (23)), depends
on the diagonal of the Hessian matrix. While computing the Hessian diagonal may be tractable
in some cases, in general it may cost the time equivalent of D function evaluations to compute
Martens et al. [2012]. Given a high dimensional problem, we can avoid this bottleneck in multiple
ways. The ﬁrst is simply to ignore the random variation in the Jacobian term due to (cid:15) — if we
ﬁx z to be mλ (as we do with the data term), the portion of the Jacobian that corresponds to
sλ will be zero (in Eq. (15)). This will result in the same Hessian-vector-product-based estimator
for ˜gmλ but will set ˜gsλ = 0, yielding variance reduction for the mean parameter but not the
scale.

Alternatively, we can estimate the Hessian diagonal on the ﬂy. If we use L > 1 samples at each
iteration, we can create a per-sample estimate of the sλ-scaled diagonal of the Hessian using the
other samples Bekas et al. [2007]. As the scaled diagonal estimator is unbiased, we can construct
an unbiased estimate of the control variate mean to use in lieu of the actual mean (possibly
increasing the ﬁnal variance). A similar local baseline strategy is used for variance reduction in
Mnih and Rezende [2016].

RV-RGE Estimators We introduce three diﬀerent estimators based on variations of the gra-
dient approximation deﬁned in Eqs. (21), (22), and (23), each adressing the Hessian operations
diﬀerently.

• The Full Hessian estimator implements the three equations as written and can be used

when it is computationally feasible to use the full Hessian.

• The Hessian Diagonal estimator replaces the Hessian in (21) with a diagonal approximation,

useful for models with a cheap Hessian diagonal.

• The Hessian-vector product + local approximation (HVP+Local) uses an eﬃcient Hessian-
vector product in Eqs. (21) and (22), while approximating the diagonal term in Eq. (23)
using a local baseline. The HVP+Local approximation is geared toward models where
Hessian-vector products can be computed, but the exact diagonal of the Hessian cannot.

We detail the RV-RGE algorithm in Algorithm 1 and compare properties of these three estimators
to the pure Monte Carlo estimator in the following section.

3.3. Related Work

Recently, Roeder et al. [2017] introduced a variance reduction technique for reparameterization
gradients that ignores the parameter score component of the gradient and can be viewed as a
type of control variate for the gradient throughout the optimization procedure. This approach is
complementary to our method — our approximation is typically more accurate near the beginning
of the optimization procedure, whereas the estimator in Roeder et al. [2017] is low-variance near
convergence. We hope to incorporate information from both control variates in future work.
Per-sample estimators in a multi-sample setting for variational inference were used in Mnih and

A.C. Miller et al./Reducing Reparameterization Gradient Variance

9

Table 1
Comparing variances for RV-RGEs: L = 10-sample estimators. Values are percentage of pure MC RGE
variance — a value of 100 indicates equal variation L = 10 samples, a value of 1 percent indicates a 100-fold
decrease in variance (lower is better).

Iteration

Estimator

Ave V(·)

V(|| · ||) Ave V(·)

V(|| · ||) Ave V(·)

V(|| · ||)

gmλ

ln gsλ

gλ

early

mid

late

Full Hessian
Hessian Diag
HVP + Local

Full Hessian
Hessian Diag
HVP + Local

Full Hessian
Hessian Diag
HVP + Local

1.279
34.691
1.279

0.075
38.891
0.075

0.042
40.292
0.042

1.139
23.764
1.139

0.068
21.283
0.068

0.030
53.922
0.030

0.001
0.003
0.013

0.113
6.295
30.754

1.686
23.644
98.523

0.002
0.012
0.039

0.143
7.480
39.156

0.431
28.024
99.811

0.008
0.194
0.020

0.076
38.740
0.218

0.043
40.281
0.110

1.039
21.684
1.037

0.068
21.260
0.071

0.030
53.777
0.022

(a) adam with step size = 0.05

(b) adam with step size = .10

Fig 3: MCVI optimization trace applied to the frisk model for two values of L and step size.
We run the standard MC gradient estimator (solid line) and the RV-RGE with L = 2 and 10
samples.

Rezende [2016]. We employ this technique in a diﬀerent way; we use it to estimate computationally
intractable quantities needed to keep the gradient estimator unbiased. Black box variational
inference used control variates and Rao-Blackwellization to reduce the variance of score-function
estimators Ranganath et al. [2014]. Our development of variance reduction for reparameterization
gradients compliments their work. Other variance reduction techniques for stochastic gradient
descent have focused on stochasticity due to data subsampling Johnson and Zhang [2013], Wang
et al. [2013]. Johnson and Zhang [2013] cache statistics about the entire dataset at each epoch
to use as a control variate for noisy mini-batch gradients.

4. Experiments and Analysis

In this section we empirically examine the variance properties of RVRGs and stochastic optimiza-
tion for two real-data examples — a hierarchical Poisson GLM and a Bayesian neural network.5

• Hierarchical Poisson GLM The frisk model is a hierarchical Poisson GLM, described in

Appendix C.1. This non-conjugate model has a D = 37 dimensional posterior.

• Bayesian Neural Network The non-conjugate bnn model is a Bayesian neural network ap-
plied to the wine dataset, (see Appendix C.2) and has a D = 653 dimensional posterior.

5Code is available at https://github.com/andymiller/ReducedVarianceReparamGradients.

A.C. Miller et al./Reducing Reparameterization Gradient Variance

10

(a) adam with step size = 0.05

(b) adam with step size = 0.10

Fig 4: MCVI optimization for the bnn model applied to the wine data for various L and step
sizes. The standard MC gradient estimator (dotted) was run with 2, 10, and 50 samples; RV-RGE
(solid) was run with 2 and 10 samples. In 4b the 2-sample MC estimator falls below the frame.

Quantifying Gradient Variance Reduction We measure the variance reduction of the RGE
observed at various iterates, λt, during execution of gradient descent. Both the gradient mag-
nitude, and the marginal variance of the gradient elements — using a sample of 1000 gradients
— are reported. Further, we inspect both the mean mλ and log-scale ln sλ parameters sepa-
rately. Table 3 compares gradient variances for the frisk model for four estimators: i) pure
Monte Carlo (MC), ii) Full Hessian, iii) Hessian Diagonal, and iv) Hessian-vector product +
local approximation (HVP+Local).

Each entry in the table measures the percent of the variance of the pure Monte Carlo estimator.
We show the average variance over each component AveV(·), and the variance of the norm V(||·||).
We separate out variance in mean parameters, gm, log scale parameters, ln gs, and the entire
vector gλ. The reduction in variance is dramatic. Using HVP+Local, in the norm of the mean
parameters we see between a 80× and 3,000× reduction in variance depending on the progress of
the optimizer. The importance of the full Hessian-vector product for reducing mean parameter
variance is also demonstrated as the Hessian diagonal only reduces mean parameter variance by
a factor of 2-5.

For the variational scale parameters, ln gs, we see that early on the HVP+Local approximation
is able to reduce parameter variance by a large factor (≈ 2,000×). However, at later iterates the
HVP+Local scale parameter variance is on par with the Monte Carlo estimator, while the full
Hessian estimator still enjoys huge variance reduction. This indicates that, by this point, most of
the noise is the local Hessian diagonal estimator. We also note that in this problem, most of the
estimator variance is in the mean parameters. Because of this, the norm of the entire parameter
gradient, gλ is reduced by 100 − 5,000×. In Appendix D we report results for other values of L
as a comparison.

Optimizer Convergence and Stability We compare the optimization traces for the frisk
and bnn model for the MC and the HVP+Local estimators under various conditions. At each
iteration we estimate the true ELBO value using 2000 Monte Carlo samples. We optimize the
ELBO objective using adam Kingma and Ba [2015] for two step sizes, each trace starting at the
same value of λ0.

Figure 3 compares ELBO optimization traces for L = 2 and L = 10 samples and step sizes .05
and .1 for the frisk model. We see that the HVP+Local estimators make early progress and
converge quickly. We also see that the L = 3 pure MC estimator results in noisy optimization
paths. Figure 4 shows objective value as a function of wall clock time under various settings for the

A.C. Miller et al./Reducing Reparameterization Gradient Variance

11

bnn model. The HVP+Local estimator does more work per iteration, however it tends to converge
faster. We observe the L = 10 HVP+Local outperforming the L = 50 MC estimator.

5. Conclusion

Variational inference reframes an integration problem as an optimization problem with the caveat
that each step of the optimization procedure solves an easier integration problem. For general
models, each sub-integration problem is itself intractable, and must be estimated, typically with
Monte Carlo samples. Our work has shown that we can use more information about the variational
family to create tighter estimators of the ELBO gradient, which leads to faster and more stable
optimization. The eﬃcacy of our approach relies on the complexity of the RGE distribution to
be well-captured by linear structure which may not be true for all models. However, we found
the idea eﬀective for non-conjugate hierarchical Bayesian models and a neural network.

Our presentation is a speciﬁc instantiation of a more general idea — using cheap linear structure
to remove variation from stochastic gradient estimates. We would like to extend this idea to
more ﬂexible variational distributions, including ﬂow distributions Rezende and Mohamed [2015]
and hierarchical distributions Ranganath et al. [2016], as well as model/inference schemes with
recognition networks Kingma and Welling [2014].

Acknowledgements

The authors would like to thank Finale Doshi-Velez, Mike Hughes, Taylor Killian, Andrew Ross,
and Matt Hoﬀman for helpful conversations and comments on this work. ACM is supported by
the Applied Mathematics Program within the Oﬃce of Science Advanced Scientiﬁc Computing
Research of the U.S. Department of Energy under contract No. DE-AC02-05CH11231. NJF is
supported by a Washington Research Foundation Innovation Postdoctoral Fellowship in Neuro-
engineering and Data Science. RPA is supported by NSF IIS-1421780 and the Alfred P. Sloan
Foundation.

References

arXiv:1701.07875, 2017.

Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein GAN.

arXiv preprint

Costas Bekas, Eﬀrosyni Kokiopoulou, and Yousef Saad. An estimator for the diagonal of a matrix.

Applied numerical mathematics, 57(11):1214–1229, 2007.

David M Blei, Alp Kucukelbir, and Jon D McAuliﬀe. Variational inference: A review for statis-

ticians. Journal of the American Statistical Association, 2017.

Andrew Gelman and Jennifer Hill. Data Analysis Using Regression and Multilevel/Hierarchical

Models. Cambridge University Press, 2006.

Andrew Gelman, Jeﬀrey Fagan, and Alex Kiss. An analysis of the NYPDs stop-and-frisk policy
in the context of claims of racial bias. Journal of the American Statistical Association, 102:
813–823, 2007.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. In Advances in Neural
Information Processing Systems, pages 2672–2680, 2014.

A.C. Miller et al./Reducing Reparameterization Gradient Variance

12

Matthew D Hoﬀman, David M Blei, Chong Wang, and John William Paisley. Stochastic varia-

tional inference. Journal of Machine Learning Research, 14(1):1303–1347, 2013.

Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in Neural Information Processing Systems, pages 315–323, 2013.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings

of the International Conference on Learning Representations, 2015.

Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In Proceedings of the

International Conference on Learning Representations, 2014.

Dougal Maclaurin, David Duvenaud, Matthew Johnson, and Ryan P. Adams. Autograd: Reverse-

mode diﬀerentiation of native Python, 2015. URL http://github.com/HIPS/autograd.

James Martens, Ilya Sutskever, and Kevin Swersky. Estimating the Hessian by back-propagating

curvature. In Proceedings of the International Conference on Machine Learning, 2012.

Andriy Mnih and Danilo Rezende. Variational inference for Monte Carlo objectives. In Proceed-

ings of The 33rd International Conference on Machine Learning, pages 2188–2196, 2016.

Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv

Barak A Pearlmutter. Fast exact multiplication by the Hessian. Neural computation, 6(1):

Rajesh Ranganath, Sean Gerrish, and David M Blei. Black box variational inference. In AIS-

preprint arXiv:1610.03483, 2016.

147–160, 1994.

TATS, pages 814–822, 2014.

Rajesh Ranganath, Dustin Tran, and David M Blei. Hierarchical variational models. In Inter-

national Conference on Machine Learning, 2016.

Danilo Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows. In Proceed-
ings of the 32nd International Conference on Machine Learning (ICML-15), pages 1530–1538,
2015.

Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
In International Conference on Machine

approximate inference in deep generative models.
Learning, 2014.

Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Math-

ematical Statistics, pages 400–407, 1951.

Geoﬀrey Roeder, Yuhuai Wu Wu, and David Duvenaud. Sticking the landing: An asymptotically
zero-variance gradient estimator for variational inference. arXiv preprint arXiv:1703.09194,
2017.

Francisco R Ruiz, Michalis Titsias RC AUEB, and David Blei. The generalized reparameteriza-
tion gradient. In Advances in Neural Information Processing Systems, pages 460–468, 2016.
Dustin Tran, Matthew D Hoﬀman, Rif A Saurous, Eugene Brevdo, Kevin Murphy, and David M
In Proceedings of the International Conference on

Blei. Deep probabilistic programming.
Learning Representations, 2017.

Chong Wang, Xi Chen, Alexander J Smola, and Eric P Xing. Variance reduction for stochastic
gradient optimization. In Advances in Neural Information Processing Systems, pages 181–189,
2013.

A.C. Miller et al./Reducing Reparameterization Gradient Variance

13

Appendix A: Control Variates

Control variates are random quantities that are used to reduce the variance of a statistical estimator
without trading any bias. Concretely, given an unbiased estimator ˆg such that E[ˆg] = g (the quantity
of interest), our goal is to construct another unbiased estimator with lower variance. We can do this by
deﬁning a control variate ˜g with known expectation ˜m. We can write our new estimator as

g(cv) = ˆg − c · (˜g − ˜m) .

Clearly the new estimator has the same expectation as the original estimator, but a diﬀerent variance.
We can reduce the variance of g(cv) by setting c optimally.

Consider a univariate ˆg and ˜g, and without loss of generality, take ˜m = 0. The variance of g(cv) can be
written

V(g(cv)) = E[(ˆg − c · ˜g)2] − E[ˆg]2

= E[ˆg2 + c2 · ˜g2 − 2cˆg ˜g] − E[ˆg]2
= E[ˆg2] + c2E[˜g2] − 2cE[ˆg ˜g] − E[ˆg]2

We minimize the variance with respect to c by taking the derivative and setting equal to zero, which
implies

The covariance C(ˆg, ˜g) is typically not known a priori and must be estimated. It can be shown, under
the optimal c∗, that the variance of g(cv) is

where ρ is the correlation coeﬃcient between ˜g and ˆg.

When ˆg and ˜g are length D vectors, we can construct an estimator that depends on a matrix-valued free
parameter, C ∈ RD×D

We can show that the C that minimizes the Tr(C(g(cv))) — the sum of the marginal variances — is
given by

c∗ =

E[ˆg ˜g]
E[˜g2]

=

C(ˆg, ˜g)
V(˜g)

V(g(cv)) = (1 − ρ2)V(ˆg)

g(cv) = ˆg − C(˜g − ˜m) .

C ∗ = Σ−1

˜g Σˆg,˜g

where Σ˜g is the covariance matrix of the control variate vector, and Σˆg,˜g is the cross covariance between
ˆg and ˜g.

Intuitively, a control variate is injecting information into the estimator in the form of linear structure. If
the two quantities, ˜g and ˆg are perfectly correlated, then we already know the mean and estimation is
not necessary. As the two become uncorrelated, the linear estimator becomes less and less informative,
and reverts to the original quantity.

A.1. Control Variates and Approximate Functions

In our setting, we approximate the distribution of some function f ((cid:15)) where (cid:15) ∼ N (0, I) by a ﬁrst order
Taylor expansion about 0 — for now we examine the univariate case

f1((cid:15)) = f (0) + f (cid:48)(0)(cid:15)

(cid:15) ∈ R

(33)

(25)

(26)

(27)

(28)

(29)

(30)

(31)

(32)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

A.C. Miller et al./Reducing Reparameterization Gradient Variance

14

If we wish to use f1((cid:15)) as a control variate for f ((cid:15)), we need to characterize the covariance between the
two random variables. Because the form of f ((cid:15)) is general, it is diﬃcult to analyze. We instead derive
the covariance between f1((cid:15)) and the second-order expansion

as a surrogate.

f2((cid:15)) = f (0) + f (cid:48)(0)(cid:15) + f (cid:48)(cid:48)(0)/2(cid:15)2

C(f1((cid:15)), f2((cid:15))) = E [(f1((cid:15)) − E[f1((cid:15))])(f2((cid:15)) − E[f2((cid:15))])]

= E (cid:2)(f (cid:48)(0)(cid:15)) (cid:0)f (cid:48)(0)(cid:15) + f (cid:48)(cid:48)(0)/2(cid:15)2 − f (cid:48)(cid:48)(0)/2(cid:1)(cid:3)
= E (cid:2)f (cid:48)(0)2(cid:15)2 + (f (cid:48)(0)f (cid:48)(cid:48)(0)/2)(cid:15)3 − (f (cid:48)(0)f (cid:48)(cid:48)(0)/2)(cid:15)(cid:3)
= E (cid:2)f (cid:48)(0)2(cid:15)2(cid:3)
= V[f1((cid:15))]

where note that E[(cid:15)3] = 0. Recall that the optimal control variate can be written

c∗ = C(f1((cid:15)), f2((cid:15)))/V[f1((cid:15))]
= V[f1((cid:15))]/V[f1((cid:15))] = 1 .

Appendix B: Algorithm Details

We summarize an optimization routine using RV-RGE in Algorithm 1. The diﬀerent variants rely on the
diﬀerent forms of H(·, ·) and diag(H). The full Hessian estimator calculates these terms exactly. The
diagonal Hessian estimates the Hessian-vector product with the diagonal of the Hessian. The HVP+Local
estimator computes the Hessian-vector product exactly, but estimates the scale approximation mean using
other samples.

We also note that there are ways to optimize the additional Hessian-vector product computation. Because
each Hessian is evaluated at the same mλ, we can cache the computation in the forward pass, and only
repeat the backwards pass for each sample, as implemented in Maclaurin et al. [2015].

Appendix C: Model Deﬁnitions

C.1. Multi-level Poisson GLM

Our second test model is a 37-dimensional posterior resulting from a hierarchical Poisson GLM. This
model measures the relative rates of stop-and-frisk events for diﬀerent ethnicities and in diﬀerent precincts
Gelman et al. [2007], and has been used as illustrative example of multi-level modeling [Gelman and Hill,
2006, Chapter 15, Section 1].

ln σ2

α, ln σ2

µ ∼ N (0, 102)
β ∼ N (0, 102)
αe ∼ N (0, σ2
α)
βp ∼ N (0, σ2
β)

ln λep = µ + αe + βp + ln Nep

Yep ∼ P(λep)

mean oﬀset

group variances

ethnicity eﬀect

precinct eﬀect

log rate

stop-and-frisk events

A.C. Miller et al./Reducing Reparameterization Gradient Variance

15

where Yep are the number of stop-and-frisk events within ethnicity group e and precinct p over some
ﬁxed period of time; Nep is the total number of arrests of ethnicity group e in precinct p over the same
period of time; αe and βp are the ethnicity and precinct eﬀects.

C.2. Bayesian Neural Network

We implement a 50-unit hidden layer neural network with ReLU activation functions. We place a normal
prior over each weight in the neural network, governed by the same variance (with an inverse Gamma
prior). We also place an inverse Gamma prior over the observation variance The model can be written
as

α ∼ Gamma(1, .1)

τ ∼ Gamma(1, .1)

wi ∼ N (0, 1/α)
y|x, w, τ ∼ N (φ(x, w), 1/τ )

weight prior hyper

noise prior hyper

weights

output distribution

(42)

(43)

(44)

(45)

where w = {w} is the set of weights, and φ(x, w) is a multi-layer perceptron that maps input x to
approximate output y as a function of parameters w. We denote the set of parameters as θ (cid:44) (w, α, τ ).
We approximate the posterior p(w, α, τ |D), where D is the training set of {xn, yn}N
n=1 input-output
pairs.

We use a 100-row subsample of the wine dataset from the UCI repository https://archive.ics.uci.
edu/ml/datasets/Wine+Quality.

A.C. Miller et al./Reducing Reparameterization Gradient Variance

16

Appendix D: Variance Reduction

Below are additional variance reduction measurements for the frisk model for diﬀerent values of L,
samples drawn per iteration.

Table 2
frisk model variance comparison: L = 3-sample estimators

Iteration

Estimator

Ave V(·)

V(|| · ||) Ave V(·)

V(|| · ||) Ave V(·)

V(|| · ||)

gmλ

ln gsλ

gλ

MC
Full Hessian
Hessian Diag
HVP + Local

MC
Full Hessian
Hessian Diag
HVP + Local

MC
Full Hessian
Hessian Diag
HVP + Local

100.000
1.184
35.541
1.184

100.000
0.080
39.016
0.080

100.000
0.044
39.280
0.044

100.000
1.022
25.012
1.022

100.000
0.075
22.832
0.075

100.000
0.024
38.799
0.024

100.000
0.001
0.003
0.012

100.000
0.122
6.617
31.992

100.000
1.782
22.915
98.290

100.000
0.002
0.011
0.039

100.000
0.169
8.097
46.160

100.000
0.879
21.913
99.679

100.000
0.007
0.201
0.019

100.000
0.081
38.868
0.227

100.000
0.045
39.268
0.116

100.000
0.902
22.090
0.900

100.000
0.075
22.804
0.078

100.000
0.023
38.725
0.014

Table 3
frisk model variance comparison: L = 50-sample estimators

Iteration

Estimator

Ave V(·)

V(|| · ||) Ave V(·)

V(|| · ||) Ave V(·)

V(|| · ||)

gmλ

ln gsλ

gλ

MC
Full Hessian
Hessian Diag
HVP + Local

MC
Full Hessian
Hessian Diag
HVP + Local

MC
Full Hessian
Hessian Diag
HVP + Local

100.000
1.276
35.146
1.276

100.000
0.081
37.534
0.081

100.000
0.042
39.972
0.042

100.000
1.127
24.018
1.127

100.000
0.074
21.773
0.074

100.000
0.043
101.263
0.043

100.000
0.001
0.003
0.013

100.000
0.125
7.204
31.278

100.000
1.894
24.450
98.588

100.000
0.002
0.012
0.039

100.000
0.121
7.035
32.275

100.000
0.296
27.174
99.539

100.000
0.008
0.197
0.020

100.000
0.081
37.394
0.225

100.000
0.044
39.961
0.112

100.000
1.080
23.028
1.079

100.000
0.074
21.752
0.076

100.000
0.043
101.019
0.033

early

mid

late

early

mid

late

7
1
0
2
 
y
a
M
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
0
8
8
7
0
.
5
0
7
1
:
v
i
X
r
a

Reducing Reparameterization Gradient Variance

Andrew C. Miller∗, Nicholas J. Foti† , Alexander D’Amour§ , and Ryan P. Adams‡

Abstract: Optimization with noisy gradients has become ubiquitous in statistics and
machine learning. Reparameterization gradients, or gradient estimates computed via the
“reparameterization trick,” represent a class of noisy gradients often used in Monte Carlo
variational inference (MCVI). However, when these gradient estimators are too noisy, the
optimization procedure can be slow or fail to converge. One way to reduce noise is to use
more samples for the gradient estimate, but this can be computationally expensive. Instead,
we view the noisy gradient as a random variable, and form an inexpensive approximation of
the generating procedure for the gradient sample. This approximation has high correlation
with the noisy gradient by construction, making it a useful control variate for variance
reduction. We demonstrate our approach on non-conjugate multi-level hierarchical mod-
els and a Bayesian neural net where we observed gradient variance reductions of multiple
orders of magnitude (20-2,000×).

1. Introduction

Representing massive datasets with ﬂexible probabilistic models has been central to the success
of many statistics and machine learning applications, but the computational burden of ﬁtting
these models is a major hurdle. For optimization-based ﬁtting methods, a central approach to this
problem has been replacing expensive evaluations of the gradient of the objective function with
cheap, unbiased, stochastic estimates of the gradient. For example, stochastic gradient descent
using small mini-batches of (conditionally) i.i.d. data to estimate the gradient at each iteration is
a popular approach with massive data sets. Alternatively, some learning methods sample directly
from a generative model or approximating distribution to estimate the gradients of interest, for
example, in learning algorithms for implicit models Mohamed and Lakshminarayanan [2016],
Tran et al. [2017] and generative adversarial networks Arjovsky et al. [2017], Goodfellow et al.
[2014].

Approximate Bayesian inference using variational techniques (variational inference, or VI) has
also motivated the development of new stochastic gradient estimators, as the variational approach
reframes the integration problem of inference as an optimization problem Blei et al. [2017]. VI ap-
proaches seek out the distribution from a well-understood variational family of distributions that
best approximates an intractable posterior distribution. The VI objective function itself is often
intractable, but recent work has shown that it can be optimized with stochastic gradient methods
that use Monte Carlo estimates of the gradient Kingma and Welling [2014], Ranganath et al.
[2014], Rezende et al. [2014], We call this approach Monte Carlo variational inference (MCVI). In
MCVI, generating samples from an approximate posterior distribution is the source of gradient
stochasticity. Alternatively, stochastic variational inference (SVI) Hoﬀman et al. [2013] and other
stochastic optimization procedures induce stochasticity through data subsampling; MCVI can be
augmented with data subsampling to accelerate computation for large data sets.

∗Harvard University, e-mail: acm@seas.harvard.edu, url: http://andymiller.github.io
†University of Washington, e-mail: nfoti@uw.edu
‡Harvard University, Google Brain e-mail: rpa@seas.harvard.edu
§UC Berkeley e-mail: alexdamour@berkeley.edu

1

A.C. Miller et al./Reducing Reparameterization Gradient Variance

2

The two commonly used MCVI gradient estimators are the score function gradient Ranganath
et al. [2014] and the reparameterization gradient Kingma and Welling [2014], Rezende et al.
[2014]. Broadly speaking, score function estimates can be applied to both discrete and contin-
uous variables, but often have high variance and thus are frequently used in conjunction with
variance reduction techniques. On the other hand, the reparameterization gradient often has
lower variance, but is restricted to continuous random variables. See Ruiz et al. [2016] for a uni-
fying perspective on these two estimators. Like other stochastic gradient methods, the success of
MCVI depends on controlling the variance of the stochastic gradient estimator.

√

In this work, we present a novel approach to controlling the variance of the reparameterization
gradient estimator in MCVI. Existing MCVI methods control this variance na¨ıvely by averaging
several gradient estimates, which becomes expensive for large data sets and complex models,
N ). Our approach exploits the fact that, in MCVI,
with error that only diminishes as O(1/
the randomness in the gradient estimator is completely determined by a known Monte Carlo
generating process; this allows us to leverage knowledge about this generating process to de-noise
the gradient estimator. In particular, we construct a cheaply computed control variate based on
an analytical linear approximation to the gradient estimator. Taking a linear combination of a
na¨ıve gradient estimate with this control variate yields a new estimator for the gradient that
remains unbiased but has lower variance. We apply the idea to Gaussian approximating families
and measure the reduction in gradient variance under various conditions. We observe a 20-2,000
× reduction in variance of the gradient norm in some conditions, and much faster convergence
and more stable behavior of optimization traces.

2. Background

Variational Inference Given a model, p(z, D) = p(D|z)p(z), of data D and parameters/latent
variables z, the goal of VI is to approximate the posterior distribution p(z|D). VI approximates
this intractable posterior distribution with one from a simpler family, Q = {q(z; λ), λ ∈ Λ},
parameterized by variational parameters λ. VI procedures seek out the member of that fam-
ily, q(·; λ) ∈ Q, that minimizes some divergence between the approximation q and the true pos-
terior p(z|D).

Variational inference can be framed as an optimization problem, usually in terms of Kullback-
Leibler (KL) divergence, of the following form

λ∗ = arg min

KL(q(z; λ) || p(z|D)) = arg min

Ez∼qλ [ln q(z; λ) − ln p(z|D)] .

λ∈Λ

λ∈Λ

The task is to ﬁnd a setting of λ that makes q(z; λ) close to the posterior p(z|D) in KL diver-
gence.1 Directly computing the KL divergence requires evaluating the posterior itself; therefore,
VI procedures use the evidence lower bound (ELBO) as the optimization objective

L(λ) = Ez∼qλ [ln p(z, D) − ln q(z; λ)]

(1)

which, when maximized, minimizes the KL divergence between q(z; λ) and p(z|D).

To maximize the ELBO with gradient methods, we need to compute the gradient of the ELBO, gλ.
The gradient inherits the ELBO’s form as an expectation, which is in general an intractable quan-
tity to compute. In this work, we focus on reparameterization gradient estimators (RGEs) com-
puted using the reparameterization trick. The reparameterization trick exploits the structure of

1We use q(z; λ) and qλ(z) interchangeably.

A.C. Miller et al./Reducing Reparameterization Gradient Variance

3

(a) step size = .05

(b) step size = .1

Fig 1: Optimization traces for MCVI applied to a Bayesian neural network with various hyper-
parameter settings. Each trace is running adam Kingma and Ba [2015]. The three lines in each
plot correspond to three diﬀerent numbers of samples, L, used to estimate the gradient at each
step. (Left) small stepsize; (Right) stepsize 10 times larger. Large step sizes allow for quicker
progress, however noisier (i.e., small L) gradients combined with large step sizes result in chaotic
optimization dynamics. The converging traces reach diﬀerent ELBOs due to the illustrative con-
stant learning rates; in practice, one decreases the step size over time in keeping with Robbins
and Monro [1951].

the variational data generating procedure — the mechanism by which z is simulated from qλ(z).
To compute the RGE, we ﬁrst express the sampling procedure from qλ(z) as a diﬀerentiable
map

(cid:15) ∼ q0((cid:15))
z = T ((cid:15); λ)

independent of λ

diﬀerentiable map

where the initial distribution q0 and T are jointly deﬁned such that z ∼ q(z; λ) has the de-
sired distribution. As a simple concrete example, if we set q(z; λ) to be a diagonal Gaussian
N (mλ, diag(s2
+ . The sampling procedure could
then be deﬁned as

λ)) where λ = [mλ, sλ], mλ ∈ RD, and sλ ∈ RD

(cid:15) ∼ N (0, ID) ,

z = T ((cid:15); λ) = mλ + sλ (cid:12) (cid:15)

where s (cid:12) (cid:15) denotes an element-wise product. We will also use x/y and x2 to denote pointwise
division and squaring, respectively. Given this map, the reparameterization gradient estimator
is simply the gradient of a Monte Carlo ELBO estimate with respect to λ. For a single sample,
this is

(cid:15) ∼ q0((cid:15)) ,

ˆgλ (cid:44) ∇λ [ln p(T ((cid:15); λ), D) − ln q(T ((cid:15); λ); λ)]

and the L-sample approximation can be computed by the sample average

(2)

(3)

(4)

(5)

ˆg(L)
λ =

ˆgλ((cid:15)(cid:96)).

1
L

L
(cid:88)

(cid:96)=1

Crucially, the reparameterization gradient is unbiased, E[ˆgλ] = ∇λL(λ), guaranteeing the con-
vergence of stochastic gradient optimization procedures that use it Robbins and Monro [1951].

A.C. Miller et al./Reducing Reparameterization Gradient Variance

4

Gradient Variance and Convergence The eﬃciency of Monte Carlo variational inference
hinges on the magnitude of gradient noise and the step size chosen for the optimization procedure.
When the gradient noise is large, smaller gradient steps must be taken to avoid unstable dynamics
of the iterates. However, a smaller step size increases the number of iterations that must be
performed to reach convergence.

We illustrate this trade-oﬀ in Figure 1, which shows realizations of an optimization proce-
dure applied to a Bayesian neural network using reparameterization gradients. The posterior
is over D = 653 parameters that we approximate with a diagonal Gaussian (see Appendix C.2).
We compare the progress of the Adam algorithm using various numbers of samples Kingma and
Ba [2015], ﬁxing the learning rate. The noise present in the single-sample estimator causes ex-
tremely slow convergence, whereas the lower noise 50-sample estimator quickly converges, albeit
at 50 times the cost.

The upshot is that with low noise gradients we are able to safely take larger steps, enabling
faster convergence to a local optimum. The natural question is, how can we reduce the variance
of gradient estimates without introducing too much extra computation? Our approach is to use
information about the variational model, q(·; λ) and carefully construct a control variate to the
gradient.

Control Variates Control variates are random quantities that are used to reduce the variance
of a statistical estimator without introducing any bias by injecting information into the estimator.
Given an unbiased estimator ˆg such that E[ˆg] = g (the quantity of interest), our goal is to
construct another unbiased estimator with lower variance. We can do this by deﬁning a control
variate ˜g with known expectation ˜m. We can write our new estimator as

g(cv) = ˆg − C(˜g − ˜m) .

(6)

where C ∈ RD×D for D-dimensional ˆg. Clearly the new estimator has the same expectation
as the original estimator, but a diﬀerent variance. We can attain optimal variance reduction
by appropriately setting C. Intuitively, the optimal C is very similar to a regression coeﬃcient
— it is related to the covariance between the control variate and the original estimator. See
Appendix A for further details on optimally setting C.

3. Method: Modeling Reparameterization Gradients

In this section we develop our main contribution, a new gradient estimator that can dramatically
reduce reparameterization gradient variance. In MCVI, the reparameterization gradient estimator
(RGE) is a Monte Carlo estimator of the true gradient — the estimator itself is a random variable.
This random variable is generated using the “reparameterization trick” — we ﬁrst generate
some randomness (cid:15) and then compute the gradient of the ELBO with respect to λ holding (cid:15)
ﬁxed. This results in a complex distribution from which we can generate samples, but in general
cannot characterize due to the complexity of the term arising from the gradient of the true
posterior.

However, we do have a lot of information about the sampling procedure — we know the vari-
ational distribution ln q(z; λ), the transformation T , and we can evaluate the model joint den-
sity ln p(z, D) pointwise. Furthermore, with automatic diﬀerentiation, it is often straightforward
to obtain gradients and Hessian-vector products of our model ln p(z, D). We propose a scheme
that uses the structure of qλ and curvature of ln p(z, D) to construct a tractable approximation of

A.C. Miller et al./Reducing Reparameterization Gradient Variance

5

the distribution of the RGE.2 This approximation has a known mean and is correlated with the
RGE distribution, allowing us to use it as a control variate to reduce the RGE variance.

Given a variational family parameterized by λ, we can decompose the ELBO gradient into a few
terms that reveal its “data generating procedure”

(cid:15) ∼ q0 ,

z = T ((cid:15); λ)

ˆgλ (cid:44) ˆg(z; λ) =

∂ ln p(z, D)
∂z
(cid:123)(cid:122)
data term

(cid:125)

(cid:124)

∂z
∂λ

−

∂ ln qλ(z)
∂z
(cid:123)(cid:122)
(cid:125)
(cid:124)
pathwise score

∂z
∂λ

−

∂ ln qλ(z)
∂λ
(cid:123)(cid:122)
parameter score

(cid:124)

(cid:125)

.

(7)

(8)

Certain terms in Eq. (8) have tractable distributions. The Jacobian of T (·; λ), given by ∂z/∂λ,
is deﬁned by our choice of q(z; λ). For some transformations T we can exactly compute the
distribution of the Jacobian given the distribution of (cid:15). The pathwise and parameter score terms
are gradients of our approximate distribution with respect to λ (via z or directly). If our approx-
imation is tractable (e.g., a multivariate Gaussian), we can exactly characterize the distribution
for these components.3

However, the data term in Eq. (8) involves a potentially complicated function of the latent
variable z (and therefore a complicated function of (cid:15)), resulting in a diﬃcult-to-characterize
distribution. Our goal is to construct an approximation to the distribution of ∂ ln p(z, D)/∂z and
its interaction with ∂z/∂λ given a ﬁxed distribution over (cid:15). If the approximation yields random
variables that are highly correlated with ˆgλ, then we can use it to reduce the variance of that
RGE sample.

Linearizing the data term To simplify notation, we write the data term of the gradient
as

f (z(cid:48)) (cid:44) ∂ ln p(z, D)

∂z

(cid:12)
(cid:12)
(cid:12)z=z(cid:48)

,

where f : RD (cid:55)→ RD since z ∈ RD. We then linearize f about some value z0
˜f (z) = f (z0) + [Jzf (z)] (z − z0) = f (z0) + H(z0)(z − z0)

where H(z0) is the Hessian of the model, ln p(z, D), with respect to z evaluated at z0,

H(z0) = Jz

(cid:20) ∂ ln p(z, D)
∂z

(cid:21)

.

z=z0

Note that even though this uses second-order information about the model, it is a ﬁrst-order
approximation of the gradient. We also view this as a transformation of the random (cid:15) for a ﬁxed
λ

˜fλ((cid:15)) = f (z0) + H(z0)(T ((cid:15), λ) − z0) ,

which is linear in z = T ((cid:15), λ). For some forms of T we can analytically derive the distribution
of the random variable ˜fλ((cid:15)). In Eq. (8), the data term interacts with the Jacobian of T , given
by

Jλ(cid:48)((cid:15)) (cid:44) ∂z
∂λ

=

∂T ((cid:15), λ)
∂λ

(cid:12)
(cid:12)
(cid:12)λ=λ(cid:48)

.

2We require the model ln p(z, D) to be twice diﬀerentiable.
3In fact, we know that the expectation of the parameter score term is zero, and removing that term altogether

can sometimes be a source of variance reduction that we do not explore here Roeder et al. [2017].

(9)

(10)

(11)

(12)

(13)

A.C. Miller et al./Reducing Reparameterization Gradient Variance

6

which importantly is a function of the same (cid:15) as in Eq. (12). We form our approximation of the
ﬁrst term in Eq. (8) by multiplying Eqs. (12) and (13):

˜g(data)
λ

((cid:15)) (cid:44) ˜fλ((cid:15))Jλ((cid:15)) .

(14)

The tractability of this approximation hinges on how Eq. (14) depends on (cid:15). When q(z; λ) is
multivariate normal, we show that this approximation has a computable mean and can be used
to reduce variance in MCVI settings. In the following sections we describe and empirically test
this variance reduction technique applied to diagonal Gaussian posterior approximations.

3.1. Gaussian Variational Families

Perhaps the most common choice of approximating distribution for MCVI is a diagonal Gaussian,
parameterized by a mean mλ ∈ RD and scales sλ ∈ RD

+ . 4 The log pdf is

ln q(z; mλ, s2

λ) = −

(z − mλ)(cid:124) (cid:2)diag(s2

λ)(cid:3)−1

(z − mλ) −

ln s2

λ,d −

ln(2π) .

1
2

(cid:88)

d

D
2

1
2

To generate a random variate z from this distribution, we use the sampling procedure in Eq. (4).
We denote the Monte Carlo RGE as ˆgλ (cid:44) [ˆgmλ , ˆgsλ ]. From this variational distribution, it is
straightforward to derive the distributions of the pathwise score, param score, and Jacobian
terms in Eq. (8).

The Jacobian term of the sampling procedure has two straightforward components

The pathwise score term is the partial derivative of the approximate log density with respect to z,
ignoring variation due to the variational distribution parameters and noting that z = mλ + sλ (cid:12) (cid:15):

The parameter score term is the partial derivative of the approximation log density with respect
to variational parameters λ, ignoring variation due to z. The mλ and sλ components are given
by

∂z
∂mλ

= ID ,

= diag((cid:15)) .

∂z
∂sλ

= −diag(s2

λ)−1(z − mλ) = −(cid:15)/sλ .

= (z − mλ)/s2

λ = (cid:15)/sλ

= −1/sλ − (z − mλ)2/s2

λ =

(cid:15)2 − 1
sλ

.

∂ ln q
∂z

∂ ln q
∂mλ
∂ ln q
∂sλ

The data term, f (z), multiplied by the Jacobian of T is all that remains to be approximated
in Eq. (8). We linearize f around z0 = mλ where the approximation is expected to be accu-
rate

˜fλ((cid:15)) = f (mλ) + H(mλ) ((mλ + sλ (cid:12) (cid:15)) − mλ)
λ)H(mλ)(cid:124)(cid:1) .

∼ N (cid:0)f (mλ), H(mλ)diag(s2

4For diagonal Gaussian q, we deﬁne λ = [mλ, sλ].

(15)

(16)

(17)

(18)

(19)

(20)

A.C. Miller et al./Reducing Reparameterization Gradient Variance

7

(cid:15)

ˆgλ

˜gλ

L

Fig 2: Relationship be-
tween the base randomness
(cid:15), RGE ˆg, and approxima-
tion ˜g. Arrows indicate de-
terministic functions. Shar-
ing (cid:15) correlates the random
variables. We know the dis-
tribution of ˜g, which allows
us to use it as a control
variate for ˆg.

Algorithm 1 Gradient descent with RV-RGE with a diagonal
Gaussian variational family
1: procedure RV-RGE-Optimize(λ1, ln p(z, D), (cid:32)L)
2:
3:
4:
5:
6:
7:

(cid:15)((cid:96)) ∼ N (0, ID) for (cid:96) = 1, . . . , L
ˆg((cid:96))
← ∇λ ln p(z((cid:15)((cid:96)), λt), D)
λt
˜g((cid:96))

(cid:46) Base randomness q0
(cid:46) Reparameterization gradients

f (z) ← ∇z ln p(z, D)
H(za, zb) ← (cid:2)∇2
for t = 1, . . . , T do

z ln p(za, D)(cid:3) zb

(cid:46) Mean approx

(cid:16)

mλt

← f (mλt ) + H(mλt , sλt (cid:12) (cid:15)((cid:96)))
(cid:17)
˜g((cid:96))
f (mλt ) + H(mλt , sλt (cid:12) (cid:15)((cid:96)))
←
sλt
E[˜gmλt
E[˜gsλt
ˆg(RV )
(cid:96) ˆg(cid:96)
λt
λt
λt+1 ← grad-update(λt, ˆg(RV )

− E[˜gλt ])
)

− (˜g(cid:96)
λt

] ← f (mλt )
(cid:46) Mean approx expectation
] ← diag(H(mλt )) (cid:12) sλt + 1/sλt (cid:46) Scale approx expectation
= 1
(cid:46) Subtract control variate
L

(cid:80)

(cid:46) Gradient step (sgd, adam, etc.)

(cid:12) (cid:15) + 1
sλt

(cid:46) Scale approx

λt

8:
9:
10:

11:

12:
13:

return λT

Putting It Together: Full RGE Approximation We write the complete approximation
of the RGE in Eq. (8) by combining Eqs. (15), (16), (17), (18), and (20) which results in two
components that are concatenated, ˜gλ = [˜gmλ , ˜gsλ ]. Each component is deﬁned as

˜gmλ = ˜fλ((cid:15)) + (cid:15)/sλ − (cid:15)/sλ

= f (mλ) + H(mλ)(sλ (cid:12) (cid:15))

˜gsλ = ˜fλ((cid:15)) (cid:12) (cid:15) + ((cid:15)/sλ) (cid:12) (cid:15) −

= (f (mλ) + H(mλ)(sλ (cid:12) (cid:15))) (cid:12) (cid:15) +

(cid:15)2 − 1
sλ

(21)

(22)

1
sλ

.

To summarize, we have constructed an approximation, ˜gλ, of the reparameterization gradient,
ˆgλ, as a function of (cid:15). Because both ˜gλ and ˆgλ are functions of the same random variable (cid:15),
and because we have mimicked the random process that generates true gradient samples, the
two gradient estimators will be correlated. This approximation yields two tractable distributions
— a Gaussian for the mean parameter gradient, gmλ , and a location shifted, scaled non-central
χ2 for the scale parameter gradient gsλ . Importantly, we can compute the mean of each compo-
nent

E[˜gmλ ] = f (mλ) ,

E[˜gsλ] = diag(H(mλ)) (cid:12) sλ + 1/sλ .

(23)

We use ˜gλ (along with its expectation) as a control variate to reduce the variance of the RGE
ˆgλ.

3.2. Reduced Variance Reparameterization Gradient Estimators

Now that we have constructed a tractable gradient approximation, ˜gλ, with high correlation to
the original reparameterization gradient estimator, ˆgλ, we can use it as a control variate as in
Eq. (6)

ˆg(RV )
λ

= ˆgλ − C(˜gλ − E[˜gλ]).

(24)

The optimal value for C is the covariance between ˜gλ and ˆgλ (see Appendix A). We can try
to estimate the value of C (or a diagonal approximation to C) on the ﬂy, or we can simply

A.C. Miller et al./Reducing Reparameterization Gradient Variance

8

ﬁx this value. In our case, because we are using an accurate linear approximation to the trans-
formation of a spherical Gaussian, the optimal value of C will be close to the identity (see
Appendix A.1).

High Dimensional Models For models with high dimensional posteriors, direct manipulation
of the Hessian is computationally intractable. However, our approximations in Eqs. (21) and (22)
only require a Hessian-vector product, which can be computed nearly as eﬃciently as the gra-
dient Pearlmutter [1994]. We note that the mean of the control variate ˜gsλ (Eq. (23)), depends
on the diagonal of the Hessian matrix. While computing the Hessian diagonal may be tractable
in some cases, in general it may cost the time equivalent of D function evaluations to compute
Martens et al. [2012]. Given a high dimensional problem, we can avoid this bottleneck in multiple
ways. The ﬁrst is simply to ignore the random variation in the Jacobian term due to (cid:15) — if we
ﬁx z to be mλ (as we do with the data term), the portion of the Jacobian that corresponds to
sλ will be zero (in Eq. (15)). This will result in the same Hessian-vector-product-based estimator
for ˜gmλ but will set ˜gsλ = 0, yielding variance reduction for the mean parameter but not the
scale.

Alternatively, we can estimate the Hessian diagonal on the ﬂy. If we use L > 1 samples at each
iteration, we can create a per-sample estimate of the sλ-scaled diagonal of the Hessian using the
other samples Bekas et al. [2007]. As the scaled diagonal estimator is unbiased, we can construct
an unbiased estimate of the control variate mean to use in lieu of the actual mean (possibly
increasing the ﬁnal variance). A similar local baseline strategy is used for variance reduction in
Mnih and Rezende [2016].

RV-RGE Estimators We introduce three diﬀerent estimators based on variations of the gra-
dient approximation deﬁned in Eqs. (21), (22), and (23), each adressing the Hessian operations
diﬀerently.

• The Full Hessian estimator implements the three equations as written and can be used

when it is computationally feasible to use the full Hessian.

• The Hessian Diagonal estimator replaces the Hessian in (21) with a diagonal approximation,

useful for models with a cheap Hessian diagonal.

• The Hessian-vector product + local approximation (HVP+Local) uses an eﬃcient Hessian-
vector product in Eqs. (21) and (22), while approximating the diagonal term in Eq. (23)
using a local baseline. The HVP+Local approximation is geared toward models where
Hessian-vector products can be computed, but the exact diagonal of the Hessian cannot.

We detail the RV-RGE algorithm in Algorithm 1 and compare properties of these three estimators
to the pure Monte Carlo estimator in the following section.

3.3. Related Work

Recently, Roeder et al. [2017] introduced a variance reduction technique for reparameterization
gradients that ignores the parameter score component of the gradient and can be viewed as a
type of control variate for the gradient throughout the optimization procedure. This approach is
complementary to our method — our approximation is typically more accurate near the beginning
of the optimization procedure, whereas the estimator in Roeder et al. [2017] is low-variance near
convergence. We hope to incorporate information from both control variates in future work.
Per-sample estimators in a multi-sample setting for variational inference were used in Mnih and

A.C. Miller et al./Reducing Reparameterization Gradient Variance

9

Table 1
Comparing variances for RV-RGEs: L = 10-sample estimators. Values are percentage of pure MC RGE
variance — a value of 100 indicates equal variation L = 10 samples, a value of 1 percent indicates a 100-fold
decrease in variance (lower is better).

Iteration

Estimator

Ave V(·)

V(|| · ||) Ave V(·)

V(|| · ||) Ave V(·)

V(|| · ||)

gmλ

ln gsλ

gλ

early

mid

late

Full Hessian
Hessian Diag
HVP + Local

Full Hessian
Hessian Diag
HVP + Local

Full Hessian
Hessian Diag
HVP + Local

1.279
34.691
1.279

0.075
38.891
0.075

0.042
40.292
0.042

1.139
23.764
1.139

0.068
21.283
0.068

0.030
53.922
0.030

0.001
0.003
0.013

0.113
6.295
30.754

1.686
23.644
98.523

0.002
0.012
0.039

0.143
7.480
39.156

0.431
28.024
99.811

0.008
0.194
0.020

0.076
38.740
0.218

0.043
40.281
0.110

1.039
21.684
1.037

0.068
21.260
0.071

0.030
53.777
0.022

(a) adam with step size = 0.05

(b) adam with step size = .10

Fig 3: MCVI optimization trace applied to the frisk model for two values of L and step size.
We run the standard MC gradient estimator (solid line) and the RV-RGE with L = 2 and 10
samples.

Rezende [2016]. We employ this technique in a diﬀerent way; we use it to estimate computationally
intractable quantities needed to keep the gradient estimator unbiased. Black box variational
inference used control variates and Rao-Blackwellization to reduce the variance of score-function
estimators Ranganath et al. [2014]. Our development of variance reduction for reparameterization
gradients compliments their work. Other variance reduction techniques for stochastic gradient
descent have focused on stochasticity due to data subsampling Johnson and Zhang [2013], Wang
et al. [2013]. Johnson and Zhang [2013] cache statistics about the entire dataset at each epoch
to use as a control variate for noisy mini-batch gradients.

4. Experiments and Analysis

In this section we empirically examine the variance properties of RVRGs and stochastic optimiza-
tion for two real-data examples — a hierarchical Poisson GLM and a Bayesian neural network.5

• Hierarchical Poisson GLM The frisk model is a hierarchical Poisson GLM, described in

Appendix C.1. This non-conjugate model has a D = 37 dimensional posterior.

• Bayesian Neural Network The non-conjugate bnn model is a Bayesian neural network ap-
plied to the wine dataset, (see Appendix C.2) and has a D = 653 dimensional posterior.

5Code is available at https://github.com/andymiller/ReducedVarianceReparamGradients.

A.C. Miller et al./Reducing Reparameterization Gradient Variance

10

(a) adam with step size = 0.05

(b) adam with step size = 0.10

Fig 4: MCVI optimization for the bnn model applied to the wine data for various L and step
sizes. The standard MC gradient estimator (dotted) was run with 2, 10, and 50 samples; RV-RGE
(solid) was run with 2 and 10 samples. In 4b the 2-sample MC estimator falls below the frame.

Quantifying Gradient Variance Reduction We measure the variance reduction of the RGE
observed at various iterates, λt, during execution of gradient descent. Both the gradient mag-
nitude, and the marginal variance of the gradient elements — using a sample of 1000 gradients
— are reported. Further, we inspect both the mean mλ and log-scale ln sλ parameters sepa-
rately. Table 3 compares gradient variances for the frisk model for four estimators: i) pure
Monte Carlo (MC), ii) Full Hessian, iii) Hessian Diagonal, and iv) Hessian-vector product +
local approximation (HVP+Local).

Each entry in the table measures the percent of the variance of the pure Monte Carlo estimator.
We show the average variance over each component AveV(·), and the variance of the norm V(||·||).
We separate out variance in mean parameters, gm, log scale parameters, ln gs, and the entire
vector gλ. The reduction in variance is dramatic. Using HVP+Local, in the norm of the mean
parameters we see between a 80× and 3,000× reduction in variance depending on the progress of
the optimizer. The importance of the full Hessian-vector product for reducing mean parameter
variance is also demonstrated as the Hessian diagonal only reduces mean parameter variance by
a factor of 2-5.

For the variational scale parameters, ln gs, we see that early on the HVP+Local approximation
is able to reduce parameter variance by a large factor (≈ 2,000×). However, at later iterates the
HVP+Local scale parameter variance is on par with the Monte Carlo estimator, while the full
Hessian estimator still enjoys huge variance reduction. This indicates that, by this point, most of
the noise is the local Hessian diagonal estimator. We also note that in this problem, most of the
estimator variance is in the mean parameters. Because of this, the norm of the entire parameter
gradient, gλ is reduced by 100 − 5,000×. In Appendix D we report results for other values of L
as a comparison.

Optimizer Convergence and Stability We compare the optimization traces for the frisk
and bnn model for the MC and the HVP+Local estimators under various conditions. At each
iteration we estimate the true ELBO value using 2000 Monte Carlo samples. We optimize the
ELBO objective using adam Kingma and Ba [2015] for two step sizes, each trace starting at the
same value of λ0.

Figure 3 compares ELBO optimization traces for L = 2 and L = 10 samples and step sizes .05
and .1 for the frisk model. We see that the HVP+Local estimators make early progress and
converge quickly. We also see that the L = 3 pure MC estimator results in noisy optimization
paths. Figure 4 shows objective value as a function of wall clock time under various settings for the

A.C. Miller et al./Reducing Reparameterization Gradient Variance

11

bnn model. The HVP+Local estimator does more work per iteration, however it tends to converge
faster. We observe the L = 10 HVP+Local outperforming the L = 50 MC estimator.

5. Conclusion

Variational inference reframes an integration problem as an optimization problem with the caveat
that each step of the optimization procedure solves an easier integration problem. For general
models, each sub-integration problem is itself intractable, and must be estimated, typically with
Monte Carlo samples. Our work has shown that we can use more information about the variational
family to create tighter estimators of the ELBO gradient, which leads to faster and more stable
optimization. The eﬃcacy of our approach relies on the complexity of the RGE distribution to
be well-captured by linear structure which may not be true for all models. However, we found
the idea eﬀective for non-conjugate hierarchical Bayesian models and a neural network.

Our presentation is a speciﬁc instantiation of a more general idea — using cheap linear structure
to remove variation from stochastic gradient estimates. We would like to extend this idea to
more ﬂexible variational distributions, including ﬂow distributions Rezende and Mohamed [2015]
and hierarchical distributions Ranganath et al. [2016], as well as model/inference schemes with
recognition networks Kingma and Welling [2014].

Acknowledgements

The authors would like to thank Finale Doshi-Velez, Mike Hughes, Taylor Killian, Andrew Ross,
and Matt Hoﬀman for helpful conversations and comments on this work. ACM is supported by
the Applied Mathematics Program within the Oﬃce of Science Advanced Scientiﬁc Computing
Research of the U.S. Department of Energy under contract No. DE-AC02-05CH11231. NJF is
supported by a Washington Research Foundation Innovation Postdoctoral Fellowship in Neuro-
engineering and Data Science. RPA is supported by NSF IIS-1421780 and the Alfred P. Sloan
Foundation.

References

arXiv:1701.07875, 2017.

Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein GAN.

arXiv preprint

Costas Bekas, Eﬀrosyni Kokiopoulou, and Yousef Saad. An estimator for the diagonal of a matrix.

Applied numerical mathematics, 57(11):1214–1229, 2007.

David M Blei, Alp Kucukelbir, and Jon D McAuliﬀe. Variational inference: A review for statis-

ticians. Journal of the American Statistical Association, 2017.

Andrew Gelman and Jennifer Hill. Data Analysis Using Regression and Multilevel/Hierarchical

Models. Cambridge University Press, 2006.

Andrew Gelman, Jeﬀrey Fagan, and Alex Kiss. An analysis of the NYPDs stop-and-frisk policy
in the context of claims of racial bias. Journal of the American Statistical Association, 102:
813–823, 2007.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. In Advances in Neural
Information Processing Systems, pages 2672–2680, 2014.

A.C. Miller et al./Reducing Reparameterization Gradient Variance

12

Matthew D Hoﬀman, David M Blei, Chong Wang, and John William Paisley. Stochastic varia-

tional inference. Journal of Machine Learning Research, 14(1):1303–1347, 2013.

Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in Neural Information Processing Systems, pages 315–323, 2013.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings

of the International Conference on Learning Representations, 2015.

Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In Proceedings of the

International Conference on Learning Representations, 2014.

Dougal Maclaurin, David Duvenaud, Matthew Johnson, and Ryan P. Adams. Autograd: Reverse-

mode diﬀerentiation of native Python, 2015. URL http://github.com/HIPS/autograd.

James Martens, Ilya Sutskever, and Kevin Swersky. Estimating the Hessian by back-propagating

curvature. In Proceedings of the International Conference on Machine Learning, 2012.

Andriy Mnih and Danilo Rezende. Variational inference for Monte Carlo objectives. In Proceed-

ings of The 33rd International Conference on Machine Learning, pages 2188–2196, 2016.

Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv

Barak A Pearlmutter. Fast exact multiplication by the Hessian. Neural computation, 6(1):

Rajesh Ranganath, Sean Gerrish, and David M Blei. Black box variational inference. In AIS-

preprint arXiv:1610.03483, 2016.

147–160, 1994.

TATS, pages 814–822, 2014.

Rajesh Ranganath, Dustin Tran, and David M Blei. Hierarchical variational models. In Inter-

national Conference on Machine Learning, 2016.

Danilo Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows. In Proceed-
ings of the 32nd International Conference on Machine Learning (ICML-15), pages 1530–1538,
2015.

Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
In International Conference on Machine

approximate inference in deep generative models.
Learning, 2014.

Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Math-

ematical Statistics, pages 400–407, 1951.

Geoﬀrey Roeder, Yuhuai Wu Wu, and David Duvenaud. Sticking the landing: An asymptotically
zero-variance gradient estimator for variational inference. arXiv preprint arXiv:1703.09194,
2017.

Francisco R Ruiz, Michalis Titsias RC AUEB, and David Blei. The generalized reparameteriza-
tion gradient. In Advances in Neural Information Processing Systems, pages 460–468, 2016.
Dustin Tran, Matthew D Hoﬀman, Rif A Saurous, Eugene Brevdo, Kevin Murphy, and David M
In Proceedings of the International Conference on

Blei. Deep probabilistic programming.
Learning Representations, 2017.

Chong Wang, Xi Chen, Alexander J Smola, and Eric P Xing. Variance reduction for stochastic
gradient optimization. In Advances in Neural Information Processing Systems, pages 181–189,
2013.

A.C. Miller et al./Reducing Reparameterization Gradient Variance

13

Appendix A: Control Variates

Control variates are random quantities that are used to reduce the variance of a statistical estimator
without trading any bias. Concretely, given an unbiased estimator ˆg such that E[ˆg] = g (the quantity
of interest), our goal is to construct another unbiased estimator with lower variance. We can do this by
deﬁning a control variate ˜g with known expectation ˜m. We can write our new estimator as

g(cv) = ˆg − c · (˜g − ˜m) .

Clearly the new estimator has the same expectation as the original estimator, but a diﬀerent variance.
We can reduce the variance of g(cv) by setting c optimally.

Consider a univariate ˆg and ˜g, and without loss of generality, take ˜m = 0. The variance of g(cv) can be
written

V(g(cv)) = E[(ˆg − c · ˜g)2] − E[ˆg]2

= E[ˆg2 + c2 · ˜g2 − 2cˆg ˜g] − E[ˆg]2
= E[ˆg2] + c2E[˜g2] − 2cE[ˆg ˜g] − E[ˆg]2

We minimize the variance with respect to c by taking the derivative and setting equal to zero, which
implies

The covariance C(ˆg, ˜g) is typically not known a priori and must be estimated. It can be shown, under
the optimal c∗, that the variance of g(cv) is

where ρ is the correlation coeﬃcient between ˜g and ˆg.

When ˆg and ˜g are length D vectors, we can construct an estimator that depends on a matrix-valued free
parameter, C ∈ RD×D

We can show that the C that minimizes the Tr(C(g(cv))) — the sum of the marginal variances — is
given by

c∗ =

E[ˆg ˜g]
E[˜g2]

=

C(ˆg, ˜g)
V(˜g)

V(g(cv)) = (1 − ρ2)V(ˆg)

g(cv) = ˆg − C(˜g − ˜m) .

C ∗ = Σ−1

˜g Σˆg,˜g

where Σ˜g is the covariance matrix of the control variate vector, and Σˆg,˜g is the cross covariance between
ˆg and ˜g.

Intuitively, a control variate is injecting information into the estimator in the form of linear structure. If
the two quantities, ˜g and ˆg are perfectly correlated, then we already know the mean and estimation is
not necessary. As the two become uncorrelated, the linear estimator becomes less and less informative,
and reverts to the original quantity.

A.1. Control Variates and Approximate Functions

In our setting, we approximate the distribution of some function f ((cid:15)) where (cid:15) ∼ N (0, I) by a ﬁrst order
Taylor expansion about 0 — for now we examine the univariate case

f1((cid:15)) = f (0) + f (cid:48)(0)(cid:15)

(cid:15) ∈ R

(33)

(25)

(26)

(27)

(28)

(29)

(30)

(31)

(32)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

A.C. Miller et al./Reducing Reparameterization Gradient Variance

14

If we wish to use f1((cid:15)) as a control variate for f ((cid:15)), we need to characterize the covariance between the
two random variables. Because the form of f ((cid:15)) is general, it is diﬃcult to analyze. We instead derive
the covariance between f1((cid:15)) and the second-order expansion

as a surrogate.

f2((cid:15)) = f (0) + f (cid:48)(0)(cid:15) + f (cid:48)(cid:48)(0)/2(cid:15)2

C(f1((cid:15)), f2((cid:15))) = E [(f1((cid:15)) − E[f1((cid:15))])(f2((cid:15)) − E[f2((cid:15))])]

= E (cid:2)(f (cid:48)(0)(cid:15)) (cid:0)f (cid:48)(0)(cid:15) + f (cid:48)(cid:48)(0)/2(cid:15)2 − f (cid:48)(cid:48)(0)/2(cid:1)(cid:3)
= E (cid:2)f (cid:48)(0)2(cid:15)2 + (f (cid:48)(0)f (cid:48)(cid:48)(0)/2)(cid:15)3 − (f (cid:48)(0)f (cid:48)(cid:48)(0)/2)(cid:15)(cid:3)
= E (cid:2)f (cid:48)(0)2(cid:15)2(cid:3)
= V[f1((cid:15))]

where note that E[(cid:15)3] = 0. Recall that the optimal control variate can be written

c∗ = C(f1((cid:15)), f2((cid:15)))/V[f1((cid:15))]
= V[f1((cid:15))]/V[f1((cid:15))] = 1 .

Appendix B: Algorithm Details

We summarize an optimization routine using RV-RGE in Algorithm 1. The diﬀerent variants rely on the
diﬀerent forms of H(·, ·) and diag(H). The full Hessian estimator calculates these terms exactly. The
diagonal Hessian estimates the Hessian-vector product with the diagonal of the Hessian. The HVP+Local
estimator computes the Hessian-vector product exactly, but estimates the scale approximation mean using
other samples.

We also note that there are ways to optimize the additional Hessian-vector product computation. Because
each Hessian is evaluated at the same mλ, we can cache the computation in the forward pass, and only
repeat the backwards pass for each sample, as implemented in Maclaurin et al. [2015].

Appendix C: Model Deﬁnitions

C.1. Multi-level Poisson GLM

Our second test model is a 37-dimensional posterior resulting from a hierarchical Poisson GLM. This
model measures the relative rates of stop-and-frisk events for diﬀerent ethnicities and in diﬀerent precincts
Gelman et al. [2007], and has been used as illustrative example of multi-level modeling [Gelman and Hill,
2006, Chapter 15, Section 1].

ln σ2

α, ln σ2

µ ∼ N (0, 102)
β ∼ N (0, 102)
αe ∼ N (0, σ2
α)
βp ∼ N (0, σ2
β)

ln λep = µ + αe + βp + ln Nep

Yep ∼ P(λep)

mean oﬀset

group variances

ethnicity eﬀect

precinct eﬀect

log rate

stop-and-frisk events

A.C. Miller et al./Reducing Reparameterization Gradient Variance

15

where Yep are the number of stop-and-frisk events within ethnicity group e and precinct p over some
ﬁxed period of time; Nep is the total number of arrests of ethnicity group e in precinct p over the same
period of time; αe and βp are the ethnicity and precinct eﬀects.

C.2. Bayesian Neural Network

We implement a 50-unit hidden layer neural network with ReLU activation functions. We place a normal
prior over each weight in the neural network, governed by the same variance (with an inverse Gamma
prior). We also place an inverse Gamma prior over the observation variance The model can be written
as

α ∼ Gamma(1, .1)

τ ∼ Gamma(1, .1)

wi ∼ N (0, 1/α)
y|x, w, τ ∼ N (φ(x, w), 1/τ )

weight prior hyper

noise prior hyper

weights

output distribution

(42)

(43)

(44)

(45)

where w = {w} is the set of weights, and φ(x, w) is a multi-layer perceptron that maps input x to
approximate output y as a function of parameters w. We denote the set of parameters as θ (cid:44) (w, α, τ ).
We approximate the posterior p(w, α, τ |D), where D is the training set of {xn, yn}N
n=1 input-output
pairs.

We use a 100-row subsample of the wine dataset from the UCI repository https://archive.ics.uci.
edu/ml/datasets/Wine+Quality.

A.C. Miller et al./Reducing Reparameterization Gradient Variance

16

Appendix D: Variance Reduction

Below are additional variance reduction measurements for the frisk model for diﬀerent values of L,
samples drawn per iteration.

Table 2
frisk model variance comparison: L = 3-sample estimators

Iteration

Estimator

Ave V(·)

V(|| · ||) Ave V(·)

V(|| · ||) Ave V(·)

V(|| · ||)

gmλ

ln gsλ

gλ

MC
Full Hessian
Hessian Diag
HVP + Local

MC
Full Hessian
Hessian Diag
HVP + Local

MC
Full Hessian
Hessian Diag
HVP + Local

100.000
1.184
35.541
1.184

100.000
0.080
39.016
0.080

100.000
0.044
39.280
0.044

100.000
1.022
25.012
1.022

100.000
0.075
22.832
0.075

100.000
0.024
38.799
0.024

100.000
0.001
0.003
0.012

100.000
0.122
6.617
31.992

100.000
1.782
22.915
98.290

100.000
0.002
0.011
0.039

100.000
0.169
8.097
46.160

100.000
0.879
21.913
99.679

100.000
0.007
0.201
0.019

100.000
0.081
38.868
0.227

100.000
0.045
39.268
0.116

100.000
0.902
22.090
0.900

100.000
0.075
22.804
0.078

100.000
0.023
38.725
0.014

Table 3
frisk model variance comparison: L = 50-sample estimators

Iteration

Estimator

Ave V(·)

V(|| · ||) Ave V(·)

V(|| · ||) Ave V(·)

V(|| · ||)

gmλ

ln gsλ

gλ

MC
Full Hessian
Hessian Diag
HVP + Local

MC
Full Hessian
Hessian Diag
HVP + Local

MC
Full Hessian
Hessian Diag
HVP + Local

100.000
1.276
35.146
1.276

100.000
0.081
37.534
0.081

100.000
0.042
39.972
0.042

100.000
1.127
24.018
1.127

100.000
0.074
21.773
0.074

100.000
0.043
101.263
0.043

100.000
0.001
0.003
0.013

100.000
0.125
7.204
31.278

100.000
1.894
24.450
98.588

100.000
0.002
0.012
0.039

100.000
0.121
7.035
32.275

100.000
0.296
27.174
99.539

100.000
0.008
0.197
0.020

100.000
0.081
37.394
0.225

100.000
0.044
39.961
0.112

100.000
1.080
23.028
1.079

100.000
0.074
21.752
0.076

100.000
0.043
101.019
0.033

early

mid

late

early

mid

late

