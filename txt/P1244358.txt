Dealing with Out-Of-Vocabulary Problem in Sentence Alignment Using
Word Similarity

Hai-Long Trieu
Japan Advanced Institute of
Science and Technology
trieulh@jaist.ac.jp

Le-Minh Nguyen
Japan Advanced Institute of
Science and Technology
nguyenml@jaist.ac.jp

Phuong-Thai Nguyen
Vietnam National University,
Hanoi, Vietnam
thainp@vnu.edu.vn

Abstract

Sentence alignment plays an essential role in
building bilingual corpora which are valuable
resources for many applications like statisti-
cal machine translation. In various approaches
of sentence alignment, length-and-word-based
methods which are based on sentence length
and word correspondences have been shown to
be the most effective. Nevertheless a drawback
of using bilingual dictionaries trained by IBM
Models in length-and-word-based methods is
the problem of out-of-vocabulary (OOV). We
propose using word similarity learned from
monolingual corpora to overcome the prob-
lem. Experimental results showed that our
method can reduce the OOV ratio and achieve
a better performance than some other length-
and-word-based methods. This implies that
using word similarity learned from monolin-
gual data may help to deal with OOV problem
in sentence alignment.

Keywords:

sentence
vocabulary, word similarity, monolingual data

alignment,

out-of-

1

Introduction

Sentence alignment plays an important role in build-
ing bilingual corpora for statistical machine transla-
tion and many other tasks. Given documents from
two languages, the task is to align sentences which
are translations of each other. There are three main
methods in sentence alignment including length-
based, word-based, and the combination of the first
two methods. Length-based methods were proposed
in (Brown et al., 1991; Gale and Church, 1993).

(Wu, 1994) and (Melamed, 1996) introduced meth-
ods based on word correspondences. Length-based
and word-based methods were also combined to
make hybrid methods (Moore, 2002; Varga et al.,
2007).

Length-based methods which are only based on
the number of words or characters in sentence pairs
can run very fast but show a low accuracy. Mean-
while, word-based methods which use bilingual lex-
icon gain high accuracy, but heavily depend on avail-
able lexical resources. The length-and-word-based
methods which combine length-based and word-
based methods (Moore, 2002; Varga et al., 2007) do
not depend on lexical resources and overcome the
problem of low accuracy in length-based methods.
Nonetheless, a drawback of these length-and-word-
based methods which trained a bilingual dictionary
using IBM models is the OOV problem.

In this work, we propose an approach to deal with
the OOV problem in sentence alignment based on
word similarity learned from monolingual corpora.
Words that were not contained in the bilingual dic-
tionaries were replaced by their similar words from
the monolingual corpora. Experiments conducted on
English-Vietnamese sentence alignment showed that
using word similarity learned from monolingual cor-
pora can help to reduce the OOV ratio and lead to an
improvement in comparison with some other length-
and-word-based methods.

We describe phases used in our method in Section
2. Experimental results and discussions are analysed
in Section 3. An overview of related researches is
discussed in Section 4, and conclusions are drawn in
Section 5.

Figure 1: Phases in our model; S: the text of source language, T: the text of target language; S1, T1: sentences aligned
by the length-based phase; S2, T2: sentences aligned by the length-and-word-based phase; S’, T’: monolingual corpora
of the source and target languages, respectively. The components of the length-and-word-based method (Moore, 2002)
are bounded by the dashed frame.

2 Method

In this section, we describe phases used in our
method, which include four phases: the length-based
phase, the training bilingual dictionaries, using word
similarity to deal with the OOV problem, and the
combination of length-based and word-based meth-
ods. The model is illustrated in Figure 1.

2.1 Length-based Phase

Let le and lv be the lengths of English and Viet-
namese sentences, respectively. Then, le and lv
varies according to Poisson distribution as follows:

P (lv|le) = exp−lvr (ler)lv
lv!

(1)

Where r is the ratio of the mean length of Viet-
namese sentences to the mean length of English sen-
tences.

As shown in (Moore, 2002), the length-based
phase based on the Poisson distribution was slightly
better than the Gaussian distribution proposed by
(Brown et al., 1991).

log(

) − μ)2

lv
le
2σ2

−

P (lv|le) = αexp

(2)
Where μ and σ2 are the mean and variance of
the Gaussian distribution, respectively. The length-
based model based on the Poisson distribution was
shown to be simpler to estimate than the model
based on the Gaussian distribution which has to it-
eratively estimate the variance σ2 using the expecta-
tion maximization (EM) algorithm.

Our model was based on the length-based model

using the Poisson distribution.

2.2 Training IBM Model 1

Sentence pairs extracted from the length-based
phase are then used to train IBM Model 1 (Brown
et al., 1993) to build a bilingual dictionary.

Let e and v be English and Vietnamese sentences,
respectively. The procedure of generating sentence v
from a sentence e with the length of le is as follows:

1. Selecting a length lv for the sentence v

2. For each word position j in { 1..lv} of v:

(a) Selecting a word ei in e
(b) For each pair (j, ei): choosing a word vj to

fill the position j

P (v|e) =

(cid:14)
(le + 1)lv

lv(cid:4)

le(cid:3)

j=1

i=0

tr(vj|ei)

(3)

Where (cid:14) is the uniform probability for all possible

lengths of v.

2.3 Using Word Similarity to Deal with OOV

In the sentence alignment task based on word cor-
respondences, bilingual dictionaries trained on IBM
models can help to produce highly accurate sentence
pairs when they contain reliable word pairs with a
high percentage of vocabulary coverage. The OOV
problem appears when the bilingual dictionary does
not contain word pairs which are necessary to pro-
duce a correct alignment of sentences. The higher
the OOV ratio, the lower the performance. The bilin-
gual dictionary can also be expanded by training
IBM models on available bilingual data. However,
such resources are very rare especially for low-
resource language pairs like English-Vietnamese.
Meanwhile, monolingual data is easy to acquire in
an abundant amount. We propose using word sim-
ilarity learned from monolingual corpora to over-
come the OOV problem.

Monolingual corpora of English and Vietnamese
were used to train two word similarity models sep-
arately using a continuous bag-of-word model. In
continuous bag-of-words models, words are pre-
dicted based on their context, and words that appear
in the same context tend to be clustered together as
similar words. We used word2vec (Mikolov et al.,
2013), a powerful continuous bag-of-words model to
train word similarity. The word2vec model can run
very fast and enables to train continuous vector rep-
resentations of words on large data sets.

The word similarity models were then used to en-

rich the bilingual dictionary.

1. Let (ei − vj) be a word pair in the dictionary
in which ei is the English word, and vj is the
Vietnamese word.

2. Let

im

(a) sim(ei) = {e(cid:3)
(b) sim(vj) = {v(cid:3)

i1, ..., e(cid:3)
j1, ..., v(cid:3)
be sets of similar words of ei and vj, respec-
tively.

}
}

jn

3. The dictionary can be expanded as follows:

(a) For e(cid:3) in sim(ei): add pairs (e(cid:3) − vj) to the

(b) For v(cid:3) in sim(vj): add pairs (ei −v(cid:3)) to the

(c) score(e(cid:3) − vj) = score(ei − vj) ∗

(d) score(ei − v(cid:3)) = score(ei − vj) ∗

dictionary

dictionary

cosine(ei − e(cid:3))

cosine(vj − v(cid:3))

Where score(a, b) is the word translation proba-
bility of the word pair (a, b) by training IBM Model
1. cosine(a, b) is the cosine similarity between a
and b from word similarity models.

The expanded dictionary can help to cover a
higher ratio of vocabulary, which reduces the OOV
ratio and improves overall performance.

2.4 Length-based and Word-based

The expanded dictionary was then combined with
the length-based phase described in Section 2.1 to
produce final alignments, which are described as fol-
lows:

P (e, v) =

P1−1(le, lv)
(le + 1)lv

(

lv(cid:4)

le(cid:3)

j=1

i=0

tr(vj|ei))(

fu(ei))

le(cid:3)

i=1

(4)
Where fu is the observed relative unigram fre-
quency of the word in the text in the corresponding
language.

3 Experiments

3.1 Setup

We conducted experiments on the sentence align-
ment task for English-Vietnamese, a low-resource
language pair. We evaluated our method on the test
set collected from the website.1 After preprocessing
the collected data, we conducted sentence alignment
manually to achieve the reference data. We publish

1http://www.vietnamtourism.com/

these data sets on the website.2 The statistics of these
data sets are shown in Table 1.

3.2 Training Word Similarity

Statistics
Sentences (English)
Sentences (Vietnamese)
Average length (English)
Average length (Vietnamese)
Vocabulary Size (English)
Vocabulary Size (Vietnamese)
Reference Set

Test Data
1,705
1,746
22
22
6,144
5,547
837

Table 1: Statistics of Test Corpus

In order to produce a more reliable bilingual
dictionary, we added an available bilingual corpus
to train IBM Model 1, which was collected from
the IWSLT2015 workshop.3 The dataset contains
subtitles of TED talks (Cettolo et al., 2012). The
IWSLT2015 training data is shown in Table 2.

Statistics
Sentences (English)
Sentences (Vietnamese)
Average length (English)
Average length (Vietnamese)
Vocabulary Size (English)
Vocabulary Size (Vietnamese)

iwslt15
129,327
129,327
19
18
46,669
50,667

Table 2: Statistics of the IWSLT15 Corpus

In the preprocessing steps, we tokenized these
datasets using the tokenizer of Moses script4 for En-
glish and JVnTextpro5 for Vietnamese. The datasets
were then lowercased. For Vietnamese, we con-
ducted word segmentation using JVnTextpro.

For the sentence alignment algorithm, we reim-
plemented phases in the model (Moore, 2002) using
Java.

To evaluate performance we used common met-
rics: Precision, Recall, and F-measure (Véronis and
Langlais, 2000).

2https://github.com/nguyenlab/SentAlign-Similarity
3https://sites.google.com/site/iwsltevaluation2015/mt-track
4http://www.statmt.org/moses/?n=moses.baseline
5http://jvntextpro.sourceforge.net/

In order to train word similarity models, we used En-
glish and Vietnamese monolingual corpora. For En-
glish we used the one-billion-words6 dataset which
contains almost 1B words. To build a huge mono-
lingual corpus of Vietnamese, we extracted articles
from the web (www.baomoi.com)7. The data set was
then preprocessed to achieve 22 million Vietnamese
sentences.

We used word2vec from gensim python8 to train
two word-similarity models on the monolingual cor-
pora. We set the cbow model with configurations:
window size=5, vector size=100, min count = 10.
The word2vec trained model of Vietnamese is also
available on the website.2

3.3 Result and Discussion

We compared our model with the two other length-
and-word-based methods: M-align9 (Moore, 2002)
and Hun-align10 (Varga et al., 2007). We showed
how our method can deal with the OOV problem.

We setup the length-based phase’s threshold to
0.99 to extract highest sentence pairs. Then in the
length-and-word-based phase, we setup the thresh-
old to 0.9 to ensure a high confidence. Experimental
results are shown in Table 3.

Setup
Reference
Results
Correct
Precision
Recall
F-measure

M-align Hun-align OurMethod
837
609
433
71.10%
51.73%
59.89%

837
1373
616
44.87%
73.60%
55.75%

837
580
412
71.03%
49.22%
58.15%

Table 3: Experimental results. (Reference, Results, Cor-
rect: number of sentence pairs in reference set, results
from systems, and correct sentences, respectively.)

Overall, the performance of our model slightly
improved the M-align in all scores of precision, re-
call, and f-measure. Our model also gained higher

6http://www.statmt.org/lm-benchmark/
7http://www.baomoi.com/
8https://radimrehurek.com/gensim/models/word2vec.html
9http://research.microsoft.com/en-us/downloads/aafd5dcf-

4dcc-49b2-8a22-f7055113e656/

10http://mokk.bme.hu/en/resources/hunalign/

performance than Hun-align. Although Hun-align
can achieve the highest recall of 73.60% due to the
approach that Hun-align constructs dictionaries, the
method produced a number of error results, so this
caused the lowest precision.

A problem of using the IBM Model 1 as in
Moore’s method was the OOV. When the dictionary
cannot cover a high ratio of vocabulary, it decreases
the contribution of the word-based phase. The aver-
age OOV ratio is shown in Table 4. In comparison
with M-align, using word similarity in our model re-
duced the OOV ratio from 7.37% to 4.33% in En-
glish and from 7.74% to 6.80% in Vietnamese vo-
cabulary. By using word similarity models we over-
came the problem of OOV. The following discussion
will show how the word similarity models helped to
reduce the OOV ratio.

Setup
#vocab. en
#vocab. vi
OOV en
OOV vi

Test M-align Our Model
28,371
25,481
4.33%
6.80%

27,872
25,326
7.37%
7.74%

1,705
1,746
NA
NA

Table 4: Average OOV ratio.

We describe word similarity models using
word2vec with examples. Tables 5 and 6 show ex-
amples of OOV words and their most similar words
extracted from the word similarity models. The word
similarity models can explore not only helpful sim-
ilar words in terms of variants in morphology but
also words that share the same meaning but differ-
ent morphemes. There are useful similar words that
can have the same meaning as the OOV words like
word pairs ("intends" and "aims") or ("honours" and
"awards"), ("quát", "mắng"), ("ghe", "đò"). How-
ever, because in the word2vec model words are pre-
dicted based on their context in terms of windows,
some word pairs may contain different meanings like
("bangkok", "jakarta"), or ("pagoda", "citadel"),
("phở", "cơm"). Therefore extracting suitable simi-
lar words is also needed to be further investigated.

We show an example of how our method deals
with the OOV problem in Table 7. The word
pairs (reunification-thống_nhất) and (impressively-
mạnh_mẽ) were not covered by the dictionary us-
ing IBM Model 1, and this became an example of

OOV
Similar
Words Words
intends
intends
intends

aims
refuses
plans

Cosine
Similarity
0.74
0.74
0.66

honours
honours
honours

honors
prizes
awards

bangkok
bangkok

jerusalem
jakarta

pagoda
pagoda
pagoda

temple
tower
citadel

0.71
0.65
0.62

0.65
0.61

0.86
0.76
0.73

Table 5: Examples of English Word Similarity Model

OOV
Words
quát (to shout)
quát (to shout)

Similar
Words
mắng (to scold)
nạt (to bully)

Cosine
Similarity
0.35
0.32

hủy (to destroy)
hủy (to destroy)
hủy (to destroy)

hoại (to ruin)
dỡ (to unload)
phá (to demolish)

ghe (junk)
ghe (junk)
ghe (junk)

thuyền (boat)
xuồng (whaleboat)
đò (ferry)

phở (noodle soup)
phở (noodle soup)

cháo (rice gruel)
cơm (rice)

0.50
0.42
0.36

0.64
0.61
0.56

0.67
0.65

Table 6: Examples of Vietnamese Word Similarity
Model. The italic words in brackets are corresponding
English meaning which were translated by the authors.

Language

English

Sentence

since the reunification in 1975 , vietnam ’ s architecture has been impressively developing .

Vietnamese

từ sau ngày đất_nước thống_nhất ( 1975 ) kiến_trúc việt_nam phát_triển khá mạnh_mẽ .

(Translation) After the country was unified (1975), vietnam’s architecture has been developing rather impressively.

Table 7: An example of English-Vietnamese OOV. The translations to English (italic) were conducted by the authors.

Similar
Words
independence

OOV
Words
reunification
reunification unification
reunification

peace

Cosine
Similarity
0.71
0.67
0.62

impressively
impressively
impressively
impressively

amazingly
impressive
exquisitely
brilliantly

0.74
0.74
0.72
0.71

Table 8: An example of similar word pairs trained on
monolingual corpus

OOV. Examples of similar word pairs are shown in
Table 8, and translation word pairs trained by IBM
Model 1 are shown in Table 9. Because (reunifica-
tion-unification) was a similar word pair, and the
translation word pair (unification-thống_nhất) was
contained in the dictionary, the new translation word
pair (reunification-thống_nhất) was then created.
Similarly, the new translation word pair (impres-
sively-mạnh_mẽ) was created via the similar word
pair (impressively-impressive) and the translation
word pair (impressive-mạnh_mẽ). Table 10 shows
induced translation word pairs. By using word sim-
ilarity learned from monolingual corpora, a num-
ber of OOV words can be replaced by their similar
words, which helped to reduce the OOV ratio and
improve performance in overall.

4 Related Work

is an essential

Sentence alignment
task in nat-
ural language processing, which builds bilingual
corpora, a valuable resource in many applications
like statistical machine translation, word sense dis-
ambiguation, information retrieval, etc. The task
can be solved based on the number of words or

Score English

0.597130
0.051708

independence
independence

Vietnamese
độc_lập (independent)
sự_độc_lập (independence)

0.130447 unification
unification
0.130447
unification
0.130446

thống_nhất (to unify)
sự_thống_nhất (unification)
sự_hợp_nhất (unify)

0.551291
0.002927
0.002440

impressive
impressive
impressive

ấn_tượng (impression)
mạnh_mẽ (impressive)
kinh_ngạc (amazed)

Table 9: An example of bilingual dictionary trained by
IBM Model 1 (Score: translation probability); the trans-
lations to English (italic) were conducted by the authors.

Score English

Vietnamese
thống_nhất (to unify)

reunification
impressively mạnh_mẽ (impressive)

0.215471
0.369082

Table 10: Induced translation word pairs; the translations
to English (italic) were conducted by the authors.

characters(Brown et al., 1991; Gale and Church,
1993). These methods are fast and effective in
some closed language pairs like English-French but
achieve low performance in language pairs like
English-Chinese. Word-based methods were pro-
posed in (Kay and R¨oscheisen, 1993; Chen, 1993;
Wu, 1994; Melamed, 1996; Ma, 2006), based on lex-
ical resources. These methods showed better perfor-
mance than length-based methods, but they depend
on available linguistic resources, which are rare and
expensive to achieve in almost all language pairs,
especially in low-resource languages like English-
Vietnamese. Hybrid methods which combine length-
based and word-based methods as shown in (Moore,
2002; Varga et al., 2007) can overcome the low ac-
curacy of length-based methods, and these methods
also do not depend on lexical resources.

(Varga et al., 2007) proposed building bilingual
corpora for medium-density languages. This can
overcome the problem of the unavailability of bilin-
gual resources of low-resource languages by build-
ing dictionaries and merge them to make a huge dic-
tionary to cover a high ratio of vocabulary. However,
because the method does not compute the score of
word pairs in dictionaries, this leads to a low preci-
sion. Moore’s method (Moore, 2002) can gain high
accuracy, but the method has to deal with the OOV
problem. Our model is similar to Moore’s method,
but we can overcome the OOV problem based on
word similarity learned from monolingual corpora
using a continuous bag-of-words model.

Continuous bag-of-words models were proposed
in (Mikolov et al., 2013), which can learn word sim-
ilarity on very monolingual data. The model also has
been applied to learn phrase similarity on monolin-
gual data to improve statistical machine translation
(Zhao et al., 2015).

In using monolingual data for alignment tasks,
(Trieu et al., 2014) proposed using word clustering
trained on monolingual data to improve the Moore’s
method (Moore, 2002). In our model, we also based
on word similarity learned from monolingual data,
but we used a strong technique of word vector
representation, word2vec, to learn word similarity.
(Songyot and Chiang, 2014) proposed a method us-
ing word similarity from monolingual corpora to im-
prove machine translation. In the work of (Songyot
and Chiang, 2014), the word similarity is trained

based on a word context model using a feedforward
neural network and then applied to improve statisti-
cal machine translation.

The idea of using the word similarity model
learned from monolingual data based on word2vec
in our work is closed to the research of (Li et
al., 2016). In (Li et al., 2016), the word similarity
model is used to substitute rare words in neural ma-
chine translation. In our work, we adopted the word
similarity model to overcome the out-of-vocabulary
problem in sentence alignment.

5 Conclusion

In this work, we propose using word similarity to
overcome the problem of OOV in sentence align-
ment. The word2vec model was trained on mono-
lingual corpora to produce word-similarity models.
These models were then combined with the bilingual
word dictionary trained on IBM Model 1, which
were integrated to length-and-word-based phase in
a sentence alignment algorithm. Our method can re-
duce the OOV ratio with similar words learned from
monolingual corpora, which leads to an improve-
ment in comparison with some other length-and-
word-based methods. Using word similarity trained
on monolingual corpora based on a distributed word
representation model like word2vec may help to re-
duce the OOV in sentence alignment. Some aspects
of this work need to be more investigated in fu-
ture work like: applying word similarity in sentence
alignment in a large scale data; exploring the contri-
bution of word2vec in this task like using both the
cbow and skip-gram models. We also plan to further
leverage monolingual corpora to sentence alignment
and then apply to statistical machine translation, es-
pecially for low-resource languages.

References

Peter F Brown, Jennifer C Lai, and Robert L Mercer.
1991. Aligning sentences in parallel corpora. In Pro-
ceedings of the 29th annual meeting on Association for
Computational Linguistics, pages 169–176. Associa-
tion for Computational Linguistics.

Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263–311.

Kai Zhao, Hany Hassan, and Michael Auli. 2015. Learn-
ing translation models from monolingual continuous
representations.
In Proceedings of the 2015 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies.

Mauro Cettolo, Christian Girardi, and Marcello Federico.
2012. Wit3: Web inventory of transcribed and trans-
lated talks.
In Proceedings of the 16th Conference
of the European Association for Machine Translation
(EAMT), pages 261–268.

Stanley F Chen. 1993. Aligning sentences in bilingual
corpora using lexical information. In Proceedings of
the 31st annual meeting on Association for Computa-
tional Linguistics, pages 9–16. Association for Com-
putational Linguistics.

William A Gale and Kenneth W Church.

1993. A
program for aligning sentences in bilingual corpora.
Computational linguistics, 19(1):75–102.

Martin Kay and Martin R¨oscheisen.

Computational

1993.

Text-
linguistics,

translation alignment.
19(1):121–142.

Xiaoqing Li, Jiajun Zhang, and Chengqing Zong. 2016.
Towards zero unknown word in neural machine trans-
lation. In Proceedings of the 25th International Con-
ference on Artificial Intelligence.

Xiaoyi Ma. 2006. Champollion: A robust parallel text
sentence aligner.
In LREC 2006: Fifth International
Conference on Language Resources and Evaluation,
pages 489–492.
I Dan Melamed.

1996. A geometric approach to
mapping bitext correspondence. arXiv preprint cmp-
lg/9609009.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word representa-
tions in vector space. arXiv preprint arXiv:1301.3781.
Robert C Moore. 2002. Fast and accurate sentence

alignment of bilingual corpora. Springer.

Theerawat Songyot and David Chiang. 2014. Improving
word alignment using word similarity. In Proceedings
of the 2015 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1840–1845.

Hai-Long Trieu, Phuong-Thai Nguyen, and Kim-Anh
Improving moore’s sentence align-
Nguyen. 2014.
ment method using bilingual word clustering.
In
Knowledge and Systems Engineering, pages 149–160.
Springer.

Dániel Varga, Péter Halácsy, András Kornai, Viktor
Nagy, László Németh, and Viktor Trón. 2007. Paral-
lel corpora for medium density languages. Amsterdam
studies in the theory and history of linguistic science
series 4, 292:247.

Jean Véronis and Philippe Langlais. 2000. Evaluation of
parallel text alignment systems. In Parallel text pro-
cessing, pages 369–388. Springer.

Dekai Wu. 1994. Aligning a parallel english-chinese cor-
pus statistically with lexical criteria. In Proceedings of
the 32nd annual meeting on Association for Computa-
tional Linguistics, pages 80–87. Association for Com-
putational Linguistics.

