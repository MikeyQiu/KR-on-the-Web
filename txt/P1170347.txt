8
1
0
2
 
c
e
D
 
4
2
 
 
]

V
C
.
s
c
[
 
 
5
v
2
0
2
9
0
.
3
0
8
1
:
v
i
X
r
a

Unsupervised Depth Estimation,
3D Face Rotation and Replacement

Joel Ruben Antony Moniz1∗, Christopher Beckham2,3∗, Simon Rajotte2,3,
Sina Honari2, Christopher Pal2,3,4
1Carnegie Mellon University, 2Mila-University of Montreal, 3Polytechnique Montreal, 4Element AI
1jrmoniz@andrew.cmu.edu, 2honaris@iro.umontreal.ca, 3firstname.lastname@polymtl.ca

Abstract

We present an unsupervised approach for learning to estimate three dimensional
(3D) facial structure from a single image while also predicting 3D viewpoint
transformations that match a desired pose and facial geometry. We achieve this
by inferring the depth of facial keypoints of an input image in an unsupervised
manner, without using any form of ground-truth depth information. We show how
it is possible to use these depths as intermediate computations within a new back-
propable loss to predict the parameters of a 3D afﬁne transformation matrix that
maps inferred 3D keypoints of an input face to the corresponding 2D keypoints on
a desired target facial geometry or pose. Our resulting approach, called DepthNets,
can therefore be used to infer plausible 3D transformations from one face pose
to another, allowing faces to be frontalized, transformed into 3D models or even
warped to another pose and facial geometry. Lastly, we identify certain shortcom-
ings with our formulation, and explore adversarial image translation techniques as
a post-processing step to re-synthesize complete head shots for faces re-targeted to
different poses or identities. 1

1

Introduction

Face rotation is an important task in computer vision.
It has been used to frontalize faces for
veriﬁcation [8; 19; 25; 28] or to generate faces of arbitrary poses [22; 18]. In this paper we present
a novel unsupervised learning technique for face rotation and warping from a 2D source image –
whose facial appearance will be used in the rotation – to a target face – to which the facial pose
and geometry inferred from the source image is mapped. A use case is when we have an image of
someone in a particular target pose and we want to put a given source face into that pose, without
knowing the exact target face pose. This can be leveraged, for example, in the advertisement industry,
when putting someone in a particular location can be costly or unfeasible, or in the movie industry
when the main actor’s limited time or high cost can enforce using another actor whose face can be
later replaced by the main actor’s. This is achieved through estimating the source face depth and
the 3D afﬁne parameters that warp the source to the target face using neural networks. These neural
networks use a novel loss formulation for the structured prediction of keypoint depths. Once the 3D
afﬁne transformation matrix is estimated, it can be used to warp the source image onto the target
face geometry using a textured triangular mesh. The use of a 3D afﬁne transform means that we
can capture both a 3D rotation of the face to a new viewpoint as well as a global non-Euclidean
warping of the geometry to match a target face. We call these neural networks Depth Estimation-Pose
Transformation Hybrid Networks, or DepthNets in short.

Our ﬁrst contribution is to propose a neural architecture that predicts both the depth of source
keypoints as well as the parameters of a 3D geometric afﬁne transformation which constitute the

∗Indicates equal contribution.
1Code will be released at: https://github.com/joelmoniz/DepthNets/

32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.

explicit outputs of the DepthNet model. The predicted depth and afﬁne transformation could be then
used to map a source face to a target face for object orientation, distortion and viewpoint changes.

Our second contribution consists of making the observation that given 3D source and 2D target
keypoints, closed form least squares solutions exist for estimating geometric afﬁne transformation
models between these sets of keypoint correspondences, and we can therefore develop a model that
captures the dependency between depth and the afﬁne transformation parameters. More speciﬁ-
cally, we express the afﬁne transformation as a function of the pseudoinverse transformation of 2D
keypoints in a source image – augmented by inferred depths – and the target keypoints. Thus, the
second and major contribution in this work is capturing the relationship between an estimated afﬁne
transformation and the inferred depth as a deterministic relationship. In this formulation, DepthNet
only predicts depth values explicitly and the afﬁne parameters are inferred through a pseudoinverse
transformation of source and target keypoints. Here, one can directly optimize through the solutions
of what might otherwise be formulated as a secondary minimization step.

Our proposed DepthNet can map the central region of the source face to the target geometry. This
leads to background mismatch when warping one face to another. Finally, our third contribution is to
use an adversarial unpaired image-to-image transformation approach to repair the appearance of 3D
models inferred from DepthNet. Together these contributions allow 3D models of faces that construct
realistic images in the target pose. Our proposed method can be used for pose normalization or face
swaps with no manually speciﬁed 3D face model. To the best of our knowledge, this is the ﬁrst such
neural network based model that estimates a 3D afﬁne transformation model for face rotation which
neither requires ground-truth 3D images nor any ground truth 3D face information such as depth.

As we have outlined above, our approach uses neural networks for inferring depth and geometric
transformation – referred to as DepthNets; and, an adversarial image-to-image transformation network
which improves the quality of the appearance of a 3D model inferred from a DepthNet.

2 Our Approach

DepthNets

We propose three DepthNet formulations, described in Sections 2.1, 2.2, and 2.3. For each of the
three models we explore two architectural scenarios: (A) a Siamese-like architecture that uses the
source and target images themselves as well as keypoints extracted from these images, and (B) a
fully-connected neural network variant which uses only facial keypoints in the source and target
images. See Figure 1 (left) for details.

Figure 1: (Left) DepthNet architecture. The blue region is only used in case (A) and the red part is used in
both cases (A) and (B), described in Section 2. The orange output (the 8 afﬁne transformation parameters) is
predicted only by model variations described in Sections 2.1 and 2.2, and not the model described in section
2.3. All three models predict the N depth values of the source keypoints. C, P, and FC correspond to valid conv,
pool and fully-connected layers. The two paths of Siamese network share parameters and the black dots indicate
concatenating keypoint values to FC units. (Right) Visualizing face rotation by re-projecting a frontal face (far
left) to a range of other poses deﬁned by the faces in the row above (in each pair of rows). In this experiment, we
only use keypoints from the top-row in the DepthNet model (Model 7 in Table 1).

It is interesting to note that if DepthNets are used to register a set of images of objects to the same
common viewpoint, the same image and geometry can be used as the target. This is the case for
the frontalization of faces, for example. While the DepthNet framework is sufﬁciently general to be
applied to any object type where 2D keypoint detections have been made, our experiments here focus
on faces. We describe the three variants of DepthNets below.

2

2.1 Predicting Depth and Viewpoint Separately

In this variant of DepthNets, the model predicts both depths and viewpoint geometry, but as separate
explicit outputs of a neural network. The input is comprised of only the geometry and pose of the
source and target faces (encoded in the form of a 2D keypoint template), in case (B), or both keypoints
and images of the source and target faces, in case (A). The key phases of this stage are described by
the sequence of steps given below:

1. Keypoint extraction: Raw (x, y) pixel coordinates corresponding to the keypoints of each image
are extracted using a Recombinator Network (RCN) [10] architecture, and then concatenated before
being passed into the keypoint processing step.
2. (Optional) Image Feature Extraction: DepthNets can be conditioned on only keypoints, case (B),
or on keypoints and the original images, case (A). We can therefore optionally subject the source and
target images to alternating conv-maxpool layers. If this component of the architecture is used, the
last spatial feature maps in the Siamese architecture are concatenated before being given to a set of
densely connected hidden layers.
3. Keypoint processing: In this step keypoints are passed through a set of hidden layers. If the Image
Feature Extraction stage is used, the keypoints are concatenated to image features, the output of
which is in turn fed to densely connected layers. The output layer of this phase will be of size N + 8,
where N is the number of keypoints. The ﬁrst N points represent the Depth proxy, and the last 8
points form a 4 × 2 matrix representing the learned parameters of the afﬁne transform. See Figure 1.
4. Geometric Afﬁne Transformation Normalizer: This phase applies the predicted afﬁne transform
on each (depth augmented) source keypoint to estimate its target location. Let (xi
s) represent the
ith source keypoint, (xi
n) the corresponding source normalized keypoint estimated by applying
the afﬁne transformation matrix, (xi
t) the ith target keypoint (as ground truth (GT)), and Is and It
represent the source and target images respectively. Depending on which underlying architectural
variant we use, two cases arise: one that utilizes only the keypoints (B), and another utilizing
both the keypoints and the images (A). Since the keypoints are generated using RCNs, they are
technically functions of the input images: [xs, ys] = R(Is), and [xt, yt] = R(It). Depending
on the (A) or (B) variant, the ith keypoint’s predicted depth proxy zi
p is inferred as a function
of the input keypoints, or both input keypoints and input images.
In both cases the keypoints
are derived from the images, so zi
p(Is, It). Similarly, the 3D-2D afﬁne transform F is a
function of the images, such that F = F(Is, It), where the 8 predicted parameters are: m =
{m1, m2, m3, tx, m4, m5, m6, ty)}. These constitute the 3D-2D afﬁne transform which is used by
all keypoints. In other words, each of the i points is transformed using xi
s, or:

n = F(Is, It) xi

p = zi

n, yi

s, yi

t, yi

(cid:20) xi
n
yi
n

(cid:21)

=

(cid:20)m1 m2 m3
m4 m5 m6






(cid:21)

tx
ty

xi
s
yi
s
zi
p(Is, It)
1






The loss function of a DepthNet is obtained by transforming the source face to match the target face
using the simple squared error of the corresponding target object’s keypoint vector xt = [xt, yt]T
, as GT values, and the source object’s normalized keypoint vector [xn, yn]T . The loss for one
example where we predict depth and afﬁne viewpoint geometry can therefore be expressed as:

L =

K
(cid:88)

i=1

(cid:13)
(cid:13)xi
(cid:13)

t − F(Is, It) [xi

s yi

s zi

p(Is, It)]T (cid:13)
2
(cid:13)
(cid:13)

(1)

5. Image Warper: This phase consists of using the depth proxy and afﬁne transform matrix generated
to actually warp the face from its source pose to be matched to the target object geometry. The ﬁnal
projection to 2D is achieved by simply dropping the transformed z coordinate (which corresponds
to an orthographic projection model). In the case of DepthNets, this orthographic projection is
effectively embedded in the Geometric Afﬁne Transformation Normalizer step, since the afﬁne
corresponding to the z coordinate is not predicted, essentially dropping it.
As we operate on keypoints, the actual warping of pixels can be performed with a high quality
OpenGL pipeline that performs the warp separately from the rest of the architecture. Source image,
keypoints augmented with depth, and the afﬁne matrix are passed to OpenGL pipeline to warp the
source image towards the target pose. This OpenGL warping is not needed during DepthNet training,
which means we do not have to do feedforward or backprop through OpenGL. In Summary, for step 1
the RCN model [10] is used, for steps 2 to 4 the DepthNet model, shown in Figure 1 (left), is trained,

3

and for step 5 an OpenGL pipeline is used. No data or parameters are needed to train the OpenGL
pipeline. It warps images by directly using the provided data.

2.2 Estimating Viewpoint Geometry as a Second Step

In this model variant, training is similar to Section 2.1 and the model outputs depth and 3D
afﬁne transformation parameters. However, at test time, rather than using the predicted 3D afﬁne
transformation for pairs of faces, we use only the predicted depths and estimate the afﬁne ge-
ometry parameters as a second estimation step. More precisely, given 3D points for a scene
and the corresponding 2D points for a target geometry it is possible to formulate the estima-
tion of a 3D afﬁne transformation as a linear least squares estimation problem. An overdeter-
mined system of the form Am = xt for this problem can be constructed as shown in (2).

This corresponds to an afﬁne camera model
followed by an orthographic projection to
2D keypoints. This setup also leads to
the following closed form solution for the
afﬁne transformation parameters:

m = [AT A]−1AT xt,

(3)

y1
s
0
y2
s
0

z1
s
0
z2
s
0

x1
s
0
x2
s
0













xK
s
0

yK
s
0

zK
s
0

0
x1
s
0
x2
s

0
xK
s

0
y1
s
0
y2
s
...
0
yK
s

0
z1
s
0
z2
s

1 0
0 1
1 0
0 1

0
zK
s

1 0
0 1





































m1
m2
m3
m4
m5
m6
tx
ty

























x1
t
y1
t
x2
t
y2
t
...
xK
t
yK
t

=

(2)

where this pseudoinverse based transformation is parameterized by the reference points and their
predicted depths.

2.3

Joint Viewpoint and Depth Prediction

Our key observation is that one can alternatively use the closed form analytical solution, measured in
Eq. (3), for the least squares estimation problem as the underlying afﬁne transformation matrix within
the loss function. This leads to a special form of structured prediction problem for geometrically
consistent depths and afﬁne transformation matrix. For each image we have L =

K
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:21)

(cid:20) xi
t
yi
t
(cid:124) (cid:123)(cid:122) (cid:125)
xi
t

(cid:20)m1 m2 m3
m4 m5 m6

−

(cid:124)

(cid:123)(cid:122)
m

tx
ty

(cid:21)

(cid:125)







(cid:124)

xi
s
yi
s
zi
p(Is, It)
1
(cid:123)(cid:122)
xi
s







(cid:125)

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

K
(cid:88)

i=1

(cid:13)
(cid:13)xi

t − reshape(cid:2)[AT A]−1AT xt

(cid:3)xi

s

(cid:13)
2
(cid:13)

where the matrix A is parameterized as a function of xs as shown in Eq. (2). In this variant, the
model explicitly outputs only depth values during train and test time. The afﬁne transformation matrix
in the equation above is replaced by Eq. (3), which measures the afﬁne transformation as a pure
function of source and target keypoints plus the inferred depth. The big difference of this formulation
compared to Sections 2.1 and 2.2 is that geometric afﬁne transformation parameters are no longer
predicted by DepthNet during training and at both train and test time – it solves the least square loss
through the pseudoinverse based transformation. Since zi
t }j=1...N ) is predicted
within the analytical formulation of the solution to the least squares minimization problem, we can
backpropagate through the solution of a minimization problem that depends on the predicted depths.
While we leverage keypoints for depth estimation, the proposed approach is novel in how the depth is
estimated. Note that it is unsupervised with respect to depth labels. No depth supervision either by
using depth targets (as in [2; 13; 15]), or by using depth in an adversarial setting (as in [24]), is used
to estimate depth values for the base DepthNet models described in Sections 2.1, 2.2, and 2.3.

s = zi

p({xj

s, xj

t , yj

s, yj

The depths learned for keypoints by these approaches are not necessarily true depths, but are likely to
strongly correlate with the actual depth of each keypoint. This is because even though the method
succeeds (as we shall see below) in aligning poses, the inferred depth and the afﬁne transform may
each be scaled by factors so as to cancel each other out (i.e., by factors which are multiplicative
inverses of each other). Real world viewpoint geometry also involves perspective projection.

Adversarial Image-to-Image Transformation

DepthNet transforms the central region of the source face to the target pose. Inevitably, the face
background will be missing, which might make the proposed method unsuitable for many application

4

where the full face is required. To address this issue, we utilize CycleGAN [30], an adversarial
image-to-image translation technique. This serves to repair the background of faces that have
undergone frontalization or face swap through the DepthNet pipeline. Importantly, the adversarial
nature of CycleGAN allows one to perform image transformation between two domains without
the requirement of paired data. In our work, we perform experiments translating between various
domains of interest but one example is translating between the domain of images in the dataset (i.e.
the ground truth) and the domain of images where the DepthNet output is pasted onto the face region
(in the case of face-swap). By doing so we clean the face background in an unsupervised manner.

3 Experiments

3.1 DepthNet Evaluation on Paired Faces

For the experiments in this section, we use a subset of the VGG dataset [16], with training and
validating on all possible pairs of images belonging to the same identity for 2401 identities. This
yields 322,227 train and 43,940 validation pairs. Check experimental setup details in Supplementary.

Model

Color MSE MSE_norm

1) A simple 2D afﬁne registration
2) A 3D afﬁne registration model using an average 3D face template
3) A DepthNet that separately estimates depth and geometry
4) The model above, but with a Siamese CNN image model
5) Secondary least squares estimation for visual geometry using the depths from 3)
6) Secondary least squares estimation for visual geometry using the depths from 4)
7) Backpropagation through the pseudoinverse based solution for visual geometry
8) The model above, but with a Siamese CNN image model

grey
purple
brown
violet
red
green
orange
blue

1.562
0.724
0.568
0.539
0.400
0.399
0.357
0.349

9.547
7.486
6.292
6.115
5.184
5.175
4.932
4.891

Table 1: (left) Comparing the Mean Squared Error (MSE) and MSE normalized by inter-ocular distance
(MSE_norm) of different models. (right) Histogram of Mean Squared Errors. The second column in the Table
(on left) corresponds to the color of the model in the ﬁgure (on right).

We explore the three variants of DepthNets described in Sections 2.1, 2.2, and 2.3, each with two
architectural cases (A) and (B), depending on whether image features are used in addition to keypoints
or not. We also compare with a number of baselines. We measure the mean square error (MSE)
between the estimated keypoints on the target face (source face normalized keypoints) and ground
truth target keypoints. Results for the following models are shown in Table 1:

1) A baseline model registrations using a simple 2D afﬁne transformation.

2) We generate a 3D average face template from the 3DFAW dataset [14; 26; 6] by aligning the 3D
keypoints of all faces in the dataset to a front-facing face using Procrustes superimposition. We report
error by mapping the template face to each source face via Procrustes superimposition (to get a 3D
face f ) and then use an afﬁne transformation from the 3D face f to the target face.

3, 4) We use our proposed approach to predict both depth and geometry (described in Sections 2.1).

5, 6) These models described in Section 2.2. Note that during training, these two cases are similar to
models 3 and 4 in Table 1.

7, 8) The pseudo-inverse formulation model described in Section 2.3.

As observed in Table 1, a simple 2D afﬁne transform (model 1) without estimating depth and a
template 3D face (model 2) get high errors on mapping to the target faces. DepthNet models get lower
errors and the pseudo-inverse formulation (models 7 and 8) further reduces the error by 10%. The
CNN models slightly reduce errors compared to their equivalent models that rely only on keypoints.

3.2 DepthNet Evaluation on Unpaired Faces and Comparison to other Models

In this section we train DepthNet on unpaired faces belonging to different identities and compare
with other models that estimate depth. We use the 3DFAW dataset [14; 26; 6] that contains 66
3D keypoints to facilitate comparing with ground truth (GT) depth. It provides 13,671 train and
4,500 valid images. We extract from the valid set, 75 frontal, left and right looking faces yielding
a total of 225 test images, which provides a total of 50,400 source and target pairs. We train the
psuedoinverse DepthNet model that relies on only keypoints (model 7 in Table 1). We also train
a variant of DepthNet that applies an adversarial loss on the depth values (DepthNet+GAN). This
model uses a conditional discriminator that is conditioned on 2D keypoints and discriminates GT
from estimated depth values. The model is trained with both keypoint and adversarial losses.

5

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)

Source

Target

Left
Front
Right

DepthNet

DepthNet + GAN

Left
24.67
25.54
21.66

Front Right
29.70
27.71
26.19
27.22
23.87
21.48

Avg
27.36
26.32
22.34

Left
59.78
58.77
59.97

Front Right
59.63
59.67
58.61
58.67
59.60
59.70

Avg
59.69
58.68
59.76

Table 2: Comparing DepthCorr for different DepthNet models when mapping variant source to target poses. The
Avg column measures the average over the three preceding columns.

We measure the correlation matrix between GT and estimated depths, where the element k in the
diagonal indicates the correlation between estimated and ground truth depth values for keypoint k,
yielding a value between -1 and 1. We report the sum of absolute values of the diagonal of this
matrix, indicated by DepthCorr. We compare DepthNet models on DepthCorr in Table 2. For this
experiment we take every possible pair of source to target faces, where source and target are one of
{left, front, right} looking faces. This yields a total of 5,550 pairs when the source and the target are
from the same subset, and 5,625 pairs otherwise. This experiment measures the accuracy of depth
estimation of the DepthNet models on different orientations of source-target faces. The baseline
DepthNet model that does not leverage the depth labels performs well in different cases. DepthCorr
improves more than twice for the DepthNet+GAN model, indicating a direct supervision loss using
depth labels can enhance the depth estimation.

Model

GT Depth
AIGN [24]
MOFA [20]
DepthNet (Ours)
DepthNet + GAN (Ours)

Need Depth Manual Init. MSE (×10−5) Depth Correlation Matrix Trace (DepthCorr)
Right pose
66
49.04
17.54
22.34
59.76

Front pose
66
50.81
15.97
26.32
58.68

8.86 ± 6.55
9.06 ± 6.61
8.75 ± 6.33
7.65 ± 6.97
8.74 ± 6.24

Left pose
66
44.08
11.14
27.36
59.69

-
No
Yes
No
No

Yes
Yes
No
No
Yes

Table 3: Comparing MSE and DepthCorr for different models. A lower MSE indicates the model maps better to
the target faces. A higher DepthCorr indicates more correlation between estimated and GT depths.

We compare our two DepthNet models with three baselines: 1) AIGN [24], 2) MOFA [20] and 3)
GT Depth (no model trained). AIGN estimates 3D keypoints conditioned on 2D heatmaps of the
keypoints. MOFA estimates a 3D mesh using only an image. We implemented the AIGN model
and asked the authors of MOFA to run their model on our test-set. They provided MOFA’s results
for 134 images in the test set. In Table 3 we compare these three models with our DepthNet models
on DepthCorr. We also compare them on MSE, which is measured between GT and estimated
target keypoints. Since the three baselines estimate depth on a single image due to their different
model formulation, we ﬁrst measure m using closed form solution in Eq. 3 and then apply m to
the estimated source keypoints to get the target keypoint estimations. We contrast the estimated
values with the GT target keypoints. As shown in Table 3, GT depth has the highest DepthCorr
(the maximum possible value). The depths estimated by DepthNet+GAN and AIGN have stronger
correlation to GT depth compared to the baseline DepthNet and MOFA, while baseline DepthNet
performs better than MOFA. On MSE the baseline DepthNet model gets smaller MSE when mapping
to target faces, indicating it is better suited for this task.

In Figure 2 we plot heatmaps of the estimated depth of different models (on Y axis) and the GT depth
(on X axis) aggregated over all 66 keypoints on all test data. As can be seen, the depth estimated
by DepthNet+GAN and AIGN models form a 45 degree rotated ellipses showing a stronger linear
correspondence with respect to the GT depth compared to the the baseline DepthNet and MOFA.

Figure 2: Predicted (Y axis) versus Ground Truth (X axis) depth heatmaps for different models.

In Figure 3 we show some estimated depth samples for different models (see more samples in Figure
S1). AIGN and DepthNet+GAN generate more realistic results. MOFA generates very similar
face templates for different poses. Baseline DepthNet estimates reliable depth values in most cases,
however it has some failure modes as shown in the last row.

6

By comparing different models in Table 3, MOFA requires proper initialization to map face meshes
to each image. AIGN requires depth labels to train the model. Our baseline DepthNet model neither
require any depth labels nor any manual tuning. The results also show DepthNet can work well on
unpaired data. We would also like to emphasize that MOFA and AIGN are designed to estimate a 3D
model, while DepthNet is designed to estimate the parameters that facilitate warping a face pose to
another without having depth values, so these models are designed to solve different problems.

Figure 3: Depth visualization for different models (color coded by depth). From left to right: RGB image,
Ground Truth, DepthNet, DepthNet+GAN, AIGN and MOFA estimated depth values.

An interesting observation is that GT depth gets a higher MSE compared to DepthNet. This can be
due to not having a perspective projection between source and target faces. However, since DepthNet
is trained to map to the target faces, it learns the afﬁne parameters in a way to minimize this loss.

3.3 Face Rotation, Replacement and Adversarial Repair

In this section we show how DepthNet can be used for different applications. In Figure 1 (right) we
visualize the face rotation by re-projecting a frontal face, from Multi-PIE [6], (far left) to a range of
other poses deﬁned by the faces in the row above. Since DepthNet (case B) computes transformation
on keypoints rather than pixels it is robust to illuminations changes between source and target faces.
See Figures S2 to S5 for further examples. Note that DepthNet preserves well the identity. However,
it carries forward the emotion from source to target since using a global afﬁne transformation imparts
a degree of robustness to dramatic expression changes. The views in these ﬁgures are rendered from
a 3D model in OpenGL. Note the model can align well to the target face poses.

Figure 4: Background synthesis with CycleGAN. Left to right: source face; keypoints overlaid; DepthNet (DN);
DN + background → frontal;

In another experiment, we do face frontalization with synthesized background. Here we use Cy-
cleGAN to add background detail to a face that has been frontalized with DepthNet. Referring to
Figure 4, we perform this by conditioning the CycleGAN on the DepthNet image (column 3) and the
background of column 2 (masking interior face region determined by the convex hull spanned by
the keypoints). The second domain contains ground truth frontal faces. This experiment shows how
to leverage DepthNet for full face generation. Note that we do not use identity information in this
experiment. However, it can be used to better preserve the identity.

Figure 5: Left to right: source face; target face; warp to target; repaired result.

7

Finally, we do face swaps, where we warp the face of one identity onto the geometry and background
of another identity using DepthNet. To do so, we paste the rotated face by DepthNet onto the
background of the target image and train a CycleGAN to map from the domain of ‘swapped in faces’
to the ground truth faces in our dataset, effectively learning to clean up face swaps so that the face
region matches the hair and background. Some examples of this procedure are shown in Figure 5.

4 Related Work

4.1

3D Transformation on Faces

While there is a large body of literature on 3D facial analysis, many standard techniques are not
applicable to our setting here. As an example, morphable models [1] cover a wide variety of
approaches which are capable of high quality 3D reconstructions, but such methods usually require
3D face scans or reconstructions from multi-view stereo to be assembled so as to learn complex
parametric distributions over face shapes. A close approach to our own is that of [7] on viewing
real world faces in 3D. Similar to our work, this approach does not require aligned 3D face scans,
highly engineered models or manual interventions. They make the observation that if 2D keypoints
can be obtained from a single input image of a face and these keypoints are matched to an arbitrary
3D target geometry, then standard camera calibration techniques can be used to estimate plausible
intrinsics and extrinsics of the camera. This allows the estimated camera matrix, 3D rotation matrix
and 3D translation vector to be used to transform the target 3D model to the pose of the query image
from which an approximate depth can be obtained. Hassner et. al [8] explore the use of a single
unmodiﬁed 3D surface as an approximation to the shape of all input faces. In contrast, our approach
only requires 2D keypoints from the source and target faces as input. It then estimates the depth of
the source face keypoints, thereby inferring an image speciﬁc 3D model of the face.

DeepFace [19] uses face frontalization to improve the performance of a face veriﬁcation system. It
uses a 3D mask composed of facial keypoints, detects the corresponding locations of these keypoints
in the image, and maps the 2D keypoints onto a 3D face model to frontalize it. DeepFace, however,
maps to a template 3D face, therefore always mapping to a speciﬁc pose and geometry. DepthNet, on
the other hand, can map to any pose and geometry, giving it more expressive ﬂexibility.

4.2 Generative Adversarial Networks on Face Rotation

Recently, adversarial models in [12; 22; 25; 27; 18; 28] have explored face rotation. TP-GAN [12]
performs face frontalization through introducing several losses to preserve identity and symmetry of
the frontalized faces. PIM [28] frontalizes faces in a composed adversarial loss and then extracts pose
invariant features for face recognition. These models are mainly aimed for face veriﬁcation, where
they can only do face-frontalization. Another limitation of these models is in requiring ground truth
frontal images of the same identity during training. DR-GAN [22] rotates faces to any target pose by
using a discriminator that also does identity classiﬁcation in addition to pose prediction, to preserve
id and pose. While these models do pure face rotation of a 2D face, our model can warp the input
face to any other target face, allowing warping the input face to any other identity, with a different
geometry and pose. Moreover, our model also estimates the 3D geometric afﬁne transformation
parameters explicitly, allowing these parameters to be used later, e.g., for face texture swap.

FF-GAN [25], DA-GAN [27], and FaceID-GAN [18] estimate parameters of either a 3D Morphable
Model (3DMM), as in [25; 18], or source to target pose transformation, as in [27]. FF-GAN uses
3DMM parameters to frontalize faces in an adversarial approach, while FaceID-GAN uses the
3DMM parameters to generate any target pose. These models, however, train 3DMM on ground
truth labels such as identity, expression and pose. DepthNet, on the other hand, estimates depth and
afﬁne transformation parameters without requiring ground truth afﬁne or depth labels or pre-training.
Similar to DepthNet, DA-GAN [27] estimates parameters of an afﬁne transformation model that
maps a 2D face to a 3D face. Unlike DepthNet that estimates depth on the source face, DA-GAN
uses depth in a template target face. While their approach eliminates the need for depth estimation, it
only allows the source face to be mapped to the target template geometry, while DepthNet can map
the source face to any target geometry, provided by a target image, or its keypoints. We demonstrate
the application of this ﬂexibility for the face replacement task.

The aforementioned adversarial models use an identity preserving loss to maintain identity. The core
DepthNet model does not need identity labels and preserves well the identity (as shown in Figure 1

8

(right)). However, the identity information can be used by the proposed adversarial components, as in
background synthesis, to further improve the results. Unlike some of these models that take target
pose as input, DepthNet uses the target keypoints to estimate the target geometry and does not require
the target pose. This has several advantages; 1) DepthNet can map to the geometry of the target face
in addition to the pose, and 2) in the face replacement task, DepthNet can replace the target face with
the warped source face directly onto the target face location. Its application is shown in the face swap
experiment in Section 3.3.

4.3 Depth Estimation

Thewlis et. al [21] propose a mapping technique to learn a proxy of 2D landmarks in an unsupervised
way. A semi-supervised technique has been also proposed in [11] that improves landmark localization
by using weaker class labels (e.g. emotion or pose) and also by making the model predict equivariant
variations of landmarks when such transformations are applied to the image. Similar to these
approaches, DepthNet also maps a source to a target to learn its parameters. However, unlike these
two approaches that estimate 2D landmarks, DepthNet estimates the depth of the landmarks using
2D matching of keypoints, by formulating afﬁne parameters as a function of depth augmentated
keypoints in a closed form solution.

While several models [13; 2; 15] estimate depth with direct supervision, there has been recent models
[29; 5; 3] that estimate depth in an unsupervised training procedure. These models rely on pixel
reconstruction by using frames that are captured from very similar scenes, e.g. nearby frames of
a video [29] or left-right frames captures by stereo cameras [5; 3]. These models estimate depth
on one frame and then by using the disparity map, measure how pixel values of nearby frames
compare to each other. To do this, they also require camera intrinsic parameters, e.g. focal length or
distance between cameras. Unlike these models, our approach does not require source to target pixel
mapping. This allows mapping faces from different people with completely different skin colors,
without knowing camera parameters or how they are positioned with respect to each other. Therefore,
DepthNet is not susceptible to variations in illuminations or lighting between source and target faces.

Tung et. al [23] estimate 3D human pose in videos, where they use synthetic data to pre-train internal
parameters of the model and ﬁne-tune them by keypoint, segmentation and motion loss. Adversarial
Inverse Graphics Networks (AIGN) [24] estimates 3D human pose from 2D keypoint heatmaps in a
semi-supervised manner with a similar formulation to that of CycleGAN. They apply an adversarial
loss on the 3D pose to make them look realistic. These models leverage the depth values either
through synthetic data [23], or by adversarial usage of ground truth depth values [24]. Unlike these
models, DepthNet does not rely on any depth signal, either directly or indirectly. MOFA [20] builds
a 3D face mesh using a single image, where the 3D face parameters such as 3D shape and skin
reﬂectance are estimated by an encoder and then using a differentiable model they are rendered back
to the image by the decoder. This model requires manual initialization to map the input image to the
3D mesh, since otherwise it is doing an unconstrained optimization by adapting both the face pose
and the skin reﬂectance. Our model, however, does not require any manual initialization.

5 Conclusion

We have proposed a novel approach to 3D face model creation which enables pose normalization
without using any ground truth depth data. We achieve our best quantitative keypoint registration
results using our novel formulation for predicting depth and 3D visual geometry simultaneously,
learned by backpropagating through the analytic solution for the visual geometry estimation problem
expressed as a function of predicted depths. We have illustrated the quality and utility of the depths
and 3D transformations obtained using our method by transforming source faces to a wide variety of
target poses and geometries. Our technique can be used for face rotation and replacement and when
combined with adversarial repair it can blend warped faces to also synthesize the background. The
proposed model, however, carries forward emotion from source to target due to learning a shared
afﬁne parameters for all keypoints. Moreover, for extreme non-frontal faces, while DepthNet can
extract the transformation params (since it only relies on keypoints), OpenGL cannot extract texture
due to occlusion. We show an example of how to address this in the supplementary material. An
interesting extension to this paper can be replacing the OpenGL pipeline with a generative adversarial
framework that synthesizes a face using the parameters estimated by DepthNet.

9

6 Acknowledgments

We would like to thank Samsung and Google for partially funding this project. We are also thankful
to Compute Canada and Calcul Quebec for providing computational resources, and to Poonam Goyal
for helpful discussions.

References

[1] Blanz, Volker and Vetter, Thomas. A morphable model for the synthesis of 3d faces.

In
Proceedings of the 26th annual conference on Computer graphics and interactive techniques,
pp. 187–194. ACM Press/Addison-Wesley Publishing Co., 1999.

[2] Eigen, David, Puhrsch, Christian, and Fergus, Rob. Depth map prediction from a single image
using a multi-scale deep network. In Advances in Neural Information Processing Systems
(NIPS), pp. 2366–2374, 2014.

[3] Garg, Ravi, BG, Vijay Kumar, Carneiro, Gustavo, and Reid, Ian. Unsupervised cnn for single
view depth estimation: Geometry to the rescue. In Proceedings of the European Conference on
Computer Vision (ECCV), pp. 740–756. Springer, 2016.

[4] Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty of training deep feedforward
neural networks. In International conference on Artiﬁcial Intelligence and Statistics (AISTATS),
pp. 249–256, 2010.

[5] Godard, Clément, Mac Aodha, Oisin, and Brostow, Gabriel J. Unsupervised monocular depth
estimation with left-right consistency. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2017.

[6] Gross, Ralph, Matthews, Iain, Cohn, Jeffrey, Kanade, Takeo, and Baker, Simon. Multi-pie.

Image and Vision Computing, 28(5):807–813, 2010.

[7] Hassner, Tal. Viewing real-world faces in 3d.

In Proceedings of the IEEE International

Conference on Computer Vision (ICCV), pp. 3607–3614, 2013.

[8] Hassner, Tal, Harel, Shai, Paz, Eran, and Enbar, Roee. Effective face frontalization in uncon-
strained images. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 4295–4304, 2015.

[9] He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE
International Conference on Computer Vision (ICCV), pp. 1026–1034, 2015.

[10] Honari, Sina, Yosinski, Jason, Vincent, Pascal, and Pal, Christopher. Recombinator networks:
Learning coarse-to-ﬁne feature aggregation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pp. 5743–5752, 2016.

[11] Honari, Sina, Molchanov, Pavlo, Tyree, Stephen, Vincent, Pascal, Pal, Christopher, and Kautz,
Jan. Improving landmark localization with semi-supervised learning. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.

[12] Huang, Rui, Zhang, Shu, Li, Tianyu, and He, Ran. Beyond face rotation: Global and local
perception gan for photorealistic and identity preserving frontal view synthesis. In Proceedings
of the IEEE International Conference on Computer Vision (ICCV), Oct 2017.

[13] Jackson, Aaron S, Bulat, Adrian, Argyriou, Vasileios, and Tzimiropoulos, Georgios. Large pose
3d face reconstruction from a single image via direct volumetric cnn regression. In Proceedings
of the IEEE International Conference on Computer Vision (ICCV), pp. 1031–1039. IEEE, 2017.

[14] Jeni, László A, Cohn, Jeffrey F, and Kanade, Takeo. Dense 3d face alignment from 2d videos in
real-time. In IEEE International Conference and Workshops on Automatic Face and Gesture
Recognition (FG), volume 1, pp. 1–8. IEEE, 2015.

10

[15] Liu, Fayao, Shen, Chunhua, and Lin, Guosheng. Deep convolutional neural ﬁelds for depth
estimation from a single image. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 5162–5170, 2015.

[16] Parkhi, Omkar M, Vedaldi, Andrea, and Zisserman, Andrew. Deep face recognition. Proceedings

of the British Machine Vision Conference (BMVC), 2015.

[17] Sagonas, Christos, Tzimiropoulos, Georgios, Zafeiriou, Stefanos, and Pantic, Maja. 300 faces
in-the-wild challenge: The ﬁrst facial landmark localization challenge. In Proceedings of the
IEEE International Conference on Computer Vision Workshops (CVPRW), pp. 397–403, 2013.

[18] Shen, Yujun, Luo, Ping, Yan, Junjie, Wang, Xiaogang, and Tang, Xiaoou. Faceid-gan: Learning
a symmetry three-player gan for identity-preserving face synthesis. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 821–830, 2018.

[19] Taigman, Yaniv, Yang, Ming, Ranzato, Marc’Aurelio, and Wolf, Lior. Deepface: Closing the
gap to human-level performance in face veriﬁcation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 1701–1708, 2014.

[20] Tewari, Ayush, Zollhöfer, Michael, Kim, Hyeongwoo, Garrido, Pablo, Bernard, Florian, Perez,
Patrick, and Theobalt, Christian. Mofa: Model-based deep convolutional face autoencoder for
unsupervised monocular reconstruction. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), volume 2, 2017.

[21] Thewlis, James, Bilen, Hakan, and Vedaldi, Andrea. Unsupervised learning of object landmarks
by factorized spatial embeddings. In Proceedings of the IEEE International Conference on
Computer Vision (ICCV), volume 1, pp. 5, 2017.

[22] Tran, Luan, Yin, Xi, and Liu, Xiaoming. Disentangled representation learning gan for pose-
invariant face recognition. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017.

[23] Tung, Hsiao-Yu, Tung, Hsiao-Wei, Yumer, Ersin, and Fragkiadaki, Katerina. Self-supervised
learning of motion capture. In Advances in Neural Information Processing Systems (NIPS), pp.
5242–5252, 2017.

[24] Tung, Hsiao-Yu Fish, Harley, Adam W, Seto, William, and Fragkiadaki, Katerina. Adversarial
inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation from
unpaired supervision. In Proceedings of the IEEE International Conference on Computer Vision
(ICCV), volume 2, 2017.

[25] Yin, Xi, Yu, Xiang, Sohn, Kihyuk, Liu, Xiaoming, and Chandraker, Manmohan. Towards
large-pose face frontalization in the wild. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), pp. 1–10, 2017.

[26] Zhang, Xing, Yin, Lijun, Cohn, Jeffrey F, Canavan, Shaun, Reale, Michael, Horowitz, Andy,
Liu, Peng, and Girard, Jeffrey M. Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic
facial expression database. Image and Vision Computing, 32(10):692–706, 2014.

[27] Zhao, Jian, Xiong, Lin, Jayashree, Panasonic Karlekar, Li, Jianshu, Zhao, Fang, Wang, Zhecan,
Pranata, Panasonic Sugiri, Shen, Panasonic Shengmei, Yan, Shuicheng, and Feng, Jiashi. Dual-
agent gans for photorealistic and identity preserving proﬁle face synthesis. In Advances in
Neural Information Processing Systems (NIPS), pp. 66–76, 2017.

[28] Zhao, Jian, Cheng, Yu, Xu, Yan, Xiong, Lin, Li, Jianshu, Zhao, Fang, Jayashree, Karlekar,
Pranata, Sugiri, Shen, Shengmei, Xing, Junliang, et al. Towards pose invariant face recognition
in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 2207–2216, 2018.

[29] Zhou, Tinghui, Brown, Matthew, Snavely, Noah, and Lowe, David G. Unsupervised learning of
depth and ego-motion from video. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2017.

[30] Zhu, Jun-Yan, Park, Taesung, Isola, Phillip, and Efros, Alexei A. Unpaired image-to-image
translation using cycle-consistent adversarial networks. In Proceedings of the IEEE International
Conference on Computer Vision (ICCV), 2017.

11

Unsupervised Depth Estimation, 3D Face Rotation and
Replacement: Supplementary Information

S.1 Experimental Setup

The RCN, the DepthNets and the CycleGAN modules are trained separately. Each model is trained
using standard techniques for the model class and has a separate objective to be optimized. DepthNet
does not use the OpenGL pipeline during training and only uses it to render faces at test time, allowing
DepthNet to train faster.

S.1.1 DepthNet Experimental Details

Here we describe the details of the experiments carried out in Section 3.1. Our DepthNet architectures
require keypoints of both source and target images to be extracted. For this, the image is ﬁrst passed
through the VGG-Face [16] face detector. The face crops are then scaled down to 80 × 80 and
converted to greyscale, following which they are passed through RCN to obtain N = 68 keypoints
on each image. The RCN is trained exactly as described in [10], using the 300W dataset [17].

The keypoint only variant of our model involves concatenating all detected keypoints and passing
them through a two-layer deep fully connected network, with 256 hidden units and o output units.
The size of o depends on whether we are predicting only the depth, in which case o = N , or both
depth and afﬁne transformation parameters, in which case o = N + 8.

As discussed above, it is possible to augment these models with a Siamese CNN module (case A). In
the model variants that also use the image, we pass both the source and the target images through
three conv-maxpool layers with shared weights of size (32, 4, 2), (48, 3, 2), (64, 2, 2), respectively for
the representation (num_filters, filter_size, pool_size). The network’s outputs for the
source and target faces are then concatenated before passing them into a 4-layered fully connected
network with respective output sizes of 2048, 512, 256, and o. The keypoints are concatenated to the
512-unit layer before being passed to the last two layers. See Figure 1 (left) for an illustration of the
model. We explore these Siamese CNN augmented variants in models 4, 6 and 8 in Table 1.

We set the initial learning rate to 0.001 and use a Nesterov momentum optimizer (with a momentum
of 0.9) in all our experiments. With the exception of the last layer, we initialize all weights with
a Glorot initialization scheme [4], with the weights sampled from a uniform distribution. We use
a ReLU gain [9], set all biases to 0, and apply a ReLU non-linearity after every layer. In the ﬁnal
output layer, we do not apply any non-linearity and initialize the weights to 0. The biases of units that
represent depths are initialized to a random Normal distribution with µ = 0 and σ = 0.5, while those
that form the predicted afﬁne transform are initialized with the equivalent of a "ﬂattened" identity
transform. All models have been trained for 500 epochs.

We point out that except for a comparison between learning rates in the set {0.01, 0.001, 0.0001}
over few (less than 10) epochs, to ﬁnd a learning rate that the model seems to train well with, we
have not performed a hyperparameter search, and anticipate that the performance of the model can be
made even better by searching the hyperparameter space on a per model basis and by using deeper
(or modiﬁed) architectures.

S.2 Depth Visualization

We show further estimated depth values by different models in Figure S1. DepthNet+GAN and AIGN
generate the closest depth values to the GT depth. The baseline DepthNet model estimates reliable
depth values for most cases, however it has some degree of inaccuracy, as shown in the last two rows.
In DepthNet, the estimated depth indicates the position of each keypoint relative to other keypoints
rather than with respect to a source and importantly it is done without any supervision. MOFA, which
is also unsupervised, generates very similar face templates for different cases.

12

Figure S1: Depth visualization for different models (color coded by depth). The Depth axis is the one pointing
into the page. From left to right: RGB image, Ground Truth, DepthNet, DepthNet+GAN, AIGN and MOFA
estimated depth values.

S.3 Additional Camera Sweep Visualizations

In this section, we present additional visualizations along the lines of those shown in Figure 1 (right)
in Section 2 of the main paper. Frontal faces selected from the Multi-PIE dataset [6] are re-projected
to match several other poses corresponding to a person with a different identity. The DepthNets
predicts reliable proxy depths, which when coupled with the analytically obtained afﬁne transform
(obtained from the least-squares pseudo-inverse-based solution described in Section 2.3 of the main
paper) yields faces close to the desired target face geometry when passed through the OpenGL
pipeline.

We show camera sweeps for frontal source faces in Figures S2 and S3 and non-frontal source faces in
Figure S4. In Figure S5 we use the same target identity as the source face, showing how much the

13

Figure S2: Projecting a frontal face (far left) to a range of other poses deﬁned by faces in the row above.

generated face differs from the ground truth target. For all samples in Figures S2, S3, S4, and S5 we
use the DepthNet model that relies on only key-points (model 7 in Table 1).

Note that for non-frontal source faces, the quality of images is reduced specially for frontal target
faces. This is due to lack of adequate texture on the occluded side of the face to be transferred to the
target pose by OpenGL pipeline, rather than inaccuracies in the afﬁne transformation parameters. In
order to reduce the side-affects, we use either of the source face or its ﬂipped version, that is closer to
the target face pose, and then warp the face to the target keypoints.

Figure S3: Projecting a frontal face (far left) to a range of other poses deﬁned by faces in the row above.

Figure S4: Re-projecting a non-frontal face (far left) to a range of other poses deﬁned by faces in the row above.

Figure S5: Rotating a face (far left) to a range of other poses deﬁned by faces of the same identity in the row
above. On the top row frontal and on the bottom row non-frontal source faces are shown.

S.4 CycleGAN

Suppose we have some images belonging to one of two sets x ∈ X and y ∈ Y , where x denotes a
DepthNet-resulting face and y a ground truth face which is frontal. We wish to learn two functions

14

(cid:105)

(cid:105)

,

(cid:104)

Ex,y

min
G,F

(cid:104)

Ex,y

min
DX ,DY

F : X → Y and G : Y → X which are able to map an image from one set to the corresponding
image in the other. Correspondingly, we have two discriminators DX and DY which try to detect
whether the image in that particular set is real or generated. While we are only interested in the
function F : X → Y (since this is mapping to the distribution of ground truth faces) the formulation
of CycleGAN requires that we learn mappings in both directions during training. We optimize the
following objectives for the two generators F and G:

(cid:96)(DX (G(y)), 1) + (cid:96)(DY (F (x)), 1) + λ||y − F (G(y))||1 + λ||x − G(F (x))||1

(4)

And the following for the two discriminators DX and DY :

(cid:96)(DX (x), 1) + (cid:96)(DX (G(y)), 0) + (cid:96)(DY (y), 1) + (cid:96)(DY (F (x)), 0)

(5)

where 0/1 denote fake/real, (cid:96)() is the squared error loss and λ is a coefﬁcient for the cycle-consistency
(reconstruction) loss.

In the case where we did adversarial background synthesis, x is a channel-wise concatenation of the
DepthNet-frontalized face and the background of the original (pre-frontalized) image. For face-swap
cleanup, x is simply a source face which has been warped to a target face and pasted on top.

Once the network has been trained, we can disregard all other functions and use F to clean up faces
which are low quality due to artifacts from warping.

In terms of architectural details the generators and discriminators used were those described in the ap-
pendix of the CycleGAN paper [30]. In short, the generator consists of three conv-BN-relu blocks
which downsample the input, followed by nine ResNet blocks (which can be interpreted as iteratively
performing transformations over the downsampled representation), followed by deconv-BN-relu
blocks to upsample the representation back into the original input size. For training, we use the
same hyperparameters as most CycleGAN implementations which is using the Adam optimizer with
learning rate α = 2 × 10−4, β1 = 0.5, β2 = 0.999. However, instead of using a batch size of 1 we
use the largest possible batch size, which was 16 for a 12GB GPU.

Note that in order to produce better translations, the dataset we used for all CycleGAN experiments
contain both the VGG and the CelebA datasets, which has signiﬁcantly more images.

S.4.1 Background Synthesis

We present extra visualizations for the CycleGAN which performs background synthesis on CelebA,
corresponding to Figure 4 in the main paper. These are shown in Figure S6.

Figure S6: Background synthesis with CycleGAN. Left to right: source face; keypoints overlaid; DepthNet
(DN); DN + background → frontal

S.4.2 Face Replacement and Adversarial Repair

In Figure S7 we provide extra face swap samples on CelebA, corresponding to Figure 5 in the
main paper, where a source face is warped to a target face pose using DepthNet, pasted onto the

15

target image and then passed to a CycleGAN to adapt the face skin of the warped source face to the
background and hairstyle of the target face.

Figure S7: Face swap experiment with CycleGAN. Left to right: source face; target face; warp to target with
DepthNet; repaired result with CycleGAN. The source face is taken and warped onto the target face. The
background and hairstyle is then adapted to the target face.

S.4.3 Extreme Pose Face Clean-up

If the source image has an extreme pose, the texture will be missing on the occluded side of the
face and the OpenGL pipeline cannot rotate the face without artifacts. Note that this shortcoming is
due to lack of texture on the occluded side of the face rather than a deﬁciency of the transformation
parameters measured by DepthNet.

We performed an experiment using CycleGAN to ﬁx such artifacts. For this experiment we take
source images from CelebA and ﬁrst frontalize it by using DepthNet. Since the frontalized faces have
artifacts due to stretch of texture on the occluded side of the face by the OpenGL pipeline, we train a
CycleGAN that takes DepthNet frontalizaed faces plus the background of the original non-frontal
image (as two images) in one domain and the ground truth frontal faces in the other domain. The
CycleGAN learns to clean-up these artifacts. Finally, we take the GAN-repaired frontalized faces
and project it to different target poses using DepthNet. In Figure S8 we visualize camera sweep for
source faces in the wild that have extreme poses. The cycleGAN reasonably cleans the face artifacts
and then DepthNet projects it to different poses. This is just one approach to address the extreme
pose occlusion artifacts. We see alternative methods for addressing this issue as promising directions
for future research.

Figure S8: Re-projecting a non-frontal face (far left) from CelebA to a range of other poses deﬁned by faces in
the row above. Top row (in each pair) depicts the target faces from Multi-PIE [6]. The bottom row shows from
left to right: source face, souce face frontalized by DepthNet, adversarial-repaired face, the repaired source face
projected to the target poses (4th to 10th columns).

16

8
1
0
2
 
c
e
D
 
4
2
 
 
]

V
C
.
s
c
[
 
 
5
v
2
0
2
9
0
.
3
0
8
1
:
v
i
X
r
a

Unsupervised Depth Estimation,
3D Face Rotation and Replacement

Joel Ruben Antony Moniz1∗, Christopher Beckham2,3∗, Simon Rajotte2,3,
Sina Honari2, Christopher Pal2,3,4
1Carnegie Mellon University, 2Mila-University of Montreal, 3Polytechnique Montreal, 4Element AI
1jrmoniz@andrew.cmu.edu, 2honaris@iro.umontreal.ca, 3firstname.lastname@polymtl.ca

Abstract

We present an unsupervised approach for learning to estimate three dimensional
(3D) facial structure from a single image while also predicting 3D viewpoint
transformations that match a desired pose and facial geometry. We achieve this
by inferring the depth of facial keypoints of an input image in an unsupervised
manner, without using any form of ground-truth depth information. We show how
it is possible to use these depths as intermediate computations within a new back-
propable loss to predict the parameters of a 3D afﬁne transformation matrix that
maps inferred 3D keypoints of an input face to the corresponding 2D keypoints on
a desired target facial geometry or pose. Our resulting approach, called DepthNets,
can therefore be used to infer plausible 3D transformations from one face pose
to another, allowing faces to be frontalized, transformed into 3D models or even
warped to another pose and facial geometry. Lastly, we identify certain shortcom-
ings with our formulation, and explore adversarial image translation techniques as
a post-processing step to re-synthesize complete head shots for faces re-targeted to
different poses or identities. 1

1

Introduction

Face rotation is an important task in computer vision.
It has been used to frontalize faces for
veriﬁcation [8; 19; 25; 28] or to generate faces of arbitrary poses [22; 18]. In this paper we present
a novel unsupervised learning technique for face rotation and warping from a 2D source image –
whose facial appearance will be used in the rotation – to a target face – to which the facial pose
and geometry inferred from the source image is mapped. A use case is when we have an image of
someone in a particular target pose and we want to put a given source face into that pose, without
knowing the exact target face pose. This can be leveraged, for example, in the advertisement industry,
when putting someone in a particular location can be costly or unfeasible, or in the movie industry
when the main actor’s limited time or high cost can enforce using another actor whose face can be
later replaced by the main actor’s. This is achieved through estimating the source face depth and
the 3D afﬁne parameters that warp the source to the target face using neural networks. These neural
networks use a novel loss formulation for the structured prediction of keypoint depths. Once the 3D
afﬁne transformation matrix is estimated, it can be used to warp the source image onto the target
face geometry using a textured triangular mesh. The use of a 3D afﬁne transform means that we
can capture both a 3D rotation of the face to a new viewpoint as well as a global non-Euclidean
warping of the geometry to match a target face. We call these neural networks Depth Estimation-Pose
Transformation Hybrid Networks, or DepthNets in short.

Our ﬁrst contribution is to propose a neural architecture that predicts both the depth of source
keypoints as well as the parameters of a 3D geometric afﬁne transformation which constitute the

∗Indicates equal contribution.
1Code will be released at: https://github.com/joelmoniz/DepthNets/

32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.

explicit outputs of the DepthNet model. The predicted depth and afﬁne transformation could be then
used to map a source face to a target face for object orientation, distortion and viewpoint changes.

Our second contribution consists of making the observation that given 3D source and 2D target
keypoints, closed form least squares solutions exist for estimating geometric afﬁne transformation
models between these sets of keypoint correspondences, and we can therefore develop a model that
captures the dependency between depth and the afﬁne transformation parameters. More speciﬁ-
cally, we express the afﬁne transformation as a function of the pseudoinverse transformation of 2D
keypoints in a source image – augmented by inferred depths – and the target keypoints. Thus, the
second and major contribution in this work is capturing the relationship between an estimated afﬁne
transformation and the inferred depth as a deterministic relationship. In this formulation, DepthNet
only predicts depth values explicitly and the afﬁne parameters are inferred through a pseudoinverse
transformation of source and target keypoints. Here, one can directly optimize through the solutions
of what might otherwise be formulated as a secondary minimization step.

Our proposed DepthNet can map the central region of the source face to the target geometry. This
leads to background mismatch when warping one face to another. Finally, our third contribution is to
use an adversarial unpaired image-to-image transformation approach to repair the appearance of 3D
models inferred from DepthNet. Together these contributions allow 3D models of faces that construct
realistic images in the target pose. Our proposed method can be used for pose normalization or face
swaps with no manually speciﬁed 3D face model. To the best of our knowledge, this is the ﬁrst such
neural network based model that estimates a 3D afﬁne transformation model for face rotation which
neither requires ground-truth 3D images nor any ground truth 3D face information such as depth.

As we have outlined above, our approach uses neural networks for inferring depth and geometric
transformation – referred to as DepthNets; and, an adversarial image-to-image transformation network
which improves the quality of the appearance of a 3D model inferred from a DepthNet.

2 Our Approach

DepthNets

We propose three DepthNet formulations, described in Sections 2.1, 2.2, and 2.3. For each of the
three models we explore two architectural scenarios: (A) a Siamese-like architecture that uses the
source and target images themselves as well as keypoints extracted from these images, and (B) a
fully-connected neural network variant which uses only facial keypoints in the source and target
images. See Figure 1 (left) for details.

Figure 1: (Left) DepthNet architecture. The blue region is only used in case (A) and the red part is used in
both cases (A) and (B), described in Section 2. The orange output (the 8 afﬁne transformation parameters) is
predicted only by model variations described in Sections 2.1 and 2.2, and not the model described in section
2.3. All three models predict the N depth values of the source keypoints. C, P, and FC correspond to valid conv,
pool and fully-connected layers. The two paths of Siamese network share parameters and the black dots indicate
concatenating keypoint values to FC units. (Right) Visualizing face rotation by re-projecting a frontal face (far
left) to a range of other poses deﬁned by the faces in the row above (in each pair of rows). In this experiment, we
only use keypoints from the top-row in the DepthNet model (Model 7 in Table 1).

It is interesting to note that if DepthNets are used to register a set of images of objects to the same
common viewpoint, the same image and geometry can be used as the target. This is the case for
the frontalization of faces, for example. While the DepthNet framework is sufﬁciently general to be
applied to any object type where 2D keypoint detections have been made, our experiments here focus
on faces. We describe the three variants of DepthNets below.

2

2.1 Predicting Depth and Viewpoint Separately

In this variant of DepthNets, the model predicts both depths and viewpoint geometry, but as separate
explicit outputs of a neural network. The input is comprised of only the geometry and pose of the
source and target faces (encoded in the form of a 2D keypoint template), in case (B), or both keypoints
and images of the source and target faces, in case (A). The key phases of this stage are described by
the sequence of steps given below:

1. Keypoint extraction: Raw (x, y) pixel coordinates corresponding to the keypoints of each image
are extracted using a Recombinator Network (RCN) [10] architecture, and then concatenated before
being passed into the keypoint processing step.
2. (Optional) Image Feature Extraction: DepthNets can be conditioned on only keypoints, case (B),
or on keypoints and the original images, case (A). We can therefore optionally subject the source and
target images to alternating conv-maxpool layers. If this component of the architecture is used, the
last spatial feature maps in the Siamese architecture are concatenated before being given to a set of
densely connected hidden layers.
3. Keypoint processing: In this step keypoints are passed through a set of hidden layers. If the Image
Feature Extraction stage is used, the keypoints are concatenated to image features, the output of
which is in turn fed to densely connected layers. The output layer of this phase will be of size N + 8,
where N is the number of keypoints. The ﬁrst N points represent the Depth proxy, and the last 8
points form a 4 × 2 matrix representing the learned parameters of the afﬁne transform. See Figure 1.
4. Geometric Afﬁne Transformation Normalizer: This phase applies the predicted afﬁne transform
on each (depth augmented) source keypoint to estimate its target location. Let (xi
s) represent the
ith source keypoint, (xi
n) the corresponding source normalized keypoint estimated by applying
the afﬁne transformation matrix, (xi
t) the ith target keypoint (as ground truth (GT)), and Is and It
represent the source and target images respectively. Depending on which underlying architectural
variant we use, two cases arise: one that utilizes only the keypoints (B), and another utilizing
both the keypoints and the images (A). Since the keypoints are generated using RCNs, they are
technically functions of the input images: [xs, ys] = R(Is), and [xt, yt] = R(It). Depending
on the (A) or (B) variant, the ith keypoint’s predicted depth proxy zi
p is inferred as a function
of the input keypoints, or both input keypoints and input images.
In both cases the keypoints
are derived from the images, so zi
p(Is, It). Similarly, the 3D-2D afﬁne transform F is a
function of the images, such that F = F(Is, It), where the 8 predicted parameters are: m =
{m1, m2, m3, tx, m4, m5, m6, ty)}. These constitute the 3D-2D afﬁne transform which is used by
all keypoints. In other words, each of the i points is transformed using xi
s, or:

n = F(Is, It) xi

p = zi

n, yi

s, yi

t, yi

(cid:20) xi
n
yi
n

(cid:21)

=

(cid:20)m1 m2 m3
m4 m5 m6






(cid:21)

tx
ty

xi
s
yi
s
zi
p(Is, It)
1






The loss function of a DepthNet is obtained by transforming the source face to match the target face
using the simple squared error of the corresponding target object’s keypoint vector xt = [xt, yt]T
, as GT values, and the source object’s normalized keypoint vector [xn, yn]T . The loss for one
example where we predict depth and afﬁne viewpoint geometry can therefore be expressed as:

L =

K
(cid:88)

i=1

(cid:13)
(cid:13)xi
(cid:13)

t − F(Is, It) [xi

s yi

s zi

p(Is, It)]T (cid:13)
2
(cid:13)
(cid:13)

(1)

5. Image Warper: This phase consists of using the depth proxy and afﬁne transform matrix generated
to actually warp the face from its source pose to be matched to the target object geometry. The ﬁnal
projection to 2D is achieved by simply dropping the transformed z coordinate (which corresponds
to an orthographic projection model). In the case of DepthNets, this orthographic projection is
effectively embedded in the Geometric Afﬁne Transformation Normalizer step, since the afﬁne
corresponding to the z coordinate is not predicted, essentially dropping it.
As we operate on keypoints, the actual warping of pixels can be performed with a high quality
OpenGL pipeline that performs the warp separately from the rest of the architecture. Source image,
keypoints augmented with depth, and the afﬁne matrix are passed to OpenGL pipeline to warp the
source image towards the target pose. This OpenGL warping is not needed during DepthNet training,
which means we do not have to do feedforward or backprop through OpenGL. In Summary, for step 1
the RCN model [10] is used, for steps 2 to 4 the DepthNet model, shown in Figure 1 (left), is trained,

3

and for step 5 an OpenGL pipeline is used. No data or parameters are needed to train the OpenGL
pipeline. It warps images by directly using the provided data.

2.2 Estimating Viewpoint Geometry as a Second Step

In this model variant, training is similar to Section 2.1 and the model outputs depth and 3D
afﬁne transformation parameters. However, at test time, rather than using the predicted 3D afﬁne
transformation for pairs of faces, we use only the predicted depths and estimate the afﬁne ge-
ometry parameters as a second estimation step. More precisely, given 3D points for a scene
and the corresponding 2D points for a target geometry it is possible to formulate the estima-
tion of a 3D afﬁne transformation as a linear least squares estimation problem. An overdeter-
mined system of the form Am = xt for this problem can be constructed as shown in (2).

This corresponds to an afﬁne camera model
followed by an orthographic projection to
2D keypoints. This setup also leads to
the following closed form solution for the
afﬁne transformation parameters:

m = [AT A]−1AT xt,

(3)

y1
s
0
y2
s
0

z1
s
0
z2
s
0

x1
s
0
x2
s
0













xK
s
0

yK
s
0

zK
s
0

0
x1
s
0
x2
s

0
xK
s

0
y1
s
0
y2
s
...
0
yK
s

0
z1
s
0
z2
s

1 0
0 1
1 0
0 1

0
zK
s

1 0
0 1





































m1
m2
m3
m4
m5
m6
tx
ty

























x1
t
y1
t
x2
t
y2
t
...
xK
t
yK
t

=

(2)

where this pseudoinverse based transformation is parameterized by the reference points and their
predicted depths.

2.3

Joint Viewpoint and Depth Prediction

Our key observation is that one can alternatively use the closed form analytical solution, measured in
Eq. (3), for the least squares estimation problem as the underlying afﬁne transformation matrix within
the loss function. This leads to a special form of structured prediction problem for geometrically
consistent depths and afﬁne transformation matrix. For each image we have L =

K
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:21)

(cid:20) xi
t
yi
t
(cid:124) (cid:123)(cid:122) (cid:125)
xi
t

(cid:20)m1 m2 m3
m4 m5 m6

−

(cid:124)

(cid:123)(cid:122)
m

tx
ty

(cid:21)

(cid:125)







(cid:124)

xi
s
yi
s
zi
p(Is, It)
1
(cid:123)(cid:122)
xi
s







(cid:125)

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

K
(cid:88)

i=1

(cid:13)
(cid:13)xi

t − reshape(cid:2)[AT A]−1AT xt

(cid:3)xi

s

(cid:13)
2
(cid:13)

where the matrix A is parameterized as a function of xs as shown in Eq. (2). In this variant, the
model explicitly outputs only depth values during train and test time. The afﬁne transformation matrix
in the equation above is replaced by Eq. (3), which measures the afﬁne transformation as a pure
function of source and target keypoints plus the inferred depth. The big difference of this formulation
compared to Sections 2.1 and 2.2 is that geometric afﬁne transformation parameters are no longer
predicted by DepthNet during training and at both train and test time – it solves the least square loss
through the pseudoinverse based transformation. Since zi
t }j=1...N ) is predicted
within the analytical formulation of the solution to the least squares minimization problem, we can
backpropagate through the solution of a minimization problem that depends on the predicted depths.
While we leverage keypoints for depth estimation, the proposed approach is novel in how the depth is
estimated. Note that it is unsupervised with respect to depth labels. No depth supervision either by
using depth targets (as in [2; 13; 15]), or by using depth in an adversarial setting (as in [24]), is used
to estimate depth values for the base DepthNet models described in Sections 2.1, 2.2, and 2.3.

s = zi

p({xj

s, xj

t , yj

s, yj

The depths learned for keypoints by these approaches are not necessarily true depths, but are likely to
strongly correlate with the actual depth of each keypoint. This is because even though the method
succeeds (as we shall see below) in aligning poses, the inferred depth and the afﬁne transform may
each be scaled by factors so as to cancel each other out (i.e., by factors which are multiplicative
inverses of each other). Real world viewpoint geometry also involves perspective projection.

Adversarial Image-to-Image Transformation

DepthNet transforms the central region of the source face to the target pose. Inevitably, the face
background will be missing, which might make the proposed method unsuitable for many application

4

where the full face is required. To address this issue, we utilize CycleGAN [30], an adversarial
image-to-image translation technique. This serves to repair the background of faces that have
undergone frontalization or face swap through the DepthNet pipeline. Importantly, the adversarial
nature of CycleGAN allows one to perform image transformation between two domains without
the requirement of paired data. In our work, we perform experiments translating between various
domains of interest but one example is translating between the domain of images in the dataset (i.e.
the ground truth) and the domain of images where the DepthNet output is pasted onto the face region
(in the case of face-swap). By doing so we clean the face background in an unsupervised manner.

3 Experiments

3.1 DepthNet Evaluation on Paired Faces

For the experiments in this section, we use a subset of the VGG dataset [16], with training and
validating on all possible pairs of images belonging to the same identity for 2401 identities. This
yields 322,227 train and 43,940 validation pairs. Check experimental setup details in Supplementary.

Model

Color MSE MSE_norm

1) A simple 2D afﬁne registration
2) A 3D afﬁne registration model using an average 3D face template
3) A DepthNet that separately estimates depth and geometry
4) The model above, but with a Siamese CNN image model
5) Secondary least squares estimation for visual geometry using the depths from 3)
6) Secondary least squares estimation for visual geometry using the depths from 4)
7) Backpropagation through the pseudoinverse based solution for visual geometry
8) The model above, but with a Siamese CNN image model

grey
purple
brown
violet
red
green
orange
blue

1.562
0.724
0.568
0.539
0.400
0.399
0.357
0.349

9.547
7.486
6.292
6.115
5.184
5.175
4.932
4.891

Table 1: (left) Comparing the Mean Squared Error (MSE) and MSE normalized by inter-ocular distance
(MSE_norm) of different models. (right) Histogram of Mean Squared Errors. The second column in the Table
(on left) corresponds to the color of the model in the ﬁgure (on right).

We explore the three variants of DepthNets described in Sections 2.1, 2.2, and 2.3, each with two
architectural cases (A) and (B), depending on whether image features are used in addition to keypoints
or not. We also compare with a number of baselines. We measure the mean square error (MSE)
between the estimated keypoints on the target face (source face normalized keypoints) and ground
truth target keypoints. Results for the following models are shown in Table 1:

1) A baseline model registrations using a simple 2D afﬁne transformation.

2) We generate a 3D average face template from the 3DFAW dataset [14; 26; 6] by aligning the 3D
keypoints of all faces in the dataset to a front-facing face using Procrustes superimposition. We report
error by mapping the template face to each source face via Procrustes superimposition (to get a 3D
face f ) and then use an afﬁne transformation from the 3D face f to the target face.

3, 4) We use our proposed approach to predict both depth and geometry (described in Sections 2.1).

5, 6) These models described in Section 2.2. Note that during training, these two cases are similar to
models 3 and 4 in Table 1.

7, 8) The pseudo-inverse formulation model described in Section 2.3.

As observed in Table 1, a simple 2D afﬁne transform (model 1) without estimating depth and a
template 3D face (model 2) get high errors on mapping to the target faces. DepthNet models get lower
errors and the pseudo-inverse formulation (models 7 and 8) further reduces the error by 10%. The
CNN models slightly reduce errors compared to their equivalent models that rely only on keypoints.

3.2 DepthNet Evaluation on Unpaired Faces and Comparison to other Models

In this section we train DepthNet on unpaired faces belonging to different identities and compare
with other models that estimate depth. We use the 3DFAW dataset [14; 26; 6] that contains 66
3D keypoints to facilitate comparing with ground truth (GT) depth. It provides 13,671 train and
4,500 valid images. We extract from the valid set, 75 frontal, left and right looking faces yielding
a total of 225 test images, which provides a total of 50,400 source and target pairs. We train the
psuedoinverse DepthNet model that relies on only keypoints (model 7 in Table 1). We also train
a variant of DepthNet that applies an adversarial loss on the depth values (DepthNet+GAN). This
model uses a conditional discriminator that is conditioned on 2D keypoints and discriminates GT
from estimated depth values. The model is trained with both keypoint and adversarial losses.

5

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)

Source

Target

Left
Front
Right

DepthNet

DepthNet + GAN

Left
24.67
25.54
21.66

Front Right
29.70
27.71
26.19
27.22
23.87
21.48

Avg
27.36
26.32
22.34

Left
59.78
58.77
59.97

Front Right
59.63
59.67
58.61
58.67
59.60
59.70

Avg
59.69
58.68
59.76

Table 2: Comparing DepthCorr for different DepthNet models when mapping variant source to target poses. The
Avg column measures the average over the three preceding columns.

We measure the correlation matrix between GT and estimated depths, where the element k in the
diagonal indicates the correlation between estimated and ground truth depth values for keypoint k,
yielding a value between -1 and 1. We report the sum of absolute values of the diagonal of this
matrix, indicated by DepthCorr. We compare DepthNet models on DepthCorr in Table 2. For this
experiment we take every possible pair of source to target faces, where source and target are one of
{left, front, right} looking faces. This yields a total of 5,550 pairs when the source and the target are
from the same subset, and 5,625 pairs otherwise. This experiment measures the accuracy of depth
estimation of the DepthNet models on different orientations of source-target faces. The baseline
DepthNet model that does not leverage the depth labels performs well in different cases. DepthCorr
improves more than twice for the DepthNet+GAN model, indicating a direct supervision loss using
depth labels can enhance the depth estimation.

Model

GT Depth
AIGN [24]
MOFA [20]
DepthNet (Ours)
DepthNet + GAN (Ours)

Need Depth Manual Init. MSE (×10−5) Depth Correlation Matrix Trace (DepthCorr)
Right pose
66
49.04
17.54
22.34
59.76

Front pose
66
50.81
15.97
26.32
58.68

8.86 ± 6.55
9.06 ± 6.61
8.75 ± 6.33
7.65 ± 6.97
8.74 ± 6.24

Left pose
66
44.08
11.14
27.36
59.69

-
No
Yes
No
No

Yes
Yes
No
No
Yes

Table 3: Comparing MSE and DepthCorr for different models. A lower MSE indicates the model maps better to
the target faces. A higher DepthCorr indicates more correlation between estimated and GT depths.

We compare our two DepthNet models with three baselines: 1) AIGN [24], 2) MOFA [20] and 3)
GT Depth (no model trained). AIGN estimates 3D keypoints conditioned on 2D heatmaps of the
keypoints. MOFA estimates a 3D mesh using only an image. We implemented the AIGN model
and asked the authors of MOFA to run their model on our test-set. They provided MOFA’s results
for 134 images in the test set. In Table 3 we compare these three models with our DepthNet models
on DepthCorr. We also compare them on MSE, which is measured between GT and estimated
target keypoints. Since the three baselines estimate depth on a single image due to their different
model formulation, we ﬁrst measure m using closed form solution in Eq. 3 and then apply m to
the estimated source keypoints to get the target keypoint estimations. We contrast the estimated
values with the GT target keypoints. As shown in Table 3, GT depth has the highest DepthCorr
(the maximum possible value). The depths estimated by DepthNet+GAN and AIGN have stronger
correlation to GT depth compared to the baseline DepthNet and MOFA, while baseline DepthNet
performs better than MOFA. On MSE the baseline DepthNet model gets smaller MSE when mapping
to target faces, indicating it is better suited for this task.

In Figure 2 we plot heatmaps of the estimated depth of different models (on Y axis) and the GT depth
(on X axis) aggregated over all 66 keypoints on all test data. As can be seen, the depth estimated
by DepthNet+GAN and AIGN models form a 45 degree rotated ellipses showing a stronger linear
correspondence with respect to the GT depth compared to the the baseline DepthNet and MOFA.

Figure 2: Predicted (Y axis) versus Ground Truth (X axis) depth heatmaps for different models.

In Figure 3 we show some estimated depth samples for different models (see more samples in Figure
S1). AIGN and DepthNet+GAN generate more realistic results. MOFA generates very similar
face templates for different poses. Baseline DepthNet estimates reliable depth values in most cases,
however it has some failure modes as shown in the last row.

6

By comparing different models in Table 3, MOFA requires proper initialization to map face meshes
to each image. AIGN requires depth labels to train the model. Our baseline DepthNet model neither
require any depth labels nor any manual tuning. The results also show DepthNet can work well on
unpaired data. We would also like to emphasize that MOFA and AIGN are designed to estimate a 3D
model, while DepthNet is designed to estimate the parameters that facilitate warping a face pose to
another without having depth values, so these models are designed to solve different problems.

Figure 3: Depth visualization for different models (color coded by depth). From left to right: RGB image,
Ground Truth, DepthNet, DepthNet+GAN, AIGN and MOFA estimated depth values.

An interesting observation is that GT depth gets a higher MSE compared to DepthNet. This can be
due to not having a perspective projection between source and target faces. However, since DepthNet
is trained to map to the target faces, it learns the afﬁne parameters in a way to minimize this loss.

3.3 Face Rotation, Replacement and Adversarial Repair

In this section we show how DepthNet can be used for different applications. In Figure 1 (right) we
visualize the face rotation by re-projecting a frontal face, from Multi-PIE [6], (far left) to a range of
other poses deﬁned by the faces in the row above. Since DepthNet (case B) computes transformation
on keypoints rather than pixels it is robust to illuminations changes between source and target faces.
See Figures S2 to S5 for further examples. Note that DepthNet preserves well the identity. However,
it carries forward the emotion from source to target since using a global afﬁne transformation imparts
a degree of robustness to dramatic expression changes. The views in these ﬁgures are rendered from
a 3D model in OpenGL. Note the model can align well to the target face poses.

Figure 4: Background synthesis with CycleGAN. Left to right: source face; keypoints overlaid; DepthNet (DN);
DN + background → frontal;

In another experiment, we do face frontalization with synthesized background. Here we use Cy-
cleGAN to add background detail to a face that has been frontalized with DepthNet. Referring to
Figure 4, we perform this by conditioning the CycleGAN on the DepthNet image (column 3) and the
background of column 2 (masking interior face region determined by the convex hull spanned by
the keypoints). The second domain contains ground truth frontal faces. This experiment shows how
to leverage DepthNet for full face generation. Note that we do not use identity information in this
experiment. However, it can be used to better preserve the identity.

Figure 5: Left to right: source face; target face; warp to target; repaired result.

7

Finally, we do face swaps, where we warp the face of one identity onto the geometry and background
of another identity using DepthNet. To do so, we paste the rotated face by DepthNet onto the
background of the target image and train a CycleGAN to map from the domain of ‘swapped in faces’
to the ground truth faces in our dataset, effectively learning to clean up face swaps so that the face
region matches the hair and background. Some examples of this procedure are shown in Figure 5.

4 Related Work

4.1

3D Transformation on Faces

While there is a large body of literature on 3D facial analysis, many standard techniques are not
applicable to our setting here. As an example, morphable models [1] cover a wide variety of
approaches which are capable of high quality 3D reconstructions, but such methods usually require
3D face scans or reconstructions from multi-view stereo to be assembled so as to learn complex
parametric distributions over face shapes. A close approach to our own is that of [7] on viewing
real world faces in 3D. Similar to our work, this approach does not require aligned 3D face scans,
highly engineered models or manual interventions. They make the observation that if 2D keypoints
can be obtained from a single input image of a face and these keypoints are matched to an arbitrary
3D target geometry, then standard camera calibration techniques can be used to estimate plausible
intrinsics and extrinsics of the camera. This allows the estimated camera matrix, 3D rotation matrix
and 3D translation vector to be used to transform the target 3D model to the pose of the query image
from which an approximate depth can be obtained. Hassner et. al [8] explore the use of a single
unmodiﬁed 3D surface as an approximation to the shape of all input faces. In contrast, our approach
only requires 2D keypoints from the source and target faces as input. It then estimates the depth of
the source face keypoints, thereby inferring an image speciﬁc 3D model of the face.

DeepFace [19] uses face frontalization to improve the performance of a face veriﬁcation system. It
uses a 3D mask composed of facial keypoints, detects the corresponding locations of these keypoints
in the image, and maps the 2D keypoints onto a 3D face model to frontalize it. DeepFace, however,
maps to a template 3D face, therefore always mapping to a speciﬁc pose and geometry. DepthNet, on
the other hand, can map to any pose and geometry, giving it more expressive ﬂexibility.

4.2 Generative Adversarial Networks on Face Rotation

Recently, adversarial models in [12; 22; 25; 27; 18; 28] have explored face rotation. TP-GAN [12]
performs face frontalization through introducing several losses to preserve identity and symmetry of
the frontalized faces. PIM [28] frontalizes faces in a composed adversarial loss and then extracts pose
invariant features for face recognition. These models are mainly aimed for face veriﬁcation, where
they can only do face-frontalization. Another limitation of these models is in requiring ground truth
frontal images of the same identity during training. DR-GAN [22] rotates faces to any target pose by
using a discriminator that also does identity classiﬁcation in addition to pose prediction, to preserve
id and pose. While these models do pure face rotation of a 2D face, our model can warp the input
face to any other target face, allowing warping the input face to any other identity, with a different
geometry and pose. Moreover, our model also estimates the 3D geometric afﬁne transformation
parameters explicitly, allowing these parameters to be used later, e.g., for face texture swap.

FF-GAN [25], DA-GAN [27], and FaceID-GAN [18] estimate parameters of either a 3D Morphable
Model (3DMM), as in [25; 18], or source to target pose transformation, as in [27]. FF-GAN uses
3DMM parameters to frontalize faces in an adversarial approach, while FaceID-GAN uses the
3DMM parameters to generate any target pose. These models, however, train 3DMM on ground
truth labels such as identity, expression and pose. DepthNet, on the other hand, estimates depth and
afﬁne transformation parameters without requiring ground truth afﬁne or depth labels or pre-training.
Similar to DepthNet, DA-GAN [27] estimates parameters of an afﬁne transformation model that
maps a 2D face to a 3D face. Unlike DepthNet that estimates depth on the source face, DA-GAN
uses depth in a template target face. While their approach eliminates the need for depth estimation, it
only allows the source face to be mapped to the target template geometry, while DepthNet can map
the source face to any target geometry, provided by a target image, or its keypoints. We demonstrate
the application of this ﬂexibility for the face replacement task.

The aforementioned adversarial models use an identity preserving loss to maintain identity. The core
DepthNet model does not need identity labels and preserves well the identity (as shown in Figure 1

8

(right)). However, the identity information can be used by the proposed adversarial components, as in
background synthesis, to further improve the results. Unlike some of these models that take target
pose as input, DepthNet uses the target keypoints to estimate the target geometry and does not require
the target pose. This has several advantages; 1) DepthNet can map to the geometry of the target face
in addition to the pose, and 2) in the face replacement task, DepthNet can replace the target face with
the warped source face directly onto the target face location. Its application is shown in the face swap
experiment in Section 3.3.

4.3 Depth Estimation

Thewlis et. al [21] propose a mapping technique to learn a proxy of 2D landmarks in an unsupervised
way. A semi-supervised technique has been also proposed in [11] that improves landmark localization
by using weaker class labels (e.g. emotion or pose) and also by making the model predict equivariant
variations of landmarks when such transformations are applied to the image. Similar to these
approaches, DepthNet also maps a source to a target to learn its parameters. However, unlike these
two approaches that estimate 2D landmarks, DepthNet estimates the depth of the landmarks using
2D matching of keypoints, by formulating afﬁne parameters as a function of depth augmentated
keypoints in a closed form solution.

While several models [13; 2; 15] estimate depth with direct supervision, there has been recent models
[29; 5; 3] that estimate depth in an unsupervised training procedure. These models rely on pixel
reconstruction by using frames that are captured from very similar scenes, e.g. nearby frames of
a video [29] or left-right frames captures by stereo cameras [5; 3]. These models estimate depth
on one frame and then by using the disparity map, measure how pixel values of nearby frames
compare to each other. To do this, they also require camera intrinsic parameters, e.g. focal length or
distance between cameras. Unlike these models, our approach does not require source to target pixel
mapping. This allows mapping faces from different people with completely different skin colors,
without knowing camera parameters or how they are positioned with respect to each other. Therefore,
DepthNet is not susceptible to variations in illuminations or lighting between source and target faces.

Tung et. al [23] estimate 3D human pose in videos, where they use synthetic data to pre-train internal
parameters of the model and ﬁne-tune them by keypoint, segmentation and motion loss. Adversarial
Inverse Graphics Networks (AIGN) [24] estimates 3D human pose from 2D keypoint heatmaps in a
semi-supervised manner with a similar formulation to that of CycleGAN. They apply an adversarial
loss on the 3D pose to make them look realistic. These models leverage the depth values either
through synthetic data [23], or by adversarial usage of ground truth depth values [24]. Unlike these
models, DepthNet does not rely on any depth signal, either directly or indirectly. MOFA [20] builds
a 3D face mesh using a single image, where the 3D face parameters such as 3D shape and skin
reﬂectance are estimated by an encoder and then using a differentiable model they are rendered back
to the image by the decoder. This model requires manual initialization to map the input image to the
3D mesh, since otherwise it is doing an unconstrained optimization by adapting both the face pose
and the skin reﬂectance. Our model, however, does not require any manual initialization.

5 Conclusion

We have proposed a novel approach to 3D face model creation which enables pose normalization
without using any ground truth depth data. We achieve our best quantitative keypoint registration
results using our novel formulation for predicting depth and 3D visual geometry simultaneously,
learned by backpropagating through the analytic solution for the visual geometry estimation problem
expressed as a function of predicted depths. We have illustrated the quality and utility of the depths
and 3D transformations obtained using our method by transforming source faces to a wide variety of
target poses and geometries. Our technique can be used for face rotation and replacement and when
combined with adversarial repair it can blend warped faces to also synthesize the background. The
proposed model, however, carries forward emotion from source to target due to learning a shared
afﬁne parameters for all keypoints. Moreover, for extreme non-frontal faces, while DepthNet can
extract the transformation params (since it only relies on keypoints), OpenGL cannot extract texture
due to occlusion. We show an example of how to address this in the supplementary material. An
interesting extension to this paper can be replacing the OpenGL pipeline with a generative adversarial
framework that synthesizes a face using the parameters estimated by DepthNet.

9

6 Acknowledgments

We would like to thank Samsung and Google for partially funding this project. We are also thankful
to Compute Canada and Calcul Quebec for providing computational resources, and to Poonam Goyal
for helpful discussions.

References

[1] Blanz, Volker and Vetter, Thomas. A morphable model for the synthesis of 3d faces.

In
Proceedings of the 26th annual conference on Computer graphics and interactive techniques,
pp. 187–194. ACM Press/Addison-Wesley Publishing Co., 1999.

[2] Eigen, David, Puhrsch, Christian, and Fergus, Rob. Depth map prediction from a single image
using a multi-scale deep network. In Advances in Neural Information Processing Systems
(NIPS), pp. 2366–2374, 2014.

[3] Garg, Ravi, BG, Vijay Kumar, Carneiro, Gustavo, and Reid, Ian. Unsupervised cnn for single
view depth estimation: Geometry to the rescue. In Proceedings of the European Conference on
Computer Vision (ECCV), pp. 740–756. Springer, 2016.

[4] Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty of training deep feedforward
neural networks. In International conference on Artiﬁcial Intelligence and Statistics (AISTATS),
pp. 249–256, 2010.

[5] Godard, Clément, Mac Aodha, Oisin, and Brostow, Gabriel J. Unsupervised monocular depth
estimation with left-right consistency. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2017.

[6] Gross, Ralph, Matthews, Iain, Cohn, Jeffrey, Kanade, Takeo, and Baker, Simon. Multi-pie.

Image and Vision Computing, 28(5):807–813, 2010.

[7] Hassner, Tal. Viewing real-world faces in 3d.

In Proceedings of the IEEE International

Conference on Computer Vision (ICCV), pp. 3607–3614, 2013.

[8] Hassner, Tal, Harel, Shai, Paz, Eran, and Enbar, Roee. Effective face frontalization in uncon-
strained images. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 4295–4304, 2015.

[9] He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE
International Conference on Computer Vision (ICCV), pp. 1026–1034, 2015.

[10] Honari, Sina, Yosinski, Jason, Vincent, Pascal, and Pal, Christopher. Recombinator networks:
Learning coarse-to-ﬁne feature aggregation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pp. 5743–5752, 2016.

[11] Honari, Sina, Molchanov, Pavlo, Tyree, Stephen, Vincent, Pascal, Pal, Christopher, and Kautz,
Jan. Improving landmark localization with semi-supervised learning. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.

[12] Huang, Rui, Zhang, Shu, Li, Tianyu, and He, Ran. Beyond face rotation: Global and local
perception gan for photorealistic and identity preserving frontal view synthesis. In Proceedings
of the IEEE International Conference on Computer Vision (ICCV), Oct 2017.

[13] Jackson, Aaron S, Bulat, Adrian, Argyriou, Vasileios, and Tzimiropoulos, Georgios. Large pose
3d face reconstruction from a single image via direct volumetric cnn regression. In Proceedings
of the IEEE International Conference on Computer Vision (ICCV), pp. 1031–1039. IEEE, 2017.

[14] Jeni, László A, Cohn, Jeffrey F, and Kanade, Takeo. Dense 3d face alignment from 2d videos in
real-time. In IEEE International Conference and Workshops on Automatic Face and Gesture
Recognition (FG), volume 1, pp. 1–8. IEEE, 2015.

10

[15] Liu, Fayao, Shen, Chunhua, and Lin, Guosheng. Deep convolutional neural ﬁelds for depth
estimation from a single image. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 5162–5170, 2015.

[16] Parkhi, Omkar M, Vedaldi, Andrea, and Zisserman, Andrew. Deep face recognition. Proceedings

of the British Machine Vision Conference (BMVC), 2015.

[17] Sagonas, Christos, Tzimiropoulos, Georgios, Zafeiriou, Stefanos, and Pantic, Maja. 300 faces
in-the-wild challenge: The ﬁrst facial landmark localization challenge. In Proceedings of the
IEEE International Conference on Computer Vision Workshops (CVPRW), pp. 397–403, 2013.

[18] Shen, Yujun, Luo, Ping, Yan, Junjie, Wang, Xiaogang, and Tang, Xiaoou. Faceid-gan: Learning
a symmetry three-player gan for identity-preserving face synthesis. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 821–830, 2018.

[19] Taigman, Yaniv, Yang, Ming, Ranzato, Marc’Aurelio, and Wolf, Lior. Deepface: Closing the
gap to human-level performance in face veriﬁcation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 1701–1708, 2014.

[20] Tewari, Ayush, Zollhöfer, Michael, Kim, Hyeongwoo, Garrido, Pablo, Bernard, Florian, Perez,
Patrick, and Theobalt, Christian. Mofa: Model-based deep convolutional face autoencoder for
unsupervised monocular reconstruction. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), volume 2, 2017.

[21] Thewlis, James, Bilen, Hakan, and Vedaldi, Andrea. Unsupervised learning of object landmarks
by factorized spatial embeddings. In Proceedings of the IEEE International Conference on
Computer Vision (ICCV), volume 1, pp. 5, 2017.

[22] Tran, Luan, Yin, Xi, and Liu, Xiaoming. Disentangled representation learning gan for pose-
invariant face recognition. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017.

[23] Tung, Hsiao-Yu, Tung, Hsiao-Wei, Yumer, Ersin, and Fragkiadaki, Katerina. Self-supervised
learning of motion capture. In Advances in Neural Information Processing Systems (NIPS), pp.
5242–5252, 2017.

[24] Tung, Hsiao-Yu Fish, Harley, Adam W, Seto, William, and Fragkiadaki, Katerina. Adversarial
inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation from
unpaired supervision. In Proceedings of the IEEE International Conference on Computer Vision
(ICCV), volume 2, 2017.

[25] Yin, Xi, Yu, Xiang, Sohn, Kihyuk, Liu, Xiaoming, and Chandraker, Manmohan. Towards
large-pose face frontalization in the wild. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), pp. 1–10, 2017.

[26] Zhang, Xing, Yin, Lijun, Cohn, Jeffrey F, Canavan, Shaun, Reale, Michael, Horowitz, Andy,
Liu, Peng, and Girard, Jeffrey M. Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic
facial expression database. Image and Vision Computing, 32(10):692–706, 2014.

[27] Zhao, Jian, Xiong, Lin, Jayashree, Panasonic Karlekar, Li, Jianshu, Zhao, Fang, Wang, Zhecan,
Pranata, Panasonic Sugiri, Shen, Panasonic Shengmei, Yan, Shuicheng, and Feng, Jiashi. Dual-
agent gans for photorealistic and identity preserving proﬁle face synthesis. In Advances in
Neural Information Processing Systems (NIPS), pp. 66–76, 2017.

[28] Zhao, Jian, Cheng, Yu, Xu, Yan, Xiong, Lin, Li, Jianshu, Zhao, Fang, Jayashree, Karlekar,
Pranata, Sugiri, Shen, Shengmei, Xing, Junliang, et al. Towards pose invariant face recognition
in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 2207–2216, 2018.

[29] Zhou, Tinghui, Brown, Matthew, Snavely, Noah, and Lowe, David G. Unsupervised learning of
depth and ego-motion from video. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2017.

[30] Zhu, Jun-Yan, Park, Taesung, Isola, Phillip, and Efros, Alexei A. Unpaired image-to-image
translation using cycle-consistent adversarial networks. In Proceedings of the IEEE International
Conference on Computer Vision (ICCV), 2017.

11

Unsupervised Depth Estimation, 3D Face Rotation and
Replacement: Supplementary Information

S.1 Experimental Setup

The RCN, the DepthNets and the CycleGAN modules are trained separately. Each model is trained
using standard techniques for the model class and has a separate objective to be optimized. DepthNet
does not use the OpenGL pipeline during training and only uses it to render faces at test time, allowing
DepthNet to train faster.

S.1.1 DepthNet Experimental Details

Here we describe the details of the experiments carried out in Section 3.1. Our DepthNet architectures
require keypoints of both source and target images to be extracted. For this, the image is ﬁrst passed
through the VGG-Face [16] face detector. The face crops are then scaled down to 80 × 80 and
converted to greyscale, following which they are passed through RCN to obtain N = 68 keypoints
on each image. The RCN is trained exactly as described in [10], using the 300W dataset [17].

The keypoint only variant of our model involves concatenating all detected keypoints and passing
them through a two-layer deep fully connected network, with 256 hidden units and o output units.
The size of o depends on whether we are predicting only the depth, in which case o = N , or both
depth and afﬁne transformation parameters, in which case o = N + 8.

As discussed above, it is possible to augment these models with a Siamese CNN module (case A). In
the model variants that also use the image, we pass both the source and the target images through
three conv-maxpool layers with shared weights of size (32, 4, 2), (48, 3, 2), (64, 2, 2), respectively for
the representation (num_filters, filter_size, pool_size). The network’s outputs for the
source and target faces are then concatenated before passing them into a 4-layered fully connected
network with respective output sizes of 2048, 512, 256, and o. The keypoints are concatenated to the
512-unit layer before being passed to the last two layers. See Figure 1 (left) for an illustration of the
model. We explore these Siamese CNN augmented variants in models 4, 6 and 8 in Table 1.

We set the initial learning rate to 0.001 and use a Nesterov momentum optimizer (with a momentum
of 0.9) in all our experiments. With the exception of the last layer, we initialize all weights with
a Glorot initialization scheme [4], with the weights sampled from a uniform distribution. We use
a ReLU gain [9], set all biases to 0, and apply a ReLU non-linearity after every layer. In the ﬁnal
output layer, we do not apply any non-linearity and initialize the weights to 0. The biases of units that
represent depths are initialized to a random Normal distribution with µ = 0 and σ = 0.5, while those
that form the predicted afﬁne transform are initialized with the equivalent of a "ﬂattened" identity
transform. All models have been trained for 500 epochs.

We point out that except for a comparison between learning rates in the set {0.01, 0.001, 0.0001}
over few (less than 10) epochs, to ﬁnd a learning rate that the model seems to train well with, we
have not performed a hyperparameter search, and anticipate that the performance of the model can be
made even better by searching the hyperparameter space on a per model basis and by using deeper
(or modiﬁed) architectures.

S.2 Depth Visualization

We show further estimated depth values by different models in Figure S1. DepthNet+GAN and AIGN
generate the closest depth values to the GT depth. The baseline DepthNet model estimates reliable
depth values for most cases, however it has some degree of inaccuracy, as shown in the last two rows.
In DepthNet, the estimated depth indicates the position of each keypoint relative to other keypoints
rather than with respect to a source and importantly it is done without any supervision. MOFA, which
is also unsupervised, generates very similar face templates for different cases.

12

Figure S1: Depth visualization for different models (color coded by depth). The Depth axis is the one pointing
into the page. From left to right: RGB image, Ground Truth, DepthNet, DepthNet+GAN, AIGN and MOFA
estimated depth values.

S.3 Additional Camera Sweep Visualizations

In this section, we present additional visualizations along the lines of those shown in Figure 1 (right)
in Section 2 of the main paper. Frontal faces selected from the Multi-PIE dataset [6] are re-projected
to match several other poses corresponding to a person with a different identity. The DepthNets
predicts reliable proxy depths, which when coupled with the analytically obtained afﬁne transform
(obtained from the least-squares pseudo-inverse-based solution described in Section 2.3 of the main
paper) yields faces close to the desired target face geometry when passed through the OpenGL
pipeline.

We show camera sweeps for frontal source faces in Figures S2 and S3 and non-frontal source faces in
Figure S4. In Figure S5 we use the same target identity as the source face, showing how much the

13

Figure S2: Projecting a frontal face (far left) to a range of other poses deﬁned by faces in the row above.

generated face differs from the ground truth target. For all samples in Figures S2, S3, S4, and S5 we
use the DepthNet model that relies on only key-points (model 7 in Table 1).

Note that for non-frontal source faces, the quality of images is reduced specially for frontal target
faces. This is due to lack of adequate texture on the occluded side of the face to be transferred to the
target pose by OpenGL pipeline, rather than inaccuracies in the afﬁne transformation parameters. In
order to reduce the side-affects, we use either of the source face or its ﬂipped version, that is closer to
the target face pose, and then warp the face to the target keypoints.

Figure S3: Projecting a frontal face (far left) to a range of other poses deﬁned by faces in the row above.

Figure S4: Re-projecting a non-frontal face (far left) to a range of other poses deﬁned by faces in the row above.

Figure S5: Rotating a face (far left) to a range of other poses deﬁned by faces of the same identity in the row
above. On the top row frontal and on the bottom row non-frontal source faces are shown.

S.4 CycleGAN

Suppose we have some images belonging to one of two sets x ∈ X and y ∈ Y , where x denotes a
DepthNet-resulting face and y a ground truth face which is frontal. We wish to learn two functions

14

(cid:105)

(cid:105)

,

(cid:104)

Ex,y

min
G,F

(cid:104)

Ex,y

min
DX ,DY

F : X → Y and G : Y → X which are able to map an image from one set to the corresponding
image in the other. Correspondingly, we have two discriminators DX and DY which try to detect
whether the image in that particular set is real or generated. While we are only interested in the
function F : X → Y (since this is mapping to the distribution of ground truth faces) the formulation
of CycleGAN requires that we learn mappings in both directions during training. We optimize the
following objectives for the two generators F and G:

(cid:96)(DX (G(y)), 1) + (cid:96)(DY (F (x)), 1) + λ||y − F (G(y))||1 + λ||x − G(F (x))||1

(4)

And the following for the two discriminators DX and DY :

(cid:96)(DX (x), 1) + (cid:96)(DX (G(y)), 0) + (cid:96)(DY (y), 1) + (cid:96)(DY (F (x)), 0)

(5)

where 0/1 denote fake/real, (cid:96)() is the squared error loss and λ is a coefﬁcient for the cycle-consistency
(reconstruction) loss.

In the case where we did adversarial background synthesis, x is a channel-wise concatenation of the
DepthNet-frontalized face and the background of the original (pre-frontalized) image. For face-swap
cleanup, x is simply a source face which has been warped to a target face and pasted on top.

Once the network has been trained, we can disregard all other functions and use F to clean up faces
which are low quality due to artifacts from warping.

In terms of architectural details the generators and discriminators used were those described in the ap-
pendix of the CycleGAN paper [30]. In short, the generator consists of three conv-BN-relu blocks
which downsample the input, followed by nine ResNet blocks (which can be interpreted as iteratively
performing transformations over the downsampled representation), followed by deconv-BN-relu
blocks to upsample the representation back into the original input size. For training, we use the
same hyperparameters as most CycleGAN implementations which is using the Adam optimizer with
learning rate α = 2 × 10−4, β1 = 0.5, β2 = 0.999. However, instead of using a batch size of 1 we
use the largest possible batch size, which was 16 for a 12GB GPU.

Note that in order to produce better translations, the dataset we used for all CycleGAN experiments
contain both the VGG and the CelebA datasets, which has signiﬁcantly more images.

S.4.1 Background Synthesis

We present extra visualizations for the CycleGAN which performs background synthesis on CelebA,
corresponding to Figure 4 in the main paper. These are shown in Figure S6.

Figure S6: Background synthesis with CycleGAN. Left to right: source face; keypoints overlaid; DepthNet
(DN); DN + background → frontal

S.4.2 Face Replacement and Adversarial Repair

In Figure S7 we provide extra face swap samples on CelebA, corresponding to Figure 5 in the
main paper, where a source face is warped to a target face pose using DepthNet, pasted onto the

15

target image and then passed to a CycleGAN to adapt the face skin of the warped source face to the
background and hairstyle of the target face.

Figure S7: Face swap experiment with CycleGAN. Left to right: source face; target face; warp to target with
DepthNet; repaired result with CycleGAN. The source face is taken and warped onto the target face. The
background and hairstyle is then adapted to the target face.

S.4.3 Extreme Pose Face Clean-up

If the source image has an extreme pose, the texture will be missing on the occluded side of the
face and the OpenGL pipeline cannot rotate the face without artifacts. Note that this shortcoming is
due to lack of texture on the occluded side of the face rather than a deﬁciency of the transformation
parameters measured by DepthNet.

We performed an experiment using CycleGAN to ﬁx such artifacts. For this experiment we take
source images from CelebA and ﬁrst frontalize it by using DepthNet. Since the frontalized faces have
artifacts due to stretch of texture on the occluded side of the face by the OpenGL pipeline, we train a
CycleGAN that takes DepthNet frontalizaed faces plus the background of the original non-frontal
image (as two images) in one domain and the ground truth frontal faces in the other domain. The
CycleGAN learns to clean-up these artifacts. Finally, we take the GAN-repaired frontalized faces
and project it to different target poses using DepthNet. In Figure S8 we visualize camera sweep for
source faces in the wild that have extreme poses. The cycleGAN reasonably cleans the face artifacts
and then DepthNet projects it to different poses. This is just one approach to address the extreme
pose occlusion artifacts. We see alternative methods for addressing this issue as promising directions
for future research.

Figure S8: Re-projecting a non-frontal face (far left) from CelebA to a range of other poses deﬁned by faces in
the row above. Top row (in each pair) depicts the target faces from Multi-PIE [6]. The bottom row shows from
left to right: source face, souce face frontalized by DepthNet, adversarial-repaired face, the repaired source face
projected to the target poses (4th to 10th columns).

16

8
1
0
2
 
c
e
D
 
4
2
 
 
]

V
C
.
s
c
[
 
 
5
v
2
0
2
9
0
.
3
0
8
1
:
v
i
X
r
a

Unsupervised Depth Estimation,
3D Face Rotation and Replacement

Joel Ruben Antony Moniz1∗, Christopher Beckham2,3∗, Simon Rajotte2,3,
Sina Honari2, Christopher Pal2,3,4
1Carnegie Mellon University, 2Mila-University of Montreal, 3Polytechnique Montreal, 4Element AI
1jrmoniz@andrew.cmu.edu, 2honaris@iro.umontreal.ca, 3firstname.lastname@polymtl.ca

Abstract

We present an unsupervised approach for learning to estimate three dimensional
(3D) facial structure from a single image while also predicting 3D viewpoint
transformations that match a desired pose and facial geometry. We achieve this
by inferring the depth of facial keypoints of an input image in an unsupervised
manner, without using any form of ground-truth depth information. We show how
it is possible to use these depths as intermediate computations within a new back-
propable loss to predict the parameters of a 3D afﬁne transformation matrix that
maps inferred 3D keypoints of an input face to the corresponding 2D keypoints on
a desired target facial geometry or pose. Our resulting approach, called DepthNets,
can therefore be used to infer plausible 3D transformations from one face pose
to another, allowing faces to be frontalized, transformed into 3D models or even
warped to another pose and facial geometry. Lastly, we identify certain shortcom-
ings with our formulation, and explore adversarial image translation techniques as
a post-processing step to re-synthesize complete head shots for faces re-targeted to
different poses or identities. 1

1

Introduction

Face rotation is an important task in computer vision.
It has been used to frontalize faces for
veriﬁcation [8; 19; 25; 28] or to generate faces of arbitrary poses [22; 18]. In this paper we present
a novel unsupervised learning technique for face rotation and warping from a 2D source image –
whose facial appearance will be used in the rotation – to a target face – to which the facial pose
and geometry inferred from the source image is mapped. A use case is when we have an image of
someone in a particular target pose and we want to put a given source face into that pose, without
knowing the exact target face pose. This can be leveraged, for example, in the advertisement industry,
when putting someone in a particular location can be costly or unfeasible, or in the movie industry
when the main actor’s limited time or high cost can enforce using another actor whose face can be
later replaced by the main actor’s. This is achieved through estimating the source face depth and
the 3D afﬁne parameters that warp the source to the target face using neural networks. These neural
networks use a novel loss formulation for the structured prediction of keypoint depths. Once the 3D
afﬁne transformation matrix is estimated, it can be used to warp the source image onto the target
face geometry using a textured triangular mesh. The use of a 3D afﬁne transform means that we
can capture both a 3D rotation of the face to a new viewpoint as well as a global non-Euclidean
warping of the geometry to match a target face. We call these neural networks Depth Estimation-Pose
Transformation Hybrid Networks, or DepthNets in short.

Our ﬁrst contribution is to propose a neural architecture that predicts both the depth of source
keypoints as well as the parameters of a 3D geometric afﬁne transformation which constitute the

∗Indicates equal contribution.
1Code will be released at: https://github.com/joelmoniz/DepthNets/

32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.

explicit outputs of the DepthNet model. The predicted depth and afﬁne transformation could be then
used to map a source face to a target face for object orientation, distortion and viewpoint changes.

Our second contribution consists of making the observation that given 3D source and 2D target
keypoints, closed form least squares solutions exist for estimating geometric afﬁne transformation
models between these sets of keypoint correspondences, and we can therefore develop a model that
captures the dependency between depth and the afﬁne transformation parameters. More speciﬁ-
cally, we express the afﬁne transformation as a function of the pseudoinverse transformation of 2D
keypoints in a source image – augmented by inferred depths – and the target keypoints. Thus, the
second and major contribution in this work is capturing the relationship between an estimated afﬁne
transformation and the inferred depth as a deterministic relationship. In this formulation, DepthNet
only predicts depth values explicitly and the afﬁne parameters are inferred through a pseudoinverse
transformation of source and target keypoints. Here, one can directly optimize through the solutions
of what might otherwise be formulated as a secondary minimization step.

Our proposed DepthNet can map the central region of the source face to the target geometry. This
leads to background mismatch when warping one face to another. Finally, our third contribution is to
use an adversarial unpaired image-to-image transformation approach to repair the appearance of 3D
models inferred from DepthNet. Together these contributions allow 3D models of faces that construct
realistic images in the target pose. Our proposed method can be used for pose normalization or face
swaps with no manually speciﬁed 3D face model. To the best of our knowledge, this is the ﬁrst such
neural network based model that estimates a 3D afﬁne transformation model for face rotation which
neither requires ground-truth 3D images nor any ground truth 3D face information such as depth.

As we have outlined above, our approach uses neural networks for inferring depth and geometric
transformation – referred to as DepthNets; and, an adversarial image-to-image transformation network
which improves the quality of the appearance of a 3D model inferred from a DepthNet.

2 Our Approach

DepthNets

We propose three DepthNet formulations, described in Sections 2.1, 2.2, and 2.3. For each of the
three models we explore two architectural scenarios: (A) a Siamese-like architecture that uses the
source and target images themselves as well as keypoints extracted from these images, and (B) a
fully-connected neural network variant which uses only facial keypoints in the source and target
images. See Figure 1 (left) for details.

Figure 1: (Left) DepthNet architecture. The blue region is only used in case (A) and the red part is used in
both cases (A) and (B), described in Section 2. The orange output (the 8 afﬁne transformation parameters) is
predicted only by model variations described in Sections 2.1 and 2.2, and not the model described in section
2.3. All three models predict the N depth values of the source keypoints. C, P, and FC correspond to valid conv,
pool and fully-connected layers. The two paths of Siamese network share parameters and the black dots indicate
concatenating keypoint values to FC units. (Right) Visualizing face rotation by re-projecting a frontal face (far
left) to a range of other poses deﬁned by the faces in the row above (in each pair of rows). In this experiment, we
only use keypoints from the top-row in the DepthNet model (Model 7 in Table 1).

It is interesting to note that if DepthNets are used to register a set of images of objects to the same
common viewpoint, the same image and geometry can be used as the target. This is the case for
the frontalization of faces, for example. While the DepthNet framework is sufﬁciently general to be
applied to any object type where 2D keypoint detections have been made, our experiments here focus
on faces. We describe the three variants of DepthNets below.

2

2.1 Predicting Depth and Viewpoint Separately

In this variant of DepthNets, the model predicts both depths and viewpoint geometry, but as separate
explicit outputs of a neural network. The input is comprised of only the geometry and pose of the
source and target faces (encoded in the form of a 2D keypoint template), in case (B), or both keypoints
and images of the source and target faces, in case (A). The key phases of this stage are described by
the sequence of steps given below:

1. Keypoint extraction: Raw (x, y) pixel coordinates corresponding to the keypoints of each image
are extracted using a Recombinator Network (RCN) [10] architecture, and then concatenated before
being passed into the keypoint processing step.
2. (Optional) Image Feature Extraction: DepthNets can be conditioned on only keypoints, case (B),
or on keypoints and the original images, case (A). We can therefore optionally subject the source and
target images to alternating conv-maxpool layers. If this component of the architecture is used, the
last spatial feature maps in the Siamese architecture are concatenated before being given to a set of
densely connected hidden layers.
3. Keypoint processing: In this step keypoints are passed through a set of hidden layers. If the Image
Feature Extraction stage is used, the keypoints are concatenated to image features, the output of
which is in turn fed to densely connected layers. The output layer of this phase will be of size N + 8,
where N is the number of keypoints. The ﬁrst N points represent the Depth proxy, and the last 8
points form a 4 × 2 matrix representing the learned parameters of the afﬁne transform. See Figure 1.
4. Geometric Afﬁne Transformation Normalizer: This phase applies the predicted afﬁne transform
on each (depth augmented) source keypoint to estimate its target location. Let (xi
s) represent the
ith source keypoint, (xi
n) the corresponding source normalized keypoint estimated by applying
the afﬁne transformation matrix, (xi
t) the ith target keypoint (as ground truth (GT)), and Is and It
represent the source and target images respectively. Depending on which underlying architectural
variant we use, two cases arise: one that utilizes only the keypoints (B), and another utilizing
both the keypoints and the images (A). Since the keypoints are generated using RCNs, they are
technically functions of the input images: [xs, ys] = R(Is), and [xt, yt] = R(It). Depending
on the (A) or (B) variant, the ith keypoint’s predicted depth proxy zi
p is inferred as a function
of the input keypoints, or both input keypoints and input images.
In both cases the keypoints
are derived from the images, so zi
p(Is, It). Similarly, the 3D-2D afﬁne transform F is a
function of the images, such that F = F(Is, It), where the 8 predicted parameters are: m =
{m1, m2, m3, tx, m4, m5, m6, ty)}. These constitute the 3D-2D afﬁne transform which is used by
all keypoints. In other words, each of the i points is transformed using xi
s, or:

n = F(Is, It) xi

p = zi

n, yi

s, yi

t, yi

(cid:20) xi
n
yi
n

(cid:21)

=

(cid:20)m1 m2 m3
m4 m5 m6






(cid:21)

tx
ty

xi
s
yi
s
zi
p(Is, It)
1






The loss function of a DepthNet is obtained by transforming the source face to match the target face
using the simple squared error of the corresponding target object’s keypoint vector xt = [xt, yt]T
, as GT values, and the source object’s normalized keypoint vector [xn, yn]T . The loss for one
example where we predict depth and afﬁne viewpoint geometry can therefore be expressed as:

L =

K
(cid:88)

i=1

(cid:13)
(cid:13)xi
(cid:13)

t − F(Is, It) [xi

s yi

s zi

p(Is, It)]T (cid:13)
2
(cid:13)
(cid:13)

(1)

5. Image Warper: This phase consists of using the depth proxy and afﬁne transform matrix generated
to actually warp the face from its source pose to be matched to the target object geometry. The ﬁnal
projection to 2D is achieved by simply dropping the transformed z coordinate (which corresponds
to an orthographic projection model). In the case of DepthNets, this orthographic projection is
effectively embedded in the Geometric Afﬁne Transformation Normalizer step, since the afﬁne
corresponding to the z coordinate is not predicted, essentially dropping it.
As we operate on keypoints, the actual warping of pixels can be performed with a high quality
OpenGL pipeline that performs the warp separately from the rest of the architecture. Source image,
keypoints augmented with depth, and the afﬁne matrix are passed to OpenGL pipeline to warp the
source image towards the target pose. This OpenGL warping is not needed during DepthNet training,
which means we do not have to do feedforward or backprop through OpenGL. In Summary, for step 1
the RCN model [10] is used, for steps 2 to 4 the DepthNet model, shown in Figure 1 (left), is trained,

3

and for step 5 an OpenGL pipeline is used. No data or parameters are needed to train the OpenGL
pipeline. It warps images by directly using the provided data.

2.2 Estimating Viewpoint Geometry as a Second Step

In this model variant, training is similar to Section 2.1 and the model outputs depth and 3D
afﬁne transformation parameters. However, at test time, rather than using the predicted 3D afﬁne
transformation for pairs of faces, we use only the predicted depths and estimate the afﬁne ge-
ometry parameters as a second estimation step. More precisely, given 3D points for a scene
and the corresponding 2D points for a target geometry it is possible to formulate the estima-
tion of a 3D afﬁne transformation as a linear least squares estimation problem. An overdeter-
mined system of the form Am = xt for this problem can be constructed as shown in (2).

This corresponds to an afﬁne camera model
followed by an orthographic projection to
2D keypoints. This setup also leads to
the following closed form solution for the
afﬁne transformation parameters:

m = [AT A]−1AT xt,

(3)

y1
s
0
y2
s
0

z1
s
0
z2
s
0

x1
s
0
x2
s
0













xK
s
0

yK
s
0

zK
s
0

0
x1
s
0
x2
s

0
xK
s

0
y1
s
0
y2
s
...
0
yK
s

0
z1
s
0
z2
s

1 0
0 1
1 0
0 1

0
zK
s

1 0
0 1





































m1
m2
m3
m4
m5
m6
tx
ty

























x1
t
y1
t
x2
t
y2
t
...
xK
t
yK
t

=

(2)

where this pseudoinverse based transformation is parameterized by the reference points and their
predicted depths.

2.3

Joint Viewpoint and Depth Prediction

Our key observation is that one can alternatively use the closed form analytical solution, measured in
Eq. (3), for the least squares estimation problem as the underlying afﬁne transformation matrix within
the loss function. This leads to a special form of structured prediction problem for geometrically
consistent depths and afﬁne transformation matrix. For each image we have L =

K
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:21)

(cid:20) xi
t
yi
t
(cid:124) (cid:123)(cid:122) (cid:125)
xi
t

(cid:20)m1 m2 m3
m4 m5 m6

−

(cid:124)

(cid:123)(cid:122)
m

tx
ty

(cid:21)

(cid:125)







(cid:124)

xi
s
yi
s
zi
p(Is, It)
1
(cid:123)(cid:122)
xi
s







(cid:125)

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

K
(cid:88)

i=1

(cid:13)
(cid:13)xi

t − reshape(cid:2)[AT A]−1AT xt

(cid:3)xi

s

(cid:13)
2
(cid:13)

where the matrix A is parameterized as a function of xs as shown in Eq. (2). In this variant, the
model explicitly outputs only depth values during train and test time. The afﬁne transformation matrix
in the equation above is replaced by Eq. (3), which measures the afﬁne transformation as a pure
function of source and target keypoints plus the inferred depth. The big difference of this formulation
compared to Sections 2.1 and 2.2 is that geometric afﬁne transformation parameters are no longer
predicted by DepthNet during training and at both train and test time – it solves the least square loss
through the pseudoinverse based transformation. Since zi
t }j=1...N ) is predicted
within the analytical formulation of the solution to the least squares minimization problem, we can
backpropagate through the solution of a minimization problem that depends on the predicted depths.
While we leverage keypoints for depth estimation, the proposed approach is novel in how the depth is
estimated. Note that it is unsupervised with respect to depth labels. No depth supervision either by
using depth targets (as in [2; 13; 15]), or by using depth in an adversarial setting (as in [24]), is used
to estimate depth values for the base DepthNet models described in Sections 2.1, 2.2, and 2.3.

s = zi

p({xj

s, xj

t , yj

s, yj

The depths learned for keypoints by these approaches are not necessarily true depths, but are likely to
strongly correlate with the actual depth of each keypoint. This is because even though the method
succeeds (as we shall see below) in aligning poses, the inferred depth and the afﬁne transform may
each be scaled by factors so as to cancel each other out (i.e., by factors which are multiplicative
inverses of each other). Real world viewpoint geometry also involves perspective projection.

Adversarial Image-to-Image Transformation

DepthNet transforms the central region of the source face to the target pose. Inevitably, the face
background will be missing, which might make the proposed method unsuitable for many application

4

where the full face is required. To address this issue, we utilize CycleGAN [30], an adversarial
image-to-image translation technique. This serves to repair the background of faces that have
undergone frontalization or face swap through the DepthNet pipeline. Importantly, the adversarial
nature of CycleGAN allows one to perform image transformation between two domains without
the requirement of paired data. In our work, we perform experiments translating between various
domains of interest but one example is translating between the domain of images in the dataset (i.e.
the ground truth) and the domain of images where the DepthNet output is pasted onto the face region
(in the case of face-swap). By doing so we clean the face background in an unsupervised manner.

3 Experiments

3.1 DepthNet Evaluation on Paired Faces

For the experiments in this section, we use a subset of the VGG dataset [16], with training and
validating on all possible pairs of images belonging to the same identity for 2401 identities. This
yields 322,227 train and 43,940 validation pairs. Check experimental setup details in Supplementary.

Model

Color MSE MSE_norm

1) A simple 2D afﬁne registration
2) A 3D afﬁne registration model using an average 3D face template
3) A DepthNet that separately estimates depth and geometry
4) The model above, but with a Siamese CNN image model
5) Secondary least squares estimation for visual geometry using the depths from 3)
6) Secondary least squares estimation for visual geometry using the depths from 4)
7) Backpropagation through the pseudoinverse based solution for visual geometry
8) The model above, but with a Siamese CNN image model

grey
purple
brown
violet
red
green
orange
blue

1.562
0.724
0.568
0.539
0.400
0.399
0.357
0.349

9.547
7.486
6.292
6.115
5.184
5.175
4.932
4.891

Table 1: (left) Comparing the Mean Squared Error (MSE) and MSE normalized by inter-ocular distance
(MSE_norm) of different models. (right) Histogram of Mean Squared Errors. The second column in the Table
(on left) corresponds to the color of the model in the ﬁgure (on right).

We explore the three variants of DepthNets described in Sections 2.1, 2.2, and 2.3, each with two
architectural cases (A) and (B), depending on whether image features are used in addition to keypoints
or not. We also compare with a number of baselines. We measure the mean square error (MSE)
between the estimated keypoints on the target face (source face normalized keypoints) and ground
truth target keypoints. Results for the following models are shown in Table 1:

1) A baseline model registrations using a simple 2D afﬁne transformation.

2) We generate a 3D average face template from the 3DFAW dataset [14; 26; 6] by aligning the 3D
keypoints of all faces in the dataset to a front-facing face using Procrustes superimposition. We report
error by mapping the template face to each source face via Procrustes superimposition (to get a 3D
face f ) and then use an afﬁne transformation from the 3D face f to the target face.

3, 4) We use our proposed approach to predict both depth and geometry (described in Sections 2.1).

5, 6) These models described in Section 2.2. Note that during training, these two cases are similar to
models 3 and 4 in Table 1.

7, 8) The pseudo-inverse formulation model described in Section 2.3.

As observed in Table 1, a simple 2D afﬁne transform (model 1) without estimating depth and a
template 3D face (model 2) get high errors on mapping to the target faces. DepthNet models get lower
errors and the pseudo-inverse formulation (models 7 and 8) further reduces the error by 10%. The
CNN models slightly reduce errors compared to their equivalent models that rely only on keypoints.

3.2 DepthNet Evaluation on Unpaired Faces and Comparison to other Models

In this section we train DepthNet on unpaired faces belonging to different identities and compare
with other models that estimate depth. We use the 3DFAW dataset [14; 26; 6] that contains 66
3D keypoints to facilitate comparing with ground truth (GT) depth. It provides 13,671 train and
4,500 valid images. We extract from the valid set, 75 frontal, left and right looking faces yielding
a total of 225 test images, which provides a total of 50,400 source and target pairs. We train the
psuedoinverse DepthNet model that relies on only keypoints (model 7 in Table 1). We also train
a variant of DepthNet that applies an adversarial loss on the depth values (DepthNet+GAN). This
model uses a conditional discriminator that is conditioned on 2D keypoints and discriminates GT
from estimated depth values. The model is trained with both keypoint and adversarial losses.

5

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)

Source

Target

Left
Front
Right

DepthNet

DepthNet + GAN

Left
24.67
25.54
21.66

Front Right
29.70
27.71
26.19
27.22
23.87
21.48

Avg
27.36
26.32
22.34

Left
59.78
58.77
59.97

Front Right
59.63
59.67
58.61
58.67
59.60
59.70

Avg
59.69
58.68
59.76

Table 2: Comparing DepthCorr for different DepthNet models when mapping variant source to target poses. The
Avg column measures the average over the three preceding columns.

We measure the correlation matrix between GT and estimated depths, where the element k in the
diagonal indicates the correlation between estimated and ground truth depth values for keypoint k,
yielding a value between -1 and 1. We report the sum of absolute values of the diagonal of this
matrix, indicated by DepthCorr. We compare DepthNet models on DepthCorr in Table 2. For this
experiment we take every possible pair of source to target faces, where source and target are one of
{left, front, right} looking faces. This yields a total of 5,550 pairs when the source and the target are
from the same subset, and 5,625 pairs otherwise. This experiment measures the accuracy of depth
estimation of the DepthNet models on different orientations of source-target faces. The baseline
DepthNet model that does not leverage the depth labels performs well in different cases. DepthCorr
improves more than twice for the DepthNet+GAN model, indicating a direct supervision loss using
depth labels can enhance the depth estimation.

Model

GT Depth
AIGN [24]
MOFA [20]
DepthNet (Ours)
DepthNet + GAN (Ours)

Need Depth Manual Init. MSE (×10−5) Depth Correlation Matrix Trace (DepthCorr)
Right pose
66
49.04
17.54
22.34
59.76

Front pose
66
50.81
15.97
26.32
58.68

8.86 ± 6.55
9.06 ± 6.61
8.75 ± 6.33
7.65 ± 6.97
8.74 ± 6.24

Left pose
66
44.08
11.14
27.36
59.69

-
No
Yes
No
No

Yes
Yes
No
No
Yes

Table 3: Comparing MSE and DepthCorr for different models. A lower MSE indicates the model maps better to
the target faces. A higher DepthCorr indicates more correlation between estimated and GT depths.

We compare our two DepthNet models with three baselines: 1) AIGN [24], 2) MOFA [20] and 3)
GT Depth (no model trained). AIGN estimates 3D keypoints conditioned on 2D heatmaps of the
keypoints. MOFA estimates a 3D mesh using only an image. We implemented the AIGN model
and asked the authors of MOFA to run their model on our test-set. They provided MOFA’s results
for 134 images in the test set. In Table 3 we compare these three models with our DepthNet models
on DepthCorr. We also compare them on MSE, which is measured between GT and estimated
target keypoints. Since the three baselines estimate depth on a single image due to their different
model formulation, we ﬁrst measure m using closed form solution in Eq. 3 and then apply m to
the estimated source keypoints to get the target keypoint estimations. We contrast the estimated
values with the GT target keypoints. As shown in Table 3, GT depth has the highest DepthCorr
(the maximum possible value). The depths estimated by DepthNet+GAN and AIGN have stronger
correlation to GT depth compared to the baseline DepthNet and MOFA, while baseline DepthNet
performs better than MOFA. On MSE the baseline DepthNet model gets smaller MSE when mapping
to target faces, indicating it is better suited for this task.

In Figure 2 we plot heatmaps of the estimated depth of different models (on Y axis) and the GT depth
(on X axis) aggregated over all 66 keypoints on all test data. As can be seen, the depth estimated
by DepthNet+GAN and AIGN models form a 45 degree rotated ellipses showing a stronger linear
correspondence with respect to the GT depth compared to the the baseline DepthNet and MOFA.

Figure 2: Predicted (Y axis) versus Ground Truth (X axis) depth heatmaps for different models.

In Figure 3 we show some estimated depth samples for different models (see more samples in Figure
S1). AIGN and DepthNet+GAN generate more realistic results. MOFA generates very similar
face templates for different poses. Baseline DepthNet estimates reliable depth values in most cases,
however it has some failure modes as shown in the last row.

6

By comparing different models in Table 3, MOFA requires proper initialization to map face meshes
to each image. AIGN requires depth labels to train the model. Our baseline DepthNet model neither
require any depth labels nor any manual tuning. The results also show DepthNet can work well on
unpaired data. We would also like to emphasize that MOFA and AIGN are designed to estimate a 3D
model, while DepthNet is designed to estimate the parameters that facilitate warping a face pose to
another without having depth values, so these models are designed to solve different problems.

Figure 3: Depth visualization for different models (color coded by depth). From left to right: RGB image,
Ground Truth, DepthNet, DepthNet+GAN, AIGN and MOFA estimated depth values.

An interesting observation is that GT depth gets a higher MSE compared to DepthNet. This can be
due to not having a perspective projection between source and target faces. However, since DepthNet
is trained to map to the target faces, it learns the afﬁne parameters in a way to minimize this loss.

3.3 Face Rotation, Replacement and Adversarial Repair

In this section we show how DepthNet can be used for different applications. In Figure 1 (right) we
visualize the face rotation by re-projecting a frontal face, from Multi-PIE [6], (far left) to a range of
other poses deﬁned by the faces in the row above. Since DepthNet (case B) computes transformation
on keypoints rather than pixels it is robust to illuminations changes between source and target faces.
See Figures S2 to S5 for further examples. Note that DepthNet preserves well the identity. However,
it carries forward the emotion from source to target since using a global afﬁne transformation imparts
a degree of robustness to dramatic expression changes. The views in these ﬁgures are rendered from
a 3D model in OpenGL. Note the model can align well to the target face poses.

Figure 4: Background synthesis with CycleGAN. Left to right: source face; keypoints overlaid; DepthNet (DN);
DN + background → frontal;

In another experiment, we do face frontalization with synthesized background. Here we use Cy-
cleGAN to add background detail to a face that has been frontalized with DepthNet. Referring to
Figure 4, we perform this by conditioning the CycleGAN on the DepthNet image (column 3) and the
background of column 2 (masking interior face region determined by the convex hull spanned by
the keypoints). The second domain contains ground truth frontal faces. This experiment shows how
to leverage DepthNet for full face generation. Note that we do not use identity information in this
experiment. However, it can be used to better preserve the identity.

Figure 5: Left to right: source face; target face; warp to target; repaired result.

7

Finally, we do face swaps, where we warp the face of one identity onto the geometry and background
of another identity using DepthNet. To do so, we paste the rotated face by DepthNet onto the
background of the target image and train a CycleGAN to map from the domain of ‘swapped in faces’
to the ground truth faces in our dataset, effectively learning to clean up face swaps so that the face
region matches the hair and background. Some examples of this procedure are shown in Figure 5.

4 Related Work

4.1

3D Transformation on Faces

While there is a large body of literature on 3D facial analysis, many standard techniques are not
applicable to our setting here. As an example, morphable models [1] cover a wide variety of
approaches which are capable of high quality 3D reconstructions, but such methods usually require
3D face scans or reconstructions from multi-view stereo to be assembled so as to learn complex
parametric distributions over face shapes. A close approach to our own is that of [7] on viewing
real world faces in 3D. Similar to our work, this approach does not require aligned 3D face scans,
highly engineered models or manual interventions. They make the observation that if 2D keypoints
can be obtained from a single input image of a face and these keypoints are matched to an arbitrary
3D target geometry, then standard camera calibration techniques can be used to estimate plausible
intrinsics and extrinsics of the camera. This allows the estimated camera matrix, 3D rotation matrix
and 3D translation vector to be used to transform the target 3D model to the pose of the query image
from which an approximate depth can be obtained. Hassner et. al [8] explore the use of a single
unmodiﬁed 3D surface as an approximation to the shape of all input faces. In contrast, our approach
only requires 2D keypoints from the source and target faces as input. It then estimates the depth of
the source face keypoints, thereby inferring an image speciﬁc 3D model of the face.

DeepFace [19] uses face frontalization to improve the performance of a face veriﬁcation system. It
uses a 3D mask composed of facial keypoints, detects the corresponding locations of these keypoints
in the image, and maps the 2D keypoints onto a 3D face model to frontalize it. DeepFace, however,
maps to a template 3D face, therefore always mapping to a speciﬁc pose and geometry. DepthNet, on
the other hand, can map to any pose and geometry, giving it more expressive ﬂexibility.

4.2 Generative Adversarial Networks on Face Rotation

Recently, adversarial models in [12; 22; 25; 27; 18; 28] have explored face rotation. TP-GAN [12]
performs face frontalization through introducing several losses to preserve identity and symmetry of
the frontalized faces. PIM [28] frontalizes faces in a composed adversarial loss and then extracts pose
invariant features for face recognition. These models are mainly aimed for face veriﬁcation, where
they can only do face-frontalization. Another limitation of these models is in requiring ground truth
frontal images of the same identity during training. DR-GAN [22] rotates faces to any target pose by
using a discriminator that also does identity classiﬁcation in addition to pose prediction, to preserve
id and pose. While these models do pure face rotation of a 2D face, our model can warp the input
face to any other target face, allowing warping the input face to any other identity, with a different
geometry and pose. Moreover, our model also estimates the 3D geometric afﬁne transformation
parameters explicitly, allowing these parameters to be used later, e.g., for face texture swap.

FF-GAN [25], DA-GAN [27], and FaceID-GAN [18] estimate parameters of either a 3D Morphable
Model (3DMM), as in [25; 18], or source to target pose transformation, as in [27]. FF-GAN uses
3DMM parameters to frontalize faces in an adversarial approach, while FaceID-GAN uses the
3DMM parameters to generate any target pose. These models, however, train 3DMM on ground
truth labels such as identity, expression and pose. DepthNet, on the other hand, estimates depth and
afﬁne transformation parameters without requiring ground truth afﬁne or depth labels or pre-training.
Similar to DepthNet, DA-GAN [27] estimates parameters of an afﬁne transformation model that
maps a 2D face to a 3D face. Unlike DepthNet that estimates depth on the source face, DA-GAN
uses depth in a template target face. While their approach eliminates the need for depth estimation, it
only allows the source face to be mapped to the target template geometry, while DepthNet can map
the source face to any target geometry, provided by a target image, or its keypoints. We demonstrate
the application of this ﬂexibility for the face replacement task.

The aforementioned adversarial models use an identity preserving loss to maintain identity. The core
DepthNet model does not need identity labels and preserves well the identity (as shown in Figure 1

8

(right)). However, the identity information can be used by the proposed adversarial components, as in
background synthesis, to further improve the results. Unlike some of these models that take target
pose as input, DepthNet uses the target keypoints to estimate the target geometry and does not require
the target pose. This has several advantages; 1) DepthNet can map to the geometry of the target face
in addition to the pose, and 2) in the face replacement task, DepthNet can replace the target face with
the warped source face directly onto the target face location. Its application is shown in the face swap
experiment in Section 3.3.

4.3 Depth Estimation

Thewlis et. al [21] propose a mapping technique to learn a proxy of 2D landmarks in an unsupervised
way. A semi-supervised technique has been also proposed in [11] that improves landmark localization
by using weaker class labels (e.g. emotion or pose) and also by making the model predict equivariant
variations of landmarks when such transformations are applied to the image. Similar to these
approaches, DepthNet also maps a source to a target to learn its parameters. However, unlike these
two approaches that estimate 2D landmarks, DepthNet estimates the depth of the landmarks using
2D matching of keypoints, by formulating afﬁne parameters as a function of depth augmentated
keypoints in a closed form solution.

While several models [13; 2; 15] estimate depth with direct supervision, there has been recent models
[29; 5; 3] that estimate depth in an unsupervised training procedure. These models rely on pixel
reconstruction by using frames that are captured from very similar scenes, e.g. nearby frames of
a video [29] or left-right frames captures by stereo cameras [5; 3]. These models estimate depth
on one frame and then by using the disparity map, measure how pixel values of nearby frames
compare to each other. To do this, they also require camera intrinsic parameters, e.g. focal length or
distance between cameras. Unlike these models, our approach does not require source to target pixel
mapping. This allows mapping faces from different people with completely different skin colors,
without knowing camera parameters or how they are positioned with respect to each other. Therefore,
DepthNet is not susceptible to variations in illuminations or lighting between source and target faces.

Tung et. al [23] estimate 3D human pose in videos, where they use synthetic data to pre-train internal
parameters of the model and ﬁne-tune them by keypoint, segmentation and motion loss. Adversarial
Inverse Graphics Networks (AIGN) [24] estimates 3D human pose from 2D keypoint heatmaps in a
semi-supervised manner with a similar formulation to that of CycleGAN. They apply an adversarial
loss on the 3D pose to make them look realistic. These models leverage the depth values either
through synthetic data [23], or by adversarial usage of ground truth depth values [24]. Unlike these
models, DepthNet does not rely on any depth signal, either directly or indirectly. MOFA [20] builds
a 3D face mesh using a single image, where the 3D face parameters such as 3D shape and skin
reﬂectance are estimated by an encoder and then using a differentiable model they are rendered back
to the image by the decoder. This model requires manual initialization to map the input image to the
3D mesh, since otherwise it is doing an unconstrained optimization by adapting both the face pose
and the skin reﬂectance. Our model, however, does not require any manual initialization.

5 Conclusion

We have proposed a novel approach to 3D face model creation which enables pose normalization
without using any ground truth depth data. We achieve our best quantitative keypoint registration
results using our novel formulation for predicting depth and 3D visual geometry simultaneously,
learned by backpropagating through the analytic solution for the visual geometry estimation problem
expressed as a function of predicted depths. We have illustrated the quality and utility of the depths
and 3D transformations obtained using our method by transforming source faces to a wide variety of
target poses and geometries. Our technique can be used for face rotation and replacement and when
combined with adversarial repair it can blend warped faces to also synthesize the background. The
proposed model, however, carries forward emotion from source to target due to learning a shared
afﬁne parameters for all keypoints. Moreover, for extreme non-frontal faces, while DepthNet can
extract the transformation params (since it only relies on keypoints), OpenGL cannot extract texture
due to occlusion. We show an example of how to address this in the supplementary material. An
interesting extension to this paper can be replacing the OpenGL pipeline with a generative adversarial
framework that synthesizes a face using the parameters estimated by DepthNet.

9

6 Acknowledgments

We would like to thank Samsung and Google for partially funding this project. We are also thankful
to Compute Canada and Calcul Quebec for providing computational resources, and to Poonam Goyal
for helpful discussions.

References

[1] Blanz, Volker and Vetter, Thomas. A morphable model for the synthesis of 3d faces.

In
Proceedings of the 26th annual conference on Computer graphics and interactive techniques,
pp. 187–194. ACM Press/Addison-Wesley Publishing Co., 1999.

[2] Eigen, David, Puhrsch, Christian, and Fergus, Rob. Depth map prediction from a single image
using a multi-scale deep network. In Advances in Neural Information Processing Systems
(NIPS), pp. 2366–2374, 2014.

[3] Garg, Ravi, BG, Vijay Kumar, Carneiro, Gustavo, and Reid, Ian. Unsupervised cnn for single
view depth estimation: Geometry to the rescue. In Proceedings of the European Conference on
Computer Vision (ECCV), pp. 740–756. Springer, 2016.

[4] Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty of training deep feedforward
neural networks. In International conference on Artiﬁcial Intelligence and Statistics (AISTATS),
pp. 249–256, 2010.

[5] Godard, Clément, Mac Aodha, Oisin, and Brostow, Gabriel J. Unsupervised monocular depth
estimation with left-right consistency. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2017.

[6] Gross, Ralph, Matthews, Iain, Cohn, Jeffrey, Kanade, Takeo, and Baker, Simon. Multi-pie.

Image and Vision Computing, 28(5):807–813, 2010.

[7] Hassner, Tal. Viewing real-world faces in 3d.

In Proceedings of the IEEE International

Conference on Computer Vision (ICCV), pp. 3607–3614, 2013.

[8] Hassner, Tal, Harel, Shai, Paz, Eran, and Enbar, Roee. Effective face frontalization in uncon-
strained images. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 4295–4304, 2015.

[9] He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE
International Conference on Computer Vision (ICCV), pp. 1026–1034, 2015.

[10] Honari, Sina, Yosinski, Jason, Vincent, Pascal, and Pal, Christopher. Recombinator networks:
Learning coarse-to-ﬁne feature aggregation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pp. 5743–5752, 2016.

[11] Honari, Sina, Molchanov, Pavlo, Tyree, Stephen, Vincent, Pascal, Pal, Christopher, and Kautz,
Jan. Improving landmark localization with semi-supervised learning. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.

[12] Huang, Rui, Zhang, Shu, Li, Tianyu, and He, Ran. Beyond face rotation: Global and local
perception gan for photorealistic and identity preserving frontal view synthesis. In Proceedings
of the IEEE International Conference on Computer Vision (ICCV), Oct 2017.

[13] Jackson, Aaron S, Bulat, Adrian, Argyriou, Vasileios, and Tzimiropoulos, Georgios. Large pose
3d face reconstruction from a single image via direct volumetric cnn regression. In Proceedings
of the IEEE International Conference on Computer Vision (ICCV), pp. 1031–1039. IEEE, 2017.

[14] Jeni, László A, Cohn, Jeffrey F, and Kanade, Takeo. Dense 3d face alignment from 2d videos in
real-time. In IEEE International Conference and Workshops on Automatic Face and Gesture
Recognition (FG), volume 1, pp. 1–8. IEEE, 2015.

10

[15] Liu, Fayao, Shen, Chunhua, and Lin, Guosheng. Deep convolutional neural ﬁelds for depth
estimation from a single image. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 5162–5170, 2015.

[16] Parkhi, Omkar M, Vedaldi, Andrea, and Zisserman, Andrew. Deep face recognition. Proceedings

of the British Machine Vision Conference (BMVC), 2015.

[17] Sagonas, Christos, Tzimiropoulos, Georgios, Zafeiriou, Stefanos, and Pantic, Maja. 300 faces
in-the-wild challenge: The ﬁrst facial landmark localization challenge. In Proceedings of the
IEEE International Conference on Computer Vision Workshops (CVPRW), pp. 397–403, 2013.

[18] Shen, Yujun, Luo, Ping, Yan, Junjie, Wang, Xiaogang, and Tang, Xiaoou. Faceid-gan: Learning
a symmetry three-player gan for identity-preserving face synthesis. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 821–830, 2018.

[19] Taigman, Yaniv, Yang, Ming, Ranzato, Marc’Aurelio, and Wolf, Lior. Deepface: Closing the
gap to human-level performance in face veriﬁcation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 1701–1708, 2014.

[20] Tewari, Ayush, Zollhöfer, Michael, Kim, Hyeongwoo, Garrido, Pablo, Bernard, Florian, Perez,
Patrick, and Theobalt, Christian. Mofa: Model-based deep convolutional face autoencoder for
unsupervised monocular reconstruction. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), volume 2, 2017.

[21] Thewlis, James, Bilen, Hakan, and Vedaldi, Andrea. Unsupervised learning of object landmarks
by factorized spatial embeddings. In Proceedings of the IEEE International Conference on
Computer Vision (ICCV), volume 1, pp. 5, 2017.

[22] Tran, Luan, Yin, Xi, and Liu, Xiaoming. Disentangled representation learning gan for pose-
invariant face recognition. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017.

[23] Tung, Hsiao-Yu, Tung, Hsiao-Wei, Yumer, Ersin, and Fragkiadaki, Katerina. Self-supervised
learning of motion capture. In Advances in Neural Information Processing Systems (NIPS), pp.
5242–5252, 2017.

[24] Tung, Hsiao-Yu Fish, Harley, Adam W, Seto, William, and Fragkiadaki, Katerina. Adversarial
inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation from
unpaired supervision. In Proceedings of the IEEE International Conference on Computer Vision
(ICCV), volume 2, 2017.

[25] Yin, Xi, Yu, Xiang, Sohn, Kihyuk, Liu, Xiaoming, and Chandraker, Manmohan. Towards
large-pose face frontalization in the wild. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), pp. 1–10, 2017.

[26] Zhang, Xing, Yin, Lijun, Cohn, Jeffrey F, Canavan, Shaun, Reale, Michael, Horowitz, Andy,
Liu, Peng, and Girard, Jeffrey M. Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic
facial expression database. Image and Vision Computing, 32(10):692–706, 2014.

[27] Zhao, Jian, Xiong, Lin, Jayashree, Panasonic Karlekar, Li, Jianshu, Zhao, Fang, Wang, Zhecan,
Pranata, Panasonic Sugiri, Shen, Panasonic Shengmei, Yan, Shuicheng, and Feng, Jiashi. Dual-
agent gans for photorealistic and identity preserving proﬁle face synthesis. In Advances in
Neural Information Processing Systems (NIPS), pp. 66–76, 2017.

[28] Zhao, Jian, Cheng, Yu, Xu, Yan, Xiong, Lin, Li, Jianshu, Zhao, Fang, Jayashree, Karlekar,
Pranata, Sugiri, Shen, Shengmei, Xing, Junliang, et al. Towards pose invariant face recognition
in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 2207–2216, 2018.

[29] Zhou, Tinghui, Brown, Matthew, Snavely, Noah, and Lowe, David G. Unsupervised learning of
depth and ego-motion from video. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2017.

[30] Zhu, Jun-Yan, Park, Taesung, Isola, Phillip, and Efros, Alexei A. Unpaired image-to-image
translation using cycle-consistent adversarial networks. In Proceedings of the IEEE International
Conference on Computer Vision (ICCV), 2017.

11

Unsupervised Depth Estimation, 3D Face Rotation and
Replacement: Supplementary Information

S.1 Experimental Setup

The RCN, the DepthNets and the CycleGAN modules are trained separately. Each model is trained
using standard techniques for the model class and has a separate objective to be optimized. DepthNet
does not use the OpenGL pipeline during training and only uses it to render faces at test time, allowing
DepthNet to train faster.

S.1.1 DepthNet Experimental Details

Here we describe the details of the experiments carried out in Section 3.1. Our DepthNet architectures
require keypoints of both source and target images to be extracted. For this, the image is ﬁrst passed
through the VGG-Face [16] face detector. The face crops are then scaled down to 80 × 80 and
converted to greyscale, following which they are passed through RCN to obtain N = 68 keypoints
on each image. The RCN is trained exactly as described in [10], using the 300W dataset [17].

The keypoint only variant of our model involves concatenating all detected keypoints and passing
them through a two-layer deep fully connected network, with 256 hidden units and o output units.
The size of o depends on whether we are predicting only the depth, in which case o = N , or both
depth and afﬁne transformation parameters, in which case o = N + 8.

As discussed above, it is possible to augment these models with a Siamese CNN module (case A). In
the model variants that also use the image, we pass both the source and the target images through
three conv-maxpool layers with shared weights of size (32, 4, 2), (48, 3, 2), (64, 2, 2), respectively for
the representation (num_filters, filter_size, pool_size). The network’s outputs for the
source and target faces are then concatenated before passing them into a 4-layered fully connected
network with respective output sizes of 2048, 512, 256, and o. The keypoints are concatenated to the
512-unit layer before being passed to the last two layers. See Figure 1 (left) for an illustration of the
model. We explore these Siamese CNN augmented variants in models 4, 6 and 8 in Table 1.

We set the initial learning rate to 0.001 and use a Nesterov momentum optimizer (with a momentum
of 0.9) in all our experiments. With the exception of the last layer, we initialize all weights with
a Glorot initialization scheme [4], with the weights sampled from a uniform distribution. We use
a ReLU gain [9], set all biases to 0, and apply a ReLU non-linearity after every layer. In the ﬁnal
output layer, we do not apply any non-linearity and initialize the weights to 0. The biases of units that
represent depths are initialized to a random Normal distribution with µ = 0 and σ = 0.5, while those
that form the predicted afﬁne transform are initialized with the equivalent of a "ﬂattened" identity
transform. All models have been trained for 500 epochs.

We point out that except for a comparison between learning rates in the set {0.01, 0.001, 0.0001}
over few (less than 10) epochs, to ﬁnd a learning rate that the model seems to train well with, we
have not performed a hyperparameter search, and anticipate that the performance of the model can be
made even better by searching the hyperparameter space on a per model basis and by using deeper
(or modiﬁed) architectures.

S.2 Depth Visualization

We show further estimated depth values by different models in Figure S1. DepthNet+GAN and AIGN
generate the closest depth values to the GT depth. The baseline DepthNet model estimates reliable
depth values for most cases, however it has some degree of inaccuracy, as shown in the last two rows.
In DepthNet, the estimated depth indicates the position of each keypoint relative to other keypoints
rather than with respect to a source and importantly it is done without any supervision. MOFA, which
is also unsupervised, generates very similar face templates for different cases.

12

Figure S1: Depth visualization for different models (color coded by depth). The Depth axis is the one pointing
into the page. From left to right: RGB image, Ground Truth, DepthNet, DepthNet+GAN, AIGN and MOFA
estimated depth values.

S.3 Additional Camera Sweep Visualizations

In this section, we present additional visualizations along the lines of those shown in Figure 1 (right)
in Section 2 of the main paper. Frontal faces selected from the Multi-PIE dataset [6] are re-projected
to match several other poses corresponding to a person with a different identity. The DepthNets
predicts reliable proxy depths, which when coupled with the analytically obtained afﬁne transform
(obtained from the least-squares pseudo-inverse-based solution described in Section 2.3 of the main
paper) yields faces close to the desired target face geometry when passed through the OpenGL
pipeline.

We show camera sweeps for frontal source faces in Figures S2 and S3 and non-frontal source faces in
Figure S4. In Figure S5 we use the same target identity as the source face, showing how much the

13

Figure S2: Projecting a frontal face (far left) to a range of other poses deﬁned by faces in the row above.

generated face differs from the ground truth target. For all samples in Figures S2, S3, S4, and S5 we
use the DepthNet model that relies on only key-points (model 7 in Table 1).

Note that for non-frontal source faces, the quality of images is reduced specially for frontal target
faces. This is due to lack of adequate texture on the occluded side of the face to be transferred to the
target pose by OpenGL pipeline, rather than inaccuracies in the afﬁne transformation parameters. In
order to reduce the side-affects, we use either of the source face or its ﬂipped version, that is closer to
the target face pose, and then warp the face to the target keypoints.

Figure S3: Projecting a frontal face (far left) to a range of other poses deﬁned by faces in the row above.

Figure S4: Re-projecting a non-frontal face (far left) to a range of other poses deﬁned by faces in the row above.

Figure S5: Rotating a face (far left) to a range of other poses deﬁned by faces of the same identity in the row
above. On the top row frontal and on the bottom row non-frontal source faces are shown.

S.4 CycleGAN

Suppose we have some images belonging to one of two sets x ∈ X and y ∈ Y , where x denotes a
DepthNet-resulting face and y a ground truth face which is frontal. We wish to learn two functions

14

(cid:105)

(cid:105)

,

(cid:104)

Ex,y

min
G,F

(cid:104)

Ex,y

min
DX ,DY

F : X → Y and G : Y → X which are able to map an image from one set to the corresponding
image in the other. Correspondingly, we have two discriminators DX and DY which try to detect
whether the image in that particular set is real or generated. While we are only interested in the
function F : X → Y (since this is mapping to the distribution of ground truth faces) the formulation
of CycleGAN requires that we learn mappings in both directions during training. We optimize the
following objectives for the two generators F and G:

(cid:96)(DX (G(y)), 1) + (cid:96)(DY (F (x)), 1) + λ||y − F (G(y))||1 + λ||x − G(F (x))||1

(4)

And the following for the two discriminators DX and DY :

(cid:96)(DX (x), 1) + (cid:96)(DX (G(y)), 0) + (cid:96)(DY (y), 1) + (cid:96)(DY (F (x)), 0)

(5)

where 0/1 denote fake/real, (cid:96)() is the squared error loss and λ is a coefﬁcient for the cycle-consistency
(reconstruction) loss.

In the case where we did adversarial background synthesis, x is a channel-wise concatenation of the
DepthNet-frontalized face and the background of the original (pre-frontalized) image. For face-swap
cleanup, x is simply a source face which has been warped to a target face and pasted on top.

Once the network has been trained, we can disregard all other functions and use F to clean up faces
which are low quality due to artifacts from warping.

In terms of architectural details the generators and discriminators used were those described in the ap-
pendix of the CycleGAN paper [30]. In short, the generator consists of three conv-BN-relu blocks
which downsample the input, followed by nine ResNet blocks (which can be interpreted as iteratively
performing transformations over the downsampled representation), followed by deconv-BN-relu
blocks to upsample the representation back into the original input size. For training, we use the
same hyperparameters as most CycleGAN implementations which is using the Adam optimizer with
learning rate α = 2 × 10−4, β1 = 0.5, β2 = 0.999. However, instead of using a batch size of 1 we
use the largest possible batch size, which was 16 for a 12GB GPU.

Note that in order to produce better translations, the dataset we used for all CycleGAN experiments
contain both the VGG and the CelebA datasets, which has signiﬁcantly more images.

S.4.1 Background Synthesis

We present extra visualizations for the CycleGAN which performs background synthesis on CelebA,
corresponding to Figure 4 in the main paper. These are shown in Figure S6.

Figure S6: Background synthesis with CycleGAN. Left to right: source face; keypoints overlaid; DepthNet
(DN); DN + background → frontal

S.4.2 Face Replacement and Adversarial Repair

In Figure S7 we provide extra face swap samples on CelebA, corresponding to Figure 5 in the
main paper, where a source face is warped to a target face pose using DepthNet, pasted onto the

15

target image and then passed to a CycleGAN to adapt the face skin of the warped source face to the
background and hairstyle of the target face.

Figure S7: Face swap experiment with CycleGAN. Left to right: source face; target face; warp to target with
DepthNet; repaired result with CycleGAN. The source face is taken and warped onto the target face. The
background and hairstyle is then adapted to the target face.

S.4.3 Extreme Pose Face Clean-up

If the source image has an extreme pose, the texture will be missing on the occluded side of the
face and the OpenGL pipeline cannot rotate the face without artifacts. Note that this shortcoming is
due to lack of texture on the occluded side of the face rather than a deﬁciency of the transformation
parameters measured by DepthNet.

We performed an experiment using CycleGAN to ﬁx such artifacts. For this experiment we take
source images from CelebA and ﬁrst frontalize it by using DepthNet. Since the frontalized faces have
artifacts due to stretch of texture on the occluded side of the face by the OpenGL pipeline, we train a
CycleGAN that takes DepthNet frontalizaed faces plus the background of the original non-frontal
image (as two images) in one domain and the ground truth frontal faces in the other domain. The
CycleGAN learns to clean-up these artifacts. Finally, we take the GAN-repaired frontalized faces
and project it to different target poses using DepthNet. In Figure S8 we visualize camera sweep for
source faces in the wild that have extreme poses. The cycleGAN reasonably cleans the face artifacts
and then DepthNet projects it to different poses. This is just one approach to address the extreme
pose occlusion artifacts. We see alternative methods for addressing this issue as promising directions
for future research.

Figure S8: Re-projecting a non-frontal face (far left) from CelebA to a range of other poses deﬁned by faces in
the row above. Top row (in each pair) depicts the target faces from Multi-PIE [6]. The bottom row shows from
left to right: source face, souce face frontalized by DepthNet, adversarial-repaired face, the repaired source face
projected to the target poses (4th to 10th columns).

16

8
1
0
2
 
c
e
D
 
4
2
 
 
]

V
C
.
s
c
[
 
 
5
v
2
0
2
9
0
.
3
0
8
1
:
v
i
X
r
a

Unsupervised Depth Estimation,
3D Face Rotation and Replacement

Joel Ruben Antony Moniz1∗, Christopher Beckham2,3∗, Simon Rajotte2,3,
Sina Honari2, Christopher Pal2,3,4
1Carnegie Mellon University, 2Mila-University of Montreal, 3Polytechnique Montreal, 4Element AI
1jrmoniz@andrew.cmu.edu, 2honaris@iro.umontreal.ca, 3firstname.lastname@polymtl.ca

Abstract

We present an unsupervised approach for learning to estimate three dimensional
(3D) facial structure from a single image while also predicting 3D viewpoint
transformations that match a desired pose and facial geometry. We achieve this
by inferring the depth of facial keypoints of an input image in an unsupervised
manner, without using any form of ground-truth depth information. We show how
it is possible to use these depths as intermediate computations within a new back-
propable loss to predict the parameters of a 3D afﬁne transformation matrix that
maps inferred 3D keypoints of an input face to the corresponding 2D keypoints on
a desired target facial geometry or pose. Our resulting approach, called DepthNets,
can therefore be used to infer plausible 3D transformations from one face pose
to another, allowing faces to be frontalized, transformed into 3D models or even
warped to another pose and facial geometry. Lastly, we identify certain shortcom-
ings with our formulation, and explore adversarial image translation techniques as
a post-processing step to re-synthesize complete head shots for faces re-targeted to
different poses or identities. 1

1

Introduction

Face rotation is an important task in computer vision.
It has been used to frontalize faces for
veriﬁcation [8; 19; 25; 28] or to generate faces of arbitrary poses [22; 18]. In this paper we present
a novel unsupervised learning technique for face rotation and warping from a 2D source image –
whose facial appearance will be used in the rotation – to a target face – to which the facial pose
and geometry inferred from the source image is mapped. A use case is when we have an image of
someone in a particular target pose and we want to put a given source face into that pose, without
knowing the exact target face pose. This can be leveraged, for example, in the advertisement industry,
when putting someone in a particular location can be costly or unfeasible, or in the movie industry
when the main actor’s limited time or high cost can enforce using another actor whose face can be
later replaced by the main actor’s. This is achieved through estimating the source face depth and
the 3D afﬁne parameters that warp the source to the target face using neural networks. These neural
networks use a novel loss formulation for the structured prediction of keypoint depths. Once the 3D
afﬁne transformation matrix is estimated, it can be used to warp the source image onto the target
face geometry using a textured triangular mesh. The use of a 3D afﬁne transform means that we
can capture both a 3D rotation of the face to a new viewpoint as well as a global non-Euclidean
warping of the geometry to match a target face. We call these neural networks Depth Estimation-Pose
Transformation Hybrid Networks, or DepthNets in short.

Our ﬁrst contribution is to propose a neural architecture that predicts both the depth of source
keypoints as well as the parameters of a 3D geometric afﬁne transformation which constitute the

∗Indicates equal contribution.
1Code will be released at: https://github.com/joelmoniz/DepthNets/

32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.

explicit outputs of the DepthNet model. The predicted depth and afﬁne transformation could be then
used to map a source face to a target face for object orientation, distortion and viewpoint changes.

Our second contribution consists of making the observation that given 3D source and 2D target
keypoints, closed form least squares solutions exist for estimating geometric afﬁne transformation
models between these sets of keypoint correspondences, and we can therefore develop a model that
captures the dependency between depth and the afﬁne transformation parameters. More speciﬁ-
cally, we express the afﬁne transformation as a function of the pseudoinverse transformation of 2D
keypoints in a source image – augmented by inferred depths – and the target keypoints. Thus, the
second and major contribution in this work is capturing the relationship between an estimated afﬁne
transformation and the inferred depth as a deterministic relationship. In this formulation, DepthNet
only predicts depth values explicitly and the afﬁne parameters are inferred through a pseudoinverse
transformation of source and target keypoints. Here, one can directly optimize through the solutions
of what might otherwise be formulated as a secondary minimization step.

Our proposed DepthNet can map the central region of the source face to the target geometry. This
leads to background mismatch when warping one face to another. Finally, our third contribution is to
use an adversarial unpaired image-to-image transformation approach to repair the appearance of 3D
models inferred from DepthNet. Together these contributions allow 3D models of faces that construct
realistic images in the target pose. Our proposed method can be used for pose normalization or face
swaps with no manually speciﬁed 3D face model. To the best of our knowledge, this is the ﬁrst such
neural network based model that estimates a 3D afﬁne transformation model for face rotation which
neither requires ground-truth 3D images nor any ground truth 3D face information such as depth.

As we have outlined above, our approach uses neural networks for inferring depth and geometric
transformation – referred to as DepthNets; and, an adversarial image-to-image transformation network
which improves the quality of the appearance of a 3D model inferred from a DepthNet.

2 Our Approach

DepthNets

We propose three DepthNet formulations, described in Sections 2.1, 2.2, and 2.3. For each of the
three models we explore two architectural scenarios: (A) a Siamese-like architecture that uses the
source and target images themselves as well as keypoints extracted from these images, and (B) a
fully-connected neural network variant which uses only facial keypoints in the source and target
images. See Figure 1 (left) for details.

Figure 1: (Left) DepthNet architecture. The blue region is only used in case (A) and the red part is used in
both cases (A) and (B), described in Section 2. The orange output (the 8 afﬁne transformation parameters) is
predicted only by model variations described in Sections 2.1 and 2.2, and not the model described in section
2.3. All three models predict the N depth values of the source keypoints. C, P, and FC correspond to valid conv,
pool and fully-connected layers. The two paths of Siamese network share parameters and the black dots indicate
concatenating keypoint values to FC units. (Right) Visualizing face rotation by re-projecting a frontal face (far
left) to a range of other poses deﬁned by the faces in the row above (in each pair of rows). In this experiment, we
only use keypoints from the top-row in the DepthNet model (Model 7 in Table 1).

It is interesting to note that if DepthNets are used to register a set of images of objects to the same
common viewpoint, the same image and geometry can be used as the target. This is the case for
the frontalization of faces, for example. While the DepthNet framework is sufﬁciently general to be
applied to any object type where 2D keypoint detections have been made, our experiments here focus
on faces. We describe the three variants of DepthNets below.

2

2.1 Predicting Depth and Viewpoint Separately

In this variant of DepthNets, the model predicts both depths and viewpoint geometry, but as separate
explicit outputs of a neural network. The input is comprised of only the geometry and pose of the
source and target faces (encoded in the form of a 2D keypoint template), in case (B), or both keypoints
and images of the source and target faces, in case (A). The key phases of this stage are described by
the sequence of steps given below:

1. Keypoint extraction: Raw (x, y) pixel coordinates corresponding to the keypoints of each image
are extracted using a Recombinator Network (RCN) [10] architecture, and then concatenated before
being passed into the keypoint processing step.
2. (Optional) Image Feature Extraction: DepthNets can be conditioned on only keypoints, case (B),
or on keypoints and the original images, case (A). We can therefore optionally subject the source and
target images to alternating conv-maxpool layers. If this component of the architecture is used, the
last spatial feature maps in the Siamese architecture are concatenated before being given to a set of
densely connected hidden layers.
3. Keypoint processing: In this step keypoints are passed through a set of hidden layers. If the Image
Feature Extraction stage is used, the keypoints are concatenated to image features, the output of
which is in turn fed to densely connected layers. The output layer of this phase will be of size N + 8,
where N is the number of keypoints. The ﬁrst N points represent the Depth proxy, and the last 8
points form a 4 × 2 matrix representing the learned parameters of the afﬁne transform. See Figure 1.
4. Geometric Afﬁne Transformation Normalizer: This phase applies the predicted afﬁne transform
on each (depth augmented) source keypoint to estimate its target location. Let (xi
s) represent the
ith source keypoint, (xi
n) the corresponding source normalized keypoint estimated by applying
the afﬁne transformation matrix, (xi
t) the ith target keypoint (as ground truth (GT)), and Is and It
represent the source and target images respectively. Depending on which underlying architectural
variant we use, two cases arise: one that utilizes only the keypoints (B), and another utilizing
both the keypoints and the images (A). Since the keypoints are generated using RCNs, they are
technically functions of the input images: [xs, ys] = R(Is), and [xt, yt] = R(It). Depending
on the (A) or (B) variant, the ith keypoint’s predicted depth proxy zi
p is inferred as a function
of the input keypoints, or both input keypoints and input images.
In both cases the keypoints
are derived from the images, so zi
p(Is, It). Similarly, the 3D-2D afﬁne transform F is a
function of the images, such that F = F(Is, It), where the 8 predicted parameters are: m =
{m1, m2, m3, tx, m4, m5, m6, ty)}. These constitute the 3D-2D afﬁne transform which is used by
all keypoints. In other words, each of the i points is transformed using xi
s, or:

n = F(Is, It) xi

p = zi

n, yi

s, yi

t, yi

(cid:20) xi
n
yi
n

(cid:21)

=

(cid:20)m1 m2 m3
m4 m5 m6






(cid:21)

tx
ty

xi
s
yi
s
zi
p(Is, It)
1






The loss function of a DepthNet is obtained by transforming the source face to match the target face
using the simple squared error of the corresponding target object’s keypoint vector xt = [xt, yt]T
, as GT values, and the source object’s normalized keypoint vector [xn, yn]T . The loss for one
example where we predict depth and afﬁne viewpoint geometry can therefore be expressed as:

L =

K
(cid:88)

i=1

(cid:13)
(cid:13)xi
(cid:13)

t − F(Is, It) [xi

s yi

s zi

p(Is, It)]T (cid:13)
2
(cid:13)
(cid:13)

(1)

5. Image Warper: This phase consists of using the depth proxy and afﬁne transform matrix generated
to actually warp the face from its source pose to be matched to the target object geometry. The ﬁnal
projection to 2D is achieved by simply dropping the transformed z coordinate (which corresponds
to an orthographic projection model). In the case of DepthNets, this orthographic projection is
effectively embedded in the Geometric Afﬁne Transformation Normalizer step, since the afﬁne
corresponding to the z coordinate is not predicted, essentially dropping it.
As we operate on keypoints, the actual warping of pixels can be performed with a high quality
OpenGL pipeline that performs the warp separately from the rest of the architecture. Source image,
keypoints augmented with depth, and the afﬁne matrix are passed to OpenGL pipeline to warp the
source image towards the target pose. This OpenGL warping is not needed during DepthNet training,
which means we do not have to do feedforward or backprop through OpenGL. In Summary, for step 1
the RCN model [10] is used, for steps 2 to 4 the DepthNet model, shown in Figure 1 (left), is trained,

3

and for step 5 an OpenGL pipeline is used. No data or parameters are needed to train the OpenGL
pipeline. It warps images by directly using the provided data.

2.2 Estimating Viewpoint Geometry as a Second Step

In this model variant, training is similar to Section 2.1 and the model outputs depth and 3D
afﬁne transformation parameters. However, at test time, rather than using the predicted 3D afﬁne
transformation for pairs of faces, we use only the predicted depths and estimate the afﬁne ge-
ometry parameters as a second estimation step. More precisely, given 3D points for a scene
and the corresponding 2D points for a target geometry it is possible to formulate the estima-
tion of a 3D afﬁne transformation as a linear least squares estimation problem. An overdeter-
mined system of the form Am = xt for this problem can be constructed as shown in (2).

This corresponds to an afﬁne camera model
followed by an orthographic projection to
2D keypoints. This setup also leads to
the following closed form solution for the
afﬁne transformation parameters:

m = [AT A]−1AT xt,

(3)

y1
s
0
y2
s
0

z1
s
0
z2
s
0

x1
s
0
x2
s
0













xK
s
0

yK
s
0

zK
s
0

0
x1
s
0
x2
s

0
xK
s

0
y1
s
0
y2
s
...
0
yK
s

0
z1
s
0
z2
s

1 0
0 1
1 0
0 1

0
zK
s

1 0
0 1





































m1
m2
m3
m4
m5
m6
tx
ty

























x1
t
y1
t
x2
t
y2
t
...
xK
t
yK
t

=

(2)

where this pseudoinverse based transformation is parameterized by the reference points and their
predicted depths.

2.3

Joint Viewpoint and Depth Prediction

Our key observation is that one can alternatively use the closed form analytical solution, measured in
Eq. (3), for the least squares estimation problem as the underlying afﬁne transformation matrix within
the loss function. This leads to a special form of structured prediction problem for geometrically
consistent depths and afﬁne transformation matrix. For each image we have L =

K
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:21)

(cid:20) xi
t
yi
t
(cid:124) (cid:123)(cid:122) (cid:125)
xi
t

(cid:20)m1 m2 m3
m4 m5 m6

−

(cid:124)

(cid:123)(cid:122)
m

tx
ty

(cid:21)

(cid:125)







(cid:124)

xi
s
yi
s
zi
p(Is, It)
1
(cid:123)(cid:122)
xi
s







(cid:125)

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

K
(cid:88)

i=1

(cid:13)
(cid:13)xi

t − reshape(cid:2)[AT A]−1AT xt

(cid:3)xi

s

(cid:13)
2
(cid:13)

where the matrix A is parameterized as a function of xs as shown in Eq. (2). In this variant, the
model explicitly outputs only depth values during train and test time. The afﬁne transformation matrix
in the equation above is replaced by Eq. (3), which measures the afﬁne transformation as a pure
function of source and target keypoints plus the inferred depth. The big difference of this formulation
compared to Sections 2.1 and 2.2 is that geometric afﬁne transformation parameters are no longer
predicted by DepthNet during training and at both train and test time – it solves the least square loss
through the pseudoinverse based transformation. Since zi
t }j=1...N ) is predicted
within the analytical formulation of the solution to the least squares minimization problem, we can
backpropagate through the solution of a minimization problem that depends on the predicted depths.
While we leverage keypoints for depth estimation, the proposed approach is novel in how the depth is
estimated. Note that it is unsupervised with respect to depth labels. No depth supervision either by
using depth targets (as in [2; 13; 15]), or by using depth in an adversarial setting (as in [24]), is used
to estimate depth values for the base DepthNet models described in Sections 2.1, 2.2, and 2.3.

s = zi

p({xj

s, xj

t , yj

s, yj

The depths learned for keypoints by these approaches are not necessarily true depths, but are likely to
strongly correlate with the actual depth of each keypoint. This is because even though the method
succeeds (as we shall see below) in aligning poses, the inferred depth and the afﬁne transform may
each be scaled by factors so as to cancel each other out (i.e., by factors which are multiplicative
inverses of each other). Real world viewpoint geometry also involves perspective projection.

Adversarial Image-to-Image Transformation

DepthNet transforms the central region of the source face to the target pose. Inevitably, the face
background will be missing, which might make the proposed method unsuitable for many application

4

where the full face is required. To address this issue, we utilize CycleGAN [30], an adversarial
image-to-image translation technique. This serves to repair the background of faces that have
undergone frontalization or face swap through the DepthNet pipeline. Importantly, the adversarial
nature of CycleGAN allows one to perform image transformation between two domains without
the requirement of paired data. In our work, we perform experiments translating between various
domains of interest but one example is translating between the domain of images in the dataset (i.e.
the ground truth) and the domain of images where the DepthNet output is pasted onto the face region
(in the case of face-swap). By doing so we clean the face background in an unsupervised manner.

3 Experiments

3.1 DepthNet Evaluation on Paired Faces

For the experiments in this section, we use a subset of the VGG dataset [16], with training and
validating on all possible pairs of images belonging to the same identity for 2401 identities. This
yields 322,227 train and 43,940 validation pairs. Check experimental setup details in Supplementary.

Model

Color MSE MSE_norm

1) A simple 2D afﬁne registration
2) A 3D afﬁne registration model using an average 3D face template
3) A DepthNet that separately estimates depth and geometry
4) The model above, but with a Siamese CNN image model
5) Secondary least squares estimation for visual geometry using the depths from 3)
6) Secondary least squares estimation for visual geometry using the depths from 4)
7) Backpropagation through the pseudoinverse based solution for visual geometry
8) The model above, but with a Siamese CNN image model

grey
purple
brown
violet
red
green
orange
blue

1.562
0.724
0.568
0.539
0.400
0.399
0.357
0.349

9.547
7.486
6.292
6.115
5.184
5.175
4.932
4.891

Table 1: (left) Comparing the Mean Squared Error (MSE) and MSE normalized by inter-ocular distance
(MSE_norm) of different models. (right) Histogram of Mean Squared Errors. The second column in the Table
(on left) corresponds to the color of the model in the ﬁgure (on right).

We explore the three variants of DepthNets described in Sections 2.1, 2.2, and 2.3, each with two
architectural cases (A) and (B), depending on whether image features are used in addition to keypoints
or not. We also compare with a number of baselines. We measure the mean square error (MSE)
between the estimated keypoints on the target face (source face normalized keypoints) and ground
truth target keypoints. Results for the following models are shown in Table 1:

1) A baseline model registrations using a simple 2D afﬁne transformation.

2) We generate a 3D average face template from the 3DFAW dataset [14; 26; 6] by aligning the 3D
keypoints of all faces in the dataset to a front-facing face using Procrustes superimposition. We report
error by mapping the template face to each source face via Procrustes superimposition (to get a 3D
face f ) and then use an afﬁne transformation from the 3D face f to the target face.

3, 4) We use our proposed approach to predict both depth and geometry (described in Sections 2.1).

5, 6) These models described in Section 2.2. Note that during training, these two cases are similar to
models 3 and 4 in Table 1.

7, 8) The pseudo-inverse formulation model described in Section 2.3.

As observed in Table 1, a simple 2D afﬁne transform (model 1) without estimating depth and a
template 3D face (model 2) get high errors on mapping to the target faces. DepthNet models get lower
errors and the pseudo-inverse formulation (models 7 and 8) further reduces the error by 10%. The
CNN models slightly reduce errors compared to their equivalent models that rely only on keypoints.

3.2 DepthNet Evaluation on Unpaired Faces and Comparison to other Models

In this section we train DepthNet on unpaired faces belonging to different identities and compare
with other models that estimate depth. We use the 3DFAW dataset [14; 26; 6] that contains 66
3D keypoints to facilitate comparing with ground truth (GT) depth. It provides 13,671 train and
4,500 valid images. We extract from the valid set, 75 frontal, left and right looking faces yielding
a total of 225 test images, which provides a total of 50,400 source and target pairs. We train the
psuedoinverse DepthNet model that relies on only keypoints (model 7 in Table 1). We also train
a variant of DepthNet that applies an adversarial loss on the depth values (DepthNet+GAN). This
model uses a conditional discriminator that is conditioned on 2D keypoints and discriminates GT
from estimated depth values. The model is trained with both keypoint and adversarial losses.

5

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)

Source

Target

Left
Front
Right

DepthNet

DepthNet + GAN

Left
24.67
25.54
21.66

Front Right
29.70
27.71
26.19
27.22
23.87
21.48

Avg
27.36
26.32
22.34

Left
59.78
58.77
59.97

Front Right
59.63
59.67
58.61
58.67
59.60
59.70

Avg
59.69
58.68
59.76

Table 2: Comparing DepthCorr for different DepthNet models when mapping variant source to target poses. The
Avg column measures the average over the three preceding columns.

We measure the correlation matrix between GT and estimated depths, where the element k in the
diagonal indicates the correlation between estimated and ground truth depth values for keypoint k,
yielding a value between -1 and 1. We report the sum of absolute values of the diagonal of this
matrix, indicated by DepthCorr. We compare DepthNet models on DepthCorr in Table 2. For this
experiment we take every possible pair of source to target faces, where source and target are one of
{left, front, right} looking faces. This yields a total of 5,550 pairs when the source and the target are
from the same subset, and 5,625 pairs otherwise. This experiment measures the accuracy of depth
estimation of the DepthNet models on different orientations of source-target faces. The baseline
DepthNet model that does not leverage the depth labels performs well in different cases. DepthCorr
improves more than twice for the DepthNet+GAN model, indicating a direct supervision loss using
depth labels can enhance the depth estimation.

Model

GT Depth
AIGN [24]
MOFA [20]
DepthNet (Ours)
DepthNet + GAN (Ours)

Need Depth Manual Init. MSE (×10−5) Depth Correlation Matrix Trace (DepthCorr)
Right pose
66
49.04
17.54
22.34
59.76

Front pose
66
50.81
15.97
26.32
58.68

8.86 ± 6.55
9.06 ± 6.61
8.75 ± 6.33
7.65 ± 6.97
8.74 ± 6.24

Left pose
66
44.08
11.14
27.36
59.69

-
No
Yes
No
No

Yes
Yes
No
No
Yes

Table 3: Comparing MSE and DepthCorr for different models. A lower MSE indicates the model maps better to
the target faces. A higher DepthCorr indicates more correlation between estimated and GT depths.

We compare our two DepthNet models with three baselines: 1) AIGN [24], 2) MOFA [20] and 3)
GT Depth (no model trained). AIGN estimates 3D keypoints conditioned on 2D heatmaps of the
keypoints. MOFA estimates a 3D mesh using only an image. We implemented the AIGN model
and asked the authors of MOFA to run their model on our test-set. They provided MOFA’s results
for 134 images in the test set. In Table 3 we compare these three models with our DepthNet models
on DepthCorr. We also compare them on MSE, which is measured between GT and estimated
target keypoints. Since the three baselines estimate depth on a single image due to their different
model formulation, we ﬁrst measure m using closed form solution in Eq. 3 and then apply m to
the estimated source keypoints to get the target keypoint estimations. We contrast the estimated
values with the GT target keypoints. As shown in Table 3, GT depth has the highest DepthCorr
(the maximum possible value). The depths estimated by DepthNet+GAN and AIGN have stronger
correlation to GT depth compared to the baseline DepthNet and MOFA, while baseline DepthNet
performs better than MOFA. On MSE the baseline DepthNet model gets smaller MSE when mapping
to target faces, indicating it is better suited for this task.

In Figure 2 we plot heatmaps of the estimated depth of different models (on Y axis) and the GT depth
(on X axis) aggregated over all 66 keypoints on all test data. As can be seen, the depth estimated
by DepthNet+GAN and AIGN models form a 45 degree rotated ellipses showing a stronger linear
correspondence with respect to the GT depth compared to the the baseline DepthNet and MOFA.

Figure 2: Predicted (Y axis) versus Ground Truth (X axis) depth heatmaps for different models.

In Figure 3 we show some estimated depth samples for different models (see more samples in Figure
S1). AIGN and DepthNet+GAN generate more realistic results. MOFA generates very similar
face templates for different poses. Baseline DepthNet estimates reliable depth values in most cases,
however it has some failure modes as shown in the last row.

6

By comparing different models in Table 3, MOFA requires proper initialization to map face meshes
to each image. AIGN requires depth labels to train the model. Our baseline DepthNet model neither
require any depth labels nor any manual tuning. The results also show DepthNet can work well on
unpaired data. We would also like to emphasize that MOFA and AIGN are designed to estimate a 3D
model, while DepthNet is designed to estimate the parameters that facilitate warping a face pose to
another without having depth values, so these models are designed to solve different problems.

Figure 3: Depth visualization for different models (color coded by depth). From left to right: RGB image,
Ground Truth, DepthNet, DepthNet+GAN, AIGN and MOFA estimated depth values.

An interesting observation is that GT depth gets a higher MSE compared to DepthNet. This can be
due to not having a perspective projection between source and target faces. However, since DepthNet
is trained to map to the target faces, it learns the afﬁne parameters in a way to minimize this loss.

3.3 Face Rotation, Replacement and Adversarial Repair

In this section we show how DepthNet can be used for different applications. In Figure 1 (right) we
visualize the face rotation by re-projecting a frontal face, from Multi-PIE [6], (far left) to a range of
other poses deﬁned by the faces in the row above. Since DepthNet (case B) computes transformation
on keypoints rather than pixels it is robust to illuminations changes between source and target faces.
See Figures S2 to S5 for further examples. Note that DepthNet preserves well the identity. However,
it carries forward the emotion from source to target since using a global afﬁne transformation imparts
a degree of robustness to dramatic expression changes. The views in these ﬁgures are rendered from
a 3D model in OpenGL. Note the model can align well to the target face poses.

Figure 4: Background synthesis with CycleGAN. Left to right: source face; keypoints overlaid; DepthNet (DN);
DN + background → frontal;

In another experiment, we do face frontalization with synthesized background. Here we use Cy-
cleGAN to add background detail to a face that has been frontalized with DepthNet. Referring to
Figure 4, we perform this by conditioning the CycleGAN on the DepthNet image (column 3) and the
background of column 2 (masking interior face region determined by the convex hull spanned by
the keypoints). The second domain contains ground truth frontal faces. This experiment shows how
to leverage DepthNet for full face generation. Note that we do not use identity information in this
experiment. However, it can be used to better preserve the identity.

Figure 5: Left to right: source face; target face; warp to target; repaired result.

7

Finally, we do face swaps, where we warp the face of one identity onto the geometry and background
of another identity using DepthNet. To do so, we paste the rotated face by DepthNet onto the
background of the target image and train a CycleGAN to map from the domain of ‘swapped in faces’
to the ground truth faces in our dataset, effectively learning to clean up face swaps so that the face
region matches the hair and background. Some examples of this procedure are shown in Figure 5.

4 Related Work

4.1

3D Transformation on Faces

While there is a large body of literature on 3D facial analysis, many standard techniques are not
applicable to our setting here. As an example, morphable models [1] cover a wide variety of
approaches which are capable of high quality 3D reconstructions, but such methods usually require
3D face scans or reconstructions from multi-view stereo to be assembled so as to learn complex
parametric distributions over face shapes. A close approach to our own is that of [7] on viewing
real world faces in 3D. Similar to our work, this approach does not require aligned 3D face scans,
highly engineered models or manual interventions. They make the observation that if 2D keypoints
can be obtained from a single input image of a face and these keypoints are matched to an arbitrary
3D target geometry, then standard camera calibration techniques can be used to estimate plausible
intrinsics and extrinsics of the camera. This allows the estimated camera matrix, 3D rotation matrix
and 3D translation vector to be used to transform the target 3D model to the pose of the query image
from which an approximate depth can be obtained. Hassner et. al [8] explore the use of a single
unmodiﬁed 3D surface as an approximation to the shape of all input faces. In contrast, our approach
only requires 2D keypoints from the source and target faces as input. It then estimates the depth of
the source face keypoints, thereby inferring an image speciﬁc 3D model of the face.

DeepFace [19] uses face frontalization to improve the performance of a face veriﬁcation system. It
uses a 3D mask composed of facial keypoints, detects the corresponding locations of these keypoints
in the image, and maps the 2D keypoints onto a 3D face model to frontalize it. DeepFace, however,
maps to a template 3D face, therefore always mapping to a speciﬁc pose and geometry. DepthNet, on
the other hand, can map to any pose and geometry, giving it more expressive ﬂexibility.

4.2 Generative Adversarial Networks on Face Rotation

Recently, adversarial models in [12; 22; 25; 27; 18; 28] have explored face rotation. TP-GAN [12]
performs face frontalization through introducing several losses to preserve identity and symmetry of
the frontalized faces. PIM [28] frontalizes faces in a composed adversarial loss and then extracts pose
invariant features for face recognition. These models are mainly aimed for face veriﬁcation, where
they can only do face-frontalization. Another limitation of these models is in requiring ground truth
frontal images of the same identity during training. DR-GAN [22] rotates faces to any target pose by
using a discriminator that also does identity classiﬁcation in addition to pose prediction, to preserve
id and pose. While these models do pure face rotation of a 2D face, our model can warp the input
face to any other target face, allowing warping the input face to any other identity, with a different
geometry and pose. Moreover, our model also estimates the 3D geometric afﬁne transformation
parameters explicitly, allowing these parameters to be used later, e.g., for face texture swap.

FF-GAN [25], DA-GAN [27], and FaceID-GAN [18] estimate parameters of either a 3D Morphable
Model (3DMM), as in [25; 18], or source to target pose transformation, as in [27]. FF-GAN uses
3DMM parameters to frontalize faces in an adversarial approach, while FaceID-GAN uses the
3DMM parameters to generate any target pose. These models, however, train 3DMM on ground
truth labels such as identity, expression and pose. DepthNet, on the other hand, estimates depth and
afﬁne transformation parameters without requiring ground truth afﬁne or depth labels or pre-training.
Similar to DepthNet, DA-GAN [27] estimates parameters of an afﬁne transformation model that
maps a 2D face to a 3D face. Unlike DepthNet that estimates depth on the source face, DA-GAN
uses depth in a template target face. While their approach eliminates the need for depth estimation, it
only allows the source face to be mapped to the target template geometry, while DepthNet can map
the source face to any target geometry, provided by a target image, or its keypoints. We demonstrate
the application of this ﬂexibility for the face replacement task.

The aforementioned adversarial models use an identity preserving loss to maintain identity. The core
DepthNet model does not need identity labels and preserves well the identity (as shown in Figure 1

8

(right)). However, the identity information can be used by the proposed adversarial components, as in
background synthesis, to further improve the results. Unlike some of these models that take target
pose as input, DepthNet uses the target keypoints to estimate the target geometry and does not require
the target pose. This has several advantages; 1) DepthNet can map to the geometry of the target face
in addition to the pose, and 2) in the face replacement task, DepthNet can replace the target face with
the warped source face directly onto the target face location. Its application is shown in the face swap
experiment in Section 3.3.

4.3 Depth Estimation

Thewlis et. al [21] propose a mapping technique to learn a proxy of 2D landmarks in an unsupervised
way. A semi-supervised technique has been also proposed in [11] that improves landmark localization
by using weaker class labels (e.g. emotion or pose) and also by making the model predict equivariant
variations of landmarks when such transformations are applied to the image. Similar to these
approaches, DepthNet also maps a source to a target to learn its parameters. However, unlike these
two approaches that estimate 2D landmarks, DepthNet estimates the depth of the landmarks using
2D matching of keypoints, by formulating afﬁne parameters as a function of depth augmentated
keypoints in a closed form solution.

While several models [13; 2; 15] estimate depth with direct supervision, there has been recent models
[29; 5; 3] that estimate depth in an unsupervised training procedure. These models rely on pixel
reconstruction by using frames that are captured from very similar scenes, e.g. nearby frames of
a video [29] or left-right frames captures by stereo cameras [5; 3]. These models estimate depth
on one frame and then by using the disparity map, measure how pixel values of nearby frames
compare to each other. To do this, they also require camera intrinsic parameters, e.g. focal length or
distance between cameras. Unlike these models, our approach does not require source to target pixel
mapping. This allows mapping faces from different people with completely different skin colors,
without knowing camera parameters or how they are positioned with respect to each other. Therefore,
DepthNet is not susceptible to variations in illuminations or lighting between source and target faces.

Tung et. al [23] estimate 3D human pose in videos, where they use synthetic data to pre-train internal
parameters of the model and ﬁne-tune them by keypoint, segmentation and motion loss. Adversarial
Inverse Graphics Networks (AIGN) [24] estimates 3D human pose from 2D keypoint heatmaps in a
semi-supervised manner with a similar formulation to that of CycleGAN. They apply an adversarial
loss on the 3D pose to make them look realistic. These models leverage the depth values either
through synthetic data [23], or by adversarial usage of ground truth depth values [24]. Unlike these
models, DepthNet does not rely on any depth signal, either directly or indirectly. MOFA [20] builds
a 3D face mesh using a single image, where the 3D face parameters such as 3D shape and skin
reﬂectance are estimated by an encoder and then using a differentiable model they are rendered back
to the image by the decoder. This model requires manual initialization to map the input image to the
3D mesh, since otherwise it is doing an unconstrained optimization by adapting both the face pose
and the skin reﬂectance. Our model, however, does not require any manual initialization.

5 Conclusion

We have proposed a novel approach to 3D face model creation which enables pose normalization
without using any ground truth depth data. We achieve our best quantitative keypoint registration
results using our novel formulation for predicting depth and 3D visual geometry simultaneously,
learned by backpropagating through the analytic solution for the visual geometry estimation problem
expressed as a function of predicted depths. We have illustrated the quality and utility of the depths
and 3D transformations obtained using our method by transforming source faces to a wide variety of
target poses and geometries. Our technique can be used for face rotation and replacement and when
combined with adversarial repair it can blend warped faces to also synthesize the background. The
proposed model, however, carries forward emotion from source to target due to learning a shared
afﬁne parameters for all keypoints. Moreover, for extreme non-frontal faces, while DepthNet can
extract the transformation params (since it only relies on keypoints), OpenGL cannot extract texture
due to occlusion. We show an example of how to address this in the supplementary material. An
interesting extension to this paper can be replacing the OpenGL pipeline with a generative adversarial
framework that synthesizes a face using the parameters estimated by DepthNet.

9

6 Acknowledgments

We would like to thank Samsung and Google for partially funding this project. We are also thankful
to Compute Canada and Calcul Quebec for providing computational resources, and to Poonam Goyal
for helpful discussions.

References

[1] Blanz, Volker and Vetter, Thomas. A morphable model for the synthesis of 3d faces.

In
Proceedings of the 26th annual conference on Computer graphics and interactive techniques,
pp. 187–194. ACM Press/Addison-Wesley Publishing Co., 1999.

[2] Eigen, David, Puhrsch, Christian, and Fergus, Rob. Depth map prediction from a single image
using a multi-scale deep network. In Advances in Neural Information Processing Systems
(NIPS), pp. 2366–2374, 2014.

[3] Garg, Ravi, BG, Vijay Kumar, Carneiro, Gustavo, and Reid, Ian. Unsupervised cnn for single
view depth estimation: Geometry to the rescue. In Proceedings of the European Conference on
Computer Vision (ECCV), pp. 740–756. Springer, 2016.

[4] Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty of training deep feedforward
neural networks. In International conference on Artiﬁcial Intelligence and Statistics (AISTATS),
pp. 249–256, 2010.

[5] Godard, Clément, Mac Aodha, Oisin, and Brostow, Gabriel J. Unsupervised monocular depth
estimation with left-right consistency. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2017.

[6] Gross, Ralph, Matthews, Iain, Cohn, Jeffrey, Kanade, Takeo, and Baker, Simon. Multi-pie.

Image and Vision Computing, 28(5):807–813, 2010.

[7] Hassner, Tal. Viewing real-world faces in 3d.

In Proceedings of the IEEE International

Conference on Computer Vision (ICCV), pp. 3607–3614, 2013.

[8] Hassner, Tal, Harel, Shai, Paz, Eran, and Enbar, Roee. Effective face frontalization in uncon-
strained images. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 4295–4304, 2015.

[9] He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE
International Conference on Computer Vision (ICCV), pp. 1026–1034, 2015.

[10] Honari, Sina, Yosinski, Jason, Vincent, Pascal, and Pal, Christopher. Recombinator networks:
Learning coarse-to-ﬁne feature aggregation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pp. 5743–5752, 2016.

[11] Honari, Sina, Molchanov, Pavlo, Tyree, Stephen, Vincent, Pascal, Pal, Christopher, and Kautz,
Jan. Improving landmark localization with semi-supervised learning. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.

[12] Huang, Rui, Zhang, Shu, Li, Tianyu, and He, Ran. Beyond face rotation: Global and local
perception gan for photorealistic and identity preserving frontal view synthesis. In Proceedings
of the IEEE International Conference on Computer Vision (ICCV), Oct 2017.

[13] Jackson, Aaron S, Bulat, Adrian, Argyriou, Vasileios, and Tzimiropoulos, Georgios. Large pose
3d face reconstruction from a single image via direct volumetric cnn regression. In Proceedings
of the IEEE International Conference on Computer Vision (ICCV), pp. 1031–1039. IEEE, 2017.

[14] Jeni, László A, Cohn, Jeffrey F, and Kanade, Takeo. Dense 3d face alignment from 2d videos in
real-time. In IEEE International Conference and Workshops on Automatic Face and Gesture
Recognition (FG), volume 1, pp. 1–8. IEEE, 2015.

10

[15] Liu, Fayao, Shen, Chunhua, and Lin, Guosheng. Deep convolutional neural ﬁelds for depth
estimation from a single image. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 5162–5170, 2015.

[16] Parkhi, Omkar M, Vedaldi, Andrea, and Zisserman, Andrew. Deep face recognition. Proceedings

of the British Machine Vision Conference (BMVC), 2015.

[17] Sagonas, Christos, Tzimiropoulos, Georgios, Zafeiriou, Stefanos, and Pantic, Maja. 300 faces
in-the-wild challenge: The ﬁrst facial landmark localization challenge. In Proceedings of the
IEEE International Conference on Computer Vision Workshops (CVPRW), pp. 397–403, 2013.

[18] Shen, Yujun, Luo, Ping, Yan, Junjie, Wang, Xiaogang, and Tang, Xiaoou. Faceid-gan: Learning
a symmetry three-player gan for identity-preserving face synthesis. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 821–830, 2018.

[19] Taigman, Yaniv, Yang, Ming, Ranzato, Marc’Aurelio, and Wolf, Lior. Deepface: Closing the
gap to human-level performance in face veriﬁcation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 1701–1708, 2014.

[20] Tewari, Ayush, Zollhöfer, Michael, Kim, Hyeongwoo, Garrido, Pablo, Bernard, Florian, Perez,
Patrick, and Theobalt, Christian. Mofa: Model-based deep convolutional face autoencoder for
unsupervised monocular reconstruction. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), volume 2, 2017.

[21] Thewlis, James, Bilen, Hakan, and Vedaldi, Andrea. Unsupervised learning of object landmarks
by factorized spatial embeddings. In Proceedings of the IEEE International Conference on
Computer Vision (ICCV), volume 1, pp. 5, 2017.

[22] Tran, Luan, Yin, Xi, and Liu, Xiaoming. Disentangled representation learning gan for pose-
invariant face recognition. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017.

[23] Tung, Hsiao-Yu, Tung, Hsiao-Wei, Yumer, Ersin, and Fragkiadaki, Katerina. Self-supervised
learning of motion capture. In Advances in Neural Information Processing Systems (NIPS), pp.
5242–5252, 2017.

[24] Tung, Hsiao-Yu Fish, Harley, Adam W, Seto, William, and Fragkiadaki, Katerina. Adversarial
inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation from
unpaired supervision. In Proceedings of the IEEE International Conference on Computer Vision
(ICCV), volume 2, 2017.

[25] Yin, Xi, Yu, Xiang, Sohn, Kihyuk, Liu, Xiaoming, and Chandraker, Manmohan. Towards
large-pose face frontalization in the wild. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), pp. 1–10, 2017.

[26] Zhang, Xing, Yin, Lijun, Cohn, Jeffrey F, Canavan, Shaun, Reale, Michael, Horowitz, Andy,
Liu, Peng, and Girard, Jeffrey M. Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic
facial expression database. Image and Vision Computing, 32(10):692–706, 2014.

[27] Zhao, Jian, Xiong, Lin, Jayashree, Panasonic Karlekar, Li, Jianshu, Zhao, Fang, Wang, Zhecan,
Pranata, Panasonic Sugiri, Shen, Panasonic Shengmei, Yan, Shuicheng, and Feng, Jiashi. Dual-
agent gans for photorealistic and identity preserving proﬁle face synthesis. In Advances in
Neural Information Processing Systems (NIPS), pp. 66–76, 2017.

[28] Zhao, Jian, Cheng, Yu, Xu, Yan, Xiong, Lin, Li, Jianshu, Zhao, Fang, Jayashree, Karlekar,
Pranata, Sugiri, Shen, Shengmei, Xing, Junliang, et al. Towards pose invariant face recognition
in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 2207–2216, 2018.

[29] Zhou, Tinghui, Brown, Matthew, Snavely, Noah, and Lowe, David G. Unsupervised learning of
depth and ego-motion from video. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2017.

[30] Zhu, Jun-Yan, Park, Taesung, Isola, Phillip, and Efros, Alexei A. Unpaired image-to-image
translation using cycle-consistent adversarial networks. In Proceedings of the IEEE International
Conference on Computer Vision (ICCV), 2017.

11

Unsupervised Depth Estimation, 3D Face Rotation and
Replacement: Supplementary Information

S.1 Experimental Setup

The RCN, the DepthNets and the CycleGAN modules are trained separately. Each model is trained
using standard techniques for the model class and has a separate objective to be optimized. DepthNet
does not use the OpenGL pipeline during training and only uses it to render faces at test time, allowing
DepthNet to train faster.

S.1.1 DepthNet Experimental Details

Here we describe the details of the experiments carried out in Section 3.1. Our DepthNet architectures
require keypoints of both source and target images to be extracted. For this, the image is ﬁrst passed
through the VGG-Face [16] face detector. The face crops are then scaled down to 80 × 80 and
converted to greyscale, following which they are passed through RCN to obtain N = 68 keypoints
on each image. The RCN is trained exactly as described in [10], using the 300W dataset [17].

The keypoint only variant of our model involves concatenating all detected keypoints and passing
them through a two-layer deep fully connected network, with 256 hidden units and o output units.
The size of o depends on whether we are predicting only the depth, in which case o = N , or both
depth and afﬁne transformation parameters, in which case o = N + 8.

As discussed above, it is possible to augment these models with a Siamese CNN module (case A). In
the model variants that also use the image, we pass both the source and the target images through
three conv-maxpool layers with shared weights of size (32, 4, 2), (48, 3, 2), (64, 2, 2), respectively for
the representation (num_filters, filter_size, pool_size). The network’s outputs for the
source and target faces are then concatenated before passing them into a 4-layered fully connected
network with respective output sizes of 2048, 512, 256, and o. The keypoints are concatenated to the
512-unit layer before being passed to the last two layers. See Figure 1 (left) for an illustration of the
model. We explore these Siamese CNN augmented variants in models 4, 6 and 8 in Table 1.

We set the initial learning rate to 0.001 and use a Nesterov momentum optimizer (with a momentum
of 0.9) in all our experiments. With the exception of the last layer, we initialize all weights with
a Glorot initialization scheme [4], with the weights sampled from a uniform distribution. We use
a ReLU gain [9], set all biases to 0, and apply a ReLU non-linearity after every layer. In the ﬁnal
output layer, we do not apply any non-linearity and initialize the weights to 0. The biases of units that
represent depths are initialized to a random Normal distribution with µ = 0 and σ = 0.5, while those
that form the predicted afﬁne transform are initialized with the equivalent of a "ﬂattened" identity
transform. All models have been trained for 500 epochs.

We point out that except for a comparison between learning rates in the set {0.01, 0.001, 0.0001}
over few (less than 10) epochs, to ﬁnd a learning rate that the model seems to train well with, we
have not performed a hyperparameter search, and anticipate that the performance of the model can be
made even better by searching the hyperparameter space on a per model basis and by using deeper
(or modiﬁed) architectures.

S.2 Depth Visualization

We show further estimated depth values by different models in Figure S1. DepthNet+GAN and AIGN
generate the closest depth values to the GT depth. The baseline DepthNet model estimates reliable
depth values for most cases, however it has some degree of inaccuracy, as shown in the last two rows.
In DepthNet, the estimated depth indicates the position of each keypoint relative to other keypoints
rather than with respect to a source and importantly it is done without any supervision. MOFA, which
is also unsupervised, generates very similar face templates for different cases.

12

Figure S1: Depth visualization for different models (color coded by depth). The Depth axis is the one pointing
into the page. From left to right: RGB image, Ground Truth, DepthNet, DepthNet+GAN, AIGN and MOFA
estimated depth values.

S.3 Additional Camera Sweep Visualizations

In this section, we present additional visualizations along the lines of those shown in Figure 1 (right)
in Section 2 of the main paper. Frontal faces selected from the Multi-PIE dataset [6] are re-projected
to match several other poses corresponding to a person with a different identity. The DepthNets
predicts reliable proxy depths, which when coupled with the analytically obtained afﬁne transform
(obtained from the least-squares pseudo-inverse-based solution described in Section 2.3 of the main
paper) yields faces close to the desired target face geometry when passed through the OpenGL
pipeline.

We show camera sweeps for frontal source faces in Figures S2 and S3 and non-frontal source faces in
Figure S4. In Figure S5 we use the same target identity as the source face, showing how much the

13

Figure S2: Projecting a frontal face (far left) to a range of other poses deﬁned by faces in the row above.

generated face differs from the ground truth target. For all samples in Figures S2, S3, S4, and S5 we
use the DepthNet model that relies on only key-points (model 7 in Table 1).

Note that for non-frontal source faces, the quality of images is reduced specially for frontal target
faces. This is due to lack of adequate texture on the occluded side of the face to be transferred to the
target pose by OpenGL pipeline, rather than inaccuracies in the afﬁne transformation parameters. In
order to reduce the side-affects, we use either of the source face or its ﬂipped version, that is closer to
the target face pose, and then warp the face to the target keypoints.

Figure S3: Projecting a frontal face (far left) to a range of other poses deﬁned by faces in the row above.

Figure S4: Re-projecting a non-frontal face (far left) to a range of other poses deﬁned by faces in the row above.

Figure S5: Rotating a face (far left) to a range of other poses deﬁned by faces of the same identity in the row
above. On the top row frontal and on the bottom row non-frontal source faces are shown.

S.4 CycleGAN

Suppose we have some images belonging to one of two sets x ∈ X and y ∈ Y , where x denotes a
DepthNet-resulting face and y a ground truth face which is frontal. We wish to learn two functions

14

(cid:105)

(cid:105)

,

(cid:104)

Ex,y

min
G,F

(cid:104)

Ex,y

min
DX ,DY

F : X → Y and G : Y → X which are able to map an image from one set to the corresponding
image in the other. Correspondingly, we have two discriminators DX and DY which try to detect
whether the image in that particular set is real or generated. While we are only interested in the
function F : X → Y (since this is mapping to the distribution of ground truth faces) the formulation
of CycleGAN requires that we learn mappings in both directions during training. We optimize the
following objectives for the two generators F and G:

(cid:96)(DX (G(y)), 1) + (cid:96)(DY (F (x)), 1) + λ||y − F (G(y))||1 + λ||x − G(F (x))||1

(4)

And the following for the two discriminators DX and DY :

(cid:96)(DX (x), 1) + (cid:96)(DX (G(y)), 0) + (cid:96)(DY (y), 1) + (cid:96)(DY (F (x)), 0)

(5)

where 0/1 denote fake/real, (cid:96)() is the squared error loss and λ is a coefﬁcient for the cycle-consistency
(reconstruction) loss.

In the case where we did adversarial background synthesis, x is a channel-wise concatenation of the
DepthNet-frontalized face and the background of the original (pre-frontalized) image. For face-swap
cleanup, x is simply a source face which has been warped to a target face and pasted on top.

Once the network has been trained, we can disregard all other functions and use F to clean up faces
which are low quality due to artifacts from warping.

In terms of architectural details the generators and discriminators used were those described in the ap-
pendix of the CycleGAN paper [30]. In short, the generator consists of three conv-BN-relu blocks
which downsample the input, followed by nine ResNet blocks (which can be interpreted as iteratively
performing transformations over the downsampled representation), followed by deconv-BN-relu
blocks to upsample the representation back into the original input size. For training, we use the
same hyperparameters as most CycleGAN implementations which is using the Adam optimizer with
learning rate α = 2 × 10−4, β1 = 0.5, β2 = 0.999. However, instead of using a batch size of 1 we
use the largest possible batch size, which was 16 for a 12GB GPU.

Note that in order to produce better translations, the dataset we used for all CycleGAN experiments
contain both the VGG and the CelebA datasets, which has signiﬁcantly more images.

S.4.1 Background Synthesis

We present extra visualizations for the CycleGAN which performs background synthesis on CelebA,
corresponding to Figure 4 in the main paper. These are shown in Figure S6.

Figure S6: Background synthesis with CycleGAN. Left to right: source face; keypoints overlaid; DepthNet
(DN); DN + background → frontal

S.4.2 Face Replacement and Adversarial Repair

In Figure S7 we provide extra face swap samples on CelebA, corresponding to Figure 5 in the
main paper, where a source face is warped to a target face pose using DepthNet, pasted onto the

15

target image and then passed to a CycleGAN to adapt the face skin of the warped source face to the
background and hairstyle of the target face.

Figure S7: Face swap experiment with CycleGAN. Left to right: source face; target face; warp to target with
DepthNet; repaired result with CycleGAN. The source face is taken and warped onto the target face. The
background and hairstyle is then adapted to the target face.

S.4.3 Extreme Pose Face Clean-up

If the source image has an extreme pose, the texture will be missing on the occluded side of the
face and the OpenGL pipeline cannot rotate the face without artifacts. Note that this shortcoming is
due to lack of texture on the occluded side of the face rather than a deﬁciency of the transformation
parameters measured by DepthNet.

We performed an experiment using CycleGAN to ﬁx such artifacts. For this experiment we take
source images from CelebA and ﬁrst frontalize it by using DepthNet. Since the frontalized faces have
artifacts due to stretch of texture on the occluded side of the face by the OpenGL pipeline, we train a
CycleGAN that takes DepthNet frontalizaed faces plus the background of the original non-frontal
image (as two images) in one domain and the ground truth frontal faces in the other domain. The
CycleGAN learns to clean-up these artifacts. Finally, we take the GAN-repaired frontalized faces
and project it to different target poses using DepthNet. In Figure S8 we visualize camera sweep for
source faces in the wild that have extreme poses. The cycleGAN reasonably cleans the face artifacts
and then DepthNet projects it to different poses. This is just one approach to address the extreme
pose occlusion artifacts. We see alternative methods for addressing this issue as promising directions
for future research.

Figure S8: Re-projecting a non-frontal face (far left) from CelebA to a range of other poses deﬁned by faces in
the row above. Top row (in each pair) depicts the target faces from Multi-PIE [6]. The bottom row shows from
left to right: source face, souce face frontalized by DepthNet, adversarial-repaired face, the repaired source face
projected to the target poses (4th to 10th columns).

16

8
1
0
2
 
c
e
D
 
4
2
 
 
]

V
C
.
s
c
[
 
 
5
v
2
0
2
9
0
.
3
0
8
1
:
v
i
X
r
a

Unsupervised Depth Estimation,
3D Face Rotation and Replacement

Joel Ruben Antony Moniz1∗, Christopher Beckham2,3∗, Simon Rajotte2,3,
Sina Honari2, Christopher Pal2,3,4
1Carnegie Mellon University, 2Mila-University of Montreal, 3Polytechnique Montreal, 4Element AI
1jrmoniz@andrew.cmu.edu, 2honaris@iro.umontreal.ca, 3firstname.lastname@polymtl.ca

Abstract

We present an unsupervised approach for learning to estimate three dimensional
(3D) facial structure from a single image while also predicting 3D viewpoint
transformations that match a desired pose and facial geometry. We achieve this
by inferring the depth of facial keypoints of an input image in an unsupervised
manner, without using any form of ground-truth depth information. We show how
it is possible to use these depths as intermediate computations within a new back-
propable loss to predict the parameters of a 3D afﬁne transformation matrix that
maps inferred 3D keypoints of an input face to the corresponding 2D keypoints on
a desired target facial geometry or pose. Our resulting approach, called DepthNets,
can therefore be used to infer plausible 3D transformations from one face pose
to another, allowing faces to be frontalized, transformed into 3D models or even
warped to another pose and facial geometry. Lastly, we identify certain shortcom-
ings with our formulation, and explore adversarial image translation techniques as
a post-processing step to re-synthesize complete head shots for faces re-targeted to
different poses or identities. 1

1

Introduction

Face rotation is an important task in computer vision.
It has been used to frontalize faces for
veriﬁcation [8; 19; 25; 28] or to generate faces of arbitrary poses [22; 18]. In this paper we present
a novel unsupervised learning technique for face rotation and warping from a 2D source image –
whose facial appearance will be used in the rotation – to a target face – to which the facial pose
and geometry inferred from the source image is mapped. A use case is when we have an image of
someone in a particular target pose and we want to put a given source face into that pose, without
knowing the exact target face pose. This can be leveraged, for example, in the advertisement industry,
when putting someone in a particular location can be costly or unfeasible, or in the movie industry
when the main actor’s limited time or high cost can enforce using another actor whose face can be
later replaced by the main actor’s. This is achieved through estimating the source face depth and
the 3D afﬁne parameters that warp the source to the target face using neural networks. These neural
networks use a novel loss formulation for the structured prediction of keypoint depths. Once the 3D
afﬁne transformation matrix is estimated, it can be used to warp the source image onto the target
face geometry using a textured triangular mesh. The use of a 3D afﬁne transform means that we
can capture both a 3D rotation of the face to a new viewpoint as well as a global non-Euclidean
warping of the geometry to match a target face. We call these neural networks Depth Estimation-Pose
Transformation Hybrid Networks, or DepthNets in short.

Our ﬁrst contribution is to propose a neural architecture that predicts both the depth of source
keypoints as well as the parameters of a 3D geometric afﬁne transformation which constitute the

∗Indicates equal contribution.
1Code will be released at: https://github.com/joelmoniz/DepthNets/

32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.

explicit outputs of the DepthNet model. The predicted depth and afﬁne transformation could be then
used to map a source face to a target face for object orientation, distortion and viewpoint changes.

Our second contribution consists of making the observation that given 3D source and 2D target
keypoints, closed form least squares solutions exist for estimating geometric afﬁne transformation
models between these sets of keypoint correspondences, and we can therefore develop a model that
captures the dependency between depth and the afﬁne transformation parameters. More speciﬁ-
cally, we express the afﬁne transformation as a function of the pseudoinverse transformation of 2D
keypoints in a source image – augmented by inferred depths – and the target keypoints. Thus, the
second and major contribution in this work is capturing the relationship between an estimated afﬁne
transformation and the inferred depth as a deterministic relationship. In this formulation, DepthNet
only predicts depth values explicitly and the afﬁne parameters are inferred through a pseudoinverse
transformation of source and target keypoints. Here, one can directly optimize through the solutions
of what might otherwise be formulated as a secondary minimization step.

Our proposed DepthNet can map the central region of the source face to the target geometry. This
leads to background mismatch when warping one face to another. Finally, our third contribution is to
use an adversarial unpaired image-to-image transformation approach to repair the appearance of 3D
models inferred from DepthNet. Together these contributions allow 3D models of faces that construct
realistic images in the target pose. Our proposed method can be used for pose normalization or face
swaps with no manually speciﬁed 3D face model. To the best of our knowledge, this is the ﬁrst such
neural network based model that estimates a 3D afﬁne transformation model for face rotation which
neither requires ground-truth 3D images nor any ground truth 3D face information such as depth.

As we have outlined above, our approach uses neural networks for inferring depth and geometric
transformation – referred to as DepthNets; and, an adversarial image-to-image transformation network
which improves the quality of the appearance of a 3D model inferred from a DepthNet.

2 Our Approach

DepthNets

We propose three DepthNet formulations, described in Sections 2.1, 2.2, and 2.3. For each of the
three models we explore two architectural scenarios: (A) a Siamese-like architecture that uses the
source and target images themselves as well as keypoints extracted from these images, and (B) a
fully-connected neural network variant which uses only facial keypoints in the source and target
images. See Figure 1 (left) for details.

Figure 1: (Left) DepthNet architecture. The blue region is only used in case (A) and the red part is used in
both cases (A) and (B), described in Section 2. The orange output (the 8 afﬁne transformation parameters) is
predicted only by model variations described in Sections 2.1 and 2.2, and not the model described in section
2.3. All three models predict the N depth values of the source keypoints. C, P, and FC correspond to valid conv,
pool and fully-connected layers. The two paths of Siamese network share parameters and the black dots indicate
concatenating keypoint values to FC units. (Right) Visualizing face rotation by re-projecting a frontal face (far
left) to a range of other poses deﬁned by the faces in the row above (in each pair of rows). In this experiment, we
only use keypoints from the top-row in the DepthNet model (Model 7 in Table 1).

It is interesting to note that if DepthNets are used to register a set of images of objects to the same
common viewpoint, the same image and geometry can be used as the target. This is the case for
the frontalization of faces, for example. While the DepthNet framework is sufﬁciently general to be
applied to any object type where 2D keypoint detections have been made, our experiments here focus
on faces. We describe the three variants of DepthNets below.

2

2.1 Predicting Depth and Viewpoint Separately

In this variant of DepthNets, the model predicts both depths and viewpoint geometry, but as separate
explicit outputs of a neural network. The input is comprised of only the geometry and pose of the
source and target faces (encoded in the form of a 2D keypoint template), in case (B), or both keypoints
and images of the source and target faces, in case (A). The key phases of this stage are described by
the sequence of steps given below:

1. Keypoint extraction: Raw (x, y) pixel coordinates corresponding to the keypoints of each image
are extracted using a Recombinator Network (RCN) [10] architecture, and then concatenated before
being passed into the keypoint processing step.
2. (Optional) Image Feature Extraction: DepthNets can be conditioned on only keypoints, case (B),
or on keypoints and the original images, case (A). We can therefore optionally subject the source and
target images to alternating conv-maxpool layers. If this component of the architecture is used, the
last spatial feature maps in the Siamese architecture are concatenated before being given to a set of
densely connected hidden layers.
3. Keypoint processing: In this step keypoints are passed through a set of hidden layers. If the Image
Feature Extraction stage is used, the keypoints are concatenated to image features, the output of
which is in turn fed to densely connected layers. The output layer of this phase will be of size N + 8,
where N is the number of keypoints. The ﬁrst N points represent the Depth proxy, and the last 8
points form a 4 × 2 matrix representing the learned parameters of the afﬁne transform. See Figure 1.
4. Geometric Afﬁne Transformation Normalizer: This phase applies the predicted afﬁne transform
on each (depth augmented) source keypoint to estimate its target location. Let (xi
s) represent the
ith source keypoint, (xi
n) the corresponding source normalized keypoint estimated by applying
the afﬁne transformation matrix, (xi
t) the ith target keypoint (as ground truth (GT)), and Is and It
represent the source and target images respectively. Depending on which underlying architectural
variant we use, two cases arise: one that utilizes only the keypoints (B), and another utilizing
both the keypoints and the images (A). Since the keypoints are generated using RCNs, they are
technically functions of the input images: [xs, ys] = R(Is), and [xt, yt] = R(It). Depending
on the (A) or (B) variant, the ith keypoint’s predicted depth proxy zi
p is inferred as a function
of the input keypoints, or both input keypoints and input images.
In both cases the keypoints
are derived from the images, so zi
p(Is, It). Similarly, the 3D-2D afﬁne transform F is a
function of the images, such that F = F(Is, It), where the 8 predicted parameters are: m =
{m1, m2, m3, tx, m4, m5, m6, ty)}. These constitute the 3D-2D afﬁne transform which is used by
all keypoints. In other words, each of the i points is transformed using xi
s, or:

n = F(Is, It) xi

p = zi

n, yi

s, yi

t, yi

(cid:20) xi
n
yi
n

(cid:21)

=

(cid:20)m1 m2 m3
m4 m5 m6






(cid:21)

tx
ty

xi
s
yi
s
zi
p(Is, It)
1






The loss function of a DepthNet is obtained by transforming the source face to match the target face
using the simple squared error of the corresponding target object’s keypoint vector xt = [xt, yt]T
, as GT values, and the source object’s normalized keypoint vector [xn, yn]T . The loss for one
example where we predict depth and afﬁne viewpoint geometry can therefore be expressed as:

L =

K
(cid:88)

i=1

(cid:13)
(cid:13)xi
(cid:13)

t − F(Is, It) [xi

s yi

s zi

p(Is, It)]T (cid:13)
2
(cid:13)
(cid:13)

(1)

5. Image Warper: This phase consists of using the depth proxy and afﬁne transform matrix generated
to actually warp the face from its source pose to be matched to the target object geometry. The ﬁnal
projection to 2D is achieved by simply dropping the transformed z coordinate (which corresponds
to an orthographic projection model). In the case of DepthNets, this orthographic projection is
effectively embedded in the Geometric Afﬁne Transformation Normalizer step, since the afﬁne
corresponding to the z coordinate is not predicted, essentially dropping it.
As we operate on keypoints, the actual warping of pixels can be performed with a high quality
OpenGL pipeline that performs the warp separately from the rest of the architecture. Source image,
keypoints augmented with depth, and the afﬁne matrix are passed to OpenGL pipeline to warp the
source image towards the target pose. This OpenGL warping is not needed during DepthNet training,
which means we do not have to do feedforward or backprop through OpenGL. In Summary, for step 1
the RCN model [10] is used, for steps 2 to 4 the DepthNet model, shown in Figure 1 (left), is trained,

3

and for step 5 an OpenGL pipeline is used. No data or parameters are needed to train the OpenGL
pipeline. It warps images by directly using the provided data.

2.2 Estimating Viewpoint Geometry as a Second Step

In this model variant, training is similar to Section 2.1 and the model outputs depth and 3D
afﬁne transformation parameters. However, at test time, rather than using the predicted 3D afﬁne
transformation for pairs of faces, we use only the predicted depths and estimate the afﬁne ge-
ometry parameters as a second estimation step. More precisely, given 3D points for a scene
and the corresponding 2D points for a target geometry it is possible to formulate the estima-
tion of a 3D afﬁne transformation as a linear least squares estimation problem. An overdeter-
mined system of the form Am = xt for this problem can be constructed as shown in (2).

This corresponds to an afﬁne camera model
followed by an orthographic projection to
2D keypoints. This setup also leads to
the following closed form solution for the
afﬁne transformation parameters:

m = [AT A]−1AT xt,

(3)

y1
s
0
y2
s
0

z1
s
0
z2
s
0

x1
s
0
x2
s
0













xK
s
0

yK
s
0

zK
s
0

0
x1
s
0
x2
s

0
xK
s

0
y1
s
0
y2
s
...
0
yK
s

0
z1
s
0
z2
s

1 0
0 1
1 0
0 1

0
zK
s

1 0
0 1





































m1
m2
m3
m4
m5
m6
tx
ty

























x1
t
y1
t
x2
t
y2
t
...
xK
t
yK
t

=

(2)

where this pseudoinverse based transformation is parameterized by the reference points and their
predicted depths.

2.3

Joint Viewpoint and Depth Prediction

Our key observation is that one can alternatively use the closed form analytical solution, measured in
Eq. (3), for the least squares estimation problem as the underlying afﬁne transformation matrix within
the loss function. This leads to a special form of structured prediction problem for geometrically
consistent depths and afﬁne transformation matrix. For each image we have L =

K
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:21)

(cid:20) xi
t
yi
t
(cid:124) (cid:123)(cid:122) (cid:125)
xi
t

(cid:20)m1 m2 m3
m4 m5 m6

−

(cid:124)

(cid:123)(cid:122)
m

tx
ty

(cid:21)

(cid:125)







(cid:124)

xi
s
yi
s
zi
p(Is, It)
1
(cid:123)(cid:122)
xi
s







(cid:125)

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

K
(cid:88)

i=1

(cid:13)
(cid:13)xi

t − reshape(cid:2)[AT A]−1AT xt

(cid:3)xi

s

(cid:13)
2
(cid:13)

where the matrix A is parameterized as a function of xs as shown in Eq. (2). In this variant, the
model explicitly outputs only depth values during train and test time. The afﬁne transformation matrix
in the equation above is replaced by Eq. (3), which measures the afﬁne transformation as a pure
function of source and target keypoints plus the inferred depth. The big difference of this formulation
compared to Sections 2.1 and 2.2 is that geometric afﬁne transformation parameters are no longer
predicted by DepthNet during training and at both train and test time – it solves the least square loss
through the pseudoinverse based transformation. Since zi
t }j=1...N ) is predicted
within the analytical formulation of the solution to the least squares minimization problem, we can
backpropagate through the solution of a minimization problem that depends on the predicted depths.
While we leverage keypoints for depth estimation, the proposed approach is novel in how the depth is
estimated. Note that it is unsupervised with respect to depth labels. No depth supervision either by
using depth targets (as in [2; 13; 15]), or by using depth in an adversarial setting (as in [24]), is used
to estimate depth values for the base DepthNet models described in Sections 2.1, 2.2, and 2.3.

s = zi

p({xj

s, xj

t , yj

s, yj

The depths learned for keypoints by these approaches are not necessarily true depths, but are likely to
strongly correlate with the actual depth of each keypoint. This is because even though the method
succeeds (as we shall see below) in aligning poses, the inferred depth and the afﬁne transform may
each be scaled by factors so as to cancel each other out (i.e., by factors which are multiplicative
inverses of each other). Real world viewpoint geometry also involves perspective projection.

Adversarial Image-to-Image Transformation

DepthNet transforms the central region of the source face to the target pose. Inevitably, the face
background will be missing, which might make the proposed method unsuitable for many application

4

where the full face is required. To address this issue, we utilize CycleGAN [30], an adversarial
image-to-image translation technique. This serves to repair the background of faces that have
undergone frontalization or face swap through the DepthNet pipeline. Importantly, the adversarial
nature of CycleGAN allows one to perform image transformation between two domains without
the requirement of paired data. In our work, we perform experiments translating between various
domains of interest but one example is translating between the domain of images in the dataset (i.e.
the ground truth) and the domain of images where the DepthNet output is pasted onto the face region
(in the case of face-swap). By doing so we clean the face background in an unsupervised manner.

3 Experiments

3.1 DepthNet Evaluation on Paired Faces

For the experiments in this section, we use a subset of the VGG dataset [16], with training and
validating on all possible pairs of images belonging to the same identity for 2401 identities. This
yields 322,227 train and 43,940 validation pairs. Check experimental setup details in Supplementary.

Model

Color MSE MSE_norm

1) A simple 2D afﬁne registration
2) A 3D afﬁne registration model using an average 3D face template
3) A DepthNet that separately estimates depth and geometry
4) The model above, but with a Siamese CNN image model
5) Secondary least squares estimation for visual geometry using the depths from 3)
6) Secondary least squares estimation for visual geometry using the depths from 4)
7) Backpropagation through the pseudoinverse based solution for visual geometry
8) The model above, but with a Siamese CNN image model

grey
purple
brown
violet
red
green
orange
blue

1.562
0.724
0.568
0.539
0.400
0.399
0.357
0.349

9.547
7.486
6.292
6.115
5.184
5.175
4.932
4.891

Table 1: (left) Comparing the Mean Squared Error (MSE) and MSE normalized by inter-ocular distance
(MSE_norm) of different models. (right) Histogram of Mean Squared Errors. The second column in the Table
(on left) corresponds to the color of the model in the ﬁgure (on right).

We explore the three variants of DepthNets described in Sections 2.1, 2.2, and 2.3, each with two
architectural cases (A) and (B), depending on whether image features are used in addition to keypoints
or not. We also compare with a number of baselines. We measure the mean square error (MSE)
between the estimated keypoints on the target face (source face normalized keypoints) and ground
truth target keypoints. Results for the following models are shown in Table 1:

1) A baseline model registrations using a simple 2D afﬁne transformation.

2) We generate a 3D average face template from the 3DFAW dataset [14; 26; 6] by aligning the 3D
keypoints of all faces in the dataset to a front-facing face using Procrustes superimposition. We report
error by mapping the template face to each source face via Procrustes superimposition (to get a 3D
face f ) and then use an afﬁne transformation from the 3D face f to the target face.

3, 4) We use our proposed approach to predict both depth and geometry (described in Sections 2.1).

5, 6) These models described in Section 2.2. Note that during training, these two cases are similar to
models 3 and 4 in Table 1.

7, 8) The pseudo-inverse formulation model described in Section 2.3.

As observed in Table 1, a simple 2D afﬁne transform (model 1) without estimating depth and a
template 3D face (model 2) get high errors on mapping to the target faces. DepthNet models get lower
errors and the pseudo-inverse formulation (models 7 and 8) further reduces the error by 10%. The
CNN models slightly reduce errors compared to their equivalent models that rely only on keypoints.

3.2 DepthNet Evaluation on Unpaired Faces and Comparison to other Models

In this section we train DepthNet on unpaired faces belonging to different identities and compare
with other models that estimate depth. We use the 3DFAW dataset [14; 26; 6] that contains 66
3D keypoints to facilitate comparing with ground truth (GT) depth. It provides 13,671 train and
4,500 valid images. We extract from the valid set, 75 frontal, left and right looking faces yielding
a total of 225 test images, which provides a total of 50,400 source and target pairs. We train the
psuedoinverse DepthNet model that relies on only keypoints (model 7 in Table 1). We also train
a variant of DepthNet that applies an adversarial loss on the depth values (DepthNet+GAN). This
model uses a conditional discriminator that is conditioned on 2D keypoints and discriminates GT
from estimated depth values. The model is trained with both keypoint and adversarial losses.

5

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)

Source

Target

Left
Front
Right

DepthNet

DepthNet + GAN

Left
24.67
25.54
21.66

Front Right
29.70
27.71
26.19
27.22
23.87
21.48

Avg
27.36
26.32
22.34

Left
59.78
58.77
59.97

Front Right
59.63
59.67
58.61
58.67
59.60
59.70

Avg
59.69
58.68
59.76

Table 2: Comparing DepthCorr for different DepthNet models when mapping variant source to target poses. The
Avg column measures the average over the three preceding columns.

We measure the correlation matrix between GT and estimated depths, where the element k in the
diagonal indicates the correlation between estimated and ground truth depth values for keypoint k,
yielding a value between -1 and 1. We report the sum of absolute values of the diagonal of this
matrix, indicated by DepthCorr. We compare DepthNet models on DepthCorr in Table 2. For this
experiment we take every possible pair of source to target faces, where source and target are one of
{left, front, right} looking faces. This yields a total of 5,550 pairs when the source and the target are
from the same subset, and 5,625 pairs otherwise. This experiment measures the accuracy of depth
estimation of the DepthNet models on different orientations of source-target faces. The baseline
DepthNet model that does not leverage the depth labels performs well in different cases. DepthCorr
improves more than twice for the DepthNet+GAN model, indicating a direct supervision loss using
depth labels can enhance the depth estimation.

Model

GT Depth
AIGN [24]
MOFA [20]
DepthNet (Ours)
DepthNet + GAN (Ours)

Need Depth Manual Init. MSE (×10−5) Depth Correlation Matrix Trace (DepthCorr)
Right pose
66
49.04
17.54
22.34
59.76

Front pose
66
50.81
15.97
26.32
58.68

8.86 ± 6.55
9.06 ± 6.61
8.75 ± 6.33
7.65 ± 6.97
8.74 ± 6.24

Left pose
66
44.08
11.14
27.36
59.69

-
No
Yes
No
No

Yes
Yes
No
No
Yes

Table 3: Comparing MSE and DepthCorr for different models. A lower MSE indicates the model maps better to
the target faces. A higher DepthCorr indicates more correlation between estimated and GT depths.

We compare our two DepthNet models with three baselines: 1) AIGN [24], 2) MOFA [20] and 3)
GT Depth (no model trained). AIGN estimates 3D keypoints conditioned on 2D heatmaps of the
keypoints. MOFA estimates a 3D mesh using only an image. We implemented the AIGN model
and asked the authors of MOFA to run their model on our test-set. They provided MOFA’s results
for 134 images in the test set. In Table 3 we compare these three models with our DepthNet models
on DepthCorr. We also compare them on MSE, which is measured between GT and estimated
target keypoints. Since the three baselines estimate depth on a single image due to their different
model formulation, we ﬁrst measure m using closed form solution in Eq. 3 and then apply m to
the estimated source keypoints to get the target keypoint estimations. We contrast the estimated
values with the GT target keypoints. As shown in Table 3, GT depth has the highest DepthCorr
(the maximum possible value). The depths estimated by DepthNet+GAN and AIGN have stronger
correlation to GT depth compared to the baseline DepthNet and MOFA, while baseline DepthNet
performs better than MOFA. On MSE the baseline DepthNet model gets smaller MSE when mapping
to target faces, indicating it is better suited for this task.

In Figure 2 we plot heatmaps of the estimated depth of different models (on Y axis) and the GT depth
(on X axis) aggregated over all 66 keypoints on all test data. As can be seen, the depth estimated
by DepthNet+GAN and AIGN models form a 45 degree rotated ellipses showing a stronger linear
correspondence with respect to the GT depth compared to the the baseline DepthNet and MOFA.

Figure 2: Predicted (Y axis) versus Ground Truth (X axis) depth heatmaps for different models.

In Figure 3 we show some estimated depth samples for different models (see more samples in Figure
S1). AIGN and DepthNet+GAN generate more realistic results. MOFA generates very similar
face templates for different poses. Baseline DepthNet estimates reliable depth values in most cases,
however it has some failure modes as shown in the last row.

6

By comparing different models in Table 3, MOFA requires proper initialization to map face meshes
to each image. AIGN requires depth labels to train the model. Our baseline DepthNet model neither
require any depth labels nor any manual tuning. The results also show DepthNet can work well on
unpaired data. We would also like to emphasize that MOFA and AIGN are designed to estimate a 3D
model, while DepthNet is designed to estimate the parameters that facilitate warping a face pose to
another without having depth values, so these models are designed to solve different problems.

Figure 3: Depth visualization for different models (color coded by depth). From left to right: RGB image,
Ground Truth, DepthNet, DepthNet+GAN, AIGN and MOFA estimated depth values.

An interesting observation is that GT depth gets a higher MSE compared to DepthNet. This can be
due to not having a perspective projection between source and target faces. However, since DepthNet
is trained to map to the target faces, it learns the afﬁne parameters in a way to minimize this loss.

3.3 Face Rotation, Replacement and Adversarial Repair

In this section we show how DepthNet can be used for different applications. In Figure 1 (right) we
visualize the face rotation by re-projecting a frontal face, from Multi-PIE [6], (far left) to a range of
other poses deﬁned by the faces in the row above. Since DepthNet (case B) computes transformation
on keypoints rather than pixels it is robust to illuminations changes between source and target faces.
See Figures S2 to S5 for further examples. Note that DepthNet preserves well the identity. However,
it carries forward the emotion from source to target since using a global afﬁne transformation imparts
a degree of robustness to dramatic expression changes. The views in these ﬁgures are rendered from
a 3D model in OpenGL. Note the model can align well to the target face poses.

Figure 4: Background synthesis with CycleGAN. Left to right: source face; keypoints overlaid; DepthNet (DN);
DN + background → frontal;

In another experiment, we do face frontalization with synthesized background. Here we use Cy-
cleGAN to add background detail to a face that has been frontalized with DepthNet. Referring to
Figure 4, we perform this by conditioning the CycleGAN on the DepthNet image (column 3) and the
background of column 2 (masking interior face region determined by the convex hull spanned by
the keypoints). The second domain contains ground truth frontal faces. This experiment shows how
to leverage DepthNet for full face generation. Note that we do not use identity information in this
experiment. However, it can be used to better preserve the identity.

Figure 5: Left to right: source face; target face; warp to target; repaired result.

7

Finally, we do face swaps, where we warp the face of one identity onto the geometry and background
of another identity using DepthNet. To do so, we paste the rotated face by DepthNet onto the
background of the target image and train a CycleGAN to map from the domain of ‘swapped in faces’
to the ground truth faces in our dataset, effectively learning to clean up face swaps so that the face
region matches the hair and background. Some examples of this procedure are shown in Figure 5.

4 Related Work

4.1

3D Transformation on Faces

While there is a large body of literature on 3D facial analysis, many standard techniques are not
applicable to our setting here. As an example, morphable models [1] cover a wide variety of
approaches which are capable of high quality 3D reconstructions, but such methods usually require
3D face scans or reconstructions from multi-view stereo to be assembled so as to learn complex
parametric distributions over face shapes. A close approach to our own is that of [7] on viewing
real world faces in 3D. Similar to our work, this approach does not require aligned 3D face scans,
highly engineered models or manual interventions. They make the observation that if 2D keypoints
can be obtained from a single input image of a face and these keypoints are matched to an arbitrary
3D target geometry, then standard camera calibration techniques can be used to estimate plausible
intrinsics and extrinsics of the camera. This allows the estimated camera matrix, 3D rotation matrix
and 3D translation vector to be used to transform the target 3D model to the pose of the query image
from which an approximate depth can be obtained. Hassner et. al [8] explore the use of a single
unmodiﬁed 3D surface as an approximation to the shape of all input faces. In contrast, our approach
only requires 2D keypoints from the source and target faces as input. It then estimates the depth of
the source face keypoints, thereby inferring an image speciﬁc 3D model of the face.

DeepFace [19] uses face frontalization to improve the performance of a face veriﬁcation system. It
uses a 3D mask composed of facial keypoints, detects the corresponding locations of these keypoints
in the image, and maps the 2D keypoints onto a 3D face model to frontalize it. DeepFace, however,
maps to a template 3D face, therefore always mapping to a speciﬁc pose and geometry. DepthNet, on
the other hand, can map to any pose and geometry, giving it more expressive ﬂexibility.

4.2 Generative Adversarial Networks on Face Rotation

Recently, adversarial models in [12; 22; 25; 27; 18; 28] have explored face rotation. TP-GAN [12]
performs face frontalization through introducing several losses to preserve identity and symmetry of
the frontalized faces. PIM [28] frontalizes faces in a composed adversarial loss and then extracts pose
invariant features for face recognition. These models are mainly aimed for face veriﬁcation, where
they can only do face-frontalization. Another limitation of these models is in requiring ground truth
frontal images of the same identity during training. DR-GAN [22] rotates faces to any target pose by
using a discriminator that also does identity classiﬁcation in addition to pose prediction, to preserve
id and pose. While these models do pure face rotation of a 2D face, our model can warp the input
face to any other target face, allowing warping the input face to any other identity, with a different
geometry and pose. Moreover, our model also estimates the 3D geometric afﬁne transformation
parameters explicitly, allowing these parameters to be used later, e.g., for face texture swap.

FF-GAN [25], DA-GAN [27], and FaceID-GAN [18] estimate parameters of either a 3D Morphable
Model (3DMM), as in [25; 18], or source to target pose transformation, as in [27]. FF-GAN uses
3DMM parameters to frontalize faces in an adversarial approach, while FaceID-GAN uses the
3DMM parameters to generate any target pose. These models, however, train 3DMM on ground
truth labels such as identity, expression and pose. DepthNet, on the other hand, estimates depth and
afﬁne transformation parameters without requiring ground truth afﬁne or depth labels or pre-training.
Similar to DepthNet, DA-GAN [27] estimates parameters of an afﬁne transformation model that
maps a 2D face to a 3D face. Unlike DepthNet that estimates depth on the source face, DA-GAN
uses depth in a template target face. While their approach eliminates the need for depth estimation, it
only allows the source face to be mapped to the target template geometry, while DepthNet can map
the source face to any target geometry, provided by a target image, or its keypoints. We demonstrate
the application of this ﬂexibility for the face replacement task.

The aforementioned adversarial models use an identity preserving loss to maintain identity. The core
DepthNet model does not need identity labels and preserves well the identity (as shown in Figure 1

8

(right)). However, the identity information can be used by the proposed adversarial components, as in
background synthesis, to further improve the results. Unlike some of these models that take target
pose as input, DepthNet uses the target keypoints to estimate the target geometry and does not require
the target pose. This has several advantages; 1) DepthNet can map to the geometry of the target face
in addition to the pose, and 2) in the face replacement task, DepthNet can replace the target face with
the warped source face directly onto the target face location. Its application is shown in the face swap
experiment in Section 3.3.

4.3 Depth Estimation

Thewlis et. al [21] propose a mapping technique to learn a proxy of 2D landmarks in an unsupervised
way. A semi-supervised technique has been also proposed in [11] that improves landmark localization
by using weaker class labels (e.g. emotion or pose) and also by making the model predict equivariant
variations of landmarks when such transformations are applied to the image. Similar to these
approaches, DepthNet also maps a source to a target to learn its parameters. However, unlike these
two approaches that estimate 2D landmarks, DepthNet estimates the depth of the landmarks using
2D matching of keypoints, by formulating afﬁne parameters as a function of depth augmentated
keypoints in a closed form solution.

While several models [13; 2; 15] estimate depth with direct supervision, there has been recent models
[29; 5; 3] that estimate depth in an unsupervised training procedure. These models rely on pixel
reconstruction by using frames that are captured from very similar scenes, e.g. nearby frames of
a video [29] or left-right frames captures by stereo cameras [5; 3]. These models estimate depth
on one frame and then by using the disparity map, measure how pixel values of nearby frames
compare to each other. To do this, they also require camera intrinsic parameters, e.g. focal length or
distance between cameras. Unlike these models, our approach does not require source to target pixel
mapping. This allows mapping faces from different people with completely different skin colors,
without knowing camera parameters or how they are positioned with respect to each other. Therefore,
DepthNet is not susceptible to variations in illuminations or lighting between source and target faces.

Tung et. al [23] estimate 3D human pose in videos, where they use synthetic data to pre-train internal
parameters of the model and ﬁne-tune them by keypoint, segmentation and motion loss. Adversarial
Inverse Graphics Networks (AIGN) [24] estimates 3D human pose from 2D keypoint heatmaps in a
semi-supervised manner with a similar formulation to that of CycleGAN. They apply an adversarial
loss on the 3D pose to make them look realistic. These models leverage the depth values either
through synthetic data [23], or by adversarial usage of ground truth depth values [24]. Unlike these
models, DepthNet does not rely on any depth signal, either directly or indirectly. MOFA [20] builds
a 3D face mesh using a single image, where the 3D face parameters such as 3D shape and skin
reﬂectance are estimated by an encoder and then using a differentiable model they are rendered back
to the image by the decoder. This model requires manual initialization to map the input image to the
3D mesh, since otherwise it is doing an unconstrained optimization by adapting both the face pose
and the skin reﬂectance. Our model, however, does not require any manual initialization.

5 Conclusion

We have proposed a novel approach to 3D face model creation which enables pose normalization
without using any ground truth depth data. We achieve our best quantitative keypoint registration
results using our novel formulation for predicting depth and 3D visual geometry simultaneously,
learned by backpropagating through the analytic solution for the visual geometry estimation problem
expressed as a function of predicted depths. We have illustrated the quality and utility of the depths
and 3D transformations obtained using our method by transforming source faces to a wide variety of
target poses and geometries. Our technique can be used for face rotation and replacement and when
combined with adversarial repair it can blend warped faces to also synthesize the background. The
proposed model, however, carries forward emotion from source to target due to learning a shared
afﬁne parameters for all keypoints. Moreover, for extreme non-frontal faces, while DepthNet can
extract the transformation params (since it only relies on keypoints), OpenGL cannot extract texture
due to occlusion. We show an example of how to address this in the supplementary material. An
interesting extension to this paper can be replacing the OpenGL pipeline with a generative adversarial
framework that synthesizes a face using the parameters estimated by DepthNet.

9

6 Acknowledgments

We would like to thank Samsung and Google for partially funding this project. We are also thankful
to Compute Canada and Calcul Quebec for providing computational resources, and to Poonam Goyal
for helpful discussions.

References

[1] Blanz, Volker and Vetter, Thomas. A morphable model for the synthesis of 3d faces.

In
Proceedings of the 26th annual conference on Computer graphics and interactive techniques,
pp. 187–194. ACM Press/Addison-Wesley Publishing Co., 1999.

[2] Eigen, David, Puhrsch, Christian, and Fergus, Rob. Depth map prediction from a single image
using a multi-scale deep network. In Advances in Neural Information Processing Systems
(NIPS), pp. 2366–2374, 2014.

[3] Garg, Ravi, BG, Vijay Kumar, Carneiro, Gustavo, and Reid, Ian. Unsupervised cnn for single
view depth estimation: Geometry to the rescue. In Proceedings of the European Conference on
Computer Vision (ECCV), pp. 740–756. Springer, 2016.

[4] Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty of training deep feedforward
neural networks. In International conference on Artiﬁcial Intelligence and Statistics (AISTATS),
pp. 249–256, 2010.

[5] Godard, Clément, Mac Aodha, Oisin, and Brostow, Gabriel J. Unsupervised monocular depth
estimation with left-right consistency. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2017.

[6] Gross, Ralph, Matthews, Iain, Cohn, Jeffrey, Kanade, Takeo, and Baker, Simon. Multi-pie.

Image and Vision Computing, 28(5):807–813, 2010.

[7] Hassner, Tal. Viewing real-world faces in 3d.

In Proceedings of the IEEE International

Conference on Computer Vision (ICCV), pp. 3607–3614, 2013.

[8] Hassner, Tal, Harel, Shai, Paz, Eran, and Enbar, Roee. Effective face frontalization in uncon-
strained images. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 4295–4304, 2015.

[9] He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE
International Conference on Computer Vision (ICCV), pp. 1026–1034, 2015.

[10] Honari, Sina, Yosinski, Jason, Vincent, Pascal, and Pal, Christopher. Recombinator networks:
Learning coarse-to-ﬁne feature aggregation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pp. 5743–5752, 2016.

[11] Honari, Sina, Molchanov, Pavlo, Tyree, Stephen, Vincent, Pascal, Pal, Christopher, and Kautz,
Jan. Improving landmark localization with semi-supervised learning. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.

[12] Huang, Rui, Zhang, Shu, Li, Tianyu, and He, Ran. Beyond face rotation: Global and local
perception gan for photorealistic and identity preserving frontal view synthesis. In Proceedings
of the IEEE International Conference on Computer Vision (ICCV), Oct 2017.

[13] Jackson, Aaron S, Bulat, Adrian, Argyriou, Vasileios, and Tzimiropoulos, Georgios. Large pose
3d face reconstruction from a single image via direct volumetric cnn regression. In Proceedings
of the IEEE International Conference on Computer Vision (ICCV), pp. 1031–1039. IEEE, 2017.

[14] Jeni, László A, Cohn, Jeffrey F, and Kanade, Takeo. Dense 3d face alignment from 2d videos in
real-time. In IEEE International Conference and Workshops on Automatic Face and Gesture
Recognition (FG), volume 1, pp. 1–8. IEEE, 2015.

10

[15] Liu, Fayao, Shen, Chunhua, and Lin, Guosheng. Deep convolutional neural ﬁelds for depth
estimation from a single image. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 5162–5170, 2015.

[16] Parkhi, Omkar M, Vedaldi, Andrea, and Zisserman, Andrew. Deep face recognition. Proceedings

of the British Machine Vision Conference (BMVC), 2015.

[17] Sagonas, Christos, Tzimiropoulos, Georgios, Zafeiriou, Stefanos, and Pantic, Maja. 300 faces
in-the-wild challenge: The ﬁrst facial landmark localization challenge. In Proceedings of the
IEEE International Conference on Computer Vision Workshops (CVPRW), pp. 397–403, 2013.

[18] Shen, Yujun, Luo, Ping, Yan, Junjie, Wang, Xiaogang, and Tang, Xiaoou. Faceid-gan: Learning
a symmetry three-player gan for identity-preserving face synthesis. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 821–830, 2018.

[19] Taigman, Yaniv, Yang, Ming, Ranzato, Marc’Aurelio, and Wolf, Lior. Deepface: Closing the
gap to human-level performance in face veriﬁcation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 1701–1708, 2014.

[20] Tewari, Ayush, Zollhöfer, Michael, Kim, Hyeongwoo, Garrido, Pablo, Bernard, Florian, Perez,
Patrick, and Theobalt, Christian. Mofa: Model-based deep convolutional face autoencoder for
unsupervised monocular reconstruction. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), volume 2, 2017.

[21] Thewlis, James, Bilen, Hakan, and Vedaldi, Andrea. Unsupervised learning of object landmarks
by factorized spatial embeddings. In Proceedings of the IEEE International Conference on
Computer Vision (ICCV), volume 1, pp. 5, 2017.

[22] Tran, Luan, Yin, Xi, and Liu, Xiaoming. Disentangled representation learning gan for pose-
invariant face recognition. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017.

[23] Tung, Hsiao-Yu, Tung, Hsiao-Wei, Yumer, Ersin, and Fragkiadaki, Katerina. Self-supervised
learning of motion capture. In Advances in Neural Information Processing Systems (NIPS), pp.
5242–5252, 2017.

[24] Tung, Hsiao-Yu Fish, Harley, Adam W, Seto, William, and Fragkiadaki, Katerina. Adversarial
inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation from
unpaired supervision. In Proceedings of the IEEE International Conference on Computer Vision
(ICCV), volume 2, 2017.

[25] Yin, Xi, Yu, Xiang, Sohn, Kihyuk, Liu, Xiaoming, and Chandraker, Manmohan. Towards
large-pose face frontalization in the wild. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), pp. 1–10, 2017.

[26] Zhang, Xing, Yin, Lijun, Cohn, Jeffrey F, Canavan, Shaun, Reale, Michael, Horowitz, Andy,
Liu, Peng, and Girard, Jeffrey M. Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic
facial expression database. Image and Vision Computing, 32(10):692–706, 2014.

[27] Zhao, Jian, Xiong, Lin, Jayashree, Panasonic Karlekar, Li, Jianshu, Zhao, Fang, Wang, Zhecan,
Pranata, Panasonic Sugiri, Shen, Panasonic Shengmei, Yan, Shuicheng, and Feng, Jiashi. Dual-
agent gans for photorealistic and identity preserving proﬁle face synthesis. In Advances in
Neural Information Processing Systems (NIPS), pp. 66–76, 2017.

[28] Zhao, Jian, Cheng, Yu, Xu, Yan, Xiong, Lin, Li, Jianshu, Zhao, Fang, Jayashree, Karlekar,
Pranata, Sugiri, Shen, Shengmei, Xing, Junliang, et al. Towards pose invariant face recognition
in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 2207–2216, 2018.

[29] Zhou, Tinghui, Brown, Matthew, Snavely, Noah, and Lowe, David G. Unsupervised learning of
depth and ego-motion from video. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2017.

[30] Zhu, Jun-Yan, Park, Taesung, Isola, Phillip, and Efros, Alexei A. Unpaired image-to-image
translation using cycle-consistent adversarial networks. In Proceedings of the IEEE International
Conference on Computer Vision (ICCV), 2017.

11

Unsupervised Depth Estimation, 3D Face Rotation and
Replacement: Supplementary Information

S.1 Experimental Setup

The RCN, the DepthNets and the CycleGAN modules are trained separately. Each model is trained
using standard techniques for the model class and has a separate objective to be optimized. DepthNet
does not use the OpenGL pipeline during training and only uses it to render faces at test time, allowing
DepthNet to train faster.

S.1.1 DepthNet Experimental Details

Here we describe the details of the experiments carried out in Section 3.1. Our DepthNet architectures
require keypoints of both source and target images to be extracted. For this, the image is ﬁrst passed
through the VGG-Face [16] face detector. The face crops are then scaled down to 80 × 80 and
converted to greyscale, following which they are passed through RCN to obtain N = 68 keypoints
on each image. The RCN is trained exactly as described in [10], using the 300W dataset [17].

The keypoint only variant of our model involves concatenating all detected keypoints and passing
them through a two-layer deep fully connected network, with 256 hidden units and o output units.
The size of o depends on whether we are predicting only the depth, in which case o = N , or both
depth and afﬁne transformation parameters, in which case o = N + 8.

As discussed above, it is possible to augment these models with a Siamese CNN module (case A). In
the model variants that also use the image, we pass both the source and the target images through
three conv-maxpool layers with shared weights of size (32, 4, 2), (48, 3, 2), (64, 2, 2), respectively for
the representation (num_filters, filter_size, pool_size). The network’s outputs for the
source and target faces are then concatenated before passing them into a 4-layered fully connected
network with respective output sizes of 2048, 512, 256, and o. The keypoints are concatenated to the
512-unit layer before being passed to the last two layers. See Figure 1 (left) for an illustration of the
model. We explore these Siamese CNN augmented variants in models 4, 6 and 8 in Table 1.

We set the initial learning rate to 0.001 and use a Nesterov momentum optimizer (with a momentum
of 0.9) in all our experiments. With the exception of the last layer, we initialize all weights with
a Glorot initialization scheme [4], with the weights sampled from a uniform distribution. We use
a ReLU gain [9], set all biases to 0, and apply a ReLU non-linearity after every layer. In the ﬁnal
output layer, we do not apply any non-linearity and initialize the weights to 0. The biases of units that
represent depths are initialized to a random Normal distribution with µ = 0 and σ = 0.5, while those
that form the predicted afﬁne transform are initialized with the equivalent of a "ﬂattened" identity
transform. All models have been trained for 500 epochs.

We point out that except for a comparison between learning rates in the set {0.01, 0.001, 0.0001}
over few (less than 10) epochs, to ﬁnd a learning rate that the model seems to train well with, we
have not performed a hyperparameter search, and anticipate that the performance of the model can be
made even better by searching the hyperparameter space on a per model basis and by using deeper
(or modiﬁed) architectures.

S.2 Depth Visualization

We show further estimated depth values by different models in Figure S1. DepthNet+GAN and AIGN
generate the closest depth values to the GT depth. The baseline DepthNet model estimates reliable
depth values for most cases, however it has some degree of inaccuracy, as shown in the last two rows.
In DepthNet, the estimated depth indicates the position of each keypoint relative to other keypoints
rather than with respect to a source and importantly it is done without any supervision. MOFA, which
is also unsupervised, generates very similar face templates for different cases.

12

Figure S1: Depth visualization for different models (color coded by depth). The Depth axis is the one pointing
into the page. From left to right: RGB image, Ground Truth, DepthNet, DepthNet+GAN, AIGN and MOFA
estimated depth values.

S.3 Additional Camera Sweep Visualizations

In this section, we present additional visualizations along the lines of those shown in Figure 1 (right)
in Section 2 of the main paper. Frontal faces selected from the Multi-PIE dataset [6] are re-projected
to match several other poses corresponding to a person with a different identity. The DepthNets
predicts reliable proxy depths, which when coupled with the analytically obtained afﬁne transform
(obtained from the least-squares pseudo-inverse-based solution described in Section 2.3 of the main
paper) yields faces close to the desired target face geometry when passed through the OpenGL
pipeline.

We show camera sweeps for frontal source faces in Figures S2 and S3 and non-frontal source faces in
Figure S4. In Figure S5 we use the same target identity as the source face, showing how much the

13

Figure S2: Projecting a frontal face (far left) to a range of other poses deﬁned by faces in the row above.

generated face differs from the ground truth target. For all samples in Figures S2, S3, S4, and S5 we
use the DepthNet model that relies on only key-points (model 7 in Table 1).

Note that for non-frontal source faces, the quality of images is reduced specially for frontal target
faces. This is due to lack of adequate texture on the occluded side of the face to be transferred to the
target pose by OpenGL pipeline, rather than inaccuracies in the afﬁne transformation parameters. In
order to reduce the side-affects, we use either of the source face or its ﬂipped version, that is closer to
the target face pose, and then warp the face to the target keypoints.

Figure S3: Projecting a frontal face (far left) to a range of other poses deﬁned by faces in the row above.

Figure S4: Re-projecting a non-frontal face (far left) to a range of other poses deﬁned by faces in the row above.

Figure S5: Rotating a face (far left) to a range of other poses deﬁned by faces of the same identity in the row
above. On the top row frontal and on the bottom row non-frontal source faces are shown.

S.4 CycleGAN

Suppose we have some images belonging to one of two sets x ∈ X and y ∈ Y , where x denotes a
DepthNet-resulting face and y a ground truth face which is frontal. We wish to learn two functions

14

(cid:105)

(cid:105)

,

(cid:104)

Ex,y

min
G,F

(cid:104)

Ex,y

min
DX ,DY

F : X → Y and G : Y → X which are able to map an image from one set to the corresponding
image in the other. Correspondingly, we have two discriminators DX and DY which try to detect
whether the image in that particular set is real or generated. While we are only interested in the
function F : X → Y (since this is mapping to the distribution of ground truth faces) the formulation
of CycleGAN requires that we learn mappings in both directions during training. We optimize the
following objectives for the two generators F and G:

(cid:96)(DX (G(y)), 1) + (cid:96)(DY (F (x)), 1) + λ||y − F (G(y))||1 + λ||x − G(F (x))||1

(4)

And the following for the two discriminators DX and DY :

(cid:96)(DX (x), 1) + (cid:96)(DX (G(y)), 0) + (cid:96)(DY (y), 1) + (cid:96)(DY (F (x)), 0)

(5)

where 0/1 denote fake/real, (cid:96)() is the squared error loss and λ is a coefﬁcient for the cycle-consistency
(reconstruction) loss.

In the case where we did adversarial background synthesis, x is a channel-wise concatenation of the
DepthNet-frontalized face and the background of the original (pre-frontalized) image. For face-swap
cleanup, x is simply a source face which has been warped to a target face and pasted on top.

Once the network has been trained, we can disregard all other functions and use F to clean up faces
which are low quality due to artifacts from warping.

In terms of architectural details the generators and discriminators used were those described in the ap-
pendix of the CycleGAN paper [30]. In short, the generator consists of three conv-BN-relu blocks
which downsample the input, followed by nine ResNet blocks (which can be interpreted as iteratively
performing transformations over the downsampled representation), followed by deconv-BN-relu
blocks to upsample the representation back into the original input size. For training, we use the
same hyperparameters as most CycleGAN implementations which is using the Adam optimizer with
learning rate α = 2 × 10−4, β1 = 0.5, β2 = 0.999. However, instead of using a batch size of 1 we
use the largest possible batch size, which was 16 for a 12GB GPU.

Note that in order to produce better translations, the dataset we used for all CycleGAN experiments
contain both the VGG and the CelebA datasets, which has signiﬁcantly more images.

S.4.1 Background Synthesis

We present extra visualizations for the CycleGAN which performs background synthesis on CelebA,
corresponding to Figure 4 in the main paper. These are shown in Figure S6.

Figure S6: Background synthesis with CycleGAN. Left to right: source face; keypoints overlaid; DepthNet
(DN); DN + background → frontal

S.4.2 Face Replacement and Adversarial Repair

In Figure S7 we provide extra face swap samples on CelebA, corresponding to Figure 5 in the
main paper, where a source face is warped to a target face pose using DepthNet, pasted onto the

15

target image and then passed to a CycleGAN to adapt the face skin of the warped source face to the
background and hairstyle of the target face.

Figure S7: Face swap experiment with CycleGAN. Left to right: source face; target face; warp to target with
DepthNet; repaired result with CycleGAN. The source face is taken and warped onto the target face. The
background and hairstyle is then adapted to the target face.

S.4.3 Extreme Pose Face Clean-up

If the source image has an extreme pose, the texture will be missing on the occluded side of the
face and the OpenGL pipeline cannot rotate the face without artifacts. Note that this shortcoming is
due to lack of texture on the occluded side of the face rather than a deﬁciency of the transformation
parameters measured by DepthNet.

We performed an experiment using CycleGAN to ﬁx such artifacts. For this experiment we take
source images from CelebA and ﬁrst frontalize it by using DepthNet. Since the frontalized faces have
artifacts due to stretch of texture on the occluded side of the face by the OpenGL pipeline, we train a
CycleGAN that takes DepthNet frontalizaed faces plus the background of the original non-frontal
image (as two images) in one domain and the ground truth frontal faces in the other domain. The
CycleGAN learns to clean-up these artifacts. Finally, we take the GAN-repaired frontalized faces
and project it to different target poses using DepthNet. In Figure S8 we visualize camera sweep for
source faces in the wild that have extreme poses. The cycleGAN reasonably cleans the face artifacts
and then DepthNet projects it to different poses. This is just one approach to address the extreme
pose occlusion artifacts. We see alternative methods for addressing this issue as promising directions
for future research.

Figure S8: Re-projecting a non-frontal face (far left) from CelebA to a range of other poses deﬁned by faces in
the row above. Top row (in each pair) depicts the target faces from Multi-PIE [6]. The bottom row shows from
left to right: source face, souce face frontalized by DepthNet, adversarial-repaired face, the repaired source face
projected to the target poses (4th to 10th columns).

16

8
1
0
2
 
c
e
D
 
4
2
 
 
]

V
C
.
s
c
[
 
 
5
v
2
0
2
9
0
.
3
0
8
1
:
v
i
X
r
a

Unsupervised Depth Estimation,
3D Face Rotation and Replacement

Joel Ruben Antony Moniz1∗, Christopher Beckham2,3∗, Simon Rajotte2,3,
Sina Honari2, Christopher Pal2,3,4
1Carnegie Mellon University, 2Mila-University of Montreal, 3Polytechnique Montreal, 4Element AI
1jrmoniz@andrew.cmu.edu, 2honaris@iro.umontreal.ca, 3firstname.lastname@polymtl.ca

Abstract

We present an unsupervised approach for learning to estimate three dimensional
(3D) facial structure from a single image while also predicting 3D viewpoint
transformations that match a desired pose and facial geometry. We achieve this
by inferring the depth of facial keypoints of an input image in an unsupervised
manner, without using any form of ground-truth depth information. We show how
it is possible to use these depths as intermediate computations within a new back-
propable loss to predict the parameters of a 3D afﬁne transformation matrix that
maps inferred 3D keypoints of an input face to the corresponding 2D keypoints on
a desired target facial geometry or pose. Our resulting approach, called DepthNets,
can therefore be used to infer plausible 3D transformations from one face pose
to another, allowing faces to be frontalized, transformed into 3D models or even
warped to another pose and facial geometry. Lastly, we identify certain shortcom-
ings with our formulation, and explore adversarial image translation techniques as
a post-processing step to re-synthesize complete head shots for faces re-targeted to
different poses or identities. 1

1

Introduction

Face rotation is an important task in computer vision.
It has been used to frontalize faces for
veriﬁcation [8; 19; 25; 28] or to generate faces of arbitrary poses [22; 18]. In this paper we present
a novel unsupervised learning technique for face rotation and warping from a 2D source image –
whose facial appearance will be used in the rotation – to a target face – to which the facial pose
and geometry inferred from the source image is mapped. A use case is when we have an image of
someone in a particular target pose and we want to put a given source face into that pose, without
knowing the exact target face pose. This can be leveraged, for example, in the advertisement industry,
when putting someone in a particular location can be costly or unfeasible, or in the movie industry
when the main actor’s limited time or high cost can enforce using another actor whose face can be
later replaced by the main actor’s. This is achieved through estimating the source face depth and
the 3D afﬁne parameters that warp the source to the target face using neural networks. These neural
networks use a novel loss formulation for the structured prediction of keypoint depths. Once the 3D
afﬁne transformation matrix is estimated, it can be used to warp the source image onto the target
face geometry using a textured triangular mesh. The use of a 3D afﬁne transform means that we
can capture both a 3D rotation of the face to a new viewpoint as well as a global non-Euclidean
warping of the geometry to match a target face. We call these neural networks Depth Estimation-Pose
Transformation Hybrid Networks, or DepthNets in short.

Our ﬁrst contribution is to propose a neural architecture that predicts both the depth of source
keypoints as well as the parameters of a 3D geometric afﬁne transformation which constitute the

∗Indicates equal contribution.
1Code will be released at: https://github.com/joelmoniz/DepthNets/

32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.

explicit outputs of the DepthNet model. The predicted depth and afﬁne transformation could be then
used to map a source face to a target face for object orientation, distortion and viewpoint changes.

Our second contribution consists of making the observation that given 3D source and 2D target
keypoints, closed form least squares solutions exist for estimating geometric afﬁne transformation
models between these sets of keypoint correspondences, and we can therefore develop a model that
captures the dependency between depth and the afﬁne transformation parameters. More speciﬁ-
cally, we express the afﬁne transformation as a function of the pseudoinverse transformation of 2D
keypoints in a source image – augmented by inferred depths – and the target keypoints. Thus, the
second and major contribution in this work is capturing the relationship between an estimated afﬁne
transformation and the inferred depth as a deterministic relationship. In this formulation, DepthNet
only predicts depth values explicitly and the afﬁne parameters are inferred through a pseudoinverse
transformation of source and target keypoints. Here, one can directly optimize through the solutions
of what might otherwise be formulated as a secondary minimization step.

Our proposed DepthNet can map the central region of the source face to the target geometry. This
leads to background mismatch when warping one face to another. Finally, our third contribution is to
use an adversarial unpaired image-to-image transformation approach to repair the appearance of 3D
models inferred from DepthNet. Together these contributions allow 3D models of faces that construct
realistic images in the target pose. Our proposed method can be used for pose normalization or face
swaps with no manually speciﬁed 3D face model. To the best of our knowledge, this is the ﬁrst such
neural network based model that estimates a 3D afﬁne transformation model for face rotation which
neither requires ground-truth 3D images nor any ground truth 3D face information such as depth.

As we have outlined above, our approach uses neural networks for inferring depth and geometric
transformation – referred to as DepthNets; and, an adversarial image-to-image transformation network
which improves the quality of the appearance of a 3D model inferred from a DepthNet.

2 Our Approach

DepthNets

We propose three DepthNet formulations, described in Sections 2.1, 2.2, and 2.3. For each of the
three models we explore two architectural scenarios: (A) a Siamese-like architecture that uses the
source and target images themselves as well as keypoints extracted from these images, and (B) a
fully-connected neural network variant which uses only facial keypoints in the source and target
images. See Figure 1 (left) for details.

Figure 1: (Left) DepthNet architecture. The blue region is only used in case (A) and the red part is used in
both cases (A) and (B), described in Section 2. The orange output (the 8 afﬁne transformation parameters) is
predicted only by model variations described in Sections 2.1 and 2.2, and not the model described in section
2.3. All three models predict the N depth values of the source keypoints. C, P, and FC correspond to valid conv,
pool and fully-connected layers. The two paths of Siamese network share parameters and the black dots indicate
concatenating keypoint values to FC units. (Right) Visualizing face rotation by re-projecting a frontal face (far
left) to a range of other poses deﬁned by the faces in the row above (in each pair of rows). In this experiment, we
only use keypoints from the top-row in the DepthNet model (Model 7 in Table 1).

It is interesting to note that if DepthNets are used to register a set of images of objects to the same
common viewpoint, the same image and geometry can be used as the target. This is the case for
the frontalization of faces, for example. While the DepthNet framework is sufﬁciently general to be
applied to any object type where 2D keypoint detections have been made, our experiments here focus
on faces. We describe the three variants of DepthNets below.

2

2.1 Predicting Depth and Viewpoint Separately

In this variant of DepthNets, the model predicts both depths and viewpoint geometry, but as separate
explicit outputs of a neural network. The input is comprised of only the geometry and pose of the
source and target faces (encoded in the form of a 2D keypoint template), in case (B), or both keypoints
and images of the source and target faces, in case (A). The key phases of this stage are described by
the sequence of steps given below:

1. Keypoint extraction: Raw (x, y) pixel coordinates corresponding to the keypoints of each image
are extracted using a Recombinator Network (RCN) [10] architecture, and then concatenated before
being passed into the keypoint processing step.
2. (Optional) Image Feature Extraction: DepthNets can be conditioned on only keypoints, case (B),
or on keypoints and the original images, case (A). We can therefore optionally subject the source and
target images to alternating conv-maxpool layers. If this component of the architecture is used, the
last spatial feature maps in the Siamese architecture are concatenated before being given to a set of
densely connected hidden layers.
3. Keypoint processing: In this step keypoints are passed through a set of hidden layers. If the Image
Feature Extraction stage is used, the keypoints are concatenated to image features, the output of
which is in turn fed to densely connected layers. The output layer of this phase will be of size N + 8,
where N is the number of keypoints. The ﬁrst N points represent the Depth proxy, and the last 8
points form a 4 × 2 matrix representing the learned parameters of the afﬁne transform. See Figure 1.
4. Geometric Afﬁne Transformation Normalizer: This phase applies the predicted afﬁne transform
on each (depth augmented) source keypoint to estimate its target location. Let (xi
s) represent the
ith source keypoint, (xi
n) the corresponding source normalized keypoint estimated by applying
the afﬁne transformation matrix, (xi
t) the ith target keypoint (as ground truth (GT)), and Is and It
represent the source and target images respectively. Depending on which underlying architectural
variant we use, two cases arise: one that utilizes only the keypoints (B), and another utilizing
both the keypoints and the images (A). Since the keypoints are generated using RCNs, they are
technically functions of the input images: [xs, ys] = R(Is), and [xt, yt] = R(It). Depending
on the (A) or (B) variant, the ith keypoint’s predicted depth proxy zi
p is inferred as a function
of the input keypoints, or both input keypoints and input images.
In both cases the keypoints
are derived from the images, so zi
p(Is, It). Similarly, the 3D-2D afﬁne transform F is a
function of the images, such that F = F(Is, It), where the 8 predicted parameters are: m =
{m1, m2, m3, tx, m4, m5, m6, ty)}. These constitute the 3D-2D afﬁne transform which is used by
all keypoints. In other words, each of the i points is transformed using xi
s, or:

n = F(Is, It) xi

p = zi

n, yi

s, yi

t, yi

(cid:20) xi
n
yi
n

(cid:21)

=

(cid:20)m1 m2 m3
m4 m5 m6






(cid:21)

tx
ty

xi
s
yi
s
zi
p(Is, It)
1






The loss function of a DepthNet is obtained by transforming the source face to match the target face
using the simple squared error of the corresponding target object’s keypoint vector xt = [xt, yt]T
, as GT values, and the source object’s normalized keypoint vector [xn, yn]T . The loss for one
example where we predict depth and afﬁne viewpoint geometry can therefore be expressed as:

L =

K
(cid:88)

i=1

(cid:13)
(cid:13)xi
(cid:13)

t − F(Is, It) [xi

s yi

s zi

p(Is, It)]T (cid:13)
2
(cid:13)
(cid:13)

(1)

5. Image Warper: This phase consists of using the depth proxy and afﬁne transform matrix generated
to actually warp the face from its source pose to be matched to the target object geometry. The ﬁnal
projection to 2D is achieved by simply dropping the transformed z coordinate (which corresponds
to an orthographic projection model). In the case of DepthNets, this orthographic projection is
effectively embedded in the Geometric Afﬁne Transformation Normalizer step, since the afﬁne
corresponding to the z coordinate is not predicted, essentially dropping it.
As we operate on keypoints, the actual warping of pixels can be performed with a high quality
OpenGL pipeline that performs the warp separately from the rest of the architecture. Source image,
keypoints augmented with depth, and the afﬁne matrix are passed to OpenGL pipeline to warp the
source image towards the target pose. This OpenGL warping is not needed during DepthNet training,
which means we do not have to do feedforward or backprop through OpenGL. In Summary, for step 1
the RCN model [10] is used, for steps 2 to 4 the DepthNet model, shown in Figure 1 (left), is trained,

3

and for step 5 an OpenGL pipeline is used. No data or parameters are needed to train the OpenGL
pipeline. It warps images by directly using the provided data.

2.2 Estimating Viewpoint Geometry as a Second Step

In this model variant, training is similar to Section 2.1 and the model outputs depth and 3D
afﬁne transformation parameters. However, at test time, rather than using the predicted 3D afﬁne
transformation for pairs of faces, we use only the predicted depths and estimate the afﬁne ge-
ometry parameters as a second estimation step. More precisely, given 3D points for a scene
and the corresponding 2D points for a target geometry it is possible to formulate the estima-
tion of a 3D afﬁne transformation as a linear least squares estimation problem. An overdeter-
mined system of the form Am = xt for this problem can be constructed as shown in (2).

This corresponds to an afﬁne camera model
followed by an orthographic projection to
2D keypoints. This setup also leads to
the following closed form solution for the
afﬁne transformation parameters:

m = [AT A]−1AT xt,

(3)

y1
s
0
y2
s
0

z1
s
0
z2
s
0

x1
s
0
x2
s
0













xK
s
0

yK
s
0

zK
s
0

0
x1
s
0
x2
s

0
xK
s

0
y1
s
0
y2
s
...
0
yK
s

0
z1
s
0
z2
s

1 0
0 1
1 0
0 1

0
zK
s

1 0
0 1





































m1
m2
m3
m4
m5
m6
tx
ty

























x1
t
y1
t
x2
t
y2
t
...
xK
t
yK
t

=

(2)

where this pseudoinverse based transformation is parameterized by the reference points and their
predicted depths.

2.3

Joint Viewpoint and Depth Prediction

Our key observation is that one can alternatively use the closed form analytical solution, measured in
Eq. (3), for the least squares estimation problem as the underlying afﬁne transformation matrix within
the loss function. This leads to a special form of structured prediction problem for geometrically
consistent depths and afﬁne transformation matrix. For each image we have L =

K
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:21)

(cid:20) xi
t
yi
t
(cid:124) (cid:123)(cid:122) (cid:125)
xi
t

(cid:20)m1 m2 m3
m4 m5 m6

−

(cid:124)

(cid:123)(cid:122)
m

tx
ty

(cid:21)

(cid:125)







(cid:124)

xi
s
yi
s
zi
p(Is, It)
1
(cid:123)(cid:122)
xi
s







(cid:125)

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

K
(cid:88)

i=1

(cid:13)
(cid:13)xi

t − reshape(cid:2)[AT A]−1AT xt

(cid:3)xi

s

(cid:13)
2
(cid:13)

where the matrix A is parameterized as a function of xs as shown in Eq. (2). In this variant, the
model explicitly outputs only depth values during train and test time. The afﬁne transformation matrix
in the equation above is replaced by Eq. (3), which measures the afﬁne transformation as a pure
function of source and target keypoints plus the inferred depth. The big difference of this formulation
compared to Sections 2.1 and 2.2 is that geometric afﬁne transformation parameters are no longer
predicted by DepthNet during training and at both train and test time – it solves the least square loss
through the pseudoinverse based transformation. Since zi
t }j=1...N ) is predicted
within the analytical formulation of the solution to the least squares minimization problem, we can
backpropagate through the solution of a minimization problem that depends on the predicted depths.
While we leverage keypoints for depth estimation, the proposed approach is novel in how the depth is
estimated. Note that it is unsupervised with respect to depth labels. No depth supervision either by
using depth targets (as in [2; 13; 15]), or by using depth in an adversarial setting (as in [24]), is used
to estimate depth values for the base DepthNet models described in Sections 2.1, 2.2, and 2.3.

s = zi

p({xj

s, xj

t , yj

s, yj

The depths learned for keypoints by these approaches are not necessarily true depths, but are likely to
strongly correlate with the actual depth of each keypoint. This is because even though the method
succeeds (as we shall see below) in aligning poses, the inferred depth and the afﬁne transform may
each be scaled by factors so as to cancel each other out (i.e., by factors which are multiplicative
inverses of each other). Real world viewpoint geometry also involves perspective projection.

Adversarial Image-to-Image Transformation

DepthNet transforms the central region of the source face to the target pose. Inevitably, the face
background will be missing, which might make the proposed method unsuitable for many application

4

where the full face is required. To address this issue, we utilize CycleGAN [30], an adversarial
image-to-image translation technique. This serves to repair the background of faces that have
undergone frontalization or face swap through the DepthNet pipeline. Importantly, the adversarial
nature of CycleGAN allows one to perform image transformation between two domains without
the requirement of paired data. In our work, we perform experiments translating between various
domains of interest but one example is translating between the domain of images in the dataset (i.e.
the ground truth) and the domain of images where the DepthNet output is pasted onto the face region
(in the case of face-swap). By doing so we clean the face background in an unsupervised manner.

3 Experiments

3.1 DepthNet Evaluation on Paired Faces

For the experiments in this section, we use a subset of the VGG dataset [16], with training and
validating on all possible pairs of images belonging to the same identity for 2401 identities. This
yields 322,227 train and 43,940 validation pairs. Check experimental setup details in Supplementary.

Model

Color MSE MSE_norm

1) A simple 2D afﬁne registration
2) A 3D afﬁne registration model using an average 3D face template
3) A DepthNet that separately estimates depth and geometry
4) The model above, but with a Siamese CNN image model
5) Secondary least squares estimation for visual geometry using the depths from 3)
6) Secondary least squares estimation for visual geometry using the depths from 4)
7) Backpropagation through the pseudoinverse based solution for visual geometry
8) The model above, but with a Siamese CNN image model

grey
purple
brown
violet
red
green
orange
blue

1.562
0.724
0.568
0.539
0.400
0.399
0.357
0.349

9.547
7.486
6.292
6.115
5.184
5.175
4.932
4.891

Table 1: (left) Comparing the Mean Squared Error (MSE) and MSE normalized by inter-ocular distance
(MSE_norm) of different models. (right) Histogram of Mean Squared Errors. The second column in the Table
(on left) corresponds to the color of the model in the ﬁgure (on right).

We explore the three variants of DepthNets described in Sections 2.1, 2.2, and 2.3, each with two
architectural cases (A) and (B), depending on whether image features are used in addition to keypoints
or not. We also compare with a number of baselines. We measure the mean square error (MSE)
between the estimated keypoints on the target face (source face normalized keypoints) and ground
truth target keypoints. Results for the following models are shown in Table 1:

1) A baseline model registrations using a simple 2D afﬁne transformation.

2) We generate a 3D average face template from the 3DFAW dataset [14; 26; 6] by aligning the 3D
keypoints of all faces in the dataset to a front-facing face using Procrustes superimposition. We report
error by mapping the template face to each source face via Procrustes superimposition (to get a 3D
face f ) and then use an afﬁne transformation from the 3D face f to the target face.

3, 4) We use our proposed approach to predict both depth and geometry (described in Sections 2.1).

5, 6) These models described in Section 2.2. Note that during training, these two cases are similar to
models 3 and 4 in Table 1.

7, 8) The pseudo-inverse formulation model described in Section 2.3.

As observed in Table 1, a simple 2D afﬁne transform (model 1) without estimating depth and a
template 3D face (model 2) get high errors on mapping to the target faces. DepthNet models get lower
errors and the pseudo-inverse formulation (models 7 and 8) further reduces the error by 10%. The
CNN models slightly reduce errors compared to their equivalent models that rely only on keypoints.

3.2 DepthNet Evaluation on Unpaired Faces and Comparison to other Models

In this section we train DepthNet on unpaired faces belonging to different identities and compare
with other models that estimate depth. We use the 3DFAW dataset [14; 26; 6] that contains 66
3D keypoints to facilitate comparing with ground truth (GT) depth. It provides 13,671 train and
4,500 valid images. We extract from the valid set, 75 frontal, left and right looking faces yielding
a total of 225 test images, which provides a total of 50,400 source and target pairs. We train the
psuedoinverse DepthNet model that relies on only keypoints (model 7 in Table 1). We also train
a variant of DepthNet that applies an adversarial loss on the depth values (DepthNet+GAN). This
model uses a conditional discriminator that is conditioned on 2D keypoints and discriminates GT
from estimated depth values. The model is trained with both keypoint and adversarial losses.

5

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)

Source

Target

Left
Front
Right

DepthNet

DepthNet + GAN

Left
24.67
25.54
21.66

Front Right
29.70
27.71
26.19
27.22
23.87
21.48

Avg
27.36
26.32
22.34

Left
59.78
58.77
59.97

Front Right
59.63
59.67
58.61
58.67
59.60
59.70

Avg
59.69
58.68
59.76

Table 2: Comparing DepthCorr for different DepthNet models when mapping variant source to target poses. The
Avg column measures the average over the three preceding columns.

We measure the correlation matrix between GT and estimated depths, where the element k in the
diagonal indicates the correlation between estimated and ground truth depth values for keypoint k,
yielding a value between -1 and 1. We report the sum of absolute values of the diagonal of this
matrix, indicated by DepthCorr. We compare DepthNet models on DepthCorr in Table 2. For this
experiment we take every possible pair of source to target faces, where source and target are one of
{left, front, right} looking faces. This yields a total of 5,550 pairs when the source and the target are
from the same subset, and 5,625 pairs otherwise. This experiment measures the accuracy of depth
estimation of the DepthNet models on different orientations of source-target faces. The baseline
DepthNet model that does not leverage the depth labels performs well in different cases. DepthCorr
improves more than twice for the DepthNet+GAN model, indicating a direct supervision loss using
depth labels can enhance the depth estimation.

Model

GT Depth
AIGN [24]
MOFA [20]
DepthNet (Ours)
DepthNet + GAN (Ours)

Need Depth Manual Init. MSE (×10−5) Depth Correlation Matrix Trace (DepthCorr)
Right pose
66
49.04
17.54
22.34
59.76

Front pose
66
50.81
15.97
26.32
58.68

8.86 ± 6.55
9.06 ± 6.61
8.75 ± 6.33
7.65 ± 6.97
8.74 ± 6.24

Left pose
66
44.08
11.14
27.36
59.69

-
No
Yes
No
No

Yes
Yes
No
No
Yes

Table 3: Comparing MSE and DepthCorr for different models. A lower MSE indicates the model maps better to
the target faces. A higher DepthCorr indicates more correlation between estimated and GT depths.

We compare our two DepthNet models with three baselines: 1) AIGN [24], 2) MOFA [20] and 3)
GT Depth (no model trained). AIGN estimates 3D keypoints conditioned on 2D heatmaps of the
keypoints. MOFA estimates a 3D mesh using only an image. We implemented the AIGN model
and asked the authors of MOFA to run their model on our test-set. They provided MOFA’s results
for 134 images in the test set. In Table 3 we compare these three models with our DepthNet models
on DepthCorr. We also compare them on MSE, which is measured between GT and estimated
target keypoints. Since the three baselines estimate depth on a single image due to their different
model formulation, we ﬁrst measure m using closed form solution in Eq. 3 and then apply m to
the estimated source keypoints to get the target keypoint estimations. We contrast the estimated
values with the GT target keypoints. As shown in Table 3, GT depth has the highest DepthCorr
(the maximum possible value). The depths estimated by DepthNet+GAN and AIGN have stronger
correlation to GT depth compared to the baseline DepthNet and MOFA, while baseline DepthNet
performs better than MOFA. On MSE the baseline DepthNet model gets smaller MSE when mapping
to target faces, indicating it is better suited for this task.

In Figure 2 we plot heatmaps of the estimated depth of different models (on Y axis) and the GT depth
(on X axis) aggregated over all 66 keypoints on all test data. As can be seen, the depth estimated
by DepthNet+GAN and AIGN models form a 45 degree rotated ellipses showing a stronger linear
correspondence with respect to the GT depth compared to the the baseline DepthNet and MOFA.

Figure 2: Predicted (Y axis) versus Ground Truth (X axis) depth heatmaps for different models.

In Figure 3 we show some estimated depth samples for different models (see more samples in Figure
S1). AIGN and DepthNet+GAN generate more realistic results. MOFA generates very similar
face templates for different poses. Baseline DepthNet estimates reliable depth values in most cases,
however it has some failure modes as shown in the last row.

6

By comparing different models in Table 3, MOFA requires proper initialization to map face meshes
to each image. AIGN requires depth labels to train the model. Our baseline DepthNet model neither
require any depth labels nor any manual tuning. The results also show DepthNet can work well on
unpaired data. We would also like to emphasize that MOFA and AIGN are designed to estimate a 3D
model, while DepthNet is designed to estimate the parameters that facilitate warping a face pose to
another without having depth values, so these models are designed to solve different problems.

Figure 3: Depth visualization for different models (color coded by depth). From left to right: RGB image,
Ground Truth, DepthNet, DepthNet+GAN, AIGN and MOFA estimated depth values.

An interesting observation is that GT depth gets a higher MSE compared to DepthNet. This can be
due to not having a perspective projection between source and target faces. However, since DepthNet
is trained to map to the target faces, it learns the afﬁne parameters in a way to minimize this loss.

3.3 Face Rotation, Replacement and Adversarial Repair

In this section we show how DepthNet can be used for different applications. In Figure 1 (right) we
visualize the face rotation by re-projecting a frontal face, from Multi-PIE [6], (far left) to a range of
other poses deﬁned by the faces in the row above. Since DepthNet (case B) computes transformation
on keypoints rather than pixels it is robust to illuminations changes between source and target faces.
See Figures S2 to S5 for further examples. Note that DepthNet preserves well the identity. However,
it carries forward the emotion from source to target since using a global afﬁne transformation imparts
a degree of robustness to dramatic expression changes. The views in these ﬁgures are rendered from
a 3D model in OpenGL. Note the model can align well to the target face poses.

Figure 4: Background synthesis with CycleGAN. Left to right: source face; keypoints overlaid; DepthNet (DN);
DN + background → frontal;

In another experiment, we do face frontalization with synthesized background. Here we use Cy-
cleGAN to add background detail to a face that has been frontalized with DepthNet. Referring to
Figure 4, we perform this by conditioning the CycleGAN on the DepthNet image (column 3) and the
background of column 2 (masking interior face region determined by the convex hull spanned by
the keypoints). The second domain contains ground truth frontal faces. This experiment shows how
to leverage DepthNet for full face generation. Note that we do not use identity information in this
experiment. However, it can be used to better preserve the identity.

Figure 5: Left to right: source face; target face; warp to target; repaired result.

7

Finally, we do face swaps, where we warp the face of one identity onto the geometry and background
of another identity using DepthNet. To do so, we paste the rotated face by DepthNet onto the
background of the target image and train a CycleGAN to map from the domain of ‘swapped in faces’
to the ground truth faces in our dataset, effectively learning to clean up face swaps so that the face
region matches the hair and background. Some examples of this procedure are shown in Figure 5.

4 Related Work

4.1

3D Transformation on Faces

While there is a large body of literature on 3D facial analysis, many standard techniques are not
applicable to our setting here. As an example, morphable models [1] cover a wide variety of
approaches which are capable of high quality 3D reconstructions, but such methods usually require
3D face scans or reconstructions from multi-view stereo to be assembled so as to learn complex
parametric distributions over face shapes. A close approach to our own is that of [7] on viewing
real world faces in 3D. Similar to our work, this approach does not require aligned 3D face scans,
highly engineered models or manual interventions. They make the observation that if 2D keypoints
can be obtained from a single input image of a face and these keypoints are matched to an arbitrary
3D target geometry, then standard camera calibration techniques can be used to estimate plausible
intrinsics and extrinsics of the camera. This allows the estimated camera matrix, 3D rotation matrix
and 3D translation vector to be used to transform the target 3D model to the pose of the query image
from which an approximate depth can be obtained. Hassner et. al [8] explore the use of a single
unmodiﬁed 3D surface as an approximation to the shape of all input faces. In contrast, our approach
only requires 2D keypoints from the source and target faces as input. It then estimates the depth of
the source face keypoints, thereby inferring an image speciﬁc 3D model of the face.

DeepFace [19] uses face frontalization to improve the performance of a face veriﬁcation system. It
uses a 3D mask composed of facial keypoints, detects the corresponding locations of these keypoints
in the image, and maps the 2D keypoints onto a 3D face model to frontalize it. DeepFace, however,
maps to a template 3D face, therefore always mapping to a speciﬁc pose and geometry. DepthNet, on
the other hand, can map to any pose and geometry, giving it more expressive ﬂexibility.

4.2 Generative Adversarial Networks on Face Rotation

Recently, adversarial models in [12; 22; 25; 27; 18; 28] have explored face rotation. TP-GAN [12]
performs face frontalization through introducing several losses to preserve identity and symmetry of
the frontalized faces. PIM [28] frontalizes faces in a composed adversarial loss and then extracts pose
invariant features for face recognition. These models are mainly aimed for face veriﬁcation, where
they can only do face-frontalization. Another limitation of these models is in requiring ground truth
frontal images of the same identity during training. DR-GAN [22] rotates faces to any target pose by
using a discriminator that also does identity classiﬁcation in addition to pose prediction, to preserve
id and pose. While these models do pure face rotation of a 2D face, our model can warp the input
face to any other target face, allowing warping the input face to any other identity, with a different
geometry and pose. Moreover, our model also estimates the 3D geometric afﬁne transformation
parameters explicitly, allowing these parameters to be used later, e.g., for face texture swap.

FF-GAN [25], DA-GAN [27], and FaceID-GAN [18] estimate parameters of either a 3D Morphable
Model (3DMM), as in [25; 18], or source to target pose transformation, as in [27]. FF-GAN uses
3DMM parameters to frontalize faces in an adversarial approach, while FaceID-GAN uses the
3DMM parameters to generate any target pose. These models, however, train 3DMM on ground
truth labels such as identity, expression and pose. DepthNet, on the other hand, estimates depth and
afﬁne transformation parameters without requiring ground truth afﬁne or depth labels or pre-training.
Similar to DepthNet, DA-GAN [27] estimates parameters of an afﬁne transformation model that
maps a 2D face to a 3D face. Unlike DepthNet that estimates depth on the source face, DA-GAN
uses depth in a template target face. While their approach eliminates the need for depth estimation, it
only allows the source face to be mapped to the target template geometry, while DepthNet can map
the source face to any target geometry, provided by a target image, or its keypoints. We demonstrate
the application of this ﬂexibility for the face replacement task.

The aforementioned adversarial models use an identity preserving loss to maintain identity. The core
DepthNet model does not need identity labels and preserves well the identity (as shown in Figure 1

8

(right)). However, the identity information can be used by the proposed adversarial components, as in
background synthesis, to further improve the results. Unlike some of these models that take target
pose as input, DepthNet uses the target keypoints to estimate the target geometry and does not require
the target pose. This has several advantages; 1) DepthNet can map to the geometry of the target face
in addition to the pose, and 2) in the face replacement task, DepthNet can replace the target face with
the warped source face directly onto the target face location. Its application is shown in the face swap
experiment in Section 3.3.

4.3 Depth Estimation

Thewlis et. al [21] propose a mapping technique to learn a proxy of 2D landmarks in an unsupervised
way. A semi-supervised technique has been also proposed in [11] that improves landmark localization
by using weaker class labels (e.g. emotion or pose) and also by making the model predict equivariant
variations of landmarks when such transformations are applied to the image. Similar to these
approaches, DepthNet also maps a source to a target to learn its parameters. However, unlike these
two approaches that estimate 2D landmarks, DepthNet estimates the depth of the landmarks using
2D matching of keypoints, by formulating afﬁne parameters as a function of depth augmentated
keypoints in a closed form solution.

While several models [13; 2; 15] estimate depth with direct supervision, there has been recent models
[29; 5; 3] that estimate depth in an unsupervised training procedure. These models rely on pixel
reconstruction by using frames that are captured from very similar scenes, e.g. nearby frames of
a video [29] or left-right frames captures by stereo cameras [5; 3]. These models estimate depth
on one frame and then by using the disparity map, measure how pixel values of nearby frames
compare to each other. To do this, they also require camera intrinsic parameters, e.g. focal length or
distance between cameras. Unlike these models, our approach does not require source to target pixel
mapping. This allows mapping faces from different people with completely different skin colors,
without knowing camera parameters or how they are positioned with respect to each other. Therefore,
DepthNet is not susceptible to variations in illuminations or lighting between source and target faces.

Tung et. al [23] estimate 3D human pose in videos, where they use synthetic data to pre-train internal
parameters of the model and ﬁne-tune them by keypoint, segmentation and motion loss. Adversarial
Inverse Graphics Networks (AIGN) [24] estimates 3D human pose from 2D keypoint heatmaps in a
semi-supervised manner with a similar formulation to that of CycleGAN. They apply an adversarial
loss on the 3D pose to make them look realistic. These models leverage the depth values either
through synthetic data [23], or by adversarial usage of ground truth depth values [24]. Unlike these
models, DepthNet does not rely on any depth signal, either directly or indirectly. MOFA [20] builds
a 3D face mesh using a single image, where the 3D face parameters such as 3D shape and skin
reﬂectance are estimated by an encoder and then using a differentiable model they are rendered back
to the image by the decoder. This model requires manual initialization to map the input image to the
3D mesh, since otherwise it is doing an unconstrained optimization by adapting both the face pose
and the skin reﬂectance. Our model, however, does not require any manual initialization.

5 Conclusion

We have proposed a novel approach to 3D face model creation which enables pose normalization
without using any ground truth depth data. We achieve our best quantitative keypoint registration
results using our novel formulation for predicting depth and 3D visual geometry simultaneously,
learned by backpropagating through the analytic solution for the visual geometry estimation problem
expressed as a function of predicted depths. We have illustrated the quality and utility of the depths
and 3D transformations obtained using our method by transforming source faces to a wide variety of
target poses and geometries. Our technique can be used for face rotation and replacement and when
combined with adversarial repair it can blend warped faces to also synthesize the background. The
proposed model, however, carries forward emotion from source to target due to learning a shared
afﬁne parameters for all keypoints. Moreover, for extreme non-frontal faces, while DepthNet can
extract the transformation params (since it only relies on keypoints), OpenGL cannot extract texture
due to occlusion. We show an example of how to address this in the supplementary material. An
interesting extension to this paper can be replacing the OpenGL pipeline with a generative adversarial
framework that synthesizes a face using the parameters estimated by DepthNet.

9

6 Acknowledgments

We would like to thank Samsung and Google for partially funding this project. We are also thankful
to Compute Canada and Calcul Quebec for providing computational resources, and to Poonam Goyal
for helpful discussions.

References

[1] Blanz, Volker and Vetter, Thomas. A morphable model for the synthesis of 3d faces.

In
Proceedings of the 26th annual conference on Computer graphics and interactive techniques,
pp. 187–194. ACM Press/Addison-Wesley Publishing Co., 1999.

[2] Eigen, David, Puhrsch, Christian, and Fergus, Rob. Depth map prediction from a single image
using a multi-scale deep network. In Advances in Neural Information Processing Systems
(NIPS), pp. 2366–2374, 2014.

[3] Garg, Ravi, BG, Vijay Kumar, Carneiro, Gustavo, and Reid, Ian. Unsupervised cnn for single
view depth estimation: Geometry to the rescue. In Proceedings of the European Conference on
Computer Vision (ECCV), pp. 740–756. Springer, 2016.

[4] Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty of training deep feedforward
neural networks. In International conference on Artiﬁcial Intelligence and Statistics (AISTATS),
pp. 249–256, 2010.

[5] Godard, Clément, Mac Aodha, Oisin, and Brostow, Gabriel J. Unsupervised monocular depth
estimation with left-right consistency. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2017.

[6] Gross, Ralph, Matthews, Iain, Cohn, Jeffrey, Kanade, Takeo, and Baker, Simon. Multi-pie.

Image and Vision Computing, 28(5):807–813, 2010.

[7] Hassner, Tal. Viewing real-world faces in 3d.

In Proceedings of the IEEE International

Conference on Computer Vision (ICCV), pp. 3607–3614, 2013.

[8] Hassner, Tal, Harel, Shai, Paz, Eran, and Enbar, Roee. Effective face frontalization in uncon-
strained images. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 4295–4304, 2015.

[9] He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE
International Conference on Computer Vision (ICCV), pp. 1026–1034, 2015.

[10] Honari, Sina, Yosinski, Jason, Vincent, Pascal, and Pal, Christopher. Recombinator networks:
Learning coarse-to-ﬁne feature aggregation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pp. 5743–5752, 2016.

[11] Honari, Sina, Molchanov, Pavlo, Tyree, Stephen, Vincent, Pascal, Pal, Christopher, and Kautz,
Jan. Improving landmark localization with semi-supervised learning. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.

[12] Huang, Rui, Zhang, Shu, Li, Tianyu, and He, Ran. Beyond face rotation: Global and local
perception gan for photorealistic and identity preserving frontal view synthesis. In Proceedings
of the IEEE International Conference on Computer Vision (ICCV), Oct 2017.

[13] Jackson, Aaron S, Bulat, Adrian, Argyriou, Vasileios, and Tzimiropoulos, Georgios. Large pose
3d face reconstruction from a single image via direct volumetric cnn regression. In Proceedings
of the IEEE International Conference on Computer Vision (ICCV), pp. 1031–1039. IEEE, 2017.

[14] Jeni, László A, Cohn, Jeffrey F, and Kanade, Takeo. Dense 3d face alignment from 2d videos in
real-time. In IEEE International Conference and Workshops on Automatic Face and Gesture
Recognition (FG), volume 1, pp. 1–8. IEEE, 2015.

10

[15] Liu, Fayao, Shen, Chunhua, and Lin, Guosheng. Deep convolutional neural ﬁelds for depth
estimation from a single image. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 5162–5170, 2015.

[16] Parkhi, Omkar M, Vedaldi, Andrea, and Zisserman, Andrew. Deep face recognition. Proceedings

of the British Machine Vision Conference (BMVC), 2015.

[17] Sagonas, Christos, Tzimiropoulos, Georgios, Zafeiriou, Stefanos, and Pantic, Maja. 300 faces
in-the-wild challenge: The ﬁrst facial landmark localization challenge. In Proceedings of the
IEEE International Conference on Computer Vision Workshops (CVPRW), pp. 397–403, 2013.

[18] Shen, Yujun, Luo, Ping, Yan, Junjie, Wang, Xiaogang, and Tang, Xiaoou. Faceid-gan: Learning
a symmetry three-player gan for identity-preserving face synthesis. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 821–830, 2018.

[19] Taigman, Yaniv, Yang, Ming, Ranzato, Marc’Aurelio, and Wolf, Lior. Deepface: Closing the
gap to human-level performance in face veriﬁcation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 1701–1708, 2014.

[20] Tewari, Ayush, Zollhöfer, Michael, Kim, Hyeongwoo, Garrido, Pablo, Bernard, Florian, Perez,
Patrick, and Theobalt, Christian. Mofa: Model-based deep convolutional face autoencoder for
unsupervised monocular reconstruction. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), volume 2, 2017.

[21] Thewlis, James, Bilen, Hakan, and Vedaldi, Andrea. Unsupervised learning of object landmarks
by factorized spatial embeddings. In Proceedings of the IEEE International Conference on
Computer Vision (ICCV), volume 1, pp. 5, 2017.

[22] Tran, Luan, Yin, Xi, and Liu, Xiaoming. Disentangled representation learning gan for pose-
invariant face recognition. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017.

[23] Tung, Hsiao-Yu, Tung, Hsiao-Wei, Yumer, Ersin, and Fragkiadaki, Katerina. Self-supervised
learning of motion capture. In Advances in Neural Information Processing Systems (NIPS), pp.
5242–5252, 2017.

[24] Tung, Hsiao-Yu Fish, Harley, Adam W, Seto, William, and Fragkiadaki, Katerina. Adversarial
inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation from
unpaired supervision. In Proceedings of the IEEE International Conference on Computer Vision
(ICCV), volume 2, 2017.

[25] Yin, Xi, Yu, Xiang, Sohn, Kihyuk, Liu, Xiaoming, and Chandraker, Manmohan. Towards
large-pose face frontalization in the wild. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), pp. 1–10, 2017.

[26] Zhang, Xing, Yin, Lijun, Cohn, Jeffrey F, Canavan, Shaun, Reale, Michael, Horowitz, Andy,
Liu, Peng, and Girard, Jeffrey M. Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic
facial expression database. Image and Vision Computing, 32(10):692–706, 2014.

[27] Zhao, Jian, Xiong, Lin, Jayashree, Panasonic Karlekar, Li, Jianshu, Zhao, Fang, Wang, Zhecan,
Pranata, Panasonic Sugiri, Shen, Panasonic Shengmei, Yan, Shuicheng, and Feng, Jiashi. Dual-
agent gans for photorealistic and identity preserving proﬁle face synthesis. In Advances in
Neural Information Processing Systems (NIPS), pp. 66–76, 2017.

[28] Zhao, Jian, Cheng, Yu, Xu, Yan, Xiong, Lin, Li, Jianshu, Zhao, Fang, Jayashree, Karlekar,
Pranata, Sugiri, Shen, Shengmei, Xing, Junliang, et al. Towards pose invariant face recognition
in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 2207–2216, 2018.

[29] Zhou, Tinghui, Brown, Matthew, Snavely, Noah, and Lowe, David G. Unsupervised learning of
depth and ego-motion from video. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2017.

[30] Zhu, Jun-Yan, Park, Taesung, Isola, Phillip, and Efros, Alexei A. Unpaired image-to-image
translation using cycle-consistent adversarial networks. In Proceedings of the IEEE International
Conference on Computer Vision (ICCV), 2017.

11

Unsupervised Depth Estimation, 3D Face Rotation and
Replacement: Supplementary Information

S.1 Experimental Setup

The RCN, the DepthNets and the CycleGAN modules are trained separately. Each model is trained
using standard techniques for the model class and has a separate objective to be optimized. DepthNet
does not use the OpenGL pipeline during training and only uses it to render faces at test time, allowing
DepthNet to train faster.

S.1.1 DepthNet Experimental Details

Here we describe the details of the experiments carried out in Section 3.1. Our DepthNet architectures
require keypoints of both source and target images to be extracted. For this, the image is ﬁrst passed
through the VGG-Face [16] face detector. The face crops are then scaled down to 80 × 80 and
converted to greyscale, following which they are passed through RCN to obtain N = 68 keypoints
on each image. The RCN is trained exactly as described in [10], using the 300W dataset [17].

The keypoint only variant of our model involves concatenating all detected keypoints and passing
them through a two-layer deep fully connected network, with 256 hidden units and o output units.
The size of o depends on whether we are predicting only the depth, in which case o = N , or both
depth and afﬁne transformation parameters, in which case o = N + 8.

As discussed above, it is possible to augment these models with a Siamese CNN module (case A). In
the model variants that also use the image, we pass both the source and the target images through
three conv-maxpool layers with shared weights of size (32, 4, 2), (48, 3, 2), (64, 2, 2), respectively for
the representation (num_filters, filter_size, pool_size). The network’s outputs for the
source and target faces are then concatenated before passing them into a 4-layered fully connected
network with respective output sizes of 2048, 512, 256, and o. The keypoints are concatenated to the
512-unit layer before being passed to the last two layers. See Figure 1 (left) for an illustration of the
model. We explore these Siamese CNN augmented variants in models 4, 6 and 8 in Table 1.

We set the initial learning rate to 0.001 and use a Nesterov momentum optimizer (with a momentum
of 0.9) in all our experiments. With the exception of the last layer, we initialize all weights with
a Glorot initialization scheme [4], with the weights sampled from a uniform distribution. We use
a ReLU gain [9], set all biases to 0, and apply a ReLU non-linearity after every layer. In the ﬁnal
output layer, we do not apply any non-linearity and initialize the weights to 0. The biases of units that
represent depths are initialized to a random Normal distribution with µ = 0 and σ = 0.5, while those
that form the predicted afﬁne transform are initialized with the equivalent of a "ﬂattened" identity
transform. All models have been trained for 500 epochs.

We point out that except for a comparison between learning rates in the set {0.01, 0.001, 0.0001}
over few (less than 10) epochs, to ﬁnd a learning rate that the model seems to train well with, we
have not performed a hyperparameter search, and anticipate that the performance of the model can be
made even better by searching the hyperparameter space on a per model basis and by using deeper
(or modiﬁed) architectures.

S.2 Depth Visualization

We show further estimated depth values by different models in Figure S1. DepthNet+GAN and AIGN
generate the closest depth values to the GT depth. The baseline DepthNet model estimates reliable
depth values for most cases, however it has some degree of inaccuracy, as shown in the last two rows.
In DepthNet, the estimated depth indicates the position of each keypoint relative to other keypoints
rather than with respect to a source and importantly it is done without any supervision. MOFA, which
is also unsupervised, generates very similar face templates for different cases.

12

Figure S1: Depth visualization for different models (color coded by depth). The Depth axis is the one pointing
into the page. From left to right: RGB image, Ground Truth, DepthNet, DepthNet+GAN, AIGN and MOFA
estimated depth values.

S.3 Additional Camera Sweep Visualizations

In this section, we present additional visualizations along the lines of those shown in Figure 1 (right)
in Section 2 of the main paper. Frontal faces selected from the Multi-PIE dataset [6] are re-projected
to match several other poses corresponding to a person with a different identity. The DepthNets
predicts reliable proxy depths, which when coupled with the analytically obtained afﬁne transform
(obtained from the least-squares pseudo-inverse-based solution described in Section 2.3 of the main
paper) yields faces close to the desired target face geometry when passed through the OpenGL
pipeline.

We show camera sweeps for frontal source faces in Figures S2 and S3 and non-frontal source faces in
Figure S4. In Figure S5 we use the same target identity as the source face, showing how much the

13

Figure S2: Projecting a frontal face (far left) to a range of other poses deﬁned by faces in the row above.

generated face differs from the ground truth target. For all samples in Figures S2, S3, S4, and S5 we
use the DepthNet model that relies on only key-points (model 7 in Table 1).

Note that for non-frontal source faces, the quality of images is reduced specially for frontal target
faces. This is due to lack of adequate texture on the occluded side of the face to be transferred to the
target pose by OpenGL pipeline, rather than inaccuracies in the afﬁne transformation parameters. In
order to reduce the side-affects, we use either of the source face or its ﬂipped version, that is closer to
the target face pose, and then warp the face to the target keypoints.

Figure S3: Projecting a frontal face (far left) to a range of other poses deﬁned by faces in the row above.

Figure S4: Re-projecting a non-frontal face (far left) to a range of other poses deﬁned by faces in the row above.

Figure S5: Rotating a face (far left) to a range of other poses deﬁned by faces of the same identity in the row
above. On the top row frontal and on the bottom row non-frontal source faces are shown.

S.4 CycleGAN

Suppose we have some images belonging to one of two sets x ∈ X and y ∈ Y , where x denotes a
DepthNet-resulting face and y a ground truth face which is frontal. We wish to learn two functions

14

(cid:105)

(cid:105)

,

(cid:104)

Ex,y

min
G,F

(cid:104)

Ex,y

min
DX ,DY

F : X → Y and G : Y → X which are able to map an image from one set to the corresponding
image in the other. Correspondingly, we have two discriminators DX and DY which try to detect
whether the image in that particular set is real or generated. While we are only interested in the
function F : X → Y (since this is mapping to the distribution of ground truth faces) the formulation
of CycleGAN requires that we learn mappings in both directions during training. We optimize the
following objectives for the two generators F and G:

(cid:96)(DX (G(y)), 1) + (cid:96)(DY (F (x)), 1) + λ||y − F (G(y))||1 + λ||x − G(F (x))||1

(4)

And the following for the two discriminators DX and DY :

(cid:96)(DX (x), 1) + (cid:96)(DX (G(y)), 0) + (cid:96)(DY (y), 1) + (cid:96)(DY (F (x)), 0)

(5)

where 0/1 denote fake/real, (cid:96)() is the squared error loss and λ is a coefﬁcient for the cycle-consistency
(reconstruction) loss.

In the case where we did adversarial background synthesis, x is a channel-wise concatenation of the
DepthNet-frontalized face and the background of the original (pre-frontalized) image. For face-swap
cleanup, x is simply a source face which has been warped to a target face and pasted on top.

Once the network has been trained, we can disregard all other functions and use F to clean up faces
which are low quality due to artifacts from warping.

In terms of architectural details the generators and discriminators used were those described in the ap-
pendix of the CycleGAN paper [30]. In short, the generator consists of three conv-BN-relu blocks
which downsample the input, followed by nine ResNet blocks (which can be interpreted as iteratively
performing transformations over the downsampled representation), followed by deconv-BN-relu
blocks to upsample the representation back into the original input size. For training, we use the
same hyperparameters as most CycleGAN implementations which is using the Adam optimizer with
learning rate α = 2 × 10−4, β1 = 0.5, β2 = 0.999. However, instead of using a batch size of 1 we
use the largest possible batch size, which was 16 for a 12GB GPU.

Note that in order to produce better translations, the dataset we used for all CycleGAN experiments
contain both the VGG and the CelebA datasets, which has signiﬁcantly more images.

S.4.1 Background Synthesis

We present extra visualizations for the CycleGAN which performs background synthesis on CelebA,
corresponding to Figure 4 in the main paper. These are shown in Figure S6.

Figure S6: Background synthesis with CycleGAN. Left to right: source face; keypoints overlaid; DepthNet
(DN); DN + background → frontal

S.4.2 Face Replacement and Adversarial Repair

In Figure S7 we provide extra face swap samples on CelebA, corresponding to Figure 5 in the
main paper, where a source face is warped to a target face pose using DepthNet, pasted onto the

15

target image and then passed to a CycleGAN to adapt the face skin of the warped source face to the
background and hairstyle of the target face.

Figure S7: Face swap experiment with CycleGAN. Left to right: source face; target face; warp to target with
DepthNet; repaired result with CycleGAN. The source face is taken and warped onto the target face. The
background and hairstyle is then adapted to the target face.

S.4.3 Extreme Pose Face Clean-up

If the source image has an extreme pose, the texture will be missing on the occluded side of the
face and the OpenGL pipeline cannot rotate the face without artifacts. Note that this shortcoming is
due to lack of texture on the occluded side of the face rather than a deﬁciency of the transformation
parameters measured by DepthNet.

We performed an experiment using CycleGAN to ﬁx such artifacts. For this experiment we take
source images from CelebA and ﬁrst frontalize it by using DepthNet. Since the frontalized faces have
artifacts due to stretch of texture on the occluded side of the face by the OpenGL pipeline, we train a
CycleGAN that takes DepthNet frontalizaed faces plus the background of the original non-frontal
image (as two images) in one domain and the ground truth frontal faces in the other domain. The
CycleGAN learns to clean-up these artifacts. Finally, we take the GAN-repaired frontalized faces
and project it to different target poses using DepthNet. In Figure S8 we visualize camera sweep for
source faces in the wild that have extreme poses. The cycleGAN reasonably cleans the face artifacts
and then DepthNet projects it to different poses. This is just one approach to address the extreme
pose occlusion artifacts. We see alternative methods for addressing this issue as promising directions
for future research.

Figure S8: Re-projecting a non-frontal face (far left) from CelebA to a range of other poses deﬁned by faces in
the row above. Top row (in each pair) depicts the target faces from Multi-PIE [6]. The bottom row shows from
left to right: source face, souce face frontalized by DepthNet, adversarial-repaired face, the repaired source face
projected to the target poses (4th to 10th columns).

16

8
1
0
2
 
c
e
D
 
4
2
 
 
]

V
C
.
s
c
[
 
 
5
v
2
0
2
9
0
.
3
0
8
1
:
v
i
X
r
a

Unsupervised Depth Estimation,
3D Face Rotation and Replacement

Joel Ruben Antony Moniz1∗, Christopher Beckham2,3∗, Simon Rajotte2,3,
Sina Honari2, Christopher Pal2,3,4
1Carnegie Mellon University, 2Mila-University of Montreal, 3Polytechnique Montreal, 4Element AI
1jrmoniz@andrew.cmu.edu, 2honaris@iro.umontreal.ca, 3firstname.lastname@polymtl.ca

Abstract

We present an unsupervised approach for learning to estimate three dimensional
(3D) facial structure from a single image while also predicting 3D viewpoint
transformations that match a desired pose and facial geometry. We achieve this
by inferring the depth of facial keypoints of an input image in an unsupervised
manner, without using any form of ground-truth depth information. We show how
it is possible to use these depths as intermediate computations within a new back-
propable loss to predict the parameters of a 3D afﬁne transformation matrix that
maps inferred 3D keypoints of an input face to the corresponding 2D keypoints on
a desired target facial geometry or pose. Our resulting approach, called DepthNets,
can therefore be used to infer plausible 3D transformations from one face pose
to another, allowing faces to be frontalized, transformed into 3D models or even
warped to another pose and facial geometry. Lastly, we identify certain shortcom-
ings with our formulation, and explore adversarial image translation techniques as
a post-processing step to re-synthesize complete head shots for faces re-targeted to
different poses or identities. 1

1

Introduction

Face rotation is an important task in computer vision.
It has been used to frontalize faces for
veriﬁcation [8; 19; 25; 28] or to generate faces of arbitrary poses [22; 18]. In this paper we present
a novel unsupervised learning technique for face rotation and warping from a 2D source image –
whose facial appearance will be used in the rotation – to a target face – to which the facial pose
and geometry inferred from the source image is mapped. A use case is when we have an image of
someone in a particular target pose and we want to put a given source face into that pose, without
knowing the exact target face pose. This can be leveraged, for example, in the advertisement industry,
when putting someone in a particular location can be costly or unfeasible, or in the movie industry
when the main actor’s limited time or high cost can enforce using another actor whose face can be
later replaced by the main actor’s. This is achieved through estimating the source face depth and
the 3D afﬁne parameters that warp the source to the target face using neural networks. These neural
networks use a novel loss formulation for the structured prediction of keypoint depths. Once the 3D
afﬁne transformation matrix is estimated, it can be used to warp the source image onto the target
face geometry using a textured triangular mesh. The use of a 3D afﬁne transform means that we
can capture both a 3D rotation of the face to a new viewpoint as well as a global non-Euclidean
warping of the geometry to match a target face. We call these neural networks Depth Estimation-Pose
Transformation Hybrid Networks, or DepthNets in short.

Our ﬁrst contribution is to propose a neural architecture that predicts both the depth of source
keypoints as well as the parameters of a 3D geometric afﬁne transformation which constitute the

∗Indicates equal contribution.
1Code will be released at: https://github.com/joelmoniz/DepthNets/

32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.

explicit outputs of the DepthNet model. The predicted depth and afﬁne transformation could be then
used to map a source face to a target face for object orientation, distortion and viewpoint changes.

Our second contribution consists of making the observation that given 3D source and 2D target
keypoints, closed form least squares solutions exist for estimating geometric afﬁne transformation
models between these sets of keypoint correspondences, and we can therefore develop a model that
captures the dependency between depth and the afﬁne transformation parameters. More speciﬁ-
cally, we express the afﬁne transformation as a function of the pseudoinverse transformation of 2D
keypoints in a source image – augmented by inferred depths – and the target keypoints. Thus, the
second and major contribution in this work is capturing the relationship between an estimated afﬁne
transformation and the inferred depth as a deterministic relationship. In this formulation, DepthNet
only predicts depth values explicitly and the afﬁne parameters are inferred through a pseudoinverse
transformation of source and target keypoints. Here, one can directly optimize through the solutions
of what might otherwise be formulated as a secondary minimization step.

Our proposed DepthNet can map the central region of the source face to the target geometry. This
leads to background mismatch when warping one face to another. Finally, our third contribution is to
use an adversarial unpaired image-to-image transformation approach to repair the appearance of 3D
models inferred from DepthNet. Together these contributions allow 3D models of faces that construct
realistic images in the target pose. Our proposed method can be used for pose normalization or face
swaps with no manually speciﬁed 3D face model. To the best of our knowledge, this is the ﬁrst such
neural network based model that estimates a 3D afﬁne transformation model for face rotation which
neither requires ground-truth 3D images nor any ground truth 3D face information such as depth.

As we have outlined above, our approach uses neural networks for inferring depth and geometric
transformation – referred to as DepthNets; and, an adversarial image-to-image transformation network
which improves the quality of the appearance of a 3D model inferred from a DepthNet.

2 Our Approach

DepthNets

We propose three DepthNet formulations, described in Sections 2.1, 2.2, and 2.3. For each of the
three models we explore two architectural scenarios: (A) a Siamese-like architecture that uses the
source and target images themselves as well as keypoints extracted from these images, and (B) a
fully-connected neural network variant which uses only facial keypoints in the source and target
images. See Figure 1 (left) for details.

Figure 1: (Left) DepthNet architecture. The blue region is only used in case (A) and the red part is used in
both cases (A) and (B), described in Section 2. The orange output (the 8 afﬁne transformation parameters) is
predicted only by model variations described in Sections 2.1 and 2.2, and not the model described in section
2.3. All three models predict the N depth values of the source keypoints. C, P, and FC correspond to valid conv,
pool and fully-connected layers. The two paths of Siamese network share parameters and the black dots indicate
concatenating keypoint values to FC units. (Right) Visualizing face rotation by re-projecting a frontal face (far
left) to a range of other poses deﬁned by the faces in the row above (in each pair of rows). In this experiment, we
only use keypoints from the top-row in the DepthNet model (Model 7 in Table 1).

It is interesting to note that if DepthNets are used to register a set of images of objects to the same
common viewpoint, the same image and geometry can be used as the target. This is the case for
the frontalization of faces, for example. While the DepthNet framework is sufﬁciently general to be
applied to any object type where 2D keypoint detections have been made, our experiments here focus
on faces. We describe the three variants of DepthNets below.

2

2.1 Predicting Depth and Viewpoint Separately

In this variant of DepthNets, the model predicts both depths and viewpoint geometry, but as separate
explicit outputs of a neural network. The input is comprised of only the geometry and pose of the
source and target faces (encoded in the form of a 2D keypoint template), in case (B), or both keypoints
and images of the source and target faces, in case (A). The key phases of this stage are described by
the sequence of steps given below:

1. Keypoint extraction: Raw (x, y) pixel coordinates corresponding to the keypoints of each image
are extracted using a Recombinator Network (RCN) [10] architecture, and then concatenated before
being passed into the keypoint processing step.
2. (Optional) Image Feature Extraction: DepthNets can be conditioned on only keypoints, case (B),
or on keypoints and the original images, case (A). We can therefore optionally subject the source and
target images to alternating conv-maxpool layers. If this component of the architecture is used, the
last spatial feature maps in the Siamese architecture are concatenated before being given to a set of
densely connected hidden layers.
3. Keypoint processing: In this step keypoints are passed through a set of hidden layers. If the Image
Feature Extraction stage is used, the keypoints are concatenated to image features, the output of
which is in turn fed to densely connected layers. The output layer of this phase will be of size N + 8,
where N is the number of keypoints. The ﬁrst N points represent the Depth proxy, and the last 8
points form a 4 × 2 matrix representing the learned parameters of the afﬁne transform. See Figure 1.
4. Geometric Afﬁne Transformation Normalizer: This phase applies the predicted afﬁne transform
on each (depth augmented) source keypoint to estimate its target location. Let (xi
s) represent the
ith source keypoint, (xi
n) the corresponding source normalized keypoint estimated by applying
the afﬁne transformation matrix, (xi
t) the ith target keypoint (as ground truth (GT)), and Is and It
represent the source and target images respectively. Depending on which underlying architectural
variant we use, two cases arise: one that utilizes only the keypoints (B), and another utilizing
both the keypoints and the images (A). Since the keypoints are generated using RCNs, they are
technically functions of the input images: [xs, ys] = R(Is), and [xt, yt] = R(It). Depending
on the (A) or (B) variant, the ith keypoint’s predicted depth proxy zi
p is inferred as a function
of the input keypoints, or both input keypoints and input images.
In both cases the keypoints
are derived from the images, so zi
p(Is, It). Similarly, the 3D-2D afﬁne transform F is a
function of the images, such that F = F(Is, It), where the 8 predicted parameters are: m =
{m1, m2, m3, tx, m4, m5, m6, ty)}. These constitute the 3D-2D afﬁne transform which is used by
all keypoints. In other words, each of the i points is transformed using xi
s, or:

n = F(Is, It) xi

p = zi

n, yi

s, yi

t, yi

(cid:20) xi
n
yi
n

(cid:21)

=

(cid:20)m1 m2 m3
m4 m5 m6






(cid:21)

tx
ty

xi
s
yi
s
zi
p(Is, It)
1






The loss function of a DepthNet is obtained by transforming the source face to match the target face
using the simple squared error of the corresponding target object’s keypoint vector xt = [xt, yt]T
, as GT values, and the source object’s normalized keypoint vector [xn, yn]T . The loss for one
example where we predict depth and afﬁne viewpoint geometry can therefore be expressed as:

L =

K
(cid:88)

i=1

(cid:13)
(cid:13)xi
(cid:13)

t − F(Is, It) [xi

s yi

s zi

p(Is, It)]T (cid:13)
2
(cid:13)
(cid:13)

(1)

5. Image Warper: This phase consists of using the depth proxy and afﬁne transform matrix generated
to actually warp the face from its source pose to be matched to the target object geometry. The ﬁnal
projection to 2D is achieved by simply dropping the transformed z coordinate (which corresponds
to an orthographic projection model). In the case of DepthNets, this orthographic projection is
effectively embedded in the Geometric Afﬁne Transformation Normalizer step, since the afﬁne
corresponding to the z coordinate is not predicted, essentially dropping it.
As we operate on keypoints, the actual warping of pixels can be performed with a high quality
OpenGL pipeline that performs the warp separately from the rest of the architecture. Source image,
keypoints augmented with depth, and the afﬁne matrix are passed to OpenGL pipeline to warp the
source image towards the target pose. This OpenGL warping is not needed during DepthNet training,
which means we do not have to do feedforward or backprop through OpenGL. In Summary, for step 1
the RCN model [10] is used, for steps 2 to 4 the DepthNet model, shown in Figure 1 (left), is trained,

3

and for step 5 an OpenGL pipeline is used. No data or parameters are needed to train the OpenGL
pipeline. It warps images by directly using the provided data.

2.2 Estimating Viewpoint Geometry as a Second Step

In this model variant, training is similar to Section 2.1 and the model outputs depth and 3D
afﬁne transformation parameters. However, at test time, rather than using the predicted 3D afﬁne
transformation for pairs of faces, we use only the predicted depths and estimate the afﬁne ge-
ometry parameters as a second estimation step. More precisely, given 3D points for a scene
and the corresponding 2D points for a target geometry it is possible to formulate the estima-
tion of a 3D afﬁne transformation as a linear least squares estimation problem. An overdeter-
mined system of the form Am = xt for this problem can be constructed as shown in (2).

This corresponds to an afﬁne camera model
followed by an orthographic projection to
2D keypoints. This setup also leads to
the following closed form solution for the
afﬁne transformation parameters:

m = [AT A]−1AT xt,

(3)

y1
s
0
y2
s
0

z1
s
0
z2
s
0

x1
s
0
x2
s
0













xK
s
0

yK
s
0

zK
s
0

0
x1
s
0
x2
s

0
xK
s

0
y1
s
0
y2
s
...
0
yK
s

0
z1
s
0
z2
s

1 0
0 1
1 0
0 1

0
zK
s

1 0
0 1





































m1
m2
m3
m4
m5
m6
tx
ty

























x1
t
y1
t
x2
t
y2
t
...
xK
t
yK
t

=

(2)

where this pseudoinverse based transformation is parameterized by the reference points and their
predicted depths.

2.3

Joint Viewpoint and Depth Prediction

Our key observation is that one can alternatively use the closed form analytical solution, measured in
Eq. (3), for the least squares estimation problem as the underlying afﬁne transformation matrix within
the loss function. This leads to a special form of structured prediction problem for geometrically
consistent depths and afﬁne transformation matrix. For each image we have L =

K
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:21)

(cid:20) xi
t
yi
t
(cid:124) (cid:123)(cid:122) (cid:125)
xi
t

(cid:20)m1 m2 m3
m4 m5 m6

−

(cid:124)

(cid:123)(cid:122)
m

tx
ty

(cid:21)

(cid:125)







(cid:124)

xi
s
yi
s
zi
p(Is, It)
1
(cid:123)(cid:122)
xi
s







(cid:125)

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

K
(cid:88)

i=1

(cid:13)
(cid:13)xi

t − reshape(cid:2)[AT A]−1AT xt

(cid:3)xi

s

(cid:13)
2
(cid:13)

where the matrix A is parameterized as a function of xs as shown in Eq. (2). In this variant, the
model explicitly outputs only depth values during train and test time. The afﬁne transformation matrix
in the equation above is replaced by Eq. (3), which measures the afﬁne transformation as a pure
function of source and target keypoints plus the inferred depth. The big difference of this formulation
compared to Sections 2.1 and 2.2 is that geometric afﬁne transformation parameters are no longer
predicted by DepthNet during training and at both train and test time – it solves the least square loss
through the pseudoinverse based transformation. Since zi
t }j=1...N ) is predicted
within the analytical formulation of the solution to the least squares minimization problem, we can
backpropagate through the solution of a minimization problem that depends on the predicted depths.
While we leverage keypoints for depth estimation, the proposed approach is novel in how the depth is
estimated. Note that it is unsupervised with respect to depth labels. No depth supervision either by
using depth targets (as in [2; 13; 15]), or by using depth in an adversarial setting (as in [24]), is used
to estimate depth values for the base DepthNet models described in Sections 2.1, 2.2, and 2.3.

s = zi

p({xj

s, xj

t , yj

s, yj

The depths learned for keypoints by these approaches are not necessarily true depths, but are likely to
strongly correlate with the actual depth of each keypoint. This is because even though the method
succeeds (as we shall see below) in aligning poses, the inferred depth and the afﬁne transform may
each be scaled by factors so as to cancel each other out (i.e., by factors which are multiplicative
inverses of each other). Real world viewpoint geometry also involves perspective projection.

Adversarial Image-to-Image Transformation

DepthNet transforms the central region of the source face to the target pose. Inevitably, the face
background will be missing, which might make the proposed method unsuitable for many application

4

where the full face is required. To address this issue, we utilize CycleGAN [30], an adversarial
image-to-image translation technique. This serves to repair the background of faces that have
undergone frontalization or face swap through the DepthNet pipeline. Importantly, the adversarial
nature of CycleGAN allows one to perform image transformation between two domains without
the requirement of paired data. In our work, we perform experiments translating between various
domains of interest but one example is translating between the domain of images in the dataset (i.e.
the ground truth) and the domain of images where the DepthNet output is pasted onto the face region
(in the case of face-swap). By doing so we clean the face background in an unsupervised manner.

3 Experiments

3.1 DepthNet Evaluation on Paired Faces

For the experiments in this section, we use a subset of the VGG dataset [16], with training and
validating on all possible pairs of images belonging to the same identity for 2401 identities. This
yields 322,227 train and 43,940 validation pairs. Check experimental setup details in Supplementary.

Model

Color MSE MSE_norm

1) A simple 2D afﬁne registration
2) A 3D afﬁne registration model using an average 3D face template
3) A DepthNet that separately estimates depth and geometry
4) The model above, but with a Siamese CNN image model
5) Secondary least squares estimation for visual geometry using the depths from 3)
6) Secondary least squares estimation for visual geometry using the depths from 4)
7) Backpropagation through the pseudoinverse based solution for visual geometry
8) The model above, but with a Siamese CNN image model

grey
purple
brown
violet
red
green
orange
blue

1.562
0.724
0.568
0.539
0.400
0.399
0.357
0.349

9.547
7.486
6.292
6.115
5.184
5.175
4.932
4.891

Table 1: (left) Comparing the Mean Squared Error (MSE) and MSE normalized by inter-ocular distance
(MSE_norm) of different models. (right) Histogram of Mean Squared Errors. The second column in the Table
(on left) corresponds to the color of the model in the ﬁgure (on right).

We explore the three variants of DepthNets described in Sections 2.1, 2.2, and 2.3, each with two
architectural cases (A) and (B), depending on whether image features are used in addition to keypoints
or not. We also compare with a number of baselines. We measure the mean square error (MSE)
between the estimated keypoints on the target face (source face normalized keypoints) and ground
truth target keypoints. Results for the following models are shown in Table 1:

1) A baseline model registrations using a simple 2D afﬁne transformation.

2) We generate a 3D average face template from the 3DFAW dataset [14; 26; 6] by aligning the 3D
keypoints of all faces in the dataset to a front-facing face using Procrustes superimposition. We report
error by mapping the template face to each source face via Procrustes superimposition (to get a 3D
face f ) and then use an afﬁne transformation from the 3D face f to the target face.

3, 4) We use our proposed approach to predict both depth and geometry (described in Sections 2.1).

5, 6) These models described in Section 2.2. Note that during training, these two cases are similar to
models 3 and 4 in Table 1.

7, 8) The pseudo-inverse formulation model described in Section 2.3.

As observed in Table 1, a simple 2D afﬁne transform (model 1) without estimating depth and a
template 3D face (model 2) get high errors on mapping to the target faces. DepthNet models get lower
errors and the pseudo-inverse formulation (models 7 and 8) further reduces the error by 10%. The
CNN models slightly reduce errors compared to their equivalent models that rely only on keypoints.

3.2 DepthNet Evaluation on Unpaired Faces and Comparison to other Models

In this section we train DepthNet on unpaired faces belonging to different identities and compare
with other models that estimate depth. We use the 3DFAW dataset [14; 26; 6] that contains 66
3D keypoints to facilitate comparing with ground truth (GT) depth. It provides 13,671 train and
4,500 valid images. We extract from the valid set, 75 frontal, left and right looking faces yielding
a total of 225 test images, which provides a total of 50,400 source and target pairs. We train the
psuedoinverse DepthNet model that relies on only keypoints (model 7 in Table 1). We also train
a variant of DepthNet that applies an adversarial loss on the depth values (DepthNet+GAN). This
model uses a conditional discriminator that is conditioned on 2D keypoints and discriminates GT
from estimated depth values. The model is trained with both keypoint and adversarial losses.

5

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)

Source

Target

Left
Front
Right

DepthNet

DepthNet + GAN

Left
24.67
25.54
21.66

Front Right
29.70
27.71
26.19
27.22
23.87
21.48

Avg
27.36
26.32
22.34

Left
59.78
58.77
59.97

Front Right
59.63
59.67
58.61
58.67
59.60
59.70

Avg
59.69
58.68
59.76

Table 2: Comparing DepthCorr for different DepthNet models when mapping variant source to target poses. The
Avg column measures the average over the three preceding columns.

We measure the correlation matrix between GT and estimated depths, where the element k in the
diagonal indicates the correlation between estimated and ground truth depth values for keypoint k,
yielding a value between -1 and 1. We report the sum of absolute values of the diagonal of this
matrix, indicated by DepthCorr. We compare DepthNet models on DepthCorr in Table 2. For this
experiment we take every possible pair of source to target faces, where source and target are one of
{left, front, right} looking faces. This yields a total of 5,550 pairs when the source and the target are
from the same subset, and 5,625 pairs otherwise. This experiment measures the accuracy of depth
estimation of the DepthNet models on different orientations of source-target faces. The baseline
DepthNet model that does not leverage the depth labels performs well in different cases. DepthCorr
improves more than twice for the DepthNet+GAN model, indicating a direct supervision loss using
depth labels can enhance the depth estimation.

Model

GT Depth
AIGN [24]
MOFA [20]
DepthNet (Ours)
DepthNet + GAN (Ours)

Need Depth Manual Init. MSE (×10−5) Depth Correlation Matrix Trace (DepthCorr)
Right pose
66
49.04
17.54
22.34
59.76

Front pose
66
50.81
15.97
26.32
58.68

8.86 ± 6.55
9.06 ± 6.61
8.75 ± 6.33
7.65 ± 6.97
8.74 ± 6.24

Left pose
66
44.08
11.14
27.36
59.69

-
No
Yes
No
No

Yes
Yes
No
No
Yes

Table 3: Comparing MSE and DepthCorr for different models. A lower MSE indicates the model maps better to
the target faces. A higher DepthCorr indicates more correlation between estimated and GT depths.

We compare our two DepthNet models with three baselines: 1) AIGN [24], 2) MOFA [20] and 3)
GT Depth (no model trained). AIGN estimates 3D keypoints conditioned on 2D heatmaps of the
keypoints. MOFA estimates a 3D mesh using only an image. We implemented the AIGN model
and asked the authors of MOFA to run their model on our test-set. They provided MOFA’s results
for 134 images in the test set. In Table 3 we compare these three models with our DepthNet models
on DepthCorr. We also compare them on MSE, which is measured between GT and estimated
target keypoints. Since the three baselines estimate depth on a single image due to their different
model formulation, we ﬁrst measure m using closed form solution in Eq. 3 and then apply m to
the estimated source keypoints to get the target keypoint estimations. We contrast the estimated
values with the GT target keypoints. As shown in Table 3, GT depth has the highest DepthCorr
(the maximum possible value). The depths estimated by DepthNet+GAN and AIGN have stronger
correlation to GT depth compared to the baseline DepthNet and MOFA, while baseline DepthNet
performs better than MOFA. On MSE the baseline DepthNet model gets smaller MSE when mapping
to target faces, indicating it is better suited for this task.

In Figure 2 we plot heatmaps of the estimated depth of different models (on Y axis) and the GT depth
(on X axis) aggregated over all 66 keypoints on all test data. As can be seen, the depth estimated
by DepthNet+GAN and AIGN models form a 45 degree rotated ellipses showing a stronger linear
correspondence with respect to the GT depth compared to the the baseline DepthNet and MOFA.

Figure 2: Predicted (Y axis) versus Ground Truth (X axis) depth heatmaps for different models.

In Figure 3 we show some estimated depth samples for different models (see more samples in Figure
S1). AIGN and DepthNet+GAN generate more realistic results. MOFA generates very similar
face templates for different poses. Baseline DepthNet estimates reliable depth values in most cases,
however it has some failure modes as shown in the last row.

6

By comparing different models in Table 3, MOFA requires proper initialization to map face meshes
to each image. AIGN requires depth labels to train the model. Our baseline DepthNet model neither
require any depth labels nor any manual tuning. The results also show DepthNet can work well on
unpaired data. We would also like to emphasize that MOFA and AIGN are designed to estimate a 3D
model, while DepthNet is designed to estimate the parameters that facilitate warping a face pose to
another without having depth values, so these models are designed to solve different problems.

Figure 3: Depth visualization for different models (color coded by depth). From left to right: RGB image,
Ground Truth, DepthNet, DepthNet+GAN, AIGN and MOFA estimated depth values.

An interesting observation is that GT depth gets a higher MSE compared to DepthNet. This can be
due to not having a perspective projection between source and target faces. However, since DepthNet
is trained to map to the target faces, it learns the afﬁne parameters in a way to minimize this loss.

3.3 Face Rotation, Replacement and Adversarial Repair

In this section we show how DepthNet can be used for different applications. In Figure 1 (right) we
visualize the face rotation by re-projecting a frontal face, from Multi-PIE [6], (far left) to a range of
other poses deﬁned by the faces in the row above. Since DepthNet (case B) computes transformation
on keypoints rather than pixels it is robust to illuminations changes between source and target faces.
See Figures S2 to S5 for further examples. Note that DepthNet preserves well the identity. However,
it carries forward the emotion from source to target since using a global afﬁne transformation imparts
a degree of robustness to dramatic expression changes. The views in these ﬁgures are rendered from
a 3D model in OpenGL. Note the model can align well to the target face poses.

Figure 4: Background synthesis with CycleGAN. Left to right: source face; keypoints overlaid; DepthNet (DN);
DN + background → frontal;

In another experiment, we do face frontalization with synthesized background. Here we use Cy-
cleGAN to add background detail to a face that has been frontalized with DepthNet. Referring to
Figure 4, we perform this by conditioning the CycleGAN on the DepthNet image (column 3) and the
background of column 2 (masking interior face region determined by the convex hull spanned by
the keypoints). The second domain contains ground truth frontal faces. This experiment shows how
to leverage DepthNet for full face generation. Note that we do not use identity information in this
experiment. However, it can be used to better preserve the identity.

Figure 5: Left to right: source face; target face; warp to target; repaired result.

7

Finally, we do face swaps, where we warp the face of one identity onto the geometry and background
of another identity using DepthNet. To do so, we paste the rotated face by DepthNet onto the
background of the target image and train a CycleGAN to map from the domain of ‘swapped in faces’
to the ground truth faces in our dataset, effectively learning to clean up face swaps so that the face
region matches the hair and background. Some examples of this procedure are shown in Figure 5.

4 Related Work

4.1

3D Transformation on Faces

While there is a large body of literature on 3D facial analysis, many standard techniques are not
applicable to our setting here. As an example, morphable models [1] cover a wide variety of
approaches which are capable of high quality 3D reconstructions, but such methods usually require
3D face scans or reconstructions from multi-view stereo to be assembled so as to learn complex
parametric distributions over face shapes. A close approach to our own is that of [7] on viewing
real world faces in 3D. Similar to our work, this approach does not require aligned 3D face scans,
highly engineered models or manual interventions. They make the observation that if 2D keypoints
can be obtained from a single input image of a face and these keypoints are matched to an arbitrary
3D target geometry, then standard camera calibration techniques can be used to estimate plausible
intrinsics and extrinsics of the camera. This allows the estimated camera matrix, 3D rotation matrix
and 3D translation vector to be used to transform the target 3D model to the pose of the query image
from which an approximate depth can be obtained. Hassner et. al [8] explore the use of a single
unmodiﬁed 3D surface as an approximation to the shape of all input faces. In contrast, our approach
only requires 2D keypoints from the source and target faces as input. It then estimates the depth of
the source face keypoints, thereby inferring an image speciﬁc 3D model of the face.

DeepFace [19] uses face frontalization to improve the performance of a face veriﬁcation system. It
uses a 3D mask composed of facial keypoints, detects the corresponding locations of these keypoints
in the image, and maps the 2D keypoints onto a 3D face model to frontalize it. DeepFace, however,
maps to a template 3D face, therefore always mapping to a speciﬁc pose and geometry. DepthNet, on
the other hand, can map to any pose and geometry, giving it more expressive ﬂexibility.

4.2 Generative Adversarial Networks on Face Rotation

Recently, adversarial models in [12; 22; 25; 27; 18; 28] have explored face rotation. TP-GAN [12]
performs face frontalization through introducing several losses to preserve identity and symmetry of
the frontalized faces. PIM [28] frontalizes faces in a composed adversarial loss and then extracts pose
invariant features for face recognition. These models are mainly aimed for face veriﬁcation, where
they can only do face-frontalization. Another limitation of these models is in requiring ground truth
frontal images of the same identity during training. DR-GAN [22] rotates faces to any target pose by
using a discriminator that also does identity classiﬁcation in addition to pose prediction, to preserve
id and pose. While these models do pure face rotation of a 2D face, our model can warp the input
face to any other target face, allowing warping the input face to any other identity, with a different
geometry and pose. Moreover, our model also estimates the 3D geometric afﬁne transformation
parameters explicitly, allowing these parameters to be used later, e.g., for face texture swap.

FF-GAN [25], DA-GAN [27], and FaceID-GAN [18] estimate parameters of either a 3D Morphable
Model (3DMM), as in [25; 18], or source to target pose transformation, as in [27]. FF-GAN uses
3DMM parameters to frontalize faces in an adversarial approach, while FaceID-GAN uses the
3DMM parameters to generate any target pose. These models, however, train 3DMM on ground
truth labels such as identity, expression and pose. DepthNet, on the other hand, estimates depth and
afﬁne transformation parameters without requiring ground truth afﬁne or depth labels or pre-training.
Similar to DepthNet, DA-GAN [27] estimates parameters of an afﬁne transformation model that
maps a 2D face to a 3D face. Unlike DepthNet that estimates depth on the source face, DA-GAN
uses depth in a template target face. While their approach eliminates the need for depth estimation, it
only allows the source face to be mapped to the target template geometry, while DepthNet can map
the source face to any target geometry, provided by a target image, or its keypoints. We demonstrate
the application of this ﬂexibility for the face replacement task.

The aforementioned adversarial models use an identity preserving loss to maintain identity. The core
DepthNet model does not need identity labels and preserves well the identity (as shown in Figure 1

8

(right)). However, the identity information can be used by the proposed adversarial components, as in
background synthesis, to further improve the results. Unlike some of these models that take target
pose as input, DepthNet uses the target keypoints to estimate the target geometry and does not require
the target pose. This has several advantages; 1) DepthNet can map to the geometry of the target face
in addition to the pose, and 2) in the face replacement task, DepthNet can replace the target face with
the warped source face directly onto the target face location. Its application is shown in the face swap
experiment in Section 3.3.

4.3 Depth Estimation

Thewlis et. al [21] propose a mapping technique to learn a proxy of 2D landmarks in an unsupervised
way. A semi-supervised technique has been also proposed in [11] that improves landmark localization
by using weaker class labels (e.g. emotion or pose) and also by making the model predict equivariant
variations of landmarks when such transformations are applied to the image. Similar to these
approaches, DepthNet also maps a source to a target to learn its parameters. However, unlike these
two approaches that estimate 2D landmarks, DepthNet estimates the depth of the landmarks using
2D matching of keypoints, by formulating afﬁne parameters as a function of depth augmentated
keypoints in a closed form solution.

While several models [13; 2; 15] estimate depth with direct supervision, there has been recent models
[29; 5; 3] that estimate depth in an unsupervised training procedure. These models rely on pixel
reconstruction by using frames that are captured from very similar scenes, e.g. nearby frames of
a video [29] or left-right frames captures by stereo cameras [5; 3]. These models estimate depth
on one frame and then by using the disparity map, measure how pixel values of nearby frames
compare to each other. To do this, they also require camera intrinsic parameters, e.g. focal length or
distance between cameras. Unlike these models, our approach does not require source to target pixel
mapping. This allows mapping faces from different people with completely different skin colors,
without knowing camera parameters or how they are positioned with respect to each other. Therefore,
DepthNet is not susceptible to variations in illuminations or lighting between source and target faces.

Tung et. al [23] estimate 3D human pose in videos, where they use synthetic data to pre-train internal
parameters of the model and ﬁne-tune them by keypoint, segmentation and motion loss. Adversarial
Inverse Graphics Networks (AIGN) [24] estimates 3D human pose from 2D keypoint heatmaps in a
semi-supervised manner with a similar formulation to that of CycleGAN. They apply an adversarial
loss on the 3D pose to make them look realistic. These models leverage the depth values either
through synthetic data [23], or by adversarial usage of ground truth depth values [24]. Unlike these
models, DepthNet does not rely on any depth signal, either directly or indirectly. MOFA [20] builds
a 3D face mesh using a single image, where the 3D face parameters such as 3D shape and skin
reﬂectance are estimated by an encoder and then using a differentiable model they are rendered back
to the image by the decoder. This model requires manual initialization to map the input image to the
3D mesh, since otherwise it is doing an unconstrained optimization by adapting both the face pose
and the skin reﬂectance. Our model, however, does not require any manual initialization.

5 Conclusion

We have proposed a novel approach to 3D face model creation which enables pose normalization
without using any ground truth depth data. We achieve our best quantitative keypoint registration
results using our novel formulation for predicting depth and 3D visual geometry simultaneously,
learned by backpropagating through the analytic solution for the visual geometry estimation problem
expressed as a function of predicted depths. We have illustrated the quality and utility of the depths
and 3D transformations obtained using our method by transforming source faces to a wide variety of
target poses and geometries. Our technique can be used for face rotation and replacement and when
combined with adversarial repair it can blend warped faces to also synthesize the background. The
proposed model, however, carries forward emotion from source to target due to learning a shared
afﬁne parameters for all keypoints. Moreover, for extreme non-frontal faces, while DepthNet can
extract the transformation params (since it only relies on keypoints), OpenGL cannot extract texture
due to occlusion. We show an example of how to address this in the supplementary material. An
interesting extension to this paper can be replacing the OpenGL pipeline with a generative adversarial
framework that synthesizes a face using the parameters estimated by DepthNet.

9

6 Acknowledgments

We would like to thank Samsung and Google for partially funding this project. We are also thankful
to Compute Canada and Calcul Quebec for providing computational resources, and to Poonam Goyal
for helpful discussions.

References

[1] Blanz, Volker and Vetter, Thomas. A morphable model for the synthesis of 3d faces.

In
Proceedings of the 26th annual conference on Computer graphics and interactive techniques,
pp. 187–194. ACM Press/Addison-Wesley Publishing Co., 1999.

[2] Eigen, David, Puhrsch, Christian, and Fergus, Rob. Depth map prediction from a single image
using a multi-scale deep network. In Advances in Neural Information Processing Systems
(NIPS), pp. 2366–2374, 2014.

[3] Garg, Ravi, BG, Vijay Kumar, Carneiro, Gustavo, and Reid, Ian. Unsupervised cnn for single
view depth estimation: Geometry to the rescue. In Proceedings of the European Conference on
Computer Vision (ECCV), pp. 740–756. Springer, 2016.

[4] Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty of training deep feedforward
neural networks. In International conference on Artiﬁcial Intelligence and Statistics (AISTATS),
pp. 249–256, 2010.

[5] Godard, Clément, Mac Aodha, Oisin, and Brostow, Gabriel J. Unsupervised monocular depth
estimation with left-right consistency. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2017.

[6] Gross, Ralph, Matthews, Iain, Cohn, Jeffrey, Kanade, Takeo, and Baker, Simon. Multi-pie.

Image and Vision Computing, 28(5):807–813, 2010.

[7] Hassner, Tal. Viewing real-world faces in 3d.

In Proceedings of the IEEE International

Conference on Computer Vision (ICCV), pp. 3607–3614, 2013.

[8] Hassner, Tal, Harel, Shai, Paz, Eran, and Enbar, Roee. Effective face frontalization in uncon-
strained images. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 4295–4304, 2015.

[9] He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE
International Conference on Computer Vision (ICCV), pp. 1026–1034, 2015.

[10] Honari, Sina, Yosinski, Jason, Vincent, Pascal, and Pal, Christopher. Recombinator networks:
Learning coarse-to-ﬁne feature aggregation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pp. 5743–5752, 2016.

[11] Honari, Sina, Molchanov, Pavlo, Tyree, Stephen, Vincent, Pascal, Pal, Christopher, and Kautz,
Jan. Improving landmark localization with semi-supervised learning. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.

[12] Huang, Rui, Zhang, Shu, Li, Tianyu, and He, Ran. Beyond face rotation: Global and local
perception gan for photorealistic and identity preserving frontal view synthesis. In Proceedings
of the IEEE International Conference on Computer Vision (ICCV), Oct 2017.

[13] Jackson, Aaron S, Bulat, Adrian, Argyriou, Vasileios, and Tzimiropoulos, Georgios. Large pose
3d face reconstruction from a single image via direct volumetric cnn regression. In Proceedings
of the IEEE International Conference on Computer Vision (ICCV), pp. 1031–1039. IEEE, 2017.

[14] Jeni, László A, Cohn, Jeffrey F, and Kanade, Takeo. Dense 3d face alignment from 2d videos in
real-time. In IEEE International Conference and Workshops on Automatic Face and Gesture
Recognition (FG), volume 1, pp. 1–8. IEEE, 2015.

10

[15] Liu, Fayao, Shen, Chunhua, and Lin, Guosheng. Deep convolutional neural ﬁelds for depth
estimation from a single image. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 5162–5170, 2015.

[16] Parkhi, Omkar M, Vedaldi, Andrea, and Zisserman, Andrew. Deep face recognition. Proceedings

of the British Machine Vision Conference (BMVC), 2015.

[17] Sagonas, Christos, Tzimiropoulos, Georgios, Zafeiriou, Stefanos, and Pantic, Maja. 300 faces
in-the-wild challenge: The ﬁrst facial landmark localization challenge. In Proceedings of the
IEEE International Conference on Computer Vision Workshops (CVPRW), pp. 397–403, 2013.

[18] Shen, Yujun, Luo, Ping, Yan, Junjie, Wang, Xiaogang, and Tang, Xiaoou. Faceid-gan: Learning
a symmetry three-player gan for identity-preserving face synthesis. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 821–830, 2018.

[19] Taigman, Yaniv, Yang, Ming, Ranzato, Marc’Aurelio, and Wolf, Lior. Deepface: Closing the
gap to human-level performance in face veriﬁcation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 1701–1708, 2014.

[20] Tewari, Ayush, Zollhöfer, Michael, Kim, Hyeongwoo, Garrido, Pablo, Bernard, Florian, Perez,
Patrick, and Theobalt, Christian. Mofa: Model-based deep convolutional face autoencoder for
unsupervised monocular reconstruction. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), volume 2, 2017.

[21] Thewlis, James, Bilen, Hakan, and Vedaldi, Andrea. Unsupervised learning of object landmarks
by factorized spatial embeddings. In Proceedings of the IEEE International Conference on
Computer Vision (ICCV), volume 1, pp. 5, 2017.

[22] Tran, Luan, Yin, Xi, and Liu, Xiaoming. Disentangled representation learning gan for pose-
invariant face recognition. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017.

[23] Tung, Hsiao-Yu, Tung, Hsiao-Wei, Yumer, Ersin, and Fragkiadaki, Katerina. Self-supervised
learning of motion capture. In Advances in Neural Information Processing Systems (NIPS), pp.
5242–5252, 2017.

[24] Tung, Hsiao-Yu Fish, Harley, Adam W, Seto, William, and Fragkiadaki, Katerina. Adversarial
inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation from
unpaired supervision. In Proceedings of the IEEE International Conference on Computer Vision
(ICCV), volume 2, 2017.

[25] Yin, Xi, Yu, Xiang, Sohn, Kihyuk, Liu, Xiaoming, and Chandraker, Manmohan. Towards
large-pose face frontalization in the wild. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), pp. 1–10, 2017.

[26] Zhang, Xing, Yin, Lijun, Cohn, Jeffrey F, Canavan, Shaun, Reale, Michael, Horowitz, Andy,
Liu, Peng, and Girard, Jeffrey M. Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic
facial expression database. Image and Vision Computing, 32(10):692–706, 2014.

[27] Zhao, Jian, Xiong, Lin, Jayashree, Panasonic Karlekar, Li, Jianshu, Zhao, Fang, Wang, Zhecan,
Pranata, Panasonic Sugiri, Shen, Panasonic Shengmei, Yan, Shuicheng, and Feng, Jiashi. Dual-
agent gans for photorealistic and identity preserving proﬁle face synthesis. In Advances in
Neural Information Processing Systems (NIPS), pp. 66–76, 2017.

[28] Zhao, Jian, Cheng, Yu, Xu, Yan, Xiong, Lin, Li, Jianshu, Zhao, Fang, Jayashree, Karlekar,
Pranata, Sugiri, Shen, Shengmei, Xing, Junliang, et al. Towards pose invariant face recognition
in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 2207–2216, 2018.

[29] Zhou, Tinghui, Brown, Matthew, Snavely, Noah, and Lowe, David G. Unsupervised learning of
depth and ego-motion from video. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2017.

[30] Zhu, Jun-Yan, Park, Taesung, Isola, Phillip, and Efros, Alexei A. Unpaired image-to-image
translation using cycle-consistent adversarial networks. In Proceedings of the IEEE International
Conference on Computer Vision (ICCV), 2017.

11

Unsupervised Depth Estimation, 3D Face Rotation and
Replacement: Supplementary Information

S.1 Experimental Setup

The RCN, the DepthNets and the CycleGAN modules are trained separately. Each model is trained
using standard techniques for the model class and has a separate objective to be optimized. DepthNet
does not use the OpenGL pipeline during training and only uses it to render faces at test time, allowing
DepthNet to train faster.

S.1.1 DepthNet Experimental Details

Here we describe the details of the experiments carried out in Section 3.1. Our DepthNet architectures
require keypoints of both source and target images to be extracted. For this, the image is ﬁrst passed
through the VGG-Face [16] face detector. The face crops are then scaled down to 80 × 80 and
converted to greyscale, following which they are passed through RCN to obtain N = 68 keypoints
on each image. The RCN is trained exactly as described in [10], using the 300W dataset [17].

The keypoint only variant of our model involves concatenating all detected keypoints and passing
them through a two-layer deep fully connected network, with 256 hidden units and o output units.
The size of o depends on whether we are predicting only the depth, in which case o = N , or both
depth and afﬁne transformation parameters, in which case o = N + 8.

As discussed above, it is possible to augment these models with a Siamese CNN module (case A). In
the model variants that also use the image, we pass both the source and the target images through
three conv-maxpool layers with shared weights of size (32, 4, 2), (48, 3, 2), (64, 2, 2), respectively for
the representation (num_filters, filter_size, pool_size). The network’s outputs for the
source and target faces are then concatenated before passing them into a 4-layered fully connected
network with respective output sizes of 2048, 512, 256, and o. The keypoints are concatenated to the
512-unit layer before being passed to the last two layers. See Figure 1 (left) for an illustration of the
model. We explore these Siamese CNN augmented variants in models 4, 6 and 8 in Table 1.

We set the initial learning rate to 0.001 and use a Nesterov momentum optimizer (with a momentum
of 0.9) in all our experiments. With the exception of the last layer, we initialize all weights with
a Glorot initialization scheme [4], with the weights sampled from a uniform distribution. We use
a ReLU gain [9], set all biases to 0, and apply a ReLU non-linearity after every layer. In the ﬁnal
output layer, we do not apply any non-linearity and initialize the weights to 0. The biases of units that
represent depths are initialized to a random Normal distribution with µ = 0 and σ = 0.5, while those
that form the predicted afﬁne transform are initialized with the equivalent of a "ﬂattened" identity
transform. All models have been trained for 500 epochs.

We point out that except for a comparison between learning rates in the set {0.01, 0.001, 0.0001}
over few (less than 10) epochs, to ﬁnd a learning rate that the model seems to train well with, we
have not performed a hyperparameter search, and anticipate that the performance of the model can be
made even better by searching the hyperparameter space on a per model basis and by using deeper
(or modiﬁed) architectures.

S.2 Depth Visualization

We show further estimated depth values by different models in Figure S1. DepthNet+GAN and AIGN
generate the closest depth values to the GT depth. The baseline DepthNet model estimates reliable
depth values for most cases, however it has some degree of inaccuracy, as shown in the last two rows.
In DepthNet, the estimated depth indicates the position of each keypoint relative to other keypoints
rather than with respect to a source and importantly it is done without any supervision. MOFA, which
is also unsupervised, generates very similar face templates for different cases.

12

Figure S1: Depth visualization for different models (color coded by depth). The Depth axis is the one pointing
into the page. From left to right: RGB image, Ground Truth, DepthNet, DepthNet+GAN, AIGN and MOFA
estimated depth values.

S.3 Additional Camera Sweep Visualizations

In this section, we present additional visualizations along the lines of those shown in Figure 1 (right)
in Section 2 of the main paper. Frontal faces selected from the Multi-PIE dataset [6] are re-projected
to match several other poses corresponding to a person with a different identity. The DepthNets
predicts reliable proxy depths, which when coupled with the analytically obtained afﬁne transform
(obtained from the least-squares pseudo-inverse-based solution described in Section 2.3 of the main
paper) yields faces close to the desired target face geometry when passed through the OpenGL
pipeline.

We show camera sweeps for frontal source faces in Figures S2 and S3 and non-frontal source faces in
Figure S4. In Figure S5 we use the same target identity as the source face, showing how much the

13

Figure S2: Projecting a frontal face (far left) to a range of other poses deﬁned by faces in the row above.

generated face differs from the ground truth target. For all samples in Figures S2, S3, S4, and S5 we
use the DepthNet model that relies on only key-points (model 7 in Table 1).

Note that for non-frontal source faces, the quality of images is reduced specially for frontal target
faces. This is due to lack of adequate texture on the occluded side of the face to be transferred to the
target pose by OpenGL pipeline, rather than inaccuracies in the afﬁne transformation parameters. In
order to reduce the side-affects, we use either of the source face or its ﬂipped version, that is closer to
the target face pose, and then warp the face to the target keypoints.

Figure S3: Projecting a frontal face (far left) to a range of other poses deﬁned by faces in the row above.

Figure S4: Re-projecting a non-frontal face (far left) to a range of other poses deﬁned by faces in the row above.

Figure S5: Rotating a face (far left) to a range of other poses deﬁned by faces of the same identity in the row
above. On the top row frontal and on the bottom row non-frontal source faces are shown.

S.4 CycleGAN

Suppose we have some images belonging to one of two sets x ∈ X and y ∈ Y , where x denotes a
DepthNet-resulting face and y a ground truth face which is frontal. We wish to learn two functions

14

(cid:105)

(cid:105)

,

(cid:104)

Ex,y

min
G,F

(cid:104)

Ex,y

min
DX ,DY

F : X → Y and G : Y → X which are able to map an image from one set to the corresponding
image in the other. Correspondingly, we have two discriminators DX and DY which try to detect
whether the image in that particular set is real or generated. While we are only interested in the
function F : X → Y (since this is mapping to the distribution of ground truth faces) the formulation
of CycleGAN requires that we learn mappings in both directions during training. We optimize the
following objectives for the two generators F and G:

(cid:96)(DX (G(y)), 1) + (cid:96)(DY (F (x)), 1) + λ||y − F (G(y))||1 + λ||x − G(F (x))||1

(4)

And the following for the two discriminators DX and DY :

(cid:96)(DX (x), 1) + (cid:96)(DX (G(y)), 0) + (cid:96)(DY (y), 1) + (cid:96)(DY (F (x)), 0)

(5)

where 0/1 denote fake/real, (cid:96)() is the squared error loss and λ is a coefﬁcient for the cycle-consistency
(reconstruction) loss.

In the case where we did adversarial background synthesis, x is a channel-wise concatenation of the
DepthNet-frontalized face and the background of the original (pre-frontalized) image. For face-swap
cleanup, x is simply a source face which has been warped to a target face and pasted on top.

Once the network has been trained, we can disregard all other functions and use F to clean up faces
which are low quality due to artifacts from warping.

In terms of architectural details the generators and discriminators used were those described in the ap-
pendix of the CycleGAN paper [30]. In short, the generator consists of three conv-BN-relu blocks
which downsample the input, followed by nine ResNet blocks (which can be interpreted as iteratively
performing transformations over the downsampled representation), followed by deconv-BN-relu
blocks to upsample the representation back into the original input size. For training, we use the
same hyperparameters as most CycleGAN implementations which is using the Adam optimizer with
learning rate α = 2 × 10−4, β1 = 0.5, β2 = 0.999. However, instead of using a batch size of 1 we
use the largest possible batch size, which was 16 for a 12GB GPU.

Note that in order to produce better translations, the dataset we used for all CycleGAN experiments
contain both the VGG and the CelebA datasets, which has signiﬁcantly more images.

S.4.1 Background Synthesis

We present extra visualizations for the CycleGAN which performs background synthesis on CelebA,
corresponding to Figure 4 in the main paper. These are shown in Figure S6.

Figure S6: Background synthesis with CycleGAN. Left to right: source face; keypoints overlaid; DepthNet
(DN); DN + background → frontal

S.4.2 Face Replacement and Adversarial Repair

In Figure S7 we provide extra face swap samples on CelebA, corresponding to Figure 5 in the
main paper, where a source face is warped to a target face pose using DepthNet, pasted onto the

15

target image and then passed to a CycleGAN to adapt the face skin of the warped source face to the
background and hairstyle of the target face.

Figure S7: Face swap experiment with CycleGAN. Left to right: source face; target face; warp to target with
DepthNet; repaired result with CycleGAN. The source face is taken and warped onto the target face. The
background and hairstyle is then adapted to the target face.

S.4.3 Extreme Pose Face Clean-up

If the source image has an extreme pose, the texture will be missing on the occluded side of the
face and the OpenGL pipeline cannot rotate the face without artifacts. Note that this shortcoming is
due to lack of texture on the occluded side of the face rather than a deﬁciency of the transformation
parameters measured by DepthNet.

We performed an experiment using CycleGAN to ﬁx such artifacts. For this experiment we take
source images from CelebA and ﬁrst frontalize it by using DepthNet. Since the frontalized faces have
artifacts due to stretch of texture on the occluded side of the face by the OpenGL pipeline, we train a
CycleGAN that takes DepthNet frontalizaed faces plus the background of the original non-frontal
image (as two images) in one domain and the ground truth frontal faces in the other domain. The
CycleGAN learns to clean-up these artifacts. Finally, we take the GAN-repaired frontalized faces
and project it to different target poses using DepthNet. In Figure S8 we visualize camera sweep for
source faces in the wild that have extreme poses. The cycleGAN reasonably cleans the face artifacts
and then DepthNet projects it to different poses. This is just one approach to address the extreme
pose occlusion artifacts. We see alternative methods for addressing this issue as promising directions
for future research.

Figure S8: Re-projecting a non-frontal face (far left) from CelebA to a range of other poses deﬁned by faces in
the row above. Top row (in each pair) depicts the target faces from Multi-PIE [6]. The bottom row shows from
left to right: source face, souce face frontalized by DepthNet, adversarial-repaired face, the repaired source face
projected to the target poses (4th to 10th columns).

16

