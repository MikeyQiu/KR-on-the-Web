7
1
0
2
 
n
a
J
 
0
3
 
 
]
L
C
.
s
c
[
 
 
1
v
3
3
5
8
0
.
1
0
7
1
:
v
i
X
r
a

Graph-Based Semi-Supervised Conditional Random Fields For Spoken
Language Understanding Using Unaligned Data

Mohammad Aliannejadi
Amirkabir University
of Technology
(Tehran Polytechnic)
m.aliannejadi@aut.ac.ir

Masoud Kiaeeha
Sharif University of
Technology
kiaeeha@ce.sharif.edu

Shahram Khadivi &
Saeed Shiry Ghidary
Amirkabir University
of Technology
(Tehran Polytechnic)
{khadivi, shiry}@aut.ac.ir

Abstract

experiment

graph-based

We
Semi-
Supervised Learning (SSL) of Conditional
Random Fields (CRF) for the application
of Spoken Language Understanding
(SLU) on unaligned data. The aligned
labels for examples are obtained using
IBM Model. We adapt a baseline semi-
supervised CRF by deﬁning new feature
set and altering the label propagation al-
gorithm. Our results demonstrate that our
proposed approach signiﬁcantly improves
the performance of the supervised model
by utilizing the knowledge gained from
the graph.

1 Introduction

The aim of Spoken Language Understanding
(SLU) is to interpret the intention of the user’s ut-
terance. More speciﬁcally, a SLU system attempts
to ﬁnd a mapping from user’s utterance in natu-
ral language, to the limited set of concepts that is
structured and meaningful for the computer. As an
example, for the sample utterance:

I want to return to Dallas on Thursday

It’s corresponding output would be:
GOAL : RETURN
TOLOC.CITY = Dallas
RETURN.DATE = Thursday.
SLU can be widely used in many real world appli-
cations; however, data processing costs may im-
pede practicability of it. Thus, attempting to train
a SLU model using less training data is a key issue.
The ﬁrst statistical SLU system was based
on hidden Markov model and modeled using a
ﬁnite state semantic tagger employed in AT&T’s
CHRONUS
system (Pieraccini et al., 1992).
Their semantic representation was ﬂat-concept;
but,
later He and Young (2005) extended the
representation to a hierarchical structure and

the

problem using
a push-down
modeled
There are other works which
automaton.
labeling
have dealt with SLU as a sequential
Raymond and Riccardi (2007) and
problem.
Wang and Acero (2006) have
fully annotated
the data and trained the model in discriminative
frameworks such as CRF. CRF captures many
complex dependencies and models the sequential
relations between the labels;
it is a
powerful framework for SLU.

therefore,

The Semi-Supervised Learning (SSL) approach
has drawn a raft of interest among the machine
learning community basically because of its prac-
tical application (Chapelle et al., 2006). Manual
tagging of data can take considerable effort and
time; however, in the training phase of SSL, a
large amount of unlabeled data along with a small
amount of labeled data is provided. This makes it
more practicable and cost effective than providing
a fully labeled set of training data; thus, SSL is
more favorable.

active

the most

Graph-based SSL,

area
of research in SSL in the recent years, has
shown to outperform other SSL methods
(Chapelle et al., 2006).
Graph-based SSL al-
gorithms are generally run in two steps: graph
construction and label propagation.
Graph
construction is the most important step in graph-
based SSL; and,
the fundamental approach is
to assign labeled and unlabeled examples to
nodes of the graph. Then, a similarity function
is applied to compute similarity between pairs
of nodes. The computed similarities are then
assigned as the weight of the edges connecting
Label propaga-
the nodes (Zhu et al., 2003).
tion operates on the constructed graph. Based
on the constraints or properties derived from
the graph,
labels are propagated from a few
labeled nodes to the entire graph. These con-
straints
(Zhu et al., 2003;
Talukdar et al., 2008;
Subramanya et al., 2010;

include smoothness

Garrette and Baldridge, 2013),
(Das and Smith, 2012; Zeng et al., 2013).

and

sparsity

Labeling unaligned training data requires
much less effort compared to aligned data
(He and Young, 2005). Nevertheless, unaligned
data cannot be used to train a CRF model directly
since CRF requires fully-annotated data. On the
other hand, robust parameter estimation of a CRF
model requires a large set of training data which
is unrealistic in many practical applications. To
overcome this problem, the work in this paper ap-
plies semi-supervised CRF on unlabeled data. It is
motivated by the hypothesis that data is aligned to
labels in a monotone manner, and words appear-
ing in similar contexts tend to have same labels.
Under these circumstances, we were able to reach
1.64% improvement on the F-score over the super-
vised CRF and 1.38% improvement on the F-score
over the self trained CRF.

In the following section we describe the algo-
rithm this work is based on and our proposed al-
gorithm. In Section 3 we evaluate our work and in
the ﬁnal section conclusions are drawn.

2 Semi-supervised Spoken Language

Understanding

The input data is unaligned and represented
as a semantic tree, which is described in
(He and Young, 2005). The training sentences and
their corresponding semantic trees can be aligned
monotonically; hence, we chose IBM Model 5
(Khadivi and Ney, 2005) to ﬁnd the best align-
ment between the words and nodes of the seman-
tic tree (labels). Thus, we have circumvented the
problem of unaligned data. More detailed expla-
nation about this process can be found in our pre-
vious work (Aliannejadi et al., 2014). This data
is then used to train the supervised and semi-
supervised CRFs.

2.1 Semi-supervised CRF

The proposed semi-supervised learning algorithm
is based on (Subramanya et al., 2010). Here, we
quickly review this algorithm (Algorithm 1).

In the ﬁrst step, the CRF model is trained on the

labeled data (Dl) according to (1):

Λ∗ = arg min

Λ∈RK h −

l

X
i=1

log p(yi|xi; Λ) + γkΛk2

i,
(1)

Algorithm 1 Semi-Supervised Training of CRF
1: Λ(n=0) = TrainCRF(Dl)
2: G = BuildGraph(Dl ∪ Du)
3: {r} = CalcEmpiricalDistribution(Dl)
4: while not converged do
5:

6:

{m} = CalcMarginals(Du, Λn)
{q} = AverageMarginals(m)
{ˆq} = LabelPropagation(q, r)
u = ViterbiDecode({ˆq}, Λn)
Λn+1 = RetrainCRF(Dl ∪ Dv

7:
8: Dv
9:
10: end while
11: Return ﬁnal Λn

u, Λn);

where Λ∗ is the optimal parameter set of the base
CRF model and kΛk2 is the squared ℓ2-norm reg-
ularizer whose impact is adjusted by γ. At the ﬁrst
line, Λ∗ is assigned to Λ(n=0) i.e.
the initial pa-
rameter set of the model.

In the next step, the k-NN similarity graph (G)
is constructed (line 2), which will be discussed in
more detail in Section 2.3. In the third step, the
empirical label distribution (r) on the labeled data
is computed. The main loop of the algorithm is
then started and the execution continues until the
results converge.

Marginal probability of labels (m) are then com-
puted on the unlabeled data (Du) using Forward-
Backward algorithm with the parameters of the
previous CRF model (Λn), and in the next step,
all the marginal label probabilities of each trigram
are averaged over its occurrences (line 5 and 6).

In label propagation (line 7), trigram marginals
(q) are propagated through the similarity graph
using an iterative algorithm. Thus, they become
smooth. Empirical label distribution (r) serves as
the priori label information for labeled data and
trigram marginals (q) act as the seed labels. More
detailed discussion is found in Section 2.4.

Afterwards, having the results of label propaga-
tion (ˆq) and previous CRF model parameters, la-
bels of the unlabeled data are estimated by com-
bining the interpolated label marginals and the
CRF transition potentials (line 8). For every word
position j for i indexing over sentences, interpo-
lated label marginals are calculated as follows:

ˆp(y(j)

i = y|xi) = αp(y(j)

i = y|xi; Λn)
+ (1 − α)ˆqT (i,j)(y),

(2)

where T (i, j) is a trigram centered at position j of
the ith sentence and α is the interpolation factor.

Description

Feature

Context
Left Context
Right Context
Center Word in trigram
Center is Class
Center is Preposition
Left is Preposition

x1 x2 x3 x4 x5
x1 x2
x4 x5
x3
IsClass(x3)
IsP reposition(x3)
IsP reposition(x2)

Table 1: Context Features used for constructing
the similarity graph

In the ﬁnal step, the previous CRF model pa-
rameters are regularized using the labels estimated
for the unlabeled data in the previous step (line 9)
as follows:

Λn+1 = arg min

Λ∈RK h −

log p(yi|xi; Λn)

l

X
i=1

− η

log p(yi|xi; Λn) + γkΛk2

i,

(3)

u

X
i=l+1

where η is a trade-off parameter whose setting is
discussed later in Section 3.

2.2 CRF Features

are

By aligning the training data, many infor-
saved which are omit-
mative
labels
(Wang and Acero, 2006;
ted in other works
Raymond and Riccardi, 2007). By saving these
information, the ﬁrst order label dependency helps
the model to predict the labels more precisely.
the
Therefore the model manages to predict
labels using less lexical features and the feature
window that was [-4,+2] in previous works is
reduced to [0,+2]. Using smaller feature win-
dow improves the generalization of the model
(Aliannejadi et al., 2014).

2.3 Similarity Graph

In our work we have considered trigrams as the
nodes of the graph and extracted features of each
trigram x2 x3 x4 according to the 5-word con-
text x1 x2 x3 x4 x5 it appears in. These features
are carefully selected so that nodes are correctly
placed in neighborhood of the ones having simi-
lar labels. Table 1 presents the feature set that we
have applied to construct the similarity graph.

IsClass feature impacts the structure of the
In the pre-processing phase

graph signiﬁcantly.

speciﬁc words are marked as classes according to
the corpus’ accompanying database. As an ex-
ample, city names such as Dallas and Baltimore
are represented as city name which is a class type.
Since these classes play an important role in calcu-
lating similarity of the nodes, IsClass feature is
used to determine if a given position in a context
is a class type.

Furthermore, prepositions like from and be-
tween are also important, e.g. when two trigrams
like ”from Washington to” and ”between Dallas
and” are compared. The two trigrams are totally
different while both of them begin with a prepo-
sition and are continued with a class. Therefore,
IsP reposition feature would be particularly suit-
able to increase the similarity score of these two
trigrams.
In many cases, these features have a
signiﬁcant effect in assigning a better similarity
score.

To deﬁne a similarity measure, we compute
the Pointwise Mutual Information (PMI) between
all occurrences of a trigram and each of the
features. The PMI measure transforms the in-
dependence assumption into a ratio (Lin, 1998;
Razmara et al., 2013). Then, the similarity be-
tween two nodes is measured as the cosine dis-
tance between their PMI vectors. We carefully ex-
amined the similarity graph on the training data
and found out the head and tail trigrams of each
sentence which contain dummy words, make the
graph sparse. Hence, we have ignored those tri-
grams.

2.4 Label Propagation

After statistical alignment, the training data gets
noisy. Hence, use of traditional label propagation
algorithms causes an error propagation over the
whole graph and degrades the whole system per-
formance. Thus, we make use of the Modiﬁed Ad-
sorption (MAD) algorithm for label propagation.
MAD algorithm controls the label propagation
more strictly. This is accomplished by limiting
the amount of information that passes from a node
to another (Talukdar and Pereira, 2010). Soft label
vectors ˆYv are found by solving the unconstrained
optimization problem in (4):

hµ1(Yl − ˆYl)⊤S (Yl − ˆYl)

min
ˆY X
l∈C
+ µ2 ˆYl

⊤

L′ ˆYl + µ3(cid:13)
where µi are hyper-parameters and Rl

ˆYl − Rl(cid:13)
(cid:13)
(cid:13)

i,

2

(4)

is the

empirical label distribution over labels i.e.
the
prior belief about the labeling of a node. The
ﬁrst term of the summation is related to label
score injection from the initial score of the node
and makes the output match the seed labels Yl
(Razmara et al., 2013). The second term is asso-
ciated with label score acquisition from neighbor
nodes i.e. smooths the labels according to the sim-
ilarity graph. In the last term, the labels are regu-
larized to match a priori label Rl in order to avoid
false labels for high degree unlabeled nodes. A
solution to the optimization problem in (4) can
be found with an efﬁcient iterative algorithm de-
scribed in (Talukdar and Crammer, 2009).

Many errors of the alignment model are cor-
rected through label propagation using the MAD
algorithm; whereas, those errors are propagated
in traditional label propagation algorithms such as
the one mentioned in (Subramanya et al., 2010).

2.5 System Overview

We have implemented the Graph Construc-
implemented
tion in Java and the CRF is
by modifying the source code of CRFSuite
(Okazaki, 2007). We have also modiﬁed Junto
toolkit (Talukdar and Pereira, 2010) and used it
for graph propagation. The whole source code of
our system is available online1. The input utter-
ances and their corresponding semantic trees are
aligned using GIZA++ (Och and Ney, 2000); and
then used to train the base CRF model. The graph
is constructed using the labeled and unlabeled data
and the main loop of the algorithm continues until
convergence. The ﬁnal parameters of the CRF are
retained for decoding in the test phase.

3 Experimental Results

section we evaluate our

In this
results on
Air Travel Information Service (ATIS) data-set
(Dahl et al., 1994) which consists of 4478 train-
ing, 500 development and 896 test utterances.
The development set was chosen randomly. To
evaluate our work, we have compared our results
with results from Supervised CRF and Self-trained
CRF (Yarowsky, 1995).

For our experiments we set hyper-parameters as
for graph propagation, µ1 = 1, µ2 =
follows:
0.01, µ3 = 0.01, for Viterbi decoding, α = 0.1,
for CRF-retraining, η = 0.1, γ = 0.01. We have

1https://github.com/maxxkia/g-ssl-crf

% of Labeled Data
30
20
10
88.64
87.69
86.07
88.64
87.73
86.34
89.12
88.75
Semi-supervised CRF 87.72

Supervised CRF
Self-trained CRF

Table 2:
Slot/Value F-score in %.

Comparison of

training results.

chosen these parameters along with graph fea-
tures and graph-related parameters by evaluating
the model on the development set. We employed
the L-BFGS algorithm to optimize CRF objective
functions; which is designed to be fast and low-
memory consumer for the high-dimensional opti-
mization problems (Bertsekas, 1999).

We have post-processed the sequence of labels
to obtain the slots and their values. The slot-
value pair is compared to the reference test set and
the result is reported in F-score of slot classiﬁca-
tion. Table 2 demonstrates results obtained from
our semi-supervised CRF algorithm compared to
the supervised CRF and self-trained CRF. Experi-
ments were carried out having 10%, 20% and 30%
of data being labeled. For each of these tests, la-
beled set was selected randomly from the train-
ing set. This procedure was done 10 times and
the reported results are the average of the results
thereof. The Supervised CRF model is trained
only on the labeled fraction of the data. How-
ever, the Self-trained CRF and Semi-supervised
CRF have access to the rest of the data as well,
which are unlabeled. Our Supervised CRF gained
91.02 F-score with 100% of the data labeled which
performs better compared to 89.32% F-score of
Raymond and Riccardi (2007) CRF model.

As shown in Table 2, the proposed method per-
forms better compared to supervised CRF and
self-trained CRF. The most signiﬁcant improve-
ment occurs when only 10% of training set is
labeled; where we gain 1.65% improvement on
F-score compared to supervised CRF and 1.38%
compared to self-trained CRF.

4 Conclusion

We presented a simple algorithm to train CRF in a
semi-supervised manner using unaligned data for
SLU. By saving many informative labels in the
alignment phase, the base model is trained using
fewer features. The parameters of the CRF model
are estimated using much less labeled data by

regularizing the model using a nearest-neighbor
graph. Results demonstrate that our proposed al-
gorithm signiﬁcantly improves the performance
compared to supervised and self-trained CRF.

References

[Aliannejadi et al.2014] Mohammad

Aliannejadi,
Shahram Khadivi, SaeedShiry Ghidary, and Mo-
hammadHadi Bokaei. 2014. Discriminative spoken
language understanding using statistical machine
translation alignment models.
In Ali Movaghar,
Mansour Jamzad, and Hossein Asadi, editors, Artiﬁ-
cial Intelligence and Signal Processing, volume 427
of Communications in Computer and Information
Science, pages 194–202. Springer
International
Publishing.

[Bertsekas1999] Dimitri P Bertsekas. 1999. Nonlinear

programming.

[Chapelle et al.2006] Olivier

Chapelle,

Sch¨olkopf, Alexander Zien, et al.
supervised learning, volume 2.
Cambridge.

2006.

Bernhard
Semi-
MIT press

[Dahl et al.1994] Deborah A. Dahl, Madeleine Bates,
Michael Brown, William Fisher, Kate Hunicke-
Smith, David Pallett, Christine Pao, Alexander Rud-
nicky, and Elizabeth Shriberg. 1994. Expanding the
scope of the atis task: The atis-3 corpus. In Proceed-
ings of the Workshop on Human Language Technol-
ogy, HLT ’94, pages 43–48, Stroudsburg, PA, USA.
Association for Computational Linguistics.

[Das and Smith2012] Dipanjan Das and Noah A.
Smith. 2012. Graph-based lexicon expansion with
sparsity-inducing penalties.
In Proceedings of the
2012 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies, NAACL HLT ’12,
pages 677–687, Stroudsburg, PA, USA. Association
for Computational Linguistics.

[Garrette and Baldridge2013] Dan Garrette and Jason
Baldridge. 2013. Learning a part-of-speech tag-
ger from two hours of annotation. In Proceedings
of NAACL-HLT, pages 138–147.

[He and Young2005] Yulan He and Steve Young. 2005.
Semantic processing using the hidden vector state
model. Computer Speech & Language, 19(1):85 –
106.

[Lin1998] Dekang Lin. 1998. Automatic retrieval and
In Proceedings of the
clustering of similar words.
36th Annual Meeting of the Association for Compu-
tational Linguistics and 17th International Confer-
ence on Computational Linguistics - Volume 2, ACL
’98, pages 768–774, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

[Och and Ney2000] Franz Josef Och and Hermann Ney.
2000. Giza++: Training of statistical translation
models.

[Okazaki2007] Naoaki Okazaki. 2007. Crfsuite: a fast
implementation of conditional random ﬁelds (crfs).
URL http://www.chokkan.org/software/crfsuite.

[Pieraccini et al.1992] R. Pieraccini, E. Tzoukermann,
Z. Gorelov, J. Gauvain, E. Levin, Chin-Hui Lee, and
J.G. Wilpon. 1992. A speech understanding sys-
tem based on statistical representation of semantics.
In Acoustics, Speech, and Signal Processing, 1992.
ICASSP-92., 1992 IEEE International Conference
on, volume 1, pages 193–196 vol.1, Mar.

[Raymond and Riccardi2007] Christian Raymond and
Giuseppe Riccardi. 2007. Generative and discrim-
inative algorithms for spoken language understand-
ing.
In International Conference on Speech Com-
munication and Technologies, pages 1605–1608,
Antwerp, Belgium, August.

[Razmara et al.2013] Majid Razmara, Maryam Siah-
bani, Gholamreza Haffari, and Anoop Sarkar.
2013. Graph propagation for paraphrasing out-of-
vocabulary words in statistical machine translation.
In Proceedings of the Conference of the Association
for Computational Linguistics (ACL).

[Subramanya et al.2010] Amarnag Subramanya, Slav
Petrov, and Fernando Pereira.
2010. Efﬁcient
graph-based semi-supervised learning of structured
tagging models.
In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’10, pages 167–176, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

[Talukdar and Crammer2009] ParthaPratim Talukdar
2009. New regularized
and Koby Crammer.
algorithms for transductive learning.
In Wray
Buntine, Marko Grobelnik, Dunja Mladeni, and
John Shawe-Taylor, editors, Machine Learning
and Knowledge Discovery in Databases, volume
5782 of Lecture Notes in Computer Science, pages
442–457. Springer Berlin Heidelberg.

[Khadivi and Ney2005] Shahram Khadivi and Her-
mann Ney.
2005. Automatic ﬁltering of bilin-
gual corpora for statistical machine translation. In
Andr´es Montoyo, Rafael Mu´noz, and Elisabeth
M´etais, editors, Natural Language Processing and
Information Systems, volume 3513 of Lecture Notes
in Computer Science, pages 263–274. Springer
Berlin Heidelberg.

[Talukdar and Pereira2010] Partha Pratim Talukdar and
Fernando Pereira. 2010. Experiments in graph-
based semi-supervised learning methods for class-
instance acquisition. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ’10, pages 1473–1481, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

[Talukdar et al.2008] Partha Pratim Talukdar, Joseph
Reisinger, Marius Pas¸ca, Deepak Ravichandran,
Rahul Bhagat, and Fernando Pereira.
2008.
Weakly-supervised acquisition of labeled class in-
stances using graph random walks. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP ’08, pages 582–
590, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

[Wang and Acero2006] Ye-Yi Wang and Alex Acero.
2006. Discriminative models for spoken language
understanding.
In International Conference on
Speech Communication and Technologies.

[Yarowsky1995] David Yarowsky.

1995. Unsuper-
vised word sense disambiguation rivaling supervised
methods. In Proceedings of the 33rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ’95, pages 189–196, Stroudsburg, PA, USA.
Association for Computational Linguistics.

[Zeng et al.2013] Xiaodong Zeng, Derek F Wong,
Lidia S Chao, and Isabel Trancoso. 2013. Graph-
based semi-supervised model for joint chinese word
segmentation and part-of-speech tagging.
In ACL,
pages 770–779.

[Zhu et al.2003] Xiaojin Zhu, Zoubin Ghahramani,
John Lafferty, et al. 2003. Semi-supervised learn-
ing using gaussian ﬁelds and harmonic functions. In
ICML, volume 3, pages 912–919.

This figure "graph.jpg" is available in "jpg"(cid:10) format from:

http://arxiv.org/ps/1701.08533v1

This figure "mainflowchart-en.jpg" is available in "jpg"(cid:10) format from:

http://arxiv.org/ps/1701.08533v1

7
1
0
2
 
n
a
J
 
0
3
 
 
]
L
C
.
s
c
[
 
 
1
v
3
3
5
8
0
.
1
0
7
1
:
v
i
X
r
a

Graph-Based Semi-Supervised Conditional Random Fields For Spoken
Language Understanding Using Unaligned Data

Mohammad Aliannejadi
Amirkabir University
of Technology
(Tehran Polytechnic)
m.aliannejadi@aut.ac.ir

Masoud Kiaeeha
Sharif University of
Technology
kiaeeha@ce.sharif.edu

Shahram Khadivi &
Saeed Shiry Ghidary
Amirkabir University
of Technology
(Tehran Polytechnic)
{khadivi, shiry}@aut.ac.ir

Abstract

experiment

graph-based

We
Semi-
Supervised Learning (SSL) of Conditional
Random Fields (CRF) for the application
of Spoken Language Understanding
(SLU) on unaligned data. The aligned
labels for examples are obtained using
IBM Model. We adapt a baseline semi-
supervised CRF by deﬁning new feature
set and altering the label propagation al-
gorithm. Our results demonstrate that our
proposed approach signiﬁcantly improves
the performance of the supervised model
by utilizing the knowledge gained from
the graph.

1 Introduction

The aim of Spoken Language Understanding
(SLU) is to interpret the intention of the user’s ut-
terance. More speciﬁcally, a SLU system attempts
to ﬁnd a mapping from user’s utterance in natu-
ral language, to the limited set of concepts that is
structured and meaningful for the computer. As an
example, for the sample utterance:

I want to return to Dallas on Thursday

It’s corresponding output would be:
GOAL : RETURN
TOLOC.CITY = Dallas
RETURN.DATE = Thursday.
SLU can be widely used in many real world appli-
cations; however, data processing costs may im-
pede practicability of it. Thus, attempting to train
a SLU model using less training data is a key issue.
The ﬁrst statistical SLU system was based
on hidden Markov model and modeled using a
ﬁnite state semantic tagger employed in AT&T’s
CHRONUS
system (Pieraccini et al., 1992).
Their semantic representation was ﬂat-concept;
but,
later He and Young (2005) extended the
representation to a hierarchical structure and

the

problem using
a push-down
modeled
There are other works which
automaton.
labeling
have dealt with SLU as a sequential
Raymond and Riccardi (2007) and
problem.
Wang and Acero (2006) have
fully annotated
the data and trained the model in discriminative
frameworks such as CRF. CRF captures many
complex dependencies and models the sequential
relations between the labels;
it is a
powerful framework for SLU.

therefore,

The Semi-Supervised Learning (SSL) approach
has drawn a raft of interest among the machine
learning community basically because of its prac-
tical application (Chapelle et al., 2006). Manual
tagging of data can take considerable effort and
time; however, in the training phase of SSL, a
large amount of unlabeled data along with a small
amount of labeled data is provided. This makes it
more practicable and cost effective than providing
a fully labeled set of training data; thus, SSL is
more favorable.

active

the most

Graph-based SSL,

area
of research in SSL in the recent years, has
shown to outperform other SSL methods
(Chapelle et al., 2006).
Graph-based SSL al-
gorithms are generally run in two steps: graph
construction and label propagation.
Graph
construction is the most important step in graph-
based SSL; and,
the fundamental approach is
to assign labeled and unlabeled examples to
nodes of the graph. Then, a similarity function
is applied to compute similarity between pairs
of nodes. The computed similarities are then
assigned as the weight of the edges connecting
Label propaga-
the nodes (Zhu et al., 2003).
tion operates on the constructed graph. Based
on the constraints or properties derived from
the graph,
labels are propagated from a few
labeled nodes to the entire graph. These con-
straints
(Zhu et al., 2003;
Talukdar et al., 2008;
Subramanya et al., 2010;

include smoothness

Garrette and Baldridge, 2013),
(Das and Smith, 2012; Zeng et al., 2013).

and

sparsity

Labeling unaligned training data requires
much less effort compared to aligned data
(He and Young, 2005). Nevertheless, unaligned
data cannot be used to train a CRF model directly
since CRF requires fully-annotated data. On the
other hand, robust parameter estimation of a CRF
model requires a large set of training data which
is unrealistic in many practical applications. To
overcome this problem, the work in this paper ap-
plies semi-supervised CRF on unlabeled data. It is
motivated by the hypothesis that data is aligned to
labels in a monotone manner, and words appear-
ing in similar contexts tend to have same labels.
Under these circumstances, we were able to reach
1.64% improvement on the F-score over the super-
vised CRF and 1.38% improvement on the F-score
over the self trained CRF.

In the following section we describe the algo-
rithm this work is based on and our proposed al-
gorithm. In Section 3 we evaluate our work and in
the ﬁnal section conclusions are drawn.

2 Semi-supervised Spoken Language

Understanding

The input data is unaligned and represented
as a semantic tree, which is described in
(He and Young, 2005). The training sentences and
their corresponding semantic trees can be aligned
monotonically; hence, we chose IBM Model 5
(Khadivi and Ney, 2005) to ﬁnd the best align-
ment between the words and nodes of the seman-
tic tree (labels). Thus, we have circumvented the
problem of unaligned data. More detailed expla-
nation about this process can be found in our pre-
vious work (Aliannejadi et al., 2014). This data
is then used to train the supervised and semi-
supervised CRFs.

2.1 Semi-supervised CRF

The proposed semi-supervised learning algorithm
is based on (Subramanya et al., 2010). Here, we
quickly review this algorithm (Algorithm 1).

In the ﬁrst step, the CRF model is trained on the

labeled data (Dl) according to (1):

Λ∗ = arg min

Λ∈RK h −

l

X
i=1

log p(yi|xi; Λ) + γkΛk2

i,
(1)

Algorithm 1 Semi-Supervised Training of CRF
1: Λ(n=0) = TrainCRF(Dl)
2: G = BuildGraph(Dl ∪ Du)
3: {r} = CalcEmpiricalDistribution(Dl)
4: while not converged do
5:

6:

{m} = CalcMarginals(Du, Λn)
{q} = AverageMarginals(m)
{ˆq} = LabelPropagation(q, r)
u = ViterbiDecode({ˆq}, Λn)
Λn+1 = RetrainCRF(Dl ∪ Dv

7:
8: Dv
9:
10: end while
11: Return ﬁnal Λn

u, Λn);

where Λ∗ is the optimal parameter set of the base
CRF model and kΛk2 is the squared ℓ2-norm reg-
ularizer whose impact is adjusted by γ. At the ﬁrst
line, Λ∗ is assigned to Λ(n=0) i.e.
the initial pa-
rameter set of the model.

In the next step, the k-NN similarity graph (G)
is constructed (line 2), which will be discussed in
more detail in Section 2.3. In the third step, the
empirical label distribution (r) on the labeled data
is computed. The main loop of the algorithm is
then started and the execution continues until the
results converge.

Marginal probability of labels (m) are then com-
puted on the unlabeled data (Du) using Forward-
Backward algorithm with the parameters of the
previous CRF model (Λn), and in the next step,
all the marginal label probabilities of each trigram
are averaged over its occurrences (line 5 and 6).

In label propagation (line 7), trigram marginals
(q) are propagated through the similarity graph
using an iterative algorithm. Thus, they become
smooth. Empirical label distribution (r) serves as
the priori label information for labeled data and
trigram marginals (q) act as the seed labels. More
detailed discussion is found in Section 2.4.

Afterwards, having the results of label propaga-
tion (ˆq) and previous CRF model parameters, la-
bels of the unlabeled data are estimated by com-
bining the interpolated label marginals and the
CRF transition potentials (line 8). For every word
position j for i indexing over sentences, interpo-
lated label marginals are calculated as follows:

ˆp(y(j)

i = y|xi) = αp(y(j)

i = y|xi; Λn)
+ (1 − α)ˆqT (i,j)(y),

(2)

where T (i, j) is a trigram centered at position j of
the ith sentence and α is the interpolation factor.

Description

Feature

Context
Left Context
Right Context
Center Word in trigram
Center is Class
Center is Preposition
Left is Preposition

x1 x2 x3 x4 x5
x1 x2
x4 x5
x3
IsClass(x3)
IsP reposition(x3)
IsP reposition(x2)

Table 1: Context Features used for constructing
the similarity graph

In the ﬁnal step, the previous CRF model pa-
rameters are regularized using the labels estimated
for the unlabeled data in the previous step (line 9)
as follows:

Λn+1 = arg min

Λ∈RK h −

log p(yi|xi; Λn)

l

X
i=1

− η

log p(yi|xi; Λn) + γkΛk2

i,

(3)

u

X
i=l+1

where η is a trade-off parameter whose setting is
discussed later in Section 3.

2.2 CRF Features

are

By aligning the training data, many infor-
saved which are omit-
mative
labels
(Wang and Acero, 2006;
ted in other works
Raymond and Riccardi, 2007). By saving these
information, the ﬁrst order label dependency helps
the model to predict the labels more precisely.
the
Therefore the model manages to predict
labels using less lexical features and the feature
window that was [-4,+2] in previous works is
reduced to [0,+2]. Using smaller feature win-
dow improves the generalization of the model
(Aliannejadi et al., 2014).

2.3 Similarity Graph

In our work we have considered trigrams as the
nodes of the graph and extracted features of each
trigram x2 x3 x4 according to the 5-word con-
text x1 x2 x3 x4 x5 it appears in. These features
are carefully selected so that nodes are correctly
placed in neighborhood of the ones having simi-
lar labels. Table 1 presents the feature set that we
have applied to construct the similarity graph.

IsClass feature impacts the structure of the
In the pre-processing phase

graph signiﬁcantly.

speciﬁc words are marked as classes according to
the corpus’ accompanying database. As an ex-
ample, city names such as Dallas and Baltimore
are represented as city name which is a class type.
Since these classes play an important role in calcu-
lating similarity of the nodes, IsClass feature is
used to determine if a given position in a context
is a class type.

Furthermore, prepositions like from and be-
tween are also important, e.g. when two trigrams
like ”from Washington to” and ”between Dallas
and” are compared. The two trigrams are totally
different while both of them begin with a prepo-
sition and are continued with a class. Therefore,
IsP reposition feature would be particularly suit-
able to increase the similarity score of these two
trigrams.
In many cases, these features have a
signiﬁcant effect in assigning a better similarity
score.

To deﬁne a similarity measure, we compute
the Pointwise Mutual Information (PMI) between
all occurrences of a trigram and each of the
features. The PMI measure transforms the in-
dependence assumption into a ratio (Lin, 1998;
Razmara et al., 2013). Then, the similarity be-
tween two nodes is measured as the cosine dis-
tance between their PMI vectors. We carefully ex-
amined the similarity graph on the training data
and found out the head and tail trigrams of each
sentence which contain dummy words, make the
graph sparse. Hence, we have ignored those tri-
grams.

2.4 Label Propagation

After statistical alignment, the training data gets
noisy. Hence, use of traditional label propagation
algorithms causes an error propagation over the
whole graph and degrades the whole system per-
formance. Thus, we make use of the Modiﬁed Ad-
sorption (MAD) algorithm for label propagation.
MAD algorithm controls the label propagation
more strictly. This is accomplished by limiting
the amount of information that passes from a node
to another (Talukdar and Pereira, 2010). Soft label
vectors ˆYv are found by solving the unconstrained
optimization problem in (4):

hµ1(Yl − ˆYl)⊤S (Yl − ˆYl)

min
ˆY X
l∈C
+ µ2 ˆYl

⊤

L′ ˆYl + µ3(cid:13)
where µi are hyper-parameters and Rl

ˆYl − Rl(cid:13)
(cid:13)
(cid:13)

i,

2

(4)

is the

empirical label distribution over labels i.e.
the
prior belief about the labeling of a node. The
ﬁrst term of the summation is related to label
score injection from the initial score of the node
and makes the output match the seed labels Yl
(Razmara et al., 2013). The second term is asso-
ciated with label score acquisition from neighbor
nodes i.e. smooths the labels according to the sim-
ilarity graph. In the last term, the labels are regu-
larized to match a priori label Rl in order to avoid
false labels for high degree unlabeled nodes. A
solution to the optimization problem in (4) can
be found with an efﬁcient iterative algorithm de-
scribed in (Talukdar and Crammer, 2009).

Many errors of the alignment model are cor-
rected through label propagation using the MAD
algorithm; whereas, those errors are propagated
in traditional label propagation algorithms such as
the one mentioned in (Subramanya et al., 2010).

2.5 System Overview

We have implemented the Graph Construc-
implemented
tion in Java and the CRF is
by modifying the source code of CRFSuite
(Okazaki, 2007). We have also modiﬁed Junto
toolkit (Talukdar and Pereira, 2010) and used it
for graph propagation. The whole source code of
our system is available online1. The input utter-
ances and their corresponding semantic trees are
aligned using GIZA++ (Och and Ney, 2000); and
then used to train the base CRF model. The graph
is constructed using the labeled and unlabeled data
and the main loop of the algorithm continues until
convergence. The ﬁnal parameters of the CRF are
retained for decoding in the test phase.

3 Experimental Results

section we evaluate our

In this
results on
Air Travel Information Service (ATIS) data-set
(Dahl et al., 1994) which consists of 4478 train-
ing, 500 development and 896 test utterances.
The development set was chosen randomly. To
evaluate our work, we have compared our results
with results from Supervised CRF and Self-trained
CRF (Yarowsky, 1995).

For our experiments we set hyper-parameters as
for graph propagation, µ1 = 1, µ2 =
follows:
0.01, µ3 = 0.01, for Viterbi decoding, α = 0.1,
for CRF-retraining, η = 0.1, γ = 0.01. We have

1https://github.com/maxxkia/g-ssl-crf

% of Labeled Data
30
20
10
88.64
87.69
86.07
88.64
87.73
86.34
89.12
88.75
Semi-supervised CRF 87.72

Supervised CRF
Self-trained CRF

Table 2:
Slot/Value F-score in %.

Comparison of

training results.

chosen these parameters along with graph fea-
tures and graph-related parameters by evaluating
the model on the development set. We employed
the L-BFGS algorithm to optimize CRF objective
functions; which is designed to be fast and low-
memory consumer for the high-dimensional opti-
mization problems (Bertsekas, 1999).

We have post-processed the sequence of labels
to obtain the slots and their values. The slot-
value pair is compared to the reference test set and
the result is reported in F-score of slot classiﬁca-
tion. Table 2 demonstrates results obtained from
our semi-supervised CRF algorithm compared to
the supervised CRF and self-trained CRF. Experi-
ments were carried out having 10%, 20% and 30%
of data being labeled. For each of these tests, la-
beled set was selected randomly from the train-
ing set. This procedure was done 10 times and
the reported results are the average of the results
thereof. The Supervised CRF model is trained
only on the labeled fraction of the data. How-
ever, the Self-trained CRF and Semi-supervised
CRF have access to the rest of the data as well,
which are unlabeled. Our Supervised CRF gained
91.02 F-score with 100% of the data labeled which
performs better compared to 89.32% F-score of
Raymond and Riccardi (2007) CRF model.

As shown in Table 2, the proposed method per-
forms better compared to supervised CRF and
self-trained CRF. The most signiﬁcant improve-
ment occurs when only 10% of training set is
labeled; where we gain 1.65% improvement on
F-score compared to supervised CRF and 1.38%
compared to self-trained CRF.

4 Conclusion

We presented a simple algorithm to train CRF in a
semi-supervised manner using unaligned data for
SLU. By saving many informative labels in the
alignment phase, the base model is trained using
fewer features. The parameters of the CRF model
are estimated using much less labeled data by

regularizing the model using a nearest-neighbor
graph. Results demonstrate that our proposed al-
gorithm signiﬁcantly improves the performance
compared to supervised and self-trained CRF.

References

[Aliannejadi et al.2014] Mohammad

Aliannejadi,
Shahram Khadivi, SaeedShiry Ghidary, and Mo-
hammadHadi Bokaei. 2014. Discriminative spoken
language understanding using statistical machine
translation alignment models.
In Ali Movaghar,
Mansour Jamzad, and Hossein Asadi, editors, Artiﬁ-
cial Intelligence and Signal Processing, volume 427
of Communications in Computer and Information
Science, pages 194–202. Springer
International
Publishing.

[Bertsekas1999] Dimitri P Bertsekas. 1999. Nonlinear

programming.

[Chapelle et al.2006] Olivier

Chapelle,

Sch¨olkopf, Alexander Zien, et al.
supervised learning, volume 2.
Cambridge.

2006.

Bernhard
Semi-
MIT press

[Dahl et al.1994] Deborah A. Dahl, Madeleine Bates,
Michael Brown, William Fisher, Kate Hunicke-
Smith, David Pallett, Christine Pao, Alexander Rud-
nicky, and Elizabeth Shriberg. 1994. Expanding the
scope of the atis task: The atis-3 corpus. In Proceed-
ings of the Workshop on Human Language Technol-
ogy, HLT ’94, pages 43–48, Stroudsburg, PA, USA.
Association for Computational Linguistics.

[Das and Smith2012] Dipanjan Das and Noah A.
Smith. 2012. Graph-based lexicon expansion with
sparsity-inducing penalties.
In Proceedings of the
2012 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies, NAACL HLT ’12,
pages 677–687, Stroudsburg, PA, USA. Association
for Computational Linguistics.

[Garrette and Baldridge2013] Dan Garrette and Jason
Baldridge. 2013. Learning a part-of-speech tag-
ger from two hours of annotation. In Proceedings
of NAACL-HLT, pages 138–147.

[He and Young2005] Yulan He and Steve Young. 2005.
Semantic processing using the hidden vector state
model. Computer Speech & Language, 19(1):85 –
106.

[Lin1998] Dekang Lin. 1998. Automatic retrieval and
In Proceedings of the
clustering of similar words.
36th Annual Meeting of the Association for Compu-
tational Linguistics and 17th International Confer-
ence on Computational Linguistics - Volume 2, ACL
’98, pages 768–774, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

[Och and Ney2000] Franz Josef Och and Hermann Ney.
2000. Giza++: Training of statistical translation
models.

[Okazaki2007] Naoaki Okazaki. 2007. Crfsuite: a fast
implementation of conditional random ﬁelds (crfs).
URL http://www.chokkan.org/software/crfsuite.

[Pieraccini et al.1992] R. Pieraccini, E. Tzoukermann,
Z. Gorelov, J. Gauvain, E. Levin, Chin-Hui Lee, and
J.G. Wilpon. 1992. A speech understanding sys-
tem based on statistical representation of semantics.
In Acoustics, Speech, and Signal Processing, 1992.
ICASSP-92., 1992 IEEE International Conference
on, volume 1, pages 193–196 vol.1, Mar.

[Raymond and Riccardi2007] Christian Raymond and
Giuseppe Riccardi. 2007. Generative and discrim-
inative algorithms for spoken language understand-
ing.
In International Conference on Speech Com-
munication and Technologies, pages 1605–1608,
Antwerp, Belgium, August.

[Razmara et al.2013] Majid Razmara, Maryam Siah-
bani, Gholamreza Haffari, and Anoop Sarkar.
2013. Graph propagation for paraphrasing out-of-
vocabulary words in statistical machine translation.
In Proceedings of the Conference of the Association
for Computational Linguistics (ACL).

[Subramanya et al.2010] Amarnag Subramanya, Slav
Petrov, and Fernando Pereira.
2010. Efﬁcient
graph-based semi-supervised learning of structured
tagging models.
In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’10, pages 167–176, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

[Talukdar and Crammer2009] ParthaPratim Talukdar
2009. New regularized
and Koby Crammer.
algorithms for transductive learning.
In Wray
Buntine, Marko Grobelnik, Dunja Mladeni, and
John Shawe-Taylor, editors, Machine Learning
and Knowledge Discovery in Databases, volume
5782 of Lecture Notes in Computer Science, pages
442–457. Springer Berlin Heidelberg.

[Khadivi and Ney2005] Shahram Khadivi and Her-
mann Ney.
2005. Automatic ﬁltering of bilin-
gual corpora for statistical machine translation. In
Andr´es Montoyo, Rafael Mu´noz, and Elisabeth
M´etais, editors, Natural Language Processing and
Information Systems, volume 3513 of Lecture Notes
in Computer Science, pages 263–274. Springer
Berlin Heidelberg.

[Talukdar and Pereira2010] Partha Pratim Talukdar and
Fernando Pereira. 2010. Experiments in graph-
based semi-supervised learning methods for class-
instance acquisition. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ’10, pages 1473–1481, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

[Talukdar et al.2008] Partha Pratim Talukdar, Joseph
Reisinger, Marius Pas¸ca, Deepak Ravichandran,
Rahul Bhagat, and Fernando Pereira.
2008.
Weakly-supervised acquisition of labeled class in-
stances using graph random walks. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP ’08, pages 582–
590, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

[Wang and Acero2006] Ye-Yi Wang and Alex Acero.
2006. Discriminative models for spoken language
understanding.
In International Conference on
Speech Communication and Technologies.

[Yarowsky1995] David Yarowsky.

1995. Unsuper-
vised word sense disambiguation rivaling supervised
methods. In Proceedings of the 33rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ’95, pages 189–196, Stroudsburg, PA, USA.
Association for Computational Linguistics.

[Zeng et al.2013] Xiaodong Zeng, Derek F Wong,
Lidia S Chao, and Isabel Trancoso. 2013. Graph-
based semi-supervised model for joint chinese word
segmentation and part-of-speech tagging.
In ACL,
pages 770–779.

[Zhu et al.2003] Xiaojin Zhu, Zoubin Ghahramani,
John Lafferty, et al. 2003. Semi-supervised learn-
ing using gaussian ﬁelds and harmonic functions. In
ICML, volume 3, pages 912–919.

This figure "graph.jpg" is available in "jpg"(cid:10) format from:

http://arxiv.org/ps/1701.08533v1

This figure "mainflowchart-en.jpg" is available in "jpg"(cid:10) format from:

http://arxiv.org/ps/1701.08533v1

7
1
0
2
 
n
a
J
 
0
3
 
 
]
L
C
.
s
c
[
 
 
1
v
3
3
5
8
0
.
1
0
7
1
:
v
i
X
r
a

Graph-Based Semi-Supervised Conditional Random Fields For Spoken
Language Understanding Using Unaligned Data

Mohammad Aliannejadi
Amirkabir University
of Technology
(Tehran Polytechnic)
m.aliannejadi@aut.ac.ir

Masoud Kiaeeha
Sharif University of
Technology
kiaeeha@ce.sharif.edu

Shahram Khadivi &
Saeed Shiry Ghidary
Amirkabir University
of Technology
(Tehran Polytechnic)
{khadivi, shiry}@aut.ac.ir

Abstract

experiment

graph-based

We
Semi-
Supervised Learning (SSL) of Conditional
Random Fields (CRF) for the application
of Spoken Language Understanding
(SLU) on unaligned data. The aligned
labels for examples are obtained using
IBM Model. We adapt a baseline semi-
supervised CRF by deﬁning new feature
set and altering the label propagation al-
gorithm. Our results demonstrate that our
proposed approach signiﬁcantly improves
the performance of the supervised model
by utilizing the knowledge gained from
the graph.

1 Introduction

The aim of Spoken Language Understanding
(SLU) is to interpret the intention of the user’s ut-
terance. More speciﬁcally, a SLU system attempts
to ﬁnd a mapping from user’s utterance in natu-
ral language, to the limited set of concepts that is
structured and meaningful for the computer. As an
example, for the sample utterance:

I want to return to Dallas on Thursday

It’s corresponding output would be:
GOAL : RETURN
TOLOC.CITY = Dallas
RETURN.DATE = Thursday.
SLU can be widely used in many real world appli-
cations; however, data processing costs may im-
pede practicability of it. Thus, attempting to train
a SLU model using less training data is a key issue.
The ﬁrst statistical SLU system was based
on hidden Markov model and modeled using a
ﬁnite state semantic tagger employed in AT&T’s
CHRONUS
system (Pieraccini et al., 1992).
Their semantic representation was ﬂat-concept;
but,
later He and Young (2005) extended the
representation to a hierarchical structure and

the

problem using
a push-down
modeled
There are other works which
automaton.
labeling
have dealt with SLU as a sequential
Raymond and Riccardi (2007) and
problem.
Wang and Acero (2006) have
fully annotated
the data and trained the model in discriminative
frameworks such as CRF. CRF captures many
complex dependencies and models the sequential
relations between the labels;
it is a
powerful framework for SLU.

therefore,

The Semi-Supervised Learning (SSL) approach
has drawn a raft of interest among the machine
learning community basically because of its prac-
tical application (Chapelle et al., 2006). Manual
tagging of data can take considerable effort and
time; however, in the training phase of SSL, a
large amount of unlabeled data along with a small
amount of labeled data is provided. This makes it
more practicable and cost effective than providing
a fully labeled set of training data; thus, SSL is
more favorable.

active

the most

Graph-based SSL,

area
of research in SSL in the recent years, has
shown to outperform other SSL methods
(Chapelle et al., 2006).
Graph-based SSL al-
gorithms are generally run in two steps: graph
construction and label propagation.
Graph
construction is the most important step in graph-
based SSL; and,
the fundamental approach is
to assign labeled and unlabeled examples to
nodes of the graph. Then, a similarity function
is applied to compute similarity between pairs
of nodes. The computed similarities are then
assigned as the weight of the edges connecting
Label propaga-
the nodes (Zhu et al., 2003).
tion operates on the constructed graph. Based
on the constraints or properties derived from
the graph,
labels are propagated from a few
labeled nodes to the entire graph. These con-
straints
(Zhu et al., 2003;
Talukdar et al., 2008;
Subramanya et al., 2010;

include smoothness

Garrette and Baldridge, 2013),
(Das and Smith, 2012; Zeng et al., 2013).

and

sparsity

Labeling unaligned training data requires
much less effort compared to aligned data
(He and Young, 2005). Nevertheless, unaligned
data cannot be used to train a CRF model directly
since CRF requires fully-annotated data. On the
other hand, robust parameter estimation of a CRF
model requires a large set of training data which
is unrealistic in many practical applications. To
overcome this problem, the work in this paper ap-
plies semi-supervised CRF on unlabeled data. It is
motivated by the hypothesis that data is aligned to
labels in a monotone manner, and words appear-
ing in similar contexts tend to have same labels.
Under these circumstances, we were able to reach
1.64% improvement on the F-score over the super-
vised CRF and 1.38% improvement on the F-score
over the self trained CRF.

In the following section we describe the algo-
rithm this work is based on and our proposed al-
gorithm. In Section 3 we evaluate our work and in
the ﬁnal section conclusions are drawn.

2 Semi-supervised Spoken Language

Understanding

The input data is unaligned and represented
as a semantic tree, which is described in
(He and Young, 2005). The training sentences and
their corresponding semantic trees can be aligned
monotonically; hence, we chose IBM Model 5
(Khadivi and Ney, 2005) to ﬁnd the best align-
ment between the words and nodes of the seman-
tic tree (labels). Thus, we have circumvented the
problem of unaligned data. More detailed expla-
nation about this process can be found in our pre-
vious work (Aliannejadi et al., 2014). This data
is then used to train the supervised and semi-
supervised CRFs.

2.1 Semi-supervised CRF

The proposed semi-supervised learning algorithm
is based on (Subramanya et al., 2010). Here, we
quickly review this algorithm (Algorithm 1).

In the ﬁrst step, the CRF model is trained on the

labeled data (Dl) according to (1):

Λ∗ = arg min

Λ∈RK h −

l

X
i=1

log p(yi|xi; Λ) + γkΛk2

i,
(1)

Algorithm 1 Semi-Supervised Training of CRF
1: Λ(n=0) = TrainCRF(Dl)
2: G = BuildGraph(Dl ∪ Du)
3: {r} = CalcEmpiricalDistribution(Dl)
4: while not converged do
5:

6:

{m} = CalcMarginals(Du, Λn)
{q} = AverageMarginals(m)
{ˆq} = LabelPropagation(q, r)
u = ViterbiDecode({ˆq}, Λn)
Λn+1 = RetrainCRF(Dl ∪ Dv

7:
8: Dv
9:
10: end while
11: Return ﬁnal Λn

u, Λn);

where Λ∗ is the optimal parameter set of the base
CRF model and kΛk2 is the squared ℓ2-norm reg-
ularizer whose impact is adjusted by γ. At the ﬁrst
line, Λ∗ is assigned to Λ(n=0) i.e.
the initial pa-
rameter set of the model.

In the next step, the k-NN similarity graph (G)
is constructed (line 2), which will be discussed in
more detail in Section 2.3. In the third step, the
empirical label distribution (r) on the labeled data
is computed. The main loop of the algorithm is
then started and the execution continues until the
results converge.

Marginal probability of labels (m) are then com-
puted on the unlabeled data (Du) using Forward-
Backward algorithm with the parameters of the
previous CRF model (Λn), and in the next step,
all the marginal label probabilities of each trigram
are averaged over its occurrences (line 5 and 6).

In label propagation (line 7), trigram marginals
(q) are propagated through the similarity graph
using an iterative algorithm. Thus, they become
smooth. Empirical label distribution (r) serves as
the priori label information for labeled data and
trigram marginals (q) act as the seed labels. More
detailed discussion is found in Section 2.4.

Afterwards, having the results of label propaga-
tion (ˆq) and previous CRF model parameters, la-
bels of the unlabeled data are estimated by com-
bining the interpolated label marginals and the
CRF transition potentials (line 8). For every word
position j for i indexing over sentences, interpo-
lated label marginals are calculated as follows:

ˆp(y(j)

i = y|xi) = αp(y(j)

i = y|xi; Λn)
+ (1 − α)ˆqT (i,j)(y),

(2)

where T (i, j) is a trigram centered at position j of
the ith sentence and α is the interpolation factor.

Description

Feature

Context
Left Context
Right Context
Center Word in trigram
Center is Class
Center is Preposition
Left is Preposition

x1 x2 x3 x4 x5
x1 x2
x4 x5
x3
IsClass(x3)
IsP reposition(x3)
IsP reposition(x2)

Table 1: Context Features used for constructing
the similarity graph

In the ﬁnal step, the previous CRF model pa-
rameters are regularized using the labels estimated
for the unlabeled data in the previous step (line 9)
as follows:

Λn+1 = arg min

Λ∈RK h −

log p(yi|xi; Λn)

l

X
i=1

− η

log p(yi|xi; Λn) + γkΛk2

i,

(3)

u

X
i=l+1

where η is a trade-off parameter whose setting is
discussed later in Section 3.

2.2 CRF Features

are

By aligning the training data, many infor-
saved which are omit-
mative
labels
(Wang and Acero, 2006;
ted in other works
Raymond and Riccardi, 2007). By saving these
information, the ﬁrst order label dependency helps
the model to predict the labels more precisely.
the
Therefore the model manages to predict
labels using less lexical features and the feature
window that was [-4,+2] in previous works is
reduced to [0,+2]. Using smaller feature win-
dow improves the generalization of the model
(Aliannejadi et al., 2014).

2.3 Similarity Graph

In our work we have considered trigrams as the
nodes of the graph and extracted features of each
trigram x2 x3 x4 according to the 5-word con-
text x1 x2 x3 x4 x5 it appears in. These features
are carefully selected so that nodes are correctly
placed in neighborhood of the ones having simi-
lar labels. Table 1 presents the feature set that we
have applied to construct the similarity graph.

IsClass feature impacts the structure of the
In the pre-processing phase

graph signiﬁcantly.

speciﬁc words are marked as classes according to
the corpus’ accompanying database. As an ex-
ample, city names such as Dallas and Baltimore
are represented as city name which is a class type.
Since these classes play an important role in calcu-
lating similarity of the nodes, IsClass feature is
used to determine if a given position in a context
is a class type.

Furthermore, prepositions like from and be-
tween are also important, e.g. when two trigrams
like ”from Washington to” and ”between Dallas
and” are compared. The two trigrams are totally
different while both of them begin with a prepo-
sition and are continued with a class. Therefore,
IsP reposition feature would be particularly suit-
able to increase the similarity score of these two
trigrams.
In many cases, these features have a
signiﬁcant effect in assigning a better similarity
score.

To deﬁne a similarity measure, we compute
the Pointwise Mutual Information (PMI) between
all occurrences of a trigram and each of the
features. The PMI measure transforms the in-
dependence assumption into a ratio (Lin, 1998;
Razmara et al., 2013). Then, the similarity be-
tween two nodes is measured as the cosine dis-
tance between their PMI vectors. We carefully ex-
amined the similarity graph on the training data
and found out the head and tail trigrams of each
sentence which contain dummy words, make the
graph sparse. Hence, we have ignored those tri-
grams.

2.4 Label Propagation

After statistical alignment, the training data gets
noisy. Hence, use of traditional label propagation
algorithms causes an error propagation over the
whole graph and degrades the whole system per-
formance. Thus, we make use of the Modiﬁed Ad-
sorption (MAD) algorithm for label propagation.
MAD algorithm controls the label propagation
more strictly. This is accomplished by limiting
the amount of information that passes from a node
to another (Talukdar and Pereira, 2010). Soft label
vectors ˆYv are found by solving the unconstrained
optimization problem in (4):

hµ1(Yl − ˆYl)⊤S (Yl − ˆYl)

min
ˆY X
l∈C
+ µ2 ˆYl

⊤

L′ ˆYl + µ3(cid:13)
where µi are hyper-parameters and Rl

ˆYl − Rl(cid:13)
(cid:13)
(cid:13)

i,

2

(4)

is the

empirical label distribution over labels i.e.
the
prior belief about the labeling of a node. The
ﬁrst term of the summation is related to label
score injection from the initial score of the node
and makes the output match the seed labels Yl
(Razmara et al., 2013). The second term is asso-
ciated with label score acquisition from neighbor
nodes i.e. smooths the labels according to the sim-
ilarity graph. In the last term, the labels are regu-
larized to match a priori label Rl in order to avoid
false labels for high degree unlabeled nodes. A
solution to the optimization problem in (4) can
be found with an efﬁcient iterative algorithm de-
scribed in (Talukdar and Crammer, 2009).

Many errors of the alignment model are cor-
rected through label propagation using the MAD
algorithm; whereas, those errors are propagated
in traditional label propagation algorithms such as
the one mentioned in (Subramanya et al., 2010).

2.5 System Overview

We have implemented the Graph Construc-
implemented
tion in Java and the CRF is
by modifying the source code of CRFSuite
(Okazaki, 2007). We have also modiﬁed Junto
toolkit (Talukdar and Pereira, 2010) and used it
for graph propagation. The whole source code of
our system is available online1. The input utter-
ances and their corresponding semantic trees are
aligned using GIZA++ (Och and Ney, 2000); and
then used to train the base CRF model. The graph
is constructed using the labeled and unlabeled data
and the main loop of the algorithm continues until
convergence. The ﬁnal parameters of the CRF are
retained for decoding in the test phase.

3 Experimental Results

section we evaluate our

In this
results on
Air Travel Information Service (ATIS) data-set
(Dahl et al., 1994) which consists of 4478 train-
ing, 500 development and 896 test utterances.
The development set was chosen randomly. To
evaluate our work, we have compared our results
with results from Supervised CRF and Self-trained
CRF (Yarowsky, 1995).

For our experiments we set hyper-parameters as
for graph propagation, µ1 = 1, µ2 =
follows:
0.01, µ3 = 0.01, for Viterbi decoding, α = 0.1,
for CRF-retraining, η = 0.1, γ = 0.01. We have

1https://github.com/maxxkia/g-ssl-crf

% of Labeled Data
30
20
10
88.64
87.69
86.07
88.64
87.73
86.34
89.12
88.75
Semi-supervised CRF 87.72

Supervised CRF
Self-trained CRF

Table 2:
Slot/Value F-score in %.

Comparison of

training results.

chosen these parameters along with graph fea-
tures and graph-related parameters by evaluating
the model on the development set. We employed
the L-BFGS algorithm to optimize CRF objective
functions; which is designed to be fast and low-
memory consumer for the high-dimensional opti-
mization problems (Bertsekas, 1999).

We have post-processed the sequence of labels
to obtain the slots and their values. The slot-
value pair is compared to the reference test set and
the result is reported in F-score of slot classiﬁca-
tion. Table 2 demonstrates results obtained from
our semi-supervised CRF algorithm compared to
the supervised CRF and self-trained CRF. Experi-
ments were carried out having 10%, 20% and 30%
of data being labeled. For each of these tests, la-
beled set was selected randomly from the train-
ing set. This procedure was done 10 times and
the reported results are the average of the results
thereof. The Supervised CRF model is trained
only on the labeled fraction of the data. How-
ever, the Self-trained CRF and Semi-supervised
CRF have access to the rest of the data as well,
which are unlabeled. Our Supervised CRF gained
91.02 F-score with 100% of the data labeled which
performs better compared to 89.32% F-score of
Raymond and Riccardi (2007) CRF model.

As shown in Table 2, the proposed method per-
forms better compared to supervised CRF and
self-trained CRF. The most signiﬁcant improve-
ment occurs when only 10% of training set is
labeled; where we gain 1.65% improvement on
F-score compared to supervised CRF and 1.38%
compared to self-trained CRF.

4 Conclusion

We presented a simple algorithm to train CRF in a
semi-supervised manner using unaligned data for
SLU. By saving many informative labels in the
alignment phase, the base model is trained using
fewer features. The parameters of the CRF model
are estimated using much less labeled data by

regularizing the model using a nearest-neighbor
graph. Results demonstrate that our proposed al-
gorithm signiﬁcantly improves the performance
compared to supervised and self-trained CRF.

References

[Aliannejadi et al.2014] Mohammad

Aliannejadi,
Shahram Khadivi, SaeedShiry Ghidary, and Mo-
hammadHadi Bokaei. 2014. Discriminative spoken
language understanding using statistical machine
translation alignment models.
In Ali Movaghar,
Mansour Jamzad, and Hossein Asadi, editors, Artiﬁ-
cial Intelligence and Signal Processing, volume 427
of Communications in Computer and Information
Science, pages 194–202. Springer
International
Publishing.

[Bertsekas1999] Dimitri P Bertsekas. 1999. Nonlinear

programming.

[Chapelle et al.2006] Olivier

Chapelle,

Sch¨olkopf, Alexander Zien, et al.
supervised learning, volume 2.
Cambridge.

2006.

Bernhard
Semi-
MIT press

[Dahl et al.1994] Deborah A. Dahl, Madeleine Bates,
Michael Brown, William Fisher, Kate Hunicke-
Smith, David Pallett, Christine Pao, Alexander Rud-
nicky, and Elizabeth Shriberg. 1994. Expanding the
scope of the atis task: The atis-3 corpus. In Proceed-
ings of the Workshop on Human Language Technol-
ogy, HLT ’94, pages 43–48, Stroudsburg, PA, USA.
Association for Computational Linguistics.

[Das and Smith2012] Dipanjan Das and Noah A.
Smith. 2012. Graph-based lexicon expansion with
sparsity-inducing penalties.
In Proceedings of the
2012 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies, NAACL HLT ’12,
pages 677–687, Stroudsburg, PA, USA. Association
for Computational Linguistics.

[Garrette and Baldridge2013] Dan Garrette and Jason
Baldridge. 2013. Learning a part-of-speech tag-
ger from two hours of annotation. In Proceedings
of NAACL-HLT, pages 138–147.

[He and Young2005] Yulan He and Steve Young. 2005.
Semantic processing using the hidden vector state
model. Computer Speech & Language, 19(1):85 –
106.

[Lin1998] Dekang Lin. 1998. Automatic retrieval and
In Proceedings of the
clustering of similar words.
36th Annual Meeting of the Association for Compu-
tational Linguistics and 17th International Confer-
ence on Computational Linguistics - Volume 2, ACL
’98, pages 768–774, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

[Och and Ney2000] Franz Josef Och and Hermann Ney.
2000. Giza++: Training of statistical translation
models.

[Okazaki2007] Naoaki Okazaki. 2007. Crfsuite: a fast
implementation of conditional random ﬁelds (crfs).
URL http://www.chokkan.org/software/crfsuite.

[Pieraccini et al.1992] R. Pieraccini, E. Tzoukermann,
Z. Gorelov, J. Gauvain, E. Levin, Chin-Hui Lee, and
J.G. Wilpon. 1992. A speech understanding sys-
tem based on statistical representation of semantics.
In Acoustics, Speech, and Signal Processing, 1992.
ICASSP-92., 1992 IEEE International Conference
on, volume 1, pages 193–196 vol.1, Mar.

[Raymond and Riccardi2007] Christian Raymond and
Giuseppe Riccardi. 2007. Generative and discrim-
inative algorithms for spoken language understand-
ing.
In International Conference on Speech Com-
munication and Technologies, pages 1605–1608,
Antwerp, Belgium, August.

[Razmara et al.2013] Majid Razmara, Maryam Siah-
bani, Gholamreza Haffari, and Anoop Sarkar.
2013. Graph propagation for paraphrasing out-of-
vocabulary words in statistical machine translation.
In Proceedings of the Conference of the Association
for Computational Linguistics (ACL).

[Subramanya et al.2010] Amarnag Subramanya, Slav
Petrov, and Fernando Pereira.
2010. Efﬁcient
graph-based semi-supervised learning of structured
tagging models.
In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’10, pages 167–176, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

[Talukdar and Crammer2009] ParthaPratim Talukdar
2009. New regularized
and Koby Crammer.
algorithms for transductive learning.
In Wray
Buntine, Marko Grobelnik, Dunja Mladeni, and
John Shawe-Taylor, editors, Machine Learning
and Knowledge Discovery in Databases, volume
5782 of Lecture Notes in Computer Science, pages
442–457. Springer Berlin Heidelberg.

[Khadivi and Ney2005] Shahram Khadivi and Her-
mann Ney.
2005. Automatic ﬁltering of bilin-
gual corpora for statistical machine translation. In
Andr´es Montoyo, Rafael Mu´noz, and Elisabeth
M´etais, editors, Natural Language Processing and
Information Systems, volume 3513 of Lecture Notes
in Computer Science, pages 263–274. Springer
Berlin Heidelberg.

[Talukdar and Pereira2010] Partha Pratim Talukdar and
Fernando Pereira. 2010. Experiments in graph-
based semi-supervised learning methods for class-
instance acquisition. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ’10, pages 1473–1481, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

[Talukdar et al.2008] Partha Pratim Talukdar, Joseph
Reisinger, Marius Pas¸ca, Deepak Ravichandran,
Rahul Bhagat, and Fernando Pereira.
2008.
Weakly-supervised acquisition of labeled class in-
stances using graph random walks. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP ’08, pages 582–
590, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

[Wang and Acero2006] Ye-Yi Wang and Alex Acero.
2006. Discriminative models for spoken language
understanding.
In International Conference on
Speech Communication and Technologies.

[Yarowsky1995] David Yarowsky.

1995. Unsuper-
vised word sense disambiguation rivaling supervised
methods. In Proceedings of the 33rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ’95, pages 189–196, Stroudsburg, PA, USA.
Association for Computational Linguistics.

[Zeng et al.2013] Xiaodong Zeng, Derek F Wong,
Lidia S Chao, and Isabel Trancoso. 2013. Graph-
based semi-supervised model for joint chinese word
segmentation and part-of-speech tagging.
In ACL,
pages 770–779.

[Zhu et al.2003] Xiaojin Zhu, Zoubin Ghahramani,
John Lafferty, et al. 2003. Semi-supervised learn-
ing using gaussian ﬁelds and harmonic functions. In
ICML, volume 3, pages 912–919.

This figure "graph.jpg" is available in "jpg"(cid:10) format from:

http://arxiv.org/ps/1701.08533v1

This figure "mainflowchart-en.jpg" is available in "jpg"(cid:10) format from:

http://arxiv.org/ps/1701.08533v1

