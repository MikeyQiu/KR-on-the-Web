8
1
0
2
 
b
e
F
 
7
2
 
 
]

G
L
.
s
c
[
 
 
2
v
7
6
4
0
1
.
5
0
7
1
:
v
i
X
r
a

Federated Multi-Task Learning

Virginia Smith
Stanford
smithv@stanford.edu

Chao-Kai Chiang∗
USC
chaokaic@usc.edu

Maziar Sanjabi∗
USC
maziarsanjabi@gmail.com

Ameet Talwalkar
CMU
talwalkar@cmu.edu

Abstract

Federated learning poses new statistical and systems challenges in training machine
learning models over distributed networks of devices. In this work, we show that
multi-task learning is naturally suited to handle the statistical challenges of this
setting, and propose a novel systems-aware optimization method, MOCHA, that is
robust to practical systems issues. Our method and theory for the ﬁrst time consider
issues of high communication cost, stragglers, and fault tolerance for distributed
multi-task learning. The resulting method achieves signiﬁcant speedups compared
to alternatives in the federated setting, as we demonstrate through simulations on
real-world federated datasets.

1

Introduction

Mobile phones, wearable devices, and smart homes are just a few of the modern distributed networks
generating massive amounts of data each day. Due to the growing storage and computational
power of devices in these networks, it is increasingly attractive to store data locally and push more
network computation to the edge. The nascent ﬁeld of federated learning explores training statistical
models directly on devices [37]. Examples of potential applications include: learning sentiment,
semantic location, or activities of mobile phone users; predicting health events like low blood sugar
or heart attack risk from wearable devices; or detecting burglaries within smart homes [3, 39, 42].
Following [25, 36, 26], we summarize the unique challenges of federated learning below.

1. Statistical Challenges: The aim in federated learning is to ﬁt a model to data, {X1, . . . , Xm},
generated by m distributed nodes. Each node, t ∈ [m], collects data in a non-IID manner across the
network, with data on each node being generated by a distinct distribution Xt ∼ Pt. The number
of data points on each node, nt, may also vary signiﬁcantly, and there may be an underlying
structure present that captures the relationship amongst nodes and their associated distributions.
2. Systems Challenges: There are typically a large number of nodes, m, in the network, and
communication is often a signiﬁcant bottleneck. Additionally, the storage, computational, and
communication capacities of each node may differ due to variability in hardware (CPU, memory),
network connection (3G, 4G, WiFi), and power (battery level). These systems challenges, com-
pounded with unbalanced data and statistical heterogeneity, make issues such as stragglers and
fault tolerance signiﬁcantly more prevalent than in typical data center environments.

In this work, we propose a modeling approach that differs signiﬁcantly from prior work on federated
learning, where the aim thus far has been to train a single global model across the network [25, 36, 26].
Instead, we address statistical challenges in the federated setting by learning separate models for each
node, {w1, . . . , wm}. This can be naturally captured through a multi-task learning (MTL) framework,
where the goal is to consider ﬁtting separate but related models simultaneously [14, 2, 57, 28].
Unfortunately, current multi-task learning methods are not suited to handle the systems challenges
that arise in federated learning, including high communication cost, stragglers, and fault tolerance.
Addressing these challenges is therefore a key component of our work.

∗Authors contributed equally.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

1.1 Contributions

We make the following contributions. First, we show that MTL is a natural choice to handle statistical
challenges in the federated setting. Second, we develop a novel method, MOCHA, to solve a general
MTL problem. Our method generalizes the distributed optimization method COCOA [22, 31] in
order to address systems challenges associated with network size and node heterogeneity. Third, we
provide convergence guarantees for MOCHA that carefully consider these unique systems challenges
and provide insight into practical performance. Finally, we demonstrate the superior empirical
performance of MOCHA with a new benchmarking suite of federated datasets.

2 Related Work

Learning Beyond the Data Center. Computing SQL-like queries across distributed, low-powered
nodes is a decades-long area of research that has been explored under the purview of query processing
in sensor networks, computing at the edge, and fog computing [32, 12, 33, 8, 18, 15]. Recent works
have also considered training machine learning models centrally but serving and storing them locally,
e.g., this is a common approach in mobile user modeling and personalization [27, 43, 44]. However,
as the computational power of the nodes within distributed networks grows, it is possible to do even
more work locally, which has led to recent interest in federated learning.2 In contrast to our proposed
approach, existing federated learning approaches [25, 36, 26, 37] aim to learn a single global model
across the data.3 This limits their ability to deal with non-IID data and structure amongst the nodes.
These works also come without convergence guarantees, and have not addressed practical issues of
stragglers or fault tolerance, which are important characteristics of the federated setting. The work
proposed here is, to the best of our knowledge, the ﬁrst federated learning framework to consider
these challenges, theoretically and in practice.

Multi-Task Learning.
In multi-task learning, the goal is to learn models for multiple related tasks
simultaneously. While the MTL literature is extensive, most MTL modeling approaches can be
broadly categorized into two groups based on how they capture relationships amongst tasks. The ﬁrst
(e.g., [14, 4, 11, 24]) assumes that a clustered, sparse, or low-rank structure between the tasks is known
a priori. A second group instead assumes that the task relationships are not known beforehand and
can be learned directly from the data (e.g., [21, 57, 16]). In this work, we focus our attention on this
latter group, as task relationships may not be known beforehand in real-world settings. In comparison
to learning a single global model, these MTL approaches can directly capture relationships amongst
non-IID and unbalanced data, which makes them particularly well-suited for the statistical challenges
of federated learning. We demonstrate this empirically on real-world federated datasets in Section 5.
However, although MTL is a natural modeling choice to address the statistical challenges of federated
learning, currently proposed methods for distributed MTL (discussed below) do not adequately
address the systems challenges associated with federated learning.

Distributed Multi-Task Learning. Distributed multi-task learning is a relatively new area of
research, in which the aim is to solve an MTL problem when data for each task is distributed over a
network. While several recent works [1, 35, 54, 55] have considered the issue of distributed MTL
training, the proposed methods do not allow for ﬂexibility of communication versus computation.
As a result, they are unable to efﬁciently handle concerns of fault tolerance and stragglers, the latter
of which stems from both data and system heterogeneity. The works of [23] and [7] allow for
asynchronous updates to help mitigate stragglers, but do not address fault tolerance. Moreover, [23]
provides no convergence guarantees, and the convergence of [7] relies on a bounded delay assumption
that is impractical for the federated setting, where delays may be signiﬁcant and devices may drop
out completely. Finally, [30] proposes a method and setup leveraging the distributed framework
COCOA [22, 31], which we show in Section 4 to be a special case of the more general approach in
this work. However, the authors in [30] do not explore the federated setting, and their assumption
that the same amount of work is done locally on each node is prohibitive in federated settings, where
unbalance is common due to data and system variability.

2The term on-device learning has been used to describe both the task of model training and of model serving.

Due to the ambiguity of this phrase, we exclusively use the term federated learning.

3While not the focus of our work, we note privacy is an important concern in the federated setting, and that
the privacy beneﬁts associated with global federated learning (as discussed in [36]) also apply to our approach.

2

3 Federated Multi-Task Learning

In federated learning, the aim is to learn a model over data that resides on, and has been generated by,
m distributed nodes. As a running example, consider learning the activities of mobile phone users in
a cell network based on their individual sensor, text, or image data. Each node (phone), t ∈ [m], may
generate data via a distinct distribution, and so it is natural to ﬁt separate models, {w1, . . . , wm},
to the distributed data—one for each local dataset. However, structure between models frequently
exists (e.g., people may behave similarly when using their phones), and modeling these relationships
via multi-task learning is a natural strategy to improve performance and boost the effective sample
size for each node [10, 2, 5]. In this section, we suggest a general MTL framework for the federated
setting, and propose a novel method, MOCHA, to handle the systems challenges of federated MTL.

3.1 General Multi-Task Learning Setup

Given data Xt ∈ Rd×nt from m nodes, multi-task learning ﬁts separate weight vectors wt ∈ Rd to
the data for each task (node) through arbitrary convex loss functions (cid:96)t (e.g., the hinge loss for SVM
models). Many MTL problems can be captured via the following general formulation:

(cid:40) m
(cid:88)

nt(cid:88)

t=1

i=1

min
W,Ω

(cid:96)t(wT

t xi

t, yi

t) + R(W, Ω)

,

(cid:41)

(1)

where W := [w1, . . . , wm] ∈ Rd×m is a matrix whose t-th column is the weight vector for the
t-th task. The matrix Ω ∈ Rm×m models relationships amongst tasks, and is either known a
priori or estimated while simultaneously learning task models. MTL problems differ based on their
assumptions on R, which takes Ω as input and promotes some suitable structure amongst the tasks.

As an example, several popular MTL approaches assume that tasks form clusters based on whether or
not they are related [14, 21, 57, 58]. This can be expressed via the following bi-convex formulation:
R(W, Ω) = λ1 tr(cid:0)WΩWT (cid:1) + λ2(cid:107)W(cid:107)2
F ,
(2)
with constants λ1, λ2 > 0, and where the second term performs L2 regularization on each local
model. We use a similar formulation (14) in our experiments in Section 5, and provide details on
other common classes of MTL models that can be formulated via (1) in Appendix B.

3.2 MOCHA: A Framework for Federated Multi-Task Learning

In the federated setting, the aim is to train statistical models directly on the edge, and thus we
solve (1) while assuming that the data {X1, . . . , Xm} is distributed across m nodes or devices.
Before proposing our federated method for solving (1), we make the following observations:

• Observation 1: In general, (1) is not jointly convex in W and Ω, and even in the cases where (1)

is convex, solving for W and Ω simultaneously can be difﬁcult [5].

• Observation 2: When ﬁxing Ω, updating W depends on both the data X, which is distributed

across the nodes, and the structure Ω, which is known centrally.

• Observation 3: When ﬁxing W, optimizing for Ω only depends on W and not on the data X.

Based on these observations, it is natural to propose an alternating optimization approach to solve
problem (1), in which at each iteration we ﬁx either W or Ω and optimize over the other, alternating
until convergence is reached. Note that solving for Ω is not dependent on the data and therefore can
be computed centrally; as such, we defer to prior work for this step [58, 21, 57, 16]. In Appendix B,
we discuss updates to Ω for several common MTL models.

In this work, we focus on developing an efﬁcient distributed optimization method for the W step. In
traditional data center environments, the task of distributed training is a well-studied problem, and
various communication-efﬁcient frameworks have been recently proposed, including the state-of-the-
art primal-dual COCOA framework [22, 31]. Although COCOA can be extended directly to update
W in a distributed fashion across the nodes, it cannot handle the unique systems challenges of the
federated environment, such as stragglers and fault tolerance, as discussed in Section 3.4. To this
end, we extend COCOA and propose a new method, MOCHA, for federated multi-task learning. Our
method is given in Algorithm 1 and described in detail in Sections 3.3 and 3.4.

3

Set subproblem parameter σ(cid:48) and number of federated iterations, Hi
for iterations h = 0, 1, · · · , Hi do

Algorithm 1 MOCHA: Federated Multi-Task Learning Framework
1: Input: Data Xt from t = 1, . . . , m tasks, stored on one of m nodes, and initial matrix Ω0
2: Starting point α(0) := 0 ∈ Rn, v(0) := 0 ∈ Rb
3: for iterations i = 0, 1, . . . do
4:
5:
6:
7:
8:
9:
10:
Update Ω centrally based on w(α) for latest α
11:
12: Central node computes w = w(α) based on the lastest α
13: return: W := [w1, . . . , wm]

call local solver, returning θh
update local variables αt ← αt + ∆αt
return updates ∆vt := Xt∆αt

for tasks t ∈ {1, 2, . . . , m} in parallel over m nodes do

reduce: vt ← vt + ∆vt

t -approximate solution ∆αt of the local subproblem (4)

3.3 Federated Update of W

To update W in the federated setting, we begin by extending works on distributed primal-dual
optimization [22, 31, 30] to apply to the generalized multi-task framework (1). This involves deriving
the appropriate dual formulation, subproblems, and problem parameters, as we detail below.

Dual problem. Considering the dual formulation of (1) will allow us to better separate the global
problem into distributed subproblems for federated computation across the nodes. Let n := (cid:80)m
t=1 nt
and X := Diag(X1, · · · , Xm) ∈ Rmd×n. With Ω ﬁxed, the dual of problem (1), deﬁned with
respect to dual variables α ∈ Rn, is given by:
m
(cid:88)

nt(cid:88)

(cid:40)

(cid:41)

min
α

D(α) :=

t (−αi
(cid:96)∗

t) + R∗(Xα)

,

i=1
t and R∗ are the conjugate dual functions of (cid:96)t and R, respectively, and αi

where (cid:96)∗
t is the dual
variable for the data point (xi
t). Note that R∗ depends on Ω, but for the sake of simplicity, we
have removed this in our notation. To derive distributed subproblems from this global dual, we make
an assumption described below on the regularizer R.

t, yi

t=1

(3)

Assumption 1. Given Ω, we assume that there exists a symmetric positive deﬁnite matrix M ∈
Rmd×md, depending on Ω, for which the function R is strongly convex with respect to M−1. Note
that this corresponds to assuming that R∗ will be smooth with respect to matrix M.
Remark 1. We can reformulate the MTL regularizer in the form of ¯R(w, ¯Ω) = R(W, Ω), where
w ∈ Rmd is a vector containing the columns of W and ¯Ω := Ω ⊗ Id×d ∈ Rmd×md. For example,
we can rewrite the regularizer in (2) as ¯R(w, ¯Ω) = tr(cid:0)wT (λ1 ¯Ω + λ2I)w(cid:1). Writing the regularizer
in this form, it is clear that it is strongly convex with respect to matrix M−1 = λ1 ¯Ω + λ2I.

Data-local quadratic subproblems. To solve (1) across distributed nodes, we deﬁne the following
data-local subproblems, which are formed via a careful quadratic approximation of the dual problem
(3) to separate computation across the nodes. These subproblems ﬁnd updates ∆αt ∈ Rnt to the dual
variables in α corresponding to a single node t, and only require accessing data which is available
locally, i.e., Xt for node t. The t-th subproblem is given by:
nt(cid:88)

Gσ(cid:48)
t (∆αt; vt, αt) :=

t (−αi
(cid:96)∗

t−∆αi

t)+(cid:104)wt(α), Xt∆αt(cid:105)+

(cid:107)Xt∆αt(cid:107)2

Mt

+c(α) ,

(4)

min
∆αt

σ(cid:48)
2

i=1

m R∗(Xα), and Mt ∈ Rd×d is the t-th diagonal block of the symmetric positive
where c(α) := 1
deﬁnite matrix M. Given dual variables α, corresponding primal variables can be found via w(α) =
∇R∗(Xα), where wt(α) is the t-th block in the vector w(α). Note that computing w(α) requires
the vector v = Xα. The t-th block of v, vt ∈ Rd, is the only information that must be communicated
between nodes at each iteration. Finally, σ(cid:48) > 0 measures the difﬁculty of the data partitioning, and
helps to relate progress made to the subproblems to the global dual problem. It can be easily selected
based on M for many applications of interest; we provide details in Lemma 9 of the Appendix.

4

3.4 Practical Considerations

During MOCHA’s federated update of W, the central node requires a response from all workers before
performing a synchronous update. In the federated setting, a naive execution of this communication
protocol could introduce dramatic straggler effects due to node heterogeneity. To avoid stragglers,
MOCHA provides the t-th node with the ﬂexibility to approximately solve its subproblem Gσ(cid:48)
t (·),
where the quality of the approximation is controled by a per-node parameter θh
t . The following
factors determine the quality of the t-th node’s solution to its subproblem:

1. Statistical challenges, such as the size of Xt and the intrinsic difﬁculty of subproblem Gσ(cid:48)
2. Systems challenges, such as the node’s storage, computational, and communication capacities
due to hardware (CPU, memory), network connection (3G, 4G, WiFi), and power (battery level).

t (·).

3. A global clock cycle imposed by the central node specifying a deadline for receiving updates.

t from the current clock cycle and statistical/systems setting. θh
t = 0 indicates an exact solution to Gσ(cid:48)

We deﬁne θh
t as a function of these factors, and assume that each node has a controller that may
derive θh
t ranges from zero to one,
where θh
t (·) and θh
t = 1 indicates that node t made no progress
during iteration h (which we refer to as a dropped node). For instance, a node may ‘drop’ if it runs
out of battery, or if its network bandwidth deteriorates during iteration h and it is thus unable to return
its update within the current clock cycle. A formal deﬁnition of θh

t is provided in (5) of Section 4.

MOCHA mitigates stragglers by enabling the t-th node to deﬁne its own θh
t . On every iteration
h, the local updates that a node performs and sends in a clock cycle will yield a speciﬁc value
for θh
t . As discussed in Section 4, MOCHA is additionally robust to a small fraction of nodes
periodically dropping and performing no local updates (i.e., θh
:= 1) under suitable conditions,
t
as deﬁned in Assumption 2. In contrast, prior work of COCOA may suffer from severe straggler
effects in federated settings, as it requires a ﬁxed θh
t = θ across all nodes and all iterations while still
maintaining synchronous updates, and it does not allow for the case of dropped nodes (θ := 1).

Finally, we note that asynchronous updating schemes are an alternative approach to mitigate stragglers.
We do not consider these approaches in this work, in part due to the fact that the bounded-delay
assumptions associated with most asynchronous schemes limit fault tolerance. However, it would be
interesting to further explore the differences and connections between asynchronous methods and
approximation-based, synchronous methods like MOCHA in future work.

4 Convergence Analysis

MOCHA is based on a bi-convex alternating approach, which is guaranteed to converge [17, 45] to
a stationary solution of problem (1). In the case where this problem is jointly convex with respect
to W and Ω, such a solution is also optimal. In the rest of this section, we therefore focus on the
convergence of solving the W update of MOCHA in the federated setting. Following the discussion
in Section 3.4, we ﬁrst introduce the following per-node, per-round approximation parameter.
Deﬁnition 1 (Per-Node-Per-Iteration-Approximation Parameter). At each iteration h, we deﬁne the
accuracy level of the solution calculated by node t to its subproblem (4) as:
t ; v(h), α(h)
t
)

; v(h), α(h)

t (∆α(cid:63)

) − Gσ(cid:48)

θh
t :=

(5)

)

,

t

t (∆α(h)
Gσ(cid:48)
t (0; v(h), α(h)
Gσ(cid:48)
t is the minimizer of subproblem Gσ(cid:48)

t

t
) − Gσ(cid:48)
t (∆α(cid:63)
t (· ; v(h), α(h)

t

t

t ; v(h), α(h)
). We allow this value to vary between
t are made by node t at iteration h.

t := 1 meaning that no updates to subproblem Gσ(cid:48)

where ∆α(cid:63)
[0, 1], with θh

While the ﬂexible per-node, per-iteration approximation parameter θh
t in (5) allows the consideration
of stragglers and fault tolerance, these additional degrees of freedom also pose new challenges in
providing convergence guarantees for MOCHA. We introduce the following assumption on θh
t to
provide our convergence guarantees.
Assumption 2. Let Hh := (α(h), α(h−1), · · · , α(1)) be the dual vector history until the beginning
of iteration h, and deﬁne Θh
t :=
t = 1] ≤ pmax < 1 and ˆΘh
P[θh

t |Hh]. For all tasks t and all iterations h, we assume ph
t |Hh, θh

t := E[θh
t := E[θh

t < 1] ≤ Θmax < 1.

5

This assumption states that at each iteration, the probability of a node sending a result is non-zero,
and that the quality of the returned result is, on average, better than the previous iterate. Compared
to [49, 30] which assumes θh
t = θ < 1, our assumption is signiﬁcantly less restrictive and better
models the federated setting, where nodes are unreliable and may periodically drop out.

Using Assumption 2, we derive the following theorem, which characterizes the convergence of the
federated update of MOCHA in ﬁnite horizon when the losses (cid:96)t in (1) are smooth.
Theorem 1. Assume that the losses (cid:96)t are (1/µ)-smooth. Then, under Assumptions 1 and 2, there
exists a constant s ∈ (0, 1] such that for any given convergence target (cid:15)D, choosing H such that
1
(1 − ¯Θ)s

H ≥

log

(6)

,

n
(cid:15)D

will satisfy E[D(α(H)) − D(α(cid:63))] ≤ (cid:15)D .
Here, ¯Θ := pmax + (1 − pmax)Θmax < 1. While Theorem 1 is concerned with ﬁnite horizon conver-
gence, it is possible to get asymptotic convergence results, i.e., H → ∞, with milder assumptions on
the stragglers; see Corollary 8 in the Appendix for details.

When the loss functions are non-smooth, e.g., the hinge loss for SVM models, we provide the
following sub-linear convergence for L-Lipschitz losses.
Theorem 2. If the loss functions (cid:96)t are L-Lipschitz, then there exists a constant σ, deﬁned in (24),
such that for any given (cid:15)D > 0, if we choose

(7)

H ≥ H0 +

(cid:24)

2
(1 − ¯Θ)
(cid:25)

(cid:20)

(cid:18)

max

1,

2L2σσ(cid:48)
n2(cid:15)D

(cid:19) (cid:25)
,

(cid:24)

with H0 ≥

h0+

then ¯α := 1

H−H0

(cid:80)H

16L2σσ(cid:48)
(1 − ¯Θ)n2(cid:15)D
h=H0+1 α(h) will satisfy E[D( ¯α) − D(α(cid:63))] ≤ (cid:15)D .

1
(1 − ¯Θ)

, h0 =

1 +

log

(cid:18) 2n2(D(α(cid:63)) − D(α0))
4L2σσ(cid:48)

(cid:19)(cid:21)

,

+

These theorems guarantee that MOCHA will converge in the federated setting, under mild assumptions
on stragglers and capabilities of the nodes. While these results consider convergence in terms of the
dual, we show that they hold analogously for the duality gap. We provide all proofs in Appendix C.
Remark 2. Following from the discussion in Section 3.4, our method and theory generalize the
results in [22, 31]. In the limiting case that all θh
t are identical, our results extend the results of
COCOA to the multi-task framework described in (1).
Remark 3. Note that the methods in [22, 31] have an aggregation parameter γ ∈ (0, 1]. Though we
prove our results for a general γ, we simplify the method and results here by setting γ := 1, which
has been shown to have the best performance, both theoretically and empirically [31].

5 Simulations

In this section we validate the empirical performance of MOCHA. First, we introduce a benchmarking
suite of real-world federated datasets and show that multi-task learning is well-suited to handle the
statistical challenges of the federated setting. Next, we demonstrate MOCHA’s ability to handle strag-
glers, both from statistical and systems heterogeneity. Finally, we explore the performance of MOCHA
when devices periodically drop out. Our code is available at: github.com/gingsmith/fmtl.

5.1 Federated Datasets

In our simulations, we use several real-world datasets that have been generated in federated settings.
We provide additional details in the Appendix, including information about data sizes, nt.

• Google Glass (GLEAM)4: This dataset consists of two hours of high resolution sensor data
collected from 38 participants wearing Google Glass for the purpose of activity recognition.
Following [41], we featurize the raw accelerometer, gyroscope, and magnetometer data into 180
statistical, spectral, and temporal features. We model each participant as a separate task, and
predict between eating and other activities (e.g., walking, talking, drinking).

4

http://www.skleinberg.org/data/GLEAM.tar.gz

6

• Human Activity Recognition5: Mobile phone accelerometer and gyroscope data collected from
30 individuals, performing one of six activities: {walking, walking-upstairs, walking-downstairs,
sitting, standing, lying-down}. We use the provided 561-length feature vectors of time and
frequency domain variables generated for each instance [3]. We model each individual as a
separate task and predict between sitting and the other activities.

• Vehicle Sensor6: Acoustic, seismic, and infrared sensor data collected from a distributed network
of 23 sensors, deployed with the aim of classifying vehicles driving by a segment of road [13].
Each instance is described by 50 acoustic and 50 seismic features. We model each sensor as a
separate task and predict between AAV-type and DW-type vehicles.

5.2 Multi-Task Learning for the Federated Setting

We demonstrate the beneﬁts of multi-task learning for the federated setting by comparing the error
rates of a multi-task model to that of a fully local model (i.e., learning a model for each task separately)
and a fully global model (i.e., combining the data from all tasks and learning one single model). Work
on federated learning thus far has been limited to the study of fully global models [25, 36, 26].

We use a cluster-regularized multi-task model [57], as described in Section 3.1. For each dataset from
Section 5.1, we randomly split the data into 75% training and 25% testing, and learn multi-task, local,
and global support vector machine models, selecting the best regularization parameter, λ ∈{1e-5,
1e-4, 1e-3, 1e-2, 0.1, 1, 10}, for each model using 5-fold cross-validation. We repeat this process 10
times and report the average prediction error across tasks, averaged across these 10 trials.

Table 1: Average prediction error: Means and standard errors over 10 random shufﬂes.

Model Human Activity Google Glass Vehicle Sensor

Global

2.23 (0.30)

5.34 (0.26)

13.4 (0.26)

Local

MTL

1.34 (0.21)

4.92 (0.26)

7.81 (0.13)

0.46 (0.11)

2.02 (0.15)

6.59 (0.21)

In Table 1, we see that for each dataset, multi-task learning signiﬁcantly outperforms the other
models in terms of achieving the lowest average error across tasks. The global model, as proposed
in [25, 36, 26] performs the worst, particularly for the Human Activity and Vehicle Sensor datasets.
Although the datasets are already somewhat unbalanced, we note that a global modeling approach may
beneﬁt tasks with a very small number of instances, as information can be shared across tasks. For this
reason, we additionally explore the performance of global, local, and multi-task modeling for highly
skewed data in Table 4 of the Appendix. Although the performance of the global model improves
slightly relative to local modeling in this setting, the global model still performs the worst for the
majority of the datasets, and MTL still signiﬁcantly outperforms both global and local approaches.

5.3 Straggler Avoidance

Two challenges that are prevalent in federated learning are stragglers and high communication.
Stragglers can occur when a subset of the devices take much longer than others to perform local
updates, which can be caused either by statistical or systems heterogeneity. Communication can also
exacerbate poor performance, as it can be slower than computation by many orders of magnitude in
typical cellular or wireless networks [52, 20, 48, 9, 38]. In our experiments below, we simulate the
time needed to run each method by tracking the operations and communication complexities, and
scaling the communication cost relative to computation by one, two, or three orders of magnitude,
respectively. These numbers correspond roughly to the clock rate vs. network bandwidth/latency
(see, e.g., [52]) for modern cellular and wireless networks. Details are provided in Appendix E.

5

6

https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones
http://www.ecs.umass.edu/~mduarte/Software.html

7

Figure 1: The performance of MOCHA compared to other distributed methods for the W update of (1). While
increasing communication tends to decrease the performance of the mini-batch methods, MOCHA performs well
in high communication settings. In all settings, MOCHA with varied approximation values, Θh
t , performs better
than without (i.e., naively generalizing COCOA), as it avoids stragglers from statistical heterogeneity.

Statistical Heterogeneity. We explore the effect of statistical heterogeneity on stragglers for various
methods and communication regimes (3G, LTE, WiFi). For a ﬁxed communication network, we
compare MOCHA to COCOA, which has a single θ parameter, and to mini-batch stochastic gradient
descent (Mb-SGD) and mini-batch stochastic dual coordinate ascent (Mb-SDCA), which have limited
communication ﬂexibility depending on the batch size. We tune all compared methods for best
performance, as we detail in Appendix E. In Figure 1, we see that while the performance degrades
for mini-batch methods in high communication regimes, MOCHA and COCOA are robust to high
communication. However, COCOA is signiﬁcantly affected by stragglers—because θ is ﬁxed across
nodes and rounds, difﬁcult subproblems adversely impact convergence. In contrast, MOCHA performs
well regardless of communication cost and is robust to statistical heterogeneity.

Systems Heterogeneity. MOCHA is also equipped to handle heterogeneity from changing systems
environments, such as battery power, memory, or network connection, as we show in Figure 2. In
particular, we simulate systems heterogeneity by randomly choosing the number of local iterations
for MOCHA or the mini-batch size for mini-batch methods, between 10% and 100% of the minimum
number of local data points for high variability environments, to between 90% and 100% for low
variability (see Appendix E for full details). We do not vary the performance of COCOA, as the
impact from statistical heterogeneity alone signiﬁcantly reduces performance. However, adding
systems heterogeneity would reduce performance even further, as the maximum θ value across all
nodes would only increase if additional systems challenges were introduced.

5.4 Tolerance to Dropped Nodes

Finally, we explore the effect of nodes dropping
on the performance of MOCHA. We do not draw
comparisons to other methods, as to the best of
our knowledge, no other methods for distributed
multi-task learning directly address fault toler-
ance. In MOCHA, we incorporate this setting
by allowing θh
t := 1, as explored theoretically
in Section 4. In Figure 3, we look at the per-
formance of MOCHA, either for one ﬁxed W
update, or running the entire MOCHA method, as
the probability that nodes drop at each iteration
(ph
t in Assumption 2) increases. We see that the
performance of MOCHA is robust to relatively
high values of ph
t , both during a single update of
W and in how this affects the performance of
the overall method. However, as intuition would
suggest, if one of the nodes never sends updates
(i.e., ph
1 := 1 for all h, green dotted line), the
method does not converge to the correct solution.
This provides validation for our Assumption 2.

Figure 2: MOCHA can handle variability from systems
heterogeneity.

Figure 3: The performance of MOCHA is robust to
nodes periodically dropping out (fault tolerance).

8

6 Discussion

To address the statistical and systems challenges of the burgeoning federated learning setting, we have
presented MOCHA, a novel systems-aware optimization framework for federated multi-task learning.
Our method and theory for the ﬁrst time consider issues of high communication cost, stragglers,
and fault tolerance for multi-task learning in the federated environment. While MOCHA does not
apply to non-convex deep learning models in its current form, we note that there may be natural
connections between this approach and “convexiﬁed” deep learning models [6, 34, 51, 56] in the
context of kernelized federated multi-task learning.

We thank Brendan McMahan, Chloé Kiddon, Jakub Koneˇcný, Evan Sparks, Xinghao Pan, Lisha Li,
and Hang Qi for valuable discussions and feedback.

Acknowledgements

References

[1] A. Ahmed, A. Das, and A. J. Smola. Scalable hierarchical multitask learning algorithms for conversion

optimization in display advertising. In Conference on Web Search and Data Mining, 2014.

[2] R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unlabeled

data. Journal of Machine Learning Research, 6:1817–1853, 2005.

[3] D. Anguita, A. Ghio, L. Oneto, X. Parra, and J. L. Reyes-Ortiz. A public domain dataset for human activity
recognition using smartphones. In European Symposium on Artiﬁcial Neural Networks, Computational
Intelligence and Machine Learning, 2013.

[4] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In Neural Information Processing

[5] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning,

Systems, 2007.

73(3):243–272, 2008.

[6] Ö. Aslan, X. Zhang, and D. Schuurmans. Convex deep learning via normalized kernels. In Advances in

Neural Information Processing Systems, 2014.

[7] I. M. Baytas, M. Yan, A. K. Jain, and J. Zhou. Asynchronous multi-task learning. In International

Conference on Data Mining, 2016.

[8] F. Bonomi, R. Milito, J. Zhu, and S. Addepalli. Fog computing and its role in the internet of things. In

SIGCOMM Workshop on Mobile Cloud Computing, 2012.

[9] A. Carroll and G. Heiser. An analysis of power consumption in a smartphone. In USENIX Annual Technical

Conference, 2010.

[10] R. Caruana. Multitask learning. Machine Learning, 28:41–75, 1997.
[11] J. Chen, J. Zhou, and J. Ye. Integrating low-rank and group-sparse structures for robust multi-task learning.

In Conference on Knowledge Discovery and Data Mining, 2011.

[12] A. Deshpande, C. Guestrin, S. R. Madden, J. M. Hellerstein, and W. Hong. Model-based approximate

querying in sensor networks. VLDB Journal, 14(4):417–443, 2005.

[13] M. F. Duarte and Y. H. Hu. Vehicle classiﬁcation in distributed sensor networks. Journal of Parallel and

Distributed Computing, 64(7):826–838, 2004.

[14] T. Evgeniou and M. Pontil. Regularized multi-task learning. In Conference on Knowledge Discovery and

Data Mining, 2004.

[15] P. Garcia Lopez, A. Montresor, D. Epema, A. Datta, T. Higashino, A. Iamnitchi, M. Barcellos, P. Felber,
and E. Riviere. Edge-centric computing: Vision and challenges. SIGCOMM Computer Communication
Review, 45(5):37–42, 2015.

[16] A. R. Gonçalves, F. J. Von Zuben, and A. Banerjee. Multi-task sparse structure learning with gaussian

copula models. Journal of Machine Learning Research, 17(33):1–30, 2016.

[17] J. Gorski, F. Pfeuffer, and K. Klamroth. Biconvex sets and optimization with biconvex functions: a survey

and extensions. Mathematical Methods of Operations Research, 66(3):373–407, 2007.

[18] K. Hong, D. Lillethun, U. Ramachandran, B. Ottenwälder, and B. Koldehofe. Mobile fog: A programming
model for large-scale applications on the internet of things. In SIGCOMM Workshop on Mobile Cloud
Computing, 2013.

[19] C.-J. Hsieh, M. A. Sustik, I. S. Dhillon, and P. Ravikumar. Sparse Inverse Covariance Matrix Estimation

Using Quadratic Approximation. In Neural Information Processing Systems 27, 2014.

9

[20] J. Huang, F. Qian, Y. Guo, Y. Zhou, Q. Xu, Z. M. Mao, S. Sen, and O. Spatscheck. An in-depth study of
lte: Effect of network protocol and application behavior on performance. In ACM SIGCOMM Conference,
2013.

[21] L. Jacob, J.-p. Vert, and F. R. Bach. Clustered multi-task learning: A convex formulation. In Neural

Information Processing Systems, 2009.

[22] M. Jaggi, V. Smith, J. Terhorst, S. Krishnan, T. Hofmann, and M. I. Jordan. Communication-Efﬁcient

Distributed Dual Coordinate Ascent. In Neural Information Processing Systems, 2014.

[23] X. Jin, P. Luo, F. Zhuang, J. He, and Q. He. Collaborating between local and global learning for distributed

online multiple tasks. In Conference on Information and Knowledge Management, 2015.

[24] S. Kim and E. P. Xing. Statistical estimation of correlated genome associations to a quantitative trait

[25] J. Koneˇcn`y, H. B. McMahan, and D. Ramage. Federated optimization: Distributed optimization beyond

network. PLoS Genet, 5(8):e1000587, 2009.

the datacenter. arXiv:1511.03575, 2015.

[26] J. Koneˇcn`y, H. B. McMahan, F. X. Yu, P. Richtárik, A. T. Suresh, and D. Bacon. Federated learning:

Strategies for improving communication efﬁciency. arXiv:1610.05492, 2016.

[27] T. Kuﬂik, J. Kay, and B. Kummerfeld. Challenges and solutions of ubiquitous user modeling. In Ubiquitous

display environments, pages 7–30. Springer, 2012.

[28] A. Kumar and H. Daumé. Learning task grouping and overlap in multi-task learning. In International

Conference on Machine Learning, 2012.

[29] S. L. Lauritzen. Graphical Models, volume 17. Clarendon Press, 1996.
[30] S. Liu, S. J. Pan, and Q. Ho. Distributed multi-task relationship learning. Conference on Knowledge

Discovery and Data Mining, 2017.

[31] C. Ma, V. Smith, M. Jaggi, M. I. Jordan, P. Richtárik, and M. Takáˇc. Adding vs. averaging in distributed

primal-dual optimization. In International Conference on Machine Learning, 2015.

[32] S. Madden, M. J. Franklin, J. M. Hellerstein, and W. Hong. TAG: A tiny aggregation service for ad-hoc

sensor networks. In Symposium on Operating Systems Design and Implementation, 2002.

[33] S. Madden, M. J. Franklin, J. M. Hellerstein, and W. Hong. TinyDB: An acquisitional query processing

system for sensor networks. ACM Transactions on Database Systems, 30(1):122–173, 2005.

[34] J. Mairal, P. Koniusz, Z. Harchaoui, and C. Schmid. Convolutional kernel networks. In Neural Information

Processing Systems, 2014.

[35] D. Mateos-Núñez and J. Cortés. Distributed optimization for multi-task learning via nuclear-norm

approximation. In IFAC Workshop on Distributed Estimation and Control in Networked Systems, 2015.

[36] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efﬁcient learning

of deep networks from decentralized data. In Conference on Artiﬁcial Intelligence and Statistics, 2017.
Ramage.

http://www.googblogs.com/

McMahan

and

D.

B.

[37] H.

federated-learning-collaborative-machine-learning-without-centralized-training-data/.
Google, 2017.

[38] A. P. Miettinen and J. K. Nurminen. Energy efﬁciency of mobile clients in cloud computing. In USENIX

Conference on Hot Topics in Cloud Computing, 2010.

[39] A. Pantelopoulos and N. G. Bourbakis. A survey on wearable sensor-based systems for health monitoring

and prognosis. IEEE Transactions on Systems, Man, and Cybernetics, 40(1):1–12, 2010.

[40] H. Qi, E. R. Sparks, and A. Talwalkar. Paleo: A performance model for deep neural networks.

In

International Conference on Learning Representations, 2017.

[41] S. A. Rahman, C. Merck, Y. Huang, and S. Kleinberg. Unintrusive eating recognition using google glass.

In Conference on Pervasive Computing Technologies for Healthcare, 2015.

[42] P. Rashidi and D. J. Cook. Keeping the resident in the loop: Adapting the smart home to the user. IEEE

Transactions on systems, man, and cybernetics, 39(5):949–959, 2009.

[43] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. XNOR-Net: ImageNet classiﬁcation using binary

convolutional neural networks. In European Conference on Computer Vision, 2016.

[44] S. Ravi.

https://research.googleblog.com/2017/02/on-device-machine-intelligence.

html. Google, 2017.

[45] M. Razaviyayn, M. Hong, and Z.-Q. Luo. A uniﬁed convergence analysis of block successive minimization

methods for nonsmooth optimization. SIAM Journal on Optimization, 23(2):1126–1153, 2013.

[46] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal Estimated sub-GrAdient SOlver for SVM.

International Conference on Machine Learning, June 2007.

[47] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss minimiza-

tion. Journal of Machine Learning Research, 14:567–599, 2013.

[48] D. Singelée, S. Seys, L. Batina, and I. Verbauwhede. The communication and computation cost of wireless

security. In ACM Conference on Wireless Network Security, 2011.

10

[49] V. Smith, S. Forte, C. Ma, M. Takác, M. I. Jordan, and M. Jaggi. CoCoA: A general framework for

communication-efﬁcient distributed optimization. arXiv:1611.02189, 2016.

[50] M. Takáˇc, A. Bijral, P. Richtárik, and N. Srebro. Mini-Batch Primal and Dual Methods for SVMs. In

International Conference on Machine Learning, 2013.

[51] C.-Y. Tsai, A. M. Saxe, and D. Cox. Tensor switching networks. In Neural Information Processing Systems,

2016.

[52] C. Van Berkel. Multi-core for mobile phones. In Proceedings of the Conference on Design, Automation

and Test in Europe, pages 1260–1265. European Design and Automation Association, 2009.

[53] H. Wang, A. Banerjee, C.-J. Hsieh, P. K. Ravikumar, and I. S. Dhillon. Large scale distributed sparse

precision estimation. In Neural Information Processing Systems, 2013.

[54] J. Wang, M. Kolar, and N. Srebro. Distributed multi-task learning. In Conference on Artiﬁcial Intelligence

and Statistics, 2016.

arXiv:1603.02185, 2016.

[55] J. Wang, M. Kolar, and N. Srebro. Distributed multi-task learning with shared representation.

[56] Y. Zhang, P. Liang, and M. J. Wainwright. Convexiﬁed convolutional neural networks. International

Conference on Machine Learning, 2017.

[57] Y. Zhang and D.-Y. Yeung. A convex formulation for learning task relationships in multi-task learning. In

Conference on Uncertainty in Artiﬁcial Intelligence, 2010.

[58] J. Zhou, J. Chen, and J. Ye. Clustered multi-task learning via alternating structure optimization. In Neural

Information Processing Systems, 2011.

11

A Preliminaries

Notation. We use Id×d to represent an identity matrix of size d × d. When the context allows, we
use the notation I to denote an identity matrix of an appropriate size. We also use ⊗ to denote the
Kronecker product between two matrices.
Deﬁnition 2 (Matrix norm). Given a symmetric positive deﬁnite matrix M, the norm of u with
respect to M is given by (cid:107)u(cid:107)M :=
Deﬁnition 3 (L-smooth). A convex function f is L-smooth with respect to M if

uT Mu .

√

f (u) ≤ f (v) + (cid:104)∇f (v), u − v(cid:105) +

(cid:107)u − v(cid:107)2
M

∀u, v .

L
2

If M = I then, we simply say f is L-smooth.
Deﬁnition 4 (τ -strongly convex). A function f is τ -strongly convex with respect to M if

f (u) ≥ f (v) + (cid:104)z, u − v(cid:105) +

∀u, v, z ∈ ∂f (v) ,

τ
2

(cid:107)u − v(cid:107)2
M

where ∂f (v) is the set of sub-differentials of function f at v. If M = I then, we simply say f is
τ -strongly convex.
Deﬁnition 5. The function f is called L-Lipchitz if for any x and y in its domain

(8)

(9)

(10)

|f (x) − f (y)| ≤ L(cid:107)x − y(cid:107)2 .

If a function f is L-Lipchitz then its dual will be L-bounded, i.e., for any α such that (cid:107)α(cid:107)2 > L,
then f ∗(α) = +∞.

B Multi-Task Learning

In this section, we summarize several popular multi-task learning formulations that can be written in
the form of (1) and can therefore be addressed by our framework, MOCHA. While the W update is
discussed in Section 3, we provide details here on how to solve the Ω update for these formulations.

B.1 Multi-Task Learning Formulations

MTL with cluster structure.
In MTL models that assume a cluster structure, the weight vectors
for each task, wt, are assumed to ‘close’ according to some metric to other weight vectors from tasks
in the same cluster. This idea goes back to mean-regularized MTL [14], which assumes that all the
tasks form one cluster, and that the weight vectors are close to their mean. Such a regularizer could
be formulated in the form of (1) by choosing Ω = (Im×m − 1
m 11T )2, where Im×m is the identity
matrix of size m × m and 1m represents a vector of all ones with size m. In this case, we set R to be
R(W, Ω) = λ1 tr(cid:0)WΩWT (cid:1) + λ2(cid:107)W(cid:107)2
F ,
where λ1, λ2 > 0 are parameters. Note that in this formulation, the structural dependence matrix Ω
is known a-priori. However, it is natural to assume multiple clusters exist, and to learn this clustering
structure directly from the data [58]. For such a model, the problem formulation is non-convex if a
perfect clustering structure is imposed [58, 21]. However, by performing a convex relaxation, the
following regularizer is obtained [58, 21]

(11)

R(W, Ω) = λ tr(cid:0)W(ηI + Ω)−1WT (cid:1), Ω ∈ Q =

Q | Q (cid:23) 0, tr(Q) = k, Q (cid:22) I

,

(12)

(cid:26)

(cid:27)

where λ and η are regularization parameters, k is the number of clusters, and Ω deﬁnes the clustering
structure.

MTL with probabilistic priors. Another set of MTL models that can be realized by our framework
enforce structure by putting probabilistic priors on the dependence among the columns of W. For
example, in [57] it is assumed that the weight matrix W has a prior distribution of the form:

W ∼

N (0, σ2I)

MN (0, Id×d ⊗ Ω) ,

(13)

(cid:32) m
(cid:89)

i=1

(cid:33)

12

where N (0, σ2I) denotes the normal distribution with mean 0 and covariance σ2I, and
MN (0, Id×d ⊗ Ω) denotes the matrix normal distribution with mean 0, row covariance Id×d,
and column covariance Ω. This prior generates a regularizer of the following form [57]:

R(W, Ω) = λ

(cid:18) 1
σ2 (cid:107)W(cid:107)2 + tr(cid:0)WΩ−1WT (cid:1) + d log |Ω|

(cid:19)

, λ > 0 .

Unfortunately, such a regularizer is non-convex with respect to Ω due to the concavity of log |Ω|. To
obtain a jointly convex formulation in Ω and W, the authors in [57] propose omitting log |Ω| and
controlling the complexity of Ω by adding a constraint on tr(Ω):

R(W, Ω) = λ

(cid:18) 1
σ2 (cid:107)W(cid:107)2 + tr(cid:0)WΩ−1WT (cid:1)

(cid:19)

(cid:26)

(cid:27)

, Ω ∈ Q =

Q | Q (cid:23) 0, tr(Q) = 1

.

(14)

It is worth noting that unlike the clustered MTL formulations, such as (2), the probabilistic formulation
in (14) can model both positive and negative relationships among the tasks through the covariance
matrix.

MTL with graphical models. Another way of modeling task relationships is through the precision
matrix. This is popular in graphical models literature [29] because it encodes conditional indepen-
dence among variables. In other words, if we denote the precision matrix among tasks in matrix
variate Gaussian prior with Ω, then Ωi,j = 0 if and only if tasks weights wi and wj are independent
given the rest of the task weights [16]. Therefore, assuming sparsity in the structure among the tasks
translates to sparsity in matrix Ω. As a result, we can formulate a sparsity-promoting regularizer by:

R(W, Ω) = λ

(cid:18) 1
σ2 (cid:107)W(cid:107)2 + tr(cid:0)WΩWT (cid:1) − d log |Ω|

(cid:19)

+ λ1(cid:107)W(cid:107)1 + λ2(cid:107)Ω(cid:107)1 ,

(15)

where λ1, λ2 ≥ 0 control the sparsity of W and Ω respectively [16]. It is worth noting that although
this problem is jointly non-convex in W and Ω, it is bi-convex.

B.2 Strong Convexity of MTL Regularizers

Recall that in Assumption 1, we presumed that the vectorized formulation of the MTL regularizer is
strongly convex with respect to a matrix M−1. In this subsection we discuss the choice of matrix M
for the widely-used MTL formulations introduced in Section B.1.

Using the notation from Remark 1 for the clustered MTL formulation (11), it is easy to see that
¯R(w, ¯Ω) = λ1wT ¯Ωw + λ2(cid:107)w(cid:107)2
2, where ¯Ω := Ω ⊗ Id×d. As a result, it is clear that ¯R(w, ¯Ω) is
1-strongly convex with respect to M−1 = λ1 ¯Ω + λ2Imd×md.
Using a similar reasoning, it is easy to see that the matrix M can be chosen as λ−1(ηI + ¯Ω),
λ−1( 1

σ2 I + ¯Ω)−1 for (12), (14) and (15) respectively.

σ2 I + ¯Ω−1)−1 and λ−1( 1

B.3 Optimizing Ω in MTL Formulations

In this section, we brieﬂy cover approaches to update Ω in the MTL formulations introduced in
Section B.1. First, it is clear that (2) does not require any updates to Ω, as it is assumed to be ﬁxed.
In (12), it can be shown [58, 21] that the optimal solution for Ω has the same column space as the
rows of W. Therefore, the problem boils down to solving a simple convex optimization problem
over the eigenvalues of Ω; see [58, 21] for details. Although outside the scope of this paper, we
note that the bottleneck of this approach to ﬁnding Ω is computing the SVD of W, which can be
a challenging problem when m is large. In the probabilistic model of (14), the Ω update is given
in [57] by (WT W) 1
2 , which requires computing the eigenvalue decomposition of WT W. For
the graphical model formulation, the problem of solving for Ω is called sparse precision matrix
estimation or graphical lasso [16]. This is a well-studied problem, and many scalable algorithms have
been proposed to solve it [53, 16, 19].

B.3.1 Reducing the Size of Ω by Sharing Tasks

One interesting aspect of MOCHA is that the method can be easily modiﬁed to accommodate the
sharing of tasks among the nodes without any change to the local solvers. This property helps the
central node to reduce the size of Ω and the complexity of its update with minimal changes to the
whole system. The following remark highlights this capability.

13

Remark 4. MOCHA can be modiﬁed to solve problems when there are tasks that are shared among
nodes. In this case, each node still solves a data local sub-problem based on its own data for the task,
but the central node needs to do an additional aggregation step to add the results for all the nodes
that share the data of each task. This reduces the size of matrix Ω and simpliﬁes its update.

C Convergence Analysis

In the rest of this section we use the superscript (h) or h to denote the corresponding
Notation.
variable at iteration (h) of the federated update in MOCHA. When context allows, we drop the
superscript to simplify notation.

In order to provide a general convergence analysis, similar to the ones provided in [22, 31, 49], we
assume an aggregation parameter γ ∈ (0, 1] in this section. With such an aggregation parameter, the
updates in each federated iteration would be αt ← αt + γ∆αt and vt ← vt + γ∆vt. For a more
detailed discussion on the role of aggregation parameter, see Appendix D.1. Note that in Algorithm 1,
MOCHA is presented assuming γ = 1 for simplicity.

Before proving our convergence guarantees, we provide several useful deﬁnitions and key lemmas.
Deﬁnition 6. For each task t, deﬁne

σt := max
α∈Rnt

(cid:107)Xtα(cid:107)2
(cid:107)α(cid:107)2

Mt

and σmax := max
t∈[m]

σt.

Deﬁnition 7. For any α, deﬁne the duality gap as

where P(W) := (cid:80)m
t=1

(cid:80)nt

i=1 (cid:96)t(wT

t xi

t, yi

t) + R(W, Ω) as in (1).

G(α) := D(α) − (−P(W(α))),

(16)

(17)

The following lemma uses Assumption 2 to bound the average performance of θh
providing global convergence guarantees for MOCHA.
Lemma 3. Under Assumption 2, Θh

t ≤ ¯Θ = pmax + (1 − pmax)Θmax < 1.

t , which is crucial in

Θh

Proof. Recalling the deﬁnitions ph
t = E[θh
= P[θh
= ph

t |Hh]
t = 1] · E[θh
t · 1 + (1 − ph

t |θh
t ) · ˆΘh

t ≤ ¯Θ < 1,

t := P[θh

t = 1] and ˆΘh

t := E[θh

t |θh

t < 1, Hh], we have

t = 1, Hh] + (1 − P[θh

t < 1]) · E[θh

t |θh

t < 1, Hh]

where the last inequality is due to Assumption 2, and the fact that ˆΘh

t < 1 by deﬁnition.

The next key lemma bounds the dual objective of an iterate based on the dual objective of the previous
iterate and the objectives of local subproblems.
Lemma 4. For any α, ∆α ∈ Rn and γ ∈ (0, 1] if σ(cid:48) satisﬁes (28), then

D (α + γ∆α) ≤ (1 − γ)D(α) + γ

Gσ(cid:48)
t (∆αt; v, αt) .

(18)

m
(cid:88)

t=1

Proof. The proof of this lemma is similar to [49, Lemma 1] and follows from the deﬁnition of local
sub-problems, smoothness of R∗ and the choice of σ(cid:48) in (28).

Recall that if the functions (cid:96)t are (1/µ)-smooth, their conjugates (cid:96)∗
lemma below provides a bound on the amount of improvement in dual objective in each iteration.
Lemma 5. If the functions (cid:96)∗

t are µ-strongly convex for some µ ≥ 0. Then, for any s ∈ [0, 1].

t will be µ-strongly convex. The

E[D(α(h)) − D(α(h+1))|Hh] ≥ γ

(1 − ¯Θ)

sGt(α(h)) −

(19)

(cid:18)

(cid:19)

Jt

,

σ(cid:48)s2
2

m
(cid:88)

t=1

14

where

Gt(α) :=

t (−αi

t) + (cid:96)t(wt(α)(cid:62)xi

t, yi

t) + αi

twt(α)(cid:62)xi
t

(cid:3) ,

nt(cid:88)

i=1

(cid:2)(cid:96)∗

Jt := − µ(1−s)

σ(cid:48)s (cid:107)(ut − α(h)

t

)(cid:107)2 + (cid:107)Xt(ut − α(h)

)(cid:107)2

t

Mt

,

for ut ∈ Rnt with

t ∈ ∂(cid:96)t(wt(α)(cid:62)xi
ui

t, yi

t) .

(20)

(21)

(22)

Proof. Applying Lemma 4 and recalling D(α) = (cid:80)m
t (0; v, αt), we can ﬁrst bound the
improvement for each task separately. Following a similar approach as in the proof of [49, Lemma 7]
we can obtain the bound (19) which bounds the improvement from α(h) to α(h+1).

t=1 Gσ(cid:48)

The following lemma relates the improvement of the dual objective in one iteration to the duality gap
for the smooth loss functions (cid:96)t.
Lemma 6. If the loss functions (cid:96)t are (1/µ)-smooth, then there exists a proper constants s ∈ (0, 1] ,
such that for any γ ∈ (0, 1] at any iteration h

(cid:104)

E

D(α(h)) − D(α(h+1))|Hh

≥ sγ(1 − ¯Θ)G(α(h)),

(cid:105)

(23)

where G(α(h)) is the duality gap of α(h) which is deﬁned in (17).

Proof. Recall the deﬁnition of σmax in (16). Now, if we carefully choose s = µ/(µ + σmaxσ(cid:48)), it
is easy to show that Jt ≤ 0 in (19); see [49, Theorem 11] for details. The ﬁnal result follows as a
consequence of Lemma 5.

Note that Lemma 5 holds even if the functions are non-smooth, i.e. µ = 0. However, we cannot infer
sufﬁcient decrease of Lemma 6 from Lemma 5 when µ = 0. Therefore, we need additional tools
when the losses are L-Liptchitz. The ﬁrst is the following lemma, which bounds the J term in (19).
Lemma 7. Assume that the loss functions (cid:96)t are L-Lipschitz. Denote J := (cid:80)m
t=1 Jt, where Jt is
deﬁned in (21), then

J ≤ 4L2

σtnt := 4L2σ,

(24)

m
(cid:88)

t=1

where σt is deﬁned in (16).

Proof. The proof is similar to [31, Lemma 6] and using the deﬁnitions of σ and σt and the fact that
the losses are L-Lipchitz.

C.1 Convergence Analysis for Smooth Losses

C.1.1 Proof of Theorem 1

Let us rewrite (23) from Lemma 6 as

E[D(α(h)) − D(α(h+1))|Hh] = D(α(h)) − D(α(cid:63)) + E[D(α(cid:63)) − D(α(h+1))|Hh]
≥ sγ(1 − ¯Θ)G(α(h))
≥ sγ(1 − ¯Θ)

D(α(h)) − D(α(cid:63))

(cid:17)

(cid:16)

,

where the last inequality is due to weak duality, i.e. G(α(h)) ≥ D(α(h)) − D(α(cid:63)). Re-arranging the
terms in the above inequality, we can easily get

E[D(α(h+1)) − D(α(cid:63))|Hh] ≤ (cid:0)1 − sγ(1 − ¯Θ)(cid:1) (cid:16)

D(α(h)) − D(α(cid:63))

(cid:17)

(25)

15

Recursively applying this inequality and taking expectations from both sides, we arrive at

E[D(α(h+1)) − D(α(cid:63))] ≤ (cid:0)1 − sγ(1 − ¯Θ)(cid:1)h+1 (cid:16)

D(α(0)) − D(α(cid:63))

(cid:17)

.

(26)

Now we can use a simple bound on the initial duality gap [49, Lemma 10], which states that
D(α(0)) − D(α(cid:63)) ≤ n, to get the ﬁnal result. It is worth noting that we can translate the bound on
the dual distance to optimality to the bound on the duality gap using the following inequalities
sγ(1 − ¯Θ) E[G(α(H))] ≤ E[D(α(H)) − D(α(H+1))] ≤ E[D(α(H)) − D(α(cid:63))] ≤ (cid:15)D,

(27)
where the ﬁrst inequality is due to (23), the second inequality is due to the optimality of α(cid:63), and the
last inequality is the bound we just proved for the dual distance to optimality.

C.1.2 Asymptotic Convergence

In the case of smooth loss functions, it is possible to get asymptotic convergence results under milder
assumptions. The following corollary is an extension of Theorem 1.
Corollary 8. If the loss functions (cid:96)t are µ-smooth, then under Assumption 1, E[D(α(H))−D(α(cid:63))] →
0 as H → ∞ if either of the following conditions hold

• lim suph→∞ ph
• For any task t, (cid:0)1 − ph

t < 1 and lim suph→∞
(cid:17)
(cid:1) ×

1 − ˆΘh
t

(cid:16)

t

ˆΘh

t < 1.

equal to 1.

= ω( 1

h ). Note that in this case limh→∞ ph

t can be

Proof. The proof is similar to the proof of Theorem 1. We can use the same steps to get a sufﬁcient
decrease inequality like the one in (25), with ¯Θ replaced with ¯Θh := maxt Θh
t .
E[D(α(h+1)) − D(α(cid:63))|Hh] ≤ (cid:0)1 − sγ(1 − ¯Θh)(cid:1) (cid:16)

D(α(h)) − D(α(cid:63))

(cid:17)

The rest of the argument follows by applying this inequality recursively and using the assumptions in
the corollary.

C.2 Convergence Analysis for Lipschitz Losses: Proof for Theorem 2

Proof. For L-Lipschitz loss functions, the proof follows the same line of reasoning as the proof of
Theorem 8 in [31] and therefore we do not cover it in detail. Unlike the case with smooth losses,
it is not possible to bound the decrease in dual objective by (23). However, we can use Lemma
5 with µ = 0. The next step is to bound J = (cid:80)m
t=1 Jt in (19), which can be done via Lemma 7.
Finally, we apply the inequalities recursively, choose s carefully, and bound the terms in the ﬁnal
inequality. We refer the reader to the proof of Theorem 8 in [31] for more details. It is worth noting
that similar to Theorem 1, we can similarly get bounds on the expected duality gap, instead of the
dual objective.

D Choosing σ(cid:48)

In order to guarantee the convergence of the federated update of MOCHA, the parameter σ(cid:48) must
satisfy:

σ(cid:48)

m
(cid:88)

t=1

(cid:107)Xtαt(cid:107)2

Mt

≥ γ(cid:107)Xα(cid:107)2

M ∀α ∈ Rn ,

(28)

where γ ∈ (0, 1] is the aggregation parameter for MOCHA Algorithm. Note that in Algorithm 1 we
have assumed that γ = 1. Based on Remark 1, it can be seen that the matrix M in Assumption 1 can
be chosen of the form M = ¯M ⊗ Id×d, where ¯M is a positive deﬁnite matrix of size m × m. For
such a matrix, the following lemma shows how to choose σ(cid:48).
Lemma 9. For any positive deﬁnite matrix M = ¯M ⊗ Id×d,
| ¯Mtt(cid:48)|
¯Mtt

σ(cid:48) := γ max

m
(cid:88)

(29)

t

satisﬁes the inequality (28).

t(cid:48)=1

16

Proof. First of all it is worth noting that for any t, Mt = ¯Mt ⊗ Id×d. For any α ∈ Rn

γ(cid:107)Xα(cid:107)2

M = γ

¯Mtt(cid:48)(cid:104)Xtαt, Xt(cid:48)αt(cid:48)(cid:105)

(cid:88)

t,t(cid:48)

(cid:88)

t,t(cid:48)

1
2

(cid:32)

≤ γ

| ¯Mtt(cid:48)|

(cid:107)Xtαt(cid:107)2

Mt

+

(cid:107)Xt(cid:48)αt(cid:48)(cid:107)2

Mt(cid:48)

1
¯Mt(cid:48)t(cid:48)

(cid:19)

(cid:18) 1
¯Mtt
(cid:33)

(cid:88)

(cid:88)

= γ

t

t(cid:48)

| ¯Mtt(cid:48)|
¯Mtt

≤ σ(cid:48) (cid:88)

(cid:107)Xtαt(cid:107)2

,

Mt

t

(cid:107)Xtαt(cid:107)2

Mt

where the ﬁrst inequality is due to Cauchy-Schwartz and the second inequality is due to deﬁnition of
σ(cid:48).

Remark 5. Based on the proof of Lemma 9, it is easy to see that we can choose σ(cid:48) differently across
the tasks in our algorithm to allow tasks that are more loosely correlated with other tasks to update
more aggressively. To be more speciﬁc, if we choose σ(cid:48)
, then it it is possible to show
that γ(cid:107)Xα(cid:107)2
for any α, and the rest of the convergence proofs will follow.

t = γ (cid:80)

| ¯Mtt(cid:48) |
¯Mtt

t(cid:48)

M ≤ (cid:80)m

t=1 σ(cid:48)

t(cid:107)Xtαt(cid:107)2

Mt

D.1 The Role of Aggregation Parameter γ

The following remark highlights the role of aggregation parameter γ.
Remark 6. Note that the when γ < 1 the chosen σ(cid:48) in (28) would be smaller compared to the case
where γ = 1. This means that the local subproblems would be solved with less restrictive regularizer.
Therefore, the resulting ∆α would be more aggressive. As a result, we need to do a more conservative
update α + γ∆α in order to guarantee the convergence.

Although aggregation parameter γ is proposed to capture this trade off between aggressive subprob-
lems and conservative updates, in most practical scenarios γ = 1 has the best empirical performance.

E Simulation Details

E.1 Datasets

In this section, we provide additional details and results of our empirical study.

In Table 2, we provide details on the number of tasks (m), feature size (d), and per-task data size
(nt) for each federated dataset described in Section 5. The standard deviation nσ is a measure data
skew, and calculates the deviation in the sizes of training data points for each task, nt. All datasets
are publicly available.

Table 2: Federated Datasets for Empirical Study.

Dataset

Tasks (m)

Features (d) Min nt Max nt

Human Activity

Google Glass

Vehicle Sensor

30

38

23

561

180

100

210

524

872

306

581

1,933

Std. Deviation nσ
26.75

11.07

267.47

E.2 Multi-Task Learning with Highly Skewed Data

To generate highly skewed data, we sample from the original training datasets so that the task dataset
sizes differ by at least two orders of magnitude. The sizes of these highly skewed datasets are shown
in Table 3. When looking at the performance of local, global, and multi-task models for these datasets

17

Table 3: Skewed Datasets for Empirical Study.

Dataset

Tasks (m)

Features (d) Min nt Max nt

Std. Deviation σ

HA-Skew

GG-Skew

VS-Skew

30

38

23

561

180

100

3

6

19

306

581

1,933

84.41

111.79

486.08

(Table 4), the global model performs slightly better in this setting (particularly for the Human Activity
dataset). However, multi-task learning still signiﬁcantly outperforms all models.

Table 4: Average prediction error for skewed data: means and standard errors over 10 random shufﬂes.

Model HA-Skew
2.41 (0.30)
Global

GG-Skew
5.38 (0.26)

VS-Skew
13.58 (0.23)

Local

3.87 (0.37)

4.96 (0.20)

8.15 (0.19)

MTL

1.93 (0.44)

3.28 (0.15)

6.91 (0.21)

In this section, we provide details on our experimental setup and compared methods.

E.3 Implementation Details

Methods.

• Mb-SGD. Mini-batch stochastic gradient descent is a standard, widely used method for parallel and
distributed optimization. See, e.g., a discussion of this method for the SVM models of interest [46].
We tune both the mini-batch size and step size for best performance using grid search.

• Mb-SDCA. Mini-batch SDCA aims to improve mini-batch SGD by employing coordinate ascent
in the dual, which has encouraging theoretical and practical backings [47, 50]. For all experiments,
we scale the updates for mini-batch stochastic dual coordinate ascent at each round by β
b for
mini-batch size b and β ∈ [1, b], and tune both parameters with grid search.

• COCOA. We generalize COCOA [22, 31] to solve (1), and tune θ, the ﬁxed approximation
parameter, between [0, 1) via grid search. For both COCOA, and MOCHA, we use coordinate
ascent as a local solver for the dual subproblems (4).

• MOCHA. The only parameter necessary to tune for MOCHA is the level of approximation quality
θh
t , which can be directly tuned via Hi, the number of local iterations of the iterative method run
locally. In Section 4, our theory relates this parameter to global convergence, and we discuss the
practical effects of this parameter in Section 3.4.

Computation and Communication Complexities. We provide a brief summary of the above
methods from the point of view of computation, communication, and memory complexities. MOCHA
is superior in terms of its computation complexity compared to other distributed optimization methods,
as MOCHA allows for ﬂexibility in its update of W. At one extreme, the update can be based on a
single data point per iteration in parallel, similar to parallel SGD. At the other extreme, MOCHA
can completely solve the subproblems on each machine, similar to methods such as ADMM. This
ﬂexibility of computation yields direct beneﬁts in terms of communication complexity, as performing
additional local computation will result in fewer communication steps. Note that all methods,
including MOCHA, communicate the same size vector at each iteration, and so the main difference is
in how many communication rounds are necessary for convergence. In terms of memory, MOCHA
must maintain the task matrix, Ω, on the master server. While this overhead is greater than most non-
MTL (global or local) approaches, the task matrix is typically low-rank by design and the overhead is
thus manageable. We discuss methods for computing Ω in further detail in Section B.3.

18

Estimated Time. To estimate the time to run methods in the federated setting, we carefully count
the ﬂoating-point operations (FLOPs) performed in each local iteration for each method, as well as the
size and frequency of communication. We convert these counts to estimated time (in milliseconds),
using known clock rate and bandwidth/latency numbers for mobile phones in 3G, LTE, and wireless
networks [52, 20, 48, 9, 38]. In particular, we use the following standard model for the cost of one
round, h, of local computation / communication on a node t:

Time(h, t) :=

+ Comm(h, t)

(30)

FLOPs(h, t)
Clock Rate(t)

Note that the communication cost Comm(h, t) includes both bandwidth and latency measures. De-
tailed models of this type have been used to closely match the performance of real-world systems [40].

Statistical Heterogeneity. To account for statistical heterogeneity, MOCHA and the mini-batch
methods (Mb-SGD and Mb-SDCA) can adjust the number of local iterations or batch size, respectively,
to account for difﬁcult local problems or high data skew. However, because COCOA uses a ﬁxed
accuracy parameter θ across both the tasks and rounds, changes in the subproblem difﬁculty and
data skew can make the computation on some nodes much slower than on others. For COCOA, we
compute θ via the duality gap, and carefully tune this parameter between [0, 1) for best performance.
Despite this, the number of local iterations needed for θ varies signiﬁcantly across nodes, and as the
method runs, the iterations tend to increase as the subproblems become more difﬁcult.

Systems Heterogeneity. Beyond statistical heterogeneity, there can be variability in the systems
themselves that cause changes in performance. For example, low battery levels, poor network
connections, or low memory may reduce the ability a solver has on a local node to compute updates.
As discussed in Section 3.4, MOCHA assumes that the central node sets some global clock cycle, and
the t-th worker determines the amount of feasible local computation given this clock cycle along with
its systems constraints. This speciﬁed amount of local computation corresponds to some implicit
value of θh
t based on the underlying systems and statistical challenges for the t-th node.
To model this setup in our simulations, it sufﬁces to ﬁx a global clock sycle and then randomly
assign various amounts of local computation to each local node at each iteration. Speciﬁcally, in
our simulations we charge all nodes the same ﬁxed computation cost at each iteration over an LTE
network, but force some nodes to perform less updates given their current systems constraints. At
each round, we assign the number of updates for node t between [0.1nmin, nmin] for high variability
environments, and between [0.9nmin, nmin] for low variability environments, where nmin := mint nt
is the minimum number of local data points across tasks.

For the mini-batch methods, we vary the mini-batch size in a similar fashion. However, we do not
follow this same process for COCOA, as this would require making the θ parameter worse than what
was optimally tuned given statistical heterogeneity. Hence, in these simulations we do not introduce
any additional variability (and thus present overly optimistic results for COCOA). In spite of this, we
see that in both low and high variability settings, MOCHA signiﬁcantly outperforms all other methods
and is robust to systems-related heterogeneity.

Fault Tolerance. Finally, we demonstrate that MOCHA can handle nodes periodically dropping
out, which is also supported in our convergence results in Section 4. We perform this simulation
using the notation deﬁned in Assumption 2, i.e., that each node t temporarily drops on iteration h
with probability ph
t . In our simulations, we modify this probability directly and show that MOCHA is
robust to fault tolerance in Figure 3. However, note that this robustness is not merely due to statistical
redundancy: If we are to drop out a node entirely (as shown in the green dotted line), MOCHA will
not converge to the correct solution. This provides insight into our Assumption 2, which requires that
the probability that a node drops at each round cannot be exactly equal to one.

19

8
1
0
2
 
b
e
F
 
7
2
 
 
]

G
L
.
s
c
[
 
 
2
v
7
6
4
0
1
.
5
0
7
1
:
v
i
X
r
a

Federated Multi-Task Learning

Virginia Smith
Stanford
smithv@stanford.edu

Chao-Kai Chiang∗
USC
chaokaic@usc.edu

Maziar Sanjabi∗
USC
maziarsanjabi@gmail.com

Ameet Talwalkar
CMU
talwalkar@cmu.edu

Abstract

Federated learning poses new statistical and systems challenges in training machine
learning models over distributed networks of devices. In this work, we show that
multi-task learning is naturally suited to handle the statistical challenges of this
setting, and propose a novel systems-aware optimization method, MOCHA, that is
robust to practical systems issues. Our method and theory for the ﬁrst time consider
issues of high communication cost, stragglers, and fault tolerance for distributed
multi-task learning. The resulting method achieves signiﬁcant speedups compared
to alternatives in the federated setting, as we demonstrate through simulations on
real-world federated datasets.

1

Introduction

Mobile phones, wearable devices, and smart homes are just a few of the modern distributed networks
generating massive amounts of data each day. Due to the growing storage and computational
power of devices in these networks, it is increasingly attractive to store data locally and push more
network computation to the edge. The nascent ﬁeld of federated learning explores training statistical
models directly on devices [37]. Examples of potential applications include: learning sentiment,
semantic location, or activities of mobile phone users; predicting health events like low blood sugar
or heart attack risk from wearable devices; or detecting burglaries within smart homes [3, 39, 42].
Following [25, 36, 26], we summarize the unique challenges of federated learning below.

1. Statistical Challenges: The aim in federated learning is to ﬁt a model to data, {X1, . . . , Xm},
generated by m distributed nodes. Each node, t ∈ [m], collects data in a non-IID manner across the
network, with data on each node being generated by a distinct distribution Xt ∼ Pt. The number
of data points on each node, nt, may also vary signiﬁcantly, and there may be an underlying
structure present that captures the relationship amongst nodes and their associated distributions.
2. Systems Challenges: There are typically a large number of nodes, m, in the network, and
communication is often a signiﬁcant bottleneck. Additionally, the storage, computational, and
communication capacities of each node may differ due to variability in hardware (CPU, memory),
network connection (3G, 4G, WiFi), and power (battery level). These systems challenges, com-
pounded with unbalanced data and statistical heterogeneity, make issues such as stragglers and
fault tolerance signiﬁcantly more prevalent than in typical data center environments.

In this work, we propose a modeling approach that differs signiﬁcantly from prior work on federated
learning, where the aim thus far has been to train a single global model across the network [25, 36, 26].
Instead, we address statistical challenges in the federated setting by learning separate models for each
node, {w1, . . . , wm}. This can be naturally captured through a multi-task learning (MTL) framework,
where the goal is to consider ﬁtting separate but related models simultaneously [14, 2, 57, 28].
Unfortunately, current multi-task learning methods are not suited to handle the systems challenges
that arise in federated learning, including high communication cost, stragglers, and fault tolerance.
Addressing these challenges is therefore a key component of our work.

∗Authors contributed equally.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

1.1 Contributions

We make the following contributions. First, we show that MTL is a natural choice to handle statistical
challenges in the federated setting. Second, we develop a novel method, MOCHA, to solve a general
MTL problem. Our method generalizes the distributed optimization method COCOA [22, 31] in
order to address systems challenges associated with network size and node heterogeneity. Third, we
provide convergence guarantees for MOCHA that carefully consider these unique systems challenges
and provide insight into practical performance. Finally, we demonstrate the superior empirical
performance of MOCHA with a new benchmarking suite of federated datasets.

2 Related Work

Learning Beyond the Data Center. Computing SQL-like queries across distributed, low-powered
nodes is a decades-long area of research that has been explored under the purview of query processing
in sensor networks, computing at the edge, and fog computing [32, 12, 33, 8, 18, 15]. Recent works
have also considered training machine learning models centrally but serving and storing them locally,
e.g., this is a common approach in mobile user modeling and personalization [27, 43, 44]. However,
as the computational power of the nodes within distributed networks grows, it is possible to do even
more work locally, which has led to recent interest in federated learning.2 In contrast to our proposed
approach, existing federated learning approaches [25, 36, 26, 37] aim to learn a single global model
across the data.3 This limits their ability to deal with non-IID data and structure amongst the nodes.
These works also come without convergence guarantees, and have not addressed practical issues of
stragglers or fault tolerance, which are important characteristics of the federated setting. The work
proposed here is, to the best of our knowledge, the ﬁrst federated learning framework to consider
these challenges, theoretically and in practice.

Multi-Task Learning.
In multi-task learning, the goal is to learn models for multiple related tasks
simultaneously. While the MTL literature is extensive, most MTL modeling approaches can be
broadly categorized into two groups based on how they capture relationships amongst tasks. The ﬁrst
(e.g., [14, 4, 11, 24]) assumes that a clustered, sparse, or low-rank structure between the tasks is known
a priori. A second group instead assumes that the task relationships are not known beforehand and
can be learned directly from the data (e.g., [21, 57, 16]). In this work, we focus our attention on this
latter group, as task relationships may not be known beforehand in real-world settings. In comparison
to learning a single global model, these MTL approaches can directly capture relationships amongst
non-IID and unbalanced data, which makes them particularly well-suited for the statistical challenges
of federated learning. We demonstrate this empirically on real-world federated datasets in Section 5.
However, although MTL is a natural modeling choice to address the statistical challenges of federated
learning, currently proposed methods for distributed MTL (discussed below) do not adequately
address the systems challenges associated with federated learning.

Distributed Multi-Task Learning. Distributed multi-task learning is a relatively new area of
research, in which the aim is to solve an MTL problem when data for each task is distributed over a
network. While several recent works [1, 35, 54, 55] have considered the issue of distributed MTL
training, the proposed methods do not allow for ﬂexibility of communication versus computation.
As a result, they are unable to efﬁciently handle concerns of fault tolerance and stragglers, the latter
of which stems from both data and system heterogeneity. The works of [23] and [7] allow for
asynchronous updates to help mitigate stragglers, but do not address fault tolerance. Moreover, [23]
provides no convergence guarantees, and the convergence of [7] relies on a bounded delay assumption
that is impractical for the federated setting, where delays may be signiﬁcant and devices may drop
out completely. Finally, [30] proposes a method and setup leveraging the distributed framework
COCOA [22, 31], which we show in Section 4 to be a special case of the more general approach in
this work. However, the authors in [30] do not explore the federated setting, and their assumption
that the same amount of work is done locally on each node is prohibitive in federated settings, where
unbalance is common due to data and system variability.

2The term on-device learning has been used to describe both the task of model training and of model serving.

Due to the ambiguity of this phrase, we exclusively use the term federated learning.

3While not the focus of our work, we note privacy is an important concern in the federated setting, and that
the privacy beneﬁts associated with global federated learning (as discussed in [36]) also apply to our approach.

2

3 Federated Multi-Task Learning

In federated learning, the aim is to learn a model over data that resides on, and has been generated by,
m distributed nodes. As a running example, consider learning the activities of mobile phone users in
a cell network based on their individual sensor, text, or image data. Each node (phone), t ∈ [m], may
generate data via a distinct distribution, and so it is natural to ﬁt separate models, {w1, . . . , wm},
to the distributed data—one for each local dataset. However, structure between models frequently
exists (e.g., people may behave similarly when using their phones), and modeling these relationships
via multi-task learning is a natural strategy to improve performance and boost the effective sample
size for each node [10, 2, 5]. In this section, we suggest a general MTL framework for the federated
setting, and propose a novel method, MOCHA, to handle the systems challenges of federated MTL.

3.1 General Multi-Task Learning Setup

Given data Xt ∈ Rd×nt from m nodes, multi-task learning ﬁts separate weight vectors wt ∈ Rd to
the data for each task (node) through arbitrary convex loss functions (cid:96)t (e.g., the hinge loss for SVM
models). Many MTL problems can be captured via the following general formulation:

(cid:40) m
(cid:88)

nt(cid:88)

t=1

i=1

min
W,Ω

(cid:96)t(wT

t xi

t, yi

t) + R(W, Ω)

,

(cid:41)

(1)

where W := [w1, . . . , wm] ∈ Rd×m is a matrix whose t-th column is the weight vector for the
t-th task. The matrix Ω ∈ Rm×m models relationships amongst tasks, and is either known a
priori or estimated while simultaneously learning task models. MTL problems differ based on their
assumptions on R, which takes Ω as input and promotes some suitable structure amongst the tasks.

As an example, several popular MTL approaches assume that tasks form clusters based on whether or
not they are related [14, 21, 57, 58]. This can be expressed via the following bi-convex formulation:
R(W, Ω) = λ1 tr(cid:0)WΩWT (cid:1) + λ2(cid:107)W(cid:107)2
F ,
(2)
with constants λ1, λ2 > 0, and where the second term performs L2 regularization on each local
model. We use a similar formulation (14) in our experiments in Section 5, and provide details on
other common classes of MTL models that can be formulated via (1) in Appendix B.

3.2 MOCHA: A Framework for Federated Multi-Task Learning

In the federated setting, the aim is to train statistical models directly on the edge, and thus we
solve (1) while assuming that the data {X1, . . . , Xm} is distributed across m nodes or devices.
Before proposing our federated method for solving (1), we make the following observations:

• Observation 1: In general, (1) is not jointly convex in W and Ω, and even in the cases where (1)

is convex, solving for W and Ω simultaneously can be difﬁcult [5].

• Observation 2: When ﬁxing Ω, updating W depends on both the data X, which is distributed

across the nodes, and the structure Ω, which is known centrally.

• Observation 3: When ﬁxing W, optimizing for Ω only depends on W and not on the data X.

Based on these observations, it is natural to propose an alternating optimization approach to solve
problem (1), in which at each iteration we ﬁx either W or Ω and optimize over the other, alternating
until convergence is reached. Note that solving for Ω is not dependent on the data and therefore can
be computed centrally; as such, we defer to prior work for this step [58, 21, 57, 16]. In Appendix B,
we discuss updates to Ω for several common MTL models.

In this work, we focus on developing an efﬁcient distributed optimization method for the W step. In
traditional data center environments, the task of distributed training is a well-studied problem, and
various communication-efﬁcient frameworks have been recently proposed, including the state-of-the-
art primal-dual COCOA framework [22, 31]. Although COCOA can be extended directly to update
W in a distributed fashion across the nodes, it cannot handle the unique systems challenges of the
federated environment, such as stragglers and fault tolerance, as discussed in Section 3.4. To this
end, we extend COCOA and propose a new method, MOCHA, for federated multi-task learning. Our
method is given in Algorithm 1 and described in detail in Sections 3.3 and 3.4.

3

Set subproblem parameter σ(cid:48) and number of federated iterations, Hi
for iterations h = 0, 1, · · · , Hi do

Algorithm 1 MOCHA: Federated Multi-Task Learning Framework
1: Input: Data Xt from t = 1, . . . , m tasks, stored on one of m nodes, and initial matrix Ω0
2: Starting point α(0) := 0 ∈ Rn, v(0) := 0 ∈ Rb
3: for iterations i = 0, 1, . . . do
4:
5:
6:
7:
8:
9:
10:
Update Ω centrally based on w(α) for latest α
11:
12: Central node computes w = w(α) based on the lastest α
13: return: W := [w1, . . . , wm]

call local solver, returning θh
update local variables αt ← αt + ∆αt
return updates ∆vt := Xt∆αt

for tasks t ∈ {1, 2, . . . , m} in parallel over m nodes do

reduce: vt ← vt + ∆vt

t -approximate solution ∆αt of the local subproblem (4)

3.3 Federated Update of W

To update W in the federated setting, we begin by extending works on distributed primal-dual
optimization [22, 31, 30] to apply to the generalized multi-task framework (1). This involves deriving
the appropriate dual formulation, subproblems, and problem parameters, as we detail below.

Dual problem. Considering the dual formulation of (1) will allow us to better separate the global
problem into distributed subproblems for federated computation across the nodes. Let n := (cid:80)m
t=1 nt
and X := Diag(X1, · · · , Xm) ∈ Rmd×n. With Ω ﬁxed, the dual of problem (1), deﬁned with
respect to dual variables α ∈ Rn, is given by:
m
(cid:88)

nt(cid:88)

(cid:40)

(cid:41)

min
α

D(α) :=

t (−αi
(cid:96)∗

t) + R∗(Xα)

,

i=1
t and R∗ are the conjugate dual functions of (cid:96)t and R, respectively, and αi

where (cid:96)∗
t is the dual
variable for the data point (xi
t). Note that R∗ depends on Ω, but for the sake of simplicity, we
have removed this in our notation. To derive distributed subproblems from this global dual, we make
an assumption described below on the regularizer R.

t, yi

t=1

(3)

Assumption 1. Given Ω, we assume that there exists a symmetric positive deﬁnite matrix M ∈
Rmd×md, depending on Ω, for which the function R is strongly convex with respect to M−1. Note
that this corresponds to assuming that R∗ will be smooth with respect to matrix M.
Remark 1. We can reformulate the MTL regularizer in the form of ¯R(w, ¯Ω) = R(W, Ω), where
w ∈ Rmd is a vector containing the columns of W and ¯Ω := Ω ⊗ Id×d ∈ Rmd×md. For example,
we can rewrite the regularizer in (2) as ¯R(w, ¯Ω) = tr(cid:0)wT (λ1 ¯Ω + λ2I)w(cid:1). Writing the regularizer
in this form, it is clear that it is strongly convex with respect to matrix M−1 = λ1 ¯Ω + λ2I.

Data-local quadratic subproblems. To solve (1) across distributed nodes, we deﬁne the following
data-local subproblems, which are formed via a careful quadratic approximation of the dual problem
(3) to separate computation across the nodes. These subproblems ﬁnd updates ∆αt ∈ Rnt to the dual
variables in α corresponding to a single node t, and only require accessing data which is available
locally, i.e., Xt for node t. The t-th subproblem is given by:
nt(cid:88)

Gσ(cid:48)
t (∆αt; vt, αt) :=

t (−αi
(cid:96)∗

t−∆αi

t)+(cid:104)wt(α), Xt∆αt(cid:105)+

(cid:107)Xt∆αt(cid:107)2

Mt

+c(α) ,

(4)

min
∆αt

σ(cid:48)
2

i=1

m R∗(Xα), and Mt ∈ Rd×d is the t-th diagonal block of the symmetric positive
where c(α) := 1
deﬁnite matrix M. Given dual variables α, corresponding primal variables can be found via w(α) =
∇R∗(Xα), where wt(α) is the t-th block in the vector w(α). Note that computing w(α) requires
the vector v = Xα. The t-th block of v, vt ∈ Rd, is the only information that must be communicated
between nodes at each iteration. Finally, σ(cid:48) > 0 measures the difﬁculty of the data partitioning, and
helps to relate progress made to the subproblems to the global dual problem. It can be easily selected
based on M for many applications of interest; we provide details in Lemma 9 of the Appendix.

4

3.4 Practical Considerations

During MOCHA’s federated update of W, the central node requires a response from all workers before
performing a synchronous update. In the federated setting, a naive execution of this communication
protocol could introduce dramatic straggler effects due to node heterogeneity. To avoid stragglers,
MOCHA provides the t-th node with the ﬂexibility to approximately solve its subproblem Gσ(cid:48)
t (·),
where the quality of the approximation is controled by a per-node parameter θh
t . The following
factors determine the quality of the t-th node’s solution to its subproblem:

1. Statistical challenges, such as the size of Xt and the intrinsic difﬁculty of subproblem Gσ(cid:48)
2. Systems challenges, such as the node’s storage, computational, and communication capacities
due to hardware (CPU, memory), network connection (3G, 4G, WiFi), and power (battery level).

t (·).

3. A global clock cycle imposed by the central node specifying a deadline for receiving updates.

t from the current clock cycle and statistical/systems setting. θh
t = 0 indicates an exact solution to Gσ(cid:48)

We deﬁne θh
t as a function of these factors, and assume that each node has a controller that may
derive θh
t ranges from zero to one,
where θh
t (·) and θh
t = 1 indicates that node t made no progress
during iteration h (which we refer to as a dropped node). For instance, a node may ‘drop’ if it runs
out of battery, or if its network bandwidth deteriorates during iteration h and it is thus unable to return
its update within the current clock cycle. A formal deﬁnition of θh

t is provided in (5) of Section 4.

MOCHA mitigates stragglers by enabling the t-th node to deﬁne its own θh
t . On every iteration
h, the local updates that a node performs and sends in a clock cycle will yield a speciﬁc value
for θh
t . As discussed in Section 4, MOCHA is additionally robust to a small fraction of nodes
periodically dropping and performing no local updates (i.e., θh
:= 1) under suitable conditions,
t
as deﬁned in Assumption 2. In contrast, prior work of COCOA may suffer from severe straggler
effects in federated settings, as it requires a ﬁxed θh
t = θ across all nodes and all iterations while still
maintaining synchronous updates, and it does not allow for the case of dropped nodes (θ := 1).

Finally, we note that asynchronous updating schemes are an alternative approach to mitigate stragglers.
We do not consider these approaches in this work, in part due to the fact that the bounded-delay
assumptions associated with most asynchronous schemes limit fault tolerance. However, it would be
interesting to further explore the differences and connections between asynchronous methods and
approximation-based, synchronous methods like MOCHA in future work.

4 Convergence Analysis

MOCHA is based on a bi-convex alternating approach, which is guaranteed to converge [17, 45] to
a stationary solution of problem (1). In the case where this problem is jointly convex with respect
to W and Ω, such a solution is also optimal. In the rest of this section, we therefore focus on the
convergence of solving the W update of MOCHA in the federated setting. Following the discussion
in Section 3.4, we ﬁrst introduce the following per-node, per-round approximation parameter.
Deﬁnition 1 (Per-Node-Per-Iteration-Approximation Parameter). At each iteration h, we deﬁne the
accuracy level of the solution calculated by node t to its subproblem (4) as:
t ; v(h), α(h)
t
)

; v(h), α(h)

t (∆α(cid:63)

) − Gσ(cid:48)

θh
t :=

(5)

)

,

t

t (∆α(h)
Gσ(cid:48)
t (0; v(h), α(h)
Gσ(cid:48)
t is the minimizer of subproblem Gσ(cid:48)

t

t
) − Gσ(cid:48)
t (∆α(cid:63)
t (· ; v(h), α(h)

t

t

t ; v(h), α(h)
). We allow this value to vary between
t are made by node t at iteration h.

t := 1 meaning that no updates to subproblem Gσ(cid:48)

where ∆α(cid:63)
[0, 1], with θh

While the ﬂexible per-node, per-iteration approximation parameter θh
t in (5) allows the consideration
of stragglers and fault tolerance, these additional degrees of freedom also pose new challenges in
providing convergence guarantees for MOCHA. We introduce the following assumption on θh
t to
provide our convergence guarantees.
Assumption 2. Let Hh := (α(h), α(h−1), · · · , α(1)) be the dual vector history until the beginning
of iteration h, and deﬁne Θh
t :=
t = 1] ≤ pmax < 1 and ˆΘh
P[θh

t |Hh]. For all tasks t and all iterations h, we assume ph
t |Hh, θh

t := E[θh
t := E[θh

t < 1] ≤ Θmax < 1.

5

This assumption states that at each iteration, the probability of a node sending a result is non-zero,
and that the quality of the returned result is, on average, better than the previous iterate. Compared
to [49, 30] which assumes θh
t = θ < 1, our assumption is signiﬁcantly less restrictive and better
models the federated setting, where nodes are unreliable and may periodically drop out.

Using Assumption 2, we derive the following theorem, which characterizes the convergence of the
federated update of MOCHA in ﬁnite horizon when the losses (cid:96)t in (1) are smooth.
Theorem 1. Assume that the losses (cid:96)t are (1/µ)-smooth. Then, under Assumptions 1 and 2, there
exists a constant s ∈ (0, 1] such that for any given convergence target (cid:15)D, choosing H such that
1
(1 − ¯Θ)s

H ≥

log

(6)

,

n
(cid:15)D

will satisfy E[D(α(H)) − D(α(cid:63))] ≤ (cid:15)D .
Here, ¯Θ := pmax + (1 − pmax)Θmax < 1. While Theorem 1 is concerned with ﬁnite horizon conver-
gence, it is possible to get asymptotic convergence results, i.e., H → ∞, with milder assumptions on
the stragglers; see Corollary 8 in the Appendix for details.

When the loss functions are non-smooth, e.g., the hinge loss for SVM models, we provide the
following sub-linear convergence for L-Lipschitz losses.
Theorem 2. If the loss functions (cid:96)t are L-Lipschitz, then there exists a constant σ, deﬁned in (24),
such that for any given (cid:15)D > 0, if we choose

(7)

H ≥ H0 +

(cid:24)

2
(1 − ¯Θ)
(cid:25)

(cid:20)

(cid:18)

max

1,

2L2σσ(cid:48)
n2(cid:15)D

(cid:19) (cid:25)
,

(cid:24)

with H0 ≥

h0+

then ¯α := 1

H−H0

(cid:80)H

16L2σσ(cid:48)
(1 − ¯Θ)n2(cid:15)D
h=H0+1 α(h) will satisfy E[D( ¯α) − D(α(cid:63))] ≤ (cid:15)D .

1
(1 − ¯Θ)

, h0 =

1 +

log

(cid:18) 2n2(D(α(cid:63)) − D(α0))
4L2σσ(cid:48)

(cid:19)(cid:21)

,

+

These theorems guarantee that MOCHA will converge in the federated setting, under mild assumptions
on stragglers and capabilities of the nodes. While these results consider convergence in terms of the
dual, we show that they hold analogously for the duality gap. We provide all proofs in Appendix C.
Remark 2. Following from the discussion in Section 3.4, our method and theory generalize the
results in [22, 31]. In the limiting case that all θh
t are identical, our results extend the results of
COCOA to the multi-task framework described in (1).
Remark 3. Note that the methods in [22, 31] have an aggregation parameter γ ∈ (0, 1]. Though we
prove our results for a general γ, we simplify the method and results here by setting γ := 1, which
has been shown to have the best performance, both theoretically and empirically [31].

5 Simulations

In this section we validate the empirical performance of MOCHA. First, we introduce a benchmarking
suite of real-world federated datasets and show that multi-task learning is well-suited to handle the
statistical challenges of the federated setting. Next, we demonstrate MOCHA’s ability to handle strag-
glers, both from statistical and systems heterogeneity. Finally, we explore the performance of MOCHA
when devices periodically drop out. Our code is available at: github.com/gingsmith/fmtl.

5.1 Federated Datasets

In our simulations, we use several real-world datasets that have been generated in federated settings.
We provide additional details in the Appendix, including information about data sizes, nt.

• Google Glass (GLEAM)4: This dataset consists of two hours of high resolution sensor data
collected from 38 participants wearing Google Glass for the purpose of activity recognition.
Following [41], we featurize the raw accelerometer, gyroscope, and magnetometer data into 180
statistical, spectral, and temporal features. We model each participant as a separate task, and
predict between eating and other activities (e.g., walking, talking, drinking).

4

http://www.skleinberg.org/data/GLEAM.tar.gz

6

• Human Activity Recognition5: Mobile phone accelerometer and gyroscope data collected from
30 individuals, performing one of six activities: {walking, walking-upstairs, walking-downstairs,
sitting, standing, lying-down}. We use the provided 561-length feature vectors of time and
frequency domain variables generated for each instance [3]. We model each individual as a
separate task and predict between sitting and the other activities.

• Vehicle Sensor6: Acoustic, seismic, and infrared sensor data collected from a distributed network
of 23 sensors, deployed with the aim of classifying vehicles driving by a segment of road [13].
Each instance is described by 50 acoustic and 50 seismic features. We model each sensor as a
separate task and predict between AAV-type and DW-type vehicles.

5.2 Multi-Task Learning for the Federated Setting

We demonstrate the beneﬁts of multi-task learning for the federated setting by comparing the error
rates of a multi-task model to that of a fully local model (i.e., learning a model for each task separately)
and a fully global model (i.e., combining the data from all tasks and learning one single model). Work
on federated learning thus far has been limited to the study of fully global models [25, 36, 26].

We use a cluster-regularized multi-task model [57], as described in Section 3.1. For each dataset from
Section 5.1, we randomly split the data into 75% training and 25% testing, and learn multi-task, local,
and global support vector machine models, selecting the best regularization parameter, λ ∈{1e-5,
1e-4, 1e-3, 1e-2, 0.1, 1, 10}, for each model using 5-fold cross-validation. We repeat this process 10
times and report the average prediction error across tasks, averaged across these 10 trials.

Table 1: Average prediction error: Means and standard errors over 10 random shufﬂes.

Model Human Activity Google Glass Vehicle Sensor

Global

2.23 (0.30)

5.34 (0.26)

13.4 (0.26)

Local

MTL

1.34 (0.21)

4.92 (0.26)

7.81 (0.13)

0.46 (0.11)

2.02 (0.15)

6.59 (0.21)

In Table 1, we see that for each dataset, multi-task learning signiﬁcantly outperforms the other
models in terms of achieving the lowest average error across tasks. The global model, as proposed
in [25, 36, 26] performs the worst, particularly for the Human Activity and Vehicle Sensor datasets.
Although the datasets are already somewhat unbalanced, we note that a global modeling approach may
beneﬁt tasks with a very small number of instances, as information can be shared across tasks. For this
reason, we additionally explore the performance of global, local, and multi-task modeling for highly
skewed data in Table 4 of the Appendix. Although the performance of the global model improves
slightly relative to local modeling in this setting, the global model still performs the worst for the
majority of the datasets, and MTL still signiﬁcantly outperforms both global and local approaches.

5.3 Straggler Avoidance

Two challenges that are prevalent in federated learning are stragglers and high communication.
Stragglers can occur when a subset of the devices take much longer than others to perform local
updates, which can be caused either by statistical or systems heterogeneity. Communication can also
exacerbate poor performance, as it can be slower than computation by many orders of magnitude in
typical cellular or wireless networks [52, 20, 48, 9, 38]. In our experiments below, we simulate the
time needed to run each method by tracking the operations and communication complexities, and
scaling the communication cost relative to computation by one, two, or three orders of magnitude,
respectively. These numbers correspond roughly to the clock rate vs. network bandwidth/latency
(see, e.g., [52]) for modern cellular and wireless networks. Details are provided in Appendix E.

5

6

https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones
http://www.ecs.umass.edu/~mduarte/Software.html

7

Figure 1: The performance of MOCHA compared to other distributed methods for the W update of (1). While
increasing communication tends to decrease the performance of the mini-batch methods, MOCHA performs well
in high communication settings. In all settings, MOCHA with varied approximation values, Θh
t , performs better
than without (i.e., naively generalizing COCOA), as it avoids stragglers from statistical heterogeneity.

Statistical Heterogeneity. We explore the effect of statistical heterogeneity on stragglers for various
methods and communication regimes (3G, LTE, WiFi). For a ﬁxed communication network, we
compare MOCHA to COCOA, which has a single θ parameter, and to mini-batch stochastic gradient
descent (Mb-SGD) and mini-batch stochastic dual coordinate ascent (Mb-SDCA), which have limited
communication ﬂexibility depending on the batch size. We tune all compared methods for best
performance, as we detail in Appendix E. In Figure 1, we see that while the performance degrades
for mini-batch methods in high communication regimes, MOCHA and COCOA are robust to high
communication. However, COCOA is signiﬁcantly affected by stragglers—because θ is ﬁxed across
nodes and rounds, difﬁcult subproblems adversely impact convergence. In contrast, MOCHA performs
well regardless of communication cost and is robust to statistical heterogeneity.

Systems Heterogeneity. MOCHA is also equipped to handle heterogeneity from changing systems
environments, such as battery power, memory, or network connection, as we show in Figure 2. In
particular, we simulate systems heterogeneity by randomly choosing the number of local iterations
for MOCHA or the mini-batch size for mini-batch methods, between 10% and 100% of the minimum
number of local data points for high variability environments, to between 90% and 100% for low
variability (see Appendix E for full details). We do not vary the performance of COCOA, as the
impact from statistical heterogeneity alone signiﬁcantly reduces performance. However, adding
systems heterogeneity would reduce performance even further, as the maximum θ value across all
nodes would only increase if additional systems challenges were introduced.

5.4 Tolerance to Dropped Nodes

Finally, we explore the effect of nodes dropping
on the performance of MOCHA. We do not draw
comparisons to other methods, as to the best of
our knowledge, no other methods for distributed
multi-task learning directly address fault toler-
ance. In MOCHA, we incorporate this setting
by allowing θh
t := 1, as explored theoretically
in Section 4. In Figure 3, we look at the per-
formance of MOCHA, either for one ﬁxed W
update, or running the entire MOCHA method, as
the probability that nodes drop at each iteration
(ph
t in Assumption 2) increases. We see that the
performance of MOCHA is robust to relatively
high values of ph
t , both during a single update of
W and in how this affects the performance of
the overall method. However, as intuition would
suggest, if one of the nodes never sends updates
(i.e., ph
1 := 1 for all h, green dotted line), the
method does not converge to the correct solution.
This provides validation for our Assumption 2.

Figure 2: MOCHA can handle variability from systems
heterogeneity.

Figure 3: The performance of MOCHA is robust to
nodes periodically dropping out (fault tolerance).

8

6 Discussion

To address the statistical and systems challenges of the burgeoning federated learning setting, we have
presented MOCHA, a novel systems-aware optimization framework for federated multi-task learning.
Our method and theory for the ﬁrst time consider issues of high communication cost, stragglers,
and fault tolerance for multi-task learning in the federated environment. While MOCHA does not
apply to non-convex deep learning models in its current form, we note that there may be natural
connections between this approach and “convexiﬁed” deep learning models [6, 34, 51, 56] in the
context of kernelized federated multi-task learning.

We thank Brendan McMahan, Chloé Kiddon, Jakub Koneˇcný, Evan Sparks, Xinghao Pan, Lisha Li,
and Hang Qi for valuable discussions and feedback.

Acknowledgements

References

[1] A. Ahmed, A. Das, and A. J. Smola. Scalable hierarchical multitask learning algorithms for conversion

optimization in display advertising. In Conference on Web Search and Data Mining, 2014.

[2] R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unlabeled

data. Journal of Machine Learning Research, 6:1817–1853, 2005.

[3] D. Anguita, A. Ghio, L. Oneto, X. Parra, and J. L. Reyes-Ortiz. A public domain dataset for human activity
recognition using smartphones. In European Symposium on Artiﬁcial Neural Networks, Computational
Intelligence and Machine Learning, 2013.

[4] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In Neural Information Processing

[5] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning,

Systems, 2007.

73(3):243–272, 2008.

[6] Ö. Aslan, X. Zhang, and D. Schuurmans. Convex deep learning via normalized kernels. In Advances in

Neural Information Processing Systems, 2014.

[7] I. M. Baytas, M. Yan, A. K. Jain, and J. Zhou. Asynchronous multi-task learning. In International

Conference on Data Mining, 2016.

[8] F. Bonomi, R. Milito, J. Zhu, and S. Addepalli. Fog computing and its role in the internet of things. In

SIGCOMM Workshop on Mobile Cloud Computing, 2012.

[9] A. Carroll and G. Heiser. An analysis of power consumption in a smartphone. In USENIX Annual Technical

Conference, 2010.

[10] R. Caruana. Multitask learning. Machine Learning, 28:41–75, 1997.
[11] J. Chen, J. Zhou, and J. Ye. Integrating low-rank and group-sparse structures for robust multi-task learning.

In Conference on Knowledge Discovery and Data Mining, 2011.

[12] A. Deshpande, C. Guestrin, S. R. Madden, J. M. Hellerstein, and W. Hong. Model-based approximate

querying in sensor networks. VLDB Journal, 14(4):417–443, 2005.

[13] M. F. Duarte and Y. H. Hu. Vehicle classiﬁcation in distributed sensor networks. Journal of Parallel and

Distributed Computing, 64(7):826–838, 2004.

[14] T. Evgeniou and M. Pontil. Regularized multi-task learning. In Conference on Knowledge Discovery and

Data Mining, 2004.

[15] P. Garcia Lopez, A. Montresor, D. Epema, A. Datta, T. Higashino, A. Iamnitchi, M. Barcellos, P. Felber,
and E. Riviere. Edge-centric computing: Vision and challenges. SIGCOMM Computer Communication
Review, 45(5):37–42, 2015.

[16] A. R. Gonçalves, F. J. Von Zuben, and A. Banerjee. Multi-task sparse structure learning with gaussian

copula models. Journal of Machine Learning Research, 17(33):1–30, 2016.

[17] J. Gorski, F. Pfeuffer, and K. Klamroth. Biconvex sets and optimization with biconvex functions: a survey

and extensions. Mathematical Methods of Operations Research, 66(3):373–407, 2007.

[18] K. Hong, D. Lillethun, U. Ramachandran, B. Ottenwälder, and B. Koldehofe. Mobile fog: A programming
model for large-scale applications on the internet of things. In SIGCOMM Workshop on Mobile Cloud
Computing, 2013.

[19] C.-J. Hsieh, M. A. Sustik, I. S. Dhillon, and P. Ravikumar. Sparse Inverse Covariance Matrix Estimation

Using Quadratic Approximation. In Neural Information Processing Systems 27, 2014.

9

[20] J. Huang, F. Qian, Y. Guo, Y. Zhou, Q. Xu, Z. M. Mao, S. Sen, and O. Spatscheck. An in-depth study of
lte: Effect of network protocol and application behavior on performance. In ACM SIGCOMM Conference,
2013.

[21] L. Jacob, J.-p. Vert, and F. R. Bach. Clustered multi-task learning: A convex formulation. In Neural

Information Processing Systems, 2009.

[22] M. Jaggi, V. Smith, J. Terhorst, S. Krishnan, T. Hofmann, and M. I. Jordan. Communication-Efﬁcient

Distributed Dual Coordinate Ascent. In Neural Information Processing Systems, 2014.

[23] X. Jin, P. Luo, F. Zhuang, J. He, and Q. He. Collaborating between local and global learning for distributed

online multiple tasks. In Conference on Information and Knowledge Management, 2015.

[24] S. Kim and E. P. Xing. Statistical estimation of correlated genome associations to a quantitative trait

[25] J. Koneˇcn`y, H. B. McMahan, and D. Ramage. Federated optimization: Distributed optimization beyond

network. PLoS Genet, 5(8):e1000587, 2009.

the datacenter. arXiv:1511.03575, 2015.

[26] J. Koneˇcn`y, H. B. McMahan, F. X. Yu, P. Richtárik, A. T. Suresh, and D. Bacon. Federated learning:

Strategies for improving communication efﬁciency. arXiv:1610.05492, 2016.

[27] T. Kuﬂik, J. Kay, and B. Kummerfeld. Challenges and solutions of ubiquitous user modeling. In Ubiquitous

display environments, pages 7–30. Springer, 2012.

[28] A. Kumar and H. Daumé. Learning task grouping and overlap in multi-task learning. In International

Conference on Machine Learning, 2012.

[29] S. L. Lauritzen. Graphical Models, volume 17. Clarendon Press, 1996.
[30] S. Liu, S. J. Pan, and Q. Ho. Distributed multi-task relationship learning. Conference on Knowledge

Discovery and Data Mining, 2017.

[31] C. Ma, V. Smith, M. Jaggi, M. I. Jordan, P. Richtárik, and M. Takáˇc. Adding vs. averaging in distributed

primal-dual optimization. In International Conference on Machine Learning, 2015.

[32] S. Madden, M. J. Franklin, J. M. Hellerstein, and W. Hong. TAG: A tiny aggregation service for ad-hoc

sensor networks. In Symposium on Operating Systems Design and Implementation, 2002.

[33] S. Madden, M. J. Franklin, J. M. Hellerstein, and W. Hong. TinyDB: An acquisitional query processing

system for sensor networks. ACM Transactions on Database Systems, 30(1):122–173, 2005.

[34] J. Mairal, P. Koniusz, Z. Harchaoui, and C. Schmid. Convolutional kernel networks. In Neural Information

Processing Systems, 2014.

[35] D. Mateos-Núñez and J. Cortés. Distributed optimization for multi-task learning via nuclear-norm

approximation. In IFAC Workshop on Distributed Estimation and Control in Networked Systems, 2015.

[36] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efﬁcient learning

of deep networks from decentralized data. In Conference on Artiﬁcial Intelligence and Statistics, 2017.
Ramage.

http://www.googblogs.com/

McMahan

and

D.

B.

[37] H.

federated-learning-collaborative-machine-learning-without-centralized-training-data/.
Google, 2017.

[38] A. P. Miettinen and J. K. Nurminen. Energy efﬁciency of mobile clients in cloud computing. In USENIX

Conference on Hot Topics in Cloud Computing, 2010.

[39] A. Pantelopoulos and N. G. Bourbakis. A survey on wearable sensor-based systems for health monitoring

and prognosis. IEEE Transactions on Systems, Man, and Cybernetics, 40(1):1–12, 2010.

[40] H. Qi, E. R. Sparks, and A. Talwalkar. Paleo: A performance model for deep neural networks.

In

International Conference on Learning Representations, 2017.

[41] S. A. Rahman, C. Merck, Y. Huang, and S. Kleinberg. Unintrusive eating recognition using google glass.

In Conference on Pervasive Computing Technologies for Healthcare, 2015.

[42] P. Rashidi and D. J. Cook. Keeping the resident in the loop: Adapting the smart home to the user. IEEE

Transactions on systems, man, and cybernetics, 39(5):949–959, 2009.

[43] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. XNOR-Net: ImageNet classiﬁcation using binary

convolutional neural networks. In European Conference on Computer Vision, 2016.

[44] S. Ravi.

https://research.googleblog.com/2017/02/on-device-machine-intelligence.

html. Google, 2017.

[45] M. Razaviyayn, M. Hong, and Z.-Q. Luo. A uniﬁed convergence analysis of block successive minimization

methods for nonsmooth optimization. SIAM Journal on Optimization, 23(2):1126–1153, 2013.

[46] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal Estimated sub-GrAdient SOlver for SVM.

International Conference on Machine Learning, June 2007.

[47] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss minimiza-

tion. Journal of Machine Learning Research, 14:567–599, 2013.

[48] D. Singelée, S. Seys, L. Batina, and I. Verbauwhede. The communication and computation cost of wireless

security. In ACM Conference on Wireless Network Security, 2011.

10

[49] V. Smith, S. Forte, C. Ma, M. Takác, M. I. Jordan, and M. Jaggi. CoCoA: A general framework for

communication-efﬁcient distributed optimization. arXiv:1611.02189, 2016.

[50] M. Takáˇc, A. Bijral, P. Richtárik, and N. Srebro. Mini-Batch Primal and Dual Methods for SVMs. In

International Conference on Machine Learning, 2013.

[51] C.-Y. Tsai, A. M. Saxe, and D. Cox. Tensor switching networks. In Neural Information Processing Systems,

2016.

[52] C. Van Berkel. Multi-core for mobile phones. In Proceedings of the Conference on Design, Automation

and Test in Europe, pages 1260–1265. European Design and Automation Association, 2009.

[53] H. Wang, A. Banerjee, C.-J. Hsieh, P. K. Ravikumar, and I. S. Dhillon. Large scale distributed sparse

precision estimation. In Neural Information Processing Systems, 2013.

[54] J. Wang, M. Kolar, and N. Srebro. Distributed multi-task learning. In Conference on Artiﬁcial Intelligence

and Statistics, 2016.

arXiv:1603.02185, 2016.

[55] J. Wang, M. Kolar, and N. Srebro. Distributed multi-task learning with shared representation.

[56] Y. Zhang, P. Liang, and M. J. Wainwright. Convexiﬁed convolutional neural networks. International

Conference on Machine Learning, 2017.

[57] Y. Zhang and D.-Y. Yeung. A convex formulation for learning task relationships in multi-task learning. In

Conference on Uncertainty in Artiﬁcial Intelligence, 2010.

[58] J. Zhou, J. Chen, and J. Ye. Clustered multi-task learning via alternating structure optimization. In Neural

Information Processing Systems, 2011.

11

A Preliminaries

Notation. We use Id×d to represent an identity matrix of size d × d. When the context allows, we
use the notation I to denote an identity matrix of an appropriate size. We also use ⊗ to denote the
Kronecker product between two matrices.
Deﬁnition 2 (Matrix norm). Given a symmetric positive deﬁnite matrix M, the norm of u with
respect to M is given by (cid:107)u(cid:107)M :=
Deﬁnition 3 (L-smooth). A convex function f is L-smooth with respect to M if

uT Mu .

√

f (u) ≤ f (v) + (cid:104)∇f (v), u − v(cid:105) +

(cid:107)u − v(cid:107)2
M

∀u, v .

L
2

If M = I then, we simply say f is L-smooth.
Deﬁnition 4 (τ -strongly convex). A function f is τ -strongly convex with respect to M if

f (u) ≥ f (v) + (cid:104)z, u − v(cid:105) +

∀u, v, z ∈ ∂f (v) ,

τ
2

(cid:107)u − v(cid:107)2
M

where ∂f (v) is the set of sub-differentials of function f at v. If M = I then, we simply say f is
τ -strongly convex.
Deﬁnition 5. The function f is called L-Lipchitz if for any x and y in its domain

(8)

(9)

(10)

|f (x) − f (y)| ≤ L(cid:107)x − y(cid:107)2 .

If a function f is L-Lipchitz then its dual will be L-bounded, i.e., for any α such that (cid:107)α(cid:107)2 > L,
then f ∗(α) = +∞.

B Multi-Task Learning

In this section, we summarize several popular multi-task learning formulations that can be written in
the form of (1) and can therefore be addressed by our framework, MOCHA. While the W update is
discussed in Section 3, we provide details here on how to solve the Ω update for these formulations.

B.1 Multi-Task Learning Formulations

MTL with cluster structure.
In MTL models that assume a cluster structure, the weight vectors
for each task, wt, are assumed to ‘close’ according to some metric to other weight vectors from tasks
in the same cluster. This idea goes back to mean-regularized MTL [14], which assumes that all the
tasks form one cluster, and that the weight vectors are close to their mean. Such a regularizer could
be formulated in the form of (1) by choosing Ω = (Im×m − 1
m 11T )2, where Im×m is the identity
matrix of size m × m and 1m represents a vector of all ones with size m. In this case, we set R to be
R(W, Ω) = λ1 tr(cid:0)WΩWT (cid:1) + λ2(cid:107)W(cid:107)2
F ,
where λ1, λ2 > 0 are parameters. Note that in this formulation, the structural dependence matrix Ω
is known a-priori. However, it is natural to assume multiple clusters exist, and to learn this clustering
structure directly from the data [58]. For such a model, the problem formulation is non-convex if a
perfect clustering structure is imposed [58, 21]. However, by performing a convex relaxation, the
following regularizer is obtained [58, 21]

(11)

R(W, Ω) = λ tr(cid:0)W(ηI + Ω)−1WT (cid:1), Ω ∈ Q =

Q | Q (cid:23) 0, tr(Q) = k, Q (cid:22) I

,

(12)

(cid:26)

(cid:27)

where λ and η are regularization parameters, k is the number of clusters, and Ω deﬁnes the clustering
structure.

MTL with probabilistic priors. Another set of MTL models that can be realized by our framework
enforce structure by putting probabilistic priors on the dependence among the columns of W. For
example, in [57] it is assumed that the weight matrix W has a prior distribution of the form:

W ∼

N (0, σ2I)

MN (0, Id×d ⊗ Ω) ,

(13)

(cid:32) m
(cid:89)

i=1

(cid:33)

12

where N (0, σ2I) denotes the normal distribution with mean 0 and covariance σ2I, and
MN (0, Id×d ⊗ Ω) denotes the matrix normal distribution with mean 0, row covariance Id×d,
and column covariance Ω. This prior generates a regularizer of the following form [57]:

R(W, Ω) = λ

(cid:18) 1
σ2 (cid:107)W(cid:107)2 + tr(cid:0)WΩ−1WT (cid:1) + d log |Ω|

(cid:19)

, λ > 0 .

Unfortunately, such a regularizer is non-convex with respect to Ω due to the concavity of log |Ω|. To
obtain a jointly convex formulation in Ω and W, the authors in [57] propose omitting log |Ω| and
controlling the complexity of Ω by adding a constraint on tr(Ω):

R(W, Ω) = λ

(cid:18) 1
σ2 (cid:107)W(cid:107)2 + tr(cid:0)WΩ−1WT (cid:1)

(cid:19)

(cid:26)

(cid:27)

, Ω ∈ Q =

Q | Q (cid:23) 0, tr(Q) = 1

.

(14)

It is worth noting that unlike the clustered MTL formulations, such as (2), the probabilistic formulation
in (14) can model both positive and negative relationships among the tasks through the covariance
matrix.

MTL with graphical models. Another way of modeling task relationships is through the precision
matrix. This is popular in graphical models literature [29] because it encodes conditional indepen-
dence among variables. In other words, if we denote the precision matrix among tasks in matrix
variate Gaussian prior with Ω, then Ωi,j = 0 if and only if tasks weights wi and wj are independent
given the rest of the task weights [16]. Therefore, assuming sparsity in the structure among the tasks
translates to sparsity in matrix Ω. As a result, we can formulate a sparsity-promoting regularizer by:

R(W, Ω) = λ

(cid:18) 1
σ2 (cid:107)W(cid:107)2 + tr(cid:0)WΩWT (cid:1) − d log |Ω|

(cid:19)

+ λ1(cid:107)W(cid:107)1 + λ2(cid:107)Ω(cid:107)1 ,

(15)

where λ1, λ2 ≥ 0 control the sparsity of W and Ω respectively [16]. It is worth noting that although
this problem is jointly non-convex in W and Ω, it is bi-convex.

B.2 Strong Convexity of MTL Regularizers

Recall that in Assumption 1, we presumed that the vectorized formulation of the MTL regularizer is
strongly convex with respect to a matrix M−1. In this subsection we discuss the choice of matrix M
for the widely-used MTL formulations introduced in Section B.1.

Using the notation from Remark 1 for the clustered MTL formulation (11), it is easy to see that
¯R(w, ¯Ω) = λ1wT ¯Ωw + λ2(cid:107)w(cid:107)2
2, where ¯Ω := Ω ⊗ Id×d. As a result, it is clear that ¯R(w, ¯Ω) is
1-strongly convex with respect to M−1 = λ1 ¯Ω + λ2Imd×md.
Using a similar reasoning, it is easy to see that the matrix M can be chosen as λ−1(ηI + ¯Ω),
λ−1( 1

σ2 I + ¯Ω)−1 for (12), (14) and (15) respectively.

σ2 I + ¯Ω−1)−1 and λ−1( 1

B.3 Optimizing Ω in MTL Formulations

In this section, we brieﬂy cover approaches to update Ω in the MTL formulations introduced in
Section B.1. First, it is clear that (2) does not require any updates to Ω, as it is assumed to be ﬁxed.
In (12), it can be shown [58, 21] that the optimal solution for Ω has the same column space as the
rows of W. Therefore, the problem boils down to solving a simple convex optimization problem
over the eigenvalues of Ω; see [58, 21] for details. Although outside the scope of this paper, we
note that the bottleneck of this approach to ﬁnding Ω is computing the SVD of W, which can be
a challenging problem when m is large. In the probabilistic model of (14), the Ω update is given
in [57] by (WT W) 1
2 , which requires computing the eigenvalue decomposition of WT W. For
the graphical model formulation, the problem of solving for Ω is called sparse precision matrix
estimation or graphical lasso [16]. This is a well-studied problem, and many scalable algorithms have
been proposed to solve it [53, 16, 19].

B.3.1 Reducing the Size of Ω by Sharing Tasks

One interesting aspect of MOCHA is that the method can be easily modiﬁed to accommodate the
sharing of tasks among the nodes without any change to the local solvers. This property helps the
central node to reduce the size of Ω and the complexity of its update with minimal changes to the
whole system. The following remark highlights this capability.

13

Remark 4. MOCHA can be modiﬁed to solve problems when there are tasks that are shared among
nodes. In this case, each node still solves a data local sub-problem based on its own data for the task,
but the central node needs to do an additional aggregation step to add the results for all the nodes
that share the data of each task. This reduces the size of matrix Ω and simpliﬁes its update.

C Convergence Analysis

In the rest of this section we use the superscript (h) or h to denote the corresponding
Notation.
variable at iteration (h) of the federated update in MOCHA. When context allows, we drop the
superscript to simplify notation.

In order to provide a general convergence analysis, similar to the ones provided in [22, 31, 49], we
assume an aggregation parameter γ ∈ (0, 1] in this section. With such an aggregation parameter, the
updates in each federated iteration would be αt ← αt + γ∆αt and vt ← vt + γ∆vt. For a more
detailed discussion on the role of aggregation parameter, see Appendix D.1. Note that in Algorithm 1,
MOCHA is presented assuming γ = 1 for simplicity.

Before proving our convergence guarantees, we provide several useful deﬁnitions and key lemmas.
Deﬁnition 6. For each task t, deﬁne

σt := max
α∈Rnt

(cid:107)Xtα(cid:107)2
(cid:107)α(cid:107)2

Mt

and σmax := max
t∈[m]

σt.

Deﬁnition 7. For any α, deﬁne the duality gap as

where P(W) := (cid:80)m
t=1

(cid:80)nt

i=1 (cid:96)t(wT

t xi

t, yi

t) + R(W, Ω) as in (1).

G(α) := D(α) − (−P(W(α))),

(16)

(17)

The following lemma uses Assumption 2 to bound the average performance of θh
providing global convergence guarantees for MOCHA.
Lemma 3. Under Assumption 2, Θh

t ≤ ¯Θ = pmax + (1 − pmax)Θmax < 1.

t , which is crucial in

Θh

Proof. Recalling the deﬁnitions ph
t = E[θh
= P[θh
= ph

t |Hh]
t = 1] · E[θh
t · 1 + (1 − ph

t |θh
t ) · ˆΘh

t ≤ ¯Θ < 1,

t := P[θh

t = 1] and ˆΘh

t := E[θh

t |θh

t < 1, Hh], we have

t = 1, Hh] + (1 − P[θh

t < 1]) · E[θh

t |θh

t < 1, Hh]

where the last inequality is due to Assumption 2, and the fact that ˆΘh

t < 1 by deﬁnition.

The next key lemma bounds the dual objective of an iterate based on the dual objective of the previous
iterate and the objectives of local subproblems.
Lemma 4. For any α, ∆α ∈ Rn and γ ∈ (0, 1] if σ(cid:48) satisﬁes (28), then

D (α + γ∆α) ≤ (1 − γ)D(α) + γ

Gσ(cid:48)
t (∆αt; v, αt) .

(18)

m
(cid:88)

t=1

Proof. The proof of this lemma is similar to [49, Lemma 1] and follows from the deﬁnition of local
sub-problems, smoothness of R∗ and the choice of σ(cid:48) in (28).

Recall that if the functions (cid:96)t are (1/µ)-smooth, their conjugates (cid:96)∗
lemma below provides a bound on the amount of improvement in dual objective in each iteration.
Lemma 5. If the functions (cid:96)∗

t are µ-strongly convex for some µ ≥ 0. Then, for any s ∈ [0, 1].

t will be µ-strongly convex. The

E[D(α(h)) − D(α(h+1))|Hh] ≥ γ

(1 − ¯Θ)

sGt(α(h)) −

(19)

(cid:18)

(cid:19)

Jt

,

σ(cid:48)s2
2

m
(cid:88)

t=1

14

where

Gt(α) :=

t (−αi

t) + (cid:96)t(wt(α)(cid:62)xi

t, yi

t) + αi

twt(α)(cid:62)xi
t

(cid:3) ,

nt(cid:88)

i=1

(cid:2)(cid:96)∗

Jt := − µ(1−s)

σ(cid:48)s (cid:107)(ut − α(h)

t

)(cid:107)2 + (cid:107)Xt(ut − α(h)

)(cid:107)2

t

Mt

,

for ut ∈ Rnt with

t ∈ ∂(cid:96)t(wt(α)(cid:62)xi
ui

t, yi

t) .

(20)

(21)

(22)

Proof. Applying Lemma 4 and recalling D(α) = (cid:80)m
t (0; v, αt), we can ﬁrst bound the
improvement for each task separately. Following a similar approach as in the proof of [49, Lemma 7]
we can obtain the bound (19) which bounds the improvement from α(h) to α(h+1).

t=1 Gσ(cid:48)

The following lemma relates the improvement of the dual objective in one iteration to the duality gap
for the smooth loss functions (cid:96)t.
Lemma 6. If the loss functions (cid:96)t are (1/µ)-smooth, then there exists a proper constants s ∈ (0, 1] ,
such that for any γ ∈ (0, 1] at any iteration h

(cid:104)

E

D(α(h)) − D(α(h+1))|Hh

≥ sγ(1 − ¯Θ)G(α(h)),

(cid:105)

(23)

where G(α(h)) is the duality gap of α(h) which is deﬁned in (17).

Proof. Recall the deﬁnition of σmax in (16). Now, if we carefully choose s = µ/(µ + σmaxσ(cid:48)), it
is easy to show that Jt ≤ 0 in (19); see [49, Theorem 11] for details. The ﬁnal result follows as a
consequence of Lemma 5.

Note that Lemma 5 holds even if the functions are non-smooth, i.e. µ = 0. However, we cannot infer
sufﬁcient decrease of Lemma 6 from Lemma 5 when µ = 0. Therefore, we need additional tools
when the losses are L-Liptchitz. The ﬁrst is the following lemma, which bounds the J term in (19).
Lemma 7. Assume that the loss functions (cid:96)t are L-Lipschitz. Denote J := (cid:80)m
t=1 Jt, where Jt is
deﬁned in (21), then

J ≤ 4L2

σtnt := 4L2σ,

(24)

m
(cid:88)

t=1

where σt is deﬁned in (16).

Proof. The proof is similar to [31, Lemma 6] and using the deﬁnitions of σ and σt and the fact that
the losses are L-Lipchitz.

C.1 Convergence Analysis for Smooth Losses

C.1.1 Proof of Theorem 1

Let us rewrite (23) from Lemma 6 as

E[D(α(h)) − D(α(h+1))|Hh] = D(α(h)) − D(α(cid:63)) + E[D(α(cid:63)) − D(α(h+1))|Hh]
≥ sγ(1 − ¯Θ)G(α(h))
≥ sγ(1 − ¯Θ)

D(α(h)) − D(α(cid:63))

(cid:16)

(cid:17)

,

where the last inequality is due to weak duality, i.e. G(α(h)) ≥ D(α(h)) − D(α(cid:63)). Re-arranging the
terms in the above inequality, we can easily get

E[D(α(h+1)) − D(α(cid:63))|Hh] ≤ (cid:0)1 − sγ(1 − ¯Θ)(cid:1) (cid:16)

D(α(h)) − D(α(cid:63))

(cid:17)

(25)

15

Recursively applying this inequality and taking expectations from both sides, we arrive at

E[D(α(h+1)) − D(α(cid:63))] ≤ (cid:0)1 − sγ(1 − ¯Θ)(cid:1)h+1 (cid:16)

D(α(0)) − D(α(cid:63))

(cid:17)

.

(26)

Now we can use a simple bound on the initial duality gap [49, Lemma 10], which states that
D(α(0)) − D(α(cid:63)) ≤ n, to get the ﬁnal result. It is worth noting that we can translate the bound on
the dual distance to optimality to the bound on the duality gap using the following inequalities
sγ(1 − ¯Θ) E[G(α(H))] ≤ E[D(α(H)) − D(α(H+1))] ≤ E[D(α(H)) − D(α(cid:63))] ≤ (cid:15)D,

(27)
where the ﬁrst inequality is due to (23), the second inequality is due to the optimality of α(cid:63), and the
last inequality is the bound we just proved for the dual distance to optimality.

C.1.2 Asymptotic Convergence

In the case of smooth loss functions, it is possible to get asymptotic convergence results under milder
assumptions. The following corollary is an extension of Theorem 1.
Corollary 8. If the loss functions (cid:96)t are µ-smooth, then under Assumption 1, E[D(α(H))−D(α(cid:63))] →
0 as H → ∞ if either of the following conditions hold

• lim suph→∞ ph
• For any task t, (cid:0)1 − ph

t < 1 and lim suph→∞
(cid:17)
(cid:1) ×

1 − ˆΘh
t

(cid:16)

t

ˆΘh

t < 1.

equal to 1.

= ω( 1

h ). Note that in this case limh→∞ ph

t can be

Proof. The proof is similar to the proof of Theorem 1. We can use the same steps to get a sufﬁcient
decrease inequality like the one in (25), with ¯Θ replaced with ¯Θh := maxt Θh
t .
E[D(α(h+1)) − D(α(cid:63))|Hh] ≤ (cid:0)1 − sγ(1 − ¯Θh)(cid:1) (cid:16)

D(α(h)) − D(α(cid:63))

(cid:17)

The rest of the argument follows by applying this inequality recursively and using the assumptions in
the corollary.

C.2 Convergence Analysis for Lipschitz Losses: Proof for Theorem 2

Proof. For L-Lipschitz loss functions, the proof follows the same line of reasoning as the proof of
Theorem 8 in [31] and therefore we do not cover it in detail. Unlike the case with smooth losses,
it is not possible to bound the decrease in dual objective by (23). However, we can use Lemma
5 with µ = 0. The next step is to bound J = (cid:80)m
t=1 Jt in (19), which can be done via Lemma 7.
Finally, we apply the inequalities recursively, choose s carefully, and bound the terms in the ﬁnal
inequality. We refer the reader to the proof of Theorem 8 in [31] for more details. It is worth noting
that similar to Theorem 1, we can similarly get bounds on the expected duality gap, instead of the
dual objective.

D Choosing σ(cid:48)

In order to guarantee the convergence of the federated update of MOCHA, the parameter σ(cid:48) must
satisfy:

σ(cid:48)

m
(cid:88)

t=1

(cid:107)Xtαt(cid:107)2

Mt

≥ γ(cid:107)Xα(cid:107)2

M ∀α ∈ Rn ,

(28)

where γ ∈ (0, 1] is the aggregation parameter for MOCHA Algorithm. Note that in Algorithm 1 we
have assumed that γ = 1. Based on Remark 1, it can be seen that the matrix M in Assumption 1 can
be chosen of the form M = ¯M ⊗ Id×d, where ¯M is a positive deﬁnite matrix of size m × m. For
such a matrix, the following lemma shows how to choose σ(cid:48).
Lemma 9. For any positive deﬁnite matrix M = ¯M ⊗ Id×d,
| ¯Mtt(cid:48)|
¯Mtt

σ(cid:48) := γ max

m
(cid:88)

(29)

t

satisﬁes the inequality (28).

t(cid:48)=1

16

Proof. First of all it is worth noting that for any t, Mt = ¯Mt ⊗ Id×d. For any α ∈ Rn

γ(cid:107)Xα(cid:107)2

M = γ

¯Mtt(cid:48)(cid:104)Xtαt, Xt(cid:48)αt(cid:48)(cid:105)

(cid:88)

t,t(cid:48)

(cid:88)

t,t(cid:48)

1
2

(cid:32)

≤ γ

| ¯Mtt(cid:48)|

(cid:107)Xtαt(cid:107)2

Mt

+

(cid:107)Xt(cid:48)αt(cid:48)(cid:107)2

Mt(cid:48)

1
¯Mt(cid:48)t(cid:48)

(cid:19)

(cid:18) 1
¯Mtt
(cid:33)

(cid:88)

(cid:88)

= γ

t

t(cid:48)

| ¯Mtt(cid:48)|
¯Mtt

≤ σ(cid:48) (cid:88)

(cid:107)Xtαt(cid:107)2

,

Mt

t

(cid:107)Xtαt(cid:107)2

Mt

where the ﬁrst inequality is due to Cauchy-Schwartz and the second inequality is due to deﬁnition of
σ(cid:48).

Remark 5. Based on the proof of Lemma 9, it is easy to see that we can choose σ(cid:48) differently across
the tasks in our algorithm to allow tasks that are more loosely correlated with other tasks to update
more aggressively. To be more speciﬁc, if we choose σ(cid:48)
, then it it is possible to show
that γ(cid:107)Xα(cid:107)2
for any α, and the rest of the convergence proofs will follow.

t = γ (cid:80)

| ¯Mtt(cid:48) |
¯Mtt

t(cid:48)

M ≤ (cid:80)m

t=1 σ(cid:48)

t(cid:107)Xtαt(cid:107)2

Mt

D.1 The Role of Aggregation Parameter γ

The following remark highlights the role of aggregation parameter γ.
Remark 6. Note that the when γ < 1 the chosen σ(cid:48) in (28) would be smaller compared to the case
where γ = 1. This means that the local subproblems would be solved with less restrictive regularizer.
Therefore, the resulting ∆α would be more aggressive. As a result, we need to do a more conservative
update α + γ∆α in order to guarantee the convergence.

Although aggregation parameter γ is proposed to capture this trade off between aggressive subprob-
lems and conservative updates, in most practical scenarios γ = 1 has the best empirical performance.

E Simulation Details

E.1 Datasets

In this section, we provide additional details and results of our empirical study.

In Table 2, we provide details on the number of tasks (m), feature size (d), and per-task data size
(nt) for each federated dataset described in Section 5. The standard deviation nσ is a measure data
skew, and calculates the deviation in the sizes of training data points for each task, nt. All datasets
are publicly available.

Table 2: Federated Datasets for Empirical Study.

Dataset

Tasks (m)

Features (d) Min nt Max nt

Human Activity

Google Glass

Vehicle Sensor

30

38

23

561

180

100

210

524

872

306

581

1,933

Std. Deviation nσ
26.75

11.07

267.47

E.2 Multi-Task Learning with Highly Skewed Data

To generate highly skewed data, we sample from the original training datasets so that the task dataset
sizes differ by at least two orders of magnitude. The sizes of these highly skewed datasets are shown
in Table 3. When looking at the performance of local, global, and multi-task models for these datasets

17

Table 3: Skewed Datasets for Empirical Study.

Dataset

Tasks (m)

Features (d) Min nt Max nt

Std. Deviation σ

HA-Skew

GG-Skew

VS-Skew

30

38

23

561

180

100

3

6

19

306

581

1,933

84.41

111.79

486.08

(Table 4), the global model performs slightly better in this setting (particularly for the Human Activity
dataset). However, multi-task learning still signiﬁcantly outperforms all models.

Table 4: Average prediction error for skewed data: means and standard errors over 10 random shufﬂes.

Model HA-Skew
2.41 (0.30)
Global

GG-Skew
5.38 (0.26)

VS-Skew
13.58 (0.23)

Local

3.87 (0.37)

4.96 (0.20)

8.15 (0.19)

MTL

1.93 (0.44)

3.28 (0.15)

6.91 (0.21)

In this section, we provide details on our experimental setup and compared methods.

E.3 Implementation Details

Methods.

• Mb-SGD. Mini-batch stochastic gradient descent is a standard, widely used method for parallel and
distributed optimization. See, e.g., a discussion of this method for the SVM models of interest [46].
We tune both the mini-batch size and step size for best performance using grid search.

• Mb-SDCA. Mini-batch SDCA aims to improve mini-batch SGD by employing coordinate ascent
in the dual, which has encouraging theoretical and practical backings [47, 50]. For all experiments,
we scale the updates for mini-batch stochastic dual coordinate ascent at each round by β
b for
mini-batch size b and β ∈ [1, b], and tune both parameters with grid search.

• COCOA. We generalize COCOA [22, 31] to solve (1), and tune θ, the ﬁxed approximation
parameter, between [0, 1) via grid search. For both COCOA, and MOCHA, we use coordinate
ascent as a local solver for the dual subproblems (4).

• MOCHA. The only parameter necessary to tune for MOCHA is the level of approximation quality
θh
t , which can be directly tuned via Hi, the number of local iterations of the iterative method run
locally. In Section 4, our theory relates this parameter to global convergence, and we discuss the
practical effects of this parameter in Section 3.4.

Computation and Communication Complexities. We provide a brief summary of the above
methods from the point of view of computation, communication, and memory complexities. MOCHA
is superior in terms of its computation complexity compared to other distributed optimization methods,
as MOCHA allows for ﬂexibility in its update of W. At one extreme, the update can be based on a
single data point per iteration in parallel, similar to parallel SGD. At the other extreme, MOCHA
can completely solve the subproblems on each machine, similar to methods such as ADMM. This
ﬂexibility of computation yields direct beneﬁts in terms of communication complexity, as performing
additional local computation will result in fewer communication steps. Note that all methods,
including MOCHA, communicate the same size vector at each iteration, and so the main difference is
in how many communication rounds are necessary for convergence. In terms of memory, MOCHA
must maintain the task matrix, Ω, on the master server. While this overhead is greater than most non-
MTL (global or local) approaches, the task matrix is typically low-rank by design and the overhead is
thus manageable. We discuss methods for computing Ω in further detail in Section B.3.

18

Estimated Time. To estimate the time to run methods in the federated setting, we carefully count
the ﬂoating-point operations (FLOPs) performed in each local iteration for each method, as well as the
size and frequency of communication. We convert these counts to estimated time (in milliseconds),
using known clock rate and bandwidth/latency numbers for mobile phones in 3G, LTE, and wireless
networks [52, 20, 48, 9, 38]. In particular, we use the following standard model for the cost of one
round, h, of local computation / communication on a node t:

Time(h, t) :=

+ Comm(h, t)

(30)

FLOPs(h, t)
Clock Rate(t)

Note that the communication cost Comm(h, t) includes both bandwidth and latency measures. De-
tailed models of this type have been used to closely match the performance of real-world systems [40].

Statistical Heterogeneity. To account for statistical heterogeneity, MOCHA and the mini-batch
methods (Mb-SGD and Mb-SDCA) can adjust the number of local iterations or batch size, respectively,
to account for difﬁcult local problems or high data skew. However, because COCOA uses a ﬁxed
accuracy parameter θ across both the tasks and rounds, changes in the subproblem difﬁculty and
data skew can make the computation on some nodes much slower than on others. For COCOA, we
compute θ via the duality gap, and carefully tune this parameter between [0, 1) for best performance.
Despite this, the number of local iterations needed for θ varies signiﬁcantly across nodes, and as the
method runs, the iterations tend to increase as the subproblems become more difﬁcult.

Systems Heterogeneity. Beyond statistical heterogeneity, there can be variability in the systems
themselves that cause changes in performance. For example, low battery levels, poor network
connections, or low memory may reduce the ability a solver has on a local node to compute updates.
As discussed in Section 3.4, MOCHA assumes that the central node sets some global clock cycle, and
the t-th worker determines the amount of feasible local computation given this clock cycle along with
its systems constraints. This speciﬁed amount of local computation corresponds to some implicit
value of θh
t based on the underlying systems and statistical challenges for the t-th node.
To model this setup in our simulations, it sufﬁces to ﬁx a global clock sycle and then randomly
assign various amounts of local computation to each local node at each iteration. Speciﬁcally, in
our simulations we charge all nodes the same ﬁxed computation cost at each iteration over an LTE
network, but force some nodes to perform less updates given their current systems constraints. At
each round, we assign the number of updates for node t between [0.1nmin, nmin] for high variability
environments, and between [0.9nmin, nmin] for low variability environments, where nmin := mint nt
is the minimum number of local data points across tasks.

For the mini-batch methods, we vary the mini-batch size in a similar fashion. However, we do not
follow this same process for COCOA, as this would require making the θ parameter worse than what
was optimally tuned given statistical heterogeneity. Hence, in these simulations we do not introduce
any additional variability (and thus present overly optimistic results for COCOA). In spite of this, we
see that in both low and high variability settings, MOCHA signiﬁcantly outperforms all other methods
and is robust to systems-related heterogeneity.

Fault Tolerance. Finally, we demonstrate that MOCHA can handle nodes periodically dropping
out, which is also supported in our convergence results in Section 4. We perform this simulation
using the notation deﬁned in Assumption 2, i.e., that each node t temporarily drops on iteration h
with probability ph
t . In our simulations, we modify this probability directly and show that MOCHA is
robust to fault tolerance in Figure 3. However, note that this robustness is not merely due to statistical
redundancy: If we are to drop out a node entirely (as shown in the green dotted line), MOCHA will
not converge to the correct solution. This provides insight into our Assumption 2, which requires that
the probability that a node drops at each round cannot be exactly equal to one.

19

