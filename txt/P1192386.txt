On Tree-Based Neural Sentence Modeling

Haoyue Shi †,‡,∗

Hao Zhou‡

Jiaze Chen‡

Lei Li‡

†: School of EECS, Peking University, Beijing, China
hyshi@pku.edu.cn
‡: ByteDance AI Lab, Beijing, China
{zhouhao.nlp, chenjiaze, lileilab}@bytedance.com

8
1
0
2
 
g
u
A
 
9
2
 
 
]
L
C
.
s
c
[
 
 
1
v
4
4
6
9
0
.
8
0
8
1
:
v
i
X
r
a

Abstract

Neural networks with tree-based sentence en-
coders have shown better results on many
downstream tasks. Most of existing tree-based
encoders adopt syntactic parsing trees as the
explicit structure prior. To study the effec-
tiveness of different tree structures, we re-
place the parsing trees with trivial trees (i.e.,
binary balanced tree, left-branching tree and
right-branching tree) in the encoders. Though
trivial trees contain no syntactic information,
those encoders get competitive or even better
results on all of the ten downstream tasks we
investigated. This surprising result indicates
that explicit syntax guidance may not be the
main contributor to the superior performances
of tree-based neural sentence modeling. Fur-
ther analysis show that tree modeling gives
better results when crucial words are closer
to the ﬁnal representation. Additional experi-
ments give more clues on how to design an ef-
fective tree-based encoder. Our code is open-
source and available at https://github.
com/ExplorerFreda/TreeEnc.

1

Introduction

Sentence modeling is a crucial problem in natural
language processing (NLP). Recurrent neural net-
works with long short term memory (Hochreiter
and Schmidhuber, 1997) or gated recurrent units
(Cho et al., 2014) are commonly used sentence
modeling approaches. These models embed sen-
tences into a vector space and the resulting vectors
can be used for classiﬁcation or sequence genera-
tion in the downstream tasks.

In addition to the plain sequence of hidden
units, recent work on sequence modeling proposes
to impose tree structure in the encoder (Socher
et al., 2013; Tai et al., 2015; Zhu et al., 2015).

∗ Now at Toyota Technological Institute at Chicago,
freda@ttic.edu. This work was done when HS was an
intern researcher at ByteDance AI Lab.

These tree-based LSTMs introduce syntax tree as
an intuitive structure prior for sentence modeling.
They have already obtained promising results in
many NLP tasks, such as natural language infer-
ence (Bowman et al., 2016; Chen et al., 2017c) and
machine translation (Eriguchi et al., 2016; Chen
et al., 2017a,b; Zhou et al., 2017). Li et al. (2015)
empirically concludes that syntax tree-based sen-
tence modeling are effective for tasks requiring
relative long-term context features.

On the other hand, some works propose to
abandon the syntax tree but to adopt the latent
tree for sentence modeling (Choi et al., 2018;
Yogatama et al., 2017; Maillard et al., 2017;
Williams et al., 2018). Such latent trees are di-
rectly learned from the downstream task with re-
inforcement learning (Williams, 1992) or Gum-
bel Softmax (Jang et al., 2017; Maddison et al.,
2017). However, Williams et al. (2018) empiri-
cally show that, Gumbel softmax produces unsta-
ble latent trees with the same hyper-parameters
but different initializations, while reinforcement
learning (Williams et al., 2018) even tends to gen-
erate left-branching trees. Neither gives meaning-
ful latent trees in syntax, but each method still ob-
tains considerable improvements in performance.
This indicates that syntax may not be the main
contributor to the performance gains.

With the above observation, we bring up the fol-
lowing questions: What does matter in tree-based
sentence modeling? If tree structures are neces-
sary in encoding the sentences, what mostly con-
tributes to the improvement in downstream tasks?
We attempt to investigate the driving force of the
improvement by latent trees without syntax.

In this paper, we empirically study the effec-
tiveness of tree structures in sentence modeling.
We compare the performance of bi-LSTM and ﬁve
tree LSTM encoders with different tree layouts,
including the syntax tree, latent tree (from Gum-

bel softmax) and three kinds of designed trivial
trees (binary balance tree, left-branching tree and
right-branching tree). Experiments are conducted
on 10 different tasks, which are grouped into three
categories, namely the single sentence classiﬁca-
tion (5 tasks), sentence relation classiﬁcation (2
tasks), and sentence generation (3 tasks). These
tasks depend on different granularities of features,
and the comparison among them can help us learn
more about the results. We repeat all the exper-
iments 5 times and take the average to avoid the
instability caused by random initialization of deep
learning models.

We get the following conclusions:
• Tree structures are helpful to sentence mod-
eling on classiﬁcation tasks, especially for
tasks which need global (long-term) context
features, which is consistent with previous
ﬁndings (Li et al., 2015).

• Trivial trees outperform syntactic trees, indi-
cating that syntax may not be the main con-
tributor to the gains of tree encoding, at least
on the ten tasks we investigate.

• Further experiments shows that, given strong
priors, tree based methods give better results
when crucial words are closer to the ﬁnal rep-
If structure priors are unavail-
resentation.
able, balanced tree is a good choice, as it
makes the path distances between word and
sentence encoding to be roughly equal, and
in such case, tree encoding can learn the cru-
cial words itself more easily.

2 Experimental Framework

We show the applied encoder-classiﬁer/decoder
framework for each group of tasks in Figure 1.
Our framework has two main components: the en-
coder part and the classiﬁer/decoder part. In gen-
eral, models encode a sentence to a length-ﬁxed
vector, and then applies the vector as the feature
for classiﬁcation and generation.

We ﬁx the structure of the classiﬁer/decoder,
and propose to use ﬁve different types of tree
structures for the encoder part including:

• Parsing tree. We apply binary constituency
tree as the representative, which is widely
used in natural language inference (Bow-
man et al., 2016) and machine translation
(Eriguchi et al., 2016; Chen et al., 2017a).
Dependency parsing trees (Zhou et al., 2015,
2016a) are not considered in this paper.

(a)
framework for
generation.

Encoder-decoder
sentence

(b)
framework for
classiﬁcation.

Encoder-classiﬁer
sentence

(c) Siamese encoder-classiﬁer framework for
sentence relation classiﬁcation.

Figure 1: The encoder-classiﬁer/decoder frame-
work for three different groups of tasks. We ap-
ply multi-layer perceptron (MLP) for classiﬁca-
tion, and left-to-right decoders for generation in
all experiments.

• Binary balanced tree. To construct a binary
balanced tree, we recursively divide a group
of n leafs into two contiguous groups with
the size of (cid:100) n
2 (cid:101) and (cid:98) n
2 (cid:99), until each group has
only one leaf node left.

• Gumbel

trees, which are produced by
straight-forward Gumbel softmax models
(Choi et al., 2018). Note that Gumbel trees
are not stable to sentences (Williams et al.,
2018), and we only draw a sample among all
of them.

• Left-branching trees. We combine two
nodes from left to right, to construct a left-
branching tree, which is similar to those

(a) Parsing tree.

(b) Balanced tree.

(c) Gumbel tree.

(d) Left-branching tree. (e) Right-branching tree.

Figure 2: Examples of different tree structures for the encoder part.

generated by the reinforce based RL-SPINN
model (Williams et al., 2018).

• Right-branching trees.

In contrast to left-
branching ones, nodes are combined from
right to left to form a right-branching tree.
We show an intuitive view of the ﬁve types of tree
structures in Figure 2. In addition, existing works
(Choi et al., 2018; Williams et al., 2018) show
that using hidden states of bidirectional RNNs as
leaf node representations (bi-leaf-RNN) instead of
word embeddings may improve the performance
of tree LSTMs, as leaf RNNs help encode context
information more completely. Our framework also
support leaf RNNs for tree LSTMs.

3 Description of Investigated Tasks

We conduct experiments on 10 different tasks,
which are grouped into 3 categories, namely the
single sentence classiﬁcation (5 tasks), sentence
relation classiﬁcation (2 tasks), and sentence gen-
eration (3 tasks). Each of the tasks is compat-
ible to the encoder-classiﬁer/decoder framework
shown in Figure 1. These tasks cover a wide range
of NLP applications, and depend on different gran-
ularities of features.

Note that the datasets may use articles or para-
graphs as instances, some of which consist of only
one sentence. For each dataset, we only pick the
subset of single-sentence instances for our experi-
ments, and the detailed meta-data is in Table 1.

3.1 Sentence Classiﬁcation

First, we introduce four text classiﬁcation datasets
from Zhang et al. (2015), including AG’s News,
Amazon Review Polarity , Amazon Review Full
and DBpedia. Additionally, noticing that parsing
tree was shown to be effective (Li et al., 2015) on
the task of word-level semantic relation classiﬁ-
cation (Hendrickx et al., 2009), we also add this
dataset to our selections.

AG’s News (AGN). Each sample in this dataset
is an article, associated with a label indicating its
topic: world, sports, business or sci/tech.

Amazon Review Polarity (ARP). The Ama-
zon Review dataset is obtained from the Stanford
Network Analysis Project (SNAP; McAuley and
Leskovec, 2013).
It collects a large amount of
product reviews as paragraphs, associated with a
star rate from 1 (most negative) to 5 (most posi-
tive). In this dataset, 3-star reviews are dropped,
while others are classiﬁed into two groups: posi-
tive (4 or 5 stars) and negative (1 or 2 stars).

Amazon Review Full (ARF). Similar to the
ARP dataset, the ARF dataset is also collected
from Amazon product reviews. Labels in this
dataset are integers from 1 to 5.

DBpedia. DBpedia is a crowd-sourced commu-
nity effort to extract structured information from
Wikipedia (Lehmann et al., 2015). Zhang et al.
(2015) select 14 non-overlapping classes from
DBpedia 2014 to construct this dataset. Each
sample is given by the title and abstract of the
Wikipedia article, associated with the class label.

Relation

Semantic

(WSR)
Word-Level
SemEval-2010 Task 8 (Hendrickx et al., 2009) is
to ﬁnd semantic relationships between pairs of
nominals. Each sample is given by a sentence,
of which two nominals are explicitly indicated,
associated with manually labeled semantic rela-
tion between the two nominals. For example, the
sentence “My [apartment]e1 has a pretty large
[kitchen]e2 .” has the label component-whole(e2,
e1). Different from retrieving the path between
two labels (Li et al., 2015; Socher et al., 2013), we
feed the entire sentence together with the nominal
indicators (i.e., tags of e1 and e2) as words to the
framework. We also ignore the order of e1 and e2
in the labels given by the dataset. Thus, this task
turns to be a 10-way classiﬁcation one.

3.2 Sentence Relation Classiﬁcation

To evaluate how well a model can capture seman-
tic relation between sentences, we introduce the
second group of tasks: sentence relation classiﬁ-
cation.

Natural Language Inference (NLI). The Stan-
ford Natural Language Inference (SNLI) Corpus
(Bowman et al., 2015) is a challenging dataset
for sentence-level textual entailment. It has 550K
training sentence pairs, as well as 10K for devel-
opment and 10K for test. Each pair consists of two
relative sentences, associated with a label which is
one of entailment, contradiction and neutral.

Conjunction Prediction (Conj).
Information
about the coherence relation between two sen-
tences is sometimes apparent in the text explicitly
this is the case when-
(Miltsakaki et al., 2004):
ever the second sentence starts with a conjunction
phrase. Jernite et al. (2017) propose a method to
create conjunction prediction dataset from unla-
beled corpus. They create a list of phrases, which
can be classiﬁed into nine types, as conjunction in-
dicators. The object of this task is to recover the
conjunction type of given two sentences, which
can be used to evaluate how well a model captures
the semantic meaning of sentences. We apply the
method proposed by Jernite et al. (2017) on the
Wikipedia corpus to create our conj dataset.

3.3 Sentence Generation

We also include the sentence generation tasks in
our experiments, to investigate the representation
ability of different encoders over global (long-
term) context features. Note that our framework is
based on encoding, which is different from those
attention based approaches.

Paraphrasing (Para). Quora Question Pair
Dataset is a widely applied dataset to evaluate
paraphrasing models (Wang et al., 2017; Li et al.,
2017b). 1 In this work, we treat the paraphrasing
task as a sequence-to-sequence one, and evaluate
on it with our sentence generation framework.

Machine Translation (MT). Machine transla-
tion, especially cross-language-family machine
translation,
is a complex task, which requires
models to capture the semantic meanings of sen-
tences well. We apply a large challenging English-
Chinese sentence translation task for this inves-
tigation, which is adopted by a variety of neural
translation work (Tu et al., 2016; Li et al., 2017a;
Chen et al., 2017a). We extract the parallel data

1https://data.quora.com/

First-Quora-Dataset-Release-Question-Pairs

Dataset

#Sentence
Dev

Train

Test

#Cls Avg.
Len

Sentence Classiﬁcation

News
ARP
ARF
DBpedia
WSR

60K 6.7K 4.3K
128K 14K 16K
110K 12K 27K
106K 11K 15K
2.7K
891
7.1K

4
2
5
14
10

31.5
33.7
33.8
20.1
23.1

Sentence Relation

SNLI
Conj

550K 10K 10K
552K 10K 10K

3
9

11.2
23.3

Sentence Generation

Para
MT
AE

2K

98K
3K N/A 10.2
1.2M 20K 80K N/A 34.1
1.2M 20K 80K N/A 34.1

Table 1: Meta-data of the downstream tasks we
investigated. For each task, we list the quantity of
instances in train/dev/test set, the average length
(by words) of sentences (source sentence only for
generation task), as well as the number of classes
if applicable.

from the LDC corpora,2 selecting 1.2M from them
as our training set, 20K and 80K of them as our
development set and test set, respectively.

Auto-Encoding (AE). We extract the English
part of the machine translation dataset to form a
auto-encoding task, which is also compatible with
our encoder-decoder framework.

4 Experiments

In this section, we present our experimental re-
sults and analysis. Section 4.1 introduces our set-
up for all the experiments. Section 4.2 shows
the main results and analysis on ten downstream
tasks grouped into three classes, which can cover
a wide range of NLP applications. Regarding that
trivial tree based LSTMs perform the best among
all models, we draw two hypotheses, which are
i) right-branching tree beneﬁts a lot from strong
structural priors; ii) balanced tree wins because
it fairly treats all words so that crucial informa-
tion could be more easily learned by the LSTM
gates automatically. We test the hypotheses in

2The corpora includes LDC2002E18, LDC2003E07,
LDC2004T07,

portion

of

LDC2003E14,
LDC2004T08 and LDC2005T06

Hansards

Model

AGN ARP ARF DBpedia WSR NLI

Sentence Classiﬁcation

Sentence Relation

Conj Para MT

Sentence Generation
AE

Latent Trees

Gumbel

+bi-leaf-RNN

(Constituency) Parsing Trees

91.8
91.8

87.1
88.1

48.4
49.7

98.6
98.7

66.7
69.2

80.4
82.9

51.2
53.7

20.4
20.5

17.4
22.3

39.5
75.3

Parsing

+bi-leaf-RNN

91.9
92.0

87.5
88.0

49.4
49.6

98.8
98.8

66.6
68.6

81.3
82.8

52.4
53.4

19.9
20.4

19.1
22.2

44.3
72.9

Trivial Trees

Balanced

+bi-leaf-RNN
Left-branching
+bi-leaf-RNN
Right-branching
+bi-leaf-RNN

Linear Structures

LSTM

+bidirectional

Avg. Length

92.0
92.1
91.9
91.2
91.9
91.9

91.7
91.7

31.5

87.7
87.8
87.6
87.6
87.7
87.9

87.8
87.8

33.7

49.1
49.7
48.5
48.9
49.0
49.4

48.8
49.2

33.8

98.7
98.8
98.7
98.6
98.8
98.7

98.6
98.7

20.1

66.2
69.6
67.8
67.7
68.6
68.7

81.1
82.6
81.3
82.8
81.0
82.8

66.1
67.4

82.6
82.8

23.1

11.2

52.1
54.0
50.9
53.3
51.3
53.5

52.8
53.3

23.3

19.7
20.5
19.9
20.6
20.4
20.9

19.0
22.3
19.2
21.6
19.7
23.1

49.4
76.0
48.0
72.9
54.7
80.4

20.3
20.2

19.1
21.3

46.9
67.0

10.2

34.1

34.1

Table 2: Test results for different encoder architectures trained by a uniﬁed encoder-classiﬁer/decoder
framework. We report accuracy (×100) for classiﬁcation tasks, and BLEU score (Papineni et al., 2002;
word-level for English targets and char-level for Chinese targets) for generation tasks. Large is better for
both of the metrics. The best number(s) for each task are in bold. In addition, average sentence length
(in words) of each dataset is attached in the last row with underline.

Section 4.3. Finally, we compare the performance
of linear and tree LSTMs with three widely ap-
plied pooling mechanisms in Section 4.4.

4.1 Set-up

In experiments, we ﬁx the structure of the clas-
siﬁer as a two-layer MLP with ReLU activation,
and the structure of decoder as GRU-based recur-
3 The
rent neural networks (Cho et al., 2014).
hidden-layer size of MLP is ﬁxed to 1024, while
that of GRU is adapted from the size of sentence
encoding. We initialize the word embeddings with
300-dimensional GloVe (Pennington et al., 2014)
vectors.4 We apply 300-dimensional bidirectional
(600-dimensional in total) LSTM as leaf RNN
when necessary. We use Adam (Kingma and Ba,
2015) optimizer to train all the models, with the
learning rate of 1e-3 and batch size of 64. In the

3We observe that ReLU can signiﬁcantly boost the perfor-

mance of Bi-LSTM on SNLI.

training stage, we drop the samples with the length
of either source sentence or target sentence larger
than 64. We do not apply any regularization or
dropout term in all experiments except the task of
WSR, on which we tune dropout term with respect
to the development set. We generate the binary
parsing tree for the datasets without parsing trees
using ZPar (Zhang and Clark, 2011).5 More de-
tails are summarized in supplementary materials.

4.2 Main Results

In this subsection, we aim to compare the results
from different encoders. We do not include any
attention (Wang et al., 2016; Lin et al., 2017)
or pooling (Collobert and Weston, 2008; Socher
et al., 2011; Zhou et al., 2016b) mechanism here,
in order to avoid distractions and make the encoder
structure affects the most. We will further analyze
pooling mechanisms in Section 4.4.

Table 2 presents the performances of different

4http://nlp.stanford.edu/data/glove.

5https://www.sutd.edu.sg/cmsresource/

840B.300d.zip

faculty/yuezhang/zpar.html

encoders on a variety of downstream tasks, which
lead to the following observations:

this section, we analyze why these trees achieve
high scores in deep.

Tree encoders are useful on some tasks. We
get the same conclusion with Li et al. (2015)
that tree-based encoders perform better on tasks
requiring long-term context features. Despit-
ing the linear structured left-branching and right-
branching tree encoders, we ﬁnd that, tree-based
encoders generally perform better than Bi-LSTMs
on tasks of sentence relation and sentence genera-
tion, which may require relatively more long term
context features for obtaining better performances.
However, the improvements of tree encoders on
NLI and Para are relatively small, which may
be caused by that sentences of the two tasks are
shorter than others, and the tree encoder does not
get enough advantages to capture long-term con-
text in short sentences.

Trivial tree encoders outperform other en-
coders. Surprisingly, binary balanced tree en-
coder gets the best results on most tasks of clas-
siﬁcation and right-branching tree encoder tends
to be the best on sentence generation. Note that
binary balanced tree and right-branching tree are
only trivial tree structures, but outperform syntac-
tic tree and latent tree encoders. The latent tree
is really competitive on some tasks, as its struc-
ture is directly tuned by the corresponding tasks.
However, it only beats the binary balanced tree by
very small margins on NLI and ARP. We will give
analysis about this in Section 4.3.

Larger quantity of parameters is not the only
reason of the improvements. Table 2 shows
that tree encoders beneﬁt a lot from adding leaf-
LSTM, which brings not only sentence level in-
formation to leaf nodes, but also more parame-
ters than the bi-LSTM encoder. However, left-
branching tree LSTM has a quite similar struc-
ture with linear LSTM, and it can be viewed as
It has the
a linear LSTM-on-LSTM structure.
same amounts of parameters as other tree-based
encoders, but still falls behind the balance tree en-
coder on most of the tasks. This indicates that
larger quantity of parameters is at least not the
only reason for binary balance tree LSTM en-
coders to gain improvements against bi-LSTMs.

4.3 Why Trivial Trees Work Better?

Binary balanced tree and right-branching are triv-
ial ones, hardly containing syntax information. In

4.3.1 Right Branching Tree Beneﬁts from

Strong Structural Prior

We argue that right-branching trees beneﬁt from
its strong structural prior. In sentence generation
tasks, models generate sentences from left to right,
which makes words in the left of the source sen-
tence more important (Sutskever et al., 2014). If
the encoder fails to memorize the left words, the
information about right words would not help due
to the error propagation. In right-branching trees,
left words of the sentence are closer to the ﬁnal
representation, which makes the left words are
more easy to be memorized, and we call this struc-
ture prior. Oppositely, in the case of left-branching
trees, right words of the sentence are closer to the
representation.

To validate our hypothesis, we propose to vi-
sualize the Jacobian as word-level saliency (Shi
et al., 2018), which can be viewed as the contri-
bution of each word to the sentence encoding:

J(s, w) = (cid:107)∇s(w)(cid:107)1 =

(cid:88)

i,j

|

∂si
∂wj

|

where s = (s1, s2, · · · , sp)T denotes the embed-
ding of a sentence, and w = (w1, w2, · · · , wq)T
denotes embedding of a word. We can compute
the saliency score using backward propagation.
For a word in a sentence, higher saliency score
means more contribution to sentence encoding.

We present the visualization in Figure 3 using
It
the visualization tool from Lin et al. (2017).
shows that right-branching tree LSTM encoders
tend to look at the left part of the sentence, which
is very helpful to the ﬁnal generation performance,
as left words are more crucial. Balanced trees also
have this feature and we think it is because balance
tree treats these words fairly, and crucial informa-
tion could be more easily learned by the LSTM
gates automatically.

However, bi-LSTM and left-branching tree
LSTM also pay much attention to words in the
right (especially the last two words), which maybe
caused by the short path from the right words to
the root representation, in the two corresponding
tree structures.

Additionally, Table 3 shows that models trained
with the same hyper-parameters but different ini-
tializations have strong agreement with each other.

(a) Balanced tree, MT.

(b) Left-branching tree, MT.

(c) Right-branching, MT.

(d) Bi-LSTM, MT.

(e) Balanced tree, AE.

(f) Left-branching tree, AE.

(g) Right-branching, AE.

(h) Bi-LSTM, AE.

Figure 3: Saliency visualization of words in learned MT and AE models. Darker means more important
to the sentence encoding.

Model

Balanced (BiLRNN)
Left-Branching (BiLRNN)
Right-Branching (BiLRNN)
Bi-LSTM

MT

93.1
94.2
92.3
96.4

AE

96.9
95.4
95.1
96.1

Table 3:
Mean average Pearson correlation
(×100) across ﬁve models trained with same
hyper-parameters. For each testing sentence, we
compute the saliency scores of words. Cross-
model Pearson correlation can show the agreement
of two models on one sentence, and average Pear-
son correlation is computed through all sentences.
We report mean average Pearson correlation of the
5 × 4 model pairs.

Thus, “looking at the ﬁrst words” is a stable be-
havior of balanced and right-branching tree LSTM
encoders in sentence generation tasks. So is “look-
ing at the ﬁrst and the last words” for Bi-LSTMs
and left-branching tree LSTMs.

4.3.2 Binary Balanced Tree Beneﬁts from

Shallowness

Compared to syntactic and latent trees, the only
advantage of balanced tree we can hypothesize is
that, it is shallower and more balanced than others.
Shallowness may lead to shorter path for informa-
tion propagation from leafs to the root representa-
tion, and makes the representation learning more
easy due to the reduction of errors in the propaga-
tion process. Balance makes the tree fairly treats

(a) ρ-depth line for WSR.

(b) ρ-Acc. line for WSR.

(c) ρ-depth line for MT.

(d) ρ-BLEU line for MT.

(e) ρ-depth line for AE.

(f) ρ-BLEU line for AE.

ρ-depth and ρ-performance lines for
Figure 4:
three tasks. There is a trend that the depth drops
and the performance raises with the growth of ρ.

all leaf nodes, which makes it more easily to au-
tomatically select the crucial information over all
words in a sentence.

To test our hypothesis, we conduct the follow-
ing experiments. We select three tasks, on which
binary balanced tree encoder wins Bi-LSTMs with
a large margin (WSR, MT and AE). We gener-
ate random binary trees for sentences, while con-

(a) Length-Accuracy lines for WSR.

(b) Length-BLEU lines for MT.

(c) Length-BLEU lines for AE.

Figure 5: Length-performance lines for the further investigated tasks. We divide test instances into
several groups by length, and report the performance on each group respectively. Sentences with length
in [1, 8] are put to the ﬁrst group, and the group i(i ≥ 2) covers the range of [4i + 1, 4i + 4] in length. ]

trolling the depth using a hyper-parameter ρ. We
start by a group with all words (nodes) in the sen-
tence. At each time, we separate n nodes to two
continuous groups sized ((cid:100) n
2 (cid:99)) with proba-
bility ρ, while those sized (n − 1, 1) with prob-
ability 1 − ρ. Trees generated with ρ = 0 are
exactly left-branching trees, and those generated
with ρ = 1 are binary balanced trees. The ex-
pected node depth of the tree turns smaller with ρ
varies from 0 to 1.

2 (cid:101), (cid:98) n

Figure 4 shows that, in general, trees with shal-
lower node depth have better performance on all
of the three tasks (for binary tree, shallower also
means more balanced), which validates our above
hypothesis that binary balanced tree gains the re-
ward from its shallow and balanced structures.

Additionally, Figure 5 demonstrates that bi-
nary balanced trees work especially better with
relative long sentences. As desired, on short-
sentence groups,
the performance gap between
Bi-LSTM and binary balanced tree LSTM is not
obvious, while it grows with the test sentences
turning longer. This explains why tree-based en-
coder gives small improvements on NLI and Para,
because sentences on these two tasks are much
shorter than others.

4.4 Can Pooling Replace Tree Encoder?

Max pooling (Collobert and Weston, 2008; Zhao
et al., 2015), mean pooling (Conneau et al., 2017)
and self-attentive pooling (also known as self-
attention; Santos et al., 2016; Liu et al., 2016; Lin
et al., 2017) are three popular and efﬁcient choices
to improve sentence encoding. In this part, we will
compare the performance of tree LSTMs and bi-
LSTM on the tasks of WSR, MT and AE, with
each pooling mechanism respectively, aiming to
demonstrate the role that pooling plays in sentence

(a) Balanced tree.

(b) Bi-LSTM.

Figure 6: An illustration of the investigated self-
attentive pooling mechanism.

modeling, and validate whether tree encoders can
be replaced by pooling.

As shown in Figure 6, for linear LSTMs, we
apply pooling mechanism to all hidden states; as
for tree LSTMs, pooling is applied to all hidden
states and leaf states of tree LSTMs. Implementa-
tion details are summarized in the supplementary
materials.

Table 4 shows that max and attentive pooling
improve all the structures on the task of WSR, but
all the pooling mechanisms fail on MT and AE
that require the encoding to capture complete in-
formation of sentences, while pooling mechanism
may cause the loss of information through the pro-
cedure. The result indicates that, though pooling
mechanism is efﬁcient on some tasks, it cannot
totally gain the advantages brought by tree struc-
tures. Additionally, we think the attention mech-

Model

Bi-LSTM

+max-pooling
+mean-pooling
+self-attention

Parsing (BiLRNN)

+max-pooling
+mean-pooling
+self-attention

Balanced (BiLRNN)
+max-pooling
+mean-pooling
+self-attention

Left (BiLRNN)

+max-pooling
+mean-pooling
+self-attention

Right (BiLRNN)

+max-pooling
+mean-pooling
+self-attention

WSR

MT

AE

67.4
71.8 ↑
64.3 ↓
72.5 ↑

68.6
69.7 ↑
58.0 ↓
72.2 ↑

69.6
70.6 ↑
54.1 ↓
72.5 ↑

67.7
71.2 ↑
67.3 ↓
72.1 ↑

68.7
71.6 ↑
67.2 ↓
72.4 ↑

21.3
21.6 ↑
21.8 ↑
21.2 ↓

22.2
21.8 ↓
21.2 ↓
21.5 ↓

22.3
21.6 ↓
21.3 ↓
21.6 ↓

21.6
20.5 ↓
21.4 ↓
21.6 –

23.1
21.6 ↓
22.1 ↓
21.6 ↓

67.0
48.0 ↓
47.8 ↓
60.4 ↓

72.9
48.3 ↓
50.7 ↓
69.1↓

76.0
48.5 ↓
52.7 ↓
69.5 ↓

72.9
47.6 ↓
51.8 ↓
70.2 ↓

80.4
48.4 ↓
53.9 ↓
68.9 ↓

Table 4: Performance of tree and linear-structured
encoders with or without pooling, on the selected
three tasks. We report accuracy (×100), char-level
BLEU for MT and word-level BLEU for AE. All
of the tree models have bidirectional leaf RNNs
(BiLRNN). The best number(s) for each task are
in bold. The top and down arrows indicate the
increment or decrement of each pooling mecha-
nism, against the baseline of pure tree based en-
coder with the same structure.

anism has the beneﬁts of the balanced tree mod-
eling, which also fairly treat all words and learn
the crucial parts automatically. The path from rep-
resentation to words in attention are even shorter
than the balanced tree. Thus the fact that attentive
pooling outperforms balanced trees on WSR is not
surprising to us.

5 Discussions

Balanced tree for sentence modeling has been
explored by Munkhdalai and Yu (2017) and
Williams et al. (2018) in natural language infer-
ence (NLI). However, Munkhdalai and Yu (2017)
focus on designing inter-attention on trees, instead
of comparing balanced tree with other linguistic
trees in the same setting. Williams et al. (2018) do

compare balanced trees with latent trees, but bal-
anced tree does not outperform the latent one in
their experiments, which is consistent with ours.
We analyze it in Section 4.2 that sentences in NLI
are too short for the balanced tree to show the ad-
vantage.

Levy et al. (2018) argue that LSTM works
for the gates ability to compute an element-wise
weighted sum. In such case, tree LSTM can also
be regarded as a special case of attention, espe-
cially for the balanced-tree modeling, which also
automatically select the crucial information from
all word representation. Kim et al. (2017) pro-
pose a tree structured attention networks, which
combine the beneﬁts of tree modeling and atten-
tion, and the tree structures in their model are also
learned instead of the syntax trees.

Although binary parsing trees do not produce
better numbers than trivial trees on many down-
stream tasks, it is still worth noting that we are
not claiming the useless of parsing trees, which
are intuitively reasonable for human language
understanding. A recent work (Blevins et al.,
2018) shows that RNN sentence encodings di-
rectly learned from downstream tasks can capture
implicit syntax information. Their interesting re-
sult may explain why explicit syntactic guidance
does not work for tree LSTMs. In summary, we
still believe in the potential of linguistic features to
improve neural sentence modeling, and we hope
our investigation could give some sense to after-
wards hypothetical exploring of designing more
effective tree-based encoders.

6 Conclusions

In this work, we propose to empirically investigate
what contributes mostly in the tree-based neural
sentence encoding. We ﬁnd that trivial trees with-
out syntax surprisingly give better results, com-
pared to the syntax tree and the latent tree. Fur-
ther analysis indicates that the balanced tree gains
from its shallow and balance properties compared
to other trees, and right-branching tree beneﬁts
from its strong structural prior under the setting
of left-to-right decoder.

Acknowledgements

We thank Hang Li, Yue Zhang, Lili Mou and
Jiayuan Mao for their helpful comments on this
work, and the anonymous reviewers for their valu-
able feedback.

References

Terra Blevins, Omer Levy, and Luke Zettlemoyer.
2018. Deep RNNs Encode Soft Hierarchical Syn-
tax. In Proc. of ACL.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A Large Anno-
tated Corpus for Learning Natural Language Infer-
ence. In Proc. of EMNLP.

Samuel R. Bowman, Jon Gauthier, Abhinav Ras-
togi, Raghav Gupta, Christopher D. Manning, and
Christopher Potts. 2016. A Fast Uniﬁed Model for
In Proc. of
Parsing and Sentence Understanding.
ACL.

Huadong Chen, Shujian Huang, David Chiang, and Jia-
jun Chen. 2017a. Improved Neural Machine Trans-
lation with a Syntax-Aware Encoder and Decoder.
In Proc. of ACL.

Kehai Chen, Rui Wang, Masao Utiyama, Lemao Liu,
Akihiro Tamura, Eiichiro Sumita, and Tiejun Zhao.
2017b. Neural Machine Translation with Source
Dependency Representation. In Proc. of EMNLP.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017c. Enhanced LSTM
for Natural Language Inference. In Proc. of ACL.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learn-
ing Phrase Representations using RNN Encoder–
Decoder for Statistical Machine Translation.
In
Proc. of EMNLP.

Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2018.
Learning to Compose Task-Speciﬁc Tree Structures.
In Proc. of AAAI.

Ronan Collobert and Jason Weston. 2008. A Uni-
ﬁed Architecture for Natural Language Processing:
Deep Neural Networks with Multitask Learning. In
Proc. of ICML.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc
Barrault, and Antoine Bordes. 2017. Supervised
Learning of Universal Sentence Representations
from Natural Language Inference Data. In Proc. of
EMNLP.

Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa
Tree-to-Sequence Attentional

Tsuruoka. 2016.
Neural Machine Translation. In Proc. of ACL.

Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid ´O S´eaghdha, Sebastian
Pad´o, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2009.
Semeval-2010 Task 8:
Multi-Way Classiﬁcation of Semantic Relations be-
tween Pairs of Nominals. In Proc. of the Workshop
on Semantic Evaluations: Recent Achievements and
Future Directions.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long

Short-Term Memory. Neural Computation.

Eric Jang, Shixiang Gu, and Ben Poole. 2017. Cat-
egorical Reparameterization with Gumbel-Softmax.
In Proc. of ICLR.

Yacine Jernite, Samuel R. Bowman, and David Sontag.
2017. Discourse-based Objectives for Fast Unsu-
pervised Sentence Representation Learning. arXiv
preprint arXiv:1705.00557.

Yoon Kim, Carl Denton, Luong Hoang, and Alexan-
der M Rush. 2017. Structured Attention Networks.
In Proc. of ICLR.

Diederik P Kingma and Jimmy Ba. 2015. Adam: A
In Proc. of

Method for Stochastic Optimization.
ICLR.

Jens Lehmann, Robert

Isele, Max Jakob, Anja
Jentzsch, Dimitris Kontokostas, Pablo N Mendes,
Sebastian Hellmann, Mohamed Morsey, Patrick Van
Kleef, S¨oren Auer, et al. 2015. DBpedia–A Large-
Scale, Multilingual Knowledge Base Extracted from
Wikipedia. Semantic Web.

Omer Levy, Kenton Lee, Nicholas FitzGerald, and
Luke Zettlemoyer. 2018. Long Short-Term Mem-
ory as a Dynamically Computed Element-wise
Weighted Sum. In Proc. of ACL.

Jiwei Li, Thang Luong, Dan Jurafsky, and Eduard
Hovy. 2015. When Are Tree Structures Necessary
for Deep Learning of Representations? In Proc. of
EMNLP.

Junhui Li, Deyi Xiong, Zhaopeng Tu, Muhua Zhu,
Min Zhang, and Guodong Zhou. 2017a. Modeling
Source Syntax for Neural Machine Translation. In
Proc. of ACL.

Zichao Li, Xin Jiang, Lifeng Shang, and Hang
Paraphrase Generation with Deep
preprint

Li. 2017b.
Reinforcement Learning.
arXiv:1711.00279.

arXiv

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A Structured Self-Attentive Sentence
Embedding. In Proc. of ICLR.

Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang.
2016. Learning Natural Language Inference us-
ing Bidirectional LSTM Model and Inner-Attention.
arXiv preprint arXiv:1605.09090.

Chris J Maddison, Andriy Mnih, and Yee Whye Teh.
2017. The Concrete Distribution: A Continuous Re-
laxation of Discrete Random Variables. In Proc. of
ICLR.

Jean Maillard, Stephen Clark, and Dani Yogatama.
Jointly Learning Sentence embeddings and
arXiv

2017.
Syntax with Unsupervised Tree-LSTMs.
preprint arXiv:1705.09189.

Julian McAuley and Jure Leskovec. 2013. Hidden Fac-
tors and Hidden Topics: Understanding Rating Di-
In Proc. of the 7th
mensions with Review Text.
ACM Conference on Recommender Systems.

Adina Williams, Andrew Drozdov, and Samuel R.
Bowman. 2018. Do Latent Tree Learning Models
Identify Meaningful Structure in Sentences? Trans-
action of ACL.

Eleni Miltsakaki, Rashmi Prasad, Aravind K. Joshi,
and Bonnie L Webber. 2004. The Penn Discourse
Treebank. In Proc. of LREC.

Ronald J. Williams. 1992. Simple Statistical Gradient-
Following Algorithms for Connectionist Reinforce-
ment Learning. In Reinforcement Learning.

Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan,
and Zhi Jin. 2016. Natural language inference by
In
tree-based convolution and heuristic matching.
Proc. of ACL.

Dani Yogatama, Phil Blunsom, Chris Dyer, Edward
Grefenstette, and Wang Ling. 2017. Learning to
Compose Words into Sentences with Reinforcement
Learning. Proc. of ICLR.

Tsendsuren Munkhdalai and Hong Yu. 2017. Neural
Tree Indexers for Text Understanding. In Proc. of
EACL.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-Level Convolutional Networks for Text
Classiﬁcation. In Proc. of NIPS.

Yue Zhang and Stephen Clark. 2011. Syntactic Pro-
cessing using the Generalized Perceptron and Beam
Search. Computational Linguistics.

Han Zhao, Zhengdong Lu, and Pascal Poupart. 2015.
In

Self-Adaptive Hierarchical Sentence Model.
Proc. of IJCAI.

Hao Zhou, Zhaopeng Tu, Shujian Huang, Xiaohua Liu,
Hang Li, and Jiajun Chen. 2017. Chunk-Based Bi-
Scale Decoder for Neural Machine Translation. In
Proc. of ACL.

Hao Zhou, Yue Zhang, Shujian Huang, and Jiajun
Chen. 2015. A Neural Probabilistic Structured-
Prediction Model for Transition-based Dependency
Parsing. In Proc. of ACL.

Hao Zhou, Yue Zhang, Shujian Huang, Junsheng Zhou,
Xin-Yu Dai, and Jiajun Chen. 2016a. A Search-
Based Dynamic Reranking Model for Dependency
Parsing. In Proc. of ACL.

Peng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu,
Hongyun Bao, and Bo Xu. 2016b. Text Classiﬁ-
cation Improved by Integrating Bidirectional LSTM
In Proc. of
with Two-Dimensional Max Pooling.
COLING.

Xiaodan Zhu, Parinaz Sobihani, and Hongyu Guo.
2015. Long Short-Term Memory Over Recursive
Structures. In Proc. of ICML.

Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: A Method for Auto-
matic Evaluation of Machine Translation. In Proc.
of ACL.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global Vectors for Word
Representation. In Proc. of EMNLP.

Cicero dos Santos, Ming Tan, Bing Xiang, and Bowen
Zhou. 2016. Attentive Pooling Networks. arXiv
preprint arXiv:1602.03609.

Haoyue Shi, Jiayuan Mao, Tete Xiao, Yuning Jiang,
and Jian Sun. 2018. Learning Visually-Grounded
Semantics from Contrastive Adversarial Samples.
In Proc. of COLING.

Richard Socher, Eric H. Huang,

Jeffrey Pennin,
Christopher D. Manning, and Andrew Y. Ng. 2011.
Dynamic Pooling and Unfolding Recursive Autoen-
coders for Paraphrase Detection. In Proc. of NIPS.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive Deep Models for
Semantic Compositionality over a Sentiment Tree-
bank. In Proc. of EMNLP.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to Sequence Learning with Neural Net-
works. In Proc. of NIPS.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015.
Improved Semantic Representa-
tions From Tree-Structured Long Short-Term Mem-
ory Networks. In Proc. of ACL-IJCNLP.

Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,
and Hang Li. 2016. Modeling Coverage for Neural
Machine Translation. In Proc. of ACL.

Yequan Wang, Minlie Huang, Li Zhao, and Xiaoyan
Zhu. 2016. Attention-Based LSTM for Aspect-
Level Sentiment Classiﬁcation. In Proc. of EMNLP.

Zhiguo Wang, Wael Hamza, and Radu Florian. 2017.
Bilateral Multi-Perspective Matching for Natural
Language Sentences. In Proc. of IJCAI.

A Implementation Details

A.2 Sentence Relation Classiﬁcation

Our codebase is built on PyTorch 0.3.0.6 All the
sentences was tokenized with SpaCy.7

A.1 Sentence Encoding

We use LSTM based sentence encodings as the
extracted features of sentences for downstream
classiﬁcation or generation tasks. We use typi-
cal long short term memory (LSTM; Hochreiter
and Schmidhuber, 1997) units for linear struc-
tures, which can be summarized as:

ft = σ(Wf · [ht−1, xt] + bf )
it = σ(Wi · [ht−1, xt] + bi)
˜ct = tanh(Wc · [ht−1, xt] + bc)
ot = σ(Wo · [ht−1, xt] + bo)
ct = ftct−1 + it ˜ct
ht = ot tanh(ct)

where t indicates the time step of a state; ht is
the hidden state and xt is the input vector. We
apply binary tree LSTM units adapted from Zhu
et al. (2015) for binary tree LSTMs, which can be
summarized as:

fl = σ(Wl · [hl, hr] + bl)
fr = σ(Wr · [hl, hr] + br)
it = σ(Wi · [hl, hr] + bi)
˜ct = tanh(Wc · [hl, hr] + bc)
ot = σ(Wo · [ht−1, xt] + bo)
ct = flcl + frcr + it ˜ct
ht = ot tanh(ct)

where the subscript t denotes the current state,
and l, r denote the left and right child states re-
spectively. We also apply LSTM (Hochreiter and
Schmidhuber, 1997) as leaf-node RNN when nec-
essary.

It is worth noting that left-branching tree LSTM
without leaf-node RNN is structurally equivalent
to unidirectional LSTM. The only difference be-
tween them, which may cause the slight difference
on performance, comes from the implementation
of LSTM units.

The candidate set of dropout ratio we explore
for the task of word-level semantic relation (WSR)
is {0, 0.1, 0.15, 0.2, 0.3, 0.5}.

6https://pytorch.org/docs/0.3.0
7https://spacy.io

In the task of sentence relation classiﬁcation, the
feature vector consists of the concatenation of
two sentence vectors, their difference, and their
element-wise product (Mou et al., 2016):

z =















s1
s2
s1 − s2
s1 (cid:12) s2

A.3 Pooling Mechanism

Following (Socher et al., 2011), we apply pooling
mechanism to all leaf states (of tree LSTMs) and
hidden states. The detailed pooling methods are
described as follows.

Max Pooling. Max pooling takes the max value
for each dimension

H = (h1, h2, · · · , hm)

si =

m
max
j=1

hj,i

i = 1, 2, · · · , d

s = (s1, s2, · · · , sd)T

where hi denotes a leaf state in tree LSTMs or a
hidden state; m = 2n−1 for tree LSTMs and m =
n for linear LSTMs; s denotes the ﬁnal sentence
encoding.

Mean Pooling. Mean pooling (average pooling)
takes the average of all hidden states as the sen-
tence representation, which can be summarized as:

H = (h1, h2, · · · , hm)
m
(cid:88)

s =

1
m

hi

i=1

Self-Attention. We follow Conneau et al. (2017)
and Lin et al. (2017) to build a self-attentive mech-
anism, which can be summarized as:

H = (h1, h2, · · · , hm)
a = softmax(wT
s = HaT

β tanh(WαH))

where a denotes attention weights computed by
learned parameters Wα and wβ.
In all experi-
ments, wβ is a 128-d vector.

