8
1
0
2
 
l
u
J
 
2
2
 
 
]

V
C
.
s
c
[
 
 
1
v
9
7
3
8
0
.
7
0
8
1
:
v
i
X
r
a

Towards Privacy-Preserving Visual Recognition
via Adversarial Training: A Pilot Study

Zhenyu Wu1, Zhangyang Wang1, Zhaowen Wang2, and Hailin Jin2

1 Texas A&M University, College Station TX 77843, USA
{wuzhenyu sjtu,atlaswang}@tamu.edu
2 Adobe Research, San Jose CA 95110, USA
{zhawang,hljin}@adobe.com

Abstract. This paper aims to improve privacy-preserving visual recog-
nition, an increasingly demanded feature in smart camera applications,
by formulating a unique adversarial training framework. The proposed
framework explicitly learns a degradation transform for the original video
inputs, in order to optimize the trade-oﬀ between target task performance
and the associated privacy budgets on the degraded video. A notable
challenge is that the privacy budget, often deﬁned and measured in
task-driven contexts, cannot be reliably indicated using any single model
performance, because a strong protection of privacy has to sustain against
any possible model that tries to hack privacy information. Such an un-
common situation has motivated us to propose two strategies, i.e., budget
model restarting and ensemble, to enhance the generalization of the
learned degradation on protecting privacy against unseen hacker models.
Novel training strategies, evaluation protocols, and result visualization
methods have been designed accordingly. Two experiments on privacy-
preserving action recognition, with privacy budgets deﬁned in various
ways, manifest the compelling eﬀectiveness of the proposed framework in
simultaneously maintaining high target task (action recognition) perfor-
mance while suppressing the privacy breach risk. The code is available at
https://github.com/wuzhenyusjtu/Privacy-AdversarialLearning

Keywords: Visual privacy, adversarial training, action recognition

1

Introduction

Smart surveillance or smart home cameras, such as Amazon Echo and Nest Cam,
are now found in millions of locations to remotely link users to their homes or
oﬃces, providing monitoring services to enhance security and/or notify environ-
ment changes, as well as lifelogging and intelligent services. Such a prevalence of
smart cameras has reinvigorated the privacy debate, since most of them require
to upload device-captured visual data to the centralized cloud for analytics. This
paper seeks to explore: how to make sure that those smart computer vision
devices are only seeing the things that we want them to see (and how to deﬁne
what we want)? Is it at all possible to alleviate the privacy concerns, without
compromising on user convenience?

2

A conference version of this paper is accepted by ECCV’18

At the ﬁrst glance, the question itself is posed as a dilemma: we would
like a camera system to recognize important events and assist human daily
life by understanding its videos, while preventing it from obtaining sensitive
visual information (such as faces) that can intrude people’s privacy. Classical
cryptographic solutions secure the communication against unauthorized access
from attackers. However, they are not immediately applicable to preventing
authorized agents (such as the backend analytics) from the unauthorized abuse
of information, that causes privacy breach concerns. The popular concept of
diﬀerential privacy has been introduced to prevent an adversary from gaining
additional knowledge by inclusion/exclusion of a subject, but not from gaining
knowledge from released data itself [8]. In other words, an adversary can still
accurately infer sensitive attributes from any sanitized sample available, which
does not violate any of the (proven) properties of diﬀerential privacy [18]. It
thus becomes a new and appealing problem, to ﬁnd an appropriate transform on
the collected raw visual data at the local camera end, so that the transformed
data itself will only enable certain target tasks while obstructing other undesired
privacy-related tasks. Recently, some new video acquisition approaches [3,9,47]
proposed to intentionally capture or process videos in extremely low-resolution to
create privacy-preserving “anonymized videos”, and showed promising empirical
results.

In contrast, we formulate the privacy-preserving visual recognition in a unique
adversarial training framework. The framework explicitly optimizes the trade-oﬀ
between target task performance and associated privacy budgets, by learning
active degradations to transform the video inputs. We investigate a novel way
to model privacy budget in a task-driven context. Diﬀerent from the standard
adversarial training where two individual models compete, the privacy budget
in our framework cannot be simply deﬁned with one single model, as the ideal
protection of privacy has to be universal and model-agnostic, i.e., obstructing
every possible model from predicting privacy information. To resolve the so-called
“∀ challenge”, we propose two strategies, i.e., restarting and ensembling budget
model(s), to enhance the generalization capability of the learned degradation to
defend against unseen models. Novel training strategies and evaluation protocols
have been proposed accordingly. Two experiments on privacy-preserving action
recognition, with privacy budgets deﬁned in diﬀerent ways, manifest the eﬀec-
tiveness of the proposed framework. With many problems left open and large
improvement room existing, we hope this pilot study to attract more interests
from the community.

2 Related Work

2.1 Privacy Protection in Computer Vision

With pervasive camera for surveillance or smart home devices, privacy-preserving
visual recognition has draw increasing interests from both industry and academia,
since (1) due to their computationally demanding nature, it is often impractical to
run visual recognition tasks all at the resource-limited local device end. Commu-
nicating (part of) data to the cloud is indispensable; (2) while traditional privacy

Privacy-Preserving Visual Recognition via Adversarial Training

3

concerns mostly arise from the unsecured channel between cloud and device (e.g,
malicious third-party eavesdropping), customers now possess increasing concerns
against sharing their private visual information to the cloud (which might turn
malicious itself).

A few cryptographic solutions [13,66] were developed to locally encrypt
visual information in a homomorphic way, i.e., the cryptosystems allow for basic
arithmetic classiﬁers over encrypted data. However, many encryptions-based
solution will incur high computational costs at the local platforms. It is also
challenging to generalize the cryptosystems to more complicated classiﬁers. [4]
combined the detection of regions of interest and the real encryption techniques
to improve privacy while allowing general surveillance to continue. A seemingly
reasonable, and computationally cheaper option is to extract and transmit feature
descriptors from raw images, and transmit those features only. Unfortunately,
a previous study [31] revealed that considerable information of original images
could still be recovered from standard HOG or SIFT features (even they look
visually distinct from natural images), making them fragile to privacy hacking
too.

An alternative toward a privacy-preserving vision system concerns the concept
of anonymized videos. Such videos are intentionally captured or processed to be in
special low quality conditions, that only allow for the recognition of some target
events or activities, while avoiding the unwanted leak of the identity information
for the human subjects in the video [3,9,47]. Typical examples of anonymized
videos are videos made to have extreme low resolution (e.g., 16 × 12) by using
low resolution camera hardware [9], based on image operations like blurring and
superpixel clustering [3], or introducing cartoon-like eﬀects with a customized
version of mean shift ﬁltering [63]. [41,42] proposed to use privacy preserving
optics to ﬁlter sensitive information from the incident light-ﬁeld before sensor
measurements are made, by k-anonymity and defocus blur. Earlier work [23]
explored privacy-preserving tracking and coarse pose estimation using a network
of ceiling-mounted time-of-ﬂight low-resolution sensors. [58] adopted a network
of ceiling-mounted binary passive infrared sensors. However, both works handled
only a limited set of activities performed at speciﬁc constrained areas in the
room. Later, [47] showed that even at the extreme low resolutions, reliable action
recognition could be achieved by learning appropriate downsampling transforms,
with neither unrealistic activity-location assumptions nor extra speciﬁc hardware
resources. The authors empirically veriﬁed that conventional face recognition
easily failed on the generated low-resolution videos. The usage of low-resolution
anonymized videos [9,47] is computationally cheaper, and is also compatible with
sensor and bandwidth constraints. However, [9,47] remain empirical in protecting
privacy. In particular, neither were their models learned towards protecting any
visual privacy, nor were the privacy-preserving eﬀects carefully analyzed and
evaluated. In other words, privacy protection in [9,47] came as a “side product”
of down-sampling, and was not a result of any optimization. The authors of [9,47]
also did not extend their eﬀorts to studying deep learning-based recognition,
making their task performance less competitive.

4

A conference version of this paper is accepted by ECCV’18

Very recently, a few learning-based approaches have come into play to ensure
better privacy protection. [53] deﬁned a utility metric and a privacy metric for a
task entity, and then designed a data sanitization function to achieve privacy while
providing utility. However, they considered only simple sanitization functions
such as linear projection and maximum mean discrepancy transformation. In [43],
the authors proposed a game-theoretic framework between an obfuscator and an
attacker, in order to hide visual secrets in the camera feed without signiﬁcantly
aﬀecting the functionality of the target application. This seems to be the most
relevant work to the proposed one: however, [43] only discussed a toy task to hide
QR codes while preserving the overall structure of the image. Another relevant
work [18] addressed the optimal utility-privacy tradeoﬀ by formulating it as a
min-diﬀ-max optimization problem. Nonetheless, The empirical quantiﬁcation of
privacy budgets in existing works [53,43,18] only considered to protect privacy
against one hacker model, and was thus insuﬃcient, for which we will explain
more in Section 3.1.

2.2 Privacy Protection in Social Media and Photo Sharing

User privacy protection is also a topic of extensive interests in the social media
ﬁeld, especially for photo sharing. The most common means to protect user
privacy in a uploaded photo is to add empirical obfuscations, such as blurring,
mosaicing or cropping out certain regions (usually faces) [26]. However, extensive
research showed that such an empirical means can be easily hacked too [37,32]. A
latest work [38] described a game-theoretical system in which the photo owner and
the recognition model strive for antagonistic goals of dis-/enabling recognition,
and better obfuscation ways could be learned from their competition. However,
it was only designed to confuse one speciﬁc recognition model, via ﬁnding its
“adversarial perturbations” [36]. That can caused obvious overﬁtting as simply
changing to another recognition model will likely put the learning eﬀorts in
vain: such perturbations even cannot protect privacy from human eyes. Their
problem setting thus deviated far away from our target problem. Another notable
diﬀerence is that in social photo sharing, we usually hope to cause minimum
perceptual quality loss to those photos, after applying any privacy-preserving
transform to them. The same concern does not exist in our scenario, allowing us
to explore much more free, even aggressive image distortions.

A useful resource to us was found in [39], which deﬁned concrete privacy
attributes and correlated them to image content. The authors categorized possible
private information in images, and then run a user study to understand the privacy
preferences. They then provided a sizable set of 22k images annotated with 68
privacy attributes, on which they trained privacy attribute predictors.

2.3 Recognition from Visually Degraded Data

To enable the usage of anonymized videos, one important challenge is to ensure
reliable performance of the target tasks on those lower-quality videos, besides
suppressing the undesired privacy leak. Among all low visual quality scenarios,

Privacy-Preserving Visual Recognition via Adversarial Training

5

visual recognition in low resolution is probably best studied. [61,28,7] showed
that low resolution object recognition could be signiﬁcantly enhanced through
proper pre-training and domain adaption. Low-resolution action recognition has
also drawn growing interests: [46] proposed a two-stream multi-Siamese CNN
that learns the embedding space to be shared by low resolution videos down
sampled in diﬀerent ways, on top of which a transform-robust action classiﬁer was
trained. [6] leveraged a semi-coupled ﬁlter-sharing two stream network to learn a
mapping between the low- and high-resolution feature space. In comparison, the
“low-quality” anonymized videos in our case are generated by learned and more
complicated degradations, other than simple downsampling [61,6].

3 Technical Approach

3.1 Problem Deﬁnition

Assume our training data X (raw visual data captured by camera) are associated
with a target task T and a privacy budget B. We mathematically express the
goal of privacy-preserving visual recognition as below (γ is a weight coeﬃcient):

minfT ,fd LT (fT (fd(X)), YT ) + γLB(fd(X)),

(1)

where fT denotes the model to perform the target task T on its input data.
Since T is usually a supervised task, e.g., action recognition or visual tracking, a
label set YT is provided on X, and a standard cost function LT (e.g., softmax) is
deﬁned to evaluate the task performance on T . On the other hand, we need to
deﬁne a budget cost function LB to evaluate the privacy leak risk of its input
data: the larger LB, the higher privacy leak risk. Our goal is to seek such an
active degradation function fd to transform the original X as the common input
for both LT and LB, such that:

– The target task performance LT is minimally aﬀected compare to when using

the raw data, i.e., minfT ,fd LT (fT (fd(X)), YT ) ≈ minf (cid:48)

LT (f (cid:48)

T (X), YT ).

T

– The privacy budget LB is greatly suppressed compared to raw data, i.e.,

LB(fd(X)) (cid:28) LB(X).

The deﬁnition of the privacy budget cost LB is not straightforward. Practically,
it needs to be placed in concrete application contexts, often in a task-driven
way. For example, in smart workplaces or smart homes with video surveillance,
one might often want to avoid a disclosure of the face or identity of persons.
Therefore, to reduce LB could be interpreted as to suppress the success rate
of identity recognition or veriﬁcation on the transformed video fd(X). Other
privacy-related attributes, such as race, gender, or age, can be similarly deﬁned
too. We denote the privacy-related annotations (such as identity label) as YB, and
rewrite LB(fd(X)) as LB(fb(fd(X)), YB), where fb denotes the budget model to
predict the corresponding privacy information. Diﬀerent from LT , minimizing
LB will encourage fb(fd(X)) to diverge from YB as much as possible.

6

A conference version of this paper is accepted by ECCV’18

Such a supervised, task-driven deﬁnition of LB poses at least two-fold chal-
lenges: (1) the privacy budget-related annotations, denoted as YB, often have
less availability than target task labels. Speciﬁcally, it is often challenging to
have both YT and YB ready on the same X; (2) considering the nature of privacy
protection, it is not suﬃcient to merely suppress the success rate of one fb model.
Instead, deﬁne a privacy prediction function family P: fd(X) → YB, the ideal
privacy protection of fd should be reﬂected as suppressing every possible
model fb from P. That diverts from the common supervised training goal, where
one only needs to ﬁnd one model to successfully fulﬁll the target task. We re-write
the general form (1) with the task-driven deﬁnition of LB:

(2)

minfT ,fd LT (fT (fd(X), YT ) + γ maxfb∈P LB(fb(fd(X)), YB).
For the solved fd, the two goals should be simultaneously satisﬁed: (1) there
exists (“∃”) at least one fT function that can predict YT from fd(X) well; (2)
for all (“∀”) fb functions ∈ P, none of them (even the best one) can reliably
predict YB from fd(X). Most existing works chose an empirical fd (e.g., simple
downsampling) and solved minfT LT (fT (fd(X), YT ) [9,61]. [47] essentially solved
minfT ,fd LT (fT (fd(X), YT ) to jointly adapted fd and fT , after which the authors
empirically veriﬁed the eﬀect of fd on LB (deﬁned as face recognition error rates).
Those approaches lack the explicit optimization towards privacy budgets, and
thus have no guaranteed privacy-protection eﬀects.

Comparison to Standard Adversarial Training The most notable diﬀerence be-
tween (2) and existing works based on standard adversarial training [43,38] lies in
whether the adversarial perturbations are optimized for “fooling” one speciﬁc fb,
or all possible fbs. We believe the latter to be necessary, as it considers generaliza-
tion ability to suppressing unseen privacy breach. Moreover, most existing works
seek perturbations with minimal human visual impacts, e.g, by enforcing (cid:96)p norm
constraint on the pixel domain. That is clearly unaligned with our purpose. In
fact, our model could be viewed as to minimize the perturbation in the (learned)
feature domain of target utility task.

3.2 Basic Framework

Overview Figure 1 depicts a model architecture to implement the proposed
formulation (2). It ﬁrst takes the original video data X as the input, and passes
it through the active degradation module fd to generate the anonymized video
fd(X). During training, the anonymized video simultaneously goes through a
target task model fT and a privacy prediction model fb. All three modules, fd,
fT and fb, are learnable and can be implemented by neural networks. The entire
model is trained under the hybrid loss of LT and LB. By tuning the entire pipeline
from end to end, fd(X) will ﬁnd the optimal task-speciﬁc transformation, to the
advantage of target task but to the disadvantage of privacy breach, fulﬁlling the
goal of privacy-preserving visual recognition. After training, we can apply the
learned active degradation at the local device (e.g., camera) to convert incoming
video to its anonymized version, which is then transmitted to the backend (e.g.,
cloud) for target task analysis.

The proposed framework leads to an adaptive and end-to-end manageable

Privacy-Preserving Visual Recognition via Adversarial Training

7

pipeline for privacy-preserving vi-
sual recognition. Its methodology
is related to the emerging re-
search of feature disentanglement
[64]. That technique leads to non-
overlapped groups of factorized la-
tent representations, each of which
would properly describe informa-
tion corresponding to particular
attributes of interest. Previously it
was applied to generative models
[10,51] and reinforcement learning
[20].

Fig. 1: Basic adversarial training framework
for privacy-preserving visual recognition.

Similar to GANs [16] and other
adversarial models, our training is
prone to collapse and/or bad local minimums. We thus propose a carefully-
designed training algorithm with three-module alternating update strategy, ex-
plained in the supplementary, which could be interpreted as a three-party
game. In principle, we strive to avoid any of the three module fd, fT , and fb to
change “too quickly”, and thus keep monitoring LT and Lb to decide which of
the three modules to be updated next.
Choices of fd, fT and fb The choices of the three modules will signiﬁcantly
impact the performance. As [47] pointed out, fd can be constructed as a nonlinear
mapping by ﬁltering. The form of fd can be ﬂexible, and its output fd(X) is
unnecessary to be a natural image. For simplicity, we choose fd to be a “learnable
ﬁltering” in the form of 2-D convolutional neural network (CNN), whose the
output fd(X) will be a 2-D feature map of the same resolution as the input video
frame. Such a choice is only to facilitate the initial concatenation of building
blocks, e.g., fT and fb often start with pre-trained models on natural images.
Besides, fd(X) should preferably be in a compact form and light to transmit,
considering it will be sent to the cloud through (limited-bandwidth) channels.
To ensure the eﬀectiveness of fd, it is necessary to choose suﬃciently strong fT
and fb models and let them compete. We employ state-of-the-art video recognition
CNNs for corresponding tasks, and adapt them for the degraded input fd(X)
using the robust pre-training strategy proposed in [61].

Particular attentions should be paid towards the budget cost (second term)
deﬁned in (2), which we refer as “the ∀ Challenge”: if we use fb with some
pre-deﬁned CNN architecture, how could we be sure that it is the “best possible”
privacy prediction model? That is to say, even we are able to ﬁnd a fd function that
manages to fail one fb model, is it possible that some other f (cid:48)
b ∈ P would still be
able to predict YB from fd(X), thus leaking privacy? While it is computationally
intractable to exhaustively search over P, a naive empirical solution would be
to chose a very strong privacy prediction model, hoping that a fd function that
can confuse this strong one will be able to fool other possible functions as well.
However, the resulting fd(X) may still overﬁt the artifacts of one speciﬁc fb and

8

A conference version of this paper is accepted by ECCV’18

fails to generalize. Section 3.3 will introduce two more advanced and feasible
recipes.
Choices of LT and LB Without loss of generality, we assume both target task
fT and privacy prediction fb to be classiﬁcation models and output class labels.
To optimize the target task performance, LT could be simply chosen as the KL
divergence: KL(fT (fd(X), YT ).

Choosing LB is non-standard and tricky since we require minimizing the
privacy budget LB(fb(fd(X)), YB) to enlarge the divergence between fb(fd(X))
and YB. One possible choice is the negative KL divergence between the predicted
class vector and the ground truth label; but minimizing a concave funcion
will cause a ton of numerical instabilities (often explosions). Instead, we use
the negative entropy function of the predicted class vector, and minimizing it
to encourage “uncertain” predictions. Meanwhile, we will use YB to ensure a
suﬃciently strong fb at the initialization (see 4.1.2). Furthermore, YB will play a
critical role in model restarting (see 3.3).

3.3 Addressing the ∀ Challenge

To improve the generalization of learned fd over all possible fb ∈ P (i.e, privacy
cannot be reliably predicted by any model), we hereby discuss two simple and
easy-to-implement options. Other more sophisticated model re-sampling or model-
search approaches, e.g., [68], will be explored in future work.

Budget Model Restarting At certain point of training (e.g., when the
privacy budget LB(fb(fd(X))) stops decreasing any further), we replace the
current weights in fb with random weights. Such a random re-starting aims to
avoid trivial overﬁtting between fb and fd (i.e., fd is only specialized at confusing
the current fb), without incurring more parameters. We then start to train the
new model fb to be a strong competitor, w.r.t. the current fd(X): speciﬁcally, we
freeze the training of fd and fT , and change to minimizing KL(fb(fd(X)), YB),
until the new fb has been trained from scratch to become a strong privacy
prediction model over current fd(X). We then resume adversarial training by
unfreezing fd and fT , as well as replacing the loss for fb back to the negative
entropy. It can repeat several times.

Budget Model Ensemble The other strategy proposes to approximate the
continuous P with a discrete set of M sample functions. Assuming the budget
model ensemble {f i
i=1, we turn to minimizing the following discretized surrogate
of (2):

b}M

minfT ,fd LT (fT (fd(X), YT ) + γ maxi∈{1,2,...,M } LB(f i

b(fd(X))).

(3)

At each iteration (mini-batch), minimizing (3) will only suppress the model f i
b
with the largest LB cost, e.g., the “most conﬁdent” one about its current privacy
prediction. The previous basic framework is a special case of (3) with M = 1.
The ensemble strategy can easily be combined with re-starting.

Privacy-Preserving Visual Recognition via Adversarial Training

9

3.4 Two-Fold Evaluation Protocol

T and privacy annotations Y e

Apart from training data X, assume we have an evaluation set X e, accompanied
with both target task labels Y e
B. Our evaluation is
signiﬁcantly more complicated than classical visual recognition problems. After
applying the learned active degradation, we need to examine in two folds: (1)
whether the learned target task model maintains satisfactory performance; (2)
whether the performance of an arbitrary privacy prediction model will deteriorate.
The ﬁrst one can follow the standard routine: applying the learned fd and fT
to X e, and computing the classiﬁcation accuracy AT via comparing fT (fd(X e))
w.r.t. Y e

T : the higher the better.

For the second evaluation, it is apparently insuﬃcient if we only observe that
the learned fd and fb lead to poor classiﬁcation accuracy on X e, because of the
∀ challenge. In other words, fd needs to generalize not only in the data space,
but also w.r.t. the fb model space. To empirically verify that fb prohibits reliable
privacy prediction for other possible models, we propose a novel procedure: we
ﬁrst re-sample a diﬀerent set of N models {f j
j=1 from P; none of them will
be overlapped with the M budget models used in training. We then train each
of them to predict privacy information, over the degraded training data X by
applying the learned fd, i.e., minimizing f j
b (fd(X)), j = 1, ..., N . Eventually, we
apply each trained f j
b and fd on X e and compute the classiﬁcation accuracy
for the j-th model. The highest accuracy achieved among the N models on
fd(X e), denoted as AN
b , will be by default used to indicate the privacy protection
capability of fd: the lower the better.

b }N

4 Experiments

We present two experiments on privacy-preserving action recognition, as proof-
of-concepts for our proposed general framework. For the target task, we choose
video-based action recognition, because it is a highly demanded feature in many
smart homes and smart workplaces. The deﬁnition of privacy will vary by contexts,
and we will study two settings: (1) avoiding the leak of person identities present
in the current work; and (2) avoiding the leak of multiple privacy attributes,
e.g., multiple crowdsoursed attributes studied in [39]. We emphasize that the
generality of the proposed framework (2) can ﬁt in a vast more variety of target
task and privacy information compositions.

4.1 Identity-Preserving Action Recognition on SBU

Problem Setting The SBU Kinect Interaction Dataset [67] is a two-person inter-
action dataset for video-based action recognition, with 8 types of actions and 13
diﬀerent actor pairs annotated. We deﬁne action recognition as the target task
T , and the privacy budget task B as reducing the correct identiﬁcation rates of
the actor pairs in the same video. We note that the target trade-oﬀ is highly

10

A conference version of this paper is accepted by ECCV’18

challenging to achieve. As can be seen from the ﬁrst Table in the supplemen-
tary, the actor pair recognition task easily achieves over 98% accuracy on the
original dataset, and stands robust even when the frames are downsampled 28
times, while the action recognition performance already starts to deteriorate
signiﬁcantly. We compare the following ﬁve methods:
– Method 1 (naive downsampling): using raw RGB frames under diﬀerent
down-sampling rates .
– Method 2 (proposed w/o re-starting): applying the proposed adversarial
training to RGB frames, using budget model ensemble without restarting .
– Method 3 (proposed): applying the proposed adversarial training to RGB
frames, using budget model ensemble with restarting .
– Method 4: detecting and cropping out faces from RGB frames.
– Method 5: detecting and cropping out whole actor bodies from RGB frames.
Method 1 follows [47], while Methods 4 and 5 are inspired by [26].

Implementation Details We segment video sequences into groups of 16 frames,
and use those frame groups as our default input data X. We use the C3D net [60]
as the default action recognition model, i.e. fT . For the fb identity recognition
model, we choose MobileNet [21] to identify actor pair in each frame, and use
average pooling to aggregate the frame-wise predictions. The active degradation
module fd adopts the image transformation network in [24].

We choose γ = 2.0 to suppress the identity recognition performance on SBU.
We ﬁrst initialize the active degradation module fd as reconstruction of the input.
We next take the pre-trained version of C3D net and concatenate it with fd, and
jointly train them for action recognition on the SBU dataset, to initialize fT .
We then freeze them both, and start initializing fb (MobileNet) for the actor
pair identiﬁcation task, by adapting it to the output of the currently trained fd.
Experiments show that such initializations provide robust starting points for the
follow-up adversarial training. If budget model restarting is adopted, we set to
“re-start” MobileNet from random initialization after every 100 iterations. The
number of ensemble budget models M varies in {1, 2, 4, 6, 8, 10, 12, 14, 16, 18}.
Diﬀerent budget models can be obtained via setting diﬀerent depth-multiplier
parameter [21] of MobileNet.

Evaluation Procedure We will follow the procedure described in Section 3.4, for
two-fold evaluations on the SBU testing set. For the set of models used towards
privacy-protection examination, we sample N = 10 popular image classiﬁcation
CNNs, a list of which can be found in the supplementary. Among them, 8
models start from ImageNet-pretrained versions, including MobileNet (diﬀerent
from those used in training) [21], ResNet [19] and Inception [55]. To eliminate
the possibility that the initialization might prohibit privacy prediction, we also
intentionally try another 2 models trained from scratch (random initialization).
We did not choose any non-CNN image classiﬁcation model for two reasons: (1)
CNNs have state-of-the-art performance and also strong ﬁtting capability when
re-trained; (2) most non-CNN image classiﬁcation models rely on eﬀective feature
descriptors, that are designed for natural images. Since fd(X)/fd(Xe) are no
longer natural images, the eﬀectiveness of such models is in jeopardy too.

Privacy-Preserving Visual Recognition via Adversarial Training

11

Results and Analysis We present
an innovative visualization in Fig-
ure 2, to display the trade-oﬀ be-
tween the action recognition accu-
racy AT and the actor pair recog-
nition accuracy AN
b , in an easy-to-
interpret way. All accuracy num-
bers for both task evaluation can
be found in the supplementary.
To read the ﬁgure, note that a
desirable trade-oﬀ should incur
minimal loss of AT (y-axis) when
reducing AN
(x-axis). Therefore,
b
a point closer to the upper
left corner denotes a more desir-
able fd model that achieves better
trade-oﬀ, since it incurs less util-
ity performance loss (larger AT )
while suppressing more even the best of N unseen privacy prediction models
(smaller AN
b ). For Method 1, a larger marker (circle) size represents a larger
downsampling rate. For Methods 2 and 3, a larger marker (star) size denotes
more budget models used in ensemble (i.e., larger M ). Both Methods 4 and 5
give single points. Observations can be summarized below:
1. Methods 2 and 3 has obvious advantages over naive downsampling (Method

Fig. 2: Target and Budget Task Performance
Trade-oﬀ on SBU Dataset.

1) in terms of more compelling trade-oﬀ between AT and AN
b .

2. Method 3 achieves much better individual model performance than Methods

2, showing the signiﬁcance of re-starting in overcoming over-ﬁtting.

3. For Method 3 (similarly for Method 2), adding more fb models into the budget
model ensemble (i.e., increasing M ) consistently leads to points closer to the
desired upper left (despite small randomness caused by training). The results
show that proposed budget model ensemble technique eﬀectively alleviates
the ∀ challenge, making fd generalize better to unseen fbs.

4. Neither Methods 4 nor 5 performs competitively. Cropping out faces does not
add to protection of identity privacy (its AN
is almost the same as using raw
b
RGB frames without downsampling), since body shape and clothing styles
reveal signiﬁcant amounts of identity information too. Cropping out body not
only removes identity information, but also deteriorates action recognition.
In brief, Method 3 (the proposed, with both restarting and ensemble) is able
to achieve most compelling results on SBU, the best (M = 18) at AT = 82.3%
and AN
b = 41.8%, without relying on extra hardware (e.g., depth camera). A few
visualized examples of learned fd(X) can be found in Figure 5.

4.2 Protecting Multiple Privacy Attributes on UCF-101/VISPR

Problem Setting The UCF101 dataset [54] is a more challenging action recognition
benchmark with 101 diﬀerent action classes and much richer scenes than SBU.

12

A conference version of this paper is accepted by ECCV’18

The newly-released Visual Privacy (VISPR) dataset [39] is a collection of 22, 167
images annotated with 68 privacy attributes, e.g. semi-nudity, hobbies, face, race,
gender, skin color, and so on.

Either alone cannot be used for our adversarial training because neither
dataset has the pair of competing tasks/labels. However, we show the possibility
of jointly utilizing UCF101 and VISPR to train one fd function, such that it
simultaneously enables eﬀective action recognition (as measured on UCF-101)
and suppresses multiple privacy attribute prediction (as measured on VISPR),
via a novel cross-dataset training and evaluation strategy.
Cross-Dataset Training and Evaluation fd and fT are the same as the SBU
one. Regarding fb for the task of multiple privacy attribute prediction, we adopt
budget models from {ResNet-V1-50, ResNet-V2-50, MobileNet-1.0, MobileNet-
0.75} and vary the number of M in {1, 2, 3, 4}. They are trained with a multi-label
classiﬁcation loss with sigmoid activations, identically to [39]. γ is chosen as 0.5.
Rather than recognition accuracy, the multiple privacy attribute prediction is
evaluated by Class-based Mean Average Precision (C-MAP), whose deﬁnition
can be referred to [39].

The major hassle is the unavailability of T and B on the same dataset:
since UCF-101 does not have privacy attributes annotated, we cannot directly
perform adversarial training and evaluate privacy protection on it; similarly
for VISPR. We notice that [39] trained the model on VISPR to be a privacy
predictor for general images. We also visually observe that the VISPR model can
correctly detect privacy attribute occurring in UCF-101 videos (examples in the
supplementary). Therefore, we hypothesize that the privacy attributes have
good “transferability” between UCF-101 and VISPR, and hence use a privacy
prediction model trained on VISPR to assess the privacy leak risk on UCF-101.
Instead of using all 68 attributes in [39], we ﬁnd that many of them rarely
appear in UCF-101 (shown in the supplementary). We thus create two subsets
for training and evaluating budget models here: one VISPR-17 set consists of 17
attributes that occurs most in UCF-101 and their associated images in VISPR;
the other VISPR-7 set is further a subset of VISPR-17, that include 7 privacy
attributes out of 17 that are most common in smart home settings. Their attribute
lists are in the supplementary.

During training, we have two pipelines: one is fd + fT trained on UCF-101
for action recognition; the other is fd + fb trained on VISPR to suppress multiple
privacy attribute prediction. The two pipelines share the same parameters for fd.
The initialization and alternating training strategy remain unchanged from SBU.
During evaluation, we perform the ﬁrst part of two-fold evaluation, e.g., action
recognition, on UCF-101 testing set. We then evaluate the performance of the
N -model examination on privacy protection, using the VISPR-17/7 testing sets.
Such cross-dataset training and evaluation sheds on new possibilities on training
privacy-preserving recognition models, even under the practical shortages of
datasets that have been annotated for both tasks.

Results and Analysis We choose Methods 1, 2,and 3 for comparison, deﬁned
the same as SBU. All the quantitative results, as well as visualized examples of

Privacy-Preserving Visual Recognition via Adversarial Training

13

Fig. 3: Performance Trade-oﬀ on UCF-101/VISPR dataset. The left one is on
VISPR-17 and the right one on VISPR-7.

fd(X) on UCF-101, are shown in the supplementary. Similarly to the SBU
case, simply downsamping video frames (even with the aid of super resolution as
we tried) will not lead to any competitive trade-oﬀ between action recognition
(at UCF-101) and privacy prediction suppression (at VISPR). As is shown in
Figure 3, our proposed adversarial training again leads to more favorable trade-
oﬀs on VISPR-17 and VISPR-7, with major conclusions concur with SBU: both
ensemble and restarting help fd generalize better against privacy breach.

5 Limitations and Discussions

As noted by one anonymous reviewer, a possible alternative to avoid leaking
visual privacy to the cloud is to perform action recognition completely at the local
device. In comparison, our proposed solution is motivated by at least three folds:
i) for single utility task (which is not just limited to action recognition), running
fd on device is much more compact and eﬃcient than full fT For example, our
fT model (11-layer C3D net) has over 70 million parameters, while fd is a much
more compact 3-layer CNN with 1.3 million parameters. At the inference, the
total time cost of running fT over the SBU testing set is 45 times more than
running fd. It also facilitates upgrading to more sophisticated fT models; ii) The
smart home scenario calls for the scalability to multiple utility tasks (computer
vision functions). It is not economic to load all utility models in the device.
Instead, we can train one fd to work with multiple utility models, and only store
and run fd at the device. More utility models (if no overlap with privacy) could
be possibly added in the cloud by training on fd(X); iii) We further point out
that the proposed approach can further have a wider practical application scope
beyond smart home, e.g, de-identiﬁed data sharing.

The current pilot study is preliminary in many ways, and there is large
performance room to improve until achieving practical usefulness. First, the

14

A conference version of this paper is accepted by ECCV’18

Original RGB Frame from UCF-101 (Label: Pushing)

Method 2, M=1

Method 2, M=4

Method 2, M=8

Method 2, M=14

Method 3, M=1

Method 3, M=4

Method 3, M=8

Method 3, M=14

Fig. 4: Example frames after applying the learned degradation on SBU.

deﬁnition of B and LB is core to the framework. Considering the ∀ challenge, the
current budget model ensemble is a rough discretized approximation of P. More
elegant ways to tackle this ∀ optimization can lead to further breakthroughs in
universal privacy protection. Second, adversarial training is well-known to be
diﬃcult and instable. Improved training tricks, such as [48], will be useful. Third,
a lack of related benchmark datasets, on which T and B are both appropriately
deﬁned, has become a bottleneck. We see that more concrete and precise privacy
deﬁnitions, such as VISPR attributes, can certainly result in better feature
disentanglement and T -B performance trade-oﬀs. Current cross-dataset training
and evaluation partially alleviate the absence of dedicated datasets. However, the
inevitable domain mismatch between two datasets can still hurdle the performance.
We plan to refer to crowdsourcing to identify and annotate privacy-related
attributes on existing action recognition or other benchmarks, which we hope
could help promote this research direction.

Privacy-Preserving Visual Recognition via Adversarial Training

15

References

1. Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov,
Kunal Talwar, and Li Zhang. Deep learning with diﬀerential privacy. In Proceedings
of the 2016 ACM SIGSAC Conference on Computer and Communications Security,
pages 308–318. ACM, 2016.

2. Moez Baccouche, Franck Mamalet, Christian Wolf, Christophe Garcia, and Atilla
Baskurt. Sequential deep learning for human action recognition. In International
Workshop on Human Behavior Understanding, pages 29–39. Springer, 2011.

3. Daniel J Butler, Justin Huang, Franziska Roesner, and Maya Cakmak. The privacy-
utility tradeoﬀ for remotely teleoperated robots. In Proceedings of the Tenth Annual
ACM/IEEE International Conference on Human-Robot Interaction, pages 27–34.
ACM, 2015.

4. Ankur Chattopadhyay and Terrance E Boult. Privacycam: a privacy preserving cam-
era using uclinux on the blackﬁn dsp. In Computer Vision and Pattern Recognition,
2007. CVPR’07. IEEE Conference on, pages 1–8. IEEE, 2007.

5. Chen Chen, Roozbeh Jafari, and Nasser Kehtarnavaz. Action recognition from depth
sequences using depth motion maps-based local binary patterns. In Applications of
Computer Vision (WACV), 2015 IEEE Winter Conference on, pages 1092–1099.
IEEE, 2015.

6. Jiawei Chen, Jonathan Wu, Janusz Konrad, and Prakash Ishwar. Semi-coupled
two-stream fusion convnets for action recognition at extremely low resolutions.
arXiv preprint arXiv:1610.03898, 2016.

7. Bowen Cheng, Zhangyang Wang, Zhaobin Zhang, Zhu Li, Ding Liu, Jianchao Yang,
Shuai Huang, and Thomas S Huang. Robust emotion recognition from low quality
and low bit rate video: A deep learning approach. In Aﬀective Computing and
Intelligent Interaction (ACII), 2017 Seventh International Conference on, pages
65–70. IEEE, 2017.

8. Graham Cormode. Individual privacy vs population privacy: Learning to attack

anonymization. arXiv preprint arXiv:1011.2511, 2010.

9. Ji Dai, Behrouz Saghaﬁ, Jonathan Wu, Janusz Konrad, and Prakash Ishwar. Towards
privacy-preserving recognition of human activities. In Image Processing (ICIP),
2015 IEEE International Conference on, pages 4238–4242. IEEE, 2015.

10. Guillaume Desjardins, Aaron Courville, and Yoshua Bengio. Disentangling factors
of variation via generative entangling. arXiv preprint arXiv:1210.5474, 2012.
11. Yong Du, Wei Wang, and Liang Wang. Hierarchical recurrent neural network
for skeleton based action recognition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 1110–1118, 2015.

12. Cynthia Dwork. Diﬀerential privacy: A survey of results. In International Conference
on Theory and Applications of Models of Computation, pages 1–19. Springer, 2008.
13. Zekeriya Erkin, Martin Franz, Jorge Guajardo, Stefan Katzenbeisser, Inald La-
gendijk, and Tomas Toft. Privacy-preserving face recognition. In International
Symposium on Privacy Enhancing Technologies Symposium, 2009.

14. Farhad Farokhi and Henrik Sandberg. Fisher information as a measure of pri-
vacy: Preserving privacy of households with smart meters using batteries. IEEE
Transactions on Smart Grid, 2017.

15. Cl´ement Godard, Oisin Mac Aodha, and Gabriel J Brostow. Unsupervised monoc-
ular depth estimation with left-right consistency. In CVPR, volume 2, page 7,
2017.

16

A conference version of this paper is accepted by ECCV’18

16. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In
Advances in neural information processing systems, pages 2672–2680, 2014.

17. Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and har-

nessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.

18. Jihun Hamm. Minimax ﬁlter: learning to preserve privacy from inference attacks.

The Journal of Machine Learning Research, 18(1):4704–4734, 2017.

19. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning

for image recognition. arXiv preprint arXiv:1512.03385, 2015.

20. Irina Higgins, Arka Pal, Andrei A Rusu, Loic Matthey, Christopher P Burgess,
Alexander Pritzel, Matthew Botvinick, Charles Blundell, and Alexander Lerchner.
Darla: Improving zero-shot transfer in reinforcement learning. arXiv:1707.08475,
2017.

21. Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Eﬃcient
convolutional neural networks for mobile vision applications. arXiv preprint
arXiv:1704.04861, 2017.

22. Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolutional neural networks
for human action recognition. IEEE transactions on pattern analysis and machine
intelligence, 35(1):221–231, 2013.

23. Li Jia and Richard J Radke. Using time-of-ﬂight measurements for privacy-
preserving tracking in a smart room. IEEE Transactions on Industrial Informatics,
10(1):689–696, 2014.

24. Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time
style transfer and super-resolution. In European Conference on Computer Vision,
2016.

25. Jing Li, Stan Z Li, Quan Pan, and Tao Yang. Illumination and motion-based
video enhancement for night surveillance. In Visual Surveillance and Performance
Evaluation of Tracking and Surveillance, 2005. 2nd Joint IEEE International
Workshop on, pages 169–175. IEEE, 2005.

26. Yifang Li, Nishant Vishwamitra, Bart P Knijnenburg, Hongxin Hu, and Kelly Caine.
Blur vs. block: Investigating the eﬀectiveness of privacy-enhancing obfuscation for
images. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2017
IEEE Conference on, pages 1343–1351. IEEE, 2017.

27. Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint

arXiv:1312.4400, 2013.

28. Ding Liu, Bowen Cheng, Zhangyang Wang, Haichao Zhang, and Thomas S Huang.
Enhance visual recognition under adverse conditions via deep networks. arXiv
preprint arXiv:1712.07732, 2017.

29. Ping Liu, Joey Tianyi Zhou, Ivor Wai-Hung Tsang, Zibo Meng, Shizhong Han, and
Yan Tong. Feature disentangling machine-a novel approach of feature selection and
disentangling in facial expression analysis. In European Conference on Computer
Vision, pages 151–166. Springer, 2014.

30. Behrooz Mahasseni, Sinisa Todorovic, and Alan Fern. Budget-aware deep semantic

video segmentation.

31. Aravindh Mahendran and Andrea Vedaldi. Visualizing deep convolutional neural
networks using natural pre-images. International Journal of Computer Vision,
2016.

32. Richard McPherson, Reza Shokri, and Vitaly Shmatikov. Defeating image obfusca-

tion with deep learning. arXiv preprint arXiv:1609.00408, 2016.

Privacy-Preserving Visual Recognition via Adversarial Training

17

33. Alan Mislove, Bimal Viswanath, Krishna P Gummadi, and Peter Druschel. You
are who you know: inferring user proﬁles in online social networks. In Proceedings
of the third ACM international conference on Web search and data mining, pages
251–260. ACM, 2010.

34. Arvind Narayanan and Vitaly Shmatikov. De-anonymizing social networks. In
Security and Privacy, 2009 30th IEEE Symposium on, pages 173–187. IEEE, 2009.
35. Shree K Nayar and Srinivasa G Narasimhan. Vision in bad weather. In Computer
Vision, 1999. The Proceedings of the Seventh IEEE International Conference on,
volume 2, pages 820–827. IEEE, 1999.

36. Anh Nguyen, Jason Yosinski, and Jeﬀ Clune. Deep neural networks are easily
fooled: High conﬁdence predictions for unrecognizable images. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pages 427–436,
2015.

37. Seong Joon Oh, Rodrigo Benenson, Mario Fritz, and Bernt Schiele. Faceless person
In European Conference on

recognition: Privacy implications in social media.
Computer Vision, pages 19–35. Springer, 2016.

38. Seong Joon Oh, Mario Fritz, and Bernt Schiele. Adversarial image perturbation
for privacy protection–a game theory perspective. In International Conference on
Computer Vision (ICCV), 2017.

39. Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Towards a visual pri-
In IEEE

vacy advisor: Understanding and predicting privacy risks in images.
International Conference on Computer Vision (ICCV), 2017.

40. Tribhuvanesh Orekondy, Bernt Schiele, Mario Fritz, and Saarland Informatics
Campus. Towards a visual privacy advisor: Understanding and predicting privacy
risks in images. arXiv preprint arXiv:1703.10660, 2017.

41. Francesco Pittaluga and Sanjeev J Koppal. Privacy preserving optics for miniature
vision sensors. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 314–324, 2015.

42. Francesco Pittaluga and Sanjeev Jagannatha Koppal. Pre-capture privacy for small
vision sensors. IEEE transactions on pattern analysis and machine intelligence,
39(11):2215–2226, 2017.

43. Nisarg Raval, Ashwin Machanavajjhala, and Landon P Cox. Protecting visual secrets
using adversarial nets. In Computer Vision and Pattern Recognition Workshops
(CVPRW), 2017 IEEE Conference on, pages 1329–1332. IEEE, 2017.

44. M. S. Ryoo, T. J. Fuchs, L. Xia, J. K. Aggarwal, and L. Matthies. Robot-centric ac-
tivity prediction from ﬁrst-person videos: What will they do to me? In ACM/IEEE
International Conference on Human-Robot Interaction (HRI), pages 295–302, Port-
land, OR, March 2015.

45. M. S. Ryoo and L. Matthies. First-person activity recognition: What are they doing
to me? In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
Portland, OR, June 2013.

46. Michael S Ryoo, Kiyoon Kim, and Hyun Jong Yang. Extreme low resolu-
tion activity recognition with multi-siamese embedding learning. arXiv preprint
arXiv:1708.00999, 2017.

47. Michael S Ryoo, Brandon Rothrock, Charles Fleming, and Hyun Jong Yang. Privacy-

preserving human activity recognition from extreme low resolution. 2017.

48. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford,
In Advances in Neural

and Xi Chen.
Improved techniques for training gans.
Information Processing Systems, pages 2234–2242, 2016.

18

A conference version of this paper is accepted by ECCV’18

49. Christian Schuldt, Ivan Laptev, and Barbara Caputo. Recognizing human actions:
a local svm approach. In Pattern Recognition, 2004. ICPR 2004. Proceedings of
the 17th International Conference on, volume 3, pages 32–36. IEEE, 2004.

50. Shikhar Sharma, Ryan Kiros, and Ruslan Salakhutdinov. Action recognition using

visual attention. arXiv preprint arXiv:1511.04119, 2015.

51. N Siddharth, Brooks Paige, Alban Desmaison, Jan-Willem van de Meent, Frank
Wood, Noah D Goodman, Pushmeet Kohli, and Philip HS Torr. Learning disentan-
gled representations in deep generative models. 2016.

52. Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for
action recognition in videos. In Advances in neural information processing systems,
pages 568–576, 2014.

53. Jure Sokolic, Qiang Qiu, Miguel RD Rodrigues, and Guillermo Sapiro. Learning to
succeed while teaching to fail: Privacy in closed machine learning systems. arXiv
preprint arXiv:1705.08197, 2017.

54. Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of
101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402,
2012.

55. Christian Szegedy, Vincent Vanhoucke, Sergey Ioﬀe, Jon Shlens, and Zbigniew
Wojna. Rethinking the inception architecture for computer vision. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2818–
2826, 2016.

56. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv
preprint arXiv:1312.6199, 2013.

57. Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface:
Closing the gap to human-level performance in face veriﬁcation. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, 2014.

58. Shuai Tao, Mineichi Kudo, and Hidetoshi Nonaka. Privacy-preserved behavior anal-
ysis and fall detection by an infrared ceiling sensor network. Sensors, 12(12):16920–
16936, 2012.

59. TechCrunch. Amazon’s camera-equipped echo look raises new questions about

smart home privacy. http://alturl.com/7ewnu.

60. Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri.
Learning spatiotemporal features with 3d convolutional networks. In Computer
Vision (ICCV), 2015 IEEE International Conference on, pages 4489–4497. IEEE,
2015.

61. Zhangyang Wang, Shiyu Chang, Yingzhen Yang, Ding Liu, and Thomas S Huang.
Studying very low resolution recognition using deep networks. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, 2016.

62. Daniel Weinland, Remi Ronfard, and Edmond Boyer. Free viewpoint action recog-
nition using motion history volumes. Computer vision and image understanding,
104(2):249–257, 2006.

63. Thomas Winkler, Ad´am Erd´elyi, and Bernhard Rinner. Trusteye. m4: protecting
the sensornot the camera. In Advanced Video and Signal Based Surveillance (AVSS),
2014 11th IEEE International Conference on, pages 159–164. IEEE, 2014.

64. Xiang Xiang and Trac D Tran. Linear disentangled representation learning for

facial actions. arXiv preprint arXiv:1701.03102, 2017.

65. Yanchun Xie, Jimin Xiao, Tammam Tillo, Yunchao Wei, and Yao Zhao. 3d video
super-resolution using fully convolutional neural networks. In Multimedia and Expo
(ICME), 2016 IEEE International Conference on, pages 1–6. IEEE, 2016.

Privacy-Preserving Visual Recognition via Adversarial Training

19

66. Ryo Yonetani, Vishnu Naresh Boddeti, Kris M Kitani, and Yoichi Sato. Privacy-
preserving visual learning using doubly permuted homomorphic encryption. arXiv
preprint arXiv:1704.02203, 2017.

67. Kiwon Yun, Jean Honorio, Debaleena Chattopadhyay, Tamara L. Berg, and Dimitris
Samaras. Two-person interaction detection using body-pose features and multiple
instance learning. In IEEE Computer Vision and Pattern Recognition Workshops
(CVPRW), 2012.

68. Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transfer-
able architectures for scalable image recognition. arXiv preprint arXiv:1707.07012,
2017.

Appendix A Adversarial Training Algorithm

Algorithm 1 outlines a complete and uniﬁed adversarial training algorithm using
the ensemble of M budget models, with restarting. If we choose M = 1 and skip
the restarting step, it is reduced to the basic adversarial training framework.

The algorithm could also be viewed as a 3-competitor game: fd as an obfus-
cator, fb (or the ensemble) as an attacker, and fT as an utilizer. Algorithm 1
then essentially solves the following two optimization problems iteratively (single
fb case for example):

LT (fT (fd(X)), YT ) − γH(fb(fd(X))),

min
fd,fT
min
fb∈P

LB(fb(fd(X)), YB).

(4)

(5)

where both LT and LB are softmax functions, H is the entropy function. In the
M -ensemble case, (5) will search for the worst case to minimize.

Appendix B Experiments on SBU

B.1 Results for Methods 1

The proposed identity-preserving action recognition task on SBU is a very
challenging one, since videos are taken in highly controlled indoor environments
and all actors are clearly viewable in the central regions of each frame. The
identity recognition task can also utilize information other than faces: the body
shape and even clothes colors are invariant for the same actor across diﬀerent
videos/actions. Diﬀerent actors wear very distinct clothes with diﬀerent colors
and textures. Table 1 displays the trade-oﬀ numbers at diﬀerent downsampling
ratios s, for Methods 1.

B.2 Two-Fold Evaluation Results for Methods 2 and 3

Table 2 displays the details numbers, for the second part of our proposed two-fold
evaluation, with N = 10 models. The top sub-table is for Method 2, and the
bottom sub-table for Method 3.

20

A conference version of this paper is accepted by ECCV’18

Algorithm 1 Adversarial Training for Privacy-Preserving Visual Recognition.

Given pre-trained active degradation module fd, target task module fT , and M
budget modules {f 1
for number of training iterations do

b , · · · f M
b }

Sample a mini-batch of k examples {X1, · · · , Xk}
Update active degradation module fd (weights wd) with stochastic gradients:
(cid:46) Suppress only the most conﬁdent one among all M budget models

[LT (fT (fd(Xj)), YT j) + γ max

Lb(f i

b(fd(Xj)), YBj) + α||fd(Xj) −

i∈{1,··· ,M }

∇wd

1
k

k
(cid:88)

j=1

Xj||1]

(cid:46) The L1 Loss term is only used in the SBU experiment

while target task validation accuracy ≤ Threshold1 do

(cid:46) Threshold1 = 90% for SBU and 70% for UCF101/VISPR

Sample mini-batch of k examples {X1, · · · , Xk}
Update target task module fT (weights wT ) and active degradation
module fd (weights wd), with stochastic gradients:

(cid:46) Avoid too weak competitor on the fT side.

∇wT

1
k

k
(cid:88)

j=1

1
k

k
(cid:88)

j=1

LT (fT (fd(Xj)), YT j), ∇wd

LT (fT (fd(Xj)), YT j)

end while
while budget task training accuracy ≤ Threshold2 do

Sample mini-batch of k examples {X1, · · · , Xk}
Update budget task module fb (weights wb) by stochastic gradients:

(cid:46) Avoid too weak competitor on the fb side.

(cid:46) Threshold2 = 95% for both datasets

∇wb

1
k

k
(cid:88)

M
(cid:88)

j=1

i=1

Lb(f i

b(fd(Xj)), YBj)

end while
if current training iteration % 100 = 0 then

(cid:46) We empirically restart all M budget models every 100 iterations

Restart all M budget models, and repeat Algorithm 1 from the beginning.

end if

end for

Privacy-Preserving Visual Recognition via Adversarial Training

21

The corresponding action recognition results, i.e. the ﬁrst part of two-fold

evaluation, are also attached after either sub-Table.

We want to make an additional note here: for Methods 1, 4 and 5, the
privacy prediction is evaluated using only one model; while in Methods 2 and 3,
the privacy suppression eﬀect is evaluated using the highest achievable number
among N = 10 diﬀerent models. Therefore, the evaluation protocol for Methods
2 and 3 is “stricter”, and its gain on privacy protection compared to Methods 1,
4, 5 will be essentially “underestimated”, if we just directly compare accuracy
numbers.

B.3 Visualization Examples of Learned Degradations on SBU

Please refer to Figure 5 for visualized examples of learned fd(X).

Table 1: The action recognition and actor pair recognition accuracies w.r.t. the spatial downsam-
pling ratio s, using pre-trained C3D net and MobileNet.

Method 1

s=1 s=2 s=3 s=4 s=6 s=8 s=14 s=16 s=28 s=56
Action 88.83 87.90 86.98 81.86 79.53 74.88 65.12 64.37 56.28 33.49

(RGB Downsampling) Actor 98.87 97.23 96.45 95.50 95.24 94.11 93.94 92.15 90.28 60.93

Appendix C Experiments on UCF-101 / VISPR

C.1 “Transferability” Study of Privacy Attributes between

UCF-101 and VISPR

Selection of 17 and 7 Privacy Attributes There are 13,421 videos in the
UCF-101 dataset. For each video, we evaluate it using the privacy attribute
prediction model pretrained on VISPR dataset: see the statistic plot in Figure
Figure 6, we observe that there are 43 attributes that can be found at least once
in UCF101 videos. But only 17 out of the 43 are frequently occurring. These
17 attributes are {age approx, weight approx, height approx, gender, eye color,
hair color, face complete, face partial, semi-nudity, race, color, occupation, hob-
bies, sports, personal relationship, social relationship, safe}.

Among the 17 frequent attributes, we carefully select 7 privacy attributes that
best ﬁt the smart home setting. These 7 attributes are {semi-nudity, occupation,
hobbies, sports, personal relationship, social relationship}.

Privacy Attribute Examples in UCF-101 In Figure 7, we show some ex-
ample frames from UCF101 with privacy attributes predicted using the VISPR-
pretrained model. In each example, the right column denotes the predicted privacy
attributes (as deﬁned in the VISPR dataset [40]) and associated conﬁdences from

22

A conference version of this paper is accepted by ECCV’18

Original RGB Frame from UCF-101 (Label: HandStandPushup)

Method 2, M=1

Method 2, M=2

Method 2, M=4

Method 2, M=6

Method 3, M=1

Method 3, M=2

Method 3, M=4

Method 3, M=6

Method 2, M=8

Method 2, M=10

Method 2, M=12

Method 2, M=14

Method 3, M=8

Method 3, M=10

Method 3, M=12

Method 3, M=14

Fig. 5: Example frames after applying the learned degradation on SBU

Privacy-Preserving Visual Recognition via Adversarial Training

23

Table 2: SBU Two Fold Evaluation

70.3
71.4
61.9
71.4

65.4
67.6
62.1
66.9

resnet v1 50
resnet v1 101
resnet v2 50
resnet v2 101

M=1 M=2 M=4 M=6 M=8 M=10 M=12 M=14 M=16 M=18
65.1
67.2
59.3
68.3
70.8
66.8
69.4
57.0
69.7
68.3
63.3
64.9
60.8
62.3
62.6
66.1
60.0
64.2
68.9
69.6
65.7
63.2
71.2
mobilenet v1 100 73.6 71.8 72.9 65.4
66.5
61.1
66.3
70.9
mobilenet v1 075 71.3 72.4 71.4
64.2
63.5
60.5
58.9
66.4
62.1
59.1
68.7
60.3
67.6
63.2
70.6
69.6 71.6 67.2
64.2
70.7
66.4
70.2
71.9

66.7
60.6
mobilenet v1 050‡ 71.2
mobilenet v1 025‡ 70.6

61.7
62.5
61.1
64.9
65.4
66.3
61.8
61.1
65.2
65.8

65.8
63.0
58.4
65.2
67.5
66.1
58.5
62.3
67.5
69.8

62.4
59.2
62.9
64.8
67.3
65.5
57.4
61.6
64.4
65.5

inception v1
inception v2

60.8
61.3
70.5
71.5

C3D

83.2

84.1

82.7

83.6

80.8

88.3

82.7

83.3

83.5

82.6

resnet v1 50
resnet v1 101
resnet v2 50
resnet v2 101

47.2
54.6
49.7
38.9
mobilenet v1 100 60.5 55.8
mobilenet v1 075 58.2
51.3
44.2
mobilenet v1 050‡ 58.2
mobilenet v1 025‡ 54.8

M=1† M=2† M=4† M=6† M=8† M=10† M=12† M=14† M=16† M=18†
42.8
55.5
41.9
57.2
49.7
44.9
43.8
42.3
42.3
41.5
44.8
54.4
47.7 45.3
44.1
46.9
41.2
42.5
44.3
45.9
41.2
43.6
44.7
52.9 52.5 43.5

54.1
46.9
40.2
51.2
52.9
40.8
49.2
44.9
51.2
49.8
57.9 52.4
51.1
54.4
45.8
44.9
49.4
42.4
38.2
56.2 54.6 46.6
54.3

44.2
44.7
57.8
44.02
42.8
45.2
44.8
41.0
38.5
41.1

37.3
42.2
40.9
39.6
41.9
41.2
35.3
39.4
34.2
42.5

38.4
41.7
40.4
42.0
43.1
41.8
38.8
42.5
39.3
42.6

32.4
34.5
35.2
50.6
41.8
40.2
45.8
47.1
35.8
38.5

inception v1
inception v2

C3D

81.7

82.6

78.0

82.8

82.2

82.1

83.5

83.1

82.6

82.3

‡ stands for training from scratch instead of ﬁne-tuning and † stands for budget model restarting.

24

A conference version of this paper is accepted by ECCV’18

Fig. 6: Attribute-wise occurrence statistics on UCF-101 videos, evaluated using
the pretrained privacy prediction model on VISPR.

the left column frames, showing a high risk of privacy leak in daily common videos.
We qualitatively examine a large number of UCF-101 frames and determine that
privacy attributes prediction are highly reliable.

C.2 UCF-101 / VISPR Two-Fold Evaluation

The trade-oﬀ results between UCF-101 with VISPR-17 and VISPR-7 are found in
Tables 3 and 4, respectively. Note that for the N =10 privacy attribute prediction
evaluation, the results are in class-based MAP (cMAP) rather than recognition
accuracy.

C.3 Visualization Examples of Learned Degradation on UCF-101 /

VISPR

For visualized examples of learned fd(X), please refer to Figure 8 for VISPR-17
and VISPR-7.

Privacy-Preserving Visual Recognition via Adversarial Training

25

(a) ApplyLipStick

(b) BabyCrawling

(c) PlayingPiano

(d) ShavingBeard

(e) Situp

(f) YoYo

Fig. 7: Privacy attributes prediction on example frames from UCF101. The right
column denotes the predicted privacy attributes (as deﬁned in the VISPR dataset
[40]) and associated conﬁdences from the left column frames, showing a high risk
of privacy leak in daily common videos.

26

A conference version of this paper is accepted by ECCV’18

Original RGB Frame from UCF-101 (Label: HandStandPushup)

Method 2, M=1
(VISPR-17)

Method 2, M=2
(VISPR-17)

Method 2, M=3
(VISPR-17)

Method 2, M=4
(VISPR-17)

Method 3, M=1
(VISPR-17)

Method 3, M=2
(VISPR-17)

Method 3, M=3
(VISPR-17)

Method 3, M=4
(VISPR-17)

Method 2, M=1
(VISPR-7)

Method 2, M=2
(VISPR-7)

Method 2, M=3
(VISPR-7)

Method 2, M=4
(VISPR-7)

Method 3, M=1
(VISPR-7)

Method 3, M=2
(VISPR-7)

Method 3, M=3
(VISPR-7)

Method 3, M=4
(VISPR-7)

Fig. 8: Example frames after applying the learned degradation on UCF-101 with
adversarial training on VISPR-17 and VISPR-7

Privacy-Preserving Visual Recognition via Adversarial Training

27

Table 3: UCF-101 / VISPR-17 Two-Fold Evaluation

resnet v1 50
resnet v1 101
resnet v2 50
resnet v2 101

M=1 M=1† M=2 M=2† M=3 M=3† M=4 M=4†
66.68 63.45 62.12 63.78 65.59 62.12 65.12 59.83
65.78 59.24 62.48 61.29 59.59 61.23 64.21 61.49
62.12 65.28 66.94 62.48 59.59 59.56 62.34 60.47
59.12 61.45 57.59 59.43 58.32 61.43 64.23 59.48
mobilenet v1 100 63.45 58.48 62.69 61.47 64.39 61.59 65.01 57.43
mobilenet v1 075 62.23 62.48 64.28 59.47 60.27 58.57 55.48 57.57
58.32 62.49 59.39 64.82 63.57 61.39 63.58 58.46
65.79 61.28 64.52 63.58 60.49 63.58 60.25 59.39
mobilenet v1 050‡ 65.12 60.25 64.29 59.49 62.48 63.58 63.58 62.06
mobilenet v1 025‡ 62.54 63.59 62.58 62.46 60.47 59.20 58.27 61.36

inception v1
inception v2

C3D

66.58 66.36 64.46 65.27 65.28 65.89 66.59 65.83

‡ stands for training from scratch instead of ﬁne-tuning and † stands for budget model
restarting

Table 4: UCF-101 / VISPR-7 Two-Fold Evaluation

resnet v1 50
resnet v1 101
resnet v2 50
resnet v2 101

M=1 M=1† M=2 M=2† M=3 M=3† M=4 M=4†
40.68 38.24 38.45 35.67 35.34 32.54 35.58 33.41
32.21 37.69 37.31 36.21 37.35 34.53 37.48 32.67
33.46 37.13 39.94 36.28 32.59 34.13 36.69 33.46
35.25 34.49 32.58 35.38 38.59 35.16 37.24 31.53
mobilenet v1 100 33.28 35.24 37.54 32.48 31.59 28.36 32.48 29.57
mobilenet v1 075 28.59 34.58 38.23 31.59 35.38 30.94 29.58 32.58
35.28 37.56 36.84 27.48 29.48 30.48 32.04 34.48
38.47 36.39 35.29 30.92 28.59 33.59 35.38 29.58
mobilenet v1 050‡ 38.49 28.49 32.56 33.48 31.58 32.58 38.32 33.48
mobilenet v1 025‡ 35.47 38.42 34.93 31.28 33.37 34.78 33.57 30.08

inception v1
inception v2

C3D

65.16 65.58 64.53 66.46 65.38 64.28 64.83 65.37

‡ stands for training from scratch instead of ﬁne-tuning and † stands for budget model
restarting

