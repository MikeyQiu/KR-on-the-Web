Newton-based maximum likelihood estimation
in nonlinear state space models

Manon Kok∗, Johan Dahlin†, Thomas B. Sch¨on‡ and Adrian Wills§

September 14, 2015

Abstract

Maximum likelihood (ML) estimation using Newton’s method in nonlinear state space models (SSMs)

is a challenging problem due to the analytical intractability of the log-likelihood and its gradient

and Hessian. We estimate the gradient and Hessian using Fisher’s identity in combination with a

smoothing algorithm. We explore two approximations of the log-likelihood and of the solution of

the smoothing problem. The ﬁrst is a linearization approximation which is computationally cheap,

but the accuracy typically varies between models. The second is a sampling approximation which is

asymptotically valid for any SSM but is more computationally costly. We demonstrate our approach

for ML parameter estimation on simulated data from two diﬀerent SSMs with encouraging results.

Keywords: Maximum likelihood, parameter estimation, nonlinear state space models, Fisher’s iden-

tity, extended Kalman ﬁlters, particle methods, Newton optimization.

The data and source code are available at http://users.isy.liu.se/en/rt/manko/ and GitHub:

https://github.com/compops/newton-sysid2015.

5
1
0
2
 
p
e
S
 
1
1
 
 
]

O
C

.
t
a
t
s
[
 
 
2
v
5
5
6
3
0
.
2
0
5
1
:
v
i
X
r
a

∗Department of Electrical Engineering, Link¨oping University, Link¨oping, Sweden. E-mail: manon.kok@liu.se.
†Department of Electrical Engineering, Link¨oping University, Link¨oping, Sweden. E-mail: johan.dahlin@liu.se.
‡Department of Information Technology, Uppsala University, Uppsala, Sweden. E-mail: thomas.schon@it.uu.se.
§School of Engineering, University of Newcastle, Australia. E-mail: adrian.g.wills@gmail.com.

1

1

Introduction

Maximum likelihood (ML) parameter estimation is a ubiquitous problem in control and system identiﬁ-

cation, see e.g. Ljung [1999] for an overview. We focus on ML estimation in nonlinear state space models

(SSMs),

by solving

where xt and yt denote the latent state variable and the measurement at time t, respectively. The

functions fθ(·) and gθ(·) denote the dynamic and the measurement model, respectively, including the

noise terms denoted by vt and et. The ML estimate of the static parameter vector θ ∈ Θ ⊆ Rp is obtained

xt+1 = fθ(xt, vt),

yt = gθ(xt, et),

(cid:98)θML = argmax

(cid:96)θ(y1:N ),

θ

(1)

(2)

where (cid:96)θ(y1:N ) (cid:44) log pθ(y1:N ) denotes the log-likelihood function and y1:N = {y1, . . . , yN }.

In this paper, we aim to solve (2) using Newton methods [Nocedal and Wright, 2006]. These enjoy

quadratic convergence rates but require estimates of the log-likelihood and its gradient and Hessian.

For linear Gaussian SSMs, we can compute these quantities exactly by the use of a Kalman ﬁlter (KF;

Kalman, 1960) and by using a Kalman smoother together with Fisher’s identity [Fisher, 1925]. This

approach has previously been explored by e.g. Segal and Weinstein [1989]. An alternative approach is

to compute the gradient recursively via the sensitivity derivatives [˚Astr¨om, 1980]. However, neither of

these approaches can be applied for general SSMs as they require us to solve the analytically intractable

state estimation problem for (1).

The main contribution of this paper is an extension of the results by Segal and Weinstein [1989] to

general SSMs in order to solve (2). To this end, we explore two approximations of the log-likelihood and

the solution of the smoothing problem, using linearization and using sampling methods. The linearization

approximation estimates the log-likelihood using an extended Kalman ﬁlter (EKF), see e.g. Gustafsson

[2012]. The smoothing problem is solved using Gauss-Newton optimization. The sampling approximation

is based on particle ﬁltering and smoothing [Doucet and Johansen, 2011].

The main diﬀerences between these two approximations are the accuracy of the estimates and the

computational complexity. Sampling methods provide asymptotically consistent estimates, but at a

high computational cost. It is therefore beneﬁcial to investigate the linearization approximation, which

provides estimates at a much smaller computational cost. However, the accuracy of the linearization

typically varies between diﬀerent models, requiring evaluation before they can be applied.

2

The problem of ML estimation in nonlinear SSMs is widely considered in literature. One method

used by e.g. Kok and Sch¨on [2014] approximates the log-likelihood in (2) using an EKF. The ML es-

timation problem is subsequently solved using quasi-Newton optimization for which the gradients are

approximated using ﬁnite diﬀerences. This results in a rapidly increasing computational complexity as

the number of parameters grows. Other approaches are based on gradient ascent algorithms together

with particle methods, see e.g. Poyiadjis et al. [2011] and Doucet et al. [2013]. These approaches typi-

cally require the user to pre-deﬁne step sizes, which can be challenging in problems with a highly skewed

log-likelihood. Lastly, gradient-free methods based on simultaneous perturbation stochastic approxi-

mation [?], Gaussian processes optimization [Dahlin and Lindsten, 2014] and expectation maximiza-

tion [Sch¨on et al., 2011, Kokkala et al., 2014] can be used. However, these methods typically do not

enjoy the quadratic convergence rate of Newton methods, see e.g. Poyiadjis et al. [2011] for a discussion.

2 Strategies for computing derivatives of the log-likelihood

A Newton algorithm to solve (2) iterates the update

θk+1 = θk − εk

H(θk)

(cid:104)

(cid:105)−1(cid:104)

(cid:105)
G(θk)

,

G(θk) (cid:44) ∂

H(θk) (cid:44) Ey1:N

∂θ log pθ(y1:N )(cid:12)
(cid:104) ∂2
∂2θ log pθ(y1:N )(cid:12)

(cid:12)θ=θk

,

(cid:105)

,

(cid:12)θ=θk

(3)

where εk denotes a step length determined for instance using a line search algorithm [Nocedal and Wright,

2006] or using an update based on stochastic approximation [Poyiadjis et al., 2011]. Here, we introduce

the notation G(θk) and H(θk) for the gradient and the Hessian of the log-likelihood, respectively. In

Algorithm 1, we present the full procedure for solving the ML problem (2) using (3).

To make use of Algorithm 1, we require estimates of the gradient and the Hessian. We estimate the

gradient via Fisher’s identity,

(cid:90)

G(θ) =

∂
∂θ log pθ(x1:N , y1:N )pθ(x1:N |y1:N ) dx1:N .

(4)

Here, computation of the gradient of the log-likelihood amounts to a marginalization of the gradient of the

complete data log-likelihood with respect to the states. As the states are unknown, this marginalization

cannot be carried out in closed form, which is the reason why the gradients cannot be computed exactly.

The complete data likelihood for an SSM is given by

pθ(x1:N , y1:N ) = pθ(x1)

fθ(xt+1|xt)

gθ(yt|xt),

(5)

N
(cid:89)

t=1

N −1
(cid:89)

t=1

3

Algorithm 1 Newton method for ML parameter estimation
Inputs: Initial parameter θ0, maximum no. iterations K.
Outputs: ML parameter estimate (cid:98)θML.

1: Set k = 0

2: while exit condition is not satisﬁed do

a: Run an algorithm to estimate the log-likelihood (cid:98)(cid:96)(θk), its gradient (cid:98)G(θk) and its Hessian (cid:98)H(θk).
b: Determine εk using e.g. a line search algorithm or a stochastic schedule.
c: Apply the Newton update (3) to obtain θk+1.
d: Set k = k + 1.

end while

3: Set (cid:98)θML = θk.

where pθ(x1), fθ(xt+1|xt) and gθ(yt|xt) are given by (1). In this paper, we assume that we can compute

the gradient of the logarithm of this expression with respect to the parameter vector. By inserting (5)

into (4), we can make use of the Markov property of the model to obtain

G(θk) =

N
(cid:88)

(cid:90)

t=1

(cid:124)

ξθk (xt+1:t) (cid:44) ∂

ξθk (xt+1:t)pθk (xt+1:t|y1:N ) dxt+1:t
(cid:123)(cid:122)
(cid:125)
(cid:44)Gt(θk)
∂θ log fθ(xt+1|xt)(cid:12)

+ ∂

(cid:12)θ=θk

,

∂θ log gθ(yt|xt)(cid:12)

(cid:12)θ=θk

,

where we interpret fθ(x1|x0) = p(x1) for brevity. The remaining problem is to estimate the two-step

joint smoothing distribution pθk (xt+1:t|y1:N ) and insert it into the expression.

Furthermore, the Hessian can be approximated using the gradient estimates according to Segal and

Weinstein [1989] and Meilijson [1989]. The estimator is given by

(cid:98)H(θk) =

(cid:104)

1
N

(cid:105)(cid:104)

(cid:105)(cid:62)

G(θk)

G(θk)

−

(cid:104)
Gt(θk)

(cid:105)(cid:104)

Gt(θk)

(cid:105)(cid:62)

N
(cid:88)

t=1

which is a consistent estimator of the expected information matrix. That is, we have that (cid:98)H(θ(cid:63)) → H(θ(cid:63))

as N → ∞ for the true parameters θ(cid:63) if the gradient estimate (cid:98)G(θ) is consistent. The advantage of this

estimator is that Hessian estimates are obtained as a by-product from the estimation of the gradient.

Both estimators in (6) and (7), require estimates of the intractable two-step smoothing distribution.

In the two subsequent sections, we discuss the details of how to make use of the linearization and sampling

approximations to estimate this smoothing distribution.

(6)

(7)

4

3 Linearization approximation

To make use of linearization approximations to estimate the gradient, we treat (6) as an expected value

of the form

G(θk) =

(cid:104)

Eθk

ξθk (xt+1:t)(cid:12)

(cid:12)y1:N

(cid:105)

.

N
(cid:88)

t=1

In Section 3.1, we ﬁrst consider the linear case and recapitulate the calculation in Segal and Weinstein

[1989]. These results are extended to nonlinear SSMs with Gaussian additive noise in Section 3.2.

3.1 Linear models with additive Gaussian noise

The linear Gaussian SSM is given by the following special case of (1),

xt+1 = F (θ)xt + vt(θ),

vt(θ) ∼ N (0, Q(θ)),

yt = G(θ)xt + et(θ),

et(θ) ∼ N (0, R(θ)).

For notational brevity, we assume that the initial state is distributed as x1 ∼ N (µx, P1) and is indepen-

dent of the parameters θ. For this model, we can express the ML problem as

(cid:98)θML = argmax

θ

N
(cid:88)

t=1

log N

(cid:16)
yt; (cid:98)yt|t−1(θ), St(θ)

(cid:17)
,

where it is possible to compute the predictive likelihood (cid:98)yt|t−1(θ) and its covariance St(θ) using a KF.
To solve (10) using Algorithm 1, the gradient and Hessian of the log-likelihood need to be computed.

Using (8), we ﬁrst note that the complete data log-likelihood can be expressed as

log pθ(x1:N , y1:N ) = − N −1

log det Q − N

2

2 log det R−

1

2 (cid:107)x1 − µx(cid:107)2

P −1
1

− 1

1
2

N −1
(cid:88)

t=1

2 log det P1−
N
(cid:88)

Q−1 − 1
2

t=1

(cid:107)xt+1 − F xt(cid:107)2

(cid:107)yt − Gxt(cid:107)2

R−1 ,

(11)

where the explicit dependency on θ has been omitted for notational simplicity. The gradient of the

(8)

(9)

(10)

5

log-likelihood (8) can then be written as

(cid:98)G(θ) = − N −1

2 Tr (cid:0)Q−1 ∂Q
(cid:32) N
(cid:88)

∂θ

(cid:1) − N

2 Tr (cid:0)R−1 ∂R
∂θ
(cid:33)

(cid:1) −

(Pt|N + (cid:98)xt|N (cid:98)xT

t|N ) ∂Q−1

∂θ

−

t=2
(cid:32) N
(cid:88)

t=2

(cid:32) N
(cid:88)

t=2

1
2 Tr

1
2 Tr

Tr

N
(cid:88)

t=1

1
2 Tr

(Pt−1|N + (cid:98)xt−1|N (cid:98)xT

t−1|N ) ∂F TQ−1F

∂θ

(Pt−1,t|N + (cid:98)xt−1|N (cid:98)xT

t|N ) ∂Q−1F

∂θ

(cid:33)

+

(cid:33)

+

N
(cid:88)

t=1

yT
t

∂R−1G

∂θ (cid:98)xt|N − 1

2

yT
t

∂R−1
∂θ yt−

(cid:32) N
(cid:88)

t=1

(Pt|N + (cid:98)xt|N (cid:98)xT

t|N ) ∂GTR−1G

∂θ

(cid:33)

,

(12)

(13)

where (cid:98)xt|N and Pt|N denote the smoothed state estimates and their covariances, respectively. The term
Pt−1,t|N denotes the covariance between the states xt−1 and xt. These quantities can be computed using

a Kalman smoother, see e.g. Rauch et al. [1965].

3.2 Nonlinear models with additive Gaussian noise

A slightly more general SSM is given by

xt+1 = fθ(xt) + vt(θ),

vt(θ) ∼ N (0, Q(θ)),

yt = gθ(xt) + et(θ),

et(θ) ∼ N (0, R(θ)).

For this model, the ML problem assumes the same form as in Section 3.1, but the predictive likelihood

(cid:98)yt|t−1(θ) and its covariance St(θ) cannot be computed exactly. Instead, we replace them with estimates
obtained from an EKF.

To compute the gradient and the Hessian of the log-likelihood, assume for a moment that we can

compute the complete data log-likelihood exactly. The gradient (8) can then be computed exactly in

two special cases:

1. When part of the SSM (13) is linear and the parameters only enter in the linear part of the model.

In this case, the gradients reduce to their respective linear parts in (12).

2. When the expectation in (8) can be computed exactly even though the model is nonlinear, for

example in the case of quadratic terms.

The assumption that the complete data log-likelihood can be computed exactly is clearly not true for

nonlinear SSMs. However, good estimates of (cid:98)xt|N can be obtained by solving the optimization problem

(cid:98)xt|N = argmax

x1:N

pθk (x1:N , y1:N ).

(14)

6

Algorithm 2 Computation of the log-likelihood and its derivatives using linearization approximation
Inputs: A parameter estimate θk
Outputs: An estimate of the log-likelihood (cid:98)(cid:96)(θk) and its gradient (cid:98)G(θk) and Hessian (cid:98)H(θk).

1: Run an EKF using θk to obtain (cid:98)(cid:96)(θk).

2: Solve the smoothing problem (14).

a: Initialize x1

1:N |N using the state estimates from the EKF.

b: while exit condition is not satisﬁed do
i: Compute the gradient J (xi

using (15).

ii: Use a Gauss-Newton update similar to (3) to obtain xi+1
t|N .
iii: Set i = i + 1.

t|N ) and approximate the Hessian of the smoothing problem

end while

c: Set (cid:98)x1:N |N = xi+1

1:N |N .

3: Compute Pt,t|N and Pt−1,t|N using (15).

4: Use (8) to estimate the gradient (cid:98)G(θk) and use (7) to determine the Hessian (cid:98)H(θk).

This is a nonlinear least-squares problem and can be solved eﬃciently using a standard Gauss-Newton

solver due to its inherent sparsity. The state covariances Pt|N and Pt−1,t|N can be approximated from

the inverse of the approximate Hessian of the complete data log-likelihood

P1:N,1:N |N ((cid:98)xt|N ) ≈

J ((cid:98)xt|N ) (cid:44) ∂

(cid:18)(cid:104)

(cid:105)(cid:104)

(cid:105)T(cid:19)−1

J ((cid:98)xt|N )

J ((cid:98)xt|N )
(cid:12)
(cid:12)
∂x pθk (x1:N , y1:N )
(cid:12)x1:N =(cid:98)x1:N

,

,

(15)

where P1:N,1:N |N represents the smoothed covariance matrix representing the covariance between all

states. Since we are only interested in Pt|N (which is short notation for Pt,t|N ) and Pt−1,t|N , we are

only interested in a few components of P1:N,1:N |N . Hence, it is not necessary to form an explicit inverse,

which would be intractable for large N .

4 Sampling approximation

An alternative approximation of the log-likelihood and the solution to the smoothing problem uses

sampling methods. The approximation is based on particle ﬁltering and smoothing and can be applied

to more general nonlinear SSMs than the special cases introduced in Section 3.

Particle methods are a speciﬁc instance of sequential Monte Carlo (SMC; Doucet and Johansen,

2011) methods, when this family of algorithms is applied on general SSMs. The particle ﬁlter (PF) is

a combination of sequential importance sampling and resampling applied to estimate the states of an

7

Algorithm 3 Computation of the log-likelihood and its derivatives using sampling approximation
Inputs: Parameter estimate θk and no. particles M .
Outputs: Estimate of gradient (cid:98)G(θk) and Hessian (cid:98)H(θk).

1: Run a bPF using θk to estimate the particle system.
(all operations are carried out over i, j = 1, . . . , M )

a: Sample x(i)
b: for t = 2 to N do

1 ∼ pθk (x1) and compute w(i)

1 by Step iii.

j(cid:1) = w(j)
t−1.

ii: Propagation: Sample x(i)
iii: Weighting: Calculate ¯w(i)
endfor

t ∼ fθk
t = gθk

(cid:1).

t

t
t−1

(cid:12)xa(i)
(cid:12)
(cid:0)x(i)
(cid:0)yt|x(i)

t

2: Run a smoothing algorithm, e.g. FL or FFBSi.

i: Resampling: sample a new ancestor a(i)
t

from a multinomial distribution with P(cid:0)a(i)

t =

(cid:1), which by normalisation (over i) gives w(i)

.

t

3: Use (19) or (21) together with (7) to estimate the gradient (cid:98)G(θk) and the Hessian (cid:98)H(θk).

SSM. For example, the two-step smoothing distribution required in (6) can be estimated by

(cid:98)pθ(dxt+1:t|y1:N ) (cid:44)

w(i)

N δx(i)

t+1:t

(dxt+1:t),

M
(cid:88)

i=1

(16)

and x(i)
where w(i)
t
system ΓM = (cid:8)x(i)

t denote the weights and locations of the particles obtained by the PF. The particle
1:N , w(i)

(cid:9)M
i=1 required to compute (16) is obtained by an iterative algorithm with three
steps: resampling, propagation and weighting. The most commonly used PF is the bootstrap particle

1:N

ﬁlter (bPF), which is summarized in Step 1 of Algorithm 3.

The estimator in (16) can be directly inserted into (6) to estimate the gradient of the log-likelihood.

However, this typically results in estimates suﬀering from high variance due to the problem of path

degeneracy. This is a result of the successive resamplings of the particle system (Step i in Algorithm 3)

that collapse the particle trajectories. Hence, all particles share a common ancestor, i.e. eﬀectively we

have M ≈ 1.

Alternative methods instead rely on particle smoothing to estimate the two-step smoothing distri-

bution, which results in estimators with better accuracy but with an increase in the computational

complexity. In this paper, we consider two diﬀerent particle smoothers: the ﬁxed-lag (FL; Kitagawa and

Sato, 2001) smoother and the forward ﬁlter backward simulator (FFBSi). There are numerous other

alternatives and we refer to Lindsten and Sch¨on [2013] for a survey of the current state-of-the-art.

8

4.1 Fixed-lag smoothing

The FL smoother relies upon the assumption that the SSM is mixing fast and forgets about its past within

a few time steps. More speciﬁcally, this means that we can approximate the two-step joint smoothing

distribution by

where κt = max(N, t + 1 + ∆) for some ﬁxed-lag 0 < ∆ ≤ N . The resulting estimator has the form

(cid:98)pθk (dxt+1:t|y1:N ) ≈ (cid:98)pθk (dxt+1:t|y1:κt),

(cid:98)pθk (dxt+1:t|y1:N ) (cid:44)

w(i)
κt

δ˜x(i)

κt,t+1:t

(dxt+1:t),

M
(cid:88)

i=1

(cid:98)G(θk) =

N
(cid:88)

M
(cid:88)

t=1

i=1

(cid:16)

w(i)
κt

ξθk

˜x(i)
κt,t+1:t

(cid:17)

.

(17)

(18)

(19)

where ˜x(i)

κt,t denotes the ancestor at time t of particle x(i)

κt . The gradient can subsequently be estimated

by inserting (18) into (6) to obtain

The implementation of this expression is straightforward and is summarized in Algorithm 3. See Dahlin

[2014] for the complete derivation and algorithm of the FL smoother. The FL smoother retains the

computational complexity of the PF, i.e. O(N M ). Furthermore, it enjoys improved statistical properties

under some general mixing assumptions of the SSM. A drawback with the FL smoother is a persisting

bias that does not vanish in the limit when M → ∞.

4.2 Forward ﬁlter backward simulator

The second smoother that we consider is the FFBSi algorithm, which enjoys even better statistical

properties than the FL smoother but with a computational complexity proportional to O(N M ¯M ),

where ¯M denotes the number of particles in the backward sweep. The estimates obtained with the

FFBSi smoother are asymptotically consistent and therefore unbiased in the limit when M, ¯M → ∞.

The FFBSi algorithm can be seen as the particle approximation of the Rauch-Tung-Striebel (RTS)

smoother. The gradient can be estimated using a backward sweep after a forward pass using bPF in
Algorithm 3. In the backward sweep, we draw ¯M trajectories (cid:8)˜x(j)

(cid:9) ¯M
j=1 by rejection sampling from the

1:N

backward kernel given by

(cid:98)Bt(dxt|xt+1) (cid:44)

M
(cid:88)

i=1

w(i)
t fθk (xt+1|x(i)
t )
l=1 w(l)

t fθk (xt+1|x(l)
t )

(cid:80)M

δx(i)

t

(dxt),

(20)

9

where ΓM generated by the forward pass. The gradient can subsequently be estimated as

(cid:98)G(θk) =

1
¯M

N
(cid:88)

¯M
(cid:88)

t=1

i=1

(cid:16)

˜x(i)
t+1:t

(cid:17)

.

ξθk

(21)

To decrease the computational time, we make use of the early stopping rule discussed by ?. Here, we

sample the ﬁrst Mlimit backward trajectories using rejection sampling and the remaining using standard

FFBSi. This results in a computationally eﬃcient and accurate smoothing algorithm. See Algorithm 6

in Lindsten and Sch¨on [2013] for the complete algorithm and a discussion of its statistical properties.

5 Simulation results

We evaluate the proposed methods by considering two diﬀerent SSMs. For both models, we compare the

estimates obtained using four diﬀerent methods:

1. ALG2: The Newton-based optimization in Algorithm 1 in combination with estimates of the

log-likelihood and its derivatives from Algorithm 2.

2. ALG3FL: The Newton-based optimization in Algorithm 1 in combination with estimates of

the log-likelihood and its derivatives from Algorithm 3 using an FL smoother, with ∆ = 12 and

M = 2 000. We use the sequence of step sizes given by εk = k−2/3 as suggested by Poyiadjis et al.

[2011].

instead.

3. ALG3FFBSi: Similar to ALG3FL but instead of an FL smoother we use an FFBSi smoother,

with M = 2 000, ¯M = 100 and Mlimit = 10.

4. NUM: A quasi-Newton algorithm, which computes the log-likelihood using an EKF as in Sec-

tion 3.2, but the gradients are found using ﬁnite-diﬀerences of the approximate log-likelihood

The latter method is included in the comparison because it is commonly used in ML parameter

estimation approaches where the log-likelihood is approximated using linearization, see e.g. Kok and

Sch¨on [2014] for a concrete example. It is computationally expensive for large dimensions of the parameter

vector, but is cheap for the low dimensional parameter vectors we consider here. Source code for the

simulated examples is available via the ﬁrst author’s homepage.1

10

Figure 1: Comparison of the diﬀerent methods for the SSM (22). Upper: The estimates of θ1 and θ2
using 100 data sets. Lower: trace plots of the optimization methods for one of the data sets for θ1 (left)
and θ2 (right).

11

5.1 Parameters in the linear part of the SSM

The ﬁrst model that we consider is given by

xt+1 = arctan xt + vt,

vt ∼ N (0, 1),

yt = θ1xt + θ2 + et,

et ∼ N (0, 0.12).

(22)

The unknown parameter vector is θ = {θ1, θ2}. The model (22) has nonlinear dynamics, but the

measurement equation is linear. This corresponds to a model of type 1 as discussed in Section 3.2.

As the parameters are located in the linear measurement equation, the expressions for the gradient (8)

and Hessian (7) in Algorithm 2 are equal to their linear counterparts in (12).

We simulate 100 data sets each consisting of N = 1 000 samples and true parameters θ(cid:63) = {0.5, 0.3}

to compare the accuracy of the proposed methods. All methods are initialized in θ0 = {0.7, 0.0}. The

parameter estimates from the four methods are presented in the upper plot in Fig. 1. The bias and mean

square errors (MSEs) as compared to θ(cid:63) are also represented in Table 1. Note that in the model (22),

positive and negative θk,1 are equally likely. Hence, without loss of generality we mirror all negative

solutions to the positive plane.

To illustrate the convergence of the diﬀerent methods, the parameter estimates as a function of the

iterations in the Newton method are shown in the lower plot in Fig. 1. As can be seen, all four methods

converge, but ALG2 and NUM require far less iterations than the ALG3FL and ALG3FFBSi.

For the SSM (22) it can be concluded that ALG2 outperforms the other methods both in terms of the

bias and the MSE. The reason is that, as argued in Section 3.2, accurate gradient and Hessian estimates

can be obtained for this model because of its structure. Note that ALG2 and NUM not only require the

least computational time per iteration but also require far less iterations to converge than ALF3FL and

ALG3FFBSi, making them by far the most computationally eﬃcient algorithms.

5.2 Parameters in the nonlinear part of the SSM

The second model that we consider is given by

xt+1 = θ1 arctan xt + vt,

vt ∼ N (0, 1)

yt = θ2xt + et,

et ∼ N (0, 0.12),

(23)

where θ1 scales the current state in the nonlinear state dynamics. We apply the same evaluation procedure

as for the previous model but with θ(cid:63) = {0.7, 0.5} and θ0 = {0.5, 0.7}.

Note that for this case, the gradients for ALG2 can not be computed analytically. Denoting θk,1 the

1http://users.isy.liu.se/en/rt/manko/

12

Figure 2: Comparison of the diﬀerent methods for the SSM (23). Upper: The estimates of θ1 and θ2
using 100 data sets. Lower: trace plots of the optimization methods for one of the data sets for θ1 (left)
and θ2 (right).

13

Table 1: The bias, MSE as compared to θ(cid:63) and the computational time averaged over 100 data sets of
N = 1 000 for the SSMs (22) (model 1) and (23) (model 2). The boldface font indicates the smallest
MSE and bias.

Alg.

Bias (·10−4) MSE (·10−4)

Time

1 ALG2

ALG3FL
ALG3FFBSi
NUM

2 ALG2

ALG3FL
ALG3FFBSi
NUM

l
e
d
o
M

l
e
d
o
M

θ1

10
38
31
-4

284
53
55
31

θ2

10
-214
53
48

-55
35
31
-27

θ1

1
2
1
1

28
24
24
23

θ2

10
16
11
10

2
2
2
1

(sec/iter)

0.81
4
19
0.19

2.73
5
22
0.19

estimate of θ1 at the kth iteration, we approximate the gradient with respect to θk,1 by

(cid:98)G(θk,1) =

Eθk,1

(cid:104) ∂
∂θk,1

log pθk,1(xt|xt−1)|y1:N

(cid:105)

(cid:104)

Eθk,1

− 1

2 (xt − θk,1 arctan xt−1)2 |y1:N

(cid:105)

N
(cid:88)

t=2

N
(cid:88)

t=2

N
(cid:88)

t=2

=

≈

(cid:0)

− 1
2

(cid:98)xt|N − θk,1 arctan (cid:98)xt−1|N

(cid:1)2

.

(24)

Due to the necessity of the approximation (24), we expect ALG2 to perform worse for the SSM (23) as

compared to (22). This is conﬁrmed by the results shown in Fig. 2 and summarized in Table 1. For this

model, NUM outperforms the other methods both in bias, MSE and computational time.

6 Conclusions

We have considered the problem of ML parameter estimation in nonlinear SSMs using Newton methods.

We determine the gradient and Hessian of the log-likelihood using Fisher’s identity in combination with

an algorithm to obtain smoothed state estimates. We have applied this method to simulated data from

two nonlinear SSMs. The ﬁrst SSM has nonlinear dynamics, but the measurement equation is linear.

The parameters only enter the measurement equation. Because of this, accurate parameter estimates

are obtained using linearization approximations of the log-likelihood and its gradient and Hessian. In

the second SSM, however, the parameters enter in both the linear and in the nonlinear part of the SSM.

Because of this, the linearization approximations of the gradient and Hessian are of poor quality, leading

to worse parameter estimates. For this SSM, the methods based on sampling approximations perform

considerably better. However, a computationally cheap quasi-Newton algorithm which computes the log-

likelihood using an EKF and ﬁnds the gradients using ﬁnite-diﬀerences outperforms the other methods

14

on this example. Note that this algorithm is only computationally cheap for small dimensions of the

parameter vector. Furthermore, it highly depends on the quality of the EKF state estimates.

In future work, we would like to study the quality of the estimates for a wider range of nonlinear

models. It would also be interesting to compare the method introduced in this work to a solution based

on expectation maximization. Furthermore, solving optimization problems by making use of the noisy

estimates of the gradient and the Hessian that are provided by the particle smoother is an interesting

and probably fruitful direction for future work.

15

References

K. J. ˚Astr¨om. Maximum likelihood and prediction error methods. Automatica, 16(5):551–574, 1980.

J. Dahlin. Sequential Monte Carlo for inference in nonlinear state space models. Licentiate’s thesis no.

1652, Link¨oping University, Link¨oping, Sweden, may 2014.

J. Dahlin and F. Lindsten. Particle ﬁlter-based Gaussian process optimisation for parameter inference. In

Proceedings of the 19th World Congress of the International Federation of Automatic Control (IFAC),

Cape Town, South Africa, August 2014.

A. Doucet and A. Johansen. A tutorial on particle ﬁltering and smoothing: Fifteen years later.

In

D. Crisan and B. Rozovsky, editors, The Oxford Handbook of Nonlinear Filtering. Oxford University

Press, 2011.

A. Doucet, P. E. Jacob, and S. Rubenthaler. Derivative-free estimation of the score vector and observed

information matrix with application to state-space models. Pre-print, 2013. arXiv:1304.5768.

R. A. Fisher. Theory of statistical estimation. In Mathematical Proceedings of the Cambridge Philosoph-

ical Society, volume 22, pages 700–725. Cambridge Univ Press, 1925.

F. Gustafsson. Statistical Sensor Fusion. Studentlitteratur, 2012.

R. E. Kalman. A new approach to linear ﬁltering and prediction problems. Journal of Fluids Engineering,

82(1):35–45, 1960.

Springer, 2001.

G. Kitagawa and S. Sato. Monte Carlo smoothing and self-organising state-space model. In A. Doucet,

N. de Fretias, and N. Gordon, editors, Sequential Monte Carlo methods in practice, pages 177–195.

M. Kok and T. B. Sch¨on. Maximum likelihood calibration of a magnetometer using inertial sensors. In

Proceedings of the 19th World Congress of the International Federation of Automatic Control (IFAC),

Cape Town, South Africa, August 2014.

J. Kokkala, A. Solin, and S. S¨arkk¨a. Expectation maximization based parameter estimation by sigma-

point and particle smoothing.

In Proceedings of the 17th International Conference on Information

Fusion, Salamanca, Spain, July 2014.

F. Lindsten and T. B. Sch¨on. Backward simulation methods for Monte Carlo statistical inference. In

Foundations and Trends in Machine Learning, volume 6, pages 1–143, August 2013.

L. Ljung. System Identiﬁcation, Theory for the User. Prentice Hall PTR, 2nd edition, 1999.

16

I. Meilijson. A fast improvement to the EM algorithm on its own terms. Journal of the Royal Statistical

Society. Series B (Methodological), 51(1):127–138, 1989.

J. Nocedal and S. J. Wright. Numerical Optimization. Springer Series in Operations Research, 2nd

edition, 2006.

G. Poyiadjis, A. Doucet, and S. S. Singh. Particle approximations of the score and observed information

matrix in state space models with application to parameter estimation. Biometrika, 98(1):65–80, 2011.

H. E. Rauch, C. T. Striebel, and F. Tung. Maximum likelihood estimates of linear dynamic systems.

AIAA journal, 3(8):1445–1450, 1965.

T. B. Sch¨on, A. Wills, and B. Ninness. System identiﬁcation of nonlinear state-space models. Automatica,

47(1):39–49, 2011.

35(3):682–687, 1989.

M. Segal and E. Weinstein. A new method for evaluating the log-likelihood gradient, the Hessian, and

the Fisher information matrix for linear dynamic systems. IEEE Transactions on Information Theory,

17

