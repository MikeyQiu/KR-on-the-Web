Weakly-Supervised Semantic Segmentation Network with
Deep Seeded Region Growing

Zilong Huang1, Xinggang Wang1∗, Jiasi Wang1, Wenyu Liu1, and Jingdong Wang2
1School of Electronic Information and Communications, Huazhong University of Science and Technology
2Microsoft Research Asia
{hzl,xgwang,wangjiasi,liuwy}@hust.edu.cn jingdw@microsoft.com

Abstract

This paper studies the problem of learning image seman-
tic segmentation networks only using image-level labels as
supervision, which is important since it can signiﬁcantly
reduce human annotation efforts. Recent state-of-the-art
methods on this problem ﬁrst infer the sparse and discrimi-
native regions for each object class using a deep classiﬁca-
tion network, then train semantic a segmentation network
using the discriminative regions as supervision.
Inspired
by the traditional image segmentation methods of seeded
region growing, we propose to train a semantic segmenta-
tion network starting from the discriminative regions and
progressively increase the pixel-level supervision using by
seeded region growing. The seeded region growing module
is integrated in a deep segmentation network and can bene-
ﬁt from deep features. Different from conventional deep net-
works which have ﬁxed/static labels, the proposed weakly-
supervised network generates new labels using the contex-
tual information within an image. The proposed method
signiﬁcantly outperforms the weakly-supervised semantic
segmentation methods using static labels, and obtains the
state-of-the-art performance, which are 63.2% mIoU score
on the PASCAL VOC 2012 test set and 26.0% mIoU score
on the COCO dataset.

1. Introduction

Deep Convolutional Neural Networks (DCNN) have
achieved great successes on the image semantic segmen-
tation problem [5, 18] thanks to a large amount of fully-
annotated images. However, collecting large-scale accu-
rate pixel-level annotation is time-consuming and typically
requires substantial ﬁnancial investments. Unlabeled and
weakly-labeled visual data, however, can be collected in
large amounts in a relatively fast and cheap manner. There-
fore, a promising direction in the computer vision research

∗Corresponding author.

Person
Horse

Training Data

Prediction

Ground Truth

Epoch #0

Epoch #6

Epoch #12

Figure 1. The top row orderly shows a training image with
the image-level labels, the segmentation result of our proposed
method only using image-level supervision, and the ground truth.
Our segmentation result is very close to the ground truth anno-
tated by human. The bottom row shows the dynamic supervi-
sion in several epochs during the training of the proposed weakly-
supervised semantic segmentation network. (The black represents
background and the white represents unlabeled/ignore pixels).

is to develop object recognition methods that can learn from
unlabeled or weakly labeled images [14, 32].

In this paper, we study the problem of learning seman-
tic segmentation networks from weakly-labeled images.
Among various settings of weak label, image-level anno-
tation is one of the most economical and most efﬁcient set-
In this context, every training image has its image
ting.
class/category labels.
It means objects belonging to the
class labels appear in the image. However, the locations
of the objects are unknown. We need to infer the pixel-level
locations of the objects. Thus, the main problem in training
weakly-supervised semantic segmentation networks is how
to accurately assign image-level labels to their correspond-
ing pixels.

To establish the desired pixel-label correspondence

7014

there is a very insightful research work.
in training,
Kolesnikov et al. [14] employed an image classiﬁcation
network with classiﬁcation activation maps (CAM) [37]
method to select the most discriminative regions, and used
the regions as pixel-level supervision for segmentation net-
works. Compared to the early weakly-supervised semantic
segmentation methods [22, 20], the discriminative region
based approach signiﬁcantly improved the performance of
this challenging task. However, in [14], the discriminative
regions are small and sparse as shown in the epoch #0 image
in Figure 1. In training, the supervision of the semantic seg-
mentation network is ﬁxed as the sparse discriminative re-
gions. Thus, we name the learning strategy in [14] as “static
supervision”. The static supervision setting deviates from
the requirement of semantic segmentation task that requires
accurate and complete object regions for training segmen-
tation models.

To address the issue, we propose to expand the discrim-
inative regions to cover the whole objects during training
segmentation networks. In practice, the pixels around the
discriminative regions are always belonging to the same ob-
jects because semantic labels of the same object have spa-
tial continuity. Our motivation is that, using the image la-
bels enables to ﬁnd small and sparse discriminative regions
from the object of interest, termed as “seed cues”, the neigh-
boring pixels of seed cues with similar features (e.g. color,
texture or deep features) could have the same labels as the
seed cues. We utilize the classical Seeded Region Growing
(SRG) method [1] to model this process for generating ac-
curate and complete pixel-level labels. Here we can train
semantic segmentation networks under supervision of the
pixel-level labels. Different from [14, 19], the pixel-level
labels are dynamic. The dynamic supervision is quite dif-
ferent from traditional network training using ﬁxed super-
vision. In our case, we let the network generate new labels
of the input training example, i.e., the training image. SRG
is integrated into the deep segmentation network and can
be optimized end-to-end and enjoys the deep features. We
name the proposed method as “deep seeded region growing
(DSRG)” for weakly-supervised semantic segmentation.

In practice, the seed cues localized by classiﬁcation net-
work is small but with high precision. It is a natural way
to choose the seed cues as the seed points in SRG. Besides,
to measure the similarity between the seed points and ad-
jacent pixels for region growing, we make use of the seg-
mentation map which is output of the segmentation network
as features. Thus, SRG treats the seed cues as initial seed
points; then the adjacent pixels in segmentation map with
high probabilities on their corresponding categories take the
same labels as the seed cues. This process is repeated until
there are no pixels satisfying the above constraints. In the
end, the output of DSRG is used as the supervision for train-
ing segmentation network. In the training phase, the super-

vision is used to form the loss function, termed as “seeding
loss”. In seeded regions, the loss is the same as full super-
vise loss function in [5]; the other positions are ignored by
the seeding loss.

During training, the DSRG approach gradually enriches
the supervision information of the segmentation network.
As shown in Figure 1, the supervision in epoch #0 is ac-
tually the seed cues generated by classiﬁcation model, the
cues localize the head of person and the horse, which are
the most discriminative regions in the image. With the in-
creasing of epochs, the dynamic supervision gradually ap-
proaches the ground truth and cover the whole object con-
tent precisely. Meanwhile, the dynamic supervision ides
the network to produce competitive segmentation result. To
ensure the stability of training, DSRG always choose the
original seed cues as initial seed points.

In the experiments, we demonstrate the effectiveness of
our approach on the challenging PASCAL VOC 2012 Se-
mantic Segmentation benchmark [8] and COCO, and show
that we achieve the new state-of-the-art results. In addition,
we provide an analysis of the DSRG approach by carrying
out some ablation studies.

In summary, the main contributions of this paper are

summarized below:

• In deep semantic segmentation network, we utilize the
seeded region growing [1] mechanism, which enables
the network safely generates new pixel-level labels for
weakly-supervised semantic segmentation. Besides,
the network can be optimized in an end-to-end man-
ner and is easy to train.

• Our work obtains

the

state-of-the-art weakly-
supervised semantic segmentation performance on the
PASCAL VOC segmentation benchmark and COCO
dataset. The mIoU of our method are 61.4% and
63.2% on pascal voc val set and test set respectively,
which are better than many sophisticated systems and
are getting closer to the fully supervised segmentation
system [6] (67.6/70.3% mIoU on val/test set).

The rest of this paper is organized as follows. We ﬁrst
review related work in Section 2 and describe the architec-
ture of our approach in Section 3. In Section 4, the detailed
procedure to improve the quality of dynamic supervision is
discussed and experimental results are analyzed. Section 5
presents our conclusion and future work.

2. Related work

The last years have seen a renewed interest on weakly-
supervised visual
learning. Various weakly-supervised
methods have been proposed for learning to perform se-
mantic segmentation with coarser annotations, such as im-
age labels [20, 36], points [2], scribbles [16], and bounding

7015

boxes [7, 20] etc. In this work, we focus on using image
labels as the main form of supervision, which is a simple
supervision for training semantic segmentation models.

2.1. Pixel labeling from image level supervision

Pinheiro et al. [23] proposed a novel LSE pooling
method which puts more weight on pixels which are impor-
tant for classifying the image during training. Papandreou
et al. [20] adopted an alternating training procedure based
on the Expectation-Maximization algorithm to dynamically
predict semantic foreground and background pixels. Qi et
al. [24] proposed a uniﬁed framework that includes the se-
mantic segmentation and object localization branches. [27]
proposed a novel method to extract markedly more accurate
masks from the pre-trained network itself. Wei et al. [35]
presented a simple to complex learning method to gradually
enhance the segmentation network. [29] proposed a method
based on CNN-based class-speciﬁc saliency maps and fully-
connected CRF. Roy et al. [26] presented a novel deep ar-
chitecture which fuses three different cues toward semantic
segmentation.

Recently, Kolesnikov et al. [14] proposed to localize
seed cues according to classiﬁcation networks for training
segmentation network. However, [14] can only obtain small
and sparse object-related seeds for supervision. To solve
this problem, Oh et al. [19] proposed using a saliency model
as additional information to exploit object extent. Wei et
al. [33] used adversarial erasing manner to iteratively train
multiple classiﬁcation networks for expanding discrimina-
tive regions. Arslan et al. [4] also utilized adversarial eras-
ing manner to allow the saliency detection network to dis-
cover new salient regions of object. Once true negative re-
gions are generated, they have no chance to be correct them.
In contrast, our proposed DSRG approach is very simple
and convenient to start from the seed cues and progressively
reﬁne the pixel-level labels as the dynamic supervision in
training phase.

Both [20] and the proposed method generate dynamic
pixel-level labels to train semantic segmentation networks.
However, there are several major improvements in this pa-
per. Different from [20] where the latent pixel-level su-
pervision is approximated by applying argmax function on
biased segmentation maps, we instead propose to use the
Seeded Region Growing to ﬁnd accurate and reliable latent
pixel-level supervision. With the help of the object seed
cues, our DSRG training approach is robust to very noisy
segmentation map in the beginning of training and generate
pixel-level supervision with high accuracy all along.

2.2. Seeded Region Growing

The Seeded Region Growing (SRG) [1] is an unsuper-
vised approach to segmentation that examines neighbor-
ing pixels of initial seed points and determines whether the

pixel neighbors should be added to the region depending on
a region similarity criterion. Two major concerns must be
handled when performing a segmentation based on region
growing: where to place the initial seeds in the image do-
main and which similarity criterion should be adopted to
characterize the image regions. The most common way
to select some seed pixels as seed based on simple hand-
crafted criterion [28] (e.g.
color, intensity, or texture).
Meanwhile, the similarity criterion [3] is always deﬁned
on hand-crafted features. These settings result in over-
segmentation and bad segmentation. In contrast, the DSRG
utilizes seed cues generated by classiﬁcation network as the
initial seed to avoid wrong seed placement. Besides, We
compute pixel similarity using deep learning features which
have been proven to have high-level semantics. Thus, the
DSRG can reduce over-segmentation and do not have the
merge procedure of the traditional SRG.

3. Approach

In this section, we give the details of the proposed DSRG
training approach for weakly-supervised semantic segmen-
tation. At ﬁrst, we will introduce how we generate seed cues
from a deep classiﬁcation network. Then, we will introduce
a balanced seed loss function which uses seed cues as su-
pervision to guide the weakly-supervised semantic segmen-
tation network. At last, to address the problem that the seed
cues are small and sparse, we propose the DSRG training.

3.1. Seed generation with classiﬁcation network

We utilize a deep classiﬁcation network to locate dis-
criminative regions as seed cues under image-level super-
vision. Image-level labels do not explicitly provide any in-
formation about the position of semantic objects. But, re-
cently, it has been shown that high-quality seeds indicating
discriminative object regions can be obtained by learning a
classiﬁcation network under the supervision of image-level
labels [30, 37]. The classiﬁcation network is fully convolu-
tional and the position of discriminative object regions are
preserved in the deep layers of the network.

In our framework, we employ the CAMs [37] method
for localizing the foreground classes. The procedures are
brieﬂy described as follows. We use a modiﬁed VGG-16
network [14] to initialize our classiﬁcation network.
In
the network, global average pooling (GAP) is applied on
conv7; the generated tensor is used as image representa-
tion and classiﬁed using a fully-connected layer; ﬁnally, the
fully-connected classiﬁer is applied to conv7 to generate a
heatmap for each object class. Then the discriminative ob-
ject regions are obtained by applying a hard threshold to the
heatmap.

Besides of the seed cues in foreground, we also ﬁnd seed
cues in the background. For localizing background, we uti-
lize the saliency detection technology from [12], and simply

7016

Person

Table

Cat

Bottle

S

Seed

DSRG

G

Softmax

H

Seeding Loss

Downscale

CRF

CRF

Boundary Loss

Figure 2. Overview of the proposed Deep Seeded Region Growing training approach. The Region Growing module takes the seed cues and
segmentation map as input produces latent pixel-wise supervision which is more accurate and more complete than seed cues. Our method
iterates between reﬁning pixel-wise supervision and optimizing the parameters of a segmentation network.

select the regions in normalized saliency maps whose pixels
are with low saliency values as background. The resulted
seed cues from foreground and background are stacked to-
gether into a single channel segmentation mask.

3.2. Seeding loss

After obtaining the seed cues, we introduce how to train
an image semantic segmentation network using the seed
cues. The balanced seeding loss is proposed to encourage
predictions of the segmentation network to match only seed
cues given by the classiﬁcation network while ignoring the
rest of the pixels in the image. Considering the unbalanced
distribution of the seed cues of foreground and background,
the balanced seeding loss has two normalization coefﬁcients
for foreground and background, respectively, which is dif-
ferent from the seed loss in [14].

Let C be the set of classes that are present in the image
(excluding background) and ¯C be the background. Suppose
that Sc is a set of locations that are classiﬁed to class c.
Then, the balanced seeding loss ℓseed is deﬁned as follows:

ℓseed = −

log Hu,c

1
|Sc| X
c∈C

Pc∈C
1
|Sc| X
c∈ ¯C

Pc∈ ¯C

X
u∈Sc

X
u∈Sc

−

log Hu,c,

(1)

in which Hu,c denotes the probability of class c at position
u of segmentation map H.

Besides, we use a boundary loss ℓboundary which pro-
posed in [14] to encourage segmentation map to match up
with object boundaries. Ultimately, the segmentation net-
work are optimized by minimizing a loss function:

ℓ = ℓseed + ℓboundary.

(2)

3.3. Deep seeded region growing

In the introduced seeding loss, we can ﬁnd the seed cues
are sparse.
In practice, there are about 40% pixels have
labels. During training, the labels are ﬁxed following con-
ventional setting of training deep networks. Our idea is to
grow the seed cues to unlabeled pixels. Thus, we could
have denser supervision to train better segmentation net-
works. The basis of seed cues growing is that in image there
are small homogeneous regions in which the pixels should
have the same label. The small homogeneous regions are
usually used in low-level vision, such as generating super-
pixels [25]. To formulate the seed cues growing problem,
here we refer to a classical algorithm, Seeded Region Grow-
ing (SRG) [1].

In SRG, some seed pixels are initially selected based on
some simple hand-crafted criterion (e.g. color, intensity,
or texture). Once the initial seeds are placed, the growth
process seeks to obtain homogeneous image regions, i.e.,
it tries to segment the image into regions with the property
that each connected component of a region contains exactly
one of the initial seeds.

We propose to integrate SRG into deep segmentation
networks for weakly-supervised semantic segmentation.
The yield method is termed as “deep seeded region growing
(DSRG)”.

Once the initial seeds are initialized by classiﬁcation net-
work, the regions are then grown from these seed points to
adjacent unlabeled points depending on a region similarity
criterion. The similarity criterion deﬁnes whether a candi-
date pixel should be incorporated into a speciﬁc region or
not. Now, the major concerns must be handled when per-
forming learning a semantic segmentation network based
on region growing: which similarity criterion should be

7017

adopted to characterize the image regions? In the follow-
ing, we detail the strategies to handle the problem.

The similarity criteria P we make here is the simple
probability threshold value of a pixel in segmentation map
H generated by segmentation network.

selected as new supervision for training segmentation net-
work. We denote the |C| + 1 iterative visiting process as
DSRG(S, H), which means a region growing step. The
ﬁnal updated S = [S0, · · · , SC] is used as the supervision
and applied to train segmentation network with seeding loss
in Eqn (1). In Figure 2, the DSRG(S, H) is plugged into
the framework of the proposed segmentation network.

P (Hu,c, θc) =

c = arg max

Hu,c′ ,

(3)

4. Experiments

TRUE Hu,c ≥ θc and

c′
FALSE otherwise.





in which Hu,c refers to the probability value of the pixel
at position u that belongs to class c . And θ is the proba-
bility threshold value. In practice, we do not set different
thresholds for different categories. The foreground cate-
gories share a same threshold θf and the background has
another threshold θb. Traditional SRG usually has a phe-
nomenon of over-segmentation since low-level image fea-
tures is not robust to inter-class appearance of object.
In
DSRG, we compute pixel similarity using deep learning
features which have been proven to have high-level seman-
tics. Thus, the DSRG can reduce over-segmentation and do
not have the merge procedure of the traditional SRG.

Now, we can take segmentation map H and seed cues
S as inputs to perform region growing. DSRG is an iter-
ative visiting process for each class. We denote the itera-
tive visiting process of class c as Vc, c ∈ [0, |C|], where
c = 0 means the background class. In an iteration of Vc,
we visit all the positions in Sc in a row-ﬁrst manner. When
visiting a pixel Q, we denote the set of unlabeled pixels
in Q’s 8-connectivity neighborhoods as R. For Ru ∈ R,
its probability of being class c is denoted as Hu,c as de-
scribed above. Then Ru is classiﬁed based on P as fol-
lows:

the pixel at u is labeled as c;

1: if P (Hu,c, θc) then
2:
3: else
4:
5: end if

the pixel at u keeps unlabeled state.

After visiting all the positions, we append all the newly
labeled pixels to Sc. Once Sc is changed, we will visit
the updated Sc again. Otherwise, Vc stops. The termina-
tion criteria is different with classical SRG in which ev-
ery pixel must have a label. Because it is difﬁcult to tell
the label of a pixel with a low conﬁdence predicted by seg-
mentation network. However, with increasing capability of
segmentation network, the amount of unlabeled pixels de-
creases and the objects extent are covered with correct la-
bels. Besides, to reduce the redundancy visits in Vc, we
ﬁrst compute connected components of regions that meet
the requirement in Eqn (3), and then the connected compo-
nents which consist the initial seed regions take the same
label as the initial seed. These connected components are

4.1. Experimental setup

Dataset and Evaluation Metrics We evaluate the pro-
posed approach on the PASCAL VOC 2012 segmentation
benchmark dataset [8] and COCO dataset [17]. PASCAL
VOC: It contains three parts: training (train, 1464 images),
validation (val, 1449 images) and testing (test, 1456 im-
ages). Following the common practice [6, 33], we aug-
ment the training part by additional images from [9]. In our
experiments, only image-level labels are utilized for train-
ing. We compare our method with other state-of-the-arts
on both val and test sets. The standard intersection over
union (IOU) criterion and pixel-wise accuracy are adopted
for evaluation on PASCAL val dataset. The result on the
test set is obtained by submitting the predicted results to the
ofﬁcial PASCAL VOC evaluation server. COCO: its train-
ing set contains 80k samples with only image-level labels
and it’s val set contains 40k samples for evaluation. Per-
formance is evaluated in terms of pixel IoU averaged on 81
categories. Experimental analysis of the proposed approach
is conducted on the val set.

Training/Testing Settings
We adopt the slightly modiﬁed version of the 16-layer
VGG network from [14] for the classiﬁcation network and
DeepLab-ASPP from [6] for the segmentation network.
They are all initialized by the VGG-16 [31] pretrained on
ImageNet. SGD with mini-batch is used for training classi-
ﬁcation and segmentation network. We use the momentum
of 0.9 and a weight decay of 0.0005. The batch size is 20,
the dropout rate is 0.5 and the weight decay parameter is
0.0005. The initial learning rate is 5e-4 and it is decreased
by a factor of 10 every 2000 iterations.

For seed generation, those pixels belonging to top 20%
of the largest value (a fraction suggested by [14, 33]) in the
heatmap are considered as foreground object regions. We
use saliency maps from [12] to produce the background lo-
calization cues. We adopt the normalized saliency value
0.06 as the threshold to obtain background localization cues
(i.e. pixels whose saliency values are smaller than 0.06 are
considered as background). For the similarity criteria in
DSRG, we set θb and θf to 0.99 and 0.85, respectively. For
CRF, we use the default values from the Koltun public im-
plementation as parameters for the pairwise interactions.

In test phase, the learned segmentation network is ap-

7018

Table 1. Comparison of weakly-supervised semantic segmentation
methods on VOC 2012 val and test set

Method

Training Val Test

Supervision: Image-level Labels

(* methods implicitly use pixel-level supervision)

(† methods implicitly use box supervision)

Supervision: Image-level Labels

SN B* [34]

MIL-seg* [23]

TransferNet* [10]

AF-MCG* [24]

GuidedSeg† [19]

MIL-FCN [22]

CCNN [21]

MIL-bb [23]

EM-Adapt [20]

DCSM [29]

BFBP [27]

STC [35]

SEC [14]

AF-SS [24]

Combining Cues [26]

AE-PSL [33]

DCSP [4]

Supervision: Image-level Labels

DSRG (VGG16)

DSRG (Resnet101)

10k

41.9 40.6

700k

42.0 43.2

70k

10k

20k

10k

700k

700k

10k

10k

10k

50k

10k

10k

10k

10k

10k

10k

10k

52.1 51.2

54.3 55.5

55.7 56.7

25.7 24.9

35.3 35.6

37.8 37.0

38.2 39.6

44.1 45.1

46.6 48.0

49.8 51.2

50.7 51.7

52.6 52.7

52.8 53.7

55.0 55.7

58.6 59.2

59.0 60.41
61.4 63.22

plied to produce probability map for each testing image.
Then, we upscale the predicted probability map to match
the size of the input image, and then apply a fully-connected
CRF [15] to reﬁne the segmentation result.

Reproducibility. Our approach is implemented based
on Caffe [11]. All networks are trained on a single NVIDIA
GeForce GTX TITAN X GPU. The code is available at
https://github.com/speedinghzl/DSRG.

4.2. Comparisons with state of the arts

Results of other state-of-the-art weakly-supervised se-
mantic segmentation solutions on PASCAL VOC valida-
tion and test dataset are summarized in Table 1. We pro-

1http://host.robots.ox.ac.uk:8080/anonymous/

2http://host.robots.ox.ac.uk:8080/anonymous/

ZZT4TI.html

LWX93L.html

Image

Ground Truth  

Prediction 

Figure 3. Qualitative segmentation results on the VOC 2012 val
set. One failure case is shown in the last row.

vide these results for reference and emphasize that they
should not be directly compared with our method. Because
the methods were trained on different training sets or with
different kinds of annotations, bounding boxes, spots and
image-level labels. Among the approaches, CCNN [21],
MIL-seg [23], STC [35], GuidedSeg [19], and TransferNet
[10] use more images for training (700K, 700K, 50K, 20K
and 70K, respectively). All the other methods are based on
10K training images and built on top of the VGG16 model.

The results show that our method substantially outper-
forms all the previous techniques using image-level labels
for weak supervision. AE-PSL [33] and DCSP [4] achieve
the best performance among the baselines. However, adver-

7019

Table 2. Comparison of mIoU using different settings of our approach on VOC 2012 val set

g
k
Method b

e
n
a
l
p

e
k
i
b

d
r
i
b

t
a
o
b

e
l
t
t
o
b

s
u
b

r
a
c

t
a
c

r
i
a
h
c

w
o
c

e
l
b
a
t

g
o
d

e
s
r
o
h

r
o
t
o
m

n
o
s
r
e
p

t
n
a
l
p

p
e
e
h
s

a
f
o
s

n
i
a
r
t

v mIoU

t

baseline 82.5 67.5 23.2 65.7 29.7 47.5 71.8 66.8 76.7 23.3 51.7 26.2 69.7 54.2 63.2 57.2 33.7 64.5 33.5 48.7 46.1 52.5

+BSL 82.4 71.9 29.1 67.7 32.4 49.8 75.5 67.9 74.7 22.8 54.9 26.6 64.3 55.7 64.7 56.0 35.0 67.7 32.7 50.2 45.8 53.6

+DSRG 86.6 70.5 28.8 70.6 34.7 55.7 74.9 70.1 80.2 24.1 63.6 24.8 76.6 64.1 64.9 72.3 38.5 68.7 35.8 51.8 51.9 57.6

+Retrain 87.5 73.1 28.4 75.4 39.5 54.5 78.2 71.3 80.6 25.0 63.3 25.4 77.8 65.4 65.2 72.8 41.2 74.3 34.1 52.1 53.0 59.0

sarial erasing is employed by AE-PSL to expand the seed
cues for supervision, which needs to iteratively train multi-
ple classiﬁcation networks. DCSP also utilizes adversarial
erasing manner to allow the saliency network to discover
new salient regions of object.
It does not require the re-
training of the network after each erasing, but DCSP may
introduce some true negative regions due to over erasing.
In contrast, the proposed DSRG approach is very simple
and convenient to reﬁne supervision online and our method
obtains better results than DCSP and AE-PSL. Compared
with those methods only using image-level labels for su-
pervision, the proposed DSRG(VGG16) method improves
upon the best performance by over 1.2% on test set. It can
be seen that our method achieves 60.4% mIoU on test set.
Besides, Our DSRG (Resnet101) achieves 63.2% mIOU on
test set.

4.3. Qualitative results

Fig. 3 shows some successful segmentation results.
It
shows our method can produce accurate segmentations even
for complicated images and recover ﬁne details of the
boundary. One typical failure case is given in the bot-
tom row of Fig. 3. This failure mode is that the model
cannot pick out object regions from background precisely.
As is typical for weakly-supervised systems, strongly co-
occurring categories (such as train and rails, sculls and oars,
snowbikes and snow) cannot be separated without ﬁnner-
grained information [13].

4.4. Ablation studies

In order to further prove the effect of the different com-
ponents, we conduct some ablation experiments with differ-
ent settings of VGG16 based DSRG. In Table 2, the “base-
line” denotes our implemented SEC [14], our result is much
better than [14] (50.4 mAP without Lexpand), due to the dif-
ferent background locating technology [12] and details. The
“+BSL” denotes replacing the original seeding loss with the
balanced seeding loss in Eqn (1); the “+DSRG” denotes
adding DSRG training approach. We can observe that the
weighted seeding loss improves the performance by 1.1%
compared with baseline. And, DSRG improves further the
performance by 4%, demonstrating the signiﬁcant effective-
ness of DSRG. It is most noticeable for animals and person,

Table 3. Per-class IOU on COCO using image tags during training

l
a
m
i
n
A

e
l
c
i
h
e
V

r
o
o
d
t
u
O

Cat.
  Class
BG   background
  person
P
  bicycle
  car
  motorcycle
  airplane
  bus
  train
  truck
  boat
  traffic light
  fire hydrant
  stop sign
  parking meter
  bench
  bird
  cat
  dog
  horse
  sheep
  cow
  elephant
  bear
  zebra
  giraffe
  backpack
  umbrella
  handbag
  tie
  suitcase
  frisbee
  skis
  snowboard
  sports ball
  kite
  baseball bat
  baseball glove
  skateboard
  surfboard
  tennis racket
  bottle

y
r
o
s
s
e
c
c
A

t
r
o
p
S

K

80.6

d
o
o
F

e
r
u
t
i
n
r
u
F

e
r
a
w
n
e
h
c
t
i

SEC BFBP Ours Cat.
74.3
43.6
24.2
15.9
52.1
36.6
37.7
30.1
24.1
17.3
16.7
55.9
48.4
25.2
16.4
34.7
57.2
45.2
34.4
40.3
41.4
62.9
59.1
59.8
48.8
0.3
26.0
0.5
6.5
16.7
12.3
1.6
5.3
7.9
9.1
1.0
0.6
7.1
7.7
9.1
13.2

68.8
27.5
18.2
7.2
40.5
32.0
39.2
26.5
17.5
16.5
3.9
33.1
28.4
25.5
12.4
31.1
52.8
44.1
34.2
38.0
42.1
65.2
57.0
65.0
55.6
3.2
28.1
1.1
5.5
21.3
5.6
1.0
2.8
1.9
10.3
1.7
0.5
6.6
3.3
5.5
9.6

30.4
22.1
54.2
45.2
38.7
33.2
25.9
20.6
16.2
60.4
51.0
26.3
22.3
41.5
62.2
55.6
42.3
47.1
49.3
67.1
62.6
63.2
54.3
0.2
35.3
0.7
7.0
23.4
13.0
1.5
16.3
9.8
17.4
4.8
1.2
14.4
13.5
6.8
22.3

s
c
i
n
o
r
t
c
e
l
E

e
c
n
a
i
l
p
p
A

r
o
o
d
n
I

  Class
  wine glass
  cup
  fork
  knife
  spoon
  bowl
  banana
  apple
  sandwich
  orange
  broccoli
  carrot
  hot dog
  pizza
  donut
  cake
  chair
  couch
  potted plant
  bed
  dining table
  toilet
  tv
  laptop
  mouse
  remote
  keyboard
  cell phone
  microwave
  oven
  toaster
  sink
  refrigerator
  book
  clock
  vase
  scissors
  teddy bear
  hair dryer
  toothbrush
  mean IOU

SEC BFBP Ours
24.0
17.5
22.3
20.4
5.6
17.9
0.0
0.5
1.8
5.0
1.0
1.4
0.5
0.6
0.6
18.8
13.3
12.5
46.4
44.9
43.6
24.3
18.9
23.6
24.5
21.4
22.8
41.2
35.0
44.3
35.7
27.0
36.8
15.3
16.0
6.7
24.9
22.5
31.2
56.2
57.8
50.9
34.2
36.2
32.8
6.9
17.0
12.0
9.7
8.2
7.8
17.7
13.9
5.6
14.3
7.4
6.2
32.4
29.8
23.4
3.8
2.0
0.0
43.6
30.1
38.5
25.3
14.8
19.2
21.1
19.9
20.1
0.9
0.4
3.5
20.6
9.9
17.5
12.3
19.9
12.5
33.0
26.1
32.1
11.2
9.8
8.2
12.4
16.4
13.7
0.0
0.0
0.0
17.8
9.5
10.8
15.5
13.2
4.0
12.3
7.5
0.4
20.7
16.5
17.8
23.9
13.4
18.4
17.3
12.2
16.5
46.3
41.0
47.0
0.0
0.0
0.0
4.5
2.0
2.8
26.0
20.4
22.4

e.g. the improvement for segmenting dog/horse/cow/person
is about 10%. Besides, we ﬁrst employ the trained seg-
mentation model of “+DSRG” to on all the training im-
ages. Then, the predicted segmentation masks are used as
supervision for training the segmentation network for an-
other round in a fully-supervised way. As shown in Table 2,
the performance provided by this extra training (denoted as
“+Retrain”) is further improved from 57.6% to 59.0%. We
do not observe further performance gain by performing ad-
ditional retrain steps.

In addition, we tried different values of θf and θb to
ﬁnd the best performing region growing strategy. The re-

7020

Table 4. Performance on PASCAL VOC 2012 val dataset for dif-
ferent θ

θf

θb

0.99

0.95

0.90

0.99

0.95

0.90

0.85

0.80

57.45

57.59

57.63

57.69

57.66

57.43

57.56

57.64

57.67

57.63

57.23

57.35

57.40

57.44

57.45

sults are shown for different values of θ in Tab 4. The re-
sults show that our method is robust to the region growing
thresholds θ. To explore the effect of only performing re-
gion growing for foreground or background object, we set
θb = ∞, θf = 0.85 for only conducting region growing
for foreground object, the performance on PASCAL VOC
val dataset is 55.9% mIoU. When θb = 0.99, θf = ∞,
the performance is 54.3% mIoU. The results show that only
conducting region growing for foreground object or back-
ground object is also improve the performance. However,
it can achieve best performance when simultaneously con-
ducting region growing for foreground object and back-
ground object.

4.5. The quality improvement of dynamic supervi 

sion over epochs

In this section the qualities of the new pixel labels as dy-
namic supervision, obtained from DSRG, at each epoch, are
evaluated. Compared with ground truths that are annotated
by human, we could use the mean accuracy, mean recall and
IoU to measure the quality of the supervision reﬁned by our
approach. In Fig. 4, the supervision that generated by classi-
ﬁcation network has somewhat high precision(62.6%), low
recall(32.1%) and low IoU(30.0%). With the increasing of
epochs, the precision of seed remains a high value, and the
recall and IoU get signiﬁcant improvements. At epoch #12,
the mean precision, mean recall and mean IoU are 63.9%,
65.4%, and 57.1%, respectively. It demonstrates that DSRG
can ﬁnd the object extent and improve the quality of su-
pervision, which explains why the proposed DSRG training
procedure works excellently on the weakly supervised se-
mantic segmentation task. Additional examples in the sup-
plementary materials shows the gradually reﬁning supervi-
sion starting from seed cues during training.

4.6. COCO results

To further demonstrate the generality of our method, we
conducted a set of experiments on COCO. Unlike in PAS-
CAL VOC, the majority of COCO samples were collected
from non-iconic images in a complex natural context. We
provide the per-class IoU of SEC [14], BFBP [27] and our
approach in Table 3. Our VGG16 based DSRG obtains re-
markable better results, especially in Person, Animal, Ve-

)

%

(

n
o
i
s
i
v
r
e
p
u
s
 
e
h
t
 
f
o
y
t
i
l

 

a
u
q
e
h
T

 

70

65

60

55

50

45

40

35

30

25

Precision

Recall

IoU

12

0

1

2

Epoch #

Figure 4. The quality of the dynamic supervision (%) with respect
to the epochs.

hicle etc, but performs poorly on small ones, such as In-
door and Kitchenware. Altogether, our DSRG method im-
proves upon the best performance by over 3.6% on val set.
It can be seen that our method achieves 26.0% mIoU on val
set. Meanwhile, compared with the performance of fully
supervised method (40.98% mIoU), these results on COCO
evidence that there is much space for progress in weakly-
supervised semantic segmentation. Developing solutions
that handle small objects could be an interesting direction
for future research.

5. Conclusion and future work

We have addressed the problem of training semantic
segmentation networks only using image-level supervision.
Image-level labels alone can provide high-quality seeds, or
discriminative object regions, but inferring full object ex-
tents is a very difﬁcult problem. We propose a DSRG train-
ing approach gradually improves the quality and extent ob-
ject regions and itself is supervised the object regions. We
demonstrate that our approach outperforms previous state-
of-the-art methods under the same experimental conditions.
We also clearly identify the effectiveness of region growing
mechanism within the semantic segmentation network in
the experiments. In future work, we will focus on designing
more effective weakly-supervised strategies and improving
seed quality.

Acknowledgements

We really appreciate the enormous help from Lichao
Huang at Horizon Robotics and Chunyu Wang at MSRA.
This work was partly supported by NSFC (No.61733007,
No.61503145) and the fund of HUST-Horizon Computer
Vision Research Center. Xinggang Wang was sponsored by
CCF-Tencent Open Research Fund, the Program for HUST
Academic Frontier Youth Team, and Young Elite Sponsor-
ship Program by CAST, No. YESS 20150077.

7021

References

[1] R. Adams and L. Bischof. Seeded region growing.

IEEE

TPAMI, 16(6):641–647, 1994. 2, 3, 4

[2] A. Bearman, O. Russakovsky, V. Ferrari, and L. Fei-Fei.
Whats the point: Semantic segmentation with point super-
vision. In Proc. ECCV, pages 549–565. Springer, 2016. 2
[3] V. Borges, M. C. F. de Oliveira, T. Silva, A. Vieira, and
B. Hamann. Region growing for segmenting green microal-
gae images. IEEE/ACM Transactions on Computational Bi-
ology and Bioinformatics, 2016. 3

[4] A. Chaudhry, P. K. Dokania, and P. H. Torr. Discover-
ing class-speciﬁc pixels for weakly-supervised semantic seg-
mentation. arXiv preprint arXiv:1707.05821, 2017. 3, 6
[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Semantic image segmentation with deep con-
arXiv preprint
volutional nets and fully connected crfs.
arXiv:1412.7062, 2014. 1, 2

[6] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. arXiv preprint arXiv:1606.00915, 2016. 2, 5
[7] J. Dai, K. He, and J. Sun. Boxsup: Exploiting bounding
boxes to supervise convolutional networks for semantic seg-
mentation. In Proc. ICCV, pages 1635–1643, 2015. 3
[8] M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams,
J. Winn, and A. Zisserman. The pascal visual object classes
challenge: A retrospective. IJCV, 111(1):98–136, 2015. 2, 5
[9] B. Hariharan, P. Arbel´aez, L. Bourdev, S. Maji, and J. Malik.
In Proc. ICCV,

Semantic contours from inverse detectors.
pages 991–998. IEEE, 2011. 5

[10] S. Hong, J. Oh, H. Lee, and B. Han. Learning transferrable
knowledge for semantic segmentation with deep convolu-
In Proc. CVPR, pages 3204–3212,
tional neural network.
2016. 6

[11] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding. In Proc. ACM, pages
675–678. ACM, 2014. 6

[12] H. Jiang, J. Wang, Z. Yuan, Y. Wu, N. Zheng, and S. Li.
Salient object detection: A discriminative regional feature
In Proc. CVPR, pages 2083–2090,
integration approach.
2013. 3, 5, 7

[13] A. Kolesnikov and C. H. Lampert.

Improving weakly-
supervised object localization by micro-annotation. arXiv
preprint arXiv:1605.05538, 2016. 7

[14] A. Kolesnikov and C. H. Lampert. Seed, expand and con-
strain: Three principles for weakly-supervised image seg-
mentation. In Proc. ECCV, pages 695–711. Springer, 2016.
1, 2, 3, 4, 5, 6, 7, 8

[15] V. Koltun. Efﬁcient inference in fully connected crfs with
gaussian edge potentials. Proc. NIPS, 2(3):4, 2011. 6

[16] D. Lin, J. Dai, J. Jia, K. He, and J. Sun.

Scribble-
sup: Scribble-supervised convolutional networks for seman-
tic segmentation. In Proc. CVPR, pages 3159–3167, 2016.
2

[17] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In European conference on computer
vision, pages 740–755. Springer, 2014. 5

[18] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proc. CVPR, pages
3431–3440, 2015. 1

[19] S. J. Oh, R. Benenson, A. Khoreva, Z. Akata, M. Fritz, and
B. Schiele. Exploiting saliency for object segmentation from
image level labels. arXiv preprint arXiv:1701.08261, 2017.
2, 3, 6

[20] G. Papandreou, L.-C. Chen, K. Murphy, and A. L. Yuille.
Weakly-and semi-supervised learning of a dcnn for seman-
tic image segmentation. arXiv preprint arXiv:1502.02734,
2015. 2, 3, 6

[21] D. Pathak, P. Krahenbuhl, and T. Darrell. Constrained con-
volutional neural networks for weakly supervised segmenta-
tion. In Proceedings CVPR, pages 1796–1804, 2015. 6
[22] D. Pathak, E. Shelhamer, J. Long, and T. Darrell. Fully
convolutional multi-class multiple instance learning. arXiv
preprint arXiv:1412.7144, 2014. 2, 6

[23] P. O. Pinheiro and R. Collobert. From image-level to pixel-
level labeling with convolutional networks. In Proc. CVPR,
pages 1713–1721, 2015. 3, 6

[24] X. Qi, Z. Liu, J. Shi, H. Zhao, and J. Jia. Augmented feed-
back in semantic segmentation under image level supervi-
sion. In Proc. ECCV, pages 90–105. Springer, 2016. 3, 6
[25] X. Ren and J. Malik. Learning a classiﬁcation model for

segmentation. In null, page 10. IEEE, 2003. 4

[26] A. Roy and S. Todorovic. Combining bottom-up, top-down,
and smoothness cues for weakly supervised image segmen-
tation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 3529–3538, 2017. 3,
6

[27] F. Saleh, M. S. A. Akbarian, M. Salzmann, L. Petersson,
S. Gould, and J. M. Alvarez. Built-in foreground/background
prior for weakly-supervised semantic segmentation. In Proc.
ECCV, pages 413–432. Springer, 2016. 3, 6, 8

[28] F. Y. Shih and S. Cheng. Automatic seeded region growing
for color image segmentation. Image and vision computing,
23(10):877–886, 2005. 3

[29] W. Shimoda and K. Yanai. Distinct class-speciﬁc saliency
maps for weakly supervised semantic segmentation. In Proc.
ECCV, pages 218–234. Springer, 2016. 3, 6

[30] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside
convolutional networks: Visualising image classiﬁcation
models and saliency maps. arXiv preprint arXiv:1312.6034,
2013. 3

[31] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 5

[32] P. Tang, X. Wang, X. Bai, and W. Liu. Multiple instance
detection network with online instance classiﬁer reﬁnement.
In Proc. CVPR, 2017. 1

[33] Y. Wei, J. Feng, X. Liang, M.-M. Cheng, Y. Zhao, and
S. Yan. Object region mining with adversarial erasing:
A simple classiﬁcation to semantic segmentation approach.
arXiv preprint arXiv:1703.08448, 2017. 3, 5, 6

7022

[34] Y. Wei, X. Liang, Y. Chen, Z. Jie, Y. Xiao, Y. Zhao, and
S. Yan. Learning to segment with image-level annotations.
Pattern Recognition, 59:234–244, 2016. 6

[35] Y. Wei, X. Liang, Y. Chen, X. Shen, M.-M. Cheng, J. Feng,
Y. Zhao, and S. Yan. Stc: A simple to complex framework
for weakly-supervised semantic segmentation. IEEE TPAMI,
2016. 3, 6

[36] J. Xu, A. G. Schwing, and R. Urtasun. Learning to segment
under various forms of weak supervision. In Proc. CVPR,
pages 3781–3790, 2015. 2

[37] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Tor-
ralba. Learning deep features for discriminative localization.
In Proc. CVPR, pages 2921–2929, 2016. 2, 3

7023

Weakly-Supervised Semantic Segmentation Network with
Deep Seeded Region Growing

Zilong Huang1, Xinggang Wang1∗, Jiasi Wang1, Wenyu Liu1, and Jingdong Wang2
1School of Electronic Information and Communications, Huazhong University of Science and Technology
2Microsoft Research Asia
{hzl,xgwang,wangjiasi,liuwy}@hust.edu.cn jingdw@microsoft.com

Abstract

This paper studies the problem of learning image seman-
tic segmentation networks only using image-level labels as
supervision, which is important since it can signiﬁcantly
reduce human annotation efforts. Recent state-of-the-art
methods on this problem ﬁrst infer the sparse and discrimi-
native regions for each object class using a deep classiﬁca-
tion network, then train semantic a segmentation network
using the discriminative regions as supervision.
Inspired
by the traditional image segmentation methods of seeded
region growing, we propose to train a semantic segmenta-
tion network starting from the discriminative regions and
progressively increase the pixel-level supervision using by
seeded region growing. The seeded region growing module
is integrated in a deep segmentation network and can bene-
ﬁt from deep features. Different from conventional deep net-
works which have ﬁxed/static labels, the proposed weakly-
supervised network generates new labels using the contex-
tual information within an image. The proposed method
signiﬁcantly outperforms the weakly-supervised semantic
segmentation methods using static labels, and obtains the
state-of-the-art performance, which are 63.2% mIoU score
on the PASCAL VOC 2012 test set and 26.0% mIoU score
on the COCO dataset.

1. Introduction

Deep Convolutional Neural Networks (DCNN) have
achieved great successes on the image semantic segmen-
tation problem [5, 18] thanks to a large amount of fully-
annotated images. However, collecting large-scale accu-
rate pixel-level annotation is time-consuming and typically
requires substantial ﬁnancial investments. Unlabeled and
weakly-labeled visual data, however, can be collected in
large amounts in a relatively fast and cheap manner. There-
fore, a promising direction in the computer vision research

∗Corresponding author.

Person
Horse

Training Data

Prediction

Ground Truth

Epoch #0

Epoch #6

Epoch #12

Figure 1. The top row orderly shows a training image with
the image-level labels, the segmentation result of our proposed
method only using image-level supervision, and the ground truth.
Our segmentation result is very close to the ground truth anno-
tated by human. The bottom row shows the dynamic supervi-
sion in several epochs during the training of the proposed weakly-
supervised semantic segmentation network. (The black represents
background and the white represents unlabeled/ignore pixels).

is to develop object recognition methods that can learn from
unlabeled or weakly labeled images [14, 32].

In this paper, we study the problem of learning seman-
tic segmentation networks from weakly-labeled images.
Among various settings of weak label, image-level anno-
tation is one of the most economical and most efﬁcient set-
In this context, every training image has its image
ting.
class/category labels.
It means objects belonging to the
class labels appear in the image. However, the locations
of the objects are unknown. We need to infer the pixel-level
locations of the objects. Thus, the main problem in training
weakly-supervised semantic segmentation networks is how
to accurately assign image-level labels to their correspond-
ing pixels.

To establish the desired pixel-label correspondence

7014

there is a very insightful research work.
in training,
Kolesnikov et al. [14] employed an image classiﬁcation
network with classiﬁcation activation maps (CAM) [37]
method to select the most discriminative regions, and used
the regions as pixel-level supervision for segmentation net-
works. Compared to the early weakly-supervised semantic
segmentation methods [22, 20], the discriminative region
based approach signiﬁcantly improved the performance of
this challenging task. However, in [14], the discriminative
regions are small and sparse as shown in the epoch #0 image
in Figure 1. In training, the supervision of the semantic seg-
mentation network is ﬁxed as the sparse discriminative re-
gions. Thus, we name the learning strategy in [14] as “static
supervision”. The static supervision setting deviates from
the requirement of semantic segmentation task that requires
accurate and complete object regions for training segmen-
tation models.

To address the issue, we propose to expand the discrim-
inative regions to cover the whole objects during training
segmentation networks. In practice, the pixels around the
discriminative regions are always belonging to the same ob-
jects because semantic labels of the same object have spa-
tial continuity. Our motivation is that, using the image la-
bels enables to ﬁnd small and sparse discriminative regions
from the object of interest, termed as “seed cues”, the neigh-
boring pixels of seed cues with similar features (e.g. color,
texture or deep features) could have the same labels as the
seed cues. We utilize the classical Seeded Region Growing
(SRG) method [1] to model this process for generating ac-
curate and complete pixel-level labels. Here we can train
semantic segmentation networks under supervision of the
pixel-level labels. Different from [14, 19], the pixel-level
labels are dynamic. The dynamic supervision is quite dif-
ferent from traditional network training using ﬁxed super-
vision. In our case, we let the network generate new labels
of the input training example, i.e., the training image. SRG
is integrated into the deep segmentation network and can
be optimized end-to-end and enjoys the deep features. We
name the proposed method as “deep seeded region growing
(DSRG)” for weakly-supervised semantic segmentation.

In practice, the seed cues localized by classiﬁcation net-
work is small but with high precision. It is a natural way
to choose the seed cues as the seed points in SRG. Besides,
to measure the similarity between the seed points and ad-
jacent pixels for region growing, we make use of the seg-
mentation map which is output of the segmentation network
as features. Thus, SRG treats the seed cues as initial seed
points; then the adjacent pixels in segmentation map with
high probabilities on their corresponding categories take the
same labels as the seed cues. This process is repeated until
there are no pixels satisfying the above constraints. In the
end, the output of DSRG is used as the supervision for train-
ing segmentation network. In the training phase, the super-

vision is used to form the loss function, termed as “seeding
loss”. In seeded regions, the loss is the same as full super-
vise loss function in [5]; the other positions are ignored by
the seeding loss.

During training, the DSRG approach gradually enriches
the supervision information of the segmentation network.
As shown in Figure 1, the supervision in epoch #0 is ac-
tually the seed cues generated by classiﬁcation model, the
cues localize the head of person and the horse, which are
the most discriminative regions in the image. With the in-
creasing of epochs, the dynamic supervision gradually ap-
proaches the ground truth and cover the whole object con-
tent precisely. Meanwhile, the dynamic supervision ides
the network to produce competitive segmentation result. To
ensure the stability of training, DSRG always choose the
original seed cues as initial seed points.

In the experiments, we demonstrate the effectiveness of
our approach on the challenging PASCAL VOC 2012 Se-
mantic Segmentation benchmark [8] and COCO, and show
that we achieve the new state-of-the-art results. In addition,
we provide an analysis of the DSRG approach by carrying
out some ablation studies.

In summary, the main contributions of this paper are

summarized below:

• In deep semantic segmentation network, we utilize the
seeded region growing [1] mechanism, which enables
the network safely generates new pixel-level labels for
weakly-supervised semantic segmentation. Besides,
the network can be optimized in an end-to-end man-
ner and is easy to train.

• Our work obtains

the

state-of-the-art weakly-
supervised semantic segmentation performance on the
PASCAL VOC segmentation benchmark and COCO
dataset. The mIoU of our method are 61.4% and
63.2% on pascal voc val set and test set respectively,
which are better than many sophisticated systems and
are getting closer to the fully supervised segmentation
system [6] (67.6/70.3% mIoU on val/test set).

The rest of this paper is organized as follows. We ﬁrst
review related work in Section 2 and describe the architec-
ture of our approach in Section 3. In Section 4, the detailed
procedure to improve the quality of dynamic supervision is
discussed and experimental results are analyzed. Section 5
presents our conclusion and future work.

2. Related work

The last years have seen a renewed interest on weakly-
supervised visual
learning. Various weakly-supervised
methods have been proposed for learning to perform se-
mantic segmentation with coarser annotations, such as im-
age labels [20, 36], points [2], scribbles [16], and bounding

7015

boxes [7, 20] etc. In this work, we focus on using image
labels as the main form of supervision, which is a simple
supervision for training semantic segmentation models.

2.1. Pixel labeling from image level supervision

Pinheiro et al. [23] proposed a novel LSE pooling
method which puts more weight on pixels which are impor-
tant for classifying the image during training. Papandreou
et al. [20] adopted an alternating training procedure based
on the Expectation-Maximization algorithm to dynamically
predict semantic foreground and background pixels. Qi et
al. [24] proposed a uniﬁed framework that includes the se-
mantic segmentation and object localization branches. [27]
proposed a novel method to extract markedly more accurate
masks from the pre-trained network itself. Wei et al. [35]
presented a simple to complex learning method to gradually
enhance the segmentation network. [29] proposed a method
based on CNN-based class-speciﬁc saliency maps and fully-
connected CRF. Roy et al. [26] presented a novel deep ar-
chitecture which fuses three different cues toward semantic
segmentation.

Recently, Kolesnikov et al. [14] proposed to localize
seed cues according to classiﬁcation networks for training
segmentation network. However, [14] can only obtain small
and sparse object-related seeds for supervision. To solve
this problem, Oh et al. [19] proposed using a saliency model
as additional information to exploit object extent. Wei et
al. [33] used adversarial erasing manner to iteratively train
multiple classiﬁcation networks for expanding discrimina-
tive regions. Arslan et al. [4] also utilized adversarial eras-
ing manner to allow the saliency detection network to dis-
cover new salient regions of object. Once true negative re-
gions are generated, they have no chance to be correct them.
In contrast, our proposed DSRG approach is very simple
and convenient to start from the seed cues and progressively
reﬁne the pixel-level labels as the dynamic supervision in
training phase.

Both [20] and the proposed method generate dynamic
pixel-level labels to train semantic segmentation networks.
However, there are several major improvements in this pa-
per. Different from [20] where the latent pixel-level su-
pervision is approximated by applying argmax function on
biased segmentation maps, we instead propose to use the
Seeded Region Growing to ﬁnd accurate and reliable latent
pixel-level supervision. With the help of the object seed
cues, our DSRG training approach is robust to very noisy
segmentation map in the beginning of training and generate
pixel-level supervision with high accuracy all along.

2.2. Seeded Region Growing

The Seeded Region Growing (SRG) [1] is an unsuper-
vised approach to segmentation that examines neighbor-
ing pixels of initial seed points and determines whether the

pixel neighbors should be added to the region depending on
a region similarity criterion. Two major concerns must be
handled when performing a segmentation based on region
growing: where to place the initial seeds in the image do-
main and which similarity criterion should be adopted to
characterize the image regions. The most common way
to select some seed pixels as seed based on simple hand-
crafted criterion [28] (e.g.
color, intensity, or texture).
Meanwhile, the similarity criterion [3] is always deﬁned
on hand-crafted features. These settings result in over-
segmentation and bad segmentation. In contrast, the DSRG
utilizes seed cues generated by classiﬁcation network as the
initial seed to avoid wrong seed placement. Besides, We
compute pixel similarity using deep learning features which
have been proven to have high-level semantics. Thus, the
DSRG can reduce over-segmentation and do not have the
merge procedure of the traditional SRG.

3. Approach

In this section, we give the details of the proposed DSRG
training approach for weakly-supervised semantic segmen-
tation. At ﬁrst, we will introduce how we generate seed cues
from a deep classiﬁcation network. Then, we will introduce
a balanced seed loss function which uses seed cues as su-
pervision to guide the weakly-supervised semantic segmen-
tation network. At last, to address the problem that the seed
cues are small and sparse, we propose the DSRG training.

3.1. Seed generation with classiﬁcation network

We utilize a deep classiﬁcation network to locate dis-
criminative regions as seed cues under image-level super-
vision. Image-level labels do not explicitly provide any in-
formation about the position of semantic objects. But, re-
cently, it has been shown that high-quality seeds indicating
discriminative object regions can be obtained by learning a
classiﬁcation network under the supervision of image-level
labels [30, 37]. The classiﬁcation network is fully convolu-
tional and the position of discriminative object regions are
preserved in the deep layers of the network.

In our framework, we employ the CAMs [37] method
for localizing the foreground classes. The procedures are
brieﬂy described as follows. We use a modiﬁed VGG-16
network [14] to initialize our classiﬁcation network.
In
the network, global average pooling (GAP) is applied on
conv7; the generated tensor is used as image representa-
tion and classiﬁed using a fully-connected layer; ﬁnally, the
fully-connected classiﬁer is applied to conv7 to generate a
heatmap for each object class. Then the discriminative ob-
ject regions are obtained by applying a hard threshold to the
heatmap.

Besides of the seed cues in foreground, we also ﬁnd seed
cues in the background. For localizing background, we uti-
lize the saliency detection technology from [12], and simply

7016

Person

Table

Cat

Bottle

S

Seed

DSRG

G

Softmax

H

Seeding Loss

Downscale

CRF

CRF

Boundary Loss

Figure 2. Overview of the proposed Deep Seeded Region Growing training approach. The Region Growing module takes the seed cues and
segmentation map as input produces latent pixel-wise supervision which is more accurate and more complete than seed cues. Our method
iterates between reﬁning pixel-wise supervision and optimizing the parameters of a segmentation network.

select the regions in normalized saliency maps whose pixels
are with low saliency values as background. The resulted
seed cues from foreground and background are stacked to-
gether into a single channel segmentation mask.

3.2. Seeding loss

After obtaining the seed cues, we introduce how to train
an image semantic segmentation network using the seed
cues. The balanced seeding loss is proposed to encourage
predictions of the segmentation network to match only seed
cues given by the classiﬁcation network while ignoring the
rest of the pixels in the image. Considering the unbalanced
distribution of the seed cues of foreground and background,
the balanced seeding loss has two normalization coefﬁcients
for foreground and background, respectively, which is dif-
ferent from the seed loss in [14].

Let C be the set of classes that are present in the image
(excluding background) and ¯C be the background. Suppose
that Sc is a set of locations that are classiﬁed to class c.
Then, the balanced seeding loss ℓseed is deﬁned as follows:

ℓseed = −

log Hu,c

1
|Sc| X
c∈C

Pc∈C
1
|Sc| X
c∈ ¯C

Pc∈ ¯C

X
u∈Sc

X
u∈Sc

−

log Hu,c,

(1)

in which Hu,c denotes the probability of class c at position
u of segmentation map H.

Besides, we use a boundary loss ℓboundary which pro-
posed in [14] to encourage segmentation map to match up
with object boundaries. Ultimately, the segmentation net-
work are optimized by minimizing a loss function:

ℓ = ℓseed + ℓboundary.

(2)

3.3. Deep seeded region growing

In the introduced seeding loss, we can ﬁnd the seed cues
are sparse.
In practice, there are about 40% pixels have
labels. During training, the labels are ﬁxed following con-
ventional setting of training deep networks. Our idea is to
grow the seed cues to unlabeled pixels. Thus, we could
have denser supervision to train better segmentation net-
works. The basis of seed cues growing is that in image there
are small homogeneous regions in which the pixels should
have the same label. The small homogeneous regions are
usually used in low-level vision, such as generating super-
pixels [25]. To formulate the seed cues growing problem,
here we refer to a classical algorithm, Seeded Region Grow-
ing (SRG) [1].

In SRG, some seed pixels are initially selected based on
some simple hand-crafted criterion (e.g. color, intensity,
or texture). Once the initial seeds are placed, the growth
process seeks to obtain homogeneous image regions, i.e.,
it tries to segment the image into regions with the property
that each connected component of a region contains exactly
one of the initial seeds.

We propose to integrate SRG into deep segmentation
networks for weakly-supervised semantic segmentation.
The yield method is termed as “deep seeded region growing
(DSRG)”.

Once the initial seeds are initialized by classiﬁcation net-
work, the regions are then grown from these seed points to
adjacent unlabeled points depending on a region similarity
criterion. The similarity criterion deﬁnes whether a candi-
date pixel should be incorporated into a speciﬁc region or
not. Now, the major concerns must be handled when per-
forming learning a semantic segmentation network based
on region growing: which similarity criterion should be

7017

adopted to characterize the image regions? In the follow-
ing, we detail the strategies to handle the problem.

The similarity criteria P we make here is the simple
probability threshold value of a pixel in segmentation map
H generated by segmentation network.

selected as new supervision for training segmentation net-
work. We denote the |C| + 1 iterative visiting process as
DSRG(S, H), which means a region growing step. The
ﬁnal updated S = [S0, · · · , SC] is used as the supervision
and applied to train segmentation network with seeding loss
in Eqn (1). In Figure 2, the DSRG(S, H) is plugged into
the framework of the proposed segmentation network.

P (Hu,c, θc) =

c = arg max

Hu,c′ ,

(3)

4. Experiments

TRUE Hu,c ≥ θc and

c′
FALSE otherwise.





in which Hu,c refers to the probability value of the pixel
at position u that belongs to class c . And θ is the proba-
bility threshold value. In practice, we do not set different
thresholds for different categories. The foreground cate-
gories share a same threshold θf and the background has
another threshold θb. Traditional SRG usually has a phe-
nomenon of over-segmentation since low-level image fea-
tures is not robust to inter-class appearance of object.
In
DSRG, we compute pixel similarity using deep learning
features which have been proven to have high-level seman-
tics. Thus, the DSRG can reduce over-segmentation and do
not have the merge procedure of the traditional SRG.

Now, we can take segmentation map H and seed cues
S as inputs to perform region growing. DSRG is an iter-
ative visiting process for each class. We denote the itera-
tive visiting process of class c as Vc, c ∈ [0, |C|], where
c = 0 means the background class. In an iteration of Vc,
we visit all the positions in Sc in a row-ﬁrst manner. When
visiting a pixel Q, we denote the set of unlabeled pixels
in Q’s 8-connectivity neighborhoods as R. For Ru ∈ R,
its probability of being class c is denoted as Hu,c as de-
scribed above. Then Ru is classiﬁed based on P as fol-
lows:

the pixel at u is labeled as c;

1: if P (Hu,c, θc) then
2:
3: else
4:
5: end if

the pixel at u keeps unlabeled state.

After visiting all the positions, we append all the newly
labeled pixels to Sc. Once Sc is changed, we will visit
the updated Sc again. Otherwise, Vc stops. The termina-
tion criteria is different with classical SRG in which ev-
ery pixel must have a label. Because it is difﬁcult to tell
the label of a pixel with a low conﬁdence predicted by seg-
mentation network. However, with increasing capability of
segmentation network, the amount of unlabeled pixels de-
creases and the objects extent are covered with correct la-
bels. Besides, to reduce the redundancy visits in Vc, we
ﬁrst compute connected components of regions that meet
the requirement in Eqn (3), and then the connected compo-
nents which consist the initial seed regions take the same
label as the initial seed. These connected components are

4.1. Experimental setup

Dataset and Evaluation Metrics We evaluate the pro-
posed approach on the PASCAL VOC 2012 segmentation
benchmark dataset [8] and COCO dataset [17]. PASCAL
VOC: It contains three parts: training (train, 1464 images),
validation (val, 1449 images) and testing (test, 1456 im-
ages). Following the common practice [6, 33], we aug-
ment the training part by additional images from [9]. In our
experiments, only image-level labels are utilized for train-
ing. We compare our method with other state-of-the-arts
on both val and test sets. The standard intersection over
union (IOU) criterion and pixel-wise accuracy are adopted
for evaluation on PASCAL val dataset. The result on the
test set is obtained by submitting the predicted results to the
ofﬁcial PASCAL VOC evaluation server. COCO: its train-
ing set contains 80k samples with only image-level labels
and it’s val set contains 40k samples for evaluation. Per-
formance is evaluated in terms of pixel IoU averaged on 81
categories. Experimental analysis of the proposed approach
is conducted on the val set.

Training/Testing Settings
We adopt the slightly modiﬁed version of the 16-layer
VGG network from [14] for the classiﬁcation network and
DeepLab-ASPP from [6] for the segmentation network.
They are all initialized by the VGG-16 [31] pretrained on
ImageNet. SGD with mini-batch is used for training classi-
ﬁcation and segmentation network. We use the momentum
of 0.9 and a weight decay of 0.0005. The batch size is 20,
the dropout rate is 0.5 and the weight decay parameter is
0.0005. The initial learning rate is 5e-4 and it is decreased
by a factor of 10 every 2000 iterations.

For seed generation, those pixels belonging to top 20%
of the largest value (a fraction suggested by [14, 33]) in the
heatmap are considered as foreground object regions. We
use saliency maps from [12] to produce the background lo-
calization cues. We adopt the normalized saliency value
0.06 as the threshold to obtain background localization cues
(i.e. pixels whose saliency values are smaller than 0.06 are
considered as background). For the similarity criteria in
DSRG, we set θb and θf to 0.99 and 0.85, respectively. For
CRF, we use the default values from the Koltun public im-
plementation as parameters for the pairwise interactions.

In test phase, the learned segmentation network is ap-

7018

Table 1. Comparison of weakly-supervised semantic segmentation
methods on VOC 2012 val and test set

Method

Training Val Test

Supervision: Image-level Labels

(* methods implicitly use pixel-level supervision)

(† methods implicitly use box supervision)

Supervision: Image-level Labels

SN B* [34]

MIL-seg* [23]

TransferNet* [10]

AF-MCG* [24]

GuidedSeg† [19]

MIL-FCN [22]

CCNN [21]

MIL-bb [23]

EM-Adapt [20]

DCSM [29]

BFBP [27]

STC [35]

SEC [14]

AF-SS [24]

Combining Cues [26]

AE-PSL [33]

DCSP [4]

Supervision: Image-level Labels

DSRG (VGG16)

DSRG (Resnet101)

10k

41.9 40.6

700k

42.0 43.2

70k

10k

20k

10k

700k

700k

10k

10k

10k

50k

10k

10k

10k

10k

10k

10k

10k

52.1 51.2

54.3 55.5

55.7 56.7

25.7 24.9

35.3 35.6

37.8 37.0

38.2 39.6

44.1 45.1

46.6 48.0

49.8 51.2

50.7 51.7

52.6 52.7

52.8 53.7

55.0 55.7

58.6 59.2

59.0 60.41
61.4 63.22

plied to produce probability map for each testing image.
Then, we upscale the predicted probability map to match
the size of the input image, and then apply a fully-connected
CRF [15] to reﬁne the segmentation result.

Reproducibility. Our approach is implemented based
on Caffe [11]. All networks are trained on a single NVIDIA
GeForce GTX TITAN X GPU. The code is available at
https://github.com/speedinghzl/DSRG.

4.2. Comparisons with state of the arts

Results of other state-of-the-art weakly-supervised se-
mantic segmentation solutions on PASCAL VOC valida-
tion and test dataset are summarized in Table 1. We pro-

1http://host.robots.ox.ac.uk:8080/anonymous/

2http://host.robots.ox.ac.uk:8080/anonymous/

ZZT4TI.html

LWX93L.html

Image

Ground Truth  

Prediction 

Figure 3. Qualitative segmentation results on the VOC 2012 val
set. One failure case is shown in the last row.

vide these results for reference and emphasize that they
should not be directly compared with our method. Because
the methods were trained on different training sets or with
different kinds of annotations, bounding boxes, spots and
image-level labels. Among the approaches, CCNN [21],
MIL-seg [23], STC [35], GuidedSeg [19], and TransferNet
[10] use more images for training (700K, 700K, 50K, 20K
and 70K, respectively). All the other methods are based on
10K training images and built on top of the VGG16 model.

The results show that our method substantially outper-
forms all the previous techniques using image-level labels
for weak supervision. AE-PSL [33] and DCSP [4] achieve
the best performance among the baselines. However, adver-

7019

Table 2. Comparison of mIoU using different settings of our approach on VOC 2012 val set

g
k
Method b

e
n
a
l
p

e
k
i
b

d
r
i
b

t
a
o
b

e
l
t
t
o
b

s
u
b

r
a
c

t
a
c

r
i
a
h
c

w
o
c

e
l
b
a
t

g
o
d

e
s
r
o
h

r
o
t
o
m

n
o
s
r
e
p

t
n
a
l
p

p
e
e
h
s

a
f
o
s

n
i
a
r
t

v mIoU

t

baseline 82.5 67.5 23.2 65.7 29.7 47.5 71.8 66.8 76.7 23.3 51.7 26.2 69.7 54.2 63.2 57.2 33.7 64.5 33.5 48.7 46.1 52.5

+BSL 82.4 71.9 29.1 67.7 32.4 49.8 75.5 67.9 74.7 22.8 54.9 26.6 64.3 55.7 64.7 56.0 35.0 67.7 32.7 50.2 45.8 53.6

+DSRG 86.6 70.5 28.8 70.6 34.7 55.7 74.9 70.1 80.2 24.1 63.6 24.8 76.6 64.1 64.9 72.3 38.5 68.7 35.8 51.8 51.9 57.6

+Retrain 87.5 73.1 28.4 75.4 39.5 54.5 78.2 71.3 80.6 25.0 63.3 25.4 77.8 65.4 65.2 72.8 41.2 74.3 34.1 52.1 53.0 59.0

sarial erasing is employed by AE-PSL to expand the seed
cues for supervision, which needs to iteratively train multi-
ple classiﬁcation networks. DCSP also utilizes adversarial
erasing manner to allow the saliency network to discover
new salient regions of object.
It does not require the re-
training of the network after each erasing, but DCSP may
introduce some true negative regions due to over erasing.
In contrast, the proposed DSRG approach is very simple
and convenient to reﬁne supervision online and our method
obtains better results than DCSP and AE-PSL. Compared
with those methods only using image-level labels for su-
pervision, the proposed DSRG(VGG16) method improves
upon the best performance by over 1.2% on test set. It can
be seen that our method achieves 60.4% mIoU on test set.
Besides, Our DSRG (Resnet101) achieves 63.2% mIOU on
test set.

4.3. Qualitative results

Fig. 3 shows some successful segmentation results.
It
shows our method can produce accurate segmentations even
for complicated images and recover ﬁne details of the
boundary. One typical failure case is given in the bot-
tom row of Fig. 3. This failure mode is that the model
cannot pick out object regions from background precisely.
As is typical for weakly-supervised systems, strongly co-
occurring categories (such as train and rails, sculls and oars,
snowbikes and snow) cannot be separated without ﬁnner-
grained information [13].

4.4. Ablation studies

In order to further prove the effect of the different com-
ponents, we conduct some ablation experiments with differ-
ent settings of VGG16 based DSRG. In Table 2, the “base-
line” denotes our implemented SEC [14], our result is much
better than [14] (50.4 mAP without Lexpand), due to the dif-
ferent background locating technology [12] and details. The
“+BSL” denotes replacing the original seeding loss with the
balanced seeding loss in Eqn (1); the “+DSRG” denotes
adding DSRG training approach. We can observe that the
weighted seeding loss improves the performance by 1.1%
compared with baseline. And, DSRG improves further the
performance by 4%, demonstrating the signiﬁcant effective-
ness of DSRG. It is most noticeable for animals and person,

Table 3. Per-class IOU on COCO using image tags during training

l
a
m
i
n
A

e
l
c
i
h
e
V

r
o
o
d
t
u
O

Cat.
  Class
BG   background
  person
P
  bicycle
  car
  motorcycle
  airplane
  bus
  train
  truck
  boat
  traffic light
  fire hydrant
  stop sign
  parking meter
  bench
  bird
  cat
  dog
  horse
  sheep
  cow
  elephant
  bear
  zebra
  giraffe
  backpack
  umbrella
  handbag
  tie
  suitcase
  frisbee
  skis
  snowboard
  sports ball
  kite
  baseball bat
  baseball glove
  skateboard
  surfboard
  tennis racket
  bottle

y
r
o
s
s
e
c
c
A

t
r
o
p
S

K

80.6

d
o
o
F

e
r
u
t
i
n
r
u
F

e
r
a
w
n
e
h
c
t
i

SEC BFBP Ours Cat.
74.3
43.6
24.2
15.9
52.1
36.6
37.7
30.1
24.1
17.3
16.7
55.9
48.4
25.2
16.4
34.7
57.2
45.2
34.4
40.3
41.4
62.9
59.1
59.8
48.8
0.3
26.0
0.5
6.5
16.7
12.3
1.6
5.3
7.9
9.1
1.0
0.6
7.1
7.7
9.1
13.2

68.8
27.5
18.2
7.2
40.5
32.0
39.2
26.5
17.5
16.5
3.9
33.1
28.4
25.5
12.4
31.1
52.8
44.1
34.2
38.0
42.1
65.2
57.0
65.0
55.6
3.2
28.1
1.1
5.5
21.3
5.6
1.0
2.8
1.9
10.3
1.7
0.5
6.6
3.3
5.5
9.6

30.4
22.1
54.2
45.2
38.7
33.2
25.9
20.6
16.2
60.4
51.0
26.3
22.3
41.5
62.2
55.6
42.3
47.1
49.3
67.1
62.6
63.2
54.3
0.2
35.3
0.7
7.0
23.4
13.0
1.5
16.3
9.8
17.4
4.8
1.2
14.4
13.5
6.8
22.3

s
c
i
n
o
r
t
c
e
l
E

e
c
n
a
i
l
p
p
A

r
o
o
d
n
I

  Class
  wine glass
  cup
  fork
  knife
  spoon
  bowl
  banana
  apple
  sandwich
  orange
  broccoli
  carrot
  hot dog
  pizza
  donut
  cake
  chair
  couch
  potted plant
  bed
  dining table
  toilet
  tv
  laptop
  mouse
  remote
  keyboard
  cell phone
  microwave
  oven
  toaster
  sink
  refrigerator
  book
  clock
  vase
  scissors
  teddy bear
  hair dryer
  toothbrush
  mean IOU

SEC BFBP Ours
24.0
17.5
22.3
20.4
5.6
17.9
0.0
0.5
1.8
5.0
1.0
1.4
0.5
0.6
0.6
18.8
13.3
12.5
46.4
44.9
43.6
24.3
18.9
23.6
24.5
21.4
22.8
41.2
35.0
44.3
35.7
27.0
36.8
15.3
16.0
6.7
24.9
22.5
31.2
56.2
57.8
50.9
34.2
36.2
32.8
6.9
17.0
12.0
9.7
8.2
7.8
17.7
13.9
5.6
14.3
7.4
6.2
32.4
29.8
23.4
3.8
2.0
0.0
43.6
30.1
38.5
25.3
14.8
19.2
21.1
19.9
20.1
0.9
0.4
3.5
20.6
9.9
17.5
12.3
19.9
12.5
33.0
26.1
32.1
11.2
9.8
8.2
12.4
16.4
13.7
0.0
0.0
0.0
17.8
9.5
10.8
15.5
13.2
4.0
12.3
7.5
0.4
20.7
16.5
17.8
23.9
13.4
18.4
17.3
12.2
16.5
46.3
41.0
47.0
0.0
0.0
0.0
4.5
2.0
2.8
26.0
20.4
22.4

e.g. the improvement for segmenting dog/horse/cow/person
is about 10%. Besides, we ﬁrst employ the trained seg-
mentation model of “+DSRG” to on all the training im-
ages. Then, the predicted segmentation masks are used as
supervision for training the segmentation network for an-
other round in a fully-supervised way. As shown in Table 2,
the performance provided by this extra training (denoted as
“+Retrain”) is further improved from 57.6% to 59.0%. We
do not observe further performance gain by performing ad-
ditional retrain steps.

In addition, we tried different values of θf and θb to
ﬁnd the best performing region growing strategy. The re-

7020

Table 4. Performance on PASCAL VOC 2012 val dataset for dif-
ferent θ

θf

θb

0.99

0.95

0.90

0.99

0.95

0.90

0.85

0.80

57.45

57.59

57.63

57.69

57.66

57.43

57.56

57.64

57.67

57.63

57.23

57.35

57.40

57.44

57.45

sults are shown for different values of θ in Tab 4. The re-
sults show that our method is robust to the region growing
thresholds θ. To explore the effect of only performing re-
gion growing for foreground or background object, we set
θb = ∞, θf = 0.85 for only conducting region growing
for foreground object, the performance on PASCAL VOC
val dataset is 55.9% mIoU. When θb = 0.99, θf = ∞,
the performance is 54.3% mIoU. The results show that only
conducting region growing for foreground object or back-
ground object is also improve the performance. However,
it can achieve best performance when simultaneously con-
ducting region growing for foreground object and back-
ground object.

4.5. The quality improvement of dynamic supervi 

sion over epochs

In this section the qualities of the new pixel labels as dy-
namic supervision, obtained from DSRG, at each epoch, are
evaluated. Compared with ground truths that are annotated
by human, we could use the mean accuracy, mean recall and
IoU to measure the quality of the supervision reﬁned by our
approach. In Fig. 4, the supervision that generated by classi-
ﬁcation network has somewhat high precision(62.6%), low
recall(32.1%) and low IoU(30.0%). With the increasing of
epochs, the precision of seed remains a high value, and the
recall and IoU get signiﬁcant improvements. At epoch #12,
the mean precision, mean recall and mean IoU are 63.9%,
65.4%, and 57.1%, respectively. It demonstrates that DSRG
can ﬁnd the object extent and improve the quality of su-
pervision, which explains why the proposed DSRG training
procedure works excellently on the weakly supervised se-
mantic segmentation task. Additional examples in the sup-
plementary materials shows the gradually reﬁning supervi-
sion starting from seed cues during training.

4.6. COCO results

To further demonstrate the generality of our method, we
conducted a set of experiments on COCO. Unlike in PAS-
CAL VOC, the majority of COCO samples were collected
from non-iconic images in a complex natural context. We
provide the per-class IoU of SEC [14], BFBP [27] and our
approach in Table 3. Our VGG16 based DSRG obtains re-
markable better results, especially in Person, Animal, Ve-

)

%

(

n
o
i
s
i
v
r
e
p
u
s
 
e
h
t
 
f
o
y
t
i
l

 

a
u
q
e
h
T

 

70

65

60

55

50

45

40

35

30

25

Precision

Recall

IoU

12

0

1

2

Epoch #

Figure 4. The quality of the dynamic supervision (%) with respect
to the epochs.

hicle etc, but performs poorly on small ones, such as In-
door and Kitchenware. Altogether, our DSRG method im-
proves upon the best performance by over 3.6% on val set.
It can be seen that our method achieves 26.0% mIoU on val
set. Meanwhile, compared with the performance of fully
supervised method (40.98% mIoU), these results on COCO
evidence that there is much space for progress in weakly-
supervised semantic segmentation. Developing solutions
that handle small objects could be an interesting direction
for future research.

5. Conclusion and future work

We have addressed the problem of training semantic
segmentation networks only using image-level supervision.
Image-level labels alone can provide high-quality seeds, or
discriminative object regions, but inferring full object ex-
tents is a very difﬁcult problem. We propose a DSRG train-
ing approach gradually improves the quality and extent ob-
ject regions and itself is supervised the object regions. We
demonstrate that our approach outperforms previous state-
of-the-art methods under the same experimental conditions.
We also clearly identify the effectiveness of region growing
mechanism within the semantic segmentation network in
the experiments. In future work, we will focus on designing
more effective weakly-supervised strategies and improving
seed quality.

Acknowledgements

We really appreciate the enormous help from Lichao
Huang at Horizon Robotics and Chunyu Wang at MSRA.
This work was partly supported by NSFC (No.61733007,
No.61503145) and the fund of HUST-Horizon Computer
Vision Research Center. Xinggang Wang was sponsored by
CCF-Tencent Open Research Fund, the Program for HUST
Academic Frontier Youth Team, and Young Elite Sponsor-
ship Program by CAST, No. YESS 20150077.

7021

References

[1] R. Adams and L. Bischof. Seeded region growing.

IEEE

TPAMI, 16(6):641–647, 1994. 2, 3, 4

[2] A. Bearman, O. Russakovsky, V. Ferrari, and L. Fei-Fei.
Whats the point: Semantic segmentation with point super-
vision. In Proc. ECCV, pages 549–565. Springer, 2016. 2
[3] V. Borges, M. C. F. de Oliveira, T. Silva, A. Vieira, and
B. Hamann. Region growing for segmenting green microal-
gae images. IEEE/ACM Transactions on Computational Bi-
ology and Bioinformatics, 2016. 3

[4] A. Chaudhry, P. K. Dokania, and P. H. Torr. Discover-
ing class-speciﬁc pixels for weakly-supervised semantic seg-
mentation. arXiv preprint arXiv:1707.05821, 2017. 3, 6
[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Semantic image segmentation with deep con-
arXiv preprint
volutional nets and fully connected crfs.
arXiv:1412.7062, 2014. 1, 2

[6] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. arXiv preprint arXiv:1606.00915, 2016. 2, 5
[7] J. Dai, K. He, and J. Sun. Boxsup: Exploiting bounding
boxes to supervise convolutional networks for semantic seg-
mentation. In Proc. ICCV, pages 1635–1643, 2015. 3
[8] M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams,
J. Winn, and A. Zisserman. The pascal visual object classes
challenge: A retrospective. IJCV, 111(1):98–136, 2015. 2, 5
[9] B. Hariharan, P. Arbel´aez, L. Bourdev, S. Maji, and J. Malik.
In Proc. ICCV,

Semantic contours from inverse detectors.
pages 991–998. IEEE, 2011. 5

[10] S. Hong, J. Oh, H. Lee, and B. Han. Learning transferrable
knowledge for semantic segmentation with deep convolu-
In Proc. CVPR, pages 3204–3212,
tional neural network.
2016. 6

[11] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding. In Proc. ACM, pages
675–678. ACM, 2014. 6

[12] H. Jiang, J. Wang, Z. Yuan, Y. Wu, N. Zheng, and S. Li.
Salient object detection: A discriminative regional feature
In Proc. CVPR, pages 2083–2090,
integration approach.
2013. 3, 5, 7

[13] A. Kolesnikov and C. H. Lampert.

Improving weakly-
supervised object localization by micro-annotation. arXiv
preprint arXiv:1605.05538, 2016. 7

[14] A. Kolesnikov and C. H. Lampert. Seed, expand and con-
strain: Three principles for weakly-supervised image seg-
mentation. In Proc. ECCV, pages 695–711. Springer, 2016.
1, 2, 3, 4, 5, 6, 7, 8

[15] V. Koltun. Efﬁcient inference in fully connected crfs with
gaussian edge potentials. Proc. NIPS, 2(3):4, 2011. 6

[16] D. Lin, J. Dai, J. Jia, K. He, and J. Sun.

Scribble-
sup: Scribble-supervised convolutional networks for seman-
tic segmentation. In Proc. CVPR, pages 3159–3167, 2016.
2

[17] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In European conference on computer
vision, pages 740–755. Springer, 2014. 5

[18] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In Proc. CVPR, pages
3431–3440, 2015. 1

[19] S. J. Oh, R. Benenson, A. Khoreva, Z. Akata, M. Fritz, and
B. Schiele. Exploiting saliency for object segmentation from
image level labels. arXiv preprint arXiv:1701.08261, 2017.
2, 3, 6

[20] G. Papandreou, L.-C. Chen, K. Murphy, and A. L. Yuille.
Weakly-and semi-supervised learning of a dcnn for seman-
tic image segmentation. arXiv preprint arXiv:1502.02734,
2015. 2, 3, 6

[21] D. Pathak, P. Krahenbuhl, and T. Darrell. Constrained con-
volutional neural networks for weakly supervised segmenta-
tion. In Proceedings CVPR, pages 1796–1804, 2015. 6
[22] D. Pathak, E. Shelhamer, J. Long, and T. Darrell. Fully
convolutional multi-class multiple instance learning. arXiv
preprint arXiv:1412.7144, 2014. 2, 6

[23] P. O. Pinheiro and R. Collobert. From image-level to pixel-
level labeling with convolutional networks. In Proc. CVPR,
pages 1713–1721, 2015. 3, 6

[24] X. Qi, Z. Liu, J. Shi, H. Zhao, and J. Jia. Augmented feed-
back in semantic segmentation under image level supervi-
sion. In Proc. ECCV, pages 90–105. Springer, 2016. 3, 6
[25] X. Ren and J. Malik. Learning a classiﬁcation model for

segmentation. In null, page 10. IEEE, 2003. 4

[26] A. Roy and S. Todorovic. Combining bottom-up, top-down,
and smoothness cues for weakly supervised image segmen-
tation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 3529–3538, 2017. 3,
6

[27] F. Saleh, M. S. A. Akbarian, M. Salzmann, L. Petersson,
S. Gould, and J. M. Alvarez. Built-in foreground/background
prior for weakly-supervised semantic segmentation. In Proc.
ECCV, pages 413–432. Springer, 2016. 3, 6, 8

[28] F. Y. Shih and S. Cheng. Automatic seeded region growing
for color image segmentation. Image and vision computing,
23(10):877–886, 2005. 3

[29] W. Shimoda and K. Yanai. Distinct class-speciﬁc saliency
maps for weakly supervised semantic segmentation. In Proc.
ECCV, pages 218–234. Springer, 2016. 3, 6

[30] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside
convolutional networks: Visualising image classiﬁcation
models and saliency maps. arXiv preprint arXiv:1312.6034,
2013. 3

[31] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 5

[32] P. Tang, X. Wang, X. Bai, and W. Liu. Multiple instance
detection network with online instance classiﬁer reﬁnement.
In Proc. CVPR, 2017. 1

[33] Y. Wei, J. Feng, X. Liang, M.-M. Cheng, Y. Zhao, and
S. Yan. Object region mining with adversarial erasing:
A simple classiﬁcation to semantic segmentation approach.
arXiv preprint arXiv:1703.08448, 2017. 3, 5, 6

7022

[34] Y. Wei, X. Liang, Y. Chen, Z. Jie, Y. Xiao, Y. Zhao, and
S. Yan. Learning to segment with image-level annotations.
Pattern Recognition, 59:234–244, 2016. 6

[35] Y. Wei, X. Liang, Y. Chen, X. Shen, M.-M. Cheng, J. Feng,
Y. Zhao, and S. Yan. Stc: A simple to complex framework
for weakly-supervised semantic segmentation. IEEE TPAMI,
2016. 3, 6

[36] J. Xu, A. G. Schwing, and R. Urtasun. Learning to segment
under various forms of weak supervision. In Proc. CVPR,
pages 3781–3790, 2015. 2

[37] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Tor-
ralba. Learning deep features for discriminative localization.
In Proc. CVPR, pages 2921–2929, 2016. 2, 3

7023

