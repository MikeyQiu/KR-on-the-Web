6
1
0
2
 
p
e
S
 
0
3
 
 
]
L
C
.
s
c
[
 
 
1
v
2
5
5
9
0
.
9
0
6
1
:
v
i
X
r
a

Controlling Output Length in Neural Encoder-Decoders

Yuta Kikuchi1
kikuchi@lr.pi.titech.ac.jp

Graham Neubig2∗
gneubig@cs.cmu.edu

Ryohei Sasano1
sasano@pi.titech.ac.jp

Hiroya Takamura1
takamura@pi.titech.ac.jp

Manabu Okumura1
oku@pi.titecjh.ac.jp

1Tokyo Institute of Technology, Japan
2Carnegie Mellon University, USA

Abstract

Neural encoder-decoder models have shown
great success in many sequence generation
tasks. However, previous work has not in-
vestigated situations in which we would like
to control the length of encoder-decoder out-
puts. This capability is crucial for applica-
tions such as text summarization, in which
we have to generate concise summaries with
In this paper, we pro-
a desired length.
pose methods for controlling the output se-
quence length for neural encoder-decoder
models: two decoding-based methods and two
learning-based methods.1 Results show that
our learning-based methods have the capabil-
ity to control length without degrading sum-
mary quality in a summarization task.

1

Introduction

Since its ﬁrst use for machine translation (Kalch-
brenner and Blunsom, 2013; Cho et al., 2014;
Sutskever et al., 2014),
the encoder-decoder ap-
proach has demonstrated great success in many
other sequence generation tasks including image
caption generation (Vinyals et al., 2015b; Xu et
al., 2015), parsing (Vinyals et al., 2015a), dialogue
response generation (Li et al., 2016a; Serban et
al., 2016) and sentence summarization (Rush et al.,
2015; Chopra et al., 2016). In particular, in this pa-
per we focus on sentence summarization, which as
its name suggests, consists of generating shorter ver-
sions of sentences for applications such as document

∗ This work was done when the author was at the Nara In-

stitute of Science and Technology.

1Available at https://github.com/kiyukuta/lencon.

summarization (Nenkova and McKeown, 2011) or
headline generation (Dorr et al., 2003). Recently,
Rush et al. (2015) automatically constructed large
training data for sentence summarization, and this
has led to the rapid development of neural sentence
summarization (NSS) or neural headline generation
(NHG) models. There are already many studies that
address this task (Nallapati et al., 2016; Ayana et al.,
2016; Ranzato et al., 2015; Lopyrev, 2015; Gulcehre
et al., 2016; Gu et al., 2016; Chopra et al., 2016).

One of the essential properties that text summa-
rization systems should have is the ability to gen-
erate a summary with the desired length. Desired
lengths of summaries strongly depends on the scene
of use, such as the granularity of information the
user wants to understand, or the monitor size of the
device the user has. The length also depends on the
amount of information contained in the given source
document. Hence, in the traditional setting of text
summarization, both the source document and the
desired length of the summary will be given as input
to a summarization system. However, methods for
controlling the output sequence length of encoder-
decoder models have not been investigated yet, de-
spite their importance in these settings.

In this paper, we propose and investigate four
methods for controlling the output sequence length
for neural encoder-decoder models. The former two
methods are decoding-based; they receive the de-
sired length during the decoding process, and the
training process is the same as standard encoder-
decoder models.
The latter two methods are
learning-based; we modify the network architecture
to receive the desired length as input.

In experiments, we show that the learning-based
methods outperform the decoding-based methods
for long (such as 50 or 75 byte) summaries. We
also ﬁnd that despite this additional length-control
capability, the proposed methods remain competi-
tive to existing methods on standard settings of the
DUC2004 shared task-1.

2 Background

2.1 Related Work

Text summarization is one of the oldest ﬁelds of
study in natural language processing, and many
summarization methods have focused speciﬁcally
on sentence compression or headline generation.
Traditional approaches to this task focus on word
deletion using rule-based (Dorr et al., 2003; Zajic
et al., 2004) or statistical (Woodsend et al., 2010;
Galanis and Androutsopoulos, 2010; Filippova and
Strube, 2008; Filippova and Altun, 2013; Filip-
pova et al., 2015) methods. There are also several
studies of abstractive sentence summarization us-
ing syntactic transduction (Cohn and Lapata, 2008;
Napoles et al., 2011) or taking a phrase-based sta-
tistical machine translation approach (Banko et al.,
2000; Wubben et al., 2012; Cohn and Lapata, 2013).
Recent work has adopted techniques such as
encoder-decoder (Kalchbrenner and Blunsom, 2013;
Sutskever et al., 2014; Cho et al., 2014) and atten-
tional (Bahdanau et al., 2015; Luong et al., 2015)
neural network models from the ﬁeld of machine
translation, and tailored them to the sentence sum-
marization task. Rush et al. (2015) were the ﬁrst
to pose sentence summarization as a new target task
for neural sequence-to-sequence learning. Several
studies have used this task as one of the bench-
marks of their neural sequence transduction meth-
ods (Ranzato et al., 2015; Lopyrev, 2015; Ayana
et al., 2016). Some studies address the other im-
portant phenomena frequently occurred in human-
written summaries, such as copying from the source
document (Gu et al., 2016; Gulcehre et al., 2016).
Nallapati et al.
(2016) investigate a way to solve
many important problems capturing keywords, or
inputting multiple sentences.

Neural encoder-decoders can also be viewed as
statistical language models conditioned on the tar-
get sentence context. Rosenfeld et al. (2001) have

proposed whole-sentence language models that can
consider features such as sentence length. However,
as described in the introduction, to our knowledge,
explicitly controlling length of output sequences in
neural language models or encoder-decoders has not
been investigated.

Finally, there are some studies to modify the out-
put sequence according some meta information such
as the dialogue act (Wen et al., 2015), user person-
ality (Li et al., 2016b), or politeness (Sennrich et al.,
2016). However, these studies have not focused on
length, the topic of this paper.

2.2

Importance of Controlling Output Length

As we already mentioned in Section 1, the most
standard setting in text summarization is to input
both the source document and the desired length of
the summary to a summarization system. Summa-
rization systems thus must be able to generate sum-
maries of various lengths. Obviously, this property
is also essential for summarization methods based
on neural encoder-decoder models.

Since an encoder-decoder model is a completely
data-driven approach, the output sequence length
depends on the training data that the model is trained
on. For example, we use sentence-summary pairs
extracted from the Annotated English Gigaword cor-
pus as training data (Rush et al., 2015), and the
average length of human-written summary is 51.38
bytes. Figure 1 shows the statistics of the corpus.
When we train a standard encoder-decoder model
and perform the standard beam search decoding on
the corpus, the average length of its output sequence
is 38.02 byte.

However,

there are other situations where we
want summaries with other lengths. For exam-
ple, DUC2004 is a shared task where the maximum
length of summaries is set to 75 bytes, and summa-
rization systems would beneﬁt from generating sen-
tences up to this length limit.

While recent NSS models themselves cannot con-
trol their output length, Rush et al. (2015) and others
following use an ad-hoc method, in which the sys-
tem is inhibited from generating the end-of-sentence
to the tag and
(EOS) tag by assigning a score of

−∞

(a) ﬁrst sentence (181.87)

(b) article headline (51.38)
Figure 1: Histograms of ﬁrst sentence length, headline length, and their ratio in Annotated Gigaword English Giga-

(c) ratio (0.30)

word corpus. Bracketed values in each subcaption are averages.

a given source sentence, the summarizer generates
a shortened version of the input (i.e. N > M ),
as summary sentence y = (y1, y2, y3, ..., yM ). The
x) us-
model estimates conditional probability p(y
|
ing parameters trained on large training data consist-
ing of sentence-summary pairs. Typically, this con-
ditional probability is factorized as the product of
conditional probabilities of the next word in the se-
quence:

p(y

x) =
|

p(yt

y<t, x),

|

M
(cid:89)

t=1

where y<t = (y1, y2, y3, ..., yt−1). In the following,
we describe how to compute p(yt

y<t, x).
|

3.1 Encoder

We use the bi-directional RNN (BiRNN) as en-
coder which has been shown effective in neural ma-
chine translation (Bahdanau et al., 2015) and speech
recognition (Schuster and Paliwal, 1997; Graves et
al., 2013).

A BiRNN processes the source sentence for
both forward and backward directions with two
separate RNNs. During the encoding process,
the BiRNN computes both forward hidden states
(−→h 1, −→h 2, ..., −→h N ) and backward hidden states
(←−h 1, ←−h 2, ..., ←−h N ) as follows:

−→h t = g(−→h t−1, xt),

←−h t = g(←−h t+1, xt).

While g can be any kind of recurrent unit, we use
long short-term memory (LSTM) (Hochreiter and
Schmidhuber, 1997) networks that have memory
cells for both directions (−→c t and ←−c t).

Figure 2: The encoder-decoder architecture we used as a

base model in this paper.

generating a ﬁxed number of words2, and ﬁnally the
output summaries are truncated to 75 bytes. Ideally,
the models should be able to change the output se-
quence depending on the given output length, and to
output the EOS tag at the appropriate time point in a
natural manner.

3 Network Architecture: Encoder-Decoder

with Attention

In this section, we describe the model architec-
ture used for our experiments: an encoder-decoder
consisting of bi-directional RNNs and an attention
mechanism. Figure 2 shows the architecture of the
model.

Suppose that the source sentence is represented as
a sequence of words x = (x1, x2, x3, ..., xN ). For

to

the

2According

code
num-
(https://github.com/facebook/NAMAS),
ber of words is set to 15, which is too long for the DUC2004
setting. The average number of words of human summaries in
the evaluation set is 10.43.

published
the

default

After encoding, we set the initial hidden states s0

4 Controlling Length in Encoder-decoders

and memory-cell m0 of the decoder as follows:

s0 = ←−h 1,
m0 = ←−c 1.

3.2 Decoder and Attender

Our decoder is based on an RNN with LSTM g:

st = g(st−1, xt).

We also use the attention mechanism developed
by Luong et al. (2015), which uses st to compute
contextual information dt of time step t. We ﬁrst
summarize the forward and backward encoder states
by taking their sum ¯hi = −→h i + ←−h i, and then calcu-
late the context vector dt as the weighted sum of
these summarized vectors:

dt =

(cid:88)

ati¯hi,

i

where at is the weight at the t-th step for ¯hi com-
puted by a softmax operation:

ati =

exp(st
·
¯h(cid:48) exp(st

(cid:80)

¯hi)

.

¯h(cid:48))

·

After context vector dt is calculated, the model
updates the distribution over the next word as fol-
lows:

˜st = tanh(Whs[st; dt] + bhs),

p(yt

y<t, x) = softmax(Wso ˜st + bso).
|

Note that ˜st is also provided as input to the LSTM
with yt for the next step, which is called the input
feeding architecture (Luong et al., 2015).

3.3 Training and Decoding

The training objective of our models is to maximize
log likelihood of the sentence-summary pairs in a
given training set D:

Lt(θ) =

(cid:88)

log p(y

x; θ),

|

(x,y)∈D
(cid:89)

p(y

x; θ) =

|

p(yt

y<t, x).
|

t

Once models are trained, we use beam search to ﬁnd
the output that maximizes the conditional probabil-
ity.

In this section, we propose our four methods that
can control the length of the output in the encoder-
In the ﬁrst two methods, the
decoder framework.
decoding process is used to control the output length
without changing the model itself. In the other two
methods, the model itself has been changed and is
trained to obtain the capability of controlling the
length. Following the evaluation dataset used in our
experiments, we use bytes as the unit of length, al-
though our models can use either words or bytes as
necessary.

4.1 f ixLen: Beam Search without EOS Tags

The ﬁrst method we examine is a decoding approach
similar to the one taken in many recent NSS meth-
ods that is slightly less ad-hoc. In this method, we
inhibit the decoder from generating the EOS tag by
assigning it a score of
. Since the model can-
−∞
not stop the decoding process by itself, we simply
stop the decoding process when the length of output
sequence reaches the desired length. More speciﬁ-
cally, during beam search, when the length of the se-
quence generated so far exceeds the desired length,
the last word is replaced with the EOS tag and also
the score of the last word is replaced with the score
of the EOS tag (EOS replacement).

4.2 f ixRng: Discarding Out-of-range

Sequences

Our second decoding method is based on discarding
out-of-range sequences, and is not inhibited from
generating the EOS tag, allowing it to decide when
to stop generation. Instead, we deﬁne the legitimate
range of the sequence by setting minimum and max-
imum lengths. Speciﬁcally, in addition to the normal
beam search procedure, we set two rules:

•

•

If the model generates the EOS tag when the
output sequence is shorter than the minimum
length, we discard the sequence from the beam.

If the generated sequence exceeds the maxi-
mum length, we also discard the sequence from
the beam. We then replace its last word with
the EOS tag and add this sequence to the beam

(EOS replacement in Section 4.1).3

In other words, we keep only the sequences that
contain the EOS tag and are in the deﬁned length
range. This method is a compromise that allows
the model some ﬂexibility to plan the generated se-
quences, but only within a certain acceptable length
range.

It should be noted that this method needs a larger
beam size if the desired length is very different from
the average summary length in the training data, as
it will need to preserve hypotheses that have the de-
sired length.

4.3 LenEmb: Length Embedding as
Additional Input for the LSTM

Our third method is a learning-based method specif-
ically trained to control the length of the output se-
quence. Inspired by previous work that has demon-
strated that additional inputs to decoder models can
effectively control the characteristics of the output
(Wen et al., 2015; Li et al., 2016b), this model pro-
vides information about the length in the form of an
additional input to the net. Speciﬁcally, the model
RD for each potential
uses an embedding e2(lt)
desired length, which is parameterized by a length
RD×L where L is the
embedding matrix Wle
∈
number of length types. In the decoding process, we
input the embedding of the remaining length lt as
additional input to the LSTM (Figure 3). lt is initial-
ized after the encoding process and updated during
the decoding process as follows:

∈

l1 = length,

lt+1 =

(cid:26) 0
lt

byte(yt)

−

byte(yt)

(lt
(otherwise),

−

≤

0)

where byte(yt) is the length of output word yt and
length is the desired length. We learn the values
of the length embedding matrix Wle during train-
ing. This method provides additional information
about the amount of length remaining in the output
sequence, allowing the decoder to “plan” its output
based on the remaining number of words it can gen-
erate.

3This is a workaround to prevent the situation in which all

sequences are discarded from a beam.

Figure 3: LenEmb: remaining length is used as addi-

tional input for the LSTM of the decoder.

Figure 4: LenInit: initial state of the decoder’s memory

cell m0 manages output length.

4.4 LenInit: Length-based Memory Cell

Initialization

While LenEmb inputs the remaining length lt to the
decoder at each step of the decoding process, the
LenInit method inputs the desired length once at
the initial state of the decoder. Figure 4 shows the ar-
chitecture of LenInit. Speciﬁcally, the model uses
the memory cell mt to control the output length by
initializing the states of decoder (hidden state s0 and
memory cell m0) as follows:

s0 = ←−h 1,
m0 = bc ∗

length,

(1)

where bc
∈
is the desired length.

RH is a trainable parameter and length

While the model of LenEmb is guided towards
the appropriate output length by inputting the re-
maining length at each step, this LenInit attempts
to provide the model with the ability to manage the
output length on its own using its inner state. Specif-
ically, the memory cell of LSTM networks is suit-
able for this endeavour, as it is possible for LSTMs

(a) ﬁrst sentence (206.91)

(c) ratio (0.35)
(b) summary (70.00)
Figure 5: Histograms of ﬁrst sentence length, summary length, and their ratio in DUC2004.

to learn functions that, for example, subtract a ﬁxed
amount from a particular memory cell every time
they output a word. Although other ways for man-
aging the length are also possible,4 we found this
approach to be both simple and effective.

5 Experiment

5.1 Dataset

We trained our models on a part of the Annotated
English Gigaword corpus (Napoles et al., 2012),
which Rush et al. (2015) constructed for sentence
summarization. We perform preprocessing using the
standard script for the dataset5. The dataset con-
sists of approximately 3.6 million pairs of the ﬁrst
sentence from each source document and its head-
line. Figure 1 shows the length histograms of the
summaries in the training set. The vocabulary size
is 116,875 for the source documents and 67,564
for the target summaries including the beginning-of-
sentence, end-of-sentence, and unknown word tags.
For LenEmb and LenInit, we input the length of
each headline during training. Note that we do not
train multiple summarization models for each head-
line length, but a single model that is capable of con-
trolling the length of its output.

We evaluate the methods on the evaluation set
of DUC2004 task-1 (generating very short single-
document summaries). In this task, summarization
systems are required to create a very short sum-
mary for each given document. Summaries over
the length limit (75 bytes) will be truncated and
there is no bonus for creating a shorter summary.
The evaluation set consists of 500 source documents
and 4 human-written (reference) summaries for each

4For example, we can also add another memory cell for

managing the length.

5https://github.com/facebook/NAMAS

source document. Figure 5 shows the length his-
tograms of the summaries in the evaluation set. Note
that the human-written summaries are not always as
long as 75 bytes. We used three variants of ROUGE
(Lin, 2004) as evaluation metrics: ROUGE-1 (uni-
gram), ROUGE-2 (bigram), and ROUGE-L (longest
common subsequence). The two-sided permutation
test (Chinchor, 1992) was used for statistical signif-
icance testing (p

0.05).

≤

5.2

Implementation

We use Adam (Kingma and Ba, 2015) (α=0.001,
β1=0.9, β2=0.999, eps=10−8) to optimize param-
eters with a mini-batch of size 80. Before every
10,000 updates, we ﬁrst sampled 800,000 training
examples and made groups of 80 examples with
the same source sentence length, and shufﬂed the
10,000 groups.

We set the dimension of word embeddings to 100
and that of the hidden state to 200. For LSTMs,
we initialize the bias of the forget gate to 1.0 and
use 0.0 for the other gate biases (J´ozefowicz et al.,
2015). We use Chainer (Tokui et al., 2015) to im-
plement our models. For LenEmb, we set L to 300,
which is larger than the longest summary lengths in
our dataset (see Figure 1-(b) and Figure 5-(b)).

For all methods except f ixRng, we found a beam
size of 10 to be sufﬁcient, but for f ixRng we used
a beam size of 30 because it more aggressively dis-
cards candidate sequences from its beams during de-
coding.

6 Result

6.1 ROUGE Evaluation

Table 1 shows the ROUGE scores of each method
with various length limits (30, 50 and 75 byte). Re-
gardless of the length limit set for the summariza-

model
f ixLen
f ixRng
LenEmb(0,L)
LenInit(0,L)
LenEmb(0,∞)
LenInit(0,∞)

30 byte
R-2
3.10∗
3.08∗
3.21
3.27
3.30
3.49

R-1
14.34
13.83∗
14.23
14.31
13.75
13.92

R-L
13.23
12.88
13.02
13.19
12.68
12.90

R-1
20.00∗
20.08∗
20.78
20.87
20.62
20.87

R-L
18.26∗
18.19∗
18.57
19.00
18.64
19.09

R-1
25.87∗
26.01
26.73
25.87
26.42
25.29∗

75 byte
R-2
7.93
7.69∗
8.39
8.27
8.26
8.00

R-L
23.07∗
22.77∗
23.88
23.24
23.59
22.71∗

50 byte
R-2
5.98
5.74
5.97
6.16
6.22
6.19

∗

Table 1: ROUGE scores with various length limits. The scores with

are signiﬁcantly worse than the best score in

the column (bolded).

source

reference
f ixLen (30)
(50)
(75)
f ixRng (30)
(50)
(75)
LenEmb (30)
(50)
(75)
LenInit (30)
(50)
(75)

ﬁve-time world champion michelle kwan withdrew from the #### us ﬁgure skating championships
on wednesday , but will petition us skating ofﬁcials for the chance to compete at the #### turin
olympics .
injury leaves kwan ’s olympic hopes in limbo
kwan withdraws from us gp
kwan withdraws from us skating championships
kwan pulls out of us ﬁgure skating championships for turin olympics
kwan withdraws from us gp
kwan withdraws from ﬁgure skating championships
kwan pulls out of us ﬁgure skating championships for turin olympics bid
kwan withdraws from us skating
kwan withdraws from us ﬁgure skating championships
world champion kwan withdraws from #### olympic ﬁgure skating championships
kwan quits us ﬁgure skating
kwan withdraws from #### us ﬁgure skating worlds
kwan withdraws from #### us ﬁgure skating championships for #### olympics

Table 2: Examples of the output of each method with various speciﬁed lengths.

tion methods, we use the same reference summaries.
Note that, f ixLen and f ixRng generate the sum-
maries with a hard constraint due to their decod-
ing process, which allows them to follow the hard
constraint on length. Hence, when we calculate the
scores of LenEmb and LenInit, we impose a hard
constraint on length to make the comparison fair
(i.e. LenEmb(0,L) and LenInit(0,L) in the table).
Speciﬁcally, we use the same beam search as that
for f ixRng with minimum length of 0.

For the purpose of showing the length control
capability of LenEmb and LenInit, we show at
the bottom two lines the results of the standard
beam search without the hard constraints on the
length6. We will use the results of LenEmb(0,∞)
and LenInit(0,∞) in the discussions in Sections 6.2
and 6.3.

The results show that the learning-based meth-

6 f ixRng is equivalence to the standard beam search when

we set the range as (0, ∞).

ods (LenEmb and LenInit) tend to outperform
decoding-based methods (f ixLen and f ixRng) for
the longer summaries of 50 and 75 bytes. How-
ever, in the 30-byte setting, there is no signiﬁcant
difference between these two types of methods. We
hypothesize that this is because average compres-
sion rate in the training data is 30% (Figure 1-(c))
while the 30-byte setting forces the model to gen-
erate summaries with 15.38% in average compres-
sion rate, and thus the learning-based models did not
have enough training data to learn compression at
such a steep rate.

6.2 Examples of Generated Summaries

Tables 2 and 3 show examples from the validation
set of the Annotated Gigaword Corpus. The ta-
bles show that all models, including both learning-
based methods and decoding-based methods, can of-
ten generate well-formed sentences.

We can see various paraphrases of “#### us ﬁgure

source

reference
f ixLen (30)
(50)
(75)
f ixRng (30)
(50)
(75)
LenEmb (30)
(50)
(75)
LenInit (30)
(50)
(75)

at least two people have tested positive for the bird ﬂu virus in eastern turkey , health minister
recep akdag told a news conference wednesday .
two test positive for bird ﬂu virus in turkey
two infected with bird ﬂu
two infected with bird ﬂu in eastern turkey
two people tested positive for bird ﬂu in eastern turkey says minister
two infected with bird ﬂu
two more infected with bird ﬂu in eastern turkey
two people tested positive for bird ﬂu in eastern turkey says minister
two bird ﬂu cases in turkey
two conﬁrmed positive for bird ﬂu in eastern turkey
at least two bird ﬂu patients test positive for bird ﬂu in eastern turkey
two cases of bird ﬂu in turkey
two people tested positive for bird ﬂu in turkey
two people tested positive for bird ﬂu in eastern turkey health conference

Table 3: More examples of the output of each method.

championships”7 and “withdrew”. Some examples
are generated as a single noun phrase (LenEmb(30)
and LenInit(30)) which may be suitable for the
short length setting.

6.3 Length Control Capability of
Learning-based Models

Figure 6 shows histograms of output length from the
standard encoder-decoder, LenEmb, and LenInit.
While the output lengths from the standard model
disperse widely, the lengths from our learning-based
models are concentrated to the desired length. These
histograms clearly show the length controlling capa-
bility of our learning-based models.

Table 4-(a) shows the ﬁnal state of the beam when
LenInit generates the sentence with a length of 30
bytes for the example with standard beam search in
Table 3. We can see all the sentences in the beam
are generated with length close to the desired length.
This shows that our method has obtained the ability
to control the output length as expected. For com-
parison, Table 4-(b) shows the ﬁnal state of the beam
if we perform standard beam search in the stan-
dard encoder-decoder model (used in f ixLen and
f ixRng). Although each sentence is well-formed,
the lengths of them are much more varied.

6.4 Comparison with Existing Methods

task-1. Although the objective of this paper is not to
obtain state-of-the-art scores on this evaluation set, it
is of interest whether our length-controllable models
are competitive on this task. Table 5 shows that the
scores of our methods, which are copied from Table
1, in addition to the scores of some existing methods.
ABS (Rush et al., 2015) is the most standard model
of neural sentence summarization and is the most
similar method to our baseline setting (f ixLen).
This table shows that the score of f ixLen is com-
parable to those of the existing methods. The table
also shows the LenEmb and the LenInit have the
capability of controlling the length without decreas-
ing the ROUGE score.

7 Conclusion

In this paper, we presented the ﬁrst examination of
the problem of controlling length in neural encoder-
decoder models, from the point of view of sum-
marization. We examined methods for controlling
length of output sequences:
two decoding-based
methods (f ixLen and f ixRng) and two learning-
based methods (LenEmb and LenInit). The re-
sults showed that learning-based methods generally
outperform the decoding-based methods, and the
learning-based methods obtained the capability of
controlling the output length without losing ROUGE
score compared to existing summarization methods.

Finally, we compare our methods to existing meth-
ods on standard settings of the DUC2004 shared

Acknowledgments

7Note that “#” is a normalized number and “us” is “US”

(United States).

This work was supported by JSPS KAKENHI Grant
Number JP26280080. We are grateful to have the

logp(y|x)
-4.27
-4.41
-4.65
-5.25
-5.27
-5.51
-5.55
-5.72
-6.04

byte
31
28
30
30
31
29
32
30
30

candidate summary
two cases of bird ﬂu in turkey
two bird ﬂu cases in turkey
two people tested for bird ﬂu
two people tested in e. turkey
two bird ﬂu cases in e. turkey
two bird ﬂu cases in eastern
two people tested in east turkey
two bird ﬂu cases in turkey :
two people fail bird ﬂu virus

logp(y|x)
-5.05
-5.13
-5.30
-5.49
-5.52
-5.55
-6.00
-6.04
-6.50

byte
57
50
39
51
32
44
49
54
49

candidate summary
two people tested positive for bird ﬂu in eastern turkey
two tested positive for bird ﬂu in eastern turkey
two people tested positive for bird ﬂu
two people infected with bird ﬂu in eastern turkey
two tested positive for bird ﬂu
two infected with bird ﬂu in eastern turkey
two more infected with bird ﬂu in eastern turkey
two more conﬁrmed cases of bird ﬂu in eastern turkey
two people tested positive for bird ﬂu in turkey

(a) the beam of LenInit

(b) the beam of the standard encoder-decoder

Table 4: Final state of the beam when the learning-based model is instructed to output a 30 byte summary for the

source document in Table 3.

(a) encoder-decoder

(b) LenEmb

(c) LenInit

Figure 6: Histograms of output lengths generated by (a) the standard encoder-decoder , (b) LenEmb, and (c)
LenInit. For LenEmb and LenInit, the bracketed numbers in each region are the desired lengths we
set.

model
f ixLen
f ixRng
LenEmb
LenInit
ABS(Rush et al., 2015)
ABS+(Rush et al., 2015)
RAS-Elman(Chopra et al., 2016)
RAS-LSTM(Chopra et al., 2016)
Table 5: Comparison with
DUC2004.
Note that
reproduced from Table 1.

R-1
25.88
26.02
26.73
25.87
26.55
28.18
28.97
27.41

R-2
7.93
7.69
8.40
8.28
7.06
8.49
8.26
7.69

R-L
23.07
22.78
23.88
23.25
22.05
23.81
24.06
23.06

existing

studies

for
rows are

top four

opportunity to use the Kurisu server of Dwango Co.,
Ltd. for our experiments.

References

[Ayana et al.2016] Ayana, S. Shen, Z. Liu, and M. Sun.
2016. Neural Headline Generation with Minimum
Risk Training. CoRR, abs/1604.01904.

Cho, and Yoshua Bengio.
2015. Neural machine
translation by jointly learning to align and translate.
In Proceedings of ICLR15.

[Banko et al.2000] Michele Banko, Vibhu O. Mittal, and
2000. Headline generation
In Proceedings of

Michael J. Witbrock.
based on statistical translation.
ACL00, pages 318–325.

[Chinchor1992] Nancy Chinchor.

1992. The statisti-
cal signiﬁcance of the muc-4 results. In Proceedings
MUC4 ’92, pages 30–50.

[Cho et al.2014] Kyunghyun Cho, Bart van Merrienboer,
Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder for
statistical machine translation. In Proceedings of the
EMNLP14, pages 1724–1734.

[Chopra et al.2016] Sumit Chopra, Michael Auli, and
Alexander M. Rush. 2016. Abstractive sentence sum-
marization with attentive recurrent neural networks. In
Proceedings of NAACL-HLT16, pages 93–98.

[Cohn and Lapata2008] Trevor Cohn and Mirella Lapata.
2008. Sentence compression beyond word deletion.
In Proceedings of COLING08, pages 137–144.

[Bahdanau et al.2015] Dzmitry Bahdanau, Kyunghyun

[Cohn and Lapata2013] Trevor Cohn and Mirella Lapata.

2013. An abstractive approach to sentence compres-
sion. ACM TIST13, 4(3):41:1–41:35, July.

models.
110–119.

In Proceedings of NAACL-HLT16, pages

[Dorr et al.2003] Bonnie Dorr, David Zajic, and Richard
Schwartz. 2003. Hedge trimmer: A parse-and-trim
approach to headline generation. In Proceedings of the
HLT-NAACL 03 Text Summarization Workshop, pages
1–8.

[Filippova and Altun2013] Katja Filippova and Yasemin
Altun. 2013. Overcoming the lack of parallel data in
sentence compression. In Proceedings of EMNLP13,
pages 1481–1491.

[Filippova and Strube2008] Katja Filippova and Michael
Strube. 2008. Dependency tree based sentence com-
pression. In Proceedings of INLG08, pages 25–32.
[Filippova et al.2015] Katja Filippova, Enrique Alfon-
seca, Carlos A. Colmenares, Lukasz Kaiser, and Oriol
Vinyals. 2015. Sentence compression by deletion
with lstms. In Proceedings of EMNLP15, pages 360–
368.

[Galanis and Androutsopoulos2010] Dimitrios Galanis
and Ion Androutsopoulos. 2010. An extractive super-
vised two-stage method for sentence compression. In
Proceedings of NAACL-HLT10, pages 885–893.
[Graves et al.2013] A. Graves, N. Jaitly, and A. r. Mo-
hamed. 2013. Hybrid speech recognition with deep
bidirectional lstm. In Proceedings of IEEE Workshop
on ASRU13, pages 273–278.

[Gu et al.2016] Jiatao Gu, Zhengdong Lu, Hang Li, and
Victor O.K. Li. 2016. Incorporating copying mech-
anism in sequence-to-sequence learning. In Proceed-
ings of ACL16, pages 1631–1640.

[Gulcehre et al.2016] Caglar Gulcehre, Sungjin Ahn,
Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio.
2016. Pointing the unknown words. In Proceedings of
ACL16, pages 140–149.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural Computation, 9(8):1735–1780.

[J´ozefowicz et al.2015] Rafal

J´ozefowicz, Wojciech
Zaremba, and Ilya Sutskever. 2015. An empirical
exploration of recurrent network architectures.
In
Proceedings of ICML15, pages 2342–2350.

[Kalchbrenner and Blunsom2013] Nal Kalchbrenner and
Phil Blunsom. 2013. Recurrent continuous translation
In Proceedings of EMNLP13, pages 1700–
models.
1709, Seattle, Washington, USA, October. Association
for Computational Linguistics.

[Kingma and Ba2015] Diederik P. Kingma and Jimmy
Ba. 2015. Adam: A method for stochastic optimiza-
tion. In Proceedings of ICLR15.

[Li et al.2016a] Jiwei Li, Michel Galley, Chris Brockett,
Jianfeng Gao, and Bill Dolan. 2016a. A diversity-
promoting objective function for neural conversation

[Li et al.2016b] Jiwei Li, Michel Galley, Chris Brockett,
Georgios Spithourakis, Jianfeng Gao, and Bill Dolan.
2016b. A persona-based neural conversation model.
In Proceedings of ACL16, pages 994–1003.

[Lin2004] Chin-Yew Lin. 2004. Rouge: A package for
automatic evaluation of summaries. In Proceedings of
the ACL04 Workshop, pages 74–81.

[Lopyrev2015] Konstantin Lopyrev. 2015. Generating
news headlines with recurrent neural networks. CoRR,
abs/1512.01712.

[Luong et al.2015] Thang Luong, Hieu Pham,

and
Christopher D. Manning. 2015. Effective approaches
to attention-based neural machine translation.
In
Proceedings of EMNLP15, pages 1412–1421.

[Nallapati et al.2016] Ramesh Nallapati, Bing Xiang, and
Bowen Zhou. 2016. Sequence-to-sequence rnns for
text summarization. CoRR, abs/1602.06023.

[Napoles et al.2011] Courtney Napoles, Chris Callison-
Burch, Juri Ganitkevitch, and Benjamin Van Durme.
2011.
Paraphrastic sentence compression with a
character-based metric: Tightening without deletion.
In Proceedings of the Workshop on Monolingual Text-
To-Text Generation, pages 84–90.

[Napoles et al.2012] Courtney Napoles, Matthew Gorm-
ley, and Benjamin Van Durme. 2012. Annotated gi-
gaword. In Proceedings of the Joint Workshop on Au-
tomatic Knowledge Base Construction and Web-scale
Knowledge Extraction, pages 95–100.

[Nenkova and McKeown2011] Ani Nenkova and Kath-
leen McKeown. 2011. Automatic summarization. In
Foundations and Trends R
in Information Retrieval,
(cid:13)
volume 2-3, pages 103–233.
Sumit
[Ranzato et al.2015] Marc’Aurelio
Chopra, Michael Auli,
and Wojciech Zaremba.
2015. Sequence level training with recurrent neural
networks. CoRR, abs/1511.06732.

Ranzato,

[Rosenfeld et al.2001] Ronald Rosenfeld, Stanley F.
Chen, and Xiaojin Zhu.
2001. Whole-sentence
exponential language models: a vehicle for linguistic-
statistical integration. Computer Speech & Language,
15(1):55–73.

[Rush et al.2015] Alexander M. Rush, Sumit Chopra, and
Jason Weston. 2015. A neural attention model for
In Proceedings
abstractive sentence summarization.
of EMNLP15, pages 379–389.

[Schuster and Paliwal1997] M. Schuster and K.K. Pali-
Bidirectional recurrent neural net-
IEEE Transactions on Signal Processing,

1997.

wal.
works.
45(11):2673–2681.

[Sennrich et al.2016] Rico Sennrich, Barry Haddow, and
Alexandra Birch. 2016. Controlling politeness in neu-

ral machine translation via side constraints.
ceedings of NAACL-HLT16, pages 35–40.

In Pro-

[Serban et al.2016] Iulian Vlad Serban, Alessandro Sor-
doni, Yoshua Bengio, Aaron C. Courville, and Joelle
Pineau. 2016. Building end-to-end dialogue systems
using generative hierarchical neural network models.
In Proceedings of AAAI16, pages 3776–3784.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and
Quoc V Le. 2014. Sequence to sequence learning with
In Proceedings of NIPS14, pages
neural networks.
3104–3112.

[Tokui et al.2015] Seiya Tokui, Kenta Oono, Shohei
Hido, and Justin Clayton. 2015. Chainer: a next-
generation open source framework for deep learning.
In Proceedings of NIPS15 Workshop on LearningSys.
[Vinyals et al.2015a] Oriol Vinyals, Lukasz Kaiser, Terry
Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hin-
ton. 2015a. Grammar as a foreign language. In Pro-
ceedings of NIPS15, pages 2773–2781.

[Vinyals et al.2015b] Oriol Vinyals, Alexander Toshev,
Samy Bengio, and Dumitru Erhan. 2015b. Show and
In Proceed-
tell: A neural image caption generator.
ings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 3156–3164.

[Wen et al.2015] Tsung-Hsien Wen, Milica Gasic, Nikola
Mrkˇsi´c, Pei-Hao Su, David Vandyke, and Steve
Young. 2015. Semantically conditioned lstm-based
natural language generation for spoken dialogue sys-
tems. In Proceedings of EMNLP15, pages 1711–1721,
Lisbon, Portugal, September. Association for Compu-
tational Linguistics.

[Woodsend et al.2010] Kristian Woodsend, Yansong
Feng, and Mirella Lapata. 2010. Title generation with
In Proceedings of the
quasi-synchronous grammar.
EMNLP10, pages 513–523.

[Wubben et al.2012] Sander Wubben, Antal van den
Bosch, and Emiel Krahmer. 2012. Sentence simpli-
ﬁcation by monolingual machine translation. In Pro-
ceedings of ACL12, pages 1015–1024.

[Xu et al.2015] Kelvin Xu,

Jimmy Ba, Ryan Kiros,
Kyunghyun Cho, Aaron Courville, Ruslan Salakhudi-
nov, Rich Zemel, and Yoshua Bengio. 2015. Show,
attend and tell: Neural image caption generation with
visual attention.
In David Blei and Francis Bach,
editors, Proceedings of ICML15, pages 2048–2057.
JMLR Workshop and Conference Proceedings.
[Zajic et al.2004] David Zajic, Bonnie J Dorr,

and
R. Schwartz. 2004. Bbn/umd at duc-2004: Topiary.
In Proceedings of NAACL-HLT04 Document Under-
standing Workshop, pages 112 – 119.

6
1
0
2
 
p
e
S
 
0
3
 
 
]
L
C
.
s
c
[
 
 
1
v
2
5
5
9
0
.
9
0
6
1
:
v
i
X
r
a

Controlling Output Length in Neural Encoder-Decoders

Yuta Kikuchi1
kikuchi@lr.pi.titech.ac.jp

Graham Neubig2∗
gneubig@cs.cmu.edu

Ryohei Sasano1
sasano@pi.titech.ac.jp

Hiroya Takamura1
takamura@pi.titech.ac.jp

Manabu Okumura1
oku@pi.titecjh.ac.jp

1Tokyo Institute of Technology, Japan
2Carnegie Mellon University, USA

Abstract

Neural encoder-decoder models have shown
great success in many sequence generation
tasks. However, previous work has not in-
vestigated situations in which we would like
to control the length of encoder-decoder out-
puts. This capability is crucial for applica-
tions such as text summarization, in which
we have to generate concise summaries with
In this paper, we pro-
a desired length.
pose methods for controlling the output se-
quence length for neural encoder-decoder
models: two decoding-based methods and two
learning-based methods.1 Results show that
our learning-based methods have the capabil-
ity to control length without degrading sum-
mary quality in a summarization task.

1

Introduction

Since its ﬁrst use for machine translation (Kalch-
brenner and Blunsom, 2013; Cho et al., 2014;
Sutskever et al., 2014),
the encoder-decoder ap-
proach has demonstrated great success in many
other sequence generation tasks including image
caption generation (Vinyals et al., 2015b; Xu et
al., 2015), parsing (Vinyals et al., 2015a), dialogue
response generation (Li et al., 2016a; Serban et
al., 2016) and sentence summarization (Rush et al.,
2015; Chopra et al., 2016). In particular, in this pa-
per we focus on sentence summarization, which as
its name suggests, consists of generating shorter ver-
sions of sentences for applications such as document

∗ This work was done when the author was at the Nara In-

stitute of Science and Technology.

1Available at https://github.com/kiyukuta/lencon.

summarization (Nenkova and McKeown, 2011) or
headline generation (Dorr et al., 2003). Recently,
Rush et al. (2015) automatically constructed large
training data for sentence summarization, and this
has led to the rapid development of neural sentence
summarization (NSS) or neural headline generation
(NHG) models. There are already many studies that
address this task (Nallapati et al., 2016; Ayana et al.,
2016; Ranzato et al., 2015; Lopyrev, 2015; Gulcehre
et al., 2016; Gu et al., 2016; Chopra et al., 2016).

One of the essential properties that text summa-
rization systems should have is the ability to gen-
erate a summary with the desired length. Desired
lengths of summaries strongly depends on the scene
of use, such as the granularity of information the
user wants to understand, or the monitor size of the
device the user has. The length also depends on the
amount of information contained in the given source
document. Hence, in the traditional setting of text
summarization, both the source document and the
desired length of the summary will be given as input
to a summarization system. However, methods for
controlling the output sequence length of encoder-
decoder models have not been investigated yet, de-
spite their importance in these settings.

In this paper, we propose and investigate four
methods for controlling the output sequence length
for neural encoder-decoder models. The former two
methods are decoding-based; they receive the de-
sired length during the decoding process, and the
training process is the same as standard encoder-
decoder models.
The latter two methods are
learning-based; we modify the network architecture
to receive the desired length as input.

In experiments, we show that the learning-based
methods outperform the decoding-based methods
for long (such as 50 or 75 byte) summaries. We
also ﬁnd that despite this additional length-control
capability, the proposed methods remain competi-
tive to existing methods on standard settings of the
DUC2004 shared task-1.

2 Background

2.1 Related Work

Text summarization is one of the oldest ﬁelds of
study in natural language processing, and many
summarization methods have focused speciﬁcally
on sentence compression or headline generation.
Traditional approaches to this task focus on word
deletion using rule-based (Dorr et al., 2003; Zajic
et al., 2004) or statistical (Woodsend et al., 2010;
Galanis and Androutsopoulos, 2010; Filippova and
Strube, 2008; Filippova and Altun, 2013; Filip-
pova et al., 2015) methods. There are also several
studies of abstractive sentence summarization us-
ing syntactic transduction (Cohn and Lapata, 2008;
Napoles et al., 2011) or taking a phrase-based sta-
tistical machine translation approach (Banko et al.,
2000; Wubben et al., 2012; Cohn and Lapata, 2013).
Recent work has adopted techniques such as
encoder-decoder (Kalchbrenner and Blunsom, 2013;
Sutskever et al., 2014; Cho et al., 2014) and atten-
tional (Bahdanau et al., 2015; Luong et al., 2015)
neural network models from the ﬁeld of machine
translation, and tailored them to the sentence sum-
marization task. Rush et al. (2015) were the ﬁrst
to pose sentence summarization as a new target task
for neural sequence-to-sequence learning. Several
studies have used this task as one of the bench-
marks of their neural sequence transduction meth-
ods (Ranzato et al., 2015; Lopyrev, 2015; Ayana
et al., 2016). Some studies address the other im-
portant phenomena frequently occurred in human-
written summaries, such as copying from the source
document (Gu et al., 2016; Gulcehre et al., 2016).
Nallapati et al.
(2016) investigate a way to solve
many important problems capturing keywords, or
inputting multiple sentences.

Neural encoder-decoders can also be viewed as
statistical language models conditioned on the tar-
get sentence context. Rosenfeld et al. (2001) have

proposed whole-sentence language models that can
consider features such as sentence length. However,
as described in the introduction, to our knowledge,
explicitly controlling length of output sequences in
neural language models or encoder-decoders has not
been investigated.

Finally, there are some studies to modify the out-
put sequence according some meta information such
as the dialogue act (Wen et al., 2015), user person-
ality (Li et al., 2016b), or politeness (Sennrich et al.,
2016). However, these studies have not focused on
length, the topic of this paper.

2.2

Importance of Controlling Output Length

As we already mentioned in Section 1, the most
standard setting in text summarization is to input
both the source document and the desired length of
the summary to a summarization system. Summa-
rization systems thus must be able to generate sum-
maries of various lengths. Obviously, this property
is also essential for summarization methods based
on neural encoder-decoder models.

Since an encoder-decoder model is a completely
data-driven approach, the output sequence length
depends on the training data that the model is trained
on. For example, we use sentence-summary pairs
extracted from the Annotated English Gigaword cor-
pus as training data (Rush et al., 2015), and the
average length of human-written summary is 51.38
bytes. Figure 1 shows the statistics of the corpus.
When we train a standard encoder-decoder model
and perform the standard beam search decoding on
the corpus, the average length of its output sequence
is 38.02 byte.

However,

there are other situations where we
want summaries with other lengths. For exam-
ple, DUC2004 is a shared task where the maximum
length of summaries is set to 75 bytes, and summa-
rization systems would beneﬁt from generating sen-
tences up to this length limit.

While recent NSS models themselves cannot con-
trol their output length, Rush et al. (2015) and others
following use an ad-hoc method, in which the sys-
tem is inhibited from generating the end-of-sentence
to the tag and
(EOS) tag by assigning a score of

−∞

(a) ﬁrst sentence (181.87)

(b) article headline (51.38)
Figure 1: Histograms of ﬁrst sentence length, headline length, and their ratio in Annotated Gigaword English Giga-

(c) ratio (0.30)

word corpus. Bracketed values in each subcaption are averages.

a given source sentence, the summarizer generates
a shortened version of the input (i.e. N > M ),
as summary sentence y = (y1, y2, y3, ..., yM ). The
x) us-
model estimates conditional probability p(y
|
ing parameters trained on large training data consist-
ing of sentence-summary pairs. Typically, this con-
ditional probability is factorized as the product of
conditional probabilities of the next word in the se-
quence:

p(y

x) =
|

p(yt

y<t, x),

|

M
(cid:89)

t=1

where y<t = (y1, y2, y3, ..., yt−1). In the following,
we describe how to compute p(yt

y<t, x).
|

3.1 Encoder

We use the bi-directional RNN (BiRNN) as en-
coder which has been shown effective in neural ma-
chine translation (Bahdanau et al., 2015) and speech
recognition (Schuster and Paliwal, 1997; Graves et
al., 2013).

A BiRNN processes the source sentence for
both forward and backward directions with two
separate RNNs. During the encoding process,
the BiRNN computes both forward hidden states
(−→h 1, −→h 2, ..., −→h N ) and backward hidden states
(←−h 1, ←−h 2, ..., ←−h N ) as follows:

−→h t = g(−→h t−1, xt),

←−h t = g(←−h t+1, xt).

While g can be any kind of recurrent unit, we use
long short-term memory (LSTM) (Hochreiter and
Schmidhuber, 1997) networks that have memory
cells for both directions (−→c t and ←−c t).

Figure 2: The encoder-decoder architecture we used as a

base model in this paper.

generating a ﬁxed number of words2, and ﬁnally the
output summaries are truncated to 75 bytes. Ideally,
the models should be able to change the output se-
quence depending on the given output length, and to
output the EOS tag at the appropriate time point in a
natural manner.

3 Network Architecture: Encoder-Decoder

with Attention

In this section, we describe the model architec-
ture used for our experiments: an encoder-decoder
consisting of bi-directional RNNs and an attention
mechanism. Figure 2 shows the architecture of the
model.

Suppose that the source sentence is represented as
a sequence of words x = (x1, x2, x3, ..., xN ). For

to

the

2According

code
num-
(https://github.com/facebook/NAMAS),
ber of words is set to 15, which is too long for the DUC2004
setting. The average number of words of human summaries in
the evaluation set is 10.43.

published
the

default

After encoding, we set the initial hidden states s0

4 Controlling Length in Encoder-decoders

and memory-cell m0 of the decoder as follows:

s0 = ←−h 1,
m0 = ←−c 1.

3.2 Decoder and Attender

Our decoder is based on an RNN with LSTM g:

st = g(st−1, xt).

We also use the attention mechanism developed
by Luong et al. (2015), which uses st to compute
contextual information dt of time step t. We ﬁrst
summarize the forward and backward encoder states
by taking their sum ¯hi = −→h i + ←−h i, and then calcu-
late the context vector dt as the weighted sum of
these summarized vectors:

dt =

(cid:88)

ati¯hi,

i

where at is the weight at the t-th step for ¯hi com-
puted by a softmax operation:

ati =

exp(st
·
¯h(cid:48) exp(st

(cid:80)

¯hi)

.

¯h(cid:48))

·

After context vector dt is calculated, the model
updates the distribution over the next word as fol-
lows:

˜st = tanh(Whs[st; dt] + bhs),

p(yt

y<t, x) = softmax(Wso ˜st + bso).
|

Note that ˜st is also provided as input to the LSTM
with yt for the next step, which is called the input
feeding architecture (Luong et al., 2015).

3.3 Training and Decoding

The training objective of our models is to maximize
log likelihood of the sentence-summary pairs in a
given training set D:

Lt(θ) =

(cid:88)

log p(y

x; θ),

|

(x,y)∈D
(cid:89)

p(y

x; θ) =

|

p(yt

y<t, x).
|

t

Once models are trained, we use beam search to ﬁnd
the output that maximizes the conditional probabil-
ity.

In this section, we propose our four methods that
can control the length of the output in the encoder-
In the ﬁrst two methods, the
decoder framework.
decoding process is used to control the output length
without changing the model itself. In the other two
methods, the model itself has been changed and is
trained to obtain the capability of controlling the
length. Following the evaluation dataset used in our
experiments, we use bytes as the unit of length, al-
though our models can use either words or bytes as
necessary.

4.1 f ixLen: Beam Search without EOS Tags

The ﬁrst method we examine is a decoding approach
similar to the one taken in many recent NSS meth-
ods that is slightly less ad-hoc. In this method, we
inhibit the decoder from generating the EOS tag by
assigning it a score of
. Since the model can-
−∞
not stop the decoding process by itself, we simply
stop the decoding process when the length of output
sequence reaches the desired length. More speciﬁ-
cally, during beam search, when the length of the se-
quence generated so far exceeds the desired length,
the last word is replaced with the EOS tag and also
the score of the last word is replaced with the score
of the EOS tag (EOS replacement).

4.2 f ixRng: Discarding Out-of-range

Sequences

Our second decoding method is based on discarding
out-of-range sequences, and is not inhibited from
generating the EOS tag, allowing it to decide when
to stop generation. Instead, we deﬁne the legitimate
range of the sequence by setting minimum and max-
imum lengths. Speciﬁcally, in addition to the normal
beam search procedure, we set two rules:

•

•

If the model generates the EOS tag when the
output sequence is shorter than the minimum
length, we discard the sequence from the beam.

If the generated sequence exceeds the maxi-
mum length, we also discard the sequence from
the beam. We then replace its last word with
the EOS tag and add this sequence to the beam

(EOS replacement in Section 4.1).3

In other words, we keep only the sequences that
contain the EOS tag and are in the deﬁned length
range. This method is a compromise that allows
the model some ﬂexibility to plan the generated se-
quences, but only within a certain acceptable length
range.

It should be noted that this method needs a larger
beam size if the desired length is very different from
the average summary length in the training data, as
it will need to preserve hypotheses that have the de-
sired length.

4.3 LenEmb: Length Embedding as
Additional Input for the LSTM

Our third method is a learning-based method specif-
ically trained to control the length of the output se-
quence. Inspired by previous work that has demon-
strated that additional inputs to decoder models can
effectively control the characteristics of the output
(Wen et al., 2015; Li et al., 2016b), this model pro-
vides information about the length in the form of an
additional input to the net. Speciﬁcally, the model
RD for each potential
uses an embedding e2(lt)
desired length, which is parameterized by a length
RD×L where L is the
embedding matrix Wle
∈
number of length types. In the decoding process, we
input the embedding of the remaining length lt as
additional input to the LSTM (Figure 3). lt is initial-
ized after the encoding process and updated during
the decoding process as follows:

∈

l1 = length,

lt+1 =

(cid:26) 0
lt

byte(yt)

−

byte(yt)

(lt
(otherwise),

−

≤

0)

where byte(yt) is the length of output word yt and
length is the desired length. We learn the values
of the length embedding matrix Wle during train-
ing. This method provides additional information
about the amount of length remaining in the output
sequence, allowing the decoder to “plan” its output
based on the remaining number of words it can gen-
erate.

3This is a workaround to prevent the situation in which all

sequences are discarded from a beam.

Figure 3: LenEmb: remaining length is used as addi-

tional input for the LSTM of the decoder.

Figure 4: LenInit: initial state of the decoder’s memory

cell m0 manages output length.

4.4 LenInit: Length-based Memory Cell

Initialization

While LenEmb inputs the remaining length lt to the
decoder at each step of the decoding process, the
LenInit method inputs the desired length once at
the initial state of the decoder. Figure 4 shows the ar-
chitecture of LenInit. Speciﬁcally, the model uses
the memory cell mt to control the output length by
initializing the states of decoder (hidden state s0 and
memory cell m0) as follows:

s0 = ←−h 1,
m0 = bc ∗

length,

(1)

where bc
∈
is the desired length.

RH is a trainable parameter and length

While the model of LenEmb is guided towards
the appropriate output length by inputting the re-
maining length at each step, this LenInit attempts
to provide the model with the ability to manage the
output length on its own using its inner state. Specif-
ically, the memory cell of LSTM networks is suit-
able for this endeavour, as it is possible for LSTMs

(a) ﬁrst sentence (206.91)

(c) ratio (0.35)
(b) summary (70.00)
Figure 5: Histograms of ﬁrst sentence length, summary length, and their ratio in DUC2004.

to learn functions that, for example, subtract a ﬁxed
amount from a particular memory cell every time
they output a word. Although other ways for man-
aging the length are also possible,4 we found this
approach to be both simple and effective.

5 Experiment

5.1 Dataset

We trained our models on a part of the Annotated
English Gigaword corpus (Napoles et al., 2012),
which Rush et al. (2015) constructed for sentence
summarization. We perform preprocessing using the
standard script for the dataset5. The dataset con-
sists of approximately 3.6 million pairs of the ﬁrst
sentence from each source document and its head-
line. Figure 1 shows the length histograms of the
summaries in the training set. The vocabulary size
is 116,875 for the source documents and 67,564
for the target summaries including the beginning-of-
sentence, end-of-sentence, and unknown word tags.
For LenEmb and LenInit, we input the length of
each headline during training. Note that we do not
train multiple summarization models for each head-
line length, but a single model that is capable of con-
trolling the length of its output.

We evaluate the methods on the evaluation set
of DUC2004 task-1 (generating very short single-
document summaries). In this task, summarization
systems are required to create a very short sum-
mary for each given document. Summaries over
the length limit (75 bytes) will be truncated and
there is no bonus for creating a shorter summary.
The evaluation set consists of 500 source documents
and 4 human-written (reference) summaries for each

4For example, we can also add another memory cell for

managing the length.

5https://github.com/facebook/NAMAS

source document. Figure 5 shows the length his-
tograms of the summaries in the evaluation set. Note
that the human-written summaries are not always as
long as 75 bytes. We used three variants of ROUGE
(Lin, 2004) as evaluation metrics: ROUGE-1 (uni-
gram), ROUGE-2 (bigram), and ROUGE-L (longest
common subsequence). The two-sided permutation
test (Chinchor, 1992) was used for statistical signif-
icance testing (p

0.05).

≤

5.2

Implementation

We use Adam (Kingma and Ba, 2015) (α=0.001,
β1=0.9, β2=0.999, eps=10−8) to optimize param-
eters with a mini-batch of size 80. Before every
10,000 updates, we ﬁrst sampled 800,000 training
examples and made groups of 80 examples with
the same source sentence length, and shufﬂed the
10,000 groups.

We set the dimension of word embeddings to 100
and that of the hidden state to 200. For LSTMs,
we initialize the bias of the forget gate to 1.0 and
use 0.0 for the other gate biases (J´ozefowicz et al.,
2015). We use Chainer (Tokui et al., 2015) to im-
plement our models. For LenEmb, we set L to 300,
which is larger than the longest summary lengths in
our dataset (see Figure 1-(b) and Figure 5-(b)).

For all methods except f ixRng, we found a beam
size of 10 to be sufﬁcient, but for f ixRng we used
a beam size of 30 because it more aggressively dis-
cards candidate sequences from its beams during de-
coding.

6 Result

6.1 ROUGE Evaluation

Table 1 shows the ROUGE scores of each method
with various length limits (30, 50 and 75 byte). Re-
gardless of the length limit set for the summariza-

model
f ixLen
f ixRng
LenEmb(0,L)
LenInit(0,L)
LenEmb(0,∞)
LenInit(0,∞)

30 byte
R-2
3.10∗
3.08∗
3.21
3.27
3.30
3.49

R-1
14.34
13.83∗
14.23
14.31
13.75
13.92

R-L
13.23
12.88
13.02
13.19
12.68
12.90

R-1
20.00∗
20.08∗
20.78
20.87
20.62
20.87

R-L
18.26∗
18.19∗
18.57
19.00
18.64
19.09

R-1
25.87∗
26.01
26.73
25.87
26.42
25.29∗

75 byte
R-2
7.93
7.69∗
8.39
8.27
8.26
8.00

R-L
23.07∗
22.77∗
23.88
23.24
23.59
22.71∗

50 byte
R-2
5.98
5.74
5.97
6.16
6.22
6.19

∗

Table 1: ROUGE scores with various length limits. The scores with

are signiﬁcantly worse than the best score in

the column (bolded).

source

reference
f ixLen (30)
(50)
(75)
f ixRng (30)
(50)
(75)
LenEmb (30)
(50)
(75)
LenInit (30)
(50)
(75)

ﬁve-time world champion michelle kwan withdrew from the #### us ﬁgure skating championships
on wednesday , but will petition us skating ofﬁcials for the chance to compete at the #### turin
olympics .
injury leaves kwan ’s olympic hopes in limbo
kwan withdraws from us gp
kwan withdraws from us skating championships
kwan pulls out of us ﬁgure skating championships for turin olympics
kwan withdraws from us gp
kwan withdraws from ﬁgure skating championships
kwan pulls out of us ﬁgure skating championships for turin olympics bid
kwan withdraws from us skating
kwan withdraws from us ﬁgure skating championships
world champion kwan withdraws from #### olympic ﬁgure skating championships
kwan quits us ﬁgure skating
kwan withdraws from #### us ﬁgure skating worlds
kwan withdraws from #### us ﬁgure skating championships for #### olympics

Table 2: Examples of the output of each method with various speciﬁed lengths.

tion methods, we use the same reference summaries.
Note that, f ixLen and f ixRng generate the sum-
maries with a hard constraint due to their decod-
ing process, which allows them to follow the hard
constraint on length. Hence, when we calculate the
scores of LenEmb and LenInit, we impose a hard
constraint on length to make the comparison fair
(i.e. LenEmb(0,L) and LenInit(0,L) in the table).
Speciﬁcally, we use the same beam search as that
for f ixRng with minimum length of 0.

For the purpose of showing the length control
capability of LenEmb and LenInit, we show at
the bottom two lines the results of the standard
beam search without the hard constraints on the
length6. We will use the results of LenEmb(0,∞)
and LenInit(0,∞) in the discussions in Sections 6.2
and 6.3.

The results show that the learning-based meth-

6 f ixRng is equivalence to the standard beam search when

we set the range as (0, ∞).

ods (LenEmb and LenInit) tend to outperform
decoding-based methods (f ixLen and f ixRng) for
the longer summaries of 50 and 75 bytes. How-
ever, in the 30-byte setting, there is no signiﬁcant
difference between these two types of methods. We
hypothesize that this is because average compres-
sion rate in the training data is 30% (Figure 1-(c))
while the 30-byte setting forces the model to gen-
erate summaries with 15.38% in average compres-
sion rate, and thus the learning-based models did not
have enough training data to learn compression at
such a steep rate.

6.2 Examples of Generated Summaries

Tables 2 and 3 show examples from the validation
set of the Annotated Gigaword Corpus. The ta-
bles show that all models, including both learning-
based methods and decoding-based methods, can of-
ten generate well-formed sentences.

We can see various paraphrases of “#### us ﬁgure

source

reference
f ixLen (30)
(50)
(75)
f ixRng (30)
(50)
(75)
LenEmb (30)
(50)
(75)
LenInit (30)
(50)
(75)

at least two people have tested positive for the bird ﬂu virus in eastern turkey , health minister
recep akdag told a news conference wednesday .
two test positive for bird ﬂu virus in turkey
two infected with bird ﬂu
two infected with bird ﬂu in eastern turkey
two people tested positive for bird ﬂu in eastern turkey says minister
two infected with bird ﬂu
two more infected with bird ﬂu in eastern turkey
two people tested positive for bird ﬂu in eastern turkey says minister
two bird ﬂu cases in turkey
two conﬁrmed positive for bird ﬂu in eastern turkey
at least two bird ﬂu patients test positive for bird ﬂu in eastern turkey
two cases of bird ﬂu in turkey
two people tested positive for bird ﬂu in turkey
two people tested positive for bird ﬂu in eastern turkey health conference

Table 3: More examples of the output of each method.

championships”7 and “withdrew”. Some examples
are generated as a single noun phrase (LenEmb(30)
and LenInit(30)) which may be suitable for the
short length setting.

6.3 Length Control Capability of
Learning-based Models

Figure 6 shows histograms of output length from the
standard encoder-decoder, LenEmb, and LenInit.
While the output lengths from the standard model
disperse widely, the lengths from our learning-based
models are concentrated to the desired length. These
histograms clearly show the length controlling capa-
bility of our learning-based models.

Table 4-(a) shows the ﬁnal state of the beam when
LenInit generates the sentence with a length of 30
bytes for the example with standard beam search in
Table 3. We can see all the sentences in the beam
are generated with length close to the desired length.
This shows that our method has obtained the ability
to control the output length as expected. For com-
parison, Table 4-(b) shows the ﬁnal state of the beam
if we perform standard beam search in the stan-
dard encoder-decoder model (used in f ixLen and
f ixRng). Although each sentence is well-formed,
the lengths of them are much more varied.

6.4 Comparison with Existing Methods

task-1. Although the objective of this paper is not to
obtain state-of-the-art scores on this evaluation set, it
is of interest whether our length-controllable models
are competitive on this task. Table 5 shows that the
scores of our methods, which are copied from Table
1, in addition to the scores of some existing methods.
ABS (Rush et al., 2015) is the most standard model
of neural sentence summarization and is the most
similar method to our baseline setting (f ixLen).
This table shows that the score of f ixLen is com-
parable to those of the existing methods. The table
also shows the LenEmb and the LenInit have the
capability of controlling the length without decreas-
ing the ROUGE score.

7 Conclusion

In this paper, we presented the ﬁrst examination of
the problem of controlling length in neural encoder-
decoder models, from the point of view of sum-
marization. We examined methods for controlling
length of output sequences:
two decoding-based
methods (f ixLen and f ixRng) and two learning-
based methods (LenEmb and LenInit). The re-
sults showed that learning-based methods generally
outperform the decoding-based methods, and the
learning-based methods obtained the capability of
controlling the output length without losing ROUGE
score compared to existing summarization methods.

Finally, we compare our methods to existing meth-
ods on standard settings of the DUC2004 shared

Acknowledgments

7Note that “#” is a normalized number and “us” is “US”

(United States).

This work was supported by JSPS KAKENHI Grant
Number JP26280080. We are grateful to have the

logp(y|x)
-4.27
-4.41
-4.65
-5.25
-5.27
-5.51
-5.55
-5.72
-6.04

byte
31
28
30
30
31
29
32
30
30

candidate summary
two cases of bird ﬂu in turkey
two bird ﬂu cases in turkey
two people tested for bird ﬂu
two people tested in e. turkey
two bird ﬂu cases in e. turkey
two bird ﬂu cases in eastern
two people tested in east turkey
two bird ﬂu cases in turkey :
two people fail bird ﬂu virus

logp(y|x)
-5.05
-5.13
-5.30
-5.49
-5.52
-5.55
-6.00
-6.04
-6.50

byte
57
50
39
51
32
44
49
54
49

candidate summary
two people tested positive for bird ﬂu in eastern turkey
two tested positive for bird ﬂu in eastern turkey
two people tested positive for bird ﬂu
two people infected with bird ﬂu in eastern turkey
two tested positive for bird ﬂu
two infected with bird ﬂu in eastern turkey
two more infected with bird ﬂu in eastern turkey
two more conﬁrmed cases of bird ﬂu in eastern turkey
two people tested positive for bird ﬂu in turkey

(a) the beam of LenInit

(b) the beam of the standard encoder-decoder

Table 4: Final state of the beam when the learning-based model is instructed to output a 30 byte summary for the

source document in Table 3.

(a) encoder-decoder

(b) LenEmb

(c) LenInit

Figure 6: Histograms of output lengths generated by (a) the standard encoder-decoder , (b) LenEmb, and (c)
LenInit. For LenEmb and LenInit, the bracketed numbers in each region are the desired lengths we
set.

model
f ixLen
f ixRng
LenEmb
LenInit
ABS(Rush et al., 2015)
ABS+(Rush et al., 2015)
RAS-Elman(Chopra et al., 2016)
RAS-LSTM(Chopra et al., 2016)
Table 5: Comparison with
DUC2004.
Note that
reproduced from Table 1.

R-1
25.88
26.02
26.73
25.87
26.55
28.18
28.97
27.41

R-2
7.93
7.69
8.40
8.28
7.06
8.49
8.26
7.69

R-L
23.07
22.78
23.88
23.25
22.05
23.81
24.06
23.06

existing

studies

for
rows are

top four

opportunity to use the Kurisu server of Dwango Co.,
Ltd. for our experiments.

References

[Ayana et al.2016] Ayana, S. Shen, Z. Liu, and M. Sun.
2016. Neural Headline Generation with Minimum
Risk Training. CoRR, abs/1604.01904.

Cho, and Yoshua Bengio.
2015. Neural machine
translation by jointly learning to align and translate.
In Proceedings of ICLR15.

[Banko et al.2000] Michele Banko, Vibhu O. Mittal, and
2000. Headline generation
In Proceedings of

Michael J. Witbrock.
based on statistical translation.
ACL00, pages 318–325.

[Chinchor1992] Nancy Chinchor.

1992. The statisti-
cal signiﬁcance of the muc-4 results. In Proceedings
MUC4 ’92, pages 30–50.

[Cho et al.2014] Kyunghyun Cho, Bart van Merrienboer,
Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder for
statistical machine translation. In Proceedings of the
EMNLP14, pages 1724–1734.

[Chopra et al.2016] Sumit Chopra, Michael Auli, and
Alexander M. Rush. 2016. Abstractive sentence sum-
marization with attentive recurrent neural networks. In
Proceedings of NAACL-HLT16, pages 93–98.

[Cohn and Lapata2008] Trevor Cohn and Mirella Lapata.
2008. Sentence compression beyond word deletion.
In Proceedings of COLING08, pages 137–144.

[Bahdanau et al.2015] Dzmitry Bahdanau, Kyunghyun

[Cohn and Lapata2013] Trevor Cohn and Mirella Lapata.

2013. An abstractive approach to sentence compres-
sion. ACM TIST13, 4(3):41:1–41:35, July.

models.
110–119.

In Proceedings of NAACL-HLT16, pages

[Dorr et al.2003] Bonnie Dorr, David Zajic, and Richard
Schwartz. 2003. Hedge trimmer: A parse-and-trim
approach to headline generation. In Proceedings of the
HLT-NAACL 03 Text Summarization Workshop, pages
1–8.

[Filippova and Altun2013] Katja Filippova and Yasemin
Altun. 2013. Overcoming the lack of parallel data in
sentence compression. In Proceedings of EMNLP13,
pages 1481–1491.

[Filippova and Strube2008] Katja Filippova and Michael
Strube. 2008. Dependency tree based sentence com-
pression. In Proceedings of INLG08, pages 25–32.
[Filippova et al.2015] Katja Filippova, Enrique Alfon-
seca, Carlos A. Colmenares, Lukasz Kaiser, and Oriol
Vinyals. 2015. Sentence compression by deletion
with lstms. In Proceedings of EMNLP15, pages 360–
368.

[Galanis and Androutsopoulos2010] Dimitrios Galanis
and Ion Androutsopoulos. 2010. An extractive super-
vised two-stage method for sentence compression. In
Proceedings of NAACL-HLT10, pages 885–893.
[Graves et al.2013] A. Graves, N. Jaitly, and A. r. Mo-
hamed. 2013. Hybrid speech recognition with deep
bidirectional lstm. In Proceedings of IEEE Workshop
on ASRU13, pages 273–278.

[Gu et al.2016] Jiatao Gu, Zhengdong Lu, Hang Li, and
Victor O.K. Li. 2016. Incorporating copying mech-
anism in sequence-to-sequence learning. In Proceed-
ings of ACL16, pages 1631–1640.

[Gulcehre et al.2016] Caglar Gulcehre, Sungjin Ahn,
Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio.
2016. Pointing the unknown words. In Proceedings of
ACL16, pages 140–149.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural Computation, 9(8):1735–1780.

[J´ozefowicz et al.2015] Rafal

J´ozefowicz, Wojciech
Zaremba, and Ilya Sutskever. 2015. An empirical
exploration of recurrent network architectures.
In
Proceedings of ICML15, pages 2342–2350.

[Kalchbrenner and Blunsom2013] Nal Kalchbrenner and
Phil Blunsom. 2013. Recurrent continuous translation
In Proceedings of EMNLP13, pages 1700–
models.
1709, Seattle, Washington, USA, October. Association
for Computational Linguistics.

[Kingma and Ba2015] Diederik P. Kingma and Jimmy
Ba. 2015. Adam: A method for stochastic optimiza-
tion. In Proceedings of ICLR15.

[Li et al.2016a] Jiwei Li, Michel Galley, Chris Brockett,
Jianfeng Gao, and Bill Dolan. 2016a. A diversity-
promoting objective function for neural conversation

[Li et al.2016b] Jiwei Li, Michel Galley, Chris Brockett,
Georgios Spithourakis, Jianfeng Gao, and Bill Dolan.
2016b. A persona-based neural conversation model.
In Proceedings of ACL16, pages 994–1003.

[Lin2004] Chin-Yew Lin. 2004. Rouge: A package for
automatic evaluation of summaries. In Proceedings of
the ACL04 Workshop, pages 74–81.

[Lopyrev2015] Konstantin Lopyrev. 2015. Generating
news headlines with recurrent neural networks. CoRR,
abs/1512.01712.

[Luong et al.2015] Thang Luong, Hieu Pham,

and
Christopher D. Manning. 2015. Effective approaches
to attention-based neural machine translation.
In
Proceedings of EMNLP15, pages 1412–1421.

[Nallapati et al.2016] Ramesh Nallapati, Bing Xiang, and
Bowen Zhou. 2016. Sequence-to-sequence rnns for
text summarization. CoRR, abs/1602.06023.

[Napoles et al.2011] Courtney Napoles, Chris Callison-
Burch, Juri Ganitkevitch, and Benjamin Van Durme.
2011.
Paraphrastic sentence compression with a
character-based metric: Tightening without deletion.
In Proceedings of the Workshop on Monolingual Text-
To-Text Generation, pages 84–90.

[Napoles et al.2012] Courtney Napoles, Matthew Gorm-
ley, and Benjamin Van Durme. 2012. Annotated gi-
gaword. In Proceedings of the Joint Workshop on Au-
tomatic Knowledge Base Construction and Web-scale
Knowledge Extraction, pages 95–100.

[Nenkova and McKeown2011] Ani Nenkova and Kath-
leen McKeown. 2011. Automatic summarization. In
Foundations and Trends R
in Information Retrieval,
(cid:13)
volume 2-3, pages 103–233.
Sumit
[Ranzato et al.2015] Marc’Aurelio
Chopra, Michael Auli,
and Wojciech Zaremba.
2015. Sequence level training with recurrent neural
networks. CoRR, abs/1511.06732.

Ranzato,

[Rosenfeld et al.2001] Ronald Rosenfeld, Stanley F.
Chen, and Xiaojin Zhu.
2001. Whole-sentence
exponential language models: a vehicle for linguistic-
statistical integration. Computer Speech & Language,
15(1):55–73.

[Rush et al.2015] Alexander M. Rush, Sumit Chopra, and
Jason Weston. 2015. A neural attention model for
In Proceedings
abstractive sentence summarization.
of EMNLP15, pages 379–389.

[Schuster and Paliwal1997] M. Schuster and K.K. Pali-
Bidirectional recurrent neural net-
IEEE Transactions on Signal Processing,

1997.

wal.
works.
45(11):2673–2681.

[Sennrich et al.2016] Rico Sennrich, Barry Haddow, and
Alexandra Birch. 2016. Controlling politeness in neu-

ral machine translation via side constraints.
ceedings of NAACL-HLT16, pages 35–40.

In Pro-

[Serban et al.2016] Iulian Vlad Serban, Alessandro Sor-
doni, Yoshua Bengio, Aaron C. Courville, and Joelle
Pineau. 2016. Building end-to-end dialogue systems
using generative hierarchical neural network models.
In Proceedings of AAAI16, pages 3776–3784.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and
Quoc V Le. 2014. Sequence to sequence learning with
In Proceedings of NIPS14, pages
neural networks.
3104–3112.

[Tokui et al.2015] Seiya Tokui, Kenta Oono, Shohei
Hido, and Justin Clayton. 2015. Chainer: a next-
generation open source framework for deep learning.
In Proceedings of NIPS15 Workshop on LearningSys.
[Vinyals et al.2015a] Oriol Vinyals, Lukasz Kaiser, Terry
Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hin-
ton. 2015a. Grammar as a foreign language. In Pro-
ceedings of NIPS15, pages 2773–2781.

[Vinyals et al.2015b] Oriol Vinyals, Alexander Toshev,
Samy Bengio, and Dumitru Erhan. 2015b. Show and
In Proceed-
tell: A neural image caption generator.
ings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 3156–3164.

[Wen et al.2015] Tsung-Hsien Wen, Milica Gasic, Nikola
Mrkˇsi´c, Pei-Hao Su, David Vandyke, and Steve
Young. 2015. Semantically conditioned lstm-based
natural language generation for spoken dialogue sys-
tems. In Proceedings of EMNLP15, pages 1711–1721,
Lisbon, Portugal, September. Association for Compu-
tational Linguistics.

[Woodsend et al.2010] Kristian Woodsend, Yansong
Feng, and Mirella Lapata. 2010. Title generation with
In Proceedings of the
quasi-synchronous grammar.
EMNLP10, pages 513–523.

[Wubben et al.2012] Sander Wubben, Antal van den
Bosch, and Emiel Krahmer. 2012. Sentence simpli-
ﬁcation by monolingual machine translation. In Pro-
ceedings of ACL12, pages 1015–1024.

[Xu et al.2015] Kelvin Xu,

Jimmy Ba, Ryan Kiros,
Kyunghyun Cho, Aaron Courville, Ruslan Salakhudi-
nov, Rich Zemel, and Yoshua Bengio. 2015. Show,
attend and tell: Neural image caption generation with
visual attention.
In David Blei and Francis Bach,
editors, Proceedings of ICML15, pages 2048–2057.
JMLR Workshop and Conference Proceedings.
[Zajic et al.2004] David Zajic, Bonnie J Dorr,

and
R. Schwartz. 2004. Bbn/umd at duc-2004: Topiary.
In Proceedings of NAACL-HLT04 Document Under-
standing Workshop, pages 112 – 119.

6
1
0
2
 
p
e
S
 
0
3
 
 
]
L
C
.
s
c
[
 
 
1
v
2
5
5
9
0
.
9
0
6
1
:
v
i
X
r
a

Controlling Output Length in Neural Encoder-Decoders

Yuta Kikuchi1
kikuchi@lr.pi.titech.ac.jp

Graham Neubig2∗
gneubig@cs.cmu.edu

Ryohei Sasano1
sasano@pi.titech.ac.jp

Hiroya Takamura1
takamura@pi.titech.ac.jp

Manabu Okumura1
oku@pi.titecjh.ac.jp

1Tokyo Institute of Technology, Japan
2Carnegie Mellon University, USA

Abstract

Neural encoder-decoder models have shown
great success in many sequence generation
tasks. However, previous work has not in-
vestigated situations in which we would like
to control the length of encoder-decoder out-
puts. This capability is crucial for applica-
tions such as text summarization, in which
we have to generate concise summaries with
In this paper, we pro-
a desired length.
pose methods for controlling the output se-
quence length for neural encoder-decoder
models: two decoding-based methods and two
learning-based methods.1 Results show that
our learning-based methods have the capabil-
ity to control length without degrading sum-
mary quality in a summarization task.

1

Introduction

Since its ﬁrst use for machine translation (Kalch-
brenner and Blunsom, 2013; Cho et al., 2014;
Sutskever et al., 2014),
the encoder-decoder ap-
proach has demonstrated great success in many
other sequence generation tasks including image
caption generation (Vinyals et al., 2015b; Xu et
al., 2015), parsing (Vinyals et al., 2015a), dialogue
response generation (Li et al., 2016a; Serban et
al., 2016) and sentence summarization (Rush et al.,
2015; Chopra et al., 2016). In particular, in this pa-
per we focus on sentence summarization, which as
its name suggests, consists of generating shorter ver-
sions of sentences for applications such as document

∗ This work was done when the author was at the Nara In-

stitute of Science and Technology.

1Available at https://github.com/kiyukuta/lencon.

summarization (Nenkova and McKeown, 2011) or
headline generation (Dorr et al., 2003). Recently,
Rush et al. (2015) automatically constructed large
training data for sentence summarization, and this
has led to the rapid development of neural sentence
summarization (NSS) or neural headline generation
(NHG) models. There are already many studies that
address this task (Nallapati et al., 2016; Ayana et al.,
2016; Ranzato et al., 2015; Lopyrev, 2015; Gulcehre
et al., 2016; Gu et al., 2016; Chopra et al., 2016).

One of the essential properties that text summa-
rization systems should have is the ability to gen-
erate a summary with the desired length. Desired
lengths of summaries strongly depends on the scene
of use, such as the granularity of information the
user wants to understand, or the monitor size of the
device the user has. The length also depends on the
amount of information contained in the given source
document. Hence, in the traditional setting of text
summarization, both the source document and the
desired length of the summary will be given as input
to a summarization system. However, methods for
controlling the output sequence length of encoder-
decoder models have not been investigated yet, de-
spite their importance in these settings.

In this paper, we propose and investigate four
methods for controlling the output sequence length
for neural encoder-decoder models. The former two
methods are decoding-based; they receive the de-
sired length during the decoding process, and the
training process is the same as standard encoder-
decoder models.
The latter two methods are
learning-based; we modify the network architecture
to receive the desired length as input.

In experiments, we show that the learning-based
methods outperform the decoding-based methods
for long (such as 50 or 75 byte) summaries. We
also ﬁnd that despite this additional length-control
capability, the proposed methods remain competi-
tive to existing methods on standard settings of the
DUC2004 shared task-1.

2 Background

2.1 Related Work

Text summarization is one of the oldest ﬁelds of
study in natural language processing, and many
summarization methods have focused speciﬁcally
on sentence compression or headline generation.
Traditional approaches to this task focus on word
deletion using rule-based (Dorr et al., 2003; Zajic
et al., 2004) or statistical (Woodsend et al., 2010;
Galanis and Androutsopoulos, 2010; Filippova and
Strube, 2008; Filippova and Altun, 2013; Filip-
pova et al., 2015) methods. There are also several
studies of abstractive sentence summarization us-
ing syntactic transduction (Cohn and Lapata, 2008;
Napoles et al., 2011) or taking a phrase-based sta-
tistical machine translation approach (Banko et al.,
2000; Wubben et al., 2012; Cohn and Lapata, 2013).
Recent work has adopted techniques such as
encoder-decoder (Kalchbrenner and Blunsom, 2013;
Sutskever et al., 2014; Cho et al., 2014) and atten-
tional (Bahdanau et al., 2015; Luong et al., 2015)
neural network models from the ﬁeld of machine
translation, and tailored them to the sentence sum-
marization task. Rush et al. (2015) were the ﬁrst
to pose sentence summarization as a new target task
for neural sequence-to-sequence learning. Several
studies have used this task as one of the bench-
marks of their neural sequence transduction meth-
ods (Ranzato et al., 2015; Lopyrev, 2015; Ayana
et al., 2016). Some studies address the other im-
portant phenomena frequently occurred in human-
written summaries, such as copying from the source
document (Gu et al., 2016; Gulcehre et al., 2016).
Nallapati et al.
(2016) investigate a way to solve
many important problems capturing keywords, or
inputting multiple sentences.

Neural encoder-decoders can also be viewed as
statistical language models conditioned on the tar-
get sentence context. Rosenfeld et al. (2001) have

proposed whole-sentence language models that can
consider features such as sentence length. However,
as described in the introduction, to our knowledge,
explicitly controlling length of output sequences in
neural language models or encoder-decoders has not
been investigated.

Finally, there are some studies to modify the out-
put sequence according some meta information such
as the dialogue act (Wen et al., 2015), user person-
ality (Li et al., 2016b), or politeness (Sennrich et al.,
2016). However, these studies have not focused on
length, the topic of this paper.

2.2

Importance of Controlling Output Length

As we already mentioned in Section 1, the most
standard setting in text summarization is to input
both the source document and the desired length of
the summary to a summarization system. Summa-
rization systems thus must be able to generate sum-
maries of various lengths. Obviously, this property
is also essential for summarization methods based
on neural encoder-decoder models.

Since an encoder-decoder model is a completely
data-driven approach, the output sequence length
depends on the training data that the model is trained
on. For example, we use sentence-summary pairs
extracted from the Annotated English Gigaword cor-
pus as training data (Rush et al., 2015), and the
average length of human-written summary is 51.38
bytes. Figure 1 shows the statistics of the corpus.
When we train a standard encoder-decoder model
and perform the standard beam search decoding on
the corpus, the average length of its output sequence
is 38.02 byte.

However,

there are other situations where we
want summaries with other lengths. For exam-
ple, DUC2004 is a shared task where the maximum
length of summaries is set to 75 bytes, and summa-
rization systems would beneﬁt from generating sen-
tences up to this length limit.

While recent NSS models themselves cannot con-
trol their output length, Rush et al. (2015) and others
following use an ad-hoc method, in which the sys-
tem is inhibited from generating the end-of-sentence
to the tag and
(EOS) tag by assigning a score of

−∞

(a) ﬁrst sentence (181.87)

(b) article headline (51.38)
Figure 1: Histograms of ﬁrst sentence length, headline length, and their ratio in Annotated Gigaword English Giga-

(c) ratio (0.30)

word corpus. Bracketed values in each subcaption are averages.

a given source sentence, the summarizer generates
a shortened version of the input (i.e. N > M ),
as summary sentence y = (y1, y2, y3, ..., yM ). The
x) us-
model estimates conditional probability p(y
|
ing parameters trained on large training data consist-
ing of sentence-summary pairs. Typically, this con-
ditional probability is factorized as the product of
conditional probabilities of the next word in the se-
quence:

p(y

x) =
|

p(yt

y<t, x),

|

M
(cid:89)

t=1

where y<t = (y1, y2, y3, ..., yt−1). In the following,
we describe how to compute p(yt

y<t, x).
|

3.1 Encoder

We use the bi-directional RNN (BiRNN) as en-
coder which has been shown effective in neural ma-
chine translation (Bahdanau et al., 2015) and speech
recognition (Schuster and Paliwal, 1997; Graves et
al., 2013).

A BiRNN processes the source sentence for
both forward and backward directions with two
separate RNNs. During the encoding process,
the BiRNN computes both forward hidden states
(−→h 1, −→h 2, ..., −→h N ) and backward hidden states
(←−h 1, ←−h 2, ..., ←−h N ) as follows:

−→h t = g(−→h t−1, xt),

←−h t = g(←−h t+1, xt).

While g can be any kind of recurrent unit, we use
long short-term memory (LSTM) (Hochreiter and
Schmidhuber, 1997) networks that have memory
cells for both directions (−→c t and ←−c t).

Figure 2: The encoder-decoder architecture we used as a

base model in this paper.

generating a ﬁxed number of words2, and ﬁnally the
output summaries are truncated to 75 bytes. Ideally,
the models should be able to change the output se-
quence depending on the given output length, and to
output the EOS tag at the appropriate time point in a
natural manner.

3 Network Architecture: Encoder-Decoder

with Attention

In this section, we describe the model architec-
ture used for our experiments: an encoder-decoder
consisting of bi-directional RNNs and an attention
mechanism. Figure 2 shows the architecture of the
model.

Suppose that the source sentence is represented as
a sequence of words x = (x1, x2, x3, ..., xN ). For

to

the

2According

code
num-
(https://github.com/facebook/NAMAS),
ber of words is set to 15, which is too long for the DUC2004
setting. The average number of words of human summaries in
the evaluation set is 10.43.

published
the

default

After encoding, we set the initial hidden states s0

4 Controlling Length in Encoder-decoders

and memory-cell m0 of the decoder as follows:

s0 = ←−h 1,
m0 = ←−c 1.

3.2 Decoder and Attender

Our decoder is based on an RNN with LSTM g:

st = g(st−1, xt).

We also use the attention mechanism developed
by Luong et al. (2015), which uses st to compute
contextual information dt of time step t. We ﬁrst
summarize the forward and backward encoder states
by taking their sum ¯hi = −→h i + ←−h i, and then calcu-
late the context vector dt as the weighted sum of
these summarized vectors:

dt =

(cid:88)

ati¯hi,

i

where at is the weight at the t-th step for ¯hi com-
puted by a softmax operation:

ati =

exp(st
·
¯h(cid:48) exp(st

(cid:80)

¯hi)

.

¯h(cid:48))

·

After context vector dt is calculated, the model
updates the distribution over the next word as fol-
lows:

˜st = tanh(Whs[st; dt] + bhs),

p(yt

y<t, x) = softmax(Wso ˜st + bso).
|

Note that ˜st is also provided as input to the LSTM
with yt for the next step, which is called the input
feeding architecture (Luong et al., 2015).

3.3 Training and Decoding

The training objective of our models is to maximize
log likelihood of the sentence-summary pairs in a
given training set D:

Lt(θ) =

(cid:88)

log p(y

x; θ),

|

(x,y)∈D
(cid:89)

p(y

x; θ) =

|

p(yt

y<t, x).
|

t

Once models are trained, we use beam search to ﬁnd
the output that maximizes the conditional probabil-
ity.

In this section, we propose our four methods that
can control the length of the output in the encoder-
In the ﬁrst two methods, the
decoder framework.
decoding process is used to control the output length
without changing the model itself. In the other two
methods, the model itself has been changed and is
trained to obtain the capability of controlling the
length. Following the evaluation dataset used in our
experiments, we use bytes as the unit of length, al-
though our models can use either words or bytes as
necessary.

4.1 f ixLen: Beam Search without EOS Tags

The ﬁrst method we examine is a decoding approach
similar to the one taken in many recent NSS meth-
ods that is slightly less ad-hoc. In this method, we
inhibit the decoder from generating the EOS tag by
assigning it a score of
. Since the model can-
−∞
not stop the decoding process by itself, we simply
stop the decoding process when the length of output
sequence reaches the desired length. More speciﬁ-
cally, during beam search, when the length of the se-
quence generated so far exceeds the desired length,
the last word is replaced with the EOS tag and also
the score of the last word is replaced with the score
of the EOS tag (EOS replacement).

4.2 f ixRng: Discarding Out-of-range

Sequences

Our second decoding method is based on discarding
out-of-range sequences, and is not inhibited from
generating the EOS tag, allowing it to decide when
to stop generation. Instead, we deﬁne the legitimate
range of the sequence by setting minimum and max-
imum lengths. Speciﬁcally, in addition to the normal
beam search procedure, we set two rules:

•

•

If the model generates the EOS tag when the
output sequence is shorter than the minimum
length, we discard the sequence from the beam.

If the generated sequence exceeds the maxi-
mum length, we also discard the sequence from
the beam. We then replace its last word with
the EOS tag and add this sequence to the beam

(EOS replacement in Section 4.1).3

In other words, we keep only the sequences that
contain the EOS tag and are in the deﬁned length
range. This method is a compromise that allows
the model some ﬂexibility to plan the generated se-
quences, but only within a certain acceptable length
range.

It should be noted that this method needs a larger
beam size if the desired length is very different from
the average summary length in the training data, as
it will need to preserve hypotheses that have the de-
sired length.

4.3 LenEmb: Length Embedding as
Additional Input for the LSTM

Our third method is a learning-based method specif-
ically trained to control the length of the output se-
quence. Inspired by previous work that has demon-
strated that additional inputs to decoder models can
effectively control the characteristics of the output
(Wen et al., 2015; Li et al., 2016b), this model pro-
vides information about the length in the form of an
additional input to the net. Speciﬁcally, the model
RD for each potential
uses an embedding e2(lt)
desired length, which is parameterized by a length
RD×L where L is the
embedding matrix Wle
∈
number of length types. In the decoding process, we
input the embedding of the remaining length lt as
additional input to the LSTM (Figure 3). lt is initial-
ized after the encoding process and updated during
the decoding process as follows:

∈

l1 = length,

lt+1 =

(cid:26) 0
lt

byte(yt)

−

byte(yt)

(lt
(otherwise),

−

≤

0)

where byte(yt) is the length of output word yt and
length is the desired length. We learn the values
of the length embedding matrix Wle during train-
ing. This method provides additional information
about the amount of length remaining in the output
sequence, allowing the decoder to “plan” its output
based on the remaining number of words it can gen-
erate.

3This is a workaround to prevent the situation in which all

sequences are discarded from a beam.

Figure 3: LenEmb: remaining length is used as addi-

tional input for the LSTM of the decoder.

Figure 4: LenInit: initial state of the decoder’s memory

cell m0 manages output length.

4.4 LenInit: Length-based Memory Cell

Initialization

While LenEmb inputs the remaining length lt to the
decoder at each step of the decoding process, the
LenInit method inputs the desired length once at
the initial state of the decoder. Figure 4 shows the ar-
chitecture of LenInit. Speciﬁcally, the model uses
the memory cell mt to control the output length by
initializing the states of decoder (hidden state s0 and
memory cell m0) as follows:

s0 = ←−h 1,
m0 = bc ∗

length,

(1)

where bc
∈
is the desired length.

RH is a trainable parameter and length

While the model of LenEmb is guided towards
the appropriate output length by inputting the re-
maining length at each step, this LenInit attempts
to provide the model with the ability to manage the
output length on its own using its inner state. Specif-
ically, the memory cell of LSTM networks is suit-
able for this endeavour, as it is possible for LSTMs

(a) ﬁrst sentence (206.91)

(c) ratio (0.35)
(b) summary (70.00)
Figure 5: Histograms of ﬁrst sentence length, summary length, and their ratio in DUC2004.

to learn functions that, for example, subtract a ﬁxed
amount from a particular memory cell every time
they output a word. Although other ways for man-
aging the length are also possible,4 we found this
approach to be both simple and effective.

5 Experiment

5.1 Dataset

We trained our models on a part of the Annotated
English Gigaword corpus (Napoles et al., 2012),
which Rush et al. (2015) constructed for sentence
summarization. We perform preprocessing using the
standard script for the dataset5. The dataset con-
sists of approximately 3.6 million pairs of the ﬁrst
sentence from each source document and its head-
line. Figure 1 shows the length histograms of the
summaries in the training set. The vocabulary size
is 116,875 for the source documents and 67,564
for the target summaries including the beginning-of-
sentence, end-of-sentence, and unknown word tags.
For LenEmb and LenInit, we input the length of
each headline during training. Note that we do not
train multiple summarization models for each head-
line length, but a single model that is capable of con-
trolling the length of its output.

We evaluate the methods on the evaluation set
of DUC2004 task-1 (generating very short single-
document summaries). In this task, summarization
systems are required to create a very short sum-
mary for each given document. Summaries over
the length limit (75 bytes) will be truncated and
there is no bonus for creating a shorter summary.
The evaluation set consists of 500 source documents
and 4 human-written (reference) summaries for each

4For example, we can also add another memory cell for

managing the length.

5https://github.com/facebook/NAMAS

source document. Figure 5 shows the length his-
tograms of the summaries in the evaluation set. Note
that the human-written summaries are not always as
long as 75 bytes. We used three variants of ROUGE
(Lin, 2004) as evaluation metrics: ROUGE-1 (uni-
gram), ROUGE-2 (bigram), and ROUGE-L (longest
common subsequence). The two-sided permutation
test (Chinchor, 1992) was used for statistical signif-
icance testing (p

0.05).

≤

5.2

Implementation

We use Adam (Kingma and Ba, 2015) (α=0.001,
β1=0.9, β2=0.999, eps=10−8) to optimize param-
eters with a mini-batch of size 80. Before every
10,000 updates, we ﬁrst sampled 800,000 training
examples and made groups of 80 examples with
the same source sentence length, and shufﬂed the
10,000 groups.

We set the dimension of word embeddings to 100
and that of the hidden state to 200. For LSTMs,
we initialize the bias of the forget gate to 1.0 and
use 0.0 for the other gate biases (J´ozefowicz et al.,
2015). We use Chainer (Tokui et al., 2015) to im-
plement our models. For LenEmb, we set L to 300,
which is larger than the longest summary lengths in
our dataset (see Figure 1-(b) and Figure 5-(b)).

For all methods except f ixRng, we found a beam
size of 10 to be sufﬁcient, but for f ixRng we used
a beam size of 30 because it more aggressively dis-
cards candidate sequences from its beams during de-
coding.

6 Result

6.1 ROUGE Evaluation

Table 1 shows the ROUGE scores of each method
with various length limits (30, 50 and 75 byte). Re-
gardless of the length limit set for the summariza-

model
f ixLen
f ixRng
LenEmb(0,L)
LenInit(0,L)
LenEmb(0,∞)
LenInit(0,∞)

30 byte
R-2
3.10∗
3.08∗
3.21
3.27
3.30
3.49

R-1
14.34
13.83∗
14.23
14.31
13.75
13.92

R-L
13.23
12.88
13.02
13.19
12.68
12.90

R-1
20.00∗
20.08∗
20.78
20.87
20.62
20.87

R-L
18.26∗
18.19∗
18.57
19.00
18.64
19.09

R-1
25.87∗
26.01
26.73
25.87
26.42
25.29∗

75 byte
R-2
7.93
7.69∗
8.39
8.27
8.26
8.00

R-L
23.07∗
22.77∗
23.88
23.24
23.59
22.71∗

50 byte
R-2
5.98
5.74
5.97
6.16
6.22
6.19

∗

Table 1: ROUGE scores with various length limits. The scores with

are signiﬁcantly worse than the best score in

the column (bolded).

source

reference
f ixLen (30)
(50)
(75)
f ixRng (30)
(50)
(75)
LenEmb (30)
(50)
(75)
LenInit (30)
(50)
(75)

ﬁve-time world champion michelle kwan withdrew from the #### us ﬁgure skating championships
on wednesday , but will petition us skating ofﬁcials for the chance to compete at the #### turin
olympics .
injury leaves kwan ’s olympic hopes in limbo
kwan withdraws from us gp
kwan withdraws from us skating championships
kwan pulls out of us ﬁgure skating championships for turin olympics
kwan withdraws from us gp
kwan withdraws from ﬁgure skating championships
kwan pulls out of us ﬁgure skating championships for turin olympics bid
kwan withdraws from us skating
kwan withdraws from us ﬁgure skating championships
world champion kwan withdraws from #### olympic ﬁgure skating championships
kwan quits us ﬁgure skating
kwan withdraws from #### us ﬁgure skating worlds
kwan withdraws from #### us ﬁgure skating championships for #### olympics

Table 2: Examples of the output of each method with various speciﬁed lengths.

tion methods, we use the same reference summaries.
Note that, f ixLen and f ixRng generate the sum-
maries with a hard constraint due to their decod-
ing process, which allows them to follow the hard
constraint on length. Hence, when we calculate the
scores of LenEmb and LenInit, we impose a hard
constraint on length to make the comparison fair
(i.e. LenEmb(0,L) and LenInit(0,L) in the table).
Speciﬁcally, we use the same beam search as that
for f ixRng with minimum length of 0.

For the purpose of showing the length control
capability of LenEmb and LenInit, we show at
the bottom two lines the results of the standard
beam search without the hard constraints on the
length6. We will use the results of LenEmb(0,∞)
and LenInit(0,∞) in the discussions in Sections 6.2
and 6.3.

The results show that the learning-based meth-

6 f ixRng is equivalence to the standard beam search when

we set the range as (0, ∞).

ods (LenEmb and LenInit) tend to outperform
decoding-based methods (f ixLen and f ixRng) for
the longer summaries of 50 and 75 bytes. How-
ever, in the 30-byte setting, there is no signiﬁcant
difference between these two types of methods. We
hypothesize that this is because average compres-
sion rate in the training data is 30% (Figure 1-(c))
while the 30-byte setting forces the model to gen-
erate summaries with 15.38% in average compres-
sion rate, and thus the learning-based models did not
have enough training data to learn compression at
such a steep rate.

6.2 Examples of Generated Summaries

Tables 2 and 3 show examples from the validation
set of the Annotated Gigaword Corpus. The ta-
bles show that all models, including both learning-
based methods and decoding-based methods, can of-
ten generate well-formed sentences.

We can see various paraphrases of “#### us ﬁgure

source

reference
f ixLen (30)
(50)
(75)
f ixRng (30)
(50)
(75)
LenEmb (30)
(50)
(75)
LenInit (30)
(50)
(75)

at least two people have tested positive for the bird ﬂu virus in eastern turkey , health minister
recep akdag told a news conference wednesday .
two test positive for bird ﬂu virus in turkey
two infected with bird ﬂu
two infected with bird ﬂu in eastern turkey
two people tested positive for bird ﬂu in eastern turkey says minister
two infected with bird ﬂu
two more infected with bird ﬂu in eastern turkey
two people tested positive for bird ﬂu in eastern turkey says minister
two bird ﬂu cases in turkey
two conﬁrmed positive for bird ﬂu in eastern turkey
at least two bird ﬂu patients test positive for bird ﬂu in eastern turkey
two cases of bird ﬂu in turkey
two people tested positive for bird ﬂu in turkey
two people tested positive for bird ﬂu in eastern turkey health conference

Table 3: More examples of the output of each method.

championships”7 and “withdrew”. Some examples
are generated as a single noun phrase (LenEmb(30)
and LenInit(30)) which may be suitable for the
short length setting.

6.3 Length Control Capability of
Learning-based Models

Figure 6 shows histograms of output length from the
standard encoder-decoder, LenEmb, and LenInit.
While the output lengths from the standard model
disperse widely, the lengths from our learning-based
models are concentrated to the desired length. These
histograms clearly show the length controlling capa-
bility of our learning-based models.

Table 4-(a) shows the ﬁnal state of the beam when
LenInit generates the sentence with a length of 30
bytes for the example with standard beam search in
Table 3. We can see all the sentences in the beam
are generated with length close to the desired length.
This shows that our method has obtained the ability
to control the output length as expected. For com-
parison, Table 4-(b) shows the ﬁnal state of the beam
if we perform standard beam search in the stan-
dard encoder-decoder model (used in f ixLen and
f ixRng). Although each sentence is well-formed,
the lengths of them are much more varied.

6.4 Comparison with Existing Methods

task-1. Although the objective of this paper is not to
obtain state-of-the-art scores on this evaluation set, it
is of interest whether our length-controllable models
are competitive on this task. Table 5 shows that the
scores of our methods, which are copied from Table
1, in addition to the scores of some existing methods.
ABS (Rush et al., 2015) is the most standard model
of neural sentence summarization and is the most
similar method to our baseline setting (f ixLen).
This table shows that the score of f ixLen is com-
parable to those of the existing methods. The table
also shows the LenEmb and the LenInit have the
capability of controlling the length without decreas-
ing the ROUGE score.

7 Conclusion

In this paper, we presented the ﬁrst examination of
the problem of controlling length in neural encoder-
decoder models, from the point of view of sum-
marization. We examined methods for controlling
length of output sequences:
two decoding-based
methods (f ixLen and f ixRng) and two learning-
based methods (LenEmb and LenInit). The re-
sults showed that learning-based methods generally
outperform the decoding-based methods, and the
learning-based methods obtained the capability of
controlling the output length without losing ROUGE
score compared to existing summarization methods.

Finally, we compare our methods to existing meth-
ods on standard settings of the DUC2004 shared

Acknowledgments

7Note that “#” is a normalized number and “us” is “US”

(United States).

This work was supported by JSPS KAKENHI Grant
Number JP26280080. We are grateful to have the

logp(y|x)
-4.27
-4.41
-4.65
-5.25
-5.27
-5.51
-5.55
-5.72
-6.04

byte
31
28
30
30
31
29
32
30
30

candidate summary
two cases of bird ﬂu in turkey
two bird ﬂu cases in turkey
two people tested for bird ﬂu
two people tested in e. turkey
two bird ﬂu cases in e. turkey
two bird ﬂu cases in eastern
two people tested in east turkey
two bird ﬂu cases in turkey :
two people fail bird ﬂu virus

logp(y|x)
-5.05
-5.13
-5.30
-5.49
-5.52
-5.55
-6.00
-6.04
-6.50

byte
57
50
39
51
32
44
49
54
49

candidate summary
two people tested positive for bird ﬂu in eastern turkey
two tested positive for bird ﬂu in eastern turkey
two people tested positive for bird ﬂu
two people infected with bird ﬂu in eastern turkey
two tested positive for bird ﬂu
two infected with bird ﬂu in eastern turkey
two more infected with bird ﬂu in eastern turkey
two more conﬁrmed cases of bird ﬂu in eastern turkey
two people tested positive for bird ﬂu in turkey

(a) the beam of LenInit

(b) the beam of the standard encoder-decoder

Table 4: Final state of the beam when the learning-based model is instructed to output a 30 byte summary for the

source document in Table 3.

(a) encoder-decoder

(b) LenEmb

(c) LenInit

Figure 6: Histograms of output lengths generated by (a) the standard encoder-decoder , (b) LenEmb, and (c)
LenInit. For LenEmb and LenInit, the bracketed numbers in each region are the desired lengths we
set.

model
f ixLen
f ixRng
LenEmb
LenInit
ABS(Rush et al., 2015)
ABS+(Rush et al., 2015)
RAS-Elman(Chopra et al., 2016)
RAS-LSTM(Chopra et al., 2016)
Table 5: Comparison with
DUC2004.
Note that
reproduced from Table 1.

R-1
25.88
26.02
26.73
25.87
26.55
28.18
28.97
27.41

R-2
7.93
7.69
8.40
8.28
7.06
8.49
8.26
7.69

R-L
23.07
22.78
23.88
23.25
22.05
23.81
24.06
23.06

existing

studies

for
rows are

top four

opportunity to use the Kurisu server of Dwango Co.,
Ltd. for our experiments.

References

[Ayana et al.2016] Ayana, S. Shen, Z. Liu, and M. Sun.
2016. Neural Headline Generation with Minimum
Risk Training. CoRR, abs/1604.01904.

Cho, and Yoshua Bengio.
2015. Neural machine
translation by jointly learning to align and translate.
In Proceedings of ICLR15.

[Banko et al.2000] Michele Banko, Vibhu O. Mittal, and
2000. Headline generation
In Proceedings of

Michael J. Witbrock.
based on statistical translation.
ACL00, pages 318–325.

[Chinchor1992] Nancy Chinchor.

1992. The statisti-
cal signiﬁcance of the muc-4 results. In Proceedings
MUC4 ’92, pages 30–50.

[Cho et al.2014] Kyunghyun Cho, Bart van Merrienboer,
Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder for
statistical machine translation. In Proceedings of the
EMNLP14, pages 1724–1734.

[Chopra et al.2016] Sumit Chopra, Michael Auli, and
Alexander M. Rush. 2016. Abstractive sentence sum-
marization with attentive recurrent neural networks. In
Proceedings of NAACL-HLT16, pages 93–98.

[Cohn and Lapata2008] Trevor Cohn and Mirella Lapata.
2008. Sentence compression beyond word deletion.
In Proceedings of COLING08, pages 137–144.

[Bahdanau et al.2015] Dzmitry Bahdanau, Kyunghyun

[Cohn and Lapata2013] Trevor Cohn and Mirella Lapata.

2013. An abstractive approach to sentence compres-
sion. ACM TIST13, 4(3):41:1–41:35, July.

models.
110–119.

In Proceedings of NAACL-HLT16, pages

[Dorr et al.2003] Bonnie Dorr, David Zajic, and Richard
Schwartz. 2003. Hedge trimmer: A parse-and-trim
approach to headline generation. In Proceedings of the
HLT-NAACL 03 Text Summarization Workshop, pages
1–8.

[Filippova and Altun2013] Katja Filippova and Yasemin
Altun. 2013. Overcoming the lack of parallel data in
sentence compression. In Proceedings of EMNLP13,
pages 1481–1491.

[Filippova and Strube2008] Katja Filippova and Michael
Strube. 2008. Dependency tree based sentence com-
pression. In Proceedings of INLG08, pages 25–32.
[Filippova et al.2015] Katja Filippova, Enrique Alfon-
seca, Carlos A. Colmenares, Lukasz Kaiser, and Oriol
Vinyals. 2015. Sentence compression by deletion
with lstms. In Proceedings of EMNLP15, pages 360–
368.

[Galanis and Androutsopoulos2010] Dimitrios Galanis
and Ion Androutsopoulos. 2010. An extractive super-
vised two-stage method for sentence compression. In
Proceedings of NAACL-HLT10, pages 885–893.
[Graves et al.2013] A. Graves, N. Jaitly, and A. r. Mo-
hamed. 2013. Hybrid speech recognition with deep
bidirectional lstm. In Proceedings of IEEE Workshop
on ASRU13, pages 273–278.

[Gu et al.2016] Jiatao Gu, Zhengdong Lu, Hang Li, and
Victor O.K. Li. 2016. Incorporating copying mech-
anism in sequence-to-sequence learning. In Proceed-
ings of ACL16, pages 1631–1640.

[Gulcehre et al.2016] Caglar Gulcehre, Sungjin Ahn,
Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio.
2016. Pointing the unknown words. In Proceedings of
ACL16, pages 140–149.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural Computation, 9(8):1735–1780.

[J´ozefowicz et al.2015] Rafal

J´ozefowicz, Wojciech
Zaremba, and Ilya Sutskever. 2015. An empirical
exploration of recurrent network architectures.
In
Proceedings of ICML15, pages 2342–2350.

[Kalchbrenner and Blunsom2013] Nal Kalchbrenner and
Phil Blunsom. 2013. Recurrent continuous translation
In Proceedings of EMNLP13, pages 1700–
models.
1709, Seattle, Washington, USA, October. Association
for Computational Linguistics.

[Kingma and Ba2015] Diederik P. Kingma and Jimmy
Ba. 2015. Adam: A method for stochastic optimiza-
tion. In Proceedings of ICLR15.

[Li et al.2016a] Jiwei Li, Michel Galley, Chris Brockett,
Jianfeng Gao, and Bill Dolan. 2016a. A diversity-
promoting objective function for neural conversation

[Li et al.2016b] Jiwei Li, Michel Galley, Chris Brockett,
Georgios Spithourakis, Jianfeng Gao, and Bill Dolan.
2016b. A persona-based neural conversation model.
In Proceedings of ACL16, pages 994–1003.

[Lin2004] Chin-Yew Lin. 2004. Rouge: A package for
automatic evaluation of summaries. In Proceedings of
the ACL04 Workshop, pages 74–81.

[Lopyrev2015] Konstantin Lopyrev. 2015. Generating
news headlines with recurrent neural networks. CoRR,
abs/1512.01712.

[Luong et al.2015] Thang Luong, Hieu Pham,

and
Christopher D. Manning. 2015. Effective approaches
to attention-based neural machine translation.
In
Proceedings of EMNLP15, pages 1412–1421.

[Nallapati et al.2016] Ramesh Nallapati, Bing Xiang, and
Bowen Zhou. 2016. Sequence-to-sequence rnns for
text summarization. CoRR, abs/1602.06023.

[Napoles et al.2011] Courtney Napoles, Chris Callison-
Burch, Juri Ganitkevitch, and Benjamin Van Durme.
2011.
Paraphrastic sentence compression with a
character-based metric: Tightening without deletion.
In Proceedings of the Workshop on Monolingual Text-
To-Text Generation, pages 84–90.

[Napoles et al.2012] Courtney Napoles, Matthew Gorm-
ley, and Benjamin Van Durme. 2012. Annotated gi-
gaword. In Proceedings of the Joint Workshop on Au-
tomatic Knowledge Base Construction and Web-scale
Knowledge Extraction, pages 95–100.

[Nenkova and McKeown2011] Ani Nenkova and Kath-
leen McKeown. 2011. Automatic summarization. In
Foundations and Trends R
in Information Retrieval,
(cid:13)
volume 2-3, pages 103–233.
Sumit
[Ranzato et al.2015] Marc’Aurelio
Chopra, Michael Auli,
and Wojciech Zaremba.
2015. Sequence level training with recurrent neural
networks. CoRR, abs/1511.06732.

Ranzato,

[Rosenfeld et al.2001] Ronald Rosenfeld, Stanley F.
Chen, and Xiaojin Zhu.
2001. Whole-sentence
exponential language models: a vehicle for linguistic-
statistical integration. Computer Speech & Language,
15(1):55–73.

[Rush et al.2015] Alexander M. Rush, Sumit Chopra, and
Jason Weston. 2015. A neural attention model for
In Proceedings
abstractive sentence summarization.
of EMNLP15, pages 379–389.

[Schuster and Paliwal1997] M. Schuster and K.K. Pali-
Bidirectional recurrent neural net-
IEEE Transactions on Signal Processing,

1997.

wal.
works.
45(11):2673–2681.

[Sennrich et al.2016] Rico Sennrich, Barry Haddow, and
Alexandra Birch. 2016. Controlling politeness in neu-

ral machine translation via side constraints.
ceedings of NAACL-HLT16, pages 35–40.

In Pro-

[Serban et al.2016] Iulian Vlad Serban, Alessandro Sor-
doni, Yoshua Bengio, Aaron C. Courville, and Joelle
Pineau. 2016. Building end-to-end dialogue systems
using generative hierarchical neural network models.
In Proceedings of AAAI16, pages 3776–3784.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and
Quoc V Le. 2014. Sequence to sequence learning with
In Proceedings of NIPS14, pages
neural networks.
3104–3112.

[Tokui et al.2015] Seiya Tokui, Kenta Oono, Shohei
Hido, and Justin Clayton. 2015. Chainer: a next-
generation open source framework for deep learning.
In Proceedings of NIPS15 Workshop on LearningSys.
[Vinyals et al.2015a] Oriol Vinyals, Lukasz Kaiser, Terry
Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hin-
ton. 2015a. Grammar as a foreign language. In Pro-
ceedings of NIPS15, pages 2773–2781.

[Vinyals et al.2015b] Oriol Vinyals, Alexander Toshev,
Samy Bengio, and Dumitru Erhan. 2015b. Show and
In Proceed-
tell: A neural image caption generator.
ings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 3156–3164.

[Wen et al.2015] Tsung-Hsien Wen, Milica Gasic, Nikola
Mrkˇsi´c, Pei-Hao Su, David Vandyke, and Steve
Young. 2015. Semantically conditioned lstm-based
natural language generation for spoken dialogue sys-
tems. In Proceedings of EMNLP15, pages 1711–1721,
Lisbon, Portugal, September. Association for Compu-
tational Linguistics.

[Woodsend et al.2010] Kristian Woodsend, Yansong
Feng, and Mirella Lapata. 2010. Title generation with
In Proceedings of the
quasi-synchronous grammar.
EMNLP10, pages 513–523.

[Wubben et al.2012] Sander Wubben, Antal van den
Bosch, and Emiel Krahmer. 2012. Sentence simpli-
ﬁcation by monolingual machine translation. In Pro-
ceedings of ACL12, pages 1015–1024.

[Xu et al.2015] Kelvin Xu,

Jimmy Ba, Ryan Kiros,
Kyunghyun Cho, Aaron Courville, Ruslan Salakhudi-
nov, Rich Zemel, and Yoshua Bengio. 2015. Show,
attend and tell: Neural image caption generation with
visual attention.
In David Blei and Francis Bach,
editors, Proceedings of ICML15, pages 2048–2057.
JMLR Workshop and Conference Proceedings.
[Zajic et al.2004] David Zajic, Bonnie J Dorr,

and
R. Schwartz. 2004. Bbn/umd at duc-2004: Topiary.
In Proceedings of NAACL-HLT04 Document Under-
standing Workshop, pages 112 – 119.

6
1
0
2
 
p
e
S
 
0
3
 
 
]
L
C
.
s
c
[
 
 
1
v
2
5
5
9
0
.
9
0
6
1
:
v
i
X
r
a

Controlling Output Length in Neural Encoder-Decoders

Yuta Kikuchi1
kikuchi@lr.pi.titech.ac.jp

Graham Neubig2∗
gneubig@cs.cmu.edu

Ryohei Sasano1
sasano@pi.titech.ac.jp

Hiroya Takamura1
takamura@pi.titech.ac.jp

Manabu Okumura1
oku@pi.titecjh.ac.jp

1Tokyo Institute of Technology, Japan
2Carnegie Mellon University, USA

Abstract

Neural encoder-decoder models have shown
great success in many sequence generation
tasks. However, previous work has not in-
vestigated situations in which we would like
to control the length of encoder-decoder out-
puts. This capability is crucial for applica-
tions such as text summarization, in which
we have to generate concise summaries with
In this paper, we pro-
a desired length.
pose methods for controlling the output se-
quence length for neural encoder-decoder
models: two decoding-based methods and two
learning-based methods.1 Results show that
our learning-based methods have the capabil-
ity to control length without degrading sum-
mary quality in a summarization task.

1

Introduction

Since its ﬁrst use for machine translation (Kalch-
brenner and Blunsom, 2013; Cho et al., 2014;
Sutskever et al., 2014),
the encoder-decoder ap-
proach has demonstrated great success in many
other sequence generation tasks including image
caption generation (Vinyals et al., 2015b; Xu et
al., 2015), parsing (Vinyals et al., 2015a), dialogue
response generation (Li et al., 2016a; Serban et
al., 2016) and sentence summarization (Rush et al.,
2015; Chopra et al., 2016). In particular, in this pa-
per we focus on sentence summarization, which as
its name suggests, consists of generating shorter ver-
sions of sentences for applications such as document

∗ This work was done when the author was at the Nara In-

stitute of Science and Technology.

1Available at https://github.com/kiyukuta/lencon.

summarization (Nenkova and McKeown, 2011) or
headline generation (Dorr et al., 2003). Recently,
Rush et al. (2015) automatically constructed large
training data for sentence summarization, and this
has led to the rapid development of neural sentence
summarization (NSS) or neural headline generation
(NHG) models. There are already many studies that
address this task (Nallapati et al., 2016; Ayana et al.,
2016; Ranzato et al., 2015; Lopyrev, 2015; Gulcehre
et al., 2016; Gu et al., 2016; Chopra et al., 2016).

One of the essential properties that text summa-
rization systems should have is the ability to gen-
erate a summary with the desired length. Desired
lengths of summaries strongly depends on the scene
of use, such as the granularity of information the
user wants to understand, or the monitor size of the
device the user has. The length also depends on the
amount of information contained in the given source
document. Hence, in the traditional setting of text
summarization, both the source document and the
desired length of the summary will be given as input
to a summarization system. However, methods for
controlling the output sequence length of encoder-
decoder models have not been investigated yet, de-
spite their importance in these settings.

In this paper, we propose and investigate four
methods for controlling the output sequence length
for neural encoder-decoder models. The former two
methods are decoding-based; they receive the de-
sired length during the decoding process, and the
training process is the same as standard encoder-
decoder models.
The latter two methods are
learning-based; we modify the network architecture
to receive the desired length as input.

In experiments, we show that the learning-based
methods outperform the decoding-based methods
for long (such as 50 or 75 byte) summaries. We
also ﬁnd that despite this additional length-control
capability, the proposed methods remain competi-
tive to existing methods on standard settings of the
DUC2004 shared task-1.

2 Background

2.1 Related Work

Text summarization is one of the oldest ﬁelds of
study in natural language processing, and many
summarization methods have focused speciﬁcally
on sentence compression or headline generation.
Traditional approaches to this task focus on word
deletion using rule-based (Dorr et al., 2003; Zajic
et al., 2004) or statistical (Woodsend et al., 2010;
Galanis and Androutsopoulos, 2010; Filippova and
Strube, 2008; Filippova and Altun, 2013; Filip-
pova et al., 2015) methods. There are also several
studies of abstractive sentence summarization us-
ing syntactic transduction (Cohn and Lapata, 2008;
Napoles et al., 2011) or taking a phrase-based sta-
tistical machine translation approach (Banko et al.,
2000; Wubben et al., 2012; Cohn and Lapata, 2013).
Recent work has adopted techniques such as
encoder-decoder (Kalchbrenner and Blunsom, 2013;
Sutskever et al., 2014; Cho et al., 2014) and atten-
tional (Bahdanau et al., 2015; Luong et al., 2015)
neural network models from the ﬁeld of machine
translation, and tailored them to the sentence sum-
marization task. Rush et al. (2015) were the ﬁrst
to pose sentence summarization as a new target task
for neural sequence-to-sequence learning. Several
studies have used this task as one of the bench-
marks of their neural sequence transduction meth-
ods (Ranzato et al., 2015; Lopyrev, 2015; Ayana
et al., 2016). Some studies address the other im-
portant phenomena frequently occurred in human-
written summaries, such as copying from the source
document (Gu et al., 2016; Gulcehre et al., 2016).
Nallapati et al.
(2016) investigate a way to solve
many important problems capturing keywords, or
inputting multiple sentences.

Neural encoder-decoders can also be viewed as
statistical language models conditioned on the tar-
get sentence context. Rosenfeld et al. (2001) have

proposed whole-sentence language models that can
consider features such as sentence length. However,
as described in the introduction, to our knowledge,
explicitly controlling length of output sequences in
neural language models or encoder-decoders has not
been investigated.

Finally, there are some studies to modify the out-
put sequence according some meta information such
as the dialogue act (Wen et al., 2015), user person-
ality (Li et al., 2016b), or politeness (Sennrich et al.,
2016). However, these studies have not focused on
length, the topic of this paper.

2.2

Importance of Controlling Output Length

As we already mentioned in Section 1, the most
standard setting in text summarization is to input
both the source document and the desired length of
the summary to a summarization system. Summa-
rization systems thus must be able to generate sum-
maries of various lengths. Obviously, this property
is also essential for summarization methods based
on neural encoder-decoder models.

Since an encoder-decoder model is a completely
data-driven approach, the output sequence length
depends on the training data that the model is trained
on. For example, we use sentence-summary pairs
extracted from the Annotated English Gigaword cor-
pus as training data (Rush et al., 2015), and the
average length of human-written summary is 51.38
bytes. Figure 1 shows the statistics of the corpus.
When we train a standard encoder-decoder model
and perform the standard beam search decoding on
the corpus, the average length of its output sequence
is 38.02 byte.

However,

there are other situations where we
want summaries with other lengths. For exam-
ple, DUC2004 is a shared task where the maximum
length of summaries is set to 75 bytes, and summa-
rization systems would beneﬁt from generating sen-
tences up to this length limit.

While recent NSS models themselves cannot con-
trol their output length, Rush et al. (2015) and others
following use an ad-hoc method, in which the sys-
tem is inhibited from generating the end-of-sentence
to the tag and
(EOS) tag by assigning a score of

−∞

(a) ﬁrst sentence (181.87)

(b) article headline (51.38)
Figure 1: Histograms of ﬁrst sentence length, headline length, and their ratio in Annotated Gigaword English Giga-

(c) ratio (0.30)

word corpus. Bracketed values in each subcaption are averages.

a given source sentence, the summarizer generates
a shortened version of the input (i.e. N > M ),
as summary sentence y = (y1, y2, y3, ..., yM ). The
x) us-
model estimates conditional probability p(y
|
ing parameters trained on large training data consist-
ing of sentence-summary pairs. Typically, this con-
ditional probability is factorized as the product of
conditional probabilities of the next word in the se-
quence:

p(y

x) =
|

p(yt

y<t, x),

|

M
(cid:89)

t=1

where y<t = (y1, y2, y3, ..., yt−1). In the following,
we describe how to compute p(yt

y<t, x).
|

3.1 Encoder

We use the bi-directional RNN (BiRNN) as en-
coder which has been shown effective in neural ma-
chine translation (Bahdanau et al., 2015) and speech
recognition (Schuster and Paliwal, 1997; Graves et
al., 2013).

A BiRNN processes the source sentence for
both forward and backward directions with two
separate RNNs. During the encoding process,
the BiRNN computes both forward hidden states
(−→h 1, −→h 2, ..., −→h N ) and backward hidden states
(←−h 1, ←−h 2, ..., ←−h N ) as follows:

−→h t = g(−→h t−1, xt),

←−h t = g(←−h t+1, xt).

While g can be any kind of recurrent unit, we use
long short-term memory (LSTM) (Hochreiter and
Schmidhuber, 1997) networks that have memory
cells for both directions (−→c t and ←−c t).

Figure 2: The encoder-decoder architecture we used as a

base model in this paper.

generating a ﬁxed number of words2, and ﬁnally the
output summaries are truncated to 75 bytes. Ideally,
the models should be able to change the output se-
quence depending on the given output length, and to
output the EOS tag at the appropriate time point in a
natural manner.

3 Network Architecture: Encoder-Decoder

with Attention

In this section, we describe the model architec-
ture used for our experiments: an encoder-decoder
consisting of bi-directional RNNs and an attention
mechanism. Figure 2 shows the architecture of the
model.

Suppose that the source sentence is represented as
a sequence of words x = (x1, x2, x3, ..., xN ). For

to

the

2According

code
num-
(https://github.com/facebook/NAMAS),
ber of words is set to 15, which is too long for the DUC2004
setting. The average number of words of human summaries in
the evaluation set is 10.43.

published
the

default

After encoding, we set the initial hidden states s0

4 Controlling Length in Encoder-decoders

and memory-cell m0 of the decoder as follows:

s0 = ←−h 1,
m0 = ←−c 1.

3.2 Decoder and Attender

Our decoder is based on an RNN with LSTM g:

st = g(st−1, xt).

We also use the attention mechanism developed
by Luong et al. (2015), which uses st to compute
contextual information dt of time step t. We ﬁrst
summarize the forward and backward encoder states
by taking their sum ¯hi = −→h i + ←−h i, and then calcu-
late the context vector dt as the weighted sum of
these summarized vectors:

dt =

(cid:88)

ati¯hi,

i

where at is the weight at the t-th step for ¯hi com-
puted by a softmax operation:

ati =

exp(st
·
¯h(cid:48) exp(st

(cid:80)

¯hi)

.

¯h(cid:48))

·

After context vector dt is calculated, the model
updates the distribution over the next word as fol-
lows:

˜st = tanh(Whs[st; dt] + bhs),

p(yt

y<t, x) = softmax(Wso ˜st + bso).
|

Note that ˜st is also provided as input to the LSTM
with yt for the next step, which is called the input
feeding architecture (Luong et al., 2015).

3.3 Training and Decoding

The training objective of our models is to maximize
log likelihood of the sentence-summary pairs in a
given training set D:

Lt(θ) =

(cid:88)

log p(y

x; θ),

|

(x,y)∈D
(cid:89)

p(y

x; θ) =

|

p(yt

y<t, x).
|

t

Once models are trained, we use beam search to ﬁnd
the output that maximizes the conditional probabil-
ity.

In this section, we propose our four methods that
can control the length of the output in the encoder-
In the ﬁrst two methods, the
decoder framework.
decoding process is used to control the output length
without changing the model itself. In the other two
methods, the model itself has been changed and is
trained to obtain the capability of controlling the
length. Following the evaluation dataset used in our
experiments, we use bytes as the unit of length, al-
though our models can use either words or bytes as
necessary.

4.1 f ixLen: Beam Search without EOS Tags

The ﬁrst method we examine is a decoding approach
similar to the one taken in many recent NSS meth-
ods that is slightly less ad-hoc. In this method, we
inhibit the decoder from generating the EOS tag by
assigning it a score of
. Since the model can-
−∞
not stop the decoding process by itself, we simply
stop the decoding process when the length of output
sequence reaches the desired length. More speciﬁ-
cally, during beam search, when the length of the se-
quence generated so far exceeds the desired length,
the last word is replaced with the EOS tag and also
the score of the last word is replaced with the score
of the EOS tag (EOS replacement).

4.2 f ixRng: Discarding Out-of-range

Sequences

Our second decoding method is based on discarding
out-of-range sequences, and is not inhibited from
generating the EOS tag, allowing it to decide when
to stop generation. Instead, we deﬁne the legitimate
range of the sequence by setting minimum and max-
imum lengths. Speciﬁcally, in addition to the normal
beam search procedure, we set two rules:

•

•

If the model generates the EOS tag when the
output sequence is shorter than the minimum
length, we discard the sequence from the beam.

If the generated sequence exceeds the maxi-
mum length, we also discard the sequence from
the beam. We then replace its last word with
the EOS tag and add this sequence to the beam

(EOS replacement in Section 4.1).3

In other words, we keep only the sequences that
contain the EOS tag and are in the deﬁned length
range. This method is a compromise that allows
the model some ﬂexibility to plan the generated se-
quences, but only within a certain acceptable length
range.

It should be noted that this method needs a larger
beam size if the desired length is very different from
the average summary length in the training data, as
it will need to preserve hypotheses that have the de-
sired length.

4.3 LenEmb: Length Embedding as
Additional Input for the LSTM

Our third method is a learning-based method specif-
ically trained to control the length of the output se-
quence. Inspired by previous work that has demon-
strated that additional inputs to decoder models can
effectively control the characteristics of the output
(Wen et al., 2015; Li et al., 2016b), this model pro-
vides information about the length in the form of an
additional input to the net. Speciﬁcally, the model
RD for each potential
uses an embedding e2(lt)
desired length, which is parameterized by a length
RD×L where L is the
embedding matrix Wle
∈
number of length types. In the decoding process, we
input the embedding of the remaining length lt as
additional input to the LSTM (Figure 3). lt is initial-
ized after the encoding process and updated during
the decoding process as follows:

∈

l1 = length,

lt+1 =

(cid:26) 0
lt

byte(yt)

−

byte(yt)

(lt
(otherwise),

−

≤

0)

where byte(yt) is the length of output word yt and
length is the desired length. We learn the values
of the length embedding matrix Wle during train-
ing. This method provides additional information
about the amount of length remaining in the output
sequence, allowing the decoder to “plan” its output
based on the remaining number of words it can gen-
erate.

3This is a workaround to prevent the situation in which all

sequences are discarded from a beam.

Figure 3: LenEmb: remaining length is used as addi-

tional input for the LSTM of the decoder.

Figure 4: LenInit: initial state of the decoder’s memory

cell m0 manages output length.

4.4 LenInit: Length-based Memory Cell

Initialization

While LenEmb inputs the remaining length lt to the
decoder at each step of the decoding process, the
LenInit method inputs the desired length once at
the initial state of the decoder. Figure 4 shows the ar-
chitecture of LenInit. Speciﬁcally, the model uses
the memory cell mt to control the output length by
initializing the states of decoder (hidden state s0 and
memory cell m0) as follows:

s0 = ←−h 1,
m0 = bc ∗

length,

(1)

where bc
∈
is the desired length.

RH is a trainable parameter and length

While the model of LenEmb is guided towards
the appropriate output length by inputting the re-
maining length at each step, this LenInit attempts
to provide the model with the ability to manage the
output length on its own using its inner state. Specif-
ically, the memory cell of LSTM networks is suit-
able for this endeavour, as it is possible for LSTMs

(a) ﬁrst sentence (206.91)

(c) ratio (0.35)
(b) summary (70.00)
Figure 5: Histograms of ﬁrst sentence length, summary length, and their ratio in DUC2004.

to learn functions that, for example, subtract a ﬁxed
amount from a particular memory cell every time
they output a word. Although other ways for man-
aging the length are also possible,4 we found this
approach to be both simple and effective.

5 Experiment

5.1 Dataset

We trained our models on a part of the Annotated
English Gigaword corpus (Napoles et al., 2012),
which Rush et al. (2015) constructed for sentence
summarization. We perform preprocessing using the
standard script for the dataset5. The dataset con-
sists of approximately 3.6 million pairs of the ﬁrst
sentence from each source document and its head-
line. Figure 1 shows the length histograms of the
summaries in the training set. The vocabulary size
is 116,875 for the source documents and 67,564
for the target summaries including the beginning-of-
sentence, end-of-sentence, and unknown word tags.
For LenEmb and LenInit, we input the length of
each headline during training. Note that we do not
train multiple summarization models for each head-
line length, but a single model that is capable of con-
trolling the length of its output.

We evaluate the methods on the evaluation set
of DUC2004 task-1 (generating very short single-
document summaries). In this task, summarization
systems are required to create a very short sum-
mary for each given document. Summaries over
the length limit (75 bytes) will be truncated and
there is no bonus for creating a shorter summary.
The evaluation set consists of 500 source documents
and 4 human-written (reference) summaries for each

4For example, we can also add another memory cell for

managing the length.

5https://github.com/facebook/NAMAS

source document. Figure 5 shows the length his-
tograms of the summaries in the evaluation set. Note
that the human-written summaries are not always as
long as 75 bytes. We used three variants of ROUGE
(Lin, 2004) as evaluation metrics: ROUGE-1 (uni-
gram), ROUGE-2 (bigram), and ROUGE-L (longest
common subsequence). The two-sided permutation
test (Chinchor, 1992) was used for statistical signif-
icance testing (p

0.05).

≤

5.2

Implementation

We use Adam (Kingma and Ba, 2015) (α=0.001,
β1=0.9, β2=0.999, eps=10−8) to optimize param-
eters with a mini-batch of size 80. Before every
10,000 updates, we ﬁrst sampled 800,000 training
examples and made groups of 80 examples with
the same source sentence length, and shufﬂed the
10,000 groups.

We set the dimension of word embeddings to 100
and that of the hidden state to 200. For LSTMs,
we initialize the bias of the forget gate to 1.0 and
use 0.0 for the other gate biases (J´ozefowicz et al.,
2015). We use Chainer (Tokui et al., 2015) to im-
plement our models. For LenEmb, we set L to 300,
which is larger than the longest summary lengths in
our dataset (see Figure 1-(b) and Figure 5-(b)).

For all methods except f ixRng, we found a beam
size of 10 to be sufﬁcient, but for f ixRng we used
a beam size of 30 because it more aggressively dis-
cards candidate sequences from its beams during de-
coding.

6 Result

6.1 ROUGE Evaluation

Table 1 shows the ROUGE scores of each method
with various length limits (30, 50 and 75 byte). Re-
gardless of the length limit set for the summariza-

model
f ixLen
f ixRng
LenEmb(0,L)
LenInit(0,L)
LenEmb(0,∞)
LenInit(0,∞)

30 byte
R-2
3.10∗
3.08∗
3.21
3.27
3.30
3.49

R-1
14.34
13.83∗
14.23
14.31
13.75
13.92

R-L
13.23
12.88
13.02
13.19
12.68
12.90

R-1
20.00∗
20.08∗
20.78
20.87
20.62
20.87

R-L
18.26∗
18.19∗
18.57
19.00
18.64
19.09

R-1
25.87∗
26.01
26.73
25.87
26.42
25.29∗

75 byte
R-2
7.93
7.69∗
8.39
8.27
8.26
8.00

R-L
23.07∗
22.77∗
23.88
23.24
23.59
22.71∗

50 byte
R-2
5.98
5.74
5.97
6.16
6.22
6.19

∗

Table 1: ROUGE scores with various length limits. The scores with

are signiﬁcantly worse than the best score in

the column (bolded).

source

reference
f ixLen (30)
(50)
(75)
f ixRng (30)
(50)
(75)
LenEmb (30)
(50)
(75)
LenInit (30)
(50)
(75)

ﬁve-time world champion michelle kwan withdrew from the #### us ﬁgure skating championships
on wednesday , but will petition us skating ofﬁcials for the chance to compete at the #### turin
olympics .
injury leaves kwan ’s olympic hopes in limbo
kwan withdraws from us gp
kwan withdraws from us skating championships
kwan pulls out of us ﬁgure skating championships for turin olympics
kwan withdraws from us gp
kwan withdraws from ﬁgure skating championships
kwan pulls out of us ﬁgure skating championships for turin olympics bid
kwan withdraws from us skating
kwan withdraws from us ﬁgure skating championships
world champion kwan withdraws from #### olympic ﬁgure skating championships
kwan quits us ﬁgure skating
kwan withdraws from #### us ﬁgure skating worlds
kwan withdraws from #### us ﬁgure skating championships for #### olympics

Table 2: Examples of the output of each method with various speciﬁed lengths.

tion methods, we use the same reference summaries.
Note that, f ixLen and f ixRng generate the sum-
maries with a hard constraint due to their decod-
ing process, which allows them to follow the hard
constraint on length. Hence, when we calculate the
scores of LenEmb and LenInit, we impose a hard
constraint on length to make the comparison fair
(i.e. LenEmb(0,L) and LenInit(0,L) in the table).
Speciﬁcally, we use the same beam search as that
for f ixRng with minimum length of 0.

For the purpose of showing the length control
capability of LenEmb and LenInit, we show at
the bottom two lines the results of the standard
beam search without the hard constraints on the
length6. We will use the results of LenEmb(0,∞)
and LenInit(0,∞) in the discussions in Sections 6.2
and 6.3.

The results show that the learning-based meth-

6 f ixRng is equivalence to the standard beam search when

we set the range as (0, ∞).

ods (LenEmb and LenInit) tend to outperform
decoding-based methods (f ixLen and f ixRng) for
the longer summaries of 50 and 75 bytes. How-
ever, in the 30-byte setting, there is no signiﬁcant
difference between these two types of methods. We
hypothesize that this is because average compres-
sion rate in the training data is 30% (Figure 1-(c))
while the 30-byte setting forces the model to gen-
erate summaries with 15.38% in average compres-
sion rate, and thus the learning-based models did not
have enough training data to learn compression at
such a steep rate.

6.2 Examples of Generated Summaries

Tables 2 and 3 show examples from the validation
set of the Annotated Gigaword Corpus. The ta-
bles show that all models, including both learning-
based methods and decoding-based methods, can of-
ten generate well-formed sentences.

We can see various paraphrases of “#### us ﬁgure

source

reference
f ixLen (30)
(50)
(75)
f ixRng (30)
(50)
(75)
LenEmb (30)
(50)
(75)
LenInit (30)
(50)
(75)

at least two people have tested positive for the bird ﬂu virus in eastern turkey , health minister
recep akdag told a news conference wednesday .
two test positive for bird ﬂu virus in turkey
two infected with bird ﬂu
two infected with bird ﬂu in eastern turkey
two people tested positive for bird ﬂu in eastern turkey says minister
two infected with bird ﬂu
two more infected with bird ﬂu in eastern turkey
two people tested positive for bird ﬂu in eastern turkey says minister
two bird ﬂu cases in turkey
two conﬁrmed positive for bird ﬂu in eastern turkey
at least two bird ﬂu patients test positive for bird ﬂu in eastern turkey
two cases of bird ﬂu in turkey
two people tested positive for bird ﬂu in turkey
two people tested positive for bird ﬂu in eastern turkey health conference

Table 3: More examples of the output of each method.

championships”7 and “withdrew”. Some examples
are generated as a single noun phrase (LenEmb(30)
and LenInit(30)) which may be suitable for the
short length setting.

6.3 Length Control Capability of
Learning-based Models

Figure 6 shows histograms of output length from the
standard encoder-decoder, LenEmb, and LenInit.
While the output lengths from the standard model
disperse widely, the lengths from our learning-based
models are concentrated to the desired length. These
histograms clearly show the length controlling capa-
bility of our learning-based models.

Table 4-(a) shows the ﬁnal state of the beam when
LenInit generates the sentence with a length of 30
bytes for the example with standard beam search in
Table 3. We can see all the sentences in the beam
are generated with length close to the desired length.
This shows that our method has obtained the ability
to control the output length as expected. For com-
parison, Table 4-(b) shows the ﬁnal state of the beam
if we perform standard beam search in the stan-
dard encoder-decoder model (used in f ixLen and
f ixRng). Although each sentence is well-formed,
the lengths of them are much more varied.

6.4 Comparison with Existing Methods

task-1. Although the objective of this paper is not to
obtain state-of-the-art scores on this evaluation set, it
is of interest whether our length-controllable models
are competitive on this task. Table 5 shows that the
scores of our methods, which are copied from Table
1, in addition to the scores of some existing methods.
ABS (Rush et al., 2015) is the most standard model
of neural sentence summarization and is the most
similar method to our baseline setting (f ixLen).
This table shows that the score of f ixLen is com-
parable to those of the existing methods. The table
also shows the LenEmb and the LenInit have the
capability of controlling the length without decreas-
ing the ROUGE score.

7 Conclusion

In this paper, we presented the ﬁrst examination of
the problem of controlling length in neural encoder-
decoder models, from the point of view of sum-
marization. We examined methods for controlling
length of output sequences:
two decoding-based
methods (f ixLen and f ixRng) and two learning-
based methods (LenEmb and LenInit). The re-
sults showed that learning-based methods generally
outperform the decoding-based methods, and the
learning-based methods obtained the capability of
controlling the output length without losing ROUGE
score compared to existing summarization methods.

Finally, we compare our methods to existing meth-
ods on standard settings of the DUC2004 shared

Acknowledgments

7Note that “#” is a normalized number and “us” is “US”

(United States).

This work was supported by JSPS KAKENHI Grant
Number JP26280080. We are grateful to have the

logp(y|x)
-4.27
-4.41
-4.65
-5.25
-5.27
-5.51
-5.55
-5.72
-6.04

byte
31
28
30
30
31
29
32
30
30

candidate summary
two cases of bird ﬂu in turkey
two bird ﬂu cases in turkey
two people tested for bird ﬂu
two people tested in e. turkey
two bird ﬂu cases in e. turkey
two bird ﬂu cases in eastern
two people tested in east turkey
two bird ﬂu cases in turkey :
two people fail bird ﬂu virus

logp(y|x)
-5.05
-5.13
-5.30
-5.49
-5.52
-5.55
-6.00
-6.04
-6.50

byte
57
50
39
51
32
44
49
54
49

candidate summary
two people tested positive for bird ﬂu in eastern turkey
two tested positive for bird ﬂu in eastern turkey
two people tested positive for bird ﬂu
two people infected with bird ﬂu in eastern turkey
two tested positive for bird ﬂu
two infected with bird ﬂu in eastern turkey
two more infected with bird ﬂu in eastern turkey
two more conﬁrmed cases of bird ﬂu in eastern turkey
two people tested positive for bird ﬂu in turkey

(a) the beam of LenInit

(b) the beam of the standard encoder-decoder

Table 4: Final state of the beam when the learning-based model is instructed to output a 30 byte summary for the

source document in Table 3.

(a) encoder-decoder

(b) LenEmb

(c) LenInit

Figure 6: Histograms of output lengths generated by (a) the standard encoder-decoder , (b) LenEmb, and (c)
LenInit. For LenEmb and LenInit, the bracketed numbers in each region are the desired lengths we
set.

model
f ixLen
f ixRng
LenEmb
LenInit
ABS(Rush et al., 2015)
ABS+(Rush et al., 2015)
RAS-Elman(Chopra et al., 2016)
RAS-LSTM(Chopra et al., 2016)
Table 5: Comparison with
DUC2004.
Note that
reproduced from Table 1.

R-1
25.88
26.02
26.73
25.87
26.55
28.18
28.97
27.41

R-2
7.93
7.69
8.40
8.28
7.06
8.49
8.26
7.69

R-L
23.07
22.78
23.88
23.25
22.05
23.81
24.06
23.06

existing

studies

for
rows are

top four

opportunity to use the Kurisu server of Dwango Co.,
Ltd. for our experiments.

References

[Ayana et al.2016] Ayana, S. Shen, Z. Liu, and M. Sun.
2016. Neural Headline Generation with Minimum
Risk Training. CoRR, abs/1604.01904.

Cho, and Yoshua Bengio.
2015. Neural machine
translation by jointly learning to align and translate.
In Proceedings of ICLR15.

[Banko et al.2000] Michele Banko, Vibhu O. Mittal, and
2000. Headline generation
In Proceedings of

Michael J. Witbrock.
based on statistical translation.
ACL00, pages 318–325.

[Chinchor1992] Nancy Chinchor.

1992. The statisti-
cal signiﬁcance of the muc-4 results. In Proceedings
MUC4 ’92, pages 30–50.

[Cho et al.2014] Kyunghyun Cho, Bart van Merrienboer,
Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder for
statistical machine translation. In Proceedings of the
EMNLP14, pages 1724–1734.

[Chopra et al.2016] Sumit Chopra, Michael Auli, and
Alexander M. Rush. 2016. Abstractive sentence sum-
marization with attentive recurrent neural networks. In
Proceedings of NAACL-HLT16, pages 93–98.

[Cohn and Lapata2008] Trevor Cohn and Mirella Lapata.
2008. Sentence compression beyond word deletion.
In Proceedings of COLING08, pages 137–144.

[Bahdanau et al.2015] Dzmitry Bahdanau, Kyunghyun

[Cohn and Lapata2013] Trevor Cohn and Mirella Lapata.

2013. An abstractive approach to sentence compres-
sion. ACM TIST13, 4(3):41:1–41:35, July.

models.
110–119.

In Proceedings of NAACL-HLT16, pages

[Dorr et al.2003] Bonnie Dorr, David Zajic, and Richard
Schwartz. 2003. Hedge trimmer: A parse-and-trim
approach to headline generation. In Proceedings of the
HLT-NAACL 03 Text Summarization Workshop, pages
1–8.

[Filippova and Altun2013] Katja Filippova and Yasemin
Altun. 2013. Overcoming the lack of parallel data in
sentence compression. In Proceedings of EMNLP13,
pages 1481–1491.

[Filippova and Strube2008] Katja Filippova and Michael
Strube. 2008. Dependency tree based sentence com-
pression. In Proceedings of INLG08, pages 25–32.
[Filippova et al.2015] Katja Filippova, Enrique Alfon-
seca, Carlos A. Colmenares, Lukasz Kaiser, and Oriol
Vinyals. 2015. Sentence compression by deletion
with lstms. In Proceedings of EMNLP15, pages 360–
368.

[Galanis and Androutsopoulos2010] Dimitrios Galanis
and Ion Androutsopoulos. 2010. An extractive super-
vised two-stage method for sentence compression. In
Proceedings of NAACL-HLT10, pages 885–893.
[Graves et al.2013] A. Graves, N. Jaitly, and A. r. Mo-
hamed. 2013. Hybrid speech recognition with deep
bidirectional lstm. In Proceedings of IEEE Workshop
on ASRU13, pages 273–278.

[Gu et al.2016] Jiatao Gu, Zhengdong Lu, Hang Li, and
Victor O.K. Li. 2016. Incorporating copying mech-
anism in sequence-to-sequence learning. In Proceed-
ings of ACL16, pages 1631–1640.

[Gulcehre et al.2016] Caglar Gulcehre, Sungjin Ahn,
Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio.
2016. Pointing the unknown words. In Proceedings of
ACL16, pages 140–149.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural Computation, 9(8):1735–1780.

[J´ozefowicz et al.2015] Rafal

J´ozefowicz, Wojciech
Zaremba, and Ilya Sutskever. 2015. An empirical
exploration of recurrent network architectures.
In
Proceedings of ICML15, pages 2342–2350.

[Kalchbrenner and Blunsom2013] Nal Kalchbrenner and
Phil Blunsom. 2013. Recurrent continuous translation
In Proceedings of EMNLP13, pages 1700–
models.
1709, Seattle, Washington, USA, October. Association
for Computational Linguistics.

[Kingma and Ba2015] Diederik P. Kingma and Jimmy
Ba. 2015. Adam: A method for stochastic optimiza-
tion. In Proceedings of ICLR15.

[Li et al.2016a] Jiwei Li, Michel Galley, Chris Brockett,
Jianfeng Gao, and Bill Dolan. 2016a. A diversity-
promoting objective function for neural conversation

[Li et al.2016b] Jiwei Li, Michel Galley, Chris Brockett,
Georgios Spithourakis, Jianfeng Gao, and Bill Dolan.
2016b. A persona-based neural conversation model.
In Proceedings of ACL16, pages 994–1003.

[Lin2004] Chin-Yew Lin. 2004. Rouge: A package for
automatic evaluation of summaries. In Proceedings of
the ACL04 Workshop, pages 74–81.

[Lopyrev2015] Konstantin Lopyrev. 2015. Generating
news headlines with recurrent neural networks. CoRR,
abs/1512.01712.

[Luong et al.2015] Thang Luong, Hieu Pham,

and
Christopher D. Manning. 2015. Effective approaches
to attention-based neural machine translation.
In
Proceedings of EMNLP15, pages 1412–1421.

[Nallapati et al.2016] Ramesh Nallapati, Bing Xiang, and
Bowen Zhou. 2016. Sequence-to-sequence rnns for
text summarization. CoRR, abs/1602.06023.

[Napoles et al.2011] Courtney Napoles, Chris Callison-
Burch, Juri Ganitkevitch, and Benjamin Van Durme.
2011.
Paraphrastic sentence compression with a
character-based metric: Tightening without deletion.
In Proceedings of the Workshop on Monolingual Text-
To-Text Generation, pages 84–90.

[Napoles et al.2012] Courtney Napoles, Matthew Gorm-
ley, and Benjamin Van Durme. 2012. Annotated gi-
gaword. In Proceedings of the Joint Workshop on Au-
tomatic Knowledge Base Construction and Web-scale
Knowledge Extraction, pages 95–100.

[Nenkova and McKeown2011] Ani Nenkova and Kath-
leen McKeown. 2011. Automatic summarization. In
Foundations and Trends R
in Information Retrieval,
(cid:13)
volume 2-3, pages 103–233.
Sumit
[Ranzato et al.2015] Marc’Aurelio
Chopra, Michael Auli,
and Wojciech Zaremba.
2015. Sequence level training with recurrent neural
networks. CoRR, abs/1511.06732.

Ranzato,

[Rosenfeld et al.2001] Ronald Rosenfeld, Stanley F.
Chen, and Xiaojin Zhu.
2001. Whole-sentence
exponential language models: a vehicle for linguistic-
statistical integration. Computer Speech & Language,
15(1):55–73.

[Rush et al.2015] Alexander M. Rush, Sumit Chopra, and
Jason Weston. 2015. A neural attention model for
In Proceedings
abstractive sentence summarization.
of EMNLP15, pages 379–389.

[Schuster and Paliwal1997] M. Schuster and K.K. Pali-
Bidirectional recurrent neural net-
IEEE Transactions on Signal Processing,

1997.

wal.
works.
45(11):2673–2681.

[Sennrich et al.2016] Rico Sennrich, Barry Haddow, and
Alexandra Birch. 2016. Controlling politeness in neu-

ral machine translation via side constraints.
ceedings of NAACL-HLT16, pages 35–40.

In Pro-

[Serban et al.2016] Iulian Vlad Serban, Alessandro Sor-
doni, Yoshua Bengio, Aaron C. Courville, and Joelle
Pineau. 2016. Building end-to-end dialogue systems
using generative hierarchical neural network models.
In Proceedings of AAAI16, pages 3776–3784.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and
Quoc V Le. 2014. Sequence to sequence learning with
In Proceedings of NIPS14, pages
neural networks.
3104–3112.

[Tokui et al.2015] Seiya Tokui, Kenta Oono, Shohei
Hido, and Justin Clayton. 2015. Chainer: a next-
generation open source framework for deep learning.
In Proceedings of NIPS15 Workshop on LearningSys.
[Vinyals et al.2015a] Oriol Vinyals, Lukasz Kaiser, Terry
Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hin-
ton. 2015a. Grammar as a foreign language. In Pro-
ceedings of NIPS15, pages 2773–2781.

[Vinyals et al.2015b] Oriol Vinyals, Alexander Toshev,
Samy Bengio, and Dumitru Erhan. 2015b. Show and
In Proceed-
tell: A neural image caption generator.
ings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 3156–3164.

[Wen et al.2015] Tsung-Hsien Wen, Milica Gasic, Nikola
Mrkˇsi´c, Pei-Hao Su, David Vandyke, and Steve
Young. 2015. Semantically conditioned lstm-based
natural language generation for spoken dialogue sys-
tems. In Proceedings of EMNLP15, pages 1711–1721,
Lisbon, Portugal, September. Association for Compu-
tational Linguistics.

[Woodsend et al.2010] Kristian Woodsend, Yansong
Feng, and Mirella Lapata. 2010. Title generation with
In Proceedings of the
quasi-synchronous grammar.
EMNLP10, pages 513–523.

[Wubben et al.2012] Sander Wubben, Antal van den
Bosch, and Emiel Krahmer. 2012. Sentence simpli-
ﬁcation by monolingual machine translation. In Pro-
ceedings of ACL12, pages 1015–1024.

[Xu et al.2015] Kelvin Xu,

Jimmy Ba, Ryan Kiros,
Kyunghyun Cho, Aaron Courville, Ruslan Salakhudi-
nov, Rich Zemel, and Yoshua Bengio. 2015. Show,
attend and tell: Neural image caption generation with
visual attention.
In David Blei and Francis Bach,
editors, Proceedings of ICML15, pages 2048–2057.
JMLR Workshop and Conference Proceedings.
[Zajic et al.2004] David Zajic, Bonnie J Dorr,

and
R. Schwartz. 2004. Bbn/umd at duc-2004: Topiary.
In Proceedings of NAACL-HLT04 Document Under-
standing Workshop, pages 112 – 119.

