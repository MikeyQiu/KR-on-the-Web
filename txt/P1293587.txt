HybridFusion: Real-Time Performance Capture
Using a Single Depth Sensor and Sparse IMUs

Zerong Zheng1, Tao Yu1,2, Hao Li3, Kaiwen Guo4, Qionghai Dai1, Lu Fang5,
and Yebin Liu1

1 Tsinghua University, Beijing, China
2 Beihang University, Beijing, China
3 University of Southern California, Los Angeles, CA, USA
4 Google Inc, Mountain View, CA, USA.
5 Tsinghua-Berkeley Shenzhen Institute, Tsinghua University

Abstract. We propose a light-weight yet highly robust method for real-
time human performance capture based on a single depth camera and
sparse inertial measurement units (IMUs). Our method combines non-
rigid surface tracking and volumetric fusion to simultaneously recon-
struct challenging motions, detailed geometries and the inner human
body of a clothed subject. The proposed hybrid motion tracking algo-
rithm and eﬃcient per-frame sensor calibration technique enable non-
rigid surface reconstruction for fast motions and challenging poses with
severe occlusions. Signiﬁcant fusion artifacts are reduced using a new
conﬁdence measurement for our adaptive TSDF-based fusion. The above
contributions are mutually beneﬁcial in our reconstruction system, which
enable practical human performance capture that is real-time, robust,
low-cost and easy to deploy. Experiments show that extremely challeng-
ing performances and loop closure problems can be handled successfully.

Keywords: Performance Capture; Real-time; Single-View; IMU.

1

Introduction

The 3D acquisition of human performances has been a challenging topic for
decades due to the shape and deformation complexity of dynamic surfaces, es-
pecially for clothed subjects. To ensure high-ﬁdelity digitalization, sophisticated
multi-camera array systems [8, 4, 5, 44, 17, 24, 7, 14, 30] are preferred for profes-
sional productions. TotalCapture [13], the state-of-the-art human performance
capture system, uses more than 500 cameras to minimize occlusions during
human-object interactions. Not only are these systems diﬃcult to deploy and
costly, they also come with a signiﬁcant amount of synchronization, calibration,
and data processing eﬀort.

On the other end of the spectrum, the recent trend of using a single depth
camera for dynamic scene reconstruction [25, 12, 10, 32] provides a very conve-
nient and real-time approach for performance capture combined with online non-
rigid volumetric depth fusion. However, such monocular systems are limited to

2

Z. Zheng et al.

slow and controlled motions. While improvement has been demonstrated lately
in systems like BodyFusion [45], DoubleFusion [46] and SobolevFusion [33], it is
still impossible to reconstruct occluded limb motions (Fig.1(b)) and ensure loop
closure during online reconstruction. For practical deployment, such as gaming,
where fast motion is expected and possibly interactions between multiple users,
it is necessary to ensure continuously reliable performance capture.

Fig. 1. The state-of-the-art methods easily get failed under severe occlusions. (a,d):
color references captured from Kinect (up) and a 3rd person view (down). (b,e) and
(c,f): results of DoubleFusion and our method rendered in the 3rd person view.

We propose HybridFusion, a real-time dynamic surface reconstruction system
that achieves high-quality reconstruction of extremely challenging performances
using hybrid sensors, i.e., a single depth camera and several inertial measurement
units (IMUs) sparsely located on the body. Intuitively, for the cases of extremely
fast or highly occluded or self-rotating limb motions, which cannot be handled
by the optical sensors alone, the IMUs can provide high frame rate orientation
information that help infer better human motion estimations. Moreover, they are
low cost and easy to wear. For other cases, a single depth camera owns suﬃcient
capacity to achieve robust reconstruction, so as to maintain the light-weight and
convenient property of the whole system compared to multi-camera ones.

Combining IMUs with depth sensors within a non-rigid depth fusion frame-
work is non-trivial. First, we need to minimize the eﬀort and experience required
for mounting and calibrating each IMU. We, therefore, propose a per-frame sen-
sor calibration algorithm integrated into the tracking procedure to get accurate
IMU calibration without any additional extra steps. We also extend the non-
rigid tracking optimization to a hybrid tracking optimization by adding the IMU
constraints. Moreover, previous tracking&fusion methods [25, 46] may generate
seriously deteriorated reconstruction results for challenging motions and occlu-
sions due to the wrongly fused geometry, which will further aﬀect the tracking
performance, and vice versa. We thus propose a simple yet eﬀective scheme that
jointly models the inﬂuence of body-camera distance, fast motions and occlu-
sions in one metric, which guides the TSDF (Truncated Signed Distance Field)
fusion to achieve robust and precise results even under challenging motions (see
Fig.1). Using such a light-weight hybrid setup, we believe HybridFusion presents
the right sweet spot for practical performance capture system as it is real-time,
robust and easy to deploy. Commodity users can capture high-quality body per-
formances and 3D content for gaming, VR/AR applications at home.

HybridFusion

3

Note that IMUs or even hybrid sensors have been adopted previously to im-
prove the skeleton-based motion tracking [11, 29, 20, 22]. Comparing with these
state-of-the-art hybrid motion capture systems like [11], the superiority of Hy-
bridFusion is twofold: for one, our system can reconstruct the detailed outer
surface of the subject and estimate the inner body shape simultaneously, while
[11] needs a pre-deﬁned model as input; for another, our system can track the
non-rigid motion of the outer surface, while [11] outputs skeleton poses merely.
By further examining the diﬀerences in the skeleton tracking solely, our sys-
tem still demonstrates substantially higher accuracy. In [11] IMU readings are
only used to query similar poses in a database, yet we integrate the inertial
measurements into a hybrid tracking energy. The detailed model and non-rigid
registration further improve the accuracy of pose estimation, since a detailed
geometry model with an embedding deformation node graph better describes
the motion of the user than a body model driven by a kinetic chain.

The main contributions of HybridFusion can be summarized as follows.

– Hybrid motion tracking. We propose a hybrid non-rigid tracking algo-
rithm for accurate skeleton motion and non-rigid surface motion tracking in
real-time. We introduce an IMU term that signiﬁcantly improves the tracking
performance even under severe occlusion.

– Sensor calibration. We introduce a per-frame sensor calibration method
to optimize the relationship between each IMU and its attached body part
[29, 20, 2],
during the capture process. Unlike other IMU-based methods
this method removes the requirement of explicit calibration and provides
accurate calibration results along the sequence.

– Adaptive Geometry fusion. To address the problem that previous TSDF
fusion methods are vulnerable in some challenging cases (far body-camera
distance, fast motions, occlusions, etc.), we propose an adaptive TSDF fu-
sion method that considers all the factors above in one tracking conﬁdence
measurement to get more robust and detailed TSDF fusion results.

2 Related Work

The related work can be classiﬁed into two categories: IMU-based human per-
formance capture and volumetric dynamic reconstruction. We refer readers to
overview of prior works including pre-scanned template based dynamic recon-
struction [15, 35, 41, 47, 9, 43], shape template based dynamic reconstruction [1,
18, 3, 31, 30] and free-form dynamic reconstruction [16, 23, 27, 36, 38] in [46].

IMU-based Human Performance Capture A line of research on combin-
ing vision and IMUs [11, 29, 20, 22, 28, 21] or even using IMUs alone[42] targets at
high quality human performance capture. Among all of those works, Malleson et
al. [20] combined multi-view color inputs, sparse IMUs and SMPL model [18] in
a real-time full-body skeleton motion capture system. Pons-moll et al. [29] used
multi-view color inputs, sparse IMUs and pre-scanned user templates to per-
form full-body motion capture oﬄine. The system is improved by using 6 IMUs

4

Z. Zheng et al.

alone [42] to reconstruct natural human skeleton motion using global optimiza-
tion method, but still oﬄine. Vlasic et al.
[40] used the output of the inertial
sensors for extended kalman ﬁlter to perform human skeleton motion capture.
Tautges et al. [37] and Ronit et al. [34] both utilized sparse accelerometer data
and data-driven methods to retrieve correct poses in the database. Helten et al.
[11] used the most similar setup to our method (single-view depth information,
sparse IMUs and parametric human body model). They combined generative
tracker and discriminative tracker that retrieving closest poses in a dataset and
perform real-time human motion tracking. However, the parametric body model
cannot describe detailed surfaces of clothing.

Non-rigid Surface Integration Starting from DynamicFusion [25], non-
rigid surface integration methods get more and more popular
[10, 32, 12] be-
cause of the single-view, real-time and template-free properties. It also inspires
a branch of multi-view volumetric dynamic reconstruction methods [7, 6] that
achieved high quality reconstruction results. The basic idea of non-rigid sur-
face integration is to perform non-rigid surface tracking and TSDF surface fu-
sion iteratively, such that the surface information gets more and more complete
along the scene motions when unseen surface parts get observed and tracked.
To improve the reconstruction performance of DynamicFusion on human body
motions, BodyFusion [45] integrated articulated human motion prior (skeleton
kinematic chain structure) and constraint the non-rigid deformation and skele-
ton motion to be similar. DoubleFusion [46] leveraged parametric body model
(SMPL [18]) in non-rigid surface integration to improve the tracking, loop clo-
sure and fusion performance, and achieved the state-of-the-art single-view human
performance capture results. However, all of these methods are still incompetent
to handle fast and challenging motions, especially for occluded motions.

3 Overview

Fig. 2. Illustration of HybridFusion pipeline.

Initialization We adopt 8 IMUs that sparsely located on the upper and
lower limbs of the performer as shown in Fig. 2. It is worth mentioning that
unlike [42, 20] which require IMUs to be speciﬁc to model vertices, the IMUs
in our system are attached to bones as we merely trust and use the orientation
measurements. Such strategy greatly relaxes users’ eﬀorts to wear the sensors

HybridFusion

5

since they only need to ensure the IMUs are attached to the correct bones
and roughly aligned with their length directions. Here the number of IMUs is
determined by the balance between performance and convenience, as further
elaborated in Sec.7.3.

The performer is required to start with a rough A-pose. After getting the
ﬁrst depth frame, we use it to initialize the TSDF volume by projecting the
depth pixels into the volume, and then estimate the initial shape parameters
β0 and pose θ0 using volumetric shape-pose optimization [46]. We construct a
”double node graph” consisting of predeﬁned on-body node graph and free-form
sampled far-body node graph. We use θ0 and the initial IMU readings to initialize
sensor calibration. The triangle mesh is extracted from the TSDF volume with
Marching Cube algorithm [19].

Main Pipeline The lack of ground truth transformation between IMUs and
their attached bones leads to unstable tracking performance in our hybrid mo-
tion tracking step. Therefore, we keep optimizing the sensor calibration frame by
frame, and the calibration gets more and more accurate thanks to the increasing
number of successfully tracked frames with diﬀerent skeleton poses. Following
[46], we also optimize the inner body shape and the canonical pose. In summary,
our pipeline performs hybrid motion tracking, adaptive geometry fusion, volu-
metric shape-pose optimization and sensor calibration sequentially, as shown in
Fig.2. Below is a brief introduction of the main components of our pipeline.

– Hybrid Motion Tracking Given the current depth map and the IMU
measurements, we propose to jointly track the skeletal motion and the surface
non-rigid deformation through a new hybrid motion tracking algorithm. We
construct a new energy term to constrain the orientations of the skeleton
bones using the orientation measurements of their corresponding IMUs.
– Adaptive Geometry Fusion To improve the robustness of the fusion step,
we propose an adaptive fusion method that utilizes tracking conﬁdence to
adjust the weight of TSDF fusion adaptively. The tracking conﬁdence can
be estimated according to the normal equations in the current procedure of
hybrid motion tracking.

– Volumetric Shape-Pose Optimization We perform volumetric shape-
pose optimization after adaptive geometry fusion. Based on the updated
TSDF volume, we optimize the inner body shape and canonical pose to
obtain better canonical body ﬁtting and skeleton embedding.

– Sensor Calibration Given the motion tracking results and IMU readings
at current frame, we optimize the sensor calibration to acquire more accurate
estimations of the transformations between IMUs and their corresponding
bones, as well as more accurate transformation estimation between the in-
ertial coordinate and the camera coordinate.

4 Hybrid Motion Tracking

Since our pipeline focuses on performance capture of human, we adopt a double-
layer surface representation for motion tracking, which has been proved to be

6

Z. Zheng et al.

eﬃcient and robust in [46]. Similar to [9, 45, 46], our motion tracking is un-
der the assumption that human motion largely follows articulated structures.
Therefore, we use two kinds of motion parameterizations, skeleton motions and
non-rigid node deformation. Combining IMU orientation informations, we con-
struct a energy function for hybrid motion tracking in order to solve the two
motion components in a joint optimization scheme. Given the depth map Dt
and inertial measurements Mt of current frame t, the energy function is:

Emot = λIMUEIMU + λdepthEdepth + λbindEbind + λregEreg + λpriEpri,

(1)

where EIMU, Edepth, Ebind, Ereg and Eprior represent IMU, depth, binding, reg-
ularization and pose prior term respectively. EIMU and Edepth are data terms
that constrain the results to be consistant with IMU and depth input, Ebind
regularizes the surface non-rigid deformation with articulated skeleton motion,
Ereg constrains the locally as-rigid-as-possible property of the node graph and
Eprior is used to penalize unnatural human poses. To simplify the notation, we
claim that all variables in this section take their values at the current frame t,
and drop their subscripts of frame index.

IMU term To bridge the sensors’ measurements and hybrid motion track-
ing pipeline, we select N = 8 binding bones on the SMPL model (Fig. 2 Initial-
ization) for the N inertial sensors, and these bones are denoted by bIM U
(i =
1, . . . , N ). The IMU term penalizes the orientation diﬀerence between IMU read-
ings and the estimated orientations of their attached binding bones:

i

2

F

EIMU =

RI2C

RiR−1

S2B,i − R

bIM U
i

(2)

i∈S (cid:13)
X
(cid:13)
(cid:13)

(cid:1)(cid:13)
(cid:13)
e
(cid:13)
Ri is the orientation measurement of i-th
where S is the index set of IMUs;
sensor in the inertial coordinate system. RI2C is the rotation oﬀset between the
inertial coordinate and the camera coordinate system, while RS2B,i is the oﬀset
between the i-th IMU and its corresponding bone; more details are elaborated in
Sec.5. R(bIM U
),
which is deﬁned as:

) is the rotational part of the skeleton skinning matrix G(bIM U

e

(cid:0)

i

i

G(bIM U
i

) = Gj =

exp

θk ˆξk

,

(3)

k∈Kj
Y

(cid:17)
where j is the index of bIM U
in the skeleton structure; Gj is the cascaded
rigid transformation of jth bone; Kj represents parent bones indices of jth bone
along the backward kinematic chain; exp(θk ˆξk) is the exponential map of the
twist associated with kth bone.

(cid:16)

i

Note that RI2C and RB2S are crucial parameters determining the eﬀec-
tiveness of the IMU term, and therefore they are continually optimized in our
pipeline even though we can obtain suﬃciently accurate estimations through ini-
tial calculation. We provide more details about calculating and optimizing RI2C
and RS2B in Sec.5.

The other energy terms in Eqn.1 are detailed in [45, 46], as well as the eﬃcient
GPU solver for motion tracking. Please refer to these two papers for more details.

HybridFusion

7

5 Sensor Calibration

On one hand, an inertial sensor gives orientation measurements in the inertial co-
ordinate system, which is typically deﬁned by the gravity ﬁeld and geomagnetic
ﬁeld. On the other hand, our performance capture system runs in the camera
coordinate system, which is independent of the inertial coordinate. The rela-
tionship between these two coordinates can be described as a constant mapping
denoted by RI2C. Based on the mapping, we can transform all IMU outputs from
inertial coordinate to the camera coordinate system, as formulated in Eqn.2. As

Fig. 3. Illustration of diﬀerent coordinates and their relationship.

illustrated in Fig.3, several coordinate systems are involved in order to estimate
the mapping: (1)the i-th IMU sensor coordinate system CSi , which is aligned
with the ith sensor itself, and changes when the sensor moves, (2)the inertial
coordinate system CI, which remains static all the time, (3)the i-th bone coor-
dinate system CBi , which is aligned with the bone associated with the ith IMU
sensor, and changes when the subject acts or moves, (4)the camera coordinate
system CC, which also remains static. Accordingly, RS2B is the transformation
from CS to CB, RI2C is from CI to CS, and their inverse transformations are
denoted as RB2S and RC2I .

5.1

Initial Sensor Calibration

We calculate an approximation of RI2C during the initialization of our pipeline.
After ﬁtting the SMPL model to the depth image, the mapping RB2C,i: CBi →
CC is available according to RB2C,i = Rt0
, where the subscript t0 is the
index of the ﬁrst frame. Besides, we can also obtain the mapping from CI to CSi
by assigning the inverse matrix of the sensor’s reading at the ﬁrst frame: RI2S,i =
R−1
i,t0 . To transform CI into CC through the path CI → CSi → CBi → CC, we
need to know the rotation oﬀset between the IMUs and their corresponding bone
coordinate systems RS2B,i: CSi → CBi . We assume that they are constant as the
e
sensors are tightly attached to the limbs and we then predeﬁne them according
to the placement of the sensors. Thus, we can compute RI2C by

bIM U
i

(cid:0)

(cid:1)

RI2C = SLERP
i=1,...,N

= SLERP
i=1,...,N

{(RI2C,i , wi)} = SLERP
i=1,...,N
R−1

RS2B,i

bIM U
i

Rt0

n(cid:16)

(cid:0)

(cid:1)

i,t0 , wi

,

(cid:17)o

e

{(RB2C,iRS2B,iRI2S,i , wi)}

(4)

8

Z. Zheng et al.

where SLERP {·} is the operator of spherical linear interpolation, and wi is the
interpolation weight, which is set to 1/N in our experiment.

5.2 Per-Frame Calibration Optimization

Even though the inﬂuence of measurement noises tends to be diminished by
averaging RI2C,i (Sec.5.1), the solution of the initial sensor calibration is still
prone to errors due to the sparse IMU setup and the rough assignments of RS2B,i.
Therefore, we propose an eﬃcient method to continuously optimize the sensor
calibration. As formulated in Sec.4, the orientation measurements and motion
estimation are related by RI2C and RB2S,i:

RI2C

Ri = R

bIM U
i

R−1

B2S,i,

thus we can compute the accumulated rotations from t0 to t as:
e
i,t0 R−1

I2C = Rt

bIM U
i

bIM U
i

R−1
t0

RI2C

R−1

Ri,t

.

(cid:0)

(cid:1)

(5)

(6)

Given the motion tracking results, we estimate the optimal rotation oﬀset of
frame t according to

e

e

(cid:0)

(cid:1)

(cid:0)

(cid:1)

ˆRI2C = arg min

RI2C

RI2C

Ri,t

R−1

i,t0 R−1

I2C − Rt

bIM U
i

R−1
t0

bIM U
i

(cid:1)(cid:13)
(cid:13)
(cid:13)
and then update RI2C by blending the solution with the original value:

e

e

(cid:1)

(cid:0)

(cid:0)

i∈S (cid:13)
X
(cid:13)
(cid:13)

2

F

,

(7)

RI2C ← SLERP

(RI2C, w) ;

ˆRI2C, ω

(8)

where w, ω are both interpolation weights. We set w = 1 − 1
t to make
sure the ﬁnal solution coverage to a stable global optimum. We optimize RS2B,i
in similar ways.

t , ω = 1

n

(cid:16)

(cid:17)o

6 Adaptive Geometry Fusion

Similar to prior works [26, 12, 7, 10, 46], we integrate depth maps into a reference
volume. To deal with the ambiguity caused by voxel collision, we follow [10,
46, 7] to detect collided voxels by voting the TSDF value at live frame and
avoid integrating depth information into these voxels. Besides voxel collision, the
surface fusion still suﬀers from inaccurate motion tracking, which is a factor that
previous fusion methods do not consider. Inspired by previous works addressing
the uncertainty of parameter estimation[48, 39], we propose to fuse geometry
adaptively according to the tracking conﬁdence that measures the performance
of hybrid motion tracking. Speciﬁcally, we denote xt as the motion parameters
being solved and assume it approximately follows a normal distribution:

p(xt|Dt, Mt) ≃ N (µt, Σt) ,

(9)

HybridFusion

9

where µt is the solution of motion tracking and the covariance Σt measures the
tracking uncertainty. By assuming p(xt|Dt, Mt) ∝ exp(−Emot), we can approx-
imate the covariance as

Σt = σ2

JT J

−1

(10)

where J is the Jacobian of Emot.

(cid:0)

(cid:1)

Fig. 4. Visualization of the estimated per-node tracking conﬁdence in 3 scenarios: large
body-camera distance(a), fast motions(b) and occlusions(c).

t

We regard the diagonal of Σ−1

as the conﬁdence vector of the solution µt,
which contains the conﬁdence of both skeleton tracking and non-rigid tracking
parameters calculated by our hybrid motion tracking algorithm. Since the TSDF
fusion step only needs node graph to perform non-rigid deformation [25], we
merge the two types of motion tracking conﬁdence together to get a more ac-
curate estimation of hybrid tracking conﬁdence for each node. Therefore, the
tracking conﬁdence Ctrack (xk) corresponding to a node xk can be computed as

t )xk

diag( ¯Σ−1
ηxk

t )bj

diag( ¯Σ−1
ηbj

, 1

Ctrack (xk) = (1−λ) min

, 1

+λ

wj,xk min

(cid:18)

!
(11)
where B is the index set of bones; diag( ¯Σ−1
t )bj are the averaged
covariance values of all ICP iterations corresponding to the kth node and jth
bone respectively. wj,xk is the skinning weight associated with xk.

j∈B
X
t )xk and diag( ¯Σ−1

 

(cid:19)

To better illustrate the tracking conﬁdence, we classify the performance cap-
ture scenarios that will adversely impact the tracking performance into 3 cat-
egories (far body-camera distance, fast motions and occlusions) and visualized
the estimated tracking conﬁdence of each node in these scenarios in Fig. 4. Since
the quality of depth input is inversely proportional to body-camera distance and
the low quality depth will signiﬁcantly deteriorate the tracking and fusion per-
formance, the tracking conﬁdence of all nodes declines when the body is far from
the camera (Fig. 4(a)); Moreover, the nodes under fast motions also have low
tracking conﬁdence (Fig. 4(b)), as the tracking performance for fast motions is
usually worse than slow motions due to the blurred depth input and lack of cor-
respondences; Last, for single-view capture system, occlusions will lead to lack
of observations and worse tracking performance of corresponding body parts.
Thus, the tracking conﬁdence of occluded nodes decreases as in Fig. 4(c).

After calculating the tracking conﬁdence, we perform adaptive geometry fu-
sion as follows. For a voxel v, D(v) denotes the TSDF value of the voxel, W(v)
denotes its accumulated fusion weight, d(v) is the projective signed distance

10

Z. Zheng et al.

function (PSDF) value, and ω(v) is the fusion weight of v at current frame:

ω′(v) =

Ctrack (xk) , ω(v) =

0
ω′(v)

(

ω′(v) < τ ,
otherwise.

(12)

xk∈N (v)
X

Finally, the voxel is updated by

D(v) ←

D(v)W(v) + d(v)ω(v)
W(v) + ω(v)

, W(v) ← W(v) + ω(v)

(13)

where N (v) is the collection of the KNN deformation nodes of voxel v, and τ is
a threshold controlling the minimum integration weight.

7 Experiments

We evaluate the performance of our proposed method in this section. In Sec.7.1
we present details on the setup of our system and report the main parameters
of our pipeline. Then we compare our system with the state-of-the-art method
both qualitatively and quantitatively in Sec.7.2. We also provide evaluations of
our main contributions in Sec.7.3.

Fig.5 demonstrates the reconstructed dynamic geometries and the inner body
shapes on several motion sequences, including sports, dancing and so on. From
the results we can see that our system is able to reconstruct various kinds of
challenging motions and inner body shapes using a single-view setup.

Fig. 5. Example results reconstructed by our system. In each grid, the left image is
the color reference; the middle one is the fused surface geometry; and the right one is
the inner body shape estimated by our system.

HybridFusion

11

7.1 System Setup

For the hard-ware setup, we use Kinect One and Noitom Legacy suite as the
depth sensor and inertial sensors respectively. Our system runs in real-time (33ms
per frame) on a NVIDIA TITAN X GPU and an Intel Core i7-6700K CPU. The
majority of the running time is spent on the joint motion tracking (23 ms) and
the adaptive geometric fusion (6 ms). The sensor calibration optimization takes
1 ms while the shape-pose optimization takes 3 ms.

The weights of energy terms serve to balance the impact of diﬀerent tracking
cues, where the weight of IMU term is set to 5.0, while the other energy weights
are identical to [16]. More speciﬁcally, the strategy of assigning λIM U is to ensure
that (1) the IMU term can produce rough pose estimations, when there is a lack
of correspondences (fast motion and/or occlusion), and (2) the IMU term does
not aﬀect the tracking adversely, when enough correspondences are available.
Note that λdepth = 1.0 and λbind = 1.0 initially, and the binding term will be
gradually relaxed so as to capture the detailed non-rigd motion of the surface.
The weights of the regularization term and prior term are ﬁxed to 5.0 and 0.01
respectively, avoiding undesirable results.

7.2 Comparison

We compare against the state-of-the-art method, DoubleFusion[46] on 4 se-
quences, as shown in Fig.6. The tracking performance of our system clearly
outperforms DoubleFusion especially under severe occlusions. To make quanti-
tative comparison, we capture several sequences using the Vicon and our system
simultaneously. Both systems are synchronized by ﬂashing the infrared LED.
We calibrate these two systems spatially by manually selecting the correspond-
ing point pairs and calculate their transformation. After that, we transform the
marker positions from the Vicon coordinate into the camera coordinate at the
ﬁrst frame, followed by tracking their motions using the motion ﬁeld and com-
paring the per-frame positions with the Vicon-detected ground-truth. We do the
same tests on DoubleFusion. Fig.7 presents the curves of per-frame maximum
error of DoubleFusion and our method on one sequence. We also list the average
errors over the entire sequence in Tab.1. From the numerical results we can see
that our system achieve the higher tracking accuracy than DoubleFusion.

Table 1. Average numerical errors on the entire sequence.

Method
Avg.of Max.Err. (m)

DoubleFusion HybridFusion

0.0854

0.0655

We also compare our skeleton tracking performance against the state-of-the-
art hybrid tracker, [11], using its published dataset. As depicted in Tab.2, our
system maintains more accurate and stable performance for skeleton tracking,
inducing much smaller tracking errors than [11].

12

Z. Zheng et al.

Table 2. Average joint tracking error and standard deviation in millimeters (compared
with [11]).

Sequence
Helten et al. [11] 35.7(24.9) 47.4(31.4) 44.4(33.8) 34.7(25.4) 59.1(45.3) 56.2(41.6)
20.9(15.2) 27.6(19.6) 27.0(17.6) 15.5(15.6) 43.5(33.6) 40.9(27.5)
Ours

D2

D1

D5

D3

D6

D4

7.3 Evaluation

Sensor Calibration. In Fig.8, we evaluate the proposed per-frame sensor cali-
bration on a simple sequence. Fig.8(c) is the surface reconstruction results only
using initial calibration results as described in Sec.5.1, without the per-frame cal-
ibration optimization step(Sec.5.2). We can see that the joint motion tracking
performance suﬀers from the inaccuracy of the initial calibration results. More-
over, the erroneous motion tracking performance will lead to erroneous surface
fusion results (ghost hands and legs). With the per-frame calibration optimiza-
tion algorithm, our system can generate accurate motion tracking and surface
fusion results as shown in Fig.8(d).

Fig. 6. Qualitative comparison against DoubleFusion. 1st row: Color and depth image
as reference. 2nd and 3rd rows: The results reconstructed by DoubleFusion and our
system respectively.

Fig. 7. Quantitative comparison on tracking accuracy against DoubleFusion. (a): The
curves of maximum position error. (b): The results of our system on two time instances.

Adaptive Geometry Fusion. We also evaluate the eﬀectiveness of the adap-
tive geometric fusion method. We captured several sequences in three challeng-

HybridFusion

13

Fig. 8. Evaluation of per-frame sensor calibration optimization.(a)(b): Color and depth
images as reference. (c): The reconstruction results without calibration optimization.
(d): The reconstruction results with calibration optimization.

ing scenarios for detailed surface fusion, which include far body-camera distance,
body-part occlusion and fast motion. We then compare our adaptive geometry
fusion method against previous fusion method used in [26, 10, 45, 46]. In Fig.9,
the results of the previous fusion method are presented on the left side of each
sub-ﬁgure, while the reconstruction results with adaptive fusion are shown on
the right. As shown in Fig. 4, the fusion weights in our system can be auto-
matically adjusted (set to a very small value or skip the fusion step) in all the
situations, resulting in more plausible and detailed surface fusion results.

Fig. 9. Evaluation of adaptive fusion under far body-camera distance (a), occlusions
(b) and fast motions (c). In each sub-ﬁgure, the left mesh is fused by previous fusion
method and the right one is fused using our adaptive fusion method.

Challenging Loop Closure. In order to evaluate the performance of our sys-
tem on challenging loop closure, we capture several challenging turning around
motions. The results are shown in Fig.10. As we can see, DoubleFusion fails to
track the motion of the performer’s arms and legs when they are occluded by
the body and ﬁnally generates unsatisfactory loop closure results. In contrast,
our system is able to track those motions under severe occlusions, generating
complete and plausible models with such challenging turning around motions.
The Number of IMUs. To better evaluate our contributions, we also make
experiments on the number of IMUs used in hybrid motion tracking. In Fig.11,
the performer wears the full set of Noitom Legacy suite containing 17 IMUs
attached on diﬀerent body parts and performs several challenging motion such
as leapfrogging, punching and so on. Regarding the tracking results with 17 IMUs
as the ground-truth, we can get an estimation of tracking errors using diﬀerent
sensor setups. In Fig.11, we present the average position error of joints using
diﬀerent numbers of IMUs. This experiment proves that using 8 IMUs (less than

14

Z. Zheng et al.

Fig. 10. Evaluation of the performance of our system on loop closure. We show the
results in diﬀerent frames. (a,d): Color reference. (b,e): The results reconstructed by
DoubleFusion. (c,f): The results generated by our system.

a half of the full set) with a single depth camera can achieve accurate tracking
while preserving the convenience for usage.

Fig. 11. Evaluation of the number of IMUs. (a): the curves of average position error
of joints under diﬀerent conﬁgurations. (b): illustration of the 4 IMU conﬁgurations.

8 Discussion

Conclusion In this paper, we have presented a practical and highly robust real-
time human performance capture system that can simultaneously reconstruct
challenging motions, detailed surface geometries and plausible inner body shapes
using a single depth camera and sparse IMUs. We believe the practicability of our
system enables light-weight, robust and real-time human performance capture,
which makes it possible for users to capture high-quality 4D performances even at
home. The real-time reconstructed results can be used in both AR/VR, gaming
and virtual try-on applications.
Limitations Our system cannot reconstruct very accurate surface mesh when
people wearing very wide cloth because the cloth deformations are too complex
for our sparse node-graph deformation model. Also, human-object interactions
are very challenging, using divide-and-conquer scheme may provide plausible
results. Although the IMUs we used are relatively small and easy to wear, it
may still limit body motions. However, as the IMUs are getting more and more
small and accurate, we believe the system setup can be even easier in the future.
Acknowledgement This work is supported by the National Key Foundation
for Exploring Scientiﬁc Instrument of China No.2013YQ140517; the National
NSF of China grant No.61522111, No.61531014, No.61233005, No.61722209 and
No.61331015. Hao Li was supportedby the ONR YIP grant N00014-17-S-FO14,
the CONIX Research Center, an SRC program sponsored by DARPA, the U.S.
ARL under contract number W911NF-14-D-0005, Adobe, and Sony.

HybridFusion

15

References

1. Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., Davis, J.: Scape:
Shape completion and animation of people. ACM Trans. Graph. 24(3), 408–416
(Jul 2005)

2. Baak, A., Helten, T., M¨uller, M., Pons-Moll, G., Rosenhahn, B., Seidel, H.P.:
Analyzing and evaluating markerless motion tracking using inertial sensors. In:
European Conference on Computer Vision (ECCV Workshops) (Sep 2010)

3. Bogo, F., Kanazawa, A., Lassner, C., Gehler, P., Romero, J., Black, M.J.: Keep it
SMPL: Automatic estimation of 3D human pose and shape from a single image.
In: Computer Vision – ECCV 2016. Lecture Notes in Computer Science, Springer
International Publishing (Oct 2016)

4. Bradley, D., Popa, T., Sheﬀer, A., Heidrich, W., Boubekeur, T.: Markerless garment

capture. In: ACM TOG. vol. 27, p. 99. ACM (2008)

5. Brox, T., Rosenhahn, B., Gall, J., Cremers, D.: Combined region and motion-based
3d tracking of rigid and articulated objects. IEEE TPAMI 32(3), 402–415 (2010)
6. Dou, M., Davidson, P., Fanello, S.R., Khamis, S., Kowdle, A., Rhemann, C.,
Tankovich, V., Izadi, S.: Motion2fusion: Real-time volumetric performance cap-
ture. ACM Trans. Graph. 36(6), 246:1–246:16 (Nov 2017)

7. Dou, M., Khamis, S., Degtyarev, Y., Davidson, P., Fanello, S.R., Kowdle, A., Es-
colano, S.O., Rhemann, C., Kim, D., Taylor, J., et al.: Fusion4d: real-time perfor-
mance capture of challenging scenes. ACM TOG 35(4), 114 (2016)

8. Gall, J., Stoll, C., De Aguiar, E., Theobalt, C., Rosenhahn, B., Seidel, H.P.: Motion
capture using joint skeleton tracking and surface estimation. In: CVPR. pp. 1746–
1753. IEEE (2009)

9. Guo, K., Xu, F., Wang, Y., Liu, Y., Dai, Q.: Robust non-rigid motion tracking and
surface reconstruction using l0 regularization. In: ICCV. pp. 3083–3091 (2015)
10. Guo, K., Xu, F., Yu, T., Liu, X., Dai, Q., Liu, Y.: Real-time geometry, albedo and
motion reconstruction using a single rgbd camera. ACM Transactions on Graphics
(TOG) (2017)

11. Helten, T., Muller, M., Seidel, H.P., Theobalt, C.: Real-time body tracking with
one depth camera and inertial sensors. In: The IEEE International Conference on
Computer Vision (ICCV) (December 2013)

12. Innmann, M., Zollh¨ofer, M., Nießner, M., Theobalt, C., Stamminger, M.: Vol-
umedeform: Real-time volumetric non-rigid reconstruction. In: ECCV (2016)
13. Joo, H., Simon, T., Sheikh, Y.: Total capture: A 3d deformation model for tracking

faces, hands, and bodies. In: CVPR. IEEE (2018)

14. Leroy, V., Franco, J.S., Boyer, E.: Multi-view dynamic shape reﬁnement using local

temporal integration. In: ICCV. IEEE (2017)

15. Li, H., Adams, B., Guibas, L.J., Pauly, M.: Robust single-view geometry and mo-

tion reconstruction. In: ACM TOG. vol. 28, p. 175. ACM (2009)

16. Liao, M., Zhang, Q., Wang, H., Yang, R., Gong, M.: Modeling deformable objects

from a single depth camera. In: ICCV (2009)

17. Liu, Y., Gall, J., Stoll, C., Dai, Q., Seidel, H., Theobalt, C.: Markerless motion
capture of multiple characters using multiview image segmentation. IEEE Trans.
Pattern Anal. Mach. Intell. 35(11), 2720–2735 (2013)

18. Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: A
skinned multi-person linear model. ACM Trans. Graphics (Proc. SIGGRAPH Asia)
34(6), 248:1–248:16 (Oct 2015)

16

Z. Zheng et al.

19. Lorensen, W.E., Cline, H.E.: Marching cubes: A high resolution 3d surface con-
struction algorithm. In: Proceedings of the 14th Annual Conference on Computer
Graphics and Interactive Techniques. pp. 163–169. SIGGRAPH ’87, ACM, New
York, NY, USA (1987)

20. Malleson, C., Volino, M., Gilbert, A., Trumble, M., Collomosse, J., Hilton, A.: Real-
time full-body motion capture from video and imus. In: 2017 Fifth International
Conference on 3D Vision (3DV) (2017)

21. von Marcard, T., Henschel, R., Black, M., Rosenhahn, B., Pons-Moll, G.: Recov-
ering accurate 3d human pose in the wild using imus and a moving camera. In:
European Conference on Computer Vision (sep 2018)

22. von Marcard, T., Pons-Moll, G., Rosenhahn, B.: Human pose estimation from
video and imus. Transactions on Pattern Analysis and Machine Intelligence PAMI
(Jan 2016)

23. Mitra, N.J., Fl¨ory, S., Ovsjanikov, M., Gelfand, N., Guibas, L.J., Pottmann, H.:

Dynamic geometry registration. In: SGP. pp. 173–182 (2007)

24. Mustafa, A., Kim, H., Guillemaut, J., Hilton, A.: General dynamic scene recon-
struction from multiple view video. In: 2015 IEEE International Conference on
Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015. pp. 900–908
(2015)

25. Newcombe, R.A., Fox, D., Seitz, S.M.: Dynamicfusion: Reconstruction and tracking

of non-rigid scenes in real-time. In: CVPR (June 2015)

26. Newcombe, R.A., Fox, D., Seitz, S.M.: Dynamicfusion: Reconstruction and tracking
of non-rigid scenes in real-time. In: The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) (June 2015)

27. Pekelny, Y., Gotsman, C.: Articulated object reconstruction and markerless motion
capture from depth video. In: CGF. vol. 27, pp. 399–408. Wiley Online Library
(2008)

28. Pons-Moll, G., Baak, A., Gall, J., Leal-Taixe, L., Mueller, M., Seidel, H.P., Rosen-
hahn, B.: Outdoor human motion capture using inverse kinematics and von mises-
ﬁsher sampling. In: IEEE International Conference on Computer Vision (ICCV).
pp. 1243–1250 (nov 2011)

29. Pons-Moll, G., Baak, A., Helten, T., M¨uller, M., Seidel, H.P., Rosenhahn, B.:
Multisensor-fusion for 3d full-body human motion capture. In: IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) (jun 2010)

30. Pons-Moll, G., Pujades, S., Hu, S., Black, M.: ClothCap: Seamless 4D clothing cap-
ture and retargeting. ACM Transactions on Graphics, (Proc. SIGGRAPH) 36(4)
(2017), two ﬁrst authors contributed equally

31. Pons-Moll, G., Romero, J., Mahmood, N., Black, M.J.: Dyna: A model of dynamic
human shape in motion. ACM Transactions on Graphics, (Proc. SIGGRAPH)
34(4), 120:1–120:14 (Aug 2015)

32. Slavcheva, M., Baust, M., Cremers, D., Ilic, S.: KillingFusion: Non-rigid 3D Re-
construction without Correspondences. In: IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) (2017)

33. Slavcheva, M., Baust, M., Ilic, S.: SobolevFusion: 3D Reconstruction of Scenes
Undergoing Free Non-rigid Motion. In: IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) (2018)

34. Slyper, R., Hodgins, J.: Action Capture with Accelerometers. In: Gross, M., James,
D. (eds.) Eurographics/SIGGRAPH Symposium on Computer Animation. The
Eurographics Association (2008)

35. Sumner, R.W., Schmid, J., Pauly, M.: Embedded deformation for shape manipu-
lation. In: SIGGRAPH. SIGGRAPH ’07, ACM, New York, NY, USA (2007)

HybridFusion

17

36. S¨ußmuth, J., Winter, M., Greiner, G.: Reconstructing animated meshes from time-
varying point clouds. In: CGF. vol. 27, pp. 1469–1476. Blackwell Publishing Ltd
(2008)

37. Tautges, J., Zinke, A., Kr¨uger, B., Baumann, J., Weber, A., Helten, T., M¨uller,
M., Seidel, H.P., Eberhardt, B.: Motion reconstruction using sparse accelerometer
data. ACM Trans. Graph. 30(3), 18:1–18:12 (May 2011)

38. Tevs, A., Berner, A., Wand, M., Ihrke, I., Bokeloh, M., Kerber, J., Seidel, H.P.:
Animation cartography-intrinsic reconstruction of shape and motion. ACM TOG
31(2), 12 (2012)

39. Tkach, A., Tagliasacchi, A., Remelli, E., Pauly, M., Fitzgibbon, A.: Online gener-
ative model personalization for hand tracking. ACM Trans. Graph. 36(6), 243:1–
243:11 (Nov 2017)

40. Vlasic, D., Adelsberger, R., Vannucci, G., Barnwell, J., Gross, M., Matusik, W.,
Popovi, J.: Practical motion capture in everyday surroundings. In: in Proc. SIG-
GRAPH 2007, ACM (2007)

41. Vlasic, D., Baran, I., Matusik, W., Popovi´c, J.: Articulated mesh animation from

multi-view silhouettes. In: ACM TOG. vol. 27, p. 97. ACM (2008)

42. von Marcard, T., Rosenhahn, B., Black, M., Pons-Moll, G.: Sparse inertial poser:
Automatic 3d human pose estimation from sparse imus. Computer Graphics Forum
36(2), Proceedings of the 38th Annual Conference of the European Association for
Computer Graphics (Eurographics) pp. 349–360 (2017)

43. Xu, W., Chatterjee, A., Zoll¨ofer, M., Rhodin, H., Mehta, D., Seidel, H.P., Theobalt,
C.: MonoPerfCap: Human Performance Capture from Monocular Video. ACM
TOG (2017)

44. Ye, G., Liu, Y., Hasler, N., Ji, X., Dai, Q., Theobalt, C.: Performance capture of

interacting characters with handheld kinects. In: ECCV (2012)

45. Yu, T., Guo, K., Xu, F., Dong, Y., Su, Z., Zhao, J., Li, J., Dai, Q., Liu, Y.:
Bodyfusion: Real-time capture of human motion and surface geometry using a
single depth camera. In: The IEEE International Conference on Computer Vision
(ICCV). ACM (October 2017)

46. Yu, T., Zheng, Z., Guo, K., Zhao, J., Dai, Q., Li, H., Pons-Moll, G., Liu, Y.: Dou-
blefusion: Real-time capture of human performance with inner body shape from a
depth sensor. In: IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) (2018)

47. Zollh¨ofer, M., Nießner, M., Izadi, S., Rehmann, C., Zach, C., Fisher, M., Wu, C.,
Fitzgibbon, A., Loop, C., Theobalt, C., et al.: Real-time non-rigid reconstruction
using an rgb-d camera. ACM TOG 33(4), 156 (2014)

48. Zou, D., Tan, P.: Coslam: Collaborative visual slam in dynamic environments.
IEEE Transactions on Pattern Analysis and Machine Intelligence 35(2), 354–366
(Feb 2013)

