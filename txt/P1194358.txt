8
1
0
2
 
b
e
F
 
6
1
 
 
]

V
C
.
s
c
[
 
 
3
v
1
4
0
4
0
.
7
0
7
1
:
v
i
X
r
a

Deep Learning with Topological Signatures

Christoph Hofer
Department of Computer Science
University of Salzburg, Austria
chofer@cosy.sbg.ac.at

Roland Kwitt
Department of Computer Science
University of Salzburg, Austria
Roland.Kwitt@sbg.ac.at

Marc Niethammer
UNC Chapel Hill, NC, USA
mn@cs.unc.edu

Andreas Uhl
Department of Computer Science
University of Salzburg, Austria
uhl@cosy.sbg.ac.at

Abstract

Inferring topological and geometrical information from data can offer an alternative
perspective on machine learning problems. Methods from topological data analysis,
e.g., persistent homology, enable us to obtain such information, typically in the form
of summary representations of topological features. However, such topological
signatures often come with an unusual structure (e.g., multisets of intervals) that is
highly impractical for most machine learning techniques. While many strategies
have been proposed to map these topological signatures into machine learning
compatible representations, they suffer from being agnostic to the target learning
task. In contrast, we propose a technique that enables us to input topological
signatures to deep neural networks and learn a task-optimal representation during
training. Our approach is realized as a novel input layer with favorable theoretical
properties. Classiﬁcation experiments on 2D object shapes and social network
graphs demonstrate the versatility of the approach and, in case of the latter, we
even outperform the state-of-the-art by a large margin.

1

Introduction

Methods from algebraic topology have only recently emerged in the machine learning community,
most prominently under the term topological data analysis (TDA) [7]. Since TDA enables us to
infer relevant topological and geometrical information from data, it can offer a novel and potentially
beneﬁcial perspective on various machine learning problems. Two compelling beneﬁts of TDA
are (1) its versatility, i.e., we are not restricted to any particular kind of data (such as images,
sensor measurements, time-series, graphs, etc.) and (2) its robustness to noise. Several works have
demonstrated that TDA can be beneﬁcial in a diverse set of problems, such as studying the manifold
of natural image patches [8], analyzing activity patterns of the visual cortex [28], classiﬁcation of 3D
surface meshes [27, 22], clustering [11], or recognition of 2D object shapes [29].
Currently, the most widely-used tool from TDA is persistent homology [15, 14]. Essentially1,
persistent homology allows us to track topological changes as we analyze data at multiple “scales”.
As the scale changes, topological features (such as connected components, holes, etc.) appear and
disappear. Persistent homology associates a lifespan to these features in the form of a birth and
a death time. The collection of (birth, death) tuples forms a multiset that can be visualized as a
persistence diagram or a barcode, also referred to as a topological signature of the data. However,
leveraging these signatures for learning purposes poses considerable challenges, mostly due to their

1We will make these concepts more concrete in Sec. 2.

Figure 1: Illustration of the proposed network input layer for topological signatures. Each signature, in the
form of a persistence diagram D ∈ D (left), is projected w.r.t. a collection of structure elements. The layer’s
+ is set a-priori and
learnable parameters θ are the locations µi and the scales σi of these elements; ν ∈ R
meant to discount the impact of points with low persistence (and, in many cases, of low discriminative power).
The layer output y is a concatenation of the projections. In this illustration, N = 2 and hence y = (y1, y2)(cid:62).

unusual structure as a multiset. While there exist suitable metrics to compare signatures (e.g., the
Wasserstein metric), they are highly impractical for learning, as they require solving optimal matching
problems.

Related work. In order to deal with these issues, several strategies have been proposed. In [2] for
instance, Adcock et al. use invariant theory to “coordinatize” the space of barcodes. This allows to
map barcodes to vectors of ﬁxed size which can then be fed to standard machine learning techniques,
such as support vector machines (SVMs). Alternatively, Adams et al. [1] map barcodes to so-called
persistence images which, upon discretization, can also be interpreted as vectors and used with
standard learning techniques. Along another line of research, Bubenik [6] proposes a mapping
of barcodes into a Banach space. This has been shown to be particularly viable in a statistical
context (see, e.g., [10]). The mapping outputs a representation referred to as a persistence landscape.
Interestingly, under a speciﬁc choice of parameters, barcodes are mapped into L2(R2) and the
inner-product in that space can be used to construct a valid kernel function. Similar, kernel-based
techniques, have also recently been studied by Reininghaus et al. [27], Kwitt et al. [20] and Kusano
et al. [19].

While all previously mentioned approaches retain certain stability properties of the original repre-
sentation with respect to common metrics in TDA (such as the Wasserstein or Bottleneck distances),
they also share one common drawback: the mapping of topological signatures to a representation that
is compatible with existing learning techniques is pre-deﬁned. Consequently, it is ﬁxed and therefore
agnostic to any speciﬁc learning task. This is clearly suboptimal, as the eminent success of deep
neural networks (e.g., [18, 17]) has shown that learning representations is a preferable approach.
Furthermore, techniques based on kernels [27, 20, 19] for instance, additionally suffer scalability
issues, as training typically scales poorly with the number of samples (e.g., roughly cubic in case of
kernel-SVMs). In the spirit of end-to-end training, we therefore aim for an approach that allows to
learn a task-optimal representation of topological signatures. We additionally remark that, e.g., Qi et
al. [25] or Ravanbakhsh et al. [26] have proposed architectures that can handle sets, but only with
ﬁxed size. In our context, this is impractical as the capability of handling sets with varying cardinality
is a requirement to handle persistent homology in a machine learning setting.

Contribution. To realize this idea, we advocate a novel input layer for deep neural networks that
takes a topological signature (in our case, a persistence diagram), and computes a parametrized
projection that can be learned during network training. Speciﬁcally, this layer is designed such that
its output is stable with respect to the 1-Wasserstein distance (similar to [27] or [1]). To demonstrate
the versatility of this approach, we present experiments on 2D object shape classiﬁcation and the
classiﬁcation of social network graphs. On the latter, we improve the state-of-the-art by a large
margin, clearly demonstrating the power of combining TDA with deep learning in this context.

2 Background

For space reasons, we only provide a brief overview of the concepts that are relevant to this work and
refer the reader to [16] or [14] for further details.

Homology. The key concept of homology theory is to study the properties of some object X by
means of (commutative) algebra. In particular, we assign to X a sequence of modules C0, C1, . . .

2

ker ∂n. A structure
which are connected by homomorphisms ∂n : Cn →
of this form is called a chain complex and by studying its homology groups Hn = ker ∂n/ im ∂n+1
we can derive properties of X.

Cn−1 such that im ∂n+1 ⊆

A prominent example of a homology theory is simplicial homology. Throughout this work, it is
the used homology theory and hence we will now concretize the already presented ideas. Let K
be a simplicial complex and Kn its n-skeleton. Then we set Cn(K) as the vector space gener-
ated (freely) by Kn over Z/2Z2. The connecting homomorphisms ∂n : Cn(K)
Cn−1(K) are
→
called boundary operators. For a simplex σ = [x0, . . . , xn]
Kn, we deﬁne them as ∂n(σ) =
(cid:80)n
i=0[x0, . . . , xi−1, xi+1, . . . , xn] and linearly extend this to Cn(K), i.e., ∂n((cid:80) σi) = (cid:80) ∂n(σi).
Persistent homology. Let K be a simplicial complex and (K i)m
i=0 a sequence of simplicial com-
K m = K. Then, (K i)m
i=0 is called a ﬁltration of K. If we
plexes such that
∅
use the extra information provided by the ﬁltration of K, we obtain the following sequence of chain
complexes (left),

⊆ · · · ⊆

= K 0

K 1

⊆

∈

n = Cn(K i
where C i
homology groups, deﬁned by

n) and ι denotes the inclusion. This then leads to the concept of persistent

H i,j

n/(im ∂j

n = ker ∂i
The ranks, βi,j
n , of these homology groups (i.e., the n-th persistent Betti numbers),
capture the number of homological features of dimensionality n (e.g., connected components for
n = 0, holes for n = 1, etc.) that persist from i to (at least) j. In fact, according to [14, Fundamental
Lemma of Persistent Homology], the quantities

n = rank H i,j

ker ∂i
n)

n+1 ∩

j .

for

≤

i

n = (βi,j−1
µi,j

n

βi,j
n )

(βi−1,j−1

n

βi−1,j
n

)

for

i < j

−

−

−

(1)

encode all the information about the persistent Betti numbers of dimension n.

Topological signatures. A typical way to obtain a ﬁltration of K is to consider sublevel sets of a
function f : C0(K)
R. This function can be easily lifted to higher-dimensional chain groups of
K by

→

f ([v0, . . . , vn]) = max
{
, we obtain (Ki)m
|

i
≤
i=0 by setting K0 =

f ([vi]) : 0

≤
∅

≤

f (C0(K))
|
m, where a1 <

, ai]) for
Given m =
< am is the sorted sequence of values of f (C0(K)). If we construct
1
i
a multiset such that, for i < j, the point (ai, aj) is inserted with multiplicity µi,j
n , we effectively
encode the persistent homology of dimension n w.r.t. the sublevel set ﬁltration induced by f . Upon
adding diagonal points with inﬁnite multiplicity, we obtain the following structure:

−∞

· · ·

≤

.

n
and Ki = f −1((

}

Deﬁnition 1 (Persistence diagram). Let ∆ =
diagonal R2
R2 : x0 = x1}
(x0, x1)
∆ =
{
∈
R2 : x1 > x0}
R2
(x0, x1)
∈
{

(cid:63) =

R2

x
{

∆ : mult(x) =

be the multiset of the
, where mult denotes the multiplicity function and let
, is a multiset of the form

∞}

∈

. A persistence diagram,

=

x : x

D

{

R

2
(cid:63)} ∪

∈

D
∆ .

We denote by D the set of all persistence diagrams of the form

∆

<

.

|D \

|

∞

For a given complex K of dimension nmax and a function f (of the discussed form), we can interpret
persistent homology as a mapping (K, f )
(
Di is the diagram of
D0, . . . ,
dimension i and nmax the dimension of K. We can additionally add a metric structure to the space of
persistence diagrams by introducing the notion of distances.

Dnmax−1), where

(cid:55)→

2Simplicial homology is not speciﬁc to Z/2Z, but it’s a typical choice, since it allows us to interpret n-chains

as sets of n-simplices.

3

E

(cid:33) 1

p

Deﬁnition 2 (Bottleneck, Wasserstein distance). For two persistence diagrams
their Bottleneck (w∞) and Wasserstein (wq

p) distances by

D

and

, we deﬁne

(cid:32)

(cid:88)

x∈D

.

D → E

w∞(

,

D

E

) = inf
η

sup
x∈D ||

x

−

η(x)

∞ and wq
p(
||

,

D

E

) = inf
η

x

||

−

η(x)

p
q
||

,

(2)

where p, q

N and the inﬁmum is taken over all bijections η :

∈

Essentially, this facilitates studying stability/continuity properties of topological signatures w.r.t.
metrics in the ﬁltration or complex space; we refer the reader to [12],[13], [9] for a selection of
important stability results.
Remark. By setting µi,∞
n −
referred to as essential. This change can be lifted to D by setting R2
x1 > x0}

, we extend Eq. (1) to features which never disappear, also
) :

. In Sec. 5, we will see that essential features can offer discriminative information.

(x0, x1)
{

n = βi,m

βi−1,m
n

∪ {∞}

(cid:63) =

(R

×

R

∈

3 A network layer for topological signatures

x

∈

D

D

{
such that points on R2

In this section, we introduce the proposed (parametrized) network layer for topological signatures
(in the form of persistence diagrams). The key idea is to take any
and deﬁne a projection w.r.t. a
collection (of ﬁxed size N ) of structure elements.
In the following, we set R+ :=
R : x > 0
0
R : x
, resp., and start by
}
}
∆ lie on the x-axis, see Fig. 1. The y-axis can then be
rotating points of
interpreted as the persistence of features. Formally, we let b0 and b1 be the unit vectors in directions
(1, 1)(cid:62) and (
).
x, b1(cid:105)
R
(cid:63) ∪
(cid:104)
−
∆ clock-wise by π/4. We will later see that this construction is beneﬁcial
This rotates points in R(cid:63) ∪
for a closer analysis of the layers’ properties. Similar to [27, 19], we choose exponential functions as
structure elements, but other choices are possible (see Lemma 1). Differently to [27, 19], however,
our structure elements are not at ﬁxed locations (i.e., one element per point in
), but their locations
and scales are learned during training.
Deﬁnition 3. Let µ = (µ0, µ1)(cid:62)

1, 1)(cid:62) and deﬁne a mapping ρ : R2

+
0 such that x

R+, σ = (σ0, σ1)

R+. We deﬁne

R+ and ν

x, b0(cid:105)

and R

+
0 :=

∆ →

x
{

R+

R2

R2

(
(cid:104)

(cid:55)→

≥

×

D

R

R

∈

,

×

∈

as follows:

∈

×
sµ,σ,ν : R

∈
R

R

+
0 →

×

e−σ2

0 (x0−µ0)2−σ2

1 (x1−µ1)2

,






0,

x1 ∈
, x1 ∈

[ν,

)

∞

(0, ν)

x1 = 0

sµ,σ,ν

(cid:0)(x0, x1)(cid:1) =

e−σ2

0 (x0−µ0)2−σ2

1 (ln(

x1
ν )ν+ν−µ1)2

A persistence diagram

D

is then projected w.r.t. sµ,σ,ν via
(cid:88)

Sµ,σ,ν : D

R,

→

D (cid:55)→

x∈D

sµ,σ,ν(ρ(x)) .

Remark. Note that sµ,σ,ν is continuous in x1 as
(cid:16) x
ν

x = lim
x→ν

lim
x1→0

lim
x→ν

ν + ν

and

ln

(cid:17)

sµ,σ,ν

(cid:0)(x0, x1)(cid:1) = 0 = sµ,σ,ν

(cid:0)(x0, 0)(cid:1)

and e(·) is continuous. Further, sµ,σ,ν is differentiable on R

R+, since

1 = lim
x→ν+

∂x1
∂x1

(x) and

lim
x→ν−

∂ (cid:0)ln (cid:0) x1

ν
∂x1

(x) = lim
x→ν−

= 1 .

ν
x

×
(cid:1) ν + ν(cid:1)

Also note that we use the log-transform in Eq. (4) to guarantee that sµ,σ,ν satisﬁes the conditions of
Lemma 1; this is, however, only one possible choice. Finally, given a collection of structure elements
Sµi,σi,ν, we combine them to form the output of the network layer.

(3)

(4)

4

Deﬁnition 4. Let N

N, θ = (µi, σi)N −1

∈

i=0 ∈
+
0 )N

(R

(cid:0)(R

×

D (cid:55)→

(R+

R+)

×
(cid:0)Sµi,σi,ν(

×
)(cid:1)N −1
i=0 .

D

θ,ν : D
S

→

as the concatenation of all N mappings deﬁned in Eq. (4).

R+)(cid:1)N and ν

R+. We deﬁne

∈

Importantly, a network layer implementing Def. 4 is trainable via backpropagation, as (1) sµi,σi,ν is
differentiable in µi, σi, (2) Sµi,σi,ν(
θ,ν is just a concatenation.

) is a ﬁnite sum of sµi,σi,ν and (3)

D

S

4 Theoretical properties

In this section, we demonstrate that the proposed layer is stable w.r.t. the 1-Wasserstein distance wq
1,
see Eq. (2). In fact, this claim will follow from a more general result, stating sufﬁcient conditions on
functions s : R2
Lemma 1. Let

0 such that a construction in the form of Eq. (3) is stable w.r.t. wq
1.

∆ →

(cid:63) ∪

R2

R

+

s : R

2
(cid:63) ∪

R

2
∆ →

+
0

R

(cid:107) · (cid:107)q and constant Ks

have the following properties:

(i) s is Lipschitz continuous w.r.t.
(ii) s(x(cid:1) = 0, for x

R2
∆

∈

Then, for two persistence diagrams

,

D

E ∈

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

x∈D

s(x)

−

D, it holds that
(cid:12)
(cid:12)
(cid:12)
s(y)
(cid:12)
(cid:12)
(cid:12)

Ks ·

≤

(cid:88)

y∈E

wq
1(

,

) .

D

E

(5)

Proof. see Appendix B

Remark. At this point, we want to clarify that Lemma 1 is not speciﬁc to sµ,σ,ν (e.g., as in Def. 3).
Rather, Lemma 1 yields sufﬁcient conditions to construct a w1-stable input layer. Our choice of
sµ,σ,ν is just a natural example that fulﬁls those requirements and, hence,
θ,ν is just one possible
representative of a whole family of input layers.

S

With the result of Lemma 1 in mind, we turn to the speciﬁc case of
S
properties w.r.t. wq
1. The following lemma is important in this context.
Lemma 2. sµ,σ,ν has absolutely bounded ﬁrst-order partial derivatives w.r.t. x0 and x1 on R

θ,ν and analyze its stability

R+.

×

Proof. see Appendix B

Theorem 1.

θ,ν is Lipschitz continuous with respect to wq
S

1 on D.

Proof. Lemma 2 immediately implies that sµ,σ,ν from Eq. (3) is Lipschitz continuous w.r.t
Consequently, s = sµ,σ,ν ◦
satisﬁed by construction. Hence, Sµ,σ,ν is Lipschitz continuous w.r.t. wq
Lipschitz in each coordinate and therefore Liptschitz continuous.

|| · ||q.
ρ satisﬁes property (i) from Lemma 1; property (ii) from Lemma 1 is
θ,ν is

1. Consequently,

S

Interestingly, the stability result of Theorem 1 is comparable to the stability results in [1] or [27]
(which are also w.r.t. wq
1 and in the setting of diagrams with ﬁnitely-many points). However, contrary
to previous works, if we would chop-off the input layer after network training, we would then have a
θ,ν of persistence diagrams that is speciﬁcally-tailored to the learning task on which the
mapping
S
network was trained.

5

Figure 2: Height function ﬁltration of a “clean” (left, green points) and a “noisy” (right, blue points) shape
along direction d = (0, −1)(cid:62). This example demonstrates the insensitivity of homology towards noise, as the
added noise only (1) slightly shifts the dominant points (upper left corner) and (2) produces additional points
close to the diagonal, which have little impact on the Wasserstein distance and the output of our layer.

5 Experiments

To demonstrate the versatility of the proposed approach, we present experiments with two totally
different types of data: (1) 2D shapes of objects, represented as binary images and (2) social network
graphs, given by their adjacency matrix. In both cases, the learning task is classiﬁcation. In each
experiment we ensured a balanced group size (per label) and used a 90/10 random training/test
split; all reported results are averaged over ﬁve runs with ﬁxed ν = 0.1. In practice, points in input
diagrams were thresholded at 0.01 for computational reasons. Additionally, we conducted a reference
experiment on all datasets using simple vectorization (see Sec. 5.3) of the persistence diagrams in
combination with a linear SVM.
Implementation. All experiments were implemented in PyTorch3, using DIPHA4 and Perseus [23].
Source code is publicly-available at https://github.com/c-hofer/nips2017.

5.1 Classiﬁcation of 2D object shapes

We apply persistent homology combined with our proposed input layer to two different datasets of
binary 2D object shapes: (1) the Animal dataset, introduced in [3] which consists of 20 different
animal classes, 100 samples each; (2) the MPEG-7 dataset which consists of 70 classes of different
object/animal contours, 20 samples each (see [21] for more details).

Filtration. The requirements to use persistent homology on 2D shapes are twofold: First, we need
to assign a simplicial complex to each shape; second, we need to appropriately ﬁltrate the complex.
While, in principle, we could analyze contour features, such as curvature, and choose a sublevel set
ﬁltration based on that, such a strategy requires substantial preprocessing of the discrete data (e.g.,
smoothing). Instead, we choose to work with the raw pixel data and leverage the persistent homology
transform, introduced by Turner et al. [29]. The ﬁltration in that case is based on sublevel sets of
the height function, computed from multiple directions (see Fig. 2). Practically, this means that we
directly construct a simplicial complex from the binary image. We set K0 as the set of all pixels
which are contained in the object. Then, a 1-simplex [p0, p1] is in the 1-skeleton K1 iff p0 and p1
are 4–neighbors on the pixel grid. To ﬁltrate the constructed complex, we deﬁne by b the barycenter
of the object and with r the radius of its bounding circle around b. Finally, we deﬁne, for [p]
K0
and d
. Function values are lifted to K1 by
b, d
(cid:105)
taking the maximum, cf. Sec. 2. Finally, let di be the 32 equidistantly distributed directions in S1,
Di)32
starting from (1, 0). For each shape, we get a vector of persistence diagrams (
Di is the
0-th diagram obtained by ﬁltration along di. As most objects do not differ in homology groups of
higher dimensions (> 0), we did not use the corresponding persistence diagrams.

S1, the ﬁltration function by f ([p]) = 1/r

i=1 where

· (cid:104)

−

∈

∈

p

Network architecture. While the full network is listed in the supplementary material (Fig. 6), the
key architectural choices are: 32 independent input branches, i.e., one for each ﬁltration direction.
Further, the i-th branch gets, as input, the vector of persistence diagrams from directions di−1, di
and di+1. This is a straightforward approach to capture dependencies among the ﬁltration directions.
We use cross-entropy loss to train the network for 400 epochs, using stochastic gradient descent
(SGD) with mini-batches of size 128 and an initial learning rate of 0.1 (halved every 25-th epoch).

3https://github.com/pytorch/pytorch
4https://bitbucket.org/dipha/dipha

6

MPEG-7

Animal

‡Skeleton paths
‡Class segment sets
†ICS
†BCF

Ours

86.7
90.9
96.6
97.2

91.8

67.9
69.7
78.4
83.4

69.5

Figure 3: Left: some examples from the MPEG-7 (bottom) and Animal (top) datasets. Right: Classiﬁcation
results, compared to the two best (†) and two worst (‡) results reported in [30].

Results. Fig. 3 shows a selection of 2D object shapes from both datasets, together with the obtained
classiﬁcation results. We list the two best (
) results as reported in [30]. While,
†
on the one hand, using topological signatures is below the state-of-the-art, the proposed architecture
is still better than other approaches that are speciﬁcally tailored to the problem. Most notably, our
approach does not require any speciﬁc data preprocessing, whereas all other competitors listed in
Fig. 3 require, e.g., some sort of contour extraction. Furthermore, the proposed architecture readily
S2. Fig. 4 (Right) shows an exemplary
generalizes to 3D with the only difference that in this case di ∈
visualization of the position of the learned structure elements for the Animal dataset.

) and two worst (

‡

5.2 Classiﬁcation of social network graphs

In this experiment, we consider the problem of graph classiﬁcation, where vertices are unlabeled
= (V, E), where V denotes the set of
and edges are undirected. That is, a graph
vertices and E denotes the set of edges. We evaluate our approach on the challenging problem of
social network classiﬁcation, using the two largest benchmark datasets from [31], i.e., reddit-5k
(5 classes, 5k graphs) and reddit-12k (11 classes,
12k graphs). Each sample in these datasets
represents a discussion graph and the classes indicate subreddits (e.g., worldnews, video, etc.).

is given by

≈

G

G

V

[v]

= (V, E) is straightforward: we set
Filtration. The construction of a simplicial complex from
K0 =
. We choose a very simple ﬁltration based on
}
the vertex degree, i.e., the number of incident edges to a vertex v
V . Hence, for [v0]
K0 we get
f ([v0]) = deg(v0)/ maxv∈V deg(v) and again lift f to K1 by taking the maximum. Note that chain
groups are trivial for dimension > 1, hence, all features in dimension 1 are essential.

v0, v1} ∈
{

[v0, v1] :
{

and K1 =

E

∈

∈

∈

G

{

}

Network architecture. Our network has four input branches: two for each dimension (0 and 1) of
the homological features, split into essential and non-essential ones, see Sec. 2. We train the network
for 500 epochs using SGD and cross-entropy loss with an initial learning rate of 0.1 (reddit_5k), or
0.4 (reddit_12k). The full network architecture is listed in the supplementary material (Fig. 7).

Results. Fig. 5 (right) compares our proposed strategy to state-of-the-art approaches from the
literature. In particular, we compare against (1) the graphlet kernel (GK) and deep graphlet kernel
(DGK) results from [31], (2) the Patchy-SAN (PSCN) results from [24] and (3) a recently reported
graph-feature + random forest approach (RF) from [4]. As we can see, using topological signatures
in our proposed setting considerably outperforms the current state-of-the-art on both datasets. This is
an interesting observation, as PSCN [24] for instance, also relies on node degrees and an extension of
the convolution operation to graphs. Further, the results reveal that including essential features is key
to these improvements.

5.3 Vectorization of persistence diagrams

D

we calculate its persistence, i.e., d

Here, we brieﬂy present a reference experiment we conducted following Bendich et al. [5]. The idea
is to directly use the persistence diagrams as features via vectorization. For each point (b, d) in a
persistence diagram
b. We then sort the calculated persistences
by magnitude from high to low and take the ﬁrst N values. Hence, we get, for each persistence
diagram, a vector of dimension N (if
< N , we pad with zero). We used this technique
on all four data sets. As can be seen from the results in Table 4 (averaged over 10 cross-validation
runs), vectorization performs poorly on MPEG-7 and Animal but can lead to competitive rates on
reddit-5k and reddit-12k. Nevertheless, the obtained performance is considerably inferior to our
proposed approach.

|D \

∆

−

|

7

N

MPEG-7
Animal
reddit-5k
reddit-12k

5

81.8
48.8
37.1
24.2

10

82.3
50.0
38.2
24.6

20

79.7
46.2
39.7
27.9

40

74.5
42.4
42.1
29.8

80

68.2
39.3
43.8
31.5

160

64.4
36.0
45.2
31.6

Ours

91.8
69.5
54.5
44.5

N ) persistence diagrams
Figure 4: Left: Classiﬁcation accuracies for a linear SVM trained on vectorized (in R
(see Sec. 5.3). Right: Exemplary visualization of the learned structure elements (in 0-th dimension) for the
Animal dataset and ﬁltration direction d = (−1, 0)(cid:62). Centers of the learned elements are marked in blue.

reddit-5k

reddit-12k

GK [31]
DGK [31]
PSCN [24]
RF [4]

Ours (w/o essential)
Ours (w/ essential)

41.0
41.3
49.1
50.9

49.1
54.5

31.8
32.2
41.3
42.7

38.5
44.5

Figure 5: Left: Illustration of graph ﬁltration by vertex degree, i.e., f ≡ deg (for different choices of ai, see
Sec. 2). Right: Classiﬁcation results as reported in [31] for GK and DGK, Patchy-SAN (PSCN) as reported in
.
[24] and feature-based random-forest (RF) classiﬁcation from [4].

Finally, we remark that in both experiments, tests with the kernel of [27] turned out to be computa-
tionally impractical, (1) on shape data due to the need to evaluate the kernel for all ﬁltration directions
and (2) on graphs due the large number of samples and the number of points in each diagram.

6 Discussion

We have presented, to the best of our knowledge, the ﬁrst approach towards learning task-optimal
stable representations of topological signatures, in our case persistence diagrams. Our particular
realization of this idea, i.e., as an input layer to deep neural networks, not only enables us to learn with
topological signatures, but also to use them as additional (and potentially complementary) inputs to
existing deep architectures. From a theoretical point of view, we remark that the presented structure
elements are not restricted to exponential functions, so long as the conditions of Lemma 1 are met.
One drawback of the proposed approach, however, is the artiﬁcial bending of the persistence axis (see
Fig. 1) by a logarithmic transformation; in fact, other strategies might be possible and better suited
in certain situations. A detailed investigation of this issue is left for future work. From a practical
perspective, it is also worth pointing out that, in principle, the proposed layer could be used to handle
any kind of input that comes in the form of multisets (of Rn), whereas previous works only allow
to handle sets of ﬁxed size (see Sec. 1). In summary, we argue that our experiments show strong
evidence that topological features of data can be beneﬁcial in many learning tasks, not necessarily to
replace existing inputs, but rather as a complementary source of discriminative information.

8

A Technical results

Lemma 3. Let α

i)

lim
x→0

ln(x)
x

·

R+, β

R, γ

∈
∈
∈
e−α(ln(x)γ+β)2 = 0

R+. We have

ii)

lim
x→0

1
x ·

e−α(ln(x)γ+β)2 = 0 .

Proof. We omit the proof for brevity (see supplementary material for details), but remark that only
(i) needs to be shown as (ii) follows immediately.

B Proofs

D \

E \

E0 =
D

Proof of Lemma 1. Let ϕ be a bijection between

) and let
and
∆. To show the result of Eq. (5), we consider the following decomposition:

which realizes wq
1(

D

D

E

E

,

∆,

D0 =

= ϕ−1(
= (ϕ−1(
(cid:124)

E0)
∪
E0)
(cid:123)(cid:122)
A

ϕ−1(∆)
∆)
(cid:125)

∪

E0)
(cid:123)(cid:122)
B
Except for the term D, all sets are ﬁnite. In fact, ϕ realizes the Wasserstein distance wq
ϕ(cid:12)
= id. Therefore, s(x) = s(ϕ(x)) = 0 for x
(cid:12)D
∈
in the summation and it sufﬁces to consider E = A

C. It follows that

(ϕ−1(∆)
(cid:124)
(cid:123)(cid:122)
C

(ϕ−1(∆)
(cid:124)
(cid:123)(cid:122)
D

(ϕ−1(
(cid:124)

∆)
(cid:125)

∆)
(cid:125)

⊂

∪

∩

∩

∪

\

\

∆)
(cid:125)

1 which implies
∆. Consequently, we can ignore D

(6)

∪

D since D
B
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∪
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:88)

x∈E

(cid:88)

−

x∈D

(cid:88)

x∈E

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

s(x)

s(ϕ(x))

s(x)

s(ϕ(x))

(cid:88)

(cid:12)
(cid:12)
(cid:12)
s(ϕ(x))
(cid:12)
(cid:12) ≤
||q = Ks ·

ϕ(x)

x∈E

−

s(x)

(cid:88)

x∈E

−

x

||

(cid:88)

x∈D

s(x)
|

−

s(ϕ(x))

|

x∈D

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Ks ·

(cid:88)

x∈E

=

=

≤

x

||

−

ϕ(x)

||q = Ks ·

wq
1(

,

) .

D

E

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
x∈D

s(x)

(cid:88)

y∈E

−

(cid:12)
(cid:12)
(cid:12)
s(y)
(cid:12)
(cid:12)
(cid:12)

Proof of Lemma 2. Since sµ,σ,ν is deﬁned differently for x1 ∈
distinguish these two cases. In the following x0 ∈
(1) x1 ∈

): The partial derivative w.r.t. xi is given as

R.

[ν,

∞

[ν,

) and x1 ∈

∞

(0, ν), we need to

(cid:18) ∂
∂xi

(cid:19)

sµ,σ,ν

(x0, x1) = C

i (xi−µi)2(cid:19)

e−σ2

(x0, x1)

·

(cid:18) ∂
∂xi
e−σ2

i (xi−µi)2

= C

(
·
−
) which is not dependent on xi. For all cases, i.e., x0 → ∞

i )(xi −

µi) ,

·

2σ2

, x0 →

(7)

where C is just the part of exp(
·

, it holds that Eq. (7)

0.

→

and x1 → ∞

−∞
(2) x1 ∈
behaviour for x0 → ∞
(cid:19)
(cid:18) ∂
∂x1

sµ,σ,ν

(0, ν): The partial derivative w.r.t. x0 is similar to Eq. (7) with the same asymptotic

and x0 → −∞
(x0, x1) = C

. However, for the partial derivative w.r.t. x1 we get
(cid:18) ∂
∂x1
e( ... )

ν )ν+ν−µ1)2 (cid:19)

(x0, x1)

e−σ2

ν + ν

1 (ln(

ln

(cid:17)

(cid:16)

(cid:17)

x1

µ1

2σ2
1)

ν
x1

·

·

·

= C

(
−

·

·
(cid:16) x1
ν

(cid:17)

(cid:18)

ln

·

(cid:123)(cid:122)
(a)

(cid:16) x1
ν
ν
x1

·

(cid:19)

(cid:125)

= C (cid:48)

e( ... )

(cid:16)

·

(cid:124)

−

−

+(ν

µ1)

e( ... )

(cid:17)

.

1
x1
(cid:125)

·

(cid:124)

·
(cid:123)(cid:122)
(b)

(8)

As x1 →
Eq. (8)
→
we conclude that they are absolutely bounded.

0, we can invoke Lemma 4 (i) to handle (a) and Lemma 4 (ii) to handle (b); conclusively,
0. As the partial derivatives w.r.t. xi are continuous and their limits are 0 on R, R+, resp.,

9

References

[1] H. Adams, T. Emerson, M. Kirby, R. Neville, C. Peterson, P. Shipman, S. Chepushtanova, E. Hanson,
F. Motta, and L. Ziegelmeier. Persistence images: A stable vector representation of persistent homology.
JMLR, 18(8):1–35, 2017.

[2] A. Adcock, E. Carlsson, and G. Carlsson. The ring of algebraic functions on persistence bar codes. CoRR,

2013. https://arxiv.org/abs/1304.0530.

[3] X. Bai, W. Liu, and Z. Tu. Integrating contour and skeleton for shape classiﬁcation. In ICCV Workshops,

2009.

[4] I. Barnett, N. Malik, M.L. Kuijjer, P.J. Mucha, and J.-P. Onnela. Feature-based classiﬁcation of networks.

CoRR, 2016. https://arxiv.org/abs/1610.05868.

[5] P. Bendich, J.S. Marron, E. Miller, A. Pieloch, and S. Skwerer. Persistent homology analysis of brain artery

trees. Ann. Appl. Stat, 10(2), 2016.

[6] P. Bubenik. Statistical topological data analysis using persistence landscapes. JMLR, 16(1):77–102, 2015.

[7] G. Carlsson. Topology and data. Bull. Amer. Math. Soc., 46:255–308, 2009.

[8] G. Carlsson, T. Ishkhanov, V. de Silva, and A. Zomorodian. On the local behavior of spaces of natural

images. IJCV, 76:1–12, 2008.

[9] F. Chazal, D. Cohen-Steiner, L. J. Guibas, F. Mémoli, and S. Y. Oudot. Gromov-Hausdorff stable signatures

for shapes using persistence. Comput. Graph. Forum, 28(5):1393–1403, 2009.

[10] F. Chazal, B.T. Fasy, F. Lecci, A. Rinaldo, and L. Wassermann. Stochastic convergence of persistence

landscapes and silhouettes. JoCG, 6(2):140–161, 2014.

[11] F. Chazal, L.J. Guibas, S.Y. Oudot, and P. Skraba. Persistence-based clustering in Riemannian manifolds.

[12] D. Cohen-Steiner, H. Edelsbrunner, and J. Harer. Stability of persistence diagrams. Discrete Comput.

J. ACM, 60(6):41–79, 2013.

Geom., 37(1):103–120, 2007.

[13] D. Cohen-Steiner, H. Edelsbrunner, J. Harer, and Y. Mileyko. Lipschitz functions have Lp-stable persistence.

Found. Comput. Math., 10(2):127–139, 2010.

[14] H. Edelsbrunner and J. L. Harer. Computational Topology : An Introduction. American Mathematical

Society, 2010.

[15] H. Edelsbrunner, D. Letcher, and A. Zomorodian. Topological persistence and simpliﬁcation. Discrete

Comput. Geom., 28(4):511–533, 2002.

[16] A. Hatcher. Algebraic Topology. Cambridge University Press, Cambridge, 2002.

[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.

[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural

networks. In NIPS, 2012.

analysis. In ICML, 2016.

perspective. In NIPS, 2015.

contour. In CVPR, 2000.

[19] G. Kusano, K. Fukumizu, and Y. Hiraoka. Persistence weighted Gaussian kernel for topological data

[20] R. Kwitt, S. Huber, M. Niethammer, W. Lin, and U. Bauer. Statistical topological data analysis - a kernel

[21] L. Latecki, R. Lakamper, and T. Eckhardt. Shape descriptors for non-rigid shapes with a single closed

[22] C. Li, M. Ovsjanikov, and F. Chazal. Persistence-based structural recognition. In CVPR, 2014.

[23] K. Mischaikow and V. Nanda. Morse theory for ﬁltrations and efﬁcient computation of persistent homology.

Discrete Comput. Geom., 50(2):330–353, 2013.

[24] M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. In ICML,

[25] C.R. Qi, H. Su, K. Mo, and L.J. Guibas. PointNet: Deep learning on point sets for 3D classiﬁcation and

2016.

segmentation. In CVPR, 2017.

[26] S. Ravanbakhsh, S. Schneider, and B. Póczos. Deep learning with sets and point clouds. In ICLR, 2017.

[27] R. Reininghaus, U. Bauer, S. Huber, and R. Kwitt. A stable multi-scale kernel for topological machine

learning. In CVPR, 2015.

[28] G. Singh, F. Memoli, T. Ishkhanov, G. Sapiro, G. Carlsson, and D.L. Ringach. Topological analysis of

population activity in visual cortex. J. Vis., 8(8), 2008.

10

[29] K. Turner, S. Mukherjee, and D. M. Boyer. Persistent homology transform for modeling shapes and

[30] X. Wang, B. Feng, X. Bai, W. Liu, and L.J. Latecki. Bag of contour fragments for robust shape classiﬁcation.

surfaces. Inf. Inference, 3(4):310–344, 2014.

Pattern Recognit., 47(6):2116–2125, 2014.

[31] P. Yanardag and S.V.N. Vishwanathan. Deep graph kernels. In KDD, 2015.

This supplementary material contains technical details that were left-out in the original submission
for brevity. When necessary, we refer to the submitted manuscript.

C Additional proofs

In the manuscript, we omitted the proof for the following technical lemma. For completeness, the
lemma is repeated and its proof is given below.
R+, β
Lemma 4. Let α

R+. We have

R and γ

∈

∈
e−α(ln(x)γ+β)2 = 0

∈

·

e−α(ln(x)γ+β)2 = 0 .

(i) lim
x→0

ln(x)
x

(ii) lim
x→0

1
x ·

Proof. We only need to prove the ﬁrst statement, as the second follows immediately. Hence, consider

lim
x→0

ln(x)
x

·

e−α(ln(x)γ+β)2

ln(x)

e− ln(x)

e−α(ln(x)γ+β)2

= lim
x→0

= lim
x→0

ln(x)

= lim
x→0

ln(x)

·

·

·

·

e−α(ln(x)γ+β)2−ln(x)
eα(ln(x)γ+β)2+ln(x)(cid:17)−1

(cid:16)

(∗)
= lim
x→0

∂
∂x

ln(x)

(cid:18) ∂
∂x

·

eα(ln(x)γ+β)2+ln(x)

(cid:19)−1

(cid:18)

(cid:18)

eα(ln(x)γ+β)2+ln(x)

1
x ·
(cid:16)
eα(ln(x)γ+β)2+ln(x)

·

2α(ln(x)γ + β)

(cid:19)(cid:19)−1

γ
x

+

1
x
(cid:17)−1

(2α(ln(x)γ + β)γ + 1)

·

= lim
x→0

= lim
x→0

= 0

).
where we use de l’Hôpital’s rule in (
∗

D Network architectures

2D object shape classiﬁcation. Fig. 6 illustrates the network architecture used for 2D object shape
classiﬁcation in [Manuscript, Sec. 5.1]. Note that the persistence diagrams from three consecutive
ﬁltration directions di share one input layer. As we use 32 directions, we have 32 input branches.
3 and a stride of 1. The max-pooling
The convolution operation operates with kernels of size 1
operates along the ﬁlter dimension. For better readability, we have added the output size of certain
layers. We train with the network with stochastic gradient descent (SGD) and a mini-batch size of
128 for 300 epochs. Every 20th epoch, the learning rate (initially set to 0.1) is halved.

×

×

1

11

Graph classiﬁcation. Fig. 7 illustrates the network architecture used for graph classiﬁcation in
Sec. 5.2. In detail, we have 3 input branches: ﬁrst, we split 0-dimensional features into essential
and non-essential ones; second, since there are only essential features in dimension 1 (see Sec. 5.2,
Filtration) we do not need a branch for non-essential features. We train the network using SGD with
mini-batches of size 128 for 300 epochs. The initial learning rate is set to 0.1 (reddit_5k) and 0.4
(reddit_12k), resp., and halved every 20th epochs.

D.1 Technical handling of essential features

In case of of 2D object shapes, the death times of essential features are mapped to the max. ﬁltration
value and kept in the original persistence diagrams. In fact, for Animal and MPEG-7, there is always
only one connected component and consequently only one essential feature in dimension 0 (i.e., it
does not make sense to handle this one point in a separate input branch).

In case of social network graphs, essential features are mapped to the real line (using their birth time)
and handled in separate input branches (see Fig. 7) with 1D structure elements. This is in contrast to
the 2D object shape experiments, as we might have many essential features (in dimensions 0 and 1)
that require handling in separate input branches.

12

Figure 6: 2D object shape classiﬁcation network architecture.

13

Figure 7: Graph classiﬁcation network architecture.

14

8
1
0
2
 
b
e
F
 
6
1
 
 
]

V
C
.
s
c
[
 
 
3
v
1
4
0
4
0
.
7
0
7
1
:
v
i
X
r
a

Deep Learning with Topological Signatures

Christoph Hofer
Department of Computer Science
University of Salzburg, Austria
chofer@cosy.sbg.ac.at

Roland Kwitt
Department of Computer Science
University of Salzburg, Austria
Roland.Kwitt@sbg.ac.at

Marc Niethammer
UNC Chapel Hill, NC, USA
mn@cs.unc.edu

Andreas Uhl
Department of Computer Science
University of Salzburg, Austria
uhl@cosy.sbg.ac.at

Abstract

Inferring topological and geometrical information from data can offer an alternative
perspective on machine learning problems. Methods from topological data analysis,
e.g., persistent homology, enable us to obtain such information, typically in the form
of summary representations of topological features. However, such topological
signatures often come with an unusual structure (e.g., multisets of intervals) that is
highly impractical for most machine learning techniques. While many strategies
have been proposed to map these topological signatures into machine learning
compatible representations, they suffer from being agnostic to the target learning
task. In contrast, we propose a technique that enables us to input topological
signatures to deep neural networks and learn a task-optimal representation during
training. Our approach is realized as a novel input layer with favorable theoretical
properties. Classiﬁcation experiments on 2D object shapes and social network
graphs demonstrate the versatility of the approach and, in case of the latter, we
even outperform the state-of-the-art by a large margin.

1

Introduction

Methods from algebraic topology have only recently emerged in the machine learning community,
most prominently under the term topological data analysis (TDA) [7]. Since TDA enables us to
infer relevant topological and geometrical information from data, it can offer a novel and potentially
beneﬁcial perspective on various machine learning problems. Two compelling beneﬁts of TDA
are (1) its versatility, i.e., we are not restricted to any particular kind of data (such as images,
sensor measurements, time-series, graphs, etc.) and (2) its robustness to noise. Several works have
demonstrated that TDA can be beneﬁcial in a diverse set of problems, such as studying the manifold
of natural image patches [8], analyzing activity patterns of the visual cortex [28], classiﬁcation of 3D
surface meshes [27, 22], clustering [11], or recognition of 2D object shapes [29].
Currently, the most widely-used tool from TDA is persistent homology [15, 14]. Essentially1,
persistent homology allows us to track topological changes as we analyze data at multiple “scales”.
As the scale changes, topological features (such as connected components, holes, etc.) appear and
disappear. Persistent homology associates a lifespan to these features in the form of a birth and
a death time. The collection of (birth, death) tuples forms a multiset that can be visualized as a
persistence diagram or a barcode, also referred to as a topological signature of the data. However,
leveraging these signatures for learning purposes poses considerable challenges, mostly due to their

1We will make these concepts more concrete in Sec. 2.

Figure 1: Illustration of the proposed network input layer for topological signatures. Each signature, in the
form of a persistence diagram D ∈ D (left), is projected w.r.t. a collection of structure elements. The layer’s
+ is set a-priori and
learnable parameters θ are the locations µi and the scales σi of these elements; ν ∈ R
meant to discount the impact of points with low persistence (and, in many cases, of low discriminative power).
The layer output y is a concatenation of the projections. In this illustration, N = 2 and hence y = (y1, y2)(cid:62).

unusual structure as a multiset. While there exist suitable metrics to compare signatures (e.g., the
Wasserstein metric), they are highly impractical for learning, as they require solving optimal matching
problems.

Related work. In order to deal with these issues, several strategies have been proposed. In [2] for
instance, Adcock et al. use invariant theory to “coordinatize” the space of barcodes. This allows to
map barcodes to vectors of ﬁxed size which can then be fed to standard machine learning techniques,
such as support vector machines (SVMs). Alternatively, Adams et al. [1] map barcodes to so-called
persistence images which, upon discretization, can also be interpreted as vectors and used with
standard learning techniques. Along another line of research, Bubenik [6] proposes a mapping
of barcodes into a Banach space. This has been shown to be particularly viable in a statistical
context (see, e.g., [10]). The mapping outputs a representation referred to as a persistence landscape.
Interestingly, under a speciﬁc choice of parameters, barcodes are mapped into L2(R2) and the
inner-product in that space can be used to construct a valid kernel function. Similar, kernel-based
techniques, have also recently been studied by Reininghaus et al. [27], Kwitt et al. [20] and Kusano
et al. [19].

While all previously mentioned approaches retain certain stability properties of the original repre-
sentation with respect to common metrics in TDA (such as the Wasserstein or Bottleneck distances),
they also share one common drawback: the mapping of topological signatures to a representation that
is compatible with existing learning techniques is pre-deﬁned. Consequently, it is ﬁxed and therefore
agnostic to any speciﬁc learning task. This is clearly suboptimal, as the eminent success of deep
neural networks (e.g., [18, 17]) has shown that learning representations is a preferable approach.
Furthermore, techniques based on kernels [27, 20, 19] for instance, additionally suffer scalability
issues, as training typically scales poorly with the number of samples (e.g., roughly cubic in case of
kernel-SVMs). In the spirit of end-to-end training, we therefore aim for an approach that allows to
learn a task-optimal representation of topological signatures. We additionally remark that, e.g., Qi et
al. [25] or Ravanbakhsh et al. [26] have proposed architectures that can handle sets, but only with
ﬁxed size. In our context, this is impractical as the capability of handling sets with varying cardinality
is a requirement to handle persistent homology in a machine learning setting.

Contribution. To realize this idea, we advocate a novel input layer for deep neural networks that
takes a topological signature (in our case, a persistence diagram), and computes a parametrized
projection that can be learned during network training. Speciﬁcally, this layer is designed such that
its output is stable with respect to the 1-Wasserstein distance (similar to [27] or [1]). To demonstrate
the versatility of this approach, we present experiments on 2D object shape classiﬁcation and the
classiﬁcation of social network graphs. On the latter, we improve the state-of-the-art by a large
margin, clearly demonstrating the power of combining TDA with deep learning in this context.

2 Background

For space reasons, we only provide a brief overview of the concepts that are relevant to this work and
refer the reader to [16] or [14] for further details.

Homology. The key concept of homology theory is to study the properties of some object X by
means of (commutative) algebra. In particular, we assign to X a sequence of modules C0, C1, . . .

2

ker ∂n. A structure
which are connected by homomorphisms ∂n : Cn →
of this form is called a chain complex and by studying its homology groups Hn = ker ∂n/ im ∂n+1
we can derive properties of X.

Cn−1 such that im ∂n+1 ⊆

A prominent example of a homology theory is simplicial homology. Throughout this work, it is
the used homology theory and hence we will now concretize the already presented ideas. Let K
be a simplicial complex and Kn its n-skeleton. Then we set Cn(K) as the vector space gener-
ated (freely) by Kn over Z/2Z2. The connecting homomorphisms ∂n : Cn(K)
Cn−1(K) are
→
called boundary operators. For a simplex σ = [x0, . . . , xn]
Kn, we deﬁne them as ∂n(σ) =
(cid:80)n
i=0[x0, . . . , xi−1, xi+1, . . . , xn] and linearly extend this to Cn(K), i.e., ∂n((cid:80) σi) = (cid:80) ∂n(σi).
Persistent homology. Let K be a simplicial complex and (K i)m
i=0 a sequence of simplicial com-
K m = K. Then, (K i)m
i=0 is called a ﬁltration of K. If we
plexes such that
∅
use the extra information provided by the ﬁltration of K, we obtain the following sequence of chain
complexes (left),

⊆ · · · ⊆

= K 0

K 1

⊆

∈

n = Cn(K i
where C i
homology groups, deﬁned by

n) and ι denotes the inclusion. This then leads to the concept of persistent

H i,j

n/(im ∂j

n = ker ∂i
The ranks, βi,j
n , of these homology groups (i.e., the n-th persistent Betti numbers),
capture the number of homological features of dimensionality n (e.g., connected components for
n = 0, holes for n = 1, etc.) that persist from i to (at least) j. In fact, according to [14, Fundamental
Lemma of Persistent Homology], the quantities

n = rank H i,j

ker ∂i
n)

n+1 ∩

j .

for

≤

i

n = (βi,j−1
µi,j

n

βi,j
n )

(βi−1,j−1

n

βi−1,j
n

)

for

i < j

−

−

−

(1)

encode all the information about the persistent Betti numbers of dimension n.

Topological signatures. A typical way to obtain a ﬁltration of K is to consider sublevel sets of a
function f : C0(K)
R. This function can be easily lifted to higher-dimensional chain groups of
K by

→

f ([v0, . . . , vn]) = max
{
, we obtain (Ki)m
|

i
≤
i=0 by setting K0 =

f ([vi]) : 0

≤
∅

≤

f (C0(K))
|
m, where a1 <

, ai]) for
Given m =
< am is the sorted sequence of values of f (C0(K)). If we construct
1
i
a multiset such that, for i < j, the point (ai, aj) is inserted with multiplicity µi,j
n , we effectively
encode the persistent homology of dimension n w.r.t. the sublevel set ﬁltration induced by f . Upon
adding diagonal points with inﬁnite multiplicity, we obtain the following structure:

−∞

· · ·

≤

.

n
and Ki = f −1((

}

Deﬁnition 1 (Persistence diagram). Let ∆ =
diagonal R2
R2 : x0 = x1}
(x0, x1)
∆ =
{
∈
R2 : x1 > x0}
R2
(x0, x1)
∈
{

(cid:63) =

R2

x
{

∆ : mult(x) =

be the multiset of the
, where mult denotes the multiplicity function and let
, is a multiset of the form

∞}

∈

. A persistence diagram,

=

x : x

D

{

R

2
(cid:63)} ∪

∈

D
∆ .

We denote by D the set of all persistence diagrams of the form

∆

<

.

|D \

|

∞

For a given complex K of dimension nmax and a function f (of the discussed form), we can interpret
persistent homology as a mapping (K, f )
(
Di is the diagram of
D0, . . . ,
dimension i and nmax the dimension of K. We can additionally add a metric structure to the space of
persistence diagrams by introducing the notion of distances.

Dnmax−1), where

(cid:55)→

2Simplicial homology is not speciﬁc to Z/2Z, but it’s a typical choice, since it allows us to interpret n-chains

as sets of n-simplices.

3

E

(cid:33) 1

p

Deﬁnition 2 (Bottleneck, Wasserstein distance). For two persistence diagrams
their Bottleneck (w∞) and Wasserstein (wq

p) distances by

D

and

, we deﬁne

(cid:32)

(cid:88)

x∈D

.

D → E

w∞(

,

D

E

) = inf
η

sup
x∈D ||

x

−

η(x)

∞ and wq
p(
||

,

D

E

) = inf
η

x

||

−

η(x)

p
q
||

,

(2)

where p, q

N and the inﬁmum is taken over all bijections η :

∈

Essentially, this facilitates studying stability/continuity properties of topological signatures w.r.t.
metrics in the ﬁltration or complex space; we refer the reader to [12],[13], [9] for a selection of
important stability results.
Remark. By setting µi,∞
n −
referred to as essential. This change can be lifted to D by setting R2
x1 > x0}

, we extend Eq. (1) to features which never disappear, also
) :

. In Sec. 5, we will see that essential features can offer discriminative information.

(x0, x1)
{

n = βi,m

βi−1,m
n

∪ {∞}

(cid:63) =

(R

×

R

∈

3 A network layer for topological signatures

x

∈

D

D

{
such that points on R2

In this section, we introduce the proposed (parametrized) network layer for topological signatures
(in the form of persistence diagrams). The key idea is to take any
and deﬁne a projection w.r.t. a
collection (of ﬁxed size N ) of structure elements.
In the following, we set R+ :=
R : x > 0
0
R : x
, resp., and start by
}
}
∆ lie on the x-axis, see Fig. 1. The y-axis can then be
rotating points of
interpreted as the persistence of features. Formally, we let b0 and b1 be the unit vectors in directions
(1, 1)(cid:62) and (
).
x, b1(cid:105)
R
(cid:63) ∪
(cid:104)
−
∆ clock-wise by π/4. We will later see that this construction is beneﬁcial
This rotates points in R(cid:63) ∪
for a closer analysis of the layers’ properties. Similar to [27, 19], we choose exponential functions as
structure elements, but other choices are possible (see Lemma 1). Differently to [27, 19], however,
our structure elements are not at ﬁxed locations (i.e., one element per point in
), but their locations
and scales are learned during training.
Deﬁnition 3. Let µ = (µ0, µ1)(cid:62)

1, 1)(cid:62) and deﬁne a mapping ρ : R2

+
0 such that x

R+, σ = (σ0, σ1)

R+. We deﬁne

R+ and ν

x, b0(cid:105)

and R

+
0 :=

∆ →

x
{

R+

R2

R2

(
(cid:104)

(cid:55)→

≥

×

D

R

R

∈

,

×

∈

as follows:

∈

×
sµ,σ,ν : R

∈
R

R

+
0 →

×

e−σ2

0 (x0−µ0)2−σ2

1 (x1−µ1)2

,






0,

x1 ∈
, x1 ∈

[ν,

)

∞

(0, ν)

x1 = 0

sµ,σ,ν

(cid:0)(x0, x1)(cid:1) =

e−σ2

0 (x0−µ0)2−σ2

1 (ln(

x1
ν )ν+ν−µ1)2

A persistence diagram

D

is then projected w.r.t. sµ,σ,ν via
(cid:88)

Sµ,σ,ν : D

R,

→

D (cid:55)→

x∈D

sµ,σ,ν(ρ(x)) .

Remark. Note that sµ,σ,ν is continuous in x1 as
(cid:16) x
ν

x = lim
x→ν

lim
x1→0

lim
x→ν

ν + ν

and

ln

(cid:17)

sµ,σ,ν

(cid:0)(x0, x1)(cid:1) = 0 = sµ,σ,ν

(cid:0)(x0, 0)(cid:1)

and e(·) is continuous. Further, sµ,σ,ν is differentiable on R

R+, since

1 = lim
x→ν+

∂x1
∂x1

(x) and

lim
x→ν−

∂ (cid:0)ln (cid:0) x1

ν
∂x1

(x) = lim
x→ν−

= 1 .

ν
x

×
(cid:1) ν + ν(cid:1)

Also note that we use the log-transform in Eq. (4) to guarantee that sµ,σ,ν satisﬁes the conditions of
Lemma 1; this is, however, only one possible choice. Finally, given a collection of structure elements
Sµi,σi,ν, we combine them to form the output of the network layer.

(3)

(4)

4

Deﬁnition 4. Let N

N, θ = (µi, σi)N −1

∈

i=0 ∈
+
0 )N

(R

(cid:0)(R

×

D (cid:55)→

(R+

R+)

×
(cid:0)Sµi,σi,ν(

×
)(cid:1)N −1
i=0 .

D

θ,ν : D
S

→

as the concatenation of all N mappings deﬁned in Eq. (4).

R+)(cid:1)N and ν

R+. We deﬁne

∈

Importantly, a network layer implementing Def. 4 is trainable via backpropagation, as (1) sµi,σi,ν is
differentiable in µi, σi, (2) Sµi,σi,ν(
θ,ν is just a concatenation.

) is a ﬁnite sum of sµi,σi,ν and (3)

D

S

4 Theoretical properties

In this section, we demonstrate that the proposed layer is stable w.r.t. the 1-Wasserstein distance wq
1,
see Eq. (2). In fact, this claim will follow from a more general result, stating sufﬁcient conditions on
functions s : R2
Lemma 1. Let

0 such that a construction in the form of Eq. (3) is stable w.r.t. wq
1.

∆ →

(cid:63) ∪

R2

R

+

s : R

2
(cid:63) ∪

R

2
∆ →

+
0

R

(cid:107) · (cid:107)q and constant Ks

have the following properties:

(i) s is Lipschitz continuous w.r.t.
(ii) s(x(cid:1) = 0, for x

R2
∆

∈

Then, for two persistence diagrams

,

D

E ∈

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

x∈D

s(x)

−

D, it holds that
(cid:12)
(cid:12)
(cid:12)
s(y)
(cid:12)
(cid:12)
(cid:12)

Ks ·

≤

(cid:88)

y∈E

wq
1(

,

) .

D

E

(5)

Proof. see Appendix B

Remark. At this point, we want to clarify that Lemma 1 is not speciﬁc to sµ,σ,ν (e.g., as in Def. 3).
Rather, Lemma 1 yields sufﬁcient conditions to construct a w1-stable input layer. Our choice of
sµ,σ,ν is just a natural example that fulﬁls those requirements and, hence,
θ,ν is just one possible
representative of a whole family of input layers.

S

With the result of Lemma 1 in mind, we turn to the speciﬁc case of
S
properties w.r.t. wq
1. The following lemma is important in this context.
Lemma 2. sµ,σ,ν has absolutely bounded ﬁrst-order partial derivatives w.r.t. x0 and x1 on R

θ,ν and analyze its stability

R+.

×

Proof. see Appendix B

Theorem 1.

θ,ν is Lipschitz continuous with respect to wq
S

1 on D.

Proof. Lemma 2 immediately implies that sµ,σ,ν from Eq. (3) is Lipschitz continuous w.r.t
Consequently, s = sµ,σ,ν ◦
satisﬁed by construction. Hence, Sµ,σ,ν is Lipschitz continuous w.r.t. wq
Lipschitz in each coordinate and therefore Liptschitz continuous.

|| · ||q.
ρ satisﬁes property (i) from Lemma 1; property (ii) from Lemma 1 is
θ,ν is

1. Consequently,

S

Interestingly, the stability result of Theorem 1 is comparable to the stability results in [1] or [27]
(which are also w.r.t. wq
1 and in the setting of diagrams with ﬁnitely-many points). However, contrary
to previous works, if we would chop-off the input layer after network training, we would then have a
θ,ν of persistence diagrams that is speciﬁcally-tailored to the learning task on which the
mapping
S
network was trained.

5

Figure 2: Height function ﬁltration of a “clean” (left, green points) and a “noisy” (right, blue points) shape
along direction d = (0, −1)(cid:62). This example demonstrates the insensitivity of homology towards noise, as the
added noise only (1) slightly shifts the dominant points (upper left corner) and (2) produces additional points
close to the diagonal, which have little impact on the Wasserstein distance and the output of our layer.

5 Experiments

To demonstrate the versatility of the proposed approach, we present experiments with two totally
different types of data: (1) 2D shapes of objects, represented as binary images and (2) social network
graphs, given by their adjacency matrix. In both cases, the learning task is classiﬁcation. In each
experiment we ensured a balanced group size (per label) and used a 90/10 random training/test
split; all reported results are averaged over ﬁve runs with ﬁxed ν = 0.1. In practice, points in input
diagrams were thresholded at 0.01 for computational reasons. Additionally, we conducted a reference
experiment on all datasets using simple vectorization (see Sec. 5.3) of the persistence diagrams in
combination with a linear SVM.
Implementation. All experiments were implemented in PyTorch3, using DIPHA4 and Perseus [23].
Source code is publicly-available at https://github.com/c-hofer/nips2017.

5.1 Classiﬁcation of 2D object shapes

We apply persistent homology combined with our proposed input layer to two different datasets of
binary 2D object shapes: (1) the Animal dataset, introduced in [3] which consists of 20 different
animal classes, 100 samples each; (2) the MPEG-7 dataset which consists of 70 classes of different
object/animal contours, 20 samples each (see [21] for more details).

Filtration. The requirements to use persistent homology on 2D shapes are twofold: First, we need
to assign a simplicial complex to each shape; second, we need to appropriately ﬁltrate the complex.
While, in principle, we could analyze contour features, such as curvature, and choose a sublevel set
ﬁltration based on that, such a strategy requires substantial preprocessing of the discrete data (e.g.,
smoothing). Instead, we choose to work with the raw pixel data and leverage the persistent homology
transform, introduced by Turner et al. [29]. The ﬁltration in that case is based on sublevel sets of
the height function, computed from multiple directions (see Fig. 2). Practically, this means that we
directly construct a simplicial complex from the binary image. We set K0 as the set of all pixels
which are contained in the object. Then, a 1-simplex [p0, p1] is in the 1-skeleton K1 iff p0 and p1
are 4–neighbors on the pixel grid. To ﬁltrate the constructed complex, we deﬁne by b the barycenter
of the object and with r the radius of its bounding circle around b. Finally, we deﬁne, for [p]
K0
and d
. Function values are lifted to K1 by
b, d
(cid:105)
taking the maximum, cf. Sec. 2. Finally, let di be the 32 equidistantly distributed directions in S1,
Di)32
starting from (1, 0). For each shape, we get a vector of persistence diagrams (
Di is the
0-th diagram obtained by ﬁltration along di. As most objects do not differ in homology groups of
higher dimensions (> 0), we did not use the corresponding persistence diagrams.

S1, the ﬁltration function by f ([p]) = 1/r

i=1 where

· (cid:104)

−

∈

∈

p

Network architecture. While the full network is listed in the supplementary material (Fig. 6), the
key architectural choices are: 32 independent input branches, i.e., one for each ﬁltration direction.
Further, the i-th branch gets, as input, the vector of persistence diagrams from directions di−1, di
and di+1. This is a straightforward approach to capture dependencies among the ﬁltration directions.
We use cross-entropy loss to train the network for 400 epochs, using stochastic gradient descent
(SGD) with mini-batches of size 128 and an initial learning rate of 0.1 (halved every 25-th epoch).

3https://github.com/pytorch/pytorch
4https://bitbucket.org/dipha/dipha

6

MPEG-7

Animal

‡Skeleton paths
‡Class segment sets
†ICS
†BCF

Ours

86.7
90.9
96.6
97.2

91.8

67.9
69.7
78.4
83.4

69.5

Figure 3: Left: some examples from the MPEG-7 (bottom) and Animal (top) datasets. Right: Classiﬁcation
results, compared to the two best (†) and two worst (‡) results reported in [30].

Results. Fig. 3 shows a selection of 2D object shapes from both datasets, together with the obtained
classiﬁcation results. We list the two best (
) results as reported in [30]. While,
†
on the one hand, using topological signatures is below the state-of-the-art, the proposed architecture
is still better than other approaches that are speciﬁcally tailored to the problem. Most notably, our
approach does not require any speciﬁc data preprocessing, whereas all other competitors listed in
Fig. 3 require, e.g., some sort of contour extraction. Furthermore, the proposed architecture readily
S2. Fig. 4 (Right) shows an exemplary
generalizes to 3D with the only difference that in this case di ∈
visualization of the position of the learned structure elements for the Animal dataset.

) and two worst (

‡

5.2 Classiﬁcation of social network graphs

In this experiment, we consider the problem of graph classiﬁcation, where vertices are unlabeled
= (V, E), where V denotes the set of
and edges are undirected. That is, a graph
vertices and E denotes the set of edges. We evaluate our approach on the challenging problem of
social network classiﬁcation, using the two largest benchmark datasets from [31], i.e., reddit-5k
(5 classes, 5k graphs) and reddit-12k (11 classes,
12k graphs). Each sample in these datasets
represents a discussion graph and the classes indicate subreddits (e.g., worldnews, video, etc.).

is given by

≈

G

G

V

[v]

= (V, E) is straightforward: we set
Filtration. The construction of a simplicial complex from
K0 =
. We choose a very simple ﬁltration based on
}
the vertex degree, i.e., the number of incident edges to a vertex v
V . Hence, for [v0]
K0 we get
f ([v0]) = deg(v0)/ maxv∈V deg(v) and again lift f to K1 by taking the maximum. Note that chain
groups are trivial for dimension > 1, hence, all features in dimension 1 are essential.

v0, v1} ∈
{

[v0, v1] :
{

and K1 =

E

∈

∈

∈

G

{

}

Network architecture. Our network has four input branches: two for each dimension (0 and 1) of
the homological features, split into essential and non-essential ones, see Sec. 2. We train the network
for 500 epochs using SGD and cross-entropy loss with an initial learning rate of 0.1 (reddit_5k), or
0.4 (reddit_12k). The full network architecture is listed in the supplementary material (Fig. 7).

Results. Fig. 5 (right) compares our proposed strategy to state-of-the-art approaches from the
literature. In particular, we compare against (1) the graphlet kernel (GK) and deep graphlet kernel
(DGK) results from [31], (2) the Patchy-SAN (PSCN) results from [24] and (3) a recently reported
graph-feature + random forest approach (RF) from [4]. As we can see, using topological signatures
in our proposed setting considerably outperforms the current state-of-the-art on both datasets. This is
an interesting observation, as PSCN [24] for instance, also relies on node degrees and an extension of
the convolution operation to graphs. Further, the results reveal that including essential features is key
to these improvements.

5.3 Vectorization of persistence diagrams

D

we calculate its persistence, i.e., d

Here, we brieﬂy present a reference experiment we conducted following Bendich et al. [5]. The idea
is to directly use the persistence diagrams as features via vectorization. For each point (b, d) in a
persistence diagram
b. We then sort the calculated persistences
by magnitude from high to low and take the ﬁrst N values. Hence, we get, for each persistence
diagram, a vector of dimension N (if
< N , we pad with zero). We used this technique
on all four data sets. As can be seen from the results in Table 4 (averaged over 10 cross-validation
runs), vectorization performs poorly on MPEG-7 and Animal but can lead to competitive rates on
reddit-5k and reddit-12k. Nevertheless, the obtained performance is considerably inferior to our
proposed approach.

|D \

∆

−

|

7

N

MPEG-7
Animal
reddit-5k
reddit-12k

5

81.8
48.8
37.1
24.2

10

82.3
50.0
38.2
24.6

20

79.7
46.2
39.7
27.9

40

74.5
42.4
42.1
29.8

80

68.2
39.3
43.8
31.5

160

64.4
36.0
45.2
31.6

Ours

91.8
69.5
54.5
44.5

N ) persistence diagrams
Figure 4: Left: Classiﬁcation accuracies for a linear SVM trained on vectorized (in R
(see Sec. 5.3). Right: Exemplary visualization of the learned structure elements (in 0-th dimension) for the
Animal dataset and ﬁltration direction d = (−1, 0)(cid:62). Centers of the learned elements are marked in blue.

reddit-5k

reddit-12k

GK [31]
DGK [31]
PSCN [24]
RF [4]

Ours (w/o essential)
Ours (w/ essential)

41.0
41.3
49.1
50.9

49.1
54.5

31.8
32.2
41.3
42.7

38.5
44.5

Figure 5: Left: Illustration of graph ﬁltration by vertex degree, i.e., f ≡ deg (for different choices of ai, see
Sec. 2). Right: Classiﬁcation results as reported in [31] for GK and DGK, Patchy-SAN (PSCN) as reported in
.
[24] and feature-based random-forest (RF) classiﬁcation from [4].

Finally, we remark that in both experiments, tests with the kernel of [27] turned out to be computa-
tionally impractical, (1) on shape data due to the need to evaluate the kernel for all ﬁltration directions
and (2) on graphs due the large number of samples and the number of points in each diagram.

6 Discussion

We have presented, to the best of our knowledge, the ﬁrst approach towards learning task-optimal
stable representations of topological signatures, in our case persistence diagrams. Our particular
realization of this idea, i.e., as an input layer to deep neural networks, not only enables us to learn with
topological signatures, but also to use them as additional (and potentially complementary) inputs to
existing deep architectures. From a theoretical point of view, we remark that the presented structure
elements are not restricted to exponential functions, so long as the conditions of Lemma 1 are met.
One drawback of the proposed approach, however, is the artiﬁcial bending of the persistence axis (see
Fig. 1) by a logarithmic transformation; in fact, other strategies might be possible and better suited
in certain situations. A detailed investigation of this issue is left for future work. From a practical
perspective, it is also worth pointing out that, in principle, the proposed layer could be used to handle
any kind of input that comes in the form of multisets (of Rn), whereas previous works only allow
to handle sets of ﬁxed size (see Sec. 1). In summary, we argue that our experiments show strong
evidence that topological features of data can be beneﬁcial in many learning tasks, not necessarily to
replace existing inputs, but rather as a complementary source of discriminative information.

8

A Technical results

Lemma 3. Let α

i)

lim
x→0

ln(x)
x

·

R+, β

R, γ

∈
∈
∈
e−α(ln(x)γ+β)2 = 0

R+. We have

ii)

lim
x→0

1
x ·

e−α(ln(x)γ+β)2 = 0 .

Proof. We omit the proof for brevity (see supplementary material for details), but remark that only
(i) needs to be shown as (ii) follows immediately.

B Proofs

D \

E \

E0 =
D

Proof of Lemma 1. Let ϕ be a bijection between

) and let
and
∆. To show the result of Eq. (5), we consider the following decomposition:

which realizes wq
1(

D

D

E

E

,

∆,

D0 =

= ϕ−1(
= (ϕ−1(
(cid:124)

E0)
∪
E0)
(cid:123)(cid:122)
A

ϕ−1(∆)
∆)
(cid:125)

∪

E0)
(cid:123)(cid:122)
B
Except for the term D, all sets are ﬁnite. In fact, ϕ realizes the Wasserstein distance wq
ϕ(cid:12)
= id. Therefore, s(x) = s(ϕ(x)) = 0 for x
(cid:12)D
∈
in the summation and it sufﬁces to consider E = A

C. It follows that

(ϕ−1(∆)
(cid:124)
(cid:123)(cid:122)
C

(ϕ−1(∆)
(cid:124)
(cid:123)(cid:122)
D

(ϕ−1(
(cid:124)

∆)
(cid:125)

∆)
(cid:125)

⊂

∪

∩

∪

∩

\

\

∆)
(cid:125)

1 which implies
∆. Consequently, we can ignore D

(6)

∪

D since D
B
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∪
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:88)

x∈E

(cid:88)

−

x∈D

(cid:88)

x∈E

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

s(x)

s(ϕ(x))

s(x)

s(ϕ(x))

(cid:88)

(cid:12)
(cid:12)
(cid:12)
s(ϕ(x))
(cid:12)
(cid:12) ≤
||q = Ks ·

ϕ(x)

x∈E

−

s(x)

(cid:88)

x∈E

−

x

||

(cid:88)

x∈D

s(x)
|

−

s(ϕ(x))

|

x∈D

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Ks ·

(cid:88)

x∈E

=

=

≤

x

||

−

ϕ(x)

||q = Ks ·

wq
1(

,

) .

D

E

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
x∈D

s(x)

(cid:88)

y∈E

−

(cid:12)
(cid:12)
(cid:12)
s(y)
(cid:12)
(cid:12)
(cid:12)

Proof of Lemma 2. Since sµ,σ,ν is deﬁned differently for x1 ∈
distinguish these two cases. In the following x0 ∈
(1) x1 ∈

): The partial derivative w.r.t. xi is given as

R.

[ν,

∞

[ν,

) and x1 ∈

∞

(0, ν), we need to

(cid:18) ∂
∂xi

(cid:19)

sµ,σ,ν

(x0, x1) = C

i (xi−µi)2(cid:19)

e−σ2

(x0, x1)

·

(cid:18) ∂
∂xi
e−σ2

i (xi−µi)2

= C

(
·
−
) which is not dependent on xi. For all cases, i.e., x0 → ∞

i )(xi −

µi) ,

·

2σ2

, x0 →

(7)

where C is just the part of exp(
·

, it holds that Eq. (7)

0.

→

and x1 → ∞

−∞
(2) x1 ∈
behaviour for x0 → ∞
(cid:19)
(cid:18) ∂
∂x1

sµ,σ,ν

(0, ν): The partial derivative w.r.t. x0 is similar to Eq. (7) with the same asymptotic

and x0 → −∞
(x0, x1) = C

. However, for the partial derivative w.r.t. x1 we get
(cid:18) ∂
∂x1
e( ... )

ν )ν+ν−µ1)2 (cid:19)

(x0, x1)

e−σ2

ν + ν

1 (ln(

ln

(cid:16)

(cid:17)

(cid:17)

x1

µ1

2σ2
1)

ν
x1

·

·

·

= C

(
−

·

·
(cid:16) x1
ν

(cid:17)

(cid:18)

ln

·

(cid:123)(cid:122)
(a)

(cid:16) x1
ν
ν
x1

·

(cid:19)

(cid:125)

= C (cid:48)

e( ... )

(cid:16)

·

(cid:124)

−

−

+(ν

µ1)

e( ... )

(cid:17)

.

1
x1
(cid:125)

·

(cid:124)

·
(cid:123)(cid:122)
(b)

(8)

As x1 →
Eq. (8)
→
we conclude that they are absolutely bounded.

0, we can invoke Lemma 4 (i) to handle (a) and Lemma 4 (ii) to handle (b); conclusively,
0. As the partial derivatives w.r.t. xi are continuous and their limits are 0 on R, R+, resp.,

9

References

[1] H. Adams, T. Emerson, M. Kirby, R. Neville, C. Peterson, P. Shipman, S. Chepushtanova, E. Hanson,
F. Motta, and L. Ziegelmeier. Persistence images: A stable vector representation of persistent homology.
JMLR, 18(8):1–35, 2017.

[2] A. Adcock, E. Carlsson, and G. Carlsson. The ring of algebraic functions on persistence bar codes. CoRR,

2013. https://arxiv.org/abs/1304.0530.

[3] X. Bai, W. Liu, and Z. Tu. Integrating contour and skeleton for shape classiﬁcation. In ICCV Workshops,

2009.

[4] I. Barnett, N. Malik, M.L. Kuijjer, P.J. Mucha, and J.-P. Onnela. Feature-based classiﬁcation of networks.

CoRR, 2016. https://arxiv.org/abs/1610.05868.

[5] P. Bendich, J.S. Marron, E. Miller, A. Pieloch, and S. Skwerer. Persistent homology analysis of brain artery

trees. Ann. Appl. Stat, 10(2), 2016.

[6] P. Bubenik. Statistical topological data analysis using persistence landscapes. JMLR, 16(1):77–102, 2015.

[7] G. Carlsson. Topology and data. Bull. Amer. Math. Soc., 46:255–308, 2009.

[8] G. Carlsson, T. Ishkhanov, V. de Silva, and A. Zomorodian. On the local behavior of spaces of natural

images. IJCV, 76:1–12, 2008.

[9] F. Chazal, D. Cohen-Steiner, L. J. Guibas, F. Mémoli, and S. Y. Oudot. Gromov-Hausdorff stable signatures

for shapes using persistence. Comput. Graph. Forum, 28(5):1393–1403, 2009.

[10] F. Chazal, B.T. Fasy, F. Lecci, A. Rinaldo, and L. Wassermann. Stochastic convergence of persistence

landscapes and silhouettes. JoCG, 6(2):140–161, 2014.

[11] F. Chazal, L.J. Guibas, S.Y. Oudot, and P. Skraba. Persistence-based clustering in Riemannian manifolds.

[12] D. Cohen-Steiner, H. Edelsbrunner, and J. Harer. Stability of persistence diagrams. Discrete Comput.

J. ACM, 60(6):41–79, 2013.

Geom., 37(1):103–120, 2007.

[13] D. Cohen-Steiner, H. Edelsbrunner, J. Harer, and Y. Mileyko. Lipschitz functions have Lp-stable persistence.

Found. Comput. Math., 10(2):127–139, 2010.

[14] H. Edelsbrunner and J. L. Harer. Computational Topology : An Introduction. American Mathematical

Society, 2010.

[15] H. Edelsbrunner, D. Letcher, and A. Zomorodian. Topological persistence and simpliﬁcation. Discrete

Comput. Geom., 28(4):511–533, 2002.

[16] A. Hatcher. Algebraic Topology. Cambridge University Press, Cambridge, 2002.

[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.

[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural

networks. In NIPS, 2012.

analysis. In ICML, 2016.

perspective. In NIPS, 2015.

contour. In CVPR, 2000.

[19] G. Kusano, K. Fukumizu, and Y. Hiraoka. Persistence weighted Gaussian kernel for topological data

[20] R. Kwitt, S. Huber, M. Niethammer, W. Lin, and U. Bauer. Statistical topological data analysis - a kernel

[21] L. Latecki, R. Lakamper, and T. Eckhardt. Shape descriptors for non-rigid shapes with a single closed

[22] C. Li, M. Ovsjanikov, and F. Chazal. Persistence-based structural recognition. In CVPR, 2014.

[23] K. Mischaikow and V. Nanda. Morse theory for ﬁltrations and efﬁcient computation of persistent homology.

Discrete Comput. Geom., 50(2):330–353, 2013.

[24] M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. In ICML,

[25] C.R. Qi, H. Su, K. Mo, and L.J. Guibas. PointNet: Deep learning on point sets for 3D classiﬁcation and

2016.

segmentation. In CVPR, 2017.

[26] S. Ravanbakhsh, S. Schneider, and B. Póczos. Deep learning with sets and point clouds. In ICLR, 2017.

[27] R. Reininghaus, U. Bauer, S. Huber, and R. Kwitt. A stable multi-scale kernel for topological machine

learning. In CVPR, 2015.

[28] G. Singh, F. Memoli, T. Ishkhanov, G. Sapiro, G. Carlsson, and D.L. Ringach. Topological analysis of

population activity in visual cortex. J. Vis., 8(8), 2008.

10

[29] K. Turner, S. Mukherjee, and D. M. Boyer. Persistent homology transform for modeling shapes and

[30] X. Wang, B. Feng, X. Bai, W. Liu, and L.J. Latecki. Bag of contour fragments for robust shape classiﬁcation.

surfaces. Inf. Inference, 3(4):310–344, 2014.

Pattern Recognit., 47(6):2116–2125, 2014.

[31] P. Yanardag and S.V.N. Vishwanathan. Deep graph kernels. In KDD, 2015.

This supplementary material contains technical details that were left-out in the original submission
for brevity. When necessary, we refer to the submitted manuscript.

C Additional proofs

In the manuscript, we omitted the proof for the following technical lemma. For completeness, the
lemma is repeated and its proof is given below.
R+, β
Lemma 4. Let α

R+. We have

R and γ

∈

∈
e−α(ln(x)γ+β)2 = 0

∈

·

e−α(ln(x)γ+β)2 = 0 .

(i) lim
x→0

ln(x)
x

(ii) lim
x→0

1
x ·

Proof. We only need to prove the ﬁrst statement, as the second follows immediately. Hence, consider

lim
x→0

ln(x)
x

·

e−α(ln(x)γ+β)2

ln(x)

e− ln(x)

e−α(ln(x)γ+β)2

= lim
x→0

= lim
x→0

ln(x)

= lim
x→0

ln(x)

·

·

·

·

e−α(ln(x)γ+β)2−ln(x)
eα(ln(x)γ+β)2+ln(x)(cid:17)−1

(cid:16)

(∗)
= lim
x→0

∂
∂x

ln(x)

(cid:18) ∂
∂x

·

eα(ln(x)γ+β)2+ln(x)

(cid:19)−1

(cid:18)

(cid:18)

eα(ln(x)γ+β)2+ln(x)

1
x ·
(cid:16)
eα(ln(x)γ+β)2+ln(x)

·

2α(ln(x)γ + β)

(cid:19)(cid:19)−1

γ
x

+

1
x
(cid:17)−1

(2α(ln(x)γ + β)γ + 1)

·

= lim
x→0

= lim
x→0

= 0

).
where we use de l’Hôpital’s rule in (
∗

D Network architectures

2D object shape classiﬁcation. Fig. 6 illustrates the network architecture used for 2D object shape
classiﬁcation in [Manuscript, Sec. 5.1]. Note that the persistence diagrams from three consecutive
ﬁltration directions di share one input layer. As we use 32 directions, we have 32 input branches.
3 and a stride of 1. The max-pooling
The convolution operation operates with kernels of size 1
operates along the ﬁlter dimension. For better readability, we have added the output size of certain
layers. We train with the network with stochastic gradient descent (SGD) and a mini-batch size of
128 for 300 epochs. Every 20th epoch, the learning rate (initially set to 0.1) is halved.

×

×

1

11

Graph classiﬁcation. Fig. 7 illustrates the network architecture used for graph classiﬁcation in
Sec. 5.2. In detail, we have 3 input branches: ﬁrst, we split 0-dimensional features into essential
and non-essential ones; second, since there are only essential features in dimension 1 (see Sec. 5.2,
Filtration) we do not need a branch for non-essential features. We train the network using SGD with
mini-batches of size 128 for 300 epochs. The initial learning rate is set to 0.1 (reddit_5k) and 0.4
(reddit_12k), resp., and halved every 20th epochs.

D.1 Technical handling of essential features

In case of of 2D object shapes, the death times of essential features are mapped to the max. ﬁltration
value and kept in the original persistence diagrams. In fact, for Animal and MPEG-7, there is always
only one connected component and consequently only one essential feature in dimension 0 (i.e., it
does not make sense to handle this one point in a separate input branch).

In case of social network graphs, essential features are mapped to the real line (using their birth time)
and handled in separate input branches (see Fig. 7) with 1D structure elements. This is in contrast to
the 2D object shape experiments, as we might have many essential features (in dimensions 0 and 1)
that require handling in separate input branches.

12

Figure 6: 2D object shape classiﬁcation network architecture.

13

Figure 7: Graph classiﬁcation network architecture.

14

8
1
0
2
 
b
e
F
 
6
1
 
 
]

V
C
.
s
c
[
 
 
3
v
1
4
0
4
0
.
7
0
7
1
:
v
i
X
r
a

Deep Learning with Topological Signatures

Christoph Hofer
Department of Computer Science
University of Salzburg, Austria
chofer@cosy.sbg.ac.at

Roland Kwitt
Department of Computer Science
University of Salzburg, Austria
Roland.Kwitt@sbg.ac.at

Marc Niethammer
UNC Chapel Hill, NC, USA
mn@cs.unc.edu

Andreas Uhl
Department of Computer Science
University of Salzburg, Austria
uhl@cosy.sbg.ac.at

Abstract

Inferring topological and geometrical information from data can offer an alternative
perspective on machine learning problems. Methods from topological data analysis,
e.g., persistent homology, enable us to obtain such information, typically in the form
of summary representations of topological features. However, such topological
signatures often come with an unusual structure (e.g., multisets of intervals) that is
highly impractical for most machine learning techniques. While many strategies
have been proposed to map these topological signatures into machine learning
compatible representations, they suffer from being agnostic to the target learning
task. In contrast, we propose a technique that enables us to input topological
signatures to deep neural networks and learn a task-optimal representation during
training. Our approach is realized as a novel input layer with favorable theoretical
properties. Classiﬁcation experiments on 2D object shapes and social network
graphs demonstrate the versatility of the approach and, in case of the latter, we
even outperform the state-of-the-art by a large margin.

1

Introduction

Methods from algebraic topology have only recently emerged in the machine learning community,
most prominently under the term topological data analysis (TDA) [7]. Since TDA enables us to
infer relevant topological and geometrical information from data, it can offer a novel and potentially
beneﬁcial perspective on various machine learning problems. Two compelling beneﬁts of TDA
are (1) its versatility, i.e., we are not restricted to any particular kind of data (such as images,
sensor measurements, time-series, graphs, etc.) and (2) its robustness to noise. Several works have
demonstrated that TDA can be beneﬁcial in a diverse set of problems, such as studying the manifold
of natural image patches [8], analyzing activity patterns of the visual cortex [28], classiﬁcation of 3D
surface meshes [27, 22], clustering [11], or recognition of 2D object shapes [29].
Currently, the most widely-used tool from TDA is persistent homology [15, 14]. Essentially1,
persistent homology allows us to track topological changes as we analyze data at multiple “scales”.
As the scale changes, topological features (such as connected components, holes, etc.) appear and
disappear. Persistent homology associates a lifespan to these features in the form of a birth and
a death time. The collection of (birth, death) tuples forms a multiset that can be visualized as a
persistence diagram or a barcode, also referred to as a topological signature of the data. However,
leveraging these signatures for learning purposes poses considerable challenges, mostly due to their

1We will make these concepts more concrete in Sec. 2.

Figure 1: Illustration of the proposed network input layer for topological signatures. Each signature, in the
form of a persistence diagram D ∈ D (left), is projected w.r.t. a collection of structure elements. The layer’s
+ is set a-priori and
learnable parameters θ are the locations µi and the scales σi of these elements; ν ∈ R
meant to discount the impact of points with low persistence (and, in many cases, of low discriminative power).
The layer output y is a concatenation of the projections. In this illustration, N = 2 and hence y = (y1, y2)(cid:62).

unusual structure as a multiset. While there exist suitable metrics to compare signatures (e.g., the
Wasserstein metric), they are highly impractical for learning, as they require solving optimal matching
problems.

Related work. In order to deal with these issues, several strategies have been proposed. In [2] for
instance, Adcock et al. use invariant theory to “coordinatize” the space of barcodes. This allows to
map barcodes to vectors of ﬁxed size which can then be fed to standard machine learning techniques,
such as support vector machines (SVMs). Alternatively, Adams et al. [1] map barcodes to so-called
persistence images which, upon discretization, can also be interpreted as vectors and used with
standard learning techniques. Along another line of research, Bubenik [6] proposes a mapping
of barcodes into a Banach space. This has been shown to be particularly viable in a statistical
context (see, e.g., [10]). The mapping outputs a representation referred to as a persistence landscape.
Interestingly, under a speciﬁc choice of parameters, barcodes are mapped into L2(R2) and the
inner-product in that space can be used to construct a valid kernel function. Similar, kernel-based
techniques, have also recently been studied by Reininghaus et al. [27], Kwitt et al. [20] and Kusano
et al. [19].

While all previously mentioned approaches retain certain stability properties of the original repre-
sentation with respect to common metrics in TDA (such as the Wasserstein or Bottleneck distances),
they also share one common drawback: the mapping of topological signatures to a representation that
is compatible with existing learning techniques is pre-deﬁned. Consequently, it is ﬁxed and therefore
agnostic to any speciﬁc learning task. This is clearly suboptimal, as the eminent success of deep
neural networks (e.g., [18, 17]) has shown that learning representations is a preferable approach.
Furthermore, techniques based on kernels [27, 20, 19] for instance, additionally suffer scalability
issues, as training typically scales poorly with the number of samples (e.g., roughly cubic in case of
kernel-SVMs). In the spirit of end-to-end training, we therefore aim for an approach that allows to
learn a task-optimal representation of topological signatures. We additionally remark that, e.g., Qi et
al. [25] or Ravanbakhsh et al. [26] have proposed architectures that can handle sets, but only with
ﬁxed size. In our context, this is impractical as the capability of handling sets with varying cardinality
is a requirement to handle persistent homology in a machine learning setting.

Contribution. To realize this idea, we advocate a novel input layer for deep neural networks that
takes a topological signature (in our case, a persistence diagram), and computes a parametrized
projection that can be learned during network training. Speciﬁcally, this layer is designed such that
its output is stable with respect to the 1-Wasserstein distance (similar to [27] or [1]). To demonstrate
the versatility of this approach, we present experiments on 2D object shape classiﬁcation and the
classiﬁcation of social network graphs. On the latter, we improve the state-of-the-art by a large
margin, clearly demonstrating the power of combining TDA with deep learning in this context.

2 Background

For space reasons, we only provide a brief overview of the concepts that are relevant to this work and
refer the reader to [16] or [14] for further details.

Homology. The key concept of homology theory is to study the properties of some object X by
means of (commutative) algebra. In particular, we assign to X a sequence of modules C0, C1, . . .

2

ker ∂n. A structure
which are connected by homomorphisms ∂n : Cn →
of this form is called a chain complex and by studying its homology groups Hn = ker ∂n/ im ∂n+1
we can derive properties of X.

Cn−1 such that im ∂n+1 ⊆

A prominent example of a homology theory is simplicial homology. Throughout this work, it is
the used homology theory and hence we will now concretize the already presented ideas. Let K
be a simplicial complex and Kn its n-skeleton. Then we set Cn(K) as the vector space gener-
ated (freely) by Kn over Z/2Z2. The connecting homomorphisms ∂n : Cn(K)
Cn−1(K) are
→
called boundary operators. For a simplex σ = [x0, . . . , xn]
Kn, we deﬁne them as ∂n(σ) =
(cid:80)n
i=0[x0, . . . , xi−1, xi+1, . . . , xn] and linearly extend this to Cn(K), i.e., ∂n((cid:80) σi) = (cid:80) ∂n(σi).
Persistent homology. Let K be a simplicial complex and (K i)m
i=0 a sequence of simplicial com-
K m = K. Then, (K i)m
i=0 is called a ﬁltration of K. If we
plexes such that
∅
use the extra information provided by the ﬁltration of K, we obtain the following sequence of chain
complexes (left),

⊆ · · · ⊆

= K 0

K 1

⊆

∈

n = Cn(K i
where C i
homology groups, deﬁned by

n) and ι denotes the inclusion. This then leads to the concept of persistent

H i,j

n/(im ∂j

n = ker ∂i
The ranks, βi,j
n , of these homology groups (i.e., the n-th persistent Betti numbers),
capture the number of homological features of dimensionality n (e.g., connected components for
n = 0, holes for n = 1, etc.) that persist from i to (at least) j. In fact, according to [14, Fundamental
Lemma of Persistent Homology], the quantities

n = rank H i,j

ker ∂i
n)

n+1 ∩

j .

for

≤

i

n = (βi,j−1
µi,j

n

βi,j
n )

(βi−1,j−1

n

βi−1,j
n

)

for

i < j

−

−

−

(1)

encode all the information about the persistent Betti numbers of dimension n.

Topological signatures. A typical way to obtain a ﬁltration of K is to consider sublevel sets of a
function f : C0(K)
R. This function can be easily lifted to higher-dimensional chain groups of
K by

→

f ([v0, . . . , vn]) = max
{
, we obtain (Ki)m
|

i
≤
i=0 by setting K0 =

f ([vi]) : 0

≤
∅

≤

f (C0(K))
|
m, where a1 <

, ai]) for
Given m =
< am is the sorted sequence of values of f (C0(K)). If we construct
1
i
a multiset such that, for i < j, the point (ai, aj) is inserted with multiplicity µi,j
n , we effectively
encode the persistent homology of dimension n w.r.t. the sublevel set ﬁltration induced by f . Upon
adding diagonal points with inﬁnite multiplicity, we obtain the following structure:

−∞

· · ·

≤

.

n
and Ki = f −1((

}

Deﬁnition 1 (Persistence diagram). Let ∆ =
diagonal R2
R2 : x0 = x1}
(x0, x1)
∆ =
{
∈
R2 : x1 > x0}
R2
(x0, x1)
∈
{

(cid:63) =

R2

x
{

∆ : mult(x) =

be the multiset of the
, where mult denotes the multiplicity function and let
, is a multiset of the form

∞}

∈

. A persistence diagram,

=

x : x

D

{

R

2
(cid:63)} ∪

∈

D
∆ .

We denote by D the set of all persistence diagrams of the form

∆

<

.

|D \

|

∞

For a given complex K of dimension nmax and a function f (of the discussed form), we can interpret
persistent homology as a mapping (K, f )
(
Di is the diagram of
D0, . . . ,
dimension i and nmax the dimension of K. We can additionally add a metric structure to the space of
persistence diagrams by introducing the notion of distances.

Dnmax−1), where

(cid:55)→

2Simplicial homology is not speciﬁc to Z/2Z, but it’s a typical choice, since it allows us to interpret n-chains

as sets of n-simplices.

3

E

(cid:33) 1

p

Deﬁnition 2 (Bottleneck, Wasserstein distance). For two persistence diagrams
their Bottleneck (w∞) and Wasserstein (wq

p) distances by

D

and

, we deﬁne

(cid:32)

(cid:88)

x∈D

.

D → E

w∞(

,

D

E

) = inf
η

sup
x∈D ||

x

−

η(x)

∞ and wq
p(
||

,

D

E

) = inf
η

x

||

−

η(x)

p
q
||

,

(2)

where p, q

N and the inﬁmum is taken over all bijections η :

∈

Essentially, this facilitates studying stability/continuity properties of topological signatures w.r.t.
metrics in the ﬁltration or complex space; we refer the reader to [12],[13], [9] for a selection of
important stability results.
Remark. By setting µi,∞
n −
referred to as essential. This change can be lifted to D by setting R2
x1 > x0}

, we extend Eq. (1) to features which never disappear, also
) :

. In Sec. 5, we will see that essential features can offer discriminative information.

(x0, x1)
{

n = βi,m

βi−1,m
n

∪ {∞}

(cid:63) =

(R

×

R

∈

3 A network layer for topological signatures

x

∈

D

D

{
such that points on R2

In this section, we introduce the proposed (parametrized) network layer for topological signatures
(in the form of persistence diagrams). The key idea is to take any
and deﬁne a projection w.r.t. a
collection (of ﬁxed size N ) of structure elements.
In the following, we set R+ :=
R : x > 0
0
R : x
, resp., and start by
}
}
∆ lie on the x-axis, see Fig. 1. The y-axis can then be
rotating points of
interpreted as the persistence of features. Formally, we let b0 and b1 be the unit vectors in directions
(1, 1)(cid:62) and (
).
x, b1(cid:105)
R
(cid:63) ∪
(cid:104)
−
∆ clock-wise by π/4. We will later see that this construction is beneﬁcial
This rotates points in R(cid:63) ∪
for a closer analysis of the layers’ properties. Similar to [27, 19], we choose exponential functions as
structure elements, but other choices are possible (see Lemma 1). Differently to [27, 19], however,
our structure elements are not at ﬁxed locations (i.e., one element per point in
), but their locations
and scales are learned during training.
Deﬁnition 3. Let µ = (µ0, µ1)(cid:62)

1, 1)(cid:62) and deﬁne a mapping ρ : R2

+
0 such that x

R+, σ = (σ0, σ1)

R+. We deﬁne

R+ and ν

x, b0(cid:105)

and R

+
0 :=

∆ →

x
{

R+

R2

R2

(
(cid:104)

(cid:55)→

≥

×

D

R

R

∈

,

×

∈

as follows:

∈

×
sµ,σ,ν : R

∈
R

R

+
0 →

×

e−σ2

0 (x0−µ0)2−σ2

1 (x1−µ1)2

,






0,

x1 ∈
, x1 ∈

[ν,

)

∞

(0, ν)

x1 = 0

sµ,σ,ν

(cid:0)(x0, x1)(cid:1) =

e−σ2

0 (x0−µ0)2−σ2

1 (ln(

x1
ν )ν+ν−µ1)2

A persistence diagram

D

is then projected w.r.t. sµ,σ,ν via
(cid:88)

Sµ,σ,ν : D

R,

→

D (cid:55)→

x∈D

sµ,σ,ν(ρ(x)) .

Remark. Note that sµ,σ,ν is continuous in x1 as
(cid:16) x
ν

x = lim
x→ν

lim
x1→0

lim
x→ν

ν + ν

and

ln

(cid:17)

sµ,σ,ν

(cid:0)(x0, x1)(cid:1) = 0 = sµ,σ,ν

(cid:0)(x0, 0)(cid:1)

and e(·) is continuous. Further, sµ,σ,ν is differentiable on R

R+, since

1 = lim
x→ν+

∂x1
∂x1

(x) and

lim
x→ν−

∂ (cid:0)ln (cid:0) x1

ν
∂x1

(x) = lim
x→ν−

= 1 .

ν
x

×
(cid:1) ν + ν(cid:1)

Also note that we use the log-transform in Eq. (4) to guarantee that sµ,σ,ν satisﬁes the conditions of
Lemma 1; this is, however, only one possible choice. Finally, given a collection of structure elements
Sµi,σi,ν, we combine them to form the output of the network layer.

(3)

(4)

4

Deﬁnition 4. Let N

N, θ = (µi, σi)N −1

∈

i=0 ∈
+
0 )N

(R

(cid:0)(R

×

D (cid:55)→

(R+

R+)

×
(cid:0)Sµi,σi,ν(

×
)(cid:1)N −1
i=0 .

D

θ,ν : D
S

→

as the concatenation of all N mappings deﬁned in Eq. (4).

R+)(cid:1)N and ν

R+. We deﬁne

∈

Importantly, a network layer implementing Def. 4 is trainable via backpropagation, as (1) sµi,σi,ν is
differentiable in µi, σi, (2) Sµi,σi,ν(
θ,ν is just a concatenation.

) is a ﬁnite sum of sµi,σi,ν and (3)

D

S

4 Theoretical properties

In this section, we demonstrate that the proposed layer is stable w.r.t. the 1-Wasserstein distance wq
1,
see Eq. (2). In fact, this claim will follow from a more general result, stating sufﬁcient conditions on
functions s : R2
Lemma 1. Let

0 such that a construction in the form of Eq. (3) is stable w.r.t. wq
1.

∆ →

(cid:63) ∪

R2

R

+

s : R

2
(cid:63) ∪

R

2
∆ →

+
0

R

(cid:107) · (cid:107)q and constant Ks

have the following properties:

(i) s is Lipschitz continuous w.r.t.
(ii) s(x(cid:1) = 0, for x

R2
∆

∈

Then, for two persistence diagrams

,

D

E ∈

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

x∈D

s(x)

−

D, it holds that
(cid:12)
(cid:12)
(cid:12)
s(y)
(cid:12)
(cid:12)
(cid:12)

Ks ·

≤

(cid:88)

y∈E

wq
1(

,

) .

D

E

(5)

Proof. see Appendix B

Remark. At this point, we want to clarify that Lemma 1 is not speciﬁc to sµ,σ,ν (e.g., as in Def. 3).
Rather, Lemma 1 yields sufﬁcient conditions to construct a w1-stable input layer. Our choice of
sµ,σ,ν is just a natural example that fulﬁls those requirements and, hence,
θ,ν is just one possible
representative of a whole family of input layers.

S

With the result of Lemma 1 in mind, we turn to the speciﬁc case of
S
properties w.r.t. wq
1. The following lemma is important in this context.
Lemma 2. sµ,σ,ν has absolutely bounded ﬁrst-order partial derivatives w.r.t. x0 and x1 on R

θ,ν and analyze its stability

R+.

×

Proof. see Appendix B

Theorem 1.

θ,ν is Lipschitz continuous with respect to wq
S

1 on D.

Proof. Lemma 2 immediately implies that sµ,σ,ν from Eq. (3) is Lipschitz continuous w.r.t
Consequently, s = sµ,σ,ν ◦
satisﬁed by construction. Hence, Sµ,σ,ν is Lipschitz continuous w.r.t. wq
Lipschitz in each coordinate and therefore Liptschitz continuous.

|| · ||q.
ρ satisﬁes property (i) from Lemma 1; property (ii) from Lemma 1 is
θ,ν is

1. Consequently,

S

Interestingly, the stability result of Theorem 1 is comparable to the stability results in [1] or [27]
(which are also w.r.t. wq
1 and in the setting of diagrams with ﬁnitely-many points). However, contrary
to previous works, if we would chop-off the input layer after network training, we would then have a
θ,ν of persistence diagrams that is speciﬁcally-tailored to the learning task on which the
mapping
S
network was trained.

5

Figure 2: Height function ﬁltration of a “clean” (left, green points) and a “noisy” (right, blue points) shape
along direction d = (0, −1)(cid:62). This example demonstrates the insensitivity of homology towards noise, as the
added noise only (1) slightly shifts the dominant points (upper left corner) and (2) produces additional points
close to the diagonal, which have little impact on the Wasserstein distance and the output of our layer.

5 Experiments

To demonstrate the versatility of the proposed approach, we present experiments with two totally
different types of data: (1) 2D shapes of objects, represented as binary images and (2) social network
graphs, given by their adjacency matrix. In both cases, the learning task is classiﬁcation. In each
experiment we ensured a balanced group size (per label) and used a 90/10 random training/test
split; all reported results are averaged over ﬁve runs with ﬁxed ν = 0.1. In practice, points in input
diagrams were thresholded at 0.01 for computational reasons. Additionally, we conducted a reference
experiment on all datasets using simple vectorization (see Sec. 5.3) of the persistence diagrams in
combination with a linear SVM.
Implementation. All experiments were implemented in PyTorch3, using DIPHA4 and Perseus [23].
Source code is publicly-available at https://github.com/c-hofer/nips2017.

5.1 Classiﬁcation of 2D object shapes

We apply persistent homology combined with our proposed input layer to two different datasets of
binary 2D object shapes: (1) the Animal dataset, introduced in [3] which consists of 20 different
animal classes, 100 samples each; (2) the MPEG-7 dataset which consists of 70 classes of different
object/animal contours, 20 samples each (see [21] for more details).

Filtration. The requirements to use persistent homology on 2D shapes are twofold: First, we need
to assign a simplicial complex to each shape; second, we need to appropriately ﬁltrate the complex.
While, in principle, we could analyze contour features, such as curvature, and choose a sublevel set
ﬁltration based on that, such a strategy requires substantial preprocessing of the discrete data (e.g.,
smoothing). Instead, we choose to work with the raw pixel data and leverage the persistent homology
transform, introduced by Turner et al. [29]. The ﬁltration in that case is based on sublevel sets of
the height function, computed from multiple directions (see Fig. 2). Practically, this means that we
directly construct a simplicial complex from the binary image. We set K0 as the set of all pixels
which are contained in the object. Then, a 1-simplex [p0, p1] is in the 1-skeleton K1 iff p0 and p1
are 4–neighbors on the pixel grid. To ﬁltrate the constructed complex, we deﬁne by b the barycenter
of the object and with r the radius of its bounding circle around b. Finally, we deﬁne, for [p]
K0
and d
. Function values are lifted to K1 by
b, d
(cid:105)
taking the maximum, cf. Sec. 2. Finally, let di be the 32 equidistantly distributed directions in S1,
Di)32
starting from (1, 0). For each shape, we get a vector of persistence diagrams (
Di is the
0-th diagram obtained by ﬁltration along di. As most objects do not differ in homology groups of
higher dimensions (> 0), we did not use the corresponding persistence diagrams.

S1, the ﬁltration function by f ([p]) = 1/r

i=1 where

· (cid:104)

−

∈

∈

p

Network architecture. While the full network is listed in the supplementary material (Fig. 6), the
key architectural choices are: 32 independent input branches, i.e., one for each ﬁltration direction.
Further, the i-th branch gets, as input, the vector of persistence diagrams from directions di−1, di
and di+1. This is a straightforward approach to capture dependencies among the ﬁltration directions.
We use cross-entropy loss to train the network for 400 epochs, using stochastic gradient descent
(SGD) with mini-batches of size 128 and an initial learning rate of 0.1 (halved every 25-th epoch).

3https://github.com/pytorch/pytorch
4https://bitbucket.org/dipha/dipha

6

MPEG-7

Animal

‡Skeleton paths
‡Class segment sets
†ICS
†BCF

Ours

86.7
90.9
96.6
97.2

91.8

67.9
69.7
78.4
83.4

69.5

Figure 3: Left: some examples from the MPEG-7 (bottom) and Animal (top) datasets. Right: Classiﬁcation
results, compared to the two best (†) and two worst (‡) results reported in [30].

Results. Fig. 3 shows a selection of 2D object shapes from both datasets, together with the obtained
classiﬁcation results. We list the two best (
) results as reported in [30]. While,
†
on the one hand, using topological signatures is below the state-of-the-art, the proposed architecture
is still better than other approaches that are speciﬁcally tailored to the problem. Most notably, our
approach does not require any speciﬁc data preprocessing, whereas all other competitors listed in
Fig. 3 require, e.g., some sort of contour extraction. Furthermore, the proposed architecture readily
S2. Fig. 4 (Right) shows an exemplary
generalizes to 3D with the only difference that in this case di ∈
visualization of the position of the learned structure elements for the Animal dataset.

) and two worst (

‡

5.2 Classiﬁcation of social network graphs

In this experiment, we consider the problem of graph classiﬁcation, where vertices are unlabeled
= (V, E), where V denotes the set of
and edges are undirected. That is, a graph
vertices and E denotes the set of edges. We evaluate our approach on the challenging problem of
social network classiﬁcation, using the two largest benchmark datasets from [31], i.e., reddit-5k
(5 classes, 5k graphs) and reddit-12k (11 classes,
12k graphs). Each sample in these datasets
represents a discussion graph and the classes indicate subreddits (e.g., worldnews, video, etc.).

is given by

≈

G

G

V

[v]

= (V, E) is straightforward: we set
Filtration. The construction of a simplicial complex from
K0 =
. We choose a very simple ﬁltration based on
}
the vertex degree, i.e., the number of incident edges to a vertex v
V . Hence, for [v0]
K0 we get
f ([v0]) = deg(v0)/ maxv∈V deg(v) and again lift f to K1 by taking the maximum. Note that chain
groups are trivial for dimension > 1, hence, all features in dimension 1 are essential.

v0, v1} ∈
{

[v0, v1] :
{

and K1 =

E

∈

∈

∈

G

{

}

Network architecture. Our network has four input branches: two for each dimension (0 and 1) of
the homological features, split into essential and non-essential ones, see Sec. 2. We train the network
for 500 epochs using SGD and cross-entropy loss with an initial learning rate of 0.1 (reddit_5k), or
0.4 (reddit_12k). The full network architecture is listed in the supplementary material (Fig. 7).

Results. Fig. 5 (right) compares our proposed strategy to state-of-the-art approaches from the
literature. In particular, we compare against (1) the graphlet kernel (GK) and deep graphlet kernel
(DGK) results from [31], (2) the Patchy-SAN (PSCN) results from [24] and (3) a recently reported
graph-feature + random forest approach (RF) from [4]. As we can see, using topological signatures
in our proposed setting considerably outperforms the current state-of-the-art on both datasets. This is
an interesting observation, as PSCN [24] for instance, also relies on node degrees and an extension of
the convolution operation to graphs. Further, the results reveal that including essential features is key
to these improvements.

5.3 Vectorization of persistence diagrams

D

we calculate its persistence, i.e., d

Here, we brieﬂy present a reference experiment we conducted following Bendich et al. [5]. The idea
is to directly use the persistence diagrams as features via vectorization. For each point (b, d) in a
persistence diagram
b. We then sort the calculated persistences
by magnitude from high to low and take the ﬁrst N values. Hence, we get, for each persistence
diagram, a vector of dimension N (if
< N , we pad with zero). We used this technique
on all four data sets. As can be seen from the results in Table 4 (averaged over 10 cross-validation
runs), vectorization performs poorly on MPEG-7 and Animal but can lead to competitive rates on
reddit-5k and reddit-12k. Nevertheless, the obtained performance is considerably inferior to our
proposed approach.

|D \

∆

−

|

7

N

MPEG-7
Animal
reddit-5k
reddit-12k

5

81.8
48.8
37.1
24.2

10

82.3
50.0
38.2
24.6

20

79.7
46.2
39.7
27.9

40

74.5
42.4
42.1
29.8

80

68.2
39.3
43.8
31.5

160

64.4
36.0
45.2
31.6

Ours

91.8
69.5
54.5
44.5

N ) persistence diagrams
Figure 4: Left: Classiﬁcation accuracies for a linear SVM trained on vectorized (in R
(see Sec. 5.3). Right: Exemplary visualization of the learned structure elements (in 0-th dimension) for the
Animal dataset and ﬁltration direction d = (−1, 0)(cid:62). Centers of the learned elements are marked in blue.

reddit-5k

reddit-12k

GK [31]
DGK [31]
PSCN [24]
RF [4]

Ours (w/o essential)
Ours (w/ essential)

41.0
41.3
49.1
50.9

49.1
54.5

31.8
32.2
41.3
42.7

38.5
44.5

Figure 5: Left: Illustration of graph ﬁltration by vertex degree, i.e., f ≡ deg (for different choices of ai, see
Sec. 2). Right: Classiﬁcation results as reported in [31] for GK and DGK, Patchy-SAN (PSCN) as reported in
.
[24] and feature-based random-forest (RF) classiﬁcation from [4].

Finally, we remark that in both experiments, tests with the kernel of [27] turned out to be computa-
tionally impractical, (1) on shape data due to the need to evaluate the kernel for all ﬁltration directions
and (2) on graphs due the large number of samples and the number of points in each diagram.

6 Discussion

We have presented, to the best of our knowledge, the ﬁrst approach towards learning task-optimal
stable representations of topological signatures, in our case persistence diagrams. Our particular
realization of this idea, i.e., as an input layer to deep neural networks, not only enables us to learn with
topological signatures, but also to use them as additional (and potentially complementary) inputs to
existing deep architectures. From a theoretical point of view, we remark that the presented structure
elements are not restricted to exponential functions, so long as the conditions of Lemma 1 are met.
One drawback of the proposed approach, however, is the artiﬁcial bending of the persistence axis (see
Fig. 1) by a logarithmic transformation; in fact, other strategies might be possible and better suited
in certain situations. A detailed investigation of this issue is left for future work. From a practical
perspective, it is also worth pointing out that, in principle, the proposed layer could be used to handle
any kind of input that comes in the form of multisets (of Rn), whereas previous works only allow
to handle sets of ﬁxed size (see Sec. 1). In summary, we argue that our experiments show strong
evidence that topological features of data can be beneﬁcial in many learning tasks, not necessarily to
replace existing inputs, but rather as a complementary source of discriminative information.

8

A Technical results

Lemma 3. Let α

i)

lim
x→0

ln(x)
x

·

R+, β

R, γ

∈
∈
∈
e−α(ln(x)γ+β)2 = 0

R+. We have

ii)

lim
x→0

1
x ·

e−α(ln(x)γ+β)2 = 0 .

Proof. We omit the proof for brevity (see supplementary material for details), but remark that only
(i) needs to be shown as (ii) follows immediately.

B Proofs

D \

E \

E0 =
D

Proof of Lemma 1. Let ϕ be a bijection between

) and let
and
∆. To show the result of Eq. (5), we consider the following decomposition:

which realizes wq
1(

D

D

E

E

,

∆,

D0 =

= ϕ−1(
= (ϕ−1(
(cid:124)

E0)
∪
E0)
(cid:123)(cid:122)
A

ϕ−1(∆)
∆)
(cid:125)

∪

E0)
(cid:123)(cid:122)
B
Except for the term D, all sets are ﬁnite. In fact, ϕ realizes the Wasserstein distance wq
ϕ(cid:12)
= id. Therefore, s(x) = s(ϕ(x)) = 0 for x
(cid:12)D
∈
in the summation and it sufﬁces to consider E = A

C. It follows that

(ϕ−1(∆)
(cid:124)
(cid:123)(cid:122)
C

(ϕ−1(∆)
(cid:124)
(cid:123)(cid:122)
D

(ϕ−1(
(cid:124)

∆)
(cid:125)

∆)
(cid:125)

⊂

∪

∩

∪

∩

\

\

∆)
(cid:125)

1 which implies
∆. Consequently, we can ignore D

(6)

∪

D since D
B
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∪
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:88)

x∈E

(cid:88)

−

x∈D

(cid:88)

x∈E

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

s(x)

s(ϕ(x))

s(x)

s(ϕ(x))

(cid:88)

(cid:12)
(cid:12)
(cid:12)
s(ϕ(x))
(cid:12)
(cid:12) ≤
||q = Ks ·

ϕ(x)

x∈E

−

s(x)

(cid:88)

x∈E

−

x

||

(cid:88)

x∈D

s(x)
|

−

s(ϕ(x))

|

x∈D

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Ks ·

(cid:88)

x∈E

=

=

≤

x

||

−

ϕ(x)

||q = Ks ·

wq
1(

,

) .

D

E

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
x∈D

s(x)

(cid:88)

y∈E

−

(cid:12)
(cid:12)
(cid:12)
s(y)
(cid:12)
(cid:12)
(cid:12)

Proof of Lemma 2. Since sµ,σ,ν is deﬁned differently for x1 ∈
distinguish these two cases. In the following x0 ∈
(1) x1 ∈

): The partial derivative w.r.t. xi is given as

R.

[ν,

∞

[ν,

) and x1 ∈

∞

(0, ν), we need to

(cid:18) ∂
∂xi

(cid:19)

sµ,σ,ν

(x0, x1) = C

i (xi−µi)2(cid:19)

e−σ2

(x0, x1)

·

(cid:18) ∂
∂xi
e−σ2

i (xi−µi)2

= C

(
·
−
) which is not dependent on xi. For all cases, i.e., x0 → ∞

i )(xi −

µi) ,

·

2σ2

, x0 →

(7)

where C is just the part of exp(
·

, it holds that Eq. (7)

0.

→

and x1 → ∞

−∞
(2) x1 ∈
behaviour for x0 → ∞
(cid:19)
(cid:18) ∂
∂x1

sµ,σ,ν

(0, ν): The partial derivative w.r.t. x0 is similar to Eq. (7) with the same asymptotic

and x0 → −∞
(x0, x1) = C

. However, for the partial derivative w.r.t. x1 we get
(cid:18) ∂
∂x1
e( ... )

ν )ν+ν−µ1)2 (cid:19)

(x0, x1)

e−σ2

ν + ν

1 (ln(

ln

(cid:17)

(cid:16)

(cid:17)

x1

µ1

2σ2
1)

ν
x1

·

·

·

= C

(
−

·

·
(cid:16) x1
ν

(cid:17)

(cid:18)

ln

·

(cid:123)(cid:122)
(a)

(cid:16) x1
ν
ν
x1

·

(cid:19)

(cid:125)

= C (cid:48)

e( ... )

(cid:16)

·

(cid:124)

−

−

+(ν

µ1)

e( ... )

(cid:17)

.

1
x1
(cid:125)

·

(cid:124)

·
(cid:123)(cid:122)
(b)

(8)

As x1 →
Eq. (8)
→
we conclude that they are absolutely bounded.

0, we can invoke Lemma 4 (i) to handle (a) and Lemma 4 (ii) to handle (b); conclusively,
0. As the partial derivatives w.r.t. xi are continuous and their limits are 0 on R, R+, resp.,

9

References

[1] H. Adams, T. Emerson, M. Kirby, R. Neville, C. Peterson, P. Shipman, S. Chepushtanova, E. Hanson,
F. Motta, and L. Ziegelmeier. Persistence images: A stable vector representation of persistent homology.
JMLR, 18(8):1–35, 2017.

[2] A. Adcock, E. Carlsson, and G. Carlsson. The ring of algebraic functions on persistence bar codes. CoRR,

2013. https://arxiv.org/abs/1304.0530.

[3] X. Bai, W. Liu, and Z. Tu. Integrating contour and skeleton for shape classiﬁcation. In ICCV Workshops,

2009.

[4] I. Barnett, N. Malik, M.L. Kuijjer, P.J. Mucha, and J.-P. Onnela. Feature-based classiﬁcation of networks.

CoRR, 2016. https://arxiv.org/abs/1610.05868.

[5] P. Bendich, J.S. Marron, E. Miller, A. Pieloch, and S. Skwerer. Persistent homology analysis of brain artery

trees. Ann. Appl. Stat, 10(2), 2016.

[6] P. Bubenik. Statistical topological data analysis using persistence landscapes. JMLR, 16(1):77–102, 2015.

[7] G. Carlsson. Topology and data. Bull. Amer. Math. Soc., 46:255–308, 2009.

[8] G. Carlsson, T. Ishkhanov, V. de Silva, and A. Zomorodian. On the local behavior of spaces of natural

images. IJCV, 76:1–12, 2008.

[9] F. Chazal, D. Cohen-Steiner, L. J. Guibas, F. Mémoli, and S. Y. Oudot. Gromov-Hausdorff stable signatures

for shapes using persistence. Comput. Graph. Forum, 28(5):1393–1403, 2009.

[10] F. Chazal, B.T. Fasy, F. Lecci, A. Rinaldo, and L. Wassermann. Stochastic convergence of persistence

landscapes and silhouettes. JoCG, 6(2):140–161, 2014.

[11] F. Chazal, L.J. Guibas, S.Y. Oudot, and P. Skraba. Persistence-based clustering in Riemannian manifolds.

[12] D. Cohen-Steiner, H. Edelsbrunner, and J. Harer. Stability of persistence diagrams. Discrete Comput.

J. ACM, 60(6):41–79, 2013.

Geom., 37(1):103–120, 2007.

[13] D. Cohen-Steiner, H. Edelsbrunner, J. Harer, and Y. Mileyko. Lipschitz functions have Lp-stable persistence.

Found. Comput. Math., 10(2):127–139, 2010.

[14] H. Edelsbrunner and J. L. Harer. Computational Topology : An Introduction. American Mathematical

Society, 2010.

[15] H. Edelsbrunner, D. Letcher, and A. Zomorodian. Topological persistence and simpliﬁcation. Discrete

Comput. Geom., 28(4):511–533, 2002.

[16] A. Hatcher. Algebraic Topology. Cambridge University Press, Cambridge, 2002.

[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.

[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural

networks. In NIPS, 2012.

analysis. In ICML, 2016.

perspective. In NIPS, 2015.

contour. In CVPR, 2000.

[19] G. Kusano, K. Fukumizu, and Y. Hiraoka. Persistence weighted Gaussian kernel for topological data

[20] R. Kwitt, S. Huber, M. Niethammer, W. Lin, and U. Bauer. Statistical topological data analysis - a kernel

[21] L. Latecki, R. Lakamper, and T. Eckhardt. Shape descriptors for non-rigid shapes with a single closed

[22] C. Li, M. Ovsjanikov, and F. Chazal. Persistence-based structural recognition. In CVPR, 2014.

[23] K. Mischaikow and V. Nanda. Morse theory for ﬁltrations and efﬁcient computation of persistent homology.

Discrete Comput. Geom., 50(2):330–353, 2013.

[24] M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. In ICML,

[25] C.R. Qi, H. Su, K. Mo, and L.J. Guibas. PointNet: Deep learning on point sets for 3D classiﬁcation and

2016.

segmentation. In CVPR, 2017.

[26] S. Ravanbakhsh, S. Schneider, and B. Póczos. Deep learning with sets and point clouds. In ICLR, 2017.

[27] R. Reininghaus, U. Bauer, S. Huber, and R. Kwitt. A stable multi-scale kernel for topological machine

learning. In CVPR, 2015.

[28] G. Singh, F. Memoli, T. Ishkhanov, G. Sapiro, G. Carlsson, and D.L. Ringach. Topological analysis of

population activity in visual cortex. J. Vis., 8(8), 2008.

10

[29] K. Turner, S. Mukherjee, and D. M. Boyer. Persistent homology transform for modeling shapes and

[30] X. Wang, B. Feng, X. Bai, W. Liu, and L.J. Latecki. Bag of contour fragments for robust shape classiﬁcation.

surfaces. Inf. Inference, 3(4):310–344, 2014.

Pattern Recognit., 47(6):2116–2125, 2014.

[31] P. Yanardag and S.V.N. Vishwanathan. Deep graph kernels. In KDD, 2015.

This supplementary material contains technical details that were left-out in the original submission
for brevity. When necessary, we refer to the submitted manuscript.

C Additional proofs

In the manuscript, we omitted the proof for the following technical lemma. For completeness, the
lemma is repeated and its proof is given below.
R+, β
Lemma 4. Let α

R+. We have

R and γ

∈

∈
e−α(ln(x)γ+β)2 = 0

∈

·

e−α(ln(x)γ+β)2 = 0 .

(i) lim
x→0

ln(x)
x

(ii) lim
x→0

1
x ·

Proof. We only need to prove the ﬁrst statement, as the second follows immediately. Hence, consider

lim
x→0

ln(x)
x

·

e−α(ln(x)γ+β)2

ln(x)

e− ln(x)

e−α(ln(x)γ+β)2

= lim
x→0

= lim
x→0

ln(x)

= lim
x→0

ln(x)

·

·

·

·

e−α(ln(x)γ+β)2−ln(x)
eα(ln(x)γ+β)2+ln(x)(cid:17)−1

(cid:16)

(∗)
= lim
x→0

∂
∂x

ln(x)

(cid:18) ∂
∂x

·

eα(ln(x)γ+β)2+ln(x)

(cid:19)−1

(cid:18)

(cid:18)

eα(ln(x)γ+β)2+ln(x)

1
x ·
(cid:16)
eα(ln(x)γ+β)2+ln(x)

·

2α(ln(x)γ + β)

(cid:19)(cid:19)−1

γ
x

+

1
x
(cid:17)−1

(2α(ln(x)γ + β)γ + 1)

·

= lim
x→0

= lim
x→0

= 0

).
where we use de l’Hôpital’s rule in (
∗

D Network architectures

2D object shape classiﬁcation. Fig. 6 illustrates the network architecture used for 2D object shape
classiﬁcation in [Manuscript, Sec. 5.1]. Note that the persistence diagrams from three consecutive
ﬁltration directions di share one input layer. As we use 32 directions, we have 32 input branches.
3 and a stride of 1. The max-pooling
The convolution operation operates with kernels of size 1
operates along the ﬁlter dimension. For better readability, we have added the output size of certain
layers. We train with the network with stochastic gradient descent (SGD) and a mini-batch size of
128 for 300 epochs. Every 20th epoch, the learning rate (initially set to 0.1) is halved.

×

×

1

11

Graph classiﬁcation. Fig. 7 illustrates the network architecture used for graph classiﬁcation in
Sec. 5.2. In detail, we have 3 input branches: ﬁrst, we split 0-dimensional features into essential
and non-essential ones; second, since there are only essential features in dimension 1 (see Sec. 5.2,
Filtration) we do not need a branch for non-essential features. We train the network using SGD with
mini-batches of size 128 for 300 epochs. The initial learning rate is set to 0.1 (reddit_5k) and 0.4
(reddit_12k), resp., and halved every 20th epochs.

D.1 Technical handling of essential features

In case of of 2D object shapes, the death times of essential features are mapped to the max. ﬁltration
value and kept in the original persistence diagrams. In fact, for Animal and MPEG-7, there is always
only one connected component and consequently only one essential feature in dimension 0 (i.e., it
does not make sense to handle this one point in a separate input branch).

In case of social network graphs, essential features are mapped to the real line (using their birth time)
and handled in separate input branches (see Fig. 7) with 1D structure elements. This is in contrast to
the 2D object shape experiments, as we might have many essential features (in dimensions 0 and 1)
that require handling in separate input branches.

12

Figure 6: 2D object shape classiﬁcation network architecture.

13

Figure 7: Graph classiﬁcation network architecture.

14

8
1
0
2
 
b
e
F
 
6
1
 
 
]

V
C
.
s
c
[
 
 
3
v
1
4
0
4
0
.
7
0
7
1
:
v
i
X
r
a

Deep Learning with Topological Signatures

Christoph Hofer
Department of Computer Science
University of Salzburg, Austria
chofer@cosy.sbg.ac.at

Roland Kwitt
Department of Computer Science
University of Salzburg, Austria
Roland.Kwitt@sbg.ac.at

Marc Niethammer
UNC Chapel Hill, NC, USA
mn@cs.unc.edu

Andreas Uhl
Department of Computer Science
University of Salzburg, Austria
uhl@cosy.sbg.ac.at

Abstract

Inferring topological and geometrical information from data can offer an alternative
perspective on machine learning problems. Methods from topological data analysis,
e.g., persistent homology, enable us to obtain such information, typically in the form
of summary representations of topological features. However, such topological
signatures often come with an unusual structure (e.g., multisets of intervals) that is
highly impractical for most machine learning techniques. While many strategies
have been proposed to map these topological signatures into machine learning
compatible representations, they suffer from being agnostic to the target learning
task. In contrast, we propose a technique that enables us to input topological
signatures to deep neural networks and learn a task-optimal representation during
training. Our approach is realized as a novel input layer with favorable theoretical
properties. Classiﬁcation experiments on 2D object shapes and social network
graphs demonstrate the versatility of the approach and, in case of the latter, we
even outperform the state-of-the-art by a large margin.

1

Introduction

Methods from algebraic topology have only recently emerged in the machine learning community,
most prominently under the term topological data analysis (TDA) [7]. Since TDA enables us to
infer relevant topological and geometrical information from data, it can offer a novel and potentially
beneﬁcial perspective on various machine learning problems. Two compelling beneﬁts of TDA
are (1) its versatility, i.e., we are not restricted to any particular kind of data (such as images,
sensor measurements, time-series, graphs, etc.) and (2) its robustness to noise. Several works have
demonstrated that TDA can be beneﬁcial in a diverse set of problems, such as studying the manifold
of natural image patches [8], analyzing activity patterns of the visual cortex [28], classiﬁcation of 3D
surface meshes [27, 22], clustering [11], or recognition of 2D object shapes [29].
Currently, the most widely-used tool from TDA is persistent homology [15, 14]. Essentially1,
persistent homology allows us to track topological changes as we analyze data at multiple “scales”.
As the scale changes, topological features (such as connected components, holes, etc.) appear and
disappear. Persistent homology associates a lifespan to these features in the form of a birth and
a death time. The collection of (birth, death) tuples forms a multiset that can be visualized as a
persistence diagram or a barcode, also referred to as a topological signature of the data. However,
leveraging these signatures for learning purposes poses considerable challenges, mostly due to their

1We will make these concepts more concrete in Sec. 2.

Figure 1: Illustration of the proposed network input layer for topological signatures. Each signature, in the
form of a persistence diagram D ∈ D (left), is projected w.r.t. a collection of structure elements. The layer’s
+ is set a-priori and
learnable parameters θ are the locations µi and the scales σi of these elements; ν ∈ R
meant to discount the impact of points with low persistence (and, in many cases, of low discriminative power).
The layer output y is a concatenation of the projections. In this illustration, N = 2 and hence y = (y1, y2)(cid:62).

unusual structure as a multiset. While there exist suitable metrics to compare signatures (e.g., the
Wasserstein metric), they are highly impractical for learning, as they require solving optimal matching
problems.

Related work. In order to deal with these issues, several strategies have been proposed. In [2] for
instance, Adcock et al. use invariant theory to “coordinatize” the space of barcodes. This allows to
map barcodes to vectors of ﬁxed size which can then be fed to standard machine learning techniques,
such as support vector machines (SVMs). Alternatively, Adams et al. [1] map barcodes to so-called
persistence images which, upon discretization, can also be interpreted as vectors and used with
standard learning techniques. Along another line of research, Bubenik [6] proposes a mapping
of barcodes into a Banach space. This has been shown to be particularly viable in a statistical
context (see, e.g., [10]). The mapping outputs a representation referred to as a persistence landscape.
Interestingly, under a speciﬁc choice of parameters, barcodes are mapped into L2(R2) and the
inner-product in that space can be used to construct a valid kernel function. Similar, kernel-based
techniques, have also recently been studied by Reininghaus et al. [27], Kwitt et al. [20] and Kusano
et al. [19].

While all previously mentioned approaches retain certain stability properties of the original repre-
sentation with respect to common metrics in TDA (such as the Wasserstein or Bottleneck distances),
they also share one common drawback: the mapping of topological signatures to a representation that
is compatible with existing learning techniques is pre-deﬁned. Consequently, it is ﬁxed and therefore
agnostic to any speciﬁc learning task. This is clearly suboptimal, as the eminent success of deep
neural networks (e.g., [18, 17]) has shown that learning representations is a preferable approach.
Furthermore, techniques based on kernels [27, 20, 19] for instance, additionally suffer scalability
issues, as training typically scales poorly with the number of samples (e.g., roughly cubic in case of
kernel-SVMs). In the spirit of end-to-end training, we therefore aim for an approach that allows to
learn a task-optimal representation of topological signatures. We additionally remark that, e.g., Qi et
al. [25] or Ravanbakhsh et al. [26] have proposed architectures that can handle sets, but only with
ﬁxed size. In our context, this is impractical as the capability of handling sets with varying cardinality
is a requirement to handle persistent homology in a machine learning setting.

Contribution. To realize this idea, we advocate a novel input layer for deep neural networks that
takes a topological signature (in our case, a persistence diagram), and computes a parametrized
projection that can be learned during network training. Speciﬁcally, this layer is designed such that
its output is stable with respect to the 1-Wasserstein distance (similar to [27] or [1]). To demonstrate
the versatility of this approach, we present experiments on 2D object shape classiﬁcation and the
classiﬁcation of social network graphs. On the latter, we improve the state-of-the-art by a large
margin, clearly demonstrating the power of combining TDA with deep learning in this context.

2 Background

For space reasons, we only provide a brief overview of the concepts that are relevant to this work and
refer the reader to [16] or [14] for further details.

Homology. The key concept of homology theory is to study the properties of some object X by
means of (commutative) algebra. In particular, we assign to X a sequence of modules C0, C1, . . .

2

ker ∂n. A structure
which are connected by homomorphisms ∂n : Cn →
of this form is called a chain complex and by studying its homology groups Hn = ker ∂n/ im ∂n+1
we can derive properties of X.

Cn−1 such that im ∂n+1 ⊆

A prominent example of a homology theory is simplicial homology. Throughout this work, it is
the used homology theory and hence we will now concretize the already presented ideas. Let K
be a simplicial complex and Kn its n-skeleton. Then we set Cn(K) as the vector space gener-
ated (freely) by Kn over Z/2Z2. The connecting homomorphisms ∂n : Cn(K)
Cn−1(K) are
→
called boundary operators. For a simplex σ = [x0, . . . , xn]
Kn, we deﬁne them as ∂n(σ) =
(cid:80)n
i=0[x0, . . . , xi−1, xi+1, . . . , xn] and linearly extend this to Cn(K), i.e., ∂n((cid:80) σi) = (cid:80) ∂n(σi).
Persistent homology. Let K be a simplicial complex and (K i)m
i=0 a sequence of simplicial com-
K m = K. Then, (K i)m
i=0 is called a ﬁltration of K. If we
plexes such that
∅
use the extra information provided by the ﬁltration of K, we obtain the following sequence of chain
complexes (left),

⊆ · · · ⊆

= K 0

K 1

⊆

∈

n = Cn(K i
where C i
homology groups, deﬁned by

n) and ι denotes the inclusion. This then leads to the concept of persistent

H i,j

n/(im ∂j

n = ker ∂i
The ranks, βi,j
n , of these homology groups (i.e., the n-th persistent Betti numbers),
capture the number of homological features of dimensionality n (e.g., connected components for
n = 0, holes for n = 1, etc.) that persist from i to (at least) j. In fact, according to [14, Fundamental
Lemma of Persistent Homology], the quantities

n = rank H i,j

ker ∂i
n)

n+1 ∩

j .

for

≤

i

n = (βi,j−1
µi,j

n

βi,j
n )

(βi−1,j−1

n

βi−1,j
n

)

for

i < j

−

−

−

(1)

encode all the information about the persistent Betti numbers of dimension n.

Topological signatures. A typical way to obtain a ﬁltration of K is to consider sublevel sets of a
function f : C0(K)
R. This function can be easily lifted to higher-dimensional chain groups of
K by

→

f ([v0, . . . , vn]) = max
{
, we obtain (Ki)m
|

i
≤
i=0 by setting K0 =

f ([vi]) : 0

≤
∅

≤

f (C0(K))
|
m, where a1 <

, ai]) for
Given m =
< am is the sorted sequence of values of f (C0(K)). If we construct
1
i
a multiset such that, for i < j, the point (ai, aj) is inserted with multiplicity µi,j
n , we effectively
encode the persistent homology of dimension n w.r.t. the sublevel set ﬁltration induced by f . Upon
adding diagonal points with inﬁnite multiplicity, we obtain the following structure:

−∞

· · ·

≤

.

n
and Ki = f −1((

}

Deﬁnition 1 (Persistence diagram). Let ∆ =
diagonal R2
R2 : x0 = x1}
(x0, x1)
∆ =
{
∈
R2 : x1 > x0}
R2
(x0, x1)
∈
{

(cid:63) =

R2

x
{

∆ : mult(x) =

be the multiset of the
, where mult denotes the multiplicity function and let
, is a multiset of the form

∞}

∈

. A persistence diagram,

=

x : x

D

{

R

2
(cid:63)} ∪

∈

D
∆ .

We denote by D the set of all persistence diagrams of the form

∆

<

.

|D \

|

∞

For a given complex K of dimension nmax and a function f (of the discussed form), we can interpret
persistent homology as a mapping (K, f )
(
Di is the diagram of
D0, . . . ,
dimension i and nmax the dimension of K. We can additionally add a metric structure to the space of
persistence diagrams by introducing the notion of distances.

Dnmax−1), where

(cid:55)→

2Simplicial homology is not speciﬁc to Z/2Z, but it’s a typical choice, since it allows us to interpret n-chains

as sets of n-simplices.

3

E

(cid:33) 1

p

Deﬁnition 2 (Bottleneck, Wasserstein distance). For two persistence diagrams
their Bottleneck (w∞) and Wasserstein (wq

p) distances by

D

and

, we deﬁne

(cid:32)

(cid:88)

x∈D

.

D → E

w∞(

,

D

E

) = inf
η

sup
x∈D ||

x

−

η(x)

∞ and wq
p(
||

,

D

E

) = inf
η

x

||

−

η(x)

p
q
||

,

(2)

where p, q

N and the inﬁmum is taken over all bijections η :

∈

Essentially, this facilitates studying stability/continuity properties of topological signatures w.r.t.
metrics in the ﬁltration or complex space; we refer the reader to [12],[13], [9] for a selection of
important stability results.
Remark. By setting µi,∞
n −
referred to as essential. This change can be lifted to D by setting R2
x1 > x0}

, we extend Eq. (1) to features which never disappear, also
) :

. In Sec. 5, we will see that essential features can offer discriminative information.

(x0, x1)
{

n = βi,m

βi−1,m
n

∪ {∞}

(cid:63) =

(R

×

R

∈

3 A network layer for topological signatures

x

∈

D

D

{
such that points on R2

In this section, we introduce the proposed (parametrized) network layer for topological signatures
(in the form of persistence diagrams). The key idea is to take any
and deﬁne a projection w.r.t. a
collection (of ﬁxed size N ) of structure elements.
In the following, we set R+ :=
R : x > 0
0
R : x
, resp., and start by
}
}
∆ lie on the x-axis, see Fig. 1. The y-axis can then be
rotating points of
interpreted as the persistence of features. Formally, we let b0 and b1 be the unit vectors in directions
(1, 1)(cid:62) and (
).
x, b1(cid:105)
R
(cid:63) ∪
(cid:104)
−
∆ clock-wise by π/4. We will later see that this construction is beneﬁcial
This rotates points in R(cid:63) ∪
for a closer analysis of the layers’ properties. Similar to [27, 19], we choose exponential functions as
structure elements, but other choices are possible (see Lemma 1). Differently to [27, 19], however,
our structure elements are not at ﬁxed locations (i.e., one element per point in
), but their locations
and scales are learned during training.
Deﬁnition 3. Let µ = (µ0, µ1)(cid:62)

1, 1)(cid:62) and deﬁne a mapping ρ : R2

+
0 such that x

R+, σ = (σ0, σ1)

R+. We deﬁne

R+ and ν

x, b0(cid:105)

and R

+
0 :=

∆ →

x
{

R+

R2

R2

(
(cid:104)

(cid:55)→

≥

×

D

R

R

∈

,

×

∈

as follows:

∈

×
sµ,σ,ν : R

∈
R

R

+
0 →

×

e−σ2

0 (x0−µ0)2−σ2

1 (x1−µ1)2

,






0,

x1 ∈
, x1 ∈

[ν,

)

∞

(0, ν)

x1 = 0

sµ,σ,ν

(cid:0)(x0, x1)(cid:1) =

e−σ2

0 (x0−µ0)2−σ2

1 (ln(

x1
ν )ν+ν−µ1)2

A persistence diagram

D

is then projected w.r.t. sµ,σ,ν via
(cid:88)

Sµ,σ,ν : D

R,

→

D (cid:55)→

x∈D

sµ,σ,ν(ρ(x)) .

Remark. Note that sµ,σ,ν is continuous in x1 as
(cid:16) x
ν

x = lim
x→ν

lim
x1→0

lim
x→ν

ν + ν

and

ln

(cid:17)

sµ,σ,ν

(cid:0)(x0, x1)(cid:1) = 0 = sµ,σ,ν

(cid:0)(x0, 0)(cid:1)

and e(·) is continuous. Further, sµ,σ,ν is differentiable on R

R+, since

1 = lim
x→ν+

∂x1
∂x1

(x) and

lim
x→ν−

∂ (cid:0)ln (cid:0) x1

ν
∂x1

(x) = lim
x→ν−

= 1 .

ν
x

×
(cid:1) ν + ν(cid:1)

Also note that we use the log-transform in Eq. (4) to guarantee that sµ,σ,ν satisﬁes the conditions of
Lemma 1; this is, however, only one possible choice. Finally, given a collection of structure elements
Sµi,σi,ν, we combine them to form the output of the network layer.

(3)

(4)

4

Deﬁnition 4. Let N

N, θ = (µi, σi)N −1

∈

i=0 ∈
+
0 )N

(R

(cid:0)(R

×

D (cid:55)→

(R+

R+)

×
(cid:0)Sµi,σi,ν(

×
)(cid:1)N −1
i=0 .

D

θ,ν : D
S

→

as the concatenation of all N mappings deﬁned in Eq. (4).

R+)(cid:1)N and ν

R+. We deﬁne

∈

Importantly, a network layer implementing Def. 4 is trainable via backpropagation, as (1) sµi,σi,ν is
differentiable in µi, σi, (2) Sµi,σi,ν(
θ,ν is just a concatenation.

) is a ﬁnite sum of sµi,σi,ν and (3)

D

S

4 Theoretical properties

In this section, we demonstrate that the proposed layer is stable w.r.t. the 1-Wasserstein distance wq
1,
see Eq. (2). In fact, this claim will follow from a more general result, stating sufﬁcient conditions on
functions s : R2
Lemma 1. Let

0 such that a construction in the form of Eq. (3) is stable w.r.t. wq
1.

∆ →

(cid:63) ∪

R2

R

+

s : R

2
(cid:63) ∪

R

2
∆ →

+
0

R

(cid:107) · (cid:107)q and constant Ks

have the following properties:

(i) s is Lipschitz continuous w.r.t.
(ii) s(x(cid:1) = 0, for x

R2
∆

∈

Then, for two persistence diagrams

,

D

E ∈

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

x∈D

s(x)

−

D, it holds that
(cid:12)
(cid:12)
(cid:12)
s(y)
(cid:12)
(cid:12)
(cid:12)

Ks ·

≤

(cid:88)

y∈E

wq
1(

,

) .

D

E

(5)

Proof. see Appendix B

Remark. At this point, we want to clarify that Lemma 1 is not speciﬁc to sµ,σ,ν (e.g., as in Def. 3).
Rather, Lemma 1 yields sufﬁcient conditions to construct a w1-stable input layer. Our choice of
sµ,σ,ν is just a natural example that fulﬁls those requirements and, hence,
θ,ν is just one possible
representative of a whole family of input layers.

S

With the result of Lemma 1 in mind, we turn to the speciﬁc case of
S
properties w.r.t. wq
1. The following lemma is important in this context.
Lemma 2. sµ,σ,ν has absolutely bounded ﬁrst-order partial derivatives w.r.t. x0 and x1 on R

θ,ν and analyze its stability

R+.

×

Proof. see Appendix B

Theorem 1.

θ,ν is Lipschitz continuous with respect to wq
S

1 on D.

Proof. Lemma 2 immediately implies that sµ,σ,ν from Eq. (3) is Lipschitz continuous w.r.t
Consequently, s = sµ,σ,ν ◦
satisﬁed by construction. Hence, Sµ,σ,ν is Lipschitz continuous w.r.t. wq
Lipschitz in each coordinate and therefore Liptschitz continuous.

|| · ||q.
ρ satisﬁes property (i) from Lemma 1; property (ii) from Lemma 1 is
θ,ν is

1. Consequently,

S

Interestingly, the stability result of Theorem 1 is comparable to the stability results in [1] or [27]
(which are also w.r.t. wq
1 and in the setting of diagrams with ﬁnitely-many points). However, contrary
to previous works, if we would chop-off the input layer after network training, we would then have a
θ,ν of persistence diagrams that is speciﬁcally-tailored to the learning task on which the
mapping
S
network was trained.

5

Figure 2: Height function ﬁltration of a “clean” (left, green points) and a “noisy” (right, blue points) shape
along direction d = (0, −1)(cid:62). This example demonstrates the insensitivity of homology towards noise, as the
added noise only (1) slightly shifts the dominant points (upper left corner) and (2) produces additional points
close to the diagonal, which have little impact on the Wasserstein distance and the output of our layer.

5 Experiments

To demonstrate the versatility of the proposed approach, we present experiments with two totally
different types of data: (1) 2D shapes of objects, represented as binary images and (2) social network
graphs, given by their adjacency matrix. In both cases, the learning task is classiﬁcation. In each
experiment we ensured a balanced group size (per label) and used a 90/10 random training/test
split; all reported results are averaged over ﬁve runs with ﬁxed ν = 0.1. In practice, points in input
diagrams were thresholded at 0.01 for computational reasons. Additionally, we conducted a reference
experiment on all datasets using simple vectorization (see Sec. 5.3) of the persistence diagrams in
combination with a linear SVM.
Implementation. All experiments were implemented in PyTorch3, using DIPHA4 and Perseus [23].
Source code is publicly-available at https://github.com/c-hofer/nips2017.

5.1 Classiﬁcation of 2D object shapes

We apply persistent homology combined with our proposed input layer to two different datasets of
binary 2D object shapes: (1) the Animal dataset, introduced in [3] which consists of 20 different
animal classes, 100 samples each; (2) the MPEG-7 dataset which consists of 70 classes of different
object/animal contours, 20 samples each (see [21] for more details).

Filtration. The requirements to use persistent homology on 2D shapes are twofold: First, we need
to assign a simplicial complex to each shape; second, we need to appropriately ﬁltrate the complex.
While, in principle, we could analyze contour features, such as curvature, and choose a sublevel set
ﬁltration based on that, such a strategy requires substantial preprocessing of the discrete data (e.g.,
smoothing). Instead, we choose to work with the raw pixel data and leverage the persistent homology
transform, introduced by Turner et al. [29]. The ﬁltration in that case is based on sublevel sets of
the height function, computed from multiple directions (see Fig. 2). Practically, this means that we
directly construct a simplicial complex from the binary image. We set K0 as the set of all pixels
which are contained in the object. Then, a 1-simplex [p0, p1] is in the 1-skeleton K1 iff p0 and p1
are 4–neighbors on the pixel grid. To ﬁltrate the constructed complex, we deﬁne by b the barycenter
of the object and with r the radius of its bounding circle around b. Finally, we deﬁne, for [p]
K0
and d
. Function values are lifted to K1 by
b, d
(cid:105)
taking the maximum, cf. Sec. 2. Finally, let di be the 32 equidistantly distributed directions in S1,
Di)32
starting from (1, 0). For each shape, we get a vector of persistence diagrams (
Di is the
0-th diagram obtained by ﬁltration along di. As most objects do not differ in homology groups of
higher dimensions (> 0), we did not use the corresponding persistence diagrams.

S1, the ﬁltration function by f ([p]) = 1/r

i=1 where

· (cid:104)

−

∈

∈

p

Network architecture. While the full network is listed in the supplementary material (Fig. 6), the
key architectural choices are: 32 independent input branches, i.e., one for each ﬁltration direction.
Further, the i-th branch gets, as input, the vector of persistence diagrams from directions di−1, di
and di+1. This is a straightforward approach to capture dependencies among the ﬁltration directions.
We use cross-entropy loss to train the network for 400 epochs, using stochastic gradient descent
(SGD) with mini-batches of size 128 and an initial learning rate of 0.1 (halved every 25-th epoch).

3https://github.com/pytorch/pytorch
4https://bitbucket.org/dipha/dipha

6

MPEG-7

Animal

‡Skeleton paths
‡Class segment sets
†ICS
†BCF

Ours

86.7
90.9
96.6
97.2

91.8

67.9
69.7
78.4
83.4

69.5

Figure 3: Left: some examples from the MPEG-7 (bottom) and Animal (top) datasets. Right: Classiﬁcation
results, compared to the two best (†) and two worst (‡) results reported in [30].

Results. Fig. 3 shows a selection of 2D object shapes from both datasets, together with the obtained
classiﬁcation results. We list the two best (
) results as reported in [30]. While,
†
on the one hand, using topological signatures is below the state-of-the-art, the proposed architecture
is still better than other approaches that are speciﬁcally tailored to the problem. Most notably, our
approach does not require any speciﬁc data preprocessing, whereas all other competitors listed in
Fig. 3 require, e.g., some sort of contour extraction. Furthermore, the proposed architecture readily
S2. Fig. 4 (Right) shows an exemplary
generalizes to 3D with the only difference that in this case di ∈
visualization of the position of the learned structure elements for the Animal dataset.

) and two worst (

‡

5.2 Classiﬁcation of social network graphs

In this experiment, we consider the problem of graph classiﬁcation, where vertices are unlabeled
= (V, E), where V denotes the set of
and edges are undirected. That is, a graph
vertices and E denotes the set of edges. We evaluate our approach on the challenging problem of
social network classiﬁcation, using the two largest benchmark datasets from [31], i.e., reddit-5k
(5 classes, 5k graphs) and reddit-12k (11 classes,
12k graphs). Each sample in these datasets
represents a discussion graph and the classes indicate subreddits (e.g., worldnews, video, etc.).

is given by

≈

G

G

V

[v]

= (V, E) is straightforward: we set
Filtration. The construction of a simplicial complex from
K0 =
. We choose a very simple ﬁltration based on
}
the vertex degree, i.e., the number of incident edges to a vertex v
V . Hence, for [v0]
K0 we get
f ([v0]) = deg(v0)/ maxv∈V deg(v) and again lift f to K1 by taking the maximum. Note that chain
groups are trivial for dimension > 1, hence, all features in dimension 1 are essential.

v0, v1} ∈
{

[v0, v1] :
{

and K1 =

E

∈

∈

∈

G

{

}

Network architecture. Our network has four input branches: two for each dimension (0 and 1) of
the homological features, split into essential and non-essential ones, see Sec. 2. We train the network
for 500 epochs using SGD and cross-entropy loss with an initial learning rate of 0.1 (reddit_5k), or
0.4 (reddit_12k). The full network architecture is listed in the supplementary material (Fig. 7).

Results. Fig. 5 (right) compares our proposed strategy to state-of-the-art approaches from the
literature. In particular, we compare against (1) the graphlet kernel (GK) and deep graphlet kernel
(DGK) results from [31], (2) the Patchy-SAN (PSCN) results from [24] and (3) a recently reported
graph-feature + random forest approach (RF) from [4]. As we can see, using topological signatures
in our proposed setting considerably outperforms the current state-of-the-art on both datasets. This is
an interesting observation, as PSCN [24] for instance, also relies on node degrees and an extension of
the convolution operation to graphs. Further, the results reveal that including essential features is key
to these improvements.

5.3 Vectorization of persistence diagrams

D

we calculate its persistence, i.e., d

Here, we brieﬂy present a reference experiment we conducted following Bendich et al. [5]. The idea
is to directly use the persistence diagrams as features via vectorization. For each point (b, d) in a
persistence diagram
b. We then sort the calculated persistences
by magnitude from high to low and take the ﬁrst N values. Hence, we get, for each persistence
diagram, a vector of dimension N (if
< N , we pad with zero). We used this technique
on all four data sets. As can be seen from the results in Table 4 (averaged over 10 cross-validation
runs), vectorization performs poorly on MPEG-7 and Animal but can lead to competitive rates on
reddit-5k and reddit-12k. Nevertheless, the obtained performance is considerably inferior to our
proposed approach.

|D \

∆

−

|

7

N

MPEG-7
Animal
reddit-5k
reddit-12k

5

81.8
48.8
37.1
24.2

10

82.3
50.0
38.2
24.6

20

79.7
46.2
39.7
27.9

40

74.5
42.4
42.1
29.8

80

68.2
39.3
43.8
31.5

160

64.4
36.0
45.2
31.6

Ours

91.8
69.5
54.5
44.5

N ) persistence diagrams
Figure 4: Left: Classiﬁcation accuracies for a linear SVM trained on vectorized (in R
(see Sec. 5.3). Right: Exemplary visualization of the learned structure elements (in 0-th dimension) for the
Animal dataset and ﬁltration direction d = (−1, 0)(cid:62). Centers of the learned elements are marked in blue.

reddit-5k

reddit-12k

GK [31]
DGK [31]
PSCN [24]
RF [4]

Ours (w/o essential)
Ours (w/ essential)

41.0
41.3
49.1
50.9

49.1
54.5

31.8
32.2
41.3
42.7

38.5
44.5

Figure 5: Left: Illustration of graph ﬁltration by vertex degree, i.e., f ≡ deg (for different choices of ai, see
Sec. 2). Right: Classiﬁcation results as reported in [31] for GK and DGK, Patchy-SAN (PSCN) as reported in
.
[24] and feature-based random-forest (RF) classiﬁcation from [4].

Finally, we remark that in both experiments, tests with the kernel of [27] turned out to be computa-
tionally impractical, (1) on shape data due to the need to evaluate the kernel for all ﬁltration directions
and (2) on graphs due the large number of samples and the number of points in each diagram.

6 Discussion

We have presented, to the best of our knowledge, the ﬁrst approach towards learning task-optimal
stable representations of topological signatures, in our case persistence diagrams. Our particular
realization of this idea, i.e., as an input layer to deep neural networks, not only enables us to learn with
topological signatures, but also to use them as additional (and potentially complementary) inputs to
existing deep architectures. From a theoretical point of view, we remark that the presented structure
elements are not restricted to exponential functions, so long as the conditions of Lemma 1 are met.
One drawback of the proposed approach, however, is the artiﬁcial bending of the persistence axis (see
Fig. 1) by a logarithmic transformation; in fact, other strategies might be possible and better suited
in certain situations. A detailed investigation of this issue is left for future work. From a practical
perspective, it is also worth pointing out that, in principle, the proposed layer could be used to handle
any kind of input that comes in the form of multisets (of Rn), whereas previous works only allow
to handle sets of ﬁxed size (see Sec. 1). In summary, we argue that our experiments show strong
evidence that topological features of data can be beneﬁcial in many learning tasks, not necessarily to
replace existing inputs, but rather as a complementary source of discriminative information.

8

A Technical results

Lemma 3. Let α

i)

lim
x→0

ln(x)
x

·

R+, β

R, γ

∈
∈
∈
e−α(ln(x)γ+β)2 = 0

R+. We have

ii)

lim
x→0

1
x ·

e−α(ln(x)γ+β)2 = 0 .

Proof. We omit the proof for brevity (see supplementary material for details), but remark that only
(i) needs to be shown as (ii) follows immediately.

B Proofs

D \

E \

E0 =
D

Proof of Lemma 1. Let ϕ be a bijection between

) and let
and
∆. To show the result of Eq. (5), we consider the following decomposition:

which realizes wq
1(

D

D

E

E

,

∆,

D0 =

= ϕ−1(
= (ϕ−1(
(cid:124)

E0)
∪
E0)
(cid:123)(cid:122)
A

ϕ−1(∆)
∆)
(cid:125)

∪

E0)
(cid:123)(cid:122)
B
Except for the term D, all sets are ﬁnite. In fact, ϕ realizes the Wasserstein distance wq
ϕ(cid:12)
= id. Therefore, s(x) = s(ϕ(x)) = 0 for x
(cid:12)D
∈
in the summation and it sufﬁces to consider E = A

C. It follows that

(ϕ−1(∆)
(cid:124)
(cid:123)(cid:122)
D

(ϕ−1(∆)
(cid:124)
(cid:123)(cid:122)
C

(ϕ−1(
(cid:124)

∆)
(cid:125)

∆)
(cid:125)

⊂

∩

∩

∪

∪

\

\

∆)
(cid:125)

1 which implies
∆. Consequently, we can ignore D

(6)

∪

D since D
B
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∪
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:88)

x∈E

(cid:88)

−

x∈D

(cid:88)

x∈E

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

s(x)

s(ϕ(x))

s(x)

s(ϕ(x))

(cid:88)

(cid:12)
(cid:12)
(cid:12)
s(ϕ(x))
(cid:12)
(cid:12) ≤
||q = Ks ·

ϕ(x)

x∈E

−

s(x)

(cid:88)

x∈E

−

x

||

(cid:88)

x∈D

s(x)
|

−

s(ϕ(x))

|

x∈D

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Ks ·

(cid:88)

x∈E

=

=

≤

x

||

−

ϕ(x)

||q = Ks ·

wq
1(

,

) .

D

E

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
x∈D

s(x)

(cid:88)

y∈E

−

(cid:12)
(cid:12)
(cid:12)
s(y)
(cid:12)
(cid:12)
(cid:12)

Proof of Lemma 2. Since sµ,σ,ν is deﬁned differently for x1 ∈
distinguish these two cases. In the following x0 ∈
(1) x1 ∈

): The partial derivative w.r.t. xi is given as

R.

[ν,

∞

[ν,

) and x1 ∈

∞

(0, ν), we need to

(cid:18) ∂
∂xi

(cid:19)

sµ,σ,ν

(x0, x1) = C

i (xi−µi)2(cid:19)

e−σ2

(x0, x1)

·

(cid:18) ∂
∂xi
e−σ2

i (xi−µi)2

= C

(
·
−
) which is not dependent on xi. For all cases, i.e., x0 → ∞

i )(xi −

µi) ,

·

2σ2

, x0 →

(7)

where C is just the part of exp(
·

, it holds that Eq. (7)

0.

→

and x1 → ∞

−∞
(2) x1 ∈
behaviour for x0 → ∞
(cid:19)
(cid:18) ∂
∂x1

sµ,σ,ν

(0, ν): The partial derivative w.r.t. x0 is similar to Eq. (7) with the same asymptotic

and x0 → −∞
(x0, x1) = C

. However, for the partial derivative w.r.t. x1 we get
(cid:18) ∂
∂x1
e( ... )

ν )ν+ν−µ1)2 (cid:19)

(x0, x1)

e−σ2

ν + ν

1 (ln(

ln

(cid:17)

(cid:17)

(cid:16)

x1

µ1

2σ2
1)

ν
x1

·

·

·

= C

(
−

·

·
(cid:16) x1
ν

(cid:17)

(cid:18)

ln

·

(cid:123)(cid:122)
(a)

(cid:16) x1
ν
ν
x1

·

(cid:19)

(cid:125)

= C (cid:48)

e( ... )

(cid:16)

·

(cid:124)

−

−

+(ν

µ1)

e( ... )

(cid:17)

.

1
x1
(cid:125)

·

(cid:124)

·
(cid:123)(cid:122)
(b)

(8)

As x1 →
Eq. (8)
→
we conclude that they are absolutely bounded.

0, we can invoke Lemma 4 (i) to handle (a) and Lemma 4 (ii) to handle (b); conclusively,
0. As the partial derivatives w.r.t. xi are continuous and their limits are 0 on R, R+, resp.,

9

References

[1] H. Adams, T. Emerson, M. Kirby, R. Neville, C. Peterson, P. Shipman, S. Chepushtanova, E. Hanson,
F. Motta, and L. Ziegelmeier. Persistence images: A stable vector representation of persistent homology.
JMLR, 18(8):1–35, 2017.

[2] A. Adcock, E. Carlsson, and G. Carlsson. The ring of algebraic functions on persistence bar codes. CoRR,

2013. https://arxiv.org/abs/1304.0530.

[3] X. Bai, W. Liu, and Z. Tu. Integrating contour and skeleton for shape classiﬁcation. In ICCV Workshops,

2009.

[4] I. Barnett, N. Malik, M.L. Kuijjer, P.J. Mucha, and J.-P. Onnela. Feature-based classiﬁcation of networks.

CoRR, 2016. https://arxiv.org/abs/1610.05868.

[5] P. Bendich, J.S. Marron, E. Miller, A. Pieloch, and S. Skwerer. Persistent homology analysis of brain artery

trees. Ann. Appl. Stat, 10(2), 2016.

[6] P. Bubenik. Statistical topological data analysis using persistence landscapes. JMLR, 16(1):77–102, 2015.

[7] G. Carlsson. Topology and data. Bull. Amer. Math. Soc., 46:255–308, 2009.

[8] G. Carlsson, T. Ishkhanov, V. de Silva, and A. Zomorodian. On the local behavior of spaces of natural

images. IJCV, 76:1–12, 2008.

[9] F. Chazal, D. Cohen-Steiner, L. J. Guibas, F. Mémoli, and S. Y. Oudot. Gromov-Hausdorff stable signatures

for shapes using persistence. Comput. Graph. Forum, 28(5):1393–1403, 2009.

[10] F. Chazal, B.T. Fasy, F. Lecci, A. Rinaldo, and L. Wassermann. Stochastic convergence of persistence

landscapes and silhouettes. JoCG, 6(2):140–161, 2014.

[11] F. Chazal, L.J. Guibas, S.Y. Oudot, and P. Skraba. Persistence-based clustering in Riemannian manifolds.

[12] D. Cohen-Steiner, H. Edelsbrunner, and J. Harer. Stability of persistence diagrams. Discrete Comput.

J. ACM, 60(6):41–79, 2013.

Geom., 37(1):103–120, 2007.

[13] D. Cohen-Steiner, H. Edelsbrunner, J. Harer, and Y. Mileyko. Lipschitz functions have Lp-stable persistence.

Found. Comput. Math., 10(2):127–139, 2010.

[14] H. Edelsbrunner and J. L. Harer. Computational Topology : An Introduction. American Mathematical

Society, 2010.

[15] H. Edelsbrunner, D. Letcher, and A. Zomorodian. Topological persistence and simpliﬁcation. Discrete

Comput. Geom., 28(4):511–533, 2002.

[16] A. Hatcher. Algebraic Topology. Cambridge University Press, Cambridge, 2002.

[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.

[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural

networks. In NIPS, 2012.

analysis. In ICML, 2016.

perspective. In NIPS, 2015.

contour. In CVPR, 2000.

[19] G. Kusano, K. Fukumizu, and Y. Hiraoka. Persistence weighted Gaussian kernel for topological data

[20] R. Kwitt, S. Huber, M. Niethammer, W. Lin, and U. Bauer. Statistical topological data analysis - a kernel

[21] L. Latecki, R. Lakamper, and T. Eckhardt. Shape descriptors for non-rigid shapes with a single closed

[22] C. Li, M. Ovsjanikov, and F. Chazal. Persistence-based structural recognition. In CVPR, 2014.

[23] K. Mischaikow and V. Nanda. Morse theory for ﬁltrations and efﬁcient computation of persistent homology.

Discrete Comput. Geom., 50(2):330–353, 2013.

[24] M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. In ICML,

[25] C.R. Qi, H. Su, K. Mo, and L.J. Guibas. PointNet: Deep learning on point sets for 3D classiﬁcation and

2016.

segmentation. In CVPR, 2017.

[26] S. Ravanbakhsh, S. Schneider, and B. Póczos. Deep learning with sets and point clouds. In ICLR, 2017.

[27] R. Reininghaus, U. Bauer, S. Huber, and R. Kwitt. A stable multi-scale kernel for topological machine

learning. In CVPR, 2015.

[28] G. Singh, F. Memoli, T. Ishkhanov, G. Sapiro, G. Carlsson, and D.L. Ringach. Topological analysis of

population activity in visual cortex. J. Vis., 8(8), 2008.

10

[29] K. Turner, S. Mukherjee, and D. M. Boyer. Persistent homology transform for modeling shapes and

[30] X. Wang, B. Feng, X. Bai, W. Liu, and L.J. Latecki. Bag of contour fragments for robust shape classiﬁcation.

surfaces. Inf. Inference, 3(4):310–344, 2014.

Pattern Recognit., 47(6):2116–2125, 2014.

[31] P. Yanardag and S.V.N. Vishwanathan. Deep graph kernels. In KDD, 2015.

This supplementary material contains technical details that were left-out in the original submission
for brevity. When necessary, we refer to the submitted manuscript.

C Additional proofs

In the manuscript, we omitted the proof for the following technical lemma. For completeness, the
lemma is repeated and its proof is given below.
R+, β
Lemma 4. Let α

R+. We have

R and γ

∈

∈
e−α(ln(x)γ+β)2 = 0

∈

·

e−α(ln(x)γ+β)2 = 0 .

(i) lim
x→0

ln(x)
x

(ii) lim
x→0

1
x ·

Proof. We only need to prove the ﬁrst statement, as the second follows immediately. Hence, consider

lim
x→0

ln(x)
x

·

e−α(ln(x)γ+β)2

ln(x)

e− ln(x)

e−α(ln(x)γ+β)2

= lim
x→0

= lim
x→0

ln(x)

= lim
x→0

ln(x)

·

·

·

·

e−α(ln(x)γ+β)2−ln(x)
eα(ln(x)γ+β)2+ln(x)(cid:17)−1

(cid:16)

(∗)
= lim
x→0

∂
∂x

ln(x)

(cid:18) ∂
∂x

·

eα(ln(x)γ+β)2+ln(x)

(cid:19)−1

(cid:18)

(cid:18)

eα(ln(x)γ+β)2+ln(x)

1
x ·
(cid:16)
eα(ln(x)γ+β)2+ln(x)

·

2α(ln(x)γ + β)

(cid:19)(cid:19)−1

γ
x

+

1
x
(cid:17)−1

(2α(ln(x)γ + β)γ + 1)

·

= lim
x→0

= lim
x→0

= 0

).
where we use de l’Hôpital’s rule in (
∗

D Network architectures

2D object shape classiﬁcation. Fig. 6 illustrates the network architecture used for 2D object shape
classiﬁcation in [Manuscript, Sec. 5.1]. Note that the persistence diagrams from three consecutive
ﬁltration directions di share one input layer. As we use 32 directions, we have 32 input branches.
3 and a stride of 1. The max-pooling
The convolution operation operates with kernels of size 1
operates along the ﬁlter dimension. For better readability, we have added the output size of certain
layers. We train with the network with stochastic gradient descent (SGD) and a mini-batch size of
128 for 300 epochs. Every 20th epoch, the learning rate (initially set to 0.1) is halved.

×

×

1

11

Graph classiﬁcation. Fig. 7 illustrates the network architecture used for graph classiﬁcation in
Sec. 5.2. In detail, we have 3 input branches: ﬁrst, we split 0-dimensional features into essential
and non-essential ones; second, since there are only essential features in dimension 1 (see Sec. 5.2,
Filtration) we do not need a branch for non-essential features. We train the network using SGD with
mini-batches of size 128 for 300 epochs. The initial learning rate is set to 0.1 (reddit_5k) and 0.4
(reddit_12k), resp., and halved every 20th epochs.

D.1 Technical handling of essential features

In case of of 2D object shapes, the death times of essential features are mapped to the max. ﬁltration
value and kept in the original persistence diagrams. In fact, for Animal and MPEG-7, there is always
only one connected component and consequently only one essential feature in dimension 0 (i.e., it
does not make sense to handle this one point in a separate input branch).

In case of social network graphs, essential features are mapped to the real line (using their birth time)
and handled in separate input branches (see Fig. 7) with 1D structure elements. This is in contrast to
the 2D object shape experiments, as we might have many essential features (in dimensions 0 and 1)
that require handling in separate input branches.

12

Figure 6: 2D object shape classiﬁcation network architecture.

13

Figure 7: Graph classiﬁcation network architecture.

14

