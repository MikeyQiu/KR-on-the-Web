SphereFace: Deep Hypersphere Embedding for Face Recognition

Weiyang Liu1 Yandong Wen2

1Georgia Institute of Technology

Zhiding Yu2 Ming Li3 Bhiksha Raj2
2Carnegie Mellon University

3Sun Yat-Sen University

Le Song1

wyliu@gatech.edu, {yandongw,yzhiding}@andrew.cmu.edu, lsong@cc.gatech.edu

8
1
0
2
 
n
a
J
 
9
2
 
 
]

V
C
.
s
c
[
 
 
4
v
3
6
0
8
0
.
4
0
7
1
:
v
i
X
r
a

Abstract

This paper addresses deep face recognition (FR) prob-
lem under open-set protocol, where ideal face features are
expected to have smaller maximal intra-class distance than
minimal inter-class distance under a suitably chosen met-
ric space. However, few existing algorithms can effectively
achieve this criterion. To this end, we propose the angular
softmax (A-Softmax) loss that enables convolutional neural
networks (CNNs) to learn angularly discriminative features.
Geometrically, A-Softmax loss can be viewed as imposing
discriminative constraints on a hypersphere manifold, which
intrinsically matches the prior that faces also lie on a mani-
fold. Moreover, the size of angular margin can be quantita-
tively adjusted by a parameter m. We further derive speciﬁc
m to approximate the ideal feature criterion. Extensive anal-
ysis and experiments on Labeled Face in the Wild (LFW),
Youtube Faces (YTF) and MegaFace Challenge show the
superiority of A-Softmax loss in FR tasks. The code has also
been made publicly available1.

1. Introduction

Recent years have witnessed the great success of convo-
lutional neural networks (CNNs) in face recognition (FR).
Owing to advanced network architectures [13, 23, 29, 4] and
discriminative learning approaches [25, 22, 34], deep CNNs
have boosted the FR performance to an unprecedent level.
Typically, face recognition can be categorized as face identi-
ﬁcation and face veriﬁcation [8, 11]. The former classiﬁes a
face to a speciﬁc identity, while the latter determines whether
a pair of faces belongs to the same identity.

In terms of testing protocol, face recognition can be eval-
uated under closed-set or open-set settings, as illustrated
in Fig. 1. For closed-set protocol, all testing identities are
predeﬁned in training set. It is natural to classify testing face
images to the given identities. In this scenario, face veriﬁca-
tion is equivalent to performing identiﬁcation for a pair of
faces respectively (see left side of Fig. 1). Therefore, closed-
set FR can be well addressed as a classiﬁcation problem,

1See the code at https://github.com/wy1iu/sphereface.

Figure 1: Comparison of open-set and closed-set face recognition.

where features are expected to be separable. For open-set
protocol, the testing identities are usually disjoint from the
training set, which makes FR more challenging yet close to
practice. Since it is impossible to classify faces to known
identities in training set, we need to map faces to a discrimi-
native feature space. In this scenario, face identiﬁcation can
be viewed as performing face veriﬁcation between the probe
face and every identity in the gallery (see right side of Fig. 1).
Open-set FR is essentially a metric learning problem, where
the key is to learn discriminative large-margin features.

Desired features for open-set FR are expected to satisfy
the criterion that the maximal intra-class distance is smaller
than the minimal inter-class distance under a certain metric
space. This criterion is necessary if we want to achieve
perfect accuracy using nearest neighbor. However, learning
features with this criterion is generally difﬁcult because of
the intrinsically large intra-class variation and high inter-
class similarity [21] that faces exhibit.

Few CNN-based approaches are able to effectively for-
mulate the aforementioned criterion in loss functions. Pi-

Figure 2: Comparison among softmax loss, modiﬁed softmax loss and A-Softmax loss. In this toy experiment, we construct a CNN to learn 2-D features on a
subset of the CASIA face dataset. In speciﬁc, we set the output dimension of FC1 layer as 2 and visualize the learned features. Yellow dots represent the
ﬁrst class face features, while purple dots represent the second class face features. One can see that features learned by the original softmax loss can not be
classiﬁed simply via angles, while modiﬁed softmax loss can. Our A-Softmax loss can further increase the angular margin of learned features.

oneering work [30, 26] learn face features via the softmax
loss2, but softmax loss only learns separable features that are
not discriminative enough. To address this, some methods
combine softmax loss with contrastive loss [25, 28] or center
loss [34] to enhance the discrimination power of features.
[22] adopts triplet loss to supervise the embedding learning,
leading to state-of-the-art face recognition results. However,
center loss only explicitly encourages intra-class compact-
ness. Both contrastive loss [3] and triplet loss [22] can not
constrain on each individual sample, and thus require care-
fully designed pair/triplet mining procedure, which is both
time-consuming and performance-sensitive.

It seems to be a widely recognized choice to impose Eu-
clidean margin to learned features, but a question arises: Is
Euclidean margin always suitable for learning discrimina-
tive face features? To answer this question, we ﬁrst look into
how Euclidean margin based losses are applied to FR.

Most recent approaches [25, 28, 34] combine Euclidean
margin based losses with softmax loss to construct a joint
supervision. However, as can be observed from Fig. 2, the
features learned by softmax loss have intrinsic angular dis-
tribution (also veriﬁed by [34]). In some sense, Euclidean
margin based losses are incompatible with softmax loss, so
it is not well motivated to combine these two type of losses.
In this paper, we propose to incorporate angular margin
instead. We start with a binary-class case to analyze the
softmax loss. The decision boundary in softmax loss is
(W1 − W2)x + b1 − b2 = 0, where Wi and bi are weights
and bias3 in softmax loss, respectively.
If we deﬁne x
as a feature vector and constrain (cid:107)W1(cid:107) = (cid:107)W2(cid:107) = 1 and
b1 = b2 = 0, the decision boundary becomes (cid:107)x(cid:107)(cos(θ1) −
cos(θ2)) = 0, where θi is the angle between Wi and x. The
new decision boundary only depends on θ1 and θ2. Modiﬁed
softmax loss is able to directly optimize angles, enabling
CNNs to learn angularly distributed features (Fig. 2).

Compared to original softmax loss, the features learned
by modiﬁed softmax loss are angularly distributed, but not
necessarily more discriminative. To the end, we generalize
the modiﬁed softmax loss to angular softmax (A-Softmax)

2Following [16], we deﬁne the softmax loss as the combination of the

last fully connected layer, softmax function and cross-entropy loss.

3If not speciﬁed, the weights and biases in the paper are corresponding

to the fully connected layer in the softmax loss.

loss. Speciﬁcally, we introduce an integer m (m ≥ 1) to
quantitatively control the decision boundary.
In binary-
class case, the decision boundaries for class 1 and class
2 become (cid:107)x(cid:107)(cos(mθ1)−cos(θ2))=0 and (cid:107)x(cid:107)(cos(θ1)−
cos(mθ2))=0, respectively. m quantitatively controls the
size of angular margin. Furthermore, A-Softmax loss can be
easily generalized to multiple classes, similar to softmax loss.
By optimizing A-Softmax loss, the decision regions become
more separated, simultaneously enlarging the inter-class mar-
gin and compressing the intra-class angular distribution.

A-Softmax loss has clear geometric interpretation. Su-
pervised by A-Softmax loss, the learned features construct a
discriminative angular distance metric that is equivalent to
geodesic distance on a hypersphere manifold. A-Softmax
loss can be interpreted as constraining learned features to
be discriminative on a hypersphere manifold, which intrin-
sically matches the prior that face images lie on a manifold
[14, 5, 31]. The close connection between A-Softmax loss
and hypersphere manifolds makes the learned features more
effective for face recognition. For this reason, we term the
learned features as SphereFace.

Moreover, A-Softmax loss can quantitatively adjust the
angular margin via a parameter m, enabling us to do quanti-
tative analysis. In the light of this, we derive lower bounds
for the parameter m to approximate the desired open-set
FR criterion that the maximal intra-class distance should be
smaller than the minimal inter-class distance.

Our major contributions can be summarized as follows:

(1) We propose A-Softmax loss for CNNs to learn dis-
criminative face features with clear and novel geometric
interpretation. The learned features discriminatively span
on a hypersphere manifold, which intrinsically matches the
prior that faces also lie on a manifold.

(2) We derive lower bounds for m such that A-Softmax
loss can approximate the learning task that minimal inter-
class distance is larger than maximal intra-class distance.

(3) We are the very ﬁrst to show the effectiveness of
angular margin in FR. Trained on publicly available CASIA
dataset [37], SphereFace achieves competitive results on
several benchmarks, including Labeled Face in the Wild
(LFW), Youtube Faces (YTF) and MegaFace Challenge 1.

2. Related Work

Metric learning. Metric learning aims to learn a sim-
ilarity (distance) function. Traditional metric learning
[36, 33, 12, 38] usually learns a matrix A for a distance met-
ric (cid:107)x1 − x2(cid:107)A = (cid:112)(x1 − x2)T A(x1 − x2) upon the given
features x1, x2. Recently, prevailing deep metric learning
[7, 17, 24, 30, 25, 22, 34] usually uses neural networks
to automatically learn discriminative features x1, x2 fol-
lowed by a simple distance metric such as Euclidean dis-
tance (cid:107)x1 − x2(cid:107)2. Most widely used loss functions for deep
metric learning are contrastive loss [1, 3] and triplet loss
[32, 22, 6], and both impose Euclidean margin to features.

Deep face recognition. Deep face recognition is ar-
guably one of the most active research area in the past few
years. [30, 26] address the open-set FR using CNNs super-
vised by softmax loss, which essentially treats open-set FR
as a multi-class classiﬁcation problem. [25] combines con-
trastive loss and softmax loss to jointly supervise the CNN
training, greatly boosting the performance. [22] uses triplet
loss to learn a uniﬁed face embedding. Training on nearly
200 million face images, they achieve current state-of-the-art
FR accuracy. Inspired by linear discriminant analysis, [34]
proposes center loss for CNNs and also obtains promising
performance. In general, current well-performing CNNs
[28, 15] for FR are mostly built on either contrastive loss or
triplet loss. One could notice that state-of-the-art FR meth-
ods usually adopt ideas (e.g. contrastive loss, triplet loss)
from metric learning, showing open-set FR could be well
addressed by discriminative metric learning.

L-Softmax loss [16] also implicitly involves the concept
of angles. As a regularization method, it shows great im-
provement on closed-set classiﬁcation problems. Differently,
A-Softmax loss is developed to learn discriminative face em-
bedding. The explicit connections to hypersphere manifold
makes our learned features particularly suitable for open-set
FR problem, as veriﬁed by our experiments. In addition,
the angular margin in A-Softmax loss is explicitly imposed
and can be quantitatively controlled (e.g. lower bounds to
approximate desired feature criterion), while [16] can only
be analyzed qualitatively.

3. Deep Hypersphere Embedding

3.1. Revisiting the Softmax Loss

We revisit the softmax loss by looking into the decision
criteria of softmax loss. In binary-class case, the posterior
probabilities obtained by softmax loss are

p1 =

p2 =

exp(W T

1 x + b1) + exp(W T

2 x + b2)

exp(W T

1 x + b1)

exp(W T

2 x + b2)

exp(W T

1 x + b1) + exp(W T

2 x + b2)

(1)

(2)

1 x + b1 and W T

i, respectively. The predicted label will be assigned to
class 1 if p1 > p2 and class 2 if p1 < p2. By comparing
p1 and p2, it is clear that W T
2 x + b2 de-
termine the classiﬁcation result. The decision boundary is
(W1 − W2)x + b1 − b2 = 0. We then rewrite W T
i x + bi as
(cid:107)W T
i (cid:107)(cid:107)x(cid:107) cos(θi) + bi where θi is the angle between Wi
and x. Notice that if we normalize the weights and zero
the biases ((cid:107)Wi(cid:107) = 1, bi =0), the posterior probabilities be-
come p1=(cid:107)x(cid:107) cos(θ1) and p2=(cid:107)x(cid:107) cos(θ2). Note that p1
and p2 share the same x, the ﬁnal result only depends on
the angles θ1 and θ2. The decision boundary also becomes
cos(θ1)−cos(θ2)=0 (i.e. angular bisector of vector W1 and
W2). Although the above analysis is built on binary-calss
case, it is trivial to generalize the analysis to multi-class case.
During training, the modiﬁed softmax loss ((cid:107)Wi(cid:107)=1, bi =0)
encourages features from the i-th class to have smaller angle
θi (larger cosine distance) than others, which makes angles
between Wi and features a reliable metric for classiﬁcation.
To give a formal expression for the modiﬁed softmax loss,
we ﬁrst deﬁne the input feature xi and its label yi. The
original softmax loss can be written as

L =

Li =

1
N

(cid:88)

i

1
N

(cid:88)

i

− log (cid:0) efyi
j efj

(cid:80)

(cid:1)

(3)

where fj denotes the j-th element (j ∈ [1, K], K is the
class number) of the class score vector f , and N is the
In CNNs, f is usually the
number of training samples.
output of a fully connected layer W , so fj = W T
j xi + bj
and fyi = W T
xi + byi where xi, Wj, Wyi are the i-th
yi
training sample, the j-th and yi-th column of W respectively.
We further reformulate Li in Eq. (3) as
Li = − log (cid:0) eW T
j eW T

j xi+bj

xi+byi

(cid:80)

yi

(cid:1)

(4)

= − log (cid:0) e(cid:107)Wyi (cid:107)(cid:107)xi(cid:107) cos(θyi,i)+byi
j e(cid:107)Wj (cid:107)(cid:107)xi(cid:107) cos(θj,i)+bj

(cid:80)

(cid:1)

in which θj,i(0 ≤ θj,i ≤ π) is the angle between vector Wj
and xi. As analyzed above, we ﬁrst normalize (cid:107)Wj(cid:107) = 1, ∀j
in each iteration and zero the biases. Then we have the
modiﬁed softmax loss:

Lmodiﬁed =

1
N

(cid:88)

i

− log (cid:0) e(cid:107)xi(cid:107) cos(θyi,i)
j e(cid:107)xi(cid:107) cos(θj,i)

(cid:80)

(cid:1)

(5)

Although we can learn features with angular boundary with
the modiﬁed softmax loss, these features are still not neces-
sarily discriminative. Since we use angles as the distance
metric, it is natural to incorporate angular margin to learned
features in order to enhance the discrimination power. To
this end, we propose a novel way to combine angular margin.

3.2. Introducing Angular Margin to Softmax Loss

where x is the learned feature vector. Wi and bi are weights
and bias of last fully connected layer corresponding to class

Instead of designing a new type of loss function and con-
structing a weighted combination with softmax loss (similar

Loss Function
Softmax Loss
Modiﬁed Softmax Loss

A-Softmax Loss

Decision Boundary
(W1 − W2)x + b1 − b2 = 0
(cid:107)x(cid:107)(cos θ1 − cos θ2) = 0
(cid:107)x(cid:107)(cos mθ1 − cos θ2) = 0 for class 1
(cid:107)x(cid:107)(cos θ1 − cos mθ2) = 0 for class 2

Table 1: Comparison of decision boundaries in binary case. Note that, θi is
the angle between Wi and x.

to contrastive loss) , we propose a more natural way to learn
angular margin. From the previous analysis of softmax loss,
we learn that decision boundaries can greatly affect the fea-
ture distribution, so our basic idea is to manipulate decision
boundaries to produce angular margin. We ﬁrst give a moti-
vating binary-class example to explain how our idea works.
Assume a learned feature x from class 1 is given and θi
is the angle between x and Wi, it is known that the modiﬁed
softmax loss requires cos(θ1) > cos(θ2) to correctly classify
x. But what if we instead require cos(mθ1) > cos(θ2) where
m ≥ 2 is a integer in order to correctly classify x? It is
essentially making the decision more stringent than previ-
ous, because we require a lower bound4 of cos(θ1) to be
larger than cos(θ2). The decision boundary for class 1 is
cos(mθ1) = cos(θ2). Similarly, if we require cos(mθ2) >
cos(θ1) to correctly classify features from class 2, the deci-
sion boundary for class 2 is cos(mθ2) = cos(θ1). Suppose
all training samples are correctly classiﬁed, such decision
boundaries will produce an angular margin of m−1
2 where
θ1
2 is the angle between W1 and W2. From angular per-
spective, correctly classifying x from identity 1 requires
θ1 < θ2
m , while correctly classifying x from identity 2 re-
quires θ2 < θ1
m . Both are more difﬁcult than original θ1 < θ2
and θ2 < θ1, respectively. By directly formulating this idea
into the modiﬁed softmax loss Eq. (5), we have

m+1 θ1

Lang =

(cid:88)

− log (cid:0)

1
N

i

e(cid:107)xi(cid:107) cos(mθyi,i)

(cid:1)

e(cid:107)xi(cid:107) cos(mθyi,i) + (cid:80)

e(cid:107)xi(cid:107) cos(θj,i)

j(cid:54)=yi

(6)
where θyi,i has to be in the range of [0, π
m ]. In order to
get rid of this restriction and make it optimizable in CNNs,
we expand the deﬁnition range of cos(θyi,i) by generaliz-
ing it to a monotonically decreasing angle function ψ(θyi,i)
which should be equal to cos(θyi,i) in [0, π
m ]. Therefore, our
proposed A-Softmax loss is formulated as:

Lang =

(cid:88)

− log (cid:0)

1
N

i

e(cid:107)xi(cid:107)ψ(θyi,i)

(cid:1) (7)

e(cid:107)xi(cid:107)ψ(θyi,i) + (cid:80)

e(cid:107)xi(cid:107) cos(θj,i)

j(cid:54)=yi

in which we deﬁne ψ(θyi,i) = (−1)k cos(mθyi,i) − 2k,
θyi,i ∈ [ kπ
m ] and k ∈ [0, m − 1]. m ≥ 1 is an inte-
ger that controls the size of angular margin. When m = 1, it
becomes the modiﬁed softmax loss.

m , (k+1)π

The justiﬁcation of A-Softmax loss can also be made from
decision boundary perspective. A-Softmax loss adopts dif-
ferent decision boundary for different class (each boundary

4The inequality cos(θ1) > cos(mθ1) holds while θ1 ∈ [0, π

m ], m ≥ 2.

Figure 3: Geometry Interpretation of Euclidean margin loss (e.g. contrastive
loss, triplet loss, center loss, etc.), modiﬁed softmax loss and A-Softmax
loss. The ﬁrst row is 2D feature constraint, and the second row is 3D feature
constraint. The orange region indicates the discriminative constraint for
class 1, while the green region is for class 2.

is more stringent than the original), thus producing angular
margin. The comparison of decision boundaries is given in
Table 1. From original softmax loss to modiﬁed softmax
loss, it is from optimizing inner product to optimizing angles.
From modiﬁed softmax loss to A-Softmax loss, it makes
the decision boundary more stringent and separated. The
angular margin increases with larger m and be zero if m = 1.
Supervised by A-Softmax loss, CNNs learn face features
with geometrically interpretable angular margin. Because A-
Softmax loss requires Wi = 1, bi = 0, it makes the prediction
only depends on angles between the sample x and Wi. So
x can be classiﬁed to the identity with smallest angle. The
parameter m is added for the purpose of learning an angular
margin between different identities.

To facilitate gradient computation and back propagation,
we replace cos(θj,i) and cos(mθyi,i) with the expressions
only containing W and xi, which is easily done by deﬁni-
tion of cosine and multi-angle formula (also the reason why
we need m to be an integer). Without θ, we can compute
derivative with respect to x and W , similar to softmax loss.

3.3. Hypersphere Interpretation of A-Softmax Loss

A-Softmax loss has stronger requirements for a correct
classiﬁcation when m ≥ 2, which generates an angular classi-
ﬁcation margin between learned features of different classes.
A-Softmax loss not only imposes discriminative power to
the learned features via angular margin, but also renders nice
and novel hypersphere interpretation. As shown in Fig. 3,
A-Softmax loss is equivalent to learning features that are
discriminative on a hypersphere manifold, while Euclidean
margin losses learn features in Euclidean space.

To simplify, We take the binary case to analyze the hyper-
sphere interpretation. Considering a sample x from class 1
and two column weights W1, W2, the classiﬁcation rule for

j v2

A-Softmax loss is cos(mθ1) > cos(θ2), equivalently mθ1 <
θ2. Notice that θ1, θ2 are equal to their corresponding arc
5 on unit hypersphere {vj, ∀j| (cid:80)
length ω1, ω2
j =1, v≥0}.
Because (cid:107)W (cid:107)1 = (cid:107)W (cid:107)2 = 1, the decision replies on the arc
length ω1 and ω2. The decision boundary is equivalent to
mω1 = ω2, and the constrained region for correctly classify-
ing x to class 1 is mω1 < ω2. Geometrically speaking, this
is a hypercircle-like region lying on a hypersphere manifold.
For example, it is a circle-like region on the unit sphere in
3D case, as illustrated in Fig. 3. Note that larger m leads to
smaller hypercircle-like region for each class, which is an ex-
plicit discriminative constraint on a manifold. For better un-
derstanding, Fig. 3 provides 2D and 3D visualizations. One
can see that A-Softmax loss imposes arc length constraint on
a unit circle in 2D case and circle-like region constraint on a
unit sphere in 3D case. Our analysis shows that optimizing
angles with A-Softmax loss essentially makes the learned
features more discriminative on a hypersphere.

3.4. Properties of A-Softmax Loss

Property 1. A-Softmax loss deﬁnes a large angular mar-
gin learning task with adjustable difﬁculty. With larger m,
the angular margin becomes larger, the constrained region
on the manifold becomes smaller, and the corresponding
learning task also becomes more difﬁcult.

We know that the larger m is, the larger angular margin
A-Softmax loss constrains. There exists a minimal m that
constrains the maximal intra-class angular distance to be
smaller than the minimal inter-class angular distance, which
can also be observed in our experiments.

Deﬁnition 1 (minimal m for desired feature distribution).
mmin is the minimal value such that while m > mmin, A-
Softmax loss deﬁnes a learning task where the maximal intra-
class angular feature distance is constrained to be smaller
than the minimal inter-class angular feature distance.

3.

√

Property 2 (lower bound of mmin in binary-class case). In
binary-class case, we have mmin ≥ 2 +
Proof. We consider the space spaned by W1 and W2. Be-
cause m ≥ 2, it is easy to obtain the maximal angle that class
1 spans is θ12
m+1 where θ12 is the angle between W1
and W2. To require the maximal intra-class feature angular
distance smaller than the minimal inter-class feature angular
distance, we need to constrain

m−1 + θ12

+

θ12
θ12
m + 1
m − 1
(cid:125)
(cid:123)(cid:122)
(cid:124)
max intra-class angle

≤

2π − θ12
m + 1

+

≤

θ12
m + 1
(cid:125)

(cid:124)

(cid:123)(cid:122)
max intra-class angle

(cid:125)

(cid:124)

(m − 1)θ12
m + 1
(cid:123)(cid:122)
min inter-class angle
(m − 1)θ12
m + 1
(cid:123)(cid:122)
min inter-class angle

(cid:125)

(cid:124)

, θ12 ≤

m − 1
m

π

, θ12 >

m − 1
m

π

(8)

(9)

5ωi is the shortest arc length (geodesic distance) between Wi and the
projected point of sample x on the unit hypersphere, while the correspond-
ing θi is the angle between Wi and x.

After solving these two inequalities, we could have mmin ≥
2 +

3, which is a lower bound for binary case.

√

Property 3 (lower bound of mmin in multi-class case). Un-
der the assumption that Wi, ∀i are uniformly spaced in the
Euclidean space, we have mmin ≥ 3.

Proof. We consider the 2D k-class (k ≥ 3) scenario for the
lower bound. Because Wi, ∀i are uniformly spaced in the
2D Euclidean space, we have θi+1
is the
angle between Wi and Wi+1. Since Wi, ∀i are symmetric,
we only need to analyze one of them. For the i-th class (Wi),
We need to constrain

k where θi+1

i = 2π

i

+

θi
θi+1
i−1
i
m + 1
m + 1
(cid:124)
(cid:125)
(cid:123)(cid:122)
max intra-class angle

≤ min

(cid:124)

(cid:26) (m − 1)θi+1
i
m + 1

,

(m − 1)θi

(cid:27)

i−1

m + 1

(10)

(cid:123)(cid:122)
min inter-class angle

(cid:125)

After solving this inequality, we obtain mmin ≥ 3, which is
a lower bound for multi-class case.

Based on this, we use m = 4 to approximate the desired
feature distribution criteria. Since the lower bounds are not
necessarily tight, giving a tighter lower bound and a upper
bound under certain conditions is also possible, which we
leave to the future work. Experiments also show that larger
m consistently works better and m = 4 will usually sufﬁce.

3.5. Discussions

Why angular margin. First and most importantly, angu-
lar margin directly links to discriminativeness on a manifold,
which intrinsically matches the prior that faces also lie on
a manifold. Second, incorporating angular margin to soft-
max loss is actually a more natural choice. As Fig. 2 shows,
features learned by the original softmax loss have an intrin-
sic angular distribution. So directly combining Euclidean
margin constraints with softmax loss is not reasonable.

Comparison with existing losses. In deep FR task, the
most popular and well-performing loss functions include
contrastive loss, triplet loss and center loss. First, they only
impose Euclidean margin to the learned features (w/o normal-
ization), while ours instead directly considers angular margin
which is naturally motivated. Second, both contrastive loss
and triplet loss suffer from data expansion when constituting
the pairs/triplets from the training set, while ours requires no
sample mining and imposes discriminative constraints to the
entire mini-batches (compared to contrastive and triplet loss
that only affect a few representative pairs/triplets).

4. Experiments (more in Appendix)

4.1. Experimental Settings

Preprocessing. We only use standard preprocessing. The
face landmarks in all images are detected by MTCNN [39].
The cropped faces are obtained by similarity transforma-
tion. Each pixel ([0, 255]) in RGB images is normalized by
subtracting 127.5 and then being divided by 128.

Layer

4-layer CNN

10-layer CNN

Conv1.x

[3×3, 64]×1, S2

[3×3, 64]×1, S2

20-layer CNN
[3×3, 64]×1, S2
(cid:34)

(cid:35)

3 × 3, 64

× 1

3 × 3, 64

36-layer CNN
[3×3, 64]×1, S2
(cid:34)

(cid:35)

3 × 3, 64

× 2

3 × 3, 64

64-layer CNN
[3×3, 64]×1, S2
(cid:34)

(cid:35)

3 × 3, 64

× 3

3 × 3, 64

Conv2.x

[3×3, 128]×1, S2

Conv3.x

[3×3, 256]×1, S2

[3×3, 128]×1, S2
(cid:34)

3 × 3, 128

(cid:35)

[3×3, 128]×1, S2
(cid:34)

3 × 3, 128

(cid:35)

[3×3, 128]×1, S2
(cid:34)

3 × 3, 128

(cid:35)

[3×3, 128]×1, S2
(cid:34)

3 × 3, 128

(cid:35)

3 × 3, 128

3 × 3, 128

3 × 3, 128

3 × 3, 128

[3×3, 256]×1, S2
(cid:34)

3 × 3, 256

(cid:35)

[3×3, 256]×1, S2
(cid:34)

3 × 3, 256

(cid:35)

[3×3, 256]×1, S2
(cid:34)

3 × 3, 256

(cid:35)

[3×3, 256]×1, S2
(cid:34)
3 × 3, 256

(cid:35)

× 8

× 1

× 2

3 × 3, 256

3 × 3, 256

3 × 3, 256

Conv4.x

[3×3, 512]×1, S2

[3×3, 512]×1, S2

FC1

512

512

[3×3, 512]×1, S2
(cid:34)

3 × 3, 512

(cid:35)

[3×3, 512]×1, S2
(cid:34)

3 × 3, 512

(cid:35)

3 × 3, 512

512

3 × 3, 512

512

× 4

× 8

× 2

× 16

3 × 3, 256
[3×3, 512]×1, S2
(cid:34)

3 × 3, 512

(cid:35)

× 3

3 × 3, 512

512

× 2

× 4

× 1

Table 2: Our CNN architectures with different convolutional layers. Conv1.x, Conv2.x and Conv3.x denote convolution units that may contain multiple
convolution layers and residual units are shown in double-column brackets. E.g., [3×3, 64]×4 denotes 4 cascaded convolution layers with 64 ﬁlters of size
3×3, and S2 denotes stride 2. FC1 is the fully connected layer.

CNNs Setup. Caffe [10] is used to implement A-Softmax
loss and CNNs. The general framework to train and extract
SphereFace features is shown in Fig. 4. We use residual
units [4] in our CNN architecture. For fairness, all compared
methods use the same CNN architecture (including residual
units) as SphereFace. CNNs with different depths (4, 10, 20,
36, 64) are used to better evaluate our method. The speciﬁc
settings for difffernt CNNs we used are given in Table 2.
According to the analysis in Section 3.4, we usually set m
as 4 in A-Softmax loss unless speciﬁed. These models are
trained with batch size of 128 on four GPUs. The learning
rate begins with 0.1 and is divided by 10 at the 16K, 24K
iterations. The training is ﬁnished at 28K iterations.

Figure 4: Training and Extracting SphereFace features.

Training Data. We use publicly available web-collected
training dataset CASIA-WebFace [37] (after excluding the
images of identities appearing in testing sets) to train our
CNN models. CASIA-WebFace has 494,414 face images
belonging to 10,575 different individuals. These face images
are horizontally ﬂipped for data augmentation. Notice that
the scale of our training data (0.49M) is relatively small, es-
pecially compared to other private datasets used in DeepFace
[30] (4M), VGGFace [20] (2M) and FaceNet [22] (200M).
Testing. We extract the deep features (SphereFace) from
the output of the FC1 layer. For all experiments, the ﬁnal
representation of a testing face is obtained by concatenating
its original face features and its horizontally ﬂipped features.
The score (metric) is computed by the cosine distance of two
features. The nearest neighbor classiﬁer and thresholding
are used for face identiﬁcation and veriﬁcation, respectively.

4.2. Exploratory Experiments

Effect of m. To show that larger m leads to larger an-
gular margin (i.e. more discriminative feature distribution
on manifold), we perform a toy example with different m.
We train A-Softmax loss with 6 individuals that have the
most samples in CASIA-WebFace. We set the output feature
dimension (FC1) as 3 and visualize the training samples in
Fig. 5. One can observe that larger m leads to more dis-
criminative distribution on the sphere and also larger angular
margin, as expected. We also use class 1 (blue) and class
2 (dark green) to construct positive and negative pairs to
evaluate the angle distribution of features from the same
class and different classes. The angle distribution of positive
and negative pairs (the second row of Fig. 5) quantitatively
shows the angular margin becomes larger while m increases
and every class also becomes more distinct with each other.
Besides visual comparison, we also perform face recogni-
tion on LFW and YTF to evaluate the effect of m. For fair
comparison, we use 64-layer CNN (Table 2) for all losses.
Results are given in Table 3. One can observe that while
m becomes larger, the accuracy of A-Softmax loss also be-
comes better, which shows that larger angular margin can
bring stronger discrimination power.

Dataset
LFW
YTF

Original
97.88
93.1

m=1
97.90
93.2

m=2
98.40
93.8

m=3
99.25
94.4

m=4
99.42
95.0

Table 3: Accuracy(%) comparison of different m (A-Softmax loss) and
original softmax loss on LFW and YTF dataset.

Effect of CNN architectures. We train A-Softmax loss
(m = 4) and original softmax loss with different number
of convolution layers. Speciﬁc CNN architectures can be
found in Table 2. From Fig. 6, one can observe that A-
Softmax loss consistently outperforms CNNs with softmax
loss (1.54%∼1.91%), indicating that A-Softmax loss is more
suitable for open-set FR. Besides, the difﬁcult learning task

Figure 5: Visualization of features learned with different m. The ﬁrst row shows the 3D features projected on the unit sphere. The projected points are the
intersection points of the feature vectors and the unit sphere. The second row shows the angle distribution of both positive pairs and negative pairs (we choose
class 1 and class 2 from the subset to construct positive and negative pairs). Orange area indicates positive pairs while blue indicates negative pairs. All angles
are represented in radian. Note that, this visualization experiment uses a 6-class subset of the CASIA-WebFace dataset.

deﬁned by A-Softmax loss makes full use of the superior
learning capability of deeper architectures. A-Softmax loss
greatly improve the veriﬁcation accuracy from 98.20% to
99.42% on LFW, and from 93.4% to 95.0% on YTF. On
the contrary, the improvement of deeper standard CNNs is
unsatisfactory and also easily get saturated (from 96.60% to
97.75% on LFW, from 91.1% to 93.1% on YTF).

Figure 6: Accuracy (%) on LFW and YTF with different number of convo-
lutional layers. Left side is for LFW, while right side is for YTF.

4.3. Experiments on LFW and YTF

LFW dataset [9] includes 13,233 face images from 5749
different identities, and YTF dataset [35] includes 3,424
videos from 1,595 different individuals. Both datasets con-
tains faces with large variations in pose, expression and
illuminations. We follow the unrestricted with labeled out-
side data protocol [8] on both datasets. The performance of
SphereFace are evaluated on 6,000 face pairs from LFW and
5,000 video pairs from YTF. The results are given in Table 4.
For contrastive loss and center loss, we follow the FR con-
vention to form a weighted combination with softmax loss.
The weights are selected via cross validation on training set.
For L-Softmax [16], we also use m = 4. All the compared

Method
DeepFace [30]
FaceNet [22]
Deep FR [20]
DeepID2+ [27]
DeepID2+ [27]
Baidu [15]
Center Face [34]
Yi et al. [37]
Ding et al. [2]
Liu et al. [16]
Softmax Loss
Softmax+Contrastive [26]
Triplet Loss [22]
L-Softmax Loss [16]
Softmax+Center Loss [34]
SphereFace

Models
3
1
1
1
25
1
1
1
1
1
1
1
1
1
1
1

Data
4M*
200M*
2.6M
300K*
300K*
1.3M*
0.7M*
WebFace
WebFace
WebFace
WebFace
WebFace
WebFace
WebFace
WebFace
WebFace

LFW YTF
91.4
97.35
99.65
95.1
97.3
98.95
N/A
98.70
93.2
99.47
N/A
99.13
94.9
99.28
92.2
97.73
N/A
98.43
N/A
98.71
93.1
97.88
93.5
98.78
93.4
98.70
94.0
99.10
94.4
99.05
95.0
99.42

Table 4: Accuracy (%) on LFW and YTF dataset. * denotes the outside data
is private (not publicly available). For fair comparison, all loss functions
(including ours) we implemented use 64-layer CNN architecture in Table 2.

loss functions share the same 64-layer CNN architecture.

Most of the existing face veriﬁcation systems achieve
high performance with huge training data or model ensem-
ble. While using single model trained on publicly available
dataset (CAISA-WebFace, relatively small and having noisy
labels), SphereFace achieves 99.42% and 95.0% accuracies
on LFW and YTF datasets. It is the current best performance
trained on WebFace and considerably better than the other
models trained on the same dataset. Compared with models
trained on high-quality private datasets, SphereFace is still
very competitive, outperforming most of the existing results
in Table 4. One should notice that our single model perfor-
mance is only worse than Google FaceNet which is trained
with more than 200 million data.

Figure 7: CMC and ROC curves of different methods under the small training set protocol.

Method
NTechLAB - facenx large
Vocord - DeepVo1
Deepsense - Large
Shanghai Tech
Google - FaceNet v8
Beijing FaceAll_Norm_1600
Beijing FaceAll_1600
Deepsense - Small
SIAT_MMLAB
Barebones FR - cnn
NTechLAB - facenx_small
3DiVi Company - tdvm6
Softmax Loss
Softmax+Contrastive Loss [26]
Triplet Loss [22]
L-Softmax Loss [16]
Softmax+Center Loss [34]
SphereFace (single model)
SphereFace (3-patch ensemble)

protocol
Large
Large
Large
Large
Large
Large
Large
Small
Small
Small
Small
Small
Small
Small
Small
Small
Small
Small
Small

Rank1 Acc.
73.300
75.127
74.799
74.049
70.496
64.804
63.977
70.983
65.233
59.363
58.218
33.705
54.855
65.219
64.797
67.128
65.494
72.729
75.766

Ver.
85.081
67.318
87.764
86.369
86.473
67.118
63.960
82.851
76.720
59.036
66.366
36.927
65.925
78.865
78.322
80.423
80.146
85.561
89.142

Table 5: Performance (%) on MegaFace challenge. “Rank-1 Acc.” indicates
rank-1 identiﬁcation accuracy with 1M distractors, and “Ver.” indicates
veriﬁcation TAR for 10−6 FAR. TAR and FAR denote True Accept Rate
and False Accept Rate respectively. For fair comparison, all loss functions
(including ours) we implemented use the same deep CNN architecture.

For fair comparison, we also implement the softmax loss,
contrastive loss, center loss, triplet loss, L-Softmax loss [16]
and train them with the same 64-layer CNN architecture as
A-Softmax loss. As can be observed in Table 4, SphereFace
consistently outperforms the features learned by all these
compared losses, showing its superiority in FR tasks.

4.4. Experiments on MegaFace Challenge

MegaFace dataset [18] is a recently released testing bench-
mark with very challenging task to evaluate the performance
of face recognition methods at the million scale of distractors.
MegaFace dataset contains a gallery set and a probe set. The
gallery set contains more than 1 million images from 690K
different individuals. The probe set consists of two existing
datasets: Facescrub [19] and FGNet. MegaFace has several
testing scenarios including identiﬁcation, veriﬁcation and
pose invariance under two protocols (large or small training
set). The training set is viewed as small if it is less than
0.5M. We evaluate SphereFace under the small training set

protocol. We adopt two testing protocols: face identiﬁcation
and veriﬁcation. The results are given in Fig. 7 and Tabel
5. Note that we use simple 3-patch feature concatenation
ensemble as the ﬁnal performance of SphereFace.

Fig. 7 and Tabel 5 show that SphereFace (3 patches en-
semble) beats the second best result by a large margins (4.8%
for rank-1 identiﬁcation rate and 6.3% for veriﬁcation rate)
on MegaFace benchmark under the small training dataset
protocol. Compared to the models trained on large dataset
(500 million for Google and 18 million for NTechLAB), our
method still performs better (0.64% for id. rate and 1.4%
for veri. rate). Moreover, in contrast to their sophisticated
network design, we only employ typical CNN architecture
supervised by A-Softamx to achieve such excellent perfor-
mance. For single model SphereFace, the accuracy of face
identiﬁcation and veriﬁcation are still 72.73% and 85.56%
respectively, which already outperforms most state-of-the-
art methods. For better evaluation, we also implement the
softmax loss, contrastive loss, center loss, triplet loss and L-
Softmax loss [16]. Compared to these loss functions trained
with the same CNN architecture and dataset, SphereFace also
shows signiﬁcant and consistent improvements. These re-
sults convincingly demonstrate that the proposed SphereFace
is well designed for open-set face recognition. One can also
see that learning features with large inter-class angular mar-
gin can signiﬁcantly improve the open-set FR performance.

5. Concluding Remarks

This paper presents a novel deep hypersphere embedding
approach for face recognition. In speciﬁc, we propose the
angular softmax loss for CNNs to learn discriminative face
features (SphereFace) with angular margin. A-Softmax loss
renders nice geometric interpretation by constraining learned
features to be discriminative on a hypersphere manifold,
which intrinsically matches the prior that faces also lie on
a non-linear manifold. This connection makes A-Softmax
very effective for learning face representation. Competitive
results on several popular face benchmarks demonstrate the
superiority and great potentials of our approach. We also
believe A-Softmax loss could also beneﬁt some other tasks
like object recognition, person re-identiﬁcation, etc.

References

[1] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity
metric discriminatively, with application to face veriﬁcation.
In CVPR, 2005. 3

[2] C. Ding and D. Tao. Robust face recognition via multimodal
deep face representation. IEEE TMM, 17(11):2049–2058,
2015. 7

[3] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduc-
tion by learning an invariant mapping. In CVPR, 2006. 2,
3

[4] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1, 6

[5] X. He, S. Yan, Y. Hu, P. Niyogi, and H.-J. Zhang. Face
recognition using laplacianfaces. TPAMI, 27(3):328–340,
2005. 2

[6] E. Hoffer and N. Ailon. Deep metric learning using triplet

network. arXiv preprint:1412.6622, 2014. 3

[7] J. Hu, J. Lu, and Y.-P. Tan. Discriminative deep metric learn-
ing for face veriﬁcation in the wild. In CVPR, 2014. 3
[8] G. B. Huang and E. Learned-Miller. Labeled faces in the
wild: Updates and new reporting procedures. Dept. Comput.
Sci., Univ. Massachusetts Amherst, Amherst, MA, USA, Tech.
Rep, pages 14–003, 2014. 1, 7

[9] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller.
Labeled faces in the wild: A database for studying face recog-
nition in unconstrained environments. Technical report, Tech-
nical Report, 2007. 7

[10] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long,
R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Con-
volutional architecture for fast feature embedding. arXiv
preprint:1408.5093, 2014. 6

[11] I. Kemelmacher-Shlizerman, S. M. Seitz, D. Miller, and
E. Brossard. The megaface benchmark: 1 million faces for
recognition at scale. In CVPR, 2016. 1

[12] M. Köstinger, M. Hirzer, P. Wohlhart, P. M. Roth, and
H. Bischof. Large scale metric learning from equivalence
constraints. In CVPR, 2012. 3

[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks. In
NIPS, 2012. 1

[14] K.-C. Lee, J. Ho, M.-H. Yang, and D. Kriegman. Video-based
face recognition using probabilistic appearance manifolds. In
CVPR, 2003. 2

[15] J. Liu, Y. Deng, and C. Huang. Targeting ultimate ac-
arXiv

curacy: Face recognition via deep embedding.
preprint:1506.07310, 2015. 3, 7

[16] W. Liu, Y. Wen, Z. Yu, and M. Yang. Large-margin softmax
loss for convolutional neural networks. In ICML, 2016. 2, 3,
7, 8, 10, 11, 12

[17] J. Lu, G. Wang, W. Deng, P. Moulin, and J. Zhou. Multi-
manifold deep metric learning for image set classiﬁcation. In
CVPR, 2015. 3

[18] D. Miller, E. Brossard, S. Seitz, and I. Kemelmacher-
Shlizerman. Megaface: A million faces for recognition at
scale. arXiv preprint:1505.02108, 2015. 8

[19] H.-W. Ng and S. Winkler. A data-driven approach to cleaning

large face datasets. In ICIP, 2014. 8

[20] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face

recognition. In BMVC, 2015. 6, 7

[21] A. Ross and A. K. Jain. Multimodal biometrics: An overview.
In Signal Processing Conference, 2004 12th European, pages
1221–1224. IEEE, 2004. 1

[22] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A
uniﬁed embedding for face recognition and clustering. In
CVPR, 2015. 1, 2, 3, 6, 7, 8

[23] K. Simonyan and A. Zisserman. Very deep convolu-
tional networks for large-scale image recognition. arXiv
preprint:1409.1556, 2014. 1

[24] H. O. Song, Y. Xiang, S. Jegelka, and S. Savarese. Deep
metric learning via lifted structured feature embedding. In
CVPR, 2016. 3

[25] Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning face
representation by joint identiﬁcation-veriﬁcation. In NIPS,
2014. 1, 2, 3

[26] Y. Sun, X. Wang, and X. Tang. Deep learning face represen-
tation from predicting 10,000 classes. In CVPR, 2014. 2, 3,
7, 8

[27] Y. Sun, X. Wang, and X. Tang. Deeply learned face repre-
sentations are sparse, selective, and robust. In CVPR, 2015.
7

[28] Y. Sun, X. Wang, and X. Tang. Sparsifying neural network
connections for face recognition. In CVPR, 2016. 2, 3
[29] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper
with convolutions. In CVPR, 2015. 1

[30] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface:
Closing the gap to human-level performance in face veriﬁca-
tion. In CVPR, 2014. 2, 3, 6, 7

[31] A. Talwalkar, S. Kumar, and H. Rowley. Large-scale manifold

learning. In CVPR, 2008. 2

[32] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin,
B. Chen, and Y. Wu. Learning ﬁne-grained image similarity
with deep ranking. In CVPR, 2014. 3

[33] K. Q. Weinberger and L. K. Saul. Distance metric learning
for large margin nearest neighbor classiﬁcation. Journal of
Machine Learning Research, 10(Feb):207–244, 2009. 3
[34] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discriminative
feature learning approach for deep face recognition. In ECCV,
2016. 1, 2, 3, 7, 8

[35] L. Wolf, T. Hassner, and I. Maoz. Face recognition in un-
constrained videos with matched background similarity. In
CVPR, 2011. 7

[36] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. Dis-
tance metric learning with application to clustering with side-
information. NIPS, 2003. 3

[37] D. Yi, Z. Lei, S. Liao, and S. Z. Li. Learning face represen-
tation from scratch. arXiv preprint:1411.7923, 2014. 2, 6,
7

[38] Y. Ying and P. Li. Distance metric learning with eigenvalue

optimization. JMLR, 13(Jan):1–26, 2012. 3

[39] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao. Joint face detec-
tion and alignment using multi-task cascaded convolutional
networks. arXiv preprint:1604.02878, 2016. 5

A. The intuition of removing the last ReLU

Appendix

Standard CNNs usually connect ReLU to the bottom of FC1, so the learned features will only distribute in the non-negative
range [0, +∞), which limits the feasible learning space (angle) for the CNNs. To address this shortcoming, both SphereFace
and [16] ﬁrst propose to remove the ReLU nonlinearity that is connected to the bottom of FC1 in SphereFace networks.
Intuitively, removing the ReLU can greatly beneﬁt the feature learning, since it provides larger feasible learning space (from
angular perspective).

Visualization on MNIST. Fig. 8 shows the 2-D visualization of feature distributions in MNIST with and without the last
ReLU. One can observe with ReLU the 2-D feature could only distribute in the ﬁrst quadrant. Without the last ReLU, the
learned feature distribution is much more reasonable.

Figure 8: 2-D visualization before and after removing the last ReLU.

B. Normalizing the weights could reduce the prior caused by the training data imbalance

We have emphasized in the main paper that normalizing the weights can give better geometric interpretation. Besides this,
we also justify why we want to normalize the weights from a different perspective. We ﬁnd that normalizing the weights can
implicitly reduce the prior brought by the training data imbalance issue (e.g., the long-tail distribution of the training data). In
other words, we argue that normalizing the weights can partially address the training data imbalance problem.

Figure 9: Norm of Wi and sample number of class i in MNIST dataset and CASIA-WebFace dataset.

We have an empirical study on the relation between the sample number of each class and the 2-norm of the weights
corresponding to the same class (the i-th column of W is associated to the i-th class). By computing the norm of Wi and
sample number of class i with respect to each class (see Fig. 9), we ﬁnd that the larger sample number a class has, the larger
the associated norm of weights tends to be. We argue that the norm of weights Wi with respect to class i is largely determined
by its sample distribution and sample number. Therefore, norm of weights Wi, ∀i can be viewed as a learned prior hidden in
training datasets. Eliminating such prior is often beneﬁcial to face veriﬁcation. This is because face veriﬁcation requires to test
on a dataset whose idenities can not appear in training datasets, so the prior from training dataset should not be transferred to
the testing. This prior may even be harmful to face veriﬁcation performance. To eliminate such prior, we normalize the norm
of weights of FC26.

6FC2 refers to the fully connected layer in the softmax loss (or A-Softmax loss).

C. Empirical experiment of zeroing out the biases

Figure 10: Biases of last fully connected layer learned in CASIA-WebFace dataset.

Standard CNNs usually preserve the bias term in the fully connected layers, but these bias terms make it difﬁcult to analyze
the proposed A-Softmax loss. This is because SphereFace aims to optimize the angle and produce the angular margin. With
bias of FC2, the angular geometry interpretation becomes much more difﬁcult to analyze. To facilitate the analysis, we zero
out the bias of FC2 following [16]. By setting the bias of FC2 to zero, the A-Softmax loss has clear geometry interpretation
and therefore becomes much easier to analyze. We show all the biases of FC2 from a CASIA-pretrained model in Fig. 10. One
can observe that the most of the biases are near zero, indicating these biases are not necessarily useful for face veriﬁcation.

Figure 11: 2-D visualization with and without bias of last fully connected layer in MNIST.

Visualization on MNIST. We visualize the 2-D feature distribution in MNIST dataset with and without bias in Fig. 11.
One can observe that zeroing out the bias has no direct inﬂuence on the feature distribution. The features learned with and
without bias can both make full use of the learning space.

D. 2D visualization of A-Softmax loss on MNIST

We visualize the 2-D feature distribution on MNIST in Fig. 12. It is obvious that with larger m the learned features become
much more discriminative due to the larger inter-class angular margin. Most importantly, the learned discriminative features
also generalize really well in the testing set.

Figure 12: 2-D MNIST visualization of features learned by the softmax loss and the A-Softmax loss (m = 2, 3, 4).

E. Angular Fisher score for evaluating the feature discriminativeness and ablation study on our

proposed modiﬁcations

We ﬁrst propose an angular Fisher score for evaluating the feature discriminativeness in angular margin feature learning.

The angular Fisher score (AFS) is deﬁned by

AF S =

Sw
Sb

(11)

where the within-class scatter value is deﬁned as Sw = (cid:80)
(1 − cos(cid:104)xj, mi(cid:105)) and the between-class scatter value is
deﬁned as Sb = (cid:80)
i ni(1 − cos(cid:104)mi, m(cid:105)). Xi is the i-th class samples, mi is the mean vector of features from class i, m is the
mean vector of the whole dataset, and ni is the sample number of class i. In general, the lower the ﬁsher value is, the more
discriminative the features are.

xj ∈Xi

(cid:80)

i

Next, we perform a comprehensive ablation study on all the proposed modiﬁcations: removing last ReLU, removing Biases,
normalizing weights and applying A-Softmax loss. The experiments are performed using the 4-layer CNN described in Table 2.
The models are trained on CASIA dataset and tested on LFW dataset. The setting is exactly the same as the LFW experiment
in the main paper. As shown in Table 6, we could observe that all our modiﬁcation leads to peformance improvement and our
A-Softmax could greatly increase the angular feature discriminativeness.

CNN
A
B
C
D
E
F
G

Remove Last ReLU
No
Yes
Yes
Yes
Yes
Yes
Yes

Remove Biases
No
No
Yes
Yes
Yes
Yes
Yes

Normalize Weights
No
No
No
Yes
Yes
Yes
Yes

A-Softmax
No
No
No
No
Yes (m=2)
Yes (m=3)
Yes (m=4)

Accuracy
95.13
96.37
96.40
96.63
97.67
97.82
98.20

Angular Fisher Score
0.3477
0.2835
0.2815
0.2462
0.2277
0.1791
0.1709

Table 6: Veriﬁcation accuracy (%) on LFW dataset.

F. Experiments on MegaFace with different convolutional layers

We also perform the experiment on MegaFace dataset with CNN of different convolutional layers. The results in Table 7
show that the A-Softmax loss could make best use of the network capacity. With more convolutional layers, the A-Softmax
loss (i.e., SphereFace) performs better. Most notably, SphereFace with only 4 convolutional layer could peform better than the
softmax loss with 64 convolutional layers, which validates the superiority of our A-Softmax loss.

Method

protocol

Softmax Loss (64 conv layers)
SphereFace (4 conv layers)
SphereFace (10 conv layers)
SphereFace (20 conv layers)
SphereFace (36 conv layers)
SphereFace (64 conv layers)

Small
Small
Small
Small
Small
Small

Rank-1 Id. Acc.
with 1M distractors
54.855
57.529
65.335
69.623
71.257
72.729

Ver. TAR
for 10−6 FAR
65.925
68.547
78.069
83.159
84.052
85.561

Table 7: Performance (%) on MegaFace challenge with different convolutional layers. TAR and FAR denote True Accept Rate and False Accept Rate
respectively. For all the SphereFace models, we use m = 4. With larger m and proper network optimization, the performance could potentially keep
increasing.

G. The annealing optimization strategy for A-Softmax loss

The optimization of the A-Softmax loss is similar to the L-Softmax loss [16]. We use an annealing optimization strategy
to train the network with A-Softmax loss. To be simple, the annealing strategy is essentially supervising the newtork from
an easy task (i.e., large λ) gradually to a difﬁcult task (i.e., small λ). Speciﬁcally, we let fyi = λ(cid:107)xi(cid:107) cos(θyi )+(cid:107)xi(cid:107)ψ(θyi )
and
start the stochastic gradient descent initially with a very large λ (it is equivalent to optimizing the original softmax). Then
we gradually reduce λ during training. Ideally λ can be gradually reduced to zero, but in practice, a small value will usually
sufﬁce. In most of our face experiments, decaying λ to 5 has already lead to impressive results. Smaller λ could potentially
yield a better performance but is also more difﬁcult to train.

1+λ

H. Details of the 3-patch ensemble strategy in MegaFace challenge

We adopt a common strategy to perform the 3-patch ensemble, as shown in Fig. 13. Although using more patches could keep
increasing the performance, but considering the tradeoff between efﬁciency and accuracy, we use 3-patch simple concatenation
ensemble (without the use of PCA). The 3 patches can be selected by cross-validation. The 3 patches we use in the paper are
exactly the same as in Fig. 13.

Figure 13: 3-Patch ensembles in SphereFace for MegaFace challenge.

SphereFace: Deep Hypersphere Embedding for Face Recognition

Weiyang Liu1 Yandong Wen2

1Georgia Institute of Technology

Zhiding Yu2 Ming Li3 Bhiksha Raj2
2Carnegie Mellon University

3Sun Yat-Sen University

Le Song1

wyliu@gatech.edu, {yandongw,yzhiding}@andrew.cmu.edu, lsong@cc.gatech.edu

8
1
0
2
 
n
a
J
 
9
2
 
 
]

V
C
.
s
c
[
 
 
4
v
3
6
0
8
0
.
4
0
7
1
:
v
i
X
r
a

Abstract

This paper addresses deep face recognition (FR) prob-
lem under open-set protocol, where ideal face features are
expected to have smaller maximal intra-class distance than
minimal inter-class distance under a suitably chosen met-
ric space. However, few existing algorithms can effectively
achieve this criterion. To this end, we propose the angular
softmax (A-Softmax) loss that enables convolutional neural
networks (CNNs) to learn angularly discriminative features.
Geometrically, A-Softmax loss can be viewed as imposing
discriminative constraints on a hypersphere manifold, which
intrinsically matches the prior that faces also lie on a mani-
fold. Moreover, the size of angular margin can be quantita-
tively adjusted by a parameter m. We further derive speciﬁc
m to approximate the ideal feature criterion. Extensive anal-
ysis and experiments on Labeled Face in the Wild (LFW),
Youtube Faces (YTF) and MegaFace Challenge show the
superiority of A-Softmax loss in FR tasks. The code has also
been made publicly available1.

1. Introduction

Recent years have witnessed the great success of convo-
lutional neural networks (CNNs) in face recognition (FR).
Owing to advanced network architectures [13, 23, 29, 4] and
discriminative learning approaches [25, 22, 34], deep CNNs
have boosted the FR performance to an unprecedent level.
Typically, face recognition can be categorized as face identi-
ﬁcation and face veriﬁcation [8, 11]. The former classiﬁes a
face to a speciﬁc identity, while the latter determines whether
a pair of faces belongs to the same identity.

In terms of testing protocol, face recognition can be eval-
uated under closed-set or open-set settings, as illustrated
in Fig. 1. For closed-set protocol, all testing identities are
predeﬁned in training set. It is natural to classify testing face
images to the given identities. In this scenario, face veriﬁca-
tion is equivalent to performing identiﬁcation for a pair of
faces respectively (see left side of Fig. 1). Therefore, closed-
set FR can be well addressed as a classiﬁcation problem,

1See the code at https://github.com/wy1iu/sphereface.

Figure 1: Comparison of open-set and closed-set face recognition.

where features are expected to be separable. For open-set
protocol, the testing identities are usually disjoint from the
training set, which makes FR more challenging yet close to
practice. Since it is impossible to classify faces to known
identities in training set, we need to map faces to a discrimi-
native feature space. In this scenario, face identiﬁcation can
be viewed as performing face veriﬁcation between the probe
face and every identity in the gallery (see right side of Fig. 1).
Open-set FR is essentially a metric learning problem, where
the key is to learn discriminative large-margin features.

Desired features for open-set FR are expected to satisfy
the criterion that the maximal intra-class distance is smaller
than the minimal inter-class distance under a certain metric
space. This criterion is necessary if we want to achieve
perfect accuracy using nearest neighbor. However, learning
features with this criterion is generally difﬁcult because of
the intrinsically large intra-class variation and high inter-
class similarity [21] that faces exhibit.

Few CNN-based approaches are able to effectively for-
mulate the aforementioned criterion in loss functions. Pi-

Figure 2: Comparison among softmax loss, modiﬁed softmax loss and A-Softmax loss. In this toy experiment, we construct a CNN to learn 2-D features on a
subset of the CASIA face dataset. In speciﬁc, we set the output dimension of FC1 layer as 2 and visualize the learned features. Yellow dots represent the
ﬁrst class face features, while purple dots represent the second class face features. One can see that features learned by the original softmax loss can not be
classiﬁed simply via angles, while modiﬁed softmax loss can. Our A-Softmax loss can further increase the angular margin of learned features.

oneering work [30, 26] learn face features via the softmax
loss2, but softmax loss only learns separable features that are
not discriminative enough. To address this, some methods
combine softmax loss with contrastive loss [25, 28] or center
loss [34] to enhance the discrimination power of features.
[22] adopts triplet loss to supervise the embedding learning,
leading to state-of-the-art face recognition results. However,
center loss only explicitly encourages intra-class compact-
ness. Both contrastive loss [3] and triplet loss [22] can not
constrain on each individual sample, and thus require care-
fully designed pair/triplet mining procedure, which is both
time-consuming and performance-sensitive.

It seems to be a widely recognized choice to impose Eu-
clidean margin to learned features, but a question arises: Is
Euclidean margin always suitable for learning discrimina-
tive face features? To answer this question, we ﬁrst look into
how Euclidean margin based losses are applied to FR.

Most recent approaches [25, 28, 34] combine Euclidean
margin based losses with softmax loss to construct a joint
supervision. However, as can be observed from Fig. 2, the
features learned by softmax loss have intrinsic angular dis-
tribution (also veriﬁed by [34]). In some sense, Euclidean
margin based losses are incompatible with softmax loss, so
it is not well motivated to combine these two type of losses.
In this paper, we propose to incorporate angular margin
instead. We start with a binary-class case to analyze the
softmax loss. The decision boundary in softmax loss is
(W1 − W2)x + b1 − b2 = 0, where Wi and bi are weights
and bias3 in softmax loss, respectively.
If we deﬁne x
as a feature vector and constrain (cid:107)W1(cid:107) = (cid:107)W2(cid:107) = 1 and
b1 = b2 = 0, the decision boundary becomes (cid:107)x(cid:107)(cos(θ1) −
cos(θ2)) = 0, where θi is the angle between Wi and x. The
new decision boundary only depends on θ1 and θ2. Modiﬁed
softmax loss is able to directly optimize angles, enabling
CNNs to learn angularly distributed features (Fig. 2).

Compared to original softmax loss, the features learned
by modiﬁed softmax loss are angularly distributed, but not
necessarily more discriminative. To the end, we generalize
the modiﬁed softmax loss to angular softmax (A-Softmax)

2Following [16], we deﬁne the softmax loss as the combination of the

last fully connected layer, softmax function and cross-entropy loss.

3If not speciﬁed, the weights and biases in the paper are corresponding

to the fully connected layer in the softmax loss.

loss. Speciﬁcally, we introduce an integer m (m ≥ 1) to
quantitatively control the decision boundary.
In binary-
class case, the decision boundaries for class 1 and class
2 become (cid:107)x(cid:107)(cos(mθ1)−cos(θ2))=0 and (cid:107)x(cid:107)(cos(θ1)−
cos(mθ2))=0, respectively. m quantitatively controls the
size of angular margin. Furthermore, A-Softmax loss can be
easily generalized to multiple classes, similar to softmax loss.
By optimizing A-Softmax loss, the decision regions become
more separated, simultaneously enlarging the inter-class mar-
gin and compressing the intra-class angular distribution.

A-Softmax loss has clear geometric interpretation. Su-
pervised by A-Softmax loss, the learned features construct a
discriminative angular distance metric that is equivalent to
geodesic distance on a hypersphere manifold. A-Softmax
loss can be interpreted as constraining learned features to
be discriminative on a hypersphere manifold, which intrin-
sically matches the prior that face images lie on a manifold
[14, 5, 31]. The close connection between A-Softmax loss
and hypersphere manifolds makes the learned features more
effective for face recognition. For this reason, we term the
learned features as SphereFace.

Moreover, A-Softmax loss can quantitatively adjust the
angular margin via a parameter m, enabling us to do quanti-
tative analysis. In the light of this, we derive lower bounds
for the parameter m to approximate the desired open-set
FR criterion that the maximal intra-class distance should be
smaller than the minimal inter-class distance.

Our major contributions can be summarized as follows:

(1) We propose A-Softmax loss for CNNs to learn dis-
criminative face features with clear and novel geometric
interpretation. The learned features discriminatively span
on a hypersphere manifold, which intrinsically matches the
prior that faces also lie on a manifold.

(2) We derive lower bounds for m such that A-Softmax
loss can approximate the learning task that minimal inter-
class distance is larger than maximal intra-class distance.

(3) We are the very ﬁrst to show the effectiveness of
angular margin in FR. Trained on publicly available CASIA
dataset [37], SphereFace achieves competitive results on
several benchmarks, including Labeled Face in the Wild
(LFW), Youtube Faces (YTF) and MegaFace Challenge 1.

2. Related Work

Metric learning. Metric learning aims to learn a sim-
ilarity (distance) function. Traditional metric learning
[36, 33, 12, 38] usually learns a matrix A for a distance met-
ric (cid:107)x1 − x2(cid:107)A = (cid:112)(x1 − x2)T A(x1 − x2) upon the given
features x1, x2. Recently, prevailing deep metric learning
[7, 17, 24, 30, 25, 22, 34] usually uses neural networks
to automatically learn discriminative features x1, x2 fol-
lowed by a simple distance metric such as Euclidean dis-
tance (cid:107)x1 − x2(cid:107)2. Most widely used loss functions for deep
metric learning are contrastive loss [1, 3] and triplet loss
[32, 22, 6], and both impose Euclidean margin to features.

Deep face recognition. Deep face recognition is ar-
guably one of the most active research area in the past few
years. [30, 26] address the open-set FR using CNNs super-
vised by softmax loss, which essentially treats open-set FR
as a multi-class classiﬁcation problem. [25] combines con-
trastive loss and softmax loss to jointly supervise the CNN
training, greatly boosting the performance. [22] uses triplet
loss to learn a uniﬁed face embedding. Training on nearly
200 million face images, they achieve current state-of-the-art
FR accuracy. Inspired by linear discriminant analysis, [34]
proposes center loss for CNNs and also obtains promising
performance. In general, current well-performing CNNs
[28, 15] for FR are mostly built on either contrastive loss or
triplet loss. One could notice that state-of-the-art FR meth-
ods usually adopt ideas (e.g. contrastive loss, triplet loss)
from metric learning, showing open-set FR could be well
addressed by discriminative metric learning.

L-Softmax loss [16] also implicitly involves the concept
of angles. As a regularization method, it shows great im-
provement on closed-set classiﬁcation problems. Differently,
A-Softmax loss is developed to learn discriminative face em-
bedding. The explicit connections to hypersphere manifold
makes our learned features particularly suitable for open-set
FR problem, as veriﬁed by our experiments. In addition,
the angular margin in A-Softmax loss is explicitly imposed
and can be quantitatively controlled (e.g. lower bounds to
approximate desired feature criterion), while [16] can only
be analyzed qualitatively.

3. Deep Hypersphere Embedding

3.1. Revisiting the Softmax Loss

We revisit the softmax loss by looking into the decision
criteria of softmax loss. In binary-class case, the posterior
probabilities obtained by softmax loss are

p1 =

p2 =

exp(W T

1 x + b1) + exp(W T

2 x + b2)

exp(W T

1 x + b1)

exp(W T

2 x + b2)

exp(W T

1 x + b1) + exp(W T

2 x + b2)

(1)

(2)

1 x + b1 and W T

i, respectively. The predicted label will be assigned to
class 1 if p1 > p2 and class 2 if p1 < p2. By comparing
p1 and p2, it is clear that W T
2 x + b2 de-
termine the classiﬁcation result. The decision boundary is
(W1 − W2)x + b1 − b2 = 0. We then rewrite W T
i x + bi as
(cid:107)W T
i (cid:107)(cid:107)x(cid:107) cos(θi) + bi where θi is the angle between Wi
and x. Notice that if we normalize the weights and zero
the biases ((cid:107)Wi(cid:107) = 1, bi =0), the posterior probabilities be-
come p1=(cid:107)x(cid:107) cos(θ1) and p2=(cid:107)x(cid:107) cos(θ2). Note that p1
and p2 share the same x, the ﬁnal result only depends on
the angles θ1 and θ2. The decision boundary also becomes
cos(θ1)−cos(θ2)=0 (i.e. angular bisector of vector W1 and
W2). Although the above analysis is built on binary-calss
case, it is trivial to generalize the analysis to multi-class case.
During training, the modiﬁed softmax loss ((cid:107)Wi(cid:107)=1, bi =0)
encourages features from the i-th class to have smaller angle
θi (larger cosine distance) than others, which makes angles
between Wi and features a reliable metric for classiﬁcation.
To give a formal expression for the modiﬁed softmax loss,
we ﬁrst deﬁne the input feature xi and its label yi. The
original softmax loss can be written as

L =

Li =

1
N

(cid:88)

i

1
N

(cid:88)

i

− log (cid:0) efyi
j efj

(cid:80)

(cid:1)

(3)

where fj denotes the j-th element (j ∈ [1, K], K is the
class number) of the class score vector f , and N is the
In CNNs, f is usually the
number of training samples.
output of a fully connected layer W , so fj = W T
j xi + bj
and fyi = W T
xi + byi where xi, Wj, Wyi are the i-th
yi
training sample, the j-th and yi-th column of W respectively.
We further reformulate Li in Eq. (3) as
Li = − log (cid:0) eW T
j eW T

j xi+bj

xi+byi

(cid:80)

yi

(cid:1)

(4)

= − log (cid:0) e(cid:107)Wyi (cid:107)(cid:107)xi(cid:107) cos(θyi,i)+byi
j e(cid:107)Wj (cid:107)(cid:107)xi(cid:107) cos(θj,i)+bj

(cid:80)

(cid:1)

in which θj,i(0 ≤ θj,i ≤ π) is the angle between vector Wj
and xi. As analyzed above, we ﬁrst normalize (cid:107)Wj(cid:107) = 1, ∀j
in each iteration and zero the biases. Then we have the
modiﬁed softmax loss:

Lmodiﬁed =

1
N

(cid:88)

i

− log (cid:0) e(cid:107)xi(cid:107) cos(θyi,i)
j e(cid:107)xi(cid:107) cos(θj,i)

(cid:80)

(cid:1)

(5)

Although we can learn features with angular boundary with
the modiﬁed softmax loss, these features are still not neces-
sarily discriminative. Since we use angles as the distance
metric, it is natural to incorporate angular margin to learned
features in order to enhance the discrimination power. To
this end, we propose a novel way to combine angular margin.

3.2. Introducing Angular Margin to Softmax Loss

where x is the learned feature vector. Wi and bi are weights
and bias of last fully connected layer corresponding to class

Instead of designing a new type of loss function and con-
structing a weighted combination with softmax loss (similar

Loss Function
Softmax Loss
Modiﬁed Softmax Loss

A-Softmax Loss

Decision Boundary
(W1 − W2)x + b1 − b2 = 0
(cid:107)x(cid:107)(cos θ1 − cos θ2) = 0
(cid:107)x(cid:107)(cos mθ1 − cos θ2) = 0 for class 1
(cid:107)x(cid:107)(cos θ1 − cos mθ2) = 0 for class 2

Table 1: Comparison of decision boundaries in binary case. Note that, θi is
the angle between Wi and x.

to contrastive loss) , we propose a more natural way to learn
angular margin. From the previous analysis of softmax loss,
we learn that decision boundaries can greatly affect the fea-
ture distribution, so our basic idea is to manipulate decision
boundaries to produce angular margin. We ﬁrst give a moti-
vating binary-class example to explain how our idea works.
Assume a learned feature x from class 1 is given and θi
is the angle between x and Wi, it is known that the modiﬁed
softmax loss requires cos(θ1) > cos(θ2) to correctly classify
x. But what if we instead require cos(mθ1) > cos(θ2) where
m ≥ 2 is a integer in order to correctly classify x? It is
essentially making the decision more stringent than previ-
ous, because we require a lower bound4 of cos(θ1) to be
larger than cos(θ2). The decision boundary for class 1 is
cos(mθ1) = cos(θ2). Similarly, if we require cos(mθ2) >
cos(θ1) to correctly classify features from class 2, the deci-
sion boundary for class 2 is cos(mθ2) = cos(θ1). Suppose
all training samples are correctly classiﬁed, such decision
boundaries will produce an angular margin of m−1
2 where
θ1
2 is the angle between W1 and W2. From angular per-
spective, correctly classifying x from identity 1 requires
θ1 < θ2
m , while correctly classifying x from identity 2 re-
quires θ2 < θ1
m . Both are more difﬁcult than original θ1 < θ2
and θ2 < θ1, respectively. By directly formulating this idea
into the modiﬁed softmax loss Eq. (5), we have

m+1 θ1

Lang =

(cid:88)

− log (cid:0)

1
N

i

e(cid:107)xi(cid:107) cos(mθyi,i)

(cid:1)

e(cid:107)xi(cid:107) cos(mθyi,i) + (cid:80)

e(cid:107)xi(cid:107) cos(θj,i)

j(cid:54)=yi

(6)
where θyi,i has to be in the range of [0, π
m ]. In order to
get rid of this restriction and make it optimizable in CNNs,
we expand the deﬁnition range of cos(θyi,i) by generaliz-
ing it to a monotonically decreasing angle function ψ(θyi,i)
which should be equal to cos(θyi,i) in [0, π
m ]. Therefore, our
proposed A-Softmax loss is formulated as:

Lang =

(cid:88)

− log (cid:0)

1
N

i

e(cid:107)xi(cid:107)ψ(θyi,i)

(cid:1) (7)

e(cid:107)xi(cid:107)ψ(θyi,i) + (cid:80)

e(cid:107)xi(cid:107) cos(θj,i)

j(cid:54)=yi

in which we deﬁne ψ(θyi,i) = (−1)k cos(mθyi,i) − 2k,
θyi,i ∈ [ kπ
m ] and k ∈ [0, m − 1]. m ≥ 1 is an inte-
ger that controls the size of angular margin. When m = 1, it
becomes the modiﬁed softmax loss.

m , (k+1)π

The justiﬁcation of A-Softmax loss can also be made from
decision boundary perspective. A-Softmax loss adopts dif-
ferent decision boundary for different class (each boundary

4The inequality cos(θ1) > cos(mθ1) holds while θ1 ∈ [0, π

m ], m ≥ 2.

Figure 3: Geometry Interpretation of Euclidean margin loss (e.g. contrastive
loss, triplet loss, center loss, etc.), modiﬁed softmax loss and A-Softmax
loss. The ﬁrst row is 2D feature constraint, and the second row is 3D feature
constraint. The orange region indicates the discriminative constraint for
class 1, while the green region is for class 2.

is more stringent than the original), thus producing angular
margin. The comparison of decision boundaries is given in
Table 1. From original softmax loss to modiﬁed softmax
loss, it is from optimizing inner product to optimizing angles.
From modiﬁed softmax loss to A-Softmax loss, it makes
the decision boundary more stringent and separated. The
angular margin increases with larger m and be zero if m = 1.
Supervised by A-Softmax loss, CNNs learn face features
with geometrically interpretable angular margin. Because A-
Softmax loss requires Wi = 1, bi = 0, it makes the prediction
only depends on angles between the sample x and Wi. So
x can be classiﬁed to the identity with smallest angle. The
parameter m is added for the purpose of learning an angular
margin between different identities.

To facilitate gradient computation and back propagation,
we replace cos(θj,i) and cos(mθyi,i) with the expressions
only containing W and xi, which is easily done by deﬁni-
tion of cosine and multi-angle formula (also the reason why
we need m to be an integer). Without θ, we can compute
derivative with respect to x and W , similar to softmax loss.

3.3. Hypersphere Interpretation of A-Softmax Loss

A-Softmax loss has stronger requirements for a correct
classiﬁcation when m ≥ 2, which generates an angular classi-
ﬁcation margin between learned features of different classes.
A-Softmax loss not only imposes discriminative power to
the learned features via angular margin, but also renders nice
and novel hypersphere interpretation. As shown in Fig. 3,
A-Softmax loss is equivalent to learning features that are
discriminative on a hypersphere manifold, while Euclidean
margin losses learn features in Euclidean space.

To simplify, We take the binary case to analyze the hyper-
sphere interpretation. Considering a sample x from class 1
and two column weights W1, W2, the classiﬁcation rule for

j v2

A-Softmax loss is cos(mθ1) > cos(θ2), equivalently mθ1 <
θ2. Notice that θ1, θ2 are equal to their corresponding arc
5 on unit hypersphere {vj, ∀j| (cid:80)
length ω1, ω2
j =1, v≥0}.
Because (cid:107)W (cid:107)1 = (cid:107)W (cid:107)2 = 1, the decision replies on the arc
length ω1 and ω2. The decision boundary is equivalent to
mω1 = ω2, and the constrained region for correctly classify-
ing x to class 1 is mω1 < ω2. Geometrically speaking, this
is a hypercircle-like region lying on a hypersphere manifold.
For example, it is a circle-like region on the unit sphere in
3D case, as illustrated in Fig. 3. Note that larger m leads to
smaller hypercircle-like region for each class, which is an ex-
plicit discriminative constraint on a manifold. For better un-
derstanding, Fig. 3 provides 2D and 3D visualizations. One
can see that A-Softmax loss imposes arc length constraint on
a unit circle in 2D case and circle-like region constraint on a
unit sphere in 3D case. Our analysis shows that optimizing
angles with A-Softmax loss essentially makes the learned
features more discriminative on a hypersphere.

3.4. Properties of A-Softmax Loss

Property 1. A-Softmax loss deﬁnes a large angular mar-
gin learning task with adjustable difﬁculty. With larger m,
the angular margin becomes larger, the constrained region
on the manifold becomes smaller, and the corresponding
learning task also becomes more difﬁcult.

We know that the larger m is, the larger angular margin
A-Softmax loss constrains. There exists a minimal m that
constrains the maximal intra-class angular distance to be
smaller than the minimal inter-class angular distance, which
can also be observed in our experiments.

Deﬁnition 1 (minimal m for desired feature distribution).
mmin is the minimal value such that while m > mmin, A-
Softmax loss deﬁnes a learning task where the maximal intra-
class angular feature distance is constrained to be smaller
than the minimal inter-class angular feature distance.

3.

√

Property 2 (lower bound of mmin in binary-class case). In
binary-class case, we have mmin ≥ 2 +
Proof. We consider the space spaned by W1 and W2. Be-
cause m ≥ 2, it is easy to obtain the maximal angle that class
1 spans is θ12
m+1 where θ12 is the angle between W1
and W2. To require the maximal intra-class feature angular
distance smaller than the minimal inter-class feature angular
distance, we need to constrain

m−1 + θ12

+

θ12
θ12
m + 1
m − 1
(cid:125)
(cid:123)(cid:122)
(cid:124)
max intra-class angle

≤

2π − θ12
m + 1

+

≤

θ12
m + 1
(cid:125)

(cid:124)

(cid:123)(cid:122)
max intra-class angle

(cid:124)

(cid:125)

(m − 1)θ12
m + 1
(cid:123)(cid:122)
min inter-class angle
(m − 1)θ12
m + 1
(cid:123)(cid:122)
min inter-class angle

(cid:125)

(cid:124)

, θ12 ≤

m − 1
m

π

, θ12 >

m − 1
m

π

(8)

(9)

5ωi is the shortest arc length (geodesic distance) between Wi and the
projected point of sample x on the unit hypersphere, while the correspond-
ing θi is the angle between Wi and x.

After solving these two inequalities, we could have mmin ≥
2 +

3, which is a lower bound for binary case.

√

Property 3 (lower bound of mmin in multi-class case). Un-
der the assumption that Wi, ∀i are uniformly spaced in the
Euclidean space, we have mmin ≥ 3.

Proof. We consider the 2D k-class (k ≥ 3) scenario for the
lower bound. Because Wi, ∀i are uniformly spaced in the
2D Euclidean space, we have θi+1
is the
angle between Wi and Wi+1. Since Wi, ∀i are symmetric,
we only need to analyze one of them. For the i-th class (Wi),
We need to constrain

k where θi+1

i = 2π

i

+

θi
θi+1
i−1
i
m + 1
m + 1
(cid:124)
(cid:125)
(cid:123)(cid:122)
max intra-class angle

≤ min

(cid:124)

(cid:26) (m − 1)θi+1
i
m + 1

,

(m − 1)θi

(cid:27)

i−1

m + 1

(10)

(cid:123)(cid:122)
min inter-class angle

(cid:125)

After solving this inequality, we obtain mmin ≥ 3, which is
a lower bound for multi-class case.

Based on this, we use m = 4 to approximate the desired
feature distribution criteria. Since the lower bounds are not
necessarily tight, giving a tighter lower bound and a upper
bound under certain conditions is also possible, which we
leave to the future work. Experiments also show that larger
m consistently works better and m = 4 will usually sufﬁce.

3.5. Discussions

Why angular margin. First and most importantly, angu-
lar margin directly links to discriminativeness on a manifold,
which intrinsically matches the prior that faces also lie on
a manifold. Second, incorporating angular margin to soft-
max loss is actually a more natural choice. As Fig. 2 shows,
features learned by the original softmax loss have an intrin-
sic angular distribution. So directly combining Euclidean
margin constraints with softmax loss is not reasonable.

Comparison with existing losses. In deep FR task, the
most popular and well-performing loss functions include
contrastive loss, triplet loss and center loss. First, they only
impose Euclidean margin to the learned features (w/o normal-
ization), while ours instead directly considers angular margin
which is naturally motivated. Second, both contrastive loss
and triplet loss suffer from data expansion when constituting
the pairs/triplets from the training set, while ours requires no
sample mining and imposes discriminative constraints to the
entire mini-batches (compared to contrastive and triplet loss
that only affect a few representative pairs/triplets).

4. Experiments (more in Appendix)

4.1. Experimental Settings

Preprocessing. We only use standard preprocessing. The
face landmarks in all images are detected by MTCNN [39].
The cropped faces are obtained by similarity transforma-
tion. Each pixel ([0, 255]) in RGB images is normalized by
subtracting 127.5 and then being divided by 128.

Layer

4-layer CNN

10-layer CNN

Conv1.x

[3×3, 64]×1, S2

[3×3, 64]×1, S2

20-layer CNN
[3×3, 64]×1, S2
(cid:34)

(cid:35)

3 × 3, 64

× 1

3 × 3, 64

36-layer CNN
[3×3, 64]×1, S2
(cid:34)

(cid:35)

3 × 3, 64

× 2

3 × 3, 64

64-layer CNN
[3×3, 64]×1, S2
(cid:34)

(cid:35)

3 × 3, 64

× 3

3 × 3, 64

Conv2.x

[3×3, 128]×1, S2

Conv3.x

[3×3, 256]×1, S2

[3×3, 128]×1, S2
(cid:34)

3 × 3, 128

(cid:35)

[3×3, 128]×1, S2
(cid:34)

3 × 3, 128

(cid:35)

[3×3, 128]×1, S2
(cid:34)

3 × 3, 128

(cid:35)

[3×3, 128]×1, S2
(cid:34)

3 × 3, 128

(cid:35)

3 × 3, 128

3 × 3, 128

3 × 3, 128

3 × 3, 128

[3×3, 256]×1, S2
(cid:34)

3 × 3, 256

(cid:35)

[3×3, 256]×1, S2
(cid:34)

3 × 3, 256

(cid:35)

[3×3, 256]×1, S2
(cid:34)

3 × 3, 256

(cid:35)

[3×3, 256]×1, S2
(cid:34)
3 × 3, 256

(cid:35)

× 8

× 1

× 2

3 × 3, 256

3 × 3, 256

3 × 3, 256

Conv4.x

[3×3, 512]×1, S2

[3×3, 512]×1, S2

FC1

512

512

[3×3, 512]×1, S2
(cid:34)

3 × 3, 512

(cid:35)

[3×3, 512]×1, S2
(cid:34)

3 × 3, 512

(cid:35)

3 × 3, 512

512

3 × 3, 512

512

× 4

× 8

× 2

× 16

3 × 3, 256
[3×3, 512]×1, S2
(cid:34)

3 × 3, 512

(cid:35)

× 3

3 × 3, 512

512

× 2

× 4

× 1

Table 2: Our CNN architectures with different convolutional layers. Conv1.x, Conv2.x and Conv3.x denote convolution units that may contain multiple
convolution layers and residual units are shown in double-column brackets. E.g., [3×3, 64]×4 denotes 4 cascaded convolution layers with 64 ﬁlters of size
3×3, and S2 denotes stride 2. FC1 is the fully connected layer.

CNNs Setup. Caffe [10] is used to implement A-Softmax
loss and CNNs. The general framework to train and extract
SphereFace features is shown in Fig. 4. We use residual
units [4] in our CNN architecture. For fairness, all compared
methods use the same CNN architecture (including residual
units) as SphereFace. CNNs with different depths (4, 10, 20,
36, 64) are used to better evaluate our method. The speciﬁc
settings for difffernt CNNs we used are given in Table 2.
According to the analysis in Section 3.4, we usually set m
as 4 in A-Softmax loss unless speciﬁed. These models are
trained with batch size of 128 on four GPUs. The learning
rate begins with 0.1 and is divided by 10 at the 16K, 24K
iterations. The training is ﬁnished at 28K iterations.

Figure 4: Training and Extracting SphereFace features.

Training Data. We use publicly available web-collected
training dataset CASIA-WebFace [37] (after excluding the
images of identities appearing in testing sets) to train our
CNN models. CASIA-WebFace has 494,414 face images
belonging to 10,575 different individuals. These face images
are horizontally ﬂipped for data augmentation. Notice that
the scale of our training data (0.49M) is relatively small, es-
pecially compared to other private datasets used in DeepFace
[30] (4M), VGGFace [20] (2M) and FaceNet [22] (200M).
Testing. We extract the deep features (SphereFace) from
the output of the FC1 layer. For all experiments, the ﬁnal
representation of a testing face is obtained by concatenating
its original face features and its horizontally ﬂipped features.
The score (metric) is computed by the cosine distance of two
features. The nearest neighbor classiﬁer and thresholding
are used for face identiﬁcation and veriﬁcation, respectively.

4.2. Exploratory Experiments

Effect of m. To show that larger m leads to larger an-
gular margin (i.e. more discriminative feature distribution
on manifold), we perform a toy example with different m.
We train A-Softmax loss with 6 individuals that have the
most samples in CASIA-WebFace. We set the output feature
dimension (FC1) as 3 and visualize the training samples in
Fig. 5. One can observe that larger m leads to more dis-
criminative distribution on the sphere and also larger angular
margin, as expected. We also use class 1 (blue) and class
2 (dark green) to construct positive and negative pairs to
evaluate the angle distribution of features from the same
class and different classes. The angle distribution of positive
and negative pairs (the second row of Fig. 5) quantitatively
shows the angular margin becomes larger while m increases
and every class also becomes more distinct with each other.
Besides visual comparison, we also perform face recogni-
tion on LFW and YTF to evaluate the effect of m. For fair
comparison, we use 64-layer CNN (Table 2) for all losses.
Results are given in Table 3. One can observe that while
m becomes larger, the accuracy of A-Softmax loss also be-
comes better, which shows that larger angular margin can
bring stronger discrimination power.

Dataset
LFW
YTF

Original
97.88
93.1

m=1
97.90
93.2

m=2
98.40
93.8

m=3
99.25
94.4

m=4
99.42
95.0

Table 3: Accuracy(%) comparison of different m (A-Softmax loss) and
original softmax loss on LFW and YTF dataset.

Effect of CNN architectures. We train A-Softmax loss
(m = 4) and original softmax loss with different number
of convolution layers. Speciﬁc CNN architectures can be
found in Table 2. From Fig. 6, one can observe that A-
Softmax loss consistently outperforms CNNs with softmax
loss (1.54%∼1.91%), indicating that A-Softmax loss is more
suitable for open-set FR. Besides, the difﬁcult learning task

Figure 5: Visualization of features learned with different m. The ﬁrst row shows the 3D features projected on the unit sphere. The projected points are the
intersection points of the feature vectors and the unit sphere. The second row shows the angle distribution of both positive pairs and negative pairs (we choose
class 1 and class 2 from the subset to construct positive and negative pairs). Orange area indicates positive pairs while blue indicates negative pairs. All angles
are represented in radian. Note that, this visualization experiment uses a 6-class subset of the CASIA-WebFace dataset.

deﬁned by A-Softmax loss makes full use of the superior
learning capability of deeper architectures. A-Softmax loss
greatly improve the veriﬁcation accuracy from 98.20% to
99.42% on LFW, and from 93.4% to 95.0% on YTF. On
the contrary, the improvement of deeper standard CNNs is
unsatisfactory and also easily get saturated (from 96.60% to
97.75% on LFW, from 91.1% to 93.1% on YTF).

Figure 6: Accuracy (%) on LFW and YTF with different number of convo-
lutional layers. Left side is for LFW, while right side is for YTF.

4.3. Experiments on LFW and YTF

LFW dataset [9] includes 13,233 face images from 5749
different identities, and YTF dataset [35] includes 3,424
videos from 1,595 different individuals. Both datasets con-
tains faces with large variations in pose, expression and
illuminations. We follow the unrestricted with labeled out-
side data protocol [8] on both datasets. The performance of
SphereFace are evaluated on 6,000 face pairs from LFW and
5,000 video pairs from YTF. The results are given in Table 4.
For contrastive loss and center loss, we follow the FR con-
vention to form a weighted combination with softmax loss.
The weights are selected via cross validation on training set.
For L-Softmax [16], we also use m = 4. All the compared

Method
DeepFace [30]
FaceNet [22]
Deep FR [20]
DeepID2+ [27]
DeepID2+ [27]
Baidu [15]
Center Face [34]
Yi et al. [37]
Ding et al. [2]
Liu et al. [16]
Softmax Loss
Softmax+Contrastive [26]
Triplet Loss [22]
L-Softmax Loss [16]
Softmax+Center Loss [34]
SphereFace

Models
3
1
1
1
25
1
1
1
1
1
1
1
1
1
1
1

Data
4M*
200M*
2.6M
300K*
300K*
1.3M*
0.7M*
WebFace
WebFace
WebFace
WebFace
WebFace
WebFace
WebFace
WebFace
WebFace

LFW YTF
91.4
97.35
99.65
95.1
97.3
98.95
N/A
98.70
93.2
99.47
N/A
99.13
94.9
99.28
92.2
97.73
N/A
98.43
N/A
98.71
93.1
97.88
93.5
98.78
93.4
98.70
94.0
99.10
94.4
99.05
95.0
99.42

Table 4: Accuracy (%) on LFW and YTF dataset. * denotes the outside data
is private (not publicly available). For fair comparison, all loss functions
(including ours) we implemented use 64-layer CNN architecture in Table 2.

loss functions share the same 64-layer CNN architecture.

Most of the existing face veriﬁcation systems achieve
high performance with huge training data or model ensem-
ble. While using single model trained on publicly available
dataset (CAISA-WebFace, relatively small and having noisy
labels), SphereFace achieves 99.42% and 95.0% accuracies
on LFW and YTF datasets. It is the current best performance
trained on WebFace and considerably better than the other
models trained on the same dataset. Compared with models
trained on high-quality private datasets, SphereFace is still
very competitive, outperforming most of the existing results
in Table 4. One should notice that our single model perfor-
mance is only worse than Google FaceNet which is trained
with more than 200 million data.

Figure 7: CMC and ROC curves of different methods under the small training set protocol.

Method
NTechLAB - facenx large
Vocord - DeepVo1
Deepsense - Large
Shanghai Tech
Google - FaceNet v8
Beijing FaceAll_Norm_1600
Beijing FaceAll_1600
Deepsense - Small
SIAT_MMLAB
Barebones FR - cnn
NTechLAB - facenx_small
3DiVi Company - tdvm6
Softmax Loss
Softmax+Contrastive Loss [26]
Triplet Loss [22]
L-Softmax Loss [16]
Softmax+Center Loss [34]
SphereFace (single model)
SphereFace (3-patch ensemble)

protocol
Large
Large
Large
Large
Large
Large
Large
Small
Small
Small
Small
Small
Small
Small
Small
Small
Small
Small
Small

Rank1 Acc.
73.300
75.127
74.799
74.049
70.496
64.804
63.977
70.983
65.233
59.363
58.218
33.705
54.855
65.219
64.797
67.128
65.494
72.729
75.766

Ver.
85.081
67.318
87.764
86.369
86.473
67.118
63.960
82.851
76.720
59.036
66.366
36.927
65.925
78.865
78.322
80.423
80.146
85.561
89.142

Table 5: Performance (%) on MegaFace challenge. “Rank-1 Acc.” indicates
rank-1 identiﬁcation accuracy with 1M distractors, and “Ver.” indicates
veriﬁcation TAR for 10−6 FAR. TAR and FAR denote True Accept Rate
and False Accept Rate respectively. For fair comparison, all loss functions
(including ours) we implemented use the same deep CNN architecture.

For fair comparison, we also implement the softmax loss,
contrastive loss, center loss, triplet loss, L-Softmax loss [16]
and train them with the same 64-layer CNN architecture as
A-Softmax loss. As can be observed in Table 4, SphereFace
consistently outperforms the features learned by all these
compared losses, showing its superiority in FR tasks.

4.4. Experiments on MegaFace Challenge

MegaFace dataset [18] is a recently released testing bench-
mark with very challenging task to evaluate the performance
of face recognition methods at the million scale of distractors.
MegaFace dataset contains a gallery set and a probe set. The
gallery set contains more than 1 million images from 690K
different individuals. The probe set consists of two existing
datasets: Facescrub [19] and FGNet. MegaFace has several
testing scenarios including identiﬁcation, veriﬁcation and
pose invariance under two protocols (large or small training
set). The training set is viewed as small if it is less than
0.5M. We evaluate SphereFace under the small training set

protocol. We adopt two testing protocols: face identiﬁcation
and veriﬁcation. The results are given in Fig. 7 and Tabel
5. Note that we use simple 3-patch feature concatenation
ensemble as the ﬁnal performance of SphereFace.

Fig. 7 and Tabel 5 show that SphereFace (3 patches en-
semble) beats the second best result by a large margins (4.8%
for rank-1 identiﬁcation rate and 6.3% for veriﬁcation rate)
on MegaFace benchmark under the small training dataset
protocol. Compared to the models trained on large dataset
(500 million for Google and 18 million for NTechLAB), our
method still performs better (0.64% for id. rate and 1.4%
for veri. rate). Moreover, in contrast to their sophisticated
network design, we only employ typical CNN architecture
supervised by A-Softamx to achieve such excellent perfor-
mance. For single model SphereFace, the accuracy of face
identiﬁcation and veriﬁcation are still 72.73% and 85.56%
respectively, which already outperforms most state-of-the-
art methods. For better evaluation, we also implement the
softmax loss, contrastive loss, center loss, triplet loss and L-
Softmax loss [16]. Compared to these loss functions trained
with the same CNN architecture and dataset, SphereFace also
shows signiﬁcant and consistent improvements. These re-
sults convincingly demonstrate that the proposed SphereFace
is well designed for open-set face recognition. One can also
see that learning features with large inter-class angular mar-
gin can signiﬁcantly improve the open-set FR performance.

5. Concluding Remarks

This paper presents a novel deep hypersphere embedding
approach for face recognition. In speciﬁc, we propose the
angular softmax loss for CNNs to learn discriminative face
features (SphereFace) with angular margin. A-Softmax loss
renders nice geometric interpretation by constraining learned
features to be discriminative on a hypersphere manifold,
which intrinsically matches the prior that faces also lie on
a non-linear manifold. This connection makes A-Softmax
very effective for learning face representation. Competitive
results on several popular face benchmarks demonstrate the
superiority and great potentials of our approach. We also
believe A-Softmax loss could also beneﬁt some other tasks
like object recognition, person re-identiﬁcation, etc.

References

[1] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity
metric discriminatively, with application to face veriﬁcation.
In CVPR, 2005. 3

[2] C. Ding and D. Tao. Robust face recognition via multimodal
deep face representation. IEEE TMM, 17(11):2049–2058,
2015. 7

[3] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduc-
tion by learning an invariant mapping. In CVPR, 2006. 2,
3

[4] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1, 6

[5] X. He, S. Yan, Y. Hu, P. Niyogi, and H.-J. Zhang. Face
recognition using laplacianfaces. TPAMI, 27(3):328–340,
2005. 2

[6] E. Hoffer and N. Ailon. Deep metric learning using triplet

network. arXiv preprint:1412.6622, 2014. 3

[7] J. Hu, J. Lu, and Y.-P. Tan. Discriminative deep metric learn-
ing for face veriﬁcation in the wild. In CVPR, 2014. 3
[8] G. B. Huang and E. Learned-Miller. Labeled faces in the
wild: Updates and new reporting procedures. Dept. Comput.
Sci., Univ. Massachusetts Amherst, Amherst, MA, USA, Tech.
Rep, pages 14–003, 2014. 1, 7

[9] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller.
Labeled faces in the wild: A database for studying face recog-
nition in unconstrained environments. Technical report, Tech-
nical Report, 2007. 7

[10] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long,
R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Con-
volutional architecture for fast feature embedding. arXiv
preprint:1408.5093, 2014. 6

[11] I. Kemelmacher-Shlizerman, S. M. Seitz, D. Miller, and
E. Brossard. The megaface benchmark: 1 million faces for
recognition at scale. In CVPR, 2016. 1

[12] M. Köstinger, M. Hirzer, P. Wohlhart, P. M. Roth, and
H. Bischof. Large scale metric learning from equivalence
constraints. In CVPR, 2012. 3

[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks. In
NIPS, 2012. 1

[14] K.-C. Lee, J. Ho, M.-H. Yang, and D. Kriegman. Video-based
face recognition using probabilistic appearance manifolds. In
CVPR, 2003. 2

[15] J. Liu, Y. Deng, and C. Huang. Targeting ultimate ac-
arXiv

curacy: Face recognition via deep embedding.
preprint:1506.07310, 2015. 3, 7

[16] W. Liu, Y. Wen, Z. Yu, and M. Yang. Large-margin softmax
loss for convolutional neural networks. In ICML, 2016. 2, 3,
7, 8, 10, 11, 12

[17] J. Lu, G. Wang, W. Deng, P. Moulin, and J. Zhou. Multi-
manifold deep metric learning for image set classiﬁcation. In
CVPR, 2015. 3

[18] D. Miller, E. Brossard, S. Seitz, and I. Kemelmacher-
Shlizerman. Megaface: A million faces for recognition at
scale. arXiv preprint:1505.02108, 2015. 8

[19] H.-W. Ng and S. Winkler. A data-driven approach to cleaning

large face datasets. In ICIP, 2014. 8

[20] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face

recognition. In BMVC, 2015. 6, 7

[21] A. Ross and A. K. Jain. Multimodal biometrics: An overview.
In Signal Processing Conference, 2004 12th European, pages
1221–1224. IEEE, 2004. 1

[22] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A
uniﬁed embedding for face recognition and clustering. In
CVPR, 2015. 1, 2, 3, 6, 7, 8

[23] K. Simonyan and A. Zisserman. Very deep convolu-
tional networks for large-scale image recognition. arXiv
preprint:1409.1556, 2014. 1

[24] H. O. Song, Y. Xiang, S. Jegelka, and S. Savarese. Deep
metric learning via lifted structured feature embedding. In
CVPR, 2016. 3

[25] Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning face
representation by joint identiﬁcation-veriﬁcation. In NIPS,
2014. 1, 2, 3

[26] Y. Sun, X. Wang, and X. Tang. Deep learning face represen-
tation from predicting 10,000 classes. In CVPR, 2014. 2, 3,
7, 8

[27] Y. Sun, X. Wang, and X. Tang. Deeply learned face repre-
sentations are sparse, selective, and robust. In CVPR, 2015.
7

[28] Y. Sun, X. Wang, and X. Tang. Sparsifying neural network
connections for face recognition. In CVPR, 2016. 2, 3
[29] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper
with convolutions. In CVPR, 2015. 1

[30] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface:
Closing the gap to human-level performance in face veriﬁca-
tion. In CVPR, 2014. 2, 3, 6, 7

[31] A. Talwalkar, S. Kumar, and H. Rowley. Large-scale manifold

learning. In CVPR, 2008. 2

[32] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin,
B. Chen, and Y. Wu. Learning ﬁne-grained image similarity
with deep ranking. In CVPR, 2014. 3

[33] K. Q. Weinberger and L. K. Saul. Distance metric learning
for large margin nearest neighbor classiﬁcation. Journal of
Machine Learning Research, 10(Feb):207–244, 2009. 3
[34] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discriminative
feature learning approach for deep face recognition. In ECCV,
2016. 1, 2, 3, 7, 8

[35] L. Wolf, T. Hassner, and I. Maoz. Face recognition in un-
constrained videos with matched background similarity. In
CVPR, 2011. 7

[36] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. Dis-
tance metric learning with application to clustering with side-
information. NIPS, 2003. 3

[37] D. Yi, Z. Lei, S. Liao, and S. Z. Li. Learning face represen-
tation from scratch. arXiv preprint:1411.7923, 2014. 2, 6,
7

[38] Y. Ying and P. Li. Distance metric learning with eigenvalue

optimization. JMLR, 13(Jan):1–26, 2012. 3

[39] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao. Joint face detec-
tion and alignment using multi-task cascaded convolutional
networks. arXiv preprint:1604.02878, 2016. 5

A. The intuition of removing the last ReLU

Appendix

Standard CNNs usually connect ReLU to the bottom of FC1, so the learned features will only distribute in the non-negative
range [0, +∞), which limits the feasible learning space (angle) for the CNNs. To address this shortcoming, both SphereFace
and [16] ﬁrst propose to remove the ReLU nonlinearity that is connected to the bottom of FC1 in SphereFace networks.
Intuitively, removing the ReLU can greatly beneﬁt the feature learning, since it provides larger feasible learning space (from
angular perspective).

Visualization on MNIST. Fig. 8 shows the 2-D visualization of feature distributions in MNIST with and without the last
ReLU. One can observe with ReLU the 2-D feature could only distribute in the ﬁrst quadrant. Without the last ReLU, the
learned feature distribution is much more reasonable.

Figure 8: 2-D visualization before and after removing the last ReLU.

B. Normalizing the weights could reduce the prior caused by the training data imbalance

We have emphasized in the main paper that normalizing the weights can give better geometric interpretation. Besides this,
we also justify why we want to normalize the weights from a different perspective. We ﬁnd that normalizing the weights can
implicitly reduce the prior brought by the training data imbalance issue (e.g., the long-tail distribution of the training data). In
other words, we argue that normalizing the weights can partially address the training data imbalance problem.

Figure 9: Norm of Wi and sample number of class i in MNIST dataset and CASIA-WebFace dataset.

We have an empirical study on the relation between the sample number of each class and the 2-norm of the weights
corresponding to the same class (the i-th column of W is associated to the i-th class). By computing the norm of Wi and
sample number of class i with respect to each class (see Fig. 9), we ﬁnd that the larger sample number a class has, the larger
the associated norm of weights tends to be. We argue that the norm of weights Wi with respect to class i is largely determined
by its sample distribution and sample number. Therefore, norm of weights Wi, ∀i can be viewed as a learned prior hidden in
training datasets. Eliminating such prior is often beneﬁcial to face veriﬁcation. This is because face veriﬁcation requires to test
on a dataset whose idenities can not appear in training datasets, so the prior from training dataset should not be transferred to
the testing. This prior may even be harmful to face veriﬁcation performance. To eliminate such prior, we normalize the norm
of weights of FC26.

6FC2 refers to the fully connected layer in the softmax loss (or A-Softmax loss).

C. Empirical experiment of zeroing out the biases

Figure 10: Biases of last fully connected layer learned in CASIA-WebFace dataset.

Standard CNNs usually preserve the bias term in the fully connected layers, but these bias terms make it difﬁcult to analyze
the proposed A-Softmax loss. This is because SphereFace aims to optimize the angle and produce the angular margin. With
bias of FC2, the angular geometry interpretation becomes much more difﬁcult to analyze. To facilitate the analysis, we zero
out the bias of FC2 following [16]. By setting the bias of FC2 to zero, the A-Softmax loss has clear geometry interpretation
and therefore becomes much easier to analyze. We show all the biases of FC2 from a CASIA-pretrained model in Fig. 10. One
can observe that the most of the biases are near zero, indicating these biases are not necessarily useful for face veriﬁcation.

Figure 11: 2-D visualization with and without bias of last fully connected layer in MNIST.

Visualization on MNIST. We visualize the 2-D feature distribution in MNIST dataset with and without bias in Fig. 11.
One can observe that zeroing out the bias has no direct inﬂuence on the feature distribution. The features learned with and
without bias can both make full use of the learning space.

D. 2D visualization of A-Softmax loss on MNIST

We visualize the 2-D feature distribution on MNIST in Fig. 12. It is obvious that with larger m the learned features become
much more discriminative due to the larger inter-class angular margin. Most importantly, the learned discriminative features
also generalize really well in the testing set.

Figure 12: 2-D MNIST visualization of features learned by the softmax loss and the A-Softmax loss (m = 2, 3, 4).

E. Angular Fisher score for evaluating the feature discriminativeness and ablation study on our

proposed modiﬁcations

We ﬁrst propose an angular Fisher score for evaluating the feature discriminativeness in angular margin feature learning.

The angular Fisher score (AFS) is deﬁned by

AF S =

Sw
Sb

(11)

where the within-class scatter value is deﬁned as Sw = (cid:80)
(1 − cos(cid:104)xj, mi(cid:105)) and the between-class scatter value is
deﬁned as Sb = (cid:80)
i ni(1 − cos(cid:104)mi, m(cid:105)). Xi is the i-th class samples, mi is the mean vector of features from class i, m is the
mean vector of the whole dataset, and ni is the sample number of class i. In general, the lower the ﬁsher value is, the more
discriminative the features are.

xj ∈Xi

(cid:80)

i

Next, we perform a comprehensive ablation study on all the proposed modiﬁcations: removing last ReLU, removing Biases,
normalizing weights and applying A-Softmax loss. The experiments are performed using the 4-layer CNN described in Table 2.
The models are trained on CASIA dataset and tested on LFW dataset. The setting is exactly the same as the LFW experiment
in the main paper. As shown in Table 6, we could observe that all our modiﬁcation leads to peformance improvement and our
A-Softmax could greatly increase the angular feature discriminativeness.

CNN
A
B
C
D
E
F
G

Remove Last ReLU
No
Yes
Yes
Yes
Yes
Yes
Yes

Remove Biases
No
No
Yes
Yes
Yes
Yes
Yes

Normalize Weights
No
No
No
Yes
Yes
Yes
Yes

A-Softmax
No
No
No
No
Yes (m=2)
Yes (m=3)
Yes (m=4)

Accuracy
95.13
96.37
96.40
96.63
97.67
97.82
98.20

Angular Fisher Score
0.3477
0.2835
0.2815
0.2462
0.2277
0.1791
0.1709

Table 6: Veriﬁcation accuracy (%) on LFW dataset.

F. Experiments on MegaFace with different convolutional layers

We also perform the experiment on MegaFace dataset with CNN of different convolutional layers. The results in Table 7
show that the A-Softmax loss could make best use of the network capacity. With more convolutional layers, the A-Softmax
loss (i.e., SphereFace) performs better. Most notably, SphereFace with only 4 convolutional layer could peform better than the
softmax loss with 64 convolutional layers, which validates the superiority of our A-Softmax loss.

Method

protocol

Softmax Loss (64 conv layers)
SphereFace (4 conv layers)
SphereFace (10 conv layers)
SphereFace (20 conv layers)
SphereFace (36 conv layers)
SphereFace (64 conv layers)

Small
Small
Small
Small
Small
Small

Rank-1 Id. Acc.
with 1M distractors
54.855
57.529
65.335
69.623
71.257
72.729

Ver. TAR
for 10−6 FAR
65.925
68.547
78.069
83.159
84.052
85.561

Table 7: Performance (%) on MegaFace challenge with different convolutional layers. TAR and FAR denote True Accept Rate and False Accept Rate
respectively. For all the SphereFace models, we use m = 4. With larger m and proper network optimization, the performance could potentially keep
increasing.

G. The annealing optimization strategy for A-Softmax loss

The optimization of the A-Softmax loss is similar to the L-Softmax loss [16]. We use an annealing optimization strategy
to train the network with A-Softmax loss. To be simple, the annealing strategy is essentially supervising the newtork from
an easy task (i.e., large λ) gradually to a difﬁcult task (i.e., small λ). Speciﬁcally, we let fyi = λ(cid:107)xi(cid:107) cos(θyi )+(cid:107)xi(cid:107)ψ(θyi )
and
start the stochastic gradient descent initially with a very large λ (it is equivalent to optimizing the original softmax). Then
we gradually reduce λ during training. Ideally λ can be gradually reduced to zero, but in practice, a small value will usually
sufﬁce. In most of our face experiments, decaying λ to 5 has already lead to impressive results. Smaller λ could potentially
yield a better performance but is also more difﬁcult to train.

1+λ

H. Details of the 3-patch ensemble strategy in MegaFace challenge

We adopt a common strategy to perform the 3-patch ensemble, as shown in Fig. 13. Although using more patches could keep
increasing the performance, but considering the tradeoff between efﬁciency and accuracy, we use 3-patch simple concatenation
ensemble (without the use of PCA). The 3 patches can be selected by cross-validation. The 3 patches we use in the paper are
exactly the same as in Fig. 13.

Figure 13: 3-Patch ensembles in SphereFace for MegaFace challenge.

SphereFace: Deep Hypersphere Embedding for Face Recognition

Weiyang Liu1 Yandong Wen2

1Georgia Institute of Technology

Zhiding Yu2 Ming Li3 Bhiksha Raj2
2Carnegie Mellon University

3Sun Yat-Sen University

Le Song1

wyliu@gatech.edu, {yandongw,yzhiding}@andrew.cmu.edu, lsong@cc.gatech.edu

8
1
0
2
 
n
a
J
 
9
2
 
 
]

V
C
.
s
c
[
 
 
4
v
3
6
0
8
0
.
4
0
7
1
:
v
i
X
r
a

Abstract

This paper addresses deep face recognition (FR) prob-
lem under open-set protocol, where ideal face features are
expected to have smaller maximal intra-class distance than
minimal inter-class distance under a suitably chosen met-
ric space. However, few existing algorithms can effectively
achieve this criterion. To this end, we propose the angular
softmax (A-Softmax) loss that enables convolutional neural
networks (CNNs) to learn angularly discriminative features.
Geometrically, A-Softmax loss can be viewed as imposing
discriminative constraints on a hypersphere manifold, which
intrinsically matches the prior that faces also lie on a mani-
fold. Moreover, the size of angular margin can be quantita-
tively adjusted by a parameter m. We further derive speciﬁc
m to approximate the ideal feature criterion. Extensive anal-
ysis and experiments on Labeled Face in the Wild (LFW),
Youtube Faces (YTF) and MegaFace Challenge show the
superiority of A-Softmax loss in FR tasks. The code has also
been made publicly available1.

1. Introduction

Recent years have witnessed the great success of convo-
lutional neural networks (CNNs) in face recognition (FR).
Owing to advanced network architectures [13, 23, 29, 4] and
discriminative learning approaches [25, 22, 34], deep CNNs
have boosted the FR performance to an unprecedent level.
Typically, face recognition can be categorized as face identi-
ﬁcation and face veriﬁcation [8, 11]. The former classiﬁes a
face to a speciﬁc identity, while the latter determines whether
a pair of faces belongs to the same identity.

In terms of testing protocol, face recognition can be eval-
uated under closed-set or open-set settings, as illustrated
in Fig. 1. For closed-set protocol, all testing identities are
predeﬁned in training set. It is natural to classify testing face
images to the given identities. In this scenario, face veriﬁca-
tion is equivalent to performing identiﬁcation for a pair of
faces respectively (see left side of Fig. 1). Therefore, closed-
set FR can be well addressed as a classiﬁcation problem,

1See the code at https://github.com/wy1iu/sphereface.

Figure 1: Comparison of open-set and closed-set face recognition.

where features are expected to be separable. For open-set
protocol, the testing identities are usually disjoint from the
training set, which makes FR more challenging yet close to
practice. Since it is impossible to classify faces to known
identities in training set, we need to map faces to a discrimi-
native feature space. In this scenario, face identiﬁcation can
be viewed as performing face veriﬁcation between the probe
face and every identity in the gallery (see right side of Fig. 1).
Open-set FR is essentially a metric learning problem, where
the key is to learn discriminative large-margin features.

Desired features for open-set FR are expected to satisfy
the criterion that the maximal intra-class distance is smaller
than the minimal inter-class distance under a certain metric
space. This criterion is necessary if we want to achieve
perfect accuracy using nearest neighbor. However, learning
features with this criterion is generally difﬁcult because of
the intrinsically large intra-class variation and high inter-
class similarity [21] that faces exhibit.

Few CNN-based approaches are able to effectively for-
mulate the aforementioned criterion in loss functions. Pi-

Figure 2: Comparison among softmax loss, modiﬁed softmax loss and A-Softmax loss. In this toy experiment, we construct a CNN to learn 2-D features on a
subset of the CASIA face dataset. In speciﬁc, we set the output dimension of FC1 layer as 2 and visualize the learned features. Yellow dots represent the
ﬁrst class face features, while purple dots represent the second class face features. One can see that features learned by the original softmax loss can not be
classiﬁed simply via angles, while modiﬁed softmax loss can. Our A-Softmax loss can further increase the angular margin of learned features.

oneering work [30, 26] learn face features via the softmax
loss2, but softmax loss only learns separable features that are
not discriminative enough. To address this, some methods
combine softmax loss with contrastive loss [25, 28] or center
loss [34] to enhance the discrimination power of features.
[22] adopts triplet loss to supervise the embedding learning,
leading to state-of-the-art face recognition results. However,
center loss only explicitly encourages intra-class compact-
ness. Both contrastive loss [3] and triplet loss [22] can not
constrain on each individual sample, and thus require care-
fully designed pair/triplet mining procedure, which is both
time-consuming and performance-sensitive.

It seems to be a widely recognized choice to impose Eu-
clidean margin to learned features, but a question arises: Is
Euclidean margin always suitable for learning discrimina-
tive face features? To answer this question, we ﬁrst look into
how Euclidean margin based losses are applied to FR.

Most recent approaches [25, 28, 34] combine Euclidean
margin based losses with softmax loss to construct a joint
supervision. However, as can be observed from Fig. 2, the
features learned by softmax loss have intrinsic angular dis-
tribution (also veriﬁed by [34]). In some sense, Euclidean
margin based losses are incompatible with softmax loss, so
it is not well motivated to combine these two type of losses.
In this paper, we propose to incorporate angular margin
instead. We start with a binary-class case to analyze the
softmax loss. The decision boundary in softmax loss is
(W1 − W2)x + b1 − b2 = 0, where Wi and bi are weights
and bias3 in softmax loss, respectively.
If we deﬁne x
as a feature vector and constrain (cid:107)W1(cid:107) = (cid:107)W2(cid:107) = 1 and
b1 = b2 = 0, the decision boundary becomes (cid:107)x(cid:107)(cos(θ1) −
cos(θ2)) = 0, where θi is the angle between Wi and x. The
new decision boundary only depends on θ1 and θ2. Modiﬁed
softmax loss is able to directly optimize angles, enabling
CNNs to learn angularly distributed features (Fig. 2).

Compared to original softmax loss, the features learned
by modiﬁed softmax loss are angularly distributed, but not
necessarily more discriminative. To the end, we generalize
the modiﬁed softmax loss to angular softmax (A-Softmax)

2Following [16], we deﬁne the softmax loss as the combination of the

last fully connected layer, softmax function and cross-entropy loss.

3If not speciﬁed, the weights and biases in the paper are corresponding

to the fully connected layer in the softmax loss.

loss. Speciﬁcally, we introduce an integer m (m ≥ 1) to
quantitatively control the decision boundary.
In binary-
class case, the decision boundaries for class 1 and class
2 become (cid:107)x(cid:107)(cos(mθ1)−cos(θ2))=0 and (cid:107)x(cid:107)(cos(θ1)−
cos(mθ2))=0, respectively. m quantitatively controls the
size of angular margin. Furthermore, A-Softmax loss can be
easily generalized to multiple classes, similar to softmax loss.
By optimizing A-Softmax loss, the decision regions become
more separated, simultaneously enlarging the inter-class mar-
gin and compressing the intra-class angular distribution.

A-Softmax loss has clear geometric interpretation. Su-
pervised by A-Softmax loss, the learned features construct a
discriminative angular distance metric that is equivalent to
geodesic distance on a hypersphere manifold. A-Softmax
loss can be interpreted as constraining learned features to
be discriminative on a hypersphere manifold, which intrin-
sically matches the prior that face images lie on a manifold
[14, 5, 31]. The close connection between A-Softmax loss
and hypersphere manifolds makes the learned features more
effective for face recognition. For this reason, we term the
learned features as SphereFace.

Moreover, A-Softmax loss can quantitatively adjust the
angular margin via a parameter m, enabling us to do quanti-
tative analysis. In the light of this, we derive lower bounds
for the parameter m to approximate the desired open-set
FR criterion that the maximal intra-class distance should be
smaller than the minimal inter-class distance.

Our major contributions can be summarized as follows:

(1) We propose A-Softmax loss for CNNs to learn dis-
criminative face features with clear and novel geometric
interpretation. The learned features discriminatively span
on a hypersphere manifold, which intrinsically matches the
prior that faces also lie on a manifold.

(2) We derive lower bounds for m such that A-Softmax
loss can approximate the learning task that minimal inter-
class distance is larger than maximal intra-class distance.

(3) We are the very ﬁrst to show the effectiveness of
angular margin in FR. Trained on publicly available CASIA
dataset [37], SphereFace achieves competitive results on
several benchmarks, including Labeled Face in the Wild
(LFW), Youtube Faces (YTF) and MegaFace Challenge 1.

2. Related Work

Metric learning. Metric learning aims to learn a sim-
ilarity (distance) function. Traditional metric learning
[36, 33, 12, 38] usually learns a matrix A for a distance met-
ric (cid:107)x1 − x2(cid:107)A = (cid:112)(x1 − x2)T A(x1 − x2) upon the given
features x1, x2. Recently, prevailing deep metric learning
[7, 17, 24, 30, 25, 22, 34] usually uses neural networks
to automatically learn discriminative features x1, x2 fol-
lowed by a simple distance metric such as Euclidean dis-
tance (cid:107)x1 − x2(cid:107)2. Most widely used loss functions for deep
metric learning are contrastive loss [1, 3] and triplet loss
[32, 22, 6], and both impose Euclidean margin to features.

Deep face recognition. Deep face recognition is ar-
guably one of the most active research area in the past few
years. [30, 26] address the open-set FR using CNNs super-
vised by softmax loss, which essentially treats open-set FR
as a multi-class classiﬁcation problem. [25] combines con-
trastive loss and softmax loss to jointly supervise the CNN
training, greatly boosting the performance. [22] uses triplet
loss to learn a uniﬁed face embedding. Training on nearly
200 million face images, they achieve current state-of-the-art
FR accuracy. Inspired by linear discriminant analysis, [34]
proposes center loss for CNNs and also obtains promising
performance. In general, current well-performing CNNs
[28, 15] for FR are mostly built on either contrastive loss or
triplet loss. One could notice that state-of-the-art FR meth-
ods usually adopt ideas (e.g. contrastive loss, triplet loss)
from metric learning, showing open-set FR could be well
addressed by discriminative metric learning.

L-Softmax loss [16] also implicitly involves the concept
of angles. As a regularization method, it shows great im-
provement on closed-set classiﬁcation problems. Differently,
A-Softmax loss is developed to learn discriminative face em-
bedding. The explicit connections to hypersphere manifold
makes our learned features particularly suitable for open-set
FR problem, as veriﬁed by our experiments. In addition,
the angular margin in A-Softmax loss is explicitly imposed
and can be quantitatively controlled (e.g. lower bounds to
approximate desired feature criterion), while [16] can only
be analyzed qualitatively.

3. Deep Hypersphere Embedding

3.1. Revisiting the Softmax Loss

We revisit the softmax loss by looking into the decision
criteria of softmax loss. In binary-class case, the posterior
probabilities obtained by softmax loss are

p1 =

p2 =

exp(W T

1 x + b1) + exp(W T

2 x + b2)

exp(W T

1 x + b1)

exp(W T

2 x + b2)

exp(W T

1 x + b1) + exp(W T

2 x + b2)

(1)

(2)

1 x + b1 and W T

i, respectively. The predicted label will be assigned to
class 1 if p1 > p2 and class 2 if p1 < p2. By comparing
p1 and p2, it is clear that W T
2 x + b2 de-
termine the classiﬁcation result. The decision boundary is
(W1 − W2)x + b1 − b2 = 0. We then rewrite W T
i x + bi as
(cid:107)W T
i (cid:107)(cid:107)x(cid:107) cos(θi) + bi where θi is the angle between Wi
and x. Notice that if we normalize the weights and zero
the biases ((cid:107)Wi(cid:107) = 1, bi =0), the posterior probabilities be-
come p1=(cid:107)x(cid:107) cos(θ1) and p2=(cid:107)x(cid:107) cos(θ2). Note that p1
and p2 share the same x, the ﬁnal result only depends on
the angles θ1 and θ2. The decision boundary also becomes
cos(θ1)−cos(θ2)=0 (i.e. angular bisector of vector W1 and
W2). Although the above analysis is built on binary-calss
case, it is trivial to generalize the analysis to multi-class case.
During training, the modiﬁed softmax loss ((cid:107)Wi(cid:107)=1, bi =0)
encourages features from the i-th class to have smaller angle
θi (larger cosine distance) than others, which makes angles
between Wi and features a reliable metric for classiﬁcation.
To give a formal expression for the modiﬁed softmax loss,
we ﬁrst deﬁne the input feature xi and its label yi. The
original softmax loss can be written as

L =

Li =

1
N

(cid:88)

i

1
N

(cid:88)

i

− log (cid:0) efyi
j efj

(cid:80)

(cid:1)

(3)

where fj denotes the j-th element (j ∈ [1, K], K is the
class number) of the class score vector f , and N is the
In CNNs, f is usually the
number of training samples.
output of a fully connected layer W , so fj = W T
j xi + bj
and fyi = W T
xi + byi where xi, Wj, Wyi are the i-th
yi
training sample, the j-th and yi-th column of W respectively.
We further reformulate Li in Eq. (3) as
Li = − log (cid:0) eW T
j eW T

j xi+bj

xi+byi

(cid:80)

yi

(cid:1)

(4)

= − log (cid:0) e(cid:107)Wyi (cid:107)(cid:107)xi(cid:107) cos(θyi,i)+byi
j e(cid:107)Wj (cid:107)(cid:107)xi(cid:107) cos(θj,i)+bj

(cid:80)

(cid:1)

in which θj,i(0 ≤ θj,i ≤ π) is the angle between vector Wj
and xi. As analyzed above, we ﬁrst normalize (cid:107)Wj(cid:107) = 1, ∀j
in each iteration and zero the biases. Then we have the
modiﬁed softmax loss:

Lmodiﬁed =

1
N

(cid:88)

i

− log (cid:0) e(cid:107)xi(cid:107) cos(θyi,i)
j e(cid:107)xi(cid:107) cos(θj,i)

(cid:80)

(cid:1)

(5)

Although we can learn features with angular boundary with
the modiﬁed softmax loss, these features are still not neces-
sarily discriminative. Since we use angles as the distance
metric, it is natural to incorporate angular margin to learned
features in order to enhance the discrimination power. To
this end, we propose a novel way to combine angular margin.

3.2. Introducing Angular Margin to Softmax Loss

where x is the learned feature vector. Wi and bi are weights
and bias of last fully connected layer corresponding to class

Instead of designing a new type of loss function and con-
structing a weighted combination with softmax loss (similar

Loss Function
Softmax Loss
Modiﬁed Softmax Loss

A-Softmax Loss

Decision Boundary
(W1 − W2)x + b1 − b2 = 0
(cid:107)x(cid:107)(cos θ1 − cos θ2) = 0
(cid:107)x(cid:107)(cos mθ1 − cos θ2) = 0 for class 1
(cid:107)x(cid:107)(cos θ1 − cos mθ2) = 0 for class 2

Table 1: Comparison of decision boundaries in binary case. Note that, θi is
the angle between Wi and x.

to contrastive loss) , we propose a more natural way to learn
angular margin. From the previous analysis of softmax loss,
we learn that decision boundaries can greatly affect the fea-
ture distribution, so our basic idea is to manipulate decision
boundaries to produce angular margin. We ﬁrst give a moti-
vating binary-class example to explain how our idea works.
Assume a learned feature x from class 1 is given and θi
is the angle between x and Wi, it is known that the modiﬁed
softmax loss requires cos(θ1) > cos(θ2) to correctly classify
x. But what if we instead require cos(mθ1) > cos(θ2) where
m ≥ 2 is a integer in order to correctly classify x? It is
essentially making the decision more stringent than previ-
ous, because we require a lower bound4 of cos(θ1) to be
larger than cos(θ2). The decision boundary for class 1 is
cos(mθ1) = cos(θ2). Similarly, if we require cos(mθ2) >
cos(θ1) to correctly classify features from class 2, the deci-
sion boundary for class 2 is cos(mθ2) = cos(θ1). Suppose
all training samples are correctly classiﬁed, such decision
boundaries will produce an angular margin of m−1
2 where
θ1
2 is the angle between W1 and W2. From angular per-
spective, correctly classifying x from identity 1 requires
θ1 < θ2
m , while correctly classifying x from identity 2 re-
quires θ2 < θ1
m . Both are more difﬁcult than original θ1 < θ2
and θ2 < θ1, respectively. By directly formulating this idea
into the modiﬁed softmax loss Eq. (5), we have

m+1 θ1

Lang =

(cid:88)

− log (cid:0)

1
N

i

e(cid:107)xi(cid:107) cos(mθyi,i)

(cid:1)

e(cid:107)xi(cid:107) cos(mθyi,i) + (cid:80)

e(cid:107)xi(cid:107) cos(θj,i)

j(cid:54)=yi

(6)
where θyi,i has to be in the range of [0, π
m ]. In order to
get rid of this restriction and make it optimizable in CNNs,
we expand the deﬁnition range of cos(θyi,i) by generaliz-
ing it to a monotonically decreasing angle function ψ(θyi,i)
which should be equal to cos(θyi,i) in [0, π
m ]. Therefore, our
proposed A-Softmax loss is formulated as:

Lang =

(cid:88)

− log (cid:0)

1
N

i

e(cid:107)xi(cid:107)ψ(θyi,i)

(cid:1) (7)

e(cid:107)xi(cid:107)ψ(θyi,i) + (cid:80)

e(cid:107)xi(cid:107) cos(θj,i)

j(cid:54)=yi

in which we deﬁne ψ(θyi,i) = (−1)k cos(mθyi,i) − 2k,
θyi,i ∈ [ kπ
m ] and k ∈ [0, m − 1]. m ≥ 1 is an inte-
ger that controls the size of angular margin. When m = 1, it
becomes the modiﬁed softmax loss.

m , (k+1)π

The justiﬁcation of A-Softmax loss can also be made from
decision boundary perspective. A-Softmax loss adopts dif-
ferent decision boundary for different class (each boundary

4The inequality cos(θ1) > cos(mθ1) holds while θ1 ∈ [0, π

m ], m ≥ 2.

Figure 3: Geometry Interpretation of Euclidean margin loss (e.g. contrastive
loss, triplet loss, center loss, etc.), modiﬁed softmax loss and A-Softmax
loss. The ﬁrst row is 2D feature constraint, and the second row is 3D feature
constraint. The orange region indicates the discriminative constraint for
class 1, while the green region is for class 2.

is more stringent than the original), thus producing angular
margin. The comparison of decision boundaries is given in
Table 1. From original softmax loss to modiﬁed softmax
loss, it is from optimizing inner product to optimizing angles.
From modiﬁed softmax loss to A-Softmax loss, it makes
the decision boundary more stringent and separated. The
angular margin increases with larger m and be zero if m = 1.
Supervised by A-Softmax loss, CNNs learn face features
with geometrically interpretable angular margin. Because A-
Softmax loss requires Wi = 1, bi = 0, it makes the prediction
only depends on angles between the sample x and Wi. So
x can be classiﬁed to the identity with smallest angle. The
parameter m is added for the purpose of learning an angular
margin between different identities.

To facilitate gradient computation and back propagation,
we replace cos(θj,i) and cos(mθyi,i) with the expressions
only containing W and xi, which is easily done by deﬁni-
tion of cosine and multi-angle formula (also the reason why
we need m to be an integer). Without θ, we can compute
derivative with respect to x and W , similar to softmax loss.

3.3. Hypersphere Interpretation of A-Softmax Loss

A-Softmax loss has stronger requirements for a correct
classiﬁcation when m ≥ 2, which generates an angular classi-
ﬁcation margin between learned features of different classes.
A-Softmax loss not only imposes discriminative power to
the learned features via angular margin, but also renders nice
and novel hypersphere interpretation. As shown in Fig. 3,
A-Softmax loss is equivalent to learning features that are
discriminative on a hypersphere manifold, while Euclidean
margin losses learn features in Euclidean space.

To simplify, We take the binary case to analyze the hyper-
sphere interpretation. Considering a sample x from class 1
and two column weights W1, W2, the classiﬁcation rule for

j v2

A-Softmax loss is cos(mθ1) > cos(θ2), equivalently mθ1 <
θ2. Notice that θ1, θ2 are equal to their corresponding arc
5 on unit hypersphere {vj, ∀j| (cid:80)
length ω1, ω2
j =1, v≥0}.
Because (cid:107)W (cid:107)1 = (cid:107)W (cid:107)2 = 1, the decision replies on the arc
length ω1 and ω2. The decision boundary is equivalent to
mω1 = ω2, and the constrained region for correctly classify-
ing x to class 1 is mω1 < ω2. Geometrically speaking, this
is a hypercircle-like region lying on a hypersphere manifold.
For example, it is a circle-like region on the unit sphere in
3D case, as illustrated in Fig. 3. Note that larger m leads to
smaller hypercircle-like region for each class, which is an ex-
plicit discriminative constraint on a manifold. For better un-
derstanding, Fig. 3 provides 2D and 3D visualizations. One
can see that A-Softmax loss imposes arc length constraint on
a unit circle in 2D case and circle-like region constraint on a
unit sphere in 3D case. Our analysis shows that optimizing
angles with A-Softmax loss essentially makes the learned
features more discriminative on a hypersphere.

3.4. Properties of A-Softmax Loss

Property 1. A-Softmax loss deﬁnes a large angular mar-
gin learning task with adjustable difﬁculty. With larger m,
the angular margin becomes larger, the constrained region
on the manifold becomes smaller, and the corresponding
learning task also becomes more difﬁcult.

We know that the larger m is, the larger angular margin
A-Softmax loss constrains. There exists a minimal m that
constrains the maximal intra-class angular distance to be
smaller than the minimal inter-class angular distance, which
can also be observed in our experiments.

Deﬁnition 1 (minimal m for desired feature distribution).
mmin is the minimal value such that while m > mmin, A-
Softmax loss deﬁnes a learning task where the maximal intra-
class angular feature distance is constrained to be smaller
than the minimal inter-class angular feature distance.

3.

√

Property 2 (lower bound of mmin in binary-class case). In
binary-class case, we have mmin ≥ 2 +
Proof. We consider the space spaned by W1 and W2. Be-
cause m ≥ 2, it is easy to obtain the maximal angle that class
1 spans is θ12
m+1 where θ12 is the angle between W1
and W2. To require the maximal intra-class feature angular
distance smaller than the minimal inter-class feature angular
distance, we need to constrain

m−1 + θ12

+

θ12
θ12
m + 1
m − 1
(cid:125)
(cid:123)(cid:122)
(cid:124)
max intra-class angle

≤

2π − θ12
m + 1

+

≤

θ12
m + 1
(cid:125)

(cid:124)

(cid:123)(cid:122)
max intra-class angle

(cid:125)

(cid:124)

(m − 1)θ12
m + 1
(cid:123)(cid:122)
min inter-class angle
(m − 1)θ12
m + 1
(cid:123)(cid:122)
min inter-class angle

(cid:124)

(cid:125)

, θ12 ≤

m − 1
m

π

, θ12 >

m − 1
m

π

(8)

(9)

5ωi is the shortest arc length (geodesic distance) between Wi and the
projected point of sample x on the unit hypersphere, while the correspond-
ing θi is the angle between Wi and x.

After solving these two inequalities, we could have mmin ≥
2 +

3, which is a lower bound for binary case.

√

Property 3 (lower bound of mmin in multi-class case). Un-
der the assumption that Wi, ∀i are uniformly spaced in the
Euclidean space, we have mmin ≥ 3.

Proof. We consider the 2D k-class (k ≥ 3) scenario for the
lower bound. Because Wi, ∀i are uniformly spaced in the
2D Euclidean space, we have θi+1
is the
angle between Wi and Wi+1. Since Wi, ∀i are symmetric,
we only need to analyze one of them. For the i-th class (Wi),
We need to constrain

k where θi+1

i = 2π

i

+

θi
θi+1
i−1
i
m + 1
m + 1
(cid:124)
(cid:125)
(cid:123)(cid:122)
max intra-class angle

≤ min

(cid:124)

(cid:26) (m − 1)θi+1
i
m + 1

,

(m − 1)θi

(cid:27)

i−1

m + 1

(10)

(cid:123)(cid:122)
min inter-class angle

(cid:125)

After solving this inequality, we obtain mmin ≥ 3, which is
a lower bound for multi-class case.

Based on this, we use m = 4 to approximate the desired
feature distribution criteria. Since the lower bounds are not
necessarily tight, giving a tighter lower bound and a upper
bound under certain conditions is also possible, which we
leave to the future work. Experiments also show that larger
m consistently works better and m = 4 will usually sufﬁce.

3.5. Discussions

Why angular margin. First and most importantly, angu-
lar margin directly links to discriminativeness on a manifold,
which intrinsically matches the prior that faces also lie on
a manifold. Second, incorporating angular margin to soft-
max loss is actually a more natural choice. As Fig. 2 shows,
features learned by the original softmax loss have an intrin-
sic angular distribution. So directly combining Euclidean
margin constraints with softmax loss is not reasonable.

Comparison with existing losses. In deep FR task, the
most popular and well-performing loss functions include
contrastive loss, triplet loss and center loss. First, they only
impose Euclidean margin to the learned features (w/o normal-
ization), while ours instead directly considers angular margin
which is naturally motivated. Second, both contrastive loss
and triplet loss suffer from data expansion when constituting
the pairs/triplets from the training set, while ours requires no
sample mining and imposes discriminative constraints to the
entire mini-batches (compared to contrastive and triplet loss
that only affect a few representative pairs/triplets).

4. Experiments (more in Appendix)

4.1. Experimental Settings

Preprocessing. We only use standard preprocessing. The
face landmarks in all images are detected by MTCNN [39].
The cropped faces are obtained by similarity transforma-
tion. Each pixel ([0, 255]) in RGB images is normalized by
subtracting 127.5 and then being divided by 128.

Layer

4-layer CNN

10-layer CNN

Conv1.x

[3×3, 64]×1, S2

[3×3, 64]×1, S2

20-layer CNN
[3×3, 64]×1, S2
(cid:34)

(cid:35)

3 × 3, 64

× 1

3 × 3, 64

36-layer CNN
[3×3, 64]×1, S2
(cid:34)

(cid:35)

3 × 3, 64

× 2

3 × 3, 64

64-layer CNN
[3×3, 64]×1, S2
(cid:34)

(cid:35)

3 × 3, 64

× 3

3 × 3, 64

Conv2.x

[3×3, 128]×1, S2

Conv3.x

[3×3, 256]×1, S2

[3×3, 128]×1, S2
(cid:34)

3 × 3, 128

(cid:35)

[3×3, 128]×1, S2
(cid:34)

3 × 3, 128

(cid:35)

[3×3, 128]×1, S2
(cid:34)

3 × 3, 128

(cid:35)

[3×3, 128]×1, S2
(cid:34)

3 × 3, 128

(cid:35)

3 × 3, 128

3 × 3, 128

3 × 3, 128

3 × 3, 128

[3×3, 256]×1, S2
(cid:34)

3 × 3, 256

(cid:35)

[3×3, 256]×1, S2
(cid:34)

3 × 3, 256

(cid:35)

[3×3, 256]×1, S2
(cid:34)

3 × 3, 256

(cid:35)

[3×3, 256]×1, S2
(cid:34)
3 × 3, 256

(cid:35)

× 8

× 1

× 2

3 × 3, 256

3 × 3, 256

3 × 3, 256

Conv4.x

[3×3, 512]×1, S2

[3×3, 512]×1, S2

FC1

512

512

[3×3, 512]×1, S2
(cid:34)

3 × 3, 512

(cid:35)

[3×3, 512]×1, S2
(cid:34)

3 × 3, 512

(cid:35)

3 × 3, 512

512

3 × 3, 512

512

× 4

× 8

× 2

× 16

3 × 3, 256
[3×3, 512]×1, S2
(cid:34)

3 × 3, 512

(cid:35)

× 3

3 × 3, 512

512

× 2

× 4

× 1

Table 2: Our CNN architectures with different convolutional layers. Conv1.x, Conv2.x and Conv3.x denote convolution units that may contain multiple
convolution layers and residual units are shown in double-column brackets. E.g., [3×3, 64]×4 denotes 4 cascaded convolution layers with 64 ﬁlters of size
3×3, and S2 denotes stride 2. FC1 is the fully connected layer.

CNNs Setup. Caffe [10] is used to implement A-Softmax
loss and CNNs. The general framework to train and extract
SphereFace features is shown in Fig. 4. We use residual
units [4] in our CNN architecture. For fairness, all compared
methods use the same CNN architecture (including residual
units) as SphereFace. CNNs with different depths (4, 10, 20,
36, 64) are used to better evaluate our method. The speciﬁc
settings for difffernt CNNs we used are given in Table 2.
According to the analysis in Section 3.4, we usually set m
as 4 in A-Softmax loss unless speciﬁed. These models are
trained with batch size of 128 on four GPUs. The learning
rate begins with 0.1 and is divided by 10 at the 16K, 24K
iterations. The training is ﬁnished at 28K iterations.

Figure 4: Training and Extracting SphereFace features.

Training Data. We use publicly available web-collected
training dataset CASIA-WebFace [37] (after excluding the
images of identities appearing in testing sets) to train our
CNN models. CASIA-WebFace has 494,414 face images
belonging to 10,575 different individuals. These face images
are horizontally ﬂipped for data augmentation. Notice that
the scale of our training data (0.49M) is relatively small, es-
pecially compared to other private datasets used in DeepFace
[30] (4M), VGGFace [20] (2M) and FaceNet [22] (200M).
Testing. We extract the deep features (SphereFace) from
the output of the FC1 layer. For all experiments, the ﬁnal
representation of a testing face is obtained by concatenating
its original face features and its horizontally ﬂipped features.
The score (metric) is computed by the cosine distance of two
features. The nearest neighbor classiﬁer and thresholding
are used for face identiﬁcation and veriﬁcation, respectively.

4.2. Exploratory Experiments

Effect of m. To show that larger m leads to larger an-
gular margin (i.e. more discriminative feature distribution
on manifold), we perform a toy example with different m.
We train A-Softmax loss with 6 individuals that have the
most samples in CASIA-WebFace. We set the output feature
dimension (FC1) as 3 and visualize the training samples in
Fig. 5. One can observe that larger m leads to more dis-
criminative distribution on the sphere and also larger angular
margin, as expected. We also use class 1 (blue) and class
2 (dark green) to construct positive and negative pairs to
evaluate the angle distribution of features from the same
class and different classes. The angle distribution of positive
and negative pairs (the second row of Fig. 5) quantitatively
shows the angular margin becomes larger while m increases
and every class also becomes more distinct with each other.
Besides visual comparison, we also perform face recogni-
tion on LFW and YTF to evaluate the effect of m. For fair
comparison, we use 64-layer CNN (Table 2) for all losses.
Results are given in Table 3. One can observe that while
m becomes larger, the accuracy of A-Softmax loss also be-
comes better, which shows that larger angular margin can
bring stronger discrimination power.

Dataset
LFW
YTF

Original
97.88
93.1

m=1
97.90
93.2

m=2
98.40
93.8

m=3
99.25
94.4

m=4
99.42
95.0

Table 3: Accuracy(%) comparison of different m (A-Softmax loss) and
original softmax loss on LFW and YTF dataset.

Effect of CNN architectures. We train A-Softmax loss
(m = 4) and original softmax loss with different number
of convolution layers. Speciﬁc CNN architectures can be
found in Table 2. From Fig. 6, one can observe that A-
Softmax loss consistently outperforms CNNs with softmax
loss (1.54%∼1.91%), indicating that A-Softmax loss is more
suitable for open-set FR. Besides, the difﬁcult learning task

Figure 5: Visualization of features learned with different m. The ﬁrst row shows the 3D features projected on the unit sphere. The projected points are the
intersection points of the feature vectors and the unit sphere. The second row shows the angle distribution of both positive pairs and negative pairs (we choose
class 1 and class 2 from the subset to construct positive and negative pairs). Orange area indicates positive pairs while blue indicates negative pairs. All angles
are represented in radian. Note that, this visualization experiment uses a 6-class subset of the CASIA-WebFace dataset.

deﬁned by A-Softmax loss makes full use of the superior
learning capability of deeper architectures. A-Softmax loss
greatly improve the veriﬁcation accuracy from 98.20% to
99.42% on LFW, and from 93.4% to 95.0% on YTF. On
the contrary, the improvement of deeper standard CNNs is
unsatisfactory and also easily get saturated (from 96.60% to
97.75% on LFW, from 91.1% to 93.1% on YTF).

Figure 6: Accuracy (%) on LFW and YTF with different number of convo-
lutional layers. Left side is for LFW, while right side is for YTF.

4.3. Experiments on LFW and YTF

LFW dataset [9] includes 13,233 face images from 5749
different identities, and YTF dataset [35] includes 3,424
videos from 1,595 different individuals. Both datasets con-
tains faces with large variations in pose, expression and
illuminations. We follow the unrestricted with labeled out-
side data protocol [8] on both datasets. The performance of
SphereFace are evaluated on 6,000 face pairs from LFW and
5,000 video pairs from YTF. The results are given in Table 4.
For contrastive loss and center loss, we follow the FR con-
vention to form a weighted combination with softmax loss.
The weights are selected via cross validation on training set.
For L-Softmax [16], we also use m = 4. All the compared

Method
DeepFace [30]
FaceNet [22]
Deep FR [20]
DeepID2+ [27]
DeepID2+ [27]
Baidu [15]
Center Face [34]
Yi et al. [37]
Ding et al. [2]
Liu et al. [16]
Softmax Loss
Softmax+Contrastive [26]
Triplet Loss [22]
L-Softmax Loss [16]
Softmax+Center Loss [34]
SphereFace

Models
3
1
1
1
25
1
1
1
1
1
1
1
1
1
1
1

Data
4M*
200M*
2.6M
300K*
300K*
1.3M*
0.7M*
WebFace
WebFace
WebFace
WebFace
WebFace
WebFace
WebFace
WebFace
WebFace

LFW YTF
91.4
97.35
99.65
95.1
97.3
98.95
N/A
98.70
93.2
99.47
N/A
99.13
94.9
99.28
92.2
97.73
N/A
98.43
N/A
98.71
93.1
97.88
93.5
98.78
93.4
98.70
94.0
99.10
94.4
99.05
95.0
99.42

Table 4: Accuracy (%) on LFW and YTF dataset. * denotes the outside data
is private (not publicly available). For fair comparison, all loss functions
(including ours) we implemented use 64-layer CNN architecture in Table 2.

loss functions share the same 64-layer CNN architecture.

Most of the existing face veriﬁcation systems achieve
high performance with huge training data or model ensem-
ble. While using single model trained on publicly available
dataset (CAISA-WebFace, relatively small and having noisy
labels), SphereFace achieves 99.42% and 95.0% accuracies
on LFW and YTF datasets. It is the current best performance
trained on WebFace and considerably better than the other
models trained on the same dataset. Compared with models
trained on high-quality private datasets, SphereFace is still
very competitive, outperforming most of the existing results
in Table 4. One should notice that our single model perfor-
mance is only worse than Google FaceNet which is trained
with more than 200 million data.

Figure 7: CMC and ROC curves of different methods under the small training set protocol.

Method
NTechLAB - facenx large
Vocord - DeepVo1
Deepsense - Large
Shanghai Tech
Google - FaceNet v8
Beijing FaceAll_Norm_1600
Beijing FaceAll_1600
Deepsense - Small
SIAT_MMLAB
Barebones FR - cnn
NTechLAB - facenx_small
3DiVi Company - tdvm6
Softmax Loss
Softmax+Contrastive Loss [26]
Triplet Loss [22]
L-Softmax Loss [16]
Softmax+Center Loss [34]
SphereFace (single model)
SphereFace (3-patch ensemble)

protocol
Large
Large
Large
Large
Large
Large
Large
Small
Small
Small
Small
Small
Small
Small
Small
Small
Small
Small
Small

Rank1 Acc.
73.300
75.127
74.799
74.049
70.496
64.804
63.977
70.983
65.233
59.363
58.218
33.705
54.855
65.219
64.797
67.128
65.494
72.729
75.766

Ver.
85.081
67.318
87.764
86.369
86.473
67.118
63.960
82.851
76.720
59.036
66.366
36.927
65.925
78.865
78.322
80.423
80.146
85.561
89.142

Table 5: Performance (%) on MegaFace challenge. “Rank-1 Acc.” indicates
rank-1 identiﬁcation accuracy with 1M distractors, and “Ver.” indicates
veriﬁcation TAR for 10−6 FAR. TAR and FAR denote True Accept Rate
and False Accept Rate respectively. For fair comparison, all loss functions
(including ours) we implemented use the same deep CNN architecture.

For fair comparison, we also implement the softmax loss,
contrastive loss, center loss, triplet loss, L-Softmax loss [16]
and train them with the same 64-layer CNN architecture as
A-Softmax loss. As can be observed in Table 4, SphereFace
consistently outperforms the features learned by all these
compared losses, showing its superiority in FR tasks.

4.4. Experiments on MegaFace Challenge

MegaFace dataset [18] is a recently released testing bench-
mark with very challenging task to evaluate the performance
of face recognition methods at the million scale of distractors.
MegaFace dataset contains a gallery set and a probe set. The
gallery set contains more than 1 million images from 690K
different individuals. The probe set consists of two existing
datasets: Facescrub [19] and FGNet. MegaFace has several
testing scenarios including identiﬁcation, veriﬁcation and
pose invariance under two protocols (large or small training
set). The training set is viewed as small if it is less than
0.5M. We evaluate SphereFace under the small training set

protocol. We adopt two testing protocols: face identiﬁcation
and veriﬁcation. The results are given in Fig. 7 and Tabel
5. Note that we use simple 3-patch feature concatenation
ensemble as the ﬁnal performance of SphereFace.

Fig. 7 and Tabel 5 show that SphereFace (3 patches en-
semble) beats the second best result by a large margins (4.8%
for rank-1 identiﬁcation rate and 6.3% for veriﬁcation rate)
on MegaFace benchmark under the small training dataset
protocol. Compared to the models trained on large dataset
(500 million for Google and 18 million for NTechLAB), our
method still performs better (0.64% for id. rate and 1.4%
for veri. rate). Moreover, in contrast to their sophisticated
network design, we only employ typical CNN architecture
supervised by A-Softamx to achieve such excellent perfor-
mance. For single model SphereFace, the accuracy of face
identiﬁcation and veriﬁcation are still 72.73% and 85.56%
respectively, which already outperforms most state-of-the-
art methods. For better evaluation, we also implement the
softmax loss, contrastive loss, center loss, triplet loss and L-
Softmax loss [16]. Compared to these loss functions trained
with the same CNN architecture and dataset, SphereFace also
shows signiﬁcant and consistent improvements. These re-
sults convincingly demonstrate that the proposed SphereFace
is well designed for open-set face recognition. One can also
see that learning features with large inter-class angular mar-
gin can signiﬁcantly improve the open-set FR performance.

5. Concluding Remarks

This paper presents a novel deep hypersphere embedding
approach for face recognition. In speciﬁc, we propose the
angular softmax loss for CNNs to learn discriminative face
features (SphereFace) with angular margin. A-Softmax loss
renders nice geometric interpretation by constraining learned
features to be discriminative on a hypersphere manifold,
which intrinsically matches the prior that faces also lie on
a non-linear manifold. This connection makes A-Softmax
very effective for learning face representation. Competitive
results on several popular face benchmarks demonstrate the
superiority and great potentials of our approach. We also
believe A-Softmax loss could also beneﬁt some other tasks
like object recognition, person re-identiﬁcation, etc.

References

[1] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity
metric discriminatively, with application to face veriﬁcation.
In CVPR, 2005. 3

[2] C. Ding and D. Tao. Robust face recognition via multimodal
deep face representation. IEEE TMM, 17(11):2049–2058,
2015. 7

[3] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduc-
tion by learning an invariant mapping. In CVPR, 2006. 2,
3

[4] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1, 6

[5] X. He, S. Yan, Y. Hu, P. Niyogi, and H.-J. Zhang. Face
recognition using laplacianfaces. TPAMI, 27(3):328–340,
2005. 2

[6] E. Hoffer and N. Ailon. Deep metric learning using triplet

network. arXiv preprint:1412.6622, 2014. 3

[7] J. Hu, J. Lu, and Y.-P. Tan. Discriminative deep metric learn-
ing for face veriﬁcation in the wild. In CVPR, 2014. 3
[8] G. B. Huang and E. Learned-Miller. Labeled faces in the
wild: Updates and new reporting procedures. Dept. Comput.
Sci., Univ. Massachusetts Amherst, Amherst, MA, USA, Tech.
Rep, pages 14–003, 2014. 1, 7

[9] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller.
Labeled faces in the wild: A database for studying face recog-
nition in unconstrained environments. Technical report, Tech-
nical Report, 2007. 7

[10] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long,
R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Con-
volutional architecture for fast feature embedding. arXiv
preprint:1408.5093, 2014. 6

[11] I. Kemelmacher-Shlizerman, S. M. Seitz, D. Miller, and
E. Brossard. The megaface benchmark: 1 million faces for
recognition at scale. In CVPR, 2016. 1

[12] M. Köstinger, M. Hirzer, P. Wohlhart, P. M. Roth, and
H. Bischof. Large scale metric learning from equivalence
constraints. In CVPR, 2012. 3

[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks. In
NIPS, 2012. 1

[14] K.-C. Lee, J. Ho, M.-H. Yang, and D. Kriegman. Video-based
face recognition using probabilistic appearance manifolds. In
CVPR, 2003. 2

[15] J. Liu, Y. Deng, and C. Huang. Targeting ultimate ac-
arXiv

curacy: Face recognition via deep embedding.
preprint:1506.07310, 2015. 3, 7

[16] W. Liu, Y. Wen, Z. Yu, and M. Yang. Large-margin softmax
loss for convolutional neural networks. In ICML, 2016. 2, 3,
7, 8, 10, 11, 12

[17] J. Lu, G. Wang, W. Deng, P. Moulin, and J. Zhou. Multi-
manifold deep metric learning for image set classiﬁcation. In
CVPR, 2015. 3

[18] D. Miller, E. Brossard, S. Seitz, and I. Kemelmacher-
Shlizerman. Megaface: A million faces for recognition at
scale. arXiv preprint:1505.02108, 2015. 8

[19] H.-W. Ng and S. Winkler. A data-driven approach to cleaning

large face datasets. In ICIP, 2014. 8

[20] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face

recognition. In BMVC, 2015. 6, 7

[21] A. Ross and A. K. Jain. Multimodal biometrics: An overview.
In Signal Processing Conference, 2004 12th European, pages
1221–1224. IEEE, 2004. 1

[22] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A
uniﬁed embedding for face recognition and clustering. In
CVPR, 2015. 1, 2, 3, 6, 7, 8

[23] K. Simonyan and A. Zisserman. Very deep convolu-
tional networks for large-scale image recognition. arXiv
preprint:1409.1556, 2014. 1

[24] H. O. Song, Y. Xiang, S. Jegelka, and S. Savarese. Deep
metric learning via lifted structured feature embedding. In
CVPR, 2016. 3

[25] Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning face
representation by joint identiﬁcation-veriﬁcation. In NIPS,
2014. 1, 2, 3

[26] Y. Sun, X. Wang, and X. Tang. Deep learning face represen-
tation from predicting 10,000 classes. In CVPR, 2014. 2, 3,
7, 8

[27] Y. Sun, X. Wang, and X. Tang. Deeply learned face repre-
sentations are sparse, selective, and robust. In CVPR, 2015.
7

[28] Y. Sun, X. Wang, and X. Tang. Sparsifying neural network
connections for face recognition. In CVPR, 2016. 2, 3
[29] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper
with convolutions. In CVPR, 2015. 1

[30] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface:
Closing the gap to human-level performance in face veriﬁca-
tion. In CVPR, 2014. 2, 3, 6, 7

[31] A. Talwalkar, S. Kumar, and H. Rowley. Large-scale manifold

learning. In CVPR, 2008. 2

[32] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin,
B. Chen, and Y. Wu. Learning ﬁne-grained image similarity
with deep ranking. In CVPR, 2014. 3

[33] K. Q. Weinberger and L. K. Saul. Distance metric learning
for large margin nearest neighbor classiﬁcation. Journal of
Machine Learning Research, 10(Feb):207–244, 2009. 3
[34] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discriminative
feature learning approach for deep face recognition. In ECCV,
2016. 1, 2, 3, 7, 8

[35] L. Wolf, T. Hassner, and I. Maoz. Face recognition in un-
constrained videos with matched background similarity. In
CVPR, 2011. 7

[36] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. Dis-
tance metric learning with application to clustering with side-
information. NIPS, 2003. 3

[37] D. Yi, Z. Lei, S. Liao, and S. Z. Li. Learning face represen-
tation from scratch. arXiv preprint:1411.7923, 2014. 2, 6,
7

[38] Y. Ying and P. Li. Distance metric learning with eigenvalue

optimization. JMLR, 13(Jan):1–26, 2012. 3

[39] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao. Joint face detec-
tion and alignment using multi-task cascaded convolutional
networks. arXiv preprint:1604.02878, 2016. 5

A. The intuition of removing the last ReLU

Appendix

Standard CNNs usually connect ReLU to the bottom of FC1, so the learned features will only distribute in the non-negative
range [0, +∞), which limits the feasible learning space (angle) for the CNNs. To address this shortcoming, both SphereFace
and [16] ﬁrst propose to remove the ReLU nonlinearity that is connected to the bottom of FC1 in SphereFace networks.
Intuitively, removing the ReLU can greatly beneﬁt the feature learning, since it provides larger feasible learning space (from
angular perspective).

Visualization on MNIST. Fig. 8 shows the 2-D visualization of feature distributions in MNIST with and without the last
ReLU. One can observe with ReLU the 2-D feature could only distribute in the ﬁrst quadrant. Without the last ReLU, the
learned feature distribution is much more reasonable.

Figure 8: 2-D visualization before and after removing the last ReLU.

B. Normalizing the weights could reduce the prior caused by the training data imbalance

We have emphasized in the main paper that normalizing the weights can give better geometric interpretation. Besides this,
we also justify why we want to normalize the weights from a different perspective. We ﬁnd that normalizing the weights can
implicitly reduce the prior brought by the training data imbalance issue (e.g., the long-tail distribution of the training data). In
other words, we argue that normalizing the weights can partially address the training data imbalance problem.

Figure 9: Norm of Wi and sample number of class i in MNIST dataset and CASIA-WebFace dataset.

We have an empirical study on the relation between the sample number of each class and the 2-norm of the weights
corresponding to the same class (the i-th column of W is associated to the i-th class). By computing the norm of Wi and
sample number of class i with respect to each class (see Fig. 9), we ﬁnd that the larger sample number a class has, the larger
the associated norm of weights tends to be. We argue that the norm of weights Wi with respect to class i is largely determined
by its sample distribution and sample number. Therefore, norm of weights Wi, ∀i can be viewed as a learned prior hidden in
training datasets. Eliminating such prior is often beneﬁcial to face veriﬁcation. This is because face veriﬁcation requires to test
on a dataset whose idenities can not appear in training datasets, so the prior from training dataset should not be transferred to
the testing. This prior may even be harmful to face veriﬁcation performance. To eliminate such prior, we normalize the norm
of weights of FC26.

6FC2 refers to the fully connected layer in the softmax loss (or A-Softmax loss).

C. Empirical experiment of zeroing out the biases

Figure 10: Biases of last fully connected layer learned in CASIA-WebFace dataset.

Standard CNNs usually preserve the bias term in the fully connected layers, but these bias terms make it difﬁcult to analyze
the proposed A-Softmax loss. This is because SphereFace aims to optimize the angle and produce the angular margin. With
bias of FC2, the angular geometry interpretation becomes much more difﬁcult to analyze. To facilitate the analysis, we zero
out the bias of FC2 following [16]. By setting the bias of FC2 to zero, the A-Softmax loss has clear geometry interpretation
and therefore becomes much easier to analyze. We show all the biases of FC2 from a CASIA-pretrained model in Fig. 10. One
can observe that the most of the biases are near zero, indicating these biases are not necessarily useful for face veriﬁcation.

Figure 11: 2-D visualization with and without bias of last fully connected layer in MNIST.

Visualization on MNIST. We visualize the 2-D feature distribution in MNIST dataset with and without bias in Fig. 11.
One can observe that zeroing out the bias has no direct inﬂuence on the feature distribution. The features learned with and
without bias can both make full use of the learning space.

D. 2D visualization of A-Softmax loss on MNIST

We visualize the 2-D feature distribution on MNIST in Fig. 12. It is obvious that with larger m the learned features become
much more discriminative due to the larger inter-class angular margin. Most importantly, the learned discriminative features
also generalize really well in the testing set.

Figure 12: 2-D MNIST visualization of features learned by the softmax loss and the A-Softmax loss (m = 2, 3, 4).

E. Angular Fisher score for evaluating the feature discriminativeness and ablation study on our

proposed modiﬁcations

We ﬁrst propose an angular Fisher score for evaluating the feature discriminativeness in angular margin feature learning.

The angular Fisher score (AFS) is deﬁned by

AF S =

Sw
Sb

(11)

where the within-class scatter value is deﬁned as Sw = (cid:80)
(1 − cos(cid:104)xj, mi(cid:105)) and the between-class scatter value is
deﬁned as Sb = (cid:80)
i ni(1 − cos(cid:104)mi, m(cid:105)). Xi is the i-th class samples, mi is the mean vector of features from class i, m is the
mean vector of the whole dataset, and ni is the sample number of class i. In general, the lower the ﬁsher value is, the more
discriminative the features are.

xj ∈Xi

(cid:80)

i

Next, we perform a comprehensive ablation study on all the proposed modiﬁcations: removing last ReLU, removing Biases,
normalizing weights and applying A-Softmax loss. The experiments are performed using the 4-layer CNN described in Table 2.
The models are trained on CASIA dataset and tested on LFW dataset. The setting is exactly the same as the LFW experiment
in the main paper. As shown in Table 6, we could observe that all our modiﬁcation leads to peformance improvement and our
A-Softmax could greatly increase the angular feature discriminativeness.

CNN
A
B
C
D
E
F
G

Remove Last ReLU
No
Yes
Yes
Yes
Yes
Yes
Yes

Remove Biases
No
No
Yes
Yes
Yes
Yes
Yes

Normalize Weights
No
No
No
Yes
Yes
Yes
Yes

A-Softmax
No
No
No
No
Yes (m=2)
Yes (m=3)
Yes (m=4)

Accuracy
95.13
96.37
96.40
96.63
97.67
97.82
98.20

Angular Fisher Score
0.3477
0.2835
0.2815
0.2462
0.2277
0.1791
0.1709

Table 6: Veriﬁcation accuracy (%) on LFW dataset.

F. Experiments on MegaFace with different convolutional layers

We also perform the experiment on MegaFace dataset with CNN of different convolutional layers. The results in Table 7
show that the A-Softmax loss could make best use of the network capacity. With more convolutional layers, the A-Softmax
loss (i.e., SphereFace) performs better. Most notably, SphereFace with only 4 convolutional layer could peform better than the
softmax loss with 64 convolutional layers, which validates the superiority of our A-Softmax loss.

Method

protocol

Softmax Loss (64 conv layers)
SphereFace (4 conv layers)
SphereFace (10 conv layers)
SphereFace (20 conv layers)
SphereFace (36 conv layers)
SphereFace (64 conv layers)

Small
Small
Small
Small
Small
Small

Rank-1 Id. Acc.
with 1M distractors
54.855
57.529
65.335
69.623
71.257
72.729

Ver. TAR
for 10−6 FAR
65.925
68.547
78.069
83.159
84.052
85.561

Table 7: Performance (%) on MegaFace challenge with different convolutional layers. TAR and FAR denote True Accept Rate and False Accept Rate
respectively. For all the SphereFace models, we use m = 4. With larger m and proper network optimization, the performance could potentially keep
increasing.

G. The annealing optimization strategy for A-Softmax loss

The optimization of the A-Softmax loss is similar to the L-Softmax loss [16]. We use an annealing optimization strategy
to train the network with A-Softmax loss. To be simple, the annealing strategy is essentially supervising the newtork from
an easy task (i.e., large λ) gradually to a difﬁcult task (i.e., small λ). Speciﬁcally, we let fyi = λ(cid:107)xi(cid:107) cos(θyi )+(cid:107)xi(cid:107)ψ(θyi )
and
start the stochastic gradient descent initially with a very large λ (it is equivalent to optimizing the original softmax). Then
we gradually reduce λ during training. Ideally λ can be gradually reduced to zero, but in practice, a small value will usually
sufﬁce. In most of our face experiments, decaying λ to 5 has already lead to impressive results. Smaller λ could potentially
yield a better performance but is also more difﬁcult to train.

1+λ

H. Details of the 3-patch ensemble strategy in MegaFace challenge

We adopt a common strategy to perform the 3-patch ensemble, as shown in Fig. 13. Although using more patches could keep
increasing the performance, but considering the tradeoff between efﬁciency and accuracy, we use 3-patch simple concatenation
ensemble (without the use of PCA). The 3 patches can be selected by cross-validation. The 3 patches we use in the paper are
exactly the same as in Fig. 13.

Figure 13: 3-Patch ensembles in SphereFace for MegaFace challenge.

SphereFace: Deep Hypersphere Embedding for Face Recognition

Weiyang Liu1 Yandong Wen2

1Georgia Institute of Technology

Zhiding Yu2 Ming Li3 Bhiksha Raj2
2Carnegie Mellon University

3Sun Yat-Sen University

Le Song1

wyliu@gatech.edu, {yandongw,yzhiding}@andrew.cmu.edu, lsong@cc.gatech.edu

8
1
0
2
 
n
a
J
 
9
2
 
 
]

V
C
.
s
c
[
 
 
4
v
3
6
0
8
0
.
4
0
7
1
:
v
i
X
r
a

Abstract

This paper addresses deep face recognition (FR) prob-
lem under open-set protocol, where ideal face features are
expected to have smaller maximal intra-class distance than
minimal inter-class distance under a suitably chosen met-
ric space. However, few existing algorithms can effectively
achieve this criterion. To this end, we propose the angular
softmax (A-Softmax) loss that enables convolutional neural
networks (CNNs) to learn angularly discriminative features.
Geometrically, A-Softmax loss can be viewed as imposing
discriminative constraints on a hypersphere manifold, which
intrinsically matches the prior that faces also lie on a mani-
fold. Moreover, the size of angular margin can be quantita-
tively adjusted by a parameter m. We further derive speciﬁc
m to approximate the ideal feature criterion. Extensive anal-
ysis and experiments on Labeled Face in the Wild (LFW),
Youtube Faces (YTF) and MegaFace Challenge show the
superiority of A-Softmax loss in FR tasks. The code has also
been made publicly available1.

1. Introduction

Recent years have witnessed the great success of convo-
lutional neural networks (CNNs) in face recognition (FR).
Owing to advanced network architectures [13, 23, 29, 4] and
discriminative learning approaches [25, 22, 34], deep CNNs
have boosted the FR performance to an unprecedent level.
Typically, face recognition can be categorized as face identi-
ﬁcation and face veriﬁcation [8, 11]. The former classiﬁes a
face to a speciﬁc identity, while the latter determines whether
a pair of faces belongs to the same identity.

In terms of testing protocol, face recognition can be eval-
uated under closed-set or open-set settings, as illustrated
in Fig. 1. For closed-set protocol, all testing identities are
predeﬁned in training set. It is natural to classify testing face
images to the given identities. In this scenario, face veriﬁca-
tion is equivalent to performing identiﬁcation for a pair of
faces respectively (see left side of Fig. 1). Therefore, closed-
set FR can be well addressed as a classiﬁcation problem,

1See the code at https://github.com/wy1iu/sphereface.

Figure 1: Comparison of open-set and closed-set face recognition.

where features are expected to be separable. For open-set
protocol, the testing identities are usually disjoint from the
training set, which makes FR more challenging yet close to
practice. Since it is impossible to classify faces to known
identities in training set, we need to map faces to a discrimi-
native feature space. In this scenario, face identiﬁcation can
be viewed as performing face veriﬁcation between the probe
face and every identity in the gallery (see right side of Fig. 1).
Open-set FR is essentially a metric learning problem, where
the key is to learn discriminative large-margin features.

Desired features for open-set FR are expected to satisfy
the criterion that the maximal intra-class distance is smaller
than the minimal inter-class distance under a certain metric
space. This criterion is necessary if we want to achieve
perfect accuracy using nearest neighbor. However, learning
features with this criterion is generally difﬁcult because of
the intrinsically large intra-class variation and high inter-
class similarity [21] that faces exhibit.

Few CNN-based approaches are able to effectively for-
mulate the aforementioned criterion in loss functions. Pi-

Figure 2: Comparison among softmax loss, modiﬁed softmax loss and A-Softmax loss. In this toy experiment, we construct a CNN to learn 2-D features on a
subset of the CASIA face dataset. In speciﬁc, we set the output dimension of FC1 layer as 2 and visualize the learned features. Yellow dots represent the
ﬁrst class face features, while purple dots represent the second class face features. One can see that features learned by the original softmax loss can not be
classiﬁed simply via angles, while modiﬁed softmax loss can. Our A-Softmax loss can further increase the angular margin of learned features.

oneering work [30, 26] learn face features via the softmax
loss2, but softmax loss only learns separable features that are
not discriminative enough. To address this, some methods
combine softmax loss with contrastive loss [25, 28] or center
loss [34] to enhance the discrimination power of features.
[22] adopts triplet loss to supervise the embedding learning,
leading to state-of-the-art face recognition results. However,
center loss only explicitly encourages intra-class compact-
ness. Both contrastive loss [3] and triplet loss [22] can not
constrain on each individual sample, and thus require care-
fully designed pair/triplet mining procedure, which is both
time-consuming and performance-sensitive.

It seems to be a widely recognized choice to impose Eu-
clidean margin to learned features, but a question arises: Is
Euclidean margin always suitable for learning discrimina-
tive face features? To answer this question, we ﬁrst look into
how Euclidean margin based losses are applied to FR.

Most recent approaches [25, 28, 34] combine Euclidean
margin based losses with softmax loss to construct a joint
supervision. However, as can be observed from Fig. 2, the
features learned by softmax loss have intrinsic angular dis-
tribution (also veriﬁed by [34]). In some sense, Euclidean
margin based losses are incompatible with softmax loss, so
it is not well motivated to combine these two type of losses.
In this paper, we propose to incorporate angular margin
instead. We start with a binary-class case to analyze the
softmax loss. The decision boundary in softmax loss is
(W1 − W2)x + b1 − b2 = 0, where Wi and bi are weights
and bias3 in softmax loss, respectively.
If we deﬁne x
as a feature vector and constrain (cid:107)W1(cid:107) = (cid:107)W2(cid:107) = 1 and
b1 = b2 = 0, the decision boundary becomes (cid:107)x(cid:107)(cos(θ1) −
cos(θ2)) = 0, where θi is the angle between Wi and x. The
new decision boundary only depends on θ1 and θ2. Modiﬁed
softmax loss is able to directly optimize angles, enabling
CNNs to learn angularly distributed features (Fig. 2).

Compared to original softmax loss, the features learned
by modiﬁed softmax loss are angularly distributed, but not
necessarily more discriminative. To the end, we generalize
the modiﬁed softmax loss to angular softmax (A-Softmax)

2Following [16], we deﬁne the softmax loss as the combination of the

last fully connected layer, softmax function and cross-entropy loss.

3If not speciﬁed, the weights and biases in the paper are corresponding

to the fully connected layer in the softmax loss.

loss. Speciﬁcally, we introduce an integer m (m ≥ 1) to
quantitatively control the decision boundary.
In binary-
class case, the decision boundaries for class 1 and class
2 become (cid:107)x(cid:107)(cos(mθ1)−cos(θ2))=0 and (cid:107)x(cid:107)(cos(θ1)−
cos(mθ2))=0, respectively. m quantitatively controls the
size of angular margin. Furthermore, A-Softmax loss can be
easily generalized to multiple classes, similar to softmax loss.
By optimizing A-Softmax loss, the decision regions become
more separated, simultaneously enlarging the inter-class mar-
gin and compressing the intra-class angular distribution.

A-Softmax loss has clear geometric interpretation. Su-
pervised by A-Softmax loss, the learned features construct a
discriminative angular distance metric that is equivalent to
geodesic distance on a hypersphere manifold. A-Softmax
loss can be interpreted as constraining learned features to
be discriminative on a hypersphere manifold, which intrin-
sically matches the prior that face images lie on a manifold
[14, 5, 31]. The close connection between A-Softmax loss
and hypersphere manifolds makes the learned features more
effective for face recognition. For this reason, we term the
learned features as SphereFace.

Moreover, A-Softmax loss can quantitatively adjust the
angular margin via a parameter m, enabling us to do quanti-
tative analysis. In the light of this, we derive lower bounds
for the parameter m to approximate the desired open-set
FR criterion that the maximal intra-class distance should be
smaller than the minimal inter-class distance.

Our major contributions can be summarized as follows:

(1) We propose A-Softmax loss for CNNs to learn dis-
criminative face features with clear and novel geometric
interpretation. The learned features discriminatively span
on a hypersphere manifold, which intrinsically matches the
prior that faces also lie on a manifold.

(2) We derive lower bounds for m such that A-Softmax
loss can approximate the learning task that minimal inter-
class distance is larger than maximal intra-class distance.

(3) We are the very ﬁrst to show the effectiveness of
angular margin in FR. Trained on publicly available CASIA
dataset [37], SphereFace achieves competitive results on
several benchmarks, including Labeled Face in the Wild
(LFW), Youtube Faces (YTF) and MegaFace Challenge 1.

2. Related Work

Metric learning. Metric learning aims to learn a sim-
ilarity (distance) function. Traditional metric learning
[36, 33, 12, 38] usually learns a matrix A for a distance met-
ric (cid:107)x1 − x2(cid:107)A = (cid:112)(x1 − x2)T A(x1 − x2) upon the given
features x1, x2. Recently, prevailing deep metric learning
[7, 17, 24, 30, 25, 22, 34] usually uses neural networks
to automatically learn discriminative features x1, x2 fol-
lowed by a simple distance metric such as Euclidean dis-
tance (cid:107)x1 − x2(cid:107)2. Most widely used loss functions for deep
metric learning are contrastive loss [1, 3] and triplet loss
[32, 22, 6], and both impose Euclidean margin to features.

Deep face recognition. Deep face recognition is ar-
guably one of the most active research area in the past few
years. [30, 26] address the open-set FR using CNNs super-
vised by softmax loss, which essentially treats open-set FR
as a multi-class classiﬁcation problem. [25] combines con-
trastive loss and softmax loss to jointly supervise the CNN
training, greatly boosting the performance. [22] uses triplet
loss to learn a uniﬁed face embedding. Training on nearly
200 million face images, they achieve current state-of-the-art
FR accuracy. Inspired by linear discriminant analysis, [34]
proposes center loss for CNNs and also obtains promising
performance. In general, current well-performing CNNs
[28, 15] for FR are mostly built on either contrastive loss or
triplet loss. One could notice that state-of-the-art FR meth-
ods usually adopt ideas (e.g. contrastive loss, triplet loss)
from metric learning, showing open-set FR could be well
addressed by discriminative metric learning.

L-Softmax loss [16] also implicitly involves the concept
of angles. As a regularization method, it shows great im-
provement on closed-set classiﬁcation problems. Differently,
A-Softmax loss is developed to learn discriminative face em-
bedding. The explicit connections to hypersphere manifold
makes our learned features particularly suitable for open-set
FR problem, as veriﬁed by our experiments. In addition,
the angular margin in A-Softmax loss is explicitly imposed
and can be quantitatively controlled (e.g. lower bounds to
approximate desired feature criterion), while [16] can only
be analyzed qualitatively.

3. Deep Hypersphere Embedding

3.1. Revisiting the Softmax Loss

We revisit the softmax loss by looking into the decision
criteria of softmax loss. In binary-class case, the posterior
probabilities obtained by softmax loss are

p1 =

p2 =

exp(W T

1 x + b1) + exp(W T

2 x + b2)

exp(W T

1 x + b1)

exp(W T

2 x + b2)

exp(W T

1 x + b1) + exp(W T

2 x + b2)

(1)

(2)

1 x + b1 and W T

i, respectively. The predicted label will be assigned to
class 1 if p1 > p2 and class 2 if p1 < p2. By comparing
p1 and p2, it is clear that W T
2 x + b2 de-
termine the classiﬁcation result. The decision boundary is
(W1 − W2)x + b1 − b2 = 0. We then rewrite W T
i x + bi as
(cid:107)W T
i (cid:107)(cid:107)x(cid:107) cos(θi) + bi where θi is the angle between Wi
and x. Notice that if we normalize the weights and zero
the biases ((cid:107)Wi(cid:107) = 1, bi =0), the posterior probabilities be-
come p1=(cid:107)x(cid:107) cos(θ1) and p2=(cid:107)x(cid:107) cos(θ2). Note that p1
and p2 share the same x, the ﬁnal result only depends on
the angles θ1 and θ2. The decision boundary also becomes
cos(θ1)−cos(θ2)=0 (i.e. angular bisector of vector W1 and
W2). Although the above analysis is built on binary-calss
case, it is trivial to generalize the analysis to multi-class case.
During training, the modiﬁed softmax loss ((cid:107)Wi(cid:107)=1, bi =0)
encourages features from the i-th class to have smaller angle
θi (larger cosine distance) than others, which makes angles
between Wi and features a reliable metric for classiﬁcation.
To give a formal expression for the modiﬁed softmax loss,
we ﬁrst deﬁne the input feature xi and its label yi. The
original softmax loss can be written as

L =

Li =

1
N

(cid:88)

i

1
N

(cid:88)

i

− log (cid:0) efyi
j efj

(cid:80)

(cid:1)

(3)

where fj denotes the j-th element (j ∈ [1, K], K is the
class number) of the class score vector f , and N is the
In CNNs, f is usually the
number of training samples.
output of a fully connected layer W , so fj = W T
j xi + bj
and fyi = W T
xi + byi where xi, Wj, Wyi are the i-th
yi
training sample, the j-th and yi-th column of W respectively.
We further reformulate Li in Eq. (3) as
Li = − log (cid:0) eW T
j eW T

j xi+bj

xi+byi

(cid:80)

yi

(cid:1)

(4)

= − log (cid:0) e(cid:107)Wyi (cid:107)(cid:107)xi(cid:107) cos(θyi,i)+byi
j e(cid:107)Wj (cid:107)(cid:107)xi(cid:107) cos(θj,i)+bj

(cid:80)

(cid:1)

in which θj,i(0 ≤ θj,i ≤ π) is the angle between vector Wj
and xi. As analyzed above, we ﬁrst normalize (cid:107)Wj(cid:107) = 1, ∀j
in each iteration and zero the biases. Then we have the
modiﬁed softmax loss:

Lmodiﬁed =

1
N

(cid:88)

i

− log (cid:0) e(cid:107)xi(cid:107) cos(θyi,i)
j e(cid:107)xi(cid:107) cos(θj,i)

(cid:80)

(cid:1)

(5)

Although we can learn features with angular boundary with
the modiﬁed softmax loss, these features are still not neces-
sarily discriminative. Since we use angles as the distance
metric, it is natural to incorporate angular margin to learned
features in order to enhance the discrimination power. To
this end, we propose a novel way to combine angular margin.

3.2. Introducing Angular Margin to Softmax Loss

where x is the learned feature vector. Wi and bi are weights
and bias of last fully connected layer corresponding to class

Instead of designing a new type of loss function and con-
structing a weighted combination with softmax loss (similar

Loss Function
Softmax Loss
Modiﬁed Softmax Loss

A-Softmax Loss

Decision Boundary
(W1 − W2)x + b1 − b2 = 0
(cid:107)x(cid:107)(cos θ1 − cos θ2) = 0
(cid:107)x(cid:107)(cos mθ1 − cos θ2) = 0 for class 1
(cid:107)x(cid:107)(cos θ1 − cos mθ2) = 0 for class 2

Table 1: Comparison of decision boundaries in binary case. Note that, θi is
the angle between Wi and x.

to contrastive loss) , we propose a more natural way to learn
angular margin. From the previous analysis of softmax loss,
we learn that decision boundaries can greatly affect the fea-
ture distribution, so our basic idea is to manipulate decision
boundaries to produce angular margin. We ﬁrst give a moti-
vating binary-class example to explain how our idea works.
Assume a learned feature x from class 1 is given and θi
is the angle between x and Wi, it is known that the modiﬁed
softmax loss requires cos(θ1) > cos(θ2) to correctly classify
x. But what if we instead require cos(mθ1) > cos(θ2) where
m ≥ 2 is a integer in order to correctly classify x? It is
essentially making the decision more stringent than previ-
ous, because we require a lower bound4 of cos(θ1) to be
larger than cos(θ2). The decision boundary for class 1 is
cos(mθ1) = cos(θ2). Similarly, if we require cos(mθ2) >
cos(θ1) to correctly classify features from class 2, the deci-
sion boundary for class 2 is cos(mθ2) = cos(θ1). Suppose
all training samples are correctly classiﬁed, such decision
boundaries will produce an angular margin of m−1
2 where
θ1
2 is the angle between W1 and W2. From angular per-
spective, correctly classifying x from identity 1 requires
θ1 < θ2
m , while correctly classifying x from identity 2 re-
quires θ2 < θ1
m . Both are more difﬁcult than original θ1 < θ2
and θ2 < θ1, respectively. By directly formulating this idea
into the modiﬁed softmax loss Eq. (5), we have

m+1 θ1

Lang =

(cid:88)

− log (cid:0)

1
N

i

e(cid:107)xi(cid:107) cos(mθyi,i)

(cid:1)

e(cid:107)xi(cid:107) cos(mθyi,i) + (cid:80)

e(cid:107)xi(cid:107) cos(θj,i)

j(cid:54)=yi

(6)
where θyi,i has to be in the range of [0, π
m ]. In order to
get rid of this restriction and make it optimizable in CNNs,
we expand the deﬁnition range of cos(θyi,i) by generaliz-
ing it to a monotonically decreasing angle function ψ(θyi,i)
which should be equal to cos(θyi,i) in [0, π
m ]. Therefore, our
proposed A-Softmax loss is formulated as:

Lang =

(cid:88)

− log (cid:0)

1
N

i

e(cid:107)xi(cid:107)ψ(θyi,i)

(cid:1) (7)

e(cid:107)xi(cid:107)ψ(θyi,i) + (cid:80)

e(cid:107)xi(cid:107) cos(θj,i)

j(cid:54)=yi

in which we deﬁne ψ(θyi,i) = (−1)k cos(mθyi,i) − 2k,
θyi,i ∈ [ kπ
m ] and k ∈ [0, m − 1]. m ≥ 1 is an inte-
ger that controls the size of angular margin. When m = 1, it
becomes the modiﬁed softmax loss.

m , (k+1)π

The justiﬁcation of A-Softmax loss can also be made from
decision boundary perspective. A-Softmax loss adopts dif-
ferent decision boundary for different class (each boundary

4The inequality cos(θ1) > cos(mθ1) holds while θ1 ∈ [0, π

m ], m ≥ 2.

Figure 3: Geometry Interpretation of Euclidean margin loss (e.g. contrastive
loss, triplet loss, center loss, etc.), modiﬁed softmax loss and A-Softmax
loss. The ﬁrst row is 2D feature constraint, and the second row is 3D feature
constraint. The orange region indicates the discriminative constraint for
class 1, while the green region is for class 2.

is more stringent than the original), thus producing angular
margin. The comparison of decision boundaries is given in
Table 1. From original softmax loss to modiﬁed softmax
loss, it is from optimizing inner product to optimizing angles.
From modiﬁed softmax loss to A-Softmax loss, it makes
the decision boundary more stringent and separated. The
angular margin increases with larger m and be zero if m = 1.
Supervised by A-Softmax loss, CNNs learn face features
with geometrically interpretable angular margin. Because A-
Softmax loss requires Wi = 1, bi = 0, it makes the prediction
only depends on angles between the sample x and Wi. So
x can be classiﬁed to the identity with smallest angle. The
parameter m is added for the purpose of learning an angular
margin between different identities.

To facilitate gradient computation and back propagation,
we replace cos(θj,i) and cos(mθyi,i) with the expressions
only containing W and xi, which is easily done by deﬁni-
tion of cosine and multi-angle formula (also the reason why
we need m to be an integer). Without θ, we can compute
derivative with respect to x and W , similar to softmax loss.

3.3. Hypersphere Interpretation of A-Softmax Loss

A-Softmax loss has stronger requirements for a correct
classiﬁcation when m ≥ 2, which generates an angular classi-
ﬁcation margin between learned features of different classes.
A-Softmax loss not only imposes discriminative power to
the learned features via angular margin, but also renders nice
and novel hypersphere interpretation. As shown in Fig. 3,
A-Softmax loss is equivalent to learning features that are
discriminative on a hypersphere manifold, while Euclidean
margin losses learn features in Euclidean space.

To simplify, We take the binary case to analyze the hyper-
sphere interpretation. Considering a sample x from class 1
and two column weights W1, W2, the classiﬁcation rule for

j v2

A-Softmax loss is cos(mθ1) > cos(θ2), equivalently mθ1 <
θ2. Notice that θ1, θ2 are equal to their corresponding arc
5 on unit hypersphere {vj, ∀j| (cid:80)
length ω1, ω2
j =1, v≥0}.
Because (cid:107)W (cid:107)1 = (cid:107)W (cid:107)2 = 1, the decision replies on the arc
length ω1 and ω2. The decision boundary is equivalent to
mω1 = ω2, and the constrained region for correctly classify-
ing x to class 1 is mω1 < ω2. Geometrically speaking, this
is a hypercircle-like region lying on a hypersphere manifold.
For example, it is a circle-like region on the unit sphere in
3D case, as illustrated in Fig. 3. Note that larger m leads to
smaller hypercircle-like region for each class, which is an ex-
plicit discriminative constraint on a manifold. For better un-
derstanding, Fig. 3 provides 2D and 3D visualizations. One
can see that A-Softmax loss imposes arc length constraint on
a unit circle in 2D case and circle-like region constraint on a
unit sphere in 3D case. Our analysis shows that optimizing
angles with A-Softmax loss essentially makes the learned
features more discriminative on a hypersphere.

3.4. Properties of A-Softmax Loss

Property 1. A-Softmax loss deﬁnes a large angular mar-
gin learning task with adjustable difﬁculty. With larger m,
the angular margin becomes larger, the constrained region
on the manifold becomes smaller, and the corresponding
learning task also becomes more difﬁcult.

We know that the larger m is, the larger angular margin
A-Softmax loss constrains. There exists a minimal m that
constrains the maximal intra-class angular distance to be
smaller than the minimal inter-class angular distance, which
can also be observed in our experiments.

Deﬁnition 1 (minimal m for desired feature distribution).
mmin is the minimal value such that while m > mmin, A-
Softmax loss deﬁnes a learning task where the maximal intra-
class angular feature distance is constrained to be smaller
than the minimal inter-class angular feature distance.

3.

√

Property 2 (lower bound of mmin in binary-class case). In
binary-class case, we have mmin ≥ 2 +
Proof. We consider the space spaned by W1 and W2. Be-
cause m ≥ 2, it is easy to obtain the maximal angle that class
1 spans is θ12
m+1 where θ12 is the angle between W1
and W2. To require the maximal intra-class feature angular
distance smaller than the minimal inter-class feature angular
distance, we need to constrain

m−1 + θ12

+

θ12
θ12
m + 1
m − 1
(cid:125)
(cid:123)(cid:122)
(cid:124)
max intra-class angle

≤

2π − θ12
m + 1

+

≤

θ12
m + 1
(cid:125)

(cid:124)

(cid:123)(cid:122)
max intra-class angle

(cid:125)

(cid:124)

(m − 1)θ12
m + 1
(cid:123)(cid:122)
min inter-class angle
(m − 1)θ12
m + 1
(cid:123)(cid:122)
min inter-class angle

(cid:125)

(cid:124)

, θ12 ≤

m − 1
m

π

, θ12 >

m − 1
m

π

(8)

(9)

5ωi is the shortest arc length (geodesic distance) between Wi and the
projected point of sample x on the unit hypersphere, while the correspond-
ing θi is the angle between Wi and x.

After solving these two inequalities, we could have mmin ≥
2 +

3, which is a lower bound for binary case.

√

Property 3 (lower bound of mmin in multi-class case). Un-
der the assumption that Wi, ∀i are uniformly spaced in the
Euclidean space, we have mmin ≥ 3.

Proof. We consider the 2D k-class (k ≥ 3) scenario for the
lower bound. Because Wi, ∀i are uniformly spaced in the
2D Euclidean space, we have θi+1
is the
angle between Wi and Wi+1. Since Wi, ∀i are symmetric,
we only need to analyze one of them. For the i-th class (Wi),
We need to constrain

k where θi+1

i = 2π

i

+

θi
θi+1
i−1
i
m + 1
m + 1
(cid:124)
(cid:125)
(cid:123)(cid:122)
max intra-class angle

≤ min

(cid:124)

(cid:26) (m − 1)θi+1
i
m + 1

,

(m − 1)θi

(cid:27)

i−1

m + 1

(10)

(cid:123)(cid:122)
min inter-class angle

(cid:125)

After solving this inequality, we obtain mmin ≥ 3, which is
a lower bound for multi-class case.

Based on this, we use m = 4 to approximate the desired
feature distribution criteria. Since the lower bounds are not
necessarily tight, giving a tighter lower bound and a upper
bound under certain conditions is also possible, which we
leave to the future work. Experiments also show that larger
m consistently works better and m = 4 will usually sufﬁce.

3.5. Discussions

Why angular margin. First and most importantly, angu-
lar margin directly links to discriminativeness on a manifold,
which intrinsically matches the prior that faces also lie on
a manifold. Second, incorporating angular margin to soft-
max loss is actually a more natural choice. As Fig. 2 shows,
features learned by the original softmax loss have an intrin-
sic angular distribution. So directly combining Euclidean
margin constraints with softmax loss is not reasonable.

Comparison with existing losses. In deep FR task, the
most popular and well-performing loss functions include
contrastive loss, triplet loss and center loss. First, they only
impose Euclidean margin to the learned features (w/o normal-
ization), while ours instead directly considers angular margin
which is naturally motivated. Second, both contrastive loss
and triplet loss suffer from data expansion when constituting
the pairs/triplets from the training set, while ours requires no
sample mining and imposes discriminative constraints to the
entire mini-batches (compared to contrastive and triplet loss
that only affect a few representative pairs/triplets).

4. Experiments (more in Appendix)

4.1. Experimental Settings

Preprocessing. We only use standard preprocessing. The
face landmarks in all images are detected by MTCNN [39].
The cropped faces are obtained by similarity transforma-
tion. Each pixel ([0, 255]) in RGB images is normalized by
subtracting 127.5 and then being divided by 128.

Layer

4-layer CNN

10-layer CNN

Conv1.x

[3×3, 64]×1, S2

[3×3, 64]×1, S2

20-layer CNN
[3×3, 64]×1, S2
(cid:34)

(cid:35)

3 × 3, 64

× 1

3 × 3, 64

36-layer CNN
[3×3, 64]×1, S2
(cid:34)

(cid:35)

3 × 3, 64

× 2

3 × 3, 64

64-layer CNN
[3×3, 64]×1, S2
(cid:34)

(cid:35)

3 × 3, 64

× 3

3 × 3, 64

Conv2.x

[3×3, 128]×1, S2

Conv3.x

[3×3, 256]×1, S2

[3×3, 128]×1, S2
(cid:34)

3 × 3, 128

(cid:35)

[3×3, 128]×1, S2
(cid:34)

3 × 3, 128

(cid:35)

[3×3, 128]×1, S2
(cid:34)

3 × 3, 128

(cid:35)

[3×3, 128]×1, S2
(cid:34)

3 × 3, 128

(cid:35)

3 × 3, 128

3 × 3, 128

3 × 3, 128

3 × 3, 128

[3×3, 256]×1, S2
(cid:34)

3 × 3, 256

(cid:35)

[3×3, 256]×1, S2
(cid:34)

3 × 3, 256

(cid:35)

[3×3, 256]×1, S2
(cid:34)

3 × 3, 256

(cid:35)

[3×3, 256]×1, S2
(cid:34)
3 × 3, 256

(cid:35)

× 8

× 1

× 2

3 × 3, 256

3 × 3, 256

3 × 3, 256

Conv4.x

[3×3, 512]×1, S2

[3×3, 512]×1, S2

FC1

512

512

[3×3, 512]×1, S2
(cid:34)

3 × 3, 512

(cid:35)

[3×3, 512]×1, S2
(cid:34)

3 × 3, 512

(cid:35)

3 × 3, 512

512

3 × 3, 512

512

× 4

× 8

× 2

× 16

3 × 3, 256
[3×3, 512]×1, S2
(cid:34)

3 × 3, 512

(cid:35)

× 3

3 × 3, 512

512

× 2

× 4

× 1

Table 2: Our CNN architectures with different convolutional layers. Conv1.x, Conv2.x and Conv3.x denote convolution units that may contain multiple
convolution layers and residual units are shown in double-column brackets. E.g., [3×3, 64]×4 denotes 4 cascaded convolution layers with 64 ﬁlters of size
3×3, and S2 denotes stride 2. FC1 is the fully connected layer.

CNNs Setup. Caffe [10] is used to implement A-Softmax
loss and CNNs. The general framework to train and extract
SphereFace features is shown in Fig. 4. We use residual
units [4] in our CNN architecture. For fairness, all compared
methods use the same CNN architecture (including residual
units) as SphereFace. CNNs with different depths (4, 10, 20,
36, 64) are used to better evaluate our method. The speciﬁc
settings for difffernt CNNs we used are given in Table 2.
According to the analysis in Section 3.4, we usually set m
as 4 in A-Softmax loss unless speciﬁed. These models are
trained with batch size of 128 on four GPUs. The learning
rate begins with 0.1 and is divided by 10 at the 16K, 24K
iterations. The training is ﬁnished at 28K iterations.

Figure 4: Training and Extracting SphereFace features.

Training Data. We use publicly available web-collected
training dataset CASIA-WebFace [37] (after excluding the
images of identities appearing in testing sets) to train our
CNN models. CASIA-WebFace has 494,414 face images
belonging to 10,575 different individuals. These face images
are horizontally ﬂipped for data augmentation. Notice that
the scale of our training data (0.49M) is relatively small, es-
pecially compared to other private datasets used in DeepFace
[30] (4M), VGGFace [20] (2M) and FaceNet [22] (200M).
Testing. We extract the deep features (SphereFace) from
the output of the FC1 layer. For all experiments, the ﬁnal
representation of a testing face is obtained by concatenating
its original face features and its horizontally ﬂipped features.
The score (metric) is computed by the cosine distance of two
features. The nearest neighbor classiﬁer and thresholding
are used for face identiﬁcation and veriﬁcation, respectively.

4.2. Exploratory Experiments

Effect of m. To show that larger m leads to larger an-
gular margin (i.e. more discriminative feature distribution
on manifold), we perform a toy example with different m.
We train A-Softmax loss with 6 individuals that have the
most samples in CASIA-WebFace. We set the output feature
dimension (FC1) as 3 and visualize the training samples in
Fig. 5. One can observe that larger m leads to more dis-
criminative distribution on the sphere and also larger angular
margin, as expected. We also use class 1 (blue) and class
2 (dark green) to construct positive and negative pairs to
evaluate the angle distribution of features from the same
class and different classes. The angle distribution of positive
and negative pairs (the second row of Fig. 5) quantitatively
shows the angular margin becomes larger while m increases
and every class also becomes more distinct with each other.
Besides visual comparison, we also perform face recogni-
tion on LFW and YTF to evaluate the effect of m. For fair
comparison, we use 64-layer CNN (Table 2) for all losses.
Results are given in Table 3. One can observe that while
m becomes larger, the accuracy of A-Softmax loss also be-
comes better, which shows that larger angular margin can
bring stronger discrimination power.

Dataset
LFW
YTF

Original
97.88
93.1

m=1
97.90
93.2

m=2
98.40
93.8

m=3
99.25
94.4

m=4
99.42
95.0

Table 3: Accuracy(%) comparison of different m (A-Softmax loss) and
original softmax loss on LFW and YTF dataset.

Effect of CNN architectures. We train A-Softmax loss
(m = 4) and original softmax loss with different number
of convolution layers. Speciﬁc CNN architectures can be
found in Table 2. From Fig. 6, one can observe that A-
Softmax loss consistently outperforms CNNs with softmax
loss (1.54%∼1.91%), indicating that A-Softmax loss is more
suitable for open-set FR. Besides, the difﬁcult learning task

Figure 5: Visualization of features learned with different m. The ﬁrst row shows the 3D features projected on the unit sphere. The projected points are the
intersection points of the feature vectors and the unit sphere. The second row shows the angle distribution of both positive pairs and negative pairs (we choose
class 1 and class 2 from the subset to construct positive and negative pairs). Orange area indicates positive pairs while blue indicates negative pairs. All angles
are represented in radian. Note that, this visualization experiment uses a 6-class subset of the CASIA-WebFace dataset.

deﬁned by A-Softmax loss makes full use of the superior
learning capability of deeper architectures. A-Softmax loss
greatly improve the veriﬁcation accuracy from 98.20% to
99.42% on LFW, and from 93.4% to 95.0% on YTF. On
the contrary, the improvement of deeper standard CNNs is
unsatisfactory and also easily get saturated (from 96.60% to
97.75% on LFW, from 91.1% to 93.1% on YTF).

Figure 6: Accuracy (%) on LFW and YTF with different number of convo-
lutional layers. Left side is for LFW, while right side is for YTF.

4.3. Experiments on LFW and YTF

LFW dataset [9] includes 13,233 face images from 5749
different identities, and YTF dataset [35] includes 3,424
videos from 1,595 different individuals. Both datasets con-
tains faces with large variations in pose, expression and
illuminations. We follow the unrestricted with labeled out-
side data protocol [8] on both datasets. The performance of
SphereFace are evaluated on 6,000 face pairs from LFW and
5,000 video pairs from YTF. The results are given in Table 4.
For contrastive loss and center loss, we follow the FR con-
vention to form a weighted combination with softmax loss.
The weights are selected via cross validation on training set.
For L-Softmax [16], we also use m = 4. All the compared

Method
DeepFace [30]
FaceNet [22]
Deep FR [20]
DeepID2+ [27]
DeepID2+ [27]
Baidu [15]
Center Face [34]
Yi et al. [37]
Ding et al. [2]
Liu et al. [16]
Softmax Loss
Softmax+Contrastive [26]
Triplet Loss [22]
L-Softmax Loss [16]
Softmax+Center Loss [34]
SphereFace

Models
3
1
1
1
25
1
1
1
1
1
1
1
1
1
1
1

Data
4M*
200M*
2.6M
300K*
300K*
1.3M*
0.7M*
WebFace
WebFace
WebFace
WebFace
WebFace
WebFace
WebFace
WebFace
WebFace

LFW YTF
91.4
97.35
99.65
95.1
97.3
98.95
N/A
98.70
93.2
99.47
N/A
99.13
94.9
99.28
92.2
97.73
N/A
98.43
N/A
98.71
93.1
97.88
93.5
98.78
93.4
98.70
94.0
99.10
94.4
99.05
95.0
99.42

Table 4: Accuracy (%) on LFW and YTF dataset. * denotes the outside data
is private (not publicly available). For fair comparison, all loss functions
(including ours) we implemented use 64-layer CNN architecture in Table 2.

loss functions share the same 64-layer CNN architecture.

Most of the existing face veriﬁcation systems achieve
high performance with huge training data or model ensem-
ble. While using single model trained on publicly available
dataset (CAISA-WebFace, relatively small and having noisy
labels), SphereFace achieves 99.42% and 95.0% accuracies
on LFW and YTF datasets. It is the current best performance
trained on WebFace and considerably better than the other
models trained on the same dataset. Compared with models
trained on high-quality private datasets, SphereFace is still
very competitive, outperforming most of the existing results
in Table 4. One should notice that our single model perfor-
mance is only worse than Google FaceNet which is trained
with more than 200 million data.

Figure 7: CMC and ROC curves of different methods under the small training set protocol.

Method
NTechLAB - facenx large
Vocord - DeepVo1
Deepsense - Large
Shanghai Tech
Google - FaceNet v8
Beijing FaceAll_Norm_1600
Beijing FaceAll_1600
Deepsense - Small
SIAT_MMLAB
Barebones FR - cnn
NTechLAB - facenx_small
3DiVi Company - tdvm6
Softmax Loss
Softmax+Contrastive Loss [26]
Triplet Loss [22]
L-Softmax Loss [16]
Softmax+Center Loss [34]
SphereFace (single model)
SphereFace (3-patch ensemble)

protocol
Large
Large
Large
Large
Large
Large
Large
Small
Small
Small
Small
Small
Small
Small
Small
Small
Small
Small
Small

Rank1 Acc.
73.300
75.127
74.799
74.049
70.496
64.804
63.977
70.983
65.233
59.363
58.218
33.705
54.855
65.219
64.797
67.128
65.494
72.729
75.766

Ver.
85.081
67.318
87.764
86.369
86.473
67.118
63.960
82.851
76.720
59.036
66.366
36.927
65.925
78.865
78.322
80.423
80.146
85.561
89.142

Table 5: Performance (%) on MegaFace challenge. “Rank-1 Acc.” indicates
rank-1 identiﬁcation accuracy with 1M distractors, and “Ver.” indicates
veriﬁcation TAR for 10−6 FAR. TAR and FAR denote True Accept Rate
and False Accept Rate respectively. For fair comparison, all loss functions
(including ours) we implemented use the same deep CNN architecture.

For fair comparison, we also implement the softmax loss,
contrastive loss, center loss, triplet loss, L-Softmax loss [16]
and train them with the same 64-layer CNN architecture as
A-Softmax loss. As can be observed in Table 4, SphereFace
consistently outperforms the features learned by all these
compared losses, showing its superiority in FR tasks.

4.4. Experiments on MegaFace Challenge

MegaFace dataset [18] is a recently released testing bench-
mark with very challenging task to evaluate the performance
of face recognition methods at the million scale of distractors.
MegaFace dataset contains a gallery set and a probe set. The
gallery set contains more than 1 million images from 690K
different individuals. The probe set consists of two existing
datasets: Facescrub [19] and FGNet. MegaFace has several
testing scenarios including identiﬁcation, veriﬁcation and
pose invariance under two protocols (large or small training
set). The training set is viewed as small if it is less than
0.5M. We evaluate SphereFace under the small training set

protocol. We adopt two testing protocols: face identiﬁcation
and veriﬁcation. The results are given in Fig. 7 and Tabel
5. Note that we use simple 3-patch feature concatenation
ensemble as the ﬁnal performance of SphereFace.

Fig. 7 and Tabel 5 show that SphereFace (3 patches en-
semble) beats the second best result by a large margins (4.8%
for rank-1 identiﬁcation rate and 6.3% for veriﬁcation rate)
on MegaFace benchmark under the small training dataset
protocol. Compared to the models trained on large dataset
(500 million for Google and 18 million for NTechLAB), our
method still performs better (0.64% for id. rate and 1.4%
for veri. rate). Moreover, in contrast to their sophisticated
network design, we only employ typical CNN architecture
supervised by A-Softamx to achieve such excellent perfor-
mance. For single model SphereFace, the accuracy of face
identiﬁcation and veriﬁcation are still 72.73% and 85.56%
respectively, which already outperforms most state-of-the-
art methods. For better evaluation, we also implement the
softmax loss, contrastive loss, center loss, triplet loss and L-
Softmax loss [16]. Compared to these loss functions trained
with the same CNN architecture and dataset, SphereFace also
shows signiﬁcant and consistent improvements. These re-
sults convincingly demonstrate that the proposed SphereFace
is well designed for open-set face recognition. One can also
see that learning features with large inter-class angular mar-
gin can signiﬁcantly improve the open-set FR performance.

5. Concluding Remarks

This paper presents a novel deep hypersphere embedding
approach for face recognition. In speciﬁc, we propose the
angular softmax loss for CNNs to learn discriminative face
features (SphereFace) with angular margin. A-Softmax loss
renders nice geometric interpretation by constraining learned
features to be discriminative on a hypersphere manifold,
which intrinsically matches the prior that faces also lie on
a non-linear manifold. This connection makes A-Softmax
very effective for learning face representation. Competitive
results on several popular face benchmarks demonstrate the
superiority and great potentials of our approach. We also
believe A-Softmax loss could also beneﬁt some other tasks
like object recognition, person re-identiﬁcation, etc.

References

[1] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity
metric discriminatively, with application to face veriﬁcation.
In CVPR, 2005. 3

[2] C. Ding and D. Tao. Robust face recognition via multimodal
deep face representation. IEEE TMM, 17(11):2049–2058,
2015. 7

[3] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduc-
tion by learning an invariant mapping. In CVPR, 2006. 2,
3

[4] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In CVPR, 2016. 1, 6

[5] X. He, S. Yan, Y. Hu, P. Niyogi, and H.-J. Zhang. Face
recognition using laplacianfaces. TPAMI, 27(3):328–340,
2005. 2

[6] E. Hoffer and N. Ailon. Deep metric learning using triplet

network. arXiv preprint:1412.6622, 2014. 3

[7] J. Hu, J. Lu, and Y.-P. Tan. Discriminative deep metric learn-
ing for face veriﬁcation in the wild. In CVPR, 2014. 3
[8] G. B. Huang and E. Learned-Miller. Labeled faces in the
wild: Updates and new reporting procedures. Dept. Comput.
Sci., Univ. Massachusetts Amherst, Amherst, MA, USA, Tech.
Rep, pages 14–003, 2014. 1, 7

[9] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller.
Labeled faces in the wild: A database for studying face recog-
nition in unconstrained environments. Technical report, Tech-
nical Report, 2007. 7

[10] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long,
R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Con-
volutional architecture for fast feature embedding. arXiv
preprint:1408.5093, 2014. 6

[11] I. Kemelmacher-Shlizerman, S. M. Seitz, D. Miller, and
E. Brossard. The megaface benchmark: 1 million faces for
recognition at scale. In CVPR, 2016. 1

[12] M. Köstinger, M. Hirzer, P. Wohlhart, P. M. Roth, and
H. Bischof. Large scale metric learning from equivalence
constraints. In CVPR, 2012. 3

[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

Imagenet
classiﬁcation with deep convolutional neural networks. In
NIPS, 2012. 1

[14] K.-C. Lee, J. Ho, M.-H. Yang, and D. Kriegman. Video-based
face recognition using probabilistic appearance manifolds. In
CVPR, 2003. 2

[15] J. Liu, Y. Deng, and C. Huang. Targeting ultimate ac-
arXiv

curacy: Face recognition via deep embedding.
preprint:1506.07310, 2015. 3, 7

[16] W. Liu, Y. Wen, Z. Yu, and M. Yang. Large-margin softmax
loss for convolutional neural networks. In ICML, 2016. 2, 3,
7, 8, 10, 11, 12

[17] J. Lu, G. Wang, W. Deng, P. Moulin, and J. Zhou. Multi-
manifold deep metric learning for image set classiﬁcation. In
CVPR, 2015. 3

[18] D. Miller, E. Brossard, S. Seitz, and I. Kemelmacher-
Shlizerman. Megaface: A million faces for recognition at
scale. arXiv preprint:1505.02108, 2015. 8

[19] H.-W. Ng and S. Winkler. A data-driven approach to cleaning

large face datasets. In ICIP, 2014. 8

[20] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face

recognition. In BMVC, 2015. 6, 7

[21] A. Ross and A. K. Jain. Multimodal biometrics: An overview.
In Signal Processing Conference, 2004 12th European, pages
1221–1224. IEEE, 2004. 1

[22] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A
uniﬁed embedding for face recognition and clustering. In
CVPR, 2015. 1, 2, 3, 6, 7, 8

[23] K. Simonyan and A. Zisserman. Very deep convolu-
tional networks for large-scale image recognition. arXiv
preprint:1409.1556, 2014. 1

[24] H. O. Song, Y. Xiang, S. Jegelka, and S. Savarese. Deep
metric learning via lifted structured feature embedding. In
CVPR, 2016. 3

[25] Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning face
representation by joint identiﬁcation-veriﬁcation. In NIPS,
2014. 1, 2, 3

[26] Y. Sun, X. Wang, and X. Tang. Deep learning face represen-
tation from predicting 10,000 classes. In CVPR, 2014. 2, 3,
7, 8

[27] Y. Sun, X. Wang, and X. Tang. Deeply learned face repre-
sentations are sparse, selective, and robust. In CVPR, 2015.
7

[28] Y. Sun, X. Wang, and X. Tang. Sparsifying neural network
connections for face recognition. In CVPR, 2016. 2, 3
[29] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper
with convolutions. In CVPR, 2015. 1

[30] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface:
Closing the gap to human-level performance in face veriﬁca-
tion. In CVPR, 2014. 2, 3, 6, 7

[31] A. Talwalkar, S. Kumar, and H. Rowley. Large-scale manifold

learning. In CVPR, 2008. 2

[32] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin,
B. Chen, and Y. Wu. Learning ﬁne-grained image similarity
with deep ranking. In CVPR, 2014. 3

[33] K. Q. Weinberger and L. K. Saul. Distance metric learning
for large margin nearest neighbor classiﬁcation. Journal of
Machine Learning Research, 10(Feb):207–244, 2009. 3
[34] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discriminative
feature learning approach for deep face recognition. In ECCV,
2016. 1, 2, 3, 7, 8

[35] L. Wolf, T. Hassner, and I. Maoz. Face recognition in un-
constrained videos with matched background similarity. In
CVPR, 2011. 7

[36] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. Dis-
tance metric learning with application to clustering with side-
information. NIPS, 2003. 3

[37] D. Yi, Z. Lei, S. Liao, and S. Z. Li. Learning face represen-
tation from scratch. arXiv preprint:1411.7923, 2014. 2, 6,
7

[38] Y. Ying and P. Li. Distance metric learning with eigenvalue

optimization. JMLR, 13(Jan):1–26, 2012. 3

[39] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao. Joint face detec-
tion and alignment using multi-task cascaded convolutional
networks. arXiv preprint:1604.02878, 2016. 5

A. The intuition of removing the last ReLU

Appendix

Standard CNNs usually connect ReLU to the bottom of FC1, so the learned features will only distribute in the non-negative
range [0, +∞), which limits the feasible learning space (angle) for the CNNs. To address this shortcoming, both SphereFace
and [16] ﬁrst propose to remove the ReLU nonlinearity that is connected to the bottom of FC1 in SphereFace networks.
Intuitively, removing the ReLU can greatly beneﬁt the feature learning, since it provides larger feasible learning space (from
angular perspective).

Visualization on MNIST. Fig. 8 shows the 2-D visualization of feature distributions in MNIST with and without the last
ReLU. One can observe with ReLU the 2-D feature could only distribute in the ﬁrst quadrant. Without the last ReLU, the
learned feature distribution is much more reasonable.

Figure 8: 2-D visualization before and after removing the last ReLU.

B. Normalizing the weights could reduce the prior caused by the training data imbalance

We have emphasized in the main paper that normalizing the weights can give better geometric interpretation. Besides this,
we also justify why we want to normalize the weights from a different perspective. We ﬁnd that normalizing the weights can
implicitly reduce the prior brought by the training data imbalance issue (e.g., the long-tail distribution of the training data). In
other words, we argue that normalizing the weights can partially address the training data imbalance problem.

Figure 9: Norm of Wi and sample number of class i in MNIST dataset and CASIA-WebFace dataset.

We have an empirical study on the relation between the sample number of each class and the 2-norm of the weights
corresponding to the same class (the i-th column of W is associated to the i-th class). By computing the norm of Wi and
sample number of class i with respect to each class (see Fig. 9), we ﬁnd that the larger sample number a class has, the larger
the associated norm of weights tends to be. We argue that the norm of weights Wi with respect to class i is largely determined
by its sample distribution and sample number. Therefore, norm of weights Wi, ∀i can be viewed as a learned prior hidden in
training datasets. Eliminating such prior is often beneﬁcial to face veriﬁcation. This is because face veriﬁcation requires to test
on a dataset whose idenities can not appear in training datasets, so the prior from training dataset should not be transferred to
the testing. This prior may even be harmful to face veriﬁcation performance. To eliminate such prior, we normalize the norm
of weights of FC26.

6FC2 refers to the fully connected layer in the softmax loss (or A-Softmax loss).

C. Empirical experiment of zeroing out the biases

Figure 10: Biases of last fully connected layer learned in CASIA-WebFace dataset.

Standard CNNs usually preserve the bias term in the fully connected layers, but these bias terms make it difﬁcult to analyze
the proposed A-Softmax loss. This is because SphereFace aims to optimize the angle and produce the angular margin. With
bias of FC2, the angular geometry interpretation becomes much more difﬁcult to analyze. To facilitate the analysis, we zero
out the bias of FC2 following [16]. By setting the bias of FC2 to zero, the A-Softmax loss has clear geometry interpretation
and therefore becomes much easier to analyze. We show all the biases of FC2 from a CASIA-pretrained model in Fig. 10. One
can observe that the most of the biases are near zero, indicating these biases are not necessarily useful for face veriﬁcation.

Figure 11: 2-D visualization with and without bias of last fully connected layer in MNIST.

Visualization on MNIST. We visualize the 2-D feature distribution in MNIST dataset with and without bias in Fig. 11.
One can observe that zeroing out the bias has no direct inﬂuence on the feature distribution. The features learned with and
without bias can both make full use of the learning space.

D. 2D visualization of A-Softmax loss on MNIST

We visualize the 2-D feature distribution on MNIST in Fig. 12. It is obvious that with larger m the learned features become
much more discriminative due to the larger inter-class angular margin. Most importantly, the learned discriminative features
also generalize really well in the testing set.

Figure 12: 2-D MNIST visualization of features learned by the softmax loss and the A-Softmax loss (m = 2, 3, 4).

E. Angular Fisher score for evaluating the feature discriminativeness and ablation study on our

proposed modiﬁcations

We ﬁrst propose an angular Fisher score for evaluating the feature discriminativeness in angular margin feature learning.

The angular Fisher score (AFS) is deﬁned by

AF S =

Sw
Sb

(11)

where the within-class scatter value is deﬁned as Sw = (cid:80)
(1 − cos(cid:104)xj, mi(cid:105)) and the between-class scatter value is
deﬁned as Sb = (cid:80)
i ni(1 − cos(cid:104)mi, m(cid:105)). Xi is the i-th class samples, mi is the mean vector of features from class i, m is the
mean vector of the whole dataset, and ni is the sample number of class i. In general, the lower the ﬁsher value is, the more
discriminative the features are.

xj ∈Xi

(cid:80)

i

Next, we perform a comprehensive ablation study on all the proposed modiﬁcations: removing last ReLU, removing Biases,
normalizing weights and applying A-Softmax loss. The experiments are performed using the 4-layer CNN described in Table 2.
The models are trained on CASIA dataset and tested on LFW dataset. The setting is exactly the same as the LFW experiment
in the main paper. As shown in Table 6, we could observe that all our modiﬁcation leads to peformance improvement and our
A-Softmax could greatly increase the angular feature discriminativeness.

CNN
A
B
C
D
E
F
G

Remove Last ReLU
No
Yes
Yes
Yes
Yes
Yes
Yes

Remove Biases
No
No
Yes
Yes
Yes
Yes
Yes

Normalize Weights
No
No
No
Yes
Yes
Yes
Yes

A-Softmax
No
No
No
No
Yes (m=2)
Yes (m=3)
Yes (m=4)

Accuracy
95.13
96.37
96.40
96.63
97.67
97.82
98.20

Angular Fisher Score
0.3477
0.2835
0.2815
0.2462
0.2277
0.1791
0.1709

Table 6: Veriﬁcation accuracy (%) on LFW dataset.

F. Experiments on MegaFace with different convolutional layers

We also perform the experiment on MegaFace dataset with CNN of different convolutional layers. The results in Table 7
show that the A-Softmax loss could make best use of the network capacity. With more convolutional layers, the A-Softmax
loss (i.e., SphereFace) performs better. Most notably, SphereFace with only 4 convolutional layer could peform better than the
softmax loss with 64 convolutional layers, which validates the superiority of our A-Softmax loss.

Method

protocol

Softmax Loss (64 conv layers)
SphereFace (4 conv layers)
SphereFace (10 conv layers)
SphereFace (20 conv layers)
SphereFace (36 conv layers)
SphereFace (64 conv layers)

Small
Small
Small
Small
Small
Small

Rank-1 Id. Acc.
with 1M distractors
54.855
57.529
65.335
69.623
71.257
72.729

Ver. TAR
for 10−6 FAR
65.925
68.547
78.069
83.159
84.052
85.561

Table 7: Performance (%) on MegaFace challenge with different convolutional layers. TAR and FAR denote True Accept Rate and False Accept Rate
respectively. For all the SphereFace models, we use m = 4. With larger m and proper network optimization, the performance could potentially keep
increasing.

G. The annealing optimization strategy for A-Softmax loss

The optimization of the A-Softmax loss is similar to the L-Softmax loss [16]. We use an annealing optimization strategy
to train the network with A-Softmax loss. To be simple, the annealing strategy is essentially supervising the newtork from
an easy task (i.e., large λ) gradually to a difﬁcult task (i.e., small λ). Speciﬁcally, we let fyi = λ(cid:107)xi(cid:107) cos(θyi )+(cid:107)xi(cid:107)ψ(θyi )
and
start the stochastic gradient descent initially with a very large λ (it is equivalent to optimizing the original softmax). Then
we gradually reduce λ during training. Ideally λ can be gradually reduced to zero, but in practice, a small value will usually
sufﬁce. In most of our face experiments, decaying λ to 5 has already lead to impressive results. Smaller λ could potentially
yield a better performance but is also more difﬁcult to train.

1+λ

H. Details of the 3-patch ensemble strategy in MegaFace challenge

We adopt a common strategy to perform the 3-patch ensemble, as shown in Fig. 13. Although using more patches could keep
increasing the performance, but considering the tradeoff between efﬁciency and accuracy, we use 3-patch simple concatenation
ensemble (without the use of PCA). The 3 patches can be selected by cross-validation. The 3 patches we use in the paper are
exactly the same as in Fig. 13.

Figure 13: 3-Patch ensembles in SphereFace for MegaFace challenge.

