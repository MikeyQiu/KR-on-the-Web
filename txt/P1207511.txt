9
1
0
2
 
v
o
N
 
4
 
 
]
L
C
.
s
c
[
 
 
3
v
9
3
1
6
0
.
5
0
9
1
:
v
i
X
r
a

Aligning Visual Regions and Textual Concepts for
Semantic-Grounded Image Representations

Fenglin Liu1∗, Yuanxin Liu3,4∗, Xuancheng Ren2∗, Xiaodong He5, Xu Sun2
1ADSPLAB, School of ECE, Peking University, Shenzhen, China
2MOE Key Laboratory of Computational Linguistics, School of EECS, Peking University
3Institute of Information Engineering, Chinese Academy of Sciences
4School of Cyber Security, University of Chinese Academy of Sciences
5JD AI Research
{fenglinliu98, renxc, xusun}@pku.edu.cn, liuyuanxin@iie.ac.cn
xiaodong.he@jd.com

Abstract

In vision-and-language grounding problems, ﬁne-grained representations of the
image are considered to be of paramount importance. Most of the current systems
incorporate visual features and textual concepts as a sketch of an image. However,
plainly inferred representations are usually undesirable in that they are composed
of separate components, the relations of which are elusive. In this work, we aim at
representing an image with a set of integrated visual regions and corresponding
textual concepts, reﬂecting certain semantics. To this end, we build the Mutual
Iterative Attention (MIA) module, which integrates correlated visual features and
textual concepts, respectively, by aligning the two modalities. We evaluate the
proposed approach on two representative vision-and-language grounding tasks,
i.e., image captioning and visual question answering. In both tasks, the semantic-
grounded image representations consistently boost the performance of the baseline
models under all metrics across the board. The results demonstrate that our ap-
proach is effective and generalizes well to a wide range of models for image-related
applications.2

1

Introduction

Recently, there is a surge of research interest in multidisciplinary tasks such as image captioning
[7] and visual question answering (VQA) [3], trying to explain the interaction between vision and
language. In image captioning, an intelligence system takes an image as input and generates a
description in the form of natural language. VQA is a more challenging problem that takes an extra
question into account and requires the model to give an answer depending on both the image and
the question. Despite their different application scenarios, a shared goal is to understand the image,
which necessitates the acquisition of grounded image representations.

In the literature, an image is typically represented in two fundamental forms: visual features and
textual concepts (see Figure 1). Visual Features [32, 2, 20] represent an image in the vision domain
and contain abundant visual information. For CNN-based visual features, an image is split into
equally-sized visual regions without encoding global relationships such as position and adjacency. To
obtain better image representations with respect to concrete objects, RCNN-based visual features that
are deﬁned by bounding boxes of interests are proposed. Nevertheless, the visual features are based

∗Equal contribution.
2The code is available at https://github.com/fenglinliu98/MIA

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

Figure 1: Illustrations of commonly-used image representations (from left to right): CNN-based grid
visual features, RCNN-based region visual features, textual concepts, and scene-graphs.

on regions and are not associated with the actual words, which means the semantic inconsistency
between the two domains has to be resolved by the downstream systems themselves. Textual
Concepts [8, 37, 33] represent an image in the language domain and introduce semantic information.
They consist of unordered visual words, irrespective of afﬁliation and positional relations, making it
difﬁcult for the system to infer the underlying semantic and spatial relationships. Moreover, due to
the lack of visual reference, some concepts may induce semantic ambiguity, e.g., the word mouse can
either refer to a mammal or an electronic device. Scene-Graphs [36] are the combination of the two
kinds of representations. They use region-based visual features to represent the objects and textual
concepts to represent the relationships. However, to construct a scene-graph, a complicated pipeline
is required and error propagation cannot be avoided.

For image representations used for text-oriented purposes, it is often desirable to integrate the two
forms of image information. Existing downstream systems achieve that by using both kinds of image
representations in the decoding process, mostly ignoring the innate alignment between the modalities.
As the semantics of the visual features and the textual concepts are usually inconsistent, the systems
have to devote themselves to learn such alignment. Besides, these representations only contain local
features, lacking global structural information. Those problems make it hard for the systems to
understand the image efﬁciently.

In this paper, we work toward constructing integrated image representations from vision and language
in the encoding process. The objective is achieved by the proposed Mutual Iterative Attention (MIA)
module, which aligns the visual features and textual concepts with their relevant counterparts in
each domain. The motivation comes from the fact that correlated features in one domain can be
linked up by a feature in another domain, which has connections with all of them. In implementation,
we perform mutual attention iteratively between the two domains to realize the procedure without
annotated alignment data. The visual receptive ﬁelds gradually concentrate on salient visual regions,
and the original word-level concepts are gradually merged to recapitulate corresponding visual
regions. In addition, the aligned visual features and textual concepts provide a more clear deﬁnition
of the image aspects they represent.

The contributions of this paper are as follows:

• For vision-and-language grounding problems, we introduce integrated image representations
based on the alignment between visual regions and textual concepts to describe the salient
combination of local features in a certain modality.

• We propose a novel attention-based strategy, namely the Mutual Iterative Attention (MIA),
which uses the features from the other domain as the guide for integrating the features in the
current domain without mixing in the heterogeneous information.

• According to the extensive experiments on the MSCOCO image captioning dataset and VQA
v2.0 dataset, when equipped with the MIA, improvements on the baselines are witnessed in all
metrics. This demonstrates that the semantic-grounded image representations are effective and
can generalize to a wide range of models.

2 Approach

The proposed approach acts on plainly extracted image features from vision and language, e.g.,
convolutional feature maps, regions of interest (RoI), and visual words (textual concepts), and reﬁnes

2

those features so that they can describe visual semantics, i.e., meaningful compositions of such
features, which are then used in the downstream tasks to replace the original features. Figure 2 gives
an overview and an example of our approach.

2.1 Visual Features and Textual Concepts

Visual features and textual concepts are widely used
[37, 35, 17, 23] as the information sources for image-
grounded text generation. In common practice, visual fea-
tures are extracted by ResNet [10], GoogLeNet [29] and
VGG [27], and are rich in low-level visual information [33].
Recently, more and more work adopted regions of interest
(RoI) proposed by RCNN-like models as visual features,
and each RoI is supposed to contain a speciﬁc object in
the image. Textual concepts are introduced to compensate
the lack of high-level semantic information in visual fea-
tures [8, 33, 37]. Speciﬁcally, they consist of visual words
that can be objects (e.g., dog, bike), attributes (e.g., young,
black) and relations (e.g., sitting, holding). The embedding
vectors of these visual words are then taken as the textual
concepts. It is worth noticing that to obtain visual features
and textual concepts, only the image itself is needed as
input, and no external text information about the image is
required, meaning that they can be used for any vision-and-
language grounding problems. In the following, we denote
the visual features and textual concepts for an image as I
and T , respectively.

2.2 Learning Alignment

To form the alignment between the visual regions and the
textual words, we adopt the attention mechanism from
Vaswani et al. [30], which is designed initially to obtain
contextual representations for sentences in machine trans-
lation and has proven to be effective in capturing alignment
of different languages and structure of sentences.

2.2.1 Mutual Attention

Figure 2: Overview of our approach.
We take as input visual features and tex-
tual concepts (the lower) and repeat a
mutual attention mechanism (the mid-
dle) to combine the local features from
each domain, resulting in integrated im-
age representations reﬂecting certain se-
mantics of the image (the upper).

Mutual Attention contains two sub-layers. The ﬁrst sub-layer makes use of multi-head attention to
learn the correlated features in a certain domain by querying the other domain. The second sub-layer
uses feed-forward layer to add sufﬁcient expressive power.

The multi-head attention is composed of k parallel heads. Each head is formulated as a scaled
dot-product attention:

Atti(Q, S) = softmax

SW V
i ,

i = 1, . . . , k

(1)

(cid:32)

QW Q
i (SW K
√
dk

i )T

(cid:33)

i , W K
i

where Q ∈ Rm×dh and S ∈ Rn×dh stand for m querying features and n source features, respectively;
W Q
i ∈ Rdh×dk are learnable parameters of linear transformations; dh is the size of the
input features and dk = dh/k is the size of the output features for each attention head. Results from
each head are concatenated and passed through a linear transformation to construct the output:

, W V

MultiHeadAtt(Q, S) = [Att1(Q, S), . . . , Attk(Q, S)]W O

(2)

where W O ∈ Rdh×dh is the parameter to be learned. The multi-head attention integrates n source
features into m output features in the order of querying features. To simplify computation, we keep
m the same as n.

3

Following the multi-head attention is a fully-connected network, deﬁned as:

FCN(X) = max

(cid:16)

0, XW (1) + b(1)(cid:17)

W (2) + b(2)

(3)

where W (1) and W (2) are matrices for linear transformation; b(1) and b(2) are the bias terms. Each
sub-layer is followed by an operation sequence of dropout [28], shortcut connection3 [10], and layer
normalization [4].

Finally, the mutual attention is conducted as:

I (cid:48) = FCN(MultiHeadAtt(T, I)), T (cid:48) = FCN(MultiHeadAtt(I (cid:48), T ))

(4)

i.e., visual features are ﬁrst integrated according to textual concepts, and then textual concepts are
integrated according to integrated visual features. It is worth noticing that it is also possible to reverse
the order by ﬁrst constructing correlated textual concepts. However, in our preliminary experiments,
we found that the presented order performs better. The related results and explanations are given in
the supplementary materials for reference.

The knowledge from either domain can serve as the guide for combining local features and extracting
structural relationships of the other domain. For example, as shown by the upper left instance of
the four instances in Figure 2, the textual concept woman integrates the regions that include the
woman, which then draw in textual concepts sitting, girl, shirt, young. In addition, the mutual
attention aligns the two kinds of features, because for the same position in the two feature matrices,
the integrated visual feature and the integrated textual concept are co-referential and represent the
same high-level visual semantics. This approach also ensures that the reﬁned visual features only
contain homogeneous information because the information from the other domain only serves as the
attentive weight and is not part of the ﬁnal values.

2.2.2 Mutual Iterative Attention

To reﬁne both the visual features and the textual concepts, we propose to perform mutual attention
iteratively. The process in Eq. (4) that uses the original features is considered as the ﬁrst round:

I1 = FCN(MultiHeadAtt(T0, I0)), T1 = FCN(MultiHeadAtt(I1, T0))

(5)

where I0, T0, I1 and T1 represent the original visual features, the original textual concepts, the macro
visual features, and the macro textual concepts, respectively. By repeating the same process for N
times, we obtain the ﬁnal outputs of the two stacks:

IN = FCN(MultiHeadAtt(TN −1, IN −1)), TN = FCN(MultiHeadAtt(IN , TN −1))

(6)

It is important to note that in each iteration, the parameters of the mutual attention are shared. However,
as in each iteration more information is integrated into each feature, it is possible that iterating too
many times would cause the over-smoothing problem that all features represent essentially the same
and the overall semantics of the image. To avoid such problem, we apply the aforementioned post-
processing operations to the output of each layer, but with the shortcut connection from the input of
each layer (not the sub-layer). The shortcut serves as a semantic anchor that prevents the peripheral
information from extending the pivotal visual or textual features too much and keeps the position of
each semantic-grounded feature stable in the feature matrices.

For the downstream tasks consuming both visual features and textual concepts of images, IN and
TN can be directly used to replace the original features, respectively, because the number and the
size of the features are kept through the procedure. However, since the visual features and the textual
concepts are already aligned, we can directly add them up to get the output that makes the best
of their respective advantages, even for the tasks that originally only consumes one kind of image
representations:

MIA(I, T ) = LayerNorm(IN + TN )
As a result, the reﬁned features overcome the aforementioned weaknesses of existing image represen-
tations, providing a better start point for downstream tasks. For tasks using both kinds features, each
kind feature can be replaced with MIA-reﬁned features.

(7)

3We build the shortcut connection by adding the source features to the sub-layer outputs, instead of the

querying features in Vaswani et al. [30], to ensure no heterogeneous information is injected.

4

Figure 3: Illustration of how to equip the baseline models with our MIA. MIA aligns and integrates
the original image representations from two modalities. Left: For image captioning, the semantic-
grounded image representations are used to replace both kinds of original image features. Right: For
VQA, MIA only substitutes the image representations, and the question representations are preserved.

As annotated alignment data is not easy to obtain and the alignment learning lacks direct supervision,
we adopt the distantly-supervised learning and reﬁne the integrated image representations with
downstream tasks. As shown by previous work [30], when trained on machine translation, the
attention can learn correlation of words quite well. As the proposed method focuses on building
semantic-grounded image representations, it can be easily incorporated in the downstream models
to substitute the original image representations, which in turn provides supervision for the mutual
iterative attention. Speciﬁcally, we experiment with the task of image captioning and VQA. To use
the proposed approach, MIA is added to the downstream models as a preprocessing component.
Figure 3 illustrates how to equip the baseline systems with MIA, through two examples for image
captioning and VQA, respectively. As we can see, MIA substitutes the original image representations
with semantic-grounded image representations. For VQA, the question representations are preserved.
Besides, MIA does not affect the original experimental settings and training strategies.

3 Experiment

We evaluate the proposed approach on two multi-modal tasks, i.e., image captioning and visual
question answering (VQA). We ﬁrst conduct experiments on representative systems that use different
kinds of image representations to demonstrate the effectiveness of the proposed semantic-grounded
image representations, and then provide analysis of the key components of the MIA module.

Before introducing the results and the analysis, we ﬁrst describe some common settings. The
proposed MIA relies on both visual features and textual concepts to produce semantic-grounded
image representations. Considering the diverse forms of the original image representations, unless
otherwise speciﬁed, they are obtained as follows: (1) the grid visual features are from a ResNet-152
pretrained on ImageNet, (2) the region-based visual features are from a variant of Faster R-CNN [25],
which is provided by Anderson et al. [2] and pre-trained on Visual Genome [13], and (3) the textual
concepts are extracted by a concept extractor in Fang et al. [8] trained on the MSCOCO captioning
dataset using Multiple Instance Learning [38]. The number of textual concepts is kept the same as
the visual features, i.e., 49 for grid visual features and 36 for region visual features, by keeping only
the top concepts. The settings of MIA are the same for the two tasks, which reﬂects the generality
of our method. Particularly, we use 8 heads (k = 8) and iterate twice (N = 2), according to the
performance on the validation set. For detailed settings, please refer to the supplementary material.

3.1

Image Captioning

Dataset and Evaluation Metrics. We conduct experiments on the MSCOCO image captioning
dataset [7] and use SPICE [1], CIDEr [31], BLEU [24], METEOR [5] and ROUGE [15] as evaluation
metrics, which are calculated by MSCOCO captioning evaluation toolkit [7]. Please note that
following common practice [19, 2, 18], we adopt the dataset split from Karpathy and Li [11] and the
results are not comparable to those from the online MSCOCO evaluation server.

Baselines. Given an image, the image captioning task aims to generate a descriptive sentence
accordingly. To evaluate how the proposed semantic-grounded image representation helps the
downstream tasks, we ﬁrst design ﬁve representative baseline models that take as input different
image representations based on previous work. They are (1) Visual Attention, which uses grid visual

5

Table 1: Results of the representative systems on the image captioning task.

BLEU-1

BLEU-2

BLEU-3

BLEU-4 METEOR

ROUGE

CIDEr

SPICE

Methods

Visual Attention
w/ MIA

Concept Attention
w/ MIA

Visual Condition
w/ MIA

Concept Condition
w/ MIA

Visual Regional Attention
w/ MIA

72.6
74.5

72.6
73.8

73.3
73.9

72.9
73.9

75.2
75.6

56.0
58.4

55.9
57.4

56.9
57.3

56.2
57.3

58.9
59.4

42.2
44.4

42.5
43.8

43.4
43.9

42.8
43.9

45.2
45.7

31.7
33.6

32.5
33.6

33.0
33.7

32.7
33.7

34.7
35.4

26.5
26.8

26.5
27.1

26.8
26.9

26.4
26.9

27.6
28.0

54.6
55.8

54.4
55.3

54.8
55.1

54.4
55.1

56.0
56.4

103.0
106.7

103.2
107.9

105.2
107.2

104.4
107.2

111.2
114.1

19.3
20.1

19.4
20.3

19.5
19.8

19.3
19.8

20.6
21.1

Table 2: Evaluation of systems that use reinforcement
learning on the MSCOCO image captioning dataset.

Table 3: The overall accuracy on the
VQA v2.0 test dataset.

Methods

BLEU-4 METEOR

ROUGE

CIDEr

SPICE

Methods

Test-dev

Test-std

Up-Down
w/ MIA

Transformer
w/ MIA

36.5
37.0

39.0
39.5

28.0
28.2

28.4
29.0

57.0
57.4

58.6
58.7

120.9
122.2

126.3
129.6

21.5
21.7

21.7
22.7

Up-Down
w/ MIA

BAN
w/ MIA

67.3
68.8

69.6
70.2

67.5
69.1

69.8
70.3

features as the attention source for each decoding step, (2) Concept Attention, which uses textual
concepts as the attention source, (3) Visual Condition, which takes textual concepts as extra input at
the ﬁrst decoding step but grid visual features in the following decoding steps, (4) Concept Condition,
which, in contrast to Visual Condition, takes grid visual features at the ﬁrst decoding step but textual
concepts in the following decoding steps, and (5) Visual Regional Attention, which uses region-based
visual features as the attention source. For those models, the traditional cross-entropy based training
objective is used. We also check on the effect of MIA on more advanced captioning models, including
(6) Up-Down [2], which uses region-based visual features, and (7) Transformer, which adapts the
Transformer-Base model in Vaswani et al. [30] by taking the region-based visual features as input.
Those advanced models adopt CIDEr-based training objective using reinforcement training [26].

Results. In Table 1, we can see that the models enjoy an increase of 2%∼5% in terms of both
SPICE and CIDEr, with the proposed MIA. Especially, “Visual Attention w/ MIA” and “Concept
Attention w/ MIA” are able to pay attention to integrated representation collections instead of the
separate grid visual features or textual concepts. Besides, the baselines also enjoy the beneﬁt from
the semantic-grounded image representations, which can be veriﬁed by the improvement of “Visual
Regional Attention w/ MIA”. The results demonstrate the effectiveness and universality of MIA. As
shown in Table 2, the proposed method can still bring improvements to the strong baselines under
the reinforcement learning settings. Besides, it also suggests that our approach is compatible with
both the RNN based (Up-Down) and self-attention based (Transformer) language generators. We
also investigate the effect of incorporating MIA with the scene-graph based model [34], the results
are provided in the supplementary material, where we can also see consistent improvements. In all,
the baselines are promoted in all metrics across the board, which indicates that the reﬁned image
representations are less prone to the variations of model structures (e.g., with or without attention,
and the architecture of downstream language generator), hyper-parameters (e.g., learning rate and
batch size), original image representations (e.g., CNN, RCNN-based visual features, textual concepts
and scene-graphs), and learning paradigm (e.g., cross-entropy and CIDEr based objective).

3.2 Visual Question Answering

Dataset and Evaluation Metrics.

We experiment on the VQA v2.0 dataset [9], which is comprised of image-based question-answer
pairs labeled by human annotators. The questions are categorized into three types, namely Yes/No,

6

Table 4: Ablation analysis of the proposed approach. As we can see, incorporating MIA-reﬁned
image representation from a single modality can also lead to overall improvements.

Methods

BLEU-1

BLEU-2

BLEU-3

BLEU-4 METEOR

ROUGE

CIDEr

SPICE

Visual Attention
w/ IN
w/ MIA

Concept Attention
w/ TN
w/ MIA

72.6
74.7
74.5

72.6
73.7
73.8

56.0
58.5
58.4

55.9
57.0
57.4

42.2
44.6
44.4

42.5
43.4
43.8

31.7
33.7
33.6

32.5
33.1
33.6

26.5
26.5
26.8

26.5
26.8
27.1

54.6
55.2
55.8

54.4
55.0
55.3

103.0
105.7
106.7

103.2
106.5
107.9

19.3
19.6
20.1

19.4
20.0
20.3

Figure 4: Model performance variation under different metrics with the increase of iteration times.
VA and CA stand for Visual Attention and Concept Attention, respectively.

Number and other categories. We report the model performance based on overall accuracy on both
the test-dev and test-std sets, which is calculated by the standard VQA metric [3].

Baselines. Given an image and a question about the image, the visual question answering task aims
to generate the correct answer, which is modeled as a classiﬁcation task. We choose Up-Down [2]
and BAN [12] for comparison. They both use region-based visual features as image representations
and GRU-encoded hidden states as question representations, and make classiﬁcation based on their
combination. However, Up-Down only uses the ﬁnal sentence vector to obtain the weight of each
visual region, while BAN uses a bilinear attention to obtain the weight for each pair of visual region
and question word. BAN is the previous state-of-the-art on the VQA v2.0 dataset.

Results. As shown in Table 3, an overall improvement is achieved when applying MIA to the
baselines, which validates that our method generalizes well to different tasks. Especially, on the
answer type Number, the MIA promotes the accuracy of Up-Down from 47.5% to 51.2% and BAN
from 50.9% to 53.1%. The signiﬁcant improvements suggest that the reﬁned image representations
are more accurate in counting thanks to integrating semantically related objects.

3.3 Analysis

In this section, we analyze the effect of the proposed approach and provide insights of the MIA
module, in an attempt to answer the following questions: (1) Is the mutual attention necessary for
integrating semantically-related features? (2) Is the improvement spurious because MIA uses two
kinds input features while some of the baseline models only use one? (3) How does the iteration time
affect the alignment process? and (4) Does the mutual attention actually align the two modality?

Effect of mutual attention. Mutual attention serves as a way to integrate correlated features by
aligning modalities, which is our main proposal. Another way to integrate features is to only rely
on information from one domain, which can be achieved by replacing mutual attention with self-
attention. However, this method is found to be less effective than MIA, scoring 96.6 and 105.4 for
Visual Attention and Concept Attention, respectively, in terms of CIDEr. Especially, the performance
of the Visual Attention has even been impaired, which suggests that only using information from
one domain is insufﬁcient to construct meaningful region or concept groups that are beneﬁcial to
describing images and conﬁrms our main motivation. Besides, as the self-attention and the mutual
attention shares the same multi-head attention structure, it also indicates that the improvement comes
from the alignment of the two modalities rather than the application of the attention structure.

Ablation Study. As the deployment of MIA inevitably introduces information from the other
modality, we conduct ablation studies to investigate whether the improvement is derived from the
well-aligned and integrated image representations or the additional source information. As shown in

7

Figure 5: Visualization of the integrated image representations. Please view in color. We show
the representations with different iteration N for two images. We choose three visual features and
corresponding textual concepts with clear semantic implication and highlight them with distinct
colors. As we see, with N increasing, the alignment becomes more focused and more speciﬁc, but the
combination of related features are less represented.

Table 4, when using the same single-modal features as the corresponding baselines, our method can
still promote the performance. Thanks to the mutual iterative attention process, “Visual Attention
w/ IN ” and “Concept Attention w/ IN ” can pay attention to integrated visual features and textual
concepts, respectively. This frees the decoder from associating the unrelated original features in each
domain, which may explain for the improvements. The performance in terms of SPICE and CIDEr is
further elevated when TN and IN are combined. The progressively increased scores demonstrate that
the improvements indeed come from the reﬁned semantic-grounded image representations produced
by MIA, rather than the introduction of additional information.

The SPICE sub-category results show that IN helps the baselines to generate captions that are more
detailed in count and size, TN results in more comprehensiveness in objects, and MIA can help the
baselines to achieve a caption that is detailed in all sub-categories. Due to limited space, the scores
are provided in the supplementary materials. For output samples and intuitive comparisons, please
refer to the supplementary materials.

Effect of iteration times. We select two representative models, i.e., Visual Attention and Concept
Attention, to analyze the effect of iteration times. Figure 4 presents the performance of Visual
Attention (VA) and Concept Attention (CA) under different evaluation metrics when equipped with
the MIA. We evaluate with iteration times ranging from 1 to 5. The scores ﬁrst rise and then decline
with the increase of N , as a holistic trend. With one accord, the performances consistently reach the
best at the second iteration, for the reason of which we set N = 2. It suggests that a single iteration
does not sufﬁce to align visual features and textual concepts. With each round of mutual attention,
the image representations become increasingly focused, which explains the promotion in the ﬁrst few
iterations. As for the falling back phenomenon, we speculate that the integration effect of MIA can
also unexpectedly eliminate some useful information by assigning them low attention weights. The
absent of these key elements results in less comprehensive captions. The visualization in Figure 5
also attests to our arguments.

Visualization. We visualize the integration of the image representations in Figure 5. The colors in
the images and the heatmaps reﬂect the accumulated attention weights assigned to the original image
representations until the current iteration. As we can see in the left plots of Figure 5, the attended
visual regions are general in the ﬁrst iteration, thereby assigning comparable weights to a number
of visual words with low relevance. Taking the indoor image as an example, the red-colored visual

8

region in the left plot focuses not only on the related words (e.g. computer and monitor) but also the
words that describe peripheral objects (e.g. pictures on the wall), and words that are incorrect (e.g.
television). In this case, the inter-domain alignment is weak and the integration of features within
a certain domain is not concentrated, making the image representations undesirable. As the two
modalities iteratively attend to each other, the features in the two domains gradually concentrate on
concrete objects and corresponding visual words. In the third iteration where the model performance
peaks (among the visualized iterations), the boundaries of the visual regions are well-deﬁned and
the dominant visual words making up the textual concepts are satisfactory. However, the features
are over-concentrated in the ﬁfth iteration, ﬁltering out some requisite information. For example,
the red region shrinks to a single person in the ﬁrst example, and a single monitor in the second
example, which reduces the information about number (e.g., group, three, computers and monitors)
and attribute (e.g., skis). Hence, it is necessary to decide an appropriate number of iteration for
acquiring better image representations.

4 Related Work

Representing images. A number of neural approaches have been proposed to obtain image repre-
sentations in various forms. An intuitive method is to extract visual features using a CNN or a RCNN.
The former splits an image into a uniform grid of visual regions (Figure 1 (a)), and the latter produces
object-level visual features based on bounding boxes (Figure 1 (b)), which has proven to be more
effective. For image captioning, Fang et al. [8], Wu et al. [33] and You et al. [37] augmented the
information source with textual concepts that are given by a predictor, which is trained to ﬁnd the most
frequent words in the captions. A most recent advance [36] built graphs over the RCNN-detected
visual regions, whose relationships are modeled as directed edges in a scene-graph, which is further
encoded via a Graph Convolutional Network (GCN).

Visual-semantic alignment. To acquire integrated image representations, we introduce the Mutual
Iterative Attention (MIA) strategy, which is based on the self-attention mechanism [30], to align the
visual features and textual concepts. It is worth noticing that for image captioning, Karpathy and
Li [11] also introduced the notion of visual-semantic alignment. They endowed the RCNN-based
visual features with semantic information by minimizing their distance in a multimodal embedding
space with corresponding segments of the ground-truth caption, which is quite different from our
concept-based iterative alignment. In the ﬁeld of VQA, some recent efforts [21, 12, 6, 22] have
also been dedicated to study the image-question alignment. Such alignment intends to explore the
latent relation between important question words and image regions. Differently, we focus on a
more general purpose of building semantic-grounded image representations through the alignment
between visual regions and corresponding textual concepts. The learned semantic-grounded image
representations, as shown by our experiments, are complementary to the VQA models that are based
on image-question alignment.

5 Conclusions

We focus on building integrated image representations to describe salient image regions from both
visual and semantic perspective to address the lack of structural relationship among individual
features. The proposed Mutual Iterative Attention (MIA) strategy aligns the visual regions and textual
concepts by conducting mutual attention over the two modalities in an iterative way. The reﬁned
image representations may provide a better start point for vision-and-language grounding problems.
In our empirical studies on the MSCOCO image captioning dataset and the VQA v2.0 dataset, the
proposed MIA exhibits compelling effectiveness in boosting the baseline systems. The results and
relevant analysis demonstrate that the semantic-grounded image representations are essential to
the improvements and generalize well to a wide range of existing systems for vision-and-language
grounding tasks.

Acknowledgments

This work was supported in part by National Natural Science Foundation of China (No. 61673028).
We thank all the anonymous reviewers for their constructive comments and suggestions. Xu Sun is
the corresponding author of this paper.

9

References

[1] P. Anderson, B. Fernando, M. Johnson, and S. Gould. SPICE: Semantic propositional image

caption evaluation. In ECCV, 2016.

[2] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang. Bottom-up and

top-down attention for image captioning and VQA. In CVPR, 2018.

[3] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh. VQA: Visual

question answering. In ICCV, 2015.

[4] L. J. Ba, R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450,

2016.

[5] S. Banerjee and A. Lavie. METEOR: An automatic metric for MT evaluation with improved

correlation with human judgments. In IEEvaluation@ACL, 2005.

[6] H. Ben-younes, R. Cadène, M. Cord, and N. Thome. MUTAN: Multimodal tucker fusion for

visual question answering. In ICCV, 2017.

[7] X. Chen, H. Fang, T. Lin, R. Vedantam, S. Gupta, P. Dollár, and C. L. Zitnick. Microsoft COCO
captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.

[8] H. Fang, S. Gupta, F. N. Iandola, R. K. Srivastava, L. Deng, P. Dollár, J. Gao, X. He, M. Mitchell,
J. C. Platt, C. L. Zitnick, and G. Zweig. From captions to visual concepts and back. In CVPR,
2015.

[9] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. Making the V in VQA matter:
Elevating the role of image understanding in visual question answering. In CVPR, 2017.

[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR,

2016.

CVPR, 2015.

[11] A. Karpathy and F. Li. Deep visual-semantic alignments for generating image descriptions. In

[12] J. Kim, J. Jun, and B. Zhang. Bilinear attention networks. In NeurIPS, 2018.

[13] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L. Li,
D. A. Shamma, M. S. Bernstein, and F. Li. Visual Genome: Connecting language and vision
using crowdsourced dense image annotations. IJCV, 2017.

[14] C. Lin and E. H. Hovy. Automatic evaluation of summaries using n-gram co-occurrence

statistics. In HLT-NAACL, 2003.

[15] C.-Y. Lin. ROUGE: A package for automatic evaluation of summaries. In ACL Workshop, 2004.

[16] T. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick.

Microsoft COCO: common objects in context. In ECCV, 2014.

[17] F. Liu, X. Ren, Y. Liu, H. Wang, and X. Sun. simNet: Stepwise image-topic merging network

for generating detailed and comprehensive image captions. In EMNLP, 2018.

[18] F. Liu, X. Ren, Y. Liu, K. Lei, and X. Sun. Exploring and distilling cross-modal information for

image captioning. In IJCAI, 2019.

[19] J. Lu, C. Xiong, D. Parikh, and R. Socher. Knowing when to look: Adaptive attention via a

visual sentinel for image captioning. In CVPR, 2017.

[20] J. Lu, J. Yang, D. Batra, and D. Parikh. Neural baby talk. In CVPR, 2018.

[21] H. Nam, J. Ha, and J. Kim. Dual attention networks for multimodal reasoning and matching. In

CVPR, 2017.

[22] D. Nguyen and T. Okatani. Improved fusion of visual and language representations by dense

symmetric co-attention for visual question answering. In CVPR, 2018.

10

[23] Y. Pan, T. Yao, H. Li, and T. Mei. Video captioning with transferred semantic attributes. In

CVPR, 2017.

[24] K. Papineni, S. Roukos, T. Ward, and W. Zhu. BLEU: A method for automatic evaluation of

machine translation. In ACL, 2002.

[25] S. Ren, K. He, R. B. Girshick, and J. Sun. Faster R-CNN: Towards real-time object detection

with region proposal networks. In NIPS, 2015.

[26] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel. Self-critical sequence training for

image captioning. In CVPR, 2017.

[27] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image

recognition. arXiv preprint arXiv:1409.1556, 2014.

[28] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a

simple way to prevent neural networks from overﬁtting. JMLR, 2014.

[29] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and

A. Rabinovich. Going deeper with convolutions. In CVPR, 2015.

[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and

I. Polosukhin. Attention is all you need. In NIPS, 2017.

[31] R. Vedantam, C. L. Zitnick, and D. Parikh. CIDEr: Consensus-based image description

evaluation. In CVPR, 2015.

generator. In CVPR, 2015.

[32] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption

[33] Q. Wu, C. Shen, L. Liu, A. R. Dick, and A. van den Hengel. What value do explicit high level

concepts have in vision to language problems? In CVPR, 2016.

[34] X. Yang, K. Tang, H. Zhang, and J. Cai. Auto-encoding scene graphs for image captioning. In

CVPR, 2019.

2017.

2018.

2006.

CVPR, 2016.

[35] T. Yao, Y. Pan, Y. Li, Z. Qiu, and T. Mei. Boosting image captioning with attributes. In ICCV,

[36] T. Yao, Y. Pan, Y. Li, and T. Mei. Exploring visual relationship for image captioning. In ECCV,

[37] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo. Image captioning with semantic attention. In

[38] C. Zhang, J. C. Platt, and P. A. Viola. Multiple instance boosting for object detection. In NIPS,

11

A Experiments on Image Captioning

A.1 Dataset and Evaluation Metrics

MSCOCO [7] is a popular dataset for image captioning. It contains 123,287 images, each of which is
paired with 5 descriptive sentences. Following common practice [19, 2, 18, 17], we report results
with the help of the MSCOCO captioning evaluation toolkit [7], and use the publicly-available splits
provided by Karpathy and Li [11], where the validation set and test set both contain 5,000 images.
The toolkit includes the commonly-used evaluation metrics SPICE, CIDEr, BLEU, METEOR and
ROUGE in image captioning task. SPICE [1] and CIDEr [31] are customized for evaluating image
captioning systems, based on scene-graph matching and n-gram matching, respectively. BLEU [24]
and METEOR [5] are originally designed for machine translation, and ROUGE [14, 15] measures
the quality of summaries.

A.2 Baselines and Implementation Details

We design ﬁve representative baselines, which are built on the models in previous work. Especially,
since our main contribution is to provide a new kind of image representations, those baselines use
different kinds of image representations, and we keep the backbone of those models as neat as
possible.

The baselines are described as follows:

• Visual Attention. The Visual Attention model is adapted from the spatial attention model in
Lu et al. [19]. It uses the 49 grid visual features I ∈ RLI ×dh from the last convolutional layer of
ResNet-152 as the image representation. The decoder is a LSTM model initialized with two zero
vectors: h0 = 0, c0 = 0, which is the same for the other baselines, with the exception that Visual
Regional Attention contains two LSTM decoders. For each decoding step, the decoder takes the
t , added with the averaged visual features Ia = 1
caption embedding we
i=1 Ii as input to the
LI
LSTM:

(cid:80)LI

ht = LSTM (ht−1, we

t + Ia)

Then, the LSTM output ht is used as a query to attend to the visual features:

αt = softmax (cid:0)wα tanh (cid:0)WI I T ⊕ Whht

(cid:1)(cid:1) ,

ct = αtI

where the wα, WI and Wh are the learnable parameters. ⊕ denotes the matrix-vector addition, which
is calculated by adding the vector to each column of the matrix. Finally, the LSTM output and the
attended visual features are used to predict the next word:

yt ∼ pt = softmax (Wp (ht + ct))

To augment Visual Attention with MIA, both the visual features in Eq. (8) and Eq. (9) are replaced
with the MIA-reﬁned image representations. For “Visual Attention w/ IN ”, we only use MIA-reﬁned
visual features IN for the same replacement.

• Concept Attention. The Concept Attention model is built by adapting the semantic attention model
in You et al. [37]. We predict 49 textual concepts T ∈ RLT ×dh using a textual concept extractor
proposed by Fang et al. [8]. Similar to You et al. [37], the averaged visual features Ia is fed to the
decoder at the ﬁrst time step, which means h1 = LSTM(h0, Ia). For the subsequent decoding steps,
the input is a sum of the caption embedding we
i=1 Ti:

t and the averaged textual concepts Ta = 1
LT

(cid:80)LT

ht = LSTM (ht−1, we

t + Ta)

An attention operation is performed over the textual concepts, with ht as the query:
αt = softmax (cid:0)wα tanh (cid:0)WT T T ⊕ Whht

ct = αtT

(cid:1)(cid:1) ,

Then, like Eq. (10), a softmax layer predicts the output word distribution. For the deployment of
MIA, the original textual concepts are replaced with reﬁned image representations or reﬁned textual
concepts TN , in similar fashion as for the original visual features.

• Visual Condition. The Visual Condition model is adapted from the LSTM-A4 model in Yao et al.
[35], jointly considers the visual features and textual concepts. Speciﬁcally, Visual Condition refers

(8)

(9)

(10)

(11)

(12)

12

to the averaged textual concepts Ta at the ﬁrst decoding step, while takes the sum of the averaged
visual features Ia and the word embedding we

t as input for the subsequent steps:

The LSTM output ht is then followed with a softmax layer to predcit the next word:

h1 = LSTM (h0, Ta)
ht = LSTM (ht−1, we

t + Ia) ,

t ≥ 2

yt ∼ pt = softmax (Wpht)

After being processed by MIA, the reﬁned image representations are used as replacement for both the
visual features and textual concepts in Eq. (13) and Eq. (14), respectively.

• Concept Condition. The Concept Condition model is adapted from the LSTM-A5 model in Yao
et al. [35]. In contrast to Visual Condition, it reverses the order of input visual features and textual
concepts, which can be deﬁned as follows:

h1 = LSTM (h0, Ia)
ht = LSTM (ht−1, we

t + Ta) ,

t ≥ 2

The prediction of the next caption word and the utilization of MIA are also similar to Visual Condition.

• Visual Regional Attention. The Visual Regional Attention model is adapted from the Up-Down
model in Anderson et al. [2]. It uses the 36 region-based visual features, which are extracted by a
variant of Faster R-CNN [25]. The Faster R-CNN is provided by Anderson et al. [2] and is pre-trained
on Visual Genome [13]. Two stacked LSTMs are adopted for caption generation, both of which are
initialized with zero hidden states: h1
0 = 0; h2
0 = 0. At each decoding step, the ﬁrst
LSTM takes the caption embedding we
t , concatenated with the averaged visual features Ia as input:

0 = 0, c1

0 = 0, c2

h1
t = LSTM1

(cid:0)h1

t−1, [we

t ; Ia](cid:1)

Then, the ﬁrst LSTM output h1

t is used as a query to attend to the region-based visual features:
(cid:1)(cid:1) ,

αt = softmax (cid:0)wα tanh (cid:0)WI I T ⊕ Whh1

ct = αtI

t

(19)

After that, the second LSTM takes the ﬁrst LSTM output h1
features ct as input, followed by a softmax layer to predict the target word:

t , concatenated with the attended visual

(13)
(14)

(15)

(16)
(17)

(18)

(20)

(21)

h2
t = LSTM2

(cid:0)h2

t−1, [h1

t ; ct](cid:1)

yt ∼ pt = softmax (cid:0)Wph2

(cid:1)

t

For the MIA-augmented model, we replace the region-based visual features in Eq. (18) and Eq. (19)
with MIA-reﬁned image representations.

B Experiments on Visual Question Answering

B.1 Dataset and Evaluation Metrics

We evaluate the models on VQA version 2.0 [9], which is comprised of image-based question-answer
pairs labeled by human annotators, where the images are collected from the MSCOCO dataset [16].

VQA v2.0 is an updated version of previous VQA 1.0 with much more annotations and less dataset
bias. VQA v2.0 is split into train, validation and test sets. There are 82,783, 40,504 and 81,434
images, (443,757, 214,354 and 447,793 corresponding questions) in the training, validation and test
set, respectively. The questions are categorized into three types, namely Yes/No, Number and other
categories. Each question is accompanied with 10 answers composed by the annotators. Answers
with the highest frequency are treated as the ground-truth. Evaluation is conducted on the test set, the
reported accuracies are calculated by the standard VQA metric [3], with occasional disagreement
between annotators being considered.

13

B.2 Baselines and Implementation Details

We choose Up-Down [2] and BAN [12] for comparison, where the former is the winner of VQA
challenge 2017 and the latter is the state-of-the-art on VQA v2.0 dataset. They both use region-based
visual features as image representations and GRU-encoded hidden states as question representations,
and make classiﬁcation based on their combination. However, Up-Down only uses the ﬁnal sentence
vector to obtain the weight of each visual region, while BAN uses a bilinear attention to obtain the
weight for each pair of visual region and question word. For equipping with our MIA, we simply
replace the original visual features with semantic-grounded image representations provided by MIA.

C Further Experimental Analysis

C.1 Effect of Guiding Scheme

We can either start with the textual concepts guiding the integration of the visual features or let the
latter to take the initiative. Even if the role of visual features and textual concepts are equivalent in
mutual attention, the choice of guiding scheme could make a difference. We examine the performance
of Visual Attention and Concept Attention when the visual features ﬁrst attend to the textual concepts.
As shown in Table 5, the model scores are inferior to that of the alternative scheme. Especially, the
performance of the Visual Attention has even been impaired. The rationale for such phenomenon
is presumably the limited visual receptive ﬁeld of the original visual features, which makes them
inadequate to integrate the textual concepts. As to the textual concepts, they are inherently good at
describing integrated visual regions, as they contain high-level semantic information.

Table 5: Evaluation of different guiding scheme.

Model

BLEU-1

BLEU-2

BLEU-3

BLEU-4 METEOR

ROUGE

CIDEr

SPICE

Visual Attention
IN -> TN
TN -> IN

Concept Attention
IN -> TN
TN -> IN

72.6
73.2
74.5

72.6
73.2
73.8

56.0
56.8
58.4

55.9
56.5
57.4

42.2
42.8
44.4

42.5
43.0
43.8

31.7
32.0
33.6

32.5
32.9
33.6

26.5
25.5
26.8

26.5
26.6
27.1

54.6
53.9
55.8

54.4
54.7
55.3

103.0
99.0
106.7

103.2
105.5
107.9

19.3
18.7
20.1

19.4
19.5
20.3

C.2 Effect of Incorporating MIA with Scene-Graph based Models

SGAEf use [34], which learns ﬁner representations of an image through scene-graphs, is the state-
of-the-art image captioning system at the time of our submission. We incorporate the scene-graph
based features with MIA-reﬁned image representations, and see whether MIA can still help SGAE.
As presented in Table 6, MIA also boosts the performance of SGAE, indicating that MIA learns very
effective representations even for scene-graphs.

Table 6: Evaluation of on the scene-graph based model.

Methods

BLEU-4 METEOR

ROUGE

CIDEr

SPICE

SGAEf use
w/ MIA

39.3
39.6

28.5
29.0

58.8
58.9

129.6
130.1

22.3
22.8

C.3 SPICE Sub-Category Results

For a better understanding of the differences of the generated captions by different methods, we report
the breakdown of SPICE F-scores (see Table 7). As we can see, the IN , TN and MIA promotes the
baselines over almost all sub-categories. Especially, the IN is good at associating related parts in the
image, which is demonstrated by the increased scores in Count and Size. and the TN collects relevant
textual concepts, providing comprehensive context that is detailed in objects. Encouragingly, when
incorporating IN and TN at the same time, i.e., w/ MIA, the advantages of the IN and TN are united
to produce a balanced improvement. It proves the effectiveness of our approach.

14

Table 7: Variation of model performance under the breakdown of SPICE F-scores. We can ﬁnd that
the w/ TN has a higher Object scores than the baselines, and the w/ IN reaches better scores in Count
and Size. As we can see, incorporating Mutual Iterative Attention (MIA) directly on the baselines,
leads to overall improvements.

Methods

SPICE

All

Objects

Attributes

Relations

Color

Count

Size

Visual Attention
w/ IN
w/ MIA

Concept Attention
w/ TN
w/ MIA

19.3
19.6
20.1

19.4
20.0
20.3

35.2
35.8
36.4

34.8
35.8
36.1

9.1
9.3
9.8

10.3
10.7
11.4

5.3
5.5
5.7

5.3
5.4
5.5

10.7
9.9
10.8

13.5
13.6
14.1

3.0
7.2
6.9

4.7
4.2
7.1

3.3
4.2
3.9

4.8
4.6
5.2

C.4 Samples of Generated Captions

We show the captions generated by the method w/o MIA and the method w/ MIA to intuitively analyze
the differences of the methods. As shown in Table 8, the w/ IN is good at portraying the number and
size but is less speciﬁc in objects. The w/ TN includes more objects but lacks details, such as number.
The proposed MIA can help the baselines to achieves a very good balance.

Table 8: Examples of the captions generated by different methods. For every example, we show the
top-10 relevant textual concepts. Based on the Mutual Iterative Attention (MIA) over the source infor-
mation, from the generated captions, we can ﬁnd that the w/ TN results in more comprehensiveness
in objects. The w/ IN helps the baselines to generate captions that are more detailed in count and
size, and the w/ MIA is able to generate more complete captions that is detailed both in the objects,
attributes, relations and color.

Image

Concepts

Captions

Visual Attention (Based on Visual Features)

Concept Attention (Based on Textual Concepts)

water boat
luggage sitting
black ocean
large white
suitcases near

standing zebras
zebra ﬁeld grass
dry tall close
stand large

Reference: a number of suitcases on the boat in the sea.

Baseline: a suitcase sitting on top of a body of water.

w/ IN : a couple of luggage sitting on top of a boat.
w/ MIA: a couple of black luggage sitting on the edge of the water.

Reference: two zebras stand in a ﬁeld with tall grass.

Baseline: a zebra standing in the middle of a ﬁeld.

w/ IN : two large zebras standing in a grass ﬁeld.
w/ MIA: a couple of zebras standing on top of a dry grass ﬁeld.

vase ﬂowers
table glass
display sitting
orange ﬁlled red
yellow

bus double
decker red street
down city road
driving stop

Reference: orange, red and white ﬂowers in vases on tables.

Baseline: a vase ﬁlled with some orange ﬂowers.

w/ TN : a vase ﬁlled with yellow ﬂowers on top of a table.
w/ MIA: a small vase ﬁlled with red and orange ﬂowers on a table.

Reference: a red double decker bus is driving on a city street.

Baseline: a red bus driving down a street.

w/ TN : a double decker bus driving down a city street.
w/ MIA: a red double decker bus driving down a city street.

15

