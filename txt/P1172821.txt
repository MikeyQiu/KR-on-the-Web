A Read-Write Memory Network for Movie Story Understanding

Seil Na1, Sangho Lee1, Jisung Kim2, Gunhee Kim1
1Seoul National University, 2SK Telecom
{seil.na, sangho.lee}@vision.snu.ac.kr, joyful.kim@sk.com, gunhee@snu.ac.kr
https://github.com/seilna/RWMN

8
1
0
2
 
r
a

M
 
6
1
 
 
]

V
C
.
s
c
[
 
 
4
v
5
4
3
9
0
.
9
0
7
1
:
v
i
X
r
a

Abstract

We propose a novel memory network model named Read-
Write Memory Network (RWMN) to perform question and
answering tasks for large-scale, multimodal movie story
understanding. The key focus of our RWMN model is to
design the read network and the write network that con-
sist of multiple convolutional layers, which enable memory
read and write operations to have high capacity and ﬂex-
ibility. While existing memory-augmented network models
treat each memory slot as an independent block, our use of
multi-layered CNNs allows the model to read and write se-
quential memory cells as chunks, which is more reasonable
to represent a sequential story because adjacent memory
blocks often have strong correlations. For evaluation, we
apply our model to all the six tasks of the MovieQA bench-
mark [26], and achieve the best accuracies on several tasks,
especially on the visual QA task. Our model shows a poten-
tial to better understand not only the content in the story,
but also more abstract information, such as relationships
between characters and the reasons for their actions.

1. Introduction

For many problems of video understanding, including
video classiﬁcation [1, 14], video captioning [30, 31] and
MovieQA [26], it is key to success for models to correctly
process, represent, and store long sequential information.
In the era of deep learning, one prevailing approach to
model sequential input is to use recurrent neural networks
(RNNs) [17] which store the given information into a hid-
den memory and update it over time. However, RNNs ac-
cumulate information in a single ﬁxed-length memory re-
gardless of the length of an input sequence, thus tend to fail
to utilize far-distant information due to a vanishing gradient
problem, which is still not fully solved even with advanced
models such as LSTM [12] and GRU [3].

As another recent alternative to resolve this issue, many
studies attempt to leverage an external memory structure
for neural networks, often referred to as neural memory

Figure 1. The intuition of the RWMN (Read-Write Memory Net-
work) model for movie question and answering tasks. Using
read/write networks of multi-layered CNNs, it abstracts a given
series of frames stepwise to capture higher-level sequential infor-
mation and stores it into memory slots. It eventually helps answer
complex questions of movie QAs.

networks [8, 9, 10, 16, 25, 29]. One key beneﬁt of exter-
nal memory is to enable a neural model to cache sequen-
tial inputs in memory slots, and explicitly utilize even far
early information. Such ability is particularly powerful to
solve question and answering (QA) problems, which of-
ten require models to memorize a large amount of infor-
mation, and correctly access the most relevant information
to a given question. For this reason, memory networks
have been popularly applied as state-of-the-art approaches
to many QA tasks, such as bAbI task [28], SQuAD [23],
and LSMDC [24].

MovieQA [26] is another challenging visual QA dataset,
in which models need to understand movies over two hours
long, and solve QA problems related to movie content and
plots. The MovieQA benchmark consists of six tasks ac-
cording to which sources of information is usable to solve
the QA problems, including videos, subtitles, DVS, scripts,
plot synopses, and open-end information. Understanding a
movie is a highly challenging task; it is necessary not only
to understand the content of individual video frames such
as a characters’ actions, places of events, but also to infer
more abstract and high-level knowledge such as reasons of a

characters’ behaviors, and relationships between them. For
instance, in the Harry Potter movie, to answer a question
(Q. What does Harry trick Lucius into doing? A. Freeing
Dobby), models need to realize that Dobby was a Lucius’s
house elf, wanted to escape from him, had a positive rela-
tionship with Harry, and Harry helped him. Some of such
information is visually or textually observable in the movie,
but much information like relationships between characters
and correlations between events should be deduced.

Our objective is to propose a novel memory network
model to perform QA tasks for large-scale, multimodal
movie story understanding. That is, the input to the model
can be very long (e.g. videos more than two hours long),
or be multimodal (e.g. text-only or video-text pairs). The
key focus of our novel memory network named Read-Write
Memory Networks (RWMN) is on deﬁning the memory
read/write operations to have high capacity and ﬂexibility,
for which we propose the read and write networks that con-
sist of multiple convolutional layers. Existing neural mem-
ory network models treat each memory slot as an indepen-
dent block. However, adjacent memory blocks often have
strong correlations, which are the case to represent a se-
quential story. That is, when human understands a story,
the entire story is often recognized as a sequence of closely-
interconnected abstract events. Hence, preferably memory
networks need to read and write sequential memory cells as
chunks, which are implemented by multiple convolutional
layers of the read and write network.

To conclude introduction, we summarize the contribu-

tions of this work as follows.

1. We propose a novel memory network named RWMN
that enables the model to ﬂexibly read and write more
complex and abstract information into memory slots
through read/write networks. To the best of our knowl-
edge, it is the ﬁrst attempt to leverage multi-layer
CNNs for read/write operations of a memory network.

2. The RWMN shows the best accuracies on several tasks
of MovieQA benchmark [26]; as of the ICCV2017
submission deadline (March 27, 2017 23:59 GMT),
our RWMN achieves the best performance for four out
of ﬁve tasks in the validation set, and four out of six
tasks in the test set. Our quantitative and qualitative
evaluation also assures that the read/write networks ef-
fectively utilize higher-level information in the exter-
nal memory, especially on the visual QA task.

2. Related Work

Neural Memory Networks. Recently, much research
has been done to model sequential data using explicit mem-
ory architecture. The memory access of existing memory
network models can be classiﬁed into content-based ad-

dressing and location-based addressing [8]. The content-
based addressing (e.g. [9, 29, 19]) lets the controller to gen-
erate a key vector and measure its similarity with each mem-
ory cell to ﬁnd out which cells are to be attended as the rele-
vant cells to the key vector. Location-based addressing (e.g.
[8]), on the other hand, enables simple arithmetic operations
that ﬁnd out the addresses to store or retrieve information,
regardless of the content of the key vector.

Neural Turing Machine (NTM) [8] and its extensions of
DNC [9], D-NTM [10], focus on learning the entire process
of memory interaction (read/write operations), and thus the
degree of freedom (or capability) of the model is high in
solving a given problem. They have been successfully ap-
plied to complex tasks such as sorting, sequence copying,
and graph traversal. The memory networks of [16, 25, 29]
address the QA problems using continuous memory repre-
sentation similar to the NTM. However, while the NTM
leverages both content-based and location-based address-
ing, they use only the former (content-based) memory inter-
action. They apply the concept of multi-hops to recurrently
read the memory, which results in performance improve-
ment in solving QA problems that require causal reasoning.
The work of [19, 32] proposes a key-value memory network
that stores information in the form of (key, value) pairs into
the external knowledge base. These methods are good at
solving QA problems that focus on the content or facts in a
context such as WikiMovies [19] and bAbI dataset [28].

The work of [2, 22] deals with how to make the
read/write operations scalable with extremely large amount
of memory. Chandar et al. [2] propose to organize memory
hierarchically, and Rae et al. [22] make read and write op-
erations sparse, thereby increasing scalability and reducing
the cost of operations. Cesc et al. [21] adopt convolutional
read from memory to jointly represent nearby ordered mem-
ory slots.

Compared to all the previous models, our RWMN model
is explicitly equipped with learnable read/write networks of
CNNs, which are specialized in storing and utilizing more
abstract information, such as relationships between charac-
ters, reasons for characters’ speciﬁc behaviors, as well as
understanding of facts in a given story.

Models for MovieQA. Among the models applied to
the MovieQA benchmark [26], the end-to-end memory net-
work [25] is the state-of-the-art approach.
It splits each
movie into shot subshots, and constructs memory slots
with video and subtitle features.
It then uses content-
based addressing to attend on the information relevant to
a given question. Recently, Wang and Jiang [27] present the
compare-aggregate framework for word-level matching to
measure the similarity of sentences. However, it is applied
to only a single task (plot synopses) of MovieQA.

There have been also several studies to solve Video
QA tasks in other datasets, such as LSMDC [24], MSR-

VTT [30], and TGIF-QA [13], which mainly focus on un-
derstanding short video clips, and answering about factual
elements in the clips. Yu et al. [31] achieve compelling
performance in video captioning, video QA, and video
retrieval by constructing an end-to-end trainable concept-
word-detector along with vision-to-language models.

3. Read-Write Memory Network (RWMN)

Figure 2 shows the overall structure of our RWMN. The
RWMN is trained to store the movie content with proper
representation in the memory, extract relevant information
from memory cells in response to a given query, and select
correct answer from ﬁve choices.

Based on the QA format of MovieQA dataset [26], the
input of the model is (i) a sequence of video segment
and subtitle pairs Smovie = {(v1, s1), ..., (vn, sn)} for the
whole movie, which takes about 2 hours (n ∼ 1, 558 on av-
erage), (ii) a question q for the movie, and (iii) ﬁve answer
candidates a = {a1, ..., a5}. In the video+subtitle task of
MovieQA, for example, each si is a dialog sentence of a
character, and vi = {vi1, ..., vim} is a video subshot (i.e. a
set of frames) sampled at 6 fps that are temporally aligned
with si. The output is a conﬁdence score vector over the
ﬁve answer candidates.

In the following, we explain the architecture according
to information ﬂow, from movie embedding to answer se-
lection via write/read networks.

3.1. Movie Embedding

We convert each subshot vi and text sentence si into fea-
ture representation as follows. For each frame vij ∈ vi, we
ﬁrst obtain its feature vij by applying the ResNet-152 [11]
pretrained on ImageNet [4]. We then mean-pool over all
frames as vi = (cid:80)
j vij ∈ R7×7×2,048, as a representation
of the subshot vi. For each sentence si, we ﬁrst divide the
sentence into words, apply the pretrained Word2Vec [18],
and then mean-pool with the position encoding (PE) [25] as
si = (cid:80)

j PE(sij) ∈ R300.

Finally, to obtain a multimodal space embedding of vi
and si, we use the Compact Bilinear Pooling (CBP) [6] as

several adjacent utterances and scenes in a form of events
or episodes. That is, each memory cell needs to associate
neighboring movie embeddings, instead of storing each of
n movie embedding separately. To implement this idea of
jointly storing adjacent embeddings into every slot, we ex-
ploit a convolutional neural network (CNN) as the write net-
work. We experimentally conﬁrm the following CNN de-
sign after thorough tests, by varying the dimensions, depths,
strides of convolution layers.

To the movie embedding E ∈ Rn×4,096, we ﬁrst ap-
ply a fully connected (FC) layer with parameter Wc ∈
R4,096×d, bc ∈ Rd to project each E[i] into a d-dimensional
vector. The FC layer reduces the dimension of E in order
to equalize the dimensions of query embedding and answer
embedding, which is also beneﬁcial to reduce the number of
required convolution operations later. We then use a convo-
h ×1×f w
conv ∈ Rf w
lution layer consisting of a ﬁlter ww
c ,
whose vertical and horizontal ﬁlter size is f w
h =
d, the number of ﬁlter channel is f w
c = 3 and strides are
v = 30 and sw
sw

v ×f w
v = 40, f w

h = 1, respectively:
M = ReLU(conv((EWc + bc), ww

conv, bw))

(2)

where conv (input, ﬁlter, bias) indicates the convolution
layer, bw ∈ Rf w
c is a bias, and ReLU indicates the element-
wise ReLU activation [20]. Finally, the generated memory
is M ∈ Rm×d×3, where m = (cid:98)((n − 1)/sw

v + 1)(cid:99).

Note that the write network can employ multiple convo-
lutional layers. If the number of layers is νw, then we obtain
M by recursively applying

M(l+1) = ReLU(conv(M(l), ww(l)

conv, b(l)

w ))

(3)

from l = 1 . . . , νw − 1. In section 4, we will report the
result of ablation study to ﬁnd out the best-performing νw.

3.3. The Read Network

The read network takes a question q and then generate

answer from a compatibility between q and M.

Question embedding. We embed the question sentence
q as follows. We ﬁrst obtain the Word2Vec vector [18] q as
done in section 3.1, and then project it as follows.

E[i] = CBP(vi, si) ∈ R4,096.

(1)

u = Wqq + bq

(4)

We perform this procedure for all n pairs of subshots and
text, resulting in a 2D movie embedding matrix E ∈
Rn×4,096, which is the input of our write network.

3.2. The Write Network

The write network takes a movie embedding matrix E
as an input and generates a memory tensor M as output.
The write network is motivated by that when human under-
stands a movie, she does not remember it as a simple se-
quence of speech and visual content, but rather ties together

where parameters are Wq ∈ Rd×300 and bq ∈ Rd.

Next the read network takes the memory M and the
query embedding u as input, and generates the conﬁdence
score vector o ∈ Rd as follows.

Query-dependent memory embedding. We ﬁrst trans-
form the memory M to be query-dependent. Its intuition is
that, according to the query, different types of information
must be retrieved from the memory slots. For example, for
the Harry Potter movie, suppose that one memory slot con-
tains the information about a particular scene where Harry

Figure 2. Illustration of the proposed Read-Write Network. (a) The multimodal movie embedding E is obtained using the ResNet feature
(b) The write memory M abstracts higher-level
and the Word2Vec representation from movie subshots and subscripts (section 3.1).
sequential information through multiple convolution layers (section 3.2). (c) The query-dependent memory Mq is obtained via the Compact
Bilinear Pooling (CBP) between the query and each slot of M, and then the read memory Mr is constructed through convolution layers
(section 3.3). (d) Finally, the answer with the highest conﬁdence score is chosen out of ﬁve candidates (section 3.4).

is chanting magic spells. This memory slot should be read
differently according to two different questions Q1: What
color is Harry wearing? and Q2: Why is Harry chanting
magic spells? In section 4, we will empirically show the
effectiveness of this question-dependent memory update.

To transform the memory M into a query-dependent
memory Mq ∈ Rm×d×3, we apply the CBP [6] between
each memory cell of M and the query embedding u as

Mq[i, :, j] = CBP(M[i, :, j], u)

(5)

for all i = 1, · · · , m, and j = 1, 2, 3.

Convolutional memory read. As done in the write net-
work, we also leverage a CNN to implement the read net-
work. Our intuition is that, for correctly answering the ques-
tion of movie understanding, it is important to connect and
relate a series of scenes as a whole. Therefore, we use the
CNN architecture to access chunks of sequential memory
slots. We obtain the reconstructed memory Mr by applying
h×3×f r
convolutional layers with a ﬁlter wr
whose vertical and horizontal ﬁlter size is f r
h = d,
the number of ﬁlter channel is f r
c = 3 and strides are
sr
v = 1, sr
h = 1, respectively. Finally, the reconstructed
memory is Mr ∈ Rc×d×3 with c = (cid:98)(m − 1)/sr

v ×f r
v = 3, f r

conv ∈ Rf r

v + 1(cid:99):

c

read network can also have a νr number of stacks of con-
volutional layers; the formulation is the same with Eq.(3)
only except replacing M, ww
conv, br,
respectively. We will also report the results of ablation study
about different νr in section 4.

conv, bw with Mr, wr

3.4. Answer Selection

Next we compute the attention matrix p ∈ Rc×3 through
applying the softmax to the dot product between the query
embedding u and each cell of memory Mr:

p[i, j] = softmax(Mr[i, :, j] · u)

(7)

where · indicates the dot product. Finally, the output vector
o ∈ Rd is obtained through a weighted sum between each
memory cell of Mr and the attention vector p:

o[i] =

Mr[j, i, k]p[j, k].

(8)

c
(cid:88)

3
(cid:88)

j=1

k=1

Next we obtain the embedding of ﬁve answer candidate sen-
tences {a} as done for the question in Eq.(4) with sharing
the parameters Wq and bq. As a result, we compute the
embedding of answer candidates g ∈ R5×d.

We compute the conﬁdence vector z ∈ R5 by ﬁnding the

Mr = ReLU(conv(Mq, wr

conv, br))

(6)

similarity between g and the weighted sum of o and u.

where br ∈ R3 is a bias term. As in the write network, the

z = softmax((αo + (1 − α)u)T g),

(9)

Story sources
Videos and subtitles
Subtitles
DVS
Scripts
Plot synopses

# movie
140
408
60
199
408

# QA pairs
6,462
14,944
2,446
7,810
14,944

Table 1. The number of movies and QA pairs according to data
sources in the MovieQA dataset [26].

where α ∈ [0, 1] is a trainable parameter. Finally, we
predict the answer y with the highest conﬁdence score:
y = argmaxi∈[1,5](zi).

3.5. Training

For training of our model, we minimize the softmax
cross-entropy between the prediction z and the groundtruth
one-hot vector zgt. All training parameters are initialized
with the Xavier method [7]. Experimentally, we select the
Adagrad [5] optimizer with a mini-batch size of 32, a learn-
ing rate of 0.001, and an initial accumulator value of 0.1.
We train our model up to 200 epochs, although we actively
use the early stopping to avoid overﬁtting due to the small
size of the MovieQA dataset. We repeat training each model
with 12 different random initializations, and select the one
with the lowest cost.

4. Experiments

We evaluate the proposed RWMN model for all the tasks
of MovieQA benchmark [26]. We defer more experimental
results and implementation details to the supplementary ﬁle.

4.1. MovieQA Tasks and Experimental Setting

As summarized in Table 1, MovieQA dataset [26] con-
tains 408 movies and 14,944 multiple choice QA pairs, each
of which consists of ﬁve answer choices with only one cor-
rect answer. The dataset provides with ﬁve types of story
sources associated with the movies: videos, subtitles, DVS,
scripts, and plot synopses, based on which the MovieQA
challenge hosts 6 subtasks, according to which sources of
information are differently used: (i) video+subtitle, (ii) sub-
titles only, (iii) DVS only, (iv) scripts only, (v) plot synopses
only, and (vi) open-ended. That is, there are one video-text
QA task, and four text-only QA tasks, and one open-end
QA task with no restriction on additional story sources. We
strictly follow the test protocols of the challenge, including
training/validation/test split and evaluation metrics. More
details of the dataset and rules are available in [26] and its
homepage1.

Among six tasks, we discuss our results with more focus
on the video+subtitle task, because it is the only VQA task
that requires both video and text understanding, whereas the

1http://movieqa.cs.toronto.edu/.

Methods

OVQAP
Simple MLP
LSTM + CNN
LSTM + Discriminative CNN
VCFSM
DEMN [15]
MEMN2N [26]
RWMN-noRW
RWMN-noR
RWMN-noQ
RWMN-noVid
RWMN
RWMN-bag
RWMN-ensemble

Video+Subtitle

val
–
–
–
–
–
–
34.20
34.20
36.50
38.17
37.20
38.67
38.37
38.30

test
23.61
24.09
23.45
24.32
24.09
29.97
–
–
–
–
–
36.25
35.69
–

Table 2. Performance comparison for the video+subtitle task on
MovieQA public validation/test dataset. (–) means that the method
does not participate on the task. Baselines include DEMM (Deep
embedded memory network), OVQAP (Only video question an-
swer pairs) and VCFSM (Video clip features with simple MLP).

other tasks are text-only. We weight less on the plot syn-
opses only task, since plot synopses are given with a ques-
tion, and all the QA pairs are generated from plot synopses,
this task can be tackled using simple word/sentence match-
ing algorithms (with little movie understanding), achieving
a very high accuracy of 77.63%.

We solve the video+subtitle task using the proposed
RWMN model in Figure 2. For the four text-only QA
tasks, no visual sources {v1, ..., vn} are given, thus we
use {s1, ..., sn} only to construct the movie embedding E
of Eq.(1) without the CBP. Except this, we use the same
RWMN model to solve four text-only QA tasks.

4.2. Baselines

We compare the performance of our approach with those
of all the methods proposed in the original MovieQA pa-
per [26] or in the ofﬁcial MovieQA leaderboard2. We de-
scribe the baseline names in the caption of each result table.
In order to measure the effects of key components of
the RWMN, we experiment with ﬁve variants: (i) (RWMN-
noRW) model without read/write networks, (ii) (RWMN-
noR) model with only the write network, (iii) (RWMN-
noQ) model without query-dependant memory embedding,
(iv) (RWMN-noVid) model trained without using videos to
quantify the importance of visual input, and (v) (RWMN)
model with both write/read networks.

We also test two ensemble versions of our model. Since
the MovieQA dataset size is relatively small compared
to task difﬁculty (e.g. 4,318 training QA examples in
video+subtitle category), models often suffer from severe

2http://movieqa.cs.toronto.edu/leaderboard/ as of

the ICCV2017 submission deadline (March 27, 2017 23:59 GMT).

Method

MEMN2N [26]
SSCB-W2V [26]
SSCB-TF-IDF [26]
SSCB Fusion [26]
CNN Word Matching [27]
Convnet Fusion (TF-IDF + Word2Vec)
Longest Answer
RWMN

Subtitle

Script

DVS

val
38.0
24.8
27.6
27.7
–
–
–
40.4

test
36.9
23.7
26.5
–
–
–
–
38.5

val
42.3
25.0
26.1
28.7
–
–
–
44.0

test
37.0
24.4
23.9
–
–
–
–
39.4

val
33.0
24.8
24.5
24.8
–
–
–
40.0

test
35.0
24.9
23.3
–
–
–
–
34.2

Plot Synopses Open-end
val
40.6
45.1
48.5
56.7
72.1
–
–
37.0

test
38.4
45.6
47.4
56.7
72.9
77.6
–
34.8

test
–
–
–
–
–
–
25.6
36.6

Table 3. Performance comparison for all the tasks on MovieQA public validation/test dataset. (–) indicates that the method does not
participate on the task. The description of baselines with no reference can be found in the MovieQA leaderboard.

overﬁtting, which the ensemble methods can mitigate. The
ﬁrst (RWMN-bag) is a bagged version of our approach, in
which we independently learn RWMN models on 30 boot-
strapped datasets, and obtain the averaged prediction. The
second (RWMN-ensemble) is a simple ensemble, in which
we independently train 20 models with different random
initializations, and compute the average prediction.

4.3. Quantitative Results

We below report the results of each method on the val-
idation and test sets, both of which are not used for train-
ing at all. While the original MovieQA paper [26] reports
the results on the validation set only, the ofﬁcial leader-
board shows the performance on the test set only, for which
groundtruth answers are not observable and the evaluation
is performed through the evaluation server. The test sub-
mission to the server is limited to once every 72 hours.

As of the ICCV2017 submission deadline, our RWMN
achieves the best performance for four out of ﬁve tasks in
the validation set, and four out of six tasks in the test set.

Results of VQA task. Table 2 compares the perfor-
mance of our RWMN model with those of baselines for
the video+subtitle task. We observe that RWMN achieves
the best performance on both validation and test sets. For
example, in the test set, RWMN attains 36.25%, which is
signiﬁcantly better than the runner-up DEMN of 29.97%.

As expected, the RWMN with both read/write networks
is the best among our variants on both validation and test
sets. It implicates that read/write networks play a key role in
improving movie understanding. For example, the RWMN-
noR with only write network attains higher performance
than the RWMN-noRW, which has similar or lower perfor-
mance than other existing models. The RWMN-noQ with-
out question-dependent memory embedding also underper-
forms the normal RWMN, which shows that the memory
update according to the question is indeed helpful to se-
lect a more relevant answer to the question. Finally, the
RWMN-noVid is not as good as the RWMN, meaning that
our RWMN successfully exploits both full videos and subti-
tles for training. Interestingly, the ensemble methods of our

model, RWMN-bag and RWMN-ensemble, slightly under-
perform the single model RWMN.

Results of text-only tasks. Table 3 shows the results on
the validation and test sets for text-only categories (i.e. sub-
title only, DVS only, script only, plot synopses only). For
the open-end task, we simply use the plot synopses version
of our method, which outperforms the only trivial baseline
for the test set (i.e. selecting the longest answer choice).

Our RWMN achieves the best performance in all tasks
except for DVS-test set and plot synopses task. We also
observe that the ensemble methods hardly improve the per-
formance of our method noticeably. As discussed before,
the memory network approaches including our RWMN and
MEMN2N are not outstanding in the plot synopses only
category. It is mainly due to that the queries and answer
choices are made directly from the plot sentences, and thus,
this task can be tackled better by word/sentence matching
methods with little story comprehension. In addition, each
plot synopsis consists of about 35 sentences on average as a
summary of a movie, which is much shorter than other data
types, for examples, about 1,558 sentences of subtitles per
movie. Therefore, the memory abstraction by our method
becomes less critical to solve the problems in this category.
One important difference between the four text-only
tasks is that each story source has a different n (i.e. the
number of sentences), and thus the density of information
contained in each sentence is also different. For exam-
ple, the average n of the scripts is about 2,877 per movie,
while the average n of DVS is about 636; thus, each sen-
tence in the script contains low-level details, while each
sentence in the DVS contain high-level and abstract content.
Given that the performance improvement by our RWMN is
more signiﬁcant in the DVS only task (e.g. RWMN: 40.0
and MEMN2N: 33.0), it can be seen that our proposal to
read/write networks may be more beneﬁcial to understand
and answer high-level and abstract content.

4.4. Ablation Results

We experiment the performance variation according to
the structure of CNNs in the write/read networks. Among

v , f w
v , sr

hyperparameters of the RWMN, the following three com-
binations have signiﬁcant effects on the performance of
the model; i) conv-ﬁlter/stride sizes of the write network
(f w
v , sw
c ), ii) conv-ﬁlter/stride sizes of the read net-
v, f r
work (f r
c ), and iii) number of read/write CNN layers
νr, νw. Regarding the convolutions, the larger the convo-
lution ﬁlter sizes, the more memories are read/written as
a chunk. Also, as the stride size decreases or the number
of output channels increases, the total number of memory
blocks increases.

Table 4 summarizes the performance variation on the
video+subtitle task according to different combinations of
these three hyperparameters. We make several observations
from the results. First, as the number of CNN layers in
read/write network increases, the capacity of memory inter-
action may increase as well; yet the performance becomes
worsen. Presumably, the main reason may be overﬁtting
due to a relative small dataset size of MovieQA as dis-
cussed. It is hinted by our results that the two-layer CNN is
the best for training performance, while the one-layer CNN
is the best for validation. Second, we observe that there
is no absolute magic number of how many memory slots
should be read/written as a single chunk and how many
strides the memory controller moves. If the stride height
is too small or too large compared to the height of a con-
volution ﬁlter, the performance decreases. It means that the
performance can be degraded when too much information
is read/written as a single abstracted slot, when too much
information is overlapped in adjacent reads/writes (due to a
small stride), or when the information overlap is too coarse
(due to a high stride). We present more ablation results to
the supplementary ﬁle.

Figure 3 compares between the MEMN2N [26] and
our RWMN model according to question types in the
video+subtitle task. We examine the results of six question
types, according to what starting word is used in the ques-
tion: Who, Where, When, What, Why, and How. Usually,
Why questions require abstraction and high-level reasoning
to answer correctly (e.g. Why did Harry end his relationship
with Helen?, Why does Michael depart for Sicily?). On the
other hand, Who and When questions primarily deal with
factual elements (e.g. Who is Harry’s girlfriend?, When
does Grissom plan to set up Napier to be murdered?). Com-
pared to the MEMN2N [26], our RWMN shows higher per-
formance enhancement in the questions starting with Why,
which may implicate the superiority of the RWMN to deals
with high-level reasoning questions.

4.5. Qualitative Results

Figure 4 illustrates selected qualitative examples of
video+subtitle problems solved by our methods, including
four success and two near-miss cases.
In each example,
we present a sampled query video, a question, and ﬁve

# Layers
νr
νw
0
0
0
1
0
1
1
1
1
1
1
2
1
2
2
2
2
2
1
3
1
3
1
3
1
3

Write network
(f w
vi, sw
vi, f w
ci )
–
(40,7,1)
(40,30,3)
(40,30,3)
(40,60,3)
(40,10,3), (10,5,3)
(5,3,1), (5,3,1)
(4,2,1), (4,2,1)
(4,2,1), (4,2,1)
(10,3,3), (40,3,3), (100,3,3)
(40,3,3), (10,3,3), (10,3,3)
(40,3,3), (40,3,3), (40,3,3)
(100,3,3), (40,3,3), (10,3,3)

Read network Acc.
(f r
vi, sr
vi, f w
ri)
–
–
–
(3,1,1)
(3,1,1)
(3,1,1)
(3,1,1)
(3,1,1), (3,1,1)
(4,2,1), (4,2,1)
(3,1,1)
(3,1,1)
(3,1,1)
(3,1,1)

34.2
33.9
36.5
38.6
33.6
37.2
37.3
36.9
37.3
35.1
37.9
35.7
35.8

Table 4. Performance of the RWMN on the video+subtitle task, ac-
cording to the structure parameters of write/read networks. νw/r:
the number of layers for write/read networks, (f w/r
):
the height and the stride of convolution ﬁlters, and the number of
output channels.

, f w/r
ci

, sw/r
vi

vi

Figure 3. Accuracy comparison between RWMN and the
MEMN2N [26] baseline on the video+subtitle task according to
question types. The RWMN leads higher improvement for Why
questions that often require abstract and high-level understanding.

answer choices in which groundtruth is in bold and our
model’s selection is red checked. We also show on which
parts our RWMN attends over entire movies, along with the
groundtruth (GT) attention maps indicating the temporal lo-
cations of the clips where the question is actually generated,
provided by the dataset. As examples show, movie question
answering is highly challenging, and sometimes is not easy
even for human.

Our predicted attention often agrees well with the GT;
the RWMN can implicitly learn where to place its attention
in a very long movie for answering, although such infor-
mation is not available for training. However, sometimes
the RWMN can ﬁnd correct answers even with the attention
mismatch with the GT. It is due to that the MovieQA dataset
also includes many questions that are hardly solvable with
only attending on the GT parts. That is, some questions re-

Figure 4. Qualitative examples of MovieQA video+subtitle problems solved by our methods (success cases in the top two rows, and failure
cases in the last row). Bold sentences are groundtruth answers and red check symbols indicate our model’s selection. In each example, we
also show on which parts our RWMN model attend over entire movie. The attention by the RWMN often matches well with the groundtruth
(GT) where the question is actually generated.

quire understanding the relationship between characters or
progress of event development, for which attending beyond
GT parts is necessary.

5. Conclusion

We proposed a new memory network model named
Read-Write Memory Network (RWMN), whose key idea is
to propose the CNN-based read/write network that enable
the model to have highly-capable and ﬂexible read/write
operations. We empirically validated that the proposed
read/write networks indeed improve the performance of vi-
sual question answering tasks for large-scale, multimodal

movie story understanding.
Speciﬁcally, our approach
achieved the best accuracies in multiple tasks of MovieQA
benchmark, with a signiﬁcant improvement on visual QA
task. We believe that there are several future research di-
rections that go beyond this work. First, we can apply our
approach to other QA tasks that require complicated story
understanding. Second, we can explore better video and
text representation methods beyond ResNet and Word2Vec.
Acknowledgements. This research is partially sup-
ported by SK Telecom and Basic Science Research Pro-
gram through National Research Foundation of Korea
(2015R1C1A1A02036562). Gunhee Kim is the corre-
sponding author.

[18] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. Distributed Representations of Words and Phrases
and Their Compositionality. In NIPS, 2013. 3

[19] A. Miller, A. Fisch, J. Dodge, A.-H. Karimi, A. Bordes, and
J. Weston. Key-value Memory Networks for Directly Read-
ing Documents. In EMNLP, 2016. 2

[20] V. Nair and G. E. Hinton. Rectiﬁed Linear Units Improve

Restricted Boltzmann Machines. In ICML, 2010. 3

[21] C. C. Park, B. Kim, and G. Kim. Attend to You: Personal-
ized Image Captioning with Context Sequence Memory Net-
works. 2

[22] J. Rae, J. J. Hunt, I. Danihelka, T. Harley, A. W. Senior,
G. Wayne, A. Graves, and T. Lillicrap. Scaling Memory-
Augmented Neural Networks with Sparse Reads and Writes.
In NIPS, 2016. 2

[23] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. Squad:
100,000+ Questions for Machine Comprehension of Text. In
EMNLP, 2016. 1

[24] A. Rohrbach, A. Torabi, M. Rohrbach, N. Tandon, C. Pal,
H. Larochelle, A. Courville, and B. Schiele. Movie Descrip-
tion. IJCV, 123(1):94–120, 2017. 1, 2

[25] S. Sukhbaatar, J. Weston, R. Fergus, et al. End-to-End Mem-

ory Networks. In NIPS, 2015. 1, 2, 3

[26] M. Tapaswi, Y. Zhu, R. Stiefelhagen, A. Torralba, R. Ur-
tasun, and S. Fidler. MovieQA: Understanding Stories in
Movies through Question-answering. In CVPR, 2016. 1, 2,
3, 5, 6, 7

[27] S. Wang and J. Jiang. A Compare-Aggregate Model for

Matching Text Sequences. In ICLR, 2017. 2, 6

[28] J. Weston, A. Bordes, S. Chopra, A. M. Rush, B. van
Merri¨enboer, A. Joulin, and T. Mikolov.
Towards AI-
complete Question Answering: A Set of Prerequisite Toy
Tasks. arXiv preprint arXiv:1502.05698, 2015. 1, 2

[29] J. Weston, S. Chopra, and A. Bordes. Memory Networks.

ICLR, 2015. 1, 2

[30] J. Xu, T. Mei, T. Yao, and Y. Rui. MSR-VTT: A Large Video
In
Description Dataset for Bridging Video and Language.
CVPR, 2016. 1, 2

[31] Y. Yu, H. Ko, Jongwook, and G. Kim. End-to-end Concept
Word Detection for Video Captioning, Retrieval, and Ques-
tion Answering. In CVPR, 2017. 1, 3

[32] J. Zhang, X. Shi, I. King, and D.-Y. Yeung. Dynamic Key-
Value Memory Network for Knowledge Tracing. In WWW,
2017. 2

References

[1] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici,
B. Varadarajan, and S. Vijayanarasimhan. Youtube-8M: A
Large-scale Video Classiﬁcation Benchmark. arXiv preprint
arXiv:1609.08675, 2016. 1

[2] S. Chandar, S. Ahn, H. Larochelle, P. Vincent, G. Tesauro,
arXiv

and Y. Bengio. Hierarchical Memory Networks.
preprint arXiv:1605.07427, 2016. 2

[3] K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau,
F. Bougares, H. Schwenk, and Y. Bengio. Learning Phrase
Representations Using RNN Encoder-Decoder for Statistical
Machine Translation. In EMNLP, 2014. 1

[4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
In
Imagenet: A Large-scale Hierarchical Image Database.
CVPR, 2009. 3

[5] J. Duchi, E. Hazan, and Y. Singer. Adaptive Subgradient
Methods for Online Learning and Stochastic Optimization.
JMLR, pages 2121–2159, 2011. 5

[6] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and
M. Rohrbach. Multimodal Compact Bilinear Pooling for Vi-
sual Question Answering and Visual Grounding. In EMNLP,
2016. 3, 4

[7] X. Glorot and Y. Bengio. Understanding the Difﬁculty of
Training Deep Feedforward Neural Networks. In AISTATS,
2010. 5

[8] A. Graves, G. Wayne, and I. Danihelka. Neural Turing Ma-

chines. arXiv preprint arXiv:1410.5401, 2014. 1, 2

[9] A. Graves, G. Wayne, M. Reynolds, T. Harley, I. Danihelka,
A. Grabska-Barwi´nska, S. G. Colmenarejo, E. Grefenstette,
T. Ramalho, J. Agapiou, et al. Hybrid Computing Using a
Neural Network with Dynamic External Memory. Nature,
538:471–476, 2016. 1, 2

[10] C. Gulcehre, S. Chandar, K. Cho, and Y. Bengio. Dy-
namic Neural Turing Machine with Soft and Hard Address-
ing Schemes. In ICLR, 2017. 1, 2

[11] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning

for Image Recognition. In CVPR, 2016. 3

[12] S. Hochreiter and J. Schmidhuber. Long Short-term Mem-

ory. Neural computation, 9(8):1735–1780, 1997. 1

[13] Y. Jang, Y. Song, Y. Yu, Y. Kim, and G. Kim. TGIF-QA:
Toward Spatio-Temporal Reasoning in Visual Question An-
swering. In CVPR, 2017. 2

[14] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale Video Classiﬁcation with Con-
volutional Neural Networks. In CVPR, 2014. 1

[15] K.-M. Kim, M.-O. Heo, S.-H. Choi, and B.-T. Zhang. Deep-
story: video story qa by deep embedded memory networks.
IJCAI, 2017. 5

[16] A. Kumar, O. Irsoy, J. Su, J. Bradbury, R. English, B. Pierce,
P. Ondruska, I. Gulrajani, and R. Socher. Ask Me Anything:
Dynamic Memory Networks for Natural Language Process-
ing. ICML, 2016. 1, 2

[17] T. Mikolov, M. Karaﬁ´at, L. Burget, J. Cernock`y, and S. Khu-
danpur. Recurrent Neural Network Based Language Model.
In Interspeech, 2010. 1

