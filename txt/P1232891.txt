Iterative Document Representation Learning Towards Summarization
with Polishing

Xiuying Chen1, Shen Gao2, Chongyang Tao2, Yan Song3, Dongyan Zhao1,2 and Rui Yan1,2,∗
1Center for Data Science, Peking University, Beijing, China
2Institute of Computer Science and Technology, Peking University, Beijing, China
3Tencent AI Lab
{xy-chen,shengao,chongyangtao,zhaody,ruiyan}@pku.edu.cn
clksong@tencent.com

Abstract

In this paper, we introduce Iterative Text Sum-
marization (ITS), an iteration-based model for
supervised extractive text summarization, in-
spired by the observation that it is often nec-
essary for a human to read an article multiple
times in order to fully understand and summa-
rize its contents. Current summarization ap-
proaches read through a document only once
to generate a document representation, result-
ing in a sub-optimal representation. To ad-
dress this issue we introduce a model which
iteratively polishes the document representa-
tion on many passes through the document. As
part of our model, we also introduce a selec-
tive reading mechanism that decides more ac-
curately the extent to which each sentence in
the model should be updated. Experimental
results on the CNN/DailyMail and DUC2002
datasets demonstrate that our model signiﬁ-
cantly outperforms state-of-the-art extractive
systems when evaluated by machines and by
humans.

1

Introduction

A summary is a shortened version of a text doc-
ument which maintains the most important ideas
from the original article. Automatic text summa-
rization is a process by which a machine gleans the
most important concepts from an article, removing
secondary or redundant concepts. Nowadays as
there is a growing need for storing and digesting
large amounts of textual data, automatic summa-
rization systems have signiﬁcant usage potential
in society.

Extractive summarization is a technique for
generating summaries by directly choosing a sub-
set of salient sentences from the original docu-
ment to constitute the summary. Most efforts
made towards extractive summarization either rely

∗Corresponding author: Rui Yan (ruiyan@pku.edu.cn)

on human-engineered features such as sentence
length, word position, and frequency (Cohen,
2002; Radev et al., 2004; Woodsend and Lapata,
2010; Yan et al., 2011a,b, 2012) or use neural
networks to automatically learn features for sen-
tence selection (Cheng and Lapata, 2016; Nallap-
ati et al., 2016a).

Although existing extractive summarization
methods have achieved great success, one limita-
tion they share is that they generate the summary
after only one pass through the document. How-
in real-world human cognitive processes,
ever,
people read a document multiple times in order
to capture the main ideas. Browsing through the
document only once often means the model cannot
fully get at the document’s main ideas, leading to
a subpar summarization. We share two examples
of this. (1) Consider the situation where we almost
ﬁnish reading a long article and forget some main
points in the beginning. We are likely to go back
and review the part that we forget. (2) To write
a good summary, we usually ﬁrst browse through
the document to obtain a general understanding of
the article, then perform a more intensive reading
to select salient points to include in the summary.
In terms of model design, we believe that letting
a model read through a document multiple times,
polishing and updating its internal representation
of the document can lead to better understanding
and better summarization.

To achieve this, we design a model that we call
Iterative Text Summarization (ITS) consisting of a
novel “iteration mechanism” and “selective read-
ITS is an iterative process, read-
ing module”.
ing through the document many times. There is
one encoder, one decoder, and one iterative unit in
each iteration. They work together to polish doc-
ument representation. The ﬁnal labeling part uses
outputs from all iterations to generate summaries.
The selective reading module we design is a modi-

9
1
0
2
 
y
a
M
 
0
3
 
 
]
L
C
.
s
c
[
 
 
2
v
4
2
3
0
1
.
9
0
8
1
:
v
i
X
r
a

ﬁed version of a Gated Recurrent Unit (GRU) net-
work, which can decide how much of the hidden
state of each sentence should be retained or up-
dated based on its relationship with the document.

Overall, our contribution includes:

1. We propose Iterative Text Summarization
(ITS), an iteration based summary generator
which uses a sequence classiﬁer to extract
salient sentences from documents.

2. We introduce a novel iterative neural net-
work model which repeatedly polishes the
distributed representation of document in-
stead of generating that once for all. Besides,
we propose a selective reading mechanism,
which decides how much information should
be updated of each sentence based on its re-
lationship with the polished document rep-
resentation. Our entire architecture can be
trained in an end-to-end fashion.

3. We evaluate our summarization model on
representative CNN/DailyMail corpora and
benchmark DUC2002 dataset. Experimen-
tal results demonstrate that our model out-
performs state-of-the-art extractive systems
when evaluated automatically and by human.

2 Related Work

Our research builds on previous works in two
ﬁelds: summarization and iterative modeling.

Text summarization can be classiﬁed into ex-
tractive summarization and abstractive summa-
rization. Extractive summarization aims to gener-
ate a summary by integrating the most salient sen-
tences in the document. Abstractive summariza-
tion aims to generate new content that concisely
paraphrases the document from scratch.

With the emergence of powerful neural net-
work models for text processing, a vast majority of
the literature on document summarization is ded-
icated to abstractive summarization. These mod-
els typically take the form of convolutional neu-
ral networks (CNN) or recurrent neural networks
(RNN). For example, Rush et al. (2015) propose
an encoder-decoder model which uses a local at-
tention mechanism to generate summaries. Nal-
lapati et al. (2016b) further develop this work by
addressing problems that had not been adequately
solved by the basic architecture, such as keyword
modeling and capturing the hierarchy of sentence-
to-word structures. In a follow-up work, Nallapati

et al. (2017) propose a new summarization model
which generates summaries by sampling a topic
one sentence at a time, then producing words us-
ing an RNN decoder conditioned on the sentence
topic. Another related work is by See et al. (2017),
where the authors use “pointing” and “coverage”
techniques to generate more accurate summaries.

Despite the focus on abstractive summarization,
extractive summarization remains an attractive
method as it is capable of generating more gram-
matically and semantically correct summaries.
This is the method we follow in this work. In ex-
tractive summarization, Cheng and Lapata (2016)
propose a general framework for single-document
text summarization using a hierarchical article en-
coder composed with an attention-based extractor.
Following this, Nallapati et al. (2016a) propose
a simple RNN-based sequence classiﬁer which
outperforms or matches the state-of-art models at
the time.
In another approach, Narayan et al.
(2018) use a reinforcement learning method to op-
timize the Rouge evaluation metric for text sum-
marization. The most recent work on this topic
is (Wu and Hu, 2018), where the authors train a
reinforced neural extractive summarization model
called RNES that captures cross-sentence coher-
ence patterns. Due to the fact that they use a dif-
ferent dataset and have not released their code, we
are unable to compare our models with theirs.

The idea of iteration has not been well explored
for summarization. One related study is Xiong
et al. (2016)’s work on dynamic memory net-
works, which designs neural networks with mem-
ory and attention mechanisms that exhibit certain
reasoning capabilities required for question an-
swering. Another related work is (Yan, 2016),
where they generate poetry with iterative polish-
ing sn chema. Similiar method can also be applied
on couplet generation as in (Yan et al., 2016). We
take some inspiration from their work but focus on
document summarization. Another related work is
(Singh et al., 2017), where the authors present a
deep network called Hybrid MemNet for the sin-
gle document summarization task, using a mem-
ory network as the document encoder. Compared
to them, we do not borrow the memory network
structure but propose a new iterative architecture.

3 Methodology

3.1 Problem Formulation

In this work, we propose Iterative Text Sum-
marization (ITS), an iteration-based supervised
model for extractive text summarization. We treat
the extractive summarization task as a sequence
labeling problem, in which each sentence is vis-
ited sequentially and a binary label that determines
whether or not it will be included in the ﬁnal sum-
mary is generated.

1, . . . , wi

ITS takes as input a list of sentences s =
{s1, . . . , sns}, where ns is the number of sen-
tences in the document. Each sentence si is a
list of words: si = {wi
nw }, where nw is
the word length of the sentence. The goal of ITS
is to generate a score vector y = {y1, . . . , yns}
for each sentence, where each score yi ∈ [0, 1]
denotes the sentence’s extracting probability, that
is, the probability that the corresponding sentence
si will be extracted to be included in the sum-
mary. We train our model in a supervised man-
ner, using a corresponding gold summary written
by human experts for each document in training
set. We use an unsupervised method to convert
the human-written summaries to gold label vec-
tor y(cid:48) = {y(cid:48)
ns}, where y(cid:48)
i ∈ {0, 1} denotes
whether the i-th sentence is selected (1) or not (0).
Next, during training process, the cross entropy
loss is calculated between y and y(cid:48), which is min-
imized to optimize y. Finally, we select three sen-
tences with the highest score according to y to be
the extracted summary. We detail our model be-
low.

1, ..., y(cid:48)

3.2 Model Architecture

ITS is depicted in Fig.1. It consists of multiple it-
erations with one encoder, one decoder, and one
iteration unit in each iteration. We combine the
outputs of decoders in all iterations to generate the
extracting probabilities in the ﬁnal labeling mod-
ule.

Our encoder is illustrated in the shaded region in
the left half of Fig.1. It takes as input all sentences
as well as the document representation from the
previous unit Dk−1, processes them through sev-
eral neural networks, and outputs the ﬁnal state to
the iterative unit module which updates the docu-
ment representation.

Our decoder takes the form of a bidirectional
RNN. It takes the representation of sentence gen-
erated by the encoder as input, and its initial state

is the polished document representation Dk. Our
last module, the sentence labeling module, con-
catenates the hidden states of all decoders together
to generate an integrated score for each sentence.
As we apply supervised training, the objective
is to maximize the likelihood of all sentence labels
y(cid:48) = {y(cid:48)
1, ..., y(cid:48)
ns} given the input document s and
model parameters θ:

log p(y(cid:48)|s; θ) =

log p(y(cid:48)

i|s; θ)

(1)

ns(cid:88)

i=1

4 Our Model

4.1 Encoder

In this subsection, we describe the encoding pro-
cess of our model. For brevity, we drop the super-
script k when focusing on a particular layer. All
the W ’s and b’s in this section with different su-
perscripts or subscripts are the parameters to be
learned.

Sentence Encoder: Given a discrete set of sen-
tences s = {s1, . . . , sns}, we use a word embed-
ding matrix M ∈ RV ×D to embed each word wi
in sentence si into continuous space ˆwi, where V
is the vocabulary size, D is the dimension of word
embedding.

The sentence encoder can be based on a variety
of encoding schemes. Simply taking the average
of embeddings of words in a sentence will cause
too much information loss, while using GRUs or
Long Short-Term Memory (LSTM) requires more
computational resources and is prone to overﬁt-
ting. Considering above, we select positional en-
coding described in (Sukhbaatar et al., 2015) as
our sentence encoding method. Each sentence rep-
resentation ˆsi is calculated by ˆsi = (cid:80)nw
j=1 lj ◦ ˆwi
j,
where ◦ is element-wise multiplication, lj is a col-
umn vector computed as lj,d = (1− j
D )(1−
nw
2j
nw

), lj,d denotes the d-th dimension of lj.
Note that throughout this study, we use GRUs
as our RNN cells since they can alleviate the over-
ﬁtting problem as conﬁrmed by our experiments.
As our selective reading mechanism (which will
be explained later) is a modiﬁed version of orig-
inal GRU cell, we give the details of the GRU
here. GRU is a gating mechanism in recurrent
neural networks, introduced in (Cho et al., 2014).
Their performance was found to be similar to that
of LSTM cell but using fewer parameters as de-
scribed in (Hochreiter and Schmidhuber, 1997).
The GRU cell consists of an update gate vector

)−( d

Figure 1: Model Structure: There is one encoder, one decoder and one iterative unit (which is used to polish
document representation) in each iteration. The ﬁnal labeling part is used to generating the extracting probabilities
for all sentences combining hidden states of decoders in all iterations. We take a document consists of three
sentences for example here.

ui, a reset gate vector ri, and an output vector
hi. For each time step i with input xi and pre-
vious hidden state hi−1, the updated hidden state
hi = GRU(xi, hi−1) is computed by:

ui = σ(W (u)xi + U (u)hi−1 + b(u))
ri = σ(W (r)xi + U (r)hi−1 + b(r))
(3)
˜hi = tanh(W (h)xi + ri ◦ U hi−1 + b(h)) (4)
hi = ui ◦ ˜hi + (1 − ui) ◦ hi−1
(5)

(2)

σ

the

is
sigmoid
W (u), W (r), W (h)

activation
where
∈
function,
RnH ×nI , U (u), U (r), U ∈ RnH ×nH , nH is
the hidden size, nI is the size of input xi.

To further study the interactions and informa-
tion exchanges between sentences, we establish a
Bi-directional GRU (Bi-GRU) network taking the
sentence representation as input:

−→si = GRUfwd(ˆsi, −−→si−1)
←−si = GRUbwd(ˆsi, ←−−si−1)
←→si = −→si + ←−si

(6)

(7)

(8)

where ˆsi is the sentence representation input at
time step i, −→si is the hidden state of the forward
GRU at time step i, and ←−si is the hidden state of
the backward GRU. This architecture allows in-
formation to ﬂow back and forth to generate new
sentence representation ←→si .

Document Encoder: We must initialize a doc-
ument representation before polishing it. Gener-
ating the document representation from sentence
representations is a process similar to generat-
ing the sentence representation from word embed-
dings. This time we need to compress the whole
document, not just a sentence, into a vector. Be-
cause the information a vector can contain is lim-
ited, rather than to use another neural network, we
simply use a non-linear transformation of the av-
erage pooling of the concatenated hidden states of
the above Bi-GRU to generate the document rep-
resentation, as written below:
ns(cid:88)

[−→si ; ←−si ] + b)

(9)

D0 = tanh(W

1
ns

i=1

where ‘[·;·]’ is the concatenation operation.

Selective Reading module: Now we can for-
mally introduce the selective reading module in
Fig.1. This module is a bidirectional RNN con-
sisting of modiﬁed GRU cells whose input is the
sentence representation ←→s = {←→s1 , ..., ←→sns}. In
the original version of GRU, the update gate ui
in Equation 2 is used to decide how much of hid-
den state should be retained and how much should
be updated. However, due to the way ui is calcu-
lated, it is sensitive to the position and ordering of
sentences, but loses information captured by the
polished document representation.

Herein, we propose a modiﬁed GRU cell that

replace the ui with the newly computed update
gate gi. The new cell takes in two inputs, the sen-
tence representation and the document representa-
tion from the last iteration, rather than merely the
sentence representation. For each sentence, the se-
lective network generates an update gate vector gi
in the following way:

fi = [←→si ◦ Dk−1; ←→si ; Dk−1]
Fi = W (2)tanh(W (1)fi + b(1)) + b(2)

(10)

(11)

(12)

gi =

exp(Fi)
j=1 exp(Fj)

(cid:80)ns

where ←→si
is the i-th sentence representation,
Dk−1 is the document representation from last it-
eration. Equation 5 now becomes:

hi = gi ◦ ˜hi + (1 − gi) ◦ hi−1

(13)

We use this “selective reading module” to auto-
matically decide to which extent the information
of each sentence should be updated based on its
relationship with the polished document. In this
way, the modiﬁed GRU network can grasp more
accurate information from the document.

4.2

Iterative Unit

After each sentence passes through the selective
reading module, we wish to update the document
representation Dk−1 with the newly constructed
sentence representations. The iterative unit (also
depicted above in Fig.1) is designed for this pur-
pose. We use a GRUiter cell to generate the pol-
ished document representation, whose input is the
ﬁnal state of the selective reading network from
the previous iteration, hns and whose initial state
is set to the document representation of the previ-
ous iteration, Dk−1. The updated document rep-
resentation is computed by:

Dk = GRUiter(hns, Dk−1)

(14)

4.3 Decoder

Next, we describe our decoders, which are de-
picted shaded in the right part of Fig.1. Follow-
ing most sequence labeling task (Xue and Palmer,
2004; Carreras and M`arquez, 2005) where they
learn a feature vector for each sentence, we use a
bidirectional GRUdec network in each iteration to
output features so as to calculate extracting proba-
bilities. For k-th iteration, given the sentence rep-
resentation ←→s as input and the document repre-
sentation Dk as the initial state, our decoder en-
codes the features of all sentences in the hidden

state hk = {hk

0, ..., hk
ns}:
i = GRUdec(←→s , hk
hk
hk
0 = Dk
4.4 Sentence Labeling Module

i−1)

(15)

(16)

Next, we use the feature of each sentence to gener-
ate corresponding extracting probability. Since we
have one decoder in each iteration, if we directly
transform the hidden states in each iteration to ex-
tracting probabilities, we will end up with several
scores for each sentence. Either taking the aver-
age or summing them together by speciﬁc weights
is inappropriate and inelegant. Hence, we concate-
nate hidden states of all decoders together and ap-
ply a multi-layer perceptron to them to generate
the extracting probabilities:
y = W (4)tanh(W (3)[h1; ...; hk] + b(3)) + b(4)
(17)
where y = {y1, ..., yns}, yi is the extracting prob-
ability for each setence.
In this way, we let the
model learn by itself how to utilize the outputs of
all iterations and assign to each hidden state a re-
liable weight. In section 6, we will show that this
labeling method outperforms other methods.

5 Experiment Setup

In this section, we present our experimental setup
for training and estimating our summarization
model. We ﬁrst introduce the datasets used for
training and evaluation, and then introduce our ex-
perimental details and evaluation protocol.

5.1 Datasets

In order to make a fair comparison with our base-
lines, we used the CNN/Dailymail corpus which
was constructed by Hermann et al. (2015). We
used the standard splits for training, validation and
testing in each corpus (90,266/1,220/1,093 doc-
uments for CNN and 196,557/12,147/10,396 for
DailyMail). We followed previous studies in us-
ing the human-written story highlight in each arti-
cle as a gold-standard abstractive summary. These
highlights were used to generate gold labels when
training and testing our model using the greedy
search method similar to (Nallapati et al., 2016a).
We also tested ITS on an out-of-domain cor-
pus, DUC2002, which consists of 567 documents.
Documents in this corpus belong to 59 various
clusters and each cluster has a unique topic. Each
document has two gold summaries written by hu-
man experts of length around 100 words.

5.2

Implementation Details

We implemented our model in Tensorﬂow (Abadi
et al., 2016). The code for our models is avail-
able online1. We mostly followed the settings in
(Nallapati et al., 2016a) and trained the model us-
ing the Adam optimizer (Kingma and Ba, 2014)
with initial learning rate 0.001 and anneals of 0.5
every 6 epochs until reaching 30 epochs. We se-
lected three sentences with highest scores as sum-
mary. After preliminary exploration, we found
that arranging them according to their scores con-
Ex-
sistently achieved the best performance.
periments were performed with a batch size of
64 documents. We used 100-dimension GloVe
(Pennington et al., 2014) embeddings trained on
Wikipedia 2014 as our embedding initialization
with a vocabulary size limited to 100k for speed
purposes. We initialized out-of-vocabulary word
embeddings over a uniform distribution within [-
0.2,0,2]. We also padded or cut sentences to con-
tain exactly 70 words. Each GRU module had 1
layer with 200-dimensional hidden states and with
either an initial state set up as described above or
a random initial state. To prevent overﬁtting, we
used dropout after each GRU network and embed-
ding layer, and also applied L2 loss to all unbi-
ased variables. The iteration number was set to 5
if not speciﬁed. A detailed discussion about itera-
tion number can be found in section 7.

5.3 Baselines

On all datasets we used the Lead-3 method as a
baseline, which simply chooses the ﬁrst three sen-
tences in a document as the gold summary. On
DailyMail datasets, we report the performance of
SummaRuNNer in (Nallapati et al., 2016a) and
the model in (Cheng and Lapata, 2016), as well
as a logistic regression classiﬁer (LReg) that they
used as a baseline. We reimplemented the Hy-
brid MemNet model in (Singh et al., 2017) as one
of our baselines since they only reported the per-
formance of 500 samples in their paper. Also,
Narayan et al. (2018) released their code2 for the
REFRESH model, we used their code to produce
Rouge recall scores on the DailyMail dataset as
they only reported results on CNN/DailyMail joint
dataset. Baselines on CNN dataset are similar.

1https://github.com/yingtaomj/Iterati
ve-Document-Representation-Learning-Tow
ards-Summarization-with-Polishing

2https://github.com/EdinburghNLP/Refr

esh

On DUC2002 corpus, we compare our model with
several baselines such as Integer Linear Program-
ming (ILR) and LReg. We also report the perfor-
mance of the newest neural networks model in-
cluding (Nallapati et al., 2016a; Cheng and Lap-
ata, 2016; Singh et al., 2017).

5.4 Evaluation

In the evaluation procedure, we used the Rouge
scores, i.e. Rouge-1, Rouge-2, and Rouge-L, cor-
responding to the matches of unigram, bigrams,
and Longest Common Subsequence (LCS) respec-
tively, to estimate our model. We obtained our
Rouge scores using the standard pyrouge pack-
age3. To compare with other related works, we
used full-length F1 score on the CNN corpus, lim-
ited length of 75 bytes and 275 bytes recall score
on DailyMail corpus. As for the DUC2002 corpus,
following the ofﬁcial guidelines, we examined the
Rouge recall score at the length of 75 words. All
results in our experiment are statistically signiﬁ-
cant using 95% conﬁdence interval as estimated
by Rouge script.

Schluter (2017) noted that only using the Rouge
metric to evaluate summarization quality can be
misleading. Therefore, we also evaluated our
model using human evaluation. Five highly ed-
ucated participants were asked to rank 40 sum-
maries produced by four models: the Lead-3 base-
line, Hybrid MemNet, ITS, and human-authored
highlights. We chose Hybrid MemNet as one of
the human evaluation baselines since its perfor-
mance is relatively high compared to other base-
lines.
Judging criteria included informativeness
and coherence. Test cases were randomly sampled
from DailyMail test set.

6 Experiment analysis

Table 1 shows the performance comparison of
our model with other baselines on the DailyMail
dataset with respect to Rouge score at 75 bytes
and 275 bytes of summary length. Our model
performs consistently and signiﬁcantly better than
other models on 75 bytes, while on 275 bytes, the
improvement margin is smaller. One possible in-
terpretation is that our model has high precision
on top rank outputs, but the accuracy is lower for
lower rank sentences.
In addition, (Cheng and
Lapata, 2016) used additional supervised training

3https://pypi.python.org/pypi/pyrouge

/0.1.0

Rouge-1 Rouge-2 Rouge-L Rouge-1 Rouge-2 Rouge-L

DailyMail

Lead-3
LReg(500)
Cheng et.al’16
SummaRuNNer
REFRESH
Hybrid MemNet
ITS

21.9
18.5
22.7
26.2
24.1
26.3
27.4

b75

7.2
6.9
8.5
10.8
11.5
11.2
11.9

11.6
10.2
12.5
14.4
12.5
15.5
16.1

40.5
-
42.2
42
40.3
41.4
42.4

b275

14.9
-
17.3
16.9
15.1
16.7
17.4

32.6
-
34.8
34.1
32.9
33.2
34.1

Table 1: Comparison with other baselines on DailyMail test dataset using Rouge recall score with respect to the
abstractive ground truth at 75 bytes and at 275 bytes.

CNN
Lead-3
Cheng et.al’16
Hybrid MemNet
REFRESH
ITS

Rouge-1 Rouge-2 Rouge-L
11.1
10.0
11.3
11.7
12.6

29.1
28.4
29.9
30.4
30.8

25.9
25.0
26.4
26.9
27.6

Table 2: Comparison with other baselines on CNN test
dataset using full-length F1 variants of Rouge.

to create sentence-level extractive labels to train
their model, while our model uses an unsupervised
greedy approximation instead.

We also examined the performance of our
model on CNN dataset as listed in Table 2. To
compare with other models, we used full-length
Rouge F1 metric as reported by Narayan et al.
(2018).
Results demonstrate that our model
has a consistently best performance on different
datasets.

In Table 3, we present the performance of ITS
on the out of domain DUC dataset. Our model out-
performs or matches other basic models including
LReg and ILR as well as neural network baselines
such as SummaRuNNer with respect to the ground
truth at 75 bytes, which shows that our model can
be adapted to different copora maintaining high
accuracy.

In order to explore the impact of internal struc-
ture of ITS, we also conducted an ablation study
in Table 4. The ﬁrst variation is the same model
without the selective reading module. The sec-
ond one sets the iteration number to one, that is, a
model without iteration process. The last variation
is to apply MLP on the output from the last itera-
tion instead of concatenating the hidden states of
all decoders. All other settings and parameters are
the same. Performances of these models are worse
than that of ITS in all metrics, which demonstrates

DUC2002
Lead-3
LReg
ILP
Cheng et.al’16
SummaRuNNer
Hybrid MemNet
ITS

Rouge-1 Rouge-2 Rouge-L
21.0
20.7
21.3
23.0
23.1
23.0
23.4

40.2
40.3
42.8
43.5
43.0
43.1
43.5

43.6
43.8
45.4
47.4
46.6
46.9
47.6

Table 3:
Comparison with other baselines on
DUC2002 dataset using Rouge recall score with re-
spect to the abstractive ground truth at 75 bytes.

Variations
ITS
27.4
w/o selective reading 27.1
26.9
w/o iteration
27.2
w/o concatenation

Rouge-1Rouge-2Rouge-L
11.9
11.6
11.6
11.7

16.1
15.4
15.8
15.9

Table 4: Ablation study on DailyMail test dataset with
respect to the abstractive ground truth at 75 bytes.

the preeminence of ITS. More importantly, by this
controlled experiment, we can verify the contribu-
tion of different module in ITS.

7 Further discussion

Analysis of iteration number: We did a broad
sweep of experiments to further investigate the in-
ﬂuence of iteration process on the generated sum-
mary quality. First, we studied the inﬂuence of
iteration number. In order to make a fair compar-
ison between models with different iteration num-
ber, we trained all models for same epochs without
tuning. Fig.2 illustrates the relationship between
iteration number and the Rouge score at 75 bytes
of summary length on DailyMail test dataset. The
result shows that the Rouge score increases with
the number of iteration to begin with. After reach-
ing the upper limit it begins to drop. Note that

Models
Lead-3
Hybrid MemNet
ITS
Gold

1st
0.12
0.24
0.31
0.33

2nd
0.11
0.25
0.34
0.30

3rd
0.25
0.28
0.23
0.24

4th
0.52
0.23
0.12
0.13

Table 5: System ranking comparison with other base-
lines on DailyMail corpus. Rank 1 is the best and Rank
4 is the worst. Each score represents the percentage of
the summary under this rank.

sitivity between iterations as shown in Fig.3(b). To
be speciﬁc, the sentences which are not preferred
by iteration 3 remain low probabilities in the next
two iterations, while sentences with relatively high
scores are still preferred by iteration 4 and 5.

Human Evaluation: We gave human evalua-
tors three system-generated summaries, generated
by Lead-3, Hybrid MemNet, ITS, as well as the
human-written gold standard summary, and asked
them to rank these summaries based on summary
informativeness and coherence. Table 5 shows the
percentages of summaries of different models un-
der each rank scored by human experts. It is not
surprising that gold standard has the most sum-
maries of the highest quality. Our model has the
most summaries under 2nd rank, thus can be con-
sidered 2nd best, following are Hybrid MemNet
and Lead-3, as they are ranked mostly 3rd and 4th.
By case study, we found that a number of sum-
maries generated by Hybrid MemNet have two
sentences the same as ITS out of three, however,
the third distinct sentence from our model always
leads to a better evaluation result considering over-
all informativeness and coherence. Readers can
refer to the appendix to see our case study.

8 Conclusion

In this work, we introduce ITS, an iteration based
extractive summarization model, inspired by the
observation that it is often necessary for a hu-
man to read the article multiple times to fully un-
derstand and summarize it. Experimental results
on CNN/DailyMail and DUC corpora demonstrate
the effectiveness of our model.

Acknowledgments

We would like to thank the anonymous review-
ers for their constructive comments. We would
also like to thank Jin-ge Yao and Zhengyuan Ma
for their valuable advice on this project. This
work was supported by the National Key Re-

Figure 2: Relationship between number of iteration
and Rouge score on DailyMail test dataset with respect
to the ground truth at 75 bytes.

(a)

(b)

Figure 3: The predicted extracting probabilities for
each sentence calculated by the output of each iteration.

the result of training the model for only one epoch
outperforms the state-of-the-art in (Singh et al.,
2017), which demonstrates that our selective read-
ing module is effective. The fact that continu-
ing this process increase the performance conﬁrms
that the iteration idea behind our model is useful in
practice. Based on above observation, we set the
default iteration number to be 5.

Analysis of polishing process: Next, to fully
investigate how the iterative process inﬂuences the
extracting results, we draw heatmaps of the ex-
tracting probabilities for each decoder at each it-
eration. We pick two representative cases in Fig.3,
where the x-axis represents the sentence index and
y-axis is the iteration number, x-axis labels are
omitted. The darker the color is, the higher the
In Fig.3(a), it can be
extracting probability is.
seen that when the iteration begins, most sentences
have similar probabilities. As we increase the
number of iteration, some probabilities begin to
fall and others saturate. This means that the model
already has preferred sentences to select. Another
interesting feature we found is that there is a tran-

search and Development Program of China (No.
2017YFC0804001), the National Science Foun-
dation of China (NSFC No.
61876196, No.
61672058). Rui Yan was sponsored by CCF-
Tencent Open Research Fund and Microsoft Re-
search Asia (MSRA) Collaborative Research Pro-
gram.

References

Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng
Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard,
et al. 2016. Tensorﬂow: A system for large-scale
machine learning. In OSDI, volume 16, pages 265–
283.

Xavier Carreras and Llu´ıs M`arquez. 2005.

Intro-
duction to the conll-2005 shared task: Semantic
role labeling. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learn-
ing, CONLL ’05, pages 152–164, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Jianpeng Cheng and Mirella Lapata. 2016. Neural
summarization by extracting sentences and words.

Kyunghyun Cho, Bart Van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014.
Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. Computer Sci-
ence.

Kevin Bretonnel Cohen. 2002. Natural language pro-
cessing for online applications: Text retrieval, ex-
Language,
traction and categorization (review).
80(3):510–511.

Karl Moritz Hermann, Edward Grefenstette, Lasse Es-
peholt, Will Kay, Mustafa Suleyman, and Phil Blun-
som. 2015. Teaching machines to read and compre-
hend. pages 1693–1701.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. Computer Sci-
ence.

Ramesh Nallapati, Igor Melnyk, Abhishek Kumar, and
Bowen Zhou. 2017. Sengen: Sentence generating
neural variational topic model.

Ramesh Nallapati, Feifei Zhai, and Bowen Zhou.
2016a. Summarunner: A recurrent neural network
based sequence model for extractive summarization
of documents.

Ramesh Nallapati, Bowen Zhou, Cicero Nogueira Dos
Santos, Caglar Gulcehre, and Bing Xiang. 2016b.

Abstractive text summarization using sequence-to-
sequence rnns and beyond.

Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018. Ranking sentences for extractive summariza-
tion with reinforcement learning.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
In Conference on Empirical Meth-
representation.
ods in Natural Language Processing, pages 1532–
1543.

Dragomir R Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda Celebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam,
Danyu Liu, et al. 2004. Mead-a platform for mul-
tidocument multilingual text summarization.
In
LREC.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. Computer Science.

Natalie Schluter. 2017. The limits of automatic sum-
In Proceedings of
marisation according to rouge.
the 15th Conference of the European Chapter of the
Association for Computational Linguistics: Volume
2, Short Papers, volume 2, pages 41–45.

Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. CoRR, abs/1704.04368.

Abhishek Kumar Singh, Manish Gupta, and Vasudeva
Varma. 2017. Hybrid memnet for extractive sum-
marization. pages 2303–2306.

Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,
and Rob Fergus. 2015. End-to-end memory net-
works. Computer Science.

Kristian Woodsend and Mirella Lapata. 2010. Auto-
In Meeting
matic generation of story highlights.
of the Association for Computational Linguistics,
pages 565–574.

Yuxiang Wu and Baotian Hu. 2018. Learning to extract
coherent summary via deep reinforcement learning.
arXiv preprint arXiv:1804.07036.

Caiming Xiong, Stephen Merity, and Richard Socher.
2016. Dynamic memory networks for visual and
textual question answering.

Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of the 2004 Conference on Empirical Methods in
Natural Language Processing.

Rui Yan. 2016. i, poet: Automatic poetry composition
through recurrent neural networks with iterative pol-
ishing schema. In IJCAI, pages 2238–2244.

Rui Yan, Cheng Te Li, Xiaohua Hu, and Ming Zhang.
2016. Chinese couplet generation with neural net-
work structures. In Meeting of the Association for
Computational Linguistics, pages 2347–2357.

Rui Yan, Jian Yun Nie, and Xiaoming Li. 2011a. Sum-
marize what you are interested in: An optimization
framework for interactive personalized summariza-
tion. In Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP 2011, 27-31 July
2011, John Mcintyre Conference Centre, Edinburgh,
Uk, A Meeting of Sigdat, A Special Interest Group of
the ACL, pages 1342–1351.

Rui Yan, Xiaojun Wan, Mirella Lapata, Wayne Xin
Zhao, Pu-Jen Cheng, and Xiaoming Li. 2012. Vi-
sualizing timelines: Evolutionary summarization
via iterative reinforcement between text and image
In Proceedings of the 21st ACM inter-
streams.
national conference on Information and knowledge
management, pages 275–284. ACM.

Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011b. Evolution-
ary timeline summarization: a balanced optimiza-
In Inter-
tion framework via iterative substitution.
national ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 745–
754.

Iterative Document Representation Learning Towards Summarization
with Polishing

Xiuying Chen1, Shen Gao2, Chongyang Tao2, Yan Song3, Dongyan Zhao1,2 and Rui Yan1,2,∗
1Center for Data Science, Peking University, Beijing, China
2Institute of Computer Science and Technology, Peking University, Beijing, China
3Tencent AI Lab
{xy-chen,shengao,chongyangtao,zhaody,ruiyan}@pku.edu.cn
clksong@tencent.com

Abstract

In this paper, we introduce Iterative Text Sum-
marization (ITS), an iteration-based model for
supervised extractive text summarization, in-
spired by the observation that it is often nec-
essary for a human to read an article multiple
times in order to fully understand and summa-
rize its contents. Current summarization ap-
proaches read through a document only once
to generate a document representation, result-
ing in a sub-optimal representation. To ad-
dress this issue we introduce a model which
iteratively polishes the document representa-
tion on many passes through the document. As
part of our model, we also introduce a selec-
tive reading mechanism that decides more ac-
curately the extent to which each sentence in
the model should be updated. Experimental
results on the CNN/DailyMail and DUC2002
datasets demonstrate that our model signiﬁ-
cantly outperforms state-of-the-art extractive
systems when evaluated by machines and by
humans.

1

Introduction

A summary is a shortened version of a text doc-
ument which maintains the most important ideas
from the original article. Automatic text summa-
rization is a process by which a machine gleans the
most important concepts from an article, removing
secondary or redundant concepts. Nowadays as
there is a growing need for storing and digesting
large amounts of textual data, automatic summa-
rization systems have signiﬁcant usage potential
in society.

Extractive summarization is a technique for
generating summaries by directly choosing a sub-
set of salient sentences from the original docu-
ment to constitute the summary. Most efforts
made towards extractive summarization either rely

∗Corresponding author: Rui Yan (ruiyan@pku.edu.cn)

on human-engineered features such as sentence
length, word position, and frequency (Cohen,
2002; Radev et al., 2004; Woodsend and Lapata,
2010; Yan et al., 2011a,b, 2012) or use neural
networks to automatically learn features for sen-
tence selection (Cheng and Lapata, 2016; Nallap-
ati et al., 2016a).

Although existing extractive summarization
methods have achieved great success, one limita-
tion they share is that they generate the summary
after only one pass through the document. How-
in real-world human cognitive processes,
ever,
people read a document multiple times in order
to capture the main ideas. Browsing through the
document only once often means the model cannot
fully get at the document’s main ideas, leading to
a subpar summarization. We share two examples
of this. (1) Consider the situation where we almost
ﬁnish reading a long article and forget some main
points in the beginning. We are likely to go back
and review the part that we forget. (2) To write
a good summary, we usually ﬁrst browse through
the document to obtain a general understanding of
the article, then perform a more intensive reading
to select salient points to include in the summary.
In terms of model design, we believe that letting
a model read through a document multiple times,
polishing and updating its internal representation
of the document can lead to better understanding
and better summarization.

To achieve this, we design a model that we call
Iterative Text Summarization (ITS) consisting of a
novel “iteration mechanism” and “selective read-
ITS is an iterative process, read-
ing module”.
ing through the document many times. There is
one encoder, one decoder, and one iterative unit in
each iteration. They work together to polish doc-
ument representation. The ﬁnal labeling part uses
outputs from all iterations to generate summaries.
The selective reading module we design is a modi-

9
1
0
2
 
y
a
M
 
0
3
 
 
]
L
C
.
s
c
[
 
 
2
v
4
2
3
0
1
.
9
0
8
1
:
v
i
X
r
a

ﬁed version of a Gated Recurrent Unit (GRU) net-
work, which can decide how much of the hidden
state of each sentence should be retained or up-
dated based on its relationship with the document.

Overall, our contribution includes:

1. We propose Iterative Text Summarization
(ITS), an iteration based summary generator
which uses a sequence classiﬁer to extract
salient sentences from documents.

2. We introduce a novel iterative neural net-
work model which repeatedly polishes the
distributed representation of document in-
stead of generating that once for all. Besides,
we propose a selective reading mechanism,
which decides how much information should
be updated of each sentence based on its re-
lationship with the polished document rep-
resentation. Our entire architecture can be
trained in an end-to-end fashion.

3. We evaluate our summarization model on
representative CNN/DailyMail corpora and
benchmark DUC2002 dataset. Experimen-
tal results demonstrate that our model out-
performs state-of-the-art extractive systems
when evaluated automatically and by human.

2 Related Work

Our research builds on previous works in two
ﬁelds: summarization and iterative modeling.

Text summarization can be classiﬁed into ex-
tractive summarization and abstractive summa-
rization. Extractive summarization aims to gener-
ate a summary by integrating the most salient sen-
tences in the document. Abstractive summariza-
tion aims to generate new content that concisely
paraphrases the document from scratch.

With the emergence of powerful neural net-
work models for text processing, a vast majority of
the literature on document summarization is ded-
icated to abstractive summarization. These mod-
els typically take the form of convolutional neu-
ral networks (CNN) or recurrent neural networks
(RNN). For example, Rush et al. (2015) propose
an encoder-decoder model which uses a local at-
tention mechanism to generate summaries. Nal-
lapati et al. (2016b) further develop this work by
addressing problems that had not been adequately
solved by the basic architecture, such as keyword
modeling and capturing the hierarchy of sentence-
to-word structures. In a follow-up work, Nallapati

et al. (2017) propose a new summarization model
which generates summaries by sampling a topic
one sentence at a time, then producing words us-
ing an RNN decoder conditioned on the sentence
topic. Another related work is by See et al. (2017),
where the authors use “pointing” and “coverage”
techniques to generate more accurate summaries.

Despite the focus on abstractive summarization,
extractive summarization remains an attractive
method as it is capable of generating more gram-
matically and semantically correct summaries.
This is the method we follow in this work. In ex-
tractive summarization, Cheng and Lapata (2016)
propose a general framework for single-document
text summarization using a hierarchical article en-
coder composed with an attention-based extractor.
Following this, Nallapati et al. (2016a) propose
a simple RNN-based sequence classiﬁer which
outperforms or matches the state-of-art models at
the time.
In another approach, Narayan et al.
(2018) use a reinforcement learning method to op-
timize the Rouge evaluation metric for text sum-
marization. The most recent work on this topic
is (Wu and Hu, 2018), where the authors train a
reinforced neural extractive summarization model
called RNES that captures cross-sentence coher-
ence patterns. Due to the fact that they use a dif-
ferent dataset and have not released their code, we
are unable to compare our models with theirs.

The idea of iteration has not been well explored
for summarization. One related study is Xiong
et al. (2016)’s work on dynamic memory net-
works, which designs neural networks with mem-
ory and attention mechanisms that exhibit certain
reasoning capabilities required for question an-
swering. Another related work is (Yan, 2016),
where they generate poetry with iterative polish-
ing sn chema. Similiar method can also be applied
on couplet generation as in (Yan et al., 2016). We
take some inspiration from their work but focus on
document summarization. Another related work is
(Singh et al., 2017), where the authors present a
deep network called Hybrid MemNet for the sin-
gle document summarization task, using a mem-
ory network as the document encoder. Compared
to them, we do not borrow the memory network
structure but propose a new iterative architecture.

3 Methodology

3.1 Problem Formulation

In this work, we propose Iterative Text Sum-
marization (ITS), an iteration-based supervised
model for extractive text summarization. We treat
the extractive summarization task as a sequence
labeling problem, in which each sentence is vis-
ited sequentially and a binary label that determines
whether or not it will be included in the ﬁnal sum-
mary is generated.

1, . . . , wi

ITS takes as input a list of sentences s =
{s1, . . . , sns}, where ns is the number of sen-
tences in the document. Each sentence si is a
list of words: si = {wi
nw }, where nw is
the word length of the sentence. The goal of ITS
is to generate a score vector y = {y1, . . . , yns}
for each sentence, where each score yi ∈ [0, 1]
denotes the sentence’s extracting probability, that
is, the probability that the corresponding sentence
si will be extracted to be included in the sum-
mary. We train our model in a supervised man-
ner, using a corresponding gold summary written
by human experts for each document in training
set. We use an unsupervised method to convert
the human-written summaries to gold label vec-
tor y(cid:48) = {y(cid:48)
ns}, where y(cid:48)
i ∈ {0, 1} denotes
whether the i-th sentence is selected (1) or not (0).
Next, during training process, the cross entropy
loss is calculated between y and y(cid:48), which is min-
imized to optimize y. Finally, we select three sen-
tences with the highest score according to y to be
the extracted summary. We detail our model be-
low.

1, ..., y(cid:48)

3.2 Model Architecture

ITS is depicted in Fig.1. It consists of multiple it-
erations with one encoder, one decoder, and one
iteration unit in each iteration. We combine the
outputs of decoders in all iterations to generate the
extracting probabilities in the ﬁnal labeling mod-
ule.

Our encoder is illustrated in the shaded region in
the left half of Fig.1. It takes as input all sentences
as well as the document representation from the
previous unit Dk−1, processes them through sev-
eral neural networks, and outputs the ﬁnal state to
the iterative unit module which updates the docu-
ment representation.

Our decoder takes the form of a bidirectional
RNN. It takes the representation of sentence gen-
erated by the encoder as input, and its initial state

is the polished document representation Dk. Our
last module, the sentence labeling module, con-
catenates the hidden states of all decoders together
to generate an integrated score for each sentence.
As we apply supervised training, the objective
is to maximize the likelihood of all sentence labels
y(cid:48) = {y(cid:48)
1, ..., y(cid:48)
ns} given the input document s and
model parameters θ:

log p(y(cid:48)|s; θ) =

log p(y(cid:48)

i|s; θ)

(1)

ns(cid:88)

i=1

4 Our Model

4.1 Encoder

In this subsection, we describe the encoding pro-
cess of our model. For brevity, we drop the super-
script k when focusing on a particular layer. All
the W ’s and b’s in this section with different su-
perscripts or subscripts are the parameters to be
learned.

Sentence Encoder: Given a discrete set of sen-
tences s = {s1, . . . , sns}, we use a word embed-
ding matrix M ∈ RV ×D to embed each word wi
in sentence si into continuous space ˆwi, where V
is the vocabulary size, D is the dimension of word
embedding.

The sentence encoder can be based on a variety
of encoding schemes. Simply taking the average
of embeddings of words in a sentence will cause
too much information loss, while using GRUs or
Long Short-Term Memory (LSTM) requires more
computational resources and is prone to overﬁt-
ting. Considering above, we select positional en-
coding described in (Sukhbaatar et al., 2015) as
our sentence encoding method. Each sentence rep-
resentation ˆsi is calculated by ˆsi = (cid:80)nw
j=1 lj ◦ ˆwi
j,
where ◦ is element-wise multiplication, lj is a col-
umn vector computed as lj,d = (1− j
D )(1−
nw
2j
nw

), lj,d denotes the d-th dimension of lj.
Note that throughout this study, we use GRUs
as our RNN cells since they can alleviate the over-
ﬁtting problem as conﬁrmed by our experiments.
As our selective reading mechanism (which will
be explained later) is a modiﬁed version of orig-
inal GRU cell, we give the details of the GRU
here. GRU is a gating mechanism in recurrent
neural networks, introduced in (Cho et al., 2014).
Their performance was found to be similar to that
of LSTM cell but using fewer parameters as de-
scribed in (Hochreiter and Schmidhuber, 1997).
The GRU cell consists of an update gate vector

)−( d

Figure 1: Model Structure: There is one encoder, one decoder and one iterative unit (which is used to polish
document representation) in each iteration. The ﬁnal labeling part is used to generating the extracting probabilities
for all sentences combining hidden states of decoders in all iterations. We take a document consists of three
sentences for example here.

ui, a reset gate vector ri, and an output vector
hi. For each time step i with input xi and pre-
vious hidden state hi−1, the updated hidden state
hi = GRU(xi, hi−1) is computed by:

ui = σ(W (u)xi + U (u)hi−1 + b(u))
ri = σ(W (r)xi + U (r)hi−1 + b(r))
(3)
˜hi = tanh(W (h)xi + ri ◦ U hi−1 + b(h)) (4)
hi = ui ◦ ˜hi + (1 − ui) ◦ hi−1
(5)

(2)

σ

the

is
sigmoid
W (u), W (r), W (h)

activation
where
∈
function,
RnH ×nI , U (u), U (r), U ∈ RnH ×nH , nH is
the hidden size, nI is the size of input xi.

To further study the interactions and informa-
tion exchanges between sentences, we establish a
Bi-directional GRU (Bi-GRU) network taking the
sentence representation as input:

−→si = GRUfwd(ˆsi, −−→si−1)
←−si = GRUbwd(ˆsi, ←−−si−1)
←→si = −→si + ←−si

(6)

(7)

(8)

where ˆsi is the sentence representation input at
time step i, −→si is the hidden state of the forward
GRU at time step i, and ←−si is the hidden state of
the backward GRU. This architecture allows in-
formation to ﬂow back and forth to generate new
sentence representation ←→si .

Document Encoder: We must initialize a doc-
ument representation before polishing it. Gener-
ating the document representation from sentence
representations is a process similar to generat-
ing the sentence representation from word embed-
dings. This time we need to compress the whole
document, not just a sentence, into a vector. Be-
cause the information a vector can contain is lim-
ited, rather than to use another neural network, we
simply use a non-linear transformation of the av-
erage pooling of the concatenated hidden states of
the above Bi-GRU to generate the document rep-
resentation, as written below:
ns(cid:88)

[−→si ; ←−si ] + b)

(9)

D0 = tanh(W

1
ns

i=1

where ‘[·;·]’ is the concatenation operation.

Selective Reading module: Now we can for-
mally introduce the selective reading module in
Fig.1. This module is a bidirectional RNN con-
sisting of modiﬁed GRU cells whose input is the
sentence representation ←→s = {←→s1 , ..., ←→sns}. In
the original version of GRU, the update gate ui
in Equation 2 is used to decide how much of hid-
den state should be retained and how much should
be updated. However, due to the way ui is calcu-
lated, it is sensitive to the position and ordering of
sentences, but loses information captured by the
polished document representation.

Herein, we propose a modiﬁed GRU cell that

replace the ui with the newly computed update
gate gi. The new cell takes in two inputs, the sen-
tence representation and the document representa-
tion from the last iteration, rather than merely the
sentence representation. For each sentence, the se-
lective network generates an update gate vector gi
in the following way:

fi = [←→si ◦ Dk−1; ←→si ; Dk−1]
Fi = W (2)tanh(W (1)fi + b(1)) + b(2)

(10)

(11)

(12)

gi =

exp(Fi)
j=1 exp(Fj)

(cid:80)ns

where ←→si
is the i-th sentence representation,
Dk−1 is the document representation from last it-
eration. Equation 5 now becomes:

hi = gi ◦ ˜hi + (1 − gi) ◦ hi−1

(13)

We use this “selective reading module” to auto-
matically decide to which extent the information
of each sentence should be updated based on its
relationship with the polished document. In this
way, the modiﬁed GRU network can grasp more
accurate information from the document.

4.2

Iterative Unit

After each sentence passes through the selective
reading module, we wish to update the document
representation Dk−1 with the newly constructed
sentence representations. The iterative unit (also
depicted above in Fig.1) is designed for this pur-
pose. We use a GRUiter cell to generate the pol-
ished document representation, whose input is the
ﬁnal state of the selective reading network from
the previous iteration, hns and whose initial state
is set to the document representation of the previ-
ous iteration, Dk−1. The updated document rep-
resentation is computed by:

Dk = GRUiter(hns, Dk−1)

(14)

4.3 Decoder

Next, we describe our decoders, which are de-
picted shaded in the right part of Fig.1. Follow-
ing most sequence labeling task (Xue and Palmer,
2004; Carreras and M`arquez, 2005) where they
learn a feature vector for each sentence, we use a
bidirectional GRUdec network in each iteration to
output features so as to calculate extracting proba-
bilities. For k-th iteration, given the sentence rep-
resentation ←→s as input and the document repre-
sentation Dk as the initial state, our decoder en-
codes the features of all sentences in the hidden

state hk = {hk

0, ..., hk
ns}:
i = GRUdec(←→s , hk
hk
hk
0 = Dk
4.4 Sentence Labeling Module

i−1)

(15)

(16)

Next, we use the feature of each sentence to gener-
ate corresponding extracting probability. Since we
have one decoder in each iteration, if we directly
transform the hidden states in each iteration to ex-
tracting probabilities, we will end up with several
scores for each sentence. Either taking the aver-
age or summing them together by speciﬁc weights
is inappropriate and inelegant. Hence, we concate-
nate hidden states of all decoders together and ap-
ply a multi-layer perceptron to them to generate
the extracting probabilities:
y = W (4)tanh(W (3)[h1; ...; hk] + b(3)) + b(4)
(17)
where y = {y1, ..., yns}, yi is the extracting prob-
ability for each setence.
In this way, we let the
model learn by itself how to utilize the outputs of
all iterations and assign to each hidden state a re-
liable weight. In section 6, we will show that this
labeling method outperforms other methods.

5 Experiment Setup

In this section, we present our experimental setup
for training and estimating our summarization
model. We ﬁrst introduce the datasets used for
training and evaluation, and then introduce our ex-
perimental details and evaluation protocol.

5.1 Datasets

In order to make a fair comparison with our base-
lines, we used the CNN/Dailymail corpus which
was constructed by Hermann et al. (2015). We
used the standard splits for training, validation and
testing in each corpus (90,266/1,220/1,093 doc-
uments for CNN and 196,557/12,147/10,396 for
DailyMail). We followed previous studies in us-
ing the human-written story highlight in each arti-
cle as a gold-standard abstractive summary. These
highlights were used to generate gold labels when
training and testing our model using the greedy
search method similar to (Nallapati et al., 2016a).
We also tested ITS on an out-of-domain cor-
pus, DUC2002, which consists of 567 documents.
Documents in this corpus belong to 59 various
clusters and each cluster has a unique topic. Each
document has two gold summaries written by hu-
man experts of length around 100 words.

5.2

Implementation Details

We implemented our model in Tensorﬂow (Abadi
et al., 2016). The code for our models is avail-
able online1. We mostly followed the settings in
(Nallapati et al., 2016a) and trained the model us-
ing the Adam optimizer (Kingma and Ba, 2014)
with initial learning rate 0.001 and anneals of 0.5
every 6 epochs until reaching 30 epochs. We se-
lected three sentences with highest scores as sum-
mary. After preliminary exploration, we found
that arranging them according to their scores con-
Ex-
sistently achieved the best performance.
periments were performed with a batch size of
64 documents. We used 100-dimension GloVe
(Pennington et al., 2014) embeddings trained on
Wikipedia 2014 as our embedding initialization
with a vocabulary size limited to 100k for speed
purposes. We initialized out-of-vocabulary word
embeddings over a uniform distribution within [-
0.2,0,2]. We also padded or cut sentences to con-
tain exactly 70 words. Each GRU module had 1
layer with 200-dimensional hidden states and with
either an initial state set up as described above or
a random initial state. To prevent overﬁtting, we
used dropout after each GRU network and embed-
ding layer, and also applied L2 loss to all unbi-
ased variables. The iteration number was set to 5
if not speciﬁed. A detailed discussion about itera-
tion number can be found in section 7.

5.3 Baselines

On all datasets we used the Lead-3 method as a
baseline, which simply chooses the ﬁrst three sen-
tences in a document as the gold summary. On
DailyMail datasets, we report the performance of
SummaRuNNer in (Nallapati et al., 2016a) and
the model in (Cheng and Lapata, 2016), as well
as a logistic regression classiﬁer (LReg) that they
used as a baseline. We reimplemented the Hy-
brid MemNet model in (Singh et al., 2017) as one
of our baselines since they only reported the per-
formance of 500 samples in their paper. Also,
Narayan et al. (2018) released their code2 for the
REFRESH model, we used their code to produce
Rouge recall scores on the DailyMail dataset as
they only reported results on CNN/DailyMail joint
dataset. Baselines on CNN dataset are similar.

1https://github.com/yingtaomj/Iterati
ve-Document-Representation-Learning-Tow
ards-Summarization-with-Polishing

2https://github.com/EdinburghNLP/Refr

esh

On DUC2002 corpus, we compare our model with
several baselines such as Integer Linear Program-
ming (ILR) and LReg. We also report the perfor-
mance of the newest neural networks model in-
cluding (Nallapati et al., 2016a; Cheng and Lap-
ata, 2016; Singh et al., 2017).

5.4 Evaluation

In the evaluation procedure, we used the Rouge
scores, i.e. Rouge-1, Rouge-2, and Rouge-L, cor-
responding to the matches of unigram, bigrams,
and Longest Common Subsequence (LCS) respec-
tively, to estimate our model. We obtained our
Rouge scores using the standard pyrouge pack-
age3. To compare with other related works, we
used full-length F1 score on the CNN corpus, lim-
ited length of 75 bytes and 275 bytes recall score
on DailyMail corpus. As for the DUC2002 corpus,
following the ofﬁcial guidelines, we examined the
Rouge recall score at the length of 75 words. All
results in our experiment are statistically signiﬁ-
cant using 95% conﬁdence interval as estimated
by Rouge script.

Schluter (2017) noted that only using the Rouge
metric to evaluate summarization quality can be
misleading. Therefore, we also evaluated our
model using human evaluation. Five highly ed-
ucated participants were asked to rank 40 sum-
maries produced by four models: the Lead-3 base-
line, Hybrid MemNet, ITS, and human-authored
highlights. We chose Hybrid MemNet as one of
the human evaluation baselines since its perfor-
mance is relatively high compared to other base-
lines.
Judging criteria included informativeness
and coherence. Test cases were randomly sampled
from DailyMail test set.

6 Experiment analysis

Table 1 shows the performance comparison of
our model with other baselines on the DailyMail
dataset with respect to Rouge score at 75 bytes
and 275 bytes of summary length. Our model
performs consistently and signiﬁcantly better than
other models on 75 bytes, while on 275 bytes, the
improvement margin is smaller. One possible in-
terpretation is that our model has high precision
on top rank outputs, but the accuracy is lower for
lower rank sentences.
In addition, (Cheng and
Lapata, 2016) used additional supervised training

3https://pypi.python.org/pypi/pyrouge

/0.1.0

Rouge-1 Rouge-2 Rouge-L Rouge-1 Rouge-2 Rouge-L

DailyMail

Lead-3
LReg(500)
Cheng et.al’16
SummaRuNNer
REFRESH
Hybrid MemNet
ITS

21.9
18.5
22.7
26.2
24.1
26.3
27.4

b75

7.2
6.9
8.5
10.8
11.5
11.2
11.9

11.6
10.2
12.5
14.4
12.5
15.5
16.1

40.5
-
42.2
42
40.3
41.4
42.4

b275

14.9
-
17.3
16.9
15.1
16.7
17.4

32.6
-
34.8
34.1
32.9
33.2
34.1

Table 1: Comparison with other baselines on DailyMail test dataset using Rouge recall score with respect to the
abstractive ground truth at 75 bytes and at 275 bytes.

CNN
Lead-3
Cheng et.al’16
Hybrid MemNet
REFRESH
ITS

Rouge-1 Rouge-2 Rouge-L
11.1
10.0
11.3
11.7
12.6

29.1
28.4
29.9
30.4
30.8

25.9
25.0
26.4
26.9
27.6

Table 2: Comparison with other baselines on CNN test
dataset using full-length F1 variants of Rouge.

to create sentence-level extractive labels to train
their model, while our model uses an unsupervised
greedy approximation instead.

We also examined the performance of our
model on CNN dataset as listed in Table 2. To
compare with other models, we used full-length
Rouge F1 metric as reported by Narayan et al.
(2018).
Results demonstrate that our model
has a consistently best performance on different
datasets.

In Table 3, we present the performance of ITS
on the out of domain DUC dataset. Our model out-
performs or matches other basic models including
LReg and ILR as well as neural network baselines
such as SummaRuNNer with respect to the ground
truth at 75 bytes, which shows that our model can
be adapted to different copora maintaining high
accuracy.

In order to explore the impact of internal struc-
ture of ITS, we also conducted an ablation study
in Table 4. The ﬁrst variation is the same model
without the selective reading module. The sec-
ond one sets the iteration number to one, that is, a
model without iteration process. The last variation
is to apply MLP on the output from the last itera-
tion instead of concatenating the hidden states of
all decoders. All other settings and parameters are
the same. Performances of these models are worse
than that of ITS in all metrics, which demonstrates

DUC2002
Lead-3
LReg
ILP
Cheng et.al’16
SummaRuNNer
Hybrid MemNet
ITS

Rouge-1 Rouge-2 Rouge-L
21.0
20.7
21.3
23.0
23.1
23.0
23.4

40.2
40.3
42.8
43.5
43.0
43.1
43.5

43.6
43.8
45.4
47.4
46.6
46.9
47.6

Table 3:
Comparison with other baselines on
DUC2002 dataset using Rouge recall score with re-
spect to the abstractive ground truth at 75 bytes.

Variations
ITS
27.4
w/o selective reading 27.1
26.9
w/o iteration
27.2
w/o concatenation

Rouge-1Rouge-2Rouge-L
11.9
11.6
11.6
11.7

16.1
15.4
15.8
15.9

Table 4: Ablation study on DailyMail test dataset with
respect to the abstractive ground truth at 75 bytes.

the preeminence of ITS. More importantly, by this
controlled experiment, we can verify the contribu-
tion of different module in ITS.

7 Further discussion

Analysis of iteration number: We did a broad
sweep of experiments to further investigate the in-
ﬂuence of iteration process on the generated sum-
mary quality. First, we studied the inﬂuence of
iteration number. In order to make a fair compar-
ison between models with different iteration num-
ber, we trained all models for same epochs without
tuning. Fig.2 illustrates the relationship between
iteration number and the Rouge score at 75 bytes
of summary length on DailyMail test dataset. The
result shows that the Rouge score increases with
the number of iteration to begin with. After reach-
ing the upper limit it begins to drop. Note that

Models
Lead-3
Hybrid MemNet
ITS
Gold

1st
0.12
0.24
0.31
0.33

2nd
0.11
0.25
0.34
0.30

3rd
0.25
0.28
0.23
0.24

4th
0.52
0.23
0.12
0.13

Table 5: System ranking comparison with other base-
lines on DailyMail corpus. Rank 1 is the best and Rank
4 is the worst. Each score represents the percentage of
the summary under this rank.

sitivity between iterations as shown in Fig.3(b). To
be speciﬁc, the sentences which are not preferred
by iteration 3 remain low probabilities in the next
two iterations, while sentences with relatively high
scores are still preferred by iteration 4 and 5.

Human Evaluation: We gave human evalua-
tors three system-generated summaries, generated
by Lead-3, Hybrid MemNet, ITS, as well as the
human-written gold standard summary, and asked
them to rank these summaries based on summary
informativeness and coherence. Table 5 shows the
percentages of summaries of different models un-
der each rank scored by human experts. It is not
surprising that gold standard has the most sum-
maries of the highest quality. Our model has the
most summaries under 2nd rank, thus can be con-
sidered 2nd best, following are Hybrid MemNet
and Lead-3, as they are ranked mostly 3rd and 4th.
By case study, we found that a number of sum-
maries generated by Hybrid MemNet have two
sentences the same as ITS out of three, however,
the third distinct sentence from our model always
leads to a better evaluation result considering over-
all informativeness and coherence. Readers can
refer to the appendix to see our case study.

8 Conclusion

In this work, we introduce ITS, an iteration based
extractive summarization model, inspired by the
observation that it is often necessary for a hu-
man to read the article multiple times to fully un-
derstand and summarize it. Experimental results
on CNN/DailyMail and DUC corpora demonstrate
the effectiveness of our model.

Acknowledgments

We would like to thank the anonymous review-
ers for their constructive comments. We would
also like to thank Jin-ge Yao and Zhengyuan Ma
for their valuable advice on this project. This
work was supported by the National Key Re-

Figure 2: Relationship between number of iteration
and Rouge score on DailyMail test dataset with respect
to the ground truth at 75 bytes.

(a)

(b)

Figure 3: The predicted extracting probabilities for
each sentence calculated by the output of each iteration.

the result of training the model for only one epoch
outperforms the state-of-the-art in (Singh et al.,
2017), which demonstrates that our selective read-
ing module is effective. The fact that continu-
ing this process increase the performance conﬁrms
that the iteration idea behind our model is useful in
practice. Based on above observation, we set the
default iteration number to be 5.

Analysis of polishing process: Next, to fully
investigate how the iterative process inﬂuences the
extracting results, we draw heatmaps of the ex-
tracting probabilities for each decoder at each it-
eration. We pick two representative cases in Fig.3,
where the x-axis represents the sentence index and
y-axis is the iteration number, x-axis labels are
omitted. The darker the color is, the higher the
In Fig.3(a), it can be
extracting probability is.
seen that when the iteration begins, most sentences
have similar probabilities. As we increase the
number of iteration, some probabilities begin to
fall and others saturate. This means that the model
already has preferred sentences to select. Another
interesting feature we found is that there is a tran-

search and Development Program of China (No.
2017YFC0804001), the National Science Foun-
dation of China (NSFC No.
61876196, No.
61672058). Rui Yan was sponsored by CCF-
Tencent Open Research Fund and Microsoft Re-
search Asia (MSRA) Collaborative Research Pro-
gram.

References

Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng
Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard,
et al. 2016. Tensorﬂow: A system for large-scale
machine learning. In OSDI, volume 16, pages 265–
283.

Xavier Carreras and Llu´ıs M`arquez. 2005.

Intro-
duction to the conll-2005 shared task: Semantic
role labeling. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learn-
ing, CONLL ’05, pages 152–164, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Jianpeng Cheng and Mirella Lapata. 2016. Neural
summarization by extracting sentences and words.

Kyunghyun Cho, Bart Van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014.
Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. Computer Sci-
ence.

Kevin Bretonnel Cohen. 2002. Natural language pro-
cessing for online applications: Text retrieval, ex-
Language,
traction and categorization (review).
80(3):510–511.

Karl Moritz Hermann, Edward Grefenstette, Lasse Es-
peholt, Will Kay, Mustafa Suleyman, and Phil Blun-
som. 2015. Teaching machines to read and compre-
hend. pages 1693–1701.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. Computer Sci-
ence.

Ramesh Nallapati, Igor Melnyk, Abhishek Kumar, and
Bowen Zhou. 2017. Sengen: Sentence generating
neural variational topic model.

Ramesh Nallapati, Feifei Zhai, and Bowen Zhou.
2016a. Summarunner: A recurrent neural network
based sequence model for extractive summarization
of documents.

Ramesh Nallapati, Bowen Zhou, Cicero Nogueira Dos
Santos, Caglar Gulcehre, and Bing Xiang. 2016b.

Abstractive text summarization using sequence-to-
sequence rnns and beyond.

Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018. Ranking sentences for extractive summariza-
tion with reinforcement learning.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
In Conference on Empirical Meth-
representation.
ods in Natural Language Processing, pages 1532–
1543.

Dragomir R Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda Celebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam,
Danyu Liu, et al. 2004. Mead-a platform for mul-
tidocument multilingual text summarization.
In
LREC.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. Computer Science.

Natalie Schluter. 2017. The limits of automatic sum-
In Proceedings of
marisation according to rouge.
the 15th Conference of the European Chapter of the
Association for Computational Linguistics: Volume
2, Short Papers, volume 2, pages 41–45.

Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. CoRR, abs/1704.04368.

Abhishek Kumar Singh, Manish Gupta, and Vasudeva
Varma. 2017. Hybrid memnet for extractive sum-
marization. pages 2303–2306.

Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,
and Rob Fergus. 2015. End-to-end memory net-
works. Computer Science.

Kristian Woodsend and Mirella Lapata. 2010. Auto-
In Meeting
matic generation of story highlights.
of the Association for Computational Linguistics,
pages 565–574.

Yuxiang Wu and Baotian Hu. 2018. Learning to extract
coherent summary via deep reinforcement learning.
arXiv preprint arXiv:1804.07036.

Caiming Xiong, Stephen Merity, and Richard Socher.
2016. Dynamic memory networks for visual and
textual question answering.

Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of the 2004 Conference on Empirical Methods in
Natural Language Processing.

Rui Yan. 2016. i, poet: Automatic poetry composition
through recurrent neural networks with iterative pol-
ishing schema. In IJCAI, pages 2238–2244.

Rui Yan, Cheng Te Li, Xiaohua Hu, and Ming Zhang.
2016. Chinese couplet generation with neural net-
work structures. In Meeting of the Association for
Computational Linguistics, pages 2347–2357.

Rui Yan, Jian Yun Nie, and Xiaoming Li. 2011a. Sum-
marize what you are interested in: An optimization
framework for interactive personalized summariza-
tion. In Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP 2011, 27-31 July
2011, John Mcintyre Conference Centre, Edinburgh,
Uk, A Meeting of Sigdat, A Special Interest Group of
the ACL, pages 1342–1351.

Rui Yan, Xiaojun Wan, Mirella Lapata, Wayne Xin
Zhao, Pu-Jen Cheng, and Xiaoming Li. 2012. Vi-
sualizing timelines: Evolutionary summarization
via iterative reinforcement between text and image
In Proceedings of the 21st ACM inter-
streams.
national conference on Information and knowledge
management, pages 275–284. ACM.

Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011b. Evolution-
ary timeline summarization: a balanced optimiza-
In Inter-
tion framework via iterative substitution.
national ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 745–
754.

