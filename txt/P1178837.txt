Cross-lingual Dependency Parsing with Unlabeled Auxiliary Languages

Wasi Uddin Ahmad1, Zhisong Zhang2, Xuezhe Ma2
wasiahmad@cs.ucla.edu,{zhisongz,xuezhem}@cs.cmu.edu
Kai-Wei Chang1, Nanyun Peng3
kwchang@cs.ucla.edu,npeng@isi.edu

1University of California, Los Angeles, 2Carnegie Mellon University
3University of Southern California

Abstract

Cross-lingual transfer learning has become an
important weapon to battle the unavailabil-
ity of annotated resources for low-resource
languages. One of the fundamental
tech-
niques to transfer across languages is learn-
ing language-agnostic representations, in the
form of word embeddings or contextual en-
codings.
In this work, we propose to lever-
age unannotated sentences from auxiliary lan-
guages to help learning language-agnostic rep-
resentations. Speciﬁcally, we explore adver-
sarial training for learning contextual encoders
that produce invariant representations across
languages to facilitate cross-lingual transfer.
We conduct experiments on cross-lingual de-
pendency parsing where we train a depen-
dency parser on a source language and trans-
fer it to a wide range of target languages. Ex-
periments on 28 target languages demonstrate
that adversarial training signiﬁcantly improves
the overall transfer performances under several
different settings. We conduct a careful analy-
sis to evaluate the language-agnostic represen-
tations resulted from adversarial training.

1

Introduction

transfer, where a model

learned
Cross-lingual
from one language is transferred to another, has
become an important technique to improve the
quality and coverage of natural language process-
ing (NLP) tools for languages in the world. This
technique has been widely applied in many ap-
plications, including part-of-speech (POS) tagging
(Kim et al., 2017), dependency parsing (Ma and
Xia, 2014), named entity recognition (Xie et al.,
2018), entity linking (Sil et al., 2018), coreference
resolution (Kundu et al., 2018), and question an-
swering (Joty et al., 2017). Noteworthy improve-
ments are achieved on low resource language ap-
plications due to cross-lingual transfer learning.

In this paper, we study cross-lingual transfer for
dependency parsing. A dependency parser con-
sists of (1) an encoder that transforms an input text
sequence into latent representations and (2) a de-
coding algorithm that generates the corresponding
parse tree. In cross-lingual transfer, most recent
approaches assume that the inputs from different
languages are aligned into the same embedding
space via multilingual word embeddings or multi-
lingual contextualized word vectors, such that the
parser trained on a source language can be trans-
ferred to target languages. However, when train-
ing a parser on the source language, the encoder
not only learns to embed a sentence but it also car-
ries language-speciﬁc properties, such as word or-
der typology. Therefore, the parser suffers when it
is transferred to a language with different language
properties. Motivated by this, we study how to
train an encoder for generating language-agnostic
representations that can be transferred across a
wide variety of languages.

We propose to utilize unlabeled sentences of
one or more auxiliary languages to train an en-
coder that
learns language-agnostic contextual
representations of sentences to facilitate cross-
lingual transfer. To utilize the unlabeled auxil-
iary language corpora, we adopt adversarial train-
ing (Goodfellow et al., 2014) of the encoder and
a classiﬁer that predicts the language identity of
an input sentence from its encoded representation
produced by the encoder. The adversarial training
encourages the encoder to produce language in-
variant representations such that the language clas-
siﬁer fails to predict the correct language identity.
As the encoder is jointly trained with a loss for the
primary task on the source language and adversar-
ial loss on all languages, we hypothesize that it
will learn to capture task-speciﬁc features as well
as generic structural patterns applicable to many
languages, and thus have better transferrability.

9
1
0
2
 
p
e
S
 
0
2
 
 
]
L
C
.
s
c
[
 
 
1
v
5
6
2
9
0
.
9
0
9
1
:
v
i
X
r
a

To verify the proposed approach, we conduct
experiments on neural dependency parsers trained
on English (source language) and directly trans-
fer them to 28 target languages, with or without
the assistance of unlabeled data from auxiliary lan-
guages. We chose dependency parsing as the pri-
mary task since it is one of the core NLP appli-
cations and the development of Universal Depen-
dencies (Nivre et al., 2016) provides consistent an-
notations across languages, allowing us to investi-
gate transfer learning in a wide range of languages.
Thorough experiments and analyses are conducted
to address the following research questions:

• Does encoder trained with adversarial train-
ing generate language-agnostic representa-
tions?

• Does language-agnostic representations im-

prove cross-language transfer?

Experimental results show that the proposed ap-
proach consistently outperform a strong baseline
parser (Ahmad et al., 2019), with a signiﬁcant
margin in two family of languages.
In addition,
we conduct experiments to consolidate our ﬁnd-
ings with different types of input representations
and encoders. Our experiment code is publicly
available to facilitate future research.1

2 Training Language-agnostic Encoders

We train the encoder of a dependency parser in
an adversarial fashion to guide it to avoid captur-
ing language-speciﬁc information.
In particular,
we introduce a language identiﬁcation task where
a classiﬁer predicts the language identity (id) of
an input sentence from its encoded representation.
Then the encoder is trained such that the classiﬁer
fails to predict the language id while the parser de-
coder predicts the parse tree accurately from the
encoded representation. We hypothesize that such
an encoder would have better cross-lingual trans-
ferability. The overall architecture of our model is
illustrated in Figure 1. In the following, we present
the details of the model and training method.

2.1 Architecture

Our model consists of three basic components, (1)
a general encoder, (2) a decoder for parsing, and
(3) a classiﬁer for language identiﬁcation. The en-
coder learns to generate contextualized represen-
tations for the input sentence (a word sequence)

1https://github.com/wasiahmad/cross lingual parsing

Figure 1: An overview of our experimental model con-
sists of three basic components: (1) Encoder, (2) (Pars-
ing) Decoder, and (3) (Language) Classiﬁer. We also
show how parsing and adversarial losses (Lp and Ld)
are back propagated for parameter updates.

which are fed to the decoder and the classiﬁer to
predict the dependency structure and the language
identity (id) of that sentence.

The encoder and the decoder jointly form the
parsing model and we consider two alternatives2
from (Ahmad et al., 2019): “SelfAtt-Graph” and
“RNN-Stack”. The “SelfAtt-Graph” parser con-
sists of a modiﬁed self-attentional encoder (Shaw
et al., 2018) and a graph-based deep bi-afﬁne
decoder (Dozat and Manning, 2017), while the
“RNN-Stack” parser is composed of a Recur-
rent Neural Network (RNN) based encoder and a
stack-pointer decoder (Ma et al., 2018).

We stack a classiﬁer (a linear classiﬁer or a
multi-layer Perceptron (MLP)) on top of the en-
coder to perform the language identiﬁcation task.
The identiﬁcation task can be framed as either a
word- or sentence-level classiﬁcation task. For
the sentence-level classiﬁcation, we apply average
pooling3 on the contextual word representations
generated by the encoder to form a ﬁxed-length
representation of the input sequence, which is fed
to the classiﬁer. For the word-level classiﬁcation,
we perform language classiﬁcation for each token
individually.

2Ahmad et al. (2019) studied order-sensitive and order-free
models and their performances in cross-lingual transfer. In
this work, we adopt two typical ones and study the effects of
adversarial training on them.
3We also experimented with max-pooling and weighted pool-
ing but average pooling resulted in stable performance.

Algorithm 1 Training procedure.
Parameters to be trained: Encoder (θg), Decoder
(θp), and Classiﬁer (θd)
X a = Annotated source language data
X b = Unlabeled auxiliary language data
I = Number of warm-up iterations
k = Number of learning steps for the discriminator
(D) at each iteration
λ = Coefﬁcient of Ld
α1, α1 = learning rate; B = Batch size
Require:

1: for j = 0, · · · , I do
2:

Update θg := θg − α1∇θg Lp
Update θp := θp − α1∇θpLp

4: for j = I, · · · , num iter do
5:

for k steps do

a)B/2
i=1 ← Sample a batch from X a
b)B/2
i=1 ← Sample a batch from X b

(xi
(xi
Update θd := θd − α2∇θdLd

Total loss L := Lp − λLd
Update θg := θg − α1∇θg L
Update θp := θp − α1∇θpL

3:

6:

7:

8:

9:

10:

11:

In this work, following the terminology in ad-
versarial learning literature, we interchangeably
call the encoder as the generator, G and the classi-
ﬁer as the discriminator, D.

2.2 Training

Algorithm 1 describes the training procedure. We
have two types of loss functions: Lp for the pars-
ing task and Ld for the language identiﬁcation
task. For the former, we update the encoder and
the decoder as in the regular training of a parser.
For the latter, we adopt adversarial training to up-
date the encoder and the classiﬁer. We present the
detailed training schemes in the following.

2.2.1 Parsing

To train the parser, we adopt both cross-entropy
objectives for these two types of parsers as in
(Dozat and Manning, 2017; Ma et al., 2018). The
encoder and the decoder are jointly trained to op-
timize the probability of the dependency trees (y)
given sentences (x):

Lp = − log p(y|x).

The probability of a tree can be further factor-
ized into the products of the probabilities of each
token’s (m) head decision (h(m)) for the graph-

based parser, or the probabilities of each transition
step decision (ti) for the transition-based parser:

Graph: Lp = −

Transition: Lp = −

(cid:88)

(cid:88)
i

log p(h(m)|x, m),

m
log p(ti|x, t<i).

2.2.2 Language Identiﬁcation
Our objective is to train the contextual encoder
in a dependency parsing model such that it en-
codes language speciﬁc features as little as pos-
sible, which may help cross-lingual transfer. To
achieve our goal, we utilize adversarial training by
employing unlabeled auxiliary language corpora.

Setup We adopt the basic generative adversarial
network (GAN) for the adversarial training. We
assume that X a and X b be the corpora of the
source and auxiliary language sentences, respec-
tively. The discriminator acts as a binary classiﬁer
and is adopted to distinguish the source and auxil-
iary languages. For the training of the discrimina-
tor, weights are updated according to the original
classiﬁcation loss:

E

Ld = Ex∼X a[log D(G(x)]+
x∼X b[log (1 − D(G(x))].
For the training of dependency parsing, the gen-
erator, G collaborates with the parser but acts as an
adversary with respect to the discriminator. There-
fore, the generator weights (θg) are updated by
minimizing the loss function,

L = Lp − λLd,

where λ is used to scale the discriminator loss
(Ld). In this way, the generator is guided to build
language-agnostic representations in order to fool
the discriminator while being helpful for the pars-
ing task. Meanwhile, the parser can be guided to
rely more on the language-agnostic features.

Alternatives We also consider two alternative
techniques for the adversarial training: Gradient
Reversal (GR) (Ganin et al., 2016) and Wasser-
stein GAN (WGAN) (Arjovsky et al., 2017). As
opposed to GAN based training, in GR setup, the
discriminator acts as a multiclass classiﬁer that
predicts language identity of the input sentence,
and we use multi-class cross-entropy loss. We also
study Wasserstein GAN (WGAN), which is pro-
posed by Arjovsky et al. (2017) to improve the sta-
bility of GAN based learning. Its loss function is
shown as follows.

Ld = Ex∼X a[D(G(x)] − E

x∼X b[D(G(x)],

Language
Families
Afro-Asiatic
Austronesian
IE.Baltic
IE.Germanic

IE.Indic
IE.Latin
IE.Romance

IE.Slavic

Korean
Uralic

Languages

Arabic (ar), Hebrew (he)
Indonesian (id)
Latvian (lv)
Danish (da), Dutch (nl), English (en),
German (de), Norwegian (no),
Swedish (sv)
Hindi (hi)
Latin (la)
Catalan (ca), French (fr), Italian (it),
Portuguese (pt), Romanian (ro),
Spanish (es)
Bulgarian (bg), Croatian (hr), Czech
(cs), Polish (pl), Russian (ru), Slovak
(sk), Slovenian (sl), Ukrainian (uk)
Korean (ko)
Estonian (et), Finnish (ﬁ)

Table 1: The selected 29 languages for experiments
from UD v2.2 (Nivre et al., 2018).

here, the annotations are similar to those in the
GAN setting.

3 Experiments and Analysis

In this section, we discuss our experiments
and analysis on cross-lingual dependency parsing
transfer from a variety of perspectives and show
the advantages of adversarial training.

Settings.
In our experiments, we study single-
source parsing transfer, where a parsing model is
trained on one source language and directly ap-
plied to the target languages. We conduct experi-
ments on the Universal Dependencies (UD) Tree-
banks (v2.2) (Nivre et al., 2018) using 29 lan-
guages, as shown in Table 1. We use the publicly
available implementation4 of the “SelfAtt-Graph”
and “RNN-Stack” parsers.5 Ahmad et al. (2019)
show that the “SelfAtt-Graph” parser captures less
language-speciﬁc information and performs bet-
ter than the ‘RNN-Stack” parser for distant tar-
get languages. Therefore, we use the “SelfAtt-
Graph” parser in most of our experiments. Be-
sides, the multilingual variant of BERT (mBERT)
(Devlin et al., 2019) has shown to perform well
in cross-lingual tasks (Wu and Dredze, 2019) and
outperform the models trained on multilingual
word embeddings by a large margin. Therefore,
we consider conducting experiments with both
multilingual word embeddings and mBERT. We
use aligned multilingual word embeddings (Smith

4https://github.com/uclanlp/CrossLingualDepParser
5We adopt the same hyper-parameters, experiment settings
and evaluation metrics as those in (Ahmad et al., 2019).

et al., 2017; Bojanowski et al., 2017) with 300 di-
mensionss or contextualized word representations
provided by multilingual BERT6 (Devlin et al.,
2019) with 768 dimensions as the word represen-
In addition, we use the Gold universal
tations.
POS tags to form the input representations.7 We
freeze the word representations during training to
avoid the risk of disarranging the multilingual rep-
resentation alignments.

We select six auxiliary languages8 (French, Por-
tuguese, Spanish, Russian, German, and Latin) for
unsupervised language adaptation via adversarial
training. We tune the scaling parameter λ in the
range of [0.1, 0.01, 0.001] on the source language
validation set and report the test performance with
the best value. For gradient reversal (GR) and
GAN based adversarial objectives, we use Adam
(Kingma and Ba, 2015) to optimize the discrim-
inator parameters, and for WGAN, we use RM-
SProp (Tieleman and Hinton, 2012). The learning
rate is set to 0.001 and 0.00005 for Adam and RM-
SProp, respectively. We train the parsing models
for 400 and 500 epochs with multilingual BERT
and multilingual word embeddings respectively.
We tune the parameter I (as shown in Algorithm
1) in the range of [50, 100, 150].

Language Test. The goal of training the con-
textual encoder adversarially with unlabeled data
from auxiliary languages is to encourage the en-
coder to capture more language-agnostic represen-
tations and less language-dependent features. To
test whether the contextual encoders retain lan-
guage information after adversarial training, we
train a multi-layer Perceptron (MLP) with softmax
on top of the ﬁxed contextual encoders to perform
a 7-way classiﬁcation task.9
If a contextual en-
coder performs better in the language test, it in-
dicates that the encoder retains language speciﬁc
information.

3.1 Results and Analysis

Table 2 presents the main transfer results of the
“SelfAtt-Graph” parser when training on only En-
glish (en, baseline), English with French (en-
fr), and English with Russian (en-ru). The re-

6https://github.com/huggingface/pytorch-transformers
7We concatenate the word and POS representations. In our
future work, we will conduct transfer learning for both POS
tagging and dependency parsing.
8We want to cover languages from different families and with
varying distances from the source language (English).
9With the source (English) and six auxiliary languages.

Lang

en
no
sv
fr
pt
da
es
it
hr
ca
pl
uk
sl
nl
bg
ru
de
he
cs
ro
sk
id
lv
ﬁ
et
ar
la
ko
hi
Average

Multilingual Word Embeddings
(en-fr)
(en)
90.23/88.23
90.01/88.08
80.60/72.83
80.82/72.94
80.33/72.54
79.90/72.16
77.71/72.35 78.49†/73.30†
76.88†/67.74
76.41/67.35
76.58/68.11
75.99/67.64
74.14/65.78
73.76/65.46
80.89/75.61 81.33†/76.14†
62.21/52.67 63.38†/53.83†
73.46†/64.71
73.18/64.53
74.65/62.72 75.65†/63.31†
59.25/51.92 60.58†/52.72†
68.14/56.52
67.51/56.42
68.80/60.23
68.54/59.99
80.01†/68.42
79.09/67.61
60.91/52.03 61.42†/52.27†
71.41/61.97
70.70/61.41
55.70/48.08 57.33†/49.37†
63.30/54.14 63.94†/54.63†
65.86/54.76
65.13/53.98
67.46†/58.77
66.79/58.23
49.85/44.09 52.05†/45.76†
70.45/49.47
70.03/49.38
65.84/48.61
66.11/48.73
65.01/44.78 65.31†/45.12†
37.63/27.48 38.72†/28.00†
47.74/34.90 48.80†/35.64†
34.44/16.18
33.98/15.93
36.34/27.43
36.72/27.40
65.92/55.86 66.40†/56.22†

(en-ru)
89.93/87.93
80.98/73.10
80.43/72.68
78.31/73.29
77.09†/67.81
76.25/68.03
74.08/65.84
80.70/75.57
63.11†/53.62†
73.40/64.90†
75.93/63.60
60.81†/52.66†
68.40/56.87
69.23†/60.51†
79.72/68.39
61.67†/52.41†
71.05/61.84
57.15†/49.36†
64.37†/55.08†
65.57/54.42
67.42†/58.70
51.57/45.31
70.67†/49.61†
66.28/48.82
65.38†/45.32†
38.98†/27.89†
49.17†/35.73†
34.23/16.08
37.37†/28.01†
66.53†/56.32†

Multilingual BERT
(en)
(en-fr)
93.19/91.21
92.81/90.97
85.81/79.03
85.50/78.64
85.64/78.58
85.61/78.34
85.22/80.78
84.76/80.26
82.93/73.33
82.71/73.13
82.40/73.68
82.36/73.53
80.81/72.66
81.11/72.80
87.07/82.38
86.90/82.22
73.39†/62.20
72.96/62.65
80.30/71.42
80.40/71.42
81.51/69.25 82.33†/69.91†
70.24/61.61
69.98/61.52
74.60/62.52
75.15/63.12
76.94/68.28
76.76/68.35
87.08/75.40
86.82/75.47
72.31/62.15
71.92/62.09
78.04/69.23
78.66/69.81
64.97†/55.63
64.46/55.82
74.57†/63.86
73.78/63.52
75.10/62.99 75.85†/63.92†
77.08†/67.57
76.30/67.38
57.45†/50.27
56.80/50.24
75.63/53.93
75.27/53.78
71.59/53.81
71.35/53.63
71.73/51.27
71.55/50.98
49.27/37.62 50.37†/39.37†
51.48/38.00
51.83/38.20
38.03/20.59
38.10/20.62
45.40/35.03 47.74†/35.90†
73.55/62.99
73.34/62.93

(en-ru)
92.77/90.86
85.43/78.76
85.44/78.33
85.91†/81.63†
83.43†/73.88†
82.36/73.86†
81.38†/73.29†
87.41/82.67
74.20†/63.55†
80.75/71.78
82.48†/70.54†
71.21†/62.84†
75.50/63.65†
76.89/68.76†
87.61†/75.94†
72.88†/62.94†
79.08†/70.26†
65.30†/55.76
74.56†/64.17†
76.06†/63.78†
77.86†/68.28†
57.30†/50.70†
75.62/54.29
71.74/53.79
71.25/51.16
50.95†/39.57†
52.20/38.28
38.98†/21.54†
46.10†/34.74
73.88†/63.43†

Table 2: Cross-lingual transfer performances (UAS%/LAS%, excluding punctuation) of the SelfAtt-Graph parser
(Ahmad et al., 2019) on the test sets. In column 1, languages are sorted by the word-ordering distance to English.
(en-fr) and (en-ru) denotes the source-auxiliary language pairs. ‘†’ indicates that the adversarially trained model
results are statistically signiﬁcantly better (by permutation test, p < 0.05) than the model trained only on the source
language (en). Results show that the utilization of unlabeled auxiliary language corpora improves cross-lingual
transfer performance signiﬁcantly.

sults demonstrate that the adversarial training with
the auxiliary language identiﬁcation task beneﬁts
cross-lingual transfer with a small performance
drop on the source language. When multi-lingual
embedding is employed, the performance signiﬁ-
cantly improves, in terms of UAS of 0.48 and 0.61
over the 29 languages when French and Russian
are used as the auxiliary language, respectively.
When richer multilingual representation technique
like mBERT is employed, adversarial training can
still improve cross-lingual transfer performances
(0.21 and 0.54 UAS over the 29 languages by us-
ing French and Russian, respectively).

Next, we apply adversarial

training on the
“RNN-Stack” parser and show the results in Ta-
ble 3. Similar to the “SelfAtt-Graph”parser, the
“RNN-Stack” parser resulted in signiﬁcant im-
provements in cross-lingual transfer from unsu-

pervised language adaptation. We discuss our de-
tailed experimental analysis in the following.

3.1.1

Impact of Adversarial Training

To understand the impact of different adversar-
ial training types and objectives, we apply adver-
sarial training on both word- and sentence-level
with gradient reversal (GR), GAN, and WGAN
objectives. We provide the average cross-lingual
transfer performances in Table 4 for different ad-
versarial training setups. Among the adversar-
ial training objectives, we observe that in most
cases, the GAN objective results in better per-
formances than the GR and WGAN objectives.
Our ﬁnding is in contrast to Adel et al. (2018)
where GR was reported to be the better objective.
To further investigate, we perform the language
test on the encoders trained via these two objec-
tives. We ﬁnd that the GR-based trained encoders

perform consistently better than the GAN based
ones on the language identiﬁcation task, show-
ing that via GAN-based training, the encoders be-
come more language-agnostic.
In a comparison
between GAN and WGAN, we notice that GAN-
based training consistently performs better.

Comparing word- and sentence-level adversar-
ial training, we observe that predicting language
identity at the word-level is slightly more use-
ful for the “SelfAtt-Graph” model, while the
sentence-level adversarial training results in better
performances for the “RNN-Stack” model. There
is no clear dominant strategy.

In addition, we study the effect of using a lin-
ear classiﬁer or a multi-layer Perceptron (MLP) as
the discriminator and ﬁnd that the interaction be-
tween the encoder and the linear classiﬁer resulted
in improvements.10

3.1.2 Adversarial v.s. Multi-task Training

In section 3.1.1, we study the effect of learning
language-agnostic representation by using auxil-
iary language with adversarial training. An al-
ternative way to leverage auxiliary language cor-
pora is by encoding language-speciﬁc information
in the representation via multi-task learning.
In
the multi-task learning (MTL) setup, the model
observes the same amount of data (both labeled
and unlabeled) as the adversarially trained (AT)
model. The only difference between the MTL and
AT models is that in the MTL models, the contex-
tual encoders are encouraged to capture language-
dependent features while in the AT models, they
are trained to encode language-agnostic features.
The experiment results using multi-task learn-
ing in comparison with the adversarial training are
presented in Table 5. Interestingly, although the
MTL objective sounds contradiction to adversar-
ial learning, it has a positive effect on the cross-
lingual parsing, as the representations are learned
with certain additional information from new (un-
labeled) data. Using MTL, we sometimes observe
improvements over the baseline parser, as indi-
cated with the † sign, while the AT models consis-
tently perform better than both the baseline and the
MTL model (as shown in Columns 2–5 in Table
5). The comparisons on parsing performances do
not reveal whether the contextual encoders learn to

10This is a known issue in GAN training as the discriminator
becomes too strong, it fails to provide useful signals to the
generator. In our case, MLP as the discriminator predicts
the language labels with higher accuracy and thus fails.

Lang
en
no
sv
fr
pt
da
es
it
hr
ca
pl
uk
sl
nl
bg
ru
de
he
cs
ro
sk
id
lv
ﬁ
et
ar
la
ko
hi

(en-fr)
(en)
89.88/87.66
89.65/87.43
80.42/72.49
80.20/72.11
81.14/73.44†
81.02/72.95
77.45/72.72
77.42/72.27
75.94/67.40
76.09/67.47
76.87/68.06 77.43†/68.62†
73.92/65.95 74.32†/66.35†
80.09/75.36 80.98†/76.00†
59.53/49.19 60.00†/50.02†
73.73/65.11
73.62/64.97
71.48/57.43 72.48†/59.19†
57.23/49.67 58.38†/51.04†
65.48/53.40 66.11†/54.21†
67.57/59.71†
67.13/59.15
77.28/65.77 77.79†/66.66†
58.70/49.34 59.77†/50.77†
70.03/59.45†
69.71/58.51
52.97/45.73 53.63†/46.49†
60.99/51.63 61.60†/52.41†
62.01/51.03
62.49/51.30
64.44/56.01 65.03†/56.65†
45.46/40.61†
45.08/40.00
70.22/48.46 71.08†/49.10†
65.59/48.31†
65.39/47.78
65.01/44.27
64.73/43.84
30.98/23.83 31.91†/24.72†
45.28/33.08
44.94/32.94
33.50/14.36
32.87/14.10
27.66/19.22
27.63/19.16
Average 64.09/53.93 64.51†/54.52†

(en-ru)
89.67/87.56
80.73†/72.65†
81.20/73.37
77.78/73.10
76.39†/67.85†
77.92†/69.24†
74.83†/66.83†
81.04†/76.06†
60.16†/50.16†
74.18†/65.59†
72.55†/58.38†
58.57†/50.88†
66.23†/54.09†
67.76†/59.96†
78.02†/66.53†
59.98†/50.51†
70.05/59.38†
54.72†/47.34†
61.81†/52.45†
63.22†/51.91†
65.36†/56.67†
46.82†/41.63†
70.76/48.86
65.42/47.84
65.04/44.16
32.83†/25.34†
45.12/33.11
32.60/14.11
26.72/18.96
64.74†/54.64†

Table 3: Cross-lingual transfer results (UAS%/LAS%,
excluding punctuation) of the RNN-Stack parser on the
‘†’ indicates that the adversarially trained
test sets.
model results are statistically signiﬁcantly better (by
permutation test, p < 0.05) than the model trained only
on the source language (en).

encode language-agnostic or dependent features.

Therefore, we perform language test with the
MTL and AT (GAN based) encoders, and the re-
sults are shown in Table 5, Columns 6–7. The re-
sults indicate that the MTL encoders consistently
perform better than the AT encoders, which veri-
ﬁes our hypothesis that adversarial training moti-
vates the contextual encoders to encode language-
agnostic features.

3.1.3
Impact of Auxiliary Languages
To analyze the effects of the auxiliary languages
in cross-language transfer via adversarial train-
ing, we perform experiments by pairing up11 the
source language (English) with six different lan-

11We also conduct experiments on multiple languages as the
auxiliary language. For GAN and WGAN-based training,
we concatenate the corpora of multiple languages and treat
them as one auxiliary language. In these set of experiments,
we do not observe any apparent improvements.

SelfAtt-Graph

RNN-Stack

AT

en-fr

en-ru

en-fr

en-ru

word
66.19
GR
66.40
GAN
WGAN 66.24

sent
66.21
66.29
66.18

word
66.38
66.53
66.40

sent
66.38
66.41
66.27

word
64.51
64.40
64.29

sent
64.51
64.51
64.34

word
64.52
64.63
64.57

sent
64.52
64.74
64.57

Table 4: Average cross-lingual transfer performances (UAS%, excluding punctuation) on the test sets using differ-
ent adversarial training objective and setting. Multilingual word embeddings are used for these experiments.

Lang
(Src. + Aux.)
en + fr
en + pt
en + es
en + ru
en + de
en + la

Auxiliary Language Perf.

Average Cross-lingual Perf.

AT
78.49/73.30†
76.53/67.45†
73.66/65.48
61.67/52.41†
71.65/62.11†
49.22/35.94†

MTL
78.26/72.98†
75.88/66.75
74.04/65.83†
61.08/52.04
71.17/61.88
48.04/35.09†

AT
66.40/56.22
66.40/56.22
66.38/56.24
66.53/56.32
66.41/56.13
66.45/56.20

MTL
66.18/56.04
66.27/56.08
66.22/56.12
66.35/56.20
66.18/56.12
66.17/56.05

Lang. Test Perf.
MTL
59.94
72.02
74.52
60.56
72.08
64.91

AT
62.25
60.17
56.78
37.34
61.22
50.04

Table 5: Comparison between adversarial training (AT) and multi-task learning (MTL) of the contextual encoders.
Columns 2–5 demonstrate the parsing performances (UAS%/LAS%, excluding punctuation) on the auxiliary lan-
guages and average of the 29 languages. Columns 6–7 present accuracy (%) of the language label prediction test.
‘†’ indicates that the performance is higher than the baseline performance (shown in the 2nd column of Table 2).

multilingual multilingual

Aux.
lang
pt
ru
de
es
fr
la

Avg. Dist.
to other lang Word Emb.
66.40/56.22
66.53/56.32
66.41/56.13
66.38/56.24
66.40/56.22
66.45/56.20

0.144
0.146
0.151
0.151
0.160
0.242

BERT
73.47/63.11
73.88/63.43
73.92/63.56
71.71/62.49
73.55/62.99
73.69/63.29

Table 6: Average cross-lingual transfer performances
(UAS%/LAS%, w/o punctuation) on the test sets using
SelfAtt-Graph parser when different languages play the
role of the auxiliary language in adversarial training.

guages (spanning Germanic, Romance, Slavic,
and Latin language families) as the auxiliary lan-
guage. The average cross-lingual transfer perfor-
mances are presented in Table 6 and the results
suggest that Russian (ru) and German (de) are bet-
ter candidates for auxiliary languages.

We then dive deeper into the effects of auxil-
iary languages trying to understand whether aux-
iliary languages particularly beneﬁt target lan-
guages that are closer to them12 or from the same
family.
Intuitively, we would assume when the
auxiliary language has a smaller average distance
to all the target languages, the cross-lingual trans-
fer performance would be better. However, from
the results in Table 6, we do not see such a pattern.
For example, Portuguese (pt) has the smallest av-
erage distance to other languages among the aux-

12The language distances are computed based on word order

characteristics as suggested in Ahmad et al. (2019).

iliary languages we tested, but it is not among the
better auxiliary languages.

We further zoom in the cross-lingual trans-
fer improvements for each language families as
shown in Table 7. We hypothesis that the auxil-
iary languages to be more helpful for the target
languages in the same family. The experimen-
tal results moderately correlate with our expecta-
tion. Speciﬁcally, the Germanic family beneﬁts
the most from employing German (de) as the aux-
iliary language; similarly Slavic family with Rus-
sian (ru) as the auxiliary language (although Ger-
man as the auxiliary language brings similar im-
provements). The Romance family is an exception
because it beneﬁts the least from using French (fr)
as the auxiliary language. This may due to the fact
that French is too closed to English, thus is less
suitable to be used as an auxiliary language.

4 Related Work

Unsupervised Cross-lingual Parsing. Unsu-
pervised cross-lingual
transfer for dependency
parsing has been studied over the past few years
(Agi´c et al., 2014; Ma and Xia, 2014; Xiao and
Guo, 2014; Tiedemann, 2015; Guo et al., 2015;
Aufrant et al., 2015; Rasooli and Collins, 2015;
Duong et al., 2015; Schlichtkrull and Søgaard,
2017; Ahmad et al., 2019; Rasooli and Collins,
2019; He et al., 2019). Here, “unsupervised trans-
fer” refers to the setting where a parsing model
trained only on the source language is directly

Lang (en,ru) - en (en,fr) - en (en,de) - en (en,la) - en
IE.Slavic Family
0.43/-0.45

1.52/1.02

1.24/0.90
0.06/-0.13
0.35/0.53 -0.55/-0.60 -0.04/0.14 -0.17/-0.50
1.54/1.33 -0.29/-0.09
1.23/1.32
1.03/0.98
0.82/0.98
0.97/1.29
0.01/0.04
0.49/0.41
0.79/0.47
0.20/0.34
1.07/1.11
0.96/0.85
-0.08/0.05
0.91/0.81
0.78/0.65
0.56/0.66
1.88/1.04
1.56/0.90
0.17/0.17
1.02/0.86
0.98/0.86

0.26/0.09
0.82/0.66
0.26/-0.07
0.39/0.06
0.79/0.34
0.78/0.19
0.4/0.03

IE.Romance Family

0.49/0.60
0.50/0.55 -0.22/-0.20 0.54/0.80
0.95/0.86
0.69/0.85 -0.46/-0.52 0.49/0.16
0.57/0.63
0.44/0.51
0.45/0.39
0.30/0.14
0.34/0.29 -0.17/-0.16 -0.22/-0.17 0.26/0.18
0.10/0.28
0.35/0.36
1.62/1.73
0.96/0.79
0.64/0.69
0.57/0.58

-0.10/0.00
0.75/0.93
0.02/0.03

0.64/0.70
1.32/1.32
0.54/0.53

IE.Germanic Family

-0.42/-0.35 -0.38/-0.24 -0.35/-0.25 -0.15/-0.20
-0.38/-0.27 -0.31/-0.39 -0.41/-0.15 -0.22/-0.24
-0.12/0.35 -0.02/0.18
-0.17/-0.01 0.03/0.24
-0.15/0.08 -0.46/-0.25
0.00/0.33
0.04/0.15
0.57/0.42
0.13/0.41
0.95/0.89
0.18/-0.07
0.25/0.43
0.42/0.45 -0.62/-0.58 1.41/1.40
0.00/0.06
-0.07/0.09 -0.18/-0.15 0.22/0.39

hr
sl
uk
pl
bg
ru
cs
sk
Avg.

pt
fr
es
it
ca
ro
Avg.

en
no
sv
da
nl
de
Avg.

Table 7: Average cross-lingual performance difference
between the SelfAtt-Graph parser trained on the source
(en) and an auxiliary (x) language and the SelfAtt-
Graph parser trained only on English (en) language
(UAS%/LAS%, excluding punctuation). We use multi-
lingual BERT in this set of experiments.

transferred to the target languages. In this work,
we relax the setting by allowing unlabeled data
from one or more auxiliary (helper) languages
other than the source language. This setting has
been explored in a few prior works. Cohen et al.
(2011) learn a generative target language parser
with unannotated target data as a linear interpo-
lation of the source language parsers. T¨ackstr¨om
et al. (2013) adopt unlabeled target language data
and a learning method that can incorporate di-
verse knowledge sources through ambiguous la-
beling for transfer parsing.
In comparison, we
leverage unlabeled auxiliary language data to learn
language-agnostic contextual representations to
improve cross-lingual transfer.

Multilingual Representation Learning. The
basic of the unsupervised cross-lingual parsing is
that we can align the representations of differ-
ent languages into the same space, at least at the
word level. The recent development of bilingual
or multilingual word embeddings provide us with
such shared representations. We refer the readers

to the surveys of Ruder et al. (2017) and Glavaˇs
et al. (2019) for details. The main idea is that
we can train a model on top of the source lan-
guage embeddings which are aligned to the same
space as the target language embeddings and thus
all the model parameters can be directly shared
across languages. During transfer to a target lan-
guage, we simply replace the source language em-
beddings with the target language embeddings.
This idea is further extended to learn multilin-
gual contextualized word representations, for ex-
ample, multilingual BERT (Devlin et al., 2019),
have been shown very effective for many cross-
lingual transfer tasks (Wu and Dredze, 2019). In
this work, we show that further improvements can
be achieved by adaptating the contextual encoders
via unlabeled auxiliary languages even when the
encoders are trained on top of multilingual BERT.

Adversarial Training. The concept of adversar-
ial training via Generative Adversarial Networks
(GANs) (Goodfellow et al., 2014; Szegedy et al.,
2014; Goodfellow et al., 2015) was initially in-
troduced in computer vision for image classiﬁca-
tion and received enormous success in improving
model’s robustness on input images with pertur-
bations. Later many variants of GANs (Arjovsky
et al., 2017; Gulrajani et al., 2017) were proposed
to improve its’ training stability. In NLP, adver-
sarial training was ﬁrst utilized for domain adap-
tation (Ganin et al., 2016). Since then adversarial
training has started to receive an increasing inter-
est in the NLP community and applied to many
NLP applications including part-of-speech (POS)
tagging (Gui et al., 2017; Yasunaga et al., 2018),
dependency parsing (Sato et al., 2017), relation ex-
traction (Wu et al., 2017), text classiﬁcation (Miy-
ato et al., 2017; Liu et al., 2017; Chen and Cardie,
2018), dialogue generation (Li et al., 2017).

In the context of cross-lingual NLP tasks, many
recent works adopted adversarial training, such
as in sequence tagging (Adel et al., 2018), text
classiﬁcation (Xu and Yang, 2017; Chen et al.,
2018), word embedding induction (Zhang et al.,
2017; Lample et al., 2018), relation classiﬁcation
(Zou et al., 2018), opinion mining (Wang and Pan,
2018), and question-question similarity reranking
(Joty et al., 2017). However, existing approaches
only consider using the target language as the aux-
It is unclear whether the lan-
iliary language.
guage invariant representations learned by previ-
ously proposed methods can perform well on a

wide variety of unseen languages. To the best of
our knowledge, we are the ﬁrst to study the effects
of language-agnostic representations on a broad
spectrum of languages.

5 Conclusion

In this paper, we study learning language invari-
ant contextual encoders for cross-lingual trans-
fer. Speciﬁcally, we leverage unlabeled sentences
from auxiliary languages and adversarial training
to induce language-agnostic encoders to improve
the performances of the cross-lingual dependency
parsing. Experiments and analysis using English
as the source language and six foreign languages
as the auxiliary languages not only show improve-
ments on cross-lingual dependency parsing, but
also demonstrates that contextual encoders suc-
cessfully learns not to capture language-dependent
features through adversarial training. In the future,
we plan to investigate the effectiveness of adver-
sarial training for multi-source transfer to parsing
and other cross-lingual NLP applications.

Acknowledgments

We thank the anonymous reviewers for their help-
ful feedback. This work was supported in part by
National Science Foundation Grant IIS-1760523.

References

Heike Adel, Anton Bryl, David Weiss, and Aliak-
sei Severyn. 2018. Adversarial neural networks
for cross-lingual sequence tagging. arXiv preprint
arXiv:1808.04736.

ˇZeljko Agi´c, J¨org Tiedemann, Kaja Dobrovoljc, Si-
mon Krek, Danijela Merkler, and Sara Moˇze. 2014.
Cross-lingual dependency parsing of related lan-
guages with rich morphosyntactic tagsets.
In
EMNLP 2014 Workshop on Language Technology
for Closely Related Languages and Language Vari-
ants.

Wasi Uddin Ahmad, Zhisong Zhang, Zuezhe Ma, Ed-
uard Hovy, Kai-Wei Chang, and Nanyun Peng.
2019. On difﬁculties of cross-lingual transfer with
order differences: A case study on dependency pars-
In Proceedings of the 2019 Conference of
ing.
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies.

Lauriane Aufrant, Guillaume Wisniewski, and Franc¸ois
Yvon. 2015. Zero-resource dependency parsing:
Boosting delexicalized cross-lingual transfer with
linguistic knowledge. In COLING 2016, the 26th In-
ternational Conference on Computational Linguis-
tics, pages 119–130. The COLING 2016 Organizing
Committee.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Xilun Chen and Claire Cardie. 2018. Multinomial ad-
versarial networks for multi-domain text classiﬁca-
In Proceedings of the 2018 Conference of
tion.
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1226–1240.

Xilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie,
and Kilian Weinberger. 2018. Adversarial deep av-
eraging networks for cross-lingual sentiment classi-
ﬁcation. Transactions of the Association for Com-
putational Linguistics, 6:557–570.

Shay B. Cohen, Dipanjan Das, and Noah A. Smith.
2011. Unsupervised structure prediction with non-
In Proceedings of
parallel multilingual guidance.
the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 50–61, Edinburgh,
Scotland, UK. Association for Computational Lin-
guistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4171–4186.

Timothy Dozat and Christopher D Manning. 2017.
Deep biafﬁne attention for neural dependency pars-
ing. Internation Conference on Learning Represen-
tations.

Long Duong, Trevor Cohn, Steven Bird, and Paul
Cook. 2015. Cross-lingual transfer for unsupervised
In Pro-
dependency parsing without parallel data.
ceedings of the Nineteenth Conference on Computa-
tional Natural Language Learning, pages 113–122.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, Franc¸ois Lavi-
olette, Mario Marchand, and Victor Lempitsky.
2016. Domain-adversarial training of neural net-
works. The Journal of Machine Learning Research,
17(1):2096–2030.

Martin Arjovsky, Soumith Chintala, and L´eon Bottou.
2017. Wasserstein generative adversarial networks.
In Proceedings of the 34th International Conference
on Machine Learning, pages 214–223. PMLR.

Goran Glavaˇs, Robert Litschko, Sebastian Ruder, and
Ivan Vuli´c. 2019. How to (properly) evaluate cross-
lingual word embeddings: On strong baselines,
comparative analyses, and some misconceptions. In

Proceedings of the 57th Annual Meeting of the As-
sociation for Computational Linguistics, pages 710–
721.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
In Advances in neural information
versarial nets.
processing systems, pages 2672–2680.

Ian J Goodfellow, Jonathon Shlens, and Christian
Szegedy. 2015. Explaining and harnessing adversar-
ial examples. In Internation Conference on Learn-
ing Representations.

Tao Gui, Qi Zhang, Haoran Huang, Minlong Peng, and
Xuanjing Huang. 2017. Part-of-speech tagging for
In Pro-
twitter with adversarial neural networks.
ceedings of the 2017 Conference on Empirical Meth-
ods in Natural Language Processing, pages 2411–
2420.

Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vin-
cent Dumoulin, and Aaron C Courville. 2017. Im-
In Advances
proved training of wasserstein gans.
in Neural Information Processing Systems, pages
5767–5777.

Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng
Wang, and Ting Liu. 2015. Cross-lingual depen-
dency parsing based on distributed representations.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), vol-
ume 1, pages 1234–1244.

Junxian He, Zhisong Zhang, Taylor Berg-Kiripatrick,
and Graham Neubig. 2019. Cross-lingual syntactic
transfer through unsupervised adaptation of invert-
ible projections. In Proceedings of the 57th Annual
Meeting of the Association for Computational Lin-
guistics, pages 3211–3223.

Shaﬁq Joty, Preslav Nakov, Llu´ıs M`arquez, and Israa
Jaradat. 2017. Cross-language learning with ad-
In Proceedings of the
versarial neural networks.
21st Conference on Computational Natural Lan-
guage Learning (CoNLL 2017), pages 226–237.

Joo-Kyung Kim, Young-Bum Kim, Ruhi Sarikaya, and
Eric Fosler-Lussier. 2017. Cross-lingual transfer
learning for pos tagging without cross-lingual re-
sources. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 2832–2838.

Diederik P Kingma and Jimmy Ba. 2015. Adam: A
International

method for stochastic optimization.
Conference on Learning Representations.

Gourab Kundu, Avi Sil, Radu Florian, and Wael
Hamza. 2018. Neural cross-lingual coreference res-
olution and its application to entity linking. In Pro-
ceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short

Papers), pages 395–400. Association for Computa-
tional Linguistics.

Guillaume Lample, Alexis Conneau, Ludovic Denoyer,
Herv´e J´egou, et al. 2018. Word translation without
parallel data. In Internation Conference on Learning
Representations.

Jiwei Li, Will Monroe, Tianlin Shi, S´ebastien Jean,
Alan Ritter, and Dan Jurafsky. 2017. Adversarial
learning for neural dialogue generation. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing, pages 2157–2169,
Copenhagen, Denmark. Association for Computa-
tional Linguistics.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017.
Adversarial multi-task learning for text classiﬁca-
In Proceedings of the 55th Annual Meet-
tion.
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 1–10, Vancouver,
Canada. Association for Computational Linguistics.

Xuezhe Ma, Zecong Hu, Jingzhou Liu, Nanyun Peng,
Graham Neubig, and Eduard Hovy. 2018. Stack-
In Pro-
pointer networks for dependency parsing.
ceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers).

Xuezhe Ma and Fei Xia. 2014. Unsupervised depen-
dency parsing with transferring distribution via par-
In Pro-
allel guidance and entropy regularization.
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1337–1348.

Takeru Miyato, Andrew M Dai, and Ian Goodfel-
low. 2017. Adversarial training methods for semi-
In Internation Con-
supervised text classiﬁcation.
ference on Learning Representations.

Joakim Nivre, Mitchell Abrams,

ˇZeljko Agi´c, and
et al. 2018. Universal dependencies 2.2. LIN-
DAT/CLARIN digital library at the Institute of For-
mal and Applied Linguistics ( ´UFAL), Faculty of
Mathematics and Physics, Charles University.

Joakim Nivre, Marie-Catherine De Marneffe, Filip
Ginter, Yoav Goldberg, Jan Hajic, Christopher D
Manning, Ryan T McDonald, Slav Petrov, Sampo
Pyysalo, Natalia Silveira, et al. 2016. Universal de-
pendencies v1: A multilingual treebank collection.
In Proceedings of the Tenth International Confer-
ence on Language Resources and Evaluation (LREC
2016).

Mohammad Sadegh Rasooli and Michael Collins.
2015. Density-driven cross-lingual transfer of de-
pendency parsers. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 328–338.

Mohammad Sadegh Rasooli and Michael Collins.
2019. Low-resource syntactic transfer with unsuper-
vised source reordering. Proceedings of the 2019

Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies.

Sebastian Ruder, Anders Søgaard, and Ivan Vulic.
2017. A survey of cross-lingual embedding models.
arXiv preprint arXiv:1706.04902.

Motoki Sato, Hitoshi Manabe, Hiroshi Noji, and Yuji
Matsumoto. 2017. Adversarial training for cross-
domain universal dependency parsing. In Proceed-
ings of the CoNLL 2017 Shared Task: Multilingual
Parsing from Raw Text to Universal Dependencies,
pages 71–79.

Michael Schlichtkrull and Anders Søgaard. 2017.
Cross-lingual dependency parsing with late decod-
In Proceed-
ing for truly low-resource languages.
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics:
Volume 1, Long Papers, pages 220–229. Association
for Computational Linguistics.

Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
2018. Self-attention with relative position represen-
In Proceedings of the 2018 Conference of
tations.
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers), pages 464–468.
Association for Computational Linguistics.

Avirup Sil, Gourab Kundu, Radu Florian, and Wael
Hamza. 2018. Neural cross-lingual entity linking.
In Thirty-Second AAAI Conference on Artiﬁcial In-
telligence.

Samuel L Smith, David HP Turban, Steven Hamblin,
and Nils Y Hammerla. 2017. Ofﬂine bilingual word
vectors, orthogonal transformations and the inverted
softmax. Internation Conference on Learning Rep-
resentations.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and
Rob Fergus. 2014.
Intriguing properties of neural
In Internation Conference on Learning
networks.
Representations.

Oscar T¨ackstr¨om, Ryan McDonald, and Joakim Nivre.
2013. Target language adaptation of discriminative
In Proceedings of the 2013 Con-
transfer parsers.
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1061–1071. Association
for Computational Linguistics.

J¨org Tiedemann. 2015.

Cross-lingual dependency
parsing with universal dependencies and predicted
pos labels. In Proceedings of the Third International
Conference on Dependency Linguistics (Depling
2015), pages 340–349.

Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture
6.5-rmsprop: Divide the gradient by a running av-
erage of its recent magnitude. COURSERA: Neural
networks for machine learning, 4(2):26–31.

Wenya Wang and Sinno Jialin Pan. 2018. Transition-
based adversarial network for cross-lingual aspect
In Proceedings of the Twenty-Seventh
extraction.
International Joint Conference on Artiﬁcial Intel-
ligence, IJCAI-18, pages 4475–4481. International
Joint Conferences on Artiﬁcial Intelligence Organi-
zation.

Shijie Wu and Mark Dredze. 2019. Beto, bentz, be-
cas: The surprising cross-lingual effectiveness of
bert. Proceedings of the 2019 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).

Yi Wu, David Bamman, and Stuart Russell. 2017. Ad-
versarial training for relation extraction. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing, pages 1778–1783.

Min Xiao and Yuhong Guo. 2014. Distributed word
representation learning for cross-lingual dependency
In Proceedings of the Eighteenth Confer-
parsing.
ence on Computational Natural Language Learning,
pages 119–129.

Jiateng Xie, Zhilin Yang, Graham Neubig, Noah A.
Smith, and Jaime Carbonell. 2018. Neural cross-
lingual named entity recognition with minimal re-
sources. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 369–379. Association for Computational
Linguistics.

Ruochen Xu and Yiming Yang. 2017. Cross-lingual
distillation for text classiﬁcation. In Proceedings of
the 55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers).

Michihiro Yasunaga,

Jungo Kasai, and Dragomir
Radev. 2018. Robust multilingual part-of-speech
tagging via adversarial training. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Pa-
pers), pages 976–986, New Orleans, Louisiana. As-
sociation for Computational Linguistics.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017. Adversarial training for unsupervised
In Proceedings of the
bilingual lexicon induction.
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1959–1970.

Bowei Zou, Zengzhuang Xu, Yu Hong, and Guodong
Zhou. 2018. Adversarial feature adaptation for
cross-lingual relation classiﬁcation. In Proceedings
of the 27th International Conference on Computa-
tional Linguistics, pages 437–448.

