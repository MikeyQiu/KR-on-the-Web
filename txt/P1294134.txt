From Credit Assignment to Entropy Regularization:
Two New Algorithms for Neural Sequence Prediction

Zihang Dai∗ , Qizhe Xie∗ , Eduard Hovy
Language Technologies Institute
Carnegie Mellon University
{dzihang, qizhex, hovy}@cs.cmu.edu

8
1
0
2
 
r
p
A
 
9
2
 
 
]
L
C
.
s
c
[
 
 
1
v
4
7
9
0
1
.
4
0
8
1
:
v
i
X
r
a

Abstract

In this work, we study the credit as-
signment problem in reward augmented
maximum likelihood (RAML) learning,
and establish a theoretical equivalence
between the token-level counterpart of
RAML and the entropy regularized rein-
forcement learning. Inspired by the con-
nection, we propose two sequence pre-
diction algorithms, one extending RAML
with ﬁne-grained credit assignment and
the other improving Actor-Critic with a
systematic entropy regularization. On two
benchmark datasets, we show the pro-
posed algorithms outperform RAML and
Actor-Critic respectively, providing new
alternatives to sequence prediction.

1

Introduction

Modeling and predicting discrete sequences is the
central problem to many natural language process-
ing tasks. In the last few years, the adaption of re-
current neural networks (RNNs) and the sequence-
to-sequence model (seq2seq) (Sutskever et al.,
2014; Bahdanau et al., 2014) has led to a wide
range of successes in conditional sequence pre-
diction, including machine translation (Sutskever
et al., 2014; Bahdanau et al., 2014), automatic
summarization (Rush et al., 2015), image cap-
tioning (Karpathy and Fei-Fei, 2015; Vinyals
et al., 2015; Xu et al., 2015) and speech recogni-
tion (Chan et al., 2016).

Despite the distinct evaluation metrics for the
aforementioned tasks, the standard training algo-
rithm has been the same for all of them. Specif-
ically, the algorithm is based on maximum likeli-
hood estimation (MLE), which maximizes the log-

∗ Equal contribution.

likelihood of the “ground-truth” sequences empir-
ically observed.1

While largely effective, the MLE algorithm has
two obvious weaknesses. Firstly, the MLE train-
ing ignores the information of the task speciﬁc
metric. As a result, the potentially large discrep-
ancy between the log-likelihood during training
and the task evaluation metric at test time can lead
to a suboptimal solution. Secondly, MLE can suf-
fer from the exposure bias, which refers to the
phenomenon that the model is never exposed to
its own failures during training, and thus cannot
recover from an error at test time. Fundamen-
tally, this issue roots from the difﬁculty in statisti-
cally modeling the exponentially large space of se-
quences, where most combinations cannot be cov-
ered by the observed data.

To tackle these two weaknesses, there have been
various efforts recently, which we summarize into
two broad categories:

• A widely explored idea is to directly opti-
mize the task metric for sequences produced by
the model, with the speciﬁc approaches rang-
ing from minimum risk training (MRT) (Shen
et al., 2015) and learning as search optimization
(LaSO) (Daum´e III and Marcu, 2005; Wise-
man and Rush, 2016) to reinforcement learn-
ing (RL) (Ranzato et al., 2015; Bahdanau et al.,
2016).
In spite of the technical differences,
the key component to make these training al-
gorithms practically efﬁcient is often a delicate
credit assignment scheme, which transforms
the sequence-level signal into dedicated smaller
units (e.g., token-level or chunk-level), and al-
locates them to speciﬁc decisions, allowing for
efﬁcient optimization with a much lower vari-
ance. For instance, the beam search optimiza-

1In this work, we use the terms “ground-truth” and “refer-
ence” to refer to the empirical observations interchangeably.

tion (BSO) (Wiseman and Rush, 2016) utilizes
the position of margin violations to produce sig-
nals to the speciﬁc chunks, while the actor-critic
(AC) algorithm (Bahdanau et al., 2016) trains a
critic to enable token-level signals.

• Another alternative idea is to construct a task
metric dependent target distribution, and train
the model to match this task-speciﬁc target in-
stead of the empirical data distribution. As a
typical example, the reward augmented maxi-
mum likelihood (RAML) (Norouzi et al., 2016)
deﬁnes the target distribution as the exponen-
tiated pay-off (sequence-level reward) distribu-
tion. This way, RAML not only can incorporate
the task metric information into training, but it
can also alleviate the exposure bias by expos-
ing imperfect outputs to the model. However,
RAML only works on the sequence-level train-
ing signal.

In this work, we are intrigued by the question
whether it is possible to incorporate the idea of
ﬁne-grained credit assignment into RAML. More
speciﬁcally, inspired by the token-level signal used
in AC, we aim to ﬁnd the token-level counter-
part of the sequence-level RAML, i.e., deﬁning
a token-level target distribution for each auto-
regressive conditional factor to match. Motived by
the question, we ﬁrst formally deﬁne the desider-
ata the token-level counterpart needs to satisfy and
derive the corresponding solution (§2). Then, we
establish a theoretical connection between the de-
rived token-level RAML and entropy regularized
RL (§3). Motivated by this connection, we pro-
pose two algorithms for neural sequence predic-
tion, where one is the token-level extension to
RAML, and the other a RAML-inspired improve-
ment to the AC (§4). We empirically evaluate the
two proposed algorithms, and show different lev-
els of improvement over the corresponding base-
line. We further study the importance of vari-
ous techniques used in our experiments, providing
practical suggestions to readers (§6).

2 Token-level Equivalence of RAML

We ﬁrst introduce the notations used throughout
the paper. Firstly, capital letters will denote ran-
dom variables and lower-case letters are the val-
ues to take. As we mainly focus on conditional
sequence prediction, we use x for the conditional
input, and y for the target sequence. With y denot-
ing a sequence, yj
i then denotes the subsequence

from position i to j inclusively, while yt denotes
the single value at position t. Also, we use |y| to
indicate the length of the sequence. To emphasize
the ground-truth data used for training, we add su-
perscript ∗ to the input and target, i.e., x∗ and y∗.
In addition, we use Y to denote the set of all pos-
sible sequences with one and only one eos symbol
at the end, and W to denote the set of all possible
symbols in a position. Finally, we assume length
of sequences in Y is bounded by T .

2.1 Background: RAML

As discussed in §1, given a ground-truth pair
(x∗, y∗), RAML deﬁnes the target distribution us-
ing the exponentiated pay-off of sequences, i.e.,

PR(y | x∗, y∗) =

exp (R(y; y∗)/τ )
y(cid:48)∈Y exp (R(y(cid:48); y∗)/τ )

,

(cid:80)

(1)

where R(y; y∗) is the sequence-level reward, such
as BLEU score, and τ is the temperature hyper-
parameter controlling the sharpness. With the deﬁ-
nition, the RAML algorithm simply minimizes the
cross entropy (CE) between the target distribution
and the model distribution Pθ(Y | x∗), i.e.,

CE (cid:0)PR(Y | x∗, y∗)(cid:107)Pθ(Y | x∗)(cid:1) .

(2)

min
θ

Note that, this is quite similar to the MLE training,
except that the target distribution is different. With
the particular choice of target distribution, RAML
not only makes sure the ground-truth reference re-
mains the mode, but also allows the model to ex-
plore sequences that are not exactly the same as
the reference but have relatively high rewards.

Compared to algorithms trying to directly opti-
mize task metric, RAML avoids the difﬁculty of
tracking and sampling from the model distribution
that is consistently changing. Hence, RAML en-
joys a much more stable optimization without the
need of pretraining. However, in order to opti-
mize the RAML objective (Eqn. (2)), one needs
to sample from the exponentiated pay-off distribu-
tion, which is quite challenging in practice. Thus,
importance sampling is often used (Norouzi et al.,
2016; Ma et al., 2017). We leave the details of the
practical implementation to Appendix B.1.

2.2 Token-level Target Distribution

Despite the appealing properties, RAML only op-
erates on the sequence-level reward. As a result,
the reward gap between any two sequences cannot
be attributed to the responsible decisions precisely,

t=1 Pθ(yt

which often leads to a low sample efﬁciency. Ide-
ally, since we rely on the auto-regressive factor-
ization Pθ(y | x∗) = (cid:81)|y|
, x∗),
the optimization would be much more efﬁcient if
we have the target distribution for each token-level
factor Pθ(Yt | yt−1
, x∗) to match. Conceptually,
this is exactly how the AC algorithm improves
upon the vanilla sequence-level REINFORCE al-
gorithm (Ranzato et al., 2015).

| yt−1
1

1

With this idea in mind, we set out to ﬁnd such
a token-level target. Firstly, we assume the token-
level target shares the form of a Boltzmann distri-
bution but parameterized by some unknown nega-
tive energy function QR, i.e.,2

1

1

1

(cid:80)

, y∗) =

, yt; y∗)/τ (cid:1)

exp (cid:0)QR(yt−1
w∈W exp (cid:0)QR(yt−1

PQR (yt | yt−1

, w; y∗)/τ (cid:1) .
(3)
Intuitively, QR(yt−1
, w; y∗) measures how much
future pay-off one can expect if w is generated,
given the current status yt−1
and the reference y∗.
1
This quantity highly resembles the action-value
function (Q-function) in reinforcement learning.
As we will show later, it is indeed the case.

1

Before we state the desiderata for QR, we need
to extend the deﬁnition of R in order to evaluate
the goodness of an unﬁnished partial prediction,
i.e., sequences without an eos sufﬁx. Let Y − be
the set of unﬁnished sequences, following Bah-
danau et al. (2016), we deﬁne the pay-off function
R for a partial sequence ˆy ∈ Y −, |ˆy| < T as

R(ˆy; y∗) = R(ˆy + eos; y∗),

(4)

where the + indicates string concatenation.

With the extension, we are ready to state two

requirements for QR:

1. Marginal match: For PQR to be the token-level
equivalence of PR, the sequence-level marginal
distribution induced by PQR must match PR,
i.e., for any y ∈ Y,

|y|
(cid:89)

t=1

PQR (yt | yt−1

1

) = PR(y).

(5)

Note that there are inﬁnitely many QR’s satisfy-
ing Eqn. (5), because adding any constant value
to QR does not change the Boltzmann distribu-
tion, known as shift-invariance w.r.t. the energy.

2. Terminal condition: Secondly, let’s consider
the value of QR when emitting an eos symbol to
immediately terminate the generation. As men-
tioned earlier, QR measures the expected future
pay-off. Since the emission of eos ends the gen-
eration, the future pay-off can only come from
the immediate increase of the pay-off. Thus, we
require QR to be the incremental pay-off when
producing eos, i.e.

QR(ˆy, eos; y∗) = R(ˆy + eos; y∗) − R(ˆy; y∗),
for any ˆy ∈ Y −. Since Eqn. (6) enforces the
absolute of QR at a point, it also solves the am-
biguity caused by the shift-invariance property.

(6)

Based on the two requirements, we can derive the
form QR, which is summarized by Proposition 1.
Proposition 1. PQR and QR satisfy requirements
(5) and (6) if and only if for any ground-truth pair
(x∗, y∗) and any sequence prediction y ∈ Y,

QR(yt−1

1

1; y∗) − R(yt−1
, yt; y∗) = R(yt
(cid:16)
(cid:88)
QR(yt

1
1, w; y∗)/τ

exp

; y∗)
(cid:17)

,

+ τ log

(7)

w∈W

when t < |y|, and otherwise, i.e., when t = |y|

QR(yt−1

1

, yt; y∗) = R(yt

1; y∗) − R(yt−1

1

; y∗).

(8)

Proof. See Appendix A.1.

Note that, instead of giving an explicit form for
the token-level target distribution, Proposition 1
only provides an equivalent condition in the form
of an implicit recursion. Thus, we haven’t ob-
tained a practical algorithm yet. However, as we
will discuss next, the recursion has a deep connec-
tion to entropy regularized RL, which ultimately
inspires our proposed algorithms.

3 Connection to Entropy-regularized RL

Before we dive into the connection, we ﬁrst give
a brief review of the entropy-regularized RL. For
an in-depth treatment, we refer readers to (Ziebart,
2010; Schulman et al., 2017).

3.1 Background: Entropy-regularized RL

Following the standard convention of RL, we de-
note a Markov decision process (MDP) by a tu-
ple M = (S, A, ps, r, γ), where S, A, ps, r, γ are
the state space, action space, transition probabil-
ity, reward function and discounting factor respec-
tively.3

2To avoid clutter, the conditioning on x∗ will be omitted

3In sequence prediction, we are only interested in the pe-

in the sequel, assuming it’s clear from the context.

riodic (ﬁnite horizon) case.

(9)

• the reward function r corresponds to the in-

cremental pay-off deﬁned in Eqn. (13),

Based on the notation,

the goal of entropy-
regularized RL augments is to learn a policy π(at |
st) which maximizes the discounted expected fu-
ture return and causal entropy (Ziebart, 2010), i.e.,

max
π

(cid:88)

t

E
st∼ρs,at∼π(·|st)

γt−1[r(st, at) + αH(π(· | st))],

where H denotes the entropy and α is a hyper-
parameter controlling the relative importance be-
tween the reward and the entropy.
Intuitively,
compared to standard RL, the extra entropy term
encourages exploration and promotes multi-modal
behaviors. Such properties are highly favorable in
a complex environment.

Given an entropy-regularized MDP, for any
ﬁxed policy π, the state-value function V π(s) and
the action-value function Qπ can be deﬁned as

V π(s) =

E
a∼π(·|s)

[Qπ(s, a)] + αH(π(· | s)),

Qπ(s, a) = r(s, a) + E

[γV π(s(cid:48))].

s(cid:48)∼ρs

With the deﬁnitions above, it can further be
proved (Ziebart, 2010; Schulman et al., 2017) that
the optimal state-value function V ∗, the action-
value function Q∗ and the corresponding optimal
policy π∗ satisfy the following equations

V ∗(s) = α log

exp (cid:0)Q∗(s, a)/α(cid:1) ,

(cid:88)

a∈A

Q∗(s, a) = r(s, a) + γ E

[V ∗(s(cid:48))],

π∗(a | s) =

(cid:80)

s(cid:48)∼ρs
exp (Q∗(s, a)/α)
a(cid:48)∈A exp (Q∗(s, a(cid:48))/α)

.

(10)

(11)

(12)

Here, Eqn.
(10) and (11) are essentially the
entropy-regularized counterparts of the optimal
Bellman equations in standard RL. Following pre-
vious literature, we will refer to Eqn. (10) and (11)
as the optimal soft Bellman equations, and the V ∗
and Q∗ as optimal soft value functions.

3.2 An RL Equivalence of the Token-level

RAML

To reveal the connection, it is convenient to deﬁne
the incremental pay-off

r(yt−1
1

, yt; y∗) = R(yt

1; y∗) − R(yt−1

1

; y∗),

(13)

and the last term of Eqn. (7) as

VR(yt

1; y∗) = τ log

(cid:88)

(cid:16)
QR(yt

exp

1, w; y∗)/τ

(cid:17)

(14)

w∈W

1

1

, yt; y∗) + VR(yt

, yt; y∗) = r(yt−1

Substituting the two deﬁnitions into Eqn. (7), the
recursion simpliﬁes as
QR(yt−1

1; y∗). (15)
Now, it is easy to see that the Eqn. (14) and (15),
which are derived from the token-level RAML,
highly resemble the optimal soft Bellman equa-
tions (10) and (11) in entropy-regularized RL. The
following Corollary formalizes the connection.
Corollary 1. For any ground-truth pair (x∗, y∗),
the recursion speciﬁed by Eqn. (13), (14) and (15)
is equivalent to the optimal soft Bellman equation
of a “deterministic” MDP in entropy-regularized
reinforcement learning, denoted as MR, where
• the state space S corresponds to Y −,
• the action space A corresponds to W,

• the transition probability ρs is a deterministic

process deﬁned by string concatenation

• the discounting factor γ = 1,

• the entropy hyper-parameter α = τ ,

• and a period terminates either when eos is
emitted or when its length reaches T and we
enforce the generation of eos.

Moreover, the optimal soft value functions V ∗ and
Q∗ of the MDP exactly match the VR and QR de-
ﬁned by Eqn. (14) and (15) respectively. The op-
timal policy π∗ is hence equivalent to the token-
level target distribution PQR.

Proof. See Appendix A.1.

The connection established by Corollary 1 is

quite inspiring:

• Firstly, it provides a rigorous and generalized
view of the connection between RAML and
entropy-regularized RL. In the original work,
Norouzi et al. (2016) point out RAML can be
seen as reversing the direction of KL (Pθ(cid:107)PR),
which is a sequence-level view of the connec-
tion. Now, with the equivalence between the
token-level target PQR and the optimal Q∗, it
generalizes to matching the future action values
consisting of both the reward and the entropy.

• Secondly, due to the equivalence, if we solve
the optimal soft Q-function of the correspond-
ing MDP, we directly obtain the token-level tar-
get distribution. This hints at a practical algo-
rithm with token-level credit assignment.

• Moreover, since RAML is able to improve
upon MLE by injecting entropy, the entropy-
regularized RL counterpart of the standard AC
algorithm should also lead to an improvement
in a similar manner.

4 Proposed Algorithms

In this section, we explore the insights gained from
Corollary 1 and present two new algorithms for
sequence prediction.

4.1 Value Augmented Maximum Likelihood

The ﬁrst algorithm we consider is the token-level
extension of RAML, which we have been dis-
cussing since §2. As mentioned at the end of
§2.2, Proposition 1 only gives an implicit form of
QR, and so is the token-level target distribution
PQR (Eqn.
(3)). However, thanks to Corollary
1, we now know that QR is the same as the op-
timal soft action-value function Q∗ of the entropy-
regularized MDP MR. Hence, by ﬁnding the Q∗,
we will have access to PQR.

At the ﬁrst sight, it seems recovering Q∗ is as
difﬁcult as solving the original sequence predic-
tion problem, because solving Q∗ from the MDP is
essentially the same as learning the optimal policy
for sequence prediction. However, it is not true be-
cause QR (i.e., PQR) can condition on the correct
reference y∗. In contrast, the model distribution
Pθ can only depend on x∗. Therefore, the func-
tion approximator trained to recover Q∗ can take
y∗ as input, making the estimation task much eas-
ier. Intuitively, when recovering Q∗, we are trying
to train an ideal “oracle”, which has access to the
ground-truth reference output, to decide the best
behavior (policy) given any arbitrary (good or not)
state.

Thus, following the reasoning above, we ﬁrst
train a parametric function approximator Qφ to
search the optimal soft action value.
In this
work,
for simplicity, we employ the Soft Q-
learning algorithm (Schulman et al., 2017) to per-
form the policy optimization. In a nutshell, Soft
Q-Learning is the entropy-regularized version of
Q-Learning, an off-policy algorithm which mini-
mizes the mean squared soft Bellman residual ac-
cording to Eqn. (11). Speciﬁcally, given ground-
truth pair (x∗, y∗), for any trajectory y ∈ Y, the
training objective is

|y|
(cid:88)

(cid:104)

t=1

min
φ

Qφ(yt−1
1

, yt; y∗) − ˆQφ(yt−1

, yt; y∗)

1

, (16)

(cid:105)2

1

1

(10).

, yt; y∗) + Vφ(yt

, yt; y∗) = r(yt−1

1; y∗) = τ log (cid:80)

w∈W exp (cid:0)Qφ(yt

where ˆQφ(yt−1
1; y∗)
is the one-step look-ahead target Q-value, and
1, w; y∗)/τ (cid:1) as
Vφ(yt
deﬁned in Eqn.
In the recent instantia-
tion of Q-Learning (Mnih et al., 2015), to sta-
bilize training, the target Q-value is often esti-
mated by a separate slowly updated target net-
work. In our case, as we have access to a signif-
icant amount of reference sequences, we ﬁnd the
target network not necessary. Thus, we directly
optimize Eqn. (16) using gradient descent, and let
the gradient ﬂow through both Qφ(yt−1
, yt; y∗)
1; y∗) (Baird, 1995).
and Vφ(yt

1

After the training of Qφ converges, we ﬁx the
parameters of Qφ, and optimize the cross en-
(cid:1) w.r.t.
tropy CE (cid:0)PQφ(cid:107)Pθ
the model parameters
θ, which is equivalent to4

min
θ

E
y∼PQφ





|y|
(cid:88)

t=1

CE (cid:0)PQφ (Yt | yt−1

1

)(cid:107)Pθ(Yt | yt−1

1


)(cid:1)

 .

| yt−1
1

| yt−1
1

(17)
Compared to the of objective of RAML in Eqn.
) allows us
(2), having access to PQφ(Yt
to provide a distinct token-level target for each
) of the model.
conditional factor Pθ(Yt
While directly sampling from PR is practically in-
feasible (§2.1), having a parametric target distri-
bution PQφ makes it theoretically possible to sam-
ple from PQφ and perform the optimization. How-
ever, empirically, we ﬁnd the samples from PQφ
are not diverse enough (§6). Hence, we fall back to
the same importance sampling approach (see Ap-
pendix B.2) as used in RAML.

Finally, since the algorithm utilizes the optimal
soft action-value function to construct the token-
level target, we will refer to it as value augmented
maximum likelihood (VAML) in the sequel.

4.2 Entropy-regularized Actor Critic

The second algorithm follows the discussion at the
end of §3.2, which is essentially an actor-critic al-
gorithm based on the entropy-regularized MDP in
Corollary 1. For this reason, we name the algo-
rithm entropy-regularized actor critic (ERAC). As
with standard AC algorithm, the training process
interleaves the evaluation of current policy using
the parametric critic Qφ and the optimization of
the actor policy πθ given the current critic.

Critic Training. The critic is trained to perform
policy evaluation using the temporal difference

4See Appendix A.2 for a detailed derivation.

learning (TD), which minimizes the TD error

min
φ

E
y∼πθ

|y|
(cid:88)

(cid:104)

t=1

Qφ(yt−1
1

, yt; y∗) − ˆQ ¯φ(yt−1

1

, yt; y∗)

(cid:105)2

(18)
where the TD target ˆQ ¯φ is constructed based on
ﬁxed policy iteration in Eqn. (9), i.e.,

ˆQ ¯φ(yt−1

1

, yt; y∗) = r(yt−1

, yt) + τ H(πθ(· | yt
1))
1, w; y∗).
1)Q ¯φ(yt

1
πθ(w | yt

(19)

(cid:88)

+

w∈W

It is worthwhile to emphasize that the objective
(18) trains the critic Qφ to evaluate the current pol-
icy. Hence, it is entirely different from the objec-
tive (16), which is performing policy optimization
by Soft Q-Learning. Also, the trajectories y used
in (18) are sequences drawn from the actor policy
πθ, while objective (16) theoretically accepts any
trajectory since Soft Q-Learning can be fully off-
policy.5 Finally, following Bahdanau et al. (2016),
the TD target ˆQ ¯φ in Eqn.
(9) is evaluated us-
ing a target network, which is indicated by the
bar sign above the parameters, i.e., ¯φ. The target
network is slowly updated by linearly interpolat-
ing with the up-to-date network, i.e., the update is
¯φ ← βφ + (1 − β) ¯φ for β in (0, 1) (Lillicrap et al.,
2015).

We also adapt another technique proposed by
Bahdanau et al. (2016), which smooths the critic
by minimizing the “variance” of Q-values, i.e.,

min
φ

λvar E
y∼πθ

|y|
(cid:88)

(cid:88)

t=1

w∈W

(cid:2)Qφ(yt

1, w; y∗) − ¯Qφ(yt

1; y∗)(cid:3)2

(cid:80)

1; y∗) = 1
|W|

where ¯Qφ(yt
1, w(cid:48); y∗) is
the mean Q-value, and λvar is a hyper-parameter
controlling the relative weight between the TD
loss and the smooth loss.

w(cid:48)∈W Qφ(yt

Actor Training. Given the critic Qφ, the actor
gradient (to maximize the expected return) is given
by the policy gradient theorem of the entropy-
regularized RL (Schulman et al., 2017), which has
the form

|y|
(cid:88)

(cid:88)

E
y∼πθ

∇θπθ(w | yt−1

)Qφ(yt−1

, w; y∗)

1

1

t=1

w∈W
+ τ ∇θH(πθ(· | yt−1

1

)).

(20)

Here, for each step t, we follow Bahdanau et al.
(2016) to sum over the entire symbol set W, in-
stead of using the single sample estimation often

5Different from Bahdanau et al. (2016), we don’t use a de-
layed actor network to collect trajectories for critic training.

seen in RL. Hence, no baseline is employed.
It
is worth mentioning that Eqn. (20) is not simply
adding an entropy term to the standard policy gra-
dient as in A3C (Mnih et al., 2016). The difference
lies in that the critic Qφ trained by Eqn. (18) ad-
ditionally captures the entropy from future steps,
while the ∇θH term only captures the entropy of
the current step.

Finally, similar to (Bahdanau et al., 2016), we
ﬁnd it necessary to ﬁrst pretrain the actor using
MLE and then pretrain the critic before the actor-
critic training. Also, to prevent divergence dur-
ing actor-critic training, it is helpful to continue
performing MLE training along with Eqn.
(20),
though using a smaller weight λmle.

5 Related Work

Task Loss Optimization and Exposure Bias
Apart from the previously introduced RAML,
BSO, Actor-Critic (§1), MIXER (Ranzato et al.,
2015) also utilizes chunk-level signals where the
length of chunk grows as training proceeds.
In
contrast, minimum risk training (Shen et al., 2015)
directly optimizes sentence-level BLEU. As a re-
sult, it requires a large number (100) of samples
per data to work well. To solve the exposure bias,
scheduled sampling (Bengio et al., 2015) adopts a
curriculum learning strategy to bridge the training
and the inference. Professor forcing (Lamb et al.,
2016) introduces an adversarial training mecha-
nism to encourage the dynamics of the model to
be the same at training time and inference time.
For image caption, self-critic sequence training
(SCST) (Rennie et al., 2016) extends the MIXER
algorithm with an improved baseline based on the
current model performance.

Entropy-regularized RL Entropy regulariza-
tion been explored by early work in RL and in-
verse RL (Williams and Peng, 1991; Ziebart et al.,
2008). Lately, Schulman et al. (2017) establish
the equivalence between policy gradients and Soft
Q-Learning under entropy-regularized RL. Mo-
tivated by the multi-modal behavior induced by
entropy-regularized RL, Haarnoja et al. (2017) ap-
ply energy-based policy and Soft Q-Learning to
continuous domain. Later, Nachum et al. (2017)
proposes path consistency learning, which can be
seen as a multi-step extension to Soft Q-Learning.
More recently, in the domain of simulated con-
trol, Haarnoja et al. (2018) also consider the ac-
tor critic algorithm under the framework of en-

tropy regularized reinforcement learning. Despite
the conceptual similarity to ERAC presented here,
Haarnoja et al. (2018) focuses on continuous con-
trol and employs the advantage actor critic variant
as in (Mnih et al., 2016), while ERAC follows the
Q actor critic as in (Bahdanau et al., 2016).

6 Experiments

6.1 Experiment Settings

In this work, we focus on two sequence prediction
tasks: machine translation and image captioning.
Due to the space limit, we only present the infor-
mation necessary to compare the empirical results
at this moment. For a more detailed description,
we refer readers to Appendix B and the code6.

Machine Translation Following Ranzato et al.
(2015), we evaluate on IWSLT 2014 German-to-
English dataset (Mauro et al., 2012). The cor-
pus contains approximately 153K sentence pairs
in the training set. We follow the pre-processing
procedure used in (Ranzato et al., 2015).

Architecture wise, we employ a seq2seq model
with dot-product attention (Bahdanau et al., 2014;
Luong et al., 2015), where the encoder is a bidirec-
tional LSTM (Hochreiter and Schmidhuber, 1997)
with each direction being size 128, and the de-
coder is another LSTM of size 256. Moreover, we
consider two variants of the decoder, one using the
input feeding technique (Luong et al., 2015) and
the other not.

For all algorithms, the sequence-level BLEU
score is employed as the pay-off function R, while
the corpus-level BLEU score (Papineni et al.,
The
2002) is used for the ﬁnal evaluation.
sequence-level BLEU score is scaled up by the
sentence length so that the scale of the immediate
reward at each step is invariant to the length.

Image Captioning For image captioning, we
consider the MSCOCO dataset (Lin et al., 2014).
We adapt the same preprocessing procedure and
the train/dev/test split used by Karpathy and Fei-
Fei (2015).

The NIC (Vinyals et al., 2015) is employed as
the baseline model, where a feature vector of the
image is extracted by a pre-trained CNN and then
used to initialize the LSTM decoder. Different
from the original NIC model, we employ a pre-
trained 101-layer ResNet (He et al., 2016) rather
than a GoogLeNet as the CNN encoder.

6https://github.com/zihangdai/ERAC-VAML

For training, each image-caption pair is treated
sample, and sequence-level BLEU
as an i.i.d.
score is used as the pay-off. For testing, the stan-
dard multi-reference BLEU4 is used.

6.2 Comparison with the Direct Baseline

Firstly, we compare ERAC and VAML with their
corresponding direct baselines, namely AC (Bah-
danau et al., 2016) and RAML (Norouzi et al.,
2016) respectively. As a reference, the perfor-
mance of MLE is also provided.

Due to non-neglected performance variance ob-
served across different runs, we run each algo-
rithm for 9 times with different random seeds,7
and report the average performance, the standard
deviation and the performance range (min, max).

Machine Translation The results on MT are
summarized in the left half of Tab. 1. Firstly,
all four advanced algorithms signiﬁcantly outper-
form the MLE baseline. More importantly, both
VAML and ERAC improve upon their direct base-
lines, RAML and AC, by a clear margin on aver-
age. The result suggests the two proposed algo-
rithms both well combine the beneﬁts of a delicate
credit assignment scheme and the entropy regular-
ization, achieving improved performance.

Image Captioning The results on image cap-
tioning are shown in the right half of Tab. 1. De-
spite the similar overall trend, the improvement of
VAML over RAML is smaller compared to that
in MT. Meanwhile, the improvement from AC to
ERAC becomes larger in comparison. We sus-
pect this is due to the multi-reference nature of
the MSCOCO dataset, where a larger entropy is
preferred. As a result, the explicit entropy regu-
larization in ERAC becomes immediately fruitful.
On the other hand, with multiple references, it can
be more difﬁcult to learn a good oracle Q∗ (Eqn.
(15)). Hence, the token-level target can be less ac-
curate, resulting in smaller improvement.

6.3 Comparison with Existing Work

To further evaluate the proposed algorithms, we
compare ERAC and VAML with the large body
of existing algorithms evaluated on IWSTL 2014.
As a note of caution, previous works don’t employ
the exactly same architectures (e.g. number of lay-
ers, hidden size, attention type, etc.). Despite that,

7For AC, ERAC and VAML, 3 different critics are trained

ﬁrst, and each critic is then used to train 3 actors.

Algorithm

MT (w/o input feeding)
Mean

Min Max

MT (w/ input feeding)
Mean

Min Max

Image Captioning

Mean

Min Max

MLE

RAML
VAML

AC
ERAC

27.01 ± 0.20

26.72

27.27

28.06 ± 0.15

27.84

28.22

29.54 ± 0.21

29.27

29.89

27.74 ± 0.15
28.16 ± 0.11

28.04 ± 0.05
28.30 ± 0.06

27.47
28.00

27.97
28.25

27.93
28.26

28.10
28.42

28.56 ± 0.15
28.84 ± 0.10

29.05 ± 0.06
29.31 ± 0.04

28.35
28.62

28.95
29.26

28.80
28.94

29.16
29.36

29.84 ± 0.21
29.93 ± 0.22

30.90 ± 0.20
31.44 ± 0.22

29.50
29.51

30.49
31.07

30.17
30.24

31.16
31.82

Table 1: Test results on two benchmark tasks. Bold faces highlight the best in the corresponding category.

for VAML and ERAC, we use an architecture that
is most similar to the majority of previous works,
which is the one described in §6.1 with input feed-
ing.

Based on the setting, the comparison is summa-
rized in Table 2.8 As we can see, both VAML and
ERAC outperform previous methods, with ERAC
leading the comparison with a signiﬁcant margin.
This further veriﬁes the effectiveness of the two
proposed algorithms.

(cid:72)(cid:72)
λvar

β

(cid:72)(cid:72)(cid:72)(cid:72)

0.001

0.01

0.1

1

0
0.001

27.91
29.41

26.27†
29.26

28.88
29.32

27.38†
27.44

Table 3: Average validation BLEU of ERAC. As
a reference, the average BLEU is 28.1 for MLE.
λvar = 0 means not using the smoothing technique.
β = 1 means not using a target network. † indi-
cates excluding extreme values due to divergence.

Algorithm

MIXER (Ranzato et al., 2015)
BSO (Wiseman and Rush, 2016)
Q(BLEU) (Li et al., 2017)
AC (Bahdanau et al., 2016)
RAML (Ma et al., 2017)

VAML
ERAC

BLEU

20.73
27.9
28.3
28.53
28.77

28.94
29.36

Table 2: Comparison with existing algorithms on
IWSTL 2014 dataset for MT. All numbers of pre-
vious algorithms are from the original work.

6.4 Ablation Study

Due to the overall excellence of ERAC, we study
the importance of various components of it, hope-
fully offering a practical guide for readers. As
the input feeding technique largely slows down
the training, we conduct the ablation based on the
model variant without input feeding.

Firstly, we study the importance of two tech-
niques aimed for training stability, namely the tar-
get network and the smoothing technique (§4.2).
Based on the MT task, we vary the update speed β
of the target critic, and the λvar, which controls the

8For a more detailed comparison of performance together

with the model architectures, see Table 7 in Appendix C.

strength of the smoothness regularization. The av-
erage validation performances of different hyper-
parameter values are summarized in Tab. 3.

• Comparing the two rows of Tab. 3, the smooth-
ing technique consistently leads to performance
improvement across all values of τ . In fact, re-
moving the smoothing objective often causes
the training to diverge, especially when β =
0.01 and 1. But interestingly, we ﬁnd the di-
vergence does not happen if we update the tar-
get network a little bit faster (β = 0.1) or quite
slowly (β = 0.001).

• In addition, even with the smoothing technique,
the target network is still necessary. When the
target network is not used (β = 1), the perfor-
mance drops below the MLE baseline. How-
ever, as long as a target network is employed to
ensure the training stability, the speciﬁc choice
of target network update rate does not matter
as much. Empirically, it seems using a slower
(β = 0.001) update rate yields the best result.

Next, we investigate the effect of enforcing dif-
ferent levels of entropy by varying the entropy
hyper-parameter τ . As shown in Fig. 1, it seems
there is always a sweet spot for the level of en-
tropy. On the one hand, posing an over strong en-

(a) Machine translation

(b) Image captioning

Figure 1: ERAC’s average performance over multiple runs on two tasks when varying τ .

7 Discussion

In this work, motivated by the intriguing con-
nection between the token-level RAML and the
entropy-regularized RL, we propose two algo-
rithms for neural sequence prediction. Despite the
distinct training procedures, both algorithms com-
bine the idea of ﬁne-grained credit assignment and
the entropy regularization, leading to positive em-
pirical results.

However, many problems remain widely open.
In particular, the oracle Q-function Qφ we obtain
is far from perfect. We believe the ground-truth
reference contains sufﬁcient information for such
an oracle, and the current bottleneck lies in the RL
algorithm. Given the numerous potential applica-
tions of such an oracle, we believe improving its
accuracy will be a promising future direction.

tropy regularization can easily cause the actor to
diverge. Speciﬁcally, the model diverges when τ
reaches 0.03 on the image captioning task or 0.06
on the machine translation task. On the other hand,
as we decrease τ from the best value to 0, the per-
formance monotonically decreases as well. This
observation further veriﬁes the effectiveness of en-
tropy regularization in ERAC, which well matches
our theoretical analysis.

Finally, as discussed in §4.2, ERAC takes the ef-
fect of future entropy into consideration, and thus
is different from simply adding an entropy term
to the standard policy gradient as in A3C (Mnih
et al., 2016). To verify the importance of explicitly
modeling the entropy from future steps, we com-
pared ERAC with the variant that only applies the
entropy regularization to the actor but not to the
critic. In other words, the τ is set to 0 when per-
forming policy evaluating according to Eqn. (4.2),
while the τ for the entropy gradient in Eqn. (20)
remains. The comparison result based on 9 runs
on test set of IWSTL 2014 is shown in Table 4. As
we can see, simply adding a local entropy gradient
does not even improve upon the AC. This further
veriﬁes the difference between ERAC and A3C,
and shows the importance of taking future entropy
into consideration.

Algorithm

Mean

ERAC
ERAC w/o Future Ent.
AC

28.30 ± 0.06
28.06 ± 0.05
28.04 ± 0.05

Max

28.42
28.11
28.10

Table 4: Comparing ERAC with the variant with-
out considering future entropy.

References

Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu,
Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. 2016. An actor-critic
algorithm for sequence prediction. arXiv preprint
arXiv:1607.07086 .

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473 .

Leemon Baird. 1995. Residual algorithms: Reinforce-
ment learning with function approximation. In Ma-
chine Learning Proceedings 1995, Elsevier, pages
30–37.

Alex M Lamb, Anirudh Goyal ALIAS PARTH
GOYAL, Ying Zhang, Saizheng Zhang, Aaron C
Courville, and Yoshua Bengio. 2016.
Professor
forcing: A new algorithm for training recurrent net-
works. In Advances In Neural Information Process-
ing Systems. pages 4601–4609.

Jiwei Li, Will Monroe, and Dan Jurafsky. 2017. Learn-
ing to decode for future success. arXiv preprint
arXiv:1701.06549 .

Timothy P Lillicrap, Jonathan J Hunt, Alexander
Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. 2015. Continu-
ous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971 .

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and
Noam Shazeer. 2015. Scheduled sampling for se-
quence prediction with recurrent neural networks.
In Advances in Neural Information Processing Sys-
tems. pages 1171–1179.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar,
and C Lawrence Zitnick. 2014. Microsoft coco:
In European confer-
Common objects in context.
ence on computer vision. Springer, pages 740–755.

William Chan, Navdeep Jaitly, Quoc Le, and Oriol
Vinyals. 2016. Listen, attend and spell: A neural
network for large vocabulary conversational speech
In Acoustics, Speech and Signal Pro-
recognition.
cessing (ICASSP), 2016 IEEE International Confer-
ence on. IEEE, pages 4960–4964.

Hal Daum´e III and Daniel Marcu. 2005. Learning
as search optimization: Approximate large margin
In Proceedings
methods for structured prediction.
of the 22nd international conference on Machine
learning. ACM, pages 169–176.

Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and
learning
arXiv preprint

Sergey Levine. 2017.
with deep energy-based policies.
arXiv:1702.08165 .

Reinforcement

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and
Sergey Levine. 2018. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with
a stochastic actor. arXiv preprint arXiv:1801.01290
.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
In Proceedings of the IEEE conference on
nition.
computer vision and pattern recognition. pages 770–
778.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation

Long short-term memory.
9(8):1735–1780.

Po-Sen Huang, Chong Wang, Dengyong Zhou, and
Li Deng. 2017. Toward neural phrasebased machine
translation .

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
the IEEE conference
tions.
on computer vision and pattern recognition. pages
3128–3137.

In Proceedings of

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. arXiv preprint
arXiv:1508.04025 .

Xuezhe Ma, Pengcheng Yin, Jingzhou Liu, Graham
Softmax q-
Neubig, and Eduard Hovy. 2017.
distribution estimation for structured prediction: A
theoretical interpretation for raml. arXiv preprint
arXiv:1705.07136 .

Cettolo Mauro, Girardi Christian, and Federico Mar-
cello. 2012. Wit3: Web inventory of transcribed and
translated talks. In Conference of European Associ-
ation for Machine Translation. pages 261–268.

Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi
Mirza, Alex Graves, Timothy Lillicrap, Tim Harley,
David Silver, and Koray Kavukcuoglu. 2016. Asyn-
chronous methods for deep reinforcement learning.
In International Conference on Machine Learning.
pages 1928–1937.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidje-
land, Georg Ostrovski, et al. 2015. Human-level
control through deep reinforcement learning. Na-
ture 518(7540):529.

Oﬁr Nachum, Mohammad Norouzi, Kelvin Xu, and
Dale Schuurmans. 2017. Bridging the gap between
value and policy based reinforcement learning.
In
Advances in Neural Information Processing Sys-
tems. pages 2772–2782.

Mohammad Norouzi, Samy Bengio, Navdeep Jaitly,
Mike Schuster, Yonghui Wu, Dale Schuurmans,
et al. 2016. Reward augmented maximum likeli-
hood for neural structured prediction. In Advances
In Neural Information Processing Systems. pages
1723–1731.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th annual meeting on association for compu-
tational linguistics. Association for Computational
Linguistics, pages 311–318.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2015. Sequence level train-
ing with recurrent neural networks. arXiv preprint
arXiv:1511.06732 .

Steven J Rennie, Etienne Marcheret, Youssef Mroueh,
Jarret Ross, and Vaibhava Goel. 2016. Self-critical
arXiv
sequence training for image captioning.
preprint arXiv:1612.00563 .

Alexander M Rush, Sumit Chopra, and Jason We-
A neural attention model for ab-
arXiv preprint

ston. 2015.
stractive sentence summarization.
arXiv:1509.00685 .

John Schulman, Pieter Abbeel, and Xi Chen. 2017.
Equivalence between policy gradients and soft q-
learning. arXiv preprint arXiv:1704.06440 .

Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2015. Minimum
risk training for neural machine translation. arXiv
preprint arXiv:1512.02433 .

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural im-
age caption generator. In Computer Vision and Pat-
tern Recognition (CVPR), 2015 IEEE Conference
on. IEEE, pages 3156–3164.

Ronald J Williams and Jing Peng. 1991. Function opti-
mization using connectionist reinforcement learning
algorithms. Connection Science 3(3):241–268.

Sam Wiseman and Alexander M Rush. 2016.
Sequence-to-sequence learning as beam-search op-
timization. arXiv preprint arXiv:1606.02960 .

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. 2015. Show, attend and tell:
Neural image caption generation with visual at-
In International Conference on Machine
tention.
Learning. pages 2048–2057.

Brian D Ziebart. 2010. Modeling purposeful adaptive
behavior with the principle of maximum causal en-
tropy. Carnegie Mellon University.

Brian D Ziebart, Andrew L Maas, J Andrew Bagnell,
and Anind K Dey. 2008. Maximum entropy inverse
reinforcement learning. In AAAI. Chicago, IL, USA,
volume 8, pages 1433–1438.

A Proofs

A.1 Main Proofs
Proposition 1. For any ground-truth pair (x∗, y∗), PQR and QR satisfy the following marginal match
condition and terminal condition:

|y|
(cid:89)

t=1

PQR(yt | yt−1

1

) = PR(y | x∗) ∀y ∈ Y

QR(ˆy, eos; y∗) = R(ˆy + eos; y∗) − R(ˆy; y∗) ∀ˆy ∈ Y −

if and only if for any y ∈ Y,

QR(yt−1

, yt; y∗) =

1

(cid:40)

R(yt
R(yt

1; y∗) − R(yt−1
1; y∗) − R(yt−1

1

1

; y∗) + τ log (cid:80)
; y∗),

w∈W exp (cid:0)QR(yt

1, w; y∗)/τ (cid:1) ,

(21)

(22)

t < |y|
t = |y|

(23)

Proof. To avoid clutter, we drop the dependency on x∗ and y∗. The following proof holds for each
possible pair of (x∗, y∗).

Firstly, it is easy to see that the terminal condition in Eqn. (22) exactly corresponds to the t = |y| case

of Eqn. (23), since yt = eos for y ∈ Y. So, we will focus on the non-terminal case next.

Sufﬁciency For convenience, deﬁne VR(yt
is true. Then for any y ∈ Y,

1) = τ log (cid:80)

w∈W exp (cid:0)QR(yt

1, w)/τ (cid:1). Suppose Eqn. (23)

PQR(y) =

PQR(yt | yt−1

1

)

(cid:32) (cid:80)|y|

t=1 QR(yt−1

1

(cid:33)

, yt) − VR(yt−1
τ

1

)

|y|
(cid:89)

t=1

= exp

= exp

= exp

(cid:18) R(y) − VR(∅)
τ

(cid:19)

(cid:32) (cid:80)|y|
t=1

(cid:2)R(yt

1) − R(yt−1

1

)(cid:3) + (cid:80)|y|−1
t=1 VR(yt
τ

1) − (cid:80)|y|

t=1 VR(yt−1

1

)

(cid:33)

where VR(∅) denotes VR(yt
by construction, we have

1) when t = 0 and yt

1 is an empty set. Since PQR(y) is a valid distribution

Hence,

VR(∅) =

exp

(cid:88)

y∈Y

(cid:19)

(cid:18) R(y)
τ

PQR(y) =

(cid:80)

R(y)/τ
y(cid:48)∈Y R(y(cid:48))/τ

= PR(y),

which satisﬁes the marginal match requirement.

Necessity Now, we show that the speciﬁc formulation of QR (Eqn. (23)) is also a necessary condition
of the marginal match condition (Eqn. (21)).

The token-level target distribution can be simpliﬁed as

PQR(yt | yt−1

1

) =

exp (cid:0)QR(yt−1
w∈W exp (cid:0)QR(yt−1

1

1

, yt)/τ (cid:1)

(cid:80)

, w)/τ (cid:1) = exp

(cid:18) QR(yt−1

1

, yt) − VR(yt−1

)

1

(cid:19)

.

τ

Suppose Eqn. (21) is true. For any y ∈ Y − and t ≤ |y| and deﬁne y(cid:48) = yt

1+eos and y(cid:48)(cid:48) = yt−1

1 +eos.

Obviously, it follows y(cid:48), y(cid:48)(cid:48) ∈ Y. Also, by deﬁnition,

PR(y(cid:48)) = PR(eos | yt
PR(y(cid:48)(cid:48)) = PR(eos | yt−1

1) × PR(yt | yt−1
1
) × PR(yt−1
)

1

1

) × PR(yt−1

)

1

Then, consider the ratio

exp

(cid:18) R(y(cid:48)) − R(y(cid:48)(cid:48))
τ

= exp

PR(y(cid:48))
PR(y(cid:48)(cid:48))
(cid:19)

=

PR(eos | yt

PR(eos | yt−1

1) × PR(yt | yt−1

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
PR(yt−1
) ×
)
1
(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
PR(yt−1
)
1
(cid:19)
1, eos) − VR(yt
1)

) ×

1

1

(cid:18) QR(yt

(cid:46)

exp

(cid:18) QR(yt−1

1

τ

, eos) −
τ

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
VR(yt−1
)

1

(cid:19)

(cid:18) QR(yt−1

1

× exp

, yt) −
τ

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
VR(yt−1
)

1

(cid:19)

R(y(cid:48)) − R(y(cid:48)(cid:48)) = QR(yt

1, eos) − QR(yt−1

1

, eos) − VR(yt

1) + QR(yt−1

1

, yt).

Now, by the terminal condition (Eqn. (22)), we essentially have

QR(yt
QR(yt−1

1, eos) = R(yt
, eos) = R(yt−1

1 + eos) − R(yt
1) = 0
1 + eos) − R(yt−1

1

1

) = 0

Thus, it follows

R(y(cid:48)) − R(y(cid:48)(cid:48)) = QR(yt−1
, yt) = R(yt

⇐⇒ QR(yt−1

1

, yt) − VR(yt
1)
) + τ log

1) − R(yt−1

1

1

exp (cid:0)QR(yt

1, w)/τ (cid:1) ,

(cid:88)

w∈W

which completes the proof.

Corollary 1. Please refer to §3.2 for the Corollary.

Proof. Similarly, we drop the dependency on x∗ and y∗ to avoid clutter. We ﬁrst prove the equivalence
of Q∗(yt−1

, yt) with QR(yt−1

, yt) by induction.

1

1

• Base case: When t = T , for any y ∈ Y, yT can only be eos. So, by deﬁnition, we have

V ∗(yT −1
1
(cid:88)

, eos)

) = Q∗(yT −1
(cid:16)

1
Q∗(yT −1
1

exp

(cid:17)

, a)/τ

⇐⇒ τ log

a∈W
=⇒ Q∗(yT −1

1

, a) = −∞, ∀a (cid:54)= eos.

= Q∗(yT −1

, eos)

1

Q∗(yT −1
1

, yT ) =

(cid:40)

r(yT −1
1
−∞,

, eos),

if yT = eos
otherwise

Hence,

For the ﬁrst case, it directly follows

Q∗(yT −1
1

, eos) = r(yT −1

1

, eos) = R(yT −1

1 + eos) − R(yT −1

1

) = QR(yT −1

1

, eos).

For the second case, since only eos is allowed to be generated, the target distribution PQR should
be a single-point distribution at eos. This is equivalent to deﬁne

QR(yT −1
1

, a) = −∞, ∀a (cid:54)= eos,

which proves the second case. Combining the two cases, it concludes

Q∗(yT −1
1

, a) = QR(yT −1

1

, a), ∀y ∈ Y, a ∈ W.

• Induction step: When 0 < t < T , assume the equivalence holds when k > t, i.e.,

Q∗(yk−1

, w) = QR(yk−1

1

1

, w), ∀k > t, w ∈ W.

Then,

Q∗(yt−1

, yt) = r(yt−1

1

1

, yt) + γ E
s(cid:48)∼ρs

[α log

(cid:88)

exp (cid:0)Q∗(s(cid:48), a)/α(cid:1)]

= r(yt−1

1

, yt) + τ log

a∈A
exp (cid:0)Q∗(yt

1, a)/τ (cid:1)

= r(yt−1

1

, yt) + τ log

exp (cid:0)QR(yt

1, a)/τ (cid:1)

(Q∗(yk

1, a) = QR(yk

1, a) for k ≥ t)

(α = τ, A = W)

(cid:88)

a∈W
(cid:88)

a∈W

= QR(yt−1

1

, yt).

Thus, Q∗(yt−1

, yt) = QR(yt−1

1

1

, yt) holds for t ∈ [1, T ].

With the equivalence between QR and Q∗, we can easily prove V ∗ = VR and π∗ = PQR,
exp (cid:0)Q∗(yt−1

V ∗(yt−1

, a)/α(cid:1)

) = α log

(cid:88)

1

exp (cid:0)Q∗(yt−1

, a)/τ (cid:1)

(α = τ, A = W)

π∗(yt | yt−1

1

) =

(cid:80)

1

1

a∈A
(cid:88)

= τ log

a∈W
= VR(yt−1
)

, yt)/τ (cid:1)

1

1
exp (cid:0)Q∗(yt−1
w∈W exp (cid:0)Q∗(yt−1
exp (cid:0)QR(yt−1
w∈W exp (cid:0)QR(yt−1

1

1

1

, yt)/τ (cid:1)

, yt)/τ (cid:1)

, yt)/τ (cid:1)

=

(cid:80)

= PQR(yt | yt−1

1

)

A.2 Other Proofs

We derive the equivalence between the VAML’s objective (Eqn. (17)) and the RAML’s objective (Eqn.
(2)).
CE (cid:0)PQφ(cid:107)Pθ
= − E

(cid:1)

log Pθ(y)

y∼PQφ

= − E

y∼PQφ

|y|
(cid:88)

t=1

log Pθ(yt | yt−1

)

1

T
(cid:88)

t=1

E
yt
1∼PQφ

(Y t
1 )

log Pθ(yt | yt−1

)

1

= −

=

=

=

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

(cid:34)

−

E
yt−1
1 ∼PQφ

(Yt−1
1

)

yt∼PQφ

E
(Yt|yt−1

1

)

log Pθ(yt | yt−1

1

(cid:35)
)

E
yt−1
1 ∼PQφ

(Yt−1
1

)

CE (cid:0)PQφ(Yt | yt−1

1

)(cid:107)Pθ(Yt | yt−1

1

)(cid:1)

E
yt−1
1 ∼PQφ

(Yt−1
1

)

yt∈W

(cid:88)

PQφ(yt | yt−1

1

) CE (cid:0)PQφ(Yt | yt−1
1
(cid:123)(cid:122)
(cid:124)
const. w.r.t. yt

)(cid:107)Pθ(Yt | yt−1

1

)(cid:1)
(cid:125)

(T is longest possible length)

(cid:2)CE (cid:0)PQφ(Yt | yt−1

1

)(cid:107)Pθ(Yt | yt−1

1

)(cid:1)(cid:3)

=

T
(cid:88)

t=1

E
yt−1
1 ∼PQφ
(cid:124)

E
(W |yt−1
)
(cid:125)

1

yt∈PQφ

(Yt−1
1

)
(cid:123)(cid:122)
yt
1∼PQφ

E

(Yt
1)

=

T
(cid:88)

t=1

E
yt
1∼PQφ

(Yt
1)

=

E
y∼PQφ

(Y)

|y|
(cid:88)

t=1

(cid:2)CE (cid:0)PQφ(Yt | yt−1

1

)(cid:107)Pθ(Yt | yt−1

1

)(cid:1)(cid:3)

CE (cid:0)PQφ(Yt | yt−1

1

)(cid:107)Pθ(Yt | yt−1

1

)(cid:1)

B Implementation Details

B.1 RAML
In RAML, we want to optimize the cross entropy CE (PR(Y | x∗, y∗)(cid:107)Pθ(Y | x∗)). As discussed in
§2.1, directly sampling from the exponentiated pay-off distribution PR(Y | x∗) is impractical. Hence,
normalized importance sampling has been exploited in previous work (Norouzi et al., 2016; Ma et al.,
2017). Deﬁne the proposal distribution to be PS(Y | x∗, y∗). Then, the objective can be rewritten as

CE (PR(Y | x∗, y∗)(cid:107)Pθ(Y | x∗)) = −

E
y∼PS (Y|x∗,y∗)

PR(y | x∗, y∗)
PS(y | x∗, y∗)

log Pθ(y | x∗)

exp(R(y,y∗)/τ )
˜PS (y|x∗,y∗)

= −

E
y∼PS (Y|x∗,y∗)

= −

E
y∼PS (Y|x∗,y∗)

exp(R(y(cid:48),y∗)/τ )
˜PS (y(cid:48)|x∗,y∗)

Ey(cid:48)∼PS (Y|x∗,y∗)
w(y, y∗)
Ey(cid:48)∼PS (Y|x∗,y∗) w(y(cid:48), y∗)

log Pθ(y | x∗)

log Pθ(y | x∗)

≈ −

M
(cid:88)

i=1

w(y(i), y∗)
i=1 w(y(i), y∗)

(cid:80)M

log Pθ(y(i) | x∗),

where w(y, y∗) = exp(R(y,y∗)/τ )
is the unnormalized importance weight, ˜PS denotes the unnormalized
˜PS (y|x∗,y∗)
˜PS
Z , M is the number of samples used, and y(i) is the i-th sample drawn from the

probability of PS =
proposal distribution PS(Y | x∗, y∗).

With importance sampling, the problem turns to what proposal distribution we should use. In the
original work (Norouzi et al., 2016), the proposal distribution is deﬁned by the hamming distance as
used. Ma et al. (2017) ﬁnd that it sufﬁces to perform N -gram replacement of the reference sentence.
Speciﬁcally, PS(Y | x∗, y∗) can be a uniform distribution deﬁned on set Yngram where Yngram is obtained
by randomly replacing an n-gram of y∗ (n ≤ 4).

In this work, we adapt the simple n-gram replacement distribution, denoted as Pngram(Y | x∗, y∗),

which simpliﬁes the RAML objective into

−

min
θ

M
(cid:88)

i=1

exp (cid:0)R(y(i), y∗)/τ (cid:1)
i=1 exp (cid:0)R(y(i), y∗)/τ (cid:1) log Pθ(y(i) | x∗)

(cid:80)M

Following Ma et al. (2017), we make sure the reference sequence is always among the M samples used.

B.2 VAML

As discussed in §4, the VAML training consists of two phases:

• In the ﬁrst phase, Soft Q-Learning is used to train Qφ based on Eqn. (16). Since Soft Q-Learning

accepts off-policy trajectories, in this work, we use two types of off-policy sequences:

1. The ﬁrst type is simply the ground-truth sequence, which provides strong learning signals.

2. The second type of sequences is actually drawn from the same n-gram replacement distribution
discussed above. The reason is that in the second training phase, such n-gram replaced trajecto-
ries will be used. Since the learned Qφ won’t be perfect, we hope the exposing Qφ with these
trajectories can improve its accuracy on them, making the second phase of training easier.

Algorithm 1 summarizes the ﬁrst phase.

Algorithm 1 VAML Phase 1: Soft Q-Learning to approximate Q∗
Require: A Q-function approximator Qφ with parameter φ, and the hyper-parameters τ , M .

1: while Not Converged do
2:

Receive a random example (x∗, y∗).
Sample M − 1 sequences {y(i)}M −1
i=1
Compute all the rewards r(yt−1
, yt; y∗) for each y ∈ {y(i)}M
Compute the target Q-values for each y ∈ {y(i)}M

from Pngram(Y | x∗, y∗) and let y(M ) = y∗.
i=1 and t = 1, . . . , |y|.

i=1 and t = 1, . . . , |y|

1

3:

4:

5:

ˆQφ(yt−1

1

, yt; y∗) = r(yt−1

, yt; y∗) + τ log

1

exp (cid:0)Qφ(yt

1, w; y∗)/τ (cid:1) .

(cid:88)

w∈W

6:

Compute the Soft-Q Learning loss

LSoftQ =

1
M

M
(cid:88)

|y(i)|
(cid:88)

i=1

t=1

(cid:13)
(cid:13)Qφ(y(i)t−1
(cid:13)

1

, y(i)
t

; y∗) − ˆQφ(y(i)t−1

1

, y(i)
t

(cid:13)
2
; y∗)
(cid:13)
(cid:13)
2

.

Update Qφ according to the loss LSoftQ.

7:
8: end while

• Once the Qφ is well trained in the ﬁrst phase, the second phase is to minimize the cross entropy

CE (cid:0)PQφ(Y | x∗, y∗)(cid:107)Pθ(Y | x∗)(cid:1) based on Eqn. (17), i.e.,

min
θ

E
y∼PQφ





|y|
(cid:88)

t=1

CE (cid:0)PQφ(Yt | yt−1

1

)(cid:107)Pθ(Yt | yt−1

1

)(cid:1)

 .



Ideally, we would like to directly sample from PQφ, and perform the optimization. However, we ﬁnd
samples from PQφ are quite similar to each other. We conjecture this results from both the imperfect
training in the ﬁrst phase, and the intrinsic difﬁculty of getting diverse samples from an exponentially
large space when the distribution is high concentrated.

Nevertheless, for this work, we fall back to the same importance sampling method as used in RAML
and use the n-gram replacement distribution as the proposal. Hence, the objective becomes

CE (cid:0)PQφ(Yt | yt−1

1

)(cid:107)Pθ(Yt | yt−1

1





)(cid:1)

E
y∼PQφ

|y|
(cid:88)





t=1


= E

y∼Pngram

w(y, y∗)
Ey(cid:48)∼Pngram(Y|x∗,y∗) w(y(cid:48), y∗)



|y|
(cid:88)

t=1

≈

M
(cid:88)

i=1

exp (cid:0)R(y(i), y∗)/τ (cid:1)
i=1 exp (cid:0)R(y(i), y∗)/τ (cid:1)

(cid:80)M

|y(i)|
(cid:88)





t=1

CE (cid:0)PQφ(Yt | yt−1

1

)(cid:107)Pθ(Yt | yt−1

1

)(cid:1)

(cid:16)

CE

PQφ(Yt | y(i)t−1

1

)(cid:107)Pθ(Yt | y(i)t−1

)

1

(cid:17)

 .







However, we found directly using this objective does not yield improved performance compared to
RAML, mostly likely due to some erratic estimations of Qφ. Thus, we only use this objective for

some step with certain probability κ ∈ (0, 1), leaving others trained by MLE. Formally, deﬁne

Jκ(yt

1) =

E
z∼Bernoulli(κ)

(cid:2)zCE (cid:0)PQφ(Yt | yt−1

1

)(cid:107)Pθ(Yt | yt−1

1

)(cid:1) − (1 − z) log Pθ(yt | yt−1

)(cid:3) ,

1

the VAML objective practically used is

M
(cid:88)

i=1

exp (cid:0)R(y(i), y∗)/τ (cid:1)
i=1 exp (cid:0)R(y(i), y∗)/τ (cid:1)

(cid:80)M

min
θ



Jκ(y(i)t
1)

 .

|y(i)|
(cid:88)





t=1

Algorithm 2 VAML Phase 2: Sequence model training with token-level target
Require: A sequence prediction model Pθ with parameter θ, a pre-trained Q-function approximator Qφ,

Algorithm 2 summarizes the second phase.

and hyper-parameters τ , M , κ

1: while Not Converged do
2:

Receive a random example (x∗, y∗).
Sample M − 1 sequences {y(i)}M −1
i=1
Compute the VAML loss using

3:

4:

Update Pθ according to the loss LVAML.

5:
6: end while

B.3 ERAC

from Pngram(Y | x∗, y∗) and let y(M ) = y∗.

LVAML =

M
(cid:88)

i=1

exp (cid:0)R(y(i), y∗)/τ (cid:1)
i=1 exp (cid:0)R(y(i), y∗)/τ (cid:1)

(cid:80)M



Jκ(y(i)t
1)

 .

|y(i)|
(cid:88)





t=1

Following Bahdanau et al. (2016), we ﬁrst pre-train the actor, then train the critic with the ﬁxed actor
and ﬁnally ﬁne-tune them together. The speciﬁc procedure for training ERAC is

• Pre-training the actor using maximum likelihood training

• Pre-training the critic using Algorithm 3 with the actor ﬁxed

• Fine-tuning both the actor and critic with Algorithm 3

B.4 Hyper-parameters

RAML & VAML The hyper-parameters for RAML and VAML training are summarized in Tab. 5.
We set the gradient clipping value to 5.0 for both the Q-function approximator Qφ and the sequence
prediction model Pθ, except for the sequence prediction model in the captioning task where the gradient
clipping value is set to 1.0.

AC & ERAC As described in §B.3, the training using AC and ERAC involves three phases. The hyper-
parameters used for ERAC training in each phase are summarized in Table 6. In all phases, the learning
rate is halved when there is no improvement on the validation set. We use the same hyper-parameters
for AC training, except that the entropy regularization coefﬁcient τ is 0. Similar to the VAML case,
the gradient clipping value is set to 5.0 for both the actor and the critic, except that we set the gradient
clipping value to 1.0 for the actor in the captioning task.

, yt; y∗) and an actor πθ(w | yt

1) with weights φ and θ respectively, and

Algorithm 3 ERAC Algorithm
Require: A critic Qφ(yt−1

1

hyper-parameters τ , β, λvar, λmle

1: Initialize delayed target critic Q ¯φ with the same weights: ¯φ = φ.
2: while Not Converged do
3:

Receive a random example (x∗, y∗).
Generate a sequence y from πθ.
Compute the rewards r(yt−1
Compute targets for the critic

1

4:

5:

6:

, yt; y∗) for t = 1, . . . , |y|.

ˆQ ¯φ(yt−1

1

, yt; y∗) = r(yt−1

, yt) + τ H(πθ(· | yt

1)) +

1

πθ(w | yt

1)Q ¯φ(yt

1, w; y∗).

(cid:88)

w∈W

(cid:105)2

(cid:88)

w∈W

7:

Compute loss for critic

|y|
(cid:88)

(cid:104)

t=1

8:

Compute loss for actor





|y|
(cid:88)

(cid:88)

t=1

w∈W

Lcritic =

Qφ(yt−1

1

, yt; y∗) − ˆQ ¯φ(yt−1

1

, yt; y∗)

+ λvar

(cid:2)Qφ(yt−1

1

, w; y∗) − ¯Qφ(yt−1

1

; y∗)(cid:3)2

,

where

¯Qφ(yt−1

1

; y∗) =

Qφ(yt−1

1

, w(cid:48); y∗)

1
|W|

(cid:88)

w(cid:48)∈W

Lactor = −

πθ(w | yt−1

)Qφ(yt−1

, w; y∗) + τ H(πθ(· | yt−1

)) + λmle

1

1

1

log πθ(y∗

t | y∗t−1

1

)





|y∗|
(cid:88)

t=1

9:

10:

Update critic according to the loss Lcritic.
If actor is not ﬁxed, update actor according to the loss Lactor
Update delayed target critic: ¯φ = βφ + (1 − β) ¯φ

11:
12: end while

Hyper-parameters VAML-1 VAML-2 RAML VAML-1 VAML-2 RAML

Machine Translation

Image Captioning

optimizer
learning rate
batch size
M
τ
κ

Adam
0.001
50
5
0.4
N.A.

SGD
0.6
42
5
0.4
0.2

SGD
0.6
42
5
0.4
N.A.

Adam
0.001
32 × 5
2
0.7
N.A.

SGD
0.5
32 × 5
6
0.7
0.1

SGD
0.5
32 × 5
6
0.7
N.A.

Table 5: Optimization related hyper-parameters of RAML and VAML for two tasks. “VAML-1” and
“VAML-2” indicate the phase 1 and phase 2 of VAML training respectively. “N.A.” means not applicable.
“32 × 5” indicates using 32 images each with 5 reference captions.

C Comparison with Previous Work

The detailed comparison with previous work in shown in Table 7. Under different comparable architec-
tures (1 layer or 2 layers), ERAC outperforms previous algorithms with a clear margin.

Hyper-parameters

MT w/ input feeding MT w/o input feeding

Image Captioning

optimizer
learning rate
batch size

optimizer
learning rate
batch size
τ (entropy regularization)
β (target net speed)
λvar (smoothness)

optimizer
learning rate
batch size
τ (entropy regularization)
β (target net speed)
λvar (smoothness)
λMLE

Pre-train Actor

Pre-train Critic

Joint Training

SGD
0.6
50

Adam
0.001
50
0.05
0.001
0.001

Adam
0.0001
50
0.05
0.001
0.001
0.1

SGD
0.6
50

Adam
0.001
50
0.04
0.001
0.001

Adam
0.0001
50
0.04
0.001
0.001
0.1

Table 6: Hyper-parameters for ERAC training

SGD
0.5
32 × 5

Adam
0.001
32 × 5
0.01
0.001
0.001

Adam
0.0001
32 × 5
0.01
0.001
0.001
0.1

Algorithm

MIXER (Ranzato et al., 2015)
BSO (Wiseman and Rush, 2016)
Q(BLEU) (Li et al., 2017)
AC (Bahdanau et al., 2016)
RAML (Ma et al., 2017)

Encoder

NN Type

1-layer CNN

Size

256

1-layer BiLSTM 128 × 2
1-layer BiLSTM 128 × 2
256 × 2
1-layer BiGRU
1-layer BiLSTM 256 × 2

1-layer LSTM 256
1-layer LSTM 256
1-layer LSTM 256
1-layer GRU
256
1-layer LSTM 256

NN Type

Size Attention

Input Feed

Decoder

1-layer BiLSTM 128 × 2
1-layer BiLSTM 128 × 2

1-layer LSTM 256
1-layer LSTM 256

NPMT (Huang et al., 2017)
NPMT+LM (Huang et al., 2017)

2-layer BiGRU
2-layer BiGRU

256 × 2
256 × 2

2-layer LSTM 512
2-layer LSTM 512

N.A.
N.A.

2-layer BiLSTM 256 × 2

2-layer LSTM 512

Dot-Prod

VAML
ERAC

ERAC

Dot-Prod
Dot-Prod
Dot-Prod
MLP
Dot-Prod

Dot-Prod
Dot-Prod

BLEU

20.73
27.9
28.3
28.53
28.77

28.94
29.36

29.92
30.08

30.85

N
Y
Y
Y
Y

Y
Y

N.A.
N.A.

Y

Table 7: Comparison of algorithms with detailed architecture information on the IWSTL 2014 dataset for MT.

From Credit Assignment to Entropy Regularization:
Two New Algorithms for Neural Sequence Prediction

Zihang Dai∗ , Qizhe Xie∗ , Eduard Hovy
Language Technologies Institute
Carnegie Mellon University
{dzihang, qizhex, hovy}@cs.cmu.edu

8
1
0
2
 
r
p
A
 
9
2
 
 
]
L
C
.
s
c
[
 
 
1
v
4
7
9
0
1
.
4
0
8
1
:
v
i
X
r
a

Abstract

In this work, we study the credit as-
signment problem in reward augmented
maximum likelihood (RAML) learning,
and establish a theoretical equivalence
between the token-level counterpart of
RAML and the entropy regularized rein-
forcement learning. Inspired by the con-
nection, we propose two sequence pre-
diction algorithms, one extending RAML
with ﬁne-grained credit assignment and
the other improving Actor-Critic with a
systematic entropy regularization. On two
benchmark datasets, we show the pro-
posed algorithms outperform RAML and
Actor-Critic respectively, providing new
alternatives to sequence prediction.

1

Introduction

Modeling and predicting discrete sequences is the
central problem to many natural language process-
ing tasks. In the last few years, the adaption of re-
current neural networks (RNNs) and the sequence-
to-sequence model (seq2seq) (Sutskever et al.,
2014; Bahdanau et al., 2014) has led to a wide
range of successes in conditional sequence pre-
diction, including machine translation (Sutskever
et al., 2014; Bahdanau et al., 2014), automatic
summarization (Rush et al., 2015), image cap-
tioning (Karpathy and Fei-Fei, 2015; Vinyals
et al., 2015; Xu et al., 2015) and speech recogni-
tion (Chan et al., 2016).

Despite the distinct evaluation metrics for the
aforementioned tasks, the standard training algo-
rithm has been the same for all of them. Specif-
ically, the algorithm is based on maximum likeli-
hood estimation (MLE), which maximizes the log-

∗ Equal contribution.

likelihood of the “ground-truth” sequences empir-
ically observed.1

While largely effective, the MLE algorithm has
two obvious weaknesses. Firstly, the MLE train-
ing ignores the information of the task speciﬁc
metric. As a result, the potentially large discrep-
ancy between the log-likelihood during training
and the task evaluation metric at test time can lead
to a suboptimal solution. Secondly, MLE can suf-
fer from the exposure bias, which refers to the
phenomenon that the model is never exposed to
its own failures during training, and thus cannot
recover from an error at test time. Fundamen-
tally, this issue roots from the difﬁculty in statisti-
cally modeling the exponentially large space of se-
quences, where most combinations cannot be cov-
ered by the observed data.

To tackle these two weaknesses, there have been
various efforts recently, which we summarize into
two broad categories:

• A widely explored idea is to directly opti-
mize the task metric for sequences produced by
the model, with the speciﬁc approaches rang-
ing from minimum risk training (MRT) (Shen
et al., 2015) and learning as search optimization
(LaSO) (Daum´e III and Marcu, 2005; Wise-
man and Rush, 2016) to reinforcement learn-
ing (RL) (Ranzato et al., 2015; Bahdanau et al.,
2016).
In spite of the technical differences,
the key component to make these training al-
gorithms practically efﬁcient is often a delicate
credit assignment scheme, which transforms
the sequence-level signal into dedicated smaller
units (e.g., token-level or chunk-level), and al-
locates them to speciﬁc decisions, allowing for
efﬁcient optimization with a much lower vari-
ance. For instance, the beam search optimiza-

1In this work, we use the terms “ground-truth” and “refer-
ence” to refer to the empirical observations interchangeably.

tion (BSO) (Wiseman and Rush, 2016) utilizes
the position of margin violations to produce sig-
nals to the speciﬁc chunks, while the actor-critic
(AC) algorithm (Bahdanau et al., 2016) trains a
critic to enable token-level signals.

• Another alternative idea is to construct a task
metric dependent target distribution, and train
the model to match this task-speciﬁc target in-
stead of the empirical data distribution. As a
typical example, the reward augmented maxi-
mum likelihood (RAML) (Norouzi et al., 2016)
deﬁnes the target distribution as the exponen-
tiated pay-off (sequence-level reward) distribu-
tion. This way, RAML not only can incorporate
the task metric information into training, but it
can also alleviate the exposure bias by expos-
ing imperfect outputs to the model. However,
RAML only works on the sequence-level train-
ing signal.

In this work, we are intrigued by the question
whether it is possible to incorporate the idea of
ﬁne-grained credit assignment into RAML. More
speciﬁcally, inspired by the token-level signal used
in AC, we aim to ﬁnd the token-level counter-
part of the sequence-level RAML, i.e., deﬁning
a token-level target distribution for each auto-
regressive conditional factor to match. Motived by
the question, we ﬁrst formally deﬁne the desider-
ata the token-level counterpart needs to satisfy and
derive the corresponding solution (§2). Then, we
establish a theoretical connection between the de-
rived token-level RAML and entropy regularized
RL (§3). Motivated by this connection, we pro-
pose two algorithms for neural sequence predic-
tion, where one is the token-level extension to
RAML, and the other a RAML-inspired improve-
ment to the AC (§4). We empirically evaluate the
two proposed algorithms, and show different lev-
els of improvement over the corresponding base-
line. We further study the importance of vari-
ous techniques used in our experiments, providing
practical suggestions to readers (§6).

2 Token-level Equivalence of RAML

We ﬁrst introduce the notations used throughout
the paper. Firstly, capital letters will denote ran-
dom variables and lower-case letters are the val-
ues to take. As we mainly focus on conditional
sequence prediction, we use x for the conditional
input, and y for the target sequence. With y denot-
ing a sequence, yj
i then denotes the subsequence

from position i to j inclusively, while yt denotes
the single value at position t. Also, we use |y| to
indicate the length of the sequence. To emphasize
the ground-truth data used for training, we add su-
perscript ∗ to the input and target, i.e., x∗ and y∗.
In addition, we use Y to denote the set of all pos-
sible sequences with one and only one eos symbol
at the end, and W to denote the set of all possible
symbols in a position. Finally, we assume length
of sequences in Y is bounded by T .

2.1 Background: RAML

As discussed in §1, given a ground-truth pair
(x∗, y∗), RAML deﬁnes the target distribution us-
ing the exponentiated pay-off of sequences, i.e.,

PR(y | x∗, y∗) =

exp (R(y; y∗)/τ )
y(cid:48)∈Y exp (R(y(cid:48); y∗)/τ )

,

(cid:80)

(1)

where R(y; y∗) is the sequence-level reward, such
as BLEU score, and τ is the temperature hyper-
parameter controlling the sharpness. With the deﬁ-
nition, the RAML algorithm simply minimizes the
cross entropy (CE) between the target distribution
and the model distribution Pθ(Y | x∗), i.e.,

CE (cid:0)PR(Y | x∗, y∗)(cid:107)Pθ(Y | x∗)(cid:1) .

(2)

min
θ

Note that, this is quite similar to the MLE training,
except that the target distribution is different. With
the particular choice of target distribution, RAML
not only makes sure the ground-truth reference re-
mains the mode, but also allows the model to ex-
plore sequences that are not exactly the same as
the reference but have relatively high rewards.

Compared to algorithms trying to directly opti-
mize task metric, RAML avoids the difﬁculty of
tracking and sampling from the model distribution
that is consistently changing. Hence, RAML en-
joys a much more stable optimization without the
need of pretraining. However, in order to opti-
mize the RAML objective (Eqn. (2)), one needs
to sample from the exponentiated pay-off distribu-
tion, which is quite challenging in practice. Thus,
importance sampling is often used (Norouzi et al.,
2016; Ma et al., 2017). We leave the details of the
practical implementation to Appendix B.1.

2.2 Token-level Target Distribution

Despite the appealing properties, RAML only op-
erates on the sequence-level reward. As a result,
the reward gap between any two sequences cannot
be attributed to the responsible decisions precisely,

t=1 Pθ(yt

which often leads to a low sample efﬁciency. Ide-
ally, since we rely on the auto-regressive factor-
ization Pθ(y | x∗) = (cid:81)|y|
, x∗),
the optimization would be much more efﬁcient if
we have the target distribution for each token-level
factor Pθ(Yt | yt−1
, x∗) to match. Conceptually,
this is exactly how the AC algorithm improves
upon the vanilla sequence-level REINFORCE al-
gorithm (Ranzato et al., 2015).

| yt−1
1

1

With this idea in mind, we set out to ﬁnd such
a token-level target. Firstly, we assume the token-
level target shares the form of a Boltzmann distri-
bution but parameterized by some unknown nega-
tive energy function QR, i.e.,2

1

1

1

(cid:80)

, y∗) =

, yt; y∗)/τ (cid:1)

exp (cid:0)QR(yt−1
w∈W exp (cid:0)QR(yt−1

PQR (yt | yt−1

, w; y∗)/τ (cid:1) .
(3)
Intuitively, QR(yt−1
, w; y∗) measures how much
future pay-off one can expect if w is generated,
given the current status yt−1
and the reference y∗.
1
This quantity highly resembles the action-value
function (Q-function) in reinforcement learning.
As we will show later, it is indeed the case.

1

Before we state the desiderata for QR, we need
to extend the deﬁnition of R in order to evaluate
the goodness of an unﬁnished partial prediction,
i.e., sequences without an eos sufﬁx. Let Y − be
the set of unﬁnished sequences, following Bah-
danau et al. (2016), we deﬁne the pay-off function
R for a partial sequence ˆy ∈ Y −, |ˆy| < T as

R(ˆy; y∗) = R(ˆy + eos; y∗),

(4)

where the + indicates string concatenation.

With the extension, we are ready to state two

requirements for QR:

1. Marginal match: For PQR to be the token-level
equivalence of PR, the sequence-level marginal
distribution induced by PQR must match PR,
i.e., for any y ∈ Y,

|y|
(cid:89)

t=1

PQR (yt | yt−1

1

) = PR(y).

(5)

Note that there are inﬁnitely many QR’s satisfy-
ing Eqn. (5), because adding any constant value
to QR does not change the Boltzmann distribu-
tion, known as shift-invariance w.r.t. the energy.

2. Terminal condition: Secondly, let’s consider
the value of QR when emitting an eos symbol to
immediately terminate the generation. As men-
tioned earlier, QR measures the expected future
pay-off. Since the emission of eos ends the gen-
eration, the future pay-off can only come from
the immediate increase of the pay-off. Thus, we
require QR to be the incremental pay-off when
producing eos, i.e.

QR(ˆy, eos; y∗) = R(ˆy + eos; y∗) − R(ˆy; y∗),
for any ˆy ∈ Y −. Since Eqn. (6) enforces the
absolute of QR at a point, it also solves the am-
biguity caused by the shift-invariance property.

(6)

Based on the two requirements, we can derive the
form QR, which is summarized by Proposition 1.
Proposition 1. PQR and QR satisfy requirements
(5) and (6) if and only if for any ground-truth pair
(x∗, y∗) and any sequence prediction y ∈ Y,

QR(yt−1

1

1; y∗) − R(yt−1
, yt; y∗) = R(yt
(cid:16)
(cid:88)
QR(yt

1
1, w; y∗)/τ

exp

; y∗)
(cid:17)

,

+ τ log

(7)

w∈W

when t < |y|, and otherwise, i.e., when t = |y|

QR(yt−1

1

, yt; y∗) = R(yt

1; y∗) − R(yt−1

1

; y∗).

(8)

Proof. See Appendix A.1.

Note that, instead of giving an explicit form for
the token-level target distribution, Proposition 1
only provides an equivalent condition in the form
of an implicit recursion. Thus, we haven’t ob-
tained a practical algorithm yet. However, as we
will discuss next, the recursion has a deep connec-
tion to entropy regularized RL, which ultimately
inspires our proposed algorithms.

3 Connection to Entropy-regularized RL

Before we dive into the connection, we ﬁrst give
a brief review of the entropy-regularized RL. For
an in-depth treatment, we refer readers to (Ziebart,
2010; Schulman et al., 2017).

3.1 Background: Entropy-regularized RL

Following the standard convention of RL, we de-
note a Markov decision process (MDP) by a tu-
ple M = (S, A, ps, r, γ), where S, A, ps, r, γ are
the state space, action space, transition probabil-
ity, reward function and discounting factor respec-
tively.3

2To avoid clutter, the conditioning on x∗ will be omitted

3In sequence prediction, we are only interested in the pe-

in the sequel, assuming it’s clear from the context.

riodic (ﬁnite horizon) case.

(9)

• the reward function r corresponds to the in-

cremental pay-off deﬁned in Eqn. (13),

Based on the notation,

the goal of entropy-
regularized RL augments is to learn a policy π(at |
st) which maximizes the discounted expected fu-
ture return and causal entropy (Ziebart, 2010), i.e.,

max
π

(cid:88)

t

E
st∼ρs,at∼π(·|st)

γt−1[r(st, at) + αH(π(· | st))],

where H denotes the entropy and α is a hyper-
parameter controlling the relative importance be-
tween the reward and the entropy.
Intuitively,
compared to standard RL, the extra entropy term
encourages exploration and promotes multi-modal
behaviors. Such properties are highly favorable in
a complex environment.

Given an entropy-regularized MDP, for any
ﬁxed policy π, the state-value function V π(s) and
the action-value function Qπ can be deﬁned as

V π(s) =

E
a∼π(·|s)

[Qπ(s, a)] + αH(π(· | s)),

Qπ(s, a) = r(s, a) + E

[γV π(s(cid:48))].

s(cid:48)∼ρs

With the deﬁnitions above, it can further be
proved (Ziebart, 2010; Schulman et al., 2017) that
the optimal state-value function V ∗, the action-
value function Q∗ and the corresponding optimal
policy π∗ satisfy the following equations

V ∗(s) = α log

exp (cid:0)Q∗(s, a)/α(cid:1) ,

(cid:88)

a∈A

Q∗(s, a) = r(s, a) + γ E

[V ∗(s(cid:48))],

π∗(a | s) =

(cid:80)

s(cid:48)∼ρs
exp (Q∗(s, a)/α)
a(cid:48)∈A exp (Q∗(s, a(cid:48))/α)

.

(10)

(11)

(12)

Here, Eqn.
(10) and (11) are essentially the
entropy-regularized counterparts of the optimal
Bellman equations in standard RL. Following pre-
vious literature, we will refer to Eqn. (10) and (11)
as the optimal soft Bellman equations, and the V ∗
and Q∗ as optimal soft value functions.

3.2 An RL Equivalence of the Token-level

RAML

To reveal the connection, it is convenient to deﬁne
the incremental pay-off

r(yt−1
1

, yt; y∗) = R(yt

1; y∗) − R(yt−1

1

; y∗),

(13)

and the last term of Eqn. (7) as

VR(yt

1; y∗) = τ log

(cid:88)

(cid:16)
QR(yt

exp

1, w; y∗)/τ

(cid:17)

(14)

w∈W

1

1

, yt; y∗) + VR(yt

, yt; y∗) = r(yt−1

Substituting the two deﬁnitions into Eqn. (7), the
recursion simpliﬁes as
QR(yt−1

1; y∗). (15)
Now, it is easy to see that the Eqn. (14) and (15),
which are derived from the token-level RAML,
highly resemble the optimal soft Bellman equa-
tions (10) and (11) in entropy-regularized RL. The
following Corollary formalizes the connection.
Corollary 1. For any ground-truth pair (x∗, y∗),
the recursion speciﬁed by Eqn. (13), (14) and (15)
is equivalent to the optimal soft Bellman equation
of a “deterministic” MDP in entropy-regularized
reinforcement learning, denoted as MR, where
• the state space S corresponds to Y −,
• the action space A corresponds to W,

• the transition probability ρs is a deterministic

process deﬁned by string concatenation

• the discounting factor γ = 1,

• the entropy hyper-parameter α = τ ,

• and a period terminates either when eos is
emitted or when its length reaches T and we
enforce the generation of eos.

Moreover, the optimal soft value functions V ∗ and
Q∗ of the MDP exactly match the VR and QR de-
ﬁned by Eqn. (14) and (15) respectively. The op-
timal policy π∗ is hence equivalent to the token-
level target distribution PQR.

Proof. See Appendix A.1.

The connection established by Corollary 1 is

quite inspiring:

• Firstly, it provides a rigorous and generalized
view of the connection between RAML and
entropy-regularized RL. In the original work,
Norouzi et al. (2016) point out RAML can be
seen as reversing the direction of KL (Pθ(cid:107)PR),
which is a sequence-level view of the connec-
tion. Now, with the equivalence between the
token-level target PQR and the optimal Q∗, it
generalizes to matching the future action values
consisting of both the reward and the entropy.

• Secondly, due to the equivalence, if we solve
the optimal soft Q-function of the correspond-
ing MDP, we directly obtain the token-level tar-
get distribution. This hints at a practical algo-
rithm with token-level credit assignment.

• Moreover, since RAML is able to improve
upon MLE by injecting entropy, the entropy-
regularized RL counterpart of the standard AC
algorithm should also lead to an improvement
in a similar manner.

4 Proposed Algorithms

In this section, we explore the insights gained from
Corollary 1 and present two new algorithms for
sequence prediction.

4.1 Value Augmented Maximum Likelihood

The ﬁrst algorithm we consider is the token-level
extension of RAML, which we have been dis-
cussing since §2. As mentioned at the end of
§2.2, Proposition 1 only gives an implicit form of
QR, and so is the token-level target distribution
PQR (Eqn.
(3)). However, thanks to Corollary
1, we now know that QR is the same as the op-
timal soft action-value function Q∗ of the entropy-
regularized MDP MR. Hence, by ﬁnding the Q∗,
we will have access to PQR.

At the ﬁrst sight, it seems recovering Q∗ is as
difﬁcult as solving the original sequence predic-
tion problem, because solving Q∗ from the MDP is
essentially the same as learning the optimal policy
for sequence prediction. However, it is not true be-
cause QR (i.e., PQR) can condition on the correct
reference y∗. In contrast, the model distribution
Pθ can only depend on x∗. Therefore, the func-
tion approximator trained to recover Q∗ can take
y∗ as input, making the estimation task much eas-
ier. Intuitively, when recovering Q∗, we are trying
to train an ideal “oracle”, which has access to the
ground-truth reference output, to decide the best
behavior (policy) given any arbitrary (good or not)
state.

Thus, following the reasoning above, we ﬁrst
train a parametric function approximator Qφ to
search the optimal soft action value.
In this
work,
for simplicity, we employ the Soft Q-
learning algorithm (Schulman et al., 2017) to per-
form the policy optimization. In a nutshell, Soft
Q-Learning is the entropy-regularized version of
Q-Learning, an off-policy algorithm which mini-
mizes the mean squared soft Bellman residual ac-
cording to Eqn. (11). Speciﬁcally, given ground-
truth pair (x∗, y∗), for any trajectory y ∈ Y, the
training objective is

|y|
(cid:88)

(cid:104)

t=1

min
φ

Qφ(yt−1
1

, yt; y∗) − ˆQφ(yt−1

, yt; y∗)

1

, (16)

(cid:105)2

1

1

(10).

, yt; y∗) + Vφ(yt

, yt; y∗) = r(yt−1

1; y∗) = τ log (cid:80)

w∈W exp (cid:0)Qφ(yt

where ˆQφ(yt−1
1; y∗)
is the one-step look-ahead target Q-value, and
1, w; y∗)/τ (cid:1) as
Vφ(yt
deﬁned in Eqn.
In the recent instantia-
tion of Q-Learning (Mnih et al., 2015), to sta-
bilize training, the target Q-value is often esti-
mated by a separate slowly updated target net-
work. In our case, as we have access to a signif-
icant amount of reference sequences, we ﬁnd the
target network not necessary. Thus, we directly
optimize Eqn. (16) using gradient descent, and let
the gradient ﬂow through both Qφ(yt−1
, yt; y∗)
1; y∗) (Baird, 1995).
and Vφ(yt

1

After the training of Qφ converges, we ﬁx the
parameters of Qφ, and optimize the cross en-
(cid:1) w.r.t.
tropy CE (cid:0)PQφ(cid:107)Pθ
the model parameters
θ, which is equivalent to4

min
θ

E
y∼PQφ





|y|
(cid:88)

t=1

CE (cid:0)PQφ (Yt | yt−1

1

)(cid:107)Pθ(Yt | yt−1

1


)(cid:1)

 .

| yt−1
1

| yt−1
1

(17)
Compared to the of objective of RAML in Eqn.
) allows us
(2), having access to PQφ(Yt
to provide a distinct token-level target for each
) of the model.
conditional factor Pθ(Yt
While directly sampling from PR is practically in-
feasible (§2.1), having a parametric target distri-
bution PQφ makes it theoretically possible to sam-
ple from PQφ and perform the optimization. How-
ever, empirically, we ﬁnd the samples from PQφ
are not diverse enough (§6). Hence, we fall back to
the same importance sampling approach (see Ap-
pendix B.2) as used in RAML.

Finally, since the algorithm utilizes the optimal
soft action-value function to construct the token-
level target, we will refer to it as value augmented
maximum likelihood (VAML) in the sequel.

4.2 Entropy-regularized Actor Critic

The second algorithm follows the discussion at the
end of §3.2, which is essentially an actor-critic al-
gorithm based on the entropy-regularized MDP in
Corollary 1. For this reason, we name the algo-
rithm entropy-regularized actor critic (ERAC). As
with standard AC algorithm, the training process
interleaves the evaluation of current policy using
the parametric critic Qφ and the optimization of
the actor policy πθ given the current critic.

Critic Training. The critic is trained to perform
policy evaluation using the temporal difference

4See Appendix A.2 for a detailed derivation.

learning (TD), which minimizes the TD error

min
φ

E
y∼πθ

|y|
(cid:88)

(cid:104)

t=1

Qφ(yt−1
1

, yt; y∗) − ˆQ ¯φ(yt−1

1

, yt; y∗)

(cid:105)2

(18)
where the TD target ˆQ ¯φ is constructed based on
ﬁxed policy iteration in Eqn. (9), i.e.,

ˆQ ¯φ(yt−1

1

, yt; y∗) = r(yt−1

, yt) + τ H(πθ(· | yt
1))
1, w; y∗).
1)Q ¯φ(yt

1
πθ(w | yt

(19)

(cid:88)

+

w∈W

It is worthwhile to emphasize that the objective
(18) trains the critic Qφ to evaluate the current pol-
icy. Hence, it is entirely different from the objec-
tive (16), which is performing policy optimization
by Soft Q-Learning. Also, the trajectories y used
in (18) are sequences drawn from the actor policy
πθ, while objective (16) theoretically accepts any
trajectory since Soft Q-Learning can be fully off-
policy.5 Finally, following Bahdanau et al. (2016),
the TD target ˆQ ¯φ in Eqn.
(9) is evaluated us-
ing a target network, which is indicated by the
bar sign above the parameters, i.e., ¯φ. The target
network is slowly updated by linearly interpolat-
ing with the up-to-date network, i.e., the update is
¯φ ← βφ + (1 − β) ¯φ for β in (0, 1) (Lillicrap et al.,
2015).

We also adapt another technique proposed by
Bahdanau et al. (2016), which smooths the critic
by minimizing the “variance” of Q-values, i.e.,

min
φ

λvar E
y∼πθ

|y|
(cid:88)

(cid:88)

t=1

w∈W

(cid:2)Qφ(yt

1, w; y∗) − ¯Qφ(yt

1; y∗)(cid:3)2

(cid:80)

1; y∗) = 1
|W|

where ¯Qφ(yt
1, w(cid:48); y∗) is
the mean Q-value, and λvar is a hyper-parameter
controlling the relative weight between the TD
loss and the smooth loss.

w(cid:48)∈W Qφ(yt

Actor Training. Given the critic Qφ, the actor
gradient (to maximize the expected return) is given
by the policy gradient theorem of the entropy-
regularized RL (Schulman et al., 2017), which has
the form

|y|
(cid:88)

(cid:88)

E
y∼πθ

∇θπθ(w | yt−1

)Qφ(yt−1

, w; y∗)

1

1

t=1

w∈W
+ τ ∇θH(πθ(· | yt−1

1

)).

(20)

Here, for each step t, we follow Bahdanau et al.
(2016) to sum over the entire symbol set W, in-
stead of using the single sample estimation often

5Different from Bahdanau et al. (2016), we don’t use a de-
layed actor network to collect trajectories for critic training.

seen in RL. Hence, no baseline is employed.
It
is worth mentioning that Eqn. (20) is not simply
adding an entropy term to the standard policy gra-
dient as in A3C (Mnih et al., 2016). The difference
lies in that the critic Qφ trained by Eqn. (18) ad-
ditionally captures the entropy from future steps,
while the ∇θH term only captures the entropy of
the current step.

Finally, similar to (Bahdanau et al., 2016), we
ﬁnd it necessary to ﬁrst pretrain the actor using
MLE and then pretrain the critic before the actor-
critic training. Also, to prevent divergence dur-
ing actor-critic training, it is helpful to continue
performing MLE training along with Eqn.
(20),
though using a smaller weight λmle.

5 Related Work

Task Loss Optimization and Exposure Bias
Apart from the previously introduced RAML,
BSO, Actor-Critic (§1), MIXER (Ranzato et al.,
2015) also utilizes chunk-level signals where the
length of chunk grows as training proceeds.
In
contrast, minimum risk training (Shen et al., 2015)
directly optimizes sentence-level BLEU. As a re-
sult, it requires a large number (100) of samples
per data to work well. To solve the exposure bias,
scheduled sampling (Bengio et al., 2015) adopts a
curriculum learning strategy to bridge the training
and the inference. Professor forcing (Lamb et al.,
2016) introduces an adversarial training mecha-
nism to encourage the dynamics of the model to
be the same at training time and inference time.
For image caption, self-critic sequence training
(SCST) (Rennie et al., 2016) extends the MIXER
algorithm with an improved baseline based on the
current model performance.

Entropy-regularized RL Entropy regulariza-
tion been explored by early work in RL and in-
verse RL (Williams and Peng, 1991; Ziebart et al.,
2008). Lately, Schulman et al. (2017) establish
the equivalence between policy gradients and Soft
Q-Learning under entropy-regularized RL. Mo-
tivated by the multi-modal behavior induced by
entropy-regularized RL, Haarnoja et al. (2017) ap-
ply energy-based policy and Soft Q-Learning to
continuous domain. Later, Nachum et al. (2017)
proposes path consistency learning, which can be
seen as a multi-step extension to Soft Q-Learning.
More recently, in the domain of simulated con-
trol, Haarnoja et al. (2018) also consider the ac-
tor critic algorithm under the framework of en-

tropy regularized reinforcement learning. Despite
the conceptual similarity to ERAC presented here,
Haarnoja et al. (2018) focuses on continuous con-
trol and employs the advantage actor critic variant
as in (Mnih et al., 2016), while ERAC follows the
Q actor critic as in (Bahdanau et al., 2016).

6 Experiments

6.1 Experiment Settings

In this work, we focus on two sequence prediction
tasks: machine translation and image captioning.
Due to the space limit, we only present the infor-
mation necessary to compare the empirical results
at this moment. For a more detailed description,
we refer readers to Appendix B and the code6.

Machine Translation Following Ranzato et al.
(2015), we evaluate on IWSLT 2014 German-to-
English dataset (Mauro et al., 2012). The cor-
pus contains approximately 153K sentence pairs
in the training set. We follow the pre-processing
procedure used in (Ranzato et al., 2015).

Architecture wise, we employ a seq2seq model
with dot-product attention (Bahdanau et al., 2014;
Luong et al., 2015), where the encoder is a bidirec-
tional LSTM (Hochreiter and Schmidhuber, 1997)
with each direction being size 128, and the de-
coder is another LSTM of size 256. Moreover, we
consider two variants of the decoder, one using the
input feeding technique (Luong et al., 2015) and
the other not.

For all algorithms, the sequence-level BLEU
score is employed as the pay-off function R, while
the corpus-level BLEU score (Papineni et al.,
The
2002) is used for the ﬁnal evaluation.
sequence-level BLEU score is scaled up by the
sentence length so that the scale of the immediate
reward at each step is invariant to the length.

Image Captioning For image captioning, we
consider the MSCOCO dataset (Lin et al., 2014).
We adapt the same preprocessing procedure and
the train/dev/test split used by Karpathy and Fei-
Fei (2015).

The NIC (Vinyals et al., 2015) is employed as
the baseline model, where a feature vector of the
image is extracted by a pre-trained CNN and then
used to initialize the LSTM decoder. Different
from the original NIC model, we employ a pre-
trained 101-layer ResNet (He et al., 2016) rather
than a GoogLeNet as the CNN encoder.

6https://github.com/zihangdai/ERAC-VAML

For training, each image-caption pair is treated
sample, and sequence-level BLEU
as an i.i.d.
score is used as the pay-off. For testing, the stan-
dard multi-reference BLEU4 is used.

6.2 Comparison with the Direct Baseline

Firstly, we compare ERAC and VAML with their
corresponding direct baselines, namely AC (Bah-
danau et al., 2016) and RAML (Norouzi et al.,
2016) respectively. As a reference, the perfor-
mance of MLE is also provided.

Due to non-neglected performance variance ob-
served across different runs, we run each algo-
rithm for 9 times with different random seeds,7
and report the average performance, the standard
deviation and the performance range (min, max).

Machine Translation The results on MT are
summarized in the left half of Tab. 1. Firstly,
all four advanced algorithms signiﬁcantly outper-
form the MLE baseline. More importantly, both
VAML and ERAC improve upon their direct base-
lines, RAML and AC, by a clear margin on aver-
age. The result suggests the two proposed algo-
rithms both well combine the beneﬁts of a delicate
credit assignment scheme and the entropy regular-
ization, achieving improved performance.

Image Captioning The results on image cap-
tioning are shown in the right half of Tab. 1. De-
spite the similar overall trend, the improvement of
VAML over RAML is smaller compared to that
in MT. Meanwhile, the improvement from AC to
ERAC becomes larger in comparison. We sus-
pect this is due to the multi-reference nature of
the MSCOCO dataset, where a larger entropy is
preferred. As a result, the explicit entropy regu-
larization in ERAC becomes immediately fruitful.
On the other hand, with multiple references, it can
be more difﬁcult to learn a good oracle Q∗ (Eqn.
(15)). Hence, the token-level target can be less ac-
curate, resulting in smaller improvement.

6.3 Comparison with Existing Work

To further evaluate the proposed algorithms, we
compare ERAC and VAML with the large body
of existing algorithms evaluated on IWSTL 2014.
As a note of caution, previous works don’t employ
the exactly same architectures (e.g. number of lay-
ers, hidden size, attention type, etc.). Despite that,

7For AC, ERAC and VAML, 3 different critics are trained

ﬁrst, and each critic is then used to train 3 actors.

Algorithm

MT (w/o input feeding)
Mean

Min Max

MT (w/ input feeding)
Mean

Min Max

Image Captioning

Mean

Min Max

MLE

RAML
VAML

AC
ERAC

27.01 ± 0.20

26.72

27.27

28.06 ± 0.15

27.84

28.22

29.54 ± 0.21

29.27

29.89

27.74 ± 0.15
28.16 ± 0.11

28.04 ± 0.05
28.30 ± 0.06

27.47
28.00

27.97
28.25

27.93
28.26

28.10
28.42

28.56 ± 0.15
28.84 ± 0.10

29.05 ± 0.06
29.31 ± 0.04

28.35
28.62

28.95
29.26

28.80
28.94

29.16
29.36

29.84 ± 0.21
29.93 ± 0.22

30.90 ± 0.20
31.44 ± 0.22

29.50
29.51

30.49
31.07

30.17
30.24

31.16
31.82

Table 1: Test results on two benchmark tasks. Bold faces highlight the best in the corresponding category.

for VAML and ERAC, we use an architecture that
is most similar to the majority of previous works,
which is the one described in §6.1 with input feed-
ing.

Based on the setting, the comparison is summa-
rized in Table 2.8 As we can see, both VAML and
ERAC outperform previous methods, with ERAC
leading the comparison with a signiﬁcant margin.
This further veriﬁes the effectiveness of the two
proposed algorithms.

(cid:72)(cid:72)
λvar

β

(cid:72)(cid:72)(cid:72)(cid:72)

0.001

0.01

0.1

1

0
0.001

27.91
29.41

26.27†
29.26

28.88
29.32

27.38†
27.44

Table 3: Average validation BLEU of ERAC. As
a reference, the average BLEU is 28.1 for MLE.
λvar = 0 means not using the smoothing technique.
β = 1 means not using a target network. † indi-
cates excluding extreme values due to divergence.

Algorithm

MIXER (Ranzato et al., 2015)
BSO (Wiseman and Rush, 2016)
Q(BLEU) (Li et al., 2017)
AC (Bahdanau et al., 2016)
RAML (Ma et al., 2017)

VAML
ERAC

BLEU

20.73
27.9
28.3
28.53
28.77

28.94
29.36

Table 2: Comparison with existing algorithms on
IWSTL 2014 dataset for MT. All numbers of pre-
vious algorithms are from the original work.

6.4 Ablation Study

Due to the overall excellence of ERAC, we study
the importance of various components of it, hope-
fully offering a practical guide for readers. As
the input feeding technique largely slows down
the training, we conduct the ablation based on the
model variant without input feeding.

Firstly, we study the importance of two tech-
niques aimed for training stability, namely the tar-
get network and the smoothing technique (§4.2).
Based on the MT task, we vary the update speed β
of the target critic, and the λvar, which controls the

8For a more detailed comparison of performance together

with the model architectures, see Table 7 in Appendix C.

strength of the smoothness regularization. The av-
erage validation performances of different hyper-
parameter values are summarized in Tab. 3.

• Comparing the two rows of Tab. 3, the smooth-
ing technique consistently leads to performance
improvement across all values of τ . In fact, re-
moving the smoothing objective often causes
the training to diverge, especially when β =
0.01 and 1. But interestingly, we ﬁnd the di-
vergence does not happen if we update the tar-
get network a little bit faster (β = 0.1) or quite
slowly (β = 0.001).

• In addition, even with the smoothing technique,
the target network is still necessary. When the
target network is not used (β = 1), the perfor-
mance drops below the MLE baseline. How-
ever, as long as a target network is employed to
ensure the training stability, the speciﬁc choice
of target network update rate does not matter
as much. Empirically, it seems using a slower
(β = 0.001) update rate yields the best result.

Next, we investigate the effect of enforcing dif-
ferent levels of entropy by varying the entropy
hyper-parameter τ . As shown in Fig. 1, it seems
there is always a sweet spot for the level of en-
tropy. On the one hand, posing an over strong en-

(a) Machine translation

(b) Image captioning

Figure 1: ERAC’s average performance over multiple runs on two tasks when varying τ .

7 Discussion

In this work, motivated by the intriguing con-
nection between the token-level RAML and the
entropy-regularized RL, we propose two algo-
rithms for neural sequence prediction. Despite the
distinct training procedures, both algorithms com-
bine the idea of ﬁne-grained credit assignment and
the entropy regularization, leading to positive em-
pirical results.

However, many problems remain widely open.
In particular, the oracle Q-function Qφ we obtain
is far from perfect. We believe the ground-truth
reference contains sufﬁcient information for such
an oracle, and the current bottleneck lies in the RL
algorithm. Given the numerous potential applica-
tions of such an oracle, we believe improving its
accuracy will be a promising future direction.

tropy regularization can easily cause the actor to
diverge. Speciﬁcally, the model diverges when τ
reaches 0.03 on the image captioning task or 0.06
on the machine translation task. On the other hand,
as we decrease τ from the best value to 0, the per-
formance monotonically decreases as well. This
observation further veriﬁes the effectiveness of en-
tropy regularization in ERAC, which well matches
our theoretical analysis.

Finally, as discussed in §4.2, ERAC takes the ef-
fect of future entropy into consideration, and thus
is different from simply adding an entropy term
to the standard policy gradient as in A3C (Mnih
et al., 2016). To verify the importance of explicitly
modeling the entropy from future steps, we com-
pared ERAC with the variant that only applies the
entropy regularization to the actor but not to the
critic. In other words, the τ is set to 0 when per-
forming policy evaluating according to Eqn. (4.2),
while the τ for the entropy gradient in Eqn. (20)
remains. The comparison result based on 9 runs
on test set of IWSTL 2014 is shown in Table 4. As
we can see, simply adding a local entropy gradient
does not even improve upon the AC. This further
veriﬁes the difference between ERAC and A3C,
and shows the importance of taking future entropy
into consideration.

Algorithm

Mean

ERAC
ERAC w/o Future Ent.
AC

28.30 ± 0.06
28.06 ± 0.05
28.04 ± 0.05

Max

28.42
28.11
28.10

Table 4: Comparing ERAC with the variant with-
out considering future entropy.

References

Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu,
Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. 2016. An actor-critic
algorithm for sequence prediction. arXiv preprint
arXiv:1607.07086 .

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473 .

Leemon Baird. 1995. Residual algorithms: Reinforce-
ment learning with function approximation. In Ma-
chine Learning Proceedings 1995, Elsevier, pages
30–37.

Alex M Lamb, Anirudh Goyal ALIAS PARTH
GOYAL, Ying Zhang, Saizheng Zhang, Aaron C
Courville, and Yoshua Bengio. 2016.
Professor
forcing: A new algorithm for training recurrent net-
works. In Advances In Neural Information Process-
ing Systems. pages 4601–4609.

Jiwei Li, Will Monroe, and Dan Jurafsky. 2017. Learn-
ing to decode for future success. arXiv preprint
arXiv:1701.06549 .

Timothy P Lillicrap, Jonathan J Hunt, Alexander
Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. 2015. Continu-
ous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971 .

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and
Noam Shazeer. 2015. Scheduled sampling for se-
quence prediction with recurrent neural networks.
In Advances in Neural Information Processing Sys-
tems. pages 1171–1179.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar,
and C Lawrence Zitnick. 2014. Microsoft coco:
In European confer-
Common objects in context.
ence on computer vision. Springer, pages 740–755.

William Chan, Navdeep Jaitly, Quoc Le, and Oriol
Vinyals. 2016. Listen, attend and spell: A neural
network for large vocabulary conversational speech
In Acoustics, Speech and Signal Pro-
recognition.
cessing (ICASSP), 2016 IEEE International Confer-
ence on. IEEE, pages 4960–4964.

Hal Daum´e III and Daniel Marcu. 2005. Learning
as search optimization: Approximate large margin
In Proceedings
methods for structured prediction.
of the 22nd international conference on Machine
learning. ACM, pages 169–176.

Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and
learning
arXiv preprint

Sergey Levine. 2017.
with deep energy-based policies.
arXiv:1702.08165 .

Reinforcement

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and
Sergey Levine. 2018. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with
a stochastic actor. arXiv preprint arXiv:1801.01290
.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
In Proceedings of the IEEE conference on
nition.
computer vision and pattern recognition. pages 770–
778.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation

Long short-term memory.
9(8):1735–1780.

Po-Sen Huang, Chong Wang, Dengyong Zhou, and
Li Deng. 2017. Toward neural phrasebased machine
translation .

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
the IEEE conference
tions.
on computer vision and pattern recognition. pages
3128–3137.

In Proceedings of

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. arXiv preprint
arXiv:1508.04025 .

Xuezhe Ma, Pengcheng Yin, Jingzhou Liu, Graham
Softmax q-
Neubig, and Eduard Hovy. 2017.
distribution estimation for structured prediction: A
theoretical interpretation for raml. arXiv preprint
arXiv:1705.07136 .

Cettolo Mauro, Girardi Christian, and Federico Mar-
cello. 2012. Wit3: Web inventory of transcribed and
translated talks. In Conference of European Associ-
ation for Machine Translation. pages 261–268.

Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi
Mirza, Alex Graves, Timothy Lillicrap, Tim Harley,
David Silver, and Koray Kavukcuoglu. 2016. Asyn-
chronous methods for deep reinforcement learning.
In International Conference on Machine Learning.
pages 1928–1937.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidje-
land, Georg Ostrovski, et al. 2015. Human-level
control through deep reinforcement learning. Na-
ture 518(7540):529.

Oﬁr Nachum, Mohammad Norouzi, Kelvin Xu, and
Dale Schuurmans. 2017. Bridging the gap between
value and policy based reinforcement learning.
In
Advances in Neural Information Processing Sys-
tems. pages 2772–2782.

Mohammad Norouzi, Samy Bengio, Navdeep Jaitly,
Mike Schuster, Yonghui Wu, Dale Schuurmans,
et al. 2016. Reward augmented maximum likeli-
hood for neural structured prediction. In Advances
In Neural Information Processing Systems. pages
1723–1731.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th annual meeting on association for compu-
tational linguistics. Association for Computational
Linguistics, pages 311–318.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2015. Sequence level train-
ing with recurrent neural networks. arXiv preprint
arXiv:1511.06732 .

Steven J Rennie, Etienne Marcheret, Youssef Mroueh,
Jarret Ross, and Vaibhava Goel. 2016. Self-critical
arXiv
sequence training for image captioning.
preprint arXiv:1612.00563 .

Alexander M Rush, Sumit Chopra, and Jason We-
A neural attention model for ab-
arXiv preprint

ston. 2015.
stractive sentence summarization.
arXiv:1509.00685 .

John Schulman, Pieter Abbeel, and Xi Chen. 2017.
Equivalence between policy gradients and soft q-
learning. arXiv preprint arXiv:1704.06440 .

Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2015. Minimum
risk training for neural machine translation. arXiv
preprint arXiv:1512.02433 .

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural im-
age caption generator. In Computer Vision and Pat-
tern Recognition (CVPR), 2015 IEEE Conference
on. IEEE, pages 3156–3164.

Ronald J Williams and Jing Peng. 1991. Function opti-
mization using connectionist reinforcement learning
algorithms. Connection Science 3(3):241–268.

Sam Wiseman and Alexander M Rush. 2016.
Sequence-to-sequence learning as beam-search op-
timization. arXiv preprint arXiv:1606.02960 .

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. 2015. Show, attend and tell:
Neural image caption generation with visual at-
In International Conference on Machine
tention.
Learning. pages 2048–2057.

Brian D Ziebart. 2010. Modeling purposeful adaptive
behavior with the principle of maximum causal en-
tropy. Carnegie Mellon University.

Brian D Ziebart, Andrew L Maas, J Andrew Bagnell,
and Anind K Dey. 2008. Maximum entropy inverse
reinforcement learning. In AAAI. Chicago, IL, USA,
volume 8, pages 1433–1438.

A Proofs

A.1 Main Proofs
Proposition 1. For any ground-truth pair (x∗, y∗), PQR and QR satisfy the following marginal match
condition and terminal condition:

|y|
(cid:89)

t=1

PQR(yt | yt−1

1

) = PR(y | x∗) ∀y ∈ Y

QR(ˆy, eos; y∗) = R(ˆy + eos; y∗) − R(ˆy; y∗) ∀ˆy ∈ Y −

if and only if for any y ∈ Y,

QR(yt−1

, yt; y∗) =

1

(cid:40)

R(yt
R(yt

1; y∗) − R(yt−1
1; y∗) − R(yt−1

1

1

; y∗) + τ log (cid:80)
; y∗),

w∈W exp (cid:0)QR(yt

1, w; y∗)/τ (cid:1) ,

(21)

(22)

t < |y|
t = |y|

(23)

Proof. To avoid clutter, we drop the dependency on x∗ and y∗. The following proof holds for each
possible pair of (x∗, y∗).

Firstly, it is easy to see that the terminal condition in Eqn. (22) exactly corresponds to the t = |y| case

of Eqn. (23), since yt = eos for y ∈ Y. So, we will focus on the non-terminal case next.

Sufﬁciency For convenience, deﬁne VR(yt
is true. Then for any y ∈ Y,

1) = τ log (cid:80)

w∈W exp (cid:0)QR(yt

1, w)/τ (cid:1). Suppose Eqn. (23)

PQR(y) =

PQR(yt | yt−1

1

)

(cid:32) (cid:80)|y|

t=1 QR(yt−1

1

(cid:33)

, yt) − VR(yt−1
τ

1

)

|y|
(cid:89)

t=1

= exp

= exp

= exp

(cid:18) R(y) − VR(∅)
τ

(cid:19)

(cid:32) (cid:80)|y|
t=1

(cid:2)R(yt

1) − R(yt−1

1

)(cid:3) + (cid:80)|y|−1
t=1 VR(yt
τ

1) − (cid:80)|y|

t=1 VR(yt−1

1

)

(cid:33)

where VR(∅) denotes VR(yt
by construction, we have

1) when t = 0 and yt

1 is an empty set. Since PQR(y) is a valid distribution

Hence,

VR(∅) =

exp

(cid:88)

y∈Y

(cid:19)

(cid:18) R(y)
τ

PQR(y) =

(cid:80)

R(y)/τ
y(cid:48)∈Y R(y(cid:48))/τ

= PR(y),

which satisﬁes the marginal match requirement.

Necessity Now, we show that the speciﬁc formulation of QR (Eqn. (23)) is also a necessary condition
of the marginal match condition (Eqn. (21)).

The token-level target distribution can be simpliﬁed as

PQR(yt | yt−1

1

) =

exp (cid:0)QR(yt−1
w∈W exp (cid:0)QR(yt−1

1

1

, yt)/τ (cid:1)

(cid:80)

, w)/τ (cid:1) = exp

(cid:18) QR(yt−1

1

, yt) − VR(yt−1

)

1

(cid:19)

.

τ

Suppose Eqn. (21) is true. For any y ∈ Y − and t ≤ |y| and deﬁne y(cid:48) = yt

1+eos and y(cid:48)(cid:48) = yt−1

1 +eos.

Obviously, it follows y(cid:48), y(cid:48)(cid:48) ∈ Y. Also, by deﬁnition,

PR(y(cid:48)) = PR(eos | yt
PR(y(cid:48)(cid:48)) = PR(eos | yt−1

1) × PR(yt | yt−1
1
) × PR(yt−1
)

1

1

) × PR(yt−1

)

1

Then, consider the ratio

exp

(cid:18) R(y(cid:48)) − R(y(cid:48)(cid:48))
τ

= exp

PR(y(cid:48))
PR(y(cid:48)(cid:48))
(cid:19)

=

PR(eos | yt

PR(eos | yt−1

1) × PR(yt | yt−1

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
PR(yt−1
) ×
)
1
(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
PR(yt−1
)
1
(cid:19)
1, eos) − VR(yt
1)

) ×

1

1

(cid:18) QR(yt

(cid:46)

exp

(cid:18) QR(yt−1

1

τ

, eos) −
τ

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
VR(yt−1
)

1

(cid:19)

(cid:18) QR(yt−1

1

× exp

, yt) −
τ

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)
VR(yt−1
)

1

(cid:19)

R(y(cid:48)) − R(y(cid:48)(cid:48)) = QR(yt

1, eos) − QR(yt−1

1

, eos) − VR(yt

1) + QR(yt−1

1

, yt).

Now, by the terminal condition (Eqn. (22)), we essentially have

QR(yt
QR(yt−1

1, eos) = R(yt
, eos) = R(yt−1

1 + eos) − R(yt
1) = 0
1 + eos) − R(yt−1

1

1

) = 0

Thus, it follows

R(y(cid:48)) − R(y(cid:48)(cid:48)) = QR(yt−1
, yt) = R(yt

⇐⇒ QR(yt−1

1

, yt) − VR(yt
1)
) + τ log

1) − R(yt−1

1

1

exp (cid:0)QR(yt

1, w)/τ (cid:1) ,

(cid:88)

w∈W

which completes the proof.

Corollary 1. Please refer to §3.2 for the Corollary.

Proof. Similarly, we drop the dependency on x∗ and y∗ to avoid clutter. We ﬁrst prove the equivalence
of Q∗(yt−1

, yt) with QR(yt−1

, yt) by induction.

1

1

• Base case: When t = T , for any y ∈ Y, yT can only be eos. So, by deﬁnition, we have

V ∗(yT −1
1
(cid:88)

, eos)

) = Q∗(yT −1
(cid:16)

1
Q∗(yT −1
1

exp

(cid:17)

, a)/τ

⇐⇒ τ log

a∈W
=⇒ Q∗(yT −1

1

, a) = −∞, ∀a (cid:54)= eos.

= Q∗(yT −1

, eos)

1

Q∗(yT −1
1

, yT ) =

(cid:40)

r(yT −1
1
−∞,

, eos),

if yT = eos
otherwise

Hence,

For the ﬁrst case, it directly follows

Q∗(yT −1
1

, eos) = r(yT −1

1

, eos) = R(yT −1

1 + eos) − R(yT −1

1

) = QR(yT −1

1

, eos).

For the second case, since only eos is allowed to be generated, the target distribution PQR should
be a single-point distribution at eos. This is equivalent to deﬁne

QR(yT −1
1

, a) = −∞, ∀a (cid:54)= eos,

which proves the second case. Combining the two cases, it concludes

Q∗(yT −1
1

, a) = QR(yT −1

1

, a), ∀y ∈ Y, a ∈ W.

• Induction step: When 0 < t < T , assume the equivalence holds when k > t, i.e.,

Q∗(yk−1

, w) = QR(yk−1

1

1

, w), ∀k > t, w ∈ W.

Then,

Q∗(yt−1

, yt) = r(yt−1

1

1

, yt) + γ E
s(cid:48)∼ρs

[α log

(cid:88)

exp (cid:0)Q∗(s(cid:48), a)/α(cid:1)]

= r(yt−1

1

, yt) + τ log

a∈A
exp (cid:0)Q∗(yt

1, a)/τ (cid:1)

= r(yt−1

1

, yt) + τ log

exp (cid:0)QR(yt

1, a)/τ (cid:1)

(Q∗(yk

1, a) = QR(yk

1, a) for k ≥ t)

(α = τ, A = W)

(cid:88)

a∈W
(cid:88)

a∈W

= QR(yt−1

1

, yt).

Thus, Q∗(yt−1

, yt) = QR(yt−1

1

1

, yt) holds for t ∈ [1, T ].

With the equivalence between QR and Q∗, we can easily prove V ∗ = VR and π∗ = PQR,
exp (cid:0)Q∗(yt−1

V ∗(yt−1

, a)/α(cid:1)

) = α log

(cid:88)

1

exp (cid:0)Q∗(yt−1

, a)/τ (cid:1)

(α = τ, A = W)

π∗(yt | yt−1

1

) =

(cid:80)

1

1

a∈A
(cid:88)

= τ log

a∈W
= VR(yt−1
)

, yt)/τ (cid:1)

1

1
exp (cid:0)Q∗(yt−1
w∈W exp (cid:0)Q∗(yt−1
exp (cid:0)QR(yt−1
w∈W exp (cid:0)QR(yt−1

1

1

1

, yt)/τ (cid:1)

, yt)/τ (cid:1)

, yt)/τ (cid:1)

=

(cid:80)

= PQR(yt | yt−1

1

)

A.2 Other Proofs

We derive the equivalence between the VAML’s objective (Eqn. (17)) and the RAML’s objective (Eqn.
(2)).
CE (cid:0)PQφ(cid:107)Pθ
= − E

(cid:1)

log Pθ(y)

y∼PQφ

= − E

y∼PQφ

|y|
(cid:88)

t=1

log Pθ(yt | yt−1

)

1

T
(cid:88)

t=1

E
yt
1∼PQφ

(Y t
1 )

log Pθ(yt | yt−1

)

1

= −

=

=

=

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

(cid:34)

−

E
yt−1
1 ∼PQφ

(Yt−1
1

)

yt∼PQφ

E
(Yt|yt−1

1

)

log Pθ(yt | yt−1

1

(cid:35)
)

E
yt−1
1 ∼PQφ

(Yt−1
1

)

CE (cid:0)PQφ(Yt | yt−1

1

)(cid:107)Pθ(Yt | yt−1

1

)(cid:1)

E
yt−1
1 ∼PQφ

(Yt−1
1

)

yt∈W

(cid:88)

PQφ(yt | yt−1

1

) CE (cid:0)PQφ(Yt | yt−1
1
(cid:123)(cid:122)
(cid:124)
const. w.r.t. yt

)(cid:107)Pθ(Yt | yt−1

1

)(cid:1)
(cid:125)

(T is longest possible length)

(cid:2)CE (cid:0)PQφ(Yt | yt−1

1

)(cid:107)Pθ(Yt | yt−1

1

)(cid:1)(cid:3)

=

T
(cid:88)

t=1

E
yt−1
1 ∼PQφ
(cid:124)

E
(W |yt−1
)
(cid:125)

1

yt∈PQφ

(Yt−1
1

)
(cid:123)(cid:122)
yt
1∼PQφ

E

(Yt
1)

=

T
(cid:88)

t=1

E
yt
1∼PQφ

(Yt
1)

=

E
y∼PQφ

(Y)

|y|
(cid:88)

t=1

(cid:2)CE (cid:0)PQφ(Yt | yt−1

1

)(cid:107)Pθ(Yt | yt−1

1

)(cid:1)(cid:3)

CE (cid:0)PQφ(Yt | yt−1

1

)(cid:107)Pθ(Yt | yt−1

1

)(cid:1)

B Implementation Details

B.1 RAML
In RAML, we want to optimize the cross entropy CE (PR(Y | x∗, y∗)(cid:107)Pθ(Y | x∗)). As discussed in
§2.1, directly sampling from the exponentiated pay-off distribution PR(Y | x∗) is impractical. Hence,
normalized importance sampling has been exploited in previous work (Norouzi et al., 2016; Ma et al.,
2017). Deﬁne the proposal distribution to be PS(Y | x∗, y∗). Then, the objective can be rewritten as

CE (PR(Y | x∗, y∗)(cid:107)Pθ(Y | x∗)) = −

E
y∼PS (Y|x∗,y∗)

PR(y | x∗, y∗)
PS(y | x∗, y∗)

log Pθ(y | x∗)

exp(R(y,y∗)/τ )
˜PS (y|x∗,y∗)

= −

E
y∼PS (Y|x∗,y∗)

= −

E
y∼PS (Y|x∗,y∗)

exp(R(y(cid:48),y∗)/τ )
˜PS (y(cid:48)|x∗,y∗)

Ey(cid:48)∼PS (Y|x∗,y∗)
w(y, y∗)
Ey(cid:48)∼PS (Y|x∗,y∗) w(y(cid:48), y∗)

log Pθ(y | x∗)

log Pθ(y | x∗)

≈ −

M
(cid:88)

i=1

w(y(i), y∗)
i=1 w(y(i), y∗)

(cid:80)M

log Pθ(y(i) | x∗),

where w(y, y∗) = exp(R(y,y∗)/τ )
is the unnormalized importance weight, ˜PS denotes the unnormalized
˜PS (y|x∗,y∗)
˜PS
Z , M is the number of samples used, and y(i) is the i-th sample drawn from the

probability of PS =
proposal distribution PS(Y | x∗, y∗).

With importance sampling, the problem turns to what proposal distribution we should use. In the
original work (Norouzi et al., 2016), the proposal distribution is deﬁned by the hamming distance as
used. Ma et al. (2017) ﬁnd that it sufﬁces to perform N -gram replacement of the reference sentence.
Speciﬁcally, PS(Y | x∗, y∗) can be a uniform distribution deﬁned on set Yngram where Yngram is obtained
by randomly replacing an n-gram of y∗ (n ≤ 4).

In this work, we adapt the simple n-gram replacement distribution, denoted as Pngram(Y | x∗, y∗),

which simpliﬁes the RAML objective into

−

min
θ

M
(cid:88)

i=1

exp (cid:0)R(y(i), y∗)/τ (cid:1)
i=1 exp (cid:0)R(y(i), y∗)/τ (cid:1) log Pθ(y(i) | x∗)

(cid:80)M

Following Ma et al. (2017), we make sure the reference sequence is always among the M samples used.

B.2 VAML

As discussed in §4, the VAML training consists of two phases:

• In the ﬁrst phase, Soft Q-Learning is used to train Qφ based on Eqn. (16). Since Soft Q-Learning

accepts off-policy trajectories, in this work, we use two types of off-policy sequences:

1. The ﬁrst type is simply the ground-truth sequence, which provides strong learning signals.

2. The second type of sequences is actually drawn from the same n-gram replacement distribution
discussed above. The reason is that in the second training phase, such n-gram replaced trajecto-
ries will be used. Since the learned Qφ won’t be perfect, we hope the exposing Qφ with these
trajectories can improve its accuracy on them, making the second phase of training easier.

Algorithm 1 summarizes the ﬁrst phase.

Algorithm 1 VAML Phase 1: Soft Q-Learning to approximate Q∗
Require: A Q-function approximator Qφ with parameter φ, and the hyper-parameters τ , M .

1: while Not Converged do
2:

Receive a random example (x∗, y∗).
Sample M − 1 sequences {y(i)}M −1
i=1
Compute all the rewards r(yt−1
, yt; y∗) for each y ∈ {y(i)}M
Compute the target Q-values for each y ∈ {y(i)}M

from Pngram(Y | x∗, y∗) and let y(M ) = y∗.
i=1 and t = 1, . . . , |y|.

i=1 and t = 1, . . . , |y|

1

3:

4:

5:

ˆQφ(yt−1

1

, yt; y∗) = r(yt−1

, yt; y∗) + τ log

1

exp (cid:0)Qφ(yt

1, w; y∗)/τ (cid:1) .

(cid:88)

w∈W

6:

Compute the Soft-Q Learning loss

LSoftQ =

1
M

M
(cid:88)

|y(i)|
(cid:88)

i=1

t=1

(cid:13)
(cid:13)Qφ(y(i)t−1
(cid:13)

1

, y(i)
t

; y∗) − ˆQφ(y(i)t−1

1

, y(i)
t

(cid:13)
2
; y∗)
(cid:13)
(cid:13)
2

.

Update Qφ according to the loss LSoftQ.

7:
8: end while

• Once the Qφ is well trained in the ﬁrst phase, the second phase is to minimize the cross entropy

CE (cid:0)PQφ(Y | x∗, y∗)(cid:107)Pθ(Y | x∗)(cid:1) based on Eqn. (17), i.e.,

min
θ

E
y∼PQφ





|y|
(cid:88)

t=1

CE (cid:0)PQφ(Yt | yt−1

1

)(cid:107)Pθ(Yt | yt−1

1

)(cid:1)

 .



Ideally, we would like to directly sample from PQφ, and perform the optimization. However, we ﬁnd
samples from PQφ are quite similar to each other. We conjecture this results from both the imperfect
training in the ﬁrst phase, and the intrinsic difﬁculty of getting diverse samples from an exponentially
large space when the distribution is high concentrated.

Nevertheless, for this work, we fall back to the same importance sampling method as used in RAML
and use the n-gram replacement distribution as the proposal. Hence, the objective becomes

CE (cid:0)PQφ(Yt | yt−1

1

)(cid:107)Pθ(Yt | yt−1

1





)(cid:1)

E
y∼PQφ

|y|
(cid:88)





t=1


= E

y∼Pngram

w(y, y∗)
Ey(cid:48)∼Pngram(Y|x∗,y∗) w(y(cid:48), y∗)



|y|
(cid:88)

t=1

≈

M
(cid:88)

i=1

exp (cid:0)R(y(i), y∗)/τ (cid:1)
i=1 exp (cid:0)R(y(i), y∗)/τ (cid:1)

(cid:80)M

|y(i)|
(cid:88)





t=1

CE (cid:0)PQφ(Yt | yt−1

1

)(cid:107)Pθ(Yt | yt−1

1

)(cid:1)

(cid:16)

CE

PQφ(Yt | y(i)t−1

1

)(cid:107)Pθ(Yt | y(i)t−1

)

1

(cid:17)

 .







However, we found directly using this objective does not yield improved performance compared to
RAML, mostly likely due to some erratic estimations of Qφ. Thus, we only use this objective for

some step with certain probability κ ∈ (0, 1), leaving others trained by MLE. Formally, deﬁne

Jκ(yt

1) =

E
z∼Bernoulli(κ)

(cid:2)zCE (cid:0)PQφ(Yt | yt−1

1

)(cid:107)Pθ(Yt | yt−1

1

)(cid:1) − (1 − z) log Pθ(yt | yt−1

)(cid:3) ,

1

the VAML objective practically used is

M
(cid:88)

i=1

exp (cid:0)R(y(i), y∗)/τ (cid:1)
i=1 exp (cid:0)R(y(i), y∗)/τ (cid:1)

(cid:80)M

min
θ



Jκ(y(i)t
1)

 .

|y(i)|
(cid:88)





t=1

Algorithm 2 VAML Phase 2: Sequence model training with token-level target
Require: A sequence prediction model Pθ with parameter θ, a pre-trained Q-function approximator Qφ,

Algorithm 2 summarizes the second phase.

and hyper-parameters τ , M , κ

1: while Not Converged do
2:

Receive a random example (x∗, y∗).
Sample M − 1 sequences {y(i)}M −1
i=1
Compute the VAML loss using

3:

4:

Update Pθ according to the loss LVAML.

5:
6: end while

B.3 ERAC

from Pngram(Y | x∗, y∗) and let y(M ) = y∗.

LVAML =

M
(cid:88)

i=1

exp (cid:0)R(y(i), y∗)/τ (cid:1)
i=1 exp (cid:0)R(y(i), y∗)/τ (cid:1)

(cid:80)M



Jκ(y(i)t
1)

 .

|y(i)|
(cid:88)





t=1

Following Bahdanau et al. (2016), we ﬁrst pre-train the actor, then train the critic with the ﬁxed actor
and ﬁnally ﬁne-tune them together. The speciﬁc procedure for training ERAC is

• Pre-training the actor using maximum likelihood training

• Pre-training the critic using Algorithm 3 with the actor ﬁxed

• Fine-tuning both the actor and critic with Algorithm 3

B.4 Hyper-parameters

RAML & VAML The hyper-parameters for RAML and VAML training are summarized in Tab. 5.
We set the gradient clipping value to 5.0 for both the Q-function approximator Qφ and the sequence
prediction model Pθ, except for the sequence prediction model in the captioning task where the gradient
clipping value is set to 1.0.

AC & ERAC As described in §B.3, the training using AC and ERAC involves three phases. The hyper-
parameters used for ERAC training in each phase are summarized in Table 6. In all phases, the learning
rate is halved when there is no improvement on the validation set. We use the same hyper-parameters
for AC training, except that the entropy regularization coefﬁcient τ is 0. Similar to the VAML case,
the gradient clipping value is set to 5.0 for both the actor and the critic, except that we set the gradient
clipping value to 1.0 for the actor in the captioning task.

, yt; y∗) and an actor πθ(w | yt

1) with weights φ and θ respectively, and

Algorithm 3 ERAC Algorithm
Require: A critic Qφ(yt−1

1

hyper-parameters τ , β, λvar, λmle

1: Initialize delayed target critic Q ¯φ with the same weights: ¯φ = φ.
2: while Not Converged do
3:

Receive a random example (x∗, y∗).
Generate a sequence y from πθ.
Compute the rewards r(yt−1
Compute targets for the critic

1

4:

5:

6:

, yt; y∗) for t = 1, . . . , |y|.

ˆQ ¯φ(yt−1

1

, yt; y∗) = r(yt−1

, yt) + τ H(πθ(· | yt

1)) +

1

πθ(w | yt

1)Q ¯φ(yt

1, w; y∗).

(cid:88)

w∈W

(cid:105)2

(cid:88)

w∈W

7:

Compute loss for critic

|y|
(cid:88)

(cid:104)

t=1

8:

Compute loss for actor





|y|
(cid:88)

(cid:88)

t=1

w∈W

Lcritic =

Qφ(yt−1

1

, yt; y∗) − ˆQ ¯φ(yt−1

1

, yt; y∗)

+ λvar

(cid:2)Qφ(yt−1

1

, w; y∗) − ¯Qφ(yt−1

1

; y∗)(cid:3)2

,

where

¯Qφ(yt−1

1

; y∗) =

Qφ(yt−1

1

, w(cid:48); y∗)

1
|W|

(cid:88)

w(cid:48)∈W

Lactor = −

πθ(w | yt−1

)Qφ(yt−1

, w; y∗) + τ H(πθ(· | yt−1

)) + λmle

1

1

1

log πθ(y∗

t | y∗t−1

1

)





|y∗|
(cid:88)

t=1

9:

10:

Update critic according to the loss Lcritic.
If actor is not ﬁxed, update actor according to the loss Lactor
Update delayed target critic: ¯φ = βφ + (1 − β) ¯φ

11:
12: end while

Hyper-parameters VAML-1 VAML-2 RAML VAML-1 VAML-2 RAML

Machine Translation

Image Captioning

optimizer
learning rate
batch size
M
τ
κ

Adam
0.001
50
5
0.4
N.A.

SGD
0.6
42
5
0.4
0.2

SGD
0.6
42
5
0.4
N.A.

Adam
0.001
32 × 5
2
0.7
N.A.

SGD
0.5
32 × 5
6
0.7
0.1

SGD
0.5
32 × 5
6
0.7
N.A.

Table 5: Optimization related hyper-parameters of RAML and VAML for two tasks. “VAML-1” and
“VAML-2” indicate the phase 1 and phase 2 of VAML training respectively. “N.A.” means not applicable.
“32 × 5” indicates using 32 images each with 5 reference captions.

C Comparison with Previous Work

The detailed comparison with previous work in shown in Table 7. Under different comparable architec-
tures (1 layer or 2 layers), ERAC outperforms previous algorithms with a clear margin.

Hyper-parameters

MT w/ input feeding MT w/o input feeding

Image Captioning

optimizer
learning rate
batch size

optimizer
learning rate
batch size
τ (entropy regularization)
β (target net speed)
λvar (smoothness)

optimizer
learning rate
batch size
τ (entropy regularization)
β (target net speed)
λvar (smoothness)
λMLE

Pre-train Actor

Pre-train Critic

Joint Training

SGD
0.6
50

Adam
0.001
50
0.05
0.001
0.001

Adam
0.0001
50
0.05
0.001
0.001
0.1

SGD
0.6
50

Adam
0.001
50
0.04
0.001
0.001

Adam
0.0001
50
0.04
0.001
0.001
0.1

Table 6: Hyper-parameters for ERAC training

SGD
0.5
32 × 5

Adam
0.001
32 × 5
0.01
0.001
0.001

Adam
0.0001
32 × 5
0.01
0.001
0.001
0.1

Algorithm

MIXER (Ranzato et al., 2015)
BSO (Wiseman and Rush, 2016)
Q(BLEU) (Li et al., 2017)
AC (Bahdanau et al., 2016)
RAML (Ma et al., 2017)

Encoder

NN Type

1-layer CNN

Size

256

1-layer BiLSTM 128 × 2
1-layer BiLSTM 128 × 2
256 × 2
1-layer BiGRU
1-layer BiLSTM 256 × 2

1-layer LSTM 256
1-layer LSTM 256
1-layer LSTM 256
1-layer GRU
256
1-layer LSTM 256

NN Type

Size Attention

Input Feed

Decoder

1-layer BiLSTM 128 × 2
1-layer BiLSTM 128 × 2

1-layer LSTM 256
1-layer LSTM 256

NPMT (Huang et al., 2017)
NPMT+LM (Huang et al., 2017)

2-layer BiGRU
2-layer BiGRU

256 × 2
256 × 2

2-layer LSTM 512
2-layer LSTM 512

N.A.
N.A.

2-layer BiLSTM 256 × 2

2-layer LSTM 512

Dot-Prod

VAML
ERAC

ERAC

Dot-Prod
Dot-Prod
Dot-Prod
MLP
Dot-Prod

Dot-Prod
Dot-Prod

BLEU

20.73
27.9
28.3
28.53
28.77

28.94
29.36

29.92
30.08

30.85

N
Y
Y
Y
Y

Y
Y

N.A.
N.A.

Y

Table 7: Comparison of algorithms with detailed architecture information on the IWSTL 2014 dataset for MT.

