Weakly Supervised Few-shot Object Segmentation
using Co-Attention with Visual and Semantic Inputs

Mennatullah Siam1,4∗ , Naren Doraiswamy2∗ , Boris N. Oreshkin3∗ , Hengshuai Yao4
and Martin Jagersand1
1 University of Alberta
2 Indian Institute of Science
3 Element AI
4 HiSilicon, Huawei Research

0
2
0
2
 
b
e
F
 
7
 
 
]

V
C
.
s
c
[
 
 
2
v
0
4
5
9
0
.
1
0
0
2
:
v
i
X
r
a

Abstract

Signiﬁcant progress has been made recently in de-
veloping few-shot object segmentation methods.
Learning is shown to be successful in few seg-
mentation settings, including pixel-level, scribbles
and bounding boxes. This paper takes another ap-
proach, i.e., only requiring image-level classiﬁca-
tion data for few-shot object segmentation. We
propose a novel multi-modal interaction module
for few-shot object segmentation that utilizes a co-
attention mechanism using both visual and word
embedding. Our model using image-level labels
achieves 4.8% improvement over previously pro-
posed image-level few-shot object segmentation. It
also outperforms state-of-the-art methods that use
weak bounding box supervision on PASCAL-5i.
Our results show that few-shot segmentation ben-
eﬁts from utilizing word embeddings, and that we
are able to perform few-shot segmentation using
stacked joint visual semantic processing with weak
image-level labels. We further propose a novel
setup, Temporal Object Segmentation for Few-shot
Learning (TOSFL) for videos. TOSFL requires
only image-level labels for the ﬁrst frame in order
to segment objects in the following frames. TOSFL
provides a novel benchmark for video segmenta-
tion, which can be used on a variety of public video
data such as Youtube-VOS, as demonstrated in our
experiment.

1 Introduction
Existing literature in few-shot object segmentation has
mainly relied on manually labelled segmentation masks. A
few recent works [Rakelly et al., 2018; Zhang et al., 2019b;
Wang et al., 2019] started to conduct experiments using weak
annotations such as scribbles or bounding boxes. However,
these weak forms of supervision involve more manual work
compared to image level labels, which can be collected from
text and images publicly available on the web. Limited re-
search has been conducted on using image-level supervision

∗equally contributing

Figure 1: Overview of stacked co-attention to relate the support set
and query image using image-level labels. Nx: Co-attention stacked
N times. “K-shot” refers to using K support images.

for few-shot segmentation [Raza et al., 2019]. Most current
weakly supervised few-shot segmentation methods lag signif-
icantly behind their strongly supervised counterparts.

On the other hand, deep semantic segmentation networks
are very successful when trained and tested on relatively
large-scale manually labelled datasets such as PASCAL-
VOC [Everingham et al., 2015] and MS-COCO [Lin et al.,
2014]. However, the number of object categories they cover is
still limited despite the signiﬁcant sizes of the data used. The
limited number of annotated objects with pixel-wise labels in-
cluded in existing datasets restricts the applicability of deep
learning in inherently open-set domains such as robotics [De-
hghan et al., 2019; Pirk et al., 2019]. Human visual sys-
tem has the ability to generalize to new categories from a
few labelled samples.
It has been shown that adults and
even children demonstrate a phenomenon known as “stimu-
lus equivalence” when novel concepts are taught by engaging
learning to a combination of visual, textual and verbal stim-
uli [Sidman, 2009]. The relations learned from one modal-
ity to another (e.g. written words/pictures related to spoken

words) can be transferred to accelerate the learning of new
concepts and new relations (e.g. pictures of objects in re-
lation to written words) via the stimulus equivalence princi-
ple. Inspired by this, we propose a multi-modal interaction
module to bootstrap the efﬁciency of weakly supervised few-
shot object segmentation by combining the visual input with
neural word embeddings. Our method iteratively guides a
bi-directional co-attention between the support and the query
sets using both visual and neural word embedding inputs, us-
ing only image-level supervision as shown in Fig. 1. It outper-
forms [Raza et al., 2019] by 4.8% and improves over meth-
ods that use bounding box supervision [Zhang et al., 2019b;
Wang et al., 2019].

Most work in few-shot segmentation considers the static
setting where query and support images do not have tem-
poral relations. However, in real world applications such
as robotics, segmentation methods can beneﬁt from tempo-
ral continuity and multiple viewpoints. For real time seg-
mentation, it may be of tremendous beneﬁts to utilize tem-
poral knowledge existing in video sequences. Observations
that pixels moving together mostly belong to the same object
seem to be very common in videos, and it can be exploited
to improve segmentation accuracy. We propose a novel
setup, temporal object segmentation with few-shot learning
(TOSFL), where support and query images are temporally re-
lated. The TOSFL setup for video object segmentation gen-
eralizes to novel object classes as can be seen in our exper-
iments on Youtube-VOS dataset [Xu et al., 2018]. TOSFL
only requires image-level labels for the ﬁrst frames (support
images) to segment the objects that appear in the frames that
follow. The TOSFL setup is interesting because it is more
similar to the nature of learning of objects by human than the
strongly supervised static segmentation setup.

Youtube-VOS [Xu et al., 2018] provides a way to evaluate
on unseen categories. However, it does not utilize the cate-
gory labels in the segmentation model. Our setup relies on
the image-level label for the support image to segment dif-
ferent parts from the query image conditioned on the word
embeddings of this image-level label. In order to ensure the
evaluation for the few-shot method is not biased to a certain
category, it is best to split into multiple folds and evaluate on
different ones similar to [Shaban et al., 2017].

1.1 Contributions

• We propose a novel few-shot object segmentation algo-
rithm based on a multi-modal interaction module trained
using image-level supervision. It relies on a multi-stage
attention mechanism and uses both visual and semantic
representations to relate relevant spatial locations in the
support and query images.

• We propose a novel weakly supervised few-shot video
object segmentation setup. It complements the existing
few-shot object segmentation benchmarks by consider-
ing a practically important use case not covered by pre-
vious datasets. Video sequences are provided instead of
static images which can simplify the few-shot learning
problem.

• We conduct a comparative study of different architec-

tures proposed in this paper to solve few-shot object seg-
mentation with image-level supervision. Our method
compares favourably against the state-of-the-art meth-
ods relying on pixel-level supervision and outperforms
the most recent methods using weak annotations [Raza
et al., 2019; Wang et al., 2019; Zhang et al., 2019b].

2 Related Work

2.1 Few-shot Object Segmentation

Shaban et al. [2017] proposed the ﬁrst few-shot segmenta-
tion method using a second branch to predict the ﬁnal seg-
mentation layer parameters. Rakelly et al. [2018] proposed a
guidance network for few-shot segmentation where the guid-
ance branch receives the support set image-label pairs. Dong
and Xing [2018] utilized the second branch to learn proto-
types. Zhang et al. [2019b] proposed a few-shot segmen-
tation method based on a dense comparison module with a
siamese-like architecture that uses masked average pooling
to extract features on the support set, and an iterative opti-
mization module to reﬁne the predictions. Siam et al. [2019]
proposed a method to perform few-shot segmentation using
adaptive masked proxies to directly predict the parameters
of the novel classes. Zhang et al. [2019a] in a more recent
work proposed a pyramid graph network which learns atten-
tion weights between the support and query sets for further
label propagation. Wang et al. [2019] proposed prototype
alignment by performing both support-to-query and query-
to-support few-shot segmentation using prototypes.

The previous literature focused mainly on using strongly
labelled pixel-level segmentation masks for the few exam-
ples in the support set. It is labour intensive and impractical
to provide such annotations for every single novel class, es-
pecially in certain robotics applications that require to learn
online. A few recent works experimented with weaker an-
notations based on scribbles and/or bounding boxes [Rakelly
et al., 2018; Zhang et al., 2019b; Wang et al., 2019]. In our
opinion, the most promising approach to solving the problem
of intense supervision requirements in the few-shot segmen-
tation task, is to use publicly available web data with image-
level labels. Raza et al. [2019] made a ﬁrst step in this di-
rection by proposing a weakly supervised method that uses
image-level labels. However, the method lags signiﬁcantly
behind other approaches that use strongly labelled data.

2.2 Attention Mechanisms

Attention was initially proposed for neural machine transla-
tion models [Bahdanau et al., 2014]. Several approaches were
proposed for utilizing attention. Yang et al. [2016] proposed
a stacked attention network which learns attention maps se-
quentially on different levels. Lu et al. [2016] proposed co-
attention to solve a visual question and answering task by al-
ternately shifting attention between visual and question rep-
resentations. Lu et al. [2019] used co-attention in video ob-
ject segmentation between frames sampled from a video se-
quence. Hsieh et al. [2019] rely on attention mechanism to
perform one-shot object detection. However, they mainly use
it to attend to the query image since the given bounding box

Figure 2: Architecture of Few-Shot Object segmentation model with co-attention. The ⊕ operator denotes concatenation, ◦ denotes element-
wise multiplication. Only the decoder and multi-modal interaction module parameters are learned, while the encoder is pretrained on Ima-
geNet.

provides them with the region of interest in the support set im-
age. To the best of our knowledge, this work is the ﬁrst one to
explore the bidirectional attention between support and query
sets as a mechanism for solving the few-shot image segmen-
tation task with image-level supervision.

3 Proposed Method
The human perception system is inherently multi-modal. In-
spired from this and to leverage the learning of new concepts
we propose a multi-modal interaction module that embeds se-
mantic conditioning in the visual processing scheme as shown
in Fig. 2. The overall model consists of: (1) Encoder. (2)
Multi-modal Interaction module. (3) Segmentation Decoder.
The multi-modal interaction module is described in detail in
this section while the encoder and decoder modules are ex-
plained in Section 5.1. We follow a 1-way k-shot setting sim-
ilar to [Shaban et al., 2017].

3.1 Multi-Modal Interaction Module
One of the main challenges in dealing with the image-level
annotation in few-shot segmentation is that quite often both
support and query images may contain a few salient common
objects from different classes. Inferring a good prototype for
the object of interest from multi-object support images with-
out relying on pixel-level cues or even bounding boxes be-
comes particularly challenging. Yet, it is exactly in this sit-
uation, that we can expect the semantic word embeddings to
be useful at helping to disambiguate the object relationships
across support and query images. Below we discuss the tech-
nical details behind the implementation of this idea depicted
in Fig. 2. Initially, in a k-shot setting, a base network is used
to extract features from ith support set image I i
s and from
the query image Iq, which we denote as Vs ∈ RW ×H×C
and Vq ∈ RW ×H×C. Here H and W denote the height and
width of feature maps, respectively, while C denotes the num-
ber of feature channels. Furthermore, a projection layer is
used on the semantic word embeddings to construct z ∈ Rd

(d = 256). It is then spatially tiled and concatenated with
the visual features resulting in ﬂattened matrix representa-
tions ˜Vq ∈ RC×W H and ˜Vs ∈ RC×W H . An afﬁnity matrix
S is computed to capture the similarity between them via a
fully connected layer Wco ∈ RC×C learning the correlation
between feature channels:

S = ˜Vs

Wco ˜Vq.

T

The afﬁnity matrix S ∈ RW H×W H relates each pixel in ˜Vq
and ˜Vs. A softmax operation is performed on S row-wise and
column-wise depending on the desired direction of relation:

Sc = softmax(S), Sr = softmax(ST )

For example, column Sc
∗,j contains the relevance of the jth
spatial location in Vq with respect to all spatial locations of
Vs, where j = 1, ..., W H. The normalized afﬁnity matrix is
used to compute attention summaries Uq and Us:

Uq = ˜VsSc, Us = ˜VqSr.

The attention summaries are further reshaped such that
Uq, Us ∈ RW ×H×C and gated using a gating function fg
with learnable weights Wg and bias bg:

fg(Uq) = σ(Wg ∗ Uq + bg),
Uq = fg(Uq) ◦ Uq.

Here the ◦ operator denotes element-wise multiplication. The
gating function restrains the output to the interval [0, 1] using
a sigmoid activation function σ in order to mask the attention
summaries. The gated attention summaries Uq are concate-
nated with the original visual features Vq to construct the ﬁnal
output from the attention module to the decoder.

3.2 Stacked Gated Co-Attention
We propose to stack the multi-modal interaction module de-
scribed in Section 3.1 to learn an improved representation.

(a) V+S

(b) V

(c) S

Figure 3: Different variants for image-level labelled few-shot object segmentation. V+S: Stacked Co-Attention with Visual and Semantic
representations. V: Co-Attention with Visual features only. S: Conditioning on semantic representation only from word embeddings.

Stacking allows for multiple iterations between the support
and the query images. The co-attention module has two
streams fq, fs that are responsible for processing the query
image and the support set images respectively. The inputs to
q and V i
the co-attention module, V i
s , represent the visual fea-
tures at iteration i for query image and support image respec-
tively. In the ﬁrst iteration, V 0
s are the output visual
features from the encoder. Each multi-modal interaction then
follows the recursion ∀i = 0, .., N − 1:
q + fq(V i

q = φ(V i
V i+1

q and V 0

q , V i

s , z))

The nonlinear projection φ is performed on the output from
each iteration, which is composed of a 1x1 convolutional
layer followed by a ReLU activation function. We use resid-
ual connections in order to improve the gradient ﬂow and pre-
vent vanishing gradients. The support set features V i
s , ∀i =
0, .., N − 1 are computed similarly.

4 Temporal Object Segmentation with

Few-shot Learning Setup

We propose a novel few-shot video object segmentation
(VOS) task.
In this task, the image-level label of the ﬁrst
frame is provided to learn object segmentation in the sampled
frames from the ensuing sequence. This is a more challeng-
ing task than the one relying on the pixel-level supervision
in semi-supervised VOS. The task is designed as a binary
segmentation problem and the categories are split in multiple
folds, consistent with existing few-shot segmentation tasks
deﬁned on Pascal-5i and MS-COCO. This design ensures
that the proposed task assesses the ability of few-shot video
segmentation algorithms to generalize over unseen classes.
We utilize Youtube-VOS dataset training data which has 65
classes, and we split them into 5 folds. Each fold has 13
classes that are used as novel classes, while the rest are used
in the meta-training phase. A randomly sampled class Ys and
sequence V = {I1, I2, ..., IN } are used to construct the sup-
port set Sp = {(I1, Ys)} and query images Ii. For each query
image a binary segmentation mask M Y
s is constructed by la-
belling all the instances belonging to Ys as foreground. Ac-
cordingly, the same image can have multiple binary segmen-
tation masks depending on the sampled Ys.

5 Experiments

In this section we demonstrate results of experiments con-
ducted on the PASCAL-5i dataset [Shaban et al., 2017] com-
pared to state of the art methods in section 5.2. Not only do
we set strong baselines for image level labelled few shot seg-
mentation and outperform previously proposed work [Raza
et al., 2019], but we also perform close to the state of the
art conventional few shot segmentation methods that use de-
tailed pixel-wise segmentation masks. We then demonstrate
the results for the different variants of our approach depicted
in Fig. 3 and experiment with the proposed TOSFL setup in
section 5.3.

5.1 Experimental Setup
Network Details: We utilize a ResNet-50 [He et al., 2016]
encoder pre-trained on ImageNet [Deng et al., 2009] to ex-
tract visual features. The segmentation decoder is comprised
of an iterative optimization module (IOM) [Zhang et al.,
2019b] and an atrous spatial pyramid pooling (ASPP) [Chen
et al., 2017a,b]. The IOM module takes the output feature
maps from the multi-modal interaction module and the previ-
ously predicted probability map in a residual form.

Meta-Learning Setup: We sample 12,000 tasks during
the meta-training stage. In order to evaluate test performance,
we average accuracy over 5000 tasks with support and query
sets sampled from the meta-test dataset Dtest belonging to
classes Ltest. We perform 5 training runs with different ran-
dom generator seeds and report the average of the 5 runs and
the 95% conﬁdence interval.

Evaluation Protocol: PASCAL-5i splits PASCAL-VOC
20 classes into 4 folds each having 5 classes. The mean IoU
and binary IoU are the two metrics used for the evaluation
process. The mIoU computes the intersection over union for
all 5 classes within the fold and averages them neglecting the
background. Whereas the bIoU metric proposed by Rakelly
et al. [2018] computes the mean of foreground and back-
ground IoU in a class agnostic manner. We have noticed some
deviation in the validation schemes used in previous works.
Zhang et al. [2019b] follow a procedure where the valida-
tion is performed on the test classes to save the best model,
whereas Wang et al. [2019] do not perform validation and

Table 1: Quantitative results for 1-way, 1-shot segmentation on the PASCAL-5i dataset showing mean-Iou and binary-IoU. P: stands for
using pixel-wise segmentation masks for supervision. IL: stands for using weak supervision from Image-Level labels. BB: stands for using
bounding boxes for weak supervision.

Method
FG-BG
OSLSM [Shaban et al., 2017]
CoFCN [Rakelly et al., 2018]
PLSeg [Dong and Xing, 2018]
AMP [Siam et al., 2019]
PANet [Wang et al., 2019]
CANet [Zhang et al., 2019b]
PGNet [Zhang et al., 2019a]
CANet [Zhang et al., 2019b]
PANet [Wang et al., 2019]
[Raza et al., 2019]
Ours(V+S)-1
Ours(V+S)-2

Type
P
P
P
P
P
P
P
P
BB
BB
IL
IL
IL

1
-
33.6
36.7
-
41.9
42.3
52.5
56.0
-
-
-
49.5
42.5

2
-
55.3
50.6
-
50.2
58.0
65.9
66.9
-
-
-
65.5
64.8

1-shot
4
-
33.5
32.4
-
34.7
41.2
51.9
50.4
-
-
-
49.2
46.5

3
-
40.9
44.9
-
46.7
51.1
51.3
50.6
-
-
-
50.0
48.1

mIoU bIoU
55.1
-
-
40.8
60.1
41.1
61.2
-
62.2
43.4
66.5
48.1
66.2
55.4
69.9
56.0
52.0
-
45.1
-
58.7
-
65.6
53.5
50.5
64.1
±0.7 ±0.4

1
-
35.9
37.5
-
41.8
51.8
55.5
57.7
-
-
-
-
45.9

2
-
58.1
50.0
-
55.5
64.6
67.8
68.7
-
-
-
-
65.7

5-shot
3
-
42.7
44.1
-
50.3
59.8
51.9
52.9
-
-
-
-
48.6

4
-
39.1
33.9
-
39.9
46.5
53.2
54.6
-
-
-
-
46.6

mIoU
-
43.9
41.4
-
46.9
55.7
57.1
58.5
-
52.8
-
-
51.7
±0.07

rather train for a ﬁxed number of iterations. We choose the
more challenging approach in [Wang et al., 2019].

Training Details: During the meta-training, we freeze
ResNet-50 encoder weights while learning both the multi-
modal interaction module and the decoder. We train all mod-
els using momentum SGD with learning rate 0.01 that is re-
duced by 0.1 at epoch 35, 40 and 45 and momentum 0.9. L2
regularization with a factor of 5x10−4 is used to avoid over-
ﬁtting. Batch size of 4 and input resolution of 321×321 are
used during training with random horizontal ﬂipping and ran-
dom centered cropping for the support set. An input resolu-
tion of 500×500 is used for the meta-testing phase similar
to [Shaban et al., 2017].
In each fold the model is meta-
trained for a maximum number of 50 epochs on the classes
outside the test fold.

5.2 Comparison to the state-of-the-art

We compare the result of our best variant (see Fig. 3), i.e:
Stacked Co-Attention (V+S) against the other state of the
art methods for 1-way 1-shot and 5-shot segmentation on
PASCAL-5i in Table 1. We report the results for different val-
idation schemes. Ours(V+S)-1 follows [Zhang et al., 2019b]
and Ours(V+S)-2 follows [Wang et al., 2019]. Without the
utilization of segmentation mask or even sparse annotations,
our method with the least supervision of image level labels
performs (53.5%) close to the current state of the art strongly
supervised methods (56.0%) in 1-shot case and outperforms
the ones that use bounding box annotations. It improves over
the previously proposed image-level supervised method with
a signiﬁcant margin (4.8%). For the k-shot extension of our
method we perform average of the attention summaries dur-
ing the meta-training on the k-shot samples from the support
set. Table 2 demonstrates results on MS-COCO [Lin et al.,
2014] compared to the state of the art method using pixel-
wise segmentation masks for the support set.

Table 2: Quantitative Results on MS-COCO Few-shot 1-way.

Method
PANet [Wang et al., 2019]
Ours-(V+S)

Type
P
IL

1-shot
20.9
15.0

5-shot
29.7
15.6

5.3 Ablation Study

We perform an ablation study to evaluate different variants
of our method depicted in Fig. 3. Table 3 shows the results
on the three variants we proposed on PASCAL-5i. It clearly
shows that using the visual features only (V-method), lags 5%
behind utilizing word embeddings in the 1-shot case. This is
mainly due to having multiple common objects between the
support set and the query image. Semantic representation ob-
viously helps to resolve the ambiguity and improves the result
signiﬁcantly as shown in Figure 5. Going from 1 to 5 shots,
the V-method improves, because multiple shots are likely to
repeatedly contain the object of interest and the associated
ambiguity decreases, but still it lags behind both variants sup-
ported by semantic input. Interestingly, our results show that
the baseline of conditioning on semantic representation is a
very competitive variant: in the 1-shot case it even outper-
forms the (V+S) variant. However, the bottleneck in using
the simple scheme to integrate semantic representation de-
picted in Fig. 3c is that it is not able to beneﬁt from multiple
shots in the support set. The (V+S)-method in the 5-shot case
improves over the 1-shot case by 1.2% on average over the
5 runs, which conﬁrms its ability to effectively utilize more
abundant visual features in the 5-shot case. One reason could
explain the strong performance of the (S) variant. In the case
of a single shot, the word embedding pretrained on a mas-
sive text database may provide a more reliable guidance sig-
nal than a single image containing multiple objects that does
not necessarily have visual features close to the object in the
query image.

Table 4 shows the results on our proposed novel video
segmentation task, comparing variants of the proposed ap-

(a) ’bicycle’

(b) ’bottle’

(c) ’bird’

(d) ’bicycle’

(e) ’bird’

(f) ’boat’

Figure 4: Qualitative evaluation on PASCAL-5i 1-way 1-shot. The support set and prediction on the query image are shown in pairs.

Table 3: Ablation Study on 4 folds of Pascal-5i for few-shot seg-
mentation for different variants showing mean-IoU. V: visual, S: se-
mantic. V+S: both features.

Method
V
S
V+S

1-shot
44.4 ± 0.3
51.2 ± 0.6
50.5 ± 0.7

5-shot
49.1 ± 0.3
51.4 ± 0.3
51.7 ± 0.07

Table 4: Quantitative Results on Youtube-VOS One-shot weakly su-
pervised setup showing IoU per fold and mean-IoU over all folds
similar to pascal-5i. V: visual, S: semantic. V+S: both features.

Method
V
S
V+S

1
40.8
42.7
46.1

2
34.0
40.8
42.0

3
44.4
48.7
50.7

4
35.0
38.8
41.2

5
35.5
37.6
39.2

Mean-IoU
38.0 ± 0.7
41.7 ± 0.7
43.8 ± 0.5

proach. As previously, the baseline V-method based on co-
attention module with no word embeddings, similar to [Lu
et al., 2019], lags behind both S- and (V+S)-methods. It is
worth noting that unlike the conventional video object seg-
mentation setups, the proposed video object segmentation
task poses the problem as a binary segmentation task con-
ditioned on the image-level label. Both support and query
frames can have multiple salient objects appearing in them,
however the algorithm has to segment only one of them cor-
responding to the image-level label provided in the support
frame. According to our observations, this multi-object situa-
tion occurs in this task much more frequently than e.g. in the
case of Pascal-5i. Additionally, not only the target, but all the
nuisance objects present in the video sequence will relate via
different viewpoints or deformations. We demonstrate in Ta-
ble 4 that the (V+S)-method’s joint visual and semantic pro-
cessing in such scenario clearly provides signiﬁcant gain.

6 Conclusion
In this paper we proposed a multi-modal interaction mod-
ule that relates the support set image and query image us-

(a) Label ’Bike’

(b) Prediction (V)

(c) Prediction (V+S)

Figure 5: Visual Comparison between the predictions from two vari-
ants of our method.

ing both visual and word embeddings. We proposed to meta-
learn a stacked co-attention module that guides the segmen-
tation of the query based on the support set features and vice
versa. The two main takeaways from the experiments are that
(i) few-shot segmentation signiﬁcantly beneﬁts from utilizing
word embeddings and (ii) it is viable to perform high qual-
ity few-shot segmentation using stacked joint visual semantic
processing with weak image-level labels.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neu-
ral machine translation by jointly learning to align and translate.
arXiv preprint arXiv:1409.0473, 2014.

Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin
Murphy, and Alan L Yuille. Deeplab: Semantic image segmen-
tation with deep convolutional nets, atrous convolution, and fully
connected crfs. IEEE transactions on pattern analysis and ma-
chine intelligence, 40(4):834–848, 2017.

Liang-Chieh Chen, George Papandreou, Florian Schroff, and
Hartwig Adam. Rethinking atrous convolution for semantic im-
age segmentation. arXiv preprint arXiv:1706.05587, 2017.

Masood Dehghan, Zichen Zhang, Mennatullah Siam, Jun Jin, Laura
Petrich, and Martin Jagersand. Online object and task learning
via human robot interaction. In 2019 International Conference
on Robotics and Automation (ICRA), pages 2132–2138. IEEE,
2019.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-
Fei.
In
Imagenet: A large-scale hierarchical image database.
2009 IEEE conference on computer vision and pattern recogni-
tion, pages 248–255. Ieee, 2009.

Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex
Smola. Stacked attention networks for image question answer-
ing. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 21–29, 2016.

Chi Zhang, Guosheng Lin, Fayao Liu, Jiushuang Guo, Qingyao Wu,
and Rui Yao. Pyramid graph networks with connection attentions
for region-based one-shot semantic segmentation. In Proceedings
of the IEEE International Conference on Computer Vision, pages
9587–9595, 2019.

Chi Zhang, Guosheng Lin, Fayao Liu, Rui Yao, and Chunhua Shen.
Canet: Class-agnostic segmentation networks with iterative re-
ﬁnement and attentive few-shot learning. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition,
pages 5217–5226, 2019.

Nanqing Dong and Eric P. Xing. Few-shot semantic segmentation
with prototype learning. In BMVC, volume 3, page 4, 2018.

Mark Everingham, S.M. Ali Eslami, Luc Van Gool, Christopher K.I.
Williams, John Winn, and Andrew Zisserman. The pascal visual
object classes challenge: A retrospective. International journal
of computer vision, 111(1):98–136, 2015.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
In Proceedings of the
residual learning for image recognition.
IEEE conference on computer vision and pattern recognition,
pages 770–778, 2016.

Ting-I Hsieh, Yi-Chen Lo, Hwann-Tzong Chen, and Tyng-Luh Liu.
One-shot object detection with co-attention and co-excitation.
In Advances in Neural Information Processing Systems, pages
2721–2730, 2019.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro
Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick.
Microsoft coco: Common objects in context. In European con-
ference on computer vision, pages 740–755. Springer, 2014.

Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchi-
cal question-image co-attention for visual question answering. In
Advances In Neural Information Processing Systems, pages 289–
297, 2016.

Xiankai Lu, Wenguan Wang, Chao Ma, Jianbing Shen, Ling Shao,
and Fatih Porikli. See more, know more: Unsupervised video
object segmentation with co-attention siamese networks. In Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 3623–3632, 2019.

S¨oren Pirk, Mohi Khansari, Yunfei Bai, Corey Lynch, and Pierre
Sermanet. Online object representations with contrastive learn-
ing. arXiv preprint arXiv:1906.04312, 2019.

Kate Rakelly, Evan Shelhamer, Trevor Darrell, Alyosha Efros, and
Sergey Levine. Conditional networks for few-shot semantic seg-
mentation. 2018.

Hasnain Raza, Mahdyar Ravanbakhsh, Tassilo Klein, and Moin
In Proceed-
Nabi. Weakly supervised one shot segmentation.
ings of the IEEE International Conference on Computer Vision
Workshops, pages 0–0, 2019.

Amirreza Shaban, Shray Bansal, Zhen Liu, Irfan Essa, and By-
ron Boots. One-shot learning for semantic segmentation. arXiv
preprint arXiv:1709.03410, 2017.

Mennatullah Siam, Boris N Oreshkin, and Martin Jagersand. Amp:
Adaptive masked proxies for few-shot segmentation. In Proceed-
ings of the IEEE International Conference on Computer Vision,
pages 5249–5258, 2019.

Murray Sidman. Equivalence relations and behavior: An introduc-
tory tutorial. The Analysis of verbal behavior, 25(1):5–17, 2009.

Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou, and Ji-
ashi Feng. Panet: Few-shot image semantic segmentation with
prototype alignment. In Proceedings of the IEEE International
Conference on Computer Vision, pages 9197–9206, 2019.

Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang,
Jianchao Yang, and Thomas Huang. Youtube-vos: A large-
arXiv preprint
scale video object segmentation benchmark.
arXiv:1809.03327, 2018.

