Learning to Discover Cross-Domain Relations
with Generative Adversarial Networks

Taeksoo Kim 1 Moonsu Cha 1 Hyunsoo Kim 1 Jung Kwon Lee 1 Jiwon Kim 1

7
1
0
2
 
y
a
M
 
5
1
 
 
]

V
C
.
s
c
[
 
 
2
v
2
9
1
5
0
.
3
0
7
1
:
v
i
X
r
a

Abstract
While humans easily recognize relations be-
tween data from different domains without any
supervision, learning to automatically discover
them is in general very challenging and needs
many ground-truth pairs that illustrate the re-
lations. To avoid costly pairing, we address
the task of discovering cross-domain relations
given unpaired data. We propose a method based
on generative adversarial networks that learns
to discover relations between different domains
(DiscoGAN). Using the discovered relations, our
proposed network successfully transfers style
from one domain to another while preserving key
attributes such as orientation and face identity.

1. Introduction

Relations between two different domains, the way in which
concepts, objects, or people are connected, arise ubiqui-
tously. Cross-domain relations are often natural to humans.
For example, we recognize the relationship between an En-
glish sentence and its translated sentence in French. We
also choose a suit jacket with pants or shoes in the same
style to wear.

Can machines also achieve a similar ability to relate two
different image domains? This question can be reformu-
lated as a conditional image generation problem. In other
words, ﬁnding a mapping function from one domain to the
other can be thought as generating an image in one do-
main given another image in the other domain. While this
problem tackled by generative adversarial networks (GAN)
(Isola et al., 2016) has gained a huge attention recently,
most of today’s training approaches use explicitly paired
data, provided by human or another algorithm.

This problem also brings an interesting challenge from a
learning point of view. Explicitly supervised data is sel-
dom available and labeling can be labor intensive. More-

1SK T-Brain, Seoul, South Korea. Correspondence to: Taeksoo

Kim <jazzsaxmaﬁa@sktbrain.com>.

(a) Learning cross-domain relations without any extra label

(b) Handbag images (input) & Generated shoe images (output)

(c) Shoe images (input) & Generated handbag images (output)

Figure 1. Our GAN-based model trains with two independently
collected sets of images and learns how to map two domains with-
out any extra label. In this paper, we reduces this problem into
generating a new image of one domain given an image from the
other domain. (a) shows a high-level overview of the training pro-
cedure of our model with two independent sets (e.g. handbag im-
ages and shoe images). (b) and (c) show results of our method.
Our method takes a handbag (or shoe) image as an input, and
generates its corresponding shoe (or handbag) image. Again, it’s
worth noting that our method does not take any extra annotated
supervision and can self-discover relations between domains.

over, pairing images can become tricky if corresponding
images are missing in one domain or there are multiple best
candidates. Hence, we push one step further by discovering
relations between two visual domains without any explic-
itly paired data.

Learning to Discover Cross-Domain Relations with Generative Adversarial Networks

Figure 2. Three investigated models. (a) standard GAN (Goodfellow et al., 2014), (b) GAN with a reconstruction loss, (c) our proposed
model (DiscoGAN) designed to discover relations between two unpaired, unlabeled datasets. Details are described in Section 3.

In order to tackle this challenge, we introduce a model that
discovers cross-domain relations with GANs (DiscoGAN).
Unlike previous methods, our model can be trained with
two sets of images without any explicit pair labels (see Fig-
ure 1a) and does not require any pre-training. Our proposed
model can then take one image in one domain as an input
and generate its corresponding image in another domain
(see Figure 1b). The core of our model is based on two
different GANs coupled together – each of them ensures
our generative functions can map each domain to its coun-
terpart domain. A key intuition we rely on is to constraint
all images in one domain to be representable by images in
the other domain. For example, when learning to generate
a shoe image based on each handbag image, we force this
generated image to be an image-based representation of the
handbag image (and hence reconstruct the handbag image)
through a reconstruction loss, and to be as close to images
in the shoe domain as possible through a GAN loss. We
use these two properties to encourage the mapping between
two domains to be well covered on both directions (i.e. en-
couraging one-to-one rather than many-to-one or one-to-
many). In the experimental section, we show that this sim-
ple intuition discovered common properties and styles of
two domains very well.

Both experiments on toy domain and real world image
datasets support the claim that our proposed model is well-
suited for discovering cross-domain relations. When trans-
lating data points between simple 2-dimensional domains
and between face image domains, our DiscoGAN model
was more robust to the mode collapse problem compared
to two other baseline models. It also learns the bidirectional

mapping between two image domains, such as faces, cars,
chairs, edges and photos, and successfully apply them in
image translation. Translated images consistently change
speciﬁed attributes such as hair color, gender and orienta-
tion while maintaining all other components. Results also
show that our model is robust to repeated application of
translation mappings.

2. Model

We now formally deﬁne cross-domain relations and present
the problem of learning to discover such relations in two
different domains. Standard GAN model and a similar vari-
ant model with additional components are investigated for
their applicability for this task. Limitations of these mod-
els are then explained, and we propose a new architecture
based on GANs that can be used to discover cross-domain
relations.

2.1. Formulation

Relation is mathematically deﬁned as a function GAB
that maps elements from its domain A to elements in its
codomain B and GBA is similarly deﬁned. In fully unsu-
pervised setting, GAB and GBA can be arbitrarily deﬁned.
To ﬁnd a meaningful relation, we need to impose a con-
dition on the relation of interest. Here, we constrain rela-
tion to be a one-to-one correspondence (bijective mapping).
That means GAB is the inverse mapping of GBA.

The range of function GAB, the complete set of all possible
resulting values GAB(xA) for all xA’s in domain A, should
be contained in domain B and similarly for GBA(xB).

Learning to Discover Cross-Domain Relations with Generative Adversarial Networks

Figure 3. Illustration of our models on simpliﬁed one dimensional domains. (a) ideal mapping from domain A to domain B in which the
two domain A modes map to two different domain B modes, (b) GAN model failure case, (c) GAN with reconstruction model failure
case.

We now relate these constraints to objective functions. Ide-
ally, the equality GBA ◦ GAB(xA) = xA should be sat-
isﬁed, but this hard constraint is difﬁcult to optimize and
relaxed soft constraint is more desirable in the view of
optimization. For this reason, we minimize the distance
d(GBA ◦ GAB(xA), xA), where any form of metric func-
tion (L1, L2, Huber loss) can be used. Similarly, we also
need to minimize d(GAB ◦ GBA(xB), xB).

Guaranteeing that GAB maps to domain B is also very
to optimize. We relax this constraint as fol-
difﬁcult
lows: we instead minimize generative adversarial
loss
−ExA ∼PA
[log DB(GAB(xA))]. Similarly, we minimize
−ExB ∼PB
[log DA(GBA(xB ))].
Now, we explore several GAN architectures to learn with
these loss functions.

2.2. Notation and Architecture

We use the following notations in sections below. A genera-
tor network is denoted GAB : R64×64×3
, and
the subscripts denote the input and output domains and su-
perscripts denote the input and output image size. The dis-
criminator network is denoted as DB : R64×64×3
→ [0, 1],
and the subscript B denotes that it discriminates images in
domain B. Notations GBA and DA are used similarly.

→ R64×64×3
B

B

A

Each generator takes image of size 64 × 64 × 3 and feeds it
through an encoder-decoder pair. The encoder part of each
generator is composed of convolution layers with 4 × 4 ﬁl-
ters, each followed by leaky ReLU (Maas et al., 2013; Xu
et al., 2015). The decoder part is composed of deconvolu-
tion layers with 4 × 4 ﬁlters, followed by a ReLU, and out-
puts a target domain image of size 64 × 64 × 3. The number
of convolution and deconvolution layers ranges from four
to ﬁve, depending on the domain.

with 4 × 4 ﬁlters, and a ﬁnal sigmoid to output a scalar
output between [0, 1].

2.3. GAN with a Reconstruction Loss

We ﬁrst consider a standard GAN model (Goodfellow
et al., 2014) for the relation discovery task (Figure 2a).
Originally, a standard GAN takes random Gaussian noise
z, encodes it into hidden features h and generates images
such as MNIST digits. We make a slight modiﬁcation to
this model to ﬁt our task: the model we use takes in image
as input instead of noise.

In addition, since this architecture only learns one mapping
from domain A to domain B, we add a second generator
that maps domain B back into domain A (Figure 2b). We
also add a reconstruction loss term that compares the input
image with the reconstructed image. With these additional
changes, each generator in the model can learn mapping
from its input domain to output domain and discover rela-
tions between them.

A generator GAB translates input image xA from domain A
into xAB in domain B. The generated image is then trans-
lated into a domain A image xABA to match the original in-
put image (Equation 1, 2). Various forms of distance func-
tions, such as MSE, cosine distance, and hinge-loss, can be
used as the reconstruction loss d (Equation 3). The trans-
lated output xAB is then scored by the discriminator which
compares it to a real domain B sample xB .

xAB = GAB(xA)
xABA = GBA(xAB ) = GBA ◦ GAB(xA )
= d(GBA ◦ GAB(xA ), xA )
= −ExA ∼PA

[log DB(GAB(xA))]

LCON STA
LGANB

(1)

(2)

(3)

(4)

The discriminator is similar to the encoder part of the gen-
erator. In addition to the convolution layers and leaky Re-
LUs, the discriminator has an additional convolution layer

The generator GAB receives two types of losses – a re-
construction loss LCON STA
(Equation 3) that measures how
well the original input is reconstructed after a sequence of

Learning to Discover Cross-Domain Relations with Generative Adversarial Networks

two generations, and a standard GAN generator loss LGANB
(Equation 4) that measures how realistic the generated im-
age is in domain B. The discriminator receives the standard
GAN discriminator loss of Equation 6.

One key difference from the previous model is that input
images from both domains are reconstructed and that there
are two reconstruction losses: LCON STA and LCON STB .

LGAB

= LGANB

+ LCON STA

LG = LGAB
= LGANB

+ LGBA
+ LCON STA

+ LGANA

+ LCON STB

LDB

= − ExB ∼PB
− ExA ∼PA

[log DB(xB )]
[log(1 − DB(GAB(xA )))]

LD = LDA

+ LDB

(5)

(6)

(7)

(8)

During training, the generator GAB learns the mapping
from domain A to domain B under two relaxed constraints:
that domain A maps to domain B, and that the mapping
on domain B is reconstructed to domain A. However, this
model lacks a constraint on mapping from B to A, and these
two conditions alone does not guarantee a cross-domain re-
lation (as deﬁned in section 2.1) because the mapping sat-
isfying these constraints is one-directional. In other words,
the mapping is an injection, not bijection, and one-to-one
correspondence is not guaranteed.

Consider the two possibly multi-modal image domains A
and B. Figure 3 illustrates the two multi-modal data do-
mains on a simpliﬁed one-dimensional representation. Fig-
ure 3a shows the ideal mapping from input domain A to do-
main B, where each mode of data is mapped to a separate
mode in the target domain. Figure 3b, in contrast, shows the
mode collapse problem, a prevalent phenomenon in GANs,
where data from multiple modes of a domain map to a sin-
gle mode of a different domain. For instance, this case is
where the mapping GAB maps images of cars in two dif-
ferent orientations into the same mode of face images.

In some sense, the addition of a reconstruction loss to a
standard GAN is an attempt to remedy the mode collapse
problem. In Figure 3c, two domain A modes are matched
with the same domain B mode, but the domain B mode can
only direct to one of the two domain A modes. Although
the additional reconstruction loss LCON STA
forces the re-
constructed sample to match the original (Figure 3c), this
change only leads to a similar symmetric problem. The re-
construction loss leads to an oscillation between the two
states and does not resolve mode-collapsing.

2.4. Our Proposed Model: Discovery GAN

Our proposed GAN model for relation discovery – Disco-
GAN – couples the previously proposed model (Figure 2c).
Each of the two coupled models learns the mapping from
one domain to another, and also the reverse mapping to for
reconstruction. The two models are trained together simul-
taneously. The two generators GAB’s and the two gener-
ators GBA’s share parameters, and the generated images
xBA and xAB are each fed into separate discriminators LDA
and LDB , respectively.

As a result of coupling two models, the total generator loss
is the sum of GAN loss and reconstruction loss for each
partial model (Equation 7). Similarly, the total discrimina-
tor loss LD is a sum of discriminator loss for the two dis-
criminators DA and DB, which discriminate real and fake
images of domain A and domain B (Equation 8).

Now, this model is constrained by two LGAN losses and
two LCON ST losses. Therefore a bijective mapping is
achieved, and a one-to-one correspondence, which we de-
ﬁned as cross-domain relation, can be discovered.

3. Experiments

3.1. Toy Experiment

To empirically demonstrate our explanations on the differ-
ences between a standard GAN, a GAN with reconstruc-
tion loss and our proposed model (DiscoGAN), we de-
signed an illustrative experiment based on synthetic data
in 2-dimensional A and B domains. Both source and target
data samples are drawn from Gaussian mixture models.

In Figure 4, the left-most ﬁgure shows the initial state of toy
experiment where all the A domain modes map to almost
a single point because of initialization of the generator. For
all other plots the target domain 2D plane is shown with tar-
get domain modes marked with black ‘x’s. Colored points
on B domain planes represent samples from A domain that
are mapped to the B domain, and each color denotes sam-
ples from each A domain mode. In this case, the task is to
discover cross-domain relations between the A and B do-
main and translate samples from ﬁve A domain modes into
the B domain, which has ten modes spread around the arc
of a circle.

We use a neural network with three linear layers that are
each followed by a ReLU nonlinearity as the generator. For
the discriminator we use ﬁve linear layers that are each
followed by a ReLU, except for the last layer which is
switched out with a sigmoid that outputs a scalar ∈ [0, 1].
The colored background shows the output value of the
discriminator DB, which discriminates real target domain
samples from synthetic, translated samples from domain
A. The contour lines show regions of same discriminator
value.

Learning to Discover Cross-Domain Relations with Generative Adversarial Networks

(a)

(b)

(c)

(d)

Figure 4. Toy domain experiment results. The colored background shows the output value of the discriminator. ’x’ marks denote different
modes in B domain, and colored circles indicate mapped samples of domain A to domain B, where each color corresponds to a different
mode. (a) ten target domain modes and initial translations, (b) standard GAN model, (c) GAN with reconstruction loss, (d) our proposed
model DiscoGAN

The training was performed for 50,000 iterations, and due
to the domain simplicity our model often converged much
earlier. The results from this experiment match our claim
and illustrations in Figure 4 and the resulting translated
samples show very different behavior depending on the
model used.

In the baseline (standard GAN) case, many translated
points of different colors are located around the same B
domain mode. For example, navy and light-blue colored
points are located together, as well as green and orange col-
ored points. This result illustrates the mode-collapse prob-
lem of GANs since points of multiple colors (multiple A
domain modes) are mapped to the same B domain mode.
The baseline model still oscillate around B modes through-
out the iterations.

In the case of GAN with a reconstruction loss, the collaps-
ing problem is less prevalent, but navy, green and light-blue
points still overlap at a few modes. The contour plot also
demonstrates the difference from baseline: regions around
all B modes are leveled in a green colored plateau in the
baseline, allowing translated samples to freely move be-
tween modes, whereas in the single model case the regions
between B modes are clearly separated.

In addition, both this model and the standard GAN model
fail to cover all modes in B domain since the mapping from
A domain to B domain is injective. Our proposed Disco-
GAN model, on the other hand, is able to not only prevent
mode-collapse by translating into distinct well-bounded re-
gions that do not overlap, but also generate B samples in all
ten modes as the mappings in our model is bijective. It is
noticeable that the discriminator for B domain is perfectly
fooled by translated samples from A domain around B do-
main modes.

Although this experiment is limited due to its simplicity,
the results clearly support the superiority of our proposed
model over other variants of GANs.

3.2. Real Domain Experiment

To evaluate whether our DiscoGAN successfully learns
underlying relationship between domains, we trained and
tested our model using several image-to-image translation
tasks that require the use of discovered cross-domain rela-
tions between source and target domains.

In each real domain experiment, all input images and trans-
lated images were of size 64 × 64 × 3. For training, we
used learning rate of 0.0002 and used the Adam optimizer
(Kingma & Ba, 2015) with β1 = 0.5 and β2 = 0.999.
We applied Batch Normalization (Ioffe & Szegedy, 2015)
to all convolution and deconvolution layers except the ﬁrst
and the last layers, weight decay regularization coefﬁcient
of 10−4 and minibatch of size 200. All computations were
conducted on a single machine with an Nvidia Titan X Pas-
cal GPU and an Intel(R) Xeon(R) E5-1620 CPU.

3.2.1. CAR TO CAR, FACE TO FACE

We used a Car dataset (Fidler et al., 2012) which consists
of rendered images of 3D car models with varying azimuth
angles at 15◦ intervals. We split the dataset into train and
test sets and again split the train set into two groups, each
of which is used as A domain and B domain samples. In
addition to training a standard GAN model, a GAN with
a reconstruction model and a proposed DiscoGAN model,
we also trained a regressor that predicts the azimuth angle
of a car image using the train set. To evaluate, we translated
images in the test set using each of the three trained mod-
els, and azimuth angles were predicted using the regressor
for both input and translated images. Figure 5 shows the
predicted azimuth angles of input and translated images for
each model. In standard GAN and GAN with reconstruc-
tion (5a and 5b), most of the red dots are grouped in a few
clusters, indicating that most of the input images are trans-
lated into images with same azimuth, and that these mod-
els suffer from mode collapsing problem as predicted and

Learning to Discover Cross-Domain Relations with Generative Adversarial Networks

Figure 5. Car to Car translation experiment. Horizontal and vertical axes in the plots indicate predicted azimuth angles of input and
translated images, where the angle of input image ranges from -75◦ to 75◦. RMSE with respect to ground truth (blue lines) are shown
in each plot. Images in the second row are examples of input car images ranging from -75◦ to 75◦ at 15◦ intervals. Images in the third
row are corresponding translated images. (a) plot of standard GAN (b) GAN with reconstruction (c) DiscoGAN. The angles of input and
output images are highly correlated when our proposed DiscoGAN model is used. Note the angles of input and translated car images are
reversed with respect to 0◦ (i.e. mirror images).

In Figure 7a, we can see that various facial features are
well-preserved while a single desired attribute (gender) is
changed. Also, 7b and 7d shows that background is also
well-preserved and images are visually natural, although
the background does change in a few cases such as Figure
7c. An extension to this experiment was sequentially apply-
ing several translations – for example, changing the gender
and then the hair color (7e), or repeatedly applying gender
transforms (7f).

shown in Figures 3 and 4. Our proposed DiscoGAN (5c),
on the other hand, shows strong correlation between pre-
dicted angles of input and translated images, indicating that
our model successfully discovers azimuth relation between
the two domains. In this experiment, the translated images
either have the same azimuth range (5b), or the opposite
(5a and 5c) of the input images.

Next, we use a Face dataset (Paysan et al., 2009) shown in
Figure 6a, in which the data images vary in azimuth rota-
tion from -90◦ to +90◦. Similar to previous car to car exper-
iment, input images in the -90◦ to +90◦ rotation range gen-
erated output images either in the same range, from -90◦ to
+90◦, or the opposite range, from +90◦ to -90◦ when our
proposed model was used (6d). We also trained a standard
GAN and a GAN with reconstruction loss for comparison.
When a standard GAN and GAN with reconstruction loss
were used, the generated images do not vary as much as the
input images in terms of rotation. In this sense, similar to
what has been shown in previous Car to Car experiment,
the two models suffered from mode collapse.

3.2.2. FACE CONVERSION

In terms of the amount of related information between two
domains, we can consider a few extreme cases: two do-
mains sharing almost all features and two domains sharing
only one feature. To investigate former case, we applied the
face attribute conversion task on CelebA dataset (Liu et al.,
2015), where only one feature, such as gender or hair color,
varies between two domains and all the other facial features
are shared. The results are listed in Figure 7.

Figure 6. Face to Face translation experiment. (a) input face im-
ages from -90◦ to +90◦ (b) results from a standard GAN (c) results
from GAN with a reconstruction loss (d) results from our Disco-
GAN. Here our model generated images in the opposite range,
from +90◦ to -90◦.

Learning to Discover Cross-Domain Relations with Generative Adversarial Networks

Figure 7. (a,b) Translation of gender in Facescrub dataset and CelebA dataset. (c) Blond to black and black to blond hair color conversion
in CelebA dataset. (d) Wearing eyeglasses conversion in CelebA dataset (e) Results of applying a sequence of conversion of gender and
hair color (left to right) (f) Results of repeatedly applying the same conversions (upper: hair color, lower: gender)

Learning to Discover Cross-Domain Relations with Generative Adversarial Networks

Figure 8. Discovering relations of images from visually very different object classes. (a) chair to car translation. DiscoGAN is trained
on chair and car images (b) car to face translation. DiscoGAN is trained on car and face images. Our model successfully pairs images
with similar orientation.

feature of the input images while preserving visual features
of car and face domain, respectively.

3.2.4. EDGES-TO-PHOTOS

Edges-to-photos is an interesting task as it is a 1-to-N prob-
lem, where a single edge image of items such as shoes and
handbags can generate multiple colorized images of such
items. In fact, an edge image can be colored in inﬁnitely
many ways. We validated that our DiscoGAN performs
very well on this type of image-to-image translation task
and generate realistic photos of handbags (Zhu et al., 2016)
and shoes (Yu & Grauman, 2014). The generated images
are presented in Figure 9.

3.2.5. HANDBAG TO SHOES, SHOES TO HANDBAG

Finally, we investigated the case with two domains that are
visually very different, where shared features are not ex-
plicit even to humans. We trained a DiscoGAN using pre-
viously used handbags and shoes datasets, not assuming
any speciﬁc relation between those two. In the translation
results shown in Figure 1, our proposed model discovers
fashion style as a related feature between the two domains.
Note that translated results not only have similar colors and
patterns, but they also have similar level of fashion formal-
ity as the input fashion item.

4. Related Work

Recently, a novel method to train generative models named
Generative Adversarial Networks (GANs) (Goodfellow
et al., 2014) was developed. A GAN is composed of two
modules – a generator G and a discriminator D. The gen-
erator’s objective is to generate (synthesize) data samples
whose distribution closely matches that of real data sam-

Figure 9. Edges to photos experiment. Our model is trained on a
set of object sketches and colored images and learns to generate
new sketches or photos. (a) colored images of handbags are gener-
ated from sketches of handbags, (b) colored images of shoes are
generated from sketches of shoes, (c) sketches of handbags are
generated from colored images of handbags

3.2.3. CHAIR TO CAR, CAR TO FACE

We also investigated the opposite case where there is a sin-
gle shared feature between two domains. 3D rendered im-
ages of chair (Aubry et al., 2014) and the previously used
car and face datasets (Fidler et al., 2012; Paysan et al.,
2009) were used in this task. All three datasets vary along
the azimuth rotation. Figure 8 shows the results of image-
to-image translation from chair to car and from car to face
datasets. The translated images clearly match the rotation

Learning to Discover Cross-Domain Relations with Generative Adversarial Networks

ples while the discriminator’s objective is to distinguish
real ones from generated samples. The two models G and
D, formulated as a two-player minimax game, are trained
simultaneously.

Researchers have studied GANs vigorously in two years:
network models such as LAPGAN (Denton et al., 2015)
and DCGAN (Radford et al., 2016) and improved training
techniques (Salimans et al., 2016; Arjovsky et al., 2017).
More recent GAN works are described in (Goodfellow,
2017).

Several methods were developed to generate images
based on GANs. Conditional Generative Adversarial Nets
(cGANs) (Mirza & Osindero, 2014) use MNIST digit class
label as an additional information to both generator and
discriminator and can generate digit images of the speci-
ﬁed class. Similarly, Dosovitskiy et al. (2015) showed that
GAN can generate images of objects based on speciﬁed
characteristic codes such as color and viewpoint. Other ap-
proaches used conditional features from a completely dif-
ferent domain for image generation. For example, Reed
et al. (2016) used encoded text description of images as the
conditional information to generating images that match
the description.

Some researchers have attempted to use multiple GANs in
prior works. (Liu & Tuzel, 2016) proposed to couple two
GANs (coupled generative adversarial networks, CoGAN)
in which two generators and two discriminators are cou-
pled by weight-sharing to learn the joint distribution of im-
ages in two different domains without using pair-wise data.
In Stacked GANs (StackGAN) (Zhang et al., 2016), two
GANs are arranged sequentially where the Stage-I GAN
generates low resolution images given text description and
the Stage-II GAN improves the generated image into high
resolution images. Similarly, Style and Structure GAN (S2-
GAN) (Wang & Gupta, 2016) used two sequentially con-
nected GANs where the Structure GAN ﬁrst generates sur-
face normal image and the Style GAN transforms it into
natural indoor scene image.

In order to control speciﬁc attributes of an image, T. Kulka-
rni & P. Kohli (2015) proposed a method to disentangle
speciﬁc factors by explicitly controlling target code. Perar-
nau et al. (2016) tackled image generation problems condi-
tioned on speciﬁc attribute vectors by training an attribute
predictor along with latent encoder.

In addition to using conditional information such as class
labels and text encodings, several works in the ﬁeld of
image-to-image translation used images of one domain to
generate images in another domain. (Isola et al., 2016)
translated black-and-white images to colored images by
training on paired black-and-white and colored image data.
Similarly, Taigman et al. (2016) translated face images to

emojis by providing image features from pre-trained face
recognition module as conditional input to a GAN.

Recently, Tong et al. (2017) tackled mode-collapsing and
instability problems in GAN training. They introduced two
ways of regularizing general GAN objective – geometric
metrics regularizer and mode regularizer.

5. Conclusion

This paper presents a learning method to discover cross-
domain relations with a generative adversarial network
called DiscoGAN. Our approach works without any ex-
plicit pair labels and learns to relate datasets from very dif-
ferent domains. We have demonstrated that DiscoGAN can
generate high-quality images with transferred style. One
possible future direction is to modify DiscoGAN to han-
dle mixed modalities (e.g. text and image).

References

Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein

GAN. In arXiv preprint arXiv:1701.07875, 2017.

Aubry, M., Maturana, D., Efros, A. A., Russell, B., and
Sivic, J. Seeing 3d chairs: Exemplar part-based 2d-3d
alignment using a large dataset of cad models. In Pro-
ceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2014.

Denton, E. L., Chintala, S., Szlam, A., and Fergus, R. Deep
generative image models using a laplacian pyramid of
adversarial networks. In Advances in Neural Information
Processing Systems (NIPS), 2015.

Dosovitskiy, A., Springenberg, J. T., and Brox, T. Learning
to generate chairs with convolutional neural networks.
In Proceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), 2015.

Fidler, S., Dickinson, S., and Urtasun, R. 3d object de-
tection and viewpoint estimation with a deformable 3d
cuboid model. In Advances in Neural Information Pro-
cessing Systems (NIPS), 2012.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
Warde-Farley, D., Ozair, S., Courville, A., and Bengio,
Y. Generative adversarial nets. In Advances in Neural
Information Processing Systems (NIPS), 2014.

Goodfellow, Ian J. NIPS 2016 tutorial: Generative adver-
In arXiv preprint arXiv:1701.00160,

sarial networks.
2017.

Ioffe, S. and Szegedy, C. Batch normalization: Accerlerat-
ing deep network training by reducing internal covariate
shift. In arXiv preprint arXiv:1502.03167, 2015.

Learning to Discover Cross-Domain Relations with Generative Adversarial Networks

Isola, P., Zhu, J., Zhou, T., and Efros, A. A.

Image-to-
image translation with conditional adversarial networks.
In arXiv preprint arXiv:1611.07004, 2016.

Kingma, D. P. and Ba, J. Adam: A method for stochastic
In Proceedings of the 3rd International
optimization.
Conference on Learning Representations (ICLR), 2015.

Liu, M. and Tuzel, O. Coupled generative adversarial net-
In Advances in Neural Information Processing

works.
Systems (NIPS), 2016.

Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning
face attributes in the wild. In Proceedings of the IEEE
International Conference on Computer Vision (ICCV),
2015.

Tong, C., Li, Y., Jacob, A. P., Bengio, Y., and Li, W. Mode
regularized generative adversarial networks. In Proceed-
ings of the 5rd International Conference on Learning
Representations (ICLR), 2017.

Wang, X. and Gupta, A. Generative image modeling using
In European

style and structure adversarial networks.
Conference on Computer Vision (ECCV), 2016.

Xu, B., Wang, N., T., Chen, and Li, M. Empirical evalua-
tion of rectiﬁed activations in convolutional network. In
arXiv preprint arXiv:1505:00853, 2015.

Yu, A. and Grauman, K. Fine-grained visual comparisons
In Computer Vision and Pattern

with local learning.
Recognition (CVPR), June 2014.

Maas, A. L., Hannun, A. Y., and Ng, A. Y. Rectiﬁer nonlin-
earities improve neural network acoustic models. In Pro-
ceedings of the 30th International Conference on Ma-
chine Learning (ICML), 2013.

Zhang, H., Xu, Tao., Li, H., Zhang, S., Huang, X., Wang,
X., and Metaxas, D. Stackgan: Text to photo-realistic
image synthesis with stacked generative adversarial net-
works. In arXiv preprint arXiv:1612.03242, 2016.

Zhu, Jun-Yan, Kr¨ahenb¨uhl, Philipp, Shechtman, Eli, and
Efros, Alexei A. Generative visual manipulation on the
In Proceedings of European
natural image manifold.
Conference on Computer Vision (ECCV), 2016.

Mirza, M. and Osindero, S. Conditional generative adver-
sarial nets. In arXiv preprint arXiv:1411.1784, 2014.

Paysan, P., Knothe, R., Amberg, B., Romdhani, S., and Vet-
ter, T. A 3d face model for pose and illumination invari-
In Proceedings of the 6th IEEE
ant face recognition.
International Conference on Advanced Video and Signal
based Surveillance (AVSS) for Security, Safety and Mon-
itoring in Smart Environments, 2009.

Perarnau, G., van de Weijer, J., Raducanu, B., and ´Alvarez,
J. M. Invertible conditional gans for image editing. In
arXiv preprint arXiv:1611.06355, 2016.

Radford, A., Metz, L., and Chintala, S. Unsupervised rep-
resentation learning with deep convolutional generative
adversarial networks. In Proceedings of the 4th Interna-
tional Conference on Learning Representations (ICLR),
2016.

Reed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B.,
and Lee, H. Generative adversarial text to image synthe-
sis. In Proceedings of the 33rd International Conference
on Machine Learning (ICML), 2016.

Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V.,
Radford, A., and Chen, X.
Improved techniques for
training gans. In Advances in Neural Information Pro-
cessing Systems (NIPS), 2016.

T. Kulkarni, W. Whitney and P. Kohli, J. Tenenbaum. Deep
convolutional inverse graphics network. In Advances in
Neural Information Processing Systems (NIPS), 2015.

Taigman, Y., Polyak, A., and Wolf, L. Unsupervised
In arXiv preprint

cross-domain image generation.
arXiv:1611.02200, 2016.

Learning to Discover Cross-Domain Relations
with Generative Adversarial Networks

Taeksoo Kim 1 Moonsu Cha 1 Hyunsoo Kim 1 Jung Kwon Lee 1 Jiwon Kim 1

7
1
0
2
 
y
a
M
 
5
1
 
 
]

V
C
.
s
c
[
 
 
2
v
2
9
1
5
0
.
3
0
7
1
:
v
i
X
r
a

Abstract
While humans easily recognize relations be-
tween data from different domains without any
supervision, learning to automatically discover
them is in general very challenging and needs
many ground-truth pairs that illustrate the re-
lations. To avoid costly pairing, we address
the task of discovering cross-domain relations
given unpaired data. We propose a method based
on generative adversarial networks that learns
to discover relations between different domains
(DiscoGAN). Using the discovered relations, our
proposed network successfully transfers style
from one domain to another while preserving key
attributes such as orientation and face identity.

1. Introduction

Relations between two different domains, the way in which
concepts, objects, or people are connected, arise ubiqui-
tously. Cross-domain relations are often natural to humans.
For example, we recognize the relationship between an En-
glish sentence and its translated sentence in French. We
also choose a suit jacket with pants or shoes in the same
style to wear.

Can machines also achieve a similar ability to relate two
different image domains? This question can be reformu-
lated as a conditional image generation problem. In other
words, ﬁnding a mapping function from one domain to the
other can be thought as generating an image in one do-
main given another image in the other domain. While this
problem tackled by generative adversarial networks (GAN)
(Isola et al., 2016) has gained a huge attention recently,
most of today’s training approaches use explicitly paired
data, provided by human or another algorithm.

This problem also brings an interesting challenge from a
learning point of view. Explicitly supervised data is sel-
dom available and labeling can be labor intensive. More-

1SK T-Brain, Seoul, South Korea. Correspondence to: Taeksoo

Kim <jazzsaxmaﬁa@sktbrain.com>.

(a) Learning cross-domain relations without any extra label

(b) Handbag images (input) & Generated shoe images (output)

(c) Shoe images (input) & Generated handbag images (output)

Figure 1. Our GAN-based model trains with two independently
collected sets of images and learns how to map two domains with-
out any extra label. In this paper, we reduces this problem into
generating a new image of one domain given an image from the
other domain. (a) shows a high-level overview of the training pro-
cedure of our model with two independent sets (e.g. handbag im-
ages and shoe images). (b) and (c) show results of our method.
Our method takes a handbag (or shoe) image as an input, and
generates its corresponding shoe (or handbag) image. Again, it’s
worth noting that our method does not take any extra annotated
supervision and can self-discover relations between domains.

over, pairing images can become tricky if corresponding
images are missing in one domain or there are multiple best
candidates. Hence, we push one step further by discovering
relations between two visual domains without any explic-
itly paired data.

Learning to Discover Cross-Domain Relations with Generative Adversarial Networks

Figure 2. Three investigated models. (a) standard GAN (Goodfellow et al., 2014), (b) GAN with a reconstruction loss, (c) our proposed
model (DiscoGAN) designed to discover relations between two unpaired, unlabeled datasets. Details are described in Section 3.

In order to tackle this challenge, we introduce a model that
discovers cross-domain relations with GANs (DiscoGAN).
Unlike previous methods, our model can be trained with
two sets of images without any explicit pair labels (see Fig-
ure 1a) and does not require any pre-training. Our proposed
model can then take one image in one domain as an input
and generate its corresponding image in another domain
(see Figure 1b). The core of our model is based on two
different GANs coupled together – each of them ensures
our generative functions can map each domain to its coun-
terpart domain. A key intuition we rely on is to constraint
all images in one domain to be representable by images in
the other domain. For example, when learning to generate
a shoe image based on each handbag image, we force this
generated image to be an image-based representation of the
handbag image (and hence reconstruct the handbag image)
through a reconstruction loss, and to be as close to images
in the shoe domain as possible through a GAN loss. We
use these two properties to encourage the mapping between
two domains to be well covered on both directions (i.e. en-
couraging one-to-one rather than many-to-one or one-to-
many). In the experimental section, we show that this sim-
ple intuition discovered common properties and styles of
two domains very well.

Both experiments on toy domain and real world image
datasets support the claim that our proposed model is well-
suited for discovering cross-domain relations. When trans-
lating data points between simple 2-dimensional domains
and between face image domains, our DiscoGAN model
was more robust to the mode collapse problem compared
to two other baseline models. It also learns the bidirectional

mapping between two image domains, such as faces, cars,
chairs, edges and photos, and successfully apply them in
image translation. Translated images consistently change
speciﬁed attributes such as hair color, gender and orienta-
tion while maintaining all other components. Results also
show that our model is robust to repeated application of
translation mappings.

2. Model

We now formally deﬁne cross-domain relations and present
the problem of learning to discover such relations in two
different domains. Standard GAN model and a similar vari-
ant model with additional components are investigated for
their applicability for this task. Limitations of these mod-
els are then explained, and we propose a new architecture
based on GANs that can be used to discover cross-domain
relations.

2.1. Formulation

Relation is mathematically deﬁned as a function GAB
that maps elements from its domain A to elements in its
codomain B and GBA is similarly deﬁned. In fully unsu-
pervised setting, GAB and GBA can be arbitrarily deﬁned.
To ﬁnd a meaningful relation, we need to impose a con-
dition on the relation of interest. Here, we constrain rela-
tion to be a one-to-one correspondence (bijective mapping).
That means GAB is the inverse mapping of GBA.

The range of function GAB, the complete set of all possible
resulting values GAB(xA) for all xA’s in domain A, should
be contained in domain B and similarly for GBA(xB).

Learning to Discover Cross-Domain Relations with Generative Adversarial Networks

Figure 3. Illustration of our models on simpliﬁed one dimensional domains. (a) ideal mapping from domain A to domain B in which the
two domain A modes map to two different domain B modes, (b) GAN model failure case, (c) GAN with reconstruction model failure
case.

We now relate these constraints to objective functions. Ide-
ally, the equality GBA ◦ GAB(xA) = xA should be sat-
isﬁed, but this hard constraint is difﬁcult to optimize and
relaxed soft constraint is more desirable in the view of
optimization. For this reason, we minimize the distance
d(GBA ◦ GAB(xA), xA), where any form of metric func-
tion (L1, L2, Huber loss) can be used. Similarly, we also
need to minimize d(GAB ◦ GBA(xB), xB).

Guaranteeing that GAB maps to domain B is also very
to optimize. We relax this constraint as fol-
difﬁcult
lows: we instead minimize generative adversarial
loss
−ExA ∼PA
[log DB(GAB(xA))]. Similarly, we minimize
−ExB ∼PB
[log DA(GBA(xB ))].
Now, we explore several GAN architectures to learn with
these loss functions.

2.2. Notation and Architecture

We use the following notations in sections below. A genera-
tor network is denoted GAB : R64×64×3
, and
the subscripts denote the input and output domains and su-
perscripts denote the input and output image size. The dis-
criminator network is denoted as DB : R64×64×3
→ [0, 1],
and the subscript B denotes that it discriminates images in
domain B. Notations GBA and DA are used similarly.

→ R64×64×3
B

B

A

Each generator takes image of size 64 × 64 × 3 and feeds it
through an encoder-decoder pair. The encoder part of each
generator is composed of convolution layers with 4 × 4 ﬁl-
ters, each followed by leaky ReLU (Maas et al., 2013; Xu
et al., 2015). The decoder part is composed of deconvolu-
tion layers with 4 × 4 ﬁlters, followed by a ReLU, and out-
puts a target domain image of size 64 × 64 × 3. The number
of convolution and deconvolution layers ranges from four
to ﬁve, depending on the domain.

with 4 × 4 ﬁlters, and a ﬁnal sigmoid to output a scalar
output between [0, 1].

2.3. GAN with a Reconstruction Loss

We ﬁrst consider a standard GAN model (Goodfellow
et al., 2014) for the relation discovery task (Figure 2a).
Originally, a standard GAN takes random Gaussian noise
z, encodes it into hidden features h and generates images
such as MNIST digits. We make a slight modiﬁcation to
this model to ﬁt our task: the model we use takes in image
as input instead of noise.

In addition, since this architecture only learns one mapping
from domain A to domain B, we add a second generator
that maps domain B back into domain A (Figure 2b). We
also add a reconstruction loss term that compares the input
image with the reconstructed image. With these additional
changes, each generator in the model can learn mapping
from its input domain to output domain and discover rela-
tions between them.

A generator GAB translates input image xA from domain A
into xAB in domain B. The generated image is then trans-
lated into a domain A image xABA to match the original in-
put image (Equation 1, 2). Various forms of distance func-
tions, such as MSE, cosine distance, and hinge-loss, can be
used as the reconstruction loss d (Equation 3). The trans-
lated output xAB is then scored by the discriminator which
compares it to a real domain B sample xB .

xAB = GAB(xA)
xABA = GBA(xAB ) = GBA ◦ GAB(xA )
= d(GBA ◦ GAB(xA ), xA )
= −ExA ∼PA

[log DB(GAB(xA))]

LCON STA
LGANB

(1)

(2)

(3)

(4)

The discriminator is similar to the encoder part of the gen-
erator. In addition to the convolution layers and leaky Re-
LUs, the discriminator has an additional convolution layer

The generator GAB receives two types of losses – a re-
construction loss LCON STA
(Equation 3) that measures how
well the original input is reconstructed after a sequence of

Learning to Discover Cross-Domain Relations with Generative Adversarial Networks

two generations, and a standard GAN generator loss LGANB
(Equation 4) that measures how realistic the generated im-
age is in domain B. The discriminator receives the standard
GAN discriminator loss of Equation 6.

One key difference from the previous model is that input
images from both domains are reconstructed and that there
are two reconstruction losses: LCON STA and LCON STB .

LGAB

= LGANB

+ LCON STA

LG = LGAB
= LGANB

+ LGBA
+ LCON STA

+ LGANA

+ LCON STB

LDB

= − ExB ∼PB
− ExA ∼PA

[log DB(xB )]
[log(1 − DB(GAB(xA )))]

LD = LDA

+ LDB

(5)

(6)

(7)

(8)

During training, the generator GAB learns the mapping
from domain A to domain B under two relaxed constraints:
that domain A maps to domain B, and that the mapping
on domain B is reconstructed to domain A. However, this
model lacks a constraint on mapping from B to A, and these
two conditions alone does not guarantee a cross-domain re-
lation (as deﬁned in section 2.1) because the mapping sat-
isfying these constraints is one-directional. In other words,
the mapping is an injection, not bijection, and one-to-one
correspondence is not guaranteed.

Consider the two possibly multi-modal image domains A
and B. Figure 3 illustrates the two multi-modal data do-
mains on a simpliﬁed one-dimensional representation. Fig-
ure 3a shows the ideal mapping from input domain A to do-
main B, where each mode of data is mapped to a separate
mode in the target domain. Figure 3b, in contrast, shows the
mode collapse problem, a prevalent phenomenon in GANs,
where data from multiple modes of a domain map to a sin-
gle mode of a different domain. For instance, this case is
where the mapping GAB maps images of cars in two dif-
ferent orientations into the same mode of face images.

In some sense, the addition of a reconstruction loss to a
standard GAN is an attempt to remedy the mode collapse
problem. In Figure 3c, two domain A modes are matched
with the same domain B mode, but the domain B mode can
only direct to one of the two domain A modes. Although
the additional reconstruction loss LCON STA
forces the re-
constructed sample to match the original (Figure 3c), this
change only leads to a similar symmetric problem. The re-
construction loss leads to an oscillation between the two
states and does not resolve mode-collapsing.

2.4. Our Proposed Model: Discovery GAN

Our proposed GAN model for relation discovery – Disco-
GAN – couples the previously proposed model (Figure 2c).
Each of the two coupled models learns the mapping from
one domain to another, and also the reverse mapping to for
reconstruction. The two models are trained together simul-
taneously. The two generators GAB’s and the two gener-
ators GBA’s share parameters, and the generated images
xBA and xAB are each fed into separate discriminators LDA
and LDB , respectively.

As a result of coupling two models, the total generator loss
is the sum of GAN loss and reconstruction loss for each
partial model (Equation 7). Similarly, the total discrimina-
tor loss LD is a sum of discriminator loss for the two dis-
criminators DA and DB, which discriminate real and fake
images of domain A and domain B (Equation 8).

Now, this model is constrained by two LGAN losses and
two LCON ST losses. Therefore a bijective mapping is
achieved, and a one-to-one correspondence, which we de-
ﬁned as cross-domain relation, can be discovered.

3. Experiments

3.1. Toy Experiment

To empirically demonstrate our explanations on the differ-
ences between a standard GAN, a GAN with reconstruc-
tion loss and our proposed model (DiscoGAN), we de-
signed an illustrative experiment based on synthetic data
in 2-dimensional A and B domains. Both source and target
data samples are drawn from Gaussian mixture models.

In Figure 4, the left-most ﬁgure shows the initial state of toy
experiment where all the A domain modes map to almost
a single point because of initialization of the generator. For
all other plots the target domain 2D plane is shown with tar-
get domain modes marked with black ‘x’s. Colored points
on B domain planes represent samples from A domain that
are mapped to the B domain, and each color denotes sam-
ples from each A domain mode. In this case, the task is to
discover cross-domain relations between the A and B do-
main and translate samples from ﬁve A domain modes into
the B domain, which has ten modes spread around the arc
of a circle.

We use a neural network with three linear layers that are
each followed by a ReLU nonlinearity as the generator. For
the discriminator we use ﬁve linear layers that are each
followed by a ReLU, except for the last layer which is
switched out with a sigmoid that outputs a scalar ∈ [0, 1].
The colored background shows the output value of the
discriminator DB, which discriminates real target domain
samples from synthetic, translated samples from domain
A. The contour lines show regions of same discriminator
value.

Learning to Discover Cross-Domain Relations with Generative Adversarial Networks

(a)

(b)

(c)

(d)

Figure 4. Toy domain experiment results. The colored background shows the output value of the discriminator. ’x’ marks denote different
modes in B domain, and colored circles indicate mapped samples of domain A to domain B, where each color corresponds to a different
mode. (a) ten target domain modes and initial translations, (b) standard GAN model, (c) GAN with reconstruction loss, (d) our proposed
model DiscoGAN

The training was performed for 50,000 iterations, and due
to the domain simplicity our model often converged much
earlier. The results from this experiment match our claim
and illustrations in Figure 4 and the resulting translated
samples show very different behavior depending on the
model used.

In the baseline (standard GAN) case, many translated
points of different colors are located around the same B
domain mode. For example, navy and light-blue colored
points are located together, as well as green and orange col-
ored points. This result illustrates the mode-collapse prob-
lem of GANs since points of multiple colors (multiple A
domain modes) are mapped to the same B domain mode.
The baseline model still oscillate around B modes through-
out the iterations.

In the case of GAN with a reconstruction loss, the collaps-
ing problem is less prevalent, but navy, green and light-blue
points still overlap at a few modes. The contour plot also
demonstrates the difference from baseline: regions around
all B modes are leveled in a green colored plateau in the
baseline, allowing translated samples to freely move be-
tween modes, whereas in the single model case the regions
between B modes are clearly separated.

In addition, both this model and the standard GAN model
fail to cover all modes in B domain since the mapping from
A domain to B domain is injective. Our proposed Disco-
GAN model, on the other hand, is able to not only prevent
mode-collapse by translating into distinct well-bounded re-
gions that do not overlap, but also generate B samples in all
ten modes as the mappings in our model is bijective. It is
noticeable that the discriminator for B domain is perfectly
fooled by translated samples from A domain around B do-
main modes.

Although this experiment is limited due to its simplicity,
the results clearly support the superiority of our proposed
model over other variants of GANs.

3.2. Real Domain Experiment

To evaluate whether our DiscoGAN successfully learns
underlying relationship between domains, we trained and
tested our model using several image-to-image translation
tasks that require the use of discovered cross-domain rela-
tions between source and target domains.

In each real domain experiment, all input images and trans-
lated images were of size 64 × 64 × 3. For training, we
used learning rate of 0.0002 and used the Adam optimizer
(Kingma & Ba, 2015) with β1 = 0.5 and β2 = 0.999.
We applied Batch Normalization (Ioffe & Szegedy, 2015)
to all convolution and deconvolution layers except the ﬁrst
and the last layers, weight decay regularization coefﬁcient
of 10−4 and minibatch of size 200. All computations were
conducted on a single machine with an Nvidia Titan X Pas-
cal GPU and an Intel(R) Xeon(R) E5-1620 CPU.

3.2.1. CAR TO CAR, FACE TO FACE

We used a Car dataset (Fidler et al., 2012) which consists
of rendered images of 3D car models with varying azimuth
angles at 15◦ intervals. We split the dataset into train and
test sets and again split the train set into two groups, each
of which is used as A domain and B domain samples. In
addition to training a standard GAN model, a GAN with
a reconstruction model and a proposed DiscoGAN model,
we also trained a regressor that predicts the azimuth angle
of a car image using the train set. To evaluate, we translated
images in the test set using each of the three trained mod-
els, and azimuth angles were predicted using the regressor
for both input and translated images. Figure 5 shows the
predicted azimuth angles of input and translated images for
each model. In standard GAN and GAN with reconstruc-
tion (5a and 5b), most of the red dots are grouped in a few
clusters, indicating that most of the input images are trans-
lated into images with same azimuth, and that these mod-
els suffer from mode collapsing problem as predicted and

Learning to Discover Cross-Domain Relations with Generative Adversarial Networks

Figure 5. Car to Car translation experiment. Horizontal and vertical axes in the plots indicate predicted azimuth angles of input and
translated images, where the angle of input image ranges from -75◦ to 75◦. RMSE with respect to ground truth (blue lines) are shown
in each plot. Images in the second row are examples of input car images ranging from -75◦ to 75◦ at 15◦ intervals. Images in the third
row are corresponding translated images. (a) plot of standard GAN (b) GAN with reconstruction (c) DiscoGAN. The angles of input and
output images are highly correlated when our proposed DiscoGAN model is used. Note the angles of input and translated car images are
reversed with respect to 0◦ (i.e. mirror images).

In Figure 7a, we can see that various facial features are
well-preserved while a single desired attribute (gender) is
changed. Also, 7b and 7d shows that background is also
well-preserved and images are visually natural, although
the background does change in a few cases such as Figure
7c. An extension to this experiment was sequentially apply-
ing several translations – for example, changing the gender
and then the hair color (7e), or repeatedly applying gender
transforms (7f).

shown in Figures 3 and 4. Our proposed DiscoGAN (5c),
on the other hand, shows strong correlation between pre-
dicted angles of input and translated images, indicating that
our model successfully discovers azimuth relation between
the two domains. In this experiment, the translated images
either have the same azimuth range (5b), or the opposite
(5a and 5c) of the input images.

Next, we use a Face dataset (Paysan et al., 2009) shown in
Figure 6a, in which the data images vary in azimuth rota-
tion from -90◦ to +90◦. Similar to previous car to car exper-
iment, input images in the -90◦ to +90◦ rotation range gen-
erated output images either in the same range, from -90◦ to
+90◦, or the opposite range, from +90◦ to -90◦ when our
proposed model was used (6d). We also trained a standard
GAN and a GAN with reconstruction loss for comparison.
When a standard GAN and GAN with reconstruction loss
were used, the generated images do not vary as much as the
input images in terms of rotation. In this sense, similar to
what has been shown in previous Car to Car experiment,
the two models suffered from mode collapse.

3.2.2. FACE CONVERSION

In terms of the amount of related information between two
domains, we can consider a few extreme cases: two do-
mains sharing almost all features and two domains sharing
only one feature. To investigate former case, we applied the
face attribute conversion task on CelebA dataset (Liu et al.,
2015), where only one feature, such as gender or hair color,
varies between two domains and all the other facial features
are shared. The results are listed in Figure 7.

Figure 6. Face to Face translation experiment. (a) input face im-
ages from -90◦ to +90◦ (b) results from a standard GAN (c) results
from GAN with a reconstruction loss (d) results from our Disco-
GAN. Here our model generated images in the opposite range,
from +90◦ to -90◦.

Learning to Discover Cross-Domain Relations with Generative Adversarial Networks

Figure 7. (a,b) Translation of gender in Facescrub dataset and CelebA dataset. (c) Blond to black and black to blond hair color conversion
in CelebA dataset. (d) Wearing eyeglasses conversion in CelebA dataset (e) Results of applying a sequence of conversion of gender and
hair color (left to right) (f) Results of repeatedly applying the same conversions (upper: hair color, lower: gender)

Learning to Discover Cross-Domain Relations with Generative Adversarial Networks

Figure 8. Discovering relations of images from visually very different object classes. (a) chair to car translation. DiscoGAN is trained
on chair and car images (b) car to face translation. DiscoGAN is trained on car and face images. Our model successfully pairs images
with similar orientation.

feature of the input images while preserving visual features
of car and face domain, respectively.

3.2.4. EDGES-TO-PHOTOS

Edges-to-photos is an interesting task as it is a 1-to-N prob-
lem, where a single edge image of items such as shoes and
handbags can generate multiple colorized images of such
items. In fact, an edge image can be colored in inﬁnitely
many ways. We validated that our DiscoGAN performs
very well on this type of image-to-image translation task
and generate realistic photos of handbags (Zhu et al., 2016)
and shoes (Yu & Grauman, 2014). The generated images
are presented in Figure 9.

3.2.5. HANDBAG TO SHOES, SHOES TO HANDBAG

Finally, we investigated the case with two domains that are
visually very different, where shared features are not ex-
plicit even to humans. We trained a DiscoGAN using pre-
viously used handbags and shoes datasets, not assuming
any speciﬁc relation between those two. In the translation
results shown in Figure 1, our proposed model discovers
fashion style as a related feature between the two domains.
Note that translated results not only have similar colors and
patterns, but they also have similar level of fashion formal-
ity as the input fashion item.

4. Related Work

Recently, a novel method to train generative models named
Generative Adversarial Networks (GANs) (Goodfellow
et al., 2014) was developed. A GAN is composed of two
modules – a generator G and a discriminator D. The gen-
erator’s objective is to generate (synthesize) data samples
whose distribution closely matches that of real data sam-

Figure 9. Edges to photos experiment. Our model is trained on a
set of object sketches and colored images and learns to generate
new sketches or photos. (a) colored images of handbags are gener-
ated from sketches of handbags, (b) colored images of shoes are
generated from sketches of shoes, (c) sketches of handbags are
generated from colored images of handbags

3.2.3. CHAIR TO CAR, CAR TO FACE

We also investigated the opposite case where there is a sin-
gle shared feature between two domains. 3D rendered im-
ages of chair (Aubry et al., 2014) and the previously used
car and face datasets (Fidler et al., 2012; Paysan et al.,
2009) were used in this task. All three datasets vary along
the azimuth rotation. Figure 8 shows the results of image-
to-image translation from chair to car and from car to face
datasets. The translated images clearly match the rotation

Learning to Discover Cross-Domain Relations with Generative Adversarial Networks

ples while the discriminator’s objective is to distinguish
real ones from generated samples. The two models G and
D, formulated as a two-player minimax game, are trained
simultaneously.

Researchers have studied GANs vigorously in two years:
network models such as LAPGAN (Denton et al., 2015)
and DCGAN (Radford et al., 2016) and improved training
techniques (Salimans et al., 2016; Arjovsky et al., 2017).
More recent GAN works are described in (Goodfellow,
2017).

Several methods were developed to generate images
based on GANs. Conditional Generative Adversarial Nets
(cGANs) (Mirza & Osindero, 2014) use MNIST digit class
label as an additional information to both generator and
discriminator and can generate digit images of the speci-
ﬁed class. Similarly, Dosovitskiy et al. (2015) showed that
GAN can generate images of objects based on speciﬁed
characteristic codes such as color and viewpoint. Other ap-
proaches used conditional features from a completely dif-
ferent domain for image generation. For example, Reed
et al. (2016) used encoded text description of images as the
conditional information to generating images that match
the description.

Some researchers have attempted to use multiple GANs in
prior works. (Liu & Tuzel, 2016) proposed to couple two
GANs (coupled generative adversarial networks, CoGAN)
in which two generators and two discriminators are cou-
pled by weight-sharing to learn the joint distribution of im-
ages in two different domains without using pair-wise data.
In Stacked GANs (StackGAN) (Zhang et al., 2016), two
GANs are arranged sequentially where the Stage-I GAN
generates low resolution images given text description and
the Stage-II GAN improves the generated image into high
resolution images. Similarly, Style and Structure GAN (S2-
GAN) (Wang & Gupta, 2016) used two sequentially con-
nected GANs where the Structure GAN ﬁrst generates sur-
face normal image and the Style GAN transforms it into
natural indoor scene image.

In order to control speciﬁc attributes of an image, T. Kulka-
rni & P. Kohli (2015) proposed a method to disentangle
speciﬁc factors by explicitly controlling target code. Perar-
nau et al. (2016) tackled image generation problems condi-
tioned on speciﬁc attribute vectors by training an attribute
predictor along with latent encoder.

In addition to using conditional information such as class
labels and text encodings, several works in the ﬁeld of
image-to-image translation used images of one domain to
generate images in another domain. (Isola et al., 2016)
translated black-and-white images to colored images by
training on paired black-and-white and colored image data.
Similarly, Taigman et al. (2016) translated face images to

emojis by providing image features from pre-trained face
recognition module as conditional input to a GAN.

Recently, Tong et al. (2017) tackled mode-collapsing and
instability problems in GAN training. They introduced two
ways of regularizing general GAN objective – geometric
metrics regularizer and mode regularizer.

5. Conclusion

This paper presents a learning method to discover cross-
domain relations with a generative adversarial network
called DiscoGAN. Our approach works without any ex-
plicit pair labels and learns to relate datasets from very dif-
ferent domains. We have demonstrated that DiscoGAN can
generate high-quality images with transferred style. One
possible future direction is to modify DiscoGAN to han-
dle mixed modalities (e.g. text and image).

References

Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein

GAN. In arXiv preprint arXiv:1701.07875, 2017.

Aubry, M., Maturana, D., Efros, A. A., Russell, B., and
Sivic, J. Seeing 3d chairs: Exemplar part-based 2d-3d
alignment using a large dataset of cad models. In Pro-
ceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2014.

Denton, E. L., Chintala, S., Szlam, A., and Fergus, R. Deep
generative image models using a laplacian pyramid of
adversarial networks. In Advances in Neural Information
Processing Systems (NIPS), 2015.

Dosovitskiy, A., Springenberg, J. T., and Brox, T. Learning
to generate chairs with convolutional neural networks.
In Proceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), 2015.

Fidler, S., Dickinson, S., and Urtasun, R. 3d object de-
tection and viewpoint estimation with a deformable 3d
cuboid model. In Advances in Neural Information Pro-
cessing Systems (NIPS), 2012.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
Warde-Farley, D., Ozair, S., Courville, A., and Bengio,
Y. Generative adversarial nets. In Advances in Neural
Information Processing Systems (NIPS), 2014.

Goodfellow, Ian J. NIPS 2016 tutorial: Generative adver-
In arXiv preprint arXiv:1701.00160,

sarial networks.
2017.

Ioffe, S. and Szegedy, C. Batch normalization: Accerlerat-
ing deep network training by reducing internal covariate
shift. In arXiv preprint arXiv:1502.03167, 2015.

Learning to Discover Cross-Domain Relations with Generative Adversarial Networks

Isola, P., Zhu, J., Zhou, T., and Efros, A. A.

Image-to-
image translation with conditional adversarial networks.
In arXiv preprint arXiv:1611.07004, 2016.

Kingma, D. P. and Ba, J. Adam: A method for stochastic
In Proceedings of the 3rd International
optimization.
Conference on Learning Representations (ICLR), 2015.

Liu, M. and Tuzel, O. Coupled generative adversarial net-
In Advances in Neural Information Processing

works.
Systems (NIPS), 2016.

Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning
face attributes in the wild. In Proceedings of the IEEE
International Conference on Computer Vision (ICCV),
2015.

Tong, C., Li, Y., Jacob, A. P., Bengio, Y., and Li, W. Mode
regularized generative adversarial networks. In Proceed-
ings of the 5rd International Conference on Learning
Representations (ICLR), 2017.

Wang, X. and Gupta, A. Generative image modeling using
In European

style and structure adversarial networks.
Conference on Computer Vision (ECCV), 2016.

Xu, B., Wang, N., T., Chen, and Li, M. Empirical evalua-
tion of rectiﬁed activations in convolutional network. In
arXiv preprint arXiv:1505:00853, 2015.

Yu, A. and Grauman, K. Fine-grained visual comparisons
In Computer Vision and Pattern

with local learning.
Recognition (CVPR), June 2014.

Maas, A. L., Hannun, A. Y., and Ng, A. Y. Rectiﬁer nonlin-
earities improve neural network acoustic models. In Pro-
ceedings of the 30th International Conference on Ma-
chine Learning (ICML), 2013.

Zhang, H., Xu, Tao., Li, H., Zhang, S., Huang, X., Wang,
X., and Metaxas, D. Stackgan: Text to photo-realistic
image synthesis with stacked generative adversarial net-
works. In arXiv preprint arXiv:1612.03242, 2016.

Zhu, Jun-Yan, Kr¨ahenb¨uhl, Philipp, Shechtman, Eli, and
Efros, Alexei A. Generative visual manipulation on the
In Proceedings of European
natural image manifold.
Conference on Computer Vision (ECCV), 2016.

Mirza, M. and Osindero, S. Conditional generative adver-
sarial nets. In arXiv preprint arXiv:1411.1784, 2014.

Paysan, P., Knothe, R., Amberg, B., Romdhani, S., and Vet-
ter, T. A 3d face model for pose and illumination invari-
In Proceedings of the 6th IEEE
ant face recognition.
International Conference on Advanced Video and Signal
based Surveillance (AVSS) for Security, Safety and Mon-
itoring in Smart Environments, 2009.

Perarnau, G., van de Weijer, J., Raducanu, B., and ´Alvarez,
J. M. Invertible conditional gans for image editing. In
arXiv preprint arXiv:1611.06355, 2016.

Radford, A., Metz, L., and Chintala, S. Unsupervised rep-
resentation learning with deep convolutional generative
adversarial networks. In Proceedings of the 4th Interna-
tional Conference on Learning Representations (ICLR),
2016.

Reed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B.,
and Lee, H. Generative adversarial text to image synthe-
sis. In Proceedings of the 33rd International Conference
on Machine Learning (ICML), 2016.

Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V.,
Radford, A., and Chen, X.
Improved techniques for
training gans. In Advances in Neural Information Pro-
cessing Systems (NIPS), 2016.

T. Kulkarni, W. Whitney and P. Kohli, J. Tenenbaum. Deep
convolutional inverse graphics network. In Advances in
Neural Information Processing Systems (NIPS), 2015.

Taigman, Y., Polyak, A., and Wolf, L. Unsupervised
In arXiv preprint

cross-domain image generation.
arXiv:1611.02200, 2016.

