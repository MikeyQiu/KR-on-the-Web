AlignedReID: Surpassing Human-Level Performance in Person Re-Identiﬁcation

Xuan Zhang1∗, Hao Luo1,2∗†, Xing Fan1,2†, Weilai Xiang1, Yixiao Sun1, Qiqi Xiao1,
Wei Jiang2, Chi Zhang1, Jian Sun1
1Megvii, Inc. (Face++)
2Institute of Cyber-Systems and Control, Zhejiang University
{zhangxuan,xiangweilai,sunyixiao,xqq,zhangchi,sunjian}@megvii.com
{haoluocsc,xfanplus,jiangwei zju}@zju.edu.cn

8
1
0
2
 
n
a
J
 
1
3
 
 
]

V
C
.
s
c
[
 
 
2
v
4
8
1
8
0
.
1
1
7
1
:
v
i
X
r
a

Abstract

In this paper, we propose a novel method called Aligne-
dReID that extracts a global feature which is jointly learned
with local features. Global feature learning beneﬁts greatly
from local
feature learning, which performs an align-
ment/matching by calculating the shortest path between two
sets of local features, without requiring extra supervision.
After the joint learning, we only keep the global feature
to compute the similarities between images. Our method
achieves rank-1 accuracy of 94.4% on Market1501 and
97.8% on CUHK03, outperforming state-of-the-art meth-
ods by a large margin. We also evaluate human-level
performance and demonstrate that our method is the ﬁrst
to surpass human-level performance on Market1501 and
CUHK03, two widely used Person ReID datasets.

1. Introduction

Person re-identiﬁcation (ReID), identifying a person of
interest at other time or place, is a challenging task in com-
puter vision.
Its applications range from tracking people
across cameras to searching for them in a large gallery, from
grouping photos in a photo album to visitor analysis in a
retail store. Like many visual recognition problems, varia-
tions in pose, viewpoints illumination, and occlusion make
this problem non-trivial.

Traditional approaches have focused on low-level fea-
tures such as colors, shapes, and local descriptors [9, 11].
With the renaissance of deep learning, the convolutional
neural network (CNN) has dominated this ﬁeld [24, 32, 6,
54, 16, 24], by learning features in an end-to-end fashion
through various metric learning losses such as contrastive
loss [32], triplet loss [18], improved triplet loss [6], quadru-

∗Equal contribution
†The work was done when Hao and Xing were interns at MegVii, Inc.

(Face++)

Figure 1. Challenges in ReID: (a-b) inaccurate detection, (c-d)
pose misalignments, (e-f) occlusions, (g-h) very similar appear-
ance.

plet loss [3], and triplet hard loss [13].

Many CNN-based approaches learn a global feature,
without considering the spatial structure of the person. This
has a few major drawbacks: 1) inaccurate person detection
boxes might impact feature learning, e.g., Figure 1 (a-b); 2)
the pose change or non-rigid body deformation makes the
metric learning difﬁcult, e.g., Figure 1 (c-d); 3) occluded
parts of the human body might introduce irrelevant context
into the learned feature, e.g., Figure 1 (e-f); 4) it is non-
trivial to emphasis local differences in a global feature, es-
pecially when we have to distinguish two people with very
similar appearances, e.g., Figure 1 (g-h). To explicitly over-
come these drawbacks, recent studies have paid attention to
part-based, local feature learning. Some works [33, 38, 43]
divide the whole body into a few ﬁxed parts, without consid-
ering the alignment between parts. However, it still suffers
from inaccurate detection box, pose variation, and occlu-
sion. Other works use pose estimation result for the align-
ment [52, 37, 50], which requires additional supervision and
a pose estimation step (which is often error-prone).

In this paper, we propose a new approach, called Aligne-
dReID, which still learns a global feature, but performs an

1

automatic part alignment during the learning, without re-
quiring extra supervision or explicit pose estimation. In the
learning stage, we have two branches for learning a global
feature and local features jointly. In the local branch, we
align local parts by introducing a shortest path loss. In the
inference stage, we discard the local branch and only extract
the global feature. We ﬁnd that only applying the global
feature is almost as good as combining global and local fea-
tures. In other words, the global feature itself, with the aid
of local features learning, can greatly address the drawbacks
we mentioned above, in our new joint learning framework.
In addition, the form of global feature keeps our approach
attractive for the deployment of a large ReID system, with-
out costly local features matching.

We also adopt a mutual learning approach [49] in the
metric learning setting, to allow two models to learn better
representations from each other. Combining AlignedReID
and mutual learning, our system outperforms state-of-the-
art systems on Market1501, CUHK03, and CUHK-SYSU
by a large margin. To understand how well human per-
form in the ReID task, we measure the best human perfor-
mance of ten professional annotators on Market1501 and
CUHK03. We ﬁnd that our system with re-ranking [57] has
a higher level of accuracy than the human. To the best of
our knowledge, this is the ﬁrst report in which machine per-
formance exceeds human performance on the ReID task.

2. Related Work

Metric Learning. Deep metric learning methods transform
raw images into embedding features, then compute the fea-
ture distances as their similarities. Usually, two images of
the same person are deﬁned as a positive pair, whereas two
images of different persons are a negative pair. Triplet loss
[18] is motivated by the margin enforced between positive
and negative pairs. Selecting suitable samples for the train-
ing model through hard mining has been shown to be ef-
fective [13, 3, 39]. Combining softmax loss with metric
learning loss to speed up the convergence is also a popular
method [10].
Feature Alignments. Many works learn a global feature
to represent an image of a person, ignoring the spatial local
information of images. Some works consider local informa-
tion by dividing images into several parts without an align-
ment [33, 38, 43], but these methods suffer from inaccurate
detection boxes, occlusion and pose misalignment.

Recently, aligning local features by pose estimation has
become a popular approach. For instance, pose invariant
embedding (PIE) aligns pedestrians to a standard pose to
reduce the impact of pose [52] variation. A Global-Local-
Alignment Descriptor (GLAD) [37] does not directly align
pedestrians, but rather detects key pose points and extracts
local features from corresponding regions. SpindleNet [50]
uses a region proposed network (RPN) to generate several

body regions, gradually combining the response maps from
adjacent body regions at different stages. These methods re-
quire extra pose annotation and have to deal with the errors
introduced by pose estimation.
Mutual Learning.
[49] presents a deep mutual learning
strategy where an ensemble of students learn collabora-
tively and teach each other throughout the training process.
DarkRank [4] introduces a new type of knowledge-cross
sample similarity for model compression and acceleration,
achieving state-of-the-art performance. These methods use
mutual learning in classiﬁcation.
In this work, we study
mutual learning in the metric learning setting.
Re-Ranking. After obtaining the image features, most cur-
rent works choose the L2 Euclidean distance to compute a
similarity score for a ranking or retrieval task. [35, 57, 1]
perform an additional re-ranking to improve ReID accuracy.
In particular, [57] proposes a re-ranking method with k-
reciprocal encoding, which combines the original distance
and Jaccard distance.

In this section, we present our AlignedReID framework,

3. Our Approach

as shown in Figure 1.

3.1. AlignedReID

In AlignedReID, we generate a single global feature as
the ﬁnal output of the input image, and use the L2 dis-
tance as the similarity metric. However, the global feature
is learned jointly with local features in the learning stage.

For each image, we use a CNN, such as Resnet50 [12],
to extract a feature map, which is the output of the last con-
volution layer (C × H × W , where C is the channel number
and H × W is the spatial size, e.g., 2048 × 7 × 7 in Figure
1). A global feature (a C-d vector) is extracted by directly
applying global pooling on the feature map. For the local
features, a horizontal pooling, which is a global pooling in
the horizontal direction, is ﬁrst applied to extract a local fea-
ture for each row, and a 1 × 1 convolution is then applied
to reduce the channel number from C to c.
In this way,
each local feature (a c-d vector) represents a horizontal part
of the image for a person. As a result, a person image is
represented by a global feature and H local features.

The distance of two person images is the summation of
their global and local distances. The global distance is sim-
ply the L2 distance of the global features. For the local
distance, we dynamically match the local parts from top to
bottom to ﬁnd the alignment of local features with the min-
imum total distance. This is based on a simple assumption
that, for two images of the same person, the local feature
from one body part of the ﬁrst image is more similar to the
semantically corresponding body part of the other image.
two images, F =
features of
{f1, · · · , fH } and G = {g1, · · · , gH }, we ﬁrst normalize

Given the local

2

Figure 2. The framework of AlignedReID. Both the global branch and the local branch share the same convolution network to extract
the feature map. The global feature is extracted by applying global pooling directly on the feature map. For the local branch, one 1 × 1
convolution layer is applied after horizontal pooling, which is a global pooling with a horizontal orientation. Triplet hard loss is applied,
which selects triplet samples by hard sample mining according to global distances.

SH,H is the total distance of the ﬁnal shortest path (i.e., the
local distance) between the two images.

As shown in Fig. 3, images A and B are samples of
the same person. The alignment between the corresponding
body parts, such as part 1 in image A, and part 4 in im-
age B, are included in the shortest path. Meanwhile, there
are alignments between non-corresponding parts, such as
part 1 in image A, and part 1 in image B, still included
in the shortest path. These non-corresponding alignments
are necessary to maintain the order of vertical alignment, as
well as make the corresponding alignments possible. The
non-corresponding alignment has a large L2 distance, and
its gradient is close to zero in Eq.1. Hence, the contribu-
tion of such alignments in the shortest path is small. The
total distance of the shortest path, i.e., the local distance be-
tween two images, is mostly determined by the correspond-
ing alignments.

The global and local distance together deﬁne the sim-
ilarity between two images in the learning stage, and we
chose TriHard loss proposed by [13] as the metric learning
loss. For each sample, according to the global distances,
the most dissimilar one with the same identity and the most
similar one with a different identity is chosen, to obtain a
triplet. For the triplet, the loss is computed based on both
the global distance and the local distance with different mar-
gins. The reason for using the global distance to mine hard
samples is due to two considerations. First, the calculation
of the global distance is much faster than that of the local
distance. Second, we observe that there is no signiﬁcant
difference in mining hard samples using both distances.

Note that in the inference stage, we only use the global
features to compute the similarity of two person images.
We make this choice mainly because we unexpectedly ob-
served that the global feature itself is also almost as good
as the combined features. This somehow counter-intuitive
phenomenon might be caused by two factors: 1) the feature

Figure 3. Example of AlignedReID local distance computed by
ﬁnding the shortest path. The black arrows show the shortest path
in the corresponding distance matrix on the right. The black lines
show the corresponding alignment between the two images on the
left.

the distance to [0, 1) by an element-wise transformation:

di,j =

e||fi−gj ||2 − 1
e||fi−gj ||2 + 1

i, j ∈ 1, 2, 3..., H,

(1)

where di,j is the distance between the i-th vertical part of
the ﬁrst image and the j-th vertical part of the second im-
age. A distance matrix D is formed based on these dis-
tances, where its (i, j)-element is di,j. We deﬁne the local
distance between the two images as the total distance of the
shortest path from (1, 1) to (H, H) in the matrix D. The
distance can be calculated through dynamic programming
as follows:

Si,j =





di,j
Si−1,j + di,j
Si,j−1 + di,j
min(Si−1,j, Si,j−1) + di,j

i = 1, j = 1
i (cid:54)= 1, j = 1
i = 1, j (cid:54)= 1
i (cid:54)= 1, j (cid:54)= 1,

(2)

where Si,j is the total distance of the shortest path when
walking from (1, 1) to (i, j) in the distance matrix D, and

3

Figure 4. Framework of the mutual learning approach. Two networks with parameters θ1 and θ2 are trained together. Each network has
two branches: a classiﬁcation branch and a metric learning branch. The classiﬁcation branches are trained with classiﬁcation losses, and
learn each other through classiﬁcation mutual loss. The metric learning branches are trained with metric losses, which include both global
distance and local distance. Meanwhile, the metric learning branches learn each other by metric mutual loss.

map jointly learned is better than learning the global fea-
ture only, because we have exploited the structure prior of
the person image in the learning stage; 2) with the aid of
local feature matching, the global feature can pay more at-
tention to the body of the person, rather than over ﬁtting the
background.

3.2. Mutual Learning for Metric Learning

improve performance.

We apply mutual learning to train models for Aligne-
dReID, which can further
A
distillation-based model usually transfers knowledge from
a pre-trained large teacher network to a smaller student
network, such as [4].
In this paper, we train a set of
student models simultaneously, transferring knowledge be-
tween each other, such as [49]. Differing from [49], which
only adopts the Kullback-Leibler (KL) distance between
classiﬁcation probabilities, we propose a new mutual learn-
ing loss for metric learning.

The framework of our mutual

learning approach is
shown in Fig. 4. The overall loss function includes the met-
ric loss, the metric mutual loss, the classiﬁcation loss and
the classiﬁcation mutual loss. The metric loss is decided by
both the global distances and the local distances, while the
metric mutual loss is decided only by the global distances.
The classiﬁcation mutual loss is the KL divergence for clas-
siﬁcation as in [49].

Given a batch of N images, each network extracts their
global features and calculates the global distance between
each other as an N × N batch distance matrix, where M θ1
ij
and M θ2
ij denote the (i, j)-th element in the matrices sepa-

rately. The mutual learning loss is deﬁned as

LM =

1
N 2

N
(cid:88)

N
(cid:88)

(cid:16)

i

j

[ZG(M θ1

ij ) − M θ2

ij ]2

+ [M θ1

ij − ZG(M θ2

ij )]2(cid:17)

,

(3)

where ZG(·) represents the zero gradient function, which
treats the variable as constant when calculating gradients,
stopping the backpropagation in the learning stage.

By applying the zero gradient function, the second-order

gradients is

∂2LM
ij ∂M θ2
We found that it speeds up the convergence and improves
the accuracy compared to a mutual loss without the zero
gradient function.

∂M θ1

= 0.

(4)

ij

4. Experiments

In this section, we present our results on three most
widely used ReID datasets: Market1501 [53], CUHK03
[14], and CUHK-SYSU [41].

4.1. Datasets

Market1501 contains 32,668 images of 1,501 labeled
persons of six camera views. There are 751 identities in
the training set and 750 identities in the testing set. In the
original study on this proposed dataset, the author also uses
mAP as the evaluation criteria to test the algorithms.

CUHK03 contains 13,164 images of 1,360 identities.
It provides bounding boxes detected from deformable part
models (DPMs) and manual labeling.

4

CUHK-SYSU is a large-scale benchmark for a person
search, containing 18,184 images (99,809 bounding boxes)
and 8,432 identities. The training set contains 11,206 im-
ages of 5,532 query persons, whereas the test set contains
6,978 images of 2,900 persons.

Note that we only train a single model using training
samples from all three datasets, as in [40, 50]. We follow
the ofﬁcial training and evaluation protocols on Market1501
and CUHK-SYSU, and mainly report the mAP and rank-1
accuracy. For CUHK03, because we train one single model
for all benchmarks, it is slightly different from the stan-
dard procedure in [14], which splits the dataset randomly
20 times, and the gallery for testing has 100 identities each
time. We only randomly split the dataset once for training
and testing, and the gallery includes 200 identities. It means
our task might be more difﬁcult than the standard procedure.
Similarly, we evaluate our method with rank-1, -5, and -10
accuracy on CUHK03.

4.2. Implementation Details

We use Resnet50 and Resnet50-Xception (Resnet-X)
pre-trained on ImageNet [28] as the base models. Resnet50-
Xception replaces the 3 × 3 ﬁlter kernel through the Xcep-
tion cell [7], which contains one 3 × 3 channel-wise convo-
lution layer and one 1 × 1 spatial convolution layer. Each
image is resized into 224 × 224 pixels. The data augmenta-
tion includes random horizontal ﬂipping and cropping. The
margins of TriHard loss for both the global and local dis-
tances is set to 0.5, and the mini-batch size is set to 160,
in which each identity has 4 images. Each epoch includes
2000 mini-batches. We use an Adam optimizer with an ini-
tial learning rate of 10−3, and shrink this learning rate by a
factor of 0.1 at 80 and 160 epochs until achieving conver-
gence.

For mutual learning, the weight of classiﬁcation mutual
loss (KL) is set to 0.01, and the weight of metric mutual
loss is set to 0.001. The optimizer uses Adam with an ini-
tial learning rate of 3 × 10−4, which is reduced to 10−4
and 10−5 at 60 epochs and 120 epochs until convergence is
achieved.

Re-ranking is an effective technique for boosting the per-
formance of ReID [57]. We follow the method and details in
[57]. In all of our experiments, we combined metric learn-
ing loss with classiﬁcation (identiﬁcation) loss.

4.3. Advantage of AlignedReID

In this section, we analyze the advantage of our Aligne-

dReID model.

We ﬁrst show some typical results of the alignment in Fig
5. In FIg 5(a), the detection box of the right person is in-
accurate, which results in a serious misalignment of heads.
AlignedReID matches the ﬁrst part of the left image with
the ﬁrst three parts of the right image in the shortest path.

Figure 5. The black lines show the alignments of local parts be-
tween two persons: the thicker the line is, the greater it contributes
to the shortest path. Persons have the same identities in (a-c), while
persons have different identities in (d).

Fig 5(b) presents another difﬁcult situation where human
body structure is defective. The left image does not con-
tain the parts below the knee. In the alignment, the skirt
side of the right image are associated with the skirt parts of
the left one, while the leg parts of the right image provide
small contribution to the shortest path. Fig 5(c) shows an
example of occlusion, where the lower part of the persons
are invisible. The alignment shows that the occlude parts
contribute small in the shortest path, hence the other parts
are paid more attention in the learning stage. Fig 5(d) show
two different persons with similar appearances. The shirt
logo of the right person has no similar part in the left per-
son, which results in a large shortest path distance (local
distance) between these two images.

We then compare our AlignedReID with two similar net-
works: Baseline which has no local feature branch, and GL-
Baseline which has local feature branch without alignment.
In GL-Baseline, the local loss is the sum of distances of spa-
tial corresponding local features. All results are obtained by
using the same network and the same training setting. The
results are shown in Table 1. Compared to Baseline, GL-
Baseline often gets a worse accuracy. Hence, a local branch
without alignment does not help. Meanwhile, AlignedReID
boosts 3.1% ∼ 7.9% rank-1 accuracy and 3.6% ∼ 10.1%
mAP on all datasets. The local feature branch with align-
ment helps the network focus on useful image regions and
discriminates similar person images with subtle differences.

5

Base model

Resnet50

Resnet50-X

Methods
Baseline
GL-Baseline
AlignedReID
Baseline
GL-Baseline
AlignedReID

Market1501
r = 1
83.8
80.4
89.2
83.6
79.7
89.4

r = 5 mAP
88.5
94.1
86.0
92.0
92.9
96.0
87.9
93.3
85.9
92.4
91.3
95.8

CUHK-SYSU
r = 1
90.9
88.2
94.5
90.4
87.9
93.5

r = 5
96.6
95.6
98.0
95.8
95.6
97.3

mAP
64.5
58.0
72.8
61.7
57.1
71.8

CUHK03
r=5
95.8
95.0
97.5
94.5
94.7
97.1

r = 10
97.9
97.2
98.8
97.1
97.1
98.5

r=1
83.3
81.7
88.1
80.4
80.7
88.3

Table 1. Experiment results of AlignedReID. We combine metric learning loss with classiﬁcation loss in our experiments.

Loss

Baseline

Baseline+MC

Baseline+MM

AlignedReID

AlignedReID+MC

AlignedReID+MM

Base model mAP
64.5
Resnet50
61.7
Resnet50-X
67.8
Resnet50
68.7
Resnet50-X
66.8
Resnet50
66.8
Resnet50-X
72.8
Resnet50
71.8
Resnet50-X
70.9
Resnet50
71.2
Resnet50-X
79.3
Resnet50
79.3
Resnet50-X

Market1501
r = 1
83.8
83.6
86.2
87.3
86.3
86.2
89.2
89.4
88.6
88.5
91.8
91.2

r=5 mAP
88.5
94.1
87.9
93.3
89.8
94.6
89.7
95.4
90.1
95.1
90.1
94.6
92.9
96.0
91.3
95.8
91.4
95.9
91.2
95.8
97.1
94.4
94.4
96.9

CUHK-SYSU
r = 1
90.9
90.4
92.2
91.8
92.2
92.5
94.5
93.5
93.2
93.3
95.7
95.8

r=5
96.6
95.8
97.1
96.8
96.9
96.8
98.0
97.3
97.2
97.4
98.8
98.7

CUHK03
r = 5
95.8
94.5
95.4
96.2
95.6
95.8
97.5
97.1
97.0
96.8
98.9
99.6

r = 10
97.9
97.1
97.5
98.1
97.7
97.8
98.8
98.5
98.3
98.4
99.5
99.8

r = 1
83.3
80.4
83.8
84.6
83.8
84.2
88.1
88.3
87.6
86.9
92.4
92.3

Table 2. Results of mutual learning. MC stands for experiments with classiﬁcation mutual loss. MM stands for experiments with both
classiﬁcation mutual loss and metric mutual loss.

We ﬁnd that if we apply the local distance together with
the global distance in the inference stage, rank-1 accuracy
further improves approximately 0.3% ∼ 0.5%. However,
it is time consuming and not practical when searching in a
large gallery. Hence, we recommend using the global fea-
ture only.

4.4. Analysis of Mutual Learning

In the mutual learning experiment, we simultaneously
train two AlignedReID models. One model is based on
Resnet50, and the other is based on Resnet50-Xception.
We compare their performances for three cases: with both
metric mutual loss and classiﬁcation mutual loss, with only
classiﬁcation mutual loss, and with no mutual loss. We also
conduct a similar mutual learning experiment as a baseline,
where the global features are trained without local features.
The results are shown in Table 2.

Both experiments show that the metric mutual learning
method can further improve performance. With the baseline
mutual learning experiment, the classiﬁcation mutual loss
signiﬁcantly improves performance on all datasets. How-
ever, with the AlignedReID mutual learning experiment,
because the models without mutual learning perform well
enough, the classiﬁcation mutual loss cannot further im-
prove performance.

4.5. Comparison with Other Methods

In this subsection, we compare the results of Aligne-
dReID with state-of-the-art methods, in Table 3 ∼ 5.
In
the tables, AlignedReID represents our method with mutual
learning, and AlignedReID (RK) is our method with both
mutual learning and re-ranking [57] with k-reciprocal en-
coding.

On Market1501, GLAD [37] achieves an 89.9% rank-
1 accuracy, and our AlignedReID achieves a 91.8% rank-1
accuracy, exceeding it. For mAP, [13] obtains 81.1% owing
to the use of re-ranking. With the help of re-ranking, the
rank-1 accuracy and mAP are further improved to 94.4%
and 90.7% in our AlignedReID (RK), outperforming the
best of previous works by 4.5% and 9.6% separately.

On CUHK03, without re-ranking, HydraPlus-Net [20]
achieves 91.8% rank-1 accuracy and our AlignedReID
yields 92.4%. Note that our test gallery size is two times
large as that used in [20]. Furthermore, our AlignedReID
(RK) obtains a 97.8% rank-1 accuracy, exceeding state-of-
the-art by 6.0%.

There have not been many studies reported on CUHK-
SYSU. With this dataset, AlignedReID achieves 94.4%
mAP and 95.8% rank-1 accuracy, which is much higher
than any published results.

6

Figure 6. Interface of our human performance evaluation system for CUHK03. The left side shows a query image and the right side shows
10 images sampled using our deep model.

Table 3. Comparison on Market1501 in single query mode

Methods
Temporal [23]
Learning [47]
Gated [32]
Person [5]
Re-ranking [57]
Pose [52]
Scalable [1]
Improving [16]
In [13]
In (RK)[13]
Spindle[50]
Deep[49]∗
DarkRank[4]∗
GLAD[37]∗
HydraPlus-Net[20]∗
AlignedReID
AlignedReID (RK)

mAP
22.3
35.7
39.6
45.5
63.6
56.0
68.8
64.7
69.1
81.1
-
68.8
74.3
73.9
-
79.3
90.7

r=1
47.9
61.0
65.9
71.8
77.1
79.3
82.2
84.3
84.9
86.7
76.9
87.7
89.8
89.9
76.9
91.8
94.4

Table 4. Comparison on CUHK03 labeled dataset
r=10
-
94.8
-
88.3
-
99.2
-
-
95.8
98.9
98.7
98.6
99.2
99.1
99.1
99.5
99.8

Methods
Person [15]
Learning [47]
Gated [32]
A [34]
Re-ranking [57]
In [13]
Joint [42]
Deep [10]∗
Looking [2]∗
Unlabeled [56]
A [55]∗
Spindle[50]
DarkRank[4]∗
GLAD[37]∗
HydraPlus-Net[20]∗
AlignedReID
AlignedReID (RK)

r=5
-
90.0
-
80.1
-
95.2
-
-
95.2
97.6
97.1
97.8
98.4
97.9
98.4
98.9
99.6

r=1
44.6
62.6
61.8
57.3
64.0
75.5
77.5
84.1
72.4
84.6
83.4
88.5
89.7
85.0
91.8
92.4
97.8

5. Human Performance in Person ReID

Given the signiﬁcant improvement of our approach, we
are curious to ﬁnd the quality of human performance. Thus,
we conduct human performance evaluations on Market1501
and CUHK03.

To make the study feasible, for each query image, the
annotator does not have to ﬁnd the same person from the
entire gallery set. We ask him or her to pick the answer
from a much smaller set of selected images.

In CUHK03, for each query image, there is only one im-
age for the identical person in the gallery set. The anno-
tator looks for the identical person among 10 images se-
lected: our ReID model ﬁrst generates the top10 results in
the gallery set for the query image; if the “ground truth” is
not among the top10 results, we replace the 10th result with
the ground truth.

For Market1501, there may be more than one ground
truth in the gallery set. The annotator needs to pick one
from 50 images selected as follows: our ReID model gener-

Table 5. Comparison with existing methods on CUHK-SYSU

mAP
Methods
55.7
End[41]
Deep [29]∗
74.0
77.9
Neural [17]
AlignedReID 94.4

r=1
62.7
76.7
81.2
95.7

ated the top50 results in the gallery set for the query image;
if any ground truth is not among them, it would be used to
replace one non-ground truth result with the lowest rank. In
this way, we make sure that all ground truths are in the 50
selected images.

The interface of the human performance evaluation sys-
tem is presented in Fig 6. The images are randomly shuf-
ﬂed before being displayed to the annotator. The evaluation
website will be available soon. Ten professional annotators
participate in the evaluation. Because only one candidate is
chosen, we are unable to obtain the mAP of human beings

7

as a standard evaluation. The rank-1 accuracies are com-
puted for each annotator on all datasets. The best accuracy
is then used as the human performance, which is shown in
Table 6.

On Market1501, human beings achieve a 93.5% rank-
1 accuracy, which is better than all state-of-the-art meth-
ods. The rank-1 accuracy in our AlignedReID (RK) reaches
94.4% rank-1, exceeding the human performance. On
CUHK03, the human performance reaches a 95.7% rank-1
accuracy, which is much higher than any known state-of-
the-art methods. Our AlignedReID (RK) obtains a 97.8%
rank-1 accuracy, surpassing the human performance.

Figure 7 shows some examples, where an annotator se-
lected a wrong answer, while the top1 result provided by
our method is correct.

Table 6. Results of human performance evaluation. We show the
accuracies of the ﬁve annotators who did best in the evaluation.
We also show our AlignedReID results with re-ranking.

Market1501 CUHK03

Human Rank 1
Human Rank 2
Human Rank 3
Human Rank 4
Human Rank 5
AlignedReID (RK)

93.5
91.1
90.6
90.0
88.3
94.4

95.7
91.9
91.2
91.1
90.0
97.8

6. Conclusion

In this paper, we have demonstrated that an implicit
alignment of local features can substantially improve global
feature learning. This surprising result gives us an impor-
tant insight: the end-to-end learning with structure prior is
more powerful than a “blind” end-to-end learning.

Although we show that our methods outperform humans
in the Market1501 and CUHK03 datasets, it is still early
to claim that machines beat humans in general. Figure 8
presents a few “big” mistakes which seldom confuses hu-
mans. This indicates that the machine still has a lot of room
for improvement.

References

[1] S. Bai, X. Bai, and Q. Tian.

identiﬁcation on supervised smoothed manifold.
preprint arXiv:1703.08359, 2017.

Scalable person re-
arXiv

[2] I. B. Barbosa, M. Cristani, B. Caputo, A. Rognhaugen, and
T. Theoharis. Looking beyond appearances: Synthetic train-
ing data for deep cnns in re-identiﬁcation. arXiv preprint
arXiv:1701.03153, 2017.

[3] W. Chen, X. Chen, J. Zhang, and K. Huang. Beyond triplet
loss: a deep quadruplet network for person re-identiﬁcation.
arXiv preprint arXiv:1704.01719, 2017.

Figure 7. Top: query image. Middle: the result picked by an an-
notator. Bottom: top1 result by our method.

Figure 8. Top: query image. Middle: top1 result by our method.
Bottom: ground truth.

[4] Y. Chen, N. Wang, and Z. Zhang. Darkrank: Accelerating
deep metric learning via cross sample similarities transfer.
arXiv preprint arXiv:1707.01220, 2017.

[5] Y.-C. Chen, X. Zhu, W.-S. Zheng, and J.-H. Lai. Person re-
identiﬁcation by camera correlation aware feature augmen-
tation. IEEE Transactions on Pattern Analysis and Machine

8

Intelligence, 2017.

[6] D. Cheng, Y. Gong, S. Zhou, J. Wang, and N. Zheng. Person
re-identiﬁcation by multi-channel parts-based cnn with im-
proved triplet loss function. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
1335–1344, 2016.

[7] F. Chollet. Xception: Deep learning with depthwise separa-
ble convolutions. arXiv preprint arXiv:1610.02357, 2016.
[8] H. Fan, L. Zheng, and Y. Yang. Unsupervised person re-
identiﬁcation: Clustering and ﬁne-tuning. arXiv preprint
arXiv:1705.10444, 2017.

[9] M. Farenzena, L. Bazzani, A. Perina, V. Murino, and
M. Cristani. Person re-identiﬁcation by symmetry-driven ac-
cumulation of local features. In Computer Vision and Pat-
tern Recognition (CVPR), 2010 IEEE Conference on, pages
2360–2367. IEEE, 2010.

[10] M. Geng, Y. Wang, T. Xiang, and Y. Tian. Deep trans-
arXiv preprint

fer learning for person re-identiﬁcation.
arXiv:1611.05244, 2016.

[11] O. Hamdoun, F. Moutarde, B. Stanciulescu, and B. Steux.
Person re-identiﬁcation in multi-camera system by signa-
ture based on interest point descriptors collected on short
In Distributed Smart Cameras, 2008.
video sequences.
ICDSC 2008. Second ACM/IEEE International Conference
on, pages 1–6. IEEE, 2008.

[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016.

[13] A. Hermans, L. Beyer, and B. Leibe.

loss for person re-identiﬁcation.

triplet
arXiv:1703.07737, 2017.

In defense of the
arXiv preprint

[14] W. Li, R. Zhao, T. Xiao, and X. Wang. Deepreid: Deep ﬁlter
pairing neural network for person re-identiﬁcation. pages
152–159, 2014.

[15] S. Liao, Y. Hu, X. Zhu, and S. Z. Li. Person re-identiﬁcation
by local maximal occurrence representation and metric
learning. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 2197–2206,
2015.

[16] Y. Lin, L. Zheng, Z. Zheng, Y. Wu, and Y. Yang. Improv-
ing person re-identiﬁcation by attribute and identity learning.
arXiv preprint arXiv:1703.07220, 2017.

[17] H. Liu, J. Feng, Z. Jie, K. Jayashree, B. Zhao, M. Qi, J. Jiang,
and S. Yan. Neural person search machines. In The IEEE
International Conference on Computer Vision (ICCV), Oct
2017.

[18] H. Liu, J. Feng, M. Qi, J. Jiang, and S. Yan. End-to-end
comparative attention networks for person re-identiﬁcation.
IEEE Transactions on Image Processing, 2017.

[19] H. Liu, Z. Jie, K. Jayashree, M. Qi, J. Jiang, S. Yan, and
J. Feng. Video-based person re-identiﬁcation with accumula-
tive motion context. arXiv preprint arXiv:1701.00193, 2017.
[20] X. Liu, H. Zhao, M. Tian, L. Sheng, J. Shao, S. Yi, J. Yan,
and X. Wang. Hydraplus-net: Attentive deep features for
pedestrian analysis. 2017.

[21] Y. Liu, J. Yan, and W. Ouyang. Quality aware network for set

to set recognition. arXiv preprint arXiv:1704.03373, 2017.

[22] X. Ma, X. Zhu, S. Gong, X. Xie, J. Hu, K.-M. Lam, and
Y. Zhong. Person re-identiﬁcation by unsupervised video
matching. Pattern Recognition, 65:197–210, 2017.

[23] N. Martinel, A. Das, C. Micheloni, and A. K. Roy-
Chowdhury.
Temporal model adaptation for person re-
identiﬁcation. In European Conference on Computer Vision,
pages 858–877. Springer, 2016.

[24] T. Matsukawa and E. Suzuki. Person re-identiﬁcation us-
ing cnn features learned from combination of attributes. In
Pattern Recognition (ICPR), 2016 23rd International Con-
ference on, pages 2428–2433. IEEE, 2016.

[25] N. McLaughlin, J. Martinez del Rincon, and P. Miller. Re-
current convolutional network for video-based person re-
In Proceedings of the IEEE Conference
identiﬁcation.
on Computer Vision and Pattern Recognition, pages 1325–
1334, 2016.

[26] P. Peng, T. Xiang, Y. Wang, M. Pontil, S. Gong, T. Huang,
and Y. Tian. Unsupervised cross-dataset transfer learning for
In Proceedings of the IEEE Con-
person re-identiﬁcation.
ference on Computer Vision and Pattern Recognition, pages
1306–1315, 2016.

[27] F. Radenovi´c, G. Tolias, and O. Chum. Cnn image retrieval
learns from bow: Unsupervised ﬁne-tuning with hard exam-
In European Conference on Computer Vision, pages
ples.
3–20. Springer, 2016.

[28] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
Imagenet large scale visual recognition challenge.
et al.
International Journal of Computer Vision, 115(3):211–252,
2015.

[29] A. Schumann, S. Gong, and T. Schuchert. Deep learning pro-
totype domains for person re-identiﬁcation. arXiv preprint
arXiv:1610.05047, 2016.

[30] Springer. MARS: A Video Benchmark for Large-Scale Person

Re-identiﬁcation, 2016.

[31] Y. T. Tesfaye, E. Zemene, A. Prati, M. Pelillo, and
M. Shah. Multi-target tracking in multiple non-overlapping
cameras using constrained dominant sets. arXiv preprint
arXiv:1706.06196, 2017.

[32] R. R. Varior, M. Haloi, and G. Wang. Gated siamese
convolutional neural network architecture for human re-
identiﬁcation. In European Conference on Computer Vision,
pages 791–808. Springer, 2016.

[33] R. R. Varior, B. Shuai, J. Lu, D. Xu, and G. Wang. A
siamese long short-term memory architecture for human re-
identiﬁcation. In European Conference on Computer Vision,
pages 135–153. Springer, 2016.

[34] R. R. Varior, B. Shuai, J. Lu, D. Xu, and G. Wang. A
siamese long short-term memory architecture for human re-
identiﬁcation. In European Conference on Computer Vision,
pages 135–153, 2016.

[35] J. Wang, S. Zhou, J. Wang, and Q. Hou. Deep rank-
ing model by large adaptive margin learning for person re-
identiﬁcation. arXiv preprint arXiv:1707.00409, 2017.

[36] T. Wang, S. Gong, X. Zhu, and S. Wang.

Person re-
identiﬁcation by discriminative selection in video ranking.
IEEE transactions on pattern analysis and machine intelli-
gence, 38(12):2501–2514, 2016.

9

[37] L. Wei, S. Zhang, H. Yao, W. Gao, and Q. Tian. Glad:
Global-local-alignment descriptor for pedestrian retrieval.
arXiv preprint arXiv:1709.04329, 2017.

[54] L. Zheng, Y. Yang, and A. G. Hauptmann.
identiﬁcation: Past, present and future.
arXiv:1610.02984, 2016.

Person re-
arXiv preprint

[55] Z. Zheng, L. Zheng, and Y. Yang. A discriminatively learned
cnn embedding for person re-identiﬁcation. arXiv preprint
arXiv:1611.05666, 2016.

[56] Z. Zheng, L. Zheng, and Y. Yang. Unlabeled samples gener-
ated by gan improve the person re-identiﬁcation baseline in
vitro. In The IEEE International Conference on Computer
Vision (ICCV), Oct 2017.

[57] Z. Zhong, L. Zheng, D. Cao, and S. Li. Re-ranking person
re-identiﬁcation with k-reciprocal encoding. arXiv preprint
arXiv:1701.08398, 2017.

[58] Z. Zhou, Y. Huang, W. Wang, L. Wang, and T. Tan. See
the forest for the trees: Joint spatial and temporal recurrent
neural networks for video-based person re-identiﬁcation.

[38] Q. Xiao, K. Cao, H. Chen, F. Peng, and C. Zhang. Cross do-
main knowledge transfer for person re-identiﬁcation. arXiv
preprint arXiv:1611.06026, 2016.

[39] Q. Xiao, H. Luo, and C. Zhang. Margin sample mining loss:
A deep learning based method for person re-identiﬁcation.
arXiv preprint arXiv:1710.00478, 2017.

[40] T. Xiao, H. Li, W. Ouyang, and X. Wang. Learning deep
feature representations with domain guided dropout for per-
son re-identiﬁcation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 1249–
1258, 2016.

[41] T. Xiao, S. Li, B. Wang, L. Lin, and X. Wang. End-
arXiv preprint

to-end deep learning for person search.
arXiv:1604.01850, 2016.

[42] T. Xiao, S. Li, B. Wang, L. Lin, and X. Wang. Joint detection
and identiﬁcation feature learning for person search. In Proc.
CVPR, 2017.

[43] H. Yao, S. Zhang, Y. Zhang, J. Li, and Q. Tian. Deep repre-
sentation learning with part loss for person re-identiﬁcation.
arXiv preprint arXiv:1707.00798, 2017.

[44] J. You, A. Wu, X. Li, and W.-S. Zheng. Top-push video-
based person re-identiﬁcation. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 1345–1353, 2016.

[45] D. Zhang, W. Wu, H. Cheng, R. Zhang, Z. Dong, and
Z. Cai. Image-to-video person re-identiﬁcation with tempo-
rally memorized similarity learning. IEEE Transactions on
Circuits and Systems for Video Technology, 2017.

[46] L. Zhang, T. Xiang, and S. Gong. Learning a discriminative
null space for person re-identiﬁcation. In The IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
June 2016.

[47] L. Zhang, T. Xiang, and S. Gong. Learning a discriminative
null space for person re-identiﬁcation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 1239–1248, 2016.

[48] W. Zhang, S. Hu, and K. Liu. Learning compact appear-
ance representation for video-based person re-identiﬁcation.
arXiv preprint arXiv:1702.06294, 2017.

[49] Y. Zhang, T. Xiang, T. M. Hospedales, and H. Lu. Deep

mutual learning. arXiv preprint arXiv:1706.00384, 2017.

[50] H. Zhao, M. Tian, S. Sun, J. Shao, J. Yan, S. Yi, X. Wang,
and X. Tang. Spindle net: Person re-identiﬁcation with hu-
man body region guided feature decomposition and fusion.
CVPR, 2017.

[51] R. Zhao, W. Oyang, and X. Wang. Person re-identiﬁcation
by saliency learning. IEEE transactions on pattern analysis
and machine intelligence, 39(2):356–370, 2017.

[52] L. Zheng, Y. Huang, H. Lu, and Y. Yang. Pose invariant
embedding for deep person re-identiﬁcation. arXiv preprint
arXiv:1701.07732, 2017.

[53] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian.
In Com-

Scalable person re-identiﬁcation: A benchmark.
puter Vision, IEEE International Conference, 2015.

10

