5
1
0
2
 
y
a
M
 
9
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
7
1
1
5
0
.
5
0
5
1
:
v
i
X
r
a

Vector-Space Markov Random Fields via
Exponential Families

Wesley Tansey∗
Oscar Hernan Madrid Padilla†
Arun Sai Suggala‡
Pradeep Ravikumar§

Abstract

We present Vector-Space Markov Random Fields (VS-MRFs), a novel class of
undirected graphical models where each variable can belong to an arbitrary vector
space. VS-MRFs generalize a recent line of work on scalar-valued, uni-parameter
exponential family and mixed graphical models, thereby greatly broadening the
class of exponential families available (e.g., allowing multinomial and Dirichlet
distributions). Speciﬁcally, VS-MRFs are the joint graphical model distributions
where the node-conditional distributions belong to generic exponential families
with general vector space domains. We also present a sparsistent M -estimator for
learning our class of MRFs that recovers the correct set of edges with high proba-
bility. We validate our approach via a set of synthetic data experiments as well as
a real-world case study of over four million foods from the popular diet tracking
app MyFitnessPal. Our results demonstrate that our algorithm performs well em-
pirically and that VS-MRFs are capable of capturing and highlighting interesting
structure in complex, real-world data. All code for our algorithm is open source
and publicly available.

1

Introduction

Undirected graphical models, also known as Markov Random Fields (MRFs), are a
popular class of models for probability distributions over random vectors. Popular
parametric instances include Gaussian MRFs, Ising, and Potts models, but these are all
suited to speciﬁc data-types: Ising models for binary data, Gaussian MRFs for thin-
tailed continuous data, and so on. Conversely, when there is prior knowledge of the
graph structure but limited information otherwise, nonparametric approaches are avail-
able Sudderth et al. [2010]. A recent line of work has considered the challenge of
specifying classes of MRFs targeted to the data-types in the given application, when

∗Department of Computer Science, University of Texas at Austin, tansey@cs.utexas.edu (corre-

sponding author)
†Department

Statistics
oscar.madrid@utexas.edu

of

and Data

Sciences;

University

of

Texas

at Austin,

‡Department of Computer Science, University of Texas at Austin, arunsai@utexas.edu
§Department of Computer Science, University of Texas at Austin, pradeepr@cs.utexas.edu

1

the structure is unknown. For the speciﬁc case of homogeneous data, where each vari-
able in the random vector has the same data-type, Yang et al. [2012] proposed a general
subclass of homogeneous MRFs. In their construction, they imposed the restriction that
each variable conditioned on other variables belong to a shared exponential family dis-
tribution, and then performed a Hammersley-Clifford-like analysis to derive the corre-
sponding joint graphical model distribution, consistent with these node-conditional dis-
tributions. As they showed, even classical instances belong to this sub-class of MRFs;
for instance, with Gaussian MRFs and Ising models, the node-conditional distributions
follow univariate Gaussian and Bernoulli distributions respectively.

Yang et al. [2014] then proposed a class of mixed MRFs that extended this construc-
tion to allow for random vectors with variables belonging to different data types, and
allowing each node-conditional distribution to be drawn from a different univariate,
uni-parameter exponential family member (such as a Gaussian with known variance
or a Bernoulli distribution). This ﬂexibility in allowing for different univariate expo-
nential family distributions yielded a class of mixed MRFs over heterogeneous random
vectors that were capable of modeling a much wider class of distributions than was
previously feasible, opening up an entirely new suite of possible applications.

To summarize, the state of the art can specify MRFs over heterogeneous data-typed
random vectors, under the restriction that each variable conditioned on others belong to
a uni-parameter, univariate exponential family distribution. But in many applications,
such a restriction would be too onerous. For instance, a discrete random variable is
best modeled by a categorical distribution, but this is a multi-parameter exponential
family distribution, and does not satisfy the required restriction above. Other multi-
parameter exponential family distributions popular in machine learning include gamma
distributions with unknown shape parameter and Gaussian distributions with unknown
variance. Another restriction above is that the variables be scalar-valued; but in many
applications the random variables could belong to more general vector spaces, for ex-
ample a Dirichlet distribution.

As modern data modeling requirements evolve, extending MRFs beyond such re-
In this paper, we thus ex-
strictive paradigms is becoming increasingly important.
tend the above line of work in Yang et al. [2012, 2014]. As opposed to other ap-
proaches which merely cluster scalar variables Vats and Moura [2012], we allow node-
conditional distributions to belong to a generic exponential family with a general vector
space domain. We then perform a subtler Hammersley-Clifford-like analysis to derive
a novel class of vector-space MRFs (VS-MRFs) as joint distributions consistent with
these node-conditional distributions. This class of VS-MRFs provides support for the
many modelling requirements outlined above, and could thus greatly expand the po-
tential applicability of MRFs to new scientiﬁc analyses.

We also introduce an M -estimator for learning this class of VS-MRFs based on the
sparse group lasso, and show that it is sparsistent, and that it succeeds in recovering
the underlying edges of the graphical model. To solve the M -estimation problem, we
also provide a scalable optimization algorithm based on Alternating Direction Method
of Multipliers (ADMM) Boyd et al. [2011]. We validate our approach empirically via
synthetic experiments measuring performance across a variety of scenarios. We also
demonstrate the usefulness of VS-MRFs by modeling a real-world dataset of over four
million foods from the MyFitnessPal food database.

2

The remainder of this paper is organized as follows. Section 2 provides background
on mixed MRFs in the uni-parameter, univariate case. Section 3 details our general-
ization of the mixed MRF derivations to the vector-space case. Section 4 introduces
our M -estimator and derives its sparsistency statistical guarantees. Section 5 contains
our synthetic experiments and the MyFitnessPal case study. Finally, Section 6 presents
concluding remarks and potential future work.

2 Background: Scalar Mixed Graphical Models

Let X = (X1, X2, · · · Xp) be a p-dimensional random vector, where each variable Xr
has domain Xr. An undirected graphical model or a Markov Random Field (MRF) is
a family of joint distributions over the random vector X that is speciﬁed by a graph
G = (V, E), with nodes corresponding to each of the p random variables {Xr}p
r=1,
and edges that specify the factorization of the joint as:

P(X) ∝

(cid:89)

ψC(XC),

C∈C(G)

where C(G) is the set of fully connected subgraphs (or cliques) of the graph G, XC =
{Xs}s∈C denotes the subset of variables in the subset C ⊆ V , and {ψC(XC)}C∈C(G)
are clique-wise functions, each of which is a “local function” in that it only depend on
the variables in the corresponding clique, so that ψC(XC) only depends on the variable
subset XC.

Gaussian MRFs, Ising MRFs, etc. make particular parametric assumptions on these
clique-wise functions, but a key question is whether there exists a more ﬂexible speci-
ﬁcation of the form of these clique-wise functions that is targeted to the data-type and
other characteristics of the random vector X.

For the speciﬁc case where the variables are scalars, so that the domains Xr ⊆ R,
in a line of work, Yang et al. [2012, 2014] used the following construction to de-
rive a subclass of MRFs targeted to the random vector X. Suppose that for variables
Xr ∈ Xr, the following (single-parameter) univariate exponential family distribution
P (Xr) = exp{θrBr(Xr) + Cr(Xr) − Ar(θr)}, with natural parameter scalar θ, suf-
ﬁcient statistic scalar Br(Xr), base measure Cr(Xr) and log normalization constant
Ar(θ), serves as a suitable statistical model. Suppose that we use these univariate
distributions to specify conditional distributions:

P (Xr|X−r) = exp{ Er(X−r)Br(Xr)+

Cr(Xr) − Ar(X−r)}

,

(1)

where Er(·) is an arbitrary function of the rest of the variables X−r that serves as the
natural parameter. Would these node-conditional distributions for each node r ∈ V
be consistent with some joint distribution for some speciﬁcation of these functions
{Er(·)}r∈V ? Theorem 1 from Yang et al. [2014] shows that there does exist a unique

3

joint MRF distribution with the form:

P (X; θ) = exp

θrBr(Xr)

(cid:40)

(cid:88)

r∈V
(cid:88)

(cid:88)

+

r∈V

t∈N (r)

(cid:88)

+
(t1,...,tk)∈C
+ (cid:80)

θrtBt(Xt)Br(Xr) + . . .

θt1...tk (X)

Btj (Xtj )

k
(cid:89)

j=1

(cid:41)

r∈V Cr(Xr) − A(θ)

,

(2)

where A(θ) is the log-normalization constant. Their proof followed an analysis simi-
lar to the Hammersley-Clifford Theorem Lauritzen [1996], and entailed showing that
for a consistent joint, the only feasible conditional parameter functions Er(·) had the
following form:

Er(X−r) = θr +

θrtBt(Xt) + . . .

(cid:88)

t∈N (r)

(cid:88)

+

θrt2...tk (X)

Btj (Xtj )

t2,...,tk∈N (r)

k
(cid:89)

j=2

,

(3)

where θr·
neighbors of node r.

:= {θr, θrt, . . . , θrt2...tk } is a set of parameters, and N (r) is the set of

While their construction allows the speciﬁcation of targeted classes of graphical
models for heterogeneous random vectors, the conditional distribution of each variable
conditioned on the rest of the variables is assumed to be a single-parameter exponential
family distribution with a scalar sufﬁcient statistic and natural parameter. Furthermore,
their Hammersley-Clifford type analysis and sparsistency proofs relied crucially on that
assumption. However in the case of multi-parameter and multivariate distributions, the
sufﬁcient statistics are a vector; indeed the random variables need not be scalars at all
but could belong to a more general vector space. Could one construct classes of MRFs
for this more general, but prevalent, setting? In the next section, we answer in the
afﬁrmative, and present a generalization of mixed MRFs to the vector-space case, with
support for more general exponential families.

3 Generalization to the Vector-space Case

Let X = (X1, X2, · · · Xp) be a p-dimensional random vector, where each variable Xr
belongs to a vector space Xr. As in the scalar case, we will assume that a suitable
statistical model for variables Xr ∈ Xr is an exponential family distribution

P (Xr) = exp{

θrjBrj(Xr) + Cr(Xr) − Ar(θ)},

(4)

mr(cid:88)

j=1

4

with natural parameters {θrj}mr
j=1, base measure
Cr(Xr) and log normalization constant Ar(θ). We assume the sufﬁcient statistics Brj :
Xr (cid:55)→ R lie in some Hilbert space Hs, and moreover specify a minimal exponential
family so that:

j=1, and sufﬁcient statistics {Brj}mr

αjBrj(Xr) (cid:54)= c ,

(5)

mr(cid:88)

j=1

for any constant c and any vector α (cid:54)= 0. We note that even though the variables
{Xr} could lie in general vector spaces, the exponential family distribution above is
ﬁnite-dimensional. However, it has multiple parameters, which is the other facet that
distinguishes it from the single-parameter univariate setting of Yang et al. [2012, 2014].
We defer a generalization of our framework to inﬁnite-dimensional exponential fami-
lies to future work.

Suppose we use these general exponential family distributions to specify node-
conditional distributions of variables Xr conditioned on the rest of the random vari-
ables:

P (Xr|X−r) = exp{(cid:80)mr

j=1 Erj(X−r)Brj(Xr)

+Cr(Xr) − Ar(X−r)} ,

(6)

where {Erj(X−r)}mr
j=1 are arbitrary functions of the rest of the variables that serve
as natural parameters for the conditional distribution of Xr. As before, we ask the
question whether these node-conditional distributions can be consistent with some joint
distribution for some speciﬁcation of the parameter functions {Erj(X−r)}mr
j=1; the
following theorem addresses this very question.

Theorem 1. Let X = (X1, X2, . . . , Xp) be a p-dimensional random vector with node-
conditional distribution of each random vector Xr conditioned on the rest of random
variables as deﬁned in (6). These node-conditionals are consistent with a joint MRF
distribution over the random vector X, that is, Markov with respect to a graph G =
(V, E) with clique-set C, and with factors of size at most k, if and only if the functions
{Er()}r∈V specifying the node-conditional distributions have the form:

Eri(X−r) =θri +

θri;tjBtj(Xt) + . . .

(cid:88)

mt(cid:88)

t∈N (r)

j=1

(cid:88)

(cid:88)

+

t2,...,tk
∈N (r)

i2=1...mt2
...
ik=1...mtk

θri;...;tkik

Btj ij (Xtj )

k
(cid:89)

j=2

,

(7)

where θr· = {θri, θri;tj, θri;...;tkik } is a set of parameters, mt is the dimension of the
sufﬁcient statistic vector for the tth node-conditional distribution, and N (r) is the set of
neighbors of node r in graph G. The corresponding consistent joint MRF distribution
has the following form:

5

P (X|θ) = exp

θriBri(Xr) + . . .

(cid:40)

(cid:88)

mr(cid:88)

r∈V

i=1

(cid:88)

(cid:88)

+

t1,...,tk∈C

i1=1...mt1
...
ik=1...mtk

(cid:41)

Cr(Xr) − A(θ)

+

(cid:88)

r∈V

θt1i1;...;tkik

Btj ij (Xtj )

.

(8)

k
(cid:89)

j=1

We provide a Hammersley-Clifford type analysis as proof of this theorem in the
supplementary material, which however has subtleties not present in Yang et al. [2012,
2014], due to the arbitrary vector space domain of Xr, and the multiple parameters in
the exponential families, which consequently entailed leveraging the geometry of the
corresponding Hilbert spaces {Hs{s ∈ V } underlying the sufﬁcient statistics {Bsj}.

The above Theorem 1 provides us with a general class of vector-space MRFs (VS-
MRFs), where each variable could belong to more general vector space domains, and
whose conditional distributions are speciﬁed by more general ﬁnite-dimensional expo-
nential families. Consequently, many common distributions can be incorporated into
VS-MRFs that were previously unsupported or lacking in Yang et al. [2012, 2014]. For
instance, gamma and Gaussian nodes, though univariate, require vector-space param-
eters in order to be fully modeled. Additionally, multivariate distributions that were
impossible to use with previous methods, such as the multinomial and Dirichlet distri-
butions are now also available.

3.1 Pairwise conditional and joint distributions

Given the form of natural parameters in (7), the conditional distribution of a node Xr
given all other nodes X−r for the special case of pairwise MRFs (i.e. k = 2) has the
form

P (Xr|X−r, θr, θrt) = exp

θriBri(Xr)

(cid:88)

mr(cid:88)

mt(cid:88)

+

t∈N (r)

i=1

j=1

θri;tjBtj(Xt)Bri(Xr)

+Cr(Xr) − Ar(X−r, θr·)

,

(9)

= exp

Br(Xr), θr +

θrtBt(Xt)

(cid:42)








(cid:43)








+Cr(Xr) − Ar

θr +

θrtBt(Xt)



t∈N (r)
where θr is a vector formed from scalars {θri}mr
i=1, θrt is a matrix of dimension mr×mt
obtained from scalars θri;tj and (cid:104)., .(cid:105) represents dot product between two vectors. Thus,






mr(cid:88)

i=1






(cid:88)

t∈N (r)

(cid:88)

6

the joint distribution has the form

P (X|θ) =



(cid:88)

exp

(cid:42)



r∈V

+

(cid:88)

r∈V

Cr(Xr) − A(θ)

Br(Xr), θr +

θrtBt(Xt)

(cid:88)

t∈N (r)

(cid:41)

(cid:43)

,

(10)

with the log-normalization constant A(θ) = log (cid:82)
(cid:80)

r∈V Cr(Xr)}. Since A(θ) is generally intractable to calculate, we next present an

efﬁcient approach to learning the structure of VS-MRFs.

X exp{(cid:80)

r∈V (cid:104)Br(Xr), θr+(cid:80)

t∈N (r) θrtBt(Xt)(cid:105)+

4 Learning VS-MRFs

To avoid calculation of the log-normalization constant, we approximate the joint dis-
tribution in (10) with the independent product of node conditionals, also known as the
pseudo-likelihood,

P (X|θ) ≈

P (Xr|X−r, θr, θrt) .

(11)

(cid:89)

r

Let θr· = {θr, θ\r} be the set of parameters related to the node-conditional distribution
of node r, where θ\r = {θrt}t∈V \r. Since Ar() is convex for all exponential families
Wainwright and Jordan [2008], this gives us a loss function that is convex in θr·:

(cid:96)(θr·; D) = − 1
n



Br(X (i)

r ), θr +



(cid:42)

n
(cid:88)

i



(cid:43)

θrtBt(X (i)
t )

(cid:88)

t∈V \r





.

(12)

− Ar

θr +

θrtBt(X (i)
t )





(cid:88)

t∈V \r

We then seek to ﬁnd a sparse solution in terms of both edges and individual parameters
by employing the sparse group lasso regularization penalty Friedman et al. [2010],
Simon et al. [2013]:

R(θr·) = λ1

(cid:88)

√

t∈V \r

νrt ||θrt||2 + λ2

(cid:12)
(cid:12)
(cid:12)θ\r
(cid:12)

(cid:12)
(cid:12)
(cid:12)1 ,
(cid:12)

where νrt = mr × mt is the number of parameters in the pseudo-edge from node r
to node t (i.e., the edge (r, t) in the rth node-conditional). This yields a collection of
independent convex optimization problems, one for each node-conditional.

minimize
θr·

(cid:96)(θr·; D) + R(θr·)

We next present an approach to solving this problem based on Alternating Direction

(13)

(14)

7

Method of Multipliers (ADMM) Boyd et al. [2011].

4.1 Optimization Procedure

We ﬁrst introduce a slack variable z into (14) to adhere to the canonical form of
ADMM. For notational simplicity, we omit the data parameter D from the loss function
and the subscripts in θr· and Ar since it is clear we are dealing with the optimization
of a single node-conditional.

minimize
θ

(cid:96)(θ) + R(z)

subject to θ = z

,

where length(θ) = τ . The augmented Lagrangian is

Lα(θ, z, ρ) = (cid:96)(θ) + R(z) + ρT (θ − z) + (α/2) ||θ − z||2
2 .

(16)

Deﬁning the residual of the slack r = θ − z, we instead use the scaled form with
u = (1/α)ρ. ADMM proceeds in an alternating fashion, performing the following
updates at each iteration:

θk+1 = argmin

(cid:12)
(cid:12)θ − zk + uk(cid:12)
(cid:12)
(cid:96)(θ) + (α/2) (cid:12)
(cid:12)
(cid:12)
(cid:12)

zk+1 = argmin

(cid:12)
(cid:12)θk+1 − z + uk(cid:12)
(cid:12)
R(z) + (α/2) (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:17)

2

2

(cid:17)

2

2

(cid:16)

(cid:16)

θ

z

uk+1 = uk + θk+1 − zk+1

Updating θk+1. The jth subgradient of θ is gj(θ) = −Bj + ∇jA(θ) + α(θj + zk
j −
uk
j ). Note that the log-partition function, A(η), over the natural parameters, η = Bθ, is
available in closed form for most commonly-used exponential families. Thus, ∇2A(θ)
is a weighted sum of rank-one matrices. In cases where the number of samples is much
less than the total number of parameters (i.e. n << τ ), we can efﬁciently calculate
an exact Newton update in O(τ ) by leveraging the matrix inversion lemma Boyd and
Vandenberghe [2009]. Otherwise, we use a diagonal approximation of the Hessian and
perform a quasi-Newton update.

Updating zk+1. We can reformulate (18) as the proximal operator Parikh and Boyd
[2013] of R(z):

proxR/α(y) = argmin

R(z) + (α/2) ||z − y||2
2

(cid:17)

,

(cid:16)

z

where y = θk+1 + uk. From Friedman et al. [2010], it is straightforward to show

that the update has a closed-form solution for each jth block of edge parameters,

zk+1
j =

(cid:0)||S(α(yj), λ2)||2 −

α ||S(α(yj), λ2)||2 +

√

(cid:1)

νjλ1
√

S(α(yj), λ2)

+
νjλ1(1 − α)

,

8

(15)

(17)

(18)

(19)

(20)

(21)

where S(x, λ) is the soft-thresholding operator on x with cutoff at λ.

Updating uk+1. Per ADMM, closed-form is given in (19).

We iterate each of the above update steps in turn until convergence, then AND

pseudo-edges when stitching the graph back together.

4.2 Domain constraints

Many exponential family distributions require parameters with bounded domain. These
bounds correspond to afﬁne constraints on subsets of θ in the ADMM algorithm.1
Often these constraints are simple implicit restrictions to R+ or R−. In these cases
the log-normalization function A(η) serves as a built-in log-barrier function. For in-
stance, a normal distribution with unknown mean µ and unknown variance σ2 has
natural parameters η1 = µ
2σ2 , implying η2 < 0. However, since
A(η) = − η2
2 ln(−2η2), this constraint will be effectively enforced so long as we
1
4η2
are given a feasible starting point for η. Such a feasible point can always be discovered
using a standard phase I method Boyd and Vandenberghe [2009]. In the case of equal-
ity requirements, such as categorical and multinomial distributions, we can directly in-
corporate the constraints into the ADMM algorithm and solve an equality-constrained
Newton’s method when updating θ.

σ2 and η2 = − 1

− 1

4.3 Sparsistency

We next provide the mathematical conditions that ensure with high probability our
learning procedure recovers the true graph structure underlying the joint distribution.
Our results rely on similar sufﬁcient conditions to those imposed in papers analyzing
the Lasso Wainwright [2009] and the l1/l2 penalty in Jalali et al. [2011]. Before stating
the assumptions, we introduce the notation used in the proof.

4.3.1 Notation

rt (cid:54)= 0} be the true neighbourhood of node r and let dr be the degree
rj;tk : t ∈ N (r)}
rj;tk : t /∈ N (r)}. From now on
r. Let

Let N (r) = {t : θ∗
of r, i.e, dr = |N (r)|. And Sr be the index set of parameters {θ∗
r be the index of parameters {θ∗
and similarly Sc
we will overload the notation and simply use S and Sc instead of Sr and Sc
S(ex)
r = {θ∗
Let Qn

r·; D) be the sample Fischer Information matrix at node r. As
r . Finally, we write Qn
SS

before, we will ignore subscript r and use Qn instead of Qn
for the sub-matrix indexed by S.

rj;tk : θ∗
r = ∇2(cid:96)(θ∗

rj;tk (cid:54)= 0 ∧ t ∈ N (r)}.

We use the group structured norms deﬁned in Jalali et al. [2011] in our analysis.
The group structured norm ||u||G,a,b of a vector u with respect to a set of disjoint groups
G = {G1, . . . , GT } is deﬁned as ||(||uG1||b , . . . , ||uGT ||b)||a. We ignore the group G
and simply use ||u||a,b when it is clear from the context. Similarly the group structured

1Note that these subsets are different than the edge-wise groups that are L2-penalized. Rather, these

constraints apply to the sum of the ith value of each edge parameter and the ith bias weight.

9

norm ||M ||(a,b),(c,d) of a matrix Mp×p is deﬁned as
our analysis we always use b = 2, d = 2 and to minimize the notation we use ||M ||a,c
to denote ||M ||(a,2),(c,2). And we deﬁne ||M ||max as max
|Mi,j|, i.e, element wise
i,j
maximum of M.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)M 1(cid:12)
(cid:12)
(cid:12)((cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)c,d , . . . , ||M p||c,d)
(cid:12)
(cid:12)
(cid:12)a,b
(cid:12)
(cid:12)

. In

4.3.2 Assumptions

Let us begin by imposing assumptions on the sample Fisher Information matrix Qn.

Assumption 1. Dependency condition: Λmin(Qn

SS) ≥ Cmin.

Assumption 2. Incoherence condition:
(cid:12)
(cid:12)
ScS(Qn
(cid:12)Qn
(cid:12)
mmin = min

(cid:12)
SS)−1(cid:12)
(cid:12)∞,2 ≤ mmin
(cid:12)
mmax
mt.

(1−α)
√
dr

t

Assumption 3. Boundedness:
(cid:1) B (cid:0)XV \r
Λmax(E[B (cid:0)XV \r
(cid:1) = {Bt(Xt)}t∈V \r.
that B (cid:0)XV \r

(cid:1)T

for some α ∈ (0, 1] , where mmax = max

mt,

t

]) ≤ Dmax < ∞, where B (cid:0)XV \r

(cid:1) is a vector such

Note that the sufﬁcient statistics {Bri(Xr)}mr

i=1 of node r need not be bounded.
So to analyze the M-estimation problem, we make the following assumptions on log-
partition functions of joint and node-conditional distributions. These are similar to the
conditions imposed for sparsistency analysis of GLMs.

Assumption 4. The log partition function of the joint distribution satisﬁes the follow-
ing conditions: for all r ∈ V and i ∈ [mr]

1. there exists constants km, kv such that E[Bri(Xr)] ≤ km and E[Bri(Xr)2] ≤

kv,

2. there exists constant kh such that maxu:|u|≤1

∂2A(θ)
∂θ2
ri

(θ∗

ri + u, θ∗

r·) ≤ kh,

3. for scalar variable η , we deﬁne a function ¯Ar,i as:

¯Ar,i(η; θ) = log (cid:82)
(cid:42)

exp

Xp

(cid:110)
ηBri(Xr)2 + (cid:80)
(cid:43)

+ (cid:80)

s∈V

Bs(Xs), θs +

θstBt(Xt)

s∈V Cs(Xs)
(cid:111)

d(x)

(cid:88)

t∈N (s)

(22)

Then, there exists a constant kh such that maxu:|u|≤1

∂2Ar,i(η;θ∗
∂η2

r·)

(u) ≤ kh.

Assumption 5. For all r ∈ V , the log-partition function Ar(.) of the node wise condi-
tional distribution satisfy that there exists functions k1 (n, p) and k2 (n, p) such that for
(cid:12)
all feasible pairs θ and X, (cid:12)
(cid:12)
(cid:12)∇2Ar (a)(cid:12)
(cid:12)max ≤ k1 (n, p) where a ∈ [b, b + 4 k2 (n, p) max {log (n) , log (p)} 1]
(cid:12)
(cid:12)
for b := θr + (cid:80)
:=
(cid:12)
⊗i[ui, vi]. Moreoever, we assume that (cid:12)
(cid:12)
(cid:12)∇3Ar (b)(cid:12)
(cid:12)max ≤ k3 (n, p) for all feasible
(cid:12)
(cid:12)
pairs X and θ.

t∈V \r θrtBt(Xt), where for vectors u and v we deﬁne [u, v]

10

≤ λ1+λ2 ≤ M2
(cid:17)

2−α
α k1 (n, p) k2 (n, p)

α
2−α+2 mmax/mmin

λ1. Then, there ex-

d2 k1 (n, p) (k3 (n, p))2 (logp(cid:48))2 log (cid:0)p m2

(cid:1) ,

max

4.3.3 Sparsistency Theorem

Given these assumptions in 4.3.2 we are now ready to state our main sparsistency result.

Theorem 2. Consider the vector space graphical model distribution in (10) with true
parameters θ∗, edge set E and vertex set V such that the assumptions 1-5 hold. Sup-
pose that θ∗ satisﬁes min
(λ1 + λ2) and regularization parame-
(r,t)∈E

||θ∗

Cmin

2−α
α

mmax
mmin

ters λ1, λ2 satisfy M1

rt||2 ≥ 10 mmax
(cid:112)k1 (n, p)
for positive constants M1 and M2 and λ2 <
ists constants L, c1, c2 and c3 such that if n ≥ max(cid:8)L m9
4 log(p m2
max)
k1(n,p) k4 k2(n,p)2 , 8 k2
log ((cid:80)
exp(−c2 n) − exp(−c3 n), the following statements hold.

(cid:113) log(pm2
n
(cid:16)

h
k2
4

max)

max
mmin

t mt) (cid:9), with probability at least 1−c (p(cid:48))−3 ((cid:80)

t mt)−

• For each node r ∈ V , the solution of the M-estimation problem (14) is unique

• Moreover, for each node r ∈ V the M-estimation problem recovers the true

neighbourhood exactly.

where mmax = max

mt, mmin = min

mt , p(cid:48) = max(n, p).

t

t

The proof of Theorem 2 follows along similar lines to the sparsistency proof in
Yang et al. [2014], albeit with a subtler analysis to support general vector-spaces. It is
based on the primal dual witness proof technique and relies on the previous results. We
refer the interested reader to the supplementary material for additional details regarding
the proofs.

5 Experiments

We demonstrate the effectiveness of our algorithm on both synthetic data and a real-
world dataset of over four million foods logged on the popular diet app, MyFitnessPal.

5.1 Synthetic experiments

The synthetic experiments were run on a vector-space mixed MRF consisting of eight
Bernoulli, eight gamma (with unknown shape and rate), eight Gaussian (with un-
known mean and variance), and eight Dirichlet (k=3) nodes. The choice of these
node-conditional distributions is meant to highlight the ability of VS-MRFs to model
many different types of distributions. Speciﬁcally, the Bernoulli represents a univari-
ate, uni-parameter distribution that would still be possible to incorporate into existing
mixed models. The gamma and Gaussian distributions are both multi-parameter, uni-
variate distributions which would have required ﬁxing one parameter (e.g. ﬁxing the
Gaussians’ variances) to be compatible with previous approaches. Finally, the Dirich-
let distribution is multi-parameter and multivariate, thereby making VS-MRFs truly
unique in their ability to model this joint distribution.

For each experiment, we conducted 30 independent trials by generating random
weights and sampling via Gibbs sampling with a burn-in of 2000 and thinning step
size of 10. We consider two different sparsity scenarios: high (90% edge sparsity, 50%
intra-edge parameter sparsity) and low (50% edge sparsity, 10% intra-edge parameter

11

Figure 1: ROC curves for our synthetic experiments. The top left and bottom left plots
show both edge as well as within-edge-parameter recovery performance respectively,
for graphs with a high degree of sparsity. The two right plots show the same per-
formance measures, but for graphs with a relatively low degree of sparsity. The low
sparsity scenario is more challenging, requiring more data to recover the majority of
the graph.

sparsity). Edge recovery capability is examined by ﬁxing λ2 to a small value and vary-
ing λ1 over a grid of values in the range [0.0001, 0.5]; parameter recovery is examined
analogously by ﬁxing λ1 and varying λ2. We use AND graph stitching and measure
the true positive rate (TPR) and false positive rate (FPR) as the number of samples
increases from 100 to 25K.

Figure 5 shows the ROC curves at both the edge and parameter levels. The results
demonstrate that our algorithm improves well as the dataset size scales. They also
illustrate that graphs with a higher degree of sparsity are easier to recover with fewer
samples. In both the high and low sparsity graphs, the algorithm is better able to recover
the coarse-grained edge structure than the more ﬁne-grained within-edge parameter
structure, though both improve favourably with the size of the data.

5.2 MyFitnessPal Food Dataset

MyFitnessPal2 (MFP) is one of the largest diet-tracking apps in the world, with over
80M users worldwide. MFP has a vast crowd-sourced database of food data, where
each food entry contains a description, such as “Trader Joe’s Organic Carrots,” and a
vector of sixteen macro- and micro-nutrients, such as fat and vitamin C.

2http://myfitnesspal.com

12

Figure 2: The top 100 edges in the MyFitnessPal food graph. Purple rectangular
nodes correspond to macro- and micro-nutrients; green oval nodes correspond to food
description terms. Edge color is determined by the approximate effect of the edge on
the means of the node-conditionals: darker, blue edges represent lower means; brighter,
orange edges represent higher means; thickness corresponds to the norm of the edge
weight.

We treat these foods entries as random vectors with an underlying VS-MRF dis-
tribution, which we learn treating the food entries in the database as samples from the
underlying VS-MRF distribution. The text descriptions are tokenized, resulting in a
dictionary of approximately 2650 words; we use a Bernoulli distribution to model the
conditional distribution of each word. The conditional distribution of each nutrient
(on a per-calorie basis) is generally gamma distributed, but contains spikes at zero3
and large outlier values.4 The gamma distribution is undeﬁned at zero, and the outlier
values can result in numerical instability during learning, which thus suggests using
a distribution other than the vanilla gamma distribution. Such zero-inﬂated data are
common in many biostatistics applications, and are typically modeled via a mixture
model density of the form p(Z) = π δ0 + (1 − π) g(z), where δ0 is the dirac delta at
zero, and g(z) is the density of the non-zero-valued data. Unfortunately, such mixture
models are not generally representable as exponential families.

To overcome this, we introduce the following class of point-inﬂated exponential
family distributions. For any random variable Z ∈ Z, consider any exponential family
P (Z) = exp(ηT B(Z) + C(Z) − A(η)), with sufﬁcient statistics B(·), base measure
C(·), and log-normalization constant A(·). We consider an inﬂated variant of this
random variable, inﬂated at some value j; note that this could potentially lie outside the
domain Z, in which case the domain of the inﬂated random variable would become Z ∪
{j}. We then deﬁne the corresponding point-inﬂated exponential family distribution

3This is common in foods since many dishes are marketed as “fat free” or contain low nutrient density

4This occurs when foods contain few calories but a large amount of some micro-nutrient (e.g. multi-

(e.g. soda).

vitamins)

13

as:

Pinﬂ(Z) = exp (cid:8)η0I(Z = j) + ηT

1 B(z) + C(z) − Ainﬂ(η)(cid:9) ,

where Ainﬂ(η) is the log-normalization constant of the point-inﬂated distribution which
can be expressed in terms of the log-normalization constant A(·) of the uninﬂated ex-
ponential family distribution: Ainﬂ(η) = log (cid:0) exp{η0} − exp{ηT
1 B(j)I(j ∈ Z)} +
exp{A(η1)}(cid:1). Thus, as long as we have a closed form A(·) for the log-partition func-
tion of the base distribution, we can efﬁciently calculate Pinﬂ(Z). The deﬁnition also
permits an arbitrary number of inﬂated points by recursively specifying the base dis-
tribution as another point-inﬂated model. We model each of the MFP nutrients via
a two-point-inﬂated gamma distribution, with points at zero and a winsorized outlier
bucket.

Due to the size of the overall graph, presenting it in closer detail here is not fea-
sible. To give a qualitative perspective of the relationships captured by our algorithm,
we selected the top 100 edges in the MFP food graph by ranking the edges based
on their L2-norm. We then calculated their approximate contribution to the mean of
their corresponding node-conditionals to determine edge color and thickness. Figure
2 shows the results of this process, with edges that contribute positively colored in or-
ange and edges that contribute negatively colored in blue; edge thickness corresponds
to the magnitude of the contribution. A high-level view of the entire learned graph is
available in the supplementary materials.

Several interesting relationships can be discovered, even from just this small sub-
set of the overall model. For instance, the negative connection between “peanut” and
sodium may seem counter-intuitive, given the popularity of salted nuts, yet on inspec-
tion of the raw database it appears that indeed many peanut-based foods are actually
very low in sodium on a per-calorie basis. As another example, “chips” are often
thought of as a high-carb food, but the graph suggests that they actually tend to be a
bigger indicator of high fat. In general, we believe there is great potential for wide-
ranging future uses of VS-MRFs in nutrition and other scientiﬁc ﬁelds, with the MFP
case study only scratching the surface of what can be achieved.

6 Conclusion

We have presented vector-space MRFs as a ﬂexible and scalable approach to modeling
complex, heterogeneous data. In particular, we generalize the concept of mixed MRFs
to allow for node-conditional distributions to be distributed according to a generic ex-
ponential family distribution, that is potentially multi-parameter and even multivariate.
Our VS-MRF learning algorithm has reassuring sparsistency guarantees and was vali-
dated against a variety of synthetic experiments and a real-world case study. We believe
that the broad applicability of VS-MRFs will make them a valuable addition to the sci-
entiﬁc toolbox. All code for our VS-MRF implementation is publicly available.5

5https://github.com/tansey/vsmrfs

14

A Proof of Theorem 1

The proof follows the same lines as the proof in Yang et al. [2014]. Let us denote Q(X)
as log (P (X)/P (0)). Note that X = (X1, X2, · · · Xp) and each Xr belongs to a vector
space. Given any X, let us denote ¯Xs as ¯Xs = (X1, · · · , Xs−1, 0, Xs+1, · · · , Xp).
Consider the following expansion for Q(X):

Q(X) =
(cid:88)

t∈{1,··· ,p}

(cid:88)

+

t1,···tk∈
{1,··· ,p}

I[Xt (cid:54)= 0]Gt(Xt) + · · ·

I[Xt1 (cid:54)= 0, . . . Xtk (cid:54)= 0]Gt1...tk (Xt1 . . . Xtk )

where I is the indicator function which takes value 1 if its argument evaluates to true
and 0 otherwise.

Using some simple algebra and the deﬁnition Q(X) = log (P (X)/P (0)) we can

show that

exp (Q(X) − Q( ¯Xs)) = P (Xs|X1,··· ,Xs−1,Xs+1,··· ,Xp)
P (0|X1,··· ,Xs−1,Xs+1,··· ,Xp)

(24)

From (23) we have the following:

(Q(X) − Q( ¯Xs)) =



I[Xs (cid:54)= 0]

Gs(Xs) +

I[Xt (cid:54)= 0]Gs,t(Xs, Xt)

(cid:88)

t∈{1,··· ,p}\s

+

I[Xt2 (cid:54)= 0, . . . Xtk (cid:54)= 0]Gs,t2...tk (Xs, . . . Xtk )

(cid:88)

t2,···tk∈
{1,··· ,p}\s







Since the node conditional distribution follows the exponential family distribution

deﬁned in (6) we can show that:

P (0|X1,··· ,Xs−1,Xs+1,··· ,Xp) =

log P (Xs|X1,··· ,Xs−1,Xs+1,··· ,Xp)
(cid:104)Es(X−s), Bs(Xs) − Bs(0)(cid:105) + (Cs(Xs) − Cs(0))

Using (25) and (26) for left and right hand sides of (24) and setting Xt = 0 for all

t (cid:54)= s we obtain:

I[Xs (cid:54)= 0]Gs(Xs) = (cid:104)Es(0), Bs(Xs) − Bs(0)(cid:105)

+(Cs(Xs) − Cs(0))

(23)

(25)

(26)

15

Similarly setting Xr = 0 for all r /∈ {s, t} we obtain:

I[Xs (cid:54)= 0]Gs(Xs) + I[Xs (cid:54)= 0, Xt (cid:54)= 0]Gs,t(Xs, Xt) =
(cid:104)Es(0 · · · Xt · · · , 0), Bs(Xs) − Bs(0)(cid:105) + (Cs(Xs) − Cs(0))

Similarly, replacing Xs with Xt in (24) and setting Xr = 0 for all r /∈ {s, t} we

obtain:

I[Xt (cid:54)= 0]Gt(Xt) + I[Xs (cid:54)= 0, Xt (cid:54)= 0]Gs,t(Xs, Xt) =
(cid:104)Et(0 · · · Xs · · · , 0), Bt(Xt) − Bt(0)(cid:105) + (Ct(Xt) − Ct(0))

From the above three equations we arrive at the following equality:

(cid:104)Es(0 · · · Xt · · · , 0) − Es(0), Bs(Xs) − Bs(0)(cid:105) =
(cid:104)Et(0 · · · Xs · · · , 0) − Et(0), Bt(Xt) − Bt(0)(cid:105)

(27)

The above equality should hold for the node conditional distributions to be consistent
with the joint MRF distribution over X with respect to graph G. So we need to ﬁnd the
form of Er() that satisﬁes the above equation. Omitting zero vectors for clarity from
(27), we get the following:

(cid:104)Et(Xs), Bt(Xt)(cid:105)

= (cid:104)Es(Xt), Bs(Xs)(cid:105)

(cid:88)

j

Etj(Xs)Btj(Xt) =

Esl(Xt)Bsl(Xs)

(cid:88)

l

We rewrite the natural parameter functions as

Etj(Xs) =

θsl;tjBsl(Xs) + Btj(Xs)

Esl(Xt) =

θsl;tjBtj(Xt) + Bsl(Xt)

(cid:88)

(cid:88)

l

j

where ∀j Btj(Xs) are functions in the Hilbert space Hs orthogonal to the span of
functions Bs(Xs), and ∀j Bsl(Xt) are functions in the Hilbert space Ht orthogonal to
the span of functions Bt(Xt); and θsl;tj, θsl;tj are scalars. Combining (28) and (29),
we get

θsl;tjBsl(Xs)Btj(Xt) +

Btj(Xs)Btj(Xt)

=

θsl;tjBsl(Xs)Btj(Xt) +

Bsl(Xt)Bsl(Xs)

(cid:88)

(cid:88)

j
(cid:88)

l
(cid:88)

l

j

(cid:88)

j
(cid:88)

l

(28)

(29)

(30)

16

Rearranging terms in the above equation gives us the following equation:

(θsl;tj − θsl;tj)Bsl(Xs) + Btj(Xs)

Btj(Xt)

(cid:33)

(cid:32)

(cid:88)

(cid:88)

j
=

(cid:88)

l
Bsl(Xs)Bsl(Xt)

l

(31)

However, since ∀l Bsl(Xt) is orthogonal to Bt(Xt), the left and right hand sides

of the above equation are equal to 0, which leads us to the following equations.

(cid:88)

Bsl(Xs)Bsl(Xt) = 0

l

j

(cid:32)

(cid:88)

(cid:88)

l

(θsl;tj − θsl;tj)Bsl(Xs) + Btj(Xs)

Btj(Xt) = 0

(cid:33)

(32)

However since we assumed that the sufﬁcient statistics are minimal we get ∀l Bsl(Xt) =
0 from the ﬁrst equality and ∀j, l θsl;tj = θsl;tj, Btj(Xs) = 0 from the second equality.
Hence from (29), we obtain Es(Xt) = θst(Bt(Xt) − Bt(0)) and Et(Xs) =
θT
st(Bs(Xs) − Bs(0)) where θst is a matrix formed by the scalars θsl;tj such that
(θst)lj = θsl;tj and:

I[Xs (cid:54)= 0, Xt (cid:54)= 0]Gs,t(Xs, Xt) =
(Bt(Xt) − Bt(0))T θT

st(Bs(Xs) − Bs(0))

(33)

By extending this argument to higher order factors we can show that the natural param-
eters are required to be in the form speciﬁed by (7).

B Proof of Sparsistency

Before proving the sparsistency result, we will show that the sufﬁcient statistics Br(Xr)
are well behaved. Recall that Bri(Xr) indicates ith component of the vector Br(Xr).
We set the convention that whenever a variable has the subscript \r attached we will
be referring to the set of indexes {(t, j, k) : θrj;tk ∈ θr·, t (cid:54)= r}.

Proposition 1. Let {X (j)}n

j=1 have joint distribution as in (10), then,



P



1
n

n
(cid:88)

(cid:16)

j=1

(cid:16)

Bri

X (j)
r

(cid:17)(cid:17)2



(cid:18)

≥ δ

 ≤ exp

−n

(cid:19)

δ2
4k2
h

(34)

for δ ≤ min (cid:8)2 kv

3 , kh + kv

(cid:9).

17

(35)

(36)

(37)

Proof. It is clear from Taylor Series expansion and assumption 4 that

(cid:104)

exp

log E
log (cid:82)
⊗s∈[p]Xs
(cid:42)

(cid:16)

tBri (Xr)2(cid:17)(cid:105)
(cid:110)
tBri (Xr)2 +
exp

=

(cid:43)

(cid:80)
s∈V

Bs (Xs) , θ∗

θ∗
stBt (Xt)

+

s + (cid:80)
t∈N (r)
(cid:27)

v (dx)

Cs (Xs) − A(θ∗)

(cid:80)
s∈V
= ¯Ar,i (η; θ) (t; θ∗) − ¯Ar,i (η; θ) (0; θ∗)
≤ t ∂ ¯Ar,i(η;θ)
≤ t kv + t2

∂2 ¯Ar,i(η;θ)
∂η2

(0) + t2
2

(ut)

∂η

2 kh

(cid:18)

P

1
n

exp

exp

(cid:16)

(cid:17)2

(cid:80)n

X (j)
r

j=1 Bri
−nδt + n kvt + t2
(cid:17)
−n δ2
4k2
h

(cid:16)

(cid:16)

(cid:19)

≥ δ

≤
(cid:17)

2 Khn

≤

where u ∈ [0, 1]

Therefore, by the standard Chernoff bounding technique, for t ≤ 1, it follows that

for δ ≤ min (cid:8)2 kv

3 , kh + kv

(cid:9).

Proposition 2. Let X be a random vector with the distribution speciﬁed in (10). Then,
for any positive constant δ and some constant c > 0

(cid:16)

P

|Bri(Xr)| ≥ δlog(η)

(cid:17)

≤ cη−δ

Proof. Let ¯v be a unit vector with the same dimensions as θ∗
entry, corresponding to the sufﬁcient statistic Bri(Xr). Then we can write log
as:

r· and exactly one non-zero

(cid:16)

E[exp(Bri(Xr))]

(cid:17)

(cid:16)

(cid:17)

log

E[exp(Bri(Xr))]

= A(θ∗ + ¯v) − A(θ∗)

By Taylor series expansion, for some u ∈ [0, 1], we can rewrite last equation as

A(θ∗ + ¯v) − A(θ∗) =∇A(θ∗).¯v +

¯vT ∇2A(θ∗ + u ¯v)¯v

1
2

=E[Bri(Xr)](cid:107)¯v(cid:107)2

+

1
2

∂2A(θ∗ + u ¯v)
∂θ2
ri

(cid:107)¯v(cid:107)2
2

18

Using Assumption 4 we get the inequality :

A(θ∗ + ¯v) − A(θ∗) ≤ km +

kh

1
2

Now, by using Chernoff bound, for any positive constant a, we get P (Bri(Xr) ≥ a) ≤
exp(−a + km + 1

2 kh). By setting a = δlog(η) it follows that

P (Bri(Xr) ≥ δlog(η)) ≤ exp(−δlog(η) + km +

kh) ≤ cη−δ

1
2

where c = exp(km + 1

2 kh)

The proof of Sparsistency is based on the primal dual witness proof technique. First

note that the optimality condition of (14), can be written as:

∇(cid:96)(ˆθr·; D) + λ1

νrt ˆZ1,rt + λ2 ˆZ2 = 0

(38)

(cid:88)

√

t:r(cid:54)=t

where ˆZ1,rt ∈ ∂ (cid:107) ˆθrt (cid:107)2, ˆZ2 ∈ ∂ (cid:107) ˆθ\r (cid:107)1 and we denote ˆZ =
, where
ˆZ1 = { ˆZ1,rt}t∈V \r. And sub-gradients ˆZ1, ˆZ2 should satisfy the following conditions:

(cid:16) ˆZ1, ˆZ2

(cid:17)

∀i ( ˆZ2)i = sign

(ˆθr·)i

if (ˆθr·)i (cid:54)= 0

(cid:16)

(cid:17)

∀t ˆZ1,rt =

|( ˆZ2)i| ≤ 1 otherwise

ˆθrt
||ˆθrt||2
(cid:12)
(cid:12)
ˆZ1,rt
(cid:12)
(cid:12)
(cid:12)
(cid:12)

if ˆθrt (cid:54)= 0
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)2
(cid:12)

≤ 1 otherwise

(39)

Note that we can think of ˆZ1 and ˆZ2 as dual variables by appealing to Lagrangian
theory. The next lemma shows that graph structure recovery is guaranteed if the dual
is strictly feasible.

(cid:16)ˆθr·, ˆZ
for (14) such that
< 1. Then, any optimal solution ˜θr· must satisfy

(cid:17)

< 1 and

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)∞,2
(cid:12)

Lemma 1. Suppose that there exists a primal-dual pair
(cid:12)
(cid:12)
ˆZ1,Sc
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:16)˜θr·
(cid:17)
then ˆθ\r is the unique optimal solution.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)∞
(cid:12)

ˆZ2,Sc

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Sc

= 0. Moreover, if the Hessian sub-matrix [∇2(cid:96)(ˆθr·)]SS is positive deﬁnite

Proof. First, note that by Cauchy−Schwarz’s and Holder’s inequalities

(cid:104) ˆZ1,rt, ˜θrt(cid:105) ≤(cid:107) ˜θrt (cid:107)2 and(cid:104) ˆZ2, ˜θ\r(cid:105) ≤(cid:107) ˜θ\r (cid:107)1 .

(40)

19

But from (38) and the primal optimality of ˆθr· and ˜θr· for (14),

√

(cid:96)

(cid:17)

(cid:16)˜θr·
≥ min
(cid:16)ˆθr·
(cid:16)˜θr·

= (cid:96)

= (cid:96)

+ (cid:80)
t(cid:54)=r λ1
(cid:96) (θr·) + (cid:80)
(cid:17)
+ (cid:80)
+ (cid:80)

(cid:17)

θ

t(cid:54)=r λ1
t(cid:54)=r λ1

√

t(cid:54)=r λ1
√

νrt(cid:104) ˆZ1,rt, ˜θrt(cid:105) + λ2(cid:104) ˆZ2, ˜θ\r(cid:105)

νrt(cid:104) ˆZ1,rt, θrt(cid:105) + λ2(cid:104) ˆZ2, θ\r(cid:105)

√

νrt(cid:104) ˆZ1,rt, ˆθrt(cid:105) + λ2(cid:104) ˆZ2, ˆθ\r(cid:105)
(cid:12)
(cid:12)
˜θ\r
(cid:12)
(cid:12)
νrt
(cid:12)
(cid:12)

˜θrt

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)1
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)2
(cid:12)

(41)

hence, combining with (40) with (41) it follows that (cid:80)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)1
(cid:12)

t(cid:54)=r λ1

t(cid:54)=r λ1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
νrt
(cid:12)1
(cid:12)
νrt(cid:104) ˆZ1,rt, ˆθrt(cid:105)+λ2(cid:104) ˆZ2, ˆθ\r(cid:105). The result

t(cid:54)=r λ1

= (cid:80)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)2
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)2
(cid:12)

˜θ\r

ˆθ\r

ˆθrt

˜θrt

νrt

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

√

√

√

+

+

= (cid:80)
follows.

If the Hessian sub-matrix is positive deﬁnite for the restricted problem then the

problem is strictly convex and has a unique solution.

Based on the above lemma, we prove sparsistency theorem by constructing a primal-

dual witness (ˆθr·, ˆZ) with the following steps:

1. Set (ˆθr·)S = argmin((θr·)S ,0)(cid:96) ((θr·)S ; D) +λ1
2. For t ∈ S, we deﬁne ˆZ1,rt = θrt
(cid:107)θrt(cid:107)2

√

(cid:80)

t∈S

νrt ||θrt||2 + λ2 ||(θr·)S||1

and then construct ˆZ2,S by the stationary

condition.

3. Set (ˆθr·)Sc = 0

4. Set ˆZ2,Sc such that

ˆZ2,Sc

< 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)∞
(cid:12)

5. Set ˆZ1,Sc such that condition (38) is satisﬁed.

6. The ﬁnal step consists of showing, that the following conditions are satisﬁed:

(a) strict dual feasibility : the condition in Lemma 1 holds with high probabil-

ity

(b) correct neighbourhood recovery: the primal-dual pair speciﬁes the neigh-

bourhood of r, with high probability

We begin by proving some key lemmas that are key to our main theorem. The

sub-gradient optimality condition (38) can be rewritten as:

∇(cid:96)(ˆθr·; D) − ∇(cid:96)(θ∗

r·; D) = W n − λ1

νrt ˆZ1,rt − λ2 ˆZ2

(42)

(cid:88)

√

t:r(cid:54)=t

where W n = −∇(cid:96)(θ∗
value theorem coordinate wise to (42), we get:

r·; D) and θ∗

r· is the true model parameter. By applying mean-

∇2(cid:96)(θ∗

r·; D)[ˆθr· − θ∗

r·] = W n − λ1

(cid:80)
−λ2 ˆZ2 + Rn

t:r(cid:54)=t

√

νrt ˆZ1,rt

(43)

20

r·; D)]T

where Rn is the remainder term after applying mean-value theorem:Rn
∇2(cid:96)(¯θj
r· on the line between ˆθr· and θ∗
denoting the j-th row of matrix. The following lemma controls the score term W n
νrt, p(cid:48) = max(n, p). Assume that

Lemma 2. Recall νr max = max

j = [∇2(cid:96)(θ∗
r·, and with [.]T
j

r·) for some ¯θj

νrt , νr min = min

j (ˆθr· − θ∗

r·; D)−

t

(cid:113)

t

8(2−α)
α

k1(n, p) k4
√

λ1 + λ2 ≤ 4(2−α)
√

α

νr min

νr maxlog(pνr max)
nνr min
k1(n, p) k2(n, p) k4

≤

νr max

for some constant k4 ≤ min (cid:8)2 kv

3 , kh + kv

(cid:9) and suppose also that n ≥ 8 k2
h
k2
4

log ((cid:80)

t mt)

(cid:16)

\r(cid:107)∞,2 > α

2−α

νr min (λ1+λ2)
4

√

(cid:17)

≤

then,

(cid:107)W n
P
1 − c1p(cid:48)−3 ((cid:80)
Proof. Deﬁne W n
t = −∇θrt(cid:96) (θ∗
to parameter θrj;tk. Note that W n

t mt) − exp (−c2n) − exp (−c3n)
r·; D). Let W n
t,jk = 1
n

t,jk be the element in W n

t,jk where

t=1 V i

(cid:80)n

t corresponding

V i
t,jk = Brj

(cid:16)

(cid:17)

X (i)
r
(cid:16)

Btk
r + (cid:80)
θ∗

(cid:16)

(cid:17)

X (i)
−
t
s∈V \r θ∗

∇θrj;tk Ar

rsBs

(cid:16)

X (i)
s

(cid:17)(cid:17)

Btk

(cid:17)

(cid:16)

X (i)
t

so for t(cid:48) ∈ R

(44)

(45)

(cid:16)

X (i)
s

(cid:17)(cid:17)

(cid:17) (cid:21)

(cid:16)

X (i)
t

Btk
(cid:17)

(cid:16)

X (i)
r

Br

θ∗
rsBs

(cid:16)

X (i)
s

(cid:17)

(cid:16)

(cid:104)
exp

E
(cid:90)

exp

X (i)
r

(cid:17)

t(cid:48)V i
t,jk
(cid:20)
(cid:26)
Brj

t(cid:48)

|X (i)
V \r
(cid:16)

X (i)
r

(cid:105)

=
(cid:17)

−∇θrj;tk Ar
(cid:17)
(cid:16)

X (i)
r

+C

(cid:16)

r + (cid:80)
s∈V \r θ∗
θ∗
(cid:16)
X (i)
r

r Br

+ θ∗

(cid:17)

(cid:17)

(cid:16)

X (i)
t

Btk

rsBs
+ (cid:80)
s∈V \r
(cid:33) (cid:27)
(cid:17)

θ∗
rsBs

(cid:16)

X (i)
s

dXr

−Ar

r + (cid:80)
θ∗
(cid:32)

s∈V \r

(cid:26)

= exp

Ar

r + t(cid:48)Btk
θ∗

(cid:17)

(cid:16)

X (i)
t

(cid:32)

(cid:32)

θ∗
rsBs

(cid:16)

X (i)
s

(cid:33)

(cid:17)

+ (cid:80)
s∈V \r

(cid:33)

−Ar

r + (cid:80)
θ∗
s∈V \r
(cid:32)

θ∗
rsBs

(cid:16)
X (i)
s

(cid:17)

−∇θrj;tk Ar
(cid:26) ∇2

= exp

r + (cid:80)
θ∗

θ∗
rsBs

(cid:33)

(cid:17)

(cid:16)

X (i)
s

s∈V \r
Ar(c)

θrj;tk ,θrj;tk

2

(cid:17)2

(cid:16)

X (i)
t

t(cid:48)2

Btk

t(cid:48)Btk
(cid:27)

(cid:17) (cid:27)

(cid:16)

X (i)
t

where c = θ∗

r + (cid:80)

s(cid:54)=r θ∗

rsBs

(cid:0)X i

s

(cid:1) + v1t(cid:48)Btk

(cid:17)

(cid:16)

X (i)
t

for some v1 ∈ [0, 1]. There-

fore,

21

Next lets deﬁne event ε1 = (cid:8) maxi,t (cid:107)Bt

Proposition 2 we get P (εc
Assumption 5 implies that

1) ≤ c1np(cid:48)−4 ((cid:80)

(cid:16)

(cid:17)

X (i)
t
t mt) ≤ c1p(cid:48)−3 ((cid:80)

(cid:107)∞ ≤ 4 logp(cid:48)(cid:9). Then, from
t mt). If t(cid:48) ≤ k2 (n, p),

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

logE

(cid:104)
exp (cid:0)t(cid:48)V i

t,jk

(cid:1) |X (i)

V \r

(cid:105)

=

1
2

∇2

θrj;tk,θrj;tk

Ar(c)Btk

(cid:17)2

(cid:16)

X (i)
t

t(cid:48)2

n
(cid:88)

(cid:104)

logE

1
n

exp (cid:0)t(cid:48)V i

t,jk

(cid:1) |X (i)

V \r

(cid:105)

≤

i=1
k1(n,p)
2

1
n

(cid:80)n

i=1 Btk

(cid:17)2

(cid:16)

X (i)
t

t(cid:48)2

Now, lets deﬁne event ε2 = (cid:8)max

≤ k4
min{2kv/3, kh+kv}. Then, by proposition (1) we obtain that if n ≥ 8 k2
h
k2
4
:

X (i)
t

(cid:80)n

Btj

i=1

1
n

t,j

(cid:16)

(cid:16)

(cid:17)(cid:17)2

(cid:9) where k4 ≤
log((cid:80)

t∈V mt)

P (εc

2) ≤ exp

−n

+ log

mt

≤ exp (−n c2)

(46)

(cid:32)

(cid:32)

(cid:33)(cid:33)

k2
4
4k2
h

(cid:88)

t∈V

Therefore, for t(cid:48) ≤ k2(n, p),

1
n

n
(cid:88)

i=1

(cid:104)

logE

exp (cid:0)t(cid:48)V i

t,jk

(cid:1) |X (i)

V \r

(cid:105)

≤

k1(n, p)k4t(cid:48)2
2

Hence, by the standard Chernoff bound technique, for t(cid:48) ≤ k2(n, p)

(cid:32)

P

2exp

n
(cid:88)

1
n
(cid:16)

|V i

t,jk| > δ | ε1, ε2
(cid:17)(cid:17)

i=1
n

(cid:16) k1(n,p)k4t(cid:48) 2
2

− t(cid:48)δ

(cid:33)

≤

Setting t(cid:48) =

, for δ ≤ k1(n, p) k2(n, p) k4, we arrive to:

(cid:33)

|V i

t,jk| > δ | ε1, ε2

≤ 2 exp

(cid:18) −nδ2

(cid:19)

2k1(n, p)k4

λ1+λ2
√

mr mt

≤ k1(n, p) k2(n, p) k4.

It then follows that δ =

δ
k1(n,p)k4
(cid:32)

n
(cid:88)

1
n

P

i=1

√

νr min

2−α
4
satisﬁes

Supposing that α
α

νr min

√

λ1+λ2
√

2−α

4

mr mt

(47)

(48)

(49)

22

(50)

(51)

(53)

(54)

(55)

(cid:32)

P

2 exp

n
(cid:88)

|V i

1
n
i=1
(cid:16) −α2
(2−α)2

t,jk| >

√

α

νr min

2 − α

λ1 + λ2
√
mr mt
4

(cid:33)

| ε1, ε2

≤

νr min n (λ1+λ2)2
32 k1(n,p) k4 mr mt

(cid:17)

Form which, we obtain the following using union bound

√

√

(cid:16)

(cid:16)

P

P

(cid:107)W n

(cid:107)W n

t (cid:107)2 > α
2−α
t (cid:107)∞ > α
2−α
(cid:16) −α2
(2−α)2

≤ 2exp

νr min (λ1+λ2)
4

|ε1, ε2

(cid:17)

≤
(cid:17)

√

νr min (λ1+λ2)
mr mt
4
νr min n (λ1+λ2)2
32 k1(n,p)k4 mr mt

|ε1, ε2

(cid:17)

+ log (νrt)

and hence,

P

√

(cid:16)
(cid:107)W n(cid:107)∞,2 > α
2−α
νr min n (λ1 + λ2)2
−α2
(2 − α)2
32 k1(n, p)k4 νr max

νr min (λ1+λ2)
4

(cid:32)

2 exp

(cid:17)

|ε1, ε2

≤

+ log (νr max) + logp

(cid:33)

(52)

Finally for λ1 + λ2 ≥ 8(2−α)

α

k1(n, p) k4

(cid:113)

νr maxlog(pνr max)
nνr min
(cid:17)

√

, we obtain

(cid:16)

P
c1p(cid:48)−3 ((cid:80)

(cid:107)W n(cid:107)∞,2 > α
2−α

νr min (λ1+λ2)
4
t mt) + exp (−c2n) + exp (−c3n)

≤

Lemma 3. Suppose that λ1+λ2 ≤
then,

C2
min
40 logp(cid:48) Dmax dr k3(n,p) ν2

r max

and (cid:107)W n

\r(cid:107)∞,2 ≤ (λ1+λ2) α

4 (2−α)

√

νr min

,

(cid:16)

(cid:107)(θ∗

P
≥ 1 − cp(cid:48)−3 ((cid:80)

r·)S − (ˆθr·)S(cid:107)∞,2 ≤ 5
t mt)

√

νrmax

Cmin

(cid:17)

(λ1 + λ2)

for some constant c > 0.

Proof. We deﬁne F (uS) as:

F (uS) = (cid:96) ((θ∗
+λ1

r·)S + uS; D) − (cid:96) ((θ∗

(cid:88)

√

r·)S ; D)
rt + urt(cid:107)2 − (cid:107)θ∗

rt(cid:107)2)

νrt ((cid:107)θ∗

t∈N (r)
+λ2 ((cid:107) (θ∗

r·)S + uS(cid:107)1 − (cid:107) (θ∗

r·)S (cid:107)1)

From the construction of ˆθr· it is clear that ˆus = (ˆθr·)S − (θ∗

r·)S minimizes F .
And since F (0) = 0, we have F (ˆus) ≤ 0. We now show that for some B > 0 with
||uS||∞,2 = B, we have F (uS) > 0. Using this and the fact that F is convex we can
then show that ||ˆuS||∞,2 ≤ B.

Let uS an arbitrary vector with ||uS||∞,2 = 5

(λ1 + λ2). Then, from the

√

νrmax

Cmin

Taylor Series expansion of log likelihood function in F , we have:

23

(56)

(57)

(58)

(59)

F (uS) = ∇(cid:96) ((θ∗

r·)S ; D)T uS

+uT
+λ1

S ∇2(cid:96) ((θ∗
r·)S + v uS) uS
√
(cid:88)
νrt ((cid:107)θ∗

rt + urt(cid:107)2 − (cid:107)θ∗

rt(cid:107)2)

t∈N (r)
+λ2 ((cid:107) (θ∗

r·)S + uS(cid:107)1 − (cid:107) (θ∗

r·)S (cid:107)1)

for some v ∈ [0, 1]

We now bound each of the terms in the right hand side of (56). From (51) and using

Cauchy-Schwarz inequality we obtain:

where the last inequality holds because α ∈ (0, 1]. Moreover, from triangle in-

equality we have:

r·)S ; D)T uS

(cid:12)
(cid:12)
(cid:12)∇(cid:96) ((θ∗
≤ (cid:107)∇(cid:96) ((θ∗
≤ (cid:107)∇(cid:96) ((θ∗
≤ α
2−α
= 5 νr max
4 Cmin

(cid:12)
(cid:12)
(cid:12)
r·)S ; D) (cid:107)∞ (cid:107)uS(cid:107)1
√
r·)S ; D) (cid:107)∞ dr
λ1+λ2
5
dr νr max
4
Cmin
dr (λ1 + λ2)2

νr max (cid:107)uS(cid:107)∞,2
(λ1 + λ2)

(cid:88)

√

λ1

t∈S
≥ −λ1

rt + urt(cid:107)2 − (cid:107)θ∗
νrt ((cid:107)θ∗
√

(cid:88)

νrt(cid:107)urt(cid:107)2

rt(cid:107)2)

t∈N (r)
√

≥ −λ1 dr
= − 5 dr νr max

νr max (cid:107)uS(cid:107)∞,2
λ1 (λ1 + λ2)

Cmin

r·)S + uS(cid:107)1 − (cid:107) (θ∗

r·)S (cid:107)1)

λ2 ((cid:107) (θ∗
≥ −λ2 (cid:107)uS(cid:107)1
≥ −λ2 (cid:107)uS(cid:107)1
≥ −λ2 dr
= − 5 νr max
Cmin

√

νr max (cid:107)uS(cid:107)∞,2
dr λ2 (λ1 + λ2)

Also,

On the other hand, by Taylor’s approximation of ∇2(cid:96), there exists αjk ∈ [0, 1] and

˜ui
jk between θ\r and θ\r + vuS such that
r·)S + v uS)(cid:1)
(cid:0)∇2(cid:96) ((θ∗
(cid:0)∇2(cid:96) ((θ∗
Λmin

r·)S + β uS)(cid:1)

Λmin
≥ min
β∈[0,1]
≥ Λmin (Qn

max
v∈[0,1]

max
(cid:107)y(cid:107)≤1

SS) −
(cid:40)

n
(cid:88)

1
n

(cid:88)

αjk v (cid:0)∇3Ar

(cid:0)˜ui

jk

(cid:1)(cid:1)

(60)

i=1

j,k,l,t,h,s,m,s(cid:48),m(cid:48)

jkl

(cid:41)

urt;lh Bth

(cid:0)X i

t

(cid:1) ys,m,j Bsm

(cid:0)X i

s

(cid:1) Bs(cid:48)m(cid:48)

(cid:0)X i
s(cid:48)

(cid:1) ys(cid:48),m(cid:48),j(cid:48)

Consider the event ε1 as deﬁned in the previous proof. We know that P (ε1) ≥ 1 −

24

c1p(cid:48)−3 ((cid:80)

t mt). Conditioned on ε1 and using Assumption 5 we arrive to the following:

(cid:0)∇2(cid:96) ((θ∗

r·)S + v uS)(cid:1)
Λmin
≥ Cmin − 4logp(cid:48)(cid:107)uS(cid:107)1 Dmax νr maxk3 (n, p)
√
≥ Cmin − 4logp(cid:48)dr
≥ Cmin
2

νr max ||uS||∞,2 Dmax νr maxk3 (n, p)

(61)

where the last inequality holds for λ1 + λ2 ≤

C2
min
40 logp(cid:48) Dmax dr k3(n,p) ν2

.

r max

Finally using the above bounds we arrive at the following:

F (uS) ≥ dr νr max

(cid:18)

(λ1 + λ2)2

5
Cmin

1
4

(cid:19)

5
2

−1 −

+

> 0

(62)

Therefore

(cid:107)(θ∗

r·)S − (ˆθr·)S(cid:107)∞,2 ≤

(λ1 + λ2)

√

5

νrmax

Cmin

Lemma 4. Suppose that λ1 + λ2 ≤ α
2−α

α (λ1+λ1)

νr min

√

4 (2−α)

, then,

√

νr min C2
min

400 ν

5
2
r maxlogp(cid:48) Dmax k3(n,p) dr

and (cid:107)W n

\r(cid:107)∞,2 ≤

P

(cid:18) (cid:107)Rn(cid:107)∞,2
λ1 + λ2

≤

(cid:19)

√

νr min
α
4 (2 − α)

≥ 1 − cp(cid:48)−3

(cid:32)

(cid:88)

(cid:33)

mt

t

for some constant c > 0.

j = (cid:2)∇2(cid:96)(θ∗
Proof. Recall that Rn
notes the j-th row of a matrix. Let us also refer to Rn
sponding to θrj;tk. Then,

r·; D) − ∇2(cid:96)(¯θj

r·; D)(cid:3)T

(cid:17)

(cid:16)ˆθr· − θ∗

j de-
t;jk to the cordinate of Rn corre-

where (cid:2) · (cid:3)T

r·

j

Rn

t;jk =

1
n

Btk(X i
t )

(cid:34)

n
(cid:88)

i=1

(cid:34)
∇2Ar


θ∗

r +

(cid:88)

s(cid:54)=r



rsBs(X i
θ∗
s)

 −

∇2Ar

(cid:16)¯θtjk

r + (cid:80)

¯θtjk
rs Bs(X i
s)

s(cid:54)=r

(cid:35)T

⊗ Bi
r·

(cid:16)ˆθr· − θ∗

r·

(cid:17)

(cid:35)

(cid:17)

j

with Bi

the notation (cid:104)θr·, Bi
obtain

r· the vector of sufﬁcient statistics evluate at the i-th sample. Intoroducing
(cid:1), from the mean value theorem we
(cid:0)X i
s

r·(cid:105) =: θr + (cid:80)

s(cid:54)=r θrsBs

∇2Ar;jl
−vi
j,l

(cid:0)(cid:104)θ∗
(cid:2)(cid:104)ˆθr· − θ∗

r·, Bi

r·(cid:105)(cid:1) − ∇2Ar;jl
(cid:1)
r·(cid:105)(cid:3) (cid:0)∇3Ar

r·, Bi

(cid:16)

(cid:17)

(cid:104)¯θtjk
(cid:16)

r· , Bi
(cid:104)¯¯θi;tjk

r·(cid:105)
, Bi

r·

=
(cid:17)

r·(cid:105)

jl:

Therefore, combining (64) with (65) and using basic properties of krockner product we

(63)

(64)

(65)

25

ontain that

which implies

(cid:12)
(cid:12)
(cid:12)Rn

t;jk

n
(cid:88)

n

t )(cid:12)
(cid:12)
(cid:12) νr maxk3 (n, p) Dmax (cid:107)ˆθr· − θ∗
(cid:12)Btk(X i

(cid:12)
(cid:12) ≤ 1
(cid:12)
≤ 4 logp(cid:48) νr maxk3 (n, p) Dmax (cid:107)ˆθr· − θ∗

i=1

r·(cid:107)2
2

r·(cid:107)2
2

(cid:107)Rn
t (cid:107)∞,2 ≤
√
νr max logp(cid:48) νr maxk3 (n, p) Dmax (cid:107)ˆθr· − θ∗
4
√
25 dr νrmax
νr max logp(cid:48) νr maxk3 (n, p) Dmax
4
C2
min
λ1+λ2
4

νr min

2−α

√

α

r·(cid:107)2
2 ≤
λ2
1 ≤

with probbility at least 1 − cp(cid:48)−3 ((cid:80)

t mt).

We now prove theorem 2 using lemmas 2-4. Recalling that Qn = ∇2(cid:96)(θ∗

r·; D) and
the fact that we have set (ˆθr·)Sc = 0 in our primal-dual construction, we can rewrite
condition (43) as the following equations:

Qn
W n

ScS[(ˆθr·)S − (θ∗
(cid:80)
Sc − λ1

t /∈N (r)

r·)S] =
√

νrt ˆZ1,rt − λ2 ˆZ2,Sc + Rn
Sc

Qn
W n

SS[(ˆθr·)S − (θ∗
(cid:80)
S − λ1

t∈N (r)

r·)S] =
√

νrt ˆZ1,rt − λ2 ˆZ2,S + Rn
S

Since the matrix Qn

SS is invertible, the conditions (68) and (69) can be rewritten as

:

Qn

ScS(Qn

SS)−1[W n

W n

Sc − λ1

(cid:80)

t /∈N (r)

S − λ1
√

t∈N (r)
νrt ˆZ1,rt − λ2 ˆZ2,Sc + Rn
Sc

(cid:88)

√

νrt ˆZ1,rt − λ2 ˆZ2,S + Rn

S] =

Rearranging yields the following condition:

√

(cid:80)
λ1
t /∈N (r)
Sc + Rn
W n
λ2 ˆZ2,Sc + Qn

νrt ˆZ1,rt =
ScS(Qn

Sc − Qn

ScS(Qn

SS)−1[λ1

SS)−1[W n

S + Rn
S]−
√
t∈N (r)

(cid:80)

νrt ˆZ1,rt + λ2 ˆZ2,S]

Strict Dual Feasibility: we now show that for the dual sub-vector ˆZ1,Sc, we have
< 1. We get the following equation from 71, by applying triangle inequal-

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)∞,2
(cid:12)

(cid:12)
(cid:12)
ˆZ1,Sc
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ity:

(66)

(67)

(68)

(69)

(70)

(71)

26

νr min
(cid:80)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ˆZ1,Sc
√

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)∞,2
(cid:12)
νrt ˆZ1,rt

t /∈N (r)

√

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

λ1

λ1
(cid:104)

√

||W n||∞,2 + ||Rn||∞,2
+λ2
(cid:12)
+ (cid:12)
(cid:12)Qn
(cid:12)

νr max
ScS(Qn

(cid:12)
SS)−1(cid:12)
(cid:12)∞,2
(cid:12)

≤
(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤
(cid:12)∞,2
(cid:12)
(cid:105) (cid:16)
(cid:12)
1 + (cid:12)
(cid:12)Qn
(cid:12)

(cid:2)(λ1 + λ2)

dr νr max

(cid:3)

√

ScS(Qn

(cid:12)
SS)−1(cid:12)
(cid:12)∞,2
(cid:12)

√

(cid:17)

dr

(72)

where νr min = min

νrt , νr max = max

νrt and dr = |N (r)| Using mutual incoher-

ence bound 2 on the above equation gives us:

t

t

(cid:12)
(cid:12)
ˆZ1,Sc
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
√
νr min
√
√

λ1
+ λ2
λ1

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)∞,2
(cid:12)
(cid:104)
||W n||∞,2 + ||Rn||∞,2
(2 − α)
√
(cid:104)
(cid:12)
SS)−1(cid:12)
(cid:12)∞,2
(cid:12)

(cid:12)
1 + (cid:12)
(cid:12)Qn
(cid:12)

ScS(Qn

νr max
νr min

(cid:105)

(cid:16) λ1
λ2

dr

(cid:17)(cid:105)

+ 1

Using the previous lemmas we obtain the following:

(cid:12)
(cid:12)
ˆZ1,Sc
(cid:12)
(cid:12)
(cid:12)
(cid:12)
√
+ λ2
√
λ1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)∞,2
(cid:12)
(cid:104)
νr max
νr min

≤ 1
2λ1
1 + mmin
mmax

[α (λ1 + λ2)]

(1 − α)

(cid:16) λ1
λ2

(cid:17)(cid:105)

+ 1

(cid:18)

If λ2 <

α
2−α+2

√
√

νr max
νr min

(cid:19)

λ1, then,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ˆZ1,Sc

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)∞,2
(cid:12)

< 1

(73)

(74)

(75)

We have shown that the dual is strictly feasible with high probability and also the
solution is unique. And hence based on Lemma 1 the method correctly excludes all
edges not in the set of edges.

Correct Neighbourhood Recovery: To show that all correct neighbours are recov-

ered, it sufﬁces to show that

(cid:107)(θ∗

r·)S − (ˆθr·)S(cid:107)∞,2 ≤

θmin
2

where θmin = mint∈V \r ||θrt||2.

Using Lemma 3 we can show the above inequality holds if θmin ≥ 10

√

νr max

Cmin

(λ1 + λ2)

C Full MyFitnessPal Graph

Figure 3 shows a high-level view of the entire VS-MRF learned from the MyFitnessPal
food database. The three macro-nutrients (fat, carbs, and protein) correspond to the

27

Figure 3: Full MRF learned from the MyFitnessPal food database. The hubs cor-
respond to point-inﬂated gamma nutrient nodes, with the three largest hubs being the
macro-nutrients (fat, carbs, and protein).

three largest hubs with the remaining nine micro-nutrients representing smaller hubs.

References

S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2009.

S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and
statistical learning via the alternating direction method of multipliers. Foundations
and Trends R(cid:13) in Machine Learning, 3(1):1–122, 2011.

J. Friedman, T. Hastie, and R. Tibshirani. A note on the group lasso and a sparse group
lasso. arXiv preprint arXiv:1001.0736, 2010. URL http://arxiv.org/abs/
1001.0736.

A. Jalali, P. Ravikumar, V. Vasuki, and S. Sanghavi. On learning discrete graphical

models using group-sparse regularization. AI STAT, 2011.

28

S. L. Lauritzen. Graphical models. Oxford University Press, 1996.

N. Parikh and S. Boyd. Proximal algorithms. Foundations and Trends in Optimization,

1(3):123–231, 2013.

N. Simon, J. Friedman, T. Hastie, and R. Tibshirani. A sparse-group lasso. Journal of

Computational and Graphical Statistics, 22(2):231–245, 2013.

E. B. Sudderth, A. T. Ihler, M. Isard, W. T. Freeman, and A. S. Willsky. Nonparametric

belief propagation. Communications of the ACM, 53(10):95–103, 2010.

D. Vats and J. M. Moura. Finding non-overlapping clusters for generalized inference
over graphical models. Signal Processing, IEEE Transactions on, 60(12):6368–
6381, 2012.

M. J. Wainwright. Sharp thresholds for noisy and high-dimensional recovery of spar-
sity using l1-constrained quadratic programming (lasso). IEEE Transactions on In-
formation Theory, pages 2183–2202, 2009.

M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and vari-
ational inference. Foundations and Trends R(cid:13) in Machine Learning, 1(1-2):1–305,
2008.

E. Yang, G. Allen, Z. Liu, and P. Ravikumar. Graphical models via generalized linear
models. In Advances in Neural Information Processing Systems, pages 1358–1366,
2012.

E. Yang, Y. Baker, P. Ravikumar, G. Allen, and Z. Liu. Mixed graphical models via
exponential families. Proceedings of the Seventeenth International Conference on
Artiﬁcial Intelligence and Statistics, pages 1042–1050, 2014.

29

5
1
0
2
 
y
a
M
 
9
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
7
1
1
5
0
.
5
0
5
1
:
v
i
X
r
a

Vector-Space Markov Random Fields via
Exponential Families

Wesley Tansey∗
Oscar Hernan Madrid Padilla†
Arun Sai Suggala‡
Pradeep Ravikumar§

Abstract

We present Vector-Space Markov Random Fields (VS-MRFs), a novel class of
undirected graphical models where each variable can belong to an arbitrary vector
space. VS-MRFs generalize a recent line of work on scalar-valued, uni-parameter
exponential family and mixed graphical models, thereby greatly broadening the
class of exponential families available (e.g., allowing multinomial and Dirichlet
distributions). Speciﬁcally, VS-MRFs are the joint graphical model distributions
where the node-conditional distributions belong to generic exponential families
with general vector space domains. We also present a sparsistent M -estimator for
learning our class of MRFs that recovers the correct set of edges with high proba-
bility. We validate our approach via a set of synthetic data experiments as well as
a real-world case study of over four million foods from the popular diet tracking
app MyFitnessPal. Our results demonstrate that our algorithm performs well em-
pirically and that VS-MRFs are capable of capturing and highlighting interesting
structure in complex, real-world data. All code for our algorithm is open source
and publicly available.

1

Introduction

Undirected graphical models, also known as Markov Random Fields (MRFs), are a
popular class of models for probability distributions over random vectors. Popular
parametric instances include Gaussian MRFs, Ising, and Potts models, but these are all
suited to speciﬁc data-types: Ising models for binary data, Gaussian MRFs for thin-
tailed continuous data, and so on. Conversely, when there is prior knowledge of the
graph structure but limited information otherwise, nonparametric approaches are avail-
able Sudderth et al. [2010]. A recent line of work has considered the challenge of
specifying classes of MRFs targeted to the data-types in the given application, when

∗Department of Computer Science, University of Texas at Austin, tansey@cs.utexas.edu (corre-

sponding author)
†Department

Statistics
oscar.madrid@utexas.edu

of

and Data

Sciences;

University

of

Texas

at Austin,

‡Department of Computer Science, University of Texas at Austin, arunsai@utexas.edu
§Department of Computer Science, University of Texas at Austin, pradeepr@cs.utexas.edu

1

the structure is unknown. For the speciﬁc case of homogeneous data, where each vari-
able in the random vector has the same data-type, Yang et al. [2012] proposed a general
subclass of homogeneous MRFs. In their construction, they imposed the restriction that
each variable conditioned on other variables belong to a shared exponential family dis-
tribution, and then performed a Hammersley-Clifford-like analysis to derive the corre-
sponding joint graphical model distribution, consistent with these node-conditional dis-
tributions. As they showed, even classical instances belong to this sub-class of MRFs;
for instance, with Gaussian MRFs and Ising models, the node-conditional distributions
follow univariate Gaussian and Bernoulli distributions respectively.

Yang et al. [2014] then proposed a class of mixed MRFs that extended this construc-
tion to allow for random vectors with variables belonging to different data types, and
allowing each node-conditional distribution to be drawn from a different univariate,
uni-parameter exponential family member (such as a Gaussian with known variance
or a Bernoulli distribution). This ﬂexibility in allowing for different univariate expo-
nential family distributions yielded a class of mixed MRFs over heterogeneous random
vectors that were capable of modeling a much wider class of distributions than was
previously feasible, opening up an entirely new suite of possible applications.

To summarize, the state of the art can specify MRFs over heterogeneous data-typed
random vectors, under the restriction that each variable conditioned on others belong to
a uni-parameter, univariate exponential family distribution. But in many applications,
such a restriction would be too onerous. For instance, a discrete random variable is
best modeled by a categorical distribution, but this is a multi-parameter exponential
family distribution, and does not satisfy the required restriction above. Other multi-
parameter exponential family distributions popular in machine learning include gamma
distributions with unknown shape parameter and Gaussian distributions with unknown
variance. Another restriction above is that the variables be scalar-valued; but in many
applications the random variables could belong to more general vector spaces, for ex-
ample a Dirichlet distribution.

As modern data modeling requirements evolve, extending MRFs beyond such re-
In this paper, we thus ex-
strictive paradigms is becoming increasingly important.
tend the above line of work in Yang et al. [2012, 2014]. As opposed to other ap-
proaches which merely cluster scalar variables Vats and Moura [2012], we allow node-
conditional distributions to belong to a generic exponential family with a general vector
space domain. We then perform a subtler Hammersley-Clifford-like analysis to derive
a novel class of vector-space MRFs (VS-MRFs) as joint distributions consistent with
these node-conditional distributions. This class of VS-MRFs provides support for the
many modelling requirements outlined above, and could thus greatly expand the po-
tential applicability of MRFs to new scientiﬁc analyses.

We also introduce an M -estimator for learning this class of VS-MRFs based on the
sparse group lasso, and show that it is sparsistent, and that it succeeds in recovering
the underlying edges of the graphical model. To solve the M -estimation problem, we
also provide a scalable optimization algorithm based on Alternating Direction Method
of Multipliers (ADMM) Boyd et al. [2011]. We validate our approach empirically via
synthetic experiments measuring performance across a variety of scenarios. We also
demonstrate the usefulness of VS-MRFs by modeling a real-world dataset of over four
million foods from the MyFitnessPal food database.

2

The remainder of this paper is organized as follows. Section 2 provides background
on mixed MRFs in the uni-parameter, univariate case. Section 3 details our general-
ization of the mixed MRF derivations to the vector-space case. Section 4 introduces
our M -estimator and derives its sparsistency statistical guarantees. Section 5 contains
our synthetic experiments and the MyFitnessPal case study. Finally, Section 6 presents
concluding remarks and potential future work.

2 Background: Scalar Mixed Graphical Models

Let X = (X1, X2, · · · Xp) be a p-dimensional random vector, where each variable Xr
has domain Xr. An undirected graphical model or a Markov Random Field (MRF) is
a family of joint distributions over the random vector X that is speciﬁed by a graph
G = (V, E), with nodes corresponding to each of the p random variables {Xr}p
r=1,
and edges that specify the factorization of the joint as:

P(X) ∝

(cid:89)

ψC(XC),

C∈C(G)

where C(G) is the set of fully connected subgraphs (or cliques) of the graph G, XC =
{Xs}s∈C denotes the subset of variables in the subset C ⊆ V , and {ψC(XC)}C∈C(G)
are clique-wise functions, each of which is a “local function” in that it only depend on
the variables in the corresponding clique, so that ψC(XC) only depends on the variable
subset XC.

Gaussian MRFs, Ising MRFs, etc. make particular parametric assumptions on these
clique-wise functions, but a key question is whether there exists a more ﬂexible speci-
ﬁcation of the form of these clique-wise functions that is targeted to the data-type and
other characteristics of the random vector X.

For the speciﬁc case where the variables are scalars, so that the domains Xr ⊆ R,
in a line of work, Yang et al. [2012, 2014] used the following construction to de-
rive a subclass of MRFs targeted to the random vector X. Suppose that for variables
Xr ∈ Xr, the following (single-parameter) univariate exponential family distribution
P (Xr) = exp{θrBr(Xr) + Cr(Xr) − Ar(θr)}, with natural parameter scalar θ, suf-
ﬁcient statistic scalar Br(Xr), base measure Cr(Xr) and log normalization constant
Ar(θ), serves as a suitable statistical model. Suppose that we use these univariate
distributions to specify conditional distributions:

P (Xr|X−r) = exp{ Er(X−r)Br(Xr)+

Cr(Xr) − Ar(X−r)}

,

(1)

where Er(·) is an arbitrary function of the rest of the variables X−r that serves as the
natural parameter. Would these node-conditional distributions for each node r ∈ V
be consistent with some joint distribution for some speciﬁcation of these functions
{Er(·)}r∈V ? Theorem 1 from Yang et al. [2014] shows that there does exist a unique

3

joint MRF distribution with the form:

P (X; θ) = exp

θrBr(Xr)

(cid:40)

(cid:88)

r∈V
(cid:88)

(cid:88)

+

r∈V

t∈N (r)

(cid:88)

+
(t1,...,tk)∈C
+ (cid:80)

θrtBt(Xt)Br(Xr) + . . .

θt1...tk (X)

Btj (Xtj )

k
(cid:89)

j=1

(cid:41)

r∈V Cr(Xr) − A(θ)

,

(2)

where A(θ) is the log-normalization constant. Their proof followed an analysis simi-
lar to the Hammersley-Clifford Theorem Lauritzen [1996], and entailed showing that
for a consistent joint, the only feasible conditional parameter functions Er(·) had the
following form:

Er(X−r) = θr +

θrtBt(Xt) + . . .

(cid:88)

t∈N (r)

(cid:88)

+

θrt2...tk (X)

Btj (Xtj )

t2,...,tk∈N (r)

k
(cid:89)

j=2

,

(3)

where θr·
neighbors of node r.

:= {θr, θrt, . . . , θrt2...tk } is a set of parameters, and N (r) is the set of

While their construction allows the speciﬁcation of targeted classes of graphical
models for heterogeneous random vectors, the conditional distribution of each variable
conditioned on the rest of the variables is assumed to be a single-parameter exponential
family distribution with a scalar sufﬁcient statistic and natural parameter. Furthermore,
their Hammersley-Clifford type analysis and sparsistency proofs relied crucially on that
assumption. However in the case of multi-parameter and multivariate distributions, the
sufﬁcient statistics are a vector; indeed the random variables need not be scalars at all
but could belong to a more general vector space. Could one construct classes of MRFs
for this more general, but prevalent, setting? In the next section, we answer in the
afﬁrmative, and present a generalization of mixed MRFs to the vector-space case, with
support for more general exponential families.

3 Generalization to the Vector-space Case

Let X = (X1, X2, · · · Xp) be a p-dimensional random vector, where each variable Xr
belongs to a vector space Xr. As in the scalar case, we will assume that a suitable
statistical model for variables Xr ∈ Xr is an exponential family distribution

P (Xr) = exp{

θrjBrj(Xr) + Cr(Xr) − Ar(θ)},

(4)

mr(cid:88)

j=1

4

with natural parameters {θrj}mr
j=1, base measure
Cr(Xr) and log normalization constant Ar(θ). We assume the sufﬁcient statistics Brj :
Xr (cid:55)→ R lie in some Hilbert space Hs, and moreover specify a minimal exponential
family so that:

j=1, and sufﬁcient statistics {Brj}mr

αjBrj(Xr) (cid:54)= c ,

(5)

mr(cid:88)

j=1

for any constant c and any vector α (cid:54)= 0. We note that even though the variables
{Xr} could lie in general vector spaces, the exponential family distribution above is
ﬁnite-dimensional. However, it has multiple parameters, which is the other facet that
distinguishes it from the single-parameter univariate setting of Yang et al. [2012, 2014].
We defer a generalization of our framework to inﬁnite-dimensional exponential fami-
lies to future work.

Suppose we use these general exponential family distributions to specify node-
conditional distributions of variables Xr conditioned on the rest of the random vari-
ables:

P (Xr|X−r) = exp{(cid:80)mr

j=1 Erj(X−r)Brj(Xr)

+Cr(Xr) − Ar(X−r)} ,

(6)

where {Erj(X−r)}mr
j=1 are arbitrary functions of the rest of the variables that serve
as natural parameters for the conditional distribution of Xr. As before, we ask the
question whether these node-conditional distributions can be consistent with some joint
distribution for some speciﬁcation of the parameter functions {Erj(X−r)}mr
j=1; the
following theorem addresses this very question.

Theorem 1. Let X = (X1, X2, . . . , Xp) be a p-dimensional random vector with node-
conditional distribution of each random vector Xr conditioned on the rest of random
variables as deﬁned in (6). These node-conditionals are consistent with a joint MRF
distribution over the random vector X, that is, Markov with respect to a graph G =
(V, E) with clique-set C, and with factors of size at most k, if and only if the functions
{Er()}r∈V specifying the node-conditional distributions have the form:

Eri(X−r) =θri +

θri;tjBtj(Xt) + . . .

(cid:88)

mt(cid:88)

t∈N (r)

j=1

(cid:88)

(cid:88)

+

t2,...,tk
∈N (r)

i2=1...mt2
...
ik=1...mtk

θri;...;tkik

Btj ij (Xtj )

k
(cid:89)

j=2

,

(7)

where θr· = {θri, θri;tj, θri;...;tkik } is a set of parameters, mt is the dimension of the
sufﬁcient statistic vector for the tth node-conditional distribution, and N (r) is the set of
neighbors of node r in graph G. The corresponding consistent joint MRF distribution
has the following form:

5

P (X|θ) = exp

θriBri(Xr) + . . .

(cid:40)

(cid:88)

mr(cid:88)

r∈V

i=1

(cid:88)

(cid:88)

+

t1,...,tk∈C

i1=1...mt1
...
ik=1...mtk

(cid:41)

Cr(Xr) − A(θ)

+

(cid:88)

r∈V

θt1i1;...;tkik

Btj ij (Xtj )

.

(8)

k
(cid:89)

j=1

We provide a Hammersley-Clifford type analysis as proof of this theorem in the
supplementary material, which however has subtleties not present in Yang et al. [2012,
2014], due to the arbitrary vector space domain of Xr, and the multiple parameters in
the exponential families, which consequently entailed leveraging the geometry of the
corresponding Hilbert spaces {Hs{s ∈ V } underlying the sufﬁcient statistics {Bsj}.

The above Theorem 1 provides us with a general class of vector-space MRFs (VS-
MRFs), where each variable could belong to more general vector space domains, and
whose conditional distributions are speciﬁed by more general ﬁnite-dimensional expo-
nential families. Consequently, many common distributions can be incorporated into
VS-MRFs that were previously unsupported or lacking in Yang et al. [2012, 2014]. For
instance, gamma and Gaussian nodes, though univariate, require vector-space param-
eters in order to be fully modeled. Additionally, multivariate distributions that were
impossible to use with previous methods, such as the multinomial and Dirichlet distri-
butions are now also available.

3.1 Pairwise conditional and joint distributions

Given the form of natural parameters in (7), the conditional distribution of a node Xr
given all other nodes X−r for the special case of pairwise MRFs (i.e. k = 2) has the
form

P (Xr|X−r, θr, θrt) = exp

θriBri(Xr)

(cid:88)

mr(cid:88)

mt(cid:88)

+

t∈N (r)

i=1

j=1

θri;tjBtj(Xt)Bri(Xr)

+Cr(Xr) − Ar(X−r, θr·)

,

(9)

= exp

Br(Xr), θr +

θrtBt(Xt)

(cid:42)








(cid:43)








+Cr(Xr) − Ar

θr +

θrtBt(Xt)



t∈N (r)
where θr is a vector formed from scalars {θri}mr
i=1, θrt is a matrix of dimension mr×mt
obtained from scalars θri;tj and (cid:104)., .(cid:105) represents dot product between two vectors. Thus,






mr(cid:88)

i=1






(cid:88)

t∈N (r)

(cid:88)

6

the joint distribution has the form

P (X|θ) =



(cid:88)

exp

(cid:42)



r∈V

+

(cid:88)

r∈V

Cr(Xr) − A(θ)

Br(Xr), θr +

θrtBt(Xt)

(cid:88)

t∈N (r)

(cid:41)

(cid:43)

,

(10)

with the log-normalization constant A(θ) = log (cid:82)
(cid:80)

r∈V Cr(Xr)}. Since A(θ) is generally intractable to calculate, we next present an

efﬁcient approach to learning the structure of VS-MRFs.

X exp{(cid:80)

r∈V (cid:104)Br(Xr), θr+(cid:80)

t∈N (r) θrtBt(Xt)(cid:105)+

4 Learning VS-MRFs

To avoid calculation of the log-normalization constant, we approximate the joint dis-
tribution in (10) with the independent product of node conditionals, also known as the
pseudo-likelihood,

P (X|θ) ≈

P (Xr|X−r, θr, θrt) .

(11)

(cid:89)

r

Let θr· = {θr, θ\r} be the set of parameters related to the node-conditional distribution
of node r, where θ\r = {θrt}t∈V \r. Since Ar() is convex for all exponential families
Wainwright and Jordan [2008], this gives us a loss function that is convex in θr·:

(cid:96)(θr·; D) = − 1
n



Br(X (i)

r ), θr +



(cid:42)

n
(cid:88)

i



(cid:43)

θrtBt(X (i)
t )

(cid:88)

t∈V \r





.

(12)

− Ar

θr +

θrtBt(X (i)
t )





(cid:88)

t∈V \r

We then seek to ﬁnd a sparse solution in terms of both edges and individual parameters
by employing the sparse group lasso regularization penalty Friedman et al. [2010],
Simon et al. [2013]:

R(θr·) = λ1

(cid:88)

√

t∈V \r

νrt ||θrt||2 + λ2

(cid:12)
(cid:12)
(cid:12)θ\r
(cid:12)

(cid:12)
(cid:12)
(cid:12)1 ,
(cid:12)

where νrt = mr × mt is the number of parameters in the pseudo-edge from node r
to node t (i.e., the edge (r, t) in the rth node-conditional). This yields a collection of
independent convex optimization problems, one for each node-conditional.

minimize
θr·

(cid:96)(θr·; D) + R(θr·)

We next present an approach to solving this problem based on Alternating Direction

(13)

(14)

7

Method of Multipliers (ADMM) Boyd et al. [2011].

4.1 Optimization Procedure

We ﬁrst introduce a slack variable z into (14) to adhere to the canonical form of
ADMM. For notational simplicity, we omit the data parameter D from the loss function
and the subscripts in θr· and Ar since it is clear we are dealing with the optimization
of a single node-conditional.

minimize
θ

(cid:96)(θ) + R(z)

subject to θ = z

,

where length(θ) = τ . The augmented Lagrangian is

Lα(θ, z, ρ) = (cid:96)(θ) + R(z) + ρT (θ − z) + (α/2) ||θ − z||2
2 .

(16)

Deﬁning the residual of the slack r = θ − z, we instead use the scaled form with
u = (1/α)ρ. ADMM proceeds in an alternating fashion, performing the following
updates at each iteration:

θk+1 = argmin

(cid:12)
(cid:12)θ − zk + uk(cid:12)
(cid:12)
(cid:96)(θ) + (α/2) (cid:12)
(cid:12)
(cid:12)
(cid:12)

zk+1 = argmin

(cid:12)
(cid:12)θk+1 − z + uk(cid:12)
(cid:12)
R(z) + (α/2) (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:17)

2

2

(cid:17)

2

2

(cid:16)

(cid:16)

θ

z

uk+1 = uk + θk+1 − zk+1

Updating θk+1. The jth subgradient of θ is gj(θ) = −Bj + ∇jA(θ) + α(θj + zk
j −
uk
j ). Note that the log-partition function, A(η), over the natural parameters, η = Bθ, is
available in closed form for most commonly-used exponential families. Thus, ∇2A(θ)
is a weighted sum of rank-one matrices. In cases where the number of samples is much
less than the total number of parameters (i.e. n << τ ), we can efﬁciently calculate
an exact Newton update in O(τ ) by leveraging the matrix inversion lemma Boyd and
Vandenberghe [2009]. Otherwise, we use a diagonal approximation of the Hessian and
perform a quasi-Newton update.

Updating zk+1. We can reformulate (18) as the proximal operator Parikh and Boyd
[2013] of R(z):

proxR/α(y) = argmin

R(z) + (α/2) ||z − y||2
2

(cid:17)

,

(cid:16)

z

where y = θk+1 + uk. From Friedman et al. [2010], it is straightforward to show

that the update has a closed-form solution for each jth block of edge parameters,

zk+1
j =

(cid:0)||S(α(yj), λ2)||2 −

α ||S(α(yj), λ2)||2 +

√

(cid:1)

νjλ1
√

S(α(yj), λ2)

+
νjλ1(1 − α)

,

8

(15)

(17)

(18)

(19)

(20)

(21)

where S(x, λ) is the soft-thresholding operator on x with cutoff at λ.

Updating uk+1. Per ADMM, closed-form is given in (19).

We iterate each of the above update steps in turn until convergence, then AND

pseudo-edges when stitching the graph back together.

4.2 Domain constraints

Many exponential family distributions require parameters with bounded domain. These
bounds correspond to afﬁne constraints on subsets of θ in the ADMM algorithm.1
Often these constraints are simple implicit restrictions to R+ or R−. In these cases
the log-normalization function A(η) serves as a built-in log-barrier function. For in-
stance, a normal distribution with unknown mean µ and unknown variance σ2 has
natural parameters η1 = µ
2σ2 , implying η2 < 0. However, since
A(η) = − η2
2 ln(−2η2), this constraint will be effectively enforced so long as we
1
4η2
are given a feasible starting point for η. Such a feasible point can always be discovered
using a standard phase I method Boyd and Vandenberghe [2009]. In the case of equal-
ity requirements, such as categorical and multinomial distributions, we can directly in-
corporate the constraints into the ADMM algorithm and solve an equality-constrained
Newton’s method when updating θ.

σ2 and η2 = − 1

− 1

4.3 Sparsistency

We next provide the mathematical conditions that ensure with high probability our
learning procedure recovers the true graph structure underlying the joint distribution.
Our results rely on similar sufﬁcient conditions to those imposed in papers analyzing
the Lasso Wainwright [2009] and the l1/l2 penalty in Jalali et al. [2011]. Before stating
the assumptions, we introduce the notation used in the proof.

4.3.1 Notation

rt (cid:54)= 0} be the true neighbourhood of node r and let dr be the degree
rj;tk : t ∈ N (r)}
rj;tk : t /∈ N (r)}. From now on
r. Let

Let N (r) = {t : θ∗
of r, i.e, dr = |N (r)|. And Sr be the index set of parameters {θ∗
r be the index of parameters {θ∗
and similarly Sc
we will overload the notation and simply use S and Sc instead of Sr and Sc
S(ex)
r = {θ∗
Let Qn

r·; D) be the sample Fischer Information matrix at node r. As
r . Finally, we write Qn
SS

before, we will ignore subscript r and use Qn instead of Qn
for the sub-matrix indexed by S.

rj;tk : θ∗
r = ∇2(cid:96)(θ∗

rj;tk (cid:54)= 0 ∧ t ∈ N (r)}.

We use the group structured norms deﬁned in Jalali et al. [2011] in our analysis.
The group structured norm ||u||G,a,b of a vector u with respect to a set of disjoint groups
G = {G1, . . . , GT } is deﬁned as ||(||uG1||b , . . . , ||uGT ||b)||a. We ignore the group G
and simply use ||u||a,b when it is clear from the context. Similarly the group structured

1Note that these subsets are different than the edge-wise groups that are L2-penalized. Rather, these

constraints apply to the sum of the ith value of each edge parameter and the ith bias weight.

9

norm ||M ||(a,b),(c,d) of a matrix Mp×p is deﬁned as
our analysis we always use b = 2, d = 2 and to minimize the notation we use ||M ||a,c
to denote ||M ||(a,2),(c,2). And we deﬁne ||M ||max as max
|Mi,j|, i.e, element wise
i,j
maximum of M.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)M 1(cid:12)
(cid:12)
(cid:12)((cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)c,d , . . . , ||M p||c,d)
(cid:12)
(cid:12)
(cid:12)a,b
(cid:12)
(cid:12)

. In

4.3.2 Assumptions

Let us begin by imposing assumptions on the sample Fisher Information matrix Qn.

Assumption 1. Dependency condition: Λmin(Qn

SS) ≥ Cmin.

Assumption 2. Incoherence condition:
(cid:12)
(cid:12)
ScS(Qn
(cid:12)Qn
(cid:12)
mmin = min

(cid:12)
SS)−1(cid:12)
(cid:12)∞,2 ≤ mmin
(cid:12)
mmax
mt.

(1−α)
√
dr

t

Assumption 3. Boundedness:
(cid:1) B (cid:0)XV \r
Λmax(E[B (cid:0)XV \r
(cid:1) = {Bt(Xt)}t∈V \r.
that B (cid:0)XV \r

(cid:1)T

for some α ∈ (0, 1] , where mmax = max

mt,

t

]) ≤ Dmax < ∞, where B (cid:0)XV \r

(cid:1) is a vector such

Note that the sufﬁcient statistics {Bri(Xr)}mr

i=1 of node r need not be bounded.
So to analyze the M-estimation problem, we make the following assumptions on log-
partition functions of joint and node-conditional distributions. These are similar to the
conditions imposed for sparsistency analysis of GLMs.

Assumption 4. The log partition function of the joint distribution satisﬁes the follow-
ing conditions: for all r ∈ V and i ∈ [mr]

1. there exists constants km, kv such that E[Bri(Xr)] ≤ km and E[Bri(Xr)2] ≤

kv,

2. there exists constant kh such that maxu:|u|≤1

∂2A(θ)
∂θ2
ri

(θ∗

ri + u, θ∗

r·) ≤ kh,

3. for scalar variable η , we deﬁne a function ¯Ar,i as:

¯Ar,i(η; θ) = log (cid:82)
(cid:42)

exp

Xp

(cid:110)
ηBri(Xr)2 + (cid:80)
(cid:43)

+ (cid:80)

s∈V

Bs(Xs), θs +

θstBt(Xt)

s∈V Cs(Xs)
(cid:111)

d(x)

(cid:88)

t∈N (s)

(22)

Then, there exists a constant kh such that maxu:|u|≤1

∂2Ar,i(η;θ∗
∂η2

r·)

(u) ≤ kh.

Assumption 5. For all r ∈ V , the log-partition function Ar(.) of the node wise condi-
tional distribution satisfy that there exists functions k1 (n, p) and k2 (n, p) such that for
(cid:12)
all feasible pairs θ and X, (cid:12)
(cid:12)
(cid:12)∇2Ar (a)(cid:12)
(cid:12)max ≤ k1 (n, p) where a ∈ [b, b + 4 k2 (n, p) max {log (n) , log (p)} 1]
(cid:12)
(cid:12)
for b := θr + (cid:80)
:=
(cid:12)
⊗i[ui, vi]. Moreoever, we assume that (cid:12)
(cid:12)
(cid:12)∇3Ar (b)(cid:12)
(cid:12)max ≤ k3 (n, p) for all feasible
(cid:12)
(cid:12)
pairs X and θ.

t∈V \r θrtBt(Xt), where for vectors u and v we deﬁne [u, v]

10

≤ λ1+λ2 ≤ M2
(cid:17)

2−α
α k1 (n, p) k2 (n, p)

α
2−α+2 mmax/mmin

λ1. Then, there ex-

d2 k1 (n, p) (k3 (n, p))2 (logp(cid:48))2 log (cid:0)p m2

(cid:1) ,

max

4.3.3 Sparsistency Theorem

Given these assumptions in 4.3.2 we are now ready to state our main sparsistency result.

Theorem 2. Consider the vector space graphical model distribution in (10) with true
parameters θ∗, edge set E and vertex set V such that the assumptions 1-5 hold. Sup-
pose that θ∗ satisﬁes min
(λ1 + λ2) and regularization parame-
(r,t)∈E

||θ∗

Cmin

2−α
α

mmax
mmin

ters λ1, λ2 satisfy M1

rt||2 ≥ 10 mmax
(cid:112)k1 (n, p)
for positive constants M1 and M2 and λ2 <
ists constants L, c1, c2 and c3 such that if n ≥ max(cid:8)L m9
4 log(p m2
max)
k1(n,p) k4 k2(n,p)2 , 8 k2
log ((cid:80)
exp(−c2 n) − exp(−c3 n), the following statements hold.

(cid:113) log(pm2
n
(cid:16)

h
k2
4

max)

max
mmin

t mt) (cid:9), with probability at least 1−c (p(cid:48))−3 ((cid:80)

t mt)−

• For each node r ∈ V , the solution of the M-estimation problem (14) is unique

• Moreover, for each node r ∈ V the M-estimation problem recovers the true

neighbourhood exactly.

where mmax = max

mt, mmin = min

mt , p(cid:48) = max(n, p).

t

t

The proof of Theorem 2 follows along similar lines to the sparsistency proof in
Yang et al. [2014], albeit with a subtler analysis to support general vector-spaces. It is
based on the primal dual witness proof technique and relies on the previous results. We
refer the interested reader to the supplementary material for additional details regarding
the proofs.

5 Experiments

We demonstrate the effectiveness of our algorithm on both synthetic data and a real-
world dataset of over four million foods logged on the popular diet app, MyFitnessPal.

5.1 Synthetic experiments

The synthetic experiments were run on a vector-space mixed MRF consisting of eight
Bernoulli, eight gamma (with unknown shape and rate), eight Gaussian (with un-
known mean and variance), and eight Dirichlet (k=3) nodes. The choice of these
node-conditional distributions is meant to highlight the ability of VS-MRFs to model
many different types of distributions. Speciﬁcally, the Bernoulli represents a univari-
ate, uni-parameter distribution that would still be possible to incorporate into existing
mixed models. The gamma and Gaussian distributions are both multi-parameter, uni-
variate distributions which would have required ﬁxing one parameter (e.g. ﬁxing the
Gaussians’ variances) to be compatible with previous approaches. Finally, the Dirich-
let distribution is multi-parameter and multivariate, thereby making VS-MRFs truly
unique in their ability to model this joint distribution.

For each experiment, we conducted 30 independent trials by generating random
weights and sampling via Gibbs sampling with a burn-in of 2000 and thinning step
size of 10. We consider two different sparsity scenarios: high (90% edge sparsity, 50%
intra-edge parameter sparsity) and low (50% edge sparsity, 10% intra-edge parameter

11

Figure 1: ROC curves for our synthetic experiments. The top left and bottom left plots
show both edge as well as within-edge-parameter recovery performance respectively,
for graphs with a high degree of sparsity. The two right plots show the same per-
formance measures, but for graphs with a relatively low degree of sparsity. The low
sparsity scenario is more challenging, requiring more data to recover the majority of
the graph.

sparsity). Edge recovery capability is examined by ﬁxing λ2 to a small value and vary-
ing λ1 over a grid of values in the range [0.0001, 0.5]; parameter recovery is examined
analogously by ﬁxing λ1 and varying λ2. We use AND graph stitching and measure
the true positive rate (TPR) and false positive rate (FPR) as the number of samples
increases from 100 to 25K.

Figure 5 shows the ROC curves at both the edge and parameter levels. The results
demonstrate that our algorithm improves well as the dataset size scales. They also
illustrate that graphs with a higher degree of sparsity are easier to recover with fewer
samples. In both the high and low sparsity graphs, the algorithm is better able to recover
the coarse-grained edge structure than the more ﬁne-grained within-edge parameter
structure, though both improve favourably with the size of the data.

5.2 MyFitnessPal Food Dataset

MyFitnessPal2 (MFP) is one of the largest diet-tracking apps in the world, with over
80M users worldwide. MFP has a vast crowd-sourced database of food data, where
each food entry contains a description, such as “Trader Joe’s Organic Carrots,” and a
vector of sixteen macro- and micro-nutrients, such as fat and vitamin C.

2http://myfitnesspal.com

12

Figure 2: The top 100 edges in the MyFitnessPal food graph. Purple rectangular
nodes correspond to macro- and micro-nutrients; green oval nodes correspond to food
description terms. Edge color is determined by the approximate effect of the edge on
the means of the node-conditionals: darker, blue edges represent lower means; brighter,
orange edges represent higher means; thickness corresponds to the norm of the edge
weight.

We treat these foods entries as random vectors with an underlying VS-MRF dis-
tribution, which we learn treating the food entries in the database as samples from the
underlying VS-MRF distribution. The text descriptions are tokenized, resulting in a
dictionary of approximately 2650 words; we use a Bernoulli distribution to model the
conditional distribution of each word. The conditional distribution of each nutrient
(on a per-calorie basis) is generally gamma distributed, but contains spikes at zero3
and large outlier values.4 The gamma distribution is undeﬁned at zero, and the outlier
values can result in numerical instability during learning, which thus suggests using
a distribution other than the vanilla gamma distribution. Such zero-inﬂated data are
common in many biostatistics applications, and are typically modeled via a mixture
model density of the form p(Z) = π δ0 + (1 − π) g(z), where δ0 is the dirac delta at
zero, and g(z) is the density of the non-zero-valued data. Unfortunately, such mixture
models are not generally representable as exponential families.

To overcome this, we introduce the following class of point-inﬂated exponential
family distributions. For any random variable Z ∈ Z, consider any exponential family
P (Z) = exp(ηT B(Z) + C(Z) − A(η)), with sufﬁcient statistics B(·), base measure
C(·), and log-normalization constant A(·). We consider an inﬂated variant of this
random variable, inﬂated at some value j; note that this could potentially lie outside the
domain Z, in which case the domain of the inﬂated random variable would become Z ∪
{j}. We then deﬁne the corresponding point-inﬂated exponential family distribution

3This is common in foods since many dishes are marketed as “fat free” or contain low nutrient density

4This occurs when foods contain few calories but a large amount of some micro-nutrient (e.g. multi-

(e.g. soda).

vitamins)

13

as:

Pinﬂ(Z) = exp (cid:8)η0I(Z = j) + ηT

1 B(z) + C(z) − Ainﬂ(η)(cid:9) ,

where Ainﬂ(η) is the log-normalization constant of the point-inﬂated distribution which
can be expressed in terms of the log-normalization constant A(·) of the uninﬂated ex-
ponential family distribution: Ainﬂ(η) = log (cid:0) exp{η0} − exp{ηT
1 B(j)I(j ∈ Z)} +
exp{A(η1)}(cid:1). Thus, as long as we have a closed form A(·) for the log-partition func-
tion of the base distribution, we can efﬁciently calculate Pinﬂ(Z). The deﬁnition also
permits an arbitrary number of inﬂated points by recursively specifying the base dis-
tribution as another point-inﬂated model. We model each of the MFP nutrients via
a two-point-inﬂated gamma distribution, with points at zero and a winsorized outlier
bucket.

Due to the size of the overall graph, presenting it in closer detail here is not fea-
sible. To give a qualitative perspective of the relationships captured by our algorithm,
we selected the top 100 edges in the MFP food graph by ranking the edges based
on their L2-norm. We then calculated their approximate contribution to the mean of
their corresponding node-conditionals to determine edge color and thickness. Figure
2 shows the results of this process, with edges that contribute positively colored in or-
ange and edges that contribute negatively colored in blue; edge thickness corresponds
to the magnitude of the contribution. A high-level view of the entire learned graph is
available in the supplementary materials.

Several interesting relationships can be discovered, even from just this small sub-
set of the overall model. For instance, the negative connection between “peanut” and
sodium may seem counter-intuitive, given the popularity of salted nuts, yet on inspec-
tion of the raw database it appears that indeed many peanut-based foods are actually
very low in sodium on a per-calorie basis. As another example, “chips” are often
thought of as a high-carb food, but the graph suggests that they actually tend to be a
bigger indicator of high fat. In general, we believe there is great potential for wide-
ranging future uses of VS-MRFs in nutrition and other scientiﬁc ﬁelds, with the MFP
case study only scratching the surface of what can be achieved.

6 Conclusion

We have presented vector-space MRFs as a ﬂexible and scalable approach to modeling
complex, heterogeneous data. In particular, we generalize the concept of mixed MRFs
to allow for node-conditional distributions to be distributed according to a generic ex-
ponential family distribution, that is potentially multi-parameter and even multivariate.
Our VS-MRF learning algorithm has reassuring sparsistency guarantees and was vali-
dated against a variety of synthetic experiments and a real-world case study. We believe
that the broad applicability of VS-MRFs will make them a valuable addition to the sci-
entiﬁc toolbox. All code for our VS-MRF implementation is publicly available.5

5https://github.com/tansey/vsmrfs

14

A Proof of Theorem 1

The proof follows the same lines as the proof in Yang et al. [2014]. Let us denote Q(X)
as log (P (X)/P (0)). Note that X = (X1, X2, · · · Xp) and each Xr belongs to a vector
space. Given any X, let us denote ¯Xs as ¯Xs = (X1, · · · , Xs−1, 0, Xs+1, · · · , Xp).
Consider the following expansion for Q(X):

Q(X) =
(cid:88)

t∈{1,··· ,p}

(cid:88)

+

t1,···tk∈
{1,··· ,p}

I[Xt (cid:54)= 0]Gt(Xt) + · · ·

I[Xt1 (cid:54)= 0, . . . Xtk (cid:54)= 0]Gt1...tk (Xt1 . . . Xtk )

where I is the indicator function which takes value 1 if its argument evaluates to true
and 0 otherwise.

Using some simple algebra and the deﬁnition Q(X) = log (P (X)/P (0)) we can

show that

exp (Q(X) − Q( ¯Xs)) = P (Xs|X1,··· ,Xs−1,Xs+1,··· ,Xp)
P (0|X1,··· ,Xs−1,Xs+1,··· ,Xp)

(24)

From (23) we have the following:

(Q(X) − Q( ¯Xs)) =



I[Xs (cid:54)= 0]

Gs(Xs) +

I[Xt (cid:54)= 0]Gs,t(Xs, Xt)

(cid:88)

t∈{1,··· ,p}\s

+

I[Xt2 (cid:54)= 0, . . . Xtk (cid:54)= 0]Gs,t2...tk (Xs, . . . Xtk )

(cid:88)

t2,···tk∈
{1,··· ,p}\s







Since the node conditional distribution follows the exponential family distribution

deﬁned in (6) we can show that:

P (0|X1,··· ,Xs−1,Xs+1,··· ,Xp) =

log P (Xs|X1,··· ,Xs−1,Xs+1,··· ,Xp)
(cid:104)Es(X−s), Bs(Xs) − Bs(0)(cid:105) + (Cs(Xs) − Cs(0))

Using (25) and (26) for left and right hand sides of (24) and setting Xt = 0 for all

t (cid:54)= s we obtain:

I[Xs (cid:54)= 0]Gs(Xs) = (cid:104)Es(0), Bs(Xs) − Bs(0)(cid:105)

+(Cs(Xs) − Cs(0))

(23)

(25)

(26)

15

Similarly setting Xr = 0 for all r /∈ {s, t} we obtain:

I[Xs (cid:54)= 0]Gs(Xs) + I[Xs (cid:54)= 0, Xt (cid:54)= 0]Gs,t(Xs, Xt) =
(cid:104)Es(0 · · · Xt · · · , 0), Bs(Xs) − Bs(0)(cid:105) + (Cs(Xs) − Cs(0))

Similarly, replacing Xs with Xt in (24) and setting Xr = 0 for all r /∈ {s, t} we

obtain:

I[Xt (cid:54)= 0]Gt(Xt) + I[Xs (cid:54)= 0, Xt (cid:54)= 0]Gs,t(Xs, Xt) =
(cid:104)Et(0 · · · Xs · · · , 0), Bt(Xt) − Bt(0)(cid:105) + (Ct(Xt) − Ct(0))

From the above three equations we arrive at the following equality:

(cid:104)Es(0 · · · Xt · · · , 0) − Es(0), Bs(Xs) − Bs(0)(cid:105) =
(cid:104)Et(0 · · · Xs · · · , 0) − Et(0), Bt(Xt) − Bt(0)(cid:105)

(27)

The above equality should hold for the node conditional distributions to be consistent
with the joint MRF distribution over X with respect to graph G. So we need to ﬁnd the
form of Er() that satisﬁes the above equation. Omitting zero vectors for clarity from
(27), we get the following:

(cid:104)Et(Xs), Bt(Xt)(cid:105)

= (cid:104)Es(Xt), Bs(Xs)(cid:105)

(cid:88)

j

Etj(Xs)Btj(Xt) =

Esl(Xt)Bsl(Xs)

(cid:88)

l

We rewrite the natural parameter functions as

Etj(Xs) =

θsl;tjBsl(Xs) + Btj(Xs)

Esl(Xt) =

θsl;tjBtj(Xt) + Bsl(Xt)

(cid:88)

(cid:88)

l

j

where ∀j Btj(Xs) are functions in the Hilbert space Hs orthogonal to the span of
functions Bs(Xs), and ∀j Bsl(Xt) are functions in the Hilbert space Ht orthogonal to
the span of functions Bt(Xt); and θsl;tj, θsl;tj are scalars. Combining (28) and (29),
we get

θsl;tjBsl(Xs)Btj(Xt) +

Btj(Xs)Btj(Xt)

=

θsl;tjBsl(Xs)Btj(Xt) +

Bsl(Xt)Bsl(Xs)

(cid:88)

(cid:88)

j
(cid:88)

l
(cid:88)

l

j

(cid:88)

j
(cid:88)

l

(28)

(29)

(30)

16

Rearranging terms in the above equation gives us the following equation:

(θsl;tj − θsl;tj)Bsl(Xs) + Btj(Xs)

Btj(Xt)

(cid:33)

(cid:32)

(cid:88)

(cid:88)

j
=

(cid:88)

l
Bsl(Xs)Bsl(Xt)

l

(31)

However, since ∀l Bsl(Xt) is orthogonal to Bt(Xt), the left and right hand sides

of the above equation are equal to 0, which leads us to the following equations.

(cid:88)

Bsl(Xs)Bsl(Xt) = 0

l

j

(cid:32)

(cid:88)

(cid:88)

l

(θsl;tj − θsl;tj)Bsl(Xs) + Btj(Xs)

Btj(Xt) = 0

(cid:33)

(32)

However since we assumed that the sufﬁcient statistics are minimal we get ∀l Bsl(Xt) =
0 from the ﬁrst equality and ∀j, l θsl;tj = θsl;tj, Btj(Xs) = 0 from the second equality.
Hence from (29), we obtain Es(Xt) = θst(Bt(Xt) − Bt(0)) and Et(Xs) =
θT
st(Bs(Xs) − Bs(0)) where θst is a matrix formed by the scalars θsl;tj such that
(θst)lj = θsl;tj and:

I[Xs (cid:54)= 0, Xt (cid:54)= 0]Gs,t(Xs, Xt) =
(Bt(Xt) − Bt(0))T θT

st(Bs(Xs) − Bs(0))

(33)

By extending this argument to higher order factors we can show that the natural param-
eters are required to be in the form speciﬁed by (7).

B Proof of Sparsistency

Before proving the sparsistency result, we will show that the sufﬁcient statistics Br(Xr)
are well behaved. Recall that Bri(Xr) indicates ith component of the vector Br(Xr).
We set the convention that whenever a variable has the subscript \r attached we will
be referring to the set of indexes {(t, j, k) : θrj;tk ∈ θr·, t (cid:54)= r}.

Proposition 1. Let {X (j)}n

j=1 have joint distribution as in (10), then,



P



1
n

n
(cid:88)

(cid:16)

j=1

(cid:16)

Bri

X (j)
r

(cid:17)(cid:17)2



(cid:18)

≥ δ

 ≤ exp

−n

(cid:19)

δ2
4k2
h

(34)

for δ ≤ min (cid:8)2 kv

3 , kh + kv

(cid:9).

17

(35)

(36)

(37)

Proof. It is clear from Taylor Series expansion and assumption 4 that

(cid:104)

exp

log E
log (cid:82)
⊗s∈[p]Xs
(cid:42)

(cid:16)

tBri (Xr)2(cid:17)(cid:105)
(cid:110)
tBri (Xr)2 +
exp

=

(cid:43)

(cid:80)
s∈V

Bs (Xs) , θ∗

θ∗
stBt (Xt)

+

s + (cid:80)
t∈N (r)
(cid:27)

v (dx)

Cs (Xs) − A(θ∗)

(cid:80)
s∈V
= ¯Ar,i (η; θ) (t; θ∗) − ¯Ar,i (η; θ) (0; θ∗)
≤ t ∂ ¯Ar,i(η;θ)
≤ t kv + t2

∂2 ¯Ar,i(η;θ)
∂η2

(0) + t2
2

(ut)

∂η

2 kh

(cid:18)

P

1
n

exp

exp

(cid:16)

(cid:17)2

(cid:80)n

X (j)
r

j=1 Bri
−nδt + n kvt + t2
(cid:17)
−n δ2
4k2
h

(cid:16)

(cid:16)

(cid:19)

≥ δ

≤
(cid:17)

2 Khn

≤

where u ∈ [0, 1]

Therefore, by the standard Chernoff bounding technique, for t ≤ 1, it follows that

for δ ≤ min (cid:8)2 kv

3 , kh + kv

(cid:9).

Proposition 2. Let X be a random vector with the distribution speciﬁed in (10). Then,
for any positive constant δ and some constant c > 0

(cid:16)

P

|Bri(Xr)| ≥ δlog(η)

(cid:17)

≤ cη−δ

Proof. Let ¯v be a unit vector with the same dimensions as θ∗
entry, corresponding to the sufﬁcient statistic Bri(Xr). Then we can write log
as:

r· and exactly one non-zero

(cid:16)

E[exp(Bri(Xr))]

(cid:17)

(cid:16)

(cid:17)

log

E[exp(Bri(Xr))]

= A(θ∗ + ¯v) − A(θ∗)

By Taylor series expansion, for some u ∈ [0, 1], we can rewrite last equation as

A(θ∗ + ¯v) − A(θ∗) =∇A(θ∗).¯v +

¯vT ∇2A(θ∗ + u ¯v)¯v

1
2

=E[Bri(Xr)](cid:107)¯v(cid:107)2

+

1
2

∂2A(θ∗ + u ¯v)
∂θ2
ri

(cid:107)¯v(cid:107)2
2

18

Using Assumption 4 we get the inequality :

A(θ∗ + ¯v) − A(θ∗) ≤ km +

kh

1
2

Now, by using Chernoff bound, for any positive constant a, we get P (Bri(Xr) ≥ a) ≤
exp(−a + km + 1

2 kh). By setting a = δlog(η) it follows that

P (Bri(Xr) ≥ δlog(η)) ≤ exp(−δlog(η) + km +

kh) ≤ cη−δ

1
2

where c = exp(km + 1

2 kh)

The proof of Sparsistency is based on the primal dual witness proof technique. First

note that the optimality condition of (14), can be written as:

∇(cid:96)(ˆθr·; D) + λ1

νrt ˆZ1,rt + λ2 ˆZ2 = 0

(38)

(cid:88)

√

t:r(cid:54)=t

where ˆZ1,rt ∈ ∂ (cid:107) ˆθrt (cid:107)2, ˆZ2 ∈ ∂ (cid:107) ˆθ\r (cid:107)1 and we denote ˆZ =
, where
ˆZ1 = { ˆZ1,rt}t∈V \r. And sub-gradients ˆZ1, ˆZ2 should satisfy the following conditions:

(cid:16) ˆZ1, ˆZ2

(cid:17)

∀i ( ˆZ2)i = sign

(ˆθr·)i

if (ˆθr·)i (cid:54)= 0

(cid:16)

(cid:17)

∀t ˆZ1,rt =

|( ˆZ2)i| ≤ 1 otherwise

ˆθrt
||ˆθrt||2
(cid:12)
(cid:12)
ˆZ1,rt
(cid:12)
(cid:12)
(cid:12)
(cid:12)

if ˆθrt (cid:54)= 0
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)2
(cid:12)

≤ 1 otherwise

(39)

Note that we can think of ˆZ1 and ˆZ2 as dual variables by appealing to Lagrangian
theory. The next lemma shows that graph structure recovery is guaranteed if the dual
is strictly feasible.

(cid:16)ˆθr·, ˆZ
for (14) such that
< 1. Then, any optimal solution ˜θr· must satisfy

(cid:17)

< 1 and

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)∞,2
(cid:12)

Lemma 1. Suppose that there exists a primal-dual pair
(cid:12)
(cid:12)
ˆZ1,Sc
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:16)˜θr·
(cid:17)
then ˆθ\r is the unique optimal solution.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)∞
(cid:12)

ˆZ2,Sc

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Sc

= 0. Moreover, if the Hessian sub-matrix [∇2(cid:96)(ˆθr·)]SS is positive deﬁnite

Proof. First, note that by Cauchy−Schwarz’s and Holder’s inequalities

(cid:104) ˆZ1,rt, ˜θrt(cid:105) ≤(cid:107) ˜θrt (cid:107)2 and(cid:104) ˆZ2, ˜θ\r(cid:105) ≤(cid:107) ˜θ\r (cid:107)1 .

(40)

19

But from (38) and the primal optimality of ˆθr· and ˜θr· for (14),

√

(cid:96)

(cid:17)

(cid:16)˜θr·
≥ min
(cid:16)ˆθr·
(cid:16)˜θr·

= (cid:96)

= (cid:96)

+ (cid:80)
t(cid:54)=r λ1
(cid:96) (θr·) + (cid:80)
(cid:17)
+ (cid:80)
+ (cid:80)

(cid:17)

θ

t(cid:54)=r λ1
t(cid:54)=r λ1

√

t(cid:54)=r λ1
√

νrt(cid:104) ˆZ1,rt, ˜θrt(cid:105) + λ2(cid:104) ˆZ2, ˜θ\r(cid:105)

νrt(cid:104) ˆZ1,rt, θrt(cid:105) + λ2(cid:104) ˆZ2, θ\r(cid:105)

√

νrt(cid:104) ˆZ1,rt, ˆθrt(cid:105) + λ2(cid:104) ˆZ2, ˆθ\r(cid:105)
(cid:12)
(cid:12)
˜θ\r
(cid:12)
(cid:12)
νrt
(cid:12)
(cid:12)

˜θrt

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)1
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)2
(cid:12)

(41)

hence, combining with (40) with (41) it follows that (cid:80)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)1
(cid:12)

t(cid:54)=r λ1

t(cid:54)=r λ1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
νrt
(cid:12)1
(cid:12)
νrt(cid:104) ˆZ1,rt, ˆθrt(cid:105)+λ2(cid:104) ˆZ2, ˆθ\r(cid:105). The result

t(cid:54)=r λ1

= (cid:80)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)2
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)2
(cid:12)

˜θ\r

ˆθ\r

ˆθrt

˜θrt

νrt

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

√

√

√

+

+

= (cid:80)
follows.

If the Hessian sub-matrix is positive deﬁnite for the restricted problem then the

problem is strictly convex and has a unique solution.

Based on the above lemma, we prove sparsistency theorem by constructing a primal-

dual witness (ˆθr·, ˆZ) with the following steps:

1. Set (ˆθr·)S = argmin((θr·)S ,0)(cid:96) ((θr·)S ; D) +λ1
2. For t ∈ S, we deﬁne ˆZ1,rt = θrt
(cid:107)θrt(cid:107)2

√

(cid:80)

t∈S

νrt ||θrt||2 + λ2 ||(θr·)S||1

and then construct ˆZ2,S by the stationary

condition.

3. Set (ˆθr·)Sc = 0

4. Set ˆZ2,Sc such that

ˆZ2,Sc

< 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)∞
(cid:12)

5. Set ˆZ1,Sc such that condition (38) is satisﬁed.

6. The ﬁnal step consists of showing, that the following conditions are satisﬁed:

(a) strict dual feasibility : the condition in Lemma 1 holds with high probabil-

ity

(b) correct neighbourhood recovery: the primal-dual pair speciﬁes the neigh-

bourhood of r, with high probability

We begin by proving some key lemmas that are key to our main theorem. The

sub-gradient optimality condition (38) can be rewritten as:

∇(cid:96)(ˆθr·; D) − ∇(cid:96)(θ∗

r·; D) = W n − λ1

νrt ˆZ1,rt − λ2 ˆZ2

(42)

(cid:88)

√

t:r(cid:54)=t

where W n = −∇(cid:96)(θ∗
value theorem coordinate wise to (42), we get:

r·; D) and θ∗

r· is the true model parameter. By applying mean-

∇2(cid:96)(θ∗

r·; D)[ˆθr· − θ∗

r·] = W n − λ1

(cid:80)
−λ2 ˆZ2 + Rn

t:r(cid:54)=t

√

νrt ˆZ1,rt

(43)

20

r·; D)]T

where Rn is the remainder term after applying mean-value theorem:Rn
∇2(cid:96)(¯θj
r· on the line between ˆθr· and θ∗
denoting the j-th row of matrix. The following lemma controls the score term W n
νrt, p(cid:48) = max(n, p). Assume that

Lemma 2. Recall νr max = max

j = [∇2(cid:96)(θ∗
r·, and with [.]T
j

r·) for some ¯θj

νrt , νr min = min

j (ˆθr· − θ∗

r·; D)−

t

(cid:113)

t

8(2−α)
α

k1(n, p) k4
√

λ1 + λ2 ≤ 4(2−α)
√

α

νr min

νr maxlog(pνr max)
nνr min
k1(n, p) k2(n, p) k4

≤

νr max

for some constant k4 ≤ min (cid:8)2 kv

3 , kh + kv

(cid:9) and suppose also that n ≥ 8 k2
h
k2
4

log ((cid:80)

t mt)

(cid:16)

\r(cid:107)∞,2 > α

2−α

νr min (λ1+λ2)
4

√

(cid:17)

≤

then,

(cid:107)W n
P
1 − c1p(cid:48)−3 ((cid:80)
Proof. Deﬁne W n
t = −∇θrt(cid:96) (θ∗
to parameter θrj;tk. Note that W n

t mt) − exp (−c2n) − exp (−c3n)
r·; D). Let W n
t,jk = 1
n

t,jk be the element in W n

t,jk where

t=1 V i

(cid:80)n

t corresponding

V i
t,jk = Brj

(cid:16)

(cid:17)

X (i)
r
(cid:16)

Btk
r + (cid:80)
θ∗

(cid:16)

(cid:17)

X (i)
−
t
s∈V \r θ∗

∇θrj;tk Ar

rsBs

(cid:16)

X (i)
s

(cid:17)(cid:17)

Btk

(cid:17)

(cid:16)

X (i)
t

so for t(cid:48) ∈ R

(44)

(45)

(cid:16)

X (i)
s

(cid:17)(cid:17)

(cid:17) (cid:21)

(cid:16)

X (i)
t

Btk
(cid:17)

(cid:16)

X (i)
r

Br

θ∗
rsBs

(cid:16)

X (i)
s

(cid:17)

(cid:16)

(cid:104)
exp

E
(cid:90)

exp

X (i)
r

(cid:17)

t(cid:48)V i
t,jk
(cid:20)
(cid:26)
Brj

t(cid:48)

|X (i)
V \r
(cid:16)

X (i)
r

(cid:105)

=
(cid:17)

−∇θrj;tk Ar
(cid:17)
(cid:16)

X (i)
r

+C

(cid:16)

r + (cid:80)
s∈V \r θ∗
θ∗
(cid:16)
X (i)
r

r Br

+ θ∗

(cid:17)

(cid:17)

(cid:16)

X (i)
t

Btk

rsBs
+ (cid:80)
s∈V \r
(cid:33) (cid:27)
(cid:17)

θ∗
rsBs

(cid:16)

X (i)
s

dXr

−Ar

r + (cid:80)
θ∗
(cid:32)

s∈V \r

(cid:26)

= exp

Ar

r + t(cid:48)Btk
θ∗

(cid:17)

(cid:16)

X (i)
t

(cid:32)

(cid:32)

θ∗
rsBs

(cid:16)

X (i)
s

(cid:33)

(cid:17)

+ (cid:80)
s∈V \r

(cid:33)

−Ar

r + (cid:80)
θ∗
s∈V \r
(cid:32)

θ∗
rsBs

(cid:16)
X (i)
s

(cid:17)

−∇θrj;tk Ar
(cid:26) ∇2

= exp

r + (cid:80)
θ∗

θ∗
rsBs

(cid:33)

(cid:17)

(cid:16)

X (i)
s

s∈V \r
Ar(c)

θrj;tk ,θrj;tk

2

(cid:17)2

(cid:16)

X (i)
t

t(cid:48)2

Btk

t(cid:48)Btk
(cid:27)

(cid:17) (cid:27)

(cid:16)

X (i)
t

where c = θ∗

r + (cid:80)

s(cid:54)=r θ∗

rsBs

(cid:0)X i

s

(cid:1) + v1t(cid:48)Btk

(cid:17)

(cid:16)

X (i)
t

for some v1 ∈ [0, 1]. There-

fore,

21

Next lets deﬁne event ε1 = (cid:8) maxi,t (cid:107)Bt

Proposition 2 we get P (εc
Assumption 5 implies that

1) ≤ c1np(cid:48)−4 ((cid:80)

(cid:16)

(cid:17)

X (i)
t
t mt) ≤ c1p(cid:48)−3 ((cid:80)

(cid:107)∞ ≤ 4 logp(cid:48)(cid:9). Then, from
t mt). If t(cid:48) ≤ k2 (n, p),

1
n

1
n

n
(cid:88)

i=1
n
(cid:88)

i=1

logE

(cid:104)
exp (cid:0)t(cid:48)V i

t,jk

(cid:1) |X (i)

V \r

(cid:105)

=

1
2

∇2

θrj;tk,θrj;tk

Ar(c)Btk

(cid:17)2

(cid:16)

X (i)
t

t(cid:48)2

n
(cid:88)

(cid:104)

logE

1
n

exp (cid:0)t(cid:48)V i

t,jk

(cid:1) |X (i)

V \r

(cid:105)

≤

i=1
k1(n,p)
2

1
n

(cid:80)n

i=1 Btk

(cid:17)2

(cid:16)

X (i)
t

t(cid:48)2

Now, lets deﬁne event ε2 = (cid:8)max

≤ k4
min{2kv/3, kh+kv}. Then, by proposition (1) we obtain that if n ≥ 8 k2
h
k2
4
:

X (i)
t

(cid:80)n

Btj

i=1

1
n

t,j

(cid:16)

(cid:16)

(cid:17)(cid:17)2

(cid:9) where k4 ≤
log((cid:80)

t∈V mt)

P (εc

2) ≤ exp

−n

+ log

mt

≤ exp (−n c2)

(46)

(cid:32)

(cid:32)

(cid:33)(cid:33)

k2
4
4k2
h

(cid:88)

t∈V

Therefore, for t(cid:48) ≤ k2(n, p),

1
n

n
(cid:88)

i=1

(cid:104)

logE

exp (cid:0)t(cid:48)V i

t,jk

(cid:1) |X (i)

V \r

(cid:105)

≤

k1(n, p)k4t(cid:48)2
2

Hence, by the standard Chernoff bound technique, for t(cid:48) ≤ k2(n, p)

(cid:32)

P

2exp

n
(cid:88)

1
n
(cid:16)

|V i

t,jk| > δ | ε1, ε2
(cid:17)(cid:17)

i=1
n

(cid:16) k1(n,p)k4t(cid:48) 2
2

− t(cid:48)δ

(cid:33)

≤

Setting t(cid:48) =

, for δ ≤ k1(n, p) k2(n, p) k4, we arrive to:

(cid:33)

|V i

t,jk| > δ | ε1, ε2

≤ 2 exp

(cid:18) −nδ2

(cid:19)

2k1(n, p)k4

λ1+λ2
√

mr mt

≤ k1(n, p) k2(n, p) k4.

It then follows that δ =

δ
k1(n,p)k4
(cid:32)

n
(cid:88)

1
n

P

i=1

√

νr min

2−α
4
satisﬁes

Supposing that α
α

νr min

√

λ1+λ2
√

2−α

4

mr mt

(47)

(48)

(49)

22

(50)

(51)

(53)

(54)

(55)

(cid:32)

P

2 exp

n
(cid:88)

|V i

1
n
i=1
(cid:16) −α2
(2−α)2

t,jk| >

√

α

νr min

2 − α

λ1 + λ2
√
mr mt
4

(cid:33)

| ε1, ε2

≤

νr min n (λ1+λ2)2
32 k1(n,p) k4 mr mt

(cid:17)

Form which, we obtain the following using union bound

√

√

(cid:16)

(cid:16)

P

P

(cid:107)W n

(cid:107)W n

t (cid:107)2 > α
2−α
t (cid:107)∞ > α
2−α
(cid:16) −α2
(2−α)2

≤ 2exp

νr min (λ1+λ2)
4

|ε1, ε2

(cid:17)

≤
(cid:17)

√

νr min (λ1+λ2)
mr mt
4
νr min n (λ1+λ2)2
32 k1(n,p)k4 mr mt

|ε1, ε2

(cid:17)

+ log (νrt)

and hence,

P

√

(cid:16)
(cid:107)W n(cid:107)∞,2 > α
2−α
νr min n (λ1 + λ2)2
−α2
(2 − α)2
32 k1(n, p)k4 νr max

νr min (λ1+λ2)
4

(cid:32)

2 exp

(cid:17)

|ε1, ε2

≤

+ log (νr max) + logp

(cid:33)

(52)

Finally for λ1 + λ2 ≥ 8(2−α)

α

k1(n, p) k4

(cid:113)

νr maxlog(pνr max)
nνr min
(cid:17)

√

, we obtain

(cid:16)

P
c1p(cid:48)−3 ((cid:80)

(cid:107)W n(cid:107)∞,2 > α
2−α

νr min (λ1+λ2)
4
t mt) + exp (−c2n) + exp (−c3n)

≤

Lemma 3. Suppose that λ1+λ2 ≤
then,

C2
min
40 logp(cid:48) Dmax dr k3(n,p) ν2

r max

and (cid:107)W n

\r(cid:107)∞,2 ≤ (λ1+λ2) α

4 (2−α)

√

νr min

,

(cid:16)

(cid:107)(θ∗

P
≥ 1 − cp(cid:48)−3 ((cid:80)

r·)S − (ˆθr·)S(cid:107)∞,2 ≤ 5
t mt)

√

νrmax

Cmin

(cid:17)

(λ1 + λ2)

for some constant c > 0.

Proof. We deﬁne F (uS) as:

F (uS) = (cid:96) ((θ∗
+λ1

r·)S + uS; D) − (cid:96) ((θ∗

(cid:88)

√

r·)S ; D)
rt + urt(cid:107)2 − (cid:107)θ∗

rt(cid:107)2)

νrt ((cid:107)θ∗

t∈N (r)
+λ2 ((cid:107) (θ∗

r·)S + uS(cid:107)1 − (cid:107) (θ∗

r·)S (cid:107)1)

From the construction of ˆθr· it is clear that ˆus = (ˆθr·)S − (θ∗

r·)S minimizes F .
And since F (0) = 0, we have F (ˆus) ≤ 0. We now show that for some B > 0 with
||uS||∞,2 = B, we have F (uS) > 0. Using this and the fact that F is convex we can
then show that ||ˆuS||∞,2 ≤ B.

Let uS an arbitrary vector with ||uS||∞,2 = 5

(λ1 + λ2). Then, from the

√

νrmax

Cmin

Taylor Series expansion of log likelihood function in F , we have:

23

(56)

(57)

(58)

(59)

F (uS) = ∇(cid:96) ((θ∗

r·)S ; D)T uS

+uT
+λ1

S ∇2(cid:96) ((θ∗
r·)S + v uS) uS
√
(cid:88)
νrt ((cid:107)θ∗

rt + urt(cid:107)2 − (cid:107)θ∗

rt(cid:107)2)

t∈N (r)
+λ2 ((cid:107) (θ∗

r·)S + uS(cid:107)1 − (cid:107) (θ∗

r·)S (cid:107)1)

for some v ∈ [0, 1]

We now bound each of the terms in the right hand side of (56). From (51) and using

Cauchy-Schwarz inequality we obtain:

where the last inequality holds because α ∈ (0, 1]. Moreover, from triangle in-

equality we have:

r·)S ; D)T uS

(cid:12)
(cid:12)
(cid:12)∇(cid:96) ((θ∗
≤ (cid:107)∇(cid:96) ((θ∗
≤ (cid:107)∇(cid:96) ((θ∗
≤ α
2−α
= 5 νr max
4 Cmin

(cid:12)
(cid:12)
(cid:12)
r·)S ; D) (cid:107)∞ (cid:107)uS(cid:107)1
√
r·)S ; D) (cid:107)∞ dr
λ1+λ2
5
dr νr max
4
Cmin
dr (λ1 + λ2)2

νr max (cid:107)uS(cid:107)∞,2
(λ1 + λ2)

(cid:88)

√

λ1

t∈S
≥ −λ1

rt + urt(cid:107)2 − (cid:107)θ∗
νrt ((cid:107)θ∗
√

(cid:88)

νrt(cid:107)urt(cid:107)2

rt(cid:107)2)

t∈N (r)
√

≥ −λ1 dr
= − 5 dr νr max

νr max (cid:107)uS(cid:107)∞,2
λ1 (λ1 + λ2)

Cmin

r·)S + uS(cid:107)1 − (cid:107) (θ∗

r·)S (cid:107)1)

λ2 ((cid:107) (θ∗
≥ −λ2 (cid:107)uS(cid:107)1
≥ −λ2 (cid:107)uS(cid:107)1
≥ −λ2 dr
= − 5 νr max
Cmin

√

νr max (cid:107)uS(cid:107)∞,2
dr λ2 (λ1 + λ2)

Also,

On the other hand, by Taylor’s approximation of ∇2(cid:96), there exists αjk ∈ [0, 1] and

˜ui
jk between θ\r and θ\r + vuS such that
r·)S + v uS)(cid:1)
(cid:0)∇2(cid:96) ((θ∗
(cid:0)∇2(cid:96) ((θ∗
Λmin

r·)S + β uS)(cid:1)

Λmin
≥ min
β∈[0,1]
≥ Λmin (Qn

max
v∈[0,1]

max
(cid:107)y(cid:107)≤1

SS) −
(cid:40)

n
(cid:88)

1
n

(cid:88)

αjk v (cid:0)∇3Ar

(cid:0)˜ui

jk

(cid:1)(cid:1)

(60)

i=1

j,k,l,t,h,s,m,s(cid:48),m(cid:48)

jkl

(cid:41)

urt;lh Bth

(cid:0)X i

t

(cid:1) ys,m,j Bsm

(cid:0)X i

s

(cid:1) Bs(cid:48)m(cid:48)

(cid:0)X i
s(cid:48)

(cid:1) ys(cid:48),m(cid:48),j(cid:48)

Consider the event ε1 as deﬁned in the previous proof. We know that P (ε1) ≥ 1 −

24

c1p(cid:48)−3 ((cid:80)

t mt). Conditioned on ε1 and using Assumption 5 we arrive to the following:

(cid:0)∇2(cid:96) ((θ∗

r·)S + v uS)(cid:1)
Λmin
≥ Cmin − 4logp(cid:48)(cid:107)uS(cid:107)1 Dmax νr maxk3 (n, p)
√
≥ Cmin − 4logp(cid:48)dr
≥ Cmin
2

νr max ||uS||∞,2 Dmax νr maxk3 (n, p)

(61)

where the last inequality holds for λ1 + λ2 ≤

C2
min
40 logp(cid:48) Dmax dr k3(n,p) ν2

.

r max

Finally using the above bounds we arrive at the following:

F (uS) ≥ dr νr max

(cid:18)

(λ1 + λ2)2

5
Cmin

1
4

(cid:19)

5
2

−1 −

+

> 0

(62)

Therefore

(cid:107)(θ∗

r·)S − (ˆθr·)S(cid:107)∞,2 ≤

(λ1 + λ2)

√

5

νrmax

Cmin

Lemma 4. Suppose that λ1 + λ2 ≤ α
2−α

α (λ1+λ1)

νr min

√

4 (2−α)

, then,

√

νr min C2
min

400 ν

5
2
r maxlogp(cid:48) Dmax k3(n,p) dr

and (cid:107)W n

\r(cid:107)∞,2 ≤

P

(cid:18) (cid:107)Rn(cid:107)∞,2
λ1 + λ2

≤

(cid:19)

√

νr min
α
4 (2 − α)

≥ 1 − cp(cid:48)−3

(cid:32)

(cid:88)

(cid:33)

mt

t

for some constant c > 0.

j = (cid:2)∇2(cid:96)(θ∗
Proof. Recall that Rn
notes the j-th row of a matrix. Let us also refer to Rn
sponding to θrj;tk. Then,

r·; D) − ∇2(cid:96)(¯θj

r·; D)(cid:3)T

(cid:17)

(cid:16)ˆθr· − θ∗

j de-
t;jk to the cordinate of Rn corre-

where (cid:2) · (cid:3)T

r·

j

Rn

t;jk =

1
n

Btk(X i
t )

(cid:34)

n
(cid:88)

i=1

(cid:34)
∇2Ar


θ∗

r +

(cid:88)

s(cid:54)=r



rsBs(X i
θ∗
s)

 −

∇2Ar

(cid:16)¯θtjk

r + (cid:80)

¯θtjk
rs Bs(X i
s)

s(cid:54)=r

(cid:35)T

⊗ Bi
r·

(cid:16)ˆθr· − θ∗

r·

(cid:17)

(cid:35)

(cid:17)

j

with Bi

the notation (cid:104)θr·, Bi
obtain

r· the vector of sufﬁcient statistics evluate at the i-th sample. Intoroducing
(cid:1), from the mean value theorem we
(cid:0)X i
s

r·(cid:105) =: θr + (cid:80)

s(cid:54)=r θrsBs

∇2Ar;jl
−vi
j,l

(cid:0)(cid:104)θ∗
(cid:2)(cid:104)ˆθr· − θ∗

r·, Bi

r·(cid:105)(cid:1) − ∇2Ar;jl
(cid:1)
r·(cid:105)(cid:3) (cid:0)∇3Ar

r·, Bi

(cid:16)

(cid:17)

(cid:104)¯θtjk
(cid:16)

r· , Bi
(cid:104)¯¯θi;tjk

r·(cid:105)
, Bi

r·

=
(cid:17)

r·(cid:105)

jl:

Therefore, combining (64) with (65) and using basic properties of krockner product we

(63)

(64)

(65)

25

ontain that

which implies

(cid:12)
(cid:12)
(cid:12)Rn

t;jk

n
(cid:88)

n

t )(cid:12)
(cid:12)
(cid:12) νr maxk3 (n, p) Dmax (cid:107)ˆθr· − θ∗
(cid:12)Btk(X i

(cid:12)
(cid:12) ≤ 1
(cid:12)
≤ 4 logp(cid:48) νr maxk3 (n, p) Dmax (cid:107)ˆθr· − θ∗

i=1

r·(cid:107)2
2

r·(cid:107)2
2

(cid:107)Rn
t (cid:107)∞,2 ≤
√
νr max logp(cid:48) νr maxk3 (n, p) Dmax (cid:107)ˆθr· − θ∗
4
√
25 dr νrmax
νr max logp(cid:48) νr maxk3 (n, p) Dmax
4
C2
min
λ1+λ2
4

νr min

2−α

√

α

r·(cid:107)2
2 ≤
λ2
1 ≤

with probbility at least 1 − cp(cid:48)−3 ((cid:80)

t mt).

We now prove theorem 2 using lemmas 2-4. Recalling that Qn = ∇2(cid:96)(θ∗

r·; D) and
the fact that we have set (ˆθr·)Sc = 0 in our primal-dual construction, we can rewrite
condition (43) as the following equations:

Qn
W n

ScS[(ˆθr·)S − (θ∗
(cid:80)
Sc − λ1

t /∈N (r)

r·)S] =
√

νrt ˆZ1,rt − λ2 ˆZ2,Sc + Rn
Sc

Qn
W n

SS[(ˆθr·)S − (θ∗
(cid:80)
S − λ1

t∈N (r)

r·)S] =
√

νrt ˆZ1,rt − λ2 ˆZ2,S + Rn
S

Since the matrix Qn

SS is invertible, the conditions (68) and (69) can be rewritten as

:

Qn

ScS(Qn

SS)−1[W n

W n

Sc − λ1

(cid:80)

t /∈N (r)

S − λ1
√

t∈N (r)
νrt ˆZ1,rt − λ2 ˆZ2,Sc + Rn
Sc

(cid:88)

√

νrt ˆZ1,rt − λ2 ˆZ2,S + Rn

S] =

Rearranging yields the following condition:

√

(cid:80)
λ1
t /∈N (r)
Sc + Rn
W n
λ2 ˆZ2,Sc + Qn

νrt ˆZ1,rt =
ScS(Qn

Sc − Qn

ScS(Qn

SS)−1[λ1

SS)−1[W n

S + Rn
S]−
√
t∈N (r)

(cid:80)

νrt ˆZ1,rt + λ2 ˆZ2,S]

Strict Dual Feasibility: we now show that for the dual sub-vector ˆZ1,Sc, we have
< 1. We get the following equation from 71, by applying triangle inequal-

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)∞,2
(cid:12)

(cid:12)
(cid:12)
ˆZ1,Sc
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ity:

(66)

(67)

(68)

(69)

(70)

(71)

26

νr min
(cid:80)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ˆZ1,Sc
√

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)∞,2
(cid:12)
νrt ˆZ1,rt

t /∈N (r)

√

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

λ1

λ1
(cid:104)

√

||W n||∞,2 + ||Rn||∞,2
+λ2
(cid:12)
+ (cid:12)
(cid:12)Qn
(cid:12)

νr max
ScS(Qn

(cid:12)
SS)−1(cid:12)
(cid:12)∞,2
(cid:12)

≤
(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤
(cid:12)∞,2
(cid:12)
(cid:105) (cid:16)
(cid:12)
1 + (cid:12)
(cid:12)Qn
(cid:12)

(cid:2)(λ1 + λ2)

dr νr max

(cid:3)

√

ScS(Qn

(cid:12)
SS)−1(cid:12)
(cid:12)∞,2
(cid:12)

√

(cid:17)

dr

(72)

where νr min = min

νrt , νr max = max

νrt and dr = |N (r)| Using mutual incoher-

ence bound 2 on the above equation gives us:

t

t

(cid:12)
(cid:12)
ˆZ1,Sc
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
√
νr min
√
√

λ1
+ λ2
λ1

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)∞,2
(cid:12)
(cid:104)
||W n||∞,2 + ||Rn||∞,2
(2 − α)
√
(cid:104)
(cid:12)
SS)−1(cid:12)
(cid:12)∞,2
(cid:12)

(cid:12)
1 + (cid:12)
(cid:12)Qn
(cid:12)

ScS(Qn

νr max
νr min

(cid:105)

(cid:16) λ1
λ2

dr

(cid:17)(cid:105)

+ 1

Using the previous lemmas we obtain the following:

(cid:12)
(cid:12)
ˆZ1,Sc
(cid:12)
(cid:12)
(cid:12)
(cid:12)
√
+ λ2
√
λ1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)∞,2
(cid:12)
(cid:104)
νr max
νr min

≤ 1
2λ1
1 + mmin
mmax

[α (λ1 + λ2)]

(1 − α)

(cid:16) λ1
λ2

(cid:17)(cid:105)

+ 1

(cid:18)

If λ2 <

α
2−α+2

√
√

νr max
νr min

(cid:19)

λ1, then,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ˆZ1,Sc

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)∞,2
(cid:12)

< 1

(73)

(74)

(75)

We have shown that the dual is strictly feasible with high probability and also the
solution is unique. And hence based on Lemma 1 the method correctly excludes all
edges not in the set of edges.

Correct Neighbourhood Recovery: To show that all correct neighbours are recov-

ered, it sufﬁces to show that

(cid:107)(θ∗

r·)S − (ˆθr·)S(cid:107)∞,2 ≤

θmin
2

where θmin = mint∈V \r ||θrt||2.

Using Lemma 3 we can show the above inequality holds if θmin ≥ 10

√

νr max

Cmin

(λ1 + λ2)

C Full MyFitnessPal Graph

Figure 3 shows a high-level view of the entire VS-MRF learned from the MyFitnessPal
food database. The three macro-nutrients (fat, carbs, and protein) correspond to the

27

Figure 3: Full MRF learned from the MyFitnessPal food database. The hubs cor-
respond to point-inﬂated gamma nutrient nodes, with the three largest hubs being the
macro-nutrients (fat, carbs, and protein).

three largest hubs with the remaining nine micro-nutrients representing smaller hubs.

References

S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2009.

S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and
statistical learning via the alternating direction method of multipliers. Foundations
and Trends R(cid:13) in Machine Learning, 3(1):1–122, 2011.

J. Friedman, T. Hastie, and R. Tibshirani. A note on the group lasso and a sparse group
lasso. arXiv preprint arXiv:1001.0736, 2010. URL http://arxiv.org/abs/
1001.0736.

A. Jalali, P. Ravikumar, V. Vasuki, and S. Sanghavi. On learning discrete graphical

models using group-sparse regularization. AI STAT, 2011.

28

S. L. Lauritzen. Graphical models. Oxford University Press, 1996.

N. Parikh and S. Boyd. Proximal algorithms. Foundations and Trends in Optimization,

1(3):123–231, 2013.

N. Simon, J. Friedman, T. Hastie, and R. Tibshirani. A sparse-group lasso. Journal of

Computational and Graphical Statistics, 22(2):231–245, 2013.

E. B. Sudderth, A. T. Ihler, M. Isard, W. T. Freeman, and A. S. Willsky. Nonparametric

belief propagation. Communications of the ACM, 53(10):95–103, 2010.

D. Vats and J. M. Moura. Finding non-overlapping clusters for generalized inference
over graphical models. Signal Processing, IEEE Transactions on, 60(12):6368–
6381, 2012.

M. J. Wainwright. Sharp thresholds for noisy and high-dimensional recovery of spar-
sity using l1-constrained quadratic programming (lasso). IEEE Transactions on In-
formation Theory, pages 2183–2202, 2009.

M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and vari-
ational inference. Foundations and Trends R(cid:13) in Machine Learning, 1(1-2):1–305,
2008.

E. Yang, G. Allen, Z. Liu, and P. Ravikumar. Graphical models via generalized linear
models. In Advances in Neural Information Processing Systems, pages 1358–1366,
2012.

E. Yang, Y. Baker, P. Ravikumar, G. Allen, and Z. Liu. Mixed graphical models via
exponential families. Proceedings of the Seventeenth International Conference on
Artiﬁcial Intelligence and Statistics, pages 1042–1050, 2014.

29

