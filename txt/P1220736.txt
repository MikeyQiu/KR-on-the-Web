MultiVec: a Multilingual and Multilevel Representation
Learning Toolkit for NLP

Alexandre B´erard†‡, Christophe Servan‡, Olivier Pietquin†∗ and Laurent Besacier‡∗
†Univ. Lille, CNRS, Centrale Lille, Inria UMR 9189 - CRIStAL, F-59000 Lille, France
alexandre.berard@ed.univ-lille1.fr
olivier.pietquin@univ-lille1.fr

‡LIG, Univ. Grenoble Alpes
Campus Saint-Martin d’H`eres, Grenoble, France
christophe.servan@imag.fr
laurent.besacier@imag.fr

∗Institut Universitaire de France

Abstract
We present MultiVec, a new toolkit for computing continuous representations for text at different granularity levels (word-level or
sequences of words). MultiVec includes Mikolov et al. [2013b]’s word2vec features, Le and Mikolov [2014]’s paragraph vector (batch
and online) and Luong et al. [2015]’s model for bilingual distributed representations. MultiVec also includes different distance measures
between words and sequences of words. The toolkit is written in C++ and is aimed at being fast (in the same order of magnitude
as word2vec), easy to use, and easy to extend. It has been evaluated on several NLP tasks: the analogical reasoning task, sentiment
analysis, and crosslingual document classiﬁcation.

Keywords: Word embeddings, paragraph vector, bilingual word embeddings, crosslingual document classiﬁcation

1.

Introduction

There has been a growing interest in distributed representa-
tions for text, largely due to Mikolov et al. [2013a] who pro-
pose simple models which can be trained on huge amounts
of data. A number of contributions have extended this work
to phrases [Mikolov et al., 2013b], text sequences [Le and
Mikolov, 2014], bilingual distributed representations [Lu-
ong et al., 2015] [Gouws et al., 2015], or bilingual repre-
sentations for text sequences [Pham et al., 2015].
these techniques have ofﬁcial or
Although most of
non-ofﬁcial
implementations (word2vec, bivec, gensim
[ ˇReh˚uˇrek and Sojka, 2010], etc.), there has been no con-
certed effort to regroup all of these techniques in a single
toolkit.
Contribution This paper presents MultiVec, a toolkit
which enables the generation and manipulation of multi-
lingual vector representations at several granularity levels
(from word to any sequence of words). MultiVec com-
bines several techniques of the literature: it includes most
of word2vec’s features [Mikolov et al., 2013a] for learning
distributed word representations (also known as word em-
beddings); as well as an implementation of Le and Mikolov
[2014]’s paragraph vector. In addition, MultiVec can com-
pute bilingual representations on a parallel corpus using Lu-
ong et al. [2015]’s bivec model. The code (provided on
GitHub) is written in C++, and is fast, easy to use and read-
able. The models can also be used from Python code thanks
to a Python wrapper.
We provide the results and code for a number of compar-
isons and benchmarks that test the usability of this toolkit

against other toolkits in the literature.
The MultiVec toolkit has two main components: the ﬁrst
component enables the generation of new models, while the
second component uses those models to compute distances
between words or sequences. It also includes a series of
benchmarks that makes possible the evaluation of trained
models on different NLP tasks.
Outline The rest of this paper goes simply as follows: we
ﬁrst describe the models that can be trained using Multi-
Vec. Then, we describe the distance computation features.
Finally, we present the benchmarks and their results.

2. Models

2.1. Word embeddings
Mikolov et al. [2013a] offer a simpliﬁed version of Ben-
gio et al. [2003]’s neural language model, with a number
of tricks to boost performance. They present two models:
the continuous bag-of-words model (CBOW), and the skip-
gram model.
Given a sequence of words (w1, . . . , wN ),
the CBOW
model learns to predict all words wi from their surround-
ing words (wi−k, . . . , wi−1, wi+1, . . . , wi+k). The training
objective is to ﬁnd the parameters Ci and Co that maximize
the log-likelihood of the training corpus:

N
(cid:88)

i=1

log ˆP (x = wi|context = wi−k . . . wi+k)

(1)

where the conditional probability ˆP (x = w|context) of a
word given its context is estimated using the softmax func-

4188

(2)

(3)

(4)

tion over the entire vocabulary V :

with:

ˆP (x = w|context) =

eyw
ˆw∈V ey ˆw

(cid:80)

yw =

1
2k

(

(cid:88)

x∈context

Ci(x)) · Co(w)

Training is done with the stochastic gradient ascent algo-
rithm, which updates the parameters θ = (Ci, Co) after
each word w:

θ ← θ + α

∂ log ˆP (w|context)
∂θ

Ci and Co are the input and output weight matrices, which
map each vocabulary word w to a weight vector Ci/o(w) of
size D. As shown in [Mikolov et al., 2013a], after training
on a large corpus of text, the embeddings of a word Ci(w)
and Co(w)1 exhibit very interesting linguistic properties.
The skip-gram model has a similar objective function. The
key difference is that it uses the current word to predict the
context words (reversed direction).

Figure 1: The CBOW model predicts wt based on the con-
text and the skip-gram model predicts the context (sur-
rounding words) given wt.

Evaluating the softmax function is very expensive, as it in-
volves D2 × |V | computation steps. To avoid this, Mikolov
et al. [2013a] propose to use hierarchical softmax, which
reduces the complexity to D2 × log(|V |).
Another way to reduce complexity is to use a different train-
ing objective, in which instead of predicting words, we
predict whether a word is correct or not. This method is
called negative sampling [Mikolov et al., 2013b], or noise-
contrastive estimation [Mnih and Kavukcuoglu, 2013].
MultiVec includes both the CBOW and skip-gram model,
as well as the hierarchical softmax and negative sampling
training algorithms.

2.2. Paragraph vector
Paragraph vector was introduced by Le and Mikolov
[2014]. Most machine learning algorithms require their in-
put to be ﬁxed-size vectors. Hence, in order to be processed

1word2vec only exports the input weights. Our toolkit lets the

user export either of them or a sum or concatenation of both.

4189

by such algorithms, variable-size text sequences need to
be transformed into a ﬁxed-size representation. This is of-
ten done by using the so-called bag-of-words model, which
sums the ﬁxed-size representations of the individual com-
ponents (words or n-grams) of a text sequence.
These representations are either one-hot vectors (whose di-
mension is the size of the vocabulary), or more compact
vector representations (e.g. distributed representations like
word2vec). Even though this method is widely used in NLP
and IR – and is good enough in some cases – it presents
some serious limitations, in particular the loss of any infor-
mation about word order.
Paragraph vector is an alternative representation that alle-
viates some of these limitations. The architecture is very
similar to the CBOW model in word2vec.2
It only adds
a weight bias vector to the projection layer (of the same
size as the word vectors) for each sentence of the corpus.
Once the model trained on the entire corpus, each sentence
has its distributed representation, which is its correspond-
ing weight vector.

2.2.1. Online paragraph vector
The above method only works for training paragraph vec-
tors in a batch fashion (when the whole corpus is available
at once). It is also possible to pre-train a model on a given
corpus, and to infer paragraph vectors for new sentences
that were not seen in the training corpus. This is done by
doing gradient descent on a sentence as usual while freez-
ing the word weights. Le and Mikolov [2014] refer to this
method as inference step. To the best of our knowledge,
ours is the only implementation of this feature.

2.3. Bilingual word embeddings
Word embeddings can be used in multilingual tasks (e.g.
machine translation or crosslingual document classiﬁca-
tion) by training a model independently for each language.
However, the resulting representations will be in a differ-
ent vector space: similar words in different languages will
likely have very different representations.
There exist several methods to solve this problem: it is pos-
sible to train both models independently and then learn a
mapping from one representation to the other; one can also
constrain the training to keep the representations of similar
words close to each other; or the training can be performed
jointly using a parallel corpus.
Luong et al. [2015]’s bivec falls into the latter category.
This method is especially interesting because it stems di-
rectly from Mikolov et al. [2013a]’s word2vec and is thus
very easy to implement into our architecture, while provid-
ing excellent results both on bilingual tasks and monolin-
gual tasks.
For each pair of sentences in a parallel corpus, bivec tries to
predict words in the same sentence like word2vec does, but
also uses words in the source sentence to predict words in
the target sentence (and conversely). Thus, for each update
in word2vec, bivec performs 4 updates: source to source,
source to target, target to target and target to source.

2We describe only the distributed memory model (DM), as the
distributed bag-of-word (DBOW) model is not yet implemented
in MultiVec.

3. Distance measures

Word embeddings can be used to detect near matches be-
tween words. A near match is when two words differ only
in terms of morphology or inﬂection or when they are syn-
onyms or closely related semantically. Near matches are
useful to a number of NLP domains, including Information
Retrieval and Machine Translation.
The usual way to detect near matches is by using linguistic
resources, like WordNet [Princeton University, 2012], Ba-
belNet [Navigli and Ponzetto, 2012] or Dbnary [S´erasset,
2012]. As an alternative to these linguistic databases, our
toolkit can detect near matches by measuring the cosine
similarity or cosine distance between word representations.

3.1. N -gram comparison

There exist several ways to compare two sequences to each
other. As text sequences differ in length, a common way
is to use the bag-of-words model which sums the represen-
tations of each word of the sequence. This method can be
applied to any size of sequence.
We propose a similarity measure for sequences of identi-
cal length, typically n-grams. As shown in equation 5, the
similarity between the two n-grams s = (w1, w2, . . . , wn),
s(cid:48) = (w(cid:48)
n) is obtained by comparing their vec-
tor representations (v1, . . . , vn) and (v(cid:48)
n) element-
wise. Contrary to the bag-of-words model, this method is
sensitive to word order.

2, . . . , w(cid:48)

1, . . . , v(cid:48)

1, w(cid:48)

S(s, s(cid:48)) =

2) + · · · + Scos(vn, v(cid:48)

n)

(5)

(cid:107)v(cid:107)(cid:107)v(cid:48)(cid:107) is the cosine similarity be-

Scos(v1, v(cid:48)

1) + Scos(v2, v(cid:48)
n
where Scos(v, v(cid:48)) = vT v(cid:48)
tween vector v and vector v(cid:48).

4. Benchmarks

This section reports the results of a series of experiments
that compares MultiVec to other existing toolkits in the lit-
erature.
The main goal is to show that the techniques are correctly
implemented by comparing our results with their ofﬁcial
implementations (when they exist). We performed exper-
iments on three different tasks:
the analogical reasoning
task for evaluating the standard (monolingual) word em-
beddings, the sentiment analysis task for paragraph vector,
and the crosslingual document classiﬁcation (CLDC) task
for bilingual representations and paragraph vector.

4.1. Analogical reasoning task
We evaluate our toolkit on the analogical reasoning task
as described in [Mikolov et al., 2013a]. The authors
provide a dataset containing ﬁve different types of se-
mantic questions, and nine types of syntactic questions,
with a total of 19,558 questions. A question is a tuple
(word1, word2, word3, word4) in which word4 is related
(semantically or syntactically) to word3, in the same way
that word2 is related to word1. A famous example is
(king, man, queen, woman).
It has been observed that

Method

word2vec

MultiVec

MV-bi

SG

Model

CBOW

Dim Synt.
100
300
100
300
100
300
100
300
CBOW 300

35
38.6
34.4
30.6
36.3
39.6
33.5
30.8
44

CBOW

SG

Sem.
11.9
16
16.8
18.2
11.9
16
17.1
20.6
18.6

Total
28.4
32.1
29.3
27.1
29.3
32.8
28.8
27.9
36.7

Time
4
13
16
49
10
17
28
60
26

Table 1: Results (precision) of the analogical reasoning
task, on word2vec’s questions-words.txt. The
models were trained on English Europarl for 20 iterations,
with negative sampling (5 samples), with a subsampling
rate of 10−4, a window size of 5 and initial training rate
of 0.05. MV-bi is our bilingual implementation, trained on
English-German Europarl for 10 iterations. Training time
is given in minutes.

C(king) − C(man) ≈ C(queen) − C(woman), corre-
sponding to some sort of royalty concept. This task evalu-
ates the ability of the model to capture several kinds of lin-
guistic regularities. Other types of questions include for ex-
ample state-city relationships or adjective-adverb relation-
ships.
The precision as measured in this task is the percentage of
questions for which the closest word in the vocabulary to
word3 − word1 + word2 according to the cosine similarity
is exactly word4.
As shown in table 1, word2vec and MultiVec with the same
settings get very similar results.

Interestingly, bilingual models seem to perform signif-
icantly better, even on a monolingual task. The number
of epochs was intentionally halved in the bilingual case, to
make sure that this result is not simply due to a higher num-
ber of updates.

4.2. Sentiment analysis task

We evaluate our implementation of paragraph vector on the
sentiment analysis task. The same experimental protocol
as [Le and Mikolov, 2014] is used3. The IMDb dataset
contains 100,000 documents. 50,000 of those are labeled
with a positive or negative label, and 50,000 are unlabeled.
The representations of 25,000 labeled documents are used
as training examples for an SVM classiﬁer. The remaining
25,000 labeled documents are used as test examples.
Table 2 reports the results of the different models. We com-
pare the batch paragraph vector implementation provided
by Le and Mikolov [2014] with our batch and online im-
plementations. We also report results obtained by simply
averaging word embeddings.
As Mesnil et al. [2014] remarked, the results in the original
paper were obtained on unshufﬂed data. This explains why
the results reported here are much lower for MultiVec as
well as word2vec.

3A training script and a modiﬁed version of word2vec was pro-

vided by the authors on the word2vec Google group.

4190

Toolkit
word2vec

MultiVec

Method
batch par. vector
batch par. vector
online par. vector
online par. vector
bag-of-words
bag-of-words

Training data Accuracy

train+test
train+test
train
europarl
train
europarl

87.5
87.8
86.2
78
88.3
77.7

Table 2: Results of the sentiment analysis task on the IMDb
dataset. The batch models were trained on training and test
data. The online models were trained on either the training
data or English Europarl. The settings are: CBOW on 40
iterations, with 15 negative samples, a dimension of 100,
window size of 10, learning rate of 0.05 and subsampling
of 10−4.

Dim

Toolkit

Method

40

128

bivec

MultiVec

bivec

MultiVec

bag-of-words
bag-of-words
par. vector
bag-of-words
bag-of-words
par. vector

Accuracy [%]

en→de
86.1
88.1
88.4
89.0
88.9
88.2

de→en
74.4
75.3
77.6
78.6
76.4
79.1

Table 3: Results obtained within the framework of the
CLDC task using the RCV corpus. en → de signiﬁes train-
ing on English data and testing on German data; de → en
is the reverse. The settings are the same as those in [Lu-
ong et al., 2015]: skip-gram model, 30 negative samples,
10 epochs.

4.3. Crosslingual document classiﬁcation task

To evaluate the quality of our bilingual word embeddings,
we reproduce Klementiev et al. [2012]’s experiments on the
crosslingual document classiﬁcation task. This task con-
sists in classifying documents in a language using a model
that was trained with documents from another language.
Like Klementiev et al. [2012] and Luong et al. [2015], 1000
documents from the RCV corpus are used for training, and
5000 documents for testing. Each document belongs to one
of 4 categories. Document representations are computed by
doing a weighted sum of word embeddings, according to
pre-deﬁned word frequencies (TF-IDF). A perceptron clas-
siﬁer is then trained on the source-language documents and
evaluated on target-language documents.4
We compare bilingual models trained with bivec and Mul-
tiVec. We also show results obtained by computing doc-
ument representations with online paragraph vector. To
do so, we export the previously trained bilingual model to
source and target models, which are then used to compute
paragraph vector representations for source and target doc-
uments.
Table 3 shows similar results for both MultiVec and bivec.
Paragraph vector does no better than the bag-of-words rep-
resentation, but the results conﬁrm that our approach for
computing bilingual paragraph vectors is sound.

5. Conclusion
In this paper we presented MultiVec, a toolkit which aggre-
gates a number of techniques in the literature that compute
distributed representations of text.
It includes word2vec,
paragraph vector (batch and online), and bivec. All these
techniques ﬁt nicely and are interoperable with each other.
The toolkit is designed for being easy to set-up and use,
while also being easy to dive into. The project is fully open
to future contributions.
The code is provided on the project webpage5 with instal-
lation instructions and command-line usage examples.
As future work, we plan on implementing a number of
features,
including but not only (see project webpage)
bivec’s UnsupAlign model, which uses word alignment in-
formation (from GIZA++); and the distributed bag-of-word
(DBOW) model for paragraph vector.

Acknowledgments
This work was supported by the KEHATH project funded
by the French National Agency for Research (ANR) under
the grant number ANR-14-CE24-0016-03 .

References
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neu-
ral probabilistic language model. The Journal of Ma-
chine Learning Research (JMLR), 3:1137–1155, 2003.

S. Gouws, Y. Bengio, and G. Corrado.

BilBOWA:
Fast Bilingual Distributed Representations without Word
Alignments. In Proceedings of the International Confer-
ence on Machine Learning (ICML), 2015.

A. Klementiev, I. Titov, and B. Bhattarai. Inducing crosslin-
gual distributed representations of words. In Proceedings
of the International Conference on Computational Lin-
guistics (COLING), 2012.

Q. V. Le and T. Mikolov. Distributed Representations of
In Proceedings of the In-
Sentences and Documents.
ternational Conference on Machine Learning (ICML),
2014.

T. Luong, H. Pham, and C. D. Manning. Bilingual word
representations with monolingual quality in mind.
In
Proceedings of the 1st Workshop on Vector Space Mod-
eling for Natural Language Processing, 2015.

G. Mesnil, M. Ranzato, T. Mikolov, and Y. Bengio. En-
semble of generative and discriminative techniques for
sentiment analysis of movie reviews. arXiv:1412.5335
[cs], 2014.

T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient
Estimation of Word Representations in Vector Space. In
The Workshop Proceedings of the International Confer-
ence on Learning Representations (ICLR), May 2013a.

T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. Distributed Representations of Words and
Phrases and their Compositionality. In Advances in Neu-
ral Information Processing Systems (NIPS), 2013b.

4The data splits and training scripts were obtained from the

authors.

5https://github.com/eske/multivec

4191

A. Mnih and K. Kavukcuoglu. Learning word embed-
dings efﬁciently with noise-contrastive estimation.
In
Advances in Neural Information Processing Systems
(NIPS), 2013.

R. Navigli and S. P. Ponzetto. BabelNet: The Automatic
Construction, Evaluation and Application of a Wide-
Coverage Multilingual Semantic Network. Artiﬁcial In-
telligence, 193:217–250, 2012.

H. Pham, M.-T. Luong, and C. D. Manning. Learning
Distributed Representations for Multilingual Text Se-
quences. In Proceedings of NAACL-HLT, 2015.

Princeton University. About WordNet. Technical report,

Princeton University, 2012.

R. ˇReh˚uˇrek and P. Sojka. Software Framework for Topic
In Proceedings of the
Modelling with Large Corpora.
LREC 2010 Workshop on New Challenges for NLP
Frameworks, 2010.

G. S´erasset. Dbnary: Wiktionary as a LMF based Multi-
lingual RDF network. In Proceedings of the Language
Resources and Evaluation Conference (LREC), 2012.

4192

MultiVec: a Multilingual and Multilevel Representation
Learning Toolkit for NLP

Alexandre B´erard†‡, Christophe Servan‡, Olivier Pietquin†∗ and Laurent Besacier‡∗
†Univ. Lille, CNRS, Centrale Lille, Inria UMR 9189 - CRIStAL, F-59000 Lille, France
alexandre.berard@ed.univ-lille1.fr
olivier.pietquin@univ-lille1.fr

‡LIG, Univ. Grenoble Alpes
Campus Saint-Martin d’H`eres, Grenoble, France
christophe.servan@imag.fr
laurent.besacier@imag.fr

∗Institut Universitaire de France

Abstract
We present MultiVec, a new toolkit for computing continuous representations for text at different granularity levels (word-level or
sequences of words). MultiVec includes Mikolov et al. [2013b]’s word2vec features, Le and Mikolov [2014]’s paragraph vector (batch
and online) and Luong et al. [2015]’s model for bilingual distributed representations. MultiVec also includes different distance measures
between words and sequences of words. The toolkit is written in C++ and is aimed at being fast (in the same order of magnitude
as word2vec), easy to use, and easy to extend. It has been evaluated on several NLP tasks: the analogical reasoning task, sentiment
analysis, and crosslingual document classiﬁcation.

Keywords: Word embeddings, paragraph vector, bilingual word embeddings, crosslingual document classiﬁcation

1.

Introduction

There has been a growing interest in distributed representa-
tions for text, largely due to Mikolov et al. [2013a] who pro-
pose simple models which can be trained on huge amounts
of data. A number of contributions have extended this work
to phrases [Mikolov et al., 2013b], text sequences [Le and
Mikolov, 2014], bilingual distributed representations [Lu-
ong et al., 2015] [Gouws et al., 2015], or bilingual repre-
sentations for text sequences [Pham et al., 2015].
these techniques have ofﬁcial or
Although most of
non-ofﬁcial
implementations (word2vec, bivec, gensim
[ ˇReh˚uˇrek and Sojka, 2010], etc.), there has been no con-
certed effort to regroup all of these techniques in a single
toolkit.
Contribution This paper presents MultiVec, a toolkit
which enables the generation and manipulation of multi-
lingual vector representations at several granularity levels
(from word to any sequence of words). MultiVec com-
bines several techniques of the literature: it includes most
of word2vec’s features [Mikolov et al., 2013a] for learning
distributed word representations (also known as word em-
beddings); as well as an implementation of Le and Mikolov
[2014]’s paragraph vector. In addition, MultiVec can com-
pute bilingual representations on a parallel corpus using Lu-
ong et al. [2015]’s bivec model. The code (provided on
GitHub) is written in C++, and is fast, easy to use and read-
able. The models can also be used from Python code thanks
to a Python wrapper.
We provide the results and code for a number of compar-
isons and benchmarks that test the usability of this toolkit

against other toolkits in the literature.
The MultiVec toolkit has two main components: the ﬁrst
component enables the generation of new models, while the
second component uses those models to compute distances
between words or sequences. It also includes a series of
benchmarks that makes possible the evaluation of trained
models on different NLP tasks.
Outline The rest of this paper goes simply as follows: we
ﬁrst describe the models that can be trained using Multi-
Vec. Then, we describe the distance computation features.
Finally, we present the benchmarks and their results.

2. Models

2.1. Word embeddings
Mikolov et al. [2013a] offer a simpliﬁed version of Ben-
gio et al. [2003]’s neural language model, with a number
of tricks to boost performance. They present two models:
the continuous bag-of-words model (CBOW), and the skip-
gram model.
Given a sequence of words (w1, . . . , wN ),
the CBOW
model learns to predict all words wi from their surround-
ing words (wi−k, . . . , wi−1, wi+1, . . . , wi+k). The training
objective is to ﬁnd the parameters Ci and Co that maximize
the log-likelihood of the training corpus:

N
(cid:88)

i=1

log ˆP (x = wi|context = wi−k . . . wi+k)

(1)

where the conditional probability ˆP (x = w|context) of a
word given its context is estimated using the softmax func-

4188

(2)

(3)

(4)

tion over the entire vocabulary V :

with:

ˆP (x = w|context) =

eyw
ˆw∈V ey ˆw

(cid:80)

yw =

1
2k

(

(cid:88)

x∈context

Ci(x)) · Co(w)

Training is done with the stochastic gradient ascent algo-
rithm, which updates the parameters θ = (Ci, Co) after
each word w:

θ ← θ + α

∂ log ˆP (w|context)
∂θ

Ci and Co are the input and output weight matrices, which
map each vocabulary word w to a weight vector Ci/o(w) of
size D. As shown in [Mikolov et al., 2013a], after training
on a large corpus of text, the embeddings of a word Ci(w)
and Co(w)1 exhibit very interesting linguistic properties.
The skip-gram model has a similar objective function. The
key difference is that it uses the current word to predict the
context words (reversed direction).

Figure 1: The CBOW model predicts wt based on the con-
text and the skip-gram model predicts the context (sur-
rounding words) given wt.

Evaluating the softmax function is very expensive, as it in-
volves D2 × |V | computation steps. To avoid this, Mikolov
et al. [2013a] propose to use hierarchical softmax, which
reduces the complexity to D2 × log(|V |).
Another way to reduce complexity is to use a different train-
ing objective, in which instead of predicting words, we
predict whether a word is correct or not. This method is
called negative sampling [Mikolov et al., 2013b], or noise-
contrastive estimation [Mnih and Kavukcuoglu, 2013].
MultiVec includes both the CBOW and skip-gram model,
as well as the hierarchical softmax and negative sampling
training algorithms.

2.2. Paragraph vector
Paragraph vector was introduced by Le and Mikolov
[2014]. Most machine learning algorithms require their in-
put to be ﬁxed-size vectors. Hence, in order to be processed

1word2vec only exports the input weights. Our toolkit lets the

user export either of them or a sum or concatenation of both.

4189

by such algorithms, variable-size text sequences need to
be transformed into a ﬁxed-size representation. This is of-
ten done by using the so-called bag-of-words model, which
sums the ﬁxed-size representations of the individual com-
ponents (words or n-grams) of a text sequence.
These representations are either one-hot vectors (whose di-
mension is the size of the vocabulary), or more compact
vector representations (e.g. distributed representations like
word2vec). Even though this method is widely used in NLP
and IR – and is good enough in some cases – it presents
some serious limitations, in particular the loss of any infor-
mation about word order.
Paragraph vector is an alternative representation that alle-
viates some of these limitations. The architecture is very
similar to the CBOW model in word2vec.2
It only adds
a weight bias vector to the projection layer (of the same
size as the word vectors) for each sentence of the corpus.
Once the model trained on the entire corpus, each sentence
has its distributed representation, which is its correspond-
ing weight vector.

2.2.1. Online paragraph vector
The above method only works for training paragraph vec-
tors in a batch fashion (when the whole corpus is available
at once). It is also possible to pre-train a model on a given
corpus, and to infer paragraph vectors for new sentences
that were not seen in the training corpus. This is done by
doing gradient descent on a sentence as usual while freez-
ing the word weights. Le and Mikolov [2014] refer to this
method as inference step. To the best of our knowledge,
ours is the only implementation of this feature.

2.3. Bilingual word embeddings
Word embeddings can be used in multilingual tasks (e.g.
machine translation or crosslingual document classiﬁca-
tion) by training a model independently for each language.
However, the resulting representations will be in a differ-
ent vector space: similar words in different languages will
likely have very different representations.
There exist several methods to solve this problem: it is pos-
sible to train both models independently and then learn a
mapping from one representation to the other; one can also
constrain the training to keep the representations of similar
words close to each other; or the training can be performed
jointly using a parallel corpus.
Luong et al. [2015]’s bivec falls into the latter category.
This method is especially interesting because it stems di-
rectly from Mikolov et al. [2013a]’s word2vec and is thus
very easy to implement into our architecture, while provid-
ing excellent results both on bilingual tasks and monolin-
gual tasks.
For each pair of sentences in a parallel corpus, bivec tries to
predict words in the same sentence like word2vec does, but
also uses words in the source sentence to predict words in
the target sentence (and conversely). Thus, for each update
in word2vec, bivec performs 4 updates: source to source,
source to target, target to target and target to source.

2We describe only the distributed memory model (DM), as the
distributed bag-of-word (DBOW) model is not yet implemented
in MultiVec.

3. Distance measures

Word embeddings can be used to detect near matches be-
tween words. A near match is when two words differ only
in terms of morphology or inﬂection or when they are syn-
onyms or closely related semantically. Near matches are
useful to a number of NLP domains, including Information
Retrieval and Machine Translation.
The usual way to detect near matches is by using linguistic
resources, like WordNet [Princeton University, 2012], Ba-
belNet [Navigli and Ponzetto, 2012] or Dbnary [S´erasset,
2012]. As an alternative to these linguistic databases, our
toolkit can detect near matches by measuring the cosine
similarity or cosine distance between word representations.

3.1. N -gram comparison

There exist several ways to compare two sequences to each
other. As text sequences differ in length, a common way
is to use the bag-of-words model which sums the represen-
tations of each word of the sequence. This method can be
applied to any size of sequence.
We propose a similarity measure for sequences of identi-
cal length, typically n-grams. As shown in equation 5, the
similarity between the two n-grams s = (w1, w2, . . . , wn),
s(cid:48) = (w(cid:48)
n) is obtained by comparing their vec-
tor representations (v1, . . . , vn) and (v(cid:48)
n) element-
wise. Contrary to the bag-of-words model, this method is
sensitive to word order.

2, . . . , w(cid:48)

1, . . . , v(cid:48)

1, w(cid:48)

S(s, s(cid:48)) =

2) + · · · + Scos(vn, v(cid:48)

n)

(5)

(cid:107)v(cid:107)(cid:107)v(cid:48)(cid:107) is the cosine similarity be-

Scos(v1, v(cid:48)

1) + Scos(v2, v(cid:48)
n
where Scos(v, v(cid:48)) = vT v(cid:48)
tween vector v and vector v(cid:48).

4. Benchmarks

This section reports the results of a series of experiments
that compares MultiVec to other existing toolkits in the lit-
erature.
The main goal is to show that the techniques are correctly
implemented by comparing our results with their ofﬁcial
implementations (when they exist). We performed exper-
iments on three different tasks:
the analogical reasoning
task for evaluating the standard (monolingual) word em-
beddings, the sentiment analysis task for paragraph vector,
and the crosslingual document classiﬁcation (CLDC) task
for bilingual representations and paragraph vector.

4.1. Analogical reasoning task
We evaluate our toolkit on the analogical reasoning task
as described in [Mikolov et al., 2013a]. The authors
provide a dataset containing ﬁve different types of se-
mantic questions, and nine types of syntactic questions,
with a total of 19,558 questions. A question is a tuple
(word1, word2, word3, word4) in which word4 is related
(semantically or syntactically) to word3, in the same way
that word2 is related to word1. A famous example is
(king, man, queen, woman).
It has been observed that

Method

word2vec

MultiVec

MV-bi

SG

Model

CBOW

Dim Synt.
100
300
100
300
100
300
100
300
CBOW 300

35
38.6
34.4
30.6
36.3
39.6
33.5
30.8
44

CBOW

SG

Sem.
11.9
16
16.8
18.2
11.9
16
17.1
20.6
18.6

Total
28.4
32.1
29.3
27.1
29.3
32.8
28.8
27.9
36.7

Time
4
13
16
49
10
17
28
60
26

Table 1: Results (precision) of the analogical reasoning
task, on word2vec’s questions-words.txt. The
models were trained on English Europarl for 20 iterations,
with negative sampling (5 samples), with a subsampling
rate of 10−4, a window size of 5 and initial training rate
of 0.05. MV-bi is our bilingual implementation, trained on
English-German Europarl for 10 iterations. Training time
is given in minutes.

C(king) − C(man) ≈ C(queen) − C(woman), corre-
sponding to some sort of royalty concept. This task evalu-
ates the ability of the model to capture several kinds of lin-
guistic regularities. Other types of questions include for ex-
ample state-city relationships or adjective-adverb relation-
ships.
The precision as measured in this task is the percentage of
questions for which the closest word in the vocabulary to
word3 − word1 + word2 according to the cosine similarity
is exactly word4.
As shown in table 1, word2vec and MultiVec with the same
settings get very similar results.

Interestingly, bilingual models seem to perform signif-
icantly better, even on a monolingual task. The number
of epochs was intentionally halved in the bilingual case, to
make sure that this result is not simply due to a higher num-
ber of updates.

4.2. Sentiment analysis task

We evaluate our implementation of paragraph vector on the
sentiment analysis task. The same experimental protocol
as [Le and Mikolov, 2014] is used3. The IMDb dataset
contains 100,000 documents. 50,000 of those are labeled
with a positive or negative label, and 50,000 are unlabeled.
The representations of 25,000 labeled documents are used
as training examples for an SVM classiﬁer. The remaining
25,000 labeled documents are used as test examples.
Table 2 reports the results of the different models. We com-
pare the batch paragraph vector implementation provided
by Le and Mikolov [2014] with our batch and online im-
plementations. We also report results obtained by simply
averaging word embeddings.
As Mesnil et al. [2014] remarked, the results in the original
paper were obtained on unshufﬂed data. This explains why
the results reported here are much lower for MultiVec as
well as word2vec.

3A training script and a modiﬁed version of word2vec was pro-

vided by the authors on the word2vec Google group.

4190

Toolkit
word2vec

MultiVec

Method
batch par. vector
batch par. vector
online par. vector
online par. vector
bag-of-words
bag-of-words

Training data Accuracy

train+test
train+test
train
europarl
train
europarl

87.5
87.8
86.2
78
88.3
77.7

Table 2: Results of the sentiment analysis task on the IMDb
dataset. The batch models were trained on training and test
data. The online models were trained on either the training
data or English Europarl. The settings are: CBOW on 40
iterations, with 15 negative samples, a dimension of 100,
window size of 10, learning rate of 0.05 and subsampling
of 10−4.

Dim

Toolkit

Method

40

128

bivec

MultiVec

bivec

MultiVec

bag-of-words
bag-of-words
par. vector
bag-of-words
bag-of-words
par. vector

Accuracy [%]

en→de
86.1
88.1
88.4
89.0
88.9
88.2

de→en
74.4
75.3
77.6
78.6
76.4
79.1

Table 3: Results obtained within the framework of the
CLDC task using the RCV corpus. en → de signiﬁes train-
ing on English data and testing on German data; de → en
is the reverse. The settings are the same as those in [Lu-
ong et al., 2015]: skip-gram model, 30 negative samples,
10 epochs.

4.3. Crosslingual document classiﬁcation task

To evaluate the quality of our bilingual word embeddings,
we reproduce Klementiev et al. [2012]’s experiments on the
crosslingual document classiﬁcation task. This task con-
sists in classifying documents in a language using a model
that was trained with documents from another language.
Like Klementiev et al. [2012] and Luong et al. [2015], 1000
documents from the RCV corpus are used for training, and
5000 documents for testing. Each document belongs to one
of 4 categories. Document representations are computed by
doing a weighted sum of word embeddings, according to
pre-deﬁned word frequencies (TF-IDF). A perceptron clas-
siﬁer is then trained on the source-language documents and
evaluated on target-language documents.4
We compare bilingual models trained with bivec and Mul-
tiVec. We also show results obtained by computing doc-
ument representations with online paragraph vector. To
do so, we export the previously trained bilingual model to
source and target models, which are then used to compute
paragraph vector representations for source and target doc-
uments.
Table 3 shows similar results for both MultiVec and bivec.
Paragraph vector does no better than the bag-of-words rep-
resentation, but the results conﬁrm that our approach for
computing bilingual paragraph vectors is sound.

5. Conclusion
In this paper we presented MultiVec, a toolkit which aggre-
gates a number of techniques in the literature that compute
distributed representations of text.
It includes word2vec,
paragraph vector (batch and online), and bivec. All these
techniques ﬁt nicely and are interoperable with each other.
The toolkit is designed for being easy to set-up and use,
while also being easy to dive into. The project is fully open
to future contributions.
The code is provided on the project webpage5 with instal-
lation instructions and command-line usage examples.
As future work, we plan on implementing a number of
features,
including but not only (see project webpage)
bivec’s UnsupAlign model, which uses word alignment in-
formation (from GIZA++); and the distributed bag-of-word
(DBOW) model for paragraph vector.

Acknowledgments
This work was supported by the KEHATH project funded
by the French National Agency for Research (ANR) under
the grant number ANR-14-CE24-0016-03 .

References
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neu-
ral probabilistic language model. The Journal of Ma-
chine Learning Research (JMLR), 3:1137–1155, 2003.

S. Gouws, Y. Bengio, and G. Corrado.

BilBOWA:
Fast Bilingual Distributed Representations without Word
Alignments. In Proceedings of the International Confer-
ence on Machine Learning (ICML), 2015.

A. Klementiev, I. Titov, and B. Bhattarai. Inducing crosslin-
gual distributed representations of words. In Proceedings
of the International Conference on Computational Lin-
guistics (COLING), 2012.

Q. V. Le and T. Mikolov. Distributed Representations of
In Proceedings of the In-
Sentences and Documents.
ternational Conference on Machine Learning (ICML),
2014.

T. Luong, H. Pham, and C. D. Manning. Bilingual word
representations with monolingual quality in mind.
In
Proceedings of the 1st Workshop on Vector Space Mod-
eling for Natural Language Processing, 2015.

G. Mesnil, M. Ranzato, T. Mikolov, and Y. Bengio. En-
semble of generative and discriminative techniques for
sentiment analysis of movie reviews. arXiv:1412.5335
[cs], 2014.

T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient
Estimation of Word Representations in Vector Space. In
The Workshop Proceedings of the International Confer-
ence on Learning Representations (ICLR), May 2013a.

T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. Distributed Representations of Words and
Phrases and their Compositionality. In Advances in Neu-
ral Information Processing Systems (NIPS), 2013b.

4The data splits and training scripts were obtained from the

authors.

5https://github.com/eske/multivec

4191

A. Mnih and K. Kavukcuoglu. Learning word embed-
dings efﬁciently with noise-contrastive estimation.
In
Advances in Neural Information Processing Systems
(NIPS), 2013.

R. Navigli and S. P. Ponzetto. BabelNet: The Automatic
Construction, Evaluation and Application of a Wide-
Coverage Multilingual Semantic Network. Artiﬁcial In-
telligence, 193:217–250, 2012.

H. Pham, M.-T. Luong, and C. D. Manning. Learning
Distributed Representations for Multilingual Text Se-
quences. In Proceedings of NAACL-HLT, 2015.

Princeton University. About WordNet. Technical report,

Princeton University, 2012.

R. ˇReh˚uˇrek and P. Sojka. Software Framework for Topic
In Proceedings of the
Modelling with Large Corpora.
LREC 2010 Workshop on New Challenges for NLP
Frameworks, 2010.

G. S´erasset. Dbnary: Wiktionary as a LMF based Multi-
lingual RDF network. In Proceedings of the Language
Resources and Evaluation Conference (LREC), 2012.

4192

