IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. X, NO. Y, AUGUST 2017

1

END-TO-END NEURAL SEGMENTAL MODELS
FOR SPEECH RECOGNITION

Hao Tang, Student Member, IEEE, Liang Lu, Member, IEEE, Lingpeng Kong, Kevin Gimpel,
Karen Livescu, Member, IEEE, Chris Dyer, Noah A. Smith, and Steve Renals, Fellow, IEEE

7
1
0
2
 
g
u
A
 
5
1
 
 
]
L
C
.
s
c
[
 
 
2
v
1
3
5
0
0
.
8
0
7
1
:
v
i
X
r
a

Abstract—Segmental models are an alternative to frame-based
models for sequence prediction, where hypothesized path weights
are based on entire segment scores rather than a single frame at
a time. Neural segmental models are segmental models that use
neural network-based weight functions. Neural segmental models
have achieved competitive results for speech recognition, and
their end-to-end training has been explored in several studies.
In this work, we review neural segmental models, which can
be viewed as consisting of a neural network-based acoustic
encoder and a ﬁnite-state transducer decoder. We study end-to-
end segmental models with different weight functions, including
ones based on frame-level neural classiﬁers and on segmental
recurrent neural networks. We study how reducing the search
space size impacts performance under different weight functions.
We also compare several loss functions for end-to-end training.
Finally, we explore training approaches, including multi-stage
vs. end-to-end training and multitask training that combines
segmental and frame-level losses.

Index Terms—segmental models, connectionist temporal class-

iﬁcation, end-to-end training, multitask training

I. INTRODUCTION

Automatic speech recognition (ASR) has been treated as
a graph search problem since its early development [1], and
the graph search approach has been popularized by the use of
hidden Markov models (HMM) [2], [3]. Given a sequence of
acoustic feature vectors, such as log mel ﬁlter bank features
or mel frequency cepstral coefﬁcients (MFCC), recognition
proceeds by computing a weight at every time point for
every label, such as an HMM sub-phonetic state. The search
space is the set of all sub-phonetic state sequences that
corresponds to the set of all word sequences. Weights for
transitioning from one word to another (the language model)
are included at the states corresponding to boundaries between
words. Recognizing speech becomes the task of ﬁnding the
maximum-weight sequence of states, considering both the
weights from the acoustic features and the weights from the
word transitions. Since this approach computes a weight for
every acoustic feature vector, or every frame, at every time
point, it is commonly referred to as a frame-based approach.

Hao Tang, Liang Lu, Kevin Gimpel, and Karen Livescu are with Toy-
ota Technological Institute at Chicago, Chicago, IL 60637 USA (email:
{haotang,llu,kgimpel,klivescu}@ttic.edu).

Lingpeng Kong is with Carnegie Mellon University, Pittsburgh, PA 15213

USA (email: lingpengk@cs.cmu.edu).

Chris Dyer is with Google DeepMind, London, UK and Carnegie Mellon

University, Pittsburgh, PA 15213 USA (email: cdyer@google.com).

Noah A. Smith is with University of Washington, Seattle, WA 98195 USA

(email: nasmith@cs.washington.edu).

Steve Renals is with University of Edinburgh, Edinburgh, UK (email:

s.renals@ed.ac.uk)

Many model types that have been proposed as alternatives
to HMMs, such as conditional random ﬁelds (CRF) [4] and
support vector machine (SVM)-based models [5], are still
frame-based because the search space remains the same.

The inherent limitation of frame-based models is that the
weights can only depend on a ﬁxed length of input at a
given time point. In order to incorporate richer linguistic
information, units other than frames, such as segments [6],
have been proposed. A segment is a variable-length unit, such
as a phoneme [6], [7] or even a whole word [8], [9]. Models
operating on segments, known as segmental models, can take
into account
time, end time, and the associated
to compute the weights of segments. The ability to
label
incorporate arbitrary information within a segment, such as
duration [10] and acoustic landmarks [11], makes segmental
models appealing for speech recognition. In fact, segmental
models were the state of the art for phoneme recognition on
the TIMIT dataset [12] for many years [13].

the start

However, the ﬂexibility of segmental models comes at a
price. The search space of segmental models includes all
possible ways of segmenting the speech input and all possible
ways of labeling the segments, forming a signiﬁcantly larger
search space than the one that frame-based models consider.
To bypass the large search space, early development of seg-
mental models considered restricted search spaces produced by
pruning based on heuristics [6] or based on a ﬁrst-pass frame-
based recognizer [13], [9]. Segmental models that operate on
the full search space—ﬁrst-pass segmental models—have not
been explored until recently [14]. Since then, there has been
a variety of work exploring better segment representations for
ﬁrst-pass segmental models, especially ones that depend on
neural networks [15], [16], [17], [18].

Better, but more computationally expensive, segment rep-
resentations are of little practical use unless the efﬁciency
of the models is improved. Therefore, much of the work on
improving segment representations has been tied to speciﬁc ap-
proaches for reducing the search space. For example, segment
representations based on multilayer perceptrons (MLP) are
used in [15], where the search space is reduced by restricting
the form of the weight function; segment representations based
on convolutional neural networks and MLPs are explored
in [17], where the search space is reduced by pruning; segment
representations based on long short-term memory (LSTM)
networks are used in [18], where the search space is reduced
by reducing the time resolution. In this work, we will consider
different segment representations and study how they behave
under different search spaces.

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. X, NO. Y, AUGUST 2017

2

Segmental models have been proposed and rediscovered
under different names based on the deﬁnitions of the weight
functions and the training losses. For example, hidden semi-
Markov models are deﬁned in the generative setting [19];
semi-Markov CRFs [20] were introduced as segmental models
trained with a log loss; segmental structured SVMs refer to
segmental models trained with a hinge loss [21]. Segmental
CRFs [9] were proposed as segmental models trained with
marginal log loss. In this work, we will separate the loss
functions from the deﬁnition of segmental models and consider
different combinations of segmental weight functions and
losses.

When segmental models are trained with marginal log loss
(or another loss that marginalizes over segmentations), they
can be trained end to end without the need for ground truth
segmentations [18]. This property is particularly useful when
obtaining ground-truth segmentations, such as alignments at
the phonetic level, is expensive or time-consuming. Though
training systems end to end reduces the amount of human
intervention, the learned representations may not be inter-
pretable, making it harder to diagnose errors made by end-
to-end systems. In this work, we will show results comparing
segmental models trained in multiple stages with intermediate
supervision to ones trained end to end. In this context, we
explore two weight functions, one based on frame classi-
ﬁers [15], [17], [16], and one based on segmental recurrent
neural networks [22], [18]. We will also compare end-to-end
frame-based and segmental models in terms of their search
spaces and loss functions. Finally, we will use multitask
learning as a tool to constrain and to analyze the learned
representations.

II. SEGMENTAL MODELS

We consider the problem of sequence prediction, such as
speech recognition, as a graph search problem. The graph,
usually represented as a ﬁnite-state transducer (FST), is a
search space consisting of all of the ways of segmenting
and labeling the input. A vertex in the graph corresponds
to a point in time, and an edge in the graph corresponds to
a segment, that is a time span in the acoustic input and a
possible label. The graph is weighted, and the weight of an
edge corresponds to how well the edge (segment) matches the
input. To compute the weight of an edge, we ﬁrst transform
the input
to an intermediate representation with a feature
encoder. There are many choices for the type of encoder; here
we mainly consider ones based on long short-term memory
(LSTM) networks [23]. The intermediate representation is then
used to compute weights, and we refer to the weighted graph
and the graph search algorithm as the sequence decoder. Below
we will formally deﬁne these components.

Let X be the input space,
the set of all sequences of
log mel ﬁlter bank features
acoustic feature vectors, e.g.,
or mel frequency cepstral coefﬁcients (MFCCs). Speciﬁcally,
for a sequence of T vectors x = (x1, . . . , xT ) ∈ X , each
xt ∈ Rd, for t ∈ {1, . . . , T }, is a d-dimensional acoustic
feature vector, also referred to as a frame. Let Y be the output
space, the set of all label sequences, where each label in a

t

ih

p

iy

sh

ch

w((iy, s, t))

s

z

(cid:105) (cid:104)...

(cid:105) (cid:104)...

(cid:105) (cid:104)...
(cid:104)...
x1 x2

(cid:105) (cid:104)...

(cid:105) (cid:104)...
(cid:105) (cid:104)...
xs

(cid:105)

(cid:105) (cid:104)...
(cid:105) (cid:104)...
xt

(cid:105) (cid:104)...
xT

Fig. 1. An example of a segmental model. A search space is built based on
the input frames. Each edge (segment) has a start time, an end time, and a
label, which the weight function can make use of. Once the weights of edges
are computed, decoding becomes the problem of ﬁnding the maximum-weight
path.

label sequence comes from a label set L, e.g., a phoneme
set in the case of phoneme recognition. Given any T frames,
a segmentation of length K is a sequence of time points
((0 = s1, t1), . . . , (sK, tK = T )), where sk < tk and
tk = sk+1 for k ∈ {1, . . . , K}. A segment (typically denoted
e in later sections) is a tuple ((cid:96), s, t) where (cid:96) ∈ L is its label,
s is the start time, and t is the end time.

A segmental model is a tuple (Θ, w) where Θ is a set
of parameters, and w : X × E → R is a weight function
parameterized by Θ and E is the set of all segment tuples
((cid:96), s, t). The set of parameters Θ includes all parameters for
both the feature encoder and the sequence decoder. A sequence
of segments forms a path. Speciﬁcally, a path of length K is
a sequence of segments (e1, . . . , eK), where ek ∈ E for k ∈
{1, . . . , K}. Let P be the set of all paths. For any path p, we
overload w such that w(x, p) = (cid:80)
e∈p w(x, e). We will also
abbreviate w(x, e) and w(x, p) as w(e) and w(p) respectively
when the context is clear. An example is shown in Figure 1.
Given an input x ∈ X , segmental models aim to solve
sequence prediction by reducing it to ﬁnding the maximum-
weight path

argmax
p∈P

w(x, p).

(1)

The set of paths P, commonly referred to as the search space,
can be compactly represented as an FST.

Here we brieﬂy review the deﬁnition of FSTs. A multigraph
(a graph that can have multiple edges between any pair of
vertices) G is a tuple (V, E, tail, head), where V is a set
of vertices, E is a set of edges, tail
: E → V is a
function that returns the vertex where an edge starts, and
head : E → V is a function that returns the vertex where an
edge ends. We deliberately overload E, because every segment
has a corresponding edge in the graph. An FST is a tuple
(G, Σ, Λ, I, F, i, o, w), where G is a multigraph, Σ is a set of
input symbols, Λ is a set of output symbols, I ⊆ V is a set
of initial vertices, F ⊆ V is a set of ﬁnal vertices, i : E → Σ
is a function that deﬁnes the symbol an edge takes as input,
o : E → Λ is a function that deﬁnes the symbol an edge
outputs, and w : E → R is a function that puts weights on

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. X, NO. Y, AUGUST 2017

3

(cid:105)

(cid:104)...
x1

(cid:105)

(cid:104)...
x2

(cid:105)

(cid:104)...
x3

(cid:105)

(cid:104)...
x4

(cid:105)

(cid:104)...
x5

Fig. 2. An example of a search space for a ﬁve-frame utterance with a label
set L of size three and maximum segment duration D of two frames, i.e.,
T = 5, |L| = 3, and D = 2. The three edges between any two nodes are
associated with the three labels.

edges. We deliberately overload w as well, because the weight
of a segment will be the weight of the corresponding edge.

We also associate a time function τ : V → N that maps
a vertex to a time point (frame index). For convenience, we
deﬁne in(v) = {e ∈ E : head(e) = v} and out(v) = {e ∈ E :
tail(e) = v}. We will also assume there is one unique start
vertex and unique end vertex, i.e., |I| = |F | = 1, but this can
be easily relaxed. More detailed discussion of FSTs and their
applications for speech recognition can be found in [24].

To represent the set of paths P as an FST, we place a
vertex at every time point and connect vertices based on the
set of segments. Speciﬁcally, suppose we have T frames. The
set of segments E is an exhaustive enumeration of tuples
((cid:96), s, t) for all (cid:96) ∈ L and 0 ≤ s < t ≤ T . In practice,
a maximum duration D is typically imposed, i.e., for any
segment ((cid:96), s, t), t − s ≤ D, reducing the possible number
of segments from O(T 2|L|) to O(T D|L|). We create a set
of vertices V = {v0, v1, . . . , vT } such that τ (vt) = t for
t ∈ {0, 1, . . . , T }. For every segment ((cid:96), s, t) ∈ E, we create
an edge e such that i(e) = o(e) = (cid:96), tail(e) = vs, and
head(e) = vt. We set Σ = Λ = L, I = {v0}, and F = {vT }
to complete the construction of the FST given any T frames.
An example of a search space is shown in Figure 2. One
of the many beneﬁts of representing the search space as an
FST is that higher-order segmental models can be constructed
by structurally composing the search space with higher-order
language models [17].

Given the search space constructed above, inference, i.e.,
ﬁnding the maximum-weight path (1), can be done efﬁciently
with dynamic programming. Let P(u, v) be the set of paths
that starts at vertex u and ends at vertex v. By our previous
deﬁnition, P = P(v0, vT ). Deﬁne

d(v) = max

w(e).

p∈P(v0,v)

(cid:88)

e∈p

In words, d(v) is the maximum path weight for all paths
between v0 and v, and the goal is to ﬁnd d(vT ). By deﬁnition,
we have





w(e(cid:48))

 (3)

(cid:88)

e(cid:48)∈p(cid:48)

d(v) = max
e∈in(v)

max
p(cid:48)∈P(v0,tail(e))

w(e) +

(cid:104)

= max
e∈in(v)

w(e) + d(tail(e))

(cid:105)
.

Algorithm 1 shows how to compute all of the entries in d
based on the above recursion and how to backtrack to ﬁnd the
path; it is the same as the shortest path algorithm for directed

(2)

(4)

acyclic graphs [25]. Since v0, . . . , vT follows a topological
order, Algorithm 1 is guaranteed to return the maximum-
weight path.

Algorithm 1 Finding the maximum-weight path

d(v0) = 0
for v = v0, v1, . . . , vT do
d(v) = maxe∈in(v)

(cid:105)
(cid:104)
d(tail(e)) + w(e)

δ(v) = argmaxe∈in(v)

d(tail(e)) + w(e)

(cid:104)

(cid:105)

end for
u = vT , p = ∅
while u (cid:54)= v0 do
p = δ(u) ∪ p
u = tail(δ(u))

end while
return p

Since for any vertex v, |in(v)| ≤ D|L|, the runtime of
Algorithm 1 is O(T D|L|) = O(|E|). In fact, Algorithm 1
evaluates w(e) for every edge e ∈ E exactly once.

A segmental model can be trained by ﬁnding a set of
parameters Θ that minimizes a loss function L. The model
deﬁnition is not tied to any loss function, allowing us to
study the behavior of segmental models under different loss
functions.

III. WEIGHT FUNCTIONS

Here we detail two types of neural weight functions based
on prior work by ourselves and others. The term feature
function is often used in the literature to denote the function
φ : X × E → Rm for some m, where the weight function
w(x, e) is of the form θ(cid:62)φ(x, e) for some parameter vector
θ ∈ Rm. When considering neural networks, the weight func-
tion need not be a dot product, but can be any differentiable
real-valued function.

The ﬁrst type of weight function is similar to those of [15],
[17], [16], consisting of outputs of frame-level neural network
classiﬁers “summarized” in various ways over the span of a
segment; the speciﬁc formulation we use is that of [26]. The
second type of weight function is a segmental recurrent neural
network, as in [18]. We study segmental models in the context
of these particular weight functions due to their prior success.
In both cases the acoustic encoder is based on long short-term
memory networks (LSTMs) [23].

that

Recall

the weight function w takes a sequence of
acoustic features x = (x1, . . . , xT ) and a segment ((cid:96), s, t) as
input. The weight function ﬁrst passes the acoustic features
through multiple layers of LSTMs. Let h1, . . . , h ˜T be the
sequence of output vectors1 of the ﬁnal LSTM. The output of
each layer can be subsampled before feeding to the next layer
to reduce the time resolution. For example, if we subsample

t +Wbhb

t , where hf

1Speciﬁcally, ht = Wf hf

t are the output vectors
of the forward and backward LSTMs for some weight matrices Wf and Wb.
The output vector for the forward LSTM is deﬁned as hf
t ) (cid:12) of
t ,
where cf
t is the output gate at time t. The output vector
hb
t is deﬁned similarly for the backward LSTM.

t is the cell, and of

t = tanh(cf

t and hb

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. X, NO. Y, AUGUST 2017

4

at layers two and three for a 3-layer LSTM, then ˜T = T /4.
Otherwise, ˜T = T . We will use Θenc to denote the parameters
of the LSTMs, and let Θdec be the remaining parameters in
the weight function. Note that Θ = Θenc ∪ Θdec.

A. FC weight function

The ﬁrst type of weight function, termed the frame classiﬁer
(FC) weight, is similar to weight functions used in a variety
of prior work [15], [17], [16]. A frame classiﬁer takes in
the LSTM output h1, . . . , h ˜T and produces a sequence of log
probability vectors over the labels

zi = logsoftmax(W hi + b)
where zi ∈ R|L| and W and b are the parameters, for i ∈
{1, . . . ˜T }. Based on these posterior vectors, we deﬁne several
functions that summarize the posteriors over a segment:

(5)

a) frame average: The average of transformed log prob-

abilities

bilities

wavg(((cid:96), s, t)) =

1
t − s

t−1
(cid:88)

i=s

ui,(cid:96),

where ui = Wavgzi for i ∈ {1, . . . , ˜T }.

b) frame samples: A sample of transformed log proba-

wspl-j(((cid:96), s, t)) = uj,(cid:96)

at time j ∈ {(t−s)/6, (t−s)/2, 5(t−s)/6}, where ui = Wsplzi
for i ∈ {1, . . . , ˜T }.

c) boundary: The samples of transformed log probabili-
ties around the left boundary (start) and right boundary (end)
of the segment:

the label (cid:96), dk is a duration embedding vector for the duration
k in log scale, and ReLU(x) = max(x, 0). The ﬁnal weight
for the segment is deﬁned as

w(((cid:96), s, t)) = θ(cid:62)z(2)

(cid:96),s,t.

instead of encoding the LSTM output vectors
Note that
h1, . . . , h ˜T with an additional LSTM per segment as in [22],
for efﬁciency we use the left and right output vectors hs and ht
and use a simple feed-forward network to compute the weight
w(((cid:96), s, t)). When the SRNN weight function is used, Θdec
is {W1, b1, W2, b2, θ}. Although the SRNN weight function is
conceptually simple, it is more expensive to compute than the
FC weight function.

IV. LOSSES
Recall that a path p = (((cid:96)1, s1, t1), . . . , ((cid:96)K, sK, tK)) con-
sists of a label sequence y = ((cid:96)1, . . . , (cid:96)K) and a segmentation
z = ((s1, t1), . . . , (sK, tK)). We will use (y, z) and p inter-
changeably. We will denote the space of all segmentations Z.
Training aims to ﬁnd a set of parameters Θ that minimizes
the expected task loss, in our case, the expected edit distance
E(x,y)∼D[edit(y, hΘ(x))]

(12)

where h is the inference algorithm Algorithm 1 parameterized
with Θ, edit computes the edit distance of two sequences,
and the expectation is taken over samples (x, y) ∈ X × Y
drawn from a distribution D. The edit distance is discrete
and therefore difﬁcult to optimize; instead we minimize the
expected loss

E(x,y)∼D[L(Θ; x, y)],

(13)

wleft-k(((cid:96), s, t)) = uk,i−k,(cid:96)
wright-k(((cid:96), s, t)) = u(cid:48)

k,i+k,(cid:96)

(8)

(9)

where L is a surrogate loss function. Some surrogate losses
refer to a particular choice of segmentation z; in that case we
wish to minimize

where uk,i = Wleft-kzi and u(cid:48)
and i ∈ {1, . . . , ˜T }.

k,i = Wright-kzi for k = 1, 2, 3

d) duration: The label-dependent duration weight

wdur(((cid:96), s, t)) = d(cid:96),t−s.

e) bias: A label-dependent bias

wbias(((cid:96), s, t)) = b(cid:48)
(cid:96).

The ﬁnal FC weight function is the sum of all of the above
weight functions. When the FC weight function is used, Θdec
is {W, b, Wavg, Wspl, Wleft, Wright, d, b(cid:48)}.

B. SRNN weight function

The second type of weight function is based on segmental
recurrent neural networks (SRNNs) [22], [18]. Suppose the
LSTM output vectors are h1, . . . , h ˜T . To compute w(((cid:96), s, t)),
two hidden layers

z(1)
(cid:96),s,t = ReLU(W1[hs; ht; c(cid:96); dk] + b1)
(cid:96),s,t = tanh(W2z(1)
z(2)

(cid:96),s,t + b2)

E(x,y,z)∼D(cid:48)[L(Θ; x, y, z)],
where D(cid:48) is a distribution over X ×Y ×Z. We will use D(y|x)
and D(cid:48)(y, z|x) to denote the conditional distribution of y and
y, z, respectively, given the input. Since the distribution D is
unknown, we use a training set S = {(x1, y1), . . . , (xn, yn)}2
of size n to approximate the expectation and instead minimize

(14)

1
n

n
(cid:88)

i=1

L(Θ; xi, yi).

(15)

The connection between the surrogate loss L and the edit
distance depends on the choice of loss. Below we list the
loss functions we consider, along with reasons for using
them and their (sub)gradients with respect to the weight w(e)
for some edge e. The (sub)gradients are used in the ﬁrst-
order methods, such as stochastic gradient descent, that we
use for optimization. We assume that the weight function w
is differentiable and the (sub)gradients with respect to the
parameters can be obtained with backpropagation. Other losses
for training segmental models, such as ramp loss and empirical
Bayes risk, are not included here but are treated in [27].

(6)

(7)

(10)

(11)

are computed directly from the LSTM outputs before comput-
ing the ﬁnal weight, where c(cid:96) is a label embedding vector for

2A training set S = {(x1, y1, z1), . . . , (xn, yn, zn)} of size n is needed
i=1 L(Θ; xi, yi, zi).

if we optimize (14) with the approximation being 1
n

(cid:80)n

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. X, NO. Y, AUGUST 2017

There are interesting connections between loss functions and
discriminative training criteria in speech recognition (see, for
example, [28], [29]).

where

Z(x) =

exp(w(x, p(cid:48)))

(cid:88)

p(cid:48)∈P

5

(22)

Given an utterance x and a ground-truth path p = (y, z),

L(Θ; x, p) = − log P (p|x).

(23)

A. Hinge loss

the hinge loss is deﬁned as

L(Θ; x, p) = max
p(cid:48)∈P

[cost(p(cid:48), p) − w(p) + w(p(cid:48))]

(16)

where cost is a user-deﬁned, non-negative cost function. The
connection between the hinge loss and the task loss is through
the cost function. Suppose ˆp = argmaxp∈P w(p) is the best-
scoring path found by Algorithm 1. The cost of the inferred
path ˆp can be upper-bounded by the hinge loss:

cost(ˆp, p) ≤ cost(ˆp, p) − w(p) + w(ˆp) ≤ L(Θ; x, p).

(17)

When the cost function is the edit distance, minimizing the
hinge loss minimizes an upper bound on the edit distance.

The hinge loss is difﬁcult to optimize when the cost function
is the edit distance. In practice, the cost function is assumed
to be decomposable to allow efﬁcient dynamic programming:
(cid:88)

cost(p(cid:48), p) =

cost(e(cid:48), p).

(18)

e(cid:48)∈p(cid:48)

When the cost is decomposable, the hinge loss can be written
as

L(Θ; x, p) = max
p(cid:48)∈P

cost(e(cid:48), p) −

w(e) +

w(e(cid:48))

(cid:88)

e∈p

(cid:88)

e(cid:48)∈p(cid:48)





(cid:88)

e(cid:48)∈p(cid:48)
(cid:88)

e(cid:48)∈p(cid:48)

= max
p(cid:48)∈P

(cid:88)

e∈p

and the max operator in the ﬁrst term can be solved with
Algorithm 1 by adding the costs to the weights for all
segments.

A subgradient of the hinge loss with respect to w(e) is

where

∂L(Θ; x, p)
∂w(e)

= −1e∈p + 1e∈ ˜p

˜p = argmax

[cost(p(cid:48), p) + w(p(cid:48))],

p(cid:48)∈P





(19)

(20)

which is the path that maximizes the ﬁrst term in the hinge
loss, and can be obtained with Algorithm 1 with cost added.
Linear models trained with hinge loss are referred to as
support vector machines (SVM), or as structured SVMs when
applied to structured prediction problems, e.g., sequence pre-
diction in our case. Segmental models trained with the hinge
loss have been studied by [21], [27], [17].

B. Log loss

Segmental models can be treated as probabilistic models
by deﬁning probability distributions on the set of all paths.
Speciﬁcally, the probability of a path p = (y, z) is deﬁned as

P (y, z|x) = P (p|x) =

exp(w(x, p))

(21)

1
Z(x)

is the partition function. Given an input x and a ground-truth
path p, the log loss is deﬁned as

Minimizing the log loss is equivalent
to maximizing the
conditional likelihood. In addition, the conditional likelihood
can be written as

P (y, z|x) = E(y(cid:48),z(cid:48))∼P (y(cid:48),z(cid:48)|x)[1(y(cid:48),z(cid:48))=(y,z)]

= 1 − E(y(cid:48),z(cid:48))∼P (y(cid:48),z(cid:48)|x)[1(y(cid:48),z(cid:48))(cid:54)=(y,z)].

Therefore, maximizing the conditional likelihood is equivalent
to minimizing the expected zero-one loss

E(y(cid:48),z(cid:48))∼P (y(cid:48),z(cid:48)|x)[1(y(cid:48),z(cid:48))(cid:54)=(y,z)],
(24)
where P (y, z|x) is used to approximate D(cid:48)(y, z|x). The use of
the log loss can be justiﬁed by viewing the expectation above
as an approximation of (14). Segmental models trained with
log loss have been referred to as semi-Markov CRFs [20].

Since the weight for the ground-truth path p can be efﬁ-
ciently computed, we are left with the problem of computing
the partition function Z(x). The partition function can also be
computed efﬁciently with the following dynamic programming
algorithm. Recall that P(u, v) is the set of paths that start at
vertex u and end at vertex v. For any vertex v, deﬁne the
forward marginal as

α(v) = log

(cid:88)

exp(w(p(cid:48))).

(25)

α(v) = log

(cid:88)

exp



w(e)



p(cid:48)∈P(v0,v)

p(cid:48)∈P(v0,v)



(cid:88)

e∈p(cid:48)





(cid:88)

(cid:88)

= log

exp

w(e) +

p(cid:48)∈P(v0,tail(e))

= log

exp (w(e) + α(tail(e)))

e∈in(v)
(cid:88)

e∈in(v)





w(e(cid:48))

(cid:88)

e(cid:48)∈p(cid:48)

Similarly, the backward marginal at v is deﬁned as

β(v) = log

(cid:88)

exp(w(p(cid:48))),

(26)

p(cid:48)∈P(v,vT )
and has a similar recursive structure. The complete algorithm
is shown in Algorithm 2. Once all entries in α and β are
computed, the log partition function is

log Z(x) = α(vT ) = β(v0).

(27)

We store all of the entries in log space for numerical stability.

The gradient of the log loss with respect to w(e) is

∂L(Θ; x, p)
∂w(e)

= −1e∈p +

exp(w(p(cid:48)))

(cid:88)

p(cid:48)(cid:51)e

1
Z(x)
(cid:104)

= −1e∈p + exp

α(tail(e)) + w(e)

(cid:105)
+ β(head(e)) − log Z(x)

,

[cost(e(cid:48), p) + w(e(cid:48))] −

w(e),

By expanding the edges ending at v, we have

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. X, NO. Y, AUGUST 2017

6

which can also be efﬁciently computed once the marginals are
computed.

Algorithm 2 Computing forward and backward marginals

α(v0) = 0
β(vT ) = 0
logadd(a, b) = log(exp(a) + exp(b))
for v = v0, v1, . . . , vT do
(cid:104)
α(v) = logadde∈in(v)

(cid:105)
α(tail(e)) + w(e)

end for
for v = vT , vT −1, . . . , v0 do
β(v) = logadde∈out(v)

(cid:104)

end for

(cid:105)
β(head(e)) + w(e)

compute log Z(x, y). Since the term log Z(x, y) is identical
to log Z(x) except that it involves a constrained search space
considering all paths with the same label sequence y, the
strategy is to construct
the constrained search space with
an FST and run Algorithm 2 on the FST. Let F be a
chain FST that represents y, with edges {e1, . . . , e|y|}, where
i(ek) = o(ek) = yk for all k ∈ {1, . . . , |y|}. Let G be the
search space consisting of all paths in P. The term log Z(x, y)
can be efﬁciently computed by running Algorithm 2 on the
intersection of G and F , i.e., G ∩ F . Let the forward and
backward marginals computed on G ∩ F be α(cid:48) and β(cid:48). We
have log Z(x, y) = α(cid:48)(vT ) = β(cid:48)(v0).

The gradient of the marginal log loss is

∂L(Θ; x, y)
∂w(e)
1
Z(x, y)

= −

C. Marginal log loss

loss is deﬁned as

Given an input x and a label sequence y, the marginal log

L(Θ; x, y) = − log P (y|x) = − log

P (y, z|x)

(28)

(cid:88)

z∈Z

where the segmentation is marginalized compared to log loss.
Following the same argument as for log loss, the marginal
distribution can be written as

exp(w(p(cid:48))) +

exp(w(p(cid:48)))

1
Z(x)

(cid:88)

p(cid:48)(cid:51)e

(cid:88)

p(cid:48)(cid:51)e
Γ(p(cid:48))=y

= − exp

(cid:104)

(cid:105)
α(cid:48)(tail(e)) + w(e) + β(cid:48)(head(e)) − log Z(x, y)

+ exp

(cid:104)

(cid:105)
α(tail(e)) + w(e) + β(head(e)) − log Z(x)

.

and can be efﬁciently computed once the marginals are com-
puted.

P (y|x) = 1 − Ey(cid:48)∼P (y(cid:48)|x)[1y(cid:54)=y(cid:48)],

(29)

V. MULTI-STAGE TRAINING AND MULTITASK TRAINING

and maximizing the marginal distribution is equivalent
minimizing the expected zero-one loss

to

Ey(cid:48)∼P (y(cid:48)|x)[1y(cid:54)=y(cid:48)],

(30)

where P (y|x) is used to approximate D(y|x). Note that the
zero-one loss 1y(cid:54)=y(cid:48) only depends on the label sequence. While
the log loss has a connection to (14), the marginal log loss
directly approximates (13) with the above expected zero-one
loss.

Note that both the hinge and log loss depend on the
ground-truth segmentation. The marginal log loss does not
require the ground-truth segmentation, making it attractive for
tasks such as speech recognition, because collecting ground-
truth segmentations for phonemes or words is time-consuming
and/or expensive. In addition, the boundaries of phonemes and
words tend to be ambiguous, so it can be preferable to leave
the decision to the model. Segmental models trained with the
marginal log loss have been referred to as segmental CRFs [9].
To compute the marginal log loss, we can rewrite it as

L(Θ; x, y) = − log

P (y, z|x)

(31)

(cid:88)

z∈Z
(cid:88)

z∈Z

= − log

exp(w(x, (y, z))) + log Z(x)

(32)

= − log

(cid:88)

exp(w(x, p(cid:48)))

+ log Z(x) (33)

p(cid:48):Γ(p(cid:48))=y

(cid:124)

(cid:123)(cid:122)
log Z(x,y)

(cid:125)

Following the conventional ASR pipeline, we can ﬁrst build
a frame classiﬁer and use it to build the rest of the segmental
models. Such an approach, using the weight functions deﬁned
above, has been successful for training segmental models,
either for multi-stage training or as an initialization for end-
to-end training [26], [30]. We will review these training
approaches in detail and present a uniﬁed view for both.

Recall that our parameters can be divided into two parts:
Θenc for the acoustic feature encoder and Θdec for the sequence
decoder. The acoustic feature encoder can be trained jointly
with the sequence decoder, or separately with other loss func-
tions, such as the frame-wise cross entropy or the connectionist
temporal classiﬁcation (CTC) loss [31]. We refer to the case
where the encoder and decoder are trained jointly as end-to-
end training, and the case where the training is separated into
multiple stages (end-to-end or not) as multi-stage training.

Consider the end-to-end training approach. We can write

the objective

min
Θenc,Θdec

L(Θenc, Θdec),

(34)

in terms of both Θenc and Θdec where L is a loss function that
takes both the encoder and the decoder into account, such as
the hinge loss, log loss, or marginal log loss. For multi-stage
training, we use a loss function to train the encoder in the ﬁrst
stage by solving

ˆΘenc = argmin

Lenc(Θenc),

Θenc

where Γ extracts the label sequence from a path, i.e., for
p(cid:48) = (y(cid:48), z(cid:48)), Γ(p(cid:48)) = y(cid:48). Since the partition function can
be efﬁciently computed from Algorithm 2, we only need to

where Lenc can be the frame-wise cross entropy or the CTC
loss. In the second stage, we use the obtained ˆΘenc to solve

ˆΘdec = argmin

L( ˆΘenc, Θdec)

Θdec

(35)

(36)

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. X, NO. Y, AUGUST 2017

7

while holding the ﬁrst argument in the loss ﬁxed. In the third
stage, we can then use ˆΘenc and ˆΘdec as initialization and
solve (34).

In addition, we can also consider a convex combination of

multiple loss functions

min
Θenc,Θdec

λL(Θenc, Θdec) + (1 − λ)Lenc(Θenc)

(37)

where λ is the interpolation factor. End-to-end training can be
seen as optimizing (37) with λ = 1, while multi-stage training
can be seen as optimizing the second term in (37) followed
by optimizing the ﬁrst term.

While there are many beneﬁts for training systems end to
end, such as the potential to ﬁnd a better optimum and without
requiring supervision at
the intermediate level, end-to-end
training might be challenging due to optimization difﬁculties
and might require more samples. On the other hand, while
multi-stage training requires supervision at the intermediate
level, it might make the optimization easier (sometimes mak-
ing it convex), might require fewer samples, and might produce
models that are more interpretable.

VI. EXPERIMENTS

We apply segmental models to phonetic recognition on
TIMIT, a dataset consisting of a training set of 3696 utterances
and a test set, of which a subset of 192 utterances is called
the core test set. Following standard protocol [32], we use 400
utterances from the complete test set (disjoint from the core
test set) as the validation set, and report the ﬁnal results on the
core test set. In addition, we reserve 376 utterances from the
training set for tuning hyperparameters, such as optimizers,
step sizes, and dropout rates, and use the remaining 3320
utterances for training. The development set is used solely
for early stopping. As is often done for TIMIT experiments,
we collapse the 61 phones in the phone set to 48 for training,
and further collapse them to 39 for evaluation [33]. TIMIT is
phonetically transcribed, so we have the option of training
the feature encoder with frame-wise cross entropy based
on the ground-truth frame labels. The acoustic input to the
log ﬁlter bank
feature encoder consists of 40-dimensional
features (without energy) and their ﬁrst and second derivatives.
The resulting 120-dimensional acoustic features are speaker-
normalized by subtracting the per-speaker mean and dividing
by the per-speaker standard deviation of every dimension.

The feature encoder is a 3-layer bidirectional LSTM with
250 hidden units in each direction. Previous work has shown
that subsampling either the frames or the LSTM outputs can
reduce the decoding time while maintaining accuracy [34],
[35]. We consider subsampling the output of the LSTMs by a
factor of two after the second and third layers. The subsampled
encoder is referred to as a pyramid encoder [18]. Dropout [36]
is added to the input and output of the LSTMs at a rate of
0.2.

For the segmental models, we enforce a maximum segment
duration of 30 frames when a regular feature encoder is
used, and a maximum duration of 8 when a pyramid feature
encoder is used. The maximum duration is applied to all
labels, including silences. For the SRNN weight function,

following [18], the duration embedding is of size 5, the label
embedding is of size 32, and the two subsequence hidden
layers are both of size 64. All parameters of the weight
functions are initialized based on [37].

All loss functions are optimized with stochastic gradient
descent (SGD) with a minibatch size of 1 utterance. The
gradient norm is clipped to 5. The default optimizer is vanilla
SGD unless otherwise stated. We run the optimizer for 20
epochs with step size 0.1; starting from the best model among
the ﬁrst 20 epochs, we run for another 20 epochs with step
size decayed by 0.75 after each epoch (i.e., exponential decay).
We choose the epoch that has the best performance on the
development set (early stopping).

A. Multi-stage training

We ﬁrst compare different segmental models trained in
multiple stages. The ﬁrst stage trains the feature encoder Θenc
either with the frame-wise cross entropy or with the CTC loss,
and the second stage trains Θdec with hinge loss, log loss, or
marginal log loss. Finally, after the second stage, we ﬁne-tune
both Θenc and Θdec with each of the three losses.

To construct a frame classiﬁer, the 250-dimensional output
vectors of the encoder are projected down to 48 dimensions
followed by a softmax layer. Depending on whether we use a
pyramid encoder, we subsample the frame labels accordingly
during training. The resulting frame classiﬁer achieves frame
error rates of 18.3% for the regular encoder and 29.1% for
the pyramid encoder (where outputs are upsampled to evaluate
performance) on the development set.

For the CTC loss, we project the 250-dimensional output
vectors of the feature encoder down to 49 dimensions (48
phones + 1 blank) and pass them through a softmax layer.
The encoder is ﬁxed to a pyramid, and frame labels are
not required during training. The encoder trained with the
CTC loss achieves a phoneme error rate (PER) of 17.2%
on the development set with best-path decoding (followed by
removing duplicates and blanks).

In the ﬁrst set of experiments, we only use the frame
classiﬁers as encoders (pyramid or not), and compare the
two weight functions. Since we have the pretrained feature
encoders, we freeze the encoder parameters Θenc and train
the decoder parameters Θdec. The default SGD optimizer (20
epochs without decay plus 20 epochs with exponential decay)
is used for the SRNN weight function because it works well
with the two-layer networks in the weight function. For the FC
weight function, note that hinge loss and log loss are convex in
Θdec. In particular, when the encoder Θenc is frozen, optimizing
hinge loss and log loss for the FC weight function are both
convex problems. RMSprop [38] is favored over vanilla SGD
for the FC weight function with step size 10−4 and decay 0.9
for 20 epochs. After the two-stage training, we can further
optimize both the encoder and the decoder. Here vanilla SGD
is used with the step size starting from 0.1 and decayed by
0.75 after each epoch, because the training loss is already low
after two-stage training.

The multi-stage training results are shown in Table I. The
results are consistent with those reported in [17]. For the

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. X, NO. Y, AUGUST 2017

8

TABLE I
PHONE ERROR RATES (PER, %) ON THE DEVELOPMENT SET FOR
SEGMENTAL MODELS WITH DIFFERENT WEIGHT FUNCTIONS AND
DIFFERENT FEATURE ENCODERS. THE ENCODERS (PYRAMID OR NOT) ARE
TRAINED WITH THE FRAME-WISE CROSS ENTROPY. TWO-STAGE TRAINING
IS DENOTED 2s, AND TWO-STAGE TRAINING FOLLOWED BY FINE-TUNING
IS DENOTED 2s+ft. (* TOO SLOW TO COMPLETE)

pyramid

FC

SRNN

(cid:88)
(cid:88)

(cid:88)
(cid:88)

2s
2s+ft
2s
2s+ft
2s
2s+ft
2s
2s+ft

hinge
19.9
19.3
31.3
23.3
22.2*
-
27.4
24.4

log loss marginal log loss
19.9
20.7
17.9
18.2
24.4
32.0
17.9
22.3
20.5*
22.2*
-
-
21.3
24.4
18.1
22.7

TABLE II
PERS (%) FOR SEGMENTAL MODELS TRAINED IN MULTIPLE STAGES WITH
FEATURE ENCODERS PRETRAINED WITH THE FRAME-WISE CROSS
ENTROPY OR THE CTC LOSS. TWO-STAGE TRAINING IS DENOTED 2s, AND
TWO-STAGE TRAINING FOLLOWED BY FINE-TUNING IS DENOTED 2s+ft.

frame loss
2s
dev
27.4
25.9
21.3

2s+ft
dev
24.4
22.7
18.1

2s+ft
test

20.9

CTC
2s
dev
26.7
25.7
18.7

2s+ft
dev
24.1
22.7
17.8

2s+ft
test

20.2

hinge
log loss
marginal log loss

FC weight function with a regular encoder, the three losses
perform equally well, with marginal log loss having a slight
edge over the other two. Using the pyramid encoder hurts the
performance of hinge loss and log loss, but has less impact
on marginal log loss. Hinge loss and log loss might be more
sensitive to the reduced time resolution because they are tied
to a speciﬁc segmentation, while marginal log loss is more
forgiving due to the marginalization. Fine-tuning improves
over two-stage training across all cases. The conclusion stays
the same for the SRNN weight function, except that training
the SRNN weight function without the pyramid is very time-
consuming, and we only manage to complete a few epochs
in the two-stage setting. Although the best results after ﬁne
tuning are roughly the same for both weight functions, to
shorten the experimental cycle, we favor the better performer,
the SRNN weight function, with a pyramid encoder in the
two-stage setting.

After ﬁxing the weight function to the SRNN, we compare
encoders pretrained with the frame-wise cross entropy and
with the CTC loss. Results are shown in Table II. It is clear
that for all losses, using the encoder pretrained with the CTC
loss leads to better performance.

B. End-to-end training from random initialization

In this section, we compare losses for end-to-end training
of segmental models with the pyramid encoder and the SRNN
weight function. Unlike in multi-stage training, all of the ex-
periments here are trained from random initialization. Results
are shown in Table III. While the log loss and marginal log loss
achieve reasonable performance, the hinge loss completely
fails. We ﬁnd that hinge loss values on the training set are
higher compared to the multi-stage models, suggesting that
there is an optimization issue. Since log loss can be minimized

TABLE III
SEGMENTAL MODELS TRAINED END TO END WITH DIFFERENT LOSSES
COMPARED WITH THE CTC MODEL.

hinge
log loss
marginal log loss
CTC

dev
74.7
22.2
17.5
17.2

test

19.5
19.5

TABLE IV
PERS (%) FOR SEGMENTAL MODELS TRAINED END TO END WITH
MULTIPLE TASKS, I.E., THE MARGINAL LOG LOSS PLUS EITHER THE
FRAME-WISE CROSS ENTROPY OR THE CTC LOSS.

frame loss
dev
test
18.6
18.0
17.2
16.9
16.8
17.5

19.3
19.5

CTC
dev
17.5
17.0
17.0
16.7
17.0
17.2

test

18.5

19.5

λ
0.16
0.33
0.5
0.67
0.84
1.00

reasonably well, we suspect that hinge loss is difﬁcult to
minimize because of its non-smoothness. The result with
the marginal log loss is consistent with reported numbers in
previous work [18]. The performance of CTC is on par with
the segmental model trained with marginal log loss.

C. End-to-end multitask training

Instead of optimizing different losses in different stages as in
the previous section, we next optimize multiple losses jointly
from random initialization. Here we only focus on marginal
log loss paired with either the frame-wise cross entropy or the
CTC loss, because the marginal log loss is the best performer
in the previous experiments. We use early stopping based on
the PERs of the segmental model on the development set.
Results are shown in Table IV. We see that end-to-end training
with multiple tasks further improves over end-to-end training
with a single task. The best test-set result (and the best dev-
set result) is obtained by multitask training with marginal log
loss + CTC loss, and improves over the CTC error rate by 1%
absolute (19.5% −→ 18.5% on the test set).

The success of multitask learning in Table IV indicates
that there exists an encoder that can generate representations
suitable for both tasks. We further investigate the loss values
for the case of jointly optimizing the marginal log loss and
the CTC loss. The learning curve is shown in Figure 3.
In the multitask case, both the marginal log loss and the
CTC loss achieve lower values on the training set compared
to the single-task case, suggesting that multitask learning
might help optimization. However, both loss values on the
development set end up higher when multiple losses are used.
The fact that models with higher losses on the development set
end up having lower PERs is unsatisfying and needs further
investigation.

The time to compute gradients for different losses is shown
in Table V. All numbers are measured on a single quad-
core 3GHz CPU, averaged over the entire training set. As
a reference, the average real-time factor for computing the

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. X, NO. Y, AUGUST 2017

9

TABLE VI
TIMIT PERS (%) FOR VARIOUS SEGMENTAL MODELS COMPARED WITH
HMMS AND THE STATE OF THE ART. THE ACOUSTIC FEATURES CAN BE
SPEAKER INDEPENDENT (spk indep) OR SPEAKER ADAPTED WITH MEAN
AND VARIANCE NORMALIZATION (mvn) OR MAXIMUM LIKELIHOOD
LINEAR REGRESSION (fMLLR) [32]. SOME RESULTS WERE OBTAINED
WITH MFCCS AND SOME WITH LOG FILTER BANK FEATURES.

HMM-DNN [32]
HMM-CNN [42]
SUMMIT (1998) [43], [13]
segmental CRF (SCRF) (2012) [14]
SCRF + shallow NN (2012) [15]
SCRF + DNN (2015) [41]
deep segmental NN (2013) [16]
segmental cascades (2015) [17]
segmental RNN (SRNN) (2016) [18]
end-to-end + two-stage training (2016) [26]
SRNN + multitask (2017) [30]
SRNN + multitask (2017) (this work)

spk
indep
21.4
16.5
24.4
33.1
26.5

21.9
19.9

19.7

+mvn

+fMLLR

18.3

19.1

18.9

17.3

18.7
18.5

17.5

They also introduced segment-level classiﬁers and segmental
cascades for incorporating them (and other expensive features)
into segmental weight functions [17]. Lu et al. [18] introduced
an LSTM-based weight function for every segment, and were
also the ﬁrst to use pyramid LSTMs to speed up inference for
segmental models.

B. End-to-end models

Most mainstream end-to-end speech recognition models
can be broadly categorized as either frame-based models
or encoder-decoder models. CTC, HMMs, and some newer
approaches like the auto-segmentation criterion (ASG) [44]
fall under the ﬁrst category, because these models emit one
symbol for every frame. Falling under the second category,
encoder-decoder models proposed by [45], [46], [47] generate
labels one at a time while conditioning on the input and the
labels generated in the past, without an explicit alignment
between labels and frames. Since frame-based models follow
the same graph search framework as segmental models, we
will focus on discussing the connection between these and
segmental models.

Recall that training segmental models with marginal log loss
requires a search space G, a constraint FST F to limit the
search space to ground-truth labels, and the loss itself. To
compute marginal log loss, we ﬁrst compute the marginals
on G for computing the partition function Z(x), and then
compute the marginals on the intersection G∩F for computing
Z(x, y). CTC, HMMs, and ASG can all be seen as special
cases of this framework.

The search space of CTC has an edge for every label
in the label set (including the blank label) at every time
the search space G includes the edges
step. Speciﬁcally,
{e(cid:96),t : (cid:96) ∈ L, t ∈ {1, . . . , T }} with vt−1 = tail(e(cid:96),t) and
vt = head(e(cid:96),t). An example is shown in Figure 4. The
weight of an edge e(cid:96),t is the log probability of label (cid:96) at
time t. By construction,
the decision made at every time
point is independent of the decision at other time points. In
addition, since the probabilities at every time point sum to
one, the partition function Z(x) of the search space is always

seg-MLL

CTC

seg-MLL + CTC (λ = 0.67)

Fig. 3. Loss values for jointly optimizing the marginal log loss (seg-MLL)
and the CTC loss compared with optimizing them individually.

TABLE V
AVERAGE REAL-TIME FACTOR PER SAMPLE TO COMPUTE GRADIENTS FOR
DIFFERENT LOSSES.

pyramid

FC

SRNN

(cid:88)

(cid:88)

hinge
0.257
0.015
3.253
0.165

log loss marginal log loss
0.954
0.811
0.073
0.042
6.110
5.669
0.384
0.286

gradients from the outputs of a regular 3-layer LSTM is 0.301,
and reduces to 0.181 for pyramid 3-layer LSTM. Computing
the gradient of the CTC loss is also 0.181. Though the
exact number depends on the implementation, the general
trend is clear. It is faster to compute the FC weight function
than the SRNN weight function. Using the pyramid encoder
signiﬁcantly reduces the runtime. Computing the gradient of
hinge loss is fastest, and computing the gradient of marginal
log loss is the slowest. The real-time factor for decoding is
0.259 including the pyramid LSTM. Decoding in multiple
passes can be an option if speedup is needed [39].

VII. RELATED WORK

A. First-pass segmental models

Many models, such as semi-Markov CRFs [20], segmental
CRFs [9], and inverted HMMs [40], are special cases of seg-
mental models with different weight functions and trained with
different losses. In Table VI, we provide a set of highlights of
results in the development of segmental models on the TIMIT
data set. Zweig [14] was the ﬁrst to explore discriminative seg-
mental models that search over sequences and segmentations
exhaustively, and did not use neural networks. He & Fosler-
Lussier [15] ﬁrst used (shallow) neural network-based frame
classiﬁers to deﬁne weight functions, and later extended the
idea to deep neural networks in [41]. Abdel-Hamid et al. [16]
were the ﬁrst to use deep convolutional neural networks for
the weight functions, and were the ﬁrst to train segmental
models end to end. Tang et al. ﬁrst compared different losses
and training strategies for segmental models in a rescoring
framework [27] and then in ﬁrst-pass segmental models [26].

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. X, NO. Y, AUGUST 2017

10

(cid:105)

(cid:104)...
x1

(cid:105)

(cid:104)...
x2

(cid:105)

(cid:104)...
x3

(cid:105)

(cid:104)...
x4

(cid:105)

(cid:104)...
x5

Fig. 4. An example of the CTC search space for a ﬁve-frame utterance with
a label set of size three (plus one blank).

1. The constraint FST F representing the ground-truth labels
consists of the sequences of one or more labels with zero
or more blanks in between labels. For example, for the label
sequence “k ae t,” the constraint FST is the regular expression
∅∗k+∅∗ae+∅∗t+∅∗. With the above construction, marginal
log loss becomes exactly the objective of CTC.

Comparing CTC to HMMs, the search space is different
depending on the HMM topology. For example,
two-state
HMMs are used in [48]. Since the transition probabilities and
posterior probabilities are all locally normalized, the partition
function Z(x) is always 1. The constraint FST representing the
ground-truth labels consists simply of sequences of repeating
labels. For example, for the label sequence “k ae t,” the
constraint FST is the regular expression k+ae+t+. With the
above construction, marginal log loss applied to HMMs is
equivalent to lattice-free MMI [48].

For ASG, the search space is equivalent to that of one-
state HMMs. Instead of assuming conditional independence as
in CTC, ASG includes transition probabilities between states.
The constraint FST is identical to that of HMMs, with repeated
ground-truth labels. However, in ASG the weights on the edges
are not locally normalized, so the partition function Z(x) is
not always 1 and has to be computed. With the above search
space construction, marginal log loss becomes ASG.

Another approach similar to CTC proposed in [49] is called
RNN transducers. The search space of an RNN transducer is
the set of alignments from the speech signal to all possible
label sequences, so the search space grows exponentially in
the number of labels. The weight function of a path in this
approach relies on an RNN, and is not decomposable as a sum
of weights of the edges. RNN transducers are trained with
marginal log loss. By the independence assumption imposed
the partition function Z(x) is still 1, so we do
in [49],
not need to marginalize over the exponentially large space.
During decoding, however, we still have to search over the
exponentially large space with, for example, beam search.

In view of this framework, even when using the same loss
function, i.e., marginal log loss, segmental models and frame-
based models differ in their search space, weight functions,
and how the search space is constrained by the ground truth
labels during training.

C. Word recognition

First-pass segmental models have previously been success-
fully applied to word recognition [13], [50]. This previous
work treats ﬁrst-pass segmental models as a drop-in replace-
ment for HMM phoneme recognizers, because both models
serve as functions that map acoustic features to phoneme
strings. The phoneme recognizers are then composed with a
lexicon and a language model to form a word recognizer.

Recent work has explored models that directly predict
characters, avoiding the need for a lexicon [51], [52], [53] but
still allowing for improved performance when constraining the
search space with a lexicon (through FST composition) [53].
Segmental models can also be used to predict characters
simply by changing the label set.

Instead of using intermediate discrete representations, such
as phonemes or characters, recent advances in computing
power have made it feasible to directly predict words [54],
[55], [56], [57]. In this case, rather than using a pronunciation
dictionary, only a list of words is needed for decoding.
Segmental models can also be used to directly predict words
by using the list of words as the label set. This approach
is worth exploring further, although efﬁciency issues make it
nontrivial to train such models [57].

VIII. CONCLUSION
We have presented the formal framework of segmental
models and several potential losses for training such models.
Segmental models are now able to run efﬁciently enough for
end-to-end training and obtain competitive error rates. We
have explored segmental models with two types of weight
functions and various training losses on the task of phonetic
recognition. We have found that the best results obtained with
the two types of weight functions (frame classiﬁer-based and
segmental recurrent neural networks) are quite similar, and are
typically best with marginal log loss.

We also consider the relationship between segmental models
and frame-based models trained with CTC. Both models, while
having different search spaces and different weight functions,
are optimizing the same loss, the marginal log loss. Empiri-
cally, with the same feature encoder and the same optimizer,
there is no signiﬁcant difference between the two in terms of
ﬁnal performance. However, each type of model beneﬁts from
training jointly with the other in a multitask training approach.
We hope that drawing the connection between these models
will spawn more research in exploring different search spaces
and loss functions. In future work, we plan to extend this study
of segmental models to word recognition by exploiting other
efﬁciency and performance trade-offs.

REFERENCES

[1] F. Jelinek, “Continuous speech recognition by statistical methods,”

Proceedings of the IEEE, vol. 64, no. 4, pp. 532–556, 1976.

[2] L. Rabiner, “A tutorial on hidden Markov models and selected applica-
tions in speech recognition,” Proceedings of the IEEE, vol. 77, no. 2,
pp. 257–286, 1989.

[3] L. Bahl, F. Jelinek, and R. Mercer, “A maximum likelihood approach
to speech recognition,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 5, pp. 179–190, 1983.

[4] E. Fosler-Lussier, Y. He, P. Jyothi, and R. Prabhavalkar, “Conditional
random ﬁelds in speech, audio, and language processing,” Proceedings
of the IEEE, vol. 101, no. 5, pp. 1054–1075, 2013.

[5] N. Smith and M. Gales, “Speech recognition using SVMs,” in Advances

in neural information processing systems (NIPS), 2001.

[6] V. Zue, J. Glass, M. Phillips, and S. Seneff, “Acoustic segmentation and
phonetic classiﬁcation in the SUMMIT system,” in IEEE International
Conference on Acoustics, Speech, and Signal Processing (ICASSP),
1989.

[7] M. De Wachter, M. Matton, K. Demuynck, P. Wambacq, R. Cools, and
D. Van Compernolle, “Template-based continuous speech recognition,”
IEEE Transactions on Audio, Speech, and Language Processing, vol. 15,
no. 4, pp. 1377–1390, 2007.

IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, VOL. X, NO. Y, AUGUST 2017

11

[8] M. A. Bush and G. E. Kopec, “Network-based connected digit recog-
nition using vector quantization,” in IEEE International Conference on
Acoustics, Speech, and Signal Processing (ICASSP), 1985.

[9] G. Zweig and P. Nguyen, “A segmental CRF approach to large vocab-
ulary continuous speech recognition,” in IEEE Workshop on Automatic
Speech Recognition and Understanding (ASRU), 2009.

[10] G. Chung and S. Seneff, “Hierarchical duration modelling for speech
recognition using the ANGIE framework.” in Eurospeech, 1997.
[11] M. Hasegawa-Johnson, J. Baker, S. Borys, K. Chen, E. Coogan,
S. Greenberg, A. Juneja, K. Kirchhoff, K. Livescu, S. Mohan et al.,
“Landmark-based speech recognition: Report of the 2004 Johns Hopkins
summer workshop,” in IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP), 2005.

[12] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, and D. S. Pallett,
“DARPA TIMIT acoustic-phonetic continous speech corpus cd-rom. nist
speech disc 1-1.1,” NASA STI/Recon technical report, vol. 93, 1993.

[13] J. R. Glass, “A probabilistic framework for segment-based speech
recognition,” Computer Speech & Language, vol. 17, no. 2, pp. 137–152,
2003.

[14] G. Zweig, “Classiﬁcation and recognition with direct segment models,”
in IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), 2012.

[15] Y. He and E. Fosler-Lussier, “Efﬁcient segmental conditional random

ﬁelds for phone recognition,” in INTERSPEECH, 2012.

[16] O. Abdel-Hamid, L. Deng, D. Yu, and H. Jiang, “Deep segmental neural

networks for speech recognition,” in INTERSPEECH, 2013.

[17] H. Tang, W. Wang, K. Gimpel, and K. Livescu, “Discriminative seg-
mental cascades for feature-rich phone recognition,” in IEEE Workshop
on Automatic Speech Recognition and Understanding (ASRU), 2015.

[18] L. Lu, L. Kong, C. Dyer, N. A. Smith, and S. Renals, “Segmental
recurrent neural networks for end-to-end speech recognition,” in IN-
TERSPEECH, 2016.

[19] M. Ostendorf, V. Digalakis, and O. Kimball, “From HMM’s to segment
models: A uniﬁed view of stochastic modeling for speech recognition,”
IEEE Transactions on Speech and Audio Processing, pp. 360–378, 1996.
[20] S. Sarawagi and W. W. Cohen, “Semi-Markov conditional random
ﬁelds for information extraction.” in Advances in Neural Information
Processing Systems (NIPS), vol. 17, 2004.

[21] S.-X. Zhang and M. J. Gales, “Structured SVMs for automatic speech
recognition,” IEEE Transactions on Audio, Speech, and Language Pro-
cessing, vol. 21, no. 3, pp. 544–555, 2013.

[22] L. Kong, C. Dyer, and N. A. Smith, “Segmental recurrent neural
networks,” in International Conference on Learning Representations
(ICLR), 2016.

[23] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural

computation, vol. 9, no. 8, pp. 1735–1780, 1997.

Computational linguistics, vol. 23, no. 2, pp. 269–311, 1997.

[25] ——, “Semiring frameworks and algorithms for shortest-distance prob-
lems,” Journal of Automata, Languages and Combinatorics, vol. 7, no. 3,
pp. 321–350, 2002.

[26] H. Tang, W. Wang, K. Gimpel, and K. Livescu, “End-to-end training
approaches for discriminative segmental models,” in IEEE Workshop on
Spoken Language Technology (SLT), 2016.

[27] H. Tang, K. Gimpel, and K. Livescu, “A comparison of training
approaches for discriminative segmental models,” in INTERSPEECH,
2014.

[28] G. Heigold, T. Deselaers, R. Schl¨uter, and H. Ney, “Modiﬁed
MMI/MPE: A direct evaluation of the margin in speech recognition,” in
International Conference on Machine learning (ICML), 2008.

[29] E. McDermott, S. Watanabe, and A. Nakamura, “Discriminative training
based on an integrated view of MPE and MMI in margin and error
space,” in IEEE International Conference on Acoustics Speech and
Signal Processing (ICASSP), 2010.

[30] L. Lu, L. Kong, C. Dyer, and N. A. Smith, “Multi-task learning
with CTC and segmental CRF for speech recognition,” CoRR, vol.
abs/1702.06378, 2017.

[31] A. Graves, S. Fern´andez, F. Gomez, and J. Schmidhuber, “Connectionist
temporal classiﬁcation: labelling unsegmented sequence data with recur-
rent neural networks,” in International Conference on Machine Learning
(ICML), 2006.

[32] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel,
M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., “The Kaldi
speech recognition toolkit,” in IEEE Workshop on Automatic Speech
Recognition and Understanding (ASRU), 2011.

[33] K.-F. Lee, “On large-vocabulary speaker-independent continuous speech
recognition,” Speech communication, vol. 7, no. 4, pp. 375–379, 1988.

[34] V. Vanhoucke, M. Devin, and G. Heigold, “Multiframe deep neural
networks for acoustic modeling,” in IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), 2013.

[35] Y. Miao, J. Li, Y. Wang, S. Zhang, and Y. Gong, “Simplifying long short-
term memory acoustic models for fast training and decoding,” in IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2015.

[36] W. Zaremba, I. Sutskever, and O. Vinyals, “Recurrent neural network

regularization,” CoRR, vol. abs/1409.2329, 2014.

[37] X. Glorot and Y. Bengio, “Understanding the difﬁculty of training deep

feedforward neural networks,” in AISTATS, 2010.

[38] T. Tieleman and G. Hinton, “Lecture 6.5-RMSprop: Divide the gradient
by a running average of its recent magnitude,” COURSERA: Neural
networks for machine learning, 2012.

[39] H. Tang, W. Wang, K. Gimpel, and K. Livescu, “Efﬁcient segmental

cascades for speech recognition,” in INTERSPEECH, 2016.

[40] P. Doetsch, S. Hegselmann, R. Schl¨uter, and H. Ney, “Inverted HMM –
a proof of concept,” in NIPS 2016 End-to-end Learning for Speech and
Audio Processing Workshop, 2016.

[41] Y. He, “Segmental models with an exploration of acoustic and lexical
grouping in automatic speech recognition,” Ph.D. dissertation, The Ohio
State University, 2015.

[42] L. T´oth, “Phone recognition with hierarchical convolutional deep maxout
networks,” EURASIP Journal on Audio, Speech, and Music Processing,
vol. 2015, no. 1, p. 25, 2015.

[43] A. Halberstadt and J. Glass, “Heterogeneous measurements and multi-
ple classiﬁers for speech recognition,” in International Conference on
Spoken Language Processing, 1998.

[44] R. Collobert, C. Puhrsch, and G. Synnaeve, “Wav2letter: an end-to-end
convnet-based speech recognition system,” CoRR, vol. abs/1609.03193,
2016.

[45] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio,
“Attention-based models for speech recognition,” in Advances in Neural
Information Processing Systems (NIPS), 2015.

[46] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio, “End-
to-end attention-based large vocabulary speech recognition,” in IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2016.

[47] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell: A
neural network for large vocabulary conversational speech recognition,”
in IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), 2016.

[48] D. Povey, V. Peddinti, D. Galvez, P. Ghahrmani, V. Manohar, X. Na,
Y. Wang, and S. Khudanpur, “Purely sequence-trained neural networks
for ASR based on lattice-free MMI,” in INTERSPEECH, 2016.
[49] A. Graves, “Sequence transduction with recurrent neural networks,”

[50] Y. He and E. Fosler-Lussier, “Segmental conditional random ﬁelds with
deep neural networks as acoustic models for ﬁrst-pass word recognition.”
in INTERSPEECH, 2015.

[51] A. Graves and N. Jaitly, “Towards end-to-end speech recognition with
recurrent neural networks,” in International Conference on Machine
Learning (ICML), 2014.

[52] A. L. Maas, Z. Xie, D. Jurafsky, and A. Y. Ng, “Lexicon-free conver-
sational speech recognition with neural networks,” in Human Language
Technologies: The Annual Conference of the North American Chapter
of the ACL, 2015.

[53] Y. Miao, M. Gowayyed, and F. Metze, “EESEN: End-to-end speech
recognition using deep RNN models and WFST-based decoding,” in
IEEE Workshop on Automatic Speech Recognition and Understanding
(ASRU), 2015.

[54] A. L. Maas, S. D. Miller, T. M. O’neil, A. Y. Ng, and P. Nguyen, “Word-
level acoustic modeling with convolutional vector regression,” in ICML
Workshop on Representation Learning, 2012.

[55] S. Bengio and G. Heigold, “Word embeddings for speech recognition,”

in INTERSPEECH, 2014.

[56] H. Soltau, H. Liao, and H. Sak, “Neural speech recognizer: Acoustic-
to-word LSTM model for large vocabulary speech recognition,” CoRR,
vol. abs/1610.09975, 2016.

[57] K. Audhkhasi, B. Ramabhadran, G. Saon, M. Picheny, and D. Nahamoo,
“Direct acoustics-to-word models for english conversational speech
recognition,” CoRR, vol. abs/1703.07754, 2017.

[24] M. Mohri, “Finite-state transducers in language and speech processing,”

CoRR, vol. abs/1211.3711, 2012.

