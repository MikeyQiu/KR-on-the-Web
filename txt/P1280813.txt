8
1
0
2
 
r
a

M
 
9
1
 
 
]
L
C
.
s
c
[
 
 
2
v
0
9
5
9
0
.
9
0
7
1
:
v
i
X
r
a

An attentive neural architecture for joint segmentation and
parsing and its application to real estate ads

Giannis Bekoulis∗, Johannes Deleu, Thomas Demeester, Chris Develder

Ghent University – imec, IDLab, Department of Information Technology,
Technologiepark Zwijnaarde 15, 9052 Ghent, Belgium

Abstract

In processing human produced text using natural language processing (NLP) tech-

niques, two fundamental subtasks that arise are (i) segmentation of the plain text into

meaningful subunits (e.g., entities), and (ii) dependency parsing, to establish relations

between subunits. Such structural interpretation of text provides essential building

blocks for upstream expert system tasks: e.g., from interpreting textual real estate ads,

one may want to provide an accurate price estimate and/or provide selection ﬁlters for

end users looking for a particular property — which all could rely on knowing the

types and number of rooms, etc. In this paper we develop a relatively simple and ef-

fective neural joint model that performs both segmentation and dependency parsing

together, instead of one after the other as in most state-of-the-art works. We will fo-

cus in particular on the real estate ad setting, aiming to convert an ad to a structured

description, which we name property tree, comprising the tasks of (1) identifying im-

portant entities of a property (e.g., rooms) from classiﬁeds and (2) structuring them

into a tree format. In this work, we propose a new joint model that is able to tackle

the two tasks simultaneously and construct the property tree by (i) avoiding the error

propagation that would arise from the subtasks one after the other in a pipelined fash-

ion, and (ii) exploiting the interactions between the subtasks. For this purpose, we

perform an extensive comparative study of the pipeline methods and the new proposed

∗Corresponding author
Email addresses: giannis.bekoulis@ugent.be (Giannis Bekoulis),

johannes.deleu@ugent.be (Johannes Deleu), thomas.demeester@ugent.be (Thomas
Demeester), chris.develder@ugent.be (Chris Develder)

Preprint submitted to Expert Systems with Applications

March 20, 2018

joint model, reporting an improvement of over three percentage points in the overall

edge F1 score of the property tree. Also, we propose attention methods, to encourage

our model to focus on salient tokens during the construction of the property tree. Thus

we experimentally demonstrate the usefulness of attentive neural architectures for the

proposed joint model, showcasing a further improvement of two percentage points in

edge F1 score for our application. While the results demonstrated are for the particular

real estate setting, the model is generic in nature, and thus could be equally applied to

other expert system scenarios requiring the general tasks of both (i) detecting entities

(segmentation) and (ii) establishing relations among them (dependency parsing).

Keywords: neural networks, joint model, relation extraction, entity recognition,

dependency parsing

1. Introduction

Many consumer-oriented digital applications rely on input data provided by their tar-

get audience. For instance, real estate websites gather property descriptions for the

offered classiﬁeds, either from realtors or from individual sellers.

In such cases, it

is hard to strike the right balance between structured and unstructured information:

enforcing restrictions or structure upon the data format (i.e., predeﬁned form) may re-

duce the amount or diversity of the data, while unstructured data (i.e., raw text) may

require non-trivial (i.e., hard to automate) transformation to a more structured form to

be useful/practical for the intended application. In the real estate domain, textual ad-

vertisements are an extremely useful but highly unstructured way of representing real

estate properties. However, structured descriptions of the advertisements are very help-

ful, e.g., for real estate agencies to suggest the most appropriate sales/rentals for their

customers, while keeping human reading effort limited. For example, special search

ﬁlters, which are usually used by clients, cannot be directly applied to textual advertise-

ments. On the contrary, a structured representation of the property (e.g., a tree format

of the property) enables the simpliﬁcation of the unstructured textual information by

applying speciﬁc ﬁlters (e.g., based on the number of bedrooms, number of ﬂoors, or

the requirement of having a bathroom with a toilet on the ﬁrst ﬂoor), and it also ben-

2

eﬁts other related applications such as automated price prediction (Pace et al., 2000;

Nagaraja et al., 2011).

The new real estate structured prediction problem as deﬁned by Bekoulis et al.

(2017) has as main goal to construct the tree-like representation of the property (i.e.,

the property tree) based on its natural language description. This can be approached as

a relation extraction task by a pipeline of separate subtasks, comprising (i) named entity

recognition (NER) (Nadeau & Sekine, 2007) and (ii) relation extraction (Bach & Badaskar,

2007). Unlike previous studies (Li & Ji, 2014; Miwa & Bansal, 2016) on relation ex-

traction, in the work of Bekoulis et al. (2017), the relation extraction module is re-

placed by dependency parsing. Indeed, the relations that together deﬁne the structure

of the house should form a tree, where entities are part-of one another (e.g., a ﬂoor

is part-of a house, a room is part-of a ﬂoor). This property tree is structurally simi-

lar to a parse tree. Although the work of Bekoulis et al. (2017) is a step towards the

construction of the property tree, it follows a pipeline setting, which suffers from two

serious problems: (i) error propagation between the subtasks, i.e., NER and depen-

dency parsing, and (ii) cross-task dependencies are not taken into account, e.g., terms

indicating relations (includes, contains, etc.) between entities that can help the NER

module are neglected. Due to the unidirectional nature of stacking the two modules

(i.e., NER and dependency parsing) in the pipeline model, there is no information ﬂow-

ing from the dependency parsing to the NER subtask. This way, the parser is not able

to inﬂuence the predictions of the NER. Other studies on similar tasks (Li & Ji, 2014;

Kate & Mooney, 2010) have considered the two subtasks jointly. They simultaneously

extract entity mentions and relations between them usually by implementing a beam-

search on top of the ﬁrst module (i.e., NER), but these methods require the manual

extraction of hand-crafted features. Recently, deep learning with neural networks has

received much attention and several approaches (Miwa & Bansal, 2016; Zheng et al.,

2017) apply long short-term memory (LSTM) recurrent neural networks and convo-

lutional neural networks (CNNs) to achieve state-of-the-art performance on similar

problems. Those models rely on shared parameters between the NER and relation ex-

traction components, whereby the NER module is typically pre-trained separately, to

improve the training effectiveness of the joint model.

3

In this work, we propose a new joint model to solve the real estate structured pre-

diction problem. Our model is able to learn the structured prediction task without

complicated feature engineering. Whereas previous studies (Miwa & Bansal, 2016;

Zheng et al., 2017; Li et al., 2016, 2017) on joint methods focus on the relation extrac-

tion problem, we construct the property tree which comes down to solving a depen-

dency parsing problem, which is more constrained and hence more difﬁcult. Therefore,

previous methods are not directly comparable to our model and cannot be applied to

our real estate task out-of-the-box. In this work, we treat the two subtasks as one by

reformulating them into a head selection problem (Zhang et al., 2017).

This paper is a follow-up work of Bekoulis et al. (2017). Compared to the con-

ference paper that introduced the real estate extraction task and applied some basic

state-of-the-art techniques as a ﬁrst baseline solution, we now introduce: (i) advanced

neural models that consider the two subtasks jointly and (ii) modiﬁcations to the dataset

annotation representations as detailed below. More speciﬁcally, the main contributions

of this work are the following:

• We propose a new joint model that encodes the two tasks of identifying enti-

ties as well as dependencies between them, as a single head selection problem,

without the need of parameter sharing or pre-training of the ﬁrst entity recogni-

tion module separately. Moreover, instead of (i) predicting unlabeled dependen-

cies and (ii) training an additional classiﬁer to predict labels for the identiﬁed

heads (Zhang et al., 2017), our model already incorporates the dependency label

predictions in its scoring formula.

• We compare the proposed joint model against established pipeline approaches

and report an F1 improvement of 1.4% in the NER and 6.2% in the dependency

parsing subtask, corresponding to an overall edge F1 improvement of 3.4% in

the property tree.

• Compared to our original dataset (Bekoulis et al., 2017), we introduce two ex-

tensions to the data: (i) we consistently assign the ﬁrst mention of a particular

entity in order of appearance in the advertisement as the main mention of the

entity. This results in an F1 score increase of about 3% and 4% for the joint and

4

pipeline models, respectively. (ii) We add the equivalent relation to our anno-

tated dataset to explicitly express that several mentions across the ad may refer

to the same entity.

• We perform extensive analysis of several attention mechanisms that enable our

LSTM-based model to focus on informative words and phrases, reporting an

improved F1 performance of about 2.1%.

The rest of the paper is structured as follows. In Section 2, we review the related

work. Section 3 deﬁnes the problem and in Section 4, we describe the methodology

followed throughout the paper and the proposed joint model. The experimental results

are reported in Section 5. Finally, Section 6 concludes our work.

2. Related work

The real estate structured prediction problem from textual advertisements can be bro-

ken down into the sub-problems of (i) sequence labeling (identifying the core parts

of the property) and (ii) non-projective dependency parsing (connecting the identi-

ﬁed parts into a tree-like structure) (Bekoulis et al., 2017). One can address these two

steps either one by one in a pipelined approach, or simultaneously in a joint model.

The pipeline approach is the most commonly used approach (Bekoulis et al., 2017;

Fundel et al., 2007; Gurulingappa et al., 2012), treating the two steps independently

and propagating the output of the sequence labeling subtask (e.g., named entity recog-

nition) (Chiu & Nichols, 2016; Lample et al., 2016) to the relation classiﬁcation mod-

ule (dos Santos et al., 2015; Xu et al., 2015). Joint models are able to simultaneously

extract entity mentions and relations between them (Li & Ji, 2014; Miwa & Bansal,

2016). In this work, we propose a new joint model that is able to recover the tree-

like structure of the property and frame it as a dependency parsing problem, given the

non-projective tree structure we aim to output. We now present related works for the

sequence labeling and dependency parsing subtasks, as well as for the joint models.

5

2.1. Sequence labeling

Structured prediction problems become challenging due to the large output space.

Speciﬁcally in NLP, sequence labeling (e.g., NER) is the task of identifying the en-

tity mention boundaries and assigning a categorical label (e.g., POS tags) for each

identiﬁed entity in the sentence. A number of different methods have been proposed,

namely Hidden Markov Models (HMMs) (Rabiner & Juang, 1986), Conditional Ran-
dom Fields (CRFs) (Lafferty et al., 2001), Maximum Margin Markov Network (M3N)

(Taskar et al., 2003), generalized support vector machines for structured output (SVMstruct)

(Tsochantaridis et al., 2004) and Search-based Structured Prediction (SEARN) (Daum´e III et al.,

2009). Those methods heavily rely on hand-crafted features and an in-depth review can

be found in Nguyen & Guo (2007). Several variations of these models that also require

manual feature engineering have been used in different application settings (e.g., biol-

ogy, social media context) and languages (e.g., Turkish) (Jung, 2012; K¨uc¸ ¨uk & Yazıcı,

2012; Atkinson & Bull, 2012; Konkol et al., 2015). Recently, deep learning with neu-

ral networks has been succesfully applied to NER. Collobert et al. (2011) proposed

to use a convolutional neural network (CNN) followed by a CRF layer over a se-

quence of word embeddings. Recurrent Neural Networks (RNNs) constitute another

neural network architecture that has attracted attention, due to the state-of-the-art per-

formance in a series of NLP tasks (e.g., sequence-to-sequence (Sutskever et al., 2014),

parsing (Kiperwasser & Goldberg, 2016)). In this context, Gillick et al. (2016) use a

sequence-to-sequence approach for modeling the sequence labeling task. In addition,

several variants of combinations between LSTM and CRF models have been proposed

(Lample et al., 2016; Huang et al., 2015; Ma & Hovy, 2016) achieving state-of-the-art

performance on publicly available datasets.

2.2. Dependency parsing

Dependency parsing is a well studied task in the NLP community, which aims to ana-

lyze the grammatical structure of a sentence. We approach the problem of the property

tree construction as a dependency parsing task i.e., to learn the dependency arcs of

the classiﬁed. There are two well-established ways to address the dependency parsing

problem, via graph-based and transition-based parsers.

6

Graph-based: In the work of McDonald et al. (2005); McDonald & Pereira (2007)

dependency parsing requires the search of the highest scoring maximum spanning

tree in graphs for both projective (dependencies are not allowed to cross) and non-

projective (crossing dependencies are allowed) trees with the Eisner algorithm (Eisner,

1996) and the Chu-Liu-Edmonds algorithm (Chu & Liu, 1965; Edmonds, 1967) re-

spectively. It was shown that exploiting higher-order information (e.g., siblings, grand-

parental relation) in the graph, instead of just using ﬁrst-order information (i.e., par-

ent relations) (Carreras, 2007; Zhang & McDonald, 2012) may yield signiﬁcant im-

provements of the parsing accuracy but comes at the cost of an increased model com-

plexity. Koo et al. (2007) made an important step towards globally normalized mod-

els with hand-crafted features, by adapting the Matrix-Tree Theorem (MTT) (Tutte,

2001) to train over all non-projective dependency trees. We explore an MTT approach

as one of the pipeline baselines. Similar to recent advances in neural graph-based

parsing (Zhang et al., 2017; Kiperwasser & Goldberg, 2016; Wang & Chang, 2016),

we use LSTMs to capture richer contextual information compared to hand-crafted fea-

ture based methods. Our work is conceptually related to Zhang et al. (2017), who

formulated the dependency parsing problem as a head selection problem. We go a step

further in that direction, in formulating the joint parsing and labeling problem in terms

of selecting the most likely combination of head and label.

Transition-based: Transition-based parsers (Yamada & Matsumoto, 2003; Nivre et al.,

2006) replace the exact inference of the graph-based parsers by an approximate but

faster inference method. The dependency parsing problem is now solved by an ab-

stract state machine that gradually builds up the dependency tree token by token. The

goal of this kind of parsers is to ﬁnd the most probable transition sequence from an

initial to some terminal conﬁguration (i.e., a dependency parse tree, or in our case

a property tree) given a permissible set of actions (i.e., LEFT-ARC, RIGHT-ARC,

SHIFT) and they are able to handle both projective and non-projective dependen-

cies (Nivre, 2003, 2009). In the simplest case (i.e., greedy inference), a classiﬁer pre-

dicts the next transition based on the current conﬁguration. Compared to graph-based

dependency parsers, transition-based parsers are able to scale better due to the linear
time complexity while graph-based complexity rises to O(n2) in the non-projective

7

case. Chen & Manning (2014) proposed a way of learning a neural network classiﬁer

for use in a greedy, transition-based dependency parser while using low-dimensional,

dense word embeddings, without the need of manually extracting features. Globally

normalized transition-based parsers (Andor et al., 2016) can be considered an exten-

sion of Chen & Manning (2014), as they perform beam search for maintaining multiple

hypotheses and introduce global normalization with a CRF objective. Dyer et al. (2015)

introduced the stack-LSTM model with push and pop operations which is able to learn

the parser transition states while maintaining a summary embedding of its contents.

Although transition-based systems are well-known for their speed and state-of-the-art

performance, we do not include them in our study due to their already reported poor

performance in the real estate task (Bekoulis et al., 2017) compared to graph-based

parsers.

2.3. Joint learning

Adopting a pipeline strategy for the considered type of problems has two main draw-

backs: (i) sequence labeling errors propagate to the dependency parsing step, e.g., an

incorrectly identiﬁed part of the house (entity) could get connected to a truly existing

entity, and (ii) interactions between the components are not taken into account (feed-

back between the subtasks), e.g., modeling the relation between two potential entities

may help in deciding on the nature of the entities themselves. In more general rela-

tion extraction settings, a substantial amount of work (Kate & Mooney, 2010; Li & Ji,

2014; Miwa & Sasaki, 2014) jointly considered the two subtasks of entity recognition

and relation extraction. However, all of these models make use of hand-crafted features

that: (i) require manual feature engineering, (ii) generalize poorly between various ap-

plications and (iii) may require a substantial computational cost.

Recent advances on joint models for general relation extraction consider the joint

task using neural network architectures like LSTMs and CNNs (Miwa & Bansal, 2016;

Zheng et al., 2017; Li et al., 2017). Our work is however different from a typical rela-

tion extraction setup in that we aim to model directed spanning trees, or, equivalently,

non-projective dependency structures. In particular, the entities involved in a relation

are not necessarily adjacent in the text since other entities may be mentioned in be-

8

Entity type

Description

Examples

property
ﬂoor
space
subspace
ﬁeld

extra building

The property.
A ﬂoor in a building.
A room within the building.
A part of a room.
An open space inside or outside
the building.
An additional building which is
also part of the property.

bungalow, apartment
ground ﬂoor
bedroom, bathroom
shower, toilet
bbq, garden

garden house

Table 1: Real estate entity types.

tween, which complicates parsing. Indeed, in this work we focus on dependency pars-

ing due to the difﬁculty of establishing the tree-like structure instead of only relation

extraction (where each entity can have arbitrary relation arcs, regardless of other enti-

ties and their relations), which is the case for previously cited joint models. Moreover,

unlike most of these works that frame the problem as a stacking of the two components,

or at least ﬁrst train the NER module to recognize the entities and then further train to-

gether with the relation classiﬁcation module, we include the NER directly inside the

dependency parsing component.

In summary, the conceptual strengths of our joint segmentation and dependency

parsing approach (described in detail in Section 4) will be the following: compared to

state-of-the-art joint models in relation extraction, it (i) is generic in nature, without

requiring any manual feature engineering, (ii) extracts a complete tree structure rather

than a single binary relation instance.

3. Problem deﬁnition

In this section, we deﬁne the speciﬁc terms that are used in our real estate structured

prediction problem. We deﬁne an entity as an unambiguous, unique part of a property

with independent existence (e.g., bedroom, kitchen, attic). An entity mention is de-

ﬁned as one or more sequential tokens (e.g., “large apartment”) that can be potentially

linked to one or more entities. An entity mention has a unique semantic meaning and

refers to a speciﬁc entity, or a set of similar entities (e.g., “six bedrooms”). An entity

9

O r i g i n a l ad :
The p r o p e r t y i n c l u d e s a l a r g e
a p a r t m e n t w i t h a g a r a g e . The
home h a s a l i v i n g room , 3 s p a c i o u s b ed r o o m s and a b a t h r o o m .
The g a r a g e
e q u i p p e d w i t h a g a t e and a b i k e w a l l b r a c k e t .
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
S t r u c t u r e d r e p r e s e n t a t i o n :
p r o p e r t y
l a r g e

a p a r t m e n t

i s

l i v i n g room
3 s p a c i o u s b ed r o o m s
b a t h r o o m

g a r a g e

g a t e
b i k e w a l l b r a c k e t

‘ home ’

a p a r t m e n t ’ ,

| m e n t i o n = ‘ p r o p e r t y ’
| m e n t i o n = ‘ l a r g e
| m e n t i o n = ‘ l i v i n g room ’
| m e n t i o n = ‘3 s p a c i o u s bedrooms ’
| m e n t i o n = ‘ b ath r o o m ’
| m e n t i o n = ‘ g a r a g e ’
| m e n t i o n = ‘ g a t e ’
| m e n t i o n = ‘ b i k e w a l l b r a c k e t ’

Figure 1: Fictitious sample unstructured ad and corresponding structured representation as a property tree.

itself is part-of another entity and can be mentioned in the text more than once with

different entity mentions. For instance, a “house” entity could occur in the text with

entity mentions “large villa” and “a newly built house”. For the pipeline setting as pre-

sented in Bekoulis et al. (2017), we further classify entities into types (assign a named

entity type to every word in the ad). The task is transformed to a sequence labeling

problem using BIO (Beginning, Inside, Outside) encoding. The entity types are listed

in Table 1. For instance, in the sequence of tokens “large apartment”, B-PROPERTY is

assigned to the token “large” as the beginning of the entity, I-PROPERTY in the token

“apartment” as the inside of the entity but not the ﬁrst token within the entity and O

for all the other tokens that are not entities. Unlike previous studies (Miwa & Bansal,

2016; Zheng et al., 2017; Li et al., 2016, 2017), for our joint model there is no need for

this type of categorical classiﬁcation into labels since the two components are treated

uniﬁed as a single dependency parsing problem.

The goal of the real estate structured prediction task is to map the textual prop-

erty classiﬁed into a tree-like structured representation, the so-called property tree, as

illustrated in Fig. 1. In the pipeline setting, this conversion implies the detection of

(i) entities of various types and (ii) the part-of dependencies between them. For in-

stance, the entity “living room” is part-of the entity “large apartment”. In the joint

model, each token (e.g., “apartment”, “living”, “bathroom”, “includes”, “with”, “3”) is

examined separately and 4 different types of relations are deﬁned, namely part-of, seg-

10

ment, skip and equivalent. The part-of relation is similar to the way that it was deﬁned

in the pipeline setting but instead of examining entities, i.e., sequences of tokens (e.g.,

“living room”), we examine if a (individual) token is part-of another (individual) token

(e.g., “room” is part-of the “apartment”). We encode the entity identiﬁcation task with

the segment label and we follow the same approach as in the part-of relationships for

the joint model. Speciﬁcally, we examine if a token is a segment of another token (e.g.,

the token “room” is attached as a segment to the token “living”, “3” is attached as a seg-

ment to the token “bedrooms” and “spacious” is also attached as a segment to the token

“bedrooms” — this way we are able to encode the segment “3 spacious bedrooms”). By

doing so, we cast the sequence labeling subtask to a dependency parsing problem. The

tokens that are referring to the same entity belong to the equivalent relation (“home”

is equivalent to “apartment”). For each entity, we deﬁne the ﬁrst mention in order of

appearance in the text as main mention and the rest as equivalent to this main mention.

Finally, each token that does not have any of the aforementioned types of relations has

a skip relation with itself (e.g., “includes” has a skip relation with “includes”), such that

each token has a uniquely deﬁned head.

Thus, we cast the structured prediction task of extracting the property tree from the

ad as a dependency parsing problem, where (i) an entity can be part-of only one (other)

entity, because the decisions are taken simultaneously for all part-of relations (e.g., a

certain room can only be part-of a single ﬂoor), and (ii) there are a priori no restrictions

on the type of entities or tokens that can be part-of others (e.g., a room can be either

part-of a ﬂoor, or the property itself, like an apartment). It is worth mentioning that

dependency annotations for our problem exhibit a signiﬁcant number of non-projective

arcs (26%) where part-of dependencies are allowed to cross (see Fig. 3), meaning that

entities involved in the part-of relation are non-adjacent (i.e., interleaved by other en-

tities). For instance, all the entities or the tokens for the pipeline and the joint models,

that are attached to the entity “garage” are overlapping with the entities that are at-

tached to the entity “apartment”, making parsing even more complicated: handling

only projective dependencies as illustrated in Fig. 2 is an easier task. We note that the

segment dependencies do not suffer from non-projectivity, since the tokens are always

adjacent and sequential (e.g., “3 spacious bedrooms”).

11

0
property

1
large
apartment

2
living
room

3
3 spacious
bedrooms

4
bathroom

Figure 2: An example graph of projective part-of dependencies.

0
property

1
large
apartment

2
garage

3
living 
room

4
3 spacious
bedrooms

5
bathroom

6
bike wall
bracket

7
gate

Figure 3: Graph representing the part-of dependencies of Fig. 1. The dashed arcs are representing the
non-projective dependencies.

4. Methodology

We now describe the two approaches, i.e., the pipeline model and the joint model to

construct the property tree of the textual advertisements, as illustrated in Fig. 4. For

the pipeline system (Section 4.1), we (1) identify the entity mentions (Section 4.1.1),

then (2) predict the part-of dependencies between them (Section 4.1.2), and ﬁnally (3) con-

struct the tree representation (i.e., property tree) of the textual classiﬁed (e.g., as in

Fig. 1).

In step (2), we apply locally or globally trained graph-based models. We

represent the result of step (2) as a graph model, and then solve step (3) by apply-

ing the maximum spanning tree algorithm (Chu & Liu, 1965; Edmonds, 1967) for

the directed case (see McDonald et al. (2005)). We do not apply the well-known and

fast transition-based systems with hand-crafted features for non-projective dependency

structures (Nivre, 2009; Bohnet & Nivre, 2012), given the previously established poor

performance thereof in Bekoulis et al. (2017). In Section 4.2, we describe the joint

model where we perform steps (1) and (2) jointly. For step (3), we apply the maximum

spanning tree algorithm (Chu & Liu, 1965; Edmonds, 1967) similarly as in the pipeline

setting (Section 4.1).

4.1. Two-step pipeline

Below we revisit the pipeline approach presented in Bekoulis et al. (2017), which serves

as the baseline which we compare the neural models against. As mentioned before, the

pipeline model comprises two subtasks: (1) the sequence labeling and the (2) part-of

12

tree construction. In the following subsections, we describe the methods applied for

both.

4.1.1. Sequence labeling

The ﬁrst step in our pipeline approach is the sequence labeling subtask which is similar

to NER. Assuming a textual real estate classiﬁed, we (i) identify the entity mention

boundaries and (ii) map each identiﬁed entity mention to a categorical label, i.e., entity

type. In general, in the sequence labeling tasks, it is beneﬁcial to take into account

the correlations between labels in adjacent tokens, i.e., consider the neighborhood, and

jointly ﬁnd the most probable chain of labels for the given input sentence (Viterbi

algorithm for the most probable assignment). For instance, in our problem where we

follow the NER standard BIO encoding (Ratinov & Roth, 2009), the I-PROPERTY

cannot be followed by I-SPACE without ﬁrst opening the type by B-SPACE. We use

a special case of the CRF algorithm (Lafferty et al., 2001; Peng & McCallum, 2006),

namely linear chain CRFs, which is commonly applied in the problem of sequence

labeling to learn a direct mapping from the feature space to the output space (types)

where we model label sequences jointly, instead of decoding each label independently.

A linear-chain CRF with parameters w deﬁnes a conditional probability Pw(y|x) for

the sequence of labels y = y1, ..., yN given the tokens of the text advertisement x =

x1, ..., xN to be

Pw(y|x) =

exp(wT φ(x, y)),

1
Z(x)

(1)

where Z is the normalization constant and φ is the feature function that computes a

feature vector given the advertisement and the sequence of labels.

4.1.2. Part-of tree construction

The aim of the part-of tree construction subtask is to link each entity to its parent.

We approach the task as a dependency parsing problem but instead of connecting each

token to its syntactical parent, we map only the entity set I (e.g., “large villa”, “3

spacious bedrooms”) that has already been extracted by the sequence labeling subtask

to a dependency structure y. Assuming the entity set I = {e0, e1, ..., et} where t is the

number of identiﬁed entities, a dependency is a pair (p, c) where p ∈ I is the parent

13

two-step pipeline

(1) entity
recognition

(2) part-of RE

(1+2) joint entity recognition
& part-of RE

.
.
.

(3) tree
construction

joint model

Figure 4: The full structured prediction system setup.

entity and c ∈ I is the child entity. The entity e0 is the dummy root-symbol that only

appears as parent.

We will compare two approaches to predict the part-of relations: a locally trained

model (LTM) scoring all candidate edges independently, versus a global model (MTT)

which jointly scores all edges as a whole.

Locally trained model (LTM)

In the locally trained model (LTM), we adopt a traditional local discriminative method

and apply a binary classiﬁcation framework (Yamada & Matsumoto, 2003) to learn the

part-of relation model (step (2)), based on standard relation extraction features such

as the parent and child tokens and their types, the tokens in between, etc. For each

candidate parent-child pair, the classiﬁer gives a score that indicates whether it is prob-

able for the part-of relation to hold between them. The output scores are then used

for step (3), to construct the ﬁnal property tree. Following McDonald et al. (2005);

McDonald & Pereira (2007), we view the entity set I as a fully connected directed

graph G = {V, E} with the entities e1, ..., et as vertices (V ) in the graph G, and edges

E representing the part-of relations with the respective classiﬁer scores as weights.

One way to approach the problem is the greedy inference method where the predic-

tions are made independently for each parent-child pair, thus neglecting that the global

target output should form a property tree. We could adopt a threshold-based approach,

i.e., keep all edges exceeding a threshold, which obviously is not guaranteed to end

up with arc dependencies that form a tree structure (i.e., could even contain cycles).

On the other hand, we can enforce the tree structure inside the (directed) graph by

14

ﬁnding the maximum spanning tree. To this end, similar to McDonald et al. (2005);

McDonald & Pereira (2007), we apply the Edmonds’ algorithm to search for the most

probable non-projective tree structure in the weighted fully connected graph G.

Globally trained model (MTT)

The Matrix-Tree theorem (MTT) (Koo et al., 2007) is a globally normalized statistical

method that involves the learning of directed spanning trees. Unlike the locally trained

models, MTT is able to learn tree dependency structures, i.e., scoring parse trees for

a given sentence. We use D(I) to refer to all possible dependencies of the identiﬁed

entity set I, in which each dependency is represented as a tuple (h, m) in which h is

the head (or parent) and m the modiﬁer (or child). The set of all possible dependency

structures for a given entity set I is written T (I). The conditional distribution over all

dependency structures y ∈ T (I) can then be deﬁned as:

P (y|I; θ) =

1
Z(I; θ)

exp 


X
h,m∈y

θh,m





(2)

in which the coefﬁcients θh,m ∈ R for each dependency (h, m) form the real-valued

weight vector θ. The partition function Z(I; θ) is a normalization factor that alas can-

not be computed by brute-force, since it requires a summation over all y ∈ T (I),

containing an exponential number of possible dependency structures. However, an

adaptation of the MTT allows us the direct and efﬁcient computation of the partition

function Z(I; θ) as the determinant det(L(θ)) where L(θ) is the Laplacian matrix of

the graph. It is worth mentioning that although MTT learns spanning tree structures

during training, at the prediction phase, it is still required to use the maximum spanning

tree algorithm (step (3)) (McDonald et al., 2005; McDonald & Pereira, 2007) as in the

locally trained models.

4.2. Joint model

In this section, we present the new joint model sketched in Fig. 5, which simultaneously

predicts the entities in the sentence and the dependencies between them, with the ﬁnal

goal of obtaining a tree structure, i.e., the property tree. We pose the problem of the

identiﬁcation of the entity mentions and the dependency arcs between them as a head

15

Figure 5: The architecture of the joint model.

selection problem (Zhang et al., 2017). Speciﬁcally, given as input a sentence of length

N , the model outputs the predicted parent of each token of the advertisement and the

most likely dependency label between them. We begin by describing how the tokens

are represented in the model, i.e., with ﬁxed pre-trained embeddings (Section 4.2.1),

which form the input to an LSTM layer (Section 4.2.2). The LSTM outputs are used

as input to the entity and dependency scoring layer (Section 4.2.3). As an extension of

this model, we propose the use of various attention layers in between the LSTM and

scoring layer, to encourage the model to focus on salient information, as described in

Section 4.2.4. The ﬁnal output of the joint model still is not guaranteed to form a tree

structure. Therefore, we still apply Edmonds’ algorithm (i.e., step (3) from the pipeline

approach), described in Section 4.2.5.

4.2.1. Embedding Layer

The embedding layer maps each token of the input sequence x1, ..., xN of the consid-

ered advertisement to a low-dimensional vector space. We obtain the word-level em-

beddings by training the Skip-Gram word2vec model (Mikolov et al., 2013) on a large

collection of property advertisements. We add a symbol x0 in front of the N -length

input sequence, which will act as the root of the property tree, and is represented with

16

an all-zeros vector in the embedding layer.

4.2.2. Bidirectional LSTM encoding layer

Many neural network architectures have been proposed in literature: LSTMs (Hochreiter & Schmidhuber,

1997), CNNs (LeCun et al., 1989), Echo State Networks (Jaeger, 2010), or Stochastic

Conﬁguration Networks (Wang & Li, 2017), to name only a few. Many others can

be found in reference works on the topic (Goodfellow et al., 2016; Goldberg & Hirst,

2017). In this work, we use RNNs which have been proven to be particularly effective

in a number of NLP tasks (Sutskever et al., 2014; Lample et al., 2016; Miwa & Bansal,

2016). Indeed, RNNs are a common and reasonable choice to model sequential data

and inherently able to cope with varying sequence lengths. Yet, plain vanilla RNNs

tend to suffer from vanishing/exploding gradient problems and are hence not success-

ful in capturing long-term dependencies (Bengio et al., 1994; Pascanu et al., 2013).

LSTMs are a more advanced kind of RNNs, which have been successfully applied

in several tasks to capture long-term dependencies, as they are able to effectively over-

come the vanishing gradient problem. For many NLP tasks, it is crucial to represent

each word in its own context, i.e., to consider both past (left) and future (right) neigh-

boring information. An effective solution to achieve this is using a bidirectional LSTM

(BiLSTM). The basic idea is to encode each sequence from left to right (forward) and

from right to left (backward). This way, there is one hidden state which represents the

past information and another one for the future information. The high-level formulation

of an LSTM is:

hi, ci = LSTM(wi, hi−1, ci−1),

i = 0, ..., N

(3)

where in our setup wi ∈ R ˜d is the word embedding for token xi, and with the input and
states for the root symbol x0 initialized as zero vectors. Further, hi ∈ Rd and ci ∈ Rd

respectively are the output and cell state for the ith position, where d is the hidden state

size of the LSTM. Note that we chose the word embedding size the same as the LSTM
hidden state size, or ˜d = d. The outputs from left to right (forward) are written as
~hi and the outputs from the backwards direction as ~hi. The two LSTMs’ outputs at

17

position i are concatenated to form the output hi at that position of the BiLSTM:

hi = [ ~hi; ~hi],

i = 0, ..., N

(4)

4.2.3. Joint learning as head selection

In this subsection, we describe the joint learning task (i.e., identifying entities and

predicting dependencies between them), which we formulate as a head selection prob-

lem (Zhang et al., 2017). Indeed, each word xi should have a unique head (parent)

— while it can have multiple dependent words — since the ﬁnal output should form

the property tree. Unlike the standard head selection dependency parsing framework

(Zhang et al., 2017), we predict the head yi of each word xi and the relation ci between

them jointly, instead of ﬁrst obtaining binary predictions for unlabeled dependencies,

followed by an additional classiﬁer to predict the labels.

Given a text advertisement as a token sequence x = x0, x1, ..., xN where x0 is the

dummy root symbol, and a set C = {part-of, segment, equivalent, skip} of predeﬁned

labels (as deﬁned in Section 3), we aim to ﬁnd for each token xi, i ∈ {0, ..., N } the

most probable head xj , j ∈ {0, ..., N } and the most probable corresponding label c ∈

C. For convenience, we order the labels c ∈ C and identify them as ck, k ∈ {0, ..., 3}.

We model the joint probability of token xj to be the head of xi with ck the relation

between them, using a softmax:

P (head = xj, label = ck|xi) =

exp(score(hj , hi, ck))
˜j,˜k exp(score(h˜j , hi, c˜k)

P

(5)

where hi and hj are the BiLSTM encodings for words xi and xj, respectively. For

the scoring formula score(hj , hi, ck) we use a neural network layer that computes the

relative score between position i and j for a speciﬁc label ck as follows:

score(hj , hi, ck) = V T

k tanh(Ukhj + Wkhi + bk)

(6)

with trainable parameters Vk ∈ Rl, Uk ∈ Rl×2d, Wk ∈ Rl×2d, bk ∈ Rl, and l the

layer width. As detailed in Section 5.1, we set l to be smaller than 2d, similar to

Dozat & Manning (2017) due to the fact that training on superﬂuous information re-

duces the parsing speed and increases tendency towards overﬁtting. We train our model

18

by minimizing the cross-entropy loss L, written for the considered training instance as:

L =

− log P (head = yi, label = ci|xi)

(7)

N

X
i=0

where yi ∈ x and ci ∈ C are the ground truth head and label of xi, respectively. After

training, we follow a greedy inference approach and for each token, we simultaneously

keep the highest scoring head ˆyi and label ˆci for xi based on their estimated joint

probability:

( ˆyi, ˆci) = argmax
xj ∈x,ck∈C

P (head = xj, label = ck|xi)

(8)

The predictions ( ˆyi, ˆci) are made independently for each position i, neglecting that

the ﬁnal structure should be a tree. Nonetheless, as demonstrated in Section 5.2, the

highest scoring neural models are still able to come up with a tree structure for 78%

of the ads. In order to ensure a tree output in all cases, however, we apply Edmonds’

algorithm on the output.

4.2.4. Attention Layer

The attention mechanism in our structured prediction problem aims to improve the

model performance by focusing on information that is relevant to the prediction of the

most probable head for each token. As attention vector, we construct the new context
vector h∗

i as a weighted average of the BiLSTM outputs

in which the coefﬁcients a(hj, hi), also called the attention weights, are obtained as

h∗
j =

N

X
i=0

a(hj, hi) hi

a(hj, hi) =

exp(att(hj, hi))
N
˜i=0 exp(att(hj , h˜i))

.

P

(9)

(10)

The attention function att(hj, hi) is designed to measure some form of compatibility

between the representation hi for xi and hj for xj, and the attention weights a(hj, hi)

are obtained from these scores by normalization using a softmax function. In the fol-

lowing, we will describe in detail the various attention models that we tested with our

follows:

joint model.

19

Commonly used attention mechanisms

Three commonly used attention mechanisms are listed in eqs. (11) to (13): the addi-

tive (Vinyals et al., 2015), bilinear, and multiplicative attention models (Luong et al.,

2015), which have been extensively used in machine translation. Given the represen-

tations hi and hj for tokens xi and xj, we compute the attention scores as follows:

attadditive(hj, hi) = Va tanh(Uahj + Wahi + ba)

attbilinear(hj, hi) = hT

j Wbilhi

attmultiplicative(hj, hi) = hT

j hi

where Va ∈ Rl, Ua, Wa ∈ Rl×2d, Wbil ∈ R2d×2d and ba ∈ Rl are learnable parame-

ters of the model.

Biafﬁne attention

We use the biafﬁne attention model (Dozat & Manning, 2017) which has been recently

applied to dependency parsing and is a modiﬁcation of the neural graph-based approach

that was proposed by Kiperwasser & Goldberg (2016). In this model, Dozat & Manning

(2017) tried to reduce the dimensionality of the recurrent state of the LSTMs by apply-

ing a such neural network layer on top of them. This idea is based on the fact that there

is redundant information in every hidden state that (i) reduces parsing speed and (ii) in-

creases the risk of overﬁtting. To address these issues, they reduce the dimensionality

and apply a nonlinearity afterwards. The deep bilinear attention mechanism is deﬁned

as follows:

(11)

(12)

(13)

(14)

(15)

hdep
i = Vdep tanh(Udephi + bdep)

hhead
j = Vhead tanh(Uheadhj + bhead)

20

function:

Edge attention

to be:

attbiafﬁne(hhead

j

, hdep

i ) = (hhead

j

)T Wbilhdep

i + Bhhead

j

(16)

where Udep, Uhead ∈ Rl×2d, Vdep, Vhead ∈ Rp×l, Wbil ∈ Rp×p, B ∈ Rp and bdep,
bhead ∈ Rl.

Tensor attention

This section introduces the Neural Tensor Network (Socher et al., 2013) that has been

used as a scoring formula applied for relation classiﬁcation between entities. The task

can be described as link prediction between entities in an existing network of relation-

ships. We apply the tensor scoring formula as if tokens are entities, by the following

atttensor(hj, hi) = Ut tanh(hT

j Wthi + Vt(hj + hi) + bt)

(17)

where Wt ∈ R2d×l×2d, Vt ∈ Rl×2d, Ut ∈ Rl and bt ∈ Rl.

In the edge attention model, we are inspired by Gilmer et al. (2017), which applies

neural message passing in chemical structures. Assuming that words are nodes inside

the graph and the message ﬂows from node xi to xj, we deﬁne the edge representation

(18)

(19)

edge(hj, hi) = tanh(Uehj + Wehi + be)

The edge attention formula is computed as:

attedge(hj, hi) =

Asrc

edge(hj, h˜i) + Adst

1
N





N

X
˜i=0

N

X
˜j=0

edge(h˜j, hi)


where Ue, We ∈ Rl×2d, Asrc, Adst ∈ R2d×l and be ∈ Rl. The source and destination

matrices respectively encode information for the start to the end node, in the directed

edge. Running the edge attention model for several times can be achieved by stacking

the edge attention layer multiple times. This is known as message passing phase and

we can run it for several (T > 1) time steps to obtain more informative edge represen-

tations.

21

4.2.5. Tree construction step: Edmonds’ algorithm

At decoding time, greedy inference is not guaranteed to end up with arc dependencies

that form a tree structure and the classiﬁcation decision might contain cycles. In this

case, the output can be post-processed with a maximum spanning tree algorithm (as

the third step in Fig. 4). We construct the fully connected directed graph G = (V, E)

where the vertices V are the tokens of the advertisement (that are not predicted as

skips) and the dummy root symbol, E contains the edges representing the highest scor-

ing relation (e.g., part-of, segment, equivalent) with the respective cross entropy scores

serving as weights. Since G is a directed graph, s(xi, xj) is not necessarily equal to

s(xj, xi). Similar to McDonald et al. (2005), we employ Edmonds’ maximum span-

ning tree algorithm for directed graphs (Chu & Liu, 1965; Edmonds, 1967) to build a

non-projective parser. Indeed, in our setting, we have a signiﬁcant number (26% in

the dataset used for experiments, see further) of non-adjacent part-of and equivalent

relations (non-projective). It is worth noting that in the case of segment relations, the

words involved are not interleaved by other tokens and are always adjacent. We ap-

ply Edmonds’ algorithm to every graph which is constructed to get the highest scoring

graph structure, even in the cases where a tree is already formed by greedy inference.

For skips, we consider the predictions as obtained from the greedy approach and we do

not include them in the fully connected weighted graph, since Edmonds’ complexity is
O(n2) for dense graphs and might lead to slow decoding time.

In this section, we present the experimental results of our study. We describe the

dataset, the setup of the experiments and we compare the results of the methods anal-

5. Results and discussion

ysed in the previous sections.

5.1. Experimental setup

Our dataset consists of a large collection (i.e., 887,599) of Dutch property advertise-

ments from real estate agency websites. From this large dataset, a sub-collection of

2,318 classiﬁeds have been manually annotated by 3 trained human annotators (1 an-

notation per ad, 773 ads per annotator). The annotations follow the format of the

22

property tree that is described in detail in Section 3 and is illustrated in Fig. 1. The

dataset is available for research purposes, see our github codebase.1

In the experi-

ments, we use only the annotated text advertisements for the pipeline setting, i.e., LTM

(locally trained model), MTT (globally trained model). In the case of the neural net-

work approach, we train the embeddings on the large collection by using the word2vec

model (Mikolov et al., 2013) whereas in the joint learning, we use only the annotated

documents, similar to the pipeline approach. The code of the LTM and the MTT hand-

crafted systems is available on github.1 We also use our own CRF implementation. The

code for the joint model has been developed in Python with the Tensorﬂow machine

learning library (Abadi et al., 2016) and will be made public as well. For the evalu-

ation, we use 70% for training, 15% for validation and 15% as test set. We measure

the performance by computing the F1 score on the test set. The accuracy metric can

be misleading in our case since we have to deal with imbalanced data (the skip label

is over-represented). We only report numbers on the structured classes, i.e., segment

and part-of since the other dependencies (skip, equivalent) are auxiliary in the joint

models and do not directly contribute to the construction of the actual property tree.

For the overall F1, we are again only considering the structured classes. Finally, we

report the number of property trees (which shows how likely our model is to produce

trees without applying Edmonds’ algorithm, i.e., by greedy inference alone) for all the

models before applying Edmonds’ algorithm that guarantees the tree structure of the

predictions.

For the pipeline models, we train the CRF with regularization parameter λCRF =

10 and the LTM and MTT with C = 1 based on the best hyperparameters on the val-

idation set. As binary classiﬁer, we use logistic regression. For the joint model, we

train 128-dimensional word2vec embeddings on a collection of 887k advertisements.

In general, using larger embeddings dimensions (e.g., 300), does not affect the perfor-

mance of our models. We consistently used single-layer LSTMs through our experi-

ments to keep our model relatively simple and to evaluate the various attention meth-

ods on top of that. We have also reported results on the joint model using a two-layer

1https://github.com/bekou/ad_data

23

Precision

Recall

segment

part-of

segment

part-of

segment

Overall

Trees
(% of ads)

-
d
n
a
H

d
e
t
f
a
r
c

M
T
S
L

e
v
i
t
n
e
t
t

A

M
T
S
L

Model

LTM
MTT

LSTM
LSTM+E
2xLSTM+E

Additive
Bilinear
Multiplicative
Biafﬁne
Tensor
Edge1
Edge2
Edge3

73.77
73.77

70.24
70.18
73.91

72.97
70.25
71.12
70.01
71.53
71.56
72.03
71.74

60.53
61.15

65.23
63.92
69.88

65.71
66.34
66.40
64.67
64.68
67.46
66.09
67.69

70.98
70.98

77.73
77.77
75.78

76.45
79.96
77.81
78.32
76.17
78.24
75.35
78.44

60.40
61.01

70.32
71.08
71.22

70.90
72.53
71.26
71.04
70.79
71.31
70.99
73.00

72.35
72.35

73.80
73.78
74.83

74.67
74.79
74.31
73.93
73.78
74.75
73.65
74.94

F1 (%)
part-of

60.47
61.08

67.68
67.31
70.54

68.21
69.29
68.75
67.71
67.60
69.33
68.46
70.25

64.76
65.15

68.82
68.57
70.90

69.46
70.20
69.70
68.75
68.68
70.08
69.12
70.70

37.18
43.23

68.30
68.30
78.09

74.35
72.62
72.91
74.06
69.16
70.32
73.48
78.96

Table 2: Performance of the three approaches on the structured prediction task. The top rows are for the
pipeline approach, i.e., hand-crafted features. The next block of results presents the results for the neural
joint model based on LSTMs. The bottom block contains results of the joint models augmented with several
attentive architectures. Edmonds’ algorithm is applied in all of the models to retain the tree structure, except
for the LSTM joint model. The LSTM+E is the LSTM model with Edmonds’ algorithm included. The
2xLSTM+E is the same joint model but it simply uses a stack of two LSTM layers. In the experiments with
attention, we use a one-stack LSTM. The rightmost column is the percentage of the ads that are valid trees
before applying Edmonds’ (i.e., step (3) of Fig. 4), showing the ability of the model to form trees during
greedy inference. In the Edgei models, the number i stands for the number of times that we have run the
message passing phase.

stacked LSTM joint model, although it needs a higher computation time compared to

a single-layer LSTM with an attention layer on top. The hidden size of the LSTMs is

d = 128 and the size of the neural network used in the scoring and the attention layer is

ﬁxed to l = 32. The optimization algorithm used is Adam (Kingma & Ba, 2015) with
a learning rate of 10−3. To reduce the effect of overﬁtting, we regularize our model

using the dropout method (Srivastava et al., 2014). We ﬁx the dropout rate on the in-

put of the LSTM layer to 0.5 to obtain signiﬁcant improvements (∼1%-2% F1 score

increase, depending on the model). For the two-layer LSTM, we ﬁx the dropout rate

to 0.3 in each of the input layers since this leads to largest performance increase on the

validation set. We have also explored gradient clipping without further improvement

on our results. In the joint model setting, we follow the evaluation strategy of early

stopping (Caruana et al., 2000; Graves et al., 2013) based on the performance of the

validation set. In most of the experiments, we obtain the best hyperparameters after

∼60 epochs.

24

5.2. Comparison of the pipeline and the joint model

One of the main contributions of our study is the comparison of the pipeline approach

and the proposed joint model. We formulated the problem of identifying the enti-

ties (i.e., segments) and predicting the dependencies between them (i.e., construc-

tion of the property tree) as a joint model. Our neural model, unlike recent stud-

ies (Miwa & Bansal, 2016; Zheng et al., 2017) on joint models that use LSTMs to

handle similar tasks, does not need two components to model the problem (i.e., NER

and dependency parsing). To the best of our knowledge, our study is the ﬁrst that for-

mulates the task in an actual joint setting without the need to pre-train the sequence

labeling component or for parameter sharing between them, since we use only one

component for both subtasks. In Table 2, we present the results of the pipeline model

(hand-crafted) and the proposed joint model (LSTM). The improvement of the joint

model over the pipeline is unambiguous, i.e., 3.42% overall F1 score difference be-

tween MTT (highest scoring pipeline model) and LSTM+E (LSTM model with Ed-

monds’ algorithm). An additional increase of ∼2.3% is achieved when we consider

two-layer LSTMs (2xLSTM+E) for our joint model. All results in Table 2, except for

the LSTM, are presented using Edmonds’ algorithm on top, to construct the property

tree. Examining each label separately, we observe that the original LSTM+E model

(73.78%) performs better by 1.43% in entity segmentation than the CRF (72.35%).

The LSTM model achieves better performance in the entity recognition task since it

has to learn the two subtasks simultaneously resulting in interactions between the com-

ponents (i.e., NER and dependency parser). This way, the decisions for the entity

recognition can beneﬁt from predictions that are made for the part-of relations. Con-

cerning the part-of dependencies, we note that the LSTMs outperform the hand-crafted

approaches by 6.23%. Also, the number of valid trees that are constructed before ap-

plying Edmonds’ algorithm is almost twice as high for the LSTM models. Stacking

two-layer LSTMs results in an additional ∼1% improvement in the segmentation task

and ∼3% in the part-of relations. The greedy inference for the hand-crafted meth-

ods does not produce well-formed trees, meaning that post-processing with Edmonds’

algorithm (enforce tree structure) is expected to increase the performance of the hand-

crafted models compared to the LSTM model performance. Indeed, the performance

25

of the feature based hand-crafted models (i.e., LTM and MTT) without the Edmonds’

on top is not reported in Table 2 due to their poor performance in our task (i.e., ∼60%

overall F1 and ∼51% for part-of ), but after post-processing with Edmonds’ the perfor-

mance signiﬁcantly increases (i.e., ∼65%). On the other hand, applying the Edmonds’

algorithm on the LSTM model leads to marginally decreased performance (∼0.2%)

compared to the original LSTM model, probably indicating that enforcing structural

constraints is not beneﬁcial for a model that clearly has the ability to form valid tree

structures during greedy inference. Although one might be tempted not to enforce the

tree structure (post-process with Edmonds’), due to the nature of our problem, we have

to enforce tree constraints in all of the models.

5.3. Comparison of the joint and the attention model

After having established the superior performance of neural approach using LSTMs

over the more traditional (LTM and MTT) methods based on hand-crafted features, we

now discuss further improvements using attentive models. The attention mechanisms

are designed to encourage the joint model to focus on informative tokens. We exploited

several attention mechanisms as presented in Section 4.2.4. Table 2 shows the perfor-

mance of the various models. Overall, the attention models are performing better in

terms of overall F1 score compared to the original joint model with the Edmonds’ on

top. Although the performance of the Biafﬁne and the Tensor models is limited com-

pared to the improvement of the other attentive models, we focus on: (i) the Biafﬁne

model since it achieved state-of-the-art performance on the dependency parsing task

and (ii) the Tensor model because we were expecting that it would perform similarly

to the Bilinear model (it has a bilinear tensor layer). Despite its simplicity, the Bilinear

model is the second best performing attentive model in Table 2 in terms of overall F1

score. Edge3 (70.70% overall F1 score) achieves better results than the other attention

mechanisms in the entity recognition and in the dependency parsing tasks. We observe

that running the message passing step multiple times in the Edge model, gives an in-

creasing trend in the number of valid trees that were constructed before applying the

maximum spanning tree algorithm. This is not surprising since we expect that running

the message passing phase multiple times leads to improved edge representations. The

26

maximum number of trees without post-processing by Edmonds’ is attained when we

run the message passing for 3 times whereas further increasing the number beyond 3

(e.g., 4) appears no longer beneﬁcial. Stacking a second LSTM layer on top of the joint

model (2xLSTM+E) marginally improves the performance by 0.2% compared to the

Edge3 attention model. But adding a second LSTM layer comes with the additional

cost of an increased computation time compared to the joint models with the attention

layers on top. This illustrates that: (i) there might be some room for marginally im-

proving the attention models even further, and (ii) we do not have to worry about the

quadratic nature of our approach since in terms of speed the attentive models are able

to surpass the two-layer LSTMs. The sequential processing of the LSTMs might be

the reason that slows down the computation time for the 2xLSTM over the rest of the

attentive models. Speciﬁcally, on an Intel(R) Xeon(R) CPU E5-2650 v2 @ 2.60GHz

processor, the best performing model (i.e., Edge3) takes ∼2 minutes per epoch while

in the 2xLSTM case, it takes ∼2.5 minutes leading to a slowdown of ∼25%. The

percentage of the ads that are valid trees is 1% better in the Edge3 over the two-layer

LSTM showcasing the ability of the Edge model to form more valid trees during greedy

inference.

5.4. Discussion

In this section, we discuss some additional aspects of our problem and the approaches

that we follow to handle them. As we mentioned before, a single entity can be present

in the text with multiple mentions. This brings an extra difﬁculty to our task. For

instance, in the example of Fig. 1, the entity “large apartment” is expressed in the ad

with the mentions “large apartment” and “home”. Hence it is confusing to which men-

tion the other entities should be attached to. One way would be to attach them to both

and then eliminate one of the connections using Edmonds’ spanning tree algorithm,

which is the approach adopted in Bekoulis et al. (2017). The problematic issue with

this approach is that the spanning tree algorithm would randomly remove all mentions

but one, possibly resulting in uncertain outcomes. To avoid this problem, we now use

as the main mention for an entity the ﬁrst mention in order of appearance in the text

(e.g., “large apartment” in our example) and the remaining mentions (e.g., “home”)

27

are attached as equivalent mentions to the main one. Usually, the most informative

mention for an entity is the one that appears ﬁrst, because we again refer to an entity

mentioned before, often with a shorter description. Following our intuition, the neural

model increases its overall performance by ∼3% (from 66% to 69% and more than 5%

in the part-of relation) and the pipeline approaches by almost 4% (from 61%, reported

in Bekoulis et al. (2017) to 65% and more than 5% in the part-of relation).

We also experimented with introducing the equivalent relations. Although it is

a strongly under-represented class in the dataset and the model performs poorly for

this label (an equivalent edge F1 score of 10%), introducing the equivalent label is

the natural way of modeling our problem (i.e., assigning each additional mention as

equivalent to the main mention). We ﬁnd out that introducing this type of relation

leads to a slight decrease (∼1%) in the part-of and a marginal increase (∼0.3%) in

the segment relations which are the main relations while retaining the nature of our

problem. In the pipeline approach, it results in an 9% drop in the F1 score of the part-

of relation. This is the reason that the results as presented in Table 2 do not consider

the equivalent relation for the hand-crafted model to make a fair comparison in the

structured classes.

We believe our experimental comparison of the various architectural model vari-

ations provides useful ﬁndings for practitioners. Speciﬁcally, for applications requir-

ing both segmentation (entity recognition) and dependency parsing (structured predic-

tion), our ﬁndings can be qualitatively summarized as follows: (i) joint modeling is

the most appropriate approach since it reduces error propagation between the com-

ponents, (ii) the LSTM model is much more effective (than models relying on hand-

crafted features) because it automatically extracts informative features from the raw

text, (iii) attentive models are proven effective because they encourage the model to fo-

cus on salient tokens, (iv) the edge attention model leads to an improved performance

since it better encodes the information ﬂow between the entities by using graph rep-

resentations, and (v) stacking a second LSTM marginally increases the performance,

suggesting that there might be some room for slight improvement of the attention mod-

els by adding LSTM layers.

Finally, we point out how exactly our model relates to state-of-the-art in the ﬁeld.

28

Our joint model is able to both extract entity mentions (i.e., perform segmentation)

and do dependency parsing, which we demonstrate on the real estate problem. Previ-

ous studies (Kate & Mooney, 2010; Li & Ji, 2014; Miwa & Sasaki, 2014) that jointly

considered the two subtasks (i.e., segmentation and relation extraction): (i) require

manual feature engineering and (ii) generalize poorly between various applications.

On the other hand, in our work, we rely on neural network methods (i.e., LSTMs) to

automatically extract features from the real estate textual descriptions and perform the

two tasks jointly. Although there are other methods which use neural network archi-

tectures (Miwa & Bansal, 2016; Zheng et al., 2017; Li et al., 2017) that focus on the

relation extraction problem, our work is different in that we aim to model directed

spanning trees and thus to solve the dependency parsing problem which is more con-

strained and difﬁcult (than extracting single instances of binary relations). Moreover,

the cited methods require either parameter sharing or pre-training of the segmenta-

tion module, which complicates learning. Therefore, cited methods are not directly

comparable to our model and cannot be applied to our real estate task out-of-the-box.

However, our model’s main limitation is the quadratic scoring layer that increases the

time complexity of the segmentation task from linear (which is the complexity of a
conditional random ﬁeld, CRF) to O(n2). As a result, it sacriﬁces standard linear com-

plexity of the segmentation task, in order to reduce the error propagation between the

subtasks and thus perform learning in a joint, end-to-end differentiable, setting.

6. Conclusions

In this paper, we proposed an LSTM-based neural model to jointly perform segmenta-

tion and dependency parsing. We apply it to a real estate use case processing textual

ads, thus (1) identifying important entities of the property (e.g., rooms) and (2) struc-

turing them into a tree format based on the natural language description of the prop-

erty. We compared our model with the traditional pipeline approaches that have been

adapted to our task and we reported an improvement of 3.4% overall edge F1 score.

Moreover, we experimented with different attentive architectures and stacking of a sec-

ond LSTM layer over our basic joint model. The results indicate that exploiting atten-

29

tion mechanisms that encourage our model to focus on informative tokens, improves

the model performance (increase of overall edge F1 score with ∼2.1%) and increases

the ability to form valid trees in the prediction phase (4% to 10% more valid trees for

the two best scoring attention mechanisms) before applying the maximum spanning

tree algorithm.

The contribution of this study to the research in expert and intelligent systems is

three-fold: (i) we introduce a generic joint model, simultaneously solving both sub-

tasks of segmentation (i.e., entity extraction) and dependency parsing (i.e., extracting

relationships among entities), that unlike previous work in the ﬁeld does not rely on

manually engineered features, (ii) in particular for the real estate domain, extracting

a structured property tree from a textual ad, we reﬁne the annotations and addition-

ally propose attention models, compared to initial work on this application, and ﬁ-

nally (iii) we demonstrate the effectiveness of our proposed generic joint model with

extensive experiments (see aforementioned F1 improvement of 2.1%). Despite the

experimental focus on the real estate domain, we stress that the model is generic in na-

ture, and could be equally applied to other expert system scenarios requiring the general

tasks of both detecting entities (segmentation) and establishing relations among them

(dependency parsing). We furthermore note that our model, rather than focusing on

extracting a single binary relation from a sentence (as in traditional relation extraction

settings), produces a complete tree structure.

Future work can evaluate the value of our joint model we introduced in other spe-

ciﬁc application domains (e.g., biology, medicine, news) for expert and intelligent sys-

tems. For example, the method can be evaluated for entity recognition and binary

relation extraction (the ACE 04 and ACE 05 datasets; see Miwa & Bansal (2016)) or in

adverse drug effects from biomedical texts (see Li et al. (2016)). In terms of model ex-

tensions and improvements, one research issue is to address the time complexity of the

NER part by modifying the quadratic scoring layer for this component. An additional

research direction is to investigate different loss functions for the NER component (e.g.,

adopting a conditional random ﬁeld (CRF) approach), since this has been proven ef-

fective in the NER task on its own (Lample et al., 2016). A ﬁnal extension we envision

is to enable multi-label classiﬁcation of relations among entity pairs.

30

The presented research was partly performed within the MALIBU project, funded by

Flanders Innovation & Entrepreneurship (VLAIO) contract number IWT 150630.

Acknowledgments

References

Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat,

S., Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga, R., Moore, S., Murray,

D. G., Steiner, B., Tucker, P., Vasudevan, V., Warden, P., Wicke, M., Yu, Y., & Zheng,

X. (2016). Tensorﬂow: A system for large-scale machine learning. In Proceedings

of the 12th USENIX Conference on Operating Systems Design and Implementation

(pp. 265–283). Berkeley, CA, USA.

Andor, D., Alberti, C., Weiss, D., Severyn, A., Presta, A., Ganchev, K., Petrov, S.,

& Collins, M. (2016). Globally normalized transition-based neural networks.

In

Proceedings of the 54th Annual Meeting of the Association for Computational Lin-

guistics (Volume 1: Long Papers) (pp. 2442–2452). Berlin, Germany.

Atkinson, J., & Bull, V. (2012). A multi-strategy approach to biological named

entity recognition. Expert Systems with Applications, 39(17), 12968 – 12974.

doi:10.1016/j.eswa.2012.05.033.

Bach, N., & Badaskar, S. (2007). A review of relation extraction. Literature review for

Language and Statistics II, .

Bekoulis, G., Deleu, J., Demeester, T., & Develder, C. (2017). Reconstructing the

house from the ad: Structured prediction on real estate classiﬁeds. In Proceedings of

the 15th Conference of the European Chapter of the Association for Computational

Linguistics: (Volume 2, Short Papers) (pp. 274–279). Valencia, Spain.

Bengio, Y., Simard, P., & Frasconi, P. (1994). Learning long-term dependencies

with gradient descent is difﬁcult. Transactions on neural networks, 5(2), 157–166.

doi:10.1109/72.279181.

31

Bohnet, B., & Nivre, J. (2012). A transition-based system for joint part-of-speech tag-

ging and labeled non-projective dependency parsing.

In Proceedings of the 2012

Joint Conference on Empirical Methods in Natural Language Processing and Com-

putational Natural Language Learning (pp. 1455–1465). Jeju Island, Korea: Asso-

ciation for Computational Linguistics.

Carreras, X. (2007). Experiments with a higher-order projective dependency parser.

In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Lan-

guage Processing and Computational Natural Language Learning (pp. 957–961).

Prague, Czech: Association for Computational Linguistics.

Caruana, R., Lawrence, S., & Giles, L. (2000). Overﬁtting in neural nets: Backpropa-

gation, conjugate gradient, and early stopping. In Proceedings of the 13th Interna-

tional Conference on Neural Information Processing Systems (pp. 381–387). Den-

ver, USA: MIT Press.

Chen, D., & Manning, C. (2014). A fast and accurate dependency parser using neural

networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural

Language Processing (pp. 740–750). Doha, Qatar: Association for Computational

Linguistics.

Chiu, J., & Nichols, E. (2016). Named entity recognition with bidirectional LSTM-

CNNs. Transactions of the Association for Computational Linguistics, 4, 357–370.

Chu, Y.-J., & Liu, T.-H. (1965). On shortest arborescence of a directed graph. Scientia

Sinica, 14, 1396–1400.

Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).

Natural language processing (almost) from scratch. Journal of Machine Learning

Research, 12, 2493–2537.

Daum´e

III, H., Langford,

J., & Marcu, D.

(2009).

Search-based

structured

prediction.

Machine Learning

Journal,

75(3),

297–325.

doi:10.1007/s10994-009-5106-x.

32

Dozat, T., & Manning, C. D. (2017). Deep biafﬁne attention for neural dependency

parsing. In Proceedings of the International Conference for Learning Representa-

tions (pp. 1–8). Toulon, France.

Dyer, C., Ballesteros, M., Ling, W., Matthews, A., & Smith, N. A. (2015). Transition-

based dependency parsing with stack long short-term memory. In Proceedings of the

53rd Annual Meeting of the Association for Computational Linguistics and the 7th

International Joint Conference on Natural Language Processing (Volume 1: Long

Papers) (pp. 334–343). Beijing, China.

Edmonds, J. (1967). Optimum branchings. Journal of research of the National Bureau

of Standards, 71B(4), 233–240.

Eisner, J. M. (1996). Three new probabilistic models for dependency parsing: An

exploration. In Proceedings of the 16th International Conference on Computational

Linguistics (Volume 1) (pp. 340–345). Copenhagen, Denmark.

Fundel, K., Kffner, R., & Zimmer, R.

(2007).

Relex-relation extrac-

tion using dependency parse

trees.

Bioinformatics,

23(3),

365–371.

doi:10.1093/bioinformatics/btl616.

Gillick, D., Brunk, C., Vinyals, O., & Subramanya, A. (2016). Multilingual language

processing from bytes. In Proceedings of the 2016 Conference of the North Amer-

ican Chapter of the Association for Computational Linguistics: Human Language

Technologies (pp. 1296–1306). San Diego, California.

Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., & Dahl, G. E. (2017). Neural

message passing for quantum chemistry. In Proceedings of the 34th International

Conference on Machine Learning (pp. 1263–1272). Sydney, Australia: PMLR.

Goldberg, Y., & Hirst, G. (2017). Neural Network Methods in Natural Language Pro-

cessing. Morgan & Claypool Publishers.

Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.

http://www.deeplearningbook.org.

33

Graves, A., r. Mohamed, A., & Hinton, G. (2013). Speech recognition with deep

recurrent neural networks.

In Proceedings of the International Conference on

Acoustics, Speech and Signal Processing (pp. 6645–6649). Vancouver, Canada.

doi:10.1109/ICASSP.2013.6638947.

Gurulingappa, H., MateenRajpu, A., & Toldo, L. (2012). Extraction of potential ad-

verse drug events from medical case reports. Journal of Biomedical Semantics, 3(1),

1–15. doi:10.1186/2041-1480-3-15.

Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computa-

tion, 9(8), 1735–1780. doi:10.1162/neco.1997.9.8.1735.

Huang, Z., Xu, W., & Yu, K. (2015). Bidirectional LSTM-CRF models for sequence

tagging. arXiv preprint arXiv:1508.01991, .

Jaeger, H. (2010). The echo state approach to analysing and training recurrent neu-

ral networks-with an erratum note’. Bonn, Germany: German National Research

Center for Information Technology GMD Technical Report, 148(34), 13.

Jung, J. J. (2012). Online named entity recognition method for microtexts in social

networking services: A case study of twitter. Expert Systems with Applications,

39(9), 8066 – 8070. doi:10.1016/j.eswa.2012.01.136.

Kate, R. J., & Mooney, R. (2010). Joint entity and relation extraction using card-

pyramid parsing. In Proceedings of the 14th Conference on Computational Natural

Language Learning (pp. 203–212). Uppsala, Sweden: Association for Computa-

tional Linguistics.

Kingma, D., & Ba, J. (2015). Adam: A method for stochastic optimization. In Inter-

national Conference on Learning Representations. San Diego, USA.

Kiperwasser, E., & Goldberg, Y. (2016). Simple and accurate dependency parsing

using bidirectional lstm feature representations. Transactions of the Association for

Computational Linguistics, 4, 313–327.

34

Konkol, M., Brychcn, T., & Konopk, M. (2015).

Latent semantics in named

entity recognition.

Expert Systems with Applications, 42(7), 3470 – 3479.

doi:10.1016/j.eswa.2014.12.015.

Koo, T., Globerson, A., Carreras, X., & Collins, M. (2007). Structured prediction

models via the Matrix-Tree Theorem. In Proceedings of the 2007 Joint Conference

on Empirical Methods in Natural Language Processing and Computational Natural

Language Learning (pp. 141–150). Prague, Czech: Association for Computational

Linguistics.

K¨uc¸ ¨uk, D., & Yazıcı, A.

(2012).

A hybrid named entity recognizer

for

turkish.

Expert Systems with Applications, 39(3), 2733 – 2742.

doi:10.1016/j.eswa.2011.08.131.

Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional random ﬁelds: Proba-

bilistic models for segmenting and labeling sequence data. In Proceedings of the

18th International Conference on Machine Learning (pp. 282–289). San Francisco,

USA: Morgan Kaufmann.

Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., & Dyer, C. (2016).

Neural architectures for named entity recognition. In Proceedings of the 2016 Con-

ference of the North American Chapter of the Association for Computational Lin-

guistics: Human Language Technologies (pp. 260–270). San Diego, California.

LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., &

Jackel, L. D. (1989). Backpropagation applied to handwritten zip code recognition.

Neural Computation, 1(4), 541–551. doi:10.1162/neco.1989.1.4.541.

Li, F., Zhang, M., Fu, G., & Ji, D. (2017). A neural joint model for entity

and relation extraction from biomedical text. BMC Bioinformatics, 18(1), 1–11.

doi:10.1186/s12859-017-1609-9.

Li, F., Zhang, Y., Zhang, M., & Ji, D. (2016). Joint models for extracting adverse

drug events from biomedical text. In Proceedings of the Twenty-Fifth International

35

Joint Conference on Artiﬁcial Intelligence (pp. 2838–2844). New York, USA: IJ-

CAI/AAAI Press.

Li, Q., & Ji, H. (2014). Incremental joint extraction of entity mentions and relations.

In Proceedings of the 52nd Annual Meeting of the Association for Computational

Linguistics (Volume 1: Long Papers) (pp. 402–412). Baltimore, USA.

Luong, T., Pham, H., & Manning, C. D. (2015). Effective approaches to attention-

based neural machine translation. In Proceedings of the 2015 Conference on Empir-

ical Methods in Natural Language Processing (pp. 1412–1421). Lisbon, Portugal:

Association for Computational Linguistics.

Ma, X., & Hovy, E. (2016). End-to-end sequence labeling via bi-directional LSTM-

CNNs-CRF. In Proceedings of the 54th Annual Meeting of the Association for Com-

putational Linguistics (Volume 1: Long Papers) (pp. 1064–1074). Berlin, Germany.

McDonald, R., & Pereira, F. (2007). Online learning of approximate dependency pars-

ing algorithms. In Proceedings of the 11th Conference of the European Chapter of

the Association for Computational Linguistics (pp. 81–88). Trento, Italy.

McDonald, R., Pereira, F., Ribarov, K., & Hajic, J. (2005). Non-projective depen-

dency parsing using spanning tree algorithms. In Proceedings of Human Language

Technology Conference and Conference on Empirical Methods in Natural Language

Processing (pp. 523–530). Vancouver, British Columbia, Canada: Association for

Computational Linguistics.

Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed

representations of words and phrases and their compositionality. In Proceedings of

the 26th International Conference on Neural Information Processing Systems (pp.

3111–3119). Nevada, United States: Curran Associates, Inc.

Miwa, M., & Bansal, M. (2016). End-to-end relation extraction using LSTMs on se-

quences and tree structures. In Proceedings of the 54th Annual Meeting of the As-

sociation for Computational Linguistics (Volume 1: Long Papers) (pp. 1105–1116).

Berlin, Germany.

36

Miwa, M., & Sasaki, Y. (2014). Modeling joint entity and relation extraction with

table representation. In Proceedings of the 2014 Conference on Empirical Methods

in Natural Language Processing (pp. 1858–1869). Doha, Qatar: Association for

Computational Linguistics.

Nadeau, D., & Sekine, S. (2007). A survey of named entity recognition and classiﬁca-

tion. Lingvisticae Investigationes, 30(1), 3–26. doi:10.1075/li.30.1.03nad.

Nagaraja, C. H., Brown, L. D., & Zhao, L. H. (2011). An autoregressive ap-

proach to house price modeling. The Annals of Applied Statistics, 5(1), 124–149.

doi:10.1214/10-AOAS380.

Nguyen, N., & Guo, Y. (2007). Comparisons of sequence labeling algorithms and ex-

tensions. In Proceedings of the 24th International Conference on Machine Learning

(pp. 681–688). Corvallis, USA: ACM. doi:10.1145/1273496.1273582.

Nivre, J. (2003). An efﬁcient algorithm for projective dependency parsing. In Pro-

ceedings of the 8th International Workshop on Parsing Technologies (pp. 149–160).

Nancy, France.

Nivre, J. (2009). Non-projective dependency parsing in expected linear time. In Pro-

ceedings of the Joint Conference of the 47th Annual Meeting of the Association for

Computational Linguistics and the 4th International Joint Conference on Natural

Language Processing of the Asian Federation of Natural Language Processing (pp.

351–359). Singapore.

Nivre, J., Hall, J., Nilsson, J., Eryiˇgit, G., & Marinov, S. (2006). Labeled pseudo-

projective dependency parsing with support vector machines. In Proceedings of the

10th Conference on Computational Natural Language Learning (pp. 221–225). New

York, USA: Association for Computational Linguistics.

Pace, K., Barry, R., Gilley, O. W., & Sirmans, C. (2000). A method for spatialtem-

poral forecasting with an application to real estate prices. International Journal of

Forecasting, 16(2), 229 – 246. doi:10.1016/S0169-2070(99)00047-3.

37

Pascanu, R., Mikolov, T., & Bengio, Y. (2013). On the difﬁculty of training recurrent

neural networks. In Proceedings of the 30th International Conference on Interna-

tional Conference on Machine Learning (pp. 1310–1318). Atlanta, USA: JMLR.org.

Peng, F., & McCallum, A. (2006). Information extraction from research papers using

conditional random ﬁelds. Information processing & management, 42(4), 963–979.

doi:10.1016/j.ipm.2005.09.002?

Rabiner, L., & Juang, B. (1986). An introduction to hidden markov models. IEEE

ASSP Magazine, 3(1), 4–16. doi:10.1109/MASSP.1986.1165342.

Ratinov, L., & Roth, D. (2009). Design challenges and misconceptions in named en-

tity recognition. In Proceedings of the 13th Conference on Computational Natural

Language Learning (pp. 147–155). Boulder, USA: Association for Computational

Linguistics.

China.

dos Santos, C., Xiang, B., & Zhou, B. (2015). Classifying relations by ranking with

convolutional neural networks. In Proceedings of the 53rd Annual Meeting of the As-

sociation for Computational Linguistics and the 7th International Joint Conference

on Natural Language Processing (Volume 1: Long Papers) (pp. 626–634). Beijing,

Socher, R., Chen, D., Manning, C. D., & Ng, A. (2013). Reasoning with neural

tensor networks for knowledge base completion.

In Proceedings of the 26th In-

ternational Conference on Neural Information Processing Systems (pp. 926–934).

Nevada, United States: Curran Associates, Inc.

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014).

Dropout: A simple way to prevent neural networks from overﬁtting. Journal of

Machine Learning Research, 15(1), 1929–1958.

Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with

neural networks.

In Proceedings of the 27th International Conference on Neural

Information Processing Systems (pp. 3104–3112). Montreal, Canada: MIT Press.

38

Taskar, B., Guestrin, C., & Koller, D. (2003). Max-margin markov networks.

In

Proceedings of the 16th International Conference on Neural Information Processing

Systems (pp. 25–32). Bangkok, Thailand: MIT Press.

Tsochantaridis, I., Hofmann, T., Joachims, T., & Altun, Y. (2004). Support vector

machine learning for interdependent and structured output spaces. In Proceedings

of the 21st International Conference on Machine Learning (pp. 104–112). Helsinki,

Finland: ACM. doi:10.1145/1015330.1015341.

Tutte, W. T. (2001). Graph theory. In Encyclopedia of Mathematics and its Applications

(p. 138). Cambridge University Press volume 21.

Vinyals, O., Fortunato, M., & Jaitly, N. (2015). Pointer networks. In Proceedings of

the 28th International Conference on Neural Information Processing Systems (pp.

2692–2700). Montreal, Canada: Curran Associates, Inc.

Wang, D., & Li, M. (2017).

Stochastic conﬁguration networks: Fundamen-

tals and algorithms.

IEEE Transactions on Cybernetics, 47(10), 3466–3479.

doi:10.1109/TCYB.2017.2734043.

Wang, W., & Chang, B. (2016). Graph-based dependency parsing with bidirectional

lstm. In Proceedings of the 54th Annual Meeting of the Association for Computa-

tional Linguistics (Volume 1: Long Papers) (pp. 2306–2315). Berlin, Germany.

Xu, Y., Mou, L., Li, G., Chen, Y., Peng, H., & Jin, Z. (2015). Classifying relations via

long short term memory networks along shortest dependency paths. In Proceedings

of the 2015 Conference on Empirical Methods in Natural Language Processing (pp.

1785–1794). Lisbon, Portugal: Association for Computational Linguistics.

Yamada, H., & Matsumoto, Y. (2003). Statistical dependency analysis with support

vector machines. In Proceedings of the 8th International Workshop on Parsing Tech-

nologies (pp. 195–206). Nancy, France.

Zhang, H., & McDonald, R. (2012). Generalized higher-order dependency parsing with

cube pruning. In Proceedings of the 2012 Joint Conference on Empirical Methods in

39

Natural Language Processing and Computational Natural Language Learning (pp.

320–331). Jeju Island, Korea: Association for Computational Linguistics.

Zhang, X., Cheng, J., & Lapata, M. (2017). Dependency parsing as head selection.

In Proceedings of the 15th Conference of the European Chapter of the Association

for Computational Linguistics: (Volume 1, Long Papers) (pp. 665–676). Valencia,

Spain.

Zheng, S., Hao, Y., Lu, D., Bao, H., Xu, J., Hao, H., & Xu, B. (2017). Joint entity and

relation extraction based on a hybrid neural network. Neurocomputing, 257, 59 –

66. doi:10.1016/j.neucom.2016.12.075.

40

8
1
0
2
 
r
a

M
 
9
1
 
 
]
L
C
.
s
c
[
 
 
2
v
0
9
5
9
0
.
9
0
7
1
:
v
i
X
r
a

An attentive neural architecture for joint segmentation and
parsing and its application to real estate ads

Giannis Bekoulis∗, Johannes Deleu, Thomas Demeester, Chris Develder

Ghent University – imec, IDLab, Department of Information Technology,
Technologiepark Zwijnaarde 15, 9052 Ghent, Belgium

Abstract

In processing human produced text using natural language processing (NLP) tech-

niques, two fundamental subtasks that arise are (i) segmentation of the plain text into

meaningful subunits (e.g., entities), and (ii) dependency parsing, to establish relations

between subunits. Such structural interpretation of text provides essential building

blocks for upstream expert system tasks: e.g., from interpreting textual real estate ads,

one may want to provide an accurate price estimate and/or provide selection ﬁlters for

end users looking for a particular property — which all could rely on knowing the

types and number of rooms, etc. In this paper we develop a relatively simple and ef-

fective neural joint model that performs both segmentation and dependency parsing

together, instead of one after the other as in most state-of-the-art works. We will fo-

cus in particular on the real estate ad setting, aiming to convert an ad to a structured

description, which we name property tree, comprising the tasks of (1) identifying im-

portant entities of a property (e.g., rooms) from classiﬁeds and (2) structuring them

into a tree format. In this work, we propose a new joint model that is able to tackle

the two tasks simultaneously and construct the property tree by (i) avoiding the error

propagation that would arise from the subtasks one after the other in a pipelined fash-

ion, and (ii) exploiting the interactions between the subtasks. For this purpose, we

perform an extensive comparative study of the pipeline methods and the new proposed

∗Corresponding author
Email addresses: giannis.bekoulis@ugent.be (Giannis Bekoulis),

johannes.deleu@ugent.be (Johannes Deleu), thomas.demeester@ugent.be (Thomas
Demeester), chris.develder@ugent.be (Chris Develder)

Preprint submitted to Expert Systems with Applications

March 20, 2018

joint model, reporting an improvement of over three percentage points in the overall

edge F1 score of the property tree. Also, we propose attention methods, to encourage

our model to focus on salient tokens during the construction of the property tree. Thus

we experimentally demonstrate the usefulness of attentive neural architectures for the

proposed joint model, showcasing a further improvement of two percentage points in

edge F1 score for our application. While the results demonstrated are for the particular

real estate setting, the model is generic in nature, and thus could be equally applied to

other expert system scenarios requiring the general tasks of both (i) detecting entities

(segmentation) and (ii) establishing relations among them (dependency parsing).

Keywords: neural networks, joint model, relation extraction, entity recognition,

dependency parsing

1. Introduction

Many consumer-oriented digital applications rely on input data provided by their tar-

get audience. For instance, real estate websites gather property descriptions for the

offered classiﬁeds, either from realtors or from individual sellers.

In such cases, it

is hard to strike the right balance between structured and unstructured information:

enforcing restrictions or structure upon the data format (i.e., predeﬁned form) may re-

duce the amount or diversity of the data, while unstructured data (i.e., raw text) may

require non-trivial (i.e., hard to automate) transformation to a more structured form to

be useful/practical for the intended application. In the real estate domain, textual ad-

vertisements are an extremely useful but highly unstructured way of representing real

estate properties. However, structured descriptions of the advertisements are very help-

ful, e.g., for real estate agencies to suggest the most appropriate sales/rentals for their

customers, while keeping human reading effort limited. For example, special search

ﬁlters, which are usually used by clients, cannot be directly applied to textual advertise-

ments. On the contrary, a structured representation of the property (e.g., a tree format

of the property) enables the simpliﬁcation of the unstructured textual information by

applying speciﬁc ﬁlters (e.g., based on the number of bedrooms, number of ﬂoors, or

the requirement of having a bathroom with a toilet on the ﬁrst ﬂoor), and it also ben-

2

eﬁts other related applications such as automated price prediction (Pace et al., 2000;

Nagaraja et al., 2011).

The new real estate structured prediction problem as deﬁned by Bekoulis et al.

(2017) has as main goal to construct the tree-like representation of the property (i.e.,

the property tree) based on its natural language description. This can be approached as

a relation extraction task by a pipeline of separate subtasks, comprising (i) named entity

recognition (NER) (Nadeau & Sekine, 2007) and (ii) relation extraction (Bach & Badaskar,

2007). Unlike previous studies (Li & Ji, 2014; Miwa & Bansal, 2016) on relation ex-

traction, in the work of Bekoulis et al. (2017), the relation extraction module is re-

placed by dependency parsing. Indeed, the relations that together deﬁne the structure

of the house should form a tree, where entities are part-of one another (e.g., a ﬂoor

is part-of a house, a room is part-of a ﬂoor). This property tree is structurally simi-

lar to a parse tree. Although the work of Bekoulis et al. (2017) is a step towards the

construction of the property tree, it follows a pipeline setting, which suffers from two

serious problems: (i) error propagation between the subtasks, i.e., NER and depen-

dency parsing, and (ii) cross-task dependencies are not taken into account, e.g., terms

indicating relations (includes, contains, etc.) between entities that can help the NER

module are neglected. Due to the unidirectional nature of stacking the two modules

(i.e., NER and dependency parsing) in the pipeline model, there is no information ﬂow-

ing from the dependency parsing to the NER subtask. This way, the parser is not able

to inﬂuence the predictions of the NER. Other studies on similar tasks (Li & Ji, 2014;

Kate & Mooney, 2010) have considered the two subtasks jointly. They simultaneously

extract entity mentions and relations between them usually by implementing a beam-

search on top of the ﬁrst module (i.e., NER), but these methods require the manual

extraction of hand-crafted features. Recently, deep learning with neural networks has

received much attention and several approaches (Miwa & Bansal, 2016; Zheng et al.,

2017) apply long short-term memory (LSTM) recurrent neural networks and convo-

lutional neural networks (CNNs) to achieve state-of-the-art performance on similar

problems. Those models rely on shared parameters between the NER and relation ex-

traction components, whereby the NER module is typically pre-trained separately, to

improve the training effectiveness of the joint model.

3

In this work, we propose a new joint model to solve the real estate structured pre-

diction problem. Our model is able to learn the structured prediction task without

complicated feature engineering. Whereas previous studies (Miwa & Bansal, 2016;

Zheng et al., 2017; Li et al., 2016, 2017) on joint methods focus on the relation extrac-

tion problem, we construct the property tree which comes down to solving a depen-

dency parsing problem, which is more constrained and hence more difﬁcult. Therefore,

previous methods are not directly comparable to our model and cannot be applied to

our real estate task out-of-the-box. In this work, we treat the two subtasks as one by

reformulating them into a head selection problem (Zhang et al., 2017).

This paper is a follow-up work of Bekoulis et al. (2017). Compared to the con-

ference paper that introduced the real estate extraction task and applied some basic

state-of-the-art techniques as a ﬁrst baseline solution, we now introduce: (i) advanced

neural models that consider the two subtasks jointly and (ii) modiﬁcations to the dataset

annotation representations as detailed below. More speciﬁcally, the main contributions

of this work are the following:

• We propose a new joint model that encodes the two tasks of identifying enti-

ties as well as dependencies between them, as a single head selection problem,

without the need of parameter sharing or pre-training of the ﬁrst entity recogni-

tion module separately. Moreover, instead of (i) predicting unlabeled dependen-

cies and (ii) training an additional classiﬁer to predict labels for the identiﬁed

heads (Zhang et al., 2017), our model already incorporates the dependency label

predictions in its scoring formula.

• We compare the proposed joint model against established pipeline approaches

and report an F1 improvement of 1.4% in the NER and 6.2% in the dependency

parsing subtask, corresponding to an overall edge F1 improvement of 3.4% in

the property tree.

• Compared to our original dataset (Bekoulis et al., 2017), we introduce two ex-

tensions to the data: (i) we consistently assign the ﬁrst mention of a particular

entity in order of appearance in the advertisement as the main mention of the

entity. This results in an F1 score increase of about 3% and 4% for the joint and

4

pipeline models, respectively. (ii) We add the equivalent relation to our anno-

tated dataset to explicitly express that several mentions across the ad may refer

to the same entity.

• We perform extensive analysis of several attention mechanisms that enable our

LSTM-based model to focus on informative words and phrases, reporting an

improved F1 performance of about 2.1%.

The rest of the paper is structured as follows. In Section 2, we review the related

work. Section 3 deﬁnes the problem and in Section 4, we describe the methodology

followed throughout the paper and the proposed joint model. The experimental results

are reported in Section 5. Finally, Section 6 concludes our work.

2. Related work

The real estate structured prediction problem from textual advertisements can be bro-

ken down into the sub-problems of (i) sequence labeling (identifying the core parts

of the property) and (ii) non-projective dependency parsing (connecting the identi-

ﬁed parts into a tree-like structure) (Bekoulis et al., 2017). One can address these two

steps either one by one in a pipelined approach, or simultaneously in a joint model.

The pipeline approach is the most commonly used approach (Bekoulis et al., 2017;

Fundel et al., 2007; Gurulingappa et al., 2012), treating the two steps independently

and propagating the output of the sequence labeling subtask (e.g., named entity recog-

nition) (Chiu & Nichols, 2016; Lample et al., 2016) to the relation classiﬁcation mod-

ule (dos Santos et al., 2015; Xu et al., 2015). Joint models are able to simultaneously

extract entity mentions and relations between them (Li & Ji, 2014; Miwa & Bansal,

2016). In this work, we propose a new joint model that is able to recover the tree-

like structure of the property and frame it as a dependency parsing problem, given the

non-projective tree structure we aim to output. We now present related works for the

sequence labeling and dependency parsing subtasks, as well as for the joint models.

5

2.1. Sequence labeling

Structured prediction problems become challenging due to the large output space.

Speciﬁcally in NLP, sequence labeling (e.g., NER) is the task of identifying the en-

tity mention boundaries and assigning a categorical label (e.g., POS tags) for each

identiﬁed entity in the sentence. A number of different methods have been proposed,

namely Hidden Markov Models (HMMs) (Rabiner & Juang, 1986), Conditional Ran-
dom Fields (CRFs) (Lafferty et al., 2001), Maximum Margin Markov Network (M3N)

(Taskar et al., 2003), generalized support vector machines for structured output (SVMstruct)

(Tsochantaridis et al., 2004) and Search-based Structured Prediction (SEARN) (Daum´e III et al.,

2009). Those methods heavily rely on hand-crafted features and an in-depth review can

be found in Nguyen & Guo (2007). Several variations of these models that also require

manual feature engineering have been used in different application settings (e.g., biol-

ogy, social media context) and languages (e.g., Turkish) (Jung, 2012; K¨uc¸ ¨uk & Yazıcı,

2012; Atkinson & Bull, 2012; Konkol et al., 2015). Recently, deep learning with neu-

ral networks has been succesfully applied to NER. Collobert et al. (2011) proposed

to use a convolutional neural network (CNN) followed by a CRF layer over a se-

quence of word embeddings. Recurrent Neural Networks (RNNs) constitute another

neural network architecture that has attracted attention, due to the state-of-the-art per-

formance in a series of NLP tasks (e.g., sequence-to-sequence (Sutskever et al., 2014),

parsing (Kiperwasser & Goldberg, 2016)). In this context, Gillick et al. (2016) use a

sequence-to-sequence approach for modeling the sequence labeling task. In addition,

several variants of combinations between LSTM and CRF models have been proposed

(Lample et al., 2016; Huang et al., 2015; Ma & Hovy, 2016) achieving state-of-the-art

performance on publicly available datasets.

2.2. Dependency parsing

Dependency parsing is a well studied task in the NLP community, which aims to ana-

lyze the grammatical structure of a sentence. We approach the problem of the property

tree construction as a dependency parsing task i.e., to learn the dependency arcs of

the classiﬁed. There are two well-established ways to address the dependency parsing

problem, via graph-based and transition-based parsers.

6

Graph-based: In the work of McDonald et al. (2005); McDonald & Pereira (2007)

dependency parsing requires the search of the highest scoring maximum spanning

tree in graphs for both projective (dependencies are not allowed to cross) and non-

projective (crossing dependencies are allowed) trees with the Eisner algorithm (Eisner,

1996) and the Chu-Liu-Edmonds algorithm (Chu & Liu, 1965; Edmonds, 1967) re-

spectively. It was shown that exploiting higher-order information (e.g., siblings, grand-

parental relation) in the graph, instead of just using ﬁrst-order information (i.e., par-

ent relations) (Carreras, 2007; Zhang & McDonald, 2012) may yield signiﬁcant im-

provements of the parsing accuracy but comes at the cost of an increased model com-

plexity. Koo et al. (2007) made an important step towards globally normalized mod-

els with hand-crafted features, by adapting the Matrix-Tree Theorem (MTT) (Tutte,

2001) to train over all non-projective dependency trees. We explore an MTT approach

as one of the pipeline baselines. Similar to recent advances in neural graph-based

parsing (Zhang et al., 2017; Kiperwasser & Goldberg, 2016; Wang & Chang, 2016),

we use LSTMs to capture richer contextual information compared to hand-crafted fea-

ture based methods. Our work is conceptually related to Zhang et al. (2017), who

formulated the dependency parsing problem as a head selection problem. We go a step

further in that direction, in formulating the joint parsing and labeling problem in terms

of selecting the most likely combination of head and label.

Transition-based: Transition-based parsers (Yamada & Matsumoto, 2003; Nivre et al.,

2006) replace the exact inference of the graph-based parsers by an approximate but

faster inference method. The dependency parsing problem is now solved by an ab-

stract state machine that gradually builds up the dependency tree token by token. The

goal of this kind of parsers is to ﬁnd the most probable transition sequence from an

initial to some terminal conﬁguration (i.e., a dependency parse tree, or in our case

a property tree) given a permissible set of actions (i.e., LEFT-ARC, RIGHT-ARC,

SHIFT) and they are able to handle both projective and non-projective dependen-

cies (Nivre, 2003, 2009). In the simplest case (i.e., greedy inference), a classiﬁer pre-

dicts the next transition based on the current conﬁguration. Compared to graph-based

dependency parsers, transition-based parsers are able to scale better due to the linear
time complexity while graph-based complexity rises to O(n2) in the non-projective

7

case. Chen & Manning (2014) proposed a way of learning a neural network classiﬁer

for use in a greedy, transition-based dependency parser while using low-dimensional,

dense word embeddings, without the need of manually extracting features. Globally

normalized transition-based parsers (Andor et al., 2016) can be considered an exten-

sion of Chen & Manning (2014), as they perform beam search for maintaining multiple

hypotheses and introduce global normalization with a CRF objective. Dyer et al. (2015)

introduced the stack-LSTM model with push and pop operations which is able to learn

the parser transition states while maintaining a summary embedding of its contents.

Although transition-based systems are well-known for their speed and state-of-the-art

performance, we do not include them in our study due to their already reported poor

performance in the real estate task (Bekoulis et al., 2017) compared to graph-based

parsers.

2.3. Joint learning

Adopting a pipeline strategy for the considered type of problems has two main draw-

backs: (i) sequence labeling errors propagate to the dependency parsing step, e.g., an

incorrectly identiﬁed part of the house (entity) could get connected to a truly existing

entity, and (ii) interactions between the components are not taken into account (feed-

back between the subtasks), e.g., modeling the relation between two potential entities

may help in deciding on the nature of the entities themselves. In more general rela-

tion extraction settings, a substantial amount of work (Kate & Mooney, 2010; Li & Ji,

2014; Miwa & Sasaki, 2014) jointly considered the two subtasks of entity recognition

and relation extraction. However, all of these models make use of hand-crafted features

that: (i) require manual feature engineering, (ii) generalize poorly between various ap-

plications and (iii) may require a substantial computational cost.

Recent advances on joint models for general relation extraction consider the joint

task using neural network architectures like LSTMs and CNNs (Miwa & Bansal, 2016;

Zheng et al., 2017; Li et al., 2017). Our work is however different from a typical rela-

tion extraction setup in that we aim to model directed spanning trees, or, equivalently,

non-projective dependency structures. In particular, the entities involved in a relation

are not necessarily adjacent in the text since other entities may be mentioned in be-

8

Entity type

Description

Examples

property
ﬂoor
space
subspace
ﬁeld

extra building

The property.
A ﬂoor in a building.
A room within the building.
A part of a room.
An open space inside or outside
the building.
An additional building which is
also part of the property.

bungalow, apartment
ground ﬂoor
bedroom, bathroom
shower, toilet
bbq, garden

garden house

Table 1: Real estate entity types.

tween, which complicates parsing. Indeed, in this work we focus on dependency pars-

ing due to the difﬁculty of establishing the tree-like structure instead of only relation

extraction (where each entity can have arbitrary relation arcs, regardless of other enti-

ties and their relations), which is the case for previously cited joint models. Moreover,

unlike most of these works that frame the problem as a stacking of the two components,

or at least ﬁrst train the NER module to recognize the entities and then further train to-

gether with the relation classiﬁcation module, we include the NER directly inside the

dependency parsing component.

In summary, the conceptual strengths of our joint segmentation and dependency

parsing approach (described in detail in Section 4) will be the following: compared to

state-of-the-art joint models in relation extraction, it (i) is generic in nature, without

requiring any manual feature engineering, (ii) extracts a complete tree structure rather

than a single binary relation instance.

3. Problem deﬁnition

In this section, we deﬁne the speciﬁc terms that are used in our real estate structured

prediction problem. We deﬁne an entity as an unambiguous, unique part of a property

with independent existence (e.g., bedroom, kitchen, attic). An entity mention is de-

ﬁned as one or more sequential tokens (e.g., “large apartment”) that can be potentially

linked to one or more entities. An entity mention has a unique semantic meaning and

refers to a speciﬁc entity, or a set of similar entities (e.g., “six bedrooms”). An entity

9

O r i g i n a l ad :
The p r o p e r t y i n c l u d e s a l a r g e
a p a r t m e n t w i t h a g a r a g e . The
home h a s a l i v i n g room , 3 s p a c i o u s b ed r o o m s and a b a t h r o o m .
The g a r a g e
e q u i p p e d w i t h a g a t e and a b i k e w a l l b r a c k e t .
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
S t r u c t u r e d r e p r e s e n t a t i o n :
p r o p e r t y
l a r g e

a p a r t m e n t

i s

l i v i n g room
3 s p a c i o u s b ed r o o m s
b a t h r o o m

g a r a g e

g a t e
b i k e w a l l b r a c k e t

‘ home ’

a p a r t m e n t ’ ,

| m e n t i o n = ‘ p r o p e r t y ’
| m e n t i o n = ‘ l a r g e
| m e n t i o n = ‘ l i v i n g room ’
| m e n t i o n = ‘3 s p a c i o u s bedrooms ’
| m e n t i o n = ‘ b ath r o o m ’
| m e n t i o n = ‘ g a r a g e ’
| m e n t i o n = ‘ g a t e ’
| m e n t i o n = ‘ b i k e w a l l b r a c k e t ’

Figure 1: Fictitious sample unstructured ad and corresponding structured representation as a property tree.

itself is part-of another entity and can be mentioned in the text more than once with

different entity mentions. For instance, a “house” entity could occur in the text with

entity mentions “large villa” and “a newly built house”. For the pipeline setting as pre-

sented in Bekoulis et al. (2017), we further classify entities into types (assign a named

entity type to every word in the ad). The task is transformed to a sequence labeling

problem using BIO (Beginning, Inside, Outside) encoding. The entity types are listed

in Table 1. For instance, in the sequence of tokens “large apartment”, B-PROPERTY is

assigned to the token “large” as the beginning of the entity, I-PROPERTY in the token

“apartment” as the inside of the entity but not the ﬁrst token within the entity and O

for all the other tokens that are not entities. Unlike previous studies (Miwa & Bansal,

2016; Zheng et al., 2017; Li et al., 2016, 2017), for our joint model there is no need for

this type of categorical classiﬁcation into labels since the two components are treated

uniﬁed as a single dependency parsing problem.

The goal of the real estate structured prediction task is to map the textual prop-

erty classiﬁed into a tree-like structured representation, the so-called property tree, as

illustrated in Fig. 1. In the pipeline setting, this conversion implies the detection of

(i) entities of various types and (ii) the part-of dependencies between them. For in-

stance, the entity “living room” is part-of the entity “large apartment”. In the joint

model, each token (e.g., “apartment”, “living”, “bathroom”, “includes”, “with”, “3”) is

examined separately and 4 different types of relations are deﬁned, namely part-of, seg-

10

ment, skip and equivalent. The part-of relation is similar to the way that it was deﬁned

in the pipeline setting but instead of examining entities, i.e., sequences of tokens (e.g.,

“living room”), we examine if a (individual) token is part-of another (individual) token

(e.g., “room” is part-of the “apartment”). We encode the entity identiﬁcation task with

the segment label and we follow the same approach as in the part-of relationships for

the joint model. Speciﬁcally, we examine if a token is a segment of another token (e.g.,

the token “room” is attached as a segment to the token “living”, “3” is attached as a seg-

ment to the token “bedrooms” and “spacious” is also attached as a segment to the token

“bedrooms” — this way we are able to encode the segment “3 spacious bedrooms”). By

doing so, we cast the sequence labeling subtask to a dependency parsing problem. The

tokens that are referring to the same entity belong to the equivalent relation (“home”

is equivalent to “apartment”). For each entity, we deﬁne the ﬁrst mention in order of

appearance in the text as main mention and the rest as equivalent to this main mention.

Finally, each token that does not have any of the aforementioned types of relations has

a skip relation with itself (e.g., “includes” has a skip relation with “includes”), such that

each token has a uniquely deﬁned head.

Thus, we cast the structured prediction task of extracting the property tree from the

ad as a dependency parsing problem, where (i) an entity can be part-of only one (other)

entity, because the decisions are taken simultaneously for all part-of relations (e.g., a

certain room can only be part-of a single ﬂoor), and (ii) there are a priori no restrictions

on the type of entities or tokens that can be part-of others (e.g., a room can be either

part-of a ﬂoor, or the property itself, like an apartment). It is worth mentioning that

dependency annotations for our problem exhibit a signiﬁcant number of non-projective

arcs (26%) where part-of dependencies are allowed to cross (see Fig. 3), meaning that

entities involved in the part-of relation are non-adjacent (i.e., interleaved by other en-

tities). For instance, all the entities or the tokens for the pipeline and the joint models,

that are attached to the entity “garage” are overlapping with the entities that are at-

tached to the entity “apartment”, making parsing even more complicated: handling

only projective dependencies as illustrated in Fig. 2 is an easier task. We note that the

segment dependencies do not suffer from non-projectivity, since the tokens are always

adjacent and sequential (e.g., “3 spacious bedrooms”).

11

0
property

1
large
apartment

2
living
room

3
3 spacious
bedrooms

4
bathroom

Figure 2: An example graph of projective part-of dependencies.

0
property

1
large
apartment

2
garage

3
living 
room

4
3 spacious
bedrooms

5
bathroom

6
bike wall
bracket

7
gate

Figure 3: Graph representing the part-of dependencies of Fig. 1. The dashed arcs are representing the
non-projective dependencies.

4. Methodology

We now describe the two approaches, i.e., the pipeline model and the joint model to

construct the property tree of the textual advertisements, as illustrated in Fig. 4. For

the pipeline system (Section 4.1), we (1) identify the entity mentions (Section 4.1.1),

then (2) predict the part-of dependencies between them (Section 4.1.2), and ﬁnally (3) con-

struct the tree representation (i.e., property tree) of the textual classiﬁed (e.g., as in

Fig. 1).

In step (2), we apply locally or globally trained graph-based models. We

represent the result of step (2) as a graph model, and then solve step (3) by apply-

ing the maximum spanning tree algorithm (Chu & Liu, 1965; Edmonds, 1967) for

the directed case (see McDonald et al. (2005)). We do not apply the well-known and

fast transition-based systems with hand-crafted features for non-projective dependency

structures (Nivre, 2009; Bohnet & Nivre, 2012), given the previously established poor

performance thereof in Bekoulis et al. (2017). In Section 4.2, we describe the joint

model where we perform steps (1) and (2) jointly. For step (3), we apply the maximum

spanning tree algorithm (Chu & Liu, 1965; Edmonds, 1967) similarly as in the pipeline

setting (Section 4.1).

4.1. Two-step pipeline

Below we revisit the pipeline approach presented in Bekoulis et al. (2017), which serves

as the baseline which we compare the neural models against. As mentioned before, the

pipeline model comprises two subtasks: (1) the sequence labeling and the (2) part-of

12

tree construction. In the following subsections, we describe the methods applied for

both.

4.1.1. Sequence labeling

The ﬁrst step in our pipeline approach is the sequence labeling subtask which is similar

to NER. Assuming a textual real estate classiﬁed, we (i) identify the entity mention

boundaries and (ii) map each identiﬁed entity mention to a categorical label, i.e., entity

type. In general, in the sequence labeling tasks, it is beneﬁcial to take into account

the correlations between labels in adjacent tokens, i.e., consider the neighborhood, and

jointly ﬁnd the most probable chain of labels for the given input sentence (Viterbi

algorithm for the most probable assignment). For instance, in our problem where we

follow the NER standard BIO encoding (Ratinov & Roth, 2009), the I-PROPERTY

cannot be followed by I-SPACE without ﬁrst opening the type by B-SPACE. We use

a special case of the CRF algorithm (Lafferty et al., 2001; Peng & McCallum, 2006),

namely linear chain CRFs, which is commonly applied in the problem of sequence

labeling to learn a direct mapping from the feature space to the output space (types)

where we model label sequences jointly, instead of decoding each label independently.

A linear-chain CRF with parameters w deﬁnes a conditional probability Pw(y|x) for

the sequence of labels y = y1, ..., yN given the tokens of the text advertisement x =

x1, ..., xN to be

Pw(y|x) =

exp(wT φ(x, y)),

1
Z(x)

(1)

where Z is the normalization constant and φ is the feature function that computes a

feature vector given the advertisement and the sequence of labels.

4.1.2. Part-of tree construction

The aim of the part-of tree construction subtask is to link each entity to its parent.

We approach the task as a dependency parsing problem but instead of connecting each

token to its syntactical parent, we map only the entity set I (e.g., “large villa”, “3

spacious bedrooms”) that has already been extracted by the sequence labeling subtask

to a dependency structure y. Assuming the entity set I = {e0, e1, ..., et} where t is the

number of identiﬁed entities, a dependency is a pair (p, c) where p ∈ I is the parent

13

two-step pipeline

(1) entity
recognition

(2) part-of RE

(1+2) joint entity recognition
& part-of RE

.
.
.

(3) tree
construction

joint model

Figure 4: The full structured prediction system setup.

entity and c ∈ I is the child entity. The entity e0 is the dummy root-symbol that only

appears as parent.

We will compare two approaches to predict the part-of relations: a locally trained

model (LTM) scoring all candidate edges independently, versus a global model (MTT)

which jointly scores all edges as a whole.

Locally trained model (LTM)

In the locally trained model (LTM), we adopt a traditional local discriminative method

and apply a binary classiﬁcation framework (Yamada & Matsumoto, 2003) to learn the

part-of relation model (step (2)), based on standard relation extraction features such

as the parent and child tokens and their types, the tokens in between, etc. For each

candidate parent-child pair, the classiﬁer gives a score that indicates whether it is prob-

able for the part-of relation to hold between them. The output scores are then used

for step (3), to construct the ﬁnal property tree. Following McDonald et al. (2005);

McDonald & Pereira (2007), we view the entity set I as a fully connected directed

graph G = {V, E} with the entities e1, ..., et as vertices (V ) in the graph G, and edges

E representing the part-of relations with the respective classiﬁer scores as weights.

One way to approach the problem is the greedy inference method where the predic-

tions are made independently for each parent-child pair, thus neglecting that the global

target output should form a property tree. We could adopt a threshold-based approach,

i.e., keep all edges exceeding a threshold, which obviously is not guaranteed to end

up with arc dependencies that form a tree structure (i.e., could even contain cycles).

On the other hand, we can enforce the tree structure inside the (directed) graph by

14

ﬁnding the maximum spanning tree. To this end, similar to McDonald et al. (2005);

McDonald & Pereira (2007), we apply the Edmonds’ algorithm to search for the most

probable non-projective tree structure in the weighted fully connected graph G.

Globally trained model (MTT)

The Matrix-Tree theorem (MTT) (Koo et al., 2007) is a globally normalized statistical

method that involves the learning of directed spanning trees. Unlike the locally trained

models, MTT is able to learn tree dependency structures, i.e., scoring parse trees for

a given sentence. We use D(I) to refer to all possible dependencies of the identiﬁed

entity set I, in which each dependency is represented as a tuple (h, m) in which h is

the head (or parent) and m the modiﬁer (or child). The set of all possible dependency

structures for a given entity set I is written T (I). The conditional distribution over all

dependency structures y ∈ T (I) can then be deﬁned as:

P (y|I; θ) =

1
Z(I; θ)

exp 


X
h,m∈y

θh,m





(2)

in which the coefﬁcients θh,m ∈ R for each dependency (h, m) form the real-valued

weight vector θ. The partition function Z(I; θ) is a normalization factor that alas can-

not be computed by brute-force, since it requires a summation over all y ∈ T (I),

containing an exponential number of possible dependency structures. However, an

adaptation of the MTT allows us the direct and efﬁcient computation of the partition

function Z(I; θ) as the determinant det(L(θ)) where L(θ) is the Laplacian matrix of

the graph. It is worth mentioning that although MTT learns spanning tree structures

during training, at the prediction phase, it is still required to use the maximum spanning

tree algorithm (step (3)) (McDonald et al., 2005; McDonald & Pereira, 2007) as in the

locally trained models.

4.2. Joint model

In this section, we present the new joint model sketched in Fig. 5, which simultaneously

predicts the entities in the sentence and the dependencies between them, with the ﬁnal

goal of obtaining a tree structure, i.e., the property tree. We pose the problem of the

identiﬁcation of the entity mentions and the dependency arcs between them as a head

15

Figure 5: The architecture of the joint model.

selection problem (Zhang et al., 2017). Speciﬁcally, given as input a sentence of length

N , the model outputs the predicted parent of each token of the advertisement and the

most likely dependency label between them. We begin by describing how the tokens

are represented in the model, i.e., with ﬁxed pre-trained embeddings (Section 4.2.1),

which form the input to an LSTM layer (Section 4.2.2). The LSTM outputs are used

as input to the entity and dependency scoring layer (Section 4.2.3). As an extension of

this model, we propose the use of various attention layers in between the LSTM and

scoring layer, to encourage the model to focus on salient information, as described in

Section 4.2.4. The ﬁnal output of the joint model still is not guaranteed to form a tree

structure. Therefore, we still apply Edmonds’ algorithm (i.e., step (3) from the pipeline

approach), described in Section 4.2.5.

4.2.1. Embedding Layer

The embedding layer maps each token of the input sequence x1, ..., xN of the consid-

ered advertisement to a low-dimensional vector space. We obtain the word-level em-

beddings by training the Skip-Gram word2vec model (Mikolov et al., 2013) on a large

collection of property advertisements. We add a symbol x0 in front of the N -length

input sequence, which will act as the root of the property tree, and is represented with

16

an all-zeros vector in the embedding layer.

4.2.2. Bidirectional LSTM encoding layer

Many neural network architectures have been proposed in literature: LSTMs (Hochreiter & Schmidhuber,

1997), CNNs (LeCun et al., 1989), Echo State Networks (Jaeger, 2010), or Stochastic

Conﬁguration Networks (Wang & Li, 2017), to name only a few. Many others can

be found in reference works on the topic (Goodfellow et al., 2016; Goldberg & Hirst,

2017). In this work, we use RNNs which have been proven to be particularly effective

in a number of NLP tasks (Sutskever et al., 2014; Lample et al., 2016; Miwa & Bansal,

2016). Indeed, RNNs are a common and reasonable choice to model sequential data

and inherently able to cope with varying sequence lengths. Yet, plain vanilla RNNs

tend to suffer from vanishing/exploding gradient problems and are hence not success-

ful in capturing long-term dependencies (Bengio et al., 1994; Pascanu et al., 2013).

LSTMs are a more advanced kind of RNNs, which have been successfully applied

in several tasks to capture long-term dependencies, as they are able to effectively over-

come the vanishing gradient problem. For many NLP tasks, it is crucial to represent

each word in its own context, i.e., to consider both past (left) and future (right) neigh-

boring information. An effective solution to achieve this is using a bidirectional LSTM

(BiLSTM). The basic idea is to encode each sequence from left to right (forward) and

from right to left (backward). This way, there is one hidden state which represents the

past information and another one for the future information. The high-level formulation

of an LSTM is:

hi, ci = LSTM(wi, hi−1, ci−1),

i = 0, ..., N

(3)

where in our setup wi ∈ R ˜d is the word embedding for token xi, and with the input and
states for the root symbol x0 initialized as zero vectors. Further, hi ∈ Rd and ci ∈ Rd

respectively are the output and cell state for the ith position, where d is the hidden state

size of the LSTM. Note that we chose the word embedding size the same as the LSTM
hidden state size, or ˜d = d. The outputs from left to right (forward) are written as
~hi and the outputs from the backwards direction as ~hi. The two LSTMs’ outputs at

17

position i are concatenated to form the output hi at that position of the BiLSTM:

hi = [ ~hi; ~hi],

i = 0, ..., N

(4)

4.2.3. Joint learning as head selection

In this subsection, we describe the joint learning task (i.e., identifying entities and

predicting dependencies between them), which we formulate as a head selection prob-

lem (Zhang et al., 2017). Indeed, each word xi should have a unique head (parent)

— while it can have multiple dependent words — since the ﬁnal output should form

the property tree. Unlike the standard head selection dependency parsing framework

(Zhang et al., 2017), we predict the head yi of each word xi and the relation ci between

them jointly, instead of ﬁrst obtaining binary predictions for unlabeled dependencies,

followed by an additional classiﬁer to predict the labels.

Given a text advertisement as a token sequence x = x0, x1, ..., xN where x0 is the

dummy root symbol, and a set C = {part-of, segment, equivalent, skip} of predeﬁned

labels (as deﬁned in Section 3), we aim to ﬁnd for each token xi, i ∈ {0, ..., N } the

most probable head xj , j ∈ {0, ..., N } and the most probable corresponding label c ∈

C. For convenience, we order the labels c ∈ C and identify them as ck, k ∈ {0, ..., 3}.

We model the joint probability of token xj to be the head of xi with ck the relation

between them, using a softmax:

P (head = xj, label = ck|xi) =

exp(score(hj , hi, ck))
˜j,˜k exp(score(h˜j , hi, c˜k)

P

(5)

where hi and hj are the BiLSTM encodings for words xi and xj, respectively. For

the scoring formula score(hj , hi, ck) we use a neural network layer that computes the

relative score between position i and j for a speciﬁc label ck as follows:

score(hj , hi, ck) = V T

k tanh(Ukhj + Wkhi + bk)

(6)

with trainable parameters Vk ∈ Rl, Uk ∈ Rl×2d, Wk ∈ Rl×2d, bk ∈ Rl, and l the

layer width. As detailed in Section 5.1, we set l to be smaller than 2d, similar to

Dozat & Manning (2017) due to the fact that training on superﬂuous information re-

duces the parsing speed and increases tendency towards overﬁtting. We train our model

18

by minimizing the cross-entropy loss L, written for the considered training instance as:

L =

− log P (head = yi, label = ci|xi)

(7)

N

X
i=0

where yi ∈ x and ci ∈ C are the ground truth head and label of xi, respectively. After

training, we follow a greedy inference approach and for each token, we simultaneously

keep the highest scoring head ˆyi and label ˆci for xi based on their estimated joint

probability:

( ˆyi, ˆci) = argmax
xj ∈x,ck∈C

P (head = xj, label = ck|xi)

(8)

The predictions ( ˆyi, ˆci) are made independently for each position i, neglecting that

the ﬁnal structure should be a tree. Nonetheless, as demonstrated in Section 5.2, the

highest scoring neural models are still able to come up with a tree structure for 78%

of the ads. In order to ensure a tree output in all cases, however, we apply Edmonds’

algorithm on the output.

4.2.4. Attention Layer

The attention mechanism in our structured prediction problem aims to improve the

model performance by focusing on information that is relevant to the prediction of the

most probable head for each token. As attention vector, we construct the new context
vector h∗

i as a weighted average of the BiLSTM outputs

in which the coefﬁcients a(hj, hi), also called the attention weights, are obtained as

h∗
j =

N

X
i=0

a(hj, hi) hi

a(hj, hi) =

exp(att(hj, hi))
N
˜i=0 exp(att(hj , h˜i))

.

P

(9)

(10)

The attention function att(hj, hi) is designed to measure some form of compatibility

between the representation hi for xi and hj for xj, and the attention weights a(hj, hi)

are obtained from these scores by normalization using a softmax function. In the fol-

lowing, we will describe in detail the various attention models that we tested with our

follows:

joint model.

19

Commonly used attention mechanisms

Three commonly used attention mechanisms are listed in eqs. (11) to (13): the addi-

tive (Vinyals et al., 2015), bilinear, and multiplicative attention models (Luong et al.,

2015), which have been extensively used in machine translation. Given the represen-

tations hi and hj for tokens xi and xj, we compute the attention scores as follows:

attadditive(hj, hi) = Va tanh(Uahj + Wahi + ba)

attbilinear(hj, hi) = hT

j Wbilhi

attmultiplicative(hj, hi) = hT

j hi

where Va ∈ Rl, Ua, Wa ∈ Rl×2d, Wbil ∈ R2d×2d and ba ∈ Rl are learnable parame-

ters of the model.

Biafﬁne attention

We use the biafﬁne attention model (Dozat & Manning, 2017) which has been recently

applied to dependency parsing and is a modiﬁcation of the neural graph-based approach

that was proposed by Kiperwasser & Goldberg (2016). In this model, Dozat & Manning

(2017) tried to reduce the dimensionality of the recurrent state of the LSTMs by apply-

ing a such neural network layer on top of them. This idea is based on the fact that there

is redundant information in every hidden state that (i) reduces parsing speed and (ii) in-

creases the risk of overﬁtting. To address these issues, they reduce the dimensionality

and apply a nonlinearity afterwards. The deep bilinear attention mechanism is deﬁned

as follows:

(11)

(12)

(13)

(14)

(15)

hdep
i = Vdep tanh(Udephi + bdep)

hhead
j = Vhead tanh(Uheadhj + bhead)

20

function:

Edge attention

to be:

attbiafﬁne(hhead

j

, hdep

i ) = (hhead

j

)T Wbilhdep

i + Bhhead

j

(16)

where Udep, Uhead ∈ Rl×2d, Vdep, Vhead ∈ Rp×l, Wbil ∈ Rp×p, B ∈ Rp and bdep,
bhead ∈ Rl.

Tensor attention

This section introduces the Neural Tensor Network (Socher et al., 2013) that has been

used as a scoring formula applied for relation classiﬁcation between entities. The task

can be described as link prediction between entities in an existing network of relation-

ships. We apply the tensor scoring formula as if tokens are entities, by the following

atttensor(hj, hi) = Ut tanh(hT

j Wthi + Vt(hj + hi) + bt)

(17)

where Wt ∈ R2d×l×2d, Vt ∈ Rl×2d, Ut ∈ Rl and bt ∈ Rl.

In the edge attention model, we are inspired by Gilmer et al. (2017), which applies

neural message passing in chemical structures. Assuming that words are nodes inside

the graph and the message ﬂows from node xi to xj, we deﬁne the edge representation

(18)

(19)

edge(hj, hi) = tanh(Uehj + Wehi + be)

The edge attention formula is computed as:

attedge(hj, hi) =

Asrc

edge(hj, h˜i) + Adst

1
N





N

X
˜i=0

N

X
˜j=0

edge(h˜j, hi)


where Ue, We ∈ Rl×2d, Asrc, Adst ∈ R2d×l and be ∈ Rl. The source and destination

matrices respectively encode information for the start to the end node, in the directed

edge. Running the edge attention model for several times can be achieved by stacking

the edge attention layer multiple times. This is known as message passing phase and

we can run it for several (T > 1) time steps to obtain more informative edge represen-

tations.

21

4.2.5. Tree construction step: Edmonds’ algorithm

At decoding time, greedy inference is not guaranteed to end up with arc dependencies

that form a tree structure and the classiﬁcation decision might contain cycles. In this

case, the output can be post-processed with a maximum spanning tree algorithm (as

the third step in Fig. 4). We construct the fully connected directed graph G = (V, E)

where the vertices V are the tokens of the advertisement (that are not predicted as

skips) and the dummy root symbol, E contains the edges representing the highest scor-

ing relation (e.g., part-of, segment, equivalent) with the respective cross entropy scores

serving as weights. Since G is a directed graph, s(xi, xj) is not necessarily equal to

s(xj, xi). Similar to McDonald et al. (2005), we employ Edmonds’ maximum span-

ning tree algorithm for directed graphs (Chu & Liu, 1965; Edmonds, 1967) to build a

non-projective parser. Indeed, in our setting, we have a signiﬁcant number (26% in

the dataset used for experiments, see further) of non-adjacent part-of and equivalent

relations (non-projective). It is worth noting that in the case of segment relations, the

words involved are not interleaved by other tokens and are always adjacent. We ap-

ply Edmonds’ algorithm to every graph which is constructed to get the highest scoring

graph structure, even in the cases where a tree is already formed by greedy inference.

For skips, we consider the predictions as obtained from the greedy approach and we do

not include them in the fully connected weighted graph, since Edmonds’ complexity is
O(n2) for dense graphs and might lead to slow decoding time.

In this section, we present the experimental results of our study. We describe the

dataset, the setup of the experiments and we compare the results of the methods anal-

5. Results and discussion

ysed in the previous sections.

5.1. Experimental setup

Our dataset consists of a large collection (i.e., 887,599) of Dutch property advertise-

ments from real estate agency websites. From this large dataset, a sub-collection of

2,318 classiﬁeds have been manually annotated by 3 trained human annotators (1 an-

notation per ad, 773 ads per annotator). The annotations follow the format of the

22

property tree that is described in detail in Section 3 and is illustrated in Fig. 1. The

dataset is available for research purposes, see our github codebase.1

In the experi-

ments, we use only the annotated text advertisements for the pipeline setting, i.e., LTM

(locally trained model), MTT (globally trained model). In the case of the neural net-

work approach, we train the embeddings on the large collection by using the word2vec

model (Mikolov et al., 2013) whereas in the joint learning, we use only the annotated

documents, similar to the pipeline approach. The code of the LTM and the MTT hand-

crafted systems is available on github.1 We also use our own CRF implementation. The

code for the joint model has been developed in Python with the Tensorﬂow machine

learning library (Abadi et al., 2016) and will be made public as well. For the evalu-

ation, we use 70% for training, 15% for validation and 15% as test set. We measure

the performance by computing the F1 score on the test set. The accuracy metric can

be misleading in our case since we have to deal with imbalanced data (the skip label

is over-represented). We only report numbers on the structured classes, i.e., segment

and part-of since the other dependencies (skip, equivalent) are auxiliary in the joint

models and do not directly contribute to the construction of the actual property tree.

For the overall F1, we are again only considering the structured classes. Finally, we

report the number of property trees (which shows how likely our model is to produce

trees without applying Edmonds’ algorithm, i.e., by greedy inference alone) for all the

models before applying Edmonds’ algorithm that guarantees the tree structure of the

predictions.

For the pipeline models, we train the CRF with regularization parameter λCRF =

10 and the LTM and MTT with C = 1 based on the best hyperparameters on the val-

idation set. As binary classiﬁer, we use logistic regression. For the joint model, we

train 128-dimensional word2vec embeddings on a collection of 887k advertisements.

In general, using larger embeddings dimensions (e.g., 300), does not affect the perfor-

mance of our models. We consistently used single-layer LSTMs through our experi-

ments to keep our model relatively simple and to evaluate the various attention meth-

ods on top of that. We have also reported results on the joint model using a two-layer

1https://github.com/bekou/ad_data

23

Precision

Recall

segment

part-of

segment

part-of

segment

Overall

Trees
(% of ads)

-
d
n
a
H

d
e
t
f
a
r
c

M
T
S
L

e
v
i
t
n
e
t
t

A

M
T
S
L

Model

LTM
MTT

LSTM
LSTM+E
2xLSTM+E

Additive
Bilinear
Multiplicative
Biafﬁne
Tensor
Edge1
Edge2
Edge3

73.77
73.77

70.24
70.18
73.91

72.97
70.25
71.12
70.01
71.53
71.56
72.03
71.74

60.53
61.15

65.23
63.92
69.88

65.71
66.34
66.40
64.67
64.68
67.46
66.09
67.69

70.98
70.98

77.73
77.77
75.78

76.45
79.96
77.81
78.32
76.17
78.24
75.35
78.44

60.40
61.01

70.32
71.08
71.22

70.90
72.53
71.26
71.04
70.79
71.31
70.99
73.00

72.35
72.35

73.80
73.78
74.83

74.67
74.79
74.31
73.93
73.78
74.75
73.65
74.94

F1 (%)
part-of

60.47
61.08

67.68
67.31
70.54

68.21
69.29
68.75
67.71
67.60
69.33
68.46
70.25

64.76
65.15

68.82
68.57
70.90

69.46
70.20
69.70
68.75
68.68
70.08
69.12
70.70

37.18
43.23

68.30
68.30
78.09

74.35
72.62
72.91
74.06
69.16
70.32
73.48
78.96

Table 2: Performance of the three approaches on the structured prediction task. The top rows are for the
pipeline approach, i.e., hand-crafted features. The next block of results presents the results for the neural
joint model based on LSTMs. The bottom block contains results of the joint models augmented with several
attentive architectures. Edmonds’ algorithm is applied in all of the models to retain the tree structure, except
for the LSTM joint model. The LSTM+E is the LSTM model with Edmonds’ algorithm included. The
2xLSTM+E is the same joint model but it simply uses a stack of two LSTM layers. In the experiments with
attention, we use a one-stack LSTM. The rightmost column is the percentage of the ads that are valid trees
before applying Edmonds’ (i.e., step (3) of Fig. 4), showing the ability of the model to form trees during
greedy inference. In the Edgei models, the number i stands for the number of times that we have run the
message passing phase.

stacked LSTM joint model, although it needs a higher computation time compared to

a single-layer LSTM with an attention layer on top. The hidden size of the LSTMs is

d = 128 and the size of the neural network used in the scoring and the attention layer is

ﬁxed to l = 32. The optimization algorithm used is Adam (Kingma & Ba, 2015) with
a learning rate of 10−3. To reduce the effect of overﬁtting, we regularize our model

using the dropout method (Srivastava et al., 2014). We ﬁx the dropout rate on the in-

put of the LSTM layer to 0.5 to obtain signiﬁcant improvements (∼1%-2% F1 score

increase, depending on the model). For the two-layer LSTM, we ﬁx the dropout rate

to 0.3 in each of the input layers since this leads to largest performance increase on the

validation set. We have also explored gradient clipping without further improvement

on our results. In the joint model setting, we follow the evaluation strategy of early

stopping (Caruana et al., 2000; Graves et al., 2013) based on the performance of the

validation set. In most of the experiments, we obtain the best hyperparameters after

∼60 epochs.

24

5.2. Comparison of the pipeline and the joint model

One of the main contributions of our study is the comparison of the pipeline approach

and the proposed joint model. We formulated the problem of identifying the enti-

ties (i.e., segments) and predicting the dependencies between them (i.e., construc-

tion of the property tree) as a joint model. Our neural model, unlike recent stud-

ies (Miwa & Bansal, 2016; Zheng et al., 2017) on joint models that use LSTMs to

handle similar tasks, does not need two components to model the problem (i.e., NER

and dependency parsing). To the best of our knowledge, our study is the ﬁrst that for-

mulates the task in an actual joint setting without the need to pre-train the sequence

labeling component or for parameter sharing between them, since we use only one

component for both subtasks. In Table 2, we present the results of the pipeline model

(hand-crafted) and the proposed joint model (LSTM). The improvement of the joint

model over the pipeline is unambiguous, i.e., 3.42% overall F1 score difference be-

tween MTT (highest scoring pipeline model) and LSTM+E (LSTM model with Ed-

monds’ algorithm). An additional increase of ∼2.3% is achieved when we consider

two-layer LSTMs (2xLSTM+E) for our joint model. All results in Table 2, except for

the LSTM, are presented using Edmonds’ algorithm on top, to construct the property

tree. Examining each label separately, we observe that the original LSTM+E model

(73.78%) performs better by 1.43% in entity segmentation than the CRF (72.35%).

The LSTM model achieves better performance in the entity recognition task since it

has to learn the two subtasks simultaneously resulting in interactions between the com-

ponents (i.e., NER and dependency parser). This way, the decisions for the entity

recognition can beneﬁt from predictions that are made for the part-of relations. Con-

cerning the part-of dependencies, we note that the LSTMs outperform the hand-crafted

approaches by 6.23%. Also, the number of valid trees that are constructed before ap-

plying Edmonds’ algorithm is almost twice as high for the LSTM models. Stacking

two-layer LSTMs results in an additional ∼1% improvement in the segmentation task

and ∼3% in the part-of relations. The greedy inference for the hand-crafted meth-

ods does not produce well-formed trees, meaning that post-processing with Edmonds’

algorithm (enforce tree structure) is expected to increase the performance of the hand-

crafted models compared to the LSTM model performance. Indeed, the performance

25

of the feature based hand-crafted models (i.e., LTM and MTT) without the Edmonds’

on top is not reported in Table 2 due to their poor performance in our task (i.e., ∼60%

overall F1 and ∼51% for part-of ), but after post-processing with Edmonds’ the perfor-

mance signiﬁcantly increases (i.e., ∼65%). On the other hand, applying the Edmonds’

algorithm on the LSTM model leads to marginally decreased performance (∼0.2%)

compared to the original LSTM model, probably indicating that enforcing structural

constraints is not beneﬁcial for a model that clearly has the ability to form valid tree

structures during greedy inference. Although one might be tempted not to enforce the

tree structure (post-process with Edmonds’), due to the nature of our problem, we have

to enforce tree constraints in all of the models.

5.3. Comparison of the joint and the attention model

After having established the superior performance of neural approach using LSTMs

over the more traditional (LTM and MTT) methods based on hand-crafted features, we

now discuss further improvements using attentive models. The attention mechanisms

are designed to encourage the joint model to focus on informative tokens. We exploited

several attention mechanisms as presented in Section 4.2.4. Table 2 shows the perfor-

mance of the various models. Overall, the attention models are performing better in

terms of overall F1 score compared to the original joint model with the Edmonds’ on

top. Although the performance of the Biafﬁne and the Tensor models is limited com-

pared to the improvement of the other attentive models, we focus on: (i) the Biafﬁne

model since it achieved state-of-the-art performance on the dependency parsing task

and (ii) the Tensor model because we were expecting that it would perform similarly

to the Bilinear model (it has a bilinear tensor layer). Despite its simplicity, the Bilinear

model is the second best performing attentive model in Table 2 in terms of overall F1

score. Edge3 (70.70% overall F1 score) achieves better results than the other attention

mechanisms in the entity recognition and in the dependency parsing tasks. We observe

that running the message passing step multiple times in the Edge model, gives an in-

creasing trend in the number of valid trees that were constructed before applying the

maximum spanning tree algorithm. This is not surprising since we expect that running

the message passing phase multiple times leads to improved edge representations. The

26

maximum number of trees without post-processing by Edmonds’ is attained when we

run the message passing for 3 times whereas further increasing the number beyond 3

(e.g., 4) appears no longer beneﬁcial. Stacking a second LSTM layer on top of the joint

model (2xLSTM+E) marginally improves the performance by 0.2% compared to the

Edge3 attention model. But adding a second LSTM layer comes with the additional

cost of an increased computation time compared to the joint models with the attention

layers on top. This illustrates that: (i) there might be some room for marginally im-

proving the attention models even further, and (ii) we do not have to worry about the

quadratic nature of our approach since in terms of speed the attentive models are able

to surpass the two-layer LSTMs. The sequential processing of the LSTMs might be

the reason that slows down the computation time for the 2xLSTM over the rest of the

attentive models. Speciﬁcally, on an Intel(R) Xeon(R) CPU E5-2650 v2 @ 2.60GHz

processor, the best performing model (i.e., Edge3) takes ∼2 minutes per epoch while

in the 2xLSTM case, it takes ∼2.5 minutes leading to a slowdown of ∼25%. The

percentage of the ads that are valid trees is 1% better in the Edge3 over the two-layer

LSTM showcasing the ability of the Edge model to form more valid trees during greedy

inference.

5.4. Discussion

In this section, we discuss some additional aspects of our problem and the approaches

that we follow to handle them. As we mentioned before, a single entity can be present

in the text with multiple mentions. This brings an extra difﬁculty to our task. For

instance, in the example of Fig. 1, the entity “large apartment” is expressed in the ad

with the mentions “large apartment” and “home”. Hence it is confusing to which men-

tion the other entities should be attached to. One way would be to attach them to both

and then eliminate one of the connections using Edmonds’ spanning tree algorithm,

which is the approach adopted in Bekoulis et al. (2017). The problematic issue with

this approach is that the spanning tree algorithm would randomly remove all mentions

but one, possibly resulting in uncertain outcomes. To avoid this problem, we now use

as the main mention for an entity the ﬁrst mention in order of appearance in the text

(e.g., “large apartment” in our example) and the remaining mentions (e.g., “home”)

27

are attached as equivalent mentions to the main one. Usually, the most informative

mention for an entity is the one that appears ﬁrst, because we again refer to an entity

mentioned before, often with a shorter description. Following our intuition, the neural

model increases its overall performance by ∼3% (from 66% to 69% and more than 5%

in the part-of relation) and the pipeline approaches by almost 4% (from 61%, reported

in Bekoulis et al. (2017) to 65% and more than 5% in the part-of relation).

We also experimented with introducing the equivalent relations. Although it is

a strongly under-represented class in the dataset and the model performs poorly for

this label (an equivalent edge F1 score of 10%), introducing the equivalent label is

the natural way of modeling our problem (i.e., assigning each additional mention as

equivalent to the main mention). We ﬁnd out that introducing this type of relation

leads to a slight decrease (∼1%) in the part-of and a marginal increase (∼0.3%) in

the segment relations which are the main relations while retaining the nature of our

problem. In the pipeline approach, it results in an 9% drop in the F1 score of the part-

of relation. This is the reason that the results as presented in Table 2 do not consider

the equivalent relation for the hand-crafted model to make a fair comparison in the

structured classes.

We believe our experimental comparison of the various architectural model vari-

ations provides useful ﬁndings for practitioners. Speciﬁcally, for applications requir-

ing both segmentation (entity recognition) and dependency parsing (structured predic-

tion), our ﬁndings can be qualitatively summarized as follows: (i) joint modeling is

the most appropriate approach since it reduces error propagation between the com-

ponents, (ii) the LSTM model is much more effective (than models relying on hand-

crafted features) because it automatically extracts informative features from the raw

text, (iii) attentive models are proven effective because they encourage the model to fo-

cus on salient tokens, (iv) the edge attention model leads to an improved performance

since it better encodes the information ﬂow between the entities by using graph rep-

resentations, and (v) stacking a second LSTM marginally increases the performance,

suggesting that there might be some room for slight improvement of the attention mod-

els by adding LSTM layers.

Finally, we point out how exactly our model relates to state-of-the-art in the ﬁeld.

28

Our joint model is able to both extract entity mentions (i.e., perform segmentation)

and do dependency parsing, which we demonstrate on the real estate problem. Previ-

ous studies (Kate & Mooney, 2010; Li & Ji, 2014; Miwa & Sasaki, 2014) that jointly

considered the two subtasks (i.e., segmentation and relation extraction): (i) require

manual feature engineering and (ii) generalize poorly between various applications.

On the other hand, in our work, we rely on neural network methods (i.e., LSTMs) to

automatically extract features from the real estate textual descriptions and perform the

two tasks jointly. Although there are other methods which use neural network archi-

tectures (Miwa & Bansal, 2016; Zheng et al., 2017; Li et al., 2017) that focus on the

relation extraction problem, our work is different in that we aim to model directed

spanning trees and thus to solve the dependency parsing problem which is more con-

strained and difﬁcult (than extracting single instances of binary relations). Moreover,

the cited methods require either parameter sharing or pre-training of the segmenta-

tion module, which complicates learning. Therefore, cited methods are not directly

comparable to our model and cannot be applied to our real estate task out-of-the-box.

However, our model’s main limitation is the quadratic scoring layer that increases the

time complexity of the segmentation task from linear (which is the complexity of a
conditional random ﬁeld, CRF) to O(n2). As a result, it sacriﬁces standard linear com-

plexity of the segmentation task, in order to reduce the error propagation between the

subtasks and thus perform learning in a joint, end-to-end differentiable, setting.

6. Conclusions

In this paper, we proposed an LSTM-based neural model to jointly perform segmenta-

tion and dependency parsing. We apply it to a real estate use case processing textual

ads, thus (1) identifying important entities of the property (e.g., rooms) and (2) struc-

turing them into a tree format based on the natural language description of the prop-

erty. We compared our model with the traditional pipeline approaches that have been

adapted to our task and we reported an improvement of 3.4% overall edge F1 score.

Moreover, we experimented with different attentive architectures and stacking of a sec-

ond LSTM layer over our basic joint model. The results indicate that exploiting atten-

29

tion mechanisms that encourage our model to focus on informative tokens, improves

the model performance (increase of overall edge F1 score with ∼2.1%) and increases

the ability to form valid trees in the prediction phase (4% to 10% more valid trees for

the two best scoring attention mechanisms) before applying the maximum spanning

tree algorithm.

The contribution of this study to the research in expert and intelligent systems is

three-fold: (i) we introduce a generic joint model, simultaneously solving both sub-

tasks of segmentation (i.e., entity extraction) and dependency parsing (i.e., extracting

relationships among entities), that unlike previous work in the ﬁeld does not rely on

manually engineered features, (ii) in particular for the real estate domain, extracting

a structured property tree from a textual ad, we reﬁne the annotations and addition-

ally propose attention models, compared to initial work on this application, and ﬁ-

nally (iii) we demonstrate the effectiveness of our proposed generic joint model with

extensive experiments (see aforementioned F1 improvement of 2.1%). Despite the

experimental focus on the real estate domain, we stress that the model is generic in na-

ture, and could be equally applied to other expert system scenarios requiring the general

tasks of both detecting entities (segmentation) and establishing relations among them

(dependency parsing). We furthermore note that our model, rather than focusing on

extracting a single binary relation from a sentence (as in traditional relation extraction

settings), produces a complete tree structure.

Future work can evaluate the value of our joint model we introduced in other spe-

ciﬁc application domains (e.g., biology, medicine, news) for expert and intelligent sys-

tems. For example, the method can be evaluated for entity recognition and binary

relation extraction (the ACE 04 and ACE 05 datasets; see Miwa & Bansal (2016)) or in

adverse drug effects from biomedical texts (see Li et al. (2016)). In terms of model ex-

tensions and improvements, one research issue is to address the time complexity of the

NER part by modifying the quadratic scoring layer for this component. An additional

research direction is to investigate different loss functions for the NER component (e.g.,

adopting a conditional random ﬁeld (CRF) approach), since this has been proven ef-

fective in the NER task on its own (Lample et al., 2016). A ﬁnal extension we envision

is to enable multi-label classiﬁcation of relations among entity pairs.

30

The presented research was partly performed within the MALIBU project, funded by

Flanders Innovation & Entrepreneurship (VLAIO) contract number IWT 150630.

Acknowledgments

References

Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat,

S., Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga, R., Moore, S., Murray,

D. G., Steiner, B., Tucker, P., Vasudevan, V., Warden, P., Wicke, M., Yu, Y., & Zheng,

X. (2016). Tensorﬂow: A system for large-scale machine learning. In Proceedings

of the 12th USENIX Conference on Operating Systems Design and Implementation

(pp. 265–283). Berkeley, CA, USA.

Andor, D., Alberti, C., Weiss, D., Severyn, A., Presta, A., Ganchev, K., Petrov, S.,

& Collins, M. (2016). Globally normalized transition-based neural networks.

In

Proceedings of the 54th Annual Meeting of the Association for Computational Lin-

guistics (Volume 1: Long Papers) (pp. 2442–2452). Berlin, Germany.

Atkinson, J., & Bull, V. (2012). A multi-strategy approach to biological named

entity recognition. Expert Systems with Applications, 39(17), 12968 – 12974.

doi:10.1016/j.eswa.2012.05.033.

Bach, N., & Badaskar, S. (2007). A review of relation extraction. Literature review for

Language and Statistics II, .

Bekoulis, G., Deleu, J., Demeester, T., & Develder, C. (2017). Reconstructing the

house from the ad: Structured prediction on real estate classiﬁeds. In Proceedings of

the 15th Conference of the European Chapter of the Association for Computational

Linguistics: (Volume 2, Short Papers) (pp. 274–279). Valencia, Spain.

Bengio, Y., Simard, P., & Frasconi, P. (1994). Learning long-term dependencies

with gradient descent is difﬁcult. Transactions on neural networks, 5(2), 157–166.

doi:10.1109/72.279181.

31

Bohnet, B., & Nivre, J. (2012). A transition-based system for joint part-of-speech tag-

ging and labeled non-projective dependency parsing.

In Proceedings of the 2012

Joint Conference on Empirical Methods in Natural Language Processing and Com-

putational Natural Language Learning (pp. 1455–1465). Jeju Island, Korea: Asso-

ciation for Computational Linguistics.

Carreras, X. (2007). Experiments with a higher-order projective dependency parser.

In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Lan-

guage Processing and Computational Natural Language Learning (pp. 957–961).

Prague, Czech: Association for Computational Linguistics.

Caruana, R., Lawrence, S., & Giles, L. (2000). Overﬁtting in neural nets: Backpropa-

gation, conjugate gradient, and early stopping. In Proceedings of the 13th Interna-

tional Conference on Neural Information Processing Systems (pp. 381–387). Den-

ver, USA: MIT Press.

Chen, D., & Manning, C. (2014). A fast and accurate dependency parser using neural

networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural

Language Processing (pp. 740–750). Doha, Qatar: Association for Computational

Linguistics.

Chiu, J., & Nichols, E. (2016). Named entity recognition with bidirectional LSTM-

CNNs. Transactions of the Association for Computational Linguistics, 4, 357–370.

Chu, Y.-J., & Liu, T.-H. (1965). On shortest arborescence of a directed graph. Scientia

Sinica, 14, 1396–1400.

Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).

Natural language processing (almost) from scratch. Journal of Machine Learning

Research, 12, 2493–2537.

Daum´e

III, H., Langford,

J., & Marcu, D.

(2009).

Search-based

structured

prediction.

Machine Learning

Journal,

75(3),

297–325.

doi:10.1007/s10994-009-5106-x.

32

Dozat, T., & Manning, C. D. (2017). Deep biafﬁne attention for neural dependency

parsing. In Proceedings of the International Conference for Learning Representa-

tions (pp. 1–8). Toulon, France.

Dyer, C., Ballesteros, M., Ling, W., Matthews, A., & Smith, N. A. (2015). Transition-

based dependency parsing with stack long short-term memory. In Proceedings of the

53rd Annual Meeting of the Association for Computational Linguistics and the 7th

International Joint Conference on Natural Language Processing (Volume 1: Long

Papers) (pp. 334–343). Beijing, China.

Edmonds, J. (1967). Optimum branchings. Journal of research of the National Bureau

of Standards, 71B(4), 233–240.

Eisner, J. M. (1996). Three new probabilistic models for dependency parsing: An

exploration. In Proceedings of the 16th International Conference on Computational

Linguistics (Volume 1) (pp. 340–345). Copenhagen, Denmark.

Fundel, K., Kffner, R., & Zimmer, R.

(2007).

Relex-relation extrac-

tion using dependency parse

trees.

Bioinformatics,

23(3),

365–371.

doi:10.1093/bioinformatics/btl616.

Gillick, D., Brunk, C., Vinyals, O., & Subramanya, A. (2016). Multilingual language

processing from bytes. In Proceedings of the 2016 Conference of the North Amer-

ican Chapter of the Association for Computational Linguistics: Human Language

Technologies (pp. 1296–1306). San Diego, California.

Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., & Dahl, G. E. (2017). Neural

message passing for quantum chemistry. In Proceedings of the 34th International

Conference on Machine Learning (pp. 1263–1272). Sydney, Australia: PMLR.

Goldberg, Y., & Hirst, G. (2017). Neural Network Methods in Natural Language Pro-

cessing. Morgan & Claypool Publishers.

Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.

http://www.deeplearningbook.org.

33

Graves, A., r. Mohamed, A., & Hinton, G. (2013). Speech recognition with deep

recurrent neural networks.

In Proceedings of the International Conference on

Acoustics, Speech and Signal Processing (pp. 6645–6649). Vancouver, Canada.

doi:10.1109/ICASSP.2013.6638947.

Gurulingappa, H., MateenRajpu, A., & Toldo, L. (2012). Extraction of potential ad-

verse drug events from medical case reports. Journal of Biomedical Semantics, 3(1),

1–15. doi:10.1186/2041-1480-3-15.

Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computa-

tion, 9(8), 1735–1780. doi:10.1162/neco.1997.9.8.1735.

Huang, Z., Xu, W., & Yu, K. (2015). Bidirectional LSTM-CRF models for sequence

tagging. arXiv preprint arXiv:1508.01991, .

Jaeger, H. (2010). The echo state approach to analysing and training recurrent neu-

ral networks-with an erratum note’. Bonn, Germany: German National Research

Center for Information Technology GMD Technical Report, 148(34), 13.

Jung, J. J. (2012). Online named entity recognition method for microtexts in social

networking services: A case study of twitter. Expert Systems with Applications,

39(9), 8066 – 8070. doi:10.1016/j.eswa.2012.01.136.

Kate, R. J., & Mooney, R. (2010). Joint entity and relation extraction using card-

pyramid parsing. In Proceedings of the 14th Conference on Computational Natural

Language Learning (pp. 203–212). Uppsala, Sweden: Association for Computa-

tional Linguistics.

Kingma, D., & Ba, J. (2015). Adam: A method for stochastic optimization. In Inter-

national Conference on Learning Representations. San Diego, USA.

Kiperwasser, E., & Goldberg, Y. (2016). Simple and accurate dependency parsing

using bidirectional lstm feature representations. Transactions of the Association for

Computational Linguistics, 4, 313–327.

34

Konkol, M., Brychcn, T., & Konopk, M. (2015).

Latent semantics in named

entity recognition.

Expert Systems with Applications, 42(7), 3470 – 3479.

doi:10.1016/j.eswa.2014.12.015.

Koo, T., Globerson, A., Carreras, X., & Collins, M. (2007). Structured prediction

models via the Matrix-Tree Theorem. In Proceedings of the 2007 Joint Conference

on Empirical Methods in Natural Language Processing and Computational Natural

Language Learning (pp. 141–150). Prague, Czech: Association for Computational

Linguistics.

K¨uc¸ ¨uk, D., & Yazıcı, A.

(2012).

A hybrid named entity recognizer

for

turkish.

Expert Systems with Applications, 39(3), 2733 – 2742.

doi:10.1016/j.eswa.2011.08.131.

Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional random ﬁelds: Proba-

bilistic models for segmenting and labeling sequence data. In Proceedings of the

18th International Conference on Machine Learning (pp. 282–289). San Francisco,

USA: Morgan Kaufmann.

Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., & Dyer, C. (2016).

Neural architectures for named entity recognition. In Proceedings of the 2016 Con-

ference of the North American Chapter of the Association for Computational Lin-

guistics: Human Language Technologies (pp. 260–270). San Diego, California.

LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., &

Jackel, L. D. (1989). Backpropagation applied to handwritten zip code recognition.

Neural Computation, 1(4), 541–551. doi:10.1162/neco.1989.1.4.541.

Li, F., Zhang, M., Fu, G., & Ji, D. (2017). A neural joint model for entity

and relation extraction from biomedical text. BMC Bioinformatics, 18(1), 1–11.

doi:10.1186/s12859-017-1609-9.

Li, F., Zhang, Y., Zhang, M., & Ji, D. (2016). Joint models for extracting adverse

drug events from biomedical text. In Proceedings of the Twenty-Fifth International

35

Joint Conference on Artiﬁcial Intelligence (pp. 2838–2844). New York, USA: IJ-

CAI/AAAI Press.

Li, Q., & Ji, H. (2014). Incremental joint extraction of entity mentions and relations.

In Proceedings of the 52nd Annual Meeting of the Association for Computational

Linguistics (Volume 1: Long Papers) (pp. 402–412). Baltimore, USA.

Luong, T., Pham, H., & Manning, C. D. (2015). Effective approaches to attention-

based neural machine translation. In Proceedings of the 2015 Conference on Empir-

ical Methods in Natural Language Processing (pp. 1412–1421). Lisbon, Portugal:

Association for Computational Linguistics.

Ma, X., & Hovy, E. (2016). End-to-end sequence labeling via bi-directional LSTM-

CNNs-CRF. In Proceedings of the 54th Annual Meeting of the Association for Com-

putational Linguistics (Volume 1: Long Papers) (pp. 1064–1074). Berlin, Germany.

McDonald, R., & Pereira, F. (2007). Online learning of approximate dependency pars-

ing algorithms. In Proceedings of the 11th Conference of the European Chapter of

the Association for Computational Linguistics (pp. 81–88). Trento, Italy.

McDonald, R., Pereira, F., Ribarov, K., & Hajic, J. (2005). Non-projective depen-

dency parsing using spanning tree algorithms. In Proceedings of Human Language

Technology Conference and Conference on Empirical Methods in Natural Language

Processing (pp. 523–530). Vancouver, British Columbia, Canada: Association for

Computational Linguistics.

Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed

representations of words and phrases and their compositionality. In Proceedings of

the 26th International Conference on Neural Information Processing Systems (pp.

3111–3119). Nevada, United States: Curran Associates, Inc.

Miwa, M., & Bansal, M. (2016). End-to-end relation extraction using LSTMs on se-

quences and tree structures. In Proceedings of the 54th Annual Meeting of the As-

sociation for Computational Linguistics (Volume 1: Long Papers) (pp. 1105–1116).

Berlin, Germany.

36

Miwa, M., & Sasaki, Y. (2014). Modeling joint entity and relation extraction with

table representation. In Proceedings of the 2014 Conference on Empirical Methods

in Natural Language Processing (pp. 1858–1869). Doha, Qatar: Association for

Computational Linguistics.

Nadeau, D., & Sekine, S. (2007). A survey of named entity recognition and classiﬁca-

tion. Lingvisticae Investigationes, 30(1), 3–26. doi:10.1075/li.30.1.03nad.

Nagaraja, C. H., Brown, L. D., & Zhao, L. H. (2011). An autoregressive ap-

proach to house price modeling. The Annals of Applied Statistics, 5(1), 124–149.

doi:10.1214/10-AOAS380.

Nguyen, N., & Guo, Y. (2007). Comparisons of sequence labeling algorithms and ex-

tensions. In Proceedings of the 24th International Conference on Machine Learning

(pp. 681–688). Corvallis, USA: ACM. doi:10.1145/1273496.1273582.

Nivre, J. (2003). An efﬁcient algorithm for projective dependency parsing. In Pro-

ceedings of the 8th International Workshop on Parsing Technologies (pp. 149–160).

Nancy, France.

Nivre, J. (2009). Non-projective dependency parsing in expected linear time. In Pro-

ceedings of the Joint Conference of the 47th Annual Meeting of the Association for

Computational Linguistics and the 4th International Joint Conference on Natural

Language Processing of the Asian Federation of Natural Language Processing (pp.

351–359). Singapore.

Nivre, J., Hall, J., Nilsson, J., Eryiˇgit, G., & Marinov, S. (2006). Labeled pseudo-

projective dependency parsing with support vector machines. In Proceedings of the

10th Conference on Computational Natural Language Learning (pp. 221–225). New

York, USA: Association for Computational Linguistics.

Pace, K., Barry, R., Gilley, O. W., & Sirmans, C. (2000). A method for spatialtem-

poral forecasting with an application to real estate prices. International Journal of

Forecasting, 16(2), 229 – 246. doi:10.1016/S0169-2070(99)00047-3.

37

Pascanu, R., Mikolov, T., & Bengio, Y. (2013). On the difﬁculty of training recurrent

neural networks. In Proceedings of the 30th International Conference on Interna-

tional Conference on Machine Learning (pp. 1310–1318). Atlanta, USA: JMLR.org.

Peng, F., & McCallum, A. (2006). Information extraction from research papers using

conditional random ﬁelds. Information processing & management, 42(4), 963–979.

doi:10.1016/j.ipm.2005.09.002?

Rabiner, L., & Juang, B. (1986). An introduction to hidden markov models. IEEE

ASSP Magazine, 3(1), 4–16. doi:10.1109/MASSP.1986.1165342.

Ratinov, L., & Roth, D. (2009). Design challenges and misconceptions in named en-

tity recognition. In Proceedings of the 13th Conference on Computational Natural

Language Learning (pp. 147–155). Boulder, USA: Association for Computational

Linguistics.

China.

dos Santos, C., Xiang, B., & Zhou, B. (2015). Classifying relations by ranking with

convolutional neural networks. In Proceedings of the 53rd Annual Meeting of the As-

sociation for Computational Linguistics and the 7th International Joint Conference

on Natural Language Processing (Volume 1: Long Papers) (pp. 626–634). Beijing,

Socher, R., Chen, D., Manning, C. D., & Ng, A. (2013). Reasoning with neural

tensor networks for knowledge base completion.

In Proceedings of the 26th In-

ternational Conference on Neural Information Processing Systems (pp. 926–934).

Nevada, United States: Curran Associates, Inc.

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014).

Dropout: A simple way to prevent neural networks from overﬁtting. Journal of

Machine Learning Research, 15(1), 1929–1958.

Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with

neural networks.

In Proceedings of the 27th International Conference on Neural

Information Processing Systems (pp. 3104–3112). Montreal, Canada: MIT Press.

38

Taskar, B., Guestrin, C., & Koller, D. (2003). Max-margin markov networks.

In

Proceedings of the 16th International Conference on Neural Information Processing

Systems (pp. 25–32). Bangkok, Thailand: MIT Press.

Tsochantaridis, I., Hofmann, T., Joachims, T., & Altun, Y. (2004). Support vector

machine learning for interdependent and structured output spaces. In Proceedings

of the 21st International Conference on Machine Learning (pp. 104–112). Helsinki,

Finland: ACM. doi:10.1145/1015330.1015341.

Tutte, W. T. (2001). Graph theory. In Encyclopedia of Mathematics and its Applications

(p. 138). Cambridge University Press volume 21.

Vinyals, O., Fortunato, M., & Jaitly, N. (2015). Pointer networks. In Proceedings of

the 28th International Conference on Neural Information Processing Systems (pp.

2692–2700). Montreal, Canada: Curran Associates, Inc.

Wang, D., & Li, M. (2017).

Stochastic conﬁguration networks: Fundamen-

tals and algorithms.

IEEE Transactions on Cybernetics, 47(10), 3466–3479.

doi:10.1109/TCYB.2017.2734043.

Wang, W., & Chang, B. (2016). Graph-based dependency parsing with bidirectional

lstm. In Proceedings of the 54th Annual Meeting of the Association for Computa-

tional Linguistics (Volume 1: Long Papers) (pp. 2306–2315). Berlin, Germany.

Xu, Y., Mou, L., Li, G., Chen, Y., Peng, H., & Jin, Z. (2015). Classifying relations via

long short term memory networks along shortest dependency paths. In Proceedings

of the 2015 Conference on Empirical Methods in Natural Language Processing (pp.

1785–1794). Lisbon, Portugal: Association for Computational Linguistics.

Yamada, H., & Matsumoto, Y. (2003). Statistical dependency analysis with support

vector machines. In Proceedings of the 8th International Workshop on Parsing Tech-

nologies (pp. 195–206). Nancy, France.

Zhang, H., & McDonald, R. (2012). Generalized higher-order dependency parsing with

cube pruning. In Proceedings of the 2012 Joint Conference on Empirical Methods in

39

Natural Language Processing and Computational Natural Language Learning (pp.

320–331). Jeju Island, Korea: Association for Computational Linguistics.

Zhang, X., Cheng, J., & Lapata, M. (2017). Dependency parsing as head selection.

In Proceedings of the 15th Conference of the European Chapter of the Association

for Computational Linguistics: (Volume 1, Long Papers) (pp. 665–676). Valencia,

Spain.

Zheng, S., Hao, Y., Lu, D., Bao, H., Xu, J., Hao, H., & Xu, B. (2017). Joint entity and

relation extraction based on a hybrid neural network. Neurocomputing, 257, 59 –

66. doi:10.1016/j.neucom.2016.12.075.

40

8
1
0
2
 
r
a

M
 
9
1
 
 
]
L
C
.
s
c
[
 
 
2
v
0
9
5
9
0
.
9
0
7
1
:
v
i
X
r
a

An attentive neural architecture for joint segmentation and
parsing and its application to real estate ads

Giannis Bekoulis∗, Johannes Deleu, Thomas Demeester, Chris Develder

Ghent University – imec, IDLab, Department of Information Technology,
Technologiepark Zwijnaarde 15, 9052 Ghent, Belgium

Abstract

In processing human produced text using natural language processing (NLP) tech-

niques, two fundamental subtasks that arise are (i) segmentation of the plain text into

meaningful subunits (e.g., entities), and (ii) dependency parsing, to establish relations

between subunits. Such structural interpretation of text provides essential building

blocks for upstream expert system tasks: e.g., from interpreting textual real estate ads,

one may want to provide an accurate price estimate and/or provide selection ﬁlters for

end users looking for a particular property — which all could rely on knowing the

types and number of rooms, etc. In this paper we develop a relatively simple and ef-

fective neural joint model that performs both segmentation and dependency parsing

together, instead of one after the other as in most state-of-the-art works. We will fo-

cus in particular on the real estate ad setting, aiming to convert an ad to a structured

description, which we name property tree, comprising the tasks of (1) identifying im-

portant entities of a property (e.g., rooms) from classiﬁeds and (2) structuring them

into a tree format. In this work, we propose a new joint model that is able to tackle

the two tasks simultaneously and construct the property tree by (i) avoiding the error

propagation that would arise from the subtasks one after the other in a pipelined fash-

ion, and (ii) exploiting the interactions between the subtasks. For this purpose, we

perform an extensive comparative study of the pipeline methods and the new proposed

∗Corresponding author
Email addresses: giannis.bekoulis@ugent.be (Giannis Bekoulis),

johannes.deleu@ugent.be (Johannes Deleu), thomas.demeester@ugent.be (Thomas
Demeester), chris.develder@ugent.be (Chris Develder)

Preprint submitted to Expert Systems with Applications

March 20, 2018

joint model, reporting an improvement of over three percentage points in the overall

edge F1 score of the property tree. Also, we propose attention methods, to encourage

our model to focus on salient tokens during the construction of the property tree. Thus

we experimentally demonstrate the usefulness of attentive neural architectures for the

proposed joint model, showcasing a further improvement of two percentage points in

edge F1 score for our application. While the results demonstrated are for the particular

real estate setting, the model is generic in nature, and thus could be equally applied to

other expert system scenarios requiring the general tasks of both (i) detecting entities

(segmentation) and (ii) establishing relations among them (dependency parsing).

Keywords: neural networks, joint model, relation extraction, entity recognition,

dependency parsing

1. Introduction

Many consumer-oriented digital applications rely on input data provided by their tar-

get audience. For instance, real estate websites gather property descriptions for the

offered classiﬁeds, either from realtors or from individual sellers.

In such cases, it

is hard to strike the right balance between structured and unstructured information:

enforcing restrictions or structure upon the data format (i.e., predeﬁned form) may re-

duce the amount or diversity of the data, while unstructured data (i.e., raw text) may

require non-trivial (i.e., hard to automate) transformation to a more structured form to

be useful/practical for the intended application. In the real estate domain, textual ad-

vertisements are an extremely useful but highly unstructured way of representing real

estate properties. However, structured descriptions of the advertisements are very help-

ful, e.g., for real estate agencies to suggest the most appropriate sales/rentals for their

customers, while keeping human reading effort limited. For example, special search

ﬁlters, which are usually used by clients, cannot be directly applied to textual advertise-

ments. On the contrary, a structured representation of the property (e.g., a tree format

of the property) enables the simpliﬁcation of the unstructured textual information by

applying speciﬁc ﬁlters (e.g., based on the number of bedrooms, number of ﬂoors, or

the requirement of having a bathroom with a toilet on the ﬁrst ﬂoor), and it also ben-

2

eﬁts other related applications such as automated price prediction (Pace et al., 2000;

Nagaraja et al., 2011).

The new real estate structured prediction problem as deﬁned by Bekoulis et al.

(2017) has as main goal to construct the tree-like representation of the property (i.e.,

the property tree) based on its natural language description. This can be approached as

a relation extraction task by a pipeline of separate subtasks, comprising (i) named entity

recognition (NER) (Nadeau & Sekine, 2007) and (ii) relation extraction (Bach & Badaskar,

2007). Unlike previous studies (Li & Ji, 2014; Miwa & Bansal, 2016) on relation ex-

traction, in the work of Bekoulis et al. (2017), the relation extraction module is re-

placed by dependency parsing. Indeed, the relations that together deﬁne the structure

of the house should form a tree, where entities are part-of one another (e.g., a ﬂoor

is part-of a house, a room is part-of a ﬂoor). This property tree is structurally simi-

lar to a parse tree. Although the work of Bekoulis et al. (2017) is a step towards the

construction of the property tree, it follows a pipeline setting, which suffers from two

serious problems: (i) error propagation between the subtasks, i.e., NER and depen-

dency parsing, and (ii) cross-task dependencies are not taken into account, e.g., terms

indicating relations (includes, contains, etc.) between entities that can help the NER

module are neglected. Due to the unidirectional nature of stacking the two modules

(i.e., NER and dependency parsing) in the pipeline model, there is no information ﬂow-

ing from the dependency parsing to the NER subtask. This way, the parser is not able

to inﬂuence the predictions of the NER. Other studies on similar tasks (Li & Ji, 2014;

Kate & Mooney, 2010) have considered the two subtasks jointly. They simultaneously

extract entity mentions and relations between them usually by implementing a beam-

search on top of the ﬁrst module (i.e., NER), but these methods require the manual

extraction of hand-crafted features. Recently, deep learning with neural networks has

received much attention and several approaches (Miwa & Bansal, 2016; Zheng et al.,

2017) apply long short-term memory (LSTM) recurrent neural networks and convo-

lutional neural networks (CNNs) to achieve state-of-the-art performance on similar

problems. Those models rely on shared parameters between the NER and relation ex-

traction components, whereby the NER module is typically pre-trained separately, to

improve the training effectiveness of the joint model.

3

In this work, we propose a new joint model to solve the real estate structured pre-

diction problem. Our model is able to learn the structured prediction task without

complicated feature engineering. Whereas previous studies (Miwa & Bansal, 2016;

Zheng et al., 2017; Li et al., 2016, 2017) on joint methods focus on the relation extrac-

tion problem, we construct the property tree which comes down to solving a depen-

dency parsing problem, which is more constrained and hence more difﬁcult. Therefore,

previous methods are not directly comparable to our model and cannot be applied to

our real estate task out-of-the-box. In this work, we treat the two subtasks as one by

reformulating them into a head selection problem (Zhang et al., 2017).

This paper is a follow-up work of Bekoulis et al. (2017). Compared to the con-

ference paper that introduced the real estate extraction task and applied some basic

state-of-the-art techniques as a ﬁrst baseline solution, we now introduce: (i) advanced

neural models that consider the two subtasks jointly and (ii) modiﬁcations to the dataset

annotation representations as detailed below. More speciﬁcally, the main contributions

of this work are the following:

• We propose a new joint model that encodes the two tasks of identifying enti-

ties as well as dependencies between them, as a single head selection problem,

without the need of parameter sharing or pre-training of the ﬁrst entity recogni-

tion module separately. Moreover, instead of (i) predicting unlabeled dependen-

cies and (ii) training an additional classiﬁer to predict labels for the identiﬁed

heads (Zhang et al., 2017), our model already incorporates the dependency label

predictions in its scoring formula.

• We compare the proposed joint model against established pipeline approaches

and report an F1 improvement of 1.4% in the NER and 6.2% in the dependency

parsing subtask, corresponding to an overall edge F1 improvement of 3.4% in

the property tree.

• Compared to our original dataset (Bekoulis et al., 2017), we introduce two ex-

tensions to the data: (i) we consistently assign the ﬁrst mention of a particular

entity in order of appearance in the advertisement as the main mention of the

entity. This results in an F1 score increase of about 3% and 4% for the joint and

4

pipeline models, respectively. (ii) We add the equivalent relation to our anno-

tated dataset to explicitly express that several mentions across the ad may refer

to the same entity.

• We perform extensive analysis of several attention mechanisms that enable our

LSTM-based model to focus on informative words and phrases, reporting an

improved F1 performance of about 2.1%.

The rest of the paper is structured as follows. In Section 2, we review the related

work. Section 3 deﬁnes the problem and in Section 4, we describe the methodology

followed throughout the paper and the proposed joint model. The experimental results

are reported in Section 5. Finally, Section 6 concludes our work.

2. Related work

The real estate structured prediction problem from textual advertisements can be bro-

ken down into the sub-problems of (i) sequence labeling (identifying the core parts

of the property) and (ii) non-projective dependency parsing (connecting the identi-

ﬁed parts into a tree-like structure) (Bekoulis et al., 2017). One can address these two

steps either one by one in a pipelined approach, or simultaneously in a joint model.

The pipeline approach is the most commonly used approach (Bekoulis et al., 2017;

Fundel et al., 2007; Gurulingappa et al., 2012), treating the two steps independently

and propagating the output of the sequence labeling subtask (e.g., named entity recog-

nition) (Chiu & Nichols, 2016; Lample et al., 2016) to the relation classiﬁcation mod-

ule (dos Santos et al., 2015; Xu et al., 2015). Joint models are able to simultaneously

extract entity mentions and relations between them (Li & Ji, 2014; Miwa & Bansal,

2016). In this work, we propose a new joint model that is able to recover the tree-

like structure of the property and frame it as a dependency parsing problem, given the

non-projective tree structure we aim to output. We now present related works for the

sequence labeling and dependency parsing subtasks, as well as for the joint models.

5

2.1. Sequence labeling

Structured prediction problems become challenging due to the large output space.

Speciﬁcally in NLP, sequence labeling (e.g., NER) is the task of identifying the en-

tity mention boundaries and assigning a categorical label (e.g., POS tags) for each

identiﬁed entity in the sentence. A number of different methods have been proposed,

namely Hidden Markov Models (HMMs) (Rabiner & Juang, 1986), Conditional Ran-
dom Fields (CRFs) (Lafferty et al., 2001), Maximum Margin Markov Network (M3N)

(Taskar et al., 2003), generalized support vector machines for structured output (SVMstruct)

(Tsochantaridis et al., 2004) and Search-based Structured Prediction (SEARN) (Daum´e III et al.,

2009). Those methods heavily rely on hand-crafted features and an in-depth review can

be found in Nguyen & Guo (2007). Several variations of these models that also require

manual feature engineering have been used in different application settings (e.g., biol-

ogy, social media context) and languages (e.g., Turkish) (Jung, 2012; K¨uc¸ ¨uk & Yazıcı,

2012; Atkinson & Bull, 2012; Konkol et al., 2015). Recently, deep learning with neu-

ral networks has been succesfully applied to NER. Collobert et al. (2011) proposed

to use a convolutional neural network (CNN) followed by a CRF layer over a se-

quence of word embeddings. Recurrent Neural Networks (RNNs) constitute another

neural network architecture that has attracted attention, due to the state-of-the-art per-

formance in a series of NLP tasks (e.g., sequence-to-sequence (Sutskever et al., 2014),

parsing (Kiperwasser & Goldberg, 2016)). In this context, Gillick et al. (2016) use a

sequence-to-sequence approach for modeling the sequence labeling task. In addition,

several variants of combinations between LSTM and CRF models have been proposed

(Lample et al., 2016; Huang et al., 2015; Ma & Hovy, 2016) achieving state-of-the-art

performance on publicly available datasets.

2.2. Dependency parsing

Dependency parsing is a well studied task in the NLP community, which aims to ana-

lyze the grammatical structure of a sentence. We approach the problem of the property

tree construction as a dependency parsing task i.e., to learn the dependency arcs of

the classiﬁed. There are two well-established ways to address the dependency parsing

problem, via graph-based and transition-based parsers.

6

Graph-based: In the work of McDonald et al. (2005); McDonald & Pereira (2007)

dependency parsing requires the search of the highest scoring maximum spanning

tree in graphs for both projective (dependencies are not allowed to cross) and non-

projective (crossing dependencies are allowed) trees with the Eisner algorithm (Eisner,

1996) and the Chu-Liu-Edmonds algorithm (Chu & Liu, 1965; Edmonds, 1967) re-

spectively. It was shown that exploiting higher-order information (e.g., siblings, grand-

parental relation) in the graph, instead of just using ﬁrst-order information (i.e., par-

ent relations) (Carreras, 2007; Zhang & McDonald, 2012) may yield signiﬁcant im-

provements of the parsing accuracy but comes at the cost of an increased model com-

plexity. Koo et al. (2007) made an important step towards globally normalized mod-

els with hand-crafted features, by adapting the Matrix-Tree Theorem (MTT) (Tutte,

2001) to train over all non-projective dependency trees. We explore an MTT approach

as one of the pipeline baselines. Similar to recent advances in neural graph-based

parsing (Zhang et al., 2017; Kiperwasser & Goldberg, 2016; Wang & Chang, 2016),

we use LSTMs to capture richer contextual information compared to hand-crafted fea-

ture based methods. Our work is conceptually related to Zhang et al. (2017), who

formulated the dependency parsing problem as a head selection problem. We go a step

further in that direction, in formulating the joint parsing and labeling problem in terms

of selecting the most likely combination of head and label.

Transition-based: Transition-based parsers (Yamada & Matsumoto, 2003; Nivre et al.,

2006) replace the exact inference of the graph-based parsers by an approximate but

faster inference method. The dependency parsing problem is now solved by an ab-

stract state machine that gradually builds up the dependency tree token by token. The

goal of this kind of parsers is to ﬁnd the most probable transition sequence from an

initial to some terminal conﬁguration (i.e., a dependency parse tree, or in our case

a property tree) given a permissible set of actions (i.e., LEFT-ARC, RIGHT-ARC,

SHIFT) and they are able to handle both projective and non-projective dependen-

cies (Nivre, 2003, 2009). In the simplest case (i.e., greedy inference), a classiﬁer pre-

dicts the next transition based on the current conﬁguration. Compared to graph-based

dependency parsers, transition-based parsers are able to scale better due to the linear
time complexity while graph-based complexity rises to O(n2) in the non-projective

7

case. Chen & Manning (2014) proposed a way of learning a neural network classiﬁer

for use in a greedy, transition-based dependency parser while using low-dimensional,

dense word embeddings, without the need of manually extracting features. Globally

normalized transition-based parsers (Andor et al., 2016) can be considered an exten-

sion of Chen & Manning (2014), as they perform beam search for maintaining multiple

hypotheses and introduce global normalization with a CRF objective. Dyer et al. (2015)

introduced the stack-LSTM model with push and pop operations which is able to learn

the parser transition states while maintaining a summary embedding of its contents.

Although transition-based systems are well-known for their speed and state-of-the-art

performance, we do not include them in our study due to their already reported poor

performance in the real estate task (Bekoulis et al., 2017) compared to graph-based

parsers.

2.3. Joint learning

Adopting a pipeline strategy for the considered type of problems has two main draw-

backs: (i) sequence labeling errors propagate to the dependency parsing step, e.g., an

incorrectly identiﬁed part of the house (entity) could get connected to a truly existing

entity, and (ii) interactions between the components are not taken into account (feed-

back between the subtasks), e.g., modeling the relation between two potential entities

may help in deciding on the nature of the entities themselves. In more general rela-

tion extraction settings, a substantial amount of work (Kate & Mooney, 2010; Li & Ji,

2014; Miwa & Sasaki, 2014) jointly considered the two subtasks of entity recognition

and relation extraction. However, all of these models make use of hand-crafted features

that: (i) require manual feature engineering, (ii) generalize poorly between various ap-

plications and (iii) may require a substantial computational cost.

Recent advances on joint models for general relation extraction consider the joint

task using neural network architectures like LSTMs and CNNs (Miwa & Bansal, 2016;

Zheng et al., 2017; Li et al., 2017). Our work is however different from a typical rela-

tion extraction setup in that we aim to model directed spanning trees, or, equivalently,

non-projective dependency structures. In particular, the entities involved in a relation

are not necessarily adjacent in the text since other entities may be mentioned in be-

8

Entity type

Description

Examples

property
ﬂoor
space
subspace
ﬁeld

extra building

The property.
A ﬂoor in a building.
A room within the building.
A part of a room.
An open space inside or outside
the building.
An additional building which is
also part of the property.

bungalow, apartment
ground ﬂoor
bedroom, bathroom
shower, toilet
bbq, garden

garden house

Table 1: Real estate entity types.

tween, which complicates parsing. Indeed, in this work we focus on dependency pars-

ing due to the difﬁculty of establishing the tree-like structure instead of only relation

extraction (where each entity can have arbitrary relation arcs, regardless of other enti-

ties and their relations), which is the case for previously cited joint models. Moreover,

unlike most of these works that frame the problem as a stacking of the two components,

or at least ﬁrst train the NER module to recognize the entities and then further train to-

gether with the relation classiﬁcation module, we include the NER directly inside the

dependency parsing component.

In summary, the conceptual strengths of our joint segmentation and dependency

parsing approach (described in detail in Section 4) will be the following: compared to

state-of-the-art joint models in relation extraction, it (i) is generic in nature, without

requiring any manual feature engineering, (ii) extracts a complete tree structure rather

than a single binary relation instance.

3. Problem deﬁnition

In this section, we deﬁne the speciﬁc terms that are used in our real estate structured

prediction problem. We deﬁne an entity as an unambiguous, unique part of a property

with independent existence (e.g., bedroom, kitchen, attic). An entity mention is de-

ﬁned as one or more sequential tokens (e.g., “large apartment”) that can be potentially

linked to one or more entities. An entity mention has a unique semantic meaning and

refers to a speciﬁc entity, or a set of similar entities (e.g., “six bedrooms”). An entity

9

O r i g i n a l ad :
The p r o p e r t y i n c l u d e s a l a r g e
a p a r t m e n t w i t h a g a r a g e . The
home h a s a l i v i n g room , 3 s p a c i o u s b ed r o o m s and a b a t h r o o m .
The g a r a g e
e q u i p p e d w i t h a g a t e and a b i k e w a l l b r a c k e t .
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
S t r u c t u r e d r e p r e s e n t a t i o n :
p r o p e r t y
l a r g e

a p a r t m e n t

i s

l i v i n g room
3 s p a c i o u s b ed r o o m s
b a t h r o o m

g a r a g e

g a t e
b i k e w a l l b r a c k e t

‘ home ’

a p a r t m e n t ’ ,

| m e n t i o n = ‘ p r o p e r t y ’
| m e n t i o n = ‘ l a r g e
| m e n t i o n = ‘ l i v i n g room ’
| m e n t i o n = ‘3 s p a c i o u s bedrooms ’
| m e n t i o n = ‘ b ath r o o m ’
| m e n t i o n = ‘ g a r a g e ’
| m e n t i o n = ‘ g a t e ’
| m e n t i o n = ‘ b i k e w a l l b r a c k e t ’

Figure 1: Fictitious sample unstructured ad and corresponding structured representation as a property tree.

itself is part-of another entity and can be mentioned in the text more than once with

different entity mentions. For instance, a “house” entity could occur in the text with

entity mentions “large villa” and “a newly built house”. For the pipeline setting as pre-

sented in Bekoulis et al. (2017), we further classify entities into types (assign a named

entity type to every word in the ad). The task is transformed to a sequence labeling

problem using BIO (Beginning, Inside, Outside) encoding. The entity types are listed

in Table 1. For instance, in the sequence of tokens “large apartment”, B-PROPERTY is

assigned to the token “large” as the beginning of the entity, I-PROPERTY in the token

“apartment” as the inside of the entity but not the ﬁrst token within the entity and O

for all the other tokens that are not entities. Unlike previous studies (Miwa & Bansal,

2016; Zheng et al., 2017; Li et al., 2016, 2017), for our joint model there is no need for

this type of categorical classiﬁcation into labels since the two components are treated

uniﬁed as a single dependency parsing problem.

The goal of the real estate structured prediction task is to map the textual prop-

erty classiﬁed into a tree-like structured representation, the so-called property tree, as

illustrated in Fig. 1. In the pipeline setting, this conversion implies the detection of

(i) entities of various types and (ii) the part-of dependencies between them. For in-

stance, the entity “living room” is part-of the entity “large apartment”. In the joint

model, each token (e.g., “apartment”, “living”, “bathroom”, “includes”, “with”, “3”) is

examined separately and 4 different types of relations are deﬁned, namely part-of, seg-

10

ment, skip and equivalent. The part-of relation is similar to the way that it was deﬁned

in the pipeline setting but instead of examining entities, i.e., sequences of tokens (e.g.,

“living room”), we examine if a (individual) token is part-of another (individual) token

(e.g., “room” is part-of the “apartment”). We encode the entity identiﬁcation task with

the segment label and we follow the same approach as in the part-of relationships for

the joint model. Speciﬁcally, we examine if a token is a segment of another token (e.g.,

the token “room” is attached as a segment to the token “living”, “3” is attached as a seg-

ment to the token “bedrooms” and “spacious” is also attached as a segment to the token

“bedrooms” — this way we are able to encode the segment “3 spacious bedrooms”). By

doing so, we cast the sequence labeling subtask to a dependency parsing problem. The

tokens that are referring to the same entity belong to the equivalent relation (“home”

is equivalent to “apartment”). For each entity, we deﬁne the ﬁrst mention in order of

appearance in the text as main mention and the rest as equivalent to this main mention.

Finally, each token that does not have any of the aforementioned types of relations has

a skip relation with itself (e.g., “includes” has a skip relation with “includes”), such that

each token has a uniquely deﬁned head.

Thus, we cast the structured prediction task of extracting the property tree from the

ad as a dependency parsing problem, where (i) an entity can be part-of only one (other)

entity, because the decisions are taken simultaneously for all part-of relations (e.g., a

certain room can only be part-of a single ﬂoor), and (ii) there are a priori no restrictions

on the type of entities or tokens that can be part-of others (e.g., a room can be either

part-of a ﬂoor, or the property itself, like an apartment). It is worth mentioning that

dependency annotations for our problem exhibit a signiﬁcant number of non-projective

arcs (26%) where part-of dependencies are allowed to cross (see Fig. 3), meaning that

entities involved in the part-of relation are non-adjacent (i.e., interleaved by other en-

tities). For instance, all the entities or the tokens for the pipeline and the joint models,

that are attached to the entity “garage” are overlapping with the entities that are at-

tached to the entity “apartment”, making parsing even more complicated: handling

only projective dependencies as illustrated in Fig. 2 is an easier task. We note that the

segment dependencies do not suffer from non-projectivity, since the tokens are always

adjacent and sequential (e.g., “3 spacious bedrooms”).

11

0
property

1
large
apartment

2
living
room

3
3 spacious
bedrooms

4
bathroom

Figure 2: An example graph of projective part-of dependencies.

0
property

1
large
apartment

2
garage

3
living 
room

4
3 spacious
bedrooms

5
bathroom

6
bike wall
bracket

7
gate

Figure 3: Graph representing the part-of dependencies of Fig. 1. The dashed arcs are representing the
non-projective dependencies.

4. Methodology

We now describe the two approaches, i.e., the pipeline model and the joint model to

construct the property tree of the textual advertisements, as illustrated in Fig. 4. For

the pipeline system (Section 4.1), we (1) identify the entity mentions (Section 4.1.1),

then (2) predict the part-of dependencies between them (Section 4.1.2), and ﬁnally (3) con-

struct the tree representation (i.e., property tree) of the textual classiﬁed (e.g., as in

Fig. 1).

In step (2), we apply locally or globally trained graph-based models. We

represent the result of step (2) as a graph model, and then solve step (3) by apply-

ing the maximum spanning tree algorithm (Chu & Liu, 1965; Edmonds, 1967) for

the directed case (see McDonald et al. (2005)). We do not apply the well-known and

fast transition-based systems with hand-crafted features for non-projective dependency

structures (Nivre, 2009; Bohnet & Nivre, 2012), given the previously established poor

performance thereof in Bekoulis et al. (2017). In Section 4.2, we describe the joint

model where we perform steps (1) and (2) jointly. For step (3), we apply the maximum

spanning tree algorithm (Chu & Liu, 1965; Edmonds, 1967) similarly as in the pipeline

setting (Section 4.1).

4.1. Two-step pipeline

Below we revisit the pipeline approach presented in Bekoulis et al. (2017), which serves

as the baseline which we compare the neural models against. As mentioned before, the

pipeline model comprises two subtasks: (1) the sequence labeling and the (2) part-of

12

tree construction. In the following subsections, we describe the methods applied for

both.

4.1.1. Sequence labeling

The ﬁrst step in our pipeline approach is the sequence labeling subtask which is similar

to NER. Assuming a textual real estate classiﬁed, we (i) identify the entity mention

boundaries and (ii) map each identiﬁed entity mention to a categorical label, i.e., entity

type. In general, in the sequence labeling tasks, it is beneﬁcial to take into account

the correlations between labels in adjacent tokens, i.e., consider the neighborhood, and

jointly ﬁnd the most probable chain of labels for the given input sentence (Viterbi

algorithm for the most probable assignment). For instance, in our problem where we

follow the NER standard BIO encoding (Ratinov & Roth, 2009), the I-PROPERTY

cannot be followed by I-SPACE without ﬁrst opening the type by B-SPACE. We use

a special case of the CRF algorithm (Lafferty et al., 2001; Peng & McCallum, 2006),

namely linear chain CRFs, which is commonly applied in the problem of sequence

labeling to learn a direct mapping from the feature space to the output space (types)

where we model label sequences jointly, instead of decoding each label independently.

A linear-chain CRF with parameters w deﬁnes a conditional probability Pw(y|x) for

the sequence of labels y = y1, ..., yN given the tokens of the text advertisement x =

x1, ..., xN to be

Pw(y|x) =

exp(wT φ(x, y)),

1
Z(x)

(1)

where Z is the normalization constant and φ is the feature function that computes a

feature vector given the advertisement and the sequence of labels.

4.1.2. Part-of tree construction

The aim of the part-of tree construction subtask is to link each entity to its parent.

We approach the task as a dependency parsing problem but instead of connecting each

token to its syntactical parent, we map only the entity set I (e.g., “large villa”, “3

spacious bedrooms”) that has already been extracted by the sequence labeling subtask

to a dependency structure y. Assuming the entity set I = {e0, e1, ..., et} where t is the

number of identiﬁed entities, a dependency is a pair (p, c) where p ∈ I is the parent

13

two-step pipeline

(1) entity
recognition

(2) part-of RE

(1+2) joint entity recognition
& part-of RE

.
.
.

(3) tree
construction

joint model

Figure 4: The full structured prediction system setup.

entity and c ∈ I is the child entity. The entity e0 is the dummy root-symbol that only

appears as parent.

We will compare two approaches to predict the part-of relations: a locally trained

model (LTM) scoring all candidate edges independently, versus a global model (MTT)

which jointly scores all edges as a whole.

Locally trained model (LTM)

In the locally trained model (LTM), we adopt a traditional local discriminative method

and apply a binary classiﬁcation framework (Yamada & Matsumoto, 2003) to learn the

part-of relation model (step (2)), based on standard relation extraction features such

as the parent and child tokens and their types, the tokens in between, etc. For each

candidate parent-child pair, the classiﬁer gives a score that indicates whether it is prob-

able for the part-of relation to hold between them. The output scores are then used

for step (3), to construct the ﬁnal property tree. Following McDonald et al. (2005);

McDonald & Pereira (2007), we view the entity set I as a fully connected directed

graph G = {V, E} with the entities e1, ..., et as vertices (V ) in the graph G, and edges

E representing the part-of relations with the respective classiﬁer scores as weights.

One way to approach the problem is the greedy inference method where the predic-

tions are made independently for each parent-child pair, thus neglecting that the global

target output should form a property tree. We could adopt a threshold-based approach,

i.e., keep all edges exceeding a threshold, which obviously is not guaranteed to end

up with arc dependencies that form a tree structure (i.e., could even contain cycles).

On the other hand, we can enforce the tree structure inside the (directed) graph by

14

ﬁnding the maximum spanning tree. To this end, similar to McDonald et al. (2005);

McDonald & Pereira (2007), we apply the Edmonds’ algorithm to search for the most

probable non-projective tree structure in the weighted fully connected graph G.

Globally trained model (MTT)

The Matrix-Tree theorem (MTT) (Koo et al., 2007) is a globally normalized statistical

method that involves the learning of directed spanning trees. Unlike the locally trained

models, MTT is able to learn tree dependency structures, i.e., scoring parse trees for

a given sentence. We use D(I) to refer to all possible dependencies of the identiﬁed

entity set I, in which each dependency is represented as a tuple (h, m) in which h is

the head (or parent) and m the modiﬁer (or child). The set of all possible dependency

structures for a given entity set I is written T (I). The conditional distribution over all

dependency structures y ∈ T (I) can then be deﬁned as:

P (y|I; θ) =

1
Z(I; θ)

exp 


X
h,m∈y

θh,m





(2)

in which the coefﬁcients θh,m ∈ R for each dependency (h, m) form the real-valued

weight vector θ. The partition function Z(I; θ) is a normalization factor that alas can-

not be computed by brute-force, since it requires a summation over all y ∈ T (I),

containing an exponential number of possible dependency structures. However, an

adaptation of the MTT allows us the direct and efﬁcient computation of the partition

function Z(I; θ) as the determinant det(L(θ)) where L(θ) is the Laplacian matrix of

the graph. It is worth mentioning that although MTT learns spanning tree structures

during training, at the prediction phase, it is still required to use the maximum spanning

tree algorithm (step (3)) (McDonald et al., 2005; McDonald & Pereira, 2007) as in the

locally trained models.

4.2. Joint model

In this section, we present the new joint model sketched in Fig. 5, which simultaneously

predicts the entities in the sentence and the dependencies between them, with the ﬁnal

goal of obtaining a tree structure, i.e., the property tree. We pose the problem of the

identiﬁcation of the entity mentions and the dependency arcs between them as a head

15

Figure 5: The architecture of the joint model.

selection problem (Zhang et al., 2017). Speciﬁcally, given as input a sentence of length

N , the model outputs the predicted parent of each token of the advertisement and the

most likely dependency label between them. We begin by describing how the tokens

are represented in the model, i.e., with ﬁxed pre-trained embeddings (Section 4.2.1),

which form the input to an LSTM layer (Section 4.2.2). The LSTM outputs are used

as input to the entity and dependency scoring layer (Section 4.2.3). As an extension of

this model, we propose the use of various attention layers in between the LSTM and

scoring layer, to encourage the model to focus on salient information, as described in

Section 4.2.4. The ﬁnal output of the joint model still is not guaranteed to form a tree

structure. Therefore, we still apply Edmonds’ algorithm (i.e., step (3) from the pipeline

approach), described in Section 4.2.5.

4.2.1. Embedding Layer

The embedding layer maps each token of the input sequence x1, ..., xN of the consid-

ered advertisement to a low-dimensional vector space. We obtain the word-level em-

beddings by training the Skip-Gram word2vec model (Mikolov et al., 2013) on a large

collection of property advertisements. We add a symbol x0 in front of the N -length

input sequence, which will act as the root of the property tree, and is represented with

16

an all-zeros vector in the embedding layer.

4.2.2. Bidirectional LSTM encoding layer

Many neural network architectures have been proposed in literature: LSTMs (Hochreiter & Schmidhuber,

1997), CNNs (LeCun et al., 1989), Echo State Networks (Jaeger, 2010), or Stochastic

Conﬁguration Networks (Wang & Li, 2017), to name only a few. Many others can

be found in reference works on the topic (Goodfellow et al., 2016; Goldberg & Hirst,

2017). In this work, we use RNNs which have been proven to be particularly effective

in a number of NLP tasks (Sutskever et al., 2014; Lample et al., 2016; Miwa & Bansal,

2016). Indeed, RNNs are a common and reasonable choice to model sequential data

and inherently able to cope with varying sequence lengths. Yet, plain vanilla RNNs

tend to suffer from vanishing/exploding gradient problems and are hence not success-

ful in capturing long-term dependencies (Bengio et al., 1994; Pascanu et al., 2013).

LSTMs are a more advanced kind of RNNs, which have been successfully applied

in several tasks to capture long-term dependencies, as they are able to effectively over-

come the vanishing gradient problem. For many NLP tasks, it is crucial to represent

each word in its own context, i.e., to consider both past (left) and future (right) neigh-

boring information. An effective solution to achieve this is using a bidirectional LSTM

(BiLSTM). The basic idea is to encode each sequence from left to right (forward) and

from right to left (backward). This way, there is one hidden state which represents the

past information and another one for the future information. The high-level formulation

of an LSTM is:

hi, ci = LSTM(wi, hi−1, ci−1),

i = 0, ..., N

(3)

where in our setup wi ∈ R ˜d is the word embedding for token xi, and with the input and
states for the root symbol x0 initialized as zero vectors. Further, hi ∈ Rd and ci ∈ Rd

respectively are the output and cell state for the ith position, where d is the hidden state

size of the LSTM. Note that we chose the word embedding size the same as the LSTM
hidden state size, or ˜d = d. The outputs from left to right (forward) are written as
~hi and the outputs from the backwards direction as ~hi. The two LSTMs’ outputs at

17

position i are concatenated to form the output hi at that position of the BiLSTM:

hi = [ ~hi; ~hi],

i = 0, ..., N

(4)

4.2.3. Joint learning as head selection

In this subsection, we describe the joint learning task (i.e., identifying entities and

predicting dependencies between them), which we formulate as a head selection prob-

lem (Zhang et al., 2017). Indeed, each word xi should have a unique head (parent)

— while it can have multiple dependent words — since the ﬁnal output should form

the property tree. Unlike the standard head selection dependency parsing framework

(Zhang et al., 2017), we predict the head yi of each word xi and the relation ci between

them jointly, instead of ﬁrst obtaining binary predictions for unlabeled dependencies,

followed by an additional classiﬁer to predict the labels.

Given a text advertisement as a token sequence x = x0, x1, ..., xN where x0 is the

dummy root symbol, and a set C = {part-of, segment, equivalent, skip} of predeﬁned

labels (as deﬁned in Section 3), we aim to ﬁnd for each token xi, i ∈ {0, ..., N } the

most probable head xj , j ∈ {0, ..., N } and the most probable corresponding label c ∈

C. For convenience, we order the labels c ∈ C and identify them as ck, k ∈ {0, ..., 3}.

We model the joint probability of token xj to be the head of xi with ck the relation

between them, using a softmax:

P (head = xj, label = ck|xi) =

exp(score(hj , hi, ck))
˜j,˜k exp(score(h˜j , hi, c˜k)

P

(5)

where hi and hj are the BiLSTM encodings for words xi and xj, respectively. For

the scoring formula score(hj , hi, ck) we use a neural network layer that computes the

relative score between position i and j for a speciﬁc label ck as follows:

score(hj , hi, ck) = V T

k tanh(Ukhj + Wkhi + bk)

(6)

with trainable parameters Vk ∈ Rl, Uk ∈ Rl×2d, Wk ∈ Rl×2d, bk ∈ Rl, and l the

layer width. As detailed in Section 5.1, we set l to be smaller than 2d, similar to

Dozat & Manning (2017) due to the fact that training on superﬂuous information re-

duces the parsing speed and increases tendency towards overﬁtting. We train our model

18

by minimizing the cross-entropy loss L, written for the considered training instance as:

L =

− log P (head = yi, label = ci|xi)

(7)

N

X
i=0

where yi ∈ x and ci ∈ C are the ground truth head and label of xi, respectively. After

training, we follow a greedy inference approach and for each token, we simultaneously

keep the highest scoring head ˆyi and label ˆci for xi based on their estimated joint

probability:

( ˆyi, ˆci) = argmax
xj ∈x,ck∈C

P (head = xj, label = ck|xi)

(8)

The predictions ( ˆyi, ˆci) are made independently for each position i, neglecting that

the ﬁnal structure should be a tree. Nonetheless, as demonstrated in Section 5.2, the

highest scoring neural models are still able to come up with a tree structure for 78%

of the ads. In order to ensure a tree output in all cases, however, we apply Edmonds’

algorithm on the output.

4.2.4. Attention Layer

The attention mechanism in our structured prediction problem aims to improve the

model performance by focusing on information that is relevant to the prediction of the

most probable head for each token. As attention vector, we construct the new context
vector h∗

i as a weighted average of the BiLSTM outputs

in which the coefﬁcients a(hj, hi), also called the attention weights, are obtained as

h∗
j =

N

X
i=0

a(hj, hi) hi

a(hj, hi) =

exp(att(hj, hi))
N
˜i=0 exp(att(hj , h˜i))

.

P

(9)

(10)

The attention function att(hj, hi) is designed to measure some form of compatibility

between the representation hi for xi and hj for xj, and the attention weights a(hj, hi)

are obtained from these scores by normalization using a softmax function. In the fol-

lowing, we will describe in detail the various attention models that we tested with our

follows:

joint model.

19

Commonly used attention mechanisms

Three commonly used attention mechanisms are listed in eqs. (11) to (13): the addi-

tive (Vinyals et al., 2015), bilinear, and multiplicative attention models (Luong et al.,

2015), which have been extensively used in machine translation. Given the represen-

tations hi and hj for tokens xi and xj, we compute the attention scores as follows:

attadditive(hj, hi) = Va tanh(Uahj + Wahi + ba)

attbilinear(hj, hi) = hT

j Wbilhi

attmultiplicative(hj, hi) = hT

j hi

where Va ∈ Rl, Ua, Wa ∈ Rl×2d, Wbil ∈ R2d×2d and ba ∈ Rl are learnable parame-

ters of the model.

Biafﬁne attention

We use the biafﬁne attention model (Dozat & Manning, 2017) which has been recently

applied to dependency parsing and is a modiﬁcation of the neural graph-based approach

that was proposed by Kiperwasser & Goldberg (2016). In this model, Dozat & Manning

(2017) tried to reduce the dimensionality of the recurrent state of the LSTMs by apply-

ing a such neural network layer on top of them. This idea is based on the fact that there

is redundant information in every hidden state that (i) reduces parsing speed and (ii) in-

creases the risk of overﬁtting. To address these issues, they reduce the dimensionality

and apply a nonlinearity afterwards. The deep bilinear attention mechanism is deﬁned

as follows:

(11)

(12)

(13)

(14)

(15)

hdep
i = Vdep tanh(Udephi + bdep)

hhead
j = Vhead tanh(Uheadhj + bhead)

20

function:

Edge attention

to be:

attbiafﬁne(hhead

j

, hdep

i ) = (hhead

j

)T Wbilhdep

i + Bhhead

j

(16)

where Udep, Uhead ∈ Rl×2d, Vdep, Vhead ∈ Rp×l, Wbil ∈ Rp×p, B ∈ Rp and bdep,
bhead ∈ Rl.

Tensor attention

This section introduces the Neural Tensor Network (Socher et al., 2013) that has been

used as a scoring formula applied for relation classiﬁcation between entities. The task

can be described as link prediction between entities in an existing network of relation-

ships. We apply the tensor scoring formula as if tokens are entities, by the following

atttensor(hj, hi) = Ut tanh(hT

j Wthi + Vt(hj + hi) + bt)

(17)

where Wt ∈ R2d×l×2d, Vt ∈ Rl×2d, Ut ∈ Rl and bt ∈ Rl.

In the edge attention model, we are inspired by Gilmer et al. (2017), which applies

neural message passing in chemical structures. Assuming that words are nodes inside

the graph and the message ﬂows from node xi to xj, we deﬁne the edge representation

(18)

(19)

edge(hj, hi) = tanh(Uehj + Wehi + be)

The edge attention formula is computed as:

attedge(hj, hi) =

Asrc

edge(hj, h˜i) + Adst

1
N





N

X
˜i=0

N

X
˜j=0

edge(h˜j, hi)


where Ue, We ∈ Rl×2d, Asrc, Adst ∈ R2d×l and be ∈ Rl. The source and destination

matrices respectively encode information for the start to the end node, in the directed

edge. Running the edge attention model for several times can be achieved by stacking

the edge attention layer multiple times. This is known as message passing phase and

we can run it for several (T > 1) time steps to obtain more informative edge represen-

tations.

21

4.2.5. Tree construction step: Edmonds’ algorithm

At decoding time, greedy inference is not guaranteed to end up with arc dependencies

that form a tree structure and the classiﬁcation decision might contain cycles. In this

case, the output can be post-processed with a maximum spanning tree algorithm (as

the third step in Fig. 4). We construct the fully connected directed graph G = (V, E)

where the vertices V are the tokens of the advertisement (that are not predicted as

skips) and the dummy root symbol, E contains the edges representing the highest scor-

ing relation (e.g., part-of, segment, equivalent) with the respective cross entropy scores

serving as weights. Since G is a directed graph, s(xi, xj) is not necessarily equal to

s(xj, xi). Similar to McDonald et al. (2005), we employ Edmonds’ maximum span-

ning tree algorithm for directed graphs (Chu & Liu, 1965; Edmonds, 1967) to build a

non-projective parser. Indeed, in our setting, we have a signiﬁcant number (26% in

the dataset used for experiments, see further) of non-adjacent part-of and equivalent

relations (non-projective). It is worth noting that in the case of segment relations, the

words involved are not interleaved by other tokens and are always adjacent. We ap-

ply Edmonds’ algorithm to every graph which is constructed to get the highest scoring

graph structure, even in the cases where a tree is already formed by greedy inference.

For skips, we consider the predictions as obtained from the greedy approach and we do

not include them in the fully connected weighted graph, since Edmonds’ complexity is
O(n2) for dense graphs and might lead to slow decoding time.

In this section, we present the experimental results of our study. We describe the

dataset, the setup of the experiments and we compare the results of the methods anal-

5. Results and discussion

ysed in the previous sections.

5.1. Experimental setup

Our dataset consists of a large collection (i.e., 887,599) of Dutch property advertise-

ments from real estate agency websites. From this large dataset, a sub-collection of

2,318 classiﬁeds have been manually annotated by 3 trained human annotators (1 an-

notation per ad, 773 ads per annotator). The annotations follow the format of the

22

property tree that is described in detail in Section 3 and is illustrated in Fig. 1. The

dataset is available for research purposes, see our github codebase.1

In the experi-

ments, we use only the annotated text advertisements for the pipeline setting, i.e., LTM

(locally trained model), MTT (globally trained model). In the case of the neural net-

work approach, we train the embeddings on the large collection by using the word2vec

model (Mikolov et al., 2013) whereas in the joint learning, we use only the annotated

documents, similar to the pipeline approach. The code of the LTM and the MTT hand-

crafted systems is available on github.1 We also use our own CRF implementation. The

code for the joint model has been developed in Python with the Tensorﬂow machine

learning library (Abadi et al., 2016) and will be made public as well. For the evalu-

ation, we use 70% for training, 15% for validation and 15% as test set. We measure

the performance by computing the F1 score on the test set. The accuracy metric can

be misleading in our case since we have to deal with imbalanced data (the skip label

is over-represented). We only report numbers on the structured classes, i.e., segment

and part-of since the other dependencies (skip, equivalent) are auxiliary in the joint

models and do not directly contribute to the construction of the actual property tree.

For the overall F1, we are again only considering the structured classes. Finally, we

report the number of property trees (which shows how likely our model is to produce

trees without applying Edmonds’ algorithm, i.e., by greedy inference alone) for all the

models before applying Edmonds’ algorithm that guarantees the tree structure of the

predictions.

For the pipeline models, we train the CRF with regularization parameter λCRF =

10 and the LTM and MTT with C = 1 based on the best hyperparameters on the val-

idation set. As binary classiﬁer, we use logistic regression. For the joint model, we

train 128-dimensional word2vec embeddings on a collection of 887k advertisements.

In general, using larger embeddings dimensions (e.g., 300), does not affect the perfor-

mance of our models. We consistently used single-layer LSTMs through our experi-

ments to keep our model relatively simple and to evaluate the various attention meth-

ods on top of that. We have also reported results on the joint model using a two-layer

1https://github.com/bekou/ad_data

23

Precision

Recall

segment

part-of

segment

part-of

segment

Overall

Trees
(% of ads)

-
d
n
a
H

d
e
t
f
a
r
c

M
T
S
L

e
v
i
t
n
e
t
t

A

M
T
S
L

Model

LTM
MTT

LSTM
LSTM+E
2xLSTM+E

Additive
Bilinear
Multiplicative
Biafﬁne
Tensor
Edge1
Edge2
Edge3

73.77
73.77

70.24
70.18
73.91

72.97
70.25
71.12
70.01
71.53
71.56
72.03
71.74

60.53
61.15

65.23
63.92
69.88

65.71
66.34
66.40
64.67
64.68
67.46
66.09
67.69

70.98
70.98

77.73
77.77
75.78

76.45
79.96
77.81
78.32
76.17
78.24
75.35
78.44

60.40
61.01

70.32
71.08
71.22

70.90
72.53
71.26
71.04
70.79
71.31
70.99
73.00

72.35
72.35

73.80
73.78
74.83

74.67
74.79
74.31
73.93
73.78
74.75
73.65
74.94

F1 (%)
part-of

60.47
61.08

67.68
67.31
70.54

68.21
69.29
68.75
67.71
67.60
69.33
68.46
70.25

64.76
65.15

68.82
68.57
70.90

69.46
70.20
69.70
68.75
68.68
70.08
69.12
70.70

37.18
43.23

68.30
68.30
78.09

74.35
72.62
72.91
74.06
69.16
70.32
73.48
78.96

Table 2: Performance of the three approaches on the structured prediction task. The top rows are for the
pipeline approach, i.e., hand-crafted features. The next block of results presents the results for the neural
joint model based on LSTMs. The bottom block contains results of the joint models augmented with several
attentive architectures. Edmonds’ algorithm is applied in all of the models to retain the tree structure, except
for the LSTM joint model. The LSTM+E is the LSTM model with Edmonds’ algorithm included. The
2xLSTM+E is the same joint model but it simply uses a stack of two LSTM layers. In the experiments with
attention, we use a one-stack LSTM. The rightmost column is the percentage of the ads that are valid trees
before applying Edmonds’ (i.e., step (3) of Fig. 4), showing the ability of the model to form trees during
greedy inference. In the Edgei models, the number i stands for the number of times that we have run the
message passing phase.

stacked LSTM joint model, although it needs a higher computation time compared to

a single-layer LSTM with an attention layer on top. The hidden size of the LSTMs is

d = 128 and the size of the neural network used in the scoring and the attention layer is

ﬁxed to l = 32. The optimization algorithm used is Adam (Kingma & Ba, 2015) with
a learning rate of 10−3. To reduce the effect of overﬁtting, we regularize our model

using the dropout method (Srivastava et al., 2014). We ﬁx the dropout rate on the in-

put of the LSTM layer to 0.5 to obtain signiﬁcant improvements (∼1%-2% F1 score

increase, depending on the model). For the two-layer LSTM, we ﬁx the dropout rate

to 0.3 in each of the input layers since this leads to largest performance increase on the

validation set. We have also explored gradient clipping without further improvement

on our results. In the joint model setting, we follow the evaluation strategy of early

stopping (Caruana et al., 2000; Graves et al., 2013) based on the performance of the

validation set. In most of the experiments, we obtain the best hyperparameters after

∼60 epochs.

24

5.2. Comparison of the pipeline and the joint model

One of the main contributions of our study is the comparison of the pipeline approach

and the proposed joint model. We formulated the problem of identifying the enti-

ties (i.e., segments) and predicting the dependencies between them (i.e., construc-

tion of the property tree) as a joint model. Our neural model, unlike recent stud-

ies (Miwa & Bansal, 2016; Zheng et al., 2017) on joint models that use LSTMs to

handle similar tasks, does not need two components to model the problem (i.e., NER

and dependency parsing). To the best of our knowledge, our study is the ﬁrst that for-

mulates the task in an actual joint setting without the need to pre-train the sequence

labeling component or for parameter sharing between them, since we use only one

component for both subtasks. In Table 2, we present the results of the pipeline model

(hand-crafted) and the proposed joint model (LSTM). The improvement of the joint

model over the pipeline is unambiguous, i.e., 3.42% overall F1 score difference be-

tween MTT (highest scoring pipeline model) and LSTM+E (LSTM model with Ed-

monds’ algorithm). An additional increase of ∼2.3% is achieved when we consider

two-layer LSTMs (2xLSTM+E) for our joint model. All results in Table 2, except for

the LSTM, are presented using Edmonds’ algorithm on top, to construct the property

tree. Examining each label separately, we observe that the original LSTM+E model

(73.78%) performs better by 1.43% in entity segmentation than the CRF (72.35%).

The LSTM model achieves better performance in the entity recognition task since it

has to learn the two subtasks simultaneously resulting in interactions between the com-

ponents (i.e., NER and dependency parser). This way, the decisions for the entity

recognition can beneﬁt from predictions that are made for the part-of relations. Con-

cerning the part-of dependencies, we note that the LSTMs outperform the hand-crafted

approaches by 6.23%. Also, the number of valid trees that are constructed before ap-

plying Edmonds’ algorithm is almost twice as high for the LSTM models. Stacking

two-layer LSTMs results in an additional ∼1% improvement in the segmentation task

and ∼3% in the part-of relations. The greedy inference for the hand-crafted meth-

ods does not produce well-formed trees, meaning that post-processing with Edmonds’

algorithm (enforce tree structure) is expected to increase the performance of the hand-

crafted models compared to the LSTM model performance. Indeed, the performance

25

of the feature based hand-crafted models (i.e., LTM and MTT) without the Edmonds’

on top is not reported in Table 2 due to their poor performance in our task (i.e., ∼60%

overall F1 and ∼51% for part-of ), but after post-processing with Edmonds’ the perfor-

mance signiﬁcantly increases (i.e., ∼65%). On the other hand, applying the Edmonds’

algorithm on the LSTM model leads to marginally decreased performance (∼0.2%)

compared to the original LSTM model, probably indicating that enforcing structural

constraints is not beneﬁcial for a model that clearly has the ability to form valid tree

structures during greedy inference. Although one might be tempted not to enforce the

tree structure (post-process with Edmonds’), due to the nature of our problem, we have

to enforce tree constraints in all of the models.

5.3. Comparison of the joint and the attention model

After having established the superior performance of neural approach using LSTMs

over the more traditional (LTM and MTT) methods based on hand-crafted features, we

now discuss further improvements using attentive models. The attention mechanisms

are designed to encourage the joint model to focus on informative tokens. We exploited

several attention mechanisms as presented in Section 4.2.4. Table 2 shows the perfor-

mance of the various models. Overall, the attention models are performing better in

terms of overall F1 score compared to the original joint model with the Edmonds’ on

top. Although the performance of the Biafﬁne and the Tensor models is limited com-

pared to the improvement of the other attentive models, we focus on: (i) the Biafﬁne

model since it achieved state-of-the-art performance on the dependency parsing task

and (ii) the Tensor model because we were expecting that it would perform similarly

to the Bilinear model (it has a bilinear tensor layer). Despite its simplicity, the Bilinear

model is the second best performing attentive model in Table 2 in terms of overall F1

score. Edge3 (70.70% overall F1 score) achieves better results than the other attention

mechanisms in the entity recognition and in the dependency parsing tasks. We observe

that running the message passing step multiple times in the Edge model, gives an in-

creasing trend in the number of valid trees that were constructed before applying the

maximum spanning tree algorithm. This is not surprising since we expect that running

the message passing phase multiple times leads to improved edge representations. The

26

maximum number of trees without post-processing by Edmonds’ is attained when we

run the message passing for 3 times whereas further increasing the number beyond 3

(e.g., 4) appears no longer beneﬁcial. Stacking a second LSTM layer on top of the joint

model (2xLSTM+E) marginally improves the performance by 0.2% compared to the

Edge3 attention model. But adding a second LSTM layer comes with the additional

cost of an increased computation time compared to the joint models with the attention

layers on top. This illustrates that: (i) there might be some room for marginally im-

proving the attention models even further, and (ii) we do not have to worry about the

quadratic nature of our approach since in terms of speed the attentive models are able

to surpass the two-layer LSTMs. The sequential processing of the LSTMs might be

the reason that slows down the computation time for the 2xLSTM over the rest of the

attentive models. Speciﬁcally, on an Intel(R) Xeon(R) CPU E5-2650 v2 @ 2.60GHz

processor, the best performing model (i.e., Edge3) takes ∼2 minutes per epoch while

in the 2xLSTM case, it takes ∼2.5 minutes leading to a slowdown of ∼25%. The

percentage of the ads that are valid trees is 1% better in the Edge3 over the two-layer

LSTM showcasing the ability of the Edge model to form more valid trees during greedy

inference.

5.4. Discussion

In this section, we discuss some additional aspects of our problem and the approaches

that we follow to handle them. As we mentioned before, a single entity can be present

in the text with multiple mentions. This brings an extra difﬁculty to our task. For

instance, in the example of Fig. 1, the entity “large apartment” is expressed in the ad

with the mentions “large apartment” and “home”. Hence it is confusing to which men-

tion the other entities should be attached to. One way would be to attach them to both

and then eliminate one of the connections using Edmonds’ spanning tree algorithm,

which is the approach adopted in Bekoulis et al. (2017). The problematic issue with

this approach is that the spanning tree algorithm would randomly remove all mentions

but one, possibly resulting in uncertain outcomes. To avoid this problem, we now use

as the main mention for an entity the ﬁrst mention in order of appearance in the text

(e.g., “large apartment” in our example) and the remaining mentions (e.g., “home”)

27

are attached as equivalent mentions to the main one. Usually, the most informative

mention for an entity is the one that appears ﬁrst, because we again refer to an entity

mentioned before, often with a shorter description. Following our intuition, the neural

model increases its overall performance by ∼3% (from 66% to 69% and more than 5%

in the part-of relation) and the pipeline approaches by almost 4% (from 61%, reported

in Bekoulis et al. (2017) to 65% and more than 5% in the part-of relation).

We also experimented with introducing the equivalent relations. Although it is

a strongly under-represented class in the dataset and the model performs poorly for

this label (an equivalent edge F1 score of 10%), introducing the equivalent label is

the natural way of modeling our problem (i.e., assigning each additional mention as

equivalent to the main mention). We ﬁnd out that introducing this type of relation

leads to a slight decrease (∼1%) in the part-of and a marginal increase (∼0.3%) in

the segment relations which are the main relations while retaining the nature of our

problem. In the pipeline approach, it results in an 9% drop in the F1 score of the part-

of relation. This is the reason that the results as presented in Table 2 do not consider

the equivalent relation for the hand-crafted model to make a fair comparison in the

structured classes.

We believe our experimental comparison of the various architectural model vari-

ations provides useful ﬁndings for practitioners. Speciﬁcally, for applications requir-

ing both segmentation (entity recognition) and dependency parsing (structured predic-

tion), our ﬁndings can be qualitatively summarized as follows: (i) joint modeling is

the most appropriate approach since it reduces error propagation between the com-

ponents, (ii) the LSTM model is much more effective (than models relying on hand-

crafted features) because it automatically extracts informative features from the raw

text, (iii) attentive models are proven effective because they encourage the model to fo-

cus on salient tokens, (iv) the edge attention model leads to an improved performance

since it better encodes the information ﬂow between the entities by using graph rep-

resentations, and (v) stacking a second LSTM marginally increases the performance,

suggesting that there might be some room for slight improvement of the attention mod-

els by adding LSTM layers.

Finally, we point out how exactly our model relates to state-of-the-art in the ﬁeld.

28

Our joint model is able to both extract entity mentions (i.e., perform segmentation)

and do dependency parsing, which we demonstrate on the real estate problem. Previ-

ous studies (Kate & Mooney, 2010; Li & Ji, 2014; Miwa & Sasaki, 2014) that jointly

considered the two subtasks (i.e., segmentation and relation extraction): (i) require

manual feature engineering and (ii) generalize poorly between various applications.

On the other hand, in our work, we rely on neural network methods (i.e., LSTMs) to

automatically extract features from the real estate textual descriptions and perform the

two tasks jointly. Although there are other methods which use neural network archi-

tectures (Miwa & Bansal, 2016; Zheng et al., 2017; Li et al., 2017) that focus on the

relation extraction problem, our work is different in that we aim to model directed

spanning trees and thus to solve the dependency parsing problem which is more con-

strained and difﬁcult (than extracting single instances of binary relations). Moreover,

the cited methods require either parameter sharing or pre-training of the segmenta-

tion module, which complicates learning. Therefore, cited methods are not directly

comparable to our model and cannot be applied to our real estate task out-of-the-box.

However, our model’s main limitation is the quadratic scoring layer that increases the

time complexity of the segmentation task from linear (which is the complexity of a
conditional random ﬁeld, CRF) to O(n2). As a result, it sacriﬁces standard linear com-

plexity of the segmentation task, in order to reduce the error propagation between the

subtasks and thus perform learning in a joint, end-to-end differentiable, setting.

6. Conclusions

In this paper, we proposed an LSTM-based neural model to jointly perform segmenta-

tion and dependency parsing. We apply it to a real estate use case processing textual

ads, thus (1) identifying important entities of the property (e.g., rooms) and (2) struc-

turing them into a tree format based on the natural language description of the prop-

erty. We compared our model with the traditional pipeline approaches that have been

adapted to our task and we reported an improvement of 3.4% overall edge F1 score.

Moreover, we experimented with different attentive architectures and stacking of a sec-

ond LSTM layer over our basic joint model. The results indicate that exploiting atten-

29

tion mechanisms that encourage our model to focus on informative tokens, improves

the model performance (increase of overall edge F1 score with ∼2.1%) and increases

the ability to form valid trees in the prediction phase (4% to 10% more valid trees for

the two best scoring attention mechanisms) before applying the maximum spanning

tree algorithm.

The contribution of this study to the research in expert and intelligent systems is

three-fold: (i) we introduce a generic joint model, simultaneously solving both sub-

tasks of segmentation (i.e., entity extraction) and dependency parsing (i.e., extracting

relationships among entities), that unlike previous work in the ﬁeld does not rely on

manually engineered features, (ii) in particular for the real estate domain, extracting

a structured property tree from a textual ad, we reﬁne the annotations and addition-

ally propose attention models, compared to initial work on this application, and ﬁ-

nally (iii) we demonstrate the effectiveness of our proposed generic joint model with

extensive experiments (see aforementioned F1 improvement of 2.1%). Despite the

experimental focus on the real estate domain, we stress that the model is generic in na-

ture, and could be equally applied to other expert system scenarios requiring the general

tasks of both detecting entities (segmentation) and establishing relations among them

(dependency parsing). We furthermore note that our model, rather than focusing on

extracting a single binary relation from a sentence (as in traditional relation extraction

settings), produces a complete tree structure.

Future work can evaluate the value of our joint model we introduced in other spe-

ciﬁc application domains (e.g., biology, medicine, news) for expert and intelligent sys-

tems. For example, the method can be evaluated for entity recognition and binary

relation extraction (the ACE 04 and ACE 05 datasets; see Miwa & Bansal (2016)) or in

adverse drug effects from biomedical texts (see Li et al. (2016)). In terms of model ex-

tensions and improvements, one research issue is to address the time complexity of the

NER part by modifying the quadratic scoring layer for this component. An additional

research direction is to investigate different loss functions for the NER component (e.g.,

adopting a conditional random ﬁeld (CRF) approach), since this has been proven ef-

fective in the NER task on its own (Lample et al., 2016). A ﬁnal extension we envision

is to enable multi-label classiﬁcation of relations among entity pairs.

30

The presented research was partly performed within the MALIBU project, funded by

Flanders Innovation & Entrepreneurship (VLAIO) contract number IWT 150630.

Acknowledgments

References

Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat,

S., Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga, R., Moore, S., Murray,

D. G., Steiner, B., Tucker, P., Vasudevan, V., Warden, P., Wicke, M., Yu, Y., & Zheng,

X. (2016). Tensorﬂow: A system for large-scale machine learning. In Proceedings

of the 12th USENIX Conference on Operating Systems Design and Implementation

(pp. 265–283). Berkeley, CA, USA.

Andor, D., Alberti, C., Weiss, D., Severyn, A., Presta, A., Ganchev, K., Petrov, S.,

& Collins, M. (2016). Globally normalized transition-based neural networks.

In

Proceedings of the 54th Annual Meeting of the Association for Computational Lin-

guistics (Volume 1: Long Papers) (pp. 2442–2452). Berlin, Germany.

Atkinson, J., & Bull, V. (2012). A multi-strategy approach to biological named

entity recognition. Expert Systems with Applications, 39(17), 12968 – 12974.

doi:10.1016/j.eswa.2012.05.033.

Bach, N., & Badaskar, S. (2007). A review of relation extraction. Literature review for

Language and Statistics II, .

Bekoulis, G., Deleu, J., Demeester, T., & Develder, C. (2017). Reconstructing the

house from the ad: Structured prediction on real estate classiﬁeds. In Proceedings of

the 15th Conference of the European Chapter of the Association for Computational

Linguistics: (Volume 2, Short Papers) (pp. 274–279). Valencia, Spain.

Bengio, Y., Simard, P., & Frasconi, P. (1994). Learning long-term dependencies

with gradient descent is difﬁcult. Transactions on neural networks, 5(2), 157–166.

doi:10.1109/72.279181.

31

Bohnet, B., & Nivre, J. (2012). A transition-based system for joint part-of-speech tag-

ging and labeled non-projective dependency parsing.

In Proceedings of the 2012

Joint Conference on Empirical Methods in Natural Language Processing and Com-

putational Natural Language Learning (pp. 1455–1465). Jeju Island, Korea: Asso-

ciation for Computational Linguistics.

Carreras, X. (2007). Experiments with a higher-order projective dependency parser.

In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Lan-

guage Processing and Computational Natural Language Learning (pp. 957–961).

Prague, Czech: Association for Computational Linguistics.

Caruana, R., Lawrence, S., & Giles, L. (2000). Overﬁtting in neural nets: Backpropa-

gation, conjugate gradient, and early stopping. In Proceedings of the 13th Interna-

tional Conference on Neural Information Processing Systems (pp. 381–387). Den-

ver, USA: MIT Press.

Chen, D., & Manning, C. (2014). A fast and accurate dependency parser using neural

networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural

Language Processing (pp. 740–750). Doha, Qatar: Association for Computational

Linguistics.

Chiu, J., & Nichols, E. (2016). Named entity recognition with bidirectional LSTM-

CNNs. Transactions of the Association for Computational Linguistics, 4, 357–370.

Chu, Y.-J., & Liu, T.-H. (1965). On shortest arborescence of a directed graph. Scientia

Sinica, 14, 1396–1400.

Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).

Natural language processing (almost) from scratch. Journal of Machine Learning

Research, 12, 2493–2537.

Daum´e

III, H., Langford,

J., & Marcu, D.

(2009).

Search-based

structured

prediction.

Machine Learning

Journal,

75(3),

297–325.

doi:10.1007/s10994-009-5106-x.

32

Dozat, T., & Manning, C. D. (2017). Deep biafﬁne attention for neural dependency

parsing. In Proceedings of the International Conference for Learning Representa-

tions (pp. 1–8). Toulon, France.

Dyer, C., Ballesteros, M., Ling, W., Matthews, A., & Smith, N. A. (2015). Transition-

based dependency parsing with stack long short-term memory. In Proceedings of the

53rd Annual Meeting of the Association for Computational Linguistics and the 7th

International Joint Conference on Natural Language Processing (Volume 1: Long

Papers) (pp. 334–343). Beijing, China.

Edmonds, J. (1967). Optimum branchings. Journal of research of the National Bureau

of Standards, 71B(4), 233–240.

Eisner, J. M. (1996). Three new probabilistic models for dependency parsing: An

exploration. In Proceedings of the 16th International Conference on Computational

Linguistics (Volume 1) (pp. 340–345). Copenhagen, Denmark.

Fundel, K., Kffner, R., & Zimmer, R.

(2007).

Relex-relation extrac-

tion using dependency parse

trees.

Bioinformatics,

23(3),

365–371.

doi:10.1093/bioinformatics/btl616.

Gillick, D., Brunk, C., Vinyals, O., & Subramanya, A. (2016). Multilingual language

processing from bytes. In Proceedings of the 2016 Conference of the North Amer-

ican Chapter of the Association for Computational Linguistics: Human Language

Technologies (pp. 1296–1306). San Diego, California.

Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., & Dahl, G. E. (2017). Neural

message passing for quantum chemistry. In Proceedings of the 34th International

Conference on Machine Learning (pp. 1263–1272). Sydney, Australia: PMLR.

Goldberg, Y., & Hirst, G. (2017). Neural Network Methods in Natural Language Pro-

cessing. Morgan & Claypool Publishers.

Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.

http://www.deeplearningbook.org.

33

Graves, A., r. Mohamed, A., & Hinton, G. (2013). Speech recognition with deep

recurrent neural networks.

In Proceedings of the International Conference on

Acoustics, Speech and Signal Processing (pp. 6645–6649). Vancouver, Canada.

doi:10.1109/ICASSP.2013.6638947.

Gurulingappa, H., MateenRajpu, A., & Toldo, L. (2012). Extraction of potential ad-

verse drug events from medical case reports. Journal of Biomedical Semantics, 3(1),

1–15. doi:10.1186/2041-1480-3-15.

Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computa-

tion, 9(8), 1735–1780. doi:10.1162/neco.1997.9.8.1735.

Huang, Z., Xu, W., & Yu, K. (2015). Bidirectional LSTM-CRF models for sequence

tagging. arXiv preprint arXiv:1508.01991, .

Jaeger, H. (2010). The echo state approach to analysing and training recurrent neu-

ral networks-with an erratum note’. Bonn, Germany: German National Research

Center for Information Technology GMD Technical Report, 148(34), 13.

Jung, J. J. (2012). Online named entity recognition method for microtexts in social

networking services: A case study of twitter. Expert Systems with Applications,

39(9), 8066 – 8070. doi:10.1016/j.eswa.2012.01.136.

Kate, R. J., & Mooney, R. (2010). Joint entity and relation extraction using card-

pyramid parsing. In Proceedings of the 14th Conference on Computational Natural

Language Learning (pp. 203–212). Uppsala, Sweden: Association for Computa-

tional Linguistics.

Kingma, D., & Ba, J. (2015). Adam: A method for stochastic optimization. In Inter-

national Conference on Learning Representations. San Diego, USA.

Kiperwasser, E., & Goldberg, Y. (2016). Simple and accurate dependency parsing

using bidirectional lstm feature representations. Transactions of the Association for

Computational Linguistics, 4, 313–327.

34

Konkol, M., Brychcn, T., & Konopk, M. (2015).

Latent semantics in named

entity recognition.

Expert Systems with Applications, 42(7), 3470 – 3479.

doi:10.1016/j.eswa.2014.12.015.

Koo, T., Globerson, A., Carreras, X., & Collins, M. (2007). Structured prediction

models via the Matrix-Tree Theorem. In Proceedings of the 2007 Joint Conference

on Empirical Methods in Natural Language Processing and Computational Natural

Language Learning (pp. 141–150). Prague, Czech: Association for Computational

Linguistics.

K¨uc¸ ¨uk, D., & Yazıcı, A.

(2012).

A hybrid named entity recognizer

for

turkish.

Expert Systems with Applications, 39(3), 2733 – 2742.

doi:10.1016/j.eswa.2011.08.131.

Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional random ﬁelds: Proba-

bilistic models for segmenting and labeling sequence data. In Proceedings of the

18th International Conference on Machine Learning (pp. 282–289). San Francisco,

USA: Morgan Kaufmann.

Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., & Dyer, C. (2016).

Neural architectures for named entity recognition. In Proceedings of the 2016 Con-

ference of the North American Chapter of the Association for Computational Lin-

guistics: Human Language Technologies (pp. 260–270). San Diego, California.

LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., &

Jackel, L. D. (1989). Backpropagation applied to handwritten zip code recognition.

Neural Computation, 1(4), 541–551. doi:10.1162/neco.1989.1.4.541.

Li, F., Zhang, M., Fu, G., & Ji, D. (2017). A neural joint model for entity

and relation extraction from biomedical text. BMC Bioinformatics, 18(1), 1–11.

doi:10.1186/s12859-017-1609-9.

Li, F., Zhang, Y., Zhang, M., & Ji, D. (2016). Joint models for extracting adverse

drug events from biomedical text. In Proceedings of the Twenty-Fifth International

35

Joint Conference on Artiﬁcial Intelligence (pp. 2838–2844). New York, USA: IJ-

CAI/AAAI Press.

Li, Q., & Ji, H. (2014). Incremental joint extraction of entity mentions and relations.

In Proceedings of the 52nd Annual Meeting of the Association for Computational

Linguistics (Volume 1: Long Papers) (pp. 402–412). Baltimore, USA.

Luong, T., Pham, H., & Manning, C. D. (2015). Effective approaches to attention-

based neural machine translation. In Proceedings of the 2015 Conference on Empir-

ical Methods in Natural Language Processing (pp. 1412–1421). Lisbon, Portugal:

Association for Computational Linguistics.

Ma, X., & Hovy, E. (2016). End-to-end sequence labeling via bi-directional LSTM-

CNNs-CRF. In Proceedings of the 54th Annual Meeting of the Association for Com-

putational Linguistics (Volume 1: Long Papers) (pp. 1064–1074). Berlin, Germany.

McDonald, R., & Pereira, F. (2007). Online learning of approximate dependency pars-

ing algorithms. In Proceedings of the 11th Conference of the European Chapter of

the Association for Computational Linguistics (pp. 81–88). Trento, Italy.

McDonald, R., Pereira, F., Ribarov, K., & Hajic, J. (2005). Non-projective depen-

dency parsing using spanning tree algorithms. In Proceedings of Human Language

Technology Conference and Conference on Empirical Methods in Natural Language

Processing (pp. 523–530). Vancouver, British Columbia, Canada: Association for

Computational Linguistics.

Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed

representations of words and phrases and their compositionality. In Proceedings of

the 26th International Conference on Neural Information Processing Systems (pp.

3111–3119). Nevada, United States: Curran Associates, Inc.

Miwa, M., & Bansal, M. (2016). End-to-end relation extraction using LSTMs on se-

quences and tree structures. In Proceedings of the 54th Annual Meeting of the As-

sociation for Computational Linguistics (Volume 1: Long Papers) (pp. 1105–1116).

Berlin, Germany.

36

Miwa, M., & Sasaki, Y. (2014). Modeling joint entity and relation extraction with

table representation. In Proceedings of the 2014 Conference on Empirical Methods

in Natural Language Processing (pp. 1858–1869). Doha, Qatar: Association for

Computational Linguistics.

Nadeau, D., & Sekine, S. (2007). A survey of named entity recognition and classiﬁca-

tion. Lingvisticae Investigationes, 30(1), 3–26. doi:10.1075/li.30.1.03nad.

Nagaraja, C. H., Brown, L. D., & Zhao, L. H. (2011). An autoregressive ap-

proach to house price modeling. The Annals of Applied Statistics, 5(1), 124–149.

doi:10.1214/10-AOAS380.

Nguyen, N., & Guo, Y. (2007). Comparisons of sequence labeling algorithms and ex-

tensions. In Proceedings of the 24th International Conference on Machine Learning

(pp. 681–688). Corvallis, USA: ACM. doi:10.1145/1273496.1273582.

Nivre, J. (2003). An efﬁcient algorithm for projective dependency parsing. In Pro-

ceedings of the 8th International Workshop on Parsing Technologies (pp. 149–160).

Nancy, France.

Nivre, J. (2009). Non-projective dependency parsing in expected linear time. In Pro-

ceedings of the Joint Conference of the 47th Annual Meeting of the Association for

Computational Linguistics and the 4th International Joint Conference on Natural

Language Processing of the Asian Federation of Natural Language Processing (pp.

351–359). Singapore.

Nivre, J., Hall, J., Nilsson, J., Eryiˇgit, G., & Marinov, S. (2006). Labeled pseudo-

projective dependency parsing with support vector machines. In Proceedings of the

10th Conference on Computational Natural Language Learning (pp. 221–225). New

York, USA: Association for Computational Linguistics.

Pace, K., Barry, R., Gilley, O. W., & Sirmans, C. (2000). A method for spatialtem-

poral forecasting with an application to real estate prices. International Journal of

Forecasting, 16(2), 229 – 246. doi:10.1016/S0169-2070(99)00047-3.

37

Pascanu, R., Mikolov, T., & Bengio, Y. (2013). On the difﬁculty of training recurrent

neural networks. In Proceedings of the 30th International Conference on Interna-

tional Conference on Machine Learning (pp. 1310–1318). Atlanta, USA: JMLR.org.

Peng, F., & McCallum, A. (2006). Information extraction from research papers using

conditional random ﬁelds. Information processing & management, 42(4), 963–979.

doi:10.1016/j.ipm.2005.09.002?

Rabiner, L., & Juang, B. (1986). An introduction to hidden markov models. IEEE

ASSP Magazine, 3(1), 4–16. doi:10.1109/MASSP.1986.1165342.

Ratinov, L., & Roth, D. (2009). Design challenges and misconceptions in named en-

tity recognition. In Proceedings of the 13th Conference on Computational Natural

Language Learning (pp. 147–155). Boulder, USA: Association for Computational

Linguistics.

China.

dos Santos, C., Xiang, B., & Zhou, B. (2015). Classifying relations by ranking with

convolutional neural networks. In Proceedings of the 53rd Annual Meeting of the As-

sociation for Computational Linguistics and the 7th International Joint Conference

on Natural Language Processing (Volume 1: Long Papers) (pp. 626–634). Beijing,

Socher, R., Chen, D., Manning, C. D., & Ng, A. (2013). Reasoning with neural

tensor networks for knowledge base completion.

In Proceedings of the 26th In-

ternational Conference on Neural Information Processing Systems (pp. 926–934).

Nevada, United States: Curran Associates, Inc.

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014).

Dropout: A simple way to prevent neural networks from overﬁtting. Journal of

Machine Learning Research, 15(1), 1929–1958.

Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with

neural networks.

In Proceedings of the 27th International Conference on Neural

Information Processing Systems (pp. 3104–3112). Montreal, Canada: MIT Press.

38

Taskar, B., Guestrin, C., & Koller, D. (2003). Max-margin markov networks.

In

Proceedings of the 16th International Conference on Neural Information Processing

Systems (pp. 25–32). Bangkok, Thailand: MIT Press.

Tsochantaridis, I., Hofmann, T., Joachims, T., & Altun, Y. (2004). Support vector

machine learning for interdependent and structured output spaces. In Proceedings

of the 21st International Conference on Machine Learning (pp. 104–112). Helsinki,

Finland: ACM. doi:10.1145/1015330.1015341.

Tutte, W. T. (2001). Graph theory. In Encyclopedia of Mathematics and its Applications

(p. 138). Cambridge University Press volume 21.

Vinyals, O., Fortunato, M., & Jaitly, N. (2015). Pointer networks. In Proceedings of

the 28th International Conference on Neural Information Processing Systems (pp.

2692–2700). Montreal, Canada: Curran Associates, Inc.

Wang, D., & Li, M. (2017).

Stochastic conﬁguration networks: Fundamen-

tals and algorithms.

IEEE Transactions on Cybernetics, 47(10), 3466–3479.

doi:10.1109/TCYB.2017.2734043.

Wang, W., & Chang, B. (2016). Graph-based dependency parsing with bidirectional

lstm. In Proceedings of the 54th Annual Meeting of the Association for Computa-

tional Linguistics (Volume 1: Long Papers) (pp. 2306–2315). Berlin, Germany.

Xu, Y., Mou, L., Li, G., Chen, Y., Peng, H., & Jin, Z. (2015). Classifying relations via

long short term memory networks along shortest dependency paths. In Proceedings

of the 2015 Conference on Empirical Methods in Natural Language Processing (pp.

1785–1794). Lisbon, Portugal: Association for Computational Linguistics.

Yamada, H., & Matsumoto, Y. (2003). Statistical dependency analysis with support

vector machines. In Proceedings of the 8th International Workshop on Parsing Tech-

nologies (pp. 195–206). Nancy, France.

Zhang, H., & McDonald, R. (2012). Generalized higher-order dependency parsing with

cube pruning. In Proceedings of the 2012 Joint Conference on Empirical Methods in

39

Natural Language Processing and Computational Natural Language Learning (pp.

320–331). Jeju Island, Korea: Association for Computational Linguistics.

Zhang, X., Cheng, J., & Lapata, M. (2017). Dependency parsing as head selection.

In Proceedings of the 15th Conference of the European Chapter of the Association

for Computational Linguistics: (Volume 1, Long Papers) (pp. 665–676). Valencia,

Spain.

Zheng, S., Hao, Y., Lu, D., Bao, H., Xu, J., Hao, H., & Xu, B. (2017). Joint entity and

relation extraction based on a hybrid neural network. Neurocomputing, 257, 59 –

66. doi:10.1016/j.neucom.2016.12.075.

40

8
1
0
2
 
r
a

M
 
9
1
 
 
]
L
C
.
s
c
[
 
 
2
v
0
9
5
9
0
.
9
0
7
1
:
v
i
X
r
a

An attentive neural architecture for joint segmentation and
parsing and its application to real estate ads

Giannis Bekoulis∗, Johannes Deleu, Thomas Demeester, Chris Develder

Ghent University – imec, IDLab, Department of Information Technology,
Technologiepark Zwijnaarde 15, 9052 Ghent, Belgium

Abstract

In processing human produced text using natural language processing (NLP) tech-

niques, two fundamental subtasks that arise are (i) segmentation of the plain text into

meaningful subunits (e.g., entities), and (ii) dependency parsing, to establish relations

between subunits. Such structural interpretation of text provides essential building

blocks for upstream expert system tasks: e.g., from interpreting textual real estate ads,

one may want to provide an accurate price estimate and/or provide selection ﬁlters for

end users looking for a particular property — which all could rely on knowing the

types and number of rooms, etc. In this paper we develop a relatively simple and ef-

fective neural joint model that performs both segmentation and dependency parsing

together, instead of one after the other as in most state-of-the-art works. We will fo-

cus in particular on the real estate ad setting, aiming to convert an ad to a structured

description, which we name property tree, comprising the tasks of (1) identifying im-

portant entities of a property (e.g., rooms) from classiﬁeds and (2) structuring them

into a tree format. In this work, we propose a new joint model that is able to tackle

the two tasks simultaneously and construct the property tree by (i) avoiding the error

propagation that would arise from the subtasks one after the other in a pipelined fash-

ion, and (ii) exploiting the interactions between the subtasks. For this purpose, we

perform an extensive comparative study of the pipeline methods and the new proposed

∗Corresponding author
Email addresses: giannis.bekoulis@ugent.be (Giannis Bekoulis),

johannes.deleu@ugent.be (Johannes Deleu), thomas.demeester@ugent.be (Thomas
Demeester), chris.develder@ugent.be (Chris Develder)

Preprint submitted to Expert Systems with Applications

March 20, 2018

joint model, reporting an improvement of over three percentage points in the overall

edge F1 score of the property tree. Also, we propose attention methods, to encourage

our model to focus on salient tokens during the construction of the property tree. Thus

we experimentally demonstrate the usefulness of attentive neural architectures for the

proposed joint model, showcasing a further improvement of two percentage points in

edge F1 score for our application. While the results demonstrated are for the particular

real estate setting, the model is generic in nature, and thus could be equally applied to

other expert system scenarios requiring the general tasks of both (i) detecting entities

(segmentation) and (ii) establishing relations among them (dependency parsing).

Keywords: neural networks, joint model, relation extraction, entity recognition,

dependency parsing

1. Introduction

Many consumer-oriented digital applications rely on input data provided by their tar-

get audience. For instance, real estate websites gather property descriptions for the

offered classiﬁeds, either from realtors or from individual sellers.

In such cases, it

is hard to strike the right balance between structured and unstructured information:

enforcing restrictions or structure upon the data format (i.e., predeﬁned form) may re-

duce the amount or diversity of the data, while unstructured data (i.e., raw text) may

require non-trivial (i.e., hard to automate) transformation to a more structured form to

be useful/practical for the intended application. In the real estate domain, textual ad-

vertisements are an extremely useful but highly unstructured way of representing real

estate properties. However, structured descriptions of the advertisements are very help-

ful, e.g., for real estate agencies to suggest the most appropriate sales/rentals for their

customers, while keeping human reading effort limited. For example, special search

ﬁlters, which are usually used by clients, cannot be directly applied to textual advertise-

ments. On the contrary, a structured representation of the property (e.g., a tree format

of the property) enables the simpliﬁcation of the unstructured textual information by

applying speciﬁc ﬁlters (e.g., based on the number of bedrooms, number of ﬂoors, or

the requirement of having a bathroom with a toilet on the ﬁrst ﬂoor), and it also ben-

2

eﬁts other related applications such as automated price prediction (Pace et al., 2000;

Nagaraja et al., 2011).

The new real estate structured prediction problem as deﬁned by Bekoulis et al.

(2017) has as main goal to construct the tree-like representation of the property (i.e.,

the property tree) based on its natural language description. This can be approached as

a relation extraction task by a pipeline of separate subtasks, comprising (i) named entity

recognition (NER) (Nadeau & Sekine, 2007) and (ii) relation extraction (Bach & Badaskar,

2007). Unlike previous studies (Li & Ji, 2014; Miwa & Bansal, 2016) on relation ex-

traction, in the work of Bekoulis et al. (2017), the relation extraction module is re-

placed by dependency parsing. Indeed, the relations that together deﬁne the structure

of the house should form a tree, where entities are part-of one another (e.g., a ﬂoor

is part-of a house, a room is part-of a ﬂoor). This property tree is structurally simi-

lar to a parse tree. Although the work of Bekoulis et al. (2017) is a step towards the

construction of the property tree, it follows a pipeline setting, which suffers from two

serious problems: (i) error propagation between the subtasks, i.e., NER and depen-

dency parsing, and (ii) cross-task dependencies are not taken into account, e.g., terms

indicating relations (includes, contains, etc.) between entities that can help the NER

module are neglected. Due to the unidirectional nature of stacking the two modules

(i.e., NER and dependency parsing) in the pipeline model, there is no information ﬂow-

ing from the dependency parsing to the NER subtask. This way, the parser is not able

to inﬂuence the predictions of the NER. Other studies on similar tasks (Li & Ji, 2014;

Kate & Mooney, 2010) have considered the two subtasks jointly. They simultaneously

extract entity mentions and relations between them usually by implementing a beam-

search on top of the ﬁrst module (i.e., NER), but these methods require the manual

extraction of hand-crafted features. Recently, deep learning with neural networks has

received much attention and several approaches (Miwa & Bansal, 2016; Zheng et al.,

2017) apply long short-term memory (LSTM) recurrent neural networks and convo-

lutional neural networks (CNNs) to achieve state-of-the-art performance on similar

problems. Those models rely on shared parameters between the NER and relation ex-

traction components, whereby the NER module is typically pre-trained separately, to

improve the training effectiveness of the joint model.

3

In this work, we propose a new joint model to solve the real estate structured pre-

diction problem. Our model is able to learn the structured prediction task without

complicated feature engineering. Whereas previous studies (Miwa & Bansal, 2016;

Zheng et al., 2017; Li et al., 2016, 2017) on joint methods focus on the relation extrac-

tion problem, we construct the property tree which comes down to solving a depen-

dency parsing problem, which is more constrained and hence more difﬁcult. Therefore,

previous methods are not directly comparable to our model and cannot be applied to

our real estate task out-of-the-box. In this work, we treat the two subtasks as one by

reformulating them into a head selection problem (Zhang et al., 2017).

This paper is a follow-up work of Bekoulis et al. (2017). Compared to the con-

ference paper that introduced the real estate extraction task and applied some basic

state-of-the-art techniques as a ﬁrst baseline solution, we now introduce: (i) advanced

neural models that consider the two subtasks jointly and (ii) modiﬁcations to the dataset

annotation representations as detailed below. More speciﬁcally, the main contributions

of this work are the following:

• We propose a new joint model that encodes the two tasks of identifying enti-

ties as well as dependencies between them, as a single head selection problem,

without the need of parameter sharing or pre-training of the ﬁrst entity recogni-

tion module separately. Moreover, instead of (i) predicting unlabeled dependen-

cies and (ii) training an additional classiﬁer to predict labels for the identiﬁed

heads (Zhang et al., 2017), our model already incorporates the dependency label

predictions in its scoring formula.

• We compare the proposed joint model against established pipeline approaches

and report an F1 improvement of 1.4% in the NER and 6.2% in the dependency

parsing subtask, corresponding to an overall edge F1 improvement of 3.4% in

the property tree.

• Compared to our original dataset (Bekoulis et al., 2017), we introduce two ex-

tensions to the data: (i) we consistently assign the ﬁrst mention of a particular

entity in order of appearance in the advertisement as the main mention of the

entity. This results in an F1 score increase of about 3% and 4% for the joint and

4

pipeline models, respectively. (ii) We add the equivalent relation to our anno-

tated dataset to explicitly express that several mentions across the ad may refer

to the same entity.

• We perform extensive analysis of several attention mechanisms that enable our

LSTM-based model to focus on informative words and phrases, reporting an

improved F1 performance of about 2.1%.

The rest of the paper is structured as follows. In Section 2, we review the related

work. Section 3 deﬁnes the problem and in Section 4, we describe the methodology

followed throughout the paper and the proposed joint model. The experimental results

are reported in Section 5. Finally, Section 6 concludes our work.

2. Related work

The real estate structured prediction problem from textual advertisements can be bro-

ken down into the sub-problems of (i) sequence labeling (identifying the core parts

of the property) and (ii) non-projective dependency parsing (connecting the identi-

ﬁed parts into a tree-like structure) (Bekoulis et al., 2017). One can address these two

steps either one by one in a pipelined approach, or simultaneously in a joint model.

The pipeline approach is the most commonly used approach (Bekoulis et al., 2017;

Fundel et al., 2007; Gurulingappa et al., 2012), treating the two steps independently

and propagating the output of the sequence labeling subtask (e.g., named entity recog-

nition) (Chiu & Nichols, 2016; Lample et al., 2016) to the relation classiﬁcation mod-

ule (dos Santos et al., 2015; Xu et al., 2015). Joint models are able to simultaneously

extract entity mentions and relations between them (Li & Ji, 2014; Miwa & Bansal,

2016). In this work, we propose a new joint model that is able to recover the tree-

like structure of the property and frame it as a dependency parsing problem, given the

non-projective tree structure we aim to output. We now present related works for the

sequence labeling and dependency parsing subtasks, as well as for the joint models.

5

2.1. Sequence labeling

Structured prediction problems become challenging due to the large output space.

Speciﬁcally in NLP, sequence labeling (e.g., NER) is the task of identifying the en-

tity mention boundaries and assigning a categorical label (e.g., POS tags) for each

identiﬁed entity in the sentence. A number of different methods have been proposed,

namely Hidden Markov Models (HMMs) (Rabiner & Juang, 1986), Conditional Ran-
dom Fields (CRFs) (Lafferty et al., 2001), Maximum Margin Markov Network (M3N)

(Taskar et al., 2003), generalized support vector machines for structured output (SVMstruct)

(Tsochantaridis et al., 2004) and Search-based Structured Prediction (SEARN) (Daum´e III et al.,

2009). Those methods heavily rely on hand-crafted features and an in-depth review can

be found in Nguyen & Guo (2007). Several variations of these models that also require

manual feature engineering have been used in different application settings (e.g., biol-

ogy, social media context) and languages (e.g., Turkish) (Jung, 2012; K¨uc¸ ¨uk & Yazıcı,

2012; Atkinson & Bull, 2012; Konkol et al., 2015). Recently, deep learning with neu-

ral networks has been succesfully applied to NER. Collobert et al. (2011) proposed

to use a convolutional neural network (CNN) followed by a CRF layer over a se-

quence of word embeddings. Recurrent Neural Networks (RNNs) constitute another

neural network architecture that has attracted attention, due to the state-of-the-art per-

formance in a series of NLP tasks (e.g., sequence-to-sequence (Sutskever et al., 2014),

parsing (Kiperwasser & Goldberg, 2016)). In this context, Gillick et al. (2016) use a

sequence-to-sequence approach for modeling the sequence labeling task. In addition,

several variants of combinations between LSTM and CRF models have been proposed

(Lample et al., 2016; Huang et al., 2015; Ma & Hovy, 2016) achieving state-of-the-art

performance on publicly available datasets.

2.2. Dependency parsing

Dependency parsing is a well studied task in the NLP community, which aims to ana-

lyze the grammatical structure of a sentence. We approach the problem of the property

tree construction as a dependency parsing task i.e., to learn the dependency arcs of

the classiﬁed. There are two well-established ways to address the dependency parsing

problem, via graph-based and transition-based parsers.

6

Graph-based: In the work of McDonald et al. (2005); McDonald & Pereira (2007)

dependency parsing requires the search of the highest scoring maximum spanning

tree in graphs for both projective (dependencies are not allowed to cross) and non-

projective (crossing dependencies are allowed) trees with the Eisner algorithm (Eisner,

1996) and the Chu-Liu-Edmonds algorithm (Chu & Liu, 1965; Edmonds, 1967) re-

spectively. It was shown that exploiting higher-order information (e.g., siblings, grand-

parental relation) in the graph, instead of just using ﬁrst-order information (i.e., par-

ent relations) (Carreras, 2007; Zhang & McDonald, 2012) may yield signiﬁcant im-

provements of the parsing accuracy but comes at the cost of an increased model com-

plexity. Koo et al. (2007) made an important step towards globally normalized mod-

els with hand-crafted features, by adapting the Matrix-Tree Theorem (MTT) (Tutte,

2001) to train over all non-projective dependency trees. We explore an MTT approach

as one of the pipeline baselines. Similar to recent advances in neural graph-based

parsing (Zhang et al., 2017; Kiperwasser & Goldberg, 2016; Wang & Chang, 2016),

we use LSTMs to capture richer contextual information compared to hand-crafted fea-

ture based methods. Our work is conceptually related to Zhang et al. (2017), who

formulated the dependency parsing problem as a head selection problem. We go a step

further in that direction, in formulating the joint parsing and labeling problem in terms

of selecting the most likely combination of head and label.

Transition-based: Transition-based parsers (Yamada & Matsumoto, 2003; Nivre et al.,

2006) replace the exact inference of the graph-based parsers by an approximate but

faster inference method. The dependency parsing problem is now solved by an ab-

stract state machine that gradually builds up the dependency tree token by token. The

goal of this kind of parsers is to ﬁnd the most probable transition sequence from an

initial to some terminal conﬁguration (i.e., a dependency parse tree, or in our case

a property tree) given a permissible set of actions (i.e., LEFT-ARC, RIGHT-ARC,

SHIFT) and they are able to handle both projective and non-projective dependen-

cies (Nivre, 2003, 2009). In the simplest case (i.e., greedy inference), a classiﬁer pre-

dicts the next transition based on the current conﬁguration. Compared to graph-based

dependency parsers, transition-based parsers are able to scale better due to the linear
time complexity while graph-based complexity rises to O(n2) in the non-projective

7

case. Chen & Manning (2014) proposed a way of learning a neural network classiﬁer

for use in a greedy, transition-based dependency parser while using low-dimensional,

dense word embeddings, without the need of manually extracting features. Globally

normalized transition-based parsers (Andor et al., 2016) can be considered an exten-

sion of Chen & Manning (2014), as they perform beam search for maintaining multiple

hypotheses and introduce global normalization with a CRF objective. Dyer et al. (2015)

introduced the stack-LSTM model with push and pop operations which is able to learn

the parser transition states while maintaining a summary embedding of its contents.

Although transition-based systems are well-known for their speed and state-of-the-art

performance, we do not include them in our study due to their already reported poor

performance in the real estate task (Bekoulis et al., 2017) compared to graph-based

parsers.

2.3. Joint learning

Adopting a pipeline strategy for the considered type of problems has two main draw-

backs: (i) sequence labeling errors propagate to the dependency parsing step, e.g., an

incorrectly identiﬁed part of the house (entity) could get connected to a truly existing

entity, and (ii) interactions between the components are not taken into account (feed-

back between the subtasks), e.g., modeling the relation between two potential entities

may help in deciding on the nature of the entities themselves. In more general rela-

tion extraction settings, a substantial amount of work (Kate & Mooney, 2010; Li & Ji,

2014; Miwa & Sasaki, 2014) jointly considered the two subtasks of entity recognition

and relation extraction. However, all of these models make use of hand-crafted features

that: (i) require manual feature engineering, (ii) generalize poorly between various ap-

plications and (iii) may require a substantial computational cost.

Recent advances on joint models for general relation extraction consider the joint

task using neural network architectures like LSTMs and CNNs (Miwa & Bansal, 2016;

Zheng et al., 2017; Li et al., 2017). Our work is however different from a typical rela-

tion extraction setup in that we aim to model directed spanning trees, or, equivalently,

non-projective dependency structures. In particular, the entities involved in a relation

are not necessarily adjacent in the text since other entities may be mentioned in be-

8

Entity type

Description

Examples

property
ﬂoor
space
subspace
ﬁeld

extra building

The property.
A ﬂoor in a building.
A room within the building.
A part of a room.
An open space inside or outside
the building.
An additional building which is
also part of the property.

bungalow, apartment
ground ﬂoor
bedroom, bathroom
shower, toilet
bbq, garden

garden house

Table 1: Real estate entity types.

tween, which complicates parsing. Indeed, in this work we focus on dependency pars-

ing due to the difﬁculty of establishing the tree-like structure instead of only relation

extraction (where each entity can have arbitrary relation arcs, regardless of other enti-

ties and their relations), which is the case for previously cited joint models. Moreover,

unlike most of these works that frame the problem as a stacking of the two components,

or at least ﬁrst train the NER module to recognize the entities and then further train to-

gether with the relation classiﬁcation module, we include the NER directly inside the

dependency parsing component.

In summary, the conceptual strengths of our joint segmentation and dependency

parsing approach (described in detail in Section 4) will be the following: compared to

state-of-the-art joint models in relation extraction, it (i) is generic in nature, without

requiring any manual feature engineering, (ii) extracts a complete tree structure rather

than a single binary relation instance.

3. Problem deﬁnition

In this section, we deﬁne the speciﬁc terms that are used in our real estate structured

prediction problem. We deﬁne an entity as an unambiguous, unique part of a property

with independent existence (e.g., bedroom, kitchen, attic). An entity mention is de-

ﬁned as one or more sequential tokens (e.g., “large apartment”) that can be potentially

linked to one or more entities. An entity mention has a unique semantic meaning and

refers to a speciﬁc entity, or a set of similar entities (e.g., “six bedrooms”). An entity

9

O r i g i n a l ad :
The p r o p e r t y i n c l u d e s a l a r g e
a p a r t m e n t w i t h a g a r a g e . The
home h a s a l i v i n g room , 3 s p a c i o u s b ed r o o m s and a b a t h r o o m .
The g a r a g e
e q u i p p e d w i t h a g a t e and a b i k e w a l l b r a c k e t .
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
S t r u c t u r e d r e p r e s e n t a t i o n :
p r o p e r t y
l a r g e

a p a r t m e n t

i s

l i v i n g room
3 s p a c i o u s b ed r o o m s
b a t h r o o m

g a r a g e

g a t e
b i k e w a l l b r a c k e t

‘ home ’

a p a r t m e n t ’ ,

| m e n t i o n = ‘ p r o p e r t y ’
| m e n t i o n = ‘ l a r g e
| m e n t i o n = ‘ l i v i n g room ’
| m e n t i o n = ‘3 s p a c i o u s bedrooms ’
| m e n t i o n = ‘ b ath r o o m ’
| m e n t i o n = ‘ g a r a g e ’
| m e n t i o n = ‘ g a t e ’
| m e n t i o n = ‘ b i k e w a l l b r a c k e t ’

Figure 1: Fictitious sample unstructured ad and corresponding structured representation as a property tree.

itself is part-of another entity and can be mentioned in the text more than once with

different entity mentions. For instance, a “house” entity could occur in the text with

entity mentions “large villa” and “a newly built house”. For the pipeline setting as pre-

sented in Bekoulis et al. (2017), we further classify entities into types (assign a named

entity type to every word in the ad). The task is transformed to a sequence labeling

problem using BIO (Beginning, Inside, Outside) encoding. The entity types are listed

in Table 1. For instance, in the sequence of tokens “large apartment”, B-PROPERTY is

assigned to the token “large” as the beginning of the entity, I-PROPERTY in the token

“apartment” as the inside of the entity but not the ﬁrst token within the entity and O

for all the other tokens that are not entities. Unlike previous studies (Miwa & Bansal,

2016; Zheng et al., 2017; Li et al., 2016, 2017), for our joint model there is no need for

this type of categorical classiﬁcation into labels since the two components are treated

uniﬁed as a single dependency parsing problem.

The goal of the real estate structured prediction task is to map the textual prop-

erty classiﬁed into a tree-like structured representation, the so-called property tree, as

illustrated in Fig. 1. In the pipeline setting, this conversion implies the detection of

(i) entities of various types and (ii) the part-of dependencies between them. For in-

stance, the entity “living room” is part-of the entity “large apartment”. In the joint

model, each token (e.g., “apartment”, “living”, “bathroom”, “includes”, “with”, “3”) is

examined separately and 4 different types of relations are deﬁned, namely part-of, seg-

10

ment, skip and equivalent. The part-of relation is similar to the way that it was deﬁned

in the pipeline setting but instead of examining entities, i.e., sequences of tokens (e.g.,

“living room”), we examine if a (individual) token is part-of another (individual) token

(e.g., “room” is part-of the “apartment”). We encode the entity identiﬁcation task with

the segment label and we follow the same approach as in the part-of relationships for

the joint model. Speciﬁcally, we examine if a token is a segment of another token (e.g.,

the token “room” is attached as a segment to the token “living”, “3” is attached as a seg-

ment to the token “bedrooms” and “spacious” is also attached as a segment to the token

“bedrooms” — this way we are able to encode the segment “3 spacious bedrooms”). By

doing so, we cast the sequence labeling subtask to a dependency parsing problem. The

tokens that are referring to the same entity belong to the equivalent relation (“home”

is equivalent to “apartment”). For each entity, we deﬁne the ﬁrst mention in order of

appearance in the text as main mention and the rest as equivalent to this main mention.

Finally, each token that does not have any of the aforementioned types of relations has

a skip relation with itself (e.g., “includes” has a skip relation with “includes”), such that

each token has a uniquely deﬁned head.

Thus, we cast the structured prediction task of extracting the property tree from the

ad as a dependency parsing problem, where (i) an entity can be part-of only one (other)

entity, because the decisions are taken simultaneously for all part-of relations (e.g., a

certain room can only be part-of a single ﬂoor), and (ii) there are a priori no restrictions

on the type of entities or tokens that can be part-of others (e.g., a room can be either

part-of a ﬂoor, or the property itself, like an apartment). It is worth mentioning that

dependency annotations for our problem exhibit a signiﬁcant number of non-projective

arcs (26%) where part-of dependencies are allowed to cross (see Fig. 3), meaning that

entities involved in the part-of relation are non-adjacent (i.e., interleaved by other en-

tities). For instance, all the entities or the tokens for the pipeline and the joint models,

that are attached to the entity “garage” are overlapping with the entities that are at-

tached to the entity “apartment”, making parsing even more complicated: handling

only projective dependencies as illustrated in Fig. 2 is an easier task. We note that the

segment dependencies do not suffer from non-projectivity, since the tokens are always

adjacent and sequential (e.g., “3 spacious bedrooms”).

11

0
property

1
large
apartment

2
living
room

3
3 spacious
bedrooms

4
bathroom

Figure 2: An example graph of projective part-of dependencies.

0
property

1
large
apartment

2
garage

3
living 
room

4
3 spacious
bedrooms

5
bathroom

6
bike wall
bracket

7
gate

Figure 3: Graph representing the part-of dependencies of Fig. 1. The dashed arcs are representing the
non-projective dependencies.

4. Methodology

We now describe the two approaches, i.e., the pipeline model and the joint model to

construct the property tree of the textual advertisements, as illustrated in Fig. 4. For

the pipeline system (Section 4.1), we (1) identify the entity mentions (Section 4.1.1),

then (2) predict the part-of dependencies between them (Section 4.1.2), and ﬁnally (3) con-

struct the tree representation (i.e., property tree) of the textual classiﬁed (e.g., as in

Fig. 1).

In step (2), we apply locally or globally trained graph-based models. We

represent the result of step (2) as a graph model, and then solve step (3) by apply-

ing the maximum spanning tree algorithm (Chu & Liu, 1965; Edmonds, 1967) for

the directed case (see McDonald et al. (2005)). We do not apply the well-known and

fast transition-based systems with hand-crafted features for non-projective dependency

structures (Nivre, 2009; Bohnet & Nivre, 2012), given the previously established poor

performance thereof in Bekoulis et al. (2017). In Section 4.2, we describe the joint

model where we perform steps (1) and (2) jointly. For step (3), we apply the maximum

spanning tree algorithm (Chu & Liu, 1965; Edmonds, 1967) similarly as in the pipeline

setting (Section 4.1).

4.1. Two-step pipeline

Below we revisit the pipeline approach presented in Bekoulis et al. (2017), which serves

as the baseline which we compare the neural models against. As mentioned before, the

pipeline model comprises two subtasks: (1) the sequence labeling and the (2) part-of

12

tree construction. In the following subsections, we describe the methods applied for

both.

4.1.1. Sequence labeling

The ﬁrst step in our pipeline approach is the sequence labeling subtask which is similar

to NER. Assuming a textual real estate classiﬁed, we (i) identify the entity mention

boundaries and (ii) map each identiﬁed entity mention to a categorical label, i.e., entity

type. In general, in the sequence labeling tasks, it is beneﬁcial to take into account

the correlations between labels in adjacent tokens, i.e., consider the neighborhood, and

jointly ﬁnd the most probable chain of labels for the given input sentence (Viterbi

algorithm for the most probable assignment). For instance, in our problem where we

follow the NER standard BIO encoding (Ratinov & Roth, 2009), the I-PROPERTY

cannot be followed by I-SPACE without ﬁrst opening the type by B-SPACE. We use

a special case of the CRF algorithm (Lafferty et al., 2001; Peng & McCallum, 2006),

namely linear chain CRFs, which is commonly applied in the problem of sequence

labeling to learn a direct mapping from the feature space to the output space (types)

where we model label sequences jointly, instead of decoding each label independently.

A linear-chain CRF with parameters w deﬁnes a conditional probability Pw(y|x) for

the sequence of labels y = y1, ..., yN given the tokens of the text advertisement x =

x1, ..., xN to be

Pw(y|x) =

exp(wT φ(x, y)),

1
Z(x)

(1)

where Z is the normalization constant and φ is the feature function that computes a

feature vector given the advertisement and the sequence of labels.

4.1.2. Part-of tree construction

The aim of the part-of tree construction subtask is to link each entity to its parent.

We approach the task as a dependency parsing problem but instead of connecting each

token to its syntactical parent, we map only the entity set I (e.g., “large villa”, “3

spacious bedrooms”) that has already been extracted by the sequence labeling subtask

to a dependency structure y. Assuming the entity set I = {e0, e1, ..., et} where t is the

number of identiﬁed entities, a dependency is a pair (p, c) where p ∈ I is the parent

13

two-step pipeline

(1) entity
recognition

(2) part-of RE

(1+2) joint entity recognition
& part-of RE

.
.
.

(3) tree
construction

joint model

Figure 4: The full structured prediction system setup.

entity and c ∈ I is the child entity. The entity e0 is the dummy root-symbol that only

appears as parent.

We will compare two approaches to predict the part-of relations: a locally trained

model (LTM) scoring all candidate edges independently, versus a global model (MTT)

which jointly scores all edges as a whole.

Locally trained model (LTM)

In the locally trained model (LTM), we adopt a traditional local discriminative method

and apply a binary classiﬁcation framework (Yamada & Matsumoto, 2003) to learn the

part-of relation model (step (2)), based on standard relation extraction features such

as the parent and child tokens and their types, the tokens in between, etc. For each

candidate parent-child pair, the classiﬁer gives a score that indicates whether it is prob-

able for the part-of relation to hold between them. The output scores are then used

for step (3), to construct the ﬁnal property tree. Following McDonald et al. (2005);

McDonald & Pereira (2007), we view the entity set I as a fully connected directed

graph G = {V, E} with the entities e1, ..., et as vertices (V ) in the graph G, and edges

E representing the part-of relations with the respective classiﬁer scores as weights.

One way to approach the problem is the greedy inference method where the predic-

tions are made independently for each parent-child pair, thus neglecting that the global

target output should form a property tree. We could adopt a threshold-based approach,

i.e., keep all edges exceeding a threshold, which obviously is not guaranteed to end

up with arc dependencies that form a tree structure (i.e., could even contain cycles).

On the other hand, we can enforce the tree structure inside the (directed) graph by

14

ﬁnding the maximum spanning tree. To this end, similar to McDonald et al. (2005);

McDonald & Pereira (2007), we apply the Edmonds’ algorithm to search for the most

probable non-projective tree structure in the weighted fully connected graph G.

Globally trained model (MTT)

The Matrix-Tree theorem (MTT) (Koo et al., 2007) is a globally normalized statistical

method that involves the learning of directed spanning trees. Unlike the locally trained

models, MTT is able to learn tree dependency structures, i.e., scoring parse trees for

a given sentence. We use D(I) to refer to all possible dependencies of the identiﬁed

entity set I, in which each dependency is represented as a tuple (h, m) in which h is

the head (or parent) and m the modiﬁer (or child). The set of all possible dependency

structures for a given entity set I is written T (I). The conditional distribution over all

dependency structures y ∈ T (I) can then be deﬁned as:

P (y|I; θ) =

1
Z(I; θ)

exp 


X
h,m∈y

θh,m





(2)

in which the coefﬁcients θh,m ∈ R for each dependency (h, m) form the real-valued

weight vector θ. The partition function Z(I; θ) is a normalization factor that alas can-

not be computed by brute-force, since it requires a summation over all y ∈ T (I),

containing an exponential number of possible dependency structures. However, an

adaptation of the MTT allows us the direct and efﬁcient computation of the partition

function Z(I; θ) as the determinant det(L(θ)) where L(θ) is the Laplacian matrix of

the graph. It is worth mentioning that although MTT learns spanning tree structures

during training, at the prediction phase, it is still required to use the maximum spanning

tree algorithm (step (3)) (McDonald et al., 2005; McDonald & Pereira, 2007) as in the

locally trained models.

4.2. Joint model

In this section, we present the new joint model sketched in Fig. 5, which simultaneously

predicts the entities in the sentence and the dependencies between them, with the ﬁnal

goal of obtaining a tree structure, i.e., the property tree. We pose the problem of the

identiﬁcation of the entity mentions and the dependency arcs between them as a head

15

Figure 5: The architecture of the joint model.

selection problem (Zhang et al., 2017). Speciﬁcally, given as input a sentence of length

N , the model outputs the predicted parent of each token of the advertisement and the

most likely dependency label between them. We begin by describing how the tokens

are represented in the model, i.e., with ﬁxed pre-trained embeddings (Section 4.2.1),

which form the input to an LSTM layer (Section 4.2.2). The LSTM outputs are used

as input to the entity and dependency scoring layer (Section 4.2.3). As an extension of

this model, we propose the use of various attention layers in between the LSTM and

scoring layer, to encourage the model to focus on salient information, as described in

Section 4.2.4. The ﬁnal output of the joint model still is not guaranteed to form a tree

structure. Therefore, we still apply Edmonds’ algorithm (i.e., step (3) from the pipeline

approach), described in Section 4.2.5.

4.2.1. Embedding Layer

The embedding layer maps each token of the input sequence x1, ..., xN of the consid-

ered advertisement to a low-dimensional vector space. We obtain the word-level em-

beddings by training the Skip-Gram word2vec model (Mikolov et al., 2013) on a large

collection of property advertisements. We add a symbol x0 in front of the N -length

input sequence, which will act as the root of the property tree, and is represented with

16

an all-zeros vector in the embedding layer.

4.2.2. Bidirectional LSTM encoding layer

Many neural network architectures have been proposed in literature: LSTMs (Hochreiter & Schmidhuber,

1997), CNNs (LeCun et al., 1989), Echo State Networks (Jaeger, 2010), or Stochastic

Conﬁguration Networks (Wang & Li, 2017), to name only a few. Many others can

be found in reference works on the topic (Goodfellow et al., 2016; Goldberg & Hirst,

2017). In this work, we use RNNs which have been proven to be particularly effective

in a number of NLP tasks (Sutskever et al., 2014; Lample et al., 2016; Miwa & Bansal,

2016). Indeed, RNNs are a common and reasonable choice to model sequential data

and inherently able to cope with varying sequence lengths. Yet, plain vanilla RNNs

tend to suffer from vanishing/exploding gradient problems and are hence not success-

ful in capturing long-term dependencies (Bengio et al., 1994; Pascanu et al., 2013).

LSTMs are a more advanced kind of RNNs, which have been successfully applied

in several tasks to capture long-term dependencies, as they are able to effectively over-

come the vanishing gradient problem. For many NLP tasks, it is crucial to represent

each word in its own context, i.e., to consider both past (left) and future (right) neigh-

boring information. An effective solution to achieve this is using a bidirectional LSTM

(BiLSTM). The basic idea is to encode each sequence from left to right (forward) and

from right to left (backward). This way, there is one hidden state which represents the

past information and another one for the future information. The high-level formulation

of an LSTM is:

hi, ci = LSTM(wi, hi−1, ci−1),

i = 0, ..., N

(3)

where in our setup wi ∈ R ˜d is the word embedding for token xi, and with the input and
states for the root symbol x0 initialized as zero vectors. Further, hi ∈ Rd and ci ∈ Rd

respectively are the output and cell state for the ith position, where d is the hidden state

size of the LSTM. Note that we chose the word embedding size the same as the LSTM
hidden state size, or ˜d = d. The outputs from left to right (forward) are written as
~hi and the outputs from the backwards direction as ~hi. The two LSTMs’ outputs at

17

position i are concatenated to form the output hi at that position of the BiLSTM:

hi = [ ~hi; ~hi],

i = 0, ..., N

(4)

4.2.3. Joint learning as head selection

In this subsection, we describe the joint learning task (i.e., identifying entities and

predicting dependencies between them), which we formulate as a head selection prob-

lem (Zhang et al., 2017). Indeed, each word xi should have a unique head (parent)

— while it can have multiple dependent words — since the ﬁnal output should form

the property tree. Unlike the standard head selection dependency parsing framework

(Zhang et al., 2017), we predict the head yi of each word xi and the relation ci between

them jointly, instead of ﬁrst obtaining binary predictions for unlabeled dependencies,

followed by an additional classiﬁer to predict the labels.

Given a text advertisement as a token sequence x = x0, x1, ..., xN where x0 is the

dummy root symbol, and a set C = {part-of, segment, equivalent, skip} of predeﬁned

labels (as deﬁned in Section 3), we aim to ﬁnd for each token xi, i ∈ {0, ..., N } the

most probable head xj , j ∈ {0, ..., N } and the most probable corresponding label c ∈

C. For convenience, we order the labels c ∈ C and identify them as ck, k ∈ {0, ..., 3}.

We model the joint probability of token xj to be the head of xi with ck the relation

between them, using a softmax:

P (head = xj, label = ck|xi) =

exp(score(hj , hi, ck))
˜j,˜k exp(score(h˜j , hi, c˜k)

P

(5)

where hi and hj are the BiLSTM encodings for words xi and xj, respectively. For

the scoring formula score(hj , hi, ck) we use a neural network layer that computes the

relative score between position i and j for a speciﬁc label ck as follows:

score(hj , hi, ck) = V T

k tanh(Ukhj + Wkhi + bk)

(6)

with trainable parameters Vk ∈ Rl, Uk ∈ Rl×2d, Wk ∈ Rl×2d, bk ∈ Rl, and l the

layer width. As detailed in Section 5.1, we set l to be smaller than 2d, similar to

Dozat & Manning (2017) due to the fact that training on superﬂuous information re-

duces the parsing speed and increases tendency towards overﬁtting. We train our model

18

by minimizing the cross-entropy loss L, written for the considered training instance as:

L =

− log P (head = yi, label = ci|xi)

(7)

N

X
i=0

where yi ∈ x and ci ∈ C are the ground truth head and label of xi, respectively. After

training, we follow a greedy inference approach and for each token, we simultaneously

keep the highest scoring head ˆyi and label ˆci for xi based on their estimated joint

probability:

( ˆyi, ˆci) = argmax
xj ∈x,ck∈C

P (head = xj, label = ck|xi)

(8)

The predictions ( ˆyi, ˆci) are made independently for each position i, neglecting that

the ﬁnal structure should be a tree. Nonetheless, as demonstrated in Section 5.2, the

highest scoring neural models are still able to come up with a tree structure for 78%

of the ads. In order to ensure a tree output in all cases, however, we apply Edmonds’

algorithm on the output.

4.2.4. Attention Layer

The attention mechanism in our structured prediction problem aims to improve the

model performance by focusing on information that is relevant to the prediction of the

most probable head for each token. As attention vector, we construct the new context
vector h∗

i as a weighted average of the BiLSTM outputs

in which the coefﬁcients a(hj, hi), also called the attention weights, are obtained as

h∗
j =

N

X
i=0

a(hj, hi) hi

a(hj, hi) =

exp(att(hj, hi))
N
˜i=0 exp(att(hj , h˜i))

.

P

(9)

(10)

The attention function att(hj, hi) is designed to measure some form of compatibility

between the representation hi for xi and hj for xj, and the attention weights a(hj, hi)

are obtained from these scores by normalization using a softmax function. In the fol-

lowing, we will describe in detail the various attention models that we tested with our

follows:

joint model.

19

Commonly used attention mechanisms

Three commonly used attention mechanisms are listed in eqs. (11) to (13): the addi-

tive (Vinyals et al., 2015), bilinear, and multiplicative attention models (Luong et al.,

2015), which have been extensively used in machine translation. Given the represen-

tations hi and hj for tokens xi and xj, we compute the attention scores as follows:

attadditive(hj, hi) = Va tanh(Uahj + Wahi + ba)

attbilinear(hj, hi) = hT

j Wbilhi

attmultiplicative(hj, hi) = hT

j hi

where Va ∈ Rl, Ua, Wa ∈ Rl×2d, Wbil ∈ R2d×2d and ba ∈ Rl are learnable parame-

ters of the model.

Biafﬁne attention

We use the biafﬁne attention model (Dozat & Manning, 2017) which has been recently

applied to dependency parsing and is a modiﬁcation of the neural graph-based approach

that was proposed by Kiperwasser & Goldberg (2016). In this model, Dozat & Manning

(2017) tried to reduce the dimensionality of the recurrent state of the LSTMs by apply-

ing a such neural network layer on top of them. This idea is based on the fact that there

is redundant information in every hidden state that (i) reduces parsing speed and (ii) in-

creases the risk of overﬁtting. To address these issues, they reduce the dimensionality

and apply a nonlinearity afterwards. The deep bilinear attention mechanism is deﬁned

as follows:

(11)

(12)

(13)

(14)

(15)

hdep
i = Vdep tanh(Udephi + bdep)

hhead
j = Vhead tanh(Uheadhj + bhead)

20

function:

Edge attention

to be:

attbiafﬁne(hhead

j

, hdep

i ) = (hhead

j

)T Wbilhdep

i + Bhhead

j

(16)

where Udep, Uhead ∈ Rl×2d, Vdep, Vhead ∈ Rp×l, Wbil ∈ Rp×p, B ∈ Rp and bdep,
bhead ∈ Rl.

Tensor attention

This section introduces the Neural Tensor Network (Socher et al., 2013) that has been

used as a scoring formula applied for relation classiﬁcation between entities. The task

can be described as link prediction between entities in an existing network of relation-

ships. We apply the tensor scoring formula as if tokens are entities, by the following

atttensor(hj, hi) = Ut tanh(hT

j Wthi + Vt(hj + hi) + bt)

(17)

where Wt ∈ R2d×l×2d, Vt ∈ Rl×2d, Ut ∈ Rl and bt ∈ Rl.

In the edge attention model, we are inspired by Gilmer et al. (2017), which applies

neural message passing in chemical structures. Assuming that words are nodes inside

the graph and the message ﬂows from node xi to xj, we deﬁne the edge representation

(18)

(19)

edge(hj, hi) = tanh(Uehj + Wehi + be)

The edge attention formula is computed as:

attedge(hj, hi) =

Asrc

edge(hj, h˜i) + Adst

1
N





N

X
˜i=0

N

X
˜j=0

edge(h˜j, hi)


where Ue, We ∈ Rl×2d, Asrc, Adst ∈ R2d×l and be ∈ Rl. The source and destination

matrices respectively encode information for the start to the end node, in the directed

edge. Running the edge attention model for several times can be achieved by stacking

the edge attention layer multiple times. This is known as message passing phase and

we can run it for several (T > 1) time steps to obtain more informative edge represen-

tations.

21

4.2.5. Tree construction step: Edmonds’ algorithm

At decoding time, greedy inference is not guaranteed to end up with arc dependencies

that form a tree structure and the classiﬁcation decision might contain cycles. In this

case, the output can be post-processed with a maximum spanning tree algorithm (as

the third step in Fig. 4). We construct the fully connected directed graph G = (V, E)

where the vertices V are the tokens of the advertisement (that are not predicted as

skips) and the dummy root symbol, E contains the edges representing the highest scor-

ing relation (e.g., part-of, segment, equivalent) with the respective cross entropy scores

serving as weights. Since G is a directed graph, s(xi, xj) is not necessarily equal to

s(xj, xi). Similar to McDonald et al. (2005), we employ Edmonds’ maximum span-

ning tree algorithm for directed graphs (Chu & Liu, 1965; Edmonds, 1967) to build a

non-projective parser. Indeed, in our setting, we have a signiﬁcant number (26% in

the dataset used for experiments, see further) of non-adjacent part-of and equivalent

relations (non-projective). It is worth noting that in the case of segment relations, the

words involved are not interleaved by other tokens and are always adjacent. We ap-

ply Edmonds’ algorithm to every graph which is constructed to get the highest scoring

graph structure, even in the cases where a tree is already formed by greedy inference.

For skips, we consider the predictions as obtained from the greedy approach and we do

not include them in the fully connected weighted graph, since Edmonds’ complexity is
O(n2) for dense graphs and might lead to slow decoding time.

In this section, we present the experimental results of our study. We describe the

dataset, the setup of the experiments and we compare the results of the methods anal-

5. Results and discussion

ysed in the previous sections.

5.1. Experimental setup

Our dataset consists of a large collection (i.e., 887,599) of Dutch property advertise-

ments from real estate agency websites. From this large dataset, a sub-collection of

2,318 classiﬁeds have been manually annotated by 3 trained human annotators (1 an-

notation per ad, 773 ads per annotator). The annotations follow the format of the

22

property tree that is described in detail in Section 3 and is illustrated in Fig. 1. The

dataset is available for research purposes, see our github codebase.1

In the experi-

ments, we use only the annotated text advertisements for the pipeline setting, i.e., LTM

(locally trained model), MTT (globally trained model). In the case of the neural net-

work approach, we train the embeddings on the large collection by using the word2vec

model (Mikolov et al., 2013) whereas in the joint learning, we use only the annotated

documents, similar to the pipeline approach. The code of the LTM and the MTT hand-

crafted systems is available on github.1 We also use our own CRF implementation. The

code for the joint model has been developed in Python with the Tensorﬂow machine

learning library (Abadi et al., 2016) and will be made public as well. For the evalu-

ation, we use 70% for training, 15% for validation and 15% as test set. We measure

the performance by computing the F1 score on the test set. The accuracy metric can

be misleading in our case since we have to deal with imbalanced data (the skip label

is over-represented). We only report numbers on the structured classes, i.e., segment

and part-of since the other dependencies (skip, equivalent) are auxiliary in the joint

models and do not directly contribute to the construction of the actual property tree.

For the overall F1, we are again only considering the structured classes. Finally, we

report the number of property trees (which shows how likely our model is to produce

trees without applying Edmonds’ algorithm, i.e., by greedy inference alone) for all the

models before applying Edmonds’ algorithm that guarantees the tree structure of the

predictions.

For the pipeline models, we train the CRF with regularization parameter λCRF =

10 and the LTM and MTT with C = 1 based on the best hyperparameters on the val-

idation set. As binary classiﬁer, we use logistic regression. For the joint model, we

train 128-dimensional word2vec embeddings on a collection of 887k advertisements.

In general, using larger embeddings dimensions (e.g., 300), does not affect the perfor-

mance of our models. We consistently used single-layer LSTMs through our experi-

ments to keep our model relatively simple and to evaluate the various attention meth-

ods on top of that. We have also reported results on the joint model using a two-layer

1https://github.com/bekou/ad_data

23

Precision

Recall

segment

part-of

segment

part-of

segment

Overall

Trees
(% of ads)

-
d
n
a
H

d
e
t
f
a
r
c

M
T
S
L

e
v
i
t
n
e
t
t

A

M
T
S
L

Model

LTM
MTT

LSTM
LSTM+E
2xLSTM+E

Additive
Bilinear
Multiplicative
Biafﬁne
Tensor
Edge1
Edge2
Edge3

73.77
73.77

70.24
70.18
73.91

72.97
70.25
71.12
70.01
71.53
71.56
72.03
71.74

60.53
61.15

65.23
63.92
69.88

65.71
66.34
66.40
64.67
64.68
67.46
66.09
67.69

70.98
70.98

77.73
77.77
75.78

76.45
79.96
77.81
78.32
76.17
78.24
75.35
78.44

60.40
61.01

70.32
71.08
71.22

70.90
72.53
71.26
71.04
70.79
71.31
70.99
73.00

72.35
72.35

73.80
73.78
74.83

74.67
74.79
74.31
73.93
73.78
74.75
73.65
74.94

F1 (%)
part-of

60.47
61.08

67.68
67.31
70.54

68.21
69.29
68.75
67.71
67.60
69.33
68.46
70.25

64.76
65.15

68.82
68.57
70.90

69.46
70.20
69.70
68.75
68.68
70.08
69.12
70.70

37.18
43.23

68.30
68.30
78.09

74.35
72.62
72.91
74.06
69.16
70.32
73.48
78.96

Table 2: Performance of the three approaches on the structured prediction task. The top rows are for the
pipeline approach, i.e., hand-crafted features. The next block of results presents the results for the neural
joint model based on LSTMs. The bottom block contains results of the joint models augmented with several
attentive architectures. Edmonds’ algorithm is applied in all of the models to retain the tree structure, except
for the LSTM joint model. The LSTM+E is the LSTM model with Edmonds’ algorithm included. The
2xLSTM+E is the same joint model but it simply uses a stack of two LSTM layers. In the experiments with
attention, we use a one-stack LSTM. The rightmost column is the percentage of the ads that are valid trees
before applying Edmonds’ (i.e., step (3) of Fig. 4), showing the ability of the model to form trees during
greedy inference. In the Edgei models, the number i stands for the number of times that we have run the
message passing phase.

stacked LSTM joint model, although it needs a higher computation time compared to

a single-layer LSTM with an attention layer on top. The hidden size of the LSTMs is

d = 128 and the size of the neural network used in the scoring and the attention layer is

ﬁxed to l = 32. The optimization algorithm used is Adam (Kingma & Ba, 2015) with
a learning rate of 10−3. To reduce the effect of overﬁtting, we regularize our model

using the dropout method (Srivastava et al., 2014). We ﬁx the dropout rate on the in-

put of the LSTM layer to 0.5 to obtain signiﬁcant improvements (∼1%-2% F1 score

increase, depending on the model). For the two-layer LSTM, we ﬁx the dropout rate

to 0.3 in each of the input layers since this leads to largest performance increase on the

validation set. We have also explored gradient clipping without further improvement

on our results. In the joint model setting, we follow the evaluation strategy of early

stopping (Caruana et al., 2000; Graves et al., 2013) based on the performance of the

validation set. In most of the experiments, we obtain the best hyperparameters after

∼60 epochs.

24

5.2. Comparison of the pipeline and the joint model

One of the main contributions of our study is the comparison of the pipeline approach

and the proposed joint model. We formulated the problem of identifying the enti-

ties (i.e., segments) and predicting the dependencies between them (i.e., construc-

tion of the property tree) as a joint model. Our neural model, unlike recent stud-

ies (Miwa & Bansal, 2016; Zheng et al., 2017) on joint models that use LSTMs to

handle similar tasks, does not need two components to model the problem (i.e., NER

and dependency parsing). To the best of our knowledge, our study is the ﬁrst that for-

mulates the task in an actual joint setting without the need to pre-train the sequence

labeling component or for parameter sharing between them, since we use only one

component for both subtasks. In Table 2, we present the results of the pipeline model

(hand-crafted) and the proposed joint model (LSTM). The improvement of the joint

model over the pipeline is unambiguous, i.e., 3.42% overall F1 score difference be-

tween MTT (highest scoring pipeline model) and LSTM+E (LSTM model with Ed-

monds’ algorithm). An additional increase of ∼2.3% is achieved when we consider

two-layer LSTMs (2xLSTM+E) for our joint model. All results in Table 2, except for

the LSTM, are presented using Edmonds’ algorithm on top, to construct the property

tree. Examining each label separately, we observe that the original LSTM+E model

(73.78%) performs better by 1.43% in entity segmentation than the CRF (72.35%).

The LSTM model achieves better performance in the entity recognition task since it

has to learn the two subtasks simultaneously resulting in interactions between the com-

ponents (i.e., NER and dependency parser). This way, the decisions for the entity

recognition can beneﬁt from predictions that are made for the part-of relations. Con-

cerning the part-of dependencies, we note that the LSTMs outperform the hand-crafted

approaches by 6.23%. Also, the number of valid trees that are constructed before ap-

plying Edmonds’ algorithm is almost twice as high for the LSTM models. Stacking

two-layer LSTMs results in an additional ∼1% improvement in the segmentation task

and ∼3% in the part-of relations. The greedy inference for the hand-crafted meth-

ods does not produce well-formed trees, meaning that post-processing with Edmonds’

algorithm (enforce tree structure) is expected to increase the performance of the hand-

crafted models compared to the LSTM model performance. Indeed, the performance

25

of the feature based hand-crafted models (i.e., LTM and MTT) without the Edmonds’

on top is not reported in Table 2 due to their poor performance in our task (i.e., ∼60%

overall F1 and ∼51% for part-of ), but after post-processing with Edmonds’ the perfor-

mance signiﬁcantly increases (i.e., ∼65%). On the other hand, applying the Edmonds’

algorithm on the LSTM model leads to marginally decreased performance (∼0.2%)

compared to the original LSTM model, probably indicating that enforcing structural

constraints is not beneﬁcial for a model that clearly has the ability to form valid tree

structures during greedy inference. Although one might be tempted not to enforce the

tree structure (post-process with Edmonds’), due to the nature of our problem, we have

to enforce tree constraints in all of the models.

5.3. Comparison of the joint and the attention model

After having established the superior performance of neural approach using LSTMs

over the more traditional (LTM and MTT) methods based on hand-crafted features, we

now discuss further improvements using attentive models. The attention mechanisms

are designed to encourage the joint model to focus on informative tokens. We exploited

several attention mechanisms as presented in Section 4.2.4. Table 2 shows the perfor-

mance of the various models. Overall, the attention models are performing better in

terms of overall F1 score compared to the original joint model with the Edmonds’ on

top. Although the performance of the Biafﬁne and the Tensor models is limited com-

pared to the improvement of the other attentive models, we focus on: (i) the Biafﬁne

model since it achieved state-of-the-art performance on the dependency parsing task

and (ii) the Tensor model because we were expecting that it would perform similarly

to the Bilinear model (it has a bilinear tensor layer). Despite its simplicity, the Bilinear

model is the second best performing attentive model in Table 2 in terms of overall F1

score. Edge3 (70.70% overall F1 score) achieves better results than the other attention

mechanisms in the entity recognition and in the dependency parsing tasks. We observe

that running the message passing step multiple times in the Edge model, gives an in-

creasing trend in the number of valid trees that were constructed before applying the

maximum spanning tree algorithm. This is not surprising since we expect that running

the message passing phase multiple times leads to improved edge representations. The

26

maximum number of trees without post-processing by Edmonds’ is attained when we

run the message passing for 3 times whereas further increasing the number beyond 3

(e.g., 4) appears no longer beneﬁcial. Stacking a second LSTM layer on top of the joint

model (2xLSTM+E) marginally improves the performance by 0.2% compared to the

Edge3 attention model. But adding a second LSTM layer comes with the additional

cost of an increased computation time compared to the joint models with the attention

layers on top. This illustrates that: (i) there might be some room for marginally im-

proving the attention models even further, and (ii) we do not have to worry about the

quadratic nature of our approach since in terms of speed the attentive models are able

to surpass the two-layer LSTMs. The sequential processing of the LSTMs might be

the reason that slows down the computation time for the 2xLSTM over the rest of the

attentive models. Speciﬁcally, on an Intel(R) Xeon(R) CPU E5-2650 v2 @ 2.60GHz

processor, the best performing model (i.e., Edge3) takes ∼2 minutes per epoch while

in the 2xLSTM case, it takes ∼2.5 minutes leading to a slowdown of ∼25%. The

percentage of the ads that are valid trees is 1% better in the Edge3 over the two-layer

LSTM showcasing the ability of the Edge model to form more valid trees during greedy

inference.

5.4. Discussion

In this section, we discuss some additional aspects of our problem and the approaches

that we follow to handle them. As we mentioned before, a single entity can be present

in the text with multiple mentions. This brings an extra difﬁculty to our task. For

instance, in the example of Fig. 1, the entity “large apartment” is expressed in the ad

with the mentions “large apartment” and “home”. Hence it is confusing to which men-

tion the other entities should be attached to. One way would be to attach them to both

and then eliminate one of the connections using Edmonds’ spanning tree algorithm,

which is the approach adopted in Bekoulis et al. (2017). The problematic issue with

this approach is that the spanning tree algorithm would randomly remove all mentions

but one, possibly resulting in uncertain outcomes. To avoid this problem, we now use

as the main mention for an entity the ﬁrst mention in order of appearance in the text

(e.g., “large apartment” in our example) and the remaining mentions (e.g., “home”)

27

are attached as equivalent mentions to the main one. Usually, the most informative

mention for an entity is the one that appears ﬁrst, because we again refer to an entity

mentioned before, often with a shorter description. Following our intuition, the neural

model increases its overall performance by ∼3% (from 66% to 69% and more than 5%

in the part-of relation) and the pipeline approaches by almost 4% (from 61%, reported

in Bekoulis et al. (2017) to 65% and more than 5% in the part-of relation).

We also experimented with introducing the equivalent relations. Although it is

a strongly under-represented class in the dataset and the model performs poorly for

this label (an equivalent edge F1 score of 10%), introducing the equivalent label is

the natural way of modeling our problem (i.e., assigning each additional mention as

equivalent to the main mention). We ﬁnd out that introducing this type of relation

leads to a slight decrease (∼1%) in the part-of and a marginal increase (∼0.3%) in

the segment relations which are the main relations while retaining the nature of our

problem. In the pipeline approach, it results in an 9% drop in the F1 score of the part-

of relation. This is the reason that the results as presented in Table 2 do not consider

the equivalent relation for the hand-crafted model to make a fair comparison in the

structured classes.

We believe our experimental comparison of the various architectural model vari-

ations provides useful ﬁndings for practitioners. Speciﬁcally, for applications requir-

ing both segmentation (entity recognition) and dependency parsing (structured predic-

tion), our ﬁndings can be qualitatively summarized as follows: (i) joint modeling is

the most appropriate approach since it reduces error propagation between the com-

ponents, (ii) the LSTM model is much more effective (than models relying on hand-

crafted features) because it automatically extracts informative features from the raw

text, (iii) attentive models are proven effective because they encourage the model to fo-

cus on salient tokens, (iv) the edge attention model leads to an improved performance

since it better encodes the information ﬂow between the entities by using graph rep-

resentations, and (v) stacking a second LSTM marginally increases the performance,

suggesting that there might be some room for slight improvement of the attention mod-

els by adding LSTM layers.

Finally, we point out how exactly our model relates to state-of-the-art in the ﬁeld.

28

Our joint model is able to both extract entity mentions (i.e., perform segmentation)

and do dependency parsing, which we demonstrate on the real estate problem. Previ-

ous studies (Kate & Mooney, 2010; Li & Ji, 2014; Miwa & Sasaki, 2014) that jointly

considered the two subtasks (i.e., segmentation and relation extraction): (i) require

manual feature engineering and (ii) generalize poorly between various applications.

On the other hand, in our work, we rely on neural network methods (i.e., LSTMs) to

automatically extract features from the real estate textual descriptions and perform the

two tasks jointly. Although there are other methods which use neural network archi-

tectures (Miwa & Bansal, 2016; Zheng et al., 2017; Li et al., 2017) that focus on the

relation extraction problem, our work is different in that we aim to model directed

spanning trees and thus to solve the dependency parsing problem which is more con-

strained and difﬁcult (than extracting single instances of binary relations). Moreover,

the cited methods require either parameter sharing or pre-training of the segmenta-

tion module, which complicates learning. Therefore, cited methods are not directly

comparable to our model and cannot be applied to our real estate task out-of-the-box.

However, our model’s main limitation is the quadratic scoring layer that increases the

time complexity of the segmentation task from linear (which is the complexity of a
conditional random ﬁeld, CRF) to O(n2). As a result, it sacriﬁces standard linear com-

plexity of the segmentation task, in order to reduce the error propagation between the

subtasks and thus perform learning in a joint, end-to-end differentiable, setting.

6. Conclusions

In this paper, we proposed an LSTM-based neural model to jointly perform segmenta-

tion and dependency parsing. We apply it to a real estate use case processing textual

ads, thus (1) identifying important entities of the property (e.g., rooms) and (2) struc-

turing them into a tree format based on the natural language description of the prop-

erty. We compared our model with the traditional pipeline approaches that have been

adapted to our task and we reported an improvement of 3.4% overall edge F1 score.

Moreover, we experimented with different attentive architectures and stacking of a sec-

ond LSTM layer over our basic joint model. The results indicate that exploiting atten-

29

tion mechanisms that encourage our model to focus on informative tokens, improves

the model performance (increase of overall edge F1 score with ∼2.1%) and increases

the ability to form valid trees in the prediction phase (4% to 10% more valid trees for

the two best scoring attention mechanisms) before applying the maximum spanning

tree algorithm.

The contribution of this study to the research in expert and intelligent systems is

three-fold: (i) we introduce a generic joint model, simultaneously solving both sub-

tasks of segmentation (i.e., entity extraction) and dependency parsing (i.e., extracting

relationships among entities), that unlike previous work in the ﬁeld does not rely on

manually engineered features, (ii) in particular for the real estate domain, extracting

a structured property tree from a textual ad, we reﬁne the annotations and addition-

ally propose attention models, compared to initial work on this application, and ﬁ-

nally (iii) we demonstrate the effectiveness of our proposed generic joint model with

extensive experiments (see aforementioned F1 improvement of 2.1%). Despite the

experimental focus on the real estate domain, we stress that the model is generic in na-

ture, and could be equally applied to other expert system scenarios requiring the general

tasks of both detecting entities (segmentation) and establishing relations among them

(dependency parsing). We furthermore note that our model, rather than focusing on

extracting a single binary relation from a sentence (as in traditional relation extraction

settings), produces a complete tree structure.

Future work can evaluate the value of our joint model we introduced in other spe-

ciﬁc application domains (e.g., biology, medicine, news) for expert and intelligent sys-

tems. For example, the method can be evaluated for entity recognition and binary

relation extraction (the ACE 04 and ACE 05 datasets; see Miwa & Bansal (2016)) or in

adverse drug effects from biomedical texts (see Li et al. (2016)). In terms of model ex-

tensions and improvements, one research issue is to address the time complexity of the

NER part by modifying the quadratic scoring layer for this component. An additional

research direction is to investigate different loss functions for the NER component (e.g.,

adopting a conditional random ﬁeld (CRF) approach), since this has been proven ef-

fective in the NER task on its own (Lample et al., 2016). A ﬁnal extension we envision

is to enable multi-label classiﬁcation of relations among entity pairs.

30

The presented research was partly performed within the MALIBU project, funded by

Flanders Innovation & Entrepreneurship (VLAIO) contract number IWT 150630.

Acknowledgments

References

Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat,

S., Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga, R., Moore, S., Murray,

D. G., Steiner, B., Tucker, P., Vasudevan, V., Warden, P., Wicke, M., Yu, Y., & Zheng,

X. (2016). Tensorﬂow: A system for large-scale machine learning. In Proceedings

of the 12th USENIX Conference on Operating Systems Design and Implementation

(pp. 265–283). Berkeley, CA, USA.

Andor, D., Alberti, C., Weiss, D., Severyn, A., Presta, A., Ganchev, K., Petrov, S.,

& Collins, M. (2016). Globally normalized transition-based neural networks.

In

Proceedings of the 54th Annual Meeting of the Association for Computational Lin-

guistics (Volume 1: Long Papers) (pp. 2442–2452). Berlin, Germany.

Atkinson, J., & Bull, V. (2012). A multi-strategy approach to biological named

entity recognition. Expert Systems with Applications, 39(17), 12968 – 12974.

doi:10.1016/j.eswa.2012.05.033.

Bach, N., & Badaskar, S. (2007). A review of relation extraction. Literature review for

Language and Statistics II, .

Bekoulis, G., Deleu, J., Demeester, T., & Develder, C. (2017). Reconstructing the

house from the ad: Structured prediction on real estate classiﬁeds. In Proceedings of

the 15th Conference of the European Chapter of the Association for Computational

Linguistics: (Volume 2, Short Papers) (pp. 274–279). Valencia, Spain.

Bengio, Y., Simard, P., & Frasconi, P. (1994). Learning long-term dependencies

with gradient descent is difﬁcult. Transactions on neural networks, 5(2), 157–166.

doi:10.1109/72.279181.

31

Bohnet, B., & Nivre, J. (2012). A transition-based system for joint part-of-speech tag-

ging and labeled non-projective dependency parsing.

In Proceedings of the 2012

Joint Conference on Empirical Methods in Natural Language Processing and Com-

putational Natural Language Learning (pp. 1455–1465). Jeju Island, Korea: Asso-

ciation for Computational Linguistics.

Carreras, X. (2007). Experiments with a higher-order projective dependency parser.

In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Lan-

guage Processing and Computational Natural Language Learning (pp. 957–961).

Prague, Czech: Association for Computational Linguistics.

Caruana, R., Lawrence, S., & Giles, L. (2000). Overﬁtting in neural nets: Backpropa-

gation, conjugate gradient, and early stopping. In Proceedings of the 13th Interna-

tional Conference on Neural Information Processing Systems (pp. 381–387). Den-

ver, USA: MIT Press.

Chen, D., & Manning, C. (2014). A fast and accurate dependency parser using neural

networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural

Language Processing (pp. 740–750). Doha, Qatar: Association for Computational

Linguistics.

Chiu, J., & Nichols, E. (2016). Named entity recognition with bidirectional LSTM-

CNNs. Transactions of the Association for Computational Linguistics, 4, 357–370.

Chu, Y.-J., & Liu, T.-H. (1965). On shortest arborescence of a directed graph. Scientia

Sinica, 14, 1396–1400.

Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).

Natural language processing (almost) from scratch. Journal of Machine Learning

Research, 12, 2493–2537.

Daum´e

III, H., Langford,

J., & Marcu, D.

(2009).

Search-based

structured

prediction.

Machine Learning

Journal,

75(3),

297–325.

doi:10.1007/s10994-009-5106-x.

32

Dozat, T., & Manning, C. D. (2017). Deep biafﬁne attention for neural dependency

parsing. In Proceedings of the International Conference for Learning Representa-

tions (pp. 1–8). Toulon, France.

Dyer, C., Ballesteros, M., Ling, W., Matthews, A., & Smith, N. A. (2015). Transition-

based dependency parsing with stack long short-term memory. In Proceedings of the

53rd Annual Meeting of the Association for Computational Linguistics and the 7th

International Joint Conference on Natural Language Processing (Volume 1: Long

Papers) (pp. 334–343). Beijing, China.

Edmonds, J. (1967). Optimum branchings. Journal of research of the National Bureau

of Standards, 71B(4), 233–240.

Eisner, J. M. (1996). Three new probabilistic models for dependency parsing: An

exploration. In Proceedings of the 16th International Conference on Computational

Linguistics (Volume 1) (pp. 340–345). Copenhagen, Denmark.

Fundel, K., Kffner, R., & Zimmer, R.

(2007).

Relex-relation extrac-

tion using dependency parse

trees.

Bioinformatics,

23(3),

365–371.

doi:10.1093/bioinformatics/btl616.

Gillick, D., Brunk, C., Vinyals, O., & Subramanya, A. (2016). Multilingual language

processing from bytes. In Proceedings of the 2016 Conference of the North Amer-

ican Chapter of the Association for Computational Linguistics: Human Language

Technologies (pp. 1296–1306). San Diego, California.

Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., & Dahl, G. E. (2017). Neural

message passing for quantum chemistry. In Proceedings of the 34th International

Conference on Machine Learning (pp. 1263–1272). Sydney, Australia: PMLR.

Goldberg, Y., & Hirst, G. (2017). Neural Network Methods in Natural Language Pro-

cessing. Morgan & Claypool Publishers.

Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.

http://www.deeplearningbook.org.

33

Graves, A., r. Mohamed, A., & Hinton, G. (2013). Speech recognition with deep

recurrent neural networks.

In Proceedings of the International Conference on

Acoustics, Speech and Signal Processing (pp. 6645–6649). Vancouver, Canada.

doi:10.1109/ICASSP.2013.6638947.

Gurulingappa, H., MateenRajpu, A., & Toldo, L. (2012). Extraction of potential ad-

verse drug events from medical case reports. Journal of Biomedical Semantics, 3(1),

1–15. doi:10.1186/2041-1480-3-15.

Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computa-

tion, 9(8), 1735–1780. doi:10.1162/neco.1997.9.8.1735.

Huang, Z., Xu, W., & Yu, K. (2015). Bidirectional LSTM-CRF models for sequence

tagging. arXiv preprint arXiv:1508.01991, .

Jaeger, H. (2010). The echo state approach to analysing and training recurrent neu-

ral networks-with an erratum note’. Bonn, Germany: German National Research

Center for Information Technology GMD Technical Report, 148(34), 13.

Jung, J. J. (2012). Online named entity recognition method for microtexts in social

networking services: A case study of twitter. Expert Systems with Applications,

39(9), 8066 – 8070. doi:10.1016/j.eswa.2012.01.136.

Kate, R. J., & Mooney, R. (2010). Joint entity and relation extraction using card-

pyramid parsing. In Proceedings of the 14th Conference on Computational Natural

Language Learning (pp. 203–212). Uppsala, Sweden: Association for Computa-

tional Linguistics.

Kingma, D., & Ba, J. (2015). Adam: A method for stochastic optimization. In Inter-

national Conference on Learning Representations. San Diego, USA.

Kiperwasser, E., & Goldberg, Y. (2016). Simple and accurate dependency parsing

using bidirectional lstm feature representations. Transactions of the Association for

Computational Linguistics, 4, 313–327.

34

Konkol, M., Brychcn, T., & Konopk, M. (2015).

Latent semantics in named

entity recognition.

Expert Systems with Applications, 42(7), 3470 – 3479.

doi:10.1016/j.eswa.2014.12.015.

Koo, T., Globerson, A., Carreras, X., & Collins, M. (2007). Structured prediction

models via the Matrix-Tree Theorem. In Proceedings of the 2007 Joint Conference

on Empirical Methods in Natural Language Processing and Computational Natural

Language Learning (pp. 141–150). Prague, Czech: Association for Computational

Linguistics.

K¨uc¸ ¨uk, D., & Yazıcı, A.

(2012).

A hybrid named entity recognizer

for

turkish.

Expert Systems with Applications, 39(3), 2733 – 2742.

doi:10.1016/j.eswa.2011.08.131.

Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional random ﬁelds: Proba-

bilistic models for segmenting and labeling sequence data. In Proceedings of the

18th International Conference on Machine Learning (pp. 282–289). San Francisco,

USA: Morgan Kaufmann.

Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., & Dyer, C. (2016).

Neural architectures for named entity recognition. In Proceedings of the 2016 Con-

ference of the North American Chapter of the Association for Computational Lin-

guistics: Human Language Technologies (pp. 260–270). San Diego, California.

LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., &

Jackel, L. D. (1989). Backpropagation applied to handwritten zip code recognition.

Neural Computation, 1(4), 541–551. doi:10.1162/neco.1989.1.4.541.

Li, F., Zhang, M., Fu, G., & Ji, D. (2017). A neural joint model for entity

and relation extraction from biomedical text. BMC Bioinformatics, 18(1), 1–11.

doi:10.1186/s12859-017-1609-9.

Li, F., Zhang, Y., Zhang, M., & Ji, D. (2016). Joint models for extracting adverse

drug events from biomedical text. In Proceedings of the Twenty-Fifth International

35

Joint Conference on Artiﬁcial Intelligence (pp. 2838–2844). New York, USA: IJ-

CAI/AAAI Press.

Li, Q., & Ji, H. (2014). Incremental joint extraction of entity mentions and relations.

In Proceedings of the 52nd Annual Meeting of the Association for Computational

Linguistics (Volume 1: Long Papers) (pp. 402–412). Baltimore, USA.

Luong, T., Pham, H., & Manning, C. D. (2015). Effective approaches to attention-

based neural machine translation. In Proceedings of the 2015 Conference on Empir-

ical Methods in Natural Language Processing (pp. 1412–1421). Lisbon, Portugal:

Association for Computational Linguistics.

Ma, X., & Hovy, E. (2016). End-to-end sequence labeling via bi-directional LSTM-

CNNs-CRF. In Proceedings of the 54th Annual Meeting of the Association for Com-

putational Linguistics (Volume 1: Long Papers) (pp. 1064–1074). Berlin, Germany.

McDonald, R., & Pereira, F. (2007). Online learning of approximate dependency pars-

ing algorithms. In Proceedings of the 11th Conference of the European Chapter of

the Association for Computational Linguistics (pp. 81–88). Trento, Italy.

McDonald, R., Pereira, F., Ribarov, K., & Hajic, J. (2005). Non-projective depen-

dency parsing using spanning tree algorithms. In Proceedings of Human Language

Technology Conference and Conference on Empirical Methods in Natural Language

Processing (pp. 523–530). Vancouver, British Columbia, Canada: Association for

Computational Linguistics.

Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed

representations of words and phrases and their compositionality. In Proceedings of

the 26th International Conference on Neural Information Processing Systems (pp.

3111–3119). Nevada, United States: Curran Associates, Inc.

Miwa, M., & Bansal, M. (2016). End-to-end relation extraction using LSTMs on se-

quences and tree structures. In Proceedings of the 54th Annual Meeting of the As-

sociation for Computational Linguistics (Volume 1: Long Papers) (pp. 1105–1116).

Berlin, Germany.

36

Miwa, M., & Sasaki, Y. (2014). Modeling joint entity and relation extraction with

table representation. In Proceedings of the 2014 Conference on Empirical Methods

in Natural Language Processing (pp. 1858–1869). Doha, Qatar: Association for

Computational Linguistics.

Nadeau, D., & Sekine, S. (2007). A survey of named entity recognition and classiﬁca-

tion. Lingvisticae Investigationes, 30(1), 3–26. doi:10.1075/li.30.1.03nad.

Nagaraja, C. H., Brown, L. D., & Zhao, L. H. (2011). An autoregressive ap-

proach to house price modeling. The Annals of Applied Statistics, 5(1), 124–149.

doi:10.1214/10-AOAS380.

Nguyen, N., & Guo, Y. (2007). Comparisons of sequence labeling algorithms and ex-

tensions. In Proceedings of the 24th International Conference on Machine Learning

(pp. 681–688). Corvallis, USA: ACM. doi:10.1145/1273496.1273582.

Nivre, J. (2003). An efﬁcient algorithm for projective dependency parsing. In Pro-

ceedings of the 8th International Workshop on Parsing Technologies (pp. 149–160).

Nancy, France.

Nivre, J. (2009). Non-projective dependency parsing in expected linear time. In Pro-

ceedings of the Joint Conference of the 47th Annual Meeting of the Association for

Computational Linguistics and the 4th International Joint Conference on Natural

Language Processing of the Asian Federation of Natural Language Processing (pp.

351–359). Singapore.

Nivre, J., Hall, J., Nilsson, J., Eryiˇgit, G., & Marinov, S. (2006). Labeled pseudo-

projective dependency parsing with support vector machines. In Proceedings of the

10th Conference on Computational Natural Language Learning (pp. 221–225). New

York, USA: Association for Computational Linguistics.

Pace, K., Barry, R., Gilley, O. W., & Sirmans, C. (2000). A method for spatialtem-

poral forecasting with an application to real estate prices. International Journal of

Forecasting, 16(2), 229 – 246. doi:10.1016/S0169-2070(99)00047-3.

37

Pascanu, R., Mikolov, T., & Bengio, Y. (2013). On the difﬁculty of training recurrent

neural networks. In Proceedings of the 30th International Conference on Interna-

tional Conference on Machine Learning (pp. 1310–1318). Atlanta, USA: JMLR.org.

Peng, F., & McCallum, A. (2006). Information extraction from research papers using

conditional random ﬁelds. Information processing & management, 42(4), 963–979.

doi:10.1016/j.ipm.2005.09.002?

Rabiner, L., & Juang, B. (1986). An introduction to hidden markov models. IEEE

ASSP Magazine, 3(1), 4–16. doi:10.1109/MASSP.1986.1165342.

Ratinov, L., & Roth, D. (2009). Design challenges and misconceptions in named en-

tity recognition. In Proceedings of the 13th Conference on Computational Natural

Language Learning (pp. 147–155). Boulder, USA: Association for Computational

Linguistics.

China.

dos Santos, C., Xiang, B., & Zhou, B. (2015). Classifying relations by ranking with

convolutional neural networks. In Proceedings of the 53rd Annual Meeting of the As-

sociation for Computational Linguistics and the 7th International Joint Conference

on Natural Language Processing (Volume 1: Long Papers) (pp. 626–634). Beijing,

Socher, R., Chen, D., Manning, C. D., & Ng, A. (2013). Reasoning with neural

tensor networks for knowledge base completion.

In Proceedings of the 26th In-

ternational Conference on Neural Information Processing Systems (pp. 926–934).

Nevada, United States: Curran Associates, Inc.

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014).

Dropout: A simple way to prevent neural networks from overﬁtting. Journal of

Machine Learning Research, 15(1), 1929–1958.

Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with

neural networks.

In Proceedings of the 27th International Conference on Neural

Information Processing Systems (pp. 3104–3112). Montreal, Canada: MIT Press.

38

Taskar, B., Guestrin, C., & Koller, D. (2003). Max-margin markov networks.

In

Proceedings of the 16th International Conference on Neural Information Processing

Systems (pp. 25–32). Bangkok, Thailand: MIT Press.

Tsochantaridis, I., Hofmann, T., Joachims, T., & Altun, Y. (2004). Support vector

machine learning for interdependent and structured output spaces. In Proceedings

of the 21st International Conference on Machine Learning (pp. 104–112). Helsinki,

Finland: ACM. doi:10.1145/1015330.1015341.

Tutte, W. T. (2001). Graph theory. In Encyclopedia of Mathematics and its Applications

(p. 138). Cambridge University Press volume 21.

Vinyals, O., Fortunato, M., & Jaitly, N. (2015). Pointer networks. In Proceedings of

the 28th International Conference on Neural Information Processing Systems (pp.

2692–2700). Montreal, Canada: Curran Associates, Inc.

Wang, D., & Li, M. (2017).

Stochastic conﬁguration networks: Fundamen-

tals and algorithms.

IEEE Transactions on Cybernetics, 47(10), 3466–3479.

doi:10.1109/TCYB.2017.2734043.

Wang, W., & Chang, B. (2016). Graph-based dependency parsing with bidirectional

lstm. In Proceedings of the 54th Annual Meeting of the Association for Computa-

tional Linguistics (Volume 1: Long Papers) (pp. 2306–2315). Berlin, Germany.

Xu, Y., Mou, L., Li, G., Chen, Y., Peng, H., & Jin, Z. (2015). Classifying relations via

long short term memory networks along shortest dependency paths. In Proceedings

of the 2015 Conference on Empirical Methods in Natural Language Processing (pp.

1785–1794). Lisbon, Portugal: Association for Computational Linguistics.

Yamada, H., & Matsumoto, Y. (2003). Statistical dependency analysis with support

vector machines. In Proceedings of the 8th International Workshop on Parsing Tech-

nologies (pp. 195–206). Nancy, France.

Zhang, H., & McDonald, R. (2012). Generalized higher-order dependency parsing with

cube pruning. In Proceedings of the 2012 Joint Conference on Empirical Methods in

39

Natural Language Processing and Computational Natural Language Learning (pp.

320–331). Jeju Island, Korea: Association for Computational Linguistics.

Zhang, X., Cheng, J., & Lapata, M. (2017). Dependency parsing as head selection.

In Proceedings of the 15th Conference of the European Chapter of the Association

for Computational Linguistics: (Volume 1, Long Papers) (pp. 665–676). Valencia,

Spain.

Zheng, S., Hao, Y., Lu, D., Bao, H., Xu, J., Hao, H., & Xu, B. (2017). Joint entity and

relation extraction based on a hybrid neural network. Neurocomputing, 257, 59 –

66. doi:10.1016/j.neucom.2016.12.075.

40

