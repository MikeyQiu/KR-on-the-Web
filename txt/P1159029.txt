7
1
0
2
 
y
a
M
 
0
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
5
7
9
3
0
.
7
0
6
1
:
v
i
X
r
a

Estimating and Controlling the False Discovery Rate of the
PC Algorithm Using Edge-Speciﬁc P-Values

Eric V. Strobl
Department of Biomedical Informatics
University of Pittsburgh
Pittsburgh, PA 15206, USA

Peter L. Spirtes
Department of Philosophy
Carnegie Mellon University
Pittsburgh, PA 15213, USA

Shyam Visweswaran
Department of Biomedical Informatics
University of Pittsburgh
Pittsburgh, PA 15206, USA

Editor: TBA

evs17@pitt.edu

ps7z@andrew.cmu.edu

shv3@pitt.edu

Abstract

The PC algorithm allows investigators to estimate a complete partially directed acyclic
graph (CPDAG) from a ﬁnite dataset, but few groups have investigated strategies for
estimating and controlling the false discovery rate (FDR) of the edges in the CPDAG. In
this paper, we introduce PC with p-values (PC-p), a fast algorithm which robustly computes
edge-speciﬁc p-values and then estimates and controls the FDR across the edges. PC-p
speciﬁcally uses the p-values returned by many conditional independence (CI) tests to upper
bound the p-values of more complex edge-speciﬁc hypothesis tests. The algorithm then
estimates and controls the FDR using the bounded p-values and the Benjamini-Yekutieli
FDR procedure. Modiﬁcations to the original PC algorithm also help PC-p accurately
compute the upper bounds despite non-zero Type II error rates. Experiments show that
PC-p yields more accurate FDR estimation and control across the edges in a variety of
CPDAGs compared to alternative methods1.
Keywords: PC Algorithm, Causal Inference, False Discovery Rate, Bayesian Network,
Directed Acyclic Graph

1. Introduction

Discovering causal relationships is often much more important than discovering associational
relationships in the sciences. As a result, the research community has been conducting
extensive investigations into causal inference with the hope of developing practically useful
algorithms to speed-up the scientiﬁc process. This research has resulted in a wide range of
high performing algorithms over the years such as PC (Spirtes et al., 2000), FCI (Spirtes
et al., 1995, 2000), and CCD (Richardson, 1996)

1. MATLAB implementation: https://github.com/ericstrobl/PCp/

c(cid:13)2016 Eric V. Strobl and Shyam Visweswaran.

Strobl, Spirtes and Visweswaran

The PC algorithm is currently one of the most popular methods for inferring causation
from observational data. Given an observational dataset, the algorithm outputs a complete
partially directed acyclic graph (CPDAG) which has helped some investigators elucidate
important causal relationships in several domains. For example, the PC algorithm has been
used to discover new causal relationships between genes and brain regions in biology (Wu
and Ye, 2006; Li et al., 2008; Joshi et al., 2010; Sun et al., 2012; Harris and Drton, 2013;
Iyer et al., 2013; Le et al., 2013; Teramoto et al., 2014; Ha et al., 2015). The algorithm has
also been used to discover causal relations between corporate structures and strategies in
economics (Chong et al., 2009) as well as academic and musical achievements in psychology
(Mullensiefen et al., 2015).

The increased use of PC in recent years has nonetheless led to growing concern about
algorithm’s conﬁdence level in each edge of the CPDAG. For example, PC may have more
conﬁdence in the edge A − B but have less conﬁdence in the edge B → C in the subgraph
A−B → C of the CPDAG. Currently, PC alone does not output any edge-speciﬁc measure of
conﬁdence, even though scientists often must report measures of conﬁdence such as p-values
or conﬁdence intervals in their scientiﬁc articles. This incongruency has resulted in the
relatively slow adoption or even avoidance of the PC algorithm in the sciences, despite the
algorithm’s impressive capabilities in causal inference. Clearly then rectifying the problem
by developing an edge-speciﬁc measure of conﬁdence will increase the adoption of PC as
well as hopefully ease the transition of ever-more complex causal inference algorithms into
the scientiﬁc community.

We can of course consider multiple diﬀerent ways of representing the conﬁdence level
in each edge of the CPDAG. However, we choose to pay special attention to the p-value,
since it is by far the most popular notion of conﬁdence in the sciences. Indeed, nearly all
scientists report p-values in modern scientiﬁc reports because they rely on p-values to help
justify their hypotheses. We therefore would ideally like to assign a p-value to each edge in
the CPDAG as in Figure 1a in order to best integrate the algorithm within a well-known
framework. In this paper, we propose such a “causal p-value” in detail.

We however also believe that assigning p-values to each edge is not enough to ease the
transition of PC into mainstream science, since the CPDAG actually contains many edges
and therefore also represents a complicated multiple hypothesis testing problem. Fortu-
nately, the problem of multiple hypothesis testing has a long history, as scientists have
often required the results of multiple hypothesis tests in order to answer complex scien-
tiﬁc questions. Currently, a standard approach to tackling the multiple hypothesis testing
problem involves controlling the proportion of false positives among the rejected null hy-
potheses, or the false discovery rate (FDR), by using an FDR controlling procedure that
takes a desired FDR level q and a set of p-values as input. The procedure then outputs
a corresponding signiﬁcance level α∗ for the set of p-values. An investigator subsequently
rejects the null hypotheses for those tests with p-values that fall below α∗ in order to en-
sure that the expected FDR does not exceed q. For example, consider the set of p-values
{0.02, 0.01, 0.03} and suppose that the FDR controlling procedure with q = 0.1 outputs
α∗ = 0.019. Then, rejecting the null hypotheses of the ﬁrst and third hypothesis tests
guarantees that the expected FDR does not exceed 10%. Several other FDR controlling
strategies also exist, but the ease of use, speed and accuracy of the above method have
made it the most widely adopted strategy in the last two decades.

2

PC with p-values

We would therefore like to control the FDR in the edges of the CPDAG using an FDR
controlling procedure like in Figure 1b. As a ﬁrst idea, one may wonder whether an inves-
tigator can control the FDR in the CPDAG by simply feeding in all of the CI test p-values
computed by PC into an FDR controlling procedure. Unfortunately, this approach fails for
at least two reasons. First, it is unclear how to use the p-values which exceed the α∗ cut-oﬀ
to reject edges in the CPDAG, since one would ﬁrst need to elucidate the correspondence
between the p-values and edges. Second, even if one could solve this problem, the strat-
egy may only loosely bound the expected FDR. An accurate FDR controlling procedure
should instead take into account the speciﬁc computations executed by PC in order to iden-
tify a sharp bound. The p-value based approach therefore necessitates a more ﬁne-grained
strategy which has thus far remained undiscovered.

Several groups have nonetheless attempted to control the FDR in the CPDAG by avoid-
ing the complicated nature of the above problem with a diﬀerent, data re-sampling approach.
For example, Friedman and colleagues proposed to estimate the FDR by using the para-
metric bootstrap (Friedman et al., 1999). This procedure involves ﬁrst learning a causal
graph with the PC algorithm. The procedure then generates data from the causal graph
and re-applies the PC algorithm multiple times on each generated dataset to estimate the
FDR using the learnt causal graphs. An investigator can subsequently control the FDR
by repeating the above process with diﬀerent α values until he or she reaches the desired
FDR level q. However, notice that the method requires multiples calls to PC and can
therefore require too much time with high dimensional data. The procedure also requires
parametric knowledge about the underlying distribution which limits the applicability of
the method to simple cases. Fortuitously, two groups later proposed a permutation-based
method which drops the parametric assumption (Listgarten and Heckerman, 2007; Armen
and Tsamardinos, 2014). The permutation method nevertheless also requires multiple calls

(a)

A

(b)

A

.

.

B

5
2
0.0

C

0
.
1
5
0

B

5
2
0.0

0

0

.

.

0

0

0

0

1

1

E

0.001

D

E

0.001

D

C

α∗ = 0.031

Figure 1: We seek to associate edge-speciﬁc p-values to the output of the PC algorithm
such as in (a). The PC algorithm currently does not associate such p-values with
its output. We would also like to control the FDR of the edges. In (b), we set
the FDR to 0.1 and obtained a α∗ cutoﬀ of 0.031 for the output in (a), so we
eliminated the edge between B and D because its p-value exceeds α∗.

3

Strobl, Spirtes and Visweswaran

to an algorithm and in fact only applies to the parts of PC which can be decomposed
into independent searches for the parents of each vertex; this has thus far limited the ap-
plicability of the method to adjacency discovery with local to global discovery algorithms
(e.g., MMHC) and incomplete edge orientation. We conclude that both the bootstrap and
permutation approaches to FDR estimation and control are either incomplete or too time
consuming.

Another class of methods fortunately attempts to control the FDR without resampling
procedures by instead using a standard FDR controlling procedure with bounded p-values.
For instance, one method proposed in (Tsamardinos and Brown, 2008) and then reﬁned
in (Armen and Tsamardinos, 2011, 2014) assigns a p-value to each adjacency by taking
the maximum over all of the signiﬁcant p-values from the associated CI tests executed
by PC. The method then controls the FDR in the estimated adjacencies by applying an
FDR controlling procedure, such as the one proposed by Benjamini and Yekutieli (BY)
(Benjamini and Yekutieli, 2001), on the edge-speciﬁc p-values. Under faithfulness and a
zero Type II error rate, the method controls the FDR across the estimated adjacencies,
or the estimated skeleton (Armen and Tsamardinos, 2014). This two stage method also
performs comparably with the one stage method proposed in (Li et al., 2008; Li and Wang,
2009), which controls the FDR during, as opposed to after, the execution of the skeleton
discovery phase of the PC algorithm. Of course, the Type II error rate never reaches
zero in practice but researchers have also investigated a strategy for reducing the realized
Type II error rate by introducing a heuristic reliability criterion for CI tests when dealing
with discrete data (Armen and Tsamardinos, 2014). Experiments have shown that these
methods ﬁnish in a relatively short amount of time and perform well in practice. However,
the methods are also incomplete because they only apply to the skeleton discovery phase of
PC.

In this report, we build on the previous outstanding work for deriving p-values for
adjacencies by contributing a sound, complete and fast algorithm called PC with p-values
(PC-p) which appropriately combines the p-values of PC’s CI tests and then uses the BY
FDR controlling procedure to accurately control the FDR in a CPDAG. The method relies
on two upper bounds of the p-value that relate to logical conjunctions and disjunctions as
described in Section 3. These upper bounds allow us to formulate several hypothesis tests
for recovering the skeleton, discovering unshielded v-structures, and orienting additional
edges as presented in Section 4. Accurately estimating the p-values of the hypothesis tests
nonetheless requires a modiﬁed version of PC called PC-p which we propose in Section
5. Finally, we provide experimental results in Section 6 which show that PC-p’s p-value
estimates yield accurate estimates of the FDR with the BY procedure and improve upon
alternative methods.

2. Preliminaries

2.1 Causal graphs

A causal graph consists of vertices representing variables and edges representing causal
relationships between any two variables. In this paper, we will use the terms “vertices” and
“variables” interchangeably. Directed graphs are graphs where two distinct vertices can be
connected by edges “→” and “←.” We only consider simple graphs in this paper, or graphs

4

PC with p-values

with no edges originating from and connecting to the same vertex. Directed acyclic graphs
(DAGs) are directed graphs without directed cycles. We say that X and Y are adjacent
if they are connected by an edge independent of the edge’s direction. A path p from X
to Y is a set of consecutive edges (also independent of their direction) from X to Y such
that no vertex is visited more than once. Given a path between two vertices X and Y
with a middle vertex Z, the path is a chain if X → Y → Z, a fork if X ← Y → Z, and
a v-structure if X → Y ← Z. We refer to Y as a collider, if it is the middle vertex in a
v-structure. A v-structure is called an unshielded v-structure if X → Y ← Z, but X and Z
are non-adjacent. A directed path from X to Y is a set of consecutive edges with direction.
We say that X is an ancestor of Y (and Y is a descendant of X), if there exists a directed
path from X to Y .

If G is a directed graph in which X, Y and Z are disjoint sets of vertices, then X
and Y are d-connected by Z in G if and only if there exists an undirected path p between
some vertex in X and some vertex in Y such that, for every collider C on p, either C or a
descendant of C is in Z, and no non-collider on p is in Z. On the other hand, X and Y
are d-separated by Z in G if and only if they are not d-connected by Z in G. Next, the
joint probability distribution P over variables X satisﬁes the global directed Markov property
for a directed graph G if and only if, for any three disjoint subsets of variables A, B and
C from X, if A and B are d-separated given C in G, then A and B are conditionally
independent given C in P. We refer to the converse of the global directed Markov property
as d-separation faithfulness; that is, if A and B are conditionally independent given C in
P, then A and B are d-separated given C in G.

A Markov equivalence class of DAGs refers to a set of DAGs which entail the same con-
ditional independencies. A complete partially directed acyclic graph (CPDAG) is a partially
directed acyclic graph with the following properties: (1) each directed edge exists in every
DAG in the Markov equivalence class, and (2) there exists a DAG with X → Y and a DAG
with X ← Y in the Markov equivalence class for every undirected edge X − Y . A CPDAG
GC represents a DAG G, if G belongs to the Markov equivalence class described by GC.
We will occasionally use the meta-symbol “◦” at the endpoint(s) of an edge to denote the
presence or absence of an arrowhead. For example, the edge “ −◦ ” may denote either “−”
or “→”.

2.2 The PC Algorithm

The PC algorithm is comprised of three stages. We have summarized these stages as
pseudocode in Algorithms 5, 6 and 7 in Section A.1 of the Appendix. The ﬁrst stage
estimates the adjacencies of G, or the skeleton of G. Starting with a fully connected
skeleton, the algorithm attempts to eliminate the adjacency between any two variables,
say A and B, by testing if A and B are conditionally independent given some subset of the
neighbors of A or the neighbors of B. The search is performed progressively, whereby the
algorithm increases the size of the conditioning set starting from zero using a step size of 1.
The edge between A and B is removed, if A and B are rendered conditionally independent
given some subset of the neighbors of A or the neighbors of B.

The PC algorithm orients unshielded colliders in its second stage. Speciﬁcally, PC ﬁnds
triples A, B, C such that A − B − C, but A and C are non-adjacent. The algorithm

5

Strobl, Spirtes and Visweswaran

then determines whether B is contained in the set which rendered A and C conditionally
independent in the ﬁrst stage of PC. If not, A − B − C is replaced with A → B ← C.

The third and ﬁnal stage of PC involves the repetitive application of three rules to orient

as many of the remaining undirected edges as possible. The three rules include:

1.

If A − B, C → A and C and B are non-adjacent, then replace A − B with

A → B.

2.

3.

If A − B and A → C → B, then replace A − B with A → B.

(1)

If A − B, A − C → B, A − D → B, and C and D are non-adjacent,

then replace A − B with A → B.

Overall, the PC algorithm has been shown to be complete in the sense that it ﬁnds and

then orients edges up to GC, a CPDAG that represents G (Meek, 1995).

2.3 Hypothesis Testing

A hypothesis test is a method of statistical inference usually composed of one null (H0) and
one alternative (H1) hypothesis which are mutually exclusive; that is, if one occurs, then
the other cannot occur. The null hypothesis refers to the default position which asserts that
whatever one is trying to statistically infer actually did not happen. Note that the null and
alternative do not necessarily need to be logical complements of each other. For example,
one may be interested in determining whether the parameter µ is greater than zero. In this
case, the null can be deﬁned as µ = 0 while the alternative can be deﬁned as µ > 0 instead
of µ (cid:54)= 0.

A Type I error is the incorrect rejection of a true null hypothesis, or a false positive.
On the other hand, a Type II error is the failure to reject a false null hypothesis, or a false
negative. The p-value (p) is the probability of the Type I error, or the Type I error rate.
More speciﬁcally, the p-value is the probability of obtaining a result equal to or more extreme
than the observed value under the assumption of the null hypothesis. The null hypothesis
is thus rejected when the p-value is at or below a predeﬁned α threshold (typically the α
threshold is set to 0.05), because a low p-value demonstrates the improbability of the null
hypothesis.

2.4 False Discovery Rate

Multiple comparisons or multiple hypothesis testing refers to the process of considering
more than one statistical inference simultaneously. Failure to compensate for multiple
comparisons can result in erroneous inferences. For example, if an investigator performs
one hypothesis test with an α threshold of 0.05, then he or she has only a 5% chance of
making a Type I error. However, if the investigator performs 100 independent tests with
the same α threshold, then he or she has a 1 − (1 − 0.05)100 = 99.4% chance of making a
Type I error on at least one test.

In multiple hypothesis testing, the false discovery rate (FDR) at threshold α is the
expected proportion of false positives among the rejected null hypotheses. Speciﬁcally, we

6

PC with p-values

deﬁne the FDR at α as follows:

F DR(α) (cid:44) E

(cid:20)

V
max{R, 1}

(cid:21)

,

where V is the number of false positives, R is the total number of null hypotheses rejected,
and max{R, 1} ensures that F DR(α) is well-deﬁned when R = 0. We deﬁne the realized
FDR at α as V / max{R, 1}.

FDR estimation, or conservative point estimation of the FDR, refers to the process of

estimating F DR(α) in a conservative manner such that:

E[(cid:92)F DR(α)] ≥ F DR(α),

(cid:92)F DRBY (α) (cid:44)

mα (cid:80)m
1
i=1
i
max{R, 1}

.

where (cid:92)F DR(α) represents an estimate of F DR(α). We denote E[(cid:92)F DR(α)]−F DR(α) as the
estimation bias. Note that there are several ways of obtaining (cid:92)F DR(α). In 2001, Benjamini
and Yekutieli proposed the following FDR estimator for m hypothesis tests:

(2)

(3)

FDR estimators such as (cid:92)F DRBY can be used to deﬁne FDR controlling procedures. These
procedures determine the optimal threshold α∗ which achieves strong control 2 of the FDR
in the following sense:

α∗ (cid:44) arg max

{(cid:92)F DR(α) ≤ q}

α

The FDR controlling procedure based on (cid:92)F DR involves the rejection of all null hypotheses
with p-values below the α∗ threshold. We refer to the quantity F DR(α∗) − q as the control
bias. Benjamini and Yekutieli proved that the estimate (cid:92)F DRBY in particular achieves
strong control of the FDR with any form of dependence among the p-values of m hypothesis
tests.

3. Upper Bounds on the P-Value

We present two upper bounds of the Type I error rate of hypothesis tests which can be
constructed using a set of simpler hypothesis tests. These upper bounds will serve as useful
tools in Section 4 for bounding the Type I error rate of the hypothesis tests which will be
used to infer the presence or absence of edges in a CPDAG.

3.1 Union Bound

Consider the following hypothesis test for two random variables given a conditioning set:

2. Strong control of the FDR refers the process of controlling the FDR under any conﬁguration of true and
false null hypotheses; on the other hand, weak control refers to the process of controlling the FDR when
all of the null hypotheses are true. Strong control is therefore preferable to weak control.

H0 : Conditionally independent,
H1 : Conditionally dependent.

7

Strobl, Spirtes and Visweswaran

f ((cid:98)si)

−1.96

0

1.96

(cid:98)si

Figure 2: In the above standard normal case, we have Pr(|(cid:98)si| ≥ sα

(cid:12)
(cid:12)si = 0) = 0.05, where
sα
i = 1.96. We reject the null hypothesis when |(cid:98)si| falls in the blue colored regions
at the tails.

i

Trivially, we can rephrase the null and alternative in terms of a conditional independence
(CI) oracle:

H0 : The CI oracle outputs independent,
H1 : The CI oracle outputs dependent.

Now suppose we want to query m CI oracles about m CI relations. We can then consider
the following null and alternative:

H0 : All CI oracles output independent,
H1 : At least one CI oracle outputs dependent.

(4)

i

From here on, we write Pr(CI test i outputs dependent | CI oracle i outputs independent) to
(cid:12)
denote Pr(|(cid:98)si| ≥ sα
(cid:12)si = 0), where si refers to a parameter of some standardized distribution
used by CI test i, (cid:98)si a random variable and the test statistic estimating si, and sα
i a value of
si determined by an α level. We provide an example in Figure 2, where si may correspond
to Fisher’s z-statistic in the case of Fisher’s z-test for the mean parameter si = 0 of the
standard normal distribution.

8

PC with p-values

We now bound the Type I error rate of the hypothesis test (4) by using the new notation

and the union bound:

Pr(Type I error)

= Pr(at least one CI test outputs dependent|H0)

(cid:33)

= Pr

CI test i outputs dependent|H0

Pr(CI test i outputs dependent|H0)

(cid:32) m
(cid:95)

i=1

≤

=

=

m
(cid:88)

i=1
m
(cid:88)

i=1
m
(cid:88)

i=1

pi,

Pr(CI test i outputs dependent|CI oracle i outputs independent)

where pi denotes the Type I error rate of CI test i. Thus, if the r.h.s. of (5) is less than
the α threshold, then we can conclude that the Type I error rate of (4) is also below the
threshold. In other words, (5) is a conservative p-value.

Note that the third equality in the derivation of (5) uses the simplifying assumption
that the probability of the output of CI test i only depends on the output of CI oracle
i when given the outputs of all CI oracles. Several papers have used this assumption
implicitly in their proofs (Tsamardinos and Brown, 2008; Li and Wang, 2009), and we will
also use it throughout this paper. We can justify the assumption based on three facts.
First, most CI test statistics si have a limiting distribution which only depends on si = 0
under the null. For example, Fisher’s z-statistic has a limiting standard normal distribution
with mean parameter zi = 0 and constant variance. Moreover, the G-statistic for the G-
test has a limiting χ2-distribution with non-centrality parameter gi = 0 and degrees of
freedom determined by the number of cells in the contingency table. Second, existing
methods which utilize bounds based on the assumption have strong empirical performance;
loose-enough bounds therefore appear to accommodate the assumption well in most ﬁnite
sample cases. Third, recall that simplifying assumptions are not new in the causality
literature; indeed, many authors have made simplifying assumptions regarding parameter
independence for Bayesian methods which similarly increase computational eﬃciency and
achieve strong empirical performance (e.g., (Cooper and Yoo, 1999)).

We can now also generalize the bound in (5) to any hypothesis test consisting of a series
of logical disjunctions in the alternative and a series of logical conjunctions in the null.
Namely:

(5)

(6)

H0 :

oracle i outputs ¬Pi,

H1 :

oracle i outputs Pi,

m
(cid:94)

i=1
m
(cid:95)

i=1

9

Strobl, Spirtes and Visweswaran

where Pi denotes an arbitrary output of oracle i. We now have:

Pr(Type I error) = Pr(

test i outputs Pi |H0)

m
(cid:95)

i=1

Pr(test i outputs Pi|H0)

Pr(test i outputs Pi|oracle i outputs ¬Pi)

≤

=

=

m
(cid:88)

i=1
m
(cid:88)

i=1
m
(cid:88)

i=1

hi,

where hi is the Type I error rate of test i, and the second equality uses the assumption that
the probability of the output of test i only depends on the output of oracle i when given all
oracles. We will use this generalization in Section 4.

3.2 Intersection Bound

Suppose we want to perform a hypothesis test with the following null and alternative which
are diﬀerent than the null and alternative in (4):

H0 : At least one CI oracle outputs independent,
H1 : All CI oracles output dependent.

Now assume we know that the ith CI oracle outputs independent. We can then bound the
Type I error rate of (7) as follows with m queries to the CI oracle:

Pr(Type I error) = Pr(all m CI tests output dependent|H0)

≤ Pr(CI test i outputs dependent |H0)
= Pr(CI test i outputs dependent | CI oracle i outputs independent

∧ other CI oracles may output independent)

= Pr(CI test i outputs dependent | CI oracle i outputs independent)
= pi,

where the second equality again holds under the assumption that the probability of the
output of CI test i only depends on the output of CI oracle i when given the outputs of all
CI oracles. We can therefore bound the Type I error rate of (7) using the p-value of a single
CI test for which the CI oracle outputs independent. Nevertheless, in practice, we often do
not know for which query the oracle outputs independent in the null. We do however know
that at least one unknown CI oracle i outputs independent, so we can bound the Type I
error rate of (7) using the maximum over all of the m CI test p-values:

Pr(Type I error) = Pr(all m CI tests output dependent|H0)

≤ Pr(CI test i outputs dependent | CI oracle i outputs independent)
= pi ≤ max

pj.

(9)

j=1,...,m

(7)

(8)

10

PC with p-values

H0 :

oracle i outputs ¬Pi,

H1 :

oracle i outputs Pi.

m
(cid:95)

i=1
m
(cid:94)

i=1

m
(cid:94)

i=1

Note that we can generalize the above bound to any hypothesis test consisting of a
series of logical conjunctions in the alternative and a series of logical disjunctions in the
null. Namely:

(10)

(11)

We therefore have:

Pr(Type I error) = Pr(

test i outputs Pi|H0)

≤ Pr(test i outputs Pi|H0)
= Pr(test i outputs Pi|oracle i outputs ¬Pi )
= hi ≤ max

hj,

j=1,...,m

where the second equality again uses the assumption that the probability of the output of
test i only depends on the output of oracle i when given all oracles.

4. Edge-Speciﬁc Hypothesis Tests

We now show how to apply the two upper bounds of the Type I error rate to derive p-value
estimates for both the undirected and directed edges in the CPDAG as estimated by PC.
Bounding the p-value for each edge therefore amounts to adding up and/or maximizing over
the p-values returned from multiple CI tests.

Note that we will sometimes invoke a zero Type II error rate assumption in this section.
This assumption is necessary to correctly upper bound the p-values of the edge-speciﬁc
hypothesis tests of the CPDAG according to the CI tests executed by the PC algorithm. In
fact, we can always correctly bound the p-values, if we perform all of the possible CI tests
between the considered variables; however, this approach is impractical, since it ignores
the eﬃciencies of the PC algorithm. A more interesting strategy involves designing the
edge-speciﬁc hypothesis tests so that the p-value bounds are robust to Type II errors as
well as redesigning the PC algorithm to catch many Type II errors. We will discuss these
approaches in detail in Sections 4.4 and 5, so we encourage readers to accept the zero Type
II error rate assumption for now.

4.1 Skeleton Discovery

We ﬁrst consider the skeleton discovery phase of the PC algorithm. We wish to test whether
each edge is absent in the true skeleton starting from a completely connected undirected
graph. This problem has already been investigated in (Li et al., 2008; Tsamardinos and
Brown, 2008; Li and Wang, 2009; Armen and Tsamardinos, 2011, 2014), but we review it
here for completeness. We construct a hypothesis test with the following null and alterna-

11

Strobl, Spirtes and Visweswaran

(12)

(13)

(14)

(15)

(16)

tive:

H0 : A − B is absent,
H1 : A − B is present.

Now consider the following proposition, where P a(A) denotes the true parents of A:

Proposition 1 (Spirtes et al., 2000) Consider a DAG G which satisﬁes the global directed
Markov property. Moreover, assume that the probability distribution is d-separation faithful.
Then, there is an edge between two vertices A and B if and only if A and B are conditionally
dependent given any subset of P a(A) \ B and any subset of P a(B) \ A.

We thus consider the following two scenarios for the undirected edge A − B:

1.

If A and B are conditionally independent given some subset of P a(A) \ B

or some subset of P a(B) \ A, then A − B is absent.

2.

If A and B are conditionally dependent given any subset of P a(A) \ B

and any subset of P a(B) \ A, then A − B is present.

The following null and alternative are therefore equivalent to (12), where CI oracles are
queried about A and B given all possible subsets of P a(A) \ B and all possible subsets of
P a(B) \ A:

H0 : At least one CI oracle outputs independent,
H1 : All CI oracles output dependent.

Notice that the above hypothesis test is the same as the hypothesis test in (7). We can
therefore bound the p-value of (12) using:

p(cid:48)
A−B

(cid:44) max
i=1,...,q(cid:48)

pA⊥⊥B|Ri,

where Ri ⊆ {Pa(A) \ B} or Ri ⊆ {Pa(B) \ A} and q(cid:48) denotes the total number of such
subsets.

Note that the skeleton discovery phase of the PC algorithm cannot diﬀerentiate between
the parents and children of a particular vertex using its neighbors. However, we can further
bound (15) using the following quantity:

p(cid:48)
A−B ≤ max
i=1,...,q

pA⊥⊥B|Si

(cid:44) pA−B,

where Si ⊆ {N(A) \ B} or Si ⊆ {N(B) \ A} and q denotes the total number of such subsets.
Now assume that the Type II error rate of all CI tests is zero. Then, if the alternative
holds for the CI tests (conditional dependence), then the alternative is accepted. Hence, the
PC algorithm will not remove any of the edges between N (A) and A as well as any of the
edges between N (B) and B. PC therefore performs all necessary CI tests for computing
(16), so upper bounding the Type I error rate for (12) reduces to taking the maximum of
the p-values for all of the CI tests performed by PC regarding A and B. For example,
suppose we measure three random variables A, B and C. Then we obtain p-values after the
PC algorithm tests whether A ⊥⊥ B and A ⊥⊥ B|C. Suppose these p-values are (0.03, 0.04)
so that the PC algorithm with an α threshold of 0.05 determines that A − B is present.
The p-value upper bound of (12) thus corresponds to max {0.03, 0.04} = 0.04.

12

PC with p-values

4.2 Detecting V-Structures

4.2.1 Deterministic Skeleton

The hypothesis testing procedure for directed edges is more complicated than the procedure
for adjacencies. Edges can be oriented in the PC algorithm according to unshielded v-
structures or the orientation rules as described in Section 2.2. Let us ﬁrst focus on the
former and, for further simplicity, let us also assume that 1) we have access to the ground
truth skeleton and 2) no edge is involved in more than one unshielded v-structure (we will
later drop these assumptions in Section 4.2.2). Our task then is to statistically infer the
presence of an unshielded v-structure.

We now present the following null and alternative for each unshielded v-structure after

ﬁnding a triple A − C − B such that A and B are non-adjacent in the skeleton:

H0 : Unshielded A → C ← B is absent,
H1 : Unshielded A → C ← B is present.

(17)

Next, consider the following proposition:

Proposition 2 (Spirtes et al., 2000) Consider the same assumptions as Proposition 1.
Further assume that A, C are adjacent and C, B are adjacent but A, B are non-adjacent.
Then, A and B are conditionally independent given some subset of P a(A)\B which does not
include C or some subset of P a(B)\A which does not include C if and only if A → C ← B.

The following null and alternative is therefore equivalent to (17):

H0 : A and B are conditionally dependent given any subset of P a(A) \ B
which does not include C and any subset of P a(B) \ A which

H1 : A and B are conditionally independent given some subset of P a(A) \ B

which does not include C or some subset of P a(B) \ A which

(18)

does not include C,

does not include C.

The above alternative is reminiscent of the way in which PC determines the presence of an
unshielded v-structure according to Algorithm 6 in the Appendix; speciﬁcally, if C is not in
the set which renders A and B conditionally independent, then C in A − C − B must be a
collider. We however cannot bound the p-value of (18) using CI tests, because conditional
dependence is in the null and conditional independence is in the alternative, as opposed to
vice versa. As a result, we also consider the following proposition:

Proposition 3 Consider the same assumptions as Proposition 2. Then, A and B are
conditionally dependent given any subset of P a(A) \ B containing C and any subset of
P a(B) \ A containing C if and only if A → C ← B.

Proof First notice that P a(A) = {P a(A) \ B} and P a(B) = {P a(B) \ A}, since A and
B are non-adjacent. As a result, we can instead prove that the if and only if statement
holds for P a(A) and P a(B) without loss of generality.

13

Strobl, Spirtes and Visweswaran

For the forward direction, suppose A and B are conditionally dependent given any
subset of P a(A) containing C and any subset of P a(B) containing C. Then A and B are
d-connected given any subset of P a(A) containing C and any subset of P a(B) containing
C by the global directed Markov property. Clearly, C ∈ N (A) and C ∈ N (B), so C must
either be a parent of A and a parent of B, a child of A and a parent of B, a parent of A and
a child B, or a child of A and a child of B. Note that A and B are non-adjacent, so A and
B are d-separated given some subset of P a(A) or some subset of P a(B) by Proposition
1 and d-separation faithfulness. Moreover, the subset must include C if C is a parent of
A and a parent of B, a child of A and a parent of B, or a parent of A and a child B;
otherwise, A and B would be d-connected. As a result, in those three situations, we arrive
at the contradiction that A and B are d-separated given some subset of P a(A) containing
C or some subset of P a(B) containing C. We conclude that C must be a child of A and a
child of B.

For the other direction, if A → C ← B holds, then A and B are d-connected given
any subset of P a(A) containing C and any subset of P a(B) containing C. D-separation
faithfulness then implies that A and B are conditionally dependent given any subset of
P a(A) containing C and any subset of P a(B) containing C.

We can thus equivalently write (18) as:

H0 : A and B are conditionally independent given some subset of P a(A) \ B

containing C or some subset of P a(B) \ A containing C,

H1 : A and B are conditionally dependent given any subset of P a(A) \ B

containing C and any subset of P a(B) \ A containing C.

(19)

We can bound the Type I error rate of the above hypothesis test by taking the maximum
p-value over certain CI tests:

p(cid:48)
γAB|C

(cid:44) max
i=1,..,m(cid:48)

pA⊥⊥B|Mi,

where Mi denotes a subset of P a(A) \ B containing C or a subset of P a(B) \ A containing
C, and m(cid:48) is the total number of subsets Mi. Of course, in practice, we do not know which
vertices are the parents. However, we can also upper bound (19) as follows:

p(cid:48)
γAB|C

≤ max
i=1,..,m

pA⊥⊥B|Ti

(cid:44) pγAB|C ,

(20)

where Ti denotes a subset of N (A) \ B containing C or a subset of N (B) \ A containing
C, and m denotes the total number of subsets Ti. Note that we do not need the zero Type
II error rate assumption for computing (20), since we assume that the skeleton is provided.

4.2.2 Inferred Skeleton

We have considered orienting the colliders, if we have access to the ground truth skeleton.
We now consider the more complex problem of orienting the colliders, if we must also
statistically infer the skeleton.

We again consider the following null and alternative:

H0 : Unshielded A → C ← B is absent,
H1 : Unshielded A → C ← B is present.

(21)

14

PC with p-values

Now, the PC algorithm determines that the alternative holds, if all of the following condi-
tions are true:

1. A and C are conditionally dependent given any subset of P a(A) \ C

2. B and C are conditionally dependent given any subset of P a(B) \ C

and any subset of P a(C) \ A.

and any subset of P a(C) \ B.

3. A and B are conditionally dependent given any subset of P a(A) \ B

containing C and any subset of P a(B) \ A containing C.

We therefore have the following equivalent form of the null and alternative as in (21), if we
assume A and B are non-adjacent:

H0 : At least one condition from (22) does not hold,
H1 : All conditions from (22) hold.

Note that the non-adjacency assumption is reasonable because we did not have enough
Indeed, non-
statistical evidence to invalidate the assumption when we executed (12).
adjacencies are always assumed unless the data suggests that the null of (12) is unlikely.
Now, the alternative of (23) is a series of three logical conjunctions, and the null is a series
of three logical disjunctions as in (10), so the Type I error rate of (23) can be bounded
using the intersection bound:

Pr(Conditions 1, 2, 3 |H0) ≤ Pr(Any one condition |H0)

≤ max{h1, h2, h3}.

We will be using shorthand from here on. We write (23) equivalently as:

H0 : ¬(A − C) ∨ ¬(B − C) ∨ ¬γAB|C,
H1 : (A − C) ∧ (B − C) ∧ γAB|C,

where A − C, B − C, and γAB|C represent Condition 1, 2 and 3 from (22), respectively. We
therefore have a p-value bound of (21) similar to (24):

(cid:16)

Pr

(A − C) ∧ (B − C) ∧ γAB|C|H0
(cid:16)
A − C(cid:12)
(cid:16)
(cid:110)

(cid:12)¬(A − C)
A − C(cid:12)

(cid:12)¬(A − C)

, Pr

Pr

(cid:17)

(cid:16)

(cid:17)

≤ Pr

≤ max

(cid:17)

≤ max{pA−C, pB−C, pγAB|C }.

B − C(cid:12)

(cid:17)

(cid:16)

(cid:12)¬(B − C)

, Pr

γAB|C

(cid:12)
(cid:12)¬γAB|C

(cid:17)(cid:111)

Notice that computing pγAB|C requires N (A) and N (B), not just their respective empirical
estimates (cid:99)N (A) and (cid:99)N (B) which PC can discover. However, we can invoke a zero Type
II error rate assumption in order to ensure that N (A) ⊆ (cid:99)N (A) and N (B) ⊆ (cid:99)N (B) as
explained in detail in Section 4.4, so pγAB|C can still be upper bounded. The assumption

15

(22)

(23)

(24)

(25)

(26)

Strobl, Spirtes and Visweswaran

also ensures that we can upper bound pA−C and pB−C according to Section 4.1. We conclude
that a zero Type II error rate ensures that (26) can be computed.

Next, consider the situation where PC can orient any one edge by using more than one
unshielded v-structure. For example, consider the DAG in Figure 3. In this case, PC can
orient A − C by using either B1 → C or B2 → C (or both); we may therefore want to
take both situations into account. Note that the original PC algorithm always orients an
edge according to one v-structure which it picks arbitrarily according to the ordering of its
computations. We thus only require the bound (26) in this case. However, we will propose
a modiﬁed PC algorithm in Section 5 which takes into account all possible ways to orient
one edge. Now, we can use the following null and alternative for Figure 3 when assuming
that both A and B1 and A and B2 are non-adjacent:

H0 : ¬(A − C) ∨
(cid:16)

H1 : (A − C) ∧

(cid:16)

(cid:17)
[¬(B1 − C) ∨ ¬γAB1|C] ∧ [¬(B2 − C) ∨ ¬γAB2|C]

,

(cid:17)
[(B1 − C) ∧ γAB1|C] ∨ [(B2 − C) ∧ γAB2|C]

.

(27)

We can therefore bound the Type I error rate of (27) as follows, where G = ¬(A − C) and
H = [¬(B1 − C) ∨ ¬γAB1|C] ∧ [¬(B2 − C) ∨ ¬γAB2|C], H1 = ¬(B1 − C) ∨ ¬γAB1|C, and
H2 = ¬(B2 − C) ∨ ¬γAB2|C:

(cid:110)

(cid:110)

(cid:110)

(cid:16)

Pr

(cid:16)

(A − C) ∧
(cid:110)

[(B1 − C) ∧ γAB1|C] ∨ [(B2 − C) ∧ γAB2|C]

(cid:17)

(cid:17)(cid:12)
(cid:12)H0

≤ max

Pr(A − C|G), Pr

(B1 − C) ∧ γAB1|C

≤ max

Pr(A − C|G), Pr

(B1 − C) ∧ γAB1|C|H

(cid:105)

∨
(cid:17)

(cid:104)
(B2 − C) ∧ γAB2|C

(cid:105)(cid:12)
(cid:12)H

(cid:17)(cid:111)

(cid:16)

+ Pr
(cid:17)

(cid:17)(cid:111)

(B2 − C) ∧ γAB2|C|H
(cid:16)

(B2 − C) ∧ γAB2|C|H2

(cid:17)(cid:111)

(28)

(cid:16)(cid:104)

(cid:16)

(cid:16)

= max

Pr(A − C|G), Pr

(cid:16)

(B1 − C) ∧ γAB1|C|H1
(cid:110)
B1 − C(cid:12)
(cid:17)

Pr

(cid:12)¬(B1 − C)

+ Pr
(cid:17)

, Pr(γAB1|C
(cid:111)(cid:111)

≤ max

Pr(A − C|G), max
(cid:110)
B2 − C(cid:12)

Pr

(cid:16)

+ max
(cid:110)
pA−C, max{pB1−C, pγAB1|C } + max{pB2−C, pγAB2|C }

(cid:12)¬(B2 − C)

, Pr(γAB2|C

≤ max

(cid:111)
.

(cid:12)
(cid:12)¬γAB2|C)

(cid:12)
(cid:12)¬γAB1|C)

(cid:111)

Figure 3: Here, one can orient the edge A − C according to the two unshielded v-structures

A → C ← B1 and A → C ← B2.

B1

B2

A

C

16

PC with p-values

More generally, for an arbitrary number, say j, of multiple possible ways to orient A − C
by unshielded v-structures, we have:

(cid:16)

Pr

(A − C) ∧

(cid:16)

(cid:110)

≤ max

pA−C,

i=1

[(B1 − C) ∧ γAB1|C] ∨ ... ∨ [(Bj − C) ∧ γABj |C]
j
(cid:88)

(cid:111)
,
max{pBi−C, pγABi|C }

(cid:17)

(cid:17)(cid:12)
(cid:12)H0

(29)

assuming that B1, . . . , Bj are all non-adjacent to A.

4.3 Orientation Rules

We now consider bounding the p-values of edges which are oriented using the orientation
rules of the PC algorithm. Recall from (1) that the PC algorithm only requires the repeated
application of three orientation rules to be complete. We analyze these three orientation
rules in separate subsections.

4.3.1 First orientation rule

We can construct the hypothesis test for the ﬁrst orientation rule as follows according to
the suﬃcient conditions of the ﬁrst rule in (1):

H0 : ¬(A − B) ∨ ¬(C → A),
H1 : (A − B) ∧ (C → A).

(30)

We again also assume that C and B are non-adjacent. Now the PC algorithm determines
that the alternative holds, if all of the following conditions are true:

1. A − B : A and B are conditionally dependent given any subset of P a(A) \ B and any

subset of P a(B) \ A.

2. C → A : An edge is oriented from C to A under two scenarios. In the ﬁrst, the edge
is oriented because A is the collider in an unshielded v-structure. In the second, the
edge is oriented due to the previous application of an orientation rule.

We thus have a logical conjunction and can bound the Type I error rate using the intersection
bound:

Pr((A − B) ∧ (C → A)|H0) ≤ max{pA−B, pC→A},

where pC→A refers to the p-value bound for the hypothesis test of an unshielded v-structure
or a previously applied orientation rule. Of course, pC→A will be the former when the PC
algorithm begins to execute the orientation rules. More generally, for Ci → A that can
orient A − B where i = 1, ..., j, we have:

(cid:16)

Pr

(A − B) ∧

(cid:105)
(cid:104)
|H0
(C1 → B) ∨ · · · ∨ (Cj → A)

(cid:17)

(cid:110)

≤ max

pA−B,

pCi→A

(cid:111)
,

j
(cid:88)

i=1

(31)

where we require that C1, . . . , Cj are all non-adjacent to B.

17

Strobl, Spirtes and Visweswaran

4.3.2 Second orientation rule

We have the following hypothesis test according to the suﬃcient conditions of the second
rule in (1):

H0 : ¬(A − B) ∨ ¬(A → C → B),
H1 : (A − B) ∧ (A → C → B).

(32)

Hence, by conjunction:

Pr((A − B) ∧ (A → C → B)|H0) ≤ max{pA−B, pA→C→B},

where pA→C→B ≤ max{pA→C, pC→B}. The above Type I error rate can therefore be further
upper bounded by max{pA−B, pA→C, pC→B}. More generally, we have:

Pr

(A − B) ∧

(cid:104)

(cid:105)
(A → C1 → B) ∨ ... ∨ (A → Cj → B)

|H0

(cid:17)

(cid:110)

(cid:111)

(cid:110)

≤ max

pA−B,

pA→Ci→B

≤ max

pA−B,

max{pA→Ci, pCi→B}

(33)

(cid:111)
.

j
(cid:88)

i=1

(cid:16)

j
(cid:88)

i=1

4.3.3 Third orientation rule

We have the following null and alternative by the suﬃcient conditions of the third rule in
(1), assuming that C and D are non-adjacent:

H0 : ¬(A − B) ∨ ¬(A − C → B) ∨ ¬(A − D → B),
H1 : (A − B) ∧ (A − C → B) ∧ (A − D → B).

(34)

We can bound the Type I error rate of the above hypothesis test as follows:

(cid:16)

(cid:110)

Pr

(A − B) ∧ (A − C → B) ∧ (A − D → B)|H0

≤ max{pA−B, pA−C→B, pA−D→B}

≤ max

pA−B, max{pA−C, pC→B}, max{pA−D, pD→B}

(cid:17)

(cid:111)
.

The general case is slightly more complicated than the ﬁrst and second orientation rules.
In this case, we need to control the Type I error rate of accepting at least two paths as
opposed to one. Let the set D include all three-node paths from A to B with the ﬁrst edge
undirected from A to a middle vertex and the second edge directed from the middle vertex
to B such that the ith element of D is:

Di (cid:44) A − Ci → B.

Let us suppose D has a total of n elements and assume that no middle vertex Ci is adjacent
to any other middle vertex. Now, let D(cid:48) be the set containing all of the n choose 2 elements
of D. The ith element in D(cid:48) is therefore:

D(cid:48)
i

(cid:44) {A − Ck → B, A − Cl → B},

18

PC with p-values

(cid:16)

(cid:16)

(cid:104)

r
(cid:88)

i=1

(cid:104)

r
(cid:88)

i=1

where k and l are the distinct indices represented the two chosen middle vertices. Let D(cid:48)
i,1
(cid:1). We then
and D(cid:48)
have:

i,2 be the ﬁrst and second elements in D(cid:48)

i, respectively. Also let r = (cid:0)n

2

Pr

(A − B) ∧

(D(cid:48)

1,1 ∧ D(cid:48)

1,2) ∨ ... ∨ (D(cid:48)

r,1 ∧ D(cid:48)

r,2)

(cid:17)

(cid:105)(cid:12)
(cid:12)H0

(cid:110)

≤ max

pA−B,

Pr(D(cid:48)

i|H0)

(cid:111)
,

where Pr(D(cid:48)

i|H0) (cid:44) p{A−Ck→B,A−Cl→B} and is bounded as follows:

Pr(D(cid:48)

i|H0) ≤ max{pA−Ck→B, pA−Cl→B}

≤ max{pA−Ck , pCk→B, pA−Cl, pCl→B}
(cid:44) ¨Pr(D(cid:48)

i|H0),

We therefore have:

Pr

(A − B) ∧

(D(cid:48)

1,1 ∧ D(cid:48)

1,2) ∨ ... ∨ (D(cid:48)

r,1 ∧ D(cid:48)

r,2)

(cid:17)

(cid:105)(cid:12)
(cid:12)H0

(cid:110)

≤ max

pA−B,

¨Pr(D(cid:48)

i|H0)

(cid:111)
.

(35)

4.4 Summary and Analysis of the Bounds

We derived several bounds for edge orientation as summarized in Table 1. We created the
bounds by engineering speciﬁc hypothesis tests and successively applying the union and
intersection bounds accordingly. Note that j and r are usually very small in sparse graphs.
One may now wonder whether PC can actually control the bounds listed in Table 1 (we
say that a quantity can be controlled, if the quantity can be upper bounded). Recall that we

Table 1: P-value bounds for all of the edge types in a CPDAG. Note that Si ⊆ {N (A) \ B}

or Si ⊆ {N (B) \ A}.

P-Value Bound

Equation Num.

Edge Type

Undirected

Unshielded v-structure max

First orientation rule

Second orientation rule max

Third orientation rule

maxi pA⊥⊥B|Si

(cid:110)
pA−C, (cid:80)j
i=1 max{pBi−C, pγABi|C }
(cid:110)
pA−B, (cid:80)j

i=1 pCi→A

max

(cid:111)

(cid:111)

(cid:110)
pA−B, (cid:80)j
(cid:110)
pA−B, (cid:80)r

max

i=1 max{pA→Ci, pCi→B}

(cid:111)

¨Pr(D(cid:48)

i|H0)

i=1

(cid:111)

(16)

(29)

(31)

(33)

(35)

19

Strobl, Spirtes and Visweswaran

provided a rough, aﬃrmative answer to the question in Sections 4.1 and 4.2.2 by assuming
a zero Type II error rate. We now spell out a more detailed answer via a theorem whose
proof builds on the argument of Theorem 4 in (Armen and Tsamardinos, 2014).

Theorem 4 Suppose that the PC algorithm is applied to a sample from P represented by
DAG G. If we have:

1. P is d-separation faithful to G,

2. The Type II error rate is zero,

3. The PC algorithm also tests whether any two non-adjacent vertices A, B with common
neighbor C are conditionally dependent given any subset of P a(A) \ B containing C
and any subset of P a(B) \ A containing C,

then all of the p-value bounds in Table 1 can be controlled using the p-values of the CI tests
executed by PC.

Proof Consider any two vertices A and B. Algorithm 1 starts with a fully connected
graph, so we have B ∈ (cid:99)N (A) and A ∈ (cid:99)N (B) in the beginning. Note that Algorithm 1
executes testA⊥⊥B|S for all S ⊆ (cid:99)N (A) \ B and for all S ⊆ (cid:99)N (B) \ A. The zero Type II error
rate ensures the following: if the alternative holds, then the alternative is accepted. As a
result, Algorithm 1 will not remove any vertices adjacent to A and any vertices adjacent to
B with a zero Type II error rate. Hence, we always have {N (A) \ B} ⊆ { (cid:99)N (A) \ B} and
{N (B) \ A} ⊆ { (cid:99)N (B) \ A}. Algorithm 1 therefore must eventually execute testA⊥⊥B|S for
all S ⊆ {N (A) \ B} and for all S ⊆ {N (B) \ A}, so (16) can be controlled.

For (29), the p-value bounds for undirected edges can already be controlled by the
previous paragraph. We must now argue that pγAB|C can be controlled. Let C be a collider
between non-adjacent vertices A and B. Now notice that C ∈ N (A) ⊆ (cid:99)N (A) and C ∈
N (B) ⊆ (cid:99)N (B), so Algorithm 2 must execute testA⊥⊥B|S for all S ⊆ {N (A) \ B} containing
C and for all S ⊆ {N (B) \ A} containing C. Hence pγAB|C can be controlled.

Now the p-value bounds (31), (33) and (35) can be controlled trivially because the p-

value bounds for (15) and (29) can be controlled.

In other words, PC can control the bounds in Table 1 with some additional CI tests and a
zero Type II error rate.

Of course, the Type II error rate is never zero in practice, but this becomes less of an
issue as the sample size increases. We may also consider reducing the Type II error rate by
simultaneously implementing three strategies:

1. Use a liberal (higher) α threshold. We for example often use an α threshold of 0.20 in
the experiments. This is the simplest strategy which decreases the Type II error rate
but also increases the Type I error rate. However, we can then control the Type I error
rate post-hoc with an FDR controlling procedure. Of course, setting the α threshold
too high will prevent the PC algorithm from terminating within a reasonable amount
of time as well as loosen the p-value bounds, since the CI tests will fail to explain
away many edges. We therefore cannot rely entirely on this ﬁrst strategy.

20

PC with p-values

2. Use hypothesis tests whose p-value bounds are robust to Type II errors. The hy-
pothesis tests in Section 4.4 are in fact robust to such errors due to the intersection
bound as explained in detail in Appendix A.2. Brieﬂy, we can also reasonably con-
sider modifying the null hypotheses of (25), (30), (32) and (34) to “no edges between
any of the vertices.” This corresponds to converting the logical disjunctions in the
null of (10) into conjunctions which in turns leads to a less robust p-value bound
involving the minimum of a set of p-values instead of the maximum. As a result,
under-estimating one p-value in the p-value set due to Type II error(s) can cause PC
to also under-estimate the bound of (25), (30), (32) or (34).

3. Modify the PC algorithm to prevent and catch many Type II errors.

The last strategy is more complex, so we discuss it in detail in the next section.

5. The PC Algorithm with P-Values

We now propose a modiﬁed PC algorithm called PC with p-values (PC-p) that reduces the
inﬂuence of Type II errors by preventing and catching potential Type II errors. At the same
time, PC-p is correct - the algorithm operates diﬀerently than PC, but it maintains PC’s
desirable soundness and completeness properties.

The PC-p algorithm involves two ideas. First, PC-p performs skeleton discovery with the
same skeleton discovery procedure used in the PC-stable algorithm (Colombo and Maathuis,
2014). This procedure ensures that the algorithm does not skip some CI tests due to Type
II errors and variable ordering. The second idea behind PC-p involves a modiﬁcation to the
procedure for propagating edge orientations. Speciﬁcally, if two edge orientations conﬂict,
PC-p admits bidirected edges instead of over-writing previous orientations like PC. PC-p
then unorients the bidirected edges as well as the directed edges which were directly used to
infer the presence of the bidirected edges. The algorithm subsequently labels the resulting
undirected edges as “ambiguous” which ensures that PC-p does not orient additional edges
using the ambiguous edges. Indeed, the PC-p algorithm uses conﬂicts in edge orientation
to detect potential Type II errors and prevent the propagation of the errors throughout the
graph. In practice, we ﬁnd that these two modiﬁcations to the PC algorithm help PC-p
with the BY estimator achieve more accurate strong estimation and control of the FDR
than PC, as we will see in Section 6.

We now describe the PC-p algorithm in detail; however, we will not describe the com-
putation of the p-value upper bounds until Section 5.5 in order to keep the presentation
clear. We have divided the PC-p algorithm into Algorithms 1, 2, 3 and 4, where the ﬁrst
three procedures correspond to Algorithms 5, 6 and 7 of the original PC algorithm.

5.1 Skeleton Discovery

We ﬁrst consider skeleton discovery. The original PC algorithm uses Algorithm 5 to discover
the skeleton. However, Algorithm 5 can cause the sample version of the PC algorithm to skip
some CI tests due to variable ordering and Type II errors. For example, consider the causal
graph in Figure 4a as ﬁrst presented in (Colombo and Maathuis, 2014). In this example,
suppose the CI tests correctly determine that A ⊥⊥ B and B ⊥⊥ D|{A, C} but incorrectly
determine that C ⊥⊥ D|{A, E}. The incorrect inference is a Type II error, since C and D

21

Strobl, Spirtes and Visweswaran

are adjacent in the true graph. Now consider the following ordering of variables for the PC
algorithm: order1(X) = (A, D, B, C, E). In this case, the ordered pair (D, B) is considered
before (D, C) in Algorithm 5, since (D, B) comes earlier in order1(X). The PC algorithm
removes D − B because a CI test determines that D ⊥⊥ B|{A, C} and {A, C} is a subset of
N (D) = {A, B, C, E}. Next, D − C is considered and erroneously removed because a CI
test determines that D ⊥⊥ C|{A, E} and {A, E} is a subset of N (D) = {A, C, E}. We thus
ultimately obtain the skeleton in Figure 4b with order1(X).

Now consider an alternative ordering of the variables: order2(X) = (A, C, D, B, E). In
this case, (C, D) is considered before (D, B) in Algorithm 5, and the algorithm erroneously
removes C − D. Next, the algorithm considers D − B but {A, C} is not a subset of N (D) =
{A, B, E}, so D − B remains. Even when the PC algorithm eventually also considers the
same undirected edge as B − D, {A, C} is again not a subset of N (B) = {C, D, E}, so
B − D remains. In other words, (C, D) is considered ﬁrst in order2(X) which causes C
to be removed from N (D). Algorithm 5 therefore never executes testB⊥⊥D|{A,C}. We thus
ultimately obtain the skeleton in Figure 4c with order2(X).

The previous two examples show that the Type II error of incorrectly determining that
C ⊥⊥ D|{A, E} leads PC to infer two diﬀerent skeletons due to diﬀerences in variable order-
ing. Clearly, we would like to eliminate the dependency of skeleton discovery on variable
ordering and also reduce its dependency on Type II errors at the same time. Fortunately,
Colombo and Maathius proposed such a modiﬁcation of Algorithm 5 as outlined in Algo-
rithm 1. The key diﬀerence between Algorithm 5 and 1 involves the for loop in steps 5-7 of
Algorithm 1 which computes and stores the adjacency sets after each new conditioning set
size. As a result, an incorrect edge deletion due to a Type II error on line 16 of Algorithm 1
no longer eﬀects which CI tests are performed for other pairs of variables with conditioning
set size l. Indeed, the algorithm only modiﬁes the adjacency sets when it increases the con-
ditioning set size. Colombo and Maathius proved that Algorithm 1 is order-independent.
We review the proof here, since it is informative:

Proposition 5 (Colombo and Maathuis, 2014). The skeleton resulting from Algorithm 1
is order-independent.

(a)

(b)

(c)

A

B

A

B

A

B

C

E

D

C

E

D

Figure 4: An example of a situation when PC infers diﬀerent skeletons due to a Type II
error and two variable orderings.
(a) The true causal graph, (b) the skeleton
inferred by PC from order1(X), (c) the skeleton inferred by PC from order2(X).

C

E

D

22

PC with p-values

Proof Consider the removal or retention of some undirected edge A−B at some condition-
ing set size l. The ordering of the variables determines the order in which the edges (line 9)
and subsets S ⊆ a(A) and S ⊆ a(B) (line 11) are considered. However, by construction,
the order in which the edges are considered does not aﬀect the sets a(A) and a(B).

If there is at least one subset S of a(A) or a(B) such that A ⊥⊥ B|S, then any ordering
of the variables will ﬁnd a separating set for A and B (but diﬀerent orderings may lead
to diﬀerent separating sets as illustrated in Example 2 of (Colombo and Maathuis, 2014)).
Conversely, if there is no subset S(cid:48) of a(A) or a(B) such that A ⊥⊥ B|S(cid:48), then no ordering
will ﬁnd a separating set.

Hence, any ordering of the variables leads to the same edge deletions and therefore to

the same skeleton.

In other words, modifying the adjacency sets only when changing the conditioning set
size prevents PC-p from skipping some CI tests during skeleton discovery because of Type II
errors and variable ordering. As a result, Algorithm 1 enables PC-p to perform more of the
required CI tests than Algorithm 5 in order to correctly upper bound the p-value of (12).
However, notice that Algorithm 1 does not prevent all Type II errors from eﬀecting the
skeleton. The edge C − D is for example eliminated in Figure 4 regardless of the ordering
because of the erroneous conclusion that C ⊥⊥ D|{A, E}. As a result, we have C (cid:54)∈ (cid:99)N (D)
which may lead to under-estimation of the p-value bounds for undirected edges connected
to D. We will nonetheless see in Section 6 that Algorithm 1 does help PC-p achieve tighter
estimation and control of the FDR than the original skeleton discovery procedure, since
Algorithm 1 eliminates the inﬂuence of at least some Type II errors.

5.2 Unshielded V-Structures

We now describe Algorithm 2, where we use the circle edge endpoint “◦” as a meta-symbol
representing either a tail or an arrowhead. In Algorithm 2, PC-p orients edges according
to all unshielded v-structures in line 3, even if two v-structures conﬂict with each other in

(a)

B

(b)

B

(c)

B

A

C

A

C

A

C

D

E

D

E

D

E

Figure 5: Example of how Algorithm 2 deals with conﬂicting edge orientations. a) The
ground truth, b) the inferred graph with two v-structures A → B ← C and
D → C ← B that lead to the bi-directed edge B ↔ C, and c) the ﬁnal graph
after unorienting both v-structures.

23

Strobl, Spirtes and Visweswaran

the direction of a particular edge. In the case of conﬂict, PC-p admits a bidirected edge
instead of favoring one particular direction over the other. The algorithm then unorients all
v-structures involving the bidirected edges and labels the unoriented edges as “ambiguous”
in line 23 because bidirected edges may result from a Type II error. For example, consider
the ground truth in Figure 5a and assume that Algorithm 1 correctly discovers all of the
undirected edges. Moreover, assume Algorithm 1 correctly ﬁnds a separating set of B
and D that does not contain C but incorrectly ﬁnds a separating set of A and C that
does not contain B. The latter is a Type II error, since the alternative should have been
accepted rather than rejected when conditioning on a subset not containing B.
In this
case, PC-p ﬁrst orients the edges according to Figure 5b. However, notice that the two
unshielded v-structures conﬂict with each other due to the bidirected edge B ↔ C, and
PC-p cannot determine which v-structure admitted the Type II error. As a result, the
algorithm unorients all of the edges in both v-structures as in Figure 5c. PC-p then labels
the three unoriented edges as “ambiguous” so that the algorithm does not orient any other
undirected edges based on these three edges using the orientation rules. The labeling thus
prevents the algorithm from propagating Type II errors by orienting additional edges based
on the erroneous directions.

5.3 Orientation Rules

Notice that Algorithm 7 uses “else if” statements instead of all “if” statements. The “else
if” approach is of course faster, but it also causes PC to ignore any interactions between the
orientation rules in the sense that, if one rule orients an edge, then no other rule can orient
an edge. PC-p performs the orientation rules according Algorithm 3 which uses the “if”
approach to attempt to apply all three orientation rules to each non-ambiguous undirected
edge. Now, if bidirected edges exist after the rules are applied, then Algorithm 3 unorients
the edge as well as all edges involved in the suﬃcient conditions of the associated oientation
rules in lines 16-18. The algorithm then labels the unoriented edges as “ambiguous” in line
19 similar to unshielded v-structure orientation in Section 5.2. For example, in Figure 6,
rule 1 of PC-p induces a bidirected edge between A − B, so PC-p unorients and labels all
directed edges which satisfy the suﬃcient conditions of rule 1 as ambiguous; these include
D → A, C → A, E → B, and F → B.

5.4 Analysis of PC-p

We now have the following analysis of the PC-p algorithm:

Figure 6: Here, a bidirected edge between A and B results from the application of rule 1.
PC-p therefore unorients and labels all edges in the above graph as “ambiguous”
according to the suﬃcient conditions of rule 1.

D

A

E

C

B

F

24

PC with p-values

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

Data: X n, α
Result: (cid:98)G, P 1, S, I

1 Form a completely connected undirected graph (cid:98)G on the variable set in X n
2 l = −1
3 repeat
4

l = l + 1
for each variable A in (cid:98)G do

a(A) ← (cid:99)N (A)

end
repeat

Select a new ordered pair of variables (A, B) that are adjacent in (cid:98)G and
satisfy |a(A) \ B| ≥ l
repeat

Choose a new set S ⊆ {a(A) \ B} with |S| = l
p ← p-value from testA⊥⊥B|S
if p ≤ α then

Insert p into P 1

AB and P 1

BA

Delete A − B from (cid:98)G
Empty P 1
AB and P 1
BA
Insert S into SAB and SBA

else

end

until A − B is deleted from (cid:98)G or all S ⊆ {a(A) \ B} with |S| = l have been
considered ;

until all ordered pairs of adjacent variables (A, B) in (cid:98)G with |a(A) \ B| ≥ l have
been considered ;

22 until all ordered pairs of adjacent variables (A, B) in (cid:98)G satisfy |a(A) \ B| ≤ l;
23 for each nonempty P 1
AB}
24

P 1
Place the same unique identiﬁer for A − B into IAB and IBA

AB ← max{P 1

AB in P 1 do

25
26 end

Algorithm 1: Skeleton Discovery

25

Strobl, Spirtes and Visweswaran

Data: X n, (cid:98)G, P 1, S, I
Result: (cid:98)G, P 2, I

1 for all ordered pairs of non-adjacent variables (A, B) with common neighbor C do
2

if C (cid:54)∈ any set in SAB then

Replace A ◦−◦ C ◦−◦ B with A ◦→ C ←◦ B
l = 0
repeat

l = l + 1
repeat

Choose a new set S ⊆ (cid:99)N (A) including C or S ⊆ (cid:99)N (B) including C
with |S| = l
p ← p-value from testA⊥⊥B|S
Insert p into P (cid:48)(cid:48)

until all S ⊆ (cid:99)N (A) including C and all S ⊆ (cid:99)N (B) including C with
|S| = l have been considered ;

until all S ⊆ (cid:99)N (A) including C and all S ⊆ (cid:99)N (B) including C satisfy
|S| ≤ l ;
Insert max{P 1
Insert max{P 1
Empty P (cid:48)(cid:48)

AC, P (cid:48)(cid:48)} into P (cid:48)
BC, P (cid:48)(cid:48)} into P (cid:48)

BC

AC

Unorient the edge to A − C in (cid:98)G(cid:48)
For each additional edge directed to A in (cid:98)G, unorient the edge in (cid:98)G(cid:48)
For each additional edge directed to C in (cid:98)G, unorient the edge in (cid:98)G(cid:48)
Label the unoriented edges as “ambiguous” in (cid:98)G(cid:48)

end

16
17 end
18 (cid:98)G(cid:48) ← (cid:98)G
19 for each A ↔ C in (cid:98)G do
20

23
24 end
25 (cid:98)G ← (cid:98)G(cid:48)
26 for each A → C in (cid:98)G do
AC ← max
27

P 2
if one p-value in P (cid:48)

P 1

(cid:110)

28

(cid:111)

AC, sum[P (cid:48)
AC then

AC]

Place the same unique identiﬁer into IAC and IBC for unshielded collider
A → C ← B

else if more than one p-value in P (cid:48)
Place a unique identiﬁer into IAC

AC then

end

32
33 end

Algorithm 2: Unshielded V-structures

3

4

5

6

7

8

9

10

11

12

13

14

15

21

22

29

30

31

26

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

PC with p-values

Data: (cid:98)G, P 1, P 2, I
Result: (cid:98)G, P 2, I

1 repeat

(cid:98)G(cid:48) ← (cid:98)G
if A − B non-ambiguous and ∃i s.t. Ci → A with Ci and B non-adjacent in (cid:98)G
then

Replace A ◦−◦ B in (cid:98)G(cid:48) with A ◦→ B
P (cid:48)

A,B, sum{P 2

A,B ← sum

P (cid:48)

(cid:110)

CiA, ∀i s.t. Ci → A with Ci, B non-adjacent}

(cid:111)

end
if A − B non-ambiguous and ∃i s.t. A → Ci → B in (cid:98)G then

Replace A ◦−◦ B in (cid:98)G(cid:48) with A ◦→ B
(cid:104)
P (cid:48)
AB, sum

AB ← sum

max{P 2

P (cid:48)

(cid:110)

ACi

, P 2

CiB}, ∀i s.t. A → Ci → B

(cid:105)(cid:111)

end
if A − B non-ambiguous and ∃i, j s.t. A − Ci → B, A − Cj → B with A − Ci
and A − Cj non-ambiguous, and Ci and Cj non-adjacent in (cid:98)G then

(cid:110)

Replace A ◦−◦ B in (cid:98)G(cid:48) with A ◦→ B
(cid:104)
, P 2
P (cid:48)
Cj B}, ∀i, j s.t. A − Ci →
AB, sum
AB ← sum
B, A − Cj → B with A − Ci and A − Cj non-ambiguous, and Ci and Cj
non-adjacent

CiB, P 1

max{P 1

, P 2

ACj

ACi

(cid:105)(cid:111)

P (cid:48)

end
for each A ↔ B in (cid:98)G(cid:48) do

Unorient to A − B in (cid:98)G(cid:48)
For each edge in the suﬃcient conditions of each orientation rule that led to
the creation of a directed edge to A in (cid:98)G, unorient the edge in (cid:98)G(cid:48)
For each edge in the suﬃcient conditions of each orientation rule that led to
the creation of a directed edge to B in (cid:98)G, unorient the edge in (cid:98)G(cid:48)
Label the unoriented edges as “ambiguous” in (cid:98)G(cid:48)

end
(cid:98)G ← (cid:98)G(cid:48)
for each A → B in (cid:98)G do

Place a unique identiﬁer into IAB
AB, P (cid:48)
P 2

AB ← max{P 1

AB}

end
Empty P (cid:48)

26
27 until there are no more edges to orient;
28 for each non-empty cell P 1
29
30 end

AB s.t. P 2

BA ← P 1

AB, P 2

P 2

AB

AB and P 2

BA are empty do

Algorithm 3: Orientation Rules

27

Strobl, Spirtes and Visweswaran

Data: (cid:98)G, P 2, I, α, q
Result: (cid:92)F DRBY (α), (cid:98)G∗
// Estimation

of unique identiﬁers in I

// Control

identiﬁers in I

1 (cid:92)F DRBY (α) ← Solution of 2 using threshold α and m corresponding to the number

2 α∗ ← Solution of 3 using FDR level q and m corresponding to the number of unique

3 (cid:98)G∗ ← (cid:98)G with edges associated with p-values above α∗ eliminated

Algorithm 4: FDR Estimation and Control

Theorem 6 The PC-p algorithm with a CI oracle is sound and complete.

Proof PC is sound and complete, so it is enough to prove that PC-p and PC will perform
the exact same edge deletion and edge orientation operations with a CI oracle. Note that
Algorithm 1 has already been shown to be sound and complete up to skeleton discovery
(Colombo & Maathius 2014). Algorithm 1 will therefore perform the exact same edge
deletions as Algorithm 5 with a CI oracle. Now, Algorithm 2 will also perform the same
edge orientations as Algorithm 6 with a CI oracle, since there will never be conﬂicting edge
orientations. Lastly, for Algorithm 3, if there exists an edge that can be oriented by more
than rule, then the edge must be oriented in the same direction by the other two rules.
Algorithm 3 therefore returns the same edge orientations as Algorithm 7. We have proved
equivalence in outputs of Algorithms 1, 2 and 3 of PC-p to Algorithms 5, 6 and 7 of PC,
respectively. Algorithm 4 is not involved in graph structure discovery.

The output of PC-p is therefore equivalent to the output of PC in the large sample limit
with a consistent CI test, even though PC-p performs more operations than PC.

5.5 Computation of the P-Values

We now address the issue of computing the upper bounds of the p-values. Let us ﬁrst
consider Algorithm 1. Algorithm 1 takes as input the dataset X n and the signiﬁcance
threshold α. The algorithm then stores the p-values of all signiﬁcant CI tests in cell P 1

Figure 7: An example of a situation where two of PC’s orientation rules, speciﬁcally rules 2
and 3, can orient one undirected edge (A − B) in the same direction. In this case,
PC oriented all of the currently directed edges using unshielded v-structures.

F

C

D

E

A

B

28

PC with p-values

when it reaches line 14. Notice that the algorithm stores the p-values of all signiﬁcant tests
involving A and B in both P 1
BA. Algorithm 1 next computes the maximum over
the p-values for all surviving edges in line 24 as in (16).

AB and P 1

Algorithm 2 takes P 1 from Algorithm 1 as input. Moreover, unlike Algorithm 6 of PC,
Algorithm 2 also takes as input the dataset X n, since PC-p must apply (29) in order to
obtain the upper bounds of the p-values for oriented unshielded v-structures. Indeed, Algo-
rithm 2 executes testA⊥⊥B|S for all S ⊆ (cid:99)N (A) containing C and all S ⊆ (cid:99)N (B) containing
C in steps 4-12 for each A − C − B such that A and B are non-adjacent and C (cid:54)∈ SAB. Now,
Algorithm 2 ultimately stores all of the p-values needed to compute pγAB|C as in (20) in P (cid:48)(cid:48)
via line 10. Algorithm 2 then stores the maximum over pA−C and pγAB|C in P (cid:48)
BC instead
of P (cid:48)
AC in line 13. A similar set of operations eventually stores the maximum over pB−C
and pγAB|C into P (cid:48)
AC in line 14. Note that multiple elements can enter into P (cid:48)
AC
when multiple v-structures can orient one edge. Finally, in line 27, Algorithm 2 takes the
maximum over P 1
AC to obtain pA→C in
P 2 according to (29) and similarly takes the maximum over P 1
BC to
obtain pB→C in P 2.

AC as returned from Algorithm 1 and the sum of P (cid:48)

BC and the sum of P (cid:48)

BC and P (cid:48)

Algorithm 3 takes P 2 from Algorithm 2 as input. Next, in rule 1, Algorithm 3 adds
up the p-values associated with Ci → A, ∀i and places the result in P (cid:48)
AB in line 5 for
computing (31). Then, Algorithm 3 sums over the maxima of pA→Ci and pCi→B in rule
2 ∀i s.t. A → Ci → B in line 9 for ultimately computing (33). Subsequently, in rule
3, Algorithm 3 ﬁnds all n edges such that A − Ci → B. The algorithm then ﬁnds all of
the n choose 2 pairs, say r of them. For each pair, say A − C1 → B and A − C2 → B,
Algorithm 3 computes pA−C1→B and pA−C2→B as the maximum over pA−C1 and pC1→B
and the maximum over pA−C2 and pC2→B, respectively. Algorithm 3 next sums the p-values
over all r pairs in line 13 for computing (35). Note that Algorithm 3 also takes an outer-
sum involving P (cid:48)
AB in lines 5, 9 and 13 of rules 1, 2 and 3, respectively; these summations
correspond to logical disjunctions when multiple orientation rules can orient one edge in the
same direction. For example, rules 2 and 3 can orient A − B in the same direction in Figure
7. Two applications of rule 1 can also orient A − B in the same direction in Figure 6, if
we remove one of the unshielded v-structures from the graph. Now, for all non-ambiguous
edges, Algorithm 3 then stores the maximum over the p-values from Algorithm 1 and P (cid:48)
into P 2 in line 24. This process is repeated until no more edges can be oriented. Algorithm
3 ﬁnally transfers the p-values of all of the remaining undirected edges in (cid:98)G from P 1 to P 2
in lines 28- 30. The algorithm therefore eventually outputs all of the ﬁnal p-values in P 2 as
desired.

5.6 Controlling the False Discovery Rate

PC-p controls the FDR per hypothesis test as opposed to per edge, since the algorithm
can sometimes orient two edges according to the same hypothesis test during unshielded v-
structure discovery. Indeed, controlling the p-values per edge as opposed to per hypothesis
test can result in overly conservative FDR estimation or control because an FDR estimator
or controlling procedure may count the p-value of one hypothesis test multiple times.

PC-p keeps track of each distinct hypothesis test in Algorithms 1, 2 and 3 by using
indexing cell I as follows. First, Algorithm 1 assigns the same, unique identiﬁer to the p-

29

Strobl, Spirtes and Visweswaran

value bounds in both PAB and PBA in line 25. Next, if we have one v-structure A → C ← B
that orients A − C and C − B, then Algorithm 2 associates both A → C and C ← B with
the same hypothesis test and therefore the same identiﬁer in line 29. On the other hand,
if multiple unshielded v-structures can orient one edge, then Algorithm 2 assigns the edge
a unique identiﬁer in line 31, since a unique hypothesis test exists per edge in this case.
Algorithm 3 ﬁnally assigns a unique identiﬁer to each newly oriented edge in line 23 because
each newly oriented edge also corresponds to a distinct hypothesis test.

We can now use Algorithm 4 to estimate and control the FDR using the identiﬁers in
I and the p-value bounds in P 2 as returned from Algorithm 3. Algorithm 4 estimates
the FDR by solving 2 to obtain (cid:92)F DRBY , where m corresponds to the number of unique
identiﬁers in I. The algorithm subsequently controls the FDR by solving 3 to obtain α∗.
Algorithm 4 then eliminates all edges with p-values below α∗ in P 2 in order to obtain (cid:98)G∗;
this process ensures that the expected FDR does not exceed q in (cid:98)G∗.

5.7 Conclusion

We wrap-up this section with the following theorem:

Theorem 7 Consider the same assumptions as Theorem 4. Then PC-p achieves conser-
vative point estimation and strong control of the FDR across the edges in (cid:98)G.

Proof We have already shown that PC-p can control the p-values of all of the edges in (cid:98)G
from Theorem 4. Estimation follows because the solution of 2 achieves conservative point
estimation of the FDR at threshold α when the p-values are controlled (Benjamini and
Yekutieli, 2001). Similarly, control follows because eliminating the edges associated with
p-values above α∗ as obtained from 3 achieves strong control of the FDR at level q when
the p-values are in turn controlled (Benjamini and Yekutieli, 2001).

The PC-p algorithm thus corresponds to a valid method for estimating and controlling the
FDR in the estimated CPDAG.

Note ﬁnally that PC-p takes slightly longer than original PC to complete because it
performs extra computations. However, PC-p runs at approximately the same speed as
PC-stable, since v-structure detection and orientation rule application take an inﬁnitesimal
amount of time compared to skeleton discovery.

6. Experiments

6.1 Algorithms and Metrics

We evaluated six algorithms:

1. PC-p,

2. PC-p without stabilization in the skeleton discovery procedure,

3. PC-p without ambiguous labelings during v-structure orientation and orientation rule

application (PC-p without ambiguation),

4. PC-p without both stabilization and ambiguation,

30

PC with p-values

5. PC-p without hypothesis tests with robust p-value bounds − we chose the null hy-
potheses to be a series of logical conjunctions so that no edges are present between
any of the variables. As a result, the p-values take on minimal values as described in
Appendix A.2. We call this procedure PC-p without robust p-values.

6. The original PC algorithm with p-value computation − that is, we do not incorporate
stabilization, and the algorithm arbitrarily over-writes edge orientations. We compute
p-values according to the v-structure or rule which ultimately orients each edge in the
CPDAG. The algorithm also performs some additional CI tests in order to compute
20 as described in Section 4.1.

We ran these six algorithms because they are the only algorithms that allow us to compute
the FDR across the entire CPDAG from the estimated p-values.

We assessed the FDR of the above six algorithms in detail using control and estimation
bias3. An algorithm exhibits low control bias at FDR level q when an FDR controlling
procedure can accurately eliminate edges in the CPDAG using the p-values so that the
FDR is in fact q. On the other hand, an algorithm exhibits low estimation bias when an
FDR estimate closely matches the true FDR of the CPDAG. Notice that both control and
estimation bias are important and can serve diﬀerent purposes. As a result, we prefer an
algorithm that exhibits both low control and estimation bias.

We used the mean of the following quantities to assess control bias:

uc((cid:92)F DRBY , q) := max{F DR(α∗) − q, 0},
oc((cid:92)F DRBY , q) := max{q − F DR(α∗), 0},

(36)

where uc((cid:92)F DRBY , q) denotes under-control at FDR level q with the BY FDR estimate,
and oc((cid:92)F DRBY , q) similarly denotes over-control.
In the experiments, we varied q from
[0.001, 0.1] using 100 equispaced intervals. Note that we compute both under-control and
over-control per CPDAG. A method achieves strong control when the mean under-control
taken across the hypothesis tests is zero (Armen and Tsamardinos, 2014). Moreover, the less
the mean over-control, the tighter the strong control. As a result, achieving a lower mean
under-control is more important than achieving a lower mean over-control. We therefore say
that one method outperforms another if the method achieves a lower mean under-control
while also maintaining a reasonably low mean over-control.

We used the mean of the following similar quantities for estimation bias:

ue((cid:92)F DRBY , α) := max{F DR(α) − (cid:92)F DRBY (α), 0},
oe((cid:92)F DRBY , α) := max{(cid:92)F DRBY (α) − F DR(α), 0},

(37)

where ue((cid:92)F DRBY , α) denotes under-estimation at threshold level α with the BY FDR esti-
mate, and oe((cid:92)F DRBY , α) similar denotes over-estimation. We varied the α threshold from
[1E-10, 0.1] with 100 equispaced intervals in the experiments. Now, we say that estimation
is conservative in a α threshold region when the underestimation is zero. Moreover, the

3. We also measured the false negative rate using the structural Hamming distance as a metric in Figure

19 of the Appendix.

31

Strobl, Spirtes and Visweswaran

greater the over-estimation in a p-value threshold region, the more conservative the esti-
mate. A method should conservatively estimate the FDR but not do so over-conservatively.
As a result, achieving lower under-estimation is more important than achieving lower over-
estimation, and one method outperforms another if the method achieves a lower mean
under-estimation while maintaining a reasonably low mean over-estimation.

Below, we report the relative performance diﬀerences of the six algorithms in recovering
the CPDAG at a liberal α threshold of 0.20, since this threshold consistently provided a nice
tradeoﬀ between p-value bound looseness and low Type II error rates. We have reported
the results using other α thresholds of 0.01, 0.05, 0.10, and 0.15 or 0.50 in Figures 12-15 of
Appendix A.3, with similar relative performance diﬀerences between the algorithms. Figures
16-18 in the Appendix also contain results for skeleton discovery, where we compared the
original skeleton discovery procedure of PC against the same procedure with stabilization.
As expected, the stabilization procedure improved performance. We ﬁnally provide results
with the more commonly used structural Hamming distance in 19 of the Appendix; here,
PC-p achieved superior performance by conservatively estimating the graph.

Note that for the simulations in Sections 6.2 and 6.3, we generated the DAGs using the
TETRAD V package (version 5.2.1) by drawing uniformly over all DAGs with a maximum
in-degree of 2 and a maximum out-degree of 2. We then converted each of the DAGs
to linear non-recursive SEM-IEs by 1) drawing the linear coeﬃcients from independent
standard normal distributions, and 2) setting independent Gaussian distributions over the
error terms with standard deviations also drawn from the standard normal. Each linear
SEM-IE with the error distributions therefore induced a multivariate Gaussian distribution
across the observed variables. We ﬁnally ran all of the six algorithms using Fisher’s z-test
with a liberal α threshold of 0.20 and a maximum conditioning set size of 2.

6.2 Low Dimensional Inference

We generated 30 DAGs by drawing uniformly over all DAGs with 20 vertices. We converted
each of the DAGs to 5 linear non-recursive SEM-IEs. We subsequently created 5 datasets
using each linear SEM-IE with sample sizes of 100, 500, 1000, 5000, and 10000. We therefore
created a total of 30 × 5 × 5 = 750 datasets.

We analyzed the ability of the algorithms in correctly estimating the CPDAG in terms
of the four metrics proposed in Section 6.1 as well as the FDR values. Results as averaged
over DAGs, parameters and sample sizes are summarized in Figure 8. We assessed the
signiﬁcance of all inter-algorithm diﬀerences using paired Wilcoxon signed rank tests. PC-p
obtained lower mean FDR values than PC-p without robust p-values (Figures 8a and 8b; z =
-4.782, p = 1.734E-6), PC-p without stabilization (z = -4.371, p = 1.238E-5), PC-p without
ambiguation (z = -4.782, p = 1.734E-6), PC-p without both stabilization and ambiguation
(z = -4.782, p = 1.734E-6), and PC original (z = -4.433, p = 9.316E-6). Moreover, PC-p
achieved signiﬁcantly lower mean under-control than the competing methods (Figures 8c
and 8d; vs. no robust: z = -4.782, p = 1.734E − 6; vs. no stable: z = -3.898, p = 9.711E-5;
vs. no ambig: z = -4.782, p = 1.734E-6; vs. no stable & no ambig: z = −4.782, p =
1.734E-6; vs. PC original: z = -4.700, p = 2.603E-6); meanwhile, PC-p kept the mean
over-control small at 3.865% (SD: 0.795%).

32

PC with p-values

33

Figure 8: Performances of PC-p, PC-p without robust p-values (no robust), PC-p without
ambiguation (no ambig), PC-p without stabilization (no stable), PC-p without
ambiguation and stabilization (no stable & no ambig), and the original PC algo-
rithm (PC) as assessed by (a,b) the FDR, (c,d) control bias, and (e,f) estimation
bias in units of percent. PC-p achieved signiﬁcantly lower FDR, under-estimation
and under-control than the other ﬁve methods suggesting that robust p-values,
stabilization and ambiguation are all important components of PC-p.

Strobl, Spirtes and Visweswaran

Figure 9: Same setup as Figure 8 except with high dimensional data. PC-p signiﬁcantly
outperformed all other methods except PC-p without stabilization in terms of
under-control and under-estimation.

Results for estimation were similar. PC-p achieved signiﬁcantly lower mean under-
estimation than the competing methods (Figures 8e and 8f; vs. no robust: z = -4.782, p =
1.734E-6; vs. no stable: z = -4.206, p = 2.597E-5; vs. no ambig: z = −4.782, p = 1.734E-6;
vs. no stable & no ambig: z = -4.782, p = 1.734E-6; vs. PC original: z = -4.186, p =
2.843E-5). PC-p also achieved a small degree of mean over-estimation (8.222%, SD: 0.400%).
We conclude that robust p-values, stabilization, and ambiguation all help PC-p achieve the
lowest under-control and under-estimation.

6.3 High Dimensional Inference

We next tested PC-p and the other ﬁve algorithms on high dimensional graph estimation. To
do this, we generated thirty 100, thirty 200 and thirty 300 variable DAGs. We subsequently
converted each of the DAGs to one linear non-recursive SEM-IE. Finally, we generated 1000
samples from each SEM-IE in order to obtain sample size to variable ratios of 10, 5 and
3.333.

Results are summarized using the FDR, control bias, and estimation bias metrics as
averaged over the DAGs and their parameters in Figure 9. PC-p achieved similar results in
low dimensions as it did for high dimensions. Speciﬁcally, PC-p obtained lower mean FDR
values across the same α thresholds than PC-p without robust p-values (Figures 9a and 9b;
z = -4.703, p = 2.563E-6), PC-p without stabilization (z = -2.232, p = 0.026), PC-p without
ambiguation (z = -4.782, p = 1.734E −6), PC-p without both stabilization and ambiguation
(z = -4.782, p = 1.734E-6), and PC original (z = -4.782, p = 1.734E-6). Moreover, PC-p
achieved signiﬁcantly lower mean under-control than four of the ﬁve competing methods

34

PC with p-values

Figure 10: Same setup as Figure 8 except with the CYTO dataset. PC-p signiﬁcantly
outperformed all methods across all metrics except the original PC algorithm in
under-control.

(Figures 9c and 9d; vs. no robust: z = -4.623, p = 3.790E-6; vs. no ambig: z = -4.782, p =
1.734E − 6; vs. no stable & no ambig: z = -4.782, p = 1.734E-6; vs. PC original: z =
-4.782, p = 1.734E-6). PC-p did not outperform PC-p without stabilization at four of the
ﬁve threshold α thresholds tested (0.05 : z = -1.121, p = 0.262; 0.10 : z = 0.504, p = 0.614;
0.20 : z = 0.985, p = 0.324; 0.50 : z = 0.760, p = 0.447); however, PC-p did outperform
PC-p without stabilization at an α threshold of 0.01 (z = -3.692, p = 2.225E-4). Meanwhile,
PC-p kept the mean over-control small at 4.507% (SD: 0.240%).

Results for estimation were again similar. PC-p achieved signiﬁcantly lower mean under-
estimation than the competing methods except PC-p without stabilization (Figures 9e
and 9f; vs. all methods except no stable: z = -4.782, p = 1.734E-6). PC-p did not
outperform PC-p without stabilization at four of the ﬁve threshold α thresholds tested
(0.05 : z = -1.820, p = 0.069; 0.10 : z = 0.175, p = 0.861; 0.20 : z = 0.625, p = 0.532;
0.50 : z = 0.608, p = 0.543); however, PC-p did outperform PC-p without stabilization at
an α threshold of 0.01 (z = -2.293, p = 0.028). PC-p also achieved a small degree of mean
over-control (2.129%, SD: 0.084%).

We conclude that the results for control and estimation bias for high dimensional graph
estimation are similar to the low dimensional case. However, stabilization only increased
performance at lower α thresholds in the high dimensional scenario; we may nonetheless
view this as a desirable property, since a lower α threshold helps the algorithm complete
more quickly.

35

Strobl, Spirtes and Visweswaran

6.4 Real Data: CYTO

We evaluated the six algorithms on the CYTO dataset which contains single cell recordings
of the abundance of 11 phosphoproteins and phospholipids in human primary naive CD4+
T cells using ﬂow cytometry (Sachs et al., 2005). The variables in the dataset and their
causal relationships can be represented as a DAG, where vertices are proteins or lipids
and edges are phosphorylation interactions between the proteins and lipids. We used the
general perturbation samples (i.e., CD3-CD28 and CD3-CD28-ICAM2) as our observational
data; these perturbations are required to activate the phosphorylation pathways. Note
that algorithms typically cannot accurately infer the gold standard solution set using the
observational data alone, as noted by the original authors4. As a result, we created a
silver standard DAG by running LiNGAM as implemented in TETRAD V using default
parameters on the full dataset of 1,755 samples; recall that LiNGAM is a method within a
diﬀerent class of causal discovery algorithms based on functional causal models. We then
ran the six algorithms described in Section 6.1 on 1000 bootstrapped datasets of sample
size 100 using Spearman’s rho to handle the class of non-paranormal distributions.

We have summarized the results in Figure 10. PC-p obtained lower mean FDR across
the same α thresholds than PC-p without robust p-values (z = -12.774, p = 2.282E-37),
PC-p without stabilization (z = −8.402, p = 4.378E-17), PC-p without ambiguation (z =
-24.598, p = 1.343E-133), PC-p without both stabilization and ambiguation (z = -23.714, p =
2.616E-124), and original PC (z = -4.924, p = 8.469E-7). Moreover, PC-p achieved signif-
icantly lower mean under-control than four of the ﬁve competing methods (vs. no ro-
bust: z = -12.601, p = 2.081E-36; vs no stable: z = -4.310, p = 1.631E-5; vs. no ambig:
z = -24.339, p = 7.559E-131; vs. no stable & no ambig: z = -22.893, p = 5.515E-116). PC-p
did not outperform the original PC algorithm in mean under-control (z = 0.827, p = 0.408);
however, PC-p did outperform the original PC algorithm in mean under-estimation (z =
−2.662, p = 0.008). Meanwhile, PC-p kept the mean over-control small at 4.232% (SD:
1.635%). PC-p also outperformed the other four methods in mean under-estimation (vs. no
robust: z = -12.684, p = 7.289E-37; vs. no stable: z = -4.893, p = 9.917E-7; vs. no ambig:
z = -24.411, p = 1.322E-131; vs. no stable & no ambig: z = -23.301, p = 4.333E-120)
while maintaining low mean over-estimation at 1.200% (SD: 0.559%). We conclude that
PC-p outperforms the other methods similar to the results with synthetic data. PC-p only
outperformed PC in 2 of the 3 metrics, however, probably because the LiNGAM solution
is only an estimate of the ground truth.

6.5 Real Data: GDP Dynamics

One way of approximating the underlying DAG involves learning the graph with a large
number of samples. Another way uses time series data, where we know a priori that we
must have contemporaneous causal relations or causal relations directed forward in time.
In this experiment, we strip the time information from the six algorithms, and then identify
the false discoveries when algorithms mistakenly detect a causal relation directed backwards
in time. We used a time series dataset downloaded from the Economic Research Service of
the United States Department of Agriculture containing ten economic indicators per year

4. In general, we do not have gold standard causal graphs for real data, so we must approximate the solution

in some manner.

36

PC with p-values

Figure 11: Similar to Figure 8 except with the GDP dataset as well as over-control and
over-estimation bias values instead of under. PC-p did not achieve lower under-
control and under-estimation than PC, but it did achieve signiﬁcantly lower
mean FDR and over-estimation that PC.

related to GDP among 192 countries5. We speciﬁcally evaluated the algorithms on their
ability to discover causal relations among the indicators within and between 1987, 1988
and 1989, where we treated each country as an i.i.d. sample and used 100 bootstrapped
datasets.

We have summarized the results in Figure 11. PC-p again obtained lower mean FDR val-
ues across the α thresholds than PC-p without robust p-values (z = -4.623, p = 3.784E-6),
PC-p without ambiguation (z = -8.054, p = 4.128E-16), PC-p without both stabilization
and ambiguation (z = -8.135, p = 4.128E-16), and original PC (z = -6.624, p = 3.500E-11).
However, PC-p did not obtain signiﬁcantly lower mean FDR values than PC-p without
stabilization (signed-rank = 70, p = 0.600). Next, PC-p achieved signiﬁcantly lower mean
under-control than three of the ﬁve competing methods including PC-p without robust p-
values (z = -4.374, p = 1.218E-5), PC-p without ambiguation (z = -7.867, p = 3.647E-15),
and PC-p without both stabilization and ambiguation (z = -7.819, p = 5.306E-15). PC-p
did not outperform PC-p without stabilization (signed-ranked = 3, p = 1) as well as the
original PC algorithm (signed-ranked = 7, p = 0.625) in mean under-control; nevertheless,
PC-p did outperform the former in over-control (signed-ranked = 3, p = 0.020) while keep-
ing its own mean over-control low at 4.920% (SD: 0.483%). PC-p also outperformed the
same three methods in mean under-estimation (vs. no robust: z = -4.372, p = 1.229E-5; vs.
no ambig: z = -7.818, p = 5.363E-15; vs. no stable & no ambig: z = -7.770, p = 7.850E-15)
while maintaining low mean over-estimation at 2.032% (SD: 0.281%). PC-p again did
not outperform PC-p without stabilization (signed-rank = 2, p = 0.750) and original

5. Web link: http://www.ers.usda.gov/data/macroeconomics/Data/HistoricalRealGDPValues.xls

37

Strobl, Spirtes and Visweswaran

PC (signed-rank = 7, p = 0.625) in under-estimation, but it outperformed both in over-
estimation (no stable: z = -7.386, p = 1.519E-13; PC original: z = -8.682, p = 3.897E-18).
We conclude that PC-p outperforms most methods in either the under or over-metrics. The
results however are not as clean as the results with the synthetic data because we only have
access to a portion of the ground truth.

7. Conclusion

We developed a new algorithm called PC-p which outputs a causal DAG with p-value bounds
associated with each edge. One can then use the bounds with the BY procedure to achieve
almost strong control and estimation of the FDR. The PC-p algorithm speciﬁcally integrates
the skeleton discovery procedure of PC-stable, edge orientation with ambiguation, and
robust hypothesis tests in order to accurately estimate p-values bounds while maintaining
computational eﬃciency.

The PC-p algorithm represents the ﬁrst global constraint-based method which can re-
cover p-value estimates for every edge of a CPDAG. In our opinion, the algorithm is a
signiﬁcant advancement over previous methods which can only achieve strong control of the
FDR under special conditions. Moreover, PC-p lays a foundation for developing similar
methods which can also recover edge-speciﬁc p-values and achieve strong control of the
FDR for graphs recovered by algorithms such as FCI and CCD. In particular, we suspect
that a combination of the max and union bounds will also be suﬃcient for deriving upper
bounds of the edge-speciﬁc p-values for more sophisticated constraint-based methods. The
proposed approach may therefore represent one the earliest forms of a “causal p-value.”

Now readers may wonder whether PC-p can also use the p-values to control the family-
wise error rate (FWER). The answer is yes, and we recommend using the Benjamini-Holm
step-down procedure as opposed to Hochberg’s step-up procedure to control the FWER
(Hochberg, 1988), since the latter assumes positive dependency among the test statistics.
However, application of an FWER controlling procedure to constraint-based causal discov-
ery requires additional justiﬁcation, since most investigators do not use constraint-based
methods to deﬁnitively conclude causal relationships but rather to screen for potential
causal variables. With the screening goal in mind, the FWER may be too conservative in
practice, since it controls the rate of making a single Type I error across all of the hypothesis
tests as opposed to controlling the proportion of Type I errors.

In summary, we introduced an algorithm called PC-p which outputs a causal DAG along
with edge-speciﬁc p-value bounds. One can then use the BY procedure with the bounds to
achieve almost strong control or estimation of the FDR and therefore assess the algorithm’s
conﬁdence in each edge in a principled manner. We ultimately hope that this work will
encourage more applications of constraint-based causal discovery to important problems in
science.

Acknowledgments

Research reported in this publication was supported by grant U54HG008540 awarded by
the National Human Genome Research Institute through funds provided by the trans-

38

PC with p-values

NIH Big Data to Knowledge initiative. The research was also supported by the National
Library of Medicine of the National Institutes of Health under award numbers T15LM007059
and R01LM012095. The content is solely the responsibility of the authors and does not
necessarily represent the oﬃcial views of the National Institutes of Health.

39

Strobl, Spirtes and Visweswaran

Appendix A. Appendix

A.1 PC Algorithm Pseudocode

We provide pseudocode for the original PC algorithm. We summarize skeleton discovery in
Algorithm 5, unshielded v-structure discovery in Algorithm 6, and orientation rule applica-
tion in Algorithm 7.

Data: X n, α
Result: (cid:98)G, S

l = l + 1
repeat

1 Form a completely connected undirected graph (cid:98)G on the vertex set X
2 l = −1
3 repeat
4

Select a new ordered pair of variables (A, B) that are adjacent in (cid:98)G s.t.
|N (A) \ B| ≥ l
repeat

Choose a new set S ⊆ {N (A) \ B} with |S| = l using order(X)
p ← p-value from testA⊥⊥B|S
if p > α then

Delete A − B from (cid:98)G
Insert S into SA,B and SB,A

end

until A − B is deleted from (cid:98)G or all S ⊆ {N (A) \ B} with |S| = l have been
considered ;

until all ordered pairs of adjacent variables (A, B) in (cid:98)G with | (cid:99)N (A) \ B| ≥ l have
been considered ;

16 until all ordered pairs of adjacent variables (A, B) in (cid:98)G satisfy | (cid:99)N (A) \ B| ≤ l;

Algorithm 5: Skeleton Discovery

5

6

7

8

9

10

11

12

13

14

15

Data: (cid:98)G, S
Result: (cid:98)G

end

4
5 end

1 for all ordered pairs of non-adjacent variables (A, B) with common neighbor C do
2

if C (cid:54)∈ any set in Si,j then

3

Replace A − C − B with A → C ← B

Algorithm 6: Unshielded V-structures

40

PC with p-values

Data: (cid:98)G
Result: (cid:98)G

1 repeat
2

3

4

5

6

7

if A − B and ∃C s.t. C → A, and C and B are non-adjacent then

Replace A − B with A → B

else if A − B and ∃C s.t. A → C → B then

Replace A − B with A → B

else if A − B and ∃B, D s.t. A − C → B, A − D → B, and C and D are
non-adjacent then

Replace A − B with A → B

end

8
9 until there are no more edges to orient;

Algorithm 7: Orientation Rules

A.2 Hypothesis Tests with Less Robust Bounds

We claimed to propose edge-speciﬁc hypothesis tests whose bounds are robust to Type II
errors in Section 4.4. We now explain our rationale.

Consider the following modiﬁcation to (10), where we have replaced the null with a

series of logical conjunctions:

H0 :

oracle i outputs ¬Pi,

H1 :

oracle i outputs Pi.

m
(cid:94)

i=1
m
(cid:94)

i=1

m
(cid:94)

i=1

We can bound the Type I error rate of the above hypothesis test as follows:

Pr(Type I error) = Pr(

test i outputs Pi|H0)

Pr(test i outputs Pi|H0)

Pr(test i outputs Pi|oracle i outputs ¬Pi )

≤ min

i=1,...,m

= min

i=1,...,m

= min

i=1,...,m

gi,

We can use the above bound with the following variant of (26) for unshielded v-

structures:

The above hypothesis test follows from the following natural hypothesis test:

H0 : ¬(A − C) ∧ ¬(B − C) ∧ ¬γAB|C,
H1 : (A − C) ∧ (B − C) ∧ γAB|C,

H0 : All edges between A, C, B are absent,
H1 : (A − C) ∧ (B − C) ∧ γAB|C,

41

(38)

(39)

(40)

(41)

Strobl, Spirtes and Visweswaran

since the null of (41) implies the null in (40). From (39), the Type I error rate of (40) is
bounded by min{pA−C, pB−C, pγAB|C }. This is a less robust bound than (26) in terms of
the Type II error rate, since failing to control one p-value can cause an algorithm to under-
estimate the bound. For example, suppose the underlying truth corresponds to pA−C = 0.01,
pB−C = 0.03, pγAB|C = 0.02. Thus, the Type I error rate of (41) is truly bounded by 0.01.
However, suppose a Type II error causes PC-p to skip CI tests and therefore compute
pA−C = 0.01, pB−C = 0.03, pγAB|C = 0.003, where the third term is under-estimated. Then,
PC-p will under-estimate the bound at 0.003 instead of the true 0.01.

Note that generalizing (41) to account for multiple possible ways of orienting a v-

structure does not robustify the bound either, since we have:

H0 : All edges between A, C, B1 are absent, and all edges between

A, C, B2 are absent,

(cid:16)(cid:104)

H1 : (A − C) ∧

(B1 − C) ∧ γAB1|C] ∨ [(B2 − C) ∧ γAB2|C

(cid:105)(cid:17)

.

whose Type I error rate is bounded by min

(cid:111)
(cid:110)
.
pA−C, min{pB1−C, pγAB1|C }+min{pB2−C, pγAB2|C }

More broadly, we can consider the following hypothesis test:

H0 :

oracle i, j outputs ¬Pi,j

H1 :

oracle i, j outputs Pi,j

(cid:17)

,

(cid:17)

.

m
(cid:95)

(cid:16) ni(cid:94)

i=1
m
(cid:94)

j=1

(cid:16) ni(cid:94)

i=1

j=1

We bound its Type I error rate as follows:
ni(cid:94)

m
(cid:95)

Pr(Type I error) = Pr(

test i, j outputs Pi,j|H0 )

i=1

j=1

Pr(test i, j outputs Pi,j|H0)

= max

Pr(test i, j outputs Pi,j| oracle i, j outputs ¬Pi,j)

≤ max

i=1,...,m

min
j=1,...,ni

min
j

min
j

i

i

= max

gi,j,

The above bound is less robust to Type II errors than (11), since under-estimating one term
in each group i composed of ni terms can cause PC-p to also under-estimate (44).

A.3 Other Experimental Results

We have summarized the results for the low dimensional, high dimensional and real datasets
across multiple α thresholds in Figures 12, 13, 14 and 15 respectively. Relative diﬀerences in
performance largely remained consistent across the thresholds, since PC-p usually achieved
the lowest mean FDR, under-control and under-estimation values with minimal increases
in mean over-control and over-estimation.

We have also summarized the results for adjacency discovery in Figures 16, 17, and 18,
where we tested whether the skeleton discovery procedure of PC with stabilization could

42

(42)

(43)

(44)

PC with p-values

improve the estimation of the p-value bounds relative to the procedure without stabilization.
Results show that stabilization improves performance across the three metrics particularly
with the low α threshold values of 0.01 and 0.05. Note that we cannot compute the same
ﬁgures for the GDP dataset, since we can only evaluate relative performance levels based
on edge direction in this case.

We have ﬁnally summarized the results using the structural Hamming distance in Figure
19. Notice that ambiguation helps PC-p achieve signiﬁcantly lower Hamming distances
across multiple α thresholds by forcing the algorithm to conservatively orient the edges.
Again, we cannot compute the structural Hamming distances for the GDP dataset for the
aforementioned reason.

References

A. P. Armen and I. Tsamardinos. A uniﬁed approach to estimation and control of the
false discovery rate in bayesian network skeleton identiﬁcation.
In In Proceedings of
the European Symposium on Artiﬁcial Neural Networks, Computational Intelligence and
Machine Learning (ESANN, 2011.

A. P. Armen and I. Tsamardinos. Estimation and control of the false discovery rate of
bayesian network skeleton identiﬁcation. Technical Report TR-441, University of Crete,
2014.

Y. Benjamini and D. Yekutieli. Investigating the importance of self-theories of intelligence
and musicality for students’ academic and musical achievement. Annals of Statistics, 29
(4):1165–1188, 2001.

H. Chong, M. Zey, and D. A. Bessler. On corporate structure, strategy, and performance:
a study with directed acyclic graphs and pc algorithm. Managerial and Decision Infor-
matics, 31:47–62, 2009.

D. Colombo and M. H. Maathuis. Order-independent constraint-based causal structure
learning. J. Mach. Learn. Res., 15(1):3741–3782, January 2014. ISSN 1532-4435. URL
http://dl.acm.org/citation.cfm?id=2627435.2750365.

G. F. Cooper and C. Yoo. Causal discovery from a mixture of experimental and observa-
tional data. In Proceedings of the Fifteenth Conference on Uncertainty in Artiﬁcial Intel-
ligence, UAI’99, pages 116–125, San Francisco, CA, USA, 1999. Morgan Kaufmann Pub-
lishers Inc. ISBN 1-55860-614-9. URL http://dl.acm.org/citation.cfm?id=2073796.
2073810.

N. Friedman, M. Goldszmidt, and A. Wyner. Data analysis with bayesian networks: A
bootstrap approach. In Proceedings of the Fifteenth Conference on Uncertainty in Arti-
ﬁcial Intelligence, UAI’99, pages 196–205, San Francisco, CA, USA, 1999. Morgan Kauf-
mann Publishers Inc. ISBN 1-55860-614-9. URL http://dl.acm.org/citation.cfm?
id=2073796.2073819.

43

Strobl, Spirtes and Visweswaran

Figure 12: Mean FDR, control bias, and estimation bias values across multiple α thresholds

for the low dimensional datasets.

44

PC with p-values

Figure 13: Same as Figure 12 except with high dimensional datasets.

45

Strobl, Spirtes and Visweswaran

Figure 14: Same as Figure 12 except with bootstrapped CYTO datasets.

46

PC with p-values

Figure 15: Same as Figure 12 except with bootstrapped GDP datasets.

47

Strobl, Spirtes and Visweswaran

Figure 16: Mean FDR, control bias, and estimation bias values across multiple α thresholds

for the low dimensional datasets.

48

PC with p-values

Figure 17: Same as Figure 16 except with high dimensional datasets.

49

Strobl, Spirtes and Visweswaran

Figure 18: Same as Figure 16 except with bootstrapped CYTO datasets.

50

PC with p-values

Figure 19: Structural Hamming distances for the a) low dimensional, b) high dimensional
and c) CYTO datasets. The three sub-ﬁgures associate 5 bars with each algo-
rithm; these bars correspond to α thresholds of 0.01, 0.05, 0.1, 0.20 and 0.50 for
the low dimensional and CYTO datasets, and α thresholds of 0.01, 0.05, 0.1,
0.15 and 0.20 for the high dimensional datasets. Error bars represent standard
errors for a) and standard deviations otherwise.

M. J. Ha, V. Baladandayuthapani, and K. A. Do. Prognostic gene signature identiﬁcation
using causal structure learning: applications in kidney cancer. Cancer Inform, 14(Suppl
1):23–35, 2015.

N. Harris and M. Drton. Pc algorithm for nonparanormal graphical models. J. Mach.
ISSN 1532-4435. URL http://dl.acm.

Learn. Res., 14(1):3365–3383, January 2013.
org/citation.cfm?id=2567709.2567770.

Y. Hochberg. A sharper Bonferroni procedure for multiple tests of signiﬁcance. Biometrika,
75(4):800–802, December 1988. ISSN 1464-3510. doi: 10.1093/biomet/75.4.800. URL
http://dx.doi.org/10.1093/biomet/75.4.800.

S. P. Iyer, I. Shafran, D. Grayson, K. Gates, J. T. Nigg, and D. A. Fair. Inferring func-
tional connectivity in MRI using Bayesian network structure learning with a modiﬁed PC
algorithm. Neuroimage, 75:165–175, Jul 2013.

A. A. Joshi, S. H. Joshi, R. M. Leahy, D. W. Shattuck, I. Dinov, and A. W. Toga. Bayesian
approach for network modeling of brain structural features, volume 7626. 2010. ISBN
9780819480279. doi: 10.1117/12.844548.

T. D. Le, L. Liu, A. Tsykin, G. J. Goodall, B. Liu, B. Y. Sun, and J. Li.

Inferring
microRNA-mRNA causal regulatory relationships from expression data. Bioinformatics,
29(6):765–771, Mar 2013.

51

Strobl, Spirtes and Visweswaran

J. Li and Z. J. Wang. Controlling the false discovery rate of the association/causality
structure learned with the pc algorithm. J. Mach. Learn. Res., 10:475–514, June 2009.
ISSN 1532-4435. URL http://dl.acm.org/citation.cfm?id=1577069.1577086.

J. Li, Z. Wang, and M. J. McKeown. Learning brain connectivity with the false-discovery-
rate-controlled PC-algorithm. Conf Proc IEEE Eng Med Biol Soc, 2008:4617–4620, 2008.

J. Listgarten and D. Heckerman. Determining the number of non-spurious arcs in a learned
dag model: Investigation of a bayesian and a frequentist approach. In Ronald Parr and
Linda C. van der Gaag, editors, UAI, pages 251–258. AUAI Press, 2007. ISBN 0-9749039-
3-0. URL http://dblp.uni-trier.de/db/conf/uai/uai2007.html#ListgartenH07.

C. Meek. Strong completeness and faithfulness in bayesian networks. In Proceedings of the
Eleventh Conference on Uncertainty in Artiﬁcial Intelligence, UAI’95, pages 411–418,
San Francisco, CA, USA, 1995. Morgan Kaufmann Publishers Inc. ISBN 1-55860-385-9.
URL http://dl.acm.org/citation.cfm?id=2074158.2074205.

D. Mullensiefen, P. Harrison, F. Caprini, and A. Fancourt. Investigating the importance
of self-theories of intelligence and musicality for students’ academic and musical achieve-
ment. Front Psychol, 6:1702, 2015.

T. Richardson. A discovery algorithm for directed cyclic graphs.

In Proceedings of the
Twelfth International Conference on Uncertainty in Artiﬁcial Intelligence, UAI’96, pages
454–461, San Francisco, CA, USA, 1996. Morgan Kaufmann Publishers Inc.
ISBN 1-
55860-412-X. URL http://dl.acm.org/citation.cfm?id=2074284.2074338.

K. Sachs, O. Perez, D. Pe’er, D. A. Lauﬀenburger, and G. P. Nolan. Causal protein-signaling
networks derived from multiparameter single-cell data. Science, 308(5721):523–529, Apr
2005.

P. Spirtes, C. Meek, and T. Richardson. Causal inference in the presence of latent vari-
ables and selection bias. In Proceedings of the Eleventh Conference on Uncertainty in
Artiﬁcial Intelligence, UAI’95, pages 499–506, San Francisco, CA, USA, 1995. Morgan
Kaufmann Publishers Inc. ISBN 1-55860-385-9. URL http://dl.acm.org/citation.
cfm?id=2074158.2074215.

P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. MIT press,

2nd edition, 2000.

J. Sun, X. Hu, X. Huang, Y. Liu, K. Li, X. Li, J. Han, L. Guo, T. Liu, and J. Zhang.
Inferring consistent functional interaction patterns from natural stimulus FMRI data.
Neuroimage, 61(4):987–999, Jul 2012.

R. Teramoto, C. Saito, and S. Funahashi. Estimating causal eﬀects with a non-paranormal
method for the design of eﬃcient intervention experiments. BMC Bioinformatics, 15:228,
2014.

I. Tsamardinos and L. E. Brown. Bounding the false discovery rate in local bayesian network
learning. In Proceedings of the Twenty-Third AAAI Conference on Artiﬁcial Intelligence,

52

PC with p-values

AAAI 2008, Chicago, Illinois, USA, July 13-17, 2008, pages 1100–1105, 2008. URL
http://www.aaai.org/Library/AAAI/2008/aaai08-174.php.

X. Wu and Y. Ye. Exploring gene causal interactions using an enhanced constraint-based
method. Pattern Recogn., 39(12):2439–2449, December 2006.
ISSN 0031-3203. doi:
10.1016/j.patcog.2006.05.003. URL http://dx.doi.org/10.1016/j.patcog.2006.05.
003.

53

7
1
0
2
 
y
a
M
 
0
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
5
7
9
3
0
.
7
0
6
1
:
v
i
X
r
a

Estimating and Controlling the False Discovery Rate of the
PC Algorithm Using Edge-Speciﬁc P-Values

Eric V. Strobl
Department of Biomedical Informatics
University of Pittsburgh
Pittsburgh, PA 15206, USA

Peter L. Spirtes
Department of Philosophy
Carnegie Mellon University
Pittsburgh, PA 15213, USA

Shyam Visweswaran
Department of Biomedical Informatics
University of Pittsburgh
Pittsburgh, PA 15206, USA

Editor: TBA

evs17@pitt.edu

ps7z@andrew.cmu.edu

shv3@pitt.edu

Abstract

The PC algorithm allows investigators to estimate a complete partially directed acyclic
graph (CPDAG) from a ﬁnite dataset, but few groups have investigated strategies for
estimating and controlling the false discovery rate (FDR) of the edges in the CPDAG. In
this paper, we introduce PC with p-values (PC-p), a fast algorithm which robustly computes
edge-speciﬁc p-values and then estimates and controls the FDR across the edges. PC-p
speciﬁcally uses the p-values returned by many conditional independence (CI) tests to upper
bound the p-values of more complex edge-speciﬁc hypothesis tests. The algorithm then
estimates and controls the FDR using the bounded p-values and the Benjamini-Yekutieli
FDR procedure. Modiﬁcations to the original PC algorithm also help PC-p accurately
compute the upper bounds despite non-zero Type II error rates. Experiments show that
PC-p yields more accurate FDR estimation and control across the edges in a variety of
CPDAGs compared to alternative methods1.
Keywords: PC Algorithm, Causal Inference, False Discovery Rate, Bayesian Network,
Directed Acyclic Graph

1. Introduction

Discovering causal relationships is often much more important than discovering associational
relationships in the sciences. As a result, the research community has been conducting
extensive investigations into causal inference with the hope of developing practically useful
algorithms to speed-up the scientiﬁc process. This research has resulted in a wide range of
high performing algorithms over the years such as PC (Spirtes et al., 2000), FCI (Spirtes
et al., 1995, 2000), and CCD (Richardson, 1996)

1. MATLAB implementation: https://github.com/ericstrobl/PCp/

c(cid:13)2016 Eric V. Strobl and Shyam Visweswaran.

Strobl, Spirtes and Visweswaran

The PC algorithm is currently one of the most popular methods for inferring causation
from observational data. Given an observational dataset, the algorithm outputs a complete
partially directed acyclic graph (CPDAG) which has helped some investigators elucidate
important causal relationships in several domains. For example, the PC algorithm has been
used to discover new causal relationships between genes and brain regions in biology (Wu
and Ye, 2006; Li et al., 2008; Joshi et al., 2010; Sun et al., 2012; Harris and Drton, 2013;
Iyer et al., 2013; Le et al., 2013; Teramoto et al., 2014; Ha et al., 2015). The algorithm has
also been used to discover causal relations between corporate structures and strategies in
economics (Chong et al., 2009) as well as academic and musical achievements in psychology
(Mullensiefen et al., 2015).

The increased use of PC in recent years has nonetheless led to growing concern about
algorithm’s conﬁdence level in each edge of the CPDAG. For example, PC may have more
conﬁdence in the edge A − B but have less conﬁdence in the edge B → C in the subgraph
A−B → C of the CPDAG. Currently, PC alone does not output any edge-speciﬁc measure of
conﬁdence, even though scientists often must report measures of conﬁdence such as p-values
or conﬁdence intervals in their scientiﬁc articles. This incongruency has resulted in the
relatively slow adoption or even avoidance of the PC algorithm in the sciences, despite the
algorithm’s impressive capabilities in causal inference. Clearly then rectifying the problem
by developing an edge-speciﬁc measure of conﬁdence will increase the adoption of PC as
well as hopefully ease the transition of ever-more complex causal inference algorithms into
the scientiﬁc community.

We can of course consider multiple diﬀerent ways of representing the conﬁdence level
in each edge of the CPDAG. However, we choose to pay special attention to the p-value,
since it is by far the most popular notion of conﬁdence in the sciences. Indeed, nearly all
scientists report p-values in modern scientiﬁc reports because they rely on p-values to help
justify their hypotheses. We therefore would ideally like to assign a p-value to each edge in
the CPDAG as in Figure 1a in order to best integrate the algorithm within a well-known
framework. In this paper, we propose such a “causal p-value” in detail.

We however also believe that assigning p-values to each edge is not enough to ease the
transition of PC into mainstream science, since the CPDAG actually contains many edges
and therefore also represents a complicated multiple hypothesis testing problem. Fortu-
nately, the problem of multiple hypothesis testing has a long history, as scientists have
often required the results of multiple hypothesis tests in order to answer complex scien-
tiﬁc questions. Currently, a standard approach to tackling the multiple hypothesis testing
problem involves controlling the proportion of false positives among the rejected null hy-
potheses, or the false discovery rate (FDR), by using an FDR controlling procedure that
takes a desired FDR level q and a set of p-values as input. The procedure then outputs
a corresponding signiﬁcance level α∗ for the set of p-values. An investigator subsequently
rejects the null hypotheses for those tests with p-values that fall below α∗ in order to en-
sure that the expected FDR does not exceed q. For example, consider the set of p-values
{0.02, 0.01, 0.03} and suppose that the FDR controlling procedure with q = 0.1 outputs
α∗ = 0.019. Then, rejecting the null hypotheses of the ﬁrst and third hypothesis tests
guarantees that the expected FDR does not exceed 10%. Several other FDR controlling
strategies also exist, but the ease of use, speed and accuracy of the above method have
made it the most widely adopted strategy in the last two decades.

2

PC with p-values

We would therefore like to control the FDR in the edges of the CPDAG using an FDR
controlling procedure like in Figure 1b. As a ﬁrst idea, one may wonder whether an inves-
tigator can control the FDR in the CPDAG by simply feeding in all of the CI test p-values
computed by PC into an FDR controlling procedure. Unfortunately, this approach fails for
at least two reasons. First, it is unclear how to use the p-values which exceed the α∗ cut-oﬀ
to reject edges in the CPDAG, since one would ﬁrst need to elucidate the correspondence
between the p-values and edges. Second, even if one could solve this problem, the strat-
egy may only loosely bound the expected FDR. An accurate FDR controlling procedure
should instead take into account the speciﬁc computations executed by PC in order to iden-
tify a sharp bound. The p-value based approach therefore necessitates a more ﬁne-grained
strategy which has thus far remained undiscovered.

Several groups have nonetheless attempted to control the FDR in the CPDAG by avoid-
ing the complicated nature of the above problem with a diﬀerent, data re-sampling approach.
For example, Friedman and colleagues proposed to estimate the FDR by using the para-
metric bootstrap (Friedman et al., 1999). This procedure involves ﬁrst learning a causal
graph with the PC algorithm. The procedure then generates data from the causal graph
and re-applies the PC algorithm multiple times on each generated dataset to estimate the
FDR using the learnt causal graphs. An investigator can subsequently control the FDR
by repeating the above process with diﬀerent α values until he or she reaches the desired
FDR level q. However, notice that the method requires multiples calls to PC and can
therefore require too much time with high dimensional data. The procedure also requires
parametric knowledge about the underlying distribution which limits the applicability of
the method to simple cases. Fortuitously, two groups later proposed a permutation-based
method which drops the parametric assumption (Listgarten and Heckerman, 2007; Armen
and Tsamardinos, 2014). The permutation method nevertheless also requires multiple calls

(a)

A

(b)

A

.

.

B

5
2
0.0

C

0
.
1
5
0

B

5
2
0.0

0

0

.

.

0

0

0

0

1

1

E

0.001

D

E

0.001

D

C

α∗ = 0.031

Figure 1: We seek to associate edge-speciﬁc p-values to the output of the PC algorithm
such as in (a). The PC algorithm currently does not associate such p-values with
its output. We would also like to control the FDR of the edges. In (b), we set
the FDR to 0.1 and obtained a α∗ cutoﬀ of 0.031 for the output in (a), so we
eliminated the edge between B and D because its p-value exceeds α∗.

3

Strobl, Spirtes and Visweswaran

to an algorithm and in fact only applies to the parts of PC which can be decomposed
into independent searches for the parents of each vertex; this has thus far limited the ap-
plicability of the method to adjacency discovery with local to global discovery algorithms
(e.g., MMHC) and incomplete edge orientation. We conclude that both the bootstrap and
permutation approaches to FDR estimation and control are either incomplete or too time
consuming.

Another class of methods fortunately attempts to control the FDR without resampling
procedures by instead using a standard FDR controlling procedure with bounded p-values.
For instance, one method proposed in (Tsamardinos and Brown, 2008) and then reﬁned
in (Armen and Tsamardinos, 2011, 2014) assigns a p-value to each adjacency by taking
the maximum over all of the signiﬁcant p-values from the associated CI tests executed
by PC. The method then controls the FDR in the estimated adjacencies by applying an
FDR controlling procedure, such as the one proposed by Benjamini and Yekutieli (BY)
(Benjamini and Yekutieli, 2001), on the edge-speciﬁc p-values. Under faithfulness and a
zero Type II error rate, the method controls the FDR across the estimated adjacencies,
or the estimated skeleton (Armen and Tsamardinos, 2014). This two stage method also
performs comparably with the one stage method proposed in (Li et al., 2008; Li and Wang,
2009), which controls the FDR during, as opposed to after, the execution of the skeleton
discovery phase of the PC algorithm. Of course, the Type II error rate never reaches
zero in practice but researchers have also investigated a strategy for reducing the realized
Type II error rate by introducing a heuristic reliability criterion for CI tests when dealing
with discrete data (Armen and Tsamardinos, 2014). Experiments have shown that these
methods ﬁnish in a relatively short amount of time and perform well in practice. However,
the methods are also incomplete because they only apply to the skeleton discovery phase of
PC.

In this report, we build on the previous outstanding work for deriving p-values for
adjacencies by contributing a sound, complete and fast algorithm called PC with p-values
(PC-p) which appropriately combines the p-values of PC’s CI tests and then uses the BY
FDR controlling procedure to accurately control the FDR in a CPDAG. The method relies
on two upper bounds of the p-value that relate to logical conjunctions and disjunctions as
described in Section 3. These upper bounds allow us to formulate several hypothesis tests
for recovering the skeleton, discovering unshielded v-structures, and orienting additional
edges as presented in Section 4. Accurately estimating the p-values of the hypothesis tests
nonetheless requires a modiﬁed version of PC called PC-p which we propose in Section
5. Finally, we provide experimental results in Section 6 which show that PC-p’s p-value
estimates yield accurate estimates of the FDR with the BY procedure and improve upon
alternative methods.

2. Preliminaries

2.1 Causal graphs

A causal graph consists of vertices representing variables and edges representing causal
relationships between any two variables. In this paper, we will use the terms “vertices” and
“variables” interchangeably. Directed graphs are graphs where two distinct vertices can be
connected by edges “→” and “←.” We only consider simple graphs in this paper, or graphs

4

PC with p-values

with no edges originating from and connecting to the same vertex. Directed acyclic graphs
(DAGs) are directed graphs without directed cycles. We say that X and Y are adjacent
if they are connected by an edge independent of the edge’s direction. A path p from X
to Y is a set of consecutive edges (also independent of their direction) from X to Y such
that no vertex is visited more than once. Given a path between two vertices X and Y
with a middle vertex Z, the path is a chain if X → Y → Z, a fork if X ← Y → Z, and
a v-structure if X → Y ← Z. We refer to Y as a collider, if it is the middle vertex in a
v-structure. A v-structure is called an unshielded v-structure if X → Y ← Z, but X and Z
are non-adjacent. A directed path from X to Y is a set of consecutive edges with direction.
We say that X is an ancestor of Y (and Y is a descendant of X), if there exists a directed
path from X to Y .

If G is a directed graph in which X, Y and Z are disjoint sets of vertices, then X
and Y are d-connected by Z in G if and only if there exists an undirected path p between
some vertex in X and some vertex in Y such that, for every collider C on p, either C or a
descendant of C is in Z, and no non-collider on p is in Z. On the other hand, X and Y
are d-separated by Z in G if and only if they are not d-connected by Z in G. Next, the
joint probability distribution P over variables X satisﬁes the global directed Markov property
for a directed graph G if and only if, for any three disjoint subsets of variables A, B and
C from X, if A and B are d-separated given C in G, then A and B are conditionally
independent given C in P. We refer to the converse of the global directed Markov property
as d-separation faithfulness; that is, if A and B are conditionally independent given C in
P, then A and B are d-separated given C in G.

A Markov equivalence class of DAGs refers to a set of DAGs which entail the same con-
ditional independencies. A complete partially directed acyclic graph (CPDAG) is a partially
directed acyclic graph with the following properties: (1) each directed edge exists in every
DAG in the Markov equivalence class, and (2) there exists a DAG with X → Y and a DAG
with X ← Y in the Markov equivalence class for every undirected edge X − Y . A CPDAG
GC represents a DAG G, if G belongs to the Markov equivalence class described by GC.
We will occasionally use the meta-symbol “◦” at the endpoint(s) of an edge to denote the
presence or absence of an arrowhead. For example, the edge “ −◦ ” may denote either “−”
or “→”.

2.2 The PC Algorithm

The PC algorithm is comprised of three stages. We have summarized these stages as
pseudocode in Algorithms 5, 6 and 7 in Section A.1 of the Appendix. The ﬁrst stage
estimates the adjacencies of G, or the skeleton of G. Starting with a fully connected
skeleton, the algorithm attempts to eliminate the adjacency between any two variables,
say A and B, by testing if A and B are conditionally independent given some subset of the
neighbors of A or the neighbors of B. The search is performed progressively, whereby the
algorithm increases the size of the conditioning set starting from zero using a step size of 1.
The edge between A and B is removed, if A and B are rendered conditionally independent
given some subset of the neighbors of A or the neighbors of B.

The PC algorithm orients unshielded colliders in its second stage. Speciﬁcally, PC ﬁnds
triples A, B, C such that A − B − C, but A and C are non-adjacent. The algorithm

5

Strobl, Spirtes and Visweswaran

then determines whether B is contained in the set which rendered A and C conditionally
independent in the ﬁrst stage of PC. If not, A − B − C is replaced with A → B ← C.

The third and ﬁnal stage of PC involves the repetitive application of three rules to orient

as many of the remaining undirected edges as possible. The three rules include:

1.

If A − B, C → A and C and B are non-adjacent, then replace A − B with

A → B.

2.

3.

If A − B and A → C → B, then replace A − B with A → B.

(1)

If A − B, A − C → B, A − D → B, and C and D are non-adjacent,

then replace A − B with A → B.

Overall, the PC algorithm has been shown to be complete in the sense that it ﬁnds and

then orients edges up to GC, a CPDAG that represents G (Meek, 1995).

2.3 Hypothesis Testing

A hypothesis test is a method of statistical inference usually composed of one null (H0) and
one alternative (H1) hypothesis which are mutually exclusive; that is, if one occurs, then
the other cannot occur. The null hypothesis refers to the default position which asserts that
whatever one is trying to statistically infer actually did not happen. Note that the null and
alternative do not necessarily need to be logical complements of each other. For example,
one may be interested in determining whether the parameter µ is greater than zero. In this
case, the null can be deﬁned as µ = 0 while the alternative can be deﬁned as µ > 0 instead
of µ (cid:54)= 0.

A Type I error is the incorrect rejection of a true null hypothesis, or a false positive.
On the other hand, a Type II error is the failure to reject a false null hypothesis, or a false
negative. The p-value (p) is the probability of the Type I error, or the Type I error rate.
More speciﬁcally, the p-value is the probability of obtaining a result equal to or more extreme
than the observed value under the assumption of the null hypothesis. The null hypothesis
is thus rejected when the p-value is at or below a predeﬁned α threshold (typically the α
threshold is set to 0.05), because a low p-value demonstrates the improbability of the null
hypothesis.

2.4 False Discovery Rate

Multiple comparisons or multiple hypothesis testing refers to the process of considering
more than one statistical inference simultaneously. Failure to compensate for multiple
comparisons can result in erroneous inferences. For example, if an investigator performs
one hypothesis test with an α threshold of 0.05, then he or she has only a 5% chance of
making a Type I error. However, if the investigator performs 100 independent tests with
the same α threshold, then he or she has a 1 − (1 − 0.05)100 = 99.4% chance of making a
Type I error on at least one test.

In multiple hypothesis testing, the false discovery rate (FDR) at threshold α is the
expected proportion of false positives among the rejected null hypotheses. Speciﬁcally, we

6

PC with p-values

deﬁne the FDR at α as follows:

F DR(α) (cid:44) E

(cid:20)

V
max{R, 1}

(cid:21)

,

where V is the number of false positives, R is the total number of null hypotheses rejected,
and max{R, 1} ensures that F DR(α) is well-deﬁned when R = 0. We deﬁne the realized
FDR at α as V / max{R, 1}.

FDR estimation, or conservative point estimation of the FDR, refers to the process of

estimating F DR(α) in a conservative manner such that:

E[(cid:92)F DR(α)] ≥ F DR(α),

(cid:92)F DRBY (α) (cid:44)

mα (cid:80)m
1
i=1
i
max{R, 1}

.

where (cid:92)F DR(α) represents an estimate of F DR(α). We denote E[(cid:92)F DR(α)]−F DR(α) as the
estimation bias. Note that there are several ways of obtaining (cid:92)F DR(α). In 2001, Benjamini
and Yekutieli proposed the following FDR estimator for m hypothesis tests:

(2)

(3)

FDR estimators such as (cid:92)F DRBY can be used to deﬁne FDR controlling procedures. These
procedures determine the optimal threshold α∗ which achieves strong control 2 of the FDR
in the following sense:

α∗ (cid:44) arg max

{(cid:92)F DR(α) ≤ q}

α

The FDR controlling procedure based on (cid:92)F DR involves the rejection of all null hypotheses
with p-values below the α∗ threshold. We refer to the quantity F DR(α∗) − q as the control
bias. Benjamini and Yekutieli proved that the estimate (cid:92)F DRBY in particular achieves
strong control of the FDR with any form of dependence among the p-values of m hypothesis
tests.

3. Upper Bounds on the P-Value

We present two upper bounds of the Type I error rate of hypothesis tests which can be
constructed using a set of simpler hypothesis tests. These upper bounds will serve as useful
tools in Section 4 for bounding the Type I error rate of the hypothesis tests which will be
used to infer the presence or absence of edges in a CPDAG.

3.1 Union Bound

Consider the following hypothesis test for two random variables given a conditioning set:

2. Strong control of the FDR refers the process of controlling the FDR under any conﬁguration of true and
false null hypotheses; on the other hand, weak control refers to the process of controlling the FDR when
all of the null hypotheses are true. Strong control is therefore preferable to weak control.

H0 : Conditionally independent,
H1 : Conditionally dependent.

7

Strobl, Spirtes and Visweswaran

f ((cid:98)si)

−1.96

0

1.96

(cid:98)si

Figure 2: In the above standard normal case, we have Pr(|(cid:98)si| ≥ sα

(cid:12)
(cid:12)si = 0) = 0.05, where
sα
i = 1.96. We reject the null hypothesis when |(cid:98)si| falls in the blue colored regions
at the tails.

i

Trivially, we can rephrase the null and alternative in terms of a conditional independence
(CI) oracle:

H0 : The CI oracle outputs independent,
H1 : The CI oracle outputs dependent.

Now suppose we want to query m CI oracles about m CI relations. We can then consider
the following null and alternative:

H0 : All CI oracles output independent,
H1 : At least one CI oracle outputs dependent.

(4)

i

From here on, we write Pr(CI test i outputs dependent | CI oracle i outputs independent) to
(cid:12)
denote Pr(|(cid:98)si| ≥ sα
(cid:12)si = 0), where si refers to a parameter of some standardized distribution
used by CI test i, (cid:98)si a random variable and the test statistic estimating si, and sα
i a value of
si determined by an α level. We provide an example in Figure 2, where si may correspond
to Fisher’s z-statistic in the case of Fisher’s z-test for the mean parameter si = 0 of the
standard normal distribution.

8

PC with p-values

We now bound the Type I error rate of the hypothesis test (4) by using the new notation

and the union bound:

Pr(Type I error)

= Pr(at least one CI test outputs dependent|H0)

(cid:33)

= Pr

CI test i outputs dependent|H0

Pr(CI test i outputs dependent|H0)

(cid:32) m
(cid:95)

i=1

≤

=

=

m
(cid:88)

i=1
m
(cid:88)

i=1
m
(cid:88)

i=1

pi,

Pr(CI test i outputs dependent|CI oracle i outputs independent)

where pi denotes the Type I error rate of CI test i. Thus, if the r.h.s. of (5) is less than
the α threshold, then we can conclude that the Type I error rate of (4) is also below the
threshold. In other words, (5) is a conservative p-value.

Note that the third equality in the derivation of (5) uses the simplifying assumption
that the probability of the output of CI test i only depends on the output of CI oracle
i when given the outputs of all CI oracles. Several papers have used this assumption
implicitly in their proofs (Tsamardinos and Brown, 2008; Li and Wang, 2009), and we will
also use it throughout this paper. We can justify the assumption based on three facts.
First, most CI test statistics si have a limiting distribution which only depends on si = 0
under the null. For example, Fisher’s z-statistic has a limiting standard normal distribution
with mean parameter zi = 0 and constant variance. Moreover, the G-statistic for the G-
test has a limiting χ2-distribution with non-centrality parameter gi = 0 and degrees of
freedom determined by the number of cells in the contingency table. Second, existing
methods which utilize bounds based on the assumption have strong empirical performance;
loose-enough bounds therefore appear to accommodate the assumption well in most ﬁnite
sample cases. Third, recall that simplifying assumptions are not new in the causality
literature; indeed, many authors have made simplifying assumptions regarding parameter
independence for Bayesian methods which similarly increase computational eﬃciency and
achieve strong empirical performance (e.g., (Cooper and Yoo, 1999)).

We can now also generalize the bound in (5) to any hypothesis test consisting of a series
of logical disjunctions in the alternative and a series of logical conjunctions in the null.
Namely:

(5)

(6)

H0 :

oracle i outputs ¬Pi,

H1 :

oracle i outputs Pi,

m
(cid:94)

i=1
m
(cid:95)

i=1

9

Strobl, Spirtes and Visweswaran

where Pi denotes an arbitrary output of oracle i. We now have:

Pr(Type I error) = Pr(

test i outputs Pi |H0)

m
(cid:95)

i=1

Pr(test i outputs Pi|H0)

Pr(test i outputs Pi|oracle i outputs ¬Pi)

≤

=

=

m
(cid:88)

i=1
m
(cid:88)

i=1
m
(cid:88)

i=1

hi,

where hi is the Type I error rate of test i, and the second equality uses the assumption that
the probability of the output of test i only depends on the output of oracle i when given all
oracles. We will use this generalization in Section 4.

3.2 Intersection Bound

Suppose we want to perform a hypothesis test with the following null and alternative which
are diﬀerent than the null and alternative in (4):

H0 : At least one CI oracle outputs independent,
H1 : All CI oracles output dependent.

Now assume we know that the ith CI oracle outputs independent. We can then bound the
Type I error rate of (7) as follows with m queries to the CI oracle:

Pr(Type I error) = Pr(all m CI tests output dependent|H0)

≤ Pr(CI test i outputs dependent |H0)
= Pr(CI test i outputs dependent | CI oracle i outputs independent

∧ other CI oracles may output independent)

= Pr(CI test i outputs dependent | CI oracle i outputs independent)
= pi,

where the second equality again holds under the assumption that the probability of the
output of CI test i only depends on the output of CI oracle i when given the outputs of all
CI oracles. We can therefore bound the Type I error rate of (7) using the p-value of a single
CI test for which the CI oracle outputs independent. Nevertheless, in practice, we often do
not know for which query the oracle outputs independent in the null. We do however know
that at least one unknown CI oracle i outputs independent, so we can bound the Type I
error rate of (7) using the maximum over all of the m CI test p-values:

Pr(Type I error) = Pr(all m CI tests output dependent|H0)

≤ Pr(CI test i outputs dependent | CI oracle i outputs independent)
= pi ≤ max

pj.

(9)

j=1,...,m

(7)

(8)

10

PC with p-values

H0 :

oracle i outputs ¬Pi,

H1 :

oracle i outputs Pi.

m
(cid:95)

i=1
m
(cid:94)

i=1

m
(cid:94)

i=1

Note that we can generalize the above bound to any hypothesis test consisting of a
series of logical conjunctions in the alternative and a series of logical disjunctions in the
null. Namely:

(10)

(11)

We therefore have:

Pr(Type I error) = Pr(

test i outputs Pi|H0)

≤ Pr(test i outputs Pi|H0)
= Pr(test i outputs Pi|oracle i outputs ¬Pi )
= hi ≤ max

hj,

j=1,...,m

where the second equality again uses the assumption that the probability of the output of
test i only depends on the output of oracle i when given all oracles.

4. Edge-Speciﬁc Hypothesis Tests

We now show how to apply the two upper bounds of the Type I error rate to derive p-value
estimates for both the undirected and directed edges in the CPDAG as estimated by PC.
Bounding the p-value for each edge therefore amounts to adding up and/or maximizing over
the p-values returned from multiple CI tests.

Note that we will sometimes invoke a zero Type II error rate assumption in this section.
This assumption is necessary to correctly upper bound the p-values of the edge-speciﬁc
hypothesis tests of the CPDAG according to the CI tests executed by the PC algorithm. In
fact, we can always correctly bound the p-values, if we perform all of the possible CI tests
between the considered variables; however, this approach is impractical, since it ignores
the eﬃciencies of the PC algorithm. A more interesting strategy involves designing the
edge-speciﬁc hypothesis tests so that the p-value bounds are robust to Type II errors as
well as redesigning the PC algorithm to catch many Type II errors. We will discuss these
approaches in detail in Sections 4.4 and 5, so we encourage readers to accept the zero Type
II error rate assumption for now.

4.1 Skeleton Discovery

We ﬁrst consider the skeleton discovery phase of the PC algorithm. We wish to test whether
each edge is absent in the true skeleton starting from a completely connected undirected
graph. This problem has already been investigated in (Li et al., 2008; Tsamardinos and
Brown, 2008; Li and Wang, 2009; Armen and Tsamardinos, 2011, 2014), but we review it
here for completeness. We construct a hypothesis test with the following null and alterna-

11

Strobl, Spirtes and Visweswaran

(12)

(13)

(14)

(15)

(16)

tive:

H0 : A − B is absent,
H1 : A − B is present.

Now consider the following proposition, where P a(A) denotes the true parents of A:

Proposition 1 (Spirtes et al., 2000) Consider a DAG G which satisﬁes the global directed
Markov property. Moreover, assume that the probability distribution is d-separation faithful.
Then, there is an edge between two vertices A and B if and only if A and B are conditionally
dependent given any subset of P a(A) \ B and any subset of P a(B) \ A.

We thus consider the following two scenarios for the undirected edge A − B:

1.

If A and B are conditionally independent given some subset of P a(A) \ B

or some subset of P a(B) \ A, then A − B is absent.

2.

If A and B are conditionally dependent given any subset of P a(A) \ B

and any subset of P a(B) \ A, then A − B is present.

The following null and alternative are therefore equivalent to (12), where CI oracles are
queried about A and B given all possible subsets of P a(A) \ B and all possible subsets of
P a(B) \ A:

H0 : At least one CI oracle outputs independent,
H1 : All CI oracles output dependent.

Notice that the above hypothesis test is the same as the hypothesis test in (7). We can
therefore bound the p-value of (12) using:

p(cid:48)
A−B

(cid:44) max
i=1,...,q(cid:48)

pA⊥⊥B|Ri,

where Ri ⊆ {Pa(A) \ B} or Ri ⊆ {Pa(B) \ A} and q(cid:48) denotes the total number of such
subsets.

Note that the skeleton discovery phase of the PC algorithm cannot diﬀerentiate between
the parents and children of a particular vertex using its neighbors. However, we can further
bound (15) using the following quantity:

p(cid:48)
A−B ≤ max
i=1,...,q

pA⊥⊥B|Si

(cid:44) pA−B,

where Si ⊆ {N(A) \ B} or Si ⊆ {N(B) \ A} and q denotes the total number of such subsets.
Now assume that the Type II error rate of all CI tests is zero. Then, if the alternative
holds for the CI tests (conditional dependence), then the alternative is accepted. Hence, the
PC algorithm will not remove any of the edges between N (A) and A as well as any of the
edges between N (B) and B. PC therefore performs all necessary CI tests for computing
(16), so upper bounding the Type I error rate for (12) reduces to taking the maximum of
the p-values for all of the CI tests performed by PC regarding A and B. For example,
suppose we measure three random variables A, B and C. Then we obtain p-values after the
PC algorithm tests whether A ⊥⊥ B and A ⊥⊥ B|C. Suppose these p-values are (0.03, 0.04)
so that the PC algorithm with an α threshold of 0.05 determines that A − B is present.
The p-value upper bound of (12) thus corresponds to max {0.03, 0.04} = 0.04.

12

PC with p-values

4.2 Detecting V-Structures

4.2.1 Deterministic Skeleton

The hypothesis testing procedure for directed edges is more complicated than the procedure
for adjacencies. Edges can be oriented in the PC algorithm according to unshielded v-
structures or the orientation rules as described in Section 2.2. Let us ﬁrst focus on the
former and, for further simplicity, let us also assume that 1) we have access to the ground
truth skeleton and 2) no edge is involved in more than one unshielded v-structure (we will
later drop these assumptions in Section 4.2.2). Our task then is to statistically infer the
presence of an unshielded v-structure.

We now present the following null and alternative for each unshielded v-structure after

ﬁnding a triple A − C − B such that A and B are non-adjacent in the skeleton:

H0 : Unshielded A → C ← B is absent,
H1 : Unshielded A → C ← B is present.

(17)

Next, consider the following proposition:

Proposition 2 (Spirtes et al., 2000) Consider the same assumptions as Proposition 1.
Further assume that A, C are adjacent and C, B are adjacent but A, B are non-adjacent.
Then, A and B are conditionally independent given some subset of P a(A)\B which does not
include C or some subset of P a(B)\A which does not include C if and only if A → C ← B.

The following null and alternative is therefore equivalent to (17):

H0 : A and B are conditionally dependent given any subset of P a(A) \ B
which does not include C and any subset of P a(B) \ A which

H1 : A and B are conditionally independent given some subset of P a(A) \ B

which does not include C or some subset of P a(B) \ A which

(18)

does not include C,

does not include C.

The above alternative is reminiscent of the way in which PC determines the presence of an
unshielded v-structure according to Algorithm 6 in the Appendix; speciﬁcally, if C is not in
the set which renders A and B conditionally independent, then C in A − C − B must be a
collider. We however cannot bound the p-value of (18) using CI tests, because conditional
dependence is in the null and conditional independence is in the alternative, as opposed to
vice versa. As a result, we also consider the following proposition:

Proposition 3 Consider the same assumptions as Proposition 2. Then, A and B are
conditionally dependent given any subset of P a(A) \ B containing C and any subset of
P a(B) \ A containing C if and only if A → C ← B.

Proof First notice that P a(A) = {P a(A) \ B} and P a(B) = {P a(B) \ A}, since A and
B are non-adjacent. As a result, we can instead prove that the if and only if statement
holds for P a(A) and P a(B) without loss of generality.

13

Strobl, Spirtes and Visweswaran

For the forward direction, suppose A and B are conditionally dependent given any
subset of P a(A) containing C and any subset of P a(B) containing C. Then A and B are
d-connected given any subset of P a(A) containing C and any subset of P a(B) containing
C by the global directed Markov property. Clearly, C ∈ N (A) and C ∈ N (B), so C must
either be a parent of A and a parent of B, a child of A and a parent of B, a parent of A and
a child B, or a child of A and a child of B. Note that A and B are non-adjacent, so A and
B are d-separated given some subset of P a(A) or some subset of P a(B) by Proposition
1 and d-separation faithfulness. Moreover, the subset must include C if C is a parent of
A and a parent of B, a child of A and a parent of B, or a parent of A and a child B;
otherwise, A and B would be d-connected. As a result, in those three situations, we arrive
at the contradiction that A and B are d-separated given some subset of P a(A) containing
C or some subset of P a(B) containing C. We conclude that C must be a child of A and a
child of B.

For the other direction, if A → C ← B holds, then A and B are d-connected given
any subset of P a(A) containing C and any subset of P a(B) containing C. D-separation
faithfulness then implies that A and B are conditionally dependent given any subset of
P a(A) containing C and any subset of P a(B) containing C.

We can thus equivalently write (18) as:

H0 : A and B are conditionally independent given some subset of P a(A) \ B

containing C or some subset of P a(B) \ A containing C,

H1 : A and B are conditionally dependent given any subset of P a(A) \ B

containing C and any subset of P a(B) \ A containing C.

(19)

We can bound the Type I error rate of the above hypothesis test by taking the maximum
p-value over certain CI tests:

p(cid:48)
γAB|C

(cid:44) max
i=1,..,m(cid:48)

pA⊥⊥B|Mi,

where Mi denotes a subset of P a(A) \ B containing C or a subset of P a(B) \ A containing
C, and m(cid:48) is the total number of subsets Mi. Of course, in practice, we do not know which
vertices are the parents. However, we can also upper bound (19) as follows:

p(cid:48)
γAB|C

≤ max
i=1,..,m

pA⊥⊥B|Ti

(cid:44) pγAB|C ,

(20)

where Ti denotes a subset of N (A) \ B containing C or a subset of N (B) \ A containing
C, and m denotes the total number of subsets Ti. Note that we do not need the zero Type
II error rate assumption for computing (20), since we assume that the skeleton is provided.

4.2.2 Inferred Skeleton

We have considered orienting the colliders, if we have access to the ground truth skeleton.
We now consider the more complex problem of orienting the colliders, if we must also
statistically infer the skeleton.

We again consider the following null and alternative:

H0 : Unshielded A → C ← B is absent,
H1 : Unshielded A → C ← B is present.

(21)

14

PC with p-values

Now, the PC algorithm determines that the alternative holds, if all of the following condi-
tions are true:

1. A and C are conditionally dependent given any subset of P a(A) \ C

2. B and C are conditionally dependent given any subset of P a(B) \ C

and any subset of P a(C) \ A.

and any subset of P a(C) \ B.

3. A and B are conditionally dependent given any subset of P a(A) \ B

containing C and any subset of P a(B) \ A containing C.

We therefore have the following equivalent form of the null and alternative as in (21), if we
assume A and B are non-adjacent:

H0 : At least one condition from (22) does not hold,
H1 : All conditions from (22) hold.

Note that the non-adjacency assumption is reasonable because we did not have enough
Indeed, non-
statistical evidence to invalidate the assumption when we executed (12).
adjacencies are always assumed unless the data suggests that the null of (12) is unlikely.
Now, the alternative of (23) is a series of three logical conjunctions, and the null is a series
of three logical disjunctions as in (10), so the Type I error rate of (23) can be bounded
using the intersection bound:

Pr(Conditions 1, 2, 3 |H0) ≤ Pr(Any one condition |H0)

≤ max{h1, h2, h3}.

We will be using shorthand from here on. We write (23) equivalently as:

H0 : ¬(A − C) ∨ ¬(B − C) ∨ ¬γAB|C,
H1 : (A − C) ∧ (B − C) ∧ γAB|C,

where A − C, B − C, and γAB|C represent Condition 1, 2 and 3 from (22), respectively. We
therefore have a p-value bound of (21) similar to (24):

(cid:16)

Pr

(A − C) ∧ (B − C) ∧ γAB|C|H0
(cid:16)
A − C(cid:12)
(cid:16)
(cid:110)

(cid:12)¬(A − C)
A − C(cid:12)

(cid:12)¬(A − C)

, Pr

Pr

(cid:16)

(cid:17)

(cid:17)

≤ Pr

≤ max

(cid:17)

≤ max{pA−C, pB−C, pγAB|C }.

B − C(cid:12)

(cid:17)

(cid:16)

(cid:12)¬(B − C)

, Pr

γAB|C

(cid:12)
(cid:12)¬γAB|C

(cid:17)(cid:111)

Notice that computing pγAB|C requires N (A) and N (B), not just their respective empirical
estimates (cid:99)N (A) and (cid:99)N (B) which PC can discover. However, we can invoke a zero Type
II error rate assumption in order to ensure that N (A) ⊆ (cid:99)N (A) and N (B) ⊆ (cid:99)N (B) as
explained in detail in Section 4.4, so pγAB|C can still be upper bounded. The assumption

15

(22)

(23)

(24)

(25)

(26)

Strobl, Spirtes and Visweswaran

also ensures that we can upper bound pA−C and pB−C according to Section 4.1. We conclude
that a zero Type II error rate ensures that (26) can be computed.

Next, consider the situation where PC can orient any one edge by using more than one
unshielded v-structure. For example, consider the DAG in Figure 3. In this case, PC can
orient A − C by using either B1 → C or B2 → C (or both); we may therefore want to
take both situations into account. Note that the original PC algorithm always orients an
edge according to one v-structure which it picks arbitrarily according to the ordering of its
computations. We thus only require the bound (26) in this case. However, we will propose
a modiﬁed PC algorithm in Section 5 which takes into account all possible ways to orient
one edge. Now, we can use the following null and alternative for Figure 3 when assuming
that both A and B1 and A and B2 are non-adjacent:

H0 : ¬(A − C) ∨
(cid:16)

H1 : (A − C) ∧

(cid:16)

(cid:17)
[¬(B1 − C) ∨ ¬γAB1|C] ∧ [¬(B2 − C) ∨ ¬γAB2|C]

,

(cid:17)
[(B1 − C) ∧ γAB1|C] ∨ [(B2 − C) ∧ γAB2|C]

.

(27)

We can therefore bound the Type I error rate of (27) as follows, where G = ¬(A − C) and
H = [¬(B1 − C) ∨ ¬γAB1|C] ∧ [¬(B2 − C) ∨ ¬γAB2|C], H1 = ¬(B1 − C) ∨ ¬γAB1|C, and
H2 = ¬(B2 − C) ∨ ¬γAB2|C:

(cid:110)

(cid:110)

(cid:110)

(cid:16)

Pr

(cid:16)

(A − C) ∧
(cid:110)

[(B1 − C) ∧ γAB1|C] ∨ [(B2 − C) ∧ γAB2|C]

(cid:17)

(cid:17)(cid:12)
(cid:12)H0

≤ max

Pr(A − C|G), Pr

(B1 − C) ∧ γAB1|C

≤ max

Pr(A − C|G), Pr

(B1 − C) ∧ γAB1|C|H

(cid:105)

∨
(cid:17)

(cid:104)
(B2 − C) ∧ γAB2|C

(cid:105)(cid:12)
(cid:12)H

(cid:17)(cid:111)

(cid:16)

+ Pr
(cid:17)

(cid:17)(cid:111)

(B2 − C) ∧ γAB2|C|H
(cid:16)

(B2 − C) ∧ γAB2|C|H2

(cid:17)(cid:111)

(28)

(cid:16)(cid:104)

(cid:16)

(cid:16)

= max

Pr(A − C|G), Pr

(cid:16)

(B1 − C) ∧ γAB1|C|H1
(cid:110)
B1 − C(cid:12)
(cid:17)

Pr

(cid:12)¬(B1 − C)

+ Pr
(cid:17)

, Pr(γAB1|C
(cid:111)(cid:111)

≤ max

Pr(A − C|G), max
(cid:110)
B2 − C(cid:12)

(cid:16)

Pr

+ max
(cid:110)
pA−C, max{pB1−C, pγAB1|C } + max{pB2−C, pγAB2|C }

(cid:12)¬(B2 − C)

, Pr(γAB2|C

≤ max

(cid:111)
.

(cid:12)
(cid:12)¬γAB2|C)

(cid:12)
(cid:12)¬γAB1|C)

(cid:111)

Figure 3: Here, one can orient the edge A − C according to the two unshielded v-structures

A → C ← B1 and A → C ← B2.

B1

B2

A

C

16

PC with p-values

More generally, for an arbitrary number, say j, of multiple possible ways to orient A − C
by unshielded v-structures, we have:

(cid:16)

Pr

(A − C) ∧

(cid:16)

(cid:110)

≤ max

pA−C,

i=1

[(B1 − C) ∧ γAB1|C] ∨ ... ∨ [(Bj − C) ∧ γABj |C]
j
(cid:88)

(cid:111)
,
max{pBi−C, pγABi|C }

(cid:17)

(cid:17)(cid:12)
(cid:12)H0

(29)

assuming that B1, . . . , Bj are all non-adjacent to A.

4.3 Orientation Rules

We now consider bounding the p-values of edges which are oriented using the orientation
rules of the PC algorithm. Recall from (1) that the PC algorithm only requires the repeated
application of three orientation rules to be complete. We analyze these three orientation
rules in separate subsections.

4.3.1 First orientation rule

We can construct the hypothesis test for the ﬁrst orientation rule as follows according to
the suﬃcient conditions of the ﬁrst rule in (1):

H0 : ¬(A − B) ∨ ¬(C → A),
H1 : (A − B) ∧ (C → A).

(30)

We again also assume that C and B are non-adjacent. Now the PC algorithm determines
that the alternative holds, if all of the following conditions are true:

1. A − B : A and B are conditionally dependent given any subset of P a(A) \ B and any

subset of P a(B) \ A.

2. C → A : An edge is oriented from C to A under two scenarios. In the ﬁrst, the edge
is oriented because A is the collider in an unshielded v-structure. In the second, the
edge is oriented due to the previous application of an orientation rule.

We thus have a logical conjunction and can bound the Type I error rate using the intersection
bound:

Pr((A − B) ∧ (C → A)|H0) ≤ max{pA−B, pC→A},

where pC→A refers to the p-value bound for the hypothesis test of an unshielded v-structure
or a previously applied orientation rule. Of course, pC→A will be the former when the PC
algorithm begins to execute the orientation rules. More generally, for Ci → A that can
orient A − B where i = 1, ..., j, we have:

(cid:16)

Pr

(A − B) ∧

(cid:105)
(cid:104)
|H0
(C1 → B) ∨ · · · ∨ (Cj → A)

(cid:17)

(cid:110)

≤ max

pA−B,

pCi→A

(cid:111)
,

j
(cid:88)

i=1

(31)

where we require that C1, . . . , Cj are all non-adjacent to B.

17

Strobl, Spirtes and Visweswaran

4.3.2 Second orientation rule

We have the following hypothesis test according to the suﬃcient conditions of the second
rule in (1):

H0 : ¬(A − B) ∨ ¬(A → C → B),
H1 : (A − B) ∧ (A → C → B).

(32)

Hence, by conjunction:

Pr((A − B) ∧ (A → C → B)|H0) ≤ max{pA−B, pA→C→B},

where pA→C→B ≤ max{pA→C, pC→B}. The above Type I error rate can therefore be further
upper bounded by max{pA−B, pA→C, pC→B}. More generally, we have:

Pr

(A − B) ∧

(cid:104)

(cid:105)
(A → C1 → B) ∨ ... ∨ (A → Cj → B)

|H0

(cid:17)

(cid:110)

(cid:111)

(cid:110)

≤ max

pA−B,

pA→Ci→B

≤ max

pA−B,

max{pA→Ci, pCi→B}

(33)

(cid:111)
.

j
(cid:88)

i=1

(cid:16)

j
(cid:88)

i=1

4.3.3 Third orientation rule

We have the following null and alternative by the suﬃcient conditions of the third rule in
(1), assuming that C and D are non-adjacent:

H0 : ¬(A − B) ∨ ¬(A − C → B) ∨ ¬(A − D → B),
H1 : (A − B) ∧ (A − C → B) ∧ (A − D → B).

(34)

We can bound the Type I error rate of the above hypothesis test as follows:

(cid:16)

(cid:110)

Pr

(A − B) ∧ (A − C → B) ∧ (A − D → B)|H0

≤ max{pA−B, pA−C→B, pA−D→B}

≤ max

pA−B, max{pA−C, pC→B}, max{pA−D, pD→B}

(cid:17)

(cid:111)
.

The general case is slightly more complicated than the ﬁrst and second orientation rules.
In this case, we need to control the Type I error rate of accepting at least two paths as
opposed to one. Let the set D include all three-node paths from A to B with the ﬁrst edge
undirected from A to a middle vertex and the second edge directed from the middle vertex
to B such that the ith element of D is:

Di (cid:44) A − Ci → B.

Let us suppose D has a total of n elements and assume that no middle vertex Ci is adjacent
to any other middle vertex. Now, let D(cid:48) be the set containing all of the n choose 2 elements
of D. The ith element in D(cid:48) is therefore:

D(cid:48)
i

(cid:44) {A − Ck → B, A − Cl → B},

18

PC with p-values

(cid:16)

(cid:16)

(cid:104)

r
(cid:88)

i=1

(cid:104)

r
(cid:88)

i=1

where k and l are the distinct indices represented the two chosen middle vertices. Let D(cid:48)
i,1
(cid:1). We then
and D(cid:48)
have:

i,2 be the ﬁrst and second elements in D(cid:48)

i, respectively. Also let r = (cid:0)n

2

Pr

(A − B) ∧

(D(cid:48)

1,1 ∧ D(cid:48)

1,2) ∨ ... ∨ (D(cid:48)

r,1 ∧ D(cid:48)

r,2)

(cid:17)

(cid:105)(cid:12)
(cid:12)H0

(cid:110)

≤ max

pA−B,

Pr(D(cid:48)

i|H0)

(cid:111)
,

where Pr(D(cid:48)

i|H0) (cid:44) p{A−Ck→B,A−Cl→B} and is bounded as follows:

Pr(D(cid:48)

i|H0) ≤ max{pA−Ck→B, pA−Cl→B}

≤ max{pA−Ck , pCk→B, pA−Cl, pCl→B}
(cid:44) ¨Pr(D(cid:48)

i|H0),

We therefore have:

Pr

(A − B) ∧

(D(cid:48)

1,1 ∧ D(cid:48)

1,2) ∨ ... ∨ (D(cid:48)

r,1 ∧ D(cid:48)

r,2)

(cid:17)

(cid:105)(cid:12)
(cid:12)H0

(cid:110)

≤ max

pA−B,

¨Pr(D(cid:48)

i|H0)

(cid:111)
.

(35)

4.4 Summary and Analysis of the Bounds

We derived several bounds for edge orientation as summarized in Table 1. We created the
bounds by engineering speciﬁc hypothesis tests and successively applying the union and
intersection bounds accordingly. Note that j and r are usually very small in sparse graphs.
One may now wonder whether PC can actually control the bounds listed in Table 1 (we
say that a quantity can be controlled, if the quantity can be upper bounded). Recall that we

Table 1: P-value bounds for all of the edge types in a CPDAG. Note that Si ⊆ {N (A) \ B}

or Si ⊆ {N (B) \ A}.

P-Value Bound

Equation Num.

Edge Type

Undirected

Unshielded v-structure max

First orientation rule

Second orientation rule max

Third orientation rule

maxi pA⊥⊥B|Si

(cid:110)
pA−C, (cid:80)j
i=1 max{pBi−C, pγABi|C }
(cid:110)
pA−B, (cid:80)j

i=1 pCi→A

max

(cid:111)

(cid:111)

(cid:110)
pA−B, (cid:80)j
(cid:110)
pA−B, (cid:80)r

max

i=1 max{pA→Ci, pCi→B}

(cid:111)

¨Pr(D(cid:48)

i|H0)

i=1

(cid:111)

(16)

(29)

(31)

(33)

(35)

19

Strobl, Spirtes and Visweswaran

provided a rough, aﬃrmative answer to the question in Sections 4.1 and 4.2.2 by assuming
a zero Type II error rate. We now spell out a more detailed answer via a theorem whose
proof builds on the argument of Theorem 4 in (Armen and Tsamardinos, 2014).

Theorem 4 Suppose that the PC algorithm is applied to a sample from P represented by
DAG G. If we have:

1. P is d-separation faithful to G,

2. The Type II error rate is zero,

3. The PC algorithm also tests whether any two non-adjacent vertices A, B with common
neighbor C are conditionally dependent given any subset of P a(A) \ B containing C
and any subset of P a(B) \ A containing C,

then all of the p-value bounds in Table 1 can be controlled using the p-values of the CI tests
executed by PC.

Proof Consider any two vertices A and B. Algorithm 1 starts with a fully connected
graph, so we have B ∈ (cid:99)N (A) and A ∈ (cid:99)N (B) in the beginning. Note that Algorithm 1
executes testA⊥⊥B|S for all S ⊆ (cid:99)N (A) \ B and for all S ⊆ (cid:99)N (B) \ A. The zero Type II error
rate ensures the following: if the alternative holds, then the alternative is accepted. As a
result, Algorithm 1 will not remove any vertices adjacent to A and any vertices adjacent to
B with a zero Type II error rate. Hence, we always have {N (A) \ B} ⊆ { (cid:99)N (A) \ B} and
{N (B) \ A} ⊆ { (cid:99)N (B) \ A}. Algorithm 1 therefore must eventually execute testA⊥⊥B|S for
all S ⊆ {N (A) \ B} and for all S ⊆ {N (B) \ A}, so (16) can be controlled.

For (29), the p-value bounds for undirected edges can already be controlled by the
previous paragraph. We must now argue that pγAB|C can be controlled. Let C be a collider
between non-adjacent vertices A and B. Now notice that C ∈ N (A) ⊆ (cid:99)N (A) and C ∈
N (B) ⊆ (cid:99)N (B), so Algorithm 2 must execute testA⊥⊥B|S for all S ⊆ {N (A) \ B} containing
C and for all S ⊆ {N (B) \ A} containing C. Hence pγAB|C can be controlled.

Now the p-value bounds (31), (33) and (35) can be controlled trivially because the p-

value bounds for (15) and (29) can be controlled.

In other words, PC can control the bounds in Table 1 with some additional CI tests and a
zero Type II error rate.

Of course, the Type II error rate is never zero in practice, but this becomes less of an
issue as the sample size increases. We may also consider reducing the Type II error rate by
simultaneously implementing three strategies:

1. Use a liberal (higher) α threshold. We for example often use an α threshold of 0.20 in
the experiments. This is the simplest strategy which decreases the Type II error rate
but also increases the Type I error rate. However, we can then control the Type I error
rate post-hoc with an FDR controlling procedure. Of course, setting the α threshold
too high will prevent the PC algorithm from terminating within a reasonable amount
of time as well as loosen the p-value bounds, since the CI tests will fail to explain
away many edges. We therefore cannot rely entirely on this ﬁrst strategy.

20

PC with p-values

2. Use hypothesis tests whose p-value bounds are robust to Type II errors. The hy-
pothesis tests in Section 4.4 are in fact robust to such errors due to the intersection
bound as explained in detail in Appendix A.2. Brieﬂy, we can also reasonably con-
sider modifying the null hypotheses of (25), (30), (32) and (34) to “no edges between
any of the vertices.” This corresponds to converting the logical disjunctions in the
null of (10) into conjunctions which in turns leads to a less robust p-value bound
involving the minimum of a set of p-values instead of the maximum. As a result,
under-estimating one p-value in the p-value set due to Type II error(s) can cause PC
to also under-estimate the bound of (25), (30), (32) or (34).

3. Modify the PC algorithm to prevent and catch many Type II errors.

The last strategy is more complex, so we discuss it in detail in the next section.

5. The PC Algorithm with P-Values

We now propose a modiﬁed PC algorithm called PC with p-values (PC-p) that reduces the
inﬂuence of Type II errors by preventing and catching potential Type II errors. At the same
time, PC-p is correct - the algorithm operates diﬀerently than PC, but it maintains PC’s
desirable soundness and completeness properties.

The PC-p algorithm involves two ideas. First, PC-p performs skeleton discovery with the
same skeleton discovery procedure used in the PC-stable algorithm (Colombo and Maathuis,
2014). This procedure ensures that the algorithm does not skip some CI tests due to Type
II errors and variable ordering. The second idea behind PC-p involves a modiﬁcation to the
procedure for propagating edge orientations. Speciﬁcally, if two edge orientations conﬂict,
PC-p admits bidirected edges instead of over-writing previous orientations like PC. PC-p
then unorients the bidirected edges as well as the directed edges which were directly used to
infer the presence of the bidirected edges. The algorithm subsequently labels the resulting
undirected edges as “ambiguous” which ensures that PC-p does not orient additional edges
using the ambiguous edges. Indeed, the PC-p algorithm uses conﬂicts in edge orientation
to detect potential Type II errors and prevent the propagation of the errors throughout the
graph. In practice, we ﬁnd that these two modiﬁcations to the PC algorithm help PC-p
with the BY estimator achieve more accurate strong estimation and control of the FDR
than PC, as we will see in Section 6.

We now describe the PC-p algorithm in detail; however, we will not describe the com-
putation of the p-value upper bounds until Section 5.5 in order to keep the presentation
clear. We have divided the PC-p algorithm into Algorithms 1, 2, 3 and 4, where the ﬁrst
three procedures correspond to Algorithms 5, 6 and 7 of the original PC algorithm.

5.1 Skeleton Discovery

We ﬁrst consider skeleton discovery. The original PC algorithm uses Algorithm 5 to discover
the skeleton. However, Algorithm 5 can cause the sample version of the PC algorithm to skip
some CI tests due to variable ordering and Type II errors. For example, consider the causal
graph in Figure 4a as ﬁrst presented in (Colombo and Maathuis, 2014). In this example,
suppose the CI tests correctly determine that A ⊥⊥ B and B ⊥⊥ D|{A, C} but incorrectly
determine that C ⊥⊥ D|{A, E}. The incorrect inference is a Type II error, since C and D

21

Strobl, Spirtes and Visweswaran

are adjacent in the true graph. Now consider the following ordering of variables for the PC
algorithm: order1(X) = (A, D, B, C, E). In this case, the ordered pair (D, B) is considered
before (D, C) in Algorithm 5, since (D, B) comes earlier in order1(X). The PC algorithm
removes D − B because a CI test determines that D ⊥⊥ B|{A, C} and {A, C} is a subset of
N (D) = {A, B, C, E}. Next, D − C is considered and erroneously removed because a CI
test determines that D ⊥⊥ C|{A, E} and {A, E} is a subset of N (D) = {A, C, E}. We thus
ultimately obtain the skeleton in Figure 4b with order1(X).

Now consider an alternative ordering of the variables: order2(X) = (A, C, D, B, E). In
this case, (C, D) is considered before (D, B) in Algorithm 5, and the algorithm erroneously
removes C − D. Next, the algorithm considers D − B but {A, C} is not a subset of N (D) =
{A, B, E}, so D − B remains. Even when the PC algorithm eventually also considers the
same undirected edge as B − D, {A, C} is again not a subset of N (B) = {C, D, E}, so
B − D remains. In other words, (C, D) is considered ﬁrst in order2(X) which causes C
to be removed from N (D). Algorithm 5 therefore never executes testB⊥⊥D|{A,C}. We thus
ultimately obtain the skeleton in Figure 4c with order2(X).

The previous two examples show that the Type II error of incorrectly determining that
C ⊥⊥ D|{A, E} leads PC to infer two diﬀerent skeletons due to diﬀerences in variable order-
ing. Clearly, we would like to eliminate the dependency of skeleton discovery on variable
ordering and also reduce its dependency on Type II errors at the same time. Fortunately,
Colombo and Maathius proposed such a modiﬁcation of Algorithm 5 as outlined in Algo-
rithm 1. The key diﬀerence between Algorithm 5 and 1 involves the for loop in steps 5-7 of
Algorithm 1 which computes and stores the adjacency sets after each new conditioning set
size. As a result, an incorrect edge deletion due to a Type II error on line 16 of Algorithm 1
no longer eﬀects which CI tests are performed for other pairs of variables with conditioning
set size l. Indeed, the algorithm only modiﬁes the adjacency sets when it increases the con-
ditioning set size. Colombo and Maathius proved that Algorithm 1 is order-independent.
We review the proof here, since it is informative:

Proposition 5 (Colombo and Maathuis, 2014). The skeleton resulting from Algorithm 1
is order-independent.

(a)

(b)

(c)

A

B

A

B

A

B

C

E

D

C

E

D

Figure 4: An example of a situation when PC infers diﬀerent skeletons due to a Type II
error and two variable orderings.
(a) The true causal graph, (b) the skeleton
inferred by PC from order1(X), (c) the skeleton inferred by PC from order2(X).

C

E

D

22

PC with p-values

Proof Consider the removal or retention of some undirected edge A−B at some condition-
ing set size l. The ordering of the variables determines the order in which the edges (line 9)
and subsets S ⊆ a(A) and S ⊆ a(B) (line 11) are considered. However, by construction,
the order in which the edges are considered does not aﬀect the sets a(A) and a(B).

If there is at least one subset S of a(A) or a(B) such that A ⊥⊥ B|S, then any ordering
of the variables will ﬁnd a separating set for A and B (but diﬀerent orderings may lead
to diﬀerent separating sets as illustrated in Example 2 of (Colombo and Maathuis, 2014)).
Conversely, if there is no subset S(cid:48) of a(A) or a(B) such that A ⊥⊥ B|S(cid:48), then no ordering
will ﬁnd a separating set.

Hence, any ordering of the variables leads to the same edge deletions and therefore to

the same skeleton.

In other words, modifying the adjacency sets only when changing the conditioning set
size prevents PC-p from skipping some CI tests during skeleton discovery because of Type II
errors and variable ordering. As a result, Algorithm 1 enables PC-p to perform more of the
required CI tests than Algorithm 5 in order to correctly upper bound the p-value of (12).
However, notice that Algorithm 1 does not prevent all Type II errors from eﬀecting the
skeleton. The edge C − D is for example eliminated in Figure 4 regardless of the ordering
because of the erroneous conclusion that C ⊥⊥ D|{A, E}. As a result, we have C (cid:54)∈ (cid:99)N (D)
which may lead to under-estimation of the p-value bounds for undirected edges connected
to D. We will nonetheless see in Section 6 that Algorithm 1 does help PC-p achieve tighter
estimation and control of the FDR than the original skeleton discovery procedure, since
Algorithm 1 eliminates the inﬂuence of at least some Type II errors.

5.2 Unshielded V-Structures

We now describe Algorithm 2, where we use the circle edge endpoint “◦” as a meta-symbol
representing either a tail or an arrowhead. In Algorithm 2, PC-p orients edges according
to all unshielded v-structures in line 3, even if two v-structures conﬂict with each other in

(a)

B

(b)

B

(c)

B

A

C

A

C

A

C

D

E

D

E

D

E

Figure 5: Example of how Algorithm 2 deals with conﬂicting edge orientations. a) The
ground truth, b) the inferred graph with two v-structures A → B ← C and
D → C ← B that lead to the bi-directed edge B ↔ C, and c) the ﬁnal graph
after unorienting both v-structures.

23

Strobl, Spirtes and Visweswaran

the direction of a particular edge. In the case of conﬂict, PC-p admits a bidirected edge
instead of favoring one particular direction over the other. The algorithm then unorients all
v-structures involving the bidirected edges and labels the unoriented edges as “ambiguous”
in line 23 because bidirected edges may result from a Type II error. For example, consider
the ground truth in Figure 5a and assume that Algorithm 1 correctly discovers all of the
undirected edges. Moreover, assume Algorithm 1 correctly ﬁnds a separating set of B
and D that does not contain C but incorrectly ﬁnds a separating set of A and C that
does not contain B. The latter is a Type II error, since the alternative should have been
accepted rather than rejected when conditioning on a subset not containing B.
In this
case, PC-p ﬁrst orients the edges according to Figure 5b. However, notice that the two
unshielded v-structures conﬂict with each other due to the bidirected edge B ↔ C, and
PC-p cannot determine which v-structure admitted the Type II error. As a result, the
algorithm unorients all of the edges in both v-structures as in Figure 5c. PC-p then labels
the three unoriented edges as “ambiguous” so that the algorithm does not orient any other
undirected edges based on these three edges using the orientation rules. The labeling thus
prevents the algorithm from propagating Type II errors by orienting additional edges based
on the erroneous directions.

5.3 Orientation Rules

Notice that Algorithm 7 uses “else if” statements instead of all “if” statements. The “else
if” approach is of course faster, but it also causes PC to ignore any interactions between the
orientation rules in the sense that, if one rule orients an edge, then no other rule can orient
an edge. PC-p performs the orientation rules according Algorithm 3 which uses the “if”
approach to attempt to apply all three orientation rules to each non-ambiguous undirected
edge. Now, if bidirected edges exist after the rules are applied, then Algorithm 3 unorients
the edge as well as all edges involved in the suﬃcient conditions of the associated oientation
rules in lines 16-18. The algorithm then labels the unoriented edges as “ambiguous” in line
19 similar to unshielded v-structure orientation in Section 5.2. For example, in Figure 6,
rule 1 of PC-p induces a bidirected edge between A − B, so PC-p unorients and labels all
directed edges which satisfy the suﬃcient conditions of rule 1 as ambiguous; these include
D → A, C → A, E → B, and F → B.

5.4 Analysis of PC-p

We now have the following analysis of the PC-p algorithm:

Figure 6: Here, a bidirected edge between A and B results from the application of rule 1.
PC-p therefore unorients and labels all edges in the above graph as “ambiguous”
according to the suﬃcient conditions of rule 1.

D

A

E

C

B

F

24

PC with p-values

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

Data: X n, α
Result: (cid:98)G, P 1, S, I

1 Form a completely connected undirected graph (cid:98)G on the variable set in X n
2 l = −1
3 repeat
4

l = l + 1
for each variable A in (cid:98)G do

a(A) ← (cid:99)N (A)

end
repeat

Select a new ordered pair of variables (A, B) that are adjacent in (cid:98)G and
satisfy |a(A) \ B| ≥ l
repeat

Choose a new set S ⊆ {a(A) \ B} with |S| = l
p ← p-value from testA⊥⊥B|S
if p ≤ α then

Insert p into P 1

AB and P 1

BA

Delete A − B from (cid:98)G
Empty P 1
AB and P 1
BA
Insert S into SAB and SBA

else

end

until A − B is deleted from (cid:98)G or all S ⊆ {a(A) \ B} with |S| = l have been
considered ;

until all ordered pairs of adjacent variables (A, B) in (cid:98)G with |a(A) \ B| ≥ l have
been considered ;

22 until all ordered pairs of adjacent variables (A, B) in (cid:98)G satisfy |a(A) \ B| ≤ l;
23 for each nonempty P 1
AB}
24

P 1
Place the same unique identiﬁer for A − B into IAB and IBA

AB ← max{P 1

AB in P 1 do

25
26 end

Algorithm 1: Skeleton Discovery

25

Strobl, Spirtes and Visweswaran

Data: X n, (cid:98)G, P 1, S, I
Result: (cid:98)G, P 2, I

1 for all ordered pairs of non-adjacent variables (A, B) with common neighbor C do
2

if C (cid:54)∈ any set in SAB then

Replace A ◦−◦ C ◦−◦ B with A ◦→ C ←◦ B
l = 0
repeat

l = l + 1
repeat

Choose a new set S ⊆ (cid:99)N (A) including C or S ⊆ (cid:99)N (B) including C
with |S| = l
p ← p-value from testA⊥⊥B|S
Insert p into P (cid:48)(cid:48)

until all S ⊆ (cid:99)N (A) including C and all S ⊆ (cid:99)N (B) including C with
|S| = l have been considered ;

until all S ⊆ (cid:99)N (A) including C and all S ⊆ (cid:99)N (B) including C satisfy
|S| ≤ l ;
Insert max{P 1
Insert max{P 1
Empty P (cid:48)(cid:48)

AC, P (cid:48)(cid:48)} into P (cid:48)
BC, P (cid:48)(cid:48)} into P (cid:48)

BC

AC

Unorient the edge to A − C in (cid:98)G(cid:48)
For each additional edge directed to A in (cid:98)G, unorient the edge in (cid:98)G(cid:48)
For each additional edge directed to C in (cid:98)G, unorient the edge in (cid:98)G(cid:48)
Label the unoriented edges as “ambiguous” in (cid:98)G(cid:48)

end

16
17 end
18 (cid:98)G(cid:48) ← (cid:98)G
19 for each A ↔ C in (cid:98)G do
20

23
24 end
25 (cid:98)G ← (cid:98)G(cid:48)
26 for each A → C in (cid:98)G do
AC ← max
27

P 2
if one p-value in P (cid:48)

P 1

(cid:110)

28

(cid:111)

AC, sum[P (cid:48)
AC then

AC]

Place the same unique identiﬁer into IAC and IBC for unshielded collider
A → C ← B

else if more than one p-value in P (cid:48)
Place a unique identiﬁer into IAC

AC then

end

32
33 end

Algorithm 2: Unshielded V-structures

3

4

5

6

7

8

9

10

11

12

13

14

15

21

22

29

30

31

26

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

PC with p-values

Data: (cid:98)G, P 1, P 2, I
Result: (cid:98)G, P 2, I

1 repeat

(cid:98)G(cid:48) ← (cid:98)G
if A − B non-ambiguous and ∃i s.t. Ci → A with Ci and B non-adjacent in (cid:98)G
then

Replace A ◦−◦ B in (cid:98)G(cid:48) with A ◦→ B
P (cid:48)

A,B, sum{P 2

A,B ← sum

P (cid:48)

(cid:110)

CiA, ∀i s.t. Ci → A with Ci, B non-adjacent}

(cid:111)

end
if A − B non-ambiguous and ∃i s.t. A → Ci → B in (cid:98)G then

Replace A ◦−◦ B in (cid:98)G(cid:48) with A ◦→ B
(cid:104)
P (cid:48)
AB, sum

AB ← sum

max{P 2

P (cid:48)

(cid:110)

ACi

, P 2

CiB}, ∀i s.t. A → Ci → B

(cid:105)(cid:111)

end
if A − B non-ambiguous and ∃i, j s.t. A − Ci → B, A − Cj → B with A − Ci
and A − Cj non-ambiguous, and Ci and Cj non-adjacent in (cid:98)G then

(cid:110)

Replace A ◦−◦ B in (cid:98)G(cid:48) with A ◦→ B
(cid:104)
, P 2
P (cid:48)
Cj B}, ∀i, j s.t. A − Ci →
AB, sum
AB ← sum
B, A − Cj → B with A − Ci and A − Cj non-ambiguous, and Ci and Cj
non-adjacent

CiB, P 1

max{P 1

, P 2

ACj

ACi

(cid:105)(cid:111)

P (cid:48)

end
for each A ↔ B in (cid:98)G(cid:48) do

Unorient to A − B in (cid:98)G(cid:48)
For each edge in the suﬃcient conditions of each orientation rule that led to
the creation of a directed edge to A in (cid:98)G, unorient the edge in (cid:98)G(cid:48)
For each edge in the suﬃcient conditions of each orientation rule that led to
the creation of a directed edge to B in (cid:98)G, unorient the edge in (cid:98)G(cid:48)
Label the unoriented edges as “ambiguous” in (cid:98)G(cid:48)

end
(cid:98)G ← (cid:98)G(cid:48)
for each A → B in (cid:98)G do

Place a unique identiﬁer into IAB
AB, P (cid:48)
P 2

AB ← max{P 1

AB}

end
Empty P (cid:48)

26
27 until there are no more edges to orient;
28 for each non-empty cell P 1
29
30 end

AB s.t. P 2

BA ← P 1

AB, P 2

P 2

AB

AB and P 2

BA are empty do

Algorithm 3: Orientation Rules

27

Strobl, Spirtes and Visweswaran

Data: (cid:98)G, P 2, I, α, q
Result: (cid:92)F DRBY (α), (cid:98)G∗
// Estimation

of unique identiﬁers in I

// Control

identiﬁers in I

1 (cid:92)F DRBY (α) ← Solution of 2 using threshold α and m corresponding to the number

2 α∗ ← Solution of 3 using FDR level q and m corresponding to the number of unique

3 (cid:98)G∗ ← (cid:98)G with edges associated with p-values above α∗ eliminated

Algorithm 4: FDR Estimation and Control

Theorem 6 The PC-p algorithm with a CI oracle is sound and complete.

Proof PC is sound and complete, so it is enough to prove that PC-p and PC will perform
the exact same edge deletion and edge orientation operations with a CI oracle. Note that
Algorithm 1 has already been shown to be sound and complete up to skeleton discovery
(Colombo & Maathius 2014). Algorithm 1 will therefore perform the exact same edge
deletions as Algorithm 5 with a CI oracle. Now, Algorithm 2 will also perform the same
edge orientations as Algorithm 6 with a CI oracle, since there will never be conﬂicting edge
orientations. Lastly, for Algorithm 3, if there exists an edge that can be oriented by more
than rule, then the edge must be oriented in the same direction by the other two rules.
Algorithm 3 therefore returns the same edge orientations as Algorithm 7. We have proved
equivalence in outputs of Algorithms 1, 2 and 3 of PC-p to Algorithms 5, 6 and 7 of PC,
respectively. Algorithm 4 is not involved in graph structure discovery.

The output of PC-p is therefore equivalent to the output of PC in the large sample limit
with a consistent CI test, even though PC-p performs more operations than PC.

5.5 Computation of the P-Values

We now address the issue of computing the upper bounds of the p-values. Let us ﬁrst
consider Algorithm 1. Algorithm 1 takes as input the dataset X n and the signiﬁcance
threshold α. The algorithm then stores the p-values of all signiﬁcant CI tests in cell P 1

Figure 7: An example of a situation where two of PC’s orientation rules, speciﬁcally rules 2
and 3, can orient one undirected edge (A − B) in the same direction. In this case,
PC oriented all of the currently directed edges using unshielded v-structures.

F

C

D

E

A

B

28

PC with p-values

when it reaches line 14. Notice that the algorithm stores the p-values of all signiﬁcant tests
involving A and B in both P 1
BA. Algorithm 1 next computes the maximum over
the p-values for all surviving edges in line 24 as in (16).

AB and P 1

Algorithm 2 takes P 1 from Algorithm 1 as input. Moreover, unlike Algorithm 6 of PC,
Algorithm 2 also takes as input the dataset X n, since PC-p must apply (29) in order to
obtain the upper bounds of the p-values for oriented unshielded v-structures. Indeed, Algo-
rithm 2 executes testA⊥⊥B|S for all S ⊆ (cid:99)N (A) containing C and all S ⊆ (cid:99)N (B) containing
C in steps 4-12 for each A − C − B such that A and B are non-adjacent and C (cid:54)∈ SAB. Now,
Algorithm 2 ultimately stores all of the p-values needed to compute pγAB|C as in (20) in P (cid:48)(cid:48)
via line 10. Algorithm 2 then stores the maximum over pA−C and pγAB|C in P (cid:48)
BC instead
of P (cid:48)
AC in line 13. A similar set of operations eventually stores the maximum over pB−C
and pγAB|C into P (cid:48)
AC in line 14. Note that multiple elements can enter into P (cid:48)
AC
when multiple v-structures can orient one edge. Finally, in line 27, Algorithm 2 takes the
maximum over P 1
AC to obtain pA→C in
P 2 according to (29) and similarly takes the maximum over P 1
BC to
obtain pB→C in P 2.

AC as returned from Algorithm 1 and the sum of P (cid:48)

BC and the sum of P (cid:48)

BC and P (cid:48)

Algorithm 3 takes P 2 from Algorithm 2 as input. Next, in rule 1, Algorithm 3 adds
up the p-values associated with Ci → A, ∀i and places the result in P (cid:48)
AB in line 5 for
computing (31). Then, Algorithm 3 sums over the maxima of pA→Ci and pCi→B in rule
2 ∀i s.t. A → Ci → B in line 9 for ultimately computing (33). Subsequently, in rule
3, Algorithm 3 ﬁnds all n edges such that A − Ci → B. The algorithm then ﬁnds all of
the n choose 2 pairs, say r of them. For each pair, say A − C1 → B and A − C2 → B,
Algorithm 3 computes pA−C1→B and pA−C2→B as the maximum over pA−C1 and pC1→B
and the maximum over pA−C2 and pC2→B, respectively. Algorithm 3 next sums the p-values
over all r pairs in line 13 for computing (35). Note that Algorithm 3 also takes an outer-
sum involving P (cid:48)
AB in lines 5, 9 and 13 of rules 1, 2 and 3, respectively; these summations
correspond to logical disjunctions when multiple orientation rules can orient one edge in the
same direction. For example, rules 2 and 3 can orient A − B in the same direction in Figure
7. Two applications of rule 1 can also orient A − B in the same direction in Figure 6, if
we remove one of the unshielded v-structures from the graph. Now, for all non-ambiguous
edges, Algorithm 3 then stores the maximum over the p-values from Algorithm 1 and P (cid:48)
into P 2 in line 24. This process is repeated until no more edges can be oriented. Algorithm
3 ﬁnally transfers the p-values of all of the remaining undirected edges in (cid:98)G from P 1 to P 2
in lines 28- 30. The algorithm therefore eventually outputs all of the ﬁnal p-values in P 2 as
desired.

5.6 Controlling the False Discovery Rate

PC-p controls the FDR per hypothesis test as opposed to per edge, since the algorithm
can sometimes orient two edges according to the same hypothesis test during unshielded v-
structure discovery. Indeed, controlling the p-values per edge as opposed to per hypothesis
test can result in overly conservative FDR estimation or control because an FDR estimator
or controlling procedure may count the p-value of one hypothesis test multiple times.

PC-p keeps track of each distinct hypothesis test in Algorithms 1, 2 and 3 by using
indexing cell I as follows. First, Algorithm 1 assigns the same, unique identiﬁer to the p-

29

Strobl, Spirtes and Visweswaran

value bounds in both PAB and PBA in line 25. Next, if we have one v-structure A → C ← B
that orients A − C and C − B, then Algorithm 2 associates both A → C and C ← B with
the same hypothesis test and therefore the same identiﬁer in line 29. On the other hand,
if multiple unshielded v-structures can orient one edge, then Algorithm 2 assigns the edge
a unique identiﬁer in line 31, since a unique hypothesis test exists per edge in this case.
Algorithm 3 ﬁnally assigns a unique identiﬁer to each newly oriented edge in line 23 because
each newly oriented edge also corresponds to a distinct hypothesis test.

We can now use Algorithm 4 to estimate and control the FDR using the identiﬁers in
I and the p-value bounds in P 2 as returned from Algorithm 3. Algorithm 4 estimates
the FDR by solving 2 to obtain (cid:92)F DRBY , where m corresponds to the number of unique
identiﬁers in I. The algorithm subsequently controls the FDR by solving 3 to obtain α∗.
Algorithm 4 then eliminates all edges with p-values below α∗ in P 2 in order to obtain (cid:98)G∗;
this process ensures that the expected FDR does not exceed q in (cid:98)G∗.

5.7 Conclusion

We wrap-up this section with the following theorem:

Theorem 7 Consider the same assumptions as Theorem 4. Then PC-p achieves conser-
vative point estimation and strong control of the FDR across the edges in (cid:98)G.

Proof We have already shown that PC-p can control the p-values of all of the edges in (cid:98)G
from Theorem 4. Estimation follows because the solution of 2 achieves conservative point
estimation of the FDR at threshold α when the p-values are controlled (Benjamini and
Yekutieli, 2001). Similarly, control follows because eliminating the edges associated with
p-values above α∗ as obtained from 3 achieves strong control of the FDR at level q when
the p-values are in turn controlled (Benjamini and Yekutieli, 2001).

The PC-p algorithm thus corresponds to a valid method for estimating and controlling the
FDR in the estimated CPDAG.

Note ﬁnally that PC-p takes slightly longer than original PC to complete because it
performs extra computations. However, PC-p runs at approximately the same speed as
PC-stable, since v-structure detection and orientation rule application take an inﬁnitesimal
amount of time compared to skeleton discovery.

6. Experiments

6.1 Algorithms and Metrics

We evaluated six algorithms:

1. PC-p,

2. PC-p without stabilization in the skeleton discovery procedure,

3. PC-p without ambiguous labelings during v-structure orientation and orientation rule

application (PC-p without ambiguation),

4. PC-p without both stabilization and ambiguation,

30

PC with p-values

5. PC-p without hypothesis tests with robust p-value bounds − we chose the null hy-
potheses to be a series of logical conjunctions so that no edges are present between
any of the variables. As a result, the p-values take on minimal values as described in
Appendix A.2. We call this procedure PC-p without robust p-values.

6. The original PC algorithm with p-value computation − that is, we do not incorporate
stabilization, and the algorithm arbitrarily over-writes edge orientations. We compute
p-values according to the v-structure or rule which ultimately orients each edge in the
CPDAG. The algorithm also performs some additional CI tests in order to compute
20 as described in Section 4.1.

We ran these six algorithms because they are the only algorithms that allow us to compute
the FDR across the entire CPDAG from the estimated p-values.

We assessed the FDR of the above six algorithms in detail using control and estimation
bias3. An algorithm exhibits low control bias at FDR level q when an FDR controlling
procedure can accurately eliminate edges in the CPDAG using the p-values so that the
FDR is in fact q. On the other hand, an algorithm exhibits low estimation bias when an
FDR estimate closely matches the true FDR of the CPDAG. Notice that both control and
estimation bias are important and can serve diﬀerent purposes. As a result, we prefer an
algorithm that exhibits both low control and estimation bias.

We used the mean of the following quantities to assess control bias:

uc((cid:92)F DRBY , q) := max{F DR(α∗) − q, 0},
oc((cid:92)F DRBY , q) := max{q − F DR(α∗), 0},

(36)

where uc((cid:92)F DRBY , q) denotes under-control at FDR level q with the BY FDR estimate,
and oc((cid:92)F DRBY , q) similarly denotes over-control.
In the experiments, we varied q from
[0.001, 0.1] using 100 equispaced intervals. Note that we compute both under-control and
over-control per CPDAG. A method achieves strong control when the mean under-control
taken across the hypothesis tests is zero (Armen and Tsamardinos, 2014). Moreover, the less
the mean over-control, the tighter the strong control. As a result, achieving a lower mean
under-control is more important than achieving a lower mean over-control. We therefore say
that one method outperforms another if the method achieves a lower mean under-control
while also maintaining a reasonably low mean over-control.

We used the mean of the following similar quantities for estimation bias:

ue((cid:92)F DRBY , α) := max{F DR(α) − (cid:92)F DRBY (α), 0},
oe((cid:92)F DRBY , α) := max{(cid:92)F DRBY (α) − F DR(α), 0},

(37)

where ue((cid:92)F DRBY , α) denotes under-estimation at threshold level α with the BY FDR esti-
mate, and oe((cid:92)F DRBY , α) similar denotes over-estimation. We varied the α threshold from
[1E-10, 0.1] with 100 equispaced intervals in the experiments. Now, we say that estimation
is conservative in a α threshold region when the underestimation is zero. Moreover, the

3. We also measured the false negative rate using the structural Hamming distance as a metric in Figure

19 of the Appendix.

31

Strobl, Spirtes and Visweswaran

greater the over-estimation in a p-value threshold region, the more conservative the esti-
mate. A method should conservatively estimate the FDR but not do so over-conservatively.
As a result, achieving lower under-estimation is more important than achieving lower over-
estimation, and one method outperforms another if the method achieves a lower mean
under-estimation while maintaining a reasonably low mean over-estimation.

Below, we report the relative performance diﬀerences of the six algorithms in recovering
the CPDAG at a liberal α threshold of 0.20, since this threshold consistently provided a nice
tradeoﬀ between p-value bound looseness and low Type II error rates. We have reported
the results using other α thresholds of 0.01, 0.05, 0.10, and 0.15 or 0.50 in Figures 12-15 of
Appendix A.3, with similar relative performance diﬀerences between the algorithms. Figures
16-18 in the Appendix also contain results for skeleton discovery, where we compared the
original skeleton discovery procedure of PC against the same procedure with stabilization.
As expected, the stabilization procedure improved performance. We ﬁnally provide results
with the more commonly used structural Hamming distance in 19 of the Appendix; here,
PC-p achieved superior performance by conservatively estimating the graph.

Note that for the simulations in Sections 6.2 and 6.3, we generated the DAGs using the
TETRAD V package (version 5.2.1) by drawing uniformly over all DAGs with a maximum
in-degree of 2 and a maximum out-degree of 2. We then converted each of the DAGs
to linear non-recursive SEM-IEs by 1) drawing the linear coeﬃcients from independent
standard normal distributions, and 2) setting independent Gaussian distributions over the
error terms with standard deviations also drawn from the standard normal. Each linear
SEM-IE with the error distributions therefore induced a multivariate Gaussian distribution
across the observed variables. We ﬁnally ran all of the six algorithms using Fisher’s z-test
with a liberal α threshold of 0.20 and a maximum conditioning set size of 2.

6.2 Low Dimensional Inference

We generated 30 DAGs by drawing uniformly over all DAGs with 20 vertices. We converted
each of the DAGs to 5 linear non-recursive SEM-IEs. We subsequently created 5 datasets
using each linear SEM-IE with sample sizes of 100, 500, 1000, 5000, and 10000. We therefore
created a total of 30 × 5 × 5 = 750 datasets.

We analyzed the ability of the algorithms in correctly estimating the CPDAG in terms
of the four metrics proposed in Section 6.1 as well as the FDR values. Results as averaged
over DAGs, parameters and sample sizes are summarized in Figure 8. We assessed the
signiﬁcance of all inter-algorithm diﬀerences using paired Wilcoxon signed rank tests. PC-p
obtained lower mean FDR values than PC-p without robust p-values (Figures 8a and 8b; z =
-4.782, p = 1.734E-6), PC-p without stabilization (z = -4.371, p = 1.238E-5), PC-p without
ambiguation (z = -4.782, p = 1.734E-6), PC-p without both stabilization and ambiguation
(z = -4.782, p = 1.734E-6), and PC original (z = -4.433, p = 9.316E-6). Moreover, PC-p
achieved signiﬁcantly lower mean under-control than the competing methods (Figures 8c
and 8d; vs. no robust: z = -4.782, p = 1.734E − 6; vs. no stable: z = -3.898, p = 9.711E-5;
vs. no ambig: z = -4.782, p = 1.734E-6; vs. no stable & no ambig: z = −4.782, p =
1.734E-6; vs. PC original: z = -4.700, p = 2.603E-6); meanwhile, PC-p kept the mean
over-control small at 3.865% (SD: 0.795%).

32

PC with p-values

33

Figure 8: Performances of PC-p, PC-p without robust p-values (no robust), PC-p without
ambiguation (no ambig), PC-p without stabilization (no stable), PC-p without
ambiguation and stabilization (no stable & no ambig), and the original PC algo-
rithm (PC) as assessed by (a,b) the FDR, (c,d) control bias, and (e,f) estimation
bias in units of percent. PC-p achieved signiﬁcantly lower FDR, under-estimation
and under-control than the other ﬁve methods suggesting that robust p-values,
stabilization and ambiguation are all important components of PC-p.

Strobl, Spirtes and Visweswaran

Figure 9: Same setup as Figure 8 except with high dimensional data. PC-p signiﬁcantly
outperformed all other methods except PC-p without stabilization in terms of
under-control and under-estimation.

Results for estimation were similar. PC-p achieved signiﬁcantly lower mean under-
estimation than the competing methods (Figures 8e and 8f; vs. no robust: z = -4.782, p =
1.734E-6; vs. no stable: z = -4.206, p = 2.597E-5; vs. no ambig: z = −4.782, p = 1.734E-6;
vs. no stable & no ambig: z = -4.782, p = 1.734E-6; vs. PC original: z = -4.186, p =
2.843E-5). PC-p also achieved a small degree of mean over-estimation (8.222%, SD: 0.400%).
We conclude that robust p-values, stabilization, and ambiguation all help PC-p achieve the
lowest under-control and under-estimation.

6.3 High Dimensional Inference

We next tested PC-p and the other ﬁve algorithms on high dimensional graph estimation. To
do this, we generated thirty 100, thirty 200 and thirty 300 variable DAGs. We subsequently
converted each of the DAGs to one linear non-recursive SEM-IE. Finally, we generated 1000
samples from each SEM-IE in order to obtain sample size to variable ratios of 10, 5 and
3.333.

Results are summarized using the FDR, control bias, and estimation bias metrics as
averaged over the DAGs and their parameters in Figure 9. PC-p achieved similar results in
low dimensions as it did for high dimensions. Speciﬁcally, PC-p obtained lower mean FDR
values across the same α thresholds than PC-p without robust p-values (Figures 9a and 9b;
z = -4.703, p = 2.563E-6), PC-p without stabilization (z = -2.232, p = 0.026), PC-p without
ambiguation (z = -4.782, p = 1.734E −6), PC-p without both stabilization and ambiguation
(z = -4.782, p = 1.734E-6), and PC original (z = -4.782, p = 1.734E-6). Moreover, PC-p
achieved signiﬁcantly lower mean under-control than four of the ﬁve competing methods

34

PC with p-values

Figure 10: Same setup as Figure 8 except with the CYTO dataset. PC-p signiﬁcantly
outperformed all methods across all metrics except the original PC algorithm in
under-control.

(Figures 9c and 9d; vs. no robust: z = -4.623, p = 3.790E-6; vs. no ambig: z = -4.782, p =
1.734E − 6; vs. no stable & no ambig: z = -4.782, p = 1.734E-6; vs. PC original: z =
-4.782, p = 1.734E-6). PC-p did not outperform PC-p without stabilization at four of the
ﬁve threshold α thresholds tested (0.05 : z = -1.121, p = 0.262; 0.10 : z = 0.504, p = 0.614;
0.20 : z = 0.985, p = 0.324; 0.50 : z = 0.760, p = 0.447); however, PC-p did outperform
PC-p without stabilization at an α threshold of 0.01 (z = -3.692, p = 2.225E-4). Meanwhile,
PC-p kept the mean over-control small at 4.507% (SD: 0.240%).

Results for estimation were again similar. PC-p achieved signiﬁcantly lower mean under-
estimation than the competing methods except PC-p without stabilization (Figures 9e
and 9f; vs. all methods except no stable: z = -4.782, p = 1.734E-6). PC-p did not
outperform PC-p without stabilization at four of the ﬁve threshold α thresholds tested
(0.05 : z = -1.820, p = 0.069; 0.10 : z = 0.175, p = 0.861; 0.20 : z = 0.625, p = 0.532;
0.50 : z = 0.608, p = 0.543); however, PC-p did outperform PC-p without stabilization at
an α threshold of 0.01 (z = -2.293, p = 0.028). PC-p also achieved a small degree of mean
over-control (2.129%, SD: 0.084%).

We conclude that the results for control and estimation bias for high dimensional graph
estimation are similar to the low dimensional case. However, stabilization only increased
performance at lower α thresholds in the high dimensional scenario; we may nonetheless
view this as a desirable property, since a lower α threshold helps the algorithm complete
more quickly.

35

Strobl, Spirtes and Visweswaran

6.4 Real Data: CYTO

We evaluated the six algorithms on the CYTO dataset which contains single cell recordings
of the abundance of 11 phosphoproteins and phospholipids in human primary naive CD4+
T cells using ﬂow cytometry (Sachs et al., 2005). The variables in the dataset and their
causal relationships can be represented as a DAG, where vertices are proteins or lipids
and edges are phosphorylation interactions between the proteins and lipids. We used the
general perturbation samples (i.e., CD3-CD28 and CD3-CD28-ICAM2) as our observational
data; these perturbations are required to activate the phosphorylation pathways. Note
that algorithms typically cannot accurately infer the gold standard solution set using the
observational data alone, as noted by the original authors4. As a result, we created a
silver standard DAG by running LiNGAM as implemented in TETRAD V using default
parameters on the full dataset of 1,755 samples; recall that LiNGAM is a method within a
diﬀerent class of causal discovery algorithms based on functional causal models. We then
ran the six algorithms described in Section 6.1 on 1000 bootstrapped datasets of sample
size 100 using Spearman’s rho to handle the class of non-paranormal distributions.

We have summarized the results in Figure 10. PC-p obtained lower mean FDR across
the same α thresholds than PC-p without robust p-values (z = -12.774, p = 2.282E-37),
PC-p without stabilization (z = −8.402, p = 4.378E-17), PC-p without ambiguation (z =
-24.598, p = 1.343E-133), PC-p without both stabilization and ambiguation (z = -23.714, p =
2.616E-124), and original PC (z = -4.924, p = 8.469E-7). Moreover, PC-p achieved signif-
icantly lower mean under-control than four of the ﬁve competing methods (vs. no ro-
bust: z = -12.601, p = 2.081E-36; vs no stable: z = -4.310, p = 1.631E-5; vs. no ambig:
z = -24.339, p = 7.559E-131; vs. no stable & no ambig: z = -22.893, p = 5.515E-116). PC-p
did not outperform the original PC algorithm in mean under-control (z = 0.827, p = 0.408);
however, PC-p did outperform the original PC algorithm in mean under-estimation (z =
−2.662, p = 0.008). Meanwhile, PC-p kept the mean over-control small at 4.232% (SD:
1.635%). PC-p also outperformed the other four methods in mean under-estimation (vs. no
robust: z = -12.684, p = 7.289E-37; vs. no stable: z = -4.893, p = 9.917E-7; vs. no ambig:
z = -24.411, p = 1.322E-131; vs. no stable & no ambig: z = -23.301, p = 4.333E-120)
while maintaining low mean over-estimation at 1.200% (SD: 0.559%). We conclude that
PC-p outperforms the other methods similar to the results with synthetic data. PC-p only
outperformed PC in 2 of the 3 metrics, however, probably because the LiNGAM solution
is only an estimate of the ground truth.

6.5 Real Data: GDP Dynamics

One way of approximating the underlying DAG involves learning the graph with a large
number of samples. Another way uses time series data, where we know a priori that we
must have contemporaneous causal relations or causal relations directed forward in time.
In this experiment, we strip the time information from the six algorithms, and then identify
the false discoveries when algorithms mistakenly detect a causal relation directed backwards
in time. We used a time series dataset downloaded from the Economic Research Service of
the United States Department of Agriculture containing ten economic indicators per year

4. In general, we do not have gold standard causal graphs for real data, so we must approximate the solution

in some manner.

36

PC with p-values

Figure 11: Similar to Figure 8 except with the GDP dataset as well as over-control and
over-estimation bias values instead of under. PC-p did not achieve lower under-
control and under-estimation than PC, but it did achieve signiﬁcantly lower
mean FDR and over-estimation that PC.

related to GDP among 192 countries5. We speciﬁcally evaluated the algorithms on their
ability to discover causal relations among the indicators within and between 1987, 1988
and 1989, where we treated each country as an i.i.d. sample and used 100 bootstrapped
datasets.

We have summarized the results in Figure 11. PC-p again obtained lower mean FDR val-
ues across the α thresholds than PC-p without robust p-values (z = -4.623, p = 3.784E-6),
PC-p without ambiguation (z = -8.054, p = 4.128E-16), PC-p without both stabilization
and ambiguation (z = -8.135, p = 4.128E-16), and original PC (z = -6.624, p = 3.500E-11).
However, PC-p did not obtain signiﬁcantly lower mean FDR values than PC-p without
stabilization (signed-rank = 70, p = 0.600). Next, PC-p achieved signiﬁcantly lower mean
under-control than three of the ﬁve competing methods including PC-p without robust p-
values (z = -4.374, p = 1.218E-5), PC-p without ambiguation (z = -7.867, p = 3.647E-15),
and PC-p without both stabilization and ambiguation (z = -7.819, p = 5.306E-15). PC-p
did not outperform PC-p without stabilization (signed-ranked = 3, p = 1) as well as the
original PC algorithm (signed-ranked = 7, p = 0.625) in mean under-control; nevertheless,
PC-p did outperform the former in over-control (signed-ranked = 3, p = 0.020) while keep-
ing its own mean over-control low at 4.920% (SD: 0.483%). PC-p also outperformed the
same three methods in mean under-estimation (vs. no robust: z = -4.372, p = 1.229E-5; vs.
no ambig: z = -7.818, p = 5.363E-15; vs. no stable & no ambig: z = -7.770, p = 7.850E-15)
while maintaining low mean over-estimation at 2.032% (SD: 0.281%). PC-p again did
not outperform PC-p without stabilization (signed-rank = 2, p = 0.750) and original

5. Web link: http://www.ers.usda.gov/data/macroeconomics/Data/HistoricalRealGDPValues.xls

37

Strobl, Spirtes and Visweswaran

PC (signed-rank = 7, p = 0.625) in under-estimation, but it outperformed both in over-
estimation (no stable: z = -7.386, p = 1.519E-13; PC original: z = -8.682, p = 3.897E-18).
We conclude that PC-p outperforms most methods in either the under or over-metrics. The
results however are not as clean as the results with the synthetic data because we only have
access to a portion of the ground truth.

7. Conclusion

We developed a new algorithm called PC-p which outputs a causal DAG with p-value bounds
associated with each edge. One can then use the bounds with the BY procedure to achieve
almost strong control and estimation of the FDR. The PC-p algorithm speciﬁcally integrates
the skeleton discovery procedure of PC-stable, edge orientation with ambiguation, and
robust hypothesis tests in order to accurately estimate p-values bounds while maintaining
computational eﬃciency.

The PC-p algorithm represents the ﬁrst global constraint-based method which can re-
cover p-value estimates for every edge of a CPDAG. In our opinion, the algorithm is a
signiﬁcant advancement over previous methods which can only achieve strong control of the
FDR under special conditions. Moreover, PC-p lays a foundation for developing similar
methods which can also recover edge-speciﬁc p-values and achieve strong control of the
FDR for graphs recovered by algorithms such as FCI and CCD. In particular, we suspect
that a combination of the max and union bounds will also be suﬃcient for deriving upper
bounds of the edge-speciﬁc p-values for more sophisticated constraint-based methods. The
proposed approach may therefore represent one the earliest forms of a “causal p-value.”

Now readers may wonder whether PC-p can also use the p-values to control the family-
wise error rate (FWER). The answer is yes, and we recommend using the Benjamini-Holm
step-down procedure as opposed to Hochberg’s step-up procedure to control the FWER
(Hochberg, 1988), since the latter assumes positive dependency among the test statistics.
However, application of an FWER controlling procedure to constraint-based causal discov-
ery requires additional justiﬁcation, since most investigators do not use constraint-based
methods to deﬁnitively conclude causal relationships but rather to screen for potential
causal variables. With the screening goal in mind, the FWER may be too conservative in
practice, since it controls the rate of making a single Type I error across all of the hypothesis
tests as opposed to controlling the proportion of Type I errors.

In summary, we introduced an algorithm called PC-p which outputs a causal DAG along
with edge-speciﬁc p-value bounds. One can then use the BY procedure with the bounds to
achieve almost strong control or estimation of the FDR and therefore assess the algorithm’s
conﬁdence in each edge in a principled manner. We ultimately hope that this work will
encourage more applications of constraint-based causal discovery to important problems in
science.

Acknowledgments

Research reported in this publication was supported by grant U54HG008540 awarded by
the National Human Genome Research Institute through funds provided by the trans-

38

PC with p-values

NIH Big Data to Knowledge initiative. The research was also supported by the National
Library of Medicine of the National Institutes of Health under award numbers T15LM007059
and R01LM012095. The content is solely the responsibility of the authors and does not
necessarily represent the oﬃcial views of the National Institutes of Health.

39

Strobl, Spirtes and Visweswaran

Appendix A. Appendix

A.1 PC Algorithm Pseudocode

We provide pseudocode for the original PC algorithm. We summarize skeleton discovery in
Algorithm 5, unshielded v-structure discovery in Algorithm 6, and orientation rule applica-
tion in Algorithm 7.

Data: X n, α
Result: (cid:98)G, S

l = l + 1
repeat

1 Form a completely connected undirected graph (cid:98)G on the vertex set X
2 l = −1
3 repeat
4

Select a new ordered pair of variables (A, B) that are adjacent in (cid:98)G s.t.
|N (A) \ B| ≥ l
repeat

Choose a new set S ⊆ {N (A) \ B} with |S| = l using order(X)
p ← p-value from testA⊥⊥B|S
if p > α then

Delete A − B from (cid:98)G
Insert S into SA,B and SB,A

end

until A − B is deleted from (cid:98)G or all S ⊆ {N (A) \ B} with |S| = l have been
considered ;

until all ordered pairs of adjacent variables (A, B) in (cid:98)G with | (cid:99)N (A) \ B| ≥ l have
been considered ;

16 until all ordered pairs of adjacent variables (A, B) in (cid:98)G satisfy | (cid:99)N (A) \ B| ≤ l;

Algorithm 5: Skeleton Discovery

5

6

7

8

9

10

11

12

13

14

15

Data: (cid:98)G, S
Result: (cid:98)G

end

4
5 end

1 for all ordered pairs of non-adjacent variables (A, B) with common neighbor C do
2

if C (cid:54)∈ any set in Si,j then

3

Replace A − C − B with A → C ← B

Algorithm 6: Unshielded V-structures

40

PC with p-values

Data: (cid:98)G
Result: (cid:98)G

1 repeat
2

3

4

5

6

7

if A − B and ∃C s.t. C → A, and C and B are non-adjacent then

Replace A − B with A → B

else if A − B and ∃C s.t. A → C → B then

Replace A − B with A → B

else if A − B and ∃B, D s.t. A − C → B, A − D → B, and C and D are
non-adjacent then

Replace A − B with A → B

end

8
9 until there are no more edges to orient;

Algorithm 7: Orientation Rules

A.2 Hypothesis Tests with Less Robust Bounds

We claimed to propose edge-speciﬁc hypothesis tests whose bounds are robust to Type II
errors in Section 4.4. We now explain our rationale.

Consider the following modiﬁcation to (10), where we have replaced the null with a

series of logical conjunctions:

H0 :

oracle i outputs ¬Pi,

H1 :

oracle i outputs Pi.

m
(cid:94)

i=1
m
(cid:94)

i=1

m
(cid:94)

i=1

We can bound the Type I error rate of the above hypothesis test as follows:

Pr(Type I error) = Pr(

test i outputs Pi|H0)

Pr(test i outputs Pi|H0)

Pr(test i outputs Pi|oracle i outputs ¬Pi )

≤ min

i=1,...,m

= min

i=1,...,m

= min

i=1,...,m

gi,

We can use the above bound with the following variant of (26) for unshielded v-

structures:

The above hypothesis test follows from the following natural hypothesis test:

H0 : ¬(A − C) ∧ ¬(B − C) ∧ ¬γAB|C,
H1 : (A − C) ∧ (B − C) ∧ γAB|C,

H0 : All edges between A, C, B are absent,
H1 : (A − C) ∧ (B − C) ∧ γAB|C,

41

(38)

(39)

(40)

(41)

Strobl, Spirtes and Visweswaran

since the null of (41) implies the null in (40). From (39), the Type I error rate of (40) is
bounded by min{pA−C, pB−C, pγAB|C }. This is a less robust bound than (26) in terms of
the Type II error rate, since failing to control one p-value can cause an algorithm to under-
estimate the bound. For example, suppose the underlying truth corresponds to pA−C = 0.01,
pB−C = 0.03, pγAB|C = 0.02. Thus, the Type I error rate of (41) is truly bounded by 0.01.
However, suppose a Type II error causes PC-p to skip CI tests and therefore compute
pA−C = 0.01, pB−C = 0.03, pγAB|C = 0.003, where the third term is under-estimated. Then,
PC-p will under-estimate the bound at 0.003 instead of the true 0.01.

Note that generalizing (41) to account for multiple possible ways of orienting a v-

structure does not robustify the bound either, since we have:

H0 : All edges between A, C, B1 are absent, and all edges between

A, C, B2 are absent,

(cid:16)(cid:104)

H1 : (A − C) ∧

(B1 − C) ∧ γAB1|C] ∨ [(B2 − C) ∧ γAB2|C

(cid:105)(cid:17)

.

whose Type I error rate is bounded by min

(cid:111)
(cid:110)
.
pA−C, min{pB1−C, pγAB1|C }+min{pB2−C, pγAB2|C }

More broadly, we can consider the following hypothesis test:

H0 :

oracle i, j outputs ¬Pi,j

H1 :

oracle i, j outputs Pi,j

(cid:17)

,

(cid:17)

.

m
(cid:95)

(cid:16) ni(cid:94)

i=1
m
(cid:94)

j=1

(cid:16) ni(cid:94)

i=1

j=1

We bound its Type I error rate as follows:
ni(cid:94)

m
(cid:95)

Pr(Type I error) = Pr(

test i, j outputs Pi,j|H0 )

i=1

j=1

Pr(test i, j outputs Pi,j|H0)

= max

Pr(test i, j outputs Pi,j| oracle i, j outputs ¬Pi,j)

≤ max

i=1,...,m

min
j=1,...,ni

min
j

min
j

i

i

= max

gi,j,

The above bound is less robust to Type II errors than (11), since under-estimating one term
in each group i composed of ni terms can cause PC-p to also under-estimate (44).

A.3 Other Experimental Results

We have summarized the results for the low dimensional, high dimensional and real datasets
across multiple α thresholds in Figures 12, 13, 14 and 15 respectively. Relative diﬀerences in
performance largely remained consistent across the thresholds, since PC-p usually achieved
the lowest mean FDR, under-control and under-estimation values with minimal increases
in mean over-control and over-estimation.

We have also summarized the results for adjacency discovery in Figures 16, 17, and 18,
where we tested whether the skeleton discovery procedure of PC with stabilization could

42

(42)

(43)

(44)

PC with p-values

improve the estimation of the p-value bounds relative to the procedure without stabilization.
Results show that stabilization improves performance across the three metrics particularly
with the low α threshold values of 0.01 and 0.05. Note that we cannot compute the same
ﬁgures for the GDP dataset, since we can only evaluate relative performance levels based
on edge direction in this case.

We have ﬁnally summarized the results using the structural Hamming distance in Figure
19. Notice that ambiguation helps PC-p achieve signiﬁcantly lower Hamming distances
across multiple α thresholds by forcing the algorithm to conservatively orient the edges.
Again, we cannot compute the structural Hamming distances for the GDP dataset for the
aforementioned reason.

References

A. P. Armen and I. Tsamardinos. A uniﬁed approach to estimation and control of the
false discovery rate in bayesian network skeleton identiﬁcation.
In In Proceedings of
the European Symposium on Artiﬁcial Neural Networks, Computational Intelligence and
Machine Learning (ESANN, 2011.

A. P. Armen and I. Tsamardinos. Estimation and control of the false discovery rate of
bayesian network skeleton identiﬁcation. Technical Report TR-441, University of Crete,
2014.

Y. Benjamini and D. Yekutieli. Investigating the importance of self-theories of intelligence
and musicality for students’ academic and musical achievement. Annals of Statistics, 29
(4):1165–1188, 2001.

H. Chong, M. Zey, and D. A. Bessler. On corporate structure, strategy, and performance:
a study with directed acyclic graphs and pc algorithm. Managerial and Decision Infor-
matics, 31:47–62, 2009.

D. Colombo and M. H. Maathuis. Order-independent constraint-based causal structure
learning. J. Mach. Learn. Res., 15(1):3741–3782, January 2014. ISSN 1532-4435. URL
http://dl.acm.org/citation.cfm?id=2627435.2750365.

G. F. Cooper and C. Yoo. Causal discovery from a mixture of experimental and observa-
tional data. In Proceedings of the Fifteenth Conference on Uncertainty in Artiﬁcial Intel-
ligence, UAI’99, pages 116–125, San Francisco, CA, USA, 1999. Morgan Kaufmann Pub-
lishers Inc. ISBN 1-55860-614-9. URL http://dl.acm.org/citation.cfm?id=2073796.
2073810.

N. Friedman, M. Goldszmidt, and A. Wyner. Data analysis with bayesian networks: A
bootstrap approach. In Proceedings of the Fifteenth Conference on Uncertainty in Arti-
ﬁcial Intelligence, UAI’99, pages 196–205, San Francisco, CA, USA, 1999. Morgan Kauf-
mann Publishers Inc. ISBN 1-55860-614-9. URL http://dl.acm.org/citation.cfm?
id=2073796.2073819.

43

Strobl, Spirtes and Visweswaran

Figure 12: Mean FDR, control bias, and estimation bias values across multiple α thresholds

for the low dimensional datasets.

44

PC with p-values

Figure 13: Same as Figure 12 except with high dimensional datasets.

45

Strobl, Spirtes and Visweswaran

Figure 14: Same as Figure 12 except with bootstrapped CYTO datasets.

46

PC with p-values

Figure 15: Same as Figure 12 except with bootstrapped GDP datasets.

47

Strobl, Spirtes and Visweswaran

Figure 16: Mean FDR, control bias, and estimation bias values across multiple α thresholds

for the low dimensional datasets.

48

PC with p-values

Figure 17: Same as Figure 16 except with high dimensional datasets.

49

Strobl, Spirtes and Visweswaran

Figure 18: Same as Figure 16 except with bootstrapped CYTO datasets.

50

PC with p-values

Figure 19: Structural Hamming distances for the a) low dimensional, b) high dimensional
and c) CYTO datasets. The three sub-ﬁgures associate 5 bars with each algo-
rithm; these bars correspond to α thresholds of 0.01, 0.05, 0.1, 0.20 and 0.50 for
the low dimensional and CYTO datasets, and α thresholds of 0.01, 0.05, 0.1,
0.15 and 0.20 for the high dimensional datasets. Error bars represent standard
errors for a) and standard deviations otherwise.

M. J. Ha, V. Baladandayuthapani, and K. A. Do. Prognostic gene signature identiﬁcation
using causal structure learning: applications in kidney cancer. Cancer Inform, 14(Suppl
1):23–35, 2015.

N. Harris and M. Drton. Pc algorithm for nonparanormal graphical models. J. Mach.
ISSN 1532-4435. URL http://dl.acm.

Learn. Res., 14(1):3365–3383, January 2013.
org/citation.cfm?id=2567709.2567770.

Y. Hochberg. A sharper Bonferroni procedure for multiple tests of signiﬁcance. Biometrika,
75(4):800–802, December 1988. ISSN 1464-3510. doi: 10.1093/biomet/75.4.800. URL
http://dx.doi.org/10.1093/biomet/75.4.800.

S. P. Iyer, I. Shafran, D. Grayson, K. Gates, J. T. Nigg, and D. A. Fair. Inferring func-
tional connectivity in MRI using Bayesian network structure learning with a modiﬁed PC
algorithm. Neuroimage, 75:165–175, Jul 2013.

A. A. Joshi, S. H. Joshi, R. M. Leahy, D. W. Shattuck, I. Dinov, and A. W. Toga. Bayesian
approach for network modeling of brain structural features, volume 7626. 2010. ISBN
9780819480279. doi: 10.1117/12.844548.

T. D. Le, L. Liu, A. Tsykin, G. J. Goodall, B. Liu, B. Y. Sun, and J. Li.

Inferring
microRNA-mRNA causal regulatory relationships from expression data. Bioinformatics,
29(6):765–771, Mar 2013.

51

Strobl, Spirtes and Visweswaran

J. Li and Z. J. Wang. Controlling the false discovery rate of the association/causality
structure learned with the pc algorithm. J. Mach. Learn. Res., 10:475–514, June 2009.
ISSN 1532-4435. URL http://dl.acm.org/citation.cfm?id=1577069.1577086.

J. Li, Z. Wang, and M. J. McKeown. Learning brain connectivity with the false-discovery-
rate-controlled PC-algorithm. Conf Proc IEEE Eng Med Biol Soc, 2008:4617–4620, 2008.

J. Listgarten and D. Heckerman. Determining the number of non-spurious arcs in a learned
dag model: Investigation of a bayesian and a frequentist approach. In Ronald Parr and
Linda C. van der Gaag, editors, UAI, pages 251–258. AUAI Press, 2007. ISBN 0-9749039-
3-0. URL http://dblp.uni-trier.de/db/conf/uai/uai2007.html#ListgartenH07.

C. Meek. Strong completeness and faithfulness in bayesian networks. In Proceedings of the
Eleventh Conference on Uncertainty in Artiﬁcial Intelligence, UAI’95, pages 411–418,
San Francisco, CA, USA, 1995. Morgan Kaufmann Publishers Inc. ISBN 1-55860-385-9.
URL http://dl.acm.org/citation.cfm?id=2074158.2074205.

D. Mullensiefen, P. Harrison, F. Caprini, and A. Fancourt. Investigating the importance
of self-theories of intelligence and musicality for students’ academic and musical achieve-
ment. Front Psychol, 6:1702, 2015.

T. Richardson. A discovery algorithm for directed cyclic graphs.

In Proceedings of the
Twelfth International Conference on Uncertainty in Artiﬁcial Intelligence, UAI’96, pages
454–461, San Francisco, CA, USA, 1996. Morgan Kaufmann Publishers Inc.
ISBN 1-
55860-412-X. URL http://dl.acm.org/citation.cfm?id=2074284.2074338.

K. Sachs, O. Perez, D. Pe’er, D. A. Lauﬀenburger, and G. P. Nolan. Causal protein-signaling
networks derived from multiparameter single-cell data. Science, 308(5721):523–529, Apr
2005.

P. Spirtes, C. Meek, and T. Richardson. Causal inference in the presence of latent vari-
ables and selection bias. In Proceedings of the Eleventh Conference on Uncertainty in
Artiﬁcial Intelligence, UAI’95, pages 499–506, San Francisco, CA, USA, 1995. Morgan
Kaufmann Publishers Inc. ISBN 1-55860-385-9. URL http://dl.acm.org/citation.
cfm?id=2074158.2074215.

P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. MIT press,

2nd edition, 2000.

J. Sun, X. Hu, X. Huang, Y. Liu, K. Li, X. Li, J. Han, L. Guo, T. Liu, and J. Zhang.
Inferring consistent functional interaction patterns from natural stimulus FMRI data.
Neuroimage, 61(4):987–999, Jul 2012.

R. Teramoto, C. Saito, and S. Funahashi. Estimating causal eﬀects with a non-paranormal
method for the design of eﬃcient intervention experiments. BMC Bioinformatics, 15:228,
2014.

I. Tsamardinos and L. E. Brown. Bounding the false discovery rate in local bayesian network
learning. In Proceedings of the Twenty-Third AAAI Conference on Artiﬁcial Intelligence,

52

PC with p-values

AAAI 2008, Chicago, Illinois, USA, July 13-17, 2008, pages 1100–1105, 2008. URL
http://www.aaai.org/Library/AAAI/2008/aaai08-174.php.

X. Wu and Y. Ye. Exploring gene causal interactions using an enhanced constraint-based
method. Pattern Recogn., 39(12):2439–2449, December 2006.
ISSN 0031-3203. doi:
10.1016/j.patcog.2006.05.003. URL http://dx.doi.org/10.1016/j.patcog.2006.05.
003.

53

7
1
0
2
 
y
a
M
 
0
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
5
7
9
3
0
.
7
0
6
1
:
v
i
X
r
a

Estimating and Controlling the False Discovery Rate of the
PC Algorithm Using Edge-Speciﬁc P-Values

Eric V. Strobl
Department of Biomedical Informatics
University of Pittsburgh
Pittsburgh, PA 15206, USA

Peter L. Spirtes
Department of Philosophy
Carnegie Mellon University
Pittsburgh, PA 15213, USA

Shyam Visweswaran
Department of Biomedical Informatics
University of Pittsburgh
Pittsburgh, PA 15206, USA

Editor: TBA

evs17@pitt.edu

ps7z@andrew.cmu.edu

shv3@pitt.edu

Abstract

The PC algorithm allows investigators to estimate a complete partially directed acyclic
graph (CPDAG) from a ﬁnite dataset, but few groups have investigated strategies for
estimating and controlling the false discovery rate (FDR) of the edges in the CPDAG. In
this paper, we introduce PC with p-values (PC-p), a fast algorithm which robustly computes
edge-speciﬁc p-values and then estimates and controls the FDR across the edges. PC-p
speciﬁcally uses the p-values returned by many conditional independence (CI) tests to upper
bound the p-values of more complex edge-speciﬁc hypothesis tests. The algorithm then
estimates and controls the FDR using the bounded p-values and the Benjamini-Yekutieli
FDR procedure. Modiﬁcations to the original PC algorithm also help PC-p accurately
compute the upper bounds despite non-zero Type II error rates. Experiments show that
PC-p yields more accurate FDR estimation and control across the edges in a variety of
CPDAGs compared to alternative methods1.
Keywords: PC Algorithm, Causal Inference, False Discovery Rate, Bayesian Network,
Directed Acyclic Graph

1. Introduction

Discovering causal relationships is often much more important than discovering associational
relationships in the sciences. As a result, the research community has been conducting
extensive investigations into causal inference with the hope of developing practically useful
algorithms to speed-up the scientiﬁc process. This research has resulted in a wide range of
high performing algorithms over the years such as PC (Spirtes et al., 2000), FCI (Spirtes
et al., 1995, 2000), and CCD (Richardson, 1996)

1. MATLAB implementation: https://github.com/ericstrobl/PCp/

c(cid:13)2016 Eric V. Strobl and Shyam Visweswaran.

Strobl, Spirtes and Visweswaran

The PC algorithm is currently one of the most popular methods for inferring causation
from observational data. Given an observational dataset, the algorithm outputs a complete
partially directed acyclic graph (CPDAG) which has helped some investigators elucidate
important causal relationships in several domains. For example, the PC algorithm has been
used to discover new causal relationships between genes and brain regions in biology (Wu
and Ye, 2006; Li et al., 2008; Joshi et al., 2010; Sun et al., 2012; Harris and Drton, 2013;
Iyer et al., 2013; Le et al., 2013; Teramoto et al., 2014; Ha et al., 2015). The algorithm has
also been used to discover causal relations between corporate structures and strategies in
economics (Chong et al., 2009) as well as academic and musical achievements in psychology
(Mullensiefen et al., 2015).

The increased use of PC in recent years has nonetheless led to growing concern about
algorithm’s conﬁdence level in each edge of the CPDAG. For example, PC may have more
conﬁdence in the edge A − B but have less conﬁdence in the edge B → C in the subgraph
A−B → C of the CPDAG. Currently, PC alone does not output any edge-speciﬁc measure of
conﬁdence, even though scientists often must report measures of conﬁdence such as p-values
or conﬁdence intervals in their scientiﬁc articles. This incongruency has resulted in the
relatively slow adoption or even avoidance of the PC algorithm in the sciences, despite the
algorithm’s impressive capabilities in causal inference. Clearly then rectifying the problem
by developing an edge-speciﬁc measure of conﬁdence will increase the adoption of PC as
well as hopefully ease the transition of ever-more complex causal inference algorithms into
the scientiﬁc community.

We can of course consider multiple diﬀerent ways of representing the conﬁdence level
in each edge of the CPDAG. However, we choose to pay special attention to the p-value,
since it is by far the most popular notion of conﬁdence in the sciences. Indeed, nearly all
scientists report p-values in modern scientiﬁc reports because they rely on p-values to help
justify their hypotheses. We therefore would ideally like to assign a p-value to each edge in
the CPDAG as in Figure 1a in order to best integrate the algorithm within a well-known
framework. In this paper, we propose such a “causal p-value” in detail.

We however also believe that assigning p-values to each edge is not enough to ease the
transition of PC into mainstream science, since the CPDAG actually contains many edges
and therefore also represents a complicated multiple hypothesis testing problem. Fortu-
nately, the problem of multiple hypothesis testing has a long history, as scientists have
often required the results of multiple hypothesis tests in order to answer complex scien-
tiﬁc questions. Currently, a standard approach to tackling the multiple hypothesis testing
problem involves controlling the proportion of false positives among the rejected null hy-
potheses, or the false discovery rate (FDR), by using an FDR controlling procedure that
takes a desired FDR level q and a set of p-values as input. The procedure then outputs
a corresponding signiﬁcance level α∗ for the set of p-values. An investigator subsequently
rejects the null hypotheses for those tests with p-values that fall below α∗ in order to en-
sure that the expected FDR does not exceed q. For example, consider the set of p-values
{0.02, 0.01, 0.03} and suppose that the FDR controlling procedure with q = 0.1 outputs
α∗ = 0.019. Then, rejecting the null hypotheses of the ﬁrst and third hypothesis tests
guarantees that the expected FDR does not exceed 10%. Several other FDR controlling
strategies also exist, but the ease of use, speed and accuracy of the above method have
made it the most widely adopted strategy in the last two decades.

2

PC with p-values

We would therefore like to control the FDR in the edges of the CPDAG using an FDR
controlling procedure like in Figure 1b. As a ﬁrst idea, one may wonder whether an inves-
tigator can control the FDR in the CPDAG by simply feeding in all of the CI test p-values
computed by PC into an FDR controlling procedure. Unfortunately, this approach fails for
at least two reasons. First, it is unclear how to use the p-values which exceed the α∗ cut-oﬀ
to reject edges in the CPDAG, since one would ﬁrst need to elucidate the correspondence
between the p-values and edges. Second, even if one could solve this problem, the strat-
egy may only loosely bound the expected FDR. An accurate FDR controlling procedure
should instead take into account the speciﬁc computations executed by PC in order to iden-
tify a sharp bound. The p-value based approach therefore necessitates a more ﬁne-grained
strategy which has thus far remained undiscovered.

Several groups have nonetheless attempted to control the FDR in the CPDAG by avoid-
ing the complicated nature of the above problem with a diﬀerent, data re-sampling approach.
For example, Friedman and colleagues proposed to estimate the FDR by using the para-
metric bootstrap (Friedman et al., 1999). This procedure involves ﬁrst learning a causal
graph with the PC algorithm. The procedure then generates data from the causal graph
and re-applies the PC algorithm multiple times on each generated dataset to estimate the
FDR using the learnt causal graphs. An investigator can subsequently control the FDR
by repeating the above process with diﬀerent α values until he or she reaches the desired
FDR level q. However, notice that the method requires multiples calls to PC and can
therefore require too much time with high dimensional data. The procedure also requires
parametric knowledge about the underlying distribution which limits the applicability of
the method to simple cases. Fortuitously, two groups later proposed a permutation-based
method which drops the parametric assumption (Listgarten and Heckerman, 2007; Armen
and Tsamardinos, 2014). The permutation method nevertheless also requires multiple calls

(a)

A

(b)

A

.

.

B

5
2
0.0

C

0
.
1
5
0

B

5
2
0.0

0

0

.

.

0

0

0

0

1

1

E

0.001

D

E

0.001

D

C

α∗ = 0.031

Figure 1: We seek to associate edge-speciﬁc p-values to the output of the PC algorithm
such as in (a). The PC algorithm currently does not associate such p-values with
its output. We would also like to control the FDR of the edges. In (b), we set
the FDR to 0.1 and obtained a α∗ cutoﬀ of 0.031 for the output in (a), so we
eliminated the edge between B and D because its p-value exceeds α∗.

3

Strobl, Spirtes and Visweswaran

to an algorithm and in fact only applies to the parts of PC which can be decomposed
into independent searches for the parents of each vertex; this has thus far limited the ap-
plicability of the method to adjacency discovery with local to global discovery algorithms
(e.g., MMHC) and incomplete edge orientation. We conclude that both the bootstrap and
permutation approaches to FDR estimation and control are either incomplete or too time
consuming.

Another class of methods fortunately attempts to control the FDR without resampling
procedures by instead using a standard FDR controlling procedure with bounded p-values.
For instance, one method proposed in (Tsamardinos and Brown, 2008) and then reﬁned
in (Armen and Tsamardinos, 2011, 2014) assigns a p-value to each adjacency by taking
the maximum over all of the signiﬁcant p-values from the associated CI tests executed
by PC. The method then controls the FDR in the estimated adjacencies by applying an
FDR controlling procedure, such as the one proposed by Benjamini and Yekutieli (BY)
(Benjamini and Yekutieli, 2001), on the edge-speciﬁc p-values. Under faithfulness and a
zero Type II error rate, the method controls the FDR across the estimated adjacencies,
or the estimated skeleton (Armen and Tsamardinos, 2014). This two stage method also
performs comparably with the one stage method proposed in (Li et al., 2008; Li and Wang,
2009), which controls the FDR during, as opposed to after, the execution of the skeleton
discovery phase of the PC algorithm. Of course, the Type II error rate never reaches
zero in practice but researchers have also investigated a strategy for reducing the realized
Type II error rate by introducing a heuristic reliability criterion for CI tests when dealing
with discrete data (Armen and Tsamardinos, 2014). Experiments have shown that these
methods ﬁnish in a relatively short amount of time and perform well in practice. However,
the methods are also incomplete because they only apply to the skeleton discovery phase of
PC.

In this report, we build on the previous outstanding work for deriving p-values for
adjacencies by contributing a sound, complete and fast algorithm called PC with p-values
(PC-p) which appropriately combines the p-values of PC’s CI tests and then uses the BY
FDR controlling procedure to accurately control the FDR in a CPDAG. The method relies
on two upper bounds of the p-value that relate to logical conjunctions and disjunctions as
described in Section 3. These upper bounds allow us to formulate several hypothesis tests
for recovering the skeleton, discovering unshielded v-structures, and orienting additional
edges as presented in Section 4. Accurately estimating the p-values of the hypothesis tests
nonetheless requires a modiﬁed version of PC called PC-p which we propose in Section
5. Finally, we provide experimental results in Section 6 which show that PC-p’s p-value
estimates yield accurate estimates of the FDR with the BY procedure and improve upon
alternative methods.

2. Preliminaries

2.1 Causal graphs

A causal graph consists of vertices representing variables and edges representing causal
relationships between any two variables. In this paper, we will use the terms “vertices” and
“variables” interchangeably. Directed graphs are graphs where two distinct vertices can be
connected by edges “→” and “←.” We only consider simple graphs in this paper, or graphs

4

PC with p-values

with no edges originating from and connecting to the same vertex. Directed acyclic graphs
(DAGs) are directed graphs without directed cycles. We say that X and Y are adjacent
if they are connected by an edge independent of the edge’s direction. A path p from X
to Y is a set of consecutive edges (also independent of their direction) from X to Y such
that no vertex is visited more than once. Given a path between two vertices X and Y
with a middle vertex Z, the path is a chain if X → Y → Z, a fork if X ← Y → Z, and
a v-structure if X → Y ← Z. We refer to Y as a collider, if it is the middle vertex in a
v-structure. A v-structure is called an unshielded v-structure if X → Y ← Z, but X and Z
are non-adjacent. A directed path from X to Y is a set of consecutive edges with direction.
We say that X is an ancestor of Y (and Y is a descendant of X), if there exists a directed
path from X to Y .

If G is a directed graph in which X, Y and Z are disjoint sets of vertices, then X
and Y are d-connected by Z in G if and only if there exists an undirected path p between
some vertex in X and some vertex in Y such that, for every collider C on p, either C or a
descendant of C is in Z, and no non-collider on p is in Z. On the other hand, X and Y
are d-separated by Z in G if and only if they are not d-connected by Z in G. Next, the
joint probability distribution P over variables X satisﬁes the global directed Markov property
for a directed graph G if and only if, for any three disjoint subsets of variables A, B and
C from X, if A and B are d-separated given C in G, then A and B are conditionally
independent given C in P. We refer to the converse of the global directed Markov property
as d-separation faithfulness; that is, if A and B are conditionally independent given C in
P, then A and B are d-separated given C in G.

A Markov equivalence class of DAGs refers to a set of DAGs which entail the same con-
ditional independencies. A complete partially directed acyclic graph (CPDAG) is a partially
directed acyclic graph with the following properties: (1) each directed edge exists in every
DAG in the Markov equivalence class, and (2) there exists a DAG with X → Y and a DAG
with X ← Y in the Markov equivalence class for every undirected edge X − Y . A CPDAG
GC represents a DAG G, if G belongs to the Markov equivalence class described by GC.
We will occasionally use the meta-symbol “◦” at the endpoint(s) of an edge to denote the
presence or absence of an arrowhead. For example, the edge “ −◦ ” may denote either “−”
or “→”.

2.2 The PC Algorithm

The PC algorithm is comprised of three stages. We have summarized these stages as
pseudocode in Algorithms 5, 6 and 7 in Section A.1 of the Appendix. The ﬁrst stage
estimates the adjacencies of G, or the skeleton of G. Starting with a fully connected
skeleton, the algorithm attempts to eliminate the adjacency between any two variables,
say A and B, by testing if A and B are conditionally independent given some subset of the
neighbors of A or the neighbors of B. The search is performed progressively, whereby the
algorithm increases the size of the conditioning set starting from zero using a step size of 1.
The edge between A and B is removed, if A and B are rendered conditionally independent
given some subset of the neighbors of A or the neighbors of B.

The PC algorithm orients unshielded colliders in its second stage. Speciﬁcally, PC ﬁnds
triples A, B, C such that A − B − C, but A and C are non-adjacent. The algorithm

5

Strobl, Spirtes and Visweswaran

then determines whether B is contained in the set which rendered A and C conditionally
independent in the ﬁrst stage of PC. If not, A − B − C is replaced with A → B ← C.

The third and ﬁnal stage of PC involves the repetitive application of three rules to orient

as many of the remaining undirected edges as possible. The three rules include:

1.

If A − B, C → A and C and B are non-adjacent, then replace A − B with

A → B.

2.

3.

If A − B and A → C → B, then replace A − B with A → B.

(1)

If A − B, A − C → B, A − D → B, and C and D are non-adjacent,

then replace A − B with A → B.

Overall, the PC algorithm has been shown to be complete in the sense that it ﬁnds and

then orients edges up to GC, a CPDAG that represents G (Meek, 1995).

2.3 Hypothesis Testing

A hypothesis test is a method of statistical inference usually composed of one null (H0) and
one alternative (H1) hypothesis which are mutually exclusive; that is, if one occurs, then
the other cannot occur. The null hypothesis refers to the default position which asserts that
whatever one is trying to statistically infer actually did not happen. Note that the null and
alternative do not necessarily need to be logical complements of each other. For example,
one may be interested in determining whether the parameter µ is greater than zero. In this
case, the null can be deﬁned as µ = 0 while the alternative can be deﬁned as µ > 0 instead
of µ (cid:54)= 0.

A Type I error is the incorrect rejection of a true null hypothesis, or a false positive.
On the other hand, a Type II error is the failure to reject a false null hypothesis, or a false
negative. The p-value (p) is the probability of the Type I error, or the Type I error rate.
More speciﬁcally, the p-value is the probability of obtaining a result equal to or more extreme
than the observed value under the assumption of the null hypothesis. The null hypothesis
is thus rejected when the p-value is at or below a predeﬁned α threshold (typically the α
threshold is set to 0.05), because a low p-value demonstrates the improbability of the null
hypothesis.

2.4 False Discovery Rate

Multiple comparisons or multiple hypothesis testing refers to the process of considering
more than one statistical inference simultaneously. Failure to compensate for multiple
comparisons can result in erroneous inferences. For example, if an investigator performs
one hypothesis test with an α threshold of 0.05, then he or she has only a 5% chance of
making a Type I error. However, if the investigator performs 100 independent tests with
the same α threshold, then he or she has a 1 − (1 − 0.05)100 = 99.4% chance of making a
Type I error on at least one test.

In multiple hypothesis testing, the false discovery rate (FDR) at threshold α is the
expected proportion of false positives among the rejected null hypotheses. Speciﬁcally, we

6

PC with p-values

deﬁne the FDR at α as follows:

F DR(α) (cid:44) E

(cid:20)

V
max{R, 1}

(cid:21)

,

where V is the number of false positives, R is the total number of null hypotheses rejected,
and max{R, 1} ensures that F DR(α) is well-deﬁned when R = 0. We deﬁne the realized
FDR at α as V / max{R, 1}.

FDR estimation, or conservative point estimation of the FDR, refers to the process of

estimating F DR(α) in a conservative manner such that:

E[(cid:92)F DR(α)] ≥ F DR(α),

(cid:92)F DRBY (α) (cid:44)

mα (cid:80)m
1
i=1
i
max{R, 1}

.

where (cid:92)F DR(α) represents an estimate of F DR(α). We denote E[(cid:92)F DR(α)]−F DR(α) as the
estimation bias. Note that there are several ways of obtaining (cid:92)F DR(α). In 2001, Benjamini
and Yekutieli proposed the following FDR estimator for m hypothesis tests:

(2)

(3)

FDR estimators such as (cid:92)F DRBY can be used to deﬁne FDR controlling procedures. These
procedures determine the optimal threshold α∗ which achieves strong control 2 of the FDR
in the following sense:

α∗ (cid:44) arg max

{(cid:92)F DR(α) ≤ q}

α

The FDR controlling procedure based on (cid:92)F DR involves the rejection of all null hypotheses
with p-values below the α∗ threshold. We refer to the quantity F DR(α∗) − q as the control
bias. Benjamini and Yekutieli proved that the estimate (cid:92)F DRBY in particular achieves
strong control of the FDR with any form of dependence among the p-values of m hypothesis
tests.

3. Upper Bounds on the P-Value

We present two upper bounds of the Type I error rate of hypothesis tests which can be
constructed using a set of simpler hypothesis tests. These upper bounds will serve as useful
tools in Section 4 for bounding the Type I error rate of the hypothesis tests which will be
used to infer the presence or absence of edges in a CPDAG.

3.1 Union Bound

Consider the following hypothesis test for two random variables given a conditioning set:

2. Strong control of the FDR refers the process of controlling the FDR under any conﬁguration of true and
false null hypotheses; on the other hand, weak control refers to the process of controlling the FDR when
all of the null hypotheses are true. Strong control is therefore preferable to weak control.

H0 : Conditionally independent,
H1 : Conditionally dependent.

7

Strobl, Spirtes and Visweswaran

f ((cid:98)si)

−1.96

0

1.96

(cid:98)si

Figure 2: In the above standard normal case, we have Pr(|(cid:98)si| ≥ sα

(cid:12)
(cid:12)si = 0) = 0.05, where
sα
i = 1.96. We reject the null hypothesis when |(cid:98)si| falls in the blue colored regions
at the tails.

i

Trivially, we can rephrase the null and alternative in terms of a conditional independence
(CI) oracle:

H0 : The CI oracle outputs independent,
H1 : The CI oracle outputs dependent.

Now suppose we want to query m CI oracles about m CI relations. We can then consider
the following null and alternative:

H0 : All CI oracles output independent,
H1 : At least one CI oracle outputs dependent.

(4)

i

From here on, we write Pr(CI test i outputs dependent | CI oracle i outputs independent) to
(cid:12)
denote Pr(|(cid:98)si| ≥ sα
(cid:12)si = 0), where si refers to a parameter of some standardized distribution
used by CI test i, (cid:98)si a random variable and the test statistic estimating si, and sα
i a value of
si determined by an α level. We provide an example in Figure 2, where si may correspond
to Fisher’s z-statistic in the case of Fisher’s z-test for the mean parameter si = 0 of the
standard normal distribution.

8

PC with p-values

We now bound the Type I error rate of the hypothesis test (4) by using the new notation

and the union bound:

Pr(Type I error)

= Pr(at least one CI test outputs dependent|H0)

(cid:33)

= Pr

CI test i outputs dependent|H0

Pr(CI test i outputs dependent|H0)

(cid:32) m
(cid:95)

i=1

≤

=

=

m
(cid:88)

i=1
m
(cid:88)

i=1
m
(cid:88)

i=1

pi,

Pr(CI test i outputs dependent|CI oracle i outputs independent)

where pi denotes the Type I error rate of CI test i. Thus, if the r.h.s. of (5) is less than
the α threshold, then we can conclude that the Type I error rate of (4) is also below the
threshold. In other words, (5) is a conservative p-value.

Note that the third equality in the derivation of (5) uses the simplifying assumption
that the probability of the output of CI test i only depends on the output of CI oracle
i when given the outputs of all CI oracles. Several papers have used this assumption
implicitly in their proofs (Tsamardinos and Brown, 2008; Li and Wang, 2009), and we will
also use it throughout this paper. We can justify the assumption based on three facts.
First, most CI test statistics si have a limiting distribution which only depends on si = 0
under the null. For example, Fisher’s z-statistic has a limiting standard normal distribution
with mean parameter zi = 0 and constant variance. Moreover, the G-statistic for the G-
test has a limiting χ2-distribution with non-centrality parameter gi = 0 and degrees of
freedom determined by the number of cells in the contingency table. Second, existing
methods which utilize bounds based on the assumption have strong empirical performance;
loose-enough bounds therefore appear to accommodate the assumption well in most ﬁnite
sample cases. Third, recall that simplifying assumptions are not new in the causality
literature; indeed, many authors have made simplifying assumptions regarding parameter
independence for Bayesian methods which similarly increase computational eﬃciency and
achieve strong empirical performance (e.g., (Cooper and Yoo, 1999)).

We can now also generalize the bound in (5) to any hypothesis test consisting of a series
of logical disjunctions in the alternative and a series of logical conjunctions in the null.
Namely:

(5)

(6)

H0 :

oracle i outputs ¬Pi,

H1 :

oracle i outputs Pi,

m
(cid:94)

i=1
m
(cid:95)

i=1

9

Strobl, Spirtes and Visweswaran

where Pi denotes an arbitrary output of oracle i. We now have:

Pr(Type I error) = Pr(

test i outputs Pi |H0)

m
(cid:95)

i=1

Pr(test i outputs Pi|H0)

Pr(test i outputs Pi|oracle i outputs ¬Pi)

≤

=

=

m
(cid:88)

i=1
m
(cid:88)

i=1
m
(cid:88)

i=1

hi,

where hi is the Type I error rate of test i, and the second equality uses the assumption that
the probability of the output of test i only depends on the output of oracle i when given all
oracles. We will use this generalization in Section 4.

3.2 Intersection Bound

Suppose we want to perform a hypothesis test with the following null and alternative which
are diﬀerent than the null and alternative in (4):

H0 : At least one CI oracle outputs independent,
H1 : All CI oracles output dependent.

Now assume we know that the ith CI oracle outputs independent. We can then bound the
Type I error rate of (7) as follows with m queries to the CI oracle:

Pr(Type I error) = Pr(all m CI tests output dependent|H0)

≤ Pr(CI test i outputs dependent |H0)
= Pr(CI test i outputs dependent | CI oracle i outputs independent

∧ other CI oracles may output independent)

= Pr(CI test i outputs dependent | CI oracle i outputs independent)
= pi,

where the second equality again holds under the assumption that the probability of the
output of CI test i only depends on the output of CI oracle i when given the outputs of all
CI oracles. We can therefore bound the Type I error rate of (7) using the p-value of a single
CI test for which the CI oracle outputs independent. Nevertheless, in practice, we often do
not know for which query the oracle outputs independent in the null. We do however know
that at least one unknown CI oracle i outputs independent, so we can bound the Type I
error rate of (7) using the maximum over all of the m CI test p-values:

Pr(Type I error) = Pr(all m CI tests output dependent|H0)

≤ Pr(CI test i outputs dependent | CI oracle i outputs independent)
= pi ≤ max

pj.

(9)

j=1,...,m

(7)

(8)

10

PC with p-values

H0 :

oracle i outputs ¬Pi,

H1 :

oracle i outputs Pi.

m
(cid:95)

i=1
m
(cid:94)

i=1

m
(cid:94)

i=1

Note that we can generalize the above bound to any hypothesis test consisting of a
series of logical conjunctions in the alternative and a series of logical disjunctions in the
null. Namely:

(10)

(11)

We therefore have:

Pr(Type I error) = Pr(

test i outputs Pi|H0)

≤ Pr(test i outputs Pi|H0)
= Pr(test i outputs Pi|oracle i outputs ¬Pi )
= hi ≤ max

hj,

j=1,...,m

where the second equality again uses the assumption that the probability of the output of
test i only depends on the output of oracle i when given all oracles.

4. Edge-Speciﬁc Hypothesis Tests

We now show how to apply the two upper bounds of the Type I error rate to derive p-value
estimates for both the undirected and directed edges in the CPDAG as estimated by PC.
Bounding the p-value for each edge therefore amounts to adding up and/or maximizing over
the p-values returned from multiple CI tests.

Note that we will sometimes invoke a zero Type II error rate assumption in this section.
This assumption is necessary to correctly upper bound the p-values of the edge-speciﬁc
hypothesis tests of the CPDAG according to the CI tests executed by the PC algorithm. In
fact, we can always correctly bound the p-values, if we perform all of the possible CI tests
between the considered variables; however, this approach is impractical, since it ignores
the eﬃciencies of the PC algorithm. A more interesting strategy involves designing the
edge-speciﬁc hypothesis tests so that the p-value bounds are robust to Type II errors as
well as redesigning the PC algorithm to catch many Type II errors. We will discuss these
approaches in detail in Sections 4.4 and 5, so we encourage readers to accept the zero Type
II error rate assumption for now.

4.1 Skeleton Discovery

We ﬁrst consider the skeleton discovery phase of the PC algorithm. We wish to test whether
each edge is absent in the true skeleton starting from a completely connected undirected
graph. This problem has already been investigated in (Li et al., 2008; Tsamardinos and
Brown, 2008; Li and Wang, 2009; Armen and Tsamardinos, 2011, 2014), but we review it
here for completeness. We construct a hypothesis test with the following null and alterna-

11

Strobl, Spirtes and Visweswaran

(12)

(13)

(14)

(15)

(16)

tive:

H0 : A − B is absent,
H1 : A − B is present.

Now consider the following proposition, where P a(A) denotes the true parents of A:

Proposition 1 (Spirtes et al., 2000) Consider a DAG G which satisﬁes the global directed
Markov property. Moreover, assume that the probability distribution is d-separation faithful.
Then, there is an edge between two vertices A and B if and only if A and B are conditionally
dependent given any subset of P a(A) \ B and any subset of P a(B) \ A.

We thus consider the following two scenarios for the undirected edge A − B:

1.

If A and B are conditionally independent given some subset of P a(A) \ B

or some subset of P a(B) \ A, then A − B is absent.

2.

If A and B are conditionally dependent given any subset of P a(A) \ B

and any subset of P a(B) \ A, then A − B is present.

The following null and alternative are therefore equivalent to (12), where CI oracles are
queried about A and B given all possible subsets of P a(A) \ B and all possible subsets of
P a(B) \ A:

H0 : At least one CI oracle outputs independent,
H1 : All CI oracles output dependent.

Notice that the above hypothesis test is the same as the hypothesis test in (7). We can
therefore bound the p-value of (12) using:

p(cid:48)
A−B

(cid:44) max
i=1,...,q(cid:48)

pA⊥⊥B|Ri,

where Ri ⊆ {Pa(A) \ B} or Ri ⊆ {Pa(B) \ A} and q(cid:48) denotes the total number of such
subsets.

Note that the skeleton discovery phase of the PC algorithm cannot diﬀerentiate between
the parents and children of a particular vertex using its neighbors. However, we can further
bound (15) using the following quantity:

p(cid:48)
A−B ≤ max
i=1,...,q

pA⊥⊥B|Si

(cid:44) pA−B,

where Si ⊆ {N(A) \ B} or Si ⊆ {N(B) \ A} and q denotes the total number of such subsets.
Now assume that the Type II error rate of all CI tests is zero. Then, if the alternative
holds for the CI tests (conditional dependence), then the alternative is accepted. Hence, the
PC algorithm will not remove any of the edges between N (A) and A as well as any of the
edges between N (B) and B. PC therefore performs all necessary CI tests for computing
(16), so upper bounding the Type I error rate for (12) reduces to taking the maximum of
the p-values for all of the CI tests performed by PC regarding A and B. For example,
suppose we measure three random variables A, B and C. Then we obtain p-values after the
PC algorithm tests whether A ⊥⊥ B and A ⊥⊥ B|C. Suppose these p-values are (0.03, 0.04)
so that the PC algorithm with an α threshold of 0.05 determines that A − B is present.
The p-value upper bound of (12) thus corresponds to max {0.03, 0.04} = 0.04.

12

PC with p-values

4.2 Detecting V-Structures

4.2.1 Deterministic Skeleton

The hypothesis testing procedure for directed edges is more complicated than the procedure
for adjacencies. Edges can be oriented in the PC algorithm according to unshielded v-
structures or the orientation rules as described in Section 2.2. Let us ﬁrst focus on the
former and, for further simplicity, let us also assume that 1) we have access to the ground
truth skeleton and 2) no edge is involved in more than one unshielded v-structure (we will
later drop these assumptions in Section 4.2.2). Our task then is to statistically infer the
presence of an unshielded v-structure.

We now present the following null and alternative for each unshielded v-structure after

ﬁnding a triple A − C − B such that A and B are non-adjacent in the skeleton:

H0 : Unshielded A → C ← B is absent,
H1 : Unshielded A → C ← B is present.

(17)

Next, consider the following proposition:

Proposition 2 (Spirtes et al., 2000) Consider the same assumptions as Proposition 1.
Further assume that A, C are adjacent and C, B are adjacent but A, B are non-adjacent.
Then, A and B are conditionally independent given some subset of P a(A)\B which does not
include C or some subset of P a(B)\A which does not include C if and only if A → C ← B.

The following null and alternative is therefore equivalent to (17):

H0 : A and B are conditionally dependent given any subset of P a(A) \ B
which does not include C and any subset of P a(B) \ A which

H1 : A and B are conditionally independent given some subset of P a(A) \ B

which does not include C or some subset of P a(B) \ A which

(18)

does not include C,

does not include C.

The above alternative is reminiscent of the way in which PC determines the presence of an
unshielded v-structure according to Algorithm 6 in the Appendix; speciﬁcally, if C is not in
the set which renders A and B conditionally independent, then C in A − C − B must be a
collider. We however cannot bound the p-value of (18) using CI tests, because conditional
dependence is in the null and conditional independence is in the alternative, as opposed to
vice versa. As a result, we also consider the following proposition:

Proposition 3 Consider the same assumptions as Proposition 2. Then, A and B are
conditionally dependent given any subset of P a(A) \ B containing C and any subset of
P a(B) \ A containing C if and only if A → C ← B.

Proof First notice that P a(A) = {P a(A) \ B} and P a(B) = {P a(B) \ A}, since A and
B are non-adjacent. As a result, we can instead prove that the if and only if statement
holds for P a(A) and P a(B) without loss of generality.

13

Strobl, Spirtes and Visweswaran

For the forward direction, suppose A and B are conditionally dependent given any
subset of P a(A) containing C and any subset of P a(B) containing C. Then A and B are
d-connected given any subset of P a(A) containing C and any subset of P a(B) containing
C by the global directed Markov property. Clearly, C ∈ N (A) and C ∈ N (B), so C must
either be a parent of A and a parent of B, a child of A and a parent of B, a parent of A and
a child B, or a child of A and a child of B. Note that A and B are non-adjacent, so A and
B are d-separated given some subset of P a(A) or some subset of P a(B) by Proposition
1 and d-separation faithfulness. Moreover, the subset must include C if C is a parent of
A and a parent of B, a child of A and a parent of B, or a parent of A and a child B;
otherwise, A and B would be d-connected. As a result, in those three situations, we arrive
at the contradiction that A and B are d-separated given some subset of P a(A) containing
C or some subset of P a(B) containing C. We conclude that C must be a child of A and a
child of B.

For the other direction, if A → C ← B holds, then A and B are d-connected given
any subset of P a(A) containing C and any subset of P a(B) containing C. D-separation
faithfulness then implies that A and B are conditionally dependent given any subset of
P a(A) containing C and any subset of P a(B) containing C.

We can thus equivalently write (18) as:

H0 : A and B are conditionally independent given some subset of P a(A) \ B

containing C or some subset of P a(B) \ A containing C,

H1 : A and B are conditionally dependent given any subset of P a(A) \ B

containing C and any subset of P a(B) \ A containing C.

(19)

We can bound the Type I error rate of the above hypothesis test by taking the maximum
p-value over certain CI tests:

p(cid:48)
γAB|C

(cid:44) max
i=1,..,m(cid:48)

pA⊥⊥B|Mi,

where Mi denotes a subset of P a(A) \ B containing C or a subset of P a(B) \ A containing
C, and m(cid:48) is the total number of subsets Mi. Of course, in practice, we do not know which
vertices are the parents. However, we can also upper bound (19) as follows:

p(cid:48)
γAB|C

≤ max
i=1,..,m

pA⊥⊥B|Ti

(cid:44) pγAB|C ,

(20)

where Ti denotes a subset of N (A) \ B containing C or a subset of N (B) \ A containing
C, and m denotes the total number of subsets Ti. Note that we do not need the zero Type
II error rate assumption for computing (20), since we assume that the skeleton is provided.

4.2.2 Inferred Skeleton

We have considered orienting the colliders, if we have access to the ground truth skeleton.
We now consider the more complex problem of orienting the colliders, if we must also
statistically infer the skeleton.

We again consider the following null and alternative:

H0 : Unshielded A → C ← B is absent,
H1 : Unshielded A → C ← B is present.

(21)

14

PC with p-values

Now, the PC algorithm determines that the alternative holds, if all of the following condi-
tions are true:

1. A and C are conditionally dependent given any subset of P a(A) \ C

2. B and C are conditionally dependent given any subset of P a(B) \ C

and any subset of P a(C) \ A.

and any subset of P a(C) \ B.

3. A and B are conditionally dependent given any subset of P a(A) \ B

containing C and any subset of P a(B) \ A containing C.

We therefore have the following equivalent form of the null and alternative as in (21), if we
assume A and B are non-adjacent:

H0 : At least one condition from (22) does not hold,
H1 : All conditions from (22) hold.

Note that the non-adjacency assumption is reasonable because we did not have enough
Indeed, non-
statistical evidence to invalidate the assumption when we executed (12).
adjacencies are always assumed unless the data suggests that the null of (12) is unlikely.
Now, the alternative of (23) is a series of three logical conjunctions, and the null is a series
of three logical disjunctions as in (10), so the Type I error rate of (23) can be bounded
using the intersection bound:

Pr(Conditions 1, 2, 3 |H0) ≤ Pr(Any one condition |H0)

≤ max{h1, h2, h3}.

We will be using shorthand from here on. We write (23) equivalently as:

H0 : ¬(A − C) ∨ ¬(B − C) ∨ ¬γAB|C,
H1 : (A − C) ∧ (B − C) ∧ γAB|C,

where A − C, B − C, and γAB|C represent Condition 1, 2 and 3 from (22), respectively. We
therefore have a p-value bound of (21) similar to (24):

(cid:16)

Pr

(A − C) ∧ (B − C) ∧ γAB|C|H0
(cid:16)
A − C(cid:12)
(cid:16)
(cid:110)

(cid:12)¬(A − C)
A − C(cid:12)

(cid:12)¬(A − C)

, Pr

Pr

(cid:17)

(cid:17)

(cid:16)

≤ Pr

≤ max

(cid:17)

≤ max{pA−C, pB−C, pγAB|C }.

B − C(cid:12)

(cid:17)

(cid:16)

(cid:12)¬(B − C)

, Pr

γAB|C

(cid:12)
(cid:12)¬γAB|C

(cid:17)(cid:111)

Notice that computing pγAB|C requires N (A) and N (B), not just their respective empirical
estimates (cid:99)N (A) and (cid:99)N (B) which PC can discover. However, we can invoke a zero Type
II error rate assumption in order to ensure that N (A) ⊆ (cid:99)N (A) and N (B) ⊆ (cid:99)N (B) as
explained in detail in Section 4.4, so pγAB|C can still be upper bounded. The assumption

15

(22)

(23)

(24)

(25)

(26)

Strobl, Spirtes and Visweswaran

also ensures that we can upper bound pA−C and pB−C according to Section 4.1. We conclude
that a zero Type II error rate ensures that (26) can be computed.

Next, consider the situation where PC can orient any one edge by using more than one
unshielded v-structure. For example, consider the DAG in Figure 3. In this case, PC can
orient A − C by using either B1 → C or B2 → C (or both); we may therefore want to
take both situations into account. Note that the original PC algorithm always orients an
edge according to one v-structure which it picks arbitrarily according to the ordering of its
computations. We thus only require the bound (26) in this case. However, we will propose
a modiﬁed PC algorithm in Section 5 which takes into account all possible ways to orient
one edge. Now, we can use the following null and alternative for Figure 3 when assuming
that both A and B1 and A and B2 are non-adjacent:

H0 : ¬(A − C) ∨
(cid:16)

H1 : (A − C) ∧

(cid:16)

(cid:17)
[¬(B1 − C) ∨ ¬γAB1|C] ∧ [¬(B2 − C) ∨ ¬γAB2|C]

,

(cid:17)
[(B1 − C) ∧ γAB1|C] ∨ [(B2 − C) ∧ γAB2|C]

.

(27)

We can therefore bound the Type I error rate of (27) as follows, where G = ¬(A − C) and
H = [¬(B1 − C) ∨ ¬γAB1|C] ∧ [¬(B2 − C) ∨ ¬γAB2|C], H1 = ¬(B1 − C) ∨ ¬γAB1|C, and
H2 = ¬(B2 − C) ∨ ¬γAB2|C:

(cid:110)

(cid:110)

(cid:110)

(cid:16)

Pr

(cid:16)

(A − C) ∧
(cid:110)

[(B1 − C) ∧ γAB1|C] ∨ [(B2 − C) ∧ γAB2|C]

(cid:17)

(cid:17)(cid:12)
(cid:12)H0

≤ max

Pr(A − C|G), Pr

(B1 − C) ∧ γAB1|C

≤ max

Pr(A − C|G), Pr

(B1 − C) ∧ γAB1|C|H

(cid:105)

∨
(cid:17)

(cid:104)
(B2 − C) ∧ γAB2|C

(cid:105)(cid:12)
(cid:12)H

(cid:17)(cid:111)

(cid:16)

+ Pr
(cid:17)

(cid:17)(cid:111)

(B2 − C) ∧ γAB2|C|H
(cid:16)

(B2 − C) ∧ γAB2|C|H2

(cid:17)(cid:111)

(28)

(cid:16)(cid:104)

(cid:16)

(cid:16)

= max

Pr(A − C|G), Pr

(cid:16)

(B1 − C) ∧ γAB1|C|H1
(cid:110)
B1 − C(cid:12)
(cid:17)

Pr

(cid:12)¬(B1 − C)

+ Pr
(cid:17)

, Pr(γAB1|C
(cid:111)(cid:111)

≤ max

Pr(A − C|G), max
(cid:110)
B2 − C(cid:12)

(cid:16)

Pr

+ max
(cid:110)
pA−C, max{pB1−C, pγAB1|C } + max{pB2−C, pγAB2|C }

(cid:12)¬(B2 − C)

, Pr(γAB2|C

≤ max

(cid:111)
.

(cid:12)
(cid:12)¬γAB2|C)

(cid:12)
(cid:12)¬γAB1|C)

(cid:111)

Figure 3: Here, one can orient the edge A − C according to the two unshielded v-structures

A → C ← B1 and A → C ← B2.

B1

B2

A

C

16

PC with p-values

More generally, for an arbitrary number, say j, of multiple possible ways to orient A − C
by unshielded v-structures, we have:

(cid:16)

Pr

(A − C) ∧

(cid:16)

(cid:110)

≤ max

pA−C,

i=1

[(B1 − C) ∧ γAB1|C] ∨ ... ∨ [(Bj − C) ∧ γABj |C]
j
(cid:88)

(cid:111)
,
max{pBi−C, pγABi|C }

(cid:17)

(cid:17)(cid:12)
(cid:12)H0

(29)

assuming that B1, . . . , Bj are all non-adjacent to A.

4.3 Orientation Rules

We now consider bounding the p-values of edges which are oriented using the orientation
rules of the PC algorithm. Recall from (1) that the PC algorithm only requires the repeated
application of three orientation rules to be complete. We analyze these three orientation
rules in separate subsections.

4.3.1 First orientation rule

We can construct the hypothesis test for the ﬁrst orientation rule as follows according to
the suﬃcient conditions of the ﬁrst rule in (1):

H0 : ¬(A − B) ∨ ¬(C → A),
H1 : (A − B) ∧ (C → A).

(30)

We again also assume that C and B are non-adjacent. Now the PC algorithm determines
that the alternative holds, if all of the following conditions are true:

1. A − B : A and B are conditionally dependent given any subset of P a(A) \ B and any

subset of P a(B) \ A.

2. C → A : An edge is oriented from C to A under two scenarios. In the ﬁrst, the edge
is oriented because A is the collider in an unshielded v-structure. In the second, the
edge is oriented due to the previous application of an orientation rule.

We thus have a logical conjunction and can bound the Type I error rate using the intersection
bound:

Pr((A − B) ∧ (C → A)|H0) ≤ max{pA−B, pC→A},

where pC→A refers to the p-value bound for the hypothesis test of an unshielded v-structure
or a previously applied orientation rule. Of course, pC→A will be the former when the PC
algorithm begins to execute the orientation rules. More generally, for Ci → A that can
orient A − B where i = 1, ..., j, we have:

(cid:16)

Pr

(A − B) ∧

(cid:105)
(cid:104)
|H0
(C1 → B) ∨ · · · ∨ (Cj → A)

(cid:17)

(cid:110)

≤ max

pA−B,

pCi→A

(cid:111)
,

j
(cid:88)

i=1

(31)

where we require that C1, . . . , Cj are all non-adjacent to B.

17

Strobl, Spirtes and Visweswaran

4.3.2 Second orientation rule

We have the following hypothesis test according to the suﬃcient conditions of the second
rule in (1):

H0 : ¬(A − B) ∨ ¬(A → C → B),
H1 : (A − B) ∧ (A → C → B).

(32)

Hence, by conjunction:

Pr((A − B) ∧ (A → C → B)|H0) ≤ max{pA−B, pA→C→B},

where pA→C→B ≤ max{pA→C, pC→B}. The above Type I error rate can therefore be further
upper bounded by max{pA−B, pA→C, pC→B}. More generally, we have:

Pr

(A − B) ∧

(cid:104)

(cid:105)
(A → C1 → B) ∨ ... ∨ (A → Cj → B)

|H0

(cid:17)

(cid:110)

(cid:111)

(cid:110)

≤ max

pA−B,

pA→Ci→B

≤ max

pA−B,

max{pA→Ci, pCi→B}

(33)

(cid:111)
.

j
(cid:88)

i=1

(cid:16)

j
(cid:88)

i=1

4.3.3 Third orientation rule

We have the following null and alternative by the suﬃcient conditions of the third rule in
(1), assuming that C and D are non-adjacent:

H0 : ¬(A − B) ∨ ¬(A − C → B) ∨ ¬(A − D → B),
H1 : (A − B) ∧ (A − C → B) ∧ (A − D → B).

(34)

We can bound the Type I error rate of the above hypothesis test as follows:

(cid:16)

(cid:110)

Pr

(A − B) ∧ (A − C → B) ∧ (A − D → B)|H0

≤ max{pA−B, pA−C→B, pA−D→B}

≤ max

pA−B, max{pA−C, pC→B}, max{pA−D, pD→B}

(cid:17)

(cid:111)
.

The general case is slightly more complicated than the ﬁrst and second orientation rules.
In this case, we need to control the Type I error rate of accepting at least two paths as
opposed to one. Let the set D include all three-node paths from A to B with the ﬁrst edge
undirected from A to a middle vertex and the second edge directed from the middle vertex
to B such that the ith element of D is:

Di (cid:44) A − Ci → B.

Let us suppose D has a total of n elements and assume that no middle vertex Ci is adjacent
to any other middle vertex. Now, let D(cid:48) be the set containing all of the n choose 2 elements
of D. The ith element in D(cid:48) is therefore:

D(cid:48)
i

(cid:44) {A − Ck → B, A − Cl → B},

18

PC with p-values

(cid:16)

(cid:16)

(cid:104)

r
(cid:88)

i=1

(cid:104)

r
(cid:88)

i=1

where k and l are the distinct indices represented the two chosen middle vertices. Let D(cid:48)
i,1
(cid:1). We then
and D(cid:48)
have:

i,2 be the ﬁrst and second elements in D(cid:48)

i, respectively. Also let r = (cid:0)n

2

Pr

(A − B) ∧

(D(cid:48)

1,1 ∧ D(cid:48)

1,2) ∨ ... ∨ (D(cid:48)

r,1 ∧ D(cid:48)

r,2)

(cid:17)

(cid:105)(cid:12)
(cid:12)H0

(cid:110)

≤ max

pA−B,

Pr(D(cid:48)

i|H0)

(cid:111)
,

where Pr(D(cid:48)

i|H0) (cid:44) p{A−Ck→B,A−Cl→B} and is bounded as follows:

Pr(D(cid:48)

i|H0) ≤ max{pA−Ck→B, pA−Cl→B}

≤ max{pA−Ck , pCk→B, pA−Cl, pCl→B}
(cid:44) ¨Pr(D(cid:48)

i|H0),

We therefore have:

Pr

(A − B) ∧

(D(cid:48)

1,1 ∧ D(cid:48)

1,2) ∨ ... ∨ (D(cid:48)

r,1 ∧ D(cid:48)

r,2)

(cid:17)

(cid:105)(cid:12)
(cid:12)H0

(cid:110)

≤ max

pA−B,

¨Pr(D(cid:48)

i|H0)

(cid:111)
.

(35)

4.4 Summary and Analysis of the Bounds

We derived several bounds for edge orientation as summarized in Table 1. We created the
bounds by engineering speciﬁc hypothesis tests and successively applying the union and
intersection bounds accordingly. Note that j and r are usually very small in sparse graphs.
One may now wonder whether PC can actually control the bounds listed in Table 1 (we
say that a quantity can be controlled, if the quantity can be upper bounded). Recall that we

Table 1: P-value bounds for all of the edge types in a CPDAG. Note that Si ⊆ {N (A) \ B}

or Si ⊆ {N (B) \ A}.

P-Value Bound

Equation Num.

Edge Type

Undirected

Unshielded v-structure max

First orientation rule

Second orientation rule max

Third orientation rule

maxi pA⊥⊥B|Si

(cid:110)
pA−C, (cid:80)j
i=1 max{pBi−C, pγABi|C }
(cid:110)
pA−B, (cid:80)j

i=1 pCi→A

max

(cid:111)

(cid:111)

(cid:110)
pA−B, (cid:80)j
(cid:110)
pA−B, (cid:80)r

max

i=1 max{pA→Ci, pCi→B}

(cid:111)

¨Pr(D(cid:48)

i|H0)

i=1

(cid:111)

(16)

(29)

(31)

(33)

(35)

19

Strobl, Spirtes and Visweswaran

provided a rough, aﬃrmative answer to the question in Sections 4.1 and 4.2.2 by assuming
a zero Type II error rate. We now spell out a more detailed answer via a theorem whose
proof builds on the argument of Theorem 4 in (Armen and Tsamardinos, 2014).

Theorem 4 Suppose that the PC algorithm is applied to a sample from P represented by
DAG G. If we have:

1. P is d-separation faithful to G,

2. The Type II error rate is zero,

3. The PC algorithm also tests whether any two non-adjacent vertices A, B with common
neighbor C are conditionally dependent given any subset of P a(A) \ B containing C
and any subset of P a(B) \ A containing C,

then all of the p-value bounds in Table 1 can be controlled using the p-values of the CI tests
executed by PC.

Proof Consider any two vertices A and B. Algorithm 1 starts with a fully connected
graph, so we have B ∈ (cid:99)N (A) and A ∈ (cid:99)N (B) in the beginning. Note that Algorithm 1
executes testA⊥⊥B|S for all S ⊆ (cid:99)N (A) \ B and for all S ⊆ (cid:99)N (B) \ A. The zero Type II error
rate ensures the following: if the alternative holds, then the alternative is accepted. As a
result, Algorithm 1 will not remove any vertices adjacent to A and any vertices adjacent to
B with a zero Type II error rate. Hence, we always have {N (A) \ B} ⊆ { (cid:99)N (A) \ B} and
{N (B) \ A} ⊆ { (cid:99)N (B) \ A}. Algorithm 1 therefore must eventually execute testA⊥⊥B|S for
all S ⊆ {N (A) \ B} and for all S ⊆ {N (B) \ A}, so (16) can be controlled.

For (29), the p-value bounds for undirected edges can already be controlled by the
previous paragraph. We must now argue that pγAB|C can be controlled. Let C be a collider
between non-adjacent vertices A and B. Now notice that C ∈ N (A) ⊆ (cid:99)N (A) and C ∈
N (B) ⊆ (cid:99)N (B), so Algorithm 2 must execute testA⊥⊥B|S for all S ⊆ {N (A) \ B} containing
C and for all S ⊆ {N (B) \ A} containing C. Hence pγAB|C can be controlled.

Now the p-value bounds (31), (33) and (35) can be controlled trivially because the p-

value bounds for (15) and (29) can be controlled.

In other words, PC can control the bounds in Table 1 with some additional CI tests and a
zero Type II error rate.

Of course, the Type II error rate is never zero in practice, but this becomes less of an
issue as the sample size increases. We may also consider reducing the Type II error rate by
simultaneously implementing three strategies:

1. Use a liberal (higher) α threshold. We for example often use an α threshold of 0.20 in
the experiments. This is the simplest strategy which decreases the Type II error rate
but also increases the Type I error rate. However, we can then control the Type I error
rate post-hoc with an FDR controlling procedure. Of course, setting the α threshold
too high will prevent the PC algorithm from terminating within a reasonable amount
of time as well as loosen the p-value bounds, since the CI tests will fail to explain
away many edges. We therefore cannot rely entirely on this ﬁrst strategy.

20

PC with p-values

2. Use hypothesis tests whose p-value bounds are robust to Type II errors. The hy-
pothesis tests in Section 4.4 are in fact robust to such errors due to the intersection
bound as explained in detail in Appendix A.2. Brieﬂy, we can also reasonably con-
sider modifying the null hypotheses of (25), (30), (32) and (34) to “no edges between
any of the vertices.” This corresponds to converting the logical disjunctions in the
null of (10) into conjunctions which in turns leads to a less robust p-value bound
involving the minimum of a set of p-values instead of the maximum. As a result,
under-estimating one p-value in the p-value set due to Type II error(s) can cause PC
to also under-estimate the bound of (25), (30), (32) or (34).

3. Modify the PC algorithm to prevent and catch many Type II errors.

The last strategy is more complex, so we discuss it in detail in the next section.

5. The PC Algorithm with P-Values

We now propose a modiﬁed PC algorithm called PC with p-values (PC-p) that reduces the
inﬂuence of Type II errors by preventing and catching potential Type II errors. At the same
time, PC-p is correct - the algorithm operates diﬀerently than PC, but it maintains PC’s
desirable soundness and completeness properties.

The PC-p algorithm involves two ideas. First, PC-p performs skeleton discovery with the
same skeleton discovery procedure used in the PC-stable algorithm (Colombo and Maathuis,
2014). This procedure ensures that the algorithm does not skip some CI tests due to Type
II errors and variable ordering. The second idea behind PC-p involves a modiﬁcation to the
procedure for propagating edge orientations. Speciﬁcally, if two edge orientations conﬂict,
PC-p admits bidirected edges instead of over-writing previous orientations like PC. PC-p
then unorients the bidirected edges as well as the directed edges which were directly used to
infer the presence of the bidirected edges. The algorithm subsequently labels the resulting
undirected edges as “ambiguous” which ensures that PC-p does not orient additional edges
using the ambiguous edges. Indeed, the PC-p algorithm uses conﬂicts in edge orientation
to detect potential Type II errors and prevent the propagation of the errors throughout the
graph. In practice, we ﬁnd that these two modiﬁcations to the PC algorithm help PC-p
with the BY estimator achieve more accurate strong estimation and control of the FDR
than PC, as we will see in Section 6.

We now describe the PC-p algorithm in detail; however, we will not describe the com-
putation of the p-value upper bounds until Section 5.5 in order to keep the presentation
clear. We have divided the PC-p algorithm into Algorithms 1, 2, 3 and 4, where the ﬁrst
three procedures correspond to Algorithms 5, 6 and 7 of the original PC algorithm.

5.1 Skeleton Discovery

We ﬁrst consider skeleton discovery. The original PC algorithm uses Algorithm 5 to discover
the skeleton. However, Algorithm 5 can cause the sample version of the PC algorithm to skip
some CI tests due to variable ordering and Type II errors. For example, consider the causal
graph in Figure 4a as ﬁrst presented in (Colombo and Maathuis, 2014). In this example,
suppose the CI tests correctly determine that A ⊥⊥ B and B ⊥⊥ D|{A, C} but incorrectly
determine that C ⊥⊥ D|{A, E}. The incorrect inference is a Type II error, since C and D

21

Strobl, Spirtes and Visweswaran

are adjacent in the true graph. Now consider the following ordering of variables for the PC
algorithm: order1(X) = (A, D, B, C, E). In this case, the ordered pair (D, B) is considered
before (D, C) in Algorithm 5, since (D, B) comes earlier in order1(X). The PC algorithm
removes D − B because a CI test determines that D ⊥⊥ B|{A, C} and {A, C} is a subset of
N (D) = {A, B, C, E}. Next, D − C is considered and erroneously removed because a CI
test determines that D ⊥⊥ C|{A, E} and {A, E} is a subset of N (D) = {A, C, E}. We thus
ultimately obtain the skeleton in Figure 4b with order1(X).

Now consider an alternative ordering of the variables: order2(X) = (A, C, D, B, E). In
this case, (C, D) is considered before (D, B) in Algorithm 5, and the algorithm erroneously
removes C − D. Next, the algorithm considers D − B but {A, C} is not a subset of N (D) =
{A, B, E}, so D − B remains. Even when the PC algorithm eventually also considers the
same undirected edge as B − D, {A, C} is again not a subset of N (B) = {C, D, E}, so
B − D remains. In other words, (C, D) is considered ﬁrst in order2(X) which causes C
to be removed from N (D). Algorithm 5 therefore never executes testB⊥⊥D|{A,C}. We thus
ultimately obtain the skeleton in Figure 4c with order2(X).

The previous two examples show that the Type II error of incorrectly determining that
C ⊥⊥ D|{A, E} leads PC to infer two diﬀerent skeletons due to diﬀerences in variable order-
ing. Clearly, we would like to eliminate the dependency of skeleton discovery on variable
ordering and also reduce its dependency on Type II errors at the same time. Fortunately,
Colombo and Maathius proposed such a modiﬁcation of Algorithm 5 as outlined in Algo-
rithm 1. The key diﬀerence between Algorithm 5 and 1 involves the for loop in steps 5-7 of
Algorithm 1 which computes and stores the adjacency sets after each new conditioning set
size. As a result, an incorrect edge deletion due to a Type II error on line 16 of Algorithm 1
no longer eﬀects which CI tests are performed for other pairs of variables with conditioning
set size l. Indeed, the algorithm only modiﬁes the adjacency sets when it increases the con-
ditioning set size. Colombo and Maathius proved that Algorithm 1 is order-independent.
We review the proof here, since it is informative:

Proposition 5 (Colombo and Maathuis, 2014). The skeleton resulting from Algorithm 1
is order-independent.

(a)

(b)

(c)

A

B

A

B

A

B

C

E

D

C

E

D

Figure 4: An example of a situation when PC infers diﬀerent skeletons due to a Type II
error and two variable orderings.
(a) The true causal graph, (b) the skeleton
inferred by PC from order1(X), (c) the skeleton inferred by PC from order2(X).

C

E

D

22

PC with p-values

Proof Consider the removal or retention of some undirected edge A−B at some condition-
ing set size l. The ordering of the variables determines the order in which the edges (line 9)
and subsets S ⊆ a(A) and S ⊆ a(B) (line 11) are considered. However, by construction,
the order in which the edges are considered does not aﬀect the sets a(A) and a(B).

If there is at least one subset S of a(A) or a(B) such that A ⊥⊥ B|S, then any ordering
of the variables will ﬁnd a separating set for A and B (but diﬀerent orderings may lead
to diﬀerent separating sets as illustrated in Example 2 of (Colombo and Maathuis, 2014)).
Conversely, if there is no subset S(cid:48) of a(A) or a(B) such that A ⊥⊥ B|S(cid:48), then no ordering
will ﬁnd a separating set.

Hence, any ordering of the variables leads to the same edge deletions and therefore to

the same skeleton.

In other words, modifying the adjacency sets only when changing the conditioning set
size prevents PC-p from skipping some CI tests during skeleton discovery because of Type II
errors and variable ordering. As a result, Algorithm 1 enables PC-p to perform more of the
required CI tests than Algorithm 5 in order to correctly upper bound the p-value of (12).
However, notice that Algorithm 1 does not prevent all Type II errors from eﬀecting the
skeleton. The edge C − D is for example eliminated in Figure 4 regardless of the ordering
because of the erroneous conclusion that C ⊥⊥ D|{A, E}. As a result, we have C (cid:54)∈ (cid:99)N (D)
which may lead to under-estimation of the p-value bounds for undirected edges connected
to D. We will nonetheless see in Section 6 that Algorithm 1 does help PC-p achieve tighter
estimation and control of the FDR than the original skeleton discovery procedure, since
Algorithm 1 eliminates the inﬂuence of at least some Type II errors.

5.2 Unshielded V-Structures

We now describe Algorithm 2, where we use the circle edge endpoint “◦” as a meta-symbol
representing either a tail or an arrowhead. In Algorithm 2, PC-p orients edges according
to all unshielded v-structures in line 3, even if two v-structures conﬂict with each other in

(a)

B

(b)

B

(c)

B

A

C

A

C

A

C

D

E

D

E

D

E

Figure 5: Example of how Algorithm 2 deals with conﬂicting edge orientations. a) The
ground truth, b) the inferred graph with two v-structures A → B ← C and
D → C ← B that lead to the bi-directed edge B ↔ C, and c) the ﬁnal graph
after unorienting both v-structures.

23

Strobl, Spirtes and Visweswaran

the direction of a particular edge. In the case of conﬂict, PC-p admits a bidirected edge
instead of favoring one particular direction over the other. The algorithm then unorients all
v-structures involving the bidirected edges and labels the unoriented edges as “ambiguous”
in line 23 because bidirected edges may result from a Type II error. For example, consider
the ground truth in Figure 5a and assume that Algorithm 1 correctly discovers all of the
undirected edges. Moreover, assume Algorithm 1 correctly ﬁnds a separating set of B
and D that does not contain C but incorrectly ﬁnds a separating set of A and C that
does not contain B. The latter is a Type II error, since the alternative should have been
accepted rather than rejected when conditioning on a subset not containing B.
In this
case, PC-p ﬁrst orients the edges according to Figure 5b. However, notice that the two
unshielded v-structures conﬂict with each other due to the bidirected edge B ↔ C, and
PC-p cannot determine which v-structure admitted the Type II error. As a result, the
algorithm unorients all of the edges in both v-structures as in Figure 5c. PC-p then labels
the three unoriented edges as “ambiguous” so that the algorithm does not orient any other
undirected edges based on these three edges using the orientation rules. The labeling thus
prevents the algorithm from propagating Type II errors by orienting additional edges based
on the erroneous directions.

5.3 Orientation Rules

Notice that Algorithm 7 uses “else if” statements instead of all “if” statements. The “else
if” approach is of course faster, but it also causes PC to ignore any interactions between the
orientation rules in the sense that, if one rule orients an edge, then no other rule can orient
an edge. PC-p performs the orientation rules according Algorithm 3 which uses the “if”
approach to attempt to apply all three orientation rules to each non-ambiguous undirected
edge. Now, if bidirected edges exist after the rules are applied, then Algorithm 3 unorients
the edge as well as all edges involved in the suﬃcient conditions of the associated oientation
rules in lines 16-18. The algorithm then labels the unoriented edges as “ambiguous” in line
19 similar to unshielded v-structure orientation in Section 5.2. For example, in Figure 6,
rule 1 of PC-p induces a bidirected edge between A − B, so PC-p unorients and labels all
directed edges which satisfy the suﬃcient conditions of rule 1 as ambiguous; these include
D → A, C → A, E → B, and F → B.

5.4 Analysis of PC-p

We now have the following analysis of the PC-p algorithm:

Figure 6: Here, a bidirected edge between A and B results from the application of rule 1.
PC-p therefore unorients and labels all edges in the above graph as “ambiguous”
according to the suﬃcient conditions of rule 1.

D

A

E

C

B

F

24

PC with p-values

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

Data: X n, α
Result: (cid:98)G, P 1, S, I

1 Form a completely connected undirected graph (cid:98)G on the variable set in X n
2 l = −1
3 repeat
4

l = l + 1
for each variable A in (cid:98)G do

a(A) ← (cid:99)N (A)

end
repeat

Select a new ordered pair of variables (A, B) that are adjacent in (cid:98)G and
satisfy |a(A) \ B| ≥ l
repeat

Choose a new set S ⊆ {a(A) \ B} with |S| = l
p ← p-value from testA⊥⊥B|S
if p ≤ α then

Insert p into P 1

AB and P 1

BA

Delete A − B from (cid:98)G
Empty P 1
AB and P 1
BA
Insert S into SAB and SBA

else

end

until A − B is deleted from (cid:98)G or all S ⊆ {a(A) \ B} with |S| = l have been
considered ;

until all ordered pairs of adjacent variables (A, B) in (cid:98)G with |a(A) \ B| ≥ l have
been considered ;

22 until all ordered pairs of adjacent variables (A, B) in (cid:98)G satisfy |a(A) \ B| ≤ l;
23 for each nonempty P 1
AB}
24

P 1
Place the same unique identiﬁer for A − B into IAB and IBA

AB ← max{P 1

AB in P 1 do

25
26 end

Algorithm 1: Skeleton Discovery

25

Strobl, Spirtes and Visweswaran

Data: X n, (cid:98)G, P 1, S, I
Result: (cid:98)G, P 2, I

1 for all ordered pairs of non-adjacent variables (A, B) with common neighbor C do
2

if C (cid:54)∈ any set in SAB then

Replace A ◦−◦ C ◦−◦ B with A ◦→ C ←◦ B
l = 0
repeat

l = l + 1
repeat

Choose a new set S ⊆ (cid:99)N (A) including C or S ⊆ (cid:99)N (B) including C
with |S| = l
p ← p-value from testA⊥⊥B|S
Insert p into P (cid:48)(cid:48)

until all S ⊆ (cid:99)N (A) including C and all S ⊆ (cid:99)N (B) including C with
|S| = l have been considered ;

until all S ⊆ (cid:99)N (A) including C and all S ⊆ (cid:99)N (B) including C satisfy
|S| ≤ l ;
Insert max{P 1
Insert max{P 1
Empty P (cid:48)(cid:48)

AC, P (cid:48)(cid:48)} into P (cid:48)
BC, P (cid:48)(cid:48)} into P (cid:48)

BC

AC

Unorient the edge to A − C in (cid:98)G(cid:48)
For each additional edge directed to A in (cid:98)G, unorient the edge in (cid:98)G(cid:48)
For each additional edge directed to C in (cid:98)G, unorient the edge in (cid:98)G(cid:48)
Label the unoriented edges as “ambiguous” in (cid:98)G(cid:48)

end

16
17 end
18 (cid:98)G(cid:48) ← (cid:98)G
19 for each A ↔ C in (cid:98)G do
20

23
24 end
25 (cid:98)G ← (cid:98)G(cid:48)
26 for each A → C in (cid:98)G do
AC ← max
27

P 2
if one p-value in P (cid:48)

P 1

(cid:110)

28

(cid:111)

AC, sum[P (cid:48)
AC then

AC]

Place the same unique identiﬁer into IAC and IBC for unshielded collider
A → C ← B

else if more than one p-value in P (cid:48)
Place a unique identiﬁer into IAC

AC then

end

32
33 end

Algorithm 2: Unshielded V-structures

3

4

5

6

7

8

9

10

11

12

13

14

15

21

22

29

30

31

26

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

PC with p-values

Data: (cid:98)G, P 1, P 2, I
Result: (cid:98)G, P 2, I

1 repeat

(cid:98)G(cid:48) ← (cid:98)G
if A − B non-ambiguous and ∃i s.t. Ci → A with Ci and B non-adjacent in (cid:98)G
then

Replace A ◦−◦ B in (cid:98)G(cid:48) with A ◦→ B
P (cid:48)

A,B, sum{P 2

A,B ← sum

P (cid:48)

(cid:110)

CiA, ∀i s.t. Ci → A with Ci, B non-adjacent}

(cid:111)

end
if A − B non-ambiguous and ∃i s.t. A → Ci → B in (cid:98)G then

Replace A ◦−◦ B in (cid:98)G(cid:48) with A ◦→ B
(cid:104)
P (cid:48)
AB, sum

AB ← sum

max{P 2

P (cid:48)

(cid:110)

ACi

, P 2

CiB}, ∀i s.t. A → Ci → B

(cid:105)(cid:111)

end
if A − B non-ambiguous and ∃i, j s.t. A − Ci → B, A − Cj → B with A − Ci
and A − Cj non-ambiguous, and Ci and Cj non-adjacent in (cid:98)G then

(cid:110)

Replace A ◦−◦ B in (cid:98)G(cid:48) with A ◦→ B
(cid:104)
, P 2
P (cid:48)
Cj B}, ∀i, j s.t. A − Ci →
AB, sum
AB ← sum
B, A − Cj → B with A − Ci and A − Cj non-ambiguous, and Ci and Cj
non-adjacent

CiB, P 1

max{P 1

, P 2

ACj

ACi

(cid:105)(cid:111)

P (cid:48)

end
for each A ↔ B in (cid:98)G(cid:48) do

Unorient to A − B in (cid:98)G(cid:48)
For each edge in the suﬃcient conditions of each orientation rule that led to
the creation of a directed edge to A in (cid:98)G, unorient the edge in (cid:98)G(cid:48)
For each edge in the suﬃcient conditions of each orientation rule that led to
the creation of a directed edge to B in (cid:98)G, unorient the edge in (cid:98)G(cid:48)
Label the unoriented edges as “ambiguous” in (cid:98)G(cid:48)

end
(cid:98)G ← (cid:98)G(cid:48)
for each A → B in (cid:98)G do

Place a unique identiﬁer into IAB
AB, P (cid:48)
P 2

AB ← max{P 1

AB}

end
Empty P (cid:48)

26
27 until there are no more edges to orient;
28 for each non-empty cell P 1
29
30 end

AB s.t. P 2

BA ← P 1

AB, P 2

P 2

AB

AB and P 2

BA are empty do

Algorithm 3: Orientation Rules

27

Strobl, Spirtes and Visweswaran

Data: (cid:98)G, P 2, I, α, q
Result: (cid:92)F DRBY (α), (cid:98)G∗
// Estimation

of unique identiﬁers in I

// Control

identiﬁers in I

1 (cid:92)F DRBY (α) ← Solution of 2 using threshold α and m corresponding to the number

2 α∗ ← Solution of 3 using FDR level q and m corresponding to the number of unique

3 (cid:98)G∗ ← (cid:98)G with edges associated with p-values above α∗ eliminated

Algorithm 4: FDR Estimation and Control

Theorem 6 The PC-p algorithm with a CI oracle is sound and complete.

Proof PC is sound and complete, so it is enough to prove that PC-p and PC will perform
the exact same edge deletion and edge orientation operations with a CI oracle. Note that
Algorithm 1 has already been shown to be sound and complete up to skeleton discovery
(Colombo & Maathius 2014). Algorithm 1 will therefore perform the exact same edge
deletions as Algorithm 5 with a CI oracle. Now, Algorithm 2 will also perform the same
edge orientations as Algorithm 6 with a CI oracle, since there will never be conﬂicting edge
orientations. Lastly, for Algorithm 3, if there exists an edge that can be oriented by more
than rule, then the edge must be oriented in the same direction by the other two rules.
Algorithm 3 therefore returns the same edge orientations as Algorithm 7. We have proved
equivalence in outputs of Algorithms 1, 2 and 3 of PC-p to Algorithms 5, 6 and 7 of PC,
respectively. Algorithm 4 is not involved in graph structure discovery.

The output of PC-p is therefore equivalent to the output of PC in the large sample limit
with a consistent CI test, even though PC-p performs more operations than PC.

5.5 Computation of the P-Values

We now address the issue of computing the upper bounds of the p-values. Let us ﬁrst
consider Algorithm 1. Algorithm 1 takes as input the dataset X n and the signiﬁcance
threshold α. The algorithm then stores the p-values of all signiﬁcant CI tests in cell P 1

Figure 7: An example of a situation where two of PC’s orientation rules, speciﬁcally rules 2
and 3, can orient one undirected edge (A − B) in the same direction. In this case,
PC oriented all of the currently directed edges using unshielded v-structures.

F

C

D

E

A

B

28

PC with p-values

when it reaches line 14. Notice that the algorithm stores the p-values of all signiﬁcant tests
involving A and B in both P 1
BA. Algorithm 1 next computes the maximum over
the p-values for all surviving edges in line 24 as in (16).

AB and P 1

Algorithm 2 takes P 1 from Algorithm 1 as input. Moreover, unlike Algorithm 6 of PC,
Algorithm 2 also takes as input the dataset X n, since PC-p must apply (29) in order to
obtain the upper bounds of the p-values for oriented unshielded v-structures. Indeed, Algo-
rithm 2 executes testA⊥⊥B|S for all S ⊆ (cid:99)N (A) containing C and all S ⊆ (cid:99)N (B) containing
C in steps 4-12 for each A − C − B such that A and B are non-adjacent and C (cid:54)∈ SAB. Now,
Algorithm 2 ultimately stores all of the p-values needed to compute pγAB|C as in (20) in P (cid:48)(cid:48)
via line 10. Algorithm 2 then stores the maximum over pA−C and pγAB|C in P (cid:48)
BC instead
of P (cid:48)
AC in line 13. A similar set of operations eventually stores the maximum over pB−C
and pγAB|C into P (cid:48)
AC in line 14. Note that multiple elements can enter into P (cid:48)
AC
when multiple v-structures can orient one edge. Finally, in line 27, Algorithm 2 takes the
maximum over P 1
AC to obtain pA→C in
P 2 according to (29) and similarly takes the maximum over P 1
BC to
obtain pB→C in P 2.

AC as returned from Algorithm 1 and the sum of P (cid:48)

BC and the sum of P (cid:48)

BC and P (cid:48)

Algorithm 3 takes P 2 from Algorithm 2 as input. Next, in rule 1, Algorithm 3 adds
up the p-values associated with Ci → A, ∀i and places the result in P (cid:48)
AB in line 5 for
computing (31). Then, Algorithm 3 sums over the maxima of pA→Ci and pCi→B in rule
2 ∀i s.t. A → Ci → B in line 9 for ultimately computing (33). Subsequently, in rule
3, Algorithm 3 ﬁnds all n edges such that A − Ci → B. The algorithm then ﬁnds all of
the n choose 2 pairs, say r of them. For each pair, say A − C1 → B and A − C2 → B,
Algorithm 3 computes pA−C1→B and pA−C2→B as the maximum over pA−C1 and pC1→B
and the maximum over pA−C2 and pC2→B, respectively. Algorithm 3 next sums the p-values
over all r pairs in line 13 for computing (35). Note that Algorithm 3 also takes an outer-
sum involving P (cid:48)
AB in lines 5, 9 and 13 of rules 1, 2 and 3, respectively; these summations
correspond to logical disjunctions when multiple orientation rules can orient one edge in the
same direction. For example, rules 2 and 3 can orient A − B in the same direction in Figure
7. Two applications of rule 1 can also orient A − B in the same direction in Figure 6, if
we remove one of the unshielded v-structures from the graph. Now, for all non-ambiguous
edges, Algorithm 3 then stores the maximum over the p-values from Algorithm 1 and P (cid:48)
into P 2 in line 24. This process is repeated until no more edges can be oriented. Algorithm
3 ﬁnally transfers the p-values of all of the remaining undirected edges in (cid:98)G from P 1 to P 2
in lines 28- 30. The algorithm therefore eventually outputs all of the ﬁnal p-values in P 2 as
desired.

5.6 Controlling the False Discovery Rate

PC-p controls the FDR per hypothesis test as opposed to per edge, since the algorithm
can sometimes orient two edges according to the same hypothesis test during unshielded v-
structure discovery. Indeed, controlling the p-values per edge as opposed to per hypothesis
test can result in overly conservative FDR estimation or control because an FDR estimator
or controlling procedure may count the p-value of one hypothesis test multiple times.

PC-p keeps track of each distinct hypothesis test in Algorithms 1, 2 and 3 by using
indexing cell I as follows. First, Algorithm 1 assigns the same, unique identiﬁer to the p-

29

Strobl, Spirtes and Visweswaran

value bounds in both PAB and PBA in line 25. Next, if we have one v-structure A → C ← B
that orients A − C and C − B, then Algorithm 2 associates both A → C and C ← B with
the same hypothesis test and therefore the same identiﬁer in line 29. On the other hand,
if multiple unshielded v-structures can orient one edge, then Algorithm 2 assigns the edge
a unique identiﬁer in line 31, since a unique hypothesis test exists per edge in this case.
Algorithm 3 ﬁnally assigns a unique identiﬁer to each newly oriented edge in line 23 because
each newly oriented edge also corresponds to a distinct hypothesis test.

We can now use Algorithm 4 to estimate and control the FDR using the identiﬁers in
I and the p-value bounds in P 2 as returned from Algorithm 3. Algorithm 4 estimates
the FDR by solving 2 to obtain (cid:92)F DRBY , where m corresponds to the number of unique
identiﬁers in I. The algorithm subsequently controls the FDR by solving 3 to obtain α∗.
Algorithm 4 then eliminates all edges with p-values below α∗ in P 2 in order to obtain (cid:98)G∗;
this process ensures that the expected FDR does not exceed q in (cid:98)G∗.

5.7 Conclusion

We wrap-up this section with the following theorem:

Theorem 7 Consider the same assumptions as Theorem 4. Then PC-p achieves conser-
vative point estimation and strong control of the FDR across the edges in (cid:98)G.

Proof We have already shown that PC-p can control the p-values of all of the edges in (cid:98)G
from Theorem 4. Estimation follows because the solution of 2 achieves conservative point
estimation of the FDR at threshold α when the p-values are controlled (Benjamini and
Yekutieli, 2001). Similarly, control follows because eliminating the edges associated with
p-values above α∗ as obtained from 3 achieves strong control of the FDR at level q when
the p-values are in turn controlled (Benjamini and Yekutieli, 2001).

The PC-p algorithm thus corresponds to a valid method for estimating and controlling the
FDR in the estimated CPDAG.

Note ﬁnally that PC-p takes slightly longer than original PC to complete because it
performs extra computations. However, PC-p runs at approximately the same speed as
PC-stable, since v-structure detection and orientation rule application take an inﬁnitesimal
amount of time compared to skeleton discovery.

6. Experiments

6.1 Algorithms and Metrics

We evaluated six algorithms:

1. PC-p,

2. PC-p without stabilization in the skeleton discovery procedure,

3. PC-p without ambiguous labelings during v-structure orientation and orientation rule

application (PC-p without ambiguation),

4. PC-p without both stabilization and ambiguation,

30

PC with p-values

5. PC-p without hypothesis tests with robust p-value bounds − we chose the null hy-
potheses to be a series of logical conjunctions so that no edges are present between
any of the variables. As a result, the p-values take on minimal values as described in
Appendix A.2. We call this procedure PC-p without robust p-values.

6. The original PC algorithm with p-value computation − that is, we do not incorporate
stabilization, and the algorithm arbitrarily over-writes edge orientations. We compute
p-values according to the v-structure or rule which ultimately orients each edge in the
CPDAG. The algorithm also performs some additional CI tests in order to compute
20 as described in Section 4.1.

We ran these six algorithms because they are the only algorithms that allow us to compute
the FDR across the entire CPDAG from the estimated p-values.

We assessed the FDR of the above six algorithms in detail using control and estimation
bias3. An algorithm exhibits low control bias at FDR level q when an FDR controlling
procedure can accurately eliminate edges in the CPDAG using the p-values so that the
FDR is in fact q. On the other hand, an algorithm exhibits low estimation bias when an
FDR estimate closely matches the true FDR of the CPDAG. Notice that both control and
estimation bias are important and can serve diﬀerent purposes. As a result, we prefer an
algorithm that exhibits both low control and estimation bias.

We used the mean of the following quantities to assess control bias:

uc((cid:92)F DRBY , q) := max{F DR(α∗) − q, 0},
oc((cid:92)F DRBY , q) := max{q − F DR(α∗), 0},

(36)

where uc((cid:92)F DRBY , q) denotes under-control at FDR level q with the BY FDR estimate,
and oc((cid:92)F DRBY , q) similarly denotes over-control.
In the experiments, we varied q from
[0.001, 0.1] using 100 equispaced intervals. Note that we compute both under-control and
over-control per CPDAG. A method achieves strong control when the mean under-control
taken across the hypothesis tests is zero (Armen and Tsamardinos, 2014). Moreover, the less
the mean over-control, the tighter the strong control. As a result, achieving a lower mean
under-control is more important than achieving a lower mean over-control. We therefore say
that one method outperforms another if the method achieves a lower mean under-control
while also maintaining a reasonably low mean over-control.

We used the mean of the following similar quantities for estimation bias:

ue((cid:92)F DRBY , α) := max{F DR(α) − (cid:92)F DRBY (α), 0},
oe((cid:92)F DRBY , α) := max{(cid:92)F DRBY (α) − F DR(α), 0},

(37)

where ue((cid:92)F DRBY , α) denotes under-estimation at threshold level α with the BY FDR esti-
mate, and oe((cid:92)F DRBY , α) similar denotes over-estimation. We varied the α threshold from
[1E-10, 0.1] with 100 equispaced intervals in the experiments. Now, we say that estimation
is conservative in a α threshold region when the underestimation is zero. Moreover, the

3. We also measured the false negative rate using the structural Hamming distance as a metric in Figure

19 of the Appendix.

31

Strobl, Spirtes and Visweswaran

greater the over-estimation in a p-value threshold region, the more conservative the esti-
mate. A method should conservatively estimate the FDR but not do so over-conservatively.
As a result, achieving lower under-estimation is more important than achieving lower over-
estimation, and one method outperforms another if the method achieves a lower mean
under-estimation while maintaining a reasonably low mean over-estimation.

Below, we report the relative performance diﬀerences of the six algorithms in recovering
the CPDAG at a liberal α threshold of 0.20, since this threshold consistently provided a nice
tradeoﬀ between p-value bound looseness and low Type II error rates. We have reported
the results using other α thresholds of 0.01, 0.05, 0.10, and 0.15 or 0.50 in Figures 12-15 of
Appendix A.3, with similar relative performance diﬀerences between the algorithms. Figures
16-18 in the Appendix also contain results for skeleton discovery, where we compared the
original skeleton discovery procedure of PC against the same procedure with stabilization.
As expected, the stabilization procedure improved performance. We ﬁnally provide results
with the more commonly used structural Hamming distance in 19 of the Appendix; here,
PC-p achieved superior performance by conservatively estimating the graph.

Note that for the simulations in Sections 6.2 and 6.3, we generated the DAGs using the
TETRAD V package (version 5.2.1) by drawing uniformly over all DAGs with a maximum
in-degree of 2 and a maximum out-degree of 2. We then converted each of the DAGs
to linear non-recursive SEM-IEs by 1) drawing the linear coeﬃcients from independent
standard normal distributions, and 2) setting independent Gaussian distributions over the
error terms with standard deviations also drawn from the standard normal. Each linear
SEM-IE with the error distributions therefore induced a multivariate Gaussian distribution
across the observed variables. We ﬁnally ran all of the six algorithms using Fisher’s z-test
with a liberal α threshold of 0.20 and a maximum conditioning set size of 2.

6.2 Low Dimensional Inference

We generated 30 DAGs by drawing uniformly over all DAGs with 20 vertices. We converted
each of the DAGs to 5 linear non-recursive SEM-IEs. We subsequently created 5 datasets
using each linear SEM-IE with sample sizes of 100, 500, 1000, 5000, and 10000. We therefore
created a total of 30 × 5 × 5 = 750 datasets.

We analyzed the ability of the algorithms in correctly estimating the CPDAG in terms
of the four metrics proposed in Section 6.1 as well as the FDR values. Results as averaged
over DAGs, parameters and sample sizes are summarized in Figure 8. We assessed the
signiﬁcance of all inter-algorithm diﬀerences using paired Wilcoxon signed rank tests. PC-p
obtained lower mean FDR values than PC-p without robust p-values (Figures 8a and 8b; z =
-4.782, p = 1.734E-6), PC-p without stabilization (z = -4.371, p = 1.238E-5), PC-p without
ambiguation (z = -4.782, p = 1.734E-6), PC-p without both stabilization and ambiguation
(z = -4.782, p = 1.734E-6), and PC original (z = -4.433, p = 9.316E-6). Moreover, PC-p
achieved signiﬁcantly lower mean under-control than the competing methods (Figures 8c
and 8d; vs. no robust: z = -4.782, p = 1.734E − 6; vs. no stable: z = -3.898, p = 9.711E-5;
vs. no ambig: z = -4.782, p = 1.734E-6; vs. no stable & no ambig: z = −4.782, p =
1.734E-6; vs. PC original: z = -4.700, p = 2.603E-6); meanwhile, PC-p kept the mean
over-control small at 3.865% (SD: 0.795%).

32

PC with p-values

33

Figure 8: Performances of PC-p, PC-p without robust p-values (no robust), PC-p without
ambiguation (no ambig), PC-p without stabilization (no stable), PC-p without
ambiguation and stabilization (no stable & no ambig), and the original PC algo-
rithm (PC) as assessed by (a,b) the FDR, (c,d) control bias, and (e,f) estimation
bias in units of percent. PC-p achieved signiﬁcantly lower FDR, under-estimation
and under-control than the other ﬁve methods suggesting that robust p-values,
stabilization and ambiguation are all important components of PC-p.

Strobl, Spirtes and Visweswaran

Figure 9: Same setup as Figure 8 except with high dimensional data. PC-p signiﬁcantly
outperformed all other methods except PC-p without stabilization in terms of
under-control and under-estimation.

Results for estimation were similar. PC-p achieved signiﬁcantly lower mean under-
estimation than the competing methods (Figures 8e and 8f; vs. no robust: z = -4.782, p =
1.734E-6; vs. no stable: z = -4.206, p = 2.597E-5; vs. no ambig: z = −4.782, p = 1.734E-6;
vs. no stable & no ambig: z = -4.782, p = 1.734E-6; vs. PC original: z = -4.186, p =
2.843E-5). PC-p also achieved a small degree of mean over-estimation (8.222%, SD: 0.400%).
We conclude that robust p-values, stabilization, and ambiguation all help PC-p achieve the
lowest under-control and under-estimation.

6.3 High Dimensional Inference

We next tested PC-p and the other ﬁve algorithms on high dimensional graph estimation. To
do this, we generated thirty 100, thirty 200 and thirty 300 variable DAGs. We subsequently
converted each of the DAGs to one linear non-recursive SEM-IE. Finally, we generated 1000
samples from each SEM-IE in order to obtain sample size to variable ratios of 10, 5 and
3.333.

Results are summarized using the FDR, control bias, and estimation bias metrics as
averaged over the DAGs and their parameters in Figure 9. PC-p achieved similar results in
low dimensions as it did for high dimensions. Speciﬁcally, PC-p obtained lower mean FDR
values across the same α thresholds than PC-p without robust p-values (Figures 9a and 9b;
z = -4.703, p = 2.563E-6), PC-p without stabilization (z = -2.232, p = 0.026), PC-p without
ambiguation (z = -4.782, p = 1.734E −6), PC-p without both stabilization and ambiguation
(z = -4.782, p = 1.734E-6), and PC original (z = -4.782, p = 1.734E-6). Moreover, PC-p
achieved signiﬁcantly lower mean under-control than four of the ﬁve competing methods

34

PC with p-values

Figure 10: Same setup as Figure 8 except with the CYTO dataset. PC-p signiﬁcantly
outperformed all methods across all metrics except the original PC algorithm in
under-control.

(Figures 9c and 9d; vs. no robust: z = -4.623, p = 3.790E-6; vs. no ambig: z = -4.782, p =
1.734E − 6; vs. no stable & no ambig: z = -4.782, p = 1.734E-6; vs. PC original: z =
-4.782, p = 1.734E-6). PC-p did not outperform PC-p without stabilization at four of the
ﬁve threshold α thresholds tested (0.05 : z = -1.121, p = 0.262; 0.10 : z = 0.504, p = 0.614;
0.20 : z = 0.985, p = 0.324; 0.50 : z = 0.760, p = 0.447); however, PC-p did outperform
PC-p without stabilization at an α threshold of 0.01 (z = -3.692, p = 2.225E-4). Meanwhile,
PC-p kept the mean over-control small at 4.507% (SD: 0.240%).

Results for estimation were again similar. PC-p achieved signiﬁcantly lower mean under-
estimation than the competing methods except PC-p without stabilization (Figures 9e
and 9f; vs. all methods except no stable: z = -4.782, p = 1.734E-6). PC-p did not
outperform PC-p without stabilization at four of the ﬁve threshold α thresholds tested
(0.05 : z = -1.820, p = 0.069; 0.10 : z = 0.175, p = 0.861; 0.20 : z = 0.625, p = 0.532;
0.50 : z = 0.608, p = 0.543); however, PC-p did outperform PC-p without stabilization at
an α threshold of 0.01 (z = -2.293, p = 0.028). PC-p also achieved a small degree of mean
over-control (2.129%, SD: 0.084%).

We conclude that the results for control and estimation bias for high dimensional graph
estimation are similar to the low dimensional case. However, stabilization only increased
performance at lower α thresholds in the high dimensional scenario; we may nonetheless
view this as a desirable property, since a lower α threshold helps the algorithm complete
more quickly.

35

Strobl, Spirtes and Visweswaran

6.4 Real Data: CYTO

We evaluated the six algorithms on the CYTO dataset which contains single cell recordings
of the abundance of 11 phosphoproteins and phospholipids in human primary naive CD4+
T cells using ﬂow cytometry (Sachs et al., 2005). The variables in the dataset and their
causal relationships can be represented as a DAG, where vertices are proteins or lipids
and edges are phosphorylation interactions between the proteins and lipids. We used the
general perturbation samples (i.e., CD3-CD28 and CD3-CD28-ICAM2) as our observational
data; these perturbations are required to activate the phosphorylation pathways. Note
that algorithms typically cannot accurately infer the gold standard solution set using the
observational data alone, as noted by the original authors4. As a result, we created a
silver standard DAG by running LiNGAM as implemented in TETRAD V using default
parameters on the full dataset of 1,755 samples; recall that LiNGAM is a method within a
diﬀerent class of causal discovery algorithms based on functional causal models. We then
ran the six algorithms described in Section 6.1 on 1000 bootstrapped datasets of sample
size 100 using Spearman’s rho to handle the class of non-paranormal distributions.

We have summarized the results in Figure 10. PC-p obtained lower mean FDR across
the same α thresholds than PC-p without robust p-values (z = -12.774, p = 2.282E-37),
PC-p without stabilization (z = −8.402, p = 4.378E-17), PC-p without ambiguation (z =
-24.598, p = 1.343E-133), PC-p without both stabilization and ambiguation (z = -23.714, p =
2.616E-124), and original PC (z = -4.924, p = 8.469E-7). Moreover, PC-p achieved signif-
icantly lower mean under-control than four of the ﬁve competing methods (vs. no ro-
bust: z = -12.601, p = 2.081E-36; vs no stable: z = -4.310, p = 1.631E-5; vs. no ambig:
z = -24.339, p = 7.559E-131; vs. no stable & no ambig: z = -22.893, p = 5.515E-116). PC-p
did not outperform the original PC algorithm in mean under-control (z = 0.827, p = 0.408);
however, PC-p did outperform the original PC algorithm in mean under-estimation (z =
−2.662, p = 0.008). Meanwhile, PC-p kept the mean over-control small at 4.232% (SD:
1.635%). PC-p also outperformed the other four methods in mean under-estimation (vs. no
robust: z = -12.684, p = 7.289E-37; vs. no stable: z = -4.893, p = 9.917E-7; vs. no ambig:
z = -24.411, p = 1.322E-131; vs. no stable & no ambig: z = -23.301, p = 4.333E-120)
while maintaining low mean over-estimation at 1.200% (SD: 0.559%). We conclude that
PC-p outperforms the other methods similar to the results with synthetic data. PC-p only
outperformed PC in 2 of the 3 metrics, however, probably because the LiNGAM solution
is only an estimate of the ground truth.

6.5 Real Data: GDP Dynamics

One way of approximating the underlying DAG involves learning the graph with a large
number of samples. Another way uses time series data, where we know a priori that we
must have contemporaneous causal relations or causal relations directed forward in time.
In this experiment, we strip the time information from the six algorithms, and then identify
the false discoveries when algorithms mistakenly detect a causal relation directed backwards
in time. We used a time series dataset downloaded from the Economic Research Service of
the United States Department of Agriculture containing ten economic indicators per year

4. In general, we do not have gold standard causal graphs for real data, so we must approximate the solution

in some manner.

36

PC with p-values

Figure 11: Similar to Figure 8 except with the GDP dataset as well as over-control and
over-estimation bias values instead of under. PC-p did not achieve lower under-
control and under-estimation than PC, but it did achieve signiﬁcantly lower
mean FDR and over-estimation that PC.

related to GDP among 192 countries5. We speciﬁcally evaluated the algorithms on their
ability to discover causal relations among the indicators within and between 1987, 1988
and 1989, where we treated each country as an i.i.d. sample and used 100 bootstrapped
datasets.

We have summarized the results in Figure 11. PC-p again obtained lower mean FDR val-
ues across the α thresholds than PC-p without robust p-values (z = -4.623, p = 3.784E-6),
PC-p without ambiguation (z = -8.054, p = 4.128E-16), PC-p without both stabilization
and ambiguation (z = -8.135, p = 4.128E-16), and original PC (z = -6.624, p = 3.500E-11).
However, PC-p did not obtain signiﬁcantly lower mean FDR values than PC-p without
stabilization (signed-rank = 70, p = 0.600). Next, PC-p achieved signiﬁcantly lower mean
under-control than three of the ﬁve competing methods including PC-p without robust p-
values (z = -4.374, p = 1.218E-5), PC-p without ambiguation (z = -7.867, p = 3.647E-15),
and PC-p without both stabilization and ambiguation (z = -7.819, p = 5.306E-15). PC-p
did not outperform PC-p without stabilization (signed-ranked = 3, p = 1) as well as the
original PC algorithm (signed-ranked = 7, p = 0.625) in mean under-control; nevertheless,
PC-p did outperform the former in over-control (signed-ranked = 3, p = 0.020) while keep-
ing its own mean over-control low at 4.920% (SD: 0.483%). PC-p also outperformed the
same three methods in mean under-estimation (vs. no robust: z = -4.372, p = 1.229E-5; vs.
no ambig: z = -7.818, p = 5.363E-15; vs. no stable & no ambig: z = -7.770, p = 7.850E-15)
while maintaining low mean over-estimation at 2.032% (SD: 0.281%). PC-p again did
not outperform PC-p without stabilization (signed-rank = 2, p = 0.750) and original

5. Web link: http://www.ers.usda.gov/data/macroeconomics/Data/HistoricalRealGDPValues.xls

37

Strobl, Spirtes and Visweswaran

PC (signed-rank = 7, p = 0.625) in under-estimation, but it outperformed both in over-
estimation (no stable: z = -7.386, p = 1.519E-13; PC original: z = -8.682, p = 3.897E-18).
We conclude that PC-p outperforms most methods in either the under or over-metrics. The
results however are not as clean as the results with the synthetic data because we only have
access to a portion of the ground truth.

7. Conclusion

We developed a new algorithm called PC-p which outputs a causal DAG with p-value bounds
associated with each edge. One can then use the bounds with the BY procedure to achieve
almost strong control and estimation of the FDR. The PC-p algorithm speciﬁcally integrates
the skeleton discovery procedure of PC-stable, edge orientation with ambiguation, and
robust hypothesis tests in order to accurately estimate p-values bounds while maintaining
computational eﬃciency.

The PC-p algorithm represents the ﬁrst global constraint-based method which can re-
cover p-value estimates for every edge of a CPDAG. In our opinion, the algorithm is a
signiﬁcant advancement over previous methods which can only achieve strong control of the
FDR under special conditions. Moreover, PC-p lays a foundation for developing similar
methods which can also recover edge-speciﬁc p-values and achieve strong control of the
FDR for graphs recovered by algorithms such as FCI and CCD. In particular, we suspect
that a combination of the max and union bounds will also be suﬃcient for deriving upper
bounds of the edge-speciﬁc p-values for more sophisticated constraint-based methods. The
proposed approach may therefore represent one the earliest forms of a “causal p-value.”

Now readers may wonder whether PC-p can also use the p-values to control the family-
wise error rate (FWER). The answer is yes, and we recommend using the Benjamini-Holm
step-down procedure as opposed to Hochberg’s step-up procedure to control the FWER
(Hochberg, 1988), since the latter assumes positive dependency among the test statistics.
However, application of an FWER controlling procedure to constraint-based causal discov-
ery requires additional justiﬁcation, since most investigators do not use constraint-based
methods to deﬁnitively conclude causal relationships but rather to screen for potential
causal variables. With the screening goal in mind, the FWER may be too conservative in
practice, since it controls the rate of making a single Type I error across all of the hypothesis
tests as opposed to controlling the proportion of Type I errors.

In summary, we introduced an algorithm called PC-p which outputs a causal DAG along
with edge-speciﬁc p-value bounds. One can then use the BY procedure with the bounds to
achieve almost strong control or estimation of the FDR and therefore assess the algorithm’s
conﬁdence in each edge in a principled manner. We ultimately hope that this work will
encourage more applications of constraint-based causal discovery to important problems in
science.

Acknowledgments

Research reported in this publication was supported by grant U54HG008540 awarded by
the National Human Genome Research Institute through funds provided by the trans-

38

PC with p-values

NIH Big Data to Knowledge initiative. The research was also supported by the National
Library of Medicine of the National Institutes of Health under award numbers T15LM007059
and R01LM012095. The content is solely the responsibility of the authors and does not
necessarily represent the oﬃcial views of the National Institutes of Health.

39

Strobl, Spirtes and Visweswaran

Appendix A. Appendix

A.1 PC Algorithm Pseudocode

We provide pseudocode for the original PC algorithm. We summarize skeleton discovery in
Algorithm 5, unshielded v-structure discovery in Algorithm 6, and orientation rule applica-
tion in Algorithm 7.

Data: X n, α
Result: (cid:98)G, S

l = l + 1
repeat

1 Form a completely connected undirected graph (cid:98)G on the vertex set X
2 l = −1
3 repeat
4

Select a new ordered pair of variables (A, B) that are adjacent in (cid:98)G s.t.
|N (A) \ B| ≥ l
repeat

Choose a new set S ⊆ {N (A) \ B} with |S| = l using order(X)
p ← p-value from testA⊥⊥B|S
if p > α then

Delete A − B from (cid:98)G
Insert S into SA,B and SB,A

end

until A − B is deleted from (cid:98)G or all S ⊆ {N (A) \ B} with |S| = l have been
considered ;

until all ordered pairs of adjacent variables (A, B) in (cid:98)G with | (cid:99)N (A) \ B| ≥ l have
been considered ;

16 until all ordered pairs of adjacent variables (A, B) in (cid:98)G satisfy | (cid:99)N (A) \ B| ≤ l;

Algorithm 5: Skeleton Discovery

5

6

7

8

9

10

11

12

13

14

15

Data: (cid:98)G, S
Result: (cid:98)G

end

4
5 end

1 for all ordered pairs of non-adjacent variables (A, B) with common neighbor C do
2

if C (cid:54)∈ any set in Si,j then

3

Replace A − C − B with A → C ← B

Algorithm 6: Unshielded V-structures

40

PC with p-values

Data: (cid:98)G
Result: (cid:98)G

1 repeat
2

3

4

5

6

7

if A − B and ∃C s.t. C → A, and C and B are non-adjacent then

Replace A − B with A → B

else if A − B and ∃C s.t. A → C → B then

Replace A − B with A → B

else if A − B and ∃B, D s.t. A − C → B, A − D → B, and C and D are
non-adjacent then

Replace A − B with A → B

end

8
9 until there are no more edges to orient;

Algorithm 7: Orientation Rules

A.2 Hypothesis Tests with Less Robust Bounds

We claimed to propose edge-speciﬁc hypothesis tests whose bounds are robust to Type II
errors in Section 4.4. We now explain our rationale.

Consider the following modiﬁcation to (10), where we have replaced the null with a

series of logical conjunctions:

H0 :

oracle i outputs ¬Pi,

H1 :

oracle i outputs Pi.

m
(cid:94)

i=1
m
(cid:94)

i=1

m
(cid:94)

i=1

We can bound the Type I error rate of the above hypothesis test as follows:

Pr(Type I error) = Pr(

test i outputs Pi|H0)

Pr(test i outputs Pi|H0)

Pr(test i outputs Pi|oracle i outputs ¬Pi )

≤ min

i=1,...,m

= min

i=1,...,m

= min

i=1,...,m

gi,

We can use the above bound with the following variant of (26) for unshielded v-

structures:

The above hypothesis test follows from the following natural hypothesis test:

H0 : ¬(A − C) ∧ ¬(B − C) ∧ ¬γAB|C,
H1 : (A − C) ∧ (B − C) ∧ γAB|C,

H0 : All edges between A, C, B are absent,
H1 : (A − C) ∧ (B − C) ∧ γAB|C,

41

(38)

(39)

(40)

(41)

Strobl, Spirtes and Visweswaran

since the null of (41) implies the null in (40). From (39), the Type I error rate of (40) is
bounded by min{pA−C, pB−C, pγAB|C }. This is a less robust bound than (26) in terms of
the Type II error rate, since failing to control one p-value can cause an algorithm to under-
estimate the bound. For example, suppose the underlying truth corresponds to pA−C = 0.01,
pB−C = 0.03, pγAB|C = 0.02. Thus, the Type I error rate of (41) is truly bounded by 0.01.
However, suppose a Type II error causes PC-p to skip CI tests and therefore compute
pA−C = 0.01, pB−C = 0.03, pγAB|C = 0.003, where the third term is under-estimated. Then,
PC-p will under-estimate the bound at 0.003 instead of the true 0.01.

Note that generalizing (41) to account for multiple possible ways of orienting a v-

structure does not robustify the bound either, since we have:

H0 : All edges between A, C, B1 are absent, and all edges between

A, C, B2 are absent,

(cid:16)(cid:104)

H1 : (A − C) ∧

(B1 − C) ∧ γAB1|C] ∨ [(B2 − C) ∧ γAB2|C

(cid:105)(cid:17)

.

whose Type I error rate is bounded by min

(cid:111)
(cid:110)
.
pA−C, min{pB1−C, pγAB1|C }+min{pB2−C, pγAB2|C }

More broadly, we can consider the following hypothesis test:

H0 :

oracle i, j outputs ¬Pi,j

H1 :

oracle i, j outputs Pi,j

(cid:17)

,

(cid:17)

.

m
(cid:95)

(cid:16) ni(cid:94)

i=1
m
(cid:94)

j=1

(cid:16) ni(cid:94)

i=1

j=1

We bound its Type I error rate as follows:
ni(cid:94)

m
(cid:95)

Pr(Type I error) = Pr(

test i, j outputs Pi,j|H0 )

i=1

j=1

Pr(test i, j outputs Pi,j|H0)

= max

Pr(test i, j outputs Pi,j| oracle i, j outputs ¬Pi,j)

≤ max

i=1,...,m

min
j=1,...,ni

min
j

min
j

i

i

= max

gi,j,

The above bound is less robust to Type II errors than (11), since under-estimating one term
in each group i composed of ni terms can cause PC-p to also under-estimate (44).

A.3 Other Experimental Results

We have summarized the results for the low dimensional, high dimensional and real datasets
across multiple α thresholds in Figures 12, 13, 14 and 15 respectively. Relative diﬀerences in
performance largely remained consistent across the thresholds, since PC-p usually achieved
the lowest mean FDR, under-control and under-estimation values with minimal increases
in mean over-control and over-estimation.

We have also summarized the results for adjacency discovery in Figures 16, 17, and 18,
where we tested whether the skeleton discovery procedure of PC with stabilization could

42

(42)

(43)

(44)

PC with p-values

improve the estimation of the p-value bounds relative to the procedure without stabilization.
Results show that stabilization improves performance across the three metrics particularly
with the low α threshold values of 0.01 and 0.05. Note that we cannot compute the same
ﬁgures for the GDP dataset, since we can only evaluate relative performance levels based
on edge direction in this case.

We have ﬁnally summarized the results using the structural Hamming distance in Figure
19. Notice that ambiguation helps PC-p achieve signiﬁcantly lower Hamming distances
across multiple α thresholds by forcing the algorithm to conservatively orient the edges.
Again, we cannot compute the structural Hamming distances for the GDP dataset for the
aforementioned reason.

References

A. P. Armen and I. Tsamardinos. A uniﬁed approach to estimation and control of the
false discovery rate in bayesian network skeleton identiﬁcation.
In In Proceedings of
the European Symposium on Artiﬁcial Neural Networks, Computational Intelligence and
Machine Learning (ESANN, 2011.

A. P. Armen and I. Tsamardinos. Estimation and control of the false discovery rate of
bayesian network skeleton identiﬁcation. Technical Report TR-441, University of Crete,
2014.

Y. Benjamini and D. Yekutieli. Investigating the importance of self-theories of intelligence
and musicality for students’ academic and musical achievement. Annals of Statistics, 29
(4):1165–1188, 2001.

H. Chong, M. Zey, and D. A. Bessler. On corporate structure, strategy, and performance:
a study with directed acyclic graphs and pc algorithm. Managerial and Decision Infor-
matics, 31:47–62, 2009.

D. Colombo and M. H. Maathuis. Order-independent constraint-based causal structure
learning. J. Mach. Learn. Res., 15(1):3741–3782, January 2014. ISSN 1532-4435. URL
http://dl.acm.org/citation.cfm?id=2627435.2750365.

G. F. Cooper and C. Yoo. Causal discovery from a mixture of experimental and observa-
tional data. In Proceedings of the Fifteenth Conference on Uncertainty in Artiﬁcial Intel-
ligence, UAI’99, pages 116–125, San Francisco, CA, USA, 1999. Morgan Kaufmann Pub-
lishers Inc. ISBN 1-55860-614-9. URL http://dl.acm.org/citation.cfm?id=2073796.
2073810.

N. Friedman, M. Goldszmidt, and A. Wyner. Data analysis with bayesian networks: A
bootstrap approach. In Proceedings of the Fifteenth Conference on Uncertainty in Arti-
ﬁcial Intelligence, UAI’99, pages 196–205, San Francisco, CA, USA, 1999. Morgan Kauf-
mann Publishers Inc. ISBN 1-55860-614-9. URL http://dl.acm.org/citation.cfm?
id=2073796.2073819.

43

Strobl, Spirtes and Visweswaran

Figure 12: Mean FDR, control bias, and estimation bias values across multiple α thresholds

for the low dimensional datasets.

44

PC with p-values

Figure 13: Same as Figure 12 except with high dimensional datasets.

45

Strobl, Spirtes and Visweswaran

Figure 14: Same as Figure 12 except with bootstrapped CYTO datasets.

46

PC with p-values

Figure 15: Same as Figure 12 except with bootstrapped GDP datasets.

47

Strobl, Spirtes and Visweswaran

Figure 16: Mean FDR, control bias, and estimation bias values across multiple α thresholds

for the low dimensional datasets.

48

PC with p-values

Figure 17: Same as Figure 16 except with high dimensional datasets.

49

Strobl, Spirtes and Visweswaran

Figure 18: Same as Figure 16 except with bootstrapped CYTO datasets.

50

PC with p-values

Figure 19: Structural Hamming distances for the a) low dimensional, b) high dimensional
and c) CYTO datasets. The three sub-ﬁgures associate 5 bars with each algo-
rithm; these bars correspond to α thresholds of 0.01, 0.05, 0.1, 0.20 and 0.50 for
the low dimensional and CYTO datasets, and α thresholds of 0.01, 0.05, 0.1,
0.15 and 0.20 for the high dimensional datasets. Error bars represent standard
errors for a) and standard deviations otherwise.

M. J. Ha, V. Baladandayuthapani, and K. A. Do. Prognostic gene signature identiﬁcation
using causal structure learning: applications in kidney cancer. Cancer Inform, 14(Suppl
1):23–35, 2015.

N. Harris and M. Drton. Pc algorithm for nonparanormal graphical models. J. Mach.
ISSN 1532-4435. URL http://dl.acm.

Learn. Res., 14(1):3365–3383, January 2013.
org/citation.cfm?id=2567709.2567770.

Y. Hochberg. A sharper Bonferroni procedure for multiple tests of signiﬁcance. Biometrika,
75(4):800–802, December 1988. ISSN 1464-3510. doi: 10.1093/biomet/75.4.800. URL
http://dx.doi.org/10.1093/biomet/75.4.800.

S. P. Iyer, I. Shafran, D. Grayson, K. Gates, J. T. Nigg, and D. A. Fair. Inferring func-
tional connectivity in MRI using Bayesian network structure learning with a modiﬁed PC
algorithm. Neuroimage, 75:165–175, Jul 2013.

A. A. Joshi, S. H. Joshi, R. M. Leahy, D. W. Shattuck, I. Dinov, and A. W. Toga. Bayesian
approach for network modeling of brain structural features, volume 7626. 2010. ISBN
9780819480279. doi: 10.1117/12.844548.

T. D. Le, L. Liu, A. Tsykin, G. J. Goodall, B. Liu, B. Y. Sun, and J. Li.

Inferring
microRNA-mRNA causal regulatory relationships from expression data. Bioinformatics,
29(6):765–771, Mar 2013.

51

Strobl, Spirtes and Visweswaran

J. Li and Z. J. Wang. Controlling the false discovery rate of the association/causality
structure learned with the pc algorithm. J. Mach. Learn. Res., 10:475–514, June 2009.
ISSN 1532-4435. URL http://dl.acm.org/citation.cfm?id=1577069.1577086.

J. Li, Z. Wang, and M. J. McKeown. Learning brain connectivity with the false-discovery-
rate-controlled PC-algorithm. Conf Proc IEEE Eng Med Biol Soc, 2008:4617–4620, 2008.

J. Listgarten and D. Heckerman. Determining the number of non-spurious arcs in a learned
dag model: Investigation of a bayesian and a frequentist approach. In Ronald Parr and
Linda C. van der Gaag, editors, UAI, pages 251–258. AUAI Press, 2007. ISBN 0-9749039-
3-0. URL http://dblp.uni-trier.de/db/conf/uai/uai2007.html#ListgartenH07.

C. Meek. Strong completeness and faithfulness in bayesian networks. In Proceedings of the
Eleventh Conference on Uncertainty in Artiﬁcial Intelligence, UAI’95, pages 411–418,
San Francisco, CA, USA, 1995. Morgan Kaufmann Publishers Inc. ISBN 1-55860-385-9.
URL http://dl.acm.org/citation.cfm?id=2074158.2074205.

D. Mullensiefen, P. Harrison, F. Caprini, and A. Fancourt. Investigating the importance
of self-theories of intelligence and musicality for students’ academic and musical achieve-
ment. Front Psychol, 6:1702, 2015.

T. Richardson. A discovery algorithm for directed cyclic graphs.

In Proceedings of the
Twelfth International Conference on Uncertainty in Artiﬁcial Intelligence, UAI’96, pages
454–461, San Francisco, CA, USA, 1996. Morgan Kaufmann Publishers Inc.
ISBN 1-
55860-412-X. URL http://dl.acm.org/citation.cfm?id=2074284.2074338.

K. Sachs, O. Perez, D. Pe’er, D. A. Lauﬀenburger, and G. P. Nolan. Causal protein-signaling
networks derived from multiparameter single-cell data. Science, 308(5721):523–529, Apr
2005.

P. Spirtes, C. Meek, and T. Richardson. Causal inference in the presence of latent vari-
ables and selection bias. In Proceedings of the Eleventh Conference on Uncertainty in
Artiﬁcial Intelligence, UAI’95, pages 499–506, San Francisco, CA, USA, 1995. Morgan
Kaufmann Publishers Inc. ISBN 1-55860-385-9. URL http://dl.acm.org/citation.
cfm?id=2074158.2074215.

P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. MIT press,

2nd edition, 2000.

J. Sun, X. Hu, X. Huang, Y. Liu, K. Li, X. Li, J. Han, L. Guo, T. Liu, and J. Zhang.
Inferring consistent functional interaction patterns from natural stimulus FMRI data.
Neuroimage, 61(4):987–999, Jul 2012.

R. Teramoto, C. Saito, and S. Funahashi. Estimating causal eﬀects with a non-paranormal
method for the design of eﬃcient intervention experiments. BMC Bioinformatics, 15:228,
2014.

I. Tsamardinos and L. E. Brown. Bounding the false discovery rate in local bayesian network
learning. In Proceedings of the Twenty-Third AAAI Conference on Artiﬁcial Intelligence,

52

PC with p-values

AAAI 2008, Chicago, Illinois, USA, July 13-17, 2008, pages 1100–1105, 2008. URL
http://www.aaai.org/Library/AAAI/2008/aaai08-174.php.

X. Wu and Y. Ye. Exploring gene causal interactions using an enhanced constraint-based
method. Pattern Recogn., 39(12):2439–2449, December 2006.
ISSN 0031-3203. doi:
10.1016/j.patcog.2006.05.003. URL http://dx.doi.org/10.1016/j.patcog.2006.05.
003.

53

7
1
0
2
 
y
a
M
 
0
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
5
7
9
3
0
.
7
0
6
1
:
v
i
X
r
a

Estimating and Controlling the False Discovery Rate of the
PC Algorithm Using Edge-Speciﬁc P-Values

Eric V. Strobl
Department of Biomedical Informatics
University of Pittsburgh
Pittsburgh, PA 15206, USA

Peter L. Spirtes
Department of Philosophy
Carnegie Mellon University
Pittsburgh, PA 15213, USA

Shyam Visweswaran
Department of Biomedical Informatics
University of Pittsburgh
Pittsburgh, PA 15206, USA

Editor: TBA

evs17@pitt.edu

ps7z@andrew.cmu.edu

shv3@pitt.edu

Abstract

The PC algorithm allows investigators to estimate a complete partially directed acyclic
graph (CPDAG) from a ﬁnite dataset, but few groups have investigated strategies for
estimating and controlling the false discovery rate (FDR) of the edges in the CPDAG. In
this paper, we introduce PC with p-values (PC-p), a fast algorithm which robustly computes
edge-speciﬁc p-values and then estimates and controls the FDR across the edges. PC-p
speciﬁcally uses the p-values returned by many conditional independence (CI) tests to upper
bound the p-values of more complex edge-speciﬁc hypothesis tests. The algorithm then
estimates and controls the FDR using the bounded p-values and the Benjamini-Yekutieli
FDR procedure. Modiﬁcations to the original PC algorithm also help PC-p accurately
compute the upper bounds despite non-zero Type II error rates. Experiments show that
PC-p yields more accurate FDR estimation and control across the edges in a variety of
CPDAGs compared to alternative methods1.
Keywords: PC Algorithm, Causal Inference, False Discovery Rate, Bayesian Network,
Directed Acyclic Graph

1. Introduction

Discovering causal relationships is often much more important than discovering associational
relationships in the sciences. As a result, the research community has been conducting
extensive investigations into causal inference with the hope of developing practically useful
algorithms to speed-up the scientiﬁc process. This research has resulted in a wide range of
high performing algorithms over the years such as PC (Spirtes et al., 2000), FCI (Spirtes
et al., 1995, 2000), and CCD (Richardson, 1996)

1. MATLAB implementation: https://github.com/ericstrobl/PCp/

c(cid:13)2016 Eric V. Strobl and Shyam Visweswaran.

Strobl, Spirtes and Visweswaran

The PC algorithm is currently one of the most popular methods for inferring causation
from observational data. Given an observational dataset, the algorithm outputs a complete
partially directed acyclic graph (CPDAG) which has helped some investigators elucidate
important causal relationships in several domains. For example, the PC algorithm has been
used to discover new causal relationships between genes and brain regions in biology (Wu
and Ye, 2006; Li et al., 2008; Joshi et al., 2010; Sun et al., 2012; Harris and Drton, 2013;
Iyer et al., 2013; Le et al., 2013; Teramoto et al., 2014; Ha et al., 2015). The algorithm has
also been used to discover causal relations between corporate structures and strategies in
economics (Chong et al., 2009) as well as academic and musical achievements in psychology
(Mullensiefen et al., 2015).

The increased use of PC in recent years has nonetheless led to growing concern about
algorithm’s conﬁdence level in each edge of the CPDAG. For example, PC may have more
conﬁdence in the edge A − B but have less conﬁdence in the edge B → C in the subgraph
A−B → C of the CPDAG. Currently, PC alone does not output any edge-speciﬁc measure of
conﬁdence, even though scientists often must report measures of conﬁdence such as p-values
or conﬁdence intervals in their scientiﬁc articles. This incongruency has resulted in the
relatively slow adoption or even avoidance of the PC algorithm in the sciences, despite the
algorithm’s impressive capabilities in causal inference. Clearly then rectifying the problem
by developing an edge-speciﬁc measure of conﬁdence will increase the adoption of PC as
well as hopefully ease the transition of ever-more complex causal inference algorithms into
the scientiﬁc community.

We can of course consider multiple diﬀerent ways of representing the conﬁdence level
in each edge of the CPDAG. However, we choose to pay special attention to the p-value,
since it is by far the most popular notion of conﬁdence in the sciences. Indeed, nearly all
scientists report p-values in modern scientiﬁc reports because they rely on p-values to help
justify their hypotheses. We therefore would ideally like to assign a p-value to each edge in
the CPDAG as in Figure 1a in order to best integrate the algorithm within a well-known
framework. In this paper, we propose such a “causal p-value” in detail.

We however also believe that assigning p-values to each edge is not enough to ease the
transition of PC into mainstream science, since the CPDAG actually contains many edges
and therefore also represents a complicated multiple hypothesis testing problem. Fortu-
nately, the problem of multiple hypothesis testing has a long history, as scientists have
often required the results of multiple hypothesis tests in order to answer complex scien-
tiﬁc questions. Currently, a standard approach to tackling the multiple hypothesis testing
problem involves controlling the proportion of false positives among the rejected null hy-
potheses, or the false discovery rate (FDR), by using an FDR controlling procedure that
takes a desired FDR level q and a set of p-values as input. The procedure then outputs
a corresponding signiﬁcance level α∗ for the set of p-values. An investigator subsequently
rejects the null hypotheses for those tests with p-values that fall below α∗ in order to en-
sure that the expected FDR does not exceed q. For example, consider the set of p-values
{0.02, 0.01, 0.03} and suppose that the FDR controlling procedure with q = 0.1 outputs
α∗ = 0.019. Then, rejecting the null hypotheses of the ﬁrst and third hypothesis tests
guarantees that the expected FDR does not exceed 10%. Several other FDR controlling
strategies also exist, but the ease of use, speed and accuracy of the above method have
made it the most widely adopted strategy in the last two decades.

2

PC with p-values

We would therefore like to control the FDR in the edges of the CPDAG using an FDR
controlling procedure like in Figure 1b. As a ﬁrst idea, one may wonder whether an inves-
tigator can control the FDR in the CPDAG by simply feeding in all of the CI test p-values
computed by PC into an FDR controlling procedure. Unfortunately, this approach fails for
at least two reasons. First, it is unclear how to use the p-values which exceed the α∗ cut-oﬀ
to reject edges in the CPDAG, since one would ﬁrst need to elucidate the correspondence
between the p-values and edges. Second, even if one could solve this problem, the strat-
egy may only loosely bound the expected FDR. An accurate FDR controlling procedure
should instead take into account the speciﬁc computations executed by PC in order to iden-
tify a sharp bound. The p-value based approach therefore necessitates a more ﬁne-grained
strategy which has thus far remained undiscovered.

Several groups have nonetheless attempted to control the FDR in the CPDAG by avoid-
ing the complicated nature of the above problem with a diﬀerent, data re-sampling approach.
For example, Friedman and colleagues proposed to estimate the FDR by using the para-
metric bootstrap (Friedman et al., 1999). This procedure involves ﬁrst learning a causal
graph with the PC algorithm. The procedure then generates data from the causal graph
and re-applies the PC algorithm multiple times on each generated dataset to estimate the
FDR using the learnt causal graphs. An investigator can subsequently control the FDR
by repeating the above process with diﬀerent α values until he or she reaches the desired
FDR level q. However, notice that the method requires multiples calls to PC and can
therefore require too much time with high dimensional data. The procedure also requires
parametric knowledge about the underlying distribution which limits the applicability of
the method to simple cases. Fortuitously, two groups later proposed a permutation-based
method which drops the parametric assumption (Listgarten and Heckerman, 2007; Armen
and Tsamardinos, 2014). The permutation method nevertheless also requires multiple calls

(a)

A

(b)

A

.

.

B

5
2
0.0

C

0
.
1
5
0

B

5
2
0.0

0

0

.

.

0

0

0

0

1

1

E

0.001

D

E

0.001

D

C

α∗ = 0.031

Figure 1: We seek to associate edge-speciﬁc p-values to the output of the PC algorithm
such as in (a). The PC algorithm currently does not associate such p-values with
its output. We would also like to control the FDR of the edges. In (b), we set
the FDR to 0.1 and obtained a α∗ cutoﬀ of 0.031 for the output in (a), so we
eliminated the edge between B and D because its p-value exceeds α∗.

3

Strobl, Spirtes and Visweswaran

to an algorithm and in fact only applies to the parts of PC which can be decomposed
into independent searches for the parents of each vertex; this has thus far limited the ap-
plicability of the method to adjacency discovery with local to global discovery algorithms
(e.g., MMHC) and incomplete edge orientation. We conclude that both the bootstrap and
permutation approaches to FDR estimation and control are either incomplete or too time
consuming.

Another class of methods fortunately attempts to control the FDR without resampling
procedures by instead using a standard FDR controlling procedure with bounded p-values.
For instance, one method proposed in (Tsamardinos and Brown, 2008) and then reﬁned
in (Armen and Tsamardinos, 2011, 2014) assigns a p-value to each adjacency by taking
the maximum over all of the signiﬁcant p-values from the associated CI tests executed
by PC. The method then controls the FDR in the estimated adjacencies by applying an
FDR controlling procedure, such as the one proposed by Benjamini and Yekutieli (BY)
(Benjamini and Yekutieli, 2001), on the edge-speciﬁc p-values. Under faithfulness and a
zero Type II error rate, the method controls the FDR across the estimated adjacencies,
or the estimated skeleton (Armen and Tsamardinos, 2014). This two stage method also
performs comparably with the one stage method proposed in (Li et al., 2008; Li and Wang,
2009), which controls the FDR during, as opposed to after, the execution of the skeleton
discovery phase of the PC algorithm. Of course, the Type II error rate never reaches
zero in practice but researchers have also investigated a strategy for reducing the realized
Type II error rate by introducing a heuristic reliability criterion for CI tests when dealing
with discrete data (Armen and Tsamardinos, 2014). Experiments have shown that these
methods ﬁnish in a relatively short amount of time and perform well in practice. However,
the methods are also incomplete because they only apply to the skeleton discovery phase of
PC.

In this report, we build on the previous outstanding work for deriving p-values for
adjacencies by contributing a sound, complete and fast algorithm called PC with p-values
(PC-p) which appropriately combines the p-values of PC’s CI tests and then uses the BY
FDR controlling procedure to accurately control the FDR in a CPDAG. The method relies
on two upper bounds of the p-value that relate to logical conjunctions and disjunctions as
described in Section 3. These upper bounds allow us to formulate several hypothesis tests
for recovering the skeleton, discovering unshielded v-structures, and orienting additional
edges as presented in Section 4. Accurately estimating the p-values of the hypothesis tests
nonetheless requires a modiﬁed version of PC called PC-p which we propose in Section
5. Finally, we provide experimental results in Section 6 which show that PC-p’s p-value
estimates yield accurate estimates of the FDR with the BY procedure and improve upon
alternative methods.

2. Preliminaries

2.1 Causal graphs

A causal graph consists of vertices representing variables and edges representing causal
relationships between any two variables. In this paper, we will use the terms “vertices” and
“variables” interchangeably. Directed graphs are graphs where two distinct vertices can be
connected by edges “→” and “←.” We only consider simple graphs in this paper, or graphs

4

PC with p-values

with no edges originating from and connecting to the same vertex. Directed acyclic graphs
(DAGs) are directed graphs without directed cycles. We say that X and Y are adjacent
if they are connected by an edge independent of the edge’s direction. A path p from X
to Y is a set of consecutive edges (also independent of their direction) from X to Y such
that no vertex is visited more than once. Given a path between two vertices X and Y
with a middle vertex Z, the path is a chain if X → Y → Z, a fork if X ← Y → Z, and
a v-structure if X → Y ← Z. We refer to Y as a collider, if it is the middle vertex in a
v-structure. A v-structure is called an unshielded v-structure if X → Y ← Z, but X and Z
are non-adjacent. A directed path from X to Y is a set of consecutive edges with direction.
We say that X is an ancestor of Y (and Y is a descendant of X), if there exists a directed
path from X to Y .

If G is a directed graph in which X, Y and Z are disjoint sets of vertices, then X
and Y are d-connected by Z in G if and only if there exists an undirected path p between
some vertex in X and some vertex in Y such that, for every collider C on p, either C or a
descendant of C is in Z, and no non-collider on p is in Z. On the other hand, X and Y
are d-separated by Z in G if and only if they are not d-connected by Z in G. Next, the
joint probability distribution P over variables X satisﬁes the global directed Markov property
for a directed graph G if and only if, for any three disjoint subsets of variables A, B and
C from X, if A and B are d-separated given C in G, then A and B are conditionally
independent given C in P. We refer to the converse of the global directed Markov property
as d-separation faithfulness; that is, if A and B are conditionally independent given C in
P, then A and B are d-separated given C in G.

A Markov equivalence class of DAGs refers to a set of DAGs which entail the same con-
ditional independencies. A complete partially directed acyclic graph (CPDAG) is a partially
directed acyclic graph with the following properties: (1) each directed edge exists in every
DAG in the Markov equivalence class, and (2) there exists a DAG with X → Y and a DAG
with X ← Y in the Markov equivalence class for every undirected edge X − Y . A CPDAG
GC represents a DAG G, if G belongs to the Markov equivalence class described by GC.
We will occasionally use the meta-symbol “◦” at the endpoint(s) of an edge to denote the
presence or absence of an arrowhead. For example, the edge “ −◦ ” may denote either “−”
or “→”.

2.2 The PC Algorithm

The PC algorithm is comprised of three stages. We have summarized these stages as
pseudocode in Algorithms 5, 6 and 7 in Section A.1 of the Appendix. The ﬁrst stage
estimates the adjacencies of G, or the skeleton of G. Starting with a fully connected
skeleton, the algorithm attempts to eliminate the adjacency between any two variables,
say A and B, by testing if A and B are conditionally independent given some subset of the
neighbors of A or the neighbors of B. The search is performed progressively, whereby the
algorithm increases the size of the conditioning set starting from zero using a step size of 1.
The edge between A and B is removed, if A and B are rendered conditionally independent
given some subset of the neighbors of A or the neighbors of B.

The PC algorithm orients unshielded colliders in its second stage. Speciﬁcally, PC ﬁnds
triples A, B, C such that A − B − C, but A and C are non-adjacent. The algorithm

5

Strobl, Spirtes and Visweswaran

then determines whether B is contained in the set which rendered A and C conditionally
independent in the ﬁrst stage of PC. If not, A − B − C is replaced with A → B ← C.

The third and ﬁnal stage of PC involves the repetitive application of three rules to orient

as many of the remaining undirected edges as possible. The three rules include:

1.

If A − B, C → A and C and B are non-adjacent, then replace A − B with

A → B.

2.

3.

If A − B and A → C → B, then replace A − B with A → B.

(1)

If A − B, A − C → B, A − D → B, and C and D are non-adjacent,

then replace A − B with A → B.

Overall, the PC algorithm has been shown to be complete in the sense that it ﬁnds and

then orients edges up to GC, a CPDAG that represents G (Meek, 1995).

2.3 Hypothesis Testing

A hypothesis test is a method of statistical inference usually composed of one null (H0) and
one alternative (H1) hypothesis which are mutually exclusive; that is, if one occurs, then
the other cannot occur. The null hypothesis refers to the default position which asserts that
whatever one is trying to statistically infer actually did not happen. Note that the null and
alternative do not necessarily need to be logical complements of each other. For example,
one may be interested in determining whether the parameter µ is greater than zero. In this
case, the null can be deﬁned as µ = 0 while the alternative can be deﬁned as µ > 0 instead
of µ (cid:54)= 0.

A Type I error is the incorrect rejection of a true null hypothesis, or a false positive.
On the other hand, a Type II error is the failure to reject a false null hypothesis, or a false
negative. The p-value (p) is the probability of the Type I error, or the Type I error rate.
More speciﬁcally, the p-value is the probability of obtaining a result equal to or more extreme
than the observed value under the assumption of the null hypothesis. The null hypothesis
is thus rejected when the p-value is at or below a predeﬁned α threshold (typically the α
threshold is set to 0.05), because a low p-value demonstrates the improbability of the null
hypothesis.

2.4 False Discovery Rate

Multiple comparisons or multiple hypothesis testing refers to the process of considering
more than one statistical inference simultaneously. Failure to compensate for multiple
comparisons can result in erroneous inferences. For example, if an investigator performs
one hypothesis test with an α threshold of 0.05, then he or she has only a 5% chance of
making a Type I error. However, if the investigator performs 100 independent tests with
the same α threshold, then he or she has a 1 − (1 − 0.05)100 = 99.4% chance of making a
Type I error on at least one test.

In multiple hypothesis testing, the false discovery rate (FDR) at threshold α is the
expected proportion of false positives among the rejected null hypotheses. Speciﬁcally, we

6

PC with p-values

deﬁne the FDR at α as follows:

F DR(α) (cid:44) E

(cid:20)

V
max{R, 1}

(cid:21)

,

where V is the number of false positives, R is the total number of null hypotheses rejected,
and max{R, 1} ensures that F DR(α) is well-deﬁned when R = 0. We deﬁne the realized
FDR at α as V / max{R, 1}.

FDR estimation, or conservative point estimation of the FDR, refers to the process of

estimating F DR(α) in a conservative manner such that:

E[(cid:92)F DR(α)] ≥ F DR(α),

(cid:92)F DRBY (α) (cid:44)

mα (cid:80)m
1
i=1
i
max{R, 1}

.

where (cid:92)F DR(α) represents an estimate of F DR(α). We denote E[(cid:92)F DR(α)]−F DR(α) as the
estimation bias. Note that there are several ways of obtaining (cid:92)F DR(α). In 2001, Benjamini
and Yekutieli proposed the following FDR estimator for m hypothesis tests:

(2)

(3)

FDR estimators such as (cid:92)F DRBY can be used to deﬁne FDR controlling procedures. These
procedures determine the optimal threshold α∗ which achieves strong control 2 of the FDR
in the following sense:

α∗ (cid:44) arg max

{(cid:92)F DR(α) ≤ q}

α

The FDR controlling procedure based on (cid:92)F DR involves the rejection of all null hypotheses
with p-values below the α∗ threshold. We refer to the quantity F DR(α∗) − q as the control
bias. Benjamini and Yekutieli proved that the estimate (cid:92)F DRBY in particular achieves
strong control of the FDR with any form of dependence among the p-values of m hypothesis
tests.

3. Upper Bounds on the P-Value

We present two upper bounds of the Type I error rate of hypothesis tests which can be
constructed using a set of simpler hypothesis tests. These upper bounds will serve as useful
tools in Section 4 for bounding the Type I error rate of the hypothesis tests which will be
used to infer the presence or absence of edges in a CPDAG.

3.1 Union Bound

Consider the following hypothesis test for two random variables given a conditioning set:

2. Strong control of the FDR refers the process of controlling the FDR under any conﬁguration of true and
false null hypotheses; on the other hand, weak control refers to the process of controlling the FDR when
all of the null hypotheses are true. Strong control is therefore preferable to weak control.

H0 : Conditionally independent,
H1 : Conditionally dependent.

7

Strobl, Spirtes and Visweswaran

f ((cid:98)si)

−1.96

0

1.96

(cid:98)si

Figure 2: In the above standard normal case, we have Pr(|(cid:98)si| ≥ sα

(cid:12)
(cid:12)si = 0) = 0.05, where
sα
i = 1.96. We reject the null hypothesis when |(cid:98)si| falls in the blue colored regions
at the tails.

i

Trivially, we can rephrase the null and alternative in terms of a conditional independence
(CI) oracle:

H0 : The CI oracle outputs independent,
H1 : The CI oracle outputs dependent.

Now suppose we want to query m CI oracles about m CI relations. We can then consider
the following null and alternative:

H0 : All CI oracles output independent,
H1 : At least one CI oracle outputs dependent.

(4)

i

From here on, we write Pr(CI test i outputs dependent | CI oracle i outputs independent) to
(cid:12)
denote Pr(|(cid:98)si| ≥ sα
(cid:12)si = 0), where si refers to a parameter of some standardized distribution
used by CI test i, (cid:98)si a random variable and the test statistic estimating si, and sα
i a value of
si determined by an α level. We provide an example in Figure 2, where si may correspond
to Fisher’s z-statistic in the case of Fisher’s z-test for the mean parameter si = 0 of the
standard normal distribution.

8

PC with p-values

We now bound the Type I error rate of the hypothesis test (4) by using the new notation

and the union bound:

Pr(Type I error)

= Pr(at least one CI test outputs dependent|H0)

(cid:33)

= Pr

CI test i outputs dependent|H0

Pr(CI test i outputs dependent|H0)

(cid:32) m
(cid:95)

i=1

≤

=

=

m
(cid:88)

i=1
m
(cid:88)

i=1
m
(cid:88)

i=1

pi,

Pr(CI test i outputs dependent|CI oracle i outputs independent)

where pi denotes the Type I error rate of CI test i. Thus, if the r.h.s. of (5) is less than
the α threshold, then we can conclude that the Type I error rate of (4) is also below the
threshold. In other words, (5) is a conservative p-value.

Note that the third equality in the derivation of (5) uses the simplifying assumption
that the probability of the output of CI test i only depends on the output of CI oracle
i when given the outputs of all CI oracles. Several papers have used this assumption
implicitly in their proofs (Tsamardinos and Brown, 2008; Li and Wang, 2009), and we will
also use it throughout this paper. We can justify the assumption based on three facts.
First, most CI test statistics si have a limiting distribution which only depends on si = 0
under the null. For example, Fisher’s z-statistic has a limiting standard normal distribution
with mean parameter zi = 0 and constant variance. Moreover, the G-statistic for the G-
test has a limiting χ2-distribution with non-centrality parameter gi = 0 and degrees of
freedom determined by the number of cells in the contingency table. Second, existing
methods which utilize bounds based on the assumption have strong empirical performance;
loose-enough bounds therefore appear to accommodate the assumption well in most ﬁnite
sample cases. Third, recall that simplifying assumptions are not new in the causality
literature; indeed, many authors have made simplifying assumptions regarding parameter
independence for Bayesian methods which similarly increase computational eﬃciency and
achieve strong empirical performance (e.g., (Cooper and Yoo, 1999)).

We can now also generalize the bound in (5) to any hypothesis test consisting of a series
of logical disjunctions in the alternative and a series of logical conjunctions in the null.
Namely:

(5)

(6)

H0 :

oracle i outputs ¬Pi,

H1 :

oracle i outputs Pi,

m
(cid:94)

i=1
m
(cid:95)

i=1

9

Strobl, Spirtes and Visweswaran

where Pi denotes an arbitrary output of oracle i. We now have:

Pr(Type I error) = Pr(

test i outputs Pi |H0)

m
(cid:95)

i=1

Pr(test i outputs Pi|H0)

Pr(test i outputs Pi|oracle i outputs ¬Pi)

≤

=

=

m
(cid:88)

i=1
m
(cid:88)

i=1
m
(cid:88)

i=1

hi,

where hi is the Type I error rate of test i, and the second equality uses the assumption that
the probability of the output of test i only depends on the output of oracle i when given all
oracles. We will use this generalization in Section 4.

3.2 Intersection Bound

Suppose we want to perform a hypothesis test with the following null and alternative which
are diﬀerent than the null and alternative in (4):

H0 : At least one CI oracle outputs independent,
H1 : All CI oracles output dependent.

Now assume we know that the ith CI oracle outputs independent. We can then bound the
Type I error rate of (7) as follows with m queries to the CI oracle:

Pr(Type I error) = Pr(all m CI tests output dependent|H0)

≤ Pr(CI test i outputs dependent |H0)
= Pr(CI test i outputs dependent | CI oracle i outputs independent

∧ other CI oracles may output independent)

= Pr(CI test i outputs dependent | CI oracle i outputs independent)
= pi,

where the second equality again holds under the assumption that the probability of the
output of CI test i only depends on the output of CI oracle i when given the outputs of all
CI oracles. We can therefore bound the Type I error rate of (7) using the p-value of a single
CI test for which the CI oracle outputs independent. Nevertheless, in practice, we often do
not know for which query the oracle outputs independent in the null. We do however know
that at least one unknown CI oracle i outputs independent, so we can bound the Type I
error rate of (7) using the maximum over all of the m CI test p-values:

Pr(Type I error) = Pr(all m CI tests output dependent|H0)

≤ Pr(CI test i outputs dependent | CI oracle i outputs independent)
= pi ≤ max

pj.

(9)

j=1,...,m

(7)

(8)

10

PC with p-values

H0 :

oracle i outputs ¬Pi,

H1 :

oracle i outputs Pi.

m
(cid:95)

i=1
m
(cid:94)

i=1

m
(cid:94)

i=1

Note that we can generalize the above bound to any hypothesis test consisting of a
series of logical conjunctions in the alternative and a series of logical disjunctions in the
null. Namely:

(10)

(11)

We therefore have:

Pr(Type I error) = Pr(

test i outputs Pi|H0)

≤ Pr(test i outputs Pi|H0)
= Pr(test i outputs Pi|oracle i outputs ¬Pi )
= hi ≤ max

hj,

j=1,...,m

where the second equality again uses the assumption that the probability of the output of
test i only depends on the output of oracle i when given all oracles.

4. Edge-Speciﬁc Hypothesis Tests

We now show how to apply the two upper bounds of the Type I error rate to derive p-value
estimates for both the undirected and directed edges in the CPDAG as estimated by PC.
Bounding the p-value for each edge therefore amounts to adding up and/or maximizing over
the p-values returned from multiple CI tests.

Note that we will sometimes invoke a zero Type II error rate assumption in this section.
This assumption is necessary to correctly upper bound the p-values of the edge-speciﬁc
hypothesis tests of the CPDAG according to the CI tests executed by the PC algorithm. In
fact, we can always correctly bound the p-values, if we perform all of the possible CI tests
between the considered variables; however, this approach is impractical, since it ignores
the eﬃciencies of the PC algorithm. A more interesting strategy involves designing the
edge-speciﬁc hypothesis tests so that the p-value bounds are robust to Type II errors as
well as redesigning the PC algorithm to catch many Type II errors. We will discuss these
approaches in detail in Sections 4.4 and 5, so we encourage readers to accept the zero Type
II error rate assumption for now.

4.1 Skeleton Discovery

We ﬁrst consider the skeleton discovery phase of the PC algorithm. We wish to test whether
each edge is absent in the true skeleton starting from a completely connected undirected
graph. This problem has already been investigated in (Li et al., 2008; Tsamardinos and
Brown, 2008; Li and Wang, 2009; Armen and Tsamardinos, 2011, 2014), but we review it
here for completeness. We construct a hypothesis test with the following null and alterna-

11

Strobl, Spirtes and Visweswaran

(12)

(13)

(14)

(15)

(16)

tive:

H0 : A − B is absent,
H1 : A − B is present.

Now consider the following proposition, where P a(A) denotes the true parents of A:

Proposition 1 (Spirtes et al., 2000) Consider a DAG G which satisﬁes the global directed
Markov property. Moreover, assume that the probability distribution is d-separation faithful.
Then, there is an edge between two vertices A and B if and only if A and B are conditionally
dependent given any subset of P a(A) \ B and any subset of P a(B) \ A.

We thus consider the following two scenarios for the undirected edge A − B:

1.

If A and B are conditionally independent given some subset of P a(A) \ B

or some subset of P a(B) \ A, then A − B is absent.

2.

If A and B are conditionally dependent given any subset of P a(A) \ B

and any subset of P a(B) \ A, then A − B is present.

The following null and alternative are therefore equivalent to (12), where CI oracles are
queried about A and B given all possible subsets of P a(A) \ B and all possible subsets of
P a(B) \ A:

H0 : At least one CI oracle outputs independent,
H1 : All CI oracles output dependent.

Notice that the above hypothesis test is the same as the hypothesis test in (7). We can
therefore bound the p-value of (12) using:

p(cid:48)
A−B

(cid:44) max
i=1,...,q(cid:48)

pA⊥⊥B|Ri,

where Ri ⊆ {Pa(A) \ B} or Ri ⊆ {Pa(B) \ A} and q(cid:48) denotes the total number of such
subsets.

Note that the skeleton discovery phase of the PC algorithm cannot diﬀerentiate between
the parents and children of a particular vertex using its neighbors. However, we can further
bound (15) using the following quantity:

p(cid:48)
A−B ≤ max
i=1,...,q

pA⊥⊥B|Si

(cid:44) pA−B,

where Si ⊆ {N(A) \ B} or Si ⊆ {N(B) \ A} and q denotes the total number of such subsets.
Now assume that the Type II error rate of all CI tests is zero. Then, if the alternative
holds for the CI tests (conditional dependence), then the alternative is accepted. Hence, the
PC algorithm will not remove any of the edges between N (A) and A as well as any of the
edges between N (B) and B. PC therefore performs all necessary CI tests for computing
(16), so upper bounding the Type I error rate for (12) reduces to taking the maximum of
the p-values for all of the CI tests performed by PC regarding A and B. For example,
suppose we measure three random variables A, B and C. Then we obtain p-values after the
PC algorithm tests whether A ⊥⊥ B and A ⊥⊥ B|C. Suppose these p-values are (0.03, 0.04)
so that the PC algorithm with an α threshold of 0.05 determines that A − B is present.
The p-value upper bound of (12) thus corresponds to max {0.03, 0.04} = 0.04.

12

PC with p-values

4.2 Detecting V-Structures

4.2.1 Deterministic Skeleton

The hypothesis testing procedure for directed edges is more complicated than the procedure
for adjacencies. Edges can be oriented in the PC algorithm according to unshielded v-
structures or the orientation rules as described in Section 2.2. Let us ﬁrst focus on the
former and, for further simplicity, let us also assume that 1) we have access to the ground
truth skeleton and 2) no edge is involved in more than one unshielded v-structure (we will
later drop these assumptions in Section 4.2.2). Our task then is to statistically infer the
presence of an unshielded v-structure.

We now present the following null and alternative for each unshielded v-structure after

ﬁnding a triple A − C − B such that A and B are non-adjacent in the skeleton:

H0 : Unshielded A → C ← B is absent,
H1 : Unshielded A → C ← B is present.

(17)

Next, consider the following proposition:

Proposition 2 (Spirtes et al., 2000) Consider the same assumptions as Proposition 1.
Further assume that A, C are adjacent and C, B are adjacent but A, B are non-adjacent.
Then, A and B are conditionally independent given some subset of P a(A)\B which does not
include C or some subset of P a(B)\A which does not include C if and only if A → C ← B.

The following null and alternative is therefore equivalent to (17):

H0 : A and B are conditionally dependent given any subset of P a(A) \ B
which does not include C and any subset of P a(B) \ A which

H1 : A and B are conditionally independent given some subset of P a(A) \ B

which does not include C or some subset of P a(B) \ A which

(18)

does not include C,

does not include C.

The above alternative is reminiscent of the way in which PC determines the presence of an
unshielded v-structure according to Algorithm 6 in the Appendix; speciﬁcally, if C is not in
the set which renders A and B conditionally independent, then C in A − C − B must be a
collider. We however cannot bound the p-value of (18) using CI tests, because conditional
dependence is in the null and conditional independence is in the alternative, as opposed to
vice versa. As a result, we also consider the following proposition:

Proposition 3 Consider the same assumptions as Proposition 2. Then, A and B are
conditionally dependent given any subset of P a(A) \ B containing C and any subset of
P a(B) \ A containing C if and only if A → C ← B.

Proof First notice that P a(A) = {P a(A) \ B} and P a(B) = {P a(B) \ A}, since A and
B are non-adjacent. As a result, we can instead prove that the if and only if statement
holds for P a(A) and P a(B) without loss of generality.

13

Strobl, Spirtes and Visweswaran

For the forward direction, suppose A and B are conditionally dependent given any
subset of P a(A) containing C and any subset of P a(B) containing C. Then A and B are
d-connected given any subset of P a(A) containing C and any subset of P a(B) containing
C by the global directed Markov property. Clearly, C ∈ N (A) and C ∈ N (B), so C must
either be a parent of A and a parent of B, a child of A and a parent of B, a parent of A and
a child B, or a child of A and a child of B. Note that A and B are non-adjacent, so A and
B are d-separated given some subset of P a(A) or some subset of P a(B) by Proposition
1 and d-separation faithfulness. Moreover, the subset must include C if C is a parent of
A and a parent of B, a child of A and a parent of B, or a parent of A and a child B;
otherwise, A and B would be d-connected. As a result, in those three situations, we arrive
at the contradiction that A and B are d-separated given some subset of P a(A) containing
C or some subset of P a(B) containing C. We conclude that C must be a child of A and a
child of B.

For the other direction, if A → C ← B holds, then A and B are d-connected given
any subset of P a(A) containing C and any subset of P a(B) containing C. D-separation
faithfulness then implies that A and B are conditionally dependent given any subset of
P a(A) containing C and any subset of P a(B) containing C.

We can thus equivalently write (18) as:

H0 : A and B are conditionally independent given some subset of P a(A) \ B

containing C or some subset of P a(B) \ A containing C,

H1 : A and B are conditionally dependent given any subset of P a(A) \ B

containing C and any subset of P a(B) \ A containing C.

(19)

We can bound the Type I error rate of the above hypothesis test by taking the maximum
p-value over certain CI tests:

p(cid:48)
γAB|C

(cid:44) max
i=1,..,m(cid:48)

pA⊥⊥B|Mi,

where Mi denotes a subset of P a(A) \ B containing C or a subset of P a(B) \ A containing
C, and m(cid:48) is the total number of subsets Mi. Of course, in practice, we do not know which
vertices are the parents. However, we can also upper bound (19) as follows:

p(cid:48)
γAB|C

≤ max
i=1,..,m

pA⊥⊥B|Ti

(cid:44) pγAB|C ,

(20)

where Ti denotes a subset of N (A) \ B containing C or a subset of N (B) \ A containing
C, and m denotes the total number of subsets Ti. Note that we do not need the zero Type
II error rate assumption for computing (20), since we assume that the skeleton is provided.

4.2.2 Inferred Skeleton

We have considered orienting the colliders, if we have access to the ground truth skeleton.
We now consider the more complex problem of orienting the colliders, if we must also
statistically infer the skeleton.

We again consider the following null and alternative:

H0 : Unshielded A → C ← B is absent,
H1 : Unshielded A → C ← B is present.

(21)

14

PC with p-values

Now, the PC algorithm determines that the alternative holds, if all of the following condi-
tions are true:

1. A and C are conditionally dependent given any subset of P a(A) \ C

2. B and C are conditionally dependent given any subset of P a(B) \ C

and any subset of P a(C) \ A.

and any subset of P a(C) \ B.

3. A and B are conditionally dependent given any subset of P a(A) \ B

containing C and any subset of P a(B) \ A containing C.

We therefore have the following equivalent form of the null and alternative as in (21), if we
assume A and B are non-adjacent:

H0 : At least one condition from (22) does not hold,
H1 : All conditions from (22) hold.

Note that the non-adjacency assumption is reasonable because we did not have enough
Indeed, non-
statistical evidence to invalidate the assumption when we executed (12).
adjacencies are always assumed unless the data suggests that the null of (12) is unlikely.
Now, the alternative of (23) is a series of three logical conjunctions, and the null is a series
of three logical disjunctions as in (10), so the Type I error rate of (23) can be bounded
using the intersection bound:

Pr(Conditions 1, 2, 3 |H0) ≤ Pr(Any one condition |H0)

≤ max{h1, h2, h3}.

We will be using shorthand from here on. We write (23) equivalently as:

H0 : ¬(A − C) ∨ ¬(B − C) ∨ ¬γAB|C,
H1 : (A − C) ∧ (B − C) ∧ γAB|C,

where A − C, B − C, and γAB|C represent Condition 1, 2 and 3 from (22), respectively. We
therefore have a p-value bound of (21) similar to (24):

(cid:16)

Pr

(A − C) ∧ (B − C) ∧ γAB|C|H0
(cid:16)
A − C(cid:12)
(cid:16)
(cid:110)

(cid:12)¬(A − C)
A − C(cid:12)

(cid:12)¬(A − C)

, Pr

Pr

(cid:17)

(cid:17)

(cid:16)

≤ Pr

≤ max

(cid:17)

≤ max{pA−C, pB−C, pγAB|C }.

B − C(cid:12)

(cid:17)

(cid:16)

(cid:12)¬(B − C)

, Pr

γAB|C

(cid:12)
(cid:12)¬γAB|C

(cid:17)(cid:111)

Notice that computing pγAB|C requires N (A) and N (B), not just their respective empirical
estimates (cid:99)N (A) and (cid:99)N (B) which PC can discover. However, we can invoke a zero Type
II error rate assumption in order to ensure that N (A) ⊆ (cid:99)N (A) and N (B) ⊆ (cid:99)N (B) as
explained in detail in Section 4.4, so pγAB|C can still be upper bounded. The assumption

15

(22)

(23)

(24)

(25)

(26)

Strobl, Spirtes and Visweswaran

also ensures that we can upper bound pA−C and pB−C according to Section 4.1. We conclude
that a zero Type II error rate ensures that (26) can be computed.

Next, consider the situation where PC can orient any one edge by using more than one
unshielded v-structure. For example, consider the DAG in Figure 3. In this case, PC can
orient A − C by using either B1 → C or B2 → C (or both); we may therefore want to
take both situations into account. Note that the original PC algorithm always orients an
edge according to one v-structure which it picks arbitrarily according to the ordering of its
computations. We thus only require the bound (26) in this case. However, we will propose
a modiﬁed PC algorithm in Section 5 which takes into account all possible ways to orient
one edge. Now, we can use the following null and alternative for Figure 3 when assuming
that both A and B1 and A and B2 are non-adjacent:

H0 : ¬(A − C) ∨
(cid:16)

H1 : (A − C) ∧

(cid:16)

(cid:17)
[¬(B1 − C) ∨ ¬γAB1|C] ∧ [¬(B2 − C) ∨ ¬γAB2|C]

,

(cid:17)
[(B1 − C) ∧ γAB1|C] ∨ [(B2 − C) ∧ γAB2|C]

.

(27)

We can therefore bound the Type I error rate of (27) as follows, where G = ¬(A − C) and
H = [¬(B1 − C) ∨ ¬γAB1|C] ∧ [¬(B2 − C) ∨ ¬γAB2|C], H1 = ¬(B1 − C) ∨ ¬γAB1|C, and
H2 = ¬(B2 − C) ∨ ¬γAB2|C:

(cid:110)

(cid:110)

(cid:110)

(cid:16)

Pr

(cid:16)

(A − C) ∧
(cid:110)

[(B1 − C) ∧ γAB1|C] ∨ [(B2 − C) ∧ γAB2|C]

(cid:17)

(cid:17)(cid:12)
(cid:12)H0

≤ max

Pr(A − C|G), Pr

(B1 − C) ∧ γAB1|C

≤ max

Pr(A − C|G), Pr

(B1 − C) ∧ γAB1|C|H

(cid:105)

∨
(cid:17)

(cid:104)
(B2 − C) ∧ γAB2|C

(cid:105)(cid:12)
(cid:12)H

(cid:17)(cid:111)

(cid:16)

+ Pr
(cid:17)

(cid:17)(cid:111)

(B2 − C) ∧ γAB2|C|H
(cid:16)

(B2 − C) ∧ γAB2|C|H2

(cid:17)(cid:111)

(28)

(cid:16)(cid:104)

(cid:16)

(cid:16)

= max

Pr(A − C|G), Pr

(cid:16)

(B1 − C) ∧ γAB1|C|H1
(cid:110)
B1 − C(cid:12)
(cid:17)

Pr

(cid:12)¬(B1 − C)

+ Pr
(cid:17)

, Pr(γAB1|C
(cid:111)(cid:111)

≤ max

Pr(A − C|G), max
(cid:110)
B2 − C(cid:12)

Pr

(cid:16)

+ max
(cid:110)
pA−C, max{pB1−C, pγAB1|C } + max{pB2−C, pγAB2|C }

(cid:12)¬(B2 − C)

, Pr(γAB2|C

≤ max

(cid:111)
.

(cid:12)
(cid:12)¬γAB2|C)

(cid:12)
(cid:12)¬γAB1|C)

(cid:111)

Figure 3: Here, one can orient the edge A − C according to the two unshielded v-structures

A → C ← B1 and A → C ← B2.

B1

B2

A

C

16

PC with p-values

More generally, for an arbitrary number, say j, of multiple possible ways to orient A − C
by unshielded v-structures, we have:

(cid:16)

Pr

(A − C) ∧

(cid:16)

(cid:110)

≤ max

pA−C,

i=1

[(B1 − C) ∧ γAB1|C] ∨ ... ∨ [(Bj − C) ∧ γABj |C]
j
(cid:88)

(cid:111)
,
max{pBi−C, pγABi|C }

(cid:17)

(cid:17)(cid:12)
(cid:12)H0

(29)

assuming that B1, . . . , Bj are all non-adjacent to A.

4.3 Orientation Rules

We now consider bounding the p-values of edges which are oriented using the orientation
rules of the PC algorithm. Recall from (1) that the PC algorithm only requires the repeated
application of three orientation rules to be complete. We analyze these three orientation
rules in separate subsections.

4.3.1 First orientation rule

We can construct the hypothesis test for the ﬁrst orientation rule as follows according to
the suﬃcient conditions of the ﬁrst rule in (1):

H0 : ¬(A − B) ∨ ¬(C → A),
H1 : (A − B) ∧ (C → A).

(30)

We again also assume that C and B are non-adjacent. Now the PC algorithm determines
that the alternative holds, if all of the following conditions are true:

1. A − B : A and B are conditionally dependent given any subset of P a(A) \ B and any

subset of P a(B) \ A.

2. C → A : An edge is oriented from C to A under two scenarios. In the ﬁrst, the edge
is oriented because A is the collider in an unshielded v-structure. In the second, the
edge is oriented due to the previous application of an orientation rule.

We thus have a logical conjunction and can bound the Type I error rate using the intersection
bound:

Pr((A − B) ∧ (C → A)|H0) ≤ max{pA−B, pC→A},

where pC→A refers to the p-value bound for the hypothesis test of an unshielded v-structure
or a previously applied orientation rule. Of course, pC→A will be the former when the PC
algorithm begins to execute the orientation rules. More generally, for Ci → A that can
orient A − B where i = 1, ..., j, we have:

(cid:16)

Pr

(A − B) ∧

(cid:105)
(cid:104)
|H0
(C1 → B) ∨ · · · ∨ (Cj → A)

(cid:17)

(cid:110)

≤ max

pA−B,

pCi→A

(cid:111)
,

j
(cid:88)

i=1

(31)

where we require that C1, . . . , Cj are all non-adjacent to B.

17

Strobl, Spirtes and Visweswaran

4.3.2 Second orientation rule

We have the following hypothesis test according to the suﬃcient conditions of the second
rule in (1):

H0 : ¬(A − B) ∨ ¬(A → C → B),
H1 : (A − B) ∧ (A → C → B).

(32)

Hence, by conjunction:

Pr((A − B) ∧ (A → C → B)|H0) ≤ max{pA−B, pA→C→B},

where pA→C→B ≤ max{pA→C, pC→B}. The above Type I error rate can therefore be further
upper bounded by max{pA−B, pA→C, pC→B}. More generally, we have:

Pr

(A − B) ∧

(cid:104)

(cid:105)
(A → C1 → B) ∨ ... ∨ (A → Cj → B)

|H0

(cid:17)

(cid:110)

(cid:111)

(cid:110)

≤ max

pA−B,

pA→Ci→B

≤ max

pA−B,

max{pA→Ci, pCi→B}

(33)

(cid:111)
.

j
(cid:88)

i=1

(cid:16)

j
(cid:88)

i=1

4.3.3 Third orientation rule

We have the following null and alternative by the suﬃcient conditions of the third rule in
(1), assuming that C and D are non-adjacent:

H0 : ¬(A − B) ∨ ¬(A − C → B) ∨ ¬(A − D → B),
H1 : (A − B) ∧ (A − C → B) ∧ (A − D → B).

(34)

We can bound the Type I error rate of the above hypothesis test as follows:

(cid:16)

(cid:110)

Pr

(A − B) ∧ (A − C → B) ∧ (A − D → B)|H0

≤ max{pA−B, pA−C→B, pA−D→B}

≤ max

pA−B, max{pA−C, pC→B}, max{pA−D, pD→B}

(cid:17)

(cid:111)
.

The general case is slightly more complicated than the ﬁrst and second orientation rules.
In this case, we need to control the Type I error rate of accepting at least two paths as
opposed to one. Let the set D include all three-node paths from A to B with the ﬁrst edge
undirected from A to a middle vertex and the second edge directed from the middle vertex
to B such that the ith element of D is:

Di (cid:44) A − Ci → B.

Let us suppose D has a total of n elements and assume that no middle vertex Ci is adjacent
to any other middle vertex. Now, let D(cid:48) be the set containing all of the n choose 2 elements
of D. The ith element in D(cid:48) is therefore:

D(cid:48)
i

(cid:44) {A − Ck → B, A − Cl → B},

18

PC with p-values

(cid:16)

(cid:16)

(cid:104)

r
(cid:88)

i=1

(cid:104)

r
(cid:88)

i=1

where k and l are the distinct indices represented the two chosen middle vertices. Let D(cid:48)
i,1
(cid:1). We then
and D(cid:48)
have:

i,2 be the ﬁrst and second elements in D(cid:48)

i, respectively. Also let r = (cid:0)n

2

Pr

(A − B) ∧

(D(cid:48)

1,1 ∧ D(cid:48)

1,2) ∨ ... ∨ (D(cid:48)

r,1 ∧ D(cid:48)

r,2)

(cid:17)

(cid:105)(cid:12)
(cid:12)H0

(cid:110)

≤ max

pA−B,

Pr(D(cid:48)

i|H0)

(cid:111)
,

where Pr(D(cid:48)

i|H0) (cid:44) p{A−Ck→B,A−Cl→B} and is bounded as follows:

Pr(D(cid:48)

i|H0) ≤ max{pA−Ck→B, pA−Cl→B}

≤ max{pA−Ck , pCk→B, pA−Cl, pCl→B}
(cid:44) ¨Pr(D(cid:48)

i|H0),

We therefore have:

Pr

(A − B) ∧

(D(cid:48)

1,1 ∧ D(cid:48)

1,2) ∨ ... ∨ (D(cid:48)

r,1 ∧ D(cid:48)

r,2)

(cid:17)

(cid:105)(cid:12)
(cid:12)H0

(cid:110)

≤ max

pA−B,

¨Pr(D(cid:48)

i|H0)

(cid:111)
.

(35)

4.4 Summary and Analysis of the Bounds

We derived several bounds for edge orientation as summarized in Table 1. We created the
bounds by engineering speciﬁc hypothesis tests and successively applying the union and
intersection bounds accordingly. Note that j and r are usually very small in sparse graphs.
One may now wonder whether PC can actually control the bounds listed in Table 1 (we
say that a quantity can be controlled, if the quantity can be upper bounded). Recall that we

Table 1: P-value bounds for all of the edge types in a CPDAG. Note that Si ⊆ {N (A) \ B}

or Si ⊆ {N (B) \ A}.

P-Value Bound

Equation Num.

Edge Type

Undirected

Unshielded v-structure max

First orientation rule

Second orientation rule max

Third orientation rule

maxi pA⊥⊥B|Si

(cid:110)
pA−C, (cid:80)j
i=1 max{pBi−C, pγABi|C }
(cid:110)
pA−B, (cid:80)j

i=1 pCi→A

max

(cid:111)

(cid:111)

(cid:110)
pA−B, (cid:80)j
(cid:110)
pA−B, (cid:80)r

max

i=1 max{pA→Ci, pCi→B}

(cid:111)

¨Pr(D(cid:48)

i|H0)

i=1

(cid:111)

(16)

(29)

(31)

(33)

(35)

19

Strobl, Spirtes and Visweswaran

provided a rough, aﬃrmative answer to the question in Sections 4.1 and 4.2.2 by assuming
a zero Type II error rate. We now spell out a more detailed answer via a theorem whose
proof builds on the argument of Theorem 4 in (Armen and Tsamardinos, 2014).

Theorem 4 Suppose that the PC algorithm is applied to a sample from P represented by
DAG G. If we have:

1. P is d-separation faithful to G,

2. The Type II error rate is zero,

3. The PC algorithm also tests whether any two non-adjacent vertices A, B with common
neighbor C are conditionally dependent given any subset of P a(A) \ B containing C
and any subset of P a(B) \ A containing C,

then all of the p-value bounds in Table 1 can be controlled using the p-values of the CI tests
executed by PC.

Proof Consider any two vertices A and B. Algorithm 1 starts with a fully connected
graph, so we have B ∈ (cid:99)N (A) and A ∈ (cid:99)N (B) in the beginning. Note that Algorithm 1
executes testA⊥⊥B|S for all S ⊆ (cid:99)N (A) \ B and for all S ⊆ (cid:99)N (B) \ A. The zero Type II error
rate ensures the following: if the alternative holds, then the alternative is accepted. As a
result, Algorithm 1 will not remove any vertices adjacent to A and any vertices adjacent to
B with a zero Type II error rate. Hence, we always have {N (A) \ B} ⊆ { (cid:99)N (A) \ B} and
{N (B) \ A} ⊆ { (cid:99)N (B) \ A}. Algorithm 1 therefore must eventually execute testA⊥⊥B|S for
all S ⊆ {N (A) \ B} and for all S ⊆ {N (B) \ A}, so (16) can be controlled.

For (29), the p-value bounds for undirected edges can already be controlled by the
previous paragraph. We must now argue that pγAB|C can be controlled. Let C be a collider
between non-adjacent vertices A and B. Now notice that C ∈ N (A) ⊆ (cid:99)N (A) and C ∈
N (B) ⊆ (cid:99)N (B), so Algorithm 2 must execute testA⊥⊥B|S for all S ⊆ {N (A) \ B} containing
C and for all S ⊆ {N (B) \ A} containing C. Hence pγAB|C can be controlled.

Now the p-value bounds (31), (33) and (35) can be controlled trivially because the p-

value bounds for (15) and (29) can be controlled.

In other words, PC can control the bounds in Table 1 with some additional CI tests and a
zero Type II error rate.

Of course, the Type II error rate is never zero in practice, but this becomes less of an
issue as the sample size increases. We may also consider reducing the Type II error rate by
simultaneously implementing three strategies:

1. Use a liberal (higher) α threshold. We for example often use an α threshold of 0.20 in
the experiments. This is the simplest strategy which decreases the Type II error rate
but also increases the Type I error rate. However, we can then control the Type I error
rate post-hoc with an FDR controlling procedure. Of course, setting the α threshold
too high will prevent the PC algorithm from terminating within a reasonable amount
of time as well as loosen the p-value bounds, since the CI tests will fail to explain
away many edges. We therefore cannot rely entirely on this ﬁrst strategy.

20

PC with p-values

2. Use hypothesis tests whose p-value bounds are robust to Type II errors. The hy-
pothesis tests in Section 4.4 are in fact robust to such errors due to the intersection
bound as explained in detail in Appendix A.2. Brieﬂy, we can also reasonably con-
sider modifying the null hypotheses of (25), (30), (32) and (34) to “no edges between
any of the vertices.” This corresponds to converting the logical disjunctions in the
null of (10) into conjunctions which in turns leads to a less robust p-value bound
involving the minimum of a set of p-values instead of the maximum. As a result,
under-estimating one p-value in the p-value set due to Type II error(s) can cause PC
to also under-estimate the bound of (25), (30), (32) or (34).

3. Modify the PC algorithm to prevent and catch many Type II errors.

The last strategy is more complex, so we discuss it in detail in the next section.

5. The PC Algorithm with P-Values

We now propose a modiﬁed PC algorithm called PC with p-values (PC-p) that reduces the
inﬂuence of Type II errors by preventing and catching potential Type II errors. At the same
time, PC-p is correct - the algorithm operates diﬀerently than PC, but it maintains PC’s
desirable soundness and completeness properties.

The PC-p algorithm involves two ideas. First, PC-p performs skeleton discovery with the
same skeleton discovery procedure used in the PC-stable algorithm (Colombo and Maathuis,
2014). This procedure ensures that the algorithm does not skip some CI tests due to Type
II errors and variable ordering. The second idea behind PC-p involves a modiﬁcation to the
procedure for propagating edge orientations. Speciﬁcally, if two edge orientations conﬂict,
PC-p admits bidirected edges instead of over-writing previous orientations like PC. PC-p
then unorients the bidirected edges as well as the directed edges which were directly used to
infer the presence of the bidirected edges. The algorithm subsequently labels the resulting
undirected edges as “ambiguous” which ensures that PC-p does not orient additional edges
using the ambiguous edges. Indeed, the PC-p algorithm uses conﬂicts in edge orientation
to detect potential Type II errors and prevent the propagation of the errors throughout the
graph. In practice, we ﬁnd that these two modiﬁcations to the PC algorithm help PC-p
with the BY estimator achieve more accurate strong estimation and control of the FDR
than PC, as we will see in Section 6.

We now describe the PC-p algorithm in detail; however, we will not describe the com-
putation of the p-value upper bounds until Section 5.5 in order to keep the presentation
clear. We have divided the PC-p algorithm into Algorithms 1, 2, 3 and 4, where the ﬁrst
three procedures correspond to Algorithms 5, 6 and 7 of the original PC algorithm.

5.1 Skeleton Discovery

We ﬁrst consider skeleton discovery. The original PC algorithm uses Algorithm 5 to discover
the skeleton. However, Algorithm 5 can cause the sample version of the PC algorithm to skip
some CI tests due to variable ordering and Type II errors. For example, consider the causal
graph in Figure 4a as ﬁrst presented in (Colombo and Maathuis, 2014). In this example,
suppose the CI tests correctly determine that A ⊥⊥ B and B ⊥⊥ D|{A, C} but incorrectly
determine that C ⊥⊥ D|{A, E}. The incorrect inference is a Type II error, since C and D

21

Strobl, Spirtes and Visweswaran

are adjacent in the true graph. Now consider the following ordering of variables for the PC
algorithm: order1(X) = (A, D, B, C, E). In this case, the ordered pair (D, B) is considered
before (D, C) in Algorithm 5, since (D, B) comes earlier in order1(X). The PC algorithm
removes D − B because a CI test determines that D ⊥⊥ B|{A, C} and {A, C} is a subset of
N (D) = {A, B, C, E}. Next, D − C is considered and erroneously removed because a CI
test determines that D ⊥⊥ C|{A, E} and {A, E} is a subset of N (D) = {A, C, E}. We thus
ultimately obtain the skeleton in Figure 4b with order1(X).

Now consider an alternative ordering of the variables: order2(X) = (A, C, D, B, E). In
this case, (C, D) is considered before (D, B) in Algorithm 5, and the algorithm erroneously
removes C − D. Next, the algorithm considers D − B but {A, C} is not a subset of N (D) =
{A, B, E}, so D − B remains. Even when the PC algorithm eventually also considers the
same undirected edge as B − D, {A, C} is again not a subset of N (B) = {C, D, E}, so
B − D remains. In other words, (C, D) is considered ﬁrst in order2(X) which causes C
to be removed from N (D). Algorithm 5 therefore never executes testB⊥⊥D|{A,C}. We thus
ultimately obtain the skeleton in Figure 4c with order2(X).

The previous two examples show that the Type II error of incorrectly determining that
C ⊥⊥ D|{A, E} leads PC to infer two diﬀerent skeletons due to diﬀerences in variable order-
ing. Clearly, we would like to eliminate the dependency of skeleton discovery on variable
ordering and also reduce its dependency on Type II errors at the same time. Fortunately,
Colombo and Maathius proposed such a modiﬁcation of Algorithm 5 as outlined in Algo-
rithm 1. The key diﬀerence between Algorithm 5 and 1 involves the for loop in steps 5-7 of
Algorithm 1 which computes and stores the adjacency sets after each new conditioning set
size. As a result, an incorrect edge deletion due to a Type II error on line 16 of Algorithm 1
no longer eﬀects which CI tests are performed for other pairs of variables with conditioning
set size l. Indeed, the algorithm only modiﬁes the adjacency sets when it increases the con-
ditioning set size. Colombo and Maathius proved that Algorithm 1 is order-independent.
We review the proof here, since it is informative:

Proposition 5 (Colombo and Maathuis, 2014). The skeleton resulting from Algorithm 1
is order-independent.

(a)

(b)

(c)

A

B

A

B

A

B

C

E

D

C

E

D

Figure 4: An example of a situation when PC infers diﬀerent skeletons due to a Type II
error and two variable orderings.
(a) The true causal graph, (b) the skeleton
inferred by PC from order1(X), (c) the skeleton inferred by PC from order2(X).

C

E

D

22

PC with p-values

Proof Consider the removal or retention of some undirected edge A−B at some condition-
ing set size l. The ordering of the variables determines the order in which the edges (line 9)
and subsets S ⊆ a(A) and S ⊆ a(B) (line 11) are considered. However, by construction,
the order in which the edges are considered does not aﬀect the sets a(A) and a(B).

If there is at least one subset S of a(A) or a(B) such that A ⊥⊥ B|S, then any ordering
of the variables will ﬁnd a separating set for A and B (but diﬀerent orderings may lead
to diﬀerent separating sets as illustrated in Example 2 of (Colombo and Maathuis, 2014)).
Conversely, if there is no subset S(cid:48) of a(A) or a(B) such that A ⊥⊥ B|S(cid:48), then no ordering
will ﬁnd a separating set.

Hence, any ordering of the variables leads to the same edge deletions and therefore to

the same skeleton.

In other words, modifying the adjacency sets only when changing the conditioning set
size prevents PC-p from skipping some CI tests during skeleton discovery because of Type II
errors and variable ordering. As a result, Algorithm 1 enables PC-p to perform more of the
required CI tests than Algorithm 5 in order to correctly upper bound the p-value of (12).
However, notice that Algorithm 1 does not prevent all Type II errors from eﬀecting the
skeleton. The edge C − D is for example eliminated in Figure 4 regardless of the ordering
because of the erroneous conclusion that C ⊥⊥ D|{A, E}. As a result, we have C (cid:54)∈ (cid:99)N (D)
which may lead to under-estimation of the p-value bounds for undirected edges connected
to D. We will nonetheless see in Section 6 that Algorithm 1 does help PC-p achieve tighter
estimation and control of the FDR than the original skeleton discovery procedure, since
Algorithm 1 eliminates the inﬂuence of at least some Type II errors.

5.2 Unshielded V-Structures

We now describe Algorithm 2, where we use the circle edge endpoint “◦” as a meta-symbol
representing either a tail or an arrowhead. In Algorithm 2, PC-p orients edges according
to all unshielded v-structures in line 3, even if two v-structures conﬂict with each other in

(a)

B

(b)

B

(c)

B

A

C

A

C

A

C

D

E

D

E

D

E

Figure 5: Example of how Algorithm 2 deals with conﬂicting edge orientations. a) The
ground truth, b) the inferred graph with two v-structures A → B ← C and
D → C ← B that lead to the bi-directed edge B ↔ C, and c) the ﬁnal graph
after unorienting both v-structures.

23

Strobl, Spirtes and Visweswaran

the direction of a particular edge. In the case of conﬂict, PC-p admits a bidirected edge
instead of favoring one particular direction over the other. The algorithm then unorients all
v-structures involving the bidirected edges and labels the unoriented edges as “ambiguous”
in line 23 because bidirected edges may result from a Type II error. For example, consider
the ground truth in Figure 5a and assume that Algorithm 1 correctly discovers all of the
undirected edges. Moreover, assume Algorithm 1 correctly ﬁnds a separating set of B
and D that does not contain C but incorrectly ﬁnds a separating set of A and C that
does not contain B. The latter is a Type II error, since the alternative should have been
accepted rather than rejected when conditioning on a subset not containing B.
In this
case, PC-p ﬁrst orients the edges according to Figure 5b. However, notice that the two
unshielded v-structures conﬂict with each other due to the bidirected edge B ↔ C, and
PC-p cannot determine which v-structure admitted the Type II error. As a result, the
algorithm unorients all of the edges in both v-structures as in Figure 5c. PC-p then labels
the three unoriented edges as “ambiguous” so that the algorithm does not orient any other
undirected edges based on these three edges using the orientation rules. The labeling thus
prevents the algorithm from propagating Type II errors by orienting additional edges based
on the erroneous directions.

5.3 Orientation Rules

Notice that Algorithm 7 uses “else if” statements instead of all “if” statements. The “else
if” approach is of course faster, but it also causes PC to ignore any interactions between the
orientation rules in the sense that, if one rule orients an edge, then no other rule can orient
an edge. PC-p performs the orientation rules according Algorithm 3 which uses the “if”
approach to attempt to apply all three orientation rules to each non-ambiguous undirected
edge. Now, if bidirected edges exist after the rules are applied, then Algorithm 3 unorients
the edge as well as all edges involved in the suﬃcient conditions of the associated oientation
rules in lines 16-18. The algorithm then labels the unoriented edges as “ambiguous” in line
19 similar to unshielded v-structure orientation in Section 5.2. For example, in Figure 6,
rule 1 of PC-p induces a bidirected edge between A − B, so PC-p unorients and labels all
directed edges which satisfy the suﬃcient conditions of rule 1 as ambiguous; these include
D → A, C → A, E → B, and F → B.

5.4 Analysis of PC-p

We now have the following analysis of the PC-p algorithm:

Figure 6: Here, a bidirected edge between A and B results from the application of rule 1.
PC-p therefore unorients and labels all edges in the above graph as “ambiguous”
according to the suﬃcient conditions of rule 1.

D

A

E

C

B

F

24

PC with p-values

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

Data: X n, α
Result: (cid:98)G, P 1, S, I

1 Form a completely connected undirected graph (cid:98)G on the variable set in X n
2 l = −1
3 repeat
4

l = l + 1
for each variable A in (cid:98)G do

a(A) ← (cid:99)N (A)

end
repeat

Select a new ordered pair of variables (A, B) that are adjacent in (cid:98)G and
satisfy |a(A) \ B| ≥ l
repeat

Choose a new set S ⊆ {a(A) \ B} with |S| = l
p ← p-value from testA⊥⊥B|S
if p ≤ α then

Insert p into P 1

AB and P 1

BA

Delete A − B from (cid:98)G
Empty P 1
AB and P 1
BA
Insert S into SAB and SBA

else

end

until A − B is deleted from (cid:98)G or all S ⊆ {a(A) \ B} with |S| = l have been
considered ;

until all ordered pairs of adjacent variables (A, B) in (cid:98)G with |a(A) \ B| ≥ l have
been considered ;

22 until all ordered pairs of adjacent variables (A, B) in (cid:98)G satisfy |a(A) \ B| ≤ l;
23 for each nonempty P 1
AB}
24

P 1
Place the same unique identiﬁer for A − B into IAB and IBA

AB ← max{P 1

AB in P 1 do

25
26 end

Algorithm 1: Skeleton Discovery

25

Strobl, Spirtes and Visweswaran

Data: X n, (cid:98)G, P 1, S, I
Result: (cid:98)G, P 2, I

1 for all ordered pairs of non-adjacent variables (A, B) with common neighbor C do
2

if C (cid:54)∈ any set in SAB then

Replace A ◦−◦ C ◦−◦ B with A ◦→ C ←◦ B
l = 0
repeat

l = l + 1
repeat

Choose a new set S ⊆ (cid:99)N (A) including C or S ⊆ (cid:99)N (B) including C
with |S| = l
p ← p-value from testA⊥⊥B|S
Insert p into P (cid:48)(cid:48)

until all S ⊆ (cid:99)N (A) including C and all S ⊆ (cid:99)N (B) including C with
|S| = l have been considered ;

until all S ⊆ (cid:99)N (A) including C and all S ⊆ (cid:99)N (B) including C satisfy
|S| ≤ l ;
Insert max{P 1
Insert max{P 1
Empty P (cid:48)(cid:48)

AC, P (cid:48)(cid:48)} into P (cid:48)
BC, P (cid:48)(cid:48)} into P (cid:48)

BC

AC

Unorient the edge to A − C in (cid:98)G(cid:48)
For each additional edge directed to A in (cid:98)G, unorient the edge in (cid:98)G(cid:48)
For each additional edge directed to C in (cid:98)G, unorient the edge in (cid:98)G(cid:48)
Label the unoriented edges as “ambiguous” in (cid:98)G(cid:48)

end

16
17 end
18 (cid:98)G(cid:48) ← (cid:98)G
19 for each A ↔ C in (cid:98)G do
20

23
24 end
25 (cid:98)G ← (cid:98)G(cid:48)
26 for each A → C in (cid:98)G do
AC ← max
27

P 2
if one p-value in P (cid:48)

P 1

(cid:110)

28

(cid:111)

AC, sum[P (cid:48)
AC then

AC]

Place the same unique identiﬁer into IAC and IBC for unshielded collider
A → C ← B

else if more than one p-value in P (cid:48)
Place a unique identiﬁer into IAC

AC then

end

32
33 end

Algorithm 2: Unshielded V-structures

3

4

5

6

7

8

9

10

11

12

13

14

15

21

22

29

30

31

26

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

PC with p-values

Data: (cid:98)G, P 1, P 2, I
Result: (cid:98)G, P 2, I

1 repeat

(cid:98)G(cid:48) ← (cid:98)G
if A − B non-ambiguous and ∃i s.t. Ci → A with Ci and B non-adjacent in (cid:98)G
then

Replace A ◦−◦ B in (cid:98)G(cid:48) with A ◦→ B
P (cid:48)

A,B, sum{P 2

A,B ← sum

P (cid:48)

(cid:110)

CiA, ∀i s.t. Ci → A with Ci, B non-adjacent}

(cid:111)

end
if A − B non-ambiguous and ∃i s.t. A → Ci → B in (cid:98)G then

Replace A ◦−◦ B in (cid:98)G(cid:48) with A ◦→ B
(cid:104)
P (cid:48)
AB, sum

AB ← sum

max{P 2

P (cid:48)

(cid:110)

ACi

, P 2

CiB}, ∀i s.t. A → Ci → B

(cid:105)(cid:111)

end
if A − B non-ambiguous and ∃i, j s.t. A − Ci → B, A − Cj → B with A − Ci
and A − Cj non-ambiguous, and Ci and Cj non-adjacent in (cid:98)G then

(cid:110)

Replace A ◦−◦ B in (cid:98)G(cid:48) with A ◦→ B
(cid:104)
, P 2
P (cid:48)
Cj B}, ∀i, j s.t. A − Ci →
AB, sum
AB ← sum
B, A − Cj → B with A − Ci and A − Cj non-ambiguous, and Ci and Cj
non-adjacent

CiB, P 1

max{P 1

, P 2

ACj

ACi

(cid:105)(cid:111)

P (cid:48)

end
for each A ↔ B in (cid:98)G(cid:48) do

Unorient to A − B in (cid:98)G(cid:48)
For each edge in the suﬃcient conditions of each orientation rule that led to
the creation of a directed edge to A in (cid:98)G, unorient the edge in (cid:98)G(cid:48)
For each edge in the suﬃcient conditions of each orientation rule that led to
the creation of a directed edge to B in (cid:98)G, unorient the edge in (cid:98)G(cid:48)
Label the unoriented edges as “ambiguous” in (cid:98)G(cid:48)

end
(cid:98)G ← (cid:98)G(cid:48)
for each A → B in (cid:98)G do

Place a unique identiﬁer into IAB
AB, P (cid:48)
P 2

AB ← max{P 1

AB}

end
Empty P (cid:48)

26
27 until there are no more edges to orient;
28 for each non-empty cell P 1
29
30 end

AB s.t. P 2

BA ← P 1

AB, P 2

P 2

AB

AB and P 2

BA are empty do

Algorithm 3: Orientation Rules

27

Strobl, Spirtes and Visweswaran

Data: (cid:98)G, P 2, I, α, q
Result: (cid:92)F DRBY (α), (cid:98)G∗
// Estimation

of unique identiﬁers in I

// Control

identiﬁers in I

1 (cid:92)F DRBY (α) ← Solution of 2 using threshold α and m corresponding to the number

2 α∗ ← Solution of 3 using FDR level q and m corresponding to the number of unique

3 (cid:98)G∗ ← (cid:98)G with edges associated with p-values above α∗ eliminated

Algorithm 4: FDR Estimation and Control

Theorem 6 The PC-p algorithm with a CI oracle is sound and complete.

Proof PC is sound and complete, so it is enough to prove that PC-p and PC will perform
the exact same edge deletion and edge orientation operations with a CI oracle. Note that
Algorithm 1 has already been shown to be sound and complete up to skeleton discovery
(Colombo & Maathius 2014). Algorithm 1 will therefore perform the exact same edge
deletions as Algorithm 5 with a CI oracle. Now, Algorithm 2 will also perform the same
edge orientations as Algorithm 6 with a CI oracle, since there will never be conﬂicting edge
orientations. Lastly, for Algorithm 3, if there exists an edge that can be oriented by more
than rule, then the edge must be oriented in the same direction by the other two rules.
Algorithm 3 therefore returns the same edge orientations as Algorithm 7. We have proved
equivalence in outputs of Algorithms 1, 2 and 3 of PC-p to Algorithms 5, 6 and 7 of PC,
respectively. Algorithm 4 is not involved in graph structure discovery.

The output of PC-p is therefore equivalent to the output of PC in the large sample limit
with a consistent CI test, even though PC-p performs more operations than PC.

5.5 Computation of the P-Values

We now address the issue of computing the upper bounds of the p-values. Let us ﬁrst
consider Algorithm 1. Algorithm 1 takes as input the dataset X n and the signiﬁcance
threshold α. The algorithm then stores the p-values of all signiﬁcant CI tests in cell P 1

Figure 7: An example of a situation where two of PC’s orientation rules, speciﬁcally rules 2
and 3, can orient one undirected edge (A − B) in the same direction. In this case,
PC oriented all of the currently directed edges using unshielded v-structures.

F

C

D

E

A

B

28

PC with p-values

when it reaches line 14. Notice that the algorithm stores the p-values of all signiﬁcant tests
involving A and B in both P 1
BA. Algorithm 1 next computes the maximum over
the p-values for all surviving edges in line 24 as in (16).

AB and P 1

Algorithm 2 takes P 1 from Algorithm 1 as input. Moreover, unlike Algorithm 6 of PC,
Algorithm 2 also takes as input the dataset X n, since PC-p must apply (29) in order to
obtain the upper bounds of the p-values for oriented unshielded v-structures. Indeed, Algo-
rithm 2 executes testA⊥⊥B|S for all S ⊆ (cid:99)N (A) containing C and all S ⊆ (cid:99)N (B) containing
C in steps 4-12 for each A − C − B such that A and B are non-adjacent and C (cid:54)∈ SAB. Now,
Algorithm 2 ultimately stores all of the p-values needed to compute pγAB|C as in (20) in P (cid:48)(cid:48)
via line 10. Algorithm 2 then stores the maximum over pA−C and pγAB|C in P (cid:48)
BC instead
of P (cid:48)
AC in line 13. A similar set of operations eventually stores the maximum over pB−C
and pγAB|C into P (cid:48)
AC in line 14. Note that multiple elements can enter into P (cid:48)
AC
when multiple v-structures can orient one edge. Finally, in line 27, Algorithm 2 takes the
maximum over P 1
AC to obtain pA→C in
P 2 according to (29) and similarly takes the maximum over P 1
BC to
obtain pB→C in P 2.

AC as returned from Algorithm 1 and the sum of P (cid:48)

BC and the sum of P (cid:48)

BC and P (cid:48)

Algorithm 3 takes P 2 from Algorithm 2 as input. Next, in rule 1, Algorithm 3 adds
up the p-values associated with Ci → A, ∀i and places the result in P (cid:48)
AB in line 5 for
computing (31). Then, Algorithm 3 sums over the maxima of pA→Ci and pCi→B in rule
2 ∀i s.t. A → Ci → B in line 9 for ultimately computing (33). Subsequently, in rule
3, Algorithm 3 ﬁnds all n edges such that A − Ci → B. The algorithm then ﬁnds all of
the n choose 2 pairs, say r of them. For each pair, say A − C1 → B and A − C2 → B,
Algorithm 3 computes pA−C1→B and pA−C2→B as the maximum over pA−C1 and pC1→B
and the maximum over pA−C2 and pC2→B, respectively. Algorithm 3 next sums the p-values
over all r pairs in line 13 for computing (35). Note that Algorithm 3 also takes an outer-
sum involving P (cid:48)
AB in lines 5, 9 and 13 of rules 1, 2 and 3, respectively; these summations
correspond to logical disjunctions when multiple orientation rules can orient one edge in the
same direction. For example, rules 2 and 3 can orient A − B in the same direction in Figure
7. Two applications of rule 1 can also orient A − B in the same direction in Figure 6, if
we remove one of the unshielded v-structures from the graph. Now, for all non-ambiguous
edges, Algorithm 3 then stores the maximum over the p-values from Algorithm 1 and P (cid:48)
into P 2 in line 24. This process is repeated until no more edges can be oriented. Algorithm
3 ﬁnally transfers the p-values of all of the remaining undirected edges in (cid:98)G from P 1 to P 2
in lines 28- 30. The algorithm therefore eventually outputs all of the ﬁnal p-values in P 2 as
desired.

5.6 Controlling the False Discovery Rate

PC-p controls the FDR per hypothesis test as opposed to per edge, since the algorithm
can sometimes orient two edges according to the same hypothesis test during unshielded v-
structure discovery. Indeed, controlling the p-values per edge as opposed to per hypothesis
test can result in overly conservative FDR estimation or control because an FDR estimator
or controlling procedure may count the p-value of one hypothesis test multiple times.

PC-p keeps track of each distinct hypothesis test in Algorithms 1, 2 and 3 by using
indexing cell I as follows. First, Algorithm 1 assigns the same, unique identiﬁer to the p-

29

Strobl, Spirtes and Visweswaran

value bounds in both PAB and PBA in line 25. Next, if we have one v-structure A → C ← B
that orients A − C and C − B, then Algorithm 2 associates both A → C and C ← B with
the same hypothesis test and therefore the same identiﬁer in line 29. On the other hand,
if multiple unshielded v-structures can orient one edge, then Algorithm 2 assigns the edge
a unique identiﬁer in line 31, since a unique hypothesis test exists per edge in this case.
Algorithm 3 ﬁnally assigns a unique identiﬁer to each newly oriented edge in line 23 because
each newly oriented edge also corresponds to a distinct hypothesis test.

We can now use Algorithm 4 to estimate and control the FDR using the identiﬁers in
I and the p-value bounds in P 2 as returned from Algorithm 3. Algorithm 4 estimates
the FDR by solving 2 to obtain (cid:92)F DRBY , where m corresponds to the number of unique
identiﬁers in I. The algorithm subsequently controls the FDR by solving 3 to obtain α∗.
Algorithm 4 then eliminates all edges with p-values below α∗ in P 2 in order to obtain (cid:98)G∗;
this process ensures that the expected FDR does not exceed q in (cid:98)G∗.

5.7 Conclusion

We wrap-up this section with the following theorem:

Theorem 7 Consider the same assumptions as Theorem 4. Then PC-p achieves conser-
vative point estimation and strong control of the FDR across the edges in (cid:98)G.

Proof We have already shown that PC-p can control the p-values of all of the edges in (cid:98)G
from Theorem 4. Estimation follows because the solution of 2 achieves conservative point
estimation of the FDR at threshold α when the p-values are controlled (Benjamini and
Yekutieli, 2001). Similarly, control follows because eliminating the edges associated with
p-values above α∗ as obtained from 3 achieves strong control of the FDR at level q when
the p-values are in turn controlled (Benjamini and Yekutieli, 2001).

The PC-p algorithm thus corresponds to a valid method for estimating and controlling the
FDR in the estimated CPDAG.

Note ﬁnally that PC-p takes slightly longer than original PC to complete because it
performs extra computations. However, PC-p runs at approximately the same speed as
PC-stable, since v-structure detection and orientation rule application take an inﬁnitesimal
amount of time compared to skeleton discovery.

6. Experiments

6.1 Algorithms and Metrics

We evaluated six algorithms:

1. PC-p,

2. PC-p without stabilization in the skeleton discovery procedure,

3. PC-p without ambiguous labelings during v-structure orientation and orientation rule

application (PC-p without ambiguation),

4. PC-p without both stabilization and ambiguation,

30

PC with p-values

5. PC-p without hypothesis tests with robust p-value bounds − we chose the null hy-
potheses to be a series of logical conjunctions so that no edges are present between
any of the variables. As a result, the p-values take on minimal values as described in
Appendix A.2. We call this procedure PC-p without robust p-values.

6. The original PC algorithm with p-value computation − that is, we do not incorporate
stabilization, and the algorithm arbitrarily over-writes edge orientations. We compute
p-values according to the v-structure or rule which ultimately orients each edge in the
CPDAG. The algorithm also performs some additional CI tests in order to compute
20 as described in Section 4.1.

We ran these six algorithms because they are the only algorithms that allow us to compute
the FDR across the entire CPDAG from the estimated p-values.

We assessed the FDR of the above six algorithms in detail using control and estimation
bias3. An algorithm exhibits low control bias at FDR level q when an FDR controlling
procedure can accurately eliminate edges in the CPDAG using the p-values so that the
FDR is in fact q. On the other hand, an algorithm exhibits low estimation bias when an
FDR estimate closely matches the true FDR of the CPDAG. Notice that both control and
estimation bias are important and can serve diﬀerent purposes. As a result, we prefer an
algorithm that exhibits both low control and estimation bias.

We used the mean of the following quantities to assess control bias:

uc((cid:92)F DRBY , q) := max{F DR(α∗) − q, 0},
oc((cid:92)F DRBY , q) := max{q − F DR(α∗), 0},

(36)

where uc((cid:92)F DRBY , q) denotes under-control at FDR level q with the BY FDR estimate,
and oc((cid:92)F DRBY , q) similarly denotes over-control.
In the experiments, we varied q from
[0.001, 0.1] using 100 equispaced intervals. Note that we compute both under-control and
over-control per CPDAG. A method achieves strong control when the mean under-control
taken across the hypothesis tests is zero (Armen and Tsamardinos, 2014). Moreover, the less
the mean over-control, the tighter the strong control. As a result, achieving a lower mean
under-control is more important than achieving a lower mean over-control. We therefore say
that one method outperforms another if the method achieves a lower mean under-control
while also maintaining a reasonably low mean over-control.

We used the mean of the following similar quantities for estimation bias:

ue((cid:92)F DRBY , α) := max{F DR(α) − (cid:92)F DRBY (α), 0},
oe((cid:92)F DRBY , α) := max{(cid:92)F DRBY (α) − F DR(α), 0},

(37)

where ue((cid:92)F DRBY , α) denotes under-estimation at threshold level α with the BY FDR esti-
mate, and oe((cid:92)F DRBY , α) similar denotes over-estimation. We varied the α threshold from
[1E-10, 0.1] with 100 equispaced intervals in the experiments. Now, we say that estimation
is conservative in a α threshold region when the underestimation is zero. Moreover, the

3. We also measured the false negative rate using the structural Hamming distance as a metric in Figure

19 of the Appendix.

31

Strobl, Spirtes and Visweswaran

greater the over-estimation in a p-value threshold region, the more conservative the esti-
mate. A method should conservatively estimate the FDR but not do so over-conservatively.
As a result, achieving lower under-estimation is more important than achieving lower over-
estimation, and one method outperforms another if the method achieves a lower mean
under-estimation while maintaining a reasonably low mean over-estimation.

Below, we report the relative performance diﬀerences of the six algorithms in recovering
the CPDAG at a liberal α threshold of 0.20, since this threshold consistently provided a nice
tradeoﬀ between p-value bound looseness and low Type II error rates. We have reported
the results using other α thresholds of 0.01, 0.05, 0.10, and 0.15 or 0.50 in Figures 12-15 of
Appendix A.3, with similar relative performance diﬀerences between the algorithms. Figures
16-18 in the Appendix also contain results for skeleton discovery, where we compared the
original skeleton discovery procedure of PC against the same procedure with stabilization.
As expected, the stabilization procedure improved performance. We ﬁnally provide results
with the more commonly used structural Hamming distance in 19 of the Appendix; here,
PC-p achieved superior performance by conservatively estimating the graph.

Note that for the simulations in Sections 6.2 and 6.3, we generated the DAGs using the
TETRAD V package (version 5.2.1) by drawing uniformly over all DAGs with a maximum
in-degree of 2 and a maximum out-degree of 2. We then converted each of the DAGs
to linear non-recursive SEM-IEs by 1) drawing the linear coeﬃcients from independent
standard normal distributions, and 2) setting independent Gaussian distributions over the
error terms with standard deviations also drawn from the standard normal. Each linear
SEM-IE with the error distributions therefore induced a multivariate Gaussian distribution
across the observed variables. We ﬁnally ran all of the six algorithms using Fisher’s z-test
with a liberal α threshold of 0.20 and a maximum conditioning set size of 2.

6.2 Low Dimensional Inference

We generated 30 DAGs by drawing uniformly over all DAGs with 20 vertices. We converted
each of the DAGs to 5 linear non-recursive SEM-IEs. We subsequently created 5 datasets
using each linear SEM-IE with sample sizes of 100, 500, 1000, 5000, and 10000. We therefore
created a total of 30 × 5 × 5 = 750 datasets.

We analyzed the ability of the algorithms in correctly estimating the CPDAG in terms
of the four metrics proposed in Section 6.1 as well as the FDR values. Results as averaged
over DAGs, parameters and sample sizes are summarized in Figure 8. We assessed the
signiﬁcance of all inter-algorithm diﬀerences using paired Wilcoxon signed rank tests. PC-p
obtained lower mean FDR values than PC-p without robust p-values (Figures 8a and 8b; z =
-4.782, p = 1.734E-6), PC-p without stabilization (z = -4.371, p = 1.238E-5), PC-p without
ambiguation (z = -4.782, p = 1.734E-6), PC-p without both stabilization and ambiguation
(z = -4.782, p = 1.734E-6), and PC original (z = -4.433, p = 9.316E-6). Moreover, PC-p
achieved signiﬁcantly lower mean under-control than the competing methods (Figures 8c
and 8d; vs. no robust: z = -4.782, p = 1.734E − 6; vs. no stable: z = -3.898, p = 9.711E-5;
vs. no ambig: z = -4.782, p = 1.734E-6; vs. no stable & no ambig: z = −4.782, p =
1.734E-6; vs. PC original: z = -4.700, p = 2.603E-6); meanwhile, PC-p kept the mean
over-control small at 3.865% (SD: 0.795%).

32

PC with p-values

33

Figure 8: Performances of PC-p, PC-p without robust p-values (no robust), PC-p without
ambiguation (no ambig), PC-p without stabilization (no stable), PC-p without
ambiguation and stabilization (no stable & no ambig), and the original PC algo-
rithm (PC) as assessed by (a,b) the FDR, (c,d) control bias, and (e,f) estimation
bias in units of percent. PC-p achieved signiﬁcantly lower FDR, under-estimation
and under-control than the other ﬁve methods suggesting that robust p-values,
stabilization and ambiguation are all important components of PC-p.

Strobl, Spirtes and Visweswaran

Figure 9: Same setup as Figure 8 except with high dimensional data. PC-p signiﬁcantly
outperformed all other methods except PC-p without stabilization in terms of
under-control and under-estimation.

Results for estimation were similar. PC-p achieved signiﬁcantly lower mean under-
estimation than the competing methods (Figures 8e and 8f; vs. no robust: z = -4.782, p =
1.734E-6; vs. no stable: z = -4.206, p = 2.597E-5; vs. no ambig: z = −4.782, p = 1.734E-6;
vs. no stable & no ambig: z = -4.782, p = 1.734E-6; vs. PC original: z = -4.186, p =
2.843E-5). PC-p also achieved a small degree of mean over-estimation (8.222%, SD: 0.400%).
We conclude that robust p-values, stabilization, and ambiguation all help PC-p achieve the
lowest under-control and under-estimation.

6.3 High Dimensional Inference

We next tested PC-p and the other ﬁve algorithms on high dimensional graph estimation. To
do this, we generated thirty 100, thirty 200 and thirty 300 variable DAGs. We subsequently
converted each of the DAGs to one linear non-recursive SEM-IE. Finally, we generated 1000
samples from each SEM-IE in order to obtain sample size to variable ratios of 10, 5 and
3.333.

Results are summarized using the FDR, control bias, and estimation bias metrics as
averaged over the DAGs and their parameters in Figure 9. PC-p achieved similar results in
low dimensions as it did for high dimensions. Speciﬁcally, PC-p obtained lower mean FDR
values across the same α thresholds than PC-p without robust p-values (Figures 9a and 9b;
z = -4.703, p = 2.563E-6), PC-p without stabilization (z = -2.232, p = 0.026), PC-p without
ambiguation (z = -4.782, p = 1.734E −6), PC-p without both stabilization and ambiguation
(z = -4.782, p = 1.734E-6), and PC original (z = -4.782, p = 1.734E-6). Moreover, PC-p
achieved signiﬁcantly lower mean under-control than four of the ﬁve competing methods

34

PC with p-values

Figure 10: Same setup as Figure 8 except with the CYTO dataset. PC-p signiﬁcantly
outperformed all methods across all metrics except the original PC algorithm in
under-control.

(Figures 9c and 9d; vs. no robust: z = -4.623, p = 3.790E-6; vs. no ambig: z = -4.782, p =
1.734E − 6; vs. no stable & no ambig: z = -4.782, p = 1.734E-6; vs. PC original: z =
-4.782, p = 1.734E-6). PC-p did not outperform PC-p without stabilization at four of the
ﬁve threshold α thresholds tested (0.05 : z = -1.121, p = 0.262; 0.10 : z = 0.504, p = 0.614;
0.20 : z = 0.985, p = 0.324; 0.50 : z = 0.760, p = 0.447); however, PC-p did outperform
PC-p without stabilization at an α threshold of 0.01 (z = -3.692, p = 2.225E-4). Meanwhile,
PC-p kept the mean over-control small at 4.507% (SD: 0.240%).

Results for estimation were again similar. PC-p achieved signiﬁcantly lower mean under-
estimation than the competing methods except PC-p without stabilization (Figures 9e
and 9f; vs. all methods except no stable: z = -4.782, p = 1.734E-6). PC-p did not
outperform PC-p without stabilization at four of the ﬁve threshold α thresholds tested
(0.05 : z = -1.820, p = 0.069; 0.10 : z = 0.175, p = 0.861; 0.20 : z = 0.625, p = 0.532;
0.50 : z = 0.608, p = 0.543); however, PC-p did outperform PC-p without stabilization at
an α threshold of 0.01 (z = -2.293, p = 0.028). PC-p also achieved a small degree of mean
over-control (2.129%, SD: 0.084%).

We conclude that the results for control and estimation bias for high dimensional graph
estimation are similar to the low dimensional case. However, stabilization only increased
performance at lower α thresholds in the high dimensional scenario; we may nonetheless
view this as a desirable property, since a lower α threshold helps the algorithm complete
more quickly.

35

Strobl, Spirtes and Visweswaran

6.4 Real Data: CYTO

We evaluated the six algorithms on the CYTO dataset which contains single cell recordings
of the abundance of 11 phosphoproteins and phospholipids in human primary naive CD4+
T cells using ﬂow cytometry (Sachs et al., 2005). The variables in the dataset and their
causal relationships can be represented as a DAG, where vertices are proteins or lipids
and edges are phosphorylation interactions between the proteins and lipids. We used the
general perturbation samples (i.e., CD3-CD28 and CD3-CD28-ICAM2) as our observational
data; these perturbations are required to activate the phosphorylation pathways. Note
that algorithms typically cannot accurately infer the gold standard solution set using the
observational data alone, as noted by the original authors4. As a result, we created a
silver standard DAG by running LiNGAM as implemented in TETRAD V using default
parameters on the full dataset of 1,755 samples; recall that LiNGAM is a method within a
diﬀerent class of causal discovery algorithms based on functional causal models. We then
ran the six algorithms described in Section 6.1 on 1000 bootstrapped datasets of sample
size 100 using Spearman’s rho to handle the class of non-paranormal distributions.

We have summarized the results in Figure 10. PC-p obtained lower mean FDR across
the same α thresholds than PC-p without robust p-values (z = -12.774, p = 2.282E-37),
PC-p without stabilization (z = −8.402, p = 4.378E-17), PC-p without ambiguation (z =
-24.598, p = 1.343E-133), PC-p without both stabilization and ambiguation (z = -23.714, p =
2.616E-124), and original PC (z = -4.924, p = 8.469E-7). Moreover, PC-p achieved signif-
icantly lower mean under-control than four of the ﬁve competing methods (vs. no ro-
bust: z = -12.601, p = 2.081E-36; vs no stable: z = -4.310, p = 1.631E-5; vs. no ambig:
z = -24.339, p = 7.559E-131; vs. no stable & no ambig: z = -22.893, p = 5.515E-116). PC-p
did not outperform the original PC algorithm in mean under-control (z = 0.827, p = 0.408);
however, PC-p did outperform the original PC algorithm in mean under-estimation (z =
−2.662, p = 0.008). Meanwhile, PC-p kept the mean over-control small at 4.232% (SD:
1.635%). PC-p also outperformed the other four methods in mean under-estimation (vs. no
robust: z = -12.684, p = 7.289E-37; vs. no stable: z = -4.893, p = 9.917E-7; vs. no ambig:
z = -24.411, p = 1.322E-131; vs. no stable & no ambig: z = -23.301, p = 4.333E-120)
while maintaining low mean over-estimation at 1.200% (SD: 0.559%). We conclude that
PC-p outperforms the other methods similar to the results with synthetic data. PC-p only
outperformed PC in 2 of the 3 metrics, however, probably because the LiNGAM solution
is only an estimate of the ground truth.

6.5 Real Data: GDP Dynamics

One way of approximating the underlying DAG involves learning the graph with a large
number of samples. Another way uses time series data, where we know a priori that we
must have contemporaneous causal relations or causal relations directed forward in time.
In this experiment, we strip the time information from the six algorithms, and then identify
the false discoveries when algorithms mistakenly detect a causal relation directed backwards
in time. We used a time series dataset downloaded from the Economic Research Service of
the United States Department of Agriculture containing ten economic indicators per year

4. In general, we do not have gold standard causal graphs for real data, so we must approximate the solution

in some manner.

36

PC with p-values

Figure 11: Similar to Figure 8 except with the GDP dataset as well as over-control and
over-estimation bias values instead of under. PC-p did not achieve lower under-
control and under-estimation than PC, but it did achieve signiﬁcantly lower
mean FDR and over-estimation that PC.

related to GDP among 192 countries5. We speciﬁcally evaluated the algorithms on their
ability to discover causal relations among the indicators within and between 1987, 1988
and 1989, where we treated each country as an i.i.d. sample and used 100 bootstrapped
datasets.

We have summarized the results in Figure 11. PC-p again obtained lower mean FDR val-
ues across the α thresholds than PC-p without robust p-values (z = -4.623, p = 3.784E-6),
PC-p without ambiguation (z = -8.054, p = 4.128E-16), PC-p without both stabilization
and ambiguation (z = -8.135, p = 4.128E-16), and original PC (z = -6.624, p = 3.500E-11).
However, PC-p did not obtain signiﬁcantly lower mean FDR values than PC-p without
stabilization (signed-rank = 70, p = 0.600). Next, PC-p achieved signiﬁcantly lower mean
under-control than three of the ﬁve competing methods including PC-p without robust p-
values (z = -4.374, p = 1.218E-5), PC-p without ambiguation (z = -7.867, p = 3.647E-15),
and PC-p without both stabilization and ambiguation (z = -7.819, p = 5.306E-15). PC-p
did not outperform PC-p without stabilization (signed-ranked = 3, p = 1) as well as the
original PC algorithm (signed-ranked = 7, p = 0.625) in mean under-control; nevertheless,
PC-p did outperform the former in over-control (signed-ranked = 3, p = 0.020) while keep-
ing its own mean over-control low at 4.920% (SD: 0.483%). PC-p also outperformed the
same three methods in mean under-estimation (vs. no robust: z = -4.372, p = 1.229E-5; vs.
no ambig: z = -7.818, p = 5.363E-15; vs. no stable & no ambig: z = -7.770, p = 7.850E-15)
while maintaining low mean over-estimation at 2.032% (SD: 0.281%). PC-p again did
not outperform PC-p without stabilization (signed-rank = 2, p = 0.750) and original

5. Web link: http://www.ers.usda.gov/data/macroeconomics/Data/HistoricalRealGDPValues.xls

37

Strobl, Spirtes and Visweswaran

PC (signed-rank = 7, p = 0.625) in under-estimation, but it outperformed both in over-
estimation (no stable: z = -7.386, p = 1.519E-13; PC original: z = -8.682, p = 3.897E-18).
We conclude that PC-p outperforms most methods in either the under or over-metrics. The
results however are not as clean as the results with the synthetic data because we only have
access to a portion of the ground truth.

7. Conclusion

We developed a new algorithm called PC-p which outputs a causal DAG with p-value bounds
associated with each edge. One can then use the bounds with the BY procedure to achieve
almost strong control and estimation of the FDR. The PC-p algorithm speciﬁcally integrates
the skeleton discovery procedure of PC-stable, edge orientation with ambiguation, and
robust hypothesis tests in order to accurately estimate p-values bounds while maintaining
computational eﬃciency.

The PC-p algorithm represents the ﬁrst global constraint-based method which can re-
cover p-value estimates for every edge of a CPDAG. In our opinion, the algorithm is a
signiﬁcant advancement over previous methods which can only achieve strong control of the
FDR under special conditions. Moreover, PC-p lays a foundation for developing similar
methods which can also recover edge-speciﬁc p-values and achieve strong control of the
FDR for graphs recovered by algorithms such as FCI and CCD. In particular, we suspect
that a combination of the max and union bounds will also be suﬃcient for deriving upper
bounds of the edge-speciﬁc p-values for more sophisticated constraint-based methods. The
proposed approach may therefore represent one the earliest forms of a “causal p-value.”

Now readers may wonder whether PC-p can also use the p-values to control the family-
wise error rate (FWER). The answer is yes, and we recommend using the Benjamini-Holm
step-down procedure as opposed to Hochberg’s step-up procedure to control the FWER
(Hochberg, 1988), since the latter assumes positive dependency among the test statistics.
However, application of an FWER controlling procedure to constraint-based causal discov-
ery requires additional justiﬁcation, since most investigators do not use constraint-based
methods to deﬁnitively conclude causal relationships but rather to screen for potential
causal variables. With the screening goal in mind, the FWER may be too conservative in
practice, since it controls the rate of making a single Type I error across all of the hypothesis
tests as opposed to controlling the proportion of Type I errors.

In summary, we introduced an algorithm called PC-p which outputs a causal DAG along
with edge-speciﬁc p-value bounds. One can then use the BY procedure with the bounds to
achieve almost strong control or estimation of the FDR and therefore assess the algorithm’s
conﬁdence in each edge in a principled manner. We ultimately hope that this work will
encourage more applications of constraint-based causal discovery to important problems in
science.

Acknowledgments

Research reported in this publication was supported by grant U54HG008540 awarded by
the National Human Genome Research Institute through funds provided by the trans-

38

PC with p-values

NIH Big Data to Knowledge initiative. The research was also supported by the National
Library of Medicine of the National Institutes of Health under award numbers T15LM007059
and R01LM012095. The content is solely the responsibility of the authors and does not
necessarily represent the oﬃcial views of the National Institutes of Health.

39

Strobl, Spirtes and Visweswaran

Appendix A. Appendix

A.1 PC Algorithm Pseudocode

We provide pseudocode for the original PC algorithm. We summarize skeleton discovery in
Algorithm 5, unshielded v-structure discovery in Algorithm 6, and orientation rule applica-
tion in Algorithm 7.

Data: X n, α
Result: (cid:98)G, S

l = l + 1
repeat

1 Form a completely connected undirected graph (cid:98)G on the vertex set X
2 l = −1
3 repeat
4

Select a new ordered pair of variables (A, B) that are adjacent in (cid:98)G s.t.
|N (A) \ B| ≥ l
repeat

Choose a new set S ⊆ {N (A) \ B} with |S| = l using order(X)
p ← p-value from testA⊥⊥B|S
if p > α then

Delete A − B from (cid:98)G
Insert S into SA,B and SB,A

end

until A − B is deleted from (cid:98)G or all S ⊆ {N (A) \ B} with |S| = l have been
considered ;

until all ordered pairs of adjacent variables (A, B) in (cid:98)G with | (cid:99)N (A) \ B| ≥ l have
been considered ;

16 until all ordered pairs of adjacent variables (A, B) in (cid:98)G satisfy | (cid:99)N (A) \ B| ≤ l;

Algorithm 5: Skeleton Discovery

5

6

7

8

9

10

11

12

13

14

15

Data: (cid:98)G, S
Result: (cid:98)G

end

4
5 end

1 for all ordered pairs of non-adjacent variables (A, B) with common neighbor C do
2

if C (cid:54)∈ any set in Si,j then

3

Replace A − C − B with A → C ← B

Algorithm 6: Unshielded V-structures

40

PC with p-values

Data: (cid:98)G
Result: (cid:98)G

1 repeat
2

3

4

5

6

7

if A − B and ∃C s.t. C → A, and C and B are non-adjacent then

Replace A − B with A → B

else if A − B and ∃C s.t. A → C → B then

Replace A − B with A → B

else if A − B and ∃B, D s.t. A − C → B, A − D → B, and C and D are
non-adjacent then

Replace A − B with A → B

end

8
9 until there are no more edges to orient;

Algorithm 7: Orientation Rules

A.2 Hypothesis Tests with Less Robust Bounds

We claimed to propose edge-speciﬁc hypothesis tests whose bounds are robust to Type II
errors in Section 4.4. We now explain our rationale.

Consider the following modiﬁcation to (10), where we have replaced the null with a

series of logical conjunctions:

H0 :

oracle i outputs ¬Pi,

H1 :

oracle i outputs Pi.

m
(cid:94)

i=1
m
(cid:94)

i=1

m
(cid:94)

i=1

We can bound the Type I error rate of the above hypothesis test as follows:

Pr(Type I error) = Pr(

test i outputs Pi|H0)

Pr(test i outputs Pi|H0)

Pr(test i outputs Pi|oracle i outputs ¬Pi )

≤ min

i=1,...,m

= min

i=1,...,m

= min

i=1,...,m

gi,

We can use the above bound with the following variant of (26) for unshielded v-

structures:

The above hypothesis test follows from the following natural hypothesis test:

H0 : ¬(A − C) ∧ ¬(B − C) ∧ ¬γAB|C,
H1 : (A − C) ∧ (B − C) ∧ γAB|C,

H0 : All edges between A, C, B are absent,
H1 : (A − C) ∧ (B − C) ∧ γAB|C,

41

(38)

(39)

(40)

(41)

Strobl, Spirtes and Visweswaran

since the null of (41) implies the null in (40). From (39), the Type I error rate of (40) is
bounded by min{pA−C, pB−C, pγAB|C }. This is a less robust bound than (26) in terms of
the Type II error rate, since failing to control one p-value can cause an algorithm to under-
estimate the bound. For example, suppose the underlying truth corresponds to pA−C = 0.01,
pB−C = 0.03, pγAB|C = 0.02. Thus, the Type I error rate of (41) is truly bounded by 0.01.
However, suppose a Type II error causes PC-p to skip CI tests and therefore compute
pA−C = 0.01, pB−C = 0.03, pγAB|C = 0.003, where the third term is under-estimated. Then,
PC-p will under-estimate the bound at 0.003 instead of the true 0.01.

Note that generalizing (41) to account for multiple possible ways of orienting a v-

structure does not robustify the bound either, since we have:

H0 : All edges between A, C, B1 are absent, and all edges between

A, C, B2 are absent,

(cid:16)(cid:104)

H1 : (A − C) ∧

(B1 − C) ∧ γAB1|C] ∨ [(B2 − C) ∧ γAB2|C

(cid:105)(cid:17)

.

whose Type I error rate is bounded by min

(cid:111)
(cid:110)
.
pA−C, min{pB1−C, pγAB1|C }+min{pB2−C, pγAB2|C }

More broadly, we can consider the following hypothesis test:

H0 :

oracle i, j outputs ¬Pi,j

H1 :

oracle i, j outputs Pi,j

(cid:17)

,

(cid:17)

.

m
(cid:95)

(cid:16) ni(cid:94)

i=1
m
(cid:94)

j=1

(cid:16) ni(cid:94)

i=1

j=1

We bound its Type I error rate as follows:
ni(cid:94)

m
(cid:95)

Pr(Type I error) = Pr(

test i, j outputs Pi,j|H0 )

i=1

j=1

Pr(test i, j outputs Pi,j|H0)

= max

Pr(test i, j outputs Pi,j| oracle i, j outputs ¬Pi,j)

≤ max

i=1,...,m

min
j=1,...,ni

min
j

min
j

i

i

= max

gi,j,

The above bound is less robust to Type II errors than (11), since under-estimating one term
in each group i composed of ni terms can cause PC-p to also under-estimate (44).

A.3 Other Experimental Results

We have summarized the results for the low dimensional, high dimensional and real datasets
across multiple α thresholds in Figures 12, 13, 14 and 15 respectively. Relative diﬀerences in
performance largely remained consistent across the thresholds, since PC-p usually achieved
the lowest mean FDR, under-control and under-estimation values with minimal increases
in mean over-control and over-estimation.

We have also summarized the results for adjacency discovery in Figures 16, 17, and 18,
where we tested whether the skeleton discovery procedure of PC with stabilization could

42

(42)

(43)

(44)

PC with p-values

improve the estimation of the p-value bounds relative to the procedure without stabilization.
Results show that stabilization improves performance across the three metrics particularly
with the low α threshold values of 0.01 and 0.05. Note that we cannot compute the same
ﬁgures for the GDP dataset, since we can only evaluate relative performance levels based
on edge direction in this case.

We have ﬁnally summarized the results using the structural Hamming distance in Figure
19. Notice that ambiguation helps PC-p achieve signiﬁcantly lower Hamming distances
across multiple α thresholds by forcing the algorithm to conservatively orient the edges.
Again, we cannot compute the structural Hamming distances for the GDP dataset for the
aforementioned reason.

References

A. P. Armen and I. Tsamardinos. A uniﬁed approach to estimation and control of the
false discovery rate in bayesian network skeleton identiﬁcation.
In In Proceedings of
the European Symposium on Artiﬁcial Neural Networks, Computational Intelligence and
Machine Learning (ESANN, 2011.

A. P. Armen and I. Tsamardinos. Estimation and control of the false discovery rate of
bayesian network skeleton identiﬁcation. Technical Report TR-441, University of Crete,
2014.

Y. Benjamini and D. Yekutieli. Investigating the importance of self-theories of intelligence
and musicality for students’ academic and musical achievement. Annals of Statistics, 29
(4):1165–1188, 2001.

H. Chong, M. Zey, and D. A. Bessler. On corporate structure, strategy, and performance:
a study with directed acyclic graphs and pc algorithm. Managerial and Decision Infor-
matics, 31:47–62, 2009.

D. Colombo and M. H. Maathuis. Order-independent constraint-based causal structure
learning. J. Mach. Learn. Res., 15(1):3741–3782, January 2014. ISSN 1532-4435. URL
http://dl.acm.org/citation.cfm?id=2627435.2750365.

G. F. Cooper and C. Yoo. Causal discovery from a mixture of experimental and observa-
tional data. In Proceedings of the Fifteenth Conference on Uncertainty in Artiﬁcial Intel-
ligence, UAI’99, pages 116–125, San Francisco, CA, USA, 1999. Morgan Kaufmann Pub-
lishers Inc. ISBN 1-55860-614-9. URL http://dl.acm.org/citation.cfm?id=2073796.
2073810.

N. Friedman, M. Goldszmidt, and A. Wyner. Data analysis with bayesian networks: A
bootstrap approach. In Proceedings of the Fifteenth Conference on Uncertainty in Arti-
ﬁcial Intelligence, UAI’99, pages 196–205, San Francisco, CA, USA, 1999. Morgan Kauf-
mann Publishers Inc. ISBN 1-55860-614-9. URL http://dl.acm.org/citation.cfm?
id=2073796.2073819.

43

Strobl, Spirtes and Visweswaran

Figure 12: Mean FDR, control bias, and estimation bias values across multiple α thresholds

for the low dimensional datasets.

44

PC with p-values

Figure 13: Same as Figure 12 except with high dimensional datasets.

45

Strobl, Spirtes and Visweswaran

Figure 14: Same as Figure 12 except with bootstrapped CYTO datasets.

46

PC with p-values

Figure 15: Same as Figure 12 except with bootstrapped GDP datasets.

47

Strobl, Spirtes and Visweswaran

Figure 16: Mean FDR, control bias, and estimation bias values across multiple α thresholds

for the low dimensional datasets.

48

PC with p-values

Figure 17: Same as Figure 16 except with high dimensional datasets.

49

Strobl, Spirtes and Visweswaran

Figure 18: Same as Figure 16 except with bootstrapped CYTO datasets.

50

PC with p-values

Figure 19: Structural Hamming distances for the a) low dimensional, b) high dimensional
and c) CYTO datasets. The three sub-ﬁgures associate 5 bars with each algo-
rithm; these bars correspond to α thresholds of 0.01, 0.05, 0.1, 0.20 and 0.50 for
the low dimensional and CYTO datasets, and α thresholds of 0.01, 0.05, 0.1,
0.15 and 0.20 for the high dimensional datasets. Error bars represent standard
errors for a) and standard deviations otherwise.

M. J. Ha, V. Baladandayuthapani, and K. A. Do. Prognostic gene signature identiﬁcation
using causal structure learning: applications in kidney cancer. Cancer Inform, 14(Suppl
1):23–35, 2015.

N. Harris and M. Drton. Pc algorithm for nonparanormal graphical models. J. Mach.
ISSN 1532-4435. URL http://dl.acm.

Learn. Res., 14(1):3365–3383, January 2013.
org/citation.cfm?id=2567709.2567770.

Y. Hochberg. A sharper Bonferroni procedure for multiple tests of signiﬁcance. Biometrika,
75(4):800–802, December 1988. ISSN 1464-3510. doi: 10.1093/biomet/75.4.800. URL
http://dx.doi.org/10.1093/biomet/75.4.800.

S. P. Iyer, I. Shafran, D. Grayson, K. Gates, J. T. Nigg, and D. A. Fair. Inferring func-
tional connectivity in MRI using Bayesian network structure learning with a modiﬁed PC
algorithm. Neuroimage, 75:165–175, Jul 2013.

A. A. Joshi, S. H. Joshi, R. M. Leahy, D. W. Shattuck, I. Dinov, and A. W. Toga. Bayesian
approach for network modeling of brain structural features, volume 7626. 2010. ISBN
9780819480279. doi: 10.1117/12.844548.

T. D. Le, L. Liu, A. Tsykin, G. J. Goodall, B. Liu, B. Y. Sun, and J. Li.

Inferring
microRNA-mRNA causal regulatory relationships from expression data. Bioinformatics,
29(6):765–771, Mar 2013.

51

Strobl, Spirtes and Visweswaran

J. Li and Z. J. Wang. Controlling the false discovery rate of the association/causality
structure learned with the pc algorithm. J. Mach. Learn. Res., 10:475–514, June 2009.
ISSN 1532-4435. URL http://dl.acm.org/citation.cfm?id=1577069.1577086.

J. Li, Z. Wang, and M. J. McKeown. Learning brain connectivity with the false-discovery-
rate-controlled PC-algorithm. Conf Proc IEEE Eng Med Biol Soc, 2008:4617–4620, 2008.

J. Listgarten and D. Heckerman. Determining the number of non-spurious arcs in a learned
dag model: Investigation of a bayesian and a frequentist approach. In Ronald Parr and
Linda C. van der Gaag, editors, UAI, pages 251–258. AUAI Press, 2007. ISBN 0-9749039-
3-0. URL http://dblp.uni-trier.de/db/conf/uai/uai2007.html#ListgartenH07.

C. Meek. Strong completeness and faithfulness in bayesian networks. In Proceedings of the
Eleventh Conference on Uncertainty in Artiﬁcial Intelligence, UAI’95, pages 411–418,
San Francisco, CA, USA, 1995. Morgan Kaufmann Publishers Inc. ISBN 1-55860-385-9.
URL http://dl.acm.org/citation.cfm?id=2074158.2074205.

D. Mullensiefen, P. Harrison, F. Caprini, and A. Fancourt. Investigating the importance
of self-theories of intelligence and musicality for students’ academic and musical achieve-
ment. Front Psychol, 6:1702, 2015.

T. Richardson. A discovery algorithm for directed cyclic graphs.

In Proceedings of the
Twelfth International Conference on Uncertainty in Artiﬁcial Intelligence, UAI’96, pages
454–461, San Francisco, CA, USA, 1996. Morgan Kaufmann Publishers Inc.
ISBN 1-
55860-412-X. URL http://dl.acm.org/citation.cfm?id=2074284.2074338.

K. Sachs, O. Perez, D. Pe’er, D. A. Lauﬀenburger, and G. P. Nolan. Causal protein-signaling
networks derived from multiparameter single-cell data. Science, 308(5721):523–529, Apr
2005.

P. Spirtes, C. Meek, and T. Richardson. Causal inference in the presence of latent vari-
ables and selection bias. In Proceedings of the Eleventh Conference on Uncertainty in
Artiﬁcial Intelligence, UAI’95, pages 499–506, San Francisco, CA, USA, 1995. Morgan
Kaufmann Publishers Inc. ISBN 1-55860-385-9. URL http://dl.acm.org/citation.
cfm?id=2074158.2074215.

P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. MIT press,

2nd edition, 2000.

J. Sun, X. Hu, X. Huang, Y. Liu, K. Li, X. Li, J. Han, L. Guo, T. Liu, and J. Zhang.
Inferring consistent functional interaction patterns from natural stimulus FMRI data.
Neuroimage, 61(4):987–999, Jul 2012.

R. Teramoto, C. Saito, and S. Funahashi. Estimating causal eﬀects with a non-paranormal
method for the design of eﬃcient intervention experiments. BMC Bioinformatics, 15:228,
2014.

I. Tsamardinos and L. E. Brown. Bounding the false discovery rate in local bayesian network
learning. In Proceedings of the Twenty-Third AAAI Conference on Artiﬁcial Intelligence,

52

PC with p-values

AAAI 2008, Chicago, Illinois, USA, July 13-17, 2008, pages 1100–1105, 2008. URL
http://www.aaai.org/Library/AAAI/2008/aaai08-174.php.

X. Wu and Y. Ye. Exploring gene causal interactions using an enhanced constraint-based
method. Pattern Recogn., 39(12):2439–2449, December 2006.
ISSN 0031-3203. doi:
10.1016/j.patcog.2006.05.003. URL http://dx.doi.org/10.1016/j.patcog.2006.05.
003.

53

