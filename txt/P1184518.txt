5
1
0
2
 
t
c
O
 
0
3
 
 
]

G
L
.
s
c
[
 
 
1
v
8
4
0
0
0
.
1
1
5
1
:
v
i
X
r
a

The Pareto Regret Frontier for Bandits

Tor Lattimore
Department of Computing Science
University of Alberta, Canada
tor.lattimore@gmail.com

Abstract

Given a multi-armed bandit problem it may be desirable to achieve a smaller-than-usual
worst-case regret for some special actions. I show that the price for such unbalanced worst-case
regret guarantees is rather high. Speciﬁcally, if an algorithm enjoys a worst-case regret of B with
respect to some action, then there must exist another action for which the worst-case regret is at
least Ω(nK/B), where n is the horizon and K the number of actions. I also give upper bounds
in both the stochastic and adversarial settings showing that this result cannot be improved. For
the stochastic case the pareto regret frontier is characterised exactly up to constant factors.

1 Introduction

The multi-armed bandit is the simplest class of problems that exhibit the exploration/exploitation
dilemma. In each time step the learner chooses one of K actions and receives a noisy reward signal
for the chosen action. A learner’s performance is measured in terms of the regret, which is the
(expected) difference between the rewards it actually received and those it would have received (in
expectation) by choosing the optimal action.

Prior work on the regret criterion for ﬁnite-armed bandits has treated all actions uniformly and
has aimed for bounds on the regret that do not depend on which action turned out to be optimal. I
take a different approach and ask what can be achieved if some actions are given special treatment.
Focussing on worst-case bounds, I ask whether or not it is possible to achieve improved worst-case
regret for some actions, and what is the cost in terms of the regret for the remaining actions. Such
results may be useful in a variety of cases. For example, a company that is exploring some new
strategies might expect an especially small regret if its existing strategy turns out to be (nearly)
optimal.

This problem has previously been considered in the experts setting where the learner is allowed
to observe the reward for all actions in every round, not only for the action actually chosen. The
earliest work seems to be by Hutter and Poland [2005] where it is shown that the learner can assign
a prior weight to each action and pays a worst-case regret of O(√
n log ρi) for expert i where ρi
is the prior belief in expert i and n is the horizon. The uniform regret is obtained by choosing ρi =
1/K, which leads to the well-known O(√n log K) bound achieved by the exponential weighting
algorithm [Cesa-Bianchi, 2006]. The consequence of this is that an algorithm can enjoy a constant
regret with respect to a single action while suffering minimally on the remainder. The problem was
studied in more detail by Koolen [2013] where (remarkably) the author was able to exactly describe
the pareto regret frontier when K = 2.

−

Other related work (also in the experts setting) is where the objective is to obtain an improved
regret against a mixture of available experts/actions [Even-Dar et al., 2008, Kapralov and Panigrahy,
2011]. In a similar vain, Sani et al. [2014] showed that algorithms for prediction with expert advice

1

can be combined with minimal cost to obtain the best of both worlds. In the bandit setting I am only
aware of the work by Liu and Li [2015] who study the effect of the prior on the regret of Thompson
sampling in a special case. In contrast the lower bound given here applies to all algorithms in a
relatively standard setting.

The main contribution of this work is a characterisation of the pareto regret frontier (the set of

achievable worst-case regret bounds) for stochastic bandits.

Let µi ∈

R be the unknown mean of the ith arm and assume that supi,j µi −
1. In each
and receives reward gIt,t = µi + ηt where
time step the learner chooses an action It ∈ {
1, . . . , K
ηt is the noise term that I assume to be sampled independently from a 1-subgaussian distribution
that may depend on It. This model subsumes both Gaussian and Bernoulli (or bounded) rewards.
Let π be a bandit strategy, which is a function from histories of observations to an action It. Then
the n-step expected pseudo regret with respect to the ith arm is

µj ≤

}

Rπ
µ,i = nµi −

E

µIt ,

n

t=1
X

Rπ

i = sup
µ

Rπ

µ,i .

where the expectation is taken with respect to the randomness in the noise and the actions of the
policy. Throughout this work n will be ﬁxed, so is omitted from the notation. The worst-case
expected pseudo-regret with respect to arm i is

(1)

(2)

This means that Rπ
Let

∈
RK be a set deﬁned by

B ⊂

RK is a vector of worst-case pseudo regrets with respect to each of the arms.

B

=

B




[0, n]K : Bi ≥

∈

min

n,

Xj6=i

n
Bj 









for all i

.





B

. The following theorem shows that δ

describes the pareto

The boundary of
is denoted by δ
B
B
regret frontier up to constant factors.



There exist universal constants c1 = 8 and c2 = 252 such that:

Theorem

Lower bound: for ηt ∼ N
Upper bound: for all B

∈ B

(0, 1) and all strategies π we have c1(Rπ + K)

there exists a strategy π such that Rπ

∈ B
c2Bi for all i

i ≤

Observe that the lower bound relies on the assumption that the noise term be Gaussian while the
upper bound holds for subgaussian noise. The lower bound may be generalised to other noise models
such as Bernoulli, but does not hold for all subgaussian noise models. For example, it does not hold
if there is no noise (ηt = 0 almost surely).

The lower bound also applies to the adversarial framework where the rewards may be chosen
arbitrarily. Although I was not able to derive a matching upper bound in this case, a simple modiﬁ-
cation of the Exp-γ algorithm [Bubeck and Cesa-Bianchi, 2012] leads to an algorithm with

Rπ

nK
B1
where the regret is the adversarial version of the expected regret. The details may be found in the
Appendix.

and Rπ

nK
B2

for all k

1 ≤

k .

1 (cid:19)

log

B1

2 ,

≥

(cid:18)

The new results seem elegant, but disappointing. In the experts setting we have seen that the
learner can distribute a prior amongst the actions and obtain a bound on the regret depending in a
natural way on the prior weight of the optimal action. In contrast, in the bandit setting the learner
pays an enormously higher price to obtain a small regret with respect to even a single arm. In fact,
the learner must essentially choose a single arm to favour, after which the regret for the remaining
arms has very limited ﬂexibility. Unlike in the experts setting, if even a single arm enjoys constant
worst-case regret, then the worst-case regret with respect to all other arms is necessarily linear.

2

2 Preliminaries

I use the same notation as Bubeck and Cesa-Bianchi [2012]. Deﬁne Ti(t) to be the number of times
action i has been chosen after time step t and ˆµi,s to be the empirical estimate of µi from the ﬁrst s
times action i was sampled. This means that ˆµi,Ti(t−1) is the empirical estimate of µi at the start of
the tth round. I use the convention that ˆµi,0 = 0. Since the noise model is 1-subgaussian we have

ε > 0

∀

P

s
{∃

≤

t : ˆµi,s −

µi ≥

ε/s

} ≤

exp

(3)

ε2
2t

.

(cid:19)

−

(cid:18)

This result is presumably well known, but a proof is included in Appendix E for convenience. The
optimal arm is i∗ = arg maxi µi with ties broken in some arbitrary way. The optimal reward is
µ∗ = maxi µi. The gap between the mean rewards of the jth arm and the optimal arm is ∆j =
RK and has been deﬁned
µj. The vector of worst-case regrets is Rπ
µ∗
µj and ∆ji = µi −
−
. For vector Rπ and
already in Eq. (1). I write Rπ
B
}
≤
∈
R we have (Rπ + x)i = Rπ
i + x.
x

Bi for all i

RK if Rπ

1, . . . , K

i ≤

∈ {

∈

∈

3 Understanding the Frontier

Before proving the main theorem I brieﬂy describe the features of the regret frontier. First notice
that if Bi =

1) for all i, then

n(K

−

p

Bi =

n(K

1) =

−

n/(K

1) =

−

p

Xj6=i

p

n
Bj

.

Xj6=i

as expected. This particular B is witnessed up to constant factors by MOSS
Thus B
[Audibert and Bubeck, 2009] and OC-UCB [Lattimore, 2015], but not UCB [Auer et al., 2002],
which suffers Rucb

Ω(√nK log n).

∈ B

Of course the uniform choice of B is not the only option. Suppose the ﬁrst arm is special, so B1
BK ≤

should be chosen especially small. Assume without loss of generality that B1 ≤
n. Then by the main theorem we have

B2 ≤

. . .

≤

i ∈

B1 ≥

K

k

n
Bi ≥

i=2
X

i=2
X

n
Bi ≥

(k

1)n

−
Bk

.

Therefore

(k

1)n

Bk ≥

−
B1

.

(4)

This also proves the claim in the abstract, since it implies that BK ≥
1)n/B1 does not lie on the frontier because
then choosing Bk = (k

(K

−

1)n/B1. If B1 is ﬁxed,

−

n
Bk

=

K

B1

k

1 ∈

−

Xk=2

K

Xk=2
1)

Ω(B1 log K)

K
k=2 1/(k

However, if H =
1)nH/B1 does lie on
the frontier and is a factor of log K away from the lower bound given in Eq. (4). Therefore up the
a log K factor, points on the regret frontier are characterised entirely by a permutation determining
the order of worst-case regrets and the smallest worst-case regret.

Θ(log K), then choosing Bk = (k

P

−

−

∈

Perhaps the most natural choice of B (assuming again that B1 ≤
and

B1 = np

Bk = (k

1)n1−pH for k > 1 .

. . .

≤

BK) is

For p = 1/2 this leads to a bound that is at most √K log K worse than that obtained by MOSS and
OC-UCB while being a factor of √K better for a select few.

−

3

Assumptions

[0, 1] is used to avoid annoying boundary problems caused by the fact that
The assumption that ∆i ∈
time is discrete. This means that if ∆i is extremely large, then even a single sample from this arm can
cause a big regret bound. This assumption is already quite common, for example a worst-case regret
of Ω(√Kn) clearly does not hold if the gaps are permitted to be unbounded. Unfortunately there is
no perfect resolution to this annoyance. Most elegant would be to allow time to be continuous with
actions taken up to stopping times. Otherwise you have to deal with the discretisation/boundary
problem with special cases, or make assumptions as I have done here.

4 Lower Bounds

Theorem 1. Assume ηt ∼ N
strategy, then 8(Rπ + K)
∈ B

(0, 1) is sampled from a standard Gaussian. Let π be an arbitrary
.

Proof. Assume without loss of generality that Rπ
re-order the actions). If Rπ
c = 4 and deﬁne

1 = mini Rπ
1 > n/8, then the result is trivial. From now on assume Rπ

i (if this is not the case, then simply
n/8. Let

1 ≤

Deﬁne K vectors µ1, . . . , µK ∈

RK by

εk = min

1
2

,

cRπ
k
n

(cid:26)

≤

(cid:27)

1
2

.

(µk)j =

if j = 1
if j = k
otherwise .

= 1

0
εk

εj

−

1
2

+ 




Therefore the optimal action for the bandit with means µk is k. Let A =
A′ =
A. Then

and assume k

k : Rπ
{

k ≤

n/8

and

}

k : k /
∈

{

A
}

Rπ
k

(a)

≥

Rπ

µk,k

(b)

≥

εkEπ

µk 

∈

Xj6=k


Tj(n)

(c)
= εk

n

Eπ

µk Tk(n)

(d)
=





−

(cid:0)

(cid:1)

cRπ

k (n

Eπ

µk Tk(n))

,

−

n

where (a) follows since Rπ
k is the worst-case regret with respect to arm k, (b) since the gap between
the means of the kth arm and any other arm is at least εk (Note that this is also true for k = 1
i Ti(n) = n and (d) from the deﬁnition of εk.
since ε1 = mink εk. (c) follows from the fact that
Therefore

P

Eπ

µk Tk(n) .

1
c

−

≤

(cid:19)

n

1
(cid:18)
A we have

∈

Therefore for k

= 1 with k

(5)

n

1
(cid:18)

−

1
c

(cid:19)

(a)

≤

Eπ

µk Tk(n)

Eπ

µ1 Tk(n) + nεk

Eπ

µ1 Tk(n)

Eπ

µ1 T1(n) + nεk

n

−

q
µ1 Tk(n)

Eπ

(c)

≤

n
c

≤
(b)

≤

+ nεk

Eπ

µ1 Tk(n) ,

q
where (a) follows from standard entropy inequalities and a similar argument as used by Auer et al.
n, and (c) by
[1995] (details given in Appendix C), (b) since k
Eq. (5). Therefore

µ1 T1(n) + Eπ

= 1 and Eπ

µ1 Tk(n)

q

≤

Eπ

µ1 Tk(n)

2
c

,

1

−
ε2
k

≥

4

which implies that

Rπ

1 ≥

Rπ

µ1,1 =

εkEπ

µ1 Tk(n)

K

Xk=2

2
c

1

−
εk

=

1
8

n
Rπ
k

.

Xk∈A−{1}

≥

Xk∈A−{1}

Therefore for all i

A we have

∈

8Rπ

i ≥

n
Rπ
k ·

Rπ
i
Rπ
1 ≥

n
Rπ
k

.

Xk∈A−{1}

Xk∈A−{i}

Therefore

8Rπ

i + 8K

n
Rπ
k

+ 8K

−

n
Rπ
k ≥

n
Rπ
k

,

Xk∈A′−{i}

Xk6=i

≥

Xk6=i

∈ B

which implies that 8(Rπ + K)

as required.

5 Upper Bounds

I now show that the lower bound derived in the previous section is tight up to constant factors. The
algorithm is a generalisation MOSS [Audibert and Bubeck, 2009] with two modiﬁcations. First, the
width of the conﬁdence bounds are biased in a non-uniform way, and second, the upper conﬁdence
bounds are shifted. The new algorithm is functionally identical to MOSS in the special case that Bi
is uniform. Deﬁne log+(x) = max

.

0, log(x)
}
{

1: Input: n and B1, . . . , BK
2: ni = n2/B2
3: for t

1, . . . , n do

i for all i

∈

4:

It = arg max

ˆµi,Ti(t−1) +

5: end for

i

4
Ti(t

s

1)

−

log+

ni

Ti(t

1)

−

(cid:19)

r

−

(cid:18)

1
ni

Algorithm 1: Unbalanced MOSS

Theorem 2. Let B

, then the strategy π given in Algorithm 1 satisﬁes Rπ

252B.

∈ B

≤

Corollary 3. For all µ the following hold:

1. Rπ

µ,i∗

252Bi∗.

≤

≤

2. Rπ

µ,i∗

mini(n∆i + 252Bi)

The second part of the corollary is useful when Bi∗ is large, but there exists an arm for which
n∆i and Bi are both small. The proof of Theorem 2 requires a few lemmas. The ﬁrst is a some-
what standard concentration inequality that follows from a combination of the peeling argument and
Doob’s maximal inequality.

Lemma 4. Let Zi = max
1≤s≤n

µi −

ˆµi,s − r

. Then P

Zi ≥
{

∆

} ≤

20
ni∆2 for all ∆ > 0.

4
s

log+
(cid:16)

ni
s

(cid:17)

5

Proof. Using the peeling device.

P

Zi ≥
{

∆
}

(a)
= P

n : µi −

ˆµi,s ≥

≤

∆ +

4
s

r

ni
s

log+
(cid:16)
2k∆ +

)

(cid:17)

s
(∃
∞

P

(cid:26)

Xk=0
∞

(b)

≤

(c)

≤

s < 2k+1 : s(µi −
∃

ˆµi,s)

≥

exp

2k−2∆2

min

1,

Xk=0

−
(cid:0)

(cid:26)

(cid:1)

2k+1
ni (cid:27)

(d)

≤

(cid:18)

2k+2 log+
(cid:16)

r
8
log(2)

+ 8

(cid:19)

ni
2k+1

1

(cid:17)(cid:27)

·

ni∆2 ≤

20
ni∆2 ,

where (a) is just the deﬁnition of Zi, (b) follows from the union bound and re-arranging the equation
inside the probability, (c) follows from Eq. (3) and the deﬁnition of log+ and (d) is obtained by
upper bounding the sum with an integral.

In the analysis of traditional bandit algorithms the gap ∆ji measures how quickly the algorithm
can detect the difference between arms i and j. By design, however, Algorithm 1 is negatively
biasing its estimate of the empirical mean of arm i by
1/ni. This has the effect of shifting the
gaps, which I denote by ¯∆ji and deﬁne to be

p

¯∆ji = ∆ji +

1/nj −

1/ni = µi −

µj +

1/nj −

1/ni .

p

p

p

p

Lemma 5. Deﬁne stopping time τji by

τji = min

s : ˆµj,s +

4
s

r

nj
s

log+
(cid:16)

≤

(cid:17)

µj + ¯∆ji/2

.

)

If Zi < ¯∆ji/2, then Tj(n)

(

τji.

≤

Proof. Let t be the ﬁrst time step such that Tj(t

1) = τji. Then

ˆµj,Tj (t−1)+

−

nj

Tj(t

(cid:18)
¯∆ji/2

−

−

1)

(cid:19)
1/nj

p

−
¯∆ji/2

p

log+

1)

4
Tj(t
s
−
= µj + ¯∆ji −
= µi −

1/ni −

p

< ˆµi,Ti(t−1) +

1/nj ≤

µj + ¯∆ji/2

1/nj

−

p

which implies that arm j will not be chosen at time step t and so also not for any subsequent time
steps by the same argument and induction. Therefore Tj(n)

τji.

4
Ti(t

s

1)

−

log+

Ti(t

1)

−

(cid:19)

−

(cid:18)

p

1/ni ,

ni

≤

Lemma 6. If ¯∆ji > 0, then Eτji ≤

40
¯∆2
ji

+

64
¯∆2
ji

ProductLog

nj ¯∆2
ji
.
64 !

 

Proof. Let s0 be deﬁned by

s0 =

ProductLog

64
¯∆2
ji

&

nj ¯∆2
ji
64 !'

 

=

⇒ s

4
s0

log+

nj
s0 (cid:19)

(cid:18)

¯∆ji
4

.

≤

6

Therefore

n−1

s=s0+1
X
32
¯∆2
ji ≤

Eτji =

n

P

s=1
X

1 + s0 +

≤

τji ≥
{

s

} ≤

1 +

n−1

P

ˆµi,s −
(
¯∆ji
4

µi,s ≥

µi,s ≥

≤

(cid:27)

s=1
X
ˆµi,s −
(cid:26)

P

¯∆ji
2 − r

4
s

nj
s

log+
(cid:16)

∞

1 + s0 +

exp

s=s0+1
X

(cid:17)

)
s ¯∆2
ji
32 !

 −

ProductLog

nj ¯∆2
ji
64 !

,

 

≤

+

40
¯∆2
ji

1 + s0 +

64
¯∆2
ji
where the last inequality follows since ¯∆ji ≤
Proof of Theorem 2. Let ∆ = 2/√ni and A =
2 ¯∆ji and ¯∆ji ≥
1/ni + √1/nj. Letting ∆′ =

2.

p

. Then for j
j : ∆ji > ∆
}
{

A we have ∆ji ≤

∈

1/ni we have

Rπ

µ,i = E

∆jiTj(n)



p

K



j=1
X


n∆ + E


Xj∈A

2Bi + E


∆jiTj(n)





≤

(a)

≤

(b)

≤

(c)

≤





Xj∈A

Xj∈A  

Xj∈A

∆jiτji + n max
j∈A

∆ji : Zi ≥
(cid:8)

¯∆ji/2



2Bi +

80
¯∆ji

+

128
¯∆ji

ProductLog

(cid:9)

+ 4nE[Zi1

nj ¯∆2
ji
64 !!

 

Zi ≥
{

∆′

]
}

2Bi +

90√nj + 4nE[Zi1

Zi ≥
{

∆′

] ,

}

where (a) follows by using Lemma 5 to bound Tj(n)
the total number of pulls for arms j for which Zi ≥
τji in expectation using Lemma 6. (c) follows from basic calculus and because for j
¯∆ji ≥
p
4nE[Zi1

1/ni. All that remains is to bound the expectation.

τji when Zi < ¯∆ji. On the other hand,
≤
¯∆ji/2 is at most n. (b) follows by bounding
A we have

= 160Bi ,

4n∆′P

+ 4n

∆′

∆′

dz

=

∈

P

∞

z

]

Zi ≥

{

}

≤

160n
∆′ni

160n
√ni

Zi ≥
{

}

≤

Zi ≥
{

}

∆′

Z

where I have used Lemma 4 and simple identities. Putting it together we obtain

Rπ

µ,i ≤

2Bi +

90√nj + 160B1 ≤

252Bi ,

Xj∈A
and so

∈ B

where I applied the assumption B

j6=1 √nj =

j6=1 n/Bj ≤

Bi.

The above proof may be simpliﬁed in the special case that B is uniform where we recover
the minimax regret of MOSS, but with perhaps a simpler proof than was given originally by
Audibert and Bubeck [2009].

P

P

On Logarithmic Regret

In a recent technical report I demonstrated empirically that MOSS suffers sub-optimal problem-
dependent regret in terms of the minimum gap [Lattimore, 2015]. Speciﬁcally, it can happen that

Rmoss
µ,i∗

Ω

∈

K
∆min

(cid:18)

log n

,

(cid:19)

7

(6)

where ∆min = mini:∆i>0 ∆i. On the other hand, the order-optimal asymptotic regret can be signif-
icantly smaller. Speciﬁcally, UCB by Auer et al. [2002] satisﬁes

Rucb
µ,i∗

O

∈

 

1
∆i

log n

,

!

(7)

Xi:∆i>0
which for unequal gaps can be much smaller than Eq. (6) and is asymptotically order-optimal
[Lai and Robbins, 1985]. The problem is that MOSS explores only enough to obtain minimax re-
gret, but sometimes obtains minimax regret even when a more conservative algorithm would do
better. It is worth remarking that this effect is harder to observe than one might think. The example
given in the afforementioned technical report is carefully tuned to exploit this failing, but still re-
quires n = 109 and K = 103 before signiﬁcant problems arise. In all other experiments MOSS was
performing admirably in comparison to UCB.

All these problems can be avoided by modifying UCB rather than MOSS. The cost is a factor
of O(√log n). The algorithm is similar to Algorithm 1, but chooses the action that maximises the
following index.

It = arg max

ˆµi,Ti(t−1) +

i

(2 + ε) log t

s

Ti(t

1) − r

−

log n
ni

,

where ε > 0 is a ﬁxed arbitrary constant.

Theorem 7. If π is the strategy of unbalanced UCB with ni = n2/B2
of the unbalanced UCB satisﬁes:

i and B

∈ B

, then the regret

1. (problem-independent regret). Rπ

O

Bi∗ √log n

.

µ,i∗

∈

2. (problem-dependent regret). Let A =

(cid:1)

2

1/ni∗ log n

. Then

(cid:0)
i : ∆i ≥

n

p

o

1
∆i

log n

.

!

Xi∈A

Rπ

µ,i∗

O

Bi∗

∈

 

log n1

=

A
{

∅}

+

p

The proof may be found in Appendix B. The indicator function in the problem-dependent

bound vanishes for sufﬁciently large n provided ni∗
∈
o(n/√log n). Thus for reasonable choices of B1, . . . , BK the algorithm is going to enjoy the same
asymptotic performance as UCB. Theorem 7 may be proven for any index-based algorithm for which
it can be shown that

ω(log(n)), which is equivalent to Bi∗

∈

ETi(n)

O

∈

1
∆2
i

log n

,

(cid:18)
which includes (for example) KL-UCB [Capp´e et al., 2013] and Thompson sampling (see analy-
sis by Agrawal and Goyal [2012a,b] and original paper by Thompson [1933]), but not OC-UCB
[Lattimore, 2015] or MOSS [Audibert and Bubeck, 2009].

(cid:19)

A Note on Constants

The constants in the statement of Theorem 2 can be improved by carefully tuning all thresh-holds,
but the proof would grow signiﬁcantly and I would not expect a corresponding boost in practical
performance. In fact, the reverse is true, since the “weak” bounds used in the proof would propagate
to the algorithm. Also note that the 4 appearing in the square root of the unbalanced MOSS algorithm
is due to the fact that I am not assuming rewards are bounded in [0, 1] for which the variance is at
most 1/4. It is possible to replace the 4 with 2 + ε for any ε > 0 by changing the base in the peeling
argument in the proof of Lemma 4 as was done by Bubeck [2010] and others.

8

Experimental Results

2

1

3 and B2 = n

I compare MOSS and unbalanced MOSS in two simple simulated examples, both with horizon
104 i.i.d. samples, so error bars are too small
n = 5000. Each data point is an empirical average of
to see. Code/data is available in the supplementary material. The ﬁrst experiment has K = 2 arms
and B1 = n
∆) for varying ∆. As predicted,
the new algorithm performs signiﬁcantly better than MOSS for positive ∆, and signiﬁcantly worse
otherwise (Fig. 1). The second experiment has K = 10 arms. This time B1 = √n and Bk =
k=1 1/k. Results are shown for µk = ∆1
[0, 1/2] and
(k
i∗
. Again, the results agree with the theory. The unbalanced algorithm is superior to
}
MOSS for i∗
1, 2
∈ {

1)H√n with H =
1, . . . , 10

3 . I plotted the results for µ = (0,

and inferior otherwise (Fig. 2).

k = i∗
{

−
∈ {

for ∆

P

−

∼

∈

}

}

9

t
e
r
g
e
R

800

600

400

200

0

MOSS
U. MOSS

2,000

t
e
r
g
e
R

1,000

0

0

−0.4 −0.2

0.2

0.4

1

2

4

5

0
∆

Figure 1

Figure 2: θ = ∆ + (i∗

1)/2

θ

3

−

Sadly the experiments serve only to highlight the plight of the biased learner, which suffers

signiﬁcantly worse results than its unbaised counterpart for most actions.

6 Discussion

I have shown that the cost of favouritism for multi-armed bandit algorithms is rather serious. If
an algorithm exhibits a small worst-case regret for a speciﬁc action, then the worst-case regret of
the remaining actions is necessarily signiﬁcantly larger than the well-known uniform worst-case
bound of Ω(√Kn). This unfortunate result is in stark contrast to the experts setting for which there
exist algorithms that suffer constant regret with respect to a single expert at almost no cost for the
remainder. Surprisingly, the best achievable (non-uniform) worst-case bounds are determined up to
a permutation almost entirely by the value of the smallest worst-case regret.

There are some interesting open questions. Most notably, in the adversarial setting I am not sure
if the upper or lower bound is tight (or neither). It would also be nice to know if the constant factors
can be determined exactly asymptotically, but so far this has not been done even in the uniform
case. For the stochastic setting it is natural to ask if the OC-UCB algorithm can also be modiﬁed.
Intuitively one would expect this to be possible, but it would require re-working the very long proof.

Acknowledgements

I am indebted to the very careful reviewers who made many suggestions for improving this paper.
Thank you!

9

References

Shipra Agrawal and Navin Goyal. Further optimal regret bounds for thompson sampling. In Pro-
ceedings of International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2012a.

Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit prob-

lem. In Proceedings of Conference on Learning Theory (COLT), 2012b.

Jean-Yves Audibert and S´ebastien Bubeck. Minimax policies for adversarial and stochastic bandits.

In COLT, pages 217–226, 2009.

Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. Gambling in a rigged
casino: The adversarial multi-armed bandit problem. In Foundations of Computer Science, 1995.
Proceedings., 36th Annual Symposium on, pages 322–331. IEEE, 1995.

Peter Auer, Nicol´o Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit

problem. Machine Learning, 47:235–256, 2002.

Stephane Boucheron, Gabor Lugosi, and Pascal Massart. Concentration Inequalities: A Nonasymp-

totic Theory of Independence. OUP Oxford, 2013.

S´ebastien Bubeck. Bandits games and clustering foundations. PhD thesis, Universit´e des Sciences

et Technologie de Lille-Lille I, 2010.

S´ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-
armed Bandit Problems. Foundations and Trends in Machine Learning. Now Publishers Incorpo-
rated, 2012. ISBN 9781601986269.

Olivier Capp´e, Aur´elien Garivier, Odalric-Ambrym Maillard, R´emi Munos, and Gilles Stoltz.
Kullback–Leibler upper conﬁdence bounds for optimal sequential allocation. The Annals of
Statistics, 41(3):1516–1541, 2013.

Nicolo Cesa-Bianchi. Prediction, learning, and games. Cambridge University Press, 2006.

Eyal Even-Dar, Michael Kearns, Yishay Mansour, and Jennifer Wortman. Regret to the best vs.

regret to the average. Machine Learning, 72(1-2):21–37, 2008.

Marcus Hutter and Jan Poland. Adaptive online prediction by following the perturbed leader. The

Journal of Machine Learning Research, 6:639–660, 2005.

Michael Kapralov and Rina Panigrahy. Prediction strategies without loss. In Advances in Neural

Information Processing Systems, pages 828–836, 2011.

Wouter M Koolen. The pareto regret frontier. In Advances in Neural Information Processing Sys-

tems, pages 863–871, 2013.

Tze Leung Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances

in applied mathematics, 6(1):4–22, 1985.

Tor Lattimore. Optimally conﬁdent UCB : Improved regret for ﬁnite-armed bandits. Technical

report, 2015. URL http://arxiv.org/abs/1507.07880.

Che-Yu Liu and Lihong Li. On the prior sensitivity of thompson sampling.

arXiv preprint

arXiv:1506.03378, 2015.

Amir Sani, Gergely Neu, and Alessandro Lazaric. Exploiting easy data in online optimization. In

Advances in Neural Information Processing Systems, pages 810–818, 2014.

William Thompson. On the likelihood that one unknown probability exceeds another in view of the

evidence of two samples. Biometrika, 25(3/4):285–294, 1933.

10

A Table of Notation

∈

[0, 1]K

time horizon
number of available actions
time step
actions
set of achievable worst-case regrets deﬁned in Eq. (2)
boundary of
B
vector of expected rewards µ
expected return of optimal action
µ∗
µj
−
µj
µi −
bandit strategy
action chosen at time step t
regret of strategy π with respect to the kth arm
worst-case regret of strategy π with respect to the kth arm
empirical estimate of the return of the k action after s samples
number of times action k has been taken at the end of time step t
optimal action
maximum of 0 and log(x)
Gaussian with mean µ and variance σ2

B Proof of Theorem 7

Recall that the proof of UCB depends on showing that

ETi(n)

O

∈

1
∆2
i

log n

.

(cid:18)
Now unbalanced UCB operates exactly like UCB, but with shifted rewards. Therefore for unbal-
anced UCB we have

(cid:19)

n
K
t
k, i

B
δ
B
µ
µ∗
∆j
∆ji
π
It
Rπ
Rπ
k
ˆµk,s
Tk(t)
i∗
log+(x)

µ,k

(µ, σ2)

N

where

Deﬁne :

ETi(n)

O

∈

1
¯∆2
i

(cid:18)

log n

,

(cid:19)

¯∆i ≥

∆i +

log n
ni − r

log n
ni∗

.

r

A =

i : ∆i ≥

2

(

r

log n
ni∗ )

If i

A, then ∆i ≤

∈

2 ¯∆i and ¯∆i ≥

log n
ni

. Therefore

∆iETi(n)

O

∈

∆i
¯∆2
i

(cid:18)

log n

⊆

(cid:19)

1
¯∆i

(cid:18)

q

O

For i /
∈

A we have ∆i < 2

log n
ni∗ thus

log n

O

ni log n

⊆

(cid:19)

(cid:16)p

O

⊆

n
Bi

(cid:18)

(cid:17)

log n

.

(cid:19)

p

q

E

"
Xi /∈A

∆iTi(n)

O

n

# ∈

 

r

log n
ni∗ ! ⊆

O

Bi∗

log n

.

(cid:16)

p

(cid:17)

11

Therefore

Rπ

µ,i∗ =

∆iETi(n)

K

i=1
X

O

∈

  

Bi∗ +

n
Bi !

Xi∈A

p

log n

= O

Bi∗

log n

!

(cid:16)

p

(cid:17)

as required. For the problem-dependent bound we work similarly.

Rπ

µ,i∗ =

∆iETi(n)

K

i=1
X

1
¯∆i

1
∆i

O

∈

O

∈

 

Xi∈A

 

Xi∈A

log n + 1

=

A
{

∅}

Bi∗

log n

log n + 1

=

A
{

∅}

Bi∗

log n

.

!

!

p

p

C KL Techniques

Let µ1, µk ∈
the claim that

RK be two bandit environments as deﬁned in the proof of Theorem 1. Here I prove

Eπ

µk Tk(n)

Eπ

µ1 Tk(n)

−

Eπ

µ1 Tk(n) .

nεk

≤

q

{Ft}

The result follows along the same lines as the proof of the lower bounds given by Auer et al. [1995].
Ft contains information about rewards and actions chosen up to
Let
time step t. So gIt,t and 1
Ft. Let P1 and Pk be the measures
are measurable with respect to
on
Fn-measurable
random variable bounded in [0, n]. Therefore

induced by bandit problems µ1 and µk respectively. Note that Tk(n) is a

n
t=1 be a ﬁltration where
It = i
{

F

}

Eπ

µk Tk(n)

Eπ

µ1 Tk(n)

−

n sup
A |

P1(A)

P2(A)
|

−

(a)

≤

(b)

≤

1
2

n

r

KL(P1, Pk) ,

where the supremum in (a) is taken over all measurable sets (this is the total variation distance) and
(b) follows from Pinsker’s inequality. It remains to compute the KL divergence. Let P1,t and Pk,t
be the conditional measures on the tth reward. By the chain rule for the KL divergence we have

KL(P1, Pk) =

EP1 KL(P1,t, Pk,t)

(a)
= 2ε2
k

EP1

1

It = k
{

}

= 2ε2
k

Eπ

µ1 Tk(n) ,

n

t=1
X

n

t=1
X

where (a) follows by noting that if It 6
= k, then the distribution of the rewards at time step t is
the same for both bandit problems µ1 and µk. For It = k we have the difference in means is
k. For Bernoulli
(µk)k −
random noise the KL divergence is also Θ(ε2
1/2 and so a similar proof
(µ1)k ≈
works for this case. See the work by Auer et al. [1995] for an example.

(µ1)k = εk and since the distributions are Gaussian the KL divergence is 2ε2

k) provided (µk)k ≈

D Adversarial Bandits

In the adversarial setting I obtain something similar. First I introduce some new notation. Let
[0, 1] be the gain/reward from choosing action i at time step t. This is chosen in an arbitrary
gi,t ∈
way by the adversary with gi,t possibly even dependent on the actions of the learner up to time step

12

t. The regret difference between the gains obtained by the learner and those of the best action in
hindsight.

Rπ

g = max

i∈{1,...,K}

E

gi,t −

gIt,t

.

#

n

"

t=1
X

I make the most obvious modiﬁcation to the Exp3-γ algorithm, which is to bias the prior towards
the special action and tune the learning rate accordingly. The algorithm accepts as input the prior
i ρi = 1, and the learning rate η.
ρ

[0, 1]K, which must satisfy

∈

P

4:

∈

[0, 1]K, η

for t

1: Input: K, ρ
2: wi,0 = ρi for each i
1, . . . , n do
3:
∈
Let pi,t = wi,t−1
Choose action It = i with probability pi,t and observe gain gIt,t
˜ℓt,i = (1−gt,i)1{It=i}
wi,t = wi,t−1 exp

i=1 wi,t−1

η ˜ℓt,i

PK

pi,t

6:

5:

7:
8: end for

−

(cid:16)

(cid:17)

Algorithm 2: Exp3-γ

The following result follows trivially from the standard proof.

Theorem 8 (Bubeck and Cesa-Bianchi [2012]). Let π be the strategy determined by Algorithm 2,
then

Rπ

g ≤

ηKn +

log

1
η

1
ρi∗

.

ρi =

exp

2
B
1
4Kn
−
(cid:17)
(cid:16)
ρ1)/(K
−

−

(

(1

if i = 1

1) otherwise

Corollary 9. If ρ is given by

and η = B1/(2Kn), then

Rπ
g ≤ (

B1
B1
2 + 2Kn

B1 log

if i∗ = 1
otherwise .

4Kn(K−1)
B2
1

(cid:16)

(cid:17)

Proof. The proof follows immediately from Theorem 8 by noting that for i∗

= 1 we have

log

= log

1
ρi∗

K

1

−



1



exp

−
4Kn(K
B2
1

(cid:18)

B2
1
4Kn





(cid:17)

−
1)

(cid:16)
−

(cid:19)

log

≤

as required.

E Concentration

The following straight-forward concentration inequality is presumably well known and the proof of
an almost identical result is available by Boucheron et al. [2013], but an exact reference seems hard
to ﬁnd.

13

Theorem 10. Let X1, X2, . . . , Xn be independent and 1-subgaussian, then

t
∃

≤

n :

1
t

P




Xs≤t

Xs ≥

exp

≤

ε
t 


ε2
2n

.

(cid:19)

−

(cid:18)

Proof. Since Xi is 1-subgaussian, by deﬁnition it satisﬁes




R)

λ

(
∀

∈

E [exp (λXi)]

exp

λ2/2

.

≤

Now X1, X2, . . . are independent and zero mean, so by convexity of the exponential function
exp(λ

t
s=1 Xs) is a sub-martingale. Therefore if ε > 0, then by Doob’s maximal inequality

(cid:0)

(cid:1)

P

t

s=1
X

P

t
(∃

≤

n :

Xs ≥

ε

)

P

= inf
λ≥0

t
(∃

≤

n : exp

λ

Xs

exp (λε)

! ≥

)

t

 

s=1
X

λ2n
2 −

λε

(cid:19)

exp

inf
λ≥0

≤

= exp

(cid:18)
ε2
2n

−

(cid:18)

(cid:19)

as required.

14

