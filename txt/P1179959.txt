Multi-View Multi-Graph Embedding for Brain Network Clustering Analysis

Ye Liu1, Lifang He2∗, Bokai Cao1, Philip S. Yu1,3, Ann B. Ragin 4, Alex D. Leow5
1Department of Computer Science, 5Department of Bioengineering, University of Illinois at Chicago, IL, USA
2Department of Healthcare Policy and Research, Cornell University, NY, USA
3Institute for Data Science, Tsinghua University, Beijing, China
4Department of Radiology, Northwestern University, IL, USA

yliu279, caobokai, psyu
{

@uic.edu,
}

lifanghescut, alexfeuillet
}
{

@gmail.com, ann-ragin@northwestern.edu

8
1
0
2
 
n
u
J
 
9
1
 
 
]

G
L
.
s
c
[
 
 
1
v
3
0
7
7
0
.
6
0
8
1
:
v
i
X
r
a

Abstract

Network analysis of human brain connectivity is critically im-
portant for understanding brain function and disease states.
Embedding a brain network as a whole graph instance into
a meaningful low-dimensional representation can be used to
investigate disease mechanisms and inform therapeutic inter-
ventions. Moreover, by exploiting information from multiple
neuroimaging modalities or views, we are able to obtain an
embedding that is more useful than the embedding learned
from an individual view. Therefore, multi-view multi-graph
embedding becomes a crucial task. Currently only a few stud-
ies have been devoted to this topic, and most of them fo-
cus on vector-based strategy which will cause structural in-
formation contained in the original graphs lost. As a novel
attempt to tackle this problem, we propose Multi-view Multi-
graph Embedding (M2E) by stacking multi-graphs into mul-
tiple partially-symmetric tensors and using tensor techniques
to simultaneously leverage the dependencies and correlations
among multi-view and multi-graph brain networks. Extensive
experiments on real HIV and bipolar disorder brain network
datasets demonstrate the superior performance of M2E on
clustering brain networks by leveraging the multi-view multi-
graph interactions.

Index terms— Brain Network Embedding, Multi-graph
Embedding, Tensor Factorization, Multi-view Learning

Introduction

Beneﬁting from modern neuroimaging technology, there is
an increasing amount of graph data representing the human
brain, called brain networks, e.g., functional magnetic reso-
nance imaging (fMRI) and diffusion tensor imaging (DTI).
These data have complex structure, which are inherently
represented as graphs with a set of nodes and links. More-
over, the linkage structure extracted from different modal-
ities can often be treated as multi-view data. The connec-
tions in fMRI brain networks encode correlations among
brain regions in terms of functional activities, while in
DTI networks, the connections can capture the white mat-
ter ﬁber pathways that connect different brain regions. Even
if these individual views might be sufﬁcient on their own

∗Corresponding Author

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

for a given learning task, they can often provide comple-
mentary information to each other which can lead to im-
prove performance on the learning task (Ma et al. 2017a;
Sun et al. 2017). As labeled data are difﬁcult to obtain,
it is critical to leverage the multi-view information to ob-
tain an effective embedding for the clustering task. There-
fore, in this study, we focus on investigating the multi-view
multi-graph embedding problem for brain network cluster-
ing analysis. Speciﬁcally, we aim to learn the latent embed-
ding representation of multiple brain networks from mul-
In recent years, there has been an increasing
tiple views.
interest in single-graph node embedding among researchers
(Mousazadeh and Cohen 2015; Ou et al. 2016), yet it is chal-
lenging to extend to the multi-graph embedding, where we
consider the embedding of multiple graph instances together
to obtain a discriminative representation for each graph.
By exploring the consistency and complementary proper-
ties of different views, multi-view learning (Liu et al. 2013a;
Cao et al. 2014) is rendered more effective, more promising,
and has better generalization ability than single-view learn-
ing. Although there have been numerous works on single-
graph embedding and multi-view learning, to the best of our
knowledge, there is no embedding method available which
enables preserving multi-graph structures on multiple views.
There are several major challenges in multi-view multi-
graph embedding problem.
Initially, the complex graph
structure makes conventional methods difﬁcult to capture
the subtle local topological information (Jie et al. 2014). For
the subgraph based method (Cao et al. 2015), the number
of subgraphs is exponential to the size of the graphs. Thus
the subgraph enumeration process is both time and memory
consuming. Besides, simply preserving pairwise distances,
as with many spectral methods, is insufﬁcient for capturing
the structure of multiple graphs. Moreover, preserving both
local distances and graph topology is crucial for producing
effective low-dimensional representations of the brain net-
work data. Furthermore, traditional normalization strategies
cannot generate meaningful clustering results.

To address the aforementioned issues, in this paper we
propose a novel one-step Multi-view Multi-graph Embed-
ding (M2E) approach for brain network analysis. The goal
of M2E is to ﬁnd low-dimensional representations from
multi-view multi-graph data which reveal patterns and struc-
tures among the brain networks. The conceptual view of

Our approach is also closely related to the literature on
multi-view clustering and multi-view embedding. (Kumar
and Daum´e 2011; Kumar, Rai, and Daume 2011) are the
ﬁrst works proposed to solve the multi-view clustering prob-
lem via spectral projection. (Nie, Li, and Li 2016) extended
the multi-view spectral clustering to a parameter-free auto-
weighted method. Matrix factorization based methods (Liu
et al. 2013a) are another category, which mainly use non-
negative matrix factorization (NMF) to integrate multi-view
data. Additionally, (Shao, He, and Yu 2015) proposed a CP
factorization and (cid:96)1-norm regularization based method for
(Ma et al. 2017b) cou-
multi-view incomplete clustering.
pled the spectral clustering and (cid:96)2,1-norm to discriminate
the hubs and to reduce the potential inﬂuence of the hubs
for graph clustering. However, there is no method available
which enables us to take multiple graphs as input and con-
sider multi-graph structures; thus multi-view learning can-
not solve the brain network embedding problem well.

Preliminaries
In this section, we introduce some related concepts and no-
tation about tensor. Table 1 lists basic symbols that will be
used throughout the paper.

Tensors are higher order arrays that generalize the notion
of vectors and matrices. The order of a tensor is the num-
ber of dimensions (a.k.a. modes or ways). An M -th order
RI1×···×IM , where Im is the
tensor is represented as
cardinality of its m-th mode, m
[1 : M ]. All vectors are
column vectors unless otherwise speciﬁed. For an arbitrary
RI×J , its i-th row and j-th column vector are
matrix X
denoted by xi and xj, respectively.

X ∈

∈

∈

Deﬁnitions of partially symmetric tensor, mode-m matri-
cization and CP factorization are given below, which will be
used to present our model.

Deﬁnition 1 (Partial Symmetric Tensor). An M -th order
tensor is a rank-one partial symmetric tensor if it is partial
[1 : M ], and can be written
symmetric on modes i1, ..., ij ∈
as the tensor product of M vectors, i.e.,

= x(1)

X
= x(ij ).

x(M )

◦ · · · ◦

· · ·

where x(i1) =
Deﬁnition 2 (Mode-m Matricization). The mode-m matri-
RI1×···×IM , denoted by X(m) ∈
cization of a tensor
X ∈
RIm×J , where J = ΠM
q=1,q(cid:54)=mIq. Each tensor element with
indices (i1,
, iM ) maps to a matrix element (im, j), such
that

· · ·

M

j = 1 +

(cid:88)p=1,p(cid:54)=m

(ip −

1)Jp, with

Jp =

1,
Πp−1

(cid:40)

q=1,q(cid:54)=mIq, otherwise.

if p = 1 or (p = 2 and m = 1)

(1)

(2)

Deﬁnition 3 (CP Factorization). For a general tensor
X ∈
RI1×···×IM , its CANDECOMP/PARAFAC (CP) factoriza-

Figure 1: A conceptual view of Multi-view Multi-graph Em-
bedding (M2E)

M2E is shown in Figure 1. Our main contributions are sum-
marized as follows:

•

•

•

In each view, we stack all the brain networks on each
view into a tensor and use tensor and matrix techniques
to simultaneously leverage the dependencies and correla-
tions among multi-view and multi-graph data in a uniﬁed
framework. This provides an innovative perspective on
the analysis of brain network structures.
In order to reﬂect the latent clustering structure shared
by different views and different graphs, we require co-
efﬁcient matrices learned from different views towards a
consensus with soft regularization.
We present an effective optimization strategy to solve the
M2E problem, with consideration of symmetric structure
of brain networks.

Through extensive experiments on HIV and bipolar disorder
brain network datasets that contain fMRI and DTI views,
we demonstrate that M2E can signiﬁcantly boost the em-
bedding performances. Furthermore, the derived factors are
visualized which could be informative for investigating dis-
ease mechanisms.

Related Work
Brain Network Embedding makes characterization of brain
disorders at a whole-brain connectivity level possible, thus
providing a new direction for brain disease clustering. The
goal of graph embedding is to ﬁnd low-dimensional repre-
sentations of nodes that can preserve the important struc-
ture and properties of graphs (Ma et al. 2016). In particular,
(Mousazadeh and Cohen 2015) proposed a graph embedding
algorithm based on Laplacian-type operator on manifold,
which can apply to recover the geometry of data and extend
a function on new data points. Recently, (Ou et al. 2016)
established a general formulation of high-order proximity
measurements, and then applied it with generalized SVD
for graph embedding. In the ﬁeld of brain network neuro-
science, most of the existing works aim to learn the structure
from a speciﬁc kind of brain networks (Kong and Yu 2014;
Kuo et al. 2015). In contrast to the node embedding on sin-
gle graph, we aim at learning an effective graph embedding
approach on multi-view multi-graph brain networks, such as
fMRI brain network together with DTI brain network.

=

L

min

X(1),··· ,X(M )(cid:107)X − (cid:74)

· · ·

X(1),

, X(M )

(4)

2
F
(cid:75)(cid:107)

M2E Approach

Table 1: List of basic symbols.

Symbol Deﬁnition and Description

each lowercase letter represents a scale
each boldface lowercase letter represents a vector
each boldface uppercase letter represents a matrix
each calligraphic letter represents a tensor, set or space
a set of integers in the range of 1 to M inclusively.
denotes the outer product
denotes Khatri-Rao product
denotes the CP factorization

X
[1 : M ]

x
x
X

◦
(cid:12)
(cid:74)·(cid:75)

tion is

R

r=1
(cid:88)

X ≈

x(1)
r ◦ · · · ◦

x(M )
r ≡ (cid:74)

X(1), ..., X(M )

(3)

(cid:75)

where for m
∈
factor matrices of size Im ×
is used for shorthand.
and

[1 : M ], X(m) = [x(m)

, ..., x(m)
R ] are latent
R, R is the number of factors,

1

To obtain the CP factorization

, the ob-
(cid:75)
jective is to minimize the following estimation error:

X(1),
(cid:74)

, X(M )

· · ·

(cid:74)·(cid:75)

L

, X(M ). A
is not jointly convex w.r.t. X(1),
However,
widely used optimization technique is the Alternating Least
Squares (ALS) algorithm, which alternatively minimize
for each variable while ﬁxing the other, that is,

· · ·

L

X(k)

arg min
X(k) (cid:107)

X(k) −

←

X(k)(

i(cid:54)=kX(i))T
n

(cid:12)

2
F
(cid:107)

(5)

where

i(cid:54)=kX(i) = X(M )
M

X(k−1)

X(k+1)

X(1).

(cid:12)

(cid:12)

(cid:12)· · ·
Methodology
In this section, we ﬁrst deﬁne the problem of interest. Then
we formulate the proposed Multi-view Multi-graph Embed-
ding (M2E) method. Finally, we introduce an effective opti-
mization approach to solve the proposed formulation.

· · ·(cid:12)

Problem Deﬁnition
We study the problem of multi-view multi-graph embed-
ding for brain network clustering analysis. Suppose that
the problem includes N subjects with V views, where each
view has a set of N symmetric brain networks correspond-
ing to N subjects. Speciﬁcally, each brain network is rep-
resented as a weighted undirected graph, i.e., a symmet-
RM ×M where M denotes the
ric afﬁnity matrix W
number of nodes and each element reﬂects connectivity
between nodes. There exists a one-to-one mapping be-
tween nodes in different graphs, which means that all the
graphs have a common node set M . Thus, for the v-th
view, we have N graphs associated with N afﬁnity matri-
, W(v)
. We use
ces, denoted as
N }
to represent the multi-view

1 , W(v)
2 ,

(v) =
,

W(v)
(V )

D
(2),

· · ·

=

∈

(1),
D
· · ·
multi-graph instances.

{D

D

{
D

}

Figure 2: CP Factorization. The third-order partially sym-
is approximated by R rank-one tensors. The
metric tensor
r-th factor tensor is the tensor product of three vectors, i.e.,
hr ◦

hr ◦

fr.

X

∈

The goal of this work is to learn a common embedding
RN ×R, where
across all brain networks, denoted as F∗
R is the embedding dimension and each row of F∗ cor-
responds to an embedding of a brain network as a whole.
More speciﬁcally, we aim at ﬁnding F∗ by simultaneously
leveraging the dependencies and correlations among multi-
ple views and multiple graphs in
, and taking into account
the symmetric property of brain networks. In particular, we
investigate the use of learned embedding F∗ for clustering
brain networks. Let the number of clusters be K. So, we
cluster N brain networks into K groups.

D

Solving challenging multi-view multi-graph embedding
problem requires the use of “complex” structured models
– those incorporating relationships between multiple views
and multiple graphs. The multi-mode structure of tensor
provides a natural way to encode the underlying multiple
correlations between data (He et al. 2017). Inspired by the
success of tensor analysis on many structured learning prob-
lems, here we explore the use of tensor operator techniques
to consider all possible dependence relationships among dif-
ferent views and different graphs.

,

D

D

D

(V )

· · ·

.
}

(2),

Given a multi-view multi-graph dataset
(1),

=
In order to capture the multi-
{D
graph structures directly, we concatenate the afﬁnity
matrices of different subjects for each view to form a
third-order tensor comprising three modes: nodes, nodes,
, W(v)
N ]
and subjects, denoted as
[1 : V ]. Notice that since each brain
∈
network is a symmetric network, thus the resulting tensor is
a partial symmetric tensor.

(v) = [W(v)

RM ×M ×N , v

1 , W(v)
2 ,

· · ·

X

∈

Tensor provides a natural and efﬁcient representation for
multi-graph data, but there is no guarantee that such repre-
sentation will be good for subsequent learning, since learn-
ing will only be successful if the regularities that underlie the
data can be discerned by the model (He et al. 2014). In pre-
vious work, it was found that CP factorization is particularly
effective to acknowledge the connections and ﬁnd valuable
features among tensor data (Van Loan 2016). Motivated by
these observations, in the following we investigate how to
exploit the beneﬁts of CP factorization to ﬁnd an effective
embedding F∗ in the sense of multi-view partial symmetric
tensors

[1 : V ].

(v), v

A simple method is to learn a view-independent multi-

X

∈

(v), and then feed it
view representation from the tensors
into a conventional multi-view embedding method. This can
be formulated as follows:

X

V

(v)

(6)

||X

− (cid:74)

where H(v)

min
H(v),F(v)

H(v), H(v), F(v)

v=1
(cid:88)
RM ×R and F(v)

2
F
(cid:75)||
RN ×R are the latent
factor matrices obtained by CP factorization. A graphical
representation of this process in one view is given in Figure
2. H(v) can be viewed as common features of nodes among
all graphs involved in the v-th view, while F(v) are treated
as embedded features of each graph in the v-th view.

∈

∈

Based on the above obtained multi-view features F(v),
we can directly establish the following multi-view model to
learn a common embedding F∗:

V

v=1
(cid:88)

min
F∗

F(v)

λv||

F∗

2
F
||

−

(7)

where λv are the weight parameters reﬂecting the impor-
tance of different views.

However, the two-step method, referred to as M2E-TS, is
not guaranteed to produce an optimal clustering result, be-
cause multiple views and multiple graphs are explored sep-
arately. For clustering, we assume that a data point in differ-
ent views would be assigned to the same cluster with high
probability. Therefore, in terms of tensor factorization, we
require coefﬁcient matrices learned from different views to
be softly regularized towards a common consensus. This
consensus matrix is considered to reﬂect the latent cluster-
ing structure shared by different views and different graphs.
Based on this idea, we incorporate the Eq. (6) and Eq. (7)
together and achieve the following optimization problem for
M2E method:

= min

H(v),F∗,F(v)

O

V

+

v=1
(cid:88)

λv||

V

v=1
(cid:88)
F(v)

||X

− (cid:74)

F∗

2
F
||

−

Notice that the ﬁrst term is used to explore the dependen-
cies among multiple graphs, and the second term is used to
explore the consensus correlations among multiple views.
λv not only tune the relative weight among different views,
but also between the ﬁrst term and the second term. F∗ is
the ﬁnal embedding solution used for mult-view multi-graph
brain network clustering. To induce groupings on F∗, we
simply use K-means (Hartigan and Wong 1979).

In order to verify the effectiveness of soft regularization
in Eq. (8), we propose M2E-DS as compared method which
learns the latent embedding representation by using the di-
rectly shared coefﬁcient matrices F∗ for all views (Liu et al.
2013b); the objective function is shown as

V

min
H(v),F∗

(v)

H(v), H(v), F∗

(9)

||X

− (cid:74)
In this formulation, different views are treated equally. How-
ever, in reality, different views may have different effects.
The detail will be discussed in the Section Experiments.

v=1
(cid:88)

2
F
(cid:75)||

∈

RM ×R, F(v)

RN ×R and F∗

Optimization Framework
The model parameters in Eq. (8) that have to be estimated
RN ×R.
include H(v)
Since the optimization problem is not convex with respect
to H(v), F(v) and F∗ together, there is no closed-form solu-
tion. We introduce an effective iteration method to solve
this problem. The main idea is to decouple the parame-
ters using an Alternating Direction Method of Multipliers
(ADMM) approach (Boyd et al. 2011). Speciﬁcally, the fol-
lowing three steps are repeated until convergence.

∈

∈

Fixing F(v) and F∗, compute H(v) Note that
(v) is
a partially symmetric tensor and the objective function in
Eq. (8) involving a fourth-order term H(v) is difﬁcult to op-
timize directly. To obviate this problem, we use a variable
substitution technique and minimize the following objective
function

X

min

(v)

H(v), P(v), F(v)

2
F
(cid:75)||

H(v),P(v) ||X
s.t. H(v) = P(v)

− (cid:74)

(10)

where P(v) are auxiliary variables.

The augmented Lagrangian function for the problem in

Eq. (10) is

L

(H(v), P(v)) =

(cid:107)X
+ tr(U(v)T (H(v)

(v)

− (cid:74)
P(v))) +

H(v), P(v), F(v)
µ
2 (cid:107)

H(v)

−

2
F
(cid:75)(cid:107)
P(v)

−

2
F
(cid:107)
(11)

RM ×R are Lagrange multipliers, and µ is the
where U(v)
penalty parameter which can be adjusted efﬁciently accord-
ing to (Lin, Liu, and Su 2011).

∈

To compute H(v), the optimization problem in Eq. (11)

min
H(v) ||

X(v)

(1) −

H(v)D(v)T

2
F +
||

µ
2 ||

H(v)

P(v) +

−

1
µ

U(v)

2
F
||
(12)

where X(v)

RM ×(M N ) is the mode-1 matricization of
R(NM )×R.

(1) ∈
(v), and D(v) = F(v)
We rewrite Eq. (12) in the trace form as

P(v)

(cid:12)

X

∈

tr(H(v)A(v)H(v)T

)

min
H(v)

where A(v) = D(v)T
U(v).
µP(v)

−

D(v) + µ

tr(B(v)T

H(v))

−
2 I, and B(v) = 2X(v)

(1)D(v) +

(13)

The problem (13) is a univariate optimization problem,
and can be solved easily. An effective approach to solve
such a problem is by the proximal gradient method (Parikh,
Boyd, and others 2014), which updates H(v) by

H(v)

t+1 ←

H(v)

t −

1
L(v)

(2H(v)T

A(v)

B(v))

(14)

−

where L(v) is the Lipschitz coefﬁcient of Eq.(13) that

equals to the maximum eigenvalue of 2A(v).

(v)

H(v), H(v), F(v)

(8)

can be formulated as

2
F
(cid:75)||

To efﬁciently compute D(v)T

D(v), we consider the fol-
lowing property of the Khatri-Rao product of two matrices

Algorithm 1 M2E

D(v)T

D(v) = (F(v)
(cid:12)
= (F(v)T
F(v))

P(v)T

)(F(v)
(P(v)T

P(v))

(cid:12)
P(v))

∗
denotes the Hadamard product or element-wise

(15)

where
product of two matrices.

∗

sively in a similar way

Then the auxiliary matrix P(v) can be optimized succes-

P(v)

t+1 ←

P(v)

1
t −
L(v)
E(v) + µ

(2P(v)

t A(v)

B(v))

(16)

−

where A(v) = E(v)T
µH(v) + U(v). And E(v) = F(v)
X(v)

2 (I) and B(v) = 2X(v)
H(v)

(2)E(v) +
R(NM )×R and

(cid:12)

∈

RM ×(M N ) is the mode-2 matricization of

(2) ∈
Moreover, we update the Lagrange multipliers U(v) using

(v).

X

the gradient descent method by
U(v)

U(v)

t + µ(H(v)
Fixing F∗ and H(v), compute F(v) By ﬁxing F∗ and
H(v), We minimize the following objective function

t ←

P(v))

(17)

−

X(v)

(3) −

F(v)J(v)T

2
F + λ(v)||
||

F(v)

F∗

2
F
||

−

(18)

•

min
F(v) ||
where X(v)
tensor

RN ×(MM ) is the mode-3 matricization of

(3) ∈
(v) and J(v) = P(v)

H(v)

R(MM )×R.

X

(cid:12)
Such an optimization problem can be solved in a similar
way as Eq. (12), from which we get the update rule of F(v)
as follows

∈

F(v)

t+1 ←

F(v)

t −

1
L(v)

(2F(v)

t A(v)

B(v))

(19)

−

where A(v) = J(v)T
2λ(v)F∗, and L(v) is the maximum eigenvalue of 2A(v).

J(v) + λ(v)(I), Bv = 2X(v)

(3)J(v) +

•

over F∗ When H(v)
Fixing H(v) and F(v), minimize
and F(v) are ﬁxed, the problem in Eq. (8) is reduce to a
convex optimization problem with respect to F∗. By taking
the derivative of the objective function
in Eq. (8) with
O
respect to F∗ and setting it to zero, we get

O

F∗ =

V
v=1 λ(v)F(v)
V
v=1 λ(v)

(cid:80)

(20)

Based on the above analysis, we outline the optimization
(cid:80)
framework for multi-view multi-graph brain network em-
bedding in Algorithm 1.

Computational Analysis Algorithm 1 iteratively solves
Eq. (8). In each iteration solving H (v), P (v) and F (v) all
requires O(R3 + R2(2M + N + 1) + M 2N R), solving
U (v) requires O(M R), and solving F ∗ requires O(N RV ).
Overall, the time complexity involves O(M axIter(R3 +
R2(2M + N + 1) + (M 2N + M + N V )R)V ), which is
linear to the number of nodes N , so the proposed method is
applicable for larger scale of brain network.

Input: Partically-symmetric tensor

(v), weight parame-

X

ters α1 and α2, and embedding dimension R

∼ N

(0, 1), U = 0, µ = 10

1: Initialize H(v), F(v)
2: repeat
3:
4:
5:
6:
7: until convergence
Output: Consensus embedding matrix F∗

Update H(v) and P(v) by Eq. (14) and Eq. (16)
Update U(v) by Eq. (17)
Update F(v) by Eq. (19)
Update F∗ by Eq. (20)

Experiments and Evaluation
In order to empirically evaluate the performance of the pro-
posed M2E approach for multi-view multi-graph brain net-
work clustering analysis, we test our model on two real
datasets, HIV and Bipolar disorder with fMRI brain net-
works and DTI brain networks, and compare with several
state-of-the-art multi-view clustering methods.

Data Collection and Preprocessing

Human Immunodeﬁciency Virus Infection (HIV): The
original dataset is unbalanced, we randomly sampled 35
patients and 35 controls from the dataset for performance
evaluation. A detailed description about data acquisition
is available in (Cao et al. 2015). For fMRI data, we used
DPARSF 1 and SPM 2 toolboxes for preprocessing. We
construct each graph with 90 nodes where links are cre-
ated based on the correlations between different brain re-
gions. For DTI data, we used FSL toolbox3 for prepro-
cessing and parcellated each DTI image into 90 regions
by the AAL (Tzourio-Mazoyer et al. 2002).

Bipolar Disorder (BP): This dataset consists of 52 bipo-
lar subjects who are currently in euthymia and 45 healthy
controls. For fMRI data, we used the toolbox CONN 4 to
construct fMRI data of the BP brain network (Whitﬁeld-
Gabrieli and Nieto-Castanon 2012). Using the 82 labels
Freesurfer-generated cortical/subcortical gray matter re-
gions, functional brain networks were derived using pair-
wise BOLD signal correlations. For DTI, same as fMRI,
we constructed the DTI image into 82 regions.

Baselines and Metrics
We compare the proposed M2E with eight other methods for
multi-view clustering on brain networks. We adopt accuracy
and F1-score as our evaluation metrics.

(1) SEC is a single-view spectral embedding clustering
framework (Nie et al. 2011). (2) convexSub is a convex
subspace representation learning method (Guo 2013). (3)
CoRegSc is a co-regularization based multi-view spectral

1http://rfmri.org/DPARSF
2http://www.l.ion.ucl.ac.uk/spm/software/

spm8

3https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/
4http://www.nitrc.org/projects/conn

Table 2: Clustering Accuracy and F1 score on HIV dataset and BP dataset

Dataset Measure
Accuracy
F1
Precision
Recall
Accuracy
F1
Precision
Recall

HIV

BP

Method
SEC
50.00(8)
49.74(8)
50.00(9)
49.86(8)
54.64(7)
56.86(7)
58.00(9)
55.76(7)

convexSub AMGL multiNMF CoRegSc
57.14(5)
52.86(7)
60.53(4)
53.52(7)
56.10(5)
52.77(7)
65.71(4)
54.29(7)
56.70(6)
52.57(9)
60.38(6)
28.46(8)
74.78(1)
59.25(5)
61.53(5)
17.84(8)

57.23(4)
56.18(6)
57.37(5)
55.98(5)
58.52(4)
66.99(3)
58.34(8)
80.40(2)

52.86(7)
53.52(7)
52.77(7)
54.29(7)
52.77(8)
28.13(9)
74.83(2)
17.71(9)

MIC
55.72(6)
58.28(5)
61.12(4)
56.28(6)
61.86(2)
72.59(1)
59.03(6)
94.23(1)

SCMV-3DT M2E-TS M2E-DS
68.57(2)
68.57(2)
68.57(2)
68.57(3)
60.82(3)
61.22(5)
65.22(4)
57.69(6)

52.86(7)
49.23(9)
53.25(6)
45.57(9)
57.73(5)
64.34(4)
58.73(7)
71.15(3)

64.29(3)
66.67(3)
62.50(3)
71.43(2)
54.64(7)
59.26(6)
57.14(9)
61.53(5)

M2E
71.43(1)
72.22(1)
69.73(1)
75.00(1)
68.04(1)
68.69(2)
72.34(3)
65.38(4)

clustering framework (Kumar, Rai, and Daume 2011). (4)
MultiNMF is the NMF-based multi-view clustering method
by searching for a factorization that gives compatible clus-
tering solutions across multiple views (Liu et al. 2013a). (5)
MIC ﬁrst uses the kernel matrices to form an initial ten-
sor across all the multiple sources (Shao, He, and Yu 2015).
(6) AMGL is a recently proposed non-parameter multi-view
(7)
spectral learning framework (Nie, Li, and Li 2016).
SCMV-3DT uses t-product in the third-order tensor space
and represents multi-view data by a t-linear combination
with sparse and low-rank (Yin et al. 2016). (8) M2E-TS
is the two-step method of M2E. (9) M2E-DS is the directly
shared method of M2E mentioned on Section Methodology.
Since SEC is designed for single-view data, we ﬁrst con-
catenate all the views together and then apply SEC on the
concatenated views. For all the spectral clustering based
methods, we construct the RBF kernel matrices with ker-
nel width σ to be the median distance among all the brain
network samples. Following (Von Luxburg 2007), we con-
struct each graph by selecting 10-nearest neighbors among
raw data. We tune the parameters of each baseline meth-
ods using the strategy mentioned in the corresponding paper.
There are three main parameters in our model, namely λ1,
λ2 and R, where λ1 and λ2 are the weight parameters reﬂect-
ing the importance of different views, and R is the number of
factors representing the embedded dimension. We apply the
grid search to determine the optimal values of these three pa-
rameters. In particular, we empirically select λ1 and λ2 from
10−4, 10−2, ..., 104
.
{
}
For evaluation, since there are two possible label values, nor-
mal and control, for each brain network sample on both HIV
and BP datasets, we set the number of clusters K to be 2 and
test how well our method can group the brain networks of
patients and normal controls into two different clusters.

, and R is selected from
}

1, 2, ..., 20
{

In order to make a fair comparison, we apply the “Litek-
means” function in Matlab (Cai 2011) for all the compared
methods during their K-means clustering step. We repeat
this K-means clustering procedure 20 times with random
initialization, as “Litekmeans” greatly depends on initializa-
tion. For the evaluation, we repeat running the program of
each clustering methods 20 times and report the average Ac-
curacy, F1 score, Precision and Recall as the results.

Clustering Results

Table 2 shows the clustering results. We see that in terms
the proposed M2E method performs better
of accuracy,

than all the other baseline methods on both HIV and BP
datasets. The single-view clustering SEC does not dis-
tinguish the features from different views, which leads to
a poor performance than the multi-view methods. Com-
pared with subgraph method convexSub, M2E achieves bet-
ter performance, which may because our method can cap-
ture the complex multi-way relation in brain networks. The
common property of three multi-view clustering methods,
AMGL, multiNMF and CoRegSc, is that the features they
learned for each view are based on vector representations.
However, for graph instances, the structural information is
hardly persevered by the ﬂattened vector representations,
which could be the underlying reason that these three meth-
ods cannot outperform M2E. Moreover, by using tensor
techniques to model the multi-view multi-graph collectively,
M2E could learn discriminative latent representations and
graph-speciﬁc features. While MIC and SCMV-3DT also
use tensor to represent the multi-view learning, their perfor-
mance are not beyond M2E. This is mainly because they still
learn the vector representation of each graph, which makes
them fail to capture the multi-graph structure. M2E-TS can-
not explore multiple views and multiple graphs simultane-
ously; therefore it gets a worse clustering result than M2E.
Besides, by comparing with M2E-DS, the proposed M2E
utilizing the constraint to regularize clustering solutions ob-
tained from multiple views towards a consensus solution can
ﬁnd the true clustering more effectively than simply concate-
nating all the features together.

Note that in terms of F1 score, M2E is the best one on HIV
dataset, while MIC is the best on BP dataset. This is because
MIC method clusters large number of samples into the same
class while M2E produces relatively even results. Besides,
the precision-recall result also explains why our M2E does
not outperform MIC in terms of F1 score. And the other
methods like convexSub, AMGL and multiNMF have the
same problem as MIC, therefore their precision or recall re-
sults are higher than M2E. However, in our context, its not
reasonable to cluster all samples in one class, in that it can-
not distinguish patients from controls.

Parameter Sensitivity Analysis

In this section, to evaluate how the parameters of M2E af-
fects performance, we study the sensitivity of the three main
parameters, including λ1, λ2 and R, where λ1 is the param-
eter of DTI view, λ2 is the parameter of fMRI view and R is
the embedded dimension. For evaluating the regularization

(a) HIV

(b) BP

Figure 3: Accuracy with different weights λ1 and λ2 on HIV
dataset and BP dataset

(a) HIV

(b) BP

Figure 4: Accuracy with different embedded dimensions R
on HIV dataset and BP dataset

parameter λ1 and λ2, we set the R to the optimal value.

According to Figures 3a and 3b, we observe that the re-
sult is relatively sensitive to the change of λ1 and λ2, which
shows that different views have different effects on the per-
formance. Besides, when λ1 and λ2 are very large, the per-
formance gets worse. This shows that λ1 and λ2 are also
important for tuning the ﬁrst term and the second term of
objective function in Eq. (8).

Figures 4a and 4b show the performance of M2E with the
R value varying from 1 to 20. We can observe that the em-
bedded dimension has a signiﬁcant effect on the accuracy.
The highest accuracy is achieved when R equals to 7 on HIV
dataset and 12 on BP dataset. Generally speaking, the per-
formance shakes greatly with the change of the rank. But
in most cases the optimal value of R lies in a small range
of values as demonstrated in (Hao et al. 2013) and it is not
time-consuming to ﬁnd it using the grid search strategy in
practical applications.

Factor Analysis
M2E extracts H(v) consisting of h(v)
r , for r = 1, ..., R and
F(v) consisting of f (v)
, where these factors indicate the sig-
natures of sources in vertex and subject domain, respec-
tively. Due to limited space, we only show the factors on
HIV dataset here. We visualize the learning results of H(v)
and F(v) on fMRI and DTI dataset in Figures 5a and 5b.

r

We show the largest factors in terms of magnitude for
fMRI and DTI in Figures 5a and 5b. Left panel shows the
node embedded feature H(v). The coordinate system repre-
sents neuroanatomy and the color shows the activity inten-
sity of the brain region. The right panel shows the graph em-
bedded feature F(v) which represents the factor strengths for

(a) fMRI

(b) DTI

Figure 5: Embedded features of nodes learning H and
graphs learning F (left and right panels respectively) from
fMRI and DTI on HIV dataset

both patients and controls. Based on our objective function
in Eq. (8), H(v) is used to preserve the individual informa-
tion of each view and dependencies among multiple graphs.
As we can see from the left panels of Figures 5a and 5b, the
embedded neuroanatomy learned from fMRI data and DTI
data are widely different from each other. However, F(v)
is learned by forcing each view to the consensus correlation
F∗. From the right panels of Figures 5a and 5b, results from
both views show that the controls have relatively positive
correlation with node embedded feature, while the patient
have relatively negative correlation. Moreover, those neu-
roimaging ﬁndings in HIV generally support clinical obser-
vations of functional impairments in attention, psychomo-
tor speed, memory, and executive function.
In particular,
regions identiﬁed in our current study are consistent with
those reported in structural and functional MRI studies of
HIV associated neurocognitive disorder (HAND), including
regions within the frontal and parietal lobes (Risacher and
Saykin 2013).

Conclusion
We present a novel multi-view multi-graph embedding
framework based on partially-symmetric tensor factoriza-
tion for brain network analysis. The proposed M2E method
not only takes advantages of the complementary and depen-
dent information among multiple views and multiple graphs,
but also exploits the graph structures. In particular, we ﬁrst
model the multi-view multi-graph data as multiple partially-
symmetric tensors, and then learn the consensus graph em-
bedding via the integration of tensor factorization and a
multi-view embedding method. We apply our approach on
two real HIV and BP datasets with a fMRI view and a DTI
view for unsupervised brain network analysis. Extensive ex-
perimental results demonstrate the effectiveness of M2E for
multi-view multi-graph embedding on brain networks.

Acknowledgements
This work is supported in part by NSF grants No.
IIS-1526499 and CNS-1626432, NIH grant No. R01-
MH080636, and NSFC grants No. 61503253 and 61672313.

References
[Boyd et al. 2011] Boyd, S.; Parikh, N.; Chu, E.; Peleato, B.;
and Eckstein, J. 2011. Distributed optimization and statisti-
cal learning via the alternating direction method of multipli-
ers. Foundations and Trends R
in Machine Learning 3(1):1–
(cid:13)
122.
[Cai 2011] Cai, D. 2011. Litekmeans: the fastest matlab im-
plementation of kmeans. Software available at: http://www.
zjucadcg. cn/dengcai/Data/Clustering. html.
[Cao et al. 2014] Cao, B.; He, L.; Kong, X.; Philip, S. Y.;
2014. Tensor-based multi-
Hao, Z.; and Ragin, A. B.
view feature selection with applications to brain diseases.
In ICDM, 40–49. IEEE.
[Cao et al. 2015] Cao, B.; Kong, X.; Zhang, J.; Yu, P. S.; and
Ragin, A. B. 2015. Identifying hiv-induced subgraph pat-
terns in brain networks with side information. Brain infor-
matics 2(4):211–223.
[Guo 2013] Guo, Y. 2013. Convex subspace representation
learning from multi-view data. In AAAI, volume 1, 2.
[Hao et al. 2013] Hao, Z.; He, L.; Chen, B.; and Yang,
X. 2013. A linear support higher-order tensor machine
for classiﬁcation. IEEE Transactions on Image Processing
22(7):2911–2920.
[Hartigan and Wong 1979] Hartigan, J. A., and Wong, M. A.
1979. Algorithm as 136: A k-means clustering algorithm.
Journal of the Royal Statistical Society. Series C (Applied
Statistics) 28(1):100–108.
[He et al. 2014] He, L.; Kong, X.; Yu, P. S.; Yang, X.; Ragin,
A. B.; and Hao, Z. 2014. Dusk: A dual structure-preserving
kernel for supervised tensor learning with applications to
neuroimages. In SDM, 127–135. SIAM.
[He et al. 2017] He, L.; Lu, C.-T.; Ma, G.; Wang, S.; Shen,
L.; Philip, S. Y.; and Ragin, A. B. 2017. Kernelized support
tensor machines. In ICML, 1442–1451.
[Jie et al. 2014] Jie, B.; Zhang, D.; Gao, W.; Wang, Q.; Wee,
C.-Y.; and Shen, D. 2014. Integration of network topological
and connectivity properties for neuroimaging classiﬁcation.
IEEE Transactions on Biomedical Engineering 61(2):576–
589.
[Kong and Yu 2014] Kong, X., and Yu, P. S. 2014. Brain
network analysis: a data mining perspective. ACM SIGKDD
Explorations Newsletter 15(2):30–38.
[Kumar and Daum´e 2011] Kumar, A., and Daum´e, H. 2011.
A co-training approach for multi-view spectral clustering. In
ICML, 393–400.
[Kumar, Rai, and Daume 2011] Kumar, A.; Rai, P.; and
Daume, H. 2011. Co-regularized multi-view spectral clus-
tering. In NIPS, 1413–1421.
[Kuo et al. 2015] Kuo, C.-T.; Wang, X.; Walker, P.;
Carmichael, O.; Ye, J.; and Davidson, I. 2015. Uniﬁed and
contrasting cuts in multiple graphs: application to medical
imaging segmentation. In KDD, 617–626. ACM.
and Su, Z.
[Lin, Liu, and Su 2011] Lin, Z.; Liu, R.;
2011. Linearized alternating direction method with adaptive
penalty for low-rank representation. In NIPS, 612–620.

[Liu et al. 2013a] Liu, J.; Wang, C.; Gao, J.; and Han, J.
2013a. Multi-view clustering via joint nonnegative matrix
factorization. In SDM, 252–260. SIAM.
[Liu et al. 2013b] Liu, W.; Chan, J.; Bailey, J.; Leckie, C.;
and Ramamohanarao, K. 2013b. Mining labelled tensors
by discovering both their common and discriminative sub-
spaces. In SDM, 614–622. SIAM.
[Ma et al. 2016] Ma, G.; He, L.; Cao, B.; Zhang, J.; Yu, P. S.;
and Ragin, A. B. 2016. Multi-graph clustering based on
interior-node topology with applications to brain networks.
In ECML-PKDD, 476492. Springer.
[Ma et al. 2017a] Ma, G.; He, L.; Lu, C.-T.; Shao, W.; Yu,
P. S.; Leow, A. D.; and Ragin, A. B. 2017a. Multi-view
clustering with graph embedding for connectome analysis.
In CIKM.
[Ma et al. 2017b] Ma, G.; Lu, C.-T.; He, L.; Yu, P. S.; and
Ragin, A. B. 2017b. Multi-view graph embedding with hub
detection for brain network analysis. In ICDM.
[Mousazadeh and Cohen 2015] Mousazadeh, S., and Cohen,
I. 2015. Embedding and function extension on directed
graph. Signal Processing 111:137–149.
[Nie et al. 2011] Nie, F.; Zeng, Z.; Tsang, I. W.; Xu, D.; and
Zhang, C. 2011. Spectral embedded clustering: A frame-
work for in-sample and out-of-sample spectral clustering.
IEEE Transactions on Neural Networks 22(11):1796–1808.
[Nie, Li, and Li 2016] Nie, F.; Li, J.; and Li, X.
2016.
Parameter-free auto-weighted multiple graph learning: A
framework for multiview clustering and semi-supervised
classiﬁcation. In IJCAI.
[Ou et al. 2016] Ou, M.; Cui, P.; Pei, J.; Zhang, Z.; and Zhu,
W. 2016. Asymmetric transitivity preserving graph embed-
ding. In KDD, 1105–1114.
[Parikh, Boyd, and others 2014] Parikh, N.; Boyd, S.; et al.
2014. Proximal algorithms. Foundations and Trends R
in
(cid:13)
Optimization 1(3):127–239.
[Risacher and Saykin 2013] Risacher, S. L., and Saykin,
A. J. 2013. Neuroimaging biomarkers of neurodegenera-
tive diseases and dementia. In Seminars in neurology, vol-
ume 33, 386–416. Thieme Medical Publishers.
[Shao, He, and Yu 2015] Shao, W.; He, L.; and Yu, P. S.
2015. Clustering on multi-source incomplete data via tensor
modeling and factorization. In PAKDD, 485–497. Springer.
[Sun et al. 2017] Sun, L.; Wang, Y.; Cao, B.; Yu, P. S.; Srisa-
an, W.; and Leow, A. D. 2017. Sequential keystroke behav-
ioral biometrics for mobile user identiﬁcation via multi-view
deep learning. In ECML-PKDD.
[Tzourio-Mazoyer et al. 2002] Tzourio-Mazoyer, N.; Lan-
deau, B.; Papathanassiou, D.; Crivello, F.; Etard, O.; Del-
croix, N.; Mazoyer, B.; and Joliot, M. 2002. Automated
anatomical labeling of activations in spm using a macro-
scopic anatomical parcellation of the mni mri single-subject
brain. Neuroimage 15(1):273–289.
[Van Loan 2016] Van Loan, C. F. 2016. Structured ma-
In Exploiting Hidden Struc-
trix problems from tensors.
ture in Matrix Computations: Algorithms and Applications.
Springer. 1–63.

[Von Luxburg 2007] Von Luxburg, U. 2007. A tutorial on
spectral clustering. Statistics and computing 17(4):395–416.
[Whitﬁeld-Gabrieli and Nieto-Castanon 2012] Whitﬁeld-
Gabrieli, S., and Nieto-Castanon, A.
2012. Conn: a
functional connectivity toolbox for correlated and anticorre-

lated brain networks. Brain connectivity 2(3):125–141.

[Yin et al. 2016] Yin, M.; Gao, J.; Xie, S.; and Guo, Y. 2016.
Low-rank multi-view clustering in third-order tensor space.
arXiv preprint arXiv:1608.08336.

