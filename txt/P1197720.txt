Edinburgh Neural Machine Translation Systems for WMT 16

Rico Sennrich and Barry Haddow and Alexandra Birch
School of Informatics, University of Edinburgh
{rico.sennrich,a.birch}@ed.ac.uk, bhaddow@inf.ed.ac.uk

6
1
0
2
 
n
u
J
 
7
2
 
 
]
L
C
.
s
c
[
 
 
2
v
1
9
8
2
0
.
6
0
6
1
:
v
i
X
r
a

Abstract

systems
each

We participated in the WMT 2016
shared news translation task by build-
translation
neural
ing
for
four
trained
pairs,
language
English↔Czech,
in both directions:
English↔German,
English↔Romanian
and English↔Russian. Our systems are
based on an attentional encoder-decoder,
using BPE subword segmentation for
open-vocabulary translation with a ﬁxed
vocabulary. We experimented with us-
the
ing automatic back-translations of
monolingual News corpus as additional
training data, pervasive dropout,
and
target-bidirectional models. All reported
improvements,
methods give substantial
and we see improvements of 4.3–11.2
BLEU over our baseline systems.
In the
human evaluation, our systems were the
(tied) best constrained system for 7 out
of 8 translation directions in which we
participated.1 2

1 Introduction

We participated in the WMT 2016 shared news
translation task by building neural
translation
systems for four language pairs: English↔Czech,
English↔Romanian
English↔German,
and English↔Russian.
systems
are
Our
encoder-decoder
based
(Bahdanau et al., 2015),
using BPE subword
segmentation for open-vocabulary translation
with a ﬁxed vocabulary (Sennrich et al., 2016b).
We experimented with using automatic back-
translations of the monolingual News corpus as

attentional

on

an

1We have

implementation that we
the experiments as an open source toolkit:

used for
https://github.com/rsennrich/nematus

released the

2We

have
synthetic

ﬁgs,
data
https://github.com/rsennrich/wmt16-scripts

training

scripts,
and

sample

con-
trained models:

released

additional
pervasive
bidirectional models.

training data (Sennrich et al., 2016a),
target-
dropout

(Gal, 2015),

and

2 Baseline System

Our systems are attentional encoder-decoder net-
works (Bahdanau et al., 2015). We base our im-
plementation on the dl4mt-tutorial3, which we en-
hanced with new features such as ensemble decod-
ing and pervasive dropout.

We use minibatches of size 80, a maximum sen-
tence length of 50, word embeddings of size 500,
and hidden layers of size 1024. We clip the gra-
dient norm to 1.0 (Pascanu et al., 2013). We train
the models with Adadelta (Zeiler, 2012), reshuf-
ﬂing the training corpus between epochs. We
validate the model every 10 000 minibatches via
BLEU on a validation set (newstest2013, new-
stest2014, or half of newsdev2016 for EN↔RO).
We perform early stopping for single models, and
use the 4 last saved models (with models saved ev-
ery 30 000 minibatches) for the ensemble results.
Note that ensemble scores are the result of a sin-
gle training run. Due to resource limitations, we
did not train ensemble components independently,
which could result in more diverse models and bet-
ter ensembles.

Decoding is performed with beam search with
a beam size of 12. For some language pairs, we
used the AmuNMT C++ decoder4 as a more efﬁ-
cient alternative to the theano implementation of
the dl4mt tutorial.

2.1 Byte-pair encoding (BPE)

To enable open-vocabulary translation, we seg-
ment words via byte-pair encoding (BPE)5
(Sennrich et al., 2016b). BPE, originally devised

3https://github.com/nyu-dl/dl4mt-tutorial
4https://github.com/emjotde/amunmt
5https://github.com/rsennrich/subword-nmt

as a compression algorithm (Gage, 1994),
adapted to word segmentation as follows:

is

First, each word in the training vocabulary is
represented as a sequence of characters, plus an
end-of-word symbol. All characters are added to
the symbol vocabulary. Then, the most frequent
symbol pair is identiﬁed, and all its occurrences
are merged, producing a new symbol that is added
to the vocabulary. The previous step is repeated
until a set number of merge operations have been
learned.

BPE starts from a character-level segmentation,
but as we increase the number of merge opera-
tions, it becomes more and more different from a
pure character-level model in that frequent charac-
ter sequences, and even full words, are encoded as
a single symbol. This allows for a trade-off be-
tween the size of the model vocabulary and the
length of training sequences. The ordered list of
merge operations, learned on the training set, can
be applied to any text to segment words into sub-
word units that are in-vocabulary in respect to the
training set (except for unseen characters).

To increase consistency in the segmentation of
the source and target text, we combine the source
and target side of the training set for learning BPE.
For each language pair, we learn 89 500 merge op-
erations.

3 Experimental Features

3.1 Synthetic Training Data

training as described

WMT provides
task participants with large
amounts of monolingual data, both in-domain
this mono-
and out-of-domain. We exploit
lingual data
in
for
(Sennrich et al., 2016a). Speciﬁcally, we sample
a subset of the available target-side monolingual
corpora, translate it automatically into the source
side of the respective language pair, and then
use this synthetic parallel data for training. For
the back-translation
example,
is performed with a RO→EN system,
and
vice-versa.

for EN→RO,

Sennrich et al. (2016a) motivate the use of
re-
monolingual data with domain adaptation,
ducing overﬁtting, and better modelling of ﬂu-
ency. We sample monolingual data from the News
Crawl corpora6, which is in-domain with respect

6Due to recency effects, we expect last year’s corpus to be
most relevant, and sampled from News Crawl 2015 for EN-
RO, EN-RU and EN-CS; for EN-DE, we re-used data from

type
parallel
synthetic (∗ →EN)
synthetic (EN→ ∗)

DE
4.2
4.2
3.6

CS RO RU
2.1
0.6
2.0
2.0
2.0
2.3

52.0
10.0
8.2

Table 1: Amount of parallel and synthetic training
data (number of sentences, in millions) for EN-
* language pairs. For synthetic data, we separate
the data according to whether the original mono-
lingual language is English or not.

to the test set.

The

amount of monolingual data back-
translated for each translation direction ranges
from 2 million to 10 million sentences. Statistics
about the amount of parallel and synthetic training
data are shown in Table 1. With dl4mt, we
observed a translation speed of about 200 000
sentences per day (on a single Titan X GPU).

3.2 Pervasive Dropout
For English↔Romanian, we observed poor per-
formance because of overﬁtting. To mitigate this,
we apply dropout to all layers in the network, in-
cluding recurrent ones.

Previous work dropped out different units at
each time step. When applied to recurrent con-
nections, this has the downside that it impedes
the information ﬂow over long distances, and
Pham et al. (2014) propose to only apply dropout
to non-recurrent connections.

Instead, we follow the approach suggested by
Gal (2015), and use the same dropout mask at each
time step. Our implementation differs from the
recommendations by Gal (2015) in one respect:
we also drop words at random, but we do so on
a token level, not on a type level. In other words,
if a word occurs multiple times in a sentence, we
may drop out any number of its occurrences, and
not just none or all.

In our English↔Romanian experiments, we
drop out full words (both on the source and tar-
get side) with a probability of 0.1. For all other
layers, the dropout probability is set to 0.2.

3.3 Target-bidirectional Translation

We found that during decoding,
the model
would occasionally assign a high probability to
words based on the target context alone,
ig-
noring the source sentence. We speculate that

(Sennrich et al., 2016a), which was randomly sampled from
News Crawl 2007–2014.

system

baseline
+synthetic
+ensemble
+r2l reranking

EN→DE
test
dev
26.8
22.4
31.6
25.8
33.1
27.5
34.2
28.1

DE→EN
test
dev
28.5
26.4
36.2
29.9
37.5
31.5
38.6
32.1

Table 2: English↔German translation results
(BLEU) on dev (newstest2015) and test (new-
stest2016). Submitted system in bold.

this is an instance of the label bias problem
(Lafferty et al., 2001).

To mitigate this problem, we experiment with
training separate models that produce the target
text from right-to-left (r2l), and re-scoring the n-
best lists that are produced by the main (left-to-
right) models with these r2l models. Since the
right-to-left model will see a complementary tar-
get context at each time step, we expect that the
averaged probabilities will be more robust. In par-
allel to our experiments, this idea was published
by Liu et al. (2016).

We increase the size of the n-best-list to 50 for

the reranking experiments.

A possible criticism of the l-r/r-l reranking ap-
proach is that the gains actually come from adding
diversity to the ensemble, since we are now us-
ing two independent runs. However experiments
in (Liu et al., 2016) show that a l-r/r-l reranking
systems is stronger than an ensemble created from
two independent l-r runs.

4 Results

4.1 English↔German
Table 2 shows results for English↔German. We
observe improvements of 3.4–5.7 BLEU from
training with a mix of parallel and synthetic data,
compared to the baseline that is only trained on
parallel data. Using an ensemble of the last 4
checkpoints gives further improvements (1.3–1.7
BLEU). Our submitted system includes rerank-
ing of the 50-best output of the left-to-right model
with a right-to-left model – again an ensemble
of the last 4 checkpoints – with uniform weights.
This yields an improvements of 0.6–1.1 BLEU.

4.2 English↔Czech
For English→Czech, we trained our baseline
model on the complete WMT16 parallel training
set (including CzEng 1.6pre (Bojar et al., 2016)),

until we observed convergence on our heldout
set (newstest2014). This took approximately 1M
minibatches, or 3 weeks. Then we continued
training the model on a new parallel corpus,
comprising 8.2M sentences back-translated from
the Czech monolingual news2015, 5 copies of
news-commentary v11, and 9M sentences sam-
pled from Czeng 1.6pre. The model used for back-
translation was a neural MT model from earlier
experiments, trained on WMT15 data. The train-
ing on this synthetic mix continued for a further
400,000 minibatches.

The right-left model was trained using a simi-
lar process, but with the target side of the paral-
lel corpus reversed prior to training. The resulting
model had a slightly lower BLEU score on the dev
data than the standard left-right model. We can see
in Table 3 that back-translation improves perfor-
mance by 2.2–2.8 BLEU, and that the ﬁnal system
(+r2l reranking) improves by 0.7–1.0 BLEU on the
ensemble of 4, and 4.3–4.9 on the baseline.

For Czech→English the training process was
similar to the above, except that we created the
synthetic training data (back-translated from sam-
ples of news2015 monolingual English) in batches
of 2.5M, and so were able to observe the effect
of increasing the amount of synthetic data. Af-
ter training a baseline model on all the WMT16
parallel set, we continued training with a paral-
lel corpus consisting of 2 copies of the 2.5M sen-
tences of back-translated data, 5 copies of news-
commentary v11, and a matching quantity of data
sampled from Czeng 1.6pre. After training this to
convergence, we restarted training from the base-
line model using 5M sentences of back-translated
data, 5 copies of news-commentary v11, and a
matching quantity of data sampled from Czeng
1.6pre. We repeated this with 7.5M sentences
from news2015 monolingual, and then with 10M
sentences of news2015. The back-translations
were, as for English→Czech, created with an ear-
lier NMT model trained on WMT15 data. Our ﬁ-
nal Czech→English was an ensemble of 8 systems
– the last 4 save-points of the 10M synthetic data
run, and the last 4 save-points of the 7.5M run. We
show this as ensemble8 in Table 3, and the +syn-
thetic results are on the last (i.e. 10M) synthetic
data run.

We also show in Table 4 how increasing the
amount of back-translated data affects the results.
We see that most of the gain from back-translation

system

baseline
+synthetic
+ensemble
+ensemble8
+r2l reranking

EN→CS
test
dev
20.9
18.5
23.7
20.7
24.8
22.1
–
–
25.8
22.8

CS→EN
test
dev
25.3
23.8
30.1
27.2
31.0
28.6
31.4
29.0
–
–

system

baseline
+dropout
+remove diacritics
+synthetic
+ensemble

EN→RO
test
dev
19.2
20.2
23.9
24.2
-
-
28.1
29.3
28.2
29.0

RO→EN
test
dev
22.7
23.6
27.8
28.7
29.2
30.0
33.3
34.8
33.9
35.3

English↔Czech translation results
Table 3:
(BLEU) on dev (newstest2015) and test (new-
stest2016). Submitted system in bold.

Table 5: English↔Romanian translation results
(BLEU) on dev (newsdev2016), and test (new-
stest2016). Submitted system in bold.

system

baseline
+2.5M synthetic
+5M synthetic
+7.5M synthetic
+10M synthetic

best single
test
dev
25.3
23.8
29.4
26.7
29.3
27.2
29.7
27.2
30.1
27.2

ensemble4
test
dev
26.8
25.5
30.4
27.7
30.4
28.2
30.8
28.4
31.0
28.6

Czech→English translation results
Table 4:
(BLEU) on dev (newstest2015) and test (new-
stest2016), after continued training with increas-
ing amounts of back-translated synthetic data. For
each row, training was continued from the baseline
model until convergence.

increasing the
comes with the ﬁrst batch, but
amount of back-translated data does gradually im-
prove performance.

4.3 English↔Romanian
The results of our English↔Romanian experi-
ments are shown in Table 5. This language pair
has the smallest amount of parallel training data,
and we found dropout to be very effective, yield-
ing improvements of 4–5 BLEU.7

We found that the use of diacritics was inconsis-
tent in the Romanian training (and development)
data, so for Romanian→English we removed dia-
critics from the Romanian source side, obtaining
improvements of 1.3–1.4 BLEU.

Synthetic training data gives improvements of
4.1–5.1 BLEU. for English→Romanian, we found
that the best single system outperformed the en-
semble of the last 4 checkpoints on dev, and we
thus submitted the best single system as primary
system.

7We also tested dropout for EN→DE with 8 million sen-
tence pairs of training data, but found no improvement after
10 days of training. We speculate that dropout could still
be helpful for datasets of this size with longer training times
and/or larger networks.

system

baseline
+synthetic
+ensemble

EN→RU
test
dev
20.3
21.3
24.3
25.8
26.0
27.0

RU→EN
test
dev
22.5
22.7
26.9
27.1
28.0
28.3

Table 6: English↔Russian translation results
(BLEU) on dev (newstest2015) and test (new-
stest2016). Submitted system in bold.

4.4 English↔Russian
For English↔Russian, we cannot effectively learn
BPE on the joint vocabulary because alphabets
differ. We thus follow the approach described in
(Sennrich et al., 2016b), ﬁrst mapping the Russian
text into Latin characters via ISO-9 transliteration,
then learning the BPE operations on the concate-
nation of the English and latinized Russian train-
ing data, then mapping the BPE operations back
into Cyrillic alphabet. We apply the Latin BPE
operations to the English data (training data and
input), and both the Cyrillic and Latin BPE opera-
tions to the Russian data.

Translation results are shown in Table 6. As
for the other language pairs, we observe strong
improvements from synthetic training data (4–4.4
BLEU). Ensembles yield another 1.1–1.7 BLEU.

5 Shared Task Results

Table 7 shows the ranking of our submitted sys-
tems at the WMT16 shared news translation task.
Our submissions are ranked (tied) ﬁrst for 5 out of
8 translation directions in which we participated:
EN↔CS, EN↔DE, and EN→RO. They are also
the (tied) best constrained system for EN→RU
and RO→EN, or 7 out of 8 translation directions
in total.

Our models
HimL-SysComb

also

are
(Peter et al., 2016),

used

in QT21-
ranked

direction BLEU rank
EN→CS
EN→DE
EN→RO
EN→RU
CS→EN
DE→EN
RO→EN
RU→EN

1 of 9
1 of 11
2 of 10
1 of 8
1 of 4
1 of 6
2 of 5
3 of 6

human rank
of 20
1
of 15
1
of 12
1–2
of 12
2–5
of 12
1
of 10
1
of 7
2
of 10
5

shared

Automatic

at WMT16

(BLEU)
submitted systems

and human
Table 7:
(uedin-
ranking of our
translation
nmt)
task.
Automatic rankings are taken from
http://matrix.statmt.org , only con-
Human rankings
sidering primary systems.
and for
include anonymous online systems,
EN↔CS, systems from the tuning task.

news

for EN→RO,

1–2
(Junczys-Dowmunt et al., 2016),
for EN→RU, and 1–2 for RU→EN.

and

in AMU-UEDIN
2–3
ranked

6 Conclusion

We describe Edinburgh’s neural machine transla-
tion systems for the WMT16 shared news trans-
lation task. For all translation directions, we ob-
serve large improvements in translation quality
from using synthetic parallel training data, ob-
tained by back-translating in-domain monolingual
target-side data. Pervasive dropout on all layers
was used for English↔Romanian, and gave sub-
stantial improvements. For English↔German and
English→Czech, we trained a right-to-left model
with reversed target side, and we found rerank-
ing the system output with these reversed models
helpful.

Acknowledgments

This project has received funding from the Euro-
pean Union’s Horizon 2020 research and innova-
tion programme under grant agreements 645452
(QT21), 644333 (TraMOOC) and 644402 (HimL).

References

[Bahdanau et al.2015] Dzmitry Bahdanau, Kyunghyun
Cho, and Yoshua Bengio. 2015. Neural Machine
Translation by Jointly Learning to Align and Trans-
late. In Proceedings of the International Conference
on Learning Representations (ICLR).

[Bojar et al.2016] Ondˇrej Bojar, Ondˇrej Dušek, Tom
Kocmi, Jindˇrich Libovický, Michal Novák, Martin
Popel, Roman Sudarikov, and Dušan Variš. 2016.
CzEng 1.6: Enlarged Czech-English Parallel Cor-
pus with Processing Tools Dockered.
In Text,
Speech and Dialogue: 19th International Confer-
ence, TSD 2016, Brno, Czech Republic, September
12-16, 2016, Proceedings. Springer Verlag, Septem-
ber 12-16. In press.

[Gage1994] Philip Gage. 1994. A New Algorithm for
Data Compression. C Users J., 12(2):23–38, Febru-
ary.

[Gal2015] Yarin Gal. 2015. A Theoretically Grounded
Application of Dropout in Recurrent Neural Net-
works. ArXiv e-prints.

[Junczys-Dowmunt et al.2016] Marcin

Junczys-
Dowmunt, Tomasz Dwojak, and Rico Sennrich.
The AMU-UEDIN Submission to the
2016.
WMT16 News Translation Task: Attention-based
NMT Models as Feature Functions in Phrase-based
SMT.
In Proceedings of the First Conference on
Machine Translation (WMT16), Berlin, Germany.

[Lafferty et al.2001] John D. Lafferty, Andrew McCal-
lum, and Fernando C. N. Pereira. 2001. Conditional
Random Fields: Probabilistic Models for Segment-
In Proceedings
ing and Labeling Sequence Data.
of the Eighteenth International Conference on Ma-
chine Learning, pages 282–289, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.

[Liu et al.2016] Lemao Liu, Masao Utiyama, Andrew
Finch, and Eiichiro Sumita. 2016. Agreement on
Target-bidirectional Neural Machine Translation .
In NAACL HLT 16, San Diego, CA.

[Pascanu et al.2013] Razvan Pascanu, Tomas Mikolov,
and Yoshua Bengio. 2013. On the difﬁculty of train-
ing recurrent neural networks. In Proceedings of the
30th International Conference on Machine Learn-
ing, ICML 2013, pages 1310–1318, , Atlanta, GA,
USA.

[Peter et al.2016] Jan-Thorsten Peter, Tamer Alkhouli,
Hermann Ney, Matthias Huck, Fabienne Braune,
Alexander Fraser, Aleš Tamchyna, Ondˇrej Bojar,
Barry Haddow, Rico Sennrich, Frédéric Blain, Lu-
cia Specia, Jan Niehues, Alex Waibel, Alexandre
Allauzen, Lauriane Aufrant, Franck Burlot, Elena
Knyazeva, Thomas Lavergne, François Yvon, and
Marcis Pinnis. 2016. The QT21/HimL Combined
Machine Translation System. In Proceedings of the
First Conference on Machine Translation (WMT16),
Berlin, Germany.

[Pham et al.2014] Vu Pham, Théodore Bluche, Christo-
pher Kermorvant, and Jérôme Louradour.
2014.
Dropout Improves Recurrent Neural Networks for
In 14th International
Handwriting Recognition.
Conference on Frontiers in Handwriting Recogni-
tion, ICFHR 2014, Crete, Greece, September 1-4,
2014, pages 285–290.

[Sennrich et al.2016a] Rico Sennrich, Barry Haddow,
Improving Neu-
and Alexandra Birch.
ral Machine Translation Models with Monolingual
Data. In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (ACL
2016), Berlin, Germany.

2016a.

[Sennrich et al.2016b] Rico Sennrich, Barry Haddow,
and Alexandra Birch.
2016b. Neural Machine
Translation of Rare Words with Subword Units. In
Proceedings of the 54th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2016),
Berlin, Germany.

[Zeiler2012] Matthew D. Zeiler. 2012. ADADELTA:
CoRR,

An Adaptive Learning Rate Method.
abs/1212.5701.

