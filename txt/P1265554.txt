6
1
0
2
 
y
a
M
 
6
 
 
]

G
L
.
s
c
[
 
 
2
v
1
6
6
8
0
.
3
0
6
1
:
v
i
X
r
a

Regret Analysis of the Anytime Optimally Conﬁdent
UCB Algorithm

Tor Lattimore
Department of Computing Science
University of Alberta, Canada
tor.lattimore@gmail.com

Abstract

I introduce and analyse an anytime version of the Optimally Conﬁdent UCB
(OCUCB) algorithm designed for minimising the cumulative regret in ﬁnite-
armed stochastic bandits with subgaussian noise. The new algorithm is simple,
intuitive (in hindsight) and comes with the strongest ﬁnite-time regret guarantees
for a horizon-free algorithm so far. I also show a ﬁnite-time lower bound that
nearly matches the upper bound.

1 Introduction

The purpose of this article is to analyse an anytime version of the Optimally Conﬁdent UCB al-
gorithm for ﬁnite-armed subgaussian bandits [Lattimore, 2015]. For the sake of brevity I will give
neither a detailed introduction nor an exhaustive survey of the literature. Readers looking for a gen-
tle primer on multi-armed bandits might enjoy the monograph by Bubeck and Cesa-Bianchi [2012]
be the arm chosen
from which I borrow notation. Let K be the number of arms and It ∈ {
RK is the unknown vector of means and the
in round t. The reward is Xt = µIt + ηt where µ
noise term ηt is assumed to be 1-subgaussian (therefore zero-mean). The n-step pseudo-regret of
strategy π given mean vector µ with maximum mean µ∗ = maxi µi is

1, . . . , K

∈

}

Rπ

µ(n) = nµ∗

E

−

n

t=1
X

µIt ,

where the expectation is taken with respect to uncertainty in both the rewards and actions. In all
µK. The new algorithm
analysis I make the standard notational assumption that µ1 ≥
is called OCUCB-n and depends on two parameters η > 1 and ρ
(1/2, 1]. The algorithm chooses
It = t in rounds t

µ2 ≥
∈
K and subsequently It = arg maxi γi(t) with

. . .

≥

≤

γi(t) = ˆµi(t

1) +

−

2η log(Bi(t
Ti(t

−
1)

1))

,

s

−

where Ti(t
−
empirical estimate and

1) is the number of times arm i has been chosen after round t

1 and ˆµi(t

1) is its

−

Bi(t

−

1) = max 


e, log(t), t log(t)

min

Ti(t

1), Tj(t

1)ρTi(t

1)1−ρ

−

−

K





j=1
X

−

(cid:8)


Besides the algorithm, the contribution of this article is a proof that OCUCB-n satisﬁes a nearly
optimal regret bound.



(1)

−

−1

.








(cid:9)

1

Theorem 1. If ρ

[1/2, 1] and η > 1, then

∈
ROCUCB-n

µ

(n)

Cη

≤

∆i +

log max

1
∆i

n∆2

i log(n)
ki,ρ

, log(n)

,

(cid:27)(cid:19)

Xi:∆i>0 (cid:18)

where ∆i = µ∗
pends only on η. Furthermore, for all ρ

µi and ki,ρ =

−

K
j=1 min

1, ∆2ρ

{

P

∈

[0, 1] it holds that lim supn→∞ ROCUCB-n

µ

(n)/ log(n)

≤

and Cη > 0 is a constant that de-

(cid:26)
i /∆2ρ
j }

i:∆i>0

2η
∆i

.

P
Asymptotically the upper bound matches lower bound given by Lai and Robbins [1985] except for a
factor of η. In the non-asymptotic regime the additional terms inside the logarithm signiﬁcantly im-
proves on UCB. The bound in Theorem 1 corresponds to a worst-case regret that is suboptimal by a
factor of just √log log n. Algorithms achieving the minimax rate are MOSS [Audibert and Bubeck,
2009] and OCUCB, but both require advance knowledge of the horizon. The quantity ki,ρ ∈
[1, K]
may be interpreted as the number of “effective” arms with larger values leading to improved regret.
A simple observation is that ki,ρ is always non-increasing in ρ, which makes ρ = 1/2 the canonical
choice. In the special case that all suboptimal arms have the same expected payoff, then ki,ρ = K
for all ρ. Interestingly I could not ﬁnd a regime for which the algorithm is empirically sensitive to
[1/2, 1]. If ρ = 1, then except for log log additive terms the problem dependent regret enjoyed
ρ
by OCUCB-n is equivalent to OCUCB. Finally, if ρ = 0, then the asymptotic result above applies,
but the algorithm in that case essentially reduces to MOSS, which is known to suffer suboptimal
ﬁnite-time regret in certain regimes [Lattimore, 2015].

∈

∆−2
i
≤
j = µj for j

RK and suboptimal arm i.
Intuition for regret bound. Let us ﬁx a strategy π and mean vector µ
∈
Suppose that E[Ti(n)]
(0, 1). Now consider the alternative mean
reward µ′ with µ′
i = µi + 2∆i, which means that i is the optimal action
for mean vector µ′. Standard information-theoretic analysis shows that µ and µ′ are not statistically
separable at conﬁdence level δ and in particular, if ∆i is large enough, then Rπ
µ′ (n) = Ω(nδ∆i).
For mean µ′ we have ∆′
and for any reasonable algorithm we would
like

log(1/δ)/2 for some δ
= i and µ′

∆i, ∆j}

µ′
j ≈

j = µ′

max

i −

∈

{

log(n)
∆′

j ≥

Xj:∆′
j >0

Rπ

µ′ (n) = Ω(nδ∆i) .

But this implies that δ should be chosen such that

δ = O

log(n)
n





Xj:∆′
j >0



1
∆′
j∆i 

= O

log(n)ki,1/2
n∆2
i

(cid:18)

,

(cid:19)

which up to log log terms justiﬁes the near-optimality of the regret guarantee given in Theorem 1 for
ρ close to 1/2. Of course ∆ is not known in advance, so no algorithm can choose this conﬁdence
∆i should be played about as often as arm i and
level. The trick is to notice that arms j with ∆j ≤
arms j with ∆j > ∆i should be played about as much as arm i until Tj(t
. This means
that as Ti(t

1) approaches the critical number of samples ∆−2

∆−2
j
−
i we can approximate

1)

≈

min

Ti(t

1), Tj(t

−

1

1)

2 Ti(t

1
2

1)

−

−

min

∆−2
i

, ∆−1

j ∆−1

i

=

K

≈

o

j=1
X

(cid:8)

ki,1/2
∆2
i

.

(cid:9)

Then the index used by OCUCB-n is justiﬁed by ignoring log log terms and the usual n
t used
by UCB and other algorithms. Theorem 1 is proven by making the above approximation rigorous.
The argument for this choice of conﬁdence level is made concrete in Appendix A where I present a
lower bound that matches the upper bound except for log log(n) additive terms.

≈

−

K

j=1
X

n

2 Concentration

The regret guarantees rely on a number of concentration inequalities. For this section only let
n
X1, X2, . . . be i.i.d. 1-subgaussian and Sn =
t=1 Xt and ˆµn = Sn/n. The ﬁrst lemma be-
low is well known and follows trivially from the maximal inequality and the fact that the rewards
P
are 1-subguassian.

2

Important remark. For brevity I use Oη(1) to indicate a constant that depends on η but not other
variables such as n and µ. The dependence is never worse than polynomial in 1/(η

1).

−

Lemma 2. If ε > 0, then P

t
{∃

≤

n : St ≥

ε

} ≤

exp

ε2
2n

.
(cid:19)

−

(cid:18)

The following lemma analyses the likelihood that Sn ever exceeds f (n) = √2ηn log log n where
η > 1. By the law of the iterated logarithm lim supn→∞ Sn/f (n) =
1/η a.s. and for small δ it
has been shown by Garivier [2013] that

P

n : Sn ≥ s

(∃

2n log

log(n)
δ

(cid:18)

(cid:19))

p

= O(δ) .

The case where δ = Ω(1) seems not to have been analysed and relies on the usual peeling trick, but
without the union bound.

Lemma 3. There exists a monotone non-decreasing function p : (1,
η > 1 it holds that P
p(η).

2ηn log max

e, log n

)
∞

→

(0, 1] such that for all

Lemma 4. Let b > 1 and ∆ > 0 and τ = min

n : supt≥n ˆµt +

2η log(b)
t

< ∆

, then

n : Sn ≤
∀

n

p

}

≥

o

{

(cid:26)

E[τ ]

E[τ 2] = Oη(1)

1 +

1
∆2 log+(b)
(cid:19)

·

(cid:18)

≤

p

q

(cid:27)

where log+(x) = max

1, log(x)
}

{

.

The ﬁnal concentration lemma is quite powerful and forms the lynch-pin of the following analysis.

Lemma 5. Let ∆ > 0 and ρ
Furthermore, let α be the random variable given by

[0, 1] and d

∈ {

∈

1, 2, . . .
}

and λ1, . . . , λd ∈

∞

[1,

] be constants.

α = inf

α

0 : inf
s

ˆµs +

≥




log max

1,

(

d
i=1 min

Finally let β = inf


β
{

0 : β log(β) = α
}

≥

. Then

P

2η
s

v
u
u
t

α
s, λρ

{

i s1−ρ

) ≥ −

}

∆

.






(a) If ρ

(1/2, 1], then ∆E[α] = O

1
1)(η

(2ρ

(cid:18)

−

1)2

·

(cid:19)

−

d

min

∆−1,

λi

i=1
X

n

p

o

(b) If ρ

[1/2, 1], then ∆E[β] = O

min

∆−1,

λi

n
The proofs of Lemmas 3 to 5 may be found in Appendices B to D.

p

o

1

(η

(cid:18)

−

1)2

·

(cid:19)

d

i=1
X

∈

∈

3 Analysis of the KL-UCB+ Algorithm

Let us warm up by analysing a simpler algorithm, which chooses the arm that maximises the fol-
lowing index.

γi(t) = ˆµi(t

1) +

−

2η

log

t

s

Ti(t

1)

−

Ti(t

1)

(cid:19)

−

(cid:18)

.

(2)

Strategies similar
to this have been called KL-UCB+ and suggested as a heuristic by
Garivier and Capp´e [2011] (this version is speciﬁed to the subgaussian noise model). Recently
Kaufmann [2016] has established the asymptotic optimality of strategies with approximately this
form, but ﬁnite-time analysis has not been available until now. Bounding the regret will follow the
standard path of bounding E[Ti(n)] for each suboptimal arm i. Let ˆµi,s be the empirical estimate of

3

the mean of the ith arm having observed s samples. Deﬁne τi and τ∆ by

τi = min

t

≥

(

1/∆2

i : sup
s≥t

ˆµi,s +

2η
s

r

log (n∆2

i ) < µi +

∆i
2 )

τ∆ = min

t :

inf
1≤s≤n

ˆµ1,s +

log max

1,

(
τ∆i, then by the deﬁnition of τ∆i we have γ1(t)

(cid:27)

(cid:26)

s

t
s

µ1 −

≥

2η
s

.

∆i
2 )
µi + ∆i/2 and by the

≥

If Ti(t
≥
−
deﬁnition of τi

1)

τi and t

≥

γi(t) = ˆµi(t

−
which means that It 6
Ti(n) =

n

t=1
X

1) +

s

2η log(t/Ti(t
1)
Ti(t

−

1))

−

ˆµi(t

1) +

≤

−

s

2η log(n∆2
i )
1)

Ti(t

−

< µi +

∆i
2

,

= i. Therefore Ti(n) may be bounded in terms of τi and τ∆i as follows:

1
It = i
{

} ≤

τ∆i +

1
It = i and Ti(t
{

1) < τi} ≤

−

τi + τ∆i .

n

t=τ∆i +1
X

It remains to bound the expectations of τi and τ∆i. By Lemma 5a with d = 1 and ρ = 1 and λ1 =
it follows that E[τ∆i ] = Oη(1)

and by Lemma 4

∆−2
i

∞

·

E[τi] = Oη(1)

1 +

log(n∆2
i )

.

1
∆2
i

·

(cid:18)

Therefore the strategy in Eq. (2) satisﬁes:

RKL-UCB+

µ

(n) =

∆iE[Ti(n)] = Oη(1)

∆i +

log(n∆2
i )

.

Xi:∆i>0

Remark 6. Without changing the algorithm and by optimising the constants in the proof it is pos-
sible to show that lim supn→∞ RKL-UCB+
i:∆i>0 2η/∆i, which is just a factor of η
µ
away from the asymptotic lower bound of Lai and Robbins [1985].

(n)/ log(n)

≤

·

Xi:∆i>0 (cid:18)

P

(cid:19)

1
∆i

(cid:19)

4 Proof of Theorem 1

The proof follows along similar lines as the warm-up, but each step becomes more challenging,
especially controlling τ∆.

Step 1: Setup and preliminary lemmas

Deﬁne Φ to be the random set of arms for which the empirical estimate never drops below the critical
boundary given by the law of iterated logarithm.

2η1 log max
s

{

e, log s

}

≥

Φ =

i > 2 : ˆµi,s +

µi for all s

,

(3)

(

r
where η1 = (1 + η)/2. By Lemma 3, P
p(η1) > 0. It will be important that Φ only
i
} ≥
{
= j. From the deﬁnition of
includes arms i > 2 and that the events i, j
the index γ and for i
µi for all t. The following lemma shows that the
pull counts for optimistic arms “chase” those of other arms up the point that they become clearly
suboptimal.
Lemma 7. There exists a constant cη ∈
1)
ˆµi(t
−

(0, 1) depending only on η such that if (a) j
1), ∆−2
j }

∈
∈
Φ it holds that γi(t)

Φ
Φ are independent for i

µi + ∆i/2 and (c) Tj(t

cη min
{

Φ and (b)

Ti(t

= i.

1)

−

≥

≤

≤

−

∈

∈

)

Proof. First note that Tj(t
indices:

−

1)

≤

−

Ti(t

1) implies that Bj(t

1). Comparing the

, then It 6
Bi(t

−

1)

−

≥

γi(t) = ˆµi(t

1) +

−

2η log Bi(t
Ti(t

−
1)

1)

≤

s

µi +

s

2ηcη log Bj(t
1)
Tj(t

−

1)

+

∆i
2

.

−

−

4

On the other hand, by choosing cη small enough and by the deﬁnition of j

Φ:

γj(t) = ˆµj(t

1) +

−

s

2η log Bj(t
Tj(t

−
1)

1)

≥

µj +

2ηcη log Bj(t
1)
Tj(t

−

s

−

cη

Tj(t

1)

−

r

∈

1)

+

(4)

(cid:27)

(5)

we deﬁne J = K + 1
Φ with constant probability, which means that J is sub-

∅

−
1)

µ1 +

2ηcη log Bj(t
1)
Tj(t

−

> γi(t) ,

−

≥
s
which implies that It 6
Let J = min Φ be the optimistic arm with the largest return where if Φ =
and ∆J = maxi ∆i. By Lemma 3, i
exponentially distributed with rate dependent on η only. Deﬁne Ki,ρ by

= i.

∈

Ki,ρ = 1 + cη

min

1,

(

∆2ρ
i
∆2ρ
j )

,

where cη is as chosen in Lemma 7. Since P
∈
probability (this will be made formal later). Let

Xj∈Φ,j6=i
Φ
i
{

}

= Ω(1) we will have Ki,ρ = Ω(ki,ρ) with high

bi = max

, log(n), e

and

Bi = max

, log(n), e

n∆2

i log(n)
ki,ρ

(cid:26)

τi = min

s

≥

(

1
∆2
i

: sup
s′≥s

ˆµi,s′ +

2η
s′ log(Bi)

≤

µi +

(cid:27)

r

n∆2

i log(n)
Ki,ρ

(cid:26)

.

∆i
2 )

The following lemma essentially follows from Lemma 4 and the fact that J is sub-exponentially
distributed. Care must be taken because J and τi are not independent. The proof is found in Ap-
pendix E.

Lemma 8. E[τi]

E[Jτi] = Oη(1)

1 +

log(bi)

≤

1
∆2
i

·

(cid:18)

.
(cid:19)

The last lemma in this section shows that if Ti(t
of the ith arm is not too large.
Lemma 9. If Ti(t

1)

τi, then It 6

≥

−

1)

−

≥

= i or γi(t) < µi + ∆i/2.

τi, then either i is not chosen or the index

Proof. By the deﬁnition of τi we have τi ≥
if j
1)
−
∈
for all j
cη min

Φ and Tj(t
, ∆−2
∆−2
j
i

∆−2
i
Φ. Then
(cid:8)

cη min

≤

, ∆−2
j

(cid:9)

∈

∆−2
i

and ˆµi(t
, then It 6

1)

−

≤

µi + ∆i/2. By Lemma 7,

= i. Now suppose that Tj(t

1)

e, log(t), t log(t)

min

Ti(t

1), Tj(t

−

1)ρTi(t

1)1−ρ

−

−

(cid:8)

−

Bi(t

(cid:9)

1) = max 


max

e, log(n),

≤


(cid:26)

K

n∆2



j=1
X

i log(n)
Ki,ρ

(cid:27)

(cid:8)

= Bi .

Therefore from the deﬁnition of τi we have that γi(t) < µi + ∆i/2.

−

≥

−1





(cid:9)






Step 2: Regret decomposition

By Lemma 9, if Ti(n)
a j for which γj(t)
γJ (t)
µJ ≥
deﬁne a random time for each ∆ > 0.

= i or γi(t) < µi + ∆i/2. Now we must show there exists
2∆J since by deﬁnition
µi + ∆i/2 for all t. For the remaining arms we follow the idea used in Section 3 and

τi, then It 6
≥
µi + ∆i/2. This is true for arms i with ∆i ≥

≥

≥

τ∆ = min

t : inf
s≥t

sup
j

γj(s)

µ1 −

≥

(cid:26)

∆
2

.

(cid:27)

(6)

5

Then the regret is decomposed as follows

ROCUCB-n

(n)

µ

E

≤

∆iτi + 2∆J τ∆J /4 +


Xi:∆i>0


Xi:∆i<∆J /4

.

∆iτ∆i


(7)

The next step is to show that the ﬁrst sum is dominant in the above decomposition, which will lead
to the result via Lemma 8 to bound E[∆iτi].

Step 3: Bounding τ∆

This step is broken into two quite technical parts as summarised in the following lemma. The proofs
of both results are quite similar, but the second is more intricate and is given in Appendix G.
Lemma 10. The following hold:

(a). E

∆J τ∆J /4

Oη(1)

(cid:2)

≤

(cid:3)

·

Xi:∆i>0 s

1 +

log(bi)
∆2
i

(b). E



≤

Oη(1)

Xi:∆i<∆J /4

∆iτ∆i

Proof of Lemma 10a. Preparing to use Lemma 5, let λ
∆i ≥

Xi:∆i>0 s

2∆J and λi =

1 +

∞



·

otherwise. Now deﬁne random variable α by

∈

∞

log(bi)
∆2
i

.

(0,

]K be given by λi = τi for i with

r

v
u
u
t

v
u
u
t

2η
s

2η
s

2η
s

2η
s

α = inf

α

log max

1,

2η
s

≥




β

{

≥

ˆµ1,s +

0 : inf
s

v
u
u
t
0 : β log(β) = α
}

K
i=1 min

α
s, λρ
{

i s1−ρ

) ≥

}

µ1 −

∆J
8 

1) we have


−

(

≥

and β = min

. Then for t

P
β and abbreviating s = T1(t

γ1(t) = ˆµ1,s +

log B1(t

1)

−

= ˆµ1,s +

log

max

e, log(t),

 

(

t log(t)

K
i=1 min

s, Ti(t
{

−

1)ρs1−ρ

)!

}

ˆµ1,s +

≥

log max

1,

(

K
i=1 min

{

s, Ti(t

1)ρs1−ρ

−

)

}

P

P

α

α
s, λρ

µ1 −

∆J
8

,

) ≥

}
2∆J we have Ti(n)

ˆµ1,s +

log max

1,

≥

v
u
u
t

K
i=1 min

(

i s1−ρ
where the second last inequality follows since for arms with ∆i ≥
and for other arms λi =
Therefore τ∆J /4 ≤
E[∆J β] = E[E[∆J β

β and so E[∆J τ∆J /4]

Oη(1)

min

λ]]

P

∞

≤

E

{

d

|

≤

τi = λi
by deﬁnition. The last inequality follows from the deﬁnition of α.

≤
E[∆J β], which by Lemma 5b is bounded by

∆−1
J ,

λi

#

o

p

1
∆J > 0
{

}

·

"

1
{

∆J > 0
∆min

}

+

i=1
X

n

Xi:∆i>0

√τi


Oη(1)

≤

E

·

"

√τi

,

(8)

#

Xi:∆i>0

Oη(1)

≤

E

·



Xi:λi=∞,∆i=0


where the last line follows since E[J] = Oη(1) and

E





Xi:λi=∞,∆i=0

1
∆J > 0
{
∆min

}



≤



E

J
∆min (cid:21)

(cid:20)

≤

1

Oη(1)

·

∆min ≤

Oη(1) max

i :

E[τi]

.

n

p

o

6

The resulting is completed substituting E[√τi]
show that E[τi]

1 + log(bi)

Oη(1)

.

≤

p

≤

·

(cid:16)
Step 4: Putting it together

∆2
i

(cid:17)

E[τi] into Eq. (8) and applying Lemma 8 to

By substituting the bounds given in Lemma 10 into Eq. (7) and applying Lemma 8 we obtain

ROCUCB-n

(n)

µ

∆iE[τi] + Oη(1)

≤

≤

Xi:∆i>0
Oη(1)

∆i +

·

Xi:∆i>0 (cid:18)

Xi:∆i>0 s
log max

·

1
∆i

1 +

log(bi)
∆2
i

n∆2

i log(n)
ki,ρ

(cid:26)

, log(n), e

,

(cid:27)(cid:19)

which completes the proof of the ﬁnite-time bound.

Asymptotic analysis. Lemma 5 makes this straightforward. Let εn = min

∆min
2

, log− 1

4 (n)
}

and

{

αn = min

α : inf
s

ˆµ1,s +

(

2η
s

log

α
Ks

(cid:16)

(cid:17)

r

≥ −

εn

.

)

Then by Lemma 5a with ρ = 1 and λ1, . . . , λK =
we modify the deﬁnition of τ by

∞

we have supn

E[αn] = Oη(1)Kε−2

n . Then

τi,n = min

s : sup
s′≥s

(

ˆµi,s +

log(n log(n))

µ1 −

εn

≤

,

)

2η
s

r

which is chosen such that if Ti(t

1)

τi,n, then γi(t)

εn. Therefore

µ1 −

≤

−
∆maxE[αn] +

≥

ROCUCB-n

µ

(n)

≤

Xi:∆i>0
Classical analysis shows that lim supn→∞
which implies the asymptotic claim in Theorem 1.

E[τi,n]/ log(n)

≤

∆iE[τi,n]

Oη(1)

≤

∆maxK
ε2
n

·

+

∆iE[τi,n] .

Xi:∆i>0

2η∆−2
i

and limn→∞ ε−2

n / log(n) = 0,

lim sup
n→∞

ROCUCB-n

(n)

µ
log(n) ≤

2η
∆i

.

Xi:∆i>0

This naive calculation demonstrates a serious weakness of asymptotic results. The ∆maxKε−2
n
term in the regret will typically dominate the higher-order terms except when n is outrageously
large. A more careful argument (similar to the derivation of the ﬁnite-time bound) would lead to
the same asymptotic bound via a nicer ﬁnite-time bound, but the details are omitted for readability.
Interestingly the result is not dependent on ρ and so applies also to the MOSS-type algorithm that is
recovered by choosing ρ = 0.

5 Discussion

The UCB family has a new member. This one is tuned for subgaussian noise and roughly mimics
the OCUCB algorithm, but without needing advance knowledge of the horizon. The introduction of
ki,ρ is a minor reﬁnement on previous measures of difﬁculty, with the main advantage being that it
is very intuitive. The resulting algorithm is efﬁcient and close to optimal theoretically. Of course
there are open questions, some of which are detailed below.

Shrinking the conﬁdence level. Empirically the algorithm improves signiﬁcantly when the loga-
rithmic terms in the deﬁnition of Bi(t
1) are dropped. There are several arguments that theoret-
ically justify this decision. First of all if ρ > 1/2, then it is possible to replace the t log(t) term in
1) with just t and use part (a) of Lemma 5 instead of part (b). The price
the deﬁnition of Bi(t

−

−

7

is that the regret guarantee explodes as ρ tends to 1/2 (also not observed in practice). The second
improvement is to replace log(t) in the deﬁnition of Bi(t

1) with

−

K

log+
t

· 



j=1
X




(cid:8)

min

Ti(t

1), Tj(t

1)ρTi(t

1)1−ρ

−

−

−

−1





(cid:9)

,






which boosts empirical performance and rough sketches suggest minimax optimality is achieved. I
leave details for a longer article.

Improving analysis and constants. Despite its simplicity relative to OCUCB, the current analysis
is still signiﬁcantly more involved than for other variants of UCB. A cleaner proof would obviously
be desirable. In an ideal world we could choose η = 1 or (slightly worse) allow it to converge to 1
as t grows, which is the technique used in the KL-UCB algorithm [Capp´e et al., 2013, and others].
I anticipate this would lead to an asymptotically optimal algorithm.

Informational conﬁdence bounds. Speaking of KL-UCB, if the noise model is known more pre-
cisely (for example, it is bounded), then it is beneﬁcial to use conﬁdence bounds based on the KL
divergence. Such bounds are available and could be substituted directly to improve performance
without loss [Garivier, 2013, and others]. Repeating the above analysis, but exploiting the beneﬁts
of tighter conﬁdence intervals would be an interesting (non-trivial) problem due to the need to ex-
ploit the non-symmetric KL divergences. It is worth remarking that conﬁdence bounds based on
the KL divergence are also not tight. For example, for Gaussian random variables they lead to the
right exponential rate, but with the wrong leading factor, which in practice can improve performance
as evidenced by the conﬁdence bounds used by (near) Bayesian algorithms that exactly exploit the
noise model (eg., Kaufmann et al. [2012], Lattimore [2016], Kaufmann [2016]). This is related to
the “missing factor” in Hoeffding’s bound studied by Talagrand [1995].

Precise lower bounds. Perhaps the most important remaining problem for the subgaussian noise
model is the question of lower bounds. Besides the asymptotic results by Lai and Robbins [1985]
and Burnetas and Katehakis [1997] there has been some recent progress on ﬁnite-time lower bounds,
both in the OCUCB paper and a recent article by Garivier et al. [2016]. Some further progress is
made in Appendix A, but still there are regimes where the bounds are not very precise.

References

Jean-Yves Audibert and S´ebastien Bubeck. Minimax policies for adversarial and stochastic bandits.

In Proceedings of Conference on Learning Theory (COLT), pages 217–226, 2009.

S´ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-
armed Bandit Problems. Foundations and Trends in Machine Learning. Now Publishers Incorpo-
rated, 2012. ISBN 9781601986269.

Apostolos N Burnetas and Michael N Katehakis. Optimal adaptive policies for markov decision

processes. Mathematics of Operations Research, 22(1):222–255, 1997.

Olivier Capp´e, Aur´elien Garivier, Odalric-Ambrym Maillard, R´emi Munos, and Gilles Stoltz.
Kullback–Leibler upper conﬁdence bounds for optimal sequential allocation. The Annals of
Statistics, 41(3):1516–1541, 2013.

Aur´elien Garivier. Informational conﬁdence bounds for self-normalized averages and applications.

arXiv preprint arXiv:1309.3376, 2013.

Aur´elien Garivier and Olivier Capp´e. The KL-UCB algorithm for bounded stochastic bandits and

beyond. In Proceedings of Conference on Learning Theory (COLT), 2011.

Aur´elien Garivier, Pierre M´enard, and Gilles Stoltz. Explore ﬁrst, exploit next: The true shape of

regret in bandit problems. arXiv preprint arXiv:1602.07182, 2016.

Emilie Kaufmann. On Bayesian index policies for sequential resource allocation. arXiv preprint

arXiv:1601.01190, 2016.

8

Emilie Kaufmann, Olivier Capp´e, and Aur´elien Garivier. On Bayesian upper conﬁdence bounds
for bandit problems. In International Conference on Artiﬁcial Intelligence and Statistics, pages
592–600, 2012.

Tze Leung Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances

in applied mathematics, 6(1):4–22, 1985.

Tor Lattimore. Optimally conﬁdent UCB: Improved regret for ﬁnite-armed bandits. Technical report,

2015. URL http://arxiv.org/abs/1507.07880.

Tor Lattimore. Regret analysis of the ﬁnite-horizon Gittins index strategy for multi-armed bandits.

In Proceedings of Conference on Learning Theory (COLT), 2016.

Michel Talagrand. The missing factor in Hoeffding’s inequalities. In Annales de l’IHP Probabilit´es

et statistiques, volume 31, pages 689–702, 1995.

Alexandre B Tsybakov. Introduction to nonparametric estimation. Springer Science & Business

Media, 2008.

A Lower Bounds

I now prove a kind of lower bound showing that the form of the regret in Theorem 1 is approximately
correct for ρ close to 1/2. The result contains a lower order
log log(n) term, which for large n
dominates the improvements, but is meaningful in many regimes.
Theorem 11. Assume a standard Gaussian noise model and let π be any strategy and µ
be such that

1 for all i. Then one of the following holds:

[0, 1]K

n∆2
i

−

∈

ki,1/2 log(n) ≥

1. Rπ

µ(n)

1
4

≥

1
∆i

log

n∆2
i
ki,1/2 log(n)

(cid:18)

.
(cid:19)

Xi:∆i>0

2. There exists an i with ∆i > 0 such that

Rπ

µ′ (n)

1
2

≥

Xi:∆′
i>0
j = µj for j

1
∆′
i

log

n∆′2
i
k′
i,1/2 log(n) !

 

= i and ∆′

i and k′

i,ρ are deﬁned as ∆i and ki,ρ

where µ′
but using µ′.

i = µi + 2∆i and µ′

Proof. On our way to a contradiction, assume that neither of the items hold. Let i be a suboptimal
arm and µ′ be as in the second item above. I write P′ and E′ for expectation when when rewards are
sampled from µ′. Suppose

E[Ti(n)]

1
4∆2
i

≤

log

(cid:18)

n∆2
i
ki,1/2 log(n)

.

(cid:19)

(9)

Then Lemma 2.6 in the book by Tsybakov [2008] and the same argument as used by Lattimore
[2015] gives

P

Ti(n)
{

≥

n/2

+ P′

}

Ti(n) < n/2
{

} ≥

ki,1/2 log(n)
n∆2
i

2δ .

≡

By Markov’s inequality

P

Ti(n)
{
Therefore P′

Ti(n) < n/2
{

Rπ

µ′ (n)

δn∆i
2

≥

=

} ≥
K
1
2

j=1
X

n/2

≥

} ≤

2E[Ti(n)]
n

1
2n∆2
i

≤

n∆2
i
ki,1/2 log(n)

log

(cid:18)

≤

(cid:19)

log(n)
2n∆2

i ≤

δ .

δ, which implies that

min

1
∆i

,

1
∆j (cid:27)

(cid:26)

log(n)

1
2

≥

Xj:∆′
j >0

1
∆′

j  

n∆′
j
k′
j,1/2 log(n) !

,

9

which is a contradiction. Therefore Eq. (9) does not hold for all i with ∆i > 0, but this also leads
immediately to a contradiction, since then

Rπ

µ(n) =

∆iE[Ti(n)]

Xi:∆i>0

1
4

≥

Xi:∆i>0

1
∆i

log

n∆2
i
ki,1/2 log(n)

(cid:18)

.

(cid:19)

B Proof of Lemma 3

Monotonicity is obvious. Let ε > 0 be such that η = 1 + 2ε and and Gk = [(1 + ε)k, (1 + ε)k+1]
and Fk =
e, log n

2ηn log max

Gk : Sn >

. Then

n

∃

∈

n
n : Sn ≤
∀

P

n

p

2ηn log max

k

{∀

≥

0 :

Fk}
¬

=

P

Fk|¬

{¬

F1, . . . ,

Fk−1}
¬

.

{

= P

p

e, log n
{

}
o

}

o

∞

Yk=0

Now we analyse the failure event Fk.

P

Fk|¬
{

F1, . . . ,

Fk−1} ≤
¬

P

Fk}
{
n

∃

= P

n
exp

Gk : Sn >

2ηn log max

∈
2η(1 + ε)k log+log(1 + ε)k
p
2(1 + ε)k+1
1+ ε

1+ε

−

(cid:18)

(cid:19)

e, log n
{

}
o

1
k log(1 + ε)

(cid:19)

(cid:18)

.

≤

=

p

Since this is vacuous when k is small we need also need a naive bound.

n

∃

Gk : Sn ≥

∈

P

n

2ηn log max

e, log n

exp (

η) < 1 .

{

}

≤

o

−

Combining these completes the results since for sufﬁciently large k0 (depending only on η) we have
that

p(η)

exp (

ηk0)

≥

−

P

(1

−

Fk}
{

)

≥

exp (

ηk0)

−

∞

Yk=k0

∞

1

−

(cid:18)

Yk=k0  

1
k log(1 + ε)

(cid:19)

1+ ε

1+ε

> 0 .

!

C Proof of Lemma 4

Let α

1 be ﬁxed and t0 =

8η log+(b)/∆2

and tk = t02k. Then

≥

P

τ
{

αt0} ≤

≥

t
{∃

≥

αt0 : ˆµt ≥

(cid:6)

P

∞

(cid:7)
∆/2

} ≤

∞

P

Xk=0
∞

≤

exp

−

(cid:18)

α222k−2t2
8α2kt0

0∆2

≤

Xk=0
Xk=0
= O(1) and so the result follows.

(cid:19)

Therefore E

(τ /t0)2
h
D Proof of Lemma 5

i

Let η1 = (1 + η)/2 and η2 = η/η1 and

t
∃

≤

(cid:8)
exp

tk : St ≥
α2k
4

−

(cid:19)

(cid:18)

α2k−1t0∆/2

= O (exp (

α/4)) .

(cid:9)

−

Λ =

min

d

i=1
X

1
∆2 ,

λρ
i
∆2−2ρ log+

1
λi∆2

.

(cid:18)

(cid:19)(cid:27)

(cid:26)

10

Let x > 0 be ﬁxed and let Gk = [ηk

1 , ηk+1
1

]. We will use the peeling trick. First, by Lemma 2.

qk = P

inf
s∈Gk

ˆµs +




2η
s

v
u
u
t

log max

1,

(

d
i=1 min

s, λρ
{

i s1−ρ

}

) ≤ −

∆




xΛ

P

≤


s

∃


ηk+1
1

≤

d
i=1 min

1 log max

2ηηk

: Ss +v
u
u
u
t
i η(k+1)(1−ρ)
, λρ

ηk+1
1

1

1,






η2

xΛ
ηk+1
1

, λρ


i η(k+1)(1−ρ)

1

+ ∆ηk

1 ≤






o

0




P

o





P
exp

d
i=1 min

n
∆2ηk−1
1
2 !
η2

,

(cid:19)





 −

∆2ηk
1
2η

exp

o

−

(cid:18)

(a)


≤ 

P



=



P



d
i=1 min

ηk+1
1

, λρ

i η(k+1)(1−ρ)

1

n

xΛ

n

xΛ

where (a) follows by Lemma 2. By the union bound






P

inf
s

ˆµs +

log max

1,

(

2η
s

v
u
u
t

∞

d
i=1 min

ηk+1
1

P
, λρ

≤



P

n

xΛ

xΛ

d
i=1 min
i η(k+1)(1−ρ)

1

sρ, λis1−ρ
{

}

) ≤ −

∆2ηk
1
2η

exp

o

−

(cid:18)

qk

∞

Xk=0

∆

≤

η2








(cid:19)



η2

min

ηk+1
1

, λρ

i η(k+1)(1−ρ)

1

exp

∆2ηk
1
2η

−

(cid:18)

(cid:19)!

o

Xk=0

1
xΛ

≤  

= O

d

∞

Xk=0

i=1
X
η

η

1

·

n
x−η2 ,

(cid:19)
where the last line follows from Lemma 12. Therefore P

−

(cid:18)

Now the ﬁrst part follows easily since E[α]

α
{

≥

xΛ

} ≤

O

η
η−1

x−η2 .

(cid:16)
η
(η−1)2

·

(cid:17)

·

Λ. Therefore

P

∞
0

α
{

≥

xΛ

= O

}

≤

d

R

min

1
∆

(cid:26)
d

(cid:16)
i ∆2ρ−1 log+

, λρ

(cid:17)
1
λi∆2

(cid:18)

(cid:19)(cid:27)

∆E[α]

O

≤

η

(η

(cid:18)

−

1)2

·

(cid:19)

i=1
X

O

≤

(2ρ

(cid:18)

−

η
1)(η

1)2

·

(cid:19)

−

i=1
X

min

1
∆

,

(cid:26)

λi

.

(cid:27)

p

For the second part let x0 = Λ/ productlog(Λ) where productlog is the inverse of the function
x

x exp(x).

→

∞

P

E[β]

≤

0

Z

β
{

x
}

≥

dx

≤

x0 +

∞

η

(cid:18)

η

(cid:18)

η

−
η

1

(cid:19)

1

(cid:19)

−
η

·

·

x0 + O

x0 + O

x0 + O

≤

≤

≤

x0 (cid:18)
Z
Λ
log(x0)
Λ
log(x0)

(cid:19)

(cid:18)

·

(η

(cid:18)

−

1)2

(cid:19)

(cid:18)

log(x)

dx

o

∞

x
Λ

P

α

≥

n

η2

dx

Z

x0
Λ
x log(x)

(cid:19)
∞

η2

x−η2 dx

x0
Z
η2

(cid:19)

11

x1−η2
0

= O

η

(η

(cid:18)

−

1)2

·

(cid:19)

Λ
productlog(Λ)

.

If Λ < e, then the result is trivial. For Λ

e we have productlog(Λ)

1. Then

≥

≥

∆E[β]

O

≤

O

≤

(η

(cid:18)

1)2

(cid:19)

∆Λ
productlog(Λ)
d

1

−
1

−

·

·

(η

(cid:18)

1)2

(cid:19)

i=1
X

(cid:26)

min

1
∆

,

λρ
i ∆2ρ−1
productlog(Λ)

log+

1
λi∆2

.

(cid:18)

(cid:19)(cid:27)

By examining the inner minimum we see that if ∆

, then 1/∆

− 1
2
λ
i

≥

1
2

− 1
i . If ∆ < λ
2
λ
i

, then

≤

1
∆

,

λρ
i ∆2ρ−1
productlog(Λ)

log+

1
λi∆2

(cid:18)

(cid:19)(cid:27)

min

(cid:26)

1
2
λ
i

<

≤

max

1, productlog(∆−2)
{
}
i .

1
2

2λ

log+

1
λi∆2

(cid:18)

(cid:19)

Therefore E[∆t]

d
i=1 min

∆−1, √λi

as required.

η
(η−1)2

O

≤

(cid:16)

·

(cid:17)

P

(cid:8)

(cid:9)

E Proof of Lemma 8

Since J is sub-exponentially distributed with rate dependent only on η we have
By using Lemma 4 we obtain

E[J 2] = O(1).

p

E[τ 2

i ] =

E[E [τ 2
i |

Ki,ρ]]

q

q

= Oη(1)

1 +

log(Bi)

= Oη(1)

1 +

log (bi)

.

2

#

(cid:19)

1
∆2
i

·

(cid:18)

E

· v
u
u
t

"(cid:18)

The latter inequality follows by noting that Bi ≥
c > 0.

e and (1 + c log(x))2 is concave for x

e and

(cid:19)

≥

1 +

log(Bi)

1 +

log(E[Bi])

1
∆2
i

E

v
u
u
t

"(cid:18)

2

# ≤

(cid:19)

= 1 +

log

E

max

log(n),

n∆2

i log(n)
Ki,ρ

= Oη(1)

1 +

log

max

log(n),

(cid:18)

(cid:20)
1
∆2
i

·

(cid:18)

(cid:26)

(cid:18)

(cid:26)

n∆2

(cid:27)(cid:21)(cid:19)
i log(n)
ki,ρ

,

(cid:27)(cid:19)(cid:19)

where the last inequality follows from (a) Ki,ρ ≥
plies that P
Ki,ρ ≤
Holder’s inequality

cηρ(η)ki,ρ/2

= O(k−1

}

{

1 and (b) Azuma’s concentration inequality im-
i,ρ ) as shown in the following appendix. Finally by

1
∆2
i

1
∆2
i

1
∆2
i

E[Jτi]

E[J 2]E[τ 2
i ]

≤

≤

q

Oη(1)

1 +

log (bi)

.

1
∆2
i

·

(cid:18)

(cid:19)

12

F Tail Bound on Ki,ρ

Recall that Ki,ρ = 1 + cη
j∈Φ,j6=i min
Therefore by Azuma’s inequality and naive simpliﬁcation we have

and ki,ρ = 1 +

1, ∆2ρ

i /∆2ρ

j

n

o

P

K
j6=i min

1, ∆2ρ

i /∆2ρ

j

.

P

n

o

P

Ki,ρ ≤
{

cηρ(η)ki,ρ/2

P

} ≤

min

1, ∆2ρ

i /∆2ρ

j

min

1, ∆2ρ

i /∆2ρ

j




Xj∈Φ,j6=i

n

ρ(η)
2

≤

o
1, ∆2ρ

i /∆2ρ

J

Xj6=i
2

n




o




exp 

(a)

≤

− (cid:16)




(b)

≤

exp



−


(c)
= O(k−1
i,ρ ) ,

ρ(η)

j6=i min

2

P
j6=i min

n
1, ∆2ρ

i /∆2ρ

j

ρ(η)2

P

n
j6=i min
2

n

P

1, ∆2ρ

o
i /∆2ρ
J

2 
o(cid:17)




o





where (a) follows from Azuma’s inequality and (b) since min
{
exp(

1/x for all x

x)

0.

2

1, x
}

min
{

1, x
}

≤

and (c) by

−

≤

≥

G Proof of Lemma 10b

Recall that we are trying to show that

E





Xi:∆i<∆J /4

= O

∆iτ∆i 


 

Xi:∆i>0

∆iE[Jτi]

.

!

(10)

Let E be the event that ∆2 ≤
[∆J , 2∆J ]
A2 =
i : ∆i ∈
{
∆i/2. Therefore i
µ1 −
µJ ≥
given by λi = τi for i

∈
∈
A1 and λi =

. For i
}

∆J /4 and deﬁne random sets A1 =

{
A1 we have ∆i > 2∆J and since J

A1 implies that τ∆i = 1 and so Ti(n)

(2∆J ,

i : ∆i ∈
∈
τi. Let λ

Φ we have γJ (t)
(0,

and

)
}
∞
≥
]K be

≤

∈

∞

∈

otherwise.

∞

α = min

α : inf
s

ˆµ2,s +




log max

1,

(

K
i=1 min

α
s, λρ
{

i s1−ρ

) ≥

}

µ2 −

∆J
4 


.

2η
s

v
u
u
t



It is important to note that we have used ˆµ2,s in the deﬁnition of α and not ˆµ1,s that appeared in
the proof of part (a) of this lemma. The reason is to preserve independence when samples from the
β we have
ﬁrst arm are used later. Let β = min
γ2(t)

≥
∆J /2, which implies that

0 : β log(β) = α
}

. If E holds, then for t

∆J /4

β
{



≥

P

µ2 −

≥

≥

µ1 −
1
E
{

}

Ti(n)

t∆J +

≤

β +

τi ≤

τi .

Xi∈A2

Xi∈A2

Xi∈A2

Therefore for any s, t
ity implies that

≤

n the concavity of min

s,

and x

xρ combined with Jensen’s inequal-

{

·}

→

E

1
{

}

min

s, Ti(t

1)ρs1−ρ

−

Xi∈A2

(cid:8)

min

s,

≤

(cid:9)

Xi∈A2

We are getting close to an application of Lemma 5. Let ω

(0,

ρ

β +

i∈A2 τi
A2|
P
|
]K be given by

(cid:19)

(cid:18)

s1−ρ

.

)

(

∈

ωj =

+

A2|

|

j∈A2 τj/

A2|

|

τj
β/

∞






P

13

∞
if j
∈
if j
∈
otherwise ,

A1
A2

(11)

(12)

(13)

which has been chosen such that for T1(t

1) = s and if E holds, then

B1(t

1)

max

1,

−

≥

−

(

(

P

max

1,

≥

t log(t)

K
j=1 min

s, Tj(t

1)ρs1−ρ

)

}

{
t log(t)

−

K
j=1 min

s, ωρ

j s1−ρ

)

.

(cid:9)
Now let i be the index of some arm for which ∆i < ∆J /4 and deﬁne

P

(cid:8)

αi = min

α : inf
s

ˆµ1,s +

log max

1,




(

K
j=1 min

j s1−ρ

) ≥

α
s, ωρ

µ1 −

∆i
2 


2η
s

v
u
u
t

and βi = min
β

{
∆i/2 and so t∆i ≤
µ1 −
expectation can be controlled.

≥

0 : β log(β) = αi}

(cid:8)
≥
βi. At last we are able to write t∆i in terms of something for which the

. Therefore by Eq. (11), if E holds and t

βi, then γ1(t)

P



≥

(cid:9)

E





Xi:∆i<∆J /4

E

≤



∆iτ∆i


∆iβi


K

Xi:∆i<∆J /4

j=1
X

Xi:∆i<∆J /4


Oη(1)

E

≤

≤

Oη(1)

E

·

·









Xi:∆i<∆J /4


J√τj +

Xj∈A1
J 2
∆min

E

min

, √ωj

1
∆i

(cid:26)



√τj +

A2|

|



(cid:27)


√ωj +

J

∆min 







√ωj

The ﬁrst two terms are easily bounded as we shall soon see. For the last we have


Xj∈A1


Oη(1)

A2|

+ J

≤

·

|

.

E

(cid:2)

J

A2|

|

√ωj

≤

Oη(1)

E [

A2|

|

·

q

2ωj] = Oη(1)

(cid:3)

E

A2|


|

· v
u
u
u
t



Xj∈A2

τj +

β

A2|

|





Oη(1)

≤

E

A2|



|

· 



v
u
u
u
t

Xj∈A2
Bounding each term separately. For the ﬁrst, let ˜Aℓ =
that no matter the value of ∆J there exists an ℓ



E [

A2|

|

β]



[2ℓ, 2ℓ+2)

+

τj
p

j : ∆j ∈
Z with A2 ⊆
(cid:8)

Aℓ.

(cid:9)

, which is chosen such

E

A2|

|


v
u
u
u
t



Xj∈A2

τj 


˜Aℓ|

sXℓ∈Z |

Xj∈ ˜Aℓ

E[τj]

˜Aℓ|

2 max
j∈Aℓ

E[τj ]

sXℓ∈Z |

where the last inequality follows because
j
which gives the same order-bound on E[τj] for all j

{

ℓ∈Z 1

1 +

log(bj)
∆2
j

,

Xj:∆j >0 s
˜Aℓ}
= 2 for each j and from Lemma 8,
∈
˜Aℓ for ﬁxed ℓ. For the second term in
∈

(14)

∈

O(1)

O(1)

·

·

Oη(1)

·

≤

≤

≤

P

14

Eq. (13) we have

E

β

A2|

i

|
hp

(a)

≤

Oη(1)

E

·

1
∆2
J

+

1
∆J

Xj:λj =∞

Xj:λj <∞

√τj







v
|
u
u
u
t






A2| 


E

A2|


|

(b)

≤

(c)

≤

· 



v
u
u
u
t

Oη(1)

Oη(1)

Xj:∆j >0 s



1 +

Xj:λj =∞
log(bj)
∆2
j

,

+ E

A2|
|
∆J (cid:21)

(cid:20)

+

1
∆2
J 


Xj:∆j >0

E[√τj ]



where (a) follows from Lemma 5 and (b) since for all x, y
and √xy
in j
we have

√x + √y
≥
x + y. To get (c) we bound the ﬁrst term as in Eq. (14), the second by the fact that arms
2∆J and the third using Lemma 8. Finally by substituting this into Eq. (12)

A2 have ∆j ≤

0 it holds that √x + y

≤

≤

∈

E

"

Xi∈A3

∆iτ∆i

Oη(1)

# ≤

E

· 







Xj∈A1

1 +

Xj:∆j >0  

Oη(1)

≤

log(bj)
∆2

j !

,

J√τj +

J 2
∆min 

+

Xj:∆j >0  



1 +

log(bj)
∆2

j !


where the last line follows since E[J 2/∆min] = Oη(1)∆−1

min and by Lemma 8

E[J√τj]

E[J 2]E[τj ] = Oη(1)

≤

q

1 +

· s

log(bj)
∆2
j

,

which completes the proof.

H Technical Lemmas

Lemma 12. Let η > 1 and ρ

[0, 1] and λ

(0,

] and x > 0, then

∈

∈

∞

∞

Xk=0

n

min

ηk+1, λρη(1−ρ)(k+1)

exp

xηk

o

−
(cid:0)

(cid:1)

≤ 



= O

e + η

2

1
x
λρxρ−1
(cid:16)
log(η)

η

(cid:0)
1

−

(cid:19)

η

(cid:18)

log(η)
1 + 1
(cid:17)
e + log
1
x

min

·

(cid:26)

1

if xλ

≥
otherwise .

1
λx

(cid:0)
(cid:1)(cid:1)
, λρxρ−1 log+

1
λx

(cid:18)

(cid:19)(cid:27)

.

xηk), which is unimodal and so

Proof. Let f (k) = min
2 supk f (k) +

∞
k=0 f (k)

≤

P

ηk+1, λρη(k+1)(1−ρ)
∞
0 f (k)dk. If xλ
(cid:8)
R
f (k)dk

ηk exp

≥

∞

η

∞

exp(
1, then
(cid:9)

−

≤

ηkx

dk =

η
x log(η)

.

0
Z
If xλ < 1, then let kλ be such that ηkλ = λρηkλ(1−ρ) and kx be such that ηk = 1/x.

0
Z

(cid:1)

−
(cid:0)

∞

0
Z

f (k)dk

η

ηkdk + η

λρηkx(1−ρ)dk + η

λρηk(1−ρ) exp

xηk

dk

kλ

kx

Z
+ η (kx −

kλ

Z
kλ) λρxρ−1 + ηλρxρ−1

kx

∞

−
(cid:0)

η(k−kx)(1−ρ) exp

∞

kx

Z

(cid:1)
ηkx−k

dk

−
(cid:0)

(cid:1)

≤

=

≤

0
Z
λ
1
−
log(η)

λ
1
−
log(η)

+

ηλρxρ−1 log
log(η)

1
λx

(cid:0)

(cid:1)

+

ηλρxρ−1
e log(η)

15

Finally

Therefore

∞

Xk=0

n

I Table of Notation

sup
k

f (k)

min

≤

1
ex

(cid:26)

, ηλρxρ−1

.

(cid:27)

min

ηk+1, λρη(1−ρ)(k+1)

exp

xηk

o

−
(cid:0)

≤ 


(cid:1)



(cid:0)

e + η

2

1
x
λρxρ−1
(cid:16)
log(η)

log(η)
1 + 1
(cid:17)
e + log

if xλ

1

≥

otherwise .

1
λx

(cid:0)

(cid:1)(cid:1)

K

n

t

η

ρ

η1, η2

µi

ˆµi,s

ˆµi(t)

∆i

∆min

ki,ρ

Ki,ρ

τi

τ∆

p(η)

Φ

J

∆max
log+(x)
Bi and bi

constant parameter greater than 1 determining width of conﬁdence in-
terval

empirical estimate of return of arm i based on s samples

empirical estimate of return of arm i after time step t

gap between the expected returns of the best arm and the ith arm

minimal non-zero gap ∆min = min
maximum gap ∆max = maxi ∆i

∆i : ∆i > 0
{

}

number of arms

horizon

current time step

constant parameter in (1/2, 1]

η1 = (1 + η)/2 and η2 = η/η1
expected return of arm i

1, ∆2ρ

i /∆2ρ
j }

{

1, log(x)
}

max

{
see Eq. (5)

K
j=1 min
see Eq. (4)
P
see Eq. (5)

see Eq. (6)

see Lemma 3

set of optimistic arms Eq. (3)

J = min Φ

16

