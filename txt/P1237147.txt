Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

8
1
0
2
 
l
u
J
 
6
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
5
5
4
6
0
.
2
0
8
1
:
v
i
X
r
a

Mattias Teye 1 2 * Hossein Azizpour 1 * Kevin Smith 1 3

Abstract
We show that training a deep network using batch
normalization is equivalent to approximate infer-
ence in Bayesian models. We further demon-
strate that this ﬁnding allows us to make mean-
ingful estimates of the model uncertainty us-
ing conventional architectures, without modiﬁ-
cations to the network or the training proce-
dure. Our approach is thoroughly validated by
measuring the quality of uncertainty in a series
of empirical experiments on different tasks.
It
outperforms baselines with strong statistical sig-
niﬁcance, and displays competitive performance
with recent Bayesian approaches.

1. Introduction

Deep learning has dramatically advanced the state of the
art in a number of domains. Despite their unprecedented
discriminative power, deep networks are prone to make
mistakes. Nevertheless, they can already be found in set-
tings where errors carry serious repercussions such as au-
tonomous vehicles (Chen et al., 2016) and high frequency
trading. We can soon expect automated systems to screen
for various types of cancer (Esteva et al., 2017; Shen, 2017)
and diagnose biopsies (Djuric et al., 2017). As autonomous
systems based on deep learning are increasingly deployed
in settings with the potential to cause physical or economic
harm, we need to develop a better understanding of when
we can be conﬁdent in the estimates produced by deep net-
works, and when we should be less certain.

Standard deep learning techniques used for supervised
learning lack methods to account for uncertainty in the
model. This can be problematic when the network en-
counters conditions it was not exposed to during training,

* Co-ﬁrst authorship 1School of Electrical Engineering and
Computer Science, KTH Royal Institute of Technology, Stock-
holm, Sweden 2Current address: Electronic Arts, SEED, Stock-
holm, Sweden. This work was carried out at Budbee AB.
3Science for Life Laboratory. Correspondence to: Kevin Smith
<ksmith@kth.se>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

or if the network is confronted with adversarial examples
(Goodfellow et al., 2014). When exposed to data outside
the distribution it was trained on, the network is forced to
extrapolate, which can lead to unpredictable behavior.

If the network can provide information about its uncer-
tainty in addition to its point estimate, disaster may be
avoided. In this work, we focus on estimating such pre-
dictive uncertainties in deep networks (Figure 1).

The Bayesian approach provides a theoretical framework
for modeling uncertainty (Ghahramani, 2015), which has
prompted several attempts to extend neural networks (NN)
into a Bayesian setting. Most notably, Bayesian neural net-
works (BNNs) have been studied since the 1990’s (Neal,
2012), but do not scale well and struggle to compete with
modern deep learning architectures. Recently, (Gal &
Ghahramani, 2015) developed a practical solution to obtain
uncertainty estimates by casting dropout training in con-
ventional deep networks as a Bayesian approximation of a
Gaussian Process (its correspondence to a general approx-
imate Bayesian model was shown in (Gal, 2016)). They
showed that any network trained with dropout is an ap-
proximate Bayesian model, and uncertainty estimates can
be obtained by computing the variance on multiple predic-
tions with different dropout masks.

The inference in this technique, called Monte Carlo
Dropout (MCDO), has an attractive quality: it can be ap-
plied to any pre-trained networks with dropout layers. Un-
certainty estimates come (nearly) for free. However, not all
architectures use dropout, and most modern networks have
adopted other regularization techniques. Batch normaliza-
tion (BN), in particular, has become widespread thanks to
its ability to stabilize learning with improved generalization
(Ioffe & Szegedy, 2015).

An interesting aspect of BN is that the mini-batch statis-
tics used for training each iteration depend on randomly
selected batch members. We exploit this stochasticity and
show that training using batch normalization, like dropout,
can be cast as an approximate Bayesian inference. We
demonstrate how this ﬁnding allows us to make meaning-
ful estimates of the model uncertainty in a technique we
call Monte Carlo Batch Normalization (MCBN) (Figure 1).
The method we propose can be applied to any network us-
ing standard batch normalization.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Gaussian Processes show superior performance in terms of
RMSE and uncertainty quality compared to state-of-the-art
approximate BNNs (Bui et al., 2016)1. Another recent ap-
proach to Bayesian learning, Bayesian hypernetworks, use
a NN to learn a distribution of parameters over another net-
work (Krueger et al., 2017). Multiplicative Normalizing
Flows for variational Bayesian networks (MNF) (Louizos
& Welling, 2017) is a recent model that formulates a pos-
terior dependent on auxiliary variables. MNF achieves a
highly ﬂexible posterior by the application of normalizing
ﬂows to the auxiliary variables.

Although these recent techniques address some of the dif-
ﬁculties with approximate BNNs, they all require modiﬁ-
cations to the architecture or the way networks are trained,
as well as specialized knowledge from practitioners. Re-
cently, (Gal & Ghahramani, 2015) showed that a network
trained with dropout implicitly performs the VI objective.
Therefore any network trained with dropout can be treated
as an approximate Bayesian model by making multiple
predictions through the network while sampling different
dropout masks for each prediction. The mean and variance
of the predictions are used in the estimation of the mean
and variance of the predictive distribution 2.

3. Method

In the following, we introduce Bayesian models and a vari-
ational approximation using Kullback-Leibler (KL) diver-
gence following (Gal, 2016). We continue by showing that
a batch normalized deep network can be seen as an ap-
proximate Bayesian model. Employing theoretical insights
and empirical analysis, we study the induced prior on the
parameters when using batch normalization. Finally, we
describe the procedure for estimating the uncertainty of a
batch normalized network’s output.3

3.1. Bayesian Modeling

We assume a ﬁnite training set D = {(xi, yi)}i=1:N where
each (xi, yi) is a sample-label pair. Using D, we are inter-
ested in learning an inference function fω(x, y) with pa-
rameters ω. In deterministic models, the estimated label ˆy
is obtained as follows:

ˆy = arg max

fω(x, y)

y

In probabilistic models we let fω(x, y) = p(y|x, ω). In
Bayesian modeling, in contrast to ﬁnding a point estimate

1By uncertainty quality, we refer to predictive probability dis-

tributions as measured by PLL and CRPS.

2This technique is referred to as “MC Dropout” in the original

work, though we refer to it here as MCDO.

3While the method applies to FC or Conv layers, the induced

Figure 1. Training a deep network using batch normalization
is equivalent to approximate inference in Bayesian models.
Thus, uncertainty estimates can be obtained from any network
using BN through a simple procedure. At inference, several mini-
batches are constructed by taking random samples to accompany
the query. The mean and variance of the outputs are used to esti-
mate the predictive distribution (MCBN). Here, we show results
on a toy dataset from a network with three hidden layers (30 units
per layer). Training data is depicted as dots. The solid line is the
predictive mean of 500 stochastic forward passes and the shaded
areas represent the model’s uncertainty. The dashed lines depict a
minimal baseline for uncertainty (CUBN), see Section 4.1.

We validate our approach by empirical experiments on a
variety of datasets and tasks, including regression and im-
age classiﬁcation. We measure uncertainty quality relative
to a baseline of ﬁxed uncertainty, and show that MCBN
outperforms the baseline on nearly all datasets with strong
statistical signiﬁcance. We also show that the uncertainty
quality of MCBN is on par with other recent approximate
Bayesian networks.

2. Related Work

Bayesian models provide a natural framework for model-
ing uncertainty, and several approaches have been devel-
oped to adapt NNs to Bayesian reasoning. A common ap-
proach is to place a prior distribution (often a Gaussian)
over each parameter. The resulting model corresponds to
a Gaussian process for inﬁnite parameters (Neal, 1995),
and a Bayesian NN (MacKay, 1992) for a ﬁnite number of
parameters. Inference in BNNs is difﬁcult however (Gal,
2016), so focus has thus shifted to techniques that approx-
imate the posterior, approximate BNNs. Methods based on
variational inference (VI) typically rely on a fully factor-
ized approximate distribution (Kingma & Welling, 2014;
Hinton & Van Camp, 1993), but often do not scale. To alle-
viate these difﬁculties, (Graves, 2011) proposed a model
using sampling methods to estimate a factorized poste-
rior. Probabilistic backpropagation (PBP), estimates a fac-
torized posterior via expectation propagation (Hern´andez-
Lobato & Adams, 2015).

Using several strategies to address scaling issues, Deep

prior from weight decay (Section 3.3) is studied for FC layers.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

of the model parameters, the idea is to estimate an (ap-
proximate) posterior distribution of the model parameters
p(ω|D) to be used for probabilistic prediction:

(cid:90)

p(y|x, D) =

fω(x, y)p(ω|D)dω

The predicted label, ˆy, can then be accordingly obtained by
sampling p(y|x, D) or taking its maxima.

Variational Approximation In approximate Bayesian
modeling, a common approach is to learn a parame-
terized approximating distribution qθ(ω) that minimizes
KL(qθ(ω)||p(ω|D)); the Kullback-Leibler divergence of
the true posterior w.r.t. its approximation. Minimizing this
KL divergence is equivalent to the following minimization
while being free of the data term p(D) 4:

LVA(θ) := −

qθ(ω) ln fω(xi, yi)dω

N
(cid:88)

(cid:90)

i=1

+ KL(qθ(ω)||p(ω))

During optimization, we want to take the derivative of the
expected likelihood w.r.t. the learnable parameters θ. We
use the same MC estimate as in (Gal, 2016) (explained in
Appendix Section 1.1), such that one realized ˆωi is taken
for each sample i 5. Optimizing over mini-batches of size
M , the approximated objective becomes:

ˆLVA(θ) := −

ln f ˆωi (xi, yi) + KL(qθ(ω)||p(ω))

(1)

N
M

M
(cid:88)

i=1

The ﬁrst term is the data likelihood and the second term
is the divergence of the prior w.r.t. the approximated poste-
rior.

3.2. Batch Normalized Deep Nets as Bayesian Modeling

We now describe the optimization procedure of a deep net-
work with batch normalization and draw the resemblance
to the approximate Bayesian modeling in Eq (1).

The inference function of a feed-forward deep network
with L layers can be described as:

where a(.) is an element-wise nonlinearity function and
Wl is the weight vector at layer l. Furthermore, we de-
note the input to layer l as xl with x1 = x and we then set
hl = Wlxl. Parenthesized super-index for matrices (e.g.
W(j)) and vectors (e.g. x(j)) indicates jth row and element
respectively. Super-index u refers to a speciﬁc unit at layer
l, (e.g. Wu = Wl,(j), hu = hl,(j)). 6

Batch Normalization Each layer of a deep network is
constructed by several linear units whose parameters are
the rows of the weight matrix W. Batch normalization is
a unit-wise operation proposed in (Ioffe & Szegedy, 2015)
to standardize the distribution of each unit’s input. For FC
layers, it converts a unit’s input hu in the following way:

ˆhu =

hu − E[hu]
(cid:112)Var[hu]

where the expectations are computed over the training
set during evaluation, and mini-batch during training (in
deep networks, the weight matrices are often optimized us-
ing back-propagated errors calculated on mini-batches of
data)7. Therefore, during training, the estimated mean and
variance on the mini-batch B is used, which we denote by
µB and σB respectively. This makes the inference at train-
ing time for a sample x a stochastic process, varying based
on other samples in the mini-batch.

Loss Function and Optimization Training deep net-
works with mini-batch optimization involves a (regular-
ized) risk minimization with the following form:

LRR(ω) :=

l(ˆyi, yi) + Ω(ω)

1
M

M
(cid:88)

i=1

where the ﬁrst term is the empirical loss on the training
data and the second term is a regularization penalty act-
ing as a prior on model parameters ω.
If the loss l is
cross-entropy for classiﬁcation or sum-of-squares for re-
gression problems (assuming i.i.d. Gaussian noise on la-
bels), the ﬁrst term is equivalent to minimizing the negative
log-likelihood:

fω(x) = WLa(WL−1...a(W2a(W1x))

LRR(ω) := −

ln fω(xi, yi) + Ω(ω)

4Achieved by constructing the Evidence Lower Bound, called
ELBO, and assuming i.i.d. observation noise; details can be found
in Appendix Section 1.1.

5While a MC integration using a single sample is a weak ap-
proximation, in an iterative optimization for θ several samples
will be taken over time.

6For a (softmax) classiﬁcation network, fω(x) is a vector with
fω(x, y) = fω(x)(y), for regression networks with i.i.d. Gaus-
sian noise we have fω(x, y) = N (fω(x), τ −1I).

7It also learns an afﬁne transformation for each unit with pa-
afﬁne = γ(j) ˆx(j) + β(j).

rameters γ and β, omitted for brevity: ˆx(j)

1
M τ

M
(cid:88)

i=1

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

with τ = 1 for classiﬁcation.
In a network
with batch normalization, the model parameters include
{W1:L, γ1:L, β1:L, µ1:L
B }. If we decouple the learn-
able parameters θ = {W1:L, γ1:L, β1:L} from the stochas-
tic parameters ω = {µ1:L
B }, we get the following ob-
jective at each step of the mini-batch optimization:

B , σ1:L

B , σ1:L

LRR(θ) := −

ln f{θ, ˆωi}(xi, yi) + Ω(θ)

(2)

1
M τ

M
(cid:88)

i=1

where ˆωi is the means and variances for sample i’s mini-
batch at a certain training step. Note that while ˆωi formally
needs to be i.i.d. for each training example, a batch normal-
ized network samples the stochastic parameters once per
training step (mini-batch). For a large number of epochs,
however, the distribution of sampled batch members for a
given training example converges to the i.i.d. case.

B , σ1:L

In a batch normalized network, qθ(ω) corresponds to the
joint distribution of the weights, induced by the random-
ness of the normalization parameters µ1:L
B , as im-
plied by the repeated sampling from D during training.
This is an approximation of the true posterior, where we
have restricted the posterior to lie within the domain of
our parametric network and source of randomness. With
that, we can estimate the uncertainty of predictions from
a trained batch normalized network using the inherent
stochasticity of BN (Section 3.4).

3.3. Prior p(ω)

Equivalence between the VA and BN training procedures
requires ∂
∂θ of Eq. (1) and Eq. (2) to be equivalent up to a
scaling factor. This is the case if ∂
∂θ KL(qθ(ω)||p(ω)) =
N τ ∂

∂θ Ω(θ).

To reconcile this condition, one option is to let the prior
p(ω) imply the regularization term Ω(θ). Eq. (1) reveals
that the contribution of KL(qθ(ω)||p(ω)) to the optimiza-
tion objective is inversely scaled with N . For BN, this cor-
responds to a model with a small Ω(θ) when N is large. In
the limit as N → ∞, the optimization objectives of Eq. (1)
and Eq. (2) become identical with no regularization.8

Another option is to let some Ω(θ) imply p(ω).
In Ap-
pendix Section 1.4 we explore this with L2-regularization,
also called weight decay (Ω(θ) = λ (cid:80)
l=1:L ||W l||2). We
ﬁnd that unlike in MCDO (Gal, 2016), some simplifying

8To prove the existence and ﬁnd an expression of
KL(qθ(ω)||p(ω)), in Appendix Section 1.3 we ﬁnd that BN ap-
proximately induces Gaussian distributions over BN units’ means
and standard deviations, centered around the population values
given by D. We assume a factorized distribution and Gaussian
priors, and ﬁnd the corresponding KL(qθ(ω)||p(ω)) components
in Appendix Section 1.4 Eq. (7). These could be used to construct
a custom Ω(θ) for any Gaussian choice of p(ω).

assumptions are necessary to reconcile the VA and BN ob-
jectives with weight decay: no scale and shift applied to
BN layers, uncorrelated units in each layer, BN applied on
all layers, and large N and M . Given these conditions:

p(µu
p(σu

B) = N (µµ,p, σµ,p)
B) = N (µσ,p, σσ,p)

where µµ,p = 0, σµ,p → ∞, µσ,p = 0 and σσ,p → 1

.

2N τ λl

This corresponds to a wide and narrow distribution on BN
units’ means and std. devs respectively, where N accounts
for the narrowness of the prior. Due to its popularity in
deep learning, our experiments in Section 4 are performed
with weight decay.

3.4. Predictive Uncertainty in Batch Normalized Deep

Nets

In the absence of the true posterior, we rely on the approx-
imate posterior to express an approximate predictive distri-
bution:

p∗(y|x, D) :=

fω(x, y)qθ(ω)dω

(cid:90)

Following (Gal, 2016) we estimate the ﬁrst (for regression
and classiﬁcation) and second (for regression) moments of
the predictive distribution empirically (see Appendix Sec-
tion 1.5 for details):

Ep∗ [y] ≈

f ˆωi(x)

1
T

T
(cid:88)

i=1

T
(cid:88)

1
T

i=1
− Ep∗ [y](cid:124)Ep∗ [y]

Covp∗ [y] ≈ τ −1I +

f ˆωi(x)(cid:124)f ˆωi(x)

where each ˆωi corresponds to sampling the net’s stochas-
tic parameters ω = {µ1:L
B , σ1:L
B } the same way as during
training. Sampling ˆωi therefore involves sampling a batch
B from the training set and updating the parameters in the
BN units, just as if we were taking a training step with B.
From a VA perspective, training the network amounted to
minimizing KL(qθ(ω)||p(ω|D)) wrt θ. Sampling ˆωi from
the training set, and keeping the size of B consistent with
the mini-batch size used during training, ensures that qθ(ω)
during inference remains identical to the approximate pos-
terior optimized during training.

The network is trained just as a regular BN network, but
instead of replacing ω = {µ1:L
B } with population
values from D for inference, we update these parameters
stochastically, once for each forward pass.9 Pseudocode
for estimating predictive mean and variance is given in Al-
gorithm 1.

B , σ1:L

9As an alternative to using the training set D to sample ˆωi,

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Algorithm 1 MCBN Algorithm
Input: sample x, number of inferences T , batchsize b
Output: mean prediction ˆy, predictive uncertainty σ2
1: y = {}
2: loop for T iterations
3: B ∼ D // mini batch
ˆω = {µB, σB}
4:
y = y ∪ f ˆω(x)
5:
6: end loop
7: ˆy = E[y]
8: σ2 = Cov[y] + τ −1I // for regression

// mini batch mean and variance

4. Experiments and Results

We assess the uncertainty quality of MCBN quantitatively
and qualitatively. Our quantitative analysis relies on CI-
FAR10 for image classiﬁcation and eight standard regres-
sion datasets, listed in Appendix Table 1. Publicly avail-
able from the UCI Machine Learning Repository (Univer-
sity of California, 2017) and Delve (Ghahramani, 1996),
these datasets have been used to benchmark comparative
models in recent related literature (see (Hern´andez-Lobato
& Adams, 2015), (Gal & Ghahramani, 2015), (Bui et al.,
2016) and (Li & Gal, 2017)). We report results using
standard metrics, and also propose useful upper and lower
bounds to normalize these metrics for an easier interpreta-
tion in Section 4.2.

Our qualitative results include the toy dataset in Figure 1
in the style of (Karpathy, 2015), a new visualization of un-
certainty quality that plots test errors sorted by predicted
variance (Figure 2 and Appendix), and image segmentation
results (Figure 2 and Appendix).

4.1. Metrics

We evaluate uncertainty quality based on two standard met-
rics, described below: Predictive Log Likelihood (PLL)
and Continuous Ranked Probability Score (CRPS). To im-
prove the interpretability of the metrics, we propose to nor-
malize them by upper and lower bounds.

Predictive Log Likelihood (PLL) Predictive Log Like-
lihood is widely accepted as the main uncertainty quality
metric for regression (Hern´andez-Lobato & Adams, 2015;
Gal & Ghahramani, 2015; Bui et al., 2016; Li & Gal, 2017).
A key property of PLL is that it makes no assumptions
about the form of the distribution. The measure is deﬁned
for a probabilistic model fω(x) and a single observation

we could sample from the implied qθ(ω) as modeled in the Ap-
pendix. This would alleviate having to store D for use during
prediction. In our experiments we used D to sample ˆωi however,
and leave the evaluation of the modeled qθ(ω) for future research.

(yi, xi) as:

PLL(fω(x), (yi, xi)) = log p(yi|fω(xi))

where p(yi|fω(xi)) is the model’s predicted PDF evalu-
ated at yi, given the input xi. A more detailed description
is given in the Appendix Section 1.5. The metric is un-
bounded and maximized by a perfect prediction (mode at
yi) with no variance. As the predictive mode moves away
from yi, increasing the variance tends to increase PLL (by
maximizing probability mass at yi). While PLL is an ele-
gant measure, it has been criticized for allowing outliers to
have an overly negative effect on the score (Selten, 1998).

Continuous Ranked Probability Score (CRPS) Con-
tinuous Ranked Probability Score is a measure that takes
the full predicted PDF into account with less sensitivity to
outliers. A prediction with low variance that is slightly off-
set from the true observation will receive a higher score
form CRPS than PLL. In order for CRPS to be analytically
tractable, we need to assume a Gaussian unimodal predic-
tive distribution. CRPS is deﬁned as

CRPS(fω(xi), (yi, xi)) =

(cid:0)F (y) − 1(y ≥ yi)(cid:1)2

dy

(cid:90) ∞

−∞

where F (y) is the predictive CDF, and 1(y ≥ yi) = 1
if y ≥ yi and 0 otherwise (for univariate distributions)
(Gneiting & Raftery, 2007). CRPS is interpreted as the sum
of the squared area between the CDF and 0 where y < yi
and between the CDF and 1 where y ≥ yi. A perfect pre-
diction with no variance yields a CRPS of 0; for all other
cases the value is larger. CRPS has no upper bound.

4.2. Benchmark models and normalized metrics

It is difﬁcult to interpret the quality of uncertainty from
raw PLL and CRPS values. We propose to normalize the
metrics between useful lower and upper bounds. The nor-
malized measures estimate the performance of an uncer-
tainty model between the trivial solution (constant uncer-
tainty) and optimal uncertainty for each prediction. For
the lower bound, we deﬁne a baseline that predicts con-
stant variance regardless of input. The variance is set to
a ﬁxed value that optimizes CRPS on validation data. We
call this model Constant Uncertainty BN (CUBN). It re-
ﬂects our best guess of constant variance on test data –
thus, any improvement in uncertainty quality over CUBN
indicates a sensible estimate of uncertainty. We simi-
larly deﬁne a baseline for dropout, Constant Uncertainty
Dropout (CUDO). The modeling of variance (uncertainty)
by MCBN and CUBN are visualized in Figure 1.

An upper bound on uncertainty performance can also
be deﬁned for a probabilistic model f with respect to
CRPS or PLL. For each observation (yi, xi), a value

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

for the predictive variance Ti can be chosen that max-
imizes PLL or minimizes CRPS10. Using CUBN as a
lower bound and the optimized CRPS score as the up-
per bound, uncertainty estimates can be normalized be-
tween these bounds (1 indicating optimal performance,
and 0 indicating same performance as ﬁxed uncer-
this normalized measure CRPS =
tainty). We call
minT CRPS(f,(yi,xi))−CRPS(fCU ,(yi,xi)) × 100, and the PLL
analogue PLL = PLL(f,(yi,xi))−PLL(fCU ,(yi,xi))
maxT PLL(f,(yi,xi))−PLL(fCU ,(yi,xi)) ×100.

CRPS(f,(yi,xi))−CRPS(fCU ,(yi,xi))

4.3. Test setup

Our evaluation compares MCBN to MCDO (Gal &
Ghahramani, 2015) and MNF (Louizos & Welling, 2017)
using the datasets and metrics described above. Our setup
is similar to (Hern´andez-Lobato & Adams, 2015), which
was also followed by (Gal & Ghahramani, 2015). How-
ever, our comparison implements a different hyperparame-
ter selection, allows for a larger range of dropout rates, and
uses larger networks with two hidden layers.

For the regression task, all models share a similar archi-
tecture: two hidden layers with 50 units each, and ReLU
activations, with the exception of Protein Tertiary Struc-
ture dataset (100 units per hidden layer). Inputs and out-
puts were normalized during training. Results were aver-
aged over ﬁve random splits of 20% test and 80% train-
ing and cross-validation (CV) data. For each split, 5-fold
CV by grid search with a RMSE minimization objective
was used to ﬁnd training hyperparameters and optimal n.o.
epochs, out of a maximum of 2000. For BN-based mod-
els, the hyperparameter grid consisted of a weight decay
factor ranging from 0.1 to 1−15 by a log 10 scale, and a
batch size range from 32 to 1024 by a log 2 scale. For
DO-based models, the hyperparameter grid consisted of
the same weight decay range, and dropout probabilities
in {0.2, 0.1, 0.05, 0.01, 0.005, 0.001}. DO-based models
used a batch size of 32 in all evaluations. For MNF11, the
n.o. epochs was optimized, the batch size was set to 100,
and early stopping test performed each epoch (compared to
every 20th for MCBN, MCDO).

For MCBN and MCDO, the model with optimal training
hyperparameters was used to optimize τ numerically. This
optimization was made in terms of average CV CRPS for
MCBN, CUBN, MCDO, and CUDO respectively.

Estimates for the predictive distribution were obtained by
taking T = 500 stochastic forward passes through the net-
work. For each split, test set evaluation was done 5 times
with different seeds. Implementation was done in Tensor-
Flow with the Adam optimizer and a learning rate of 0.001.

10Ti can be found analytically for PLL, but must be found nu-

merically for CRPS.

11Where we used an adapted version of the authors’ code.

For
the image classiﬁcation test we use CIFAR10
(Krizhevsky & Hinton, 2009) which includes 10 object
classes with 5,000 and 1,000 images in the training and
test sets, respectively. Images are 32x32 RGB format. We
trained a ResNet32 architecture with a batch size of 32,
learning rate of 0.1, weight decay of 0.0002, leaky ReLU
slope of 0.1, and 5 residual units. SGD with momentum
was used as the optimizer.

Code for reproducing our experiments is available at
https://github.com/icml-mcbn/mcbn.

4.4. Test results

The regression experiment comparing uncertainty quality
is summarized in Table 1. We report CRPS and PLL, ex-
pressed as a percentage, which reﬂects how close the model
is to the upper bound, and check to see if the model signif-
icantly exceeds the lower bound using a one sample t-test
(signiﬁcance level is indicated by *’s). Further details are
provided in Appendix Section 1.7.

In Figure 2 (left), we present a novel visualization of un-
certainty quality for regression problems. Data are sorted
by estimated uncertainty in the x-axis. Grey dots show the
errors in model predictions, and the shaded areas show the
model uncertainty. A running mean of the errors appears
as a gray line. If uncertainty estimation is working well,
a correlation should exist between the mean error (gray
line) and uncertainty (shaded area). This indicates that the
uncertainty estimation recognizes samples with larger (or
smaller) potential for predictive errors.

We applied MCBN on the image classiﬁcation task of CI-
FAR10. The baseline in this case is the softmax distribu-
tion using the moving average for BN units. Log likeli-
hood (PLL) is the metric used to compare with the base-
line. The baseline achieves a PLL of -0.32 on the test
set, while MCBN obtains a PLL of -0.28. Table 2 shows
the performance of MCBN when using different number of
stochastic forward passes (the MCBN batchsize is ﬁxed to
the training batch size at 32). PLL improves as the number
of the stochastic passes increases, until it is signiﬁcantly
better than the softmax baseline.

To demonstrate how model uncertainty can be obtained
from an existing network with minimal effort, we applied
MCBN to an image segmentation task using Bayesian Seg-
Net with the main CamVid and PASCAL-VOC models in
(Kendall et al., 2015). We simply ran multiple forward
passes with different mini-batches randomly taken from the
train set. The models obtained from the online model zoo
have BN blocks after each layer. We recalculate mean and
variance for the ﬁrst 2 blocks only and use the training
statistics for the rest of the blocks. Mini-batches of size
10 and 36 were used for CamVid and VOC respectively

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Table 1. Uncertainty quality measured on eight regression datasets. MCBN, MCDO and MNF are compared over 5 random 80-20
splits of the data with 5 different random seeds each split. We report CRPS and PLL, uncertainty metrics CRPS and PLL normalized
to a lower bound of constant variance and upper bound that maximizes the metric expressed as a percentage (described in Section 4.2).
Higher numbers mean the model is closer to the upper bound. We check if the reported values for CRPS and PLL signiﬁcantly exceed
the lower bound using a one sample t-test (signiﬁcance level indicated by *’s). See text for further details.

MCBN

MCDO

MNF

MCBN

CRPS

Dataset

Boston
Concrete
Energy
Kin8nm
Power
Protein
Wine (Red)
Yacht

8.50 ****
3.91 ****
5.75 ****
2.85 ****
0.24 ***
2.66 ****
0.26 **
-56.39 ***

3.06 ****
*
0.93
1.37 ns
1.82 ****
-0.44 ****
0.99 ****
2.00 ****
21.42 ****

5.88 ****
3.13 ***
1.10 ns
0.53 ns
-0.89 ****
0.57 ****
0.94 ****
24.92 ****

PLL
MCDO

5.51 ****
10.92 ****
-14.28

*
-0.26 ns
3.52 ****
6.23 ****
2.91 ****

-41.54 ns

MNF

1.76 ns
-2.16 ns
-33.88 ns
0.42 ns
-0.87 ****
0.52 ****
0.83 ****
46.19 ****

10.49 ****
-36.36 **
10.89 ****
1.68 ***
0.33 **
2.56 ****
0.19

*

45.58 ****

due to memory limits. The results in Figure 2 (right) were
obtained from 20 stochastic forward passes, showing high
uncertainty near object boundaries. The VOC results are
more appealing because of larger mini-batches.

We provide additional experimental results in the Ap-
pendix. Appendix Tables 2 and 3 show the mean CRPS
and PLL values from the regression experiment. Table 4
provides the raw CRPS and PLL scores.
In Table 5 we
provide RMSE results of the MCBN and MCDO networks
in comparison with non-stochastic BN and DO networks.
These results indicate that the procedure of multiple for-
ward passes in MCBN and MCDO show slight improve-
ments in the predictive accuracy compared to their non-
Bayesian counterparts. In Tables 6 and 7, we investigate the
effect of varying batch size while keeping other hyperpa-
rameters ﬁxed. We see that performance deteriorates with
small batch sizes (≤16), a known issue of BN (Ioffe, 2017).
Similarly, results varying the number of stochastic forward
passes T is reported in Tables 8 and 9. While performance
beneﬁts from large T , in some cases T = 50 (i.e. 1/10 of
T in the main evaluation) performs well. Uncertainty-error
plots for all the datasets are provided in the Appendix.

5. Discussion

The results presented in Tables 1-2 and Appendix Tables
2-9 indicate that MCBN generates meaningful uncertainty

Table 2. Uncertainty quality for image classiﬁcation varying
number of stochastic forward passes. Uncertainty quality for
image classiﬁcation measured by PLL. ResNet32 is trained on
CIFAR10 with batch size 32. PLL improves as the sampling in-
creases until it is signiﬁcantly better than the softmax baseline
(-0.32).

estimates that correlate with actual errors in the model’s
In Table 1, we show statistically signiﬁcant
prediction.
improvements over CUBN in the majority of the datasets,
both in terms of CRPS and PLL. The visualizations in
Figure 2 and in the Appendix Figures 2-3 show correla-
tions between the estimated model uncertainty and errors
of the network’s predictions. We perform the same exper-
iments using MCDO and MNF, and ﬁnd that MCBN gen-
erally performs on par with both methods. Looking closer,
MCBN outperforms MCDO and MNF in more cases than
not, measured by CRPS. However, care must be used. The
learned parameters are different, leading to different pre-
dictive means and confounding direct comparison.

The results on the Yacht Hydrodynamics dataset seem con-
tradictory. The CRPS score for MCBN are extremely neg-
ative, while the PLL score is extremely positive. The op-
posite trend is observed for MCDO. To add to the puzzle,
the visualization in Figure 2 depicts an extremely promis-
ing uncertainty estimation that models the predictive errors
with high ﬁdelity. We hypothesize that this strange behav-
ior is due to the small size of the data set, which only con-
tains 60 test samples, or due to the Gaussian assumption
of CRPS. There is also a large variability in the model’s
accuracy on this dataset, which further confounds the mea-
surements for such limited data.

One might criticize the overall quality of uncertainty es-
timates observed in all the models we tested, due to the
magnitude of CRPS and PLL in Table 1. The scores rarely
exceed 10% improvement over the lower bound. However,
we caution that these measures should be taken in context.
The upper bound is very difﬁcult to achieve in practice –
it is optimized for each test sample individually – and the
lower bound is a quite reasonable estimate.

Number of stochastic forward passes
64
2

16

32

4

8

1

PLL

-.36

-.32

-.30

-.29

-.29

-.28

-.28

128

-.28

The study of MCBN sensitivity to batch size revealed that a
certain batch size is required for the best performance, de-
pendent on the data. When doing inference on a GPU, large

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Data sorted by estimated uncertainty

Data sorted by estimated uncertainty

Image segmentation uncertainty (CamVid and PASCAL-VOC)

Figure 2. Uncertainty-error plots (left) and segmentation and uncertainty results applying MCBN to Bayesian SegNet (right).
(left) Errors in predictions (gray dots) sorted by estimated uncertainty on select datasets. The shaded areas show model uncertainty for
MCBN (blue), MNF (violet) and MCDO (red). The light area indicates 95% CI, dark area 50% CI. Gray dots show absolute prediction
errors on the test set, and the gray line depicts a running mean of the errors. The dashed line indicates the optimized constant uncertainty.
A correlation between estimated uncertainty (shaded area) and mean error (gray) indicates the uncertainty estimates are meaningful
for estimating errors. (right) Applying MCBN to Bayesian SegNet (Kendall et al., 2015) on scenes from CamVid (3rd column) and
PASCAL-VOC (4th column). Top: original image. Middle: the Bayesian estimated segmentation. Bottom: estimated uncertainty using
MCBN for all classes. The uncertainty maps for both datasets are reasonable, but qualitatively better for PASCAL-VOC due to the larger
mini-batch size (36) compared to CamVid (10). Smaller batch sizes were used for CamVid due to memory limitations (CamVid images
are 360x480 while VOC are 224x224). See Appendix for complete results.

batch sizes may cause memory issues for cases where the
input is large and the network has a large number of param-
eters, as is common for state-of-the-art image classiﬁcation
networks. However, there are various workarounds to this
problem. One can store BN statistics, instead of batches, to
reduce memory issues. Furthermore, we can use the Gaus-
sian estimate of the BN statistics as discussed previously,
which makes memory and computation extremely efﬁcient.

6. Conclusion

In this work, we have shown that training a deep network
using batch normalization is equivalent to approximate in-
ference in Bayesian models. We show evidence that the
uncertainty estimates from MCBN correlate with actual er-
rors in the model’s prediction, and are useful for practical

tasks such as regression, image classiﬁcation, and image
segmentation. Our experiments show that MCBN yields
a signiﬁcant improvement over the optimized constant un-
certainty baseline, on par with MCDO and MNF. Our eval-
uation also suggests new normalized metrics based on use-
ful upper and lower bounds, and a new visualization which
provides an intuitive explanation of uncertainty quality.

Finally, it should be noted that over the past few years,
batch normalization has become an integral part of most –
if not all – cutting edge deep networks. We have shown that
it is possible to obtain meaningful uncertainty estimates
from existing models without modifying the network or the
training procedure. With a few lines of code, robust uncer-
tainty estimates can be obtained by computing the variance
of multiple stochastic forward passes through an existing
network.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

References

Bui, T. D., Hern´andez-Lobato, D., Li, Y., Hern´andez-
Lobato, J. M., and Turner, R. E. Deep Gaussian Pro-
cesses for Regression using Approximate Expectation
Propagation. In ICML, 2016.

Chen, X., Kundu, K., Zhang, Z., Ma, H., Fidler, S., and Ur-
tasun, R. Monocular 3d object detection for autonomous
driving. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 2147–2156,
2016.

Djuric, U., Zadeh, G., Aldape, K., and Diamandis, P. Preci-
sion histology: how deep learning is poised to revitalize
histomorphology for personalized cancer care. npj Pre-
cision Oncology, 1(1):22, 2017.

Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M.,
Blau, H. M., and Thrun, S. Dermatologist-level classiﬁ-
cation of skin cancer with deep neural networks. Nature,
Feb 2017.

Gal, Y. Uncertainty in Deep Learning. PhD thesis, Univer-

sity of Cambridge, 2016.

Gal, Y. and Ghahramani, Z. Dropout as a Bayesian Ap-
proximation : Representing Model Uncertainty in Deep
Learning. ICML, 48:1–10, 2015.

Ghahramani, Z. Delve Datasets. University of Toronto,
URL http://www.cs.toronto.edu/

1996.
{˜}delve/data/kin/desc.html.

Ghahramani, Z. Probabilistic machine learning and ar-
tiﬁcial intelligence. Nature, 521(7553):452–459, May
2015.

Gneiting, T. and Raftery, A. E. Strictly Proper Scoring
Rules, Prediction, and Estimation. Journal of the Amer-
ican Statistical Association, 102(477):359–378, 2007.

Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain-
ing and harnessing adversarial examples. arXiv preprint
arXiv:1412.6572, 2014.

Graves, A. Practical Variational Inference for Neural Net-

works. NIPS, 2011.

Hern´andez-Lobato, J. M. and Adams, R. Probabilistic
backpropagation for scalable learning of bayesian neu-
ral networks. In International Conference on Machine
Learning, pp. 1861–1869, 2015.

Hinton, G. E. and Van Camp, D. Keeping the neural net-
works simple by minimizing the description length of
the weights. In Proceedings of the sixth annual confer-
ence on Computational learning theory, pp. 5–13. ACM,
1993.

Ioffe, S. Batch renormalization: Towards reducing mini-
batch dependence in batch-normalized models. CoRR,
abs/1702.03275, 2017. URL http://arxiv.org/
abs/1702.03275.

Ioffe, S. and Szegedy, C. Batch Normalization: Accelerat-
ing Deep Network Training by Reducing Internal Co-
variate Shift. Arxiv, 2015. URL http://arxiv.
org/abs/1502.03167.

Karpathy, A. Convnetjs demo:

toy 1d regression, 2015.
http://cs.stanford.edu/people/

URL
karpathy/convnetjs/demo/regression.
html.

Kendall, A., Badrinarayanan, V., and Cipolla, R. Bayesian
SegNet: Model Uncertainty in Deep Convolutional
Encoder-Decoder Architectures for Scene Understand-
ing. CoRR, abs/1511.0, 2015. URL http://arxiv.
org/abs/1511.02680.

Kingma, D. P. and Welling, M. Auto-Encoding Variational

Bayes. In ICLR, 2014.

Krizhevsky, A. and Hinton, G. Learning multiple layers of

features from tiny images. 2009.

Krueger, D., Huang, C.-W., Islam, R., Turner, R., Lacoste,
A., and Courville, A. Bayesian hypernetworks. arXiv
preprint arXiv:1710.04759, 2017.

Lehmann, E. L.

Elements of Large-Sample Theory.

Springer Verlag, New York, 1999. ISBN 0387985956.

Li, Y. and Gal, Y. Dropout Inference in Bayesian Neural

Networks with Alpha-divergences. arXiv, 2017.

Louizos, C. and Welling, M. Multiplicative normal-
izing ﬂows for variational Bayesian neural networks.
In Precup, D. and Teh, Y. W. (eds.), Proceedings of
the 34th International Conference on Machine Learn-
ing, volume 70 of Proceedings of Machine Learn-
ing Research, pp. 2218–2227, International Convention
Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.
URL http://proceedings.mlr.press/v70/
louizos17a.html.

MacKay, D. J. A practical bayesian framework for back-
propagation networks. Neural computation, 4(3):448–
472, 1992.

Neal, R. M. Bayesian Learning for Neural Networks. PhD

thesis, University of Toronto, 1995.

Neal, R. M. Bayesian learning for neural networks, volume

118. Springer Science & Business Media, 2012.

Selten, R. Axiomatic characterization of the quadratic scor-
ing rule. Experimental Economics, 1(1):43–62, 1998.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Shen, L. End-to-end training for whole image breast can-
cer diagnosis using an all convolutional design. arXiv
preprint arXiv:1708.09427, 2017.

University of California, I. UC Irvine Machine Learning
Repository, 2017. URL https://archive.ics.
uci.edu/ml/index.html.

Wang, S. I. and Manning, C. D.
Proceedings of

Fast dropout train-
the 30th International Con-
ing.
ference on Machine Learning, 28:118–126, 2013.
URL http://machinelearning.wustl.edu/
mlpapers/papers/wang13a.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

1. Appendix

1.1. Variational Approximation

Assume we were to come up with a family of distributions parameterized by θ in order to approximate the posterior, qθ(ω).
Our goal is to set θ such that qθ(ω) is as similar to the true posterior p(ω|D) as possible.

For clarity, qθ(ω) is a distribution over stochastic parameters ω that is determined by a set of learnable parameters θ and
some source of randomness. The approximation is therefore limited by our choice of parametric function qθ(ω) as well
as the randomness.12 Given ω and an input x, an output distribution p(y|x, ω) = p(y|fω(x)) = fω(x, y) is induced by
observation noise (the conditionality of which is omitted for brevity).

One strategy for optimizing θ is to minimize KL(qθ(ω)||p(ω|D)), the KL divergence of p(ω|D) w.r.t. qθ(ω). Minimizing
KL(qθ(ω)||p(ω|D)) is equivalent to maximizing the ELBO:

(cid:90)

ω

qθ(ω) ln p(Y|X, ω)dω − KL(qθ(ω)||p(ω))

Assuming i.i.d. observation noise, this is equivalent to minimizing:

LVA(θ) := −

qθ(ω) ln p(yi|fω(xi))dω + KL(qθ(ω)||p(ω))

N
(cid:88)

(cid:90)

n=1

Instead of making the optimization on the full training set, we can use a subsampling (yielding an unbiased estimate of
LVA(θ)) for iterative optimization (as in mini-batch optimization):

ˆLVA(θ) := −

N
M

(cid:90)

(cid:88)

i∈B

ω

qθ(ω) ln p(yi|fω(xi))dω + KL(qθ(ω)||p(ω))

During optimization, we want to take the derivative of the expected log likelihood w.r.t. the learnable parameters θ. (Gal,
2016) provides an intuitive interpretation of a MC estimate for NNs trained with a SRT (equivalent to the reparametrisation
trick in (Kingma & Welling, 2014)), and this interpretation is followed here. We let an auxillary variable (cid:15) represent the
stochasticity during training such that ω = g(θ, (cid:15)). The function g and the distribution of (cid:15) are such that p(g(θ, (cid:15))) =
qθ(ω).13 Assuming qθ(ω) can be written (cid:82)
(cid:15) qθ(ω|(cid:15))p((cid:15))d(cid:15) where qθ(ω|(cid:15)) = δ(ω − g(θ, (cid:15))), this auxiliary variable yields
the estimate (full proof in (Gal, 2016)):

ˆLVA(θ) = −

N
M

(cid:90)

(cid:88)

i∈B

(cid:15)

p((cid:15)) ln p(yi|fg(θ,(cid:15))(xi))d(cid:15) + KL(qθ(ω)||p(ω))

where taking a single sample MC estimate of the integral yields the optimization objective in Eq. 1.

12In an approx. Bayesian view of a NN, qθ(ω) would correspond to the distribution of weights in the network given by some SRT.
13In a NN trained with BN, it is easy to see that g exists if we let (cid:15) represent a mini-batch selection from the training data, since all

BN units’ means and variances are completely determined by (cid:15) and θ.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

1.2. KL Divergence of factorized Gaussians

If qθ(ω) and p(ω) factorize over all stochastic parameters:

KL(qθ(ω)||p(ω)) = −

(cid:2)qθ(ωi)(cid:3) ln

(cid:90)

(cid:90)

(cid:89)

i
(cid:89)

i

(cid:90)

ω

ω

(cid:104)

= −

(cid:2)qθ(ωi)(cid:3) (cid:88)

(cid:88)

=

−

(cid:89)

(cid:2)qθ(ωi)(cid:3) ln

dω

(cid:81)
(cid:81)

i p(ωi)
i qθ(ωi)
(cid:20)

ln

p(ωi)
qθ(ωi)

i

(cid:21) (cid:89)

dωi

i
(cid:89)

i

(cid:105)

dωi

ωi(cid:54)=j

i(cid:54)=j

p(ωj)
qθ(ωj)
(cid:90)

dωj

qθ(ωj) ln

p(ωj)
qθ(ωj)

(cid:89)

qθ(ωi)dωi

(cid:105)

(cid:88)

(cid:104)

(cid:90)

ω

i

ωj

−

(cid:90)

j

j

i

(cid:88)

i
(cid:88)

=

=

=

−

qθ(ωi) ln

ωi

p(ωi)
qθ(ωi)

dωi

KL(qθ(ωi)||p(ωi))

(3)

such that KL(qθ(ω)||p(ω)) is the sum of the KL divergence terms for the individual stochastic parameters ωi. If the
factorized distributions are Gaussians, where qθ(ωi) = N (µq, σ2

q ) and p(ωi) = N (µp, σ2

p) we get:

KL(qθ(ωi)||p(ωi)) =

qθ(ωi) ln

dωi

qθ(ωi)
p(ωi)
(cid:90)

ωi

= − H(qθ(ωi)) −

qθ(ωi) ln p(ωi)dωi

= −

(1 + ln(2πσ2

q ))

(cid:90)

ωi

1
2
(cid:90)

ωi
1
2
1
2

σp
σq

−

qθ(ωi) ln

1
p)1/2
(2πσ2

(cid:110)

exp

−

(ωi − µp)2
2σ2
p

(cid:111)

dωi

(4)

= −

(1 + ln(2πσ2

+

ln(2πσ2

p) +

q ))
Eq[ω2

i ] − 2µpEq[ωi] + µ2
p

2σ2
p

= ln

+

q + (µq − µp)2
σ2
2σ2
p

−

1
2

for each KL divergence term. Here H(qθ(ωi)) = 1

2 (1 + ln(2πσ2

q )) is the differential entropy of qθ(ωi).

1.3. Distribution of µu

B, σu
B

Here we approximate the distribution of mean and standard deviation of a mini-batch, separately to two Gaussians – This
has also been empirically veriﬁed, see Figure 3 for 2 sample plots and the appendix section 1.9 for more. For the mean we
get:

where xm are the examples in the sampled batch. We will assume these are sampled i.i.d.14. Samples of the random
variable W(j)xm are then i.i.d.. Then by central limit theorem (CLT) the following holds for sufﬁciently large M (often
≥ 30):

µB =

ΣM

m=1W(j)xm
M

µB ∼ N (µ,

σ2
M

)

14Although in practice with deep learning, mini-batches are sampled without replacement, stochastic gradient descent samples with

replacement in its standard form.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 3. Batch statistics used to train the network are normal. A one-sample Kolmogorov-Smirnov test checks that µB and σB come
from a standard normal distribution. More examples are available in Appendix 1.9.

For standard deviation:

(cid:114)

σB =

ΣM

m=1(W(j)xm − µB)2
M

√

M (σB − σ) =

M

√

(cid:114)

(cid:16)

ΣM

m=1(W(j)xm − µB)2
M

√

(cid:17)

−

σ2

We want to rewrite
x = ΣM

m=1(W(j)xm−µB)2
M

:

(cid:113) ΣM

m=1(W(j)xm−µB)2
M

. We take a Taylor expansion of f (x) =

x around a = σ2. With

√

Then

so

√

√

x =

σ2 +

1
√
σ2
2

(x − σ2) + O[(x − σ2)2]

√

M (σB − σ) =

M

(cid:32)

√

1
√
σ2
2

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

(cid:34)

O

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

+

− σ2(cid:17)
− σ2(cid:17)2(cid:35)(cid:33)

√

M
2σ

=

=

1
√

2σ

M

+

(cid:16) 1
m=1(W(j)xm − µB)2 − σ2(cid:17)
ΣM
M
(cid:34)√
m=1(W(j)xm − µB)2
M
m=1(W(j)xm − µB)2 − M σ2(cid:17)
ΣM

(cid:16) ΣM

M

O

(cid:16)

+

− σ2(cid:17)2(cid:35)

(cid:34)√

O

M

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

− σ2(cid:17)2(cid:35)

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

consider ΣM

m=1(W(j)xm − µB)2. We know that E[W(j)xm] = µ and write

then

ΣM
=ΣM
=ΣM
=ΣM
=ΣM
=ΣM

m=1(W(j)xm − µB)2
m=1((W(j)xm − µ) − (µB − µ))2
m=1((W(j)xm − µ)2 + (µB − µ)2 − 2(W(j)xm − µ)(µB − µ))
m=1(W(j)xm − µ)2 + M (µB − µ)2 − 2(µB − µ)ΣM
m=1(W(j)xm − µ)2 − M (µB − µ)2
m=1((W(j)xm − µ)2 − (µB − µ)2)

m=1(W(j)xm − µ)

√

M (σB − σ) =

1
√

(cid:16)

m=1((W(j)xm − µ)2 − (µB − µ)2) − M σ2(cid:17)
ΣM

+

2σ

M

(cid:34)√

O

M

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

− σ2(cid:17)2(cid:35)

=

(cid:16)

1
√

2σ

M

ΣM

m=1(W(j)xm − µ)2 − ΣM

(cid:34)√

O

M

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

m=1(µB − µ)2 − M σ2(cid:17)
+
− σ2(cid:17)2(cid:35)

=

(cid:16)

1
√

2σ

M

ΣM

m=1((W(j)xm − µ)2 − σ2) − ΣM

(cid:34)√

O

M

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

m=1(µB − µ)2(cid:17)
− σ2(cid:17)2(cid:35)

+

=

1
√

2σ

−

M
1
√

M

2σ
(cid:34)√

+ O

M

ΣM

m=1((W(j)xm − µ)2 − σ2)

ΣM

m=1(µB − µ)2

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

− σ2(cid:17)2(cid:35)

ΣM

m=1((W(j)xm − µ)2 − σ2)
(cid:125)

(cid:123)(cid:122)
term A

=

2σ
(cid:124)

−

1
√

M

√

M
2σ

(cid:124)

(µB − µ)2
(cid:123)(cid:122)
(cid:125)
term B

(cid:34)√

(cid:16) ΣM

+ O

M

(cid:124)

m=1(W(j)xm − µB)2
M
(cid:123)(cid:122)
term C

− σ2(cid:17)2(cid:35)

(cid:125)

We go through each term in turn

Term A
We have

Term A =

ΣM

m=1((W(j)xm − µ)2 − σ2)

1
√

2σ

M

m=1(W(j)xm − µ)2 is the sum of M RVs (W(j)xm − µ)2. Note that since E[W(j)xm] = µ it holds that
where ΣM
E[(W(j)xm − µ)2] = σ2. Since (W(j)xm − µ)2 is sampled approximately iid (by assumptions above), for large enough

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

M by CLT it holds approximately that

ΣM

m=1(W(j)xm − µ)2 ∼ N (M σ2, M Var((W(j)xm − µ)2))

Var((W(j)xm − µ)2) = E[(W(j)xm − µ)2∗2] − E[(W(j)xm − µ)2]2
= E[(W(j)xm − µ)4] − σ4

ΣM

m=1((W(j)xm − µ)2 − σ2) ∼ N (0, M ∗ E[(W(j)xm − µ)4] − M σ4)

Term A ∼ N (0,

E[(W(j)xm − µ)4] − σ4
4σ2

)

Term B =

(µB − µ)2 =

M (µB − µ)(µB − µ)

√

M
2σ

√

1
2σ

Consider (µB − µ). As µB

p
−→ µ when M → ∞ we have µB − µ

p
−→ 0. We also have

√

M (µB − µ) =

ΣM

m=1W(j)xm
M

√

√

−

M µ

which by CLT is approximately Gaussian for large M . We can then make use of the Cramer-Slutzky Theorem, which
p
states that if (Xn)n≥1 and (Yn)n≥1 are two sequences such that Xn
−→ a as n → ∞ where a is a constant,
then as n → ∞, it holds that Xn ∗ Yn

d−→ X ∗ a. Thus, Term B is approximately 0 for large M.

d−→ X and Yn

(cid:34)√

Term C = O

M

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

− σ2(cid:17)2(cid:35)

Since E[(W(j)xm − µ)2] = σ2 we can make the same use of Cramer-Slutzky as for Term B, such that Term C is approxi-
mately 0 for large M.

Finalizing the distribution
We have approximately

√

M (σB − σ) ∼ N (0,

E[(W(j)xm − µ)4] − σ4
4σ2

)

σB ∼ N (σ,

E[(W(j)xm − µ)4] − σ4
4σ2M

)

Here we make use of the stochasticity from BN modeled in the Appendix section 1.3, to evaluate the implied prior on the
stochastic variables for a BN network. Speciﬁcally, we consider a BN network with fully connected layers and BN applied
to each layer, trained with L2-regularization (weight decay). In the following, we make use of the simplifying assumptions
of no scale and shift tranformations, BN applied to each layer, and independent input units to each layer.

where

Then

so

Term B
We have

Term C
We have

so

1.4. Prior

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Equivalence between the objectives of Eq. (1) and (2) requires:

(5)

(6)

(7)

∂
∂θk

KL(qθ(ω)||p(ω)) = N τ

Ω(θ)

∂
∂θk

∂
∂θk

L
(cid:88)

l=1

= N τ

λl||Wl||2

µu
B

∝
∼ N (µu,

(σu)2
M

),

σu
B

∝
∼ N (σu,

E[(W(u)x − µu)4] − (σu)4
4(σu)2M

)

KL(qθ(ω)||p(ω)) =

KL(qθ(ωi)||p(ωi))

(cid:88)

i

where θk ∈ θ, and θ is the set of weights in the network. To proceed with the LHS of Eq. (5) we ﬁrst need to ﬁnd the
approximate posterior qθ(ω) that batch normalization induces. As shown in Appendix 1.3, with some weak assumptions
and approximations the Central Limit Theorem (CLT) yields Gaussian distributions of the stochastic variables µu
B, for
large enough M . For any BN unit u:

B, σu

where µu and σu are population-level moments (i.e. moments over D).

We assume that qθ(ω) and p(ω) factorize over all stochastic variables.15 We use i as an index of the set of stochastic
variables. As shown in Eq. (3) in Appendix 1.2, the factorized distributions yield:

Note that each BN unit produces two KL(qθ(ωi)||p(ωi)) terms: one for ωi = µu
terms for one particular BN unit u, and drop the index i for brevity. We use a Gaussian prior p(ωi) = N (µp, σ2
consistency, use the notation qθ(ωi) = N (µq, σ2

q ). As shown in Eq. (4) in Appendix 1.2:

B and one for ωi = σu

B. We consider these
p) and, for

Since θk changes during training, a prior cannot depend on θk so ∂
∂θk

(µp) = ∂
∂θk

(σp) = 0. Letting (·)(cid:48) denote ∂
∂θk

(·):

KL(qθ(ωi)||p(ωi)) = ln

+

σp
σq

q + (µq − µp)2
σ2
2σ2
p

−

1
2

∂
∂θk

KL(qθ(ωi)||p(ωi)) =

σqσ(cid:48)

q + µqµ(cid:48)
σ2
p

q − µpµ(cid:48)
q

−

σ(cid:48)
q
σq

We need not consider θk past a previous layer’s BN, since a normalization step is performed before scale and shift. In the
general case with a given Gaussian p(ω), Eq. 7 evaluated on all BN units’ means and standard deviations w.r.t. all θk up to
a previous layer’s BN, would yield an expression for a custom N τ ∂
Ω(θ) that could be used for an exact VI treatment of
∂θk
BN.

In our reconciliation of weight decay however, given our assumptions of no scale and shift and BN applied to each layer,
we need only consider the weights in the same layer as the BN unit. This means that the stochastic variables in layer l are
only affected by weights in θk ∈ W l (i.e. not the scale and shift variables operating on the input to the layer). We denote
a weight connecting the k:th input unit to the u:th BN unit by W(u,k). For such weights, we need to derive µ(cid:48)
q, for
two cases: ωi = µu
B by
µσ,q and σσ,q. Using the distributions modeled in Eq. 6:

q and σ(cid:48)
B by µµ,q and σµ,q, and for σu

B. We denote the priors of the mean and std. dev for µu

B and ωi = σu

15The empirical distributions have been numerically checked to be linearly independent and the joint distribution is close to a bi-variate

Gaussian.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Case 1: ωi = µu
B

= W(u) ¯x

µµ,q =

µ(cid:48)

µ,q =

W(u)x
N

xk
N

= ¯xk

(cid:88)

x∈D
(cid:88)

x∈D

(cid:114)

σµ,q =

(σu)2
M

=

σ(cid:48)
µ,q =

σ−1
q

1
2

(cid:88)

x∈D

(cid:115)

(cid:80)

x∈D(W(u)x − µq)2
N M

2(W(u)x − µq)(xk − ¯xk)
N M

= σ−1
q

(cid:18) K
(cid:88)

i=1

W(u,i)Cov(xi, xk)

M −1

(cid:19)

where there are K input units to the layer.

Case 2: ωi = σu
B

(cid:115)

(cid:80)

µσ,q =

x∈D(W(u)x − µq)2
N
(cid:18) K
(cid:88)

= σµ,qM

1
2

(cid:19)

σ,q = σ−1
µ(cid:48)

µ,qM − 1

2

W(u,i)Cov(xi, xk))

i=1
E[(W(u)x − µu)4] − (σu)4
4(σu)2M

σσ,q =

σ(cid:48)
σ,q =

E[(W(u)x − µu)4](cid:48)σu − 2(σu)4(σu)(cid:48) − 2(σu)(cid:48)E[(W(u)x − µu)4]
4(σu)3M

Combining these results with Eq. 7 we ﬁnd that taking KL(qθ(ωi)||p(ωi)) for the mean and variance of a single BN unit u
wrt the weight from input unit k:

∂

−

σ(cid:48)
µ,q
σµ,q
σ(cid:48)
σ,q
σσ,q

∂

∂W(u,k) KL(qθ(µu
µ,q + µµ,qµ(cid:48)
σµ,qσ(cid:48)

µ,q − µµ,pµ(cid:48)

µ,q

B)||p(µu

B)) +

∂W(u,k) KL(qθ(σu

B)||p(σu

B))

σ,q

σσ,qσ(cid:48)

σ,q − µσ,pµ(cid:48)

σ2
µ,p
σ,q + µσ,qµ(cid:48)
σ2
σ,p
O(M −1) + ¯xkW(u) ¯x − µµ,p ¯xk
σ2
O(M −2) + (cid:80)K
i=1 W(u,i)Cov(xi, xk) − µσ,pO(M − 1
2 )

− O(M −1)

µ,p

−

σ2

σ,p

E[(W(u)x − µu)4](cid:48)σu − 2(σu)4(σu)(cid:48) − 2(σu)(cid:48)E[(W(u)x − µu)4]
E[(W(u)x − µu)4]σu − (σu)5

=

+

=

+

−

where we summarize the terms scaled by M with O-notation. We see that if we let M → ∞, µµ,p = 0, σµ,p → ∞,
µσ,p = 0 and σσ,p is small enough, then:

(cid:18)

∂
∂W(u,k)

KL(qθ(µu

B)||p(µu

B)) + KL(qθ(σu

B)||p(σu

B))

≈

(cid:19)

(cid:80)K

i=1 W(u,i)Cov(xi, xk)

σ2

σ,p

such that each BN layer yields the following:

(cid:88)

K
(cid:88)

u

i=1

(cid:18)

∂
∂W(u,i)

KL(qθ(µu

B)||p(µu

B)) + KL(qθ(σu

B)||p(σu

B))

(cid:19)

(cid:88)

≈

u

(cid:80)K

i=1 Wu,i (cid:80)K
σ2

σ,p,u

i2=1 Cov(xi, xi2)

(8)

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

where we denote the prior for the std. dev. of the std. dev. of BN unit u by σσ,p,u. Given our assumptions of no scale and
shift from the previous layer, and independent input features in every layer, Eq. 8 reduces to:

(cid:88)

K
(cid:88)

u

i=1

Wu,i
σ2

σ,p

if the same prior is chosen for each BN unit in the layer. We therefore ﬁnd that Eq. 5 is reconciled by p(µu
and p(σu

is small enough, which is the case if N is large.

), if

B) → N (0,

1
2N τ λl

1
2N τ λl

B) → N (0, ∞)

1.5. predictive distribution properties

This section provides derivations of properties of the predictive distribution p∗(y|x, D) in section 3.4, following (Gal,
2016). We ﬁrst ﬁnd the ﬁrst two modes of the approximate predictive distribution (with the second mode applicable to
regression), then show how to estimate the predictive log likelihood, a measure of uncertainty quality used in the evaluation.

Predictive mean Assuming Gaussian iid noise deﬁned by model precision τ ,
N (y; fω(x), τ −1I):

i.e.

fω(x, y) = p(y|fω(x)) =

fω(x, y)qθ(ω)dω

dy

(cid:17)

N (y; fω(x), τ −1I)qθ(ω)dω

dy

(cid:17)

yN (y; fω(x), τ −1I)dy

qθ(ω)dω

(cid:17)

Ep∗ [y] =

yp∗(y|x, D)dy

(cid:16) (cid:90)

ω
(cid:16) (cid:90)

y

y

ω

(cid:16) (cid:90)

y

(cid:90)

(cid:90)

y
(cid:90)

y
(cid:90)

ω

(cid:90)

ω

=

=

=

=

≈

fω(x)qθ(ω)dω

1
T

T
(cid:88)

i=1

f ˆωi(x)

where we take the MC Integral with T samples of ω for the approximation in the ﬁnal step.

Predictive variance For regression, our goal is to estimate:

Covp∗ [y] = Ep∗ [y(cid:124)y] − Ep∗ [y](cid:124)Ep∗ [y]

We ﬁnd that:

Ep∗ [y(cid:124)y] =

y(cid:124)yp∗(y|x, D)dy

(cid:90)

y
(cid:90)

y
(cid:90)

ω

(cid:90)

ω

(cid:90)

ω

=

=

=

=

(cid:16) (cid:90)

ω

y(cid:124)y

(cid:16) (cid:90)

y

fω(x, y)qθ(ω)dω

dy

(cid:17)

y(cid:124)yfω(x, y)dy

qθ(ω)dω

(cid:17)

(cid:16)

(cid:17)
Covfω(x,y)(y) + Efω(x,y)[y](cid:124)Efω(x,y)[y]

qθ(ω)dω

(cid:16)

(cid:17)
τ −1I + fω(x)(cid:124)fω(x)

qθ(ω)dω

= τ −1I + Eqθ (ω)[fω(x)(cid:124)fω(x)]

≈ τ −1I +

f ˆωi (x)(cid:124)f ˆωi(x)

1
T

T
(cid:88)

i=1

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

where we use MC integration with T samples for the ﬁnal step. The predictive covariance matrix is given by:

Covp∗ [y] ≈ τ −1I +

f ˆωi (x)(cid:124)f ˆωi(x) − Ep∗ [y](cid:124)Ep∗ [y]

1
T

T
(cid:88)

i=1

which is the sum of the variance from observation noise and the sample covariance from T stochastic forward passes
though the network.

The form of p∗ can be approximated by a Gaussian for each output dimension (for regression). We assume bounded
domains for each input dimension, wide layers throughout the network, and a uni-modal distribution of weights centered
at 0. By the Liapounov CLT condition, the ﬁrst layer then receives approximately Gaussian inputs (a proof can be found
in (Lehmann, 1999)). Having sampled µu
B from a mini-batch, each BN unit’s output is bounded. CLT thereby
continues to hold for deeper layers, including fω(x) = WLxL. A similar motivation for a Gaussian approximation of
Dropout has been presented by (Wang & Manning, 2013).

B and σu

Predictive Log Likelihood We use the Predictive Log Likelihood (PLL) as a measure to estimate the model’s uncertainty
quality. For a certain test point (yi, xi), the PLL deﬁnition and approximation can be expressed as:

PLL(fω(x), (yi, xi)) = log p(yi|fω(xi))

= log

fω(xi, yi)p(ω|D)dω

≈ log

fω(xi, yi)qθ(ω)dω

≈ log

p(yi|f ˆωj (xi))

(cid:90)

(cid:90)

1
T

T
(cid:88)

j=1

where ˆωj represents a sampled set of stochastic parameters from the approximate posterior distrubtion qθ(ω) and we take
a MC integration with T samples. For regression, due to the iid Gaussian noise, we can further develop the derivation into
the form we use when sampling:

PLL(fω(x), (yi, xi)) = log

N (yi|f ˆωj (xi), τ −1I)

1
T

T
(cid:88)

j=1

= logsumexpj=1,...,T
1
2

− log T −

log 2π +

1
2

1
2

log τ

(cid:0) −

τ ||yi − f ˆωj (xi)||2(cid:1)

Note that PLL makes no assumption on the form of the approximate predictive distribution.

1.6. Data

To assess the uncertainty quality of the various methods studied we rely on eight standard regression datasets, listed in Table
3. Publicly available from the UCI Machine Learning Repository (University of California, 2017) and Delve (Ghahramani,
1996), these datasets have been used to benchmark comparative models in recent related literature (see (Hern´andez-Lobato
& Adams, 2015), (Gal & Ghahramani, 2015), (Bui et al., 2016) and (Li & Gal, 2017)).

For image classiﬁcation, we applied MCBN using ResNet32 to CIFAR10.

For the image segmentation task, we applied MCBN using Bayesian SegNet on data from CamVid and PASCAL-VOC
using models published in (Kendall et al., 2015).

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Table 3. Regression dataset summary. Properties of the eight regression datasets used to evaluate MCBN. N is the dataset size and Q
is the n.o. input features. Only one target feature was used – we used heating load for the Energy Efﬁciency dataset, which contains
multiple target features.

Dataset name

Boston Housing
Concrete Compressive Strength
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein Tertiary Structure
Wine Quality (Red)
Yacht Hydrodynamics

N

506
1,030
768
8,192
9,568
45,730
1,599
308

Q

13
8
8
8
4
9
11
6

1.7. Extended experimental results

Below, we provide extended results measuring uncertainty quality. In Tables 4 and 5, we provide tables showing the mean
CRPS and PLL values for MCBN and MCDO. These results indicate that MCBN performs on par or better than MCDO
across several datasets. In Table 6 we provide the raw PLL and CRPS results for MCBN and MCDO. In Table 7 we provide
RMSE results of the MCBN and MCDO networks in comparison with non-stochastic BN and DO networks. These results
indicate that the procedure of multiple forward passes in MCBN and MCDO show slight improvements in the accuracy of
the network.

In Figure 4 and Figure 5, we provide a full set of our uncertainty quality visualization plots, where errors in predictions
are sorted by estimated uncertainty. The shaded areas show the model uncertainty and gray dots show absolute prediction
errors on the test set. A gray line depicts a running mean of the errors. The dashed line indicates the optimized constant
uncertainty. In these plots, we can see a correlation between estimated uncertainty (shaded area) and mean error (gray).
This trend indicates that the model uncertainty estimates can recognize samples with larger (or smaller) potential for
predictive errors.

We also conduct a sensitivity analysis to estimate how the uncertainty quality varies with batch size M and the number of
stochastic forward passes T . In tables 8 and 9 we evaluate CRPS and PLL respectively for the regression datasets when
trained and evaluated with varying batch sizes, but other hyperparameters ﬁxed (T was ﬁxed at 100). The results show that
results deteriorate when batch sizes are too small, likely stemming from the large variance of the approximate posterior.
In tables 10 and 11 we evaluate CRPS and PLL respectively for the regression datasets when trained and evaluated with
varying n.o. stochastic forward samples, but other hyperparameters ﬁxed (M was ﬁxed at 128). The results are indicative
of performance improvements with larger T , although we see improvements over baseline for some datasets already with
T = 50 (1/10:th of the T used in our main experiments).

Table 4. Uncertainty quality measured by CRPS on regression dasets. CRPS measured on eight datasets over 5 random 80-20 splits
of the data with 5 different random seeds each split. Mean values for MCBN, MCDO and MNF are reported along with standard error.
A signiﬁcance test was performed to check if CRPS signiﬁcantly exceeds the baseline. The p-value from a one sample t-test is reported.

CRPS

Dataset

MCBN

p-value

MCDO

p-value

MNF

p-value

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

8.50±0.86
3.91±0.25
5.75±0.52
2.85±0.18
0.24±0.05
2.66±0.10
0.26±0.07
-56.39±14.27

6.39E-10
4.53E-14
6.71E-11
2.33E-14
2.32E-04
2.77-12
1.26E-03
5.94E-04

3.06±0.33
0.93±0.41
1.37±0.89
1.82±0.14
-0.44±0.05
0.99±0.08
2.00±0.21
21.42±2.99

1.64E-09
3.13E-02
1.38E-01
1.64E-12
2.17E-08
2.34E-12
1.83E-09
2.16E-07

5.88±1.09
3.13±0.81
1.10±2.63
0.52±0.26
-0.89±0.15
0.57±0.03
0.93±0.12
24.92±3.77

2.01E-05
6.43E-04
6.45E-01
7.15E-02
3.36E-06
8.56E-16
6.19E-08
9.62E-06

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Table 5. Uncertainty quality measured by PLL on regression dasets. PLL measured on eight datasets over 5 random 80-20 splits of
the data with 5 different random seeds each split. Mean values for MCBN, MCDO and MNF are reported along with standard error. A
signiﬁcance test was performed to check if PLL signiﬁcantly exceeds the baseline. The p-value from a one sample t-test is reported.

PLL

Dataset

MCBN

p-value

MCDO

p-value

MNF

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

10.49±1.35
-36.36±12.12
10.89±1.16
1.68±0.37
0.33±0.14
2.56±0.23
0.19±0.09
45.58±5.18

5.41E-08
6.19E-03
1.79E-09
1.29E-04
2.72E-02
4.28E-11
3.72E-02
5.67E-09

5.51±1.05
10.92±1.78
-14.28±5.15
-0.26±0.18
3.52±0.23
6.23±0.19
2.91±0.35
-41.54±31.37

2.20E-05
2.34E-06
1.06E-02
1.53E-01
1.12E-13
2.57E-21
1.84E-08
1.97E-01

1.76±1.12
-2.16±4.19
-33.88±29.57
0.42±0.43
-0.86±0.15
0.52±0.07
0.83±0.16
46.19±4.45

p-value

1.70E-01
6.79E-01
2.70E-01
2.70E-01
7.33E-06
1.81E-07
2.27E-05
2.47E-07

Table 6. Raw (unnormalized) CRPS and PLL scores on regression datasets. CRPS and PLL measured on eight datasets over 5
random 80-20 splits of the data with 5 different random seeds each split. Mean values and standard errors are reported for MCBN,
MCDO and MNF.

Dataset

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

MCBN

1.45±0.02
2.40±0.04
0.33±0.01
0.04±0.00
2.00±0.01
1.95±0.01
0.34±0.00
0.68±0.02

CRPS
MCDO

1.41±0.02
2.42±0.04
0.26±0.00
0.04±0.00
2.00±0.01
1.95±0.00
0.33±0.00
0.32±0.01

MNF

MCBN

1.57±0.02
3.61±0.02
1.33±0.04
0.05±0.00
2.31±0.01
2.25±0.01
0.34±0.00
0.94±0.01

-2.38±0.02
-3.45±0.11
-0.94±0.04
1.21±0.01
-2.75±0.00
-2.73±0.00
-0.95±0.01
-1.39±0.03

PLL
MCDO

-2.35±0.02
-2.94±0.02
-0.80±0.04
1.24±0.00
-2.72±0.01
-2.70±0.00
-0.89±0.01
-2.57±0.69

MNF

-2.51±0.06
-3.35±0.04
-3.18±0.07
1.04±0.00
-2.86±0.01
-2.83±0.01
-0.93±0.00
-1.96±0.05

Table 7. Prediction accuracy measured by RMSE on regression datasets. RMSE measured on eight datasets over 5 random 80-20
splits of the data with 5 different random seeds each split. Mean values and standard errors are reported for for MCBN, MCDO and
MNF as well as conventional non-Bayesian models BN and DO.

Dataset

MCBN

BN

DO

MNF

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

2.75±0.05
4.78±0.09
0.59±0.02
0.07±0.00
3.74±0.01
3.66±0.01
0.62±0.00
1.23±0.05

2.77±0.05
4.89±0.08
0.57±0.01
0.07±0.00
3.74±0.01
3.69±0.01
0.62±0.00
1.28±0.06

RMSE
MCDO

2.65±0.05
4.80±0.10
0.47±0.01
0.07±0.00
3.74±0.02
3.66±0.01
0.60±0.00
0.75±0.03

2.69±0.05
4.99±0.10
0.49±0.01
0.07±0.00
3.72±0.02
3.68±0.01
0.61±0.00
0.72±0.04

2.98±0.06
6.57±0.04
2.38±0.07
0.09±0.00
4.19±0.01
4.10±0.01
0.61±0.00
2.13±0.05

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 4. Errors in predictions (gray dots) sorted by estimated uncertainty on select datasets. The shaded areas show model uncertainty
(light area 95% CI, dark area 50% CI). Gray dots show absolute prediction errors on the test set, and the gray line depicts a running
mean of the errors. The dashed line indicates the optimized constant uncertainty. A correlation between estimated uncertainty (shaded
area) and mean error (gray) indicates the uncertainty estimates are meaningful for estimating errors.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 5. Errors in predictions (gray dots) sorted by estimated uncertainty on select datasets. The shaded areas show model uncertainty
(light area 95% CI, dark area 50% CI). Gray dots show absolute prediction errors on the test set, and the gray line depicts a running
mean of the errors. The dashed line indicates the optimized constant uncertainty. A correlation between estimated uncertainty (shaded
area) and mean error (gray) indicates the uncertainty estimates are meaningful for estimating errors.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Table 8. Uncertainty quality sensitivity to batch size. A sensitivity analysis to determine how MCBN uncertainty quality varies with
batch size is measured on eight regression datasets using CRPS as the quality measure. Results are measured over 3 random 80-20 splits
of the data with 5 different random seeds each split.

CRPS
64

Batch size

8

16

32

128

256

512

1024

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

-7.1
-34.5
-61.6
-1.4
-10.5
14.5
2.2
15.1

16.6
6.0
-3.0
-4.3
0.8
4.8
1.6
-23.0

11.8
5.0
2.7
0.2
0.0
3.6
0.6
-30.4

7.2
5.1
9.8
2.8
-0.1
2.8
0.6
21.0

2.5
2.9
11.1
2.7
0.0
2.5
0.3
34.4

0.9
1.4
0.8
1.7
0.0
1.6
0.0
-

-
0.6
4.9
0.9
0.2
1.0
0.2
-

-
0.0
-
0.5
0.0
0.5
0.0
-

Table 9. Uncertainty quality sensitivity to batch size. A sensitivity analysis to determine how MCBN uncertainty quality varies with
batch size is measured on eight regression datasets using PLL as the quality measure. Results are measured over 3 random 80-20 splits
of the data with 5 different random seeds each split.

Batch size

8

16

32

128

256

512

1024

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

13.9
-113.3
-64.4
-4.9
-135.0
44.9
2.2
99.6

-36.7
-528.4
5.2
-5.4
-1.4
15.7
2.0
74.9

10.0
-10.0
-0.2
-3.1
-1.0
4.6
0.0
76.8

PLL
64

7.9
2.9
-9.6
1.6
-1.1
2.9
0.5
48.5

3.7
0.0
-14.5
2.3
-0.4
2.8
0.6
44.9

1.5
1.4
1.4
1.5
0.1
2.2
0.4
-

-
0.2
10.4
0.7
-0.1
1.2
0.0
-

-
0.0
-
0.4
0.4
0.6
0.0
-

Table 10. Uncertainty quality sensitivity to n.o. stochastic forward passes. A sensitivity analysis to determine how MCBN uncer-
tainty quality varies with the n.o. stochastic forward passes measured on eight regression datasets using CRPS as the quality measure.
Results are measured over 3 random 80-20 splits of the data with 5 different random seeds each split.

Forward passes

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

CRPS
100

2.7
2.3
4.2
2.7
0.5
2.7
-0.4
32.2

50

3.2
3.3
7.9
4.2
0.1
2.4
0.6
32.1

250

6.1
3.3
13.2
3.2
0.2
2.3
0.9
32.9

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Table 11. Uncertainty quality sensitivity to n.o. stochastic forward passes. A sensitivity analysis to determine how MCBN uncer-
tainty quality varies with the n.o. stochastic forward passes measured on eight regression datasets using PLL as the quality measure.
Results are measured over 3 random 80-20 splits of the data with 5 different random seeds each split.

Forward passes

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

250

7.8
3.8
15.7
2.5
-0.9
1.8
1.7
38.0

PLL
100

1.9
7.1
-30.5
2.2
0.7
2.0
-0.9
35.9

50

2.6
0.1
-47.3
3.4
-0.9
2.4
1.1
35.5

1.8. Uncertainty in image segmentation

We applied MCBN to an image segmentation task using Bayesian SegNet with the main CamVid and PASCAL-VOC
models in (Kendall et al., 2015). Here, we provide more image from Pascal VOC dataset in Figure 6.

1.9. Batch normalization statistics

In Figure 7 and Figure 8, we provide statistics on the batch normalization parameters used for training. The plots show the
distribution of BN mean and BN variance over different mini-batches of an actual training of Yacht dataset for one unit in
the ﬁrst hidden layer and the second hidden layer. Data is provided for different epochs and for different batch sizes.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 6. Uncertainty in image segmentation. Results applying MCBN to Bayesian SegNet (Kendall et al., 2015) on images from
PASCAL-VOC (right). Left: original. Middle: the Bayesian estimated segmentation. Right: estimated uncertainty using MCBN for all
classes. Mini-batches of size 36 were used for PASCAL-VOC on images of size 224x224. 20 inferences were conducted to estimate the
mean and variance of MCBN.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 7. The distribution of means of mini-batches during training of one of our datasets. The distribution closely follows our analyt-
ically approximated Gaussian distribution. The data is collected for one unit of each layer and is provided for different epochs and for
different batch sizes.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 8. The distribution of standard deviation of mini-batches during training of one of our datasets. The distribution closely follows
our analytically approximated Gaussian distribution. The data is collected for one unit of each layer and is provided for different epochs
and for different batch sizes.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

References

Bui, T. D., Hern´andez-Lobato, D., Li, Y., Hern´andez-Lobato, J. M., and Turner, R. E. Deep Gaussian Processes for

Regression using Approximate Expectation Propagation. In ICML, 2016.

Chen, X., Kundu, K., Zhang, Z., Ma, H., Fidler, S., and Urtasun, R. Monocular 3d object detection for autonomous driving.

In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2147–2156, 2016.

Djuric, U., Zadeh, G., Aldape, K., and Diamandis, P. Precision histology: how deep learning is poised to revitalize

histomorphology for personalized cancer care. npj Precision Oncology, 1(1):22, 2017.

Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau, H. M., and Thrun, S. Dermatologist-level classiﬁcation

of skin cancer with deep neural networks. Nature, Feb 2017.

Gal, Y. Uncertainty in Deep Learning. PhD thesis, University of Cambridge, 2016.

Gal, Y. and Ghahramani, Z. Dropout as a Bayesian Approximation : Representing Model Uncertainty in Deep Learning.

ICML, 48:1–10, 2015.

data/kin/desc.html.

Ghahramani, Z. Delve Datasets. University of Toronto, 1996. URL http://www.cs.toronto.edu/{˜}delve/

Ghahramani, Z. Probabilistic machine learning and artiﬁcial intelligence. Nature, 521(7553):452–459, May 2015.

Gneiting, T. and Raftery, A. E. Strictly Proper Scoring Rules, Prediction, and Estimation. Journal of the American

Statistical Association, 102(477):359–378, 2007.

Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples.

arXiv preprint

arXiv:1412.6572, 2014.

Graves, A. Practical Variational Inference for Neural Networks. NIPS, 2011.

Hern´andez-Lobato, J. M. and Adams, R. Probabilistic backpropagation for scalable learning of bayesian neural networks.

In International Conference on Machine Learning, pp. 1861–1869, 2015.

Hinton, G. E. and Van Camp, D. Keeping the neural networks simple by minimizing the description length of the weights.

In Proceedings of the sixth annual conference on Computational learning theory, pp. 5–13. ACM, 1993.

Ioffe, S. Batch renormalization: Towards reducing minibatch dependence in batch-normalized models. CoRR,

abs/1702.03275, 2017. URL http://arxiv.org/abs/1702.03275.

Ioffe, S. and Szegedy, C. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.

Arxiv, 2015. URL http://arxiv.org/abs/1502.03167.

Karpathy, A. Convnetjs demo: toy 1d regression, 2015. URL http://cs.stanford.edu/people/karpathy/

convnetjs/demo/regression.html.

Kendall, A., Badrinarayanan, V., and Cipolla, R. Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-
Decoder Architectures for Scene Understanding. CoRR, abs/1511.0, 2015. URL http://arxiv.org/abs/1511.
02680.

Kingma, D. P. and Welling, M. Auto-Encoding Variational Bayes. In ICLR, 2014.

Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. 2009.

Krueger, D., Huang, C.-W., Islam, R., Turner, R., Lacoste, A., and Courville, A. Bayesian hypernetworks. arXiv preprint

arXiv:1710.04759, 2017.

Lehmann, E. L. Elements of Large-Sample Theory. Springer Verlag, New York, 1999. ISBN 0387985956.

Li, Y. and Gal, Y. Dropout Inference in Bayesian Neural Networks with Alpha-divergences. arXiv, 2017.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Louizos, C. and Welling, M. Multiplicative normalizing ﬂows for variational Bayesian neural networks. In Precup, D. and
Teh, Y. W. (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings
of Machine Learning Research, pp. 2218–2227, International Convention Centre, Sydney, Australia, 06–11 Aug 2017.
PMLR. URL http://proceedings.mlr.press/v70/louizos17a.html.

MacKay, D. J. A practical bayesian framework for backpropagation networks. Neural computation, 4(3):448–472, 1992.

Neal, R. M. Bayesian Learning for Neural Networks. PhD thesis, University of Toronto, 1995.

Neal, R. M. Bayesian learning for neural networks, volume 118. Springer Science & Business Media, 2012.

Selten, R. Axiomatic characterization of the quadratic scoring rule. Experimental Economics, 1(1):43–62, 1998.

Shen, L. End-to-end training for whole image breast cancer diagnosis using an all convolutional design. arXiv preprint

University of California, I. UC Irvine Machine Learning Repository, 2017. URL https://archive.ics.uci.

arXiv:1708.09427, 2017.

edu/ml/index.html.

Wang, S. I. and Manning, C. D. Fast dropout training. Proceedings of the 30th International Conference on Machine Learn-
ing, 28:118–126, 2013. URL http://machinelearning.wustl.edu/mlpapers/papers/wang13a.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

8
1
0
2
 
l
u
J
 
6
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
5
5
4
6
0
.
2
0
8
1
:
v
i
X
r
a

Mattias Teye 1 2 * Hossein Azizpour 1 * Kevin Smith 1 3

Abstract
We show that training a deep network using batch
normalization is equivalent to approximate infer-
ence in Bayesian models. We further demon-
strate that this ﬁnding allows us to make mean-
ingful estimates of the model uncertainty us-
ing conventional architectures, without modiﬁ-
cations to the network or the training proce-
dure. Our approach is thoroughly validated by
measuring the quality of uncertainty in a series
of empirical experiments on different tasks.
It
outperforms baselines with strong statistical sig-
niﬁcance, and displays competitive performance
with recent Bayesian approaches.

1. Introduction

Deep learning has dramatically advanced the state of the
art in a number of domains. Despite their unprecedented
discriminative power, deep networks are prone to make
mistakes. Nevertheless, they can already be found in set-
tings where errors carry serious repercussions such as au-
tonomous vehicles (Chen et al., 2016) and high frequency
trading. We can soon expect automated systems to screen
for various types of cancer (Esteva et al., 2017; Shen, 2017)
and diagnose biopsies (Djuric et al., 2017). As autonomous
systems based on deep learning are increasingly deployed
in settings with the potential to cause physical or economic
harm, we need to develop a better understanding of when
we can be conﬁdent in the estimates produced by deep net-
works, and when we should be less certain.

Standard deep learning techniques used for supervised
learning lack methods to account for uncertainty in the
model. This can be problematic when the network en-
counters conditions it was not exposed to during training,

* Co-ﬁrst authorship 1School of Electrical Engineering and
Computer Science, KTH Royal Institute of Technology, Stock-
holm, Sweden 2Current address: Electronic Arts, SEED, Stock-
holm, Sweden. This work was carried out at Budbee AB.
3Science for Life Laboratory. Correspondence to: Kevin Smith
<ksmith@kth.se>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

or if the network is confronted with adversarial examples
(Goodfellow et al., 2014). When exposed to data outside
the distribution it was trained on, the network is forced to
extrapolate, which can lead to unpredictable behavior.

If the network can provide information about its uncer-
tainty in addition to its point estimate, disaster may be
avoided. In this work, we focus on estimating such pre-
dictive uncertainties in deep networks (Figure 1).

The Bayesian approach provides a theoretical framework
for modeling uncertainty (Ghahramani, 2015), which has
prompted several attempts to extend neural networks (NN)
into a Bayesian setting. Most notably, Bayesian neural net-
works (BNNs) have been studied since the 1990’s (Neal,
2012), but do not scale well and struggle to compete with
modern deep learning architectures. Recently, (Gal &
Ghahramani, 2015) developed a practical solution to obtain
uncertainty estimates by casting dropout training in con-
ventional deep networks as a Bayesian approximation of a
Gaussian Process (its correspondence to a general approx-
imate Bayesian model was shown in (Gal, 2016)). They
showed that any network trained with dropout is an ap-
proximate Bayesian model, and uncertainty estimates can
be obtained by computing the variance on multiple predic-
tions with different dropout masks.

The inference in this technique, called Monte Carlo
Dropout (MCDO), has an attractive quality: it can be ap-
plied to any pre-trained networks with dropout layers. Un-
certainty estimates come (nearly) for free. However, not all
architectures use dropout, and most modern networks have
adopted other regularization techniques. Batch normaliza-
tion (BN), in particular, has become widespread thanks to
its ability to stabilize learning with improved generalization
(Ioffe & Szegedy, 2015).

An interesting aspect of BN is that the mini-batch statis-
tics used for training each iteration depend on randomly
selected batch members. We exploit this stochasticity and
show that training using batch normalization, like dropout,
can be cast as an approximate Bayesian inference. We
demonstrate how this ﬁnding allows us to make meaning-
ful estimates of the model uncertainty in a technique we
call Monte Carlo Batch Normalization (MCBN) (Figure 1).
The method we propose can be applied to any network us-
ing standard batch normalization.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Gaussian Processes show superior performance in terms of
RMSE and uncertainty quality compared to state-of-the-art
approximate BNNs (Bui et al., 2016)1. Another recent ap-
proach to Bayesian learning, Bayesian hypernetworks, use
a NN to learn a distribution of parameters over another net-
work (Krueger et al., 2017). Multiplicative Normalizing
Flows for variational Bayesian networks (MNF) (Louizos
& Welling, 2017) is a recent model that formulates a pos-
terior dependent on auxiliary variables. MNF achieves a
highly ﬂexible posterior by the application of normalizing
ﬂows to the auxiliary variables.

Although these recent techniques address some of the dif-
ﬁculties with approximate BNNs, they all require modiﬁ-
cations to the architecture or the way networks are trained,
as well as specialized knowledge from practitioners. Re-
cently, (Gal & Ghahramani, 2015) showed that a network
trained with dropout implicitly performs the VI objective.
Therefore any network trained with dropout can be treated
as an approximate Bayesian model by making multiple
predictions through the network while sampling different
dropout masks for each prediction. The mean and variance
of the predictions are used in the estimation of the mean
and variance of the predictive distribution 2.

3. Method

In the following, we introduce Bayesian models and a vari-
ational approximation using Kullback-Leibler (KL) diver-
gence following (Gal, 2016). We continue by showing that
a batch normalized deep network can be seen as an ap-
proximate Bayesian model. Employing theoretical insights
and empirical analysis, we study the induced prior on the
parameters when using batch normalization. Finally, we
describe the procedure for estimating the uncertainty of a
batch normalized network’s output.3

3.1. Bayesian Modeling

We assume a ﬁnite training set D = {(xi, yi)}i=1:N where
each (xi, yi) is a sample-label pair. Using D, we are inter-
ested in learning an inference function fω(x, y) with pa-
rameters ω. In deterministic models, the estimated label ˆy
is obtained as follows:

ˆy = arg max

fω(x, y)

y

In probabilistic models we let fω(x, y) = p(y|x, ω). In
Bayesian modeling, in contrast to ﬁnding a point estimate

1By uncertainty quality, we refer to predictive probability dis-

tributions as measured by PLL and CRPS.

2This technique is referred to as “MC Dropout” in the original

work, though we refer to it here as MCDO.

3While the method applies to FC or Conv layers, the induced

Figure 1. Training a deep network using batch normalization
is equivalent to approximate inference in Bayesian models.
Thus, uncertainty estimates can be obtained from any network
using BN through a simple procedure. At inference, several mini-
batches are constructed by taking random samples to accompany
the query. The mean and variance of the outputs are used to esti-
mate the predictive distribution (MCBN). Here, we show results
on a toy dataset from a network with three hidden layers (30 units
per layer). Training data is depicted as dots. The solid line is the
predictive mean of 500 stochastic forward passes and the shaded
areas represent the model’s uncertainty. The dashed lines depict a
minimal baseline for uncertainty (CUBN), see Section 4.1.

We validate our approach by empirical experiments on a
variety of datasets and tasks, including regression and im-
age classiﬁcation. We measure uncertainty quality relative
to a baseline of ﬁxed uncertainty, and show that MCBN
outperforms the baseline on nearly all datasets with strong
statistical signiﬁcance. We also show that the uncertainty
quality of MCBN is on par with other recent approximate
Bayesian networks.

2. Related Work

Bayesian models provide a natural framework for model-
ing uncertainty, and several approaches have been devel-
oped to adapt NNs to Bayesian reasoning. A common ap-
proach is to place a prior distribution (often a Gaussian)
over each parameter. The resulting model corresponds to
a Gaussian process for inﬁnite parameters (Neal, 1995),
and a Bayesian NN (MacKay, 1992) for a ﬁnite number of
parameters. Inference in BNNs is difﬁcult however (Gal,
2016), so focus has thus shifted to techniques that approx-
imate the posterior, approximate BNNs. Methods based on
variational inference (VI) typically rely on a fully factor-
ized approximate distribution (Kingma & Welling, 2014;
Hinton & Van Camp, 1993), but often do not scale. To alle-
viate these difﬁculties, (Graves, 2011) proposed a model
using sampling methods to estimate a factorized poste-
rior. Probabilistic backpropagation (PBP), estimates a fac-
torized posterior via expectation propagation (Hern´andez-
Lobato & Adams, 2015).

Using several strategies to address scaling issues, Deep

prior from weight decay (Section 3.3) is studied for FC layers.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

of the model parameters, the idea is to estimate an (ap-
proximate) posterior distribution of the model parameters
p(ω|D) to be used for probabilistic prediction:

(cid:90)

p(y|x, D) =

fω(x, y)p(ω|D)dω

The predicted label, ˆy, can then be accordingly obtained by
sampling p(y|x, D) or taking its maxima.

Variational Approximation In approximate Bayesian
modeling, a common approach is to learn a parame-
terized approximating distribution qθ(ω) that minimizes
KL(qθ(ω)||p(ω|D)); the Kullback-Leibler divergence of
the true posterior w.r.t. its approximation. Minimizing this
KL divergence is equivalent to the following minimization
while being free of the data term p(D) 4:

LVA(θ) := −

qθ(ω) ln fω(xi, yi)dω

N
(cid:88)

(cid:90)

i=1

+ KL(qθ(ω)||p(ω))

During optimization, we want to take the derivative of the
expected likelihood w.r.t. the learnable parameters θ. We
use the same MC estimate as in (Gal, 2016) (explained in
Appendix Section 1.1), such that one realized ˆωi is taken
for each sample i 5. Optimizing over mini-batches of size
M , the approximated objective becomes:

ˆLVA(θ) := −

ln f ˆωi (xi, yi) + KL(qθ(ω)||p(ω))

(1)

N
M

M
(cid:88)

i=1

The ﬁrst term is the data likelihood and the second term
is the divergence of the prior w.r.t. the approximated poste-
rior.

3.2. Batch Normalized Deep Nets as Bayesian Modeling

We now describe the optimization procedure of a deep net-
work with batch normalization and draw the resemblance
to the approximate Bayesian modeling in Eq (1).

The inference function of a feed-forward deep network
with L layers can be described as:

where a(.) is an element-wise nonlinearity function and
Wl is the weight vector at layer l. Furthermore, we de-
note the input to layer l as xl with x1 = x and we then set
hl = Wlxl. Parenthesized super-index for matrices (e.g.
W(j)) and vectors (e.g. x(j)) indicates jth row and element
respectively. Super-index u refers to a speciﬁc unit at layer
l, (e.g. Wu = Wl,(j), hu = hl,(j)). 6

Batch Normalization Each layer of a deep network is
constructed by several linear units whose parameters are
the rows of the weight matrix W. Batch normalization is
a unit-wise operation proposed in (Ioffe & Szegedy, 2015)
to standardize the distribution of each unit’s input. For FC
layers, it converts a unit’s input hu in the following way:

ˆhu =

hu − E[hu]
(cid:112)Var[hu]

where the expectations are computed over the training
set during evaluation, and mini-batch during training (in
deep networks, the weight matrices are often optimized us-
ing back-propagated errors calculated on mini-batches of
data)7. Therefore, during training, the estimated mean and
variance on the mini-batch B is used, which we denote by
µB and σB respectively. This makes the inference at train-
ing time for a sample x a stochastic process, varying based
on other samples in the mini-batch.

Loss Function and Optimization Training deep net-
works with mini-batch optimization involves a (regular-
ized) risk minimization with the following form:

LRR(ω) :=

l(ˆyi, yi) + Ω(ω)

1
M

M
(cid:88)

i=1

where the ﬁrst term is the empirical loss on the training
data and the second term is a regularization penalty act-
ing as a prior on model parameters ω.
If the loss l is
cross-entropy for classiﬁcation or sum-of-squares for re-
gression problems (assuming i.i.d. Gaussian noise on la-
bels), the ﬁrst term is equivalent to minimizing the negative
log-likelihood:

fω(x) = WLa(WL−1...a(W2a(W1x))

LRR(ω) := −

ln fω(xi, yi) + Ω(ω)

4Achieved by constructing the Evidence Lower Bound, called
ELBO, and assuming i.i.d. observation noise; details can be found
in Appendix Section 1.1.

5While a MC integration using a single sample is a weak ap-
proximation, in an iterative optimization for θ several samples
will be taken over time.

6For a (softmax) classiﬁcation network, fω(x) is a vector with
fω(x, y) = fω(x)(y), for regression networks with i.i.d. Gaus-
sian noise we have fω(x, y) = N (fω(x), τ −1I).

7It also learns an afﬁne transformation for each unit with pa-
afﬁne = γ(j) ˆx(j) + β(j).

rameters γ and β, omitted for brevity: ˆx(j)

1
M τ

M
(cid:88)

i=1

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

with τ = 1 for classiﬁcation.
In a network
with batch normalization, the model parameters include
{W1:L, γ1:L, β1:L, µ1:L
B }. If we decouple the learn-
able parameters θ = {W1:L, γ1:L, β1:L} from the stochas-
tic parameters ω = {µ1:L
B }, we get the following ob-
jective at each step of the mini-batch optimization:

B , σ1:L

B , σ1:L

LRR(θ) := −

ln f{θ, ˆωi}(xi, yi) + Ω(θ)

(2)

1
M τ

M
(cid:88)

i=1

where ˆωi is the means and variances for sample i’s mini-
batch at a certain training step. Note that while ˆωi formally
needs to be i.i.d. for each training example, a batch normal-
ized network samples the stochastic parameters once per
training step (mini-batch). For a large number of epochs,
however, the distribution of sampled batch members for a
given training example converges to the i.i.d. case.

B , σ1:L

In a batch normalized network, qθ(ω) corresponds to the
joint distribution of the weights, induced by the random-
ness of the normalization parameters µ1:L
B , as im-
plied by the repeated sampling from D during training.
This is an approximation of the true posterior, where we
have restricted the posterior to lie within the domain of
our parametric network and source of randomness. With
that, we can estimate the uncertainty of predictions from
a trained batch normalized network using the inherent
stochasticity of BN (Section 3.4).

3.3. Prior p(ω)

Equivalence between the VA and BN training procedures
requires ∂
∂θ of Eq. (1) and Eq. (2) to be equivalent up to a
scaling factor. This is the case if ∂
∂θ KL(qθ(ω)||p(ω)) =
N τ ∂

∂θ Ω(θ).

To reconcile this condition, one option is to let the prior
p(ω) imply the regularization term Ω(θ). Eq. (1) reveals
that the contribution of KL(qθ(ω)||p(ω)) to the optimiza-
tion objective is inversely scaled with N . For BN, this cor-
responds to a model with a small Ω(θ) when N is large. In
the limit as N → ∞, the optimization objectives of Eq. (1)
and Eq. (2) become identical with no regularization.8

Another option is to let some Ω(θ) imply p(ω).
In Ap-
pendix Section 1.4 we explore this with L2-regularization,
also called weight decay (Ω(θ) = λ (cid:80)
l=1:L ||W l||2). We
ﬁnd that unlike in MCDO (Gal, 2016), some simplifying

8To prove the existence and ﬁnd an expression of
KL(qθ(ω)||p(ω)), in Appendix Section 1.3 we ﬁnd that BN ap-
proximately induces Gaussian distributions over BN units’ means
and standard deviations, centered around the population values
given by D. We assume a factorized distribution and Gaussian
priors, and ﬁnd the corresponding KL(qθ(ω)||p(ω)) components
in Appendix Section 1.4 Eq. (7). These could be used to construct
a custom Ω(θ) for any Gaussian choice of p(ω).

assumptions are necessary to reconcile the VA and BN ob-
jectives with weight decay: no scale and shift applied to
BN layers, uncorrelated units in each layer, BN applied on
all layers, and large N and M . Given these conditions:

p(µu
p(σu

B) = N (µµ,p, σµ,p)
B) = N (µσ,p, σσ,p)

where µµ,p = 0, σµ,p → ∞, µσ,p = 0 and σσ,p → 1

.

2N τ λl

This corresponds to a wide and narrow distribution on BN
units’ means and std. devs respectively, where N accounts
for the narrowness of the prior. Due to its popularity in
deep learning, our experiments in Section 4 are performed
with weight decay.

3.4. Predictive Uncertainty in Batch Normalized Deep

Nets

In the absence of the true posterior, we rely on the approx-
imate posterior to express an approximate predictive distri-
bution:

p∗(y|x, D) :=

fω(x, y)qθ(ω)dω

(cid:90)

Following (Gal, 2016) we estimate the ﬁrst (for regression
and classiﬁcation) and second (for regression) moments of
the predictive distribution empirically (see Appendix Sec-
tion 1.5 for details):

Ep∗ [y] ≈

f ˆωi(x)

1
T

T
(cid:88)

i=1

T
(cid:88)

1
T

i=1
− Ep∗ [y](cid:124)Ep∗ [y]

Covp∗ [y] ≈ τ −1I +

f ˆωi(x)(cid:124)f ˆωi(x)

where each ˆωi corresponds to sampling the net’s stochas-
tic parameters ω = {µ1:L
B , σ1:L
B } the same way as during
training. Sampling ˆωi therefore involves sampling a batch
B from the training set and updating the parameters in the
BN units, just as if we were taking a training step with B.
From a VA perspective, training the network amounted to
minimizing KL(qθ(ω)||p(ω|D)) wrt θ. Sampling ˆωi from
the training set, and keeping the size of B consistent with
the mini-batch size used during training, ensures that qθ(ω)
during inference remains identical to the approximate pos-
terior optimized during training.

The network is trained just as a regular BN network, but
instead of replacing ω = {µ1:L
B } with population
values from D for inference, we update these parameters
stochastically, once for each forward pass.9 Pseudocode
for estimating predictive mean and variance is given in Al-
gorithm 1.

B , σ1:L

9As an alternative to using the training set D to sample ˆωi,

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Algorithm 1 MCBN Algorithm
Input: sample x, number of inferences T , batchsize b
Output: mean prediction ˆy, predictive uncertainty σ2
1: y = {}
2: loop for T iterations
3: B ∼ D // mini batch
ˆω = {µB, σB}
4:
y = y ∪ f ˆω(x)
5:
6: end loop
7: ˆy = E[y]
8: σ2 = Cov[y] + τ −1I // for regression

// mini batch mean and variance

4. Experiments and Results

We assess the uncertainty quality of MCBN quantitatively
and qualitatively. Our quantitative analysis relies on CI-
FAR10 for image classiﬁcation and eight standard regres-
sion datasets, listed in Appendix Table 1. Publicly avail-
able from the UCI Machine Learning Repository (Univer-
sity of California, 2017) and Delve (Ghahramani, 1996),
these datasets have been used to benchmark comparative
models in recent related literature (see (Hern´andez-Lobato
& Adams, 2015), (Gal & Ghahramani, 2015), (Bui et al.,
2016) and (Li & Gal, 2017)). We report results using
standard metrics, and also propose useful upper and lower
bounds to normalize these metrics for an easier interpreta-
tion in Section 4.2.

Our qualitative results include the toy dataset in Figure 1
in the style of (Karpathy, 2015), a new visualization of un-
certainty quality that plots test errors sorted by predicted
variance (Figure 2 and Appendix), and image segmentation
results (Figure 2 and Appendix).

4.1. Metrics

We evaluate uncertainty quality based on two standard met-
rics, described below: Predictive Log Likelihood (PLL)
and Continuous Ranked Probability Score (CRPS). To im-
prove the interpretability of the metrics, we propose to nor-
malize them by upper and lower bounds.

Predictive Log Likelihood (PLL) Predictive Log Like-
lihood is widely accepted as the main uncertainty quality
metric for regression (Hern´andez-Lobato & Adams, 2015;
Gal & Ghahramani, 2015; Bui et al., 2016; Li & Gal, 2017).
A key property of PLL is that it makes no assumptions
about the form of the distribution. The measure is deﬁned
for a probabilistic model fω(x) and a single observation

we could sample from the implied qθ(ω) as modeled in the Ap-
pendix. This would alleviate having to store D for use during
prediction. In our experiments we used D to sample ˆωi however,
and leave the evaluation of the modeled qθ(ω) for future research.

(yi, xi) as:

PLL(fω(x), (yi, xi)) = log p(yi|fω(xi))

where p(yi|fω(xi)) is the model’s predicted PDF evalu-
ated at yi, given the input xi. A more detailed description
is given in the Appendix Section 1.5. The metric is un-
bounded and maximized by a perfect prediction (mode at
yi) with no variance. As the predictive mode moves away
from yi, increasing the variance tends to increase PLL (by
maximizing probability mass at yi). While PLL is an ele-
gant measure, it has been criticized for allowing outliers to
have an overly negative effect on the score (Selten, 1998).

Continuous Ranked Probability Score (CRPS) Con-
tinuous Ranked Probability Score is a measure that takes
the full predicted PDF into account with less sensitivity to
outliers. A prediction with low variance that is slightly off-
set from the true observation will receive a higher score
form CRPS than PLL. In order for CRPS to be analytically
tractable, we need to assume a Gaussian unimodal predic-
tive distribution. CRPS is deﬁned as

CRPS(fω(xi), (yi, xi)) =

(cid:0)F (y) − 1(y ≥ yi)(cid:1)2

dy

(cid:90) ∞

−∞

where F (y) is the predictive CDF, and 1(y ≥ yi) = 1
if y ≥ yi and 0 otherwise (for univariate distributions)
(Gneiting & Raftery, 2007). CRPS is interpreted as the sum
of the squared area between the CDF and 0 where y < yi
and between the CDF and 1 where y ≥ yi. A perfect pre-
diction with no variance yields a CRPS of 0; for all other
cases the value is larger. CRPS has no upper bound.

4.2. Benchmark models and normalized metrics

It is difﬁcult to interpret the quality of uncertainty from
raw PLL and CRPS values. We propose to normalize the
metrics between useful lower and upper bounds. The nor-
malized measures estimate the performance of an uncer-
tainty model between the trivial solution (constant uncer-
tainty) and optimal uncertainty for each prediction. For
the lower bound, we deﬁne a baseline that predicts con-
stant variance regardless of input. The variance is set to
a ﬁxed value that optimizes CRPS on validation data. We
call this model Constant Uncertainty BN (CUBN). It re-
ﬂects our best guess of constant variance on test data –
thus, any improvement in uncertainty quality over CUBN
indicates a sensible estimate of uncertainty. We simi-
larly deﬁne a baseline for dropout, Constant Uncertainty
Dropout (CUDO). The modeling of variance (uncertainty)
by MCBN and CUBN are visualized in Figure 1.

An upper bound on uncertainty performance can also
be deﬁned for a probabilistic model f with respect to
CRPS or PLL. For each observation (yi, xi), a value

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

for the predictive variance Ti can be chosen that max-
imizes PLL or minimizes CRPS10. Using CUBN as a
lower bound and the optimized CRPS score as the up-
per bound, uncertainty estimates can be normalized be-
tween these bounds (1 indicating optimal performance,
and 0 indicating same performance as ﬁxed uncer-
this normalized measure CRPS =
tainty). We call
minT CRPS(f,(yi,xi))−CRPS(fCU ,(yi,xi)) × 100, and the PLL
analogue PLL = PLL(f,(yi,xi))−PLL(fCU ,(yi,xi))
maxT PLL(f,(yi,xi))−PLL(fCU ,(yi,xi)) ×100.

CRPS(f,(yi,xi))−CRPS(fCU ,(yi,xi))

4.3. Test setup

Our evaluation compares MCBN to MCDO (Gal &
Ghahramani, 2015) and MNF (Louizos & Welling, 2017)
using the datasets and metrics described above. Our setup
is similar to (Hern´andez-Lobato & Adams, 2015), which
was also followed by (Gal & Ghahramani, 2015). How-
ever, our comparison implements a different hyperparame-
ter selection, allows for a larger range of dropout rates, and
uses larger networks with two hidden layers.

For the regression task, all models share a similar archi-
tecture: two hidden layers with 50 units each, and ReLU
activations, with the exception of Protein Tertiary Struc-
ture dataset (100 units per hidden layer). Inputs and out-
puts were normalized during training. Results were aver-
aged over ﬁve random splits of 20% test and 80% train-
ing and cross-validation (CV) data. For each split, 5-fold
CV by grid search with a RMSE minimization objective
was used to ﬁnd training hyperparameters and optimal n.o.
epochs, out of a maximum of 2000. For BN-based mod-
els, the hyperparameter grid consisted of a weight decay
factor ranging from 0.1 to 1−15 by a log 10 scale, and a
batch size range from 32 to 1024 by a log 2 scale. For
DO-based models, the hyperparameter grid consisted of
the same weight decay range, and dropout probabilities
in {0.2, 0.1, 0.05, 0.01, 0.005, 0.001}. DO-based models
used a batch size of 32 in all evaluations. For MNF11, the
n.o. epochs was optimized, the batch size was set to 100,
and early stopping test performed each epoch (compared to
every 20th for MCBN, MCDO).

For MCBN and MCDO, the model with optimal training
hyperparameters was used to optimize τ numerically. This
optimization was made in terms of average CV CRPS for
MCBN, CUBN, MCDO, and CUDO respectively.

Estimates for the predictive distribution were obtained by
taking T = 500 stochastic forward passes through the net-
work. For each split, test set evaluation was done 5 times
with different seeds. Implementation was done in Tensor-
Flow with the Adam optimizer and a learning rate of 0.001.

10Ti can be found analytically for PLL, but must be found nu-

merically for CRPS.

11Where we used an adapted version of the authors’ code.

For
the image classiﬁcation test we use CIFAR10
(Krizhevsky & Hinton, 2009) which includes 10 object
classes with 5,000 and 1,000 images in the training and
test sets, respectively. Images are 32x32 RGB format. We
trained a ResNet32 architecture with a batch size of 32,
learning rate of 0.1, weight decay of 0.0002, leaky ReLU
slope of 0.1, and 5 residual units. SGD with momentum
was used as the optimizer.

Code for reproducing our experiments is available at
https://github.com/icml-mcbn/mcbn.

4.4. Test results

The regression experiment comparing uncertainty quality
is summarized in Table 1. We report CRPS and PLL, ex-
pressed as a percentage, which reﬂects how close the model
is to the upper bound, and check to see if the model signif-
icantly exceeds the lower bound using a one sample t-test
(signiﬁcance level is indicated by *’s). Further details are
provided in Appendix Section 1.7.

In Figure 2 (left), we present a novel visualization of un-
certainty quality for regression problems. Data are sorted
by estimated uncertainty in the x-axis. Grey dots show the
errors in model predictions, and the shaded areas show the
model uncertainty. A running mean of the errors appears
as a gray line. If uncertainty estimation is working well,
a correlation should exist between the mean error (gray
line) and uncertainty (shaded area). This indicates that the
uncertainty estimation recognizes samples with larger (or
smaller) potential for predictive errors.

We applied MCBN on the image classiﬁcation task of CI-
FAR10. The baseline in this case is the softmax distribu-
tion using the moving average for BN units. Log likeli-
hood (PLL) is the metric used to compare with the base-
line. The baseline achieves a PLL of -0.32 on the test
set, while MCBN obtains a PLL of -0.28. Table 2 shows
the performance of MCBN when using different number of
stochastic forward passes (the MCBN batchsize is ﬁxed to
the training batch size at 32). PLL improves as the number
of the stochastic passes increases, until it is signiﬁcantly
better than the softmax baseline.

To demonstrate how model uncertainty can be obtained
from an existing network with minimal effort, we applied
MCBN to an image segmentation task using Bayesian Seg-
Net with the main CamVid and PASCAL-VOC models in
(Kendall et al., 2015). We simply ran multiple forward
passes with different mini-batches randomly taken from the
train set. The models obtained from the online model zoo
have BN blocks after each layer. We recalculate mean and
variance for the ﬁrst 2 blocks only and use the training
statistics for the rest of the blocks. Mini-batches of size
10 and 36 were used for CamVid and VOC respectively

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Table 1. Uncertainty quality measured on eight regression datasets. MCBN, MCDO and MNF are compared over 5 random 80-20
splits of the data with 5 different random seeds each split. We report CRPS and PLL, uncertainty metrics CRPS and PLL normalized
to a lower bound of constant variance and upper bound that maximizes the metric expressed as a percentage (described in Section 4.2).
Higher numbers mean the model is closer to the upper bound. We check if the reported values for CRPS and PLL signiﬁcantly exceed
the lower bound using a one sample t-test (signiﬁcance level indicated by *’s). See text for further details.

MCBN

MCDO

MNF

MCBN

CRPS

Dataset

Boston
Concrete
Energy
Kin8nm
Power
Protein
Wine (Red)
Yacht

8.50 ****
3.91 ****
5.75 ****
2.85 ****
0.24 ***
2.66 ****
0.26 **
-56.39 ***

3.06 ****
*
0.93
1.37 ns
1.82 ****
-0.44 ****
0.99 ****
2.00 ****
21.42 ****

5.88 ****
3.13 ***
1.10 ns
0.53 ns
-0.89 ****
0.57 ****
0.94 ****
24.92 ****

PLL
MCDO

5.51 ****
10.92 ****
-14.28

*
-0.26 ns
3.52 ****
6.23 ****
2.91 ****

-41.54 ns

MNF

1.76 ns
-2.16 ns
-33.88 ns
0.42 ns
-0.87 ****
0.52 ****
0.83 ****
46.19 ****

10.49 ****
-36.36 **
10.89 ****
1.68 ***
0.33 **
2.56 ****
0.19

*

45.58 ****

due to memory limits. The results in Figure 2 (right) were
obtained from 20 stochastic forward passes, showing high
uncertainty near object boundaries. The VOC results are
more appealing because of larger mini-batches.

We provide additional experimental results in the Ap-
pendix. Appendix Tables 2 and 3 show the mean CRPS
and PLL values from the regression experiment. Table 4
provides the raw CRPS and PLL scores.
In Table 5 we
provide RMSE results of the MCBN and MCDO networks
in comparison with non-stochastic BN and DO networks.
These results indicate that the procedure of multiple for-
ward passes in MCBN and MCDO show slight improve-
ments in the predictive accuracy compared to their non-
Bayesian counterparts. In Tables 6 and 7, we investigate the
effect of varying batch size while keeping other hyperpa-
rameters ﬁxed. We see that performance deteriorates with
small batch sizes (≤16), a known issue of BN (Ioffe, 2017).
Similarly, results varying the number of stochastic forward
passes T is reported in Tables 8 and 9. While performance
beneﬁts from large T , in some cases T = 50 (i.e. 1/10 of
T in the main evaluation) performs well. Uncertainty-error
plots for all the datasets are provided in the Appendix.

5. Discussion

The results presented in Tables 1-2 and Appendix Tables
2-9 indicate that MCBN generates meaningful uncertainty

Table 2. Uncertainty quality for image classiﬁcation varying
number of stochastic forward passes. Uncertainty quality for
image classiﬁcation measured by PLL. ResNet32 is trained on
CIFAR10 with batch size 32. PLL improves as the sampling in-
creases until it is signiﬁcantly better than the softmax baseline
(-0.32).

estimates that correlate with actual errors in the model’s
In Table 1, we show statistically signiﬁcant
prediction.
improvements over CUBN in the majority of the datasets,
both in terms of CRPS and PLL. The visualizations in
Figure 2 and in the Appendix Figures 2-3 show correla-
tions between the estimated model uncertainty and errors
of the network’s predictions. We perform the same exper-
iments using MCDO and MNF, and ﬁnd that MCBN gen-
erally performs on par with both methods. Looking closer,
MCBN outperforms MCDO and MNF in more cases than
not, measured by CRPS. However, care must be used. The
learned parameters are different, leading to different pre-
dictive means and confounding direct comparison.

The results on the Yacht Hydrodynamics dataset seem con-
tradictory. The CRPS score for MCBN are extremely neg-
ative, while the PLL score is extremely positive. The op-
posite trend is observed for MCDO. To add to the puzzle,
the visualization in Figure 2 depicts an extremely promis-
ing uncertainty estimation that models the predictive errors
with high ﬁdelity. We hypothesize that this strange behav-
ior is due to the small size of the data set, which only con-
tains 60 test samples, or due to the Gaussian assumption
of CRPS. There is also a large variability in the model’s
accuracy on this dataset, which further confounds the mea-
surements for such limited data.

One might criticize the overall quality of uncertainty es-
timates observed in all the models we tested, due to the
magnitude of CRPS and PLL in Table 1. The scores rarely
exceed 10% improvement over the lower bound. However,
we caution that these measures should be taken in context.
The upper bound is very difﬁcult to achieve in practice –
it is optimized for each test sample individually – and the
lower bound is a quite reasonable estimate.

Number of stochastic forward passes
64
2

16

32

8

4

1

PLL

-.36

-.32

-.30

-.29

-.29

-.28

-.28

128

-.28

The study of MCBN sensitivity to batch size revealed that a
certain batch size is required for the best performance, de-
pendent on the data. When doing inference on a GPU, large

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Data sorted by estimated uncertainty

Data sorted by estimated uncertainty

Image segmentation uncertainty (CamVid and PASCAL-VOC)

Figure 2. Uncertainty-error plots (left) and segmentation and uncertainty results applying MCBN to Bayesian SegNet (right).
(left) Errors in predictions (gray dots) sorted by estimated uncertainty on select datasets. The shaded areas show model uncertainty for
MCBN (blue), MNF (violet) and MCDO (red). The light area indicates 95% CI, dark area 50% CI. Gray dots show absolute prediction
errors on the test set, and the gray line depicts a running mean of the errors. The dashed line indicates the optimized constant uncertainty.
A correlation between estimated uncertainty (shaded area) and mean error (gray) indicates the uncertainty estimates are meaningful
for estimating errors. (right) Applying MCBN to Bayesian SegNet (Kendall et al., 2015) on scenes from CamVid (3rd column) and
PASCAL-VOC (4th column). Top: original image. Middle: the Bayesian estimated segmentation. Bottom: estimated uncertainty using
MCBN for all classes. The uncertainty maps for both datasets are reasonable, but qualitatively better for PASCAL-VOC due to the larger
mini-batch size (36) compared to CamVid (10). Smaller batch sizes were used for CamVid due to memory limitations (CamVid images
are 360x480 while VOC are 224x224). See Appendix for complete results.

batch sizes may cause memory issues for cases where the
input is large and the network has a large number of param-
eters, as is common for state-of-the-art image classiﬁcation
networks. However, there are various workarounds to this
problem. One can store BN statistics, instead of batches, to
reduce memory issues. Furthermore, we can use the Gaus-
sian estimate of the BN statistics as discussed previously,
which makes memory and computation extremely efﬁcient.

6. Conclusion

In this work, we have shown that training a deep network
using batch normalization is equivalent to approximate in-
ference in Bayesian models. We show evidence that the
uncertainty estimates from MCBN correlate with actual er-
rors in the model’s prediction, and are useful for practical

tasks such as regression, image classiﬁcation, and image
segmentation. Our experiments show that MCBN yields
a signiﬁcant improvement over the optimized constant un-
certainty baseline, on par with MCDO and MNF. Our eval-
uation also suggests new normalized metrics based on use-
ful upper and lower bounds, and a new visualization which
provides an intuitive explanation of uncertainty quality.

Finally, it should be noted that over the past few years,
batch normalization has become an integral part of most –
if not all – cutting edge deep networks. We have shown that
it is possible to obtain meaningful uncertainty estimates
from existing models without modifying the network or the
training procedure. With a few lines of code, robust uncer-
tainty estimates can be obtained by computing the variance
of multiple stochastic forward passes through an existing
network.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

References

Bui, T. D., Hern´andez-Lobato, D., Li, Y., Hern´andez-
Lobato, J. M., and Turner, R. E. Deep Gaussian Pro-
cesses for Regression using Approximate Expectation
Propagation. In ICML, 2016.

Chen, X., Kundu, K., Zhang, Z., Ma, H., Fidler, S., and Ur-
tasun, R. Monocular 3d object detection for autonomous
driving. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 2147–2156,
2016.

Djuric, U., Zadeh, G., Aldape, K., and Diamandis, P. Preci-
sion histology: how deep learning is poised to revitalize
histomorphology for personalized cancer care. npj Pre-
cision Oncology, 1(1):22, 2017.

Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M.,
Blau, H. M., and Thrun, S. Dermatologist-level classiﬁ-
cation of skin cancer with deep neural networks. Nature,
Feb 2017.

Gal, Y. Uncertainty in Deep Learning. PhD thesis, Univer-

sity of Cambridge, 2016.

Gal, Y. and Ghahramani, Z. Dropout as a Bayesian Ap-
proximation : Representing Model Uncertainty in Deep
Learning. ICML, 48:1–10, 2015.

Ghahramani, Z. Delve Datasets. University of Toronto,
URL http://www.cs.toronto.edu/

1996.
{˜}delve/data/kin/desc.html.

Ghahramani, Z. Probabilistic machine learning and ar-
tiﬁcial intelligence. Nature, 521(7553):452–459, May
2015.

Gneiting, T. and Raftery, A. E. Strictly Proper Scoring
Rules, Prediction, and Estimation. Journal of the Amer-
ican Statistical Association, 102(477):359–378, 2007.

Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain-
ing and harnessing adversarial examples. arXiv preprint
arXiv:1412.6572, 2014.

Graves, A. Practical Variational Inference for Neural Net-

works. NIPS, 2011.

Hern´andez-Lobato, J. M. and Adams, R. Probabilistic
backpropagation for scalable learning of bayesian neu-
ral networks. In International Conference on Machine
Learning, pp. 1861–1869, 2015.

Hinton, G. E. and Van Camp, D. Keeping the neural net-
works simple by minimizing the description length of
the weights. In Proceedings of the sixth annual confer-
ence on Computational learning theory, pp. 5–13. ACM,
1993.

Ioffe, S. Batch renormalization: Towards reducing mini-
batch dependence in batch-normalized models. CoRR,
abs/1702.03275, 2017. URL http://arxiv.org/
abs/1702.03275.

Ioffe, S. and Szegedy, C. Batch Normalization: Accelerat-
ing Deep Network Training by Reducing Internal Co-
variate Shift. Arxiv, 2015. URL http://arxiv.
org/abs/1502.03167.

Karpathy, A. Convnetjs demo:

toy 1d regression, 2015.
http://cs.stanford.edu/people/

URL
karpathy/convnetjs/demo/regression.
html.

Kendall, A., Badrinarayanan, V., and Cipolla, R. Bayesian
SegNet: Model Uncertainty in Deep Convolutional
Encoder-Decoder Architectures for Scene Understand-
ing. CoRR, abs/1511.0, 2015. URL http://arxiv.
org/abs/1511.02680.

Kingma, D. P. and Welling, M. Auto-Encoding Variational

Bayes. In ICLR, 2014.

Krizhevsky, A. and Hinton, G. Learning multiple layers of

features from tiny images. 2009.

Krueger, D., Huang, C.-W., Islam, R., Turner, R., Lacoste,
A., and Courville, A. Bayesian hypernetworks. arXiv
preprint arXiv:1710.04759, 2017.

Lehmann, E. L.

Elements of Large-Sample Theory.

Springer Verlag, New York, 1999. ISBN 0387985956.

Li, Y. and Gal, Y. Dropout Inference in Bayesian Neural

Networks with Alpha-divergences. arXiv, 2017.

Louizos, C. and Welling, M. Multiplicative normal-
izing ﬂows for variational Bayesian neural networks.
In Precup, D. and Teh, Y. W. (eds.), Proceedings of
the 34th International Conference on Machine Learn-
ing, volume 70 of Proceedings of Machine Learn-
ing Research, pp. 2218–2227, International Convention
Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.
URL http://proceedings.mlr.press/v70/
louizos17a.html.

MacKay, D. J. A practical bayesian framework for back-
propagation networks. Neural computation, 4(3):448–
472, 1992.

Neal, R. M. Bayesian Learning for Neural Networks. PhD

thesis, University of Toronto, 1995.

Neal, R. M. Bayesian learning for neural networks, volume

118. Springer Science & Business Media, 2012.

Selten, R. Axiomatic characterization of the quadratic scor-
ing rule. Experimental Economics, 1(1):43–62, 1998.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Shen, L. End-to-end training for whole image breast can-
cer diagnosis using an all convolutional design. arXiv
preprint arXiv:1708.09427, 2017.

University of California, I. UC Irvine Machine Learning
Repository, 2017. URL https://archive.ics.
uci.edu/ml/index.html.

Wang, S. I. and Manning, C. D.
Proceedings of

Fast dropout train-
the 30th International Con-
ing.
ference on Machine Learning, 28:118–126, 2013.
URL http://machinelearning.wustl.edu/
mlpapers/papers/wang13a.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

1. Appendix

1.1. Variational Approximation

Assume we were to come up with a family of distributions parameterized by θ in order to approximate the posterior, qθ(ω).
Our goal is to set θ such that qθ(ω) is as similar to the true posterior p(ω|D) as possible.

For clarity, qθ(ω) is a distribution over stochastic parameters ω that is determined by a set of learnable parameters θ and
some source of randomness. The approximation is therefore limited by our choice of parametric function qθ(ω) as well
as the randomness.12 Given ω and an input x, an output distribution p(y|x, ω) = p(y|fω(x)) = fω(x, y) is induced by
observation noise (the conditionality of which is omitted for brevity).

One strategy for optimizing θ is to minimize KL(qθ(ω)||p(ω|D)), the KL divergence of p(ω|D) w.r.t. qθ(ω). Minimizing
KL(qθ(ω)||p(ω|D)) is equivalent to maximizing the ELBO:

(cid:90)

ω

qθ(ω) ln p(Y|X, ω)dω − KL(qθ(ω)||p(ω))

Assuming i.i.d. observation noise, this is equivalent to minimizing:

LVA(θ) := −

qθ(ω) ln p(yi|fω(xi))dω + KL(qθ(ω)||p(ω))

N
(cid:88)

(cid:90)

n=1

Instead of making the optimization on the full training set, we can use a subsampling (yielding an unbiased estimate of
LVA(θ)) for iterative optimization (as in mini-batch optimization):

ˆLVA(θ) := −

N
M

(cid:90)

(cid:88)

i∈B

ω

qθ(ω) ln p(yi|fω(xi))dω + KL(qθ(ω)||p(ω))

During optimization, we want to take the derivative of the expected log likelihood w.r.t. the learnable parameters θ. (Gal,
2016) provides an intuitive interpretation of a MC estimate for NNs trained with a SRT (equivalent to the reparametrisation
trick in (Kingma & Welling, 2014)), and this interpretation is followed here. We let an auxillary variable (cid:15) represent the
stochasticity during training such that ω = g(θ, (cid:15)). The function g and the distribution of (cid:15) are such that p(g(θ, (cid:15))) =
qθ(ω).13 Assuming qθ(ω) can be written (cid:82)
(cid:15) qθ(ω|(cid:15))p((cid:15))d(cid:15) where qθ(ω|(cid:15)) = δ(ω − g(θ, (cid:15))), this auxiliary variable yields
the estimate (full proof in (Gal, 2016)):

ˆLVA(θ) = −

N
M

(cid:90)

(cid:88)

i∈B

(cid:15)

p((cid:15)) ln p(yi|fg(θ,(cid:15))(xi))d(cid:15) + KL(qθ(ω)||p(ω))

where taking a single sample MC estimate of the integral yields the optimization objective in Eq. 1.

12In an approx. Bayesian view of a NN, qθ(ω) would correspond to the distribution of weights in the network given by some SRT.
13In a NN trained with BN, it is easy to see that g exists if we let (cid:15) represent a mini-batch selection from the training data, since all

BN units’ means and variances are completely determined by (cid:15) and θ.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

1.2. KL Divergence of factorized Gaussians

If qθ(ω) and p(ω) factorize over all stochastic parameters:

KL(qθ(ω)||p(ω)) = −

(cid:2)qθ(ωi)(cid:3) ln

(cid:90)

(cid:90)

(cid:89)

i
(cid:89)

i

(cid:90)

ω

ω

(cid:104)

= −

(cid:2)qθ(ωi)(cid:3) (cid:88)

(cid:88)

=

−

(cid:89)

(cid:2)qθ(ωi)(cid:3) ln

dω

(cid:81)
(cid:81)

i p(ωi)
i qθ(ωi)
(cid:20)

ln

p(ωi)
qθ(ωi)

i

(cid:21) (cid:89)

dωi

i
(cid:89)

i

(cid:105)

dωi

ωi(cid:54)=j

i(cid:54)=j

p(ωj)
qθ(ωj)
(cid:90)

dωj

qθ(ωj) ln

p(ωj)
qθ(ωj)

(cid:89)

qθ(ωi)dωi

(cid:105)

(cid:88)

(cid:104)

(cid:90)

ω

i

ωj

−

(cid:90)

j

j

i

(cid:88)

i
(cid:88)

=

=

=

−

qθ(ωi) ln

ωi

p(ωi)
qθ(ωi)

dωi

KL(qθ(ωi)||p(ωi))

(3)

such that KL(qθ(ω)||p(ω)) is the sum of the KL divergence terms for the individual stochastic parameters ωi. If the
factorized distributions are Gaussians, where qθ(ωi) = N (µq, σ2

q ) and p(ωi) = N (µp, σ2

p) we get:

KL(qθ(ωi)||p(ωi)) =

qθ(ωi) ln

dωi

qθ(ωi)
p(ωi)
(cid:90)

ωi

= − H(qθ(ωi)) −

qθ(ωi) ln p(ωi)dωi

= −

(1 + ln(2πσ2

q ))

(cid:90)

ωi

1
2
(cid:90)

ωi
1
2
1
2

σp
σq

−

qθ(ωi) ln

1
p)1/2
(2πσ2

(cid:110)

exp

−

(ωi − µp)2
2σ2
p

(cid:111)

dωi

(4)

= −

(1 + ln(2πσ2

+

ln(2πσ2

p) +

q ))
Eq[ω2

i ] − 2µpEq[ωi] + µ2
p

2σ2
p

= ln

+

q + (µq − µp)2
σ2
2σ2
p

−

1
2

for each KL divergence term. Here H(qθ(ωi)) = 1

2 (1 + ln(2πσ2

q )) is the differential entropy of qθ(ωi).

1.3. Distribution of µu

B, σu
B

Here we approximate the distribution of mean and standard deviation of a mini-batch, separately to two Gaussians – This
has also been empirically veriﬁed, see Figure 3 for 2 sample plots and the appendix section 1.9 for more. For the mean we
get:

where xm are the examples in the sampled batch. We will assume these are sampled i.i.d.14. Samples of the random
variable W(j)xm are then i.i.d.. Then by central limit theorem (CLT) the following holds for sufﬁciently large M (often
≥ 30):

µB =

ΣM

m=1W(j)xm
M

µB ∼ N (µ,

σ2
M

)

14Although in practice with deep learning, mini-batches are sampled without replacement, stochastic gradient descent samples with

replacement in its standard form.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 3. Batch statistics used to train the network are normal. A one-sample Kolmogorov-Smirnov test checks that µB and σB come
from a standard normal distribution. More examples are available in Appendix 1.9.

For standard deviation:

(cid:114)

σB =

ΣM

m=1(W(j)xm − µB)2
M

√

M (σB − σ) =

M

√

(cid:114)

(cid:16)

ΣM

m=1(W(j)xm − µB)2
M

√

(cid:17)

−

σ2

We want to rewrite
x = ΣM

m=1(W(j)xm−µB)2
M

:

(cid:113) ΣM

m=1(W(j)xm−µB)2
M

. We take a Taylor expansion of f (x) =

x around a = σ2. With

√

Then

so

√

√

x =

σ2 +

1
√
σ2
2

(x − σ2) + O[(x − σ2)2]

√

M (σB − σ) =

M

(cid:32)

√

1
√
σ2
2

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

(cid:34)

O

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

+

− σ2(cid:17)
− σ2(cid:17)2(cid:35)(cid:33)

√

M
2σ

=

=

1
√

2σ

M

+

(cid:16) 1
m=1(W(j)xm − µB)2 − σ2(cid:17)
ΣM
M
(cid:34)√
m=1(W(j)xm − µB)2
M
m=1(W(j)xm − µB)2 − M σ2(cid:17)
ΣM

(cid:16) ΣM

M

O

(cid:16)

+

− σ2(cid:17)2(cid:35)

(cid:34)√

O

M

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

− σ2(cid:17)2(cid:35)

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

consider ΣM

m=1(W(j)xm − µB)2. We know that E[W(j)xm] = µ and write

then

ΣM
=ΣM
=ΣM
=ΣM
=ΣM
=ΣM

m=1(W(j)xm − µB)2
m=1((W(j)xm − µ) − (µB − µ))2
m=1((W(j)xm − µ)2 + (µB − µ)2 − 2(W(j)xm − µ)(µB − µ))
m=1(W(j)xm − µ)2 + M (µB − µ)2 − 2(µB − µ)ΣM
m=1(W(j)xm − µ)2 − M (µB − µ)2
m=1((W(j)xm − µ)2 − (µB − µ)2)

m=1(W(j)xm − µ)

√

M (σB − σ) =

1
√

(cid:16)

m=1((W(j)xm − µ)2 − (µB − µ)2) − M σ2(cid:17)
ΣM

+

2σ

M

(cid:34)√

O

M

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

− σ2(cid:17)2(cid:35)

=

(cid:16)

1
√

2σ

M

ΣM

m=1(W(j)xm − µ)2 − ΣM

(cid:34)√

O

M

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

m=1(µB − µ)2 − M σ2(cid:17)
+
− σ2(cid:17)2(cid:35)

=

(cid:16)

1
√

2σ

M

ΣM

m=1((W(j)xm − µ)2 − σ2) − ΣM

(cid:34)√

O

M

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

m=1(µB − µ)2(cid:17)
− σ2(cid:17)2(cid:35)

+

=

1
√

2σ

−

M
1
√

M

2σ
(cid:34)√

+ O

M

ΣM

m=1((W(j)xm − µ)2 − σ2)

ΣM

m=1(µB − µ)2

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

− σ2(cid:17)2(cid:35)

ΣM

m=1((W(j)xm − µ)2 − σ2)
(cid:125)

(cid:123)(cid:122)
term A

=

2σ
(cid:124)

−

1
√

M

√

M
2σ

(cid:124)

(µB − µ)2
(cid:123)(cid:122)
(cid:125)
term B

(cid:34)√

(cid:16) ΣM

+ O

M

(cid:124)

m=1(W(j)xm − µB)2
M
(cid:123)(cid:122)
term C

− σ2(cid:17)2(cid:35)

(cid:125)

We go through each term in turn

Term A
We have

Term A =

ΣM

m=1((W(j)xm − µ)2 − σ2)

1
√

2σ

M

m=1(W(j)xm − µ)2 is the sum of M RVs (W(j)xm − µ)2. Note that since E[W(j)xm] = µ it holds that
where ΣM
E[(W(j)xm − µ)2] = σ2. Since (W(j)xm − µ)2 is sampled approximately iid (by assumptions above), for large enough

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

M by CLT it holds approximately that

ΣM

m=1(W(j)xm − µ)2 ∼ N (M σ2, M Var((W(j)xm − µ)2))

Var((W(j)xm − µ)2) = E[(W(j)xm − µ)2∗2] − E[(W(j)xm − µ)2]2
= E[(W(j)xm − µ)4] − σ4

ΣM

m=1((W(j)xm − µ)2 − σ2) ∼ N (0, M ∗ E[(W(j)xm − µ)4] − M σ4)

Term A ∼ N (0,

E[(W(j)xm − µ)4] − σ4
4σ2

)

Term B =

(µB − µ)2 =

M (µB − µ)(µB − µ)

√

M
2σ

√

1
2σ

Consider (µB − µ). As µB

p
−→ µ when M → ∞ we have µB − µ

p
−→ 0. We also have

√

M (µB − µ) =

ΣM

m=1W(j)xm
M

√

√

−

M µ

which by CLT is approximately Gaussian for large M . We can then make use of the Cramer-Slutzky Theorem, which
p
states that if (Xn)n≥1 and (Yn)n≥1 are two sequences such that Xn
−→ a as n → ∞ where a is a constant,
then as n → ∞, it holds that Xn ∗ Yn

d−→ X ∗ a. Thus, Term B is approximately 0 for large M.

d−→ X and Yn

(cid:34)√

Term C = O

M

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

− σ2(cid:17)2(cid:35)

Since E[(W(j)xm − µ)2] = σ2 we can make the same use of Cramer-Slutzky as for Term B, such that Term C is approxi-
mately 0 for large M.

Finalizing the distribution
We have approximately

√

M (σB − σ) ∼ N (0,

E[(W(j)xm − µ)4] − σ4
4σ2

)

σB ∼ N (σ,

E[(W(j)xm − µ)4] − σ4
4σ2M

)

Here we make use of the stochasticity from BN modeled in the Appendix section 1.3, to evaluate the implied prior on the
stochastic variables for a BN network. Speciﬁcally, we consider a BN network with fully connected layers and BN applied
to each layer, trained with L2-regularization (weight decay). In the following, we make use of the simplifying assumptions
of no scale and shift tranformations, BN applied to each layer, and independent input units to each layer.

where

Then

so

Term B
We have

Term C
We have

so

1.4. Prior

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Equivalence between the objectives of Eq. (1) and (2) requires:

(5)

(6)

(7)

∂
∂θk

KL(qθ(ω)||p(ω)) = N τ

Ω(θ)

∂
∂θk

∂
∂θk

L
(cid:88)

l=1

= N τ

λl||Wl||2

µu
B

∝
∼ N (µu,

(σu)2
M

),

σu
B

∝
∼ N (σu,

E[(W(u)x − µu)4] − (σu)4
4(σu)2M

)

KL(qθ(ω)||p(ω)) =

KL(qθ(ωi)||p(ωi))

(cid:88)

i

where θk ∈ θ, and θ is the set of weights in the network. To proceed with the LHS of Eq. (5) we ﬁrst need to ﬁnd the
approximate posterior qθ(ω) that batch normalization induces. As shown in Appendix 1.3, with some weak assumptions
and approximations the Central Limit Theorem (CLT) yields Gaussian distributions of the stochastic variables µu
B, for
large enough M . For any BN unit u:

B, σu

where µu and σu are population-level moments (i.e. moments over D).

We assume that qθ(ω) and p(ω) factorize over all stochastic variables.15 We use i as an index of the set of stochastic
variables. As shown in Eq. (3) in Appendix 1.2, the factorized distributions yield:

Note that each BN unit produces two KL(qθ(ωi)||p(ωi)) terms: one for ωi = µu
terms for one particular BN unit u, and drop the index i for brevity. We use a Gaussian prior p(ωi) = N (µp, σ2
consistency, use the notation qθ(ωi) = N (µq, σ2

q ). As shown in Eq. (4) in Appendix 1.2:

B and one for ωi = σu

B. We consider these
p) and, for

Since θk changes during training, a prior cannot depend on θk so ∂
∂θk

(µp) = ∂
∂θk

(σp) = 0. Letting (·)(cid:48) denote ∂
∂θk

(·):

KL(qθ(ωi)||p(ωi)) = ln

+

σp
σq

q + (µq − µp)2
σ2
2σ2
p

−

1
2

∂
∂θk

KL(qθ(ωi)||p(ωi)) =

σqσ(cid:48)

q + µqµ(cid:48)
σ2
p

q − µpµ(cid:48)
q

−

σ(cid:48)
q
σq

We need not consider θk past a previous layer’s BN, since a normalization step is performed before scale and shift. In the
general case with a given Gaussian p(ω), Eq. 7 evaluated on all BN units’ means and standard deviations w.r.t. all θk up to
a previous layer’s BN, would yield an expression for a custom N τ ∂
Ω(θ) that could be used for an exact VI treatment of
∂θk
BN.

In our reconciliation of weight decay however, given our assumptions of no scale and shift and BN applied to each layer,
we need only consider the weights in the same layer as the BN unit. This means that the stochastic variables in layer l are
only affected by weights in θk ∈ W l (i.e. not the scale and shift variables operating on the input to the layer). We denote
a weight connecting the k:th input unit to the u:th BN unit by W(u,k). For such weights, we need to derive µ(cid:48)
q, for
two cases: ωi = µu
B by
µσ,q and σσ,q. Using the distributions modeled in Eq. 6:

q and σ(cid:48)
B by µµ,q and σµ,q, and for σu

B. We denote the priors of the mean and std. dev for µu

B and ωi = σu

15The empirical distributions have been numerically checked to be linearly independent and the joint distribution is close to a bi-variate

Gaussian.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Case 1: ωi = µu
B

= W(u) ¯x

µµ,q =

µ(cid:48)

µ,q =

W(u)x
N

xk
N

= ¯xk

(cid:88)

x∈D
(cid:88)

x∈D

(cid:114)

σµ,q =

(σu)2
M

=

σ(cid:48)
µ,q =

σ−1
q

1
2

(cid:88)

x∈D

(cid:115)

(cid:80)

x∈D(W(u)x − µq)2
N M

2(W(u)x − µq)(xk − ¯xk)
N M

= σ−1
q

(cid:18) K
(cid:88)

i=1

W(u,i)Cov(xi, xk)

M −1

(cid:19)

where there are K input units to the layer.

Case 2: ωi = σu
B

(cid:115)

(cid:80)

µσ,q =

x∈D(W(u)x − µq)2
N
(cid:18) K
(cid:88)

= σµ,qM

1
2

(cid:19)

σ,q = σ−1
µ(cid:48)

µ,qM − 1

2

W(u,i)Cov(xi, xk))

i=1
E[(W(u)x − µu)4] − (σu)4
4(σu)2M

σσ,q =

σ(cid:48)
σ,q =

E[(W(u)x − µu)4](cid:48)σu − 2(σu)4(σu)(cid:48) − 2(σu)(cid:48)E[(W(u)x − µu)4]
4(σu)3M

Combining these results with Eq. 7 we ﬁnd that taking KL(qθ(ωi)||p(ωi)) for the mean and variance of a single BN unit u
wrt the weight from input unit k:

∂

−

σ(cid:48)
µ,q
σµ,q
σ(cid:48)
σ,q
σσ,q

∂

∂W(u,k) KL(qθ(µu
µ,q + µµ,qµ(cid:48)
σµ,qσ(cid:48)

µ,q − µµ,pµ(cid:48)

µ,q

B)||p(µu

B)) +

∂W(u,k) KL(qθ(σu

B)||p(σu

B))

σ,q

σσ,qσ(cid:48)

σ,q − µσ,pµ(cid:48)

σ2
µ,p
σ,q + µσ,qµ(cid:48)
σ2
σ,p
O(M −1) + ¯xkW(u) ¯x − µµ,p ¯xk
σ2
O(M −2) + (cid:80)K
i=1 W(u,i)Cov(xi, xk) − µσ,pO(M − 1
2 )

− O(M −1)

µ,p

−

σ2

σ,p

E[(W(u)x − µu)4](cid:48)σu − 2(σu)4(σu)(cid:48) − 2(σu)(cid:48)E[(W(u)x − µu)4]
E[(W(u)x − µu)4]σu − (σu)5

=

+

=

+

−

where we summarize the terms scaled by M with O-notation. We see that if we let M → ∞, µµ,p = 0, σµ,p → ∞,
µσ,p = 0 and σσ,p is small enough, then:

(cid:18)

∂
∂W(u,k)

KL(qθ(µu

B)||p(µu

B)) + KL(qθ(σu

B)||p(σu

B))

≈

(cid:19)

(cid:80)K

i=1 W(u,i)Cov(xi, xk)

σ2

σ,p

such that each BN layer yields the following:

(cid:88)

K
(cid:88)

u

i=1

(cid:18)

∂
∂W(u,i)

KL(qθ(µu

B)||p(µu

B)) + KL(qθ(σu

B)||p(σu

B))

(cid:19)

(cid:88)

≈

u

(cid:80)K

i=1 Wu,i (cid:80)K
σ2

σ,p,u

i2=1 Cov(xi, xi2)

(8)

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

where we denote the prior for the std. dev. of the std. dev. of BN unit u by σσ,p,u. Given our assumptions of no scale and
shift from the previous layer, and independent input features in every layer, Eq. 8 reduces to:

(cid:88)

K
(cid:88)

u

i=1

Wu,i
σ2

σ,p

if the same prior is chosen for each BN unit in the layer. We therefore ﬁnd that Eq. 5 is reconciled by p(µu
and p(σu

is small enough, which is the case if N is large.

), if

B) → N (0,

1
2N τ λl

1
2N τ λl

B) → N (0, ∞)

1.5. predictive distribution properties

This section provides derivations of properties of the predictive distribution p∗(y|x, D) in section 3.4, following (Gal,
2016). We ﬁrst ﬁnd the ﬁrst two modes of the approximate predictive distribution (with the second mode applicable to
regression), then show how to estimate the predictive log likelihood, a measure of uncertainty quality used in the evaluation.

Predictive mean Assuming Gaussian iid noise deﬁned by model precision τ ,
N (y; fω(x), τ −1I):

i.e.

fω(x, y) = p(y|fω(x)) =

fω(x, y)qθ(ω)dω

dy

(cid:17)

N (y; fω(x), τ −1I)qθ(ω)dω

dy

(cid:17)

yN (y; fω(x), τ −1I)dy

qθ(ω)dω

(cid:17)

Ep∗ [y] =

yp∗(y|x, D)dy

(cid:16) (cid:90)

ω
(cid:16) (cid:90)

y

y

ω

(cid:16) (cid:90)

y

(cid:90)

(cid:90)

y
(cid:90)

y
(cid:90)

ω

(cid:90)

ω

=

=

=

=

≈

fω(x)qθ(ω)dω

1
T

T
(cid:88)

i=1

f ˆωi(x)

where we take the MC Integral with T samples of ω for the approximation in the ﬁnal step.

Predictive variance For regression, our goal is to estimate:

Covp∗ [y] = Ep∗ [y(cid:124)y] − Ep∗ [y](cid:124)Ep∗ [y]

We ﬁnd that:

Ep∗ [y(cid:124)y] =

y(cid:124)yp∗(y|x, D)dy

(cid:90)

y
(cid:90)

y
(cid:90)

ω

(cid:90)

ω

(cid:90)

ω

=

=

=

=

(cid:16) (cid:90)

ω

y(cid:124)y

(cid:16) (cid:90)

y

fω(x, y)qθ(ω)dω

dy

(cid:17)

y(cid:124)yfω(x, y)dy

qθ(ω)dω

(cid:17)

(cid:16)

(cid:17)
Covfω(x,y)(y) + Efω(x,y)[y](cid:124)Efω(x,y)[y]

qθ(ω)dω

(cid:16)

(cid:17)
τ −1I + fω(x)(cid:124)fω(x)

qθ(ω)dω

= τ −1I + Eqθ (ω)[fω(x)(cid:124)fω(x)]

≈ τ −1I +

f ˆωi (x)(cid:124)f ˆωi(x)

1
T

T
(cid:88)

i=1

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

where we use MC integration with T samples for the ﬁnal step. The predictive covariance matrix is given by:

Covp∗ [y] ≈ τ −1I +

f ˆωi (x)(cid:124)f ˆωi(x) − Ep∗ [y](cid:124)Ep∗ [y]

1
T

T
(cid:88)

i=1

which is the sum of the variance from observation noise and the sample covariance from T stochastic forward passes
though the network.

The form of p∗ can be approximated by a Gaussian for each output dimension (for regression). We assume bounded
domains for each input dimension, wide layers throughout the network, and a uni-modal distribution of weights centered
at 0. By the Liapounov CLT condition, the ﬁrst layer then receives approximately Gaussian inputs (a proof can be found
in (Lehmann, 1999)). Having sampled µu
B from a mini-batch, each BN unit’s output is bounded. CLT thereby
continues to hold for deeper layers, including fω(x) = WLxL. A similar motivation for a Gaussian approximation of
Dropout has been presented by (Wang & Manning, 2013).

B and σu

Predictive Log Likelihood We use the Predictive Log Likelihood (PLL) as a measure to estimate the model’s uncertainty
quality. For a certain test point (yi, xi), the PLL deﬁnition and approximation can be expressed as:

PLL(fω(x), (yi, xi)) = log p(yi|fω(xi))

= log

fω(xi, yi)p(ω|D)dω

≈ log

fω(xi, yi)qθ(ω)dω

≈ log

p(yi|f ˆωj (xi))

(cid:90)

(cid:90)

1
T

T
(cid:88)

j=1

where ˆωj represents a sampled set of stochastic parameters from the approximate posterior distrubtion qθ(ω) and we take
a MC integration with T samples. For regression, due to the iid Gaussian noise, we can further develop the derivation into
the form we use when sampling:

PLL(fω(x), (yi, xi)) = log

N (yi|f ˆωj (xi), τ −1I)

1
T

T
(cid:88)

j=1

= logsumexpj=1,...,T
1
2

− log T −

log 2π +

1
2

1
2

log τ

(cid:0) −

τ ||yi − f ˆωj (xi)||2(cid:1)

Note that PLL makes no assumption on the form of the approximate predictive distribution.

1.6. Data

To assess the uncertainty quality of the various methods studied we rely on eight standard regression datasets, listed in Table
3. Publicly available from the UCI Machine Learning Repository (University of California, 2017) and Delve (Ghahramani,
1996), these datasets have been used to benchmark comparative models in recent related literature (see (Hern´andez-Lobato
& Adams, 2015), (Gal & Ghahramani, 2015), (Bui et al., 2016) and (Li & Gal, 2017)).

For image classiﬁcation, we applied MCBN using ResNet32 to CIFAR10.

For the image segmentation task, we applied MCBN using Bayesian SegNet on data from CamVid and PASCAL-VOC
using models published in (Kendall et al., 2015).

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Table 3. Regression dataset summary. Properties of the eight regression datasets used to evaluate MCBN. N is the dataset size and Q
is the n.o. input features. Only one target feature was used – we used heating load for the Energy Efﬁciency dataset, which contains
multiple target features.

Dataset name

Boston Housing
Concrete Compressive Strength
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein Tertiary Structure
Wine Quality (Red)
Yacht Hydrodynamics

N

506
1,030
768
8,192
9,568
45,730
1,599
308

Q

13
8
8
8
4
9
11
6

1.7. Extended experimental results

Below, we provide extended results measuring uncertainty quality. In Tables 4 and 5, we provide tables showing the mean
CRPS and PLL values for MCBN and MCDO. These results indicate that MCBN performs on par or better than MCDO
across several datasets. In Table 6 we provide the raw PLL and CRPS results for MCBN and MCDO. In Table 7 we provide
RMSE results of the MCBN and MCDO networks in comparison with non-stochastic BN and DO networks. These results
indicate that the procedure of multiple forward passes in MCBN and MCDO show slight improvements in the accuracy of
the network.

In Figure 4 and Figure 5, we provide a full set of our uncertainty quality visualization plots, where errors in predictions
are sorted by estimated uncertainty. The shaded areas show the model uncertainty and gray dots show absolute prediction
errors on the test set. A gray line depicts a running mean of the errors. The dashed line indicates the optimized constant
uncertainty. In these plots, we can see a correlation between estimated uncertainty (shaded area) and mean error (gray).
This trend indicates that the model uncertainty estimates can recognize samples with larger (or smaller) potential for
predictive errors.

We also conduct a sensitivity analysis to estimate how the uncertainty quality varies with batch size M and the number of
stochastic forward passes T . In tables 8 and 9 we evaluate CRPS and PLL respectively for the regression datasets when
trained and evaluated with varying batch sizes, but other hyperparameters ﬁxed (T was ﬁxed at 100). The results show that
results deteriorate when batch sizes are too small, likely stemming from the large variance of the approximate posterior.
In tables 10 and 11 we evaluate CRPS and PLL respectively for the regression datasets when trained and evaluated with
varying n.o. stochastic forward samples, but other hyperparameters ﬁxed (M was ﬁxed at 128). The results are indicative
of performance improvements with larger T , although we see improvements over baseline for some datasets already with
T = 50 (1/10:th of the T used in our main experiments).

Table 4. Uncertainty quality measured by CRPS on regression dasets. CRPS measured on eight datasets over 5 random 80-20 splits
of the data with 5 different random seeds each split. Mean values for MCBN, MCDO and MNF are reported along with standard error.
A signiﬁcance test was performed to check if CRPS signiﬁcantly exceeds the baseline. The p-value from a one sample t-test is reported.

CRPS

Dataset

MCBN

p-value

MCDO

p-value

MNF

p-value

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

8.50±0.86
3.91±0.25
5.75±0.52
2.85±0.18
0.24±0.05
2.66±0.10
0.26±0.07
-56.39±14.27

6.39E-10
4.53E-14
6.71E-11
2.33E-14
2.32E-04
2.77-12
1.26E-03
5.94E-04

3.06±0.33
0.93±0.41
1.37±0.89
1.82±0.14
-0.44±0.05
0.99±0.08
2.00±0.21
21.42±2.99

1.64E-09
3.13E-02
1.38E-01
1.64E-12
2.17E-08
2.34E-12
1.83E-09
2.16E-07

5.88±1.09
3.13±0.81
1.10±2.63
0.52±0.26
-0.89±0.15
0.57±0.03
0.93±0.12
24.92±3.77

2.01E-05
6.43E-04
6.45E-01
7.15E-02
3.36E-06
8.56E-16
6.19E-08
9.62E-06

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Table 5. Uncertainty quality measured by PLL on regression dasets. PLL measured on eight datasets over 5 random 80-20 splits of
the data with 5 different random seeds each split. Mean values for MCBN, MCDO and MNF are reported along with standard error. A
signiﬁcance test was performed to check if PLL signiﬁcantly exceeds the baseline. The p-value from a one sample t-test is reported.

PLL

Dataset

MCBN

p-value

MCDO

p-value

MNF

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

10.49±1.35
-36.36±12.12
10.89±1.16
1.68±0.37
0.33±0.14
2.56±0.23
0.19±0.09
45.58±5.18

5.41E-08
6.19E-03
1.79E-09
1.29E-04
2.72E-02
4.28E-11
3.72E-02
5.67E-09

5.51±1.05
10.92±1.78
-14.28±5.15
-0.26±0.18
3.52±0.23
6.23±0.19
2.91±0.35
-41.54±31.37

2.20E-05
2.34E-06
1.06E-02
1.53E-01
1.12E-13
2.57E-21
1.84E-08
1.97E-01

1.76±1.12
-2.16±4.19
-33.88±29.57
0.42±0.43
-0.86±0.15
0.52±0.07
0.83±0.16
46.19±4.45

p-value

1.70E-01
6.79E-01
2.70E-01
2.70E-01
7.33E-06
1.81E-07
2.27E-05
2.47E-07

Table 6. Raw (unnormalized) CRPS and PLL scores on regression datasets. CRPS and PLL measured on eight datasets over 5
random 80-20 splits of the data with 5 different random seeds each split. Mean values and standard errors are reported for MCBN,
MCDO and MNF.

Dataset

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

MCBN

1.45±0.02
2.40±0.04
0.33±0.01
0.04±0.00
2.00±0.01
1.95±0.01
0.34±0.00
0.68±0.02

CRPS
MCDO

1.41±0.02
2.42±0.04
0.26±0.00
0.04±0.00
2.00±0.01
1.95±0.00
0.33±0.00
0.32±0.01

MNF

MCBN

1.57±0.02
3.61±0.02
1.33±0.04
0.05±0.00
2.31±0.01
2.25±0.01
0.34±0.00
0.94±0.01

-2.38±0.02
-3.45±0.11
-0.94±0.04
1.21±0.01
-2.75±0.00
-2.73±0.00
-0.95±0.01
-1.39±0.03

PLL
MCDO

-2.35±0.02
-2.94±0.02
-0.80±0.04
1.24±0.00
-2.72±0.01
-2.70±0.00
-0.89±0.01
-2.57±0.69

MNF

-2.51±0.06
-3.35±0.04
-3.18±0.07
1.04±0.00
-2.86±0.01
-2.83±0.01
-0.93±0.00
-1.96±0.05

Table 7. Prediction accuracy measured by RMSE on regression datasets. RMSE measured on eight datasets over 5 random 80-20
splits of the data with 5 different random seeds each split. Mean values and standard errors are reported for for MCBN, MCDO and
MNF as well as conventional non-Bayesian models BN and DO.

Dataset

MCBN

BN

DO

MNF

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

2.75±0.05
4.78±0.09
0.59±0.02
0.07±0.00
3.74±0.01
3.66±0.01
0.62±0.00
1.23±0.05

2.77±0.05
4.89±0.08
0.57±0.01
0.07±0.00
3.74±0.01
3.69±0.01
0.62±0.00
1.28±0.06

RMSE
MCDO

2.65±0.05
4.80±0.10
0.47±0.01
0.07±0.00
3.74±0.02
3.66±0.01
0.60±0.00
0.75±0.03

2.69±0.05
4.99±0.10
0.49±0.01
0.07±0.00
3.72±0.02
3.68±0.01
0.61±0.00
0.72±0.04

2.98±0.06
6.57±0.04
2.38±0.07
0.09±0.00
4.19±0.01
4.10±0.01
0.61±0.00
2.13±0.05

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 4. Errors in predictions (gray dots) sorted by estimated uncertainty on select datasets. The shaded areas show model uncertainty
(light area 95% CI, dark area 50% CI). Gray dots show absolute prediction errors on the test set, and the gray line depicts a running
mean of the errors. The dashed line indicates the optimized constant uncertainty. A correlation between estimated uncertainty (shaded
area) and mean error (gray) indicates the uncertainty estimates are meaningful for estimating errors.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 5. Errors in predictions (gray dots) sorted by estimated uncertainty on select datasets. The shaded areas show model uncertainty
(light area 95% CI, dark area 50% CI). Gray dots show absolute prediction errors on the test set, and the gray line depicts a running
mean of the errors. The dashed line indicates the optimized constant uncertainty. A correlation between estimated uncertainty (shaded
area) and mean error (gray) indicates the uncertainty estimates are meaningful for estimating errors.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Table 8. Uncertainty quality sensitivity to batch size. A sensitivity analysis to determine how MCBN uncertainty quality varies with
batch size is measured on eight regression datasets using CRPS as the quality measure. Results are measured over 3 random 80-20 splits
of the data with 5 different random seeds each split.

CRPS
64

Batch size

8

16

32

128

256

512

1024

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

-7.1
-34.5
-61.6
-1.4
-10.5
14.5
2.2
15.1

16.6
6.0
-3.0
-4.3
0.8
4.8
1.6
-23.0

11.8
5.0
2.7
0.2
0.0
3.6
0.6
-30.4

7.2
5.1
9.8
2.8
-0.1
2.8
0.6
21.0

2.5
2.9
11.1
2.7
0.0
2.5
0.3
34.4

0.9
1.4
0.8
1.7
0.0
1.6
0.0
-

-
0.6
4.9
0.9
0.2
1.0
0.2
-

-
0.0
-
0.5
0.0
0.5
0.0
-

Table 9. Uncertainty quality sensitivity to batch size. A sensitivity analysis to determine how MCBN uncertainty quality varies with
batch size is measured on eight regression datasets using PLL as the quality measure. Results are measured over 3 random 80-20 splits
of the data with 5 different random seeds each split.

Batch size

8

16

32

128

256

512

1024

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

13.9
-113.3
-64.4
-4.9
-135.0
44.9
2.2
99.6

-36.7
-528.4
5.2
-5.4
-1.4
15.7
2.0
74.9

10.0
-10.0
-0.2
-3.1
-1.0
4.6
0.0
76.8

PLL
64

7.9
2.9
-9.6
1.6
-1.1
2.9
0.5
48.5

3.7
0.0
-14.5
2.3
-0.4
2.8
0.6
44.9

1.5
1.4
1.4
1.5
0.1
2.2
0.4
-

-
0.2
10.4
0.7
-0.1
1.2
0.0
-

-
0.0
-
0.4
0.4
0.6
0.0
-

Table 10. Uncertainty quality sensitivity to n.o. stochastic forward passes. A sensitivity analysis to determine how MCBN uncer-
tainty quality varies with the n.o. stochastic forward passes measured on eight regression datasets using CRPS as the quality measure.
Results are measured over 3 random 80-20 splits of the data with 5 different random seeds each split.

Forward passes

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

CRPS
100

2.7
2.3
4.2
2.7
0.5
2.7
-0.4
32.2

50

3.2
3.3
7.9
4.2
0.1
2.4
0.6
32.1

250

6.1
3.3
13.2
3.2
0.2
2.3
0.9
32.9

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Table 11. Uncertainty quality sensitivity to n.o. stochastic forward passes. A sensitivity analysis to determine how MCBN uncer-
tainty quality varies with the n.o. stochastic forward passes measured on eight regression datasets using PLL as the quality measure.
Results are measured over 3 random 80-20 splits of the data with 5 different random seeds each split.

Forward passes

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

250

7.8
3.8
15.7
2.5
-0.9
1.8
1.7
38.0

PLL
100

1.9
7.1
-30.5
2.2
0.7
2.0
-0.9
35.9

50

2.6
0.1
-47.3
3.4
-0.9
2.4
1.1
35.5

1.8. Uncertainty in image segmentation

We applied MCBN to an image segmentation task using Bayesian SegNet with the main CamVid and PASCAL-VOC
models in (Kendall et al., 2015). Here, we provide more image from Pascal VOC dataset in Figure 6.

1.9. Batch normalization statistics

In Figure 7 and Figure 8, we provide statistics on the batch normalization parameters used for training. The plots show the
distribution of BN mean and BN variance over different mini-batches of an actual training of Yacht dataset for one unit in
the ﬁrst hidden layer and the second hidden layer. Data is provided for different epochs and for different batch sizes.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 6. Uncertainty in image segmentation. Results applying MCBN to Bayesian SegNet (Kendall et al., 2015) on images from
PASCAL-VOC (right). Left: original. Middle: the Bayesian estimated segmentation. Right: estimated uncertainty using MCBN for all
classes. Mini-batches of size 36 were used for PASCAL-VOC on images of size 224x224. 20 inferences were conducted to estimate the
mean and variance of MCBN.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 7. The distribution of means of mini-batches during training of one of our datasets. The distribution closely follows our analyt-
ically approximated Gaussian distribution. The data is collected for one unit of each layer and is provided for different epochs and for
different batch sizes.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 8. The distribution of standard deviation of mini-batches during training of one of our datasets. The distribution closely follows
our analytically approximated Gaussian distribution. The data is collected for one unit of each layer and is provided for different epochs
and for different batch sizes.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

References

Bui, T. D., Hern´andez-Lobato, D., Li, Y., Hern´andez-Lobato, J. M., and Turner, R. E. Deep Gaussian Processes for

Regression using Approximate Expectation Propagation. In ICML, 2016.

Chen, X., Kundu, K., Zhang, Z., Ma, H., Fidler, S., and Urtasun, R. Monocular 3d object detection for autonomous driving.

In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2147–2156, 2016.

Djuric, U., Zadeh, G., Aldape, K., and Diamandis, P. Precision histology: how deep learning is poised to revitalize

histomorphology for personalized cancer care. npj Precision Oncology, 1(1):22, 2017.

Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau, H. M., and Thrun, S. Dermatologist-level classiﬁcation

of skin cancer with deep neural networks. Nature, Feb 2017.

Gal, Y. Uncertainty in Deep Learning. PhD thesis, University of Cambridge, 2016.

Gal, Y. and Ghahramani, Z. Dropout as a Bayesian Approximation : Representing Model Uncertainty in Deep Learning.

ICML, 48:1–10, 2015.

data/kin/desc.html.

Ghahramani, Z. Delve Datasets. University of Toronto, 1996. URL http://www.cs.toronto.edu/{˜}delve/

Ghahramani, Z. Probabilistic machine learning and artiﬁcial intelligence. Nature, 521(7553):452–459, May 2015.

Gneiting, T. and Raftery, A. E. Strictly Proper Scoring Rules, Prediction, and Estimation. Journal of the American

Statistical Association, 102(477):359–378, 2007.

Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples.

arXiv preprint

arXiv:1412.6572, 2014.

Graves, A. Practical Variational Inference for Neural Networks. NIPS, 2011.

Hern´andez-Lobato, J. M. and Adams, R. Probabilistic backpropagation for scalable learning of bayesian neural networks.

In International Conference on Machine Learning, pp. 1861–1869, 2015.

Hinton, G. E. and Van Camp, D. Keeping the neural networks simple by minimizing the description length of the weights.

In Proceedings of the sixth annual conference on Computational learning theory, pp. 5–13. ACM, 1993.

Ioffe, S. Batch renormalization: Towards reducing minibatch dependence in batch-normalized models. CoRR,

abs/1702.03275, 2017. URL http://arxiv.org/abs/1702.03275.

Ioffe, S. and Szegedy, C. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.

Arxiv, 2015. URL http://arxiv.org/abs/1502.03167.

Karpathy, A. Convnetjs demo: toy 1d regression, 2015. URL http://cs.stanford.edu/people/karpathy/

convnetjs/demo/regression.html.

Kendall, A., Badrinarayanan, V., and Cipolla, R. Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-
Decoder Architectures for Scene Understanding. CoRR, abs/1511.0, 2015. URL http://arxiv.org/abs/1511.
02680.

Kingma, D. P. and Welling, M. Auto-Encoding Variational Bayes. In ICLR, 2014.

Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. 2009.

Krueger, D., Huang, C.-W., Islam, R., Turner, R., Lacoste, A., and Courville, A. Bayesian hypernetworks. arXiv preprint

arXiv:1710.04759, 2017.

Lehmann, E. L. Elements of Large-Sample Theory. Springer Verlag, New York, 1999. ISBN 0387985956.

Li, Y. and Gal, Y. Dropout Inference in Bayesian Neural Networks with Alpha-divergences. arXiv, 2017.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Louizos, C. and Welling, M. Multiplicative normalizing ﬂows for variational Bayesian neural networks. In Precup, D. and
Teh, Y. W. (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings
of Machine Learning Research, pp. 2218–2227, International Convention Centre, Sydney, Australia, 06–11 Aug 2017.
PMLR. URL http://proceedings.mlr.press/v70/louizos17a.html.

MacKay, D. J. A practical bayesian framework for backpropagation networks. Neural computation, 4(3):448–472, 1992.

Neal, R. M. Bayesian Learning for Neural Networks. PhD thesis, University of Toronto, 1995.

Neal, R. M. Bayesian learning for neural networks, volume 118. Springer Science & Business Media, 2012.

Selten, R. Axiomatic characterization of the quadratic scoring rule. Experimental Economics, 1(1):43–62, 1998.

Shen, L. End-to-end training for whole image breast cancer diagnosis using an all convolutional design. arXiv preprint

University of California, I. UC Irvine Machine Learning Repository, 2017. URL https://archive.ics.uci.

arXiv:1708.09427, 2017.

edu/ml/index.html.

Wang, S. I. and Manning, C. D. Fast dropout training. Proceedings of the 30th International Conference on Machine Learn-
ing, 28:118–126, 2013. URL http://machinelearning.wustl.edu/mlpapers/papers/wang13a.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

8
1
0
2
 
l
u
J
 
6
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
5
5
4
6
0
.
2
0
8
1
:
v
i
X
r
a

Mattias Teye 1 2 * Hossein Azizpour 1 * Kevin Smith 1 3

Abstract
We show that training a deep network using batch
normalization is equivalent to approximate infer-
ence in Bayesian models. We further demon-
strate that this ﬁnding allows us to make mean-
ingful estimates of the model uncertainty us-
ing conventional architectures, without modiﬁ-
cations to the network or the training proce-
dure. Our approach is thoroughly validated by
measuring the quality of uncertainty in a series
of empirical experiments on different tasks.
It
outperforms baselines with strong statistical sig-
niﬁcance, and displays competitive performance
with recent Bayesian approaches.

1. Introduction

Deep learning has dramatically advanced the state of the
art in a number of domains. Despite their unprecedented
discriminative power, deep networks are prone to make
mistakes. Nevertheless, they can already be found in set-
tings where errors carry serious repercussions such as au-
tonomous vehicles (Chen et al., 2016) and high frequency
trading. We can soon expect automated systems to screen
for various types of cancer (Esteva et al., 2017; Shen, 2017)
and diagnose biopsies (Djuric et al., 2017). As autonomous
systems based on deep learning are increasingly deployed
in settings with the potential to cause physical or economic
harm, we need to develop a better understanding of when
we can be conﬁdent in the estimates produced by deep net-
works, and when we should be less certain.

Standard deep learning techniques used for supervised
learning lack methods to account for uncertainty in the
model. This can be problematic when the network en-
counters conditions it was not exposed to during training,

* Co-ﬁrst authorship 1School of Electrical Engineering and
Computer Science, KTH Royal Institute of Technology, Stock-
holm, Sweden 2Current address: Electronic Arts, SEED, Stock-
holm, Sweden. This work was carried out at Budbee AB.
3Science for Life Laboratory. Correspondence to: Kevin Smith
<ksmith@kth.se>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

or if the network is confronted with adversarial examples
(Goodfellow et al., 2014). When exposed to data outside
the distribution it was trained on, the network is forced to
extrapolate, which can lead to unpredictable behavior.

If the network can provide information about its uncer-
tainty in addition to its point estimate, disaster may be
avoided. In this work, we focus on estimating such pre-
dictive uncertainties in deep networks (Figure 1).

The Bayesian approach provides a theoretical framework
for modeling uncertainty (Ghahramani, 2015), which has
prompted several attempts to extend neural networks (NN)
into a Bayesian setting. Most notably, Bayesian neural net-
works (BNNs) have been studied since the 1990’s (Neal,
2012), but do not scale well and struggle to compete with
modern deep learning architectures. Recently, (Gal &
Ghahramani, 2015) developed a practical solution to obtain
uncertainty estimates by casting dropout training in con-
ventional deep networks as a Bayesian approximation of a
Gaussian Process (its correspondence to a general approx-
imate Bayesian model was shown in (Gal, 2016)). They
showed that any network trained with dropout is an ap-
proximate Bayesian model, and uncertainty estimates can
be obtained by computing the variance on multiple predic-
tions with different dropout masks.

The inference in this technique, called Monte Carlo
Dropout (MCDO), has an attractive quality: it can be ap-
plied to any pre-trained networks with dropout layers. Un-
certainty estimates come (nearly) for free. However, not all
architectures use dropout, and most modern networks have
adopted other regularization techniques. Batch normaliza-
tion (BN), in particular, has become widespread thanks to
its ability to stabilize learning with improved generalization
(Ioffe & Szegedy, 2015).

An interesting aspect of BN is that the mini-batch statis-
tics used for training each iteration depend on randomly
selected batch members. We exploit this stochasticity and
show that training using batch normalization, like dropout,
can be cast as an approximate Bayesian inference. We
demonstrate how this ﬁnding allows us to make meaning-
ful estimates of the model uncertainty in a technique we
call Monte Carlo Batch Normalization (MCBN) (Figure 1).
The method we propose can be applied to any network us-
ing standard batch normalization.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Gaussian Processes show superior performance in terms of
RMSE and uncertainty quality compared to state-of-the-art
approximate BNNs (Bui et al., 2016)1. Another recent ap-
proach to Bayesian learning, Bayesian hypernetworks, use
a NN to learn a distribution of parameters over another net-
work (Krueger et al., 2017). Multiplicative Normalizing
Flows for variational Bayesian networks (MNF) (Louizos
& Welling, 2017) is a recent model that formulates a pos-
terior dependent on auxiliary variables. MNF achieves a
highly ﬂexible posterior by the application of normalizing
ﬂows to the auxiliary variables.

Although these recent techniques address some of the dif-
ﬁculties with approximate BNNs, they all require modiﬁ-
cations to the architecture or the way networks are trained,
as well as specialized knowledge from practitioners. Re-
cently, (Gal & Ghahramani, 2015) showed that a network
trained with dropout implicitly performs the VI objective.
Therefore any network trained with dropout can be treated
as an approximate Bayesian model by making multiple
predictions through the network while sampling different
dropout masks for each prediction. The mean and variance
of the predictions are used in the estimation of the mean
and variance of the predictive distribution 2.

3. Method

In the following, we introduce Bayesian models and a vari-
ational approximation using Kullback-Leibler (KL) diver-
gence following (Gal, 2016). We continue by showing that
a batch normalized deep network can be seen as an ap-
proximate Bayesian model. Employing theoretical insights
and empirical analysis, we study the induced prior on the
parameters when using batch normalization. Finally, we
describe the procedure for estimating the uncertainty of a
batch normalized network’s output.3

3.1. Bayesian Modeling

We assume a ﬁnite training set D = {(xi, yi)}i=1:N where
each (xi, yi) is a sample-label pair. Using D, we are inter-
ested in learning an inference function fω(x, y) with pa-
rameters ω. In deterministic models, the estimated label ˆy
is obtained as follows:

ˆy = arg max

fω(x, y)

y

In probabilistic models we let fω(x, y) = p(y|x, ω). In
Bayesian modeling, in contrast to ﬁnding a point estimate

1By uncertainty quality, we refer to predictive probability dis-

tributions as measured by PLL and CRPS.

2This technique is referred to as “MC Dropout” in the original

work, though we refer to it here as MCDO.

3While the method applies to FC or Conv layers, the induced

Figure 1. Training a deep network using batch normalization
is equivalent to approximate inference in Bayesian models.
Thus, uncertainty estimates can be obtained from any network
using BN through a simple procedure. At inference, several mini-
batches are constructed by taking random samples to accompany
the query. The mean and variance of the outputs are used to esti-
mate the predictive distribution (MCBN). Here, we show results
on a toy dataset from a network with three hidden layers (30 units
per layer). Training data is depicted as dots. The solid line is the
predictive mean of 500 stochastic forward passes and the shaded
areas represent the model’s uncertainty. The dashed lines depict a
minimal baseline for uncertainty (CUBN), see Section 4.1.

We validate our approach by empirical experiments on a
variety of datasets and tasks, including regression and im-
age classiﬁcation. We measure uncertainty quality relative
to a baseline of ﬁxed uncertainty, and show that MCBN
outperforms the baseline on nearly all datasets with strong
statistical signiﬁcance. We also show that the uncertainty
quality of MCBN is on par with other recent approximate
Bayesian networks.

2. Related Work

Bayesian models provide a natural framework for model-
ing uncertainty, and several approaches have been devel-
oped to adapt NNs to Bayesian reasoning. A common ap-
proach is to place a prior distribution (often a Gaussian)
over each parameter. The resulting model corresponds to
a Gaussian process for inﬁnite parameters (Neal, 1995),
and a Bayesian NN (MacKay, 1992) for a ﬁnite number of
parameters. Inference in BNNs is difﬁcult however (Gal,
2016), so focus has thus shifted to techniques that approx-
imate the posterior, approximate BNNs. Methods based on
variational inference (VI) typically rely on a fully factor-
ized approximate distribution (Kingma & Welling, 2014;
Hinton & Van Camp, 1993), but often do not scale. To alle-
viate these difﬁculties, (Graves, 2011) proposed a model
using sampling methods to estimate a factorized poste-
rior. Probabilistic backpropagation (PBP), estimates a fac-
torized posterior via expectation propagation (Hern´andez-
Lobato & Adams, 2015).

Using several strategies to address scaling issues, Deep

prior from weight decay (Section 3.3) is studied for FC layers.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

of the model parameters, the idea is to estimate an (ap-
proximate) posterior distribution of the model parameters
p(ω|D) to be used for probabilistic prediction:

(cid:90)

p(y|x, D) =

fω(x, y)p(ω|D)dω

The predicted label, ˆy, can then be accordingly obtained by
sampling p(y|x, D) or taking its maxima.

Variational Approximation In approximate Bayesian
modeling, a common approach is to learn a parame-
terized approximating distribution qθ(ω) that minimizes
KL(qθ(ω)||p(ω|D)); the Kullback-Leibler divergence of
the true posterior w.r.t. its approximation. Minimizing this
KL divergence is equivalent to the following minimization
while being free of the data term p(D) 4:

LVA(θ) := −

qθ(ω) ln fω(xi, yi)dω

N
(cid:88)

(cid:90)

i=1

+ KL(qθ(ω)||p(ω))

During optimization, we want to take the derivative of the
expected likelihood w.r.t. the learnable parameters θ. We
use the same MC estimate as in (Gal, 2016) (explained in
Appendix Section 1.1), such that one realized ˆωi is taken
for each sample i 5. Optimizing over mini-batches of size
M , the approximated objective becomes:

ˆLVA(θ) := −

ln f ˆωi (xi, yi) + KL(qθ(ω)||p(ω))

(1)

N
M

M
(cid:88)

i=1

The ﬁrst term is the data likelihood and the second term
is the divergence of the prior w.r.t. the approximated poste-
rior.

3.2. Batch Normalized Deep Nets as Bayesian Modeling

We now describe the optimization procedure of a deep net-
work with batch normalization and draw the resemblance
to the approximate Bayesian modeling in Eq (1).

The inference function of a feed-forward deep network
with L layers can be described as:

where a(.) is an element-wise nonlinearity function and
Wl is the weight vector at layer l. Furthermore, we de-
note the input to layer l as xl with x1 = x and we then set
hl = Wlxl. Parenthesized super-index for matrices (e.g.
W(j)) and vectors (e.g. x(j)) indicates jth row and element
respectively. Super-index u refers to a speciﬁc unit at layer
l, (e.g. Wu = Wl,(j), hu = hl,(j)). 6

Batch Normalization Each layer of a deep network is
constructed by several linear units whose parameters are
the rows of the weight matrix W. Batch normalization is
a unit-wise operation proposed in (Ioffe & Szegedy, 2015)
to standardize the distribution of each unit’s input. For FC
layers, it converts a unit’s input hu in the following way:

ˆhu =

hu − E[hu]
(cid:112)Var[hu]

where the expectations are computed over the training
set during evaluation, and mini-batch during training (in
deep networks, the weight matrices are often optimized us-
ing back-propagated errors calculated on mini-batches of
data)7. Therefore, during training, the estimated mean and
variance on the mini-batch B is used, which we denote by
µB and σB respectively. This makes the inference at train-
ing time for a sample x a stochastic process, varying based
on other samples in the mini-batch.

Loss Function and Optimization Training deep net-
works with mini-batch optimization involves a (regular-
ized) risk minimization with the following form:

LRR(ω) :=

l(ˆyi, yi) + Ω(ω)

1
M

M
(cid:88)

i=1

where the ﬁrst term is the empirical loss on the training
data and the second term is a regularization penalty act-
ing as a prior on model parameters ω.
If the loss l is
cross-entropy for classiﬁcation or sum-of-squares for re-
gression problems (assuming i.i.d. Gaussian noise on la-
bels), the ﬁrst term is equivalent to minimizing the negative
log-likelihood:

fω(x) = WLa(WL−1...a(W2a(W1x))

LRR(ω) := −

ln fω(xi, yi) + Ω(ω)

4Achieved by constructing the Evidence Lower Bound, called
ELBO, and assuming i.i.d. observation noise; details can be found
in Appendix Section 1.1.

5While a MC integration using a single sample is a weak ap-
proximation, in an iterative optimization for θ several samples
will be taken over time.

6For a (softmax) classiﬁcation network, fω(x) is a vector with
fω(x, y) = fω(x)(y), for regression networks with i.i.d. Gaus-
sian noise we have fω(x, y) = N (fω(x), τ −1I).

7It also learns an afﬁne transformation for each unit with pa-
afﬁne = γ(j) ˆx(j) + β(j).

rameters γ and β, omitted for brevity: ˆx(j)

1
M τ

M
(cid:88)

i=1

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

with τ = 1 for classiﬁcation.
In a network
with batch normalization, the model parameters include
{W1:L, γ1:L, β1:L, µ1:L
B }. If we decouple the learn-
able parameters θ = {W1:L, γ1:L, β1:L} from the stochas-
tic parameters ω = {µ1:L
B }, we get the following ob-
jective at each step of the mini-batch optimization:

B , σ1:L

B , σ1:L

LRR(θ) := −

ln f{θ, ˆωi}(xi, yi) + Ω(θ)

(2)

1
M τ

M
(cid:88)

i=1

where ˆωi is the means and variances for sample i’s mini-
batch at a certain training step. Note that while ˆωi formally
needs to be i.i.d. for each training example, a batch normal-
ized network samples the stochastic parameters once per
training step (mini-batch). For a large number of epochs,
however, the distribution of sampled batch members for a
given training example converges to the i.i.d. case.

B , σ1:L

In a batch normalized network, qθ(ω) corresponds to the
joint distribution of the weights, induced by the random-
ness of the normalization parameters µ1:L
B , as im-
plied by the repeated sampling from D during training.
This is an approximation of the true posterior, where we
have restricted the posterior to lie within the domain of
our parametric network and source of randomness. With
that, we can estimate the uncertainty of predictions from
a trained batch normalized network using the inherent
stochasticity of BN (Section 3.4).

3.3. Prior p(ω)

Equivalence between the VA and BN training procedures
requires ∂
∂θ of Eq. (1) and Eq. (2) to be equivalent up to a
scaling factor. This is the case if ∂
∂θ KL(qθ(ω)||p(ω)) =
N τ ∂

∂θ Ω(θ).

To reconcile this condition, one option is to let the prior
p(ω) imply the regularization term Ω(θ). Eq. (1) reveals
that the contribution of KL(qθ(ω)||p(ω)) to the optimiza-
tion objective is inversely scaled with N . For BN, this cor-
responds to a model with a small Ω(θ) when N is large. In
the limit as N → ∞, the optimization objectives of Eq. (1)
and Eq. (2) become identical with no regularization.8

Another option is to let some Ω(θ) imply p(ω).
In Ap-
pendix Section 1.4 we explore this with L2-regularization,
also called weight decay (Ω(θ) = λ (cid:80)
l=1:L ||W l||2). We
ﬁnd that unlike in MCDO (Gal, 2016), some simplifying

8To prove the existence and ﬁnd an expression of
KL(qθ(ω)||p(ω)), in Appendix Section 1.3 we ﬁnd that BN ap-
proximately induces Gaussian distributions over BN units’ means
and standard deviations, centered around the population values
given by D. We assume a factorized distribution and Gaussian
priors, and ﬁnd the corresponding KL(qθ(ω)||p(ω)) components
in Appendix Section 1.4 Eq. (7). These could be used to construct
a custom Ω(θ) for any Gaussian choice of p(ω).

assumptions are necessary to reconcile the VA and BN ob-
jectives with weight decay: no scale and shift applied to
BN layers, uncorrelated units in each layer, BN applied on
all layers, and large N and M . Given these conditions:

p(µu
p(σu

B) = N (µµ,p, σµ,p)
B) = N (µσ,p, σσ,p)

where µµ,p = 0, σµ,p → ∞, µσ,p = 0 and σσ,p → 1

.

2N τ λl

This corresponds to a wide and narrow distribution on BN
units’ means and std. devs respectively, where N accounts
for the narrowness of the prior. Due to its popularity in
deep learning, our experiments in Section 4 are performed
with weight decay.

3.4. Predictive Uncertainty in Batch Normalized Deep

Nets

In the absence of the true posterior, we rely on the approx-
imate posterior to express an approximate predictive distri-
bution:

p∗(y|x, D) :=

fω(x, y)qθ(ω)dω

(cid:90)

Following (Gal, 2016) we estimate the ﬁrst (for regression
and classiﬁcation) and second (for regression) moments of
the predictive distribution empirically (see Appendix Sec-
tion 1.5 for details):

Ep∗ [y] ≈

f ˆωi(x)

1
T

T
(cid:88)

i=1

T
(cid:88)

1
T

i=1
− Ep∗ [y](cid:124)Ep∗ [y]

Covp∗ [y] ≈ τ −1I +

f ˆωi(x)(cid:124)f ˆωi(x)

where each ˆωi corresponds to sampling the net’s stochas-
tic parameters ω = {µ1:L
B , σ1:L
B } the same way as during
training. Sampling ˆωi therefore involves sampling a batch
B from the training set and updating the parameters in the
BN units, just as if we were taking a training step with B.
From a VA perspective, training the network amounted to
minimizing KL(qθ(ω)||p(ω|D)) wrt θ. Sampling ˆωi from
the training set, and keeping the size of B consistent with
the mini-batch size used during training, ensures that qθ(ω)
during inference remains identical to the approximate pos-
terior optimized during training.

The network is trained just as a regular BN network, but
instead of replacing ω = {µ1:L
B } with population
values from D for inference, we update these parameters
stochastically, once for each forward pass.9 Pseudocode
for estimating predictive mean and variance is given in Al-
gorithm 1.

B , σ1:L

9As an alternative to using the training set D to sample ˆωi,

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Algorithm 1 MCBN Algorithm
Input: sample x, number of inferences T , batchsize b
Output: mean prediction ˆy, predictive uncertainty σ2
1: y = {}
2: loop for T iterations
3: B ∼ D // mini batch
ˆω = {µB, σB}
4:
y = y ∪ f ˆω(x)
5:
6: end loop
7: ˆy = E[y]
8: σ2 = Cov[y] + τ −1I // for regression

// mini batch mean and variance

4. Experiments and Results

We assess the uncertainty quality of MCBN quantitatively
and qualitatively. Our quantitative analysis relies on CI-
FAR10 for image classiﬁcation and eight standard regres-
sion datasets, listed in Appendix Table 1. Publicly avail-
able from the UCI Machine Learning Repository (Univer-
sity of California, 2017) and Delve (Ghahramani, 1996),
these datasets have been used to benchmark comparative
models in recent related literature (see (Hern´andez-Lobato
& Adams, 2015), (Gal & Ghahramani, 2015), (Bui et al.,
2016) and (Li & Gal, 2017)). We report results using
standard metrics, and also propose useful upper and lower
bounds to normalize these metrics for an easier interpreta-
tion in Section 4.2.

Our qualitative results include the toy dataset in Figure 1
in the style of (Karpathy, 2015), a new visualization of un-
certainty quality that plots test errors sorted by predicted
variance (Figure 2 and Appendix), and image segmentation
results (Figure 2 and Appendix).

4.1. Metrics

We evaluate uncertainty quality based on two standard met-
rics, described below: Predictive Log Likelihood (PLL)
and Continuous Ranked Probability Score (CRPS). To im-
prove the interpretability of the metrics, we propose to nor-
malize them by upper and lower bounds.

Predictive Log Likelihood (PLL) Predictive Log Like-
lihood is widely accepted as the main uncertainty quality
metric for regression (Hern´andez-Lobato & Adams, 2015;
Gal & Ghahramani, 2015; Bui et al., 2016; Li & Gal, 2017).
A key property of PLL is that it makes no assumptions
about the form of the distribution. The measure is deﬁned
for a probabilistic model fω(x) and a single observation

we could sample from the implied qθ(ω) as modeled in the Ap-
pendix. This would alleviate having to store D for use during
prediction. In our experiments we used D to sample ˆωi however,
and leave the evaluation of the modeled qθ(ω) for future research.

(yi, xi) as:

PLL(fω(x), (yi, xi)) = log p(yi|fω(xi))

where p(yi|fω(xi)) is the model’s predicted PDF evalu-
ated at yi, given the input xi. A more detailed description
is given in the Appendix Section 1.5. The metric is un-
bounded and maximized by a perfect prediction (mode at
yi) with no variance. As the predictive mode moves away
from yi, increasing the variance tends to increase PLL (by
maximizing probability mass at yi). While PLL is an ele-
gant measure, it has been criticized for allowing outliers to
have an overly negative effect on the score (Selten, 1998).

Continuous Ranked Probability Score (CRPS) Con-
tinuous Ranked Probability Score is a measure that takes
the full predicted PDF into account with less sensitivity to
outliers. A prediction with low variance that is slightly off-
set from the true observation will receive a higher score
form CRPS than PLL. In order for CRPS to be analytically
tractable, we need to assume a Gaussian unimodal predic-
tive distribution. CRPS is deﬁned as

CRPS(fω(xi), (yi, xi)) =

(cid:0)F (y) − 1(y ≥ yi)(cid:1)2

dy

(cid:90) ∞

−∞

where F (y) is the predictive CDF, and 1(y ≥ yi) = 1
if y ≥ yi and 0 otherwise (for univariate distributions)
(Gneiting & Raftery, 2007). CRPS is interpreted as the sum
of the squared area between the CDF and 0 where y < yi
and between the CDF and 1 where y ≥ yi. A perfect pre-
diction with no variance yields a CRPS of 0; for all other
cases the value is larger. CRPS has no upper bound.

4.2. Benchmark models and normalized metrics

It is difﬁcult to interpret the quality of uncertainty from
raw PLL and CRPS values. We propose to normalize the
metrics between useful lower and upper bounds. The nor-
malized measures estimate the performance of an uncer-
tainty model between the trivial solution (constant uncer-
tainty) and optimal uncertainty for each prediction. For
the lower bound, we deﬁne a baseline that predicts con-
stant variance regardless of input. The variance is set to
a ﬁxed value that optimizes CRPS on validation data. We
call this model Constant Uncertainty BN (CUBN). It re-
ﬂects our best guess of constant variance on test data –
thus, any improvement in uncertainty quality over CUBN
indicates a sensible estimate of uncertainty. We simi-
larly deﬁne a baseline for dropout, Constant Uncertainty
Dropout (CUDO). The modeling of variance (uncertainty)
by MCBN and CUBN are visualized in Figure 1.

An upper bound on uncertainty performance can also
be deﬁned for a probabilistic model f with respect to
CRPS or PLL. For each observation (yi, xi), a value

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

for the predictive variance Ti can be chosen that max-
imizes PLL or minimizes CRPS10. Using CUBN as a
lower bound and the optimized CRPS score as the up-
per bound, uncertainty estimates can be normalized be-
tween these bounds (1 indicating optimal performance,
and 0 indicating same performance as ﬁxed uncer-
this normalized measure CRPS =
tainty). We call
minT CRPS(f,(yi,xi))−CRPS(fCU ,(yi,xi)) × 100, and the PLL
analogue PLL = PLL(f,(yi,xi))−PLL(fCU ,(yi,xi))
maxT PLL(f,(yi,xi))−PLL(fCU ,(yi,xi)) ×100.

CRPS(f,(yi,xi))−CRPS(fCU ,(yi,xi))

4.3. Test setup

Our evaluation compares MCBN to MCDO (Gal &
Ghahramani, 2015) and MNF (Louizos & Welling, 2017)
using the datasets and metrics described above. Our setup
is similar to (Hern´andez-Lobato & Adams, 2015), which
was also followed by (Gal & Ghahramani, 2015). How-
ever, our comparison implements a different hyperparame-
ter selection, allows for a larger range of dropout rates, and
uses larger networks with two hidden layers.

For the regression task, all models share a similar archi-
tecture: two hidden layers with 50 units each, and ReLU
activations, with the exception of Protein Tertiary Struc-
ture dataset (100 units per hidden layer). Inputs and out-
puts were normalized during training. Results were aver-
aged over ﬁve random splits of 20% test and 80% train-
ing and cross-validation (CV) data. For each split, 5-fold
CV by grid search with a RMSE minimization objective
was used to ﬁnd training hyperparameters and optimal n.o.
epochs, out of a maximum of 2000. For BN-based mod-
els, the hyperparameter grid consisted of a weight decay
factor ranging from 0.1 to 1−15 by a log 10 scale, and a
batch size range from 32 to 1024 by a log 2 scale. For
DO-based models, the hyperparameter grid consisted of
the same weight decay range, and dropout probabilities
in {0.2, 0.1, 0.05, 0.01, 0.005, 0.001}. DO-based models
used a batch size of 32 in all evaluations. For MNF11, the
n.o. epochs was optimized, the batch size was set to 100,
and early stopping test performed each epoch (compared to
every 20th for MCBN, MCDO).

For MCBN and MCDO, the model with optimal training
hyperparameters was used to optimize τ numerically. This
optimization was made in terms of average CV CRPS for
MCBN, CUBN, MCDO, and CUDO respectively.

Estimates for the predictive distribution were obtained by
taking T = 500 stochastic forward passes through the net-
work. For each split, test set evaluation was done 5 times
with different seeds. Implementation was done in Tensor-
Flow with the Adam optimizer and a learning rate of 0.001.

10Ti can be found analytically for PLL, but must be found nu-

merically for CRPS.

11Where we used an adapted version of the authors’ code.

For
the image classiﬁcation test we use CIFAR10
(Krizhevsky & Hinton, 2009) which includes 10 object
classes with 5,000 and 1,000 images in the training and
test sets, respectively. Images are 32x32 RGB format. We
trained a ResNet32 architecture with a batch size of 32,
learning rate of 0.1, weight decay of 0.0002, leaky ReLU
slope of 0.1, and 5 residual units. SGD with momentum
was used as the optimizer.

Code for reproducing our experiments is available at
https://github.com/icml-mcbn/mcbn.

4.4. Test results

The regression experiment comparing uncertainty quality
is summarized in Table 1. We report CRPS and PLL, ex-
pressed as a percentage, which reﬂects how close the model
is to the upper bound, and check to see if the model signif-
icantly exceeds the lower bound using a one sample t-test
(signiﬁcance level is indicated by *’s). Further details are
provided in Appendix Section 1.7.

In Figure 2 (left), we present a novel visualization of un-
certainty quality for regression problems. Data are sorted
by estimated uncertainty in the x-axis. Grey dots show the
errors in model predictions, and the shaded areas show the
model uncertainty. A running mean of the errors appears
as a gray line. If uncertainty estimation is working well,
a correlation should exist between the mean error (gray
line) and uncertainty (shaded area). This indicates that the
uncertainty estimation recognizes samples with larger (or
smaller) potential for predictive errors.

We applied MCBN on the image classiﬁcation task of CI-
FAR10. The baseline in this case is the softmax distribu-
tion using the moving average for BN units. Log likeli-
hood (PLL) is the metric used to compare with the base-
line. The baseline achieves a PLL of -0.32 on the test
set, while MCBN obtains a PLL of -0.28. Table 2 shows
the performance of MCBN when using different number of
stochastic forward passes (the MCBN batchsize is ﬁxed to
the training batch size at 32). PLL improves as the number
of the stochastic passes increases, until it is signiﬁcantly
better than the softmax baseline.

To demonstrate how model uncertainty can be obtained
from an existing network with minimal effort, we applied
MCBN to an image segmentation task using Bayesian Seg-
Net with the main CamVid and PASCAL-VOC models in
(Kendall et al., 2015). We simply ran multiple forward
passes with different mini-batches randomly taken from the
train set. The models obtained from the online model zoo
have BN blocks after each layer. We recalculate mean and
variance for the ﬁrst 2 blocks only and use the training
statistics for the rest of the blocks. Mini-batches of size
10 and 36 were used for CamVid and VOC respectively

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Table 1. Uncertainty quality measured on eight regression datasets. MCBN, MCDO and MNF are compared over 5 random 80-20
splits of the data with 5 different random seeds each split. We report CRPS and PLL, uncertainty metrics CRPS and PLL normalized
to a lower bound of constant variance and upper bound that maximizes the metric expressed as a percentage (described in Section 4.2).
Higher numbers mean the model is closer to the upper bound. We check if the reported values for CRPS and PLL signiﬁcantly exceed
the lower bound using a one sample t-test (signiﬁcance level indicated by *’s). See text for further details.

MCBN

MCDO

MNF

MCBN

CRPS

Dataset

Boston
Concrete
Energy
Kin8nm
Power
Protein
Wine (Red)
Yacht

8.50 ****
3.91 ****
5.75 ****
2.85 ****
0.24 ***
2.66 ****
0.26 **
-56.39 ***

3.06 ****
*
0.93
1.37 ns
1.82 ****
-0.44 ****
0.99 ****
2.00 ****
21.42 ****

5.88 ****
3.13 ***
1.10 ns
0.53 ns
-0.89 ****
0.57 ****
0.94 ****
24.92 ****

PLL
MCDO

5.51 ****
10.92 ****
-14.28

*
-0.26 ns
3.52 ****
6.23 ****
2.91 ****

-41.54 ns

MNF

1.76 ns
-2.16 ns
-33.88 ns
0.42 ns
-0.87 ****
0.52 ****
0.83 ****
46.19 ****

10.49 ****
-36.36 **
10.89 ****
1.68 ***
0.33 **
2.56 ****
0.19

*

45.58 ****

due to memory limits. The results in Figure 2 (right) were
obtained from 20 stochastic forward passes, showing high
uncertainty near object boundaries. The VOC results are
more appealing because of larger mini-batches.

We provide additional experimental results in the Ap-
pendix. Appendix Tables 2 and 3 show the mean CRPS
and PLL values from the regression experiment. Table 4
provides the raw CRPS and PLL scores.
In Table 5 we
provide RMSE results of the MCBN and MCDO networks
in comparison with non-stochastic BN and DO networks.
These results indicate that the procedure of multiple for-
ward passes in MCBN and MCDO show slight improve-
ments in the predictive accuracy compared to their non-
Bayesian counterparts. In Tables 6 and 7, we investigate the
effect of varying batch size while keeping other hyperpa-
rameters ﬁxed. We see that performance deteriorates with
small batch sizes (≤16), a known issue of BN (Ioffe, 2017).
Similarly, results varying the number of stochastic forward
passes T is reported in Tables 8 and 9. While performance
beneﬁts from large T , in some cases T = 50 (i.e. 1/10 of
T in the main evaluation) performs well. Uncertainty-error
plots for all the datasets are provided in the Appendix.

5. Discussion

The results presented in Tables 1-2 and Appendix Tables
2-9 indicate that MCBN generates meaningful uncertainty

Table 2. Uncertainty quality for image classiﬁcation varying
number of stochastic forward passes. Uncertainty quality for
image classiﬁcation measured by PLL. ResNet32 is trained on
CIFAR10 with batch size 32. PLL improves as the sampling in-
creases until it is signiﬁcantly better than the softmax baseline
(-0.32).

estimates that correlate with actual errors in the model’s
In Table 1, we show statistically signiﬁcant
prediction.
improvements over CUBN in the majority of the datasets,
both in terms of CRPS and PLL. The visualizations in
Figure 2 and in the Appendix Figures 2-3 show correla-
tions between the estimated model uncertainty and errors
of the network’s predictions. We perform the same exper-
iments using MCDO and MNF, and ﬁnd that MCBN gen-
erally performs on par with both methods. Looking closer,
MCBN outperforms MCDO and MNF in more cases than
not, measured by CRPS. However, care must be used. The
learned parameters are different, leading to different pre-
dictive means and confounding direct comparison.

The results on the Yacht Hydrodynamics dataset seem con-
tradictory. The CRPS score for MCBN are extremely neg-
ative, while the PLL score is extremely positive. The op-
posite trend is observed for MCDO. To add to the puzzle,
the visualization in Figure 2 depicts an extremely promis-
ing uncertainty estimation that models the predictive errors
with high ﬁdelity. We hypothesize that this strange behav-
ior is due to the small size of the data set, which only con-
tains 60 test samples, or due to the Gaussian assumption
of CRPS. There is also a large variability in the model’s
accuracy on this dataset, which further confounds the mea-
surements for such limited data.

One might criticize the overall quality of uncertainty es-
timates observed in all the models we tested, due to the
magnitude of CRPS and PLL in Table 1. The scores rarely
exceed 10% improvement over the lower bound. However,
we caution that these measures should be taken in context.
The upper bound is very difﬁcult to achieve in practice –
it is optimized for each test sample individually – and the
lower bound is a quite reasonable estimate.

Number of stochastic forward passes
64
2

16

32

4

8

1

PLL

-.36

-.32

-.30

-.29

-.29

-.28

-.28

128

-.28

The study of MCBN sensitivity to batch size revealed that a
certain batch size is required for the best performance, de-
pendent on the data. When doing inference on a GPU, large

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Data sorted by estimated uncertainty

Data sorted by estimated uncertainty

Image segmentation uncertainty (CamVid and PASCAL-VOC)

Figure 2. Uncertainty-error plots (left) and segmentation and uncertainty results applying MCBN to Bayesian SegNet (right).
(left) Errors in predictions (gray dots) sorted by estimated uncertainty on select datasets. The shaded areas show model uncertainty for
MCBN (blue), MNF (violet) and MCDO (red). The light area indicates 95% CI, dark area 50% CI. Gray dots show absolute prediction
errors on the test set, and the gray line depicts a running mean of the errors. The dashed line indicates the optimized constant uncertainty.
A correlation between estimated uncertainty (shaded area) and mean error (gray) indicates the uncertainty estimates are meaningful
for estimating errors. (right) Applying MCBN to Bayesian SegNet (Kendall et al., 2015) on scenes from CamVid (3rd column) and
PASCAL-VOC (4th column). Top: original image. Middle: the Bayesian estimated segmentation. Bottom: estimated uncertainty using
MCBN for all classes. The uncertainty maps for both datasets are reasonable, but qualitatively better for PASCAL-VOC due to the larger
mini-batch size (36) compared to CamVid (10). Smaller batch sizes were used for CamVid due to memory limitations (CamVid images
are 360x480 while VOC are 224x224). See Appendix for complete results.

batch sizes may cause memory issues for cases where the
input is large and the network has a large number of param-
eters, as is common for state-of-the-art image classiﬁcation
networks. However, there are various workarounds to this
problem. One can store BN statistics, instead of batches, to
reduce memory issues. Furthermore, we can use the Gaus-
sian estimate of the BN statistics as discussed previously,
which makes memory and computation extremely efﬁcient.

6. Conclusion

In this work, we have shown that training a deep network
using batch normalization is equivalent to approximate in-
ference in Bayesian models. We show evidence that the
uncertainty estimates from MCBN correlate with actual er-
rors in the model’s prediction, and are useful for practical

tasks such as regression, image classiﬁcation, and image
segmentation. Our experiments show that MCBN yields
a signiﬁcant improvement over the optimized constant un-
certainty baseline, on par with MCDO and MNF. Our eval-
uation also suggests new normalized metrics based on use-
ful upper and lower bounds, and a new visualization which
provides an intuitive explanation of uncertainty quality.

Finally, it should be noted that over the past few years,
batch normalization has become an integral part of most –
if not all – cutting edge deep networks. We have shown that
it is possible to obtain meaningful uncertainty estimates
from existing models without modifying the network or the
training procedure. With a few lines of code, robust uncer-
tainty estimates can be obtained by computing the variance
of multiple stochastic forward passes through an existing
network.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

References

Bui, T. D., Hern´andez-Lobato, D., Li, Y., Hern´andez-
Lobato, J. M., and Turner, R. E. Deep Gaussian Pro-
cesses for Regression using Approximate Expectation
Propagation. In ICML, 2016.

Chen, X., Kundu, K., Zhang, Z., Ma, H., Fidler, S., and Ur-
tasun, R. Monocular 3d object detection for autonomous
driving. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 2147–2156,
2016.

Djuric, U., Zadeh, G., Aldape, K., and Diamandis, P. Preci-
sion histology: how deep learning is poised to revitalize
histomorphology for personalized cancer care. npj Pre-
cision Oncology, 1(1):22, 2017.

Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M.,
Blau, H. M., and Thrun, S. Dermatologist-level classiﬁ-
cation of skin cancer with deep neural networks. Nature,
Feb 2017.

Gal, Y. Uncertainty in Deep Learning. PhD thesis, Univer-

sity of Cambridge, 2016.

Gal, Y. and Ghahramani, Z. Dropout as a Bayesian Ap-
proximation : Representing Model Uncertainty in Deep
Learning. ICML, 48:1–10, 2015.

Ghahramani, Z. Delve Datasets. University of Toronto,
URL http://www.cs.toronto.edu/

1996.
{˜}delve/data/kin/desc.html.

Ghahramani, Z. Probabilistic machine learning and ar-
tiﬁcial intelligence. Nature, 521(7553):452–459, May
2015.

Gneiting, T. and Raftery, A. E. Strictly Proper Scoring
Rules, Prediction, and Estimation. Journal of the Amer-
ican Statistical Association, 102(477):359–378, 2007.

Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain-
ing and harnessing adversarial examples. arXiv preprint
arXiv:1412.6572, 2014.

Graves, A. Practical Variational Inference for Neural Net-

works. NIPS, 2011.

Hern´andez-Lobato, J. M. and Adams, R. Probabilistic
backpropagation for scalable learning of bayesian neu-
ral networks. In International Conference on Machine
Learning, pp. 1861–1869, 2015.

Hinton, G. E. and Van Camp, D. Keeping the neural net-
works simple by minimizing the description length of
the weights. In Proceedings of the sixth annual confer-
ence on Computational learning theory, pp. 5–13. ACM,
1993.

Ioffe, S. Batch renormalization: Towards reducing mini-
batch dependence in batch-normalized models. CoRR,
abs/1702.03275, 2017. URL http://arxiv.org/
abs/1702.03275.

Ioffe, S. and Szegedy, C. Batch Normalization: Accelerat-
ing Deep Network Training by Reducing Internal Co-
variate Shift. Arxiv, 2015. URL http://arxiv.
org/abs/1502.03167.

Karpathy, A. Convnetjs demo:

toy 1d regression, 2015.
http://cs.stanford.edu/people/

URL
karpathy/convnetjs/demo/regression.
html.

Kendall, A., Badrinarayanan, V., and Cipolla, R. Bayesian
SegNet: Model Uncertainty in Deep Convolutional
Encoder-Decoder Architectures for Scene Understand-
ing. CoRR, abs/1511.0, 2015. URL http://arxiv.
org/abs/1511.02680.

Kingma, D. P. and Welling, M. Auto-Encoding Variational

Bayes. In ICLR, 2014.

Krizhevsky, A. and Hinton, G. Learning multiple layers of

features from tiny images. 2009.

Krueger, D., Huang, C.-W., Islam, R., Turner, R., Lacoste,
A., and Courville, A. Bayesian hypernetworks. arXiv
preprint arXiv:1710.04759, 2017.

Lehmann, E. L.

Elements of Large-Sample Theory.

Springer Verlag, New York, 1999. ISBN 0387985956.

Li, Y. and Gal, Y. Dropout Inference in Bayesian Neural

Networks with Alpha-divergences. arXiv, 2017.

Louizos, C. and Welling, M. Multiplicative normal-
izing ﬂows for variational Bayesian neural networks.
In Precup, D. and Teh, Y. W. (eds.), Proceedings of
the 34th International Conference on Machine Learn-
ing, volume 70 of Proceedings of Machine Learn-
ing Research, pp. 2218–2227, International Convention
Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.
URL http://proceedings.mlr.press/v70/
louizos17a.html.

MacKay, D. J. A practical bayesian framework for back-
propagation networks. Neural computation, 4(3):448–
472, 1992.

Neal, R. M. Bayesian Learning for Neural Networks. PhD

thesis, University of Toronto, 1995.

Neal, R. M. Bayesian learning for neural networks, volume

118. Springer Science & Business Media, 2012.

Selten, R. Axiomatic characterization of the quadratic scor-
ing rule. Experimental Economics, 1(1):43–62, 1998.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Shen, L. End-to-end training for whole image breast can-
cer diagnosis using an all convolutional design. arXiv
preprint arXiv:1708.09427, 2017.

University of California, I. UC Irvine Machine Learning
Repository, 2017. URL https://archive.ics.
uci.edu/ml/index.html.

Wang, S. I. and Manning, C. D.
Proceedings of

Fast dropout train-
the 30th International Con-
ing.
ference on Machine Learning, 28:118–126, 2013.
URL http://machinelearning.wustl.edu/
mlpapers/papers/wang13a.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

1. Appendix

1.1. Variational Approximation

Assume we were to come up with a family of distributions parameterized by θ in order to approximate the posterior, qθ(ω).
Our goal is to set θ such that qθ(ω) is as similar to the true posterior p(ω|D) as possible.

For clarity, qθ(ω) is a distribution over stochastic parameters ω that is determined by a set of learnable parameters θ and
some source of randomness. The approximation is therefore limited by our choice of parametric function qθ(ω) as well
as the randomness.12 Given ω and an input x, an output distribution p(y|x, ω) = p(y|fω(x)) = fω(x, y) is induced by
observation noise (the conditionality of which is omitted for brevity).

One strategy for optimizing θ is to minimize KL(qθ(ω)||p(ω|D)), the KL divergence of p(ω|D) w.r.t. qθ(ω). Minimizing
KL(qθ(ω)||p(ω|D)) is equivalent to maximizing the ELBO:

(cid:90)

ω

qθ(ω) ln p(Y|X, ω)dω − KL(qθ(ω)||p(ω))

Assuming i.i.d. observation noise, this is equivalent to minimizing:

LVA(θ) := −

qθ(ω) ln p(yi|fω(xi))dω + KL(qθ(ω)||p(ω))

N
(cid:88)

(cid:90)

n=1

Instead of making the optimization on the full training set, we can use a subsampling (yielding an unbiased estimate of
LVA(θ)) for iterative optimization (as in mini-batch optimization):

ˆLVA(θ) := −

N
M

(cid:90)

(cid:88)

i∈B

ω

qθ(ω) ln p(yi|fω(xi))dω + KL(qθ(ω)||p(ω))

During optimization, we want to take the derivative of the expected log likelihood w.r.t. the learnable parameters θ. (Gal,
2016) provides an intuitive interpretation of a MC estimate for NNs trained with a SRT (equivalent to the reparametrisation
trick in (Kingma & Welling, 2014)), and this interpretation is followed here. We let an auxillary variable (cid:15) represent the
stochasticity during training such that ω = g(θ, (cid:15)). The function g and the distribution of (cid:15) are such that p(g(θ, (cid:15))) =
qθ(ω).13 Assuming qθ(ω) can be written (cid:82)
(cid:15) qθ(ω|(cid:15))p((cid:15))d(cid:15) where qθ(ω|(cid:15)) = δ(ω − g(θ, (cid:15))), this auxiliary variable yields
the estimate (full proof in (Gal, 2016)):

ˆLVA(θ) = −

N
M

(cid:90)

(cid:88)

i∈B

(cid:15)

p((cid:15)) ln p(yi|fg(θ,(cid:15))(xi))d(cid:15) + KL(qθ(ω)||p(ω))

where taking a single sample MC estimate of the integral yields the optimization objective in Eq. 1.

12In an approx. Bayesian view of a NN, qθ(ω) would correspond to the distribution of weights in the network given by some SRT.
13In a NN trained with BN, it is easy to see that g exists if we let (cid:15) represent a mini-batch selection from the training data, since all

BN units’ means and variances are completely determined by (cid:15) and θ.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

1.2. KL Divergence of factorized Gaussians

If qθ(ω) and p(ω) factorize over all stochastic parameters:

KL(qθ(ω)||p(ω)) = −

(cid:2)qθ(ωi)(cid:3) ln

(cid:90)

(cid:90)

(cid:89)

i
(cid:89)

i

(cid:90)

ω

ω

(cid:104)

= −

(cid:2)qθ(ωi)(cid:3) (cid:88)

(cid:88)

=

−

(cid:89)

(cid:2)qθ(ωi)(cid:3) ln

dω

(cid:81)
(cid:81)

i p(ωi)
i qθ(ωi)
(cid:20)

ln

p(ωi)
qθ(ωi)

i

(cid:21) (cid:89)

dωi

i
(cid:89)

i

(cid:105)

dωi

ωi(cid:54)=j

i(cid:54)=j

p(ωj)
qθ(ωj)
(cid:90)

dωj

qθ(ωj) ln

p(ωj)
qθ(ωj)

(cid:89)

qθ(ωi)dωi

(cid:105)

(cid:88)

(cid:104)

(cid:90)

ω

i

ωj

−

(cid:90)

j

j

i

(cid:88)

i
(cid:88)

=

=

=

−

qθ(ωi) ln

ωi

p(ωi)
qθ(ωi)

dωi

KL(qθ(ωi)||p(ωi))

(3)

such that KL(qθ(ω)||p(ω)) is the sum of the KL divergence terms for the individual stochastic parameters ωi. If the
factorized distributions are Gaussians, where qθ(ωi) = N (µq, σ2

q ) and p(ωi) = N (µp, σ2

p) we get:

KL(qθ(ωi)||p(ωi)) =

qθ(ωi) ln

dωi

qθ(ωi)
p(ωi)
(cid:90)

ωi

= − H(qθ(ωi)) −

qθ(ωi) ln p(ωi)dωi

= −

(1 + ln(2πσ2

q ))

(cid:90)

ωi

1
2
(cid:90)

ωi
1
2
1
2

σp
σq

−

qθ(ωi) ln

1
p)1/2
(2πσ2

(cid:110)

exp

−

(ωi − µp)2
2σ2
p

(cid:111)

dωi

(4)

= −

(1 + ln(2πσ2

+

ln(2πσ2

p) +

q ))
Eq[ω2

i ] − 2µpEq[ωi] + µ2
p

2σ2
p

= ln

+

q + (µq − µp)2
σ2
2σ2
p

−

1
2

for each KL divergence term. Here H(qθ(ωi)) = 1

2 (1 + ln(2πσ2

q )) is the differential entropy of qθ(ωi).

1.3. Distribution of µu

B, σu
B

Here we approximate the distribution of mean and standard deviation of a mini-batch, separately to two Gaussians – This
has also been empirically veriﬁed, see Figure 3 for 2 sample plots and the appendix section 1.9 for more. For the mean we
get:

where xm are the examples in the sampled batch. We will assume these are sampled i.i.d.14. Samples of the random
variable W(j)xm are then i.i.d.. Then by central limit theorem (CLT) the following holds for sufﬁciently large M (often
≥ 30):

µB =

ΣM

m=1W(j)xm
M

µB ∼ N (µ,

σ2
M

)

14Although in practice with deep learning, mini-batches are sampled without replacement, stochastic gradient descent samples with

replacement in its standard form.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 3. Batch statistics used to train the network are normal. A one-sample Kolmogorov-Smirnov test checks that µB and σB come
from a standard normal distribution. More examples are available in Appendix 1.9.

For standard deviation:

(cid:114)

σB =

ΣM

m=1(W(j)xm − µB)2
M

√

M (σB − σ) =

M

√

(cid:114)

(cid:16)

ΣM

m=1(W(j)xm − µB)2
M

√

(cid:17)

−

σ2

We want to rewrite
x = ΣM

m=1(W(j)xm−µB)2
M

:

(cid:113) ΣM

m=1(W(j)xm−µB)2
M

. We take a Taylor expansion of f (x) =

x around a = σ2. With

√

Then

so

√

√

x =

σ2 +

1
√
σ2
2

(x − σ2) + O[(x − σ2)2]

√

M (σB − σ) =

M

(cid:32)

√

1
√
σ2
2

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

(cid:34)

O

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

+

− σ2(cid:17)
− σ2(cid:17)2(cid:35)(cid:33)

√

M
2σ

=

=

1
√

2σ

M

+

(cid:16) 1
m=1(W(j)xm − µB)2 − σ2(cid:17)
ΣM
M
(cid:34)√
m=1(W(j)xm − µB)2
M
m=1(W(j)xm − µB)2 − M σ2(cid:17)
ΣM

(cid:16) ΣM

M

O

(cid:16)

+

− σ2(cid:17)2(cid:35)

(cid:34)√

O

M

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

− σ2(cid:17)2(cid:35)

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

consider ΣM

m=1(W(j)xm − µB)2. We know that E[W(j)xm] = µ and write

then

ΣM
=ΣM
=ΣM
=ΣM
=ΣM
=ΣM

m=1(W(j)xm − µB)2
m=1((W(j)xm − µ) − (µB − µ))2
m=1((W(j)xm − µ)2 + (µB − µ)2 − 2(W(j)xm − µ)(µB − µ))
m=1(W(j)xm − µ)2 + M (µB − µ)2 − 2(µB − µ)ΣM
m=1(W(j)xm − µ)2 − M (µB − µ)2
m=1((W(j)xm − µ)2 − (µB − µ)2)

m=1(W(j)xm − µ)

√

M (σB − σ) =

1
√

(cid:16)

m=1((W(j)xm − µ)2 − (µB − µ)2) − M σ2(cid:17)
ΣM

+

2σ

M

(cid:34)√

O

M

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

− σ2(cid:17)2(cid:35)

=

(cid:16)

1
√

2σ

M

ΣM

m=1(W(j)xm − µ)2 − ΣM

(cid:34)√

O

M

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

m=1(µB − µ)2 − M σ2(cid:17)
+
− σ2(cid:17)2(cid:35)

=

(cid:16)

1
√

2σ

M

ΣM

m=1((W(j)xm − µ)2 − σ2) − ΣM

(cid:34)√

O

M

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

m=1(µB − µ)2(cid:17)
− σ2(cid:17)2(cid:35)

+

=

1
√

2σ

−

M
1
√

M

2σ
(cid:34)√

+ O

M

ΣM

m=1((W(j)xm − µ)2 − σ2)

ΣM

m=1(µB − µ)2

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

− σ2(cid:17)2(cid:35)

ΣM

m=1((W(j)xm − µ)2 − σ2)
(cid:125)

(cid:123)(cid:122)
term A

=

2σ
(cid:124)

−

1
√

M

√

M
2σ

(cid:124)

(µB − µ)2
(cid:123)(cid:122)
(cid:125)
term B

(cid:34)√

(cid:16) ΣM

+ O

M

(cid:124)

m=1(W(j)xm − µB)2
M
(cid:123)(cid:122)
term C

− σ2(cid:17)2(cid:35)

(cid:125)

We go through each term in turn

Term A
We have

Term A =

ΣM

m=1((W(j)xm − µ)2 − σ2)

1
√

2σ

M

m=1(W(j)xm − µ)2 is the sum of M RVs (W(j)xm − µ)2. Note that since E[W(j)xm] = µ it holds that
where ΣM
E[(W(j)xm − µ)2] = σ2. Since (W(j)xm − µ)2 is sampled approximately iid (by assumptions above), for large enough

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

M by CLT it holds approximately that

ΣM

m=1(W(j)xm − µ)2 ∼ N (M σ2, M Var((W(j)xm − µ)2))

Var((W(j)xm − µ)2) = E[(W(j)xm − µ)2∗2] − E[(W(j)xm − µ)2]2
= E[(W(j)xm − µ)4] − σ4

ΣM

m=1((W(j)xm − µ)2 − σ2) ∼ N (0, M ∗ E[(W(j)xm − µ)4] − M σ4)

Term A ∼ N (0,

E[(W(j)xm − µ)4] − σ4
4σ2

)

Term B =

(µB − µ)2 =

M (µB − µ)(µB − µ)

√

M
2σ

√

1
2σ

Consider (µB − µ). As µB

p
−→ µ when M → ∞ we have µB − µ

p
−→ 0. We also have

√

M (µB − µ) =

ΣM

m=1W(j)xm
M

√

√

−

M µ

which by CLT is approximately Gaussian for large M . We can then make use of the Cramer-Slutzky Theorem, which
p
states that if (Xn)n≥1 and (Yn)n≥1 are two sequences such that Xn
−→ a as n → ∞ where a is a constant,
then as n → ∞, it holds that Xn ∗ Yn

d−→ X ∗ a. Thus, Term B is approximately 0 for large M.

d−→ X and Yn

(cid:34)√

Term C = O

M

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

− σ2(cid:17)2(cid:35)

Since E[(W(j)xm − µ)2] = σ2 we can make the same use of Cramer-Slutzky as for Term B, such that Term C is approxi-
mately 0 for large M.

Finalizing the distribution
We have approximately

√

M (σB − σ) ∼ N (0,

E[(W(j)xm − µ)4] − σ4
4σ2

)

σB ∼ N (σ,

E[(W(j)xm − µ)4] − σ4
4σ2M

)

Here we make use of the stochasticity from BN modeled in the Appendix section 1.3, to evaluate the implied prior on the
stochastic variables for a BN network. Speciﬁcally, we consider a BN network with fully connected layers and BN applied
to each layer, trained with L2-regularization (weight decay). In the following, we make use of the simplifying assumptions
of no scale and shift tranformations, BN applied to each layer, and independent input units to each layer.

where

Then

so

Term B
We have

Term C
We have

so

1.4. Prior

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Equivalence between the objectives of Eq. (1) and (2) requires:

(5)

(6)

(7)

∂
∂θk

KL(qθ(ω)||p(ω)) = N τ

Ω(θ)

∂
∂θk

∂
∂θk

L
(cid:88)

l=1

= N τ

λl||Wl||2

µu
B

∝
∼ N (µu,

(σu)2
M

),

σu
B

∝
∼ N (σu,

E[(W(u)x − µu)4] − (σu)4
4(σu)2M

)

KL(qθ(ω)||p(ω)) =

KL(qθ(ωi)||p(ωi))

(cid:88)

i

where θk ∈ θ, and θ is the set of weights in the network. To proceed with the LHS of Eq. (5) we ﬁrst need to ﬁnd the
approximate posterior qθ(ω) that batch normalization induces. As shown in Appendix 1.3, with some weak assumptions
and approximations the Central Limit Theorem (CLT) yields Gaussian distributions of the stochastic variables µu
B, for
large enough M . For any BN unit u:

B, σu

where µu and σu are population-level moments (i.e. moments over D).

We assume that qθ(ω) and p(ω) factorize over all stochastic variables.15 We use i as an index of the set of stochastic
variables. As shown in Eq. (3) in Appendix 1.2, the factorized distributions yield:

Note that each BN unit produces two KL(qθ(ωi)||p(ωi)) terms: one for ωi = µu
terms for one particular BN unit u, and drop the index i for brevity. We use a Gaussian prior p(ωi) = N (µp, σ2
consistency, use the notation qθ(ωi) = N (µq, σ2

q ). As shown in Eq. (4) in Appendix 1.2:

B and one for ωi = σu

B. We consider these
p) and, for

Since θk changes during training, a prior cannot depend on θk so ∂
∂θk

(µp) = ∂
∂θk

(σp) = 0. Letting (·)(cid:48) denote ∂
∂θk

(·):

KL(qθ(ωi)||p(ωi)) = ln

+

σp
σq

q + (µq − µp)2
σ2
2σ2
p

−

1
2

∂
∂θk

KL(qθ(ωi)||p(ωi)) =

σqσ(cid:48)

q + µqµ(cid:48)
σ2
p

q − µpµ(cid:48)
q

−

σ(cid:48)
q
σq

We need not consider θk past a previous layer’s BN, since a normalization step is performed before scale and shift. In the
general case with a given Gaussian p(ω), Eq. 7 evaluated on all BN units’ means and standard deviations w.r.t. all θk up to
a previous layer’s BN, would yield an expression for a custom N τ ∂
Ω(θ) that could be used for an exact VI treatment of
∂θk
BN.

In our reconciliation of weight decay however, given our assumptions of no scale and shift and BN applied to each layer,
we need only consider the weights in the same layer as the BN unit. This means that the stochastic variables in layer l are
only affected by weights in θk ∈ W l (i.e. not the scale and shift variables operating on the input to the layer). We denote
a weight connecting the k:th input unit to the u:th BN unit by W(u,k). For such weights, we need to derive µ(cid:48)
q, for
two cases: ωi = µu
B by
µσ,q and σσ,q. Using the distributions modeled in Eq. 6:

q and σ(cid:48)
B by µµ,q and σµ,q, and for σu

B. We denote the priors of the mean and std. dev for µu

B and ωi = σu

15The empirical distributions have been numerically checked to be linearly independent and the joint distribution is close to a bi-variate

Gaussian.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Case 1: ωi = µu
B

= W(u) ¯x

µµ,q =

µ(cid:48)

µ,q =

W(u)x
N

xk
N

= ¯xk

(cid:88)

x∈D
(cid:88)

x∈D

(cid:114)

σµ,q =

(σu)2
M

=

σ(cid:48)
µ,q =

σ−1
q

1
2

(cid:88)

x∈D

(cid:115)

(cid:80)

x∈D(W(u)x − µq)2
N M

2(W(u)x − µq)(xk − ¯xk)
N M

= σ−1
q

(cid:18) K
(cid:88)

i=1

W(u,i)Cov(xi, xk)

M −1

(cid:19)

where there are K input units to the layer.

Case 2: ωi = σu
B

(cid:115)

(cid:80)

µσ,q =

x∈D(W(u)x − µq)2
N
(cid:18) K
(cid:88)

= σµ,qM

1
2

(cid:19)

σ,q = σ−1
µ(cid:48)

µ,qM − 1

2

W(u,i)Cov(xi, xk))

i=1
E[(W(u)x − µu)4] − (σu)4
4(σu)2M

σσ,q =

σ(cid:48)
σ,q =

E[(W(u)x − µu)4](cid:48)σu − 2(σu)4(σu)(cid:48) − 2(σu)(cid:48)E[(W(u)x − µu)4]
4(σu)3M

Combining these results with Eq. 7 we ﬁnd that taking KL(qθ(ωi)||p(ωi)) for the mean and variance of a single BN unit u
wrt the weight from input unit k:

∂

−

σ(cid:48)
µ,q
σµ,q
σ(cid:48)
σ,q
σσ,q

∂

∂W(u,k) KL(qθ(µu
µ,q + µµ,qµ(cid:48)
σµ,qσ(cid:48)

µ,q − µµ,pµ(cid:48)

µ,q

B)||p(µu

B)) +

∂W(u,k) KL(qθ(σu

B)||p(σu

B))

σ,q

σσ,qσ(cid:48)

σ,q − µσ,pµ(cid:48)

σ2
µ,p
σ,q + µσ,qµ(cid:48)
σ2
σ,p
O(M −1) + ¯xkW(u) ¯x − µµ,p ¯xk
σ2
O(M −2) + (cid:80)K
i=1 W(u,i)Cov(xi, xk) − µσ,pO(M − 1
2 )

− O(M −1)

µ,p

−

σ2

σ,p

E[(W(u)x − µu)4](cid:48)σu − 2(σu)4(σu)(cid:48) − 2(σu)(cid:48)E[(W(u)x − µu)4]
E[(W(u)x − µu)4]σu − (σu)5

=

+

=

+

−

where we summarize the terms scaled by M with O-notation. We see that if we let M → ∞, µµ,p = 0, σµ,p → ∞,
µσ,p = 0 and σσ,p is small enough, then:

(cid:18)

∂
∂W(u,k)

KL(qθ(µu

B)||p(µu

B)) + KL(qθ(σu

B)||p(σu

B))

≈

(cid:19)

(cid:80)K

i=1 W(u,i)Cov(xi, xk)

σ2

σ,p

such that each BN layer yields the following:

(cid:88)

K
(cid:88)

u

i=1

(cid:18)

∂
∂W(u,i)

KL(qθ(µu

B)||p(µu

B)) + KL(qθ(σu

B)||p(σu

B))

(cid:19)

(cid:88)

≈

u

(cid:80)K

i=1 Wu,i (cid:80)K
σ2

σ,p,u

i2=1 Cov(xi, xi2)

(8)

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

where we denote the prior for the std. dev. of the std. dev. of BN unit u by σσ,p,u. Given our assumptions of no scale and
shift from the previous layer, and independent input features in every layer, Eq. 8 reduces to:

(cid:88)

K
(cid:88)

u

i=1

Wu,i
σ2

σ,p

if the same prior is chosen for each BN unit in the layer. We therefore ﬁnd that Eq. 5 is reconciled by p(µu
and p(σu

is small enough, which is the case if N is large.

), if

B) → N (0,

1
2N τ λl

1
2N τ λl

B) → N (0, ∞)

1.5. predictive distribution properties

This section provides derivations of properties of the predictive distribution p∗(y|x, D) in section 3.4, following (Gal,
2016). We ﬁrst ﬁnd the ﬁrst two modes of the approximate predictive distribution (with the second mode applicable to
regression), then show how to estimate the predictive log likelihood, a measure of uncertainty quality used in the evaluation.

Predictive mean Assuming Gaussian iid noise deﬁned by model precision τ ,
N (y; fω(x), τ −1I):

i.e.

fω(x, y) = p(y|fω(x)) =

fω(x, y)qθ(ω)dω

dy

(cid:17)

N (y; fω(x), τ −1I)qθ(ω)dω

dy

(cid:17)

yN (y; fω(x), τ −1I)dy

qθ(ω)dω

(cid:17)

Ep∗ [y] =

yp∗(y|x, D)dy

(cid:16) (cid:90)

ω
(cid:16) (cid:90)

y

y

ω

(cid:16) (cid:90)

y

(cid:90)

(cid:90)

y
(cid:90)

y
(cid:90)

ω

(cid:90)

ω

=

=

=

=

≈

fω(x)qθ(ω)dω

1
T

T
(cid:88)

i=1

f ˆωi(x)

where we take the MC Integral with T samples of ω for the approximation in the ﬁnal step.

Predictive variance For regression, our goal is to estimate:

Covp∗ [y] = Ep∗ [y(cid:124)y] − Ep∗ [y](cid:124)Ep∗ [y]

We ﬁnd that:

Ep∗ [y(cid:124)y] =

y(cid:124)yp∗(y|x, D)dy

(cid:90)

y
(cid:90)

y
(cid:90)

ω

(cid:90)

ω

(cid:90)

ω

=

=

=

=

(cid:16) (cid:90)

ω

y(cid:124)y

(cid:16) (cid:90)

y

fω(x, y)qθ(ω)dω

dy

(cid:17)

y(cid:124)yfω(x, y)dy

qθ(ω)dω

(cid:17)

(cid:16)

(cid:17)
Covfω(x,y)(y) + Efω(x,y)[y](cid:124)Efω(x,y)[y]

qθ(ω)dω

(cid:16)

(cid:17)
τ −1I + fω(x)(cid:124)fω(x)

qθ(ω)dω

= τ −1I + Eqθ (ω)[fω(x)(cid:124)fω(x)]

≈ τ −1I +

f ˆωi (x)(cid:124)f ˆωi(x)

1
T

T
(cid:88)

i=1

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

where we use MC integration with T samples for the ﬁnal step. The predictive covariance matrix is given by:

Covp∗ [y] ≈ τ −1I +

f ˆωi (x)(cid:124)f ˆωi(x) − Ep∗ [y](cid:124)Ep∗ [y]

1
T

T
(cid:88)

i=1

which is the sum of the variance from observation noise and the sample covariance from T stochastic forward passes
though the network.

The form of p∗ can be approximated by a Gaussian for each output dimension (for regression). We assume bounded
domains for each input dimension, wide layers throughout the network, and a uni-modal distribution of weights centered
at 0. By the Liapounov CLT condition, the ﬁrst layer then receives approximately Gaussian inputs (a proof can be found
in (Lehmann, 1999)). Having sampled µu
B from a mini-batch, each BN unit’s output is bounded. CLT thereby
continues to hold for deeper layers, including fω(x) = WLxL. A similar motivation for a Gaussian approximation of
Dropout has been presented by (Wang & Manning, 2013).

B and σu

Predictive Log Likelihood We use the Predictive Log Likelihood (PLL) as a measure to estimate the model’s uncertainty
quality. For a certain test point (yi, xi), the PLL deﬁnition and approximation can be expressed as:

PLL(fω(x), (yi, xi)) = log p(yi|fω(xi))

= log

fω(xi, yi)p(ω|D)dω

≈ log

fω(xi, yi)qθ(ω)dω

≈ log

p(yi|f ˆωj (xi))

(cid:90)

(cid:90)

1
T

T
(cid:88)

j=1

where ˆωj represents a sampled set of stochastic parameters from the approximate posterior distrubtion qθ(ω) and we take
a MC integration with T samples. For regression, due to the iid Gaussian noise, we can further develop the derivation into
the form we use when sampling:

PLL(fω(x), (yi, xi)) = log

N (yi|f ˆωj (xi), τ −1I)

1
T

T
(cid:88)

j=1

= logsumexpj=1,...,T
1
2

− log T −

log 2π +

1
2

1
2

log τ

(cid:0) −

τ ||yi − f ˆωj (xi)||2(cid:1)

Note that PLL makes no assumption on the form of the approximate predictive distribution.

1.6. Data

To assess the uncertainty quality of the various methods studied we rely on eight standard regression datasets, listed in Table
3. Publicly available from the UCI Machine Learning Repository (University of California, 2017) and Delve (Ghahramani,
1996), these datasets have been used to benchmark comparative models in recent related literature (see (Hern´andez-Lobato
& Adams, 2015), (Gal & Ghahramani, 2015), (Bui et al., 2016) and (Li & Gal, 2017)).

For image classiﬁcation, we applied MCBN using ResNet32 to CIFAR10.

For the image segmentation task, we applied MCBN using Bayesian SegNet on data from CamVid and PASCAL-VOC
using models published in (Kendall et al., 2015).

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Table 3. Regression dataset summary. Properties of the eight regression datasets used to evaluate MCBN. N is the dataset size and Q
is the n.o. input features. Only one target feature was used – we used heating load for the Energy Efﬁciency dataset, which contains
multiple target features.

Dataset name

Boston Housing
Concrete Compressive Strength
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein Tertiary Structure
Wine Quality (Red)
Yacht Hydrodynamics

N

506
1,030
768
8,192
9,568
45,730
1,599
308

Q

13
8
8
8
4
9
11
6

1.7. Extended experimental results

Below, we provide extended results measuring uncertainty quality. In Tables 4 and 5, we provide tables showing the mean
CRPS and PLL values for MCBN and MCDO. These results indicate that MCBN performs on par or better than MCDO
across several datasets. In Table 6 we provide the raw PLL and CRPS results for MCBN and MCDO. In Table 7 we provide
RMSE results of the MCBN and MCDO networks in comparison with non-stochastic BN and DO networks. These results
indicate that the procedure of multiple forward passes in MCBN and MCDO show slight improvements in the accuracy of
the network.

In Figure 4 and Figure 5, we provide a full set of our uncertainty quality visualization plots, where errors in predictions
are sorted by estimated uncertainty. The shaded areas show the model uncertainty and gray dots show absolute prediction
errors on the test set. A gray line depicts a running mean of the errors. The dashed line indicates the optimized constant
uncertainty. In these plots, we can see a correlation between estimated uncertainty (shaded area) and mean error (gray).
This trend indicates that the model uncertainty estimates can recognize samples with larger (or smaller) potential for
predictive errors.

We also conduct a sensitivity analysis to estimate how the uncertainty quality varies with batch size M and the number of
stochastic forward passes T . In tables 8 and 9 we evaluate CRPS and PLL respectively for the regression datasets when
trained and evaluated with varying batch sizes, but other hyperparameters ﬁxed (T was ﬁxed at 100). The results show that
results deteriorate when batch sizes are too small, likely stemming from the large variance of the approximate posterior.
In tables 10 and 11 we evaluate CRPS and PLL respectively for the regression datasets when trained and evaluated with
varying n.o. stochastic forward samples, but other hyperparameters ﬁxed (M was ﬁxed at 128). The results are indicative
of performance improvements with larger T , although we see improvements over baseline for some datasets already with
T = 50 (1/10:th of the T used in our main experiments).

Table 4. Uncertainty quality measured by CRPS on regression dasets. CRPS measured on eight datasets over 5 random 80-20 splits
of the data with 5 different random seeds each split. Mean values for MCBN, MCDO and MNF are reported along with standard error.
A signiﬁcance test was performed to check if CRPS signiﬁcantly exceeds the baseline. The p-value from a one sample t-test is reported.

CRPS

Dataset

MCBN

p-value

MCDO

p-value

MNF

p-value

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

8.50±0.86
3.91±0.25
5.75±0.52
2.85±0.18
0.24±0.05
2.66±0.10
0.26±0.07
-56.39±14.27

6.39E-10
4.53E-14
6.71E-11
2.33E-14
2.32E-04
2.77-12
1.26E-03
5.94E-04

3.06±0.33
0.93±0.41
1.37±0.89
1.82±0.14
-0.44±0.05
0.99±0.08
2.00±0.21
21.42±2.99

1.64E-09
3.13E-02
1.38E-01
1.64E-12
2.17E-08
2.34E-12
1.83E-09
2.16E-07

5.88±1.09
3.13±0.81
1.10±2.63
0.52±0.26
-0.89±0.15
0.57±0.03
0.93±0.12
24.92±3.77

2.01E-05
6.43E-04
6.45E-01
7.15E-02
3.36E-06
8.56E-16
6.19E-08
9.62E-06

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Table 5. Uncertainty quality measured by PLL on regression dasets. PLL measured on eight datasets over 5 random 80-20 splits of
the data with 5 different random seeds each split. Mean values for MCBN, MCDO and MNF are reported along with standard error. A
signiﬁcance test was performed to check if PLL signiﬁcantly exceeds the baseline. The p-value from a one sample t-test is reported.

PLL

Dataset

MCBN

p-value

MCDO

p-value

MNF

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

10.49±1.35
-36.36±12.12
10.89±1.16
1.68±0.37
0.33±0.14
2.56±0.23
0.19±0.09
45.58±5.18

5.41E-08
6.19E-03
1.79E-09
1.29E-04
2.72E-02
4.28E-11
3.72E-02
5.67E-09

5.51±1.05
10.92±1.78
-14.28±5.15
-0.26±0.18
3.52±0.23
6.23±0.19
2.91±0.35
-41.54±31.37

2.20E-05
2.34E-06
1.06E-02
1.53E-01
1.12E-13
2.57E-21
1.84E-08
1.97E-01

1.76±1.12
-2.16±4.19
-33.88±29.57
0.42±0.43
-0.86±0.15
0.52±0.07
0.83±0.16
46.19±4.45

p-value

1.70E-01
6.79E-01
2.70E-01
2.70E-01
7.33E-06
1.81E-07
2.27E-05
2.47E-07

Table 6. Raw (unnormalized) CRPS and PLL scores on regression datasets. CRPS and PLL measured on eight datasets over 5
random 80-20 splits of the data with 5 different random seeds each split. Mean values and standard errors are reported for MCBN,
MCDO and MNF.

Dataset

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

MCBN

1.45±0.02
2.40±0.04
0.33±0.01
0.04±0.00
2.00±0.01
1.95±0.01
0.34±0.00
0.68±0.02

CRPS
MCDO

1.41±0.02
2.42±0.04
0.26±0.00
0.04±0.00
2.00±0.01
1.95±0.00
0.33±0.00
0.32±0.01

MNF

MCBN

1.57±0.02
3.61±0.02
1.33±0.04
0.05±0.00
2.31±0.01
2.25±0.01
0.34±0.00
0.94±0.01

-2.38±0.02
-3.45±0.11
-0.94±0.04
1.21±0.01
-2.75±0.00
-2.73±0.00
-0.95±0.01
-1.39±0.03

PLL
MCDO

-2.35±0.02
-2.94±0.02
-0.80±0.04
1.24±0.00
-2.72±0.01
-2.70±0.00
-0.89±0.01
-2.57±0.69

MNF

-2.51±0.06
-3.35±0.04
-3.18±0.07
1.04±0.00
-2.86±0.01
-2.83±0.01
-0.93±0.00
-1.96±0.05

Table 7. Prediction accuracy measured by RMSE on regression datasets. RMSE measured on eight datasets over 5 random 80-20
splits of the data with 5 different random seeds each split. Mean values and standard errors are reported for for MCBN, MCDO and
MNF as well as conventional non-Bayesian models BN and DO.

Dataset

MCBN

BN

DO

MNF

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

2.75±0.05
4.78±0.09
0.59±0.02
0.07±0.00
3.74±0.01
3.66±0.01
0.62±0.00
1.23±0.05

2.77±0.05
4.89±0.08
0.57±0.01
0.07±0.00
3.74±0.01
3.69±0.01
0.62±0.00
1.28±0.06

RMSE
MCDO

2.65±0.05
4.80±0.10
0.47±0.01
0.07±0.00
3.74±0.02
3.66±0.01
0.60±0.00
0.75±0.03

2.69±0.05
4.99±0.10
0.49±0.01
0.07±0.00
3.72±0.02
3.68±0.01
0.61±0.00
0.72±0.04

2.98±0.06
6.57±0.04
2.38±0.07
0.09±0.00
4.19±0.01
4.10±0.01
0.61±0.00
2.13±0.05

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 4. Errors in predictions (gray dots) sorted by estimated uncertainty on select datasets. The shaded areas show model uncertainty
(light area 95% CI, dark area 50% CI). Gray dots show absolute prediction errors on the test set, and the gray line depicts a running
mean of the errors. The dashed line indicates the optimized constant uncertainty. A correlation between estimated uncertainty (shaded
area) and mean error (gray) indicates the uncertainty estimates are meaningful for estimating errors.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 5. Errors in predictions (gray dots) sorted by estimated uncertainty on select datasets. The shaded areas show model uncertainty
(light area 95% CI, dark area 50% CI). Gray dots show absolute prediction errors on the test set, and the gray line depicts a running
mean of the errors. The dashed line indicates the optimized constant uncertainty. A correlation between estimated uncertainty (shaded
area) and mean error (gray) indicates the uncertainty estimates are meaningful for estimating errors.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Table 8. Uncertainty quality sensitivity to batch size. A sensitivity analysis to determine how MCBN uncertainty quality varies with
batch size is measured on eight regression datasets using CRPS as the quality measure. Results are measured over 3 random 80-20 splits
of the data with 5 different random seeds each split.

CRPS
64

Batch size

8

16

32

128

256

512

1024

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

-7.1
-34.5
-61.6
-1.4
-10.5
14.5
2.2
15.1

16.6
6.0
-3.0
-4.3
0.8
4.8
1.6
-23.0

11.8
5.0
2.7
0.2
0.0
3.6
0.6
-30.4

7.2
5.1
9.8
2.8
-0.1
2.8
0.6
21.0

2.5
2.9
11.1
2.7
0.0
2.5
0.3
34.4

0.9
1.4
0.8
1.7
0.0
1.6
0.0
-

-
0.6
4.9
0.9
0.2
1.0
0.2
-

-
0.0
-
0.5
0.0
0.5
0.0
-

Table 9. Uncertainty quality sensitivity to batch size. A sensitivity analysis to determine how MCBN uncertainty quality varies with
batch size is measured on eight regression datasets using PLL as the quality measure. Results are measured over 3 random 80-20 splits
of the data with 5 different random seeds each split.

Batch size

8

16

32

128

256

512

1024

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

13.9
-113.3
-64.4
-4.9
-135.0
44.9
2.2
99.6

-36.7
-528.4
5.2
-5.4
-1.4
15.7
2.0
74.9

10.0
-10.0
-0.2
-3.1
-1.0
4.6
0.0
76.8

PLL
64

7.9
2.9
-9.6
1.6
-1.1
2.9
0.5
48.5

3.7
0.0
-14.5
2.3
-0.4
2.8
0.6
44.9

1.5
1.4
1.4
1.5
0.1
2.2
0.4
-

-
0.2
10.4
0.7
-0.1
1.2
0.0
-

-
0.0
-
0.4
0.4
0.6
0.0
-

Table 10. Uncertainty quality sensitivity to n.o. stochastic forward passes. A sensitivity analysis to determine how MCBN uncer-
tainty quality varies with the n.o. stochastic forward passes measured on eight regression datasets using CRPS as the quality measure.
Results are measured over 3 random 80-20 splits of the data with 5 different random seeds each split.

Forward passes

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

CRPS
100

2.7
2.3
4.2
2.7
0.5
2.7
-0.4
32.2

50

3.2
3.3
7.9
4.2
0.1
2.4
0.6
32.1

250

6.1
3.3
13.2
3.2
0.2
2.3
0.9
32.9

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Table 11. Uncertainty quality sensitivity to n.o. stochastic forward passes. A sensitivity analysis to determine how MCBN uncer-
tainty quality varies with the n.o. stochastic forward passes measured on eight regression datasets using PLL as the quality measure.
Results are measured over 3 random 80-20 splits of the data with 5 different random seeds each split.

Forward passes

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

250

7.8
3.8
15.7
2.5
-0.9
1.8
1.7
38.0

PLL
100

1.9
7.1
-30.5
2.2
0.7
2.0
-0.9
35.9

50

2.6
0.1
-47.3
3.4
-0.9
2.4
1.1
35.5

1.8. Uncertainty in image segmentation

We applied MCBN to an image segmentation task using Bayesian SegNet with the main CamVid and PASCAL-VOC
models in (Kendall et al., 2015). Here, we provide more image from Pascal VOC dataset in Figure 6.

1.9. Batch normalization statistics

In Figure 7 and Figure 8, we provide statistics on the batch normalization parameters used for training. The plots show the
distribution of BN mean and BN variance over different mini-batches of an actual training of Yacht dataset for one unit in
the ﬁrst hidden layer and the second hidden layer. Data is provided for different epochs and for different batch sizes.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 6. Uncertainty in image segmentation. Results applying MCBN to Bayesian SegNet (Kendall et al., 2015) on images from
PASCAL-VOC (right). Left: original. Middle: the Bayesian estimated segmentation. Right: estimated uncertainty using MCBN for all
classes. Mini-batches of size 36 were used for PASCAL-VOC on images of size 224x224. 20 inferences were conducted to estimate the
mean and variance of MCBN.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 7. The distribution of means of mini-batches during training of one of our datasets. The distribution closely follows our analyt-
ically approximated Gaussian distribution. The data is collected for one unit of each layer and is provided for different epochs and for
different batch sizes.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 8. The distribution of standard deviation of mini-batches during training of one of our datasets. The distribution closely follows
our analytically approximated Gaussian distribution. The data is collected for one unit of each layer and is provided for different epochs
and for different batch sizes.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

References

Bui, T. D., Hern´andez-Lobato, D., Li, Y., Hern´andez-Lobato, J. M., and Turner, R. E. Deep Gaussian Processes for

Regression using Approximate Expectation Propagation. In ICML, 2016.

Chen, X., Kundu, K., Zhang, Z., Ma, H., Fidler, S., and Urtasun, R. Monocular 3d object detection for autonomous driving.

In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2147–2156, 2016.

Djuric, U., Zadeh, G., Aldape, K., and Diamandis, P. Precision histology: how deep learning is poised to revitalize

histomorphology for personalized cancer care. npj Precision Oncology, 1(1):22, 2017.

Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau, H. M., and Thrun, S. Dermatologist-level classiﬁcation

of skin cancer with deep neural networks. Nature, Feb 2017.

Gal, Y. Uncertainty in Deep Learning. PhD thesis, University of Cambridge, 2016.

Gal, Y. and Ghahramani, Z. Dropout as a Bayesian Approximation : Representing Model Uncertainty in Deep Learning.

ICML, 48:1–10, 2015.

data/kin/desc.html.

Ghahramani, Z. Delve Datasets. University of Toronto, 1996. URL http://www.cs.toronto.edu/{˜}delve/

Ghahramani, Z. Probabilistic machine learning and artiﬁcial intelligence. Nature, 521(7553):452–459, May 2015.

Gneiting, T. and Raftery, A. E. Strictly Proper Scoring Rules, Prediction, and Estimation. Journal of the American

Statistical Association, 102(477):359–378, 2007.

Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples.

arXiv preprint

arXiv:1412.6572, 2014.

Graves, A. Practical Variational Inference for Neural Networks. NIPS, 2011.

Hern´andez-Lobato, J. M. and Adams, R. Probabilistic backpropagation for scalable learning of bayesian neural networks.

In International Conference on Machine Learning, pp. 1861–1869, 2015.

Hinton, G. E. and Van Camp, D. Keeping the neural networks simple by minimizing the description length of the weights.

In Proceedings of the sixth annual conference on Computational learning theory, pp. 5–13. ACM, 1993.

Ioffe, S. Batch renormalization: Towards reducing minibatch dependence in batch-normalized models. CoRR,

abs/1702.03275, 2017. URL http://arxiv.org/abs/1702.03275.

Ioffe, S. and Szegedy, C. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.

Arxiv, 2015. URL http://arxiv.org/abs/1502.03167.

Karpathy, A. Convnetjs demo: toy 1d regression, 2015. URL http://cs.stanford.edu/people/karpathy/

convnetjs/demo/regression.html.

Kendall, A., Badrinarayanan, V., and Cipolla, R. Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-
Decoder Architectures for Scene Understanding. CoRR, abs/1511.0, 2015. URL http://arxiv.org/abs/1511.
02680.

Kingma, D. P. and Welling, M. Auto-Encoding Variational Bayes. In ICLR, 2014.

Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. 2009.

Krueger, D., Huang, C.-W., Islam, R., Turner, R., Lacoste, A., and Courville, A. Bayesian hypernetworks. arXiv preprint

arXiv:1710.04759, 2017.

Lehmann, E. L. Elements of Large-Sample Theory. Springer Verlag, New York, 1999. ISBN 0387985956.

Li, Y. and Gal, Y. Dropout Inference in Bayesian Neural Networks with Alpha-divergences. arXiv, 2017.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Louizos, C. and Welling, M. Multiplicative normalizing ﬂows for variational Bayesian neural networks. In Precup, D. and
Teh, Y. W. (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings
of Machine Learning Research, pp. 2218–2227, International Convention Centre, Sydney, Australia, 06–11 Aug 2017.
PMLR. URL http://proceedings.mlr.press/v70/louizos17a.html.

MacKay, D. J. A practical bayesian framework for backpropagation networks. Neural computation, 4(3):448–472, 1992.

Neal, R. M. Bayesian Learning for Neural Networks. PhD thesis, University of Toronto, 1995.

Neal, R. M. Bayesian learning for neural networks, volume 118. Springer Science & Business Media, 2012.

Selten, R. Axiomatic characterization of the quadratic scoring rule. Experimental Economics, 1(1):43–62, 1998.

Shen, L. End-to-end training for whole image breast cancer diagnosis using an all convolutional design. arXiv preprint

University of California, I. UC Irvine Machine Learning Repository, 2017. URL https://archive.ics.uci.

arXiv:1708.09427, 2017.

edu/ml/index.html.

Wang, S. I. and Manning, C. D. Fast dropout training. Proceedings of the 30th International Conference on Machine Learn-
ing, 28:118–126, 2013. URL http://machinelearning.wustl.edu/mlpapers/papers/wang13a.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

8
1
0
2
 
l
u
J
 
6
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
5
5
4
6
0
.
2
0
8
1
:
v
i
X
r
a

Mattias Teye 1 2 * Hossein Azizpour 1 * Kevin Smith 1 3

Abstract
We show that training a deep network using batch
normalization is equivalent to approximate infer-
ence in Bayesian models. We further demon-
strate that this ﬁnding allows us to make mean-
ingful estimates of the model uncertainty us-
ing conventional architectures, without modiﬁ-
cations to the network or the training proce-
dure. Our approach is thoroughly validated by
measuring the quality of uncertainty in a series
of empirical experiments on different tasks.
It
outperforms baselines with strong statistical sig-
niﬁcance, and displays competitive performance
with recent Bayesian approaches.

1. Introduction

Deep learning has dramatically advanced the state of the
art in a number of domains. Despite their unprecedented
discriminative power, deep networks are prone to make
mistakes. Nevertheless, they can already be found in set-
tings where errors carry serious repercussions such as au-
tonomous vehicles (Chen et al., 2016) and high frequency
trading. We can soon expect automated systems to screen
for various types of cancer (Esteva et al., 2017; Shen, 2017)
and diagnose biopsies (Djuric et al., 2017). As autonomous
systems based on deep learning are increasingly deployed
in settings with the potential to cause physical or economic
harm, we need to develop a better understanding of when
we can be conﬁdent in the estimates produced by deep net-
works, and when we should be less certain.

Standard deep learning techniques used for supervised
learning lack methods to account for uncertainty in the
model. This can be problematic when the network en-
counters conditions it was not exposed to during training,

* Co-ﬁrst authorship 1School of Electrical Engineering and
Computer Science, KTH Royal Institute of Technology, Stock-
holm, Sweden 2Current address: Electronic Arts, SEED, Stock-
holm, Sweden. This work was carried out at Budbee AB.
3Science for Life Laboratory. Correspondence to: Kevin Smith
<ksmith@kth.se>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

or if the network is confronted with adversarial examples
(Goodfellow et al., 2014). When exposed to data outside
the distribution it was trained on, the network is forced to
extrapolate, which can lead to unpredictable behavior.

If the network can provide information about its uncer-
tainty in addition to its point estimate, disaster may be
avoided. In this work, we focus on estimating such pre-
dictive uncertainties in deep networks (Figure 1).

The Bayesian approach provides a theoretical framework
for modeling uncertainty (Ghahramani, 2015), which has
prompted several attempts to extend neural networks (NN)
into a Bayesian setting. Most notably, Bayesian neural net-
works (BNNs) have been studied since the 1990’s (Neal,
2012), but do not scale well and struggle to compete with
modern deep learning architectures. Recently, (Gal &
Ghahramani, 2015) developed a practical solution to obtain
uncertainty estimates by casting dropout training in con-
ventional deep networks as a Bayesian approximation of a
Gaussian Process (its correspondence to a general approx-
imate Bayesian model was shown in (Gal, 2016)). They
showed that any network trained with dropout is an ap-
proximate Bayesian model, and uncertainty estimates can
be obtained by computing the variance on multiple predic-
tions with different dropout masks.

The inference in this technique, called Monte Carlo
Dropout (MCDO), has an attractive quality: it can be ap-
plied to any pre-trained networks with dropout layers. Un-
certainty estimates come (nearly) for free. However, not all
architectures use dropout, and most modern networks have
adopted other regularization techniques. Batch normaliza-
tion (BN), in particular, has become widespread thanks to
its ability to stabilize learning with improved generalization
(Ioffe & Szegedy, 2015).

An interesting aspect of BN is that the mini-batch statis-
tics used for training each iteration depend on randomly
selected batch members. We exploit this stochasticity and
show that training using batch normalization, like dropout,
can be cast as an approximate Bayesian inference. We
demonstrate how this ﬁnding allows us to make meaning-
ful estimates of the model uncertainty in a technique we
call Monte Carlo Batch Normalization (MCBN) (Figure 1).
The method we propose can be applied to any network us-
ing standard batch normalization.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Gaussian Processes show superior performance in terms of
RMSE and uncertainty quality compared to state-of-the-art
approximate BNNs (Bui et al., 2016)1. Another recent ap-
proach to Bayesian learning, Bayesian hypernetworks, use
a NN to learn a distribution of parameters over another net-
work (Krueger et al., 2017). Multiplicative Normalizing
Flows for variational Bayesian networks (MNF) (Louizos
& Welling, 2017) is a recent model that formulates a pos-
terior dependent on auxiliary variables. MNF achieves a
highly ﬂexible posterior by the application of normalizing
ﬂows to the auxiliary variables.

Although these recent techniques address some of the dif-
ﬁculties with approximate BNNs, they all require modiﬁ-
cations to the architecture or the way networks are trained,
as well as specialized knowledge from practitioners. Re-
cently, (Gal & Ghahramani, 2015) showed that a network
trained with dropout implicitly performs the VI objective.
Therefore any network trained with dropout can be treated
as an approximate Bayesian model by making multiple
predictions through the network while sampling different
dropout masks for each prediction. The mean and variance
of the predictions are used in the estimation of the mean
and variance of the predictive distribution 2.

3. Method

In the following, we introduce Bayesian models and a vari-
ational approximation using Kullback-Leibler (KL) diver-
gence following (Gal, 2016). We continue by showing that
a batch normalized deep network can be seen as an ap-
proximate Bayesian model. Employing theoretical insights
and empirical analysis, we study the induced prior on the
parameters when using batch normalization. Finally, we
describe the procedure for estimating the uncertainty of a
batch normalized network’s output.3

3.1. Bayesian Modeling

We assume a ﬁnite training set D = {(xi, yi)}i=1:N where
each (xi, yi) is a sample-label pair. Using D, we are inter-
ested in learning an inference function fω(x, y) with pa-
rameters ω. In deterministic models, the estimated label ˆy
is obtained as follows:

ˆy = arg max

fω(x, y)

y

In probabilistic models we let fω(x, y) = p(y|x, ω). In
Bayesian modeling, in contrast to ﬁnding a point estimate

1By uncertainty quality, we refer to predictive probability dis-

tributions as measured by PLL and CRPS.

2This technique is referred to as “MC Dropout” in the original

work, though we refer to it here as MCDO.

3While the method applies to FC or Conv layers, the induced

Figure 1. Training a deep network using batch normalization
is equivalent to approximate inference in Bayesian models.
Thus, uncertainty estimates can be obtained from any network
using BN through a simple procedure. At inference, several mini-
batches are constructed by taking random samples to accompany
the query. The mean and variance of the outputs are used to esti-
mate the predictive distribution (MCBN). Here, we show results
on a toy dataset from a network with three hidden layers (30 units
per layer). Training data is depicted as dots. The solid line is the
predictive mean of 500 stochastic forward passes and the shaded
areas represent the model’s uncertainty. The dashed lines depict a
minimal baseline for uncertainty (CUBN), see Section 4.1.

We validate our approach by empirical experiments on a
variety of datasets and tasks, including regression and im-
age classiﬁcation. We measure uncertainty quality relative
to a baseline of ﬁxed uncertainty, and show that MCBN
outperforms the baseline on nearly all datasets with strong
statistical signiﬁcance. We also show that the uncertainty
quality of MCBN is on par with other recent approximate
Bayesian networks.

2. Related Work

Bayesian models provide a natural framework for model-
ing uncertainty, and several approaches have been devel-
oped to adapt NNs to Bayesian reasoning. A common ap-
proach is to place a prior distribution (often a Gaussian)
over each parameter. The resulting model corresponds to
a Gaussian process for inﬁnite parameters (Neal, 1995),
and a Bayesian NN (MacKay, 1992) for a ﬁnite number of
parameters. Inference in BNNs is difﬁcult however (Gal,
2016), so focus has thus shifted to techniques that approx-
imate the posterior, approximate BNNs. Methods based on
variational inference (VI) typically rely on a fully factor-
ized approximate distribution (Kingma & Welling, 2014;
Hinton & Van Camp, 1993), but often do not scale. To alle-
viate these difﬁculties, (Graves, 2011) proposed a model
using sampling methods to estimate a factorized poste-
rior. Probabilistic backpropagation (PBP), estimates a fac-
torized posterior via expectation propagation (Hern´andez-
Lobato & Adams, 2015).

Using several strategies to address scaling issues, Deep

prior from weight decay (Section 3.3) is studied for FC layers.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

of the model parameters, the idea is to estimate an (ap-
proximate) posterior distribution of the model parameters
p(ω|D) to be used for probabilistic prediction:

(cid:90)

p(y|x, D) =

fω(x, y)p(ω|D)dω

The predicted label, ˆy, can then be accordingly obtained by
sampling p(y|x, D) or taking its maxima.

Variational Approximation In approximate Bayesian
modeling, a common approach is to learn a parame-
terized approximating distribution qθ(ω) that minimizes
KL(qθ(ω)||p(ω|D)); the Kullback-Leibler divergence of
the true posterior w.r.t. its approximation. Minimizing this
KL divergence is equivalent to the following minimization
while being free of the data term p(D) 4:

LVA(θ) := −

qθ(ω) ln fω(xi, yi)dω

N
(cid:88)

(cid:90)

i=1

+ KL(qθ(ω)||p(ω))

During optimization, we want to take the derivative of the
expected likelihood w.r.t. the learnable parameters θ. We
use the same MC estimate as in (Gal, 2016) (explained in
Appendix Section 1.1), such that one realized ˆωi is taken
for each sample i 5. Optimizing over mini-batches of size
M , the approximated objective becomes:

ˆLVA(θ) := −

ln f ˆωi (xi, yi) + KL(qθ(ω)||p(ω))

(1)

N
M

M
(cid:88)

i=1

The ﬁrst term is the data likelihood and the second term
is the divergence of the prior w.r.t. the approximated poste-
rior.

3.2. Batch Normalized Deep Nets as Bayesian Modeling

We now describe the optimization procedure of a deep net-
work with batch normalization and draw the resemblance
to the approximate Bayesian modeling in Eq (1).

The inference function of a feed-forward deep network
with L layers can be described as:

where a(.) is an element-wise nonlinearity function and
Wl is the weight vector at layer l. Furthermore, we de-
note the input to layer l as xl with x1 = x and we then set
hl = Wlxl. Parenthesized super-index for matrices (e.g.
W(j)) and vectors (e.g. x(j)) indicates jth row and element
respectively. Super-index u refers to a speciﬁc unit at layer
l, (e.g. Wu = Wl,(j), hu = hl,(j)). 6

Batch Normalization Each layer of a deep network is
constructed by several linear units whose parameters are
the rows of the weight matrix W. Batch normalization is
a unit-wise operation proposed in (Ioffe & Szegedy, 2015)
to standardize the distribution of each unit’s input. For FC
layers, it converts a unit’s input hu in the following way:

ˆhu =

hu − E[hu]
(cid:112)Var[hu]

where the expectations are computed over the training
set during evaluation, and mini-batch during training (in
deep networks, the weight matrices are often optimized us-
ing back-propagated errors calculated on mini-batches of
data)7. Therefore, during training, the estimated mean and
variance on the mini-batch B is used, which we denote by
µB and σB respectively. This makes the inference at train-
ing time for a sample x a stochastic process, varying based
on other samples in the mini-batch.

Loss Function and Optimization Training deep net-
works with mini-batch optimization involves a (regular-
ized) risk minimization with the following form:

LRR(ω) :=

l(ˆyi, yi) + Ω(ω)

1
M

M
(cid:88)

i=1

where the ﬁrst term is the empirical loss on the training
data and the second term is a regularization penalty act-
ing as a prior on model parameters ω.
If the loss l is
cross-entropy for classiﬁcation or sum-of-squares for re-
gression problems (assuming i.i.d. Gaussian noise on la-
bels), the ﬁrst term is equivalent to minimizing the negative
log-likelihood:

fω(x) = WLa(WL−1...a(W2a(W1x))

LRR(ω) := −

ln fω(xi, yi) + Ω(ω)

4Achieved by constructing the Evidence Lower Bound, called
ELBO, and assuming i.i.d. observation noise; details can be found
in Appendix Section 1.1.

5While a MC integration using a single sample is a weak ap-
proximation, in an iterative optimization for θ several samples
will be taken over time.

6For a (softmax) classiﬁcation network, fω(x) is a vector with
fω(x, y) = fω(x)(y), for regression networks with i.i.d. Gaus-
sian noise we have fω(x, y) = N (fω(x), τ −1I).

7It also learns an afﬁne transformation for each unit with pa-
afﬁne = γ(j) ˆx(j) + β(j).

rameters γ and β, omitted for brevity: ˆx(j)

1
M τ

M
(cid:88)

i=1

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

with τ = 1 for classiﬁcation.
In a network
with batch normalization, the model parameters include
{W1:L, γ1:L, β1:L, µ1:L
B }. If we decouple the learn-
able parameters θ = {W1:L, γ1:L, β1:L} from the stochas-
tic parameters ω = {µ1:L
B }, we get the following ob-
jective at each step of the mini-batch optimization:

B , σ1:L

B , σ1:L

LRR(θ) := −

ln f{θ, ˆωi}(xi, yi) + Ω(θ)

(2)

1
M τ

M
(cid:88)

i=1

where ˆωi is the means and variances for sample i’s mini-
batch at a certain training step. Note that while ˆωi formally
needs to be i.i.d. for each training example, a batch normal-
ized network samples the stochastic parameters once per
training step (mini-batch). For a large number of epochs,
however, the distribution of sampled batch members for a
given training example converges to the i.i.d. case.

B , σ1:L

In a batch normalized network, qθ(ω) corresponds to the
joint distribution of the weights, induced by the random-
ness of the normalization parameters µ1:L
B , as im-
plied by the repeated sampling from D during training.
This is an approximation of the true posterior, where we
have restricted the posterior to lie within the domain of
our parametric network and source of randomness. With
that, we can estimate the uncertainty of predictions from
a trained batch normalized network using the inherent
stochasticity of BN (Section 3.4).

3.3. Prior p(ω)

Equivalence between the VA and BN training procedures
requires ∂
∂θ of Eq. (1) and Eq. (2) to be equivalent up to a
scaling factor. This is the case if ∂
∂θ KL(qθ(ω)||p(ω)) =
N τ ∂

∂θ Ω(θ).

To reconcile this condition, one option is to let the prior
p(ω) imply the regularization term Ω(θ). Eq. (1) reveals
that the contribution of KL(qθ(ω)||p(ω)) to the optimiza-
tion objective is inversely scaled with N . For BN, this cor-
responds to a model with a small Ω(θ) when N is large. In
the limit as N → ∞, the optimization objectives of Eq. (1)
and Eq. (2) become identical with no regularization.8

Another option is to let some Ω(θ) imply p(ω).
In Ap-
pendix Section 1.4 we explore this with L2-regularization,
also called weight decay (Ω(θ) = λ (cid:80)
l=1:L ||W l||2). We
ﬁnd that unlike in MCDO (Gal, 2016), some simplifying

8To prove the existence and ﬁnd an expression of
KL(qθ(ω)||p(ω)), in Appendix Section 1.3 we ﬁnd that BN ap-
proximately induces Gaussian distributions over BN units’ means
and standard deviations, centered around the population values
given by D. We assume a factorized distribution and Gaussian
priors, and ﬁnd the corresponding KL(qθ(ω)||p(ω)) components
in Appendix Section 1.4 Eq. (7). These could be used to construct
a custom Ω(θ) for any Gaussian choice of p(ω).

assumptions are necessary to reconcile the VA and BN ob-
jectives with weight decay: no scale and shift applied to
BN layers, uncorrelated units in each layer, BN applied on
all layers, and large N and M . Given these conditions:

p(µu
p(σu

B) = N (µµ,p, σµ,p)
B) = N (µσ,p, σσ,p)

where µµ,p = 0, σµ,p → ∞, µσ,p = 0 and σσ,p → 1

.

2N τ λl

This corresponds to a wide and narrow distribution on BN
units’ means and std. devs respectively, where N accounts
for the narrowness of the prior. Due to its popularity in
deep learning, our experiments in Section 4 are performed
with weight decay.

3.4. Predictive Uncertainty in Batch Normalized Deep

Nets

In the absence of the true posterior, we rely on the approx-
imate posterior to express an approximate predictive distri-
bution:

p∗(y|x, D) :=

fω(x, y)qθ(ω)dω

(cid:90)

Following (Gal, 2016) we estimate the ﬁrst (for regression
and classiﬁcation) and second (for regression) moments of
the predictive distribution empirically (see Appendix Sec-
tion 1.5 for details):

Ep∗ [y] ≈

f ˆωi(x)

1
T

T
(cid:88)

i=1

T
(cid:88)

1
T

i=1
− Ep∗ [y](cid:124)Ep∗ [y]

Covp∗ [y] ≈ τ −1I +

f ˆωi(x)(cid:124)f ˆωi(x)

where each ˆωi corresponds to sampling the net’s stochas-
tic parameters ω = {µ1:L
B , σ1:L
B } the same way as during
training. Sampling ˆωi therefore involves sampling a batch
B from the training set and updating the parameters in the
BN units, just as if we were taking a training step with B.
From a VA perspective, training the network amounted to
minimizing KL(qθ(ω)||p(ω|D)) wrt θ. Sampling ˆωi from
the training set, and keeping the size of B consistent with
the mini-batch size used during training, ensures that qθ(ω)
during inference remains identical to the approximate pos-
terior optimized during training.

The network is trained just as a regular BN network, but
instead of replacing ω = {µ1:L
B } with population
values from D for inference, we update these parameters
stochastically, once for each forward pass.9 Pseudocode
for estimating predictive mean and variance is given in Al-
gorithm 1.

B , σ1:L

9As an alternative to using the training set D to sample ˆωi,

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Algorithm 1 MCBN Algorithm
Input: sample x, number of inferences T , batchsize b
Output: mean prediction ˆy, predictive uncertainty σ2
1: y = {}
2: loop for T iterations
3: B ∼ D // mini batch
ˆω = {µB, σB}
4:
y = y ∪ f ˆω(x)
5:
6: end loop
7: ˆy = E[y]
8: σ2 = Cov[y] + τ −1I // for regression

// mini batch mean and variance

4. Experiments and Results

We assess the uncertainty quality of MCBN quantitatively
and qualitatively. Our quantitative analysis relies on CI-
FAR10 for image classiﬁcation and eight standard regres-
sion datasets, listed in Appendix Table 1. Publicly avail-
able from the UCI Machine Learning Repository (Univer-
sity of California, 2017) and Delve (Ghahramani, 1996),
these datasets have been used to benchmark comparative
models in recent related literature (see (Hern´andez-Lobato
& Adams, 2015), (Gal & Ghahramani, 2015), (Bui et al.,
2016) and (Li & Gal, 2017)). We report results using
standard metrics, and also propose useful upper and lower
bounds to normalize these metrics for an easier interpreta-
tion in Section 4.2.

Our qualitative results include the toy dataset in Figure 1
in the style of (Karpathy, 2015), a new visualization of un-
certainty quality that plots test errors sorted by predicted
variance (Figure 2 and Appendix), and image segmentation
results (Figure 2 and Appendix).

4.1. Metrics

We evaluate uncertainty quality based on two standard met-
rics, described below: Predictive Log Likelihood (PLL)
and Continuous Ranked Probability Score (CRPS). To im-
prove the interpretability of the metrics, we propose to nor-
malize them by upper and lower bounds.

Predictive Log Likelihood (PLL) Predictive Log Like-
lihood is widely accepted as the main uncertainty quality
metric for regression (Hern´andez-Lobato & Adams, 2015;
Gal & Ghahramani, 2015; Bui et al., 2016; Li & Gal, 2017).
A key property of PLL is that it makes no assumptions
about the form of the distribution. The measure is deﬁned
for a probabilistic model fω(x) and a single observation

we could sample from the implied qθ(ω) as modeled in the Ap-
pendix. This would alleviate having to store D for use during
prediction. In our experiments we used D to sample ˆωi however,
and leave the evaluation of the modeled qθ(ω) for future research.

(yi, xi) as:

PLL(fω(x), (yi, xi)) = log p(yi|fω(xi))

where p(yi|fω(xi)) is the model’s predicted PDF evalu-
ated at yi, given the input xi. A more detailed description
is given in the Appendix Section 1.5. The metric is un-
bounded and maximized by a perfect prediction (mode at
yi) with no variance. As the predictive mode moves away
from yi, increasing the variance tends to increase PLL (by
maximizing probability mass at yi). While PLL is an ele-
gant measure, it has been criticized for allowing outliers to
have an overly negative effect on the score (Selten, 1998).

Continuous Ranked Probability Score (CRPS) Con-
tinuous Ranked Probability Score is a measure that takes
the full predicted PDF into account with less sensitivity to
outliers. A prediction with low variance that is slightly off-
set from the true observation will receive a higher score
form CRPS than PLL. In order for CRPS to be analytically
tractable, we need to assume a Gaussian unimodal predic-
tive distribution. CRPS is deﬁned as

CRPS(fω(xi), (yi, xi)) =

(cid:0)F (y) − 1(y ≥ yi)(cid:1)2

dy

(cid:90) ∞

−∞

where F (y) is the predictive CDF, and 1(y ≥ yi) = 1
if y ≥ yi and 0 otherwise (for univariate distributions)
(Gneiting & Raftery, 2007). CRPS is interpreted as the sum
of the squared area between the CDF and 0 where y < yi
and between the CDF and 1 where y ≥ yi. A perfect pre-
diction with no variance yields a CRPS of 0; for all other
cases the value is larger. CRPS has no upper bound.

4.2. Benchmark models and normalized metrics

It is difﬁcult to interpret the quality of uncertainty from
raw PLL and CRPS values. We propose to normalize the
metrics between useful lower and upper bounds. The nor-
malized measures estimate the performance of an uncer-
tainty model between the trivial solution (constant uncer-
tainty) and optimal uncertainty for each prediction. For
the lower bound, we deﬁne a baseline that predicts con-
stant variance regardless of input. The variance is set to
a ﬁxed value that optimizes CRPS on validation data. We
call this model Constant Uncertainty BN (CUBN). It re-
ﬂects our best guess of constant variance on test data –
thus, any improvement in uncertainty quality over CUBN
indicates a sensible estimate of uncertainty. We simi-
larly deﬁne a baseline for dropout, Constant Uncertainty
Dropout (CUDO). The modeling of variance (uncertainty)
by MCBN and CUBN are visualized in Figure 1.

An upper bound on uncertainty performance can also
be deﬁned for a probabilistic model f with respect to
CRPS or PLL. For each observation (yi, xi), a value

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

for the predictive variance Ti can be chosen that max-
imizes PLL or minimizes CRPS10. Using CUBN as a
lower bound and the optimized CRPS score as the up-
per bound, uncertainty estimates can be normalized be-
tween these bounds (1 indicating optimal performance,
and 0 indicating same performance as ﬁxed uncer-
this normalized measure CRPS =
tainty). We call
minT CRPS(f,(yi,xi))−CRPS(fCU ,(yi,xi)) × 100, and the PLL
analogue PLL = PLL(f,(yi,xi))−PLL(fCU ,(yi,xi))
maxT PLL(f,(yi,xi))−PLL(fCU ,(yi,xi)) ×100.

CRPS(f,(yi,xi))−CRPS(fCU ,(yi,xi))

4.3. Test setup

Our evaluation compares MCBN to MCDO (Gal &
Ghahramani, 2015) and MNF (Louizos & Welling, 2017)
using the datasets and metrics described above. Our setup
is similar to (Hern´andez-Lobato & Adams, 2015), which
was also followed by (Gal & Ghahramani, 2015). How-
ever, our comparison implements a different hyperparame-
ter selection, allows for a larger range of dropout rates, and
uses larger networks with two hidden layers.

For the regression task, all models share a similar archi-
tecture: two hidden layers with 50 units each, and ReLU
activations, with the exception of Protein Tertiary Struc-
ture dataset (100 units per hidden layer). Inputs and out-
puts were normalized during training. Results were aver-
aged over ﬁve random splits of 20% test and 80% train-
ing and cross-validation (CV) data. For each split, 5-fold
CV by grid search with a RMSE minimization objective
was used to ﬁnd training hyperparameters and optimal n.o.
epochs, out of a maximum of 2000. For BN-based mod-
els, the hyperparameter grid consisted of a weight decay
factor ranging from 0.1 to 1−15 by a log 10 scale, and a
batch size range from 32 to 1024 by a log 2 scale. For
DO-based models, the hyperparameter grid consisted of
the same weight decay range, and dropout probabilities
in {0.2, 0.1, 0.05, 0.01, 0.005, 0.001}. DO-based models
used a batch size of 32 in all evaluations. For MNF11, the
n.o. epochs was optimized, the batch size was set to 100,
and early stopping test performed each epoch (compared to
every 20th for MCBN, MCDO).

For MCBN and MCDO, the model with optimal training
hyperparameters was used to optimize τ numerically. This
optimization was made in terms of average CV CRPS for
MCBN, CUBN, MCDO, and CUDO respectively.

Estimates for the predictive distribution were obtained by
taking T = 500 stochastic forward passes through the net-
work. For each split, test set evaluation was done 5 times
with different seeds. Implementation was done in Tensor-
Flow with the Adam optimizer and a learning rate of 0.001.

10Ti can be found analytically for PLL, but must be found nu-

merically for CRPS.

11Where we used an adapted version of the authors’ code.

For
the image classiﬁcation test we use CIFAR10
(Krizhevsky & Hinton, 2009) which includes 10 object
classes with 5,000 and 1,000 images in the training and
test sets, respectively. Images are 32x32 RGB format. We
trained a ResNet32 architecture with a batch size of 32,
learning rate of 0.1, weight decay of 0.0002, leaky ReLU
slope of 0.1, and 5 residual units. SGD with momentum
was used as the optimizer.

Code for reproducing our experiments is available at
https://github.com/icml-mcbn/mcbn.

4.4. Test results

The regression experiment comparing uncertainty quality
is summarized in Table 1. We report CRPS and PLL, ex-
pressed as a percentage, which reﬂects how close the model
is to the upper bound, and check to see if the model signif-
icantly exceeds the lower bound using a one sample t-test
(signiﬁcance level is indicated by *’s). Further details are
provided in Appendix Section 1.7.

In Figure 2 (left), we present a novel visualization of un-
certainty quality for regression problems. Data are sorted
by estimated uncertainty in the x-axis. Grey dots show the
errors in model predictions, and the shaded areas show the
model uncertainty. A running mean of the errors appears
as a gray line. If uncertainty estimation is working well,
a correlation should exist between the mean error (gray
line) and uncertainty (shaded area). This indicates that the
uncertainty estimation recognizes samples with larger (or
smaller) potential for predictive errors.

We applied MCBN on the image classiﬁcation task of CI-
FAR10. The baseline in this case is the softmax distribu-
tion using the moving average for BN units. Log likeli-
hood (PLL) is the metric used to compare with the base-
line. The baseline achieves a PLL of -0.32 on the test
set, while MCBN obtains a PLL of -0.28. Table 2 shows
the performance of MCBN when using different number of
stochastic forward passes (the MCBN batchsize is ﬁxed to
the training batch size at 32). PLL improves as the number
of the stochastic passes increases, until it is signiﬁcantly
better than the softmax baseline.

To demonstrate how model uncertainty can be obtained
from an existing network with minimal effort, we applied
MCBN to an image segmentation task using Bayesian Seg-
Net with the main CamVid and PASCAL-VOC models in
(Kendall et al., 2015). We simply ran multiple forward
passes with different mini-batches randomly taken from the
train set. The models obtained from the online model zoo
have BN blocks after each layer. We recalculate mean and
variance for the ﬁrst 2 blocks only and use the training
statistics for the rest of the blocks. Mini-batches of size
10 and 36 were used for CamVid and VOC respectively

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Table 1. Uncertainty quality measured on eight regression datasets. MCBN, MCDO and MNF are compared over 5 random 80-20
splits of the data with 5 different random seeds each split. We report CRPS and PLL, uncertainty metrics CRPS and PLL normalized
to a lower bound of constant variance and upper bound that maximizes the metric expressed as a percentage (described in Section 4.2).
Higher numbers mean the model is closer to the upper bound. We check if the reported values for CRPS and PLL signiﬁcantly exceed
the lower bound using a one sample t-test (signiﬁcance level indicated by *’s). See text for further details.

MCBN

MCDO

MNF

MCBN

CRPS

Dataset

Boston
Concrete
Energy
Kin8nm
Power
Protein
Wine (Red)
Yacht

8.50 ****
3.91 ****
5.75 ****
2.85 ****
0.24 ***
2.66 ****
0.26 **
-56.39 ***

3.06 ****
*
0.93
1.37 ns
1.82 ****
-0.44 ****
0.99 ****
2.00 ****
21.42 ****

5.88 ****
3.13 ***
1.10 ns
0.53 ns
-0.89 ****
0.57 ****
0.94 ****
24.92 ****

PLL
MCDO

5.51 ****
10.92 ****
-14.28

*
-0.26 ns
3.52 ****
6.23 ****
2.91 ****

-41.54 ns

MNF

1.76 ns
-2.16 ns
-33.88 ns
0.42 ns
-0.87 ****
0.52 ****
0.83 ****
46.19 ****

10.49 ****
-36.36 **
10.89 ****
1.68 ***
0.33 **
2.56 ****
0.19

*

45.58 ****

due to memory limits. The results in Figure 2 (right) were
obtained from 20 stochastic forward passes, showing high
uncertainty near object boundaries. The VOC results are
more appealing because of larger mini-batches.

We provide additional experimental results in the Ap-
pendix. Appendix Tables 2 and 3 show the mean CRPS
and PLL values from the regression experiment. Table 4
provides the raw CRPS and PLL scores.
In Table 5 we
provide RMSE results of the MCBN and MCDO networks
in comparison with non-stochastic BN and DO networks.
These results indicate that the procedure of multiple for-
ward passes in MCBN and MCDO show slight improve-
ments in the predictive accuracy compared to their non-
Bayesian counterparts. In Tables 6 and 7, we investigate the
effect of varying batch size while keeping other hyperpa-
rameters ﬁxed. We see that performance deteriorates with
small batch sizes (≤16), a known issue of BN (Ioffe, 2017).
Similarly, results varying the number of stochastic forward
passes T is reported in Tables 8 and 9. While performance
beneﬁts from large T , in some cases T = 50 (i.e. 1/10 of
T in the main evaluation) performs well. Uncertainty-error
plots for all the datasets are provided in the Appendix.

5. Discussion

The results presented in Tables 1-2 and Appendix Tables
2-9 indicate that MCBN generates meaningful uncertainty

Table 2. Uncertainty quality for image classiﬁcation varying
number of stochastic forward passes. Uncertainty quality for
image classiﬁcation measured by PLL. ResNet32 is trained on
CIFAR10 with batch size 32. PLL improves as the sampling in-
creases until it is signiﬁcantly better than the softmax baseline
(-0.32).

estimates that correlate with actual errors in the model’s
In Table 1, we show statistically signiﬁcant
prediction.
improvements over CUBN in the majority of the datasets,
both in terms of CRPS and PLL. The visualizations in
Figure 2 and in the Appendix Figures 2-3 show correla-
tions between the estimated model uncertainty and errors
of the network’s predictions. We perform the same exper-
iments using MCDO and MNF, and ﬁnd that MCBN gen-
erally performs on par with both methods. Looking closer,
MCBN outperforms MCDO and MNF in more cases than
not, measured by CRPS. However, care must be used. The
learned parameters are different, leading to different pre-
dictive means and confounding direct comparison.

The results on the Yacht Hydrodynamics dataset seem con-
tradictory. The CRPS score for MCBN are extremely neg-
ative, while the PLL score is extremely positive. The op-
posite trend is observed for MCDO. To add to the puzzle,
the visualization in Figure 2 depicts an extremely promis-
ing uncertainty estimation that models the predictive errors
with high ﬁdelity. We hypothesize that this strange behav-
ior is due to the small size of the data set, which only con-
tains 60 test samples, or due to the Gaussian assumption
of CRPS. There is also a large variability in the model’s
accuracy on this dataset, which further confounds the mea-
surements for such limited data.

One might criticize the overall quality of uncertainty es-
timates observed in all the models we tested, due to the
magnitude of CRPS and PLL in Table 1. The scores rarely
exceed 10% improvement over the lower bound. However,
we caution that these measures should be taken in context.
The upper bound is very difﬁcult to achieve in practice –
it is optimized for each test sample individually – and the
lower bound is a quite reasonable estimate.

Number of stochastic forward passes
64
2

16

32

4

8

1

PLL

-.36

-.32

-.30

-.29

-.29

-.28

-.28

128

-.28

The study of MCBN sensitivity to batch size revealed that a
certain batch size is required for the best performance, de-
pendent on the data. When doing inference on a GPU, large

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Data sorted by estimated uncertainty

Data sorted by estimated uncertainty

Image segmentation uncertainty (CamVid and PASCAL-VOC)

Figure 2. Uncertainty-error plots (left) and segmentation and uncertainty results applying MCBN to Bayesian SegNet (right).
(left) Errors in predictions (gray dots) sorted by estimated uncertainty on select datasets. The shaded areas show model uncertainty for
MCBN (blue), MNF (violet) and MCDO (red). The light area indicates 95% CI, dark area 50% CI. Gray dots show absolute prediction
errors on the test set, and the gray line depicts a running mean of the errors. The dashed line indicates the optimized constant uncertainty.
A correlation between estimated uncertainty (shaded area) and mean error (gray) indicates the uncertainty estimates are meaningful
for estimating errors. (right) Applying MCBN to Bayesian SegNet (Kendall et al., 2015) on scenes from CamVid (3rd column) and
PASCAL-VOC (4th column). Top: original image. Middle: the Bayesian estimated segmentation. Bottom: estimated uncertainty using
MCBN for all classes. The uncertainty maps for both datasets are reasonable, but qualitatively better for PASCAL-VOC due to the larger
mini-batch size (36) compared to CamVid (10). Smaller batch sizes were used for CamVid due to memory limitations (CamVid images
are 360x480 while VOC are 224x224). See Appendix for complete results.

batch sizes may cause memory issues for cases where the
input is large and the network has a large number of param-
eters, as is common for state-of-the-art image classiﬁcation
networks. However, there are various workarounds to this
problem. One can store BN statistics, instead of batches, to
reduce memory issues. Furthermore, we can use the Gaus-
sian estimate of the BN statistics as discussed previously,
which makes memory and computation extremely efﬁcient.

6. Conclusion

In this work, we have shown that training a deep network
using batch normalization is equivalent to approximate in-
ference in Bayesian models. We show evidence that the
uncertainty estimates from MCBN correlate with actual er-
rors in the model’s prediction, and are useful for practical

tasks such as regression, image classiﬁcation, and image
segmentation. Our experiments show that MCBN yields
a signiﬁcant improvement over the optimized constant un-
certainty baseline, on par with MCDO and MNF. Our eval-
uation also suggests new normalized metrics based on use-
ful upper and lower bounds, and a new visualization which
provides an intuitive explanation of uncertainty quality.

Finally, it should be noted that over the past few years,
batch normalization has become an integral part of most –
if not all – cutting edge deep networks. We have shown that
it is possible to obtain meaningful uncertainty estimates
from existing models without modifying the network or the
training procedure. With a few lines of code, robust uncer-
tainty estimates can be obtained by computing the variance
of multiple stochastic forward passes through an existing
network.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

References

Bui, T. D., Hern´andez-Lobato, D., Li, Y., Hern´andez-
Lobato, J. M., and Turner, R. E. Deep Gaussian Pro-
cesses for Regression using Approximate Expectation
Propagation. In ICML, 2016.

Chen, X., Kundu, K., Zhang, Z., Ma, H., Fidler, S., and Ur-
tasun, R. Monocular 3d object detection for autonomous
driving. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 2147–2156,
2016.

Djuric, U., Zadeh, G., Aldape, K., and Diamandis, P. Preci-
sion histology: how deep learning is poised to revitalize
histomorphology for personalized cancer care. npj Pre-
cision Oncology, 1(1):22, 2017.

Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M.,
Blau, H. M., and Thrun, S. Dermatologist-level classiﬁ-
cation of skin cancer with deep neural networks. Nature,
Feb 2017.

Gal, Y. Uncertainty in Deep Learning. PhD thesis, Univer-

sity of Cambridge, 2016.

Gal, Y. and Ghahramani, Z. Dropout as a Bayesian Ap-
proximation : Representing Model Uncertainty in Deep
Learning. ICML, 48:1–10, 2015.

Ghahramani, Z. Delve Datasets. University of Toronto,
URL http://www.cs.toronto.edu/

1996.
{˜}delve/data/kin/desc.html.

Ghahramani, Z. Probabilistic machine learning and ar-
tiﬁcial intelligence. Nature, 521(7553):452–459, May
2015.

Gneiting, T. and Raftery, A. E. Strictly Proper Scoring
Rules, Prediction, and Estimation. Journal of the Amer-
ican Statistical Association, 102(477):359–378, 2007.

Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain-
ing and harnessing adversarial examples. arXiv preprint
arXiv:1412.6572, 2014.

Graves, A. Practical Variational Inference for Neural Net-

works. NIPS, 2011.

Hern´andez-Lobato, J. M. and Adams, R. Probabilistic
backpropagation for scalable learning of bayesian neu-
ral networks. In International Conference on Machine
Learning, pp. 1861–1869, 2015.

Hinton, G. E. and Van Camp, D. Keeping the neural net-
works simple by minimizing the description length of
the weights. In Proceedings of the sixth annual confer-
ence on Computational learning theory, pp. 5–13. ACM,
1993.

Ioffe, S. Batch renormalization: Towards reducing mini-
batch dependence in batch-normalized models. CoRR,
abs/1702.03275, 2017. URL http://arxiv.org/
abs/1702.03275.

Ioffe, S. and Szegedy, C. Batch Normalization: Accelerat-
ing Deep Network Training by Reducing Internal Co-
variate Shift. Arxiv, 2015. URL http://arxiv.
org/abs/1502.03167.

Karpathy, A. Convnetjs demo:

toy 1d regression, 2015.
http://cs.stanford.edu/people/

URL
karpathy/convnetjs/demo/regression.
html.

Kendall, A., Badrinarayanan, V., and Cipolla, R. Bayesian
SegNet: Model Uncertainty in Deep Convolutional
Encoder-Decoder Architectures for Scene Understand-
ing. CoRR, abs/1511.0, 2015. URL http://arxiv.
org/abs/1511.02680.

Kingma, D. P. and Welling, M. Auto-Encoding Variational

Bayes. In ICLR, 2014.

Krizhevsky, A. and Hinton, G. Learning multiple layers of

features from tiny images. 2009.

Krueger, D., Huang, C.-W., Islam, R., Turner, R., Lacoste,
A., and Courville, A. Bayesian hypernetworks. arXiv
preprint arXiv:1710.04759, 2017.

Lehmann, E. L.

Elements of Large-Sample Theory.

Springer Verlag, New York, 1999. ISBN 0387985956.

Li, Y. and Gal, Y. Dropout Inference in Bayesian Neural

Networks with Alpha-divergences. arXiv, 2017.

Louizos, C. and Welling, M. Multiplicative normal-
izing ﬂows for variational Bayesian neural networks.
In Precup, D. and Teh, Y. W. (eds.), Proceedings of
the 34th International Conference on Machine Learn-
ing, volume 70 of Proceedings of Machine Learn-
ing Research, pp. 2218–2227, International Convention
Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.
URL http://proceedings.mlr.press/v70/
louizos17a.html.

MacKay, D. J. A practical bayesian framework for back-
propagation networks. Neural computation, 4(3):448–
472, 1992.

Neal, R. M. Bayesian Learning for Neural Networks. PhD

thesis, University of Toronto, 1995.

Neal, R. M. Bayesian learning for neural networks, volume

118. Springer Science & Business Media, 2012.

Selten, R. Axiomatic characterization of the quadratic scor-
ing rule. Experimental Economics, 1(1):43–62, 1998.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Shen, L. End-to-end training for whole image breast can-
cer diagnosis using an all convolutional design. arXiv
preprint arXiv:1708.09427, 2017.

University of California, I. UC Irvine Machine Learning
Repository, 2017. URL https://archive.ics.
uci.edu/ml/index.html.

Wang, S. I. and Manning, C. D.
Proceedings of

Fast dropout train-
the 30th International Con-
ing.
ference on Machine Learning, 28:118–126, 2013.
URL http://machinelearning.wustl.edu/
mlpapers/papers/wang13a.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

1. Appendix

1.1. Variational Approximation

Assume we were to come up with a family of distributions parameterized by θ in order to approximate the posterior, qθ(ω).
Our goal is to set θ such that qθ(ω) is as similar to the true posterior p(ω|D) as possible.

For clarity, qθ(ω) is a distribution over stochastic parameters ω that is determined by a set of learnable parameters θ and
some source of randomness. The approximation is therefore limited by our choice of parametric function qθ(ω) as well
as the randomness.12 Given ω and an input x, an output distribution p(y|x, ω) = p(y|fω(x)) = fω(x, y) is induced by
observation noise (the conditionality of which is omitted for brevity).

One strategy for optimizing θ is to minimize KL(qθ(ω)||p(ω|D)), the KL divergence of p(ω|D) w.r.t. qθ(ω). Minimizing
KL(qθ(ω)||p(ω|D)) is equivalent to maximizing the ELBO:

(cid:90)

ω

qθ(ω) ln p(Y|X, ω)dω − KL(qθ(ω)||p(ω))

Assuming i.i.d. observation noise, this is equivalent to minimizing:

LVA(θ) := −

qθ(ω) ln p(yi|fω(xi))dω + KL(qθ(ω)||p(ω))

N
(cid:88)

(cid:90)

n=1

Instead of making the optimization on the full training set, we can use a subsampling (yielding an unbiased estimate of
LVA(θ)) for iterative optimization (as in mini-batch optimization):

ˆLVA(θ) := −

N
M

(cid:90)

(cid:88)

i∈B

ω

qθ(ω) ln p(yi|fω(xi))dω + KL(qθ(ω)||p(ω))

During optimization, we want to take the derivative of the expected log likelihood w.r.t. the learnable parameters θ. (Gal,
2016) provides an intuitive interpretation of a MC estimate for NNs trained with a SRT (equivalent to the reparametrisation
trick in (Kingma & Welling, 2014)), and this interpretation is followed here. We let an auxillary variable (cid:15) represent the
stochasticity during training such that ω = g(θ, (cid:15)). The function g and the distribution of (cid:15) are such that p(g(θ, (cid:15))) =
qθ(ω).13 Assuming qθ(ω) can be written (cid:82)
(cid:15) qθ(ω|(cid:15))p((cid:15))d(cid:15) where qθ(ω|(cid:15)) = δ(ω − g(θ, (cid:15))), this auxiliary variable yields
the estimate (full proof in (Gal, 2016)):

ˆLVA(θ) = −

N
M

(cid:90)

(cid:88)

i∈B

(cid:15)

p((cid:15)) ln p(yi|fg(θ,(cid:15))(xi))d(cid:15) + KL(qθ(ω)||p(ω))

where taking a single sample MC estimate of the integral yields the optimization objective in Eq. 1.

12In an approx. Bayesian view of a NN, qθ(ω) would correspond to the distribution of weights in the network given by some SRT.
13In a NN trained with BN, it is easy to see that g exists if we let (cid:15) represent a mini-batch selection from the training data, since all

BN units’ means and variances are completely determined by (cid:15) and θ.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

1.2. KL Divergence of factorized Gaussians

If qθ(ω) and p(ω) factorize over all stochastic parameters:

KL(qθ(ω)||p(ω)) = −

(cid:2)qθ(ωi)(cid:3) ln

(cid:90)

(cid:90)

(cid:89)

i
(cid:89)

i

(cid:90)

ω

ω

(cid:104)

= −

(cid:2)qθ(ωi)(cid:3) (cid:88)

(cid:88)

=

−

(cid:89)

(cid:2)qθ(ωi)(cid:3) ln

dω

(cid:81)
(cid:81)

i p(ωi)
i qθ(ωi)
(cid:20)

ln

p(ωi)
qθ(ωi)

i

(cid:21) (cid:89)

dωi

i
(cid:89)

i

(cid:105)

dωi

ωi(cid:54)=j

i(cid:54)=j

p(ωj)
qθ(ωj)
(cid:90)

dωj

qθ(ωj) ln

p(ωj)
qθ(ωj)

(cid:89)

qθ(ωi)dωi

(cid:105)

(cid:88)

(cid:104)

(cid:90)

ω

i

ωj

−

(cid:90)

j

j

i

(cid:88)

i
(cid:88)

=

=

=

−

qθ(ωi) ln

ωi

p(ωi)
qθ(ωi)

dωi

KL(qθ(ωi)||p(ωi))

(3)

such that KL(qθ(ω)||p(ω)) is the sum of the KL divergence terms for the individual stochastic parameters ωi. If the
factorized distributions are Gaussians, where qθ(ωi) = N (µq, σ2

q ) and p(ωi) = N (µp, σ2

p) we get:

KL(qθ(ωi)||p(ωi)) =

qθ(ωi) ln

dωi

qθ(ωi)
p(ωi)
(cid:90)

ωi

= − H(qθ(ωi)) −

qθ(ωi) ln p(ωi)dωi

= −

(1 + ln(2πσ2

q ))

(cid:90)

ωi

1
2
(cid:90)

ωi
1
2
1
2

σp
σq

−

qθ(ωi) ln

1
p)1/2
(2πσ2

(cid:110)

exp

−

(ωi − µp)2
2σ2
p

(cid:111)

dωi

(4)

= −

(1 + ln(2πσ2

+

ln(2πσ2

p) +

q ))
Eq[ω2

i ] − 2µpEq[ωi] + µ2
p

2σ2
p

= ln

+

q + (µq − µp)2
σ2
2σ2
p

−

1
2

for each KL divergence term. Here H(qθ(ωi)) = 1

2 (1 + ln(2πσ2

q )) is the differential entropy of qθ(ωi).

1.3. Distribution of µu

B, σu
B

Here we approximate the distribution of mean and standard deviation of a mini-batch, separately to two Gaussians – This
has also been empirically veriﬁed, see Figure 3 for 2 sample plots and the appendix section 1.9 for more. For the mean we
get:

where xm are the examples in the sampled batch. We will assume these are sampled i.i.d.14. Samples of the random
variable W(j)xm are then i.i.d.. Then by central limit theorem (CLT) the following holds for sufﬁciently large M (often
≥ 30):

µB =

ΣM

m=1W(j)xm
M

µB ∼ N (µ,

σ2
M

)

14Although in practice with deep learning, mini-batches are sampled without replacement, stochastic gradient descent samples with

replacement in its standard form.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 3. Batch statistics used to train the network are normal. A one-sample Kolmogorov-Smirnov test checks that µB and σB come
from a standard normal distribution. More examples are available in Appendix 1.9.

For standard deviation:

(cid:114)

σB =

ΣM

m=1(W(j)xm − µB)2
M

√

M (σB − σ) =

M

√

(cid:114)

(cid:16)

ΣM

m=1(W(j)xm − µB)2
M

√

(cid:17)

−

σ2

We want to rewrite
x = ΣM

m=1(W(j)xm−µB)2
M

:

(cid:113) ΣM

m=1(W(j)xm−µB)2
M

. We take a Taylor expansion of f (x) =

x around a = σ2. With

√

Then

so

√

√

x =

σ2 +

1
√
σ2
2

(x − σ2) + O[(x − σ2)2]

√

M (σB − σ) =

M

(cid:32)

√

1
√
σ2
2

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

(cid:34)

O

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

+

− σ2(cid:17)
− σ2(cid:17)2(cid:35)(cid:33)

√

M
2σ

=

=

1
√

2σ

M

+

(cid:16) 1
m=1(W(j)xm − µB)2 − σ2(cid:17)
ΣM
M
(cid:34)√
m=1(W(j)xm − µB)2
M
m=1(W(j)xm − µB)2 − M σ2(cid:17)
ΣM

(cid:16) ΣM

M

O

(cid:16)

+

− σ2(cid:17)2(cid:35)

(cid:34)√

O

M

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

− σ2(cid:17)2(cid:35)

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

consider ΣM

m=1(W(j)xm − µB)2. We know that E[W(j)xm] = µ and write

then

ΣM
=ΣM
=ΣM
=ΣM
=ΣM
=ΣM

m=1(W(j)xm − µB)2
m=1((W(j)xm − µ) − (µB − µ))2
m=1((W(j)xm − µ)2 + (µB − µ)2 − 2(W(j)xm − µ)(µB − µ))
m=1(W(j)xm − µ)2 + M (µB − µ)2 − 2(µB − µ)ΣM
m=1(W(j)xm − µ)2 − M (µB − µ)2
m=1((W(j)xm − µ)2 − (µB − µ)2)

m=1(W(j)xm − µ)

√

M (σB − σ) =

1
√

(cid:16)

m=1((W(j)xm − µ)2 − (µB − µ)2) − M σ2(cid:17)
ΣM

+

2σ

M

(cid:34)√

O

M

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

− σ2(cid:17)2(cid:35)

=

(cid:16)

1
√

2σ

M

ΣM

m=1(W(j)xm − µ)2 − ΣM

(cid:34)√

O

M

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

m=1(µB − µ)2 − M σ2(cid:17)
+
− σ2(cid:17)2(cid:35)

=

(cid:16)

1
√

2σ

M

ΣM

m=1((W(j)xm − µ)2 − σ2) − ΣM

(cid:34)√

O

M

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

m=1(µB − µ)2(cid:17)
− σ2(cid:17)2(cid:35)

+

=

1
√

2σ

−

M
1
√

M

2σ
(cid:34)√

+ O

M

ΣM

m=1((W(j)xm − µ)2 − σ2)

ΣM

m=1(µB − µ)2

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

− σ2(cid:17)2(cid:35)

ΣM

m=1((W(j)xm − µ)2 − σ2)
(cid:125)

(cid:123)(cid:122)
term A

=

2σ
(cid:124)

−

1
√

M

√

M
2σ

(cid:124)

(µB − µ)2
(cid:123)(cid:122)
(cid:125)
term B

(cid:34)√

(cid:16) ΣM

+ O

M

(cid:124)

m=1(W(j)xm − µB)2
M
(cid:123)(cid:122)
term C

− σ2(cid:17)2(cid:35)

(cid:125)

We go through each term in turn

Term A
We have

Term A =

ΣM

m=1((W(j)xm − µ)2 − σ2)

1
√

2σ

M

m=1(W(j)xm − µ)2 is the sum of M RVs (W(j)xm − µ)2. Note that since E[W(j)xm] = µ it holds that
where ΣM
E[(W(j)xm − µ)2] = σ2. Since (W(j)xm − µ)2 is sampled approximately iid (by assumptions above), for large enough

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

M by CLT it holds approximately that

ΣM

m=1(W(j)xm − µ)2 ∼ N (M σ2, M Var((W(j)xm − µ)2))

Var((W(j)xm − µ)2) = E[(W(j)xm − µ)2∗2] − E[(W(j)xm − µ)2]2
= E[(W(j)xm − µ)4] − σ4

ΣM

m=1((W(j)xm − µ)2 − σ2) ∼ N (0, M ∗ E[(W(j)xm − µ)4] − M σ4)

Term A ∼ N (0,

E[(W(j)xm − µ)4] − σ4
4σ2

)

Term B =

(µB − µ)2 =

M (µB − µ)(µB − µ)

√

M
2σ

√

1
2σ

Consider (µB − µ). As µB

p
−→ µ when M → ∞ we have µB − µ

p
−→ 0. We also have

√

M (µB − µ) =

ΣM

m=1W(j)xm
M

√

√

−

M µ

which by CLT is approximately Gaussian for large M . We can then make use of the Cramer-Slutzky Theorem, which
p
states that if (Xn)n≥1 and (Yn)n≥1 are two sequences such that Xn
−→ a as n → ∞ where a is a constant,
then as n → ∞, it holds that Xn ∗ Yn

d−→ X ∗ a. Thus, Term B is approximately 0 for large M.

d−→ X and Yn

(cid:34)√

Term C = O

M

(cid:16) ΣM

m=1(W(j)xm − µB)2
M

− σ2(cid:17)2(cid:35)

Since E[(W(j)xm − µ)2] = σ2 we can make the same use of Cramer-Slutzky as for Term B, such that Term C is approxi-
mately 0 for large M.

Finalizing the distribution
We have approximately

√

M (σB − σ) ∼ N (0,

E[(W(j)xm − µ)4] − σ4
4σ2

)

σB ∼ N (σ,

E[(W(j)xm − µ)4] − σ4
4σ2M

)

Here we make use of the stochasticity from BN modeled in the Appendix section 1.3, to evaluate the implied prior on the
stochastic variables for a BN network. Speciﬁcally, we consider a BN network with fully connected layers and BN applied
to each layer, trained with L2-regularization (weight decay). In the following, we make use of the simplifying assumptions
of no scale and shift tranformations, BN applied to each layer, and independent input units to each layer.

where

Then

so

Term B
We have

Term C
We have

so

1.4. Prior

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Equivalence between the objectives of Eq. (1) and (2) requires:

(5)

(6)

(7)

∂
∂θk

KL(qθ(ω)||p(ω)) = N τ

Ω(θ)

∂
∂θk

∂
∂θk

L
(cid:88)

l=1

= N τ

λl||Wl||2

µu
B

∝
∼ N (µu,

(σu)2
M

),

σu
B

∝
∼ N (σu,

E[(W(u)x − µu)4] − (σu)4
4(σu)2M

)

KL(qθ(ω)||p(ω)) =

KL(qθ(ωi)||p(ωi))

(cid:88)

i

where θk ∈ θ, and θ is the set of weights in the network. To proceed with the LHS of Eq. (5) we ﬁrst need to ﬁnd the
approximate posterior qθ(ω) that batch normalization induces. As shown in Appendix 1.3, with some weak assumptions
and approximations the Central Limit Theorem (CLT) yields Gaussian distributions of the stochastic variables µu
B, for
large enough M . For any BN unit u:

B, σu

where µu and σu are population-level moments (i.e. moments over D).

We assume that qθ(ω) and p(ω) factorize over all stochastic variables.15 We use i as an index of the set of stochastic
variables. As shown in Eq. (3) in Appendix 1.2, the factorized distributions yield:

Note that each BN unit produces two KL(qθ(ωi)||p(ωi)) terms: one for ωi = µu
terms for one particular BN unit u, and drop the index i for brevity. We use a Gaussian prior p(ωi) = N (µp, σ2
consistency, use the notation qθ(ωi) = N (µq, σ2

q ). As shown in Eq. (4) in Appendix 1.2:

B and one for ωi = σu

B. We consider these
p) and, for

Since θk changes during training, a prior cannot depend on θk so ∂
∂θk

(µp) = ∂
∂θk

(σp) = 0. Letting (·)(cid:48) denote ∂
∂θk

(·):

KL(qθ(ωi)||p(ωi)) = ln

+

σp
σq

q + (µq − µp)2
σ2
2σ2
p

−

1
2

∂
∂θk

KL(qθ(ωi)||p(ωi)) =

σqσ(cid:48)

q + µqµ(cid:48)
σ2
p

q − µpµ(cid:48)
q

−

σ(cid:48)
q
σq

We need not consider θk past a previous layer’s BN, since a normalization step is performed before scale and shift. In the
general case with a given Gaussian p(ω), Eq. 7 evaluated on all BN units’ means and standard deviations w.r.t. all θk up to
a previous layer’s BN, would yield an expression for a custom N τ ∂
Ω(θ) that could be used for an exact VI treatment of
∂θk
BN.

In our reconciliation of weight decay however, given our assumptions of no scale and shift and BN applied to each layer,
we need only consider the weights in the same layer as the BN unit. This means that the stochastic variables in layer l are
only affected by weights in θk ∈ W l (i.e. not the scale and shift variables operating on the input to the layer). We denote
a weight connecting the k:th input unit to the u:th BN unit by W(u,k). For such weights, we need to derive µ(cid:48)
q, for
two cases: ωi = µu
B by
µσ,q and σσ,q. Using the distributions modeled in Eq. 6:

q and σ(cid:48)
B by µµ,q and σµ,q, and for σu

B. We denote the priors of the mean and std. dev for µu

B and ωi = σu

15The empirical distributions have been numerically checked to be linearly independent and the joint distribution is close to a bi-variate

Gaussian.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Case 1: ωi = µu
B

= W(u) ¯x

µµ,q =

µ(cid:48)

µ,q =

W(u)x
N

xk
N

= ¯xk

(cid:88)

x∈D
(cid:88)

x∈D

(cid:114)

σµ,q =

(σu)2
M

=

σ(cid:48)
µ,q =

σ−1
q

1
2

(cid:88)

x∈D

(cid:115)

(cid:80)

x∈D(W(u)x − µq)2
N M

2(W(u)x − µq)(xk − ¯xk)
N M

= σ−1
q

(cid:18) K
(cid:88)

i=1

W(u,i)Cov(xi, xk)

M −1

(cid:19)

where there are K input units to the layer.

Case 2: ωi = σu
B

(cid:115)

(cid:80)

µσ,q =

x∈D(W(u)x − µq)2
N
(cid:18) K
(cid:88)

= σµ,qM

1
2

(cid:19)

σ,q = σ−1
µ(cid:48)

µ,qM − 1

2

W(u,i)Cov(xi, xk))

i=1
E[(W(u)x − µu)4] − (σu)4
4(σu)2M

σσ,q =

σ(cid:48)
σ,q =

E[(W(u)x − µu)4](cid:48)σu − 2(σu)4(σu)(cid:48) − 2(σu)(cid:48)E[(W(u)x − µu)4]
4(σu)3M

Combining these results with Eq. 7 we ﬁnd that taking KL(qθ(ωi)||p(ωi)) for the mean and variance of a single BN unit u
wrt the weight from input unit k:

∂

−

σ(cid:48)
µ,q
σµ,q
σ(cid:48)
σ,q
σσ,q

∂

∂W(u,k) KL(qθ(µu
µ,q + µµ,qµ(cid:48)
σµ,qσ(cid:48)

µ,q − µµ,pµ(cid:48)

µ,q

B)||p(µu

B)) +

∂W(u,k) KL(qθ(σu

B)||p(σu

B))

σ,q

σσ,qσ(cid:48)

σ,q − µσ,pµ(cid:48)

σ2
µ,p
σ,q + µσ,qµ(cid:48)
σ2
σ,p
O(M −1) + ¯xkW(u) ¯x − µµ,p ¯xk
σ2
O(M −2) + (cid:80)K
i=1 W(u,i)Cov(xi, xk) − µσ,pO(M − 1
2 )

− O(M −1)

µ,p

−

σ2

σ,p

E[(W(u)x − µu)4](cid:48)σu − 2(σu)4(σu)(cid:48) − 2(σu)(cid:48)E[(W(u)x − µu)4]
E[(W(u)x − µu)4]σu − (σu)5

=

+

=

+

−

where we summarize the terms scaled by M with O-notation. We see that if we let M → ∞, µµ,p = 0, σµ,p → ∞,
µσ,p = 0 and σσ,p is small enough, then:

(cid:18)

∂
∂W(u,k)

KL(qθ(µu

B)||p(µu

B)) + KL(qθ(σu

B)||p(σu

B))

≈

(cid:19)

(cid:80)K

i=1 W(u,i)Cov(xi, xk)

σ2

σ,p

such that each BN layer yields the following:

(cid:88)

K
(cid:88)

u

i=1

(cid:18)

∂
∂W(u,i)

KL(qθ(µu

B)||p(µu

B)) + KL(qθ(σu

B)||p(σu

B))

(cid:19)

(cid:88)

≈

u

(cid:80)K

i=1 Wu,i (cid:80)K
σ2

σ,p,u

i2=1 Cov(xi, xi2)

(8)

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

where we denote the prior for the std. dev. of the std. dev. of BN unit u by σσ,p,u. Given our assumptions of no scale and
shift from the previous layer, and independent input features in every layer, Eq. 8 reduces to:

(cid:88)

K
(cid:88)

u

i=1

Wu,i
σ2

σ,p

if the same prior is chosen for each BN unit in the layer. We therefore ﬁnd that Eq. 5 is reconciled by p(µu
and p(σu

is small enough, which is the case if N is large.

), if

B) → N (0,

1
2N τ λl

1
2N τ λl

B) → N (0, ∞)

1.5. predictive distribution properties

This section provides derivations of properties of the predictive distribution p∗(y|x, D) in section 3.4, following (Gal,
2016). We ﬁrst ﬁnd the ﬁrst two modes of the approximate predictive distribution (with the second mode applicable to
regression), then show how to estimate the predictive log likelihood, a measure of uncertainty quality used in the evaluation.

Predictive mean Assuming Gaussian iid noise deﬁned by model precision τ ,
N (y; fω(x), τ −1I):

i.e.

fω(x, y) = p(y|fω(x)) =

fω(x, y)qθ(ω)dω

dy

(cid:17)

N (y; fω(x), τ −1I)qθ(ω)dω

dy

(cid:17)

yN (y; fω(x), τ −1I)dy

qθ(ω)dω

(cid:17)

Ep∗ [y] =

yp∗(y|x, D)dy

(cid:16) (cid:90)

ω
(cid:16) (cid:90)

y

y

ω

(cid:16) (cid:90)

y

(cid:90)

(cid:90)

y
(cid:90)

y
(cid:90)

ω

(cid:90)

ω

=

=

=

=

≈

fω(x)qθ(ω)dω

1
T

T
(cid:88)

i=1

f ˆωi(x)

where we take the MC Integral with T samples of ω for the approximation in the ﬁnal step.

Predictive variance For regression, our goal is to estimate:

Covp∗ [y] = Ep∗ [y(cid:124)y] − Ep∗ [y](cid:124)Ep∗ [y]

We ﬁnd that:

Ep∗ [y(cid:124)y] =

y(cid:124)yp∗(y|x, D)dy

(cid:90)

y
(cid:90)

y
(cid:90)

ω

(cid:90)

ω

(cid:90)

ω

=

=

=

=

(cid:16) (cid:90)

ω

y(cid:124)y

(cid:16) (cid:90)

y

fω(x, y)qθ(ω)dω

dy

(cid:17)

y(cid:124)yfω(x, y)dy

qθ(ω)dω

(cid:17)

(cid:16)

(cid:17)
Covfω(x,y)(y) + Efω(x,y)[y](cid:124)Efω(x,y)[y]

qθ(ω)dω

(cid:16)

(cid:17)
τ −1I + fω(x)(cid:124)fω(x)

qθ(ω)dω

= τ −1I + Eqθ (ω)[fω(x)(cid:124)fω(x)]

≈ τ −1I +

f ˆωi (x)(cid:124)f ˆωi(x)

1
T

T
(cid:88)

i=1

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

where we use MC integration with T samples for the ﬁnal step. The predictive covariance matrix is given by:

Covp∗ [y] ≈ τ −1I +

f ˆωi (x)(cid:124)f ˆωi(x) − Ep∗ [y](cid:124)Ep∗ [y]

1
T

T
(cid:88)

i=1

which is the sum of the variance from observation noise and the sample covariance from T stochastic forward passes
though the network.

The form of p∗ can be approximated by a Gaussian for each output dimension (for regression). We assume bounded
domains for each input dimension, wide layers throughout the network, and a uni-modal distribution of weights centered
at 0. By the Liapounov CLT condition, the ﬁrst layer then receives approximately Gaussian inputs (a proof can be found
in (Lehmann, 1999)). Having sampled µu
B from a mini-batch, each BN unit’s output is bounded. CLT thereby
continues to hold for deeper layers, including fω(x) = WLxL. A similar motivation for a Gaussian approximation of
Dropout has been presented by (Wang & Manning, 2013).

B and σu

Predictive Log Likelihood We use the Predictive Log Likelihood (PLL) as a measure to estimate the model’s uncertainty
quality. For a certain test point (yi, xi), the PLL deﬁnition and approximation can be expressed as:

PLL(fω(x), (yi, xi)) = log p(yi|fω(xi))

= log

fω(xi, yi)p(ω|D)dω

≈ log

fω(xi, yi)qθ(ω)dω

≈ log

p(yi|f ˆωj (xi))

(cid:90)

(cid:90)

1
T

T
(cid:88)

j=1

where ˆωj represents a sampled set of stochastic parameters from the approximate posterior distrubtion qθ(ω) and we take
a MC integration with T samples. For regression, due to the iid Gaussian noise, we can further develop the derivation into
the form we use when sampling:

PLL(fω(x), (yi, xi)) = log

N (yi|f ˆωj (xi), τ −1I)

1
T

T
(cid:88)

j=1

= logsumexpj=1,...,T
1
2

− log T −

log 2π +

1
2

1
2

log τ

(cid:0) −

τ ||yi − f ˆωj (xi)||2(cid:1)

Note that PLL makes no assumption on the form of the approximate predictive distribution.

1.6. Data

To assess the uncertainty quality of the various methods studied we rely on eight standard regression datasets, listed in Table
3. Publicly available from the UCI Machine Learning Repository (University of California, 2017) and Delve (Ghahramani,
1996), these datasets have been used to benchmark comparative models in recent related literature (see (Hern´andez-Lobato
& Adams, 2015), (Gal & Ghahramani, 2015), (Bui et al., 2016) and (Li & Gal, 2017)).

For image classiﬁcation, we applied MCBN using ResNet32 to CIFAR10.

For the image segmentation task, we applied MCBN using Bayesian SegNet on data from CamVid and PASCAL-VOC
using models published in (Kendall et al., 2015).

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Table 3. Regression dataset summary. Properties of the eight regression datasets used to evaluate MCBN. N is the dataset size and Q
is the n.o. input features. Only one target feature was used – we used heating load for the Energy Efﬁciency dataset, which contains
multiple target features.

Dataset name

Boston Housing
Concrete Compressive Strength
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein Tertiary Structure
Wine Quality (Red)
Yacht Hydrodynamics

N

506
1,030
768
8,192
9,568
45,730
1,599
308

Q

13
8
8
8
4
9
11
6

1.7. Extended experimental results

Below, we provide extended results measuring uncertainty quality. In Tables 4 and 5, we provide tables showing the mean
CRPS and PLL values for MCBN and MCDO. These results indicate that MCBN performs on par or better than MCDO
across several datasets. In Table 6 we provide the raw PLL and CRPS results for MCBN and MCDO. In Table 7 we provide
RMSE results of the MCBN and MCDO networks in comparison with non-stochastic BN and DO networks. These results
indicate that the procedure of multiple forward passes in MCBN and MCDO show slight improvements in the accuracy of
the network.

In Figure 4 and Figure 5, we provide a full set of our uncertainty quality visualization plots, where errors in predictions
are sorted by estimated uncertainty. The shaded areas show the model uncertainty and gray dots show absolute prediction
errors on the test set. A gray line depicts a running mean of the errors. The dashed line indicates the optimized constant
uncertainty. In these plots, we can see a correlation between estimated uncertainty (shaded area) and mean error (gray).
This trend indicates that the model uncertainty estimates can recognize samples with larger (or smaller) potential for
predictive errors.

We also conduct a sensitivity analysis to estimate how the uncertainty quality varies with batch size M and the number of
stochastic forward passes T . In tables 8 and 9 we evaluate CRPS and PLL respectively for the regression datasets when
trained and evaluated with varying batch sizes, but other hyperparameters ﬁxed (T was ﬁxed at 100). The results show that
results deteriorate when batch sizes are too small, likely stemming from the large variance of the approximate posterior.
In tables 10 and 11 we evaluate CRPS and PLL respectively for the regression datasets when trained and evaluated with
varying n.o. stochastic forward samples, but other hyperparameters ﬁxed (M was ﬁxed at 128). The results are indicative
of performance improvements with larger T , although we see improvements over baseline for some datasets already with
T = 50 (1/10:th of the T used in our main experiments).

Table 4. Uncertainty quality measured by CRPS on regression dasets. CRPS measured on eight datasets over 5 random 80-20 splits
of the data with 5 different random seeds each split. Mean values for MCBN, MCDO and MNF are reported along with standard error.
A signiﬁcance test was performed to check if CRPS signiﬁcantly exceeds the baseline. The p-value from a one sample t-test is reported.

CRPS

Dataset

MCBN

p-value

MCDO

p-value

MNF

p-value

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

8.50±0.86
3.91±0.25
5.75±0.52
2.85±0.18
0.24±0.05
2.66±0.10
0.26±0.07
-56.39±14.27

6.39E-10
4.53E-14
6.71E-11
2.33E-14
2.32E-04
2.77-12
1.26E-03
5.94E-04

3.06±0.33
0.93±0.41
1.37±0.89
1.82±0.14
-0.44±0.05
0.99±0.08
2.00±0.21
21.42±2.99

1.64E-09
3.13E-02
1.38E-01
1.64E-12
2.17E-08
2.34E-12
1.83E-09
2.16E-07

5.88±1.09
3.13±0.81
1.10±2.63
0.52±0.26
-0.89±0.15
0.57±0.03
0.93±0.12
24.92±3.77

2.01E-05
6.43E-04
6.45E-01
7.15E-02
3.36E-06
8.56E-16
6.19E-08
9.62E-06

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Table 5. Uncertainty quality measured by PLL on regression dasets. PLL measured on eight datasets over 5 random 80-20 splits of
the data with 5 different random seeds each split. Mean values for MCBN, MCDO and MNF are reported along with standard error. A
signiﬁcance test was performed to check if PLL signiﬁcantly exceeds the baseline. The p-value from a one sample t-test is reported.

PLL

Dataset

MCBN

p-value

MCDO

p-value

MNF

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

10.49±1.35
-36.36±12.12
10.89±1.16
1.68±0.37
0.33±0.14
2.56±0.23
0.19±0.09
45.58±5.18

5.41E-08
6.19E-03
1.79E-09
1.29E-04
2.72E-02
4.28E-11
3.72E-02
5.67E-09

5.51±1.05
10.92±1.78
-14.28±5.15
-0.26±0.18
3.52±0.23
6.23±0.19
2.91±0.35
-41.54±31.37

2.20E-05
2.34E-06
1.06E-02
1.53E-01
1.12E-13
2.57E-21
1.84E-08
1.97E-01

1.76±1.12
-2.16±4.19
-33.88±29.57
0.42±0.43
-0.86±0.15
0.52±0.07
0.83±0.16
46.19±4.45

p-value

1.70E-01
6.79E-01
2.70E-01
2.70E-01
7.33E-06
1.81E-07
2.27E-05
2.47E-07

Table 6. Raw (unnormalized) CRPS and PLL scores on regression datasets. CRPS and PLL measured on eight datasets over 5
random 80-20 splits of the data with 5 different random seeds each split. Mean values and standard errors are reported for MCBN,
MCDO and MNF.

Dataset

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

MCBN

1.45±0.02
2.40±0.04
0.33±0.01
0.04±0.00
2.00±0.01
1.95±0.01
0.34±0.00
0.68±0.02

CRPS
MCDO

1.41±0.02
2.42±0.04
0.26±0.00
0.04±0.00
2.00±0.01
1.95±0.00
0.33±0.00
0.32±0.01

MNF

MCBN

1.57±0.02
3.61±0.02
1.33±0.04
0.05±0.00
2.31±0.01
2.25±0.01
0.34±0.00
0.94±0.01

-2.38±0.02
-3.45±0.11
-0.94±0.04
1.21±0.01
-2.75±0.00
-2.73±0.00
-0.95±0.01
-1.39±0.03

PLL
MCDO

-2.35±0.02
-2.94±0.02
-0.80±0.04
1.24±0.00
-2.72±0.01
-2.70±0.00
-0.89±0.01
-2.57±0.69

MNF

-2.51±0.06
-3.35±0.04
-3.18±0.07
1.04±0.00
-2.86±0.01
-2.83±0.01
-0.93±0.00
-1.96±0.05

Table 7. Prediction accuracy measured by RMSE on regression datasets. RMSE measured on eight datasets over 5 random 80-20
splits of the data with 5 different random seeds each split. Mean values and standard errors are reported for for MCBN, MCDO and
MNF as well as conventional non-Bayesian models BN and DO.

Dataset

MCBN

BN

DO

MNF

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

2.75±0.05
4.78±0.09
0.59±0.02
0.07±0.00
3.74±0.01
3.66±0.01
0.62±0.00
1.23±0.05

2.77±0.05
4.89±0.08
0.57±0.01
0.07±0.00
3.74±0.01
3.69±0.01
0.62±0.00
1.28±0.06

RMSE
MCDO

2.65±0.05
4.80±0.10
0.47±0.01
0.07±0.00
3.74±0.02
3.66±0.01
0.60±0.00
0.75±0.03

2.69±0.05
4.99±0.10
0.49±0.01
0.07±0.00
3.72±0.02
3.68±0.01
0.61±0.00
0.72±0.04

2.98±0.06
6.57±0.04
2.38±0.07
0.09±0.00
4.19±0.01
4.10±0.01
0.61±0.00
2.13±0.05

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 4. Errors in predictions (gray dots) sorted by estimated uncertainty on select datasets. The shaded areas show model uncertainty
(light area 95% CI, dark area 50% CI). Gray dots show absolute prediction errors on the test set, and the gray line depicts a running
mean of the errors. The dashed line indicates the optimized constant uncertainty. A correlation between estimated uncertainty (shaded
area) and mean error (gray) indicates the uncertainty estimates are meaningful for estimating errors.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 5. Errors in predictions (gray dots) sorted by estimated uncertainty on select datasets. The shaded areas show model uncertainty
(light area 95% CI, dark area 50% CI). Gray dots show absolute prediction errors on the test set, and the gray line depicts a running
mean of the errors. The dashed line indicates the optimized constant uncertainty. A correlation between estimated uncertainty (shaded
area) and mean error (gray) indicates the uncertainty estimates are meaningful for estimating errors.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Table 8. Uncertainty quality sensitivity to batch size. A sensitivity analysis to determine how MCBN uncertainty quality varies with
batch size is measured on eight regression datasets using CRPS as the quality measure. Results are measured over 3 random 80-20 splits
of the data with 5 different random seeds each split.

CRPS
64

Batch size

8

16

32

128

256

512

1024

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

-7.1
-34.5
-61.6
-1.4
-10.5
14.5
2.2
15.1

16.6
6.0
-3.0
-4.3
0.8
4.8
1.6
-23.0

11.8
5.0
2.7
0.2
0.0
3.6
0.6
-30.4

7.2
5.1
9.8
2.8
-0.1
2.8
0.6
21.0

2.5
2.9
11.1
2.7
0.0
2.5
0.3
34.4

0.9
1.4
0.8
1.7
0.0
1.6
0.0
-

-
0.6
4.9
0.9
0.2
1.0
0.2
-

-
0.0
-
0.5
0.0
0.5
0.0
-

Table 9. Uncertainty quality sensitivity to batch size. A sensitivity analysis to determine how MCBN uncertainty quality varies with
batch size is measured on eight regression datasets using PLL as the quality measure. Results are measured over 3 random 80-20 splits
of the data with 5 different random seeds each split.

Batch size

8

16

32

128

256

512

1024

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

13.9
-113.3
-64.4
-4.9
-135.0
44.9
2.2
99.6

-36.7
-528.4
5.2
-5.4
-1.4
15.7
2.0
74.9

10.0
-10.0
-0.2
-3.1
-1.0
4.6
0.0
76.8

PLL
64

7.9
2.9
-9.6
1.6
-1.1
2.9
0.5
48.5

3.7
0.0
-14.5
2.3
-0.4
2.8
0.6
44.9

1.5
1.4
1.4
1.5
0.1
2.2
0.4
-

-
0.2
10.4
0.7
-0.1
1.2
0.0
-

-
0.0
-
0.4
0.4
0.6
0.0
-

Table 10. Uncertainty quality sensitivity to n.o. stochastic forward passes. A sensitivity analysis to determine how MCBN uncer-
tainty quality varies with the n.o. stochastic forward passes measured on eight regression datasets using CRPS as the quality measure.
Results are measured over 3 random 80-20 splits of the data with 5 different random seeds each split.

Forward passes

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

CRPS
100

2.7
2.3
4.2
2.7
0.5
2.7
-0.4
32.2

50

3.2
3.3
7.9
4.2
0.1
2.4
0.6
32.1

250

6.1
3.3
13.2
3.2
0.2
2.3
0.9
32.9

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Table 11. Uncertainty quality sensitivity to n.o. stochastic forward passes. A sensitivity analysis to determine how MCBN uncer-
tainty quality varies with the n.o. stochastic forward passes measured on eight regression datasets using PLL as the quality measure.
Results are measured over 3 random 80-20 splits of the data with 5 different random seeds each split.

Forward passes

Boston Housing
Concrete
Energy Efﬁciency
Kinematics 8nm
Power Plant
Protein
Wine Quality (Red)
Yacht Hydrodynamics

250

7.8
3.8
15.7
2.5
-0.9
1.8
1.7
38.0

PLL
100

1.9
7.1
-30.5
2.2
0.7
2.0
-0.9
35.9

50

2.6
0.1
-47.3
3.4
-0.9
2.4
1.1
35.5

1.8. Uncertainty in image segmentation

We applied MCBN to an image segmentation task using Bayesian SegNet with the main CamVid and PASCAL-VOC
models in (Kendall et al., 2015). Here, we provide more image from Pascal VOC dataset in Figure 6.

1.9. Batch normalization statistics

In Figure 7 and Figure 8, we provide statistics on the batch normalization parameters used for training. The plots show the
distribution of BN mean and BN variance over different mini-batches of an actual training of Yacht dataset for one unit in
the ﬁrst hidden layer and the second hidden layer. Data is provided for different epochs and for different batch sizes.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 6. Uncertainty in image segmentation. Results applying MCBN to Bayesian SegNet (Kendall et al., 2015) on images from
PASCAL-VOC (right). Left: original. Middle: the Bayesian estimated segmentation. Right: estimated uncertainty using MCBN for all
classes. Mini-batches of size 36 were used for PASCAL-VOC on images of size 224x224. 20 inferences were conducted to estimate the
mean and variance of MCBN.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 7. The distribution of means of mini-batches during training of one of our datasets. The distribution closely follows our analyt-
ically approximated Gaussian distribution. The data is collected for one unit of each layer and is provided for different epochs and for
different batch sizes.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Figure 8. The distribution of standard deviation of mini-batches during training of one of our datasets. The distribution closely follows
our analytically approximated Gaussian distribution. The data is collected for one unit of each layer and is provided for different epochs
and for different batch sizes.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

References

Bui, T. D., Hern´andez-Lobato, D., Li, Y., Hern´andez-Lobato, J. M., and Turner, R. E. Deep Gaussian Processes for

Regression using Approximate Expectation Propagation. In ICML, 2016.

Chen, X., Kundu, K., Zhang, Z., Ma, H., Fidler, S., and Urtasun, R. Monocular 3d object detection for autonomous driving.

In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2147–2156, 2016.

Djuric, U., Zadeh, G., Aldape, K., and Diamandis, P. Precision histology: how deep learning is poised to revitalize

histomorphology for personalized cancer care. npj Precision Oncology, 1(1):22, 2017.

Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau, H. M., and Thrun, S. Dermatologist-level classiﬁcation

of skin cancer with deep neural networks. Nature, Feb 2017.

Gal, Y. Uncertainty in Deep Learning. PhD thesis, University of Cambridge, 2016.

Gal, Y. and Ghahramani, Z. Dropout as a Bayesian Approximation : Representing Model Uncertainty in Deep Learning.

ICML, 48:1–10, 2015.

data/kin/desc.html.

Ghahramani, Z. Delve Datasets. University of Toronto, 1996. URL http://www.cs.toronto.edu/{˜}delve/

Ghahramani, Z. Probabilistic machine learning and artiﬁcial intelligence. Nature, 521(7553):452–459, May 2015.

Gneiting, T. and Raftery, A. E. Strictly Proper Scoring Rules, Prediction, and Estimation. Journal of the American

Statistical Association, 102(477):359–378, 2007.

Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples.

arXiv preprint

arXiv:1412.6572, 2014.

Graves, A. Practical Variational Inference for Neural Networks. NIPS, 2011.

Hern´andez-Lobato, J. M. and Adams, R. Probabilistic backpropagation for scalable learning of bayesian neural networks.

In International Conference on Machine Learning, pp. 1861–1869, 2015.

Hinton, G. E. and Van Camp, D. Keeping the neural networks simple by minimizing the description length of the weights.

In Proceedings of the sixth annual conference on Computational learning theory, pp. 5–13. ACM, 1993.

Ioffe, S. Batch renormalization: Towards reducing minibatch dependence in batch-normalized models. CoRR,

abs/1702.03275, 2017. URL http://arxiv.org/abs/1702.03275.

Ioffe, S. and Szegedy, C. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.

Arxiv, 2015. URL http://arxiv.org/abs/1502.03167.

Karpathy, A. Convnetjs demo: toy 1d regression, 2015. URL http://cs.stanford.edu/people/karpathy/

convnetjs/demo/regression.html.

Kendall, A., Badrinarayanan, V., and Cipolla, R. Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-
Decoder Architectures for Scene Understanding. CoRR, abs/1511.0, 2015. URL http://arxiv.org/abs/1511.
02680.

Kingma, D. P. and Welling, M. Auto-Encoding Variational Bayes. In ICLR, 2014.

Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. 2009.

Krueger, D., Huang, C.-W., Islam, R., Turner, R., Lacoste, A., and Courville, A. Bayesian hypernetworks. arXiv preprint

arXiv:1710.04759, 2017.

Lehmann, E. L. Elements of Large-Sample Theory. Springer Verlag, New York, 1999. ISBN 0387985956.

Li, Y. and Gal, Y. Dropout Inference in Bayesian Neural Networks with Alpha-divergences. arXiv, 2017.

Bayesian Uncertainty Estimation for Batch Normalized Deep Networks

Louizos, C. and Welling, M. Multiplicative normalizing ﬂows for variational Bayesian neural networks. In Precup, D. and
Teh, Y. W. (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings
of Machine Learning Research, pp. 2218–2227, International Convention Centre, Sydney, Australia, 06–11 Aug 2017.
PMLR. URL http://proceedings.mlr.press/v70/louizos17a.html.

MacKay, D. J. A practical bayesian framework for backpropagation networks. Neural computation, 4(3):448–472, 1992.

Neal, R. M. Bayesian Learning for Neural Networks. PhD thesis, University of Toronto, 1995.

Neal, R. M. Bayesian learning for neural networks, volume 118. Springer Science & Business Media, 2012.

Selten, R. Axiomatic characterization of the quadratic scoring rule. Experimental Economics, 1(1):43–62, 1998.

Shen, L. End-to-end training for whole image breast cancer diagnosis using an all convolutional design. arXiv preprint

University of California, I. UC Irvine Machine Learning Repository, 2017. URL https://archive.ics.uci.

arXiv:1708.09427, 2017.

edu/ml/index.html.

Wang, S. I. and Manning, C. D. Fast dropout training. Proceedings of the 30th International Conference on Machine Learn-
ing, 28:118–126, 2013. URL http://machinelearning.wustl.edu/mlpapers/papers/wang13a.

