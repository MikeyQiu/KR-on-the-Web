0
2
0
2
 
r
a

M
 
5
 
 
]

V
C
.
s
c
[
 
 
1
v
7
6
5
2
0
.
3
0
0
2
:
v
i
X
r
a

GANWRITING: CONTENT-CONDITIONED GENERATION OF
STYLED HANDWRITTEN WORD IMAGES

Lei Kang∗†, Pau Riba∗, Yaxing Wang∗, Marçal Rusiñol∗, Alicia Fornés∗, Mauricio Villegas†
∗Computer Vision Center, Universitat Autònoma de Barcelona, Barcelona, Spain
{lkang, priba, yaxing, marcal, afornes}@cvc.uab.es
†omni:us, Berlin, Germany
{lei, mauricio}@omnius.com

March 6, 2020

ABSTRACT

Although current image generation methods have reached impressive quality levels, they are still
unable to produce plausible yet diverse images of handwritten words. On the contrary, when writing
by hand, a great variability is observed across different writers, and even when analyzing words
scribbled by the same individual, involuntary variations are conspicuous. In this work, we take a
step closer to producing realistic and varied artiﬁcially rendered handwritten words. We propose a
novel method that is able to produce credible handwritten word images by conditioning the generative
process with both calligraphic style features and textual content. Our generator is guided by three
complementary learning objectives: to produce realistic images, to imitate a certain handwriting style
and to convey a speciﬁc textual content. Our model is unconstrained to any predeﬁned vocabulary,
being able to render whatever input word. Given a sample writer, it is also able to mimic its
calligraphic features in a few-shot setup. We signiﬁcantly advance over prior art and demonstrate
with qualitative, quantitative and human-based evaluations the realistic aspect of our synthetically
produced images.

Keywords Generative adversarial networks, style and content conditioning, handwritten word
images.

1

Introduction

In just a few years since the conception of Generative Adversarial Networks (GANs) [1], we have witnessed an
impressive progress on generating illusory plausible images. From the early low-resolution and hazy results, the
quality of the artiﬁcially generated images has been notably enhanced. We are now able to synthetically produce
high-resolution [2] artiﬁcial images that are indiscernible from real ones to the human observer [3].

In the original GAN architecture, inputs were randomly sampled from a latent space, so that it was hard to control which
kind of images were being generated. The conception of conditional Generative Adversarial Networks (cGANs) [4] led
to an important improvement. By allowing to condition the generative process on an input class label, the networks
were then able to produce images from different given types [5]. However, such classes had to be predeﬁned beforehand
during the training stage and thus, it was impossible to produce images from other unseen classes during inference.

But generative networks have not exclusively been used to produce synthetic images. The generation of data that is
sequential in nature has also been largely explored in the literature. Generative methods have been proposed to produce
audio signals [6], natural language excerpts [7], video streams [8] or stroke sequences [9, 10, 11, 12] able to trace
sketches, drawings or handwritten text. In all of those approaches, in order to generate sequential data, the use of
Recurrent Neural Networks (RNNs) has been adopted.

A PREPRINT - MARCH 6, 2020

Figure 1: Turing’s test. Just ﬁve of the above words are real. Try to distinguish them from the artiﬁcially generated
samples1.

Yet, for the speciﬁc case of generating handwritten text, one could also envisage the option of directly producing the
ﬁnal images instead of generating the stroke sequences needed to pencil a particular word. Such non-recurrent approach
presents several beneﬁts. First, the training procedure is more efﬁcient since recurrencies are avoided and the inherent
parallelism nature of convolutional networks is leveraged. Second, since the output is generated all at once, we avoid
the difﬁculties of learning long-range dependencies as well as vanishing gradient problems. Finally, online training data
(pen-tip location sequences), which is hard to obtain, is no longer needed.

Nevertheless, the different attempts to directly generate raw word images present an important drawback. Similarly
to the case with cGANs, most of the proposed approaches are just able to condition the word image generation to a
predeﬁned set of words, limiting its practical use. For example [13] is speciﬁcally designed to generate isolated digits,
while [14] is restricted to a handful of Chinese characters. To our best knowledge, the only exception to that is the
approach by Alonso et al. [15]. In their work they propose a non-recurrent generative architecture conditioned to input
content strings. By having such design, the generative process is not restricted to a particular predeﬁned vocabulary,
and could potentially generate any word. However, the produced results are not realistic, still exhibiting a poor quality,
sometimes producing barely legible word images. Their proposed approach also suffers from the mode collapse
problem, tending to produce images with a unique writing style. In this paper we present a non-recurrent generative
architecture conditioned to textual content sequences, that is specially tailored to produce realistic handwritten word
images, indistinguishable to humans. Real and generated images are actually difﬁcult to tell apart, as shown in Figure 1.
In order to produce diverse styled word images, we propose to condition the generative process not only with textual
content, but also with a speciﬁc writing style, deﬁned by a latent set of calligraphic attributes.

Therefore, our approach is able to artiﬁcially render realistic handwritten word images that match a certain textual
content and that mimic some style features (text skew, slant, roundness, stroke width, ligatures, etc.) from an exemplar
writer. To this end, we guide the learning process by three different learning objectives [16]. First, an adversarial
discriminator ensures that the images are realistic and that its visual appearance is as closest as possible to real
handwritten word images. Second, a style classiﬁer guarantees that the provided calligraphic attributes, characterizing
a particular handwriting style, are properly transferred to the generated word instances. Finally, a state-of-the-art
sequence-to-sequence handwritten word recognizer [17] controls that the textual contents have been properly conveyed
during the image generation. To summarize, the main contributions of the paper are the following:

• Our model conditions the handwritten word generative process both with calligraphic style features and
textual content, producing varied samples indistinguishable by humans, surpassing the quality of the current
state-of-the-art approaches.

• We introduce the use of three complementary learning objectives to guide different aspects of the generative

• We propose a character-based content conditioning that allows to generate any word, without being restricted

• We put forward a few-shot calligraphic style conditioning to avoid the mode collapse problem.

process.

to a speciﬁc vocabulary.

2 Related Work

The generation of realistic synthetic handwritten word images is a challenging task. To this day, the most convincing
approaches involved an expensive manual intervention aimed at clipping individual characters or glyphs [18, 19, 20, 21,
22]. When such approaches were combined with appropriate rendering techniques including ligatures among strokes,
textures and background blending, the obtained results were indeed impressive. Haines et al. [22] illustrated how such

1

Therealwordsare:"that","vision","asked","hits"and"writer".

2

A PREPRINT - MARCH 6, 2020

approaches could artiﬁcially generate indistinguishable manuscript excerpts as if they were written by Sir Arthur Conan
Doyle, Abraham Lincoln or Frida Kahlo. Of course such manual intervention is extremely expensive, and in order to
produce large volumes of manufactured images the use of truetype electronic fonts has also been explored [23, 24].
Although such approaches beneﬁt from a greater scalability, the realism of the generated images clearly deteriorates.

With the advent of deep learning, the generation of handwritten text was approached differently. As shown in the
seminal work by Alex Graves [9], given a reasonable amount of training data, an RNN could learn meaningful latent
spaces that encode realistic writing styles and their variations, and then generate stroke sequences that trace a certain
text string. However, such sequential approaches [9, 10, 11, 12] need temporal data, obtained by recording with a digital
stylus pen real handwritten samples, stroke-by-stroke, in vector form.

Contrary to sequential approaches, non-recurrent generative methods have been proposed to directly produce images.
Both variational auto-encoders [25] and GANs [1] were able to learn the MNIST manifold and generate artiﬁcial
handwritten digit images in the original publications. With the emergence of cGANs [4], able to condition the generative
process on an input image rather than a random noise vector, the adversarial-guided image-to-image translation problem
started to rise. Image-to-image translation has since been applied to many different style transfer applications, as
demonstrated in [26] with the pix2pix network. Since then, image translation approaches have been acquiring the ability
to disentangle style attributes from the contents of the input images, producing better style transfer results [27, 28].

Related to the generation of handwritten textual contents, such approaches have been mainly used for the synthesis
of Chinese ideograms [29, 30, 14, 31]. However, they are restricted to work over a predeﬁned set of content classes.
The incapability to generate texts out of vocabulary (OOV) limits its practical application. An exception to that is the
work of Alonso et al. [15], where the generation of handwritten word samples is conditioned by character sequences.
However, their proposal suffers from the mode collapse problem, hindering the diversity of the generated images.
Techniques like FUNIT [32], able to transfer unseen target styles to the content generated images could be beneﬁcial
for this limitation. In particular, the use of Adaptive Instance Normalization (AdaIN) layers, proposed in [33], shall
allow to align both textual content and style attributes within the generative process.

Summarizing, state-of-the-art generative methods are still unable to produce plausible yet diverse images of whatever
handwritten word automatically. In this paper we propose to condition a generative model for handwritten words with
unconstrained text sequences and stylistic typographic attributes, so that we are able to generate any word with a great
diversity over the produced results.

3 Conditioned Handwritten Generation

3.1 Problem Formulation

Let {X , Y, W} be a multi-writer handwritten word dataset, containing grayscale word images X , their corresponding
transcription strings Y and their writer identiﬁers W = {wi}N
j=1 ⊂ X be a subset of K randomly
sampled handwritten word images from the same given writer wi ∈ W. Let A be the alphabet containing the allowed
characters (letters, digits, punctuation signs, etc.), Al being all the possible text strings with length l. Given a set of
images Xi as a few-shot example of the calligraphic style attributes for writer wi on the one hand, and given a textual
content provided by any text string t ∈ Al on the other hand; the proposed generative model has the ability to combine
both sources of information. It has the objective to yield a handwritten word image having textual content equal to t and
sharing calligraphic style attributes with writer wi. Following this formulation, the generative model H is deﬁned as

i=1. Let Xi = {xwi,j}K

¯x = H (t, Xi) = H (t, {x1, . . . , xK}) ,

(1)

where ¯x is the artiﬁcially generated handwritten word image with the desired properties. Moreover, we denote ¯X as the
output distribution of the generative network H.

The proposed architecture is divided in two main components. The generative network produces human-readable
images conditioned to the combination of calligraphic style and textual content information. The second component are
the learning objectives which guide the generative process towards producing images that look realistic; exhibiting a
particular calligraphic style attributes; and having a speciﬁc textual content. Figure 2 gives an overview of our model.

3.2 Generative Network

The proposed generative architecture H consists of a calligraphic style encoder S, a textual content encoder C and a
conditioned image generator G. The overall calligraphic style of input images Xi is disentangled from their individual
textual contents, whereas the string t provides the desired content.

3

A PREPRINT - MARCH 6, 2020

Figure 2: Architecture of the proposed handwriting generation model.

Calligraphic style encoding. Given the set Xi ⊂ X of K = 15 word images from the same writer wi, the style
encoder aims at extracting the calligraphic style attributes, i.e. slant, glyph shapes, stroke width, character roundness,
ligatures etc. from the provided input samples. Speciﬁcally, our proposed network S learns a style latent space mapping,
in which the obtained style representations Fs = S(Xi) are disentangled from the actual textual contents of the images
Xi. The VGG-19-BN [34] architecture is used as the backbone of S. In order to process the input image set Xi, all the
images are resized to have the same height h, padded to meet a maximum width w and concatenated channel-wise to
end up with a single tensor h × w × K. If we ask a human to write the same word several times, slight involuntary
variations appear. In order to imitate this phenomenon, randomly choosing permutations of the subset Xi will already
produce such characteristic ﬂuctuations. In addition, an additive noise Z ∼ N (0, 1) is applied to the output latent space
to obtain a subtly distorted feature representation ˆFs = Fs + Z.
Textual content encoding. The textual content network C is devoted to produce an encoding of the given text string t
that we want to artiﬁcially write. The proposed architecture outputs content features at two different levels. Low-level
features encode the different characters that form a word and their spatial position within the string. A subsequent
broader representation aims at guiding the whole word consistency. Formally, let t ∈ Al be the input text string,
character sequences shorter than l are padded with the empty symbol ε. Let us deﬁne a character-wise embedding
function e : A → Rn. The ﬁrst step of the content encoding stage embeds with a linear layer each character c ∈ t,
represented by a one-hot vector, into a character-wise latent space. Then, the architecture is divided into two branches.
Character-wise encoding: Let g1 : Rn → Rm be a Multi-Layer Perceptron (MLP). Each embedded character e(c) is
processed individually by g1 and their results are later stacked together. In order to combine such representation with
style features, we have to ensure that the content feature map meets the shape of ˆFs. Each character embedding is
repeated multiple times horizontally to coarsely align the content features with the visual ones extracted from the style
network, and the tensor is ﬁnally vertically expanded. The two feature representations are concatenated to be fed to the
generator F = [ ˆFs (cid:107) Fc]. Such a character-wise encoding enables the network to produce OOV words, i.e. words that
have never been seen during training.
Global string encoding: Let g2 : Rl·n → R2p·q be another MLP aimed at obtaining a much broader and global string
representation. The character embeddings e(c) are concatenated into a large one-dimensional vector of size l · n that is
then processed by g2. Such global representation vector fc will be then injected into the generator splitted into p pairs
of parameters.

Both functions g1(·) and g2(·) make use of three fully-connected layers with ReLU activation functions and batch
normalization [35].

Generator. Let F be the combination of the calligraphic style attributes and the textual content information character-
wise; and fc the global textual encoding. The generator G is composed of two residual blocks [36] using the AdaIN
as the normalization layer. Then, four convolutional modules with nearest neighbor up-sampling and a ﬁnal tanh

4

A PREPRINT - MARCH 6, 2020

activation layer generates the output image ¯x. AdaIN is formally deﬁned as

AdaIN (z, α, β) = α

(cid:19)

(cid:18) z − µ (z)
σ (z)

+ β,

where z ∈ F , µ and σ are the channel-wise mean and standard deviations. The global content information is injected
four times (p = 4) during the generative process by the AdaIN layers. Their parameters α and β are obtained by
splitting fc in four pairs. Hence, the generative network is deﬁned as
¯x = H (t, Xi) = G (C (t) , S (Xi)) = G (cid:0)g1

(cid:0)ˆt(cid:1) , g2
where ˆt = [e(c); ∀c ∈ t] is the encoding of the string t character by character.

(cid:0)ˆt(cid:1) , S (Xi)(cid:1) ,

(3)

3.3 Learning Objectives

We propose to combine three complementary learning objectives: a discriminative loss, a style classiﬁcation loss and a
textual content loss. Each one of these losses aim at enforcing different properties of the desired generated image ¯x.

Discriminative Loss. Following the paradigm of GANs [1], we make use of a discriminative model D to estimate
the probability that samples come from a real source, i.e. training data X , or belong to the artiﬁcially generated
distribution ¯X . Taking the generative network H and the discriminator D, this setting corresponds to a min max
optimization problem. The proposed discriminator D starts with a convolutional layer, followed by six residual blocks
with LeakyReLU activations and average poolings. A ﬁnal binary classiﬁcation layer is used to discern between fake
and real images. Thus, the discriminative loss only controls that the general visual appearance of the generated image
looks realistic. However, it does not take into consideration neither the calligraphic styles nor the textual contents. This
loss is formally deﬁned as

Ld (H, D) = Ex∼X [log (D (x))] + E

¯x∼ ¯X [log (1 − D (¯x))] .

Style Loss. When generating realistic handwritten word images, encoding information related to calligraphic styles
not only provides diversity on the generated samples, but also prevents the mode collapse problem. Calligraphy is a
strong identiﬁer of different writers. In that sense, the proposed style loss guides the generative network H to generate
samples conditioned to a particular writing style by means of a writer classiﬁer W . Given a handwritten word image,
W tries to identify the writer wi ∈ W who produced it. The writer classiﬁer W follows the same architecture of the
discriminator D with a ﬁnal classiﬁcation MLP with the amount of writers in our training dataset. The classiﬁer W is
only optimized with real samples drawn from X , but it is used to guide the generation of the synthetic ones. We use the
cross entropy loss, formally deﬁned as

Lw (H, W ) = −E

x∼{X , ¯X }

wi log ( ˆwi)

 ,







|W|
(cid:88)

i=1

where ˆw = W (x) is the predicted probability distribution over writers in W and wi the real writer distribution.
Generated samples should be classiﬁed as the writer wi used to construct the input style conditioning image set Xi.

Content Loss. A ﬁnal handwritten word recognizer network R is used to guide our generator towards producing
synthetic word images with a speciﬁc textual content. We implemented a state-of-the-art sequence-to-sequence
model [17] for handwritten word recognition to examine whether the produced images ¯x are actually decoded as
the string t. The recognizer, depicted in Figure 3, consists of an encoder and a decoder coupled with an attention
mechanism. Handwritten word images are processed by the encoder and high-level feature representations are obtained.
A VGG-19-BN [34] architecture followed by a two-layered Bi-directional Gated Recurrent Unit (B-GRU) [37] is
used as the encoder network. The decoder is a one-directional RNN that outputs character by character predictions at
each time step. The attention mechanism dynamically aligns context features from each time step of the decoder with
high-level features from the encoder, hopefully corresponding to the next character to decode. The Kullback-Leibler
divergence loss is used as the recognition loss at each time step. This is formally deﬁned as

Lr (H, R) = −E

x∼{X , ¯X }





l
(cid:88)

|A|
(cid:88)

i=0

j=0

ti,j log



(cid:19)

 ,

(cid:18) ti,j
ˆti,j

where ˆt = R(x); ˆti being the i-th decoded character probability distribution by the word recognizer, ˆti,jbeing the
probability of j-th symbol in A for ˆti, and ti,j being the real probability corresponding to ˆti,j. The empty symbol ε is
ignored in the loss computation; ti denotes the i-th character on the input text t.

(2)

(4)

(5)

(6)

5

A PREPRINT - MARCH 6, 2020

Figure 3: Architecture of the attention-based sequence-to-sequence handwritten word recognizer R.

Algorithm 1 Training algorithm for the proposed model.

Input: Input data {X , Y, W}; alphabet A; max training iterations T
Output: Networks parameters {ΘH , ΘD, ΘW , ΘR}.

1: repeat
2:
3:
4:
5:
6:
7:
8:
9: until Max training iterations T

Get style and content mini-batches {Xi, wi}NB
Ld ← Eq. 4
Lw,r ← Eq. 5 + Eq. 6
ΘD ← ΘD + Γ(∇ΘD Ld)
ΘW,R ← ΘW,R − Γ(∇ΘW,R Lw,d)
L ← Eq. 7
ΘH ← ΘH − Γ(∇ΘH L)

3.4 End-to-end Training

i=1 and {ti}NB

i=1

(cid:46) Real and generated samples x ∼ {X , ¯X }
(cid:46) Real samples x ∼ X

(cid:46) Generated samples x ∼ ¯X

Overall, the whole architecture is trained end to end with the combination of the three proposed loss functions

L(H, D, W, R) = Ld(H, D) + Lw(H, W ) + Lr(H, R),

min
H,W,R

max
D

L(H, D, W, R).

Algorithm 1 presents the training strategy that has been followed in this work. Γ(·) denotes the optimizer function.
Note that the parameter optimization is performed in two steps. First, the discriminative loss is computed using both
real and generated samples (line 3). The style and content losses are computed by just providing real data (line 4). Even
though W and D are optimized using only real data and, therefore, they could be pre-trained independently from the
generative network H, we obtained better results by initializing all the networks from scratch and jointly training them
altogether. The network parameters ΘD are optimized by gradient ascent following the GAN paradigm whereas the
parameters ΘW and ΘR are optimized by gradient descent. Finally, the overall generator loss is computed following
Equation 7 where only the generator parameters ΘH are optimized (line 8).

(7)

(8)

4 Experiments

To carry out the different experiments, we have used a subset of the IAM corpus [38] as our multi-writer handwritten
dataset {X , Y, W}. It consists of 62, 857 handwritten word snippets, written by 500 different individuals. Each word
image has its associated writer and transcription metadata. A test subset of 160 writers has been kept apart during
training to check whether the generative model is able to cope with unseen calligraphic styles. We have also used a
subset of 22, 500 unique English words from the Brown [39] corpus as the source of strings for the content input. A test
set of 400 unique words, disjoint from the IAM transcriptions has been used to test the performance when producing
OOV words. To quantitatively measure the image quality, diversity and the ability to transfer style attributes of the
proposed approach we will use the Fréchet Inception Distance (FID) [40, 41], measuring the distance between the

6

Inception-v3 activation distributions for generated ¯X and real samples X for each writer wi separately, and ﬁnally
averaging them. Inception features, trained over ImageNet data, have not been designed to discern between different
handwriting images. Although this measure might not be ideal to evaluate our speciﬁc case, it will still serve as an
indication of the similarity between generated and real images.

A PREPRINT - MARCH 6, 2020

a) IV-S

b) IV-U

c) OOV-S

d) OOV-U

Figure 4: Word image generation. a) In-Vocabulary (IV) words and seen (S) styles; b) In-Vocabulary (IV) words and
unseen (U) styles; c) Out-of-Vocabulary (OOV) words and seen (S) styles and d) Out-of-Vocabulary (OOV) words and
unseen (U) styles.

4.1 Generating Handwritten Word Images

We present in Figure 4 an illustrative selection of generated handwritten words. We appreciate the realistic and diverse
aspect of the produced images. Qualitatively, we observe that the proposed approach is able to yield satisfactory results
even when dealing with both words and calligraphic styles never seen during training. But, when analyzing the different
experimental settings in Table 1, we appreciate that the FID measure slightly degrades when either we input an OOV
word or a style never seen during training. Nevertheless, the reached FID measures in all four settings satisfactorily
compare with the baseline achieved by real data.

Table 1: FID between generated images and real images of corresponding set.

Real images

IV-S

IV-U

OOV-S OOV-U

FID

90.43

120.07

124.30

125.87

130.68

Figure 5: t-SNE embedding visualization of 2.500 generated instances of the word "deep".

In order to show the ability of the proposed method to produce a diverse set of generated images, we present in Figure 5
a t-SNE [42] visualization of different instances produced with a ﬁxed textual content while varying the calligraphic

7

A PREPRINT - MARCH 6, 2020

style inputs. Different clusters corresponding to particular slants, stroke widths, character roundnesses, ligatures and
cursive writings are observed.

Table 2: Comparison of handwritten word generation with FUNIT [32].

Style Images

t
n
e
t
n
o
C

l
a
u
t
x
e
T

FUNIT

ours
"which"

FUNIT

ours
"Thank"

FUNIT

ours
"inside"

To further demonstrate the ability of the proposed approach to coalesce content and style information into the generated
handwritten word images, we compare in Table 2 our produced results with the outcomes of the state-of-the-art approach
FUNIT [32]. Being an image-to-image translation method, FUNIT starts with a content image and then injects the style
attributes derived from a second sample image. Although FUNIT performs well for natural scene images, it is clear that
such kind of approaches do not apply well for the speciﬁc case of handwritten words. Starting with a content image
instead of a text string conﬁnes the generative process to the shapes of the initial drawing. When infusing the style
features, the FUNIT method is only able to deform the stroke textures, often resulting in extremely distorted words.
Conversely, our proposed generative process is able to produce realistic and diverse word samples given a content text
string and a calligraphic style example. We observe how for the different produced versions of the same word, the
proposed approach is able to change style attributes as stroke width or slant, to produce both cursive words, where
all characters are connected through ligatures as well as disconnected writings, and even render the same characters
differently, e.g. note the characters n or s in "Thank" or "inside" respectively.

4.2 Latent Space Interpolations

The generator network G learns to map feature points F in the latent space to synthetic handwritten word images. Such
latent space presents a structure worth exploring. We ﬁrst interpolate in Figure 6 between two different points F A
s
and F B
s corresponding to two different calligraphic styles wA and wB while keeping the textual contents t ﬁxed. We
observe how the generated images smoothly adjust from one style to another. Again note how individual characters
evolve from one typography to another, e.g. the l from "also", or the f from "final".

Contrary to the continuous nature of the style latent space, the original content space is discrete in nature. Instead
of computing point-wise interpolations, we present in Figure 7 the obtained word images for different styles when
following a “word ladder” puzzle game, i.e. going from one word to another, one character difference at a time. Here
we observe how different contents inﬂuence stylistic aspects. Usually s and i are disconnected when rendering the
word "sired" but often appear with a ligature when jumping to the word "fired".

4.3

Impact of the Learning Objectives

Along this paper, we have proposed to guide the generation process by three complementary goals. The discriminator
loss Ld controlling the genuine appearance of the generated images ¯x. The writer classiﬁcation loss Lw forcing ¯x to
mimic the calligraphic style of input images Xi. The recognition loss Lr guaranteeing that ¯x is readable and conveys
the exact text information t. We analyze in Table 3 the effect of each learning objective.

The sole use of the Ld leads to constantly generating an image that is able to fool the discriminator. Although
the generated image looks like handwritten strokes, the content and style inputs are ignored. When combining the

8

wA

wB

A PREPRINT - MARCH 6, 2020

Real

Generated

Real

Generated

Real

Generated

Real

Generated

Real

Generated

Real

Generated

Figure 6: Latent space interpolation between two different calligraphic styles for different words while keeping contents
ﬁxed.

"three"

"threw"

"shrew"

"shred"

"sired"

"fired"

"fined"

"firer"

"fiver"

"fever"

"sever"

"seven"

Figure 7: Word ladder. From "three" to "seven" changing one character at a time, generated for ﬁve different
calligraphic styles.

Table 3: Effect of each different learning objectives when generating the content t = "vision" for different styles.

Style Images

Ld Lw Lr

FID

364.10

(cid:88) -
-
(cid:88) (cid:88) -
207.47
(cid:88) - (cid:88) 138.80
(cid:88) (cid:88) (cid:88) 130.68

discriminator with the auxiliary task of writer classiﬁcation Lw, the produced results are more encouraging, but the
input text is still ignored, always tending to generate the word "the", since it is the most common word seen during
training. When combining the discriminator with the word recognizer loss Lr, the desired word is rendered. However,
as in [15], we suffer from the mode collapse problem, always producing unvarying word instances. When combining
the three learning objectives we appreciate that we are able to correctly render the appropriate textual content while
mimicking the input styles, producing diverse results. We appreciate that the FID measure also decreases for each
successive combination.

4.4 Human Evaluation

Finally, besides providing qualitative results and evaluating the generative process with the FID measure, we also tested
whether the generated images were actually indistinguishable from real ones by human judgments. We have conducted
a human evaluation study as follows: we have asked 200 human examiners to assess whether a set of images were

9

Table 4: Human evaluation experiment where examiners had to determine whether words were real or artiﬁcially
generated.

A PREPRINT - MARCH 6, 2020

Actual

Genuine
Generated

Predicted

Real

Fake

R: 54.1
27.01
27.69
FPR: 55.4
P: 49.4 FOR: 50.8 ACC: 49.3

22.99
22.31

a) Confusion matrix (%)

b) Accuracy distribution

written by a human or artiﬁcially generated. Appraisers were presented a total of sixty images, one at a time, and they
had to choose if each of them was real of fake. To construct the test, we chose thirty real words from the IAM test
partition from ten different writers. We then generated thirty artiﬁcial samples by using OOV textual contents and by
randomly taking the previous writers as the sources for the calligraphic styles. Such sets were not curated, so the only
ﬁlter was that the generated samples had to be correctly transcribed by the word recognizer network R. In total we
collected 12, 000 responses.

In Table 4 we present the confusion matrix of the human assessments, with Accuracy (ACC), Precision (P), Recall (R),
False Positive Rate (FPR) and False Omission Rate (FOR) values. The study revealed that our generative model was
clearly perceived as plausible, since a great portion of the generated samples were deemed genuine. Only a 49.3% of
the images were properly identiﬁed, which shows a similar performance than a random binary classiﬁer. Accuracies
over different examiners were normally distributed. We also observe that the synthetically generated word images were
judged more often as being real than correctly identiﬁed as fraudulent, with a ﬁnal False Positive Rate of 55.4%.

In this paper we have presented a novel image generation architecture that produces realistic and varied artiﬁcially
rendered samples of handwritten words. Our proposed generative pipeline is able to yield credible handwritten word
images, by conditioning the generative process with both calligraphic style features and textual content. Furthermore,
by jointly guiding our generator with three different cues: a discriminator, a style classiﬁer and a content recognizer, our
model is able to render any input word, not depending on any predeﬁned vocabulary, while incorporating calligraphic
styles in a few-shot setup. Experimental results demonstrate that the proposed method yields images with such a great
realistic quality that are indistinguishable by humans.

5 Conclusion

References

[1] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,
and Yoshua Bengio. Generative adversarial nets. In Proceedings of the Neural Information Processing Systems
Conference, 2014.

[2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high ﬁdelity natural image

synthesis. In Proceedings of the International Conference on Learning Representations, 2019.

[3] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial

networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.

[4] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784,

2014.

[5] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. StarGAN: Uniﬁed
In Proceedings of the IEEE

generative adversarial networks for multi-domain image-to-image translation.
Conference on Computer Vision and Pattern Recognition, 2018.

[6] Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-Hsuan Yang. MuseGAN: Multi-track sequential generative
adversarial networks for symbolic music generation and accompaniment. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence, 2018.

[7] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. SeqGAN: Sequence generative adversarial nets with policy

gradient. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2017.

10

A PREPRINT - MARCH 6, 2020

[8] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. MoCoGAN: Decomposing motion and content
for video generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.

[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
[10] David Ha and Douglas Eck. A neural representation of sketch drawings. In Proceedings of the International

Conference on Learning Representations, 2018.

[11] Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, SM Eslami, and Oriol Vinyals. Synthesizing programs for
images using reinforced adversarial learning. In Proceedings of the International Conference on Machine Learning,
2018.

[12] Ningyuan Zheng, Yifan Jiang, and Dingjiang Huang. Strokenet: A neural painting environment. In Proceedings

of the International Conference on Learning Representations, 2019.

[13] Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. DRAW: A recurrent
neural network for image generation. In Proceedings of the International Conference on Machine Learning, 2015.

[14] Bo Chang, Qiong Zhang, Shenyi Pan, and Lili Meng. Generating handwritten Chinese characters using CycleGAN.

In Proceedings of the IEEE Winter Conference on Applications of Computer Vision, 2018.

[15] Eloi Alonso, Bastien Moysset, and Ronaldo Messina. Adversarial generation of handwritten text images condi-
tioned on sequences. In Proceedings of the International Conference on Document Analysis and Recognition,
2019.

[16] Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classiﬁer

GANs. In Proceedings of the International Conference on Machine Learning, 2017.

[17] Lei Kang, J Ignacio Toledo, Pau Riba, Mauricio Villegas, Alicia Fornés, and Marçal Rusiñol. Convolve, attend
and spell: An attention-based sequence-to-sequence model for handwritten word recognition. In Proceedings of
the German Conference on Pattern Recognition, 2018.

[18] Jue Wang, Chenyu Wu, Ying-Qing Xu, and Heung-Yeung Shum. Combining shape and physical models for online
cursive handwriting synthesis. International Journal of Document Analysis and Recognition, 7(4):219–227, 2005.

[19] Thomas Konidaris, Basilios Gatos, Kostas Ntzios, Ioannis Pratikakis, Sergios Theodoridis, and Stavros J Peran-
tonis. Keyword-guided word spotting in historical printed documents using synthetic data and user feedback.
International Journal of Document Analysis and Recognition, 9(2-4):167–177, 2007.

[20] Zhouchen Lin and Liang Wan. Style-preserving English handwriting synthesis. Pattern Recognition, 40(7):2097–

[21] Achint Oommen Thomas, Amalia Rusu, and Venu Govindaraju. Synthetic handwritten CAPTCHAs. Pattern

[22] Tom SF Haines, Oisin Mac Aodha, and Gabriel J Brostow. My text in your handwriting. ACM Transactions on

[23] Praveen Krishnan and CV Jawahar. Generating synthetic data for text recognition.

arXiv preprint

2109, 2007.

Recognition, 42(12):3365–3373, 2009.

Graphics, 35(3):1–18, 2016.

arXiv:1608.04224, 2016.

[24] Lei Kang, Marçal Rusiñol, Alicia Fornés, Pau Riba, and Mauricio Villegas. Unsupervised writer adaptation for
synthetic-to-real handwritten word recognition. In Proceedings of the IEEE Winter Conference on Applications of
Computer Vision, 2020.

[25] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In Proceedings of the International

Conference on Learning Representations, 2014.

[26] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional
adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.
[27] Yaniv Taigman, Adam Polyak, and Lior Wolf. Unsupervised cross-domain image generation. In Proceedings of

the International Conference on Learning Representations, 2017.

[28] Vinaychandran Pondenkandath, Michele Alberti, Michaël Diatta, Rolf Ingold, and Marcus Liwicki. Historical
document synthesis with generative adversarial networks. In Proceedings of the International Conference on
Document Analysis and Recognition, 2019.

[29] Pengyuan Lyu, Xiang Bai, Cong Yao, Zhen Zhu, Tengteng Huang, and Wenyu Liu. Auto-encoder guided GAN
for Chinese calligraphy synthesis. In Proceedings of the International Conference on Document Analysis and
Recognition, 2017.

[30] Yuchen Tian. zi2zi: Master chinese calligraphy with conditional adversarial networks, 2017.

11

A PREPRINT - MARCH 6, 2020

[31] Haochuan Jiang, Guanyu Yang, Kaizhu Huang, and Rui Zhang. W-net: one-shot arbitrary-style Chinese character
generation with deep neural networks. In Proceedings of the International Conference on Neural Information
Processing, 2018.

[32] Ming-Yu Liu, Xun Huang, Arun Mallya, Tero Karras, Timo Aila, Jaakko Lehtinen, and Jan Kautz. Few-shot
unsupervised image-to-image translation. In Proceedings of the IEEE International Conference on Computer
Vision, 2019.

[33] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In

Proceedings of the IEEE International Conference on Computer Vision, 2017.

[34] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In

Proceedings of the International Conference on Learning Representations, 2015.

[35] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal

covariate shift. In Proceedings of the International Conference on Machine Learning, 2015.

[36] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-image translation.

In Proceedings of the European Conference on Computer Vision, 2018.

[37] Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent
neural networks on sequence modeling. In Proceedings of the NeurIPS Workshop on Deep Learning, 2014.
[38] U-V Marti and Horst Bunke. The IAM-database: an English sentence database for ofﬂine handwriting recognition.

International Journal on Document Analysis and Recognition, 5(1):39–46, 2002.

[39] Steven Bird, Ewan Klein, and Edward Loper. Natural language processing with Python: analyzing text with the

natural language toolkit. O’Reilly Media, Inc., 2009.

[40] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained
by a two time-scale update rule converge to a local Nash equilibrium. In Proceedings of the Neural Information
Processing Systems Conference, 2017.

[41] Ali Borji. Pros and cons of GAN evaluation measures. Computer Vision and Image Understanding, 179:41–65,

[42] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning

2019.

Research, 9:2579–2605, 2008.

12

