7
1
0
2
 
r
p
A
 
0
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
8
0
4
0
.
1
0
7
1
:
v
i
X
r
a

Embedding Watermarks into Deep Neural Networks

Yusuke Uchida
KDDI Research, Inc.
Saitama, Japan

Yuki Nagai
KDDI Research, Inc.
Saitama, Japan

Shigeyuki Sakazawa
Saitama Research, Inc.
Tokyo, Japan

Shin’ichi Satoh
National Institute of Informatics
Tokyo, Japan

Abstract

Signiﬁcant progress has been made with deep neural net-
works recently. Sharing trained models of deep neural net-
works has been a very important in the rapid progress of
research and development of these systems. At the same
time, it is necessary to protect the rights to shared trained
models. To this end, we propose to use digital watermarking
technology to protect intellectual property and detect intel-
lectual property infringement in the use of trained models.
First, we formulate a new problem: embedding watermarks
into deep neural networks. We also deﬁne requirements,
embedding situations, and attack types on watermarking in
deep neural networks. Second, we propose a general frame-
work for embedding a watermark in model parameters, us-
ing a parameter regularizer. Our approach does not im-
pair the performance of networks into which a watermark
is placed because the watermark is embedded while train-
ing the host network. Finally, we perform comprehensive
experiments to reveal the potential of watermarking deep
neural networks as the basis of this new research effort. We
show that our framework can embed a watermark during
the training of a deep neural network from scratch, and dur-
ing ﬁne-tuning and distilling, without impairing its perfor-
mance. The embedded watermark does not disappear even
after ﬁne-tuning or parameter pruning; the watermark re-
mains complete even after 65% of parameters are pruned.

1. Introduction

Deep neural networks have recently been signiﬁcantly
improved. In particular, deep convolutional neural networks
(DCNN) such as LeNet [25], AlexNet [23], VGGNet [28],
GoogLeNet [29], and ResNet [16] have become de facto
standards for object recognition, image classiﬁcation, and
retrieval. In addition, many deep learning frameworks have

1

been released that help engineers and researchers to develop
systems based on deep learning or do research with less
effort. Examples of these great deep learning frameworks
are Caffe [19], Theano [3], Torch [7], Chainer [30], Tensor-
Flow [26], and Keras [5].

Although these frameworks have made it easy to utilize
deep neural networks in real applications, the training of
deep neural network models is still a difﬁcult task because it
requires a large amount of data and time; several weeks are
needed to train a very deep ResNet with the latest GPUs on
the ImageNet dataset for instance [16]. Therefore, trained
models are sometimes provided on web sites to make it
easy to try a certain model or reproduce the results in re-
search articles without training. For example, Model Zoo1
provides trained Caffe models for various tasks with useful
utility tools. Fine-tuning or transfer learning [28] is a strat-
egy to directly adapt such already trained models to another
application with minimum re-training time. Thus, sharing
trained models is very important in the rapid progress of
both research and development of deep neural network sys-
tems.
In the future, more systematic model-sharing plat-
forms may appear, by analogy with video sharing sites. Fur-
thermore, some digital distribution platforms for purchase
and sale of the trained models or even artiﬁcial intelligence
skills (e.g. Alexa Skills2) may appear, similar to Google
Play or App Store.
In these situations, it is necessary to
protect the rights to shared trained models.

To this end, we propose to utilize digital watermarking
technology, which is used to identify ownership of the copy-
right of digital content such as images, audio, and videos. In
particular, we propose a general framework to embed a wa-
termark in deep neural networks models to protect intellec-
tual property and detect intellectual property infringement
of trained models. To the best of our knowledge, this is ﬁrst
attempt to embed a watermark in a deep neural network.

1https://github.com/BVLC/caffe/wiki/Model-Zoo
2https://www.alexaskillstore.com/

The contributions of this research are three-fold, as follows:

1. We formulate a new problem: embedding watermarks
in deep neural networks. We also deﬁne requirements,
embedding situations, and attack types for watermark-
ing deep neural networks.

2. We propose a general framework to embed a water-
mark in model parameters, using a parameter regular-
izer. Our approach does not impair the performance of
networks in which a watermark is embedded.

3. We perform comprehensive experiments to reveal the

potential of watermarking deep neural networks.

2. Problem Formulation

Given a model network with or without trained parame-
ters, we deﬁne the task of watermark embedding as to em-
bed T -bit vector b ∈ {0, 1}T into the parameters of one or
more layers of the neural network. We refer to a neural net-
work in which a watermark is embedded as a host network,
and refer to the task that the host network is originally trying
to perform as the original task.

In the following, we formulate (1) requirements for an
embedded watermark or an embedding method, (2) embed-
ding situations, and (3) expected attack types against which
embedded watermarks should be robust.

2.1. Requirements

Table 1 summarizes the requirements for an effective wa-
termarking algorithm in an image domain [15, 8] and a neu-
ral network domain. While both domains share almost the
same requirements, ﬁdelity and robustness are different in
image and neural network domains.

For ﬁdelity in an image domain, it is essential to main-
tain the perceptual quality of the host image while embed-
ding a watermark. However, in a neural network domain,
the parameters themselves are not important. Instead, the
performance of the original task is important. Therefore it
is essential to maintain the performance of the trained host
network, and not to hamper the training of a host network.
Regarding robustness, as images are subject to vari-
ous signal processing operations, an embedded watermark
should stay in the host image even after these operations.
And the greatest possible modiﬁcation to a neural network
is ﬁne-tuning or transfer learning [28]. An embedded wa-
termark in a neural network should be detectable after ﬁne-
tuning or other possible modiﬁcations.

2.2. Embedding Situations

We classify the embedding situations into three types:
train-to-embed, ﬁne-tune-to-embed, and distill-to-embed,
as summarized in Table 2.

Train-to-embed is the case in which the host network is
trained from scratch while embedding a watermark where
labels for training data are available.

Fine-tune-to-embed is the case in which a watermark is
embedded while ﬁne-tuning. In this case, model parame-
ters are initialized with a pre-trained network. The network
conﬁguration near the output layer may be changed before
ﬁne-tuning.

Distill-to-embed is the case in which a watermark is
embedded into a trained network without labels using the
distilling approach [17]. Embedding is performed in ﬁne-
tuning where the predictions of the trained model are used
as labels. In the standard distill framework, a large network
(or multiple networks) is ﬁrst trained and then a smaller net-
work is trained using the predicted labels of the large net-
work in order to compress the large network. In this paper,
we use the distill framework simply to train a network with-
out labels.

The ﬁrst two situations assume that the copyright holder
of the host network is expected to embed a watermark to
the host network in training or ﬁne-tuning. Fine-tune-to-
embed is also useful when a model owner wants to embed
individual watermarks to identify those to whom the model
had been distributed. By doing so, individual instances can
be tracked. The last situation assumes that a non-copyright
holder (e.g., a platformer) is entrusted to embed a water-
mark on behalf of a copyright holder.

2.3. Expected Attack Types

Related to the requirement for robustness in Section 2.1,
we assume two types of attacks against which embedded
watermarks should be robust: ﬁne-tuning and model com-
pression. These types of attack are very speciﬁc to deep
neural networks, while one can easily imagine model com-
pression by analogy with lossy image compression in the
image domain.

Fine-tuning or transfer learning [28] seems to be the
most feasible type of attack, because it reduces the burden
of training deep neural networks. Many models have been
constructed on top of existing state-of-the-art models. Fine-
tuning alters the model parameters, and thus embedded wa-
termarks should be robust against this alteration.

Model compression is very important in deploying deep
neural networks to embedded systems or mobile devices
as it can signiﬁcantly reduce memory requirements and/or
computational cost. Lossy compression distorts model pa-
rameters, so we should explore how it affects the detection
rate.

3. Proposed Framework

In this section, we propose a framework for embedding
a watermark into a host network. Although we focus on a

2

Table 1. Requirements for an effective watermarking algorithm in the image and neural network domains.

Fidelity

Robustness

Capacity

Security

Efﬁciency

Neural networks domain
The effectiveness of the host network should
not be degraded by embedding a watermark.
The embedded watermark should be robust
against model modiﬁcations such as ﬁne-
tuning and model compression.

Image domain
The quality of the host image should not be
degraded by embedding a watermark.
The embedded watermark should be robust
against common signal processing operations
such as lossy compression, cropping, resiz-
ing, and so on.
The effective watermarking system must have the ability to embed a large amount of in-
formation.
A watermark should in general be secret and should not be accessed, read, or modiﬁed by
unauthorized parties.
The watermark embedding and extraction processes should be fast.

Table 2. Three embedding situations. Fine-tune indicates whether
parameters are initialized in embedding using already trained mod-
els, or not. Label availability indicates whether or labels for train-
ing data are available in embedding.

Fine-tune Label availability

Train-to-embed
Fine-tune-to-embed
Distill-to-embed

(cid:88)
(cid:88)

(cid:88)
(cid:88)

DCNN [25] as the host, our framework is essentially ap-
plicable to other networks such as standard multilayer per-
ceptron (MLP), recurrent neural network (RNN), and long
short-term memory (LSTM) [18].

3.1. Embedding Targets

In this paper, a watermark is assumed to be embedded
into one of the convolutional layers in a host DCNN3. Let
(S, S), D, and L respectively denote the size of the con-
volution ﬁlter, the depth of input to the convolutional layer,
and the number of ﬁlters in the convolutional layer. The pa-
rameters of this convolutional layer are characterized by the
tensor W ∈ RS×S×D×L. The bias term is ignored here.
Let us think of embedding a T -bit vector b ∈ {0, 1}T into
W . The tensor W is a set of L convolutional ﬁlters and the
order of the ﬁlters does not affect the output of the network
if the parameters of the subsequent layers are appropriately
re-ordered. In order to remove this arbitrariness in the or-
der of ﬁlters, we calculate the mean of W over L ﬁlters as
l Wijkl. Letting w ∈ RM (M = S × S × D)
W ijk = 1
L
denote a ﬂattened version of W , our objective is now to em-
bed T -bit vector b into w.

(cid:80)

It is possible to embed a watermark in a host network
by directly modifying w of a trained network, as is usu-

3Fully-connected layers can also be used but we focus on convolu-
tional layers here, because fully-connected layers are often discarded in
ﬁne-tuning.

ally done in the image domain. However, this approach de-
grades the performance of the host network in the original
task as shown later in Section 4.2.6. Instead, we propose
embedding a watermark while training a host network for
the original task so that the existence of the watermark does
not impair the performance of the host network in its orig-
inal task. To this end, we utilize a parameter regularizer,
which is an additional term in the original cost function for
the original task. The cost function E(w) with a regularizer
is deﬁned as:

E(w) = E0(w) + λER(w),

(1)

where E0(w) is the original cost function, ER(w) is a regu-
larization term that imposes a certain restriction on param-
eters w, and λ is an adjustable parameter. A regularizer is
usually used to prevent the parameters from growing too
large. L2 regularization (or weight decay [24]), L1 regu-
larization, and their combination are often used to reduce
over-ﬁtting of parameters for complex neural networks. For
instance, ER(w) = ||w||2

2 in the L2 regularization.

In contrast to these standard regularizers, our regularizer
imposes parameter w to have a certain statistical bias, as a
watermark in a training process. We refer to this regular-
izer as an embedding regularizer. Before deﬁning the em-
bedding regularizer, we explain how to extract a watermark
from w. Given a (mean) parameter vector w ∈ RM and an
embedding parameter X ∈ RT ×M , the watermark extrac-
tion is simply done by projecting w using X, followed by
thresholding at 0. More precisely, the j-th bit is extracted
as:

bj = s(ΣiXjiwi),

s(x) =

(cid:40)

1 x ≥ 0
else.
0

(2)

(3)

This process can be considered to be a binary classiﬁca-

3.2. Embedding Regularizer

where s(x) is a step function:

3

tion problem with a single-layer perceptron (without bias)4.
Therefore, it is straightforward to deﬁne the loss function
ER(w) for the embedding regularizer by using binary cross
entropy:

with random weights. These three types of embedding pa-
rameters are compared in experiments.

Our implementation of the embedding regularizer is pub-
licly available from https://github.com/yu4u/
dnn-watermark.

ER(w) = −

(bj log(yj) + (1 − bj) log(1 − yj)) , (4)

4. Experiments

T
(cid:88)

j=1

In this section, we demonstrate that our embedding reg-
ularizer can embed a watermark without impairing the per-
formance of the host network, and the embedded watermark
is robust against various types of attack.

(5)

where yj = σ(ΣiXjiwi) and σ(·) is the sigmoid function:

σ(x) =

1
1 + exp(−x)

.

We call this loss function an embedding loss function. It
may be confusing that an embedding loss function is used
to update w, not X, in our framework. In a standard percep-
tron, w is an input and X is a parameter to be learned. In our
case, w is an embedding target and X is a ﬁxed parameter.
The design of X is discussed in the following section.

This approach does not impair the performance of the
host network in the original task as conﬁrmed in exper-
iments, because deep neural networks are typically over-
parameterized. It is well-known that deep neural networks
have many local minima, and that all local minima are
likely to have an error very close to that of the global min-
imum [9, 6]. Therefore, the embedding regularizer only
needs to guide model parameters to one of a number of good
local minima so that the ﬁnal model parameters have an ar-
bitrary watermark.

3.3. Regularizer Parameters

In this section we discuss the design of the embed-
ding parameter X, which can be considered as a secret
key [15] in detecting and embedding watermarks. While
X ∈ RT ×M can be an arbitrary matrix, it will affect the
performance of an embedded watermark because it is used
in both embedding and extraction of watermarks.
In this
paper, we consider three types of X: X direct, X diff, and
X random.

= 1.

X direct is constructed so that one element in each row of
X direct is ’1’ and the others are ’0’. In this case, the j-th
bit bj is directly embedded in a certain parameter wˆi s.t.
X direct
jˆi
X diff is created so that each row has one ’1’ element and
one ’-1’ element, and the others are ’0’. Using X diff, the
j-th bit bj is embedded into the difference between wi+ and
wi− where X diff
ji+

= 1 and X diff
ji−

= −1.

Each element of X random is independently drawn from
the standard normal distribution N (0, 1). Using X random,
each bit is embedded into all instances of the parameter w

4Although this single-layer perceptron can be deepened into multi-

layer perceptron, we focus on the simplest one in this paper.

4.1. Evaluation Settings

Datasets. For experiments, we used the well-known
CIFAR-10 and Caltech-101 datasets.
The CIFAR-10
dataset [22] consists of 60,000 32 × 32 color images in 10
classes, with 6,000 images per class. These images were
separated into 50,000 training images and 10,000 test im-
ages. The Caltech-101 dataset [10] includes pictures of ob-
jects belonging to 101 categories; it contains about 40 to
800 images per category. The size of each image is roughly
300 × 200 pixels but we resized them in 32 × 32 for ﬁne-
tuning. For each category, we used 30 images for training
and at most 40 of the remaining images for testing.

Host network and training settings. We used the
wide residual network [33] as the host network. The wide
residual network is an efﬁcient variant of the residual net-
work [16]. Table 3 shows the structure of the wide residual
network with a depth parameter N and a width parameter
k. In all our experiments, we set N = 1 and k = 4, and
used SGD with Nesterov momentum and cross-entropy loss
in training. The initial learning rate was set at 0.1, weight
decay to 5.0×10−4, momentum to 0.9 and minibatch size
to 64. The learning rate was dropped by a factor of 0.2 at
60, 120 and 160 epochs, and we trained for a total of 200
epochs, following the settings used in [33].

We embedded a watermark into one of the following
the second convolution layer in the
convolution layers:
conv 2, conv 3, and conv 4 groups. In the following, we
mention the location of the host layer by simply describing
the conv 2, conv 3, or conv 4 group. In Table 3, the num-
ber M of parameter w is also shown for these layers. The
parameter λ in Eq. (1) is set to 0.01. As a watermark, we
embedded b = 1 ∈ {0, 1}T in the following experiments.

4.2. Embedding Results

First, we conﬁrm that a watermark was successfully em-
bedded in the host network by the proposed embedding reg-
ularizer. We trained the host network from scratch (train-to-
embed) on the CIFAR-10 dataset with and without embed-
ding a watermark. In the embedding case, a 256-bit water-
mark (T = 256) was embedded into the conv 2 group.

4

Table 3. Structure of the host network.

Group Output size
conv 1

32 × 32

conv 2

32 × 32

#w
N/A

× N 144 × k

Building block
[3 × 3, 16]
(cid:21)
(cid:20)3 × 3, 16 × k
3 × 3, 16 × k
(cid:20)3 × 3, 32 × k
3 × 3, 32 × k
(cid:20)3 × 3, 64 × k
3 × 3, 64 × k

(cid:21)

(cid:21)

conv 3

16 × 16

× N 288 × k

conv 4

8 × 8

1 × 1

× N 576 × k

avg-pool, fc, soft-max

N/A

Table 4 shows the best test errors and embedding losses
ER(w) of the host networks with and without embedding.
We can see that the test errors of Not embedded and ran-
dom are almost the same while those of direct and diff
are slightly larger. The embedding loss ER(w) of ran-
dom is extremely low compared with those of direct and
diff. These results indicate that the random approach can
effectively embed a watermark without impairing the per-
formance in the original task.

4.2.2 Detecting Watermarks

Figure 2 shows the histogram of the embedded watermark
σ(ΣiXjiwi) (before thresholding) with and without wa-
termarks where (a) direct, (b) diff, and (c) random pa-
rameters are used in embedding and detection.
If we bi-
narize σ(ΣiXjiwi) at a threshold of 0.5, all watermarks
are correctly detected because ∀j, σ(ΣiXjiwi) ≥ 0.5
(⇔ ΣiXjiwi ≥ 0) for all embedded cases. Please note
that we embedded b = 1 ∈ {0, 1}T as a watermark as men-
tioned before. Although random watermarks will be de-
tected for the non-embedded cases, it can be easily judged
that the watermark is not embedded because the distribution
of σ(ΣiXjiwi) is quite different from those for embedded
cases.

4.2.3 Distribution of Model Parameters

We explore how trained model parameters are affected by
the embedded watermarks. Figure 3 shows the distribution
of model parameters W (not w) with and without water-
marks. These parameters are taken only from the layer in
which a watermark was embedded. Note that W is the pa-
rameter before taking the mean over ﬁlters, and thus the
number of parameters is 3 × 3 × 64 × 64. We can see that
direct and diff signiﬁcantly alter the distribution of param-
eters while random does not. In direct, many parameters
became large and a peak appears near 2 so that their mean
over ﬁlters becomes a large positive value to reduce the em-
bedding loss. In diff, most parameters were pushed in both
positive and negative directions so that the differences be-
tween these parameters became large. In random, a water-
mark is diffused over all parameters with random weights
and thus does not signiﬁcantly alter the distribution. This is
one of the desirable properties of watermarking related the
security requirement; one may be aware of the existence of
the embedded watermarks for the direct and diff cases.

The results so far indicated that the random approach
seemed to be the best choice among the three, with low em-
bedding loss, low test error in the original task, and not al-
tering the parameter distribution. Therefore, in the follow-
ing experiments, we used the random approach in embed-
ding watermarks without explicitly indicating it.

Figure 1. Training curves for the host network on CIFAR-10 as a
function of epochs. Solid lines denote test error (y-axis on the left)
and dashed lines denote training loss E(w) (y-axis on the right).

Table 4. Test error (%) and embedding loss ER(w) with and with-
out embedding.

Not embedded
direct
diff
random

Test error
8.04
8.21
8.37
7.97

ER(w)
N/A
1.24×10−1
6.20×10−2
4.76×10−4

4.2.1 Test Error and Training Loss

Figure 1 shows the training curves for the host network in
CIFAR-10 as a function of epochs. Not embedded is the
case that the host network is trained without the embed-
ding regularizer. Embedded (direct), Embedded (diff),
and Embedded (random) respectively represent training
curves with embedding regularizers whose parameters are
X direct, X diff, and X random. We can see that the training
loss E(w) with a watermark becomes larger than the not-
embedded case if the parameters X direct and X diff are used.
This large training loss is dominated by the embedding loss
ER(w), which indicates that it is difﬁcult to embed a water-
mark directly into a parameter or even into the difference of
two parameters. On the other hand, the training loss of Em-
bedded (random) is very close to that of Not embedded.

5

(a) direct

(b) diff
Figure 2. Histogram of the embedded watermark σ(ΣiXjiwi) (before thresholding) with and without watermarks.

(c) random

(a) Not embedded

(b) direct

(c) diff

(d) random

Figure 3. Distribution of model parameters W with and without watermarks.

on the same CIFAR-10 dataset with embedding and with-
out embedding (for comparison). In the second experiment,
the host network is trained on the Caltech-101 dataset, and
then ﬁne-tuned on the CIFAR-10 dataset with and without
embedding.

Table 5 (a) shows the result of the ﬁrst experiment. Not
embedded 1st corresponds to the ﬁrst training without em-
bedding. Not embedded 2nd corresponds to the second
training without embedding and Embedded corresponds
to the second training with embedding. Figure 4 shows the
training curves of these ﬁne-tunings5. We can see that Em-
bedded achieved almost the same test error as Not embed-
ded 2nd and a very low ER(w).

Table 5 (b) shows the results of the second experiment.
Not embedded 2nd corresponds to the second training
without embedding and Embedded corresponds to the sec-
ond training with embedding. The test error and training
loss of the ﬁrst training are not shown because they are not
compatible between these two different training datasets.
From these results, it was also conﬁrmed that Embedded
achieved almost the same test error as Not embedded 2nd
and very low ER(w). Thus, we can say that the proposed
method is effective even in the ﬁne-tune-to-embed situation
(in the same and different domains).

Finally, embedding a watermark in the distill-to-embed

5Note that the learning rate was also initialized to 0.1 at the beginning
of the second training, while the learning rate was reduced to 8.0 × 10−4)
at the end of the ﬁrst training.

Figure 4. Training curves for ﬁne-tuning the host network. The
ﬁrst and second halves of epochs correspond to the ﬁrst and sec-
ond training. Solid lines denote test error (y-axis on the left) and
dashed lines denote training loss (y-axis on the right).

4.2.4 Fine-tune-to-embed and Distill-to-embed

In the above experiments, a watermark was embedded by
training the host network from scratch (train-to-embed).
Here, we evaluated the other two situations introduced in
Section 2.2: ﬁne-tune-to-embed and distill-to-embed. For
In
ﬁne-tune-to-embed, two experiments were performed.
the ﬁrst experiment, the host network was trained on the
CIFAR-10 dataset without embedding, and then ﬁne-tuned

6

Table 5. Test error (%) and embedding loss ER(w) with and with-
out embedding in ﬁne-tuning and distilling.

Table 6. Test error (%) and embedding loss ER(w) for the combi-
nations of embedded groups and sizes of embedded bits.

(a) Fine-tune-to-embed (CIFAR-10 → CIFAR-10)

Not embedded 1st
Not embedded 2nd
Embedded

Test error
8.04
7.66
7.70

ER(w)
N/A
N/A
4.93×10−4

(b) Fine-tune-to-embed (Caltech-101 → CIFAR-10)

ER(w)
N/A
4.83×10−4
(c) Distill-to-embed (CIFAR-10 → CIFAR-10)

Not embedded 2nd
Embedded

Test error
7.93
7.94

Not embedded 1st
Not embedded 2nd
Embedded

Test error
8.04
7.86
7.75

ER(w)
N/A
N/A
5.01×10−4

situation was evaluated. The host network is ﬁrst trained
on the CIFAR-10 dataset without embedding. Then, the
trained network was further ﬁne-tuned on the same CIFAR-
In this second
10 dataset with and without embedding.
training, the training labels of the CIFAR-10 dataset were
not used. Instead, the predicted values of the trained net-
work were used as soft targets [17]. In other words, no label
was used in the second training. Table 5 (c) shows the re-
sults for the distill-to-embed situation. Not embedded 1st
corresponds to the ﬁrst training and Embedded (Not em-
bedded 2nd) corresponds to the second distilling training
with embedding (without embedding). It was found that the
proposed method also achieved low test error and ER(w) in
the distill-to-embed situation.

4.2.5 Capacity of Watermark.

In this section, the capacity of the embedded watermark is
explored by embedding different sizes of watermarks into
different groups in the train-to-embed manner. Please note
that the number of parameters w of conv 2, conv 3, and
conv 4 groups were 576, 1152, and 2304, respectively. Ta-
ble 6 shows test error (%) and embedding loss ER(w) for
combinations of different embedded blocks and different
number of embedded bits. We can see that embedded loss or
test error becomes high if the number of embedded bits be-
comes larger than the number of parameters w (e.g. 2,048
bits in conv 3) because the embedding problem becomes
overdetermined in such cases. Thus, the number of embed-
ded bits should be smaller than the number of parameters
w, which is a limitation of the embedding method using a
single-layer perceptron. This limitation would be resolved
by using a multi-layer perceptron in the embedding regular-
izer.

Embedded bits

(a) Test error (%)

Embedded group
conv 3
7.98
8.22
8.12
8.93

conv 2
7.97
8.47
8.43
8.17
(b) Embedding loss

conv 4
7.92
7.84
7.84
7.75

256
512
1,024
2,048

Embedded bits

256
512
1,024
2,048

conv 2
4.76×10−4
8.11×10−4
6.74×10−2
5.35×10−1

Embedded group
conv 3
7.20×10−4
8.18×10−4
1.53×10−3
3.70×10−2

conv 4
1.10×10−2
1.25×10−2
1.53×10−2
3.06×10−2

Table 7. Losses, test error, and bit error rate (BER) after embed-
ding a watermark with different λ.

λ
0
1
10
100

1

2 ||w − w0||2
0.000
0.184
1.652
7.989

2 ER(w) Test error

1.066
0.609
0.171
0.029

8.04
8.52
10.57
13.00

BER
0.531
0.324
0.000
0.000

4.2.6 Embedding without Training

As mentioned in Section 3.2, it is possible to embed a wa-
termark to a host network by directly modifying the trained
parameter w0 as usually done in image domain. Here we try
to do this by minimizing the following loss function instead
of Eq. (1):

E(w) = 1

2 ||w − w0||2

2 + λER(w),

(6)

where the embedding loss ER(w) is minimized while min-
imizing the difference between the modiﬁed parameter w
and the original parameter w0. Table 7 summarizes the em-
bedding results after Eq. (6) against the host network trained
on the CIFAR-10 dataset. We can see that embedding fails
for λ ≤ 1 as bit error rate (BER) is larger than zero while the
test error of the original task becomes too large for λ > 1.
Thus, it is not effective to directly embed a watermark with-
out considering the original task.

4.3. Robustness of Embedded Watermarks

In this section, the robustness of a proposed watermark is
evaluated for the three attack types explained in Section 2.3:
ﬁne-tuning and model compression.

4.3.1 Robustness against Fine-tuning

Fine-tuning or transfer learning [28] seems to be the most
likely type of (unintentional) attack because it is frequently

7

Table 8. Embedding loss before ﬁne-tuning (ER(w)) and after
ﬁne-tuning (E(cid:48)
R(w)), and the best test error (%) after ﬁne-tuning.

CIFAR-10 → CIFAR-10
Caltech-101 → CIFAR-10

ER(w)
4.76×10−4
5.96×10−3

E(cid:48)

R(w)
8.66×10−4
1.56×10−2

Test error
7.69
7.88

performed on trained models to apply them to other but
similar tasks with less effort than training a network from
scratch or to avoid over-ﬁtting when sufﬁcient training data
is not available.

In this experiment, two trainings we performed; in the
ﬁrst training, a 256-bit watermark was embedded in the
conv 2 group in the train-to-embed manner, and then the
host network was further ﬁne-tuned in the second training
without embedding, to determine whether the watermark
embedded in the ﬁrst training stayed in the host network
or not, even after the second training (ﬁne-tuning).

Table 8 shows the embedding loss before ﬁne-tuning
(ER(w)) and after ﬁne-tuning (E(cid:48)
R(w)), and the best test er-
ror after ﬁne-tuning. We evaluated ﬁne-tuning in the same
domain (CIFAR-10 → CIFAR-10) and in different domains
(Caltech-101 → CIFAR-10). We can see that, in both cases,
the embedding loss was slightly increased by ﬁne-tuning
but was still low. In addition, the bit error rate of the de-
tected watermark was equal to zero in both cases. The rea-
son why the embedding loss in ﬁne-tuning in the different
domains is higher than that in the same domain is that the
Caltech-101 dataset is signiﬁcantly more difﬁcult than the
CIFAR-10 dataset in our settings; all images in the Caltech-
101 dataset were resized to 32 × 326 for compatibility with
the CIFAR-10 dataset.

4.3.2 Robustness against Model Compression

It is sometimes difﬁcult to deploy deep neural networks to
embedded systems or mobile devices because they are both
computationally intensive and memory intensive. In order
to solve this problem, the model parameters are often com-
pressed [14, 12, 13]. The compression of model parameters
can intentionally or unintentionally act as an attack against
watermarks. In this section, we evaluate the robustness of
our watermarks against model compression, in particular,
against parameter pruning [14]. In parameter pruning, pa-
rameters whose absolute values are very small are cut-off to
zero. In [13], quantization of weights and the Huffman cod-
ing of quantized values are further applied. Because quanti-
zation has less impact than parameter pruning and the Huff-
man coding is lossless compression, we focus on parameter
pruning.

In order to evaluate the robustness against parameter
pruning, we embedded a 256-bit watermark in the conv

6This size is extremely small compared with their original sizes

(roughly 300 × 200).

8

2 group while training the host network on the CIFAR-10
dataset. We removed α% of the 3 × 3 × 64 × 64 parameters
of the embedded layer and calculated embedding loss and
bit error rate. Figure 5 (a) shows embedding loss ER(w)
as a function of pruning rate α. Ascending (Descending)
represents embedding loss when the top α% parameters are
cut-off according to their absolute values in ascending (de-
scending) order. Random represents embedding loss where
α% of parameters are randomly removed. Ascending cor-
responds to parameter pruning and the others were evalu-
ated for comparison. We can see that the embedding loss of
Ascending increases more slowly than those of Descend-
ing and Random as α increases. It is reasonable that model
parameters with small absolute values have less impact on
a detected watermark because the watermark is extracted
from the dot product of the model parameter w and the con-
stant embedding parameter (weight) X.

Figure 5 (b) shows the bit error rate as a function of prun-
ing rate α. Surprisingly, the bit error rate was still zero after
removing 65% of the parameters and 2/256 even after 80%
of the parameters were pruned (Ascending). We can say
that the embedded watermark is sufﬁciently robust against
parameter pruning because, in [13], the resulting pruning
rate of convolutional layers ranged from to 16% to 65% for
the AlexNet [23], and from 42% to 78% for VGGNet [28].
Furthermore, this degree of bit error can be easily corrected
by an error correction code (e.g. the BCH code). Figure 6
shows the histogram of the detected watermark σ(ΣiXjiwi)
after pruning for α = 0.8 and 0.95. For α = 0.95, the his-
togram of the detected watermark is also shown for the host
network into which no watermark is embedded. We can see
that many of σ(ΣiXjiwi) are still close to one for the em-
bedded case, which might be used as a conﬁdence score in
judging the existence of a watermark (zero-bit watermark-
ing).

5. Conclusions and Future Work

In this paper, we have proposed a general framework
for embedding a watermark in deep neural network models
to protect the rights to the trained models. First, we for-
mulated a new problem: embedding watermarks into deep
neural networks. We also deﬁned requirements, embedding
situations, and attack types for watermarking deep neural
networks. Second, we proposed a general framework for
embedding a watermark in model parameters using a pa-
rameter regularizer. Our approach does not impair the per-
formance of networks into which a watermark is embed-
ded. Finally, we performed comprehensive experiments to
reveal the potential of watermarking deep neural networks
as the basis of this new problem. We showed that our frame-
work could embed a watermark without impairing the per-
formance of a deep neural network. The embedded water-
mark did not disappear even after ﬁne-tuning or parameter

(a) Embedding loss.

Figure 6. Histogram of the detected watermark σ(ΣiXjiwi) after
pruning.

against parameter pruning in this paper, a watermark might
be embedded in conjunction with compressing models. For
example, in [13], after parameter pruning, the network is
re-trained to learn the ﬁnal weights for the remaining sparse
parameters. Our embedding regularizer can be used in this
re-training to embed a watermark.

Network morphism. In [4, 32], a systematic study has
been done on how to morph a well-trained neural network
into a new one so that its network function can be com-
pletely preserved for further training. This network mor-
phism can constitute a severe attack against our watermark
because it may be impossible to detect the embedded water-
mark if the topology of the host network is severely modi-
ﬁed. We left the investigation how the embedded watermark
is affected by this network morphism for future work.

Steganalysis. Steganalysis [27, 21] is a method for de-
tecting the presence of secretly hidden data (e.g. steganog-
raphy or watermarks) in digital media ﬁles such as images,
video, audio, and, in our case, deep neural networks. Water-
marks ideally are robust against steganalysis. While, in this
paper, we conﬁrmed that embedding watermarks does not
signiﬁcantly change the distribution of model parameters,
more exploration is needed to evaluate robustness against
steganalysis. Conversely, developing effective steganalysis
against watermarks for deep neural networks can be an in-
teresting research topic.

Fingerprinting. Digital ﬁngerprinting is an alternative
to the watermarking approach for persistent identiﬁcation
of images [2], video [20, 31], and audio clips [1, 11]. In
this paper, we focused on one of these two important ap-
proaches. Robust ﬁngerprinting of deep neural networks is
another and complementary direction to protect deep neural
network models.

(b) Bit error rate.
Figure 5. Embedding loss and bit error rate after pruning as a func-
tion of pruning rate.

pruning; the entire watermark remained even after 65% of
the parameters were pruned.

5.1. Future Work

Although we have obtained ﬁrst insight into the new
problem of embedding a watermark in deep neural net-
works, many things remain as future work.

Watermark overwriting. A third-party user may em-
bed a different watermark in order to overwrite the origi-
nal watermark.
In our preliminary experiments, this wa-
termark overwriting caused 30.9%, 8.6%, and 0.4% bit er-
rors against watermarks in the conv 2, conv 3, and conv 4
groups when 256-bit watermarks were additionally embed-
ded. More robust watermarking against overwriting should
be explored (e.g. non-linear embedding).

Compression as embedding. Compressing deep neu-
ral networks is a very important and active research topic.
While we conﬁrmed that our watermark is very robust

9

[20] A. Joly, C. Frelicot, and O. Buisson. Content-based video
copy detection in large databases: a local ﬁngerprints sta-
tistical similarity search approach. In Proc. of ICIP, pages
505–508, 2005.

[21] J. Kodovsky, J. Fridrich, and V. Holub. Ensemble classiﬁers
for steganalysis of digital media. IEEE Trans. on Information
Forensics and Security, 7(2):432–444, 2012.

[22] A. Krizhevsky. Learning multiple layers of features from

tiny images. Tech Report, 2009.

[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
Proc. of NIPS, 2012.

Imagenet
In

[24] A. Krogh and J. A. Hertz. A simple weight decay can im-

prove generalization. In Proc. of NIPS, 1992.

[25] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998.

[26] M. Abadi, et al. Tensorﬂow: Large-scale machine learning
on heterogeneous distributed systems. arXiv:1603.04467,
2016.

[27] L. Shaohui, Y. Hongxun, and G. Wen. Neural network based
steganalysis in still images. In Proc. of ICME, 2003.
[28] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In Proc. of ICLR,
2015.

[29] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In Proc. of CVPR, 2015.

[30] S. Tokui, K. Oono, S. Hido, and J. Clayton. Chainer: a
next-generation open source framework for deep learning.
In Proc. of NIPS Workshop on Machine Learning Systems,
2015.

[31] Y. Uchida, M. Agrawal, and S. Sakazawa. Accurate content-
based video copy detection with efﬁcient feature indexing.
In Proc. of ICMR, 2011.

[32] T. Wei, C. Wang, Y. Rui, and C. W. Chen. Network mor-

phism. In Proc. of ICML, 2016.

[33] S. Zagoruyko and N. Komodakis. Wide residual networks.

In Proc. of ECCV, 2016.

References

[1] X. Anguera, A. Garzon, and T. Adamek. Mask: Robust local
features for audio ﬁngerprinting. In Proc. of ICME, 2012.
[2] J. Barr, B. Bradley, and B. T. Hannigan. Using digital wa-
termarks with image signatures to mitigate the threat of the
copy attack. In Proc. of ICASSP, pages 69–72, 2003.

[3] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu,
G. Desjardins, J. Turian, D. Warde-Farley, and Y. Bengio.
Theano: a CPU and GPU math expression compiler. In Proc.
of the Python for Scientiﬁc Computing Conference (SciPy),
2010.

[4] T. Chen, I. Goodfellow, and J. Shlens. Net2net: Accelerating
learning via knowledge transfer. In Proc. of ICLR, 2016.

[5] F. Chollet. Keras. GitHub repository, 2015.
[6] A. Choromanska, M. Henaff, M. Mathieu, G. Arous, and
Y. LeCun. The loss surfaces of multilayer networks. In Proc.
of AISTATS, 2015.

[7] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A
matlab-like environment for machine learning. In Proc. of
NIPS Workshop on BigLearn, 2011.

[8] I. Cox, M. Miller, J. Bloom, J. Fridrich, and T. Kalker. Dig-
ital Watermarking and Steganography. Morgan Kaufmann
Publishers Inc., 2 edition, 2008.

[9] Y. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli,
Identifying and attacking the saddle point
In

and Y. Bengio.
problem in high-dimensional non-convex optimization.
Proc. of NIPS, 2014.

[10] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative
visual models from few training examples: an incremen-
tal bayesian approach tested on 101 object categories.
In
Proc. of CVPR Workshop on Generative-Model Based Vi-
sion, 2004.

[11] J. Haitsma and T. Kalker. A highly robust audio ﬁngerprint-
ing system. In Proc. of ISMIR, pages 107–115, 2002.
[12] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz,
and W. J. Dally. Eie: Efﬁcient inference engine on com-
pressed deep neural network. In Proc. of ISCA, 2016.
[13] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-
pressing deep neural networks with pruning, trained quanti-
zation and huffman coding. In Proc. of ICLR, 2016.

[14] S. Han, J. Pool, J. Tran, and W. J. Dally. Learning both
weights and connections for efﬁcient neural networks.
In
Proc. of NIPS, 2015.

[15] F. Hartung and M. Kutter. Multimedia watermarking tech-

niques. Proceedings of the IEEE, 87(7):1079–1107, 1999.

[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In Proc. of CVPR, 2016.

[17] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
In Proc. of NIPS Workshop on Deep

in a neural network.
Learning and Representation Learning, 2014.

[18] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural Computation, 9(8):1735–1780, 1997.

[19] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
In Proc. of MM,
architecture for fast feature embedding.
2014.

10

7
1
0
2
 
r
p
A
 
0
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
8
0
4
0
.
1
0
7
1
:
v
i
X
r
a

Embedding Watermarks into Deep Neural Networks

Yusuke Uchida
KDDI Research, Inc.
Saitama, Japan

Yuki Nagai
KDDI Research, Inc.
Saitama, Japan

Shigeyuki Sakazawa
Saitama Research, Inc.
Tokyo, Japan

Shin’ichi Satoh
National Institute of Informatics
Tokyo, Japan

Abstract

Signiﬁcant progress has been made with deep neural net-
works recently. Sharing trained models of deep neural net-
works has been a very important in the rapid progress of
research and development of these systems. At the same
time, it is necessary to protect the rights to shared trained
models. To this end, we propose to use digital watermarking
technology to protect intellectual property and detect intel-
lectual property infringement in the use of trained models.
First, we formulate a new problem: embedding watermarks
into deep neural networks. We also deﬁne requirements,
embedding situations, and attack types on watermarking in
deep neural networks. Second, we propose a general frame-
work for embedding a watermark in model parameters, us-
ing a parameter regularizer. Our approach does not im-
pair the performance of networks into which a watermark
is placed because the watermark is embedded while train-
ing the host network. Finally, we perform comprehensive
experiments to reveal the potential of watermarking deep
neural networks as the basis of this new research effort. We
show that our framework can embed a watermark during
the training of a deep neural network from scratch, and dur-
ing ﬁne-tuning and distilling, without impairing its perfor-
mance. The embedded watermark does not disappear even
after ﬁne-tuning or parameter pruning; the watermark re-
mains complete even after 65% of parameters are pruned.

1. Introduction

Deep neural networks have recently been signiﬁcantly
improved. In particular, deep convolutional neural networks
(DCNN) such as LeNet [25], AlexNet [23], VGGNet [28],
GoogLeNet [29], and ResNet [16] have become de facto
standards for object recognition, image classiﬁcation, and
retrieval. In addition, many deep learning frameworks have

1

been released that help engineers and researchers to develop
systems based on deep learning or do research with less
effort. Examples of these great deep learning frameworks
are Caffe [19], Theano [3], Torch [7], Chainer [30], Tensor-
Flow [26], and Keras [5].

Although these frameworks have made it easy to utilize
deep neural networks in real applications, the training of
deep neural network models is still a difﬁcult task because it
requires a large amount of data and time; several weeks are
needed to train a very deep ResNet with the latest GPUs on
the ImageNet dataset for instance [16]. Therefore, trained
models are sometimes provided on web sites to make it
easy to try a certain model or reproduce the results in re-
search articles without training. For example, Model Zoo1
provides trained Caffe models for various tasks with useful
utility tools. Fine-tuning or transfer learning [28] is a strat-
egy to directly adapt such already trained models to another
application with minimum re-training time. Thus, sharing
trained models is very important in the rapid progress of
both research and development of deep neural network sys-
tems.
In the future, more systematic model-sharing plat-
forms may appear, by analogy with video sharing sites. Fur-
thermore, some digital distribution platforms for purchase
and sale of the trained models or even artiﬁcial intelligence
skills (e.g. Alexa Skills2) may appear, similar to Google
Play or App Store.
In these situations, it is necessary to
protect the rights to shared trained models.

To this end, we propose to utilize digital watermarking
technology, which is used to identify ownership of the copy-
right of digital content such as images, audio, and videos. In
particular, we propose a general framework to embed a wa-
termark in deep neural networks models to protect intellec-
tual property and detect intellectual property infringement
of trained models. To the best of our knowledge, this is ﬁrst
attempt to embed a watermark in a deep neural network.

1https://github.com/BVLC/caffe/wiki/Model-Zoo
2https://www.alexaskillstore.com/

The contributions of this research are three-fold, as follows:

1. We formulate a new problem: embedding watermarks
in deep neural networks. We also deﬁne requirements,
embedding situations, and attack types for watermark-
ing deep neural networks.

2. We propose a general framework to embed a water-
mark in model parameters, using a parameter regular-
izer. Our approach does not impair the performance of
networks in which a watermark is embedded.

3. We perform comprehensive experiments to reveal the

potential of watermarking deep neural networks.

2. Problem Formulation

Given a model network with or without trained parame-
ters, we deﬁne the task of watermark embedding as to em-
bed T -bit vector b ∈ {0, 1}T into the parameters of one or
more layers of the neural network. We refer to a neural net-
work in which a watermark is embedded as a host network,
and refer to the task that the host network is originally trying
to perform as the original task.

In the following, we formulate (1) requirements for an
embedded watermark or an embedding method, (2) embed-
ding situations, and (3) expected attack types against which
embedded watermarks should be robust.

2.1. Requirements

Table 1 summarizes the requirements for an effective wa-
termarking algorithm in an image domain [15, 8] and a neu-
ral network domain. While both domains share almost the
same requirements, ﬁdelity and robustness are different in
image and neural network domains.

For ﬁdelity in an image domain, it is essential to main-
tain the perceptual quality of the host image while embed-
ding a watermark. However, in a neural network domain,
the parameters themselves are not important. Instead, the
performance of the original task is important. Therefore it
is essential to maintain the performance of the trained host
network, and not to hamper the training of a host network.
Regarding robustness, as images are subject to vari-
ous signal processing operations, an embedded watermark
should stay in the host image even after these operations.
And the greatest possible modiﬁcation to a neural network
is ﬁne-tuning or transfer learning [28]. An embedded wa-
termark in a neural network should be detectable after ﬁne-
tuning or other possible modiﬁcations.

2.2. Embedding Situations

We classify the embedding situations into three types:
train-to-embed, ﬁne-tune-to-embed, and distill-to-embed,
as summarized in Table 2.

Train-to-embed is the case in which the host network is
trained from scratch while embedding a watermark where
labels for training data are available.

Fine-tune-to-embed is the case in which a watermark is
embedded while ﬁne-tuning. In this case, model parame-
ters are initialized with a pre-trained network. The network
conﬁguration near the output layer may be changed before
ﬁne-tuning.

Distill-to-embed is the case in which a watermark is
embedded into a trained network without labels using the
distilling approach [17]. Embedding is performed in ﬁne-
tuning where the predictions of the trained model are used
as labels. In the standard distill framework, a large network
(or multiple networks) is ﬁrst trained and then a smaller net-
work is trained using the predicted labels of the large net-
work in order to compress the large network. In this paper,
we use the distill framework simply to train a network with-
out labels.

The ﬁrst two situations assume that the copyright holder
of the host network is expected to embed a watermark to
the host network in training or ﬁne-tuning. Fine-tune-to-
embed is also useful when a model owner wants to embed
individual watermarks to identify those to whom the model
had been distributed. By doing so, individual instances can
be tracked. The last situation assumes that a non-copyright
holder (e.g., a platformer) is entrusted to embed a water-
mark on behalf of a copyright holder.

2.3. Expected Attack Types

Related to the requirement for robustness in Section 2.1,
we assume two types of attacks against which embedded
watermarks should be robust: ﬁne-tuning and model com-
pression. These types of attack are very speciﬁc to deep
neural networks, while one can easily imagine model com-
pression by analogy with lossy image compression in the
image domain.

Fine-tuning or transfer learning [28] seems to be the
most feasible type of attack, because it reduces the burden
of training deep neural networks. Many models have been
constructed on top of existing state-of-the-art models. Fine-
tuning alters the model parameters, and thus embedded wa-
termarks should be robust against this alteration.

Model compression is very important in deploying deep
neural networks to embedded systems or mobile devices
as it can signiﬁcantly reduce memory requirements and/or
computational cost. Lossy compression distorts model pa-
rameters, so we should explore how it affects the detection
rate.

3. Proposed Framework

In this section, we propose a framework for embedding
a watermark into a host network. Although we focus on a

2

Table 1. Requirements for an effective watermarking algorithm in the image and neural network domains.

Fidelity

Robustness

Capacity

Security

Efﬁciency

Neural networks domain
The effectiveness of the host network should
not be degraded by embedding a watermark.
The embedded watermark should be robust
against model modiﬁcations such as ﬁne-
tuning and model compression.

Image domain
The quality of the host image should not be
degraded by embedding a watermark.
The embedded watermark should be robust
against common signal processing operations
such as lossy compression, cropping, resiz-
ing, and so on.
The effective watermarking system must have the ability to embed a large amount of in-
formation.
A watermark should in general be secret and should not be accessed, read, or modiﬁed by
unauthorized parties.
The watermark embedding and extraction processes should be fast.

Table 2. Three embedding situations. Fine-tune indicates whether
parameters are initialized in embedding using already trained mod-
els, or not. Label availability indicates whether or labels for train-
ing data are available in embedding.

Fine-tune Label availability

Train-to-embed
Fine-tune-to-embed
Distill-to-embed

(cid:88)
(cid:88)

(cid:88)
(cid:88)

DCNN [25] as the host, our framework is essentially ap-
plicable to other networks such as standard multilayer per-
ceptron (MLP), recurrent neural network (RNN), and long
short-term memory (LSTM) [18].

3.1. Embedding Targets

In this paper, a watermark is assumed to be embedded
into one of the convolutional layers in a host DCNN3. Let
(S, S), D, and L respectively denote the size of the con-
volution ﬁlter, the depth of input to the convolutional layer,
and the number of ﬁlters in the convolutional layer. The pa-
rameters of this convolutional layer are characterized by the
tensor W ∈ RS×S×D×L. The bias term is ignored here.
Let us think of embedding a T -bit vector b ∈ {0, 1}T into
W . The tensor W is a set of L convolutional ﬁlters and the
order of the ﬁlters does not affect the output of the network
if the parameters of the subsequent layers are appropriately
re-ordered. In order to remove this arbitrariness in the or-
der of ﬁlters, we calculate the mean of W over L ﬁlters as
l Wijkl. Letting w ∈ RM (M = S × S × D)
W ijk = 1
L
denote a ﬂattened version of W , our objective is now to em-
bed T -bit vector b into w.

(cid:80)

It is possible to embed a watermark in a host network
by directly modifying w of a trained network, as is usu-

3Fully-connected layers can also be used but we focus on convolu-
tional layers here, because fully-connected layers are often discarded in
ﬁne-tuning.

ally done in the image domain. However, this approach de-
grades the performance of the host network in the original
task as shown later in Section 4.2.6. Instead, we propose
embedding a watermark while training a host network for
the original task so that the existence of the watermark does
not impair the performance of the host network in its orig-
inal task. To this end, we utilize a parameter regularizer,
which is an additional term in the original cost function for
the original task. The cost function E(w) with a regularizer
is deﬁned as:

E(w) = E0(w) + λER(w),

(1)

where E0(w) is the original cost function, ER(w) is a regu-
larization term that imposes a certain restriction on param-
eters w, and λ is an adjustable parameter. A regularizer is
usually used to prevent the parameters from growing too
large. L2 regularization (or weight decay [24]), L1 regu-
larization, and their combination are often used to reduce
over-ﬁtting of parameters for complex neural networks. For
instance, ER(w) = ||w||2

2 in the L2 regularization.

In contrast to these standard regularizers, our regularizer
imposes parameter w to have a certain statistical bias, as a
watermark in a training process. We refer to this regular-
izer as an embedding regularizer. Before deﬁning the em-
bedding regularizer, we explain how to extract a watermark
from w. Given a (mean) parameter vector w ∈ RM and an
embedding parameter X ∈ RT ×M , the watermark extrac-
tion is simply done by projecting w using X, followed by
thresholding at 0. More precisely, the j-th bit is extracted
as:

bj = s(ΣiXjiwi),

s(x) =

(cid:40)

1 x ≥ 0
else.
0

(2)

(3)

This process can be considered to be a binary classiﬁca-

3.2. Embedding Regularizer

where s(x) is a step function:

3

tion problem with a single-layer perceptron (without bias)4.
Therefore, it is straightforward to deﬁne the loss function
ER(w) for the embedding regularizer by using binary cross
entropy:

with random weights. These three types of embedding pa-
rameters are compared in experiments.

Our implementation of the embedding regularizer is pub-
licly available from https://github.com/yu4u/
dnn-watermark.

ER(w) = −

(bj log(yj) + (1 − bj) log(1 − yj)) , (4)

4. Experiments

T
(cid:88)

j=1

In this section, we demonstrate that our embedding reg-
ularizer can embed a watermark without impairing the per-
formance of the host network, and the embedded watermark
is robust against various types of attack.

(5)

where yj = σ(ΣiXjiwi) and σ(·) is the sigmoid function:

σ(x) =

1
1 + exp(−x)

.

We call this loss function an embedding loss function. It
may be confusing that an embedding loss function is used
to update w, not X, in our framework. In a standard percep-
tron, w is an input and X is a parameter to be learned. In our
case, w is an embedding target and X is a ﬁxed parameter.
The design of X is discussed in the following section.

This approach does not impair the performance of the
host network in the original task as conﬁrmed in exper-
iments, because deep neural networks are typically over-
parameterized. It is well-known that deep neural networks
have many local minima, and that all local minima are
likely to have an error very close to that of the global min-
imum [9, 6]. Therefore, the embedding regularizer only
needs to guide model parameters to one of a number of good
local minima so that the ﬁnal model parameters have an ar-
bitrary watermark.

3.3. Regularizer Parameters

In this section we discuss the design of the embed-
ding parameter X, which can be considered as a secret
key [15] in detecting and embedding watermarks. While
X ∈ RT ×M can be an arbitrary matrix, it will affect the
performance of an embedded watermark because it is used
in both embedding and extraction of watermarks.
In this
paper, we consider three types of X: X direct, X diff, and
X random.

= 1.

X direct is constructed so that one element in each row of
X direct is ’1’ and the others are ’0’. In this case, the j-th
bit bj is directly embedded in a certain parameter wˆi s.t.
X direct
jˆi
X diff is created so that each row has one ’1’ element and
one ’-1’ element, and the others are ’0’. Using X diff, the
j-th bit bj is embedded into the difference between wi+ and
wi− where X diff
ji+

= 1 and X diff
ji−

= −1.

Each element of X random is independently drawn from
the standard normal distribution N (0, 1). Using X random,
each bit is embedded into all instances of the parameter w

4Although this single-layer perceptron can be deepened into multi-

layer perceptron, we focus on the simplest one in this paper.

4.1. Evaluation Settings

Datasets. For experiments, we used the well-known
CIFAR-10 and Caltech-101 datasets.
The CIFAR-10
dataset [22] consists of 60,000 32 × 32 color images in 10
classes, with 6,000 images per class. These images were
separated into 50,000 training images and 10,000 test im-
ages. The Caltech-101 dataset [10] includes pictures of ob-
jects belonging to 101 categories; it contains about 40 to
800 images per category. The size of each image is roughly
300 × 200 pixels but we resized them in 32 × 32 for ﬁne-
tuning. For each category, we used 30 images for training
and at most 40 of the remaining images for testing.

Host network and training settings. We used the
wide residual network [33] as the host network. The wide
residual network is an efﬁcient variant of the residual net-
work [16]. Table 3 shows the structure of the wide residual
network with a depth parameter N and a width parameter
k. In all our experiments, we set N = 1 and k = 4, and
used SGD with Nesterov momentum and cross-entropy loss
in training. The initial learning rate was set at 0.1, weight
decay to 5.0×10−4, momentum to 0.9 and minibatch size
to 64. The learning rate was dropped by a factor of 0.2 at
60, 120 and 160 epochs, and we trained for a total of 200
epochs, following the settings used in [33].

We embedded a watermark into one of the following
the second convolution layer in the
convolution layers:
conv 2, conv 3, and conv 4 groups. In the following, we
mention the location of the host layer by simply describing
the conv 2, conv 3, or conv 4 group. In Table 3, the num-
ber M of parameter w is also shown for these layers. The
parameter λ in Eq. (1) is set to 0.01. As a watermark, we
embedded b = 1 ∈ {0, 1}T in the following experiments.

4.2. Embedding Results

First, we conﬁrm that a watermark was successfully em-
bedded in the host network by the proposed embedding reg-
ularizer. We trained the host network from scratch (train-to-
embed) on the CIFAR-10 dataset with and without embed-
ding a watermark. In the embedding case, a 256-bit water-
mark (T = 256) was embedded into the conv 2 group.

4

Table 3. Structure of the host network.

Group Output size
conv 1

32 × 32

conv 2

32 × 32

#w
N/A

× N 144 × k

Building block
[3 × 3, 16]
(cid:21)
(cid:20)3 × 3, 16 × k
3 × 3, 16 × k
(cid:20)3 × 3, 32 × k
3 × 3, 32 × k
(cid:20)3 × 3, 64 × k
3 × 3, 64 × k

(cid:21)

(cid:21)

conv 3

16 × 16

× N 288 × k

conv 4

8 × 8

1 × 1

× N 576 × k

avg-pool, fc, soft-max

N/A

Table 4 shows the best test errors and embedding losses
ER(w) of the host networks with and without embedding.
We can see that the test errors of Not embedded and ran-
dom are almost the same while those of direct and diff
are slightly larger. The embedding loss ER(w) of ran-
dom is extremely low compared with those of direct and
diff. These results indicate that the random approach can
effectively embed a watermark without impairing the per-
formance in the original task.

4.2.2 Detecting Watermarks

Figure 2 shows the histogram of the embedded watermark
σ(ΣiXjiwi) (before thresholding) with and without wa-
termarks where (a) direct, (b) diff, and (c) random pa-
rameters are used in embedding and detection.
If we bi-
narize σ(ΣiXjiwi) at a threshold of 0.5, all watermarks
are correctly detected because ∀j, σ(ΣiXjiwi) ≥ 0.5
(⇔ ΣiXjiwi ≥ 0) for all embedded cases. Please note
that we embedded b = 1 ∈ {0, 1}T as a watermark as men-
tioned before. Although random watermarks will be de-
tected for the non-embedded cases, it can be easily judged
that the watermark is not embedded because the distribution
of σ(ΣiXjiwi) is quite different from those for embedded
cases.

4.2.3 Distribution of Model Parameters

We explore how trained model parameters are affected by
the embedded watermarks. Figure 3 shows the distribution
of model parameters W (not w) with and without water-
marks. These parameters are taken only from the layer in
which a watermark was embedded. Note that W is the pa-
rameter before taking the mean over ﬁlters, and thus the
number of parameters is 3 × 3 × 64 × 64. We can see that
direct and diff signiﬁcantly alter the distribution of param-
eters while random does not. In direct, many parameters
became large and a peak appears near 2 so that their mean
over ﬁlters becomes a large positive value to reduce the em-
bedding loss. In diff, most parameters were pushed in both
positive and negative directions so that the differences be-
tween these parameters became large. In random, a water-
mark is diffused over all parameters with random weights
and thus does not signiﬁcantly alter the distribution. This is
one of the desirable properties of watermarking related the
security requirement; one may be aware of the existence of
the embedded watermarks for the direct and diff cases.

The results so far indicated that the random approach
seemed to be the best choice among the three, with low em-
bedding loss, low test error in the original task, and not al-
tering the parameter distribution. Therefore, in the follow-
ing experiments, we used the random approach in embed-
ding watermarks without explicitly indicating it.

Figure 1. Training curves for the host network on CIFAR-10 as a
function of epochs. Solid lines denote test error (y-axis on the left)
and dashed lines denote training loss E(w) (y-axis on the right).

Table 4. Test error (%) and embedding loss ER(w) with and with-
out embedding.

Not embedded
direct
diff
random

Test error
8.04
8.21
8.37
7.97

ER(w)
N/A
1.24×10−1
6.20×10−2
4.76×10−4

4.2.1 Test Error and Training Loss

Figure 1 shows the training curves for the host network in
CIFAR-10 as a function of epochs. Not embedded is the
case that the host network is trained without the embed-
ding regularizer. Embedded (direct), Embedded (diff),
and Embedded (random) respectively represent training
curves with embedding regularizers whose parameters are
X direct, X diff, and X random. We can see that the training
loss E(w) with a watermark becomes larger than the not-
embedded case if the parameters X direct and X diff are used.
This large training loss is dominated by the embedding loss
ER(w), which indicates that it is difﬁcult to embed a water-
mark directly into a parameter or even into the difference of
two parameters. On the other hand, the training loss of Em-
bedded (random) is very close to that of Not embedded.

5

(a) direct

(b) diff
Figure 2. Histogram of the embedded watermark σ(ΣiXjiwi) (before thresholding) with and without watermarks.

(c) random

(a) Not embedded

(b) direct

(c) diff

(d) random

Figure 3. Distribution of model parameters W with and without watermarks.

on the same CIFAR-10 dataset with embedding and with-
out embedding (for comparison). In the second experiment,
the host network is trained on the Caltech-101 dataset, and
then ﬁne-tuned on the CIFAR-10 dataset with and without
embedding.

Table 5 (a) shows the result of the ﬁrst experiment. Not
embedded 1st corresponds to the ﬁrst training without em-
bedding. Not embedded 2nd corresponds to the second
training without embedding and Embedded corresponds
to the second training with embedding. Figure 4 shows the
training curves of these ﬁne-tunings5. We can see that Em-
bedded achieved almost the same test error as Not embed-
ded 2nd and a very low ER(w).

Table 5 (b) shows the results of the second experiment.
Not embedded 2nd corresponds to the second training
without embedding and Embedded corresponds to the sec-
ond training with embedding. The test error and training
loss of the ﬁrst training are not shown because they are not
compatible between these two different training datasets.
From these results, it was also conﬁrmed that Embedded
achieved almost the same test error as Not embedded 2nd
and very low ER(w). Thus, we can say that the proposed
method is effective even in the ﬁne-tune-to-embed situation
(in the same and different domains).

Finally, embedding a watermark in the distill-to-embed

5Note that the learning rate was also initialized to 0.1 at the beginning
of the second training, while the learning rate was reduced to 8.0 × 10−4)
at the end of the ﬁrst training.

Figure 4. Training curves for ﬁne-tuning the host network. The
ﬁrst and second halves of epochs correspond to the ﬁrst and sec-
ond training. Solid lines denote test error (y-axis on the left) and
dashed lines denote training loss (y-axis on the right).

4.2.4 Fine-tune-to-embed and Distill-to-embed

In the above experiments, a watermark was embedded by
training the host network from scratch (train-to-embed).
Here, we evaluated the other two situations introduced in
Section 2.2: ﬁne-tune-to-embed and distill-to-embed. For
In
ﬁne-tune-to-embed, two experiments were performed.
the ﬁrst experiment, the host network was trained on the
CIFAR-10 dataset without embedding, and then ﬁne-tuned

6

Table 5. Test error (%) and embedding loss ER(w) with and with-
out embedding in ﬁne-tuning and distilling.

Table 6. Test error (%) and embedding loss ER(w) for the combi-
nations of embedded groups and sizes of embedded bits.

(a) Fine-tune-to-embed (CIFAR-10 → CIFAR-10)

Not embedded 1st
Not embedded 2nd
Embedded

Test error
8.04
7.66
7.70

ER(w)
N/A
N/A
4.93×10−4

(b) Fine-tune-to-embed (Caltech-101 → CIFAR-10)

ER(w)
N/A
4.83×10−4
(c) Distill-to-embed (CIFAR-10 → CIFAR-10)

Not embedded 2nd
Embedded

Test error
7.93
7.94

Not embedded 1st
Not embedded 2nd
Embedded

Test error
8.04
7.86
7.75

ER(w)
N/A
N/A
5.01×10−4

situation was evaluated. The host network is ﬁrst trained
on the CIFAR-10 dataset without embedding. Then, the
trained network was further ﬁne-tuned on the same CIFAR-
In this second
10 dataset with and without embedding.
training, the training labels of the CIFAR-10 dataset were
not used. Instead, the predicted values of the trained net-
work were used as soft targets [17]. In other words, no label
was used in the second training. Table 5 (c) shows the re-
sults for the distill-to-embed situation. Not embedded 1st
corresponds to the ﬁrst training and Embedded (Not em-
bedded 2nd) corresponds to the second distilling training
with embedding (without embedding). It was found that the
proposed method also achieved low test error and ER(w) in
the distill-to-embed situation.

4.2.5 Capacity of Watermark.

In this section, the capacity of the embedded watermark is
explored by embedding different sizes of watermarks into
different groups in the train-to-embed manner. Please note
that the number of parameters w of conv 2, conv 3, and
conv 4 groups were 576, 1152, and 2304, respectively. Ta-
ble 6 shows test error (%) and embedding loss ER(w) for
combinations of different embedded blocks and different
number of embedded bits. We can see that embedded loss or
test error becomes high if the number of embedded bits be-
comes larger than the number of parameters w (e.g. 2,048
bits in conv 3) because the embedding problem becomes
overdetermined in such cases. Thus, the number of embed-
ded bits should be smaller than the number of parameters
w, which is a limitation of the embedding method using a
single-layer perceptron. This limitation would be resolved
by using a multi-layer perceptron in the embedding regular-
izer.

Embedded bits

(a) Test error (%)

Embedded group
conv 3
7.98
8.22
8.12
8.93

conv 2
7.97
8.47
8.43
8.17
(b) Embedding loss

conv 4
7.92
7.84
7.84
7.75

256
512
1,024
2,048

Embedded bits

256
512
1,024
2,048

conv 2
4.76×10−4
8.11×10−4
6.74×10−2
5.35×10−1

Embedded group
conv 3
7.20×10−4
8.18×10−4
1.53×10−3
3.70×10−2

conv 4
1.10×10−2
1.25×10−2
1.53×10−2
3.06×10−2

Table 7. Losses, test error, and bit error rate (BER) after embed-
ding a watermark with different λ.

λ
0
1
10
100

1

2 ||w − w0||2
0.000
0.184
1.652
7.989

2 ER(w) Test error

1.066
0.609
0.171
0.029

8.04
8.52
10.57
13.00

BER
0.531
0.324
0.000
0.000

4.2.6 Embedding without Training

As mentioned in Section 3.2, it is possible to embed a wa-
termark to a host network by directly modifying the trained
parameter w0 as usually done in image domain. Here we try
to do this by minimizing the following loss function instead
of Eq. (1):

E(w) = 1

2 ||w − w0||2

2 + λER(w),

(6)

where the embedding loss ER(w) is minimized while min-
imizing the difference between the modiﬁed parameter w
and the original parameter w0. Table 7 summarizes the em-
bedding results after Eq. (6) against the host network trained
on the CIFAR-10 dataset. We can see that embedding fails
for λ ≤ 1 as bit error rate (BER) is larger than zero while the
test error of the original task becomes too large for λ > 1.
Thus, it is not effective to directly embed a watermark with-
out considering the original task.

4.3. Robustness of Embedded Watermarks

In this section, the robustness of a proposed watermark is
evaluated for the three attack types explained in Section 2.3:
ﬁne-tuning and model compression.

4.3.1 Robustness against Fine-tuning

Fine-tuning or transfer learning [28] seems to be the most
likely type of (unintentional) attack because it is frequently

7

Table 8. Embedding loss before ﬁne-tuning (ER(w)) and after
ﬁne-tuning (E(cid:48)
R(w)), and the best test error (%) after ﬁne-tuning.

CIFAR-10 → CIFAR-10
Caltech-101 → CIFAR-10

ER(w)
4.76×10−4
5.96×10−3

E(cid:48)

R(w)
8.66×10−4
1.56×10−2

Test error
7.69
7.88

performed on trained models to apply them to other but
similar tasks with less effort than training a network from
scratch or to avoid over-ﬁtting when sufﬁcient training data
is not available.

In this experiment, two trainings we performed; in the
ﬁrst training, a 256-bit watermark was embedded in the
conv 2 group in the train-to-embed manner, and then the
host network was further ﬁne-tuned in the second training
without embedding, to determine whether the watermark
embedded in the ﬁrst training stayed in the host network
or not, even after the second training (ﬁne-tuning).

Table 8 shows the embedding loss before ﬁne-tuning
(ER(w)) and after ﬁne-tuning (E(cid:48)
R(w)), and the best test er-
ror after ﬁne-tuning. We evaluated ﬁne-tuning in the same
domain (CIFAR-10 → CIFAR-10) and in different domains
(Caltech-101 → CIFAR-10). We can see that, in both cases,
the embedding loss was slightly increased by ﬁne-tuning
but was still low. In addition, the bit error rate of the de-
tected watermark was equal to zero in both cases. The rea-
son why the embedding loss in ﬁne-tuning in the different
domains is higher than that in the same domain is that the
Caltech-101 dataset is signiﬁcantly more difﬁcult than the
CIFAR-10 dataset in our settings; all images in the Caltech-
101 dataset were resized to 32 × 326 for compatibility with
the CIFAR-10 dataset.

4.3.2 Robustness against Model Compression

It is sometimes difﬁcult to deploy deep neural networks to
embedded systems or mobile devices because they are both
computationally intensive and memory intensive. In order
to solve this problem, the model parameters are often com-
pressed [14, 12, 13]. The compression of model parameters
can intentionally or unintentionally act as an attack against
watermarks. In this section, we evaluate the robustness of
our watermarks against model compression, in particular,
against parameter pruning [14]. In parameter pruning, pa-
rameters whose absolute values are very small are cut-off to
zero. In [13], quantization of weights and the Huffman cod-
ing of quantized values are further applied. Because quanti-
zation has less impact than parameter pruning and the Huff-
man coding is lossless compression, we focus on parameter
pruning.

In order to evaluate the robustness against parameter
pruning, we embedded a 256-bit watermark in the conv

6This size is extremely small compared with their original sizes

(roughly 300 × 200).

8

2 group while training the host network on the CIFAR-10
dataset. We removed α% of the 3 × 3 × 64 × 64 parameters
of the embedded layer and calculated embedding loss and
bit error rate. Figure 5 (a) shows embedding loss ER(w)
as a function of pruning rate α. Ascending (Descending)
represents embedding loss when the top α% parameters are
cut-off according to their absolute values in ascending (de-
scending) order. Random represents embedding loss where
α% of parameters are randomly removed. Ascending cor-
responds to parameter pruning and the others were evalu-
ated for comparison. We can see that the embedding loss of
Ascending increases more slowly than those of Descend-
ing and Random as α increases. It is reasonable that model
parameters with small absolute values have less impact on
a detected watermark because the watermark is extracted
from the dot product of the model parameter w and the con-
stant embedding parameter (weight) X.

Figure 5 (b) shows the bit error rate as a function of prun-
ing rate α. Surprisingly, the bit error rate was still zero after
removing 65% of the parameters and 2/256 even after 80%
of the parameters were pruned (Ascending). We can say
that the embedded watermark is sufﬁciently robust against
parameter pruning because, in [13], the resulting pruning
rate of convolutional layers ranged from to 16% to 65% for
the AlexNet [23], and from 42% to 78% for VGGNet [28].
Furthermore, this degree of bit error can be easily corrected
by an error correction code (e.g. the BCH code). Figure 6
shows the histogram of the detected watermark σ(ΣiXjiwi)
after pruning for α = 0.8 and 0.95. For α = 0.95, the his-
togram of the detected watermark is also shown for the host
network into which no watermark is embedded. We can see
that many of σ(ΣiXjiwi) are still close to one for the em-
bedded case, which might be used as a conﬁdence score in
judging the existence of a watermark (zero-bit watermark-
ing).

5. Conclusions and Future Work

In this paper, we have proposed a general framework
for embedding a watermark in deep neural network models
to protect the rights to the trained models. First, we for-
mulated a new problem: embedding watermarks into deep
neural networks. We also deﬁned requirements, embedding
situations, and attack types for watermarking deep neural
networks. Second, we proposed a general framework for
embedding a watermark in model parameters using a pa-
rameter regularizer. Our approach does not impair the per-
formance of networks into which a watermark is embed-
ded. Finally, we performed comprehensive experiments to
reveal the potential of watermarking deep neural networks
as the basis of this new problem. We showed that our frame-
work could embed a watermark without impairing the per-
formance of a deep neural network. The embedded water-
mark did not disappear even after ﬁne-tuning or parameter

(a) Embedding loss.

Figure 6. Histogram of the detected watermark σ(ΣiXjiwi) after
pruning.

against parameter pruning in this paper, a watermark might
be embedded in conjunction with compressing models. For
example, in [13], after parameter pruning, the network is
re-trained to learn the ﬁnal weights for the remaining sparse
parameters. Our embedding regularizer can be used in this
re-training to embed a watermark.

Network morphism. In [4, 32], a systematic study has
been done on how to morph a well-trained neural network
into a new one so that its network function can be com-
pletely preserved for further training. This network mor-
phism can constitute a severe attack against our watermark
because it may be impossible to detect the embedded water-
mark if the topology of the host network is severely modi-
ﬁed. We left the investigation how the embedded watermark
is affected by this network morphism for future work.

Steganalysis. Steganalysis [27, 21] is a method for de-
tecting the presence of secretly hidden data (e.g. steganog-
raphy or watermarks) in digital media ﬁles such as images,
video, audio, and, in our case, deep neural networks. Water-
marks ideally are robust against steganalysis. While, in this
paper, we conﬁrmed that embedding watermarks does not
signiﬁcantly change the distribution of model parameters,
more exploration is needed to evaluate robustness against
steganalysis. Conversely, developing effective steganalysis
against watermarks for deep neural networks can be an in-
teresting research topic.

Fingerprinting. Digital ﬁngerprinting is an alternative
to the watermarking approach for persistent identiﬁcation
of images [2], video [20, 31], and audio clips [1, 11]. In
this paper, we focused on one of these two important ap-
proaches. Robust ﬁngerprinting of deep neural networks is
another and complementary direction to protect deep neural
network models.

(b) Bit error rate.
Figure 5. Embedding loss and bit error rate after pruning as a func-
tion of pruning rate.

pruning; the entire watermark remained even after 65% of
the parameters were pruned.

5.1. Future Work

Although we have obtained ﬁrst insight into the new
problem of embedding a watermark in deep neural net-
works, many things remain as future work.

Watermark overwriting. A third-party user may em-
bed a different watermark in order to overwrite the origi-
nal watermark.
In our preliminary experiments, this wa-
termark overwriting caused 30.9%, 8.6%, and 0.4% bit er-
rors against watermarks in the conv 2, conv 3, and conv 4
groups when 256-bit watermarks were additionally embed-
ded. More robust watermarking against overwriting should
be explored (e.g. non-linear embedding).

Compression as embedding. Compressing deep neu-
ral networks is a very important and active research topic.
While we conﬁrmed that our watermark is very robust

9

[20] A. Joly, C. Frelicot, and O. Buisson. Content-based video
copy detection in large databases: a local ﬁngerprints sta-
tistical similarity search approach. In Proc. of ICIP, pages
505–508, 2005.

[21] J. Kodovsky, J. Fridrich, and V. Holub. Ensemble classiﬁers
for steganalysis of digital media. IEEE Trans. on Information
Forensics and Security, 7(2):432–444, 2012.

[22] A. Krizhevsky. Learning multiple layers of features from

tiny images. Tech Report, 2009.

[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
Proc. of NIPS, 2012.

Imagenet
In

[24] A. Krogh and J. A. Hertz. A simple weight decay can im-

prove generalization. In Proc. of NIPS, 1992.

[25] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998.

[26] M. Abadi, et al. Tensorﬂow: Large-scale machine learning
on heterogeneous distributed systems. arXiv:1603.04467,
2016.

[27] L. Shaohui, Y. Hongxun, and G. Wen. Neural network based
steganalysis in still images. In Proc. of ICME, 2003.
[28] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In Proc. of ICLR,
2015.

[29] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In Proc. of CVPR, 2015.

[30] S. Tokui, K. Oono, S. Hido, and J. Clayton. Chainer: a
next-generation open source framework for deep learning.
In Proc. of NIPS Workshop on Machine Learning Systems,
2015.

[31] Y. Uchida, M. Agrawal, and S. Sakazawa. Accurate content-
based video copy detection with efﬁcient feature indexing.
In Proc. of ICMR, 2011.

[32] T. Wei, C. Wang, Y. Rui, and C. W. Chen. Network mor-

phism. In Proc. of ICML, 2016.

[33] S. Zagoruyko and N. Komodakis. Wide residual networks.

In Proc. of ECCV, 2016.

References

[1] X. Anguera, A. Garzon, and T. Adamek. Mask: Robust local
features for audio ﬁngerprinting. In Proc. of ICME, 2012.
[2] J. Barr, B. Bradley, and B. T. Hannigan. Using digital wa-
termarks with image signatures to mitigate the threat of the
copy attack. In Proc. of ICASSP, pages 69–72, 2003.

[3] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu,
G. Desjardins, J. Turian, D. Warde-Farley, and Y. Bengio.
Theano: a CPU and GPU math expression compiler. In Proc.
of the Python for Scientiﬁc Computing Conference (SciPy),
2010.

[4] T. Chen, I. Goodfellow, and J. Shlens. Net2net: Accelerating
learning via knowledge transfer. In Proc. of ICLR, 2016.

[5] F. Chollet. Keras. GitHub repository, 2015.
[6] A. Choromanska, M. Henaff, M. Mathieu, G. Arous, and
Y. LeCun. The loss surfaces of multilayer networks. In Proc.
of AISTATS, 2015.

[7] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A
matlab-like environment for machine learning. In Proc. of
NIPS Workshop on BigLearn, 2011.

[8] I. Cox, M. Miller, J. Bloom, J. Fridrich, and T. Kalker. Dig-
ital Watermarking and Steganography. Morgan Kaufmann
Publishers Inc., 2 edition, 2008.

[9] Y. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli,
Identifying and attacking the saddle point
In

and Y. Bengio.
problem in high-dimensional non-convex optimization.
Proc. of NIPS, 2014.

[10] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative
visual models from few training examples: an incremen-
tal bayesian approach tested on 101 object categories.
In
Proc. of CVPR Workshop on Generative-Model Based Vi-
sion, 2004.

[11] J. Haitsma and T. Kalker. A highly robust audio ﬁngerprint-
ing system. In Proc. of ISMIR, pages 107–115, 2002.
[12] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz,
and W. J. Dally. Eie: Efﬁcient inference engine on com-
pressed deep neural network. In Proc. of ISCA, 2016.
[13] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-
pressing deep neural networks with pruning, trained quanti-
zation and huffman coding. In Proc. of ICLR, 2016.

[14] S. Han, J. Pool, J. Tran, and W. J. Dally. Learning both
weights and connections for efﬁcient neural networks.
In
Proc. of NIPS, 2015.

[15] F. Hartung and M. Kutter. Multimedia watermarking tech-

niques. Proceedings of the IEEE, 87(7):1079–1107, 1999.

[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In Proc. of CVPR, 2016.

[17] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
In Proc. of NIPS Workshop on Deep

in a neural network.
Learning and Representation Learning, 2014.

[18] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural Computation, 9(8):1735–1780, 1997.

[19] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
In Proc. of MM,
architecture for fast feature embedding.
2014.

10

7
1
0
2
 
r
p
A
 
0
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
8
0
4
0
.
1
0
7
1
:
v
i
X
r
a

Embedding Watermarks into Deep Neural Networks

Yusuke Uchida
KDDI Research, Inc.
Saitama, Japan

Yuki Nagai
KDDI Research, Inc.
Saitama, Japan

Shigeyuki Sakazawa
Saitama Research, Inc.
Tokyo, Japan

Shin’ichi Satoh
National Institute of Informatics
Tokyo, Japan

Abstract

Signiﬁcant progress has been made with deep neural net-
works recently. Sharing trained models of deep neural net-
works has been a very important in the rapid progress of
research and development of these systems. At the same
time, it is necessary to protect the rights to shared trained
models. To this end, we propose to use digital watermarking
technology to protect intellectual property and detect intel-
lectual property infringement in the use of trained models.
First, we formulate a new problem: embedding watermarks
into deep neural networks. We also deﬁne requirements,
embedding situations, and attack types on watermarking in
deep neural networks. Second, we propose a general frame-
work for embedding a watermark in model parameters, us-
ing a parameter regularizer. Our approach does not im-
pair the performance of networks into which a watermark
is placed because the watermark is embedded while train-
ing the host network. Finally, we perform comprehensive
experiments to reveal the potential of watermarking deep
neural networks as the basis of this new research effort. We
show that our framework can embed a watermark during
the training of a deep neural network from scratch, and dur-
ing ﬁne-tuning and distilling, without impairing its perfor-
mance. The embedded watermark does not disappear even
after ﬁne-tuning or parameter pruning; the watermark re-
mains complete even after 65% of parameters are pruned.

1. Introduction

Deep neural networks have recently been signiﬁcantly
improved. In particular, deep convolutional neural networks
(DCNN) such as LeNet [25], AlexNet [23], VGGNet [28],
GoogLeNet [29], and ResNet [16] have become de facto
standards for object recognition, image classiﬁcation, and
retrieval. In addition, many deep learning frameworks have

1

been released that help engineers and researchers to develop
systems based on deep learning or do research with less
effort. Examples of these great deep learning frameworks
are Caffe [19], Theano [3], Torch [7], Chainer [30], Tensor-
Flow [26], and Keras [5].

Although these frameworks have made it easy to utilize
deep neural networks in real applications, the training of
deep neural network models is still a difﬁcult task because it
requires a large amount of data and time; several weeks are
needed to train a very deep ResNet with the latest GPUs on
the ImageNet dataset for instance [16]. Therefore, trained
models are sometimes provided on web sites to make it
easy to try a certain model or reproduce the results in re-
search articles without training. For example, Model Zoo1
provides trained Caffe models for various tasks with useful
utility tools. Fine-tuning or transfer learning [28] is a strat-
egy to directly adapt such already trained models to another
application with minimum re-training time. Thus, sharing
trained models is very important in the rapid progress of
both research and development of deep neural network sys-
tems.
In the future, more systematic model-sharing plat-
forms may appear, by analogy with video sharing sites. Fur-
thermore, some digital distribution platforms for purchase
and sale of the trained models or even artiﬁcial intelligence
skills (e.g. Alexa Skills2) may appear, similar to Google
Play or App Store.
In these situations, it is necessary to
protect the rights to shared trained models.

To this end, we propose to utilize digital watermarking
technology, which is used to identify ownership of the copy-
right of digital content such as images, audio, and videos. In
particular, we propose a general framework to embed a wa-
termark in deep neural networks models to protect intellec-
tual property and detect intellectual property infringement
of trained models. To the best of our knowledge, this is ﬁrst
attempt to embed a watermark in a deep neural network.

1https://github.com/BVLC/caffe/wiki/Model-Zoo
2https://www.alexaskillstore.com/

The contributions of this research are three-fold, as follows:

1. We formulate a new problem: embedding watermarks
in deep neural networks. We also deﬁne requirements,
embedding situations, and attack types for watermark-
ing deep neural networks.

2. We propose a general framework to embed a water-
mark in model parameters, using a parameter regular-
izer. Our approach does not impair the performance of
networks in which a watermark is embedded.

3. We perform comprehensive experiments to reveal the

potential of watermarking deep neural networks.

2. Problem Formulation

Given a model network with or without trained parame-
ters, we deﬁne the task of watermark embedding as to em-
bed T -bit vector b ∈ {0, 1}T into the parameters of one or
more layers of the neural network. We refer to a neural net-
work in which a watermark is embedded as a host network,
and refer to the task that the host network is originally trying
to perform as the original task.

In the following, we formulate (1) requirements for an
embedded watermark or an embedding method, (2) embed-
ding situations, and (3) expected attack types against which
embedded watermarks should be robust.

2.1. Requirements

Table 1 summarizes the requirements for an effective wa-
termarking algorithm in an image domain [15, 8] and a neu-
ral network domain. While both domains share almost the
same requirements, ﬁdelity and robustness are different in
image and neural network domains.

For ﬁdelity in an image domain, it is essential to main-
tain the perceptual quality of the host image while embed-
ding a watermark. However, in a neural network domain,
the parameters themselves are not important. Instead, the
performance of the original task is important. Therefore it
is essential to maintain the performance of the trained host
network, and not to hamper the training of a host network.
Regarding robustness, as images are subject to vari-
ous signal processing operations, an embedded watermark
should stay in the host image even after these operations.
And the greatest possible modiﬁcation to a neural network
is ﬁne-tuning or transfer learning [28]. An embedded wa-
termark in a neural network should be detectable after ﬁne-
tuning or other possible modiﬁcations.

2.2. Embedding Situations

We classify the embedding situations into three types:
train-to-embed, ﬁne-tune-to-embed, and distill-to-embed,
as summarized in Table 2.

Train-to-embed is the case in which the host network is
trained from scratch while embedding a watermark where
labels for training data are available.

Fine-tune-to-embed is the case in which a watermark is
embedded while ﬁne-tuning. In this case, model parame-
ters are initialized with a pre-trained network. The network
conﬁguration near the output layer may be changed before
ﬁne-tuning.

Distill-to-embed is the case in which a watermark is
embedded into a trained network without labels using the
distilling approach [17]. Embedding is performed in ﬁne-
tuning where the predictions of the trained model are used
as labels. In the standard distill framework, a large network
(or multiple networks) is ﬁrst trained and then a smaller net-
work is trained using the predicted labels of the large net-
work in order to compress the large network. In this paper,
we use the distill framework simply to train a network with-
out labels.

The ﬁrst two situations assume that the copyright holder
of the host network is expected to embed a watermark to
the host network in training or ﬁne-tuning. Fine-tune-to-
embed is also useful when a model owner wants to embed
individual watermarks to identify those to whom the model
had been distributed. By doing so, individual instances can
be tracked. The last situation assumes that a non-copyright
holder (e.g., a platformer) is entrusted to embed a water-
mark on behalf of a copyright holder.

2.3. Expected Attack Types

Related to the requirement for robustness in Section 2.1,
we assume two types of attacks against which embedded
watermarks should be robust: ﬁne-tuning and model com-
pression. These types of attack are very speciﬁc to deep
neural networks, while one can easily imagine model com-
pression by analogy with lossy image compression in the
image domain.

Fine-tuning or transfer learning [28] seems to be the
most feasible type of attack, because it reduces the burden
of training deep neural networks. Many models have been
constructed on top of existing state-of-the-art models. Fine-
tuning alters the model parameters, and thus embedded wa-
termarks should be robust against this alteration.

Model compression is very important in deploying deep
neural networks to embedded systems or mobile devices
as it can signiﬁcantly reduce memory requirements and/or
computational cost. Lossy compression distorts model pa-
rameters, so we should explore how it affects the detection
rate.

3. Proposed Framework

In this section, we propose a framework for embedding
a watermark into a host network. Although we focus on a

2

Table 1. Requirements for an effective watermarking algorithm in the image and neural network domains.

Fidelity

Robustness

Capacity

Security

Efﬁciency

Neural networks domain
The effectiveness of the host network should
not be degraded by embedding a watermark.
The embedded watermark should be robust
against model modiﬁcations such as ﬁne-
tuning and model compression.

Image domain
The quality of the host image should not be
degraded by embedding a watermark.
The embedded watermark should be robust
against common signal processing operations
such as lossy compression, cropping, resiz-
ing, and so on.
The effective watermarking system must have the ability to embed a large amount of in-
formation.
A watermark should in general be secret and should not be accessed, read, or modiﬁed by
unauthorized parties.
The watermark embedding and extraction processes should be fast.

Table 2. Three embedding situations. Fine-tune indicates whether
parameters are initialized in embedding using already trained mod-
els, or not. Label availability indicates whether or labels for train-
ing data are available in embedding.

Fine-tune Label availability

Train-to-embed
Fine-tune-to-embed
Distill-to-embed

(cid:88)
(cid:88)

(cid:88)
(cid:88)

DCNN [25] as the host, our framework is essentially ap-
plicable to other networks such as standard multilayer per-
ceptron (MLP), recurrent neural network (RNN), and long
short-term memory (LSTM) [18].

3.1. Embedding Targets

In this paper, a watermark is assumed to be embedded
into one of the convolutional layers in a host DCNN3. Let
(S, S), D, and L respectively denote the size of the con-
volution ﬁlter, the depth of input to the convolutional layer,
and the number of ﬁlters in the convolutional layer. The pa-
rameters of this convolutional layer are characterized by the
tensor W ∈ RS×S×D×L. The bias term is ignored here.
Let us think of embedding a T -bit vector b ∈ {0, 1}T into
W . The tensor W is a set of L convolutional ﬁlters and the
order of the ﬁlters does not affect the output of the network
if the parameters of the subsequent layers are appropriately
re-ordered. In order to remove this arbitrariness in the or-
der of ﬁlters, we calculate the mean of W over L ﬁlters as
l Wijkl. Letting w ∈ RM (M = S × S × D)
W ijk = 1
L
denote a ﬂattened version of W , our objective is now to em-
bed T -bit vector b into w.

(cid:80)

It is possible to embed a watermark in a host network
by directly modifying w of a trained network, as is usu-

3Fully-connected layers can also be used but we focus on convolu-
tional layers here, because fully-connected layers are often discarded in
ﬁne-tuning.

ally done in the image domain. However, this approach de-
grades the performance of the host network in the original
task as shown later in Section 4.2.6. Instead, we propose
embedding a watermark while training a host network for
the original task so that the existence of the watermark does
not impair the performance of the host network in its orig-
inal task. To this end, we utilize a parameter regularizer,
which is an additional term in the original cost function for
the original task. The cost function E(w) with a regularizer
is deﬁned as:

E(w) = E0(w) + λER(w),

(1)

where E0(w) is the original cost function, ER(w) is a regu-
larization term that imposes a certain restriction on param-
eters w, and λ is an adjustable parameter. A regularizer is
usually used to prevent the parameters from growing too
large. L2 regularization (or weight decay [24]), L1 regu-
larization, and their combination are often used to reduce
over-ﬁtting of parameters for complex neural networks. For
instance, ER(w) = ||w||2

2 in the L2 regularization.

In contrast to these standard regularizers, our regularizer
imposes parameter w to have a certain statistical bias, as a
watermark in a training process. We refer to this regular-
izer as an embedding regularizer. Before deﬁning the em-
bedding regularizer, we explain how to extract a watermark
from w. Given a (mean) parameter vector w ∈ RM and an
embedding parameter X ∈ RT ×M , the watermark extrac-
tion is simply done by projecting w using X, followed by
thresholding at 0. More precisely, the j-th bit is extracted
as:

bj = s(ΣiXjiwi),

s(x) =

(cid:40)

1 x ≥ 0
else.
0

(2)

(3)

This process can be considered to be a binary classiﬁca-

3.2. Embedding Regularizer

where s(x) is a step function:

3

tion problem with a single-layer perceptron (without bias)4.
Therefore, it is straightforward to deﬁne the loss function
ER(w) for the embedding regularizer by using binary cross
entropy:

with random weights. These three types of embedding pa-
rameters are compared in experiments.

Our implementation of the embedding regularizer is pub-
licly available from https://github.com/yu4u/
dnn-watermark.

ER(w) = −

(bj log(yj) + (1 − bj) log(1 − yj)) , (4)

4. Experiments

T
(cid:88)

j=1

In this section, we demonstrate that our embedding reg-
ularizer can embed a watermark without impairing the per-
formance of the host network, and the embedded watermark
is robust against various types of attack.

(5)

where yj = σ(ΣiXjiwi) and σ(·) is the sigmoid function:

σ(x) =

1
1 + exp(−x)

.

We call this loss function an embedding loss function. It
may be confusing that an embedding loss function is used
to update w, not X, in our framework. In a standard percep-
tron, w is an input and X is a parameter to be learned. In our
case, w is an embedding target and X is a ﬁxed parameter.
The design of X is discussed in the following section.

This approach does not impair the performance of the
host network in the original task as conﬁrmed in exper-
iments, because deep neural networks are typically over-
parameterized. It is well-known that deep neural networks
have many local minima, and that all local minima are
likely to have an error very close to that of the global min-
imum [9, 6]. Therefore, the embedding regularizer only
needs to guide model parameters to one of a number of good
local minima so that the ﬁnal model parameters have an ar-
bitrary watermark.

3.3. Regularizer Parameters

In this section we discuss the design of the embed-
ding parameter X, which can be considered as a secret
key [15] in detecting and embedding watermarks. While
X ∈ RT ×M can be an arbitrary matrix, it will affect the
performance of an embedded watermark because it is used
in both embedding and extraction of watermarks.
In this
paper, we consider three types of X: X direct, X diff, and
X random.

= 1.

X direct is constructed so that one element in each row of
X direct is ’1’ and the others are ’0’. In this case, the j-th
bit bj is directly embedded in a certain parameter wˆi s.t.
X direct
jˆi
X diff is created so that each row has one ’1’ element and
one ’-1’ element, and the others are ’0’. Using X diff, the
j-th bit bj is embedded into the difference between wi+ and
wi− where X diff
ji+

= 1 and X diff
ji−

= −1.

Each element of X random is independently drawn from
the standard normal distribution N (0, 1). Using X random,
each bit is embedded into all instances of the parameter w

4Although this single-layer perceptron can be deepened into multi-

layer perceptron, we focus on the simplest one in this paper.

4.1. Evaluation Settings

Datasets. For experiments, we used the well-known
CIFAR-10 and Caltech-101 datasets.
The CIFAR-10
dataset [22] consists of 60,000 32 × 32 color images in 10
classes, with 6,000 images per class. These images were
separated into 50,000 training images and 10,000 test im-
ages. The Caltech-101 dataset [10] includes pictures of ob-
jects belonging to 101 categories; it contains about 40 to
800 images per category. The size of each image is roughly
300 × 200 pixels but we resized them in 32 × 32 for ﬁne-
tuning. For each category, we used 30 images for training
and at most 40 of the remaining images for testing.

Host network and training settings. We used the
wide residual network [33] as the host network. The wide
residual network is an efﬁcient variant of the residual net-
work [16]. Table 3 shows the structure of the wide residual
network with a depth parameter N and a width parameter
k. In all our experiments, we set N = 1 and k = 4, and
used SGD with Nesterov momentum and cross-entropy loss
in training. The initial learning rate was set at 0.1, weight
decay to 5.0×10−4, momentum to 0.9 and minibatch size
to 64. The learning rate was dropped by a factor of 0.2 at
60, 120 and 160 epochs, and we trained for a total of 200
epochs, following the settings used in [33].

We embedded a watermark into one of the following
the second convolution layer in the
convolution layers:
conv 2, conv 3, and conv 4 groups. In the following, we
mention the location of the host layer by simply describing
the conv 2, conv 3, or conv 4 group. In Table 3, the num-
ber M of parameter w is also shown for these layers. The
parameter λ in Eq. (1) is set to 0.01. As a watermark, we
embedded b = 1 ∈ {0, 1}T in the following experiments.

4.2. Embedding Results

First, we conﬁrm that a watermark was successfully em-
bedded in the host network by the proposed embedding reg-
ularizer. We trained the host network from scratch (train-to-
embed) on the CIFAR-10 dataset with and without embed-
ding a watermark. In the embedding case, a 256-bit water-
mark (T = 256) was embedded into the conv 2 group.

4

Table 3. Structure of the host network.

Group Output size
conv 1

32 × 32

conv 2

32 × 32

#w
N/A

× N 144 × k

Building block
[3 × 3, 16]
(cid:21)
(cid:20)3 × 3, 16 × k
3 × 3, 16 × k
(cid:20)3 × 3, 32 × k
3 × 3, 32 × k
(cid:20)3 × 3, 64 × k
3 × 3, 64 × k

(cid:21)

(cid:21)

conv 3

16 × 16

× N 288 × k

conv 4

8 × 8

1 × 1

× N 576 × k

avg-pool, fc, soft-max

N/A

Table 4 shows the best test errors and embedding losses
ER(w) of the host networks with and without embedding.
We can see that the test errors of Not embedded and ran-
dom are almost the same while those of direct and diff
are slightly larger. The embedding loss ER(w) of ran-
dom is extremely low compared with those of direct and
diff. These results indicate that the random approach can
effectively embed a watermark without impairing the per-
formance in the original task.

4.2.2 Detecting Watermarks

Figure 2 shows the histogram of the embedded watermark
σ(ΣiXjiwi) (before thresholding) with and without wa-
termarks where (a) direct, (b) diff, and (c) random pa-
rameters are used in embedding and detection.
If we bi-
narize σ(ΣiXjiwi) at a threshold of 0.5, all watermarks
are correctly detected because ∀j, σ(ΣiXjiwi) ≥ 0.5
(⇔ ΣiXjiwi ≥ 0) for all embedded cases. Please note
that we embedded b = 1 ∈ {0, 1}T as a watermark as men-
tioned before. Although random watermarks will be de-
tected for the non-embedded cases, it can be easily judged
that the watermark is not embedded because the distribution
of σ(ΣiXjiwi) is quite different from those for embedded
cases.

4.2.3 Distribution of Model Parameters

We explore how trained model parameters are affected by
the embedded watermarks. Figure 3 shows the distribution
of model parameters W (not w) with and without water-
marks. These parameters are taken only from the layer in
which a watermark was embedded. Note that W is the pa-
rameter before taking the mean over ﬁlters, and thus the
number of parameters is 3 × 3 × 64 × 64. We can see that
direct and diff signiﬁcantly alter the distribution of param-
eters while random does not. In direct, many parameters
became large and a peak appears near 2 so that their mean
over ﬁlters becomes a large positive value to reduce the em-
bedding loss. In diff, most parameters were pushed in both
positive and negative directions so that the differences be-
tween these parameters became large. In random, a water-
mark is diffused over all parameters with random weights
and thus does not signiﬁcantly alter the distribution. This is
one of the desirable properties of watermarking related the
security requirement; one may be aware of the existence of
the embedded watermarks for the direct and diff cases.

The results so far indicated that the random approach
seemed to be the best choice among the three, with low em-
bedding loss, low test error in the original task, and not al-
tering the parameter distribution. Therefore, in the follow-
ing experiments, we used the random approach in embed-
ding watermarks without explicitly indicating it.

Figure 1. Training curves for the host network on CIFAR-10 as a
function of epochs. Solid lines denote test error (y-axis on the left)
and dashed lines denote training loss E(w) (y-axis on the right).

Table 4. Test error (%) and embedding loss ER(w) with and with-
out embedding.

Not embedded
direct
diff
random

Test error
8.04
8.21
8.37
7.97

ER(w)
N/A
1.24×10−1
6.20×10−2
4.76×10−4

4.2.1 Test Error and Training Loss

Figure 1 shows the training curves for the host network in
CIFAR-10 as a function of epochs. Not embedded is the
case that the host network is trained without the embed-
ding regularizer. Embedded (direct), Embedded (diff),
and Embedded (random) respectively represent training
curves with embedding regularizers whose parameters are
X direct, X diff, and X random. We can see that the training
loss E(w) with a watermark becomes larger than the not-
embedded case if the parameters X direct and X diff are used.
This large training loss is dominated by the embedding loss
ER(w), which indicates that it is difﬁcult to embed a water-
mark directly into a parameter or even into the difference of
two parameters. On the other hand, the training loss of Em-
bedded (random) is very close to that of Not embedded.

5

(a) direct

(b) diff
Figure 2. Histogram of the embedded watermark σ(ΣiXjiwi) (before thresholding) with and without watermarks.

(c) random

(a) Not embedded

(b) direct

(c) diff

(d) random

Figure 3. Distribution of model parameters W with and without watermarks.

on the same CIFAR-10 dataset with embedding and with-
out embedding (for comparison). In the second experiment,
the host network is trained on the Caltech-101 dataset, and
then ﬁne-tuned on the CIFAR-10 dataset with and without
embedding.

Table 5 (a) shows the result of the ﬁrst experiment. Not
embedded 1st corresponds to the ﬁrst training without em-
bedding. Not embedded 2nd corresponds to the second
training without embedding and Embedded corresponds
to the second training with embedding. Figure 4 shows the
training curves of these ﬁne-tunings5. We can see that Em-
bedded achieved almost the same test error as Not embed-
ded 2nd and a very low ER(w).

Table 5 (b) shows the results of the second experiment.
Not embedded 2nd corresponds to the second training
without embedding and Embedded corresponds to the sec-
ond training with embedding. The test error and training
loss of the ﬁrst training are not shown because they are not
compatible between these two different training datasets.
From these results, it was also conﬁrmed that Embedded
achieved almost the same test error as Not embedded 2nd
and very low ER(w). Thus, we can say that the proposed
method is effective even in the ﬁne-tune-to-embed situation
(in the same and different domains).

Finally, embedding a watermark in the distill-to-embed

5Note that the learning rate was also initialized to 0.1 at the beginning
of the second training, while the learning rate was reduced to 8.0 × 10−4)
at the end of the ﬁrst training.

Figure 4. Training curves for ﬁne-tuning the host network. The
ﬁrst and second halves of epochs correspond to the ﬁrst and sec-
ond training. Solid lines denote test error (y-axis on the left) and
dashed lines denote training loss (y-axis on the right).

4.2.4 Fine-tune-to-embed and Distill-to-embed

In the above experiments, a watermark was embedded by
training the host network from scratch (train-to-embed).
Here, we evaluated the other two situations introduced in
Section 2.2: ﬁne-tune-to-embed and distill-to-embed. For
In
ﬁne-tune-to-embed, two experiments were performed.
the ﬁrst experiment, the host network was trained on the
CIFAR-10 dataset without embedding, and then ﬁne-tuned

6

Table 5. Test error (%) and embedding loss ER(w) with and with-
out embedding in ﬁne-tuning and distilling.

Table 6. Test error (%) and embedding loss ER(w) for the combi-
nations of embedded groups and sizes of embedded bits.

(a) Fine-tune-to-embed (CIFAR-10 → CIFAR-10)

Not embedded 1st
Not embedded 2nd
Embedded

Test error
8.04
7.66
7.70

ER(w)
N/A
N/A
4.93×10−4

(b) Fine-tune-to-embed (Caltech-101 → CIFAR-10)

ER(w)
N/A
4.83×10−4
(c) Distill-to-embed (CIFAR-10 → CIFAR-10)

Not embedded 2nd
Embedded

Test error
7.93
7.94

Not embedded 1st
Not embedded 2nd
Embedded

Test error
8.04
7.86
7.75

ER(w)
N/A
N/A
5.01×10−4

situation was evaluated. The host network is ﬁrst trained
on the CIFAR-10 dataset without embedding. Then, the
trained network was further ﬁne-tuned on the same CIFAR-
In this second
10 dataset with and without embedding.
training, the training labels of the CIFAR-10 dataset were
not used. Instead, the predicted values of the trained net-
work were used as soft targets [17]. In other words, no label
was used in the second training. Table 5 (c) shows the re-
sults for the distill-to-embed situation. Not embedded 1st
corresponds to the ﬁrst training and Embedded (Not em-
bedded 2nd) corresponds to the second distilling training
with embedding (without embedding). It was found that the
proposed method also achieved low test error and ER(w) in
the distill-to-embed situation.

4.2.5 Capacity of Watermark.

In this section, the capacity of the embedded watermark is
explored by embedding different sizes of watermarks into
different groups in the train-to-embed manner. Please note
that the number of parameters w of conv 2, conv 3, and
conv 4 groups were 576, 1152, and 2304, respectively. Ta-
ble 6 shows test error (%) and embedding loss ER(w) for
combinations of different embedded blocks and different
number of embedded bits. We can see that embedded loss or
test error becomes high if the number of embedded bits be-
comes larger than the number of parameters w (e.g. 2,048
bits in conv 3) because the embedding problem becomes
overdetermined in such cases. Thus, the number of embed-
ded bits should be smaller than the number of parameters
w, which is a limitation of the embedding method using a
single-layer perceptron. This limitation would be resolved
by using a multi-layer perceptron in the embedding regular-
izer.

Embedded bits

(a) Test error (%)

Embedded group
conv 3
7.98
8.22
8.12
8.93

conv 2
7.97
8.47
8.43
8.17
(b) Embedding loss

conv 4
7.92
7.84
7.84
7.75

256
512
1,024
2,048

Embedded bits

256
512
1,024
2,048

conv 2
4.76×10−4
8.11×10−4
6.74×10−2
5.35×10−1

Embedded group
conv 3
7.20×10−4
8.18×10−4
1.53×10−3
3.70×10−2

conv 4
1.10×10−2
1.25×10−2
1.53×10−2
3.06×10−2

Table 7. Losses, test error, and bit error rate (BER) after embed-
ding a watermark with different λ.

λ
0
1
10
100

1

2 ||w − w0||2
0.000
0.184
1.652
7.989

2 ER(w) Test error

1.066
0.609
0.171
0.029

8.04
8.52
10.57
13.00

BER
0.531
0.324
0.000
0.000

4.2.6 Embedding without Training

As mentioned in Section 3.2, it is possible to embed a wa-
termark to a host network by directly modifying the trained
parameter w0 as usually done in image domain. Here we try
to do this by minimizing the following loss function instead
of Eq. (1):

E(w) = 1

2 ||w − w0||2

2 + λER(w),

(6)

where the embedding loss ER(w) is minimized while min-
imizing the difference between the modiﬁed parameter w
and the original parameter w0. Table 7 summarizes the em-
bedding results after Eq. (6) against the host network trained
on the CIFAR-10 dataset. We can see that embedding fails
for λ ≤ 1 as bit error rate (BER) is larger than zero while the
test error of the original task becomes too large for λ > 1.
Thus, it is not effective to directly embed a watermark with-
out considering the original task.

4.3. Robustness of Embedded Watermarks

In this section, the robustness of a proposed watermark is
evaluated for the three attack types explained in Section 2.3:
ﬁne-tuning and model compression.

4.3.1 Robustness against Fine-tuning

Fine-tuning or transfer learning [28] seems to be the most
likely type of (unintentional) attack because it is frequently

7

Table 8. Embedding loss before ﬁne-tuning (ER(w)) and after
ﬁne-tuning (E(cid:48)
R(w)), and the best test error (%) after ﬁne-tuning.

CIFAR-10 → CIFAR-10
Caltech-101 → CIFAR-10

ER(w)
4.76×10−4
5.96×10−3

E(cid:48)

R(w)
8.66×10−4
1.56×10−2

Test error
7.69
7.88

performed on trained models to apply them to other but
similar tasks with less effort than training a network from
scratch or to avoid over-ﬁtting when sufﬁcient training data
is not available.

In this experiment, two trainings we performed; in the
ﬁrst training, a 256-bit watermark was embedded in the
conv 2 group in the train-to-embed manner, and then the
host network was further ﬁne-tuned in the second training
without embedding, to determine whether the watermark
embedded in the ﬁrst training stayed in the host network
or not, even after the second training (ﬁne-tuning).

Table 8 shows the embedding loss before ﬁne-tuning
(ER(w)) and after ﬁne-tuning (E(cid:48)
R(w)), and the best test er-
ror after ﬁne-tuning. We evaluated ﬁne-tuning in the same
domain (CIFAR-10 → CIFAR-10) and in different domains
(Caltech-101 → CIFAR-10). We can see that, in both cases,
the embedding loss was slightly increased by ﬁne-tuning
but was still low. In addition, the bit error rate of the de-
tected watermark was equal to zero in both cases. The rea-
son why the embedding loss in ﬁne-tuning in the different
domains is higher than that in the same domain is that the
Caltech-101 dataset is signiﬁcantly more difﬁcult than the
CIFAR-10 dataset in our settings; all images in the Caltech-
101 dataset were resized to 32 × 326 for compatibility with
the CIFAR-10 dataset.

4.3.2 Robustness against Model Compression

It is sometimes difﬁcult to deploy deep neural networks to
embedded systems or mobile devices because they are both
computationally intensive and memory intensive. In order
to solve this problem, the model parameters are often com-
pressed [14, 12, 13]. The compression of model parameters
can intentionally or unintentionally act as an attack against
watermarks. In this section, we evaluate the robustness of
our watermarks against model compression, in particular,
against parameter pruning [14]. In parameter pruning, pa-
rameters whose absolute values are very small are cut-off to
zero. In [13], quantization of weights and the Huffman cod-
ing of quantized values are further applied. Because quanti-
zation has less impact than parameter pruning and the Huff-
man coding is lossless compression, we focus on parameter
pruning.

In order to evaluate the robustness against parameter
pruning, we embedded a 256-bit watermark in the conv

6This size is extremely small compared with their original sizes

(roughly 300 × 200).

8

2 group while training the host network on the CIFAR-10
dataset. We removed α% of the 3 × 3 × 64 × 64 parameters
of the embedded layer and calculated embedding loss and
bit error rate. Figure 5 (a) shows embedding loss ER(w)
as a function of pruning rate α. Ascending (Descending)
represents embedding loss when the top α% parameters are
cut-off according to their absolute values in ascending (de-
scending) order. Random represents embedding loss where
α% of parameters are randomly removed. Ascending cor-
responds to parameter pruning and the others were evalu-
ated for comparison. We can see that the embedding loss of
Ascending increases more slowly than those of Descend-
ing and Random as α increases. It is reasonable that model
parameters with small absolute values have less impact on
a detected watermark because the watermark is extracted
from the dot product of the model parameter w and the con-
stant embedding parameter (weight) X.

Figure 5 (b) shows the bit error rate as a function of prun-
ing rate α. Surprisingly, the bit error rate was still zero after
removing 65% of the parameters and 2/256 even after 80%
of the parameters were pruned (Ascending). We can say
that the embedded watermark is sufﬁciently robust against
parameter pruning because, in [13], the resulting pruning
rate of convolutional layers ranged from to 16% to 65% for
the AlexNet [23], and from 42% to 78% for VGGNet [28].
Furthermore, this degree of bit error can be easily corrected
by an error correction code (e.g. the BCH code). Figure 6
shows the histogram of the detected watermark σ(ΣiXjiwi)
after pruning for α = 0.8 and 0.95. For α = 0.95, the his-
togram of the detected watermark is also shown for the host
network into which no watermark is embedded. We can see
that many of σ(ΣiXjiwi) are still close to one for the em-
bedded case, which might be used as a conﬁdence score in
judging the existence of a watermark (zero-bit watermark-
ing).

5. Conclusions and Future Work

In this paper, we have proposed a general framework
for embedding a watermark in deep neural network models
to protect the rights to the trained models. First, we for-
mulated a new problem: embedding watermarks into deep
neural networks. We also deﬁned requirements, embedding
situations, and attack types for watermarking deep neural
networks. Second, we proposed a general framework for
embedding a watermark in model parameters using a pa-
rameter regularizer. Our approach does not impair the per-
formance of networks into which a watermark is embed-
ded. Finally, we performed comprehensive experiments to
reveal the potential of watermarking deep neural networks
as the basis of this new problem. We showed that our frame-
work could embed a watermark without impairing the per-
formance of a deep neural network. The embedded water-
mark did not disappear even after ﬁne-tuning or parameter

(a) Embedding loss.

Figure 6. Histogram of the detected watermark σ(ΣiXjiwi) after
pruning.

against parameter pruning in this paper, a watermark might
be embedded in conjunction with compressing models. For
example, in [13], after parameter pruning, the network is
re-trained to learn the ﬁnal weights for the remaining sparse
parameters. Our embedding regularizer can be used in this
re-training to embed a watermark.

Network morphism. In [4, 32], a systematic study has
been done on how to morph a well-trained neural network
into a new one so that its network function can be com-
pletely preserved for further training. This network mor-
phism can constitute a severe attack against our watermark
because it may be impossible to detect the embedded water-
mark if the topology of the host network is severely modi-
ﬁed. We left the investigation how the embedded watermark
is affected by this network morphism for future work.

Steganalysis. Steganalysis [27, 21] is a method for de-
tecting the presence of secretly hidden data (e.g. steganog-
raphy or watermarks) in digital media ﬁles such as images,
video, audio, and, in our case, deep neural networks. Water-
marks ideally are robust against steganalysis. While, in this
paper, we conﬁrmed that embedding watermarks does not
signiﬁcantly change the distribution of model parameters,
more exploration is needed to evaluate robustness against
steganalysis. Conversely, developing effective steganalysis
against watermarks for deep neural networks can be an in-
teresting research topic.

Fingerprinting. Digital ﬁngerprinting is an alternative
to the watermarking approach for persistent identiﬁcation
of images [2], video [20, 31], and audio clips [1, 11]. In
this paper, we focused on one of these two important ap-
proaches. Robust ﬁngerprinting of deep neural networks is
another and complementary direction to protect deep neural
network models.

(b) Bit error rate.
Figure 5. Embedding loss and bit error rate after pruning as a func-
tion of pruning rate.

pruning; the entire watermark remained even after 65% of
the parameters were pruned.

5.1. Future Work

Although we have obtained ﬁrst insight into the new
problem of embedding a watermark in deep neural net-
works, many things remain as future work.

Watermark overwriting. A third-party user may em-
bed a different watermark in order to overwrite the origi-
nal watermark.
In our preliminary experiments, this wa-
termark overwriting caused 30.9%, 8.6%, and 0.4% bit er-
rors against watermarks in the conv 2, conv 3, and conv 4
groups when 256-bit watermarks were additionally embed-
ded. More robust watermarking against overwriting should
be explored (e.g. non-linear embedding).

Compression as embedding. Compressing deep neu-
ral networks is a very important and active research topic.
While we conﬁrmed that our watermark is very robust

9

[20] A. Joly, C. Frelicot, and O. Buisson. Content-based video
copy detection in large databases: a local ﬁngerprints sta-
tistical similarity search approach. In Proc. of ICIP, pages
505–508, 2005.

[21] J. Kodovsky, J. Fridrich, and V. Holub. Ensemble classiﬁers
for steganalysis of digital media. IEEE Trans. on Information
Forensics and Security, 7(2):432–444, 2012.

[22] A. Krizhevsky. Learning multiple layers of features from

tiny images. Tech Report, 2009.

[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
Proc. of NIPS, 2012.

Imagenet
In

[24] A. Krogh and J. A. Hertz. A simple weight decay can im-

prove generalization. In Proc. of NIPS, 1992.

[25] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998.

[26] M. Abadi, et al. Tensorﬂow: Large-scale machine learning
on heterogeneous distributed systems. arXiv:1603.04467,
2016.

[27] L. Shaohui, Y. Hongxun, and G. Wen. Neural network based
steganalysis in still images. In Proc. of ICME, 2003.
[28] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In Proc. of ICLR,
2015.

[29] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In Proc. of CVPR, 2015.

[30] S. Tokui, K. Oono, S. Hido, and J. Clayton. Chainer: a
next-generation open source framework for deep learning.
In Proc. of NIPS Workshop on Machine Learning Systems,
2015.

[31] Y. Uchida, M. Agrawal, and S. Sakazawa. Accurate content-
based video copy detection with efﬁcient feature indexing.
In Proc. of ICMR, 2011.

[32] T. Wei, C. Wang, Y. Rui, and C. W. Chen. Network mor-

phism. In Proc. of ICML, 2016.

[33] S. Zagoruyko and N. Komodakis. Wide residual networks.

In Proc. of ECCV, 2016.

References

[1] X. Anguera, A. Garzon, and T. Adamek. Mask: Robust local
features for audio ﬁngerprinting. In Proc. of ICME, 2012.
[2] J. Barr, B. Bradley, and B. T. Hannigan. Using digital wa-
termarks with image signatures to mitigate the threat of the
copy attack. In Proc. of ICASSP, pages 69–72, 2003.

[3] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu,
G. Desjardins, J. Turian, D. Warde-Farley, and Y. Bengio.
Theano: a CPU and GPU math expression compiler. In Proc.
of the Python for Scientiﬁc Computing Conference (SciPy),
2010.

[4] T. Chen, I. Goodfellow, and J. Shlens. Net2net: Accelerating
learning via knowledge transfer. In Proc. of ICLR, 2016.

[5] F. Chollet. Keras. GitHub repository, 2015.
[6] A. Choromanska, M. Henaff, M. Mathieu, G. Arous, and
Y. LeCun. The loss surfaces of multilayer networks. In Proc.
of AISTATS, 2015.

[7] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A
matlab-like environment for machine learning. In Proc. of
NIPS Workshop on BigLearn, 2011.

[8] I. Cox, M. Miller, J. Bloom, J. Fridrich, and T. Kalker. Dig-
ital Watermarking and Steganography. Morgan Kaufmann
Publishers Inc., 2 edition, 2008.

[9] Y. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli,
Identifying and attacking the saddle point
In

and Y. Bengio.
problem in high-dimensional non-convex optimization.
Proc. of NIPS, 2014.

[10] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative
visual models from few training examples: an incremen-
tal bayesian approach tested on 101 object categories.
In
Proc. of CVPR Workshop on Generative-Model Based Vi-
sion, 2004.

[11] J. Haitsma and T. Kalker. A highly robust audio ﬁngerprint-
ing system. In Proc. of ISMIR, pages 107–115, 2002.
[12] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz,
and W. J. Dally. Eie: Efﬁcient inference engine on com-
pressed deep neural network. In Proc. of ISCA, 2016.
[13] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-
pressing deep neural networks with pruning, trained quanti-
zation and huffman coding. In Proc. of ICLR, 2016.

[14] S. Han, J. Pool, J. Tran, and W. J. Dally. Learning both
weights and connections for efﬁcient neural networks.
In
Proc. of NIPS, 2015.

[15] F. Hartung and M. Kutter. Multimedia watermarking tech-

niques. Proceedings of the IEEE, 87(7):1079–1107, 1999.

[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In Proc. of CVPR, 2016.

[17] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
In Proc. of NIPS Workshop on Deep

in a neural network.
Learning and Representation Learning, 2014.

[18] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural Computation, 9(8):1735–1780, 1997.

[19] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
In Proc. of MM,
architecture for fast feature embedding.
2014.

10

7
1
0
2
 
r
p
A
 
0
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
8
0
4
0
.
1
0
7
1
:
v
i
X
r
a

Embedding Watermarks into Deep Neural Networks

Yusuke Uchida
KDDI Research, Inc.
Saitama, Japan

Yuki Nagai
KDDI Research, Inc.
Saitama, Japan

Shigeyuki Sakazawa
Saitama Research, Inc.
Tokyo, Japan

Shin’ichi Satoh
National Institute of Informatics
Tokyo, Japan

Abstract

Signiﬁcant progress has been made with deep neural net-
works recently. Sharing trained models of deep neural net-
works has been a very important in the rapid progress of
research and development of these systems. At the same
time, it is necessary to protect the rights to shared trained
models. To this end, we propose to use digital watermarking
technology to protect intellectual property and detect intel-
lectual property infringement in the use of trained models.
First, we formulate a new problem: embedding watermarks
into deep neural networks. We also deﬁne requirements,
embedding situations, and attack types on watermarking in
deep neural networks. Second, we propose a general frame-
work for embedding a watermark in model parameters, us-
ing a parameter regularizer. Our approach does not im-
pair the performance of networks into which a watermark
is placed because the watermark is embedded while train-
ing the host network. Finally, we perform comprehensive
experiments to reveal the potential of watermarking deep
neural networks as the basis of this new research effort. We
show that our framework can embed a watermark during
the training of a deep neural network from scratch, and dur-
ing ﬁne-tuning and distilling, without impairing its perfor-
mance. The embedded watermark does not disappear even
after ﬁne-tuning or parameter pruning; the watermark re-
mains complete even after 65% of parameters are pruned.

1. Introduction

Deep neural networks have recently been signiﬁcantly
improved. In particular, deep convolutional neural networks
(DCNN) such as LeNet [25], AlexNet [23], VGGNet [28],
GoogLeNet [29], and ResNet [16] have become de facto
standards for object recognition, image classiﬁcation, and
retrieval. In addition, many deep learning frameworks have

1

been released that help engineers and researchers to develop
systems based on deep learning or do research with less
effort. Examples of these great deep learning frameworks
are Caffe [19], Theano [3], Torch [7], Chainer [30], Tensor-
Flow [26], and Keras [5].

Although these frameworks have made it easy to utilize
deep neural networks in real applications, the training of
deep neural network models is still a difﬁcult task because it
requires a large amount of data and time; several weeks are
needed to train a very deep ResNet with the latest GPUs on
the ImageNet dataset for instance [16]. Therefore, trained
models are sometimes provided on web sites to make it
easy to try a certain model or reproduce the results in re-
search articles without training. For example, Model Zoo1
provides trained Caffe models for various tasks with useful
utility tools. Fine-tuning or transfer learning [28] is a strat-
egy to directly adapt such already trained models to another
application with minimum re-training time. Thus, sharing
trained models is very important in the rapid progress of
both research and development of deep neural network sys-
tems.
In the future, more systematic model-sharing plat-
forms may appear, by analogy with video sharing sites. Fur-
thermore, some digital distribution platforms for purchase
and sale of the trained models or even artiﬁcial intelligence
skills (e.g. Alexa Skills2) may appear, similar to Google
Play or App Store.
In these situations, it is necessary to
protect the rights to shared trained models.

To this end, we propose to utilize digital watermarking
technology, which is used to identify ownership of the copy-
right of digital content such as images, audio, and videos. In
particular, we propose a general framework to embed a wa-
termark in deep neural networks models to protect intellec-
tual property and detect intellectual property infringement
of trained models. To the best of our knowledge, this is ﬁrst
attempt to embed a watermark in a deep neural network.

1https://github.com/BVLC/caffe/wiki/Model-Zoo
2https://www.alexaskillstore.com/

The contributions of this research are three-fold, as follows:

1. We formulate a new problem: embedding watermarks
in deep neural networks. We also deﬁne requirements,
embedding situations, and attack types for watermark-
ing deep neural networks.

2. We propose a general framework to embed a water-
mark in model parameters, using a parameter regular-
izer. Our approach does not impair the performance of
networks in which a watermark is embedded.

3. We perform comprehensive experiments to reveal the

potential of watermarking deep neural networks.

2. Problem Formulation

Given a model network with or without trained parame-
ters, we deﬁne the task of watermark embedding as to em-
bed T -bit vector b ∈ {0, 1}T into the parameters of one or
more layers of the neural network. We refer to a neural net-
work in which a watermark is embedded as a host network,
and refer to the task that the host network is originally trying
to perform as the original task.

In the following, we formulate (1) requirements for an
embedded watermark or an embedding method, (2) embed-
ding situations, and (3) expected attack types against which
embedded watermarks should be robust.

2.1. Requirements

Table 1 summarizes the requirements for an effective wa-
termarking algorithm in an image domain [15, 8] and a neu-
ral network domain. While both domains share almost the
same requirements, ﬁdelity and robustness are different in
image and neural network domains.

For ﬁdelity in an image domain, it is essential to main-
tain the perceptual quality of the host image while embed-
ding a watermark. However, in a neural network domain,
the parameters themselves are not important. Instead, the
performance of the original task is important. Therefore it
is essential to maintain the performance of the trained host
network, and not to hamper the training of a host network.
Regarding robustness, as images are subject to vari-
ous signal processing operations, an embedded watermark
should stay in the host image even after these operations.
And the greatest possible modiﬁcation to a neural network
is ﬁne-tuning or transfer learning [28]. An embedded wa-
termark in a neural network should be detectable after ﬁne-
tuning or other possible modiﬁcations.

2.2. Embedding Situations

We classify the embedding situations into three types:
train-to-embed, ﬁne-tune-to-embed, and distill-to-embed,
as summarized in Table 2.

Train-to-embed is the case in which the host network is
trained from scratch while embedding a watermark where
labels for training data are available.

Fine-tune-to-embed is the case in which a watermark is
embedded while ﬁne-tuning. In this case, model parame-
ters are initialized with a pre-trained network. The network
conﬁguration near the output layer may be changed before
ﬁne-tuning.

Distill-to-embed is the case in which a watermark is
embedded into a trained network without labels using the
distilling approach [17]. Embedding is performed in ﬁne-
tuning where the predictions of the trained model are used
as labels. In the standard distill framework, a large network
(or multiple networks) is ﬁrst trained and then a smaller net-
work is trained using the predicted labels of the large net-
work in order to compress the large network. In this paper,
we use the distill framework simply to train a network with-
out labels.

The ﬁrst two situations assume that the copyright holder
of the host network is expected to embed a watermark to
the host network in training or ﬁne-tuning. Fine-tune-to-
embed is also useful when a model owner wants to embed
individual watermarks to identify those to whom the model
had been distributed. By doing so, individual instances can
be tracked. The last situation assumes that a non-copyright
holder (e.g., a platformer) is entrusted to embed a water-
mark on behalf of a copyright holder.

2.3. Expected Attack Types

Related to the requirement for robustness in Section 2.1,
we assume two types of attacks against which embedded
watermarks should be robust: ﬁne-tuning and model com-
pression. These types of attack are very speciﬁc to deep
neural networks, while one can easily imagine model com-
pression by analogy with lossy image compression in the
image domain.

Fine-tuning or transfer learning [28] seems to be the
most feasible type of attack, because it reduces the burden
of training deep neural networks. Many models have been
constructed on top of existing state-of-the-art models. Fine-
tuning alters the model parameters, and thus embedded wa-
termarks should be robust against this alteration.

Model compression is very important in deploying deep
neural networks to embedded systems or mobile devices
as it can signiﬁcantly reduce memory requirements and/or
computational cost. Lossy compression distorts model pa-
rameters, so we should explore how it affects the detection
rate.

3. Proposed Framework

In this section, we propose a framework for embedding
a watermark into a host network. Although we focus on a

2

Table 1. Requirements for an effective watermarking algorithm in the image and neural network domains.

Fidelity

Robustness

Capacity

Security

Efﬁciency

Neural networks domain
The effectiveness of the host network should
not be degraded by embedding a watermark.
The embedded watermark should be robust
against model modiﬁcations such as ﬁne-
tuning and model compression.

Image domain
The quality of the host image should not be
degraded by embedding a watermark.
The embedded watermark should be robust
against common signal processing operations
such as lossy compression, cropping, resiz-
ing, and so on.
The effective watermarking system must have the ability to embed a large amount of in-
formation.
A watermark should in general be secret and should not be accessed, read, or modiﬁed by
unauthorized parties.
The watermark embedding and extraction processes should be fast.

Table 2. Three embedding situations. Fine-tune indicates whether
parameters are initialized in embedding using already trained mod-
els, or not. Label availability indicates whether or labels for train-
ing data are available in embedding.

Fine-tune Label availability

Train-to-embed
Fine-tune-to-embed
Distill-to-embed

(cid:88)
(cid:88)

(cid:88)
(cid:88)

DCNN [25] as the host, our framework is essentially ap-
plicable to other networks such as standard multilayer per-
ceptron (MLP), recurrent neural network (RNN), and long
short-term memory (LSTM) [18].

3.1. Embedding Targets

In this paper, a watermark is assumed to be embedded
into one of the convolutional layers in a host DCNN3. Let
(S, S), D, and L respectively denote the size of the con-
volution ﬁlter, the depth of input to the convolutional layer,
and the number of ﬁlters in the convolutional layer. The pa-
rameters of this convolutional layer are characterized by the
tensor W ∈ RS×S×D×L. The bias term is ignored here.
Let us think of embedding a T -bit vector b ∈ {0, 1}T into
W . The tensor W is a set of L convolutional ﬁlters and the
order of the ﬁlters does not affect the output of the network
if the parameters of the subsequent layers are appropriately
re-ordered. In order to remove this arbitrariness in the or-
der of ﬁlters, we calculate the mean of W over L ﬁlters as
l Wijkl. Letting w ∈ RM (M = S × S × D)
W ijk = 1
L
denote a ﬂattened version of W , our objective is now to em-
bed T -bit vector b into w.

(cid:80)

It is possible to embed a watermark in a host network
by directly modifying w of a trained network, as is usu-

3Fully-connected layers can also be used but we focus on convolu-
tional layers here, because fully-connected layers are often discarded in
ﬁne-tuning.

ally done in the image domain. However, this approach de-
grades the performance of the host network in the original
task as shown later in Section 4.2.6. Instead, we propose
embedding a watermark while training a host network for
the original task so that the existence of the watermark does
not impair the performance of the host network in its orig-
inal task. To this end, we utilize a parameter regularizer,
which is an additional term in the original cost function for
the original task. The cost function E(w) with a regularizer
is deﬁned as:

E(w) = E0(w) + λER(w),

(1)

where E0(w) is the original cost function, ER(w) is a regu-
larization term that imposes a certain restriction on param-
eters w, and λ is an adjustable parameter. A regularizer is
usually used to prevent the parameters from growing too
large. L2 regularization (or weight decay [24]), L1 regu-
larization, and their combination are often used to reduce
over-ﬁtting of parameters for complex neural networks. For
instance, ER(w) = ||w||2

2 in the L2 regularization.

In contrast to these standard regularizers, our regularizer
imposes parameter w to have a certain statistical bias, as a
watermark in a training process. We refer to this regular-
izer as an embedding regularizer. Before deﬁning the em-
bedding regularizer, we explain how to extract a watermark
from w. Given a (mean) parameter vector w ∈ RM and an
embedding parameter X ∈ RT ×M , the watermark extrac-
tion is simply done by projecting w using X, followed by
thresholding at 0. More precisely, the j-th bit is extracted
as:

bj = s(ΣiXjiwi),

s(x) =

(cid:40)

1 x ≥ 0
else.
0

(2)

(3)

This process can be considered to be a binary classiﬁca-

3.2. Embedding Regularizer

where s(x) is a step function:

3

tion problem with a single-layer perceptron (without bias)4.
Therefore, it is straightforward to deﬁne the loss function
ER(w) for the embedding regularizer by using binary cross
entropy:

with random weights. These three types of embedding pa-
rameters are compared in experiments.

Our implementation of the embedding regularizer is pub-
licly available from https://github.com/yu4u/
dnn-watermark.

ER(w) = −

(bj log(yj) + (1 − bj) log(1 − yj)) , (4)

4. Experiments

T
(cid:88)

j=1

In this section, we demonstrate that our embedding reg-
ularizer can embed a watermark without impairing the per-
formance of the host network, and the embedded watermark
is robust against various types of attack.

(5)

where yj = σ(ΣiXjiwi) and σ(·) is the sigmoid function:

σ(x) =

1
1 + exp(−x)

.

We call this loss function an embedding loss function. It
may be confusing that an embedding loss function is used
to update w, not X, in our framework. In a standard percep-
tron, w is an input and X is a parameter to be learned. In our
case, w is an embedding target and X is a ﬁxed parameter.
The design of X is discussed in the following section.

This approach does not impair the performance of the
host network in the original task as conﬁrmed in exper-
iments, because deep neural networks are typically over-
parameterized. It is well-known that deep neural networks
have many local minima, and that all local minima are
likely to have an error very close to that of the global min-
imum [9, 6]. Therefore, the embedding regularizer only
needs to guide model parameters to one of a number of good
local minima so that the ﬁnal model parameters have an ar-
bitrary watermark.

3.3. Regularizer Parameters

In this section we discuss the design of the embed-
ding parameter X, which can be considered as a secret
key [15] in detecting and embedding watermarks. While
X ∈ RT ×M can be an arbitrary matrix, it will affect the
performance of an embedded watermark because it is used
in both embedding and extraction of watermarks.
In this
paper, we consider three types of X: X direct, X diff, and
X random.

= 1.

X direct is constructed so that one element in each row of
X direct is ’1’ and the others are ’0’. In this case, the j-th
bit bj is directly embedded in a certain parameter wˆi s.t.
X direct
jˆi
X diff is created so that each row has one ’1’ element and
one ’-1’ element, and the others are ’0’. Using X diff, the
j-th bit bj is embedded into the difference between wi+ and
wi− where X diff
ji+

= 1 and X diff
ji−

= −1.

Each element of X random is independently drawn from
the standard normal distribution N (0, 1). Using X random,
each bit is embedded into all instances of the parameter w

4Although this single-layer perceptron can be deepened into multi-

layer perceptron, we focus on the simplest one in this paper.

4.1. Evaluation Settings

Datasets. For experiments, we used the well-known
CIFAR-10 and Caltech-101 datasets.
The CIFAR-10
dataset [22] consists of 60,000 32 × 32 color images in 10
classes, with 6,000 images per class. These images were
separated into 50,000 training images and 10,000 test im-
ages. The Caltech-101 dataset [10] includes pictures of ob-
jects belonging to 101 categories; it contains about 40 to
800 images per category. The size of each image is roughly
300 × 200 pixels but we resized them in 32 × 32 for ﬁne-
tuning. For each category, we used 30 images for training
and at most 40 of the remaining images for testing.

Host network and training settings. We used the
wide residual network [33] as the host network. The wide
residual network is an efﬁcient variant of the residual net-
work [16]. Table 3 shows the structure of the wide residual
network with a depth parameter N and a width parameter
k. In all our experiments, we set N = 1 and k = 4, and
used SGD with Nesterov momentum and cross-entropy loss
in training. The initial learning rate was set at 0.1, weight
decay to 5.0×10−4, momentum to 0.9 and minibatch size
to 64. The learning rate was dropped by a factor of 0.2 at
60, 120 and 160 epochs, and we trained for a total of 200
epochs, following the settings used in [33].

We embedded a watermark into one of the following
the second convolution layer in the
convolution layers:
conv 2, conv 3, and conv 4 groups. In the following, we
mention the location of the host layer by simply describing
the conv 2, conv 3, or conv 4 group. In Table 3, the num-
ber M of parameter w is also shown for these layers. The
parameter λ in Eq. (1) is set to 0.01. As a watermark, we
embedded b = 1 ∈ {0, 1}T in the following experiments.

4.2. Embedding Results

First, we conﬁrm that a watermark was successfully em-
bedded in the host network by the proposed embedding reg-
ularizer. We trained the host network from scratch (train-to-
embed) on the CIFAR-10 dataset with and without embed-
ding a watermark. In the embedding case, a 256-bit water-
mark (T = 256) was embedded into the conv 2 group.

4

Table 3. Structure of the host network.

Group Output size
conv 1

32 × 32

conv 2

32 × 32

#w
N/A

× N 144 × k

Building block
[3 × 3, 16]
(cid:21)
(cid:20)3 × 3, 16 × k
3 × 3, 16 × k
(cid:20)3 × 3, 32 × k
3 × 3, 32 × k
(cid:20)3 × 3, 64 × k
3 × 3, 64 × k

(cid:21)

(cid:21)

conv 3

16 × 16

× N 288 × k

conv 4

8 × 8

1 × 1

× N 576 × k

avg-pool, fc, soft-max

N/A

Table 4 shows the best test errors and embedding losses
ER(w) of the host networks with and without embedding.
We can see that the test errors of Not embedded and ran-
dom are almost the same while those of direct and diff
are slightly larger. The embedding loss ER(w) of ran-
dom is extremely low compared with those of direct and
diff. These results indicate that the random approach can
effectively embed a watermark without impairing the per-
formance in the original task.

4.2.2 Detecting Watermarks

Figure 2 shows the histogram of the embedded watermark
σ(ΣiXjiwi) (before thresholding) with and without wa-
termarks where (a) direct, (b) diff, and (c) random pa-
rameters are used in embedding and detection.
If we bi-
narize σ(ΣiXjiwi) at a threshold of 0.5, all watermarks
are correctly detected because ∀j, σ(ΣiXjiwi) ≥ 0.5
(⇔ ΣiXjiwi ≥ 0) for all embedded cases. Please note
that we embedded b = 1 ∈ {0, 1}T as a watermark as men-
tioned before. Although random watermarks will be de-
tected for the non-embedded cases, it can be easily judged
that the watermark is not embedded because the distribution
of σ(ΣiXjiwi) is quite different from those for embedded
cases.

4.2.3 Distribution of Model Parameters

We explore how trained model parameters are affected by
the embedded watermarks. Figure 3 shows the distribution
of model parameters W (not w) with and without water-
marks. These parameters are taken only from the layer in
which a watermark was embedded. Note that W is the pa-
rameter before taking the mean over ﬁlters, and thus the
number of parameters is 3 × 3 × 64 × 64. We can see that
direct and diff signiﬁcantly alter the distribution of param-
eters while random does not. In direct, many parameters
became large and a peak appears near 2 so that their mean
over ﬁlters becomes a large positive value to reduce the em-
bedding loss. In diff, most parameters were pushed in both
positive and negative directions so that the differences be-
tween these parameters became large. In random, a water-
mark is diffused over all parameters with random weights
and thus does not signiﬁcantly alter the distribution. This is
one of the desirable properties of watermarking related the
security requirement; one may be aware of the existence of
the embedded watermarks for the direct and diff cases.

The results so far indicated that the random approach
seemed to be the best choice among the three, with low em-
bedding loss, low test error in the original task, and not al-
tering the parameter distribution. Therefore, in the follow-
ing experiments, we used the random approach in embed-
ding watermarks without explicitly indicating it.

Figure 1. Training curves for the host network on CIFAR-10 as a
function of epochs. Solid lines denote test error (y-axis on the left)
and dashed lines denote training loss E(w) (y-axis on the right).

Table 4. Test error (%) and embedding loss ER(w) with and with-
out embedding.

Not embedded
direct
diff
random

Test error
8.04
8.21
8.37
7.97

ER(w)
N/A
1.24×10−1
6.20×10−2
4.76×10−4

4.2.1 Test Error and Training Loss

Figure 1 shows the training curves for the host network in
CIFAR-10 as a function of epochs. Not embedded is the
case that the host network is trained without the embed-
ding regularizer. Embedded (direct), Embedded (diff),
and Embedded (random) respectively represent training
curves with embedding regularizers whose parameters are
X direct, X diff, and X random. We can see that the training
loss E(w) with a watermark becomes larger than the not-
embedded case if the parameters X direct and X diff are used.
This large training loss is dominated by the embedding loss
ER(w), which indicates that it is difﬁcult to embed a water-
mark directly into a parameter or even into the difference of
two parameters. On the other hand, the training loss of Em-
bedded (random) is very close to that of Not embedded.

5

(a) direct

(b) diff
Figure 2. Histogram of the embedded watermark σ(ΣiXjiwi) (before thresholding) with and without watermarks.

(c) random

(a) Not embedded

(b) direct

(c) diff

(d) random

Figure 3. Distribution of model parameters W with and without watermarks.

on the same CIFAR-10 dataset with embedding and with-
out embedding (for comparison). In the second experiment,
the host network is trained on the Caltech-101 dataset, and
then ﬁne-tuned on the CIFAR-10 dataset with and without
embedding.

Table 5 (a) shows the result of the ﬁrst experiment. Not
embedded 1st corresponds to the ﬁrst training without em-
bedding. Not embedded 2nd corresponds to the second
training without embedding and Embedded corresponds
to the second training with embedding. Figure 4 shows the
training curves of these ﬁne-tunings5. We can see that Em-
bedded achieved almost the same test error as Not embed-
ded 2nd and a very low ER(w).

Table 5 (b) shows the results of the second experiment.
Not embedded 2nd corresponds to the second training
without embedding and Embedded corresponds to the sec-
ond training with embedding. The test error and training
loss of the ﬁrst training are not shown because they are not
compatible between these two different training datasets.
From these results, it was also conﬁrmed that Embedded
achieved almost the same test error as Not embedded 2nd
and very low ER(w). Thus, we can say that the proposed
method is effective even in the ﬁne-tune-to-embed situation
(in the same and different domains).

Finally, embedding a watermark in the distill-to-embed

5Note that the learning rate was also initialized to 0.1 at the beginning
of the second training, while the learning rate was reduced to 8.0 × 10−4)
at the end of the ﬁrst training.

Figure 4. Training curves for ﬁne-tuning the host network. The
ﬁrst and second halves of epochs correspond to the ﬁrst and sec-
ond training. Solid lines denote test error (y-axis on the left) and
dashed lines denote training loss (y-axis on the right).

4.2.4 Fine-tune-to-embed and Distill-to-embed

In the above experiments, a watermark was embedded by
training the host network from scratch (train-to-embed).
Here, we evaluated the other two situations introduced in
Section 2.2: ﬁne-tune-to-embed and distill-to-embed. For
In
ﬁne-tune-to-embed, two experiments were performed.
the ﬁrst experiment, the host network was trained on the
CIFAR-10 dataset without embedding, and then ﬁne-tuned

6

Table 5. Test error (%) and embedding loss ER(w) with and with-
out embedding in ﬁne-tuning and distilling.

Table 6. Test error (%) and embedding loss ER(w) for the combi-
nations of embedded groups and sizes of embedded bits.

(a) Fine-tune-to-embed (CIFAR-10 → CIFAR-10)

Not embedded 1st
Not embedded 2nd
Embedded

Test error
8.04
7.66
7.70

ER(w)
N/A
N/A
4.93×10−4

(b) Fine-tune-to-embed (Caltech-101 → CIFAR-10)

ER(w)
N/A
4.83×10−4
(c) Distill-to-embed (CIFAR-10 → CIFAR-10)

Not embedded 2nd
Embedded

Test error
7.93
7.94

Not embedded 1st
Not embedded 2nd
Embedded

Test error
8.04
7.86
7.75

ER(w)
N/A
N/A
5.01×10−4

situation was evaluated. The host network is ﬁrst trained
on the CIFAR-10 dataset without embedding. Then, the
trained network was further ﬁne-tuned on the same CIFAR-
In this second
10 dataset with and without embedding.
training, the training labels of the CIFAR-10 dataset were
not used. Instead, the predicted values of the trained net-
work were used as soft targets [17]. In other words, no label
was used in the second training. Table 5 (c) shows the re-
sults for the distill-to-embed situation. Not embedded 1st
corresponds to the ﬁrst training and Embedded (Not em-
bedded 2nd) corresponds to the second distilling training
with embedding (without embedding). It was found that the
proposed method also achieved low test error and ER(w) in
the distill-to-embed situation.

4.2.5 Capacity of Watermark.

In this section, the capacity of the embedded watermark is
explored by embedding different sizes of watermarks into
different groups in the train-to-embed manner. Please note
that the number of parameters w of conv 2, conv 3, and
conv 4 groups were 576, 1152, and 2304, respectively. Ta-
ble 6 shows test error (%) and embedding loss ER(w) for
combinations of different embedded blocks and different
number of embedded bits. We can see that embedded loss or
test error becomes high if the number of embedded bits be-
comes larger than the number of parameters w (e.g. 2,048
bits in conv 3) because the embedding problem becomes
overdetermined in such cases. Thus, the number of embed-
ded bits should be smaller than the number of parameters
w, which is a limitation of the embedding method using a
single-layer perceptron. This limitation would be resolved
by using a multi-layer perceptron in the embedding regular-
izer.

Embedded bits

(a) Test error (%)

Embedded group
conv 3
7.98
8.22
8.12
8.93

conv 2
7.97
8.47
8.43
8.17
(b) Embedding loss

conv 4
7.92
7.84
7.84
7.75

256
512
1,024
2,048

Embedded bits

256
512
1,024
2,048

conv 2
4.76×10−4
8.11×10−4
6.74×10−2
5.35×10−1

Embedded group
conv 3
7.20×10−4
8.18×10−4
1.53×10−3
3.70×10−2

conv 4
1.10×10−2
1.25×10−2
1.53×10−2
3.06×10−2

Table 7. Losses, test error, and bit error rate (BER) after embed-
ding a watermark with different λ.

λ
0
1
10
100

1

2 ||w − w0||2
0.000
0.184
1.652
7.989

2 ER(w) Test error

1.066
0.609
0.171
0.029

8.04
8.52
10.57
13.00

BER
0.531
0.324
0.000
0.000

4.2.6 Embedding without Training

As mentioned in Section 3.2, it is possible to embed a wa-
termark to a host network by directly modifying the trained
parameter w0 as usually done in image domain. Here we try
to do this by minimizing the following loss function instead
of Eq. (1):

E(w) = 1

2 ||w − w0||2

2 + λER(w),

(6)

where the embedding loss ER(w) is minimized while min-
imizing the difference between the modiﬁed parameter w
and the original parameter w0. Table 7 summarizes the em-
bedding results after Eq. (6) against the host network trained
on the CIFAR-10 dataset. We can see that embedding fails
for λ ≤ 1 as bit error rate (BER) is larger than zero while the
test error of the original task becomes too large for λ > 1.
Thus, it is not effective to directly embed a watermark with-
out considering the original task.

4.3. Robustness of Embedded Watermarks

In this section, the robustness of a proposed watermark is
evaluated for the three attack types explained in Section 2.3:
ﬁne-tuning and model compression.

4.3.1 Robustness against Fine-tuning

Fine-tuning or transfer learning [28] seems to be the most
likely type of (unintentional) attack because it is frequently

7

Table 8. Embedding loss before ﬁne-tuning (ER(w)) and after
ﬁne-tuning (E(cid:48)
R(w)), and the best test error (%) after ﬁne-tuning.

CIFAR-10 → CIFAR-10
Caltech-101 → CIFAR-10

ER(w)
4.76×10−4
5.96×10−3

E(cid:48)

R(w)
8.66×10−4
1.56×10−2

Test error
7.69
7.88

performed on trained models to apply them to other but
similar tasks with less effort than training a network from
scratch or to avoid over-ﬁtting when sufﬁcient training data
is not available.

In this experiment, two trainings we performed; in the
ﬁrst training, a 256-bit watermark was embedded in the
conv 2 group in the train-to-embed manner, and then the
host network was further ﬁne-tuned in the second training
without embedding, to determine whether the watermark
embedded in the ﬁrst training stayed in the host network
or not, even after the second training (ﬁne-tuning).

Table 8 shows the embedding loss before ﬁne-tuning
(ER(w)) and after ﬁne-tuning (E(cid:48)
R(w)), and the best test er-
ror after ﬁne-tuning. We evaluated ﬁne-tuning in the same
domain (CIFAR-10 → CIFAR-10) and in different domains
(Caltech-101 → CIFAR-10). We can see that, in both cases,
the embedding loss was slightly increased by ﬁne-tuning
but was still low. In addition, the bit error rate of the de-
tected watermark was equal to zero in both cases. The rea-
son why the embedding loss in ﬁne-tuning in the different
domains is higher than that in the same domain is that the
Caltech-101 dataset is signiﬁcantly more difﬁcult than the
CIFAR-10 dataset in our settings; all images in the Caltech-
101 dataset were resized to 32 × 326 for compatibility with
the CIFAR-10 dataset.

4.3.2 Robustness against Model Compression

It is sometimes difﬁcult to deploy deep neural networks to
embedded systems or mobile devices because they are both
computationally intensive and memory intensive. In order
to solve this problem, the model parameters are often com-
pressed [14, 12, 13]. The compression of model parameters
can intentionally or unintentionally act as an attack against
watermarks. In this section, we evaluate the robustness of
our watermarks against model compression, in particular,
against parameter pruning [14]. In parameter pruning, pa-
rameters whose absolute values are very small are cut-off to
zero. In [13], quantization of weights and the Huffman cod-
ing of quantized values are further applied. Because quanti-
zation has less impact than parameter pruning and the Huff-
man coding is lossless compression, we focus on parameter
pruning.

In order to evaluate the robustness against parameter
pruning, we embedded a 256-bit watermark in the conv

6This size is extremely small compared with their original sizes

(roughly 300 × 200).

8

2 group while training the host network on the CIFAR-10
dataset. We removed α% of the 3 × 3 × 64 × 64 parameters
of the embedded layer and calculated embedding loss and
bit error rate. Figure 5 (a) shows embedding loss ER(w)
as a function of pruning rate α. Ascending (Descending)
represents embedding loss when the top α% parameters are
cut-off according to their absolute values in ascending (de-
scending) order. Random represents embedding loss where
α% of parameters are randomly removed. Ascending cor-
responds to parameter pruning and the others were evalu-
ated for comparison. We can see that the embedding loss of
Ascending increases more slowly than those of Descend-
ing and Random as α increases. It is reasonable that model
parameters with small absolute values have less impact on
a detected watermark because the watermark is extracted
from the dot product of the model parameter w and the con-
stant embedding parameter (weight) X.

Figure 5 (b) shows the bit error rate as a function of prun-
ing rate α. Surprisingly, the bit error rate was still zero after
removing 65% of the parameters and 2/256 even after 80%
of the parameters were pruned (Ascending). We can say
that the embedded watermark is sufﬁciently robust against
parameter pruning because, in [13], the resulting pruning
rate of convolutional layers ranged from to 16% to 65% for
the AlexNet [23], and from 42% to 78% for VGGNet [28].
Furthermore, this degree of bit error can be easily corrected
by an error correction code (e.g. the BCH code). Figure 6
shows the histogram of the detected watermark σ(ΣiXjiwi)
after pruning for α = 0.8 and 0.95. For α = 0.95, the his-
togram of the detected watermark is also shown for the host
network into which no watermark is embedded. We can see
that many of σ(ΣiXjiwi) are still close to one for the em-
bedded case, which might be used as a conﬁdence score in
judging the existence of a watermark (zero-bit watermark-
ing).

5. Conclusions and Future Work

In this paper, we have proposed a general framework
for embedding a watermark in deep neural network models
to protect the rights to the trained models. First, we for-
mulated a new problem: embedding watermarks into deep
neural networks. We also deﬁned requirements, embedding
situations, and attack types for watermarking deep neural
networks. Second, we proposed a general framework for
embedding a watermark in model parameters using a pa-
rameter regularizer. Our approach does not impair the per-
formance of networks into which a watermark is embed-
ded. Finally, we performed comprehensive experiments to
reveal the potential of watermarking deep neural networks
as the basis of this new problem. We showed that our frame-
work could embed a watermark without impairing the per-
formance of a deep neural network. The embedded water-
mark did not disappear even after ﬁne-tuning or parameter

(a) Embedding loss.

Figure 6. Histogram of the detected watermark σ(ΣiXjiwi) after
pruning.

against parameter pruning in this paper, a watermark might
be embedded in conjunction with compressing models. For
example, in [13], after parameter pruning, the network is
re-trained to learn the ﬁnal weights for the remaining sparse
parameters. Our embedding regularizer can be used in this
re-training to embed a watermark.

Network morphism. In [4, 32], a systematic study has
been done on how to morph a well-trained neural network
into a new one so that its network function can be com-
pletely preserved for further training. This network mor-
phism can constitute a severe attack against our watermark
because it may be impossible to detect the embedded water-
mark if the topology of the host network is severely modi-
ﬁed. We left the investigation how the embedded watermark
is affected by this network morphism for future work.

Steganalysis. Steganalysis [27, 21] is a method for de-
tecting the presence of secretly hidden data (e.g. steganog-
raphy or watermarks) in digital media ﬁles such as images,
video, audio, and, in our case, deep neural networks. Water-
marks ideally are robust against steganalysis. While, in this
paper, we conﬁrmed that embedding watermarks does not
signiﬁcantly change the distribution of model parameters,
more exploration is needed to evaluate robustness against
steganalysis. Conversely, developing effective steganalysis
against watermarks for deep neural networks can be an in-
teresting research topic.

Fingerprinting. Digital ﬁngerprinting is an alternative
to the watermarking approach for persistent identiﬁcation
of images [2], video [20, 31], and audio clips [1, 11]. In
this paper, we focused on one of these two important ap-
proaches. Robust ﬁngerprinting of deep neural networks is
another and complementary direction to protect deep neural
network models.

(b) Bit error rate.
Figure 5. Embedding loss and bit error rate after pruning as a func-
tion of pruning rate.

pruning; the entire watermark remained even after 65% of
the parameters were pruned.

5.1. Future Work

Although we have obtained ﬁrst insight into the new
problem of embedding a watermark in deep neural net-
works, many things remain as future work.

Watermark overwriting. A third-party user may em-
bed a different watermark in order to overwrite the origi-
nal watermark.
In our preliminary experiments, this wa-
termark overwriting caused 30.9%, 8.6%, and 0.4% bit er-
rors against watermarks in the conv 2, conv 3, and conv 4
groups when 256-bit watermarks were additionally embed-
ded. More robust watermarking against overwriting should
be explored (e.g. non-linear embedding).

Compression as embedding. Compressing deep neu-
ral networks is a very important and active research topic.
While we conﬁrmed that our watermark is very robust

9

[20] A. Joly, C. Frelicot, and O. Buisson. Content-based video
copy detection in large databases: a local ﬁngerprints sta-
tistical similarity search approach. In Proc. of ICIP, pages
505–508, 2005.

[21] J. Kodovsky, J. Fridrich, and V. Holub. Ensemble classiﬁers
for steganalysis of digital media. IEEE Trans. on Information
Forensics and Security, 7(2):432–444, 2012.

[22] A. Krizhevsky. Learning multiple layers of features from

tiny images. Tech Report, 2009.

[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
Proc. of NIPS, 2012.

Imagenet
In

[24] A. Krogh and J. A. Hertz. A simple weight decay can im-

prove generalization. In Proc. of NIPS, 1992.

[25] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278–2324, 1998.

[26] M. Abadi, et al. Tensorﬂow: Large-scale machine learning
on heterogeneous distributed systems. arXiv:1603.04467,
2016.

[27] L. Shaohui, Y. Hongxun, and G. Wen. Neural network based
steganalysis in still images. In Proc. of ICME, 2003.
[28] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In Proc. of ICLR,
2015.

[29] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In Proc. of CVPR, 2015.

[30] S. Tokui, K. Oono, S. Hido, and J. Clayton. Chainer: a
next-generation open source framework for deep learning.
In Proc. of NIPS Workshop on Machine Learning Systems,
2015.

[31] Y. Uchida, M. Agrawal, and S. Sakazawa. Accurate content-
based video copy detection with efﬁcient feature indexing.
In Proc. of ICMR, 2011.

[32] T. Wei, C. Wang, Y. Rui, and C. W. Chen. Network mor-

phism. In Proc. of ICML, 2016.

[33] S. Zagoruyko and N. Komodakis. Wide residual networks.

In Proc. of ECCV, 2016.

References

[1] X. Anguera, A. Garzon, and T. Adamek. Mask: Robust local
features for audio ﬁngerprinting. In Proc. of ICME, 2012.
[2] J. Barr, B. Bradley, and B. T. Hannigan. Using digital wa-
termarks with image signatures to mitigate the threat of the
copy attack. In Proc. of ICASSP, pages 69–72, 2003.

[3] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu,
G. Desjardins, J. Turian, D. Warde-Farley, and Y. Bengio.
Theano: a CPU and GPU math expression compiler. In Proc.
of the Python for Scientiﬁc Computing Conference (SciPy),
2010.

[4] T. Chen, I. Goodfellow, and J. Shlens. Net2net: Accelerating
learning via knowledge transfer. In Proc. of ICLR, 2016.

[5] F. Chollet. Keras. GitHub repository, 2015.
[6] A. Choromanska, M. Henaff, M. Mathieu, G. Arous, and
Y. LeCun. The loss surfaces of multilayer networks. In Proc.
of AISTATS, 2015.

[7] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A
matlab-like environment for machine learning. In Proc. of
NIPS Workshop on BigLearn, 2011.

[8] I. Cox, M. Miller, J. Bloom, J. Fridrich, and T. Kalker. Dig-
ital Watermarking and Steganography. Morgan Kaufmann
Publishers Inc., 2 edition, 2008.

[9] Y. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli,
Identifying and attacking the saddle point
In

and Y. Bengio.
problem in high-dimensional non-convex optimization.
Proc. of NIPS, 2014.

[10] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative
visual models from few training examples: an incremen-
tal bayesian approach tested on 101 object categories.
In
Proc. of CVPR Workshop on Generative-Model Based Vi-
sion, 2004.

[11] J. Haitsma and T. Kalker. A highly robust audio ﬁngerprint-
ing system. In Proc. of ISMIR, pages 107–115, 2002.
[12] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz,
and W. J. Dally. Eie: Efﬁcient inference engine on com-
pressed deep neural network. In Proc. of ISCA, 2016.
[13] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-
pressing deep neural networks with pruning, trained quanti-
zation and huffman coding. In Proc. of ICLR, 2016.

[14] S. Han, J. Pool, J. Tran, and W. J. Dally. Learning both
weights and connections for efﬁcient neural networks.
In
Proc. of NIPS, 2015.

[15] F. Hartung and M. Kutter. Multimedia watermarking tech-

niques. Proceedings of the IEEE, 87(7):1079–1107, 1999.

[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning

for image recognition. In Proc. of CVPR, 2016.

[17] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
In Proc. of NIPS Workshop on Deep

in a neural network.
Learning and Representation Learning, 2014.

[18] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural Computation, 9(8):1735–1780, 1997.

[19] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
In Proc. of MM,
architecture for fast feature embedding.
2014.

10

