A Robust Adaptive Stochastic
Gradient Method for Deep Learning

Caglar Gulcehre*
Universit´e de Montr´eal

Jose Sotelo*
Universit´e de Montr´eal

Marcin Moczulski
University of Oxford

Yoshua Bengio
Universit´e de Montr´eal

I. INTRODUCTION

II. DIRECTIONAL SECANT APPROXIMATION

7
1
0
2
 
r
a

M
 
2
 
 
]

G
L
.
s
c
[
 
 
1
v
8
8
7
0
0
.
3
0
7
1
:
v
i
X
r
a

Abstract—Stochastic gradient algorithms are the main focus of
large-scale optimization problems and led to important successes
in the recent advancement of the deep learning algorithms. The
convergence of SGD depends on the careful choice of learning
rate and the amount of the noise in stochastic estimates of
the gradients. In this paper, we propose an adaptive learning
rate algorithm, which utilizes stochastic curvature information
of the loss function for automatically tuning the learning rates.
The information about the element-wise curvature of the loss
function is estimated from the local statistics of the stochastic ﬁrst
order gradients. We further propose a new variance reduction
technique to speed up the convergence. In our experiments with
deep neural networks, we obtained better performance compared
to the popular stochastic gradient algorithms. 1

We develop an automatic stochastic gradient algorithm
which reduces the burden of extensive hyper-parameter search
for the optimizer. Our proposed algorithm exploits a lower
variance estimator of curvature of the cost function and uses
it to obtain an automatically tuned adaptive learning rate for
each parameter.

In deep learning and numerical optimization literature,
several papers suggest using a diagonal approximation of the
Hessian (second derivative matrix of the cost function with
respect to parameters), in order to estimate optimal learning
rates for stochastic gradient descent over high dimensional
parameter spaces [2], [3], [4]. A fundamental advantage of
using such approximation is that inverting such approximation
can be a trivial and cheap operation. However generally, for
neural networks, the inverse of the diagonal Hessian is usually
a bad approximation of the diagonal of the inverse of Hessian.
For example, obtaining a diagonal approximation of Hessian
are the Gauss-Newton matrix [5] or by ﬁnite differences [6].
Such estimations may however be very sensitive to the noise
coming from the Monte-Carlo estimates of the gradients. [3]
suggested a reliable way to estimate the local curvature in the
stochastic setting by keeping track of the variance and average
of the gradients.

We propose a different approach: instead of using a diagonal
estimate of Hessian, to estimate curvature along the direc-
tion of the gradient and we apply a new variance reduction
technique to compute it reliably. By using root mean square
the variance of gradients are reduced adaptively
statistics,

* denotes equal contribution.
1This paper is an extension/update of our previous paper [1].

with a simple transformation. We keep track of the estimation
of curvature using a technique similar to that proposed by
[3], which uses the variability of the expected loss. Standard
adaptive learning rate algorithms only scale the gradients, but
regular Newton-like second order methods, can perform more
complicate transformations, e.g. rotating the gradient vector.
Newton and quasi-newton methods can also be invariant to
afﬁne transformations in the parameter space. AdaSecant al-
gorithm is basically a stochastic rank-1 quasi-Newton method.
But in comparison with other adaptive learning algorithms, in-
stead of just scaling the gradient of each parameter, AdaSecant
can also perform an afﬁne transformation on them.

Directional Newton is a method proposed for solving equa-
tions with multiple variables[7]. The advantage of directional
Newton method proposed in[7], compared to Newton’s method
is that, it does not require a matrix inversion and still maintains
a quadratic rate of convergence.

In this paper, we develop a second-order directional Newton
method for nonlinear optimization. Step-size tk of update ∆k
for step k can be written as if it was a diagonal matrix:

∆k = −tk (cid:12) ∇θf(θk),

= − diag(tk)∇θf(θk),
= − diag(dk)(diag(Hdk))−1∇θf(θk).

where θk is the parameter vector at update k, f is the objective
function and dk is a unit vector of direction that the optimiza-
tion algorithm should follow. Denoting by hi = ∇θ
the
ith row of the Hessian matrix H and by ∇θif (θk) the ith
element of the gradient vector at update k, a reformulation of
Equation 1 for each diagonal element of the step-size diag(tk)
is:

∂f(θk)
∂θi

so effectively

∆k

i = −tk

i ∇θif(θk),
∇θif(θk)
hk
i dk

.

= −dk
i

tk
i =

dk
i
hk
i dk

.

(1)

(2)

(3)

(4)

(5)

(6)

We can approximate the per-parameter learning rate tk
i

following [8] using ﬁnite differences:

IV. VARIANCE REDUCTION FOR ROBUST STOCHASTIC
GRADIENT DESCENT

tk
i =

dk
i
hk
i dk

,

= lim
|∆k

i |→0

∆k
i
∇θif(θk + ∆k) − ∇θif(θk)

, for every i.

(8)

Let us note that alternatively one might use the R-op to
compute the Hessian-vector product for the denominator in
Equation 7 [9].

To choose a good direction dk in the stochastic setting,
we use block-normalized gradient vector that the parameters
of each layer is considered as a block and for each weight
(cid:9)
matrix Wi
i=1···k at
(cid:105)
each layer i and update k, dk =
for a
neural network with l layers.

k for θ = (cid:8)Wi
(cid:104)
dk
dk
b0
k

k and bias vector bi

k, bi
k
· · · dk
bl
k

W0
k

The update step is deﬁned as ∆k

i . The per-parameter
i can be estimated with the ﬁnite difference

i = tk

i dk

learning rate tk
approximation,

tk
i ≈

∆k
i
∇θif(θk + ∆k) − ∇θif(θk)

,

to Var(gi).

(9)

since, in the vicinity of the quadratic local minima,

∇θf(θk + ∆k) − ∇θf(θk) ≈ Hk∆k,

(10)

(7)

Variance reduction techniques for stochastic gradient esti-
mators have been well-studied in the machine learning lit-
erature. Both [10] and [11] proposed new ways of dealing
with this problem. In this paper, we proposed a new variance
reduction technique for stochastic gradient descent that relies
only on basic statistics related to the gradient. Let gi refer
to the ith element of the gradient vector g with respect
to the parameters θ and E[·] be an expectation taken over
minibatches and different trajectories of parameters.

We propose to apply the following transformation to reduce

the variance of the stochastic gradients:

˜gi =

gi + γiE[gi]
1 + γi

,

(14)

where γi is strictly a positive real number. Let us note that:

E[ ˜gi] = E[gi] and Var( ˜gi) =

1

(1 + γi)2 Var(gi).

(15)

The variance is reduced by a factor of (1 + γi)2 compared

In practice we do not have access to E[gi], therefore a biased
estimator gi based on past values of gi will be used instead.
We can rewrite the ˜gi as:

˜gi =

gi + (1 −

)E[gi],

(16)

1
1 + γi

1
1 + γi

We can therefore recover tk as

After substitution βi = 1

1+γi

, we will have:

tk = diag(∆k)(diag(Hk∆k))−1.

(11)

˜gi = βigi + (1 − βi)E[gi].

(17)

The directional secant method basically scales the gradient of
each parameter with the curvature along the direction of the
gradient vector and it is numerically stable.

III. RELATIONSHIP TO THE DIAGONAL APPROXIMATION
TO THE HESSIAN

Our secant approximation of the gradients are also very
closely tied to diagonal approximation of the Hessian matrix.
Considering that ith diagonal entry of the Hessian matrix can
be denoted as, Hii = ∂2f(θ)
. By using the ﬁnite differences,
∂θ2
i
it is possible to approximate this with as in Equation 12,

Hii = lim
|∆|→0

∇θi f(θ + ∆) − ∇θi f(θ)
∆i

,

(12)

Assuming that the diagonal of the Hessian is denoted with

A matrix, we can see the equivalence:

A ≈ diag(∇θf(θ + ∆) − ∇θf(θ)) diag(∆)−1.

(13)

The Equation 13 can be easily computed in a stochastic setting
from the consecutive minibatches.

By adapting γi or βi, it is possible to control the inﬂuence of
high variance, unbiased gi and low variance, biased gi on ˜gi.
Denoting by g(cid:48) the stochastic gradient obtained on the next
minibatch, the γi that well balances those two inﬂuences is the
one that keeps the ˜gi as close as possible to the true gradient
E[g(cid:48)
i] available. We try
to ﬁnd a regularized βi, in order to obtain a smoother estimate
of it and this yields us to more more stable estimates of βi. λ
is the regularization coefﬁcient for β.

i being the only sample of E[g(cid:48)

i] with g(cid:48)

arg min
βi

E[|| ˜gi − g(cid:48)

i||2

2] + λ(βi)2.

(18)

It can be shown that this a convex problem in βi with a closed-
form solution (details in appendix) and we can obtain the γi
from it:

γi =

E[(gi − g(cid:48)

i)(gi − E[gi])]

E[(gi − E[gi])(gi(cid:48) − E[gi]))] + λ

,

(19)

As a result, to estimate γ for each dimension, we keep track
of a estimation of
i−E[gi]))]+λ during training.
The necessary and sufﬁcient condition here, for the variance
reduction is to keep γ positive, to achieve a positive estimate of
γ we used the root mean square statistics for the expectations.

E[(gi−g(cid:48)
E[(gi−E[gi])(g(cid:48)

i)(gi−E[gi])]

V. BLOCKWISE GRADIENT NORMALIZATION

is performed and τi is adapted using

multiplication with τ −1
the following equation:

i

It

is very well-known that

the repeated application of
the non-linearities can cause the gradients to vanish [12],
[13]. Thus, in order to tackle this problem, we normalize
the gradients coming into each block/layer to have norm 1.
Assuming the normalized gradient can be denoted with ˜g,
it can be computed as, ˜g =
. We estimate, E[g] via
moving averages.

g
||E[g]||2

Blockwise gradient normalization of the gradient adds noise
to the gradients, but
in practice we did not observe any
negative impact of it. We conjecture that this is due to the angle
between the stochastic gradient and the block-normalized
gradient still being less than 90 degrees.

τi[k] = (1 −

)τi[k − 1] + 1 .

(25)

E[∆i]2
E[(∆i)2]k−1

k−1

B. Outlier Gradient Detection

Our algorithm is very similar to [6], but instead of incre-
menting τi[t+1] when an outlier is detected, the time-constant
is reset to 2.2. Note that when τi[t + 1] ≈ 2, this assigns
approximately the same amount of weight
to the current
and the average of previous observations. This mechanism
made learning more stable, because without it outlier gradients
saturate τi to a large value.

VI. ADAPTIVE STEP-SIZE IN STOCHASTIC CASE

C. Variance Reduction

In the stochastic gradient case, the step-size of the direc-
tional secant can be computed by using an expectation over
the minibatches:

Ek[ti] = Ek[

∆k
i
∇θif(θk + ∆k) − ∇θif(θk)

].

(20)

The Ek[·] that is used to compute the secant update, is taken
over the minibatches at the past values of the parameters.

Computing the expectation in Equation20 was numerically
unstable in stochastic setting. We decided to use a more
stable second order Taylor approximation of Equation 20
around ((cid:112)Ek[(αk
i = ∇θif(θk +
∆k) − ∇θif(θk). Assuming (cid:112)Ek[(αk
i ] and
(cid:112)Ek[(∆k
i )2] ≈ Ek[∆k
i ] we obtain always non-negative ap-
proximation of Ek[ti]:

i )2], (cid:112)Ek[(∆k

i )2] ≈ Ek[αk

i )2]), with αk

(cid:112)Ek[(∆k
(cid:112)Ek[(αk

Ek[ti] ≈

i )2]
i )2]
In our experiments, we used a simpler approximation, which
in practice worked as well as formulations in Equation21:

Cov(αk
Ek[(αk

i , ∆k
i )
i )2]

(21)

−

.

Ek[ti] ≈

(cid:112)Ek[(∆k
(cid:112)Ek[(αk

i )2]
i )2]

−

Ek[αk
Ek[(αk

i ∆k
i ]
i )2]

.

VII. ALGORITHMIC DETAILS

(22)

A. Approximate Variability

To compute the moving averages as also adopted by [3],
we used an algorithm to dynamically decide the time constant
based on the step size being taken. As a result algorithm that
we used will give bigger weights to the updates that have large
step-size and smaller weights to the updates that have smaller
step-size.

By assuming that ¯∆i[k] ≈ E[∆i]k, the moving average

update rule for ¯∆i[k] can be written as,

¯∆2

i [k] = (1 − τ −1

i

[k]) ¯∆2

i [k − 1] + τ −1

i

[k](tk

i ˜gk

i ),

(23)

and,

(cid:113)

¯∆i[k] =

¯∆2

i [k].

This rule for each update assigns a different weight to each
element of the gradient vector . At each iteration a scalar

The correction parameters γi (Equation19) allows for a ﬁne-
grained variance reduction for each parameter independently.
The noise in the stochastic gradient methods can have ad-
vantages both in terms of generalization and optimization. It
introduces an exploration and exploitation trade-off, which can
be controlled by upper bounding the values of γi with a value
ρi, so that thresholded γ(cid:48)

i = min(ρi, γi).
We block-wise normalized the gradients of each weight
matrix and bias vectors in g to compute the ˜g as described in
Section II. That makes AdaSecant scale-invariant, thus more
robust to the scale of the inputs and the number of the layers of
the network. We observed empirically that it was easier to train
very deep neural networks with block normalized gradient
descent. In our experiments, we ﬁxed λ to 1e − 5.

VIII. IMPROVING CONVERGENCE

Classical convergence results for SGD are based on the

conditions:

(η(i))2 < ∞ and

η(i) = ∞

(26)

(cid:88)

i

(cid:88)

i

such that the learning rate η(i) should decrease [14]. Due to the
noise in the estimation of adaptive step-sizes for AdaSecant,
the convergence would not be guaranteed. To ensure it, we
developed a new variant of Adagrad [15] with thresholding,
such that each scaling factor is lower bounded by 1. Assuming
ak
is the accumulated norm of all past gradients for ith
i
parameter at update k, it is thresholded from below ensuring
that the algorithm will converge:

ak
i =

(gj

i )2,

(cid:118)
(cid:117)
(cid:117)
(cid:116)

k
(cid:88)

j=0

i = maximum(1, ak
ρk

i ),

(27)

(28)

and

giving

(24)

1
ρi
In the initial stages of training, accumulated norm of the per-
parameter gradients can be less than 1. If the accumulated

i ˜gk
ηk
i .

i =

(29)

∆k

> ηk

i = Ek[tk

i where ηk

per-parameter norm of a gradient is less than 1, Adagrad will
augment the learning-rate determined by AdaSecant for that
update, i.e. ηk
i ] is the per-parameter
i
ρk
i
learning rate determined by AdaSecant. This behavior tends
to create unstabilities during the training with AdaSecant. Our
modiﬁcation of the Adagrad algorithm is to ensure that, it
will reduce the learning rate determined by the AdaSecant
algorithm at each update, i.e. ηk
i and the learning rate
i
ρk
i
will be bounded. At the beginning of the training, parameters
of a neural network can get 0-valued gradients, e.g. in the ex-
istence of dropout and ReLU units. However this phenomena
can cause the per-parameter learning rate scaled by Adagrad
to be unbounded.

≤ ηk

In Algorithm 1, we provide a simple pseudo-code of the

AdaSecant algorithm.

Algorithm 1: AdaSecant: minibatch-AdaSecant for adap-
tive learning rates with variance reduction

repeat

(cid:80)n

k=1 ∇(k)

draw n samples, compute the gradients g(j) where
g(j) ∈ Rn for each minibatch j, g(j) is computed
as, 1
θ f(θ)
n
estimate E[g] via moving averages.
block-wise normalize gradients of each weight matrix
and bias vector
for parameter i ∈ {1, . . . , n} do

i−E[gi]))]k

i)(gi−E[gi])]k

E[(gi−E[gi])(g(cid:48)

compute the correction term by using,
i = E[(gi−g(cid:48)
γk
compute corrected gradients ˜gi = gi+γiE[gi]
if |g(j)
i − E[gi]| >
2(cid:112)E[(gi)2] − (E[gi])2
or
2(cid:112)E[(αi)2] − (E[αi])2 then

(cid:12)
(cid:12)α(j)
(cid:12)

i − E[αi]

1+γi

(cid:12)
(cid:12)
(cid:12) >

reset the memory size for outliers τi ← 2.2

end

update moving averages according to Equation 23

)2]

η(j)
i ←

estimate learning rate
(cid:113)
Ek[(∆(k)
(cid:112)Ek[(αk

i ∆k
i ]
i
i )2]
i )2]
update memory size as in Equation 25
update parameter θj

Ek[αk
Ek[(αk

i − η(j)

i ← θj−1

−

i

· ˜g(j)
i

end

until stopping criterion is met;

IX. EXPERIMENTS

We have run experiments on character-level PTB with
GRU units, on MNIST with Maxout Networks [16] and on
handwriting synthesis using the IAM-OnDB dataset [17]. We
compare AdaSecant with popular stochastic gradient learning
algorithms: Adagrad, RMSProp [18], Adadelta [19], Adam
[20] and SGD+momentum (with linearly decaying learning

rate). AdaSecant performs as well or better as carefully tuned
algorithms for all these different tasks.

A. Ablation Study

Model

Train Log-Loss

Valid Log-Loss

Adam with 3e-4 learning rate
Adam with 1e-4 learning rate
Adam with 5e-4 learning rate
AdaSecant
AdaSecant, no VR
AdaSecant, no AG
AdaSecant, no BN
AdaSecant, no OD
AdaSecant, no VR, no AG
AdaSecant, no VR, no BN
AdaSecant, no VR, no OD
AdaSecant, no AG, no BN
AdaSecant, no AG, no OD
AdaSecant, no BN, no OD
AdaSecant, no AG, no BN, no OD
AdaSecant, no VR, no BN, no OD
AdaSecant, no VR, no AG, no OD
AdaSecant, no VR, no AG, no BN

-1.827
-1.780
-1.892
-1.881
-1.876
-1.867
-1.857
-1.780
-1.848
-1.844
-1.479
-1.878
-1.723
-1.814
-1.611
-1.531
unstable
-1.862

-1.743
-1.713
-1.773
-1.744
-1.743
-1.738
-1.784
-1.726
-1.744
-1.777
-1.442
-1.786
-1.674
-1.764
-1.573
-1.491
unstable
1.75

TABLE I
SUMMARY OF RESULTS FOR THE HANDWRITING EXPERIMENT. WE
REPORT THE BEST VALIDATION LOG-LOSS THAT WE FOUND FOR EACH
MODEL USING EARLY STOPPING. WE ALSO REPORT THE CORRESPONDING
TRAIN LOG-LOSS. IN ALL CASES, THE LOG-LOSS IS COMPUTED PER DATA
POINT.

In this section, we decompose the different parts of the
algorithm to measure the effect they have in the performance.
For this comparison, we trained a model to learn handwriting
synthesis on IAM-OnDB dataset. Our model follows closely
the architecture introduced in [18] with two modiﬁcations.
First, we use one recurrent layer of size 400 instead of three.
Second, we use GRU [21] units instead of LSTM [22] units.
Also, we use a different symbol for each of the 87 different
characters in the dataset. The code for this experiment is
available online.2

We tested different conﬁgurations that included taking away
the use of Variance Reduction (VR), Adagrad (AG), Block
Normalization (BN), and Outlier Detection (OD). Also, we
compared against ADAM [20] with different learning rates in
Figure 1. There, we observe that adasecant performs as well
as Adam with a carefully tuned learning rate.

In Figure 2, we disable each of the four components
of the algorithm. We ﬁnd that BN provides a small, but
constant advantage in performance. OD is also important for
the algorithm. Disabling OD makes training more noisy and
unstable and gives worse results. Disabling VR also makes
training unstable. AG has the least effect in the performance
of the algorithm. Furthermore, disabling more than one com-
ponent makes training even more unstable in the majority of
scenarios. A summary of the results is available in Table I.
In all cases, we use early stopping on the validation log-loss.
Furthermore, we present the train log-loss corresponding to

2https://github.com/sotelo/scribe

the best validation loss as well. Let us note that the log-loss
is computed per data point.

Fig. 1. Baseline comparison against Adam. AdaSecant performs as well as
Adam with a carefully tuned learning rate.

B. PTB Character-level LM

We have run experiments with GRU-RNN[21] on PTB
dataset for character-level language modeling over the subset
deﬁned in [23]. On this task, we use 400 GRU units with
minibatch size of 20. We train the model over the sequences of
length 150. For AdaSecant, we have not run any hyperparmeter
search, but for Adam we run a hyperparameter search for the
learning rate and gradient clipping. The learning rates are sam-
pled from log-uniform distribution between 1e − 1 and 6e − 5.
Gradient clipping threshold is sampled uniformly between 1.2
to 20. We have evaluated 20 different pairs of randomly-
sampled learning rates and gradient clipping thresholds. The
rest of the hyper-parameters are ﬁxed to their default values.
We use the model with the best validation error for Adam. For
AdaSecant algorithm, we ﬁx all the hyperparameters to their

Fig. 2. Deactivating one component at a time. BN provides a small but
constant advantage in performance. OD is important for the algorithm.
Deactivating it makes training more noisy and unstable and gives worse
results. Deactivating VR also makes training unstable.

default values. The learning curves for the both algorithms are
shown in Figure 3.

C. MNIST with Maxout Networks

The results are summarized in Figure 4 and we show
that AdaSecant converges as fast or faster than other tech-
niques, including the use of hand-tuned global learning rate
and momentum for SGD, RMSprop, and Adagrad. In our
experiments with AdaSecant algorithm, adaptive momentum
term γk
i was clipped at 1.8. In 2-layer Maxout network
experiments for SGD-momentum experiments, we used the
best hyper-parameters reported by [16], for RMSProp and
Adagrad, we crossvalidated learning rate for 15 different
learning rates sampled uniformly from the log-space. We
crossvalidated 30 different pairs of momentum and learning
rate for SGD+momentum, for RMSProp and Adagrad, we

(a) 2 layer Maxout Network

Fig. 3. Learning curves for the very well-tuned Adam vs AdaSecant algorithm
without any hyperparameter tuning. AdaSecant performs very close to the very
well-tuned Adam on PTB character-level language modeling task. This shows
us the robustness of the algorithm to its hyperparameters.

crossvalidated 15 different learning rates sampled them from
log-space uniformly for deep maxout experiments.

X. CONCLUSION

We described a new stochastic gradient algorithm with adap-
tive learning rates that is fairly insensitive to the tuning of the
hyper-parameters and doesn’t require tuning of learning rates.
Furthermore, the variance reduction technique we proposed
improves the convergence when the stochastic gradients have
high variance. Our algorithm performs as well or better than
other popular, carefully-tuned stochastic gradient algorithms.
We also present a comprehensive ablation study where we
show the effects and importance of each of the elements of
our algorithm. As future work, we should try to ﬁnd theoretical
convergence properties of the algorithm to understand it better
analytically.

ACKNOWLEDGMENTS

We thank the developers of Theano [24], Pylearn2 [25]
and Blocks [26] and the computational resources provided
by Compute Canada and Calcul Qu´ebec. This work has been
partially supported by NSERC, CIFAR, and Canada Research
Chairs, Project TIN2013-41751, grant 2014-SGR-221. Jose
Sotelo also thanks the Consejo Nacional de Ciencia y Tec-
nolog´ıa (CONACyT) as well as the Secretar´ıa de Educaci´on
P´ublica (SEP) for their support. We would like to thank Tom
Schaul for the valuable discussions. We also thank Kyunghyun
Cho and Orhan Firat for proof-reading and giving feedbacks
on the paper.

REFERENCES

[1] C. Gulcehre, M. Moczulski, and Y. Bengio, “Adasecant: robust adaptive
secant method for stochastic gradient,” arXiv preprint arXiv:1412.7419,
2014.

(b) 16 layer Maxout Network

Fig. 4. Comparison of different stochastic gradient algorithms on MNIST
with Maxout Networks. Both a) and b) are trained with dropout and maximum
column norm constraint regularization on the weights. Networks are initialized
with weights sampled from a Gaussian distribution with 0 mean and standard
deviation of 0.05. In both experiments, the proposed algorithm, AdaSecant,
seems to be converging faster and arrives to a better minima in training set.
We trained both networks for 350 epochs over the training set.

[2] S. Becker and Y. Le Cun, “Improving the convergence of back-
propagation learning with second order methods,” in Proceedings of the
1988 connectionist models summer school. San Matteo, CA: Morgan
Kaufmann, 1988, pp. 29–37.

[3] T. Schaul, S. Zhang, and Y. LeCun, “No more pesky learning rates,”

arXiv preprint arXiv:1206.1106, 2012.

[4] Y. LeCun, P. Y. Simard, and B. Pearlmutter, “Automatic learning
rate maximization by on-line estimation of the hessians eigenvectors,”
Advances in neural information processing systems, vol. 5, pp. 156–163,
1993.

[5] Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. M¨uller, “Efﬁcient
Springer, 2012,

backprop,” in Neural networks: Tricks of the trade.
pp. 9–48.

[6] T. Schaul and Y. LeCun, “Adaptive learning rates and paralleliza-
tion for stochastic, sparse, non-smooth gradients,” arXiv preprint
arXiv:1301.3764, 2013.

[7] Y. Levin and A. Ben-Israel, “Directional newton methods in n variables,”
Mathematics of Computation, vol. 71, no. 237, pp. 251–262, 2002.
[8] H.-B. An and Z.-Z. Bai, “Directional secant method for nonlinear
equations,” Journal of computational and applied mathematics, vol. 175,
no. 2, pp. 291–304, 2005.

[9] N. N. Schraudolph, “Fast curvature matrix-vector products for second-

order gradient descent,” Neural computation, vol. 14, no. 7, pp. 1723–
1738, 2002.

[10] C. Wang, X. Chen, A. Smola, and E. Xing, “Variance reduction for
stochastic gradient optimization,” in Advances in Neural Information
Processing Systems, 2013, pp. 181–189.

[11] R. Johnson and T. Zhang, “Accelerating stochastic gradient descent
using predictive variance reduction,” in Advances in Neural Information
Processing Systems, 2013, pp. 315–323.

[12] Y. Bengio, P. Simard, and P. Frasconi, “Learning long-term dependencies
with gradient descent is difﬁcult,” IEEE transactions on neural networks,
vol. 5, no. 2, pp. 157–166, 1994.

[13] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber, “Gradient
ﬂow in recurrent nets: the difﬁculty of learning long-term dependencies,”
2001.

[14] H. Robbins and S. Monro, “A stochastic approximation method,” The

annals of mathematical statistics, pp. 400–407, 1951.

[15] J. Duchi, E. Hazan, and Y. Singer, “Adaptive subgradient methods for
online learning and stochastic optimization,” The Journal of Machine
Learning Research, vol. 12, pp. 2121–2159, 2011.

[16] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Ben-
gio, “Maxout networks,” arXiv preprint arXiv:1302.4389, 2013.
[17] M. Liwicki and H. Bunke, “Iam-ondb - an on-line english sentence
database acquired from handwritten text on a whiteboard.” in ICDAR.
IEEE Computer Society, 2005, pp. 956–961.

[18] A. Graves, “Generating sequences with recurrent neural networks,” arXiv

[19] M. D. Zeiler, “Adadelta: An adaptive learning rate method,” arXiv

preprint arXiv:1308.0850, 2013.

preprint arXiv:1212.5701, 2012.

[20] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
in International Conference on Learning Representations, 2015.
[21] K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y. Bengio, “Learning phrase representations using
rnn encoder-decoder for statistical machine translation,” arXiv preprint
arXiv:1406.1078, 2014.

[22] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
Comput., vol. 9, no. 8, pp. 1735–1780, Nov. 1997. [Online]. Available:
http://dx.doi.org/10.1162/neco.1997.9.8.1735

[23] T. Mikolov, I. Sutskever, A. Deoras, H. Le, S. Kombrink, and J. Cer-
nocky, “Subword language modeling with neural networks,” preprint,
2012.

[24] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. Goodfellow, A. Berg-
eron, N. Bouchard, D. Warde-Farley, and Y. Bengio, “Theano: new
features and speed improvements,” Deep Learning and Unsupervised
Feature Learning NIPS 2012 Workshop, 2012.

[25] I. J. Goodfellow, D. Warde-Farley, P. Lamblin, V. Dumoulin, M. Mirza,
R. Pascanu, J. Bergstra, F. Bastien, and Y. Bengio, “Pylearn2: a machine
learning research library,” arXiv preprint arXiv:1308.4214, 2013.
[26] B. van Merri¨enboer, D. Bahdanau, V. Dumoulin, D. Serdyuk, D. Warde-
Farley, J. Chorowski, and Y. Bengio, “Blocks and Fuel: Frameworks for
deep learning,” ArXiv e-prints, jun 2015.

[27] Theano Development Team, “Theano: A Python framework for
fast computation of mathematical expressions,” arXiv e-prints, vol.
abs/1605.02688, May 2016. [Online]. Available: http://arxiv.org/abs/
1605.02688

APPENDIX

A. Derivation of Equation 18

∂E[(βigi + (1 − βi)E[gi] − g(cid:48)
∂βi

i)2]

+ λβ2

i = 0

E[(βigi + (1 − βi)E[gi] − g(cid:48)
i)
∂(βigi + (1 − βi)E[gi] − g(cid:48)
i)
∂βi

] + λβi = 0

E[(βigi + (1 − βi)E[gi] − g(cid:48)

i)(gi − E[gi])] + λβi = 0

E[(βigi(gi − E[gi]) + (1 − βi)E[gi](gi − E[gi])

− g(cid:48)

i(gi − E[gi])] + λβi

= 0

E[(gi − E[gi])(g(cid:48)

i − E[gi])]

E[(gi − E[gi])(gi − E[gi])] + λ
E[(gi − E[gi])(g(cid:48)

i − E[gi])]

βi =

=

Var(gi) + λ

B. Further Experimental Details

In Figure 5, we analyzed the effect of using different
minibatch sizes for AdaSecant and compared its convergence
with Adadelta in wall-clock time. For minibatch size 100
AdaSecant was able to reach the almost same training negative
log-likelihood as Adadelta after the same amount of time, but
its convergence took much longer. With minibatches of size
500 AdaSecant was able to converge faster in wallclock time
to a better local minima.

Fig. 5. In this plot, we compared AdaSecant trained by using minibatch size
of 100 and 500 with adadelta using minibatches of size 100. We performed
these experiments on MNIST with 2-layer maxout MLP using dropout.

C. More decomposition experiments

We have run experiments with the different combinations
of the components of the algorithm. We show those results on
handwriting synthesis with IAM-OnDB dataset. The results
can be observed from Figure 6, Figure 7, Figure 8, and
Figure 9 deactivating the components leads to a more unstable
training curve in the majority of scenarios.

Fig. 6. No variance reduction comparison.

Fig. 8. No block normalization comparison.

Fig. 7. No Adagrad comparison.

Fig. 9. No outlier detection comparison.

A Robust Adaptive Stochastic
Gradient Method for Deep Learning

Caglar Gulcehre*
Universit´e de Montr´eal

Jose Sotelo*
Universit´e de Montr´eal

Marcin Moczulski
University of Oxford

Yoshua Bengio
Universit´e de Montr´eal

I. INTRODUCTION

II. DIRECTIONAL SECANT APPROXIMATION

7
1
0
2
 
r
a

M
 
2
 
 
]

G
L
.
s
c
[
 
 
1
v
8
8
7
0
0
.
3
0
7
1
:
v
i
X
r
a

Abstract—Stochastic gradient algorithms are the main focus of
large-scale optimization problems and led to important successes
in the recent advancement of the deep learning algorithms. The
convergence of SGD depends on the careful choice of learning
rate and the amount of the noise in stochastic estimates of
the gradients. In this paper, we propose an adaptive learning
rate algorithm, which utilizes stochastic curvature information
of the loss function for automatically tuning the learning rates.
The information about the element-wise curvature of the loss
function is estimated from the local statistics of the stochastic ﬁrst
order gradients. We further propose a new variance reduction
technique to speed up the convergence. In our experiments with
deep neural networks, we obtained better performance compared
to the popular stochastic gradient algorithms. 1

We develop an automatic stochastic gradient algorithm
which reduces the burden of extensive hyper-parameter search
for the optimizer. Our proposed algorithm exploits a lower
variance estimator of curvature of the cost function and uses
it to obtain an automatically tuned adaptive learning rate for
each parameter.

In deep learning and numerical optimization literature,
several papers suggest using a diagonal approximation of the
Hessian (second derivative matrix of the cost function with
respect to parameters), in order to estimate optimal learning
rates for stochastic gradient descent over high dimensional
parameter spaces [2], [3], [4]. A fundamental advantage of
using such approximation is that inverting such approximation
can be a trivial and cheap operation. However generally, for
neural networks, the inverse of the diagonal Hessian is usually
a bad approximation of the diagonal of the inverse of Hessian.
For example, obtaining a diagonal approximation of Hessian
are the Gauss-Newton matrix [5] or by ﬁnite differences [6].
Such estimations may however be very sensitive to the noise
coming from the Monte-Carlo estimates of the gradients. [3]
suggested a reliable way to estimate the local curvature in the
stochastic setting by keeping track of the variance and average
of the gradients.

We propose a different approach: instead of using a diagonal
estimate of Hessian, to estimate curvature along the direc-
tion of the gradient and we apply a new variance reduction
technique to compute it reliably. By using root mean square
the variance of gradients are reduced adaptively
statistics,

* denotes equal contribution.
1This paper is an extension/update of our previous paper [1].

with a simple transformation. We keep track of the estimation
of curvature using a technique similar to that proposed by
[3], which uses the variability of the expected loss. Standard
adaptive learning rate algorithms only scale the gradients, but
regular Newton-like second order methods, can perform more
complicate transformations, e.g. rotating the gradient vector.
Newton and quasi-newton methods can also be invariant to
afﬁne transformations in the parameter space. AdaSecant al-
gorithm is basically a stochastic rank-1 quasi-Newton method.
But in comparison with other adaptive learning algorithms, in-
stead of just scaling the gradient of each parameter, AdaSecant
can also perform an afﬁne transformation on them.

Directional Newton is a method proposed for solving equa-
tions with multiple variables[7]. The advantage of directional
Newton method proposed in[7], compared to Newton’s method
is that, it does not require a matrix inversion and still maintains
a quadratic rate of convergence.

In this paper, we develop a second-order directional Newton
method for nonlinear optimization. Step-size tk of update ∆k
for step k can be written as if it was a diagonal matrix:

∆k = −tk (cid:12) ∇θf(θk),

= − diag(tk)∇θf(θk),
= − diag(dk)(diag(Hdk))−1∇θf(θk).

where θk is the parameter vector at update k, f is the objective
function and dk is a unit vector of direction that the optimiza-
tion algorithm should follow. Denoting by hi = ∇θ
the
ith row of the Hessian matrix H and by ∇θif (θk) the ith
element of the gradient vector at update k, a reformulation of
Equation 1 for each diagonal element of the step-size diag(tk)
is:

∂f(θk)
∂θi

so effectively

∆k

i = −tk

i ∇θif(θk),
∇θif(θk)
hk
i dk

.

= −dk
i

tk
i =

dk
i
hk
i dk

.

(1)

(2)

(3)

(4)

(5)

(6)

We can approximate the per-parameter learning rate tk
i

following [8] using ﬁnite differences:

IV. VARIANCE REDUCTION FOR ROBUST STOCHASTIC
GRADIENT DESCENT

tk
i =

dk
i
hk
i dk

,

= lim
|∆k

i |→0

∆k
i
∇θif(θk + ∆k) − ∇θif(θk)

, for every i.

(8)

Let us note that alternatively one might use the R-op to
compute the Hessian-vector product for the denominator in
Equation 7 [9].

To choose a good direction dk in the stochastic setting,
we use block-normalized gradient vector that the parameters
of each layer is considered as a block and for each weight
(cid:9)
matrix Wi
i=1···k at
(cid:105)
each layer i and update k, dk =
for a
neural network with l layers.

k for θ = (cid:8)Wi
(cid:104)
dk
dk
b0
k

k and bias vector bi

k, bi
k
· · · dk
bl
k

W0
k

The update step is deﬁned as ∆k

i . The per-parameter
i can be estimated with the ﬁnite difference

i = tk

i dk

learning rate tk
approximation,

tk
i ≈

∆k
i
∇θif(θk + ∆k) − ∇θif(θk)

,

to Var(gi).

(9)

since, in the vicinity of the quadratic local minima,

∇θf(θk + ∆k) − ∇θf(θk) ≈ Hk∆k,

(10)

(7)

Variance reduction techniques for stochastic gradient esti-
mators have been well-studied in the machine learning lit-
erature. Both [10] and [11] proposed new ways of dealing
with this problem. In this paper, we proposed a new variance
reduction technique for stochastic gradient descent that relies
only on basic statistics related to the gradient. Let gi refer
to the ith element of the gradient vector g with respect
to the parameters θ and E[·] be an expectation taken over
minibatches and different trajectories of parameters.

We propose to apply the following transformation to reduce

the variance of the stochastic gradients:

˜gi =

gi + γiE[gi]
1 + γi

,

(14)

where γi is strictly a positive real number. Let us note that:

E[ ˜gi] = E[gi] and Var( ˜gi) =

1

(1 + γi)2 Var(gi).

(15)

The variance is reduced by a factor of (1 + γi)2 compared

In practice we do not have access to E[gi], therefore a biased
estimator gi based on past values of gi will be used instead.
We can rewrite the ˜gi as:

˜gi =

gi + (1 −

)E[gi],

(16)

1
1 + γi

1
1 + γi

We can therefore recover tk as

After substitution βi = 1

1+γi

, we will have:

tk = diag(∆k)(diag(Hk∆k))−1.

(11)

˜gi = βigi + (1 − βi)E[gi].

(17)

The directional secant method basically scales the gradient of
each parameter with the curvature along the direction of the
gradient vector and it is numerically stable.

III. RELATIONSHIP TO THE DIAGONAL APPROXIMATION
TO THE HESSIAN

Our secant approximation of the gradients are also very
closely tied to diagonal approximation of the Hessian matrix.
Considering that ith diagonal entry of the Hessian matrix can
be denoted as, Hii = ∂2f(θ)
. By using the ﬁnite differences,
∂θ2
i
it is possible to approximate this with as in Equation 12,

Hii = lim
|∆|→0

∇θi f(θ + ∆) − ∇θi f(θ)
∆i

,

(12)

Assuming that the diagonal of the Hessian is denoted with

A matrix, we can see the equivalence:

A ≈ diag(∇θf(θ + ∆) − ∇θf(θ)) diag(∆)−1.

(13)

The Equation 13 can be easily computed in a stochastic setting
from the consecutive minibatches.

By adapting γi or βi, it is possible to control the inﬂuence of
high variance, unbiased gi and low variance, biased gi on ˜gi.
Denoting by g(cid:48) the stochastic gradient obtained on the next
minibatch, the γi that well balances those two inﬂuences is the
one that keeps the ˜gi as close as possible to the true gradient
E[g(cid:48)
i] available. We try
to ﬁnd a regularized βi, in order to obtain a smoother estimate
of it and this yields us to more more stable estimates of βi. λ
is the regularization coefﬁcient for β.

i being the only sample of E[g(cid:48)

i] with g(cid:48)

arg min
βi

E[|| ˜gi − g(cid:48)

i||2

2] + λ(βi)2.

(18)

It can be shown that this a convex problem in βi with a closed-
form solution (details in appendix) and we can obtain the γi
from it:

γi =

E[(gi − g(cid:48)

i)(gi − E[gi])]

E[(gi − E[gi])(gi(cid:48) − E[gi]))] + λ

,

(19)

As a result, to estimate γ for each dimension, we keep track
of a estimation of
i−E[gi]))]+λ during training.
The necessary and sufﬁcient condition here, for the variance
reduction is to keep γ positive, to achieve a positive estimate of
γ we used the root mean square statistics for the expectations.

E[(gi−g(cid:48)
E[(gi−E[gi])(g(cid:48)

i)(gi−E[gi])]

V. BLOCKWISE GRADIENT NORMALIZATION

is performed and τi is adapted using

multiplication with τ −1
the following equation:

i

It

is very well-known that

the repeated application of
the non-linearities can cause the gradients to vanish [12],
[13]. Thus, in order to tackle this problem, we normalize
the gradients coming into each block/layer to have norm 1.
Assuming the normalized gradient can be denoted with ˜g,
it can be computed as, ˜g =
. We estimate, E[g] via
moving averages.

g
||E[g]||2

Blockwise gradient normalization of the gradient adds noise
to the gradients, but
in practice we did not observe any
negative impact of it. We conjecture that this is due to the angle
between the stochastic gradient and the block-normalized
gradient still being less than 90 degrees.

τi[k] = (1 −

)τi[k − 1] + 1 .

(25)

E[∆i]2
E[(∆i)2]k−1

k−1

B. Outlier Gradient Detection

Our algorithm is very similar to [6], but instead of incre-
menting τi[t+1] when an outlier is detected, the time-constant
is reset to 2.2. Note that when τi[t + 1] ≈ 2, this assigns
approximately the same amount of weight
to the current
and the average of previous observations. This mechanism
made learning more stable, because without it outlier gradients
saturate τi to a large value.

VI. ADAPTIVE STEP-SIZE IN STOCHASTIC CASE

C. Variance Reduction

In the stochastic gradient case, the step-size of the direc-
tional secant can be computed by using an expectation over
the minibatches:

Ek[ti] = Ek[

∆k
i
∇θif(θk + ∆k) − ∇θif(θk)

].

(20)

The Ek[·] that is used to compute the secant update, is taken
over the minibatches at the past values of the parameters.

Computing the expectation in Equation20 was numerically
unstable in stochastic setting. We decided to use a more
stable second order Taylor approximation of Equation 20
around ((cid:112)Ek[(αk
i = ∇θif(θk +
∆k) − ∇θif(θk). Assuming (cid:112)Ek[(αk
i ] and
(cid:112)Ek[(∆k
i )2] ≈ Ek[∆k
i ] we obtain always non-negative ap-
proximation of Ek[ti]:

i )2], (cid:112)Ek[(∆k

i )2] ≈ Ek[αk

i )2]), with αk

(cid:112)Ek[(∆k
(cid:112)Ek[(αk

Ek[ti] ≈

i )2]
i )2]
In our experiments, we used a simpler approximation, which
in practice worked as well as formulations in Equation21:

Cov(αk
Ek[(αk

i , ∆k
i )
i )2]

(21)

−

.

Ek[ti] ≈

(cid:112)Ek[(∆k
(cid:112)Ek[(αk

i )2]
i )2]

−

Ek[αk
Ek[(αk

i ∆k
i ]
i )2]

.

VII. ALGORITHMIC DETAILS

(22)

A. Approximate Variability

To compute the moving averages as also adopted by [3],
we used an algorithm to dynamically decide the time constant
based on the step size being taken. As a result algorithm that
we used will give bigger weights to the updates that have large
step-size and smaller weights to the updates that have smaller
step-size.

By assuming that ¯∆i[k] ≈ E[∆i]k, the moving average

update rule for ¯∆i[k] can be written as,

¯∆2

i [k] = (1 − τ −1

i

[k]) ¯∆2

i [k − 1] + τ −1

i

[k](tk

i ˜gk

i ),

(23)

and,

(cid:113)

¯∆i[k] =

¯∆2

i [k].

This rule for each update assigns a different weight to each
element of the gradient vector . At each iteration a scalar

The correction parameters γi (Equation19) allows for a ﬁne-
grained variance reduction for each parameter independently.
The noise in the stochastic gradient methods can have ad-
vantages both in terms of generalization and optimization. It
introduces an exploration and exploitation trade-off, which can
be controlled by upper bounding the values of γi with a value
ρi, so that thresholded γ(cid:48)

i = min(ρi, γi).
We block-wise normalized the gradients of each weight
matrix and bias vectors in g to compute the ˜g as described in
Section II. That makes AdaSecant scale-invariant, thus more
robust to the scale of the inputs and the number of the layers of
the network. We observed empirically that it was easier to train
very deep neural networks with block normalized gradient
descent. In our experiments, we ﬁxed λ to 1e − 5.

VIII. IMPROVING CONVERGENCE

Classical convergence results for SGD are based on the

conditions:

(η(i))2 < ∞ and

η(i) = ∞

(26)

(cid:88)

i

(cid:88)

i

such that the learning rate η(i) should decrease [14]. Due to the
noise in the estimation of adaptive step-sizes for AdaSecant,
the convergence would not be guaranteed. To ensure it, we
developed a new variant of Adagrad [15] with thresholding,
such that each scaling factor is lower bounded by 1. Assuming
ak
is the accumulated norm of all past gradients for ith
i
parameter at update k, it is thresholded from below ensuring
that the algorithm will converge:

ak
i =

(gj

i )2,

(cid:118)
(cid:117)
(cid:117)
(cid:116)

k
(cid:88)

j=0

i = maximum(1, ak
ρk

i ),

(27)

(28)

and

giving

(24)

1
ρi
In the initial stages of training, accumulated norm of the per-
parameter gradients can be less than 1. If the accumulated

i ˜gk
ηk
i .

i =

(29)

∆k

> ηk

i = Ek[tk

i where ηk

per-parameter norm of a gradient is less than 1, Adagrad will
augment the learning-rate determined by AdaSecant for that
update, i.e. ηk
i ] is the per-parameter
i
ρk
i
learning rate determined by AdaSecant. This behavior tends
to create unstabilities during the training with AdaSecant. Our
modiﬁcation of the Adagrad algorithm is to ensure that, it
will reduce the learning rate determined by the AdaSecant
algorithm at each update, i.e. ηk
i and the learning rate
i
ρk
i
will be bounded. At the beginning of the training, parameters
of a neural network can get 0-valued gradients, e.g. in the ex-
istence of dropout and ReLU units. However this phenomena
can cause the per-parameter learning rate scaled by Adagrad
to be unbounded.

≤ ηk

In Algorithm 1, we provide a simple pseudo-code of the

AdaSecant algorithm.

Algorithm 1: AdaSecant: minibatch-AdaSecant for adap-
tive learning rates with variance reduction

repeat

(cid:80)n

k=1 ∇(k)

draw n samples, compute the gradients g(j) where
g(j) ∈ Rn for each minibatch j, g(j) is computed
as, 1
θ f(θ)
n
estimate E[g] via moving averages.
block-wise normalize gradients of each weight matrix
and bias vector
for parameter i ∈ {1, . . . , n} do

i−E[gi]))]k

i)(gi−E[gi])]k

E[(gi−E[gi])(g(cid:48)

compute the correction term by using,
i = E[(gi−g(cid:48)
γk
compute corrected gradients ˜gi = gi+γiE[gi]
if |g(j)
i − E[gi]| >
2(cid:112)E[(gi)2] − (E[gi])2
or
2(cid:112)E[(αi)2] − (E[αi])2 then

(cid:12)
(cid:12)α(j)
(cid:12)

i − E[αi]

1+γi

(cid:12)
(cid:12)
(cid:12) >

reset the memory size for outliers τi ← 2.2

end

update moving averages according to Equation 23

)2]

η(j)
i ←

estimate learning rate
(cid:113)
Ek[(∆(k)
(cid:112)Ek[(αk

i ∆k
i ]
i
i )2]
i )2]
update memory size as in Equation 25
update parameter θj

Ek[αk
Ek[(αk

i − η(j)

i ← θj−1

−

i

· ˜g(j)
i

end

until stopping criterion is met;

IX. EXPERIMENTS

We have run experiments on character-level PTB with
GRU units, on MNIST with Maxout Networks [16] and on
handwriting synthesis using the IAM-OnDB dataset [17]. We
compare AdaSecant with popular stochastic gradient learning
algorithms: Adagrad, RMSProp [18], Adadelta [19], Adam
[20] and SGD+momentum (with linearly decaying learning

rate). AdaSecant performs as well or better as carefully tuned
algorithms for all these different tasks.

A. Ablation Study

Model

Train Log-Loss

Valid Log-Loss

Adam with 3e-4 learning rate
Adam with 1e-4 learning rate
Adam with 5e-4 learning rate
AdaSecant
AdaSecant, no VR
AdaSecant, no AG
AdaSecant, no BN
AdaSecant, no OD
AdaSecant, no VR, no AG
AdaSecant, no VR, no BN
AdaSecant, no VR, no OD
AdaSecant, no AG, no BN
AdaSecant, no AG, no OD
AdaSecant, no BN, no OD
AdaSecant, no AG, no BN, no OD
AdaSecant, no VR, no BN, no OD
AdaSecant, no VR, no AG, no OD
AdaSecant, no VR, no AG, no BN

-1.827
-1.780
-1.892
-1.881
-1.876
-1.867
-1.857
-1.780
-1.848
-1.844
-1.479
-1.878
-1.723
-1.814
-1.611
-1.531
unstable
-1.862

-1.743
-1.713
-1.773
-1.744
-1.743
-1.738
-1.784
-1.726
-1.744
-1.777
-1.442
-1.786
-1.674
-1.764
-1.573
-1.491
unstable
1.75

TABLE I
SUMMARY OF RESULTS FOR THE HANDWRITING EXPERIMENT. WE
REPORT THE BEST VALIDATION LOG-LOSS THAT WE FOUND FOR EACH
MODEL USING EARLY STOPPING. WE ALSO REPORT THE CORRESPONDING
TRAIN LOG-LOSS. IN ALL CASES, THE LOG-LOSS IS COMPUTED PER DATA
POINT.

In this section, we decompose the different parts of the
algorithm to measure the effect they have in the performance.
For this comparison, we trained a model to learn handwriting
synthesis on IAM-OnDB dataset. Our model follows closely
the architecture introduced in [18] with two modiﬁcations.
First, we use one recurrent layer of size 400 instead of three.
Second, we use GRU [21] units instead of LSTM [22] units.
Also, we use a different symbol for each of the 87 different
characters in the dataset. The code for this experiment is
available online.2

We tested different conﬁgurations that included taking away
the use of Variance Reduction (VR), Adagrad (AG), Block
Normalization (BN), and Outlier Detection (OD). Also, we
compared against ADAM [20] with different learning rates in
Figure 1. There, we observe that adasecant performs as well
as Adam with a carefully tuned learning rate.

In Figure 2, we disable each of the four components
of the algorithm. We ﬁnd that BN provides a small, but
constant advantage in performance. OD is also important for
the algorithm. Disabling OD makes training more noisy and
unstable and gives worse results. Disabling VR also makes
training unstable. AG has the least effect in the performance
of the algorithm. Furthermore, disabling more than one com-
ponent makes training even more unstable in the majority of
scenarios. A summary of the results is available in Table I.
In all cases, we use early stopping on the validation log-loss.
Furthermore, we present the train log-loss corresponding to

2https://github.com/sotelo/scribe

the best validation loss as well. Let us note that the log-loss
is computed per data point.

Fig. 1. Baseline comparison against Adam. AdaSecant performs as well as
Adam with a carefully tuned learning rate.

B. PTB Character-level LM

We have run experiments with GRU-RNN[21] on PTB
dataset for character-level language modeling over the subset
deﬁned in [23]. On this task, we use 400 GRU units with
minibatch size of 20. We train the model over the sequences of
length 150. For AdaSecant, we have not run any hyperparmeter
search, but for Adam we run a hyperparameter search for the
learning rate and gradient clipping. The learning rates are sam-
pled from log-uniform distribution between 1e − 1 and 6e − 5.
Gradient clipping threshold is sampled uniformly between 1.2
to 20. We have evaluated 20 different pairs of randomly-
sampled learning rates and gradient clipping thresholds. The
rest of the hyper-parameters are ﬁxed to their default values.
We use the model with the best validation error for Adam. For
AdaSecant algorithm, we ﬁx all the hyperparameters to their

Fig. 2. Deactivating one component at a time. BN provides a small but
constant advantage in performance. OD is important for the algorithm.
Deactivating it makes training more noisy and unstable and gives worse
results. Deactivating VR also makes training unstable.

default values. The learning curves for the both algorithms are
shown in Figure 3.

C. MNIST with Maxout Networks

The results are summarized in Figure 4 and we show
that AdaSecant converges as fast or faster than other tech-
niques, including the use of hand-tuned global learning rate
and momentum for SGD, RMSprop, and Adagrad. In our
experiments with AdaSecant algorithm, adaptive momentum
term γk
i was clipped at 1.8. In 2-layer Maxout network
experiments for SGD-momentum experiments, we used the
best hyper-parameters reported by [16], for RMSProp and
Adagrad, we crossvalidated learning rate for 15 different
learning rates sampled uniformly from the log-space. We
crossvalidated 30 different pairs of momentum and learning
rate for SGD+momentum, for RMSProp and Adagrad, we

(a) 2 layer Maxout Network

Fig. 3. Learning curves for the very well-tuned Adam vs AdaSecant algorithm
without any hyperparameter tuning. AdaSecant performs very close to the very
well-tuned Adam on PTB character-level language modeling task. This shows
us the robustness of the algorithm to its hyperparameters.

crossvalidated 15 different learning rates sampled them from
log-space uniformly for deep maxout experiments.

X. CONCLUSION

We described a new stochastic gradient algorithm with adap-
tive learning rates that is fairly insensitive to the tuning of the
hyper-parameters and doesn’t require tuning of learning rates.
Furthermore, the variance reduction technique we proposed
improves the convergence when the stochastic gradients have
high variance. Our algorithm performs as well or better than
other popular, carefully-tuned stochastic gradient algorithms.
We also present a comprehensive ablation study where we
show the effects and importance of each of the elements of
our algorithm. As future work, we should try to ﬁnd theoretical
convergence properties of the algorithm to understand it better
analytically.

ACKNOWLEDGMENTS

We thank the developers of Theano [24], Pylearn2 [25]
and Blocks [26] and the computational resources provided
by Compute Canada and Calcul Qu´ebec. This work has been
partially supported by NSERC, CIFAR, and Canada Research
Chairs, Project TIN2013-41751, grant 2014-SGR-221. Jose
Sotelo also thanks the Consejo Nacional de Ciencia y Tec-
nolog´ıa (CONACyT) as well as the Secretar´ıa de Educaci´on
P´ublica (SEP) for their support. We would like to thank Tom
Schaul for the valuable discussions. We also thank Kyunghyun
Cho and Orhan Firat for proof-reading and giving feedbacks
on the paper.

REFERENCES

[1] C. Gulcehre, M. Moczulski, and Y. Bengio, “Adasecant: robust adaptive
secant method for stochastic gradient,” arXiv preprint arXiv:1412.7419,
2014.

(b) 16 layer Maxout Network

Fig. 4. Comparison of different stochastic gradient algorithms on MNIST
with Maxout Networks. Both a) and b) are trained with dropout and maximum
column norm constraint regularization on the weights. Networks are initialized
with weights sampled from a Gaussian distribution with 0 mean and standard
deviation of 0.05. In both experiments, the proposed algorithm, AdaSecant,
seems to be converging faster and arrives to a better minima in training set.
We trained both networks for 350 epochs over the training set.

[2] S. Becker and Y. Le Cun, “Improving the convergence of back-
propagation learning with second order methods,” in Proceedings of the
1988 connectionist models summer school. San Matteo, CA: Morgan
Kaufmann, 1988, pp. 29–37.

[3] T. Schaul, S. Zhang, and Y. LeCun, “No more pesky learning rates,”

arXiv preprint arXiv:1206.1106, 2012.

[4] Y. LeCun, P. Y. Simard, and B. Pearlmutter, “Automatic learning
rate maximization by on-line estimation of the hessians eigenvectors,”
Advances in neural information processing systems, vol. 5, pp. 156–163,
1993.

[5] Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. M¨uller, “Efﬁcient
Springer, 2012,

backprop,” in Neural networks: Tricks of the trade.
pp. 9–48.

[6] T. Schaul and Y. LeCun, “Adaptive learning rates and paralleliza-
tion for stochastic, sparse, non-smooth gradients,” arXiv preprint
arXiv:1301.3764, 2013.

[7] Y. Levin and A. Ben-Israel, “Directional newton methods in n variables,”
Mathematics of Computation, vol. 71, no. 237, pp. 251–262, 2002.
[8] H.-B. An and Z.-Z. Bai, “Directional secant method for nonlinear
equations,” Journal of computational and applied mathematics, vol. 175,
no. 2, pp. 291–304, 2005.

[9] N. N. Schraudolph, “Fast curvature matrix-vector products for second-

order gradient descent,” Neural computation, vol. 14, no. 7, pp. 1723–
1738, 2002.

[10] C. Wang, X. Chen, A. Smola, and E. Xing, “Variance reduction for
stochastic gradient optimization,” in Advances in Neural Information
Processing Systems, 2013, pp. 181–189.

[11] R. Johnson and T. Zhang, “Accelerating stochastic gradient descent
using predictive variance reduction,” in Advances in Neural Information
Processing Systems, 2013, pp. 315–323.

[12] Y. Bengio, P. Simard, and P. Frasconi, “Learning long-term dependencies
with gradient descent is difﬁcult,” IEEE transactions on neural networks,
vol. 5, no. 2, pp. 157–166, 1994.

[13] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber, “Gradient
ﬂow in recurrent nets: the difﬁculty of learning long-term dependencies,”
2001.

[14] H. Robbins and S. Monro, “A stochastic approximation method,” The

annals of mathematical statistics, pp. 400–407, 1951.

[15] J. Duchi, E. Hazan, and Y. Singer, “Adaptive subgradient methods for
online learning and stochastic optimization,” The Journal of Machine
Learning Research, vol. 12, pp. 2121–2159, 2011.

[16] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Ben-
gio, “Maxout networks,” arXiv preprint arXiv:1302.4389, 2013.
[17] M. Liwicki and H. Bunke, “Iam-ondb - an on-line english sentence
database acquired from handwritten text on a whiteboard.” in ICDAR.
IEEE Computer Society, 2005, pp. 956–961.

[18] A. Graves, “Generating sequences with recurrent neural networks,” arXiv

[19] M. D. Zeiler, “Adadelta: An adaptive learning rate method,” arXiv

preprint arXiv:1308.0850, 2013.

preprint arXiv:1212.5701, 2012.

[20] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
in International Conference on Learning Representations, 2015.
[21] K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y. Bengio, “Learning phrase representations using
rnn encoder-decoder for statistical machine translation,” arXiv preprint
arXiv:1406.1078, 2014.

[22] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
Comput., vol. 9, no. 8, pp. 1735–1780, Nov. 1997. [Online]. Available:
http://dx.doi.org/10.1162/neco.1997.9.8.1735

[23] T. Mikolov, I. Sutskever, A. Deoras, H. Le, S. Kombrink, and J. Cer-
nocky, “Subword language modeling with neural networks,” preprint,
2012.

[24] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. Goodfellow, A. Berg-
eron, N. Bouchard, D. Warde-Farley, and Y. Bengio, “Theano: new
features and speed improvements,” Deep Learning and Unsupervised
Feature Learning NIPS 2012 Workshop, 2012.

[25] I. J. Goodfellow, D. Warde-Farley, P. Lamblin, V. Dumoulin, M. Mirza,
R. Pascanu, J. Bergstra, F. Bastien, and Y. Bengio, “Pylearn2: a machine
learning research library,” arXiv preprint arXiv:1308.4214, 2013.
[26] B. van Merri¨enboer, D. Bahdanau, V. Dumoulin, D. Serdyuk, D. Warde-
Farley, J. Chorowski, and Y. Bengio, “Blocks and Fuel: Frameworks for
deep learning,” ArXiv e-prints, jun 2015.

[27] Theano Development Team, “Theano: A Python framework for
fast computation of mathematical expressions,” arXiv e-prints, vol.
abs/1605.02688, May 2016. [Online]. Available: http://arxiv.org/abs/
1605.02688

APPENDIX

A. Derivation of Equation 18

∂E[(βigi + (1 − βi)E[gi] − g(cid:48)
∂βi

i)2]

+ λβ2

i = 0

E[(βigi + (1 − βi)E[gi] − g(cid:48)
i)
∂(βigi + (1 − βi)E[gi] − g(cid:48)
i)
∂βi

] + λβi = 0

E[(βigi + (1 − βi)E[gi] − g(cid:48)

i)(gi − E[gi])] + λβi = 0

E[(βigi(gi − E[gi]) + (1 − βi)E[gi](gi − E[gi])

− g(cid:48)

i(gi − E[gi])] + λβi

= 0

E[(gi − E[gi])(g(cid:48)

i − E[gi])]

E[(gi − E[gi])(gi − E[gi])] + λ
E[(gi − E[gi])(g(cid:48)

i − E[gi])]

βi =

=

Var(gi) + λ

B. Further Experimental Details

In Figure 5, we analyzed the effect of using different
minibatch sizes for AdaSecant and compared its convergence
with Adadelta in wall-clock time. For minibatch size 100
AdaSecant was able to reach the almost same training negative
log-likelihood as Adadelta after the same amount of time, but
its convergence took much longer. With minibatches of size
500 AdaSecant was able to converge faster in wallclock time
to a better local minima.

Fig. 5. In this plot, we compared AdaSecant trained by using minibatch size
of 100 and 500 with adadelta using minibatches of size 100. We performed
these experiments on MNIST with 2-layer maxout MLP using dropout.

C. More decomposition experiments

We have run experiments with the different combinations
of the components of the algorithm. We show those results on
handwriting synthesis with IAM-OnDB dataset. The results
can be observed from Figure 6, Figure 7, Figure 8, and
Figure 9 deactivating the components leads to a more unstable
training curve in the majority of scenarios.

Fig. 6. No variance reduction comparison.

Fig. 8. No block normalization comparison.

Fig. 7. No Adagrad comparison.

Fig. 9. No outlier detection comparison.

