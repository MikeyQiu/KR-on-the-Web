5
1
0
2
 
y
a
M
 
7
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
2
5
8
3
.
6
0
4
1
:
v
i
X
r
a

A low variance consistent test of relative dependency

Wacha Bounliphone
CentraleSup´elec & Inria, Grande Voie des Vignes, 92295 Chˆatenay-Malabry, France

WACHA.BOUNLIPHONE@CENTRALESUPELEC.FR

Arthur Gretton
Gatsby Computational Neuroscience Unit, University College London, United Kingdom

ARTHUR.GRETTON@GMAIL.COM

Arthur Tenenhaus
CentraleSup´elec, 3 rue Joliot-Curie, 91192 Gif-Sur-Yvette, France

ARTHUR.TENENHAUS@CENTRALESUPELEC.FR

Matthew B. Blaschko
Inria & CentraleSup´elec, Grande Voie des Vignes, 92295 Chˆatenay-Malabry, France

MATTHEW.BLASCHKO@INRIA.FR

Abstract

1. Introduction

We describe a novel non-parametric statistical
hypothesis test of relative dependence between
a source variable and two candidate target vari-
ables.
Such a test enables us to determine
whether one source variable is signiﬁcantly more
dependent on a ﬁrst target variable or a sec-
ond. Dependence is measured via the Hilbert-
Schmidt Independence Criterion (HSIC), result-
ing in a pair of empirical dependence mea-
sures (source-target 1, source-target 2). We test
whether the ﬁrst dependence measure is signif-
icantly larger than the second. Modeling the
covariance between these HSIC statistics leads
to a provably more powerful test than the con-
struction of independent HSIC statistics by sub-
sampling. The resulting test is consistent and
unbiased, and (being based on U-statistics) has
favorable convergence properties. The test can
be computed in quadratic time, matching the
computational complexity of standard empiri-
cal HSIC estimators. The effectiveness of the
test is demonstrated on several real-world prob-
lems: we identify language groups from a mul-
tilingual corpus, and we prove that tumor lo-
cation is more dependent on gene expression
than chromosomal imbalances. Source code is
available for download at https://github.
com/wbounliphone/reldep.

Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copy-
right 2015 by the author(s).

Tests of dependence are important tools in statistical anal-
ysis, and are widely applied in many data analysis con-
texts. Classical criteria include Spearman’s ρ and Kendall’s
τ , which can detect non-linear monotonic dependencies.
More recent research on dependence measurement has fo-
cused on non-parametric measures of dependence, which
apply even when the dependence is nonlinear, or the vari-
ables are multivariate or non-euclidean (for instance im-
ages, strings, and graphs). The statistics for such tests are
diverse, and include kernel measures of covariance (Gret-
ton et al., 2008; Zhang et al., 2011) and correlation (Daux-
ois & Nkiet, 1998; Fukumizu et al., 2008), distance co-
variances (which are instances of kernel tests) (Sz´ekely
et al., 2007; Sejdinovic et al., 2013b), kernel regression
tests (Cortes et al., 2009; Gunn & Kandola, 2002), rank-
ings (Heller et al., 2013), and space partitioning approaches
(Gretton & Gyorﬁ, 2010; Reshef et al., 2011; Kinney & At-
wal, 2014). Specialization of such methods to univariate
linear dependence can yield similar tests to classical ap-
proaches such as Darlington (1968); Bring (1996).

For many problems in data analysis, however, the question
of whether dependence exists is secondary: there may be
multiple dependencies, and the question becomes which
dependence is the strongest. For instance, in neuroscience,
multiple stimuli may be present (e.g. visual and audio), and
it is of interest to determine which of the two has a stronger
inﬂuence on brain activity (Trommershauser et al., 2011).
In automated translation (Peters et al., 2012), it is of inter-
est to determine whether documents in a source language
are a signiﬁcantly better match to those in one target lan-
guage than to another target language, either as a measure
of difﬁculty of the respective learning tasks, or as a basic
tool for comparative linguistics.

A low variance consistent test of relative dependency

We present a statistical test which determines whether two
target variables have a signiﬁcant difference in their de-
pendence on a third, source variable. The dependence be-
tween each of the target variables and the source is com-
puted using the Hilbert-Schmidt Independence Criterion
(Gretton et al., 2005; 2008).1 Care must be taken in an-
alyzing the asymptotic behavior of the test statistics, since
the two measures of dependence will themselves be cor-
related: they are both computed with respect to the same
source. Thus, we derive the joint asymptotic distribution
of both dependencies. The derivation of our test utilizes
classical results of U -statistics (Hoeffding, 1963; Serﬂing,
1981; Arcones & Gine, 1993). In particular, we make use
of results by Hoeffding (1963) and Serﬂing (1981) to deter-
mine the asymptotic joint distributions of the statistics (see
Theorem 4). Consequently, we derive the lowest variance
unbiased estimator of the test statistic.

We prove our approach to have greater statistical power
than constructing two uncorrelated statistics on the same
data by subsampling, and testing on these. In experiments,
we are able to successfully test which of two variables is
most strongly related to a third, in synthetic examples, in a
language group identiﬁcation task, and in a task for iden-
tifying the relative strength of factors for Glioma type in a
pediatric patient population.

there do not exist competing non-
To our knowledge,
parametric tests to determine which of two dependencies
is strongest. One related area is that of multiple regression
analysis (e.g. (Sen & Srivastava, 2011)). In this case a lin-
ear model is assumed, and it is determined whether individ-
ual inputs have a statistically signiﬁcant effect on an output
variable. The procedure does not address the question of
whether the inﬂuence of one variable is higher than that of
another to a statistically signiﬁcant degree. The problem of
variable selection has also been investigated in the case of
nonlinear relations between the inputs and outputs (Cortes
et al., 2009; 2012; Song et al., 2012), however this again
does not address which of two variables most strongly in-
ﬂuences a third. A less closely related area is that of detect-
ing three-variable interactions (Sejdinovic et al., 2013a),
where it is determined whether there exists any factoriza-
tion of the joint distribution over three variables. This test
again does not address the issue of ﬁnding which connec-
tions are strongest, however.

1Dependency can also be tested with the correlation operator.
However, Fukumizu et al., (2007) show that unlike the covariance
operator, the asymptotic distribution of the norm of the correlation
operator is unknown, so the construction of a computationally ef-
ﬁcient test of relative dependence remains an open problem.

2. Deﬁnitions and description of HSIC

We base our underlying notion of dependence on the
Hilbert-Schmidt Independence Criterion (Gretton et al.,
2005; 2008; Song et al., 2012). All results in this section
except for Problem 1 can be found in these previous works.

Deﬁnition 1.
Hilbert-Schmidt Independence Criterion)

(Gretton et al., 2005, Deﬁnition 1,Lemma 1:

Let Pxy be a Borel probability measure over over (X ×
Y, Γ × Λ) with Γ and Λ the respective Borel sets on X and
Y, and Px and Py the marginal distributions on domains
X and Y. Given separable RKHSs F and G,
the Hilbert-
Schmidt Independence Criterion (HSIC) is deﬁned as the
squared HS-norm of the associated cross-covariance oper-
ator Cxy. When the kernels k, l are associated uniquely
withs respective RKHSs F and G and bounded, HSIC can
be expressed in terms of expectations of kernel functions

HSIC(F, G, Pxy) := (cid:107)Cxy(cid:107)2
HS
= Exx(cid:48)yy(cid:48) [k(x, x(cid:48))l(y, y(cid:48))] + Exx(cid:48) [k(x, x(cid:48))] Eyy(cid:48) [l(y, y(cid:48))]
− 2Exy [Ex(cid:48)[k(x, x(cid:48))]Ey(cid:48)[l(y, y(cid:48))]] .
(1)

HSIC determines independence: HSIC = 0 iff Pxy = PxPy
when kernels k and l are characteristic on their respective
marginal domains (Gretton, 2015).

With this choice, the problem we would like to solve is
described as follows:

Problem 1. Given separable RKHSs F, G, and H with
HSIC(F, G, Pxy) > 0 and HSIC(F, H, Pxz) > 0,
we test the null hypothesis H0 : HSIC(F, G, Pxy) ≤
HSIC(F, H, Pxz) versus the alternative hypothesis H1 :
HSIC(F, G, Pxy) > HSIC(F, H, Pxz) at a given sig-
niﬁcance level α.

We now describe the asymptotic behavior of the HSIC for
dependent variables.

(Song et al., 2012, Theorem 2: Unbiased
Theorem 1.
estimator for HSIC(F, G, Pxy)) We denote by S the set
of observations {(x1, y1), ..., (xm, ym)} of size m drawn
i.i.d. from Pxy. The unbiased estimator HSICm(F, G, S)
is given by

HSICm(F, G, S) =

1
m(m − 3)

×

(cid:34)

Tr( ˜K˜L) +

1(cid:48) ˜K11(cid:48) ˜L1
(m − 1)(m − 2)

−

2
m − 2

(2)

(cid:35)
1(cid:48) ˜K˜L1

where ˜K and ˜L are related to K and L by ˜Kij = (1 −
δij) ˜Kij and ˜Lij = (1 − δij)˜Lij.
(Song et al., 2012, Theorem 3: U-statistic of
Theorem 2.
HSIC) This ﬁnite sample unbiased estimator of HSIC XY
m

A low variance consistent test of relative dependency

can be written as a U-statistic,

HSIC XY

m = (m)−1
4

hijqr

(3)

(cid:88)

(i,j,q,r)∈im
4

m!
(m − 4)!

where (m)4 :=

4 denotes the
set of all 4−tuples drawn without replacement from the set
{1, . . . m}, and the kernel h of the U-statistic is deﬁned as

, the index set im

hijqr =

kst(lst + luv − 2lsu)

(4)

1
24

(i,j,q,r)
(cid:88)

(s,t,u,v)

where the kernels k and l are associated uniquely with re-
spective reproducing kernel Hilbert spaces F and G.

(Gretton et al., 2008, Theorem 1: Asymptotic
Theorem 3.
distribution of HSICm) If E[h2] < ∞, and source and
targets are not independent, then, under H1, as m → ∞,

√

d−→ N (0, σ2

m(HSIC XY

m − HSIC(F, G, Pxy))

where σ2
with Ej,q,r
:= ESj ,Sq,Sr .
mate is ˆσXY = 16 (cid:0)RXY − (HSIC XY

XY )
(5)
Ei (Ej,q,rhijqr)2 − HSIC(F, G, Pxy))
Its empirical esti-
m )2(cid:1) where

2

XY = 16

(cid:16)

(cid:17)

RXY =

m
(cid:88)


(m − 1)−1
3

1
m

(cid:88)

hijqr



and

(j,q,r)∈im
i=1
3 \ {i} denotes the set of all 3−tuples drawn

the index set im
without replacement from the set {1, . . . m} \ {i}.

3 \{i}

3. A test of relative dependence

In this section we calculate two dependent HSIC statistics
and derive the joint asymptotic distribution of these depen-
dent quantities, which is used to construct a consistent test
for Problem 1. We next construct a simpler consistent test,
by computing two independent HSIC statistics on sample
subsets. While the simpler strategy is superﬁcially attrac-
tive and less effort to implement, we prove the dependent
strategy is strictly more powerful.

3.1. Joint asymptotic distribution of HSIC and test

m and HSIC XZ

In the present section, we compute each HSIC estimate
on the full dataset, and explicitly obtain the correlations
between the resulting empirical dependence measurements
HSIC XY
m . We denote by S1 = (X, Y, Z)
the joint sample of observations which are drawn i.i.d. with
respective Borel probability measure Pxyz deﬁned on the
domain X × Y × Z. The kernels k, l and d are associated
uniquely with respective reproducing kernel Hilbert spaces
F, G and H. Moreover, K, L and D ∈ Rm×m are kernel
matrices containing kij = k(xi, xj), lij = l(yi, yj) and

1
24

1
24

(i,j,q,r)
(cid:88)

(s,t,u,v)

(i,j,q,r)
(cid:88)

(s,t,u,v)

dij = d(zi, zj). Let HSIC XY
m be respec-
tively the unbiased estimators of HSIC(F, G, Pxy) and
HSIC(F, H, Pxz), written as a sum of U-statistics with
respective kernels hijqr and gijqr as described in (4),

m and HSIC XZ

hijqr =

kst(lst + luv − 2lsu),

gijqr =

kst(dst + duv − 2dsu).

(6)

Theorem 4. (Joint asymptotic distribution of HSIC) If
E[h2] < ∞ and E[g2] < ∞, then

(cid:18)(cid:18)

√

m

HSIC XY
m
HSIC XZ
(cid:18)(cid:16)0
(cid:17)
0

m

d−→ N

(cid:19)

−

(cid:18)HSIC(F, G, Pxy)
HSIC(F, H, Pxz)
(cid:19)(cid:19)

(cid:19)(cid:19)

(cid:18) σ2
XY
σXY XZ

,

σXY XZ
σ2
XZ

,

(7)

XY and σ2

estimate of σXY XZ is

XZ are as in Theorem 3.
ˆσXY XZ

The
=

(cid:0)RXY XZ − HSIC XY

m HSIC XZ
m

(cid:1), where

where σ2
empirical
16
m

RXY XZ =


(m − 1)−2
3

1
m

m
(cid:88)

i=1

(cid:88)

hijqrgijqr

 .

(j,q,r)∈im

3 \{i}



(8)

Proof. Eq.
(8) is constructed with the deﬁnition of vari-
ance of a U-statistic as given by Serﬂing, Ch. 5 (1981),
where one variable is ﬁxed. Eq. (7) follows from the appli-
cation of Hoeffding, Theorem 7.1 (1963), which gives the
joint asymptotic distribution of U-statistics.

Based on the joint asymptotic distribution of HSIC de-
scribed in Theorem 4, we can now describe a statistical
test to solve Problem 1: given a sample S1 as described
in Section 3.1, T (S1) : {(X × Y × Z)m} → {0, 1} is
used to test the null hypothesis H0 : HSIC(F, G, Pxy) ≤
HSIC(F, H, Pxz) versus the alternative hypothesis H1 :
HSIC(F, G, Pxy) > HSIC(F, H, Pxz) at a given sig-
niﬁcance level α. This is achieved by projecting the distri-
m − HSIC XZ
bution to 1D using the statistic HSIC XY
m ,
and determining where the statistic falls relative to a
conservative estimate of the the 1 − α quantile of the
null. We now derive this conservative estimate. A sim-
ple way of achieving this is to rotate the distribution by
π
4 counter-clockwise about the origin, and to integrate
the resulting distribution projected onto the ﬁrst axis (cf.
Fig. 3). Denote the asymptotically normal distribution of
√
m ]T as N (µ, Σ). The distribution

m[HSIC XY

m HSIC XZ
resulting from rotation and projection is
(cid:1) ,
N (cid:0)[Qµ]1, [QΣQT ]11

(9)

A low variance consistent test of relative dependency

Then for m > 1 and all δ > 0 with probability at least
1 − δ, for all pxyz, the generalization bound on the differ-
ence of empirical HSIC statistics is

(10)

(11)

| {HSIC(F, G, Pxy) − HSIC(F, H, Pxz)}

− (cid:8)HSIC XY

m − HSIC XZ
m

(cid:9) |

(cid:40)(cid:114)

≤ 2

(cid:41)

log(6/δ)
α2m

+

C
m

(14)

√

2
2

1
2

where Q =

is the rotation matrix by π

4 and

√

2
2

(cid:17)
(cid:16)1 −1
1
1

[Qµ]1 =

(HSIC(F, G, Pxy) − HSIC(F, H, Pxz)) ,

[QΣQT ]11 =

(σ2

XY + σ2

XZ − 2σXY XZ).

Following the empirical distribution from Eq. (9), a test
m − HSIC XZ
with statistic HSIC XY

m has p-value

(cid:32)

p ≤ 1 − Φ

(HSIC XY
(cid:112)σ2
XY + σ2

m − HSIC XZ
m )
XZ − 2σXY XZ

(cid:33)

,

(12)

where Φ is the CDF of a standard normal distribution, and
we have made the most conservative possible assumption
that HSIC(F, G, Pxy) − HSIC(F, H, Pxz) = 0 under
the null (the null also allows for the difference in population
dependence measures to be negative).

in practice,

XZ and σ2

the test
the variances of
To implement
XY , σ2
σ2
XY XZ may be replaced by their empir-
ical estimates. The test will still be consistent for a large
enough sample size, since the estimates will be sufﬁciently
well converged to ensure the test is calibrated. Eq. (8) is
expensive to compute na¨ıvely, because even computing the
kernels hijqr and gijqr of the U -statistic itself is a non
trivial task. Following (Song et al., 2012, Section 2.5),
we ﬁrst form a vector hXY with entries corresponding to
(cid:80)
3 \{i} hijqr, and a vector hXZ with entries cor-
(j,q,r)∈im
responding to (cid:80)
3 \{i} gijqr. Collecting terms in
Eq. (4) related to kernel matrices ˜K and ˜L, hXY can be
written as

(j,q,r)∈im

(cid:17)
hXY = (m − 2)2 (cid:16) ˜K (cid:12) ˜L

1 − m( ˜K1) (cid:12) (˜L1)

(13)
(cid:17)
(Tr( ˜K˜L))1 − ˜K(˜L1) − ˜L( ˜K1)

(cid:16)

+ (m − 2)

+ (1T ˜L1) ˜K1 + (1T ˜K1)˜L1 − ((1T ˜K)(˜L1))1

3 hXY

where (cid:12) denotes the Hadamard product. Then RXY XZ
in Eq. (8) can be computed as RXY XZ = (4m)−1(m −
T hXZ. Using the order of operations implied by
1)−2
the parentheses in Eq. (13), the computational cost of the
cross covariance term is O(m2). Combining this with the
unbiased estimator of HSIC in Eq. (2) leads to a ﬁnal com-
putational complexity of O(m2).

In addition to the asymptotic consistency result, we provide
a ﬁnite sample bound on the deviation between the differ-
ence of two population HSIC statistics and the difference
of two empirical HSIC estimates.

Theorem 5 (Generalization bound on the difference of
empirical HSIC statistics). Assume that k, l, and d are
bounded almost everywhere by 1, and are non-negative.

where α > 0.24 and C are constants.

Proof. In Gretton et al., (2005) a ﬁnite sample bound is
given for a single HSIC statistic. Eq. (14) is proved by
using a union bound.

Corollary 1. HSIC XY
ulation statistic at rate O(

m −HSIC XZ
√
m).

m converges to the pop-

3.2. A simple consistent test via uncorrelated HSICs

From the result in Eq. (5), a simple, consistent test of
relative dependence can be constructed as follows: split
the samples from Px into two equal sized sets denoted
by X (cid:48) and X (cid:48)(cid:48), and drop the second half of the sam-
ple pairs with Y and the ﬁrst half of the sample pairs
with Z. We will denote the remaining samples as Y (cid:48)
and Z (cid:48)(cid:48). We can now estimate the joint distribution of
√

m[HSIC X (cid:48)Y (cid:48)

m/2 , HSIC X (cid:48)(cid:48)Z(cid:48)(cid:48)

m/2

]T as

(cid:18)(cid:18)HSIC(F, G, Pxy)
HSIC(F, H, Pxz)

(cid:19)

,

N

(cid:18)σ2

X (cid:48)Y (cid:48)
0

(cid:19)(cid:19)

0

σ2

X (cid:48)(cid:48)Z(cid:48)(cid:48)

, (15)

which we will write as N (µ(cid:48), Σ(cid:48)). Given this joint
distribution, we need to determine the distribution over
space deﬁned by HSIC(F, G, Pxy) <
the half
HSIC(F, H, Pxz). As in the previous section, we achieve
this by rotating the distribution by π
4 counter-clockwise
about the origin, and integrating the resulting distribution
projected onto the ﬁrst axis (cf. Fig. 3). The resulting pro-
jection of the rotated distribution onto the primary axis is
N (cid:0)[Qµ(cid:48)]1 , (cid:2)QΣ(cid:48)QT (cid:3)

(16)

(cid:1)

11

where

√

2
2

1
2

[Qµ(cid:48)]1 =

(HSIC(F, G, Pxy) − HSIC(F, H, Pxz)) ,

(17)

(18)

[QΣ(cid:48)QT ]11 =

(σ2

X (cid:48)Y (cid:48) + σ2

X (cid:48)(cid:48)Z(cid:48)(cid:48) ).

From this empirically estimated distribution, it is straight-
forward to construct a consistent test (cf. Eq. (12)). The
power of this test varies inversely with the variance of the
distribution in Eq. (16).

A low variance consistent test of relative dependency

3.3. The dependent test is more powerful

While discarding half the samples leads to a consistent test,
we might expect some loss of power over the approach in
Section 3.1, due to the increase in variance with lower sam-
ple size. In this section, we prove the Section 3.1 test is
more powerful than that of Section 3.2, regardless of Pxy
and Pxz.

We call the simple and consistent approach in Section 3.2,
the independent approach, and the lower variance approach
in Section 3.1, the dependent approach. The following the-
orem compares these approaches.

Theorem 6. The asymptotic relative efﬁciency (ARE) of the
independent approach relative to the dependent approach
is always greater than 1.

Remark 1. The asymptotic relative efﬁciency (ARE) is de-
ﬁned in e.g. Serﬂing (1981, Chap.5, Section 1.15.4). If mA
and mB are the sample sizes at which tests ”perform equiv-
alently” (i.e. have equal power), then the ratio mA
repre-
mB
sents the relative efﬁciency. When mA and mB tend to +∞
and the ratio mA
→ L (at equivalent performance), then
mB
the value L represents the asymptotic relative efﬁciency of
procedure B relative to procedure A. This example is rele-
vant to our case since we are comparing two test statistics
with different asymptotically Normal distributions.

The following lemma is used for the proof of Theorem 6.

Lemma 1. (Lower Variance) The variance of the depen-
dent test statistic is smaller than the variance of the inde-
pendent test statistic.

X (cid:48)Y (cid:48) = 2σ2
XY + σ2

Proof. From the convergence of moments in the applica-
tion of the central limit theorem (von Bahr, 1965), we
have that σ2
XY . Then the variance summary
in Eq. (11) is 1
2 (σ2
XZ − 2σXY XZ) and the variance
summary in Equation (18) is 1
XY + 2σ2
2 (2σ2
XZ) where in
√
m. We have that the
both cases the statistic is scaled by
variance of the independent test statistic is smaller than the
variance of the dependent test statistic when

1
2

(σ2

XY + σ2

1
XZ − 2σXY XZ) <
2
XY + σ2
⇐⇒ −2σXY XZ < σ2

(2σ2

XZ

XY + 2σ2

XZ)

(19)

which is implied by the positive deﬁniteness of Σ.

Proof of Theorem 6. The Type II error probability of the
independent test at level α is







Φ

Φ−1(1 − α) −

m−1/2(cid:0)HSIC(F, G, Pxy)
−HSIC(F, H, Pxz)(cid:1)
(cid:112)σ2

X (cid:48)Y (cid:48) + σ2

X (cid:48)(cid:48)Z(cid:48)(cid:48)







,

(20)

where we again make the most conservative possible as-
sumption that HSIC(F, G, Pxy) − HSIC(F, H, Pxz) =
0 under the null. The Type II error probability of the de-
pendent test at level α is







Φ

Φ−1(1 − α) −

m−1/2(cid:0)HSIC(F, G, Pxy)
−HSIC(F, H, Pxz)(cid:1)
XY + σ2
XZ − 2σXY XZ

(cid:112)σ2







(21)

where Φ is the CDF of the standard normal distribution.
The numerator in Eq. (20) is the same as the numerator in
Eq. (21), and the denominator in Eq. (21) is smaller due to
Lemma 1. The lower variance dependent test therefore has
higher ARE, i.e., for a sufﬁcient sample size m > τ for
some distribution dependent τ ∈ N+, the dependent test
will be more powerful than the independent test.

4. Generalizing to more than two HSIC

statistics

the dependence test

The generalization of
to more
than three random variables follows from the earlier
derivation by applying successive rotations to a higher
dimensional
joint Gaussian distribution over multiple
HSIC statistics. We assume a sample S of size m
over n domains with kernels k1, . . . , kn associated
uniquely with respective reproducing kernel Hilbert
spaces F1, . . . , Fn. We deﬁne a generalized statisti-
cal test, Tg(S) → {0, 1} to test the null hypothesis
H0
(x,y)∈{1,...,n}2 v(x,y)HSIC(Fx, Fy, Pxy) ≤
0
:
(cid:80)
(x,y)∈{1,...,n}2 v(x,y)HSIC(Fx, Fy, Pxy) > 0, where
v is a vector of weights on each HSIC statistic. We
in the previous section by set-
may recover the test
ting v(1,2) = +1 v(1,3) = −1 and v(i,j) = 0 for all
(i, j) ∈ {1, 2, 3}2 \ {(1, 2), (1, 3)}.

hypothesis Hm

: (cid:80)
versus

alternative

the

The derivation of the test follows the general strategy used
in the previous section: we construct a rotation matrix so
as to project the joint Gaussian distribution onto the ﬁrst
axis, and read the p-value from a standard normal table. To
construct the rotation matrix, we simply need to rotate v
such that it is aligned with the ﬁrst axis. Such a rotation
can be computed by composing n 2-dimensional rotation
matrices as in Algorithm 1.

5. Experiments

We apply our estimates of statistical dependence to three
challenging problems. The ﬁrst is a synthetic data experi-
ment, in which we can directly control the relative degree
of functional dependence between variates. The second ex-
periment uses a multilingual corpus to determine the rela-
tive relations between European languages. The last exper-

A low variance consistent test of relative dependency

Algorithm 1 Successive rotation for generalized high-
dimensional relative tests of dependency (cf. Section 4)
Require: v ∈ Rn
Ensure: [Qv]i = 0 ∀i (cid:54)= 1, QT Q = I

Q = I
for i = 2 to n do

vi
[Qv]1

Qi = I; θ = − tan−1
[Qi]11 = cos(θ); [Qi]1i = − sin(θ)
[Qi]i1 = sin(θ); [Qi]ii = cos(θ)
Q = QiQ

end for

)
1
,
0
(

N
1
γ
+
)
t
(
n
i
s

)
1
,
0
(

N
2
γ
+
)
t
(
n
i
s
t

)
1
,
0
(

N
3
γ
+
)
t
(
s
o
c
t

t + γ1N (0, 1)
(a) γ1 = 0.3

t cos(t) + γ2N (0, 1)
(b) γ2 = 0.3

t cos(t) + γ3N (0, 1)
(c) γ3 = 0.6

Figure 1. Illustration of a synthetic dataset sampled from the dis-
tribution in Eq. (22).

iment is a 3-block dataset which combines gene expression,
comparative genomic hybridization, and a qualitative phe-
notype measured on a sample of Glioma patients.

5.1. Synthetic experiment

We constructed 3 distributions as deﬁned in Eq. (22) and
illustrated in Figure 1.

Let t ∼ U[(0, 2π)],

(22)

(a) x1 ∼ t + γ1N (0, 1) y1 ∼ sin(t) + γ1N (0, 1)
(b) x2 ∼ t cos(t) + γ2N (0, 1) y2 ∼ t sin(t) + γ2N (0, 1)
(c) x3 ∼ t cos(t) + γ3N (0, 1) y3 ∼ t sin(t) + γ3N (0, 1)

These distributions are speciﬁed so that we can control the
relative degree of functional dependence between the vari-
ates by varying the relative size of noise scaling parameters
γ1, γ2 and γ3. The question is then whether the dependence
between (a) and (b) is larger than the dependence between
(a) and (c). In these experiments, we ﬁxed γ1 = γ2 = 0.3,
while we varied γ3, and used a Gaussian kernel with band-
width σ selected as the median pairwise distance between
data points. This kernel is sufﬁcient to obtain good perfor-
mance, although others choices exist (Gretton et al., 2012).

Figure 2. Power of the dependent and independent test as a func-
tion of γ3 on the synthetic data described in Section 5.1. For val-
ues of γ3 > 0.3 the distribution in Fig. 1(a) is closer to 1(b) than
to 1(c). The problem becomes difﬁcult as γ3 → 0.3. As predicted
by theory, the dependent test is signiﬁcantly more powerful over
almost all values of γ3 by a substantial margin.

5.2. Multilingual data

In this section, we demonstrate dependence testing to pre-
dict the relative similarity of different languages. We use
a real world dataset taken from the parallel European Par-
liament corpus (Koehn, 2005). We choose 3000 random
documents in common written in: Finnish (ﬁ), Italian (it),
French (fr), Spanish (es), Portuguese (pt), English (en),
Dutch (nl), German (de), Danish (da) and Swedish (sv).
These languages can be broadly categorized into either the
Romance, Germanic or Uralic groups (Gray & Atkinson,
2003). In this dataset, we considered each language as a
random variable and each document as an observation.

Our ﬁrst goal is to test if the statistical dependence between
two languages in the same group is greater than the sta-
tistical dependence between languages in different groups.
For pre-processing, we removed stop-words (http://
www.nltk.org) and performed stemming (http://
snowball.tartarus.org). We applied the TF-IDF
model as a feature representation and used a Gaussian ker-
nel with the bandwidth σ set per language as the median
pairwise distance between documents.

In Table 1, a selection of tests between language groups
(Germanic, Romance, and Uralic) is given: all p-values
strongly support that our relative dependence test ﬁnds the
different language groups with very high signiﬁcance.

Figure 2 shows the power of the dependent and the inde-
pendent tests as we vary γ3. It is clear from these results
that the dependent test is far more powerful than the inde-
pendent test over the great majority of γ3 values consid-
ered. Figure 3 demonstrates that this superior test power
arises due to the tighter and more concentrated distribution
of the dependent statistic.

Further, if we focus on the Romance family, our test en-
ables one to answer more ﬁne-grained questions about the
relative similarity of languages within the same group. As
before, we determine the ground truth similarities from the
topology of the tree of European languages determined by
the linguistics community (Gray & Atkinson, 2003; Bouck-
aert et al., 2012) as illustrated in Fig. 4 for the Romance

A low variance consistent test of relative dependency

(a) m=500, γ3 = 0.7
pdep = 0.0189, pindep = 0.3492

(b) m=1000, γ3 = 0.7
pdep = 10−4, pindep = 0.3690

(c) m=3000, γ3 = 0.7
pdep = 10−6, pindep = 0.2876

(d) m=500, γ3 = 1.7
pdep = 10−9, pindep = 0.982

(e) m=1000, γ3 = 1.7
pdep = 10−10, pindep = 0.0326

(f) m=3000, γ3 = 1.7
pdep = 10−13, pindep = 0.005

Figure 3. For the synthetic experiments described in Section 5.1, we plot empirical HSIC values for dependent and independent tests for
100 repeated draws with different sample sizes. Empirical p-values for each test show that the dependent distribution converges faster
than the independent distribution even at low sample size, resulting in a more powerful statistical test.

Source Target 1 Target 2 p-value
0.0066
0.0418
0.0169
0.0173
< 10−4
< 10−4
< 10−6
< 10−4
< 10−4

pt
it
es
es
nl
en
sv
en
de

ﬁ
da
ﬁ
da
ﬁ
es
fr
it
es

es
fr
it
pt
de
nl
da
sv
en

Table 1. A selection of relative dependency tests between two
pairs of HSIC statistics for the multilingual corpus data. Low p-
values indicate a source is closer to target 1 than to target 2. In all
cases, the test correctly identiﬁes that languages within the same
group are more strongly related than those in different groups.

group. We have run the test on all triplets from the cor-
pus for which the topology of the tree speciﬁes a correct
ordering of the dependencies. In a fraction of a second (ex-
cluding kernel computation), we are able to recover certain
features of the subtree of relationships between languages
present in the Romance language group (Table 2). The test
always indicates the correct relative similarity of languages
when nearby languages (pt,es) are compared with those fur-
ther away (ft,it), however errors are made when comparing
triplets of languages for which the nearest common ances-
tor is more than one link removed.

Figure 4. Partial tree of Romance languages adapted from (Gray
& Atkinson, 2003).

Source Target 1 Target 2 p-value
0.0157
0.1882
0.2147
< 10−4
< 10−4
0.7649
0.0011
< 10−8

es
pt
fr
pt
pt
fr
es
es

fr
fr
es
es
es
pt
pt
pt

it
it
it
it
fr
it
it
fr

Table 2. Relative dependency tests between Romance languages.
The tests are ordered such that a low p-value corresponds with a
conﬁrmation of the topology of the tree of Romance languages de-
termined by the linguistics community (Gray & Atkinson, 2003).

A low variance consistent test of relative dependency

dependent test
independent test

In our next tests, we evaluate our more general framework
for testing relative dependencies with more than two HSIC
statistics. We chose four languages, and tested whether the
average dependence between languages in the same group
is higher than the dependence between groups. The results
of these tests are in Table 3. As before, our test is able to
distinguish language groups with high signiﬁcance.

×10−3
6

5

4

3

2

1

0

Z
X
C
I
S
H

−1

−2

0

Source Targets p-value
de sv ﬁ < 10−9
sv en fr < 10−9
sv en it < 10−5
it es sv < 10−5
fr pt nl 0.0175

da
da
de
fr
es

Table 3. Relative dependency test between four pairs of HSIC
statistics for the multilingual corpus data. These tests show the
ability of the relative dependence test to generalize to arbitrary
numbers of HSIC statistics by constructing a rotation matrix us-
ing Algorithm 1. In all cases v = [1 1 −2].

5.3. Pediatric glioma data

Brain tumors are the most common solid tumors in children
and have the highest mortality rate of all pediatric cancers.
Despite advances in multimodality therapy, children with
pediatric high-grade gliomas (pHGG) invariably have an
overall survival of around 20% at 5 years. Depending on
their location (e.g. brainstem, central nuclei, or supraten-
torial), pHGG present different characteristics in terms of
radiological appearance, histology, and prognosis. The hy-
pothesis is that pHGG have different genetic origins and
oncogenic pathways depending on their location. Thus, the
biological processes involved in the development of the tu-
mor may be different from one location to another.

In order to evaluate such hypotheses, pre-treatment frozen
tumor samples were obtained from 53 children with newly
diagnosed pHGG from Necker Enfants Malades (Paris,
France) from Puget et al, (2012). The 53 tumors are di-
vided into 3 locations: supratentorial (HEMI), central nu-
clei (MIDL), and brain stem (DIPG). The ﬁnal dataset is or-
ganized in 3 blocks of variables deﬁned for the 53 tumors:
X is a block of indicator variables describing the location
category, the second data matrix Y provides the expression
of 15 702 genes (GE). The third data matrix Z contains the
imbalances of 1229 segments (CGH) of chromosomes.

For X, we use a linear kernel, which is characteristic for
indicator variables, and for Y and Z, the kernel was cho-
sen to be the Gaussian kernel with σ selected as the median
of pairwise distances. The p-value of our relative depen-
dency test is < 10−5. This shows that the tumor location
in the brain is more dependent on gene expression than on
chromosomal imbalances. By contrast with Section 5.1,
the independent test was also able to ﬁnd the same order-

1

2

4

5

6 ×10−3

3
HSIC XY

Figure 5. 2σ iso-curves of the Gaussian distributions estimated
from the pediatric Glioma data. As before, the dependent test
has a much lower variance than the independent test. The tests
support the stronger dependence on the tumor location to gene
expression than chromosomal imbalances.

ing of dependence, but with a p-value that is three orders of
magnitude larger (p = 0.005). Figure 5 shows iso-curves
of the Gaussian distributions estimated in the independent
and dependent tests. The empirical relative dependency is
consistent with ﬁndings in the medical literature, and pro-
vides additional statistical support for the importance of
tumor location in Glioma (Gilbertson & Gutmann, 2007;
Palm et al., 2009; Puget et al., 2012).

6. Conclusions

We have described a novel statistical test that determines
whether a source random variable is more strongly de-
pendent on one target random variable or another. This
test, built on the Hilbert-Schmidt Independence Criterion,
is low variance, consistent, and unbiased. We have shown
that our test is strictly more powerful than a test that does
not exploit the covariance between HSIC statistics, and
empirically achieves p-values several orders of magnitude
smaller. We have empirically demonstrated the test perfor-
mance on synthetic data, where the degree of dependence
could be controlled; on the challenging problem of iden-
tifying language groups from a multilingual corpus; and
for ﬁnding the most important determinant of Glioma type.
The computation and memory requirements of the test are
quadratic in the sample size, matching the performance of
HSIC and related tests for dependence between two ran-
dom variables. The test is therefore scalable to the wide
range of problem instances where non-parametric depen-
dency tests are currently applied. We have generalized the
test framework to more than two HSIC statistics, and have
given an algorithm to construct a consistent, low-variance,
unbiased test in this setting.

Acknowledgements

We thank Ioannis Antonoglou for helpful discussions. The
ﬁrst author is supported by a fellowship from Centrale-

A low variance consistent test of relative dependency

Sup´elec. This work is partially funded by the Euro-
pean Commission through ERC Grant 259112 and FP7-
MCCIG334380.

Gretton, A. and Gyorﬁ, L. Consistent nonparametric tests
Journal of Machine Learning Re-

of independence.
search, 11:1391–1423, 2010.

References

Arcones, M. A. and Gine, E. Limit theorems for U-
processes. The Annals of Probability, pp. 1494–1542,
1993.

Bouckaert, R., Lemey, P., Dunn, M., Greenhill, S. J., Alek-
seyenko, A. V., Drummond, A. J., Gray, R. D., Suchard,
M. A., and Atkinson, Q. D. Mapping the origins and ex-
pansion of the Indo-European language family. Science,
337(6097):957–960, 2012.

Bring, J. A geometric approach to compare variables in
a regression model. The American Statistician, 50(1):
57–62, 1996.

Cortes, C., Mohri, M., and Rostamizadeh, A. Learning
non-linear combinations of kernels. In Neural Informa-
tion Processing Systems, 2009.

Cortes, C., Mohri, M., and Rostamizadeh, A. Algorithms
for learning kernels based on centered alignment. Jour-
nal of Machine Learning Research, 13:795–828, 2012.

Darlington, Richard B. Multiple regression in psychologi-
cal research and practice. Psychological bulletin, 69(3):
161, 1968.

Dauxois, J. and Nkiet, G. M. Nonlinear canonical analy-
sis and independence tests. Annals of Statistics, 26(4):
1254–1278, 1998.

Fukumizu, K., Bach, F. R., and Gretton, A.

Statisti-
cal consistency of kernel canonical correlation analysis.
The Journal of Machine Learning Research, 8:361–383,
2007.

Fukumizu, K., Gretton, A., Sun, X., and Sch¨olkopf, B. Ker-
In Advances
nel measures of conditional dependence.
in Neural Information Processing Systems, pp. 489–496.
MIT Press, 2008.

Gretton, A., Bousquet, O., Smola, A. J., and Sch¨olkopf, B.
Measuring statistical dependence with Hilbert-Schmidt
In Algorithmic Learning Theory, pp. 63–77,
norms.
2005.

Gretton, A., Fukumizu, K., Teo, C.-H., Song, L.,
Sch¨olkopf, B., and Smola, A. J. A kernel statistical test
of independence. In Neural Information Processing Sys-
tems, pp. 585–592, 2008.

Gretton, A., Sejdinovic, D., Strathmann, H.and Balakrish-
nan, S., Pontil, M., Fukumizu, K., and Sriperumbudur,
B. K. Optimal kernel choice for large-scale two-sample
tests. In Advances in Neural Information Processing Sys-
tems, pp. 1205–1213, 2012.

Gunn, S. R. and Kandola, J. S. Structural modelling with
sparse kernels. Machine Learning, 48(1):137–163, 2002.

Heller, R., Heller, Y., and Gorﬁne, M. A consistent mul-
tivariate test of association based on ranks of distances.
Biometrika, 100(2):503–510, 2013.

Hoeffding, W. Probability inequalities for sums of bounded
random variables. Journal of the American statistical
association, 58(301):13–30, 1963.

Kinney, J. B. and Atwal, G. S. Equitability, mutual infor-
mation, and the maximal information coefﬁcient. Pro-
ceedings of the National Academy of Sciences, 2014.

Koehn, P. Europarl: A parallel corpus for statistical ma-
chine translation. In MT summit, volume 5, pp. 79–86,
2005.

Palm, T., Figarella-Branger, D., Chapon, F., Lacroix, C.,
Gray, F., Scaravilli, F., Ellison, D. W., Salmon, I.,
Vikkula, M., and Godfraind, C. Expression proﬁling
of ependymomas unravels localization and tumor grade-
speciﬁc tumorigenesis. Cancer, 115(17):3955–3968,
2009.

Gilbertson, R. J. and Gutmann, D. H. Tumorigenesis in the
brain: location, location, location. Cancer research, 67
(12):5579–5582, 2007.

Peters, C., Braschler, M., and Clough, P. Multilin-
gual Information Retrieval: From Research to Practice.
Springer, 2012.

Gray, R. D. and Atkinson, Q. D. Language-tree divergence
times support the Anatolian theory of Indo-European
origin. Nature, 426(6965):435–439, 2003.

Gretton, A. A simpler condition for consistency of a kernel

independence test. arXiv:1501.06103, 2015.

Puget, S., Philippe, C., Bax, D., Job, B., Varlet, P., Ju-
nier, M. P., Andreiuolo, F., Carvalho, D., Reis, R.,
and Guerrini-Rousseau, L. Mesenchymal transition
and PDGFRA ampliﬁcation/mutation are key distinct
oncogenic events in pediatric diffuse intrinsic pontine
gliomas. PloS one, 7(2):e30313, 2012.

A low variance consistent test of relative dependency

Reshef, D., Reshef, Y., Finucane, H., Grossman, S.,
McVean, G., Turnbaugh, P., Lander, E., Mitzenmacher,
M., and Sabeti, P. Detecting novel associations in large
datasets. Science, 334(6062), 2011.

Sejdinovic, D., Gretton, A., and Bergsma, W. A kernel
test for three-variable interactions. In Neural Informa-
tion Processing Systems, 2013a.

Sejdinovic, D., Sriperumbudur, B., Gretton, A., and Fuku-
mizu, K. Equivalence of distance-based and RKHS-
based statistics in hypothesis testing. Annals of Statis-
tics, 41(5):2263–2702, 2013b.

Sen, A. and Srivastava, M. Regression Analysis – Theory,
Methods, and Applications. Springer-Verlag, 2011.

Serﬂing, R. J. Approximation theorems of mathematical
statistics. Wiley Series in Probability and Statistics. Wi-
ley, 1981.

Song, L., Smola, A., Gretton, A., Bedo, J., and Borg-
wardt, K. Feature selection via dependence maximiza-
tion. Journal of Machine Learning Research, 13:1393–
1434, 2012.

Sz´ekely, G., Rizzo, M., and Bakirov, N. Measuring and
testing dependence by correlation of distances. Annals
of Statistics, 35(6):2769–2794, 2007.

Trommershauser, J., Kording, K., and Landy, M. S. Sen-
sory Cue Integration. Oxford University Press, 2011.

von Bahr, Bengt. On the convergence of moments in
the central limit theorem. The Annals of Mathematical
Statistics, 36(3):808–818, 06 1965.

Zhang, K., Peters, J., Janzing, D., B., and Sch¨olkopf, B.
Kernel-based conditional independence test and applica-
tion in causal discovery. In 27th Conference on Uncer-
tainty in Artiﬁcial Intelligence, pp. 804–813, 2011.

5
1
0
2
 
y
a
M
 
7
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
2
5
8
3
.
6
0
4
1
:
v
i
X
r
a

A low variance consistent test of relative dependency

Wacha Bounliphone
CentraleSup´elec & Inria, Grande Voie des Vignes, 92295 Chˆatenay-Malabry, France

WACHA.BOUNLIPHONE@CENTRALESUPELEC.FR

Arthur Gretton
Gatsby Computational Neuroscience Unit, University College London, United Kingdom

ARTHUR.GRETTON@GMAIL.COM

Arthur Tenenhaus
CentraleSup´elec, 3 rue Joliot-Curie, 91192 Gif-Sur-Yvette, France

ARTHUR.TENENHAUS@CENTRALESUPELEC.FR

Matthew B. Blaschko
Inria & CentraleSup´elec, Grande Voie des Vignes, 92295 Chˆatenay-Malabry, France

MATTHEW.BLASCHKO@INRIA.FR

Abstract

1. Introduction

We describe a novel non-parametric statistical
hypothesis test of relative dependence between
a source variable and two candidate target vari-
ables.
Such a test enables us to determine
whether one source variable is signiﬁcantly more
dependent on a ﬁrst target variable or a sec-
ond. Dependence is measured via the Hilbert-
Schmidt Independence Criterion (HSIC), result-
ing in a pair of empirical dependence mea-
sures (source-target 1, source-target 2). We test
whether the ﬁrst dependence measure is signif-
icantly larger than the second. Modeling the
covariance between these HSIC statistics leads
to a provably more powerful test than the con-
struction of independent HSIC statistics by sub-
sampling. The resulting test is consistent and
unbiased, and (being based on U-statistics) has
favorable convergence properties. The test can
be computed in quadratic time, matching the
computational complexity of standard empiri-
cal HSIC estimators. The effectiveness of the
test is demonstrated on several real-world prob-
lems: we identify language groups from a mul-
tilingual corpus, and we prove that tumor lo-
cation is more dependent on gene expression
than chromosomal imbalances. Source code is
available for download at https://github.
com/wbounliphone/reldep.

Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copy-
right 2015 by the author(s).

Tests of dependence are important tools in statistical anal-
ysis, and are widely applied in many data analysis con-
texts. Classical criteria include Spearman’s ρ and Kendall’s
τ , which can detect non-linear monotonic dependencies.
More recent research on dependence measurement has fo-
cused on non-parametric measures of dependence, which
apply even when the dependence is nonlinear, or the vari-
ables are multivariate or non-euclidean (for instance im-
ages, strings, and graphs). The statistics for such tests are
diverse, and include kernel measures of covariance (Gret-
ton et al., 2008; Zhang et al., 2011) and correlation (Daux-
ois & Nkiet, 1998; Fukumizu et al., 2008), distance co-
variances (which are instances of kernel tests) (Sz´ekely
et al., 2007; Sejdinovic et al., 2013b), kernel regression
tests (Cortes et al., 2009; Gunn & Kandola, 2002), rank-
ings (Heller et al., 2013), and space partitioning approaches
(Gretton & Gyorﬁ, 2010; Reshef et al., 2011; Kinney & At-
wal, 2014). Specialization of such methods to univariate
linear dependence can yield similar tests to classical ap-
proaches such as Darlington (1968); Bring (1996).

For many problems in data analysis, however, the question
of whether dependence exists is secondary: there may be
multiple dependencies, and the question becomes which
dependence is the strongest. For instance, in neuroscience,
multiple stimuli may be present (e.g. visual and audio), and
it is of interest to determine which of the two has a stronger
inﬂuence on brain activity (Trommershauser et al., 2011).
In automated translation (Peters et al., 2012), it is of inter-
est to determine whether documents in a source language
are a signiﬁcantly better match to those in one target lan-
guage than to another target language, either as a measure
of difﬁculty of the respective learning tasks, or as a basic
tool for comparative linguistics.

A low variance consistent test of relative dependency

We present a statistical test which determines whether two
target variables have a signiﬁcant difference in their de-
pendence on a third, source variable. The dependence be-
tween each of the target variables and the source is com-
puted using the Hilbert-Schmidt Independence Criterion
(Gretton et al., 2005; 2008).1 Care must be taken in an-
alyzing the asymptotic behavior of the test statistics, since
the two measures of dependence will themselves be cor-
related: they are both computed with respect to the same
source. Thus, we derive the joint asymptotic distribution
of both dependencies. The derivation of our test utilizes
classical results of U -statistics (Hoeffding, 1963; Serﬂing,
1981; Arcones & Gine, 1993). In particular, we make use
of results by Hoeffding (1963) and Serﬂing (1981) to deter-
mine the asymptotic joint distributions of the statistics (see
Theorem 4). Consequently, we derive the lowest variance
unbiased estimator of the test statistic.

We prove our approach to have greater statistical power
than constructing two uncorrelated statistics on the same
data by subsampling, and testing on these. In experiments,
we are able to successfully test which of two variables is
most strongly related to a third, in synthetic examples, in a
language group identiﬁcation task, and in a task for iden-
tifying the relative strength of factors for Glioma type in a
pediatric patient population.

there do not exist competing non-
To our knowledge,
parametric tests to determine which of two dependencies
is strongest. One related area is that of multiple regression
analysis (e.g. (Sen & Srivastava, 2011)). In this case a lin-
ear model is assumed, and it is determined whether individ-
ual inputs have a statistically signiﬁcant effect on an output
variable. The procedure does not address the question of
whether the inﬂuence of one variable is higher than that of
another to a statistically signiﬁcant degree. The problem of
variable selection has also been investigated in the case of
nonlinear relations between the inputs and outputs (Cortes
et al., 2009; 2012; Song et al., 2012), however this again
does not address which of two variables most strongly in-
ﬂuences a third. A less closely related area is that of detect-
ing three-variable interactions (Sejdinovic et al., 2013a),
where it is determined whether there exists any factoriza-
tion of the joint distribution over three variables. This test
again does not address the issue of ﬁnding which connec-
tions are strongest, however.

1Dependency can also be tested with the correlation operator.
However, Fukumizu et al., (2007) show that unlike the covariance
operator, the asymptotic distribution of the norm of the correlation
operator is unknown, so the construction of a computationally ef-
ﬁcient test of relative dependence remains an open problem.

2. Deﬁnitions and description of HSIC

We base our underlying notion of dependence on the
Hilbert-Schmidt Independence Criterion (Gretton et al.,
2005; 2008; Song et al., 2012). All results in this section
except for Problem 1 can be found in these previous works.

Deﬁnition 1.
Hilbert-Schmidt Independence Criterion)

(Gretton et al., 2005, Deﬁnition 1,Lemma 1:

Let Pxy be a Borel probability measure over over (X ×
Y, Γ × Λ) with Γ and Λ the respective Borel sets on X and
Y, and Px and Py the marginal distributions on domains
X and Y. Given separable RKHSs F and G,
the Hilbert-
Schmidt Independence Criterion (HSIC) is deﬁned as the
squared HS-norm of the associated cross-covariance oper-
ator Cxy. When the kernels k, l are associated uniquely
withs respective RKHSs F and G and bounded, HSIC can
be expressed in terms of expectations of kernel functions

HSIC(F, G, Pxy) := (cid:107)Cxy(cid:107)2
HS
= Exx(cid:48)yy(cid:48) [k(x, x(cid:48))l(y, y(cid:48))] + Exx(cid:48) [k(x, x(cid:48))] Eyy(cid:48) [l(y, y(cid:48))]
− 2Exy [Ex(cid:48)[k(x, x(cid:48))]Ey(cid:48)[l(y, y(cid:48))]] .
(1)

HSIC determines independence: HSIC = 0 iff Pxy = PxPy
when kernels k and l are characteristic on their respective
marginal domains (Gretton, 2015).

With this choice, the problem we would like to solve is
described as follows:

Problem 1. Given separable RKHSs F, G, and H with
HSIC(F, G, Pxy) > 0 and HSIC(F, H, Pxz) > 0,
we test the null hypothesis H0 : HSIC(F, G, Pxy) ≤
HSIC(F, H, Pxz) versus the alternative hypothesis H1 :
HSIC(F, G, Pxy) > HSIC(F, H, Pxz) at a given sig-
niﬁcance level α.

We now describe the asymptotic behavior of the HSIC for
dependent variables.

(Song et al., 2012, Theorem 2: Unbiased
Theorem 1.
estimator for HSIC(F, G, Pxy)) We denote by S the set
of observations {(x1, y1), ..., (xm, ym)} of size m drawn
i.i.d. from Pxy. The unbiased estimator HSICm(F, G, S)
is given by

HSICm(F, G, S) =

1
m(m − 3)

×

(cid:34)

Tr( ˜K˜L) +

1(cid:48) ˜K11(cid:48) ˜L1
(m − 1)(m − 2)

−

2
m − 2

(2)

(cid:35)
1(cid:48) ˜K˜L1

where ˜K and ˜L are related to K and L by ˜Kij = (1 −
δij) ˜Kij and ˜Lij = (1 − δij)˜Lij.
(Song et al., 2012, Theorem 3: U-statistic of
Theorem 2.
HSIC) This ﬁnite sample unbiased estimator of HSIC XY
m

A low variance consistent test of relative dependency

can be written as a U-statistic,

HSIC XY

m = (m)−1
4

hijqr

(3)

(cid:88)

(i,j,q,r)∈im
4

m!
(m − 4)!

where (m)4 :=

4 denotes the
set of all 4−tuples drawn without replacement from the set
{1, . . . m}, and the kernel h of the U-statistic is deﬁned as

, the index set im

hijqr =

kst(lst + luv − 2lsu)

(4)

1
24

(i,j,q,r)
(cid:88)

(s,t,u,v)

where the kernels k and l are associated uniquely with re-
spective reproducing kernel Hilbert spaces F and G.

(Gretton et al., 2008, Theorem 1: Asymptotic
Theorem 3.
distribution of HSICm) If E[h2] < ∞, and source and
targets are not independent, then, under H1, as m → ∞,

√

d−→ N (0, σ2

m(HSIC XY

m − HSIC(F, G, Pxy))

where σ2
with Ej,q,r
:= ESj ,Sq,Sr .
mate is ˆσXY = 16 (cid:0)RXY − (HSIC XY

XY )
(5)
Ei (Ej,q,rhijqr)2 − HSIC(F, G, Pxy))
Its empirical esti-
m )2(cid:1) where

2

XY = 16

(cid:16)

(cid:17)

RXY =

m
(cid:88)


(m − 1)−1
3

1
m

(cid:88)

hijqr



and

(j,q,r)∈im
i=1
3 \ {i} denotes the set of all 3−tuples drawn

the index set im
without replacement from the set {1, . . . m} \ {i}.

3 \{i}

3. A test of relative dependence

In this section we calculate two dependent HSIC statistics
and derive the joint asymptotic distribution of these depen-
dent quantities, which is used to construct a consistent test
for Problem 1. We next construct a simpler consistent test,
by computing two independent HSIC statistics on sample
subsets. While the simpler strategy is superﬁcially attrac-
tive and less effort to implement, we prove the dependent
strategy is strictly more powerful.

3.1. Joint asymptotic distribution of HSIC and test

m and HSIC XZ

In the present section, we compute each HSIC estimate
on the full dataset, and explicitly obtain the correlations
between the resulting empirical dependence measurements
HSIC XY
m . We denote by S1 = (X, Y, Z)
the joint sample of observations which are drawn i.i.d. with
respective Borel probability measure Pxyz deﬁned on the
domain X × Y × Z. The kernels k, l and d are associated
uniquely with respective reproducing kernel Hilbert spaces
F, G and H. Moreover, K, L and D ∈ Rm×m are kernel
matrices containing kij = k(xi, xj), lij = l(yi, yj) and

1
24

1
24

(i,j,q,r)
(cid:88)

(s,t,u,v)

(i,j,q,r)
(cid:88)

(s,t,u,v)

dij = d(zi, zj). Let HSIC XY
m be respec-
tively the unbiased estimators of HSIC(F, G, Pxy) and
HSIC(F, H, Pxz), written as a sum of U-statistics with
respective kernels hijqr and gijqr as described in (4),

m and HSIC XZ

hijqr =

kst(lst + luv − 2lsu),

gijqr =

kst(dst + duv − 2dsu).

(6)

Theorem 4. (Joint asymptotic distribution of HSIC) If
E[h2] < ∞ and E[g2] < ∞, then

(cid:18)(cid:18)

√

m

HSIC XY
m
HSIC XZ
(cid:18)(cid:16)0
(cid:17)
0

m

d−→ N

(cid:19)

−

(cid:18)HSIC(F, G, Pxy)
HSIC(F, H, Pxz)
(cid:19)(cid:19)

(cid:19)(cid:19)

(cid:18) σ2
XY
σXY XZ

,

σXY XZ
σ2
XZ

,

(7)

XY and σ2

estimate of σXY XZ is

XZ are as in Theorem 3.
ˆσXY XZ

The
=

(cid:0)RXY XZ − HSIC XY

m HSIC XZ
m

(cid:1), where

where σ2
empirical
16
m

RXY XZ =


(m − 1)−2
3

1
m

m
(cid:88)

i=1

(cid:88)

hijqrgijqr

 .

(j,q,r)∈im

3 \{i}



(8)

Proof. Eq.
(8) is constructed with the deﬁnition of vari-
ance of a U-statistic as given by Serﬂing, Ch. 5 (1981),
where one variable is ﬁxed. Eq. (7) follows from the appli-
cation of Hoeffding, Theorem 7.1 (1963), which gives the
joint asymptotic distribution of U-statistics.

Based on the joint asymptotic distribution of HSIC de-
scribed in Theorem 4, we can now describe a statistical
test to solve Problem 1: given a sample S1 as described
in Section 3.1, T (S1) : {(X × Y × Z)m} → {0, 1} is
used to test the null hypothesis H0 : HSIC(F, G, Pxy) ≤
HSIC(F, H, Pxz) versus the alternative hypothesis H1 :
HSIC(F, G, Pxy) > HSIC(F, H, Pxz) at a given sig-
niﬁcance level α. This is achieved by projecting the distri-
m − HSIC XZ
bution to 1D using the statistic HSIC XY
m ,
and determining where the statistic falls relative to a
conservative estimate of the the 1 − α quantile of the
null. We now derive this conservative estimate. A sim-
ple way of achieving this is to rotate the distribution by
π
4 counter-clockwise about the origin, and to integrate
the resulting distribution projected onto the ﬁrst axis (cf.
Fig. 3). Denote the asymptotically normal distribution of
√
m ]T as N (µ, Σ). The distribution

m[HSIC XY

m HSIC XZ
resulting from rotation and projection is
(cid:1) ,
N (cid:0)[Qµ]1, [QΣQT ]11

(9)

A low variance consistent test of relative dependency

Then for m > 1 and all δ > 0 with probability at least
1 − δ, for all pxyz, the generalization bound on the differ-
ence of empirical HSIC statistics is

(10)

(11)

| {HSIC(F, G, Pxy) − HSIC(F, H, Pxz)}

− (cid:8)HSIC XY

m − HSIC XZ
m

(cid:9) |

(cid:40)(cid:114)

≤ 2

(cid:41)

log(6/δ)
α2m

+

C
m

(14)

√

2
2

1
2

where Q =

is the rotation matrix by π

4 and

√

2
2

(cid:17)
(cid:16)1 −1
1
1

[Qµ]1 =

(HSIC(F, G, Pxy) − HSIC(F, H, Pxz)) ,

[QΣQT ]11 =

(σ2

XY + σ2

XZ − 2σXY XZ).

Following the empirical distribution from Eq. (9), a test
m − HSIC XZ
with statistic HSIC XY

m has p-value

(cid:32)

p ≤ 1 − Φ

(HSIC XY
(cid:112)σ2
XY + σ2

m − HSIC XZ
m )
XZ − 2σXY XZ

(cid:33)

,

(12)

where Φ is the CDF of a standard normal distribution, and
we have made the most conservative possible assumption
that HSIC(F, G, Pxy) − HSIC(F, H, Pxz) = 0 under
the null (the null also allows for the difference in population
dependence measures to be negative).

in practice,

XZ and σ2

the test
the variances of
To implement
XY , σ2
σ2
XY XZ may be replaced by their empir-
ical estimates. The test will still be consistent for a large
enough sample size, since the estimates will be sufﬁciently
well converged to ensure the test is calibrated. Eq. (8) is
expensive to compute na¨ıvely, because even computing the
kernels hijqr and gijqr of the U -statistic itself is a non
trivial task. Following (Song et al., 2012, Section 2.5),
we ﬁrst form a vector hXY with entries corresponding to
(cid:80)
3 \{i} hijqr, and a vector hXZ with entries cor-
(j,q,r)∈im
responding to (cid:80)
3 \{i} gijqr. Collecting terms in
Eq. (4) related to kernel matrices ˜K and ˜L, hXY can be
written as

(j,q,r)∈im

(cid:17)
hXY = (m − 2)2 (cid:16) ˜K (cid:12) ˜L

1 − m( ˜K1) (cid:12) (˜L1)

(13)
(cid:17)
(Tr( ˜K˜L))1 − ˜K(˜L1) − ˜L( ˜K1)

(cid:16)

+ (m − 2)

+ (1T ˜L1) ˜K1 + (1T ˜K1)˜L1 − ((1T ˜K)(˜L1))1

3 hXY

where (cid:12) denotes the Hadamard product. Then RXY XZ
in Eq. (8) can be computed as RXY XZ = (4m)−1(m −
T hXZ. Using the order of operations implied by
1)−2
the parentheses in Eq. (13), the computational cost of the
cross covariance term is O(m2). Combining this with the
unbiased estimator of HSIC in Eq. (2) leads to a ﬁnal com-
putational complexity of O(m2).

In addition to the asymptotic consistency result, we provide
a ﬁnite sample bound on the deviation between the differ-
ence of two population HSIC statistics and the difference
of two empirical HSIC estimates.

Theorem 5 (Generalization bound on the difference of
empirical HSIC statistics). Assume that k, l, and d are
bounded almost everywhere by 1, and are non-negative.

where α > 0.24 and C are constants.

Proof. In Gretton et al., (2005) a ﬁnite sample bound is
given for a single HSIC statistic. Eq. (14) is proved by
using a union bound.

Corollary 1. HSIC XY
ulation statistic at rate O(

m −HSIC XZ
√
m).

m converges to the pop-

3.2. A simple consistent test via uncorrelated HSICs

From the result in Eq. (5), a simple, consistent test of
relative dependence can be constructed as follows: split
the samples from Px into two equal sized sets denoted
by X (cid:48) and X (cid:48)(cid:48), and drop the second half of the sam-
ple pairs with Y and the ﬁrst half of the sample pairs
with Z. We will denote the remaining samples as Y (cid:48)
and Z (cid:48)(cid:48). We can now estimate the joint distribution of
√

m[HSIC X (cid:48)Y (cid:48)

m/2 , HSIC X (cid:48)(cid:48)Z(cid:48)(cid:48)

m/2

]T as

(cid:18)(cid:18)HSIC(F, G, Pxy)
HSIC(F, H, Pxz)

(cid:19)

,

N

(cid:18)σ2

X (cid:48)Y (cid:48)
0

(cid:19)(cid:19)

0

σ2

X (cid:48)(cid:48)Z(cid:48)(cid:48)

, (15)

which we will write as N (µ(cid:48), Σ(cid:48)). Given this joint
distribution, we need to determine the distribution over
space deﬁned by HSIC(F, G, Pxy) <
the half
HSIC(F, H, Pxz). As in the previous section, we achieve
this by rotating the distribution by π
4 counter-clockwise
about the origin, and integrating the resulting distribution
projected onto the ﬁrst axis (cf. Fig. 3). The resulting pro-
jection of the rotated distribution onto the primary axis is
N (cid:0)[Qµ(cid:48)]1 , (cid:2)QΣ(cid:48)QT (cid:3)

(16)

(cid:1)

11

where

√

2
2

1
2

[Qµ(cid:48)]1 =

(HSIC(F, G, Pxy) − HSIC(F, H, Pxz)) ,

(17)

(18)

[QΣ(cid:48)QT ]11 =

(σ2

X (cid:48)Y (cid:48) + σ2

X (cid:48)(cid:48)Z(cid:48)(cid:48) ).

From this empirically estimated distribution, it is straight-
forward to construct a consistent test (cf. Eq. (12)). The
power of this test varies inversely with the variance of the
distribution in Eq. (16).

A low variance consistent test of relative dependency

3.3. The dependent test is more powerful

While discarding half the samples leads to a consistent test,
we might expect some loss of power over the approach in
Section 3.1, due to the increase in variance with lower sam-
ple size. In this section, we prove the Section 3.1 test is
more powerful than that of Section 3.2, regardless of Pxy
and Pxz.

We call the simple and consistent approach in Section 3.2,
the independent approach, and the lower variance approach
in Section 3.1, the dependent approach. The following the-
orem compares these approaches.

Theorem 6. The asymptotic relative efﬁciency (ARE) of the
independent approach relative to the dependent approach
is always greater than 1.

Remark 1. The asymptotic relative efﬁciency (ARE) is de-
ﬁned in e.g. Serﬂing (1981, Chap.5, Section 1.15.4). If mA
and mB are the sample sizes at which tests ”perform equiv-
alently” (i.e. have equal power), then the ratio mA
repre-
mB
sents the relative efﬁciency. When mA and mB tend to +∞
and the ratio mA
→ L (at equivalent performance), then
mB
the value L represents the asymptotic relative efﬁciency of
procedure B relative to procedure A. This example is rele-
vant to our case since we are comparing two test statistics
with different asymptotically Normal distributions.

The following lemma is used for the proof of Theorem 6.

Lemma 1. (Lower Variance) The variance of the depen-
dent test statistic is smaller than the variance of the inde-
pendent test statistic.

X (cid:48)Y (cid:48) = 2σ2
XY + σ2

Proof. From the convergence of moments in the applica-
tion of the central limit theorem (von Bahr, 1965), we
have that σ2
XY . Then the variance summary
in Eq. (11) is 1
2 (σ2
XZ − 2σXY XZ) and the variance
summary in Equation (18) is 1
XY + 2σ2
2 (2σ2
XZ) where in
√
m. We have that the
both cases the statistic is scaled by
variance of the independent test statistic is smaller than the
variance of the dependent test statistic when

1
2

(σ2

XY + σ2

1
XZ − 2σXY XZ) <
2
XY + σ2
⇐⇒ −2σXY XZ < σ2

(2σ2

XZ

XY + 2σ2

XZ)

(19)

which is implied by the positive deﬁniteness of Σ.

Proof of Theorem 6. The Type II error probability of the
independent test at level α is







Φ

Φ−1(1 − α) −

m−1/2(cid:0)HSIC(F, G, Pxy)
−HSIC(F, H, Pxz)(cid:1)
(cid:112)σ2

X (cid:48)Y (cid:48) + σ2

X (cid:48)(cid:48)Z(cid:48)(cid:48)







,

(20)

where we again make the most conservative possible as-
sumption that HSIC(F, G, Pxy) − HSIC(F, H, Pxz) =
0 under the null. The Type II error probability of the de-
pendent test at level α is







Φ

Φ−1(1 − α) −

m−1/2(cid:0)HSIC(F, G, Pxy)
−HSIC(F, H, Pxz)(cid:1)
XY + σ2
XZ − 2σXY XZ

(cid:112)σ2







(21)

where Φ is the CDF of the standard normal distribution.
The numerator in Eq. (20) is the same as the numerator in
Eq. (21), and the denominator in Eq. (21) is smaller due to
Lemma 1. The lower variance dependent test therefore has
higher ARE, i.e., for a sufﬁcient sample size m > τ for
some distribution dependent τ ∈ N+, the dependent test
will be more powerful than the independent test.

4. Generalizing to more than two HSIC

statistics

the dependence test

The generalization of
to more
than three random variables follows from the earlier
derivation by applying successive rotations to a higher
dimensional
joint Gaussian distribution over multiple
HSIC statistics. We assume a sample S of size m
over n domains with kernels k1, . . . , kn associated
uniquely with respective reproducing kernel Hilbert
spaces F1, . . . , Fn. We deﬁne a generalized statisti-
cal test, Tg(S) → {0, 1} to test the null hypothesis
H0
(x,y)∈{1,...,n}2 v(x,y)HSIC(Fx, Fy, Pxy) ≤
0
:
(cid:80)
(x,y)∈{1,...,n}2 v(x,y)HSIC(Fx, Fy, Pxy) > 0, where
v is a vector of weights on each HSIC statistic. We
in the previous section by set-
may recover the test
ting v(1,2) = +1 v(1,3) = −1 and v(i,j) = 0 for all
(i, j) ∈ {1, 2, 3}2 \ {(1, 2), (1, 3)}.

hypothesis Hm

: (cid:80)
versus

alternative

the

The derivation of the test follows the general strategy used
in the previous section: we construct a rotation matrix so
as to project the joint Gaussian distribution onto the ﬁrst
axis, and read the p-value from a standard normal table. To
construct the rotation matrix, we simply need to rotate v
such that it is aligned with the ﬁrst axis. Such a rotation
can be computed by composing n 2-dimensional rotation
matrices as in Algorithm 1.

5. Experiments

We apply our estimates of statistical dependence to three
challenging problems. The ﬁrst is a synthetic data experi-
ment, in which we can directly control the relative degree
of functional dependence between variates. The second ex-
periment uses a multilingual corpus to determine the rela-
tive relations between European languages. The last exper-

A low variance consistent test of relative dependency

Algorithm 1 Successive rotation for generalized high-
dimensional relative tests of dependency (cf. Section 4)
Require: v ∈ Rn
Ensure: [Qv]i = 0 ∀i (cid:54)= 1, QT Q = I

Q = I
for i = 2 to n do

vi
[Qv]1

Qi = I; θ = − tan−1
[Qi]11 = cos(θ); [Qi]1i = − sin(θ)
[Qi]i1 = sin(θ); [Qi]ii = cos(θ)
Q = QiQ

end for

)
1
,
0
(

N
1
γ
+
)
t
(
n
i
s

)
1
,
0
(

N
2
γ
+
)
t
(
n
i
s
t

)
1
,
0
(

N
3
γ
+
)
t
(
s
o
c
t

t + γ1N (0, 1)
(a) γ1 = 0.3

t cos(t) + γ2N (0, 1)
(b) γ2 = 0.3

t cos(t) + γ3N (0, 1)
(c) γ3 = 0.6

Figure 1. Illustration of a synthetic dataset sampled from the dis-
tribution in Eq. (22).

iment is a 3-block dataset which combines gene expression,
comparative genomic hybridization, and a qualitative phe-
notype measured on a sample of Glioma patients.

5.1. Synthetic experiment

We constructed 3 distributions as deﬁned in Eq. (22) and
illustrated in Figure 1.

Let t ∼ U[(0, 2π)],

(22)

(a) x1 ∼ t + γ1N (0, 1) y1 ∼ sin(t) + γ1N (0, 1)
(b) x2 ∼ t cos(t) + γ2N (0, 1) y2 ∼ t sin(t) + γ2N (0, 1)
(c) x3 ∼ t cos(t) + γ3N (0, 1) y3 ∼ t sin(t) + γ3N (0, 1)

These distributions are speciﬁed so that we can control the
relative degree of functional dependence between the vari-
ates by varying the relative size of noise scaling parameters
γ1, γ2 and γ3. The question is then whether the dependence
between (a) and (b) is larger than the dependence between
(a) and (c). In these experiments, we ﬁxed γ1 = γ2 = 0.3,
while we varied γ3, and used a Gaussian kernel with band-
width σ selected as the median pairwise distance between
data points. This kernel is sufﬁcient to obtain good perfor-
mance, although others choices exist (Gretton et al., 2012).

Figure 2. Power of the dependent and independent test as a func-
tion of γ3 on the synthetic data described in Section 5.1. For val-
ues of γ3 > 0.3 the distribution in Fig. 1(a) is closer to 1(b) than
to 1(c). The problem becomes difﬁcult as γ3 → 0.3. As predicted
by theory, the dependent test is signiﬁcantly more powerful over
almost all values of γ3 by a substantial margin.

5.2. Multilingual data

In this section, we demonstrate dependence testing to pre-
dict the relative similarity of different languages. We use
a real world dataset taken from the parallel European Par-
liament corpus (Koehn, 2005). We choose 3000 random
documents in common written in: Finnish (ﬁ), Italian (it),
French (fr), Spanish (es), Portuguese (pt), English (en),
Dutch (nl), German (de), Danish (da) and Swedish (sv).
These languages can be broadly categorized into either the
Romance, Germanic or Uralic groups (Gray & Atkinson,
2003). In this dataset, we considered each language as a
random variable and each document as an observation.

Our ﬁrst goal is to test if the statistical dependence between
two languages in the same group is greater than the sta-
tistical dependence between languages in different groups.
For pre-processing, we removed stop-words (http://
www.nltk.org) and performed stemming (http://
snowball.tartarus.org). We applied the TF-IDF
model as a feature representation and used a Gaussian ker-
nel with the bandwidth σ set per language as the median
pairwise distance between documents.

In Table 1, a selection of tests between language groups
(Germanic, Romance, and Uralic) is given: all p-values
strongly support that our relative dependence test ﬁnds the
different language groups with very high signiﬁcance.

Figure 2 shows the power of the dependent and the inde-
pendent tests as we vary γ3. It is clear from these results
that the dependent test is far more powerful than the inde-
pendent test over the great majority of γ3 values consid-
ered. Figure 3 demonstrates that this superior test power
arises due to the tighter and more concentrated distribution
of the dependent statistic.

Further, if we focus on the Romance family, our test en-
ables one to answer more ﬁne-grained questions about the
relative similarity of languages within the same group. As
before, we determine the ground truth similarities from the
topology of the tree of European languages determined by
the linguistics community (Gray & Atkinson, 2003; Bouck-
aert et al., 2012) as illustrated in Fig. 4 for the Romance

A low variance consistent test of relative dependency

(a) m=500, γ3 = 0.7
pdep = 0.0189, pindep = 0.3492

(b) m=1000, γ3 = 0.7
pdep = 10−4, pindep = 0.3690

(c) m=3000, γ3 = 0.7
pdep = 10−6, pindep = 0.2876

(d) m=500, γ3 = 1.7
pdep = 10−9, pindep = 0.982

(e) m=1000, γ3 = 1.7
pdep = 10−10, pindep = 0.0326

(f) m=3000, γ3 = 1.7
pdep = 10−13, pindep = 0.005

Figure 3. For the synthetic experiments described in Section 5.1, we plot empirical HSIC values for dependent and independent tests for
100 repeated draws with different sample sizes. Empirical p-values for each test show that the dependent distribution converges faster
than the independent distribution even at low sample size, resulting in a more powerful statistical test.

Source Target 1 Target 2 p-value
0.0066
0.0418
0.0169
0.0173
< 10−4
< 10−4
< 10−6
< 10−4
< 10−4

pt
it
es
es
nl
en
sv
en
de

ﬁ
da
ﬁ
da
ﬁ
es
fr
it
es

es
fr
it
pt
de
nl
da
sv
en

Table 1. A selection of relative dependency tests between two
pairs of HSIC statistics for the multilingual corpus data. Low p-
values indicate a source is closer to target 1 than to target 2. In all
cases, the test correctly identiﬁes that languages within the same
group are more strongly related than those in different groups.

group. We have run the test on all triplets from the cor-
pus for which the topology of the tree speciﬁes a correct
ordering of the dependencies. In a fraction of a second (ex-
cluding kernel computation), we are able to recover certain
features of the subtree of relationships between languages
present in the Romance language group (Table 2). The test
always indicates the correct relative similarity of languages
when nearby languages (pt,es) are compared with those fur-
ther away (ft,it), however errors are made when comparing
triplets of languages for which the nearest common ances-
tor is more than one link removed.

Figure 4. Partial tree of Romance languages adapted from (Gray
& Atkinson, 2003).

Source Target 1 Target 2 p-value
0.0157
0.1882
0.2147
< 10−4
< 10−4
0.7649
0.0011
< 10−8

es
pt
fr
pt
pt
fr
es
es

fr
fr
es
es
es
pt
pt
pt

it
it
it
it
fr
it
it
fr

Table 2. Relative dependency tests between Romance languages.
The tests are ordered such that a low p-value corresponds with a
conﬁrmation of the topology of the tree of Romance languages de-
termined by the linguistics community (Gray & Atkinson, 2003).

A low variance consistent test of relative dependency

dependent test
independent test

In our next tests, we evaluate our more general framework
for testing relative dependencies with more than two HSIC
statistics. We chose four languages, and tested whether the
average dependence between languages in the same group
is higher than the dependence between groups. The results
of these tests are in Table 3. As before, our test is able to
distinguish language groups with high signiﬁcance.

×10−3
6

5

4

3

2

1

0

Z
X
C
I
S
H

−1

−2

0

Source Targets p-value
de sv ﬁ < 10−9
sv en fr < 10−9
sv en it < 10−5
it es sv < 10−5
fr pt nl 0.0175

da
da
de
fr
es

Table 3. Relative dependency test between four pairs of HSIC
statistics for the multilingual corpus data. These tests show the
ability of the relative dependence test to generalize to arbitrary
numbers of HSIC statistics by constructing a rotation matrix us-
ing Algorithm 1. In all cases v = [1 1 −2].

5.3. Pediatric glioma data

Brain tumors are the most common solid tumors in children
and have the highest mortality rate of all pediatric cancers.
Despite advances in multimodality therapy, children with
pediatric high-grade gliomas (pHGG) invariably have an
overall survival of around 20% at 5 years. Depending on
their location (e.g. brainstem, central nuclei, or supraten-
torial), pHGG present different characteristics in terms of
radiological appearance, histology, and prognosis. The hy-
pothesis is that pHGG have different genetic origins and
oncogenic pathways depending on their location. Thus, the
biological processes involved in the development of the tu-
mor may be different from one location to another.

In order to evaluate such hypotheses, pre-treatment frozen
tumor samples were obtained from 53 children with newly
diagnosed pHGG from Necker Enfants Malades (Paris,
France) from Puget et al, (2012). The 53 tumors are di-
vided into 3 locations: supratentorial (HEMI), central nu-
clei (MIDL), and brain stem (DIPG). The ﬁnal dataset is or-
ganized in 3 blocks of variables deﬁned for the 53 tumors:
X is a block of indicator variables describing the location
category, the second data matrix Y provides the expression
of 15 702 genes (GE). The third data matrix Z contains the
imbalances of 1229 segments (CGH) of chromosomes.

For X, we use a linear kernel, which is characteristic for
indicator variables, and for Y and Z, the kernel was cho-
sen to be the Gaussian kernel with σ selected as the median
of pairwise distances. The p-value of our relative depen-
dency test is < 10−5. This shows that the tumor location
in the brain is more dependent on gene expression than on
chromosomal imbalances. By contrast with Section 5.1,
the independent test was also able to ﬁnd the same order-

1

2

4

5

6 ×10−3

3
HSIC XY

Figure 5. 2σ iso-curves of the Gaussian distributions estimated
from the pediatric Glioma data. As before, the dependent test
has a much lower variance than the independent test. The tests
support the stronger dependence on the tumor location to gene
expression than chromosomal imbalances.

ing of dependence, but with a p-value that is three orders of
magnitude larger (p = 0.005). Figure 5 shows iso-curves
of the Gaussian distributions estimated in the independent
and dependent tests. The empirical relative dependency is
consistent with ﬁndings in the medical literature, and pro-
vides additional statistical support for the importance of
tumor location in Glioma (Gilbertson & Gutmann, 2007;
Palm et al., 2009; Puget et al., 2012).

6. Conclusions

We have described a novel statistical test that determines
whether a source random variable is more strongly de-
pendent on one target random variable or another. This
test, built on the Hilbert-Schmidt Independence Criterion,
is low variance, consistent, and unbiased. We have shown
that our test is strictly more powerful than a test that does
not exploit the covariance between HSIC statistics, and
empirically achieves p-values several orders of magnitude
smaller. We have empirically demonstrated the test perfor-
mance on synthetic data, where the degree of dependence
could be controlled; on the challenging problem of iden-
tifying language groups from a multilingual corpus; and
for ﬁnding the most important determinant of Glioma type.
The computation and memory requirements of the test are
quadratic in the sample size, matching the performance of
HSIC and related tests for dependence between two ran-
dom variables. The test is therefore scalable to the wide
range of problem instances where non-parametric depen-
dency tests are currently applied. We have generalized the
test framework to more than two HSIC statistics, and have
given an algorithm to construct a consistent, low-variance,
unbiased test in this setting.

Acknowledgements

We thank Ioannis Antonoglou for helpful discussions. The
ﬁrst author is supported by a fellowship from Centrale-

A low variance consistent test of relative dependency

Sup´elec. This work is partially funded by the Euro-
pean Commission through ERC Grant 259112 and FP7-
MCCIG334380.

Gretton, A. and Gyorﬁ, L. Consistent nonparametric tests
Journal of Machine Learning Re-

of independence.
search, 11:1391–1423, 2010.

References

Arcones, M. A. and Gine, E. Limit theorems for U-
processes. The Annals of Probability, pp. 1494–1542,
1993.

Bouckaert, R., Lemey, P., Dunn, M., Greenhill, S. J., Alek-
seyenko, A. V., Drummond, A. J., Gray, R. D., Suchard,
M. A., and Atkinson, Q. D. Mapping the origins and ex-
pansion of the Indo-European language family. Science,
337(6097):957–960, 2012.

Bring, J. A geometric approach to compare variables in
a regression model. The American Statistician, 50(1):
57–62, 1996.

Cortes, C., Mohri, M., and Rostamizadeh, A. Learning
non-linear combinations of kernels. In Neural Informa-
tion Processing Systems, 2009.

Cortes, C., Mohri, M., and Rostamizadeh, A. Algorithms
for learning kernels based on centered alignment. Jour-
nal of Machine Learning Research, 13:795–828, 2012.

Darlington, Richard B. Multiple regression in psychologi-
cal research and practice. Psychological bulletin, 69(3):
161, 1968.

Dauxois, J. and Nkiet, G. M. Nonlinear canonical analy-
sis and independence tests. Annals of Statistics, 26(4):
1254–1278, 1998.

Fukumizu, K., Bach, F. R., and Gretton, A.

Statisti-
cal consistency of kernel canonical correlation analysis.
The Journal of Machine Learning Research, 8:361–383,
2007.

Fukumizu, K., Gretton, A., Sun, X., and Sch¨olkopf, B. Ker-
In Advances
nel measures of conditional dependence.
in Neural Information Processing Systems, pp. 489–496.
MIT Press, 2008.

Gretton, A., Bousquet, O., Smola, A. J., and Sch¨olkopf, B.
Measuring statistical dependence with Hilbert-Schmidt
In Algorithmic Learning Theory, pp. 63–77,
norms.
2005.

Gretton, A., Fukumizu, K., Teo, C.-H., Song, L.,
Sch¨olkopf, B., and Smola, A. J. A kernel statistical test
of independence. In Neural Information Processing Sys-
tems, pp. 585–592, 2008.

Gretton, A., Sejdinovic, D., Strathmann, H.and Balakrish-
nan, S., Pontil, M., Fukumizu, K., and Sriperumbudur,
B. K. Optimal kernel choice for large-scale two-sample
tests. In Advances in Neural Information Processing Sys-
tems, pp. 1205–1213, 2012.

Gunn, S. R. and Kandola, J. S. Structural modelling with
sparse kernels. Machine Learning, 48(1):137–163, 2002.

Heller, R., Heller, Y., and Gorﬁne, M. A consistent mul-
tivariate test of association based on ranks of distances.
Biometrika, 100(2):503–510, 2013.

Hoeffding, W. Probability inequalities for sums of bounded
random variables. Journal of the American statistical
association, 58(301):13–30, 1963.

Kinney, J. B. and Atwal, G. S. Equitability, mutual infor-
mation, and the maximal information coefﬁcient. Pro-
ceedings of the National Academy of Sciences, 2014.

Koehn, P. Europarl: A parallel corpus for statistical ma-
chine translation. In MT summit, volume 5, pp. 79–86,
2005.

Palm, T., Figarella-Branger, D., Chapon, F., Lacroix, C.,
Gray, F., Scaravilli, F., Ellison, D. W., Salmon, I.,
Vikkula, M., and Godfraind, C. Expression proﬁling
of ependymomas unravels localization and tumor grade-
speciﬁc tumorigenesis. Cancer, 115(17):3955–3968,
2009.

Gilbertson, R. J. and Gutmann, D. H. Tumorigenesis in the
brain: location, location, location. Cancer research, 67
(12):5579–5582, 2007.

Peters, C., Braschler, M., and Clough, P. Multilin-
gual Information Retrieval: From Research to Practice.
Springer, 2012.

Gray, R. D. and Atkinson, Q. D. Language-tree divergence
times support the Anatolian theory of Indo-European
origin. Nature, 426(6965):435–439, 2003.

Gretton, A. A simpler condition for consistency of a kernel

independence test. arXiv:1501.06103, 2015.

Puget, S., Philippe, C., Bax, D., Job, B., Varlet, P., Ju-
nier, M. P., Andreiuolo, F., Carvalho, D., Reis, R.,
and Guerrini-Rousseau, L. Mesenchymal transition
and PDGFRA ampliﬁcation/mutation are key distinct
oncogenic events in pediatric diffuse intrinsic pontine
gliomas. PloS one, 7(2):e30313, 2012.

A low variance consistent test of relative dependency

Reshef, D., Reshef, Y., Finucane, H., Grossman, S.,
McVean, G., Turnbaugh, P., Lander, E., Mitzenmacher,
M., and Sabeti, P. Detecting novel associations in large
datasets. Science, 334(6062), 2011.

Sejdinovic, D., Gretton, A., and Bergsma, W. A kernel
test for three-variable interactions. In Neural Informa-
tion Processing Systems, 2013a.

Sejdinovic, D., Sriperumbudur, B., Gretton, A., and Fuku-
mizu, K. Equivalence of distance-based and RKHS-
based statistics in hypothesis testing. Annals of Statis-
tics, 41(5):2263–2702, 2013b.

Sen, A. and Srivastava, M. Regression Analysis – Theory,
Methods, and Applications. Springer-Verlag, 2011.

Serﬂing, R. J. Approximation theorems of mathematical
statistics. Wiley Series in Probability and Statistics. Wi-
ley, 1981.

Song, L., Smola, A., Gretton, A., Bedo, J., and Borg-
wardt, K. Feature selection via dependence maximiza-
tion. Journal of Machine Learning Research, 13:1393–
1434, 2012.

Sz´ekely, G., Rizzo, M., and Bakirov, N. Measuring and
testing dependence by correlation of distances. Annals
of Statistics, 35(6):2769–2794, 2007.

Trommershauser, J., Kording, K., and Landy, M. S. Sen-
sory Cue Integration. Oxford University Press, 2011.

von Bahr, Bengt. On the convergence of moments in
the central limit theorem. The Annals of Mathematical
Statistics, 36(3):808–818, 06 1965.

Zhang, K., Peters, J., Janzing, D., B., and Sch¨olkopf, B.
Kernel-based conditional independence test and applica-
tion in causal discovery. In 27th Conference on Uncer-
tainty in Artiﬁcial Intelligence, pp. 804–813, 2011.

5
1
0
2
 
y
a
M
 
7
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
2
5
8
3
.
6
0
4
1
:
v
i
X
r
a

A low variance consistent test of relative dependency

Wacha Bounliphone
CentraleSup´elec & Inria, Grande Voie des Vignes, 92295 Chˆatenay-Malabry, France

WACHA.BOUNLIPHONE@CENTRALESUPELEC.FR

Arthur Gretton
Gatsby Computational Neuroscience Unit, University College London, United Kingdom

ARTHUR.GRETTON@GMAIL.COM

Arthur Tenenhaus
CentraleSup´elec, 3 rue Joliot-Curie, 91192 Gif-Sur-Yvette, France

ARTHUR.TENENHAUS@CENTRALESUPELEC.FR

Matthew B. Blaschko
Inria & CentraleSup´elec, Grande Voie des Vignes, 92295 Chˆatenay-Malabry, France

MATTHEW.BLASCHKO@INRIA.FR

Abstract

1. Introduction

We describe a novel non-parametric statistical
hypothesis test of relative dependence between
a source variable and two candidate target vari-
ables.
Such a test enables us to determine
whether one source variable is signiﬁcantly more
dependent on a ﬁrst target variable or a sec-
ond. Dependence is measured via the Hilbert-
Schmidt Independence Criterion (HSIC), result-
ing in a pair of empirical dependence mea-
sures (source-target 1, source-target 2). We test
whether the ﬁrst dependence measure is signif-
icantly larger than the second. Modeling the
covariance between these HSIC statistics leads
to a provably more powerful test than the con-
struction of independent HSIC statistics by sub-
sampling. The resulting test is consistent and
unbiased, and (being based on U-statistics) has
favorable convergence properties. The test can
be computed in quadratic time, matching the
computational complexity of standard empiri-
cal HSIC estimators. The effectiveness of the
test is demonstrated on several real-world prob-
lems: we identify language groups from a mul-
tilingual corpus, and we prove that tumor lo-
cation is more dependent on gene expression
than chromosomal imbalances. Source code is
available for download at https://github.
com/wbounliphone/reldep.

Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copy-
right 2015 by the author(s).

Tests of dependence are important tools in statistical anal-
ysis, and are widely applied in many data analysis con-
texts. Classical criteria include Spearman’s ρ and Kendall’s
τ , which can detect non-linear monotonic dependencies.
More recent research on dependence measurement has fo-
cused on non-parametric measures of dependence, which
apply even when the dependence is nonlinear, or the vari-
ables are multivariate or non-euclidean (for instance im-
ages, strings, and graphs). The statistics for such tests are
diverse, and include kernel measures of covariance (Gret-
ton et al., 2008; Zhang et al., 2011) and correlation (Daux-
ois & Nkiet, 1998; Fukumizu et al., 2008), distance co-
variances (which are instances of kernel tests) (Sz´ekely
et al., 2007; Sejdinovic et al., 2013b), kernel regression
tests (Cortes et al., 2009; Gunn & Kandola, 2002), rank-
ings (Heller et al., 2013), and space partitioning approaches
(Gretton & Gyorﬁ, 2010; Reshef et al., 2011; Kinney & At-
wal, 2014). Specialization of such methods to univariate
linear dependence can yield similar tests to classical ap-
proaches such as Darlington (1968); Bring (1996).

For many problems in data analysis, however, the question
of whether dependence exists is secondary: there may be
multiple dependencies, and the question becomes which
dependence is the strongest. For instance, in neuroscience,
multiple stimuli may be present (e.g. visual and audio), and
it is of interest to determine which of the two has a stronger
inﬂuence on brain activity (Trommershauser et al., 2011).
In automated translation (Peters et al., 2012), it is of inter-
est to determine whether documents in a source language
are a signiﬁcantly better match to those in one target lan-
guage than to another target language, either as a measure
of difﬁculty of the respective learning tasks, or as a basic
tool for comparative linguistics.

A low variance consistent test of relative dependency

We present a statistical test which determines whether two
target variables have a signiﬁcant difference in their de-
pendence on a third, source variable. The dependence be-
tween each of the target variables and the source is com-
puted using the Hilbert-Schmidt Independence Criterion
(Gretton et al., 2005; 2008).1 Care must be taken in an-
alyzing the asymptotic behavior of the test statistics, since
the two measures of dependence will themselves be cor-
related: they are both computed with respect to the same
source. Thus, we derive the joint asymptotic distribution
of both dependencies. The derivation of our test utilizes
classical results of U -statistics (Hoeffding, 1963; Serﬂing,
1981; Arcones & Gine, 1993). In particular, we make use
of results by Hoeffding (1963) and Serﬂing (1981) to deter-
mine the asymptotic joint distributions of the statistics (see
Theorem 4). Consequently, we derive the lowest variance
unbiased estimator of the test statistic.

We prove our approach to have greater statistical power
than constructing two uncorrelated statistics on the same
data by subsampling, and testing on these. In experiments,
we are able to successfully test which of two variables is
most strongly related to a third, in synthetic examples, in a
language group identiﬁcation task, and in a task for iden-
tifying the relative strength of factors for Glioma type in a
pediatric patient population.

there do not exist competing non-
To our knowledge,
parametric tests to determine which of two dependencies
is strongest. One related area is that of multiple regression
analysis (e.g. (Sen & Srivastava, 2011)). In this case a lin-
ear model is assumed, and it is determined whether individ-
ual inputs have a statistically signiﬁcant effect on an output
variable. The procedure does not address the question of
whether the inﬂuence of one variable is higher than that of
another to a statistically signiﬁcant degree. The problem of
variable selection has also been investigated in the case of
nonlinear relations between the inputs and outputs (Cortes
et al., 2009; 2012; Song et al., 2012), however this again
does not address which of two variables most strongly in-
ﬂuences a third. A less closely related area is that of detect-
ing three-variable interactions (Sejdinovic et al., 2013a),
where it is determined whether there exists any factoriza-
tion of the joint distribution over three variables. This test
again does not address the issue of ﬁnding which connec-
tions are strongest, however.

1Dependency can also be tested with the correlation operator.
However, Fukumizu et al., (2007) show that unlike the covariance
operator, the asymptotic distribution of the norm of the correlation
operator is unknown, so the construction of a computationally ef-
ﬁcient test of relative dependence remains an open problem.

2. Deﬁnitions and description of HSIC

We base our underlying notion of dependence on the
Hilbert-Schmidt Independence Criterion (Gretton et al.,
2005; 2008; Song et al., 2012). All results in this section
except for Problem 1 can be found in these previous works.

Deﬁnition 1.
Hilbert-Schmidt Independence Criterion)

(Gretton et al., 2005, Deﬁnition 1,Lemma 1:

Let Pxy be a Borel probability measure over over (X ×
Y, Γ × Λ) with Γ and Λ the respective Borel sets on X and
Y, and Px and Py the marginal distributions on domains
X and Y. Given separable RKHSs F and G,
the Hilbert-
Schmidt Independence Criterion (HSIC) is deﬁned as the
squared HS-norm of the associated cross-covariance oper-
ator Cxy. When the kernels k, l are associated uniquely
withs respective RKHSs F and G and bounded, HSIC can
be expressed in terms of expectations of kernel functions

HSIC(F, G, Pxy) := (cid:107)Cxy(cid:107)2
HS
= Exx(cid:48)yy(cid:48) [k(x, x(cid:48))l(y, y(cid:48))] + Exx(cid:48) [k(x, x(cid:48))] Eyy(cid:48) [l(y, y(cid:48))]
− 2Exy [Ex(cid:48)[k(x, x(cid:48))]Ey(cid:48)[l(y, y(cid:48))]] .
(1)

HSIC determines independence: HSIC = 0 iff Pxy = PxPy
when kernels k and l are characteristic on their respective
marginal domains (Gretton, 2015).

With this choice, the problem we would like to solve is
described as follows:

Problem 1. Given separable RKHSs F, G, and H with
HSIC(F, G, Pxy) > 0 and HSIC(F, H, Pxz) > 0,
we test the null hypothesis H0 : HSIC(F, G, Pxy) ≤
HSIC(F, H, Pxz) versus the alternative hypothesis H1 :
HSIC(F, G, Pxy) > HSIC(F, H, Pxz) at a given sig-
niﬁcance level α.

We now describe the asymptotic behavior of the HSIC for
dependent variables.

(Song et al., 2012, Theorem 2: Unbiased
Theorem 1.
estimator for HSIC(F, G, Pxy)) We denote by S the set
of observations {(x1, y1), ..., (xm, ym)} of size m drawn
i.i.d. from Pxy. The unbiased estimator HSICm(F, G, S)
is given by

HSICm(F, G, S) =

1
m(m − 3)

×

(cid:34)

Tr( ˜K˜L) +

1(cid:48) ˜K11(cid:48) ˜L1
(m − 1)(m − 2)

−

2
m − 2

(2)

(cid:35)
1(cid:48) ˜K˜L1

where ˜K and ˜L are related to K and L by ˜Kij = (1 −
δij) ˜Kij and ˜Lij = (1 − δij)˜Lij.
(Song et al., 2012, Theorem 3: U-statistic of
Theorem 2.
HSIC) This ﬁnite sample unbiased estimator of HSIC XY
m

A low variance consistent test of relative dependency

can be written as a U-statistic,

HSIC XY

m = (m)−1
4

hijqr

(3)

(cid:88)

(i,j,q,r)∈im
4

m!
(m − 4)!

where (m)4 :=

4 denotes the
set of all 4−tuples drawn without replacement from the set
{1, . . . m}, and the kernel h of the U-statistic is deﬁned as

, the index set im

hijqr =

kst(lst + luv − 2lsu)

(4)

1
24

(i,j,q,r)
(cid:88)

(s,t,u,v)

where the kernels k and l are associated uniquely with re-
spective reproducing kernel Hilbert spaces F and G.

(Gretton et al., 2008, Theorem 1: Asymptotic
Theorem 3.
distribution of HSICm) If E[h2] < ∞, and source and
targets are not independent, then, under H1, as m → ∞,

√

d−→ N (0, σ2

m(HSIC XY

m − HSIC(F, G, Pxy))

where σ2
with Ej,q,r
:= ESj ,Sq,Sr .
mate is ˆσXY = 16 (cid:0)RXY − (HSIC XY

XY )
(5)
Ei (Ej,q,rhijqr)2 − HSIC(F, G, Pxy))
Its empirical esti-
m )2(cid:1) where

2

XY = 16

(cid:16)

(cid:17)

RXY =

m
(cid:88)


(m − 1)−1
3

1
m

(cid:88)

hijqr



and

(j,q,r)∈im
i=1
3 \ {i} denotes the set of all 3−tuples drawn

the index set im
without replacement from the set {1, . . . m} \ {i}.

3 \{i}

3. A test of relative dependence

In this section we calculate two dependent HSIC statistics
and derive the joint asymptotic distribution of these depen-
dent quantities, which is used to construct a consistent test
for Problem 1. We next construct a simpler consistent test,
by computing two independent HSIC statistics on sample
subsets. While the simpler strategy is superﬁcially attrac-
tive and less effort to implement, we prove the dependent
strategy is strictly more powerful.

3.1. Joint asymptotic distribution of HSIC and test

m and HSIC XZ

In the present section, we compute each HSIC estimate
on the full dataset, and explicitly obtain the correlations
between the resulting empirical dependence measurements
HSIC XY
m . We denote by S1 = (X, Y, Z)
the joint sample of observations which are drawn i.i.d. with
respective Borel probability measure Pxyz deﬁned on the
domain X × Y × Z. The kernels k, l and d are associated
uniquely with respective reproducing kernel Hilbert spaces
F, G and H. Moreover, K, L and D ∈ Rm×m are kernel
matrices containing kij = k(xi, xj), lij = l(yi, yj) and

1
24

1
24

(i,j,q,r)
(cid:88)

(s,t,u,v)

(i,j,q,r)
(cid:88)

(s,t,u,v)

dij = d(zi, zj). Let HSIC XY
m be respec-
tively the unbiased estimators of HSIC(F, G, Pxy) and
HSIC(F, H, Pxz), written as a sum of U-statistics with
respective kernels hijqr and gijqr as described in (4),

m and HSIC XZ

hijqr =

kst(lst + luv − 2lsu),

gijqr =

kst(dst + duv − 2dsu).

(6)

Theorem 4. (Joint asymptotic distribution of HSIC) If
E[h2] < ∞ and E[g2] < ∞, then

(cid:18)(cid:18)

√

m

HSIC XY
m
HSIC XZ
(cid:18)(cid:16)0
(cid:17)
0

m

d−→ N

(cid:19)

−

(cid:18)HSIC(F, G, Pxy)
HSIC(F, H, Pxz)
(cid:19)(cid:19)

(cid:19)(cid:19)

(cid:18) σ2
XY
σXY XZ

,

σXY XZ
σ2
XZ

,

(7)

XY and σ2

estimate of σXY XZ is

XZ are as in Theorem 3.
ˆσXY XZ

The
=

(cid:0)RXY XZ − HSIC XY

m HSIC XZ
m

(cid:1), where

where σ2
empirical
16
m

RXY XZ =


(m − 1)−2
3

1
m

m
(cid:88)

i=1

(cid:88)

hijqrgijqr

 .

(j,q,r)∈im

3 \{i}



(8)

Proof. Eq.
(8) is constructed with the deﬁnition of vari-
ance of a U-statistic as given by Serﬂing, Ch. 5 (1981),
where one variable is ﬁxed. Eq. (7) follows from the appli-
cation of Hoeffding, Theorem 7.1 (1963), which gives the
joint asymptotic distribution of U-statistics.

Based on the joint asymptotic distribution of HSIC de-
scribed in Theorem 4, we can now describe a statistical
test to solve Problem 1: given a sample S1 as described
in Section 3.1, T (S1) : {(X × Y × Z)m} → {0, 1} is
used to test the null hypothesis H0 : HSIC(F, G, Pxy) ≤
HSIC(F, H, Pxz) versus the alternative hypothesis H1 :
HSIC(F, G, Pxy) > HSIC(F, H, Pxz) at a given sig-
niﬁcance level α. This is achieved by projecting the distri-
m − HSIC XZ
bution to 1D using the statistic HSIC XY
m ,
and determining where the statistic falls relative to a
conservative estimate of the the 1 − α quantile of the
null. We now derive this conservative estimate. A sim-
ple way of achieving this is to rotate the distribution by
π
4 counter-clockwise about the origin, and to integrate
the resulting distribution projected onto the ﬁrst axis (cf.
Fig. 3). Denote the asymptotically normal distribution of
√
m ]T as N (µ, Σ). The distribution

m[HSIC XY

m HSIC XZ
resulting from rotation and projection is
(cid:1) ,
N (cid:0)[Qµ]1, [QΣQT ]11

(9)

A low variance consistent test of relative dependency

Then for m > 1 and all δ > 0 with probability at least
1 − δ, for all pxyz, the generalization bound on the differ-
ence of empirical HSIC statistics is

(10)

(11)

| {HSIC(F, G, Pxy) − HSIC(F, H, Pxz)}

− (cid:8)HSIC XY

m − HSIC XZ
m

(cid:9) |

(cid:40)(cid:114)

≤ 2

(cid:41)

log(6/δ)
α2m

+

C
m

(14)

√

2
2

1
2

where Q =

is the rotation matrix by π

4 and

√

2
2

(cid:17)
(cid:16)1 −1
1
1

[Qµ]1 =

(HSIC(F, G, Pxy) − HSIC(F, H, Pxz)) ,

[QΣQT ]11 =

(σ2

XY + σ2

XZ − 2σXY XZ).

Following the empirical distribution from Eq. (9), a test
m − HSIC XZ
with statistic HSIC XY

m has p-value

(cid:32)

p ≤ 1 − Φ

(HSIC XY
(cid:112)σ2
XY + σ2

m − HSIC XZ
m )
XZ − 2σXY XZ

(cid:33)

,

(12)

where Φ is the CDF of a standard normal distribution, and
we have made the most conservative possible assumption
that HSIC(F, G, Pxy) − HSIC(F, H, Pxz) = 0 under
the null (the null also allows for the difference in population
dependence measures to be negative).

in practice,

XZ and σ2

the test
the variances of
To implement
XY , σ2
σ2
XY XZ may be replaced by their empir-
ical estimates. The test will still be consistent for a large
enough sample size, since the estimates will be sufﬁciently
well converged to ensure the test is calibrated. Eq. (8) is
expensive to compute na¨ıvely, because even computing the
kernels hijqr and gijqr of the U -statistic itself is a non
trivial task. Following (Song et al., 2012, Section 2.5),
we ﬁrst form a vector hXY with entries corresponding to
(cid:80)
3 \{i} hijqr, and a vector hXZ with entries cor-
(j,q,r)∈im
responding to (cid:80)
3 \{i} gijqr. Collecting terms in
Eq. (4) related to kernel matrices ˜K and ˜L, hXY can be
written as

(j,q,r)∈im

(cid:17)
hXY = (m − 2)2 (cid:16) ˜K (cid:12) ˜L

1 − m( ˜K1) (cid:12) (˜L1)

(13)
(cid:17)
(Tr( ˜K˜L))1 − ˜K(˜L1) − ˜L( ˜K1)

(cid:16)

+ (m − 2)

+ (1T ˜L1) ˜K1 + (1T ˜K1)˜L1 − ((1T ˜K)(˜L1))1

3 hXY

where (cid:12) denotes the Hadamard product. Then RXY XZ
in Eq. (8) can be computed as RXY XZ = (4m)−1(m −
T hXZ. Using the order of operations implied by
1)−2
the parentheses in Eq. (13), the computational cost of the
cross covariance term is O(m2). Combining this with the
unbiased estimator of HSIC in Eq. (2) leads to a ﬁnal com-
putational complexity of O(m2).

In addition to the asymptotic consistency result, we provide
a ﬁnite sample bound on the deviation between the differ-
ence of two population HSIC statistics and the difference
of two empirical HSIC estimates.

Theorem 5 (Generalization bound on the difference of
empirical HSIC statistics). Assume that k, l, and d are
bounded almost everywhere by 1, and are non-negative.

where α > 0.24 and C are constants.

Proof. In Gretton et al., (2005) a ﬁnite sample bound is
given for a single HSIC statistic. Eq. (14) is proved by
using a union bound.

Corollary 1. HSIC XY
ulation statistic at rate O(

m −HSIC XZ
√
m).

m converges to the pop-

3.2. A simple consistent test via uncorrelated HSICs

From the result in Eq. (5), a simple, consistent test of
relative dependence can be constructed as follows: split
the samples from Px into two equal sized sets denoted
by X (cid:48) and X (cid:48)(cid:48), and drop the second half of the sam-
ple pairs with Y and the ﬁrst half of the sample pairs
with Z. We will denote the remaining samples as Y (cid:48)
and Z (cid:48)(cid:48). We can now estimate the joint distribution of
√

m[HSIC X (cid:48)Y (cid:48)

m/2 , HSIC X (cid:48)(cid:48)Z(cid:48)(cid:48)

m/2

]T as

(cid:18)(cid:18)HSIC(F, G, Pxy)
HSIC(F, H, Pxz)

(cid:19)

,

N

(cid:18)σ2

X (cid:48)Y (cid:48)
0

(cid:19)(cid:19)

0

σ2

X (cid:48)(cid:48)Z(cid:48)(cid:48)

, (15)

which we will write as N (µ(cid:48), Σ(cid:48)). Given this joint
distribution, we need to determine the distribution over
space deﬁned by HSIC(F, G, Pxy) <
the half
HSIC(F, H, Pxz). As in the previous section, we achieve
this by rotating the distribution by π
4 counter-clockwise
about the origin, and integrating the resulting distribution
projected onto the ﬁrst axis (cf. Fig. 3). The resulting pro-
jection of the rotated distribution onto the primary axis is
N (cid:0)[Qµ(cid:48)]1 , (cid:2)QΣ(cid:48)QT (cid:3)

(16)

(cid:1)

11

where

√

2
2

1
2

[Qµ(cid:48)]1 =

(HSIC(F, G, Pxy) − HSIC(F, H, Pxz)) ,

(17)

(18)

[QΣ(cid:48)QT ]11 =

(σ2

X (cid:48)Y (cid:48) + σ2

X (cid:48)(cid:48)Z(cid:48)(cid:48) ).

From this empirically estimated distribution, it is straight-
forward to construct a consistent test (cf. Eq. (12)). The
power of this test varies inversely with the variance of the
distribution in Eq. (16).

A low variance consistent test of relative dependency

3.3. The dependent test is more powerful

While discarding half the samples leads to a consistent test,
we might expect some loss of power over the approach in
Section 3.1, due to the increase in variance with lower sam-
ple size. In this section, we prove the Section 3.1 test is
more powerful than that of Section 3.2, regardless of Pxy
and Pxz.

We call the simple and consistent approach in Section 3.2,
the independent approach, and the lower variance approach
in Section 3.1, the dependent approach. The following the-
orem compares these approaches.

Theorem 6. The asymptotic relative efﬁciency (ARE) of the
independent approach relative to the dependent approach
is always greater than 1.

Remark 1. The asymptotic relative efﬁciency (ARE) is de-
ﬁned in e.g. Serﬂing (1981, Chap.5, Section 1.15.4). If mA
and mB are the sample sizes at which tests ”perform equiv-
alently” (i.e. have equal power), then the ratio mA
repre-
mB
sents the relative efﬁciency. When mA and mB tend to +∞
and the ratio mA
→ L (at equivalent performance), then
mB
the value L represents the asymptotic relative efﬁciency of
procedure B relative to procedure A. This example is rele-
vant to our case since we are comparing two test statistics
with different asymptotically Normal distributions.

The following lemma is used for the proof of Theorem 6.

Lemma 1. (Lower Variance) The variance of the depen-
dent test statistic is smaller than the variance of the inde-
pendent test statistic.

X (cid:48)Y (cid:48) = 2σ2
XY + σ2

Proof. From the convergence of moments in the applica-
tion of the central limit theorem (von Bahr, 1965), we
have that σ2
XY . Then the variance summary
in Eq. (11) is 1
2 (σ2
XZ − 2σXY XZ) and the variance
summary in Equation (18) is 1
XY + 2σ2
2 (2σ2
XZ) where in
√
m. We have that the
both cases the statistic is scaled by
variance of the independent test statistic is smaller than the
variance of the dependent test statistic when

1
2

(σ2

XY + σ2

1
XZ − 2σXY XZ) <
2
XY + σ2
⇐⇒ −2σXY XZ < σ2

(2σ2

XZ

XY + 2σ2

XZ)

(19)

which is implied by the positive deﬁniteness of Σ.

Proof of Theorem 6. The Type II error probability of the
independent test at level α is







Φ

Φ−1(1 − α) −

m−1/2(cid:0)HSIC(F, G, Pxy)
−HSIC(F, H, Pxz)(cid:1)
(cid:112)σ2

X (cid:48)Y (cid:48) + σ2

X (cid:48)(cid:48)Z(cid:48)(cid:48)







,

(20)

where we again make the most conservative possible as-
sumption that HSIC(F, G, Pxy) − HSIC(F, H, Pxz) =
0 under the null. The Type II error probability of the de-
pendent test at level α is







Φ

Φ−1(1 − α) −

m−1/2(cid:0)HSIC(F, G, Pxy)
−HSIC(F, H, Pxz)(cid:1)
XY + σ2
XZ − 2σXY XZ

(cid:112)σ2







(21)

where Φ is the CDF of the standard normal distribution.
The numerator in Eq. (20) is the same as the numerator in
Eq. (21), and the denominator in Eq. (21) is smaller due to
Lemma 1. The lower variance dependent test therefore has
higher ARE, i.e., for a sufﬁcient sample size m > τ for
some distribution dependent τ ∈ N+, the dependent test
will be more powerful than the independent test.

4. Generalizing to more than two HSIC

statistics

the dependence test

The generalization of
to more
than three random variables follows from the earlier
derivation by applying successive rotations to a higher
dimensional
joint Gaussian distribution over multiple
HSIC statistics. We assume a sample S of size m
over n domains with kernels k1, . . . , kn associated
uniquely with respective reproducing kernel Hilbert
spaces F1, . . . , Fn. We deﬁne a generalized statisti-
cal test, Tg(S) → {0, 1} to test the null hypothesis
H0
(x,y)∈{1,...,n}2 v(x,y)HSIC(Fx, Fy, Pxy) ≤
0
:
(cid:80)
(x,y)∈{1,...,n}2 v(x,y)HSIC(Fx, Fy, Pxy) > 0, where
v is a vector of weights on each HSIC statistic. We
in the previous section by set-
may recover the test
ting v(1,2) = +1 v(1,3) = −1 and v(i,j) = 0 for all
(i, j) ∈ {1, 2, 3}2 \ {(1, 2), (1, 3)}.

hypothesis Hm

: (cid:80)
versus

alternative

the

The derivation of the test follows the general strategy used
in the previous section: we construct a rotation matrix so
as to project the joint Gaussian distribution onto the ﬁrst
axis, and read the p-value from a standard normal table. To
construct the rotation matrix, we simply need to rotate v
such that it is aligned with the ﬁrst axis. Such a rotation
can be computed by composing n 2-dimensional rotation
matrices as in Algorithm 1.

5. Experiments

We apply our estimates of statistical dependence to three
challenging problems. The ﬁrst is a synthetic data experi-
ment, in which we can directly control the relative degree
of functional dependence between variates. The second ex-
periment uses a multilingual corpus to determine the rela-
tive relations between European languages. The last exper-

A low variance consistent test of relative dependency

Algorithm 1 Successive rotation for generalized high-
dimensional relative tests of dependency (cf. Section 4)
Require: v ∈ Rn
Ensure: [Qv]i = 0 ∀i (cid:54)= 1, QT Q = I

Q = I
for i = 2 to n do

vi
[Qv]1

Qi = I; θ = − tan−1
[Qi]11 = cos(θ); [Qi]1i = − sin(θ)
[Qi]i1 = sin(θ); [Qi]ii = cos(θ)
Q = QiQ

end for

)
1
,
0
(

N
1
γ
+
)
t
(
n
i
s

)
1
,
0
(

N
2
γ
+
)
t
(
n
i
s
t

)
1
,
0
(

N
3
γ
+
)
t
(
s
o
c
t

t + γ1N (0, 1)
(a) γ1 = 0.3

t cos(t) + γ2N (0, 1)
(b) γ2 = 0.3

t cos(t) + γ3N (0, 1)
(c) γ3 = 0.6

Figure 1. Illustration of a synthetic dataset sampled from the dis-
tribution in Eq. (22).

iment is a 3-block dataset which combines gene expression,
comparative genomic hybridization, and a qualitative phe-
notype measured on a sample of Glioma patients.

5.1. Synthetic experiment

We constructed 3 distributions as deﬁned in Eq. (22) and
illustrated in Figure 1.

Let t ∼ U[(0, 2π)],

(22)

(a) x1 ∼ t + γ1N (0, 1) y1 ∼ sin(t) + γ1N (0, 1)
(b) x2 ∼ t cos(t) + γ2N (0, 1) y2 ∼ t sin(t) + γ2N (0, 1)
(c) x3 ∼ t cos(t) + γ3N (0, 1) y3 ∼ t sin(t) + γ3N (0, 1)

These distributions are speciﬁed so that we can control the
relative degree of functional dependence between the vari-
ates by varying the relative size of noise scaling parameters
γ1, γ2 and γ3. The question is then whether the dependence
between (a) and (b) is larger than the dependence between
(a) and (c). In these experiments, we ﬁxed γ1 = γ2 = 0.3,
while we varied γ3, and used a Gaussian kernel with band-
width σ selected as the median pairwise distance between
data points. This kernel is sufﬁcient to obtain good perfor-
mance, although others choices exist (Gretton et al., 2012).

Figure 2. Power of the dependent and independent test as a func-
tion of γ3 on the synthetic data described in Section 5.1. For val-
ues of γ3 > 0.3 the distribution in Fig. 1(a) is closer to 1(b) than
to 1(c). The problem becomes difﬁcult as γ3 → 0.3. As predicted
by theory, the dependent test is signiﬁcantly more powerful over
almost all values of γ3 by a substantial margin.

5.2. Multilingual data

In this section, we demonstrate dependence testing to pre-
dict the relative similarity of different languages. We use
a real world dataset taken from the parallel European Par-
liament corpus (Koehn, 2005). We choose 3000 random
documents in common written in: Finnish (ﬁ), Italian (it),
French (fr), Spanish (es), Portuguese (pt), English (en),
Dutch (nl), German (de), Danish (da) and Swedish (sv).
These languages can be broadly categorized into either the
Romance, Germanic or Uralic groups (Gray & Atkinson,
2003). In this dataset, we considered each language as a
random variable and each document as an observation.

Our ﬁrst goal is to test if the statistical dependence between
two languages in the same group is greater than the sta-
tistical dependence between languages in different groups.
For pre-processing, we removed stop-words (http://
www.nltk.org) and performed stemming (http://
snowball.tartarus.org). We applied the TF-IDF
model as a feature representation and used a Gaussian ker-
nel with the bandwidth σ set per language as the median
pairwise distance between documents.

In Table 1, a selection of tests between language groups
(Germanic, Romance, and Uralic) is given: all p-values
strongly support that our relative dependence test ﬁnds the
different language groups with very high signiﬁcance.

Figure 2 shows the power of the dependent and the inde-
pendent tests as we vary γ3. It is clear from these results
that the dependent test is far more powerful than the inde-
pendent test over the great majority of γ3 values consid-
ered. Figure 3 demonstrates that this superior test power
arises due to the tighter and more concentrated distribution
of the dependent statistic.

Further, if we focus on the Romance family, our test en-
ables one to answer more ﬁne-grained questions about the
relative similarity of languages within the same group. As
before, we determine the ground truth similarities from the
topology of the tree of European languages determined by
the linguistics community (Gray & Atkinson, 2003; Bouck-
aert et al., 2012) as illustrated in Fig. 4 for the Romance

A low variance consistent test of relative dependency

(a) m=500, γ3 = 0.7
pdep = 0.0189, pindep = 0.3492

(b) m=1000, γ3 = 0.7
pdep = 10−4, pindep = 0.3690

(c) m=3000, γ3 = 0.7
pdep = 10−6, pindep = 0.2876

(d) m=500, γ3 = 1.7
pdep = 10−9, pindep = 0.982

(e) m=1000, γ3 = 1.7
pdep = 10−10, pindep = 0.0326

(f) m=3000, γ3 = 1.7
pdep = 10−13, pindep = 0.005

Figure 3. For the synthetic experiments described in Section 5.1, we plot empirical HSIC values for dependent and independent tests for
100 repeated draws with different sample sizes. Empirical p-values for each test show that the dependent distribution converges faster
than the independent distribution even at low sample size, resulting in a more powerful statistical test.

Source Target 1 Target 2 p-value
0.0066
0.0418
0.0169
0.0173
< 10−4
< 10−4
< 10−6
< 10−4
< 10−4

pt
it
es
es
nl
en
sv
en
de

ﬁ
da
ﬁ
da
ﬁ
es
fr
it
es

es
fr
it
pt
de
nl
da
sv
en

Table 1. A selection of relative dependency tests between two
pairs of HSIC statistics for the multilingual corpus data. Low p-
values indicate a source is closer to target 1 than to target 2. In all
cases, the test correctly identiﬁes that languages within the same
group are more strongly related than those in different groups.

group. We have run the test on all triplets from the cor-
pus for which the topology of the tree speciﬁes a correct
ordering of the dependencies. In a fraction of a second (ex-
cluding kernel computation), we are able to recover certain
features of the subtree of relationships between languages
present in the Romance language group (Table 2). The test
always indicates the correct relative similarity of languages
when nearby languages (pt,es) are compared with those fur-
ther away (ft,it), however errors are made when comparing
triplets of languages for which the nearest common ances-
tor is more than one link removed.

Figure 4. Partial tree of Romance languages adapted from (Gray
& Atkinson, 2003).

Source Target 1 Target 2 p-value
0.0157
0.1882
0.2147
< 10−4
< 10−4
0.7649
0.0011
< 10−8

es
pt
fr
pt
pt
fr
es
es

fr
fr
es
es
es
pt
pt
pt

it
it
it
it
fr
it
it
fr

Table 2. Relative dependency tests between Romance languages.
The tests are ordered such that a low p-value corresponds with a
conﬁrmation of the topology of the tree of Romance languages de-
termined by the linguistics community (Gray & Atkinson, 2003).

A low variance consistent test of relative dependency

dependent test
independent test

In our next tests, we evaluate our more general framework
for testing relative dependencies with more than two HSIC
statistics. We chose four languages, and tested whether the
average dependence between languages in the same group
is higher than the dependence between groups. The results
of these tests are in Table 3. As before, our test is able to
distinguish language groups with high signiﬁcance.

×10−3
6

5

4

3

2

1

0

Z
X
C
I
S
H

−1

−2

0

Source Targets p-value
de sv ﬁ < 10−9
sv en fr < 10−9
sv en it < 10−5
it es sv < 10−5
fr pt nl 0.0175

da
da
de
fr
es

Table 3. Relative dependency test between four pairs of HSIC
statistics for the multilingual corpus data. These tests show the
ability of the relative dependence test to generalize to arbitrary
numbers of HSIC statistics by constructing a rotation matrix us-
ing Algorithm 1. In all cases v = [1 1 −2].

5.3. Pediatric glioma data

Brain tumors are the most common solid tumors in children
and have the highest mortality rate of all pediatric cancers.
Despite advances in multimodality therapy, children with
pediatric high-grade gliomas (pHGG) invariably have an
overall survival of around 20% at 5 years. Depending on
their location (e.g. brainstem, central nuclei, or supraten-
torial), pHGG present different characteristics in terms of
radiological appearance, histology, and prognosis. The hy-
pothesis is that pHGG have different genetic origins and
oncogenic pathways depending on their location. Thus, the
biological processes involved in the development of the tu-
mor may be different from one location to another.

In order to evaluate such hypotheses, pre-treatment frozen
tumor samples were obtained from 53 children with newly
diagnosed pHGG from Necker Enfants Malades (Paris,
France) from Puget et al, (2012). The 53 tumors are di-
vided into 3 locations: supratentorial (HEMI), central nu-
clei (MIDL), and brain stem (DIPG). The ﬁnal dataset is or-
ganized in 3 blocks of variables deﬁned for the 53 tumors:
X is a block of indicator variables describing the location
category, the second data matrix Y provides the expression
of 15 702 genes (GE). The third data matrix Z contains the
imbalances of 1229 segments (CGH) of chromosomes.

For X, we use a linear kernel, which is characteristic for
indicator variables, and for Y and Z, the kernel was cho-
sen to be the Gaussian kernel with σ selected as the median
of pairwise distances. The p-value of our relative depen-
dency test is < 10−5. This shows that the tumor location
in the brain is more dependent on gene expression than on
chromosomal imbalances. By contrast with Section 5.1,
the independent test was also able to ﬁnd the same order-

1

2

4

5

6 ×10−3

3
HSIC XY

Figure 5. 2σ iso-curves of the Gaussian distributions estimated
from the pediatric Glioma data. As before, the dependent test
has a much lower variance than the independent test. The tests
support the stronger dependence on the tumor location to gene
expression than chromosomal imbalances.

ing of dependence, but with a p-value that is three orders of
magnitude larger (p = 0.005). Figure 5 shows iso-curves
of the Gaussian distributions estimated in the independent
and dependent tests. The empirical relative dependency is
consistent with ﬁndings in the medical literature, and pro-
vides additional statistical support for the importance of
tumor location in Glioma (Gilbertson & Gutmann, 2007;
Palm et al., 2009; Puget et al., 2012).

6. Conclusions

We have described a novel statistical test that determines
whether a source random variable is more strongly de-
pendent on one target random variable or another. This
test, built on the Hilbert-Schmidt Independence Criterion,
is low variance, consistent, and unbiased. We have shown
that our test is strictly more powerful than a test that does
not exploit the covariance between HSIC statistics, and
empirically achieves p-values several orders of magnitude
smaller. We have empirically demonstrated the test perfor-
mance on synthetic data, where the degree of dependence
could be controlled; on the challenging problem of iden-
tifying language groups from a multilingual corpus; and
for ﬁnding the most important determinant of Glioma type.
The computation and memory requirements of the test are
quadratic in the sample size, matching the performance of
HSIC and related tests for dependence between two ran-
dom variables. The test is therefore scalable to the wide
range of problem instances where non-parametric depen-
dency tests are currently applied. We have generalized the
test framework to more than two HSIC statistics, and have
given an algorithm to construct a consistent, low-variance,
unbiased test in this setting.

Acknowledgements

We thank Ioannis Antonoglou for helpful discussions. The
ﬁrst author is supported by a fellowship from Centrale-

A low variance consistent test of relative dependency

Sup´elec. This work is partially funded by the Euro-
pean Commission through ERC Grant 259112 and FP7-
MCCIG334380.

Gretton, A. and Gyorﬁ, L. Consistent nonparametric tests
Journal of Machine Learning Re-

of independence.
search, 11:1391–1423, 2010.

References

Arcones, M. A. and Gine, E. Limit theorems for U-
processes. The Annals of Probability, pp. 1494–1542,
1993.

Bouckaert, R., Lemey, P., Dunn, M., Greenhill, S. J., Alek-
seyenko, A. V., Drummond, A. J., Gray, R. D., Suchard,
M. A., and Atkinson, Q. D. Mapping the origins and ex-
pansion of the Indo-European language family. Science,
337(6097):957–960, 2012.

Bring, J. A geometric approach to compare variables in
a regression model. The American Statistician, 50(1):
57–62, 1996.

Cortes, C., Mohri, M., and Rostamizadeh, A. Learning
non-linear combinations of kernels. In Neural Informa-
tion Processing Systems, 2009.

Cortes, C., Mohri, M., and Rostamizadeh, A. Algorithms
for learning kernels based on centered alignment. Jour-
nal of Machine Learning Research, 13:795–828, 2012.

Darlington, Richard B. Multiple regression in psychologi-
cal research and practice. Psychological bulletin, 69(3):
161, 1968.

Dauxois, J. and Nkiet, G. M. Nonlinear canonical analy-
sis and independence tests. Annals of Statistics, 26(4):
1254–1278, 1998.

Fukumizu, K., Bach, F. R., and Gretton, A.

Statisti-
cal consistency of kernel canonical correlation analysis.
The Journal of Machine Learning Research, 8:361–383,
2007.

Fukumizu, K., Gretton, A., Sun, X., and Sch¨olkopf, B. Ker-
In Advances
nel measures of conditional dependence.
in Neural Information Processing Systems, pp. 489–496.
MIT Press, 2008.

Gretton, A., Bousquet, O., Smola, A. J., and Sch¨olkopf, B.
Measuring statistical dependence with Hilbert-Schmidt
In Algorithmic Learning Theory, pp. 63–77,
norms.
2005.

Gretton, A., Fukumizu, K., Teo, C.-H., Song, L.,
Sch¨olkopf, B., and Smola, A. J. A kernel statistical test
of independence. In Neural Information Processing Sys-
tems, pp. 585–592, 2008.

Gretton, A., Sejdinovic, D., Strathmann, H.and Balakrish-
nan, S., Pontil, M., Fukumizu, K., and Sriperumbudur,
B. K. Optimal kernel choice for large-scale two-sample
tests. In Advances in Neural Information Processing Sys-
tems, pp. 1205–1213, 2012.

Gunn, S. R. and Kandola, J. S. Structural modelling with
sparse kernels. Machine Learning, 48(1):137–163, 2002.

Heller, R., Heller, Y., and Gorﬁne, M. A consistent mul-
tivariate test of association based on ranks of distances.
Biometrika, 100(2):503–510, 2013.

Hoeffding, W. Probability inequalities for sums of bounded
random variables. Journal of the American statistical
association, 58(301):13–30, 1963.

Kinney, J. B. and Atwal, G. S. Equitability, mutual infor-
mation, and the maximal information coefﬁcient. Pro-
ceedings of the National Academy of Sciences, 2014.

Koehn, P. Europarl: A parallel corpus for statistical ma-
chine translation. In MT summit, volume 5, pp. 79–86,
2005.

Palm, T., Figarella-Branger, D., Chapon, F., Lacroix, C.,
Gray, F., Scaravilli, F., Ellison, D. W., Salmon, I.,
Vikkula, M., and Godfraind, C. Expression proﬁling
of ependymomas unravels localization and tumor grade-
speciﬁc tumorigenesis. Cancer, 115(17):3955–3968,
2009.

Gilbertson, R. J. and Gutmann, D. H. Tumorigenesis in the
brain: location, location, location. Cancer research, 67
(12):5579–5582, 2007.

Peters, C., Braschler, M., and Clough, P. Multilin-
gual Information Retrieval: From Research to Practice.
Springer, 2012.

Gray, R. D. and Atkinson, Q. D. Language-tree divergence
times support the Anatolian theory of Indo-European
origin. Nature, 426(6965):435–439, 2003.

Gretton, A. A simpler condition for consistency of a kernel

independence test. arXiv:1501.06103, 2015.

Puget, S., Philippe, C., Bax, D., Job, B., Varlet, P., Ju-
nier, M. P., Andreiuolo, F., Carvalho, D., Reis, R.,
and Guerrini-Rousseau, L. Mesenchymal transition
and PDGFRA ampliﬁcation/mutation are key distinct
oncogenic events in pediatric diffuse intrinsic pontine
gliomas. PloS one, 7(2):e30313, 2012.

A low variance consistent test of relative dependency

Reshef, D., Reshef, Y., Finucane, H., Grossman, S.,
McVean, G., Turnbaugh, P., Lander, E., Mitzenmacher,
M., and Sabeti, P. Detecting novel associations in large
datasets. Science, 334(6062), 2011.

Sejdinovic, D., Gretton, A., and Bergsma, W. A kernel
test for three-variable interactions. In Neural Informa-
tion Processing Systems, 2013a.

Sejdinovic, D., Sriperumbudur, B., Gretton, A., and Fuku-
mizu, K. Equivalence of distance-based and RKHS-
based statistics in hypothesis testing. Annals of Statis-
tics, 41(5):2263–2702, 2013b.

Sen, A. and Srivastava, M. Regression Analysis – Theory,
Methods, and Applications. Springer-Verlag, 2011.

Serﬂing, R. J. Approximation theorems of mathematical
statistics. Wiley Series in Probability and Statistics. Wi-
ley, 1981.

Song, L., Smola, A., Gretton, A., Bedo, J., and Borg-
wardt, K. Feature selection via dependence maximiza-
tion. Journal of Machine Learning Research, 13:1393–
1434, 2012.

Sz´ekely, G., Rizzo, M., and Bakirov, N. Measuring and
testing dependence by correlation of distances. Annals
of Statistics, 35(6):2769–2794, 2007.

Trommershauser, J., Kording, K., and Landy, M. S. Sen-
sory Cue Integration. Oxford University Press, 2011.

von Bahr, Bengt. On the convergence of moments in
the central limit theorem. The Annals of Mathematical
Statistics, 36(3):808–818, 06 1965.

Zhang, K., Peters, J., Janzing, D., B., and Sch¨olkopf, B.
Kernel-based conditional independence test and applica-
tion in causal discovery. In 27th Conference on Uncer-
tainty in Artiﬁcial Intelligence, pp. 804–813, 2011.

5
1
0
2
 
y
a
M
 
7
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
2
5
8
3
.
6
0
4
1
:
v
i
X
r
a

A low variance consistent test of relative dependency

Wacha Bounliphone
CentraleSup´elec & Inria, Grande Voie des Vignes, 92295 Chˆatenay-Malabry, France

WACHA.BOUNLIPHONE@CENTRALESUPELEC.FR

Arthur Gretton
Gatsby Computational Neuroscience Unit, University College London, United Kingdom

ARTHUR.GRETTON@GMAIL.COM

Arthur Tenenhaus
CentraleSup´elec, 3 rue Joliot-Curie, 91192 Gif-Sur-Yvette, France

ARTHUR.TENENHAUS@CENTRALESUPELEC.FR

Matthew B. Blaschko
Inria & CentraleSup´elec, Grande Voie des Vignes, 92295 Chˆatenay-Malabry, France

MATTHEW.BLASCHKO@INRIA.FR

Abstract

1. Introduction

We describe a novel non-parametric statistical
hypothesis test of relative dependence between
a source variable and two candidate target vari-
ables.
Such a test enables us to determine
whether one source variable is signiﬁcantly more
dependent on a ﬁrst target variable or a sec-
ond. Dependence is measured via the Hilbert-
Schmidt Independence Criterion (HSIC), result-
ing in a pair of empirical dependence mea-
sures (source-target 1, source-target 2). We test
whether the ﬁrst dependence measure is signif-
icantly larger than the second. Modeling the
covariance between these HSIC statistics leads
to a provably more powerful test than the con-
struction of independent HSIC statistics by sub-
sampling. The resulting test is consistent and
unbiased, and (being based on U-statistics) has
favorable convergence properties. The test can
be computed in quadratic time, matching the
computational complexity of standard empiri-
cal HSIC estimators. The effectiveness of the
test is demonstrated on several real-world prob-
lems: we identify language groups from a mul-
tilingual corpus, and we prove that tumor lo-
cation is more dependent on gene expression
than chromosomal imbalances. Source code is
available for download at https://github.
com/wbounliphone/reldep.

Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copy-
right 2015 by the author(s).

Tests of dependence are important tools in statistical anal-
ysis, and are widely applied in many data analysis con-
texts. Classical criteria include Spearman’s ρ and Kendall’s
τ , which can detect non-linear monotonic dependencies.
More recent research on dependence measurement has fo-
cused on non-parametric measures of dependence, which
apply even when the dependence is nonlinear, or the vari-
ables are multivariate or non-euclidean (for instance im-
ages, strings, and graphs). The statistics for such tests are
diverse, and include kernel measures of covariance (Gret-
ton et al., 2008; Zhang et al., 2011) and correlation (Daux-
ois & Nkiet, 1998; Fukumizu et al., 2008), distance co-
variances (which are instances of kernel tests) (Sz´ekely
et al., 2007; Sejdinovic et al., 2013b), kernel regression
tests (Cortes et al., 2009; Gunn & Kandola, 2002), rank-
ings (Heller et al., 2013), and space partitioning approaches
(Gretton & Gyorﬁ, 2010; Reshef et al., 2011; Kinney & At-
wal, 2014). Specialization of such methods to univariate
linear dependence can yield similar tests to classical ap-
proaches such as Darlington (1968); Bring (1996).

For many problems in data analysis, however, the question
of whether dependence exists is secondary: there may be
multiple dependencies, and the question becomes which
dependence is the strongest. For instance, in neuroscience,
multiple stimuli may be present (e.g. visual and audio), and
it is of interest to determine which of the two has a stronger
inﬂuence on brain activity (Trommershauser et al., 2011).
In automated translation (Peters et al., 2012), it is of inter-
est to determine whether documents in a source language
are a signiﬁcantly better match to those in one target lan-
guage than to another target language, either as a measure
of difﬁculty of the respective learning tasks, or as a basic
tool for comparative linguistics.

A low variance consistent test of relative dependency

We present a statistical test which determines whether two
target variables have a signiﬁcant difference in their de-
pendence on a third, source variable. The dependence be-
tween each of the target variables and the source is com-
puted using the Hilbert-Schmidt Independence Criterion
(Gretton et al., 2005; 2008).1 Care must be taken in an-
alyzing the asymptotic behavior of the test statistics, since
the two measures of dependence will themselves be cor-
related: they are both computed with respect to the same
source. Thus, we derive the joint asymptotic distribution
of both dependencies. The derivation of our test utilizes
classical results of U -statistics (Hoeffding, 1963; Serﬂing,
1981; Arcones & Gine, 1993). In particular, we make use
of results by Hoeffding (1963) and Serﬂing (1981) to deter-
mine the asymptotic joint distributions of the statistics (see
Theorem 4). Consequently, we derive the lowest variance
unbiased estimator of the test statistic.

We prove our approach to have greater statistical power
than constructing two uncorrelated statistics on the same
data by subsampling, and testing on these. In experiments,
we are able to successfully test which of two variables is
most strongly related to a third, in synthetic examples, in a
language group identiﬁcation task, and in a task for iden-
tifying the relative strength of factors for Glioma type in a
pediatric patient population.

there do not exist competing non-
To our knowledge,
parametric tests to determine which of two dependencies
is strongest. One related area is that of multiple regression
analysis (e.g. (Sen & Srivastava, 2011)). In this case a lin-
ear model is assumed, and it is determined whether individ-
ual inputs have a statistically signiﬁcant effect on an output
variable. The procedure does not address the question of
whether the inﬂuence of one variable is higher than that of
another to a statistically signiﬁcant degree. The problem of
variable selection has also been investigated in the case of
nonlinear relations between the inputs and outputs (Cortes
et al., 2009; 2012; Song et al., 2012), however this again
does not address which of two variables most strongly in-
ﬂuences a third. A less closely related area is that of detect-
ing three-variable interactions (Sejdinovic et al., 2013a),
where it is determined whether there exists any factoriza-
tion of the joint distribution over three variables. This test
again does not address the issue of ﬁnding which connec-
tions are strongest, however.

1Dependency can also be tested with the correlation operator.
However, Fukumizu et al., (2007) show that unlike the covariance
operator, the asymptotic distribution of the norm of the correlation
operator is unknown, so the construction of a computationally ef-
ﬁcient test of relative dependence remains an open problem.

2. Deﬁnitions and description of HSIC

We base our underlying notion of dependence on the
Hilbert-Schmidt Independence Criterion (Gretton et al.,
2005; 2008; Song et al., 2012). All results in this section
except for Problem 1 can be found in these previous works.

Deﬁnition 1.
Hilbert-Schmidt Independence Criterion)

(Gretton et al., 2005, Deﬁnition 1,Lemma 1:

Let Pxy be a Borel probability measure over over (X ×
Y, Γ × Λ) with Γ and Λ the respective Borel sets on X and
Y, and Px and Py the marginal distributions on domains
X and Y. Given separable RKHSs F and G,
the Hilbert-
Schmidt Independence Criterion (HSIC) is deﬁned as the
squared HS-norm of the associated cross-covariance oper-
ator Cxy. When the kernels k, l are associated uniquely
withs respective RKHSs F and G and bounded, HSIC can
be expressed in terms of expectations of kernel functions

HSIC(F, G, Pxy) := (cid:107)Cxy(cid:107)2
HS
= Exx(cid:48)yy(cid:48) [k(x, x(cid:48))l(y, y(cid:48))] + Exx(cid:48) [k(x, x(cid:48))] Eyy(cid:48) [l(y, y(cid:48))]
− 2Exy [Ex(cid:48)[k(x, x(cid:48))]Ey(cid:48)[l(y, y(cid:48))]] .
(1)

HSIC determines independence: HSIC = 0 iff Pxy = PxPy
when kernels k and l are characteristic on their respective
marginal domains (Gretton, 2015).

With this choice, the problem we would like to solve is
described as follows:

Problem 1. Given separable RKHSs F, G, and H with
HSIC(F, G, Pxy) > 0 and HSIC(F, H, Pxz) > 0,
we test the null hypothesis H0 : HSIC(F, G, Pxy) ≤
HSIC(F, H, Pxz) versus the alternative hypothesis H1 :
HSIC(F, G, Pxy) > HSIC(F, H, Pxz) at a given sig-
niﬁcance level α.

We now describe the asymptotic behavior of the HSIC for
dependent variables.

(Song et al., 2012, Theorem 2: Unbiased
Theorem 1.
estimator for HSIC(F, G, Pxy)) We denote by S the set
of observations {(x1, y1), ..., (xm, ym)} of size m drawn
i.i.d. from Pxy. The unbiased estimator HSICm(F, G, S)
is given by

HSICm(F, G, S) =

1
m(m − 3)

×

(cid:34)

Tr( ˜K˜L) +

1(cid:48) ˜K11(cid:48) ˜L1
(m − 1)(m − 2)

−

2
m − 2

(2)

(cid:35)
1(cid:48) ˜K˜L1

where ˜K and ˜L are related to K and L by ˜Kij = (1 −
δij) ˜Kij and ˜Lij = (1 − δij)˜Lij.
(Song et al., 2012, Theorem 3: U-statistic of
Theorem 2.
HSIC) This ﬁnite sample unbiased estimator of HSIC XY
m

A low variance consistent test of relative dependency

can be written as a U-statistic,

HSIC XY

m = (m)−1
4

hijqr

(3)

(cid:88)

(i,j,q,r)∈im
4

m!
(m − 4)!

where (m)4 :=

4 denotes the
set of all 4−tuples drawn without replacement from the set
{1, . . . m}, and the kernel h of the U-statistic is deﬁned as

, the index set im

hijqr =

kst(lst + luv − 2lsu)

(4)

1
24

(i,j,q,r)
(cid:88)

(s,t,u,v)

where the kernels k and l are associated uniquely with re-
spective reproducing kernel Hilbert spaces F and G.

(Gretton et al., 2008, Theorem 1: Asymptotic
Theorem 3.
distribution of HSICm) If E[h2] < ∞, and source and
targets are not independent, then, under H1, as m → ∞,

√

d−→ N (0, σ2

m(HSIC XY

m − HSIC(F, G, Pxy))

where σ2
with Ej,q,r
:= ESj ,Sq,Sr .
mate is ˆσXY = 16 (cid:0)RXY − (HSIC XY

XY )
(5)
Ei (Ej,q,rhijqr)2 − HSIC(F, G, Pxy))
Its empirical esti-
m )2(cid:1) where

2

XY = 16

(cid:16)

(cid:17)

RXY =

m
(cid:88)


(m − 1)−1
3

1
m

(cid:88)

hijqr



and

(j,q,r)∈im
i=1
3 \ {i} denotes the set of all 3−tuples drawn

the index set im
without replacement from the set {1, . . . m} \ {i}.

3 \{i}

3. A test of relative dependence

In this section we calculate two dependent HSIC statistics
and derive the joint asymptotic distribution of these depen-
dent quantities, which is used to construct a consistent test
for Problem 1. We next construct a simpler consistent test,
by computing two independent HSIC statistics on sample
subsets. While the simpler strategy is superﬁcially attrac-
tive and less effort to implement, we prove the dependent
strategy is strictly more powerful.

3.1. Joint asymptotic distribution of HSIC and test

m and HSIC XZ

In the present section, we compute each HSIC estimate
on the full dataset, and explicitly obtain the correlations
between the resulting empirical dependence measurements
HSIC XY
m . We denote by S1 = (X, Y, Z)
the joint sample of observations which are drawn i.i.d. with
respective Borel probability measure Pxyz deﬁned on the
domain X × Y × Z. The kernels k, l and d are associated
uniquely with respective reproducing kernel Hilbert spaces
F, G and H. Moreover, K, L and D ∈ Rm×m are kernel
matrices containing kij = k(xi, xj), lij = l(yi, yj) and

1
24

1
24

(i,j,q,r)
(cid:88)

(s,t,u,v)

(i,j,q,r)
(cid:88)

(s,t,u,v)

dij = d(zi, zj). Let HSIC XY
m be respec-
tively the unbiased estimators of HSIC(F, G, Pxy) and
HSIC(F, H, Pxz), written as a sum of U-statistics with
respective kernels hijqr and gijqr as described in (4),

m and HSIC XZ

hijqr =

kst(lst + luv − 2lsu),

gijqr =

kst(dst + duv − 2dsu).

(6)

Theorem 4. (Joint asymptotic distribution of HSIC) If
E[h2] < ∞ and E[g2] < ∞, then

(cid:18)(cid:18)

√

m

HSIC XY
m
HSIC XZ
(cid:18)(cid:16)0
(cid:17)
0

m

d−→ N

(cid:19)

−

(cid:18)HSIC(F, G, Pxy)
HSIC(F, H, Pxz)
(cid:19)(cid:19)

(cid:19)(cid:19)

(cid:18) σ2
XY
σXY XZ

,

σXY XZ
σ2
XZ

,

(7)

XY and σ2

estimate of σXY XZ is

XZ are as in Theorem 3.
ˆσXY XZ

The
=

(cid:0)RXY XZ − HSIC XY

m HSIC XZ
m

(cid:1), where

where σ2
empirical
16
m

RXY XZ =


(m − 1)−2
3

1
m

m
(cid:88)

i=1

(cid:88)

hijqrgijqr

 .

(j,q,r)∈im

3 \{i}



(8)

Proof. Eq.
(8) is constructed with the deﬁnition of vari-
ance of a U-statistic as given by Serﬂing, Ch. 5 (1981),
where one variable is ﬁxed. Eq. (7) follows from the appli-
cation of Hoeffding, Theorem 7.1 (1963), which gives the
joint asymptotic distribution of U-statistics.

Based on the joint asymptotic distribution of HSIC de-
scribed in Theorem 4, we can now describe a statistical
test to solve Problem 1: given a sample S1 as described
in Section 3.1, T (S1) : {(X × Y × Z)m} → {0, 1} is
used to test the null hypothesis H0 : HSIC(F, G, Pxy) ≤
HSIC(F, H, Pxz) versus the alternative hypothesis H1 :
HSIC(F, G, Pxy) > HSIC(F, H, Pxz) at a given sig-
niﬁcance level α. This is achieved by projecting the distri-
m − HSIC XZ
bution to 1D using the statistic HSIC XY
m ,
and determining where the statistic falls relative to a
conservative estimate of the the 1 − α quantile of the
null. We now derive this conservative estimate. A sim-
ple way of achieving this is to rotate the distribution by
π
4 counter-clockwise about the origin, and to integrate
the resulting distribution projected onto the ﬁrst axis (cf.
Fig. 3). Denote the asymptotically normal distribution of
√
m ]T as N (µ, Σ). The distribution

m[HSIC XY

m HSIC XZ
resulting from rotation and projection is
(cid:1) ,
N (cid:0)[Qµ]1, [QΣQT ]11

(9)

A low variance consistent test of relative dependency

Then for m > 1 and all δ > 0 with probability at least
1 − δ, for all pxyz, the generalization bound on the differ-
ence of empirical HSIC statistics is

(10)

(11)

| {HSIC(F, G, Pxy) − HSIC(F, H, Pxz)}

− (cid:8)HSIC XY

m − HSIC XZ
m

(cid:9) |

(cid:40)(cid:114)

≤ 2

(cid:41)

log(6/δ)
α2m

+

C
m

(14)

√

2
2

1
2

where Q =

is the rotation matrix by π

4 and

√

2
2

(cid:17)
(cid:16)1 −1
1
1

[Qµ]1 =

(HSIC(F, G, Pxy) − HSIC(F, H, Pxz)) ,

[QΣQT ]11 =

(σ2

XY + σ2

XZ − 2σXY XZ).

Following the empirical distribution from Eq. (9), a test
m − HSIC XZ
with statistic HSIC XY

m has p-value

(cid:32)

p ≤ 1 − Φ

(HSIC XY
(cid:112)σ2
XY + σ2

m − HSIC XZ
m )
XZ − 2σXY XZ

(cid:33)

,

(12)

where Φ is the CDF of a standard normal distribution, and
we have made the most conservative possible assumption
that HSIC(F, G, Pxy) − HSIC(F, H, Pxz) = 0 under
the null (the null also allows for the difference in population
dependence measures to be negative).

in practice,

XZ and σ2

the test
the variances of
To implement
XY , σ2
σ2
XY XZ may be replaced by their empir-
ical estimates. The test will still be consistent for a large
enough sample size, since the estimates will be sufﬁciently
well converged to ensure the test is calibrated. Eq. (8) is
expensive to compute na¨ıvely, because even computing the
kernels hijqr and gijqr of the U -statistic itself is a non
trivial task. Following (Song et al., 2012, Section 2.5),
we ﬁrst form a vector hXY with entries corresponding to
(cid:80)
3 \{i} hijqr, and a vector hXZ with entries cor-
(j,q,r)∈im
responding to (cid:80)
3 \{i} gijqr. Collecting terms in
Eq. (4) related to kernel matrices ˜K and ˜L, hXY can be
written as

(j,q,r)∈im

(cid:17)
hXY = (m − 2)2 (cid:16) ˜K (cid:12) ˜L

1 − m( ˜K1) (cid:12) (˜L1)

(13)
(cid:17)
(Tr( ˜K˜L))1 − ˜K(˜L1) − ˜L( ˜K1)

(cid:16)

+ (m − 2)

+ (1T ˜L1) ˜K1 + (1T ˜K1)˜L1 − ((1T ˜K)(˜L1))1

3 hXY

where (cid:12) denotes the Hadamard product. Then RXY XZ
in Eq. (8) can be computed as RXY XZ = (4m)−1(m −
T hXZ. Using the order of operations implied by
1)−2
the parentheses in Eq. (13), the computational cost of the
cross covariance term is O(m2). Combining this with the
unbiased estimator of HSIC in Eq. (2) leads to a ﬁnal com-
putational complexity of O(m2).

In addition to the asymptotic consistency result, we provide
a ﬁnite sample bound on the deviation between the differ-
ence of two population HSIC statistics and the difference
of two empirical HSIC estimates.

Theorem 5 (Generalization bound on the difference of
empirical HSIC statistics). Assume that k, l, and d are
bounded almost everywhere by 1, and are non-negative.

where α > 0.24 and C are constants.

Proof. In Gretton et al., (2005) a ﬁnite sample bound is
given for a single HSIC statistic. Eq. (14) is proved by
using a union bound.

Corollary 1. HSIC XY
ulation statistic at rate O(

m −HSIC XZ
√
m).

m converges to the pop-

3.2. A simple consistent test via uncorrelated HSICs

From the result in Eq. (5), a simple, consistent test of
relative dependence can be constructed as follows: split
the samples from Px into two equal sized sets denoted
by X (cid:48) and X (cid:48)(cid:48), and drop the second half of the sam-
ple pairs with Y and the ﬁrst half of the sample pairs
with Z. We will denote the remaining samples as Y (cid:48)
and Z (cid:48)(cid:48). We can now estimate the joint distribution of
√

m[HSIC X (cid:48)Y (cid:48)

m/2 , HSIC X (cid:48)(cid:48)Z(cid:48)(cid:48)

m/2

]T as

(cid:18)(cid:18)HSIC(F, G, Pxy)
HSIC(F, H, Pxz)

(cid:19)

,

N

(cid:18)σ2

X (cid:48)Y (cid:48)
0

(cid:19)(cid:19)

0

σ2

X (cid:48)(cid:48)Z(cid:48)(cid:48)

, (15)

which we will write as N (µ(cid:48), Σ(cid:48)). Given this joint
distribution, we need to determine the distribution over
space deﬁned by HSIC(F, G, Pxy) <
the half
HSIC(F, H, Pxz). As in the previous section, we achieve
this by rotating the distribution by π
4 counter-clockwise
about the origin, and integrating the resulting distribution
projected onto the ﬁrst axis (cf. Fig. 3). The resulting pro-
jection of the rotated distribution onto the primary axis is
N (cid:0)[Qµ(cid:48)]1 , (cid:2)QΣ(cid:48)QT (cid:3)

(16)

(cid:1)

11

where

√

2
2

1
2

[Qµ(cid:48)]1 =

(HSIC(F, G, Pxy) − HSIC(F, H, Pxz)) ,

(17)

(18)

[QΣ(cid:48)QT ]11 =

(σ2

X (cid:48)Y (cid:48) + σ2

X (cid:48)(cid:48)Z(cid:48)(cid:48) ).

From this empirically estimated distribution, it is straight-
forward to construct a consistent test (cf. Eq. (12)). The
power of this test varies inversely with the variance of the
distribution in Eq. (16).

A low variance consistent test of relative dependency

3.3. The dependent test is more powerful

While discarding half the samples leads to a consistent test,
we might expect some loss of power over the approach in
Section 3.1, due to the increase in variance with lower sam-
ple size. In this section, we prove the Section 3.1 test is
more powerful than that of Section 3.2, regardless of Pxy
and Pxz.

We call the simple and consistent approach in Section 3.2,
the independent approach, and the lower variance approach
in Section 3.1, the dependent approach. The following the-
orem compares these approaches.

Theorem 6. The asymptotic relative efﬁciency (ARE) of the
independent approach relative to the dependent approach
is always greater than 1.

Remark 1. The asymptotic relative efﬁciency (ARE) is de-
ﬁned in e.g. Serﬂing (1981, Chap.5, Section 1.15.4). If mA
and mB are the sample sizes at which tests ”perform equiv-
alently” (i.e. have equal power), then the ratio mA
repre-
mB
sents the relative efﬁciency. When mA and mB tend to +∞
and the ratio mA
→ L (at equivalent performance), then
mB
the value L represents the asymptotic relative efﬁciency of
procedure B relative to procedure A. This example is rele-
vant to our case since we are comparing two test statistics
with different asymptotically Normal distributions.

The following lemma is used for the proof of Theorem 6.

Lemma 1. (Lower Variance) The variance of the depen-
dent test statistic is smaller than the variance of the inde-
pendent test statistic.

X (cid:48)Y (cid:48) = 2σ2
XY + σ2

Proof. From the convergence of moments in the applica-
tion of the central limit theorem (von Bahr, 1965), we
have that σ2
XY . Then the variance summary
in Eq. (11) is 1
2 (σ2
XZ − 2σXY XZ) and the variance
summary in Equation (18) is 1
XY + 2σ2
2 (2σ2
XZ) where in
√
m. We have that the
both cases the statistic is scaled by
variance of the independent test statistic is smaller than the
variance of the dependent test statistic when

1
2

(σ2

XY + σ2

1
XZ − 2σXY XZ) <
2
XY + σ2
⇐⇒ −2σXY XZ < σ2

(2σ2

XZ

XY + 2σ2

XZ)

(19)

which is implied by the positive deﬁniteness of Σ.

Proof of Theorem 6. The Type II error probability of the
independent test at level α is







Φ

Φ−1(1 − α) −

m−1/2(cid:0)HSIC(F, G, Pxy)
−HSIC(F, H, Pxz)(cid:1)
(cid:112)σ2

X (cid:48)Y (cid:48) + σ2

X (cid:48)(cid:48)Z(cid:48)(cid:48)







,

(20)

where we again make the most conservative possible as-
sumption that HSIC(F, G, Pxy) − HSIC(F, H, Pxz) =
0 under the null. The Type II error probability of the de-
pendent test at level α is







Φ

Φ−1(1 − α) −

m−1/2(cid:0)HSIC(F, G, Pxy)
−HSIC(F, H, Pxz)(cid:1)
XY + σ2
XZ − 2σXY XZ

(cid:112)σ2







(21)

where Φ is the CDF of the standard normal distribution.
The numerator in Eq. (20) is the same as the numerator in
Eq. (21), and the denominator in Eq. (21) is smaller due to
Lemma 1. The lower variance dependent test therefore has
higher ARE, i.e., for a sufﬁcient sample size m > τ for
some distribution dependent τ ∈ N+, the dependent test
will be more powerful than the independent test.

4. Generalizing to more than two HSIC

statistics

the dependence test

The generalization of
to more
than three random variables follows from the earlier
derivation by applying successive rotations to a higher
dimensional
joint Gaussian distribution over multiple
HSIC statistics. We assume a sample S of size m
over n domains with kernels k1, . . . , kn associated
uniquely with respective reproducing kernel Hilbert
spaces F1, . . . , Fn. We deﬁne a generalized statisti-
cal test, Tg(S) → {0, 1} to test the null hypothesis
H0
(x,y)∈{1,...,n}2 v(x,y)HSIC(Fx, Fy, Pxy) ≤
0
:
(cid:80)
(x,y)∈{1,...,n}2 v(x,y)HSIC(Fx, Fy, Pxy) > 0, where
v is a vector of weights on each HSIC statistic. We
in the previous section by set-
may recover the test
ting v(1,2) = +1 v(1,3) = −1 and v(i,j) = 0 for all
(i, j) ∈ {1, 2, 3}2 \ {(1, 2), (1, 3)}.

hypothesis Hm

: (cid:80)
versus

alternative

the

The derivation of the test follows the general strategy used
in the previous section: we construct a rotation matrix so
as to project the joint Gaussian distribution onto the ﬁrst
axis, and read the p-value from a standard normal table. To
construct the rotation matrix, we simply need to rotate v
such that it is aligned with the ﬁrst axis. Such a rotation
can be computed by composing n 2-dimensional rotation
matrices as in Algorithm 1.

5. Experiments

We apply our estimates of statistical dependence to three
challenging problems. The ﬁrst is a synthetic data experi-
ment, in which we can directly control the relative degree
of functional dependence between variates. The second ex-
periment uses a multilingual corpus to determine the rela-
tive relations between European languages. The last exper-

A low variance consistent test of relative dependency

Algorithm 1 Successive rotation for generalized high-
dimensional relative tests of dependency (cf. Section 4)
Require: v ∈ Rn
Ensure: [Qv]i = 0 ∀i (cid:54)= 1, QT Q = I

Q = I
for i = 2 to n do

vi
[Qv]1

Qi = I; θ = − tan−1
[Qi]11 = cos(θ); [Qi]1i = − sin(θ)
[Qi]i1 = sin(θ); [Qi]ii = cos(θ)
Q = QiQ

end for

)
1
,
0
(

N
1
γ
+
)
t
(
n
i
s

)
1
,
0
(

N
2
γ
+
)
t
(
n
i
s
t

)
1
,
0
(

N
3
γ
+
)
t
(
s
o
c
t

t + γ1N (0, 1)
(a) γ1 = 0.3

t cos(t) + γ2N (0, 1)
(b) γ2 = 0.3

t cos(t) + γ3N (0, 1)
(c) γ3 = 0.6

Figure 1. Illustration of a synthetic dataset sampled from the dis-
tribution in Eq. (22).

iment is a 3-block dataset which combines gene expression,
comparative genomic hybridization, and a qualitative phe-
notype measured on a sample of Glioma patients.

5.1. Synthetic experiment

We constructed 3 distributions as deﬁned in Eq. (22) and
illustrated in Figure 1.

Let t ∼ U[(0, 2π)],

(22)

(a) x1 ∼ t + γ1N (0, 1) y1 ∼ sin(t) + γ1N (0, 1)
(b) x2 ∼ t cos(t) + γ2N (0, 1) y2 ∼ t sin(t) + γ2N (0, 1)
(c) x3 ∼ t cos(t) + γ3N (0, 1) y3 ∼ t sin(t) + γ3N (0, 1)

These distributions are speciﬁed so that we can control the
relative degree of functional dependence between the vari-
ates by varying the relative size of noise scaling parameters
γ1, γ2 and γ3. The question is then whether the dependence
between (a) and (b) is larger than the dependence between
(a) and (c). In these experiments, we ﬁxed γ1 = γ2 = 0.3,
while we varied γ3, and used a Gaussian kernel with band-
width σ selected as the median pairwise distance between
data points. This kernel is sufﬁcient to obtain good perfor-
mance, although others choices exist (Gretton et al., 2012).

Figure 2. Power of the dependent and independent test as a func-
tion of γ3 on the synthetic data described in Section 5.1. For val-
ues of γ3 > 0.3 the distribution in Fig. 1(a) is closer to 1(b) than
to 1(c). The problem becomes difﬁcult as γ3 → 0.3. As predicted
by theory, the dependent test is signiﬁcantly more powerful over
almost all values of γ3 by a substantial margin.

5.2. Multilingual data

In this section, we demonstrate dependence testing to pre-
dict the relative similarity of different languages. We use
a real world dataset taken from the parallel European Par-
liament corpus (Koehn, 2005). We choose 3000 random
documents in common written in: Finnish (ﬁ), Italian (it),
French (fr), Spanish (es), Portuguese (pt), English (en),
Dutch (nl), German (de), Danish (da) and Swedish (sv).
These languages can be broadly categorized into either the
Romance, Germanic or Uralic groups (Gray & Atkinson,
2003). In this dataset, we considered each language as a
random variable and each document as an observation.

Our ﬁrst goal is to test if the statistical dependence between
two languages in the same group is greater than the sta-
tistical dependence between languages in different groups.
For pre-processing, we removed stop-words (http://
www.nltk.org) and performed stemming (http://
snowball.tartarus.org). We applied the TF-IDF
model as a feature representation and used a Gaussian ker-
nel with the bandwidth σ set per language as the median
pairwise distance between documents.

In Table 1, a selection of tests between language groups
(Germanic, Romance, and Uralic) is given: all p-values
strongly support that our relative dependence test ﬁnds the
different language groups with very high signiﬁcance.

Figure 2 shows the power of the dependent and the inde-
pendent tests as we vary γ3. It is clear from these results
that the dependent test is far more powerful than the inde-
pendent test over the great majority of γ3 values consid-
ered. Figure 3 demonstrates that this superior test power
arises due to the tighter and more concentrated distribution
of the dependent statistic.

Further, if we focus on the Romance family, our test en-
ables one to answer more ﬁne-grained questions about the
relative similarity of languages within the same group. As
before, we determine the ground truth similarities from the
topology of the tree of European languages determined by
the linguistics community (Gray & Atkinson, 2003; Bouck-
aert et al., 2012) as illustrated in Fig. 4 for the Romance

A low variance consistent test of relative dependency

(a) m=500, γ3 = 0.7
pdep = 0.0189, pindep = 0.3492

(b) m=1000, γ3 = 0.7
pdep = 10−4, pindep = 0.3690

(c) m=3000, γ3 = 0.7
pdep = 10−6, pindep = 0.2876

(d) m=500, γ3 = 1.7
pdep = 10−9, pindep = 0.982

(e) m=1000, γ3 = 1.7
pdep = 10−10, pindep = 0.0326

(f) m=3000, γ3 = 1.7
pdep = 10−13, pindep = 0.005

Figure 3. For the synthetic experiments described in Section 5.1, we plot empirical HSIC values for dependent and independent tests for
100 repeated draws with different sample sizes. Empirical p-values for each test show that the dependent distribution converges faster
than the independent distribution even at low sample size, resulting in a more powerful statistical test.

Source Target 1 Target 2 p-value
0.0066
0.0418
0.0169
0.0173
< 10−4
< 10−4
< 10−6
< 10−4
< 10−4

pt
it
es
es
nl
en
sv
en
de

ﬁ
da
ﬁ
da
ﬁ
es
fr
it
es

es
fr
it
pt
de
nl
da
sv
en

Table 1. A selection of relative dependency tests between two
pairs of HSIC statistics for the multilingual corpus data. Low p-
values indicate a source is closer to target 1 than to target 2. In all
cases, the test correctly identiﬁes that languages within the same
group are more strongly related than those in different groups.

group. We have run the test on all triplets from the cor-
pus for which the topology of the tree speciﬁes a correct
ordering of the dependencies. In a fraction of a second (ex-
cluding kernel computation), we are able to recover certain
features of the subtree of relationships between languages
present in the Romance language group (Table 2). The test
always indicates the correct relative similarity of languages
when nearby languages (pt,es) are compared with those fur-
ther away (ft,it), however errors are made when comparing
triplets of languages for which the nearest common ances-
tor is more than one link removed.

Figure 4. Partial tree of Romance languages adapted from (Gray
& Atkinson, 2003).

Source Target 1 Target 2 p-value
0.0157
0.1882
0.2147
< 10−4
< 10−4
0.7649
0.0011
< 10−8

es
pt
fr
pt
pt
fr
es
es

fr
fr
es
es
es
pt
pt
pt

it
it
it
it
fr
it
it
fr

Table 2. Relative dependency tests between Romance languages.
The tests are ordered such that a low p-value corresponds with a
conﬁrmation of the topology of the tree of Romance languages de-
termined by the linguistics community (Gray & Atkinson, 2003).

A low variance consistent test of relative dependency

dependent test
independent test

In our next tests, we evaluate our more general framework
for testing relative dependencies with more than two HSIC
statistics. We chose four languages, and tested whether the
average dependence between languages in the same group
is higher than the dependence between groups. The results
of these tests are in Table 3. As before, our test is able to
distinguish language groups with high signiﬁcance.

×10−3
6

5

4

3

2

1

0

Z
X
C
I
S
H

−1

−2

0

Source Targets p-value
de sv ﬁ < 10−9
sv en fr < 10−9
sv en it < 10−5
it es sv < 10−5
fr pt nl 0.0175

da
da
de
fr
es

Table 3. Relative dependency test between four pairs of HSIC
statistics for the multilingual corpus data. These tests show the
ability of the relative dependence test to generalize to arbitrary
numbers of HSIC statistics by constructing a rotation matrix us-
ing Algorithm 1. In all cases v = [1 1 −2].

5.3. Pediatric glioma data

Brain tumors are the most common solid tumors in children
and have the highest mortality rate of all pediatric cancers.
Despite advances in multimodality therapy, children with
pediatric high-grade gliomas (pHGG) invariably have an
overall survival of around 20% at 5 years. Depending on
their location (e.g. brainstem, central nuclei, or supraten-
torial), pHGG present different characteristics in terms of
radiological appearance, histology, and prognosis. The hy-
pothesis is that pHGG have different genetic origins and
oncogenic pathways depending on their location. Thus, the
biological processes involved in the development of the tu-
mor may be different from one location to another.

In order to evaluate such hypotheses, pre-treatment frozen
tumor samples were obtained from 53 children with newly
diagnosed pHGG from Necker Enfants Malades (Paris,
France) from Puget et al, (2012). The 53 tumors are di-
vided into 3 locations: supratentorial (HEMI), central nu-
clei (MIDL), and brain stem (DIPG). The ﬁnal dataset is or-
ganized in 3 blocks of variables deﬁned for the 53 tumors:
X is a block of indicator variables describing the location
category, the second data matrix Y provides the expression
of 15 702 genes (GE). The third data matrix Z contains the
imbalances of 1229 segments (CGH) of chromosomes.

For X, we use a linear kernel, which is characteristic for
indicator variables, and for Y and Z, the kernel was cho-
sen to be the Gaussian kernel with σ selected as the median
of pairwise distances. The p-value of our relative depen-
dency test is < 10−5. This shows that the tumor location
in the brain is more dependent on gene expression than on
chromosomal imbalances. By contrast with Section 5.1,
the independent test was also able to ﬁnd the same order-

1

2

4

5

6 ×10−3

3
HSIC XY

Figure 5. 2σ iso-curves of the Gaussian distributions estimated
from the pediatric Glioma data. As before, the dependent test
has a much lower variance than the independent test. The tests
support the stronger dependence on the tumor location to gene
expression than chromosomal imbalances.

ing of dependence, but with a p-value that is three orders of
magnitude larger (p = 0.005). Figure 5 shows iso-curves
of the Gaussian distributions estimated in the independent
and dependent tests. The empirical relative dependency is
consistent with ﬁndings in the medical literature, and pro-
vides additional statistical support for the importance of
tumor location in Glioma (Gilbertson & Gutmann, 2007;
Palm et al., 2009; Puget et al., 2012).

6. Conclusions

We have described a novel statistical test that determines
whether a source random variable is more strongly de-
pendent on one target random variable or another. This
test, built on the Hilbert-Schmidt Independence Criterion,
is low variance, consistent, and unbiased. We have shown
that our test is strictly more powerful than a test that does
not exploit the covariance between HSIC statistics, and
empirically achieves p-values several orders of magnitude
smaller. We have empirically demonstrated the test perfor-
mance on synthetic data, where the degree of dependence
could be controlled; on the challenging problem of iden-
tifying language groups from a multilingual corpus; and
for ﬁnding the most important determinant of Glioma type.
The computation and memory requirements of the test are
quadratic in the sample size, matching the performance of
HSIC and related tests for dependence between two ran-
dom variables. The test is therefore scalable to the wide
range of problem instances where non-parametric depen-
dency tests are currently applied. We have generalized the
test framework to more than two HSIC statistics, and have
given an algorithm to construct a consistent, low-variance,
unbiased test in this setting.

Acknowledgements

We thank Ioannis Antonoglou for helpful discussions. The
ﬁrst author is supported by a fellowship from Centrale-

A low variance consistent test of relative dependency

Sup´elec. This work is partially funded by the Euro-
pean Commission through ERC Grant 259112 and FP7-
MCCIG334380.

Gretton, A. and Gyorﬁ, L. Consistent nonparametric tests
Journal of Machine Learning Re-

of independence.
search, 11:1391–1423, 2010.

References

Arcones, M. A. and Gine, E. Limit theorems for U-
processes. The Annals of Probability, pp. 1494–1542,
1993.

Bouckaert, R., Lemey, P., Dunn, M., Greenhill, S. J., Alek-
seyenko, A. V., Drummond, A. J., Gray, R. D., Suchard,
M. A., and Atkinson, Q. D. Mapping the origins and ex-
pansion of the Indo-European language family. Science,
337(6097):957–960, 2012.

Bring, J. A geometric approach to compare variables in
a regression model. The American Statistician, 50(1):
57–62, 1996.

Cortes, C., Mohri, M., and Rostamizadeh, A. Learning
non-linear combinations of kernels. In Neural Informa-
tion Processing Systems, 2009.

Cortes, C., Mohri, M., and Rostamizadeh, A. Algorithms
for learning kernels based on centered alignment. Jour-
nal of Machine Learning Research, 13:795–828, 2012.

Darlington, Richard B. Multiple regression in psychologi-
cal research and practice. Psychological bulletin, 69(3):
161, 1968.

Dauxois, J. and Nkiet, G. M. Nonlinear canonical analy-
sis and independence tests. Annals of Statistics, 26(4):
1254–1278, 1998.

Fukumizu, K., Bach, F. R., and Gretton, A.

Statisti-
cal consistency of kernel canonical correlation analysis.
The Journal of Machine Learning Research, 8:361–383,
2007.

Fukumizu, K., Gretton, A., Sun, X., and Sch¨olkopf, B. Ker-
In Advances
nel measures of conditional dependence.
in Neural Information Processing Systems, pp. 489–496.
MIT Press, 2008.

Gretton, A., Bousquet, O., Smola, A. J., and Sch¨olkopf, B.
Measuring statistical dependence with Hilbert-Schmidt
In Algorithmic Learning Theory, pp. 63–77,
norms.
2005.

Gretton, A., Fukumizu, K., Teo, C.-H., Song, L.,
Sch¨olkopf, B., and Smola, A. J. A kernel statistical test
of independence. In Neural Information Processing Sys-
tems, pp. 585–592, 2008.

Gretton, A., Sejdinovic, D., Strathmann, H.and Balakrish-
nan, S., Pontil, M., Fukumizu, K., and Sriperumbudur,
B. K. Optimal kernel choice for large-scale two-sample
tests. In Advances in Neural Information Processing Sys-
tems, pp. 1205–1213, 2012.

Gunn, S. R. and Kandola, J. S. Structural modelling with
sparse kernels. Machine Learning, 48(1):137–163, 2002.

Heller, R., Heller, Y., and Gorﬁne, M. A consistent mul-
tivariate test of association based on ranks of distances.
Biometrika, 100(2):503–510, 2013.

Hoeffding, W. Probability inequalities for sums of bounded
random variables. Journal of the American statistical
association, 58(301):13–30, 1963.

Kinney, J. B. and Atwal, G. S. Equitability, mutual infor-
mation, and the maximal information coefﬁcient. Pro-
ceedings of the National Academy of Sciences, 2014.

Koehn, P. Europarl: A parallel corpus for statistical ma-
chine translation. In MT summit, volume 5, pp. 79–86,
2005.

Palm, T., Figarella-Branger, D., Chapon, F., Lacroix, C.,
Gray, F., Scaravilli, F., Ellison, D. W., Salmon, I.,
Vikkula, M., and Godfraind, C. Expression proﬁling
of ependymomas unravels localization and tumor grade-
speciﬁc tumorigenesis. Cancer, 115(17):3955–3968,
2009.

Gilbertson, R. J. and Gutmann, D. H. Tumorigenesis in the
brain: location, location, location. Cancer research, 67
(12):5579–5582, 2007.

Peters, C., Braschler, M., and Clough, P. Multilin-
gual Information Retrieval: From Research to Practice.
Springer, 2012.

Gray, R. D. and Atkinson, Q. D. Language-tree divergence
times support the Anatolian theory of Indo-European
origin. Nature, 426(6965):435–439, 2003.

Gretton, A. A simpler condition for consistency of a kernel

independence test. arXiv:1501.06103, 2015.

Puget, S., Philippe, C., Bax, D., Job, B., Varlet, P., Ju-
nier, M. P., Andreiuolo, F., Carvalho, D., Reis, R.,
and Guerrini-Rousseau, L. Mesenchymal transition
and PDGFRA ampliﬁcation/mutation are key distinct
oncogenic events in pediatric diffuse intrinsic pontine
gliomas. PloS one, 7(2):e30313, 2012.

A low variance consistent test of relative dependency

Reshef, D., Reshef, Y., Finucane, H., Grossman, S.,
McVean, G., Turnbaugh, P., Lander, E., Mitzenmacher,
M., and Sabeti, P. Detecting novel associations in large
datasets. Science, 334(6062), 2011.

Sejdinovic, D., Gretton, A., and Bergsma, W. A kernel
test for three-variable interactions. In Neural Informa-
tion Processing Systems, 2013a.

Sejdinovic, D., Sriperumbudur, B., Gretton, A., and Fuku-
mizu, K. Equivalence of distance-based and RKHS-
based statistics in hypothesis testing. Annals of Statis-
tics, 41(5):2263–2702, 2013b.

Sen, A. and Srivastava, M. Regression Analysis – Theory,
Methods, and Applications. Springer-Verlag, 2011.

Serﬂing, R. J. Approximation theorems of mathematical
statistics. Wiley Series in Probability and Statistics. Wi-
ley, 1981.

Song, L., Smola, A., Gretton, A., Bedo, J., and Borg-
wardt, K. Feature selection via dependence maximiza-
tion. Journal of Machine Learning Research, 13:1393–
1434, 2012.

Sz´ekely, G., Rizzo, M., and Bakirov, N. Measuring and
testing dependence by correlation of distances. Annals
of Statistics, 35(6):2769–2794, 2007.

Trommershauser, J., Kording, K., and Landy, M. S. Sen-
sory Cue Integration. Oxford University Press, 2011.

von Bahr, Bengt. On the convergence of moments in
the central limit theorem. The Annals of Mathematical
Statistics, 36(3):808–818, 06 1965.

Zhang, K., Peters, J., Janzing, D., B., and Sch¨olkopf, B.
Kernel-based conditional independence test and applica-
tion in causal discovery. In 27th Conference on Uncer-
tainty in Artiﬁcial Intelligence, pp. 804–813, 2011.

