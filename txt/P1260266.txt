0
2
0
2
 
n
a
J
 
5
 
 
]
L
C
.
s
c
[
 
 
4
v
5
4
2
7
0
.
8
0
9
1
:
v
i
X
r
a

GlossBERT: BERT for Word Sense Disambiguation
with Gloss Knowledge

Luyao Huang, Chi Sun, Xipeng Qiu∗, Xuanjing Huang
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, China
{lyhuang18,sunc17,xpqiu,xjhuang}@fudan.edu.cn

Abstract

Word Sense Disambiguation (WSD) aims to
ﬁnd the exact sense of an ambiguous word
in a particular context. Traditional supervised
methods rarely take into consideration the lex-
ical resources like WordNet, which are widely
utilized in knowledge-based methods. Recent
studies have shown the effectiveness of incor-
porating gloss (sense deﬁnition) into neural
networks for WSD. However, compared with
traditional word expert supervised methods,
they have not achieved much improvement. In
this paper, we focus on how to better leverage
gloss knowledge in a supervised neural WSD
system. We construct context-gloss pairs and
propose three BERT-based models for WSD.
We ﬁne-tune the pre-trained BERT model on
SemCor3.0 training corpus and the experimen-
tal results on several English all-words WSD
benchmark datasets show that our approach
outperforms the state-of-the-art systems 1.

1 Introduction

Word Sense Disambiguation (WSD) is a funda-
mental task and long-standing challenge in Nat-
ural Language Processing (NLP), which aims to
ﬁnd the exact sense of an ambiguous word in a par-
ticular context (Navigli, 2009). Previous WSD ap-
proaches can be grouped into two main categories:
knowledge-based and supervised methods.

Knowledge-based WSD methods rely on lex-
ical resources like WordNet (Miller, 1995) and
usually exploit two kinds of lexical knowledge.
The gloss, which deﬁnes a word sense mean-
ing,
is ﬁrst utilized in Lesk algorithm (Lesk,
1986) and then widely taken into account in many
other approaches (Banerjee and Pedersen, 2002;
Basile et al., 2014). Besides, structural properties
of semantic graphs are mainly used in graph-based
algorithms (Agirre et al., 2014; Moro et al., 2014).

∗Corresponding author.
1https://github.com/HSLCY/GlossBERT

2010;

supervised WSD methods
Traditional
Shen et al.,
2013;
(Zhong and Ng,
Iacobacci et al.,
extracting
on
manually designed features and then train a
dedicated classiﬁer (word expert) for every target
lemma.

2016)

focus

Although word expert supervised WSD meth-
they are less ﬂexible than
ods perform better,
knowledge-based methods in the all-words WSD
task (Raganato et al., 2017a). Recent neural-based
methods are devoted to dealing with this prob-
lem. K˚ageb¨ack and Salomonsson (2016) present
a supervised classiﬁer based on Bi-LSTM, which
shares parameters among all word types except the
last layer. Raganato et al. (2017a) convert WSD
task to a sequence labeling task, thus building a
uniﬁed model for all polysemous words. However,
neither of them can totally beat the best word ex-
pert supervised methods.

More recently, Luo et al. (2018b) propose to
leverage the gloss information from WordNet
and model the semantic relationship between the
context and gloss in an improved memory net-
work. Similarly, Luo et al. (2018a) introduce a
(hierarchical) co-attention mechanism to generate
co-dependent representations for the context and
gloss. Their attempts prove that incorporating
gloss knowledge into supervised WSD approach
is helpful, but they still have not achieved much
improvement, because they may not make full use
of gloss knowledge.

In this paper, we focus on how to better lever-
age gloss information in a supervised neural WSD
system. Recently, the pre-trained language mod-
els, such as ELMo (Peters et al., 2018) and BERT
(Devlin et al., 2018), have shown their effective-
ness to alleviate the effort of feature engineering.
Especially, BERT has achieved excellent results
in question answering (QA) and natural language
inference (NLI). We construct context-gloss pairs

Sentence with four targets:
Your research stopped when a convenient assertion could be made.

Context-Gloss Pairs of the target word [research]
[CLS] Your research ... [SEP] systematic investigation to ... [SEP]
[CLS] Your research ... [SEP] a search for knowledge [SEP]
[CLS] Your research ... [SEP] inquire into [SEP]
[CLS] Your research ... [SEP] attempt to ﬁnd out in a ... [SEP]

Context-Gloss Pairs with weak supervision of the target word [research]
[CLS] Your “research” ... [SEP] research: systematic investigation to ... [SEP]
[CLS] Your “research” ... [SEP] research: a search for knowledge [SEP]
[CLS] Your “research” ... [SEP] research: inquire into [SEP]
[CLS] Your “research” ... [SEP] research: attempt to ﬁnd out in a ... [SEP]

Label
Yes
No
No
No

Label
Yes
No
No
No

Sense Key
research%1:04:00::
research%1:09:00::
research%2:31:00::
research%2:32:00::

Sense Key
research%1:04:00::
research%1:09:00::
research%2:31:00::
research%2:32:00::

Table 1: The construction methods. The sentence is taken from SemEval-2007 WSD dataset. The ellipsis “...”
indicates the remainder of the sentence or the gloss.

from glosses of all possible senses (in WordNet)
of the target word, thus treating WSD task as a
sentence-pair classiﬁcation problem. We ﬁne-tune
the pre-trained BERT model on SemCor3.0 train-
ing corpus.

Recently2, we are informed by Vial et al.
(2019)3 that they also use BERT and incorpo-
rate WordNet as lexical knowledge in their super-
vised WSD system. But our work is much differ-
ent from theirs. They exploit the semantic rela-
tionships between senses such as synonymy, hy-
pernymy and hyponymy and rely on pre-trained
BERT word vectors (feature-based approach); we
leverage gloss knowledge (sense deﬁnition) and
use BERT through ﬁne-tuning procedures. Out of
respect, we add their results in Table 3. However,
the results of their feature-based approach in the
same experimental setup (single training set and
single model) are not as good as our ﬁne-tuning
approach although their ensemble systems (with
another training set WNGC) achieve better perfor-
mance.

In particular, our contribution is two-fold:
1. We construct context-gloss pairs and propose

three BERT-based models for WSD.

2. We ﬁne-tune the pre-trained BERT model on
SemCor3.0 training corpus, and the experimental
results on several English all-words WSD bench-
mark datasets show that our approach outperforms
the state-of-the-art systems.

2 Methodology

In this section, we describe our method in detail.

2after we submitted the ﬁnal version to the conference
3their paper is available on arXiv after our ﬁrst submission

to the conference in May, 2019

2.1 Task Deﬁnition

In WSD, a sentence s usually consists of a series
of words: {w1, · · · , wm}, and some of the words
{wi1, · · · , wik } are targets {t1, · · · , tk} need to
be disambiguated. For each target t, its candi-
date senses {c1, · · · , cn} come from entries of its
lemma in a pre-deﬁned sense inventory (usually
WordNet). Therefore, WSD task aims to ﬁnd the
most suitable entry (symbolized as unique sense
key) for each target in a sentence. See a sentence
example in Table 1.

2.2 BERT

BERT (Devlin et al., 2018) is a new language rep-
resentation model, and its architecture is a multi-
layer bidirectional Transformer encoder. BERT
model is pre-trained on a large corpus and two
novel unsupervised prediction tasks, i.e., masked
language model and next sentence prediction tasks
are used in pre-training. When incorporating
BERT into downstream tasks, the ﬁne-tuning pro-
cedure is recommended. We ﬁne-tune the pre-
trained BERT model on WSD task.

BERT(Token-CLS) Since every target in a sen-
tence needs to be disambiguated to ﬁnd its ex-
act sense, WSD task can be regarded as a token-
level classiﬁcation task. To incorporate BERT to
WSD task, we take the ﬁnal hidden state of the
token corresponding to the target word (if more
than one token, we average them) and add a clas-
siﬁcation layer for every target lemma, which is
the same as the last layer of the Bi-LSTM model
(K˚ageb¨ack and Salomonsson, 2016).

2.3 GlossBERT

BERT can explicitly model the relationship of a
pair of texts, which has shown to be beneﬁcial
to many pair-wise natural language understanding
tasks. In order to fully leverage gloss information,
we propose GlossBERT to construct context-gloss
pairs from all possible senses of the target word in
WordNet, thus treating WSD task as a sentence-
pair classiﬁcation problem.

We describe our construction method with an
example (See Table 1). There are four targets in
this sentence, and here we take target word re-
search as an example:

Context-Gloss Pairs The sentence containing
target words is denoted as context sentence. For
each target word, we extract glosses of all N pos-
sible senses (here N = 4) of the target word
(research) in WordNet to obtain the gloss sen-
tence. [CLS] and [SEP] marks are added to the
context-gloss pairs to make it suitable for the in-
put of BERT model. A similar idea is also used in
aspect-based sentiment analysis (Sun et al., 2019).

Context-Gloss Pairs with Weak Supervision
Based on the previous construction method, we
add weak supervised signals to the context-gloss
pairs (see the highlighted part in Table 1). The
signal in the gloss sentence aims to point out the
target word, and the signal in the context sentence
aims to emphasize the target word considering the
situation that a target word may occur more than
one time in the same sentence.

Therefore, each target word has N context-gloss
pair training instances (label ∈ {yes, no}). When
testing, we output the probability of label = yes
of each context-gloss pair and choose the sense
corresponding to the highest probability as the pre-
diction label of the target word. We experiment
with three GlossBERT models:

GlossBERT(Token-CLS) We use context-gloss
pairs as input. We highlight the target word by tak-
ing the ﬁnal hidden state of the token correspond-
ing to the target word (if more than one token,
we average them) and add a classiﬁcation layer
(label ∈ {yes, no}).

Dataset
SemCor
SE2
SE3
SE07
SE13
SE15

Total
226036
2282
1850
455
1644
1022

Noun
87002
1066
900
159
1644
531

Verb
88334
517
588
296
0
251

Adj
31753
445
350
0
0
160

Adv
18947
254
12
0
0
80

Table 2: Statistics of the different parts of speech anno-
tations in English all-words WSD datasets.

(label ∈ {yes, no}), which does not highlight the
target word.

GlossBERT(Sent-CLS-WS) We use context-
gloss pairs with weak supervision as input. We
take the ﬁnal hidden state of the ﬁrst token [CLS]
and add a classiﬁcation layer (label ∈ {yes, no}),
which weekly highlight the target word by the
weak supervision.

3 Experiments

3.1 Datasets

The statistics of the WSD datasets are shown in
Table 2.

Training Dataset Following previous work
(Luo et al., 2018a,b; Raganato et al., 2017a,b;
Iacobacci et al., 2016; Zhong and Ng, 2010), we
choose SemCor3.0 as training corpus, which is the
largest corpus manually annotated with WordNet
sense for WSD.

Evaluation Datasets We evaluate our method
on several English all-words WSD datasets. For
a fair comparison, we use the benchmark datasets
proposed by Raganato et al. (2017b) which in-
clude ﬁve standard all-words ﬁne-grained WSD
datasets from the Senseval and SemEval com-
petitions: Senseval-2 (SE2), Senseval-3 (SE3),
SemEval-2007 (SE07), SemEval-2013 (SE13)
and SemEval-2015 (SE15). Following Luo et al.
(2018a), Luo et al. (2018b) and Raganato et al.
(2017a), we choose SE07, the smallest among
these test sets, as the development set.

GlossBERT(Sent-CLS) We use context-gloss
pairs as input. We take the ﬁnal hidden state
of the ﬁrst token [CLS] as the representation of
the whole sequence and add a classiﬁcation layer

WordNet Since Raganato et al. (2017b) map all
the sense annotations in these datasets from their
original versions to WordNet 3.0, we extract word
sense glosses from WordNet 3.0.

System
MFS baseline
Leskext+emb
Babelfy
IMS
IMS+emb
Bi-LSTM
Bi-LSTM+att.+LEX+P OS
GASext (Linear)
GASext (Concatenation)
CANs
HCAN
SemCor, hypernyms (single)
SemCor, hypernyms (ensemble)†
SemCor+WNGC, hypernyms (single)‡
SemCor+WNGC, hypernyms (ensemble)† ‡
BERT(Token-CLS)
GlossBERT(Sent-CLS)
GlossBERT(Token-CLS)
GlossBERT(Sent-CLS-WS)

Dev
SE07
54.5
56.7
51.6
61.3
62.6
-
64.8
-
-
-
-
-
69.5
-
73.4
61.1
69.2
71.9
72.5

SE2
65.6
63.0
67.0
70.9
72.2
71.1
72.0
72.4
72.2
72.2
72.8
-
77.5
-
79.7
69.7
76.5
77.0
77.7

Test Datasets
SE13
SE3
63.8
66.0
66.2
63.7
66.4
63.5
65.3
69.3
65.9
70.4
64.8
68.4
66.9
69.1
67.1
70.1
67.2
70.5
69.1
70.2
68.5
70.3
-
-
76.0
77.4
-
-
78.7
77.8
65.8
69.4
75.1
73.4
75.4
74.6
76.1
75.2

SE15 Noun Verb
49.8
67.7
67.1
51.1
70.0
64.6
50.7
68.9
70.3
55.8
70.5
69.5
56.6
71.9
71.5
55.9
69.5
68.3
57.5
71.5
71.5
58.1
71.9
72.1
57.7
72.2
72.6
56.5
73.5
72.2
58.2
72.7
72.8
-
-
-
65.9
79.6
78.3
-
-
-
68.7
81.4
82.6
57.1
70.5
69.5
64.8
78.3
79.5
66.5
78.3
79.3
66.9
79.3
80.4

Concatenation of Test Datasets
Adj
73.1
51.7
73.2
75.6
75.9
76.2
75.0
76.4
76.6
76.6
77.4
-
79.5
-
83.7
71.6
77.6
78.6
78.2

Adv
80.5
80.6
79.8
82.9
84.7
82.4
83.8
84.7
85.0
80.3
84.1
-
85.5
-
85.5
83.5
83.8
84.4
86.4

All
65.5
64.2
66.4
68.9
70.1
68.4
69.9
70.4
70.6
70.9
71.1
75.6
76.7
77.1
79.0
68.6
75.8
76.3
77.0

Table 3: F1-score (%) for ﬁne-grained English all-words WSD on the test sets in the framework of Raganato et al.
(2017b) (including the development set SE07). The six blocks list the MFS baseline, two knowledge-based sys-
tems, two traditional word expert supervised systems, six recent neural-based systems, one BERT feature-based
system and our systems, respectively. Results in ﬁrst three blocks come from Raganato et al. (2017b), and others
from the corresponding papers. † values are ensemble systems and ‡ values are models trained on both SemCor
and WNGC. Bold font indicates best single model system trained on SemCor, i.e. excludes † ‡ values since it is
meaningless to compare ensemble systems and models trained on two training sets with our single model trained
on SemCor training set only.

.

3.2 Settings

We use the pre-trained uncased BERTBASE
model4 for ﬁne-tuning, because we ﬁnd that
BERTLARGE model performs slightly worse than
BERTBASE in this task. The number of Trans-
former blocks is 12, the number of the hidden
layer is 768, the number of self-attention heads
is 12, and the total number of parameters of the
pre-trained model is 110M. When ﬁne-tuning, we
use the development set (SE07) to ﬁnd the optimal
settings for our experiments. We keep the dropout
probability at 0.1, set the number of epochs to 4.
The initial learning rate is 2e-5, and the batch size
is 64.

3.3 Results

Table 3 shows the performance of our method on
the English all-words WSD benchmark datasets.
We compare our approach with previous methods.
The ﬁrst block shows the MFS baseline, which
selects the most frequent sense in the training cor-
pus for each target word.

The second block shows two knowledge-based
systems. Leskext+emb (Basile et al., 2014) is a
variant of Lesk algorithm (Lesk, 1986) by calcu-

4https://storage.googleapis.com/bert models/2018 10 18/

uncased L-12 H-768 A-12.zip

lating the gloss-context overlap of the target word.
Babelfy (Moro et al., 2014) is a uniﬁed graph-
based approach which exploits the semantic net-
work structure from BabelNet.

The

shows

several

neural-based methods.

The third block shows two word expert tradi-
IMS (Zhong and Ng,
tional supervised systems.
2010) is a ﬂexible framework which trains SVM
classiﬁers and uses local features. And IMS+emb
(Iacobacci et al., 2016) is the best conﬁguration of
the IMS framework, which also integrates word
embeddings as features.
block
fourth

re-
cent
Bi-LSTM
(K˚ageb¨ack and Salomonsson, 2016) is a baseline
Bi-LSTM+att.+LEX+P OS
for neural models.
(Raganato et al., 2017a) is a multi-task learning
framework for WSD, POS tagging, and LEX
with self-attention mechanism, which converts
WSD to a sequence learning task.
GASext
(Luo et al., 2018b) is a variant of GAS which is a
gloss-augmented variant of the memory network
by extending gloss knowledge. CANs and HCAN
(Luo et al., 2018a) are sentence-level and hierar-
chical co-attention neural network models which
leverage gloss knowledge.

The ﬁfth block are feature-based BERT mod-
els (Vial et al., 2019) which exploit the semantic
relationships between senses such as synonymy,

hypernymy and hyponymy, and use pre-trained
BERT embeddings and transformer encoder lay-
ers. It is worth noting that our ﬁne-tuning method
is superior to their feature-based method under the
same experimental setup (single model + SemCor
training set).

In the last block, we report the performance of
our method. BERT(Token-CLS) is our baseline,
which does not incorporate gloss information, and
it performs slightly worse than previous traditional
supervised methods and recent neural-based meth-
ods. It proves that directly using BERT cannot ob-
tain performance growth. The other three methods
outperform other models by a substantial margin,
which proves that the improvements come from
leveraging BERT to better exploit gloss informa-
tion. It is worth noting that our method achieves
signiﬁcant improvements in SE07 and Verb over
previous methods, which have the highest ambi-
guity level among all datasets and all POS tags re-
spectively according to Raganato et al. (2017b).

Moreover, GlossBERT(Token-CLS) performs
better than GlossBERT(Sent-CLS), which proves
that highlighting the target word in the sentence
is important. However,
the weakly highlight-
ing method GlossBERT(Sent-CLS-WS) performs
best in most circumstances, which may result from
its combination of the advantages of the other two
methods.

3.4 Discussion

There are two main reasons for the great improve-
ments of our experimental results. First, we con-
struct context-gloss pairs and convert WSD prob-
lem to a sentence-pair classiﬁcation task which is
similar to NLI tasks and train only one classiﬁer,
which is equivalent to expanding the corpus. Sec-
ond, we leverage BERT (Devlin et al., 2018) to
better exploit the gloss information. BERT model
shows its advantage in dealing with sentence-pair
classiﬁcation tasks by its amazing improvement
on QA and NLI tasks. This advantage comes
from both of its two novel unsupervised prediction
tasks.

Compared with traditional word expert super-
vised methods, our GlossBERT shows its effec-
tiveness to alleviate the effort of feature engineer-
ing and does not require training a dedicated clas-
siﬁer for every target lemma. Compared with re-
cent neural-based methods, our solution is more
intuitive and can make better use of gloss knowl-

edge. Besides, our approach demonstrates that
when we ﬁne-tune BERT on a downstream task,
converting it into a sentence-pair classiﬁcation
task may be a good choice.

4 Conclusion

In this paper, we seek to better leverage gloss
knowledge in a supervised neural WSD system.
We propose a new solution to WSD by construct-
ing context-gloss pairs and then converting WSD
to a sentence-pair classiﬁcation task. We ﬁne-tune
the pre-trained BERT model on SemCor3.0 train-
ing corpus, and the experimental results on several
English all-words WSD benchmark datasets show
that our approach outperforms the state-of-the-art
systems.

Acknowledgments

We would like to thank the anonymous reviewers
for their valuable comments. The research work is
supported by National Natural Science Foundation
of China (No. 61751201 and 61672162), Shang-
hai Municipal Science and Technology Commis-
sion (16JC1420401 and 17JC1404100), Shanghai
Municipal Science and Technology Major Project
(No.2018SHZDZX01) and ZJLab.

References

Eneko Agirre, Oier L´opez de Lacalle, and Aitor Soroa.
2014. Random walks for knowledge-based word
sense disambiguation. Computational Linguistics,
40(1):57–84.

Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using wordnet. In International conference on
intelligent text processing and computational lin-
guistics, pages 136–145. Springer.

Pierpaolo Basile, Annalina Caputo, and Giovanni Se-
meraro. 2014. An enhanced lesk word sense dis-
ambiguation algorithm through a distributional se-
In Proceedings of COLING 2014,
mantic model.
the 25th International Conference on Computational
Linguistics: Technical Papers, pages 1591–1600.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2016. Embeddings for word sense
disambiguation: An evaluation study. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), volume 1, pages 897–907.

Chi Sun, Luyao Huang, and Xipeng Qiu. 2019. Utiliz-
ing bert for aspect-based sentiment analysis via con-
structing auxiliary sentence. In Proceedings of the
2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long and
Short Papers), pages 380–385.

Lo¨ıc Vial, Benjamin Lecouteux, and Didier Schwab.
Sense vocabulary compression through
for neu-
arXiv preprint

2019.
the semantic knowledge of wordnet
ral word sense disambiguation.
arXiv:1905.05677.

Zhi Zhong and Hwee Tou Ng. 2010. It makes sense:
A wide-coverage word sense disambiguation system
for free text. In Proceedings of the ACL 2010 system
demonstrations, pages 78–83.

Mikael K˚ageb¨ack and Hans Salomonsson. 2016. Word
sense disambiguation using a bidirectional lstm.
arXiv preprint arXiv:1606.03568.

Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a
pine cone from an ice cream cone. In Proceedings of
the 5th annual international conference on Systems
documentation, pages 24–26. Citeseer.

Fuli Luo, Tianyu Liu, Zexue He, Qiaolin Xia, Zhi-
fang Sui, and Baobao Chang. 2018a. Leveraging
gloss knowledge in neural word sense disambigua-
tion by hierarchical co-attention. In Proceedings of
the 2018 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1402–1411.

Fuli Luo, Tianyu Liu, Qiaolin Xia, Baobao Chang,
and Zhifang Sui. 2018b. Incorporating glosses into
neural word sense disambiguation. arXiv preprint
arXiv:1805.08028.

George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39–
41.

Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity linking meets word sense disam-
biguation: a uniﬁed approach. Transactions of the
Association for Computational Linguistics, 2:231–
244.

Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM computing surveys (CSUR), 41(2):10.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
In Proceedings of the 2018 Confer-
resentations.
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
2227–2237.

Alessandro Raganato, Claudio Delli Bovi, and Roberto
Navigli. 2017a. Neural sequence learning models
for word sense disambiguation. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1156–1167.

Alessandro Raganato, Jose Camacho-Collados, and
Roberto Navigli. 2017b. Word sense disambigua-
tion: A uniﬁed evaluation framework and empiri-
cal comparison. In Proceedings of the 15th Confer-
ence of the European Chapter of the Association for
Computational Linguistics: Volume 1, Long Papers,
pages 99–110.

Hui Shen, Razvan Bunescu, and Rada Mihalcea. 2013.
Coarse to ﬁne grained sense disambiguation in
wikipedia. In Second Joint Conference on Lexical
and Computational Semantics (* SEM), Volume 1:
Proceedings of the Main Conference and the Shared
Task: Semantic Textual Similarity, volume 1, pages
22–31.

